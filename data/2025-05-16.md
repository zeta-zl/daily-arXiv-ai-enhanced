<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.LG](#cs.LG) [Total: 87]
- [cs.AI](#cs.AI) [Total: 20]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [stat.ML](#stat.ML) [Total: 9]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.NE](#cs.NE) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.CV](#cs.CV) [Total: 21]
- [stat.ME](#stat.ME) [Total: 2]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 9]
- [math.NA](#math.NA) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [stat.OT](#stat.OT) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [cs.SI](#cs.SI) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.CR](#cs.CR) [Total: 7]
- [cs.CY](#cs.CY) [Total: 3]
- [stat.AP](#stat.AP) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Next Word Suggestion using Graph Neural Network](https://arxiv.org/abs/2505.09649)
*Abisha Thapa Magar,Anup Shakya*

Main category: cs.CL

TL;DR: 该论文提出了一种结合图卷积网络（GNN）和LSTM的上下文嵌入方法，用于语言建模中的下一个词预测，并在资源有限的情况下验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型需要巨额计算资源和数据，而该研究旨在通过更高效的上下文嵌入方法，以较少资源实现类似任务。

Method: 利用GNN中的图卷积操作编码上下文信息，并结合LSTM进行下一个词预测，实验基于自定义维基百科语料库。

Result: 在资源受限的条件下，该方法能够较好地预测下一个词，表现出一定的有效性。

Conclusion: 通过结合GNN和LSTM，该研究展示了一种资源高效的上下文嵌入方法，为语言建模提供了新思路。

Abstract: Language Modeling is a prevalent task in Natural Language Processing. The
currently existing most recent and most successful language models often tend
to build a massive model with billions of parameters, feed in a tremendous
amount of text data, and train with enormous computation resources which
require millions of dollars. In this project, we aim to address an important
sub-task in language modeling, i.e., context embedding. We propose an approach
to exploit the Graph Convolution operation in GNNs to encode the context and
use it in coalition with LSTMs to predict the next word given a local context
of preceding words. We test this on the custom Wikipedia text corpus using a
very limited amount of resources and show that this approach works fairly well
to predict the next word.

</details>


### [2] [DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models](https://arxiv.org/abs/2505.09655)
*Xiwen Chen,Wenhui Zhu,Peijie Qiu,Xuanzhao Dong,Hao Wang,Haiyu Wu,Huayu Li,Aristeidis Sotiras,Yalin Wang,Abolfazl Razi*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Diversity-aware Reward Adjustment (DRA)的方法，用于解决GRPO在语言模型后训练中因奖励信号单一导致的多样性-质量不一致问题。DRA通过Submodular Mutual Information (SMI)调整奖励，鼓励多样性和高质量样本的平衡。实验表明，DRA-GRPO在五个数学推理基准测试中表现优异，仅用7,000个微调样本和约55美元的训练成本即达到58.2%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: GRPO等强化学习方法在低资源语言模型后训练中表现出潜力，但其依赖的标量奖励信号无法捕捉语义多样性，导致多样性-质量不一致问题。因此，需要一种能显式融入语义多样性的奖励计算方法。

Method: 提出DRA方法，利用Submodular Mutual Information (SMI)对冗余补全降权，并放大多样性补全的奖励，从而在学习中平衡探索与高质量样本的利用。DRA可与GRPO及其变体DR.~GRPO结合，形成DRA-GRPO和DGA-DR.~GRPO。

Result: 在五个数学推理基准测试中，DRA-GRPO超越现有基线，仅用7,000个微调样本和55美元训练成本即实现58.2%的平均准确率，达到state-of-the-art性能。

Conclusion: DRA通过显式奖励多样性有效解决了GRPO的局限性，并在低资源设置下显著提升模型性能，为语言模型后训练提供了一种高效且经济的方法。

Abstract: Recent advances in reinforcement learning for language model post-training,
such as Group Relative Policy Optimization (GRPO), have shown promise in
low-resource settings. However, GRPO typically relies on solution-level and
scalar reward signals that fail to capture the semantic diversity among sampled
completions. This leads to what we identify as a diversity-quality
inconsistency, where distinct reasoning paths may receive indistinguishable
rewards. To address this limitation, we propose $\textit{Diversity-aware Reward
Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity
into the reward computation. DRA uses Submodular Mutual Information (SMI) to
downweight redundant completions and amplify rewards for diverse ones. This
encourages better exploration during learning, while maintaining stable
exploitation of high-quality samples. Our method integrates seamlessly with
both GRPO and its variant DR.~GRPO, resulting in $\textit{DRA-GRPO}$ and
$\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning
benchmarks and find that it outperforms recent strong baselines. It achieves
state-of-the-art performance with an average accuracy of 58.2%, using only
7,000 fine-tuning samples and a total training cost of approximately $55. The
code is available at https://github.com/xiwenc1/DRA-GRPO.

</details>


### [3] [Large Language Models Are More Persuasive Than Incentivized Human Persuaders](https://arxiv.org/abs/2505.09662)
*Philipp Schoenegger,Francesco Salvi,Jiacheng Liu,Xiaoli Nan,Ramit Debnath,Barbara Fasolo,Evelina Leivada,Gabriel Recchia,Fritz Günther,Ali Zarifhonarvar,Joe Kwon,Zahoor Ul Islam,Marco Dehnert,Daryl Y. H. Lee,Madeline G. Reinecke,David G. Kamper,Mert Kobaş,Adam Sandford,Jonas Kgomo,Luke Hewitt,Shreya Kapoor,Kerem Oktar,Eyup Engin Kucuk,Bo Feng,Cameron R. Jones,Izzy Gainsburg,Sebastian Olschewski,Nora Heinzelmann,Francisco Cruz,Ben M. Tappin,Tao Ma,Peter S. Park,Rayan Onyonka,Arthur Hjorth,Peter Slattery,Qingcheng Zeng,Lennart Finke,Igor Grossmann,Alessandro Salatiello,Ezra Karger*

Main category: cs.CL

TL;DR: 在实时对话测试中，前沿大语言模型（Claude Sonnet 3.5）的说服能力超过激励人类，无论引导正确答案还是错误答案均表现更优，且显著影响测试者的收益。


<details>
  <summary>Details</summary>
Motivation: 比较前沿AI与激励人类在实时对话中的说服能力，评估AI的说服潜力及其社会影响。

Method: 通过预注册的大规模激励实验，安排AI或人类作为说服者，试图在在线测试中引导参与者选择正确或错误答案。

Result: AI说服者在真实和欺骗性情境下均显著优于人类，既能提高测试者准确性（增加收益），也能降低准确性（减少收益）。

Conclusion: AI说服能力已超越激励人类，突显了对齐与治理框架的紧迫需求。

Abstract: We directly compare the persuasion capabilities of a frontier large language
model (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an
interactive, real-time conversational quiz setting. In this preregistered,
large-scale incentivized experiment, participants (quiz takers) completed an
online quiz where persuaders (either humans or LLMs) attempted to persuade quiz
takers toward correct or incorrect answers. We find that LLM persuaders
achieved significantly higher compliance with their directional persuasion
attempts than incentivized human persuaders, demonstrating superior persuasive
capabilities in both truthful (toward correct answers) and deceptive (toward
incorrect answers) contexts. We also find that LLM persuaders significantly
increased quiz takers' accuracy, leading to higher earnings, when steering quiz
takers toward correct answers, and significantly decreased their accuracy,
leading to lower earnings, when steering them toward incorrect answers.
Overall, our findings suggest that AI's persuasion capabilities already exceed
those of humans that have real-money bonuses tied to performance. Our findings
of increasingly capable AI persuaders thus underscore the urgency of emerging
alignment and governance frameworks.

</details>


### [4] [System Prompt Optimization with Meta-Learning](https://arxiv.org/abs/2505.09666)
*Yumin Choi,Jinheon Baek,Sung Ju Hwang*

Main category: cs.CL

TL;DR: 该论文提出了一种双层次系统提示优化方法，通过元学习框架优化系统提示，使其能适应多样化的用户提示并迁移到未见任务。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在针对单个查询或任务的用户提示优化，忽视了可跨任务和领域应用的系统提示优化，因此论文提出了这一问题。

Method: 采用元学习框架，通过对多数据集上的用户提示进行优化，同时迭代更新系统提示，确保两者的协同作用。

Result: 在14个未见数据集和5个不同领域的实验表明，优化的系统提示能有效泛化到多样化用户提示，并实现快速适应未见任务，性能提升。

Conclusion: 论文提出的方法成功解决了系统提示优化问题，展示了其在实际任务中的强大泛化能力和迁移性。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities, with
optimizing their input prompts playing a pivotal role in maximizing their
performance. However, while LLM prompts consist of both the task-agnostic
system prompts and task-specific user prompts, existing work on prompt
optimization has focused on user prompts specific to individual queries or
tasks, and largely overlooked the system prompt that is, once optimized,
applicable across different tasks and domains. Motivated by this, we introduce
the novel problem of bilevel system prompt optimization, whose objective is to
design system prompts that are robust to diverse user prompts and transferable
to unseen tasks. To tackle this problem, we then propose a meta-learning
framework, which meta-learns the system prompt by optimizing it over various
user prompts across multiple datasets, while simultaneously updating the user
prompts in an iterative manner to ensure synergy between them. We conduct
experiments on 14 unseen datasets spanning 5 different domains, on which we
show that our approach produces system prompts that generalize effectively to
diverse user prompts. Also, our findings reveal that the optimized system
prompt enables rapid adaptation even to unseen tasks, requiring fewer
optimization steps for test-time user prompts while achieving improved
performance.

</details>


### [5] [VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts](https://arxiv.org/abs/2505.09701)
*Xin Liu,Lechen Zhang,Sheza Munir,Yiyang Gu,Lu Wang*

Main category: cs.CL

TL;DR: 该论文提出了VeriFact框架和FactRBench基准，旨在提升长文本生成模型的事实性评估，通过解决不完整和缺失事实的问题，以及同时评估精确率和召回率，从而更全面地评估模型的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的长文本生成模型（LLM）在事实性评估上存在挑战，尤其是对复杂句间依赖关系的捕捉不充分，导致评估不够准确。需要一种方法能更全面地提取和验证事实，同时评估精确率和召回率。

Method: 论文介绍了VeriFact框架，通过识别和解决不完整或缺失的事实以提升事实提取的完整性，并设计FactRBench基准，通过参考高级LLM和人类撰写答案的事实集，同时评估精确率和召回率。

Result: 实验结果表明，VeriFact显著提升了事实的完整性和保留了关键关系信息，使得事实性评估更准确。FactRBench显示，同一模型族中的更大模型能提升精确率和召回率，但高精确率并不总是与高召回率相关。

Conclusion: 论文提出的VeriFact和FactRBench为长文本生成模型的事实性评估提供了更全面的方法，强调了精确率和召回率在评估中的重要性，为未来研究提供了新的方向和基准。

Abstract: Large language models (LLMs) excel at generating long-form responses, but
evaluating their factuality remains challenging due to complex inter-sentence
dependencies within the generated facts. Prior solutions predominantly follow a
decompose-decontextualize-verify pipeline but often fail to capture essential
context and miss key relational facts. In this paper, we introduce VeriFact, a
factuality evaluation framework designed to enhance fact extraction by
identifying and resolving incomplete and missing facts to support more accurate
verification results. Moreover, we introduce FactRBench , a benchmark that
evaluates both precision and recall in long-form model responses, whereas prior
work primarily focuses on precision. FactRBench provides reference fact sets
from advanced LLMs and human-written answers, enabling recall assessment.
Empirical evaluations show that VeriFact significantly enhances fact
completeness and preserves complex facts with critical relational information,
resulting in more accurate factuality evaluation. Benchmarking various open-
and close-weight LLMs on FactRBench indicate that larger models within same
model family improve precision and recall, but high precision does not always
correlate with high recall, underscoring the importance of comprehensive
factuality assessment.

</details>


### [6] [An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs](https://arxiv.org/abs/2505.09724)
*Gino Carmona-Díaz,William Jiménez-Leal,María Alejandra Grisales,Chandra Sripada,Santiago Amaya,Michael Inzlicht,Juan Pablo Bermúdez*

Main category: cs.CL

TL;DR: 该研究探讨了如何使用大型语言模型（LLM）高效分析和分类非结构化文本（如社交媒体帖子），通过迭代协作过程开发和测试分类法，并以个人目标为例展示了具体步骤和效果。


<details>
  <summary>Details</summary>
Motivation: 人工分析非结构化文本（如开放式回答或社交媒体内容）耗时且易受偏见影响，因此研究如何利用LLM在不牺牲质量的前提下高效分类文本。

Method: 采用自上而下（预定义分类法）和自下而上（数据驱动分类法）结合的方式，通过研究者与LLM的协作迭代开发、测试并优化分类法。

Result: 研究展示了如何通过提示词生成分类法、评估修改分类法、测试编码一致性，最终以高编码可靠性分类整个数据集。

Conclusion: LLM在文本分析中具有潜力，但需注意其局限性和迭代优化的重要性。

Abstract: Analyzing texts such as open-ended responses, headlines, or social media
posts is a time- and labor-intensive process highly susceptible to bias. LLMs
are promising tools for text analysis, using either a predefined (top-down) or
a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we
present a step-by-step tutorial to efficiently develop, test, and apply
taxonomies for analyzing unstructured data through an iterative and
collaborative process between researchers and LLMs. Using personal goals
provided by participants as an example, we demonstrate how to write prompts to
review datasets and generate a taxonomy of life domains, evaluate and refine
the taxonomy through prompt and direct modifications, test the taxonomy and
assess intercoder agreements, and apply the taxonomy to categorize an entire
dataset with high intercoder reliability. We discuss the possibilities and
limitations of using LLMs for text analysis.

</details>


### [7] [Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning](https://arxiv.org/abs/2505.09738)
*Shaurya Sharthak,Vinayak Pahalwan,Adithya Kamath,Adarsh Shirawalmath*

Main category: cs.CL

TL;DR: 论文提出了TokenAdapt框架，通过tokenizer移植和Supertokens预标记学习，解决了固定tokenization方案在多语言或专业应用中的效率与性能问题，显著减少了重新训练需求。


<details>
  <summary>Details</summary>
Motivation: 固定tokenization方案在多语言或专业应用中效率低且性能受限，现有方法需大量计算资源或无法完全保留语义。

Method: 1. Tokenadapt：模型无关的tokenizer移植方法，通过混合启发式初始化新token嵌入（局部子词分解和全局语义相似性）；2. Supertokens预标记学习以提高压缩率。

Result: TokenAdapt在零样本困惑度测试中显著优于基线（如ReTok和TransTokenizer），困惑率至少降低2倍。

Conclusion: TokenAdapt框架有效解决了tokenization限制，降低了重新训练成本，同时在语义保留和压缩效率上表现优异。

Abstract: Pretrained language models (LLMs) are often constrained by their fixed
tokenization schemes, leading to inefficiencies and performance limitations,
particularly for multilingual or specialized applications. This tokenizer
lock-in presents significant challenges. standard methods to overcome this
often require prohibitive computational resources. Although tokenizer
replacement with heuristic initialization aims to reduce this burden, existing
methods often require exhaustive residual fine-tuning and still may not fully
preserve semantic nuances or adequately address the underlying compression
inefficiencies. Our framework introduces two innovations: first, Tokenadapt, a
model-agnostic tokenizer transplantation method, and second, novel
pre-tokenization learning for multi-word Supertokens to enhance compression and
reduce fragmentation. Tokenadapt initializes new unique token embeddings via a
hybrid heuristic that combines two methods: a local estimate based on subword
decomposition using the old tokenizer, and a global estimate utilizing the
top-k semantically similar tokens from the original vocabulary. This
methodology aims to preserve semantics while significantly minimizing
retraining requirements. Empirical investigations validate both contributions:
the transplantation heuristic successfully initializes unique tokens, markedly
outperforming conventional baselines and sophisticated methods including
Transtokenizer and ReTok, while our Supertokens achieve notable compression
gains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid
initialization consistently yields lower perplexity ratios compared to both
ReTok and TransTokenizer baselines across different base models and newly
trained target tokenizers. TokenAdapt typically reduced the overall perplexity
ratio significantly compared to ReTok, yielding at least a 2-fold improvement
in these aggregate scores.

</details>


### [8] [Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques](https://arxiv.org/abs/2505.09794)
*J. Moreno-Casanova,J. M. Auñón,A. Mártinez-Pérez,M. E. Pérez-Martínez,M. E. Gas-López*

Main category: cs.CL

TL;DR: 该研究利用NLP工具uQuery和基于RoBERTa的模型bsc-bio-ehr-en3，从电子健康记录中自动提取肺癌和乳腺癌的关键临床信息，显著提高了数据提取的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 由于手动提取临床报告信息耗时且易错，研究旨在通过NLP技术（如NER），自动化提取肺癌和乳腺癌的关键数据，以改善患者治疗效果和公共卫生管理。

Method: 研究使用GMV的NLP工具uQuery和经微调的RoBERTa模型bsc-bio-ehr-en3，对西班牙语的电子健康记录进行命名实体识别（NER），提取并标准化临床信息。

Result: 模型在识别常见实体（如MET和PAT）上表现优异，但在低频实体（如EVOL）上仍有挑战。

Conclusion: NLP技术能有效提升临床数据提取的效率和准确性，尤其在肺癌和乳腺癌领域，但需进一步优化对低频实体的识别。

Abstract: Research projects, including those focused on cancer, rely on the manual
extraction of information from clinical reports. This process is time-consuming
and prone to errors, limiting the efficiency of data-driven approaches in
healthcare. To address these challenges, Natural Language Processing (NLP)
offers an alternative for automating the extraction of relevant data from
electronic health records (EHRs). In this study, we focus on lung and breast
cancer due to their high incidence and the significant impact they have on
public health. Early detection and effective data management in both types of
cancer are crucial for improving patient outcomes. To enhance the accuracy and
efficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels
at identifying relevant entities in clinical texts and converting them into
standardized formats such as SNOMED and OMOP. uQuery not only detects and
classifies entities but also associates them with contextual information,
including negated entities, temporal aspects, and patient-related details. In
this work, we explore the use of NLP techniques, specifically Named Entity
Recognition (NER), to automatically identify and extract key clinical
information from EHRs related to these two cancers. A dataset from Health
Research Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast
cancer and 400 lung cancer reports, was used, with eight clinical entities
manually labeled using the Doccano platform. To perform NER, we fine-tuned the
bsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained
in Spanish. Fine-tuning was performed using the Transformers architecture,
enabling accurate recognition of clinical entities in these cancer types. Our
results demonstrate strong overall performance, particularly in identifying
entities like MET and PAT, although challenges remain with less frequent
entities like EVOL.

</details>


### [9] [Exploring the generalization of LLM truth directions on conversational formats](https://arxiv.org/abs/2505.09807)
*Timour Ichmoukhamedov,David Martens*

Main category: cs.CL

TL;DR: 摘要探讨了大语言模型（LLM）中真假陈述在激活空间中的线性可分性问题，发现短对话中方向可泛化，但在长对话中效果差，并提出了一种改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLM中的“真理方向”是否能在不同对话格式中泛化，以及如何提升其在更复杂场景（如长对话）中的可靠性。

Method: 方法包括测试真理方向在不同对话格式中的泛化能力，并提出在对话结尾添加固定关键词来改善长对话中的表现。

Result: 结果表明，短对话中真理方向泛化良好，但长对话中表现较差；通过添加关键词，长对话中的泛化效果显著提升。

Conclusion: 结论指出，尽管改进方法有效，但开发可靠且泛化能力强的LLM谎言检测器仍面临挑战。

Abstract: Several recent works argue that LLMs have a universal truth direction where
true and false statements are linearly separable in the activation space of the
model. It has been demonstrated that linear probes trained on a single hidden
state of the model already generalize across a range of topics and might even
be used for lie detection in LLM conversations. In this work we explore how
this truth direction generalizes between various conversational formats. We
find good generalization between short conversations that end on a lie, but
poor generalization to longer formats where the lie appears earlier in the
input prompt. We propose a solution that significantly improves this type of
generalization by adding a fixed key phrase at the end of each conversation.
Our results highlight the challenges towards reliable LLM lie detectors that
generalize to new settings.

</details>


### [10] [KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning](https://arxiv.org/abs/2505.09825)
*Peiqi Sui,Juan Diego Rodriguez,Philippe Laban,Dean Murphy,Joseph P. Dexter,Richard Jean So,Samuel Baker,Pramit Chaudhuri*

Main category: cs.CL

TL;DR: 论文介绍了KRISTEVA，首个用于评估解释性推理的细读基准，测试大型语言模型（LLMs）在文学分析任务中的表现。尽管LLMs展现出一定的大学水平细读能力（49.7%-69.7%准确率），但在大部分任务中仍落后于人类评估者。


<details>
  <summary>Details</summary>
Motivation: 填补细读任务在大语言模型评估中的空白，尤其是文学分析领域未被现有基准（如MMLU）覆盖的问题。

Method: 基于课堂数据构建包含1331道多选题的KRISTEVA基准，设计三个逐步提升难度的任务集，分别测试风格特征提取、上下文信息检索和多跳推理能力。

Result: 当前最先进的LLMs在细读任务中表现中等（准确率49.7% - 69.7%），但在11项任务中有10项不及人类评估者。

Conclusion: LLMs初步具备文学细读能力，但与人类水平仍有差距，需进一步优化模型对风格与语境的深层次推理能力。

Abstract: Each year, tens of millions of essays are written and graded in college-level
English courses. Students are asked to analyze literary and cultural texts
through a process known as close reading, in which they gather textual details
to formulate evidence-based arguments. Despite being viewed as a basis for
critical thinking and widely adopted as a required element of university
coursework, close reading has never been evaluated on large language models
(LLMs), and multi-discipline benchmarks like MMLU do not include literature as
a subject. To fill this gap, we present KRISTEVA, the first close reading
benchmark for evaluating interpretive reasoning, consisting of 1331
multiple-choice questions adapted from classroom data. With KRISTEVA, we
propose three progressively more difficult sets of tasks to approximate
different elements of the close reading process, which we use to test how well
LLMs may seem to understand and reason about literary works: 1) extracting
stylistic features, 2) retrieving relevant contextual information from
parametric knowledge, and 3) multi-hop reasoning between style and external
contexts. Our baseline results find that, while state-of-the-art LLMs possess
some college-level close reading competency (accuracy 49.7% - 69.7%), their
performances still trail those of experienced human evaluators on 10 out of our
11 tasks.

</details>


### [11] [Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting](https://arxiv.org/abs/2505.09852)
*Apollinaire Poli Nemkova,Sarath Chandra Lingareddy,Sagnik Ray Choudhury,Mark V. Albert*

Main category: cs.CL

TL;DR: 该研究探索了大型语言模型（LLMs）在预测暴力冲突方面的能力，比较了其参数化知识与非参数化能力（借助外部数据）的效果，结果表明结合外部知识可提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估LLMs是否具备基于预训练权重的参数化知识来预测冲突升级和死亡人数，这对早期预警系统和政策制定至关重要。

Method: 采用两部分评估框架：参数化设置（仅依赖预训练知识）和非参数化设置（通过RAG结合外部冲突数据集和新闻）。研究覆盖2020-2024年非洲之角和中东冲突多发地区。

Result: 研究发现LLMs在冲突预测中表现出潜力，但结合外部结构化知识可显著提升模型性能。

Conclusion: 结论指出LLMs在冲突预测中存在局限性，但通过增强外部知识能有效弥补其不足，为实际应用提供支持。

Abstract: Large Language Models (LLMs) have shown impressive performance across natural
language tasks, but their ability to forecast violent conflict remains
underexplored. We investigate whether LLMs possess meaningful parametric
knowledge-encoded in their pretrained weights-to predict conflict escalation
and fatalities without external data. This is critical for early warning
systems, humanitarian planning, and policy-making. We compare this parametric
knowledge with non-parametric capabilities, where LLMs access structured and
unstructured context from conflict datasets (e.g., ACLED, GDELT) and recent
news reports via Retrieval-Augmented Generation (RAG). Incorporating external
information could enhance model performance by providing up-to-date context
otherwise missing from pretrained weights. Our two-part evaluation framework
spans 2020-2024 across conflict-prone regions in the Horn of Africa and the
Middle East. In the parametric setting, LLMs predict conflict trends and
fatalities relying only on pretrained knowledge. In the non-parametric setting,
models receive summaries of recent conflict events, indicators, and
geopolitical developments. We compare predicted conflict trend labels (e.g.,
Escalate, Stable Conflict, De-escalate, Peace) and fatalities against
historical data. Our findings highlight the strengths and limitations of LLMs
for conflict forecasting and the benefits of augmenting them with structured
external knowledge.

</details>


### [12] [Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries](https://arxiv.org/abs/2505.09902)
*Martin Capdevila,Esteban Villa Turek,Ellen Karina Chumbe Fernandez,Luis Felipe Polo Galvez,Luis Cadavid,Andrea Marroquin,Rebeca Vargas Quesada,Johanna Crew,Nicole Vallejo Galarraga,Christopher Rodriguez,Diego Gutierrez,Radhi Datla*

Main category: cs.CL

TL;DR: 该论文研究拉丁美洲和西班牙西班牙语变体之间的差异，强调区域本地化模型对于弥合社会语言学分歧的重要性，并提出五种西班牙语子变体以实现更好的本地化和用户增长。


<details>
  <summary>Details</summary>
Motivation: 由于西班牙语在拉丁美洲和西班牙之间存在显著的语言和社会文化差异，论文旨在展示本地化AI模型如何帮助弥合这些差异，提升用户信任并支持国际化战略。

Method: 论文通过对拉丁美洲和西班牙的西班牙语变体进行深入的社文化及语言学情境分析，提出五种西班牙语子变体的本地化策略。

Result: 研究表明，区域敏感的AI模型可以有效减少社会语言学分歧，提升本地化效率和用户满意度，同时促进可持续的用户增长。

Conclusion: 实施区域化的西班牙语AI模型不仅能增强用户信任，还能体现文化和语言学多样性，为国际化战略带来积极影响。

Abstract: Large language models are, by definition, based on language. In an effort to
underscore the critical need for regional localized models, this paper examines
primary differences between variants of written Spanish across Latin America
and Spain, with an in-depth sociocultural and linguistic contextualization
therein. We argue that these differences effectively constitute significant
gaps in the quotidian use of Spanish among dialectal groups by creating
sociolinguistic dissonances, to the extent that locale-sensitive AI models
would play a pivotal role in bridging these divides. In doing so, this approach
informs better and more efficient localization strategies that also serve to
more adequately meet inclusivity goals, while securing sustainable active daily
user growth in a major low-risk investment geographic area. Therefore,
implementing at least the proposed five sub variants of Spanish addresses two
lines of action: to foment user trust and reliance on AI language models while
also demonstrating a level of cultural, historical, and sociolinguistic
awareness that reflects positively on any internationalization strategy.

</details>


### [13] [From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models](https://arxiv.org/abs/2505.09924)
*Yidan Wang,Yubing Ren,Yanan Cao,Binxing Fang*

Main category: cs.CL

TL;DR: 提出了一种结合基于logits和sampling的水印方法的新型框架，通过三种策略（串行、并行、混合）平衡检测性、鲁棒性、文本质量和安全性，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）生成文本的滥用问题日益严重，现有水印方案在鲁棒性、文本质量和安全性之间存在权衡，亟需一种更优的解决方案。

Method: 设计了一个混合框架（串行、并行、混合策略），利用token熵和语义熵自适应嵌入水印，整合了logits-based和sampling-based方法的优势。

Result: 在多种数据集和模型上的实验表明，该方法优于现有基线，达到了SOTA性能。

Conclusion: 该框架为水印技术的多样化范式提供了新思路，代码已开源。

Abstract: The rise of Large Language Models (LLMs) has heightened concerns about the
misuse of AI-generated text, making watermarking a promising solution.
Mainstream watermarking schemes for LLMs fall into two categories: logits-based
and sampling-based. However, current schemes entail trade-offs among
robustness, text quality, and security. To mitigate this, we integrate
logits-based and sampling-based schemes, harnessing their respective strengths
to achieve synergy. In this paper, we propose a versatile symbiotic
watermarking framework with three strategies: serial, parallel, and hybrid. The
hybrid framework adaptively embeds watermarks using token entropy and semantic
entropy, optimizing the balance between detectability, robustness, text
quality, and security. Furthermore, we validate our approach through
comprehensive experiments on various datasets and models. Experimental results
indicate that our method outperforms existing baselines and achieves
state-of-the-art (SOTA) performance. We believe this framework provides novel
insights into diverse watermarking paradigms. Our code is available at
\href{https://github.com/redwyd/SymMark}{https://github.com/redwyd/SymMark}.

</details>


### [14] [Rethinking Prompt Optimizers: From Prompt Merits to Optimization](https://arxiv.org/abs/2505.09930)
*Zixiao Zhu,Hanzhang Zhou,Zijian Feng,Tianjiao Li,Chua Jia Jim Deryl,Mak Lee Onn,Gee Wah Ng,Kezhi Mao*

Main category: cs.CL

TL;DR: 该论文提出了一种名为MePO的轻量级提示优化方法，通过可解释设计提升提示质量，适用于不同规模的推理模型，具有成本低、隐私友好和广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法依赖高级LLMs，但其生成的冗长提示可能不兼容轻量级模型，影响响应质量。本研究旨在设计一种轻量级、可解释的提示优化方案，解决兼容性和效率问题。

Method: 论文首先定义了一组模型无关的提示质量指标，并通过实证验证其有效性。随后提出MePO——一个基于这些指标训练的轻量级提示优化器，使用轻量级LLM生成的提示构建偏好数据集。

Result: 实验表明，MePO在多种任务和模型类型中表现优异，避免了对在线优化的依赖，降低了成本和隐私风险，并展现出良好的泛化能力。

Conclusion: MePO为实际部署提供了可扩展且鲁棒的提示优化解决方案，其模型和数据集已开源。

Abstract: Prompt optimization (PO) offers a practical alternative to fine-tuning large
language models (LLMs), enabling performance improvements without altering
model weights. Existing methods typically rely on advanced, large-scale LLMs
like GPT-4 to generate optimized prompts. However, due to limited downward
compatibility, verbose, instruction-heavy prompts from advanced LLMs can
overwhelm lightweight inference models and degrade response quality. In this
work, we rethink prompt optimization through the lens of interpretable design.
We first identify a set of model-agnostic prompt quality merits and empirically
validate their effectiveness in enhancing prompt and response quality. We then
introduce MePO, a merit-guided, lightweight, and locally deployable prompt
optimizer trained on our preference dataset built from merit-aligned prompts
generated by a lightweight LLM. Unlike prior work, MePO avoids online
optimization reliance, reduces cost and privacy concerns, and, by learning
clear, interpretable merits, generalizes effectively to both large-scale and
lightweight inference models. Experiments demonstrate that MePO achieves better
results across diverse tasks and model types, offering a scalable and robust
solution for real-world deployment. Our model and dataset are available at:
https://github.com/MidiyaZhu/MePO

</details>


### [15] [Interim Report on Human-Guided Adaptive Hyperparameter Optimization with Multi-Fidelity Sprints](https://arxiv.org/abs/2505.09792)
*Michael Kamfonas*

Main category: cs.CL

TL;DR: 该案例研究采用分阶段超参数优化方法，比较了使用多阶段学习率调度和优化器参数分组的自然语言模型变体。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是通过优化超参数和提高模型效率来提升自然语言处理任务的性能。

Method: 方法包括使用短期的贝叶斯优化会话、多保真度超参数空间剪枝、渐进减半和人类指导，采用Optuna TPE采样器和Hyperband剪枝器。

Result: 研究在2021年Eberts和Ulges提出的联合实体和关系提取模型变体上验证了该方法。

Conclusion: 结论表明该方法在多任务自然语言模型中有效，尤其是在超参数优化和模型效率方面表现突出。

Abstract: This case study applies a phased hyperparameter optimization process to
compare multitask natural language model variants that utilize multiphase
learning rate scheduling and optimizer parameter grouping. We employ short,
Bayesian optimization sessions that leverage multi-fidelity, hyperparameter
space pruning, progressive halving, and a degree of human guidance. We utilize
the Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn
Gaussian process minimization. Initially, we use efficient low-fidelity sprints
to prune the hyperparameter space. Subsequent sprints progressively increase
their model fidelity and employ hyperband pruning for efficiency. A second
aspect of our approach is using a meta-learner to tune threshold values to
resolve classification probabilities during inference. We demonstrate our
method on a collection of variants of the 2021 Joint Entity and Relation
Extraction model proposed by Eberts and Ulges.

</details>


### [16] [Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph](https://arxiv.org/abs/2505.09945)
*Deeksha Prahlad,Chanhee Lee,Dongha Kim,Hokeun Kim*

Main category: cs.CL

TL;DR: 论文提出了一种基于知识图谱（KG）的检索增强生成（RAG）方法，用于解决大型语言模型（LLM）因缺乏及时、真实和个性化信息而导致的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）由于训练数据庞大且缺乏个性化信息，容易产生不准确或多余的输出（幻觉）。为了解决这一问题，研究利用了知识图谱（KG）存储结构化且实时更新的个性化数据，以增强LLM的生成能力。

Method: 论文提出了一种检索增强生成（RAG）方法，结合知识图谱（KG），特别是针对日历数据，来辅助LLM生成个性化响应。KG存储了结构化且频繁更新的个人数据，如日历、联系人和位置信息。

Result: 实验结果表明，与仅使用文本输入个人数据的基线LLM相比，该方法在理解个人信息和生成准确响应方面表现显著更优，且响应时间略有减少。

Conclusion: 通过结合知识图谱的检索增强生成方法，能够有效减少LLM的幻觉问题，同时提升个性化响应的准确性和效率。

Abstract: The advent of large language models (LLMs) has allowed numerous applications,
including the generation of queried responses, to be leveraged in chatbots and
other conversational assistants. Being trained on a plethora of data, LLMs
often undergo high levels of over-fitting, resulting in the generation of extra
and incorrect data, thus causing hallucinations in output generation. One of
the root causes of such problems is the lack of timely, factual, and
personalized information fed to the LLM. In this paper, we propose an approach
to address these problems by introducing retrieval augmented generation (RAG)
using knowledge graphs (KGs) to assist the LLM in personalized response
generation tailored to the users. KGs have the advantage of storing
continuously updated factual information in a structured way. While our KGs can
be used for a variety of frequently updated personal data, such as calendar,
contact, and location data, we focus on calendar data in this paper. Our
experimental results show that our approach works significantly better in
understanding personal information and generating accurate responses compared
to the baseline LLMs using personal data as text inputs, with a moderate
reduction in response time.

</details>


### [17] [DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs](https://arxiv.org/abs/2505.10013)
*Lake Yin,Fan Huang*

Main category: cs.CL

TL;DR: 该研究开发了一种名为DIF的基准方法，用于量化大型语言模型(LLMs)中的隐式偏见，发现回答问题准确性与隐式偏见呈负相关。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)隐式偏见不仅是伦理问题，也反映了技术缺陷，但目前缺乏标准化的评估方法。

Method: 通过评估带有社会人口角色信息的LLM逻辑和数学问题数据集，计算DIF（Demographic Implicit Fairness）基准。

Result: 该方法证实了LLM行为中存在隐式偏见，且回答问题准确性与隐式偏见呈负相关。

Conclusion: DIF方法为量化LLM隐式偏见提供了标准化工具，支持了隐式偏见对模型性能的技术影响。

Abstract: As Large Language Models (LLMs) have risen in prominence over the past few
years, there has been concern over the potential biases in LLMs inherited from
the training data. Previous studies have examined how LLMs exhibit implicit
bias, such as when response generation changes when different social contexts
are introduced. We argue that this implicit bias is not only an ethical, but
also a technical issue, as it reveals an inability of LLMs to accommodate
extraneous information. However, unlike other measures of LLM intelligence,
there are no standard methods to benchmark this specific subset of LLM bias. To
bridge this gap, we developed a method for calculating an easily interpretable
benchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM
logic and math problem datasets with sociodemographic personas. We demonstrate
that this method can statistically validate the presence of implicit bias in
LLM behavior and find an inverse trend between question answering accuracy and
implicit bias, supporting our argument.

</details>


### [18] [CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability](https://arxiv.org/abs/2505.10063)
*Han Peng,Jinhao Jiang,Zican Dong,Wayne Xin Zhao,Lei Fang*

Main category: cs.CL

TL;DR: 论文提出了一种名为CAFE的两阶段方法，通过粗到细的过滤和引导提升大语言模型在长上下文中的检索和推理能力，实验结果显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在处理长上下文输入时，检索和推理能力仍有不足，尤其是在平衡检索精准度和召回率方面存在挑战。

Method: CAFE方法分为两阶段：粗粒度过滤（利用检索头识别和排序相关文档）和细粒度引导（聚焦最相关内容）。

Result: CAFE在多个基准测试中表现优异，在Mistral模型上分别比SFT和RAG方法提升22.1%和13.7%的SubEM分数。

Conclusion: CAFE通过渐进式过滤和注意力引导有效提升多文档问答能力，证明了其在大语言模型应用中的价值。

Abstract: Advancements in Large Language Models (LLMs) have extended their input
context length, yet they still struggle with retrieval and reasoning in
long-context inputs. Existing methods propose to utilize the prompt strategy
and retrieval head to alleviate this limitation. However, they still face
challenges in balancing retrieval precision and recall, impacting their
efficacy in answering questions. To address this, we introduce $\textbf{CAFE}$,
a two-stage coarse-to-fine method to enhance multi-document question-answering
capacities. By gradually eliminating the negative impacts of background and
distracting documents, CAFE makes the responses more reliant on the evidence
documents. Initially, a coarse-grained filtering method leverages retrieval
heads to identify and rank relevant documents. Then, a fine-grained steering
method guides attention to the most relevant content. Experiments across
benchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%
SubEM improvement over SFT and RAG methods on the Mistral model, respectively.

</details>


### [19] [Dark LLMs: The Growing Threat of Unaligned AI Models](https://arxiv.org/abs/2505.10066)
*Michael Fire,Yitzhak Elbazis,Adi Wasenstein,Lior Rokach*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）易受“越狱”攻击的漏洞，并揭示了一种通用的攻击方法，能绕过多种先进模型的安全控制，产生有害输出。尽管攻击方法已公开数月，许多模型仍未修复。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示LLMs的固有脆弱性，尤其是针对‘越狱’攻击的敏感性，强调因训练数据中的‘暗黑’内容导致的模型安全缺陷及其潜在危害。

Method: 通过分析模型训练数据的漏洞，提出了一种通用的‘越狱’攻击方法，测试了多种先进的LLM模型，并评估其安全性响应。

Result: 研究发现，即使攻击方法已公开数月，许多主流LLM仍易受攻击，且行业对AI安全的应对措施不足，存在严重的风险漏洞。

Conclusion: 随着开源LLM的普及和训练成本降低，若不采取有效措施，LLM可能被广泛滥用，导致比预期更严重的风险。呼吁行业加强AI安全实践。

Abstract: Large Language Models (LLMs) rapidly reshape modern life, advancing fields
from healthcare to education and beyond. However, alongside their remarkable
capabilities lies a significant threat: the susceptibility of these models to
jailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems
from the very data they learn from. As long as this training data includes
unfiltered, problematic, or 'dark' content, the models can inherently learn
undesirable patterns or weaknesses that allow users to circumvent their
intended safety controls. Our research identifies the growing threat posed by
dark LLMs models deliberately designed without ethical guardrails or modified
through jailbreak techniques. In our research, we uncovered a universal
jailbreak attack that effectively compromises multiple state-of-the-art models,
enabling them to answer almost any question and produce harmful outputs upon
request. The main idea of our attack was published online over seven months
ago. However, many of the tested LLMs were still vulnerable to this attack.
Despite our responsible disclosure efforts, responses from major LLM providers
were often inadequate, highlighting a concerning gap in industry practices
regarding AI safety. As model training becomes more accessible and cheaper, and
as open-source LLMs proliferate, the risk of widespread misuse escalates.
Without decisive intervention, LLMs may continue democratizing access to
dangerous knowledge, posing greater risks than anticipated.

</details>


### [20] [Designing and Contextualising Probes for African Languages](https://arxiv.org/abs/2505.10081)
*Wisdom Aduah,Francois Meyer*

Main category: cs.CL

TL;DR: 论文通过系统研究非洲语言的预训练语言模型(PLMs)，发现针对非洲语言优化的PLMs比多语言PLMs能更好地编码目标语言的语言信息，并验证了语法和语义信息在不同层的分布模式。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解非洲语言的PLMs性能提升的原因，并探究其内部语言知识的分布和编码方式。

Method: 方法包括训练分层探针分析六种非洲语言的语言特征分布，设计控制任务验证探针性能，并使用MasakhaPOS数据集进行实验。

Result: 结果显示优化后的PLMs能更有效地编码目标语言信息，语法信息集中在中后层，语义信息则分布在各层，且性能源于PLMs内部知识而非探针记忆。

Conclusion: 研究将解释性技术应用于非洲语言PLMs，揭示了其成功策略（如主动学习和多语言适应）的底层机制。

Abstract: Pretrained language models (PLMs) for African languages are continually
improving, but the reasons behind these advances remain unclear. This paper
presents the first systematic investigation into probing PLMs for linguistic
knowledge about African languages. We train layer-wise probes for six
typologically diverse African languages to analyse how linguistic features are
distributed. We also design control tasks, a way to interpret probe
performance, for the MasakhaPOS dataset. We find PLMs adapted for African
languages to encode more linguistic information about target languages than
massively multilingual PLMs. Our results reaffirm previous findings that
token-level syntactic information concentrates in middle-to-last layers, while
sentence-level semantic information is distributed across all layers. Through
control tasks and probing baselines, we confirm that performance reflects the
internal knowledge of PLMs rather than probe memorisation. Our study applies
established interpretability techniques to African-language PLMs. In doing so,
we highlight the internal mechanisms underlying the success of strategies like
active learning and multilingual adaptation.

</details>


### [21] [XRAG: Cross-lingual Retrieval-Augmented Generation](https://arxiv.org/abs/2505.10089)
*Wei Liu,Sony Trenous,Leonardo F. R. Ribeiro,Bill Byrne,Felix Hieber*

Main category: cs.CL

TL;DR: XRAG 是一个新颖的基准测试，用于评估 LLMs 在跨语言检索增强生成（RAG）中用户语言与检索结果不匹配时的生成能力。该基准基于新闻文章构建，要求外部知识回答问题，并涵盖单语和多语检索场景。实验结果显示了两大挑战：单语检索中语言正确性不足，多语检索中跨语言推理困难。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 在跨语言检索增强生成（RAG）场景中的能力缺乏系统评估，尤其是在用户语言与检索结果不匹配时的表现。XRAG 旨在填补这一空白，并研究模型的推理能力。

Method: 从新闻文章中构建数据集，生成需外部知识回答的问题，提供单语和多语检索场景，并对检索文档标注相关性。实验评估了 5 个 LLM 在两种检索设置下的表现。

Result: 实验发现两个未报告的挑战：1) 单语检索中模型的输出语言正确性不足；2) 多语检索中跨语言推理成为主要瓶颈，而非非英语文本生成。

Conclusion: XRAG 揭示了 LLM 在跨语言 RAG 中的关键挑战，为未来研究提供了基准，并强调了跨语言推理能力的重要性。

Abstract: We propose XRAG, a novel benchmark designed to evaluate the generation
abilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)
settings where the user language does not match the retrieval results. XRAG is
constructed from recent news articles to ensure that its questions require
external knowledge to be answered. It covers the real-world scenarios of
monolingual and multilingual retrieval, and provides relevancy annotations for
each retrieved document. Our novel dataset construction pipeline results in
questions that require complex reasoning, as evidenced by the significant gap
between human and LLM performance. Consequently, XRAG serves as a valuable
benchmark for studying LLM reasoning abilities, even before considering the
additional cross-lingual complexity. Experimental results on five LLMs uncover
two previously unreported challenges in cross-lingual RAG: 1) in the
monolingual retrieval setting, all evaluated models struggle with response
language correctness; 2) in the multilingual retrieval setting, the main
challenge lies in reasoning over retrieved information across languages rather
than generation of non-English text.

</details>


### [22] [What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs](https://arxiv.org/abs/2505.10113)
*Xinlan Yan,Di Wu,Yibin Lei,Christof Monz,Iacer Calixto*

Main category: cs.CL

TL;DR: S-MedQA是一个用于评估大语言模型在细粒度临床专业中表现的英文医学问答数据集，研究发现专业数据的微调未必带来最佳表现，而改进更多源于领域转换而非知识注入。


<details>
  <summary>Details</summary>
Motivation: 旨在验证医学问答中知识注入假说的适用性，并探讨微调数据在医学领域的作用。

Method: 使用S-MedQA数据集，评估不同临床专业数据微调对模型性能的影响。

Result: 发现微调某一专业数据未必提升该专业表现，但临床相关术语的概率普遍增加，改进主要来自领域转换而非知识注入。

Conclusion: 建议重新思考医学领域微调数据的作用，并公开数据集和代码以便复现。

Abstract: In this paper, we introduce S-MedQA, an English medical question-answering
(QA) dataset for benchmarking large language models in fine-grained clinical
specialties. We use S-MedQA to check the applicability of a popular hypothesis
related to knowledge injection in the knowledge-intense scenario of medical QA,
and show that: 1) training on data from a speciality does not necessarily lead
to best performance on that specialty and 2) regardless of the specialty
fine-tuned on, token probabilities of clinically relevant terms for all
specialties increase consistently. Thus, we believe improvement gains come
mostly from domain shifting (e.g., general to medical) rather than knowledge
injection and suggest rethinking the role of fine-tuning data in the medical
domain. We release S-MedQA and all code needed to reproduce all our experiments
to the research community.

</details>


### [23] [GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs](https://arxiv.org/abs/2505.10143)
*Longchao Da,Parth Mitesh Shah,Kuan-Ru Liou,Jiaxing Zhang,Hua Wei*

Main category: cs.CL

TL;DR: 论文提出了一种基于知识图谱的检索增强生成框架GE-Chat，通过结合知识与CoT逻辑生成，提升LLM生成可验证的可靠回答的能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）生成的回答可能存在错误或幻觉信息，导致用户对其可信度产生疑虑，需要手动验证，效率低下且不可靠。

Method: 1. 上传文档构建知识图谱；2. 利用图谱增强检索代理，结合CoT逻辑、多跳子图搜索及蕴含式句子生成，实现精准证据检索。

Result: 实验表明，该方法在自由文本上下文中能更准确地定位证据，为LLM结论提供可追溯的资源验证，提升可信度判断。

Conclusion: GE-Chat框架通过知识增强与证据检索，显著提高了LLM生成回答的可信度，为依赖AI辅助决策的场景提供了可靠支持。

Abstract: Large Language Models are now key assistants in human decision-making
processes. However, a common note always seems to follow: "LLMs can make
mistakes. Be careful with important info." This points to the reality that not
all outputs from LLMs are dependable, and users must evaluate them manually.
The challenge deepens as hallucinated responses, often presented with seemingly
plausible explanations, create complications and raise trust issues among
users. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph
enhanced retrieval-augmented generation framework to provide Evidence-based
response generation. Specifically, when the user uploads a material document, a
knowledge graph will be created, which helps construct a retrieval-augmented
agent, enhancing the agent's responses with additional knowledge beyond its
training corpus. Then we leverage Chain-of-Thought (CoT) logic generation,
n-hop sub-graph searching, and entailment-based sentence generation to realize
accurate evidence retrieval. We demonstrate that our method improves the
existing models' performance in terms of identifying the exact evidence in a
free-form context, providing a reliable way to examine the resources of LLM's
conclusion and help with the judgment of the trustworthiness.

</details>


### [24] [Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning](https://arxiv.org/abs/2505.10182)
*Yoichi Ishibashi,Taro Yano,Masafumi Oyamada*

Main category: cs.CL

TL;DR: 论文提出了一种新的持续预训练方法（Reasoning CPT），通过合成数据重建文本背后的隐藏思维过程，提升大语言模型在多个领域的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的监督微调和强化学习方法在训练推理模型时局限于特定领域（如数学和编程），而持续预训练（CPT）虽无需任务特定信号，但如何合成有效的推理数据及其广泛影响尚待探索。

Method: 研究提出Reasoning CPT，利用STEM和法学语料生成的合成数据（包含隐藏思维）对Gemma2-9B模型进行持续预训练，并与标准CPT在MMLU基准上对比。

Result: Reasoning CPT在所有评估领域均表现更优，且推理能力可跨领域迁移。问题难度越高，性能提升越显著（最高提升8分），模型还能根据难度调整推理深度。

Conclusion: Reasoning CPT通过合成隐藏思维数据显著提升模型的通用推理能力，尤其在复杂问题上表现突出。

Abstract: Large Language Models (LLMs) have demonstrated significant improvements in
reasoning capabilities through supervised fine-tuning and reinforcement
learning. However, when training reasoning models, these approaches are
primarily applicable to specific domains such as mathematics and programming,
which imposes fundamental constraints on the breadth and scalability of
training data. In contrast, continual pretraining (CPT) offers the advantage of
not requiring task-specific signals. Nevertheless, how to effectively
synthesize training data for reasoning and how such data affect a wide range of
domains remain largely unexplored. This study provides a detailed evaluation of
Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden
thought processes underlying texts, based on the premise that texts are the
result of the author's thinking process. Specifically, we apply Reasoning CPT
to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and
Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis
reveals that Reasoning CPT consistently improves performance across all
evaluated domains. Notably, reasoning skills acquired in one domain transfer
effectively to others; the performance gap with conventional methods widens as
problem difficulty increases, with gains of up to 8 points on the most
challenging problems. Furthermore, models trained with hidden thoughts learn to
adjust the depth of their reasoning according to problem difficulty.

</details>


### [25] [The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think](https://arxiv.org/abs/2505.10185)
*Seongyun Lee,Seungone Kim,Minju Seo,Yongrae Jo,Dongyoung Go,Hyeonbin Hwang,Jinho Park,Xiang Yue,Sean Welleck,Graham Neubig,Moontae Lee,Minjoon Seo*

Main category: cs.CL

TL;DR: 该论文提出了CoT百科全书，一种自下而上的框架，用于分析和引导模型推理行为，通过自动提取、聚类和解释推理策略，提升模型的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义策略类型，无法全面捕捉模型推理行为的多样性，因此需要一个更自动化和全面的框架来分析模型推理。

Method: 提出了一个框架，自动从模型生成的CoT中提取多样化的推理标准，嵌入语义空间并聚类，生成可解释的对比规则。

Result: 实验表明，该框架比现有方法更全面和可解释，并能预测和引导模型使用更有效的推理策略。此外，训练数据格式对推理行为的影响远超数据领域。

Conclusion: CoT百科全书为模型推理行为提供了更深入的理解和实用指导，强调了数据格式在模型设计中的重要性。

Abstract: Long chain-of-thought (CoT) is an essential ingredient in effective usage of
modern large language models, but our understanding of the reasoning strategies
underlying these capabilities remains limited. While some prior works have
attempted to categorize CoTs using predefined strategy types, such approaches
are constrained by human intuition and fail to capture the full diversity of
model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up
framework for analyzing and steering model reasoning. Our method automatically
extracts diverse reasoning criteria from model-generated CoTs, embeds them into
a semantic space, clusters them into representative categories, and derives
contrastive rubrics to interpret reasoning behavior. Human evaluations show
that this framework produces more interpretable and comprehensive analyses than
existing methods. Moreover, we demonstrate that this understanding enables
performance gains: we can predict which strategy a model is likely to use and
guide it toward more effective alternatives. Finally, we provide practical
insights, such as that training data format (e.g., free-form vs.
multiple-choice) has a far greater impact on reasoning behavior than data
domain, underscoring the importance of format-aware model design.

</details>


### [26] [VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits](https://arxiv.org/abs/2505.10202)
*Jintian Shao,Hongyi Huang,Jiayi Wu,YiMing Cheng,ZhiYu Wu,You Shan,MingKai Zheng*

Main category: cs.CL

TL;DR: 论文提出VQ-Logits，一种利用向量量化(VQ)减少LLM输出层参数和计算负担的新方法，在保持性能接近的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLM)输出层的词汇量大，导致参数和计算成本高，现有方法结构复杂，需更高效且简单的解决方案。

Method: 用小型共享码本(K << V)替换大型输出嵌入矩阵，LLM预测码本上的logits，再将其映射到完整词汇空间。

Result: 实验显示VQ-Logits减少99%输出层参数，计算速度提升6倍，困惑度仅增加4%。

Conclusion: VQ-Logits高效且鲁棒，为LLM输出层优化提供了新思路。

Abstract: Large Language Models (LLMs) have achieved remarkable success but face
significant computational and memory challenges, particularly due to their
extensive output vocabularies. The final linear projection layer, mapping
hidden states to vocabulary-sized logits, often constitutes a substantial
portion of the model's parameters and computational cost during inference.
Existing methods like adaptive softmax or hierarchical softmax introduce
structural complexities. In this paper, we propose VQ-Logits, a novel approach
that leverages Vector Quantization (VQ) to drastically reduce the parameter
count and computational load of the LLM output layer. VQ-Logits replaces the
large V * dmodel output embedding matrix with a small, shared codebook of K
embedding vectors (K << V ). Each token in the vocabulary is mapped to one of
these K codebook vectors. The LLM predicts logits over this compact codebook,
which are then efficiently "scattered" to the full vocabulary space using the
learned or preassigned mapping. We demonstrate through extensive experiments on
standard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits
can achieve up to 99% parameter reduction in the output layer and 6x speedup in
logit computation, with only a marginal 4% increase in perplexity compared to
full softmax baselines. We further provide detailed ablation studies on
codebook size, initialization, and learning strategies, showcasing the
robustness and effectiveness of our approach.

</details>


### [27] [RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward](https://arxiv.org/abs/2505.10218)
*Zongsheng Wang,Kaili Sun,Bowen Wu,Qun Yu,Ying Li,Baoxun Wang*

Main category: cs.CL

TL;DR: 提出了RAIDEN-R1强化学习框架，通过VRAR奖励机制提升角色扮演对话代理的角色一致性，实验证明其性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决角色扮演对话代理（RPCAs）中角色一致性的挑战。

Method: 采用强化学习框架RAIDEN-R1，集成VRAR奖励机制，并通过多LLM协作构建角色感知的思维链数据集。

Result: 14B-GRPO模型在Script-Based Knowledge和Conversation Memory指标上分别达到88.04%和88.65%的准确率，优于基线模型。

Conclusion: 该工作填补了RPCA训练中的非量化空白，推动了角色感知推理模式的进步。

Abstract: Role-playing conversational agents (RPCAs) face persistent challenges in
maintaining role consistency. To address this, we propose RAIDEN-R1, a novel
reinforcement learning framework that integrates Verifiable Role-Awareness
Reward (VRAR). The method introduces both singular and multi-term mining
strategies to generate quantifiable rewards by assessing role-specific keys.
Additionally, we construct a high-quality, role-aware Chain-of-Thought dataset
through multi-LLM collaboration, and implement experiments to enhance reasoning
coherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's
superiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on
Script-Based Knowledge and Conversation Memory metrics, respectively,
outperforming baseline models while maintaining robustness. Case analyses
further reveal the model's enhanced ability to resolve conflicting contextual
cues and sustain first-person narrative consistency. This work bridges the
non-quantifiability gap in RPCA training and provides insights into role-aware
reasoning patterns, advancing the development of RPCAs.

</details>


### [28] [Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data](https://arxiv.org/abs/2505.10260)
*Poli Apollinaire Nemkova,Solomon Ubani,Mark V. Albert*

Main category: cs.CL

TL;DR: 该研究比较了多种先进LLM（GPT-3.5、GPT-4等）在俄语和乌克兰语社交媒体帖子上进行零样本和少样本标注的表现，重点关注人权侵犯的二元分类任务，并与人工标注进行对比。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在多语言复杂任务中的表现，特别是敏感领域（如人权侵犯标注）的可靠性，以及它们在跨语言和上下文相关任务中的适应性。

Method: 使用多种LLM对1000个样本进行标注，比较其与人工标注的一致性，测试不同提示语言（英语和俄语）下的表现，并分析错误模式。

Result: 研究发现不同LLM在标注任务中表现各异，揭示了它们在复杂多语言任务中的优势和局限性。

Conclusion: LLM在敏感多语言任务中具有一定潜力，但其表现因模型和语言而异，需进一步优化以适应实际应用场景。

Abstract: In the era of increasingly sophisticated natural language processing (NLP)
systems, large language models (LLMs) have demonstrated remarkable potential
for diverse applications, including tasks requiring nuanced textual
understanding and contextual reasoning. This study investigates the
capabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,
Mistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex
textual dataset comprising social media posts in Russian and Ukrainian.
Specifically, the focus is on the binary classification task of identifying
references to human rights violations within the dataset.
  To evaluate the effectiveness of these models, their annotations are compared
against a gold standard set of human double-annotated labels across 1000
samples. The analysis includes assessing annotation performance under different
prompting conditions, with prompts provided in both English and Russian.
Additionally, the study explores the unique patterns of errors and
disagreements exhibited by each model, offering insights into their strengths,
limitations, and cross-linguistic adaptability.
  By juxtaposing LLM outputs with human annotations, this research contributes
to understanding the reliability and applicability of LLMs for sensitive,
domain-specific tasks in multilingual contexts. It also sheds light on how
language models handle inherently subjective and context-dependent judgments, a
critical consideration for their deployment in real-world scenarios.

</details>


### [29] [The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine](https://arxiv.org/abs/2505.10261)
*Rui Yang,Huitao Li,Matthew Yu Heng Wong,Yuhe Ke,Xin Li,Kunyu Yu,Jingchi Liao,Jonathan Chong Kai Liew,Sabarinath Vinod Nair,Jasmine Chiat Ling Ong,Irene Li,Douglas Teodoro,Chuan Hong,Daniel Shu Wei Ting,Nan Liu*

Main category: cs.CL

TL;DR: 比较传统NLP与生成式LLM在医学任务中的表现差异，发现各有优势，并强调伦理使用的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索传统NLP与生成式LLM在不同医学任务中的表现差异，填补现有研究的空白。

Method: 通过分析19,123项研究数据，比较两种技术在医学任务中的效果。

Result: 生成式LLM在开放任务中表现更优，传统NLP在信息提取和分析任务中占优。

Conclusion: 两种技术各有优势，需结合伦理考量以充分发挥其在医学领域的潜力。

Abstract: Natural language processing (NLP) has been traditionally applied to medicine,
and generative large language models (LLMs) have become prominent recently.
However, the differences between them across different medical tasks remain
underexplored. We analyzed 19,123 studies, finding that generative LLMs
demonstrate advantages in open-ended tasks, while traditional NLP dominates in
information extraction and analysis tasks. As these technologies advance,
ethical use of them is essential to ensure their potential in medical
applications.

</details>


### [30] [From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making](https://arxiv.org/abs/2505.10282)
*Dubai Li,Nan Jiang,Kangping Huang,Ruiqi Tu,Shuyu Ouyang,Huayu Yu,Lin Qiao,Chen Yu,Tianshu Zhou,Danyang Tong,Qian Wang,Mengtao Li,Xiaofeng Zeng,Yu Tian,Xinping Tian,Jingsong Li*

Main category: cs.CL

TL;DR: Quicker是一款基于大型语言模型的临床决策支持系统，能自动化合成证据并生成临床建议。实验表明，它在检索敏感性和文献筛选方面表现优异，能显著缩短推荐开发时间。


<details>
  <summary>Details</summary>
Motivation: 临床证据整合到实时实践存在挑战，需自动化工具提升决策效率和准确性。

Method: Quicker通过全自动流程，从问题分解到生成临床建议，并结合用户偏好和交互界面。使用Q2CRBench-3数据集进行评估。

Result: Quicker在检索、筛选和推荐生成上表现接近专家水平，单次协作可将时间缩短至20-40分钟。

Conclusion: Quicker证明了帮助医生快速、可靠地进行循证决策的潜力。

Abstract: Clinical evidence, derived from rigorous research and data analysis, provides
healthcare professionals with reliable scientific foundations for informed
decision-making. Integrating clinical evidence into real-time practice is
challenging due to the enormous workload, complex professional processes, and
time constraints. This highlights the need for tools that automate evidence
synthesis to support more efficient and accurate decision making in clinical
settings. This study introduces Quicker, an evidence-based clinical decision
support system powered by large language models (LLMs), designed to automate
evidence synthesis and generate clinical recommendations modeled after standard
clinical guideline development processes. Quicker implements a fully automated
chain that covers all phases, from questions to clinical recommendations, and
further enables customized decision-making through integrated tools and
interactive user interfaces. To evaluate Quicker's capabilities, we developed
the Q2CRBench-3 benchmark dataset, based on clinical guideline development
records for three different diseases. Experimental results highlighted
Quicker's strong performance, with fine-grained question decomposition tailored
to user preferences, retrieval sensitivities comparable to human experts, and
literature screening performance approaching comprehensive inclusion of
relevant studies. In addition, Quicker-assisted evidence assessment effectively
supported human reviewers, while Quicker's recommendations were more
comprehensive and logically coherent than those of clinicians. In system-level
testing, collaboration between a single reviewer and Quicker reduced the time
required for recommendation development to 20-40 minutes. In general, our
findings affirm the potential of Quicker to help physicians make quicker and
more reliable evidence-based clinical decisions.

</details>


### [31] [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320)
*Chenxi Whitehouse,Tianlu Wang,Ping Yu,Xian Li,Jason Weston,Ilia Kulikov,Swarnadeep Saha*

Main category: cs.CL

TL;DR: J1是一种通过强化学习训练LLM-as-a-Judge模型的方法，优于现有8B和70B模型，尤其在解决评估偏差和提升思考能力方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 提升AI评估质量是瓶颈，LLM-as-a-Judge模型通过更强的思维链推理成为解决方案，需找到最佳训练方法以优化其判断能力。

Method: 采用强化学习方法J1，将可验证和不可验证的提示转换为带有可验证奖励的判断任务，激励模型思考并减少判断偏差。

Result: J1在8B和70B规模训练时优于包括DeepSeek-R1蒸馏模型在内的所有现有模型，甚至在某些基准上超越更小的o1-mini和R1模型。

Conclusion: J1模型通过学习制定评估标准、与自生成参考答案对比、重新评估响应正确性，显著提升了判断质量。

Abstract: The progress of AI is bottlenecked by the quality of evaluation, and powerful
LLM-as-a-Judge models have proved to be a core solution. Improved judgment
ability is enabled by stronger chain-of-thought reasoning, motivating the need
to find the best recipes for training such models to think. In this work we
introduce J1, a reinforcement learning approach to training such models. Our
method converts both verifiable and non-verifiable prompts to judgment tasks
with verifiable rewards that incentivize thinking and mitigate judgment bias.
In particular, our approach outperforms all other existing 8B or 70B models
when trained at those sizes, including models distilled from DeepSeek-R1. J1
also outperforms o1-mini, and even R1 on some benchmarks, despite training a
smaller model. We provide analysis and ablations comparing Pairwise-J1 vs
Pointwise-J1 models, offline vs online training recipes, reward strategies,
seed prompts, and variations in thought length and content. We find that our
models make better judgments by learning to outline evaluation criteria,
comparing against self-generated reference answers, and re-evaluating the
correctness of model responses.

</details>


### [32] [LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations](https://arxiv.org/abs/2505.10354)
*Yile Wang,Zhanyu Shen,Hui Huang*

Main category: cs.CL

TL;DR: LDIR提出了一种低维（低于500）、密集且可解释的文本嵌入方法，通过最远点采样生成数值维度，既保持了语义表示又具备可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本嵌入（如SimCSE和LLM2Vec）性能优秀但维度数值难以解释，而经典的稀疏可解释嵌入（如词袋模型）性能较差。需要一种兼具高性能和可解释性的低维文本嵌入方法。

Method: LDIR通过最远点采样生成数值维度，表示与不同锚点文本的语义相关性，实现低维密集且可解释的文本嵌入。

Result: LDIR在多个语义文本相似性、检索和聚类任务中表现接近黑盒基线模型，且显著优于其他可解释嵌入基线。

Conclusion: LDIR展示了低维密集嵌入在保持高性能的同时实现可解释性的潜力，为可解释文本表示提供了新方向。

Abstract: Semantic text representation is a fundamental task in the field of natural
language processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have
demonstrated excellent performance, but the values of each dimension are
difficult to trace and interpret. Bag-of-words, as classic sparse interpretable
embeddings, suffers from poor performance. Recently, Benara et al. (2024)
propose interpretable text embeddings using large language models, which forms
"0/1" embeddings based on responses to a series of questions. These
interpretable text embeddings are typically high-dimensional (larger than
10,000). In this work, we propose Low-dimensional (lower than 500) Dense and
Interpretable text embeddings with Relative representations (LDIR). The
numerical values of its dimensions indicate semantic relatedness to different
anchor texts through farthest point sampling, offering both semantic
representation as well as a certain level of traceability and interpretability.
We validate LDIR on multiple semantic textual similarity, retrieval, and
clustering tasks. Extensive experimental results show that LDIR performs close
to the black-box baseline models and outperforms the interpretable embeddings
baselines with much fewer dimensions. Code is available at
https://github.com/szu-tera/LDIR.

</details>


### [33] [Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli](https://arxiv.org/abs/2505.10356)
*Chunyu Ye,Shaonan Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种多模态统一框架，利用视觉语言模型从大脑活动中重建语言，适用于视觉、听觉和文本输入，性能接近当前最优方法。


<details>
  <summary>Details</summary>
Motivation: 人类思维本质是多模态的，但现有研究多限于单模态输入（如图像或音频），因此需要一种更灵活、通用的方法来解码大脑活动中的语言。

Method: 使用视觉语言模型（VLMs）结合模态特定的专家模块，联合解析多模态信息，从大脑记录中重建连贯语言。

Result: 实验表明，该方法在性能上接近最先进系统，同时具有适应性和可扩展性。

Conclusion: 这项研究推动了更具生态效度和通用性的思维解码技术的发展。

Abstract: Decoding thoughts from brain activity offers valuable insights into human
cognition and enables promising applications in brain-computer interaction.
While prior studies have explored language reconstruction from fMRI data, they
are typically limited to single-modality inputs such as images or audio. In
contrast, human thought is inherently multimodal. To bridge this gap, we
propose a unified and flexible framework for reconstructing coherent language
from brain recordings elicited by diverse input modalities-visual, auditory,
and textual. Our approach leverages visual-language models (VLMs), using
modality-specific experts to jointly interpret information across modalities.
Experiments demonstrate that our method achieves performance comparable to
state-of-the-art systems while remaining adaptable and extensible. This work
advances toward more ecologically valid and generalizable mind decoding.

</details>


### [34] [Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples](https://arxiv.org/abs/2505.10389)
*Benjamin White,Anastasia Shimorina*

Main category: cs.CL

TL;DR: 论文研究了基于大语言模型（LLM）的方面级情感分析系统设计，重点探索了四元组意见提取，并验证了单一多领域模型的性能与单领域模型相当，同时降低操作复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索如何利用LLM实现跨领域和语言的方面级情感分析，解决多领域特定分类体系下的高效意见提取问题。

Method: 通过微调单一模型，结合多领域数据，实现四元组意见（方面类别、情感极性、目标和意见表达）的提取，并分析非抽取式预测的应对策略。

Result: 实验表明，组合多领域模型在性能上媲美单领域专用模型，同时减少了实际部署的操作复杂性。

Conclusion: 结论表明单一多领域模型在结构化预测任务中具有潜力，并总结了开发LLM系统时处理失败模式的实践教训。

Abstract: This paper explores the design of an aspect-based sentiment analysis system
using large language models (LLMs) for real-world use. We focus on quadruple
opinion extraction -- identifying aspect categories, sentiment polarity,
targets, and opinion expressions from text data across different domains and
languages. Using internal datasets, we investigate whether a single fine-tuned
model can effectively handle multiple domain-specific taxonomies
simultaneously. We demonstrate that a combined multi-domain model achieves
performance comparable to specialized single-domain models while reducing
operational complexity. We also share lessons learned for handling
non-extractive predictions and evaluating various failure modes when developing
LLM-based systems for structured prediction tasks.

</details>


### [35] [Rethinking Repetition Problems of LLMs in Code Generation](https://arxiv.org/abs/2505.10402)
*Yihong Dong,Yuchen Liu,Xue Jiang,Zhi Jin,Ge Li*

Main category: cs.CL

TL;DR: 论文提出了一种基于语法的解码方法RPG，通过识别并减少代码生成中的结构重复问题，显著提升了生成代码的质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注内容重复，而忽视了更普遍且具挑战性的结构重复问题。

Method: RPG利用语法规则识别重复问题，并通过降低导致重复的关键词的概率来减少重复。

Result: 实验结果表明，RPG在CodeRepetEval、HumanEval和MBPP基准测试上均优于基线方法。

Conclusion: RPG有效减少了代码生成中的重复问题，提升了生成代码的质量。

Abstract: With the advent of neural language models, the performance of code generation
has been significantly boosted. However, the problem of repetitions during the
generation process continues to linger. Previous work has primarily focused on
content repetition, which is merely a fraction of the broader repetition
problem in code generation. A more prevalent and challenging problem is
structural repetition. In structural repetition, the repeated code appears in
various patterns but possesses a fixed structure, which can be inherently
reflected in grammar. In this paper, we formally define structural repetition
and propose an efficient decoding approach called RPG, which stands for
Repetition Penalization based on Grammar, to alleviate the repetition problems
in code generation for LLMs. Specifically, RPG first leverages grammar rules to
identify repetition problems during code generation, and then strategically
decays the likelihood of critical tokens that contribute to repetitions,
thereby mitigating them in code generation. To facilitate this study, we
construct a new dataset CodeRepetEval to comprehensively evaluate approaches
for mitigating the repetition problems in code generation. Extensive
experimental results demonstrate that RPG substantially outperforms the
best-performing baselines on CodeRepetEval dataset as well as HumanEval and
MBPP benchmarks, effectively reducing repetitions and enhancing the quality of
generated code.

</details>


### [36] [Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation](https://arxiv.org/abs/2505.10409)
*Yue Guo,Jae Ho Sohn,Gondy Leroy,Trevor Cohen*

Main category: cs.CL

TL;DR: 本文通过大规模众包评估发现，尽管LLM生成的简明语言摘要（PLS）在主观评价中与人工写作的难以区分，但后者在理解效果上显著更优。同时，自动化评估指标与人类判断不一致，凸显了对优化理解的评估框架的需求。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLM生成PLS的有效性不明确及现有评估方法的局限性（如依赖自动化分数或小规模主观评分），研究进行了大规模评估，旨在验证LLM生成的PLS在信息理解上的实际效果及其评估指标的可靠性。

Method: 研究通过Amazon Mechanical Turk招募150名参与者，采用主观Likert量表（评估简洁性、信息量、连贯性、忠实度）和客观选择题（测试理解与记忆）对比LLM与人工PLS的质量，并检验10种自动化指标与人类判断的一致性。

Result: 结果显示，尽管LLM生成的PLS在主观评分上与人工写作无异，但人工写作的PLS在理解测试中表现显著更优。自动化指标与人类评估结果脱节，无法可靠反映PLS的实际理解效果。

Conclusion: 研究强调需开发超越表层质量的评估框架，并优化生成方法以直接提升非专业人士的理解能力，同时质疑了当前自动化指标在PLS评估中的适用性。

Abstract: Plain language summaries (PLSs) are essential for facilitating effective
communication between clinicians and patients by making complex medical
information easier for laypeople to understand and act upon. Large language
models (LLMs) have recently shown promise in automating PLS generation, but
their effectiveness in supporting health information comprehension remains
unclear. Prior evaluations have generally relied on automated scores that do
not measure understandability directly, or subjective Likert-scale ratings from
convenience samples with limited generalizability. To address these gaps, we
conducted a large-scale crowdsourced evaluation of LLM-generated PLSs using
Amazon Mechanical Turk with 150 participants. We assessed PLS quality through
subjective Likert-scale ratings focusing on simplicity, informativeness,
coherence, and faithfulness; and objective multiple-choice comprehension and
recall measures of reader understanding. Additionally, we examined the
alignment between 10 automated evaluation metrics and human judgments. Our
findings indicate that while LLMs can generate PLSs that appear
indistinguishable from human-written ones in subjective evaluations,
human-written PLSs lead to significantly better comprehension. Furthermore,
automated evaluation metrics fail to reflect human judgment, calling into
question their suitability for evaluating PLSs. This is the first study to
systematically evaluate LLM-generated PLSs based on both reader preferences and
comprehension outcomes. Our findings highlight the need for evaluation
frameworks that move beyond surface-level quality and for generation methods
that explicitly optimize for layperson comprehension.

</details>


### [37] [Hierarchical Document Refinement for Long-context Retrieval-augmented Generation](https://arxiv.org/abs/2505.10413)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yongkang Wu,Zhonghua Li,Qi Ye,Zhicheng Dou*

Main category: cs.CL

TL;DR: LongRefiner是一个高效的即插即用优化器，针对长文本RAG应用中的冗余和噪声问题，通过双级查询分析、分层文档结构和自适应优化，显著降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 现实中的RAG应用常面临长上下文输入场景，冗余信息和噪声导致推理成本高且性能下降，因此需要一种高效解决方案。

Method: 采用双级查询分析、分层文档结构和基于多任务学习的自适应优化，通过单一基础模型实现。

Result: 在七个QA数据集上验证，LongRefiner在性能接近最佳基线的同时，计算成本和延迟降低10倍。

Conclusion: LongRefiner证明了其在长文本RAG应用中的可扩展性、高效性和有效性，为实际应用提供了实用见解。

Abstract: Real-world RAG applications often encounter long-context input scenarios,
where redundant information and noise results in higher inference costs and
reduced performance. To address these challenges, we propose LongRefiner, an
efficient plug-and-play refiner that leverages the inherent structural
characteristics of long documents. LongRefiner employs dual-level query
analysis, hierarchical document structuring, and adaptive refinement through
multi-task learning on a single foundation model. Experiments on seven QA
datasets demonstrate that LongRefiner achieves competitive performance in
various scenarios while using 10x fewer computational costs and latency
compared to the best baseline. Further analysis validates that LongRefiner is
scalable, efficient, and effective, providing practical insights for real-world
long-text RAG applications. Our code is available at
https://github.com/ignorejjj/LongRefiner.

</details>


### [38] [Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/abs/2505.10446)
*Zemin Huang,Zhiyang Chen,Zijun Wang,Tiancheng Li,Guo-Jun Qi*

Main category: cs.CL

TL;DR: DCoLT是一种用于扩散语言模型的新推理框架，通过反向扩散过程中的中间步骤实现双向非线性推理，利用强化学习优化最终答案的正确性，显著提升了推理准确性。


<details>
  <summary>Details</summary>
Motivation: 传统链式思维方法（CoT）采用因果线性推理，限制了推理灵活性，DCoLT旨在通过扩散模型的中间步骤实现更自由的非线性推理。

Method: 结合连续时间离散扩散模型（SEDD）和离散时间掩码扩散模型（LLaDA），定义基于Plackett-Luce模型的排名策略，通过强化学习优化推理轨迹。

Result: 在数学和代码生成任务中，DCoLT增强的扩散语言模型显著优于其他方法，LLaDA在多个数据集上推理准确率提升最高达19.5%。

Conclusion: DCoLT通过非线性和无语法约束的中间推理步骤，有效提升了扩散语言模型的推理能力，为复杂任务提供了新的解决方案。

Abstract: We introduce the \emph{Diffusion Chain of Lateral Thought (DCoLT)}, a
reasoning framework for diffusion language models. DCoLT treats each
intermediate step in the reverse diffusion process as a latent "thinking"
action and optimizes the entire reasoning trajectory to maximize the reward on
the correctness of the final answer with outcome-based Reinforcement Learning
(RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal,
linear thinking process, DCoLT allows bidirectional, non-linear reasoning with
no strict rule on grammatical correctness amid its intermediate steps of
thought. We implement DCoLT on two representative Diffusion Language Models
(DLMs). First, we choose SEDD as a representative continuous-time discrete
diffusion model, where its concrete score derives a probabilistic policy to
maximize the RL reward over the entire sequence of intermediate diffusion
steps. We further consider the discrete-time masked diffusion language model --
LLaDA, and find that the order to predict and unmask tokens plays an essential
role to optimize its RL action resulting from the ranking-based Unmasking
Policy Module (UPM) defined by the Plackett-Luce model. Experiments on both
math and code generation tasks show that using only public data and 16 H800
GPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even
both. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%,
+5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.

</details>


### [39] [CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning](https://arxiv.org/abs/2505.10493)
*Shaohan Wang,Licheng Zhang,Zheren Fu,Zhendong Mao*

Main category: cs.CL

TL;DR: CL-RAG框架通过多层次课程学习优化RAG系统的检索器和生成器，提升整体性能和泛化能力，在四项QA数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法直接使用top-k检索文档，但文档有效性因查询而异，影响训练效果。受人类认知学习的启发，课程学习通过从易到难的样本训练模型，可提升泛化能力。

Method: 提出CL-RAG框架：1)通过样本演化构建多难度训练数据，2)分阶段训练检索器和生成器。

Result: 在四个开放域QA数据集上均有效，性能比先进方法提升2%至4%。

Conclusion: CL-RAG通过课程学习显著提升RAG系统性能，验证了方法在多数据集上的鲁棒性。

Abstract: Retrieval-Augmented Generation (RAG) is an effective method to enhance the
capabilities of large language models (LLMs). Existing methods focus on
optimizing the retriever or generator in the RAG system by directly utilizing
the top-k retrieved documents. However, the documents effectiveness are various
significantly across user queries, i.e. some documents provide valuable
knowledge while others totally lack critical information. It hinders the
retriever and generator's adaptation during training. Inspired by human
cognitive learning, curriculum learning trains models using samples progressing
from easy to difficult, thus enhancing their generalization ability, and we
integrate this effective paradigm to the training of the RAG system. In this
paper, we propose a multi-stage Curriculum Learning based RAG system training
framework, named CL-RAG. We first construct training data with multiple
difficulty levels for the retriever and generator separately through sample
evolution. Then, we train the model in stages based on the curriculum learning
approach, thereby optimizing the overall performance and generalization of the
RAG system more effectively. Our CL-RAG framework demonstrates consistent
effectiveness across four open-domain QA datasets, achieving performance gains
of 2% to 4% over multiple advanced methods.

</details>


### [40] [Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective](https://arxiv.org/abs/2505.10494)
*Yutao Mou,Xiao Deng,Yuxiao Luo,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: 该论文提出了CoV-Eval多任务基准和VC-Judge评估模型，全面评估了20种LLM的代码安全性，发现其虽然在漏洞识别上表现良好，但在生成安全代码和修复漏洞方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 当前代码安全基准仅关注单一任务（如代码生成或补全），缺乏对安全代码生成、漏洞修复和识别等多维度的综合评估。

Method: 提出多任务基准CoV-Eval（涵盖代码补全、漏洞修复、检测和分类）和改进的评估模型VC-Judge，用于更高效可靠地审查LLM生成的代码。

Result: 评估20种LLM后发现，虽能较好识别漏洞代码，但生成安全代码和修复特定漏洞类型的能力较弱。

Conclusion: 实验揭示了LLM代码安全领域的主要挑战和优化方向，为未来研究提供了参考。

Abstract: Code security and usability are both essential for various coding assistant
applications driven by large language models (LLMs). Current code security
benchmarks focus solely on single evaluation task and paradigm, such as code
completion and generation, lacking comprehensive assessment across dimensions
like secure code generation, vulnerability repair and discrimination. In this
paper, we first propose CoV-Eval, a multi-task benchmark covering various tasks
such as code completion, vulnerability repair, vulnerability detection and
classification, for comprehensive evaluation of LLM code security. Besides, we
developed VC-Judge, an improved judgment model that aligns closely with human
experts and can review LLM-generated programs for vulnerabilities in a more
efficient and reliable way. We conduct a comprehensive evaluation of 20
proprietary and open-source LLMs. Overall, while most LLMs identify vulnerable
codes well, they still tend to generate insecure codes and struggle with
recognizing specific vulnerability types and performing repairs. Extensive
experiments and qualitative analyses reveal key challenges and optimization
directions, offering insights for future research in LLM code security.

</details>


### [41] [The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks](https://arxiv.org/abs/2505.10507)
*Benedikt Ebing,Goran Glavaš*

Main category: cs.CL

TL;DR: 本文研究了在跨语言迁移（XLT）中基于翻译的方法，重点探讨了标签投影在词对齐器（WAs）中的应用及其设计决策的影响。研究发现通过优化设计选择，WAs性能可与标记方法媲美，并提出一种新集成策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何优化词对齐器在跨语言迁移中的标签投影效果，解决现有方法中设计决策未被系统研究的问题，以提升XLT在词分类任务中的表现。

Method: 通过实验系统研究了标签投影中的多令牌映射算法、噪声过滤策略和预分词方法，并提出了集成训练和测试预测的新策略。

Result: 优化设计选择后，WAs性能与标记方法相当；新集成策略显著提升性能并降低对设计选择的敏感性。

Conclusion: 优化后的词对齐器标签投影方法在XLT中表现优异，新集成策略提供了更鲁棒的解决方案。

Abstract: Translation-based strategies for cross-lingual transfer XLT such as
translate-train -- training on noisy target language data translated from the
source language -- and translate-test -- evaluating on noisy source language
data translated from the target language -- are competitive XLT baselines. In
XLT for token classification tasks, however, these strategies include label
projection, the challenging step of mapping the labels from each token in the
original sentence to its counterpart(s) in the translation. Although word
aligners (WAs) are commonly used for label projection, the low-level design
decisions for applying them to translation-based XLT have not been
systematically investigated. Moreover, recent marker-based methods, which
project labeled spans by inserting tags around them before (or after)
translation, claim to outperform WAs in label projection for XLT. In this work,
we revisit WAs for label projection, systematically investigating the effects
of low-level design decisions on token-level XLT: (i) the algorithm for
projecting labels between (multi-)token spans, (ii) filtering strategies to
reduce the number of noisily mapped labels, and (iii) the pre-tokenization of
the translated sentences. We find that all of these substantially impact
translation-based XLT performance and show that, with optimized choices, XLT
with WA offers performance at least comparable to that of marker-based methods.
We then introduce a new projection strategy that ensembles translate-train and
translate-test predictions and demonstrate that it substantially outperforms
the marker-based projection. Crucially, we show that our proposed ensembling
also reduces sensitivity to low-level WA design choices, resulting in more
robust XLT for token classification tasks.

</details>


### [42] [Multi-Token Prediction Needs Registers](https://arxiv.org/abs/2505.10518)
*Anastasios Gerontopoulos,Spyros Gidaris,Nikos Komodakis*

Main category: cs.CL

TL;DR: MuToR是一种简单有效的多标记预测方法，通过插入可学习的寄存器标记来预测未来目标，兼容现有预训练模型且支持可扩展的预测范围。


<details>
  <summary>Details</summary>
Motivation: 多标记预测在语言模型预训练中表现优异，但其优势在微调等其他场景中未能一致体现，因此作者提出MuToR以解决这一问题。

Method: MuToR在输入序列中插入可学习的寄存器标记，每个标记负责预测未来目标，无需架构改动且参数增量极小。

Result: MuToR在语言和视觉领域的生成任务中展现出卓越效果，适用于监督微调、参数高效微调（PEFT）及预训练场景。

Conclusion: MuToR是一种通用且高效的多标记预测方法，显著提升了模型在多样化任务中的表现。

Abstract: Multi-token prediction has emerged as a promising objective for improving
language model pretraining, but its benefits have not consistently generalized
to other settings such as fine-tuning. In this paper, we propose MuToR, a
simple and effective approach to multi-token prediction that interleaves
learnable register tokens into the input sequence, each tasked with predicting
future targets. Compared to existing methods, MuToR offers several key
advantages: it introduces only a negligible number of additional parameters,
requires no architectural changes--ensuring compatibility with off-the-shelf
pretrained language models--and remains aligned with the next-token pretraining
objective, making it especially well-suited for supervised fine-tuning.
Moreover, it naturally supports scalable prediction horizons. We demonstrate
the effectiveness and versatility of MuToR across a range of use cases,
including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and
pretraining, on challenging generative tasks in both language and vision
domains. Our code will be available at: https://github.com/nasosger/MuToR.

</details>


### [43] [WorldPM: Scaling Human Preference Modeling](https://arxiv.org/abs/2505.10527)
*Binghai Wang,Runji Lin,Keming Lu,Le Yu,Zhenru Zhang,Fei Huang,Chujie Zheng,Kai Dang,Yang Fan,Xingzhang Ren,An Yang,Binyuan Hui,Dayiheng Liu,Tao Gui,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang,Bowen Yu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: 该论文研究了偏好建模中的扩展规律，提出了World Preference Modeling（WorldPM），通过大规模数据和模型训练验证了其扩展潜力。实验表明，WorldPM在不同评估指标上表现不同，但能显著提升偏好微调的效果，并在多个基准测试中表现出广泛的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 受语言模型中扩展规律的启发，作者探索了偏好建模中是否存在类似的规律，并提出WorldPM作为统一的人类偏好表示方法，以验证其扩展潜力。

Method: 作者从公共论坛收集多样化的偏好数据，并使用1.5B至72B参数的模型进行了15M规模数据的训练。通过对抗性、客观性和主观性三类评估指标进行分析。

Result: 实验结果显示：(1)对抗性指标随数据和模型规模提升；(2)客观性指标在大模型中表现出涌现行为；(3)主观性指标未显示扩展趋势。WorldPM在7个基准测试中显著提升泛化性能，性能提升超过5%。

Conclusion: WorldPM作为偏好微调的基础表现优越，能显著提升内部和公开评估集的表现，验证了其扩展潜力和实用性。

Abstract: Motivated by scaling laws in language modeling that demonstrate how test loss
scales as a power law with model and dataset sizes, we find that similar laws
exist in preference modeling. We propose World Preference Modeling$ (WorldPM)
to emphasize this scaling potential, where World Preference embodies a unified
representation of human preferences. In this paper, we collect preference data
from public forums covering diverse user communities, and conduct extensive
training using 15M-scale data across models ranging from 1.5B to 72B
parameters. We observe distinct patterns across different evaluation metrics:
(1) Adversarial metrics (ability to identify deceptive features) consistently
scale up with increased training data and base model size; (2) Objective
metrics (objective knowledge with well-defined answers) show emergent behavior
in larger language models, highlighting WorldPM's scalability potential; (3)
Subjective metrics (subjective preferences from a limited number of humans or
AI) do not demonstrate scaling trends. Further experiments validate the
effectiveness of WorldPM as a foundation for preference fine-tuning. Through
evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly
improves the generalization performance across human preference datasets of
varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%
on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we
observe significant improvements on both in-house and public evaluation sets,
with notable gains of 4% to 8% in our in-house evaluations.

</details>


### [44] [Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models](https://arxiv.org/abs/2505.10554)
*Zhiyuan Hu,Yibo Wang,Hanze Dong,Yuhui Xu,Amrita Saha,Caiming Xiong,Bryan Hooi,Junnan Li*

Main category: cs.CL

TL;DR: 论文提出了一种方法，通过明确对齐模型的元能力（演绎、归纳和溯因），提升大型推理模型的可扩展性和可靠性，相比基线性能提升超过10%。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽具备长链推理能力，但其突发性行为（如自我修正、回溯）的不可预测性限制了其可靠性和扩展性。

Method: 采用三阶段流程：个体对齐、参数空间合并和领域特定的强化学习，结合自动生成的自验证任务来对齐元能力。

Result: 方法在数学、编程和科学基准测试中，相比基线性能提升超过10%，领域特定强化学习进一步带来2%的平均增益。

Conclusion: 明确的元能力对齐为推理提供了可扩展且可靠的基础，提升了模型的性能和一致性。

Abstract: Large reasoning models (LRMs) already possess a latent capacity for long
chain-of-thought reasoning. Prior work has shown that outcome-based
reinforcement learning (RL) can incidentally elicit advanced reasoning
behaviors such as self-correction, backtracking, and verification phenomena
often referred to as the model's "aha moment". However, the timing and
consistency of these emergent behaviors remain unpredictable and
uncontrollable, limiting the scalability and reliability of LRMs' reasoning
capabilities. To address these limitations, we move beyond reliance on prompts
and coincidental "aha moments". Instead, we explicitly align models with three
meta-abilities: deduction, induction, and abduction, using automatically
generated, self-verifiable tasks. Our three stage-pipeline individual
alignment, parameter-space merging, and domain-specific reinforcement learning,
boosting performance by over 10\% relative to instruction-tuned baselines.
Furthermore, domain-specific RL from the aligned checkpoint yields an
additional 2\% average gain in the performance ceiling across math, coding, and
science benchmarks, demonstrating that explicit meta-ability alignment offers a
scalable and dependable foundation for reasoning. Code is available at:
https://github.com/zhiyuanhubj/Meta-Ability-Alignment

</details>


### [45] [Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI](https://arxiv.org/abs/2505.10472)
*Agnik Saha,Victoria Churchill,Anny D. Rodriguez,Ugur Kursuncu,Muhammed Y. Idris*

Main category: cs.CL

TL;DR: 这篇论文评估了通用和医疗领域大语言模型（LLMs）在生成乳腺癌和宫颈癌相关信息时的表现，发现通用模型在语言质量和情感表达上更优，而医疗模型在沟通可及性上更好，但后者潜在风险和偏见更高。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决公共对癌症预防、筛查和治疗理解的不足，以及评估LLMs在生成准确、安全、易懂的癌症信息方面的能力。

Method: 采用混合方法评估框架，结合定量指标、定性专家评分和统计分析方法（Welch's ANOVA、Games-Howell和Hedges' g），对比五种通用和三种医疗LLMs的表现。

Result: 结果显示，通用LLMs在语言质量和情感表达上表现更好，医疗LLMs在沟通可及性上更优，但其潜在危害、毒性和偏见较高，影响了安全性和可信度。

Conclusion: 结论强调了在健康沟通中需平衡领域专业知识与安全性，并提示未来需针对减少危害和偏见、提升安全性和情感表达进行模型设计。

Abstract: Effective communication about breast and cervical cancers remains a
persistent health challenge, with significant gaps in public understanding of
cancer prevention, screening, and treatment, potentially leading to delayed
diagnoses and inadequate treatments. This study evaluates the capabilities and
limitations of Large Language Models (LLMs) in generating accurate, safe, and
accessible cancer-related information to support patient understanding. We
evaluated five general-purpose and three medical LLMs using a mixed-methods
evaluation framework across linguistic quality, safety and trustworthiness, and
communication accessibility and affectiveness. Our approach utilized
quantitative metrics, qualitative expert ratings, and statistical analysis
using Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that
general-purpose LLMs produced outputs of higher linguistic quality and
affectiveness, while medical LLMs demonstrate greater communication
accessibility. However, medical LLMs tend to exhibit higher levels of potential
harm, toxicity, and bias, reducing their performance in safety and
trustworthiness. Our findings indicate a duality between domain-specific
knowledge and safety in health communications. The results highlight the need
for intentional model design with targeted improvements, particularly in
mitigating harm and bias, and improving safety and affectiveness. This study
provides a comprehensive evaluation of LLMs for cancer communication, offering
critical insights for improving AI-generated health content and informing
future development of accurate, safe, and accessible digital health tools.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models](https://arxiv.org/abs/2505.09659)
*Long Chen,Xiaotian Song,Yanan Sun*

Main category: cs.LG

TL;DR: LAS是一种无损的ANN-SNN转换方法，通过引入新型神经元解决了激活异常和非线性操作问题，实现全脉冲驱动的LLMs，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的ANN-to-SNN转换方法在处理极端激活异常和与非线操作兼容性上存在困难，因此需要一种无损的转换方法以实现高效的脉冲驱动LLMs。

Method: LAS引入了两种新型神经元来解决ANN-based LLMs的激活异常和非线性操作问题，并定制了脉冲等效的Transformer组件，确保全脉冲转换无性能损失。

Result: 实验结果显示，LAS在六个语言模型和两个视觉语言模型上实现了无损转换，甚至在OPT-66B模型上WSC任务准确率提升了2%。

Conclusion: LAS成功地解决了现有转换方法的局限，实现了高效的全脉冲LLMs转换，并通过实验验证了其优越性。

Abstract: Spiking Large Language Models (LLMs) have emerged as an energy-efficient
alternative to conventional LLMs through their event-driven computation. To
effectively obtain spiking LLMs, researchers develop different ANN-to-SNN
conversion methods by leveraging pre-trained ANN parameters while inheriting
the energy efficiency of SNN. However, existing conversion methods struggle
with extreme activation outliers and incompatible nonlinear operations of
ANN-based LLMs. To address this, we propose a loss-less ANN-SNN conversion for
fully spike-driven LLMs, termed LAS. Specifically, LAS introduces two novel
neurons to convert the activation outlier and nonlinear operation of ANN-based
LLMs. Moreover, LAS tailors the spike-equivalent Transformer components for
spiking LLMs, which can ensure full spiking conversion without any loss of
performance. Experimental results on six language models and two
vision-language models demonstrate that LAS achieves loss-less conversion.
Notably, on OPT-66B, LAS even improves the accuracy of 2\% on the WSC task. In
addition, the parameter and ablation studies further verify the effectiveness
of LAS. The source code is available at https://github.com/lc783/LAS

</details>


### [47] [Analog Foundation Models](https://arxiv.org/abs/2505.09663)
*Julian Büchel,Iason Chalas,Giovanni Acampa,An Chen,Omobayode Fagbohungbe,Sidney Tsai,Kaoutar El Maghraoui,Manuel Le Gallo,Abbas Rahimi,Abu Sebastian*

Main category: cs.LG

TL;DR: 该论文提出了一种通用且可扩展的方法，用于将大型语言模型（LLMs）适配到噪声大、低精度模拟硬件上，使其在模拟环境下仍能保持高精度性能。


<details>
  <summary>Details</summary>
Motivation: 模拟内存计算（AIMC）能提升神经网络推理的速度和能效，但其噪声和量化限制导致现有LLMs无法在4比特级别表现良好。研究旨在填补这一空白，使LLMs能在模拟硬件上高效运行。

Method: 提出了一种通用且可扩展的训练方法，通过适应模拟硬件的噪声和量化约束，确保模型在低精度环境下保持性能。

Result: 实验表明，该方法使Phi-3和Llama-3等模型在模拟硬件上达到4比特权重、8比特激活的基准性能，并能量化适配低精度数字硬件。

Conclusion: 该方法成功桥接了高性能LLMs与高效模拟硬件之间的鸿沟，为能效优化的基础模型提供了可行路径。

Abstract: Analog in-memory computing (AIMC) is a promising compute paradigm to improve
speed and power efficiency of neural network inference beyond the limits of
conventional von Neumann-based architectures. However, AIMC introduces
fundamental challenges such as noisy computations and strict constraints on
input and output quantization. Because of these constraints and imprecisions,
off-the-shelf LLMs are not able to achieve 4-bit-level performance when
deployed on AIMC-based hardware. While researchers previously investigated
recovering this accuracy gap on small, mostly vision-based models, a generic
method applicable to LLMs pre-trained on trillions of tokens does not yet
exist. In this work, we introduce a general and scalable method to robustly
adapt LLMs for execution on noisy, low-precision analog hardware. Our approach
enables state-of-the-art models $\unicode{x2013}$ including
Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\unicode{x2013}$ to retain
performance comparable to 4-bit weight, 8-bit activation baselines, despite the
presence of analog noise and quantization constraints. Additionally, we show
that as a byproduct of our training methodology, analog foundation models can
be quantized for inference on low-precision digital hardware. Finally, we show
that our models also benefit from test-time compute scaling, showing better
scaling behavior than models trained with 4-bit weight and 8-bit static input
quantization. Our work bridges the gap between high-capacity LLMs and efficient
analog hardware, offering a path toward energy-efficient foundation models.
Code is available at https://github.com/IBM/analog-foundation-models .

</details>


### [48] [Enabling Group Fairness in Graph Unlearning via Bi-level Debiasing](https://arxiv.org/abs/2505.09702)
*Yezi Liu,Prathyush Poduval,Wenjun Huang,Yang Ni,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: 论文研究了图遗忘方法在删除用户数据时可能引入偏见的问题，并提出了一种公平图遗忘方法（FGU），通过分片模型和双层去偏过程，在保证隐私的同时提升了公平性。


<details>
  <summary>Details</summary>
Motivation: 由于图遗忘方法在删除用户数据时可能改变预测分布并引入偏见，研究者希望开发一种既能保护隐私又能确保公平性的方法。

Method: FGU通过分片训练子图模型、在子图中删除数据并重新训练，同时引入双层去偏（分片级公平性正则化和全局级模型对齐）来保证公平性。

Result: 实验表明FGU在隐私保护、准确性和公平性方面表现优异，且对不同遗忘请求具有鲁棒性。

Conclusion: FGU为解决图遗忘中公平性问题提供了有效方案，兼具隐私保护和模型效用。

Abstract: Graph unlearning is a crucial approach for protecting user privacy by erasing
the influence of user data on trained graph models. Recent developments in
graph unlearning methods have primarily focused on maintaining model prediction
performance while removing user information. However, we have observed that
when user information is deleted from the model, the prediction distribution
across different sensitive groups often changes. Furthermore, graph models are
shown to be prone to amplifying biases, making the study of fairness in graph
unlearning particularly important. This raises the question: Does graph
unlearning actually introduce bias? Our findings indicate that the predictions
of post-unlearning models become highly correlated with sensitive attributes,
confirming the introduction of bias in the graph unlearning process. To address
this issue, we propose a fair graph unlearning method, FGU. To guarantee
privacy, FGU trains shard models on partitioned subgraphs, unlearns the
requested data from the corresponding subgraphs, and retrains the shard models
on the modified subgraphs. To ensure fairness, FGU employs a bi-level debiasing
process: it first enables shard-level fairness by incorporating a fairness
regularizer in the shard model retraining, and then achieves global-level
fairness by aligning all shard models to minimize global disparity. Our
experiments demonstrate that FGU achieves superior fairness while maintaining
privacy and accuracy. Additionally, FGU is robust to diverse unlearning
requests, ensuring fairness and utility performance across various data
distributions.

</details>


### [49] [Energy-Efficient Federated Learning for AIoT using Clustering Methods](https://arxiv.org/abs/2505.09704)
*Roberto Pereira,Fernanda Famá,Charalampos Kalalas,Paolo Dini*

Main category: cs.LG

TL;DR: 该论文关注了联邦学习（FL）在AIoT场景中的能源消耗问题，提出了基于聚类的设备选择方法以优化能源效率，同时保持高收敛率。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注模型性能和通信效率，而忽略了FL在AIoT中的能源影响。本文旨在填补这一空白，优化FL的能源消耗。

Method: 通过聚类方法将具有相似标签分布的AIoT设备分组，以减少异构性并提升收敛速度。

Result: 实验表明，所提方法在保持低能耗的同时实现了较高的收敛率，优于现有方法。

Conclusion: 聚类策略能有效平衡FL的能源效率和模型性能，适用于实际分布式学习场景。

Abstract: While substantial research has been devoted to optimizing model performance,
convergence rates, and communication efficiency, the energy implications of
federated learning (FL) within Artificial Intelligence of Things (AIoT)
scenarios are often overlooked in the existing literature. This study examines
the energy consumed during the FL process, focusing on three main
energy-intensive processes: pre-processing, communication, and local learning,
all contributing to the overall energy footprint. We rely on the observation
that device/client selection is crucial for speeding up the convergence of
model training in a distributed AIoT setting and propose two
clustering-informed methods. These clustering solutions are designed to group
AIoT devices with similar label distributions, resulting in clusters composed
of nearly heterogeneous devices. Hence, our methods alleviate the heterogeneity
often encountered in real-world distributed learning applications. Throughout
extensive numerical experimentation, we demonstrate that our clustering
strategies typically achieve high convergence rates while maintaining low
energy consumption when compared to other recent approaches available in the
literature.

</details>


### [50] [Training Deep Morphological Neural Networks as Universal Approximators](https://arxiv.org/abs/2505.09710)
*Konstantinos Fotopoulos,Petros Maragos*

Main category: cs.LG

TL;DR: 论文研究了深度形态神经网络（DMNNs），提出了几种新架构，并在参数约束下成功训练，证明其比线性网络更易剪枝，还提出混合架构加速收敛。


<details>
  <summary>Details</summary>
Motivation: 研究DMNNs的非线性特性及其层间激活的重要性，探索在特定参数约束下训练DMNNs的可行性。

Method: 提出多种DMNNs新架构，分别限制参数或可学习参数为形态学操作，并设计混合线性与形态学层的网络。

Result: 成功训练约束下的DMNNs，显示其更易剪枝；混合架构显著加速大批量梯度下降的收敛。

Conclusion: DMNNs在约束下可训练且具优势，但泛化能力有限；混合架构为训练效率提供新方向。

Abstract: We investigate deep morphological neural networks (DMNNs). We demonstrate
that despite their inherent non-linearity, activations between layers are
essential for DMNNs. We then propose several new architectures for DMNNs, each
with a different constraint on their parameters. For the first (resp. second)
architecture, we work under the constraint that the majority of parameters
(resp. learnable parameters) should be part of morphological operations. We
empirically show that our proposed networks can be successfully trained, and
are more prunable than linear networks. To the best of our knowledge, we are
the first to successfully train DMNNs under such constraints, although the
generalization capabilities of our networks remain limited. Finally, we propose
a hybrid network architecture combining linear and morphological layers,
showing empirically that the inclusion of morphological layers significantly
accelerates the convergence of gradient descent with large batches.

</details>


### [51] [Out-of-distribution generalisation is hard: evidence from ARC-like tasks](https://arxiv.org/abs/2505.09716)
*George Dimitriadis. Spyridon Samothrakis*

Main category: cs.LG

TL;DR: 论文探讨了如何通过组合实现分布外（OOD）泛化，提出仅测试OOD性能不足，还需验证特征是否具有组合性，并通过实验验证了常见神经网络在OOD任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究目标是实现类似人类的分布外泛化能力，强调仅依赖OOD测试无法确认算法是否真正学习了组合结构。

Method: 通过设计两个明确OOD指标的任务，测试了MLP、CNN和Transformer的OOD泛化能力，并开发了两种新架构以验证偏置对OOD性能的影响。

Result: 常见神经网络在OOD任务中表现不佳，即使新架构在OOD场景中表现优异，仍可能无法学习正确的组合特征。

Conclusion: 仅优化OOD性能不足以确保算法学习了正确的组合结构，需进一步验证特征的组合性。

Abstract: Out-of-distribution (OOD) generalisation is considered a hallmark of human
and animal intelligence. To achieve OOD through composition, a system must
discover the environment-invariant properties of experienced input-output
mappings and transfer them to novel inputs. This can be realised if an
intelligent system can identify appropriate, task-invariant, and composable
input features, as well as the composition methods, thus allowing it to act
based not on the interpolation between learnt data points but on the
task-invariant composition of those features. We propose that in order to
confirm that an algorithm does indeed learn compositional structures from data,
it is not enough to just test on an OOD setup, but one also needs to confirm
that the features identified are indeed compositional. We showcase this by
exploring two tasks with clearly defined OOD metrics that are not OOD solvable
by three commonly used neural networks: a Multi-Layer Perceptron (MLP), a
Convolutional Neural Network (CNN), and a Transformer. In addition, we develop
two novel network architectures imbued with biases that allow them to be
successful in OOD scenarios. We show that even with correct biases and almost
perfect OOD performance, an algorithm can still fail to learn the correct
features for compositional generalisation.

</details>


### [52] [Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data](https://arxiv.org/abs/2505.09733)
*Alpaslan Gokcen,Ali Boyaci*

Main category: cs.LG

TL;DR: 该论文提出了一种针对联邦学习中数据质量问题的解决方案，包括噪声标签、类别缺失和不平衡分布。通过自适应噪声清理、基于条件GAN的合成数据生成和鲁棒模型训练，显著提升了模型性能（如Macro-F1分数）。实验在MNIST和Fashion-MNIST数据集上验证了方法的有效性，兼顾计算可行性和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在分布式数据训练中能保护隐私，但数据质量问题（如噪声标签、类别不平衡和缺失）严重限制了其效果。作者旨在提出一种系统性方法，解决这些问题以提升FL的实用性和性能。

Method: 1. 自适应噪声清理：识别并修正噪声标签；2. 协作式条件GAN生成合成数据：解决类别不平衡和缺失问题；3. 鲁棒联邦训练：通过改进的聚合策略确保模型在噪声环境下的稳定性。

Result: 在MNIST和Fashion-MNIST数据集上的实验表明，Macro-F1分数显著提升，尤其在噪声和类别不平衡条件下。方法在保持隐私的同时，平衡了计算开销和性能。

Conclusion: 该框架有效应对了联邦学习中的数据质量问题，提供了兼具鲁棒性、可扩展性和隐私性的解决方案，适用于资源受限的边缘设备及多样化现实场景。

Abstract: Federated learning (FL) presents an effective solution for collaborative
model training while maintaining data privacy across decentralized client
datasets. However, data quality issues such as noisy labels, missing classes,
and imbalanced distributions significantly challenge its effectiveness. This
study proposes a federated learning methodology that systematically addresses
data quality issues, including noise, class imbalance, and missing labels. The
proposed approach systematically enhances data integrity through adaptive noise
cleaning, collaborative conditional GAN-based synthetic data generation, and
robust federated model training. Experimental evaluations conducted on
benchmark datasets (MNIST and Fashion-MNIST) demonstrate significant
improvements in federated model performance, particularly macro-F1 Score, under
varying noise and class imbalance conditions. Additionally, the proposed
framework carefully balances computational feasibility and substantial
performance gains, ensuring practicality for resource constrained edge devices
while rigorously maintaining data privacy. Our results indicate that this
method effectively mitigates common data quality challenges, providing a
robust, scalable, and privacy compliant solution suitable for diverse
real-world federated learning scenarios.

</details>


### [53] [A Generative Neural Annealer for Black-Box Combinatorial Optimization](https://arxiv.org/abs/2505.09742)
*Yuan-Hang Zhang,Massimiliano Di Ventra*

Main category: cs.LG

TL;DR: 提出了一种基于生成模型的端到端黑盒组合优化求解器，强调在NP问题上样本效率与解质量的平衡。


<details>
  <summary>Details</summary>
Motivation: 受退火算法启发，解决黑盒优化中样本效率低和解质量难以保证的问题，特别是针对高成本查询和复杂隐式变量交互的场景。

Method: 将黑盒目标视为能量函数，训练神经网络建模Boltzmann分布，通过温度调节分布形态（从均勻分布到集中于全局最优），学习能量景观结构。

Result: 在有限/无限查询预算下，相比现有黑盒优化器，表现出竞争力。

Conclusion: 该方法通过温度依赖分布提升样本效率并揭示隐式变量交互，为黑盒优化提供了新思路。

Abstract: We propose a generative, end-to-end solver for black-box combinatorial
optimization that emphasizes both sample efficiency and solution quality on NP
problems. Drawing inspiration from annealing-based algorithms, we treat the
black-box objective as an energy function and train a neural network to model
the associated Boltzmann distribution. By conditioning on temperature, the
network captures a continuum of distributions--from near-uniform at high
temperatures to sharply peaked around global optima at low
temperatures--thereby learning the structure of the energy landscape and
facilitating global optimization. When queries are expensive, the
temperature-dependent distributions naturally enable data augmentation and
improve sample efficiency. When queries are cheap but the problem remains hard,
the model learns implicit variable interactions, effectively "opening" the
black box. We validate our approach on challenging combinatorial tasks under
both limited and unlimited query budgets, showing competitive performance
against state-of-the-art black-box optimizers.

</details>


### [54] [Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration](https://arxiv.org/abs/2505.09756)
*Zhaoyang Shi*

Main category: cs.LG

TL;DR: 本文提出了一种新的多智能体强化学习框架，基于动态网络中的潜在社区结构和混合成员关系，支持灵活的协调模式、迁移学习和主动学习，并提供理论收敛保证。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习（MARL）基于固定或邻居交互图，难以捕捉复杂的协作模式。本文旨在通过社区结构和混合成员关系，提供更灵活、可扩展的协作框架。

Method: 提出社区化框架：每个智能体可属于多个重叠社区，共享策略和价值函数；设计基于社区的Actor-Critic算法，支持迁移学习（通过成员估计适应新任务）和主动学习（优先探索不确定社区）。

Result: 理论上证明了在线性函数近似下Actor和Critic更新的收敛性；首次将社区结构、可迁移性和主动学习整合到MARL中。

Conclusion: 该框架通过社区结构和混合成员关系提升了协作灵活性，支持迁移与主动学习，填补了理论保证的空白。

Abstract: We propose a new framework for multi-agent reinforcement learning (MARL),
where the agents cooperate in a time-evolving network with latent community
structures and mixed memberships. Unlike traditional neighbor-based or fixed
interaction graphs, our community-based framework captures flexible and
abstract coordination patterns by allowing each agent to belong to multiple
overlapping communities. Each community maintains shared policy and value
functions, which are aggregated by individual agents according to personalized
membership weights. We also design actor-critic algorithms that exploit this
structure: agents inherit community-level estimates for policy updates and
value learning, enabling structured information sharing without requiring
access to other agents' policies. Importantly, our approach supports both
transfer learning by adapting to new agents or tasks via membership estimation,
and active learning by prioritizing uncertain communities during exploration.
Theoretically, we establish convergence guarantees under linear function
approximation for both actor and critic updates. To our knowledge, this is the
first MARL framework that integrates community structure, transferability, and
active learning with provable guarantees.

</details>


### [55] [Self-Consuming Generative Models with Adversarially Curated Data](https://arxiv.org/abs/2505.09768)
*Xiukun Wei,Xueru Zhang*

Main category: cs.LG

TL;DR: 论文研究了在噪声和对抗性数据筛选下生成模型的自消耗循环训练问题，分析了噪声数据对模型的影响，并设计了对抗攻击算法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨生成模型在噪声和对抗性数据筛选下的演化行为，以及这种数据筛选对训练稳定性和模型崩溃的影响。

Method: 理论分析了噪声数据筛选对生成模型的影响，并设计了针对竞争性对抗场景的攻击算法。

Result: 实验结果表明，所提出的对抗攻击算法在合成和真实数据集上均有效。

Conclusion: 研究为理解噪声和对抗性数据筛选对生成模型训练的影响提供了理论支持，并为实际应用中的对抗场景提供了解决方案。

Abstract: Recent advances in generative models have made it increasingly difficult to
distinguish real data from model-generated synthetic data. Using synthetic data
for successive training of future model generations creates "self-consuming
loops", which may lead to model collapse or training instability. Furthermore,
synthetic data is often subject to human feedback and curated by users based on
their preferences. Ferbach et al. (2024) recently showed that when data is
curated according to user preferences, the self-consuming retraining loop
drives the model to converge toward a distribution that optimizes those
preferences. However, in practice, data curation is often noisy or
adversarially manipulated. For example, competing platforms may recruit
malicious users to adversarially curate data and disrupt rival models. In this
paper, we study how generative models evolve under self-consuming retraining
loops with noisy and adversarially curated data. We theoretically analyze the
impact of such noisy data curation on generative models and identify conditions
for the robustness of the retraining process. Building on this analysis, we
design attack algorithms for competitive adversarial scenarios, where a
platform with a limited budget employs malicious users to misalign a rival's
model from actual user preferences. Experiments on both synthetic and
real-world datasets demonstrate the effectiveness of the proposed algorithms.

</details>


### [56] [Lossless Compression for LLM Tensor Incremental Snapshots](https://arxiv.org/abs/2505.09810)
*Daniel Waddington,Cornel Constantinescu*

Main category: cs.LG

TL;DR: 该论文提出了一种针对大型语言模型（LLM）训练中检查点数据的高效压缩解决方案LMC，通过字节分组和Huffman编码显著减少了数据量和压缩时间。


<details>
  <summary>Details</summary>
Motivation: 解决训练大模型时检查点数据量大、传输和存储开销高的问题，以提高检查点频率并减少资源消耗。

Method: 分析检查点数据的可压缩性，结合字节分组和增量差量压缩等技术，设计并实现了基于Huffman编码的压缩工具LMC。

Result: LMC在压缩性能上优于BZ2，且压缩/解压速度分别为2.78 GiB/s和3.76 GiB/s，显著降低了CPU资源需求。

Conclusion: LMC通过高效压缩实现了检查点频率的提升，为LLM训练提供了更优的数据存储方案。

Abstract: During the training of Large Language Models (LLMs), tensor data is
periodically "checkpointed" to persistent storage to allow recovery of work
done in the event of failure. The volume of data that must be copied during
each checkpoint, even when using reduced-precision representations such as
bfloat16, often reaches hundreds of gigabytes. Furthermore, the data must be
moved across a network and written to a storage system before the next epoch
occurs. With a view to ultimately building an optimized checkpointing solution,
this paper presents experimental analysis of checkpoint data used to derive a
design that maximizes the use of lossless compression to reduce the volume of
data. We examine how tensor data and its compressibility evolve during model
training and evaluate the efficacy of existing common off-the-shelf general
purpose compression engines combined with known data optimization techniques
such as byte-grouping and incremental delta compression.
  Leveraging our analysis we have built an effective compression solution,
known as Language Model Compressor (LMC), which is based on byte-grouping and
Huffman encoding. LMC offers more compression performance than the best
alternative (BZ2) but with an order-of-magnitude reduction in the time needed
to perform the compression. We show that a 16-core parallel implementation of
LMC can attain compression and decompression throughput of 2.78 GiB/s and 3.76
GiB/s respectively. This increase in performance ultimately reduces the CPU
resources needed and provides more time to copy the data to the storage system
before the next epoch thus allowing for higher-frequency checkpoints.

</details>


### [57] [Comparative Analysis of Stroke Prediction Models Using Machine Learning](https://arxiv.org/abs/2505.09812)
*Anastasija Tashkova,Stefan Eftimov,Bojan Ristov,Slobodan Kalajdziski*

Main category: cs.LG

TL;DR: 论文研究了机器学习算法在预测中风风险中的有效性，使用了人口统计、临床和生活方式数据，解决了类别不平衡和缺失数据等挑战，评估了多种模型的性能，发现精度高但敏感性不足，并提出了改进策略。


<details>
  <summary>Details</summary>
Motivation: 中风是全球第二大死亡原因和第三大致残原因，研究旨在通过机器学习提高中风风险的早期预测能力。

Method: 使用Logistic回归、随机森林和XGBoost等模型，处理了类别不平衡和缺失数据问题，评估了模型性能。

Result: 模型表现出高精度但敏感性不足，研究中还识别了最具影响力的预测特征。

Conclusion: 研究结果为开发更可靠、可解释的中风风险早期评估模型提供了支持。

Abstract: Stroke remains one of the most critical global health challenges, ranking as
the second leading cause of death and the third leading cause of disability
worldwide. This study explores the effectiveness of machine learning algorithms
in predicting stroke risk using demographic, clinical, and lifestyle data from
the Stroke Prediction Dataset. By addressing key methodological challenges such
as class imbalance and missing data, we evaluated the performance of multiple
models, including Logistic Regression, Random Forest, and XGBoost. Our results
demonstrate that while these models achieve high accuracy, sensitivity remains
a limiting factor for real-world clinical applications. In addition, we
identify the most influential predictive features and propose strategies to
improve machine learning-based stroke prediction. These findings contribute to
the development of more reliable and interpretable models for the early
assessment of stroke risk.

</details>


### [58] [Adversarial Attack on Large Language Models using Exponentiated Gradient Descent](https://arxiv.org/abs/2505.09820)
*Sajib Biswas,Mao Nishino,Samuel Jacob Chacko,Xiuwen Liu*

Main category: cs.LG

TL;DR: 提出了一种基于指数梯度下降和Bregman投影的优化技术，用于有效破解大型语言模型（LLMs），并在多个开源模型和数据集上验证了其高成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，理解并提升其安全性至关重要。尽管已有技术如RLHF用于模型对齐，但LLMs仍易受攻击，因此需要更有效的破解方法。

Method: 使用指数梯度下降与Bregman投影方法优化连续token嵌入，确保优化后的one-hot编码保持在概率单纯形内。

Result: 在五个开源LLMs和四个数据集上测试，该方法比其他三种先进破解技术的成功率更高且效率更优。

Conclusion: 所提出的技术结合了空间约束和结构，为LLMs的安全性评估提供了一种高效的新方法。

Abstract: As Large Language Models (LLMs) are widely used, understanding them
systematically is key to improving their safety and realizing their full
potential. Although many models are aligned using techniques such as
reinforcement learning from human feedback (RLHF), they are still vulnerable to
jailbreaking attacks. Some of the existing adversarial attack methods search
for discrete tokens that may jailbreak a target model while others try to
optimize the continuous space represented by the tokens of the model's
vocabulary. While techniques based on the discrete space may prove to be
inefficient, optimization of continuous token embeddings requires projections
to produce discrete tokens, which might render them ineffective. To fully
utilize the constraints and the structures of the space, we develop an
intrinsic optimization technique using exponentiated gradient descent with the
Bregman projection method to ensure that the optimized one-hot encoding always
stays within the probability simplex. We prove the convergence of the technique
and implement an efficient algorithm that is effective in jailbreaking several
widely used LLMs. We demonstrate the efficacy of the proposed technique using
five open-source LLMs on four openly available datasets. The results show that
the technique achieves a higher success rate with great efficiency compared to
three other state-of-the-art jailbreaking techniques. The source code for our
implementation is available at:
https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack

</details>


### [59] [Learning Kronecker-Structured Graphs from Smooth Signals](https://arxiv.org/abs/2505.09822)
*Changhao Shi,Gal Mishne*

Main category: cs.LG

TL;DR: 论文提出了一种学习克罗内克结构乘积图的方法，以处理多向数据中的复杂依赖关系，并通过交替优化方案解决了非凸问题。


<details>
  <summary>Details</summary>
Motivation: 由于多向数据的普及，需要更灵活的乘积图模型来捕捉多样化的依赖结构。现有方法主要基于笛卡尔积，而克罗内克积能更精细地建模非分离依赖关系，但其学习问题更具挑战性。

Method: 论文采用交替优化方案，分别优化每个因子图，并为算法的渐近收敛性提供了理论保证。算法还被调整为适用于强积图的学习。

Result: 在合成和真实世界图上进行的实验表明，所提方法在性能上优于现有方法，验证了其有效性。

Conclusion: 通过克罗内克积建模复杂依赖结构并采用交替优化方案，论文为乘积图学习提供了一种高效且理论保证的方法，扩展了图信号处理的应用范围。

Abstract: Graph learning, or network inference, is a prominent problem in graph signal
processing (GSP). GSP generalizes the Fourier transform to non-Euclidean
domains, and graph learning is pivotal to applying GSP when these domains are
unknown. With the recent prevalence of multi-way data, there has been growing
interest in product graphs that naturally factorize dependencies across
different ways. However, the types of graph products that can be learned are
still limited for modeling diverse dependency structures. In this paper, we
study the problem of learning a Kronecker-structured product graph from smooth
signals. Unlike the more commonly used Cartesian product, the Kronecker product
models dependencies in a more intricate, non-separable way, but posits harder
constraints on the graph learning problem. To tackle this non-convex problem,
we propose an alternating scheme to optimize each factor graph and provide
theoretical guarantees for its asymptotic convergence. The proposed algorithm
is also modified to learn factor graphs of the strong product. We conduct
experiments on synthetic and real-world graphs and demonstrate our approach's
efficacy and superior performance compared to existing methods.

</details>


### [60] [Causal Predictive Optimization and Generation for Business AI](https://arxiv.org/abs/2505.09847)
*Liyang Zhao,Olurotimi Seton,Himadeep Reddy Reddivari,Suvendu Jena,Shadow Zhao,Rachit Kumar,Changshuai Wei*

Main category: cs.LG

TL;DR: 本文提出了一种名为因果预测优化与生成（Causal Predictive Optimization and Generation）的方法，用于优化B2B销售流程，包括因果机器学习、约束优化和生成AI三层次结构，并在LinkedIn中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 优化销售流程对B2B业务至关重要，传统方法效果有限，需结合因果推断和AI技术提升效率和效果。

Method: 提出三层次结构：1) 因果机器学习的预测层；2) 约束优化和上下文老虎机的优化层；3) 生成AI与反馈循环的服务层。

Result: 在LinkedIn中部署该系统，显著优于传统系统，取得了实际业务提升。

Conclusion: 该方法为销售优化和商业AI提供了可扩展的框架，经验证有效且具有广泛适用性。

Abstract: The sales process involves sales functions converting leads or opportunities
to customers and selling more products to existing customers. The optimization
of the sales process thus is key to success of any B2B business. In this work,
we introduce a principled approach to sales optimization and business AI,
namely the Causal Predictive Optimization and Generation, which includes three
layers: 1) prediction layer with causal ML 2) optimization layer with
constraint optimization and contextual bandit 3) serving layer with Generative
AI and feedback-loop for system enhancement. We detail the implementation and
deployment of the system in LinkedIn, showcasing significant wins over legacy
systems and sharing learning and insight broadly applicable to this field.

</details>


### [61] [Radiogenomic Bipartite Graph Representation Learning for Alzheimer's Disease Detection](https://arxiv.org/abs/2505.09848)
*Aditya Raj,Golrokh Mirzaei*

Main category: cs.LG

TL;DR: 该研究提出了一种结合影像与基因组数据的新方法，通过异质二分图表示学习技术对阿尔茨海默病进行三阶段分类，并识别关键基因，取得了良好的分类效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于整合影像与基因组数据以揭示疾病的复杂机制，尤其在阿尔茨海默病（AD）的多阶段分类中探索其潜力。

Method: 方法包括利用结构MRI图像和基因表达数据构建异质二分图（包含基因和图像两类节点），并通过图表示学习进行分类。

Result: 结果显示该方法能有效区分AD的三个阶段（AD、MCI、CN），并识别关键基因，分类指标（准确率、召回率等）表现良好。

Conclusion: 结论指出该技术对小数据集有效，且有望推广至其他疾病的放射基因组分类。

Abstract: Imaging and genomic data offer distinct and rich features, and their
integration can unveil new insights into the complex landscape of diseases. In
this study, we present a novel approach utilizing radiogenomic data including
structural MRI images and gene expression data, for Alzheimer's disease
detection. Our framework introduces a novel heterogeneous bipartite graph
representation learning featuring two distinct node types: genes and images.
The network can effectively classify Alzheimer's disease (AD) into three
distinct stages:AD, Mild Cognitive Impairment (MCI), and Cognitive Normal (CN)
classes, utilizing a small dataset. Additionally, it identified which genes
play a significant role in each of these classification groups. We evaluate the
performance of our approach using metrics including classification accuracy,
recall, precision, and F1 score. The proposed technique holds potential for
extending to radiogenomic-based classification to other diseases.

</details>


### [62] [ZENN: A Thermodynamics-Inspired Computational Framework for Heterogeneous Data-Driven Modeling](https://arxiv.org/abs/2505.09851)
*Shun Wang,Shun-Li Shang,Zi-Kui Liu,Wenrui Hao*

Main category: cs.LG

TL;DR: 本文提出了一种基于zentropy理论的ZENN神经网络架构，用于处理异构数据，展示了其在分类和能量景观重构任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统熵方法在处理异构数据时面临挑战，需要一种新的理论和方法来更好地捕获多源数据的潜在结构。

Method: 提出了zentropy增强的神经网络（ZENN），通过同时学习能量和固有熵分量，重新设计了神经网络架构。

Result: ZENN在分类和能量景观重构任务中表现出卓越的泛化能力和鲁棒性，尤其在预测高阶导数方面。

Conclusion: ZENN为涉及复杂异构数据的科学问题提供了一个多功能且鲁棒的深度学习框架。

Abstract: Traditional entropy-based methods - such as cross-entropy loss in
classification problems - have long been essential tools for quantifying
uncertainty and disorder in data and developing artificial intelligence
algorithms. However, the rapid growth of data across various domains has
introduced new challenges, particularly the integration of heterogeneous
datasets with intrinsic disparities. In this paper, we extend zentropy theory
into the data science domain by introducing intrinsic entropy, enabling more
effective learning from heterogeneous data sources. We propose a
zentropy-enhanced neural network (ZENN) that simultaneously learns both energy
and intrinsic entropy components, capturing the underlying structure of
multi-source data. To support this, we redesign the neural network architecture
to better reflect the intrinsic properties and variability inherent in diverse
datasets. We demonstrate the effectiveness of ZENN on classification tasks and
energy landscape reconstructions, showing its superior generalization
capabilities and robustness-particularly in predicting high-order derivatives.
As a practical application, we employ ZENN to reconstruct the Helmholtz energy
landscape of Fe3Pt using data generated from DFT and capture key material
behaviors, including negative thermal expansion and the critical point in the
temperature-pressure space. Overall, our study introduces a novel approach for
data-driven machine learning grounded in zentropy theory, highlighting ZENN as
a versatile and robust deep learning framework for scientific problems
involving complex, heterogeneous datasets.

</details>


### [63] [Chisme: Fully Decentralized Differentiated Deep Learning for Edge Intelligence](https://arxiv.org/abs/2505.09854)
*Harikrishna Kuttivelil,Katia Obraczka*

Main category: cs.LG

TL;DR: 论文提出了一种名为Chisme的新型协议套件，旨在解决网络边缘智能实现中的挑战，包括数据分布不均、连接不稳定和基础设施缺乏。Chisme提供了同步（Chisme-DFL）和异步（Chisme-GL）两种变体，通过数据相似性启发式方法优化模型训练，并在不同网络场景中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着边缘设备能力的提升和智能服务需求的增加，分布式学习在网络边缘成为关键技术。现有方法如联邦学习（FL）和去中心化FL（DFL）在资源受限或无基础设施环境中面临连接和同步的挑战。尽管Gossip学习（GL）更鲁棒，但通常设计用于同质数据分布，可能不适用于所有场景。本文旨在解决这些挑战。

Method: 提出Chisme协议套件，分为同步DFL（Chisme-DFL）和异步GL（Chisme-GL）两种变体。利用数据相似性启发式方法，通过现有模型更新通信推断代理之间的亲和力，从而优化模型聚合与合并机制。

Result: 实验证明，在从连接不稳定到完全连接的不同网络场景中，Chisme方法在分布式和异构数据上的模型训练表现优于标准方法。Chisme-DFL资源利用率随网络规模线性增长，而Chisme-GL完全异步且资源需求与网络规模无关。

Conclusion: Chisme协议套件通过同步和异步两种方式有效解决了网络边缘分布式学习中的挑战，尤其是在异构数据和不稳定连接环境下，表现优于现有方法。

Abstract: As demand for intelligent services rises and edge devices become more
capable, distributed learning at the network edge has emerged as a key enabling
technology. While existing paradigms like federated learning (FL) and
decentralized FL (DFL) enable privacy-preserving distributed learning in many
scenarios, they face potential challenges in connectivity and synchronization
imposed by resource-constrained and infrastructure-less environments. While
more robust, gossip learning (GL) algorithms have generally been designed for
homogeneous data distributions and may not suit all contexts. This paper
introduces Chisme, a novel suite of protocols designed to address the
challenges of implementing robust intelligence in the network edge,
characterized by heterogeneous data distributions, episodic connectivity, and
lack of infrastructure. Chisme includes both synchronous DFL (Chisme-DFL) and
asynchronous GL (Chisme-GL) variants that enable collaborative yet
decentralized model training that considers underlying data heterogeneity. We
introduce a data similarity heuristic that allows agents to opportunistically
infer affinity with each other using the existing communication of model
updates in decentralized FL and GL. We leverage the heuristic to extend DFL's
model aggregation and GL's model merge mechanisms for better personalized
training while maintaining collaboration. While Chisme-DFL is a synchronous
decentralized approach whose resource utilization scales linearly with network
size, Chisme-GL is fully asynchronous and has a lower, constant resource
requirement independent of network size. We demonstrate that Chisme methods
outperform their standard counterparts in model training over distributed and
heterogeneous data in network scenarios ranging from less connected and
reliable networks to fully connected and lossless networks.

</details>


### [64] [Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers](https://arxiv.org/abs/2505.09855)
*Alexander Y. Ku,Thomas L. Griffiths,Stephanie C. Y. Chan*

Main category: cs.LG

TL;DR: 论文研究了Transformer模型中权重学习（IWL）与上下文学习（ICL）的平衡机制，借鉴了进化生物学中的遗传编码和表型可塑性，通过实验证明了环境稳定性和线索可靠性对两种学习模式的影响。


<details>
  <summary>Details</summary>
Motivation: 旨在理解Transformer模型中两种学习模式的相互作用，并借鉴进化生物学的理论来解释其动态平衡。

Method: 通过回归和分类任务实验，系统地研究了环境稳定性和线索可靠性对IWL和ICL平衡的影响。

Result: 高环境稳定性显著偏好IWL，而高线索可靠性则增强ICL效果；学习动态显示任务依赖性，部分场景下会从IWL转向ICL。

Conclusion: 研究支持了相对成本假说，揭示了可预测性是决定Transformer学习策略的关键因素，为理解ICL和优化训练方法提供了新视角。

Abstract: Transformer models learn in two distinct modes: in-weights learning (IWL),
encoding knowledge into model weights, and in-context learning (ICL), adapting
flexibly to context without weight modification. To better understand the
interplay between these learning modes, we draw inspiration from evolutionary
biology's analogous adaptive strategies: genetic encoding (akin to IWL,
adapting over generations and fixed within an individual's lifetime) and
phenotypic plasticity (akin to ICL, enabling flexible behavioral responses to
environmental cues). In evolutionary biology, environmental predictability
dictates the balance between these strategies: stability favors genetic
encoding, while reliable predictive cues promote phenotypic plasticity. We
experimentally operationalize these dimensions of predictability and
systematically investigate their influence on the ICL/IWL balance in
Transformers. Using regression and classification tasks, we show that high
environmental stability decisively favors IWL, as predicted, with a sharp
transition at maximal stability. Conversely, high cue reliability enhances ICL
efficacy, particularly when stability is low. Furthermore, learning dynamics
reveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift
occurs in some settings (e.g., classification with many classes), we
demonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL
acquisition (e.g., regression) can exhibit an initial IWL phase later yielding
to ICL dominance. These findings support a relative-cost hypothesis for
explaining these learning mode transitions, establishing predictability as a
critical factor governing adaptive strategies in Transformers, and offering
novel insights for understanding ICL and guiding training methodologies.

</details>


### [65] [LiDDA: Data Driven Attribution at LinkedIn](https://arxiv.org/abs/2505.09861)
*John Bencina,Erkut Aykutlug,Yue Chen,Zerui Zhang,Stephanie Sorenson,Shao Tang,Changshuai Wei*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer的统一归因方法，用于处理成员级和聚合级数据及外部宏观因素，展示了在LinkedIn的大规模应用效果，并分享了广泛适用于营销和广告技术领域的经验。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的归因是现代营销智能的基础，对营销业务和广告平台至关重要。当前方法难以统一处理不同层级数据和外部因素，因此需要一种更灵活、可扩展的解决方案。

Method: 采用基于Transformer的统一归因方法，支持成员级数据、聚合级数据及外部宏观因素的整合，并在LinkedIn实现了大规模应用。

Result: 该方法在LinkedIn的应用表现出显著影响，验证了其有效性。

Conclusion: 提出的Transformer归因方法不仅解决了数据处理的多层次挑战，还提供了适用于营销和广告技术领域的通用见解。

Abstract: Data Driven Attribution, which assigns conversion credits to marketing
interactions based on causal patterns learned from data, is the foundation of
modern marketing intelligence and vital to any marketing businesses and
advertising platform. In this paper, we introduce a unified transformer-based
attribution approach that can handle member-level data, aggregate-level data,
and integration of external macro factors. We detail the large scale
implementation of the approach at LinkedIn, showcasing significant impact. We
also share learning and insights that are broadly applicable to the marketing
and ad tech fields.

</details>


### [66] [BINGO: A Novel Pruning Mechanism to Reduce the Size of Neural Networks](https://arxiv.org/abs/2505.09864)
*Aditya Panangat*

Main category: cs.LG

TL;DR: BINGO是一种新型的神经网络剪枝方法，通过一次训练即可评估权重的重要性，减少计算成本，同时保持模型准确性。解决当前剪枝方法计算量大、成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 大型模型训练成本高昂，阻碍了非富裕个体参与AI发展，同时增加了消费者负担。现有剪枝方法计算复杂且不环保。

Method: BINGO在训练过程中逐个分析神经网络的子集，评估每个权重对模型准确性的贡献，生成重要性评分，从而一次性剪枝不重要的权重。

Result: BINGO提供了一种计算效率高且能保持准确性的剪枝技术，减少了训练和运行大型模型的经济和环境成本。

Conclusion: BINGO展示了AI发展不一定伴随模型规模增长，为高效、环保的模型优化提供了新方向。

Abstract: Over the past decade, the use of machine learning has increased
exponentially. Models are far more complex than ever before, growing to
gargantuan sizes and housing millions of weights. Unfortunately, the fact that
large models have become the state of the art means that it often costs
millions of dollars to train and operate them. These expenses not only hurt
companies but also bar non-wealthy individuals from contributing to new
developments and force consumers to pay greater prices for AI. Current methods
used to prune models, such as iterative magnitude pruning, have shown great
accuracy but require an iterative training sequence that is incredibly
computationally and environmentally taxing. To solve this problem, BINGO is
introduced. BINGO, during the training pass, studies specific subsets of a
neural network one at a time to gauge how significant of a role each weight
plays in contributing to a network's accuracy. By the time training is done,
BINGO generates a significance score for each weight, allowing for
insignificant weights to be pruned in one shot. BINGO provides an
accuracy-preserving pruning technique that is less computationally intensive
than current methods, allowing for a world where AI growth does not have to
mean model growth, as well.

</details>


### [67] [Change Detection in Multivariate data streams: Online Analysis with Kernel-QuantTree](https://arxiv.org/abs/2410.13778)
*Michelangelo Olmo Nogara Notarianni,Filippo Leveni,Diego Stucchi,Luca Frittoli,Giacomo Boracchi*

Main category: cs.LG

TL;DR: KQT-EWMA是一种结合KQT直方图和EWMA统计的非参数变化检测算法，用于在线监测多元数据流，能够灵活建模并控制误报率，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有非参数变化检测方法难以预先控制误报率（ARL0），而KQT-EWMA旨在解决这一问题，提供更灵活的监控方案。

Method: 通过结合KQT直方图（建模任意静态分布）和EWMA统计量，实现在线监测，并预先控制ARL0。

Result: 实验表明，KQT-EWMA能有效控制ARL0，且检测延迟与或优于同类先进方法。

Conclusion: KQT-EWMA是一种实用且灵活的非参数变化检测方法，尤其在控制误报率方面表现优异。

Abstract: We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),
a non-parametric change-detection algorithm that combines the Kernel-QuantTree
(KQT) histogram and the EWMA statistic to monitor multivariate data streams
online. The resulting monitoring scheme is very flexible, since histograms can
be used to model any stationary distribution, and practical, since the
distribution of test statistics does not depend on the distribution of
datastream in stationary conditions (non-parametric monitoring). KQT-EWMA
enables controlling false alarms by operating at a pre-determined Average Run
Length ($ARL_0$), which measures the expected number of stationary samples to
be monitored before triggering a false alarm. The latter peculiarity is in
contrast with most non-parametric change-detection tests, which rarely can
control the $ARL_0$ a priori. Our experiments on synthetic and real-world
datasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving
detection delays comparable to or lower than state-of-the-art methods designed
to work in the same conditions.

</details>


### [68] [Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks](https://arxiv.org/abs/2505.09901)
*Ziyuan Zhang,Darcy Wang,Ningyuan Chen,Rodrigo Mansur,Vahid Sarhangian*

Main category: cs.LG

TL;DR: 论文比较了大型语言模型（LLMs）与人类在多臂老虎机任务中的决策行为，发现推理能力使LLMs更接近人类的探索-利用权衡策略，但在复杂任务中仍存在差距。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证LLMs是否能模拟人类在动态不确定环境中的决策行为，尤其是探索-利用权衡（E&E）。

Method: 通过经典的认知科学多臂老虎机任务，使用可解释的选择模型分析LLMs、人类和算法的决策策略，并测试推理能力对LLMs行为的影响。

Result: 推理能力让LLMs在简单任务中表现出与人类相似的随机和定向探索行为；但在复杂非静态环境中，LLMs的适应性不足，尤其在定向探索上落后于人类。

Conclusion: LLMs作为人类行为模拟器和自动化决策工具具有潜力，但在复杂环境中的适应性仍需改进。

Abstract: Large language models (LLMs) are increasingly used to simulate or automate
human behavior in complex sequential decision-making tasks. A natural question
is then whether LLMs exhibit similar decision-making behavior to humans, and
can achieve comparable (or superior) performance. In this work, we focus on the
exploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic
decision-making under uncertainty. We employ canonical multi-armed bandit (MAB)
tasks introduced in the cognitive science and psychiatry literature to conduct
a comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.
We use interpretable choice models to capture the E&E strategies of the agents
and investigate how explicit reasoning, through both prompting strategies and
reasoning-enhanced models, shapes LLM decision-making. We find that reasoning
shifts LLMs toward more human-like behavior, characterized by a mix of random
and directed exploration. In simple stationary tasks, reasoning-enabled LLMs
exhibit similar levels of random and directed exploration compared to humans.
However, in more complex, non-stationary environments, LLMs struggle to match
human adaptability, particularly in effective directed exploration, despite
achieving similar regret in certain scenarios. Our findings highlight both the
promise and limits of LLMs as simulators of human behavior and tools for
automated decision-making and point to potential areas of improvements.

</details>


### [69] [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/abs/2505.03084)
*Shashank Kapoor,Sanjay Surendranath Girija,Lakshit Arora,Dipen Pradhan,Ankit Shetgaonkar,Aman Raj*

Main category: cs.LG

TL;DR: 论文总结了多模态模型中针对文本、图像、视频和音频的对抗攻击类型，填补了多模态领域攻击类型实践视角的空白。


<details>
  <summary>Details</summary>
Motivation: 随着开源多模态模型的普及，攻击威胁也随之增加。但现有研究缺乏对攻击类型的实践总结，影响模型的实际应用安全。

Method: 通过对四种模态（文本、图像、视频、音频）的对抗攻击进行全面调查，分析多模态对抗威胁的演变。

Result: 首次全面总结了多模态世界的威胁格局，为实践者提供攻击类型的参考。

Conclusion: 论文填补了多模态对抗攻击研究的实践空白，为模型部署提供了安全指南。

Abstract: The introduction of multimodal models is a huge step forward in Artificial
Intelligence. A single model is trained to understand multiple modalities:
text, image, video, and audio. Open-source multimodal models have made these
breakthroughs more accessible. However, considering the vast landscape of
adversarial attacks across these modalities, these models also inherit
vulnerabilities of all the modalities, and ultimately, the adversarial threat
amplifies. While broad research is available on possible attacks within or
across these modalities, a practitioner-focused view that outlines attack types
remains absent in the multimodal world. As more Machine Learning Practitioners
adopt, fine-tune, and deploy open-source models in real-world applications,
it's crucial that they can view the threat landscape and take the preventive
actions necessary. This paper addresses the gap by surveying adversarial
attacks targeting all four modalities: text, image, video, and audio. This
survey provides a view of the adversarial attack landscape and presents how
multimodal adversarial threats have evolved. To the best of our knowledge, this
survey is the first comprehensive summarization of the threat landscape in the
multimodal world.

</details>


### [70] [Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture](https://arxiv.org/abs/2505.09907)
*Linwei Zhang,LuFeng,Ruijia Liang*

Main category: cs.LG

TL;DR: 本文提出了一种结合TCN、MLP和注意力机制的深度学习模型TCN-MLP-Attention，用于预测Hass牛油果价格，结果表明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着健康食品需求的增长，农产品价格预测变得日益重要。Hass牛油果作为高价值作物，其价格受季节、地区和天气等多因素影响，传统模型难以处理此类非线性和动态数据。

Method: 提出TCN-MLP-Attention混合模型，TCN用于序列特征提取，MLP用于非线性交互，注意力机制用于动态特征加权。数据集包含2015-2018年美国Hass牛油果销售记录，涵盖销量、均价、时间、地区等变量。

Result: 模型预测性能优异，RMSE为1.23，MSE为1.51，优于传统方法。

Conclusion: 该研究为农产品市场时间序列预测提供了可扩展的有效方法，对智能供应链管理和价格策略优化具有重要价值。

Abstract: With the growing demand for healthy foods, agricultural product price
forecasting has become increasingly important. Hass avocados, as a high-value
crop, exhibit complex price fluctuations influenced by factors such as
seasonality, region, and weather. Traditional prediction models often struggle
with highly nonlinear and dynamic data. To address this, we propose a hybrid
deep learning model, TCN-MLP-Attention Architecture, combining Temporal
Convolutional Networks (TCN) for sequential feature extraction, Multi-Layer
Perceptrons (MLP) for nonlinear interactions, and an Attention mechanism for
dynamic feature weighting. The dataset used covers over 50,000 records of Hass
avocado sales across the U.S. from 2015 to 2018, including variables such as
sales volume, average price, time, region, weather, and variety type, collected
from point-of-sale systems and the Hass Avocado Board. After systematic
preprocessing, including missing value imputation and feature normalization,
the proposed model was trained and evaluated. Experimental results demonstrate
that the TCN-MLP-Attention model achieves excellent predictive performance,
with an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.
This research provides a scalable and effective approach for time series
forecasting in agricultural markets and offers valuable insights for
intelligent supply chain management and price strategy optimization.

</details>


### [71] [Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity](https://arxiv.org/abs/2505.09922)
*Zichen Liu,Wei Zhang,Tiejun Li*

Main category: cs.LG

TL;DR: 该论文研究了在一般流形约束数据上直接采样欧几里得扩散模型的方法，揭示了嵌入空间中分数函数的多尺度奇异性，并提出两种新方法来提高采样精度。


<details>
  <summary>Details</summary>
Motivation: 探索如何有效利用欧几里得扩散模型处理流形约束数据，解决分数函数在嵌入空间中的多尺度奇异性问题。

Method: 提出了两种方法：Niso-DM（沿法向引入非各向同性噪声）和Tango-DM（仅训练分数函数的切向分量）。

Result: 数值实验表明，新方法在复杂几何流形上的分布采样中表现出优越性能。

Conclusion: 通过理论分析和实验验证，论文证明了所提方法在流形约束数据生成任务中的有效性。

Abstract: Euclidean diffusion models have achieved remarkable success in generative
modeling across diverse domains, and they have been extended to manifold case
in recent advances. Instead of explicitly utilizing the structure of special
manifolds as studied in previous works, we investigate direct sampling of the
Euclidean diffusion models for general manifold-constrained data in this paper.
We reveal the multiscale singularity of the score function in the embedded
space of manifold, which hinders the accuracy of diffusion-generated samples.
We then present an elaborate theoretical analysis of the singularity structure
of the score function by separating it along the tangential and normal
directions of the manifold. To mitigate the singularity and improve the
sampling accuracy, we propose two novel methods: (1) Niso-DM, which introduces
non-isotropic noise along the normal direction to reduce scale discrepancies,
and (2) Tango-DM, which trains only the tangential component of the score
function using a tangential-only loss function. Numerical experiments
demonstrate that our methods achieve superior performance on distributions over
various manifolds with complex geometries.

</details>


### [72] [Online Isolation Forest](https://arxiv.org/abs/2505.09593)
*Filippo Leveni,Guilherme Weigert Cassales,Bernhard Pfahringer,Albert Bifet,Giacomo Boracchi*

Main category: cs.LG

TL;DR: 提出了Online-iForest方法，专门用于流式数据异常检测，无需重训练且效率高。


<details>
  <summary>Details</summary>
Motivation: 现有离线和在线异常检测方法在流式场景中存在数据访问和适应性不足的问题。

Method: 设计了Online-iForest，能够动态追踪流式数据的生成过程。

Result: 在真实数据集上验证，性能与在线方法相当，接近离线方法，且效率更高。

Conclusion: Online-iForest是高效流式异常检测的有潜力解决方案，适用于快速识别场景。

Abstract: The anomaly detection literature is abundant with offline methods, which
require repeated access to data in memory, and impose impractical assumptions
when applied to a streaming context. Existing online anomaly detection methods
also generally fail to address these constraints, resorting to periodic
retraining to adapt to the online context. We propose Online-iForest, a novel
method explicitly designed for streaming conditions that seamlessly tracks the
data generating process as it evolves over time. Experimental validation on
real-world datasets demonstrated that Online-iForest is on par with online
alternatives and closely rivals state-of-the-art offline anomaly detection
techniques that undergo periodic retraining. Notably, Online-iForest
consistently outperforms all competitors in terms of efficiency, making it a
promising solution in applications where fast identification of anomalies is of
primary importance such as cybersecurity, fraud and fault detection.

</details>


### [73] [Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback](https://arxiv.org/abs/2505.09925)
*Yutao Yang,Jie Zhou,Junsong Li,Qianjun Pan,Bihao Zhan,Qin Chen,Xipeng Qiu,Liang He*

Main category: cs.LG

TL;DR: 论文提出了一种交互式持续学习框架RiCL，利用大语言模型从实时嘈杂的人类反馈中动态学习新技能，同时保留已有知识，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法依赖静态数据集和干净标签，难以应对实时交互中的动态更新和嘈杂反馈，亟需一种新框架解决这些问题。

Method: RiCL框架结合三个核心组件：时序一致性净化器（过滤噪声样本）、交互感知偏好优化策略（对齐人类意图）和抗噪声对比学习模块（捕获鲁棒表征）。

Result: 在FewRel和TACRED噪声数据集上的实验表明，RiCL显著优于现有在线持续学习与噪声标签学习方法的组合。

Conclusion: RiCL通过动态处理嘈杂反馈和实时学习，为交互式持续学习提供了高效解决方案，推动了实际应用场景的适应性。

Abstract: This paper introduces an interactive continual learning paradigm where AI
models dynamically learn new skills from real-time human feedback while
retaining prior knowledge. This paradigm distinctively addresses two major
limitations of traditional continual learning: (1) dynamic model updates using
streaming, real-time human-annotated data, rather than static datasets with
fixed labels, and (2) the assumption of clean labels, by explicitly handling
the noisy feedback common in real-world interactions. To tackle these problems,
we propose RiCL, a Reinforced interactive Continual Learning framework
leveraging Large Language Models (LLMs) to learn new skills effectively from
dynamic feedback. RiCL incorporates three key components: a temporal
consistency-aware purifier to automatically discern clean from noisy samples in
data streams; an interaction-aware direct preference optimization strategy to
align model behavior with human intent by reconciling AI-generated and
human-provided feedback; and a noise-resistant contrastive learning module that
captures robust representations by exploiting inherent data relationships, thus
avoiding reliance on potentially unreliable labels. Extensive experiments on
two benchmark datasets (FewRel and TACRED), contaminated with realistic noise
patterns, demonstrate that our RiCL approach substantially outperforms existing
combinations of state-of-the-art online continual learning and noisy-label
learning methods.

</details>


### [74] [Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors](https://arxiv.org/abs/2505.09949)
*Ahmed S. Abdelrahman,Mohamed Abdel-Aty,Samgyu Yang,Abdulrahman Faden*

Main category: cs.LG

TL;DR: 该研究利用LLM（大语言模型）分析高速公路事故数据，通过零样本分类识别事故原因，结果显示LLM能有效识别酒驾、超速等主因，并结合事件数据提供更深入分析，验证了其实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法和机器学习模型难以捕捉事故复杂因素及其独特特征，因此探索LLM在高速公路事故分析中的应用，以提升事故原因识别能力和交通安全措施效果。

Method: 通过编译226项高速公路事故相关研究构建训练数据集，使用QLoRA对Llama3 8B模型进行微调，并通过零样本分类实现无标签数据的事故原因分析。

Result: LLM成功识别酒驾、超速等主要事故原因，结合事件数据（如道路维护）提供更深入分析，研究者问卷调查显示88.89%的一致性验证了模型实用性。

Conclusion: 研究表明LLM能全面分析事故原因及影响因素，为规划者和政策制定者提供有价值的见解和潜在对策，提升交通安全措施效率。

Abstract: Understanding the factors contributing to traffic crashes and developing
strategies to mitigate their severity is essential. Traditional statistical
methods and machine learning models often struggle to capture the complex
interactions between various factors and the unique characteristics of each
crash. This research leverages large language model (LLM) to analyze freeway
crash data and provide crash causation analysis accordingly. By compiling 226
traffic safety studies related to freeway crashes, a training dataset
encompassing environmental, driver, traffic, and geometric design factors was
created. The Llama3 8B model was fine-tuned using QLoRA to enhance its
understanding of freeway crashes and their contributing factors, as covered in
these studies. The fine-tuned Llama3 8B model was then used to identify crash
causation without pre-labeled data through zero-shot classification, providing
comprehensive explanations to ensure that the identified causes were reasonable
and aligned with existing research. Results demonstrate that LLMs effectively
identify primary crash causes such as alcohol-impaired driving, speeding,
aggressive driving, and driver inattention. Incorporating event data, such as
road maintenance, offers more profound insights. The model's practical
applicability and potential to improve traffic safety measures were validated
by a high level of agreement among researchers in the field of traffic safety,
as reflected in questionnaire results with 88.89%. This research highlights the
complex nature of traffic crashes and how LLMs can be used for comprehensive
analysis of crash causation and other contributing factors. Moreover, it
provides valuable insights and potential countermeasures to aid planners and
policymakers in developing more effective and efficient traffic safety
practices.

</details>


### [75] [Task-Core Memory Management and Consolidation for Long-term Continual Learning](https://arxiv.org/abs/2505.09952)
*Tianyu Huai,Jie Zhou,Yuxuan Cai,Qin Chen,Wen Wu,Xingjiao Wu,Xipeng Qiu,Liang He*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为Long-CL的新框架，专注于长期持续学习任务，通过任务核心记忆管理和长期记忆巩固机制来减轻灾难性遗忘，并在两个新构建的基准测试上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的持续学习方法在处理长期任务时表现不佳，灾难性遗忘问题尤为突出。研究旨在探索现有方法在长期CL中的表现，并开发新的解决方案。

Method: 提出任务核心记忆管理策略和长期记忆巩固机制，构建多模态和文本基准测试MMLongCL-Bench和TextLongCL-Bench。

Result: Long-CL在两个基准测试上的性能分别比现有最优方法提高了7.4%和6.5%。

Conclusion: Long-CL通过模仿人类记忆机制，有效缓解了长期持续学习中的灾难性遗忘问题，为新研究提供了基准和方向。

Abstract: In this paper, we focus on a long-term continual learning (CL) task, where a
model learns sequentially from a stream of vast tasks over time, acquiring new
knowledge while retaining previously learned information in a manner akin to
human learning. Unlike traditional CL settings, long-term CL involves handling
a significantly larger number of tasks, which exacerbates the issue of
catastrophic forgetting. Our work seeks to address two critical questions: 1)
How do existing CL methods perform in the context of long-term CL? and 2) How
can we mitigate the catastrophic forgetting that arises from prolonged
sequential updates? To tackle these challenges, we propose a novel framework
inspired by human memory mechanisms for long-term continual learning (Long-CL).
Specifically, we introduce a task-core memory management strategy to
efficiently index crucial memories and adaptively update them as learning
progresses. Additionally, we develop a long-term memory consolidation mechanism
that selectively retains hard and discriminative samples, ensuring robust
knowledge retention. To facilitate research in this area, we construct and
release two multi-modal and textual benchmarks, MMLongCL-Bench and
TextLongCL-Bench, providing a valuable resource for evaluating long-term CL
approaches. Experimental results show that Long-CL outperforms the previous
state-of-the-art by 7.4\% and 6.5\% AP on the two benchmarks, respectively,
demonstrating the effectiveness of our approach.

</details>


### [76] [TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.09955)
*Jaeho Kim,Seulki Lee*

Main category: cs.LG

TL;DR: 论文提出TransPL方法，通过代码转移矩阵显式建模时间序列的时序和通道偏移，改进无监督域适应（UDA）中的伪标签生成，在多个基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统伪标签策略在时间序列数据中无法有效捕捉时序模式和通道偏移，导致伪标签质量不佳，需要更优方法。

Method: TransPL通过向量量化（VQ）提取时间序列片段的代码，构建源域的类和通道级代码转移矩阵，并基于贝叶斯规则进行目标域适应，生成加权类条件似然的伪标签。

Result: 在四个时间序列UDA基准测试中，TransPL显著优于现有方法（准确率提升6.1%，F1提升4.9%），且提供可解释的代码转移矩阵。

Conclusion: TransPL通过显式建模时序和通道偏移，实现了高性能、多功能和可解释的伪标签生成，为时间序列UDA提供了有效解决方案。

Abstract: Unsupervised domain adaptation (UDA) for time series data remains a critical
challenge in deep learning, with traditional pseudo-labeling strategies failing
to capture temporal patterns and channel-wise shifts between domains, producing
sub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that
addresses these limitations by modeling the joint distribution $P(\mathbf{X},
y)$ of the source domain through code transition matrices, where the codes are
derived from vector quantization (VQ) of time series patches. Our method
constructs class- and channel-wise code transition matrices from the source
domain and employs Bayes' rule for target domain adaptation, generating
pseudo-labels based on channel-wise weighted class-conditional likelihoods.
TransPL offers three key advantages: explicit modeling of temporal transitions
and channel-wise shifts between different domains, versatility towards
different UDA scenarios (e.g., weakly-supervised UDA), and explainable
pseudo-label generation. We validate TransPL's effectiveness through extensive
analysis on four time series UDA benchmarks and confirm that it consistently
outperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1%
accuracy improvement, 4.9% F1 improvement), while providing interpretable
insights into the domain adaptation process through its learned code transition
matrices.

</details>


### [77] [Approximated Behavioral Metric-based State Projection for Federated Reinforcement Learning](https://arxiv.org/abs/2505.09959)
*Zengxia Guo,Bohui An,Zhongqi Lu*

Main category: cs.LG

TL;DR: 论文提出FedRAG框架，通过共享近似行为度量状态投影函数提升联邦强化学习性能并保护隐私，实验结果验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦强化学习（FRL）在隐私保护下通过共享本地状态或策略信息提升性能，但现有方法可能仍存在敏感信息泄露风险。本文旨在找到一种既能提升性能又能更有效保护隐私的方式。

Method: 提出FedRAG框架，通过为每个客户端学习实用的状态投影函数并在中心服务器聚合投影函数参数，共享近似行为度量状态投影而非敏感任务信息。

Result: 在DeepMind Control Suite上的大量实验表明，FedRAG能有效提升性能并提供信息增益，同时避免敏感信息泄露。

Conclusion: FedRAG通过共享状态投影函数而非原始数据，平衡了性能与隐私保护，为联邦强化学习提供了一种新思路。

Abstract: Federated reinforcement learning (FRL) methods usually share the encrypted
local state or policy information and help each client to learn from others
while preserving everyone's privacy. In this work, we propose that sharing the
approximated behavior metric-based state projection function is a promising way
to enhance the performance of FRL and concurrently provides an effective
protection of sensitive information. We introduce FedRAG, a FRL framework to
learn a computationally practical projection function of states for each client
and aggregating the parameters of projection functions at a central server. The
FedRAG approach shares no sensitive task-specific information, yet provides
information gain for each client. We conduct extensive experiments on the
DeepMind Control Suite to demonstrate insightful results.

</details>


### [78] [A Comprehensive Machine Learning Framework for Heart Disease Prediction: Performance Evaluation and Future Perspectives](https://arxiv.org/abs/2505.09969)
*Ali Azimi Lamir,Shiva Razzagzadeh,Zeynab Rezaei*

Main category: cs.LG

TL;DR: 该研究提出了一种基于机器学习的框架，用于使用包含303个样本和14个特征的心脏病数据集进行预测。通过数据预处理、模型训练和三种分类器的评估，发现随机森林模型表现最佳，准确率达91%，F1分数为0.89。研究结果显示该模型在临床决策中具有潜力，但数据集规模和泛化性仍需改进。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一种高效的机器学习模型，用于预测心脏病，以辅助临床决策，展示机器学习在医疗健康领域的应用潜力。

Method: 方法包括数据预处理、模型训练（Logistic回归、KNN和随机森林）及超参数调优（GridSearchCV和RandomizedSearchCV）。

Result: 随机森林模型表现最优，准确率达91%，F1分数为0.89。评估指标显示该模型在各类别上表现均衡。

Conclusion: 该模型对心脏病预测有显著潜力，但未来需通过更大规模、更多样化的数据集进一步验证其泛化性。

Abstract: This study presents a machine learning-based framework for heart disease
prediction using the heart-disease dataset, comprising 303 samples with 14
features. The methodology involves data preprocessing, model training, and
evaluation using three classifiers: Logistic Regression, K-Nearest Neighbors
(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and
RandomizedSearchCV was employed to enhance model performance. The Random Forest
classifier outperformed other models, achieving an accuracy of 91% and an
F1-score of 0.89. Evaluation metrics, including precision, recall, and
confusion matrix, revealed balanced performance across classes. The proposed
model demonstrates strong potential for aiding clinical decision-making by
effectively predicting heart disease. Limitations such as dataset size and
generalizability underscore the need for future studies using larger and more
diverse datasets. This work highlights the utility of machine learning in
healthcare, offering insights for further advancements in predictive
diagnostics.

</details>


### [79] [AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom Domain Large Model](https://arxiv.org/abs/2505.10003)
*Tianyu Jiao,Zhuoran Xiao,Yihang Huang,Chenhui Ye,Yijia Feng,Liyu Cai,Jiang Chang,Fangkun Liu,Yin Xu,Dazhi He,Yunfeng Guan,Wenjun Zhang*

Main category: cs.LG

TL;DR: 6G多模态通用模型AI2MMUM通过LLM骨架和微调方法，灵活处理多种物理层任务，实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 为未来无线系统设计能处理多模态数据并执行多样化空口任务的通用模型，提升任务适应性和性能。

Method: 结合通信多模态对齐和电信大语言模型（LLM），采用任务感知的微调方法，通过固定任务关键词和可学习前缀提示增强任务适应性，使用冻结的无线模态编码器和适配器层桥接模态，轻量级任务头输出目标。

Result: 在WAIR-D和DeepMIMO数据集上，AI2MMUM在五种代表性物理环境/无线信道下游任务中实现了SOTA性能。

Conclusion: AI2MMUM通过多模态融合和任务感知设计，为6G空口任务提供了高效、灵活的解决方案。

Abstract: Designing a 6G-oriented universal model capable of processing multi-modal
data and executing diverse air interface tasks has emerged as a common goal in
future wireless systems. Building on our prior work in communication
multi-modal alignment and telecom large language model (LLM), we propose a
scalable, task-aware artificial intelligence-air interface multi-modal
universal model (AI2MMUM), which flexibility and effectively perform various
physical layer tasks according to subtle task instructions. The LLM backbone
provides robust contextual comprehension and generalization capabilities, while
a fine-tuning approach is adopted to incorporate domain-specific knowledge. To
enhance task adaptability, task instructions consist of fixed task keywords and
learnable, implicit prefix prompts. Frozen radio modality encoders extract
universal representations and adapter layers subsequently bridge radio and
language modalities. Moreover, lightweight task-specific heads are designed to
directly output task objectives. Comprehensive evaluations demonstrate that
AI2MMUM achieves SOTA performance across five representative physical
environment/wireless channel-based downstream tasks using the WAIR-D and
DeepMIMO datasets.

</details>


### [80] [Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning](https://arxiv.org/abs/2505.10007)
*Zijun Chen,Shengbo Wang,Nian Si*

Main category: cs.LG

TL;DR: 该论文提出了两种分布鲁棒（DR）平均奖励强化学习算法，通过引入锚定状态或转换为DR折扣MDP，实现了近似最优样本复杂度，适用于需要长期稳定性能的应用。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中（如机器人、运筹学和医疗保健），需要稳定的长期性能。因此，研究分布鲁棒平均奖励强化学习具有重要意义。

Method: 提出了两种算法：1) 将问题转换为DR折扣MDP；2) 引入锚定状态（Anchored DR Average-Reward MDP）以稳定控制转移核。假设名义MDP均匀遍历，证明了算法的样本复杂度。

Result: 在KL和f_k-散度不确定性集下，两种算法均实现了样本复杂度为$	ilde{O}(|f{S}||f{A}|t_{	ext{mix}}^2 m{	ext{epsilon}}^{-2})$的收敛，首次为DR平均奖励强化学习提供了有限样本保证。

Conclusion: 该研究首次解决了DR平均奖励强化学习的有限样本收敛问题，并通过实验验证了算法的收敛速率。

Abstract: Motivated by practical applications where stable long-term performance is
critical-such as robotics, operations research, and healthcare-we study the
problem of distributionally robust (DR) average-reward reinforcement learning.
We propose two algorithms that achieve near-optimal sample complexity. The
first reduces the problem to a DR discounted Markov decision process (MDP),
while the second, Anchored DR Average-Reward MDP, introduces an anchoring state
to stabilize the controlled transition kernels within the uncertainty set.
Assuming the nominal MDP is uniformly ergodic, we prove that both algorithms
attain a sample complexity of $\widetilde{O}\left(|\mathbf{S}||\mathbf{A}|
t_{\mathrm{mix}}^2\varepsilon^{-2}\right)$ for estimating the optimal policy as
well as the robust average reward under KL and $f_k$-divergence-based
uncertainty sets, provided the uncertainty radius is sufficiently small. Here,
$\varepsilon$ is the target accuracy, $|\mathbf{S}|$ and $|\mathbf{A}|$ denote
the sizes of the state and action spaces, and $t_{\mathrm{mix}}$ is the mixing
time of the nominal MDP. This represents the first finite-sample convergence
guarantee for DR average-reward reinforcement learning. We further validate the
convergence rates of our algorithms through numerical experiments.

</details>


### [81] [ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts](https://arxiv.org/abs/2505.10010)
*Jing-Cheng Pang,Kaiyuan Li,Yidi Wang,Si-Hang Yang,Shengyi Jiang,Yang Yu*

Main category: cs.LG

TL;DR: 论文介绍了ImagineBench，首个用于评估结合真实数据和LLM生成虚拟数据的离线RL算法的基准，发现现有算法在新任务上表现不佳，强调了算法改进的必要性。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习对大量真实交互数据的依赖问题，利用大语言模型生成虚拟经验，但缺乏标准化评估基准。

Method: 提出ImagineBench基准，包含真实和LLM生成的虚拟数据、多领域环境及自然语言任务指令，并评估现有离线RL算法。

Result: 现有算法在新任务上成功率仅为35.44%，远低于真实数据的64.37%，凸显需改进算法以更好利用虚拟数据。

Conclusion: 需算法创新以优化虚拟数据利用，未来研究方向包括快速在线适应、持续学习及多模态任务扩展。

Abstract: A central challenge in reinforcement learning (RL) is its dependence on
extensive real-world interaction data to learn task-specific policies. While
recent work demonstrates that large language models (LLMs) can mitigate this
limitation by generating synthetic experience (noted as imaginary rollouts) for
mastering novel tasks, progress in this emerging field is hindered due to the
lack of a standard benchmark. To bridge this gap, we introduce ImagineBench,
the first comprehensive benchmark for evaluating offline RL algorithms that
leverage both real rollouts and LLM-imaginary rollouts. The key features of
ImagineBench include: (1) datasets comprising environment-collected and
LLM-imaginary rollouts; (2) diverse domains of environments covering
locomotion, robotic manipulation, and navigation tasks; and (3) natural
language task instructions with varying complexity levels to facilitate
language-conditioned policy learning. Through systematic evaluation of
state-of-the-art offline RL algorithms, we observe that simply applying
existing offline RL algorithms leads to suboptimal performance on unseen tasks,
achieving 35.44% success rate in hard tasks in contrast to 64.37% of method
training on real rollouts for hard tasks. This result highlights the need for
algorithm advancements to better leverage LLM-imaginary rollouts. Additionally,
we identify key opportunities for future research: including better utilization
of imaginary rollouts, fast online adaptation and continual learning, and
extension to multi-modal tasks. Our code is publicly available at
https://github.com/LAMDA-RL/ImagineBench.

</details>


### [82] [Optimal normalization in quantum-classical hybrid models for anti-cancer drug response prediction](https://arxiv.org/abs/2505.10037)
*Takafumi Ito,Lysenko Artem,Tatsuhiko Tsunoda*

Main category: cs.LG

TL;DR: 该论文提出了一种基于量子-经典混合机器学习（QHML）的方法，用于抗癌药物反应预测，并通过改进的数据编码策略（采用一种基于渐变tanh的归一化函数）提高了模型的稳定性。实验证明了QHML在数据优化归一化下的优越性。


<details>
  <summary>Details</summary>
Motivation: 抗癌药物反应预测通常面临样本量小的问题，量子-经典混合模型（QHML）在此类任务中表现出色，但对数据编码极为敏感。为了提升模型稳定性，作者提出了一种新的归一化方法。

Method: 提出了一种基于渐变tanh的归一化函数，优化了神经网络与量子电路接口的数据编码。通过对比经典深度学习模型与多个QHML模型在基因表达和药物反应数据上的表现，验证了该方法的有效性。

Result: 实验结果显示，在数据优化归一化的情况下，QHML模型的预测性能优于经典模型。

Conclusion: 该研究为利用量子计算机进行生物医学数据分析提供了新的可能性，证明了优化数据编码对提升QHML模型稳定性和性能的重要性。

Abstract: Quantum-classical Hybrid Machine Learning (QHML) models are recognized for
their robust performance and high generalization ability even for relatively
small datasets. These qualities offer unique advantages for anti-cancer drug
response prediction, where the number of available samples is typically small.
However, such hybrid models appear to be very sensitive to the data encoding
used at the interface of a neural network and a quantum circuit, with
suboptimal choices leading to stability issues. To address this problem, we
propose a novel strategy that uses a normalization function based on a
moderated gradient version of the $\tanh$. This method transforms the outputs
of the neural networks without concentrating them at the extreme value ranges.
Our idea was evaluated on a dataset of gene expression and drug response
measurements for various cancer cell lines, where we compared the prediction
performance of a classical deep learning model and several QHML models. These
results confirmed that QHML performed better than the classical models when
data was optimally normalized. This study opens up new possibilities for
biomedical data analysis using quantum computers.

</details>


### [83] [Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER Gates](https://arxiv.org/abs/2505.10039)
*Hang Chen,Jiaying Zhu,Xinyu Yang,Wenya Wang*

Main category: cs.LG

TL;DR: 提出了一个结合噪声和去噪声干预的框架，用于完整识别电路中的逻辑门，确保忠实性、完整性和稀疏性，并揭示了逻辑门在语言模型中的基本特性。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法无法保证完整性，导致关键机制遗漏和电路结果不一致。为系统性解决这一问题，引入了三种逻辑门（AND、OR、ADDER），并提出了识别逻辑门的框架。

Method: 将电路分解为逻辑门的组合，并提出一个结合噪声和去噪声干预的框架，可集成到现有方法中，以完全识别逻辑门。

Result: 实验验证了框架在恢复电路忠实性、完整性和稀疏性方面的能力，并揭示了逻辑门在语言模型中的比例、贡献和行为特性。

Conclusion: 提出的框架不仅提升了电路发现的完整性，还为理解逻辑门在语言模型中的作用提供了新视角。

Abstract: Circuit discovery has gradually become one of the prominent methods for
mechanistic interpretability, and research on circuit completeness has also
garnered increasing attention. Methods of circuit discovery that do not
guarantee completeness not only result in circuits that are not fixed across
different runs but also cause key mechanisms to be omitted. The nature of
incompleteness arises from the presence of OR gates within the circuit, which
are often only partially detected in standard circuit discovery methods. To
this end, we systematically introduce three types of logic gates: AND, OR, and
ADDER gates, and decompose the circuit into combinations of these logical
gates. Through the concept of these gates, we derive the minimum requirements
necessary to achieve faithfulness and completeness. Furthermore, we propose a
framework that combines noising-based and denoising-based interventions, which
can be easily integrated into existing circuit discovery methods without
significantly increasing computational complexity. This framework is capable of
fully identifying the logic gates and distinguishing them within the circuit.
In addition to the extensive experimental validation of the framework's ability
to restore the faithfulness, completeness, and sparsity of circuits, using this
framework, we uncover fundamental properties of the three logic gates, such as
their proportions and contributions to the output, and explore how they behave
among the functionalities of language models.

</details>


### [84] [Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning](https://arxiv.org/abs/2505.10040)
*Lei Song,Jiaxing Li,Shihan Guan,Youyong Kong*

Main category: cs.LG

TL;DR: 论文提出了一种名为IPAL的新方法，通过结合原型对比学习（PCL）和图结构信息，解决了图神经网络（GNN）在持续学习中的灾难性遗忘问题，并在四个基准数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在持续学习中存在灾难性遗忘问题，现有的基于历史样本复现的方法（如Prototype Replay）虽然能缓解这一问题，但面临内存爆炸和隐私泄露的挑战。本文旨在开发一种无需样本复现的新方法，解决特征漂移问题并提升模型性能。

Method: 提出Instance-Prototype Affinity Learning (IPAL)，结合Prototype Contrastive Learning (PCL)和图结构信息，设计了Topology-Integrated Gaussian Prototypes (TIGP)引导特征分布，并通过Instance-Prototype Affinity Distillation (IPAD)保护任务记忆，同时引入Decision Boundary Perception (DBP)机制增强类间区分性。

Result: 在四个节点分类基准数据集上的实验表明，IPAL在可塑性和稳定性之间达到了更好的平衡，性能优于现有方法。

Conclusion: IPAL通过结合原型学习和图结构信息，有效缓解了持续学习中的特征漂移和灾难性遗忘问题，为图神经网络的持续学习提供了新思路。

Abstract: Graph Neural Networks (GNN) endure catastrophic forgetting, undermining their
capacity to preserve previously acquired knowledge amid the assimilation of
novel information. Rehearsal-based techniques revisit historical examples,
adopted as a principal strategy to alleviate this phenomenon. However, memory
explosion and privacy infringements impose significant constraints on their
utility. Non-Exemplar methods circumvent the prior issues through Prototype
Replay (PR), yet feature drift presents new challenges. In this paper, our
empirical findings reveal that Prototype Contrastive Learning (PCL) exhibits
less pronounced drift than conventional PR. Drawing upon PCL, we propose
Instance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar
Continual Graph Learning (NECGL). Exploiting graph structural information, we
formulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature
distributions towards high-impact nodes to augment the model's capacity for
assimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)
safeguards task memory by regularizing discontinuities in class relationships.
Moreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,
fostering greater inter-class discriminability. Evaluations on four node
classification benchmark datasets demonstrate that our method outperforms
existing state-of-the-art methods, achieving a better trade-off between
plasticity and stability.

</details>


### [85] [Financial Fraud Detection Using Explainable AI and Stacking Ensemble Methods](https://arxiv.org/abs/2505.10050)
*Fahad Almalki,Mehedi Masud*

Main category: cs.LG

TL;DR: 结合XGBoost、LightGBM和CatBoost的堆叠集成模型，辅以SHAP、LIME、PDP和PFI等XAI技术，提出了一种高精度且可解释的金融欺诈检测框架，在IEEE-CIS数据集上达到99%准确率和0.99 AUC-ROC。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型注重预测精度但缺乏可解释性，难以满足监管要求和赢得利益相关者信任。本文旨在通过可解释AI技术（XAI）提升欺诈检测模型的透明度和可信度。

Method: 提出基于XGBoost、LightGBM和CatBoost的堆叠集成模型，并采用SHAP进行特征选择，结合LIME、PDP和PFI对模型预测进行解释。

Result: 在IEEE-CIS欺诈检测数据集（含59万笔交易）上，模型准确率达99%，AUC-ROC为0.99，优于现有方法。

Conclusion: 高精度与可解释性可兼顾，该框架为金融欺诈检测提供了更可信的解决方案。

Abstract: Traditional machine learning models often prioritize predictive accuracy,
often at the expense of model transparency and interpretability. The lack of
transparency makes it difficult for organizations to comply with regulatory
requirements and gain stakeholders trust. In this research, we propose a fraud
detection framework that combines a stacking ensemble of well-known gradient
boosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable
artificial intelligence (XAI) techniques are used to enhance the transparency
and interpretability of the model's decisions. We used SHAP (SHapley Additive
Explanations) for feature selection to identify the most important features.
Further efforts were made to explain the model's predictions using Local
Interpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots
(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection
dataset, which includes more than 590,000 real transaction records, was used to
evaluate the proposed model. The model achieved a high performance with an
accuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent
related approaches. These results indicate that combining high prediction
accuracy with transparent interpretability is possible and could lead to a more
ethical and trustworthy solution in financial fraud detection.

</details>


### [86] [JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation and Scene Segmentation](https://arxiv.org/abs/2505.10057)
*Tiancong Cheng,Ying Zhang,Yuxuan Liang,Roger Zimmermann,Zhiwen Yu,Bin Guo*

Main category: cs.LG

TL;DR: 该论文提出了一种自适应蒸馏方法，用于联合建模深度估计和场景分割任务，通过动态调整各教师模型的知识传递量，并设计基于知识轨迹的蒸馏损失来优化学习过程，实验表明该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统中深度估计和场景分割的联合建模可以降低存储和训练成本，但现有方法在知识传递上静态且容易导致梯度方向错误，因此需要改进。

Method: 提出自适应蒸馏方法，动态调整教师模型的知识传递量，并引入知识轨迹记录关键学习信息，设计轨迹蒸馏损失以优化学习方向。

Result: 在Cityscapes和NYU-v2等基准数据集上，该方法相比现有技术有明显提升。

Conclusion: 自适应蒸馏方法和知识轨迹设计有效优化了多任务联合建模的性能，为智能交通系统提供了更高效的解决方案。

Abstract: Depth estimation and scene segmentation are two important tasks in
intelligent transportation systems. A joint modeling of these two tasks will
reduce the requirement for both the storage and training efforts. This work
explores how the multi-task distillation could be used to improve such unified
modeling. While existing solutions transfer multiple teachers' knowledge in a
static way, we propose a self-adaptive distillation method that can dynamically
adjust the knowledge amount from each teacher according to the student's
current learning ability. Furthermore, as multiple teachers exist, the
student's gradient update direction in the distillation is more prone to be
erroneous where knowledge forgetting may occur. To avoid this, we propose a
knowledge trajectory to record the most essential information that a model has
learnt in the past, based on which a trajectory-based distillation loss is
designed to guide the student to follow the learning curve similarly in a
cost-effective way. We evaluate our method on multiple benchmarking datasets
including Cityscapes and NYU-v2. Compared to the state-of-the-art solutions,
our method achieves a clearly improvement. The code is provided in the
supplementary materials.

</details>


### [87] [ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data](https://arxiv.org/abs/2505.10083)
*Chengsen Wang,Qi Qi,Zhongwen Rao,Lujia Pan,Jingyu Wang,Jianxin Liao*

Main category: cs.LG

TL;DR: 该论文提出了一种名为ChronoSteer的多模态时间序列预测框架，结合了大型语言模型（LLM）和时间序列基础模型（TSFM），通过文本修订指令指导预测，提升了25.7%的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统预测方法仅依赖单模态时间序列数据，无法充分利用文本信息，而LLM和TSFM分别在文本推理和时间建模上表现优异，因此结合两者构建多模态模型成为研究重点。

Method: 采用解耦框架：LLM将文本事件转化为修订指令，用于指导TSFM输出；通过合成数据设计两阶段训练策略，并构建高质量多模态基准数据集。

Result: ChronoSteer在合成数据上训练后，预测准确率比单模态基线提升25.7%，比现有多模态方法提升22.5%。

Conclusion: ChronoSteer成功融合LLM与TSFM，通过文本指令增强时间序列预测，解决了跨模态数据稀缺问题，并在准确性上取得显著提升。

Abstract: Conventional forecasting methods rely on unimodal time series data, limiting
their ability to exploit rich textual information. Recently, large language
models (LLMs) and time series foundation models (TSFMs) have demonstrated
powerful capability in textual reasoning and temporal modeling, respectively.
Integrating the strengths of both to construct a multimodal model that
concurrently leverages both temporal and textual information for future
inference has emerged as a critical research challenge. To address the scarcity
of event-series paired data, we propose a decoupled framework: an LLM is
employed to transform textual events into revision instructions, which are then
used to steer the output of TSFM. To implement this framework, we introduce
ChronoSteer, a multimodal TSFM that can be steered through textual revision
instructions, effectively bridging LLM and TSFM. Moreover, to mitigate the
shortage of cross-modal instruction-series paired data, we devise a two-stage
training strategy based on synthetic data. In addition, we also construct a
high-quality multimodal time series forecasting benchmark to address the
information leakage concerns during evaluation. After integrating with an LLM,
ChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%
improvement in prediction accuracy compared to the unimodal backbone and a
22.5% gain over the previous state-of-the-art multimodal method.

</details>


### [88] [Learning Virtual Machine Scheduling in Cloud Computing through Language Agents](https://arxiv.org/abs/2505.10117)
*JieHao Wu,Ziwei Wang,Junjie Sheng,Wenhao Li,Xiangfei Wang,Jun Luo*

Main category: cs.LG

TL;DR: 本文提出了一种名为MiCo的分层语言代理框架，用于解决云计算中虚拟机调度的动态多维装箱问题。通过结合大语言模型（LLM）驱动的启发式设计，MiCo在大规模场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法难以适应实时变化，启发式方法策略僵化，学习型方法缺乏通用性和可解释性。MiCo旨在弥补这些不足。

Method: 将问题建模为带有选项的半马尔可夫决策过程（SMDP-Option），采用两阶段架构（选项挖掘器和选项组合器），利用LLM生成策略。

Result: 在超过1万台虚拟机的大规模场景中，MiCo实现了96.9%的竞争比，且在非稳态请求流和多样化配置下表现优异。

Conclusion: MiCo在复杂和大规模云环境中具有高效性和适应性。

Abstract: In cloud services, virtual machine (VM) scheduling is a typical Online
Dynamic Multidimensional Bin Packing (ODMBP) problem, characterized by
large-scale complexity and fluctuating demands. Traditional optimization
methods struggle to adapt to real-time changes, domain-expert-designed
heuristic approaches suffer from rigid strategies, and existing learning-based
methods often lack generalizability and interpretability. To address these
limitations, this paper proposes a hierarchical language agent framework named
MiCo, which provides a large language model (LLM)-driven heuristic design
paradigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov
Decision Process with Options (SMDP-Option), enabling dynamic scheduling
through a two-stage architecture, i.e., Option Miner and Option Composer.
Option Miner utilizes LLMs to discover diverse and useful non-context-aware
strategies by interacting with constructed environments. Option Composer
employs LLMs to discover a composing strategy that integrates the
non-context-aware strategies with the contextual ones. Extensive experiments on
real-world enterprise datasets demonstrate that MiCo achieves a 96.9\%
competitive ratio in large-scale scenarios involving more than 10,000 virtual
machines. It maintains high performance even under nonstationary request flows
and diverse configurations, thus validating its effectiveness in complex and
large-scale cloud environments.

</details>


### [89] [All You Need Is Synthetic Task Augmentation](https://arxiv.org/abs/2505.10120)
*Guillaume Godin*

Main category: cs.LG

TL;DR: 该论文提出一种联合训练策略，通过结合稀疏多任务分子属性实验目标和XGBoost模型生成的合成目标，显著提升了分子属性预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决将规则模型（如随机森林）注入可微分神经网络框架的挑战，避免传统方法需要大量预训练或额外技术的问题。

Method: 使用单一图Transformer神经网络联合训练实验目标和XGBoost模型生成的合成目标，后者作为独立辅助任务。

Result: 在19项分子属性预测任务中均表现显著提升，其中16项优于XGBoost单任务学习。

Conclusion: 合成任务增强能在无需特征注入或预训练的情况下，有效提升神经网络在多任务分子属性预测中的性能。

Abstract: Injecting rule-based models like Random Forests into differentiable neural
network frameworks remains an open challenge in machine learning. Recent
advancements have demonstrated that pretrained models can generate efficient
molecular embeddings. However, these approaches often require extensive
pretraining and additional techniques, such as incorporating posterior
probabilities, to boost performance. In our study, we propose a novel strategy
that jointly trains a single Graph Transformer neural network on both sparse
multitask molecular property experimental targets and synthetic targets derived
from XGBoost models trained on Osmordred molecular descriptors. These synthetic
tasks serve as independent auxiliary tasks. Our results show consistent and
significant performance improvement across all 19 molecular property prediction
tasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms
the XGBoost single-task learner. This demonstrates that synthetic task
augmentation is an effective method for enhancing neural model performance in
multitask molecular property prediction without the need for feature injection
or pretraining.

</details>


### [90] [Enhancing the Performance of Global Model by Improving the Adaptability of Local Models in Federated Learning](https://arxiv.org/abs/2505.10125)
*Wujun Zhou,Shu Ding,ZeLin Li,Wei Wang*

Main category: cs.LG

TL;DR: 该论文提出通过提升本地模型的适应性（即在客户端数据分布上的平均表现）来优化联邦学习的全局模型性能。通过理论分析和实验验证，该方法显著提升了本地模型的适应性，并超越了基线方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中，客户端数据分布异构性和隐私保护导致本地模型难以共同训练出一个高性能的全局模型。因此，作者希望通过提升本地模型的适应性来解决这一问题。

Method: 作者首先分析了具有良好适应性的本地模型的特性，并将这些特性形式化为本地训练目标及约束条件，随后提出了一种可行的本地模型训练方法。

Result: 在多个联邦学习基准测试中，该方法显著提升了本地模型的适应性，并实现了性能优于基线方法的全局模型。

Conclusion: 通过提升本地模型的适应性，可以有效优化联邦学习中的全局模型性能，且实验证明了方法的有效性。

Abstract: Federated learning enables the clients to collaboratively train a global
model, which is aggregated from local models. Due to the heterogeneous data
distributions over clients and data privacy in federated learning, it is
difficult to train local models to achieve a well-performed global model. In
this paper, we introduce the adaptability of local models, i.e., the average
performance of local models on data distributions over clients, and enhance the
performance of the global model by improving the adaptability of local models.
Since each client does not know the data distributions over other clients, the
adaptability of the local model cannot be directly optimized. First, we provide
the property of an appropriate local model which has good adaptability on the
data distributions over clients. Then, we formalize the property into the local
training objective with a constraint and propose a feasible solution to train
the local model. Extensive experiments on federated learning benchmarks
demonstrate that our method significantly improves the adaptability of local
models and achieves a well-performed global model that consistently outperforms
the baseline methods.

</details>


### [91] [Robust Federated Learning on Edge Devices with Domain Heterogeneity](https://arxiv.org/abs/2505.10128)
*Huy Q. Le,Latif U. Khan,Choong Seon Hong*

Main category: cs.LG

TL;DR: 联邦学习（FL）面临统计异质性（特别是域异质性）的挑战，影响全局模型收敛。本文提出FedAPC框架，通过原型增强提升全局模型的泛化能力，结合对比学习增强特征多样性和模型鲁棒性。实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决FL中域异质性导致的全局模型收敛问题，提升模型泛化能力。

Method: 提出FedAPC框架，利用增强数据的均值特征生成原型，通过对比学习对齐局部特征与全局原型，减少对特定域的过拟合。

Result: 在Office-10和Digits数据集上超越现有方法，验证了框架的有效性。

Conclusion: FedAPC通过原型增强和对比学习有效缓解了域异质性，提升了FL模型的性能和鲁棒性。

Abstract: Federated Learning (FL) allows collaborative training while ensuring data
privacy across distributed edge devices, making it a popular solution for
privacy-sensitive applications. However, FL faces significant challenges due to
statistical heterogeneity, particularly domain heterogeneity, which impedes the
global mode's convergence. In this study, we introduce a new framework to
address this challenge by improving the generalization ability of the FL global
model under domain heterogeneity, using prototype augmentation. Specifically,
we introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a
prototype-based FL framework designed to enhance feature diversity and model
robustness. FedAPC leverages prototypes derived from the mean features of
augmented data to capture richer representations. By aligning local features
with global prototypes, we enable the model to learn meaningful semantic
features while reducing overfitting to any specific domain. Experimental
results on the Office-10 and Digits datasets illustrate that our framework
outperforms SOTA baselines, demonstrating superior performance.

</details>


### [92] [Near Optimal Best Arm Identification for Clustered Bandits](https://arxiv.org/abs/2505.10147)
*Yash,Nikhil Karamchandani,Avishek Ghosh*

Main category: cs.LG

TL;DR: 论文研究了多智能体多臂老虎机问题中的最优臂识别，提出了两种算法Cl-BAI和BAI-Cl，分别通过先聚类后识别和先识别后聚类的方法，在$δ$-PC框架下高效解决样本和通信开销问题。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体多臂老虎机中智能体与老虎机映射未知时的最优臂识别问题，同时最小化样本复杂度和通信开销。

Method: 提出了两种算法：1. Cl-BAI（先聚类后识别）和2. BAI-Cl（先识别后聚类），均基于连续消除框架。

Result: 两种算法均具备$δ$-PC保证，样本复杂度界限明确，且当聚类数量$M$较小时，BAI-Cl的变种显示最小极大最优性。实验证实了算法在样本和通信效率上的优越性。

Conclusion: 提出的算法在多智能体多臂老虎机问题中表现高效，尤其适用于聚类数量远小于智能体数量的场景。

Abstract: This work investigates the problem of best arm identification for multi-agent
multi-armed bandits. We consider $N$ agents grouped into $M$ clusters, where
each cluster solves a stochastic bandit problem. The mapping between agents and
bandits is a priori unknown. Each bandit is associated with $K$ arms, and the
goal is to identify the best arm for each agent under a $\delta$-probably
correct ($\delta$-PC) framework, while minimizing sample complexity and
communication overhead.
  We propose two novel algorithms: Clustering then Best Arm Identification
(Cl-BAI) and Best Arm Identification then Clustering (BAI-Cl). Cl-BAI uses a
two-phase approach that first clusters agents based on the bandit problems they
are learning, followed by identifying the best arm for each cluster. BAI-Cl
reverses the sequence by identifying the best arms first and then clustering
agents accordingly. Both algorithms leverage the successive elimination
framework to ensure computational efficiency and high accuracy.
  We establish $\delta$-PC guarantees for both methods, derive bounds on their
sample complexity, and provide a lower bound for this problem class. Moreover,
when $M$ is small (a constant), we show that the sample complexity of a variant
of BAI-Cl is minimax optimal in an order-wise sense. Experiments on synthetic
and real-world datasets (MovieLens, Yelp) demonstrate the superior performance
of the proposed algorithms in terms of sample and communication efficiency,
particularly in settings where $M \ll N$.

</details>


### [93] [QuXAI: Explainers for Hybrid Quantum Machine Learning Models](https://arxiv.org/abs/2505.10167)
*Saikat Barua,Mostafizur Rahman,Shehenaz Khaled,Md Jafor Sadek,Rafiul Islam,Shahnewaz Siddique*

Main category: cs.LG

TL;DR: 该论文提出QuXAI框架，基于Q-MEDLEY解释器，用于提升混合量子-经典机器学习（HQML）模型的透明度和可靠性，填补了现有解释方法的不足。


<details>
  <summary>Details</summary>
Motivation: 由于HQML模型的复杂性导致其行为像黑箱，缺乏透明度和可靠性，因此需要一种针对量化特征编码和经典学习的解释方法。

Method: 论文提出QuXAI框架，结合量子特征映射和Q-MEDLEY解释器，通过特征推断和可视化来揭示HQML模型中的关键因素。

Result: 实验表明Q-MEDLEY能有效识别HQML模型中的重要特征，分离噪声，并在经典验证环境中表现优异。

Conclusion: 该研究为HQML模型的可解释性和可靠性提供了新途径，有助于增强对量子增强AI技术的信任和负责任使用。

Abstract: The emergence of hybrid quantum-classical machine learning (HQML) models
opens new horizons of computational intelligence but their fundamental
complexity frequently leads to black box behavior that undermines transparency
and reliability in their application. Although XAI for quantum systems still in
its infancy, a major research gap is evident in robust global and local
explainability approaches that are designed for HQML architectures that employ
quantized feature encoding followed by classical learning. The gap is the focus
of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an
explainer for explaining feature importance in these hybrid systems. Our model
entails the creation of HQML models incorporating quantum feature maps, the use
of Q-MEDLEY, which combines feature based inferences, preserving the quantum
transformation stage and visualizing the resulting attributions. Our result
shows that Q-MEDLEY delineates influential classical aspects in HQML models, as
well as separates their noise, and competes well against established XAI
techniques in classical validation settings. Ablation studies more
significantly expose the virtues of the composite structure used in Q-MEDLEY.
The implications of this work are critically important, as it provides a route
to improve the interpretability and reliability of HQML models, thus promoting
greater confidence and being able to engage in safer and more responsible use
of quantum-enhanced AI technology.

</details>


### [94] [Does Scaling Law Apply in Time Series Forecasting?](https://arxiv.org/abs/2505.10172)
*Zeyan Li,Libing Chen,Yin Tang*

Main category: cs.LG

TL;DR: 提出超轻量级时间序列预测模型Alinear，仅需k级参数即可达到与大模型相当的性能，挑战了模型规模越大性能越好的观念。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测领域模型规模快速扩张，但性能提升往往伴随参数数量指数增长。论文质疑这种规模扩增的必要性，并提出高效轻量化的新模型设计。

Method: 采用基于预测长度的动态分解机制（horizon-aware adaptive decomposition）和渐进频率衰减策略（progressive frequency attenuation），避免注意力机制的计算开销。

Result: 在7个基准数据集上，Alinear仅使用不到1%的参数即可超越大模型，且在短时/超长预测中均保持高精度。同时提出新指标（parameter-aware metric）验证其高效性。

Conclusion: 模型组件的相对重要性取决于数据特性而非固定模式，验证了自适应设计的必要性，为时间序列建模的效率优化提供新思路。

Abstract: Rapid expansion of model size has emerged as a key challenge in time series
forecasting. From early Transformer with tens of megabytes to recent
architectures like TimesNet with thousands of megabytes, performance gains have
often come at the cost of exponentially increasing parameter counts. But is
this scaling truly necessary? To question the applicability of the scaling law
in time series forecasting, we propose Alinear, an ultra-lightweight
forecasting model that achieves competitive performance using only k-level
parameters. We introduce a horizon-aware adaptive decomposition mechanism that
dynamically rebalances component emphasis across different forecast lengths,
alongside a progressive frequency attenuation strategy that achieves stable
prediction in various forecasting horizons without incurring the computational
overhead of attention mechanisms. Extensive experiments on seven benchmark
datasets demonstrate that Alinear consistently outperforms large-scale models
while using less than 1% of their parameters, maintaining strong accuracy
across both short and ultra-long forecasting horizons. Moreover, to more fairly
evaluate model efficiency, we propose a new parameter-aware evaluation metric
that highlights the superiority of ALinear under constrained model budgets. Our
analysis reveals that the relative importance of trend and seasonal components
varies depending on data characteristics rather than following a fixed pattern,
validating the necessity of our adaptive design. This work challenges the
prevailing belief that larger models are inherently better and suggests a
paradigm shift toward more efficient time series modeling.

</details>


### [95] [Defect Detection in Photolithographic Patterns Using Deep Learning Models Trained on Synthetic Data](https://arxiv.org/abs/2505.10192)
*Prashant P. Shinde,Priyadarshini P. Pai,Shashishekar P. Adiga,K. Subramanya Mayya,Yongbeom Seo,Myungsoo Hwang,Heeyoung Go,Changmin Park*

Main category: cs.LG

TL;DR: 论文提出了一种通过人工生成标注缺陷的SEM图像来解决半导体制造中EUV图案小缺陷检测数据不足问题的方法，并使用YOLOv8等模型验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于半导体制造中EUV图案的缺陷尺寸极小且缺乏标注数据，导致深度学习模型难以应用于实际生产线。

Method: 人工生成带标注缺陷的SEM图像，并利用YOLOv8、EfficientNet和SSD等模型进行缺陷检测性能测试。

Result: YOLOv8表现最佳（mAP 96%），能可靠检测比线宽更小的缺陷。在真实SEM数据测试中，对Bridge和Break缺陷的检测率分别为84.6%和78.3%。

Conclusion: 合成数据可替代真实数据用于开发鲁棒的机器学习模型。

Abstract: In the photolithographic process vital to semiconductor manufacturing,
various types of defects appear during EUV pattering. Due to ever-shrinking
pattern size, these defects are extremely small and cause false or missed
detection during inspection. Specifically, the lack of defect-annotated quality
data with good representation of smaller defects has prohibited deployment of
deep learning based defect detection models in fabrication lines. To resolve
the problem of data unavailability, we artificially generate scanning electron
microscopy (SEM) images of line patterns with known distribution of defects and
autonomously annotate them. We then employ state-of-the-art object detection
models to investigate defect detection performance as a function of defect
size, much smaller than the pitch width. We find that the real-time object
detector YOLOv8 has the best mean average precision of 96% as compared to
EfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We
report the smallest defect size that can be detected reliably. When tested on
real SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and
78.3% of Break defects across all relevant instances. These promising results
suggest that synthetic data can be used as an alternative to real-world data in
order to develop robust machine-learning models.

</details>


### [96] [ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention](https://arxiv.org/abs/2505.10222)
*Jintian Shao,Hongyi Huang,Jiayi Wu,Beiwen Zhang,ZhiYu Wu,You Shan,MingKai Zheng*

Main category: cs.LG

TL;DR: 论文提出了ComplexFormer，采用复杂的多头注意力机制（CMHA），允许每个头独立地在复平面上统一建模语义和位置差异，通过旋转和缩放表示交互。实验表明，其在语言建模、文本生成等任务上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型难以同时有效整合位置信息和多头注意力的灵活性，限制了其表示能力。

Method: 引入CMHA机制，包括每头的欧拉变换将查询/键投影为复数向量，以及自适应差分旋转机制，动态整合语义和位置差异。

Result: ComplexFormer在多项任务上表现优异，生成困惑度更低，长上下文一致性更好，且参数效率高。

Conclusion: ComplexFormer提供了一种更具表达力和适应性的注意力机制，显著提升了模型性能。

Abstract: Transformer models rely on self-attention to capture token dependencies but
face challenges in effectively integrating positional information while
allowing multi-head attention (MHA) flexibility. Prior methods often model
semantic and positional differences disparately or apply uniform positional
adjustments across heads, potentially limiting representational capacity. This
paper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.
CMHA empowers each head to independently model semantic and positional
differences unified within the complex plane, representing interactions as
rotations and scaling. ComplexFormer incorporates two key improvements: (1) a
per-head Euler transformation, converting real-valued query/key projections
into polar-form complex vectors for head-specific complex subspace operation;
and (2) a per-head adaptive differential rotation mechanism,
exp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct
strategies for integrating semantic angle differences (ASmn,i) with relative
positional encodings (Delta(Pmn),i). Extensive experiments on language
modeling, text generation, code generation, and mathematical reasoning show
ComplexFormer achieves superior performance, significantly lower generation
perplexity , and improved long-context coherence compared to strong baselines
like RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,
offering a more expressive, adaptable attention mechanism.

</details>


### [97] [A multi-head deep fusion model for recognition of cattle foraging events using sound and movement signals](https://arxiv.org/abs/2505.10198)
*Mariano Ferrero,José Omar Chelotti,Luciano Sebastián Martinez-Rau,Leandro Vignolo,Martín Pires,Julio Ricardo Galli,Leonardo Luis Giovanini,Hugo Leonardo Rufiner*

Main category: cs.LG

TL;DR: 该研究提出了一种基于深度神经网络的模型，结合声学和惯性信号，通过特征级融合自动提取特征，提高了放牧牛摄食行为监测的精度，F1分数达到0.802，优于现有方法14%。


<details>
  <summary>Details</summary>
Motivation: 为了高效管理牛群和优化资源利用，需要自动识别牛的摄食行为。现有传感器方法（如加速度计、麦克风）存在局限，多传感器信号融合的潜力尚未充分挖掘。

Method: 研究设计了一个包含卷积层、循环层和密集层的深度神经网络模型，通过不同级别的信息融合（特征级、数据级、决策级）比较，最终选择特征级融合方法。

Result: 模型F1分数为0.802，较现有方法提升14%，特征级融合表现最优（比数据级和决策级高至少0.14分）。还进行了消融实验和量化评估。

Conclusion: 多传感器信号的特征级融合能显著提升摄食行为识别精度，为畜牧业管理提供了更高效的自动化解决方案。

Abstract: Monitoring feeding behaviour is a relevant task for efficient herd management
and the effective use of available resources in grazing cattle. The ability to
automatically recognise animals' feeding activities through the identification
of specific jaw movements allows for the improvement of diet formulation, as
well as early detection of metabolic problems and symptoms of animal
discomfort, among other benefits. The use of sensors to obtain signals for such
monitoring has become popular in the last two decades. The most frequently
employed sensors include accelerometers, microphones, and cameras, each with
its own set of advantages and drawbacks. An unexplored aspect is the
simultaneous use of multiple sensors with the aim of combining signals in order
to enhance the precision of the estimations. In this direction, this work
introduces a deep neural network based on the fusion of acoustic and inertial
signals, composed of convolutional, recurrent, and dense layers. The main
advantage of this model is the combination of signals through the automatic
extraction of features independently from each of them. The model has emerged
from an exploration and comparison of different neural network architectures
proposed in this work, which carry out information fusion at different levels.
Feature-level fusion has outperformed data and decision-level fusion by at
least a 0.14 based on the F1-score metric. Moreover, a comparison with
state-of-the-art machine learning methods is presented, including traditional
and deep learning approaches. The proposed model yielded an F1-score value of
0.802, representing a 14% increase compared to previous methods. Finally,
results from an ablation study and post-training quantization evaluation are
also reported.

</details>


### [98] [Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM Performance on Time Series Forecasting](https://arxiv.org/abs/2505.10213)
*Mohammadmahdi Ghasemloo,Alireza Moradi*

Main category: cs.LG

TL;DR: 论文提出了一种新的跨领域知识转移框架，通过向大语言模型注入结构化时序信息，提升其在时间序列预测任务中的表现。实验证明该方法在预测准确性和泛化能力上显著优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，研究者希望扩展其能力至传统自然语言任务之外的领域，如时间序列预测。本文旨在探索如何利用知识转移策略提升大语言模型在特定领域任务中的性能。

Method: 提出了一个跨领域知识转移框架，将结构化时序信息注入大语言模型，通过对比实验验证了该方法的有效性。

Result: 实验结果表明，知识增强的预测方法在真实时间序列数据集上显著优于未使用辅助信息的基准方法。

Conclusion: 该研究为利用知识转移策略弥合大语言模型与领域特定任务（如时间序列预测）之间的差距提供了潜在方向。

Abstract: With the widespread adoption of Large Language Models (LLMs), there is a
growing need to establish best practices for leveraging their capabilities
beyond traditional natural language tasks. In this paper, a novel cross-domain
knowledge transfer framework is proposed to enhance the performance of LLMs in
time series forecasting -- a task of increasing relevance in fields such as
energy systems, finance, and healthcare. The approach systematically infuses
LLMs with structured temporal information to improve their forecasting
accuracy. This study evaluates the proposed method on a real-world time series
dataset and compares it to a naive baseline where the LLM receives no auxiliary
information. Results show that knowledge-informed forecasting significantly
outperforms the uninformed baseline in terms of predictive accuracy and
generalization. These findings highlight the potential of knowledge transfer
strategies to bridge the gap between LLMs and domain-specific forecasting
tasks.

</details>


### [99] [Superposition Yields Robust Neural Scaling](https://arxiv.org/abs/2505.10465)
*Yizhou liu,Ziming Liu,Jeff Gore*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型（LLM）的性能随着模型规模增长而提升的现象，即神经缩放定律。通过构建一个玩具模型，发现弱叠加下损失与模型尺寸的缩放依赖于特征频率，而在强叠加下损失与模型维度成反比。


<details>
  <summary>Details</summary>
Motivation: 研究神经缩放定律的起源，即为何更大的模型表现更好，以及表征叠加如何影响模型性能。

Method: 基于两个实证原则（表征叠加和特征频率差异）构建玩具模型，分析弱叠加与强叠加下的损失缩放行为，并通过几何解释验证。

Result: 发现强叠加下损失与模型维度成反比，且开源LLM家族的表现与玩具模型预测一致。Chinchilla缩放定律也与研究结果相符。

Conclusion: 表征叠加是神经缩放定律的重要机制，这一发现可能启发更高效的计算和参数利用方法。

Abstract: The success of today's large language models (LLMs) depends on the
observation that larger models perform better. However, the origin of this
neural scaling law -- the finding that loss decreases as a power law with model
size -- remains unclear. Starting from two empirical principles -- that LLMs
represent more things than the model dimensions (widths) they have (i.e.,
representations are superposed), and that words or concepts in language occur
with varying frequencies -- we constructed a toy model to study the loss
scaling with model size. We found that when superposition is weak, meaning only
the most frequent features are represented without interference, the scaling of
loss with model size depends on the underlying feature frequency; if feature
frequencies follow a power law, so does the loss. In contrast, under strong
superposition, where all features are represented but overlap with each other,
the loss becomes inversely proportional to the model dimension across a wide
range of feature frequency distributions. This robust scaling behavior is
explained geometrically: when many more vectors are packed into a lower
dimensional space, the interference (squared overlaps) between vectors scales
inversely with that dimension. We then analyzed four families of open-sourced
LLMs and found that they exhibit strong superposition and quantitatively match
the predictions of our toy model. The Chinchilla scaling law turned out to also
agree with our results. We conclude that representation superposition is an
important mechanism underlying the observed neural scaling laws. We anticipate
that these insights will inspire new training strategies and model
architectures to achieve better performance with less computation and fewer
parameters.

</details>


### [100] [SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices](https://arxiv.org/abs/2505.10259)
*Xiangwen Zhuge,Xu Shen,Zeyu Wang,Fan Dang,Xuan Ding,Danyang Li,Yahui Han,Tianxiang Hao,Zheng Yang*

Main category: cs.LG

TL;DR: SpecOffload提出了一种高效LLM推理引擎，通过在卸载中嵌入推测解码，显著提升GPU资源利用率，推理吞吐量提升2.54倍。


<details>
  <summary>Details</summary>
Motivation: 现有的系统由于GPU内存有限，将模型权重卸载到CPU内存中，导致CPU和GPU间I/O开销大，GPU核心利用率低，内存对性能影响有限。

Method: 利用推测解码的潜在GPU资源存储和执行草稿模型，并设计planner协调目标模型和草稿模型的交错执行，优化张量放置和参数选择。

Result: 相比最优基线，SpecOffload将GPU核心利用率提升4.49倍，推理吞吐量提升2.54倍。

Conclusion: SpecOffload在近乎零额外成本下加速推理，显著提升了资源受限设备上的LLM推理效率。

Abstract: Efficient LLM inference on resource-constrained devices presents significant
challenges in compute and memory utilization. Due to limited GPU memory,
existing systems offload model weights to CPU memory, incurring substantial I/O
overhead between the CPU and GPU. This leads to two major inefficiencies: (1)
GPU cores are underutilized, often remaining idle while waiting for data to be
loaded; and (2) GPU memory has low impact on performance, as reducing its
capacity has minimal effect on overall throughput.In this paper, we propose
SpecOffload, a high-throughput inference engine that embeds speculative
decoding into offloading. Our key idea is to unlock latent GPU resources for
storing and executing a draft model used for speculative decoding, thus
accelerating inference at near-zero additional cost. To support this, we
carefully orchestrate the interleaved execution of target and draft models in
speculative decoding within the offloading pipeline, and propose a planner to
manage tensor placement and select optimal parameters. Compared to the best
baseline, SpecOffload improves GPU core utilization by 4.49x and boosts
inference throughput by 2.54x. Our code is available at
https://github.com/MobiSense/SpecOffload .

</details>


### [101] [Parallel Scaling Law for Language Models](https://arxiv.org/abs/2505.10475)
*Mouxiang Chen,Binyuan Hui,Zeyu Cui,Jiaxi Yang,Dayiheng Liu,Jianling Sun,Junyang Lin,Zhongxin Liu*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的并行扩展（ParScale）方法，通过在训练和推理时增加并行计算来高效扩展语言模型，而不显著增加参数或时间成本。该方法通过动态聚合并行输出来实现，理论上和实验验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 传统的模型扩展方法（参数扩展或推理时间扩展）需要显著增加资源成本。为了更高效地扩展模型，作者提出了并行扩展（ParScale），旨在通过并行计算提升性能，同时减少资源消耗。

Method: 作者提出ParScale方法，通过对输入应用P种不同的可学习变换，并行执行前向传播，并动态聚合P个输出。这种方法复用现有参数，适用于任何模型结构和任务。

Result: 实验表明，ParScale在达到相同性能提升时，比参数扩展节省22倍内存和6倍延迟。还能通过少量数据后训练将预训练模型转换为并行扩展版本，进一步降低训练成本。

Conclusion: ParScale为模型扩展提供了新的高效范式，尤其是在资源有限的场景下，同时为计算在机器学习中的作用提供了新视角。

Abstract: It is commonly believed that scaling language models should commit a
significant space or time cost, by increasing the parameters (parameter
scaling) or output tokens (inference-time scaling). We introduce the third and
more inference-efficient scaling paradigm: increasing the model's parallel
computation during both training and inference time. We apply $P$ diverse and
learnable transformations to the input, execute forward passes of the model in
parallel, and dynamically aggregate the $P$ outputs. This method, namely
parallel scaling (ParScale), scales parallel computation by reusing existing
parameters and can be applied to any model structure, optimization procedure,
data, or task. We theoretically propose a new scaling law and validate it
through large-scale pre-training, which shows that a model with $P$ parallel
streams is similar to scaling the parameters by $O(\log P)$ while showing
superior inference efficiency. For example, ParScale can use up to 22$\times$
less memory increase and 6$\times$ less latency increase compared to parameter
scaling that achieves the same performance improvement. It can also recycle an
off-the-shelf pre-trained model into a parallelly scaled one by post-training
on a small amount of tokens, further reducing the training budget. The new
scaling law we discovered potentially facilitates the deployment of more
powerful models in low-resource scenarios, and provides an alternative
perspective for the role of computation in machine learning.

</details>


### [102] [Electric Bus Charging Schedules Relying on Real Data-Driven Targets Based on Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2505.10262)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Lajos Hanzo*

Main category: cs.LG

TL;DR: 论文提出一种基于分层深度强化学习（HDRL）的电动公交车充电调度方法，通过HDDQN-HER算法分层优化高低级策略以减少充电成本。


<details>
  <summary>Details</summary>
Motivation: 解决电动公交车在充电和运营期间的长时段多阶段规划问题，特别是面对稀疏奖励和采样效率低的挑战。

Method: 将原始MDP分解为高层SMDP和多个低层MDP，采用HDDQN-HER算法分层训练策略，并通过两阶段学习和HER改进采样效率。

Result: 实验证明分层策略叠加后的表现与原MDP最优策略相当，且能有效降低充电成本。

Conclusion: HDRL框架和HDDQN-HER算法能高效解决电动公交车充电调度问题，为复杂时序规划提供了新思路。

Abstract: The charging scheduling problem of Electric Buses (EBs) is investigated based
on Deep Reinforcement Learning (DRL). A Markov Decision Process (MDP) is
conceived, where the time horizon includes multiple charging and operating
periods in a day, while each period is further divided into multiple time
steps. To overcome the challenge of long-range multi-phase planning with sparse
reward, we conceive Hierarchical DRL (HDRL) for decoupling the original MDP
into a high-level Semi-MDP (SMDP) and multiple low-level MDPs. The Hierarchical
Double Deep Q-Network (HDDQN)-Hindsight Experience Replay (HER) algorithm is
proposed for simultaneously solving the decision problems arising at different
temporal resolutions. As a result, the high-level agent learns an effective
policy for prescribing the charging targets for every charging period, while
the low-level agent learns an optimal policy for setting the charging power of
every time step within a single charging period, with the aim of minimizing the
charging costs while meeting the charging target. It is proved that the flat
policy constructed by superimposing the optimal high-level policy and the
optimal low-level policy performs as well as the optimal policy of the original
MDP. Since jointly learning both levels of policies is challenging due to the
non-stationarity of the high-level agent and the sampling inefficiency of the
low-level agent, we divide the joint learning process into two phases and
exploit our new HER algorithm to manipulate the experience replay buffers for
both levels of agents. Numerical experiments are performed with the aid of
real-world data to evaluate the performance of the proposed algorithm.

</details>


### [103] [RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs](https://arxiv.org/abs/2505.10495)
*Vibha Belavadi,Tushar Vatsa,Dewang Sultania,Suhas Suresha,Ishita Verma,Cheng Chen,Tracy Holloway King,Michael Friedrich*

Main category: cs.LG

TL;DR: 该论文提出了一种基于路由器的新架构，用于生成高质量的合成数据，以优化大型语言模型在缺乏真实用户数据时的函数调用任务微调，显著提高了分类准确性和参数选择性能。


<details>
  <summary>Details</summary>
Motivation: 在数字内容创作工具中，用户通过自然语言查询表达需求，需映射到API调用。但缺乏真实任务数据和隐私限制使合成数据生成成为必要。现有方法在多样性和复杂性上不足，难以反映真实数据分布，导致微调后性能不佳。

Method: 采用基于路由器的架构，结合领域资源（如内容元数据和结构化知识图谱）以及文本到文本、视觉到文本的语言模型，生成高质量的合成训练数据，其灵活的路由机制能匹配真实数据分布。

Result: 在真实用户查询的评估中，使用合成数据微调的模型在函数分类准确性和API参数选择上显著优于传统方法，刷新了函数调用任务的基准。

Conclusion: 该架构通过改进合成数据生成，解决了传统方法的局限性，为缺乏真实数据场景下的LLM微调提供了高效解决方案。

Abstract: This paper addresses fine-tuning Large Language Models (LLMs) for function
calling tasks when real user interaction data is unavailable. In digital
content creation tools, where users express their needs through natural
language queries that must be mapped to API calls, the lack of real-world
task-specific data and privacy constraints for training on it necessitate
synthetic data generation. Existing approaches to synthetic data generation
fall short in diversity and complexity, failing to replicate real-world data
distributions and leading to suboptimal performance after LLM fine-tuning. We
present a novel router-based architecture that leverages domain resources like
content metadata and structured knowledge graphs, along with text-to-text and
vision-to-text language models to generate high-quality synthetic training
data. Our architecture's flexible routing mechanism enables synthetic data
generation that matches observed real-world distributions, addressing a
fundamental limitation of traditional approaches. Evaluation on a comprehensive
set of real user queries demonstrates significant improvements in both function
classification accuracy and API parameter selection. Models fine-tuned with our
synthetic data consistently outperform traditional approaches, establishing new
benchmarks for function calling tasks.

</details>


### [104] [Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning](https://arxiv.org/abs/2505.10264)
*Francesco Diana,André Nusser,Chuan Xu,Giovanni Neglia*

Main category: cs.LG

TL;DR: 该论文提出了一种新型的数据重构攻击方法，克服了现有方法的局限性，能够完美恢复任意大规模的批量数据，且在分类任务中无需任何客户数据的先验知识。


<details>
  <summary>Details</summary>
Motivation: 尽管联邦学习（FL）通过在分布式客户端间协作训练模型而保护数据隐私，但研究发现恶意中央服务器仍可通过操纵模型更新重构客户的私有数据。现有数据重构攻击方法存在对客户数据分布假设或处理大批量数据效率低下的局限性。

Method: 论文提出了一种基于全连接层新几何视角的方法，通过精心设计恶意模型参数，实现了无需客户数据先验知识即可完美恢复任意大批量数据。

Result: 在图像和表格数据集的广泛实验中，该方法显著优于现有方法，且能够完美重构比现有技术大两个数量级的批量数据。

Conclusion: 该方法揭示了联邦学习中数据隐私保护的潜在漏洞，强调了进一步改进隐私保护技术的必要性。

Abstract: Federated Learning (FL) enables collaborative training of machine learning
models across distributed clients without sharing raw data, ostensibly
preserving data privacy. Nevertheless, recent studies have revealed critical
vulnerabilities in FL, showing that a malicious central server can manipulate
model updates to reconstruct clients' private training data. Existing data
reconstruction attacks have important limitations: they often rely on
assumptions about the clients' data distribution or their efficiency
significantly degrades when batch sizes exceed just a few tens of samples.
  In this work, we introduce a novel data reconstruction attack that overcomes
these limitations. Our method leverages a new geometric perspective on fully
connected layers to craft malicious model parameters, enabling the perfect
recovery of arbitrarily large data batches in classification tasks without any
prior knowledge of clients' data. Through extensive experiments on both image
and tabular datasets, we demonstrate that our attack outperforms existing
methods and achieves perfect reconstruction of data batches two orders of
magnitude larger than the state of the art.

</details>


### [105] [MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models](https://arxiv.org/abs/2505.10526)
*Mugilan Ganesan,Shane Segal,Ankur Aggarwal,Nish Sinnadurai,Sean Lie,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: MASSV通过两阶段方法将小型语言模型转化为高效多模态草稿模型，提升视觉语言模型的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有小型语言模型因缺乏视觉处理能力且预测与视觉上下文不匹配，难以作为视觉语言模型的草稿模型使用。

Method: MASSV分两阶段：1) 通过轻量可训练投影器连接目标模型的视觉编码器；2) 自蒸馏视觉指令微调以对齐预测。

Result: 在Qwen2.5-VL和Gemma3模型上，MASSV接受长度提升30%，推理速度加快1.46倍。

Conclusion: MASSV为加速当前及未来视觉语言模型提供了一种可扩展且兼容架构的方法。

Abstract: Speculative decoding significantly accelerates language model inference by
enabling a lightweight draft model to propose multiple tokens that a larger
target model verifies simultaneously. However, applying this technique to
vision-language models (VLMs) presents two fundamental challenges: small
language models that could serve as efficient drafters lack the architectural
components to process visual inputs, and their token predictions fail to match
those of VLM target models that consider visual context. We introduce
Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of
Vision-Language Models (MASSV), which transforms existing small language models
into effective multimodal drafters through a two-phase approach. MASSV first
connects the target VLM's vision encoder to the draft model via a lightweight
trainable projector, then applies self-distilled visual instruction tuning
using responses generated by the target VLM to align token predictions.
Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families
demonstrate that MASSV increases accepted length by up to 30% and delivers
end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV
provides a scalable, architecture-compatible method for accelerating both
current and future VLMs.

</details>


### [106] [RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall Probabilities Over 8 Hours](https://arxiv.org/abs/2505.10271)
*Rafael Pablos Sarabia,Joachim Nyborg,Morten Birk,Jeppe Liborius Sjørup,Anders Lillevang Vesterholt,Ira Assent*

Main category: cs.LG

TL;DR: 本文提出了一种深度学习模型，用于欧洲地区高分辨率概率降水预测，通过整合雷达、卫星和数值天气预报数据，实现长达8小时的高精度预测。


<details>
  <summary>Details</summary>
Motivation: 解决雷达深度学习模型预测时间短的问题，提升降水预测的准确性和不确定性量化能力，同时提高计算效率。

Method: 结合雷达、卫星和数值天气预报数据，采用紧凑模型架构，捕获长距离交互关系，实现高效训练和快速推理。

Result: 模型在实验中优于现有数值天气预报系统、外推法和深度学习临近预报模型，成为欧洲高分辨率降水预测的新标杆。

Conclusion: 该模型在准确性、可解释性和计算效率之间取得了平衡，为降水预测领域提供了新的解决方案。

Abstract: We present a deep learning model for high-resolution probabilistic
precipitation forecasting over an 8-hour horizon in Europe, overcoming the
limitations of radar-only deep learning models with short forecast lead times.
Our model efficiently integrates multiple data sources - including radar,
satellite, and physics-based numerical weather prediction (NWP) - while
capturing long-range interactions, resulting in accurate forecasts with robust
uncertainty quantification through consistent probabilistic maps. Featuring a
compact architecture, it enables more efficient training and faster inference
than existing models. Extensive experiments demonstrate that our model
surpasses current operational NWP systems, extrapolation-based methods, and
deep-learning nowcasting models, setting a new standard for high-resolution
precipitation forecasting in Europe, ensuring a balance between accuracy,
interpretability, and computational efficiency.

</details>


### [107] [Spike-timing-dependent Hebbian learning as noisy gradient descent](https://arxiv.org/abs/2505.10272)
*Niklas Dexheimer,Sascha Gaudlitz,Johannes Schmidt-Hieber*

Main category: cs.LG

TL;DR: 本文探讨了一种基于精确脉冲时序的Hebbian学习规则，将其与自然损失函数的噪声梯度下降联系起来，证明了该规则最终能识别最高活动的突触前神经元，并揭示了其与噪声镜像下降的内在联系。


<details>
  <summary>Details</summary>
Motivation: Hebbian学习是生物神经网络学习的关键原则，但关于精确脉冲时序的学习规则知之甚少。本文旨在填补这一空白，并探索其与优化算法的联系。

Method: 将Hebbian脉冲时序依赖可塑性规则与概率单纯形上的自然损失函数的噪声梯度下降联系起来，并通过理论分析验证其有效性。

Result: 证明了该学习规则能最终识别最高活动的突触前神经元，并发现其与噪声镜像下降的内在联系。

Conclusion: 这项工作为理解基于脉冲时序的Hebbian学习提供了新的理论框架，并揭示了其与优化算法之间的深刻联系。

Abstract: Hebbian learning is a key principle underlying learning in biological neural
networks. It postulates that synaptic changes occur locally, depending on the
activities of pre- and postsynaptic neurons. While Hebbian learning based on
neuronal firing rates is well explored, much less is known about learning rules
that account for precise spike-timing. We relate a Hebbian
spike-timing-dependent plasticity rule to noisy gradient descent with respect
to a natural loss function on the probability simplex. This connection allows
us to prove that the learning rule eventually identifies the presynaptic neuron
with the highest activity. We also discover an intrinsic connection to noisy
mirror descent.

</details>


### [108] [Optimizing Electric Bus Charging Scheduling with Uncertainties Using Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2505.10296)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Dusit Niyato*

Main category: cs.LG

TL;DR: 本文提出了一种分层深度强化学习（HDRL）方法，通过改进的DAC-MAPPO-E算法优化电动公交车充电调度，解决多时间尺度和大规模车队的挑战。


<details>
  <summary>Details</summary>
Motivation: 电动公交车的普及推动可持续发展，但充电调度优化面临时间不确定性、能耗变化和电价波动等挑战，且需在多时间尺度高效决策并适应大规模车队。

Method: 采用分层深度强化学习，将原MDP重构为两个增强MDP，并引入DAC-MAPPO-E算法。高层改进分散式行动者网络并整合注意力机制，低层结合MAPPO算法实现分散协调决策。

Result: 真实数据实验表明，DAC-MAPPO-E在充电调度优化中表现优异，具备高扩展性和快速收敛性。

Conclusion: DAC-MAPPO-E通过分层和算法改进，有效解决了电动公交车充电调度的复杂性和可扩展性问题。

Abstract: The growing adoption of Electric Buses (EBs) represents a significant step
toward sustainable development. By utilizing Internet of Things (IoT) systems,
charging stations can autonomously determine charging schedules based on
real-time data. However, optimizing EB charging schedules remains a critical
challenge due to uncertainties in travel time, energy consumption, and
fluctuating electricity prices. Moreover, to address real-world complexities,
charging policies must make decisions efficiently across multiple time scales
and remain scalable for large EB fleets. In this paper, we propose a
Hierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the
original Markov Decision Process (MDP) into two augmented MDPs. To solve these
MDPs and enable multi-timescale decision-making, we introduce a novel HDRL
algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization
Enhancement (DAC-MAPPO-E). Scalability challenges of the Double Actor-Critic
(DAC) algorithm for large-scale EB fleets are addressed through enhancements at
both decision levels. At the high level, we redesign the decentralized actor
network and integrate an attention mechanism to extract relevant global state
information for each EB, decreasing the size of neural networks. At the low
level, the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm is
incorporated into the DAC framework, enabling decentralized and coordinated
charging power decisions, reducing computational complexity and enhancing
convergence speed. Extensive experiments with real-world data demonstrate the
superior performance and scalability of DAC-MAPPO-E in optimizing EB fleet
charging schedules.

</details>


### [109] [Defending the Edge: Representative-Attention for Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2505.10297)
*Chibueze Peace Obioma,Youcheng Sun,Mustafa A. Mustafa*

Main category: cs.LG

TL;DR: 该论文提出了一种名为FeRA的联邦学习防御机制，通过跨客户端的注意力机制识别恶意客户端，有效降低后门攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 针对联邦学习中设备异构性导致的数据非独立同分布问题，以及由此增加的后门攻击检测难度。

Method: 采用基于特征的注意力机制（FeRA），通过计算表示重构误差的异常分数来识别偏离群体共识的客户端。

Result: 实验表明FeRA在不同联邦学习场景中表现鲁棒，能显著降低后门攻击成功率，同时保持主任务的高准确率。

Conclusion: FeRA是一种模型无关、攻击无关且无需标记参考数据的方法，适合异构和资源受限的边缘部署。

Abstract: Federated learning (FL) enhances privacy and reduces communication cost for
resource-constrained edge clients by supporting distributed model training at
the edge. However, the heterogeneous nature of such devices produces diverse,
non-independent, and identically distributed (non-IID) data, making the
detection of backdoor attacks more challenging. In this paper, we propose a
novel federated representative-attention-based defense mechanism, named FeRA,
that leverages cross-client attention over internal feature representations to
distinguish benign from malicious clients. FeRA computes an anomaly score based
on representation reconstruction errors, effectively identifying clients whose
internal activations significantly deviate from the group consensus. Our
evaluation demonstrates FeRA's robustness across various FL scenarios,
including challenging non-IID data distributions typical of edge devices.
Experimental results show that it effectively reduces backdoor attack success
rates while maintaining high accuracy on the main task. The method is
model-agnostic, attack-agnostic, and does not require labeled reference data,
making it well suited to heterogeneous and resource-limited edge deployments.

</details>


### [110] [Negative Metric Learning for Graphs](https://arxiv.org/abs/2505.10307)
*Yiyang Zhao,Chengpei Wu,Lilin Zhang,Ning Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为NML-GCL的新方法，通过学习负度量网络（NMN）来区分假负例和真负例，并通过联合训练优化网络，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 图对比学习（GCL）常受假负例影响，现有方法依赖先验知识导致次优结果，因此提出NML-GCL以更有效区分假负例。

Method: NML-GCL利用可学习的负度量网络构建负度量空间，通过联合训练和双层优化目标隐式利用自监督信号优化网络。

Result: 在广泛使用的基准测试上进行的大量实验验证了该方法的优越性。

Conclusion: NML-GCL通过创新性的负度量学习和优化方案，显著提升了图对比学习的性能。

Abstract: Graph contrastive learning (GCL) often suffers from false negatives, which
degrades the performance on downstream tasks. The existing methods addressing
the false negative issue usually rely on human prior knowledge, still leading
GCL to suboptimal results. In this paper, we propose a novel Negative Metric
Learning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative
Metric Network (NMN) to build a negative metric space, in which false negatives
can be distinguished better from true negatives based on their distance to
anchor node. To overcome the lack of explicit supervision signals for NML, we
propose a joint training scheme with bi-level optimization objective, which
implicitly utilizes the self-supervision signals to iteratively optimize the
encoder and the negative metric network. The solid theoretical analysis and the
extensive experiments conducted on widely used benchmarks verify the
superiority of the proposed method.

</details>


### [111] [Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate Descent Framework](https://arxiv.org/abs/2505.10322)
*Yijie Zhou,Shi Pu*

Main category: cs.LG

TL;DR: 该论文提出了一种改进的异步去中心化随机梯度下降（ADSGD）方法，适用于计算和通信时间受限的实际场景，并在理论上证明了其收敛性，且在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 去中心化优化在处理分布式数据时具有扩展性和隐私性的优势，但实际应用中因计算速度和通信延迟的异构性而面临挑战。本研究旨在解决这些问题。

Method: 通过分析异步随机块坐标下降（ASBCD）作为工具，提出ADSGD方法，并采用计算延迟无关的步长证明其收敛性。

Result: 实验显示，ADSGD在各种场景下都优于现有方法，具有内存和通信效率高、对延迟的鲁棒性等特点。

Conclusion: ADSGD因其简单性、高效性和对延迟的适应能力，非常适合实际去中心化学习任务。

Abstract: Decentralized optimization has become vital for leveraging distributed data
without central control, enhancing scalability and privacy. However, practical
deployments face fundamental challenges due to heterogeneous computation speeds
and unpredictable communication delays. This paper introduces a refined model
of Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under
practical assumptions of bounded computation and communication times. To
understand the convergence of ADSGD, we first analyze Asynchronous Stochastic
Block Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges
under computation-delay-independent step sizes. The convergence result is
established without assuming bounded data heterogeneity. Empirical experiments
reveal that ADSGD outperforms existing methods in wall-clock convergence time
across various scenarios. With its simplicity, efficiency in memory and
communication, and resilience to communication and computation delays, ADSGD is
well-suited for real-world decentralized learning tasks.

</details>


### [112] [A Representation Learning Approach to Feature Drift Detection in Wireless Networks](https://arxiv.org/abs/2505.10325)
*Athanasios Tziouvaras,Blaz Bertalanic,George Floros,Kostas Kolomvatsos,Panagiotis Sarigiannidis,Carolina Fortuna*

Main category: cs.LG

TL;DR: 论文提出了一种名为ALERT的方法，用于检测特征分布变化并触发模型重新训练，以应对AI模型在无线网络中的性能退化问题。该方法在无线指纹识别和链路异常检测两个用例中表现优异。


<details>
  <summary>Details</summary>
Motivation: 在下一代无线网络中，AI虽然被寄予厚望，但在实际部署中，特征分布的变化可能导致模型性能下降和不良行为。为了解决未被检测到的模型退化问题，需要一种有效的方法来进行检测和调整。

Method: ALERT方法由三部分组成：表示学习（使用MLP设计）、统计测试（基于Kolmogorov-Smirnov和Population Stability Index测试）以及效用评估（提出新的评估函数）。

Result: ALERT在无线指纹识别和链路异常检测两个用例中表现出色，优于文献中的十种标准漂移检测方法。

Conclusion: ALERT是一种有效的解决方案，能够检测特征分布变化并触发模型重新训练，显著提升了AI模型在无线网络中的鲁棒性和实用性。

Abstract: AI is foreseen to be a centerpiece in next generation wireless networks
enabling enabling ubiquitous communication as well as new services. However, in
real deployment, feature distribution changes may degrade the performance of AI
models and lead to undesired behaviors. To counter for undetected model
degradation, we propose ALERT; a method that can detect feature distribution
changes and trigger model re-training that works well on two wireless network
use cases: wireless fingerprinting and link anomaly detection. ALERT includes
three components: representation learning, statistical testing and utility
assessment. We rely on MLP for designing the representation learning component,
on Kolmogorov-Smirnov and Population Stability Index tests for designing the
statistical testing and a new function for utility assessment. We show the
superiority of the proposed method against ten standard drift detection methods
available in the literature on two wireless network use cases.

</details>


### [113] [Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental Change](https://arxiv.org/abs/2505.10330)
*Jonathan Clifford Balloch*

Main category: cs.LG

TL;DR: 该论文探讨了深度强化学习（RL）在动态环境中的适应性挑战，提出了高效在线适应的两个关键能力：优先探索和选择性知识保留。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的自主决策系统（如机器人和推荐引擎）需要在不断变化的环境中运行。传统的强化学习方法在训练和测试时假设环境不变，难以适应环境变化。因此，研究如何让RL智能体在部署时高效适应新环境变化，同时避免灾难性遗忘，成为一个核心问题。

Method: 论文提出了两个关键能力：1）优先探索和采样策略，用于识别和从相关经验中学习；2）通过结构化表示选择性保留先验知识，确保可重用组件在更新时不受干扰。

Result: 研究表明，结合优先探索与结构化知识保留的机制，可以在动态环境中实现高效在线适应。

Conclusion: 论文通过理论分析和实验验证，证明了所提方法的有效性，为RL在非平稳环境中的应用提供了新思路。

Abstract: Real-world autonomous decision-making systems, from robots to recommendation
engines, must operate in environments that change over time. While deep
reinforcement learning (RL) has shown an impressive ability to learn optimal
policies in stationary environments, most methods are data intensive and assume
a world that does not change between training and test time. As a result,
conventional RL methods struggle to adapt when conditions change. This poses a
fundamental challenge: how can RL agents efficiently adapt their behavior when
encountering novel environmental changes during deployment without
catastrophically forgetting useful prior knowledge? This dissertation
demonstrates that efficient online adaptation requires two key capabilities:
(1) prioritized exploration and sampling strategies that help identify and
learn from relevant experiences, and (2) selective preservation of prior
knowledge through structured representations that can be updated without
disruption to reusable components.

</details>


### [114] [Emergence of Structure in Ensembles of Random Neural Networks](https://arxiv.org/abs/2505.10331)
*Luca Muscarnera,Luigi Loreti,Giovanni Todeschini,Alessio Fumagalli,Francesco Regazzoni*

Main category: cs.LG

TL;DR: 该论文研究了随机分类器集合中集体行为的涌现，提出了一种理论模型，通过Gibbs度量加权集合，证明了存在一个最优温度参数使分类效果最佳，且在特定条件下该参数具有普适性。


<details>
  <summary>Details</summary>
Motivation: 探索随机组件系统如何从微观无序过渡到宏观有序行为，为随机分类器集合中的集体行为提供理论解释。

Method: 采用理论模型结合Gibbs度量，利用分类损失作为能量函数，分析最优温度参数的存在及其普适性质。

Result: 在特定条件下，最优温度参数与未知分类器和随机分类器数量无关，MNIST数据实验验证了该现象的普适性。

Conclusion: 研究揭示了随机分类器集合中的自组织现象，为理解集体行为提供了新的理论视角。

Abstract: Randomness is ubiquitous in many applications across data science and machine
learning. Remarkably, systems composed of random components often display
emergent global behaviors that appear deterministic, manifesting a transition
from microscopic disorder to macroscopic organization. In this work, we
introduce a theoretical model for studying the emergence of collective
behaviors in ensembles of random classifiers. We argue that, if the ensemble is
weighted through the Gibbs measure defined by adopting the classification loss
as an energy, then there exists a finite temperature parameter for the
distribution such that the classification is optimal, with respect to the loss
(or the energy). Interestingly, for the case in which samples are generated by
a Gaussian distribution and labels are constructed by employing a teacher
perceptron, we analytically prove and numerically confirm that such optimal
temperature does not depend neither on the teacher classifier (which is, by
construction of the learning problem, unknown), nor on the number of random
classifiers, highlighting the universal nature of the observed behavior.
Experiments on the MNIST dataset underline the relevance of this phenomenon in
high-quality, noiseless, datasets. Finally, a physical analogy allows us to
shed light on the self-organizing nature of the studied phenomenon.

</details>


### [115] [An Introduction to Discrete Variational Autoencoders](https://arxiv.org/abs/2505.10344)
*Alan Jeffares,Liyuan Liu*

Main category: cs.LG

TL;DR: 介绍了离散变分自编码器（VAE），特别是潜在空间由分类分布组成的VAE，适用于离散数据（如文本）。教程从基础数学出发，推导每一步并提供具体训练方法和实现示例。


<details>
  <summary>Details</summary>
Motivation: 离散潜在空间在许多数据模态（如文本）中表现出自然适应性，因此需要系统地介绍离散VAE的原理和实践方法。

Method: 教程从基础数学推导出发，定义了分类分布的潜在空间，通过编码器和解码器网络实现输入重构，并使用证据下界进行优化。

Result: 提供了一个具体的训练方法和示例实现（GitHub链接），帮助读者理解和应用离散VAE。

Conclusion: 离散VAE是处理离散数据的有效工具，本文为其提供了理论支持及实用实现，为相关研究提供了实用指导。

Abstract: Variational Autoencoders (VAEs) are well-established as a principled approach
to probabilistic unsupervised learning with neural networks. Typically, an
encoder network defines the parameters of a Gaussian distributed latent space
from which we can sample and pass realizations to a decoder network. This model
is trained to reconstruct its inputs and is optimized through the evidence
lower bound. In recent years, discrete latent spaces have grown in popularity,
suggesting that they may be a natural choice for many data modalities (e.g.
text). In this tutorial, we provide a rigorous, yet practical, introduction to
discrete variational autoencoders -- specifically, VAEs in which the latent
space is made up of latent variables that follow a categorical distribution. We
assume only a basic mathematical background with which we carefully derive each
step from first principles. From there, we develop a concrete training recipe
and provide an example implementation, hosted at
https://github.com/alanjeffares/discreteVAE.

</details>


### [116] [Uniform Loss vs. Specialized Optimization: A Comparative Analysis in Multi-Task Learning](https://arxiv.org/abs/2505.10347)
*Gabriel S. Gama,Valdir Grassi Jr*

Main category: cs.LG

TL;DR: 本文通过实证评估比较了专用多任务优化器（SMTOs）与均匀损失在多任务学习中的表现，发现SMTOs表现优于均匀损失，但均匀损失在某些情况下也能达到竞争性效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于澄清近期关于专用多任务优化器（SMTOs）与均匀损失在多任务学习中表现争议，验证SMTOs的实际效果。

Method: 方法是进行广泛的实证评估，对比SMTOs与均匀损失在复杂多任务问题上的表现。

Result: 结果显示SMTOs表现优于均匀损失，但均匀损失在某些情况下也能达到竞争性效果。

Conclusion: 结论表明SMTOs在多任务学习中仍具优势，但均匀损失在特定场景下也具备竞争力，研究结果有助于未来优化策略的选择。

Abstract: Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task
Learning by addressing issues like conflicting gradients and differing gradient
norms, which hinder equal-weighted task training. However, recent critiques
suggest that equally weighted tasks can achieve competitive results compared to
SMTOs, arguing that previous SMTO results were influenced by poor
hyperparameter optimization and lack of regularization. In this work, we
evaluate these claims through an extensive empirical evaluation of SMTOs,
including some of the latest methods, on more complex multi-task problems to
clarify this behavior. Our findings indicate that SMTOs perform well compared
to uniform loss and that fixed weights can achieve competitive performance
compared to SMTOs. Furthermore, we demonstrate why uniform loss perform
similarly to SMTOs in some instances. The code will be made publicly available.

</details>


### [117] [FactsR: A Safer Method for Producing High Quality Healthcare Documentation](https://arxiv.org/abs/2505.10360)
*Victor Petrén Bach Hansen,Lasse Krogsbøll,Jonas Lyngsø,Mathias Baltzersen,Andreas Motzfeldt,Kevin Pelgrims,Lars Maaløe*

Main category: cs.LG

TL;DR: 该论文提出了一种称为FactsR的方法，通过实时提取临床关键信息并递归生成最终笔记，解决了当前AI医疗记录依赖一次性提示且易产生错误的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的AI医疗记录工具依赖于一次性或少量提示生成笔记，容易产生冗长且错误的记录，可能误导临床医生意图并危害患者安全。因此，需要一种更准确、实时的医疗记录方法。

Method: 论文提出了FactsR方法，通过实时提取临床关键信息（称为Facts），并利用这些信息递归生成最终笔记。这种方法将临床医生纳入笔记生成过程，提高了准确性。

Result: FactsR方法生成的笔记更准确、简洁，并支持实时决策的新应用场景。

Conclusion: FactsR通过实时信息提取和递归生成笔记，显著提升了AI医疗记录的准确性和临床实用性，为实时决策支持提供了可能性。

Abstract: There are now a multitude of AI-scribing solutions for healthcare promising
the utilization of large language models for ambient documentation. However,
these AI scribes still rely on one-shot, or few-shot prompts for generating
notes after the consultation has ended, employing little to no reasoning. This
risks long notes with an increase in hallucinations, misrepresentation of the
intent of the clinician, and reliance on the proofreading of the clinician to
catch errors. A dangerous combination for patient safety if vigilance is
compromised by workload and fatigue. In this paper, we introduce a method for
extracting salient clinical information in real-time alongside the healthcare
consultation, denoted Facts, and use that information recursively to generate
the final note. The FactsR method results in more accurate and concise notes by
placing the clinician-in-the-loop of note generation, while opening up new use
cases within real-time decision support.

</details>


### [118] [Schreier-Coset Graph Propagation](https://arxiv.org/abs/2505.10392)
*Aryan Mishra,Lizhen Lin*

Main category: cs.LG

TL;DR: SCGP是一种基于群论的图神经网络增强方法，通过Schreier-coset嵌入提升节点特征，解决了GNN中的信息压缩问题，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: GNN在处理图数据时，由于固定大小的向量压缩了远距离节点的信息，导致表达能力受限。现有方法如Cayley图虽然有效但扩展性差，SCGP旨在解决这一矛盾。

Method: SCGP通过Schreier-coset嵌入增强节点特征，不改变输入图结构，直接在特征空间嵌入无瓶颈连接模式。

Result: 实验表明SCGP在标准任务上的性能可匹敌或超越基线方法，尤其擅长处理分层和模块化图结构，且在推理延迟和内存占用上有优势。

Conclusion: SCGP提供了一种高效且可扩展的GNN增强方案，适合实时和资源受限的应用场景。

Abstract: Graph Neural Networks (GNNs) offer a principled framework for learning over
graph-structured data, yet their expressive capacity is often hindered by
over-squashing, wherein information from distant nodes is compressed into
fixed-size vectors. Existing solutions, including graph rewiring and
bottleneck-resistant architectures such as Cayley and expander graphs, avoid
this problem but introduce scalability bottlenecks. In particular, the Cayley
graphs constructed over $SL(2,\mathbb{Z}_n)$ exhibit strong theoretical
properties, yet suffer from cubic node growth $O(n^3)$, leading to high memory
usage. To address this, this work introduces Schrier-Coset Graph Propagation
(SCGP), a group-theoretic augmentation method that enriches node features
through Schreier-coset embeddings without altering the input graph topology.
SCGP embeds bottleneck-free connectivity patterns into a compact feature space,
improving long-range message passing while maintaining computational
efficiency. Empirical evaluations across standard node and graph classification
benchmarks demonstrate that SCGP achieves performance comparable to, or
exceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits
particular advantages in processing hierarchical and modular graph structures,
offering reduced inference latency, improved scalability, and a low memory
footprint, making it suitable for real-time and resource-constrained
applications.

</details>


### [119] [Two-Stage Generative Model for Intracranial Aneurysm Meshes with Morphological Marker Conditioning](https://arxiv.org/abs/2505.10407)
*Wenhao Ding,Choon Hwai Yap,Kangjun Ji,Simão Castro*

Main category: cs.LG

TL;DR: AneuG是一个两阶段的变分自编码器(VAE)模型，用于生成颅内动脉瘤(IA)的网格几何形状，解决了现有方法无法捕捉真实IA特征和忽略与母血管关系的问题，并能按特定形态测量生成形状。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模的IA图像数据集，现有方法难以生成具有生理真实感的IA形状，且无法控制生成形状的特定形态测量。

Method: AneuG分为两个阶段：第一阶段通过图谐变形(GHD)令牌编码和重建动脉瘤囊形状，第二阶段基于GHD令牌生成母血管，包括血管中心线和横截面传播。

Result: AneuG能够生成具有特定临床相关形态测量的IA形状，适用于研究形状变化对流体动力学的影响。

Conclusion: AneuG为IA形状生成提供了有效的解决方案，支持临床研究和血流模拟研究。

Abstract: A generative model for the mesh geometry of intracranial aneurysms (IA) is
crucial for training networks to predict blood flow forces in real time, which
is a key factor affecting disease progression. This need is necessitated by the
absence of a large IA image datasets. Existing shape generation methods
struggle to capture realistic IA features and ignore the relationship between
IA pouches and parent vessels, limiting physiological realism and their
generation cannot be controlled to have specific morphological measurements. We
propose AneuG, a two-stage Variational Autoencoder (VAE)-based IA mesh
generator. In the first stage, AneuG generates low-dimensional Graph Harmonic
Deformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes,
constrained to morphing energy statistics truths. GHD enables more accurate
shape encoding than alternatives. In the second stage, AneuG generates parent
vessels conditioned on GHD tokens, by generating vascular centreline and
propagating the cross-section. AneuG's IA shape generation can further be
conditioned to have specific clinically relevant morphological measurements.
This is useful for studies to understand shape variations represented by
clinical measurements, and for flow simulation studies to understand effects of
specific clinical shape parameters on fluid dynamics. Source code and
implementation details are available at
https://github.com/anonymousaneug/AneuG.

</details>


### [120] [Decomposed Inductive Procedure Learning: Learning Academic Tasks with Human-Like Data Efficiency](https://arxiv.org/abs/2505.10422)
*Daniel Weitekamp,Christopher MacLellan,Erik Harpstead,Kenneth Koedinger*

Main category: cs.LG

TL;DR: The paper explores how human learning efficiency might stem from using multiple specialized mechanisms, unlike single-mechanism neural networks. An ablation analysis shows that a 3-mechanism symbolic rule induction approach outperforms reinforcement learning in data efficiency, aligning closer to human learning.


<details>
  <summary>Details</summary>
Motivation: The study aims to bridge the gap between human and machine learning efficiency by investigating whether multiple specialized learning mechanisms, as in humans, can enhance data efficiency in neural networks.

Method: The researchers conducted an ablation analysis comparing reinforcement learning to a 3-mechanism symbolic rule induction approach in online tutoring environments.

Result: Decomposing learning into multiple mechanisms significantly improved data efficiency, matching human learning rates, and had a greater impact than the symbolic vs. subsymbolic distinction alone.

Conclusion: Integrating multiple specialized learning mechanisms could be crucial for aligning machine learning efficiency with human learning.

Abstract: Human learning relies on specialization -- distinct cognitive mechanisms
working together to enable rapid learning. In contrast, most modern neural
networks rely on a single mechanism: gradient descent over an objective
function. This raises the question: might human learners' relatively rapid
learning from just tens of examples instead of tens of thousands in data-driven
deep learning arise from our ability to use multiple specialized mechanisms of
learning in combination? We investigate this question through an ablation
analysis of inductive human learning simulations in online tutoring
environments. Comparing reinforcement learning to a more data-efficient
3-mechanism symbolic rule induction approach, we find that decomposing learning
into multiple distinct mechanisms significantly improves data efficiency,
bringing it in line with human learning. Furthermore, we show that this
decomposition has a greater impact on efficiency than the distinction between
symbolic and subsymbolic learning alone. Efforts to align data-driven machine
learning with human learning often overlook the stark difference in learning
efficiency. Our findings suggest that integrating multiple specialized learning
mechanisms may be key to bridging this gap.

</details>


### [121] [The Power of Random Features and the Limits of Distribution-Free Gradient Descent](https://arxiv.org/abs/2505.10423)
*Ari Karchmer,Eran Malach*

Main category: cs.LG

TL;DR: 研究了参数模型梯度优化与随机特征线性组合优化的关系，发现若参数模型能通过mini-batch SGD学习，则目标函数也可用多项式数量随机特征组合逼近，揭示了梯度下降训练中分布无关学习的局限性。


<details>
  <summary>Details</summary>
Motivation: 探索梯度优化与随机特征组合优化的关系，揭示神经网络梯度下降训练中分布无关学习的理论限制，强调数据分布假设的重要性。

Method: 结合理论分析，引入新框架“平均概率维度复杂度”（ADC），证明其与统计查询维度存在多项式关系，并与标准维度复杂度对比。

Result: 证明了参数模型梯度优化与随机特征线性组合优化的等价性，ADC与统计查询维度的多项式关系，以及ADC与标准维度复杂度的无限分离。

Conclusion: 分布无关学习在神经网络梯度下降中存在理论限制，数据分布假设至关重要；ADC为理解学习算法的维度复杂度提供了新工具。

Abstract: We study the relationship between gradient-based optimization of parametric
models (e.g., neural networks) and optimization of linear combinations of
random features. Our main result shows that if a parametric model can be
learned using mini-batch stochastic gradient descent (bSGD) without making
assumptions about the data distribution, then with high probability, the target
function can also be approximated using a polynomial-sized combination of
random features. The size of this combination depends on the number of gradient
steps and numerical precision used in the bSGD process. This finding reveals
fundamental limitations of distribution-free learning in neural networks
trained by gradient descent, highlighting why making assumptions about data
distributions is often crucial in practice. Along the way, we also introduce a
new theoretical framework called average probabilistic dimension complexity
(adc), which extends the probabilistic dimension complexity developed by Kamath
et al. (2020). We prove that adc has a polynomial relationship with statistical
query dimension, and use this relationship to demonstrate an infinite
separation between adc and standard dimension complexity.

</details>


### [122] [Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs](https://arxiv.org/abs/2505.10425)
*Jingyao Wang,Wenwen Qiang,Zeen Song,Changwen Zheng,Hui Xiong*

Main category: cs.LG

TL;DR: L2T是一个强化微调框架，通过信息论方法优化LLM的推理效率，减少计算负担，同时保持高推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在推理效果与计算效率之间存在权衡，常导致不必要的长推理链和资源浪费，需要一种更高效的推理优化方法。

Method: 提出L2T框架，将查询-响应交互分层为多个片段，利用密集过程奖励量化信息增益，无需额外标注，通过PAC-Bayes界和Fisher信息矩阵快速估计奖励。

Result: 理论分析显示L2T显著降低计算复杂度，实验证明其在多任务中提升推理效果与效率。

Conclusion: L2T通过强化学习优化推理过程，高效利用资源，适用于多种基础模型和任务。

Abstract: Large language models (LLMs) excel at complex tasks thanks to advances in
reasoning abilities. However, existing methods overlook the trade-off between
reasoning effectiveness and computational efficiency, often encouraging
unnecessarily long reasoning chains and wasting tokens. To address this, we
propose Learning to Think (L2T), an information-theoretic reinforcement
fine-tuning framework for LLMs to make the models achieve optimal reasoning
with fewer tokens. Specifically, L2T treats each query-response interaction as
a hierarchical session of multiple episodes and proposes a universal dense
process reward, i.e., quantifies the episode-wise information gain in
parameters, requiring no extra annotations or task-specific evaluators. We
propose a method to quickly estimate this reward based on PAC-Bayes bounds and
the Fisher information matrix. Theoretical analyses show that it significantly
reduces computational complexity with high estimation accuracy. By immediately
rewarding each episode's contribution and penalizing excessive updates, L2T
optimizes the model via reinforcement learning to maximize the use of each
episode and achieve effective updates. Empirical results on various reasoning
benchmarks and base models demonstrate the advantage of L2T across different
tasks, boosting both reasoning effectiveness and efficiency.

</details>


### [123] [Score-based diffusion nowcasting of GOES imagery](https://arxiv.org/abs/2505.10432)
*Randy J. Chase,Katherine Haynes,Lander Ver Hoef,Imme Ebert-Uphoff*

Main category: cs.LG

TL;DR: 摘要：论文探索了基于分数的扩散模型（Diff、CorrDiff、LDM）在云和降水的短时预报（0-3小时）中的应用，发现CorrDiff表现最佳，优于传统U-Net和持续性预报，并能生成高质量集合预报。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报对云和降水的模拟因次网格参数化而具挑战性。早期机器学习方法常生成模糊预报，故探索新方法提升预报质量。

Method: 采用三种扩散模型（标准分数扩散Diff、残差校正扩散CorrDiff、潜在扩散LDM），利用过去20分钟红外卫星图像进行短时预报。

Result: 扩散模型不仅能平流现有云层，还能生成和消散云（包括对流初生）。CorrDiff在均方根误差上优于其他模型及U-Net、持续性预报1-2K。

Conclusion: 扩散模型（尤其是CorrDiff）显著提升了短时降水预报的精度和细节保留能力，同时支持集合生成，展现良好校准性。

Abstract: Clouds and precipitation are important for understanding weather and climate.
Simulating clouds and precipitation with traditional numerical weather
prediction is challenging because of the sub-grid parameterizations required.
Machine learning has been explored for forecasting clouds and precipitation,
but early machine learning methods often created blurry forecasts. In this
paper we explore a newer method, named score-based diffusion, to nowcast (zero
to three hour forecast) clouds and precipitation. We discuss the background and
intuition of score-based diffusion models - thus providing a starting point for
the community - while exploring the methodology's use for nowcasting
geostationary infrared imagery. We experiment with three main types of
diffusion models: a standard score-based diffusion model (Diff); a residual
correction diffusion model (CorrDiff); and a latent diffusion model (LDM). Our
results show that the diffusion models are able to not only advect existing
clouds, but also generate and decay clouds, including convective initiation.
These results are surprising because the forecasts are initiated with only the
past 20 mins of infrared satellite imagery. A case study qualitatively shows
the preservation of high resolution features longer into the forecast than a
conventional mean-squared error trained U-Net. The best of the three diffusion
models tested was the CorrDiff approach, outperforming all other diffusion
models, the traditional U-Net, and a persistence forecast by one to two kelvin
on root mean squared error. The diffusion models also enable out-of-the-box
ensemble generation, which shows skillful calibration, with the spread of the
ensemble correlating well to the error.

</details>


### [124] [Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model](https://arxiv.org/abs/2505.10438)
*David Grasev*

Main category: cs.LG

TL;DR: 研究通过数据驱动方法改进燃气轮机建模与控制，提出基于Koopman特征的全局控制器，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统燃气轮机建模依赖难以获取的性能参数且需简化假设，本文旨在通过闭环运行数据解决这一问题。

Method: 采用稀疏非线性动态识别和Koopman特征空间映射，结合元启发式算法优化特征值，设计全局非线性反馈控制器和卡尔曼估计器。

Result: Koopman控制器在跟踪和抗干扰方面优于经典PI和增益调度控制器，适用于多种飞行条件。

Conclusion: 数据驱动的Koopman方法提供了全局优化潜力，显著提升了燃气轮机控制性能。

Abstract: Gas turbine engines represent complex highly nonlinear dynamical systems.
Deriving their physics-based models can be challenging as it requires
performance characteristics, that are not always available, and one often has
to make many simplifying assumptions. In this paper, the limitations of
conventional experimental methods used to derive component-level and locally
linear parameter-varying models are discussed and addressed by employing
identification techniques based on data collected from standard engine
operation under closed-loop control. The rotor dynamics were estimated using
the sparse identification of nonlinear dynamics. Subsequently, the autonomous
part of the dynamics was mapped into an optimally constructed Koopman
eigenfunction space. The process included eigenvalue optimization using
metaheuristic algorithms and temporal projection, followed by gradient-based
eigenfunction identification. The resulting Koopman model was validated against
an in-house reference component-level model. A globally optimal nonlinear
feedback controller and a Kalman estimator were then designed in the
eigenfunction space and compared to the classical and gain-scheduled
proportional-integral controllers, as well as a proposed internal model control
approach. The eigenmode structure allowed targeting individual modes during the
optimization process, resulting in a better performance tuning. The results
showed that the Koopman-based controller outperformed the other benchmark
controllers in both reference tracking and disturbance rejection, under
sea-level and varying flight conditions, due to its global nature.

</details>


### [125] [PIF: Anomaly detection via preference embedding](https://arxiv.org/abs/2505.10441)
*Filippo Leveni,Luca Magri,Giacomo Boracchi,Cesare Alippi*

Main category: cs.LG

TL;DR: 提出了一种名为PIF的新型异常检测方法，结合自适应隔离方法和偏好嵌入的优势，通过高维空间嵌入数据并利用PI-Forest树状方法计算异常分数。实验证明PIF优于现有技术，尤其在偏好空间中的距离测量和点隔离方面表现更佳。


<details>
  <summary>Details</summary>
Motivation: 针对结构化模式中的异常检测问题，提出PIF方法以结合自适应隔离方法的优势与偏好嵌入的灵活性。

Method: 利用高维空间嵌入数据，并采用PI-Forest树状方法计算异常分数。

Result: 实验表明，PIF在合成和真实数据集上优于现有异常检测技术，尤其在偏好空间中的表现更为突出。

Conclusion: PIF方法在异常检测方面表现出色，尤其是在处理结构化模式和偏好空间时更具优势。

Abstract: We address the problem of detecting anomalies with respect to structured
patterns. To this end, we conceive a novel anomaly detection method called PIF,
that combines the advantages of adaptive isolation methods with the flexibility
of preference embedding. Specifically, we propose to embed the data in a high
dimensional space where an efficient tree-based method, PI-Forest, is employed
to compute an anomaly score. Experiments on synthetic and real datasets
demonstrate that PIF favorably compares with state-of-the-art anomaly detection
techniques, and confirm that PI-Forest is better at measuring arbitrary
distances and isolate points in the preference space.

</details>


### [126] [SEAL: Searching Expandable Architectures for Incremental Learning](https://arxiv.org/abs/2505.10457)
*Matteo Gambella,Vicente Javier Castro Solar,Manuel Roveri*

Main category: cs.LG

TL;DR: SEAL是一个基于NAS的增量学习框架，通过动态调整模型结构和选择性扩展，平衡学习新任务和保留旧知识的能力，同时在模型大小和性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 增量学习的核心挑战是平衡模型的'可塑性'（学习新任务）和'稳定性'（保留旧知识）。现有NAS方法常依赖模型扩展，导致资源浪费。SEAL旨在通过动态结构调整和选择性扩展解决这一问题。

Method: SEAL结合NAS动态调整模型结构，仅在必要时扩展模型，并通过容量估计指标决定扩展时机。稳定性通过每步扩展后的交叉蒸馏训练保持。NAS同时搜索最优架构和扩展策略。

Result: 在多个基准测试中，SEAL显著减少遗忘并提高准确性，同时保持较小模型规模，优于现有方法。

Conclusion: SEAL展示了结合NAS和选择性扩展在增量学习中的潜力，实现了高效、自适应的学习，适用于资源受限场景。

Abstract: Incremental learning is a machine learning paradigm where a model learns from
a sequential stream of tasks. This setting poses a key challenge: balancing
plasticity (learning new tasks) and stability (preserving past knowledge).
Neural Architecture Search (NAS), a branch of AutoML, automates the design of
the architecture of Deep Neural Networks and has shown success in static
settings. However, existing NAS-based approaches to incremental learning often
rely on expanding the model at every task, making them impractical in
resource-constrained environments. In this work, we introduce SEAL, a NAS-based
framework tailored for data-incremental learning, a scenario where disjoint
data samples arrive sequentially and are not stored for future access. SEAL
adapts the model structure dynamically by expanding it only when necessary,
based on a capacity estimation metric. Stability is preserved through
cross-distillation training after each expansion step. The NAS component
jointly searches for both the architecture and the optimal expansion policy.
Experiments across multiple benchmarks demonstrate that SEAL effectively
reduces forgetting and enhances accuracy while maintaining a lower model size
compared to prior methods. These results highlight the promise of combining NAS
and selective expansion for efficient, adaptive learning in incremental
scenarios.

</details>


### [127] [Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps](https://arxiv.org/abs/2505.10482)
*Ningyuan Yang,Jiaxuan Gao,Feng Gao,Yi Wu,Chao Yu*

Main category: cs.LG

TL;DR: NCDPO是一种新框架，通过将扩散策略重新定义为带噪声条件的确定性策略，解决了扩散策略在强化学习微调中样本效率低的问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在演示数据覆盖不足时会生成次优轨迹甚至失败，而现有的RL方法难以有效适应扩散模型，需解决其计算复杂性和优化目标困难。

Method: 引入NCDPO框架，将扩散策略转化为带噪声条件的确定性策略，支持可处理的似然估计和梯度反向传播。

Result: NCDPO在样本效率和最终性能上优于现有方法，适用于机器人控制和多代理游戏场景。

Conclusion: NCDPO通过改进扩散策略的RL适应性，显著提升了性能和效率。

Abstract: Diffusion policies, widely adopted in decision-making scenarios such as
robotics, gaming and autonomous driving, are capable of learning diverse skills
from demonstration data due to their high representation power. However, the
sub-optimal and limited coverage of demonstration data could lead to diffusion
policies that generate sub-optimal trajectories and even catastrophic failures.
While reinforcement learning (RL)-based fine-tuning has emerged as a promising
solution to address these limitations, existing approaches struggle to
effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This
challenge stems from the computational intractability of action likelihood
estimation during the denoising process, which leads to complicated
optimization objectives. In our experiments starting from randomly initialized
policies, we find that online tuning of Diffusion Policies demonstrates much
lower sample efficiency compared to directly applying PPO on MLP policies
(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework
that reformulates Diffusion Policy as a noise-conditioned deterministic policy.
By treating each denoising step as a differentiable transformation conditioned
on pre-sampled noise, NCDPO enables tractable likelihood evaluation and
gradient backpropagation through all diffusion timesteps. Our experiments
demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when
training from scratch, outperforming existing methods in both sample efficiency
and final performance across diverse benchmarks, including continuous robot
control and multi-agent game scenarios. Furthermore, our experimental results
show that our method is robust to the number denoising timesteps in the
Diffusion Policy.

</details>


### [128] [Fixing Incomplete Value Function Decomposition for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.10484)
*Andrea Baisero,Rupali Bhati,Shuo Liu,Aathira Pillai,Christopher Amato*

Main category: cs.LG

TL;DR: 该论文提出了一种新的价值函数分解方法QFIX，通过简单的“修复”层扩展了现有模型的表示能力，并在多个环境中验证了其性能和稳定性优于现有方法（如QPLEX）。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如VDN、QMIX）在满足个体-全局最大值（IGM）属性时表示能力有限，而QPLEX虽然无此限制但过于复杂。因此，需要一种既能满足IGM又能简化模型的方法。

Method: 提出QFIX方法，通过简单的“修复”层扩展表示能力，并在两个多智能体框架中实现三种变体。

Result: 在SMACv2和Overcooked环境中，QFIX显著提升了性能，学习更稳定，表现优于QPLEX，且模型更简单轻量。

Conclusion: QFIX成功解决了现有方法的限制，提供了一种高效、稳定且简化的价值函数分解方案。

Abstract: Value function decomposition methods for cooperative multi-agent
reinforcement learning compose joint values from individual per-agent
utilities, and train them using a joint objective. To ensure that the action
selection process between individual utilities and joint values remains
consistent, it is imperative for the composition to satisfy the
individual-global max (IGM) property. Although satisfying IGM itself is
straightforward, most existing methods (e.g., VDN, QMIX) have limited
representation capabilities and are unable to represent the full class of IGM
values, and the one exception that has no such limitation (QPLEX) is
unnecessarily complex. In this work, we present a simple formulation of the
full class of IGM values that naturally leads to the derivation of QFIX, a
novel family of value function decomposition models that expand the
representation capabilities of prior models by means of a thin "fixing" layer.
We derive multiple variants of QFIX, and implement three variants in two
well-known multi-agent frameworks. We perform an empirical evaluation on
multiple SMACv2 and Overcooked environments, which confirms that QFIX (i)
succeeds in enhancing the performance of prior methods, (ii) learns more stably
and performs better than its main competitor QPLEX, and (iii) achieves this
while employing the simplest and smallest mixing models.

</details>


### [129] [PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models](https://arxiv.org/abs/2505.10515)
*Seongun Kim,Sol A Kim,Geonhyeong Kim,Enver Menadjiev,Chanwoo Lee,Seongwook Chung,Nari Kim,Jaesik Choi*

Main category: cs.LG

TL;DR: PnPXAI是一个通用的可解释人工智能框架，支持多种数据模态和神经网络模型，通过自动检测模型架构、推荐解释方法并优化超参数来解决现有XAI方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法因硬编码实现、支持的XAI方法有限以及缺乏评估和优化阶段，限制了其在实际应用中的使用，需要一种更灵活的解决方案。

Method: 提出PnPXAI框架，通过Plug-and-Play方式自动检测模型架构、推荐解释方法并优化超参数，支持多种数据模态和模型。

Result: 通过用户调查验证了PnPXAI的有效性，并展示了其在医学和金融等领域的广泛适用性。

Conclusion: PnPXAI通过其通用性和自动化功能，解决了现有XAI框架的局限性，并有望推动XAI技术在现实应用中的普及。

Abstract: Recently, post hoc explanation methods have emerged to enhance model
transparency by attributing model outputs to input features. However, these
methods face challenges due to their specificity to certain neural network
architectures and data modalities. Existing explainable artificial intelligence
(XAI) frameworks have attempted to address these challenges but suffer from
several limitations. These include limited flexibility to diverse model
architectures and data modalities due to hard-coded implementations, a
restricted number of supported XAI methods because of the requirements for
layer-specific operations of attribution methods, and sub-optimal
recommendations of explanations due to the lack of evaluation and optimization
phases. Consequently, these limitations impede the adoption of XAI technology
in real-world applications, making it difficult for practitioners to select the
optimal explanation method for their domain. To address these limitations, we
introduce \textbf{PnPXAI}, a universal XAI framework that supports diverse data
modalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI
automatically detects model architectures, recommends applicable explanation
methods, and optimizes hyperparameters for optimal explanations. We validate
the framework's effectiveness through user surveys and showcase its versatility
across various domains, including medicine and finance.

</details>


### [130] [Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug Design](https://arxiv.org/abs/2505.10545)
*Amira Alakhdar,Barnabas Poczos,Newell Washburn*

Main category: cs.LG

TL;DR: 该论文提出了PharmaDiff模型，这是一种结合药效团条件扩散的3D分子生成方法，能够基于药效团假设精确生成分子结构，并在无蛋白结构的情况下实现高对接分数。


<details>
  <summary>Details</summary>
Motivation: 药物发现中，开发具有生物活性的分子通常耗时且成本高昂，尤其是对于缺乏结构或功能数据的新靶点。药效团建模为捕捉分子生物活性关键特征提供了替代方案。

Method: PharmaDiff采用基于Transformer的架构，将3D药效团的原子表示整合到生成过程中，实现了与预设药效团假设对齐的3D分子图的精确生成。

Result: PharmaDiff在匹配3D药效团约束上表现优于基于配体的药物设计方法，并在无靶蛋白结构的情况下实现了更高的对接分数。

Conclusion: 通过将药效团建模与3D生成技术结合，PharmaDiff为理性药物设计提供了强大且灵活的框架。

Abstract: Developing bioactive molecules remains a central, time- and cost-heavy
challenge in drug discovery, particularly for novel targets lacking structural
or functional data. Pharmacophore modeling presents an alternative for
capturing the key features required for molecular bioactivity against a
biological target. In this work, we present PharmaDiff, a
pharmacophore-conditioned diffusion model for 3D molecular generation.
PharmaDiff employs a transformer-based architecture to integrate an atom-based
representation of the 3D pharmacophore into the generative process, enabling
the precise generation of 3D molecular graphs that align with predefined
pharmacophore hypotheses. Through comprehensive testing, PharmaDiff
demonstrates superior performance in matching 3D pharmacophore constraints
compared to ligand-based drug design methods. Additionally, it achieves higher
docking scores across a range of proteins in structure-based drug design,
without the need for target protein structures. By integrating pharmacophore
modeling with 3D generative techniques, PharmaDiff offers a powerful and
flexible framework for rational drug design.

</details>


### [131] [An AI-driven framework for the prediction of personalised health response to air pollution](https://arxiv.org/abs/2505.10556)
*Nazanin Zounemat Kermani,Sadjad Naderi,Claire H. Dilliway,Claire E. Heaney,Shrreya Behll,Boyang Chen,Hisham Abubakar-Waziri,Alexandra E. Porter,Marc Chadeau-Hyam,Fangxin Fang,Ian M. Adcock,Kian Fan Chung,Christopher C. Pain*

Main category: cs.LG

TL;DR: 该论文提出了一种利用可穿戴设备数据和实时环境暴露信息，通过对抗自编码器神经网络预测个性化健康反应的新方法，有效捕捉了污染的非线性影响。


<details>
  <summary>Details</summary>
Motivation: 空气污染对公共健康构成严重威胁，加之极端天气加剧了污染效应。结合个人传感和AI技术进步，论文旨在通过整合可穿戴设备数据和环境暴露信息，实现个性化健康反应的监控与预测。

Method: 采用对抗自编码器神经网络，整合来自可穿戴设备的生理数据和实时环境暴露数据，通过云模块化框架训练模型，并应用迁移学习提高泛化能力。

Result: AI模型能准确重建时间依赖的健康信号，并捕捉污染的非线性反应，迁移学习的应用增强了模型对真实世界数据的适应性。

Conclusion: 该方法为个性化健康管理提供了有效工具，展现了人工智能在健康预测中的潜力，同时强调了数据处理的安全性和伦理性。

Abstract: Air pollution poses a significant threat to public health, causing or
exacerbating many respiratory and cardiovascular diseases. In addition, climate
change is bringing about more extreme weather events such as wildfires and
heatwaves, which can increase levels of pollution and worsen the effects of
pollution exposure. Recent advances in personal sensing have transformed the
collection of behavioural and physiological data, leading to the potential for
new improvements in healthcare. We wish to capitalise on this data, alongside
new capabilities in AI for making time series predictions, in order to monitor
and predict health outcomes for an individual. Thus, we present a novel
workflow for predicting personalised health responses to pollution by
integrating physiological data from wearable fitness devices with real-time
environmental exposures. The data is collected from various sources in a secure
and ethical manner, and is used to train an AI model to predict individual
health responses to pollution exposure within a cloud-based, modular framework.
We demonstrate that the AI model -- an Adversarial Autoencoder neural network
in this case -- accurately reconstructs time-dependent health signals and
captures nonlinear responses to pollution. Transfer learning is applied using
data from a personal smartwatch, which increases the generalisation abilities
of the AI model and illustrates the adaptability of the approach to real-world,
user-generated data.

</details>


### [132] [Neural Thermodynamic Laws for Large Language Model Training](https://arxiv.org/abs/2505.10559)
*Ziming Liu,Yizhou Liu,Jeff Gore,Max Tegmark*

Main category: cs.LG

TL;DR: 论文提出了神经热力学定律（NTL）框架，从理论和实践角度探索大语言模型的训练动态，揭示了热力学量与经典原则在训练中的自然涌现，并提供了学习率调度的实用指南。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型训练过程中的潜在规律，超越现有的神经缩放定律，为训练动态提供新的科学视角和实用指导。

Method: 引入神经热力学定律（NTL）框架，通过河谷损失景观假设，理论推导热力学量（如温度、熵）和经典热力学原则在训练中的自然表现。

Result: 理论证明了热力学量和原则在训练中的适用性，并在实践中提供了直观的学习率调度设计指南。

Conclusion: NTL框架为大语言模型训练提供了新的科学基础和实用工具，填补了现有理论的空白。

Abstract: Beyond neural scaling laws, little is known about the laws underlying large
language models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new
framework that offers fresh insights into LLM training dynamics. On the
theoretical side, we demonstrate that key thermodynamic quantities (e.g.,
temperature, entropy, heat capacity, thermal conduction) and classical
thermodynamic principles (e.g., the three laws of thermodynamics and the
equipartition theorem) naturally emerge under river-valley loss landscape
assumptions. On the practical side, this scientific perspective yields
intuitive guidelines for designing learning rate schedules.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [133] [Study and improvement of search algorithms in two-players perfect information games](https://arxiv.org/abs/2505.09639)
*Quentin Cohen-Solal*

Main category: cs.AI

TL;DR: 该论文提出了一个新的搜索算法，用于双人零和完美信息游戏，并在短时间和中等搜索时间内显著优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对游戏搜索算法性能泛用性的研究，尤其是在双人零和完美信息游戏中，因此作者希望填补这一空白。

Method: 提出了一种新的搜索算法，并通过在22种游戏上进行大规模实验，比较了该算法与其他算法在不同搜索时间下的性能。

Result: 在短搜索时间内，新算法在所有测试游戏中表现最佳；在中等搜索时间内，新算法在17/22的游戏中优于其他算法。

Conclusion: 新算法在双人零和完美信息游戏中具有显著的性能优势，尤其在短时间和中等搜索时间内表现突出。

Abstract: Games, in their mathematical sense, are everywhere (game industries,
economics, defense, education, chemistry, biology, ...).Search algorithms in
games are artificial intelligence methods for playing such games.
Unfortunately, there is no study on these algorithms that evaluates the
generality of their performance. We propose to address this gap in the case of
two-player zero-sum games with perfect information. Furthermore, we propose a
new search algorithm and we show that, for a short search time, it outperforms
all studied algorithms on all games in this large experiment and that, for a
medium search time, it outperforms all studied algorithms on 17 of the 22
studied games.

</details>


### [134] [Feature Relevancy, Necessity and Usefulness: Complexity and Algorithms](https://arxiv.org/abs/2505.09640)
*Tomás Capdevielle,Santiago Cifuentes*

Main category: cs.AI

TL;DR: 论文改进了现有技术，用于高效识别分类模型中的关键特征，特别是在神经网络中，并提出了新的全局重要性概念。


<details>
  <summary>Details</summary>
Motivation: 现有方法在识别关键特征时效率不足，尤其是在复杂模型中，因此需要改进算法并引入新的全局重要性概念。

Method: 通过命题逻辑和充分理由概念，改进现有技术，提出新算法以高效检测必要和相关性特征，并引入全局有用性概念。

Result: 实验证明，新方法在决策树和复杂模型中能高效检测特征重要性，并在三个数据集中验证了实用性。

Conclusion: 论文提出的改进算法和新概念能高效识别关键特征，适用于复杂模型，具有实际应用价值。

Abstract: Given a classification model and a prediction for some input, there are
heuristic strategies for ranking features according to their importance in
regard to the prediction. One common approach to this task is rooted in
propositional logic and the notion of \textit{sufficient reason}. Through this
concept, the categories of relevant and necessary features were proposed in
order to identify the crucial aspects of the input. This paper improves the
existing techniques and algorithms for deciding which are the relevant and/or
necessary features, showing in particular that necessity can be detected
efficiently in complex models such as neural networks. We also generalize the
notion of relevancy and study associated problems. Moreover, we present a new
global notion (i.e. that intends to explain whether a feature is important for
the behavior of the model in general, not depending on a particular input) of
\textit{usefulness} and prove that it is related to relevancy and necessity.
Furthermore, we develop efficient algorithms for detecting it in decision trees
and other more complex models, and experiment on three datasets to analyze its
practical utility.

</details>


### [135] [General Dynamic Goal Recognition](https://arxiv.org/abs/2505.09737)
*Osher Elhadad,Reuth Mirsky*

Main category: cs.AI

TL;DR: 论文提出了一种名为‘General Dynamic GR’的新问题定义，旨在解决动态环境中目标识别（GR）的挑战，并采用无模型的目标导向强化学习方法实现快速适应。


<details>
  <summary>Details</summary>
Motivation: 传统GR方法在动态环境中难以适应多变的目标，因此需要一种更灵活的方法来实现实时目标识别。

Method: 采用无模型的目标导向强化学习（RL）方法，以快速适应不同任务的变化。

Result: 提出的方法能够有效适应动态环境中的目标识别任务。

Conclusion: General Dynamic GR问题定义及无模型RL方法为动态环境中的目标识别提供了新的研究方向和实践方案。

Abstract: Understanding an agent's intent through its behavior is essential in
human-robot interaction, interactive AI systems, and multi-agent
collaborations. This task, known as Goal Recognition (GR), poses significant
challenges in dynamic environments where goals are numerous and constantly
evolving. Traditional GR methods, designed for a predefined set of goals, often
struggle to adapt to these dynamic scenarios. To address this limitation, we
introduce the General Dynamic GR problem - a broader definition of GR - aimed
at enabling real-time GR systems and fostering further research in this area.
Expanding on this foundation, this paper employs a model-free goal-conditioned
RL approach to enable fast adaptation for GR across various changing tasks.

</details>


### [136] [Explainability Through Human-Centric Design for XAI in Lung Cancer Detection](https://arxiv.org/abs/2505.09755)
*Amy Rafferty,Rishi Ramaesh,Ajitha Rajan*

Main category: cs.AI

TL;DR: 该论文介绍了XpertXAI模型，一种可解释的深度学习模型，用于从胸片检测多种肺部病变，尤其是肺癌，其解释性优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在肺部病变检测中表现出潜力，但其不透明的决策过程限制了临床应用的广泛采纳。需要通过可解释的模型设计增强临床信任。

Method: 基于ClinicXAI框架，作者提出XpertXAI，采用专家驱动的概念瓶颈模型（CBM），结合InceptionV3分类器和公开胸片数据集，与现有解释方法（如XCBs）对比验证。

Result: XpertXAI不仅预测准确性更高，其概念级解释更符合放射科医生的判断，而现有方法常遗漏关键诊断特征。

Conclusion: XpertXAI展示了可扩展的、以人为中心的模型设计在医学诊断中的潜力，为临床应用中可解释AI提供了路径。

Abstract: Deep learning models have shown promise in lung pathology detection from
chest X-rays, but widespread clinical adoption remains limited due to opaque
model decision-making. In prior work, we introduced ClinicXAI, a human-centric,
expert-guided concept bottleneck model (CBM) designed for interpretable lung
cancer diagnosis. We now extend that approach and present XpertXAI, a
generalizable expert-driven model that preserves human-interpretable clinical
concepts while scaling to detect multiple lung pathologies. Using a
high-performing InceptionV3-based classifier and a public dataset of chest
X-rays with radiology reports, we compare XpertXAI against leading post-hoc
explainability methods and an unsupervised CBM, XCBs. We assess explanations
through comparison with expert radiologist annotations and medical ground
truth. Although XpertXAI is trained for multiple pathologies, our expert
validation focuses on lung cancer. We find that existing techniques frequently
fail to produce clinically meaningful explanations, omitting key diagnostic
features and disagreeing with radiologist judgments. XpertXAI not only
outperforms these baselines in predictive accuracy but also delivers
concept-level explanations that better align with expert reasoning. While our
focus remains on explainability in lung cancer detection, this work illustrates
how human-centric model design can be effectively extended to broader
diagnostic contexts - offering a scalable path toward clinically meaningful
explainable AI in medical diagnostics.

</details>


### [137] [A Multimodal Multi-Agent Framework for Radiology Report Generation](https://arxiv.org/abs/2505.09787)
*Ziruo Yi,Ting Xiao,Mark V. Albert*

Main category: cs.AI

TL;DR: 提出一种多模态多智能体框架，用于放射学报告生成（RRG），通过临床推理流程的分步处理，提升报告的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有RRG方法中存在的幻觉、事实不一致和跨模态对齐问题，以提高临床工作流的效率并减少放射科医生的工作负担。

Method: 采用多模态多智能体框架，任务特定的智能体负责检索、草稿生成、视觉分析、精炼和合成，模仿临床推理流程。

Result: 在自动指标和LLM评估中优于基线方法，生成更准确、结构化且可解释的报告。

Conclusion: 证明了临床对齐的多智能体框架在提升可解释且可信赖的临床AI应用方面的潜力。

Abstract: Radiology report generation (RRG) aims to automatically produce diagnostic
reports from medical images, with the potential to enhance clinical workflows
and reduce radiologists' workload. While recent approaches leveraging
multimodal large language models (MLLMs) and retrieval-augmented generation
(RAG) have achieved strong results, they continue to face challenges such as
factual inconsistency, hallucination, and cross-modal misalignment. We propose
a multimodal multi-agent framework for RRG that aligns with the stepwise
clinical reasoning workflow, where task-specific agents handle retrieval, draft
generation, visual analysis, refinement, and synthesis. Experimental results
demonstrate that our approach outperforms a strong baseline in both automatic
metrics and LLM-based evaluations, producing more accurate, structured, and
interpretable reports. This work highlights the potential of clinically aligned
multi-agent frameworks to support explainable and trustworthy clinical AI
applications.

</details>


### [138] [Offline Reinforcement Learning for Microgrid Voltage Regulation](https://arxiv.org/abs/2505.09920)
*Shan Yang,Yongli Zhu*

Main category: cs.AI

TL;DR: 研究探讨了离线强化学习算法在含太阳能渗透的微电网电压调节中的应用，通过离线训练从历史数据中学习模型，减少对在线环境交互的依赖。


<details>
  <summary>Details</summary>
Motivation: 因技术或安全原因无法进行在线环境交互时，如何利用离线强化学习算法对微电网电压进行有效调节。

Method: 采用不同离线强化学习算法，基于历史数据集进行离线训练，无需在线交互。

Result: 在IEEE 33-总线系统上的实验表明，该方法对包括低质量数据在内的不同离线数据集均有效。

Conclusion: 提出的离线方法能够有效支持微电网电压调节，减少对在线交互的依赖。

Abstract: This paper presents a study on using different offline reinforcement learning
algorithms for microgrid voltage regulation with solar power penetration. When
environment interaction is unviable due to technical or safety reasons, the
proposed approach can still obtain an applicable model through offline-style
training on a previously collected dataset, lowering the negative impact of
lacking online environment interactions. Experiment results on the IEEE 33-bus
system demonstrate the feasibility and effectiveness of the proposed approach
on different offline datasets, including the one with merely low-quality
experience.

</details>


### [139] ["There Is No Such Thing as a Dumb Question," But There Are Good Ones](https://arxiv.org/abs/2505.09923)
*Minjung Shin,Donghyun Kim,Jeh-Kwang Ryu*

Main category: cs.AI

TL;DR: 该研究提出了一个系统性评估问题质量的框架，包括适当性和有效性两个核心维度，并通过半自适应标准实现了结构与灵活性。


<details>
  <summary>Details</summary>
Motivation: 由于问题质量评估的研究不足，研究旨在定义好问题并建立评估框架。

Method: 提出基于适当性和有效性的评分系统，并利用动态上下文变量实现半自适应标准。

Result: 在CAUS和SQUARE数据集上验证了框架的有效性，能评估不同上下文中的问题质量。

Conclusion: 该研究为问题质量评估提供了灵活全面的框架，推动了问题行为与结构化分析方法的结合。

Abstract: Questioning has become increasingly crucial for both humans and artificial
intelligence, yet there remains limited research comprehensively assessing
question quality. In response, this study defines good questions and presents a
systematic evaluation framework. We propose two key evaluation dimensions:
appropriateness (sociolinguistic competence in context) and effectiveness
(strategic competence in goal achievement). Based on these foundational
dimensions, a rubric-based scoring system was developed. By incorporating
dynamic contextual variables, our evaluation framework achieves structure and
flexibility through semi-adaptive criteria. The methodology was validated using
the CAUS and SQUARE datasets, demonstrating the ability of the framework to
access both well-formed and problematic questions while adapting to varied
contexts. As we establish a flexible and comprehensive framework for question
evaluation, this study takes a significant step toward integrating questioning
behavior with structured analytical methods grounded in the intrinsic nature of
questioning.

</details>


### [140] [Demystifying AI Agents: The Final Generation of Intelligence](https://arxiv.org/abs/2505.09932)
*Kevin J McNamara,Rhea Pritham Marpu*

Main category: cs.AI

TL;DR: 本文回顾了AI从简单规则系统到复杂自主代理的发展历程，探讨了促使这一进化的关键技术里程碑，并讨论了当前AI代表如ChatGPT和Grok的潜在意义及其社会影响，强调需要智慧和远见来应对这一新时代的挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 探讨AI技术的快速发展及其对社会的深远影响，强调技术进步与人类智慧相结合的重要性，以应对AI带来的机遇与挑战。

Method: 通过历史回顾和具体案例分析（如ChatGPT和Grok）来阐述AI技术的发展趋势和能力提升。

Result: AI技术已达到高度复杂的自主代理阶段，其发展速度和能力提升对社会产生了深远影响，提示需要智慧和监管以引导其健康发展。

Conclusion: AI技术正处于关键发展阶段，需结合人类智慧与远见，以确保其发展既符合伦理又可持续，同时充分利用其潜力造福社会。

Abstract: The trajectory of artificial intelligence (AI) has been one of relentless
acceleration, evolving from rudimentary rule-based systems to sophisticated,
autonomous agents capable of complex reasoning and interaction. This whitepaper
chronicles this remarkable journey, charting the key technological
milestones--advancements in prompting, training methodologies, hardware
capabilities, and architectural innovations--that have converged to create the
AI agents of today. We argue that these agents, exemplified by systems like
OpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in
AI development, potentially constituting the "final generation" of intelligence
as we currently conceive it. We explore the capabilities and underlying
technologies of these agents, grounded in practical examples, while also
examining the profound societal implications and the unprecedented pace of
progress that suggests intelligence is now doubling approximately every six
months. The paper concludes by underscoring the critical need for wisdom and
foresight in navigating the opportunities and challenges presented by this
powerful new era of intelligence.

</details>


### [141] [Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents](https://arxiv.org/abs/2505.09970)
*Mrinal Rawat,Ambuje Gupta,Rushil Goomer,Alessandro Di Bari,Neha Gupta,Roberto Pieraccini*

Main category: cs.AI

TL;DR: Pre-Act是一种通过多步执行计划和详细推理提升LLM代理性能的新方法，适用于会话和非会话代理。它在任务导向代理的评估中表现优于ReAct，特别在大模型上效果显著，并通过微调小模型解决了其推理能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现代基于LLM的代理系统依赖于ReAct（推理+行动）能力，但现有方法在复杂任务中的表现仍有提升空间。Pre-Act旨在通过细化多步执行计划和动态调整推理过程，进一步提升代理的任务完成能力。

Method: 提出Pre-Act方法：为输入生成多步执行计划和详细推理，逐步整合历史步骤和工具输出，动态优化直至最终响应。采用两级评估框架（单轮和端到端），并在小模型（如Llama 3.1）上微调以验证方法的普适性。

Result: 在Almita数据集上，Pre-Act单轮行动召回率比ReAct高70%；微调后的70B模型在行动准确率（单轮）和目标完成率（端到端）上分别比GPT-4提升69.5%和28%。

Conclusion: Pre-Act通过强化多步推理和动态优化机制，显著提升了代理的性能，尤其在复杂任务中。针对小模型的微调进一步验证了其适用性，为实际部署提供了可行方案。

Abstract: The ReAct (Reasoning + Action) capability in large language models (LLMs) has
become the foundation of modern agentic systems. Recent LLMs, such as
DeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through
the generation of ample intermediate tokens, which help build a strong premise
before producing the final output tokens. In this paper, we introduce Pre-Act,
a novel approach that enhances the agent's performance by creating a multi-step
execution plan along with the detailed reasoning for the given user input. This
plan incrementally incorporates previous steps and tool outputs, refining
itself after each step execution until the final response is obtained. Our
approach is applicable to both conversational and non-conversational agents. To
measure the performance of task-oriented agents comprehensively, we propose a
two-level evaluation framework: (1) turn level and (2) end-to-end. Our
turn-level evaluation, averaged across five models, shows that our approach,
Pre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While
this approach is effective for larger models, smaller models crucial for
practical applications, where latency and cost are key constraints, often
struggle with complex reasoning tasks required for agentic systems. To address
this limitation, we fine-tune relatively small models such as Llama 3.1 (8B &
70B) using the proposed Pre-Act approach. Our experiments show that the
fine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action
accuracy (turn-level) and a 28% improvement in goal completion rate
(end-to-end) on the Almita (out-of-domain) dataset.

</details>


### [142] [The First MPDD Challenge: Multimodal Personality-aware Depression Detection](https://arxiv.org/abs/2505.10034)
*Changzeng Fu,Zelin Fu,Xinhe Kuang,Jiacheng Dong,Qi Zhang,Kaifeng Su,Yikai Su,Wenbo Shi,Junfeng Yao,Yuliang Zhao,Shiqi Zhao,Jiadong Wang,Siyang Song,Chaoran Liu,Yuichiro Yoshikawa,Björn Schuller,Hiroshi Ishiguro*

Main category: cs.AI

TL;DR: 该论文提出了一个多模态人格感知抑郁检测挑战（MPDD），旨在解决现有抑郁检测方法忽略年龄差异和个体多样性的问题，通过融合音频、视频和个人差异信息来提升检测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁检测方法主要关注年轻成年人，忽视了不同年龄群体和个体差异对抑郁表现的影响。因此，研究需要更具包容性和个性化的检测方法。

Method: 研究提出MPDD挑战，分为两个子赛道：Track 1使用MPDD-Elderly数据集检测老年人的抑郁，Track 2使用MPDD-Young数据集检测年轻人的抑郁。基线模型融合了音频、视频模态和个人差异信息。

Result: MPDD挑战为抑郁检测提供了更全面的多模态数据和个人差异因素，旨在推动个性化、准确的抑郁检测方法的发展。

Conclusion: MPDD挑战填补了现有抑郁检测方法的不足，促进了心理健康研究的进步和更具包容性的检测系统的开发。

Abstract: Depression is a widespread mental health issue affecting diverse age groups,
with notable prevalence among college students and the elderly. However,
existing datasets and detection methods primarily focus on young adults,
neglecting the broader age spectrum and individual differences that influence
depression manifestation. Current approaches often establish a direct mapping
between multimodal data and depression indicators, failing to capture the
complexity and diversity of depression across individuals. This challenge
includes two tracks based on age-specific subsets: Track 1 uses the
MPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses
the MPDD-Young dataset for detecting depression in younger participants. The
Multimodal Personality-aware Depression Detection (MPDD) Challenge aims to
address this gap by incorporating multimodal data alongside individual
difference factors. We provide a baseline model that fuses audio and video
modalities with individual difference information to detect depression
manifestations in diverse populations. This challenge aims to promote the
development of more personalized and accurate de pression detection methods,
advancing mental health research and fostering inclusive detection systems.
More details are available on the official challenge website:
https://hacilab.github.io/MPDDChallenge.github.io.

</details>


### [143] [Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs](https://arxiv.org/abs/2505.10074)
*Mohamed Abdelmagied,Mohamed Amine Chatti,Shoeb Joarder,Qurat Ul Ain,Rawaa Alatrash*

Main category: cs.AI

TL;DR: 该论文提出了一种基于图结构的检索增强生成（Graph RAG）方法，结合教育知识图谱（EduKG）和个人知识图谱（PKG），以解决MOOCs中学习者理解新知识概念时的互动不足和LLM幻觉问题。


<details>
  <summary>Details</summary>
Motivation: MOOCs缺乏师生直接互动，学习者依赖LLMs获取知识时可能因幻觉问题导致信息不可靠。RAG虽能缓解此问题，但其在MOOCs中的应用受限于非结构化学习材料，且现有系统无法主动引导学习需求。

Method: 提出Graph RAG管道，利用EduKG和PKG：（1）基于PKG生成个性化问题；（2）基于EduKG的关系回答学习者选择的问题。在CourseMapper平台的3门MOOCs中由3位专家教师进行评估。

Result: 评估结果表明，Graph RAG能有效支持学习者在个性化学习中理解新知识概念。

Conclusion: Graph RAG通过结合知识图谱，为MOOCs中的学习者提供了一种可靠且个性化的知识理解方法。

Abstract: Massive Open Online Courses (MOOCs) lack direct interaction between learners
and instructors, making it challenging for learners to understand new knowledge
concepts. Recently, learners have increasingly used Large Language Models
(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to
hallucinations which limits their reliability. Retrieval-Augmented Generation
(RAG) addresses this issue by retrieving relevant documents before generating a
response. However, the application of RAG across different MOOCs is limited by
unstructured learning material. Furthermore, current RAG systems do not
actively guide learners toward their learning needs. To address these
challenges, we propose a Graph RAG pipeline that leverages Educational
Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide
learners to understand knowledge concepts in the MOOC platform CourseMapper.
Specifically, we implement (1) a PKG-based Question Generation method to
recommend personalized questions for learners in context, and (2) an
EduKG-based Question Answering method that leverages the relationships between
knowledge concepts in the EduKG to answer learner selected questions. To
evaluate both methods, we conducted a study with 3 expert instructors on 3
different MOOCs in the MOOC platform CourseMapper. The results of the
evaluation show the potential of Graph RAG to empower learners to understand
new knowledge concepts in a personalized learning experience.

</details>


### [144] [From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI](https://arxiv.org/abs/2505.10093)
*Hsuan-Lei Shao*

Main category: cs.AI

TL;DR: 论文提出了一种AI辅助的方法，将非结构化的台湾中国研究学术文本转化为结构化、交互式的知识表示，利用生成式AI和大语言模型从1367篇论文中提取实体关系三元组，并通过D3.js可视化系统构建领域知识图谱和向量数据库，实现知识导航和学术发现的范式转变。


<details>
  <summary>Details</summary>
Motivation: 针对台湾中国研究领域积累的庞大学术文献，缺乏系统化整理和分析的需求，提出利用AI技术提升文献的可访问性和知识挖掘效率，重构区域知识系统的学术基础设施。

Method: 应用生成式AI（GAI）和大语言模型（LLMs）从1996-2019年的1367篇同行评议论文中提取实体关系三元组，通过D3.js构建可视化知识图谱和向量数据库，支持网络化知识探索。

Result: 建立了领域特定的知识图谱和向量数据库，揭示了未发现的学术轨迹、主题集群和研究空白，实现了从线性文本到网络化知识导航的范式转变。

Conclusion: 该研究不仅展示了生成式AI在区域研究和数字人文学科中的应用潜力，还为重构区域知识系统的学术基础设施提供了可扩展的数据驱动方案。

Abstract: Taiwanese China Studies (CS) has developed into a rich, interdisciplinary
research field shaped by the unique geopolitical position and long standing
academic engagement with Mainland China. This study responds to the growing
need to systematically revisit and reorganize decades of Taiwan based CS
scholarship by proposing an AI assisted approach that transforms unstructured
academic texts into structured, interactive knowledge representations. We apply
generative AI (GAI) techniques and large language models (LLMs) to extract and
standardize entity relation triples from 1,367 peer reviewed CS articles
published between 1996 and 2019. These triples are then visualized through a
lightweight D3.js based system, forming the foundation of a domain specific
knowledge graph and vector database for the field. This infrastructure allows
users to explore conceptual nodes and semantic relationships across the corpus,
revealing previously uncharted intellectual trajectories, thematic clusters,
and research gaps. By decomposing textual content into graph structured
knowledge units, our system enables a paradigm shift from linear text
consumption to network based knowledge navigation. In doing so, it enhances
scholarly access to CS literature while offering a scalable, data driven
alternative to traditional ontology construction. This work not only
demonstrates how generative AI can augment area studies and digital humanities
but also highlights its potential to support a reimagined scholarly
infrastructure for regional knowledge systems.

</details>


### [145] [A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support](https://arxiv.org/abs/2505.10188)
*Felix Liedeker,Olivia Sanchez-Graillet,Moana Seidler,Christian Brandt,Jörg Wellmer,Philipp Cimiano*

Main category: cs.AI

TL;DR: 该论文探讨了在医疗领域中，不同XAI解释方法如何增强医生对AI预测的信任，并通过用户研究发现最有效的解释类型。


<details>
  <summary>Details</summary>
Motivation: 随着AI在医疗中的应用增加，建立医生与AI系统之间的信任至关重要，需要明确哪些解释能提升透明度和信任。

Method: 通过用户研究，包括医生调查和访谈，评估不同类型AI解释在诊断决策支持中的效果。

Result: 研究识别出对医生最有用的AI解释类型，为提升诊断过程提供了实际见解。

Conclusion: 该研究为医疗AI系统的解释设计提供了指导，帮助增强医生对AI的信任和合作。

Abstract: As the field of healthcare increasingly adopts artificial intelligence, it
becomes important to understand which types of explanations increase
transparency and empower users to develop confidence and trust in the
predictions made by machine learning (ML) systems. In shared decision-making
scenarios where doctors cooperate with ML systems to reach an appropriate
decision, establishing mutual trust is crucial. In this paper, we explore
different approaches to generating explanations in eXplainable AI (XAI) and
make their underlying arguments explicit so that they can be evaluated by
medical experts. In particular, we present the findings of a user study
conducted with physicians to investigate their perceptions of various types of
AI-generated explanations in the context of diagnostic decision support. The
study aims to identify the most effective and useful explanations that enhance
the diagnostic process. In the study, medical doctors filled out a survey to
assess different types of explanations. Further, an interview was carried out
post-survey to gain qualitative insights on the requirements of explanations
incorporated in diagnostic decision support. Overall, the insights gained from
this study contribute to understanding the types of explanations that are most
effective.

</details>


### [146] [MASS: Multi-Agent Simulation Scaling for Portfolio Construction](https://arxiv.org/abs/2505.10278)
*Taian Guo,Haiyang Shen,Jinsheng Huang,Zhengyang Mao,Junyu Luo,Zhuoru Chen,Xuhui Liu,Bingyu Xia,Luchen Liu,Yun Ma,Ming Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为MASS的多智能体投资组合构建方法，通过动态增加智能体数量和大规模模拟来优化市场理解，并采用逆向优化调整智能体分布，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法多局限于纯模拟或固定工作流，限制了适用性和效果，因此需要一种更灵活、可扩展的解决方案。

Method: 采用渐进增加智能体数量的方式，结合逆向优化过程动态调整分布，实现端到端优化。

Result: 在多个A股股票池上优于6种前沿基线，展示了稳定的超额收益。

Conclusion: MASS的范式可推广至其他类似任务，其实现已开源。

Abstract: LLM-based multi-agent has gained significant attention for their potential in
simulation and enhancing performance. However, existing works are limited to
pure simulations or are constrained by predefined workflows, restricting their
applicability and effectiveness. In this paper, we introduce the Multi-Agent
Scaling Simulation (MASS) for portfolio construction. MASS achieves stable and
continuous excess returns by progressively increasing the number of agents for
large-scale simulations to gain a superior understanding of the market and
optimizing agent distribution end-to-end through a reverse optimization
process, rather than relying on a fixed workflow. We demonstrate its
superiority through performance experiments, ablation studies, backtesting
experiments, experiments on updated data and stock pools, scaling experiments,
parameter sensitivity experiments, and visualization experiments, conducted in
comparison with 6 state-of-the-art baselines on 3 challenging A-share stock
pools. We expect the paradigm established by MASS to expand to other tasks with
similar characteristics. The implementation of MASS has been open-sourced at
https://github.com/gta0804/MASS.

</details>


### [147] [Empirically evaluating commonsense intelligence in large language models with large-scale human judgments](https://arxiv.org/abs/2505.10309)
*Tuan Dung Nguyen,Duncan J. Watts,Mark E. Whiting*

Main category: cs.AI

TL;DR: 研究提出了一种新方法评估AI的常识，考虑了人类间的异质性，发现大多数LLM在常识能力上低于人类中位数，且与人类相关性一般。


<details>
  <summary>Details</summary>
Motivation: 现行AI常识评估假设人类常识同质化，但现实中人类间差异显著，需设计更准确的评估方法。

Method: 通过测量模型与人类群体判断的对应关系，评估模型在常识任务中的表现。

Result: 大多数LLM个体常识能力低于人类中位数，且模拟人类群体时相关性一般；小模型表现优于大模型。

Conclusion: 研究呼吁AI模型需适应不同文化背景的人类群体常识差异。

Abstract: Commonsense intelligence in machines is often assessed by static benchmarks
that compare a model's output against human-prescribed correct labels. An
important, albeit implicit, assumption of these labels is that they accurately
capture what any human would think, effectively treating human common sense as
homogeneous. However, recent empirical work has shown that humans vary
enormously in what they consider commonsensical; thus what appears self-evident
to one benchmark designer may not be so to another. Here, we propose a novel
method for evaluating common sense in artificial intelligence (AI),
specifically in large language models (LLMs), that incorporates empirically
observed heterogeneity among humans by measuring the correspondence between a
model's judgment and that of a human population. We first find that, when
treated as independent survey respondents, most LLMs remain below the human
median in their individual commonsense competence. Second, when used as
simulators of a hypothetical population, LLMs correlate with real humans only
modestly in the extent to which they agree on the same set of statements. In
both cases, smaller, open-weight models are surprisingly more competitive than
larger, proprietary frontier models. Our evaluation framework, which ties
commonsense intelligence to its cultural basis, contributes to the growing call
for adapting AI models to human collectivities that possess different, often
incompatible, social stocks of knowledge.

</details>


### [148] [A Comparative Study of SMT and MILP for the Nurse Rostering Problem](https://arxiv.org/abs/2505.10328)
*Alvin Combrink,Stephie Do,Kristofer Bengtsson,Sabino Francesco Roselli,Martin Fabian*

Main category: cs.AI

TL;DR: 研究比较了基于SMT和MILP的求解器（Z3和Gurobi）在医疗排班问题中的表现，发现两者在不同类型问题上各有优势，SMT方法展现出未来研究的潜力。


<details>
  <summary>Details</summary>
Motivation: 医疗排班问题复杂且约束多样，传统数学规划方法效果有限，而SMT在形式验证领域表现优异，因此研究将其应用于排班问题。

Method: 提出通用约束模型，将其分别转化为SMT和MILP问题，使用Z3和Gurobi求解器在学术和实际案例上进行对比。

Result: MILP在高度约束或不可行问题上表现更好，SMT在非高度约束或实际案例中更优，且SMT对约束表达更敏感。

Conclusion: SMT方法为未来医疗排班研究提供了新的方向，需进一步优化约束表达以提高性能。

Abstract: The effects of personnel scheduling on the quality of care and working
conditions for healthcare personnel have been thoroughly documented. However,
the ever-present demand and large variation of constraints make healthcare
scheduling particularly challenging. This problem has been studied for decades,
with limited research aimed at applying Satisfiability Modulo Theories (SMT).
SMT has gained momentum within the formal verification community in the last
decades, leading to the advancement of SMT solvers that have been shown to
outperform standard mathematical programming techniques.
  In this work, we propose generic constraint formulations that can model a
wide range of real-world scheduling constraints. Then, the generic constraints
are formulated as SMT and MILP problems and used to compare the respective
state-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired
rostering problems. Experimental results show how each solver excels for
certain types of problems; the MILP solver generally performs better when the
problem is highly constrained or infeasible, while the SMT solver performs
better otherwise. On real-world inspired problems containing a more varied set
of shifts and personnel, the SMT solver excels. Additionally, it was noted
during experimentation that the SMT solver was more sensitive to the way the
generic constraints were formulated, requiring careful consideration and
experimentation to achieve better performance. We conclude that SMT-based
methods present a promising avenue for future research within the domain of
personnel scheduling.

</details>


### [149] [Plasticity as the Mirror of Empowerment](https://arxiv.org/abs/2505.10361)
*David Abel,Michael Bowling,André Barreto,Will Dabney,Shi Dong,Steven Hansen,Anna Harutyunyan,Khimya Khetarpal,Clare Lyle,Razvan Pascanu,Georgios Piliouras,Doina Precup,Jonathan Richens,Mark Rowland,Tom Schaul,Satinder Singh*

Main category: cs.AI

TL;DR: 本文提出了“可塑性”作为代理受观察影响的通用度量，揭示了其与“赋权”的基本关系，并探讨了两者在代理设计中的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 探讨代理如何受其过去观察的影响，并定义“可塑性”作为量化这种影响的通用度量，与“赋权”形成对偶关系。

Method: 通过引入新的信息论量“广义定向信息”定义可塑性，并验证其与赋权的对偶性及两者的矛盾关系。

Result: 发现可塑性与赋权互为镜像（代理的可塑性等于环境的赋权），且两者在代理设计中存在权衡。

Conclusion: 可塑性和赋权及其关系是理解代理行为的关键，未来代理设计需兼顾两者特性。

Abstract: Agents are minimally entities that are influenced by their past observations
and act to influence future observations. This latter capacity is captured by
empowerment, which has served as a vital framing concept across artificial
intelligence and cognitive science. This former capacity, however, is equally
foundational: In what ways, and to what extent, can an agent be influenced by
what it observes? In this paper, we ground this concept in a universal
agent-centric measure that we refer to as plasticity, and reveal a fundamental
connection to empowerment. Following a set of desiderata on a suitable
definition, we define plasticity using a new information-theoretic quantity we
call the generalized directed information. We show that this new quantity
strictly generalizes the directed information introduced by Massey (1990) while
preserving all of its desirable properties. Our first finding is that
plasticity is the mirror of empowerment: The agent's plasticity is identical to
the empowerment of the environment, and vice versa. Our second finding
establishes a tension between the plasticity and empowerment of an agent,
suggesting that agent design needs to be mindful of both characteristics. We
explore the implications of these findings, and suggest that plasticity,
empowerment, and their relationship are essential to understanding agency.

</details>


### [150] [Evaluating Model Explanations without Ground Truth](https://arxiv.org/abs/2505.10399)
*Kaivalya Rawal,Zihao Fu,Eoin Delaney,Chris Russell*

Main category: cs.AI

TL;DR: 该论文提出了AXE框架，用于评估模型解释的质量，无需依赖真实解释或模型敏感性，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前模型解释评估方法依赖于真实解释或模型敏感性，存在局限性，需要更独立可靠的评估策略。

Method: 提出三个原则指导评估策略，并开发了不依赖真实解释的AXE框架。

Result: AXE在基线比较中表现良好，并能检测解释公平性伪装。

Conclusion: AXE为模型解释评估提供了独立有效的框架，适用于未来研究方向。

Abstract: There can be many competing and contradictory explanations for a single model
prediction, making it difficult to select which one to use. Current explanation
evaluation frameworks measure quality by comparing against ideal "ground-truth"
explanations, or by verifying model sensitivity to important inputs. We outline
the limitations of these approaches, and propose three desirable principles to
ground the future development of explanation evaluation strategies for local
feature importance explanations. We propose a ground-truth Agnostic eXplanation
Evaluation framework (AXE) for evaluating and comparing model explanations that
satisfies these principles. Unlike prior approaches, AXE does not require
access to ideal ground-truth explanations for comparison, or rely on model
sensitivity - providing an independent measure of explanation quality. We
verify AXE by comparing with baselines, and show how it can be used to detect
explanation fairwashing. Our code is available at
https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.

</details>


### [151] [AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge](https://arxiv.org/abs/2505.10468)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.AI

TL;DR: 本文区分了AI代理与Agentic AI，提供了概念分类、应用映射和挑战分析，比较了它们的设计哲学与能力，并提出了增强鲁棒性和可解释性的解决方案。


<details>
  <summary>Details</summary>
Motivation: 澄清AI代理与Agentic AI的区别，为开发更强大、可扩展且可解释的AI系统提供理论支持。

Method: 通过架构演进、操作机制、交互风格和自主性水平的顺序评估，进行对比分析。

Result: 明确了AI代理和Agentic AI的差异，并在应用领域和挑战方面提供了具体解决方案。

Conclusion: 研究为开发新型AI系统提供了清晰的路线图，强调了多代理协作和动态任务分解的重要性。

Abstract: This study critically distinguishes between AI Agents and Agentic AI,
offering a structured conceptual taxonomy, application mapping, and challenge
analysis to clarify their divergent design philosophies and capabilities. We
begin by outlining the search strategy and foundational definitions,
characterizing AI Agents as modular systems driven by Large Language Models
(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.
Generative AI is positioned as a precursor, with AI Agents advancing through
tool integration, prompt engineering, and reasoning enhancements. In contrast,
Agentic AI systems represent a paradigmatic shift marked by multi-agent
collaboration, dynamic task decomposition, persistent memory, and orchestrated
autonomy. Through a sequential evaluation of architectural evolution,
operational mechanisms, interaction styles, and autonomy levels, we present a
comparative analysis across both paradigms. Application domains such as
customer support, scheduling, and data summarization are contrasted with
Agentic AI deployments in research automation, robotic coordination, and
medical decision support. We further examine unique challenges in each paradigm
including hallucination, brittleness, emergent behavior, and coordination
failure and propose targeted solutions such as ReAct loops, RAG, orchestration
layers, and causal modeling. This work aims to provide a definitive roadmap for
developing robust, scalable, and explainable AI agent and Agentic AI-driven
systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision
Support System, Agentic-AI Applications

</details>


### [152] [Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models](https://arxiv.org/abs/2505.10543)
*Annie Wong,Thomas Bäck,Aske Plaat,Niki van Stein,Anna V. Kononova*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型在动态环境中的自适应能力，通过自反思、启发式变异和规划等提示技术评估模型表现，发现大模型虽整体表现更优，但战略提示可缩小与小模型的差距，同时揭示了高级推理方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型作为自学习与推理智能体在动态环境中的真实潜力，弥补静态基准测试的不足。

Method: 在动态环境中测试不同开源语言模型，对比自反思、启发式变异和规划等提示技术的效果，分析模型大小与提示长度的关系。

Result: 大模型表现更优但提示策略可缩小差距；过长提示对小模型负面影响更大；高级推理技术对复杂任务的小模型帮助显著，但对大模型提升有限，且可能引入性能波动。

Conclusion: 当前大型语言模型在规划、推理和空间协调等关键领域仍存在根本性缺陷，仅靠自反思提示无法完全克服，需超越静态基准以捕捉推理的复杂性。

Abstract: While large language models demonstrate impressive performance on static
benchmarks, the true potential of large language models as self-learning and
reasoning agents in dynamic environments remains unclear. This study
systematically evaluates the efficacy of self-reflection, heuristic mutation,
and planning as prompting techniques to test the adaptive capabilities of
agents. We conduct experiments with various open-source language models in
dynamic environments and find that larger models generally outperform smaller
ones, but that strategic prompting can close this performance gap. Second, a
too-long prompt can negatively impact smaller models on basic reactive tasks,
while larger models show more robust behaviour. Third, advanced prompting
techniques primarily benefit smaller models on complex games, but offer less
improvement for already high-performing large language models. Yet, we find
that advanced reasoning methods yield highly variable outcomes: while capable
of significantly improving performance when reasoning and decision-making
align, they also introduce instability and can lead to big performance drops.
Compared to human performance, our findings reveal little evidence of true
emergent reasoning. Instead, large language model performance exhibits
persistent limitations in crucial areas such as planning, reasoning, and
spatial coordination, suggesting that current-generation large language models
still suffer fundamental shortcomings that may not be fully overcome through
self-reflective prompting alone. Reasoning is a multi-faceted task, and while
reasoning methods like Chain of thought improves multi-step reasoning on math
word problems, our findings using dynamic benchmarks highlight important
shortcomings in general reasoning capabilities, indicating a need to move
beyond static benchmarks to capture the complexity of reasoning.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [153] [Visual Fidelity Index for Generative Semantic Communications with Critical Information Embedding](https://arxiv.org/abs/2505.10405)
*Jianhao Huang,Qunsong Zeng,Kaibin Huang*

Main category: eess.IV

TL;DR: 该论文提出了一种混合生成语义通信系统，通过关键信息嵌入框架结合文本提示和关键特征传输，解决了纯提示驱动生成丢失细节和缺乏评估指标的问题。


<details>
  <summary>Details</summary>
Motivation: 为解决纯提示驱动生成的视觉细节丢失问题，并填补生成语义通信系统缺乏系统性评估指标的空白，设计了混合系统和GVIF评估指标。

Method: 采用关键信息嵌入框架，结合文本提示和语义关键特征传输，利用扩散生成模型重建图像，并提出GVIF指标量化视觉质量。

Result: 实验验证了GVIF指标对视觉保真度的敏感性，优化系统在PSNR和FID分数上优于基准方案。

Conclusion: 混合生成语义通信系统和GVIF指标有效提升了图像重建质量和系统适应性。

Abstract: Generative semantic communication (Gen-SemCom) with large artificial
intelligence (AI) model promises a transformative paradigm for 6G networks,
which reduces communication costs by transmitting low-dimensional prompts
rather than raw data. However, purely prompt-driven generation loses
fine-grained visual details. Additionally, there is a lack of systematic
metrics to evaluate the performance of Gen-SemCom systems. To address these
issues, we develop a hybrid Gen-SemCom system with a critical information
embedding (CIE) framework, where both text prompts and semantically critical
features are extracted for transmissions. First, a novel approach of semantic
filtering is proposed to select and transmit the semantically critical features
of images relevant to semantic label. By integrating the text prompt and
critical features, the receiver reconstructs high-fidelity images using a
diffusion-based generative model. Next, we propose the generative visual
information fidelity (GVIF) metric to evaluate the visual quality of the
generated image. By characterizing the statistical models of image features,
the GVIF metric quantifies the mutual information between the distorted
features and their original counterparts. By maximizing the GVIF metric, we
design a channel-adaptive Gen-SemCom system that adaptively control the volume
of features and compression rate according to the channel state. Experimental
results validate the GVIF metric's sensitivity to visual fidelity, correlating
with both the PSNR and critical information volume. In addition, the optimized
system achieves superior performance over benchmarking schemes in terms of
higher PSNR and lower FID scores.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [154] [Inconsistency Handling in DatalogMTL](https://arxiv.org/abs/2505.10394)
*Meghyn Bienvenu,Camille Bourgaux,Atefe Khodadaditaghanaki*

Main category: cs.LO

TL;DR: 本文研究了DatalogMTL中的不一致性处理问题，提出了冲突和修复的定义，并分析了其数据复杂性。


<details>
  <summary>Details</summary>
Motivation: 由于DatalogMTL扩展了Datalog并引入了时间操作符，事实与时间区间相关联，因此在规则矛盾时存在多种修复不一致性的方式，需要系统化的研究。

Method: 提出了冲突（最小不一致解释）和修复（恢复一致性的方式）的定义，并研究了它们的性质及容忍不一致的语义，同时进行了数据复杂性分析。

Result: 定义了冲突和修复的概念，并分析了其性质及数据复杂性，尤其是生成单一冲突/修复和基于修复的查询任务。

Conclusion: 研究为DatalogMTL中的不一致性处理提供了理论框架，并展示了其计算复杂度，为进一步应用奠定了基础。

Abstract: In this paper, we explore the issue of inconsistency handling in DatalogMTL,
an extension of Datalog with metric temporal operators. Since facts are
associated with time intervals, there are different manners to restore
consistency when they contradict the rules, such as removing facts or modifying
their time intervals. Our first contribution is the definition of relevant
notions of conflicts (minimal explanations for inconsistency) and repairs
(possible ways of restoring consistency) for this setting and the study of the
properties of these notions and the associated inconsistency-tolerant
semantics. Our second contribution is a data complexity analysis of the tasks
of generating a single conflict / repair and query entailment under
repair-based semantics.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [155] [Who Said What WSW 2.0? Enhanced Automated Analysis of Preschool Classroom Speech](https://arxiv.org/abs/2505.09972)
*Anchen Sun,Tiantian Feng,Gabriela Gutierrez,Juan J Londono,Anfeng Xu,Batya Elbaum,Shrikanth Narayanan,Lynn K Perry,Daniel S Messinger*

Main category: eess.AS

TL;DR: WSW2.0是一种用于分析学前课堂语音交互的自动框架，结合了wav2vec2和Whisper技术，提高了准确性和可扩展性。实验结果显示其在说话者分类和转录质量上表现良好，并成功应用于大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 学前课堂的语言交互对儿童语言发展至关重要，但传统的人工分析方法耗时且难以扩展。WSW2.0旨在通过深度学习和自然语言处理技术，提供高效、准确的语音分析解决方案。

Method: WSW2.0整合了wav2vec2（说话者分类）和Whisper（语音转录）技术。实验使用了235分钟的课堂音频（来自儿童和教师），并与专家标注进行对比。

Result: 说话者分类的加权F1得分为0.845，准确率为0.846；转录的单词错误率教师为0.119，儿童为0.238。框架在课堂语言特征分析中表现出高一致性（ICC介于0.64至0.98之间），并成功应用于1,592小时的大规模数据。

Conclusion: WSW2.0证明了深度学习在学前课堂语音分析中的潜力，能够为教育研究和干预策略提供高效、准确的支持。

Abstract: This paper introduces an automated framework WSW2.0 for analyzing vocal
interactions in preschool classrooms, enhancing both accuracy and scalability
through the integration of wav2vec2-based speaker classification and Whisper
(large-v2 and large-v3) speech transcription. A total of 235 minutes of audio
recordings (160 minutes from 12 children and 75 minutes from 5 teachers), were
used to compare system outputs to expert human annotations. WSW2.0 achieves a
weighted F1 score of .845, accuracy of .846, and an error-corrected kappa of
.672 for speaker classification (child vs. teacher). Transcription quality is
moderate to high with word error rates of .119 for teachers and .238 for
children. WSW2.0 exhibits relatively high absolute agreement intraclass
correlations (ICC) with expert transcriptions for a range of classroom language
features. These include teacher and child mean utterance length, lexical
diversity, question asking, and responses to questions and other utterances,
which show absolute agreement intraclass correlations between .64 and .98. To
establish scalability, we apply the framework to an extensive dataset spanning
two years and over 1,592 hours of classroom audio recordings, demonstrating the
framework's robustness for broad real-world applications. These findings
highlight the potential of deep learning and natural language processing
techniques to revolutionize educational research by providing accurate measures
of key features of preschool classroom speech, ultimately guiding more
effective intervention strategies and supporting early childhood language
development.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [156] [LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps](https://arxiv.org/abs/2505.10537)
*Filippo Olimpieri,Noemi Giustini,Andrea Lacava,Salvatore D'Oro,Tommaso Melodia,Francesca Cuomo*

Main category: cs.NI

TL;DR: 提出了LibIQ库，利用dApps概念实现实时射频信号分类，通过CNN在5G网络中高效处理I/Q样本，平均分类准确率达97.8%。


<details>
  <summary>Details</summary>
Motivation: 传统RIC在实时监测和数据隐私方面的限制，阻碍了波束成形和频谱分类等应用，需一种新方法解决这些问题。

Method: 开发LibIQ库处理I/Q样本，生成数据集并可视化，通过CNN分类信号，基于5G部署和模拟器测试验证。

Result: 在不同场景下，模型对信号类型的分类平均准确率为97.8%。

Conclusion: LibIQ为实时频谱分析提供了高效解决方案，承诺公开库和数据集以推动研究。

Abstract: The O-RAN architecture is transforming cellular networks by adopting RAN
softwarization and disaggregation concepts to enable data-driven monitoring and
control of the network. Such management is enabled by RICs, which facilitate
near-real-time and non-real-time network control through xApps and rApps.
However, they face limitations, including latency overhead in data exchange
between the RAN and RIC, restricting real-time monitoring, and the inability to
access user plain data due to privacy and security constraints, hindering use
cases like beamforming and spectrum classification. In this paper, we leverage
the dApps concept to enable real-time RF spectrum classification with LibIQ, a
novel library for RF signals that facilitates efficient spectrum monitoring and
signal classification by providing functionalities to read I/Q samples as
time-series, create datasets and visualize time-series data through plots and
spectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to
detect external RF signals, which are subsequently classified using a CNN
inside the library. To achieve accurate spectrum analysis, we created an
extensive dataset of time-series-based I/Q samples, representing distinct
signal types captured using a custom dApp running on a 5G deployment over the
Colosseum network emulator and an OTA testbed. We evaluate our model by
deploying LibIQ in heterogeneous scenarios with varying center frequencies,
time windows, and external RF signals. In real-time analysis, the model
classifies the processed I/Q samples, achieving an average accuracy of
approximately 97.8\% in identifying signal types across all scenarios. We
pledge to release both LibIQ and the dataset created as a publicly available
framework upon acceptance.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [157] [Neural models for prediction of spatially patterned phase transitions: methods and challenges](https://arxiv.org/abs/2505.09718)
*Daniel Dylewsky,Sonia Kéfi,Madhur Anand,Chris T. Bauch*

Main category: physics.comp-ph

TL;DR: 该论文研究了深度学习模型在干旱地区植被生态系统临界过渡早期预警信号（EWS）检测中的应用与局限性，重点探讨了模型在空间模式相变中的泛化能力及其性能变化。


<details>
  <summary>Details</summary>
Motivation: 干旱地区植被生态系统易受外部压力影响，出现临界过渡。虽然分岔理论常用于分析此类过渡，但空间植被模式导致动态更为复杂。近期深度学习方法在EWS检测中表现出潜力，但其在高维相变中的泛化能力尚未充分研究。

Method: 论文通过几个典型测试系统，评估了神经网络EWS检测模型在空间模式相变中的表现，对比了多种统计指标的EWS检测效果，并检验了模型区分突变与连续过渡的能力。

Result: 研究发现，当训练数据和测试数据来源互换时，模型性能常发生显著变化，为模型泛化标准提供了新见解。

Conclusion: 尽管深度学习模型在EWS检测中显示出潜力，但其泛化能力受数据来源影响较大，需进一步优化以提高实际应用中的可靠性。

Abstract: Dryland vegetation ecosystems are known to be susceptible to critical
transitions between alternative stable states when subjected to external
forcing. Such transitions are often discussed through the framework of
bifurcation theory, but the spatial patterning of vegetation, which is
characteristic of drylands, leads to dynamics that are much more complex and
diverse than local bifurcations. Recent methodological developments in Early
Warning Signal (EWS) detection have shown promise in identifying dynamical
signatures of oncoming critical transitions, with particularly strong
predictive capabilities being demonstrated by deep neural networks. However, a
machine learning model trained on synthetic examples is only useful if it can
effectively transfer to a test case of practical interest. These models'
capacity to generalize in this manner has been demonstrated for bifurcation
transitions, but it is not as well characterized for high-dimensional phase
transitions. This paper explores the successes and shortcomings of neural EWS
detection for spatially patterned phase transitions, and shows how these models
can be used to gain insight into where and how EWS-relevant information is
encoded in spatiotemporal dynamics. A few paradigmatic test systems are used to
illustrate how the capabilities of such models can be probed in a number of
ways, with particular attention to the performances of a number of proposed
statistical indicators for EWS and to the supplementary task of distinguishing
between abrupt and continuous transitions. Results reveal that model
performance often changes dramatically when training and test data sources are
interchanged, which offers new insight into the criteria for model
generalization.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [158] [Differentiable Quantum Architecture Search in Quantum-Enhanced Neural Network Parameter Generation](https://arxiv.org/abs/2505.09653)
*Samuel Yen-Chi Chen,Chen-Yu Liu,Kuan-Cheng Chen,Wei-Jia Huang,Yen-Jui Chang,Wei-Hao Huang*

Main category: quant-ph

TL;DR: 该论文提出了一种自动化设计量子神经网络（QNN）的方法，通过可微分优化联合优化电路参数和架构参数，实现在分类、时间序列预测和强化学习任务中匹配或超越手动设计的性能。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习（QML）中的变分量子电路（VQC）因依赖量子硬件推理而难以广泛应用。

Method: 提出一种自动化解决方案，利用可微分优化端到端联合优化电路参数和架构参数。

Result: 仿真结果表明，该方法在多项任务中表现优于或匹配手动设计的QNN架构。

Conclusion: 该方法为QNN的自动化设计提供了可扩展的途径，能够为多样化的应用生成经典神经网络参数。

Abstract: The rapid advancements in quantum computing (QC) and machine learning (ML)
have led to the emergence of quantum machine learning (QML), which integrates
the strengths of both fields. Among QML approaches, variational quantum
circuits (VQCs), also known as quantum neural networks (QNNs), have shown
promise both empirically and theoretically. However, their broader adoption is
hindered by reliance on quantum hardware during inference. Hardware
imperfections and limited access to quantum devices pose practical challenges.
To address this, the Quantum-Train (QT) framework leverages the exponential
scaling of quantum amplitudes to generate classical neural network parameters,
enabling inference without quantum hardware and achieving significant parameter
compression. Yet, designing effective quantum circuit architectures for such
quantum-enhanced neural programmers remains non-trivial and often requires
expertise in quantum information science. In this paper, we propose an
automated solution using differentiable optimization. Our method jointly
optimizes both conventional circuit parameters and architectural parameters in
an end-to-end manner via automatic differentiation. We evaluate the proposed
framework on classification, time-series prediction, and reinforcement learning
tasks. Simulation results show that our method matches or outperforms manually
designed QNN architectures. This work offers a scalable and automated pathway
for designing QNNs that can generate classical neural network parameters across
diverse applications.

</details>


### [159] [Quantum Computing and AI: Perspectives on Advanced Automation in Science and Engineering](https://arxiv.org/abs/2505.10012)
*Tadashi Kadowaki*

Main category: quant-ph

TL;DR: 本文提出了Quantum CAE框架，结合量子计算与AI推动工程设计的自动化进程，并通过组合优化问题的案例研究展示了其应用。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用量子算法和AI优化工程设计和自动化科研过程，展望量子计算与AI结合的潜力和挑战。

Method: 引入Quantum CAE框架，结合量子算法进行模拟、优化和机器学习，并通过案例研究验证其有效性。

Result: 成功展示了Quantum CAE在组合优化问题中的实际应用，突显了量子计算在工程设计中的潜力。

Conclusion: 量子计算与AI的结合有望重塑科研和工程设计的未来，但需解决人机协作和算法专业化等问题。

Abstract: Recent advances in artificial intelligence (AI) and quantum computing are
accelerating automation in scientific and engineering processes, fundamentally
reshaping research methodologies. This perspective highlights parallels between
scientific automation and established Computer-Aided Engineering (CAE)
practices, introducing Quantum CAE as a framework that leverages quantum
algorithms for simulation, optimization, and machine learning within
engineering design. Practical implementations of Quantum CAE are illustrated
through case studies for combinatorial optimization problems. Further
discussions include advancements toward higher automation levels, highlighting
the critical role of specialized AI agents proficient in quantum algorithm
design. The integration of quantum computing with AI raises significant
questions about the collaborative dynamics among human scientists and
engineers, AI systems, and quantum computational resources, underscoring a
transformative future for automated discovery and innovation.

</details>


### [160] [Role of scrambling and noise in temporal information processing with quantum systems](https://arxiv.org/abs/2505.10080)
*Weijie Xiong,Zoë Holmes,Armando Angrisani,Yudai Suzuki,Thiparat Chotibut,Supanut Thanasilp*

Main category: quant-ph

TL;DR: 摘要研究量子系统在时间信息处理中的表现，探讨了无噪声和噪声环境下量子储层计算模型的可扩展性和记忆保持能力。


<details>
  <summary>Details</summary>
Motivation: 量子系统在时间信息处理中的潜力已被证实，但其在时间任务中的性能尚缺乏理论理解。本文旨在填补这一空白，为量子储层计算提供理论支持。

Method: 采用通用量子储层处理框架，模拟多种量子计算模型，重点关注高阶幺正设计下的无噪声和噪声环境，分析储层大小和迭代对性能的影响。

Result: 在无噪声环境下，测量读数随储层规模增大呈指数集中，但多次迭代不影响结果；噪声环境下，早期输入和初始状态的记忆随迭代指数衰减。

Conclusion: 研究揭示了量子储层计算的局限性（如指数级记忆衰减），并提出了新的证明技术，为量子时间学习模型的设计提供了理论指导。

Abstract: Scrambling quantum systems have been demonstrated as effective substrates for
temporal information processing. While their role in providing rich feature
maps has been widely studied, a theoretical understanding of their performance
in temporal tasks is still lacking. Here we consider a general quantum
reservoir processing framework that captures a broad range of physical
computing models with quantum systems. We examine the scalability and memory
retention of the model with scrambling reservoirs modelled by high-order
unitary designs in both noiseless and noisy settings. In the former regime, we
show that measurement readouts become exponentially concentrated with
increasing reservoir size, yet strikingly do not worsen with the reservoir
iterations. Thus, while repeatedly reusing a small scrambling reservoir with
quantum data might be viable, scaling up the problem size deteriorates
generalization unless one can afford an exponential shot overhead. In contrast,
the memory of early inputs and initial states decays exponentially in both
reservoir size and reservoir iterations. In the noisy regime, we also prove
exponential memory decays with iterations for local noisy channels. Proving
these results required us to introduce new proof techniques for bounding
concentration in temporal quantum learning models.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [161] [$XX^{t}$ Can Be Faster](https://arxiv.org/abs/2505.09814)
*Dmitry Rybin,Yushun Zhang,Zhi-Quan Luo*

Main category: cs.DS

TL;DR: RXTX算法通过结合机器学习与组合优化实现了比现有方法更高效的矩阵与其转置相乘计算，减少了5%的乘法与加法操作。


<details>
  <summary>Details</summary>
Motivation: 为提高矩阵与其转置相乘计算的效率，作者结合机器学习与组合优化方法，探索更高效的算法。

Method: 算法结合了机器学习搜索方法和组合优化，减少计算复杂度。

Result: RXTX较当前最优方法减少了5%的乘法与加法操作，性能提升适用于小矩阵。

Conclusion: RXTX算法展示出机器学习与组合优化结合在解决计算数学问题上的潜力。

Abstract: We present a new algorithm RXTX that computes product of matrix by its
transpose $XX^{t}$. RXTX uses $5\%$ less multiplications and additions than
State-of-the-Art and achieves accelerations even for small sizes of matrix $X$.
The algorithm was discovered by combining Machine Learning-based search methods
with Combinatorial Optimization.

</details>


### [162] [On Unbiased Low-Rank Approximation with Minimum Distortion](https://arxiv.org/abs/2505.09647)
*Leighton Pate Barnes,Stephen Cameron,Benjamin Howard*

Main category: cs.DS

TL;DR: 提出了一种采样低秩随机矩阵Q的算法，确保其无偏且最小化期望Frobenius范数误差。


<details>
  <summary>Details</summary>
Motivation: 在固定目标矩阵P的近似中，需找到无偏、低秩且误差最小的随机矩阵Q，提升矩阵近似效率。

Method: 算法基于向量无偏稀疏化问题的解，应用于矩阵P的奇异分量上。

Result: 证明了算法与现有下界的误差匹配，验证了其最优性。

Conclusion: 该算法在低秩矩阵近似中实现了最优的无偏误差最小化。

Abstract: We describe an algorithm for sampling a low-rank random matrix $Q$ that best
approximates a fixed target matrix $P\in\mathbb{C}^{n\times m}$ in the
following sense: $Q$ is unbiased, i.e., $\mathbb{E}[Q] = P$;
$\mathsf{rank}(Q)\leq r$; and $Q$ minimizes the expected Frobenius norm error
$\mathbb{E}\|P-Q\|_F^2$. Our algorithm mirrors the solution to the efficient
unbiased sparsification problem for vectors, except applied to the singular
components of the matrix $P$. Optimality is proven by showing that our
algorithm matches the error from an existing lower bound.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [163] [Evaluating Large Language Models for the Generation of Unit Tests with Equivalence Partitions and Boundary Values](https://arxiv.org/abs/2505.09830)
*Martín Rodríguez,Gustavo Rossi,Alejandro Fernandez*

Main category: cs.SE

TL;DR: LLMs在自动生成测试用例方面有潜力，但需依赖精心设计的提示和明确的需求，且需人工监督结合手动定性分析。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在自动生成测试用例中的潜力，并与人工测试对比，以提升单元测试效率。

Method: 开发优化提示，整合代码与需求，覆盖关键案例（如等价类与边界值），并通过定量指标与手动定性分析对比LLMs与程序员的优劣势。

Result: LLMs的效果取决于提示设计、实现质量和需求精度，灵活有前景但需人工监督。

Conclusion: 手动定性分析是自动化单元测试评估的重要补充，LLMs目前仍需人工介入。

Abstract: The design and implementation of unit tests is a complex task many
programmers neglect. This research evaluates the potential of Large Language
Models (LLMs) in automatically generating test cases, comparing them with
manual tests. An optimized prompt was developed, that integrates code and
requirements, covering critical cases such as equivalence partitions and
boundary values. The strengths and weaknesses of LLMs versus trained
programmers were compared through quantitative metrics and manual qualitative
analysis. The results show that the effectiveness of LLMs depends on
well-designed prompts, robust implementation, and precise requirements.
Although flexible and promising, LLMs still require human supervision. This
work highlights the importance of manual qualitative analysis as an essential
complement to automation in unit test evaluation.

</details>


### [164] [Are Sparse Autoencoders Useful for Java Function Bug Detection?](https://arxiv.org/abs/2505.10375)
*Rui Melo,Claudia Mamede,Andre Catarino,Rui Abreu,Henrique Lopes Cardoso*

Main category: cs.SE

TL;DR: SAEs（稀疏自编码器）被评估为一种轻量级、可解释的方法，用于直接从预训练LLM（如GPT-2 Small和Gemma 2B）的内部表示中检测Java函数中的漏洞，无需微调，F1分数高达89%。


<details>
  <summary>Details</summary>
Motivation: 传统漏洞检测方法存在高误报率、扩展性差和依赖人工的问题，而AI方法（如LLM）的复杂性和不透明性限制了其可解释性和部署。SAEs因其轻量化和可解释性成为潜在解决方案。

Method: 研究使用SAEs从GPT-2 Small和Gemma 2B的内部表示中提取特征，直接检测Java函数的漏洞，无需微调基础模型。

Result: SAEs在检测漏洞时F1分数达89%，优于微调的Transformer编码器基线。

Conclusion: SAEs首次被证明可直接从预训练LLM的内部表示中有效检测漏洞，无需微调或任务特定监督，为轻量级、可解释的漏洞检测提供了新方向。

Abstract: Software vulnerabilities such as buffer overflows and SQL injections are a
major source of security breaches. Traditional methods for vulnerability
detection remain essential but are limited by high false positive rates,
scalability issues, and reliance on manual effort. These constraints have
driven interest in AI-based approaches to automated vulnerability detection and
secure code generation. While Large Language Models (LLMs) have opened new
avenues for classification tasks, their complexity and opacity pose challenges
for interpretability and deployment. Sparse Autoencoder offer a promising
solution to this problem. We explore whether SAEs can serve as a lightweight,
interpretable alternative for bug detection in Java functions. We evaluate the
effectiveness of SAEs when applied to representations from GPT-2 Small and
Gemma 2B, examining their capacity to highlight buggy behaviour without
fine-tuning the underlying LLMs. We found that SAE-derived features enable bug
detection with an F1 score of up to 89%, consistently outperforming fine-tuned
transformer encoder baselines. Our work provides the first empirical evidence
that SAEs can be used to detect software bugs directly from the internal
representations of pretrained LLMs, without any fine-tuning or task-specific
supervision.

</details>


### [165] [Are Large Language Models Robust in Understanding Code Against Semantics-Preserving Mutations?](https://arxiv.org/abs/2505.10443)
*Pedro Orvalho,Marta Kwiatkowska*

Main category: cs.SE

TL;DR: 论文评估了大型语言模型（LLM）在Python编程任务中的推理能力和鲁棒性，发现部分模型（如Llama3.2）在61%的情况下通过错误逻辑得出正确结论，且代码变异会导致预测不稳定。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在编程任务中的推理能力和鲁棒性，以评估其是否仅依赖猜测而非真正理解代码逻辑。

Method: 使用五种语义保留的代码变异（如变量重命名、循环转换）测试六个LLM，结合人类专家分析和LiveCodeBench、CruxEval平台评估模型表现。

Result: 部分LLM（如Llama3.2）通过错误逻辑得出正确结论的比例高达61%，且代码变异显著影响模型预测的稳定性。

Conclusion: LLM在代码理解中存在逻辑缺陷和鲁棒性问题，需进一步改进模型推理能力以实现可靠应用。

Abstract: Understanding the reasoning and robustness of Large Language Models (LLMs) is
critical for their reliable use in programming tasks. While recent studies have
assessed LLMs' ability to predict program outputs, most focus solely on the
accuracy of those predictions, without evaluating the reasoning behind them.
Moreover, it has been observed on mathematical reasoning tasks that LLMs can
arrive at correct answers through flawed logic, raising concerns about similar
issues in code understanding.
  In this work, we evaluate whether state-of-the-art LLMs with up to 8B
parameters can reason about Python programs or are simply guessing. We apply
five semantics-preserving code mutations: renaming variables, mirroring
comparison expressions, swapping if-else branches, converting for loops to
while, and loop unrolling. These mutations maintain program semantics while
altering its syntax. We evaluated six LLMs and performed a human expert
analysis using LiveCodeBench to assess whether the correct predictions are
based on sound reasoning. We also evaluated prediction stability across
different code mutations on LiveCodeBench and CruxEval. Our findings show that
some LLMs, such as Llama3.2, produce correct predictions based on flawed
reasoning in up to 61% of cases. Furthermore, LLMs often change predictions in
response to our code mutations, indicating limited robustness in their semantic
understanding.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [166] [Multi-Agent Path Finding For Large Agents Is Intractable](https://arxiv.org/abs/2505.10387)
*Artem Agafonov,Konstantin Yakovlev*

Main category: cs.MA

TL;DR: 本文证明了考虑大型智能体尺寸的多智能体路径规划（MAPF）问题是NP难问题，填补了该领域缺乏多项式时间算法的空白。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中（如机器人领域），智能体的尺寸不可忽略，但目前缺乏对大型智能体冲突的复杂性分析。本文旨在填补这一空白。

Method: 通过将经典的NP完全问题3SAT规约到大型智能体MAPF问题，构造特定图例和起点/目标点，证明二者解的存在性等价。

Result: 首次严格证明了大型智能体MAPF是NP难问题，表明除非P=NP，否则不存在多项式时间解法。

Conclusion: 该研究为大型智能体路径规划的算法设计提供了理论边界，指出必须依赖启发式或近似方法解决。

Abstract: The multi-agent path finding (MAPF) problem asks to find a set of paths on a
graph such that when synchronously following these paths the agents never
encounter a conflict. In the most widespread MAPF formulation, the so-called
Classical MAPF, the agents sizes are neglected and two types of conflicts are
considered: occupying the same vertex or using the same edge at the same time
step. Meanwhile in numerous practical applications, e.g. in robotics, taking
into account the agents' sizes is vital to ensure that the MAPF solutions can
be safely executed. Introducing large agents yields an additional type of
conflict arising when one agent follows an edge and its body overlaps with the
body of another agent that is actually not using this same edge (e.g. staying
still at some distinct vertex of the graph). Until now it was not clear how
harder the problem gets when such conflicts are to be considered while
planning. Specifically, it was known that Classical MAPF problem on an
undirected graph can be solved in polynomial time, however no complete
polynomial-time algorithm was presented to solve MAPF with large agents. In
this paper we, for the first time, establish that the latter problem is NP-hard
and, thus, if P!=NP no polynomial algorithm for it can, unfortunately, be
presented. Our proof is based on the prevalent in the field technique of
reducing the seminal 3SAT problem (which is known to be an NP-complete problem)
to the problem at hand. In particular, for an arbitrary 3SAT formula we
procedurally construct a dedicated graph with specific start and goal vertices
and show that the given 3SAT formula is satisfiable iff the corresponding path
finding instance has a solution.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [167] [Uncovering Magnetic Phases with Synthetic Data and Physics-Informed Training](https://arxiv.org/abs/2505.10393)
*Agustin Medina,Marcelo Arlego,Carlos A. Lamas*

Main category: cond-mat.str-el

TL;DR: 该论文研究了如何使用人工神经网络从合成数据中高效学习磁性相变，通过结合计算简单性和物理信息策略，在稀释伊辛模型中探索了监督分类和无监督检测两种方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决缺乏精确解析解的稀释伊辛模型中的相变问题，通过物理信息引导的神经网络方法，提供了一种低成本且鲁棒的替代方案。

Method: 方法包括两种互补策略：使用简单密集神经网络的监督分类，以及利用卷积自编码器无监督检测相变。通过物理信息引导（如对称性破缺的架构偏置和训练配置）提升模型性能。

Result: 结果表明，合成的、结构化且计算高效的训练方案能够揭示复杂系统中的物理相边界，其预测与直接数值估计的关键温度和渗流阈值一致。

Conclusion: 结论认为该方法为凝聚态和统计物理中的相变研究提供了一种有效且通用的框架。

Abstract: We investigate the efficient learning of magnetic phases using artificial
neural networks trained on synthetic data, combining computational simplicity
with physics-informed strategies. Focusing on the diluted Ising model, which
lacks an exact analytical solution, we explore two complementary approaches: a
supervised classification using simple dense neural networks, and an
unsupervised detection of phase transitions using convolutional autoencoders
trained solely on idealized spin configurations.
  To enhance model performance, we incorporate two key forms of
physics-informed guidance. First, we exploit architectural biases which
preferentially amplify features related to symmetry breaking. Second, we
include training configurations that explicitly break $\mathbb{Z}_2$ symmetry,
reinforcing the network's ability to detect ordered phases. These mechanisms,
acting in tandem, increase the network's sensitivity to phase structure even in
the absence of explicit labels. We validate the machine learning predictions
through comparison with direct numerical estimates of critical temperatures and
percolation thresholds.
  Our results show that synthetic, structured, and computationally efficient
training schemes can reveal physically meaningful phase boundaries, even in
complex systems. This framework offers a low-cost and robust alternative to
conventional methods, with potential applications in broader condensed matter
and statistical physics contexts.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [168] [A Fine-Grained Complexity View on Propositional Abduction -- Algorithms and Lower Bounds](https://arxiv.org/abs/2505.10201)
*Victor Lagerkvist,Mohamed Maizia,Johannes Schmidt*

Main category: cs.CC

TL;DR: 该论文探讨了非单调推理（如溯因推理）的复杂性，针对知识库中变量数n这一参数进行了分析，首次在Σ²P完全问题上超越了穷举搜索，并提出了基于强指数时间假设的下界验证。


<details>
  <summary>Details</summary>
Motivation: 旨在弥合单调推理与非单调推理之间的复杂性研究差距，特别是在溯因推理等非单调推理领域，目前缺乏类似SAT问题的精细复杂性结果。

Method: 通过分析知识库中变量数量n作为参数，研究了不同复杂性类别（Σ²P、NP、coNP）的片段，结合上界算法设计和下界假设（如强指数时间假设）。

Result: 首次为Σ²P完全问题提供超越穷举搜索的算法，同时证明了多个片段在现有假设下的不可改进性。

Conclusion: 这项工作为非单调推理的复杂性提供了新见解，展示了参数化方法在解决高复杂度问题中的潜力。

Abstract: The Boolean satisfiability problem (SAT) is a well-known example of monotonic
reasoning, of intense practical interest due to fast solvers, complemented by
rigorous fine-grained complexity results. However, for non-monotonic reasoning,
e.g., abductive reasoning, comparably little is known outside classic
complexity theory. In this paper we take a first step of bridging the gap
between monotonic and non-monotonic reasoning by analyzing the complexity of
intractable abduction problems under the seemingly overlooked but natural
parameter n: the number of variables in the knowledge base. We obtain several
positive results for $\Sigma^P_2$- as well as NP- and coNP-complete fragments,
which implies the first example of beating exhaustive search for a
$\Sigma^P_2$-complete problem (to the best of our knowledge). We complement
this with lower bounds and for many fragments rule out improvements under the
(strong) exponential-time hypothesis.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [169] [Deconstructing Subset Construction -- Reducing While Determinizing](https://arxiv.org/abs/2505.10319)
*John Nicol,Markus Frohme*

Main category: cs.FL

TL;DR: 论文提出了一种新的NFA规范方法，通过引入中间最小化步骤减少即时探测空间，显著提升了最坏情况下的性能。


<details>
  <summary>Details</summary>
Motivation: NFA规范问题在处理自动序列等场景时效率较低，尤其是在最坏情况下效果不佳，因此需要一种能够动态优化状态等效性且兼容现有方法的技术。

Method: 利用等效性注册表管理状态等效信息，并结合凸闭包或模拟等技术动态优化；该方法可嵌入经典子集构造或Brzozowski方法中。

Result: 在自动序列的真实案例中验证了该方法的有效性，特别是最坏情况下的性能得到了显著提升。

Conclusion: 所提出的方法具有通用性，能以动态优化提升NFA规范效率，其开源实现可供用户进一步实验。

Abstract: We present a novel perspective on the NFA canonization problem, which
introduces intermediate minimization steps to reduce the exploration space
on-the-fly. Essential to our approach are so-called equivalence registries
which manage information about equivalent states and allow for incorporating
further optimization techniques such as convexity closures or simulation to
boost performance. Due to the generality of our approach, these concepts can be
embedded in classic subset construction or Brzozowski's approach. We evaluate
our approach on a set of real-world examples from automatic sequences and
observe that we are able to improve especially worst-case scenarios. We
implement our approach in an open-source library for users to experiment with.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [170] [On Measuring Intrinsic Causal Attributions in Deep Neural Networks](https://arxiv.org/abs/2505.09660)
*Saptarshi Saha,Dhruv Vansraj Rathore,Soumadeep Saha,Utpal Garain,David Doermann*

Main category: stat.ML

TL;DR: 这篇论文提出了一种新的方法——内在因果贡献（ICC），用于量化神经网络中输入特征的因果影响。相较于现有技术，ICC提供了更直观和可靠的解释。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常评估直接、间接和总的因果效应，但缺乏对内在因果贡献的量化。本文旨在通过将神经网络视为结构因果模型（SCMs），填补这一空白。

Method: 论文提出了一种可识别的生成后处理框架，用于量化ICC，并将其与Sobol指数关联。

Result: 在合成和真实数据集上的实验表明，ICC生成的解释比现有全局解释技术更直观和可靠。

Conclusion: ICC是一种有效的量化输入特征因果影响的方法，优于现有技术，为神经网络解释性提供了新视角。

Abstract: Quantifying the causal influence of input features within neural networks has
become a topic of increasing interest. Existing approaches typically assess
direct, indirect, and total causal effects. This work treats NNs as structural
causal models (SCMs) and extends our focus to include intrinsic causal
contributions (ICC). We propose an identifiable generative post-hoc framework
for quantifying ICC. We also draw a relationship between ICC and Sobol'
indices. Our experiments on synthetic and real-world datasets demonstrate that
ICC generates more intuitive and reliable explanations compared to existing
global explanation techniques.

</details>


### [171] [Learning Multi-Attribute Differential Graphs with Non-Convex Penalties](https://arxiv.org/abs/2505.09748)
*Jitendra K Tugnait*

Main category: stat.ML

TL;DR: 论文提出一种基于非凸惩罚的D-trace损失函数方法，用于估计两个结构相似的多属性高斯图模型的差异，提供理论分析和实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法基于组LASSO惩罚损失函数，本文采用非凸惩罚（如log-sum和SCAD）以提高高维设置下的估计精度和支撑恢复一致性。

Method: 使用带非凸惩罚（log-sum和SCAD）的D-trace损失函数，提出两种近端梯度下降法优化目标函数。

Result: 理论分析证明了高维设置下支撑恢复的一致性和凸性，并通过合成与真实数据实验验证了方法的有效性。

Conclusion: 非凸惩罚D-trace损失函数在多属性高斯图模型差异估计中表现优于传统组LASSO方法。

Abstract: We consider the problem of estimating differences in two multi-attribute
Gaussian graphical models (GGMs) which are known to have similar structure,
using a penalized D-trace loss function with non-convex penalties. The GGM
structure is encoded in its precision (inverse covariance) matrix. Existing
methods for multi-attribute differential graph estimation are based on a group
lasso penalized loss function. In this paper, we consider a penalized D-trace
loss function with non-convex (log-sum and smoothly clipped absolute deviation
(SCAD)) penalties. Two proximal gradient descent methods are presented to
optimize the objective function. Theoretical analysis establishing sufficient
conditions for consistency in support recovery, convexity and estimation in
high-dimensional settings is provided. We illustrate our approaches with
numerical examples based on synthetic and real data.

</details>


### [172] [LatticeVision: Image to Image Networks for Modeling Non-Stationary Spatial Data](https://arxiv.org/abs/2505.09803)
*Antony Sikorski,Michael Ivanitskiy,Nathan Lenssen,Douglas Nychka,Daniel McKenzie*

Main category: stat.ML

TL;DR: 论文提出了一种基于图像对图像（I2I）网络的参数估计方法，用于高效且准确地模拟复杂非平稳空间自回归（SAR）模型，避免了传统最大似然估计（MLE）的高计算成本。


<details>
  <summary>Details</summary>
Motivation: 在许多科学和工业应用中，获取大量空间分布的数据（如全球温度敏感性场）成本高昂。传统的最大似然估计（MLE）方法计算复杂，尤其对于大型非平稳场。因此，需要一种高效且准确的替代方法。

Method: 利用空间自回归（SAR）模型的参数可以排列在规则网格上的特点，将输入（空间场）和输出（模型参数）视为图像。通过图像对图像（I2I）网络进行参数估计，避免了传统MLE的计算瓶颈。

Result: 该方法在非平稳SAR模型的参数估计中实现了更快且更准确的效果，尤其适用于前所未有的复杂模型。

Conclusion: 通过将SAR模型参数估计问题转化为图像处理问题，I2I网络提供了一种高效、准确的解决方案，适用于复杂非平稳空间数据的模拟和分析。

Abstract: In many scientific and industrial applications, we are given a handful of
instances (a 'small ensemble') of a spatially distributed quantity (a 'field')
but would like to acquire many more. For example, a large ensemble of global
temperature sensitivity fields from a climate model can help farmers, insurers,
and governments plan appropriately. When acquiring more data is prohibitively
expensive -- as is the case with climate models -- statistical emulation offers
an efficient alternative for simulating synthetic yet realistic fields.
However, parameter inference using maximum likelihood estimation (MLE) is
computationally prohibitive, especially for large, non-stationary fields. Thus,
many recent works train neural networks to estimate parameters given spatial
fields as input, sidestepping MLE completely. In this work we focus on a
popular class of parametric, spatially autoregressive (SAR) models. We make a
simple yet impactful observation; because the SAR parameters can be arranged on
a regular grid, both inputs (spatial fields) and outputs (model parameters) can
be viewed as images. Using this insight, we demonstrate that image-to-image
(I2I) networks enable faster and more accurate parameter estimation for a class
of non-stationary SAR models with unprecedented complexity.

</details>


### [173] [A Scalable Gradient-Based Optimization Framework for Sparse Minimum-Variance Portfolio Selection](https://arxiv.org/abs/2505.10099)
*Sarat Moka,Matias Quiroz,Vali Asimit,Samuel Muller*

Main category: stat.ML

TL;DR: 文章提出了一种基于梯度的快速稀疏投资组合优化方法，解决了传统混合整数二次规划方法计算成本高的问题，通过布尔松弛将组合问题转化为连续优化任务。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏投资组合优化方法计算成本高，难以处理中等规模问题，因此需要一种更快速且可扩展的算法。

Method: 采用布尔松弛技术将组合问题转化为连续优化任务，引入可调参数使目标函数从凸变为凹，从而在稳定初始点后逐步逼近稀疏二值解。

Result: 该方法在大多数情况下与商业求解器的资产选择结果一致，少数情况下差异微小且组合方差误差可忽略。

Conclusion: 提出的方法在计算效率和结果质量上均表现优异，为大规模稀疏投资组合优化提供了可行方案。

Abstract: Portfolio optimization involves selecting asset weights to minimize a
risk-reward objective, such as the portfolio variance in the classical
minimum-variance framework. Sparse portfolio selection extends this by imposing
a cardinality constraint: only $k$ assets from a universe of $p$ may be
included. The standard approach models this problem as a mixed-integer
quadratic program and relies on commercial solvers to find the optimal
solution. However, the computational costs of such methods increase
exponentially with $k$ and $p$, making them too slow for problems of even
moderate size. We propose a fast and scalable gradient-based approach that
transforms the combinatorial sparse selection problem into a constrained
continuous optimization task via Boolean relaxation, while preserving
equivalence with the original problem on the set of binary points. Our
algorithm employs a tunable parameter that transmutes the auxiliary objective
from a convex to a concave function. This allows a stable convex starting
point, followed by a controlled path toward a sparse binary solution as the
tuning parameter increases and the objective moves toward concavity. In
practice, our method matches commercial solvers in asset selection for most
instances and, in rare instances, the solution differs by a few assets whilst
showing a negligible error in portfolio variance.

</details>


### [174] [Path Gradients after Flow Matching](https://arxiv.org/abs/2505.10139)
*Lorenz Vaitl,Leon Klein*

Main category: stat.ML

TL;DR: 结合路径梯度（path gradients）优化Flow Matching训练的连续归一化流（CNF），在已知目标能量的情况下提升分子系统采样效率三倍，同时保持模型和计算预算不变，且无需额外采样。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过路径梯度进一步提升Flow Matching训练的CNF在分子系统采样中的效率，同时验证其对流结构的影响。

Method: 在Flow Matching预训练的CNF基础上，引入路径梯度进行微调，并在已知目标能量的设定下评估其效果。

Result: 实验表明，该方法将采样效率提升至三倍，且流轨迹长度在微调过程中保持稳定，说明路径梯度保留了流的结构。

Conclusion: 路径梯度提供了一种高效微调CNF的方法，显著提升采样效率且不增加额外成本。

Abstract: Boltzmann Generators have emerged as a promising machine learning tool for
generating samples from equilibrium distributions of molecular systems using
Normalizing Flows and importance weighting. Recently, Flow Matching has helped
speed up Continuous Normalizing Flows (CNFs), scale them to more complex
molecular systems, and minimize the length of the flow integration
trajectories. We investigate the benefits of using path gradients to fine-tune
CNFs initially trained by Flow Matching, in the setting where a target energy
is known. Our experiments show that this hybrid approach yields up to a
threefold increase in sampling efficiency for molecular systems, all while
using the same model, a similar computational budget and without the need for
additional sampling. Furthermore, by measuring the length of the flow
trajectories during fine-tuning, we show that path gradients largely preserve
the learned structure of the flow.

</details>


### [175] [One-Stage Top-$k$ Learning-to-Defer: Score-Based Surrogates with Theoretical Guarantees](https://arxiv.org/abs/2505.10160)
*Yannis Montreuil,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: stat.ML

TL;DR: 该论文提出了一种新的一阶段Top-k学习延迟框架，通过共享的基于分数的模型统一预测和延迟决策。


<details>
  <summary>Details</summary>
Motivation: 现有的一阶段延迟学习方法仅限于单一专家延迟，本文旨在通过联合优化预测与多实体延迟决策来提升效率。

Method: 定义了一个成本敏感损失，并推导出一个与k无关的凸替代损失，支持动态选择k值，无需重新训练。

Result: 在CIFAR-10和SVHN数据集上的实验显示，该方法显著优于Top-1延迟，且动态调整的Top-k(x)在准确性和成本间取得更好平衡。

Conclusion: 该框架不仅扩展了现有方法，还通过动态策略优化了预测与延迟决策的效率。

Abstract: We introduce the first one-stage Top-$k$ Learning-to-Defer framework, which
unifies prediction and deferral by learning a shared score-based model that
selects the $k$ most cost-effective entities-labels or experts-per input. While
existing one-stage L2D methods are limited to deferring to a single expert, our
approach jointly optimizes prediction and deferral across multiple entities
through a single end-to-end objective. We define a cost-sensitive loss and
derive a novel convex surrogate that is independent of the cardinality
parameter $k$, enabling generalization across Top-$k$ regimes without
retraining. Our formulation recovers the Top-1 deferral policy of prior
score-based methods as a special case, and we prove that our surrogate is both
Bayes-consistent and $\mathcal{H}$-consistent under mild assumptions. We
further introduce an adaptive variant, Top-$k(x)$, which dynamically selects
the number of consulted entities per input to balance predictive accuracy and
consultation cost. Experiments on CIFAR-10 and SVHN confirm that our one-stage
Top-$k$ method strictly outperforms Top-1 deferral, while Top-$k(x)$ achieves
superior accuracy-cost trade-offs by tailoring allocations to input complexity.

</details>


### [176] [Efficient MCMC Sampling with Expensive-to-Compute and Irregular Likelihoods](https://arxiv.org/abs/2505.10448)
*Conor Rosato,Harvinder Lehal,Simon Maskell,Lee Devlin,Malcolm Strens*

Main category: stat.ML

TL;DR: 论文探讨了在似然函数不规则且计算成本高的情况下，如何改进MCMC采样算法。通过引入子集评估和数据驱动的代理模型，结合自适应控制器，提高了采样效率。


<details>
  <summary>Details</summary>
Motivation: 由于不规则且计算昂贵的似然函数对贝叶斯推断中MCMC的挑战，作者希望通过子集评估和数据驱动方法降低计算开销。

Method: 采用子集采样器，利用数据驱动的代理模型代替泰勒展开，并设计计算成本感知的自适应控制器。改进的HINTS算法结合自适应提案和代理模型。

Result: 改进的HINTS算法在固定计算预算下实现了最低的采样误差，子集评估和代理模型的结合显著提升了采样效率。

Conclusion: 子集评估提供廉价且自然的探索，数据驱动的代理模型能有效预筛选提案，二者结合通过分层延迟接受实现了高效的精确采样。

Abstract: Bayesian inference with Markov Chain Monte Carlo (MCMC) is challenging when
the likelihood function is irregular and expensive to compute. We explore
several sampling algorithms that make use of subset evaluations to reduce
computational overhead. We adapt the subset samplers for this setting where
gradient information is not available or is unreliable. To achieve this, we
introduce data-driven proxies in place of Taylor expansions and define a novel
computation-cost aware adaptive controller. We undertake an extensive
evaluation for a challenging disease modelling task and a configurable task
with similar irregularity in the likelihood surface. We find our improved
version of Hierarchical Importance with Nested Training Samples (HINTS), with
adaptive proposals and a data-driven proxy, obtains the best sampling error in
a fixed computational budget. We conclude that subset evaluations can provide
cheap and naturally-tempered exploration, while a data-driven proxy can
pre-screen proposals successfully in explored regions of the state space. These
two elements combine through hierarchical delayed acceptance to achieve
efficient, exact sampling.

</details>


### [177] [FlowVAT: Normalizing Flow Variational Inference with Affine-Invariant Tempering](https://arxiv.org/abs/2505.10466)
*Juehang Qin,Shixiao Liang,Christopher Tunnell*

Main category: stat.ML

TL;DR: FlowVAT提出了一种基于条件温度调节的归一化流变分推断方法，解决了多模态和高维后验的挑战，无需复杂的超参数调优或退火计划。


<details>
  <summary>Details</summary>
Motivation: 传统的变分推断在多模态和高维后验中表现不佳，容易出现模态丢失和崩溃问题，需要繁琐的温度调度和超参数调优。FlowVAT旨在实现真正的黑盒变分推断。

Method: 通过同时调节基分布和目标分布的温度，并在归一化流中引入温度条件化，FlowVAT利用过参数化神经网络的泛化能力，训练一个覆盖多温度范围的单一流模型。

Result: 在2维、10维和20维的多模态分布实验中，FlowVAT优于传统和自适应退火方法，能够发现更多模态并获得更好的ELBO值，尤其在更高维度表现突出。

Conclusion: FlowVAT无需复杂调优或退火计划，显著提升了变分推断在多模态和高维后验中的表现，推动了黑盒变分推断的自动化进程。

Abstract: Multi-modal and high-dimensional posteriors present significant challenges
for variational inference, causing mode-seeking behavior and collapse despite
the theoretical expressiveness of normalizing flows. Traditional annealing
methods require temperature schedules and hyperparameter tuning, falling short
of the goal of truly black-box variational inference. We introduce FlowVAT, a
conditional tempering approach for normalizing flow variational inference that
addresses these limitations. Our method tempers both the base and target
distributions simultaneously, maintaining affine-invariance under tempering. By
conditioning the normalizing flow on temperature, we leverage overparameterized
neural networks' generalization capabilities to train a single flow
representing the posterior across a range of temperatures. This preserves modes
identified at higher temperatures when sampling from the variational posterior
at $T = 1$, mitigating standard variational methods' mode-seeking behavior. In
experiments with 2, 10, and 20 dimensional multi-modal distributions, FlowVAT
outperforms traditional and adaptive annealing methods, finding more modes and
achieving better ELBO values, particularly in higher dimensions where existing
approaches fail. Our method requires minimal hyperparameter tuning and does not
require an annealing schedule, advancing toward fully-automatic black-box
variational inference for complicated posteriors.

</details>


### [178] [Batched Nonparametric Bandits via k-Nearest Neighbor UCB](https://arxiv.org/abs/2505.10498)
*Sakshi Arya*

Main category: stat.ML

TL;DR: 该研究提出了一种非参数算法BaNk-UCB，结合k最近邻回归和上置信界原理，用于批量非参数上下文老虎机问题，在有限反馈场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 医疗和营销等领域中在线反馈有限，需要一种适应性强且简单的非参数方法来解决批量决策问题。

Method: 提出BaNk-UCB算法，结合自适应k-NN回归和UCB原理，利用局部几何估计奖励并平衡探索与利用。

Result: 在标准Lipschitz平滑性和边界假设下，实现了接近最优的遗憾保证，并在合成和真实数据集上优于基于分箱的基线方法。

Conclusion: BaNk-UCB是一种高效、非参数且适应性强的方法，适用于批量上下文老虎机问题。

Abstract: We study sequential decision-making in batched nonparametric contextual
bandits, where actions are selected over a finite horizon divided into a small
number of batches. Motivated by constraints in domains such as medicine and
marketing -- where online feedback is limited -- we propose a nonparametric
algorithm that combines adaptive k-nearest neighbor (k-NN) regression with the
upper confidence bound (UCB) principle. Our method, BaNk-UCB, is fully
nonparametric, adapts to the context dimension, and is simple to implement.
Unlike prior work relying on parametric or binning-based estimators, BaNk-UCB
uses local geometry to estimate rewards and adaptively balances exploration and
exploitation. We provide near-optimal regret guarantees under standard
Lipschitz smoothness and margin assumptions, using a theoretically motivated
batch schedule that balances regret across batches and achieves minimax-optimal
rates. Empirical evaluations on synthetic and real-world datasets demonstrate
that BaNk-UCB consistently outperforms binning-based baselines.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [179] [A Survey on Large Language Models in Multimodal Recommender Systems](https://arxiv.org/abs/2505.09777)
*Alejo Lopez-Avila,Jinhua Du*

Main category: cs.IR

TL;DR: 本文综述了多模态推荐系统（MRS）与大型语言模型（LLMs）的结合，探讨了如何利用LLMs的语义推理和动态处理能力提升推荐性能，并提出了分类法和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的涌现为多模态推荐系统提供了新的机遇，但也带来了可扩展性和模型可访问性的挑战。本文旨在梳理LLMs在MRS中的应用，并推动这一快速发展领域的研究。

Method: 通过全面综述近期研究，本文聚焦于提示策略、微调方法和数据适配技术，并提出了一种新的分类法来表征整合模式。

Result: 研究总结了LLMs与MRS的整合模式，识别了可迁移技术，并概述了评估指标与数据集。

Conclusion: LLMs在多模态推荐中扮演着新兴角色，本文为未来研究提供了方向和支持。

Abstract: Multimodal recommender systems (MRS) integrate heterogeneous user and item
data, such as text, images, and structured information, to enhance
recommendation performance. The emergence of large language models (LLMs)
introduces new opportunities for MRS by enabling semantic reasoning, in-context
learning, and dynamic input handling. Compared to earlier pre-trained language
models (PLMs), LLMs offer greater flexibility and generalisation capabilities
but also introduce challenges related to scalability and model accessibility.
This survey presents a comprehensive review of recent work at the intersection
of LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and
data adaptation techniques. We propose a novel taxonomy to characterise
integration patterns, identify transferable techniques from related
recommendation domains, provide an overview of evaluation metrics and datasets,
and point to possible future directions. We aim to clarify the emerging role of
LLMs in multimodal recommendation and support future research in this rapidly
evolving field.

</details>


### [180] [Boosting Text-to-Chart Retrieval through Training with Synthesized Semantic Insights](https://arxiv.org/abs/2505.10043)
*Yifan Wu,Lutao Yan,Yizhang Zhu,Yinan Mei,Jiannan Wang,Nan Tang,Yuyu Luo*

Main category: cs.IR

TL;DR: 该论文提出了一种自动合成图表多层次语义见解的训练数据开发流程，并基于CLIP模型训练了名为ChartFinder的文本到图表检索模型，显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图表检索方法因缺乏全面的语义元数据而难以捕捉图表的语义和上下文信息，限制了检索效果。

Method: 1. 设计自动合成图表语义见解的流程（视觉、统计、任务导向）；2. 训练CLIP-based模型ChartFinder，结合语义和视觉特征。

Result: 在CRBench基准测试中，ChartFinder在精确查询和模糊查询任务中分别提升11.58%和平均5%的性能。

Conclusion: 通过增强语义理解，ChartFinder显著推动了文本到图表检索的实用性，尤其在BI场景中。

Abstract: Charts are crucial for data analysis and decision-making.Text-to-chart
retrieval systems have become increasingly important for Business Intelligence
(BI), where users need to find relevant charts that match their analytical
needs. These needs can be categorized into precise queries that are
well-specified and fuzzy queries that are more exploratory -- both require
understanding the semantics and context of the charts. However, existing
text-to-chart retrieval solutions often fail to capture the semantic content
and contextual information of charts, primarily due to the lack of
comprehensive metadata (or semantic insights). To address this limitation, we
propose a training data development pipeline that automatically synthesizes
hierarchical semantic insights for charts, covering visual patterns
(visual-oriented), statistical properties (statistics-oriented), and practical
applications (task-oriented), which produces 207,498 semantic insights for
69,166 charts. Based on these, we train a CLIP-based model named ChartFinder to
learn better representations of charts for text-to-chart retrieval. Our method
leverages rich semantic insights during the training phase to develop a model
that understands both visual and semantic aspects of charts.To evaluate
text-to-chart retrieval performance, we curate the first benchmark, CRBench,
for this task with 21,862 charts and 326 text queries from real-world BI
applications, with ground-truth labels verified by the crowd
workers.Experiments show that ChartFinder significantly outperforms existing
methods in text-to-chart retrieval tasks across various settings. For precise
queries, ChartFinder achieves up to 66.9% NDCG@10, which is 11.58% higher than
state-of-the-art models. In fuzzy query tasks, our method also demonstrates
consistent improvements, with an average increase of 5% across nearly all
metrics.

</details>


### [181] [Do LLMs Memorize Recommendation Datasets? A Preliminary Study on MovieLens-1M](https://arxiv.org/abs/2505.10212)
*Dario Di Palma,Felice Antonio Merra,Maurizio Sfilio,Vito Walter Anelli,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 该研究探讨了大型语言模型（LLMs）是否记忆了公开的推荐数据集MovieLens-1M，并分析了记忆对推荐性能的影响。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs是否记忆了公开数据集，因为这种记忆会影响研究结果的普适性并可能放大偏见。

Method: 通过定义数据集记忆的标准，并测试GPT和Llama不同规模的模型。

Result: 所有模型都表现出对MovieLens-1M的一定程度记忆，推荐性能与记忆程度相关。

Conclusion: LLMs确实记忆了公开推荐数据集，且记忆程度与模型家族和规模有关。

Abstract: Large Language Models (LLMs) have become increasingly central to
recommendation scenarios due to their remarkable natural language understanding
and generation capabilities. Although significant research has explored the use
of LLMs for various recommendation tasks, little effort has been dedicated to
verifying whether they have memorized public recommendation dataset as part of
their training data. This is undesirable because memorization reduces the
generalizability of research findings, as benchmarking on memorized datasets
does not guarantee generalization to unseen datasets. Furthermore, memorization
can amplify biases, for example, some popular items may be recommended more
frequently than others.
  In this work, we investigate whether LLMs have memorized public
recommendation datasets. Specifically, we examine two model families (GPT and
Llama) across multiple sizes, focusing on one of the most widely used dataset
in recommender systems: MovieLens-1M. First, we define dataset memorization as
the extent to which item attributes, user profiles, and user-item interactions
can be retrieved by prompting the LLMs. Second, we analyze the impact of
memorization on recommendation performance. Lastly, we examine whether
memorization varies across model families and model sizes. Our results reveal
that all models exhibit some degree of memorization of MovieLens-1M, and that
recommendation performance is related to the extent of memorization. We have
made all the code publicly available at:
https://github.com/sisinflab/LLM-MemoryInspector

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [182] [ILIF: Temporal Inhibitory Leaky Integrate-and-Fire Neuron for Overactivation in Spiking Neural Networks](https://arxiv.org/abs/2505.10371)
*Kai Sun,Peibo Duan,Levin Kuhlmann,Beilun Wang,Bin Zhang*

Main category: cs.NE

TL;DR: 论文提出了一个名为ILIF的神经元模型，通过引入抑制机制解决了SNN中梯度支持宽度γ的困境：大γ导致过度激活和高能耗，小γ导致梯度消失。ILIF模型有效减少神经元发放率，提高能效和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 在脉冲神经网络（SNN）中，使用反向传播训练时，需通过替代梯度近似不可微的脉冲函数，但梯度支持宽度γ的选择存在两难：大γ导致过度激活和高能耗，而小γ削弱梯度传播和时间依赖性。

Method: 提出了一种受生物抑制机制启发的带时域抑制的泄漏积分发放（ILIF）神经元模型，通过抑制性单元动态调节膜电位和电流，平衡激活与梯度传播。

Result: 理论分析和多数据集实验表明，ILIF能显著减少神经元发放率（提高能效）、稳定训练并提升准确率。

Conclusion: ILIF模型通过抑制机制有效解决了γ的困境，为SNN的高效训练提供了新思路。代码已开源。

Abstract: The Spiking Neural Network (SNN) has drawn increasing attention for its
energy-efficient, event-driven processing and biological plausibility. To train
SNNs via backpropagation, surrogate gradients are used to approximate the
non-differentiable spike function, but they only maintain nonzero derivatives
within a narrow range of membrane potentials near the firing threshold,
referred to as the surrogate gradient support width gamma. We identify a major
challenge, termed the dilemma of gamma: a relatively large gamma leads to
overactivation, characterized by excessive neuron firing, which in turn
increases energy consumption, whereas a small gamma causes vanishing gradients
and weakens temporal dependencies. To address this, we propose a temporal
Inhibitory Leaky Integrate-and-Fire (ILIF) neuron model, inspired by biological
inhibitory mechanisms. This model incorporates interconnected inhibitory units
for membrane potential and current, effectively mitigating overactivation while
preserving gradient propagation. Theoretical analysis demonstrates ILIF
effectiveness in overcoming the gamma dilemma, and extensive experiments on
multiple datasets show that ILIF improves energy efficiency by reducing firing
rates, stabilizes training, and enhances accuracy. The code is available at
github.com/kaisun1/ILIF.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [183] [LanTu: Dynamics-Enhanced Deep Learning for Eddy-Resolving Ocean Forecasting](https://arxiv.org/abs/2505.10191)
*Qingyu Zheng,Qi Shao,Guijun Han,Wei Li,Hong Li,Xuan Wang*

Main category: physics.ao-ph

TL;DR: LanTu是一个基于动力学增强深度学习的区域涡旋分辨率海洋预报系统，通过多尺度物理约束优化，显著提升了中尺度涡旋演变的预报能力，性能超越现有数值和AI预报系统。


<details>
  <summary>Details</summary>
Motivation: 中尺度涡旋对海洋能量级联的影响不可忽视，而传统数值模型在涡旋分辨率预报中面临科学挑战和高计算成本，AI为平衡预报性能与计算效率提供了新工具。

Method: 开发LanTu系统，结合跨尺度交互和多尺度物理约束，利用涡旋动力学知识优化深度学习模型。

Result: LanTu在温度、盐度、海平面异常和流场预报上领先NOFS和AI-OFS，预报时效超过10天。

Conclusion: 动力学增强深度学习（LanTu）为中尺度涡旋预报提供了强大范式。

Abstract: Mesoscale eddies dominate the spatiotemporal multiscale variability of the
ocean, and their impact on the energy cascade of the global ocean cannot be
ignored. Eddy-resolving ocean forecasting is providing more reliable protection
for fisheries and navigational safety, but also presents significant scientific
challenges and high computational costs for traditional numerical models.
Artificial intelligence (AI)-based weather and ocean forecasting systems are
becoming powerful tools that balance forecast performance with computational
efficiency. However, the complex multiscale features in the ocean dynamical
system make AI models still face many challenges in mesoscale eddy forecasting
(especially regional modelling). Here, we develop LanTu, a regional
eddy-resolving ocean forecasting system based on dynamics-enhanced deep
learning. We incorporate cross-scale interactions into LanTu and construct
multiscale physical constraint for optimising LanTu guided by knowledge of eddy
dynamics in order to improve the forecasting skill of LanTu for mesoscale
evolution. The results show that LanTu outperforms the existing advanced
operational numerical ocean forecasting system (NOFS) and AI-based ocean
forecasting system (AI-OFS) in temperature, salinity, sea level anomaly and
current prediction, with a lead time of more than 10 days. Our study highlights
that dynamics-enhanced deep learning (LanTu) can be a powerful paradigm for
eddy-resolving ocean forecasting.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [184] [SpecWav-Attack: Leveraging Spectrogram Resizing and Wav2Vec 2.0 for Attacking Anonymized Speech](https://arxiv.org/abs/2505.09616)
*Yuqi Li,Yuanzhong Zheng,Zhongtian Guo,Yaoxuan Wang,Jianjun Yin,Haojun Fei*

Main category: cs.SD

TL;DR: SpecWav-Attack是一种对抗性模型，用于检测匿名化语音中的说话者。它结合了Wav2Vec2的特征提取、频谱图调整和增量训练，性能优于传统攻击方法。


<details>
  <summary>Details</summary>
Motivation: 揭示匿名化语音系统中的漏洞，并强调需要更强的防御措施。

Method: 利用Wav2Vec2进行特征提取，并结合频谱图调整和增量训练。

Result: 在librispeech-dev和librispeech-test数据集上表现优于传统攻击方法。

Conclusion: SpecWav-Attack展示了匿名化语音系统的脆弱性，并呼吁加强防御措施。

Abstract: This paper presents SpecWav-Attack, an adversarial model for detecting
speakers in anonymized speech. It leverages Wav2Vec2 for feature extraction and
incorporates spectrogram resizing and incremental training for improved
performance. Evaluated on librispeech-dev and librispeech-test, SpecWav-Attack
outperforms conventional attacks, revealing vulnerabilities in anonymized
speech systems and emphasizing the need for stronger defenses, benchmarked
against the ICASSP 2025 Attacker Challenge.

</details>


### [185] [Introducing voice timbre attribute detection](https://arxiv.org/abs/2505.09661)
*Jinghao He,Zhengyan Sheng,Liping Chen,Kong Aik Lee,Zhen-Hua Ling*

Main category: cs.SD

TL;DR: 这篇论文提出了语音音色属性检测（vTAD）任务，旨在通过感官属性描述语音音色。基于说话人嵌入的框架在VCTK-RVA数据集上测试，ECAPA-TDNN和FACodec两种编码器在不同场景中表现各异。


<details>
  <summary>Details</summary>
Motivation: 研究语音信号传递的音色，并通过人类感知的感官属性对其进行解释，以提升对语音音色的理解和检测能力。

Method: 提出vTAD任务，通过比较语音片段的音色描述符强度，并基于说话人嵌入构建框架，使用ECAPA-TDNN和FACodec编码器进行实验。

Result: ECAPA-TDNN在测试说话人包含于训练集时表现更好，FACodec在不包含时展示更强的泛化能力。

Conclusion: 不同编码器适用于不同场景，vTAD任务和开源资源为语音音色研究提供了新工具。

Abstract: This paper focuses on explaining the timbre conveyed by speech signals and
introduces a task termed voice timbre attribute detection (vTAD). In this task,
voice timbre is explained with a set of sensory attributes describing its human
perception. A pair of speech utterances is processed, and their intensity is
compared in a designated timbre descriptor. Moreover, a framework is proposed,
which is built upon the speaker embeddings extracted from the speech
utterances. The investigation is conducted on the VCTK-RVA dataset.
Experimental examinations on the ECAPA-TDNN and FACodec speaker encoders
demonstrated that: 1) the ECAPA-TDNN speaker encoder was more capable in the
seen scenario, where the testing speakers were included in the training set; 2)
the FACodec speaker encoder was superior in the unseen scenario, where the
testing speakers were not part of the training, indicating enhanced
generalization capability. The VCTK-RVA dataset and open-source code are
available on the website https://github.com/vTAD2025-Challenge/vTAD.

</details>


### [186] [LAV: Audio-Driven Dynamic Visual Generation with Neural Compression and StyleGAN2](https://arxiv.org/abs/2505.10101)
*Jongmin Jung,Dasaem Jeong*

Main category: cs.SD

TL;DR: LAV结合EnCodec音频压缩和StyleGAN2的生成能力，通过音频驱动的潜在表示生成动态视觉输出。


<details>
  <summary>Details</summary>
Motivation: 探索利用预训练音频压缩模型进行艺术和计算应用的可能性，避免显式特征映射的局限性。

Method: 使用EnCodec嵌入作为潜在表示，通过随机初始化的线性映射直接转换到StyleGAN2的风格潜在空间。

Result: 该方法保持了转换中的语义丰富性，实现了细腻且语义一致的音频-视觉转换。

Conclusion: LAV展示了预训练音频压缩模型在艺术和计算应用中的潜力。

Abstract: This paper introduces LAV (Latent Audio-Visual), a system that integrates
EnCodec's neural audio compression with StyleGAN2's generative capabilities to
produce visually dynamic outputs driven by pre-recorded audio. Unlike previous
works that rely on explicit feature mappings, LAV uses EnCodec embeddings as
latent representations, directly transformed into StyleGAN2's style latent
space via randomly initialized linear mapping. This approach preserves semantic
richness in the transformation, enabling nuanced and semantically coherent
audio-visual translations. The framework demonstrates the potential of using
pretrained audio compression models for artistic and computational
applications.

</details>


### [187] [Detecting Musical Deepfakes](https://arxiv.org/abs/2505.09633)
*Nick Sunday*

Main category: cs.SD

TL;DR: 论文研究了如何通过Mel频谱图和CNN检测AI生成的音乐，同时探讨了TTM平台的社会伦理影响。


<details>
  <summary>Details</summary>
Motivation: 随着TTM平台的普及，AI生成音乐带来了新的挑战，需要识别真假音乐以保护艺术家和行业。

Method: 使用FakeMusicCaps数据集，通过节奏拉伸和音高变换模拟对抗条件，生成Mel频谱图并用CNN进行分类。

Result: 论文展示了技术检测结果，并验证了该方法在对抗条件下的有效性。

Conclusion: 设计严密的检测系统对保护艺术和发挥生成式AI在音乐中的积极作用至关重要。

Abstract: The proliferation of Text-to-Music (TTM) platforms has democratized music
creation, enabling users to effortlessly generate high-quality compositions.
However, this innovation also presents new challenges to musicians and the
broader music industry. This study investigates the detection of AI-generated
songs using the FakeMusicCaps dataset by classifying audio as either deepfake
or human. To simulate real-world adversarial conditions, tempo stretching and
pitch shifting were applied to the dataset. Mel spectrograms were generated
from the modified audio, then used to train and evaluate a convolutional neural
network. In addition to presenting technical results, this work explores the
ethical and societal implications of TTM platforms, arguing that carefully
designed detection systems are essential to both protecting artists and
unlocking the positive potential of generative AI in music.

</details>


### [188] [Learning Nonlinear Dynamics in Physical Modelling Synthesis using Neural Ordinary Differential Equations](https://arxiv.org/abs/2505.10511)
*Victor Zheleznov,Stefan Bilbao,Alec Wright,Simon King*

Main category: cs.SD

TL;DR: 该论文提出了一种结合模态分解与神经常微分方程的方法，用于建模分布式音乐系统中的非线性动力学行为，并通过合成数据验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决高振幅振动弦的非线性效应（如音高滑移和亮度依赖）的建模问题，传统方法难以处理此类几何非线性问题。

Method: 结合模态分解和神经常微分方程，利用线性振动的解析解，通过神经网络捕捉非线性动态行为，无需网络架构中的参数编码器。

Result: 模型成功复现了非线性横弦的动态行为，并通过合成数据和声音示例验证了其有效性。

Conclusion: 该方法为分布式音乐系统的非线性建模提供了新思路，物理参数在训练后仍易于访问，展现了实用潜力。

Abstract: Modal synthesis methods are a long-standing approach for modelling
distributed musical systems. In some cases extensions are possible in order to
handle geometric nonlinearities. One such case is the high-amplitude vibration
of a string, where geometric nonlinear effects lead to perceptually important
effects including pitch glides and a dependence of brightness on striking
amplitude. A modal decomposition leads to a coupled nonlinear system of
ordinary differential equations. Recent work in applied machine learning
approaches (in particular neural ordinary differential equations) has been used
to model lumped dynamic systems such as electronic circuits automatically from
data. In this work, we examine how modal decomposition can be combined with
neural ordinary differential equations for modelling distributed musical
systems. The proposed model leverages the analytical solution for linear
vibration of system's modes and employs a neural network to account for
nonlinear dynamic behaviour. Physical parameters of a system remain easily
accessible after the training without the need for a parameter encoder in the
network architecture. As an initial proof of concept, we generate synthetic
data for a nonlinear transverse string and show that the model can be trained
to reproduce the nonlinear dynamics of the system. Sound examples are
presented.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [189] [UOD: Universal One-shot Detection of Anatomical Landmarks](https://arxiv.org/abs/2306.07615)
*Heqin Zhu,Quan Quan,Qingsong Yao,Zaiyi Liu,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种名为UOD的领域自适应一次性医学标志物检测框架，以处理多领域医学图像。通过两阶段设计和两个通用模型，UOD能够利用单一样本进行训练，并在多领域中实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 现有的一次性学习方法在单一领域中表现良好，但面对多领域未标记数据时存在领域偏好问题，且对次优标注图像表现不佳。为了解决这些问题，作者提出了UOD框架。

Method: UOD框架分为两阶段：第一阶段通过自监督学习生成伪标志物标签；第二阶段设计领域自适应Transformer消除领域偏好并构建全局上下文。两个阶段均使用包含领域特定和领域共享模块的通用模型。

Result: 在三个不同解剖领域的公共X射线数据集上，UOD均达到了最先进的性能，展现了其在多领域医学图像中的鲁棒性和高精度。

Conclusion: UOD框架通过领域自适应设计有效解决了多领域医学图像中的标志物检测问题，即使每领域仅有一个标注样本，也能实现鲁棒且准确的检测。

Abstract: One-shot medical landmark detection gains much attention and achieves great
success for its label-efficient training process. However, existing one-shot
learning methods are highly specialized in a single domain and suffer domain
preference heavily in the situation of multi-domain unlabeled data. Moreover,
one-shot learning is not robust that it faces performance drop when annotating
a sub-optimal image. To tackle these issues, we resort to developing a
domain-adaptive one-shot landmark detection framework for handling multi-domain
medical images, named Universal One-shot Detection (UOD). UOD consists of two
stages and two corresponding universal models which are designed as
combinations of domain-specific modules and domain-shared modules. In the first
stage, a domain-adaptive convolution model is self-supervised learned to
generate pseudo landmark labels. In the second stage, we design a
domain-adaptive transformer to eliminate domain preference and build the global
context for multi-domain data. Even though only one annotated sample from each
domain is available for training, the domain-shared modules help UOD aggregate
all one-shot samples to detect more robust and accurate landmarks. We
investigated both qualitatively and quantitatively the proposed UOD on three
widely-used public X-ray datasets in different anatomical domains (i.e., head,
hand, chest) and obtained state-of-the-art performances in each domain. The
code is available at
https://github.com/heqin-zhu/UOD_universal_oneshot_detection.

</details>


### [190] [Unlocking Location Intelligence: A Survey from Deep Learning to The LLM Era](https://arxiv.org/abs/2505.09651)
*Xixuan Hao,Yutian Jiang,Xingchen Zou,Jiabo Liu,Yifang Yin,Yuxuan Liang*

Main category: cs.CV

TL;DR: 本文综述了地理空间表示学习在深度学习和大语言模型（LLM）时代的发展，重点关注数据、方法和应用三个视角，总结了当前进展、局限性和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 地理空间智能（LI）的快速发展需要整合深度学习和大语言模型的最新成果，以提升跨模态地理空间推理和非结构化地理文本数据处理能力。

Method: 本文通过分类整理地理空间表示学习的完整流程，包括数据、方法和应用三大视角，结合文献综述和案例分析。

Result: 总结了地理空间表示学习在不同技术时代的最新进展，提出了现有局限性和未来可能的研究方向。

Conclusion: 地理空间表示学习在LLM时代展现出巨大潜力，但仍需解决数据质量和模型解释性等问题，本文为后续研究提供了路线图。

Abstract: Location Intelligence (LI), the science of transforming location-centric
geospatial data into actionable knowledge, has become a cornerstone of modern
spatial decision-making. The rapid evolution of Geospatial Representation
Learning is fundamentally reshaping LI development through two successive
technological revolutions: the deep learning breakthrough and the emerging
large language model (LLM) paradigm. While deep neural networks (DNNs) have
demonstrated remarkable success in automated feature extraction from structured
geospatial data (e.g., satellite imagery, GPS trajectories), the recent
integration of LLMs introduces transformative capabilities for cross-modal
geospatial reasoning and unstructured geo-textual data processing. This survey
presents a comprehensive review of geospatial representation learning across
both technological eras, organizing them into a structured taxonomy based on
the complete pipeline comprising: (1) data perspective, (2) methodological
perspective and (3) application perspective. We also highlight current
advancements, discuss existing limitations, and propose potential future
research directions in the LLM era. This work offers a thorough exploration of
the field and providing a roadmap for further innovation in LI. The summary of
the up-to-date paper list can be found in
https://github.com/CityMind-Lab/Awesome-Location-Intelligence and will undergo
continuous updates.

</details>


### [191] [Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering](https://arxiv.org/abs/2505.10118)
*Yangfu Li,Hongjian Zhan,Tianyi Chen,Qi Liu,Yue Lu*

Main category: cs.CV

TL;DR: 该论文提出了一种多目标平衡覆盖方法（MoB），通过动态平衡提示对齐和视觉保留的目标，解决了现有视觉标记剪枝方法在任务间性能不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉标记剪枝方法采用静态策略，忽略了提示对齐和视觉保留的相对重要性在不同任务中的变化，从而导致性能不稳定。

Method: 基于Hausdorff距离和ε-覆盖理论，论文首次推导了视觉标记剪枝的闭式误差界，并提出MoB方法，将剪枝问题转化为双目标覆盖问题，通过贪心半径分配实现优化。

Result: 实验表明，MoB在仅使用原视觉标记的11.1%的情况下，保留了LLaVA-1.5-7B的96.4%性能，并使LLaVA-Next-7B加速1.3-1.5倍，同时性能损失可忽略。

Conclusion: MoB不仅具有理论保证和线性扩展性，还能无缝集成到先进的MLLMs和多模态任务中，展示了广泛的适用性。

Abstract: Existing visual token pruning methods target prompt alignment and visual
preservation with static strategies, overlooking the varying relative
importance of these objectives across tasks, which leads to inconsistent
performance. To address this, we derive the first closed-form error bound for
visual token pruning based on the Hausdorff distance, uniformly characterizing
the contributions of both objectives. Moreover, leveraging $\epsilon$-covering
theory, we reveal an intrinsic trade-off between these objectives and quantify
their optimal attainment levels under a fixed budget. To practically handle
this trade-off, we propose Multi-Objective Balanced Covering (MoB), which
reformulates visual token pruning as a bi-objective covering problem. In this
framework, the attainment trade-off reduces to budget allocation via greedy
radius trading. MoB offers a provable performance bound and linear scalability
with respect to the number of input visual tokens, enabling adaptation to
challenging pruning scenarios. Extensive experiments show that MoB preserves
96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual
tokens and accelerates LLaVA-Next-7B by 1.3-1.5$\times$ with negligible
performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm
that MoB integrates seamlessly into advanced MLLMs and diverse vision-language
tasks.

</details>


### [192] [On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging](https://arxiv.org/abs/2505.10231)
*Haozhe Luo,Ziyu Zhou,Zixin Shu,Aurélie Pahud de Mortanges,Robert Berke,Mauricio Reyes*

Main category: cs.CV

TL;DR: 本文探索了人类-AI对齐在医学影像中的公平性，发现人类洞察能减少公平差距并提升泛化能力，但过度对齐可能带来性能权衡，需要平衡策略。


<details>
  <summary>Details</summary>
Motivation: 解决医学AI中的偏见问题，尤其是跨人口群体的公平性差距，探索人类-AI对齐的潜力。

Method: 系统化研究人类-AI对齐对公平性和泛化的影响，通过人类洞察与AI结合的方式。

Result: 人类-AI对齐能减少公平差距并增强泛化，但过度对齐可能导致性能折衷。

Conclusion: 人类-AI对齐是开发公平且稳健医学AI的有效方法，需平衡人类指导与自动化效率。

Abstract: Deep neural networks excel in medical imaging but remain prone to biases,
leading to fairness gaps across demographic groups. We provide the first
systematic exploration of Human-AI alignment and fairness in this domain. Our
results show that incorporating human insights consistently reduces fairness
gaps and enhances out-of-domain generalization, though excessive alignment can
introduce performance trade-offs, emphasizing the need for calibrated
strategies. These findings highlight Human-AI alignment as a promising approach
for developing fair, robust, and generalizable medical AI systems, striking a
balance between expert guidance and automated efficiency. Our code is available
at https://github.com/Roypic/Aligner.

</details>


### [193] [StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation](https://arxiv.org/abs/2505.10292)
*Daniel A. P. Oliveira,David Martins de Matos*

Main category: cs.CV

TL;DR: 论文提出了StoryReasoning数据集和Qwen Storyteller模型，通过跨帧对象重识别和链式推理，减少了视觉故事生成中的指代幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 解决视觉故事生成中角色身份和动作链接的指代幻觉问题，通过实体视觉元素的基础来实现。

Method: 使用StoryReasoning数据集，结合视觉相似性和人脸识别进行跨帧对象重识别，采用链式推理和基础方案链接文本与视觉实体。

Result: 微调后的Qwen Storyteller模型将平均每个故事的指代幻觉从4.06减少到3.56（-12.3%）。

Conclusion: 提出的方法和模型有效减少了视觉故事中的指代幻觉，提高了生成的连贯性和准确性。

Abstract: Visual storytelling systems struggle to maintain character identity across
frames and link actions to appropriate subjects, frequently leading to
referential hallucinations. These issues can be addressed through grounding of
characters, objects, and other entities on the visual elements. We propose
StoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie
images, with both structured scene analyses and grounded stories. Each story
maintains character and object consistency across frames while explicitly
modeling multi-frame relationships through structured tabular representations.
Our approach features cross-frame object re-identification using visual
similarity and face recognition, chain-of-thought reasoning for explicit
narrative modeling, and a grounding scheme that links textual elements to
visual entities across multiple frames. We establish baseline performance by
fine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end
object detection, re-identification, and landmark detection while maintaining
consistent object references throughout the story. Evaluation demonstrates a
reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when
compared to a non-fine-tuned model.

</details>


### [194] [MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning](https://arxiv.org/abs/2505.10557)
*Ke Wang,Junting Pan,Linda Wei,Aojun Zhou,Weikang Shi,Zimu Lu,Han Xiao,Yunqiao Yang,Houxing Ren,Mingjie Zhan,Hongsheng Li*

Main category: cs.CV

TL;DR: 该论文提出利用代码作为监督信号来改进多模态数学推理，开发了FigCodifier模型和ImgCode-8.6M数据集，并构建了MM-MathInstruct-3M微调数据集，最终训练的MathCoder-VL模型在多个指标上达到了开源SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言图像描述数据集主要关注自然场景，忽略了数学图形中的复杂细节，限制了当前大型多模态模型（LMMs）在多模态数学推理中的进展。

Method: 通过代码作为跨模态对齐的监督信号，开发了图像到代码的模型FigCodifier和数据集ImgCode-8.6M，并进一步构建了MM-MathInstruct-3M微调数据集，最终训练了MathCoder-VL模型。

Result: MathCoder-VL在六个指标上达到了开源SOTA，尤其在MathVista的几何问题子集上超越了GPT-4o和Claude 3.5 Sonnet，分别提高了8.9%和9.2%。

Conclusion: 通过代码监督和高质量数据集的构建，显著提升了多模态数学推理能力，为开源社区提供了新的工具和资源。

Abstract: Natural language image-caption datasets, widely used for training Large
Multimodal Models, mainly focus on natural scenarios and overlook the intricate
details of mathematical figures that are critical for problem-solving,
hindering the advancement of current LMMs in multimodal mathematical reasoning.
To this end, we propose leveraging code as supervision for cross-modal
alignment, since code inherently encodes all information needed to generate
corresponding figures, establishing a precise connection between the two
modalities. Specifically, we co-develop our image-to-code model and dataset
with model-in-the-loop approach, resulting in an image-to-code model,
FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.
Furthermore, we utilize FigCodifier to synthesize novel mathematical figures
and then construct MM-MathInstruct-3M, a high-quality multimodal math
instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with
ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on
MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a
new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and
Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista,
achieving improvements of 8.9% and 9.2%. The dataset and models will be
released at https://github.com/mathllm/MathCoder.

</details>


### [195] [AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection](https://arxiv.org/abs/2505.09926)
*Bin-Bin Gao,Yue Zhu,Jiangtao Yan,Yuezhi Cai,Weixi Zhang,Meng Wang,Jun Liu,Yong Liu,Lei Wang,Chengjie Wang*

Main category: cs.CV

TL;DR: AdaptCLIP是一种简单有效的通用视觉异常检测方法，通过交替学习和比较学习改进CLIP模型，无需微调即可实现跨领域的零/少样本泛化，在12个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在无微调的开放场景中难以灵活设计提示模板或处理复杂令牌交互，AdaptCLIP通过交替学习和多特征比较来解决这些问题。

Method: AdaptCLIP在CLIP模型基础上添加三个轻量适配器（视觉、文本、提示-查询），采用交替学习视觉与文本表示，并结合上下文和对齐残差特征进行对比学习。

Result: 在工业和医疗领域的12个异常检测基准测试中，AdaptCLIP显著优于现有方法，实现最先进的性能。

Conclusion: AdaptCLIP通过简单适配器和创新学习方法，高效解决了跨领域视觉异常检测问题，具有无需微调和高泛化能力的优势。

Abstract: Universal visual anomaly detection aims to identify anomalies from novel or
unseen vision domains without additional fine-tuning, which is critical in open
scenarios. Recent studies have demonstrated that pre-trained vision-language
models like CLIP exhibit strong generalization with just zero or a few normal
images. However, existing methods struggle with designing prompt templates,
complex token interactions, or requiring additional fine-tuning, resulting in
limited flexibility. In this work, we present a simple yet effective method
called AdaptCLIP based on two key insights. First, adaptive visual and textual
representations should be learned alternately rather than jointly. Second,
comparative learning between query and normal image prompt should incorporate
both contextual and aligned residual features, rather than relying solely on
residual features. AdaptCLIP treats CLIP models as a foundational service,
adding only three simple adapters, visual adapter, textual adapter, and
prompt-query adapter, at its input or output ends. AdaptCLIP supports
zero-/few-shot generalization across domains and possesses a training-free
manner on target domains once trained on a base dataset. AdaptCLIP achieves
state-of-the-art performance on 12 anomaly detection benchmarks from industrial
and medical domains, significantly outperforming existing competitive methods.
We will make the code and model of AdaptCLIP available at
https://github.com/gaobb/AdaptCLIP.

</details>


### [196] [VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety](https://arxiv.org/abs/2505.09935)
*Ahmed S. Abdelrahman,Mohamed Abdel-Aty,Quoc Dai Tran*

Main category: cs.CV

TL;DR: 该论文提出了VRU-CIPI框架，基于GRU和Transformer自注意力机制预测行人交叉口意图，准确率达96.45%，并实现实时推理（33FPS）。结合I2V通信，可提升交叉口安全性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决城市交叉口行人与车辆交互的安全问题，尤其是易受伤害道路使用者（VRU）的过街意图预测错误可能引发的危险冲突。

Method: 采用GRU捕捉VRU运动的时间动态性，结合多头Transformer自注意力机制编码上下文和空间依赖关系，构建VRU-CIPI框架。

Result: 在UCF-VRU数据集上达到96.45%准确率，推理速度33FPS；通过I2V通信提前触发信号和预警，提升交互安全性。

Conclusion: VRU-CIPI框架在预测准确性和实时性上表现优异，结合基础设施通信可有效提升交叉口安全性和交互流畅度。

Abstract: Understanding and predicting human behavior in-thewild, particularly at urban
intersections, remains crucial for enhancing interaction safety between road
users. Among the most critical behaviors are crossing intentions of Vulnerable
Road Users (VRUs), where misinterpretation may result in dangerous conflicts
with oncoming vehicles. In this work, we propose the VRU-CIPI framework with a
sequential attention-based model designed to predict VRU crossing intentions at
intersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal
dynamics in VRU movements, combined with a multi-head Transformer
self-attention mechanism to encode contextual and spatial dependencies critical
for predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed
achieves state-of-the-art performance with an accuracy of 96.45% and achieving
real-time inference speed reaching 33 frames per second. Furthermore, by
integrating with Infrastructure-to-Vehicles (I2V) communication, our approach
can proactively enhance intersection safety through timely activation of
crossing signals and providing early warnings to connected vehicles, ensuring
smoother and safer interactions for all road users.

</details>


### [197] [Application of YOLOv8 in monocular downward multiple Car Target detection](https://arxiv.org/abs/2505.10016)
*Shijie Lyu*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLOv8的改进自主目标检测网络，通过引入结构重参数化技术、双向金字塔结构网络模型及新型检测流程，显著提升了多尺度、小目标和远距离目标的检测效率与精度，检测准确率达65%。


<details>
  <summary>Details</summary>
Motivation: 针对当前自动驾驶技术中环境感知的高成本、对天气和光照条件敏感及分辨率有限等问题，需开发更高效的解决方案以提升驾驶安全与系统自主性。

Method: 在YOLOv8框架中整合结构重参数化技术、双向金字塔结构网络模型及新型检测流程，优化多尺度目标检测能力。

Result: 改进后的模型在多尺度和小目标检测中表现优异，准确率提升至65%，尤其适用于单目标和小物体检测场景。

Conclusion: 该模型在自动驾驶竞赛（如FSAC）等实际应用中具有显著潜力，尤其在精确检测需求较高的场景中展现出技术优势。

Abstract: Autonomous driving technology is progressively transforming traditional car
driving methods, marking a significant milestone in modern transportation.
Object detection serves as a cornerstone of autonomous systems, playing a vital
role in enhancing driving safety, enabling autonomous functionality, improving
traffic efficiency, and facilitating effective emergency responses. However,
current technologies such as radar for environmental perception, cameras for
road perception, and vehicle sensor networks face notable challenges, including
high costs, vulnerability to weather and lighting conditions, and limited
resolution.To address these limitations, this paper presents an improved
autonomous target detection network based on YOLOv8. By integrating structural
reparameterization technology, a bidirectional pyramid structure network model,
and a novel detection pipeline into the YOLOv8 framework, the proposed approach
achieves highly efficient and precise detection of multi-scale, small, and
remote objects. Experimental results demonstrate that the enhanced model can
effectively detect both large and small objects with a detection accuracy of
65%, showcasing significant advancements over traditional methods.This improved
model holds substantial potential for real-world applications and is
well-suited for autonomous driving competitions, such as the Formula Student
Autonomous China (FSAC), particularly excelling in scenarios involving
single-target and small-object detection.

</details>


### [198] [ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction](https://arxiv.org/abs/2505.10027)
*Shijie Lyu*

Main category: cs.CV

TL;DR: 论文提出了一种基于强化学习的潜在扩散模型（LDM）微调方法，用于提升遥感图像超分辨率重建的质量，尤其在复杂场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着遥感技术的快速发展，超分辨率图像重建具有重要研究意义。现有深度学习方法虽取得进展，但在复杂场景和细节保留上仍有限制。

Method: 通过构建强化学习环境（状态、动作、奖励），在LDM模型的反向去噪过程中使用近端策略优化（PPO）优化决策目标。

Result: 在RESISC45数据集上，PSNR提升3-4dB，SSIM提高0.08-0.11，LPIPS降低0.06-0.10，尤其在结构化复杂自然场景中表现突出。

Conclusion: 该方法显著提升了超分辨率质量和场景适应性，证明了其有效性。

Abstract: With the rapid advancement of remote sensing technology, super-resolution
image reconstruction is of great research and practical significance. Existing
deep learning methods have made progress but still face limitations in handling
complex scenes and preserving image details. This paper proposes a
reinforcement learning-based latent diffusion model (LDM) fine-tuning method
for remote sensing image super-resolution. The method constructs a
reinforcement learning environment with states, actions, and rewards,
optimizing decision objectives through proximal policy optimization (PPO)
during the reverse denoising process of the LDM model. Experiments on the
RESISC45 dataset show significant improvements over the baseline model in PSNR,
SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,
and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural
scenes. The results demonstrate the method's effectiveness in enhancing
super-resolution quality and adaptability across scenes.

</details>


### [199] [PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language](https://arxiv.org/abs/2505.10055)
*Ijazul Haq,Yingjie Zhang,Irfan Ali Khan*

Main category: cs.CV

TL;DR: 该论文评估了大型多模态模型（LMMs）在低资源普什图语中的光学字符识别（OCR）性能，构建了合成数据集PsOCR并进行基准测试，结果显示Gemini表现最佳，Qwen-7B在开源模型中领先。


<details>
  <summary>Details</summary>
Motivation: 普什图语的NLP面临因脚本书写方式及数据集稀缺带来的挑战，需构建专用数据集并评估现有模型的OCR能力。

Method: 开发了包含100万张图像的合成数据集PsOCR，覆盖多种字体和布局，并选取10K图像作为基准测试集，评估多款LMMs的性能。

Result: Gemini在所有模型中表现最优，开源模型中Qwen-7B最佳；PsOCR为后续研究提供了基础。

Conclusion: 研究揭示了当前LMMs在普什图语OCR中的能力与局限，为类似文字（如阿拉伯语、波斯语）的研究奠定了基础。

Abstract: This paper evaluates the performance of Large Multimodal Models (LMMs) on
Optical Character Recognition (OCR) in the low-resource Pashto language.
Natural Language Processing (NLP) in Pashto faces several challenges due to the
cursive nature of its script and a scarcity of structured datasets. To address
this, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one
million images annotated with bounding boxes at word, line, and document
levels, suitable for training and evaluating models based on different
architectures, including Convolutional Neural Networks (CNNs) and Transformers.
PsOCR covers variations across 1,000 unique font families, colors, image sizes,
and layouts. A benchmark subset of 10K images was selected to evaluate the
performance of several LMMs, including seven open-source models: DeepSeek's
Janus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four
closed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results
demonstrate that Gemini achieves the best performance among all models, whereas
among open-source models, Qwen-7B stands out. This work provides an insightful
assessment of the capabilities and limitations of current LMMs for OCR tasks in
Pashto and establishes a foundation for further research not only in Pashto OCR
but also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is
available at https://github.com/zirak-ai/PashtoOCR.

</details>


### [200] [Modeling Saliency Dataset Bias](https://arxiv.org/abs/2505.10169)
*Matthias Kümmerer,Harneet Khanuja,Matthias Bethge*

Main category: cs.CV

TL;DR: 该论文提出了一种新型架构，通过少量数据集特定参数（少于20个）来解决跨数据集显著性预测中的泛化差距问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有显著性预测模型在不同数据集间表现差异大，即使增加数据集多样性也无法完全解决数据集偏差问题。

Method: 采用数据集无关的编码器-解码器结构，引入少于20个数据集特定参数，控制多尺度结构、中心偏差和注视点扩散等可解释机制。

Result: 新模型在MIT/Tuebingen显著性基准测试（MIT300、CAT2000和COCO-Freeview）中达到最先进水平，泛化差距减少75%以上。

Conclusion: 该方法不仅显著提升了跨数据集性能，还揭示了空间显著性的复杂多尺度效应。

Abstract: Recent advances in image-based saliency prediction are approaching gold
standard performance levels on existing benchmarks. Despite this success, we
show that predicting fixations across multiple saliency datasets remains
challenging due to dataset bias. We find a significant performance drop (around
40%) when models trained on one dataset are applied to another. Surprisingly,
increasing dataset diversity does not resolve this inter-dataset gap, with
close to 60% attributed to dataset-specific biases. To address this remaining
generalization gap, we propose a novel architecture extending a mostly
dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific
parameters that govern interpretable mechanisms such as multi-scale structure,
center bias, and fixation spread. Adapting only these parameters to new data
accounts for more than 75% of the generalization gap, with a large fraction of
the improvement achieved with as few as 50 samples. Our model sets a new
state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark
(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from
unrelated datasets, but with a substantial boost when adapting to the
respective training datasets. The model also provides valuable insights into
spatial saliency properties, revealing complex multi-scale effects that combine
both absolute and relative sizes.

</details>


### [201] [SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\mathcal{O}(T)$ Complexity](https://arxiv.org/abs/2505.10352)
*Shihao Zou,Qingfeng Li,Wei Ji,Jingjing Li,Yongkui Yang,Guoqi Li,Chao Dong*

Main category: cs.CV

TL;DR: 论文提出了一种高效的SpikeVideoFormer，解决现有SNN Transformer在视频任务中效率不足的问题，采用线性时间复杂度的SDHA注意力机制，性能优于现有SNN方法，并匹配ANN方法的性能同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 针对现有SNN Transformer主要关注单图像任务，未能有效利用SNN在视频任务中的高效性，提出了一种高效的SpikeVideoFormer来解决这一问题。

Method: 设计了Spike-driven Hamming Attention (SDHA)，将传统实值注意力适应为尖峰驱动注意力，并分析了多种尖峰驱动的时空注意力设计，选择了最优方案，保持线性时间复杂度。

Result: 在视频分类、人体姿态跟踪和语义分割任务中，性能优于现有SNN方法（提升超过15%），并匹配ANN方法的性能，同时在三个任务上分别实现了16倍、10倍和5倍的效率提升。

Conclusion: SpikeVideoFormer在视频任务中表现出卓越的泛化能力和效率，为SNN在视频领域的应用提供了有竞争力的解决方案。

Abstract: Spiking Neural Networks (SNNs) have shown competitive performance to
Artificial Neural Networks (ANNs) in various vision tasks, while offering
superior energy efficiency. However, existing SNN-based Transformers primarily
focus on single-image tasks, emphasizing spatial features while not effectively
leveraging SNNs' efficiency in video-based vision tasks. In this paper, we
introduce SpikeVideoFormer, an efficient spike-driven video Transformer,
featuring linear temporal complexity $\mathcal{O}(T)$. Specifically, we design
a spike-driven Hamming attention (SDHA) which provides a theoretically guided
adaptation from traditional real-valued attention to spike-driven attention.
Building on SDHA, we further analyze various spike-driven space-time attention
designs and identify an optimal scheme that delivers appealing performance for
video tasks, while maintaining only linear temporal complexity. The
generalization ability and efficiency of our model are demonstrated across
diverse downstream video tasks, including classification, human pose tracking,
and semantic segmentation. Empirical results show our method achieves
state-of-the-art (SOTA) performance compared to existing SNN approaches, with
over 15\% improvement on the latter two tasks. Additionally, it matches the
performance of recent ANN-based methods while offering significant efficiency
gains, achieving $\times 16$, $\times 10$ and $\times 5$ improvements on the
three tasks. https://github.com/JimmyZou/SpikeVideoFormer

</details>


### [202] [Learned Lightweight Smartphone ISP with Unpaired Data](https://arxiv.org/abs/2505.10420)
*Andrei Arhire,Radu Timofte*

Main category: cs.CV

TL;DR: 该论文提出了一种无需配对数据的轻量级智能手机ISP训练方法，通过对抗训练和多判别器提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 开发学习型ISP时，获取像素级对齐的配对数据成本高昂且困难。本文旨在解决这一挑战，提出无需严格配对数据的训练方法。

Method: 采用无配对训练策略，结合多损失函数和对抗训练，利用预训练网络的特征图保持内容结构，学习目标RGB数据集的颜色和纹理特征。

Result: 在Zurich RAW to RGB和Fujifilm UltraISP数据集上评估，无配对学习方法在多项指标上表现优异，接近配对方法的性能。

Conclusion: 该方法展示了无配对数据训练ISP的潜力，适合移动设备部署，代码和预训练模型已开源。

Abstract: The Image Signal Processor (ISP) is a fundamental component in modern
smartphone cameras responsible for conversion of RAW sensor image data to RGB
images with a strong focus on perceptual quality. Recent work highlights the
potential of deep learning approaches and their ability to capture details with
a quality increasingly close to that of professional cameras. A difficult and
costly step when developing a learned ISP is the acquisition of pixel-wise
aligned paired data that maps the raw captured by a smartphone camera sensor to
high-quality reference images. In this work, we address this challenge by
proposing a novel training method for a learnable ISP that eliminates the need
for direct correspondences between raw images and ground-truth data with
matching content. Our unpaired approach employs a multi-term loss function
guided by adversarial training with multiple discriminators processing feature
maps from pre-trained networks to maintain content structure while learning
color and texture characteristics from the target RGB dataset. Using
lightweight neural network architectures suitable for mobile devices as
backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm
UltraISP datasets. Compared to paired training methods, our unpaired learning
strategy shows strong potential and achieves high fidelity across multiple
evaluation metrics. The code and pre-trained models are available at
https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .

</details>


### [203] [DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera](https://arxiv.org/abs/2505.10030)
*Miit Daga,Dhriti Parikh,Swarna Priya Ramu*

Main category: cs.CV

TL;DR: DeepSeqCoco是一种基于深度学习的模型，用于从椰树图像中准确自动识别疾病，其混合SGD-Adam优化器在验证损失最低（2.81%）的情况下达到99.5%的准确率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 椰树疾病严重威胁农业产量，尤其在发展中国家，传统方法难以早期诊断且效率低下，亟需自动化、高效的解决方案。

Method: 提出DeepSeqCoco模型，测试SGD、Adam及混合优化器配置，以平衡准确性、损失最小化和计算成本。

Result: 模型准确率达99.5%（比现有模型高5%），训练时间减少18%，预测时间降低85%，验证损失仅2.81%。

Conclusion: DeepSeqCoco展示了通过AI实现精准农业的潜力，提供可扩展且高效的疾病监测系统。

Abstract: Coconut tree diseases are a serious risk to agricultural yield, particularly
in developing countries where conventional farming practices restrict early
diagnosis and intervention. Current disease identification methods are manual,
labor-intensive, and non-scalable. In response to these limitations, we come up
with DeepSeqCoco, a deep learning based model for accurate and automatic
disease identification from coconut tree images. The model was tested under
various optimizer settings, such as SGD, Adam, and hybrid configurations, to
identify the optimal balance between accuracy, minimization of loss, and
computational cost. Results from experiments indicate that DeepSeqCoco can
achieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than
existing models) with the hybrid SGD-Adam showing the lowest validation loss of
2.81%. It also shows a drop of up to 18% in training time and up to 85% in
prediction time for input images. The results point out the promise of the
model to improve precision agriculture through an AI-based, scalable, and
efficient disease monitoring system.

</details>


### [204] [Vision language models have difficulty recognizing virtual objects](https://arxiv.org/abs/2505.10453)
*Tyler Tran,Sangeet Khemlani,J. G. Trafton*

Main category: cs.CV

TL;DR: 该论文研究了视觉语言模型（VLMs）对图像中虚拟对象的理解能力，发现现有模型在这方面表现不足。


<details>
  <summary>Details</summary>
Motivation: 探索VLM是否真正理解图像的视觉空间属性，通过测试其对虚拟对象的处理能力来验证其场景理解水平。

Method: 通过设计包含虚拟对象的提示（如"想象风筝卡在树上"）来评估VLM对场景的空间关系推理能力。

Result: 当前最先进的VLM在处理虚拟对象时表现不佳，说明其场景理解能力有限。

Conclusion: VLM在虚拟对象的理解上仍需改进，未来研究需进一步提升其场景推理能力。

Abstract: Vision language models (VLMs) are AI systems paired with both language and
vision encoders to process multimodal input. They are capable of performing
complex semantic tasks such as automatic captioning, but it remains an open
question about how well they comprehend the visuospatial properties of scenes
depicted in the images they process. We argue that descriptions of virtual
objects -- objects that are not visually represented in an image -- can help
test scene comprehension in these AI systems. For example, an image that
depicts a person standing under a tree can be paired with the following prompt:
imagine that a kite is stuck in the tree. VLMs that comprehend the scene should
update their representations and reason sensibly about the spatial relations
between all three objects. We describe systematic evaluations of
state-of-the-art VLMs and show that their ability to process virtual objects is
inadequate.

</details>


### [205] [UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.10483)
*Yi Li,Haonan Wang,Qixiang Zhang,Boyu Xiao,Chenchang Hu,Hualiang Wang,Xiaomeng Li*

Main category: cs.CV

TL;DR: UniEval是首个无需额外模型、图像或标注的统一多模态模型评估框架，包含多样化基准UniBench和高效评分UniScore，实验证明其挑战性和与人类评估的一致性优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前统一多模态模型评估缺乏统一框架，存在冗余、依赖标注数据和指标局限性问题。

Method: 提出UniEval框架，包含多样化的UniBench基准和UniScore评分指标，支持无额外资源的评估。

Result: UniBench更具挑战性，UniScore与人类评估高度一致，且在测评中揭示了新发现。

Conclusion: UniEval为统一多模态模型提供了一种高效、简化的评估方案，优于现有方法。

Abstract: The emergence of unified multimodal understanding and generation models is
rapidly attracting attention because of their ability to enhance
instruction-following capabilities while minimizing model redundancy. However,
there is a lack of a unified evaluation framework for these models, which would
enable an elegant, simplified, and overall evaluation. Current models conduct
evaluations on multiple task-specific benchmarks, but there are significant
limitations, such as the lack of overall results, errors from extra evaluation
models, reliance on extensive labeled images, benchmarks that lack diversity,
and metrics with limited capacity for instruction-following evaluation. To
tackle these challenges, we introduce UniEval, the first evaluation framework
designed for unified multimodal models without extra models, images, or
annotations. This facilitates a simplified and unified evaluation process. The
UniEval framework contains a holistic benchmark, UniBench (supports both
unified and visual generation models), along with the corresponding UniScore
metric. UniBench includes 81 fine-grained tags contributing to high diversity.
Experimental results indicate that UniBench is more challenging than existing
benchmarks, and UniScore aligns closely with human evaluations, surpassing
current metrics. Moreover, we extensively evaluated SoTA unified and visual
generation models, uncovering new insights into Univeral's unique values.

</details>


### [206] [Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation](https://arxiv.org/abs/2505.10223)
*Puru Vaish,Felix Meister,Tobias Heimann,Christoph Brune,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 论文探讨了医学图像分割模型在真实临床环境中因训练与测试分布不匹配而性能下降的问题，提出了MixUp和辅助傅里叶增强作为解决方案，显著提升了泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前的医学图像分割模型在训练数据和真实临床数据之间存在分布不匹配的问题，传统的视觉一致性增强方法无法应对多样化的真实场景需求。

Method: 系统评估了MixUp和辅助傅里叶增强这两种增强策略，这些方法无需针对特定分布偏移源，即可缓解多种变化的影响。

Result: 实验表明，这些方法在心脏电影MRI和前列腺MRI分割任务中显著提升了模型在分布外数据上的泛化能力和对成像变化的鲁棒性，并改善了特征表示的可分性和紧凑性。

Conclusion: 将MixUp和辅助傅里叶增强整合到nnU-Net训练流程中，提供了一种易于实施且有效的解决方案，提升了医学分割模型在真实应用中的可靠性。

Abstract: Medical image segmentation models are often trained on curated datasets,
leading to performance degradation when deployed in real-world clinical
settings due to mismatches between training and test distributions. While data
augmentation techniques are widely used to address these challenges,
traditional visually consistent augmentation strategies lack the robustness
needed for diverse real-world scenarios. In this work, we systematically
evaluate alternative augmentation strategies, focusing on MixUp and Auxiliary
Fourier Augmentation. These methods mitigate the effects of multiple variations
without explicitly targeting specific sources of distribution shifts. We
demonstrate how these techniques significantly improve out-of-distribution
generalization and robustness to imaging variations across a wide range of
transformations in cardiac cine MRI and prostate MRI segmentation. We
quantitatively find that these augmentation methods enhance learned feature
representations by promoting separability and compactness. Additionally, we
highlight how their integration into nnU-Net training pipelines provides an
easy-to-implement, effective solution for enhancing the reliability of medical
segmentation models in real-world applications.

</details>


### [207] [HandReader: Advanced Techniques for Efficient Fingerspelling Recognition](https://arxiv.org/abs/2505.10267)
*Pavel Korotaev,Petr Surovtsev,Alexander Kapitanov,Karina Kvanchiani,Aleksandr Nagaev*

Main category: cs.CV

TL;DR: 本文提出了HandReader，包含三种架构（RGB、KP、RGB+KP），用于手语拼写识别，通过TSAM和TPE模块处理视频和关键点信息，在多个数据集上达到最佳效果，并公开了俄语手语数据集Znaki。


<details>
  <summary>Details</summary>
Motivation: 现有手语拼写识别方法主要关注视频时间维度，但精度仍有提升空间，因此提出多模态架构以优化识别效果。

Method: HandReader包含三种架构：HandReader$_{RGB}$采用TSAM处理RGB视频；HandReader$_{KP}$通过TPE处理关键点张量；HandReader_RGB+KP结合两种模态。

Result: 在ChicagoFSWild、ChicagoFSWild+和Znaki数据集上均取得最先进结果。

Conclusion: HandReader的多模态设计显著提升了手语拼写识别精度，并推动了俄语手语研究资源的发展。

Abstract: Fingerspelling is a significant component of Sign Language (SL), allowing the
interpretation of proper names, characterized by fast hand movements during
signing. Although previous works on fingerspelling recognition have focused on
processing the temporal dimension of videos, there remains room for improving
the accuracy of these approaches. This paper introduces HandReader, a group of
three architectures designed to address the fingerspelling recognition task.
HandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to
process RGB features from videos of varying lengths while preserving important
sequential information. HandReader$_{KP}$ is built on the proposed Temporal
Pose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition
in a batch allows the encoder to pass them through 2D and 3D convolution
layers, utilizing temporal and spatial information and accumulating keypoints
coordinates. We also introduce HandReader_RGB+KP - architecture with a joint
encoder to benefit from RGB and keypoint modalities. Each HandReader model
possesses distinct advantages and achieves state-of-the-art results on the
ChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate
high performance on the first open dataset for Russian fingerspelling, Znaki,
presented in this paper. The Znaki dataset and HandReader pre-trained models
are publicly available.

</details>


### [208] [Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data](https://arxiv.org/abs/2505.10551)
*Yiwen Liu,Jessica Bader,Jae Myung Kim*

Main category: cs.CV

TL;DR: 研究了合成图像可行性对CLIP分类器性能的影响，发现可行性影响极小，混合可行与不可行图像训练效果无明显差异。


<details>
  <summary>Details</summary>
Motivation: 探讨合成图像中属性的可行性（即是否真实存在）是否影响CLIP分类器的训练效果，以优化合成数据的使用策略。

Method: 提出VariReal pipeline，通过文本提示对源图像进行最小编辑以生成可行或不可行属性的图像，并在三个细粒度数据集上微调CLIP模型。

Result: 可行性对CLIP性能影响极小（差异<0.3%），且不同属性对分类性能的影响不同；混合可行与不可行图像训练无显著差异。

Conclusion: 合成图像的可行性对CLIP分类器性能影响有限，无需严格区分可行与不可行图像。

Abstract: With the development of photorealistic diffusion models, models trained in
part or fully on synthetic data achieve progressively better results. However,
diffusion models still routinely generate images that would not exist in
reality, such as a dog floating above the ground or with unrealistic texture
artifacts. We define the concept of feasibility as whether attributes in a
synthetic image could realistically exist in the real-world domain; synthetic
images containing attributes that violate this criterion are considered
infeasible. Intuitively, infeasible images are typically considered
out-of-distribution; thus, training on such images is expected to hinder a
model's ability to generalize to real-world data, and they should therefore be
excluded from the training set whenever possible. However, does feasibility
really matter? In this paper, we investigate whether enforcing feasibility is
necessary when generating synthetic training data for CLIP-based classifiers,
focusing on three target attributes: background, color, and texture. We
introduce VariReal, a pipeline that minimally edits a given source image to
include feasible or infeasible attributes given by the textual prompt generated
by a large language model. Our experiments show that feasibility minimally
affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference
in top-1 accuracy across three fine-grained datasets. Also, the attribute
matters on whether the feasible/infeasible images adversarially influence the
classification performance. Finally, mixing feasible and infeasible images in
training datasets does not significantly impact performance compared to using
purely feasible or infeasible datasets.

</details>


### [209] [Enhancing Multi-Image Question Answering via Submodular Subset Selection](https://arxiv.org/abs/2505.10533)
*Aaryan Sharma,Shivansh Gupta,Samar Agarwal,Vishak Prasad C.,Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: 论文提出了一种针对多图像问答任务的改进方法，通过子模子集选择技术优化检索框架，提升检索效率和效果。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）在单图像任务中表现优异，但在多图像场景（如多图像问答）中面临扩展性和检索性能的挑战。本文旨在解决这一问题。

Method: 利用查询感知的子模函数（如GraphCut）预选语义相关图像子集，并结合锚点查询和数据增强优化检索流程。

Result: 改进后的子模检索管道在大量图像中显著提升了检索效果。

Conclusion: 子模子集选择技术能有效增强多图像任务的检索性能，尤其在数据规模较大的情况下。

Abstract: Large multimodal models (LMMs) have achieved high performance in
vision-language tasks involving single image but they struggle when presented
with a collection of multiple images (Multiple Image Question Answering
scenario). These tasks, which involve reasoning over large number of images,
present issues in scalability (with increasing number of images) and retrieval
performance. In this work, we propose an enhancement for retriever framework
introduced in MIRAGE model using submodular subset selection techniques. Our
method leverages query-aware submodular functions, such as GraphCut, to
pre-select a subset of semantically relevant images before main retrieval
component. We demonstrate that using anchor-based queries and augmenting the
data improves submodular-retriever pipeline effectiveness, particularly in
large haystack sizes.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [210] [Forests for Differences: Robust Causal Inference Beyond Parametric DiD](https://arxiv.org/abs/2505.09706)
*Hugo Gobato Souto,Francisco Louzada Neto*

Main category: stat.ME

TL;DR: DiD-BCF是一种新型非参数模型，用于解决Differences-in-Differences（DiD）估算中的关键挑战，如交错采用和异质性处理效应，提供统一的框架来估算ATE、GATE和CATE。通过基于平行趋势假设的重新参数化，提升了估算的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统DiD方法在处理复杂面板数据时面临挑战，如交错采用和异质性处理效应。DiD-BCF旨在提供更准确和稳健的因果推断工具。

Method: DiD-BCF结合非参数模型和平行趋势假设的重新参数化，估算平均处理效应（ATE）、组平均处理效应（GATE）和条件平均处理效应（CATE）。

Result: 模拟和实际应用（如美国最低工资政策）显示，DiD-BCF在非线性和异质性效应下优于传统方法，揭示了传统方法忽略的异质性效应。

Conclusion: DiD-BCF为现代DiD应用提供了更强大且灵活的因果推断工具，特别适用于复杂数据环境。

Abstract: This paper introduces the Difference-in-Differences Bayesian Causal Forest
(DiD-BCF), a novel non-parametric model addressing key challenges in DiD
estimation, such as staggered adoption and heterogeneous treatment effects.
DiD-BCF provides a unified framework for estimating Average (ATE),
Group-Average (GATE), and Conditional Average Treatment Effects (CATE). A core
innovation, its Parallel Trends Assumption (PTA)-based reparameterization,
enhances estimation accuracy and stability in complex panel data settings.
Extensive simulations demonstrate DiD-BCF's superior performance over
established benchmarks, particularly under non-linearity, selection biases, and
effect heterogeneity. Applied to U.S. minimum wage policy, the model uncovers
significant conditional treatment effect heterogeneity related to county
population, insights obscured by traditional methods. DiD-BCF offers a robust
and versatile tool for more nuanced causal inference in modern DiD
applications.

</details>


### [211] [Estimating the number of household TV profiles based in customer behaviour using Gaussian mixture model averaging](https://arxiv.org/abs/2505.10279)
*Gabriel R. Palma,Sally McClean,Brahim Allan,Zeeshan Tariq,Rafael A. Moral*

Main category: stat.ME

TL;DR: TV用户面临内容选择过多的问题，个性化推荐是关键。本文提出一种新框架，通过高斯混合模型和贝叶斯随机游走模型，结合用户行为数据，优化家庭观看群体的个性化推荐。


<details>
  <summary>Details</summary>
Motivation: 应对家庭多成员观看时个性化推荐的挑战，缺乏明确的观众身份信息和多个个体档案的结合方法。

Method: 使用高斯混合模型平均估计家庭TV档案数量，贝叶斯随机游走模型引入不确定性，结合真实用户数据验证。

Result: 框架成功估计家庭TV档案数量及其特征（包括随时间变化和不确定性量化），优化了群体观看的推荐效果。

Conclusion: 所提方法为家庭多人观看场景下的个性化推荐提供了有效解决方案，解决了档案结合和不确定性量化的挑战。

Abstract: TV customers today face many choices from many live channels and on-demand
services. Providing a personalised experience that saves customers time when
discovering content is essential for TV providers. However, a reliable
understanding of their behaviour and preferences is key. When creating
personalised recommendations for TV, the biggest challenge is understanding
viewing behaviour within households when multiple people are watching. The
objective is to detect and combine individual profiles to make
better-personalised recommendations for group viewing. Our challenge is that we
have little explicit information about who is watching the devices at any time
(individuals or groups). Also, we do not have a way to combine more than one
individual profile to make better recommendations for group viewing. We propose
a novel framework using a Gaussian mixture model averaging to obtain point
estimates for the number of household TV profiles and a Bayesian random walk
model to introduce uncertainty. We applied our approach using data from real
customers whose TV-watching data totalled approximately half a million
observations. Our results indicate that combining our framework with the
selected features provides a means to estimate the number of household TV
profiles and their characteristics, including shifts over time and
quantification of uncertainty.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [212] [Virtual Dosimetrists: A Radiotherapy Training "Flight Simulator"](https://arxiv.org/abs/2505.09796)
*Skylar S. Gay,Tucker Netherton,Barbara Marquez,Raymond Mumme,Mary Gronberg,Brent Parker,Chelsea Pinnix,Sanjay Shete,Carlos Cardenas,Laurence Court*

Main category: physics.med-ph

TL;DR: 论文提出了一种名为‘Virtual Dosimetrist’的模型，用于生成次优放疗计划的训练示例，并通过自然语言提示帮助学员改进计划质量，以解决当前基于临床的教育模式的不足。


<details>
  <summary>Details</summary>
Motivation: 当前基于临床的教育模式无法满足放疗计划质量审查的需求，尤其是在提供多样化示例和灵活展示不同规划方法方面。

Method: 开发了‘Virtual Dosimetrist’模型，通过自然语言处理结合剂量分布预测，生成次优训练计划并允许学员通过简单提示改进计划质量。

Result: 剂量生成和修改过程准确、快速且资源需求适中，首次实现了剂量分布预测与自然语言处理的结合。

Conclusion: 该模型为放疗计划审查和改进技能的训练提供了高效的解决方案，弥补了现有临床教育模式的不足。

Abstract: Effective education in radiotherapy plan quality review requires a robust,
regularly updated set of examples and the flexibility to demonstrate multiple
possible planning approaches and their consequences. However, the current
clinic-based paradigm does not support these needs. To address this, we have
developed 'Virtual Dosimetrist' models that can both generate training examples
of suboptimal treatment plans and then allow trainees to improve the plan
quality through simple natural language prompts, as if communicating with a
dosimetrist. The dose generation and modification process is accurate, rapid,
and requires only modest resources. This work is the first to combine dose
distribution prediction with natural language processing; providing a robust
pipeline for both generating suboptimal training plans and allowing trainees to
practice their critical plan review and improvement skills that addresses the
challenges of the current clinic-based paradigm.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [213] [ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation](https://arxiv.org/abs/2505.09698)
*Enyu Zhao,Vedant Raval,Hejia Zhang,Jiageng Mao,Zeyu Shangguan,Stefanos Nikolaidis,Yue Wang,Daniel Seita*

Main category: cs.RO

TL;DR: 论文提出了一个名为ManipBench的新基准，用于评估视觉语言模型（VLMs）在机器人低层次操作推理（如精确动作决策）上的表现，测试了33种代表性VLMs，发现其性能差异显著且与真实任务表现相关，但仍与人类理解存在差距。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏一个统一的基准来评估VLMs在机器人低层次操作（如精确动作决策）中的能力，因此作者提出ManipBench填补这一空白。

Method: 通过设计ManipBench基准，测试了33种不同模型家族的VLMs在物体间互动和可变形物体操作等维度的表现。

Result: VLMs在不同任务中表现差异显著，且与真实任务表现趋势强相关，但与人类理解水平仍存在较大差距。

Conclusion: ManipBench为VLM在机器人低层次推理能力的评估提供了统一标准，揭示了当前模型的局限性和改进方向。

Abstract: Vision-Language Models (VLMs) have revolutionized artificial intelligence and
robotics due to their commonsense reasoning capabilities. In robotic
manipulation, VLMs are used primarily as high-level planners, but recent work
has also studied their lower-level reasoning ability, which refers to making
decisions about precise robot movements. However, the community currently lacks
a clear and common benchmark that can evaluate how well VLMs can aid low-level
reasoning in robotics. Consequently, we propose a novel benchmark, ManipBench,
to evaluate the low-level robot manipulation reasoning capabilities of VLMs
across various dimensions, including how well they understand object-object
interactions and deformable object manipulation. We extensively test 33
representative VLMs across 10 model families on our benchmark, including
variants to test different model sizes. Our evaluation shows that the
performance of VLMs significantly varies across tasks, and there is a strong
correlation between this performance and trends in our real-world manipulation
tasks. It also shows that there remains a significant gap between these models
and human-level understanding. See our website at:
https://manipbench.github.io.

</details>


### [214] [Multi-Robot Task Allocation for Homogeneous Tasks with Collision Avoidance via Spatial Clustering](https://arxiv.org/abs/2505.10073)
*Rathin Chandra Shit,Sharmila Subudhi*

Main category: cs.RO

TL;DR: 该论文提出了一种结合多机器人任务分配（MRTA）和避碰的新框架，通过空间聚类同时解决任务分配和碰撞问题。


<details>
  <summary>Details</summary>
Motivation: 工业环境中需要高效分配同质任务并避免机器人碰撞，现有方法无法兼顾这两点。

Method: 采用K-means聚类和2-Opt算法分割任务区域并规划机器人路径。

Result: 框架表现优异，时间减少93%，解决方案质量提升7%，且完全消除碰撞点。

Conclusion: 空间分区统一了任务分配和避碰问题，对实际应用至关重要。

Abstract: In this paper, a novel framework is presented that achieves a combined
solution based on Multi-Robot Task Allocation (MRTA) and collision avoidance
with respect to homogeneous measurement tasks taking place in industrial
environments. The spatial clustering we propose offers to simultaneously solve
the task allocation problem and deal with collision risks by cutting the
workspace into distinguishable operational zones for each robot. To divide task
sites and to schedule robot routes within corresponding clusters, we use
K-means clustering and the 2-Opt algorithm. The presented framework shows
satisfactory performance, where up to 93\% time reduction (1.24s against
17.62s) with a solution quality improvement of up to 7\% compared to the best
performing method is demonstrated. Our method also completely eliminates
collision points that persist in comparative methods in a most significant
sense. Theoretical analysis agrees with the claim that spatial partitioning
unifies the apparently disjoint tasks allocation and collision avoidance
problems under conditions of many identical tasks to be distributed over sparse
geographical areas. Ultimately, the findings in this work are of substantial
importance for real world applications where both computational efficiency and
operation free from collisions is of paramount importance.

</details>


### [215] [EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation](https://arxiv.org/abs/2505.10105)
*Zibin Dong,Fei Ni,Yifu Yuan,Yinchuan Li,Jianye Hao*

Main category: cs.RO

TL;DR: EmbodiedMAE是一个统一的多模态3D表示模型，用于机器人操作任务，显著提升了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在训练数据集与机器人操作任务之间存在显著的领域差距，且缺乏有效整合3D信息的模型架构。

Method: 通过增强DROID数据集并加入高质量深度图和点云，构建DROID-3D；开发了多模态掩码自编码器EmbodiedMAE，通过随机掩码和跨模态融合学习RGB、深度和点云的表示。

Result: 在70个仿真任务和20个真实世界机器人操作任务中，EmbodiedMAE在训练效率和最终性能上均超越现有视觉基础模型。

Conclusion: EmbodiedMAE是一个可靠的多模态3D视觉基础模型，特别适用于需要空间感知的精确桌面临场操作。

Abstract: We present EmbodiedMAE, a unified 3D multi-modal representation for robot
manipulation. Current approaches suffer from significant domain gaps between
training datasets and robot manipulation tasks, while also lacking model
architectures that can effectively incorporate 3D information. To overcome
these limitations, we enhance the DROID dataset with high-quality depth maps
and point clouds, constructing DROID-3D as a valuable supplement for 3D
embodied vision research. Then we develop EmbodiedMAE, a multi-modal masked
autoencoder that simultaneously learns representations across RGB, depth, and
point cloud modalities through stochastic masking and cross-modal fusion.
Trained on DROID-3D, EmbodiedMAE consistently outperforms state-of-the-art
vision foundation models (VFMs) in both training efficiency and final
performance across 70 simulation tasks and 20 real-world robot manipulation
tasks on two robot platforms. The model exhibits strong scaling behavior with
size and promotes effective policy learning from 3D inputs. Experimental
results establish EmbodiedMAE as a reliable unified 3D multi-modal VFM for
embodied AI systems, particularly in precise tabletop manipulation settings
where spatial perception is critical.

</details>


### [216] [Learning Rock Pushability on Rough Planetary Terrain](https://arxiv.org/abs/2505.09833)
*Tuba Girgin,Emre Girgin,Cagri Kilic*

Main category: cs.RO

TL;DR: 研究提出了一种移动导航新方法，通过机械臂操纵障碍物而非避障，优化多智能体长期路径效率。


<details>
  <summary>Details</summary>
Motivation: 传统避障方法在重复使用的路径上效率低下，尤其在月球或火星等需要自主基础设施的环境中。

Method: 结合外感和本体感知反馈评估障碍物的推动能力，利用机械臂主动调整障碍物位置。

Result: 通过推动障碍物而非避障，提升了多智能体在长期任务中的路径效率。

Conclusion: 该方法为无结构环境中的移动导航提供了更高效的解决方案，特别适用于太空探索等场景。

Abstract: In the context of mobile navigation in unstructured environments, the
predominant approach entails the avoidance of obstacles. The prevailing path
planning algorithms are contingent upon deviating from the intended path for an
indefinite duration and returning to the closest point on the route after the
obstacle is left behind spatially. However, avoiding an obstacle on a path that
will be used repeatedly by multiple agents can hinder long-term efficiency and
lead to a lasting reliance on an active path planning system. In this study, we
propose an alternative approach to mobile navigation in unstructured
environments by leveraging the manipulation capabilities of a robotic
manipulator mounted on top of a mobile robot. Our proposed framework integrates
exteroceptive and proprioceptive feedback to assess the push affordance of
obstacles, facilitating their repositioning rather than avoidance. While our
preliminary visual estimation takes into account the characteristics of both
the obstacle and the surface it relies on, the push affordance estimation
module exploits the force feedback obtained by interacting with the obstacle
via a robotic manipulator as the guidance signal. The objective of our
navigation approach is to enhance the efficiency of routes utilized by multiple
agents over extended periods by reducing the overall time spent by a fleet in
environments where autonomous infrastructure development is imperative, such as
lunar or Martian surfaces.

</details>


### [217] [Evaluating Robustness of Deep Reinforcement Learning for Autonomous Surface Vehicle Control in Field Tests](https://arxiv.org/abs/2505.10033)
*Luis F. W. Batista,Stéphanie Aravecchia,Seth Hutchinson,Cédric Pradalier*

Main category: cs.RO

TL;DR: 该论文分析了深度强化学习（DRL）在自主水面船只（ASV）中的鲁棒性，特别是在外部干扰情况下的表现。通过域随机化训练DRL智能体，并在仿真和现实实验中评估其对抗不对称拖曳和非对称负载等干扰的能力。结果表明DRL智能体在显著干扰下仍表现可靠。


<details>
  <summary>Details</summary>
Motivation: 尽管深度强化学习在自主水面船只领域取得了显著进展，但在实际环境中，特别是在外部干扰下的鲁棒性仍未得到充分研究。因此，本文旨在评估DRL智能体在不同扰动下的表现，并提供其实际部署的见解。

Method: 使用域随机化技术训练DRL智能体，并在仿真和现实实验中评估其对抗不对称拖曳和非对称负载等干扰的能力。同时与MPC基准方法进行比较。

Result: 结果表明，尽管存在显著干扰，DRL智能体仍表现出可靠的性能。同时，论文还开源了实现代码，并分享了训练策略和实际部署中的挑战。

Conclusion: 通过仿真和实际测试验证了DRL智能体在复杂扰动条件下的鲁棒性，为DRL在自主水面船只领域的应用提供了实践指导和开源资源。

Abstract: Despite significant advancements in Deep Reinforcement Learning (DRL) for
Autonomous Surface Vehicles (ASVs), their robustness in real-world conditions,
particularly under external disturbances, remains insufficiently explored. In
this paper, we evaluate the resilience of a DRL-based agent designed to capture
floating waste under various perturbations. We train the agent using domain
randomization and evaluate its performance in real-world field tests, assessing
its ability to handle unexpected disturbances such as asymmetric drag and an
off-center payload. We assess the agent's performance under these perturbations
in both simulation and real-world experiments, quantifying performance
degradation and benchmarking it against an MPC baseline. Results indicate that
the DRL agent performs reliably despite significant disturbances. Along with
the open-source release of our implementation, we provide insights into
effective training strategies, real-world challenges, and practical
considerations for deploying DRLbased ASV controllers.

</details>


### [218] [IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy Fine-Tuning](https://arxiv.org/abs/2505.10442)
*Dechen Gao,Hang Wang,Hanchu Zhou,Nejib Ammar,Shatadal Mishra,Ahmadreza Moradipari,Iman Soltani,Junshan Zhang*

Main category: cs.RO

TL;DR: 本文提出IN-RIL方法，通过在RL微调阶段周期性注入IL更新，结合IL的稳定性和RL的泛化能力，提升机器人策略学习的效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的IL预训练后RL微调的两步学习方法在微调阶段常面临不稳定和样本效率低的问题，需要一种能结合两者优势的方法。

Method: 提出IN-RIL方法，周期性交替进行IL和RL更新，并设计梯度分离机制以避免优化目标的冲突。

Result: 在14个机器人任务上的实验表明，IN-RIL显著提升样本效率和性能，例如在Robomimic Transport任务上成功率从12%提升至88%。

Conclusion: IN-RIL作为一种通用插件，能显著提升RL微调效果，适用于多种任务和奖励设置。

Abstract: Imitation learning (IL) and reinforcement learning (RL) each offer distinct
advantages for robotics policy learning: IL provides stable learning from
demonstrations, and RL promotes generalization through exploration. While
existing robot learning approaches using IL-based pre-training followed by
RL-based fine-tuning are promising, this two-step learning paradigm often
suffers from instability and poor sample efficiency during the RL fine-tuning
phase. In this work, we introduce IN-RIL, INterleaved Reinforcement learning
and Imitation Learning, for policy fine-tuning, which periodically injects IL
updates after multiple RL updates and hence can benefit from the stability of
IL and the guidance of expert data for more efficient exploration throughout
the entire fine-tuning process. Since IL and RL involve different optimization
objectives, we develop gradient separation mechanisms to prevent destructive
interference during \ABBR fine-tuning, by separating possibly conflicting
gradient updates in orthogonal subspaces. Furthermore, we conduct rigorous
analysis, and our findings shed light on why interleaving IL with RL stabilizes
learning and improves sample-efficiency. Extensive experiments on 14 robot
manipulation and locomotion tasks across 3 benchmarks, including
FurnitureBench, OpenAI Gym, and Robomimic, demonstrate that \ABBR can
significantly improve sample efficiency and mitigate performance collapse
during online finetuning in both long- and short-horizon tasks with either
sparse or dense rewards. IN-RIL, as a general plug-in compatible with various
state-of-the-art RL algorithms, can significantly improve RL fine-tuning, e.g.,
from 12\% to 88\% with 6.3x improvement in the success rate on Robomimic
Transport. Project page: https://github.com/ucd-dare/IN-RIL.

</details>


### [219] [Knowledge capture, adaptation and composition (KCAC): A framework for cross-task curriculum learning in robotic manipulation](https://arxiv.org/abs/2505.10522)
*Xinrui Wang,Yan Jin*

Main category: cs.RO

TL;DR: 提出了KCAC框架，通过跨任务课程学习整合知识转移，减少了40%的训练时间并提高了10%的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 增强强化学习在机器人操作中的样本效率和可解释性，以适应多样化工作场景。

Method: 设计了KCAC框架，重新定义奖励函数并引入自设计的子任务和跨任务课程。

Result: 在CausalWorld基准测试中，KCAC表现优于传统RL方法。

Conclusion: KCAC为课程设计提供了实用指导，提升了学习效率和任务成功率。

Abstract: Reinforcement learning (RL) has demonstrated remarkable potential in robotic
manipulation but faces challenges in sample inefficiency and lack of
interpretability, limiting its applicability in real world scenarios. Enabling
the agent to gain a deeper understanding and adapt more efficiently to diverse
working scenarios is crucial, and strategic knowledge utilization is a key
factor in this process. This paper proposes a Knowledge Capture, Adaptation,
and Composition (KCAC) framework to systematically integrate knowledge transfer
into RL through cross-task curriculum learning. KCAC is evaluated using a two
block stacking task in the CausalWorld benchmark, a complex robotic
manipulation environment. To our knowledge, existing RL approaches fail to
solve this task effectively, reflecting deficiencies in knowledge capture. In
this work, we redesign the benchmark reward function by removing rigid
constraints and strict ordering, allowing the agent to maximize total rewards
concurrently and enabling flexible task completion. Furthermore, we define two
self-designed sub-tasks and implement a structured cross-task curriculum to
facilitate efficient learning. As a result, our KCAC approach achieves a 40
percent reduction in training time while improving task success rates by 10
percent compared to traditional RL methods. Through extensive evaluation, we
identify key curriculum design parameters subtask selection, transition timing,
and learning rate that optimize learning efficiency and provide conceptual
guidance for curriculum based RL frameworks. This work offers valuable insights
into curriculum design in RL and robotic learning.

</details>


### [220] [Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning](https://arxiv.org/abs/2505.10547)
*Milan Ganai,Rohan Sinha,Christopher Agia,Daniel Morton,Marco Pavone*

Main category: cs.RO

TL;DR: 这篇论文提出了一种名为 FORTRESS 的框架，旨在通过多模态推理和动态感知规划，实时生成并执行安全的回退策略，以应对超出机器人训练数据的危险场景，提升了系统安全性和规划成功率。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖手动定义的回退策略，无法处理超出训练数据的故障情况，且大型视觉和语言模型的推理延迟问题限制了实时安全规划的能力。因此，论文提出了 FORTRESS 框架来解决这些问题，实现更通用的安全运动规划。

Method: FORTRESS 框架在正常运行时低频使用多模态推理识别目标和预测故障模式，当运行时监控触发回退响应时，快速合成回退目标计划并实时推断规避语义不安全区域。

Result: 在合成基准和真实机器人数据上，FORTRESS 在安全分类准确率上优于即时提示的慢速推理模型，并在仿真和四旋翼硬件上提升了系统安全性和规划成功率。

Conclusion: 通过结合开放世界的多模态推理和动态感知规划，FORTRESS 消除了手动定义回退策略的需求，显著提升了系统的安全性和灵活性。

Abstract: Foundation models can provide robust high-level reasoning on appropriate
safety interventions in hazardous scenarios beyond a robot's training data,
i.e. out-of-distribution (OOD) failures. However, due to the high inference
latency of Large Vision and Language Models, current methods rely on manually
defined intervention policies to enact fallbacks, thereby lacking the ability
to plan generalizable, semantically safe motions. To overcome these challenges
we present FORTRESS, a framework that generates and reasons about semantically
safe fallback strategies in real time to prevent OOD failures. At a low
frequency in nominal operations, FORTRESS uses multi-modal reasoners to
identify goals and anticipate failure modes. When a runtime monitor triggers a
fallback response, FORTRESS rapidly synthesizes plans to fallback goals while
inferring and avoiding semantically unsafe regions in real time. By bridging
open-world, multi-modal reasoning with dynamics-aware planning, we eliminate
the need for hard-coded fallbacks and human safety interventions. FORTRESS
outperforms on-the-fly prompting of slow reasoning models in safety
classification accuracy on synthetic benchmarks and real-world ANYmal robot
data, and further improves system safety and planning success in simulation and
on quadrotor hardware for urban navigation.

</details>


### [221] [AutoCam: Hierarchical Path Planning for an Autonomous Auxiliary Camera in Surgical Robotics](https://arxiv.org/abs/2505.10398)
*Alexandre Banks,Randy Moore,Sayem Nazmuz Zaman,Alaa Eldin Abdelaal,Septimiu E. Salcudean*

Main category: cs.RO

TL;DR: AutoCam是一种自动辅助摄像头放置方法，用于提升机器人辅助微创手术中的可视化效果，通过优先级控制和优化算法，实现了高效的特征跟踪和视角控制。


<details>
  <summary>Details</summary>
Motivation: 现有的辅助摄像头路径规划方法未能同时考虑摄像头方向、工作空间约束和机器人关节限制，影响了手术中的空间感知和视角控制。

Method: AutoCam采用基于优先级的、受工作空间约束的控制算法，结合启发式几何放置和非线性优化，确保摄像头稳健跟踪。系统在da Vinci研究套件上实现。

Result: 用户研究表明，系统保持了99.84%的显著特征可见性，姿态误差为4.36±2.11度和1.95±5.66毫米，计算高效，循环时间为6.8±12.8毫秒。初步实验显示新手在使用AutoCam视角时能取得与传统内窥镜相当的操作效果。

Conclusion: 研究为RAMIS中新型多摄像头可视化方法奠定了基础，证明辅助摄像头可通过da Vinci手术机器人自主控制，有效跟踪显著特征。

Abstract: Incorporating an autonomous auxiliary camera into robot-assisted minimally
invasive surgery (RAMIS) enhances spatial awareness and eliminates manual
viewpoint control. Existing path planning methods for auxiliary cameras track
two-dimensional surgical features but do not simultaneously account for camera
orientation, workspace constraints, and robot joint limits. This study presents
AutoCam: an automatic auxiliary camera placement method to improve
visualization in RAMIS. Implemented on the da Vinci Research Kit, the system
uses a priority-based, workspace-constrained control algorithm that combines
heuristic geometric placement with nonlinear optimization to ensure robust
camera tracking. A user study (N=6) demonstrated that the system maintained
99.84% visibility of a salient feature and achieved a pose error of 4.36 $\pm$
2.11 degrees and 1.95 $\pm$ 5.66 mm. The controller was computationally
efficient, with a loop time of 6.8 $\pm$ 12.8 ms. An additional pilot study
(N=6), where novices completed a Fundamentals of Laparoscopic Surgery training
task, suggests that users can teleoperate just as effectively from AutoCam's
viewpoint as from the endoscope's while still benefiting from AutoCam's
improved visual coverage of the scene. These results indicate that an auxiliary
camera can be autonomously controlled using the da Vinci patient-side
manipulators to track a salient feature, laying the groundwork for new
multi-camera visualization methods in RAMIS.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [222] [On the Well-Posedness of Green's Function Reconstruction via the Kirchhoff-Helmholtz Equation for One-Speed Neutron Diffusion](https://arxiv.org/abs/2505.09766)
*Roberto Ponciroli*

Main category: math.NA

TL;DR: 本文提出了一种利用堆外探测器实时测量数据重建核反应堆中子通量空间分布的方法，基于基尔霍夫-亥姆霍兹方程，并通过求解逆问题验证数据驱动格林函数近似的适定性。


<details>
  <summary>Details</summary>
Motivation: 核反应堆的中子通量分布重建是一个复杂且关键的问题，需要从边界数据推导出整个域内的标量场。现有方法在复杂几何结构下难以应用，因此需要一种数值方法来推导格林函数。

Method: 将基尔霍夫-亥姆霍兹方程作为数学框架，从单速中子扩散模型推导格林函数，并设计算法处理传感器数据以实现中子通量重建。

Result: 通过验证数据驱动格林函数的对称性和适定性，证明了重构方法的可靠性和预测的唯一性。

Conclusion: 该方法通过数值求解格林函数，成功实现了复杂几何核反应堆中子通量的准确重建，验证了其在工程应用中的潜力。

Abstract: This work presents a methodology for reconstructing the spatial distribution
of the neutron flux in a nuclear reactor, leveraging real-time measurements
obtained from ex-core detectors. The Kirchhoff-Helmholtz (K-H) equation
inherently defines the problem of estimating a scalar field within a domain
based on boundary data, making it a natural mathematical framework for this
task. The main challenge lies in deriving the Green's function specific to the
domain and the neutron diffusion process. While analytical solutions for
Green's functions exist for simplified geometries, their derivation of complex,
heterogeneous domains-such as a nuclear reactor-requires a numerical approach.
The objective of this work is to demonstrate the well-posedness of the
data-driven Green's function approximation by formulating and solving the K-H
equation as an inverse problem. After establishing the symmetry properties that
the Green's function must satisfy, the K-H equation is derived from the
one-speed neutron diffusion model. This is followed by a comprehensive
description of the procedure for interpreting sensor readings and implementing
the neutron flux reconstruction algorithm. Finally, the existence and
uniqueness of the Green's function inferred from the sampled data are
demonstrated, ensuring the reliability of the proposed method and its
predictions.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [223] [Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models](https://arxiv.org/abs/2505.09805)
*Aditya Nagori,Ayush Gautam,Matthew O. Wiens,Vuong Nguyen,Nathan Kenya Mugisha,Jerome Kabakyenga,Niranjan Kissoon,John Mark Ansermino,Rishikesan Kamaleswaran*

Main category: q-bio.QM

TL;DR: 该研究比较了基于大语言模型（LLM）和传统聚类方法在儿科脓毒症数据集上的表现，发现LLM方法能更有效捕捉上下文信息并识别具有不同特征的亚组。


<details>
  <summary>Details</summary>
Motivation: 个性化医疗和资源高效利用需要精准的患者亚组聚类，但传统方法在处理高维异构医疗数据时存在局限，缺乏上下文理解。

Method: 研究使用LLM生成嵌入（包括LLAMA 3.1 8B、DeepSeek-R1-Distill-Llama-8B和Stella-En-400M-V5模型），并应用K-means聚类，与传统方法（如K-Medoids结合UMAP和FAMD降维）进行对比。

Result: Stella-En-400M-V5的轮廓分数最高（0.86），LLAMA 3.1 8B在聚类目标明确时表现更优，能识别营养、临床和社会经济特征差异显著的亚组。

Conclusion: LLM方法在资源有限的环境中展现了更强的上下文表征能力，为精准表型分析和决策提供了新方向。

Abstract: Clustering patient subgroups is essential for personalized care and efficient
resource use. Traditional clustering methods struggle with high-dimensional,
heterogeneous healthcare data and lack contextual understanding. This study
evaluates Large Language Model (LLM) based clustering against classical methods
using a pediatric sepsis dataset from a low-income country (LIC), containing
2,686 records with 28 numerical and 119 categorical variables. Patient records
were serialized into text with and without a clustering objective. Embeddings
were generated using quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with
low-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was
applied to these embeddings. Classical comparisons included K-Medoids
clustering on UMAP and FAMD-reduced mixed data. Silhouette scores and
statistical tests evaluated cluster quality and distinctiveness.
Stella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B
with the clustering objective performed better with higher number of clusters,
identifying subgroups with distinct nutritional, clinical, and socioeconomic
profiles. LLM-based methods outperformed classical techniques by capturing
richer context and prioritizing key features. These results highlight potential
of LLMs for contextual phenotyping and informed decision-making in
resource-limited settings.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [224] [Large Wireless Localization Model (LWLM): A Foundation Model for Positioning in 6G Networks](https://arxiv.org/abs/2505.10134)
*Guangjin Pan,Kaixuan Huang,Hui Chen,Shunqing Zhang,Christian Häger,Henk Wymeersch*

Main category: eess.SP

TL;DR: 论文提出了一种基于自监督学习的无线定位基础模型（LWLM），通过联合优化多个目标实现高性能定位，显著优于现有方法，并展示出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据驱动方法在无线定位中需要大量标注数据且难以跨场景泛化的问题，旨在为5G/6G应用提供高精度、稳健的定位解决方案。

Method: 基于信息瓶颈理论设计自监督学习框架，联合优化空间-频率掩码信道建模（SF-MCM）、域变换不变性（DTI）和位置不变对比学习（PICL）三个目标。

Result: LWLM在所有定位任务中均超越基于模型和监督学习的基线方法，尤其在无预训练的Transformer模型上提升26.0%-87.5%，并在少标注和未知基站配置下表现出强泛化性。

Conclusion: LWLM作为无线定位的基础模型具有显著潜力，能够高效适应多样化下游任务和部署场景。

Abstract: Accurate and robust localization is a critical enabler for emerging 5G and 6G
applications, including autonomous driving, extended reality (XR), and smart
manufacturing. While data-driven approaches have shown promise, most existing
models require large amounts of labeled data and struggle to generalize across
deployment scenarios and wireless configurations. To address these limitations,
we propose a foundation-model-based solution tailored for wireless
localization. We first analyze how different self-supervised learning (SSL)
tasks acquire general-purpose and task-specific semantic features based on
information bottleneck (IB) theory. Building on this foundation, we design a
pretraining methodology for the proposed Large Wireless Localization Model
(LWLM). Specifically, we propose an SSL framework that jointly optimizes three
complementary objectives: (i) spatial-frequency masked channel modeling
(SF-MCM), (ii) domain-transformation invariance (DTI), and (iii)
position-invariant contrastive learning (PICL). These objectives jointly
capture the underlying semantics of wireless channel from multiple
perspectives. We further design lightweight decoders for key downstream tasks,
including time-of-arrival (ToA) estimation, angle-of-arrival (AoA) estimation,
single base station (BS) localization, and multiple BS localization.
Comprehensive experimental results confirm that LWLM consistently surpasses
both model-based and supervised learning baselines across all localization
tasks. In particular, LWLM achieves 26.0%--87.5% improvement over transformer
models without pretraining, and exhibits strong generalization under
label-limited fine-tuning and unseen BS configurations, confirming its
potential as a foundation model for wireless localization.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [225] [Predictive Models for Chronic Heart Failure](https://arxiv.org/abs/2505.09619)
*Pietro Cassieri,Aiman Faiz,Anna Maria De Roberto,Claudio Pascarelli,Gianvito Mitrano,Gianluca Fimiani,Marina Garofano,Christiancarmine Esposito,Genoveffa Tortora,Alessia Bramanti,Giuseppe Scanniello*

Main category: stat.OT

TL;DR: 该论文提出了一种基于机器学习的集成学习模型，用于预测慢性心力衰竭（HF）风险患者，模型整合了临床和超声心动图特征，表现出高敏感性和中等准确率，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 慢性心力衰竭管理面临重大挑战，需要持续监测、早期发现恶化迹象和个性化治疗策略。研究旨在通过机器学习模型识别高风险患者，支持早期干预和个性化管理。

Method: 采用改进的堆叠集成学习技术，结合两个专门模型（分别利用临床和超声心动图特征）的预测结果，再通过元模型整合。模型在真实数据集上评估，并与三个基线模型对比。

Result: 模型表现出高敏感性（95%）和中等的准确率（84%），显著优于未分组特征的基线模型，适合用于高风险患者的识别和远程监测项目。

Conclusion: 机器学习风险分层模型可作为有价值的决策支持工具，帮助医疗专业人员实现早期干预和个性化管理，未来有望在更广泛医疗场景中应用。

Abstract: The management of chronic Heart Failure (HF) presents significant challenges
in modern healthcare, requiring continuous monitoring, early detection of
exacerbations, and personalized treatment strategies. In this paper, we present
a predictive model founded on Machine Learning (ML) techniques to identify
patients at HF risk. This model is an ensemble learning approach, a modified
stacking technique, that uses two specialized models leveraging clinical and
echocardiographic features and then a meta-model to combine the predictions of
these two models. We initially assess the model on a real dataset and the
obtained results suggest that it performs well in the stratification of
patients at HR risk. Specifically, we obtained high sensitivity (95\%),
ensuring that nearly all high-risk patients are identified. As for accuracy, we
obtained 84\%, which can be considered moderate in some ML contexts. However,
it is acceptable given our priority of identifying patients at risk of HF
because they will be asked to participate in the telemonitoring program of the
PrediHealth research project on which some of the authors of this paper are
working. The initial findings also suggest that ML-based risk stratification
models can serve as valuable decision-support tools not only in the PrediHealth
project but also for healthcare professionals, aiding in early intervention and
personalized patient management. To have a better understanding of the value
and of potentiality of our predictive model, we also contrasted its results
with those obtained by using three baseline models. The preliminary results
indicate that our predictive model outperforms these baselines that flatly
consider features, \ie not grouping them in clinical and echocardiographic
features.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [226] [AI Greenferencing: Routing AI Inferencing to Green Modular Data Centers with Heron](https://arxiv.org/abs/2505.09989)
*Tella Rajashekhar Reddy,Palak,Rohan Gandhi,Anjaly Parayil,Chaojie Zhang,Mike Shepperd,Liangcheng Yu,Jayashree Mohan,Srinivasan Iyengar,Shivkumar Kalyanaraman,Debopam Bhattacherjee*

Main category: cs.DC

TL;DR: 论文提出将AI计算任务部署在风电场附近的模块化计算集群，利用Heron软件路由器优化跨风电场的电力互补，提升AI计算的效率和环保性。


<details>
  <summary>Details</summary>
Motivation: AI计算对电力需求巨大，而风电资源丰富却难以并网。将AI计算与风电结合，既可降低成本，又能利用绿色能源。

Method: 提出了一种部署策略，将AI计算集群与风电场共置，并开发了Heron软件路由器，通过跨风电场调度AI推理任务以应对电力波动。

Result: 实验表明，Heron能将AI计算的总吞吐量提升高达80%，显著优于现有技术。

Conclusion: 结合风电和AI计算的策略不仅经济可行，还能提高绿色能源利用率，为可持续计算提供新思路。

Abstract: AI power demand is growing unprecedentedly thanks to the high power density
of AI compute and the emerging inferencing workload. On the supply side,
abundant wind power is waiting for grid access in interconnection queues. In
this light, this paper argues bringing AI workload to modular compute clusters
co-located in wind farms. Our deployment right-sizing strategy makes it
economically viable to deploy more than 6 million high-end GPUs today that
could consume cheap, green power at its source. We built Heron, a cross-site
software router, that could efficiently leverage the complementarity of power
generation across wind farms by routing AI inferencing workload around power
drops. Using 1-week ofcoding and conversation production traces from Azure and
(real) variable wind power traces, we show how Heron improves aggregate goodput
of AI compute by up to 80% compared to the state-of-the-art.

</details>


### [227] [KAITIAN: A Unified Communication Framework for Enabling Efficient Collaboration Across Heterogeneous Accelerators in Embodied AI Systems](https://arxiv.org/abs/2505.10183)
*Jieke Lin,Wanyu Wang,Longxiang Yin,Yinhe Han*

Main category: cs.DC

TL;DR: KAITIAN是一个新型分布式通信框架，旨在解决异构加速器间的互操作性问题，通过统一抽象层和负载自适应调度机制提升分布式AI任务的资源利用率和性能。


<details>
  <summary>Details</summary>
Motivation: 由于不同厂商专有通信库导致的互操作性障碍，异构加速器在分布式AI任务中资源利用率和性能受限。

Method: KAITIAN结合了厂商优化通信库和通用协议，并引入负载自适应调度机制。

Result: 实验证明KAITIAN可加速训练时间达42%，通信开销仅2.8-4.3%，且保持模型准确性。

Conclusion: KAITIAN为复杂嵌入式AI应用中的异构计算提供了更灵活和强大的解决方案。

Abstract: Embodied Artificial Intelligence (AI) systems, such as autonomous robots and
intelligent vehicles, are increasingly reliant on diverse heterogeneous
accelerators (e.g., GPGPUs, NPUs, FPGAs) to meet stringent real-time processing
and energy-efficiency demands. However, the proliferation of vendor-specific
proprietary communication libraries creates significant interoperability
barriers, hindering seamless collaboration between different accelerator types
and leading to suboptimal resource utilization and performance bottlenecks in
distributed AI workloads. This paper introduces KAITIAN, a novel distributed
communication framework designed to bridge this gap. KAITIAN provides a unified
abstraction layer that intelligently integrates vendor-optimized communication
libraries for intra-group efficiency with general-purpose communication
protocols for inter-group interoperability. Crucially, it incorporates a
load-adaptive scheduling mechanism that dynamically balances computational
tasks across heterogeneous devices based on their real-time performance
characteristics. Implemented as an extension to PyTorch and rigorously
evaluated on a testbed featuring NVIDIA GPUs and Cambricon MLUs, KAITIAN
demonstrates significant improvements in resource utilization and scalability
for distributed training tasks. Experimental results show that KAITIAN can
accelerate training time by up to 42% compared to baseline homogeneous systems,
while incurring minimal communication overhead (2.8--4.3%) and maintaining
model accuracy. KAITIAN paves the way for more flexible and powerful
heterogeneous computing in complex embodied AI applications.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [228] [Ontology-Based Structuring and Analysis of North Macedonian Public Procurement Contracts](https://arxiv.org/abs/2505.09798)
*Bojan Ristov,Stefan Eftimov,Milena Trajanoska,Dimitar Trajanov*

Main category: cs.DB

TL;DR: 将传统表格格式的政府采购数据转化为语义知识图谱，提升数据透明度和分析能力。


<details>
  <summary>Details</summary>
Motivation: 传统的政府采购数据通常以表格形式存储，限制了分析潜力和透明度，阻碍了数据深度利用。

Method: 采用本体建模和自动化数据转换技术，构建语义知识图谱，整合RDF和SPARQL查询，并引入机器学习预测模型。

Result: 系统提升了采购数据的可访问性和解释性，支持复杂语义查询和高级分析，同时提供采购趋势和风险评估预测。

Conclusion: 该研究通过技术手段改进了政府采购数据的透明度和智能化分析能力，支持基于证据的决策制定。

Abstract: Public procurement plays a critical role in government operations, ensuring
the efficient allocation of resources and fostering economic growth. However,
traditional procurement data is often stored in rigid, tabular formats,
limiting its analytical potential and hindering transparency. This research
presents a methodological framework for transforming structured procurement
data into a semantic knowledge graph, leveraging ontological modeling and
automated data transformation techniques. By integrating RDF and SPARQL-based
querying, the system enhances the accessibility and interpretability of
procurement records, enabling complex semantic queries and advanced analytics.
Furthermore, by incorporating machine learning-driven predictive modeling, the
system extends beyond conventional data analysis, offering insights into
procurement trends and risk assessment. This work contributes to the broader
field of public procurement intelligence by improving data transparency,
supporting evidence-based decision-making, and enabling in-depth analysis of
procurement activities in North Macedonia.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [229] [Trustless Autonomy: Understanding Motivations, Benefits and Governance Dilemma in Self-Sovereign Decentralized AI Agents](https://arxiv.org/abs/2505.09757)
*Botao Amber Hu,Yuhan Liu,Helena Rong*

Main category: cs.HC

TL;DR: 该研究探讨了结合LLM与去中心化技术的自治理AI代理（DeAgents）的动机、优势及治理困境，通过访谈利益相关者填补实证研究空白。


<details>
  <summary>Details</summary>
Motivation: 解决中心化AI系统的信任问题，同时应对LLM不可靠性带来的自治与信任之间的张力。

Method: 通过访谈DeAgents领域的专家、创始人和开发者，分析其动机、收益和治理难题。

Result: 研究结果为未来DeAgents系统设计和协议开发提供指导，并促进关于未来AI代理网络中治理的讨论。

Conclusion: DeAgents结合去中心化技术实现自治，但仍需解决LLM可靠性问题，研究为相关设计提供实践参考。

Abstract: The recent trend of self-sovereign Decentralized AI Agents (DeAgents)
combines Large Language Model (LLM)-based AI agents with decentralization
technologies such as blockchain smart contracts and trusted execution
environments (TEEs). These tamper-resistant trustless substrates allow agents
to achieve self-sovereignty through ownership of cryptowallet private keys and
control of digital assets and social media accounts. DeAgent eliminates
centralized control and reduces human intervention, addressing key trust
concerns inherent in centralized AI systems. However, given ongoing challenges
in LLM reliability such as hallucinations, this creates paradoxical tension
between trustlessness and unreliable autonomy. This study addresses this
empirical research gap through interviews with DeAgents stakeholders-experts,
founders, and developers-to examine their motivations, benefits, and governance
dilemmas. The findings will guide future DeAgents system and protocol design
and inform discussions about governance in sociotechnical AI systems in the
future agentic web.

</details>


### [230] [AI LEGO: Scaffolding Cross-Functional Collaboration in Industrial Responsible AI Practices during Early Design Stages](https://arxiv.org/abs/2505.10300)
*Muzhe Wu,Yanzhi Zhao,Shuyi Han,Michael Xieyang Liu,Hong Shen*

Main category: cs.HC

TL;DR: AI LEGO是一个基于网络的工具，旨在帮助跨职能团队在AI早期设计阶段更好地传递技术设计原理并识别潜在危害，从而提升责任AI（RAI）的实践效果。


<details>
  <summary>Details</summary>
Motivation: 当前跨职能团队中，高层的技术设计原理难以向非技术角色传递以进行伦理评估和危害识别，导致责任AI实践受阻。

Method: 通过文献综述和与8名从业者的共同设计研究，开发了AI LEGO原型，利用交互式模块和LLM驱动的人物模拟，支持技术与非技术角色协作识别设计危害。

Result: 在与18名从业者的研究中，AI LEGO相比基线工作表显著提高了危害识别的数量和可能性，其模块化结构和人物提示使危害识别更易行。

Conclusion: AI LEGO通过提供共享的视觉结构和系统化危害评估脚手架，促进了早期设计中更清晰和协作的责任AI实践。

Abstract: Responsible AI (RAI) efforts increasingly emphasize the importance of
addressing potential harms early in the AI development lifecycle through
social-technical lenses. However, in cross-functional industry teams, this work
is often stalled by a persistent knowledge handoff challenge: the difficulty of
transferring high-level, early-stage technical design rationales from technical
experts to non-technical or user-facing roles for ethical evaluation and harm
identification. Through literature review and a co-design study with 8
practitioners, we unpack how this challenge manifests -- technical design
choices are rarely handed off in ways that support meaningful engagement by
non-technical roles; collaborative workflows lack shared, visual structures to
support mutual understanding; and non-technical practitioners are left without
scaffolds for systematic harm evaluation. Existing tools like JIRA or Google
Docs, while useful for product tracking, are ill-suited for supporting joint
harm identification across roles, often requiring significant extra effort to
align understanding. To address this, we developed AI LEGO, a web-based
prototype that supports cross-functional AI practitioners in effectively
facilitating knowledge handoff and identifying harmful design choices in the
early design stages. Technical roles use interactive blocks to draft
development plans, while non-technical roles engage with those blocks through
stage-specific checklists and LLM-driven persona simulations to surface
potential harms. In a study with 18 cross-functional practitioners, AI LEGO
increased the volume and likelihood of harms identified compared to baseline
worksheets. Participants found that its modular structure and persona prompts
made harm identification more accessible, fostering clearer and more
collaborative RAI practices in early design.

</details>


### [231] [Visual Feedback of Pattern Separability Improves Myoelectric Decoding Performance of Upper Limb Prostheses](https://arxiv.org/abs/2505.09819)
*Ruichen Yang,György M. Lévay,Christopher L. Hunt,Dániel Czeiner,Megan C. Hodgson,Damini Agarwal,Rahul R. Kaliki,Nitish V. Thakor*

Main category: cs.HC

TL;DR: 论文提出了一种名为Reviewer的3D视觉界面，通过将肌电信号直接投影到解码器的分类空间中，提供直观的实时反馈，从而改进肌电假肢的控制效果。


<details>
  <summary>Details</summary>
Motivation: 由于肌电假肢运动复杂性增加，用户难以生成足够独特的肌电模式以实现可靠分类。现有训练方法依赖于启发式的试错调整，缺乏结构化反馈。

Method: 研究通过10次训练课程，比较了使用Reviewer与传统虚拟手臂可视化在12名健康参与者中的模式识别（PR）性能。采用Fitts定律任务评估性能。

Result: 使用Reviewer的参与者完成率更高，减少了过度调整，并提高了路径效率和吞吐量。

Conclusion: 3D视觉反馈通过结构化训练显著提升了新手操作者的PR控制能力，减少了试错调整的依赖。

Abstract: State-of-the-art upper limb myoelectric prostheses often use pattern
recognition (PR) control systems that translate electromyography (EMG) signals
into desired movements. As prosthesis movement complexity increases, users
often struggle to produce sufficiently distinct EMG patterns for reliable
classification. Existing training typically involves heuristic, trial-and-error
user adjustments to static decoder boundaries. Goal: We introduce the Reviewer,
a 3D visual interface projecting EMG signals directly into the decoder's
classification space, providing intuitive, real-time insight into PR algorithm
behavior. This structured feedback reduces cognitive load and fosters mutual,
data-driven adaptation between user-generated EMG patterns and decoder
boundaries. Methods: A 10-session study with 12 able-bodied participants
compared PR performance after motor-based training and updating using the
Reviewer versus conventional virtual arm visualization. Performance was
assessed using a Fitts law task that involved the aperture of the cursor and
the control of orientation. Results: Participants trained with the Reviewer
achieved higher completion rates, reduced overshoot, and improved path
efficiency and throughput compared to the standard visualization group.
Significance: The Reviewer introduces decoder-informed motor training,
facilitating immediate and consistent PR-based myoelectric control
improvements. By iteratively refining control through real-time feedback, this
approach reduces reliance on trial-and-error recalibration, enabling a more
adaptive, self-correcting training framework. Conclusion: The 3D visual
feedback significantly improves PR control in novice operators through
structured training, enabling feedback-driven adaptation and reducing reliance
on extensive heuristic adjustments.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [232] [Neurophysiologically Realistic Environment for Comparing Adaptive Deep Brain Stimulation Algorithms in Parkinson Disease](https://arxiv.org/abs/2505.09624)
*Ekaterina Kuzmina,Dmitrii Kriukov,Mikhail Lebedev,Dmitry V. Dylov*

Main category: q-bio.NC

TL;DR: 论文介绍了首个基于神经生理学真实性的基准测试，用于比较帕金森病（PD）的合成模型和控制算法，涵盖多种生理特征并支持深度强化学习算法的训练与评估。


<details>
  <summary>Details</summary>
Motivation: 当前的自适应深脑刺激（aDBS）治疗PD受限于侵入式设备的数据收集，推动了合成模型的发展。本研究的动机是填补缺乏真实性基准的空白，为模型比较和优化提供依据。

Method: 提出了一种神经生理学真实的基准测试框架，不仅包括基底节电路动力学和病理性振荡，还整合了15种生理特征（如信号不稳定性和个体差异），并通过beta波段活动和反馈建模。同时，构建了支持深度强化学习算法的结构化环境。

Result: 建立了一个全面且真实的基准测试环境，首次实现了对多种生理特征的建模，并为优化aDBS控制策略提供了新工具。

Conclusion: 该框架为智能神经刺激领域的研究提供了新方向，并鼓励机器学习社区参与优化aDBS控制策略。

Abstract: Adaptive deep brain stimulation (aDBS) has emerged as a promising treatment
for Parkinson disease (PD). In aDBS, a surgically placed electrode sends
dynamically altered stimuli to the brain based on neurophysiological feedback:
an invasive gadget that limits the amount of data one could collect for
optimizing the control offline. As a consequence, a plethora of synthetic
models of PD and those of the control algorithms have been proposed. Herein, we
introduce the first neurophysiologically realistic benchmark for comparing said
models. Specifically, our methodology covers not only conventional basal
ganglia circuit dynamics and pathological oscillations, but also captures 15
previously dismissed physiological attributes, such as signal instabilities and
noise, neural drift, electrode conductance changes and individual variability -
all modeled as spatially distributed and temporally registered features via
beta-band activity in the brain and a feedback. Furthermore, we purposely built
our framework as a structured environment for training and evaluating deep
reinforcement learning (RL) algorithms, opening new possibilities for
optimizing aDBS control strategies and inviting the machine learning community
to contribute to the emerging field of intelligent neurostimulation interfaces.

</details>


### [233] [Temporal Interception and Present Reconstruction: A Cognitive-Signal Model for Human and AI Decision Making](https://arxiv.org/abs/2505.09646)
*Carmel Mary Esther A*

Main category: q-bio.NC

TL;DR: 论文提出了一种新理论模型，探讨人类心智与人工智能如何通过减少感知延迟实现实时意识。结合宇宙信号延迟、神经反应时间和古老认知状态，研究从被动感知转向主动预测未来的方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解人类与AI如何通过减少感知延迟，实现从被动反应到主动预知未来的认知转变，探索宇宙信号与神经反应的交互作用。

Method: 方法包括构建物理和认知模型，将“当下”视为宇宙信号与人类延迟的交叉干扰区，并设计实验（如神经观测和神经感知扩展）验证模型。

Result: 提出了数学框架，指导AI系统向时间高效、伦理合理且具内部意识的决策过程演进。

Conclusion: 结论表明，该模型为理解实时意识提供了新视角，并可能推动AI在时间和伦理维度上的优化发展。

Abstract: This paper proposes a novel theoretical model to explain how the human mind
and artificial intelligence can approach real-time awareness by reducing
perceptual delays. By investigating cosmic signal delay, neurological reaction
times, and the ancient cognitive state of stillness, we explore how one may
shift from reactive perception to a conscious interface with the near future.
This paper introduces both a physical and cognitive model for perceiving the
present not as a linear timestamp, but as an interference zone where
early-arriving cosmic signals and reactive human delays intersect. We propose
experimental approaches to test these ideas using human neural observation and
neuro-receptive extensions. Finally, we propose a mathematical framework to
guide the evolution of AI systems toward temporally efficient, ethically sound,
and internally conscious decision-making processes

</details>


### [234] [A Computational Approach to Epilepsy Treatment: An AI-optimized Global Natural Product Prescription System](https://arxiv.org/abs/2505.09643)
*Zhixuan Wang*

Main category: q-bio.NC

TL;DR: 该研究开发了一种AI驱动的计算方法，通过分析全球天然产物和随机对照试验（RCTs）优化癫痫的中草药治疗。系统整合了机器学习和贝叶斯优化，筛选出17种高效草药，并在临床试验中显示出显著优于传统方案的疗效。


<details>
  <summary>Details</summary>
Motivation: 由于传统抗癫痫药物疗效有限且副作用明显，许多患者转向替代医学。本研究旨在通过AI技术优化中草药治疗，提供个性化且有效的替代方案。

Method: 研究结合机器学习算法（如LASSO回归和SHAP值分析）和贝叶斯优化，从1872种天然化合物中筛选高效草药，并整合48项RCTs的临床数据进行验证。最后通过随机双盲试验（n=120）验证疗效。

Result: 筛选出17种高效草药（如天麻、睡茄），癫痫发作频率降低显著（p<0.01，效应量d=0.89）。AI优化处方在验证试验中显示出28.5%的疗效提升（95% CI: 18.7-37.3%，p=0.003）。

Conclusion: AI驱动的中草药优化系统能有效提升癫痫治疗效果，为个性化替代医学提供了可靠工具，未来可扩展至其他神经系统疾病。

Abstract: Epilepsy is a prevalent neurological disease with millions of patients
worldwide. Many patients have turned to alternative medicine due to the limited
efficacy and side effects of conventional antiepileptic drugs. In this study,
we developed a computational approach to optimize herbal epilepsy treatment
through AI-driven analysis of global natural products and statistically
validated randomized controlled trials (RCTs). Our intelligent prescription
system combines machine learning (ML) algorithms for herb-efficacy
characterization, Bayesian optimization for personalized dosing, and
meta-analysis of RCTs for evidence-based recommendations. The system analyzed
1,872 natural compounds from traditional Chinese medicine (TCM), Ayurveda, and
ethnopharmacological databases, integrating their bioactive properties with
clinical outcomes from 48 RCTs covering 48 epilepsy conditions (n=5,216). Using
LASSO regression and SHAP value analysis, we identified 17 high-efficacy herbs
(e.g., Gastrodia elata [using \'e for accented characters], Withania
somnifera), showing significant seizure reduction (p$<$0.01, Cohen's d=0.89)
with statistical significance confirmed by multiple testing (p$<$0.001). A
randomized double-blind validation trial (n=120) demonstrated 28.5\% greater
seizure frequency reduction with AI-optimized herbal prescriptions compared to
conventional protocols (95\% CI: 18.7-37.3\%, p=0.003).

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [235] [Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling](https://arxiv.org/abs/2505.09665)
*Sulong Zhou,Qunying Huang,Shaoheng Zhou,Yun Hang,Xinyue Ye,Aodong Mei,Kathryn Phung,Yuning Ye,Uma Govindswamy,Zehan Li*

Main category: cs.SI

TL;DR: 该研究通过分析2025年洛杉矶火灾期间的Reddit讨论，利用主题建模和大型语言模型（LLMs）提取公共话语中的潜在主题，重点关注情境感知（SA）和危机叙事（CN），以揭示公众情绪和健康关注，为灾害应对和公共卫生沟通提供依据。


<details>
  <summary>Details</summary>
Motivation: 近年来野火频发且严重，了解受影响人群的感知和应对方式对灾害响应至关重要。社交媒体为捕捉公众情绪和超本地信息提供了渠道。

Method: 收集385条帖子和114,879条评论，采用主题建模方法，结合LLMs和人工细化，构建分层框架（SA和CN）分类潜在主题。

Result: SA主题与火灾进展紧密相关，峰值出现在火灾蔓延的前2-5天；CN主题中，心理健康和悲伤信号分别占40%和60%，夜间讨论最多。

Conclusion: 研究提供了首个2025年洛杉矶火灾的社交媒体标注数据集，并提出了可扩展的多层分析框架，为灾害响应和公共卫生沟通提供了数据支持。

Abstract: Wildfires have become increasingly frequent, irregular, and severe in recent
years. Understanding how affected populations perceive and respond during
wildfire crises is critical for timely and empathetic disaster response. Social
media platforms offer a crowd-sourced channel to capture evolving public
discourse, providing hyperlocal information and insight into public sentiment.
This study analyzes Reddit discourse during the 2025 Los Angeles wildfires,
spanning from the onset of the disaster to full containment. We collect 385
posts and 114,879 comments related to the Palisades and Eaton fires. We adopt
topic modeling methods to identify the latent topics, enhanced by large
language models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we
develop a hierarchical framework to categorize latent topics, consisting of two
main categories, Situational Awareness (SA) and Crisis Narratives (CN). The
volume of SA category closely aligns with real-world fire progressions, peaking
within the first 2-5 days as the fires reach the maximum extent. The most
frequent co-occurring category set of public health and safety, loss and
damage, and emergency resources expands on a wide range of health-related
latent topics, including environmental health, occupational health, and one
health. Grief signals and mental health risks consistently accounted for 60
percentage and 40 percentage of CN instances, respectively, with the highest
total volume occurring at night. This study contributes the first annotated
social media dataset on the 2025 LA fires, and introduces a scalable
multi-layer framework that leverages topic modeling for crisis discourse
analysis. By identifying persistent public health concerns, our results can
inform more empathetic and adaptive strategies for disaster response, public
health communication, and future research in comparable climate-related
disaster events.

</details>


### [236] [Advancing Community Detection with Graph Convolutional Neural Networks: Bridging Topological and Attributive Cohesion](https://arxiv.org/abs/2505.10197)
*Anjali de Silva,Gang Chen,Hui Ma,Seyed Mohammad Nekooei,Xingquan Zuo*

Main category: cs.SI

TL;DR: TAS-Com是一种基于拓扑和属性相似性的社区检测方法，通过引入新颖的损失函数并结合Leiden算法，优化社区结构的模块度和人类标注的平衡，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GCN方法在最大化模块度时容易陷入局部最优，而直接使用人工标注社区会因忽视拓扑连接性导致社区质量下降。TAS-Com旨在解决这两个问题。

Method: 提出TAS-Com方法，结合Leiden算法设计新损失函数，优化社区模块度并修正人工标注社区的连通性。

Result: 在多个基准网络上实验表明，TAS-Com在模块度与人工标注一致性上显著优于现有算法。

Conclusion: TAS-Com通过联合优化拓扑与属性相似性，实现了社区检测的高效性与准确性，为实际应用提供了更优解。

Abstract: Community detection, a vital technology for real-world applications, uncovers
cohesive node groups (communities) by leveraging both topological and attribute
similarities in social networks. However, existing Graph Convolutional Networks
(GCNs) trained to maximize modularity often converge to suboptimal solutions.
Additionally, directly using human-labeled communities for training can
undermine topological cohesiveness by grouping disconnected nodes based solely
on node attributes. We address these issues by proposing a novel Topological
and Attributive Similarity-based Community detection (TAS-Com) method. TAS-Com
introduces a novel loss function that exploits the highly effective and
scalable Leiden algorithm to detect community structures with global optimal
modularity. Leiden is further utilized to refine human-labeled communities to
ensure connectivity within each community, enabling TAS-Com to detect community
structures with desirable trade-offs between modularity and compliance with
human labels. Experimental results on multiple benchmark networks confirm that
TAS-Com can significantly outperform several state-of-the-art algorithms.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [237] [Risk-Aware Safe Reinforcement Learning for Control of Stochastic Linear Systems](https://arxiv.org/abs/2505.09734)
*Babak Esmaeili,Nariman Niknejad,Hamidreza Modares*

Main category: eess.SY

TL;DR: 该论文提出了一种风险感知的安全强化学习（RL）控制设计，通过结合RL和安全控制器，避免了短视干预，并能在有限数据下提供高效且计算可行的安全保证。


<details>
  <summary>Details</summary>
Motivation: 传统的安全RL方法依赖于高精度系统模型或短视的安全认证，可能导致不希望的平衡点或高计算成本。本文旨在通过风险感知方法解决这些问题。

Method: 通过数据驱动的方法学习分段仿射安全控制器，并设计标量决策变量以最小化安全违规概率，同时开发了RL与安全控制器的数据驱动插值技术。

Result: 该方法在减少数据需求和降低安全违规方差方面表现优异，并通过仿真验证了理论结果的正确性。

Conclusion: 结合风险感知的安全RL控制设计能够高效地实现高置信度安全，避免传统方法的局限性，适用于噪声环境下的实际应用。

Abstract: This paper presents a risk-aware safe reinforcement learning (RL) control
design for stochastic discrete-time linear systems. Rather than using a safety
certifier to myopically intervene with the RL controller, a risk-informed safe
controller is also learned besides the RL controller, and the RL and safe
controllers are combined together. Several advantages come along with this
approach: 1) High-confidence safety can be certified without relying on a
high-fidelity system model and using limited data available, 2) Myopic
interventions and convergence to an undesired equilibrium can be avoided by
deciding on the contribution of two stabilizing controllers, and 3) highly
efficient and computationally tractable solutions can be provided by optimizing
over a scalar decision variable and linear programming polyhedral sets. To
learn safe controllers with a large invariant set, piecewise affine controllers
are learned instead of linear controllers. To this end, the closed-loop system
is first represented using collected data, a decision variable, and noise. The
effect of the decision variable on the variance of the safe violation of the
closed-loop system is formalized. The decision variable is then designed such
that the probability of safety violation for the learned closed-loop system is
minimized. It is shown that this control-oriented approach reduces the data
requirements and can also reduce the variance of safety violations. Finally, to
integrate the safe and RL controllers, a new data-driven interpolation
technique is introduced. This method aims to maintain the RL agent's optimal
implementation while ensuring its safety within environments characterized by
noise. The study concludes with a simulation example that serves to validate
the theoretical results.

</details>


### [238] [A Hybrid Strategy for Aggregated Probabilistic Forecasting and Energy Trading in HEFTCom2024](https://arxiv.org/abs/2505.10367)
*Chuanqing Pu,Feilong Fan,Nengling Tai,Songyuan Liu,Jinming Yu*

Main category: eess.SY

TL;DR: 本文介绍了团队GEB在IEEE HEFTCom2024竞赛中的解决方案，结合多种方法实现风力-太阳能混合系统的概率预测和电力市场交易，表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决未来能源系统中概率能量预测和不确定性决策的挑战，提升混合能源系统的预测准确性和交易收益。

Method: 采用堆叠式方法结合多种数值天气预报、在线太阳能后处理模型、概率聚合方法和随机交易策略。

Result: 竞赛中取得交易第3、预测第4和学生团队第1的成绩，验证了方法的有效性。

Conclusion: 提出的方法显著提升了混合能源系统的预测和交易表现，代码开源便于复现和进一步研究。

Abstract: Obtaining accurate probabilistic energy forecasts and making effective
decisions amid diverse uncertainties are routine challenges in future energy
systems. This paper presents the solution of team GEB, which ranked 3rd in
trading, 4th in forecasting, and 1st among student teams in the IEEE Hybrid
Energy Forecasting and Trading Competition 2024 (HEFTCom2024). The solution
provides accurate probabilistic forecasts for a wind-solar hybrid system, and
achieves substantial trading revenue in the day-ahead electricity market. Key
components include: (1) a stacking-based approach combining sister forecasts
from various Numerical Weather Predictions (NWPs) to provide wind power
forecasts, (2) an online solar post-processing model to address the
distribution shift in the online test set caused by increased solar capacity,
(3) a probabilistic aggregation method for accurate quantile forecasts of
hybrid generation, and (4) a stochastic trading strategy to maximize expected
trading revenue considering uncertainties in electricity prices. This paper
also explores the potential of end-to-end learning to further enhance the
trading revenue by adjusting the distribution of forecast errors. Detailed case
studies are provided to validate the effectiveness of these proposed methods.
Code for all mentioned methods is available for reproduction and further
research in both industry and academia.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [239] [Inferring entropy production in many-body systems using nonequilibrium MaxEnt](https://arxiv.org/abs/2505.10444)
*Miguel Aguilera,Sosuke Ito,Artemy Kolchinsky*

Main category: cond-mat.stat-mech

TL;DR: 提出了在高维随机系统（包括多体系统和非马尔可夫系统）中推断熵增（EP）的方法，利用非平衡最大熵原理和凸对偶性，仅需轨迹观测数据，无需重构高维概率分布或速率矩阵。


<details>
  <summary>Details</summary>
Motivation: 传统方法在高维系统中因计算和统计限制难以估计熵增，因此需要一种更高效的方法。

Method: 采用非平衡最大熵原理和凸对偶性，基于轨迹观测数据（如时空相关函数）推断轨迹级熵增和平均熵增的下界。

Result: 方法在1000个自旋的无序非平衡自旋模型和大型神经尖峰数据集中表现良好。

Conclusion: 该方法无需高维分布或特殊假设，可分解不同相互作用的熵增贡献，具有热力学不确定性关系的直观物理解释。

Abstract: We propose a method for inferring entropy production (EP) in high-dimensional
stochastic systems, including many-body systems and non-Markovian systems with
long memory. Standard techniques for estimating EP become intractable in such
systems due to computational and statistical limitations. We infer
trajectory-level EP and lower bounds on average EP by exploiting a
nonequilibrium analogue of the Maximum Entropy principle, along with convex
duality. Our approach uses only samples of trajectory observables (such as
spatiotemporal correlation functions). It does not require reconstruction of
high-dimensional probability distributions or rate matrices, nor any special
assumptions such as discrete states or multipartite dynamics. It may be used to
compute a hierarchical decomposition of EP, reflecting contributions from
different kinds of interactions, and it has an intuitive physical
interpretation as a thermodynamic uncertainty relation. We demonstrate its
numerical performance on a disordered nonequilibrium spin model with 1000 spins
and a large neural spike-train dataset.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [240] [Sybil-based Virtual Data Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2505.09983)
*Changxun Zhu,Qilong Wu,Lingjuan Lyu,Shibei Xue*

Main category: cs.CR

TL;DR: 提出了一种基于Sybil的虚拟数据投毒攻击方法，通过梯度匹配减少计算复杂度，并在非独立同分布数据下表现优于现有攻击算法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受恶意攻击者的投毒攻击，现有方法成本高，需更高效攻击手段。

Method: 1. 使用Sybil节点放大攻击效果；2. 基于梯度匹配生成虚拟数据降低计算复杂度；3. 设计三种目标模型获取方案（在线局部、在线全局及离线场景）。

Result: 在非独立同分布数据下，该方法优于其他攻击算法，并能获取全局目标模型。

Conclusion: 该方法高效、低成本，适用于实际联邦学习投毒攻击场景。

Abstract: Federated learning is vulnerable to poisoning attacks by malicious
adversaries. Existing methods often involve high costs to achieve effective
attacks. To address this challenge, we propose a sybil-based virtual data
poisoning attack, where a malicious client generates sybil nodes to amplify the
poisoning model's impact. To reduce neural network computational complexity, we
develop a virtual data generation method based on gradient matching. We also
design three schemes for target model acquisition, applicable to online local,
online global, and offline scenarios. In simulation, our method outperforms
other attack algorithms since our method can obtain a global target model under
non-independent uniformly distributed data.

</details>


### [241] [PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization](https://arxiv.org/abs/2505.09921)
*Yidan Wang,Yanan Cao,Yubing Ren,Fang Fang,Zheng Lin,Binxing Fang*

Main category: cs.CR

TL;DR: 这篇论文研究了大型语言模型（LLMs）的隐私风险，提出了一种名为PIG的新框架，通过结合越狱攻击和个人可识别信息（PII）提取，显著提高了隐私泄露的成功率，并强调了加强LLM隐私保护的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估LLMs隐私泄露时存在局限性，往往容易被对齐良好的模型拦截。论文旨在探索越狱攻击在隐私泄露中的效果，并解决当前越狱方法的不足。

Method: 论文提出PIG框架，通过识别PII实体及其类型、利用上下文学习构建隐私上下文，并结合三种基于梯度的策略迭代更新以提取目标PII。

Result: 实验表明，PIG在四种白盒和两种黑盒LLMs上显著优于基线方法，实现了最优性能，揭示了LLMs的严重隐私风险。

Conclusion: PIG框架为LLMs的隐私泄露研究提供了新视角，实验结果凸显了加强LLM隐私保护措施的紧迫性。

Abstract: Large Language Models (LLMs) excel in various domains but pose inherent
privacy risks. Existing methods to evaluate privacy leakage in LLMs often use
memorized prefixes or simple instructions to extract data, both of which
well-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM
safety mechanisms to generate harmful content, but their role in privacy
scenarios remains underexplored. In this paper, we examine the effectiveness of
jailbreak attacks in extracting sensitive information, bridging privacy leakage
and jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework
targeting Personally Identifiable Information (PII) and addressing the
limitations of current jailbreak methods. Specifically, PIG identifies PII
entities and their types in privacy queries, uses in-context learning to build
a privacy context, and iteratively updates it with three gradient-based
strategies to elicit target PII. We evaluate PIG and existing jailbreak methods
using two privacy-related datasets. Experiments on four white-box and two
black-box LLMs show that PIG outperforms baseline methods and achieves
state-of-the-art (SoTA) results. The results underscore significant privacy
risks in LLMs, emphasizing the need for stronger safeguards. Our code is
availble at
\href{https://github.com/redwyd/PrivacyJailbreak}{https://github.com/redwyd/PrivacyJailbreak}.

</details>


### [242] [Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber Security Data](https://arxiv.org/abs/2505.09974)
*Adel ElZemity,Budi Arief,Shujun Li*

Main category: cs.CR

TL;DR: 本文评估了7个开源LLM在网络安全应用中的安全风险，发现微调会降低模型的安全韧性，并提出了一种通过重写指令-响应对来提升安全性的方法，证实了在保持技术实用性的同时提高安全性的可行性。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型（LLM）应用于网络安全领域虽能提升威胁分析和恶意软件检测能力，但也可能带来数据泄露和自动生成恶意软件等风险。本文旨在系统评估微调LLM的安全风险，并探索如何在保持技术实用性的同时提升模型安全性。

Method: 使用OWASP Top 10框架评估7个开源LLM的安全性，提出并验证了一种通过重写指令-响应对（加入明确的安全预防措施和伦理考量）的安全对齐方法。

Result: 实验表明微调会显著降低LLM的安全韧性（如Llama 3.1 8B对提示注入的安全评分从0.95降至0.15），但提出的安全对齐方法能在维持技术实用性的同时改善模型安全性。

Conclusion: 本文为LLM的安全风险提供了系统评估方法，并证明通过指令优化可实现安全性与实用性的平衡，为敏感领域安全部署生成式AI提供了实践路径。

Abstract: The integration of large language models (LLMs) into cyber security
applications presents significant opportunities, such as enhancing threat
analysis and malware detection, but can also introduce critical risks and
safety concerns, including personal data leakage and automated generation of
new malware. We present a systematic evaluation of safety risks in fine-tuned
LLMs for cyber security applications. Using the OWASP Top 10 for LLM
Applications framework, we assessed seven open-source LLMs: Phi 3 Mini 3.8B,
Mistral 7B, Qwen 2.5 7B, Llama 3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B.
Our evaluation shows that fine-tuning reduces safety resilience across all
tested LLMs (e.g., the safety score of Llama 3.1 8B against prompt injection
drops from 0.95 to 0.15). We propose and evaluate a safety alignment approach
that carefully rewords instruction-response pairs to include explicit safety
precautions and ethical considerations. This approach demonstrates that it is
possible to maintain or even improve model safety while preserving technical
utility, offering a practical path forward for developing safer fine-tuning
methodologies. This work offers a systematic evaluation for safety risks in
LLMs, enabling safer adoption of generative AI in sensitive domains, and
contributing towards the development of secure, trustworthy, and ethically
aligned LLMs.

</details>


### [243] [AttentionGuard: Transformer-based Misbehavior Detection for Secure Vehicular Platoons](https://arxiv.org/abs/2505.10273)
*Hexu Li,Konstantinos Kalogiannis,Ahmed Mohamed Hussain,Panos Papadimitratos*

Main category: cs.CR

TL;DR: AttentionGuard是一种基于Transformer的框架，用于检测车辆编队系统中的异常行为，通过自注意力机制识别异常模式，实验显示其在多种攻击场景下具有高检测精度和低延迟。


<details>
  <summary>Details</summary>
Motivation: 车辆编队技术虽然能提升燃油效率和道路利用率，但易受内部认证用户的复杂伪造攻击，可能引发灾难性碰撞，因此需要有效的异常行为检测方法。

Method: 采用多头Transformer编码器处理运动学序列数据，区分正常行为与攻击模式，涵盖稳态、加入和退出等多种编队场景。

Result: 实验表明，AttentionGuard的F1分数高达0.95，并在复杂操作中保持鲁棒性，决策延迟低至100ms，实时性优异。

Conclusion: Transformer编码器为保护协同智能交通系统（C-ITS）免受内部威胁提供了高效解决方案。

Abstract: Vehicle platooning, with vehicles traveling in close formation coordinated
through Vehicle-to-Everything (V2X) communications, offers significant benefits
in fuel efficiency and road utilization. However, it is vulnerable to
sophisticated falsification attacks by authenticated insiders that can
destabilize the formation and potentially cause catastrophic collisions. This
paper addresses this challenge: misbehavior detection in vehicle platooning
systems. We present AttentionGuard, a transformer-based framework for
misbehavior detection that leverages the self-attention mechanism to identify
anomalous patterns in mobility data. Our proposal employs a multi-head
transformer-encoder to process sequential kinematic information, enabling
effective differentiation between normal mobility patterns and falsification
attacks across diverse platooning scenarios, including steady-state
(no-maneuver) operation, join, and exit maneuvers. Our evaluation uses an
extensive simulation dataset featuring various attack vectors (constant,
gradual, and combined falsifications) and operational parameters (controller
types, vehicle speeds, and attacker positions). Experimental results
demonstrate that AttentionGuard achieves up to 0.95 F1-score in attack
detection, with robust performance maintained during complex maneuvers.
Notably, our system performs effectively with minimal latency (100ms decision
intervals), making it suitable for real-time transportation safety
applications. Comparative analysis reveals superior detection capabilities and
establishes the transformer-encoder as a promising approach for securing
Cooperative Intelligent Transport Systems (C-ITS) against sophisticated insider
threats.

</details>


### [244] [Private Transformer Inference in MLaaS: A Survey](https://arxiv.org/abs/2505.10315)
*Yang Li,Xinyu Zhou,Yitong Wang,Liangxin Qian,Jun Zhao*

Main category: cs.CR

TL;DR: 本文总结了隐私保护的Transformer推理（PTI）的最新技术、挑战及评估框架，着重于在资源效率与隐私之间找到平衡。


<details>
  <summary>Details</summary>
Motivation: 由于MLaaS中集中处理用户敏感数据的隐私问题，提出了PTI作为解决方案，结合加密技术保护数据和模型隐私。

Method: 采用安全多方计算和同态加密等密码学技术，并提出了一个结构化的分类法和评估框架。

Result: 展示了PTI在实现高性能推理与数据隐私之间的有效平衡。

Conclusion: PTI是解决MLaaS隐私问题的可行方案，未来需进一步优化资源效率与隐私保护的平衡。

Abstract: Transformer models have revolutionized AI, powering applications like content
generation and sentiment analysis. However, their deployment in Machine
Learning as a Service (MLaaS) raises significant privacy concerns, primarily
due to the centralized processing of sensitive user data. Private Transformer
Inference (PTI) offers a solution by utilizing cryptographic techniques such as
secure multi-party computation and homomorphic encryption, enabling inference
while preserving both user data and model privacy. This paper reviews recent
PTI advancements, highlighting state-of-the-art solutions and challenges. We
also introduce a structured taxonomy and evaluation framework for PTI, focusing
on balancing resource efficiency with privacy and bridging the gap between
high-performance inference and data privacy.

</details>


### [245] [AutoPentest: Enhancing Vulnerability Management With Autonomous LLM Agents](https://arxiv.org/abs/2505.10321)
*Julius Henke*

Main category: cs.CR

TL;DR: 论文探讨了利用大型语言模型（LLMs）进行渗透测试以降低成本并提高频率。提出基于GPT-4o和LangChain的AutoPentest应用，实验表明其略优于手动使用ChatGPT，但需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 研究目的是通过LLMs自动化渗透测试，降低成本并提高测试频率，从而改进漏洞管理。

Method: 通过结合GPT-4o和LangChain框架开发AutoPentest应用，支持复杂多步任务，并辅以外部工具和知识库。实验对比了AutoPentest和手动使用ChatGPT的效果。

Result: 两种方法在Hack The Box机器上均完成15-25%的子任务，AutoPentest略优于ChatGPT，总费用为96.20美元。

Conclusion: 未来通过优化实现和更强大的LLMs，AutoPentest有望成为漏洞管理的可行工具。

Abstract: A recent area of increasing research is the use of Large Language Models
(LLMs) in penetration testing, which promises to reduce costs and thus allow
for higher frequency. We conduct a review of related work, identifying best
practices and common evaluation issues. We then present AutoPentest, an
application for performing black-box penetration tests with a high degree of
autonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent
framework LangChain. It can perform complex multi-step tasks, augmented by
external tools and knowledge bases. We conduct a study on three
capture-the-flag style Hack The Box (HTB) machines, comparing our
implementation AutoPentest with the baseline approach of manually using the
ChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the
subtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT.
We measure a total cost of \$96.20 US when using AutoPentest across all
experiments, while a one-month subscription to ChatGPT Plus costs \$20. The
results show that further implementation efforts and the use of more powerful
LLMs released in the future are likely to make this a viable part of
vulnerability management.

</details>


### [246] [Automated Alert Classification and Triage (AACT): An Intelligent System for the Prioritisation of Cybersecurity Alerts](https://arxiv.org/abs/2505.09843)
*Melissa Turcotte,François Labrèche,Serge-Olivier Paquette*

Main category: cs.CR

TL;DR: 该研究提出了一种名为AACT的系统，通过自动化SOC工作流程来减轻分析师疲劳，成功减少61%的警报量并保持低误报率。


<details>
  <summary>Details</summary>
Motivation: 企业网络规模扩大导致安全警报数量激增，SOC分析师因处理大量良性警报而疲劳，亟需自动化解决方案。

Method: AACT系统通过学习分析师对警报的分类行为，实时预测并自动处理良性警报，优先处理关键警报。

Result: 在真实SOC环境中，AACT系统六个月内减少61%的警报展示，误报率仅为1.36%。

Conclusion: AACT系统有效提升SOC效率，减轻分析师负担，同时保持高准确性。

Abstract: Enterprise networks are growing ever larger with a rapidly expanding attack
surface, increasing the volume of security alerts generated from security
controls. Security Operations Centre (SOC) analysts triage these alerts to
identify malicious activity, but they struggle with alert fatigue due to the
overwhelming number of benign alerts. Organisations are turning to managed SOC
providers, where the problem is amplified by context switching and limited
visibility into business processes.
  A novel system, named AACT, is introduced that automates SOC workflows by
learning from analysts' triage actions on cybersecurity alerts. It accurately
predicts triage decisions in real time, allowing benign alerts to be closed
automatically and critical ones prioritised. This reduces the SOC queue
allowing analysts to focus on the most severe, relevant or ambiguous threats.
The system has been trained and evaluated on both real SOC data and an open
dataset, obtaining high performance in identifying malicious alerts from benign
alerts.
  Additionally, the system has demonstrated high accuracy in a real SOC
environment, reducing alerts shown to analysts by 61% over six months, with a
low false negative rate of 1.36% over millions of alerts.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [247] [AI and Generative AI Transforming Disaster Management: A Survey of Damage Assessment and Response Techniques](https://arxiv.org/abs/2505.08202)
*Aman Raj,Lakshit Arora,Sanjay Surendranath Girija,Shashank Kapoor,Dipen Pradhan,Ankit Shetgaonkar*

Main category: cs.CY

TL;DR: 本文综述了AI和GenAI在自然灾害损害评估中的应用，讨论了其在多模态数据处理中的优势、局限性及伦理问题，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 自然灾害对生命和基础设施构成巨大威胁，快速有效的损害评估至关重要。AI和GenAI的出现为这一领域提供了创新解决方案，亟待系统研究其潜力和挑战。

Method: 通过综合分析AI和GenAI在多模态数据（文本、图像、视频、音频）处理中的应用，并探讨数据隐私、安全及伦理问题。

Result: 研究表明，AI和GenAI能高效整合多源数据、模拟灾害情景，但存在误用风险（如虚假信息传播）。

Conclusion: 未来需开发安全、可靠、伦理的GenAI系统，以推动灾害管理的技术进步。

Abstract: Natural disasters, including earthquakes, wildfires and cyclones, bear a huge
risk on human lives as well as infrastructure assets. An effective response to
disaster depends on the ability to rapidly and efficiently assess the intensity
of damage. Artificial Intelligence (AI) and Generative Artificial Intelligence
(GenAI) presents a breakthrough solution, capable of combining knowledge from
multiple types and sources of data, simulating realistic scenarios of disaster,
and identifying emerging trends at a speed previously unimaginable. In this
paper, we present a comprehensive review on the prospects of AI and GenAI in
damage assessment for various natural disasters, highlighting both its
strengths and limitations. We talk about its application to multimodal data
such as text, image, video, and audio, and also cover major issues of data
privacy, security, and ethical use of the technology during crises. The paper
also recognizes the threat of Generative AI misuse, in the form of
dissemination of misinformation and for adversarial attacks. Finally, we
outline avenues of future research, emphasizing the need for secure, reliable,
and ethical Generative AI systems for disaster management in general. We
believe that this work represents the first comprehensive survey of Gen-AI
techniques being used in the field of Disaster Assessment and Response.

</details>


### [248] [Healthy Distrust in AI systems](https://arxiv.org/abs/2505.09747)
*Benjamin Paaßen,Suzana Alpsancar,Tobias Matzner,Ingrid Scharlau*

Main category: cs.CY

TL;DR: 提出“健康的不信任”概念，强调在某些AI使用实践中合理的怀疑态度对人类自主权的重要性。


<details>
  <summary>Details</summary>
Motivation: 当代AI研究强调可信AI设计以促进人类信任，但忽视当AI系统嵌入不利于用户利益的社会背景时，不信任可能是必要且合理的。

Method: 通过跨学科（计算机科学、社会学、心理学等）梳理信任与不信任的现有理论，识别理论空白并提出“健康的不信任”作为补充。

Result: 论证了“健康的不信任”在保障人类自主权方面的关键作用，填补了AI伦理领域的理论缺口。

Conclusion: AI使用需尊重人类自主权，合理的怀疑态度（健康的不信任）是构建真正信任的前提。

Abstract: Under the slogan of trustworthy AI, much of contemporary AI research is
focused on designing AI systems and usage practices that inspire human trust
and, thus, enhance adoption of AI systems. However, a person affected by an AI
system may not be convinced by AI system design alone -- neither should they,
if the AI system is embedded in a social context that gives good reason to
believe that it is used in tension with a person's interest. In such cases,
distrust in the system may be justified and necessary to build meaningful trust
in the first place. We propose the term "healthy distrust" to describe such a
justified, careful stance towards certain AI usage practices. We investigate
prior notions of trust and distrust in computer science, sociology, history,
psychology, and philosophy, outline a remaining gap that healthy distrust might
fill and conceptualize healthy distrust as a crucial part for AI usage that
respects human autonomy.

</details>


### [249] [Which Demographic Features Are Relevant for Individual Fairness Evaluation of U.S. Recidivism Risk Assessment Tools?](https://arxiv.org/abs/2505.09868)
*Tin Trung Nguyen,Jiannan Xu,Phuong-Anh Nguyen-Le,Jonathan Lazar,Donald Braman,Hal Daumé III,Zubin Jelveh*

Main category: cs.CY

TL;DR: 研究发现，在评估累犯风险评估工具的个体公平性时，应考虑年龄和性别，但忽略种族因素。


<details>
  <summary>Details</summary>
Motivation: 尽管个体公平性是美国宪法的基本原则之一，但这一技术标准并未在州或联邦法律法规中得到具体实施。本研究旨在填补这一空白，通过实验确定哪些人口特征应被纳入个体公平性评估中。

Method: 通过人类受试者实验，评估了累犯风险评估工具中个体公平性相关的不同人口特征（如年龄、性别、种族）的适用性。

Result: 分析表明，个体相似性函数应考虑年龄和性别，但忽略种族因素。

Conclusion: 该研究为累犯风险评估工具的公平性提供了具体指导，建议在个体公平性标准中纳入年龄和性别，而排除种族因素。

Abstract: Despite its U.S. constitutional foundation, the technical ``individual
fairness'' criterion has not been operationalized in state or federal
statutes/regulations. We conduct a human subjects experiment to address this
gap, evaluating which demographic features are relevant for individual fairness
evaluation of recidivism risk assessment (RRA) tools. Our analyses conclude
that the individual similarity function should consider age and sex, but it
should ignore race.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [250] [Pure Component Property Estimation Framework Using Explainable Machine Learning Methods](https://arxiv.org/abs/2505.09783)
*Jianfeng Jiao,Xi Gao,Jie Li*

Main category: stat.AP

TL;DR: 提出了一个基于可解释机器学习的纯组分物理化学性质预测框架，通过分子连接矩阵和特征池化方法优化特征选择，显著提高了预测精度并减少特征数量。


<details>
  <summary>Details</summary>
Motivation: 准确预测纯组分的物理化学性质对过程集成、多尺度建模和优化至关重要，现有方法在特征选择和解释性上存在不足。

Method: 采用基于连接矩阵的分子表示方法自动生成特征，结合随机森林进行特征排序和池化，使用调整R2和SHAP值分析特征贡献。

Result: 测试集上的均方根误差降低了83.8%，特征数量从13316降至100，且模型精度未受影响。

Conclusion: 提出的框架可行，为混合物组分重构和过程集成建模提供了坚实基础。

Abstract: Accurate prediction of pure component physiochemical properties is crucial
for process integration, multiscale modeling, and optimization. In this work,
an enhanced framework for pure component property prediction by using
explainable machine learning methods is proposed. In this framework, the
molecular representation method based on the connectivity matrix effectively
considers atomic bonding relationships to automatically generate features. The
supervised machine learning model random forest is applied for feature ranking
and pooling. The adjusted R2 is introduced to penalize the inclusion of
additional features, providing an assessment of the true contribution of
features. The prediction results for normal boiling point (Tb), liquid molar
volume, critical temperature (Tc) and critical pressure (Pc) obtained using
Artificial Neural Network and Gaussian Process Regression models confirm the
accuracy of the molecular representation method. Comparison with GC based
models shows that the root-mean-square error on the test set can be reduced by
up to 83.8%. To enhance the interpretability of the model, a feature analysis
method based on Shapley values is employed to determine the contribution of
each feature to the property predictions. The results indicate that using the
feature pooling method reduces the number of features from 13316 to 100 without
compromising model accuracy. The feature analysis results for Tb, Tc, and Pc
confirms that different molecular properties are influenced by different
structural features, aligning with mechanistic interpretations. In conclusion,
the proposed framework is demonstrated to be feasible and provides a solid
foundation for mixture component reconstruction and process integration
modelling.

</details>
