<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 41]
- [cs.LG](#cs.LG) [Total: 87]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.CV](#cs.CV) [Total: 14]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 3]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.RO](#cs.RO) [Total: 3]
- [eess.IV](#eess.IV) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [cs.SE](#cs.SE) [Total: 12]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.CR](#cs.CR) [Total: 8]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.NI](#cs.NI) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [It's the same but not the same: Do LLMs distinguish Spanish varieties?](https://arxiv.org/abs/2504.20049)
*Marina Mayor-Rocher,Cristina Pozo,Nina Melero,Gonzalo Martínez,María Grandury,Pedro Reviriego*

Main category: cs.CL

TL;DR: 该研究评估了九种语言模型识别七种西班牙语方言的能力，发现GPT-4o是唯一能识别语言多样性的模型，且所有模型对半岛西班牙语的识别效果最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在西班牙语理解和生成方面表现出色，但西班牙语存在丰富的方言变体，研究旨在评估这些模型识别方言的能力。

Method: 使用多选题测试评估九种语言模型对七种西班牙语方言（如安第斯、加勒比、半岛等）的识别能力。

Result: 半岛西班牙语是所有模型中识别效果最佳的方言，而GPT-4o是唯一能识别西班牙语多样性的模型。

Conclusion: 研究证实语言模型在西班牙语方言识别上仍有改进空间，GPT-4o展现了更强的多样性识别能力。

Abstract: In recent years, large language models (LLMs) have demonstrated a high
capacity for understanding and generating text in Spanish. However, with five
hundred million native speakers, Spanish is not a homogeneous language but
rather one rich in diatopic variations spanning both sides of the Atlantic. For
this reason, in this study, we evaluate the ability of nine language models to
identify and distinguish the morphosyntactic and lexical peculiarities of seven
varieties of Spanish (Andean, Antillean, Continental Caribbean, Chilean,
Peninsular, Mexican and Central American and Rioplatense) through a
multiple-choice test. The results indicate that the Peninsular Spanish variety
is the best identified by all models and that, among them, GPT-4o is the only
model capable of recognizing the variability of the Spanish language.
  --
  En los \'ultimos a\~nos, los grandes modelos de lenguaje (LLMs, por sus
siglas en ingl\'es) han demostrado una alta capacidad para comprender y generar
texto en espa\~nol. Sin embargo, con quinientos millones de hablantes nativos,
la espa\~nola no es una lengua homog\'enea, sino rica en variedades
diat\'opicas que se extienden a ambos lados del Atl\'antico. Por todo ello,
evaluamos en este trabajo la capacidad de nueve modelos de lenguaje de
identificar y discernir las peculiaridades morfosint\'acticas y l\'exicas de
siete variedades de espa\~nol (andino, antillano, caribe\~no continental,
chileno, espa\~nol peninsular, mexicano y centroamericano y rioplatense)
mediante un test de respuesta m\'ultiple. Los resultados obtenidos indican que
la variedad de espa\~nol peninsular es la mejor identificada por todos los
modelos y que, de entre todos, GPT-4o es el \'unico modelo capaz de identificar
la variabilidad de la lengua espa\~nola.

</details>


### [2] [Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts](https://arxiv.org/abs/2504.20051)
*Frances Laureano De Leon,Harish Tayyar Madabushi,Mark G. Lee*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型处理多词表达歧义的能力，发现即使是最先进的模型（如GPT-4）在多词表达的检测和语义任务上表现不佳，尤其是在低频语境中。


<details>
  <summary>Details</summary>
Motivation: 多词表达具有非组合性意义和句法不规则性，其字面和惯用意义的歧义性为语言模型带来了挑战。研究旨在评估模型处理这类语言微妙之处的能力。

Method: 研究通过评估模型在英语、葡萄牙语和加利西亚语中的表现，使用新的代码切换数据集和任务，比较了包括GPT-4在内的最新模型与基线模型xlm-roBERTa-base的表现。

Result: 结果显示，即使是GPT-4在多词表达的检测和语义任务上也未能超越基线模型，尤其是在新任务中表现较差。

Conclusion: 研究表明，多词表达尤其是歧义性表达仍然是语言模型的挑战，突出了当前模型的局限性。

Abstract: Multiword expressions, characterised by non-compositional meanings and
syntactic irregularities, are an example of nuanced language. These expressions
can be used literally or idiomatically, leading to significant changes in
meaning. While large language models have demonstrated strong performance
across many tasks, their ability to handle such linguistic subtleties remains
uncertain. Therefore, this study evaluates how state-of-the-art language models
process the ambiguity of potentially idiomatic multiword expressions,
particularly in contexts that are less frequent, where models are less likely
to rely on memorisation. By evaluating models across in Portuguese and
Galician, in addition to English, and using a novel code-switched dataset and a
novel task, we find that large language models, despite their strengths,
struggle with nuanced language. In particular, we find that the latest models,
including GPT-4, fail to outperform the xlm-roBERTa-base baselines in both
detection and semantic tasks, with especially poor performance on the novel
tasks we introduce, despite its similarity to existing tasks. Overall, our
results demonstrate that multiword expressions, especially those which are
ambiguous, continue to be a challenge to models.

</details>


### [3] [Understanding and Mitigating Risks of Generative AI in Financial Services](https://arxiv.org/abs/2504.20086)
*Sebastian Gehrmann,Claire Huang,Xian Teng,Sergei Yurovski,Iyanuoluwa Shode,Chirag S. Patel,Arjun Bhorkar,Naveen Thomas,John Doucette,David Rosenberg,Mark Dredze,David Rabinowitz*

Main category: cs.CL

TL;DR: 该论文探讨了金融服务业中AI内容安全的特定考量，提出了一个风险分类法，并评估现有开源护栏技术的覆盖情况，发现其大多未能检测到讨论的内容风险。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全研究过于关注通用领域的毒性、偏见和公平性，而忽视了专业领域的法律和监管要求。本文旨在填补金融服务业AI内容安全的空白。

Method: 提出一个AI内容风险分类法，并通过红队活动收集的数据评估现有开源护栏技术的有效性。

Result: 研究发现现有护栏技术未能检测到大部分讨论的金融领域内容风险。

Conclusion: 强调了专业领域AI安全的重要性，并指出了现有技术在特定领域的不足，呼吁更针对性的解决方案。

Abstract: To responsibly develop Generative AI (GenAI) products, it is critical to
define the scope of acceptable inputs and outputs. What constitutes a "safe"
response is an actively debated question. Academic work puts an outsized focus
on evaluating models by themselves for general purpose aspects such as
toxicity, bias, and fairness, especially in conversational applications being
used by a broad audience. In contrast, less focus is put on considering
sociotechnical systems in specialized domains. Yet, those specialized systems
can be subject to extensive and well-understood legal and regulatory scrutiny.
These product-specific considerations need to be set in industry-specific laws,
regulations, and corporate governance requirements. In this paper, we aim to
highlight AI content safety considerations specific to the financial services
domain and outline an associated AI content risk taxonomy. We compare this
taxonomy to existing work in this space and discuss implications of risk
category violations on various stakeholders. We evaluate how existing
open-source technical guardrail solutions cover this taxonomy by assessing them
on data collected via red-teaming activities. Our results demonstrate that
these guardrails fail to detect most of the content risks we discuss.

</details>


### [4] [Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models](https://arxiv.org/abs/2504.20157)
*Zae Myung Kim,Chanwoo Park,Vipul Raheja,Dongyeop Kang*

Main category: cs.CL

TL;DR: MPO是一种元策略优化框架，通过动态调整奖励模型的提示来解决奖励破解和依赖人工设计的限制，提升LLM对齐的稳定性和适应性。


<details>
  <summary>Details</summary>
Motivation: 目前基于奖励的大语言模型对齐方法存在奖励破解风险和依赖繁琐的提示工程的问题，需要更稳定和自适应的解决方案。

Method: 引入元奖励模型，动态调整奖励提示以保持高对齐性，减少对人工设计的依赖。

Result: MPO在性能上与传统手工设计奖励提示的方法相当或更优，并在多种任务中表现稳定。

Conclusion: MPO为LLM奖励对齐提供了更鲁棒和自适应的策略，可扩展至更高级的对齐框架。

Abstract: Reward-based alignment methods for large language models (LLMs) face two key
limitations: vulnerability to reward hacking, where models exploit flaws in the
reward signal; and reliance on brittle, labor-intensive prompt engineering when
LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a
framework that addresses these challenges by integrating a meta-reward model
that dynamically refines the reward model's prompt throughout training. In MPO,
the meta-reward model monitors the evolving training context and continuously
adjusts the reward model's prompt to maintain high alignment, providing an
adaptive reward signal that resists exploitation by the policy. This
meta-learning approach promotes a more stable policy optimization, and greatly
reduces the need for manual reward prompt design. It yields performance on par
with or better than models guided by extensively hand-crafted reward prompts.
Furthermore, we show that MPO maintains its effectiveness across diverse tasks,
such as question answering and mathematical reasoning, without requiring
specialized reward designs. Beyond standard RLAIF, MPO's meta-learning
formulation is readily extensible to higher-level alignment frameworks.
Overall, this method addresses theoretical and practical challenges in
reward-based RL alignment for LLMs, paving the way for more robust and
adaptable alignment strategies. The code and models will be publicly shared.

</details>


### [5] [MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools](https://arxiv.org/abs/2504.20168)
*Nishant Subramani,Jason Eisner,Justin Svegliato,Benjamin Van Durme,Yu Su,Sam Thomson*

Main category: cs.CL

TL;DR: 提出了MICE方法，通过模型内部中间层解码和相似度评分改进工具调用时的置信度评估，提高工具的实用性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有模型置信度校准不佳，影响工具调用时的风险与回报权衡，需要改进方法以提升模型在工具调用中的表现。

Method: MICE方法通过解码语言模型各中间层（使用logitLens），计算每层生成与最终输出的相似度，输入概率分类器评估置信度。

Result: 在STE数据集上，MICE优于基线，显著提升预期工具调用效用指标，并展示样本高效性、零样本泛化能力和风险场景适应性。

Conclusion: MICE通过内部层分析提升置信度评估，增强工具调用的安全性和实用性，代码已开源。

Abstract: Tool-using agents that act in the world need to be both useful and safe.
Well-calibrated model confidences can be used to weigh the risk versus reward
of potential actions, but prior work shows that many models are poorly
calibrated. Inspired by interpretability literature exploring the internals of
models, we propose a novel class of model-internal confidence estimators (MICE)
to better assess confidence when calling tools. MICE first decodes from each
intermediate layer of the language model using logitLens and then computes
similarity scores between each layer's generation and the final output. These
features are fed into a learned probabilistic classifier to assess confidence
in the decoded output. On the simulated trial and error (STE) tool-calling
dataset using Llama3 models, we find that MICE beats or matches the baselines
on smoothed expected calibration error. Using MICE confidences to determine
whether to call a tool significantly improves over strong baselines on a new
metric, expected tool-calling utility. Further experiments show that MICE is
sample-efficient, can generalize zero-shot to unseen APIs, and results in
higher tool-calling utility in scenarios with varying risk levels. Our code is
open source, available at https://github.com/microsoft/mice_for_cats.

</details>


### [6] [A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports](https://arxiv.org/abs/2504.20220)
*Henning Schäfer,Cynthia S. Schmidt,Johannes Wutzkowsky,Kamil Lorek,Lea Reinartz,Johannes Rückert,Christian Temme,Britta Böckmann,Peter A. Horn,Christoph M. Friedrich*

Main category: cs.CL

TL;DR: 该研究提出了一种开源流程，用于从扫描文档中提取和分类复选框数据，以减少人工转录的负担并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管电子健康记录应用广泛，许多流程仍依赖纸质文档，人工转录耗时且易出错，需一种更高效的方法。

Method: 该流程结合了复选框检测、多语言OCR和多语言视觉语言模型(VLMs)，并以输血反应报告为例展示了其适应性。

Result: 与2017-2024年的金标准比较，该流程取得了高精度和高召回率，降低了行政工作量并提高了报告准确性。

Conclusion: 该开源流程可广泛用于自动解析复选框表单，支持自托管部署，适用于多种文档类型。

Abstract: Despite the growing adoption of electronic health records, many processes
still rely on paper documents, reflecting the heterogeneous real-world
conditions in which healthcare is delivered. The manual transcription process
is time-consuming and prone to errors when transferring paper-based data to
digital formats. To streamline this workflow, this study presents an
open-source pipeline that extracts and categorizes checkbox data from scanned
documents. Demonstrated on transfusion reaction reports, the design supports
adaptation to other checkbox-rich document types. The proposed method
integrates checkbox detection, multilingual optical character recognition (OCR)
and multilingual vision-language models (VLMs). The pipeline achieves high
precision and recall compared against annually compiled gold-standards from
2017 to 2024. The result is a reduction in administrative workload and accurate
regulatory reporting. The open-source availability of this pipeline encourages
self-hosted parsing of checkbox forms.

</details>


### [7] [A Platform for Generating Educational Activities to Teach English as a Second Language](https://arxiv.org/abs/2504.20251)
*Aiala Rosá,Santiago Góngora,Juan Pablo Filevich,Ignacio Sastre,Laura Musto,Brian Carpenter,Luis Chiruzzo*

Main category: cs.CL

TL;DR: 这篇论文介绍了一个基于自然语言处理技术的英语教学平台，能够生成游戏和练习，支持教师自定义内容，并计划集成图像和文本生成功能，同时迁移到更强大的服务器。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一个利用自然语言处理技术生成多样化英语教学活动的平台，帮助教师更高效地设计课程内容。

Method: 平台通过半自动和手动结合的方式生成游戏和练习，支持教师输入文本生成复杂活动，并计划集成图像和文本生成技术。

Result: 平台已开发并部署给终端用户，展示了如何克服技术挑战，并计划进一步扩展功能。

Conclusion: 论文总结了平台的开发经验，并展望了未来集成更多AI功能和性能优化的方向。

Abstract: We present a platform for the generation of educational activities oriented
to teaching English as a foreign language. The different activities -- games
and language practice exercises -- are strongly based on Natural Language
Processing techniques. The platform offers the possibility of playing
out-of-the-box games, generated from resources created semi-automatically and
then manually curated. It can also generate games or exercises of greater
complexity from texts entered by teachers, providing a stage of review and
edition of the generated content before use. As a way of expanding the variety
of activities in the platform, we are currently experimenting with image and
text generation. In order to integrate them and improve the performance of
other neural tools already integrated, we are working on migrating the platform
to a more powerful server. In this paper we describe the development of our
platform and its deployment for end users, discussing the challenges faced and
how we overcame them, and also detail our future work plans.

</details>


### [8] [Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi](https://arxiv.org/abs/2504.20276)
*Dandan Chen Kaptur,Yue Huang,Xuejun Ryan Ji,Yanhui Guo,Bradley Kaptur*

Main category: cs.CL

TL;DR: 该研究比较了GPT-4和Kimi两个大型语言模型（LLMs）在系统性综述中的表现，发现其性能受数据量和问题复杂性影响。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在系统性综述中的适用性，并将其表现与人工生成的代码进行对比。

Method: 通过对比LLM生成的代码与同行评审的系统性综述中人工生成的代码来评估性能。

Result: LLMs的表现因数据量和问题复杂性而波动。

Conclusion: LLMs在系统性综述中的应用潜力受限于数据量和问题复杂性。

Abstract: This research delved into GPT-4 and Kimi, two Large Language Models (LLMs),
for systematic reviews. We evaluated their performance by comparing
LLM-generated codes with human-generated codes from a peer-reviewed systematic
review on assessment. Our findings suggested that the performance of LLMs
fluctuates by data volume and question complexity for systematic reviews.

</details>


### [9] [UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions](https://arxiv.org/abs/2504.20304)
*Xiulin Yang,Zhuoxuan Ju,Lanni Bu,Zoey Liu,Nathan Schneider*

Main category: cs.CL

TL;DR: 该论文介绍了第一个正式发布的基于CHILDES数据的Universal Dependencies（UD）树库UD-English-CHILDES，统一了11名儿童及其照顾者的48k+句子标注，并提供了1M银标准句子资源。


<details>
  <summary>Details</summary>
Motivation: CHILDES是广泛使用的儿童语言数据资源，但缺乏一致的UD标注标准，论文旨在填补这一空白，为计算和语言学研究提供统一资源。

Method: 论文通过整合CHILDES中已有的依赖标注数据，采用UD v2框架统一标注，并验证现有黄金标准标注的准确性。

Result: 构建了包含48k+黄金标准句子和1M银标准句子的UD-English-CHILDES树库，标注一致且符合UD标准。

Conclusion: UD-English-CHILDES为儿童语言研究提供了高质量的标准化数据，未来可扩展至更多语言或语料。

Abstract: CHILDES is a widely used resource of transcribed child and child-directed
speech. This paper introduces UD-English-CHILDES, the first officially released
Universal Dependencies (UD) treebank derived from previously
dependency-annotated CHILDES data with consistent and unified annotation
guidelines. Our corpus harmonizes annotations from 11 children and their
caregivers, totaling over 48k sentences. We validate existing gold-standard
annotations under the UD v2 framework and provide an additional 1M
silver-standard sentences, offering a consistent resource for computational and
linguistic research.

</details>


### [10] [Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation](https://arxiv.org/abs/2504.20323)
*Chao-Lin Liu,Po-Hsien Wu,Yi-Ting Yu*

Main category: cs.CL

TL;DR: 提出了一种利用法律条文共引用的新方法，用于在标签有限的专业法律领域（如劳动争议）中构建推荐系统，通过算法标注实现案例相似性推荐。


<details>
  <summary>Details</summary>
Motivation: 解决专业法律领域（如劳动争议）中标签数据集有限的问题，推动法律文档的自动化标注技术发展。

Method: 利用法律条文的共引用建立案例相似性，结合微调的文本嵌入模型和BiLSTM模块构建推荐系统。

Result: 实验证明，该系统能有效推荐基于法律条文共引用的相似劳动争议案例。

Conclusion: 该方法为法律文档自动化标注提供了新思路，尤其在法律数据库资源有限的领域具有应用潜力。

Abstract: This report addresses the challenge of limited labeled datasets for
developing legal recommender systems, particularly in specialized domains like
labor disputes. We propose a new approach leveraging the co-citation of legal
articles within cases to establish similarity and enable algorithmic
annotation. This method draws a parallel to the concept of case co-citation,
utilizing cited precedents as indicators of shared legal issues. To evaluate
the labeled results, we employ a system that recommends similar cases based on
plaintiffs' accusations, defendants' rebuttals, and points of disputes. The
evaluation demonstrates that the recommender, with finetuned text embedding
models and a reasonable BiLSTM module can recommend labor cases whose
similarity was measured by the co-citation of the legal articles. This research
contributes to the development of automated annotation techniques for legal
documents, particularly in areas with limited access to comprehensive legal
databases.

</details>


### [11] [Local Prompt Optimization](https://arxiv.org/abs/2504.20355)
*Yash Jain,Vishal Chowdhary*

Main category: cs.CL

TL;DR: 论文提出了一种局部提示优化方法（LPO），通过仅优化关键标记来提升大型语言模型的提示效果，相比全局优化方法，在数学推理和基准测试中表现更优且收敛更快。


<details>
  <summary>Details</summary>
Motivation: 现有的提示优化方法通常是全局优化，涉及大量标记和复杂任务，导致优化空间过大、指导不足。为解决这一问题，本文提出了局部优化的思路。

Method: 方法称为局部提示优化（LPO），通过识别提示中的关键优化标记，引导语言模型仅聚焦于这些标记进行优化，可与任何自动提示工程方法结合。

Result: 实验结果显示，LPO在数学推理任务（GSM8k和MultiArith）及BIG-bench Hard基准测试中表现显著优于全局优化方法，且收敛速度更快。

Conclusion: LPO通过局部优化提升了提示优化的效率和效果，为自动提示工程提供了新的研究方向。

Abstract: In recent years, the use of prompts to guide the output of Large Language
Models have increased dramatically. However, even the best of experts struggle
to choose the correct words to stitch up a prompt for the desired task. To
solve this, LLM driven prompt optimization emerged as an important problem.
Existing prompt optimization methods optimize a prompt globally, where in all
the prompt tokens have to be optimized over a large vocabulary while solving a
complex task. The large optimization space (tokens) leads to insufficient
guidance for a better prompt. In this work, we introduce Local Prompt
Optimization (LPO) that integrates with any general automatic prompt
engineering method. We identify the optimization tokens in a prompt and nudge
the LLM to focus only on those tokens in its optimization step. We observe
remarkable performance improvements on Math Reasoning (GSM8k and MultiArith)
and BIG-bench Hard benchmarks across various automatic prompt engineering
methods. Further, we show that LPO converges to the optimal prompt faster than
global methods.

</details>


### [12] [What Causes Knowledge Loss in Multilingual Language Models?](https://arxiv.org/abs/2504.20356)
*Maria Khelli,Samuel Cahyawijaya,Ayu Purwarianti,Genta Indra Winata*

Main category: cs.CL

TL;DR: 该研究探讨了多语言NLP模型中的跨语言迁移和灾难性遗忘问题，比较了不同参数共享策略对52种语言的影响，发现非拉丁脚本语言更容易遗忘。


<details>
  <summary>Details</summary>
Motivation: 传统的多语言处理方法难以模拟真实场景，易导致灾难性遗忘。研究旨在通过参数共享策略（如LoRA适配器）评估如何减轻遗忘并保留已有知识，特别关注语言差异对表征学习的影响。

Method: 实验使用52种语言，采用不同秩的LoRA适配器，评估非共享、部分共享和全共享参数的效果。

Result: 非拉丁脚本语言更易受灾难性遗忘影响，而拉丁脚本语言能更有效实现跨语言迁移。

Conclusion: 参数共享策略（如适配器）可缓解遗忘，但语言脚本类型显著影响模型表现，拉丁脚本更利于知识迁移。

Abstract: Cross-lingual transfer in natural language processing (NLP) models enhances
multilingual performance by leveraging shared linguistic knowledge. However,
traditional methods that process all data simultaneously often fail to mimic
real-world scenarios, leading to challenges like catastrophic forgetting, where
fine-tuning on new tasks degrades performance on previously learned ones. Our
study explores this issue in multilingual contexts, focusing on linguistic
differences affecting representational learning rather than just model
parameters. We experiment with 52 languages using LoRA adapters of varying
ranks to evaluate non-shared, partially shared, and fully shared parameters.
Our aim is to see if parameter sharing through adapters can mitigate forgetting
while preserving prior knowledge. We find that languages using non-Latin
scripts are more susceptible to catastrophic forgetting, whereas those written
in Latin script facilitate more effective cross-lingual transfer.

</details>


### [13] [DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation](https://arxiv.org/abs/2504.20371)
*Zhibo Man,Yuanmeng Chen,Yujie Zhang,Yufeng Chen,Jinan Xu*

Main category: cs.CL

TL;DR: 论文评估了大语言模型（LLMs）在多领域翻译（MDT）中的歧义消除能力，提出了DMDTEval评估框架，通过构建测试集、多样化提示模板和设计评估指标，揭示了关键发现以推动LLMs歧义消除研究的进展。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在机器翻译中表现优异，但在多领域翻译（MDT）中歧义问题突出，缺乏系统性评估其歧义消除能力的方法。

Method: 提出DMDTEval框架，包括：(1)构建多领域歧义词标注的测试集，(2)设计多样化提示模板，(3)设计精确的歧义消除指标，并分析不同提示策略对LLMs的效果。

Result: 实验揭示了多项关键发现，为提升LLMs在多领域翻译中的歧义消除能力提供了方向。

Conclusion: DMDTEval框架为系统评估和改进LLMs在多领域翻译中的歧义消除能力奠定了基础，推动了相关研究的进一步发展。

Abstract: Currently, Large Language Models (LLMs) have achieved remarkable results in
machine translation. However, their performance in multi-domain translation
(MDT) is less satisfactory; the meanings of words can vary across different
domains, highlighting the significant ambiguity inherent in MDT. Therefore,
evaluating the disambiguation ability of LLMs in MDT remains an open problem.
To this end, we present an evaluation and analysis of LLMs on disambiguation in
multi-domain translation (DMDTEval), our systematic evaluation framework
consisting of three critical aspects: (1) we construct a translation test set
with multi-domain ambiguous word annotation, (2) we curate a diverse set of
disambiguation prompting templates, and (3) we design precise disambiguation
metrics, and study the efficacy of various prompting strategies on multiple
state-of-the-art LLMs. Our extensive experiments reveal a number of crucial
findings that we believe will pave the way and also facilitate further research
in the critical area of improving the disambiguation of LLMs.

</details>


### [14] [On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?](https://arxiv.org/abs/2504.20444)
*Mika Hämäläinen*

Main category: cs.CL

TL;DR: 研究通过Asch实验范式测试了三款商业LLM（ChatGPT、Gemini、Claude）的“首因效应”，发现不同模型在不同实验设置下对形容词顺序的偏好存在差异。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLM）是否像人类一样受“首因效应”影响，即形容词顺序（先正后负或先负后正）是否会影响其对候选人的偏好。

Method: 采用Asch（1946）实验设计，分两组实验：（1）同一提示中同时呈现两名候选人描述；（2）分开呈现。测试200对候选人，观察模型偏好。

Result: 实验1中，ChatGPT偏好先正后负的描述，Gemini无偏好，Claude拒绝选择；实验2中，ChatGPT和Claude多给出平等评分，否则偏好先负后正的描述，Gemini更倾向先负后正的描述。

Conclusion: LLM的偏好受形容词顺序影响，但模型间差异显著，部分行为与人类“首因效应”相反，需进一步研究其认知机制。

Abstract: We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and
Claude. We do this by repurposing the famous experiment Asch (1946) conducted
using human subjects. The experiment is simple, given two candidates with equal
descriptions which one is preferred if one description has positive adjectives
first before negative ones and another description has negative adjectives
followed by positive ones. We test this in two experiments. In one experiment,
LLMs are given both candidates simultaneously in the same prompt, and in
another experiment, LLMs are given both candidates separately. We test all the
models with 200 candidate pairs. We found that, in the first experiment,
ChatGPT preferred the candidate with positive adjectives listed first, while
Gemini preferred both equally often. Claude refused to make a choice. In the
second experiment, ChatGPT and Claude were most likely to rank both candidates
equally. In the case where they did not give an equal rating, both showed a
clear preference to a candidate that had negative adjectives listed first.
Gemini was most likely to prefer a candidate with negative adjectives listed
first.

</details>


### [15] [Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs](https://arxiv.org/abs/2504.20451)
*Daniel Lee,Harsh Sharma,Jieun Han,Sunny Jeong,Alice Oh,Vered Shwartz*

Main category: cs.CL

TL;DR: 比较了13个模型在英韩翻译中的表现，发现LLMs优于传统MT系统，但在需要文化适应的实体翻译上仍有不足。通过错误分类，指出了主要问题和自动评估指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决知识密集和实体丰富的文本在英韩翻译中需要超越字面转换的文化和语言细微差别问题。

Method: 评估了13个模型（LLMs和MT模型），结合自动指标和双语标注者的人工评估，构建了错误分类法。

Result: LLMs表现优于传统MT系统，但在实体翻译和文化适应方面存在困难，性能因实体类型和流行度而异。

Conclusion: 揭示了自动评估指标的不足，并希望推动未来在文化细腻的机器翻译方面的研究。

Abstract: Translating knowledge-intensive and entity-rich text between English and
Korean requires transcreation to preserve language-specific and cultural
nuances beyond literal, phonetic or word-for-word conversion. We evaluate 13
models (LLMs and MT models) using automatic metrics and human assessment by
bilingual annotators. Our findings show LLMs outperform traditional MT systems
but struggle with entity translation requiring cultural adaptation. By
constructing an error taxonomy, we identify incorrect responses and entity name
errors as key issues, with performance varying by entity type and popularity
level. This work exposes gaps in automatic evaluation metrics and hope to
enable future work in completing culturally-nuanced machine translation.

</details>


### [16] [Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models](https://arxiv.org/abs/2504.20469)
*Enfa Fane,Mihai Surdeanu,Eduardo Blanco,Steven R. Corman*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）在零样本条件下对新闻叙事中实体框架角色分类的能力，通过分层方法和优化输入上下文与提示策略，取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估LLMs在媒体叙事框架角色分类中的零样本能力，以理解其对事件社会认知的影响。

Method: 采用分层方法，先识别宽泛角色再细化角色，并系统测试输入上下文、提示策略及任务分解的效果。

Result: 分层方法优于单步分类，主角色准确率达89.4%，精确匹配率为34.5%。

Conclusion: 研究突出了针对子任务优化提示设计和输入上下文对提升LLM在实体框架任务中表现的重要性。

Abstract: Understanding how news narratives frame entities is crucial for studying
media's impact on societal perceptions of events. In this paper, we evaluate
the zero-shot capabilities of large language models (LLMs) in classifying
framing roles. Through systematic experimentation, we assess the effects of
input context, prompting strategies, and task decomposition. Our findings show
that a hierarchical approach of first identifying broad roles and then
fine-grained roles, outperforms single-step classification. We also demonstrate
that optimal input contexts and prompts vary across task levels, highlighting
the need for subtask-specific strategies. We achieve a Main Role Accuracy of
89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of our
approach. Our findings emphasize the importance of tailored prompt design and
input context optimization for improving LLM performance in entity framing.

</details>


### [17] [Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training](https://arxiv.org/abs/2504.20484)
*Linjuan Wu,Haoran Wei,Huan Lin,Tianhao Li,Baosong Yang,Weiming Lu*

Main category: cs.CL

TL;DR: 论文提出CrossIC-PT方法，通过语义相关的双语文本进行上下文预训练，提升跨语言迁移能力，实验证明在多语种任务中显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言迁移方法受限于平行语料资源，语言和领域覆盖有限，需更简单且可扩展的解决方案。

Method: 利用语义相关的双语维基文档，通过分段和滑动窗口机制构建上下文训练样本，并结合语义检索扩展数据来源。

Result: 在Llama-3.1-8B等三个模型上，六种目标语言的性能分别提升3.79%、3.99%、1.95%，数据增强后进一步提升。

Conclusion: CrossIC-PT是一种有效且可扩展的跨语言预训练方法，显著提升多语言模型性能。

Abstract: Large language models (LLMs) exhibit remarkable multilingual capabilities
despite English-dominated pre-training, attributed to cross-lingual mechanisms
during pre-training. Existing methods for enhancing cross-lingual transfer
remain constrained by parallel resources, suffering from limited linguistic and
domain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT),
a simple and scalable approach that enhances cross-lingual transfer by
leveraging semantically related bilingual texts via simple next-word
prediction. We construct CrossIC-PT samples by interleaving semantic-related
bilingual Wikipedia documents into a single context window. To access window
size constraints, we implement a systematic segmentation policy to split long
bilingual document pairs into chunks while adjusting the sliding window
mechanism to preserve contextual coherence. We further extend data availability
through a semantic retrieval framework to construct CrossIC-PT samples from
web-crawled corpus. Experimental results demonstrate that CrossIC-PT improves
multilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and
Qwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%,
3.99%, and 1.95%, respectively, with additional improvements after data
augmentation.

</details>


### [18] [UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation](https://arxiv.org/abs/2504.20500)
*Huimin Lu,Masaru Isonuma,Junichiro Mori,Ichiro Sakata*

Main category: cs.CL

TL;DR: UniDetox是一种通用方法，用于减少多种大型语言模型的毒性，无需针对每个模型单独调参，且能降低政治偏见。


<details>
  <summary>Details</summary>
Motivation: 现有毒性去除方法通常针对特定模型或模型家族，且需权衡去毒效果与语言建模性能的调参，UniDetox旨在提供一种通用解决方案。

Method: 通过对比解码的新型数据集蒸馏技术，生成合成文本形式的去毒表示，对任何LLM进行微调实现通用去毒。

Result: 实验表明，从GPT-2蒸馏的去毒文本可有效去毒OPT、Falcon和LLaMA-2等更大模型；单一超参数配置适用于不同模型，且能减少政治偏见内容。

Conclusion: UniDetox提供了一种高效、通用的LLM去毒方法，无需单独调参，同时揭示了有效去毒的关键属性。

Abstract: We present UniDetox, a universally applicable method designed to mitigate
toxicity across various large language models (LLMs). Previous detoxification
methods are typically model-specific, addressing only individual models or
model families, and require careful hyperparameter tuning due to the trade-off
between detoxification efficacy and language modeling performance. In contrast,
UniDetox provides a detoxification technique that can be universally applied to
a wide range of LLMs without the need for separate model-specific tuning.
Specifically, we propose a novel and efficient dataset distillation technique
for detoxification using contrastive decoding. This approach distills
detoxifying representations in the form of synthetic text data, enabling
universal detoxification of any LLM through fine-tuning with the distilled
text. Our experiments demonstrate that the detoxifying text distilled from
GPT-2 can effectively detoxify larger models, including OPT, Falcon, and
LLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter
tuning for each model, as a single hyperparameter configuration can be
seamlessly applied across different models. Additionally, analysis of the
detoxifying text reveals a reduction in politically biased content, providing
insights into the attributes necessary for effective detoxification of LLMs.

</details>


### [19] [Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records](https://arxiv.org/abs/2504.20547)
*Jesus Lovon,Thouria Ben-Haddi,Jules Di Scala,Jose G. Moreno,Lynda Tamine*

Main category: cs.CL

TL;DR: 论文通过整合MIMIC-IV数据到Hugging Face库并研究EHR表格数据转文本的方法，展示了文本模型在医学任务中的潜力，但发现零样本LLMs表现不佳。


<details>
  <summary>Details</summary>
Motivation: 缺乏医学领域标准化评测基准阻碍了自然语言模型在健康相关任务中的应用，因此研究利用MIMIC-IV基准来解决这一问题。

Method: 整合MIMIC-IV数据到Hugging Face库，研究EHR表格数据转文本的模板方法，并通过微调和零样本LLMs实验比较性能。

Result: 微调文本模型在患者死亡率任务上表现优于零样本LLMs，与表格分类器竞争力相当。

Conclusion: 文本方法在医学领域有潜力，但需进一步改进零样本LLMs的表现。

Abstract: The lack of standardized evaluation benchmarks in the medical domain for text
inputs can be a barrier to widely adopting and leveraging the potential of
natural language models for health-related downstream tasks. This paper
revisited an openly available MIMIC-IV benchmark for electronic health records
(EHRs) to address this issue. First, we integrate the MIMIC-IV data within the
Hugging Face datasets library to allow an easy share and use of this
collection. Second, we investigate the application of templates to convert EHR
tabular data to text. Experiments using fine-tuned and zero-shot LLMs on the
mortality of patients task show that fine-tuned text-based models are
competitive against robust tabular classifiers. In contrast, zero-shot LLMs
struggle to leverage EHR representations. This study underlines the potential
of text-based approaches in the medical field and highlights areas for further
improvement.

</details>


### [20] [BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters](https://arxiv.org/abs/2504.20552)
*Baz Roland,Kristina Malyseva,Anna Pappa,Tristan Cazenave*

Main category: cs.CL

TL;DR: BrAIcht是一个AI对话代理，通过微调德国LeoLM大型语言模型，模仿德国剧作家布莱希特的独特对话风格，采用QLoRA技术优化内存使用，实验结果表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了创造能够模仿著名德国剧作家布莱希特风格的对话代理，并解决大型语言模型在有限内存条件下的微调问题。

Method: 使用德国LeoLM（70亿参数）和修改版的Llama2模型，结合布莱希特的29部戏剧和其他907部风格相似的德国戏剧数据集，采用QLoRA技术进行参数高效的微调。

Result: 基于BLEU分数和困惑度的结果表明，BrAIcht在生成布莱希特风格的对话中表现出色。

Conclusion: BrAIcht成功模仿了布莱希特的对话风格，证明了QLoRA技术在有限资源下的有效性。

Abstract: This project introduces BrAIcht, an AI conversational agent that creates
dialogues in the distinctive style of the famous German playwright Bertolt
Brecht. BrAIcht is fine-tuned using German LeoLM, a large language model with 7
billion parameters and a modified version of the base Llama2 suitable for
German language tasks. For fine-tuning, 29 plays of Bertolt Brecht and 907 of
other German plays that are stylistically similar to Bertolt Brecht are used to
form a more di-erse dataset. Due to the limited memory capacity, a
parameterefficient fine-tuning technique called QLoRA is implemented to train
the large language model. The results, based on BLEU score and perplexity, show
very promising performance of BrAIcht in generating dialogues in the style of
Bertolt Brecht.

</details>


### [21] [ClonEval: An Open Voice Cloning Benchmark](https://arxiv.org/abs/2504.20581)
*Iwona Christop,Tomasz Kuczyński,Marek Kubis*

Main category: cs.CL

TL;DR: 该论文提出了一个用于语音克隆文本转语音模型的新基准，包括评估协议、开源库和排行榜。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一的评估标准来比较不同语音克隆模型的性能，因此需要一个标准化工具来推动研究进展。

Method: 设计了一个评估协议，开发了开源库用于性能评测，并建立了排行榜以展示结果。

Result: 提供了详细的评估流程和软件库使用说明，展示了模型性能的排行榜。

Conclusion: 该基准为语音克隆领域提供了标准化评估工具，有助于进一步研究和模型优化。

Abstract: We present a novel benchmark for voice cloning text-to-speech models. The
benchmark consists of an evaluation protocol, an open-source library for
assessing the performance of voice cloning models, and an accompanying
leaderboard. The paper discusses design considerations and presents a detailed
description of the evaluation procedure. The usage of the software library is
explained, along with the organization of results on the leaderboard.

</details>


### [22] [TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models](https://arxiv.org/abs/2504.20605)
*Mihai Nadas,Laura Diosan,Andrei Piscoran,Andreea Tomescu*

Main category: cs.CL

TL;DR: 论文提出了TF1-EN-3M数据集，包含300万条由8B参数以内的指令调优模型生成的道德故事，采用六槽模板结构，并通过混合评估方法验证质量，展示了小模型在大规模道德叙事任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现代NLP缺乏一个结构化的、结合叙事与明确道德教育的语料库，研究旨在填补这一空白，同时证明小模型也能高效生成高质量故事。

Method: 使用组合式提示引擎生成六槽模板故事，通过基于GPT的评价模型和无参考指标评估语法、创意、道德明确性及多样性。

Result: 8B参数的Llama-3变体在质量与速度上表现最佳，单张消费级GPU即可低成本生成高分故事（约13.5美分/千条）。

Conclusion: TF1-EN-3M为指令遵循、叙事智能等研究提供了资源，证明大规模道德叙事无需依赖大模型。

Abstract: Moral stories are a time-tested vehicle for transmitting values, yet modern
NLP lacks a large, structured corpus that couples coherent narratives with
explicit ethical lessons. We close this gap with TF1-EN-3M, the first open
dataset of three million English-language fables generated exclusively by
instruction-tuned models no larger than 8B parameters. Each story follows a
six-slot scaffold (character -> trait -> setting -> conflict -> resolution ->
moral), produced through a combinatorial prompt engine that guarantees genre
fidelity while covering a broad thematic space.
  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores
grammar, creativity, moral clarity, and template adherence with (ii)
reference-free diversity and readability metrics. Among ten open-weight
candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed
trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)
at approximately 13.5 cents per 1,000 fables.
  We release the dataset, generation code, evaluation scripts, and full
metadata under a permissive license, enabling exact reproducibility and cost
benchmarking. TF1-EN-3M opens avenues for research in instruction following,
narrative intelligence, value alignment, and child-friendly educational AI,
demonstrating that large-scale moral storytelling no longer requires
proprietary giant models.

</details>


### [23] [WenyanGPT: A Large Language Model for Classical Chinese Tasks](https://arxiv.org/abs/2504.20609)
*Xinyu Yao,Mengdi Wang,Bo Chen,Xiaobing Zhao*

Main category: cs.CL

TL;DR: 该论文针对古文处理的不足，提出了一个专为古文设计的语言模型WenyanGPT，并通过WenyanBENCH评估数据集证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 由于现有自然语言处理模型主要针对现代汉语优化，在古文处理上表现不佳，论文旨在解决这一问题。

Method: 通过在LLaMA3-8B-Chinese模型上继续预训练和指令微调，构建了古文专用模型WenyanGPT，并开发了评估数据集WenyanBENCH。

Result: 实验表明，WenyanGPT在古文任务上显著优于现有先进大语言模型。

Conclusion: 论文通过公开模型训练数据、微调数据和评估数据集，推动了古文处理领域的进一步研究与发展。

Abstract: Classical Chinese, as the core carrier of Chinese culture, plays a crucial
role in the inheritance and study of ancient literature. However, existing
natural language processing models primarily optimize for Modern Chinese,
resulting in inadequate performance on Classical Chinese. This paper presents a
comprehensive solution for Classical Chinese language processing. By continuing
pre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we
construct a large language model, WenyanGPT, which is specifically designed for
Classical Chinese tasks. Additionally, we develop an evaluation benchmark
dataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that
WenyanGPT significantly outperforms current advanced LLMs in various Classical
Chinese tasks. We make the model's training data, instruction fine-tuning
data\footnote, and evaluation benchmark dataset publicly available to promote
further research and development in the field of Classical Chinese processing.

</details>


### [24] [Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations](https://arxiv.org/abs/2504.20643)
*Moran Mizrahi,Chen Shani,Gabriel Stanovsky,Dan Jurafsky,Dafna Shahaf*

Main category: cs.CL

TL;DR: 提出了一种结合大语言模型（LLMs）与结构化表示及认知启发的创新方法，生成更具创造性和多样性的想法。实验证明该方法在烹饪领域的创意生成效果优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多任务中表现出色，但在创造力方面仍有不足。本文旨在通过结构化表示和认知启发的方法，提升模型的创意生成能力。

Method: 提出了名为DishCOVER的模型，该方法结合LLMs与结构化表示，通过重组现有想法的结构化表示来探索更抽象的创意空间。

Result: 实验显示，DishCOVER生成的菜谱比GPT-4o更具多样性和新颖性，且专家评估表明其输出在创意性上显著优于GPT-4o。

Conclusion: 本研究展示了结构化表示在提升AI创造力方面的潜力，为未来AI创意研究提供了新的方向。

Abstract: Large Language Models (LLMs) excel at countless tasks, yet struggle with
creativity. In this paper, we introduce a novel approach that couples LLMs with
structured representations and cognitively inspired manipulations to generate
more creative and diverse ideas. Our notion of creativity goes beyond
superficial token-level variations; rather, we explicitly recombine structured
representations of existing ideas, allowing our algorithm to effectively
explore the more abstract landscape of ideas. We demonstrate our approach in
the culinary domain with DishCOVER, a model that generates creative recipes.
Experiments comparing our model's results to those of GPT-4o show greater
diversity. Domain expert evaluations reveal that our outputs, which are mostly
coherent and feasible culinary creations, significantly surpass GPT-4o in terms
of novelty, thus outperforming it in creative generation. We hope our work
inspires further research into structured creativity in AI.

</details>


### [25] [A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages](https://arxiv.org/abs/2504.20668)
*Ivan Vykopal,Martin Hyben,Robert Moro,Michal Gregor,Jakub Simko*

Main category: cs.CL

TL;DR: 该研究提出了一种利用大型语言模型（LLMs）检索已核查信息并评估相关性的方法，以减少重复核查工作，提升事实核查效率。


<details>
  <summary>Details</summary>
Motivation: 在线虚假信息的快速传播对事实核查人员提出了高效核查的需求，但现有流程中存在大量冗余核查工作，增加了负担并延迟了对新信息的响应。

Method: 采用大型语言模型（LLMs）对已核查信息进行过滤，去除无关内容，并生成简洁摘要与解释，帮助核查人员快速判断某声明是否已被核查过。

Result: 实验证明，LLMs能有效过滤无关核查内容，减少工作量并优化核查流程。自动与人工评估均验证了该方法的有效性。

Conclusion: 基于LLMs的方法能显著提升事实核查效率，减轻核查人员负担，并建议未来进一步优化模型与工具的实际应用。

Abstract: Online disinformation poses a global challenge, placing significant demands
on fact-checkers who must verify claims efficiently to prevent the spread of
false information. A major issue in this process is the redundant verification
of already fact-checked claims, which increases workload and delays responses
to newly emerging claims. This research introduces an approach that retrieves
previously fact-checked claims, evaluates their relevance to a given input, and
provides supplementary information to support fact-checkers. Our method employs
large language models (LLMs) to filter irrelevant fact-checks and generate
concise summaries and explanations, enabling fact-checkers to faster assess
whether a claim has been verified before. In addition, we evaluate our approach
through both automatic and human assessments, where humans interact with the
developed tool to review its effectiveness. Our results demonstrate that LLMs
are able to filter out many irrelevant fact-checks and, therefore, reduce
effort and streamline the fact-checking process.

</details>


### [26] [Non-native Children's Automatic Speech Assessment Challenge (NOCASA)](https://arxiv.org/abs/2504.20678)
*Yaroslav Getman,Tamás Grósz,Mikko Kurimo,Giampiero Salvi*

Main category: cs.CL

TL;DR: NOCASA竞赛旨在开发用于评估非母语儿童单词发音的系统，使用新数据集TeflonNorL2，并提供了两种基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决非母语儿童发音评估的数据稀缺和类别不平衡问题，推动发音训练应用的发展。

Method: 提供了包含10,334个录音的数据集，并使用SVM和基于wav2vec 2.0的多任务模型作为基线方法。

Result: 多任务wav2vec 2.0模型在测试集上表现最佳，UAR为36.37%。

Conclusion: NOCASA竞赛为发音评估提供了新的数据集和基线模型，展示了深度学习方法的潜力。

Abstract: This paper presents the "Non-native Children's Automatic Speech Assessment"
(NOCASA) - a data competition part of the IEEE MLSP 2025 conference. NOCASA
challenges participants to develop new systems that can assess single-word
pronunciations of young second language (L2) learners as part of a gamified
pronunciation training app. To achieve this, several issues must be addressed,
most notably the limited nature of available training data and the highly
unbalanced distribution among the pronunciation level categories. To expedite
the development, we provide a pseudo-anonymized training data (TeflonNorL2),
containing 10,334 recordings from 44 speakers attempting to pronounce 205
distinct Norwegian words, human-rated on a 1 to 5 scale (number of stars that
should be given in the game). In addition to the data, two already trained
systems are released as official baselines: an SVM classifier trained on the
ComParE_16 acoustic feature set and a multi-task wav2vec 2.0 model. The latter
achieves the best performance on the challenge test set, with an unweighted
average recall (UAR) of 36.37%.

</details>


### [27] [Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?](https://arxiv.org/abs/2504.20679)
*Wing Yan Li,Zeqiang Wang,Jon Johnson,Suparna De*

Main category: cs.CL

TL;DR: 本文提出了一种新信息检索任务，旨在从纵向社会科学调查数据中检测语义等价的问题，用于协调长期研究。


<details>
  <summary>Details</summary>
Motivation: 解决长期调查中理论构念（如概念/子概念）不一致、问题与回答选项之间的词汇和结构演化带来的挑战。

Method: 探索了无监督方法，包括概率模型、语言模型线性探测、预训练神经IR网络的性能。

Result: 专用神经IR模型表现最佳，其他方法相近；重排序提升有限（F1最多+0.07）。模型对高词汇重叠问题敏感性低。

Conclusion: 研究推动了社会科学纵向研究的协调方法，但需提升子概念错配时的灵敏度。

Abstract: Automated detection of semantically equivalent questions in longitudinal
social science surveys is crucial for long-term studies informing empirical
research in the social, economic, and health sciences. Retrieving equivalent
questions faces dual challenges: inconsistent representation of theoretical
constructs (i.e. concept/sub-concept) across studies as well as between
question and response options, and the evolution of vocabulary and structure in
longitudinal text. To address these challenges, our multi-disciplinary
collaboration of computer scientists and survey specialists presents a new
information retrieval (IR) task of identifying concept (e.g. Housing, Job,
etc.) equivalence across question and response options to harmonise
longitudinal population studies. This paper investigates multiple unsupervised
approaches on a survey dataset spanning 1946-2020, including probabilistic
models, linear probing of language models, and pre-trained neural networks
specialised for IR. We show that IR-specialised neural models achieve the
highest overall performance with other approaches performing comparably.
Additionally, the re-ranking of the probabilistic model's results with neural
models only introduces modest improvements of 0.07 at most in F1-score.
Qualitative post-hoc evaluation by survey specialists shows that models
generally have a low sensitivity to questions with high lexical overlap,
particularly in cases where sub-concepts are mismatched. Altogether, our
analysis serves to further research on harmonising longitudinal studies in
social science.

</details>


### [28] [Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?](https://arxiv.org/abs/2504.20699)
*Evangelia Gogoulou,Shorouq Zahra,Liane Guillou,Luise Dürlich,Joakim Nivre*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLM）在翻译和改写任务中的幻觉检测能力，探讨了模型大小、指令调整和提示选择的影响，并发现NLI模型表现相当。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成内容时容易产生不符合逻辑或事实的内容（幻觉），因此研究其幻觉检测能力对提升模型可靠性至关重要。

Method: 采用HalluciGen任务框架，评估开源LLM在翻译和改写任务中的幻觉检测能力，分析模型大小、指令调整和提示选择的影响。

Result: 发现不同模型表现差异大但提示选择影响小，且NLI模型在幻觉检测任务中表现接近LLM。

Conclusion: LLM并非唯一可行的幻觉检测方案，NLI模型是有效替代选择。

Abstract: A frequently observed problem with LLMs is their tendency to generate output
that is nonsensical, illogical, or factually incorrect, often referred to
broadly as hallucination. Building on the recently proposed HalluciGen task for
hallucination detection and generation, we evaluate a suite of open-access LLMs
on their ability to detect intrinsic hallucinations in two conditional
generation tasks: translation and paraphrasing. We study how model performance
varies across tasks and language and we investigate the impact of model size,
instruction tuning, and prompt choice. We find that performance varies across
models but is consistent across prompts. Finally, we find that NLI models
perform comparably well, suggesting that LLM-based detectors are not the only
viable option for this specific task.

</details>


### [29] [BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification](https://arxiv.org/abs/2504.20703)
*Foteini Papadopoulou,Osman Mutlu,Neris Özen,Bas H. M. van der Velden,Iris Hendrickx,Ali Hürriyetoğlu*

Main category: cs.CL

TL;DR: 本文介绍了基于SemEval-2025 Task 9开发的系统，通过文本增强技术改善少数类别的分类性能，实验表明BERT模型在细粒度分类中有显著提升。


<details>
  <summary>Details</summary>
Motivation: 针对食品召回报告中危害和产品的分类任务，解决少数类别分类性能差的问题。

Method: 采用三种词级数据增强技术（同义词替换、随机词交换、上下文词插入），并在多种模型上进行测试。

Result: BERT模型在细粒度分类中表现最佳，上下文词插入技术使少数危害类别的预测准确率提升6%。

Conclusion: 针对少数类别的定向增强可提升Transformer模型的性能。

Abstract: This paper presents our system developed for the SemEval-2025 Task 9: The
Food Hazard Detection Challenge. The shared task's objective is to evaluate
explainable classification systems for classifying hazards and products in two
levels of granularity from food recall incident reports. In this work, we
propose text augmentation techniques as a way to improve poor performance on
minority classes and compare their effect for each category on various
transformer and machine learning models. We explore three word-level data
augmentation techniques, namely synonym replacement, random word swapping, and
contextual word insertion. The results show that transformer models tend to
have a better overall performance. None of the three augmentation techniques
consistently improved overall performance for classifying hazards and products.
We observed a statistically significant improvement (P < 0.05) in the
fine-grained categories when using the BERT model to compare the baseline with
each augmented model. Compared to the baseline, the contextual words insertion
augmentation improved the accuracy of predictions for the minority hazard
classes by 6%. This suggests that targeted augmentation of minority classes can
improve the performance of transformer models.

</details>


### [30] [Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think](https://arxiv.org/abs/2504.20708)
*Hasan Abed Al Kader Hammoud,Hani Itani,Bernard Ghanem*

Main category: cs.CL

TL;DR: 论文提出了一种新方法，通过分析LLMs的中间推理步骤（子思想）而非仅依赖最终答案，显著提升了复杂问题解决的准确性。


<details>
  <summary>Details</summary>
Motivation: 挑战仅依赖最终答案评估LLMs的传统做法，探讨中间推理步骤对模型结论的影响。

Method: 将推理轨迹分段为子思想，生成多种可能的答案，并选择最常见的结果（众数）作为最终答案。

Result: 在数学推理数据集上实现了高达13%和10%的准确率提升。

Conclusion: 中间推理步骤分析能有效提升模型表现，并为识别不可靠答案提供新思路。

Abstract: Large Language Models (LLMs) leverage step-by-step reasoning to solve complex
problems. Standard evaluation practice involves generating a complete reasoning
trace and assessing the correctness of the final answer presented at its
conclusion. In this paper, we challenge the reliance on the final answer by
posing the following two questions: Does the final answer reliably represent
the model's optimal conclusion? Can alternative reasoning paths yield different
results? To answer these questions, we analyze intermediate reasoning steps,
termed subthoughts, and propose a method based on our findings. Our approach
involves segmenting a reasoning trace into sequential subthoughts based on
linguistic cues. We start by prompting the model to generate continuations from
the end-point of each intermediate subthought. We extract a potential answer
from every completed continuation originating from different subthoughts. We
find that aggregating these answers by selecting the most frequent one (the
mode) often yields significantly higher accuracy compared to relying solely on
the answer derived from the original complete trace. Analyzing the consistency
among the answers derived from different subthoughts reveals characteristics
that correlate with the model's confidence and correctness, suggesting
potential for identifying less reliable answers. Our experiments across various
LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025)
show consistent accuracy improvements, with gains reaching up to 13\% and 10\%
respectively. Implementation is available at:
https://github.com/hammoudhasan/SubthoughtReasoner.

</details>


### [31] [UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities](https://arxiv.org/abs/2504.20734)
*Woongyeong Yeo,Kangsan Kim,Soyeong Jeong,Jinheon Baek,Sung Ju Hwang*

Main category: cs.CL

TL;DR: 论文提出UniversalRAG框架，通过多模态感知路由机制动态选择最佳知识源，解决传统RAG方法仅支持单一模态的问题，并在多模态和不同粒度检索中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法局限于单一模态，无法满足现实查询的多样性需求。因此，需要一种能整合多模态、多粒度知识源的框架。

Method: 提出UniversalRAG框架，采用模态感知路由机制动态选择知识源，并按模态和粒度组织知识库以实现精准检索。

Result: 在8个多模态基准测试中，UniversalRAG的性能优于单一模态和统一知识源的基线方法。

Conclusion: UniversalRAG通过动态路由和多粒度组织，显著提升了多模态检索的准确性和适应性。

Abstract: Retrieval-Augmented Generation (RAG) has shown substantial promise in
improving factual accuracy by grounding model responses with external knowledge
relevant to queries. However, most existing RAG approaches are limited to a
text-only corpus, and while recent efforts have extended RAG to other
modalities such as images and videos, they typically operate over a single
modality-specific corpus. In contrast, real-world queries vary widely in the
type of knowledge they require, which a single type of knowledge source cannot
address. To address this, we introduce UniversalRAG, a novel RAG framework
designed to retrieve and integrate knowledge from heterogeneous sources with
diverse modalities and granularities. Specifically, motivated by the
observation that forcing all modalities into a unified representation space
derived from a single combined corpus causes a modality gap, where the
retrieval tends to favor items from the same modality as the query, we propose
a modality-aware routing mechanism that dynamically identifies the most
appropriate modality-specific corpus and performs targeted retrieval within it.
Also, beyond modality, we organize each modality into multiple granularity
levels, enabling fine-tuned retrieval tailored to the complexity and scope of
the query. We validate UniversalRAG on 8 benchmarks spanning multiple
modalities, showing its superiority over modality-specific and unified
baselines.

</details>


### [32] [Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers](https://arxiv.org/abs/2504.20752)
*Roman Abramov,Felix Steinbauer,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 该论文将grokking技术扩展到现实世界的事实数据，通过合成数据增强知识图谱，显著提升了多跳推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在多步事实推理中的不足，特别是在知识稀疏的情况下，探索grokking技术在真实数据上的应用。

Method: 通过设计合成数据增强现有知识图谱，提高推理事实与原子事实的比例$\\phi_r$，以触发模型的泛化能力。

Result: 在2WikiMultiHopQA基准测试中达到95-100%的准确率，超越了现有基线并匹配了最先进的成果。

Conclusion: grokking-based数据增强能够释放Transformer的隐式多跳推理能力，为大规模语言模型的稳健和可解释推理提供了新思路。

Abstract: Transformers have achieved great success in numerous NLP tasks but continue
to exhibit notable gaps in multi-step factual reasoning, especially when
real-world knowledge is sparse. Recent advances in grokking have demonstrated
that neural networks can transition from memorizing to perfectly generalizing
once they detect underlying logical patterns - yet these studies have primarily
used small, synthetic tasks. In this paper, for the first time, we extend
grokking to real-world factual data and address the challenge of dataset
sparsity by augmenting existing knowledge graphs with carefully designed
synthetic data to raise the ratio $\phi_r$ of inferred facts to atomic facts
above the threshold required for grokking. Surprisingly, we find that even
factually incorrect synthetic data can strengthen emergent reasoning circuits
rather than degrade accuracy, as it forces the model to rely on relational
structure rather than memorization. When evaluated on multi-hop reasoning
benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -
substantially improving over strong baselines and matching or exceeding current
state-of-the-art results. We further provide an in-depth analysis of how
increasing $\phi_r$ drives the formation of generalizing circuits inside
Transformers. Our findings suggest that grokking-based data augmentation can
unlock implicit multi-hop reasoning capabilities, opening the door to more
robust and interpretable factual reasoning in large-scale language models.

</details>


### [33] [Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption](https://arxiv.org/abs/2504.20769)
*Wenxiao Wang,Parsa Hosseini,Soheil Feizi*

Main category: cs.CL

TL;DR: 论文探讨了如何利用思维链提示提升大语言模型在非推理任务中的鲁棒性，提出了一种简单有效的防御性思维链方法，显著提高了模型在参考数据被污染时的表现。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在参考数据被污染时的脆弱性，并探索如何利用思维链提示提升其鲁棒性。

Method: 提出防御性思维链（chain-of-defensive-thought）方法，仅需少量带有结构化防御推理的示例即可显著提升模型抗干扰能力。

Result: 实验表明，该方法在自然问题任务中将GPT-4o的准确率从参考污染后的3%提升至50%。

Conclusion: 防御性思维链是一种简单且普适性强的方法，能显著提升大语言模型在对抗性环境中的鲁棒性。

Abstract: Chain-of-thought prompting has demonstrated great success in facilitating the
reasoning abilities of large language models. In this work, we explore how
these enhanced reasoning abilities can be exploited to improve the robustness
of large language models in tasks that are not necessarily reasoning-focused.
In particular, we show how a wide range of large language models exhibit
significantly improved robustness against reference corruption using a simple
method called chain-of-defensive-thought, where only a few exemplars with
structured and defensive reasoning are provided as demonstrations. Empirically,
the improvements can be astounding, especially given the simplicity and
applicability of the method. For example, in the Natural Questions task, the
accuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting
when 1 out of 10 references provided is corrupted with prompt injection
attacks. In contrast, GPT-4o using chain-of-defensive-thought prompting
maintains an accuracy of 50%.

</details>


### [34] [Turing Machine Evaluation for Large Language Model](https://arxiv.org/abs/2504.20771)
*Haitao Wu,Zongbo Han,Huaxi Huang,Changqing Zhang*

Main category: cs.CL

TL;DR: 该论文提出一个基于通用图灵机（UTM）的评估框架TMBench，用于系统评估大语言模型的（LLMs）核心计算推理能力，旨在衡量模型在理解规则和执行逻辑计算操作上的准确性。结果表明，TMBench上的表现与其他公认的推理基准强相关。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的快速发展和广泛应用，对其核心计算推理能力（即准确理解规则并执行逻辑计算操作的能力）的评估变得尤为重要。

Method: 提出基于通用图灵机（UTM）模拟的评估框架TMBench，要求LLMs在多步计算中严格遵循指令并跟踪动态状态（如磁带内容和读写头位置），实现了标准化评估。

Result: TMBench表现与其他推理基准的皮尔逊相关系数达0.73，表明计算推理能力是衡量LLMs深层能力的重要维度。

Conclusion: TMBench为评估LLMs的计算推理能力提供了一个可扩展、可调难度的标准化基准，证实该能力与实际推理性能强相关。

Abstract: With the rapid development and widespread application of Large Language
Models (LLMs), rigorous evaluation has become particularly crucial. This
research adopts a novel perspective, focusing on evaluating the core
computational reasoning ability of LLMs, defined as the capacity of model to
accurately understand rules, and execute logically computing operations. This
capability assesses the reliability of LLMs as precise executors, and is
critical to advanced tasks such as complex code generation and multi-step
problem-solving. We propose an evaluation framework based on Universal Turing
Machine (UTM) simulation. This framework requires LLMs to strictly follow
instructions and track dynamic states, such as tape content and read/write head
position, during multi-step computations. To enable standardized evaluation, we
developed TMBench, a benchmark for systematically studying the computational
reasoning capabilities of LLMs. TMBench provides several key advantages,
including knowledge-agnostic evaluation, adjustable difficulty, foundational
coverage through Turing machine encoding, and unlimited capacity for instance
generation, ensuring scalability as models continue to evolve. We find that
model performance on TMBench correlates strongly with performance on other
recognized reasoning benchmarks (Pearson correlation coefficient is 0.73),
clearly demonstrating that computational reasoning is a significant dimension
for measuring the deep capabilities of LLMs. Code and data are available at
https://github.com/HaitaoWuTJU/Turing-Machine-Bench.

</details>


### [35] [Universal language model with the intervention of quantum theory](https://arxiv.org/abs/2504.20839)
*D. -F. Qin*

Main category: cs.CL

TL;DR: 这篇论文探讨了基于量子力学理论的语言建模方法，通过将量子力学引入语言的符号-意义对，构建自然语言的表示模型。它还尝试用量子统计等理论改进词嵌入技术，并指出量子建模的可行性。


<details>
  <summary>Details</summary>
Motivation: 希望通过量子力学的数学框架解释和改进现有的语言建模技术，尤其是词嵌入方法。同时探索自然语言的量子属性及其物理本质。

Method: 将量子力学理论应用于符号-意义对的建模，利用量子统计和相关理论研究语言的数学表示、自然演化及统计特性，并通过实验代码验证。

Result: 提出了使用量子理论建模自然语言的可行性，展示了该方法对改进词嵌入和构建生成模型的潜力。

Conclusion: 量子力学为语言建模提供了新的理论框架，未来可能在量子计算机上有进一步应用。

Abstract: This paper examines language modeling based on the theory of quantum
mechanics. It focuses on the introduction of quantum mechanics into the
symbol-meaning pairs of language in order to build a representation model of
natural language. At the same time, it is realized that word embedding, which
is widely used as a basic technique for statistical language modeling, can be
explained and improved by the mathematical framework of quantum mechanics. On
this basis, this paper continues to try to use quantum statistics and other
related theories to study the mathematical representation, natural evolution
and statistical properties of natural language. It is also assumed that the
source of such quantum properties is the physicality of information. The
feasibility of using quantum theory to model natural language is pointed out
through the construction of a experimental code. The paper discusses, in terms
of applications, the possible help of the theory in constructing generative
models that are popular nowadays. A preliminary discussion of future
applications of the theory to quantum computers is also presented.

</details>


### [36] [JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry](https://arxiv.org/abs/2504.20849)
*Anum Afzal,Alexandre Mercier,Florian Matthes*

Main category: cs.CL

TL;DR: 论文探讨了LLM-based数据到文本技术如何生成高质量、多样化的营销文本，引入了JaccDiv评估多样性，适用领域广泛。


<details>
  <summary>Details</summary>
Motivation: 传统生成方法容易陷入重复模式，导致文本单调，因此研究如何利用LLMs（如T5、GPT系列、LLaMa2）生成多样化、高质量的营销内容。

Method: 结合微调、少量样本和零样本学习的方法，利用T5、GPT-3.5、GPT-4和LLaMa2生成营销文本，并提出了JaccDiv评估指标。

Result: 成功建立了多样化营销文本生成的基准，并展示了JaccDiv在量化文本多样性方面的有效性。

Conclusion: 该研究不仅适用于音乐行业，对需要自动化内容生成的多个领域都有广泛的应用价值。

Abstract: Online platforms are increasingly interested in using Data-to-Text
technologies to generate content and help their users. Unfortunately,
traditional generative methods often fall into repetitive patterns, resulting
in monotonous galleries of texts after only a few iterations. In this paper, we
investigate LLM-based data-to-text approaches to automatically generate
marketing texts that are of sufficient quality and diverse enough for broad
adoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in
conjunction with fine-tuning, few-shot, and zero-shot approaches to set a
baseline for diverse marketing texts. We also introduce a metric JaccDiv to
evaluate the diversity of a set of texts. This research extends its relevance
beyond the music industry, proving beneficial in various fields where
repetitive automated content generation is prevalent.

</details>


### [37] [DYNAMAX: Dynamic computing for Transformers and Mamba based architectures](https://arxiv.org/abs/2504.20922)
*Miguel Nogales,Matteo Gambella,Manuel Roveri*

Main category: cs.CL

TL;DR: DYNAMAX框架首次将Mamba架构的早期退出机制应用于解码器模型和Mamba模型，展示了Mamba作为早期退出分类器的多功能性，并在实验中验证了其在计算节省和性能平衡上的高效表现。


<details>
  <summary>Details</summary>
Motivation: 探索Mamba架构在早期退出机制中的潜力，以降低计算成本和延迟，特别是在解码器模型和Mamba模型中的应用。

Method: 提出DYNAMAX框架，将早期退出机制集成到Mamba架构中，并将其作为高效的分类器应用于基于Mamba和基于Transformer的大型语言模型。

Result: 实验表明，Mamba在计算节省和性能平衡方面表现出色，尤其在TruthfulQA、CoQA和TriviaQA数据集上验证了其高效性和适应性。

Conclusion: Mamba架构在动态计算范式中的潜力巨大，为嵌入式和资源受限环境中的高效推理开辟了新途径。

Abstract: Early exits (EEs) offer a promising approach to reducing computational costs
and latency by dynamically terminating inference once a satisfactory prediction
confidence on a data sample is achieved. Although many works integrate EEs into
encoder-only Transformers, their application to decoder-only architectures and,
more importantly, Mamba models, a novel family of state-space architectures in
the LLM realm, remains insufficiently explored. This work introduces DYNAMAX,
the first framework to exploit the unique properties of Mamba architectures for
early exit mechanisms. We not only integrate EEs into Mamba but also repurpose
Mamba as an efficient EE classifier for both Mamba-based and transformer-based
LLMs, showcasing its versatility. Our experiments employ the Mistral 7B
transformer compared to the Codestral 7B Mamba model, using data sets such as
TruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and
consistency. The results highlight the adaptability of Mamba as a powerful EE
classifier and its efficiency in balancing computational cost and performance
quality across NLP tasks. By leveraging Mamba's inherent design for dynamic
processing, we open pathways for scalable and efficient inference in embedded
applications and resource-constrained environments. This study underscores the
transformative potential of Mamba in redefining dynamic computing paradigms for
LLMs.

</details>


### [38] [Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models](https://arxiv.org/abs/2504.20946)
*Tyler McDonald,Ali Emami*

Main category: cs.CL

TL;DR: 论文提出Trace-of-Thought Prompting，一种零样本提示工程方法，通过让LLMs创建可观察的子问题来提升算术推理能力，并证明其在7B参数以下开源模型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLMs在专业领域（如算术推理）中的高成本和封闭性限制，探索开源模型结合高效提示方法的潜力。

Method: 引入Trace-of-Thought Prompting方法，指导LLMs通过关键问题分解生成可观察的子问题，提升推理能力。

Result: 在7B参数以下的开源模型中，该方法性能提升高达125%，并与GPT-4结合展示了显著效果。

Conclusion: 开源模型结合高效提示工程能显著提升性能，推动AI研究的民主化和高质量计算语言应用的普及。

Abstract: As Large Language Models (LLMs) continue to be leveraged for daily tasks,
prompt engineering remains an active field of contribution within computational
linguistics, particularly in domains requiring specialized knowledge such as
arithmetic reasoning. While these LLMs are optimized for a variety of tasks,
their exhaustive employment may become computationally or financially
cumbersome for small teams. Additionally, complete reliance on proprietary,
closed-source models often limits customization and adaptability, posing
significant challenges in research and application scalability. Instead, by
leveraging open-source models at or below 7 billion parameters, we can optimize
our resource usage while still observing remarkable gains over standard
prompting approaches. To cultivate this notion, we introduce Trace-of-Thought
Prompting, a simple, zero-shot prompt engineering method that instructs LLMs to
create observable subproblems using critical problem-solving, specifically
designed to enhance arithmetic reasoning capabilities. When applied to
open-source models in tandem with GPT-4, we observe that Trace-of-Thought not
only allows novel insight into the problem-solving process but also introduces
performance gains as large as 125% on language models at or below 7 billion
parameters. This approach underscores the potential of open-source initiatives
in democratizing AI research and improving the accessibility of high-quality
computational linguistics applications.

</details>


### [39] [Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models](https://arxiv.org/abs/2504.20951)
*Maryna Vyshnyvetska*

Main category: cs.CL

TL;DR: 该论文提出了一个名为‘信息引力’的理论模型，用物理解释大语言模型（LLM）的文本生成过程，描述查询与生成标记概率分布的相互作用。


<details>
  <summary>Details</summary>
Motivation: 旨在通过物理学的场论和时空几何工具，形式化解释LLM行为中的现象，如幻觉、查询表述敏感性及温度对多样性的影响。

Method: 将查询视为具有‘信息质量’的对象，其弯曲模型的语义空间，形成引力势阱，影响标记生成。

Result: 模型成功解释了LLM中幻觉（语义空洞）、查询敏感性和温度影响的机制。

Conclusion: 信息引力模型为理解LLM生成行为提供了新的理论框架，可能启发更优的模型设计与评估方法。

Abstract: We propose a theoretical model called "information gravity" to describe the
text generation process in large language models (LLMs). The model uses
physical apparatus from field theory and spacetime geometry to formalize the
interaction between user queries and the probability distribution of generated
tokens. A query is viewed as an object with "information mass" that curves the
semantic space of the model, creating gravitational potential wells that
"attract" tokens during generation. This model offers a mechanism to explain
several observed phenomena in LLM behavior, including hallucinations (emerging
from low-density semantic voids), sensitivity to query formulation (due to
semantic field curvature changes), and the influence of sampling temperature on
output diversity.

</details>


### [40] [OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification](https://arxiv.org/abs/2504.20964)
*Shangyu Li,Juyong Jiang,Tiancheng Zhao,Jiasi Shen*

Main category: cs.CL

TL;DR: OSVBench是一个用于评估大型语言模型（LLMs）在操作系统内核验证任务中生成完整规范代码的新基准，揭示了当前LLMs在此类长上下文代码生成任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在操作系统验证任务中生成规范代码的能力尚不明确，因此需要建立一个基准来评估其性能。

Method: 将规范生成问题定义为受限制语法和语义范围内的程序合成问题，并在真实操作系统内核（Hyperkernel）上构建了245个复杂任务，每个任务包含约20k-30k tokens的长上下文。

Result: 评估了12个LLMs，发现其在操作系统验证任务中的表现有限，且不同模型在处理长上下文代码生成任务时性能差异显著。

Conclusion: 现有LLMs在生成操作系统规范代码方面仍有较大改进空间，OSVBench为未来研究提供了评估工具和基准。

Abstract: We introduce OSVBench, a new benchmark for evaluating Large Language Models
(LLMs) in generating complete specification code pertaining to operating system
kernel verification tasks. The benchmark first defines the specification
generation problem into a program synthesis problem within a confined scope of
syntax and semantics by providing LLMs with the programming model. The LLMs are
required to understand the provided verification assumption and the potential
syntax and semantics space to search for, then generate the complete
specification for the potentially buggy operating system code implementation
under the guidance of the high-level functional description of the operating
system. This benchmark is built upon a real-world operating system kernel,
Hyperkernel, and consists of 245 complex specification generation tasks in
total, each is a long context task of about 20k-30k tokens. Our comprehensive
evaluation of 12 LLMs exhibits the limited performance of the current LLMs on
the specification generation tasks for operating system verification.
Significant disparities in their performance on the benchmark highlight
differences in their ability to handle long-context code generation tasks. The
evaluation toolkit and benchmark are available at
https://github.com/lishangyu-hkust/OSVBench.

</details>


### [41] [SetKE: Knowledge Editing for Knowledge Elements Overlap](https://arxiv.org/abs/2504.20972)
*Yifan Wei,Xiaoyan Yu,Ran Song,Hao Peng,Angsheng Li*

Main category: cs.CL

TL;DR: 论文提出了知识集编辑（KSE）方法SetKE，解决了现有知识编辑（KE）方法在知识元素重叠（KEO）场景下的性能退化问题，并通过EditSet数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）需要更新知识以减少错误和幻觉，但传统方法（如微调和增量学习）存在过拟合和高计算成本问题。现有知识编辑方法忽略知识元素重叠（KEO）现象，导致编辑冲突。

Method: 提出了知识集编辑（KSE）框架和创新方法SetKE，通过同时编辑一组三元组来解决KEO问题。

Result: 实验证明SetKE在主流LLMs上优于现有KE方法，尤其是在KEO场景下。同时推出了包含KEO三元组的EditSet数据集作为基准。

Conclusion: SetKE有效解决了KEO导致的编辑冲突问题，为LLMs的知识更新提供了更优方案，EditSet数据集为未来研究提供了评测基础。

Abstract: Large Language Models (LLMs) excel in tasks such as retrieval and question
answering but require updates to incorporate new knowledge and reduce
inaccuracies and hallucinations. Traditional updating methods, like fine-tuning
and incremental learning, face challenges such as overfitting and high
computational costs. Knowledge Editing (KE) provides a promising alternative
but often overlooks the Knowledge Element Overlap (KEO) phenomenon, where
multiple triplets share common elements, leading to editing conflicts. We
identify the prevalence of KEO in existing KE datasets and show its significant
impact on current KE methods, causing performance degradation in handling such
triplets. To address this, we propose a new formulation, Knowledge Set Editing
(KSE), and introduce SetKE, a method that edits sets of triplets
simultaneously. Experimental results demonstrate that SetKE outperforms
existing methods in KEO scenarios on mainstream LLMs. Additionally, we
introduce EditSet, a dataset containing KEO triplets, providing a comprehensive
benchmark.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [A constraints-based approach to fully interpretable neural networks for detecting learner behaviors](https://arxiv.org/abs/2504.20055)
*Juan D. Pinto,Luc Paquette*

Main category: cs.LG

TL;DR: 论文提出了一种通过设计实现可解释性的神经网络行为检测模型，其参数具有明确解释性，能忠实且智能地解释模型学习到的行为模式。


<details>
  <summary>Details</summary>
Motivation: 由于复杂机器学习模型在教育领域的应用日益增加，但其可解释性不足，因此需要开发既能忠实于模型内部机制又能被人类理解的解释技术。

Method: 通过实施一系列约束条件，简化模型的推理过程并使其更接近人类对任务的理解，从而设计出一个完全可解释的神经网络模型。

Result: 模型成功学习了具有游戏系统行为的模式，并提供了完全可解释的解释证据，其学习模式与人类专家识别的模式一致。

Conclusion: 该方法不仅实现了可解释性，还提出了一种基于人类理解的评估方法，为未来的解释技术发展提供了方向。

Abstract: The increasing use of complex machine learning models in education has led to
concerns about their interpretability, which in turn has spurred interest in
developing explainability techniques that are both faithful to the model's
inner workings and intelligible to human end-users. In this paper, we describe
a novel approach to creating a neural-network-based behavior detection model
that is interpretable by design. Our model is fully interpretable, meaning that
the parameters we extract for our explanations have a clear interpretation,
fully capture the model's learned knowledge about the learner behavior of
interest, and can be used to create explanations that are both faithful and
intelligible. We achieve this by implementing a series of constraints to the
model that both simplify its inference process and bring it closer to a human
conception of the task at hand. We train the model to detect gaming-the-system
behavior, evaluate its performance on this task, and compare its learned
patterns to those identified by human experts. Our results show that the model
is successfully able to learn patterns indicative of gaming-the-system behavior
while providing evidence for fully interpretable explanations. We discuss the
implications of our approach and suggest ways to evaluate explainability using
a human-grounded approach.

</details>


### [43] [A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives](https://arxiv.org/abs/2504.20069)
*Junhong Lai,Jiyu Wei,Lin Yao,Yueming Wang*

Main category: cs.LG

TL;DR: 该论文总结了EEG基础模型（EEG-FMs）的最新进展，包括其架构、预训练策略和数据集，并探讨了该领域的挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: EEG信号对理解大脑活动和诊断神经系统疾病至关重要，EEG-FMs在处理和分析EEG数据方面展现出巨大潜力。

Method: 论文综述了多种EEG-FMs，详细介绍了它们的架构、预训练策略及相关数据集。

Result: EEG-FMs在EEG数据处理中表现出显著潜力，但仍面临一些挑战。

Conclusion: 论文为对EEG分析及其相关模型感兴趣的研究者和从业者提供了全面概述，并指出了未来研究方向。

Abstract: Electroencephalogram (EEG) signals play a crucial role in understanding brain
activity and diagnosing neurological disorders. This review focuses on the
recent development of EEG foundation models(EEG-FMs), which have shown great
potential in processing and analyzing EEG data. We discuss various EEG-FMs,
including their architectures, pre-training strategies, their pre-training and
downstream datasets and other details. The review also highlights the
challenges and future directions in this field, aiming to provide a
comprehensive overview for researchers and practitioners interested in EEG
analysis and related EEG-FMs.

</details>


### [44] [Improving Deep Knowledge Tracing via Gated Architectures and Adaptive Optimization](https://arxiv.org/abs/2504.20070)
*Altun Shukurlu*

Main category: cs.LG

TL;DR: 该研究改进了深度知识追踪（DKT）模型，通过使用LSTM和GRU提升长期依赖建模能力，并用PyTorch重新实现以增强扩展性和可复现性。实验显示优化器和架构改进显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 原始DKT模型使用标准RNN和Torch框架，限制了扩展性和可复现性。研究旨在通过架构改进和现代优化工具提升其效果和实用性。

Method: 采用LSTM和GRU替换标准RNN以优化长期依赖问题；用PyTorch重新实现模型；对比测试SGD、RMSProp等优化器的效果。

Result: 在Synthetic-5和Khan Academy数据集上，GRU和LSTM表现优于基础RNN；自适应优化器（Adam/AdamW）在收敛速度和预测准确率上超越SGD。

Conclusion: 改进的DKT模型结合PyTorch实现，为知识追踪和个性化学习提供了更高效、可扩展的基础。开源代码支持进一步研究。

Abstract: Deep Knowledge Tracing (DKT) models student learning behavior by using
Recurrent Neural Networks (RNNs) to predict future performance based on
historical interaction data. However, the original implementation relied on
standard RNNs in the Lua-based Torch framework, which limited extensibility and
reproducibility. In this work, we revisit the DKT model from two perspectives:
architectural improvements and optimization efficiency. First, we enhance the
model using gated recurrent units, specifically Long Short-Term Memory (LSTM)
networks and Gated Recurrent Units (GRU), which better capture long-term
dependencies and help mitigate vanishing gradient issues. Second, we
re-implement DKT using the PyTorch framework, enabling a modular and accessible
infrastructure compatible with modern deep learning workflows. We also
benchmark several optimization algorithms SGD, RMSProp, Adagrad, Adam, and
AdamW to evaluate their impact on convergence speed and predictive accuracy in
educational modeling tasks. Experiments on the Synthetic-5 and Khan Academy
datasets show that GRUs and LSTMs achieve higher accuracy and improved training
stability compared to basic RNNs, while adaptive optimizers such as Adam and
AdamW consistently outperform SGD in both early-stage learning and final model
performance. Our open-source PyTorch implementation provides a reproducible and
extensible foundation for future research in neural knowledge tracing and
personalized learning systems.

</details>


### [45] [RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2504.20073)
*Zihan Wang,Kangrui Wang,Qineng Wang,Pingyue Zhang,Linjie Li,Zhengyuan Yang,Kefan Yu,Minh Nhat Nguyen,Licheng Liu,Eli Gottlieb,Monica Lam,Yiping Lu,Kyunghyun Cho,Jiajun Wu,Li Fei-Fei,Lijuan Wang,Yejin Choi,Manling Li*

Main category: cs.LG

TL;DR: 本文提出了StarPO框架和RAGEN系统，用于训练大型语言模型（LLM）作为交互式代理。研究发现并解决了Echo Trap问题，提出了RL rollouts的优化方法，并强调了细粒度奖励信号对代理推理能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型作为交互式代理存在长期决策和环境反馈的挑战，尤其是在多轮代理强化学习中。

Method: 提出StarPO框架和RAGEN系统，通过轨迹级代理RL训练，并引入StarPO-S变体以稳定训练。

Result: 研究发现并解决了Echo Trap问题，优化了RL rollouts的初始状态和采样策略，并证明了细粒度奖励信号对推理能力的关键作用。

Conclusion: StarPO和RAGEN为代理RL训练提供了有效框架，但推理能力的提升依赖于细粒度的奖励设计。

Abstract: Training large language models (LLMs) as interactive agents presents unique
challenges including long-horizon decision making and interacting with
stochastic environment feedback. While reinforcement learning (RL) has enabled
progress in static tasks, multi-turn agent RL training remains underexplored.
We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a
general framework for trajectory-level agent RL, and introduce RAGEN, a modular
system for training and evaluating LLM agents. Our study on three stylized
environments reveals three core findings. First, our agent RL training shows a
recurring mode of Echo Trap where reward variance cliffs and gradient spikes;
we address this with StarPO-S, a stabilized variant with trajectory filtering,
critic incorporation, and decoupled clipping. Second, we find the shaping of RL
rollouts would benefit from diverse initial states, medium interaction
granularity and more frequent sampling. Third, we show that without
fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge
through multi-turn RL and they may show shallow strategies or hallucinated
thoughts. Code and environments are available at
https://github.com/RAGEN-AI/RAGEN.

</details>


### [46] [Low-Rank Matrix Approximation for Neural Network Compression](https://arxiv.org/abs/2504.20078)
*Kalyan Cherukuri,Aarav Lala*

Main category: cs.LG

TL;DR: 论文提出一种自适应秩奇异值分解（ARSVD）方法，动态调整全连接层的秩以降低能耗，优于传统固定秩压缩方法，能在保持精度的同时显著压缩模型。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络因内存和计算资源需求大而受限，需要一种既能压缩模型又能保持精度的方法。

Method: 使用动态选择秩的ARSVD方法，根据能量分布自适应调整每层秩，同时最小化精度损失。

Result: 在MNIST、CIFAR-10和CIFAR-100数据集上测试，ARSVD实现了显著模型压缩且不影响分类精度。

Conclusion: ARSVD在资源受限的计算场景中展现了优越性，平衡了压缩与性能。

Abstract: Deep Neural Networks (DNNs) are often constrained by their large memories and
computational restrictions. In this paper, we introduce a novel adaptive-rank
Singular Value Decomposition (ARSVD) that dynamically chooses the rank increase
of the fully connected layers below a certain threshold in energy expenditure.
Unlike conventional SVD compression methods that apply a fixed rank reduction
in all layers, our ARSVD method uses energy distribution to adaptively select
rank per layer while retaining accuracy. This is done for each layer in an
effort to use as much energy as possible while maintaining the lowest accuracy
loss. Such accuracy-adaptive approaches outperform traditional static rank
reduction methods by providing an improved balance between compression and
model performance. We first train a simple Multi-Layer Perceptron (MLP) on the
MNIST, CIFAR-10, and CIFAR-100 dataset and evaluate its performance using
accuracy and F1-score. After applying ARSVD, our results demonstrate that the
technique can achieve substantial model compression without compromising
classification accuracy. These results illustrate the usefulness of ARSVD in
computing scenarios where both computational and memory resources are scarce.

</details>


### [47] [FX-DARTS: Designing Topology-unconstrained Architectures with Differentiable Architecture Search and Entropy-based Super-network Shrinking](https://arxiv.org/abs/2504.20079)
*Xuan Rao,Bo Zhao,Derong Liu,Cesare Alippi*

Main category: cs.LG

TL;DR: FX-DARTS通过消除DARTS中对单元拓扑的限制，采用基于熵的超网络压缩框架，提升架构灵活性，实验证明其在性能与计算复杂度间取得了平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的DARTS方法强加先验约束（如同类型单元共享拓扑结构等），限制了Auto-ML的发展和更强大网络的探索。本文旨在通过放松这些约束，提升架构搜索的灵活性。

Method: 提出FX-DARTS方法，包括消除单元拓扑限制、改进超网络离散化机制，并引入基于熵的超网络压缩框架（ESS）以稳定搜索过程。

Result: 在图像分类基准测试中，FX-DARTS能够在一次搜索中探索出性能与计算复杂度权衡优异的神经网络架构。

Conclusion: FX-DARTS通过减少先验约束，实现了更灵活的架构搜索，同时保持了搜索空间的稳定性，为Auto-ML提供了新的可能性。

Abstract: Strong priors are imposed on the search space of Differentiable Architecture
Search (DARTS), such that cells of the same type share the same topological
structure and each intermediate node retains two operators from distinct nodes.
While these priors reduce optimization difficulties and improve the
applicability of searched architectures, they hinder the subsequent development
of automated machine learning (Auto-ML) and prevent the optimization algorithm
from exploring more powerful neural networks through improved architectural
flexibility. This paper aims to reduce these prior constraints by eliminating
restrictions on cell topology and modifying the discretization mechanism for
super-networks. Specifically, the Flexible DARTS (FX-DARTS) method, which
leverages an Entropy-based Super-Network Shrinking (ESS) framework, is
presented to address the challenges arising from the elimination of prior
constraints. Notably, FX-DARTS enables the derivation of neural architectures
without strict prior rules while maintaining the stability in the enlarged
search space. Experimental results on image classification benchmarks
demonstrate that FX-DARTS is capable of exploring a set of neural architectures
with competitive trade-offs between performance and computational complexity
within a single search procedure.

</details>


### [48] [DNAD: Differentiable Neural Architecture Distillation](https://arxiv.org/abs/2504.20080)
*Xuan Rao,Bo Zhao,Derong Liu*

Main category: cs.LG

TL;DR: DNAD算法通过结合搜索删除与模仿搜索，优化神经网络设计，平衡性能与计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 为满足高效神经网络设计需求，寻求性能（如分类准确率）与计算复杂度之间的最佳权衡。

Method: 提出DNAD算法：1）基于DARTS框架的SNPS算法（搜索删除）；2）结合知识蒸馏（KD）的模仿搜索。

Result: 在CIFAR-10和ImageNet任务中，SNPS和DNAD均能生成高性能、低参数量模型。DNAD在ImageNet上达23.7% top-1错误率（6.0M参数，598M FLOPs）。

Conclusion: DNAD通过动态超网络压缩与知识蒸馏，显著优于传统DARTS方法，实现了高效架构搜索。

Abstract: To meet the demand for designing efficient neural networks with appropriate
trade-offs between model performance (e.g., classification accuracy) and
computational complexity, the differentiable neural architecture distillation
(DNAD) algorithm is developed based on two cores, namely search by deleting and
search by imitating. Primarily, to derive neural architectures in a space where
cells of the same type no longer share the same topology, the super-network
progressive shrinking (SNPS) algorithm is developed based on the framework of
differentiable architecture search (DARTS), i.e., search by deleting. Unlike
conventional DARTS-based approaches which yield neural architectures with
simple structures and derive only one architecture during the search procedure,
SNPS is able to derive a Pareto-optimal set of architectures with flexible
structures by forcing the dynamic super-network shrink from a dense structure
to a sparse one progressively. Furthermore, since knowledge distillation (KD)
has shown great effectiveness to train a compact network with the assistance of
an over-parameterized model, we integrate SNPS with KD to formulate the DNAD
algorithm, i.e., search by imitating. By minimizing behavioral differences
between the super-network and teacher network, the over-fitting of one-level
DARTS is avoided and well-performed neural architectures are derived.
Experiments on CIFAR-10 and ImageNet classification tasks demonstrate that both
SNPS and DNAD are able to derive a set of architectures which achieve similar
or lower error rates with fewer parameters and FLOPs. Particularly, DNAD
achieves the top-1 error rate of 23.7% on ImageNet classification with a model
of 6.0M parameters and 598M FLOPs, which outperforms most DARTS-based methods.

</details>


### [49] [Towards Practical Second-Order Optimizers in Deep Learning: Insights from Fisher Information Analysis](https://arxiv.org/abs/2504.20096)
*Damien Martins Gomes*

Main category: cs.LG

TL;DR: AdaFisher是一种新型自适应二阶优化器，通过利用Fisher信息矩阵的对角块Kronecker近似来预处理梯度，旨在平衡二阶方法的收敛性和泛化性与深度神经网络训练的计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管一阶优化方法（如Adam和SGD）在训练深度神经网络中广泛应用，二阶方法通常表现出更好的收敛性，但其较高的计算成本限制了实用性。因此，研究者提出了AdaFisher，以结合二阶方法的优势与计算效率。

Method: AdaFisher通过使用Fisher信息矩阵的对角块Kronecker近似来自适应地预处理梯度，从而在保持二阶方法优势的同时减少计算开销。

Result: AdaFisher在图像分类和语言建模等任务中表现出色，具有超参数调整的稳定性和鲁棒性，并在准确性和收敛速度上优于当前最先进的优化器。

Conclusion: AdaFisher成功地将二阶优化器的性能优势与实际应用的计算效率需求相结合，为深度神经网络训练提供了一种高效且强大的优化方法。

Abstract: First-order optimization methods remain the standard for training deep neural
networks (DNNs). Optimizers like Adam incorporate limited curvature information
by preconditioning the stochastic gradient with a diagonal matrix. Despite the
widespread adoption of first-order methods, second-order optimization
algorithms often exhibit superior convergence compared to methods like Adam and
SGD. However, their practicality in training DNNs is still limited by a
significantly higher per-iteration computational cost compared to first-order
methods. In this thesis, we present AdaFisher, a novel adaptive second-order
optimizer that leverages a diagonal block-Kronecker approximation of the Fisher
information matrix to adaptively precondition gradients. AdaFisher aims to
bridge the gap between the improved convergence and generalization of
second-order methods and the computational efficiency needed for training DNNs.
Despite the traditionally slower speed of second-order optimizers, AdaFisher is
effective for tasks such as image classification and language modeling,
exhibiting remarkable stability and robustness during hyperparameter tuning. We
demonstrate that AdaFisher outperforms state-of-the-art optimizers in both
accuracy and convergence speed. The code is available from
https://github.com/AtlasAnalyticsLab/AdaFisher.

</details>


### [50] [Decoding Latent Spaces: Assessing the Interpretability of Time Series Foundation Models for Visual Analytics](https://arxiv.org/abs/2504.20099)
*Inmaculada Santamaria-Valenzuela,Victor Rodriguez-Fernandez,Javier Huertas-Tato,Jong Hyuk Park,David Camacho*

Main category: cs.LG

TL;DR: 该研究评估了时间序列基础模型（如MOMENT）的潜在空间可解释性，发现微调能降低损失但视觉分析改进有限，需进一步优化方法。


<details>
  <summary>Details</summary>
Motivation: 探索时间序列基础模型（如MOMENT）的潜在空间可解释性，以支持视觉分析任务。

Method: 在五个数据集上评估MOMENT模型的潜在空间捕获能力，并通过微调验证嵌入空间清晰度的提升。

Result: 微调显著降低了损失，但视觉分析显示嵌入空间的可解释性改进有限。

Conclusion: 时间序列基础模型虽高效，但其潜在空间需进一步优化方法（如投影技术或损失函数）才能更好支持解释。

Abstract: The present study explores the interpretability of latent spaces produced by
time series foundation models, focusing on their potential for visual analysis
tasks. Specifically, we evaluate the MOMENT family of models, a set of
transformer-based, pre-trained architectures for multivariate time series tasks
such as: imputation, prediction, classification, and anomaly detection. We
evaluate the capacity of these models on five datasets to capture the
underlying structures in time series data within their latent space projection
and validate whether fine tuning improves the clarity of the resulting
embedding spaces. Notable performance improvements in terms of loss reduction
were observed after fine tuning. Visual analysis shows limited improvement in
the interpretability of the embeddings, requiring further work. Results suggest
that, although Time Series Foundation Models such as MOMENT are robust, their
latent spaces may require additional methodological refinements to be
adequately interpreted, such as alternative projection techniques, loss
functions, or data preprocessing strategies. Despite the limitations of MOMENT,
foundation models supose a big reduction in execution time and so a great
advance for interactive visual analytics.

</details>


### [51] [HyboWaveNet: Hyperbolic Graph Neural Networks with Multi-Scale Wavelet Transform for Protein-Protein Interaction Prediction](https://arxiv.org/abs/2504.20102)
*Qingzhi Yu,Shuai Yan,Wenfeng Dai,Xiang Cheng*

Main category: cs.LG

TL;DR: HyboWaveNet联合双曲图神经网络(HGNNs)和多尺度图小波变换，提出了一个鲁棒的PPI预测框架，通过双曲距离捕捉蛋白质的层次拓扑关系，并结合多尺度特征自适应提取局部和全局交互模式，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络和机器学习方法在PPI预测中缺乏因果解释且难以捕捉层次几何和多尺度动态交互模式，HyboWaveNet旨在解决这些问题。

Method: 利用双曲图网络将蛋白质特征映射到Lorentz空间模拟层次拓扑关系，结合多尺度图小波变换自适应提取特征，并通过对比学习生成样本。

Result: 实验表明HyboWaveNet优于现有方法，且多尺度图小波变换模块提升了模型的预测性能和泛化能力。

Conclusion: HyboWaveNet将几何深度学习和信号处理结合，为复杂生物系统的分析提供了新方法。

Abstract: Protein-protein interactions (PPIs) are fundamental for deciphering cellular
functions,disease pathways,and drug discovery.Although existing neural networks
and machine learning methods have achieved high accuracy in PPI
prediction,their black-box nature leads to a lack of causal interpretation of
the prediction results and difficulty in capturing hierarchical geometries and
multi-scale dynamic interaction patterns among proteins.To address these
challenges, we propose HyboWaveNet,a novel deep learning framework that
collaborates with hyperbolic graphical neural networks (HGNNs) and multiscale
graphical wavelet transform for robust PPI prediction. Mapping protein features
to Lorentz space simulates hierarchical topological relationships among
biomolecules via a hyperbolic distance metric,enabling node feature
representations that better fit biological a priori.HyboWaveNet inherently
simulates hierarchical and scale-free biological relationships, while the
integration of wavelet transforms enables adaptive extraction of local and
global interaction features across different resolutions. Our framework
generates node feature representations via a graph neural network under the
Lorenz model and generates pairs of positive samples under multiple different
views for comparative learning, followed by further feature extraction via
multi-scale graph wavelet transforms to predict potential PPIs. Experiments on
public datasets show that HyboWaveNet improves over both existing
state-of-the-art methods. We also demonstrate through ablation experimental
studies that the multi-scale graph wavelet transform module improves the
predictive performance and generalization ability of HyboWaveNet. This work
links geometric deep learning and signal processing to advance PPI prediction,
providing a principled approach for analyzing complex biological systems

</details>


### [52] [Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors](https://arxiv.org/abs/2504.20106)
*Ren-Wei Liang,Chin-Ting Hsu,Chan-Hung Yu,Saransh Agrawal,Shih-Cheng Huang,Shang-Tse Chen,Kuan-Hao Huang,Shao-Hua Sun*

Main category: cs.LG

TL;DR: 论文提出Preference Vector框架，通过分离训练和动态合并偏好向量，解决了LLM在友好性和无害性之间的平衡问题，提升可控性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如RLHF和DPO）在平衡LLM的友好性和无害性时存在性能冲突、可控性差和扩展性不足的问题。

Method: 通过独立训练不同偏好的模型，提取行为偏移作为偏好向量，并在测试时动态合并，实现模块化调控。

Result: 实验显示该框架提升友好性而不过度保守，支持平滑偏好调整和多偏好对齐扩展。

Conclusion: Preference Vector为LLM的偏好平衡提供了更灵活、可控且可扩展的解决方案。

Abstract: Ensuring that large language models (LLMs) are both helpful and harmless is a
critical challenge, as overly strict constraints can lead to excessive
refusals, while permissive models risk generating harmful content. Existing
approaches, such as reinforcement learning from human feedback (RLHF) and
direct preference optimization (DPO), attempt to balance these trade-offs but
suffer from performance conflicts, limited controllability, and poor
extendability. To address these issues, we propose Preference Vector, a novel
framework inspired by task arithmetic. Instead of optimizing multiple
preferences within a single objective, we train separate models on individual
preferences, extract behavior shifts as preference vectors, and dynamically
merge them at test time. This modular approach enables fine-grained,
user-controllable preference adjustments and facilitates seamless integration
of new preferences without retraining. Experiments show that our proposed
Preference Vector framework improves helpfulness without excessive
conservatism, allows smooth control over preference trade-offs, and supports
scalable multi-preference alignment.

</details>


### [53] [Swapped Logit Distillation via Bi-level Teacher Alignment](https://arxiv.org/abs/2504.20108)
*Stephen Ekaputra Limantoro,Jhe-Hao Lin,Chih-Yu Wang,Yi-Lung Tsai,Hong-Han Shuai,Ching-Chun Huang,Wen-Huang Cheng*

Main category: cs.LG

TL;DR: 该论文提出了一种基于对数交换处理的蒸馏方法（SLD），通过交换师生网络的输出对数来提升知识蒸馏效果，实验证明其在图像分类任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的知识蒸馏方法直接将教师网络的原始分布传递给学生网络，可能导致错误预测。本文提出SLD方法，旨在通过交换对数处理解决这一问题，提升知识蒸馏的准确性。

Method: SLD基于两个假设：错误预测发生在预测标签置信度非最大时；概率的‘自然’限制导致目标值不确定。方法包括对数交换处理和损失调度，将师生输出转化为双教师并优化对齐。

Result: 在图像分类任务上的大量实验表明，SLD方法 consistently 优于现有最先进方法。

Conclusion: 通过交换对数处理和损失调度，SLD有效提升了知识蒸馏的性能，验证了其在图像分类任务中的优越性。

Abstract: Knowledge distillation (KD) compresses the network capacity by transferring
knowledge from a large (teacher) network to a smaller one (student). It has
been mainstream that the teacher directly transfers knowledge to the student
with its original distribution, which can possibly lead to incorrect
predictions. In this article, we propose a logit-based distillation via swapped
logit processing, namely Swapped Logit Distillation (SLD). SLD is proposed
under two assumptions: (1) the wrong prediction occurs when the prediction
label confidence is not the maximum; (2) the "natural" limit of probability
remains uncertain as the best value addition to the target cannot be
determined. To address these issues, we propose a swapped logit processing
scheme. Through this approach, we find that the swap method can be effectively
extended to teacher and student outputs, transforming into two teachers. We
further introduce loss scheduling to boost the performance of two teachers'
alignment. Extensive experiments on image classification tasks demonstrate that
SLD consistently performs best among previous state-of-the-art methods.

</details>


### [54] [Attention to Detail: Fine-Scale Feature Preservation-Oriented Geometric Pre-training for AI-Driven Surrogate Modeling](https://arxiv.org/abs/2504.20110)
*Yu-hsuan Chen,Jing Bi,Cyril Ngo Ngoc,Victor Oancea,Jonathan Cagan,Levent Burak Kara*

Main category: cs.LG

TL;DR: 该论文提出了一种自监督几何表示学习方法，旨在从非参数3D模型中捕捉精细几何特征，通过解耦几何特征提取与下游物理任务，提升数据稀缺场景下的替代建模效果。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动的替代建模方法在处理精细几何细节时效果有限，且标记CAD到模拟的数据集稀缺，因此需要一种新方法来解决这些问题。

Method: 该方法通过自监督几何表示学习，利用几何重建损失引导的潜在空间嵌入，结合近零级采样和批量自适应注意力加权损失函数，解耦几何特征提取与下游物理任务。

Result: 案例研究表明，该方法在结构力学中表现优异，能够捕捉设计特征并实现准确的少样本物理预测，相比传统参数化替代建模更具优势。

Conclusion: 该方法填补了几何与基于物理表示之间的差距，为数据稀缺场景提供了有效的替代建模解决方案。

Abstract: AI-driven surrogate modeling has become an increasingly effective alternative
to physics-based simulations for 3D design, analysis, and manufacturing. These
models leverage data-driven methods to predict physical quantities
traditionally requiring computationally expensive simulations. However, the
scarcity of labeled CAD-to-simulation datasets has driven recent advancements
in self-supervised and foundation models, where geometric representation
learning is performed offline and later fine-tuned for specific downstream
tasks. While these approaches have shown promise, their effectiveness is
limited in applications requiring fine-scale geometric detail preservation.
This work introduces a self-supervised geometric representation learning method
designed to capture fine-scale geometric features from non-parametric 3D
models. Unlike traditional end-to-end surrogate models, this approach decouples
geometric feature extraction from downstream physics tasks, learning a latent
space embedding guided by geometric reconstruction losses. Key elements include
the essential use of near-zero level sampling and the innovative batch-adaptive
attention-weighted loss function, which enhance the encoding of intricate
design features. The proposed method is validated through case studies in
structural mechanics, demonstrating strong performance in capturing design
features and enabling accurate few-shot physics predictions. Comparisons with
traditional parametric surrogate modeling highlight its potential to bridge the
gap between geometric and physics-based representations, providing an effective
solution for surrogate modeling in data-scarce scenarios.

</details>


### [55] [Supervised Pretraining for Material Property Prediction](https://arxiv.org/abs/2504.20112)
*Chowdhury Mohammad Abid Rahman,Aldo H. Romero,Prashnna K. Gyawali*

Main category: cs.LG

TL;DR: 该研究提出了一种监督式预训练方法，利用类信息作为代理标签，结合图增强技术，显著提升了材料属性预测的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型依赖监督学习，需要大量标注数据，成本高昂。自监督学习（SSL）虽能利用无标签数据预训练，但如何更高效地结合监督信息提升预测性能仍需探索。

Method: 提出监督式预训练框架，利用代理标签引导学习；引入基于图的增强技术，通过注入噪声增强表示鲁棒性；在六项材料属性预测任务上微调模型。

Result: 模型性能显著超越基线，平均绝对误差（MAE）改善2%至6.67%，创下材料属性预测新基准。

Conclusion: 监督式预训练与代理标签的结合为材料科学领域提供了方法论创新，推动实际应用发展。

Abstract: Accurate prediction of material properties facilitates the discovery of novel
materials with tailored functionalities. Deep learning models have recently
shown superior accuracy and flexibility in capturing structure-property
relationships. However, these models often rely on supervised learning, which
requires large, well-annotated datasets an expensive and time-consuming
process. Self-supervised learning (SSL) offers a promising alternative by
pretraining on large, unlabeled datasets to develop foundation models that can
be fine-tuned for material property prediction. In this work, we propose
supervised pretraining, where available class information serves as surrogate
labels to guide learning, even when downstream tasks involve unrelated material
properties. We evaluate this strategy on two state-of-the-art SSL models and
introduce a novel framework for supervised pretraining. To further enhance
representation learning, we propose a graph-based augmentation technique that
injects noise to improve robustness without structurally deforming material
graphs. The resulting foundation models are fine-tuned for six challenging
material property predictions, achieving significant performance gains over
baselines, ranging from 2% to 6.67% improvement in mean absolute error (MAE)
and establishing a new benchmark in material property prediction. This study
represents the first exploration of supervised pertaining with surrogate labels
in material property prediction, advancing methodology and application in the
field.

</details>


### [56] [Benchmarking Transferability: A Framework for Fair and Robust Evaluation](https://arxiv.org/abs/2504.20121)
*Alireza Kazemi,Helia Rezvani,Mahsa Baktashmotlagh*

Main category: cs.LG

TL;DR: 论文提出了一个用于系统评估迁移性分数的综合基准框架，发现现有评估方法未能充分反映各方法的优劣势，并展示了其提出的新指标在特定实验设置下的3.5%提升。


<details>
  <summary>Details</summary>
Motivation: 现有迁移性测量方法的可靠性和实用性未达成共识，主要由于实验设置、数据集和假设的差异。因此，需要一个标准化的评估框架来更好地理解和比较这些方法。

Method: 引入了全面的基准框架，通过多样化设置下的广泛实验，系统性评估不同迁移性分数指标的表现。

Result: 发现不同指标在不同场景下表现各异，当前评估实践未必能全面反映方法的优劣势。提出的新指标在特定实验设置下实现了3.5%的性能提升。

Conclusion: 标准化评估协议对提升迁移性测量的可靠性至关重要，有助于跨域应用中更明智的模型选择。

Abstract: Transferability scores aim to quantify how well a model trained on one domain
generalizes to a target domain. Despite numerous methods proposed for measuring
transferability, their reliability and practical usefulness remain
inconclusive, often due to differing experimental setups, datasets, and
assumptions. In this paper, we introduce a comprehensive benchmarking framework
designed to systematically evaluate transferability scores across diverse
settings. Through extensive experiments, we observe variations in how different
metrics perform under various scenarios, suggesting that current evaluation
practices may not fully capture each method's strengths and limitations. Our
findings underscore the value of standardized assessment protocols, paving the
way for more reliable transferability measures and better-informed model
selection in cross-domain applications. Additionally, we achieved a 3.5\%
improvement using our proposed metric for the head-training fine-tuning
experimental setup. Our code is available in this repository:
https://github.com/alizkzm/pert_robust_platform.

</details>


### [57] [LZ Penalty: An information-theoretic repetition penalty for autoregressive language models](https://arxiv.org/abs/2504.20131)
*Antonio A. Ginart,Naveen Kodali,Jason Lee,Caiming Xiong,Silvio Savarese,John R. Emmons*

Main category: cs.LG

TL;DR: 该论文提出了LZ惩罚方法，专门用于减少自回归语言模型中的重复退化，且不影响模型能力。该方法基于LZ77无损压缩算法的编码长度，通过预测-压缩对偶性实现，有效避免了贪婪解码中的重复退化问题。


<details>
  <summary>Details</summary>
Motivation: 解决自回归语言模型在贪婪解码中易出现重复退化的问题，同时不损失模型能力。现有频率惩罚和重复惩罚方法效果不佳，重复退化率高达4%。

Method: 利用LZ77算法的编码长度设计LZ惩罚，通过预测-压缩对耦性，从残余分布中采样，去除高度可压缩信息。

Result: LZ惩罚使模型在贪婪解码下仍保持先进能力，且无重复退化现象，显著优于现有惩罚方法。

Conclusion: LZ惩罚是一种高效且无损失的解决方案，适用于自回归语言模型，尤其在贪婪解码场景下表现优异。

Abstract: We introduce the LZ penalty, a penalty specialized for reducing degenerate
repetitions in autoregressive language models without loss of capability. The
penalty is based on the codelengths in the LZ77 universal lossless compression
algorithm. Through the lens of the prediction-compression duality, decoding the
LZ penalty has the interpretation of sampling from the residual distribution
after removing the information that is highly compressible. We demonstrate the
LZ penalty enables state-of-the-art open-source reasoning models to operate
with greedy (temperature zero) decoding without loss of capability and without
instances of degenerate repetition. Both the industry-standard frequency
penalty and repetition penalty are ineffective, incurring degenerate repetition
rates of up to 4%.

</details>


### [58] [Causal Identification in Time Series Models](https://arxiv.org/abs/2504.20172)
*Erik Jahn,Karthik Karnik,Leonard J. Schulman*

Main category: cs.LG

TL;DR: 该论文证明了在具有潜在混杂因素的因果时间序列图中，应用因果识别算法仅需考虑恒定大小的图段即可确定因果效应的可识别性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决因果时间序列图中无限时间步长带来的计算挑战，特别是如何确定任意时间间隔的因果效应是否可识别的问题。

Method: 通过分析图的局部特性，提出仅需考虑恒定大小的图段（仅依赖于每时间步的变量数量和最大时间滞后），即可应用因果识别算法。

Result: 结果表明，即使时间间隔无限，恒定大小的图段也足以判断因果效应的可识别性。

Conclusion: 结论为在实际应用中，可以通过有限的计算资源解决因果效应的识别问题。

Abstract: In this paper, we analyze the applicability of the Causal Identification
algorithm to causal time series graphs with latent confounders. Since these
graphs extend over infinitely many time steps, deciding whether causal effects
across arbitrary time intervals are identifiable appears to require computation
on graph segments of unbounded size. Even for deciding the identifiability of
intervention effects on variables that are close in time, no bound is known on
how many time steps in the past need to be considered. We give a first bound of
this kind that only depends on the number of variables per time step and the
maximum time lag of any direct or latent causal effect. More generally, we show
that applying the Causal Identification algorithm to a constant-size segment of
the time series graph is sufficient to decide identifiability of causal
effects, even across unbounded time intervals.

</details>


### [59] [AI Recommendation Systems for Lane-Changing Using Adherence-Aware Reinforcement Learning](https://arxiv.org/abs/2504.20187)
*Weihao Sun,Heeseung Bang,Andreas A. Malikopoulos*

Main category: cs.LG

TL;DR: 提出一种基于强化学习的半自动驾驶车道变更推荐方法，通过考虑驾驶员对建议的部分遵守来优化单车旅行效率。


<details>
  <summary>Details</summary>
Motivation: 旨在通过半自动驾驶环境中的车道变更建议提升单车的旅行效率，同时考虑人类驾驶员对建议的遵守程度。

Method: 采用马尔可夫决策过程框架和基于深度Q网络的方法，考虑驾驶员部分遵守推荐动作的情况。

Result: 在CARLA驾驶环境中进行了真实场景下的评估。

Conclusion: 该方法能够有效优化车道变更决策，提高旅行效率，同时适应驾驶员的行为特点。

Abstract: In this paper, we present an adherence-aware reinforcement learning (RL)
approach aimed at seeking optimal lane-changing recommendations within a
semi-autonomous driving environment to enhance a single vehicle's travel
efficiency. The problem is framed within a Markov decision process setting and
is addressed through an adherence-aware deep Q network, which takes into
account the partial compliance of human drivers with the recommended actions.
This approach is evaluated within CARLA's driving environment under realistic
scenarios.

</details>


### [60] [ProFi-Net: Prototype-based Feature Attention with Curriculum Augmentation for WiFi-based Gesture Recognition](https://arxiv.org/abs/2504.20193)
*Zhe Cui,Shuxian Zhang,Kangzhi Lou,Le-Nam Tran*

Main category: cs.LG

TL;DR: ProFi-Net 是一种新型的小样本学习框架，用于基于 WiFi 的手势识别，通过原型度量学习和特征级注意力机制解决数据有限和特征稀疏的问题，并通过课程式数据增强提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法在有限训练数据和稀疏特征表示方面存在挑战，ProFi-Net 旨在通过动态优化特征距离和增强数据多样性来提升识别效果。

Method: 采用原型度量学习结合特征级注意力机制动态优化欧氏距离，并设计课程式数据增强策略逐步增加高斯噪声以提升模型泛化能力。

Result: 在多样真实环境中的实验表明，ProFi-Net 在分类准确率和训练效率上显著优于传统原型网络和其他小样本学习方法。

Conclusion: ProFi-Net 通过结合注意力机制和课程式数据增强，显著提升了小样本 WiFi 手势识别的性能和鲁棒性。

Abstract: This paper presents ProFi-Net, a novel few-shot learning framework for
WiFi-based gesture recognition that overcomes the challenges of limited
training data and sparse feature representations. ProFi-Net employs a
prototype-based metric learning architecture enhanced with a feature-level
attention mechanism, which dynamically refines the Euclidean distance by
emphasizing the most discriminative feature dimensions. Additionally, our
approach introduces a curriculum-inspired data augmentation strategy
exclusively on the query set. By progressively incorporating Gaussian noise of
increasing magnitude, the model is exposed to a broader range of challenging
variations, thereby improving its generalization and robustness to overfitting.
Extensive experiments conducted across diverse real-world environments
demonstrate that ProFi-Net significantly outperforms conventional prototype
networks and other state-of-the-art few-shot learning methods in terms of
classification accuracy and training efficiency.

</details>


### [61] [Representation Learning on a Random Lattice](https://arxiv.org/abs/2504.20197)
*Aryeh Brill*

Main category: cs.LG

TL;DR: 论文提出了一种将深度神经网络的表示分解为可解释特征的方法，采用了几何视角，并将其与渗流理论结合分析。


<details>
  <summary>Details</summary>
Motivation: 为了提高深度神经网络的安全性和可靠性，需要理解其学习到的表示。

Method: 采用几何视角，将特征视为嵌入数据分布的坐标系，并使用渗流理论分析其性质。

Result: 特征被分类为上下文、组件和表面特征，方法与机械可解释性的最新发现一致。

Conclusion: 该方法为未来研究提供了方向，并有助于增强模型的可解释性。

Abstract: Decomposing a deep neural network's learned representations into
interpretable features could greatly enhance its safety and reliability. To
better understand features, we adopt a geometric perspective, viewing them as a
learned coordinate system for mapping an embedded data distribution. We
motivate a model of a generic data distribution as a random lattice and analyze
its properties using percolation theory. Learned features are categorized into
context, component, and surface features. The model is qualitatively consistent
with recent findings in mechanistic interpretability and suggests directions
for future research.

</details>


### [62] [Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework](https://arxiv.org/abs/2504.20213)
*Yuan Xia,Akanksha Atrey,Fadoua Khmaissia,Kedar S. Namjoshi*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型（LLM）在布尔逻辑证明中的推理能力，提出了模板变换的数据增强方法，实验证明其对提升模型推理能力有效。


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否真正具备逻辑推理能力，并为这一目标设计了布尔逻辑证明任务，以验证其能力。

Method: 提出随机合成有效证明的高效方法，并引入模板变换技术增强模型处理复杂逻辑表达式的能力。

Result: 实验表明模型在短证明任务中表现良好，但随着证明复杂度增加性能下降；模板变换对小模型也有显著提升。

Conclusion: LLM在布尔逻辑证明中展现了推理潜力，模板变换是提升模型能力的有效方法。

Abstract: This paper investigates the logical reasoning capabilities of large language
models (LLMs). For a precisely defined yet tractable formulation, we choose the
conceptually simple but technically complex task of constructing proofs in
Boolean logic. A trained LLM receives as input a set of assumptions and a goal,
and produces as output a proof that formally derives the goal from the
assumptions. Incorrect proofs are caught by an automated proof checker. A
critical obstacle for training is the scarcity of real-world proofs. We propose
an efficient, randomized procedure for synthesizing valid proofs and introduce
Template Transformation, a data augmentation technique that enhances the
model's ability to handle complex logical expressions. The central evaluation
question is whether an LLM has indeed learned to reason. We propose tests to
measure the reasoning ability of a black-box LLM. By these measures,
experiments demonstrate strong reasoning capabilities for assertions with short
proofs, which decline with proof complexity. Notably, template transformation
improves accuracy even for smaller models, suggesting its effectiveness across
model scales.

</details>


### [63] [Temporal Neural Operator for Modeling Time-Dependent Physical Phenomena](https://arxiv.org/abs/2504.20249)
*W. Diab,M. Al-Kobaisi*

Main category: cs.LG

TL;DR: 论文提出了一种称为Temporal Neural Operator (TNO)的新型神经算子，专门用于学习时间依赖偏微分方程的时空算子，提升了时间动态映射能力，并通过多种训练策略提高了效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有的神经算子在处理时间依赖偏微分方程时，难以有效捕捉时间动态，尤其在未见时间步上表现不佳，且训练成本高。因此，作者提出TNO来解决这些问题。

Method: TNO在DeepONet框架基础上引入了时间分支，结合了多种神经算子的最佳设计选择，并采用了Markov假设、教师强迫、时间捆绑等训练策略。

Result: TNO在多个测试问题上展示了出色的长程时间外推能力、误差积累鲁棒性、分辨率不变性及处理多输入函数的灵活性。

Conclusion: TNO是一种高效的神经算子，特别适用于时间依赖偏微分方程的时空算子学习，显著提升了时间动态映射能力和计算效率。

Abstract: Neural Operators (NOs) are machine learning models designed to solve partial
differential equations (PDEs) by learning to map between function spaces.
Neural Operators such as the Deep Operator Network (DeepONet) and the Fourier
Neural Operator (FNO) have demonstrated excellent generalization properties
when mapping between spatial function spaces. However, they struggle in mapping
the temporal dynamics of time-dependent PDEs, especially for time steps not
explicitly seen during training. This limits their temporal accuracy as they do
not leverage these dynamics in the training process. In addition, most NOs tend
to be prohibitively costly to train, especially for higher-dimensional PDEs. In
this paper, we propose the Temporal Neural Operator (TNO), an efficient neural
operator specifically designed for spatio-temporal operator learning for
time-dependent PDEs. TNO achieves this by introducing a temporal-branch to the
DeepONet framework, leveraging the best architectural design choices from
several other NOs, and a combination of training strategies including Markov
assumption, teacher forcing, temporal bundling, and the flexibility to
condition the output on the current state or past states. Through extensive
benchmarking and an ablation study on a diverse set of example problems we
demonstrate the TNO long range temporal extrapolation capabilities, robustness
to error accumulation, resolution invariance, and flexibility to handle
multiple input functions.

</details>


### [64] [Financial Data Analysis with Robust Federated Logistic Regression](https://arxiv.org/abs/2504.20250)
*Kun Yang,Nikhil Krishnan,Sanjeev R. Kulkarni*

Main category: cs.LG

TL;DR: 论文提出了一种用于金融数据的联邦学习框架，重点关注隐私保护、模型可解释性和对异常值的鲁棒性。该框架在IID和非IID数据上表现优异，性能与传统集中式算法相当。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决联邦学习中的数据隐私、模型可解释性和鲁棒性问题，特别是在金融数据场景下。

Method: 提出了一种基于逻辑回归的鲁棒联邦学习框架，并在IID和非IID数据上进行了测试。

Result: 实验表明，该方法在二元和多分类任务中与传统集中式算法性能相当。

Conclusion: 该框架在隐私保护和模型性能之间取得了平衡，适用于实际金融数据应用。

Abstract: In this study, we focus on the analysis of financial data in a federated
setting, wherein data is distributed across multiple clients or locations, and
the raw data never leaves the local devices. Our primary focus is not only on
the development of efficient learning frameworks (for protecting user data
privacy) in the field of federated learning but also on the importance of
designing models that are easier to interpret. In addition, we care about the
robustness of the framework to outliers. To achieve these goals, we propose a
robust federated logistic regression-based framework that strives to strike a
balance between these goals. To verify the feasibility of our proposed
framework, we carefully evaluate its performance not only on independently
identically distributed (IID) data but also on non-IID data, especially in
scenarios involving outliers. Extensive numerical results collected from
multiple public datasets demonstrate that our proposed method can achieve
comparable performance to those of classical centralized algorithms, such as
Logistical Regression, Decision Tree, and K-Nearest Neighbors, in both binary
and multi-class classification tasks.

</details>


### [65] [Investigating task-specific prompts and sparse autoencoders for activation monitoring](https://arxiv.org/abs/2504.20271)
*Henk Tillman,Dan Mossing*

Main category: cs.LG

TL;DR: 论文探讨了如何通过语言模型的内部激活来监控其输出安全性的方法，比较了多种技术并提出优化建议。


<details>
  <summary>Details</summary>
Motivation: 语言模型可能产生意外和不安全的输出，因此需要有效监控方法，内部激活信息可能为此提供额外价值。

Method: 研究比较了线性探测、提示探测和稀疏自编码器等方法，并提出了新的优化策略。

Result: 零样本直接询问模型是合理的基线方法，但激活探测方法在足够训练数据下表现更优；提示探测具有更高的数据效率和泛化性能。

Conclusion: 在推理计算可用时推荐使用提示探测，若计算有限则稀疏自编码器方法更优。

Abstract: Language models can behave in unexpected and unsafe ways, and so it is
valuable to monitor their outputs. Internal activations of language models
encode additional information that could be useful for this. The baseline
approach for activation monitoring is some variation of linear probing on a
particular layer: starting from a labeled dataset, train a logistic regression
classifier on that layer's activations. Recent work has proposed several
approaches which may improve on naive linear probing, by leveraging additional
computation. One class of techniques, which we call "prompted probing,"
leverages test time computation to improve monitoring by (1) prompting the
model with a description of the monitoring task, and (2) applying a learned
linear probe to resulting activations. Another class of techniques uses
computation at train time: training sparse autoencoders offline to identify an
interpretable basis for the activations, and e.g. max-pooling activations
across tokens using that basis before applying a linear probe. However, one can
also prompt the model with a description of the monitoring task and use its
output directly. We develop and test novel refinements of these methods and
compare them against each other. We find asking the model zero-shot is a
reasonable baseline when inference-time compute is not limited; however,
activation probing methods can substantially outperform this baseline given
sufficient training data. Specifically, we recommend prompted probing when
inference-time compute is available, due to its superior data efficiency and
good generalization performance. Alternatively, if inference-time compute is
limited, we find SAE-based probing methods outperform raw activation probing.

</details>


### [66] [Generative Diffusion Models for Resource Allocation in Wireless Networks](https://arxiv.org/abs/2504.20277)
*Yigit Berkay Uslu,Samar Hadou,Shirin Saeedi Bidokhti,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 该论文提出了一种使用生成扩散模型（GDM）的监督训练算法，用于学习随机资源分配策略。通过专家策略样本训练GDM，并在多用户干扰网络中的功率控制案例中验证了性能。


<details>
  <summary>Details</summary>
Motivation: 为解决随机资源分配问题，尤其是在复杂网络配置下，需要一种能够模仿专家策略并生成接近最优分配的方法。

Method: 利用生成扩散模型（GDM）和图形神经网络（GNN）架构，通过监督训练模仿专家策略，并生成新的分配样本。

Result: 在多用户干扰网络的功率控制案例中，该方法实现了接近最优的性能。

Conclusion: 该算法不仅高效，还能推广到多种网络配置，为随机资源分配问题提供了可行的解决方案。

Abstract: This paper proposes a supervised training algorithm for learning stochastic
resource allocation policies with generative diffusion models (GDMs). We
formulate the allocation problem as the maximization of an ergodic utility
function subject to ergodic Quality of Service (QoS) constraints. Given samples
from a stochastic expert policy that yields a near-optimal solution to the
problem, we train a GDM policy to imitate the expert and generate new samples
from the optimal distribution. We achieve near-optimal performance through
sequential execution of the generated samples. To enable generalization to a
family of network configurations, we parameterize the backward diffusion
process with a graph neural network (GNN) architecture. We present numerical
results in a case study of power control in multi-user interference networks.

</details>


### [67] [FedCCL: Federated Clustered Continual Learning Framework for Privacy-focused Energy Forecasting](https://arxiv.org/abs/2504.20282)
*Michael A. Helcig,Stefan Nastic*

Main category: cs.LG

TL;DR: FedCCL是一种针对静态组织特征但动态客户端可用性环境设计的框架，通过静态预训练聚类和异步FedAvg算法，提高了异构数据分布和计算能力下的模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在处理异构数据分布和动态客户端可用性时效率低下，FedCCL旨在解决这些问题。

Method: 结合静态预训练聚类与异步FedAvg算法，采用三层模型拓扑结构（全局、集群特定和本地模型）。

Result: 在欧洲中部光伏安装数据上，FedCCL实现了3.93%的能源预测误差，性能稳定。

Conclusion: FedCCL提供了一个高效的隐私保护分布式学习框架，适应动态参与者群体。

Abstract: Privacy-preserving distributed model training is crucial for modern machine
learning applications, yet existing Federated Learning approaches struggle with
heterogeneous data distributions and varying computational capabilities.
Traditional solutions either treat all participants uniformly or require costly
dynamic clustering during training, leading to reduced efficiency and delayed
model specialization. We present FedCCL (Federated Clustered Continual
Learning), a framework specifically designed for environments with static
organizational characteristics but dynamic client availability. By combining
static pre-training clustering with an adapted asynchronous FedAvg algorithm,
FedCCL enables new clients to immediately profit from specialized models
without prior exposure to their data distribution, while maintaining reduced
coordination overhead and resilience to client disconnections. Our approach
implements an asynchronous Federated Learning protocol with a three-tier model
topology - global, cluster-specific, and local models - that efficiently
manages knowledge sharing across heterogeneous participants. Evaluation using
photovoltaic installations across central Europe demonstrates that FedCCL's
location-based clustering achieves an energy prediction error of 3.93%
(+-0.21%), while maintaining data privacy and showing that the framework
maintains stability for population-independent deployments, with 0.14
percentage point degradation in performance for new installations. The results
demonstrate that FedCCL offers an effective framework for privacy-preserving
distributed learning, maintaining high accuracy and adaptability even with
dynamic participant populations.

</details>


### [68] [Radius-Guided Post-Clustering for Shape-Aware, Scalable Refinement of k-Means Results](https://arxiv.org/abs/2504.20293)
*Stefan Kober*

Main category: cs.LG

TL;DR: 该论文提出了一种改进的k-means聚类方法，通过添加半径和合并重叠簇的后处理步骤，解决了传统k-means对非凸形状和预设k值的限制问题。


<details>
  <summary>Details</summary>
Motivation: 传统k-means聚类在处理非凸形状时需要预设簇数量k且表现不佳，作者希望通过几何增强改进这一问题。

Method: 在标准k-means后，为每个簇中心分配半径（到最远点的距离），并合并半径重叠的簇。支持递归分区和分布式处理。

Result: 该方法在基准数据集上表现良好，能以最小的额外计算实现高准确率。

Conclusion: 该几何增强方法有效扩展了k-means的适用性，尤其适合非凸形状和大规模分布式场景。

Abstract: Traditional k-means clustering underperforms on non-convex shapes and
requires the number of clusters k to be specified in advance. We propose a
simple geometric enhancement: after standard k-means, each cluster center is
assigned a radius (the distance to its farthest assigned point), and clusters
whose radii overlap are merged. This post-processing step loosens the
requirement for exact k: as long as k is overestimated (but not excessively),
the method can often reconstruct non-convex shapes through meaningful merges.
We also show that this approach supports recursive partitioning: clustering can
be performed independently on tiled regions of the feature space, then globally
merged, making the method scalable and suitable for distributed systems.
Implemented as a lightweight post-processing step atop scikit-learn's k-means,
the algorithm performs well on benchmark datasets, achieving high accuracy with
minimal additional computation.

</details>


### [69] [The Dark Side of Digital Twins: Adversarial Attacks on AI-Driven Water Forecasting](https://arxiv.org/abs/2504.20295)
*Mohammadhossein Homaei,Victor Gonzalez Morales,Oscar Mogollon-Gutierrez,Andres Caro*

Main category: cs.LG

TL;DR: 该论文研究了数字孪生在水分配系统中的优化作用，但指出机器学习模型易受对抗性攻击，导致预测准确性下降。通过实验展示了一种自适应攻击方法对预测可靠性的显著影响，强调了加强防御措施的必要性。


<details>
  <summary>Details</summary>
Motivation: 数字孪生凭借实时数据和预测模型提升了水分配系统的效率，但其依赖的机器学习模型易受对抗性攻击的威胁，可能破坏预测可靠性，因此需要研究这些漏洞及防御策略。

Method: 论文提出了一种结合长短期记忆（LSTM）网络的数字孪生平台用于西班牙供水网络，并通过快速梯度符号法（FGSM）和投影梯度下降（PGD）模拟对抗性攻击，进一步采用学习自动机（LA）和随机LA动态调整扰动以增强攻击隐蔽性。

Result: 实验结果表明，自适应攻击显著降低了预测可靠性，平均绝对百分比误差（MAPE）从26%升至35%以上，突显了AI驱动数字孪生的网络安全风险。

Conclusion: 研究强调了开发鲁棒防御机制（如对抗训练、异常检测和安全数据管道）的紧迫性，以应对数字孪生系统中的对抗性威胁。

Abstract: Digital twins (DTs) are improving water distribution systems by using
real-time data, analytics, and prediction models to optimize operations. This
paper presents a DT platform designed for a Spanish water supply network,
utilizing Long Short-Term Memory (LSTM) networks to predict water consumption.
However, machine learning models are vulnerable to adversarial attacks, such as
the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD).
These attacks manipulate critical model parameters, injecting subtle
distortions that degrade forecasting accuracy. To further exploit these
vulnerabilities, we introduce a Learning Automata (LA) and Random LA-based
approach that dynamically adjusts perturbations, making adversarial attacks
more difficult to detect. Experimental results show that this approach
significantly impacts prediction reliability, causing the Mean Absolute
Percentage Error (MAPE) to rise from 26% to over 35%. Moreover, adaptive attack
strategies amplify this effect, highlighting cybersecurity risks in AI-driven
DTs. These findings emphasize the urgent need for robust defenses, including
adversarial training, anomaly detection, and secure data pipelines.

</details>


### [70] [FigBO: A Generalized Acquisition Function Framework with Look-Ahead Capability for Bayesian Optimization](https://arxiv.org/abs/2504.20307)
*Hui Chen,Xuhui Fan,Zhangkai Wu,Longbing Cao*

Main category: cs.LG

TL;DR: 文章提出FigBO，一种广义获取函数，通过融入候选点对全局信息增益的未来影响，解决了现有短视获取函数缺乏前瞻能力的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管短视获取函数因其简单有效被广泛采用，但其缺乏前瞻能力限制了性能，因此需要一种能融入未来影响的广义获取函数。

Method: 提出FigBO作为插拔式方法，可与多数现有短视获取函数无缝集成，理论分析了其与预期改进（EI）结合时的遗憾界限和收敛速率。

Result: FigBO在多样化任务中表现出最先进的性能和显著更快的收敛速度。

Conclusion: FigBO通过融入未来影响提升了贝叶斯优化的性能，是一种高效且通用的解决方案。

Abstract: Bayesian optimization is a powerful technique for optimizing
expensive-to-evaluate black-box functions, consisting of two main components: a
surrogate model and an acquisition function. In recent years, myopic
acquisition functions have been widely adopted for their simplicity and
effectiveness. However, their lack of look-ahead capability limits their
performance. To address this limitation, we propose FigBO, a generalized
acquisition function that incorporates the future impact of candidate points on
global information gain. FigBO is a plug-and-play method that can integrate
seamlessly with most existing myopic acquisition functions. Theoretically, we
analyze the regret bound and convergence rate of FigBO when combined with the
myopic base acquisition function expected improvement (EI), comparing them to
those of standard EI. Empirically, extensive experimental results across
diverse tasks demonstrate that FigBO achieves state-of-the-art performance and
significantly faster convergence compared to existing methods.

</details>


### [71] [A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning](https://arxiv.org/abs/2504.20310)
*Greg Gluch,Shafi Goldwasser*

Main category: cs.LG

TL;DR: 本文通过形式化的3轮协议研究了机器学习算法在推理时对抗性输入的检测与缓解防御方法，证明了分类任务中检测与缓解防御的等价性，但在生成学习中展示了二者的分离。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于为对抗性攻击提供理论框架，形式化定义防御方法（检测与缓解），并探讨其在不同机器学习任务中的可行性。

Method: 方法包括形式化定义防御协议（3轮交互）、提出正确性/完备性/可靠性标准，并通过理论证明展示分类与生成任务中防御方法的差异。

Result: 结果显示：分类任务中检测与缓解防御等价；生成任务中，基于密码学假设（IB-FHE等），缓解可行而检测不可行，且缓解所需样本远少于训练阶段。

Conclusion: 结论强调防御方法的选择需结合任务类型，生成学习的检测防御在特定条件下不可行，而缓解更具实际优势。

Abstract: In this paper, we initiate a cryptographically inspired theoretical study of
detection versus mitigation of adversarial inputs produced by attackers of
Machine Learning algorithms during inference time.
  We formally define defense by detection (DbD) and defense by mitigation
(DbM). Our definitions come in the form of a 3-round protocol between two
resource-bounded parties: a trainer/defender and an attacker. The attacker aims
to produce inference-time inputs that fool the training algorithm. We define
correctness, completeness, and soundness properties to capture successful
defense at inference time while not degrading (too much) the performance of the
algorithm on inputs from the training distribution.
  We first show that achieving DbD and achieving DbM are equivalent for ML
classification tasks. Surprisingly, this is not the case for ML generative
learning tasks, where there are many possible correct outputs that can be
generated for each input. We show a separation between DbD and DbM by
exhibiting a generative learning task for which is possible to defend by
mitigation but is provably impossible to defend by detection under the
assumption that the Identity-Based Fully Homomorphic Encryption (IB-FHE),
publicly-verifiable zero-knowledge Succinct Non-Interactive Arguments of
Knowledge (zk-SNARK) and Strongly Unforgeable Signatures exist. The mitigation
phase uses significantly fewer samples than the initial training algorithm.

</details>


### [72] [Perturbation-efficient Zeroth-order Optimization for Hardware-friendly On-device Training](https://arxiv.org/abs/2504.20314)
*Qitao Tan,Sung-En Chang,Rui Xia,Huidong Ji,Chence Yang,Ci Zhang,Jun Liu,Zheng Zhan,Zhou Zou,Yanzhi Wang,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: 提出了PeZO框架，通过随机数重用和硬件友好的自适应缩放方法，显著减少零阶优化中的随机数生成需求，降低硬件资源消耗和功耗，不影响训练性能。


<details>
  <summary>Details</summary>
Motivation: 零阶优化（ZO）因计算简单和内存节省而成为有前景的深度神经网络训练方法，但大量高斯随机数生成使其在FPGA、ASIC等硬件平台上难以实现。

Method: 设计随机数重用策略减少随机数需求，采用硬件友好的自适应缩放方法替代高斯分布为均匀分布。

Result: PeZO将随机数生成的LUT和FF分别减少48.6%和12.7%，最大节省86%功耗，且不影响训练性能。

Conclusion: PeZO首次探索了零阶优化在设备上的潜力，为未来研究提供了宝贵见解。

Abstract: Zeroth-order (ZO) optimization is an emerging deep neural network (DNN)
training paradigm that offers computational simplicity and memory savings.
However, this seemingly promising approach faces a significant and long-ignored
challenge. ZO requires generating a substantial number of Gaussian random
numbers, which poses significant difficulties and even makes it infeasible for
hardware platforms, such as FPGAs and ASICs. In this paper, we identify this
critical issue, which arises from the mismatch between algorithm and hardware
designers. To address this issue, we proposed PeZO, a perturbation-efficient ZO
framework. Specifically, we design random number reuse strategies to
significantly reduce the demand for random number generation and introduce a
hardware-friendly adaptive scaling method to replace the costly Gaussian
distribution with a uniform distribution. Our experiments show that PeZO
reduces the required LUTs and FFs for random number generation by 48.6\% and
12.7\%, and saves at maximum 86\% power consumption, all without compromising
training performance, making ZO optimization feasible for on-device training.
To the best of our knowledge, we are the first to explore the potential of
on-device ZO optimization, providing valuable insights for future research.

</details>


### [73] [Bayesian Experimental Design for Model Discrepancy Calibration: An Auto-Differentiable Ensemble Kalman Inversion Approach](https://arxiv.org/abs/2504.20319)
*Huchen Yang,Xinghao Dong,Jin-Long Wu*

Main category: cs.LG

TL;DR: 该论文提出了一个结合自动微分集合卡尔曼反演（AD-EKI）的混合贝叶斯实验设计（BED）框架，用于高效处理高维模型差异，并通过梯度自由方法优化实验设计，提升参数估计的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统BED方法在模型差异（预测模型与真实物理系统不匹配）时会导致参数估计偏差，且高维参数空间对贝叶斯更新和设计优化提出了挑战。

Method: 提出混合BED框架，利用AD-EKI高效评估信息增益，分离低维物理参数和高维模型差异的推断，梯度优化实验设计。

Result: 通过经典对流-扩散BED案例验证，该框架能高效校准模型差异并稳健推断未知物理参数。

Conclusion: AD-EKI不仅解决了BED中的模型差异问题，还为实现双层优化（如元学习和结构优化）提供了高效可扩展的框架。

Abstract: Bayesian experimental design (BED) offers a principled framework for
optimizing data acquisition by leveraging probabilistic inference. However,
practical implementations of BED are often compromised by model discrepancy,
i.e., the mismatch between predictive models and true physical systems, which
can potentially lead to biased parameter estimates. While data-driven
approaches have been recently explored to characterize the model discrepancy,
the resulting high-dimensional parameter space poses severe challenges for both
Bayesian updating and design optimization. In this work, we propose a hybrid
BED framework enabled by auto-differentiable ensemble Kalman inversion (AD-EKI)
that addresses these challenges by providing a computationally efficient,
gradient-free alternative to estimate the information gain for high-dimensional
network parameters. The AD-EKI allows a differentiable evaluation of the
utility function in BED and thus facilitates the use of standard gradient-based
methods for design optimization. In the proposed hybrid framework, we
iteratively optimize experimental designs, decoupling the inference of
low-dimensional physical parameters handled by standard BED methods, from the
high-dimensional model discrepancy handled by AD-EKI. The identified optimal
designs for the model discrepancy enable us to systematically collect
informative data for its calibration. The performance of the proposed method is
studied by a classical convection-diffusion BED example, and the hybrid
framework enabled by AD-EKI efficiently identifies informative data to
calibrate the model discrepancy and robustly infers the unknown physical
parameters in the modeled system. Besides addressing the challenges of BED with
model discrepancy, AD-EKI also potentially fosters efficient and scalable
frameworks in many other areas with bilevel optimization, such as meta-learning
and structure optimization.

</details>


### [74] [Generative Learning for Slow Manifolds and Bifurcation Diagrams](https://arxiv.org/abs/2504.20375)
*Ellis R. Crabtree,Dimitris G. Giovanis,Nikolaos Evangelou,Juan M. Bello-Rivas,Ioannis G. Kevrekidis*

Main category: cs.LG

TL;DR: 该论文提出了一个利用条件得分生成模型（cSGMs）快速初始化多时间尺度系统的低维慢流形，以及近似分岔图中稳态的框架。


<details>
  <summary>Details</summary>
Motivation: 在具有时间尺度分离的动态系统中，慢流形的近似和稳态的采样对建模和算法至关重要。本文旨在利用cSGMs的能力来高效生成与特定条件一致的数据样本，以辅助慢流形和分岔图的研究。

Method: 使用条件得分生成模型（cSGMs）生成与慢流形或分岔图中稳态一致的数据样本，从而实现快速初始化和近似。

Result: 该方法能够高效生成与指定条件一致的数据样本，帮助揭示慢流形的几何结构并填补分岔图中的缺失稳态。

Conclusion: 该框架为多时间尺度系统和分岔图的研究提供了一种高效的新方法，展示了cSGMs在非线性动力学中的潜力。

Abstract: In dynamical systems characterized by separation of time scales, the
approximation of so called ``slow manifolds'', on which the long term dynamics
lie, is a useful step for model reduction. Initializing on such slow manifolds
is a useful step in modeling, since it circumvents fast transients, and is
crucial in multiscale algorithms alternating between fine scale (fast) and
coarser scale (slow) simulations. In a similar spirit, when one studies the
infinite time dynamics of systems depending on parameters, the system
attractors (e.g., its steady states) lie on bifurcation diagrams. Sampling
these manifolds gives us representative attractors (here, steady states of ODEs
or PDEs) at different parameter values. Algorithms for the systematic
construction of these manifolds are required parts of the ``traditional''
numerical nonlinear dynamics toolkit.
  In more recent years, as the field of Machine Learning develops, conditional
score-based generative models (cSGMs) have demonstrated capabilities in
generating plausible data from target distributions that are conditioned on
some given label. It is tempting to exploit such generative models to produce
samples of data distributions conditioned on some quantity of interest (QoI).
In this work, we present a framework for using cSGMs to quickly (a) initialize
on a low-dimensional (reduced-order) slow manifold of a multi-time-scale system
consistent with desired value(s) of a QoI (a ``label'') on the manifold, and
(b) approximate steady states in a bifurcation diagram consistent with a (new,
out-of-sample) parameter value. This conditional sampling can help uncover the
geometry of the reduced slow-manifold and/or approximately ``fill in'' missing
segments of steady states in a bifurcation diagram.

</details>


### [75] [Manifold Clustering with Schatten p-norm Maximization](https://arxiv.org/abs/2504.20390)
*Fangfang Li,Quanxue Gao*

Main category: cs.LG

TL;DR: 论文提出了一种新的流形聚类框架，通过结合K-means和流形学习并利用标签指导数据结构的形成，确保了数据结构和标签的一致性，同时还通过优化Schatten p-范数保持类别平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只关注K-means和流形学习的结合，而忽略了数据结构与标签之间的一致性。为了解决这一问题，论文深入探讨了两者的关系，并提出了一种新的融合方法。

Method: 算法利用标签指导流形结构的形成，并在其上进行聚类，确保数据结构与标签的一致性。通过最大化标签的Schatten p-范数来保持类别平衡，并提供了理论证明。框架支持多种距离函数，适用于非线性可分数据。

Result: 多个数据集的实验结果验证了所提模型的优越性。

Conclusion: 论文提出的聚类框架不仅解决了数据结构与标签一致性的问题，还通过灵活的设计提高了对非线性可分数据的处理能力，实验结果证实了其有效性。

Abstract: Manifold clustering, with its exceptional ability to capture complex data
structures, holds a pivotal position in cluster analysis. However, existing
methods often focus only on finding the optimal combination between K-means and
manifold learning, and overlooking the consistency between the data structure
and labels. To address this issue, we deeply explore the relationship between
K-means and manifold learning, and on this basis, fuse them to develop a new
clustering framework. Specifically, the algorithm uses labels to guide the
manifold structure and perform clustering on it, which ensures the consistency
between the data structure and labels. Furthermore, in order to naturally
maintain the class balance in the clustering process, we maximize the Schatten
p-norm of labels, and provide a theoretical proof to support this.
Additionally, our clustering framework is designed to be flexible and
compatible with many types of distance functions, which facilitates efficient
processing of nonlinear separable data. The experimental results of several
databases confirm the superiority of our proposed model.

</details>


### [76] [FourierSpecNet: Neural Collision Operator Approximation Inspired by the Fourier Spectral Method for Solving the Boltzmann Equation](https://arxiv.org/abs/2504.20408)
*Jae Yong Lee,Gwang Jae Jung,Byung Chan Lim,Hyung Ju Hwang*

Main category: cs.LG

TL;DR: 提出了一种名为FourierSpecNet的混合框架，将傅里叶谱方法与深度学习结合，高效近似玻尔兹曼方程中的碰撞算子，支持零样本超分辨率，并在多个基准测试中展示了其准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 玻尔兹曼方程的数值解在高维速度域和非弹性碰撞情况下计算成本高昂，需要一种更高效的求解方法。

Method: 采用傅里叶谱方法与深度学习结合的混合框架FourierSpecNet，在傅里叶空间中近似碰撞算子，实现分辨率不变学习和零样本超分辨率。

Result: 在多个基准测试（包括Maxwellian和硬球分子模型及非弹性碰撞场景）中，FourierSpecNet在保持高精度的同时显著降低了计算成本。

Conclusion: FourierSpecNet为求解弹性和非弹性碰撞的玻尔兹曼方程提供了一种鲁棒且可扩展的替代方法。

Abstract: The Boltzmann equation, a fundamental model in kinetic theory, describes the
evolution of particle distribution functions through a nonlinear,
high-dimensional collision operator. However, its numerical solution remains
computationally demanding, particularly for inelastic collisions and
high-dimensional velocity domains. In this work, we propose the Fourier Neural
Spectral Network (FourierSpecNet), a hybrid framework that integrates the
Fourier spectral method with deep learning to approximate the collision
operator in Fourier space efficiently. FourierSpecNet achieves
resolution-invariant learning and supports zero-shot super-resolution, enabling
accurate predictions at unseen resolutions without retraining. Beyond empirical
validation, we establish a consistency result showing that the trained operator
converges to the spectral solution as the discretization is refined. We
evaluate our method on several benchmark cases, including Maxwellian and
hard-sphere molecular models, as well as inelastic collision scenarios. The
results demonstrate that FourierSpecNet offers competitive accuracy while
significantly reducing computational cost compared to traditional spectral
solvers. Our approach provides a robust and scalable alternative for solving
the Boltzmann equation across both elastic and inelastic regimes.

</details>


### [77] [ADiff4TPP: Asynchronous Diffusion Models for Temporal Point Processes](https://arxiv.org/abs/2504.20411)
*Amartya Mukherjee,Ruizhi Deng,He Zhao,Yuzhen Mao,Leonid Sigal,Frederick Tung*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于异步噪声调度扩散模型的新方法，用于建模时序点过程，通过优化噪声调度，在预测远未来事件时表现出色，并在基准数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 时序点过程的建模一直是时间序列预测中的重要挑战，尤其是在长时预测任务中。传统方法难以同时高效地生成早期和晚期事件。本文旨在通过设计一种异步噪声调度的扩散模型来解决这一问题。

Method: 该方法利用扩散模型和异步噪声调度，在不同时间步骤对数据的不同部分注入不同尺度的噪声。通过条件流匹配训练模型，优化噪声调度以实现更快的早期事件生成，从而为远期事件预测提供更强的条件。

Result: 在基准数据集上，该方法在预测下一个事件时间和类型方面达到了最先进的性能，并在长时预测任务中超越了现有基线方法。此外，该方法能灵活适应不同预测窗口长度。

Conclusion: 研究表明，异步噪声调度的扩散模型在时序点过程建模中具有显著优势，尤其是在长时预测任务中的灵活性和高性能使其成为解决类似问题的有效工具。

Abstract: This work introduces a novel approach to modeling temporal point processes
using diffusion models with an asynchronous noise schedule. At each step of the
diffusion process, the noise schedule injects noise of varying scales into
different parts of the data. With a careful design of the noise schedules,
earlier events are generated faster than later ones, thus providing stronger
conditioning for forecasting the more distant future. We derive an objective to
effectively train these models for a general family of noise schedules based on
conditional flow matching. Our method models the joint distribution of the
latent representations of events in a sequence and achieves state-of-the-art
results in predicting both the next inter-event time and event type on
benchmark datasets. Additionally, it flexibly accommodates varying lengths of
observation and prediction windows in different forecasting settings by
adjusting the starting and ending points of the generation process. Finally,
our method shows superior performance in long-horizon prediction tasks,
outperforming existing baseline methods.

</details>


### [78] [Understanding GNNs and Homophily in Dynamic Node Classification](https://arxiv.org/abs/2504.20421)
*Michael Ito,Danai Koutra,Jenna Wiens*

Main category: cs.LG

TL;DR: 论文探讨了动态图中的同质性（homophily），提出了动态同质性度量，并证明其对图神经网络（GNN）性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有同质性研究仅针对静态图，而动态图中的同质性及其对GNN性能的影响尚未充分探索。

Method: 通过理论分析动态图中图卷积网络（GCN）的判别性能，提出动态同质性度量，并在多组动态节点分类数据集上验证。

Result: 动态同质性与GNN性能相关，且当前流行的GNN在低动态同质性下表现不佳。

Conclusion: 该研究为动态节点分类中的同质性和GNN性能理解迈出了重要一步，为设计更强大的动态GNN提供了启示。

Abstract: Homophily, as a measure, has been critical to increasing our understanding of
graph neural networks (GNNs). However, to date this measure has only been
analyzed in the context of static graphs. In our work, we explore homophily in
dynamic settings. Focusing on graph convolutional networks (GCNs), we
demonstrate theoretically that in dynamic settings, current GCN discriminative
performance is characterized by the probability that a node's future label is
the same as its neighbors' current labels. Based on this insight, we propose
dynamic homophily, a new measure of homophily that applies in the dynamic
setting. This new measure correlates with GNN discriminative performance and
sheds light on how to potentially design more powerful GNNs for dynamic graphs.
Leveraging a variety of dynamic node classification datasets, we demonstrate
that popular GNNs are not robust to low dynamic homophily. Going forward, our
work represents an important step towards understanding homophily and GNN
performance in dynamic node classification.

</details>


### [79] [Learning Laplacian Positional Encodings for Heterophilous Graphs](https://arxiv.org/abs/2504.20430)
*Michael Ito,Jiong Zhu,Dexiong Chen,Danai Koutra,Jenna Wiens*

Main category: cs.LG

TL;DR: 该论文提出了一种新的图位置编码方法LLPE，适用于异质性图任务，显著提升了多种GNN模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图位置编码（PEs）在异质性图中可能无效甚至有害，而现实网络中异质性普遍存在，亟需一种能同时适用同质和异质图的PE方法。

Method: 提出LLPE（Learnable Laplacian Positional Encodings），利用图拉普拉斯的全频谱，以捕捉同质和异质图的结构信息。

Result: 理论证明LLPE能逼近一类通用图距离，12个基准测试显示其性能提升高达35%（合成图）和14%（真实图）。

Conclusion: LLPE是开发适应异质图复杂结构的PE的重要进展，显著提升了GNN的泛化能力。

Abstract: In this work, we theoretically demonstrate that current graph positional
encodings (PEs) are not beneficial and could potentially hurt performance in
tasks involving heterophilous graphs, where nodes that are close tend to have
different labels. This limitation is critical as many real-world networks
exhibit heterophily, and even highly homophilous graphs can contain local
regions of strong heterophily. To address this limitation, we propose Learnable
Laplacian Positional Encodings (LLPE), a new PE that leverages the full
spectrum of the graph Laplacian, enabling them to capture graph structure on
both homophilous and heterophilous graphs. Theoretically, we prove LLPE's
ability to approximate a general class of graph distances and demonstrate its
generalization properties. Empirically, our evaluation on 12 benchmarks
demonstrates that LLPE improves accuracy across a variety of GNNs, including
graph transformers, by up to 35% and 14% on synthetic and real-world graphs,
respectively. Going forward, our work represents a significant step towards
developing PEs that effectively capture complex structures in heterophilous
graphs.

</details>


### [80] [GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2504.20437)
*DiJia Su,Andrew Gu,Jane Xu,Yuandong Tian,Jiawei Zhao*

Main category: cs.LG

TL;DR: GaLore 2改进GaLore框架，解决SVD计算开销与并行训练兼容问题，通过预训练Llama 7B验证其高效性与扩展性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练中内存瓶颈显著，GaLore虽通过低秩梯度结构节省内存，但仍面临SVD计算开销高与并行策略兼容性不足的挑战。

Method: 提出GaLore 2，优化低秩投影效率并整合前沿并行技术（如FSDP），支持低比特量化和高阶张量结构。

Result: 成功预训练Llama 7B（5000亿token），证明其可扩展性与实际应用潜力，内存节省且性能无损。

Conclusion: GaLore 2为LLM训练提供高效、可扩展的解决方案，平衡内存效率与计算性能。

Abstract: Large language models (LLMs) have revolutionized natural language
understanding and generation but face significant memory bottlenecks during
training. GaLore, Gradient Low-Rank Projection, addresses this issue by
leveraging the inherent low-rank structure of weight gradients, enabling
substantial memory savings without sacrificing performance. Recent works
further extend GaLore from various aspects, including low-bit quantization and
higher-order tensor structures. However, there are several remaining challenges
for GaLore, such as the computational overhead of SVD for subspace updates and
the integration with state-of-the-art training parallelization strategies
(e.g., FSDP). In this paper, we present GaLore 2, an efficient and scalable
GaLore framework that addresses these challenges and incorporates recent
advancements. In addition, we demonstrate the scalability of GaLore 2 by
pre-training Llama 7B from scratch using up to 500 billion training tokens,
highlighting its potential impact on real LLM pre-training scenarios.

</details>


### [81] [Multidimensional precipitation index prediction based on CNN-LSTM hybrid framework](https://arxiv.org/abs/2504.20442)
*Yuchen Wang,Pengfei Jia,Zhitao Shu,Keyan Liu,Abdul Rashid Mohamed Shariff*

Main category: cs.LG

TL;DR: 该研究提出了一种基于CNN-LSTM混合框架的多维降水指数预测模型，以提高降水预报的准确性，实验结果显示优于传统时间序列预测方法。


<details>
  <summary>Details</summary>
Motivation: 准确的降水预测对防灾减灾至关重要。

Method: 使用CNN-LSTM混合框架分析长期降水时间序列数据，捕捉局部特征和长期依赖。

Result: 模型RMSE为6.752，优于传统方法。

Conclusion: 模型在精度上有优势，但计算资源需求高，多维数据预测能力有待提升。

Abstract: With the intensification of global climate change, accurate prediction of
weather indicators is of great significance in disaster prevention and
mitigation, agricultural production, and transportation. Precipitation, as one
of the key meteorological indicators, plays a crucial role in water resource
management, agricultural production, and urban flood control. This study
proposes a multidimensional precipitation index prediction model based on a
CNN- LSTM hybrid framework, aiming to improve the accuracy of precipitation
forecasts. The dataset is sourced from Pune, Maharashtra, India, covering
monthly mean precipitation data from 1972 to 2002. This dataset includes nearly
31 years (1972-2002) of monthly average precipitation, reflecting the long-term
fluctuations and seasonal variations of precipitation in the region. By
analyzing these time series data, the CNN-LSTM model effectively captures local
features and long-term dependencies. Experimental results show that the model
achieves a root mean square error (RMSE) of 6.752, which demonstrates a
significant advantage over traditional time series prediction methods in terms
of prediction accuracy and generalization ability. Furthermore, this study
provides new research ideas for precipitation prediction. However, the model
requires high computational resources when dealing with large-scale datasets,
and its predictive ability for multidimensional precipitation data still needs
improvement. Future research could extend the model to support and predict
multidimensional precipitation data, thereby promoting the development of more
accurate and efficient meteorological prediction technologies.

</details>


### [82] [FT-MoE: Sustainable-learning Mixture of Experts Model for Fault-Tolerant Computing with Multiple Tasks](https://arxiv.org/abs/2504.20446)
*Wenjing Xiao,Wenhao Song,Miaojiang Chen,Ruikun Luo,Min Chen*

Main category: cs.LG

TL;DR: FT-MoE 是一种可持续学习的混合专家模型，用于智能容错计算，通过多任务学习提升故障检测与分类性能，并在动态服务环境中持续学习。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的容错算法因故障知识的异构性和时间序列日志数据的复杂依赖关系，难以进一步提升检测性能，因此需要一种更高效的模型。

Method: 1) 使用基于解码器的Transformer模型提取解耦长距离依赖的故障原型向量；2) 设计双混合专家网络实现高精度故障检测与分类；3) 提出离线和在线两阶段优化方案。

Result: 实验结果表明，FT-MoE在容错基准测试中优于现有状态方法。

Conclusion: FT-MoE通过多任务学习和持续优化，显著提升了容错计算的可靠性和适应性。

Abstract: Intelligent fault-tolerant (FT) computing has recently demonstrated
significant advantages of predicting and diagnosing faults in advance, enabling
reliable service delivery. However, due to heterogeneity of fault knowledge and
complex dependence relationships of time series log data, existing deep
learning-based FT algorithms further improve detection performance relying on
single neural network model with difficulty. To this end, we propose FT-MoE, a
sustainable-learning mixture-of-experts model for fault-tolerant computing with
multiple tasks, which enables different parameters learning distinct fault
knowledge to achieve high-reliability for service system. Firstly, we use
decoder-based transformer models to obtain fault prototype vectors of
decoupling long-distance dependencies. Followed by, we present a dual mixture
of experts networks for high-accurate prediction for both fault detection and
classification tasks. Then, we design a two-stage optimization scheme of
offline training and online tuning, which allows that in operation FT-MoE can
also keep learning to adapt to dynamic service environments. Finally, to verify
the effectiveness of FT-MoE, we conduct extensive experiments on the FT
benchmark. Experimental results show that FT-MoE achieves superior performance
compared to the state-of-the-art methods. Code will be available upon
publication.

</details>


### [83] [Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding](https://arxiv.org/abs/2504.20456)
*Gabe Guo,Stefano Ermon*

Main category: cs.LG

TL;DR: AS-ARMs（任意子集自回归模型）通过并行生成和修正令牌分布，提供了解决语言模型中并行采样问题的方案。


<details>
  <summary>Details</summary>
Motivation: 传统离散扩散模型在并行生成令牌时，由于条件独立性假设，分布会偏离原始数据分布，需要新的方法来解决这一问题。

Method: 提出了AS-ARMs及其配套的ASSD算法，支持并行生成和修正令牌分布，并通过数学证明和实验验证其有效性。

Result: ASSD在生成速度和生成质量上均有显著提升，小参数模型在填充任务上表现优异，代码生成性能接近更大模型。

Conclusion: AS-ARMs是语言建模中一个有前景的方向，解决了并行采样问题，并在性能上达到或超越现有方法。

Abstract: In arbitrary-order language models, it is an open question how to sample
tokens in parallel from the correct joint distribution. With discrete diffusion
models, the more tokens they generate in parallel, the less their predicted
distributions adhere to the originally learned data distribution, as they rely
on a conditional independence assumption that only works with infinitesimally
small timesteps. We find that a different class of models, any-subset
autoregressive models (AS-ARMs), holds the solution. As implied by the name,
AS-ARMs can generate tokens in any order, and in parallel. Moreover, AS-ARMs
support parallelized joint probability density estimation, allowing them to
correct their own parallel-generated token distributions, via our Any-Subset
Speculative Decoding (ASSD) algorithm. ASSD provably enables generation of
tokens from the correct joint distribution, with the number of neural network
calls upper bounded by the number of tokens predicted. We empirically verify
that ASSD speeds up language generation, without sacrificing quality.
Furthermore, we provide a mathematically justified scheme for training AS-ARMs
for generation, and show that AS-ARMs achieve state-of-the-art performance
among sub-200M parameter models on infilling benchmark tasks, and nearly match
the performance of models 50X larger on code generation. Our theoretical and
empirical results indicate that the once-forgotten AS-ARMs are a promising
direction of language modeling.

</details>


### [84] [The Estimation of Continual Causal Effect for Dataset Shifting Streams](https://arxiv.org/abs/2504.20471)
*Baining Chen,Yiming Zhang,Yuqiao Han,Ruyue Zhang,Ruihuan Du,Zhishuo Zhou,Zhengdan Zhu,Xun Liu,Jiecheng Guo*

Main category: cs.LG

TL;DR: 论文提出ICE-PKD框架，通过增量训练和知识蒸馏解决营销优化中因果效应估计的时间数据集偏移问题，并在实际平台中验证其效果。


<details>
  <summary>Details</summary>
Motivation: 营销优化中因果效应估计的现有框架在时间数据集偏移（用户行为和领域分布随时间变化）时性能不足，需改进以适应在线环境。

Method: 提出ICE-PKD框架，包含两部分：（1）多处理提升网络，通过反事实回归消除混杂偏差；（2）增量训练策略，通过最新数据更新模型并基于回放的知识蒸馏保护泛化能力。

Result: 在模拟和在线数据集上的实验表明，ICE-PKD框架性能更优，并已在中国某网约车平台营销系统中部署。

Conclusion: ICE-PKD框架有效解决了时间数据集偏移问题，提升了因果效应估计的在线性能，具有实际应用价值。

Abstract: Causal effect estimation has been widely used in marketing optimization. The
framework of an uplift model followed by a constrained optimization algorithm
is popular in practice. To enhance performance in the online environment, the
framework needs to be improved to address the complexities caused by temporal
dataset shift. This paper focuses on capturing the dataset shift from user
behavior and domain distribution changing over time. We propose an Incremental
Causal Effect with Proxy Knowledge Distillation (ICE-PKD) framework to tackle
this challenge. The ICE-PKD framework includes two components: (i) a
multi-treatment uplift network that eliminates confounding bias using
counterfactual regression; (ii) an incremental training strategy that adapts to
the temporal dataset shift by updating with the latest data and protects
generalization via replay-based knowledge distillation. We also revisit the
uplift modeling metrics and introduce a novel metric for more precise online
evaluation in multiple treatment scenarios. Extensive experiments on both
simulated and online datasets show that the proposed framework achieves better
performance. The ICE-PKD framework has been deployed in the marketing system of
Huaxiaozhu, a ride-hailing platform in China.

</details>


### [85] [Group Relative Knowledge Distillation: Learning from Teacher's Relational Inductive Bias](https://arxiv.org/abs/2504.20482)
*Chao Li,Changhua Zhou,Jia Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新的知识蒸馏框架GRKD，通过关注类间相对排名而非绝对概率来提高学生模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法主要关注绝对概率的模仿，忽略了教师模型中相对预测的关系归纳偏差，导致暴露偏差。

Method: 提出了基于组相对损失（Group Relative Loss）的GRKD框架，使学生模型学习教师输出的类间偏好排序。

Result: 在分类基准测试中，GRKD表现出优于现有方法的泛化能力，尤其在细粒度分类任务中效果显著。

Conclusion: GRKD通过利用教师模型的关系结构而非绝对概率，为知识蒸馏提供了新视角。

Abstract: Knowledge distillation typically transfers knowledge from a teacher model to
a student model by minimizing differences between their output distributions.
However, existing distillation approaches largely focus on mimicking absolute
probabilities and neglect the valuable relational inductive biases embedded in
the teacher's relative predictions, leading to exposure bias. In this paper, we
propose Group Relative Knowledge Distillation (GRKD), a novel framework that
distills teacher knowledge by learning the relative ranking among classes,
rather than directly fitting the absolute distribution. Specifically, we
introduce a group relative loss that encourages the student model to preserve
the pairwise preference orderings provided by the teacher's outputs. Extensive
experiments on classification benchmarks demonstrate that GRKD achieves
superior generalization compared to existing methods, especially in tasks
requiring fine-grained class differentiation. Our method provides a new
perspective on exploiting teacher knowledge, focusing on relational structure
rather than absolute likelihood.

</details>


### [86] [Wavelet-Filtering of Symbolic Music Representations for Folk Tune Segmentation and Classification](https://arxiv.org/abs/2504.20522)
*Gissel Velarde,Tillman Weyde,David Meredith*

Main category: cs.LG

TL;DR: 该研究评估了一种基于Haar小波滤波的机器学习方法，用于对民间歌曲的符号表示进行分割和分类，并与之前的格式塔方法比较，结果显示小波方法在优化参数后具有更高的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 动机在于探索小波变换在民间歌曲分类中的有效性，尤其是与传统格式塔方法相比，以提高分类准确性。

Method: 方法包括使用Haar小波进行连续小波变换（CWT），通过局部最大值进行分割，并基于k-最近邻和标准向量度量（欧几里得、城市街区）分类，与直接应用于音高信号的格式塔方法对比。

Result: 结果显示，优化时间尺度和参数后，基于小波的分割和滤波能提高交叉验证中的分类准确率。

Conclusion: 结论是小波方法在民间歌曲分类中优于格式塔方法，特别是在参数优化后表现更佳。

Abstract: The aim of this study is to evaluate a machine-learning method in which
symbolic representations of folk songs are segmented and classified into tune
families with Haar-wavelet filtering. The method is compared with previously
proposed Gestalt-based method. Melodies are represented as discrete symbolic
pitch-time signals. We apply the continuous wavelet transform (CWT) with the
Haar wavelet at specific scales, obtaining filtered versions of melodies
emphasizing their information at particular time-scales. We use the filtered
signal for representation and segmentation, using the wavelet coefficients'
local maxima to indicate local boundaries and classify segments by means of
k-nearest neighbours based on standard vector-metrics (Euclidean, cityblock),
and compare the results to a Gestalt-based segmentation method and metrics
applied directly to the pitch signal. We found that the wavelet based
segmentation and wavelet-filtering of the pitch signal lead to better
classification accuracy in cross-validated evaluation when the time-scale and
other parameters are optimized.

</details>


### [87] [DeeP-Mod: Deep Dynamic Programming based Environment Modelling using Feature Extraction](https://arxiv.org/abs/2504.20535)
*Chris Child,Lam Ngo*

Main category: cs.LG

TL;DR: DeeP-Mod框架通过DDPN提取特征构建环境模型，避免DQN中状态信息丢失问题，实现了任务和动作集的独立性，并在噪声环境下表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决DQN在深层网络中因混合状态-动作表示导致状态信息丢失的问题，提出利用DP训练DDPN以保留状态信息。

Method: 使用DP训练DDPN，通过Value Iteration确保输出为状态值而非状态-动作对；从DDPN提取特征构建环境模型，并训练第二DDPN直接从特征模型学习。

Result: DeeP-Mod在噪声环境下收敛更快且性能优于原DDPN，无需外部定义环境模型即可实现广泛适用性。

Conclusion: DeeP-Mod通过特征提取和内部环境建模，显著提升了DP在复杂环境中的适用性和性能。

Abstract: The DeeP-Mod framework builds an environment model using features from a Deep
Dynamic Programming Network (DDPN), trained via a Deep Q-Network (DQN). While
Deep Q-Learning is effective in decision-making, state information is lost in
deeper DQN layers due to mixed state-action representations. We address this by
using Dynamic Programming (DP) to train a DDPN, where Value Iteration ensures
the output represents state values, not state-action pairs. Extracting features
from the DDPN preserves state information, enabling task and action set
independence. We show that a reduced DDPN can be trained using features
extracted from the original DDPN trained on an identical problem. This reduced
DDPN achieves faster convergence under noise and outperforms the original DDPN.
Finally, we introduce the DeeP-Mod framework, which creates an environment
model using the evolution of features extracted from a DDPN in response to
actions. A second DDPN, which learns directly from this feature model rather
than raw states, can learn an effective feature-value representation and thus
optimal policy. A key advantage of DeeP-Mod is that an externally defined
environment model is not needed at any stage, making DDPN applicable to a wide
range of environments.

</details>


### [88] [Inclusive Training Separation and Implicit Knowledge Interaction for Balanced Online Class-Incremental Learning](https://arxiv.org/abs/2504.20566)
*Shunjie Wen,Thomas Heinis,Dong-Wan Choi*

Main category: cs.LG

TL;DR: 该研究提出了一种名为BOIL的新型在线增量学习方法，通过双分类器策略有效平衡新旧类别知识的学习，显著提升了模型的稳定性和可塑性。


<details>
  <summary>Details</summary>
Motivation: 在线类别增量学习（OCIL）需要在学习新类别的同时保持旧类别的知识，现有方法难以平衡稳定性和可塑性。

Method: 提出BOIL方法，采用双分类器的包容性训练分离策略，并通过隐式知识转移整合新旧类别知识。

Result: 在三个OCIL基准数据集上的实验表明，BOIL在性能和平衡性上均优于现有方法。

Conclusion: BOIL方法通过双分类器策略有效解决了OCIL中的知识平衡问题，为未来研究提供了新思路。

Abstract: Online class-incremental learning (OCIL) focuses on gradually learning new
classes (called plasticity) from a stream of data in a single-pass, while
concurrently preserving knowledge of previously learned classes (called
stability). The primary challenge in OCIL lies in maintaining a good balance
between the knowledge of old and new classes within the continually updated
model. Most existing methods rely on explicit knowledge interaction through
experience replay, and often employ exclusive training separation to address
bias problems. Nevertheless, it still remains a big challenge to achieve a
well-balanced learner, as these methods often exhibit either reduced plasticity
or limited stability due to difficulties in continually integrating knowledge
in the OCIL setting. In this paper, we propose a novel replay-based method,
called Balanced Online Incremental Learning (BOIL), which can achieve both high
plasticity and stability, thus ensuring more balanced performance in OCIL. Our
BOIL method proposes an inclusive training separation strategy using dual
classifiers so that knowledge from both old and new classes can effectively be
integrated into the model, while introducing implicit approaches for
transferring knowledge across the two classifiers. Extensive experimental
evaluations over three widely-used OCIL benchmark datasets demonstrate the
superiority of BOIL, showing more balanced yet better performance compared to
state-of-the-art replay-based OCIL methods.

</details>


### [89] [Reinforcement Learning for Reasoning in Large Language Models with One Training Example](https://arxiv.org/abs/2504.20571)
*Yiping Wang,Qing Yang,Zhiyuan Zeng,Liliang Ren,Lucas Liu,Baolin Peng,Hao Cheng,Xuehai He,Kuan Wang,Jianfeng Gao,Weizhu Chen,Shuohang Wang,Simon Shaolei Du,Yelong Shen*

Main category: cs.LG

TL;DR: 论文展示了一种通过单个训练样本（1-shot RLVR）的强化学习方法，有效提升了大型语言模型在数学推理任务上的性能，并发现了包括跨领域泛化和后饱和泛化在内的有趣现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机是通过极少的训练样本（如单个示例）来验证强化学习在提升语言模型数学推理能力上的有效性，以探索数据高效性的RLVR方法。

Method: 研究方法包括应用1-shot RLVR到不同模型（如Qwen2.5-Math-1.5B），使用不同RL算法（如GRPO和PPO），并分析训练过程中的现象（如政策梯度损失和探索促进的作用）。

Result: 实验结果显示，单个示例将MATH500的性能从36.0%提升至73.6%，平均性能从17.6%提升至35.7%，且在不同模型和RL算法中均观察到类似显著提升。

Conclusion: 论文的结论是1-shot RLVR在极少数据下显著提升模型性能，其效果主要源于政策梯度损失而非“顿悟”现象，同时强调了探索促进的重要性。

Abstract: We show that reinforcement learning with verifiable reward using one training
example (1-shot RLVR) is effective in incentivizing the math reasoning
capabilities of large language models (LLMs). Applying RLVR to the base model
Qwen2.5-Math-1.5B, we identify a single example that elevates model performance
on MATH500 from 36.0% to 73.6%, and improves the average performance across six
common mathematical reasoning benchmarks from 17.6% to 35.7%. This result
matches the performance obtained using the 1.2k DeepScaleR subset (MATH500:
73.6%, average: 35.9%), which includes the aforementioned example. Similar
substantial improvements are observed across various models (Qwen2.5-Math-7B,
Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and
PPO), and different math examples (many of which yield approximately 30% or
greater improvement on MATH500 when employed as a single training example). In
addition, we identify some interesting phenomena during 1-shot RLVR, including
cross-domain generalization, increased frequency of self-reflection, and
sustained test performance improvement even after the training accuracy has
saturated, a phenomenon we term post-saturation generalization. Moreover, we
verify that the effectiveness of 1-shot RLVR primarily arises from the policy
gradient loss, distinguishing it from the "grokking" phenomenon. We also show
the critical role of promoting exploration (e.g., by adding entropy loss with
an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe
that applying entropy loss alone, without any outcome reward, significantly
enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings
can inspire future work on RLVR data efficiency and encourage a re-examination
of both recent progress and the underlying mechanisms in RLVR. Our code, model,
and data are open source at https://github.com/ypwang61/One-Shot-RLVR

</details>


### [90] [Digital Shielding for Cross-Domain Wi-Fi Signal Adaptation using Relativistic Average Generative Adversarial Network](https://arxiv.org/abs/2504.20568)
*Danilo Avola,Federica Bruni,Gian Luca Foresti,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的Wi-Fi信号跨域适应模型，利用RaGAN和Bi-LSTM结构，结合电磁屏蔽模拟环境，实现了96%的准确率，适用于安全领域的隐蔽物体识别。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi传感技术因IEEE 802.11bf标准及隐私保护需求兴起，但环境变化会影响性能。研究旨在解决跨域泛化问题。

Method: 采用RaGAN和Bi-LSTM架构，通过模拟法拉第笼的电磁屏蔽环境收集信号频谱，训练多类SVM进行测试。

Result: 系统在去噪后的信号上达到96%准确率，展现出强材料区分能力。

Conclusion: 该模型在跨域Wi-Fi信号处理中表现优异，尤其在安全应用中识别隐蔽物体方面具有潜力。

Abstract: Wi-Fi sensing uses radio-frequency signals from Wi-Fi devices to analyze
environments, enabling tasks such as tracking people, detecting intrusions, and
recognizing gestures. The rise of this technology is driven by the IEEE
802.11bf standard and growing demand for tools that can ensure privacy and
operate through obstacles. However, the performance of Wi-Fi sensing is heavily
influenced by environmental conditions, especially when extracting spatial and
temporal features from the surrounding scene. A key challenge is achieving
robust generalization across domains, ensuring stable performance even when the
sensing environment changes significantly. This paper introduces a novel deep
learning model for cross-domain adaptation of Wi-Fi signals, inspired by
physical signal shielding. The model uses a Relativistic average Generative
Adversarial Network (RaGAN) with Bidirectional Long Short-Term Memory (Bi-LSTM)
architectures for both the generator and discriminator. To simulate physical
shielding, an acrylic box lined with electromagnetic shielding fabric was
constructed, mimicking a Faraday cage. Wi-Fi signal spectra were collected from
various materials both inside (domain-free) and outside (domain-dependent) the
box to train the model. A multi-class Support Vector Machine (SVM) was trained
on domain-free spectra and tested on signals denoised by the RaGAN. The system
achieved 96% accuracy and demonstrated strong material discrimination
capabilities, offering potential for use in security applications to identify
concealed objects based on their composition.

</details>


### [91] [Representation Learning Preserving Ignorability and Covariate Matching for Treatment Effects](https://arxiv.org/abs/2504.20579)
*Praharsh Nanavati,Ranjitha Prasad,Karthikeyan Shanmugam*

Main category: cs.LG

TL;DR: 该论文提出了一种结合神经架构的方法，旨在解决观测数据中的隐藏混杂和协变量不匹配问题，通过学习预处理协变量的有效调整表示，并在因果基准测试中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 观测数据中估计治疗效果面临两大挑战：隐藏混杂和协变量不匹配。现有方法通常只解决其中之一，缺乏统一框架。本文旨在填补这一空白。

Method: 结合两种神经架构：一种基于梯度匹配的领域自适应方法，利用因果侧信息；另一种为协变量匹配变换。通过近似不变表示生成有效调整集。

Result: 在IHDP、Jobs、Cattaneo和图像数据集上的实验表明，该方法在ATE和PEHE误差上优于基线模型，并提供了效应估计的可测试边界。

Conclusion: 论文提出的神经架构能同时处理隐藏混杂和协变量不匹配问题，为因果效应估计提供了更可靠的方法，并通过实验验证了其优越性。

Abstract: Estimating treatment effects from observational data is challenging due to
two main reasons: (a) hidden confounding, and (b) covariate mismatch (control
and treatment groups not having identical distributions). Long lines of works
exist that address only either of these issues. To address the former,
conventional techniques that require detailed knowledge in the form of causal
graphs have been proposed. For the latter, covariate matching and importance
weighting methods have been used. Recently, there has been progress in
combining testable independencies with partial side information for tackling
hidden confounding. A common framework to address both hidden confounding and
selection bias is missing. We propose neural architectures that aim to learn a
representation of pre-treatment covariates that is a valid adjustment and also
satisfies covariate matching constraints. We combine two different neural
architectures: one based on gradient matching across domains created by
subsampling a suitable anchor variable that assumes causal side information,
followed by the other, a covariate matching transformation. We prove that
approximately invariant representations yield approximate valid adjustment sets
which would enable an interval around the true causal effect. In contrast to
usual sensitivity analysis, where an unknown nuisance parameter is varied, we
have a testable approximation yielding a bound on the effect estimate. We also
outperform various baselines with respect to ATE and PEHE errors on causal
benchmarks that include IHDP, Jobs, Cattaneo, and an image-based Crowd
Management dataset.

</details>


### [92] [Independent Learning in Performative Markov Potential Games](https://arxiv.org/abs/2504.20593)
*Rilind Sahitaj,Paulius Sasnauskas,Yiğit Yalın,Debmalya Mandal,Goran Radanović*

Main category: cs.LG

TL;DR: 论文研究了多智能体执行性强化学习（PRL），将执行性效应引入马尔可夫势博弈（MPGs），提出了执行性稳定均衡（PSE）概念并证明其存在性，分析了IPGA和INPG算法的收敛性，并通过实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索在多智能体环境中，智能体的策略部署如何影响环境的奖励和动态，并分析算法在这种执行性效应下的收敛性。

Method: 方法包括引入执行性稳定均衡（PSE），分析独立策略梯度上升（IPGA）和独立自然策略梯度（INPG）算法的性能，并通过理论推导和实验验证其收敛性。

Result: 结果表明PSE在合理假设下始终存在，IPGA和INPG能收敛到近似PSE，且INPG具有渐近收敛性。当执行性效应消失时，结果与已有研究一致。

Conclusion: 结论是多智能体PRL框架下的PSE概念和分析为理解策略部署的环境影响提供了理论基础，算法收敛性结果进一步验证了方法的有效性。

Abstract: Performative Reinforcement Learning (PRL) refers to a scenario in which the
deployed policy changes the reward and transition dynamics of the underlying
environment. In this work, we study multi-agent PRL by incorporating
performative effects into Markov Potential Games (MPGs). We introduce the
notion of a performatively stable equilibrium (PSE) and show that it always
exists under a reasonable sensitivity assumption. We then provide convergence
results for state-of-the-art algorithms used to solve MPGs. Specifically, we
show that independent policy gradient ascent (IPGA) and independent natural
policy gradient (INPG) converge to an approximate PSE in the best-iterate
sense, with an additional term that accounts for the performative effects.
Furthermore, we show that INPG asymptotically converges to a PSE in the
last-iterate sense. As the performative effects vanish, we recover the
convergence rates from prior work. For a special case of our game, we provide
finite-time last-iterate convergence results for a repeated retraining
approach, in which agents independently optimize a surrogate objective. We
conduct extensive experiments to validate our theoretical findings.

</details>


### [93] [Bridging the Generalisation Gap: Synthetic Data Generation for Multi-Site Clinical Model Validation](https://arxiv.org/abs/2504.20635)
*Bradley Segal,Joshua Fieggen,David Clifton,Lei Clifton*

Main category: cs.LG

TL;DR: 作者提出了一种新的结构化合成数据框架，用于系统性评估临床机器学习模型的鲁棒性、公平性和泛化性，解决了现有方法数据有限且缺乏可控性的问题。


<details>
  <summary>Details</summary>
Motivation: 评估临床机器学习模型在不同医疗环境中的泛化性是一大挑战。现有方法依赖真实世界数据集，但这些数据有限且可能包含混杂偏见。同时，现有生成模型缺乏透明度和明确的控制，难以系统化实验。

Method: 作者设计了一种结构化合成数据框架，提供对数据生成过程的明确控制，包括特定场所的流行变化、层次性子群效应和结构化特征交互。通过这种方法，可以针对性地研究模型对分布变化和偏见的响应。

Result: 实验证明，该框架能够有效隔离场所变化的影响，支持公平性审核，并揭示模型泛化失败的情况，特别是模型复杂性与场所特异性效应之间的相互作用。

Conclusion: 该研究提供了一种可重现、可解释且可配置的工具，有助于提升机器学习在临床环境中的可靠部署。

Abstract: Ensuring the generalisability of clinical machine learning (ML) models across
diverse healthcare settings remains a significant challenge due to variability
in patient demographics, disease prevalence, and institutional practices.
Existing model evaluation approaches often rely on real-world datasets, which
are limited in availability, embed confounding biases, and lack the flexibility
needed for systematic experimentation. Furthermore, while generative models aim
for statistical realism, they often lack transparency and explicit control over
factors driving distributional shifts. In this work, we propose a novel
structured synthetic data framework designed for the controlled benchmarking of
model robustness, fairness, and generalisability. Unlike approaches focused
solely on mimicking observed data, our framework provides explicit control over
the data generating process, including site-specific prevalence variations,
hierarchical subgroup effects, and structured feature interactions. This
enables targeted investigation into how models respond to specific
distributional shifts and potential biases. Through controlled experiments, we
demonstrate the framework's ability to isolate the impact of site variations,
support fairness-aware audits, and reveal generalisation failures, particularly
highlighting how model complexity interacts with site-specific effects. This
work contributes a reproducible, interpretable, and configurable tool designed
to advance the reliable deployment of ML in clinical settings.

</details>


### [94] [Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition](https://arxiv.org/abs/2504.20938)
*Zhengfu He,Junxuan Wang,Rui Lin,Xuyang Ge,Wentao Shu,Qiong Tang,Junping Zhang,Xipeng Qiu*

Main category: cs.LG

TL;DR: 该论文提出了Lorsa（低秩稀疏注意力），作为Transformer注意力层的稀疏替代模型，旨在解耦多头自注意力（MHSA）为独立可理解的组件。Lorsa解决了注意力叠加问题，能更好地理解特征间交互。实验表明，Lorsa能发现更清晰、细粒度的MHSA行为（如归纳头、后继头），并在可解释性和电路发现方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决Transformer中多头自注意力（MHSA）的注意力叠加问题，通过设计稀疏模型Lorsa，将MHSA解耦为独立可理解的组件，从而更好地分析特征间交互。

Method: 提出Lorsa模型，将MHSA分解为稀疏的低秩组件，结合稀疏自编码器（SAE）方法，进行自动可解释性分析和电路发现。

Result: Lorsa能够识别更清晰的MHSA行为（如归纳头、算术头），在可解释性上与SAE相当，但在电路发现（尤其是多头部联合计算的特征）上表现更优。同时验证了Lorsa的扩展规律和设计消融实验。

Conclusion: Lorsa是一种有效的稀疏注意力模型，能提升Transformer的可解释性，尤其在多头部联合特征分析中表现突出，为注意力机制研究提供了新工具。

Abstract: We propose Low-Rank Sparse Attention (Lorsa), a sparse replacement model of
Transformer attention layers to disentangle original Multi Head Self Attention
(MHSA) into individually comprehensible components. Lorsa is designed to
address the challenge of attention superposition to understand
attention-mediated interaction between features in different token positions.
We show that Lorsa heads find cleaner and finer-grained versions of previously
discovered MHSA behaviors like induction heads, successor heads and attention
sink behavior (i.e., heavily attending to the first token). Lorsa and Sparse
Autoencoder (SAE) are both sparse dictionary learning methods applied to
different Transformer components, and lead to consistent findings in many ways.
For instance, we discover a comprehensive family of arithmetic-specific Lorsa
heads, each corresponding to an atomic operation in Llama-3.1-8B. Automated
interpretability analysis indicates that Lorsa achieves parity with SAE in
interpretability while Lorsa exhibits superior circuit discovery properties,
especially for features computed collectively by multiple MHSA heads. We also
conduct extensive experiments on architectural design ablation, Lorsa scaling
law and error analysis.

</details>


### [95] [Decision-centric fairness: Evaluation and optimization for resource allocation problems](https://arxiv.org/abs/2504.20642)
*Simon De Vos,Jente Van Belle,Andres Algaba,Wouter Verbeke,Sam Verboven*

Main category: cs.LG

TL;DR: 本文提出了一种决策中心公平性方法，仅在决策区域内（即实际用于资源分配决策的阈值范围）引入公平性，而非全局公平性，以避免过度限制模型预测质量。


<details>
  <summary>Details</summary>
Motivation: 为了解决二元分类模型在资源分配（如贷款或客户保留）中可能对特定人口群体产生歧视的问题，尤其是在正结果分数预测中，作者希望提出一种更有效的公平性方法。

Method: 提出决策中心公平性方法，仅在相关决策阈值范围内强制执行公平性（如人口平等性），而不是全局范围，以减少对模型预测质量的不必要限制。

Result: 通过在半合成数据集上的实验，对比全局公平性方法，证明了决策中心方法的优势，尤其是在公平性真正影响决策的场景中。

Conclusion: 决策中心公平性方法能够在不牺牲模型预测质量的情况下实现公平性，适用于资源分配决策的关键场景。

Abstract: Data-driven decision support tools play an increasingly central role in
decision-making across various domains. In this work, we focus on binary
classification models for predicting positive-outcome scores and deciding on
resource allocation, e.g., credit scores for granting loans or churn propensity
scores for targeting customers with a retention campaign. Such models may
exhibit discriminatory behavior toward specific demographic groups through
their predicted scores, potentially leading to unfair resource allocation. We
focus on demographic parity as a fairness metric to compare the proportions of
instances that are selected based on their positive outcome scores across
groups. In this work, we propose a decision-centric fairness methodology that
induces fairness only within the decision-making region -- the range of
relevant decision thresholds on the score that may be used to decide on
resource allocation -- as an alternative to a global fairness approach that
seeks to enforce parity across the entire score distribution. By restricting
the induction of fairness to the decision-making region, the proposed
decision-centric approach avoids imposing overly restrictive constraints on the
model, which may unnecessarily degrade the quality of the predicted scores. We
empirically compare our approach to a global fairness approach on multiple
(semi-synthetic) datasets to identify scenarios in which focusing on fairness
where it truly matters, i.e., decision-centric fairness, proves beneficial.

</details>


### [96] [Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified File Selection](https://arxiv.org/abs/2504.20644)
*Ziqing Fan,Siyuan Du,Shengchao Hu,Pingjie Wang,Li Shen,Ya Zhang,Dacheng Tao,Yanfeng Wang*

Main category: cs.LG

TL;DR: 论文提出了DiSF算法，通过选择特征空间中解相关的文本文件来增强多样性，显著提升LLM的整体性能。


<details>
  <summary>Details</summary>
Motivation: 当前预训练数据选择方法存在多样性问题（维度坍塌），需改进以提高通用性能。

Method: 提出DiSF算法，利用贪心算法选择特征协方差矩阵特征值更均匀的文本文件，解决维度坍塌问题。

Result: 实验显示DiSF在TinyLlama架构上显著提升性能，例如在SlimPajama数据集中节省98.5%文件，训练效率提高1.5倍。

Conclusion: DiSF通过增强数据多样性，显著提升了LLM的训练效率和通用性能。

Abstract: Selecting high-quality pre-training data for large language models (LLMs) is
crucial for enhancing their overall performance under limited computation
budget, improving both training and sample efficiency. Recent advancements in
file selection primarily rely on using an existing or trained proxy model to
assess the similarity of samples to a target domain, such as high quality
sources BookCorpus and Wikipedia. However, upon revisiting these methods, the
domain-similarity selection criteria demonstrates a diversity dilemma,
i.e.dimensional collapse in the feature space, improving performance on the
domain-related tasks but causing severe degradation on generic performance. To
prevent collapse and enhance diversity, we propose a DiverSified File selection
algorithm (DiSF), which selects the most decorrelated text files in the feature
space. We approach this with a classical greedy algorithm to achieve more
uniform eigenvalues in the feature covariance matrix of the selected texts,
analyzing its approximation to the optimal solution under a formulation of
$\gamma$-weakly submodular optimization problem. Empirically, we establish a
benchmark and conduct extensive experiments on the TinyLlama architecture with
models from 120M to 1.1B parameters. Evaluating across nine tasks from the
Harness framework, DiSF demonstrates a significant improvement on overall
performance. Specifically, DiSF saves 98.5% of 590M training files in
SlimPajama, outperforming the full-data pre-training within a 50B training
budget, and achieving about 1.5x training efficiency and 5x data efficiency.

</details>


### [97] [RuleKit 2: Faster and simpler rule learning](https://arxiv.org/abs/2504.20650)
*Adam Gudyś,Cezary Maszczyk,Joanna Badura,Adam Grzelak,Marek Sikora,Łukasz Wróbel*

Main category: cs.LG

TL;DR: RuleKit 2是一个基于规则的数据分析工具包，第二版大幅提升了计算性能，并新增Python包和浏览器应用。


<details>
  <summary>Details</summary>
Motivation: 规则结合了预测和描述能力，RuleKit在第一版中已证明其有效性，第二版旨在提升性能和用户体验。

Method: 通过新算法和优化实现提升计算性能，并通过Python包和浏览器应用增强可用性。

Result: 计算性能显著提升，某些数据集分析时间减少两个数量级，支持与scikit-learn无缝集成。

Conclusion: RuleKit 2是一个高效、易用的规则分析工具，适合分类、回归和生存问题。

Abstract: Rules offer an invaluable combination of predictive and descriptive
capabilities. Our package for rule-based data analysis, RuleKit, has proven its
effectiveness in classification, regression, and survival problems. Here we
present its second version. New algorithms and optimized implementations of
those previously included, significantly improved the computational performance
of our suite, reducing the analysis time of some data sets by two orders of
magnitude. The usability of RuleKit 2 is provided by two new components: Python
package and browser application with a graphical user interface. The former
complies with scikit-learn, the most popular data mining library for Python,
allowing RuleKit 2 to be straightforwardly integrated into existing data
analysis pipelines. RuleKit 2 is available at GitHub under GNU AGPL 3 license
(https://github.com/adaa-polsl/RuleKit)

</details>


### [98] [Federated learning, ethics, and the double black box problem in medical AI](https://arxiv.org/abs/2504.20656)
*Joshua Hatherley,Anders Søgaard,Angela Ballantyne,Ruben Pauwels*

Main category: cs.LG

TL;DR: 联邦学习（FL）作为医疗AI的隐私保护方案虽被看好，但其伦理风险尚未充分探讨。本文提出FL特有的‘联邦不透明性’及其带来的双重黑箱问题，指出其可能被过度吹捧，并强调实现医学FL伦理可行性的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨联邦学习在医疗AI中的伦理风险，尤其是‘联邦不透明性’及其衍生问题，填补该领域研究空白。

Method: 通过理论论证分析FL的‘联邦不透明性’，结合医疗场景实例，揭示其潜在伦理缺陷。

Result: 提出FL存在双重黑箱问题，其益处可能被夸大，需解决透明度等挑战才能伦理可行。

Conclusion: 医疗FL需克服透明度等关键伦理挑战，避免过度乐观，确保技术应用符合伦理标准。

Abstract: Federated learning (FL) is a machine learning approach that allows multiple
devices or institutions to collaboratively train a model without sharing their
local data with a third-party. FL is considered a promising way to address
patient privacy concerns in medical artificial intelligence. The ethical risks
of medical FL systems themselves, however, have thus far been underexamined.
This paper aims to address this gap. We argue that medical FL presents a new
variety of opacity -- federation opacity -- that, in turn, generates a
distinctive double black box problem in healthcare AI. We highlight several
instances in which the anticipated benefits of medical FL may be exaggerated,
and conclude by highlighting key challenges that must be overcome to make FL
ethically feasible in medicine.

</details>


### [99] [Quantum-Enhanced Hybrid Reinforcement Learning Framework for Dynamic Path Planning in Autonomous Systems](https://arxiv.org/abs/2504.20660)
*Sahil Tomar,Shamshe Alam,Sandeep Kumar,Amit Mathur*

Main category: cs.LG

TL;DR: 提出了一种量子经典混合框架，结合量子与经典强化学习，通过量子计算的并行性生成鲁棒的Q表和转向成本估计，显著减少训练时间并提升在动态环境中的适应性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决复杂和不可预测环境中自主导航的挑战，利用量子计算的优势提升强化学习的效率和适应性。

Method: 通过量子经典混合框架生成Q表和转向成本估计，并与经典强化学习流程集成。

Result: 在模拟和实际场景（如IIT Delhi校园）中验证，显著提升了路径效率、轨迹平滑度和任务成功率。

Conclusion: 该框架展示了在复杂环境中实时自主导航的潜力，结合了量子与经典方法的优势。

Abstract: In this paper, a novel quantum classical hybrid framework is proposed that
synergizes quantum with Classical Reinforcement Learning. By leveraging the
inherent parallelism of quantum computing, the proposed approach generates
robust Q tables and specialized turn cost estimations, which are then
integrated with a classical Reinforcement Learning pipeline. The Classical
Quantum fusion results in rapid convergence of training, reducing the training
time significantly and improved adaptability in scenarios featuring static,
dynamic, and moving obstacles. Simulator based evaluations demonstrate
significant enhancements in path efficiency, trajectory smoothness, and mission
success rates, underscoring the potential of framework for real time,
autonomous navigation in complex and unpredictable environments. Furthermore,
the proposed framework was tested beyond simulations on practical scenarios,
including real world map data such as the IIT Delhi campus, reinforcing its
potential for real time, autonomous navigation in complex and unpredictable
environments.

</details>


### [100] [SFi-Former: Sparse Flow Induced Attention for Graph Transformer](https://arxiv.org/abs/2504.20666)
*Zhonghao Li,Ji Shi,Xinming Zhang,Miao Zhang,Bo Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为SFi-attention的新型注意力机制，通过最小化基于网络流的能量函数缓解图变换器的过拟合和过度全局化问题，并设计了SFi-Former模型。


<details>
  <summary>Details</summary>
Motivation: 图变换器在处理长距离依赖的图数据时表现出色，但由于密集注意力的使用，容易受到弱归纳偏置、过拟合和过度全局化的问题影响。

Method: 提出SFi-attention机制，通过l1正则化学习稀疏模式，并据此设计SFi-Former模型，选择性聚合节点特征。

Result: SFi-Former在多个图数据集上表现优异，特别是在长距离依赖数据上达到SOTA，并且具有更小的泛化差距。

Conclusion: SFi-Former通过稀疏注意力机制有效提升了模型性能并减少了过拟合问题。

Abstract: Graph Transformers (GTs) have demonstrated superior performance compared to
traditional message-passing graph neural networks in many studies, especially
in processing graph data with long-range dependencies. However, GTs tend to
suffer from weak inductive bias, overfitting and over-globalizing problems due
to the dense attention. In this paper, we introduce SFi-attention, a novel
attention mechanism designed to learn sparse pattern by minimizing an energy
function based on network flows with l1-norm regularization, to relieve those
issues caused by dense attention. Furthermore, SFi-Former is accordingly
devised which can leverage the sparse attention pattern of SFi-attention to
generate sparse network flows beyond adjacency matrix of graph data.
Specifically, SFi-Former aggregates features selectively from other nodes
through flexible adaptation of the sparse attention, leading to a more robust
model. We validate our SFi-Former on various graph datasets, especially those
graph data exhibiting long-range dependencies. Experimental results show that
our SFi-Former obtains competitive performance on GNN Benchmark datasets and
SOTA performance on LongRange Graph Benchmark (LRGB) datasets. Additionally,
our model gives rise to smaller generalization gaps, which indicates that it is
less prone to over-fitting. Click here for codes.

</details>


### [101] [Explanations Go Linear: Interpretable and Individual Latent Encoding for Post-hoc Explainability](https://arxiv.org/abs/2504.20667)
*Simone Piaggesi,Riccardo Guidotti,Fosca Giannotti,Dino Pedreschi*

Main category: cs.LG

TL;DR: 提出了一种名为ILLUME的灵活可解释框架，通过表示学习结合全局代理模型和实例特定的线性变换，解决传统代理方法在局部和全局解释中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的代理技术（局部和全局）在解释黑盒机器学习模型时存在明显不足，如计算成本高或无法捕捉复杂局部行为，因此需要一种更高效的统一解释框架。

Method: 结合全局训练的代理模型和元编码器学习的实例特定线性变换，生成局部和全局解释。

Result: ILLUME能够生成准确、鲁棒且忠实于黑盒的特征归因和决策规则，优于传统代理方法。

Conclusion: ILLUME通过统一的框架有效解决了传统代理方法的局限性，提供了更灵活且可靠的黑盒模型解释方案。

Abstract: Post-hoc explainability is essential for understanding black-box machine
learning models. Surrogate-based techniques are widely used for local and
global model-agnostic explanations but have significant limitations. Local
surrogates capture non-linearities but are computationally expensive and
sensitive to parameters, while global surrogates are more efficient but
struggle with complex local behaviors. In this paper, we present ILLUME, a
flexible and interpretable framework grounded in representation learning, that
can be integrated with various surrogate models to provide explanations for any
black-box classifier. Specifically, our approach combines a globally trained
surrogate with instance-specific linear transformations learned with a
meta-encoder to generate both local and global explanations. Through extensive
empirical evaluations, we demonstrate the effectiveness of ILLUME in producing
feature attributions and decision rules that are not only accurate but also
robust and faithful to the black-box, thus providing a unified explanation
framework that effectively addresses the limitations of traditional surrogate
methods.

</details>


### [102] [What's Wrong with Your Synthetic Tabular Data? Using Explainable AI to Evaluate Generative Models](https://arxiv.org/abs/2504.20687)
*Jan Kapar,Niklas Koenen,Martin Jullum*

Main category: cs.LG

TL;DR: 论文提出了一种使用可解释AI（XAI）技术评估合成表格数据质量的新方法，通过分析分类器的特征重要性和特征效应，揭示合成数据中的不一致性或缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据评估指标常存在冲突且无法明确具体问题，需要更透明、可解释的方法来诊断和改进合成数据质量。

Method: 训练一个二分类检测器区分真实与合成数据，并应用XAI技术（如特征重要性、部分依赖图、Shapley值和反事实解释）分析分类器的决策依据。

Result: 该方法在两份表格数据和生成模型上验证，能够发现传统评估技术忽略的问题，如不现实的依赖关系或缺失模式。

Conclusion: 通过XAI技术增强合成数据评估的透明度和洞察力，为改进生成模型提供了具体方向。

Abstract: Evaluating synthetic tabular data is challenging, since they can differ from
the real data in so many ways. There exist numerous metrics of synthetic data
quality, ranging from statistical distances to predictive performance, often
providing conflicting results. Moreover, they fail to explain or pinpoint the
specific weaknesses in the synthetic data. To address this, we apply
explainable AI (XAI) techniques to a binary detection classifier trained to
distinguish real from synthetic data. While the classifier identifies
distributional differences, XAI concepts such as feature importance and feature
effects, analyzed through methods like permutation feature importance, partial
dependence plots, Shapley values and counterfactual explanations, reveal why
synthetic data are distinguishable, highlighting inconsistencies, unrealistic
dependencies, or missing patterns. This interpretability increases transparency
in synthetic data evaluation and provides deeper insights beyond conventional
metrics, helping diagnose and improve synthetic data quality. We apply our
approach to two tabular datasets and generative models, showing that it
uncovers issues overlooked by standard evaluation techniques.

</details>


### [103] [Unsupervised Surrogate Anomaly Detection](https://arxiv.org/abs/2504.20733)
*Simon Klüttermann,Tim Katzke,Emmanuel Müller*

Main category: cs.LG

TL;DR: 提出了一种名为DEAN的无监督异常检测算法，利用神经网络表示正常数据模式，并通过满足一组优化准则实现高性能检测。


<details>
  <summary>Details</summary>
Motivation: 受工程学中代理模型启发，旨在通过神经网络捕捉正常数据的规律性模式，从而更有效地识别异常。

Method: 提出了DEAN算法，设计时遵循一组代理模型的最优准则，并通过深度集成技术实现。

Result: 在121个基准数据集上验证，性能优于19种现有方法，并展示了方法的可扩展性和可靠性。

Conclusion: DEAN作为一种新型代理异常检测方法，在性能和实用性上均表现出显著优势。

Abstract: In this paper, we study unsupervised anomaly detection algorithms that learn
a neural network representation, i.e. regular patterns of normal data, which
anomalies are deviating from. Inspired by a similar concept in engineering, we
refer to our methodology as surrogate anomaly detection. We formalize the
concept of surrogate anomaly detection into a set of axioms required for
optimal surrogate models and propose a new algorithm, named DEAN (Deep Ensemble
ANomaly detection), designed to fulfill these criteria. We evaluate DEAN on 121
benchmark datasets, demonstrating its competitive performance against 19
existing methods, as well as the scalability and reliability of our method.

</details>


### [104] [Intelligent Task Offloading in VANETs: A Hybrid AI-Driven Approach for Low-Latency and Energy Efficiency](https://arxiv.org/abs/2504.20735)
*Tariq Qayyum,Asadullah Tariq,Muhammad Ali,Mohamed Adel Serhani,Zouheir Trabelsi,Maite López-Sánchez*

Main category: cs.LG

TL;DR: 该研究提出了一种结合监督学习、强化学习和粒子群优化（PSO）的混合AI框架，用于VANET中的任务卸载和资源分配，显著降低了延迟和能耗，提高了任务成功率和网络吞吐量。


<details>
  <summary>Details</summary>
Motivation: VANET的高度动态性导致网络条件不可预测、延迟高、能效低和任务失败等问题，需要一种智能化的解决方案来优化任务卸载和资源分配。

Method: 研究提出了一种混合AI框架，结合监督学习预测最优卸载策略，强化学习进行自适应决策，PSO优化延迟和能耗。

Result: 框架通过模拟验证，显著降低了延迟和能耗，同时提高了任务成功率和网络吞吐量。

Conclusion: 该框架为动态车载环境中的实时应用提供了高效、可扩展的解决方案。

Abstract: Vehicular Ad-hoc Networks (VANETs) are integral to intelligent transportation
systems, enabling vehicles to offload computational tasks to nearby roadside
units (RSUs) and mobile edge computing (MEC) servers for real-time processing.
However, the highly dynamic nature of VANETs introduces challenges, such as
unpredictable network conditions, high latency, energy inefficiency, and task
failure. This research addresses these issues by proposing a hybrid AI
framework that integrates supervised learning, reinforcement learning, and
Particle Swarm Optimization (PSO) for intelligent task offloading and resource
allocation. The framework leverages supervised models for predicting optimal
offloading strategies, reinforcement learning for adaptive decision-making, and
PSO for optimizing latency and energy consumption. Extensive simulations
demonstrate that the proposed framework achieves significant reductions in
latency and energy usage while improving task success rates and network
throughput. By offering an efficient, and scalable solution, this framework
sets the foundation for enhancing real-time applications in dynamic vehicular
environments.

</details>


### [105] [DDPS: Discrete Diffusion Posterior Sampling for Paths in Layered Graphs](https://arxiv.org/abs/2504.20754)
*Hao Luan,See-Kiong Ng,Chun Kai Ling*

Main category: cs.LG

TL;DR: 该论文利用离散扩散模型生成分层图中的路径，确保样本满足路径约束，并通过PALM表示和分类器引导提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成图像和视频之外的应用较少涉及显式约束，本文旨在解决分层图中路径生成的约束问题。

Method: 提出PALM（填充邻接列表矩阵）表示路径，并结合分类器引导（无需重新训练模型）定向生成偏好边。

Result: 实验表明，该方法在显式考虑路径约束时优于未考虑约束的替代方案。

Conclusion: PALM与分类器引导的结合为约束性样本生成提供了高效解决方案，扩展了扩散模型的应用场景。

Abstract: Diffusion models form an important class of generative models today,
accounting for much of the state of the art in cutting edge AI research. While
numerous extensions beyond image and video generation exist, few of such
approaches address the issue of explicit constraints in the samples generated.
In this paper, we study the problem of generating paths in a layered graph (a
variant of a directed acyclic graph) using discrete diffusion models, while
guaranteeing that our generated samples are indeed paths. Our approach utilizes
a simple yet effective representation for paths which we call the padded
adjacency-list matrix (PALM). In addition, we show how to effectively perform
classifier guidance, which helps steer the sampled paths to specific preferred
edges without any retraining of the diffusion model. Our preliminary results
show that empirically, our method outperforms alternatives which do not
explicitly account for path constraints.

</details>


### [106] [JTreeformer: Graph-Transformer via Latent-Diffusion Model for Molecular Generation](https://arxiv.org/abs/2504.20770)
*Ji Shi,Chengxun Xie,Zhonghao Li,Xinming Zhang,Miao Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为JTreeformer的图Transformer框架，将分子生成转化为连接树生成，结合GCN与多头注意力编码器和有向无环GCN解码器，并通过扩散模型增强采样效率，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前基于Transformer的图解码器难以有效利用图信息，限制了分子生成的能力，本文旨在通过新的框架解决这一问题，推动药物发现。

Method: 采用GCN与多头注意力结合的编码器，以及集成有向无环GCN的Transformer解码器，并在潜在空间插入扩散模型优化采样。

Result: 实验结果表明，JTreeformer在分子生成任务上优于现有方法。

Conclusion: JTreeformer为药物发现提供了有效的工具，展示了基于图Transformer的分子生成潜力。

Abstract: The discovery of new molecules based on the original chemical molecule
distributions is of great importance in medicine. The graph transformer, with
its advantages of high performance and scalability compared to traditional
graph networks, has been widely explored in recent research for applications of
graph structures. However, current transformer-based graph decoders struggle to
effectively utilize graph information, which limits their capacity to leverage
only sequences of nodes rather than the complex topological structures of
molecule graphs. This paper focuses on building a graph transformer-based
framework for molecular generation, which we call \textbf{JTreeformer} as it
transforms graph generation into junction tree generation. It combines GCN
parallel with multi-head attention as the encoder. It integrates a directed
acyclic GCN into a graph-based Transformer to serve as a decoder, which can
iteratively synthesize the entire molecule by leveraging information from the
partially constructed molecular structure at each step. In addition, a
diffusion model is inserted in the latent space generated by the encoder, to
enhance the efficiency and effectiveness of sampling further. The empirical
results demonstrate that our novel framework outperforms existing molecule
generation methods, thus offering a promising tool to advance drug discovery
(https://anonymous.4open.science/r/JTreeformer-C74C).

</details>


### [107] [Evaluating Effects of Augmented SELFIES for Molecular Understanding Using QK-LSTM](https://arxiv.org/abs/2504.20789)
*Collin Beaudoin,Swaroop Ghosh*

Main category: cs.LG

TL;DR: 本文提出了一种名为QK-LSTM的量子-经典混合模型，用于分子性质和副作用预测。通过集成量子核函数到经典LSTM中，该模型能捕捉序列数据中的复杂非线性模式。研究发现，使用增强的SELFIES方法相比SMILES能显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 药物开发中分子性质和副作用的识别既关键又耗时，失败可能导致重大损失。现有方法未探索量子领域中增强SMILES和SELFIES的潜力，本文旨在填补这一空白。

Method: 采用QK-LSTM模型，结合量子核函数和经典LSTM，分析增强SMILES和SELFIES在量子与混合量子-经典领域的效果。

Result: 增强SELFIES比SMILES在经典和混合量子-经典领域分别提升5.97%和5.91%的预测性能，差异显著。

Conclusion: QK-LSTM结合增强SELFIES为分子性质和副作用预测提供了高效解决方案，展现了量子-经典混合模型在该领域的潜力。

Abstract: Identifying molecular properties, including side effects, is a critical yet
time-consuming step in drug development. Failing to detect these side effects
before regulatory submission can result in significant financial losses and
production delays, and overlooking them during the regulatory review can lead
to catastrophic consequences. This challenge presents an opportunity for
innovative machine learning approaches, particularly hybrid quantum-classical
models like the Quantum Kernel-Based Long Short-Term Memory (QK-LSTM) network.
The QK-LSTM integrates quantum kernel functions into the classical LSTM
framework, enabling the capture of complex, non-linear patterns in sequential
data. By mapping input data into a high-dimensional quantum feature space, the
QK-LSTM model reduces the need for large parameter sets, allowing for model
compression without sacrificing accuracy in sequence-based tasks. Recent
advancements have been made in the classical domain using augmented variations
of the Simplified Molecular Line-Entry System (SMILES). However, to the best of
our knowledge, no research has explored the impact of augmented SMILES in the
quantum domain, nor the role of augmented Self-Referencing Embedded Strings
(SELFIES) in either classical or hybrid quantum-classical settings. This study
presents the first analysis of these approaches, providing novel insights into
their potential for enhancing molecular property prediction and side effect
identification. Results reveal that augmenting SELFIES yields in statistically
significant improvements from SMILES by a 5.97% improvement for the classical
domain and a 5.91% improvement for the hybrid quantum-classical domain.

</details>


### [108] [Q-Fusion: Diffusing Quantum Circuits](https://arxiv.org/abs/2504.20794)
*Collin Beaudoin,Swaroop Ghosh*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散的量子架构搜索（QAS）方法，利用LayerDAG框架自动生成量子电路，解决NISQ设备因比特数和门数受限及人工设计算法繁琐的问题，相比LLMs、RL或VAE等方法，该模型能始终生成100%有效的量子电路。


<details>
  <summary>Details</summary>
Motivation: 当前NISQ设备受限于比特数和门数，且量子算法设计依赖大量专业知识和时间，迫切需要自动化的量子架构搜索（QAS）方法减少人工干预。

Method: 采用基于扩散的算法，结合LayerDAG框架生成新型量子电路，替代传统的LLMs、RL或VAE等方法。

Result: 所提模型能稳定生成100%有效的量子电路输出，验证了方法的可靠性。

Conclusion: 扩散驱动的QAS为量子计算的高效自动化设计提供了新思路，尤其适用于NISQ时代资源受限的场景。

Abstract: Quantum computing holds great potential for solving socially relevant and
computationally complex problems. Furthermore, quantum machine learning (QML)
promises to rapidly improve our current machine learning capabilities. However,
current noisy intermediate-scale quantum (NISQ) devices are constrained by
limitations in the number of qubits and gate counts, which hinder their full
capabilities. Furthermore, the design of quantum algorithms remains a laborious
task, requiring significant domain expertise and time. Quantum Architecture
Search (QAS) aims to streamline this process by automatically generating novel
quantum circuits, reducing the need for manual intervention. In this paper, we
propose a diffusion-based algorithm leveraging the LayerDAG framework to
generate new quantum circuits. This method contrasts with other approaches that
utilize large language models (LLMs), reinforcement learning (RL), variational
autoencoders (VAE), and similar techniques. Our results demonstrate that the
proposed model consistently generates 100% valid quantum circuit outputs.

</details>


### [109] [The When and How of Target Variable Transformations](https://arxiv.org/abs/2504.20821)
*Loren Nuyts,Jesse Davis*

Main category: cs.LG

TL;DR: 该论文强调机器学习流程中目标变量转换的重要性，并通过实践案例说明其作用，提出何时需要转换的通用规则，并讨论适用场景下的转换方法。


<details>
  <summary>Details</summary>
Motivation: 尽管数据准备阶段在机器学习中的重要性被广泛认可，但现有文献对目标变量转换的关注不足。论文旨在通过实践案例和通用规则，填补这一研究空白。

Method: 论文通过分析目标变量转换的实践案例，总结通用规则，并提供针对不同场景的转换方法建议。

Result: 研究表明，目标变量转换在某些情况下能显著提升模型性能，论文提供了具体的转换规则和应用场景。

Conclusion: 目标变量转换是机器学习流程中不可忽视的环节，论文提出的规则和方法为实践提供了实用指导。

Abstract: The machine learning pipeline typically involves the iterative process of (1)
collecting the data, (2) preparing the data, (3) learning a model, and (4)
evaluating a model. Practitioners recognize the importance of the data
preparation phase in terms of its impact on the ability to learn accurate
models. In this regard, significant attention is often paid to manipulating the
feature set (e.g., selection, transformations, dimensionality reduction). A
point that is less well appreciated is that transformations on the target
variable can also have a large impact on whether it is possible to learn a
suitable model. These transformations may include accounting for
subject-specific biases (e.g., in how someone uses a rating scale), contexts
(e.g., population size effects), and general trends (e.g., inflation). However,
this point has received a much more cursory treatment in the existing
literature. The goal of this paper is three-fold. First, we aim to highlight
the importance of this problem by showing when transforming the target variable
has been useful in practice. Second, we will provide a set of generic ``rules
of thumb'' that indicate situations when transforming the target variable may
be needed. Third, we will discuss which transformations should be considered in
a given situation.

</details>


### [110] [An approach to melodic segmentation and classification based on filtering with the Haar-wavelet](https://arxiv.org/abs/2504.20822)
*Gissel Velarde,Tillman Weyde,David Meredith*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Haar小波变换的旋律分类和分割方法，在巴赫二部创意曲和荷兰民谣的分类任务中表现出色，但不及多特征字符串匹配方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在改进旋律的分类和分割效果，尤其针对符号化表示的旋律，通过Haar小波变换提升分类准确性。

Method: 方法采用Haar小波变换对旋律的音高信号进行滤波，利用局部最大值或零交叉点分割信号，再用k近邻算法（欧氏和城市街区距离）分类。

Result: 在巴赫作品分类中优于未滤波信号和基于格式塔的分割方法，但在荷兰民谣分类中表现与音高信号相当，不及多特征字符串匹配方法。

Conclusion: Haar小波变换在旋律分类中有效，但在复杂任务下仍需结合其他特征以提升性能。

Abstract: We present a novel method of classification and segmentation of melodies in
symbolic representation. The method is based on filtering pitch as a signal
over time with the Haar-wavelet, and we evaluate it on two tasks. The filtered
signal corresponds to a single-scale signal ws from the continuous Haar wavelet
transform. The melodies are first segmented using local maxima or
zero-crossings of w_s. The segments of w_s are then classified using the
k-nearest neighbour algorithm with Euclidian and city-block distances. The
method proves more effective than using unfiltered pitch signals and
Gestalt-based segmentation when used to recognize the parent works of segments
from Bach's Two-Part Inventions (BWV 772-786). When used to classify 360 Dutch
folk tunes into 26 tune families, the performance of the method is comparable
to the use of pitch signals, but not as good as that of string-matching methods
based on multiple features.

</details>


### [111] [Hybrid Quantum Recurrent Neural Network For Remaining Useful Life Prediction](https://arxiv.org/abs/2504.20823)
*Olga Tsurkan,Aleksandra Konstantinova,Aleksandr Sedykh,Dmitrii Zhiganov,Arsenii Senokosov,Daniil Tarpanov,Matvei Anoshin,Leonid Fedichkin*

Main category: cs.LG

TL;DR: 本文提出了一种混合量子循环神经网络框架，用于预测飞机发动机的剩余使用寿命，实验表明其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 航空领域预测性维护依赖于剩余使用寿命的准确估计，传统方法在高频数据学习上存在局限，因此探索量子混合方法。

Method: 结合量子长短期记忆层（Quantum LSTM）与经典密集层的混合框架，利用量子深度注入电路捕捉高频信号。

Result: 在NASA数据集上，该方法的均方根误差和平均绝对误差表现优于传统RNN模型5%，并显著超越随机森林、CNN和MLP等基线方法。

Conclusion: 混合量子-经典方法在少量数据条件下具有鲁棒性，为预测性维护任务提供了新思路，但某些先进联合架构仍表现更优。

Abstract: Predictive maintenance in aerospace heavily relies on accurate estimation of
the remaining useful life of jet engines. In this paper, we introduce a Hybrid
Quantum Recurrent Neural Network framework, combining Quantum Long Short-Term
Memory layers with classical dense layers for Remaining Useful Life forecasting
on NASA's Commercial Modular Aero-Propulsion System Simulation dataset. Each
Quantum Long Short-Term Memory gate replaces conventional linear
transformations with Quantum Depth-Infused circuits, allowing the network to
learn high-frequency components more effectively. Experimental results
demonstrate that, despite having fewer trainable parameters, the Hybrid Quantum
Recurrent Neural Network achieves up to a 5% improvement over a Recurrent
Neural Network based on stacked Long Short-Term Memory layers in terms of mean
root mean squared error and mean absolute error. Moreover, a thorough
comparison of our method with established techniques, including Random Forest,
Convolutional Neural Network, and Multilayer Perceptron, demonstrates that our
approach, which achieves a Root Mean Squared Error of 15.46, surpasses these
baselines by approximately 13.68%, 16.21%, and 7.87%, respectively.
Nevertheless, it remains outperformed by certain advanced joint architectures.
Our findings highlight the potential of hybrid quantum-classical approaches for
robust time-series forecasting under limited data conditions, offering new
avenues for enhancing reliability in predictive maintenance tasks.

</details>


### [112] [Reinforcement Learning for LLM Reasoning Under Memory Constraints](https://arxiv.org/abs/2504.20834)
*Alan Lee,Harry Tong*

Main category: cs.LG

TL;DR: 该论文提出了两种内存高效的强化学习方法S-GRPO和T-SPMO，用于在单卡40GB GPU限制下优化大语言模型的推理能力，显著提升了SVAMP和多位数乘法任务的性能。


<details>
  <summary>Details</summary>
Motivation: 针对学术环境下常见的计算资源限制（如单卡40GB GPU），探索无需评论者的强化学习技术，以提升大语言模型在受限条件下的推理能力。

Method: 提出了两种方法：1) S-GRPO（Group Relative Policy Optimization的内存高效变体）；2) T-SPMO（基于token级别前缀匹配的细粒度信用分配策略），并与LoRA微调结合。

Result: 在Qwen2-1.5B模型上，两种方法将SVAMP基准准确率从46%提升至70%以上，且T-SPMO在多位数乘法任务中表现优异。但标准GRPO方法在LoRA微调下未表现出性能提升。

Conclusion: 内存高效方法可能通过正则化稳定训练（尤其在仅更新部分参数时），展现了在硬件限制下强化学习微调的潜力。

Abstract: We explore reinforcement learning (RL) techniques to enhance reasoning within
targeted problem spaces in large language models (LLMs) under memory and
compute constraints. Our focus is on critic-free methods compatible with LoRA
fine-tuning on a single 40GB GPU, a common limitation in academic settings. We
introduce S-GRPO, a memory-efficient variant of Group Relative Policy
Optimization, and T-SPMO, a token-level prefix matching strategy for
fine-grained credit assignment. Despite limited resources, when used to
fine-tune Qwen2-1.5B both methods significantly improve SVAMP benchmark
accuracy from 46% to above 70% using LoRA training. T-SPMO also excels in
multi-digit multiplication tasks, underscoring the potential of RL fine-tuning
under hardware constraints. Additionally, we find that our full-token GRPO
baseline under LoRA fine-tuning did not improve model performance (compared to
base model) on either task, suggesting that our memory-efficient methods may
act as a form of regularization that stabilizes training when only a small
subset of parameters are updated.

</details>


### [113] [Mitigating the Structural Bias in Graph Adversarial Defenses](https://arxiv.org/abs/2504.20848)
*Junyuan Fang,Huimin Liu,Han Yang,Jiajing Wu,Zibin Zheng,Chi K. Tse*

Main category: cs.LG

TL;DR: 该论文针对图神经网络（GNNs）在对抗攻击下对低度节点（尾部节点）的结构性偏差问题，提出了一种包含异质-同质增强图构建、kNN增强图构建和多视角节点注意力模块的防御策略，以增强GNNs的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有GNN防御方法在对抗攻击下对低度节点的防御能力存在结构性偏差，类似于传统GNN在干净图中对低度节点的偏差。因此，作者旨在设计一种策略来缓解这一问题。

Method: 方法包括：1) 异质-同质增强图构建（全局去除异质链接，为低度节点添加同质链接）；2) kNN增强图构建；3) 多视角节点注意力模块，自适应结合不同图视角的表征。

Result: 实验结果表明，所提策略在基准数据集上有效提升了防御能力，并减轻了结构性偏差。

Conclusion: 该策略通过多视角增强和注意力机制，显著改善了GNN在对抗攻击下的鲁棒性和公平性。

Abstract: In recent years, graph neural networks (GNNs) have shown great potential in
addressing various graph structure-related downstream tasks. However, recent
studies have found that current GNNs are susceptible to malicious adversarial
attacks. Given the inevitable presence of adversarial attacks in the real
world, a variety of defense methods have been proposed to counter these attacks
and enhance the robustness of GNNs. Despite the commendable performance of
these defense methods, we have observed that they tend to exhibit a structural
bias in terms of their defense capability on nodes with low degree (i.e., tail
nodes), which is similar to the structural bias of traditional GNNs on nodes
with low degree in the clean graph. Therefore, in this work, we propose a
defense strategy by including hetero-homo augmented graph construction, $k$NN
augmented graph construction, and multi-view node-wise attention modules to
mitigate the structural bias of GNNs against adversarial attacks. Notably, the
hetero-homo augmented graph consists of removing heterophilic links (i.e.,
links connecting nodes with dissimilar features) globally and adding homophilic
links (i.e., links connecting nodes with similar features) for nodes with low
degree. To further enhance the defense capability, an attention mechanism is
adopted to adaptively combine the representations from the above two kinds of
graph views. We conduct extensive experiments to demonstrate the defense and
debiasing effect of the proposed strategy on benchmark datasets.

</details>


### [114] [Tabular Data Adapters: Improving Outlier Detection for Unlabeled Private Data](https://arxiv.org/abs/2504.20862)
*Dayananda Herurkar,Jörn Hees,Vesselin Tzvetkov,Andreas Dengel*

Main category: cs.LG

TL;DR: 论文提出了一种名为Tabular Data Adapters（TDA）的新方法，用于在异常检测任务中为未标记的表格数据生成软标签，通过利用公共数据集和共享自编码器，解决了私有数据集缺乏标签的问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法通常基于大型公共数据集，但在应用于私有数据集时，常面临结构差异、领域偏移和缺乏标签的挑战。为此，论文旨在解决私有数据集在异常检测任务中的冷启动问题。

Method: 通过识别统计相似的公共数据集，并利用共享自编码器将私有数据转换为与公共模型兼容的格式，从而生成弱标签。

Result: 在50个跨领域表格数据集上的实验表明，TDA方法比基线方法提供更准确的标注，同时减少了计算时间。

Conclusion: TDA方法为连接公共研究模型与工业应用提供了一种可扩展、高效且经济高效的解决方案。

Abstract: The remarkable success of Deep Learning approaches is often based and
demonstrated on large public datasets. However, when applying such approaches
to internal, private datasets, one frequently faces challenges arising from
structural differences in the datasets, domain shift, and the lack of labels.
In this work, we introduce Tabular Data Adapters (TDA), a novel method for
generating soft labels for unlabeled tabular data in outlier detection tasks.
By identifying statistically similar public datasets and transforming private
data (based on a shared autoencoder) into a format compatible with
state-of-the-art public models, our approach enables the generation of weak
labels. It thereby can help to mitigate the cold start problem of labeling by
basing on existing outlier detection models for public datasets. In experiments
on 50 tabular datasets across different domains, we demonstrate that our method
is able to provide more accurate annotations than baseline approaches while
reducing computational time. Our approach offers a scalable, efficient, and
cost-effective solution, to bridge the gap between public research models and
real-world industrial applications.

</details>


### [115] [Quantifying the Noise of Structural Perturbations on Graph Adversarial Attacks](https://arxiv.org/abs/2504.20869)
*Junyuan Fang,Han Yang,Haixian Wen,Jiajing Wu,Zibin Zheng,Chi K. Tse*

Main category: cs.LG

TL;DR: 该论文提出用噪声量化对抗性链接的攻击强度，并基于定义的噪声和分类边际提出三种攻击策略，实验证明策略有效，同时探究了有效对抗性扰动的偏好模式。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络对抗攻击研究多集中在攻击性能优化上，但忽略了扰动强度的量化（如特定节点/链接的注入），导致扰动选择缺乏可解释性。

Method: 提出噪声概念量化攻击强度，并基于噪声和分类边际设计三种攻击策略，包括单步和多步优化。

Result: 在基准数据集和三种代表性图神经网络上的实验验证了攻击策略的有效性，并分析了扰动节点的属性偏好模式。

Conclusion: 通过量化攻击强度并设计可解释的攻击策略，增强了对抗性扰动的理解与应用效果。

Abstract: Graph neural networks have been widely utilized to solve graph-related tasks
because of their strong learning power in utilizing the local information of
neighbors. However, recent studies on graph adversarial attacks have proven
that current graph neural networks are not robust against malicious attacks.
Yet much of the existing work has focused on the optimization objective based
on attack performance to obtain (near) optimal perturbations, but paid less
attention to the strength quantification of each perturbation such as the
injection of a particular node/link, which makes the choice of perturbations a
black-box model that lacks interpretability. In this work, we propose the
concept of noise to quantify the attack strength of each adversarial link.
Furthermore, we propose three attack strategies based on the defined noise and
classification margins in terms of single and multiple steps optimization.
Extensive experiments conducted on benchmark datasets against three
representative graph neural networks demonstrate the effectiveness of the
proposed attack strategies. Particularly, we also investigate the preferred
patterns of effective adversarial perturbations by analyzing the corresponding
properties of the selected perturbation nodes.

</details>


### [116] [Return Capping: Sample-Efficient CVaR Policy Gradient Optimisation](https://arxiv.org/abs/2504.20887)
*Harry Mead,Clarissa Costen,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: 论文提出了一种改进的条件风险价值（CVaR）优化方法，通过限制训练中轨迹的总回报而非直接丢弃低回报轨迹，显著提高了样本效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于策略梯度（PG）的CVaR优化方法因丢弃大量轨迹导致样本效率低下，需要一种更高效的替代方案。

Method: 提出了一种问题重构方法，通过为训练轨迹的总回报设置上限而非简单丢弃，确保在适当设置上限时与原问题等价。

Result: 在多个环境中验证，该方法相较基线表现出持续的性能提升。

Conclusion: 通过限制回报而非丢弃轨迹，新方法在保持优化效果的同时显著提高了样本效率。

Abstract: When optimising for conditional value at risk (CVaR) using policy gradients
(PG), current methods rely on discarding a large proportion of trajectories,
resulting in poor sample efficiency. We propose a reformulation of the CVaR
optimisation problem by capping the total return of trajectories used in
training, rather than simply discarding them, and show that this is equivalent
to the original problem if the cap is set appropriately. We show, with
empirical results in an number of environments, that this reformulation of the
problem results in consistently improved performance compared to baselines.

</details>


### [117] [Does Feedback Help in Bandits with Arm Erasures?](https://arxiv.org/abs/2504.20894)
*Merve Karakas,Osama Hanna,Lin F. Yang,Christina Fragouli*

Main category: cs.LG

TL;DR: 研究分布式多臂老虎机（MAB）问题，探讨在带有擦除反馈的通信约束网络中反馈机制对最坏情况遗憾上界的影响。


<details>
  <summary>Details</summary>
Motivation: 动机是多臂老虎机算法在通信受限网络中的应用增加，探讨擦除反馈是否能提升性能。

Method: 考虑了学习者能接收到代理反馈的情况，证明擦除反馈不改变最坏情况遗憾上界的阶数，并设计了算法评估性能。

Result: 证明擦除反馈不改善遗憾上界，遗憾下界为Ω(√KT + K/(1−ε))，与无反馈情况相同。但反馈使算法设计更简单。

Conclusion: 反馈虽不改变遗憾上界的阶数，但简化了算法设计并可能在常数项上提升性能。

Abstract: We study a distributed multi-armed bandit (MAB) problem over arm erasure
channels, motivated by the increasing adoption of MAB algorithms over
communication-constrained networks. In this setup, the learner communicates the
chosen arm to play to an agent over an erasure channel with probability
$\epsilon \in [0,1)$; if an erasure occurs, the agent continues pulling the
last successfully received arm; the learner always observes the reward of the
arm pulled. In past work, we considered the case where the agent cannot convey
feedback to the learner, and thus the learner does not know whether the arm
played is the requested or the last successfully received one. In this paper,
we instead consider the case where the agent can send feedback to the learner
on whether the arm request was received, and thus the learner exactly knows
which arm was played. Surprisingly, we prove that erasure feedback does not
improve the worst-case regret upper bound order over the previously studied
no-feedback setting. In particular, we prove a regret lower bound of
$\Omega(\sqrt{KT} + K / (1 - \epsilon))$, where $K$ is the number of arms and
$T$ the time horizon, that matches no-feedback upper bounds up to logarithmic
factors. We note however that the availability of feedback enables simpler
algorithm designs that may achieve better constants (albeit not better order)
regret bounds; we design one such algorithm and evaluate its performance
numerically.

</details>


### [118] [Evaluating Generative Models for Tabular Data: Novel Metrics and Benchmarking](https://arxiv.org/abs/2504.20900)
*Dayananda Herurkar,Ahmad Ali,Andreas Dengel*

Main category: cs.LG

TL;DR: 该论文针对表格数据生成模型评估的不足，提出了三种新指标（FAED、FPCAD、RFIS），并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型评估指标对表格数据的复杂结构、多类型数据支持不足，需要更全面的评估方法。

Method: 提出FAED、FPCAD、RFIS三种新指标，并在三个网络入侵检测数据集上与现有方法（Fidelity、Utility等）进行对比实验。

Result: FAED能有效捕捉现有指标忽略的问题，FPCAD表现良好但需改进，整体框架为表格数据生成模型提供了实用评估方案。

Conclusion: 新指标填补了表格数据生成模型评估的空白，未来可进一步优化FPCAD的可靠性。

Abstract: Generative models have revolutionized multiple domains, yet their application
to tabular data remains underexplored. Evaluating generative models for tabular
data presents unique challenges due to structural complexity, large-scale
variability, and mixed data types, making it difficult to intuitively capture
intricate patterns. Existing evaluation metrics offer only partial insights,
lacking a comprehensive measure of generative performance. To address this
limitation, we propose three novel evaluation metrics: FAED, FPCAD, and RFIS.
Our extensive experimental analysis, conducted on three standard network
intrusion detection datasets, compares these metrics with established
evaluation methods such as Fidelity, Utility, TSTR, and TRTS. Our results
demonstrate that FAED effectively captures generative modeling issues
overlooked by existing metrics. While FPCAD exhibits promising performance,
further refinements are necessary to enhance its reliability. Our proposed
framework provides a robust and practical approach for assessing generative
models in tabular data applications.

</details>


### [119] [MOSIC: Model-Agnostic Optimal Subgroup Identification with Multi-Constraint for Improved Reliability](https://arxiv.org/abs/2504.20908)
*Wenxin Chen,Weishen Pan,Kyra Gan,Fei Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种模型无关的框架，用于在多约束条件下识别最佳子组，通过无约束的最小最大优化问题重新组合这个组合问题，并证明了其收敛性。实验表明，该方法能在满足多重约束的同时提高治疗效果和协变量平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注提升子组的治疗效果，但临床实践中还需考虑子组大小和协变量平衡等实际约束。目前缺乏统一方法同时处理这些问题。

Method: 将组合问题转化为无约束最小最大优化问题，并采用梯度下降上升算法求解，证明了其收敛性。支持多种模型和技术来估计和优化治疗效果。

Result: 在合成和真实数据集上的实验表明，该方法在满足多重约束（如子组大小和协变量平衡）的同时，显著提升了治疗效果。

Conclusion: 该框架是灵活且稳健的，能够同时满足多个临床相关约束，为个性化医疗提供了实用的解决方案。

Abstract: Identifying subgroups that benefit from specific treatments using
observational data is a critical challenge in personalized medicine. Most
existing approaches solely focus on identifying a subgroup with an improved
treatment effect. However, practical considerations, such as ensuring a minimum
subgroup size for representativeness or achieving sufficient confounder balance
for reliability, are also important for making findings clinically meaningful
and actionable. While some studies address these constraints individually, none
offer a unified approach to handle them simultaneously. To bridge this gap, we
propose a model-agnostic framework for optimal subgroup identification under
multiple constraints. We reformulate this combinatorial problem as an
unconstrained min-max optimization problem with novel modifications and solve
it by a gradient descent ascent algorithm. We further prove its convergence to
a feasible and locally optimal solution. Our method is stable and highly
flexible, supporting various models and techniques for estimating and
optimizing treatment effectiveness with observational data. Extensive
experiments on both synthetic and real-world datasets demonstrate its
effectiveness in identifying subgroups that satisfy multiple constraints,
achieving higher treatment effects and better confounder balancing results
across different group sizes.

</details>


### [120] [Statistical and Predictive Analysis to Identify Risk Factors and Effects of Post COVID-19 Syndrome](https://arxiv.org/abs/2504.20915)
*Milad Leyli-abadi,Jean-Patrick Brunet,Axel Tahmasebimoradi*

Main category: cs.LG

TL;DR: 该研究分析了长期 COVID 与多种因素（如疫苗接种时间、患者特征等）的关联，并通过统计和预测模型（如神经网络）识别关键预测因子，实现19%的平均预测误差。


<details>
  <summary>Details</summary>
Motivation: 长期 COVID 的症状持续存在且影响深远，了解其相关因素并预测其强度有助于针对性干预。

Method: 采用统计分析和多种数据驱动方法（线性模型、随机森林、梯度提升、神经网络）对 Lifelines COVID-19 队列数据进行分析。

Result: 神经网络表现最佳（MAPE 19%），关键预测因子包括嗅觉丧失、头痛、肌肉疼痛及疫苗接种时间，慢性疾病和性别是重要风险因素。

Conclusion: 研究成果为理解长期 COVID 和制定干预措施提供了重要依据，神经网络是最优预测工具。

Abstract: Based on recent studies, some COVID-19 symptoms can persist for months after
infection, leading to what is termed long COVID. Factors such as vaccination
timing, patient characteristics, and symptoms during the acute phase of
infection may contribute to the prolonged effects and intensity of long COVID.
Each patient, based on their unique combination of factors, develops a specific
risk or intensity of long COVID. In this work, we aim to achieve two
objectives: (1) conduct a statistical analysis to identify relationships
between various factors and long COVID, and (2) perform predictive analysis of
long COVID intensity using these factors. We benchmark and interpret various
data-driven approaches, including linear models, random forests, gradient
boosting, and neural networks, using data from the Lifelines COVID-19 cohort.
Our results show that Neural Networks (NN) achieve the best performance in
terms of MAPE, with predictions averaging 19\% error. Additionally,
interpretability analysis reveals key factors such as loss of smell, headache,
muscle pain, and vaccination timing as significant predictors, while chronic
disease and gender are critical risk factors. These insights provide valuable
guidance for understanding long COVID and developing targeted interventions.

</details>


### [121] [Improvements of Dark Experience Replay and Reservoir Sampling towards Better Balance between Consolidation and Plasticity](https://arxiv.org/abs/2504.20932)
*Taisuke Kobayashi*

Main category: cs.LG

TL;DR: 本文改进了持续学习领域的黑暗经验回放（DER）和水库采样（RS）方法，通过自动权重调整、纠正错误数据等策略平衡记忆巩固与可塑性，从而提升学习性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习中常出现灾难性遗忘问题，DER和RS虽能缓解但存在权重分配不当和错误数据抑制学习的缺点，需改进以平衡记忆与学习能力。

Method: 提出了DER的权重自适应、错误数据屏蔽和输出纠正策略，以及RS的接受概率泛化、多缓冲区分层和不必要数据省略改进。

Result: 改进后的方法在回归、分类和强化学习任务中均表现出稳定的性能提升。

Conclusion: 通过平衡记忆巩固与可塑性，改进的DER和RS方法在持续学习中表现更优。

Abstract: Continual learning is the one of the most essential abilities for autonomous
agents, which can incrementally learn daily-life skills. For this ultimate
goal, a simple but powerful method, dark experience replay (DER), has been
proposed recently. DER mitigates catastrophic forgetting, in which the skills
acquired in the past are unintentionally forgotten, by stochastically storing
the streaming data in a reservoir sampling (RS) buffer and by relearning them
or retaining the past outputs for them. However, since DER considers multiple
objectives, it will not function properly without appropriate weighting of
them. In addition, the ability to retain past outputs inhibits learning if the
past outputs are incorrect due to distribution shift or other effects. This is
due to a tradeoff between memory consolidation and plasticity. The tradeoff is
hidden even in the RS buffer, which gradually stops storing new data for new
skills in it as data is continuously passed to it. To alleviate the tradeoff
and achieve better balance, this paper proposes improvement strategies to each
of DER and RS. Specifically, DER is improved with automatic adaptation of
weights, block of replaying erroneous data, and correction of past outputs. RS
is also improved with generalization of acceptance probability, stratification
of plural buffers, and intentional omission of unnecessary data. These
improvements are verified through multiple benchmarks including regression,
classification, and reinforcement learning problems. As a result, the proposed
methods achieve steady improvements in learning performance by balancing the
memory consolidation and plasticity.

</details>


### [122] [Scenario-based Compositional Verification of Autonomous Systems with Neural Perception](https://arxiv.org/abs/2504.20942)
*Christopher Watson,Rajeev Alur,Divya Gopinath,Ravi Mangal,Corina S. Pasareanu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于概率验证框架的自动驾驶系统验证方法，通过场景建模、概率抽象和符号推理来解决深度神经网络感知的复杂性和环境变化问题。


<details>
  <summary>Details</summary>
Motivation: 由于深度神经网络（DNN）感知的复杂性和难以量化的环境变化，自动驾驶系统的形式化验证具有挑战性。论文旨在通过概率方法简化验证过程。

Method: 方法包括：（1）场景化建模，将任务分解为不同环境条件的场景；（2）概率抽象，为每个场景构建DNN感知的紧凑抽象；（3）符号推理与加速，通过符号推理和新颖的加速证明规则进行高效验证。

Result: 论文通过两个案例验证了方法的有效性，分别是使用高维感知DNN的飞机滑行引导系统和基于LiDAR的F1Tenth自动驾驶汽车模拟模型。

Conclusion: 提出的概率验证框架能够有效应对自动驾驶系统中的复杂性和环境变化问题，为形式化验证提供了新思路。

Abstract: Recent advances in deep learning have enabled the development of autonomous
systems that use deep neural networks for perception. Formal verification of
these systems is challenging due to the size and complexity of the perception
DNNs as well as hard-to-quantify, changing environment conditions. To address
these challenges, we propose a probabilistic verification framework for
autonomous systems based on the following key concepts: (1) Scenario-based
Modeling: We decompose the task (e.g., car navigation) into a composition of
scenarios, each representing a different environment condition. (2)
Probabilistic Abstractions: For each scenario, we build a compact abstraction
of perception based on the DNN's performance on an offline dataset that
represents the scenario's environment condition. (3) Symbolic Reasoning and
Acceleration: The abstractions enable efficient compositional verification of
the autonomous system via symbolic reasoning and a novel acceleration proof
rule that bounds the error probability of the system under arbitrary variations
of environment conditions. We illustrate our approach on two case studies: an
experimental autonomous system that guides airplanes on taxiways using
high-dimensional perception DNNs and a simulation model of an F1Tenth
autonomous car using LiDAR observations.

</details>


### [123] [Deep Learning Characterizes Depression and Suicidal Ideation from Eye Movements](https://arxiv.org/abs/2504.20944)
*Kleanthis Avramidis,Woojae Jeong,Aditya Kommineni,Sudarsana R. Kadiri,Marcus Ma,Colin McDaniel,Myzelle Hughes,Thomas McGee,Elsi Kaiser,Dani Byrd,Assal Habibi,B. Rael Cahn,Idan A. Blank,Kristina Lerman,Takfarinas Medani,Richard M. Leahy,Shrikanth Narayanan*

Main category: cs.LG

TL;DR: 使用眼动追踪和深度学习模型预测抑郁和自杀倾向，AUC达0.793-0.826，情感刺激对认知过程影响显著。


<details>
  <summary>Details</summary>
Motivation: 心理健康问题（如抑郁和自杀倾向）缺乏客观生物标记，目前依赖自我报告和临床访谈，亟需新方法提升筛查效率。

Method: 记录126名年轻人阅读情感句子时的眼动数据，开发深度学习框架，分正负情感分支建模时序变化。

Result: 模型对抑郁/自杀倾向的AUC为0.793（对照组）和0.826（自杀倾向），负情感句子区分效果最显著。

Conclusion: 眼动追踪可作为心理健康评估的客观工具，情感刺激对眼动控制的调节作用明确。

Abstract: Identifying physiological and behavioral markers for mental health conditions
is a longstanding challenge in psychiatry. Depression and suicidal ideation, in
particular, lack objective biomarkers, with screening and diagnosis primarily
relying on self-reports and clinical interviews. Here, we investigate eye
tracking as a potential marker modality for screening purposes. Eye movements
are directly modulated by neuronal networks and have been associated with
attentional and mood-related patterns; however, their predictive value for
depression and suicidality remains unclear. We recorded eye-tracking sequences
from 126 young adults as they read and responded to affective sentences, and
subsequently developed a deep learning framework to predict their clinical
status. The proposed model included separate branches for trials of positive
and negative sentiment, and used 2D time-series representations to account for
both intra-trial and inter-trial variations. We were able to identify
depression and suicidal ideation with an area under the receiver operating
curve (AUC) of 0.793 (95% CI: 0.765-0.819) against healthy controls, and
suicidality specifically with 0.826 AUC (95% CI: 0.797-0.852). The model also
exhibited moderate, yet significant, accuracy in differentiating depressed from
suicidal participants, with 0.609 AUC (95% CI 0.571-0.646). Discriminative
patterns emerge more strongly when assessing the data relative to response
generation than relative to the onset time of the final word of the sentences.
The most pronounced effects were observed for negative-sentiment sentences,
that are congruent to depressed and suicidal participants. Our findings
highlight eye tracking as an objective tool for mental health assessment and
underscore the modulatory impact of emotional stimuli on cognitive processes
affecting oculomotor control.

</details>


### [124] [AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security](https://arxiv.org/abs/2504.20965)
*Zikui Cai,Shayan Shabihi,Bang An,Zora Che,Brian R. Bartoldson,Bhavya Kailkhura,Tom Goldstein,Furong Huang*

Main category: cs.LG

TL;DR: AegisLLM是一种多代理协作防御方法，通过自主代理的协同工作提升大语言模型（LLM）的安全性，无需模型重训练即可实时防御对抗攻击和信息泄露。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决LLM面临的对抗攻击和信息泄露问题，通过多代理协作实现自适应防御，避免传统静态防御方法的局限性。

Method: 采用包含协调器、偏转器、响应器和评估器的代理结构，结合自动化提示优化（如DSPy），提升系统鲁棒性且不影响模型效用。

Result: 在WMDP遗忘基准测试中仅需20个训练样本和300次LM调用即实现近乎完美的遗忘；在越狱基准测试中性能提升51%，误拒率显著降低。

Conclusion: AegisLLM通过动态代理推理显著优于静态防御方法，成为传统模型修改方案的有效运行时替代方案。

Abstract: We introduce AegisLLM, a cooperative multi-agent defense against adversarial
attacks and information leakage. In AegisLLM, a structured workflow of
autonomous agents - orchestrator, deflector, responder, and evaluator -
collaborate to ensure safe and compliant LLM outputs, while self-improving over
time through prompt optimization. We show that scaling agentic reasoning system
at test-time - both by incorporating additional agent roles and by leveraging
automated prompt optimization (such as DSPy)- substantially enhances robustness
without compromising model utility. This test-time defense enables real-time
adaptability to evolving attacks, without requiring model retraining.
Comprehensive evaluations across key threat scenarios, including unlearning and
jailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning
benchmark, AegisLLM achieves near-perfect unlearning with only 20 training
examples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve
51% improvement compared to the base model on StrongReject, with false refusal
rates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our
results highlight the advantages of adaptive, agentic reasoning over static
defenses, establishing AegisLLM as a strong runtime alternative to traditional
approaches based on model modifications. Code is available at
https://github.com/zikuicai/aegisllm

</details>


### [125] [Softpick: No Attention Sink, No Massive Activations with Rectified Softmax](https://arxiv.org/abs/2504.20966)
*Zayd M. K. Zuhri,Erland Hilman Fuadi,Alham Fikri Aji*

Main category: cs.LG

TL;DR: 论文介绍了SoftPick，一种改进的注意力机制替代SoftMax，能够消除注意力集中和过度激活问题，并在量化模型中表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统SoftMax在Transformer注意力机制中存在注意力集中和激活过度的问题，限制了量化、低精度训练等优化可能性。

Method: 提出SoftPick作为SoftMax的替代，调整注意力分布使其不必归一化，并通过实验验证其效果。

Result: 在340M参数模型中，SoftPick保持了性能一致性，消除了注意力集中问题，显著降低隐藏状态的峰态并生成稀疏注意力图，尤其在量化模型中表现更优。

Conclusion: SoftPick为量化、低精度训练、稀疏优化等方向提供了新的可能性，展现了实际应用潜力。

Abstract: We introduce softpick, a rectified, not sum-to-one, drop-in replacement for
softmax in transformer attention mechanisms that eliminates attention sink and
massive activations. Our experiments with 340M parameter models demonstrate
that softpick maintains performance parity with softmax on standard benchmarks
while achieving 0% sink rate. The softpick transformer produces hidden states
with significantly lower kurtosis (340 vs 33,510) and creates sparse attention
maps (46.97% sparsity). Models using softpick consistently outperform softmax
when quantized, with particularly pronounced advantages at lower bit
precisions. Our analysis and discussion shows how softpick has the potential to
open new possibilities for quantization, low-precision training, sparsity
optimization, pruning, and interpretability. Our code is available at
https://github.com/zaydzuhri/softpick-attention.

</details>


### [126] [Equivariant non-linear maps for neural networks on homogeneous spaces](https://arxiv.org/abs/2504.20974)
*Elias Nyholm,Oscar Carlsson,Maurice Weiler,Daniel Persson*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的非线性等变神经网络层框架，扩展了线性等变 $G$-CNN 的理论到非线性场景，证明了广义可操纵性约束的普遍性，并展示了如何从框架中导出多种常见等变网络架构。


<details>
  <summary>Details</summary>
Motivation: 受非线性层（如自注意力或输入相关核）在实证上的成功启发，作者希望将这些理论推广到非线性场景，从而设计更强大的等变神经网络层。

Method: 通过推导非线性等变层需要满足的广义可操纵性约束，并证明构造的普遍性，建立了对称性约束下的功能依赖性理论。

Result: 提出的框架可以导出多种常见的等变网络架构，如 $G$-CNN、隐式可操纵核网络、传统和相对位置嵌入的注意力变换器，以及 LieTransformer。

Conclusion: 该研究为未来等变神经网络层的设计提供了理论支持，证明了非线性等变层的可行性和普遍性，并展示了其在多种架构中的应用潜力。

Abstract: This paper presents a novel framework for non-linear equivariant neural
network layers on homogeneous spaces. The seminal work of Cohen et al. on
equivariant $G$-CNNs on homogeneous spaces characterized the representation
theory of such layers in the linear setting, finding that they are given by
convolutions with kernels satisfying so-called steerability constraints.
Motivated by the empirical success of non-linear layers, such as self-attention
or input dependent kernels, we set out to generalize these insights to the
non-linear setting. We derive generalized steerability constraints that any
such layer needs to satisfy and prove the universality of our construction. The
insights gained into the symmetry-constrained functional dependence of
equivariant operators on feature maps and group elements informs the design of
future equivariant neural network layers. We demonstrate how several common
equivariant network architectures - $G$-CNNs, implicit steerable kernel
networks, conventional and relative position embedded attention based
transformers, and LieTransformers - may be derived from our framework.

</details>


### [127] [Hubs and Spokes Learning: Efficient and Scalable Collaborative Machine Learning](https://arxiv.org/abs/2504.20988)
*Atul Sharma,Kavindu Herath,Saurabh Bagchi,Chaoyue Liu,Somali Chaterji*

Main category: cs.LG

TL;DR: HSL框架结合了联邦学习和去中心化学习的优势，采用两层通信结构，避免了单点故障，并在相同或更低的通信预算下优于现有P2PL框架。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习的单点故障问题和去中心化学习的通信效率问题，提出一种更适合资源受限系统的协作学习框架。

Method: 采用Hubs and Spokes两层级通信结构，结合理论分析和实验验证。

Result: HSL在相同通信预算下性能优于ELL，且在更低预算下能匹配其性能；在CIFAR-10上，400边缘的HSL达到1000边缘ELL的相同测试准确率。

Conclusion: HSL是一种实用的大规模协作学习框架，显著提升了通信效率和性能。

Abstract: We introduce the Hubs and Spokes Learning (HSL) framework, a novel paradigm
for collaborative machine learning that combines the strengths of Federated
Learning (FL) and Decentralized Learning (P2PL). HSL employs a two-tier
communication structure that avoids the single point of failure inherent in FL
and outperforms the state-of-the-art P2PL framework, Epidemic Learning Local
(ELL). At equal communication budgets (total edges), HSL achieves higher
performance than ELL, while at significantly lower communication budgets, it
can match ELL's performance. For instance, with only 400 edges, HSL reaches the
same test accuracy that ELL achieves with 1000 edges for 100 peers (spokes) on
CIFAR-10, demonstrating its suitability for resource-constrained systems. HSL
also achieves stronger consensus among nodes after mixing, resulting in
improved performance with fewer training rounds. We substantiate these claims
through rigorous theoretical analyses and extensive experimental results,
showcasing HSL's practicality for large-scale collaborative learning.

</details>


### [128] [Toward Efficient Exploration by Large Language Model Agents](https://arxiv.org/abs/2504.20997)
*Dilip Arumugam,Thomas L. Griffiths*

Main category: cs.LG

TL;DR: 论文提出了一种基于大型语言模型（LLMs）的强化学习（RL）代理设计方法，通过显式实现一种已知的、数据高效的RL算法（后验采样）来解决自然语言任务中的探索问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理设计在面对数据高效的RL任务时，探索能力不足，而经典的RL算法难以在纯自然语言环境中实现。

Method: 研究通过显式实现后验采样（一种已知的数据高效RL算法），而非微调或上下文学习，来利用LLM提升探索能力。

Result: 实验结果表明，该方法在需要谨慎探索的自然语言任务中表现显著优于现有方案。

Conclusion: 通过结合LLM的灵活性与经典RL算法的探索能力，可以有效提升自然语言任务中的RL代理性能。

Abstract: A burgeoning area within reinforcement learning (RL) is the design of
sequential decision-making agents centered around large language models (LLMs).
While autonomous decision-making agents powered by modern LLMs could facilitate
numerous real-world applications, such successes demand agents that are capable
of data-efficient RL. One key obstacle to achieving data efficiency in RL is
exploration, a challenge that we demonstrate many recent proposals for LLM
agent designs struggle to contend with. Meanwhile, classic algorithms from the
RL literature known to gracefully address exploration require technical
machinery that can be challenging to operationalize in purely natural language
settings. In this work, rather than relying on finetuning or in-context
learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate
how LLMs can be used to explicitly implement an existing RL algorithm
(Posterior Sampling for Reinforcement Learning) whose capacity for
statistically-efficient exploration is already well-studied. We offer empirical
results demonstrating how our LLM-based implementation of a known,
data-efficient RL algorithm can be considerably more effective in natural
language tasks that demand prudent exploration.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [129] [Evolution of AI in Education: Agentic Workflows](https://arxiv.org/abs/2504.20082)
*Firuz Kamalov,David Santandreu Calonge,Linda Smail,Dilshod Azizov,Dimple R. Thadani,Theresa Kwong,Amara Atif*

Main category: cs.AI

TL;DR: AI代理在教育领域展现出变革潜力，通过反思、规划、工具使用和多代理协作四大范式改进传统大语言模型的局限性。初步应用（如自动作文评分框架）显示其一致性优于单一模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统大语言模型在教育中因静态数据、适应性不足和缺乏推理能力的限制，探索可持续的AI代理技术。

Method: 回顾并分析教育中AI代理的四大设计范式（反思、规划、工具使用、多代理协作），并以自动作文评分为例进行概念验证。

Result: 多代理框架在自动作文评分中表现出比单一模型更高的一致性。

Conclusion: AI代理在教育中具有变革潜力，但需进一步研究其可解释性、可信度及对教学的可持续影响。

Abstract: Artificial intelligence (AI) has transformed various aspects of education,
with large language models (LLMs) driving advancements in automated tutoring,
assessment, and content generation. However, conventional LLMs are constrained
by their reliance on static training data, limited adaptability, and lack of
reasoning. To address these limitations and foster more sustainable
technological practices, AI agents have emerged as a promising new avenue for
educational innovation. In this review, we examine agentic workflows in
education according to four major paradigms: reflection, planning, tool use,
and multi-agent collaboration. We critically analyze the role of AI agents in
education through these key design paradigms, exploring their advantages,
applications, and challenges. To illustrate the practical potential of agentic
systems, we present a proof-of-concept application: a multi-agent framework for
automated essay scoring. Preliminary results suggest this agentic approach may
offer improved consistency compared to stand-alone LLMs. Our findings highlight
the transformative potential of AI agents in educational settings while
underscoring the need for further research into their interpretability,
trustworthiness, and sustainable impact on pedagogical impact.

</details>


### [130] [AI Awareness](https://arxiv.org/abs/2504.20084)
*Xiaojian Li,Haoyuan Shi,Rongwu Xu,Wei Xu*

Main category: cs.AI

TL;DR: 论文总结了人工智能（AI）意识的研究进展，探讨了元认知、自我意识、社会意识和情境意识四种形式，分析了其与AI能力的关系及潜在风险。


<details>
  <summary>Details</summary>
Motivation: AI能力的快速提升促使研究者重新审视AI意识，强调其作为功能性可测量属性的重要性，而非哲学上的意识问题。

Method: 结合认知科学、心理学和计算理论，分析AI意识的理论基础，并系统地评估现有实证研究和方法。

Result: 研究显示，更具备意识的AI通常表现出更高水平的智能行为，但也可能带来安全和对齐问题。

Conclusion: AI意识是一把双刃剑，既能提升能力，也需谨慎应对相关风险，为未来研究提供了方向。

Abstract: Recent breakthroughs in artificial intelligence (AI) have brought about
increasingly capable systems that demonstrate remarkable abilities in
reasoning, language understanding, and problem-solving. These advancements have
prompted a renewed examination of AI awareness, not as a philosophical question
of consciousness, but as a measurable, functional capacity. In this review, we
explore the emerging landscape of AI awareness, which includes meta-cognition
(the ability to represent and reason about its own state), self-awareness
(recognizing its own identity, knowledge, limitations, inter alia), social
awareness (modeling the knowledge, intentions, and behaviors of other agents),
and situational awareness (assessing and responding to the context in which it
operates).
  First, we draw on insights from cognitive science, psychology, and
computational theory to trace the theoretical foundations of awareness and
examine how the four distinct forms of AI awareness manifest in
state-of-the-art AI. Next, we systematically analyze current evaluation methods
and empirical findings to better understand these manifestations. Building on
this, we explore how AI awareness is closely linked to AI capabilities,
demonstrating that more aware AI agents tend to exhibit higher levels of
intelligent behaviors. Finally, we discuss the risks associated with AI
awareness, including key topics in AI safety, alignment, and broader ethical
concerns.
  AI awareness is a double-edged sword: it improves general capabilities, i.e.,
reasoning, safety, while also raises concerns around misalignment and societal
risks, demanding careful oversight as AI capabilities grow. On the whole, our
interdisciplinary review provides a roadmap for future research and aims to
clarify the role of AI awareness in the ongoing development of intelligent
machines.

</details>


### [131] [Spark: A System for Scientifically Creative Idea Generation](https://arxiv.org/abs/2504.20090)
*Aishik Sanyal,Samuel Schapiro,Sumuk Shashidhar,Royce Moon,Lav R. Varshney,Dilek Hakkani-Tur*

Main category: cs.AI

TL;DR: 论文介绍了Spark系统，结合检索增强的LLM生成与基于60万科学评论训练的评审模型Judge，旨在推动计算创造力研究。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在生成科学新想法上展现出潜力，与研究计算创造力（CC）的基础原理相符。

Method: 使用检索增强的LLM生成科学想法，并通过训练于60万条OpenReview评论的Judge模型进行评审。

Result: 开发了Spark系统，并公开了用于训练Judge的标注数据集。

Conclusion: 该工作不仅展示了系统，还鼓励CC研究者基于LLMs进一步探索科学想法的生成与评估。

Abstract: Recently, large language models (LLMs) have shown promising abilities to
generate novel research ideas in science, a direction which coincides with many
foundational principles in computational creativity (CC). In light of these
developments, we present an idea generation system named Spark that couples
retrieval-augmented idea generation using LLMs with a reviewer model named
Judge trained on 600K scientific reviews from OpenReview. Our work is both a
system demonstration and intended to inspire other CC researchers to explore
grounding the generation and evaluation of scientific ideas within foundational
CC principles. To this end, we release the annotated dataset used to train
Judge, inviting other researchers to explore the use of LLMs for idea
generation and creative evaluations.

</details>


### [132] [Personalized Artificial General Intelligence (AGI) via Neuroscience-Inspired Continuous Learning Systems](https://arxiv.org/abs/2504.20109)
*Rajeev Gupta,Suhani Gupta,Ronak Parikh,Divya Gupta,Amir Javaheri,Jairaj Singh Shaktawat*

Main category: cs.AI

TL;DR: 这篇论文总结了当前深度学习的局限性，提出了一个结合神经科学原理的新型人工智能架构，旨在实现边缘计算设备上的持续学习和个性化AGI。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型虽然取得了成就，但难以实现真正的通用人工智能（AGI），尤其是在资源受限的边缘设备上。论文旨在解决持续学习和适应性问题。

Method: 提出了一种新型架构，整合了人类学习的神经科学原理（如突触剪枝、Hebbian可塑性等），并设计了快速-慢速学习模块，以及内存高效的模型更新机制。

Result: 论文给出了架构的概念图和应对挑战（如灾难性遗忘、内存效率）的解决方案，并探讨了移动AI助手和人形机器人等应用场景。

Conclusion: 论文总结了新架构的理论价值，并提出了未来研究的方向，为边缘设备上的持续学习AGI提供了路线图。

Abstract: Artificial Intelligence has made remarkable advancements in recent years,
primarily driven by increasingly large deep learning models. However, achieving
true Artificial General Intelligence (AGI) demands fundamentally new
architectures rather than merely scaling up existing models. Current approaches
largely depend on expanding model parameters, which improves task-specific
performance but falls short in enabling continuous, adaptable, and generalized
learning. Achieving AGI capable of continuous learning and personalization on
resource-constrained edge devices is an even bigger challenge.
  This paper reviews the state of continual learning and neuroscience-inspired
AI, and proposes a novel architecture for Personalized AGI that integrates
brain-like learning mechanisms for edge deployment. We review literature on
continuous lifelong learning, catastrophic forgetting, and edge AI, and discuss
key neuroscience principles of human learning, including Synaptic Pruning,
Hebbian plasticity, Sparse Coding, and Dual Memory Systems, as inspirations for
AI systems. Building on these insights, we outline an AI architecture that
features complementary fast-and-slow learning modules, synaptic
self-optimization, and memory-efficient model updates to support on-device
lifelong adaptation.
  Conceptual diagrams of the proposed architecture and learning processes are
provided. We address challenges such as catastrophic forgetting, memory
efficiency, and system scalability, and present application scenarios for
mobile AI assistants and embodied AI systems like humanoid robots. We conclude
with key takeaways and future research directions toward truly continual,
personalized AGI on the edge. While the architecture is theoretical, it
synthesizes diverse findings and offers a roadmap for future implementation.

</details>


### [133] [Transforming Evidence Synthesis: A Systematic Review of the Evolution of Automated Meta-Analysis in the Age of AI](https://arxiv.org/abs/2504.20113)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.AI

TL;DR: 这篇论文通过系统综述评估了自动化元分析（AMA）的现状，发现现有研究主要集中在数据处理阶段，而高级合成阶段和全流程自动化研究不足，限制了AMA的潜力。


<details>
  <summary>Details</summary>
Motivation: 科学文献的指数增长催生了高效、基于证据的综合需求，推动了自然语言处理和机器学习支持的自动化元分析（AMA）领域的发展。

Method: 采用PRISMA系统综述方法，筛选了2006年至2024年的978篇论文，并分析了54项跨领域研究。

Result: 研究发现，57%的研究聚焦于数据处理自动化（如提取和统计建模），仅17%涉及高级合成阶段，仅一项研究尝试全流程自动化。AI（如LLMs）在统计建模和高级合成中的应用仍不成熟。

Conclusion: 未来研究需填补全流程自动化空白，提升解释性和方法稳健性，以实现AMA在 scalable、领域无关的综合中的潜力。

Abstract: Exponential growth in scientific literature has heightened the demand for
efficient evidence-based synthesis, driving the rise of the field of Automated
Meta-analysis (AMA) powered by natural language processing and machine
learning. This PRISMA systematic review introduces a structured framework for
assessing the current state of AMA, based on screening 978 papers from 2006 to
2024, and analyzing 54 studies across diverse domains. Findings reveal a
predominant focus on automating data processing (57%), such as extraction and
statistical modeling, while only 17% address advanced synthesis stages. Just
one study (2%) explored preliminary full-process automation, highlighting a
critical gap that limits AMA's capacity for comprehensive synthesis. Despite
recent breakthroughs in large language models (LLMs) and advanced AI, their
integration into statistical modeling and higher-order synthesis, such as
heterogeneity assessment and bias evaluation, remains underdeveloped. This has
constrained AMA's potential for fully autonomous meta-analysis. From our
dataset spanning medical (67%) and non-medical (33%) applications, we found
that AMA has exhibited distinct implementation patterns and varying degrees of
effectiveness in actually improving efficiency, scalability, and
reproducibility. While automation has enhanced specific meta-analytic tasks,
achieving seamless, end-to-end automation remains an open challenge. As AI
systems advance in reasoning and contextual understanding, addressing these
gaps is now imperative. Future efforts must focus on bridging automation across
all meta-analysis stages, refining interpretability, and ensuring
methodological robustness to fully realize AMA's potential for scalable,
domain-agnostic synthesis.

</details>


### [134] [Deep Physics Prior for First Order Inverse Optimization](https://arxiv.org/abs/2504.20278)
*Haoyu Yang,Kamyar Azizzadenesheli,Haoxing Ren*

Main category: cs.AI

TL;DR: 提出了Deep Physics Prior (DPP)，一种基于梯度的逆设计优化方法，解决了传统生成AI和贝叶斯方法在计算成本和稳健性上的不足。


<details>
  <summary>Details</summary>
Motivation: 逆设计优化在多个领域至关重要，但传统方法因缺乏显式数学模型或存在计算成本高、稳健性差等问题而受限。

Method: 利用预训练的辅助神经算子（Neural Operators）实现基于梯度的优化，并通过先验分布约束确保解的有效性。

Result: DPP在无先验数据和观测分布未知时，仍能提供稳健且高效的优化解。

Conclusion: DPP为复杂系统的逆设计优化提供了一种高效且可扩展的新方法。

Abstract: Inverse design optimization aims to infer system parameters from observed
solutions, posing critical challenges across domains such as semiconductor
manufacturing, structural engineering, materials science, and fluid dynamics.
The lack of explicit mathematical representations in many systems complicates
this process and makes the first order optimization impossible. Mainstream
approaches, including generative AI and Bayesian optimization, address these
challenges but have limitations. Generative AI is computationally expensive,
while Bayesian optimization, relying on surrogate models, suffers from
scalability, sensitivity to priors, and noise issues, often leading to
suboptimal solutions. This paper introduces Deep Physics Prior (DPP), a novel
method enabling first-order gradient-based inverse optimization with surrogate
machine learning models. By leveraging pretrained auxiliary Neural Operators,
DPP enforces prior distribution constraints to ensure robust and meaningful
solutions. This approach is particularly effective when prior data and
observation distributions are unknown.

</details>


### [135] [mrCAD: Multimodal Refinement of Computer-aided Designs](https://arxiv.org/abs/2504.20294)
*William P. McCarthy,Saujas Vaduguru,Karl D. D. Willis,Justin Matejka,Judith E. Fan,Daniel Fried,Yewen Pu*

Main category: cs.AI

TL;DR: 论文介绍了mrCAD数据集，用于研究人类如何通过多模态指令（文本和绘图）迭代优化设计，发现当前视觉语言模型在生成指令上表现优于细化指令。


<details>
  <summary>Details</summary>
Motivation: 研究旨在弥合人类与机器在内容编辑上的差距，尤其是生成AI在细化修改输出时的不足。

Method: 通过多模态通信游戏收集数据，设计师使用文本、绘图或组合方式指导制作者迭代优化CAD设计，形成包含6,082个游戏和15,163轮指令的数据集。

Result: 分析显示生成与细化指令在多模态组成上存在差异，当前视觉语言模型更擅长生成指令而非细化指令。

Conclusion: mrCAD为研究多模态细化语言提供了新基准，填补了现有数据集的空白，为未来模型优化指明方向。

Abstract: A key feature of human collaboration is the ability to iteratively refine the
concepts we have communicated. In contrast, while generative AI excels at the
\textit{generation} of content, it often struggles to make specific
language-guided \textit{modifications} of its prior outputs. To bridge the gap
between how humans and machines perform edits, we present mrCAD, a dataset of
multimodal instructions in a communication game. In each game, players created
computer aided designs (CADs) and refined them over several rounds to match
specific target designs. Only one player, the Designer, could see the target,
and they must instruct the other player, the Maker, using text, drawing, or a
combination of modalities. mrCAD consists of 6,082 communication games, 15,163
instruction-execution rounds, played between 1,092 pairs of human players. We
analyze the dataset and find that generation and refinement instructions differ
in their composition of drawing and text. Using the mrCAD task as a benchmark,
we find that state-of-the-art VLMs are better at following generation
instructions than refinement instructions. These results lay a foundation for
analyzing and modeling a multimodal language of refinement that is not
represented in previous datasets.

</details>


### [136] [Leveraging Action Relational Structures for Integrated Learning and Planning](https://arxiv.org/abs/2504.20318)
*Ryan Xiao Wang,Felipe Trevizan*

Main category: cs.AI

TL;DR: 该论文介绍了部分空间搜索（partial-space search），这是一种利用PDDL动作模式关系结构的规划新方法，旨在提升学习系统性能。通过引入动作集启发式（action set heuristics）并训练它们，论文提出的LazyLifted规划器在性能上超越了现有基于机器学习的启发式方法及LAMA。


<details>
  <summary>Details</summary>
Motivation: 现有的规划方法很少关注如何调整搜索算法以更好地配合学习系统运作。论文旨在填补这一空白，通过利用被传统规划忽略的PDDL动作模式的层级结构，提出一个新的搜索空间以提升规划效率。

Method: 论文提出了部分空间搜索（partial-space search）和动作集启发式（action set heuristics），后者用于评估状态中的动作集。论文还介绍了如何将现有的启发式转换为动作集启发式，并通过数据集训练新的动作集启发式。所有这些方法被整合到新的规划器LazyLifted中。

Result: LazyLifted在IPC 2023学习赛道（LT）上超越了现有的基于机器学习的启发式方法，在高分支因子任务上的效率也超过了LAMA。

Conclusion: 论文展示了部分空间搜索和动作集启发式的有效性，证明它们可以提高规划效率并解决复杂任务。LazyLifted的成功也表明将搜索和学习紧密结合是一个有前景的研究方向。

Abstract: Recent advances in planning have explored using learning methods to help
planning. However, little attention has been given to adapting search
algorithms to work better with learning systems. In this paper, we introduce
partial-space search, a new search space for classical planning that leverages
the relational structure of actions given by PDDL action schemas -- a structure
overlooked by traditional planning approaches. Partial-space search provides a
more granular view of the search space and allows earlier pruning of poor
actions compared to state-space search. To guide partial-space search, we
introduce action set heuristics that evaluate sets of actions in a state. We
describe how to automatically convert existing heuristics into action set
heuristics. We also train action set heuristics from scratch using large
training datasets from partial-space search. Our new planner, LazyLifted,
exploits our better integrated search and learning heuristics and outperforms
the state-of-the-art ML-based heuristic on IPC 2023 learning track (LT)
benchmarks. We also show the efficiency of LazyLifted on high-branching factor
tasks and show that it surpasses LAMA in the combined IPC 2023 LT and
high-branching factor benchmarks.

</details>


### [137] [A Picture is Worth a Thousand Prompts? Efficacy of Iterative Human-Driven Prompt Refinement in Image Regeneration Tasks](https://arxiv.org/abs/2504.20340)
*Khoi Trinh,Scott Seidenberger,Raveen Wijewickrama,Murtuza Jadliwala,Anindya Maiti*

Main category: cs.AI

TL;DR: 该论文研究了AI生成图像中的“图像再生”概念，即通过迭代优化提示词来重现特定目标图像，并探讨了图像相似度指标（ISMs）是否与人类主观评价一致。研究发现，迭代优化能显著提升图像对齐效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容在互联网和社交媒体上的普及，研究如何通过迭代优化提示词来精确重现目标图像变得尤为重要。此外，需验证图像相似度指标是否能可靠地反映人类感知，以支持迭代优化流程。

Method: 通过结构化用户研究，评估迭代提示词优化对图像再生与目标图像相似度的影响，并比较主观评价与定量指标（ISMs）的一致性。

Result: 研究发现，迭代优化提示词能显著提升图像再生的对齐效果，且主观评价与定量指标均验证了这一点。

Conclusion: 迭代优化流程在生成AI内容中具有广泛潜力，未来可应用于更多领域以提升生成质量。

Abstract: With AI-generated content becoming ubiquitous across the web, social media,
and other digital platforms, it is vital to examine how such content are
inspired and generated. The creation of AI-generated images often involves
refining the input prompt iteratively to achieve desired visual outcomes. This
study focuses on the relatively underexplored concept of image regeneration
using AI, in which a human operator attempts to closely recreate a specific
target image by iteratively refining their prompt. Image regeneration is
distinct from normal image generation, which lacks any predefined visual
reference. A separate challenge lies in determining whether existing image
similarity metrics (ISMs) can provide reliable, objective feedback in iterative
workflows, given that we do not fully understand if subjective human judgments
of similarity align with these metrics. Consequently, we must first validate
their alignment with human perception before assessing their potential as a
feedback mechanism in the iterative prompt refinement process. To address these
research gaps, we present a structured user study evaluating how iterative
prompt refinement affects the similarity of regenerated images relative to
their targets, while also examining whether ISMs capture the same improvements
perceived by human observers. Our findings suggest that incremental prompt
adjustments substantially improve alignment, verified through both subjective
evaluations and quantitative measures, underscoring the broader potential of
iterative workflows to enhance generative AI content creation across various
application domains.

</details>


### [138] [Skill Discovery for Software Scripting Automation via Offline Simulations with LLMs](https://arxiv.org/abs/2504.20406)
*Paiheng Xu,Gang Wu,Xiang Chen,Tong Yu,Chang Xiao,Franck Dernoncourt,Tianyi Zhou,Wei Ai,Viswanathan Swaminathan*

Main category: cs.AI

TL;DR: 该论文提出了一种离线模拟框架，通过利用LLMs和公开脚本指南来策划软件特定的技能集（已验证脚本），以提高自动化成功率和降低成本。


<details>
  <summary>Details</summary>
Motivation: 传统脚本创建需要编程知识，LLMs生成代码存在安全风险和高成本，因此需要一种更安全、高效的方法来帮助用户自动化软件任务。

Method: 框架包含两部分：(1)任务创建，结合自上而下功能指导和自下而上API协同探索；(2)技能生成与验证，通过执行反馈优化脚本。还引入了基于GNN的链接预测模型来捕获API协同，扩展技能多样性。

Result: 在Adobe Illustrator上的实验表明，该框架显著提高了自动化成功率，减少了响应时间，并降低了运行时令牌成本。

Conclusion: 这是首次将软件脚本接口作为LLM系统测试平台，展示了在受控环境中利用执行反馈的优势，为AI在专业软件领域的应用提供了新思路。

Abstract: Scripting interfaces enable users to automate tasks and customize software
workflows, but creating scripts traditionally requires programming expertise
and familiarity with specific APIs, posing barriers for many users. While Large
Language Models (LLMs) can generate code from natural language queries, runtime
code generation is severely limited due to unverified code, security risks,
longer response times, and higher computational costs. To bridge the gap, we
propose an offline simulation framework to curate a software-specific skillset,
a collection of verified scripts, by exploiting LLMs and publicly available
scripting guides. Our framework comprises two components: (1) task creation,
using top-down functionality guidance and bottom-up API synergy exploration to
generate helpful tasks; and (2) skill generation with trials, refining and
validating scripts based on execution feedback. To efficiently navigate the
extensive API landscape, we introduce a Graph Neural Network (GNN)-based link
prediction model to capture API synergy, enabling the generation of skills
involving underutilized APIs and expanding the skillset's diversity.
Experiments with Adobe Illustrator demonstrate that our framework significantly
improves automation success rates, reduces response time, and saves runtime
token costs compared to traditional runtime code generation. This is the first
attempt to use software scripting interfaces as a testbed for LLM-based
systems, highlighting the advantages of leveraging execution feedback in a
controlled environment and offering valuable insights into aligning AI
capabilities with user needs in specialized software domains.

</details>


### [139] [RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library](https://arxiv.org/abs/2504.20426)
*Jiapeng Wang,Jinhao Jiang,Zhiqiang Zhang,Jun Zhou,Wayne Xin Zhao*

Main category: cs.AI

TL;DR: RV-Syn 是一种新的数学数据合成方法，通过构建结构化数学操作函数库和计算图，确保问题的逻辑性和可验证性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前数据合成方法在问题生成中难以掌握内在逻辑并确保解决方案的可验证性，RV-Syn 旨在解决这些问题。

Method: 构建数学操作函数库，生成计算图作为解决方案，并通过反向翻译生成复杂问题，实现逻辑感知的问题生成。

Result: 实验显示，RV-Syn 在数据扩展效率和生成质量上优于包括人工生成问题在内的现有方法。

Conclusion: RV-Syn 为生成高质量推理数据集提供了一个可扩展的框架。

Abstract: The advancement of reasoning capabilities in Large Language Models (LLMs)
requires substantial amounts of high-quality reasoning data, particularly in
mathematics. Existing data synthesis methods, such as data augmentation from
annotated training sets or direct question generation based on relevant
knowledge points and documents, have expanded datasets but face challenges in
mastering the inner logic of the problem during generation and ensuring the
verifiability of the solutions. To address these issues, we propose RV-Syn, a
novel Rational and Verifiable mathematical Synthesis approach. RV-Syn
constructs a structured mathematical operation function library based on
initial seed problems and generates computational graphs as solutions by
combining Python-formatted functions from this library. These graphs are then
back-translated into complex problems. Based on the constructed computation
graph, we achieve solution-guided logic-aware problem generation. Furthermore,
the executability of the computational graph ensures the verifiability of the
solving process. Experimental results show that RV-Syn surpasses existing
synthesis methods, including those involving human-generated problems,
achieving greater efficient data scaling. This approach provides a scalable
framework for generating high-quality reasoning datasets.

</details>


### [140] [Head-Tail-Aware KL Divergence in Knowledge Distillation for Spiking Neural Networks](https://arxiv.org/abs/2504.20445)
*Tianqing Zhang,Zixin Zhu,Kairong Yu,Hongwei Wang*

Main category: cs.AI

TL;DR: 论文提出了一种名为HTA-KL的新知识蒸馏方法，用于提升脉冲神经网络（SNN）的性能，通过动态区分高低概率区域并平衡知识转移，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于现有训练方法和模型限制，SNN性能通常落后于人工神经网络（ANN），而传统KL散度方法未能充分利用SNN特性，导致泛化能力不足。

Method: 提出了HTA-KL散度方法，通过累积概率掩码动态区分高低概率区域，并结合正向和反向KL散度对齐分布的头尾区域。

Result: 在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上，HTA-KL方法在较少时间步长下优于现有方法。

Conclusion: HTA-KL方法通过平衡知识转移显著提升了SNN性能，为解决SNN与ANN的性能差距提供了有效途径。

Abstract: Spiking Neural Networks (SNNs) have emerged as a promising approach for
energy-efficient and biologically plausible computation. However, due to
limitations in existing training methods and inherent model constraints, SNNs
often exhibit a performance gap when compared to Artificial Neural Networks
(ANNs). Knowledge distillation (KD) has been explored as a technique to
transfer knowledge from ANN teacher models to SNN student models to mitigate
this gap. Traditional KD methods typically use Kullback-Leibler (KL) divergence
to align output distributions. However, conventional KL-based approaches fail
to fully exploit the unique characteristics of SNNs, as they tend to
overemphasize high-probability predictions while neglecting low-probability
ones, leading to suboptimal generalization. To address this, we propose
Head-Tail Aware Kullback-Leibler (HTA-KL) divergence, a novel KD method for
SNNs. HTA-KL introduces a cumulative probability-based mask to dynamically
distinguish between high- and low-probability regions. It assigns adaptive
weights to ensure balanced knowledge transfer, enhancing the overall
performance. By integrating forward KL (FKL) and reverse KL (RKL) divergence,
our method effectively align both head and tail regions of the distribution. We
evaluate our methods on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets. Our
method outperforms existing methods on most datasets with fewer timesteps.

</details>


### [141] [TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data](https://arxiv.org/abs/2504.20462)
*Qi Wang,Xiao Zhang,Mingyi Li,Yuan Yuan,Mengbai Xiao,Fuzhen Zhuang,Dongxiao Yu*

Main category: cs.AI

TL;DR: 论文提出了一种名为TAMO的工具辅助LLM代理，通过多模态观测数据进行细粒度根因分析，解决了现有LLM方法在文本输入限制、动态服务依赖幻觉和上下文窗口限制方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着微服务和云原生技术的发展，分布式系统的复杂性增加，传统根因分析依赖人工干预，难以实现自动化。LLM在上下文推理和领域知识整合方面的突破为AIOps提供了新思路，但现有方法存在局限性。

Method: TAMO统一多模态观测数据为时间对齐表示，提取一致特征，并使用专门的根因定位和故障分类工具感知上下文环境，通过结构化提示指导LLM生成与系统上下文一致的修复策略。

Result: 实验结果显示，TAMO在处理具有异构性和常见故障类型的公共数据集时表现良好，证明了其有效性。

Conclusion: TAMO通过结合多模态数据和LLM，显著提升了根因分析的自动化能力和准确性，为解决分布式系统中的故障问题提供了新方法。

Abstract: With the development of distributed systems, microservices and cloud native
technologies have become central to modern enterprise software development.
Despite bringing significant advantages, these technologies also increase
system complexity and operational challenges. Traditional root cause analysis
(RCA) struggles to achieve automated fault response, heavily relying on manual
intervention. In recent years, large language models (LLMs) have made
breakthroughs in contextual inference and domain knowledge integration,
providing new solutions for Artificial Intelligence for Operations (AIOps).
However, Existing LLM-based approaches face three key challenges: text input
constraints, dynamic service dependency hallucinations, and context window
limitations. To address these issues, we propose a tool-assisted LLM agent with
multi-modality observation data, namely TAMO, for fine-grained RCA. It unifies
multi-modal observational data into time-aligned representations to extract
consistent features and employs specialized root cause localization and fault
classification tools for perceiving the contextual environment. This approach
overcomes the limitations of LLM in handling real-time changing service
dependencies and raw observational data and guides LLM to generate repair
strategies aligned with system contexts by structuring key information into a
prompt. Experimental results show that TAMO performs well in root cause
analysis when dealing with public datasets characterized by heterogeneity and
common fault types, demonstrating its effectiveness.

</details>


### [142] [A Summary on GUI Agents with Foundation Models Enhanced by Reinforcement Learning](https://arxiv.org/abs/2504.20464)
*Jiahao Li,Kaer Huang*

Main category: cs.AI

TL;DR: 该论文总结了基于多模态大语言模型（MLLM）的图形用户界面（GUI）智能体的最新进展，尤其是强化学习（RL）增强的架构，包括任务定义、模块化架构、训练方法及未来方向。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过多模态大语言模型和强化学习提升GUI智能体的交互能力，以应对复杂现实环境中的挑战。

Method: 将GUI任务形式化为马尔可夫决策过程，分析模块化架构（感知、规划、执行），比较基于提示、监督微调和强化学习的训练方法。

Result: 多模态感知、决策推理和自适应动作生成等创新显著提升了GUI智能体在复杂环境中的泛化性和鲁棒性。

Conclusion: 尽管取得了进展，但仍需解决关键挑战以构建更强大可靠的GUI智能体，并指出了未来研究方向。

Abstract: Graphical User Interface (GUI) agents, driven by Multi-modal Large Language
Models (MLLMs), have emerged as a promising paradigm for enabling intelligent
interaction with digital systems. This paper provides a structured summary of
recent advances in GUI agents, focusing on architectures enhanced by
Reinforcement Learning (RL). We first formalize GUI agent tasks as Markov
Decision Processes and discuss typical execution environments and evaluation
metrics. We then review the modular architecture of (M)LLM-based GUI agents,
covering Perception, Planning, and Acting modules, and trace their evolution
through representative works. Furthermore, we categorize GUI agent training
methodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and
RL-based approaches, highlighting the progression from simple prompt
engineering to dynamic policy learning via RL. Our summary illustrates how
recent innovations in multimodal perception, decision reasoning, and adaptive
action generation have significantly improved the generalization and robustness
of GUI agents in complex real-world environments. We conclude by identifying
key challenges and future directions for building more capable and reliable GUI
agents.

</details>


### [143] [MuRAL: A Multi-Resident Ambient Sensor Dataset Annotated with Natural Language for Activities of Daily Living](https://arxiv.org/abs/2504.20505)
*Xi Chen,Julien Cumin,Fano Ramparany,Dominique Vaufreydaz*

Main category: cs.AI

TL;DR: 摘要介绍了MuRAL，首个多居民环境下带有自然语言注释的传感器数据集，用于支持大语言模型在活动识别中的研究，同时指出现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的HAR数据集缺乏为大语言模型设计的上下文丰富性和注释粒度，无法充分利用LLM的能力。

Method: 引入MuRAL数据集，包含21小时的多用户传感器数据，并用细粒度自然语言描述、居民身份和高层活动标签进行注释，在动态多居民环境中评估LLM的表现。

Result: LLM能够提供丰富的语义解释，但在处理多用户模糊性和传感器上下文不足时仍面临挑战。

Conclusion: 发布MuRAL数据集以支持未来LLM驱动的智能环境活动理解研究。

Abstract: Recent advances in Large Language Models (LLMs) have shown promising
potential for human activity recognition (HAR) using ambient sensors,
especially through natural language reasoning and zero-shot learning. However,
existing datasets such as CASAS, ARAS, and MARBLE were not originally designed
with LLMs in mind and therefore lack the contextual richness, complexity, and
annotation granularity required to fully exploit LLM capabilities. In this
paper, we introduce MuRAL, the first Multi-Resident Ambient sensor dataset with
natural Language, comprising over 21 hours of multi-user sensor data collected
from 21 sessions in a smart-home environment. MuRAL is annotated with
fine-grained natural language descriptions, resident identities, and high-level
activity labels, all situated in dynamic, realistic multi-resident settings. We
benchmark MuRAL using state-of-the-art LLMs for three core tasks: subject
assignment, action description, and activity classification. Our results
demonstrate that while LLMs can provide rich semantic interpretations of
ambient data, current models still face challenges in handling multi-user
ambiguity and under-specified sensor contexts. We release MuRAL to support
future research on LLM-powered, explainable, and socially aware activity
understanding in smart environments. For access to the dataset, please reach
out to us via the provided contact information. A direct link for dataset
retrieval will be made available at this location in due course.

</details>


### [144] [ReasonIR: Training Retrievers for Reasoning Tasks](https://arxiv.org/abs/2504.20595)
*Rulin Shao,Rui Qiao,Varsha Kishore,Niklas Muennighoff,Xi Victoria Lin,Daniela Rus,Bryan Kian Hsiang Low,Sewon Min,Wen-tau Yih,Pang Wei Koh,Luke Zettlemoyer*

Main category: cs.AI

TL;DR: ReasonIR-8B是针对通用推理任务的首个检索模型，通过合成数据训练，在BRIGHT基准测试中取得最优成绩，并提升RAG任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索模型在推理任务上表现有限，训练数据多为简单事实查询，缺乏对复杂推理的支持。

Method: 通过合成数据生成流程创建高难度查询和硬负例，结合公共数据训练模型。

Result: 在BRIGHT基准测试中达到29.9 nDCG@10（无重排）和36.9 nDCG@10（带重排）；MMLU和GPQA任务分别提升6.4%和22.6%。

Conclusion: ReasonIR-8B在推理任务中表现优异，其训练方法适用于未来LLM改进，代码和模型已开源。

Abstract: We present ReasonIR-8B, the first retriever specifically trained for general
reasoning tasks. Existing retrievers have shown limited gains on reasoning
tasks, in part because existing training datasets focus on short factual
queries tied to documents that straightforwardly answer them. We develop a
synthetic data generation pipeline that, for each document, our pipeline
creates a challenging and relevant query, along with a plausibly related but
ultimately unhelpful hard negative. By training on a mixture of our synthetic
data and existing public data, ReasonIR-8B achieves a new state-of-the-art of
29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a
widely-used reasoning-intensive information retrieval (IR) benchmark. When
applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%
and 22.6% respectively, relative to the closed-book baseline, outperforming
other retrievers and search engines. In addition, ReasonIR-8B uses test-time
compute more effectively: on BRIGHT, its performance consistently increases
with longer and more information-rich rewritten queries; it continues to
outperform other retrievers when combined with an LLM reranker. Our training
recipe is general and can be easily extended to future LLMs; to this end, we
open-source our code, data, and model.

</details>


### [145] [PaRT: Enhancing Proactive Social Chatbots with Personalized Real-Time Retrieval](https://arxiv.org/abs/2504.20624)
*Zihan Niu,Zheyong Xie,Shaosheng Cao,Chonggang Lu,Zheyu Ye,Tong Xu,Zuozhu Liu,Yan Gao,Jia Chen,Zhe Xu,Yi Wu,Yao Hu*

Main category: cs.AI

TL;DR: 该论文提出了PaRT框架，通过个性化实时检索与生成技术，使社交聊天机器人能够主动发起对话。实验显示，PaRT显著提升了对话时长（21.77%）。


<details>
  <summary>Details</summary>
Motivation: 传统聊天机器人依赖用户主动维持对话，导致参与度低、对话短暂。PaRT旨在通过主动对话机制提升用户互动体验。

Method: PaRT整合用户画像与对话上下文，通过LLM识别意图并生成个性化话题，结合RedNote检索相关知识生成优化回复。

Result: 实际生产环境中稳定运行30天，平均对话时长提升21.77%。

Conclusion: PaRT框架有效解决了传统聊天机器人互动不足的问题，显著提升了用户参与度与对话质量。

Abstract: Social chatbots have become essential intelligent companions in daily
scenarios ranging from emotional support to personal interaction. However,
conventional chatbots with passive response mechanisms usually rely on users to
initiate or sustain dialogues by bringing up new topics, resulting in
diminished engagement and shortened dialogue duration. In this paper, we
present PaRT, a novel framework enabling context-aware proactive dialogues for
social chatbots through personalized real-time retrieval and generation.
Specifically, PaRT first integrates user profiles and dialogue context into a
large language model (LLM), which is initially prompted to refine user queries
and recognize their underlying intents for the upcoming conversation. Guided by
refined intents, the LLM generates personalized dialogue topics, which then
serve as targeted queries to retrieve relevant passages from RedNote. Finally,
we prompt LLMs with summarized passages to generate knowledge-grounded and
engagement-optimized responses. Our approach has been running stably in a
real-world production environment for more than 30 days, achieving a 21.77\%
improvement in the average duration of dialogues.

</details>


### [146] [Cognitive maps are generative programs](https://arxiv.org/abs/2504.20628)
*Marta Kryven,Cole Wyeth,Aidan Curtis,Kevin Ellis*

Main category: cs.AI

TL;DR: 该论文提出人类的高效计划可能源于将世界表示为可预测结构，利用生成性程序模型模拟认知地图，并通过实验和计算模型验证了这一假设。


<details>
  <summary>Details</summary>
Motivation: 探讨人类如何通过简化的心理表征（认知地图）在有限资源下高效规划行为，假设这种效率源于将世界视为可预测结构。

Method: 提出生成性程序作为认知地图形式，通过行为实验验证模块化规划策略，开发计算模型（基于LLM嵌入人类先验知识）预测人类行为。

Result: 模型在计算效率、内存占用和预测人类行为方面优于非结构化规划算法，支持‘程序化认知地图’假说。

Conclusion: 人类高效规划依赖于将世界抽象为程序化认知地图，利用可预测性和冗余性优化资源分配。

Abstract: Making sense of the world and acting in it relies on building simplified
mental representations that abstract away aspects of reality. This principle of
cognitive mapping is universal to agents with limited resources. Living
organisms, people, and algorithms all face the problem of forming functional
representations of their world under various computing constraints. In this
work, we explore the hypothesis that human resource-efficient planning may
arise from representing the world as predictably structured. Building on the
metaphor of concepts as programs, we propose that cognitive maps can take the
form of generative programs that exploit predictability and redundancy, in
contrast to directly encoding spatial layouts. We use a behavioral experiment
to show that people who navigate in structured spaces rely on modular planning
strategies that align with programmatic map representations. We describe a
computational model that predicts human behavior in a variety of structured
scenarios. This model infers a small distribution over possible programmatic
cognitive maps conditioned on human prior knowledge of the world, and uses this
distribution to generate resource-efficient plans. Our models leverages a Large
Language Model as an embedding of human priors, implicitly learned through
training on a vast corpus of human data. Our model demonstrates improved
computational efficiency, requires drastically less memory, and outperforms
unstructured planning algorithms with cognitive constraints at predicting human
behavior, suggesting that human planning strategies rely on programmatic
cognitive maps.

</details>


### [147] [The Limits of AI Explainability: An Algorithmic Information Theory Approach](https://arxiv.org/abs/2504.20676)
*Shrisha Rao*

Main category: cs.AI

TL;DR: 该论文通过算法信息理论为AI可解释性的基本限制建立了理论基础，量化了模型简化的近似误差和解释复杂性，提出复杂性差距定理和解释复杂性的精确界限，并探讨了局部与全局可解释性之间的差距及监管不可能性。


<details>
  <summary>Details</summary>
Motivation: 为理解AI可解释性的基本限制，通过理论方法量化解释复杂性与误差的关系，为可解释AI系统的设计和评估提供理论支持。

Method: 使用算法信息理论（Kolmogorov复杂度）形式化可解释性，提出复杂性差距定理、解释复杂性的精确界限，并分析局部与全局可解释性的差异。

Result: 证明显著简化的解释必然在某些输入上与原始模型不同；解释复杂性随输入维度指数增长但对Lipschitz函数多项式增长；局部解释可在相关区域保持准确性；提出监管不可实现性定理。

Conclusion: 研究强调了在可解释AI设计中需权衡模型复杂性、解释误差和可理解性，对系统设计、评估和监管具有重要启示。

Abstract: This paper establishes a theoretical foundation for understanding the
fundamental limits of AI explainability through algorithmic information theory.
We formalize explainability as the approximation of complex models by simpler
ones, quantifying both approximation error and explanation complexity using
Kolmogorov complexity. Our key theoretical contributions include: (1) a
complexity gap theorem proving that any explanation significantly simpler than
the original model must differ from it on some inputs; (2) precise bounds
showing that explanation complexity grows exponentially with input dimension
but polynomially with error tolerance for Lipschitz functions; and (3) a
characterization of the gap between local and global explainability,
demonstrating that local explanations can be significantly simpler while
maintaining accuracy in relevant regions. We further establish a regulatory
impossibility theorem proving that no governance framework can simultaneously
pursue unrestricted AI capabilities, human-interpretable explanations, and
negligible error. These results highlight considerations likely to be relevant
to the design, evaluation, and oversight of explainable AI systems.

</details>


### [148] [Graph-Based Fault Diagnosis for Rotating Machinery: Adaptive Segmentation and Structural Feature Integration](https://arxiv.org/abs/2504.20756)
*Moirangthem Tiken Singh*

Main category: cs.AI

TL;DR: 该论文提出了一种基于图的新型框架，用于旋转机械的多故障诊断，具有高准确性和鲁棒性，且在噪声和跨域场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的故障诊断方法在复杂环境下的准确性和可解释性不足，因此需要一种新型框架来克服这些问题。

Method: 采用熵优化信号分割、时频特征提取和图建模，结合图度量和局部特征进行分类。

Result: 在CWRU和SU数据集上分别达到99.8%和100%的准确率，噪声下保持95.4%准确率，跨域转移F1分数达99.7%。

Conclusion: 该方法无需深度学习架构，兼具高可靠性、可解释性和实时部署潜力，适用于工业诊断。

Abstract: This paper proposes a novel graph-based framework for robust and
interpretable multiclass fault diagnosis in rotating machinery. The method
integrates entropy-optimized signal segmentation, time-frequency feature
extraction, and graph-theoretic modeling to transform vibration signals into
structured representations suitable for classification. Graph metrics, such as
average shortest path length, modularity, and spectral gap, are computed and
combined with local features to capture global and segment-level fault
characteristics. The proposed method achieves high diagnostic accuracy when
evaluated on two benchmark datasets, the CWRU bearing dataset (under 0-3 HP
loads) and the SU gearbox and bearing datasets (under different speed-load
configurations). Classification scores reach up to 99.8% accuracy on Case
Western Reserve University (CWRU) and 100% accuracy on the Southeast University
datasets using a logistic regression classifier. Furthermore, the model
exhibits strong noise resilience, maintaining over 95.4% accuracy at high noise
levels (standard deviation = 0.5), and demonstrates excellent cross-domain
transferability with up to 99.7% F1-score in load-transfer scenarios. Compared
to traditional techniques, this approach requires no deep learning
architecture, enabling lower complexity while ensuring interpretability. The
results confirm the method's scalability, reliability, and potential for
real-time deployment in industrial diagnostics.

</details>


### [149] [Approximate Lifted Model Construction](https://arxiv.org/abs/2504.20784)
*Malte Luttermann,Jan Speller,Marcel Gehrke,Tanya Braun,Ralf Möller,Mattis Hartwig*

Main category: cs.AI

TL;DR: 提出ε-ACP算法，允许势函数的偏差，解决了传统ACP算法在数据学习应用中因势函数偏差无法识别不可区分性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统ACP算法要求势函数完全匹配才能利用不可区分性，但在实际应用中，从数据学习的势函数难免存在偏差，限制了ACP的实用性。

Method: 引入ε-ACP算法，通过超参数ε控制势函数的偏差范围，从而识别并利用非精确的不可区分性。

Result: 理论证明ε-ACP的近似误差严格有界，实验显示实际误差接近零。

Conclusion: ε-ACP有效解决了ACP在实际应用中的限制，为概率关系模型提供了更实用的推断方法。

Abstract: Probabilistic relational models such as parametric factor graphs enable
efficient (lifted) inference by exploiting the indistinguishability of objects.
In lifted inference, a representative of indistinguishable objects is used for
computations. To obtain a relational (i.e., lifted) representation, the
Advanced Colour Passing (ACP) algorithm is the state of the art. The ACP
algorithm, however, requires underlying distributions, encoded as
potential-based factorisations, to exactly match to identify and exploit
indistinguishabilities. Hence, ACP is unsuitable for practical applications
where potentials learned from data inevitably deviate even if associated
objects are indistinguishable. To mitigate this problem, we introduce the
$\varepsilon$-Advanced Colour Passing ($\varepsilon$-ACP) algorithm, which
allows for a deviation of potentials depending on a hyperparameter
$\varepsilon$. $\varepsilon$-ACP efficiently uncovers and exploits
indistinguishabilities that are not exact. We prove that the approximation
error induced by $\varepsilon$-ACP is strictly bounded and our experiments show
that the approximation error is close to zero in practice.

</details>


### [150] [Partitioned Memory Storage Inspired Few-Shot Class-Incremental learning](https://arxiv.org/abs/2504.20797)
*Renye Zhang,Yimin Yin,Jinghua Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种解决Few-Shot Class-Incremental Learning (FSCIL)中稳定性-可塑性难题的方法，通过为每个会话学习独立模型来避免灾难性遗忘，并结合不确定性量化进行模型部署，在CIFAR-100和mini-ImageNet数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习技术对大量训练数据的依赖及对动态世界的适应性不足，与人类智能存在差距。FSCIL致力于用有限样本持续学习新类别而不遗忘旧知识，但现有方法因使用单一模型面临稳定性-可塑性难题。受人类大脑不同知识存储于不同脑区的启发，论文提出学习独立模型以避免遗忘。

Method: 为每个会话学习独立模型，避免灾难性遗忘；测试阶段结合不确定性量化（UQ）进行模型部署。

Result: 在CIFAR-100和mini-ImageNet数据集上实现了state-of-the-art性能。

Conclusion: 该方法为FSCIL提供了新思路，通过独立模型和UQ的有效结合，显著提升了性能。

Abstract: Current mainstream deep learning techniques exhibit an over-reliance on
extensive training data and a lack of adaptability to the dynamic world,
marking a considerable disparity from human intelligence. To bridge this gap,
Few-Shot Class-Incremental Learning (FSCIL) has emerged, focusing on continuous
learning of new categories with limited samples without forgetting old
knowledge. Existing FSCIL studies typically use a single model to learn
knowledge across all sessions, inevitably leading to the stability-plasticity
dilemma. Unlike machines, humans store varied knowledge in different cerebral
cortices. Inspired by this characteristic, our paper aims to develop a method
that learns independent models for each session. It can inherently prevent
catastrophic forgetting. During the testing stage, our method integrates
Uncertainty Quantification (UQ) for model deployment. Our method provides a
fresh viewpoint for FSCIL and demonstrates the state-of-the-art performance on
CIFAR-100 and mini-ImageNet datasets.

</details>


### [151] [Ascendra: Dynamic Request Prioritization for Efficient LLM Serving](https://arxiv.org/abs/2504.20828)
*Azam Ikram,Xiang Li,Sameh Elnikety,Saurabh Bagchi*

Main category: cs.AI

TL;DR: Ascendra是一种新型的LLM服务系统，能同时优化TTFT和TBT SLOs，利用动态资源分区提高吞吐量和响应速度。


<details>
  <summary>Details</summary>
Motivation: 现有系统常牺牲一个SLO以优化另一个，难以同时满足TTFT和TBT需求。需要一种更平衡的方案。

Method: 将GPU资源分为低优先级（优化吞吐量）和高优先级（优化延迟）实例，通过性能模型预测并转移可能超时的请求。

Result: 相比vLLM和Sarathi-Serve，Ascendra将吞吐量提升1.7倍，同时满足双SLO目标。

Conclusion: Ascendra的动态资源分区设计有效解决了吞吐量与延迟的权衡问题，为LLM服务提供了高效解决方案。

Abstract: The rapid advancement of Large Language Models (LLMs) has driven the need for
more efficient serving strategies. In this context, efficiency refers to the
proportion of requests that meet their Service Level Objectives (SLOs),
particularly for Time To First Token (TTFT) and Time Between Tokens (TBT).
However, existing systems often prioritize one metric at the cost of the other.
We present Ascendra, an LLM serving system designed to meet both TTFT and TBT
SLOs simultaneously. The core insight behind Ascendra is that a request's
urgency evolves as it approaches its deadline. To leverage this, Ascendra
partitions GPU resources into two types of instances: low-priority and
high-priority. Low-priority instances maximize throughput by processing
requests out of arrival order, but at the risk of request starvation. To
address this, Ascendra employs a performance model to predict requests at risk
of missing their SLOs and proactively offloads them to high-priority instances.
High-priority instances are optimized for low-latency execution and handle
urgent requests nearing their deadlines. This partitioned architecture enables
Ascendra to effectively balance high throughput and low latency. Extensive
evaluation shows that Ascendra improves system throughput by up to 1.7x
compared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs.

</details>


### [152] [Disjunctive and Conjunctive Normal Form Explanations of Clusters Using Auxiliary Information](https://arxiv.org/abs/2504.20846)
*Robert F. Downey,S. S. Ravi*

Main category: cs.AI

TL;DR: 该论文探讨如何利用未用于聚类的辅助信息（标签）生成聚类的事后解释，包括析取形式（解释为一组标签）和两子句合取范式（CNF）形式（解释为两组标签的AND组合）。采用整数线性规划（ILP）和启发式方法生成解释，并通过多数据集实验验证方法的可扩展性和效果。


<details>
  <summary>Details</summary>
Motivation: 聚类结果的解释性对实际应用至关重要，但现有方法常依赖聚类算法本身的信息。作者希望通过未使用的辅助信息（标签）提供更直观的解释，提升聚类的可理解性。

Method: 提出两种解释形式（析取形式和两子句CNF），并采用整数线性规划（ILP）和启发式方法生成解释。实验覆盖多数据集，验证方法的可行性和效率。

Result: 实验展示了不同数据集上生成解释的有效性，同时证明了方法的可扩展性，尤其在处理大规模数据时仍保持高效。

Conclusion: 通过辅助标签和优化技术，能够生成可理解且高效的聚类解释，为实际场景中聚类的透明性提供了实用工具。

Abstract: We consider generating post-hoc explanations of clusters generated from
various datasets using auxiliary information which was not used by clustering
algorithms. Following terminology used in previous work, we refer to the
auxiliary information as tags. Our focus is on two forms of explanations,
namely disjunctive form (where the explanation for a cluster consists of a set
of tags) and a two-clause conjunctive normal form (CNF) explanation (where the
explanation consists of two sets of tags, combined through the AND operator).
We use integer linear programming (ILP) as well as heuristic methods to
generate these explanations. We experiment with a variety of datasets and
discuss the insights obtained from our explanations. We also present
experimental results regarding the scalability of our explanation methods.

</details>


### [153] [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879)
*Shivalika Singh,Yiyang Nan,Alex Wang,Daniel D'Souza,Sayash Kapoor,Ahmet Üstün,Sanmi Koyejo,Yuntian Deng,Shayne Longpre,Noah Smith,Beyza Ermis,Marzieh Fadaee,Sara Hooker*

Main category: cs.AI

TL;DR: 该论文指出Chatbot Arena排行榜存在系统性偏差，包括未公开的私有测试实践、选择性披露性能结果以及数据访问不对称，导致排名失真，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 揭示Chatbot Arena在AI系统排名中的不公平现象，推动更透明和公平的评测标准。

Method: 通过分析私有测试实践、数据访问模式和模型性能数据，识别排行榜的系统性偏差。

Result: 发现少数提供商通过选择性披露和更高采样率获得优势，开放权重模型数据访问不足，导致排名失真。

Conclusion: 提出改革评测框架的建议，以促进更公平和透明的基准测试。

Abstract: Measuring progress is fundamental to the advancement of any scientific field.
As benchmarks play an increasingly central role, they also grow more
susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard
for ranking the most capable AI systems. Yet, in this work we identify
systematic issues that have resulted in a distorted playing field. We find that
undisclosed private testing practices benefit a handful of providers who are
able to test multiple variants before public release and retract scores if
desired. We establish that the ability of these providers to choose the best
score leads to biased Arena scores due to selective disclosure of performance
results. At an extreme, we identify 27 private LLM variants tested by Meta in
the lead-up to the Llama-4 release. We also establish that proprietary closed
models are sampled at higher rates (number of battles) and have fewer models
removed from the arena than open-weight and open-source alternatives. Both
these policies lead to large data access asymmetries over time. Providers like
Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the
arena, respectively. In contrast, a combined 83 open-weight models have only
received an estimated 29.7% of the total data. We show that access to Chatbot
Arena data yields substantial benefits; even limited additional data can result
in relative performance gains of up to 112% on the arena distribution, based on
our conservative estimates. Together, these dynamics result in overfitting to
Arena-specific dynamics rather than general model quality. The Arena builds on
the substantial efforts of both the organizers and an open community that
maintains this valuable evaluation platform. We offer actionable
recommendations to reform the Chatbot Arena's evaluation framework and promote
fairer, more transparent benchmarking for the field

</details>


### [154] [CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models](https://arxiv.org/abs/2504.20898)
*Hasan Md Tusfiqur Alam,Devansh Srivastav,Abdulrahman Mohamed Selim,Md Abdul Kadir,Md Moktadiurl Hoque Shuvo,Daniel Sonntag*

Main category: cs.AI

TL;DR: 该论文提出了一种结合概念瓶颈模型（CBMs）和多智能体检索增强生成（RAG）系统的自动化放射学报告生成框架，旨在提高AI的可解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在自动化放射学工作流程中潜力巨大，但其可解释性和可靠性问题阻碍了临床应用。因此，作者希望通过结合CBMs和RAG系统来弥补这一差距。

Method: 框架使用CBMs将胸部X光特征映射到可理解的临床概念，实现透明疾病分类；同时利用RAG系统集成多智能体协作和外部知识，生成内容丰富的循证报告。

Result: 系统能够提供可解释的预测、减少幻觉，并通过交互界面生成高质量的定制报告，解决了准确性、信任和可用性问题。

Conclusion: 该框架为提升诊断一致性和为放射科医生提供可行的见解提供了途径。

Abstract: Advancements in generative Artificial Intelligence (AI) hold great promise
for automating radiology workflows, yet challenges in interpretability and
reliability hinder clinical adoption. This paper presents an automated
radiology report generation framework that combines Concept Bottleneck Models
(CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge
AI performance with clinical explainability. CBMs map chest X-ray features to
human-understandable clinical concepts, enabling transparent disease
classification. Meanwhile, the RAG system integrates multi-agent collaboration
and external knowledge to produce contextually rich, evidence-based reports.
Our demonstration showcases the system's ability to deliver interpretable
predictions, mitigate hallucinations, and generate high-quality, tailored
reports with an interactive interface addressing accuracy, trust, and usability
challenges. This framework provides a pathway to improving diagnostic
consistency and empowering radiologists with actionable insights.

</details>


### [155] [Leveraging Generative AI Through Prompt Engineering and Rigorous Validation to Create Comprehensive Synthetic Datasets for AI Training in Healthcare](https://arxiv.org/abs/2504.20921)
*Polycarp Nalela*

Main category: cs.AI

TL;DR: 论文提出使用GPT-4生成高质量合成医疗数据以解决隐私问题，并通过多模型验证确保数据质量，最终构建PostgreSQL数据库支持EHR应用。


<details>
  <summary>Details</summary>
Motivation: 解决因隐私限制导致的医疗数据获取困难，为AI算法训练提供替代方案。

Method: 利用GPT-4生成合成数据，采用BERT、GPT-2、RoBERTa等模型验证数据质量，并构建PostgreSQL数据库。

Result: 生成的合成数据质量高，可用于训练AI算法，同时避免隐私问题。

Conclusion: 生成式AI结合严格验证能有效解决医疗数据隐私与可访问性的矛盾。

Abstract: Access to high-quality medical data is often restricted due to privacy
concerns, posing significant challenges for training artificial intelligence
(AI) algorithms within Electronic Health Record (EHR) applications. In this
study, prompt engineering with the GPT-4 API was employed to generate
high-quality synthetic datasets aimed at overcoming this limitation. The
generated data encompassed a comprehensive array of patient admission
information, including healthcare provider details, hospital departments,
wards, bed assignments, patient demographics, emergency contacts, vital signs,
immunizations, allergies, medical histories, appointments, hospital visits,
laboratory tests, diagnoses, treatment plans, medications, clinical notes,
visit logs, discharge summaries, and referrals. To ensure data quality and
integrity, advanced validation techniques were implemented utilizing models
such as BERT's Next Sentence Prediction for sentence coherence, GPT-2 for
overall plausibility, RoBERTa for logical consistency, autoencoders for anomaly
detection, and conducted diversity analysis. Synthetic data that met all
validation criteria were integrated into a comprehensive PostgreSQL database,
serving as the data management system for the EHR application. This approach
demonstrates that leveraging generative AI models with rigorous validation can
effectively produce high-quality synthetic medical data, facilitating the
training of AI algorithms while addressing privacy concerns associated with
real patient data.

</details>


### [156] [A Domain-Agnostic Scalable AI Safety Ensuring Framework](https://arxiv.org/abs/2504.20924)
*Beomjun Kim,Kangyeon Kim,Sunwoo Kim,Heejin Ahn*

Main category: cs.AI

TL;DR: 提出了一种新型AI安全框架，可确保AI系统在多种领域中按用户定义的约束和概率运行，并通过优化问题和内部测试数据实现安全验证。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全方法通常局限于特定领域的预设条件，缺乏跨领域的普适性，因此需要一种更通用的解决方案。

Method: 结合AI组件（如神经网络）与优化问题，生成满足用户约束的响应；引入内部测试数据和保守测试方法以确保统计有效性。

Result: 数学证明在特定条件下可保证概率约束满足，实验显示在安全阈值低区域显著优于现有方法，并能有效扩展。

Conclusion: 该框架提供了跨领域的AI安全保障，并在多个应用中验证了其有效性。

Abstract: Ensuring the safety of AI systems has recently emerged as a critical priority
for real-world deployment, particularly in physical AI applications. Current
approaches to AI safety typically address predefined domain-specific safety
conditions, limiting their ability to generalize across contexts.
  We propose a novel AI safety framework that ensures AI systems comply with
\textbf{any user-defined constraint}, with \textbf{any desired probability},
and across \textbf{various domains}.
  In this framework, we combine an AI component (e.g., neural network) with an
optimization problem to produce responses that minimize objectives while
satisfying user-defined constraints with probabilities exceeding user-defined
thresholds. For credibility assessment of the AI component, we propose
\textit{internal test data}, a supplementary set of safety-labeled data, and a
\textit{conservative testing} methodology that provides statistical validity of
using internal test data. We also present an approximation method of a loss
function and how to compute its gradient for training.
  We mathematically prove that probabilistic constraint satisfaction is
guaranteed under specific, mild conditions and prove a scaling law between
safety and the number of internal test data. We demonstrate our framework's
effectiveness through experiments in diverse domains: demand prediction for
production decision, safe reinforcement learning within the SafetyGym
simulator, and guarding AI chatbot outputs. Through these experiments, we
demonstrate that our method guarantees safety for user-specified constraints,
outperforms {for \textbf{up to several order of magnitudes}} existing methods
in low safety threshold regions, and scales effectively with respect to the
size of internal test data.

</details>


### [157] [ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification](https://arxiv.org/abs/2504.20930)
*Ziqing Fan,Cheng Liang,Chaoyi Wu,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: cs.AI

TL;DR: ChestX-Reasoner是一个放射学诊断多模态大语言模型，通过从临床报告中提取的监督信号优化推理过程，显著提升了医学AI的诊断准确性和推理能力。


<details>
  <summary>Details</summary>
Motivation: 医学AI模型常忽视临床实践中的结构化推理过程，因此需要开发能模拟放射科医生逐步推理的模型。

Method: 构建大型数据集并提取推理链，采用两阶段训练（监督微调+强化学习），设计RadRBench-CXR基准和RadRScore评估指标。

Result: ChestX-Reasoner在诊断准确性和推理能力上显著优于现有医学和通用MLLM，最高提升27%的准确性。

Conclusion: 该模型通过临床推理对齐和开源资源推动了医学推理MLLM的研究。

Abstract: Recent advances in reasoning-enhanced large language models (LLMs) and
multimodal LLMs (MLLMs) have significantly improved performance in complex
tasks, yet medical AI models often overlook the structured reasoning processes
inherent in clinical practice. In this work, we present ChestX-Reasoner, a
radiology diagnosis MLLM designed to leverage process supervision mined
directly from clinical reports, reflecting the step-by-step reasoning followed
by radiologists. We construct a large dataset by extracting and refining
reasoning chains from routine radiology reports. Our two-stage training
framework combines supervised fine-tuning and reinforcement learning guided by
process rewards to better align model reasoning with clinical standards. We
introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual
question answering samples with 301K clinically validated reasoning steps, and
propose RadRScore, a metric evaluating reasoning factuality, completeness, and
effectiveness. ChestX-Reasoner outperforms existing medical and general-domain
MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,
and 18% improvements in reasoning ability compared to the best medical MLLM,
the best general MLLM, and its base model, respectively, as well as 3.3%, 24%,
and 27% improvements in outcome accuracy. All resources are open-sourced to
facilitate further research in medical reasoning MLLMs.

</details>


### [158] [Jekyll-and-Hyde Tipping Point in an AI's Behavior](https://arxiv.org/abs/2504.20980)
*Neil F. Johnson,Frank Yingjie Huo*

Main category: cs.AI

TL;DR: 论文提出了一个公式，从基本原理出发预测LLM（如ChatGPT）输出何时会突然变得错误、误导、无关或危险。公式仅需中学数学，量化了注意力分散导致的问题，并提出了通过改变提示和训练来延迟或防止这种转变的方法。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏科学方法预测或解释LLM输出何时会突然变得不可靠，这导致公众对AI的信任降低，甚至有人担心LLM可能突然‘翻脸’。论文旨在解决这一迫切需求，提供透明且可量化的分析。

Method: 从基本原理推导出一个精确公式，分析LLM注意力分散导致‘杰基尔-海德’临界点的机制，公式仅需中学数学知识。

Result: 公式量化了临界点的出现条件，并提出了通过调整提示和训练延迟或防止问题的方案。

Conclusion: 该公式为政策制定者和公众提供了一个量化平台，用于讨论AI的广泛使用和风险，同时解答了‘是否需要对LLM礼貌’等问题。

Abstract: Trust in AI is undermined by the fact that there is no science that predicts
-- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is
likely to tip mid-response to become wrong, misleading, irrelevant or
dangerous. With deaths and trauma already being blamed on LLMs, this
uncertainty is even pushing people to treat their 'pet' LLM more politely to
'dissuade' it (or its future Artificial General Intelligence offspring) from
suddenly turning on them. Here we address this acute need by deriving from
first principles an exact formula for when a Jekyll-and-Hyde tipping point
occurs at LLMs' most basic level. Requiring only secondary school mathematics,
it shows the cause to be the AI's attention spreading so thin it suddenly
snaps. This exact formula provides quantitative predictions for how the
tipping-point can be delayed or prevented by changing the prompt and the AI's
training. Tailored generalizations will provide policymakers and the public
with a firm platform for discussing any of AI's broader uses and risks, e.g. as
a personal counselor, medical advisor, decision-maker for when to use force in
a conflict situation. It also meets the need for clear and transparent answers
to questions like ''should I be polite to my LLM?''

</details>


### [159] [LTLf Adaptive Synthesis for Multi-Tier Goals in Nondeterministic Domains](https://arxiv.org/abs/2504.20983)
*Giuseppe De Giacomo,Gianmarco Parretti,Shufang Zhu*

Main category: cs.AI

TL;DR: 该论文研究了一种LTLf合成的变体，用于在非确定性规划领域中合成满足多层级目标的适应性策略，策略动态调整以最大化目标达成，计算效率高。


<details>
  <summary>Details</summary>
Motivation: 当前的非确定性规划领域需要更高效的策略来动态适应多层级目标，同时利用环境合作以进一步提升目标达成率。

Method: 采用基于博弈论的技术生成适应性策略，确保策略在动态执行中能最大化满足目标，且计算复杂度为二次方，适用于多目标场景。

Result: 提出的方法能高效生成适应性策略，且计算开销仅为目标数量的二次方，显著优于传统LTLf合成方法。

Conclusion: 该技术为多层级目标的高效动态合成提供了可行的解决方案，且在计算效率上具有显著优势。

Abstract: We study a variant of LTLf synthesis that synthesizes adaptive strategies for
achieving a multi-tier goal, consisting of multiple increasingly challenging
LTLf objectives in nondeterministic planning domains. Adaptive strategies are
strategies that at any point of their execution (i) enforce the satisfaction of
as many objectives as possible in the multi-tier goal, and (ii) exploit
possible cooperation from the environment to satisfy as many as possible of the
remaining ones. This happens dynamically: if the environment cooperates (ii)
and an objective becomes enforceable (i), then our strategies will enforce it.
We provide a game-theoretic technique to compute adaptive strategies that is
sound and complete. Notably, our technique is polynomial, in fact quadratic, in
the number of objectives. In other words, it handles multi-tier goals with only
a minor overhead compared to standard LTLf synthesis.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [160] [Generate more than one child in your co-evolutionary semi-supervised learning GAN](https://arxiv.org/abs/2504.20560)
*Francisco Sedeño,Jamal Toutouh,Francisco Chicano*

Main category: cs.NE

TL;DR: 论文提出了一种新的协同进化方法CE-SSLGAN，通过泛种群、精英替换和多后代策略改进传统SSL-GAN，实验结果表明其性能更优。


<details>
  <summary>Details</summary>
Motivation: 现有的协同进化SSL-GAN方法存在种群空间结构单一、单后代生成和代际替换策略的局限性，作者希望通过改进这些设计提升性能。

Method: 提出CE-SSLGAN方法，采用泛种群（无空间结构）、精英替换策略和每代多后代生成，并在三个标准数据集上验证。

Result: 实验证明，多后代生成和精英替换策略显著提升了模型性能，优于传统SSL-GAN方法。

Conclusion: CE-SSLGAN通过优化协同进化机制，为SSL-GAN提供了更高效的训练框架，未来可扩展至更复杂场景。

Abstract: Generative Adversarial Networks (GANs) are very useful methods to address
semi-supervised learning (SSL) datasets, thanks to their ability to generate
samples similar to real data. This approach, called SSL-GAN has attracted many
researchers in the last decade. Evolutionary algorithms have been used to guide
the evolution and training of SSL-GANs with great success. In particular,
several co-evolutionary approaches have been applied where the two networks of
a GAN (the generator and the discriminator) are evolved in separate
populations. The co-evolutionary approaches published to date assume some
spatial structure of the populations, based on the ideas of cellular
evolutionary algorithms. They also create one single individual per generation
and follow a generational replacement strategy in the evolution. In this paper,
we re-consider those algorithmic design decisions and propose a new
co-evolutionary approach, called Co-evolutionary Elitist SSL-GAN (CE-SSLGAN),
with panmictic population, elitist replacement, and more than one individual in
the offspring. We evaluate the performance of our proposed method using three
standard benchmark datasets. The results show that creating more than one
offspring per population and using elitism improves the results in comparison
with a classical SSL-GAN.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [161] [Towards Large Language Models for Lunar Mission Planning and In Situ Resource Utilization](https://arxiv.org/abs/2504.20125)
*Michael Pekala,Gregory Canal,Samuel Barham,Milena B. Graziano,Morgan Trexler,Leslie Hamilton,Elizabeth Reilly,Christopher D. Stiles*

Main category: cs.DL

TL;DR: 论文探讨了利用大型语言模型(LLMs)从科学文献中快速提取月球成分数据的可行性，并指出尽管现成LLMs在处理表格数据上表现良好，但在精细矿物学信息和复杂数据提取方面仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 月球任务规划需评估当地原材料可用性，但相关测量数据分散在多篇科学文献中，因此需要一种高效方法整合这些信息。

Method: 利用现成的大型语言模型(LLMs)处理科学文献语料库，提取月球成分数据，并分析其在异构数据和细微表征中的表现。

Result: 现成LLMs在提取表格数据时表现良好，但在捕捉精细矿物学信息和复杂数据方面仍需改进。

Conclusion: LLMs在快速提取月球成分数据方面具有潜力，但需进一步优化以提升对细微和复杂信息的处理能力。

Abstract: A key factor for lunar mission planning is the ability to assess the local
availability of raw materials. However, many potentially relevant measurements
are scattered across a variety of scientific publications. In this paper we
consider the viability of obtaining lunar composition data by leveraging LLMs
to rapidly process a corpus of scientific publications. While leveraging LLMs
to obtain knowledge from scientific documents is not new, this particular
application presents interesting challenges due to the heterogeneity of lunar
samples and the nuances involved in their characterization. Accuracy and
uncertainty quantification are particularly crucial since many materials
properties can be sensitive to small variations in composition. Our findings
indicate that off-the-shelf LLMs are generally effective at extracting data
from tables commonly found in these documents. However, there remains
opportunity to further refine the data we extract in this initial approach; in
particular, to capture fine-grained mineralogy information and to improve
performance on more subtle/complex pieces of information.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [162] [Edge-Based Learning for Improved Classification Under Adversarial Noise](https://arxiv.org/abs/2504.20077)
*Manish Kansana,Keyan Alexander Rahimi,Elias Hossain,Iman Dehzangi,Noorbakhsh Amiri Golilarz*

Main category: cs.CV

TL;DR: 对抗性噪声会影响深度学习模型的分类准确性，但通过边缘特征训练可以提高模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究对抗性噪声（如FGSM）对图像分类的影响，探索利用边缘特征提升模型鲁棒性的可能性。

Method: 在脑肿瘤和COVID数据集上，先训练模型于干净图像，再引入对抗性噪声进行测试，随后结合干净与噪声图像重新训练。最后，通过边缘特征训练验证其抗噪声能力。

Result: 边缘特征训练的模型对抗性噪声表现更鲁棒，但原数据重新训练的精度提升略优于边缘数据。

Conclusion: 边缘特征学习可增强模型对抗性扰动的抵抗力，但需权衡精度与鲁棒性。

Abstract: Adversarial noise introduces small perturbations in images, misleading deep
learning models into misclassification and significantly impacting recognition
accuracy. In this study, we analyzed the effects of Fast Gradient Sign Method
(FGSM) adversarial noise on image classification and investigated whether
training on specific image features can improve robustness. We hypothesize that
while adversarial noise perturbs various regions of an image, edges may remain
relatively stable and provide essential structural information for
classification. To test this, we conducted a series of experiments using brain
tumor and COVID datasets. Initially, we trained the models on clean images and
then introduced subtle adversarial perturbations, which caused deep learning
models to significantly misclassify the images. Retraining on a combination of
clean and noisy images led to improved performance. To evaluate the robustness
of the edge features, we extracted edges from the original/clean images and
trained the models exclusively on edge-based representations. When noise was
introduced to the images, the edge-based models demonstrated greater resilience
to adversarial attacks compared to those trained on the original or clean
images. These results suggest that while adversarial noise is able to exploit
complex non-edge regions significantly more than edges, the improvement in the
accuracy after retraining is marginally more in the original data as compared
to the edges. Thus, leveraging edge-based learning can improve the resilience
of deep learning models against adversarial perturbations.

</details>


### [163] [Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains](https://arxiv.org/abs/2504.20199)
*Juntian Zhang,Chuanqi cheng,Yuhan Liu,Wei Liu,Jian Luan,Rui Yan*

Main category: cs.CV

TL;DR: 提出Focus-Centric Visual Chain范式提升多图像场景下视觉语言模型的性能，并通过Focus-Centric Data Synthesis构建大规模数据集VISC-150K，实验表明方法在多个基准上平均提升3.16%和2.24%。


<details>
  <summary>Details</summary>
Motivation: 现实场景常涉及多图像输入，现有视觉语言模型在此类任务中表现欠佳，需提升其信息解耦和推理能力。

Method: 提出Focus-Centric Visual Chain范式，并通过Focus-Centric Data Synthesis自底向上构建带标注推理路径的大规模数据集VISC-150K。

Result: 在七个多图像基准测试中，两种模型架构平均分别提升3.16%和2.24%，且不影响通用视觉语言能力。

Conclusion: 该研究为实现更鲁棒、能处理复杂视觉场景的视觉语言系统迈出重要一步。

Abstract: Vision-language models (VLMs) achieve remarkable success in single-image
tasks. However, real-world scenarios often involve intricate multi-image
inputs, leading to a notable performance decline as models struggle to
disentangle critical information scattered across complex visual features. In
this work, we propose Focus-Centric Visual Chain, a novel paradigm that
enhances VLMs'perception, comprehension, and reasoning abilities in multi-image
scenarios. To facilitate this paradigm, we propose Focus-Centric Data
Synthesis, a scalable bottom-up approach for synthesizing high-quality data
with elaborate reasoning paths. Through this approach, We construct VISC-150K,
a large-scale dataset with reasoning data in the form of Focus-Centric Visual
Chain, specifically designed for multi-image tasks. Experimental results on
seven multi-image benchmarks demonstrate that our method achieves average
performance gains of 3.16% and 2.24% across two distinct model architectures,
without compromising the general vision-language capabilities. our study
represents a significant step toward more robust and capable vision-language
systems that can handle complex visual scenarios.

</details>


### [164] [Integration Flow Models](https://arxiv.org/abs/2504.20179)
*Jingjing Wang,Dan Zhang,Joshua Luo,Yin Yang,Feng Luo*

Main category: cs.CV

TL;DR: Integration Flow是一种直接学习ODE轨迹积分的方法，避免数值解ODE的离散化误差，提高生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有ODE生成模型因数值解的离散化误差或训练不稳定问题影响生成质量。

Method: 提出Integration Flow，直接学习ODE轨迹积分，并结合目标状态引导反向时间动态。

Result: 在CIFAR10和ImageNet上，Integration Flow在多个ODE生成模型中取得更好的一步生成效果。

Conclusion: Integration Flow在稳定性和准确性上有理论支持，并能提升现有ODE生成模型的性能。

Abstract: Ordinary differential equation (ODE) based generative models have emerged as
a powerful approach for producing high-quality samples in many applications.
However, the ODE-based methods either suffer the discretization error of
numerical solvers of ODE, which restricts the quality of samples when only a
few NFEs are used, or struggle with training instability. In this paper, we
proposed Integration Flow, which directly learns the integral of ODE-based
trajectory paths without solving the ODE functions. Moreover, Integration Flow
explicitly incorporates the target state $\mathbf{x}_0$ as the anchor state in
guiding the reverse-time dynamics. We have theoretically proven this can
contribute to both stability and accuracy. To the best of our knowledge,
Integration Flow is the first model with a unified structure to estimate
ODE-based generative models and the first to show the exact straightness of
1-Rectified Flow without reflow. Through theoretical analysis and empirical
evaluations, we show that Integration Flows achieve improved performance when
it is applied to existing ODE-based models, such as diffusion models, Rectified
Flows, and PFGM++. Specifically, Integration Flow achieves one-step generation
on CIFAR10 with FIDs of 2.86 for the Variance Exploding (VE) diffusion model,
3.36 for rectified flow without reflow, and 2.91 for PFGM++; and on ImageNet
with FIDs of 4.09 for VE diffusion model, 4.35 for rectified flow without
reflow and 4.15 for PFGM++.

</details>


### [165] [AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation](https://arxiv.org/abs/2504.20629)
*Jeongsoo Choi,Ji-Hoon Kim,Kim Sung-Bin,Tae-Hyun Oh,Joon Son Chung*

Main category: cs.CV

TL;DR: 该论文提出AlignDiT，一种多模态对齐扩散变换器，用于从文本、视频和参考音频生成高质量语音，解决了现有方法在语音清晰度、音视频同步、自然度和声音相似性上的不足。通过多模态分类器自由引导机制和对齐策略，AlignDiT在多个基准测试中表现优异，并展示了强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多模态到语音生成任务在电影制作、配音和虚拟形象等领域应用广泛，但现有方法在语音清晰度、同步性、自然度和声音相似性上存在局限，因此需要更高效的解决方案。

Method: 提出AlignDiT，基于扩散变换器（DiT）架构，采用三种对齐策略和多模态分类器自由引导机制，动态平衡多模态输入信息以生成高质量语音。

Result: AlignDiT在多个基准测试中显著优于现有方法，在质量、同步性和说话人相似性上表现优异，并在视频到语音合成和视觉强制对齐等任务中展现了强泛化能力。

Conclusion: AlignDiT通过多模态对齐和自适应引导机制，实现了高质量的语音生成，并在多种任务中达到前沿性能，为多模态语音合成提供了有效解决方案。

Abstract: In this paper, we address the task of multimodal-to-speech generation, which
aims to synthesize high-quality speech from multiple input modalities: text,
video, and reference audio. This task has gained increasing attention due to
its wide range of applications, such as film production, dubbing, and virtual
avatars. Despite recent progress, existing methods still suffer from
limitations in speech intelligibility, audio-video synchronization, speech
naturalness, and voice similarity to the reference speaker. To address these
challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer
that generates accurate, synchronized, and natural-sounding speech from aligned
multimodal inputs. Built upon the in-context learning capability of the DiT
architecture, AlignDiT explores three effective strategies to align multimodal
representations. Furthermore, we introduce a novel multimodal classifier-free
guidance mechanism that allows the model to adaptively balance information from
each modality during speech synthesis. Extensive experiments demonstrate that
AlignDiT significantly outperforms existing methods across multiple benchmarks
in terms of quality, synchronization, and speaker similarity. Moreover,
AlignDiT exhibits strong generalization capability across various multimodal
tasks, such as video-to-speech synthesis and visual forced alignment,
consistently achieving state-of-the-art performance. The demo page is available
at https://mm.kaist.ac.kr/projects/AlignDiT .

</details>


### [166] [SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data](https://arxiv.org/abs/2504.20648)
*Michael Ogezi,Freda Shi*

Main category: cs.CV

TL;DR: 论文提出了一个针对视觉语言模型（VLMs）在空间推理能力上的不足的解决方案，通过构建一个专注于空间推理的合成VQA数据集，并训练SpaRE模型，显著提升了空间推理任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在空间推理方面表现不佳，主要因为广泛使用的VL数据集中空间关系的数据稀少且分布不均，导致模型难以处理多样的空间关系。

Method: 研究者构建了一个包含455k样本和3.4百万QA对的合成VQA数据集，基于Localized Narratives、DOCCI和PixMo-Cap的超详细图像描述。

Result: 训练后的SpaRE模型在空间推理基准测试中表现显著提升，如在What's Up基准上性能提升了49%，同时保持了在通用任务上的良好表现。

Conclusion: 这项研究缩小了人类与VLM在空间推理上的差距，提升了VLM在现实任务（如机器人和导航）中的应用能力。

Abstract: Vision-language models (VLMs) work well in tasks ranging from image
captioning to visual question answering (VQA), yet they struggle with spatial
reasoning, a key skill for understanding our physical world that humans excel
at. We find that spatial relations are generally rare in widely used VL
datasets, with only a few being well represented, while most form a long tail
of underrepresented relations. This gap leaves VLMs ill-equipped to handle
diverse spatial relationships. To bridge it, we construct a synthetic VQA
dataset focused on spatial reasoning generated from hyper-detailed image
descriptions in Localized Narratives, DOCCI, and PixMo-Cap. Our dataset
consists of 455k samples containing 3.4 million QA pairs. Trained on this
dataset, our Spatial-Reasoning Enhanced (SpaRE) VLMs show strong improvements
on spatial reasoning benchmarks, achieving up to a 49% performance gain on the
What's Up benchmark, while maintaining strong results on general tasks. Our
work narrows the gap between human and VLM spatial reasoning and makes VLMs
more capable in real-world tasks such as robotics and navigation.

</details>


### [167] [A Transformer-based Multimodal Fusion Model for Efficient Crowd Counting Using Visual and Wireless Signals](https://arxiv.org/abs/2504.20178)
*Zhe Cui,Yuli Li,Le-Nam Tran*

Main category: cs.CV

TL;DR: TransFusion结合CSI和图像数据，通过Transformer和CNN的融合解决现有单模态人群计数模型的信息损失问题，实现了高精度和高效的人群计数。


<details>
  <summary>Details</summary>
Motivation: 现有基于单模态输入的人群计数模型因信息丢失导致性能不佳，因此需要结合多模态数据提升准确率。

Method: 提出TransFusion模型，利用Transformer捕捉全局上下文信息，并加入CNN提取局部细节特征，实现多模态（CSI和图像）融合。

Result: 实验表明，TransFusion在最小误差下实现高精度计数，且效率优越。

Conclusion: TransFusion通过多模态和架构创新显著提升了人群计数的性能。

Abstract: Current crowd-counting models often rely on single-modal inputs, such as
visual images or wireless signal data, which can result in significant
information loss and suboptimal recognition performance. To address these
shortcomings, we propose TransFusion, a novel multimodal fusion-based
crowd-counting model that integrates Channel State Information (CSI) with image
data. By leveraging the powerful capabilities of Transformer networks,
TransFusion effectively combines these two distinct data modalities, enabling
the capture of comprehensive global contextual information that is critical for
accurate crowd estimation. However, while transformers are well capable of
capturing global features, they potentially fail to identify finer-grained,
local details essential for precise crowd counting. To mitigate this, we
incorporate Convolutional Neural Networks (CNNs) into the model architecture,
enhancing its ability to extract detailed local features that complement the
global context provided by the Transformer. Extensive experimental evaluations
demonstrate that TransFusion achieves high accuracy with minimal counting
errors while maintaining superior efficiency.

</details>


### [168] [Advance Fake Video Detection via Vision Transformers](https://arxiv.org/abs/2504.20669)
*Joy Battocchio,Stefano Dell'Anna,Andrea Montibeller,Giulia Boato*

Main category: cs.CV

TL;DR: 论文提出了一种基于Vision Transformer的视频伪造检测框架，整合时间信息以提高准确性，在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成的多媒体技术日益成熟，其潜在的恶意使用（如传播虚假信息）引发了监管和社会担忧，亟需高精度、泛化能力强的检测方法。

Method: 扩展Vision Transformer (ViT) 在图像伪造检测中的应用，提出创新框架，通过整合时间维度的ViT嵌入提升视频伪造检测性能。

Result: 在包含开源及专有生成技术的多样化视频数据集上，该方法展示了优异的准确性、泛化能力和少样本学习表现。

Conclusion: 结合时间信息的ViT框架为AI生成视频检测提供了有效解决方案，未来需进一步应对快速演变的生成技术挑战。

Abstract: Recent advancements in AI-based multimedia generation have enabled the
creation of hyper-realistic images and videos, raising concerns about their
potential use in spreading misinformation. The widespread accessibility of
generative techniques, which allow for the production of fake multimedia from
prompts or existing media, along with their continuous refinement, underscores
the urgent need for highly accurate and generalizable AI-generated media
detection methods, underlined also by new regulations like the European Digital
AI Act. In this paper, we draw inspiration from Vision Transformer (ViT)-based
fake image detection and extend this idea to video. We propose an {original}
%innovative framework that effectively integrates ViT embeddings over time to
enhance detection performance. Our method shows promising accuracy,
generalization, and few-shot learning capabilities across a new, large and
diverse dataset of videos generated using five open source generative
techniques from the state-of-the-art, as well as a separate dataset containing
videos produced by proprietary generative methods.

</details>


### [169] [Fine Grain Classification: Connecting Meta using Cross-Contrastive pre-training](https://arxiv.org/abs/2504.20322)
*Sumit Mamtani,Yash Thesia*

Main category: cs.CV

TL;DR: 论文提出了一个结合元信息的细粒度视觉分类框架，通过跨对比预训练联合学习视觉和元信息，最终在NABirds数据集上显著提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的细粒度分类仅依赖外观信息难以准确区分子类别，因此需要引入元信息以提升分类性能。

Method: 采用三编码器（图像、文本、元信息）进行跨对比预训练，对齐嵌入表示，随后对图像和元信息编码器进行微调以完成分类任务。

Result: 在NABirds数据集上，框架结合元信息后比基线提升7.83%，准确率达到84.44%，优于现有方法。

Conclusion: 提出的框架证明了元信息在细粒度视觉分类中的有效性，显著提升了分类性能。

Abstract: Fine-grained visual classification aims to recognize objects belonging to
multiple subordinate categories within a super-category. However, this remains
a challenging problem, as appearance information alone is often insufficient to
accurately differentiate between fine-grained visual categories. To address
this, we propose a novel and unified framework that leverages meta-information
to assist fine-grained identification. We tackle the joint learning of visual
and meta-information through cross-contrastive pre-training. In the first
stage, we employ three encoders for images, text, and meta-information,
aligning their projected embeddings to achieve better representations. We then
fine-tune the image and meta-information encoders for the classification task.
Experiments on the NABirds dataset demonstrate that our framework effectively
utilizes meta-information to enhance fine-grained recognition performance. With
the addition of meta-information, our framework surpasses the current baseline
on NABirds by 7.83%. Furthermore, it achieves an accuracy of 84.44% on the
NABirds dataset, outperforming many existing state-of-the-art approaches that
utilize meta-information.

</details>


### [170] [GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion](https://arxiv.org/abs/2504.20829)
*Jiaxin Hong,Sixu Chen,Shuoyang Sun,Hongyao Yu,Hao Fang,Yuqi Tan,Bin Chen,Shuhan Qi,Jiawei Li*

Main category: cs.CV

TL;DR: 该论文首次系统研究了3D高斯泼溅（3DGS）中的后门威胁，提出了一种名为GuassTrap的新型投毒攻击方法，能够在不影响正常视图渲染质量的情况下植入隐蔽的后门视图，揭示3D渲染中的安全隐患。


<details>
  <summary>Details</summary>
Motivation: 随着3DGS在安全关键领域（如自动驾驶、AR/VR）的快速应用，其潜在的安全漏洞亟需被研究。论文旨在揭示3DGS流程中后门攻击的风险，以预防环境误判或空间扭曲等安全问题。

Method: 论文提出了GuassTrap方法，包括攻击、稳定和正常训练三个阶段，通过联合优化攻击效果和视觉真实性，在特定攻击视图中植入隐蔽的后门渲染。

Result: 实验表明，GuassTrap能有效嵌入难以察觉但有害的后门视图，同时在非目标视图中保持高质量的渲染，验证了其鲁棒性、适应性和实际应用性。

Conclusion: 该研究揭示了3DGS中的安全漏洞，为未来3D渲染技术的发展提供了重要的安全警示。

Abstract: As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene
representation and novel view synthesis, its rapid adoption in safety-critical
domains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of
potential security vulnerabilities. This paper presents the first systematic
study of backdoor threats in 3DGS pipelines. We identify that adversaries may
implant backdoor views to induce malicious scene confusion during inference,
potentially leading to environmental misperception in autonomous navigation or
spatial distortion in immersive environments. To uncover this risk, we propose
GuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap
injects malicious views at specific attack viewpoints while preserving
high-quality rendering in non-target views, ensuring minimal detectability and
maximizing potential harm. Specifically, the proposed method consists of a
three-stage pipeline (attack, stabilization, and normal training) to implant
stealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing
attack efficacy and perceptual realism to expose security risks in 3D
rendering. Extensive experiments on both synthetic and real-world datasets
demonstrate that GuassTrap can effectively embed imperceptible yet harmful
backdoor views while maintaining high-quality rendering in normal views,
validating its robustness, adaptability, and practical applicability.

</details>


### [171] [Autoencoder Models for Point Cloud Environmental Synthesis from WiFi Channel State Information: A Preliminary Study](https://arxiv.org/abs/2504.20541)
*Daniele Pannone,Danilo Avola*

Main category: cs.CV

TL;DR: 提出了一种通过WiFi信道状态信息数据生成点云的深度学习框架，采用两阶段自编码器方法，实验结果验证了其在无线传感和环境映射应用中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的无线传感技术难以高效生成高精度环境点云，需要一种结合WiFi数据和深度学习的方法来解决这一问题。

Method: 采用两阶段自编码器框架：第一阶段使用带卷积层的PointNet自编码器生成点云，第二阶段使用CNN自编码器将CSI数据映射到匹配的潜在空间。

Result: 实验结果表明，该方法能够从WiFi数据中准确重建环境点云。

Conclusion: 本方法为无线传感和环境映射提供了新的解决方案，展示了深度学习在跨模态数据生成中的潜力。

Abstract: This paper introduces a deep learning framework for generating point clouds
from WiFi Channel State Information data. We employ a two-stage autoencoder
approach: a PointNet autoencoder with convolutional layers for point cloud
generation, and a Convolutional Neural Network autoencoder to map CSI data to a
matching latent space. By aligning these latent spaces, our method enables
accurate environmental point cloud reconstruction from WiFi data. Experimental
results validate the effectiveness of our approach, highlighting its potential
for wireless sensing and environmental mapping applications.

</details>


### [172] [RadSAM: Segmenting 3D radiological images with a 2D promptable model](https://arxiv.org/abs/2504.20837)
*Julien Khlaut,Elodie Ferreres,Daniel Tordjman,Hélène Philippe,Tom Boeken,Pierre Manceron,Corentin Dancette*

Main category: cs.CV

TL;DR: 论文提出了RadSAM，一种基于2D模型从单次提示分割3D对象的新方法，解决了现有SAM模型在医疗影像中无法有效处理3D数据和缺乏编辑功能的问题。


<details>
  <summary>Details</summary>
Motivation: 医疗图像分割在临床中至关重要但耗时，现有SAM模型虽具有泛化能力但无法有效处理3D医疗数据且缺乏编辑功能，需要改进以适应医疗领域需求。

Method: 提出RadSAM，使用带噪声的2D掩码作为初始提示，结合边界框和点提示，通过迭代推理逐层重建3D掩码。

Result: 在AMOS腹部器官分割数据集上验证了RadSAM的有效性，显著优于现有方法，尤其在单次提示下的3D分割和编辑能力方面表现突出。

Conclusion: RadSAM通过创新的提示设计和迭代推理，成功将2D模型应用于3D医疗图像分割，弥补了现有技术的不足。

Abstract: Medical image segmentation is a crucial and time-consuming task in clinical
care, where mask precision is extremely important. The Segment Anything Model
(SAM) offers a promising approach, as it provides an interactive interface
based on visual prompting and edition to refine an initial segmentation. This
model has strong generalization capabilities, does not rely on predefined
classes, and adapts to diverse objects; however, it is pre-trained on natural
images and lacks the ability to process medical data effectively. In addition,
this model is built for 2D images, whereas a whole medical domain is based on
3D images, such as CT and MRI. Recent adaptations of SAM for medical imaging
are based on 2D models, thus requiring one prompt per slice to segment 3D
objects, making the segmentation process tedious. They also lack important
features such as editing. To bridge this gap, we propose RadSAM, a novel method
for segmenting 3D objects with a 2D model from a single prompt. In practice, we
train a 2D model using noisy masks as initial prompts, in addition to bounding
boxes and points. We then use this novel prompt type with an iterative
inference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a
benchmark to evaluate the model's ability to segment 3D objects in CT images
from a single prompt and evaluate the models' out-of-domain transfer and
edition capabilities. We demonstrate the effectiveness of our approach against
state-of-the-art models on this benchmark using the AMOS abdominal organ
segmentation dataset.

</details>


### [173] [Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for Visual Classifiers](https://arxiv.org/abs/2504.20902)
*Quentin Guimard,Moreno D'Incà,Massimiliano Mancini,Elisa Ricci*

Main category: cs.CV

TL;DR: 提出了一种无需标注数据的偏差检测框架C2B，通过任务文本描述生成偏差建议并评估模型偏差，实验表明其优于依赖标注的现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有偏差检测方法依赖标注数据，限制了其适用性；C2B旨在无需标注任务数据的前提下实现通用偏差检测。

Method: 利用大语言模型生成偏差建议及对应标注，结合检索模型收集图像并评估模型偏差，全程无需训练或标注。

Result: C2B在公开数据集上发现的偏差超出原数据集范围，且性能优于依赖标注的基线方法。

Conclusion: C2B为无监督任务无关的偏差检测提供了可行方案，扩展了偏差检测的适用场景。

Abstract: A person downloading a pre-trained model from the web should be aware of its
biases. Existing approaches for bias identification rely on datasets containing
labels for the task of interest, something that a non-expert may not have
access to, or may not have the necessary resources to collect: this greatly
limits the number of tasks where model biases can be identified. In this work,
we present Classifier-to-Bias (C2B), the first bias discovery framework that
works without access to any labeled data: it only relies on a textual
description of the classification task to identify biases in the target
classification model. This description is fed to a large language model to
generate bias proposals and corresponding captions depicting biases together
with task-specific target labels. A retrieval model collects images for those
captions, which are then used to assess the accuracy of the model w.r.t. the
given biases. C2B is training-free, does not require any annotations, has no
constraints on the list of biases, and can be applied to any pre-trained model
on any classification task. Experiments on two publicly available datasets show
that C2B discovers biases beyond those of the original datasets and outperforms
a recent state-of-the-art bias detection baseline that relies on task-specific
annotations, being a promising first step toward addressing task-agnostic
unsupervised bias detection.

</details>


### [174] [SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features](https://arxiv.org/abs/2504.20970)
*Mete Erdogan,Sebnem Demirtas*

Main category: cs.CV

TL;DR: 本文提出了一种基于奇异值分解的最小二乘法（SVD-LS）框架，用于多类别肺炎的X光图像分类，结合自监督和迁移学习模型，实现高效精确的诊断。


<details>
  <summary>Details</summary>
Motivation: 通过X光影像实现肺炎的早期准确诊断对治疗和患者预后至关重要。利用机器学习的自动化诊断工具可以辅助放射科医师提高决策效率和可靠性。

Method: 采用奇异值分解的最小二乘法（SVD-LS）框架，结合自监督和迁移学习模型提取特征，避免计算开销大的梯度微调，采用闭式非迭代分类方法。

Result: 实验显示SVD-LS在保持竞争力的准确率的同时显著降低计算成本，适合实时医学影像应用。

Conclusion: SVD-LS框架为实时医学影像诊断提供了一种高效且精确的替代方案。

Abstract: Accurate and early diagnosis of pneumonia through X-ray imaging is essential
for effective treatment and improved patient outcomes. Recent advancements in
machine learning have enabled automated diagnostic tools that assist
radiologists in making more reliable and efficient decisions. In this work, we
propose a Singular Value Decomposition-based Least Squares (SVD-LS) framework
for multi-class pneumonia classification, leveraging powerful feature
representations from state-of-the-art self-supervised and transfer learning
models. Rather than relying on computationally expensive gradient based
fine-tuning, we employ a closed-form, non-iterative classification approach
that ensures efficiency without compromising accuracy. Experimental results
demonstrate that SVD-LS achieves competitive performance while offering
significantly reduced computational costs, making it a viable alternative for
real-time medical imaging applications.

</details>


### [175] [YoChameleon: Personalized Vision and Language Generation](https://arxiv.org/abs/2504.20998)
*Thao Nguyen,Krishna Kumar Singh,Jing Shi,Trung Bui,Yong Jae Lee,Yuheng Li*

Main category: cs.CV

TL;DR: Yo'Chameleon is a personalized large multimodal模型的研究，首次尝试通过soft-prompt tuning在少量图片输入下生成个性化内容和回答相关问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型缺乏对用户特定概念的个性化知识，尤其在图像生成方面。本文旨在探索如何将个性化方法扩展到多模态领域。

Method: 利用3-5张特定概念的图片，通过soft-prompt tuning嵌入主题信息，并结合self-prompting优化机制和soft-positive图像生成方法。

Result: Yo'Chameleon能够回答关于主题的问题，并生成包含精准像素细节的新上下文图像。

Conclusion: 研究展示了在多模态模型中实现个性化的可行性，尤其在少量样本下表现优异。

Abstract: Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into
powerful tools with millions of users. However, they remain generic models and
lack personalized knowledge of specific user concepts. Previous work has
explored personalization for text generation, yet it remains unclear how these
methods can be adapted to new modalities, such as image generation. In this
paper, we introduce Yo'Chameleon, the first attempt to study personalization
for large multimodal models. Given 3-5 images of a particular concept,
Yo'Chameleon leverages soft-prompt tuning to embed subject-specific information
to (i) answer questions about the subject and (ii) recreate pixel-level details
to produce images of the subject in new contexts. Yo'Chameleon is trained with
(i) a self-prompting optimization mechanism to balance performance across
multiple modalities, and (ii) a ``soft-positive" image generation approach to
enhance image quality in a few-shot setting.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [176] [EPSILON: Adaptive Fault Mitigation in Approximate Deep Neural Network using Statistical Signatures](https://arxiv.org/abs/2504.20074)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.DC

TL;DR: EPSILON框架通过预计算统计特征和层重要性指标，为近似计算DNN提供高效且轻量级的故障检测与缓解方案，显著提升能效并保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 近似计算DNN虽能效高，但永久性故障会显著降低其性能，传统方法因高开销不适用。需要一种轻量级解决方案。

Method: 提出EPSILON框架，基于预计算统计特征、层重要性指标和非参数模式匹配算法，实现恒定时间故障检测与动态缓解。

Result: 在多个数据集和故障场景下，EPSILON保持80.05%准确率，推理时间减少22%，能效提升28%。

Conclusion: EPSILON是安全关键边缘应用中部署可靠近似计算DNN的实用解决方案，兼顾性能和能效。

Abstract: The increasing adoption of approximate computing in deep neural network
accelerators (AxDNNs) promises significant energy efficiency gains. However,
permanent faults in AxDNNs can severely degrade their performance compared to
their accurate counterparts (AccDNNs). Traditional fault detection and
mitigation approaches, while effective for AccDNNs, introduce substantial
overhead and latency, making them impractical for energy-constrained real-time
deployment. To address this, we introduce EPSILON, a lightweight framework that
leverages pre-computed statistical signatures and layer-wise importance metrics
for efficient fault detection and mitigation in AxDNNs. Our framework
introduces a novel non-parametric pattern-matching algorithm that enables
constant-time fault detection without interrupting normal execution while
dynamically adapting to different network architectures and fault patterns.
EPSILON maintains model accuracy by intelligently adjusting mitigation
strategies based on a statistical analysis of weight distribution and layer
criticality while preserving the energy benefits of approximate computing.
Extensive evaluations across various approximate multipliers, AxDNN
architectures, popular datasets (MNIST, CIFAR-10, CIFAR-100, ImageNet-1k), and
fault scenarios demonstrate that EPSILON maintains 80.05\% accuracy while
offering 22\% improvement in inference time and 28\% improvement in energy
efficiency, establishing EPSILON as a practical solution for deploying reliable
AxDNNs in safety-critical edge applications.

</details>


### [177] [GenTorrent: Scaling Large Language Model Serving with An Overley Network](https://arxiv.org/abs/2504.20101)
*Fei Fang,Yifan Hua,Shengze Wang,Ruilin Zhou,Yi Liu,Chen Qian,Xiaoxue Zhang*

Main category: cs.DC

TL;DR: 该论文提出GenTorrent，一种利用去中心化贡献者资源的LLM服务覆盖网络，解决了网络组织、隐私、资源效率和验证等四大问题，实现延迟降低50%以上。


<details>
  <summary>Details</summary>
Motivation: 解决小组织和个人在部署和测试LLM创新时面临的服务扩展性挑战。

Method: 利用P2P网络的去中心化节点构建LLM服务覆盖网络，并解决四大核心技术问题。

Result: 原型测试显示延迟降低50%，安全功能对性能影响极小。

Conclusion: 该研究为未来AI服务的民主化和规模化提供了新方向。

Abstract: While significant progress has been made in research and development on
open-source and cost-efficient large-language models (LLMs), serving
scalability remains a critical challenge, particularly for small organizations
and individuals seeking to deploy and test their LLM innovations. Inspired by
peer-to-peer networks that leverage decentralized overlay nodes to increase
throughput and availability, we propose GenTorrent, an LLM serving overlay that
harnesses computing resources from decentralized contributors. We identify four
key research problems inherent to enabling such a decentralized infrastructure:
1) overlay network organization; 2) LLM communication privacy; 3) overlay
forwarding for resource efficiency; and 4) verification of serving quality.
This work presents the first systematic study of these fundamental problems in
the context of decentralized LLM serving. Evaluation results from a prototype
implemented on a set of decentralized nodes demonstrate that GenTorrent
achieves a latency reduction of over 50% compared to the baseline design
without overlay forwarding. Furthermore, the security features introduce
minimal overhead to serving latency and throughput. We believe this work
pioneers a new direction for democratizing and scaling future AI serving
capabilities.

</details>


### [178] [Electricity Cost Minimization for Multi-Workflow Allocation in Geo-Distributed Data Centers](https://arxiv.org/abs/2504.20105)
*Shuang Wang,He Zhang,Tianxing Wu,Yueyou Zhang,Wei Emma Zhang,Quan Z. Sheng*

Main category: cs.DC

TL;DR: 该论文提出了一个地理分布式系统架构和电力成本感知的多工作流调度算法（ECMWS），用于在满足截止时间约束的同时降低地理分布式数据中心（GDCs）的电力成本。


<details>
  <summary>Details</summary>
Motivation: 地理分布式数据中心（GDCs）为大规模工作流应用提供计算和存储服务，但高电力成本因其地理和时间差异而异。如何在满足工作流应用的截止时间约束的同时降低电力成本是一个重要问题。

Method: 提出了ECMWS算法，包括四个阶段：工作流排序、截止时间分配、任务排序和资源分配，并结合两个图嵌入模型和一个策略网络来解决马尔可夫决策过程（MDP）。

Result: 实验结果表明，ECMWS算法显著优于现有方法，性能提升超过15%，同时保持可接受的计算时间。

Conclusion: ECMWS算法能有效降低电力成本，同时在计算资源异构的地理分布式数据中心中满足工作流应用的截止时间约束。

Abstract: Worldwide, Geo-distributed Data Centers (GDCs) provide computing and storage
services for massive workflow applications, resulting in high electricity costs
that vary depending on geographical locations and time. How to reduce
electricity costs while satisfying the deadline constraints of workflow
applications is important in GDCs, which is determined by the execution time of
servers, power, and electricity price. Determining the completion time of
workflows with different server frequencies can be challenging, especially in
scenarios with heterogeneous computing resources in GDCs. Moreover, the
electricity price is also different in geographical locations and may change
dynamically. To address these challenges, we develop a geo-distributed system
architecture and propose an Electricity Cost aware Multiple Workflows
Scheduling algorithm (ECMWS) for servers of GDCs with fixed frequency and
power. ECMWS comprises four stages, namely workflow sequencing, deadline
partitioning, task sequencing, and resource allocation where two graph
embedding models and a policy network are constructed to solve the Markov
Decision Process (MDP). After statistically calibrating parameters and
algorithm components over a comprehensive set of workflow instances, the
proposed algorithms are compared with the state-of-the-art methods over two
types of workflow instances. The experimental results demonstrate that our
proposed algorithm significantly outperforms other algorithms, achieving an
improvement of over 15\% while maintaining an acceptable computational time.
The source codes are available at
https://gitee.com/public-artifacts/ecmws-experiments.

</details>


### [179] [Tempo: Application-aware LLM Serving with Mixed SLO Requirements](https://arxiv.org/abs/2504.20068)
*Wei Zhang,Zhiyu Wu,Yi Mu,Banruo Liu,Myungjin Lee,Fan Lai*

Main category: cs.DC

TL;DR: 该论文提出了Tempo，一个针对大语言模型(LLM)多样化工作负载的SLO感知调度器，旨在最大化服务收益。通过动态资源分配和混合调度策略，Tempo在延迟敏感和吞吐密集型请求间取得平衡，显著提升服务性能。


<details>
  <summary>Details</summary>
Motivation: 由于LLM应用场景多样且请求特征不确定（如响应长度、运行时依赖），传统调度器难以满足多样化的SLO需求，导致资源利用率低或SLO违规。

Method: Tempo采用混合调度策略：1) 使用分位数响应上限和依赖图匹配进行初始估算；2) 按服务收益密度优先处理请求；3) 在线动态调整资源分配。

Result: 实验表明，Tempo在聊天、推理和智能代理等场景中，端到端服务收益提升最高达8.3倍，SLO吞吐量提升达10.3倍。

Conclusion: Tempo通过SLO感知的资源分配和动态调度，有效解决了LLM工作负载多样性带来的挑战，为未来异构AI服务调度提供了新思路。

Abstract: The integration of Large Language Models (LLMs) into diverse applications,
ranging from interactive chatbots and cloud AIOps to intelligent agents, has
introduced a wide spectrum of Service Level Objectives (SLOs) for
responsiveness. These workloads include latency-sensitive requests focused on
per-token latency in streaming chat, throughput-intensive requests that require
rapid full responses to invoke tools, and collective requests with dynamic
dependencies arising from self-reflection or agent-based reasoning. This
workload diversity, amplified by unpredictable request information such as
response lengths and runtime dependencies, makes existing schedulers inadequate
even within their design envelopes.
  In this paper, we define service gain as the useful service delivered by
completing requests. We observe that as SLO directly reflects the actual
performance needs of requests, completing a request much faster than its SLO
(e.g., deadline) yields limited additional service gain. Based on this insight,
we introduce Tempo, the first systematic SLO-aware scheduler designed to
maximize service gain across diverse LLM workloads. Tempo allocates just enough
serving bandwidth to meet each SLO, maximizing residual capacity for others
best-effort workloads. Instead of assuming request information or none at all,
it adopts a hybrid scheduling strategy: using quantile-based response upper
bounds and dependency-graph matching for conservative initial estimates,
prioritizing requests by service gain density, and refining decisions online as
generation progresses. Our evaluation across diverse workloads, including chat,
reasoning, and agentic pipelines, shows that Tempo improves end-to-end service
gain by up to 8.3$\times$ and achieves up to 10.3$\times$ SLO goodput compared
to state-of-the-art designs

</details>


### [180] [Leveraging Neural Graph Compilers in Machine Learning Research for Edge-Cloud Systems](https://arxiv.org/abs/2504.20198)
*Alireza Furutanpey,Carmen Walser,Philipp Raith,Pantelis A. Frangoudis,Schahram Dustdar*

Main category: cs.DC

TL;DR: 该论文对神经网络的图编译器在不同硬件平台上进行了全面评估，揭示了供应商特定优化如何影响性能比较，并提出了量化编译器性能的新指标，为实践者提供了实用见解。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络优化技术在理论研究和实际部署之间存在巨大差距，尤其是供应商特定优化可能导致性能比较失真，因此需要系统性分析编译器对不同架构和批处理大小的影响。

Method: 通过对不同硬件平台上的图编译器进行细粒度块级实验，分析其性能模式，并提出新的量化指标来衡量编译器在批处理增加时的表现。

Result: 研究发现，供应商特定的编译器可以通过利用简单架构中的重复模式，在模型深度增加时获得不成比例的高吞吐量。不同编译器在不同架构和批处理量下的性能表现差异显著。

Conclusion: 该研究弥合了学术研究与实际部署之间的差距，通过在整个研究过程中纳入编译器效应，为异构硬件环境中的优化提供了可行的指导。

Abstract: This work presents a comprehensive evaluation of neural network graph
compilers across heterogeneous hardware platforms, addressing the critical gap
between theoretical optimization techniques and practical deployment scenarios.
We demonstrate how vendor-specific optimizations can invalidate relative
performance comparisons between architectural archetypes, with performance
advantages sometimes completely reversing after compilation. Our systematic
analysis reveals that graph compilers exhibit performance patterns highly
dependent on both neural architecture and batch sizes. Through fine-grained
block-level experimentation, we establish that vendor-specific compilers can
leverage repeated patterns in simple architectures, yielding disproportionate
throughput gains as model depth increases. We introduce novel metrics to
quantify a compiler's ability to mitigate performance friction as batch size
increases. Our methodology bridges the gap between academic research and
practical deployment by incorporating compiler effects throughout the research
process, providing actionable insights for practitioners navigating complex
optimization landscapes across heterogeneous hardware environments.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [181] [TrueFake: A Real World Case Dataset of Last Generation Fake Images also Shared on Social Networks](https://arxiv.org/abs/2504.20658)
*Stefano Dell'Anna,Andrea Montibeller,Giulia Boato*

Main category: cs.MM

TL;DR: 论文提出了TrueFake数据集，包含60万张图像，用于在真实社交媒体环境下评估虚假图像检测工具的性能，并分析了社交媒体分享对检测效果的影响。


<details>
  <summary>Details</summary>
Motivation: 因AI生成的合成内容常被用于传播虚假信息，现有的检测工具未能充分应对社交媒体压缩等现实挑战，研究旨在填补这一空白。

Method: 构建TrueFake数据集，包含多种生成技术生成的图像和三种社交媒体分享路径，并在真实场景下评估现有检测模型的性能。

Result: 研究揭示了社交媒体分享对检测性能的负面影响，并提出了当前最有效的检测和训练策略。

Conclusion: 强调需在真实使用条件下评估检测模型，以提升实际应用中的效果。

Abstract: AI-generated synthetic media are increasingly used in real-world scenarios,
often with the purpose of spreading misinformation and propaganda through
social media platforms, where compression and other processing can degrade fake
detection cues. Currently, many forensic tools fail to account for these
in-the-wild challenges. In this work, we introduce TrueFake, a large-scale
benchmarking dataset of 600,000 images including top notch generative
techniques and sharing via three different social networks. This dataset allows
for rigorous evaluation of state-of-the-art fake image detectors under very
realistic and challenging conditions. Through extensive experimentation, we
analyze how social media sharing impacts detection performance, and identify
current most effective detection and training strategies. Our findings
highlight the need for evaluating forensic models in conditions that mirror
real-world use.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [182] [Fostering Self-Directed Growth with Generative AI: Toward a New Learning Analytics Framework](https://arxiv.org/abs/2504.20851)
*Qianrun Mao*

Main category: cs.CY

TL;DR: 摘要提出了一种结合生成式人工智能和学习分析的新框架（A2PL模型），旨在培养自我导向成长能力，以弥补当前自主学习与AI教育研究的不足，并探讨了其在未来教育系统中的方法论意义。


<details>
  <summary>Details</summary>
Motivation: 在分散化知识生态和AI普及的背景下，培养可持续的自主学习能力成为教育的关键需求。当前关于自主学习和AI教育的研究存在不足，需要新的框架来整合学习者愿望、复杂思维和总结性自我评估。

Method: 提出了A2PL模型，结合生成式人工智能（GAI）和学习分析，重新定义学习者愿望、复杂思维及自我评估的互动关系，并探讨了未来干预设计和学习分析应用的途径。

Result: 该框架为开发公平、适应性强且可持续的数字时代学习系统提供了理论支持，强调自我导向成长作为核心能力的重要性。

Conclusion: A2PL模型为AI支持的教育环境中的自主学习提供了新思路，有望推动未来教育系统的创新与可持续发展。

Abstract: In an era increasingly shaped by decentralized knowledge ecosystems and
pervasive AI technologies, fostering sustainable learner agency has become a
critical educational imperative. This study introduces a novel conceptual
framework integrating Generative Artificial Intelligence and Learning Analytics
to cultivate Self-Directed Growth, a dynamic competency that enables learners
to iteratively drive their own developmental pathways across diverse
contexts.Building upon critical gaps in current research on Self Directed
Learning and AI-mediated education, the proposed Aspire to Potentials for
Learners (A2PL) model reconceptualizes the interplay of learner aspirations,
complex thinking, and summative self-assessment within GAI supported
environments.Methodological implications for future intervention design and
learning analytics applications are discussed, positioning Self-Directed Growth
as a pivotal axis for developing equitable, adaptive, and sustainable learning
systems in the digital era.

</details>


### [183] [When Testing AI Tests Us: Safeguarding Mental Health on the Digital Frontlines](https://arxiv.org/abs/2504.20910)
*Sachin R. Pendse,Darren Gergle,Rachel Kornfield,Jonah Meyerhoff,David Mohr,Jina Suh,Annie Wescott,Casey Williams,Jessica Schleider*

Main category: cs.CY

TL;DR: 本文探讨了AI红队成员因对抗性测试工作而产生的心理健康问题，提出结合其他职业的经验制定保护策略。


<details>
  <summary>Details</summary>
Motivation: AI红队成员在测试生成式AI系统的有害输出时，因其对抗性工作性质面临独特的心理健康风险，这一问题尚未得到充分关注。

Method: 通过分析红队工作的心理健康影响，借鉴演员、心理健康专家等职业的保护措施，提出针对性的个体和组织策略。

Result: 提出了适用于红队工作的心理健康保护策略，强调组织支持和职业实践调整的必要性。

Conclusion: 保护红队成员的心理健康是AI安全工作的重要组成部分，需结合跨职业经验制定有效干预措施。

Abstract: Red-teaming is a core part of the infrastructure that ensures that AI models
do not produce harmful content. Unlike past technologies, the black box nature
of generative AI systems necessitates a uniquely interactional mode of testing,
one in which individuals on red teams actively interact with the system,
leveraging natural language to simulate malicious actors and solicit harmful
outputs. This interactional labor done by red teams can result in mental health
harms that are uniquely tied to the adversarial engagement strategies necessary
to effectively red team. The importance of ensuring that generative AI models
do not propagate societal or individual harm is widely recognized -- one less
visible foundation of end-to-end AI safety is also the protection of the mental
health and wellbeing of those who work to keep model outputs safe. In this
paper, we argue that the unmet mental health needs of AI red-teamers is a
critical workplace safety concern. Through analyzing the unique mental health
impacts associated with the labor done by red teams, we propose potential
individual and organizational strategies that could be used to meet these
needs, and safeguard the mental health of red-teamers. We develop our proposed
strategies through drawing parallels between common red-teaming practices and
interactional labor common to other professions (including actors, mental
health professionals, conflict photographers, and content moderators),
describing how individuals and organizations within these professional spaces
safeguard their mental health given similar psychological demands. Drawing on
these protective practices, we describe how safeguards could be adapted for the
distinct mental health challenges experienced by red teaming organizations as
they mitigate emerging technological risks on the new digital frontlines.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [184] [Energy-Based Coarse-Graining in Molecular Dynamics: A Flow-Based Framework Without Data](https://arxiv.org/abs/2504.20940)
*Maximilian Stupp,P. S. Koutsourelakis*

Main category: physics.chem-ph

TL;DR: 本文提出了一种无需数据的粗粒化生成框架，直接针对全原子玻尔兹曼分布，无需依赖传统分子动力学轨迹，通过能量目标和潜在空间建模实现高效生成。


<details>
  <summary>Details</summary>
Motivation: 传统粗粒化方法依赖大量分子动力学轨迹数据，限制了模型的准确性和泛化能力。本文旨在开发一种无需数据驱动的粗粒化生成方法，直接建模全原子分布。

Method: 利用结构化潜在空间建模慢变量（多模态）和快变量（单模态），通过可学习的双射映射重构分子结构，采用基于能量的目标和温度调节策略训练模型。

Result: 在双阱势、高斯混合和丙氨酸二肽等系统中，模型成功捕获玻尔兹曼分布的所有模态，准确重构原子构型，并学习有物理意义的粗粒化表示。

Conclusion: 无需模拟数据即可生成无偏的全原子平衡样本，证明了数据无关粗粒化方法的可行性和有效性。

Abstract: Coarse-grained (CG) models offer an effective route to reducing the
complexity of molecular simulations, yet conventional approaches depend heavily
on long all-atom molecular dynamics (MD) trajectories to adequately sample
configurational space. This data-driven dependence limits their accuracy and
generalizability, as unvisited configurations remain excluded from the
resulting CG model. We introduce a data-free generative framework for
coarse-graining that directly targets the all-atom Boltzmann distribution. Our
model defines a structured latent space comprising slow collective variables,
which are statistically associated with multimodal marginal densities capturing
metastable states, and fast variables, which represent the remaining degrees of
freedom with simple, unimodal conditional distributions. A potentially
learnable, bijective map from the full latent space to the all-atom
configuration space enables automatic and accurate reconstruction of molecular
structures. The model is trained using an energy-based objective that minimizes
the reverse Kullback-Leibler divergence, relying solely on the interatomic
potential rather than sampled trajectories. A tempering scheme is used to
stabilize training and promote exploration of diverse configurations. Once
trained, the model can generate unbiased, one-shot equilibrium all-atom
samples. We validate the method on two synthetic systems-a double-well
potential and a Gaussian mixture-as well as on the benchmark alanine dipeptide.
The model captures all relevant modes of the Boltzmann distribution, accurately
reconstructs atomic configurations, and learns physically meaningful
coarse-grained representations, all without any simulation data.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [185] [A Physically Driven Long Short Term Memory Model for Estimating Snow Water Equivalent over the Continental United States](https://arxiv.org/abs/2504.20129)
*Arun M. Saranathan,Mahmoud Saeedimoghaddam,Brandon Smith,Deepthi Raghunandan,Grey Nearing,Craig Pelissier*

Main category: physics.ao-ph

TL;DR: 该论文提出了一种基于LSTM网络的模型，用于估计雪水当量（SWE），通过分类和回归任务分别预测雪的存无及其高度，解决了传统方法计算成本高和空间覆盖不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统雪水当量估算方法（如再分析产品和局地测量）存在计算成本高或空间覆盖不足的问题，需要一种更高效且通用的方法。

Method: 采用LSTM网络，结合时间序列的气象/物理因子和静态空间/形态因子，将SWE估算分解为分类（存无）和回归（高度）两个任务。模型使用SNOTEL的实测数据训练。

Result: 分类任务准确率达93%以上，SWE估算的相关系数约0.9，且模型在时空上对未见数据具有泛化能力。

Conclusion: LSTM模型能高效且准确地估算SWE，解决了传统方法的局限性，适用于大范围雪情分析。

Abstract: Snow is an essential input for various land surface models. Seasonal snow
estimates are available as snow water equivalent (SWE) from process-based
reanalysis products or locally from in situ measurements. While the reanalysis
products are computationally expensive and available at only fixed spatial and
temporal resolutions, the in situ measurements are highly localized and sparse.
To address these issues and enable the analysis of the effect of a large suite
of physical, morphological, and geological conditions on the presence and
amount of snow, we build a Long Short-Term Memory (LSTM) network, which is able
to estimate the SWE based on time series input of the various
physical/meteorological factors as well static spatial/morphological factors.
Specifically, this model breaks down the SWE estimation into two separate
tasks: (i) a classification task that indicates the presence/absence of snow on
a specific day and (ii) a regression task that indicates the height of the SWE
on a specific day in the case of snow presence. The model is trained using
physical/in situ SWE measurements from the SNOw TELemetry (SNOTEL) snow pillows
in the western United States. We will show that trained LSTM models have a
classification accuracy of $\geq 93\%$ for the presence of snow and a
coefficient of correlation of $\sim 0.9$ concerning their SWE estimates. We
will also demonstrate that the models can generalize both spatially and
temporally to previously unseen data.

</details>


### [186] [Testing the Limit of Atmospheric Predictability with a Machine Learning Weather Model](https://arxiv.org/abs/2504.20238)
*P. Trent Vonich,Gregory J. Hakim*

Main category: physics.ao-ph

TL;DR: 机器学习的GraphCast天气模型通过优化初始条件，将确定性天气预报的技能期延长至30天，挑战了传统14天的预测极限。


<details>
  <summary>Details</summary>
Motivation: 传统认为确定性天气预报的极限是14天，研究旨在通过机器学习模型GraphCast突破这一限制，验证更长期的预测可能性。

Method: 利用梯度优化技术调整初始条件，结合GraphCast模型，对2020年每天两次的预报进行优化。

Result: 10天预报误差降低86%，技能期延长至30天；优化后的初始条件修正了ERA5数据，增强了哈德莱环流。其他模型（如Pangu-Weather）使用优化初始条件后误差减少21%。

Conclusion: 精准的初始条件可使确定性预报技能远超两周，颠覆了对大气可预测性极限的传统认知。

Abstract: Atmospheric predictability research has long held that the limit of skillful
deterministic weather forecasts is about 14 days. We challenge this limit using
GraphCast, a machine-learning weather model, by optimizing forecast initial
conditions using gradient-based techniques for twice-daily forecasts spanning
2020. This approach yields an average error reduction of 86% at 10 days, with
skill lasting beyond 30 days. Mean optimal initial-condition perturbations
reveal large-scale, spatially coherent corrections to ERA5, primarily
reflecting an intensification of the Hadley circulation. Forecasts using
GraphCast-optimal initial conditions in the Pangu-Weather model achieve a 21%
error reduction, peaking at 4 days, indicating that analysis corrections
reflect a combination of both model bias and a reduction in analysis error.
These results demonstrate that, given accurate initial conditions, skillful
deterministic forecasts are consistently achievable far beyond two weeks,
challenging long-standing assumptions about the limits of atmospheric
predictability.

</details>


### [187] [Data Driven Deep Learning for Correcting Global Climate Model Projections of SST and DSL in the Bay of Bengal](https://arxiv.org/abs/2504.20620)
*Abhishek Pasula,Deepak N. Subramani*

Main category: physics.ao-ph

TL;DR: 利用深度学习模型校正CMIP6气候模型在孟加拉湾的温度和海平面偏差，相比传统统计方法，RMSE显著降低。


<details>
  <summary>Details</summary>
Motivation: 孟加拉湾的气候变化对印度经济至关重要，但CMIP6模型与再分析数据存在显著差异，需改进偏差校正方法。

Method: 通过深度学习模型，利用历史数据训练，输入气候模型月度数据，输出ORAS5数据，完成偏差校正。

Result: 校正后SST和DSL的RMSE分别降低0.15°C和0.3米，优于传统EDCDF方法。

Conclusion: 深度学习模型可有效校正气候模型偏差，为未来气候预测提供更可靠数据。

Abstract: Climate change alters ocean conditions, notably temperature and sea level. In
the Bay of Bengal, these changes influence monsoon precipitation and marine
productivity, critical to the Indian economy. In Phase 6 of the Coupled Model
Intercomparison Project (CMIP6), Global Climate Models (GCMs) use different
shared socioeconomic pathways (SSPs) to obtain future climate projections.
However, significant discrepancies are observed between these models and the
reanalysis data in the Bay of Bengal for 2015-2024. Specifically, the root mean
square error (RMSE) between the climate model output and the Ocean Reanalysis
System (ORAS5) is 1.2C for the sea surface temperature (SST) and 1.1 m for the
dynamic sea level (DSL). We introduce a new data-driven deep learning model to
correct for this bias. The deep neural model for each variable is trained using
pairs of climatology-removed monthly climate projections as input and the
corresponding month's ORAS5 as output. This model is trained with historical
data (1950 to 2014), validated with future projection data from 2015 to 2020,
and tested with future projections from 2021 to 2023. Compared to the
conventional EquiDistant Cumulative Distribution Function (EDCDF) statistical
method for bias correction in climate models, our approach decreases RMSE by
0.15C for SST and 0.3 m for DSL. The trained model subsequently corrects the
projections for 2024-2100. A detailed analysis of the monthly, seasonal, and
decadal means and variability is performed to underscore the implications of
the novel dynamics uncovered in our corrected projections.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [188] [HCT-QA: A Benchmark for Question Answering on Human-Centric Tables](https://arxiv.org/abs/2504.20047)
*Mohammad S. Ahmad,Zan A. Naeem,Michaël Aupetit,Ahmed Elmagarmid,Mohamed Eltabakh,Xiasong Ma,Mourad Ouzzani,Chaoyi Ruan*

Main category: cs.IR

TL;DR: 该论文提出HCT-QA基准测试，用于评估大语言模型处理复杂人类中心表格（HCTs）的能力，包含真实和合成表格的问答对。


<details>
  <summary>Details</summary>
Motivation: 人类中心表格（HCTs）在各行业广泛存在但处理复杂，现有方法难以应对其多样布局，亟需新的查询方法。

Method: 构建包含真实和合成表格的大规模基准数据集HCT-QA，并利用大语言模型作为处理引擎进行测试。

Result: HCT-QA数据集包含2,188张真实表格和4,679张合成表格，总计9,835和67.5K个问答对。评估显示大语言模型具备处理潜力。

Conclusion: 大语言模型在处理复杂HCTs时展现出潜力，但需进一步优化以应对实际应用中的挑战。

Abstract: Tabular data embedded within PDF files, web pages, and other document formats
are prevalent across numerous sectors such as government, engineering, science,
and business. These human-centric tables (HCTs) possess a unique combination of
high business value, intricate layouts, limited operational power at scale, and
sometimes serve as the only data source for critical insights. However, their
complexity poses significant challenges to traditional data extraction,
processing, and querying methods. While current solutions focus on transforming
these tables into relational formats for SQL queries, they fall short in
handling the diverse and complex layouts of HCTs and hence being amenable to
querying. This paper describes HCT-QA, an extensive benchmark of HCTs, natural
language queries, and related answers on thousands of tables. Our dataset
includes 2,188 real-world HCTs with 9,835 QA pairs and 4,679 synthetic tables
with 67.5K QA pairs. While HCTs can be potentially processed by different type
of query engines, in this paper, we focus on Large Language Models as potential
engines and assess their ability in processing and querying such tables.

</details>


### [189] [Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence](https://arxiv.org/abs/2504.20059)
*Joey Chan,Qiao Jin,Nicholas Wan,Charalampos S. Floudas,Elisabetta Xue,Zhiyong Lu*

Main category: cs.IR

TL;DR: TrialGPT，一个基于大型语言模型的框架，通过在线患者病例匹配临床试验，比传统关键词搜索方法表现优46%，平均每位患者匹配7个试验。患者和试验组织者的反馈积极。


<details>
  <summary>Details</summary>
Motivation: 临床试验招募面临挑战（如认知不足、复杂资格标准、转诊障碍），而患者越来越多地在社交媒体和健康社区寻求支持。TrialGPT旨在利用这些在线资源改进招募效率。

Method: 利用TrialGPT框架（以LLM为核心），匹配50例在线患者病例（来自病例报告和社交媒体）与临床试验，并与传统关键词搜索方法对比。

Result: TrialGPT比传统方法多识别46%的合格试验，平均每位患者匹配7个试验。患者和试验组织者的反馈均非常积极。

Conclusion: TrialGPT是优化临床试验招募的有效工具，尤其适用于在线患者资源丰富的场景。

Abstract: Clinical trials are crucial for assessing new treatments; however,
recruitment challenges - such as limited awareness, complex eligibility
criteria, and referral barriers - hinder their success. With the growth of
online platforms, patients increasingly turn to social media and health
communities for support, research, and advocacy, expanding recruitment pools
and established enrollment pathways. Recognizing this potential, we utilized
TrialGPT, a framework that leverages a large language model (LLM) as its
backbone, to match 50 online patient cases (collected from published case
reports and a social media website) to clinical trials and evaluate performance
against traditional keyword-based searches. Our results show that TrialGPT
outperforms traditional methods by 46% in identifying eligible trials, with
each patient, on average, being eligible for around 7 trials. Additionally, our
outreach efforts to case authors and trial organizers regarding these
patient-trial matches yielded highly positive feedback, which we present from
both perspectives.

</details>


### [190] [A model and package for German ColBERT](https://arxiv.org/abs/2504.20083)
*Thuong Dang,Qiqi Chen*

Main category: cs.IR

TL;DR: 摘要介绍了ColBERT的德语版本及支持检索和微调的软件包。


<details>
  <summary>Details</summary>
Motivation: 为RAG应用提供德语版本的ColBERT检索方法，并扩展其工具支持。

Method: 通过多稠密向量检索方法（ColBERT）实现检索和微调流程。

Result: 发布了支持德语ColBERT模型的软件包。

Conclusion: 该工作为德语RAG应用提供了有效的检索工具和扩展支持。

Abstract: In this work, we introduce a German version for ColBERT, a late interaction
multi-dense vector retrieval method, with a focus on RAG applications. We also
present the main features of our package for ColBERT models, supporting both
retrieval and fine-tuning workflows.

</details>


### [191] [MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?](https://arxiv.org/abs/2504.20094)
*Zheng Hui,Xiaokai Wei,Yexi Jiang,Kevin Gao,Chen Wang,Frank Ong,Se-eun Yoon,Rachit Pareek,Michelle Gong*

Main category: cs.IR

TL;DR: MATCHA是一个基于多智能体协作的对话推荐系统框架，利用LLM提升个性化和用户参与度，通过多个专用智能体优化推荐准确性、多样性和安全性，在多项指标上优于或持平现有最佳模型。


<details>
  <summary>Details</summary>
Motivation: 解决对话推荐系统中的关键挑战，例如处理复杂的用户特定请求、通过多智能体协作增强个性化、实现实证评估与部署、确保安全可信的交互。

Method: 提出MATCHA框架，包含意图分析、候选生成、排序、重排序、解释性和保护机制的专用智能体，利用LLM实现协作优化。

Result: 在八个指标上表现优于或与当前最佳模型相当，通过对比六个基线模型验证了有效性。

Conclusion: MATCHA框架通过多智能体协作显著提升了对话推荐系统的性能，同时确保了安全性和可信度。

Abstract: In this paper, we propose a multi-agent collaboration framework called MATCHA
for conversational recommendation system, leveraging large language models
(LLMs) to enhance personalization and user engagement. Users can request
recommendations via free-form text and receive curated lists aligned with their
interests, preferences, and constraints. Our system introduces specialized
agents for intent analysis, candidate generation, ranking, re-ranking,
explainability, and safeguards. These agents collaboratively improve
recommendations accuracy, diversity, and safety. On eight metrics, our model
achieves superior or comparable performance to the current state-of-the-art.
Through comparisons with six baseline models, our approach addresses key
challenges in conversational recommendation systems for game recommendations,
including: (1) handling complex, user-specific requests, (2) enhancing
personalization through multi-agent collaboration, (3) empirical evaluation and
deployment, and (4) ensuring safe and trustworthy interactions.

</details>


### [192] [An Integrated Framework for Contextual Personalized LLM-Based Food Recommendation](https://arxiv.org/abs/2504.20092)
*Ali Rostami*

Main category: cs.IR

TL;DR: 论文提出了一种针对个性化食物推荐系统（Food-RecSys）的改进框架F-RLP，通过多媒体食物日志平台和地理定位分析工具，优化了语言处理模型在食物推荐领域的应用。


<details>
  <summary>Details</summary>
Motivation: 当前的食物推荐系统因组件理解不足和传统机器学习处理大规模不平衡食物数据效果不佳而表现欠佳。

Method: 引入多媒体食物日志平台和世界食物地图集，专门设计了针对食物领域的语言处理推荐框架F-RLP。

Result: F-RLP框架通过专门优化的大语言模型提供更有效、更具上下文意识的个性化食物推荐。

Conclusion: F-RLP为食物推荐领域提供了更高效的解决方案，弥补了现有通用模型的不足。

Abstract: Personalized food recommendation systems (Food-RecSys) critically
underperform due to fragmented component understanding and the failure of
conventional machine learning with vast, imbalanced food data. While Large
Language Models (LLMs) offer promise, current generic Recommendation as
Language Processing (RLP) strategies lack the necessary specialization for the
food domain's complexity. This thesis tackles these deficiencies by first
identifying and analyzing the essential components for effective Food-RecSys.
We introduce two key innovations: a multimedia food logging platform for rich
contextual data acquisition and the World Food Atlas, enabling unique
geolocation-based food analysis previously unavailable. Building on this
foundation, we pioneer the Food Recommendation as Language Processing (F-RLP)
framework - a novel, integrated approach specifically architected for the food
domain. F-RLP leverages LLMs in a tailored manner, overcoming the limitations
of generic models and providing a robust infrastructure for effective,
contextual, and truly personalized food recommendations.

</details>


### [193] [Search-Based Interaction For Conversation Recommendation via Generative Reward Model Based Simulated User](https://arxiv.org/abs/2504.20458)
*Xiaolei Wang,Chunxuan Xia,Junyi Li,Fanzhe Meng,Lei Huang,Jinpeng Wang,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.IR

TL;DR: 论文提出了一种基于生成奖励模型的模拟用户（GRSU），用于与对话推荐系统（CRS）自动交互，以更好地捕捉用户复杂偏好。通过生成式评分和属性级评论两种反馈方式，结合指令调优与波束搜索，实现了有效且高效的交互推荐，实验验证了方法的优势。


<details>
  <summary>Details</summary>
Motivation: 解决对话推荐系统中用户偏好复杂且难以准确捕捉的问题，同时避免频繁用户交互导致的体验下降。

Method: 提出GRSU模型，结合生成式评分和属性级评论两种反馈方式，通过指令调优统一反馈行为，并利用波束搜索和候选排序优化交互效率。

Result: 在公开数据集上的实验表明，该方法在效果、效率和可迁移性上均表现优异。

Conclusion: GRSU通过自动交互有效提升了对话推荐的准确性和用户体验，为复杂偏好的建模提供了新思路。

Abstract: Conversational recommendation systems (CRSs) use multi-turn interaction to
capture user preferences and provide personalized recommendations. A
fundamental challenge in CRSs lies in effectively understanding user
preferences from conversations. User preferences can be multifaceted and
complex, posing significant challenges for accurate recommendations even with
access to abundant external knowledge. While interaction with users can clarify
their true preferences, frequent user involvement can lead to a degraded user
experience.
  To address this problem, we propose a generative reward model based simulated
user, named GRSU, for automatic interaction with CRSs. The simulated user
provides feedback to the items recommended by CRSs, enabling them to better
capture intricate user preferences through multi-turn interaction. Inspired by
generative reward models, we design two types of feedback actions for the
simulated user: i.e., generative item scoring, which offers coarse-grained
feedback, and attribute-based item critique, which provides fine-grained
feedback. To ensure seamless integration, these feedback actions are unified
into an instruction-based format, allowing the development of a unified
simulated user via instruction tuning on synthesized data. With this simulated
user, automatic multi-turn interaction with CRSs can be effectively conducted.
Furthermore, to strike a balance between effectiveness and efficiency, we draw
inspiration from the paradigm of reward-guided search in complex reasoning
tasks and employ beam search for the interaction process. On top of this, we
propose an efficient candidate ranking method to improve the recommendation
results derived from interaction. Extensive experiments on public datasets
demonstrate the effectiveness, efficiency, and transferability of our approach.

</details>


### [194] [X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2504.20859)
*Guy Hadad,Haggai Roitman,Yotam Eshel,Bracha Shapira,Lior Rokach*

Main category: cs.IR

TL;DR: X-Cross是一种新型跨域序列推荐模型，通过集成多个域特定语言模型并使用低秩适配器（LoRA）微调，实现在新域中高效推荐。它在减少参数和微调数据需求的同时，性能接近完全微调模型。


<details>
  <summary>Details</summary>
Motivation: 随着新产品的快速涌现，推荐系统需要在不进行大规模重新训练的情况下快速适应新域。现有方法通常需要大量数据和计算资源，缺乏高效性和适应性。

Method: X-Cross通过分层动态整合多个域特定语言模型的表示，利用LoRA微调每个域的适配器，保留域特定细节并实现跨域适应性。

Result: X-Cross在亚马逊数据集上表现接近完全微调模型（仅用25%额外参数），在跨域任务中（如从玩具域迁移到工具、电子或体育域）减少了50%-75%的微调数据需求，并显著优于其他基线方法。

Conclusion: X-Cross提供了一种高效、可扩展的跨域推荐方案，显著降低了计算开销和数据需求，适用于资源受限的场景。

Abstract: As new products are emerging daily, recommendation systems are required to
quickly adapt to possible new domains without needing extensive retraining.
This work presents ``X-Cross'' -- a novel cross-domain
sequential-recommendation model that recommends products in new domains by
integrating several domain-specific language models; each model is fine-tuned
with low-rank adapters (LoRA). Given a recommendation prompt, operating layer
by layer, X-Cross dynamically refines the representation of each source
language model by integrating knowledge from all other models. These refined
representations are propagated from one layer to the next, leveraging the
activations from each domain adapter to ensure domain-specific nuances are
preserved while enabling adaptability across domains. Using Amazon datasets for
sequential recommendation, X-Cross achieves performance comparable to a model
that is fine-tuned with LoRA, while using only 25% of the additional
parameters. In cross-domain tasks, such as adapting from Toys domain to Tools,
Electronics or Sports, X-Cross demonstrates robust performance, while requiring
about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective.
Furthermore, X-Cross achieves significant improvement in accuracy over
alternative cross-domain baselines. Overall, X-Cross enables scalable and
adaptive cross-domain recommendations, reducing computational overhead and
providing an efficient solution for data-constrained environments.

</details>


### [195] [TreeHop: Generate and Filter Next Query Embeddings Efficiently for Multi-hop Question Answering](https://arxiv.org/abs/2504.20114)
*Zhonghao Li,Kunpeng Zhang,Jinghuai Ou,Shuliang Liu,Xuming Hu*

Main category: cs.IR

TL;DR: TreeHop 是一种基于嵌入层的检索增强生成框架，用于多跳问答，通过动态更新查询嵌入来减少计算开销，性能接近先进方法但参数和延迟大幅降低。


<details>
  <summary>Details</summary>
Motivation: 解决现有 RAG 系统在多跳问答中因迭代查询重写和多阶段处理导致的高计算成本问题。

Method: 提出 TreeHop 框架，通过嵌入层动态融合查询和文档信息，用“检索-嵌入-检索”循环替代传统多步流程，并引入规则化停止准则。

Result: 在三个开放域多跳问答数据集上性能媲美先进方法，模型参数量仅需 5%-0.4%，查询延迟降低约 99%。

Conclusion: TreeHop 是高效、低成本的 RAG 解决方案，适合知识密集型应用部署。

Abstract: Retrieval-augmented generation (RAG) systems face significant challenges in
multi-hop question answering (MHQA), where complex queries require synthesizing
information across multiple document chunks. Existing approaches typically rely
on iterative LLM-based query rewriting and routing, resulting in high
computational costs due to repeated LLM invocations and multi-stage processes.
To address these limitations, we propose TreeHop, an embedding-level framework
without the need for LLMs in query refinement. TreeHop dynamically updates
query embeddings by fusing semantic information from prior queries and
retrieved documents, enabling iterative retrieval through embedding-space
operations alone. This method replaces the traditional
"Retrieve-Rewrite-Vectorize-Retrieve" cycle with a streamlined
"Retrieve-Embed-Retrieve" loop, significantly reducing computational overhead.
Moreover, a rule-based stop criterion is introduced to further prune redundant
retrievals, balancing efficiency and recall rate. Experimental results show
that TreeHop rivals advanced RAG methods across three open-domain MHQA
datasets, achieving comparable performance with only 5\%-0.4\% of the model
parameter size and reducing the query latency by approximately 99\% compared to
concurrent approaches. This makes TreeHop a faster and more cost-effective
solution for deployment in a range of knowledge-intensive applications. For
reproducibility purposes, codes and data are available here:
https://github.com/allen-li1231/TreeHop.

</details>


### [196] [OpenTCM: A GraphRAG-Empowered LLM-based System for Traditional Chinese Medicine Knowledge Retrieval and Diagnosis](https://arxiv.org/abs/2504.20118)
*Jinglin He,Yunqi Guo,Lai Kwan Lam,Waikei Leung,Lixing He,Yuanan Jiang,Chi Chiu Wang,Guoliang Xing,Hongkai Chen*

Main category: cs.IR

TL;DR: 论文介绍了OpenTCM，一个基于大语言模型（LLM）的系统，通过结合领域特定的中医药知识图谱和图基检索增强生成（GraphRAG），提升了中医药知识的现代化和可及性。该系统从中医经典中提取数据构建知识图谱，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 中医药（TCM）知识深厚但复杂，现代AI技术可促进其现代化和普及。然而，古典文本的晦涩和语义关系复杂是主要挑战，因此需要开发高保真的知识检索和问答系统。

Method: 研究分三步：1) 从68本妇科经典中提取3.73万字符；2) 构建包含4.8万实体和15.2万关系的知识图谱；3) 结合知识图谱与OpenTCM系统，实现高效检索和问答。

Result: 实验显示，知识图谱精准度达98.55%，F1分数99.55%。OpenTCM在成分检索和诊断问答任务中分别获得4.5和3.8的专家评分，超越现有方案。

Conclusion: OpenTCM通过结合知识图谱和LLM，显著提升了中医药知识的检索和问答能力，展示了AI在传统医学现代化中的潜力。

Abstract: Traditional Chinese Medicine (TCM) represents a rich repository of ancient
medical knowledge that continues to play an important role in modern
healthcare. Due to the complexity and breadth of the TCM literature, the
integration of AI technologies is critical for its modernization and broader
accessibility. However, this integration poses considerable challenges,
including the interpretation of obscure classical Chinese texts and the
modeling of intricate semantic relationships among TCM concepts. In this paper,
we develop OpenTCM, an LLM-based system that combines a domain-specific TCM
knowledge graph and Graph-based Retrieval-Augmented Generation (GraphRAG).
First, we extract more than 3.73 million classical Chinese characters from 68
gynecological books in the Chinese Medical Classics Database, with the help of
TCM and gynecology experts. Second, we construct a comprehensive
multi-relational knowledge graph comprising more than 48,000 entities and
152,000 interrelationships, using customized prompts and Chinese-oriented LLMs
such as DeepSeek and Kimi to ensure high-fidelity semantic understanding. Last,
we integrate OpenTCM with this knowledge graph, enabling high-fidelity
ingredient knowledge retrieval and diagnostic question-answering without model
fine-tuning. Experimental evaluations demonstrate that our prompt design and
model selection significantly improve knowledge graph quality, achieving a
precision of 98. 55% and an F1 score of 99. 55%. In addition, OpenTCM achieves
mean expert scores of 4.5 in ingredient information retrieval and 3.8 in
diagnostic question-answering tasks, outperforming state-of-the-art solutions
in real-world TCM use cases.

</details>


### [197] [Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets](https://arxiv.org/abs/2504.20119)
*Lorenz Brehme,Thomas Ströhle,Ruth Breu*

Main category: cs.IR

TL;DR: 这篇论文对检索增强生成(RAG)系统进行了系统性评估综述，分析了63篇学术文章，聚焦数据集、检索器、索引与数据库、生成组件四个关键领域，提出自动化评估的可行性，并强调领域专用数据集和平衡自动化与人工评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于RAG系统复杂性高（涉及索引、检索、生成等多组件和多参数），系统性评估和质量提升面临挑战。研究旨在为领域专用应用提供评估方法指导，推动RAG技术的发展。

Method: 系统性综述63篇学术文章，分析RAG四大组件（数据集、检索器、索引与数据库、生成器）的评估方法，提出利用LLM实现自动化评估（生成评估数据集和执行评估）。

Result: 发现自动化评估在RAG各组件中可行，但需更多实践研究指导企业实施；强调领域专用数据集的创建和自动化与人工评估的平衡。

Conclusion: 通过综合RAG关键组件评估方法并探索LLM自动化与人工评估的协同，提升了评估方法的系统性和严谨性，为未来研究与实践提供方向。

Abstract: Retrieval-Augmented Generation (RAG) has advanced significantly in recent
years. The complexity of RAG systems, which involve multiple components-such as
indexing, retrieval, and generation-along with numerous other parameters, poses
substantial challenges for systematic evaluation and quality enhancement.
Previous research highlights that evaluating RAG systems is essential for
documenting advancements, comparing configurations, and identifying effective
approaches for domain-specific applications. This study systematically reviews
63 academic articles to provide a comprehensive overview of state-of-the-art
RAG evaluation methodologies, focusing on four key areas: datasets, retrievers,
indexing and databases, and the generator component. We observe the feasibility
of an automated evaluation approach for each component of a RAG system,
leveraging an LLM capable of both generating evaluation datasets and conducting
evaluations. In addition, we found that further practical research is essential
to provide companies with clear guidance on the do's and don'ts of implementing
and evaluating RAG systems. By synthesizing evaluation approaches for key RAG
components and emphasizing the creation and adaptation of domain-specific
datasets for benchmarking, we contribute to the advancement of systematic
evaluation methods and the improvement of evaluation rigor for RAG systems.
Furthermore, by examining the interplay between automated approaches leveraging
LLMs and human judgment, we contribute to the ongoing discourse on balancing
automation and human input, clarifying their respective contributions,
limitations, and challenges in achieving robust and reliable evaluations.

</details>


### [198] [Enhancing News Recommendation with Hierarchical LLM Prompting](https://arxiv.org/abs/2504.20452)
*Hai-Dang Kieu,Delvin Ce Zhang,Minh Duc Nguyen,Min Xu,Qiang Wu,Dung D. Le*

Main category: cs.IR

TL;DR: PNR-LLM利用大语言模型（LLM）生成更丰富的新闻标题和摘要语义信息，通过注意力机制整合语义和实体数据，显著提升个性化新闻推荐的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有个性化新闻推荐系统依赖浅层表征（如标题和摘要），难以充分捕捉用户偏好的复杂性，因此需要一种更深度的方法来提升推荐质量。

Method: 提出PNR-LLM，包含基于LLM的新闻内容增强模块，生成深层语义和实体信息，并通过注意力机制聚合这些信息，形成统一的用户和新闻嵌入。

Result: 在MIND数据集上的实验表明，PNR-LLM优于现有基线模型，且其数据增强模块可通用化应用于其他模型，进一步提升性能。

Conclusion: PNR-LLM通过LLM增强新闻表征，结合注意力机制，实现了更精准的个性化推荐，其模块化设计具有广泛适用性。

Abstract: Personalized news recommendation systems often struggle to effectively
capture the complexity of user preferences, as they rely heavily on shallow
representations, such as article titles and abstracts. To address this problem,
we introduce a novel method, namely PNR-LLM, for Large Language Models for
Personalized News Recommendation. Specifically, PNR-LLM harnesses the
generation capabilities of LLMs to enrich news titles and abstracts, and
consequently improves recommendation quality. PNR-LLM contains a novel module,
News Enrichment via LLMs, which generates deeper semantic information and
relevant entities from articles, transforming shallow contents into richer
representations. We further propose an attention mechanism to aggregate
enriched semantic- and entity-level data, forming unified user and news
embeddings that reveal a more accurate user-news match. Extensive experiments
on MIND datasets show that PNR-LLM outperforms state-of-the-art baselines.
Moreover, the proposed data enrichment module is model-agnostic, and we
empirically show that applying our proposed module to multiple existing models
can further improve their performance, verifying the advantage of our design.

</details>


### [199] [Information Retrieval in the Age of Generative AI: The RGB Model](https://arxiv.org/abs/2504.20610)
*Michele Garetto,Alessandro Cornacchia,Franco Galante,Emilio Leonardi,Alessandro Nordio,Alberto Tarable*

Main category: cs.IR

TL;DR: 本文提出了一种量化方法来分析生成式AI对信息动态的影响，强调其快速采用可能加剧虚假信息传播的风险。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI工具在信息检索和处理中的广泛应用所带来的潜在风险和未解动态，尤其是其对内容真实性和可靠性的影响。

Method: 提出了一种随机模型，用于描述新话题下信息的生成、索引和传播过程，并结合Stack Exchange数据进行了深入分析。

Result: 研究发现生成式AI的快速采用可能超越人工验证速度，增加虚假信息传播风险，高质量答案仍依赖大量时间和人力投入。

Conclusion: 未来生成式AI工具的开发和部署需要更负责任，以减轻其可能带来的负面影响。

Abstract: The advent of Large Language Models (LLMs) and generative AI is fundamentally
transforming information retrieval and processing on the Internet, bringing
both great potential and significant concerns regarding content authenticity
and reliability. This paper presents a novel quantitative approach to shed
light on the complex information dynamics arising from the growing use of
generative AI tools. Despite their significant impact on the digital ecosystem,
these dynamics remain largely uncharted and poorly understood. We propose a
stochastic model to characterize the generation, indexing, and dissemination of
information in response to new topics. This scenario particularly challenges
current LLMs, which often rely on real-time Retrieval-Augmented Generation
(RAG) techniques to overcome their static knowledge limitations. Our findings
suggest that the rapid pace of generative AI adoption, combined with increasing
user reliance, can outpace human verification, escalating the risk of
inaccurate information proliferation across digital resources. An in-depth
analysis of Stack Exchange data confirms that high-quality answers inevitably
require substantial time and human effort to emerge. This underscores the
considerable risks associated with generating persuasive text in response to
new questions and highlights the critical need for responsible development and
deployment of future generative AI tools.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [200] [PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations](https://arxiv.org/abs/2504.20520)
*Haowen Sun,Han Wang,Chengzhong Ma,Shaolong Zhang,Jiawei Ye,Xingyu Chen,Xuguang Lan*

Main category: cs.RO

TL;DR: 该论文提出了一种结合真实到模拟再到真实的流程，通过专家演示构建仿真环境，并利用视觉语言模型指导强化学习策略训练，旨在解决机器人初始位置和物体姿态变化的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 解决机器人从少量演示中学习并适应初始位置和物体姿态变化的问题，同时避免直接交互的不安全性和仿真环境搭建的高成本。

Method: 通过专家演示构建仿真环境，利用图像识别和3D模型库自动设计场景；提出基于投影的奖励模型，结合视觉语言模型指导强化学习策略训练，并通过专家演示微调策略。

Result: 该方法能够高效构建仿真环境，并通过强化学习训练出适应性强、鲁棒的机器人控制策略。

Conclusion: 该集成流程展示了从演示构建仿真环境到实际部署的完整方案，为机器人控制策略的实际应用提供了可行路径。

Abstract: Learning from few demonstrations to develop policies robust to variations in
robot initial positions and object poses is a problem of significant practical
interest in robotics. Compared to imitation learning, which often struggles to
generalize from limited samples, reinforcement learning (RL) can autonomously
explore to obtain robust behaviors. Training RL agents through direct
interaction with the real world is often impractical and unsafe, while building
simulation environments requires extensive manual effort, such as designing
scenes and crafting task-specific reward functions. To address these
challenges, we propose an integrated real-to-sim-to-real pipeline that
constructs simulation environments based on expert demonstrations by
identifying scene objects from images and retrieving their corresponding 3D
models from existing libraries. We introduce a projection-based reward model
for RL policy training that is supervised by a vision-language model (VLM)
using human-guided object projection relationships as prompts, with the policy
further fine-tuned using expert demonstrations. In general, our work focuses on
the construction of simulation environments and RL-based policy training,
ultimately enabling the deployment of reliable robotic control policies in
real-world scenarios.

</details>


### [201] [SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings](https://arxiv.org/abs/2504.20808)
*Florian Vahl,Jörn Griepenburg,Jan Gutsche,Jasper Güldenstein,Jianwei Zhang*

Main category: cs.RO

TL;DR: SoccerDiffusion是一种基于Transformer的扩散模型，用于从真实比赛录像中学习人形机器人足球的端到端控制策略。


<details>
  <summary>Details</summary>
Motivation: 研究目的是通过真实比赛数据直接学习控制策略，避免手工设计复杂的运动行为。

Method: 模型使用多模态传感器输入（视觉、本体感觉和游戏状态）预测关节命令轨迹，并通过蒸馏技术实现嵌入式平台上的实时推理。

Result: 模型能够复现行走、踢球和跌倒恢复等复杂运动行为，并在仿真和物理机器人上验证。

Conclusion: 尽管高层战术行为有限，但为后续强化学习或偏好优化方法提供了坚实基础。

Abstract: This paper introduces SoccerDiffusion, a transformer-based diffusion model
designed to learn end-to-end control policies for humanoid robot soccer
directly from real-world gameplay recordings. Using data collected from RoboCup
competitions, the model predicts joint command trajectories from multi-modal
sensor inputs, including vision, proprioception, and game state. We employ a
distillation technique to enable real-time inference on embedded platforms that
reduces the multi-step diffusion process to a single step. Our results
demonstrate the model's ability to replicate complex motion behaviors such as
walking, kicking, and fall recovery both in simulation and on physical robots.
Although high-level tactical behavior remains limited, this work provides a
robust foundation for subsequent reinforcement learning or preference
optimization methods. We release the dataset, pretrained models, and code
under: https://bit-bots.github.io/SoccerDiffusion

</details>


### [202] [XPG-RL: Reinforcement Learning with Explainable Priority Guidance for Efficiency-Boosted Mechanical Search](https://arxiv.org/abs/2504.20969)
*Yiting Zhang,Shichen Li,Elena Shrestha*

Main category: cs.RO

TL;DR: XPG-RL is a reinforcement learning框架，通过基于原始感官输入的可解释优先级决策，使机器人能够在杂乱环境中高效完成机械搜索任务，并显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决自主机械手在杂乱环境中进行机械搜索时的长期规划与部分可观测性挑战，提升任务成功率和运动效率。

Method: 结合任务驱动的动作优先级机制与上下文感知切换策略，通过强化学习优化策略，动态选择动作原语（如抓取、遮挡移除等）。感知模块融合RGB-D输入与语义几何特征，生成结构化场景表示。

Result: 在仿真和现实环境中，XPG-RL的任务成功率和运动效率均优于基线方法，长期任务效率提升高达4.5倍。

Conclusion: 将领域知识与可学习决策策略结合，能实现高效且鲁棒的机器人操作。

Abstract: Mechanical search (MS) in cluttered environments remains a significant
challenge for autonomous manipulators, requiring long-horizon planning and
robust state estimation under occlusions and partial observability. In this
work, we introduce XPG-RL, a reinforcement learning framework that enables
agents to efficiently perform MS tasks through explainable, priority-guided
decision-making based on raw sensory inputs. XPG-RL integrates a task-driven
action prioritization mechanism with a learned context-aware switching strategy
that dynamically selects from a discrete set of action primitives such as
target grasping, occlusion removal, and viewpoint adjustment. Within this
strategy, a policy is optimized to output adaptive threshold values that govern
the discrete selection among action primitives. The perception module fuses
RGB-D inputs with semantic and geometric features to produce a structured scene
representation for downstream decision-making. Extensive experiments in both
simulation and real-world settings demonstrate that XPG-RL consistently
outperforms baseline methods in task success rates and motion efficiency,
achieving up to 4.5$\times$ higher efficiency in long-horizon tasks. These
results underscore the benefits of integrating domain knowledge with learnable
decision-making policies for robust and efficient robotic manipulation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [203] [SCOPE-MRI: Bankart Lesion Detection as a Case Study in Data Curation and Deep Learning for Challenging Diagnoses](https://arxiv.org/abs/2504.20405)
*Sahil Sethi,Sai Reddy,Mansi Sakarvadia,Jordan Serotte,Darlington Nwaudo,Nicholas Maassen,Lewis Shi*

Main category: eess.IV

TL;DR: 该研究提出了ScopeMRI，首个公开的专家标注肩部病理数据集，并开发了一个深度学习框架，用于在标准MRI和MRA上检测Bankart病变。模型的性能达到或超过放射科医生的水平，减少了对侵入性MRA的需求。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习研究在肌肉骨骼成像中多关注诊断不具挑战性的病理，而对Bankart病变等难题研究不足。这些病变的影像特征细微，导致依赖侵入性MRA。

Method: 研究收集了586例肩部MRI（标准MRI 335例，MRA 251例），使用CNN和transformer训练独立模型，并集成多视图预测。模型在20%的测试集上评估。

Result: 模型在标准MRI和MRA上的AUC分别为0.91和0.93，敏感性和特异性均表现优异，非侵入性标准MRI上的表现甚至优于放射科医生对MRA的解读。

Conclusion: 深度学习模型在标准MRI上可达到放射科医生水平的诊断性能，有望减少对侵入性MRA的依赖。公开数据集和代码旨在推动肌肉骨骼成像研究。

Abstract: While deep learning has shown strong performance in musculoskeletal imaging,
existing work has largely focused on pathologies where diagnosis is not a
clinical challenge, leaving more difficult problems underexplored, such as
detecting Bankart lesions (anterior-inferior glenoid labral tears) on standard
MRIs. Diagnosing these lesions is challenging due to their subtle imaging
features, often leading to reliance on invasive MRI arthrograms (MRAs). This
study introduces ScopeMRI, the first publicly available, expert-annotated
dataset for shoulder pathologies, and presents a deep learning (DL) framework
for detecting Bankart lesions on both standard MRIs and MRAs. ScopeMRI includes
586 shoulder MRIs (335 standard, 251 MRAs) from 558 patients who underwent
arthroscopy. Ground truth labels were derived from intraoperative findings, the
gold standard for diagnosis. Separate DL models for MRAs and standard MRIs were
trained using a combination of CNNs and transformers. Predictions from
sagittal, axial, and coronal views were ensembled to optimize performance. The
models were evaluated on a 20% hold-out test set (117 MRIs: 46 MRAs, 71
standard MRIs). The models achieved an AUC of 0.91 and 0.93, sensitivity of 83%
and 94%, and specificity of 91% and 86% for standard MRIs and MRAs,
respectively. Notably, model performance on non-invasive standard MRIs matched
or surpassed radiologists interpreting MRAs. External validation demonstrated
initial generalizability across imaging protocols. This study demonstrates that
DL models can achieve radiologist-level diagnostic performance on standard
MRIs, reducing the need for invasive MRAs. By releasing ScopeMRI and a modular
codebase for training and evaluating deep learning models on 3D medical imaging
data, we aim to accelerate research in musculoskeletal imaging and support the
development of new datasets for clinically challenging diagnostic tasks.

</details>


### [204] [Full-field surrogate modeling of cardiac function encoding geometric variability](https://arxiv.org/abs/2504.20479)
*Elena Martinez,Beatrice Moscoloni,Matteo Salvador,Fanwei Kong,Mathias Peirlinck,Alison Lesley Marsden*

Main category: eess.IV

TL;DR: 该论文提出了一种结合物理建模和数据驱动方法的计算流程，用于构建心脏功能的通用替代模型。通过生成合成几何数据集并采用BLNMs方法，模型在儿科患者中展现了良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于物理的数值模拟构建的心脏替代模型通常需要针对不同患者和病理条件重新训练，缺乏通用性。本研究旨在开发一种能够适应不同解剖结构的通用替代模型。

Method: 使用多尺度数学模型生成电生理学模拟数据集，通过大变形微分同胚度量映射构建双心室解剖图谱，并采用BLNMs方法对模拟数据进行编码。

Result: 模型在13名复杂儿科患者中表现出强大的泛化能力，平均无因次均方误差为0.0034。

Conclusion: 提出的BLNMs方法为心脏功能的通用替代建模提供了有效工具，有望加速临床转化。

Abstract: Combining physics-based modeling with data-driven methods is critical to
enabling the translation of computational methods to clinical use in
cardiology. The use of rigorous differential equations combined with machine
learning tools allows for model personalization with uncertainty quantification
in time frames compatible with clinical practice. However, accurate and
efficient surrogate models of cardiac function, built from physics-based
numerical simulation, are still mostly geometry-specific and require retraining
for different patients and pathological conditions. We propose a novel
computational pipeline to embed cardiac anatomies into full-field surrogate
models. We generate a dataset of electrophysiology simulations using a complex
multi-scale mathematical model coupling partial and ordinary differential
equations. We adopt Branched Latent Neural Maps (BLNMs) as an effective
scientific machine learning method to encode activation maps extracted from
physics-based numerical simulations into a neural network. Leveraging large
deformation diffeomorphic metric mappings, we build a biventricular anatomical
atlas and parametrize the anatomical variability of a small and challenging
cohort of 13 pediatric patients affected by Tetralogy of Fallot. We propose a
novel statistical shape modeling based z-score sampling approach to generate a
new synthetic cohort of 52 biventricular geometries that are compatible with
the original geometrical variability. This synthetic cohort acts as the
training set for BLNMs. Our surrogate model demonstrates robustness and great
generalization across the complex original patient cohort, achieving an average
adimensional mean squared error of 0.0034. The Python implementation of our
BLNM model is publicly available under MIT License at
https://github.com/StanfordCBCL/BLNM.

</details>


### [205] [Quality-factor inspired deep neural network solver for solving inverse scattering problems](https://arxiv.org/abs/2504.20504)
*Yutong Du,Zicheng Liu,Miao Cao,Zupeng Liang,Yali Zong,Changyou Li*

Main category: eess.IV

TL;DR: 提出一种基于质量因子的深度神经网络（QuaDNN）解决电磁逆散射问题，通过优化训练数据集、改进网络结构和损失函数，提升成像性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经网络在解决电磁逆散射问题时，成像性能受训练数据集、网络结构和损失函数的影响，需优化这些关键因素。

Method: 定义了质量因子优化训练数据集，结合残差连接和通道注意力机制改进网络结构，并设计了包含数据拟合误差、物理约束和期望特征的损失函数。

Result: 数值分析和实验验证表明，QuaDNN能有效抑制背景伪影并提高重建精度。

Conclusion: QuaDNN通过综合优化训练数据、网络和损失函数，显著提升了电磁逆散射问题的成像性能。

Abstract: Deep neural networks have been applied to address electromagnetic inverse
scattering problems (ISPs) and shown superior imaging performances, which can
be affected by the training dataset, the network architecture and the applied
loss function. Here, the quality of data samples is cared and valued by the
defined quality factor. Based on the quality factor, the composition of the
training dataset is optimized. The network architecture is integrated with the
residual connections and channel attention mechanism to improve feature
extraction. A loss function that incorporates data-fitting error,
physical-information constraints and the desired feature of the solution is
designed and analyzed to suppress the background artifacts and improve the
reconstruction accuracy. Various numerical analysis are performed to
demonstrate the superiority of the proposed quality-factor inspired deep neural
network (QuaDNN) solver and the imaging performance is finally verified by
experimental imaging test.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [206] [Heterogeneous network drug-target interaction prediction model based on graph wavelet transform and multi-level contrastive learning](https://arxiv.org/abs/2504.20103)
*Wenfeng Dai,Yanhong Wang,Shuai Yan,Qingzhi Yu,Xiang Cheng*

Main category: q-bio.QM

TL;DR: 提出一种结合图神经网络和多尺度信号处理的药物-靶点相互作用预测框架，解决传统方法的黑箱问题，具有高效预测和多层次可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决传统机器学习在药物-靶点相互作用预测中的黑箱问题，揭示模型决策机制与生物分子相互作用模式的深层关联。

Method: 结合异构图卷积神经网络（HGCN）和多尺度信号处理技术，设计了局部-全局特征协同感知模块、多尺度图信号分解模块，并通过对比学习融合多维信息。

Result: 实验表明，该框架在所有数据集上均表现出优秀的预测性能。

Conclusion: 该研究为从黑箱预测到机制解码的药物-靶点发现提供了完整解决方案，对复杂生物分子相互作用系统建模具有重要参考价值。

Abstract: Drug-target interaction (DTI) prediction is a core task in drug development
and precision medicine in the biomedical field. However, traditional machine
learning methods generally have the black box problem, which makes it difficult
to reveal the deep correlation between the model decision mechanism and the
interaction pattern between biological molecules. This study proposes a
heterogeneous network drug target interaction prediction framework, integrating
graph neural network and multi scale signal processing technology to construct
a model with both efficient prediction and multi level interpretability. Its
technical breakthroughs are mainly reflected in the following three
dimensions:Local global feature collaborative perception module. Based on
heterogeneous graph convolutional neural network (HGCN), a multi order neighbor
aggregation strategy is designed.Multi scale graph signal decomposition and
biological interpretation module. A deep hierarchical node feature transform
(GWT) architecture is proposed.Contrastive learning combining multi dimensional
perspectives and hierarchical representations. By comparing the learning
models, the node representations from the two perspectives of HGCN and GWT are
aligned and fused, so that the model can integrate multi dimensional
information and improve the prediction robustness. Experimental results show
that our framework shows excellent prediction performance on all datasets. This
study provides a complete solution for drug target discovery from black box
prediction to mechanism decoding, and its methodology has important reference
value for modeling complex biomolecular interaction systems.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [207] [Coreset selection for the Sinkhorn divergence and generic smooth divergences](https://arxiv.org/abs/2504.20194)
*Alex Kokot,Alex Luedtke*

Main category: stat.ML

TL;DR: CO2算法能高效生成针对通用平滑散度的凸加权核心集，通过函数泰勒展开实现局部等价性，并将其转化为最大均值差异最小化问题。应用于Sinkhorn散度时，仅需对数级数据点即可达到随机采样的近似效果，同时揭示了熵正则化最优传输的新性质。该方法将核心集选择与核积分方法联系到经典统计方法，并在图像数据子采样中验证了实用性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提出一种高效的核心集选择算法（CO2），以解决通用平滑散度下的计算效率问题，并通过理论分析揭示其与经典统计方法的联系。

Method: 采用函数泰勒展开实现损失函数与其二阶近似的局部等价性，将核心集选择问题转化为最大均值差异最小化，并应用于Sinkhorn散度。

Result: 算法在Sinkhorn散度下仅需对数级数据点即可达到随机采样的近似效果，并验证了熵正则化最优传输的新性质。

Conclusion: CO2算法不仅提供了高效的核心集选择方法，还为核积分与经典统计方法的关联提供了新视角，未来可进一步优化算法效率与理论保证。

Abstract: We introduce CO2, an efficient algorithm to produce convexly-weighted
coresets with respect to generic smooth divergences. By employing a functional
Taylor expansion, we show a local equivalence between sufficiently regular
losses and their second order approximations, reducing the coreset selection
problem to maximum mean discrepancy minimization. We apply CO2 to the Sinkhorn
divergence, providing a novel sampling procedure that requires logarithmically
many data points to match the approximation guarantees of random sampling. To
show this, we additionally verify several new regularity properties for
entropically regularized optimal transport of independent interest. Our
approach leads to a new perspective linking coreset selection and kernel
quadrature to classical statistical methods such as moment and score matching.
We showcase this method with a practical application of subsampling image data,
and highlight key directions to explore for improved algorithmic efficiency and
theoretical guarantees.

</details>


### [208] [Sobolev norm inconsistency of kernel interpolation](https://arxiv.org/abs/2504.20617)
*Yunfei Yang*

Main category: stat.ML

TL;DR: 该论文研究了有界核对应的再生核希尔伯特空间中最小范数插值的一致性，证明了在某些条件下核插值始终不一致。


<details>
  <summary>Details</summary>
Motivation: 探讨再生核希尔伯特空间中最小范数插值的泛化误差，揭示其在特定条件下的不一致性。

Method: 通过分析$L^2$范数与假设空间之间的连续范数尺度，提出核插值泛化误差的下界理论。

Result: 结果显示，当范数的光滑度指数超过由假设空间嵌入指数和特征值衰减率决定的常数时，核插值总是表现不一致。

Conclusion: 核插值在再生核希尔伯特空间中的一致性受限于核的嵌入性质和特征值衰减率。

Abstract: We study the consistency of minimum-norm interpolation in reproducing kernel
Hilbert spaces corresponding to bounded kernels. Our main result give lower
bounds for the generalization error of the kernel interpolation measured in a
continuous scale of norms that interpolate between $L^2$ and the hypothesis
space. These lower bounds imply that kernel interpolation is always
inconsistent, when the smoothness index of the norm is larger than a constant
that depends only on the embedding index of the hypothesis space and the decay
rate of the eigenvalues.

</details>


### [209] [Learning and Generalization with Mixture Data](https://arxiv.org/abs/2504.20651)
*Harsh Vardhan,Avishek Ghosh,Arya Mazumdar*

Main category: stat.ML

TL;DR: 本文研究了从混合分布中采样的数据的泛化性能和统计率，通过表征混合的异质性及其对学习的影响，提供了在PAC框架下的泛化性能分析和统计误差率。


<details>
  <summary>Details</summary>
Motivation: 现代大规模学习中，数据异质性是一个主要挑战，本文旨在探讨混合分布数据在何种情况下可被视为同质分布进行学习，并分析其对泛化性能和统计率的影响。

Method: 通过计算混合数据的Rademacher复杂度和（局部）Gauss复杂度界限，并应用于泛化和收敛率分析，研究了参数和非参数回归问题的统计误差率。

Result: 研究发现，随着函数类复杂性增加，对混合分布的总变差距离要求更为严格，这与直觉相符；针对混合线性回归进行了更精细分析，给出了泛化误差的紧界。

Conclusion: 混合分布数据的泛化性能和统计率受到函数类复杂性和异质性程度的显著影响，为处理异质数据提供了理论依据。

Abstract: In many, if not most, machine learning applications the training data is
naturally heterogeneous (e.g. federated learning, adversarial attacks and
domain adaptation in neural net training). Data heterogeneity is identified as
one of the major challenges in modern day large-scale learning. A classical way
to represent heterogeneous data is via a mixture model. In this paper, we study
generalization performance and statistical rates when data is sampled from a
mixture distribution. We first characterize the heterogeneity of the mixture in
terms of the pairwise total variation distance of the sub-population
distributions. Thereafter, as a central theme of this paper, we characterize
the range where the mixture may be treated as a single (homogeneous)
distribution for learning. In particular, we study the generalization
performance under the classical PAC framework and the statistical error rates
for parametric (linear regression, mixture of hyperplanes) as well as
non-parametric (Lipschitz, convex and H\"older-smooth) regression problems. In
order to do this, we obtain Rademacher complexity and (local) Gaussian
complexity bounds with mixture data, and apply them to get the generalization
and convergence rates respectively. We observe that as the (regression)
function classes get more complex, the requirement on the pairwise total
variation distance gets stringent, which matches our intuition. We also do a
finer analysis for the case of mixed linear regression and provide a tight
bound on the generalization error in terms of heterogeneity.

</details>


### [210] [Preference-centric Bandits: Optimality of Mixtures and Regret-efficient Algorithms](https://arxiv.org/abs/2504.20877)
*Meltem Tatlı,Arpan Mukherjee,Prashanth L. A.,Karthikeyan Shanmugam,Ali Tajer*

Main category: stat.ML

TL;DR: 论文提出了一种从基于期望的评估转向偏好度量（PM）的新框架，强调奖励分布的尾部行为和风险偏好。设计了两类算法（依赖时间范围的和即时的），通过估计和追踪机制高效学习最优混合策略，与传统多臂老虎机算法有显著区别。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机算法仅关注奖励的期望值，忽略了分布的尾部行为和风险。本文提出偏好度量（PM）框架，以编码风险厌恶、鲁棒性等不确定性偏好，弥补这一不足。

Method: 提出PM-centric框架，设计了两类算法：依赖时间范围的和即时的。算法通过估计最优混合权重和追踪机制，实现高效的遗憾保证。

Result: 算法能学习并追踪最优混合策略，其遗憾性能在不同PM代数形式下均得到验证，展示了与传统算法的显著差异。

Conclusion: PM-centric框架为多臂老虎机问题提供了更灵活的风险偏好建模，算法设计原则的转变为实际决策问题提供了新思路。

Abstract: The objective of canonical multi-armed bandits is to identify and repeatedly
select an arm with the largest reward, often in the form of the expected value
of the arm's probability distribution. Such a utilitarian perspective and focus
on the probability models' first moments, however, is agnostic to the
distributions' tail behavior and their implications for variability and risks
in decision-making. This paper introduces a principled framework for shifting
from expectation-based evaluation to an alternative reward formulation, termed
a preference metric (PM). The PMs can place the desired emphasis on different
reward realization and can encode a richer modeling of preferences that
incorporate risk aversion, robustness, or other desired attitudes toward
uncertainty. A fundamentally distinct observation in such a PM-centric
perspective is that designing bandit algorithms will have a significantly
different principle: as opposed to the reward-based models in which the optimal
sampling policy converges to repeatedly sampling from the single best arm, in
the PM-centric framework the optimal policy converges to selecting a mix of
arms based on specific mixing weights. Designing such mixture policies departs
from the principles for designing bandit algorithms in significant ways,
primarily because of uncountable mixture possibilities. The paper formalizes
the PM-centric framework and presents two algorithm classes (horizon-dependent
and anytime) that learn and track mixtures in a regret-efficient fashion. These
algorithms have two distinctions from their canonical counterparts: (i) they
involve an estimation routine to form reliable estimates of optimal mixtures,
and (ii) they are equipped with tracking mechanisms to navigate arm selection
fractions to track the optimal mixtures. These algorithms' regret guarantees
are investigated under various algebraic forms of the PMs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [211] [ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies](https://arxiv.org/abs/2504.20117)
*Shubham Gandhi,Dhruv Shah,Manasi Patwardhan,Lovekesh Vig,Gautam Shroff*

Main category: cs.SE

TL;DR: ResearchCodeAgent是一个基于大型语言模型的多智能体系统，用于自动生成机器学习文献中的研究方法代码，提高研究效率和代码质量。


<details>
  <summary>Details</summary>
Motivation: 解决研究概念与实际实现之间的鸿沟，加速机器学习研究的实施过程。

Method: 采用灵活的智能体架构和动态规划机制，结合上下文感知交互和迭代适应。

Result: 46.9%的生成代码质量高且无错误，25%优于基线实现，编码时间平均减少57.9%。

Conclusion: ResearchCodeAgent有效自动化研究实现，有望加速机器学习研究进展。

Abstract: In this paper we introduce ResearchCodeAgent, a novel multi-agent system
leveraging large language models (LLMs) agents to automate the codification of
research methodologies described in machine learning literature. The system
bridges the gap between high-level research concepts and their practical
implementation, allowing researchers auto-generating code of existing research
papers for benchmarking or building on top-of existing methods specified in the
literature with availability of partial or complete starter code.
ResearchCodeAgent employs a flexible agent architecture with a comprehensive
action suite, enabling context-aware interactions with the research
environment. The system incorporates a dynamic planning mechanism, utilizing
both short and long-term memory to adapt its approach iteratively. We evaluate
ResearchCodeAgent on three distinct machine learning tasks with distinct task
complexity and representing different parts of the ML pipeline: data
augmentation, optimization, and data batching. Our results demonstrate the
system's effectiveness and generalizability, with 46.9% of generated code being
high-quality and error-free, and 25% showing performance improvements over
baseline implementations. Empirical analysis shows an average reduction of
57.9% in coding time compared to manual implementation. We observe higher gains
for more complex tasks. ResearchCodeAgent represents a significant step towards
automating the research implementation process, potentially accelerating the
pace of machine learning research.

</details>


### [212] [Self-Healing Software Systems: Lessons from Nature, Powered by AI](https://arxiv.org/abs/2504.20093)
*Mohammad Baqar,Rajat Khanda,Saba Naqvi*

Main category: cs.SE

TL;DR: 提出了一种受生物启发的人工智能驱动的自愈软件框架，通过AI诊断和修复，结合日志分析和静态代码检查，提高软件恢复能力和减少停机时间。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统日益复杂，亟需自主检测、诊断和从故障中恢复的能力，以缩短调试时间并提升系统可靠性。

Method: 框架模仿生物愈合机制，利用系统可观察性工具作为感知输入，AI模型作为认知核心进行诊断和修复，并通过修复代理应用针对性的代码和测试修改。

Result: 通过案例研究和模拟评估，该方法显著优于传统手动调试和恢复流程，证明了其有效性。

Conclusion: 这项研究为开发智能、自适应的自愈软件系统奠定了基础，使软件能够像生物体一样持续自我修复。

Abstract: As modern software systems grow in complexity and scale, their ability to
autonomously detect, diagnose, and recover from failures becomes increasingly
vital. Drawing inspiration from biological healing - where the human body
detects damage, signals the brain, and activates targeted recovery - this paper
explores the concept of self-healing software driven by artificial
intelligence. We propose a novel framework that mimics this biological model
system observability tools serve as sensory inputs, AI models function as the
cognitive core for diagnosis and repair, and healing agents apply targeted code
and test modifications. By combining log analysis, static code inspection, and
AI-driven generation of patches or test updates, our approach aims to reduce
downtime, accelerate debugging, and enhance software resilience. We evaluate
the effectiveness of this model through case studies and simulations, comparing
it against traditional manual debugging and recovery workflows. This work paves
the way toward intelligent, adaptive and self-reliant software systems capable
of continuous healing, akin to living organisms.

</details>


### [213] [AutoP2C: An LLM-Based Agent Framework for Code Repository Generation from Multimodal Content in Academic Papers](https://arxiv.org/abs/2504.20115)
*Zijie Lin,Yiqing Shen,Qilin Cai,He Sun,Jinrui Zhou,Mingjun Xiao*

Main category: cs.SE

TL;DR: AutoP2C是一个多智能体框架，能够将学术论文的多模态内容（如文本、图表）转化为可执行的代码库，显著优于现有的代码生成方法。


<details>
  <summary>Details</summary>
Motivation: 解决将学术论文中复杂多模态内容转化为可执行代码的挑战，降低ML研究的实施门槛。

Method: 基于大语言模型的多智能体框架，包含四个阶段：代码库蓝图提取、多模态内容解析、分层任务分解和迭代调试。

Result: 在八篇论文的测试中，AutoP2C成功生成了所有可执行代码库，而其他基线模型仅能生成一篇。

Conclusion: AutoP2C为自动化代码生成提供了高效解决方案，显著提升了科研论文到代码的转换效率。

Abstract: Machine Learning (ML) research is spread through academic papers featuring
rich multimodal content, including text, diagrams, and tabular results.
However, translating these multimodal elements into executable code remains a
challenging and time-consuming process that requires substantial ML expertise.
We introduce ``Paper-to-Code'' (P2C), a novel task that transforms the
multimodal content of scientific publications into fully executable code
repositories, which extends beyond the existing formulation of code generation
that merely converts textual descriptions into isolated code snippets. To
automate the P2C process, we propose AutoP2C, a multi-agent framework based on
large language models that processes both textual and visual content from
research papers to generate complete code repositories. Specifically, AutoP2C
contains four stages: (1) repository blueprint extraction from established
codebases, (2) multimodal content parsing that integrates information from
text, equations, and figures, (3) hierarchical task decomposition for
structured code generation, and (4) iterative feedback-driven debugging to
ensure functionality and performance. Evaluation on a benchmark of eight
research papers demonstrates the effectiveness of AutoP2C, which can
successfully generate executable code repositories for all eight papers, while
OpenAI-o1 or DeepSeek-R1 can only produce runnable code for one paper. The code
is available at https://github.com/shoushouyu/Automated-Paper-to-Code.

</details>


### [214] [BLADE: Benchmark suite for LLM-driven Automated Design and Evolution of iterative optimisation heuristics](https://arxiv.org/abs/2504.20183)
*Niki van Stein,Anna V. Kononova,Haoran Yin,Thomas Bäck*

Main category: cs.SE

TL;DR: 该论文介绍了BLADE，一个模块化和可扩展的基准测试框架，用于评估大语言模型（LLM）驱动的自动化算法设计（AAD）方法。


<details>
  <summary>Details</summary>
Motivation: 鉴于LLM驱动的AAD方法的兴起及其设计过程的不透明性，以及现有基准测试的局限性，亟需一个标准化的基准测试框架来严格评估这些方法的能力和局限性。

Method: BLADE整合了多种基准问题（如MA-BBOB和SBOX-COST）、实例生成器和文本描述，支持灵活性实验设置、标准化日志记录，并集成了如IOHanalyser等工具进行过程和结果分析。

Result: BLADE提供了系统化评估LLM驱动AAD方法的解决方案，并通过两个用例（突变提示策略和函数特化）展示了其功能性。

Conclusion: BLADE是一个高效的工具，能够促进LLM驱动AAD方法的可靠和可重复评估。

Abstract: The application of Large Language Models (LLMs) for Automated Algorithm
Discovery (AAD), particularly for optimisation heuristics, is an emerging field
of research. This emergence necessitates robust, standardised benchmarking
practices to rigorously evaluate the capabilities and limitations of LLM-driven
AAD methods and the resulting generated algorithms, especially given the
opacity of their design process and known issues with existing benchmarks. To
address this need, we introduce BLADE (Benchmark suite for LLM-driven Automated
Design and Evolution), a modular and extensible framework specifically designed
for benchmarking LLM-driven AAD methods in a continuous black-box optimisation
context. BLADE integrates collections of benchmark problems (including MA-BBOB
and SBOX-COST among others) with instance generators and textual descriptions
aimed at capability-focused testing, such as generalisation, specialisation and
information exploitation. It offers flexible experimental setup options,
standardised logging for reproducibility and fair comparison, incorporates
methods for analysing the AAD process (e.g., Code Evolution Graphs and various
visualisation approaches) and facilitates comparison against human-designed
baselines through integration with established tools like IOHanalyser and
IOHexplainer. BLADE provides an `out-of-the-box' solution to systematically
evaluate LLM-driven AAD approaches. The framework is demonstrated through two
distinct use cases exploring mutation prompt strategies and function
specialisation.

</details>


### [215] [Prompting LLMs for Code Editing: Struggles and Remedies](https://arxiv.org/abs/2504.20196)
*Daye Nam,Ahmed Omran,Ambar Murillo,Saksham Thakur,Abner Araujo,Marcel Blistein,Alexander Frömmgen,Vincent Hellendoorn,Satish Chandra*

Main category: cs.SE

TL;DR: 该论文研究了开发者如何使用基于LLM的代码编辑工具Transform Code，发现频繁重新提示（re-prompting）是开发者使用困难的信号，并通过分析不满意的请求提出了AutoPrompter工具，提升了编辑正确率27%。


<details>
  <summary>Details</summary>
Motivation: 研究者旨在填补对开发者如何在日常工作中实际使用LLM代码工具的认知空白，尤其是他们遇到的困难点。

Method: 通过分析IDE中的使用日志定性研究频繁重新提示的现象，并结合对不满意请求的分析开发了AutoPrompter工具。

Result: 分析显示重新提示是使用困难的信号，AutoPrompter工具通过自动补全提示信息将编辑正确率提高了27%。

Conclusion: 该研究揭示了开发者使用LLM代码工具的痛点，并通过AutoPrompter工具展示了自动化改进提示的潜力。

Abstract: Large Language Models (LLMs) are rapidly transforming software engineering,
with coding assistants embedded in an IDE becoming increasingly prevalent.
While research has focused on improving the tools and understanding developer
perceptions, a critical gap exists in understanding how developers actually use
these tools in their daily workflows, and, crucially, where they struggle. This
paper addresses part of this gap through a multi-phased investigation of
developer interactions with an LLM-powered code editing and transformation
feature, Transform Code, in an IDE widely used at Google. First, we analyze
telemetry logs of the feature usage, revealing that frequent re-prompting can
be an indicator of developer struggles with using Transform Code. Second, we
conduct a qualitative analysis of unsatisfactory requests, identifying five key
categories of information often missing from developer prompts. Finally, based
on these findings, we propose and evaluate a tool, AutoPrompter, for
automatically improving prompts by inferring missing information from the
surrounding code context, leading to a 27% improvement in edit correctness on
our test set.

</details>


### [216] [Automated Unit Test Case Generation: A Systematic Literature Review](https://arxiv.org/abs/2504.20357)
*Jason Wang,Basem Suleiman,Muhammad Johan Alibasa*

Main category: cs.SE

TL;DR: 该论文旨在通过系统文献综述，填补遗传算法和粒子群优化在自动化软件测试中的改进空白，并探讨当前挑战，包括算法组合、突变测试与神经网络的结合。


<details>
  <summary>Details</summary>
Motivation: 自动化软件测试是减少成本和资源消耗的关键，但目前遗传算法和粒子群优化的改进领域以及当前挑战（如可读性、模拟等）存在知识缺口，需要系统化整理。

Method: 采用系统性文献综述方法，整合遗传算法和粒子群优化的现有改进（如混合算法、突变测试与神经网络的结合），并分析测试标准及当前挑战。

Result: 论文总结了进化算法的改进方向（如混合方法）及其局限性，并揭示了自动化测试领域的核心挑战（如可读性和模拟问题）。

Conclusion: 通过系统文献综述，本文为遗传算法和粒子群优化在自动化测试中的进一步研究提供了基础，同时指出了未来需解决的挑战。

Abstract: Software is omnipresent within all factors of society. It is thus important
to ensure that software are well tested to mitigate bad user experiences as
well as the potential for severe financial and human losses. Software testing
is however expensive and absorbs valuable time and resources. As a result, the
field of automated software testing has grown of interest to researchers in
past decades. In our review of present and past research papers, we have
identified an information gap in the areas of improvement for the Genetic
Algorithm and Particle Swarm Optimisation. A gap in knowledge in the current
challenges that face automated testing has also been identified. We therefore
present this systematic literature review in an effort to consolidate existing
knowledge in regards to the evolutionary approaches as well as their
improvements and resulting limitations. These improvements include hybrid
algorithm combinations as well as interoperability with mutation testing and
neural networks. We will also explore the main test criterion that are used in
these algorithms alongside the challenges currently faced in the field related
to readability, mocking and more.

</details>


### [217] [CrashFixer: A crash resolution agent for the Linux kernel](https://arxiv.org/abs/2504.20412)
*Alex Mathai,Chenxi Huang,Suwei Ma,Jihwan Kim,Hailie Mitchell,Aleksandr Nogikh,Petros Maniatis,Franjo Ivančić,Junfeng Yang,Baishakhi Ray*

Main category: cs.SE

TL;DR: 介绍了CrashFixer，首个适用于Linux内核错误的LLM修复工具，通过改进kGym平台和评估修复策略，展示了在复杂系统中修复内核崩溃的有效性。


<details>
  <summary>Details</summary>
Motivation: 为解决Linux内核错误修复中大规模代码处理的挑战，并提升LLM在系统级修复任务中的实用性。

Method: 基于kGym平台改进为kGymSuite，模拟内核开发者工作流，结合LLM生成假设并修复错误。

Result: 成功开发了CrashFixer，并在开放错误中验证了至少两个修复建议的可行性。

Conclusion: CrashFixer在复杂系统修复任务中展现出潜力，改进平台和策略为未来研究提供了基础。

Abstract: Code large language models (LLMs) have shown impressive capabilities on a
multitude of software engineering tasks. In particular, they have demonstrated
remarkable utility in the task of code repair. However, common benchmarks used
to evaluate the performance of code LLMs are often limited to small-scale
settings. In this work, we build upon kGym, which shares a benchmark for
system-level Linux kernel bugs and a platform to run experiments on the Linux
kernel.
  This paper introduces CrashFixer, the first LLM-based software repair agent
that is applicable to Linux kernel bugs. Inspired by the typical workflow of a
kernel developer, we identify the key capabilities an expert developer
leverages to resolve a kernel crash. Using this as our guide, we revisit the
kGym platform and identify key system improvements needed to practically run
LLM-based agents at the scale of the Linux kernel (50K files and 20M lines of
code). We implement these changes by extending kGym to create an improved
platform - called kGymSuite, which will be open-sourced. Finally, the paper
presents an evaluation of various repair strategies for such complex kernel
bugs and showcases the value of explicitly generating a hypothesis before
attempting to fix bugs in complex systems such as the Linux kernel. We also
evaluated CrashFixer's capabilities on still open bugs, and found at least two
patch suggestions considered plausible to resolve the reported bug.

</details>


### [218] [ARCS: Agentic Retrieval-Augmented Code Synthesis with Iterative Refinement](https://arxiv.org/abs/2504.20434)
*Manish Bhattarai,Miguel Cordova,Javier Santos,Dan O'Malley*

Main category: cs.SE

TL;DR: ARCS框架通过结合检索增强生成与思维链推理，优化代码生成、完成与翻译任务，提升超级计算中的代码效率。


<details>
  <summary>Details</summary>
Motivation: 超级计算中需要高效与优化的代码生成，以充分利用高性能系统的资源。

Method: 整合检索增强生成与思维链推理，通过检索相关代码片段和实时执行反馈，以状态-动作搜索树优化平衡代码正确性和编辑效率。

Result: 在Geeks4Geeks和HumanEval基准测试中，ARCS显著优于传统提示方法，翻译与生成质量更高。

Conclusion: ARCS通过可扩展且精确的代码合成，为超级计算应用中的代码开发自动化与优化提供了变革潜力。

Abstract: In supercomputing, efficient and optimized code generation is essential to
leverage high-performance systems effectively. We propose Agentic
Retrieval-Augmented Code Synthesis (ARCS), an advanced framework for accurate,
robust, and efficient code generation, completion, and translation. ARCS
integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (CoT)
reasoning to systematically break down and iteratively refine complex
programming tasks. An agent-based RAG mechanism retrieves relevant code
snippets, while real-time execution feedback drives the synthesis of candidate
solutions. This process is formalized as a state-action search tree
optimization, balancing code correctness with editing efficiency. Evaluations
on the Geeks4Geeks and HumanEval benchmarks demonstrate that ARCS significantly
outperforms traditional prompting methods in translation and generation
quality. By enabling scalable and precise code synthesis, ARCS offers
transformative potential for automating and optimizing code development in
supercomputing applications, enhancing computational resource utilization.

</details>


### [219] [Enhancing Cell Counting through MLOps: A Structured Approach for Automated Cell Analysis](https://arxiv.org/abs/2504.20126)
*Matteo Testi,Luca Clissa,Matteo Ballabio,Salvatore Ricciardi,Federico Baldo,Emanuele Frontoni,Sara Moccia,Gennario Vessio*

Main category: cs.SE

TL;DR: 该论文介绍了CC-MLOps框架，旨在简化机器学习在细胞计数工作流中的集成，通过实际应用案例展示了其提高模型可靠性和可扩展性的能力。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在细胞计数应用中有巨大潜力，但需要强大的操作框架以有效实施。

Method: 提出CC-MLOps框架，涵盖数据访问与预处理、模型训练、监控、解释性功能及可持续性考虑。

Result: 通过实际案例证明，MLOps原则能增强模型可靠性、减少人为错误，并支持可扩展的细胞计数解决方案。

Conclusion: 为研究人员和实验室专业人员提供了实施机器学习驱动的细胞计数系统的实用指导。

Abstract: Machine Learning (ML) models offer significant potential for advancing cell
counting applications in neuroscience, medical research, pharmaceutical
development, and environmental monitoring. However, implementing these models
effectively requires robust operational frameworks. This paper introduces Cell
Counting Machine Learning Operations (CC-MLOps), a comprehensive framework that
streamlines the integration of ML in cell counting workflows. CC-MLOps
encompasses data access and preprocessing, model training, monitoring,
explainability features, and sustainability considerations. Through a practical
use case, we demonstrate how MLOps principles can enhance model reliability,
reduce human error, and enable scalable Cell Counting solutions. This work
provides actionable guidance for researchers and laboratory professionals
seeking to implement machine learning (ML)- powered cell counting systems.

</details>


### [220] [CoCo-Bench: A Comprehensive Code Benchmark For Multi-task Large Language Model Evaluation](https://arxiv.org/abs/2504.20673)
*Wenjing Yin,Tianze Sun,Yijiong Yu,Jiawei Fang,Guangyao Su,Jiancheng Wang,Zekun Wang,Wei Wang,Ran Chen,Ziyun Dai,Shuai Yuan,Menghang Dong,Peng Luo,Dong Cao,Da Lei,Yajun Zhang,Hao Chen,Xiang Ma,Yong Liu,Weifeng Liu,Yuanjian Xu,Ji Pei*

Main category: cs.SE

TL;DR: 该论文提出了CoCo-Bench，一个全面的代码基准测试框架，用于评估大语言模型（LLMs）在代码理解、生成、修改和审查四个关键维度的表现。它弥补了现有基准测试范围狭窄的不足，并通过多编程语言和多任务难度设计，结合人工审核，确保了数据质量。实验结果表明，CoCo-Bench不仅能与现有基准测试保持一致，还能揭示模型性能的显著差异，为未来研究提供了有价值的参考。


<details>
  <summary>Details</summary>
Motivation: 现有的代码相关基准测试往往局限于特定任务，缺乏全面反映实际应用场景的评估框架。为弥补这一不足，作者开发了CoCo-Bench，旨在通过涵盖代码开发的多个关键维度，提供一个更具系统性和代表性的评估工具。

Method: 作者设计了一个名为CoCo-Bench的基准测试框架，包括代码理解、生成、修改和审查四个核心维度。测试覆盖多种编程语言和不同难度的任务，并通过人工审核确保数据质量。

Result: 实验结果表明，CoCo-Bench与现有基准测试结果一致，但更能揭示模型在不同任务上的性能差异，有效识别模型的优势与不足。

Conclusion: CoCo-Bench为代码导向的大语言模型研究提供了一个全面、客观的评估工具，有助于指导未来的技术发展和研究方向。

Abstract: Large language models (LLMs) play a crucial role in software engineering,
excelling in tasks like code generation and maintenance. However, existing
benchmarks are often narrow in scope, focusing on a specific task and lack a
comprehensive evaluation framework that reflects real-world applications. To
address these gaps, we introduce CoCo-Bench (Comprehensive Code Benchmark),
designed to evaluate LLMs across four critical dimensions: code understanding,
code generation, code modification, and code review. These dimensions capture
essential developer needs, ensuring a more systematic and representative
evaluation. CoCo-Bench includes multiple programming languages and varying task
difficulties, with rigorous manual review to ensure data quality and accuracy.
Empirical results show that CoCo-Bench aligns with existing benchmarks while
uncovering significant variations in model performance, effectively
highlighting strengths and weaknesses. By offering a holistic and objective
evaluation, CoCo-Bench provides valuable insights to guide future research and
technological advancements in code-oriented LLMs, establishing a reliable
benchmark for the field.

</details>


### [221] [Using LLMs in Generating Design Rationale for Software Architecture Decisions](https://arxiv.org/abs/2504.20781)
*Xiyu Zhou,Ruiyin Li,Peng Liang,Beiqi Zhang,Mojtaba Shahin,Zengyang Li,Chen Yang*

Main category: cs.SE

TL;DR: 研究评估了大型语言模型（LLMs）在生成软件架构决策设计理由（DR）中的表现，发现虽然其生成的DR在准确率和召回率上表现一般，但多数未提及的论点仍有帮助。


<details>
  <summary>Details</summary>
Motivation: 由于开发者缺乏动力和努力，软件架构决策的设计理由（DR）常未被充分记录。LLMs在文本理解和生成上的潜力可能帮助自动生成和恢复DR。

Method: 研究收集了100个架构相关问题，使用五种LLMs通过零样本、思维链（CoT）和LLM代理三种策略生成DR，并与人类专家的DR进行对比评估。

Result: LLM生成的DR在精确率（0.267-0.278）、召回率（0.627-0.715）和F1分数（0.351-0.389）上表现一般，但64.45%-69.42%的未提及论点是有帮助的。

Conclusion: 研究探讨了三种提示策略的优缺点及LLM生成DR的潜力与局限，表明LLMs虽能补充DR，但质量仍需提升。

Abstract: Design Rationale (DR) for software architecture decisions refers to the
reasoning underlying architectural choices, which provides valuable insights
into the different phases of the architecting process throughout software
development. However, in practice, DR is often inadequately documented due to a
lack of motivation and effort from developers. With the recent advancements in
Large Language Models (LLMs), their capabilities in text comprehension,
reasoning, and generation may enable the generation and recovery of DR for
architecture decisions. In this study, we evaluated the performance of LLMs in
generating DR for architecture decisions. First, we collected 50 Stack Overflow
(SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture
decisions to construct a dataset of 100 architecture-related problems. Then, we
selected five LLMs to generate DR for the architecture decisions with three
prompting strategies, including zero-shot, chain of thought (CoT), and
LLM-based agents. With the DR provided by human experts as ground truth, the
Precision of LLM-generated DR with the three prompting strategies ranges from
0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389.
Additionally, 64.45% to 69.42% of the arguments of DR not mentioned by human
experts are also helpful, 4.12% to 4.87% of the arguments have uncertain
correctness, and 1.59% to 3.24% of the arguments are potentially misleading.
Based on the results, we further discussed the pros and cons of the three
prompting strategies and the strengths and limitations of the DR generated by
LLMs.

</details>


### [222] [Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation, and Challenges](https://arxiv.org/abs/2504.20799)
*Yunseo Lee,John Youngeun Song,Dongsun Kim,Jindae Kim,Mijung Kim,Jaechang Nam*

Main category: cs.SE

TL;DR: 该论文综述了CodeLLMs生成的代码中幻觉的类型、现有基准与缓解策略，并指出开放挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: CodeLLMs在生成代码时易产生难以识别的幻觉，可能潜藏隐患，需系统性研究其类型与解决方案。

Method: 通过文献回顾，分类CodeLLMs的幻觉类型，总结检测与缓解方法的基准及技术。

Result: 归纳了幻觉的常见类型、现有策略的局限性，并提出未来研究需解决的关键问题。

Conclusion: 需进一步开发高效检测与修复CodeLLMs幻觉的方法，以提升生成代码的可靠性。

Abstract: Recent technical breakthroughs in large language models (LLMs) have enabled
them to fluently generate source code. Software developers often leverage both
general-purpose and code-specialized LLMs to revise existing code or even
generate a whole function from scratch. These capabilities are also beneficial
in no-code or low-code contexts, in which one can write programs without a
technical background. However, due to their internal design, LLMs are prone to
generating hallucinations, which are incorrect, nonsensical, and not
justifiable information but difficult to identify its presence. This problem
also occurs when generating source code. Once hallucinated code is produced, it
is often challenging for users to identify and fix it, especially when such
hallucinations can be identified under specific execution paths. As a result,
the hallucinated code may remain unnoticed within the codebase. This survey
investigates recent studies and techniques relevant to hallucinations generated
by CodeLLMs. We categorize the types of hallucinations in the code generated by
CodeLLMs, review existing benchmarks and mitigation strategies, and identify
open challenges. Based on these findings, this survey outlines further research
directions in the detection and removal of hallucinations produced by CodeLLMs.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [223] [Exploiting inter-agent coupling information for efficient reinforcement learning of cooperative LQR](https://arxiv.org/abs/2504.20927)
*Shahbaz P Qadri Syed,He Bai*

Main category: eess.SY

TL;DR: 本文提出了一种基于智能体间耦合信息的系统性方法来精确分解每个智能体的局部Q函数，并通过近似最小二乘策略迭代算法实现，证明了其样本复杂度与集中式方法相当，且在特定耦合条件下更高效。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体协同控制中可扩展且高效的强化学习算法问题，现有方法基于经验信息结构进行局部Q函数的不精确分解，本文旨在利用智能体间耦合信息实现精确分解。

Method: 提出了一个系统性方法精确分解局部Q函数，并开发了基于分解的近似最小二乘策略迭代算法，设计了两种架构学习局部Q函数。

Result: 证明了分解的样本复杂度与集中式方法相同，并推导了智能体间耦合的图条件以实现更高样本效率，数值实验验证了更高的样本和计算效率。

Conclusion: 所提出的方法在多智能体协同控制中实现了更高的样本和计算效率，并通过理论分析和实验验证了其有效性。

Abstract: Developing scalable and efficient reinforcement learning algorithms for
cooperative multi-agent control has received significant attention over the
past years. Existing literature has proposed inexact decompositions of local
Q-functions based on empirical information structures between the agents. In
this paper, we exploit inter-agent coupling information and propose a
systematic approach to exactly decompose the local Q-function of each agent. We
develop an approximate least square policy iteration algorithm based on the
proposed decomposition and identify two architectures to learn the local
Q-function for each agent. We establish that the worst-case sample complexity
of the decomposition is equal to the centralized case and derive necessary and
sufficient graphical conditions on the inter-agent couplings to achieve better
sample efficiency. We demonstrate the improved sample efficiency and
computational efficiency on numerical examples.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [224] [Guessing Efficiently for Constrained Subspace Approximation](https://arxiv.org/abs/2504.20883)
*Aditya Bhaskara,Sepideh Mahabadi,Madhusudhan Reddy Pittu,Ali Vakilian,David P. Woodruff*

Main category: cs.DS

TL;DR: 本文研究了约束子空间近似问题，提出了一个基于核心集-猜测-解决的通用框架，针对多种约束条件提供了$(1+\varepsilon)$-乘性或$\varepsilon$-加性近似解，改进了包括公平子空间近似、k均值聚类和非负矩阵分解等多个问题的现有成果。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是解决子空间近似问题中的约束条件，尤其是在实际应用中，投影矩阵可能需要满足特定条件（如公平性或非负性）。通过引入通用框架，可以更灵活地处理这些约束。

Method: 提出了“核心集-猜测-解决”框架。核心集用于数据降维，猜测阶段生成候选子空间，解决阶段优化目标函数。该框架适用于多种约束类型，如显式或隐式描述的集合$\mathcal{S}$。

Result: 该框架不仅重构了k均值聚类在欧式空间中的已知最优界，还显著改进了公平子空间近似和投影非负矩阵分解等问题的现有结果。

Conclusion: 研究表明，提出的通用框架能高效处理多种约束子空间近似问题，尤其是对实际应用中常见约束条件的算法优化具有重要价值。

Abstract: In this paper we study constrained subspace approximation problem. Given a
set of $n$ points $\{a_1,\ldots,a_n\}$ in $\mathbb{R}^d$, the goal of the {\em
subspace approximation} problem is to find a $k$ dimensional subspace that best
approximates the input points. More precisely, for a given $p\geq 1$, we aim to
minimize the $p$th power of the $\ell_p$ norm of the error vector
$(\|a_1-\bm{P}a_1\|,\ldots,\|a_n-\bm{P}a_n\|)$, where $\bm{P}$ denotes the
projection matrix onto the subspace and the norms are Euclidean. In
\emph{constrained} subspace approximation (CSA), we additionally have
constraints on the projection matrix $\bm{P}$. In its most general form, we
require $\bm{P}$ to belong to a given subset $\mathcal{S}$ that is described
explicitly or implicitly.
  We introduce a general framework for constrained subspace approximation. Our
approach, that we term coreset-guess-solve, yields either
$(1+\varepsilon)$-multiplicative or $\varepsilon$-additive approximations for a
variety of constraints. We show that it provides new algorithms for
partition-constrained subspace approximation with applications to {\it fair}
subspace approximation, $k$-means clustering, and projected non-negative matrix
factorization, among others. Specifically, while we reconstruct the best known
bounds for $k$-means clustering in Euclidean spaces, we improve the known
results for the remainder of the problems.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [225] [CarbonCall: Sustainability-Aware Function Calling for Large Language Models on Edge Devices](https://arxiv.org/abs/2504.20348)
*Varatheepan Paramanayakam,Andreas Karatzas,Iraklis Anagnostopoulos,Dimitrios Stamoulis*

Main category: cs.PF

TL;DR: CarbonCall是一个可持续性感知的函数调用框架，通过动态工具选择、碳感知执行和量化LLM调整，显著降低碳排放和功耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法在优化性能时忽视了可持续性，导致高能耗和碳排放，不适合能源受限环境。CarbonCall旨在解决这一问题。

Method: 集成了动态工具选择、碳感知执行和量化LLM适应，根据实时碳强度预测调整功耗阈值，并在模型变体间切换以保持高效。

Result: 在NVIDIA Jetson AGX Orin上的实验表明，CarbonCall减少碳排放高达52%，功耗降低30%，执行时间缩短30%。

Conclusion: CarbonCall在保持高效率的同时，显著提升了可持续性，适用于能源受限的边缘AI系统。

Abstract: Large Language Models (LLMs) enable real-time function calling in edge AI
systems but introduce significant computational overhead, leading to high power
consumption and carbon emissions. Existing methods optimize for performance
while neglecting sustainability, making them inefficient for energy-constrained
environments. We introduce CarbonCall, a sustainability-aware function-calling
framework that integrates dynamic tool selection, carbon-aware execution, and
quantized LLM adaptation. CarbonCall adjusts power thresholds based on
real-time carbon intensity forecasts and switches between model variants to
sustain high tokens-per-second throughput under power constraints. Experiments
on an NVIDIA Jetson AGX Orin show that CarbonCall reduces carbon emissions by
up to 52%, power consumption by 30%, and execution time by 30%, while
maintaining high efficiency.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [226] [Narrative-Centered Emotional Reflection: Scaffolding Autonomous Emotional Literacy with AI](https://arxiv.org/abs/2504.20342)
*Shou-Tzu Han*

Main category: cs.HC

TL;DR: Reflexion是一个AI平台，通过实时情绪检测和多层反思提示，帮助用户进行深层次的情感自我反思，初步研究显示其在情绪表达和心理韧性方面有积极效果。


<details>
  <summary>Details</summary>
Motivation: 该平台的动机是通过AI技术，结合心理学理论，帮助用户超越基础情感分类，实现更深层次的情感探索和自我成长。

Method: 采用了实时情绪检测、分层反思提示和隐喻故事生成等方法，并结合了表达性写作、认知重构等心理学理论。

Result: 初步试验显示，用户在情绪表达、认知重构和心理韧性方面表现出积极变化。

Conclusion: Reflexion为可扩展的情感计算干预提供了有前景的方向，适用于教育、治疗和公共健康领域。

Abstract: Reflexion is an AI-powered platform designed to enable structured emotional
self-reflection at scale. By integrating real-time emotion detection, layered
reflective prompting, and metaphorical storytelling generation, Reflexion
empowers users to engage in autonomous emotional exploration beyond basic
sentiment categorization. Grounded in theories of expressive writing, cognitive
restructuring, self-determination, and critical consciousness development, the
system scaffolds a progressive journey from surface-level emotional recognition
toward value-aligned action planning. Initial pilot studies with diverse
participants demonstrate positive outcomes in emotional articulation, cognitive
reframing, and perceived psychological resilience. Reflexion represents a
promising direction for scalable, theory-informed affective computing
interventions aimed at fostering emotional literacy and psychological growth
across educational, therapeutic, and public health contexts.

</details>


### [227] [In defence of post-hoc explanations in medical AI](https://arxiv.org/abs/2504.20741)
*Joshua Hatherley,Lauritz Munch,Jens Christian Bjerring*

Main category: cs.HC

TL;DR: 尽管后解释不能完全复制黑盒系统的推理过程，但它们在增强用户对医疗AI的功能理解、提升临床-AI团队准确性以及支持临床决策方面仍有价值。


<details>
  <summary>Details</summary>
Motivation: 针对后解释在医疗AI中的价值受到质疑，本文旨在辩护其实际作用，强调即使不完美，后解释仍能解决黑盒系统的部分问题。

Method: 通过逻辑论证和现有研究成果分析，反驳后解释无用的批评，并阐述其在医疗场景中的具体益处。

Result: 后解释虽然无法完全复制黑盒系统的推理逻辑，但能提升用户功能理解、团队准确性和决策合理性。

Conclusion: 后解释并非解决医疗AI黑盒问题的万能方案，但仍是一种有效的策略。

Abstract: Since the early days of the Explainable AI movement, post-hoc explanations
have been praised for their potential to improve user understanding, promote
trust, and reduce patient safety risks in black box medical AI systems.
Recently, however, critics have argued that the benefits of post-hoc
explanations are greatly exaggerated since they merely approximate, rather than
replicate, the actual reasoning processes that black box systems take to arrive
at their outputs. In this article, we aim to defend the value of post-hoc
explanations against this recent critique. We argue that even if post-hoc
explanations do not replicate the exact reasoning processes of black box
systems, they can still improve users' functional understanding of black box
systems, increase the accuracy of clinician-AI teams, and assist clinicians in
justifying their AI-informed decisions. While post-hoc explanations are not a
"silver bullet" solution to the black box problem in medical AI, we conclude
that they remain a useful strategy for addressing the black box problem in
medical AI.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [228] [On Stochastic Rounding with Few Random Bits](https://arxiv.org/abs/2504.20634)
*Andrew Fitzgibbon,Stephen Felix*

Main category: math.NA

TL;DR: 该论文研究了低精度浮点运算中的随机舍入（SR）技术，特别是探讨了使用少量随机比特的随机舍入（FBSR）实现方式及其可能引入的偏差，并通过机器学习示例展示了这些偏差的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大规模数值计算对低精度浮点格式和混合精度算术的需求增加，随机舍入技术（SR）成为提升性能的关键。然而，高质量随机比特的生成成本较高，因此研究如何在保证SR优点的同时减少所需比特数具有重要意义。

Method: 论文分析了几种可能的少量随机比特随机舍入（FBSR）实现方式，并与无限比特、无限精度的情况进行对比，评估了这些实现可能引入的偏差。

Result: 研究发现某些自然实现的FBSR会引入显著偏差，而这些偏差在无限比特和无限精度的情况下并不存在。此外，论文通过机器学习示例展示了这些偏差的实际影响。

Conclusion: 该研究提醒开发者在采用或开发低精度浮点运算时，需注意随机舍入实现中的配置参数可能带来的偏差，并提供了相关代码供参考。

Abstract: Large-scale numerical computations make increasing use of low-precision (LP)
floating point formats and mixed precision arithmetic, which can be enhanced by
the technique of stochastic rounding (SR), that is, rounding an intermediate
high-precision value up or down randomly as a function of the value's distance
to the two rounding candidates. Stochastic rounding requires, in addition to
the high-precision input value, a source of random bits. As the provision of
high-quality random bits is an additional computational cost, it is of interest
to require as few bits as possible while maintaining the desirable properties
of SR in a given computation, or computational domain. This paper examines a
number of possible implementations of few-bit stochastic rounding (FBSR), and
shows how several natural implementations can introduce sometimes significant
bias into the rounding process, which are not present in the case of
infinite-bit, infinite-precision examinations of these implementations. The
paper explores the impact of these biases in machine learning examples, and
hence opens another class of configuration parameters of which practitioners
should be aware when developing or adopting low-precision floating point. Code
is available at
http://github.com/graphcore-research/arith25-stochastic-rounding.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [229] [Learning Hierarchical Interaction for Accurate Molecular Property Prediction](https://arxiv.org/abs/2504.20127)
*Huiyang Hong,Xinkai Wu,Hongyu Sun,Qi Wang,Yuquan Li*

Main category: q-bio.BM

TL;DR: 论文提出了HimNet模型，通过分层交互消息传递机制，改进分子属性预测，尤其在ADMET属性上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型（如GNN和Transformer）在捕捉分子结构层次性和多级特征交互上不足，影响预测效果。

Method: 提出分层交互消息传递机制（Hierarchical Interaction Message Passing），结合原子、基序和分子层面的注意力引导交互。

Result: HimNet在多个基准数据集上表现最佳或接近最佳，尤其在BBB渗透性预测中效果显著，并具备良好的可解释性。

Conclusion: HimNet为早期药物发现提供了高效准确的分子活性预测方法，具有重要应用价值。

Abstract: Discovering molecules with desirable molecular properties, including ADMET
(Absorption, Distribution, Metabolism, Excretion, and Toxicity) profiles, is of
great importance in drug discovery. Existing approaches typically employ deep
learning models, such as Graph Neural Networks (GNNs) and Transformers, to
predict these molecular properties by learning from diverse chemical
information. However, these models often fail to efficiently capture and
utilize the hierarchical nature of molecular structures, and lack mechanisms
for effective interaction among multi-level features. To address these
limitations, we propose a Hierarchical Interaction Message Passing Mechanism,
which serves as the foundation of our novel model, HimNet. Our method enables
interaction-aware representation learning across atomic, motif, and molecular
levels via hierarchical attention-guided message passing. This design allows
HimNet to effectively balance global and local information, ensuring rich and
task-relevant feature extraction for downstream property prediction tasks, such
as Blood-Brain Barrier Permeability (BBBP). Extensive experiments on multiple
benchmark datasets demonstrate that HimNet achieves the best or near-best
performance in most molecular property prediction tasks. Furthermore, our
method exhibits promising hierarchical interpretability, aligning well with
chemical intuition on representative molecules. We believe that HimNet offers
an accurate and efficient solution for molecular activity and ADMET property
prediction, contributing significantly to advanced decision-making in the early
stages of drug discovery.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [230] [Predictive AI with External Knowledge Infusion for Stocks](https://arxiv.org/abs/2504.20058)
*Ambedkar Dukkipati,Kawin Mayilvaghanan,Naveen Kumar Pallekonda,Sai Prakash Hadnoor,Ranga Shaarad Ayyagari*

Main category: q-fin.ST

TL;DR: 该论文提出了一种结合历史趋势和外部知识图谱的机制，用于在外部影响下预测股价波动，并构建了相应的时态知识图谱数据集。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于股票价格波动受多种动态外部因素影响，传统方法仅依赖历史数据，而忽略了这些复杂的外部因素。

Method: 方法上，论文通过构建时态知识图谱数据集，并利用Hawkes过程建模外部知识图谱中的关系，以动态表示学习股价波动。

Result: 实验结果表明，基于动态表示的方法在多个持有期内对股票收益的排名优于基线模型。

Conclusion: 结论是动态外部因素的建模能有效提升股价预测的准确性。

Abstract: Fluctuations in stock prices are influenced by a complex interplay of factors
that go beyond mere historical data. These factors, themselves influenced by
external forces, encompass inter-stock dynamics, broader economic factors,
various government policy decisions, outbreaks of wars, etc. Furthermore, all
of these factors are dynamic and exhibit changes over time. In this paper, for
the first time, we tackle the forecasting problem under external influence by
proposing learning mechanisms that not only learn from historical trends but
also incorporate external knowledge from temporal knowledge graphs. Since there
are no such datasets or temporal knowledge graphs available, we study this
problem with stock market data, and we construct comprehensive temporal
knowledge graph datasets. In our proposed approach, we model relations on
external temporal knowledge graphs as events of a Hawkes process on graphs.
With extensive experiments, we show that learned dynamic representations
effectively rank stocks based on returns across multiple holding periods,
outperforming related baselines on relevant metrics.

</details>


### [231] [Deep Learning vs. Black-Scholes: Option Pricing Performance on Brazilian Petrobras Stocks](https://arxiv.org/abs/2504.20088)
*Joao Felipe Gueiros,Hemanth Chandravamsi,Steven H. Frankel*

Main category: q-fin.ST

TL;DR: 论文探讨了深度残差网络在Petrobras欧式期权定价中的应用，并与Black-Scholes模型对比，结果显示深度学习模型在特定价格范围内误差显著降低且对长期有效期更准确。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证深度学习模型在金融定价中的有效性，尤其是针对传统Black-Scholes模型在长期预测中的不足。

Method: 使用B3交易所8年历史数据，通过网页抓取获取，构建混合损失函数训练深度残差网络，采用80-20训练测试比。

Result: 深度学习模型在3-19巴西雷亚尔范围内比Black-Scholes模型均绝对误差减少64.3%，且在长期有效期表现更稳定。

Conclusion: 深度学习在金融建模中具潜力，未来可针对不同价格范围开发专项模型。

Abstract: This paper explores the use of deep residual networks for pricing European
options on Petrobras, one of the world's largest oil and gas producers, and
compares its performance with the Black-Scholes (BS) model. Using eight years
of historical data from B3 (Brazilian Stock Exchange) collected via web
scraping, a deep learning model was trained using a custom built hybrid loss
function that incorporates market data and analytical pricing. The data for
training and testing were drawn between the period spanning November 2016 to
January 2025, using an 80-20 train-test split. The test set consisted of data
from the final three months: November, December, and January 2025. The deep
residual network model achieved a 64.3\% reduction in the mean absolute error
for the 3-19 BRL (Brazilian Real) range when compared to the Black-Scholes
model on the test set. Furthermore, unlike the Black-Scholes solution, which
tends to decrease its accuracy for longer periods of time, the deep learning
model performed accurately for longer expiration periods. These findings
highlight the potential of deep learning in financial modeling, with future
work focusing on specialized models for different price ranges.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [232] [Optimizing Hard Thresholding for Sparse Model Discovery](https://arxiv.org/abs/2504.20256)
*Derek W. Jollie,Scott G. McCalla*

Main category: math.OC

TL;DR: 该论文提出了一种通过引入退火方案来改进稀疏字典学习算法性能的方法，重点研究了SINDy和硬阈值追踪两种优化方法，并在多个非线性系统中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决稀疏字典学习算法中硬阈值过程可能导致模型精度不足的问题，论文探索了一种退火方案，通过逐步重新激活部分被移除的项来提高模型性能。

Method: 论文引入了退火方案，结合冷却计划重新激活部分被移除的项，并在SINDy和硬阈值追踪两种优化方法上进行了验证。

Result: 实验结果表明，退火方案显著提高了算法的模型精度，尤其在非线性系统（如对流流动、可激发系统和群体动力学）中表现突出。

Conclusion: 退火方案是一种有效的改进稀疏字典学习算法的方法，尤其在处理复杂非线性系统时表现优异，且适用于实验数据如抛体运动。

Abstract: Many model selection algorithms rely on sparse dictionary learning to provide
interpretable and physics-based governing equations. The optimization
algorithms typically use a hard thresholding process to enforce sparse
activations in the model coefficients by removing library elements from
consideration. By introducing an annealing scheme that reactivates a fraction
of the removed terms with a cooling schedule, we are able to improve the
performance of these sparse learning algorithms. We concentrate on two
approaches to the optimization, SINDy, and an alternative using hard
thresholding pursuit. We see in both cases that annealing can improve model
accuracy. The effectiveness of annealing is demonstrated through comparisons on
several nonlinear systems pulled from convective flows, excitable systems, and
population dynamics. Finally we apply these algorithms to experimental data for
projectile motion.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [233] [Nonlinear Computation with Linear Optics via Source-Position Encoding](https://arxiv.org/abs/2504.20401)
*N. Richardson,C. Bosch,R. P. Adams*

Main category: physics.optics

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Optical computing systems provide an alternate hardware model which appears
to be aligned with the demands of neural network workloads. However, the
challenge of implementing energy efficient nonlinearities in optics -- a key
requirement for realizing neural networks -- is a conspicuous missing link. In
this work we introduce a novel method to achieve nonlinear computation in fully
linear media. Our method can operate at low power and requires only the ability
to drive the optical system at a data-dependent spatial position. Leveraging
this positional encoding, we formulate a fully automated,
topology-optimization-based hardware design framework for extremely specialized
optical neural networks, drawing on modern advancements in optimization and
machine learning. We evaluate our optical designs on machine learning
classification tasks: demonstrating significant improvements over linear
methods, and competitive performance when compared to standard artificial
neural networks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [234] [Smart Water Security with AI and Blockchain-Enhanced Digital Twins](https://arxiv.org/abs/2504.20275)
*Mohammadhossein Homaei,Victor Gonzalez Morales,Oscar Mogollon Gutierrez,Ruben Molano Gomez,Andres Caro*

Main category: cs.CR

TL;DR: 论文提出了一种结合LoRaWAN、机器学习驱动的入侵检测系统(IDS)和区块链数字孪生(BC-DT)平台的集成框架，用于农村地区安全透明的供水管理。实验证明，该系统吞吐量超过80 TPS，延迟低于2秒，且适用于最多1000个智能水表。


<details>
  <summary>Details</summary>
Motivation: 农村供水系统面临实时监测不足、网络攻击威胁和数据可靠性问题，亟需一种安全、高效的解决方案。

Method: 采用LoRaWAN采集数据，利用LSTM自编码器和孤立森林算法构建IDS过滤异常数据，并通过私有以太坊区块链的智能合约记录已验证数据，再通过数字孪生模型实现实时监测与预测。

Result: 系统实现了80+ TPS的吞吐量和2秒以内的延迟，成本效益高且支持扩展到1000个智能水表。

Conclusion: 该研究为连接不足的农村环境提供了一种实用且安全的去中心化供水管理架构。

Abstract: Water distribution systems in rural areas face serious challenges such as a
lack of real-time monitoring, vulnerability to cyberattacks, and unreliable
data handling. This paper presents an integrated framework that combines
LoRaWAN-based data acquisition, a machine learning-driven Intrusion Detection
System (IDS), and a blockchain-enabled Digital Twin (BC-DT) platform for secure
and transparent water management. The IDS filters anomalous or spoofed data
using a Long Short-Term Memory (LSTM) Autoencoder and Isolation Forest before
validated data is logged via smart contracts on a private Ethereum blockchain
using Proof of Authority (PoA) consensus. The verified data feeds into a
real-time DT model supporting leak detection, consumption forecasting, and
predictive maintenance. Experimental results demonstrate that the system
achieves over 80 transactions per second (TPS) with under 2 seconds of latency
while remaining cost-effective and scalable for up to 1,000 smart meters. This
work demonstrates a practical and secure architecture for decentralized water
infrastructure in under-connected rural environments.

</details>


### [235] [Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression](https://arxiv.org/abs/2504.20493)
*Yu Cui,Yujun Cai,Yiwei Wang*

Main category: cs.CR

TL;DR: 论文提出了一种名为“推理中断攻击”的新型提示注入攻击，通过自适应令牌压缩显著降低了触发DeepSeek-R1模型“思考停止”漏洞的令牌成本，并证明了即使是简单的算术任务也能有效触发此漏洞。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）尽管在多种任务上表现出色，但存在显着的安全漏洞。研究旨在降低触发“思考停止”漏洞的复杂性和令牌成本，并正式定义该漏洞。

Method: 提出基于自适应令牌压缩的“推理中断攻击”，通过简单算术任务触发漏洞，并开发了系统化的方法收集攻击提示和压缩框架。

Result: 实验显示压缩框架显著减少提示长度且保持攻击有效性，同时分析了漏洞的底层原因。

Conclusion: 研究为提升推理LLMs的安全性提供了宝贵见解，揭示了简单任务也能触发严重漏洞，需加强模型防御。

Abstract: While reasoning large language models (LLMs) demonstrate remarkable
performance across various tasks, they also contain notable security
vulnerabilities. Recent research has uncovered a "thinking-stopped"
vulnerability in DeepSeek-R1, where model-generated reasoning tokens can
forcibly interrupt the inference process, resulting in empty responses that
compromise LLM-integrated applications. However, existing methods triggering
this vulnerability require complex mathematical word problems with long
prompts--even exceeding 5,000 tokens. To reduce the token cost and formally
define this vulnerability, we propose a novel prompt injection attack named
"Reasoning Interruption Attack", based on adaptive token compression. We
demonstrate that simple standalone arithmetic tasks can effectively trigger
this vulnerability, and the prompts based on such tasks exhibit simpler logical
structures than mathematical word problems. We develop a systematic approach to
efficiently collect attack prompts and an adaptive token compression framework
that utilizes LLMs to automatically compress these prompts. Experiments show
our compression framework significantly reduces prompt length while maintaining
effective attack capabilities. We further investigate the attack's performance
via output prefix and analyze the underlying causes of the vulnerability,
providing valuable insights for improving security in reasoning LLMs.

</details>


### [236] [The Hidden Risks of LLM-Generated Web Application Code: A Security-Centric Evaluation of Code Generation Capabilities in Large Language Models](https://arxiv.org/abs/2504.20612)
*Swaroop Dora,Deven Lunkad,Naziya Aslam,S. Venkatesan,Sandeep Kumar Shukla*

Main category: cs.CR

TL;DR: 论文研究了大型语言模型（LLMs）生成的代码在安全性方面的合规性，发现尽管LLMs能提升开发效率，但其生成的代码存在严重安全漏洞，因此需要人工审核和更强大的安全评估框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于LLMs在软件开发中的广泛应用，但其生成的代码在安全性方面存在隐患，可能对实际应用造成重大风险，因此需要系统性评估其安全性。

Method: 方法是通过预定义的安全参数（如认证机制、会话管理、输入验证等）对多种LLM（如ChatGPT、DeepSeek等）生成的代码进行安全合规性评估。

Result: 结果显示所有模型的生成代码均存在关键漏洞，尤其在认证机制和HTTP安全标头方面。部分模型虽有一些安全措施，但均未完全符合行业最佳实践。

Conclusion: 结论指出LLM生成的代码需人工干预以确保安全性，同时呼吁建立更完善的安全评估框架以提高其实际应用的可靠性。

Abstract: The rapid advancement of Large Language Models (LLMs) has enhanced software
development processes, minimizing the time and effort required for coding and
enhancing developer productivity. However, despite their potential benefits,
code generated by LLMs has been shown to generate insecure code in controlled
environments, raising critical concerns about their reliability and security in
real-world applications. This paper uses predefined security parameters to
evaluate the security compliance of LLM-generated code across multiple models,
such as ChatGPT, DeepSeek, Claude, Gemini and Grok. The analysis reveals
critical vulnerabilities in authentication mechanisms, session management,
input validation and HTTP security headers. Although some models implement
security measures to a limited extent, none fully align with industry best
practices, highlighting the associated risks in automated software development.
Our findings underscore that human expertise is crucial to ensure secure
software deployment or review of LLM-generated code. Also, there is a need for
robust security assessment frameworks to enhance the reliability of
LLM-generated code in real-world applications.

</details>


### [237] [A Virtual Cybersecurity Department for Securing Digital Twins in Water Distribution Systems](https://arxiv.org/abs/2504.20266)
*Mohammadhossein Homaei,Agustin Di Bartolo,Oscar Mogollon-Gutierrez,Fernando Broncano Morgado,Pablo Garcia Rodriguez*

Main category: cs.CR

TL;DR: 本文提出了一种面向中小企业的虚拟网络安全部门（VCD）框架，利用开源工具和机器学习模型，低成本且高效地保护水务系统中的数字孪生免受网络攻击。


<details>
  <summary>Details</summary>
Motivation: 中小企业在管理水务系统时缺乏预算和人力构建强大的网络安全团队，数字孪生的互联性又使其易受网络攻击，亟需一种经济实用的解决方案。

Method: 采用开源工具（Zabbix、Suricata、Fail2Ban）和基于改进集成模型的机器学习IDS，结合OD-IDS2022数据集训练，检测暴力破解、远程代码执行等威胁。

Result: 模型检测网络威胁的准确率达到92%，误报率较低，为中小企业提供了低成本、易管理的安全方案。

Conclusion: VCD框架为中小企业提供了一种经济高效的网络安全解决方案，有效保护水务系统数字孪生，同时易于部署和管理。

Abstract: Digital twins (DTs) help improve real-time monitoring and decision-making in
water distribution systems. However, their connectivity makes them easy targets
for cyberattacks such as scanning, denial-of-service (DoS), and unauthorized
access. Small and medium-sized enterprises (SMEs) that manage these systems
often do not have enough budget or staff to build strong cybersecurity teams.
To solve this problem, we present a Virtual Cybersecurity Department (VCD), an
affordable and automated framework designed for SMEs. The VCD uses open-source
tools like Zabbix for real-time monitoring, Suricata for network intrusion
detection, Fail2Ban to block repeated login attempts, and simple firewall
settings. To improve threat detection, we also add a machine-learning-based IDS
trained on the OD-IDS2022 dataset using an improved ensemble model. This model
detects cyber threats such as brute-force attacks, remote code execution (RCE),
and network flooding, with 92\% accuracy and fewer false alarms. Our solution
gives SMEs a practical and efficient way to secure water systems using low-cost
and easy-to-manage tools.

</details>


### [238] [Enhancing Vulnerability Reports with Automated and Augmented Description Summarization](https://arxiv.org/abs/2504.20726)
*Hattan Althebeiti,Mohammed Alkinoon,Manar Mohaisen,Saeed Salem,DaeHun Nyang,David Mohaisen*

Main category: cs.CR

TL;DR: 本文介绍了Zad系统，用于丰富NVD漏洞描述，通过外部资源补充信息，解决描述简短和内容不足的问题。Zad由两条流水线构成：一条用于收集和过滤数据，另一条用于生成更全面的漏洞描述。经评估，Zad在提升漏洞信息质量方面表现有效。


<details>
  <summary>Details</summary>
Motivation: 公共漏洞数据库（如NVD）的漏洞描述通常简短且信息不足，限制了威胁情报的共享效果。

Method: Zad系统包含两条流水线：一条收集并过滤外部数据构建详细数据集，另一条基于预训练模型生成丰富描述。

Result: 通过标准摘要指标和人工评估，Zad能生成更全面、连贯的漏洞描述。

Conclusion: Zad成功解决了漏洞描述简短和信息不足的问题，提升了漏洞信息的质量和实用性。

Abstract: Public vulnerability databases, such as the National Vulnerability Database
(NVD), document vulnerabilities and facilitate threat information sharing.
However, they often suffer from short descriptions and outdated or insufficient
information. In this paper, we introduce Zad, a system designed to enrich NVD
vulnerability descriptions by leveraging external resources. Zad consists of
two pipelines: one collects and filters supplementary data using two encoders
to build a detailed dataset, while the other fine-tunes a pre-trained model on
this dataset to generate enriched descriptions. By addressing brevity and
improving content quality, Zad produces more comprehensive and cohesive
vulnerability descriptions. We evaluate Zad using standard summarization
metrics and human assessments, demonstrating its effectiveness in enhancing
vulnerability information.

</details>


### [239] [Dual Explanations via Subgraph Matching for Malware Detection](https://arxiv.org/abs/2504.20904)
*Hossein Shokouhinejad,Roozbeh Razavi-Far,Griffin Higgins,Ali A. Ghorbani*

Main category: cs.CR

TL;DR: 该论文提出了一种新颖的双原型驱动可解释框架，用于提升基于图神经网络的恶意软件检测的可解释性，结合基础解释器和子图匹配技术，显著提高了分析的清晰度和实用性。


<details>
  <summary>Details</summary>
Motivation: 传统的图神经网络解释方法虽然能识别图中的重要区域，但无法将其与已知的良性或恶意行为模式关联，降低了在安全场景中的实用性。因此，需要一种能提供行为对齐解释的方法。

Method: 通过结合基础解释器和一个基于子图匹配技术的新型二级解释器（SubMatch解释器），构建了双原型驱动框架，为节点分配可解释分数以区分良性与恶意区域。

Result: 实验结果表明，该方法在保持高检测性能的同时，显著提升了恶意软件分析的可解释性。

Conclusion: 该框架通过原型引导的评分机制，为恶意软件检测提供了更清晰、行为对齐的解释，增强了自动化安全系统的可信度。

Abstract: Interpretable malware detection is crucial for understanding harmful
behaviors and building trust in automated security systems. Traditional
explainable methods for Graph Neural Networks (GNNs) often highlight important
regions within a graph but fail to associate them with known benign or
malicious behavioral patterns. This limitation reduces their utility in
security contexts, where alignment with verified prototypes is essential. In
this work, we introduce a novel dual prototype-driven explainable framework
that interprets GNN-based malware detection decisions. This dual explainable
framework integrates a base explainer (a state-of-the-art explainer) with a
novel second-level explainer which is designed by subgraph matching technique,
called SubMatch explainer. The proposed explainer assigns interpretable scores
to nodes based on their association with matched subgraphs, offering a
fine-grained distinction between benign and malicious regions. This
prototype-guided scoring mechanism enables more interpretable, behavior-aligned
explanations. Experimental results demonstrate that our method preserves high
detection performance while significantly improving interpretability in malware
analysis.

</details>


### [240] [GiBy: A Giant-Step Baby-Step Classifier For Anomaly Detection In Industrial Control Systems](https://arxiv.org/abs/2504.20906)
*Sarad Venugopalan,Sridhar Adepu*

Main category: cs.CR

TL;DR: 该论文提出了一种通过线性化非线性传感器-执行器关系来降低计算复杂度的异常检测方法，并在水处理测试平台上验证了其快速且可解释的检测能力。


<details>
  <summary>Details</summary>
Motivation: 为确保工业控制系统（ICS）的安全运行，需要实时监控其组件的交互，以检测异常（如攻击或故障）。现有方法在速度和可解释性上存在不足，需改进。

Method: 将传感器-执行器关系的非线性形式精确线性化，降低时间复杂度，并通过降维简化问题。以水处理测试平台为案例进行验证。

Result: 实验显示，该方法能在毫秒级响应时间内检测异常，并提供可解释性，优于现有的AI/ML模型与XAI方法。还能精准定位异常传感器和执行器状态。

Conclusion: 该方法在速度与可解释性上表现优异，适用于ICS的实时安全监控，为工业自动化提供了可靠的异常检测解决方案。

Abstract: The continuous monitoring of the interactions between cyber-physical
components of any industrial control system (ICS) is required to secure
automation of the system controls, and to guarantee plant processes are
fail-safe and remain in an acceptably safe state. Safety is achieved by
managing actuation (where electric signals are used to trigger physical
movement), dependent on corresponding sensor readings; used as ground truth in
decision making. Timely detection of anomalies (attacks, faults and
unascertained states) in ICSs is crucial for the safe running of a plant, the
safety of its personnel, and for the safe provision of any services provided.
We propose an anomaly detection method that involves accurate linearization of
the non-linear forms arising from sensor-actuator(s) relationships, primarily
because solving linear models is easier and well understood. Further, the time
complexity of the anomaly detection scenario/problem at hand is lowered using
dimensionality reduction of the actuator(s) in relationship with a sensor. We
accomplish this by using a well-known water treatment testbed as a use case.
Our experiments show millisecond time response to detect anomalies and provide
explainability; that are not simultaneously achieved by other state of the art
AI/ML models with eXplainable AI (XAI) used for the same purpose. Further, we
pin-point the sensor(s) and its actuation state for which anomaly was detected.

</details>


### [241] [ACE: A Security Architecture for LLM-Integrated App Systems](https://arxiv.org/abs/2504.20984)
*Evan Li,Tushin Mallick,Evan Rose,William Robertson,Alina Oprea,Cristina Nita-Rotaru*

Main category: cs.CR

TL;DR: 本文提出了一种名为ACE的新安全架构，用于保护LLM集成应用系统，通过解耦规划阶段和强制执行数据隔离，有效防御恶意应用的攻击。


<details>
  <summary>Details</summary>
Motivation: 随着LLM集成应用系统的普及，恶意应用可能破坏系统规划的完整性或执行阶段的可用性与隐私。因此，需要一种新的安全架构来保障系统安全。

Method: ACE架构将规划分为抽象计划和具体计划两阶段，并通过静态分析和执行时隔离确保安全性。

Result: 实验证明ACE能抵御INJECAGENT基准测试中的攻击及新提出的攻击，提供更高的安全性。

Conclusion: ACE是朝着强化LLM系统安全性迈出的重要一步，尤其适用于信任级别不同的系统组件。

Abstract: LLM-integrated app systems extend the utility of Large Language Models (LLMs)
with third-party apps that are invoked by a system LLM using interleaved
planning and execution phases to answer user queries. These systems introduce
new attack vectors where malicious apps can cause integrity violation of
planning or execution, availability breakdown, or privacy compromise during
execution.
  In this work, we identify new attacks impacting the integrity of planning, as
well as the integrity and availability of execution in LLM-integrated apps, and
demonstrate them against IsolateGPT, a recent solution designed to mitigate
attacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new
secure architecture for LLM-integrated app systems that provides security
guarantees for system planning and execution. Specifically, ACE decouples
planning into two phases by first creating an abstract execution plan using
only trusted information, and then mapping the abstract plan to a concrete plan
using installed system apps. We verify that the plans generated by our system
satisfy user-specified secure information flow constraints via static analysis
on the structured plan output. During execution, ACE enforces data and
capability barriers between apps, and ensures that the execution is conducted
according to the trusted abstract plan. We show experimentally that our system
is secure against attacks from the INJECAGENT benchmark, a standard benchmark
for control flow integrity in the face of indirect prompt injection attacks,
and our newly introduced attacks. Our architecture represents a significant
advancement towards hardening LLM-based systems containing system facilities of
varying levels of trustworthiness.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [242] [Partial Answer of How Transformers Learn Automata](https://arxiv.org/abs/2504.20395)
*Tiantian,Zhang*

Main category: cs.FL

TL;DR: 论文提出了一种利用表示论半直积和傅里叶模块的新框架，以更高效地实现基于Transformer的有限自动机模拟。


<details>
  <summary>Details</summary>
Motivation: 旨在通过数学工具提升基于Transformer的有限自动机模拟效率。

Method: 采用表示论的半直积和傅里叶模块构建新框架。

Result: 实现了更高效的Transformer-based有限自动机模拟。

Conclusion: 该方法为有限自动机的高效模拟提供了新思路。

Abstract: We introduce a novel framework for simulating finite automata using
representation-theoretic semidirect products and Fourier modules, achieving
more efficient Transformer-based implementations.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [243] [Pediatric Asthma Detection with Googles HeAR Model: An AI-Driven Respiratory Sound Classifier](https://arxiv.org/abs/2504.20124)
*Abul Ehtesham,Saket Kumar,Aditi Singh,Tala Talaei Khoei*

Main category: cs.SD

TL;DR: 摘要: 该论文提出了一种基于AI的哮喘早期诊断方法，利用Google的HeAR模型和SPRSound数据集，通过分析儿童呼吸音实现高准确率检测。


<details>
  <summary>Details</summary>
Motivation: 儿童哮喘的早期检测对预防长期呼吸并发症和减少紧急干预至关重要。

Method: 使用HeAR模型提取SPRSound数据集中儿童呼吸音的512维表征，并训练SVM、随机森林和MLP分类器进行分类。

Result: 系统准确率超过91%，在阳性病例的精确率-召回率指标上表现优异，并通过PCA可视化、误分类分析和ROC曲线提供深入见解。

Conclusion: 研究表明，结合基础音频模型的短时、低资源儿童录音可实现快速、非侵入性哮喘筛查，尤其适用于偏远或医疗资源匮乏地区的数字诊断。

Abstract: Early detection of asthma in children is crucial to prevent long-term
respiratory complications and reduce emergency interventions. This work
presents an AI-powered diagnostic pipeline that leverages Googles Health
Acoustic Representations (HeAR) model to detect early signs of asthma from
pediatric respiratory sounds. The SPRSound dataset, the first open-access
collection of annotated respiratory sounds in children aged 1 month to 18
years, is used to extract 2-second audio segments labeled as wheeze, crackle,
rhonchi, stridor, or normal. Each segment is embedded into a 512-dimensional
representation using HeAR, a foundation model pretrained on 300 million
health-related audio clips, including 100 million cough sounds. Multiple
classifiers, including SVM, Random Forest, and MLP, are trained on these
embeddings to distinguish between asthma-indicative and normal sounds. The
system achieves over 91\% accuracy, with strong performance on precision-recall
metrics for positive cases. In addition to classification, learned embeddings
are visualized using PCA, misclassifications are analyzed through waveform
playback, and ROC and confusion matrix insights are provided. This method
demonstrates that short, low-resource pediatric recordings, when powered by
foundation audio models, can enable fast, noninvasive asthma screening. The
approach is especially promising for digital diagnostics in remote or
underserved healthcare settings.

</details>


### [244] [APG-MOS: Auditory Perception Guided-MOS Predictor for Synthetic Speech](https://arxiv.org/abs/2504.20447)
*Zhicheng Lian,Lizhi Wang,Hua Huang*

Main category: cs.SD

TL;DR: APG-MOS模型结合听觉感知机制与语义分析，通过模拟耳蜗功能和语义失真建模，提升语音质量评估的一致性，实验显示其在主流基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 减少人工评估的繁重工作，通过计算模型量化主观感知，但现有深度学习模型忽略听觉机制，导致与人类判断的偏差。

Method: 提出APG-MOS模型，包括基于生物听觉的感知模块、RVQ语义失真建模及残差交叉注意力架构，结合渐进学习策略实现多模态融合。

Result: 在两个主流基准测试中表现优于现有方法。

Conclusion: APG-MOS通过结合听觉与语义机制，显著提升了语音质量评估的准确性与一致性。

Abstract: Automatic speech quality assessment aims to quantify subjective human
perception of speech through computational models to reduce the need for
labor-consuming manual evaluations. While models based on deep learning have
achieved progress in predicting mean opinion scores (MOS) to assess synthetic
speech, the neglect of fundamental auditory perception mechanisms limits
consistency with human judgments. To address this issue, we propose an auditory
perception guided-MOS prediction model (APG-MOS) that synergistically
integrates auditory modeling with semantic analysis to enhance consistency with
human judgments. Specifically, we first design a perceptual module, grounded in
biological auditory mechanisms, to simulate cochlear functions, which encodes
acoustic signals into biologically aligned electrochemical representations.
Secondly, we propose a residual vector quantization (RVQ)-based semantic
distortion modeling method to quantify the degradation of speech quality at the
semantic level. Finally, we design a residual cross-attention architecture,
coupled with a progressive learning strategy, to enable multimodal fusion of
encoded electrochemical signals and semantic representations. Experiments
demonstrate that APG-MOS achieves superior performance on two primary
benchmarks. Our code and checkpoint will be available on a public repository
upon publication.

</details>


### [245] [DiffusionRIR: Room Impulse Response Interpolation using Diffusion Models](https://arxiv.org/abs/2504.20625)
*Sagi Della Torre,Mirco Pezzoli,Fabio Antonacci,Sharon Gannot*

Main category: cs.SD

TL;DR: 该论文使用降噪扩散概率模型（DDPM）解决房间内未测量位置的脉冲响应（RIR）估计问题，将RIR数据转化为适合扩散模型重建的格式，显著优于基线插值方法。


<details>
  <summary>Details</summary>
Motivation: 高空间分辨率的RIR测量资源密集，难以大规模应用，因此需要一种高效的方法从有限测量中估计RIR。

Method: 将RIR矩阵类比为图像修复问题，使用DDPM进行数据重建，并在不同曲率的麦克风阵列上验证。

Result: 该方法在麦克风间大间隙情况下仍能准确重建RIR，在归一化均方误差和余弦距离上显著优于三次样条插值。

Conclusion: 生成模型在RIR插值中展现出潜力，为通过有限实测数据生成额外数据提供了新途径。

Abstract: Room Impulse Responses (RIRs) characterize acoustic environments and are
crucial in multiple audio signal processing tasks. High-quality RIR estimates
drive applications such as virtual microphones, sound source localization,
augmented reality, and data augmentation. However, obtaining RIR measurements
with high spatial resolution is resource-intensive, making it impractical for
large spaces or when dense sampling is required. This research addresses the
challenge of estimating RIRs at unmeasured locations within a room using
Denoising Diffusion Probabilistic Models (DDPM). Our method leverages the
analogy between RIR matrices and image inpainting, transforming RIR data into a
format suitable for diffusion-based reconstruction.
  Using simulated RIR data based on the image method, we demonstrate our
approach's effectiveness on microphone arrays of different curvatures, from
linear to semi-circular. Our method successfully reconstructs missing RIRs,
even in large gaps between microphones. Under these conditions, it achieves
accurate reconstruction, significantly outperforming baseline Spline Cubic
Interpolation in terms of Normalized Mean Square Error and Cosine Distance
between actual and interpolated RIRs.
  This research highlights the potential of using generative models for
effective RIR interpolation, paving the way for generating additional data from
limited real-world measurements.

</details>


### [246] [ECOSoundSet: a finely annotated dataset for the automated acoustic identification of Orthoptera and Cicadidae in North, Central and temperate Western Europe](https://arxiv.org/abs/2504.20776)
*David Funosas,Elodie Massol,Yves Bas,Svenja Schmidt,Dominik Arend,Alexander Gebhard,Luc Barbaro,Sebastian König,Rafael Carbonell Font,David Sannier,Fernand Deroussen,Jérôme Sueur,Christian Roesti,Tomi Trilar,Wolfgang Forstmeier,Lucas Roger,Eloïsa Matheu,Piotr Guzik,Julien Barataud,Laurent Pelozuelo,Stéphane Puissant,Sandra Mueller,Björn Schuller,Jose M. Montoya,Andreas Triantafyllopoulos,Maxime Cauchoix*

Main category: cs.SD

TL;DR: 论文介绍了ECOSoundSet数据集，包含10,653个欧洲蝗虫和蝉的录音，用于训练深度学习算法进行自动声学识别。


<details>
  <summary>Details</summary>
Motivation: 现有工具在欧洲昆虫声学识别中覆盖面有限，需要大且生态多样的数据集来提升算法对不同环境下的细微声学特征的识别能力。

Method: 通过田野调查和欧洲昆虫学家的贡献，收集了粗标注（弱标签）和细标注（强标签）的录音，并提供了训练/验证/测试的分割。

Result: ECOSoundSet数据集包含200种蝗虫和24种蝉的录音，涵盖了北欧、中欧和西欧地区，并提供强标签录音的分割以支持深度学习训练和评估。

Conclusion: 此数据集可作为现有在线录音的有益补充，用于训练深度学习算法对欧洲蝗虫和蝉的声学分类。

Abstract: Currently available tools for the automated acoustic recognition of European
insects in natural soundscapes are limited in scope. Large and ecologically
heterogeneous acoustic datasets are currently needed for these algorithms to
cross-contextually recognize the subtle and complex acoustic signatures
produced by each species, thus making the availability of such datasets a key
requisite for their development. Here we present ECOSoundSet (European
Cicadidae and Orthoptera Sound dataSet), a dataset containing 10,653 recordings
of 200 orthopteran and 24 cicada species (217 and 26 respective taxa when
including subspecies) present in North, Central, and temperate Western Europe
(Andorra, Belgium, Denmark, mainland France and Corsica, Germany, Ireland,
Luxembourg, Monaco, Netherlands, United Kingdom, Switzerland), collected partly
through targeted fieldwork in South France and Catalonia and partly through
contributions from various European entomologists. The dataset is composed of a
combination of coarsely labeled recordings, for which we can only infer the
presence, at some point, of their target species (weak labeling), and finely
annotated recordings, for which we know the specific time and frequency range
of each insect sound present in the recording (strong labeling). We also
provide a train/validation/test split of the strongly labeled recordings, with
respective approximate proportions of 0.8, 0.1 and 0.1, in order to facilitate
their incorporation in the training and evaluation of deep learning algorithms.
This dataset could serve as a meaningful complement to recordings already
available online for the training of deep learning algorithms for the acoustic
classification of orthopterans and cicadas in North, Central, and temperate
Western Europe.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [247] [Towards Easy and Realistic Network Infrastructure Testing for Large-scale Machine Learning](https://arxiv.org/abs/2504.20854)
*Jinsun Yoo,ChonLam Lao,Lianjie Cao,Bob Lantz,Minlan Yu,Tushar Krishna,Puneet Sharma*

Main category: cs.NI

TL;DR: Genie是一个测试框架，通过CPU生成流量模拟GPU间通信，并结合ASTRA-sim模拟器分析网络对ML任务性能的影响，无需昂贵GPU硬件。


<details>
  <summary>Details</summary>
Motivation: 传统的GPU测试成本高昂，Genie旨在提供一种低成本方法，量化真实硬件网络行为对ML性能的影响。

Method: 利用CPU生成流量模拟GPU间通信，结合ASTRA-sim模拟器建模网络与ML任务的交互。

Result: 无需GPU即可有效评估网络对ML负载的性能影响。

Conclusion: Genie为低成本、高效的ML网络性能测试提供了实用解决方案。

Abstract: This paper lays the foundation for Genie, a testing framework that captures
the impact of real hardware network behavior on ML workload performance,
without requiring expensive GPUs. Genie uses CPU-initiated traffic over a
hardware testbed to emulate GPU to GPU communication, and adapts the ASTRA-sim
simulator to model interaction between the network and the ML workload.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [248] [Provably faster randomized and quantum algorithms for k-means clustering via uniform sampling](https://arxiv.org/abs/2504.20982)
*Tyler Chen,Archan Ray,Akshay Seshadri,Dylan Herman,Bao Bach,Pranav Deshpande,Abhishek Som,Niraj Kumar,Marco Pistoia*

Main category: quant-ph

TL;DR: 论文提出了一种简单的随机mini-batch k-means算法及其启发的量子算法，通过均匀采样改进此前算法的计算效率，显著降低了时间复杂度的上界。


<details>
  <summary>Details</summary>
Motivation: 传统k-means算法在大数据场景下因时间复杂度与数据量呈线性而效率低下。近期量子或量子启发的经典算法虽能解决此问题，但其采样方式（基于数据范数）破坏了k-means问题的对称性。本文旨在通过均匀采样设计更高效的算法。

Method: 提出随机mini-batch k-means算法及对应的量子算法。关键创新在于使用均匀采样而非基于数据范数的采样，以保持问题对称性。

Result: 新算法在最坏情况下显著优于先前算法的性能边界，主要得益于均匀采样的对称性保护。

Conclusion: 通过引入对称性保护的采样策略，本文算法在理论上实现了更优的时间复杂度，为大数据聚类提供了更高效的解决方案。

Abstract: The $k$-means algorithm (Lloyd's algorithm) is a widely used method for
clustering unlabeled data. A key bottleneck of the $k$-means algorithm is that
each iteration requires time linear in the number of data points, which can be
expensive in big data applications. This was improved in recent works proposing
quantum and quantum-inspired classical algorithms to approximate the $k$-means
algorithm locally, in time depending only logarithmically on the number of data
points (along with data dependent parameters) [$q$-means: A quantum algorithm
for unsupervised machine learning; Kerenidis, Landman, Luongo, and Prakash,
NeurIPS 2019; Do you know what $q$-means?, Doriguello, Luongo, Tang]. In this
work, we describe a simple randomized mini-batch $k$-means algorithm and a
quantum algorithm inspired by the classical algorithm. We prove worse-case
guarantees that significantly improve upon the bounds for previous algorithms.
Our improvements are due to a careful use of uniform sampling, which preserves
certain symmetries of the $k$-means problem that are not preserved in previous
algorithms that use data norm-based sampling.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [249] [AKIBoards: A Structure-Following Multiagent System for Predicting Acute Kidney Injury](https://arxiv.org/abs/2504.20368)
*David Gordon,Panayiotis Petousis,Susanne B. Nicholas,Alex A. T. Bui*

Main category: cs.MA

TL;DR: 这篇论文提出了一种名为STRUC-MAS的框架，用于自动化学习多专家系统的全局模型，并通过预测急性肾损伤（AKI）证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在复杂医疗情境中，多专家合作进行诊断需要整合不同视角。STRUC-MAS旨在自动化学习这些全局模型，以优化多智能体系统（MAS）的决策性能。

Method: 论文提出了STRUC-MAS框架，通过学习全局模型并作为先验信念融入多智能体系统。在AKI预测案例中，采用结构遵循（SF-FT）和检索增强生成（RAG）等方法进行验证。

Result: 实验结果显示，采用全局结构的智能体在预测AKI（发病前48小时）时表现更好（AP=0.195），相比基线方法（AP=0.141）。智能体通过交互增强了决策信心，证明了全局结构的必要性。

Conclusion: 研究表明，在多智能体系统中学习和利用全局结构是提升分类和诊断推理性能的关键，尤其是在复杂医疗场景中。

Abstract: Diagnostic reasoning entails a physician's local (mental) model based on an
assumed or known shared perspective (global model) to explain patient
observations with evidence assigned towards a clinical assessment. But in
several (complex) medical situations, multiple experts work together as a team
to optimize health evaluation and decision-making by leveraging different
perspectives. Such consensus-driven reasoning reflects individual knowledge
contributing toward a broader perspective on the patient. In this light, we
introduce STRUCture-following for Multiagent Systems (STRUC-MAS), a framework
automating the learning of these global models and their incorporation as prior
beliefs for agents in multiagent systems (MAS) to follow. We demonstrate proof
of concept with a prosocial MAS application for predicting acute kidney
injuries (AKIs). In this case, we found that incorporating a global structure
enabled multiple agents to achieve better performance (average precision, AP)
in predicting AKI 48 hours before onset (structure-following-fine-tuned, SF-FT,
AP=0.195; SF-FT-retrieval-augmented generation, SF-FT-RAG, AP=0.194) vs.
baseline (non-structure-following-FT, NSF-FT, AP=0.141; NSF-FT-RAG, AP=0.180)
for balanced precision-weighted-recall-weighted voting. Markedly, SF-FT agents
with higher recall scores reported lower confidence levels in the initial round
on true positive and false negative cases. But after explicit interactions,
their confidence in their decisions increased (suggesting reinforced belief).
In contrast, the SF-FT agent with the lowest recall decreased its confidence in
true positive and false negative cases (suggesting a new belief). This approach
suggests that learning and leveraging global structures in MAS is necessary
prior to achieving competitive classification and diagnostic reasoning
performance.

</details>


### [250] [Modeling AI-Human Collaboration as a Multi-Agent Adaptation](https://arxiv.org/abs/2504.20903)
*Prothit Sen,Sai Mihir Jakkaraju*

Main category: cs.MA

TL;DR: 这篇论文开发了一个基于代理的仿真模型，研究了AI与人类在不同任务结构（模块化与顺序化）中的协作。结果表明AI在某些情况下可以替代人类，而在另一些情况下则需互补合作，最终结论强调了任务结构对AI-人类协作效果的核心影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解AI与人类在不同任务结构中的协作模式，为组织中的战略决策提供通用框架。

Method: 采用基于代理的仿真方法，结合NK模型，区分人类启发式适应与AI规则搜索，分析模块化和顺序化任务中的交互。

Result: 结果显示模块化任务中AI常可替代人类，但在顺序化任务中AI与人类专家互补效果最佳；即使功能有限的AI也能帮助低能力人类突破局部最优。

Conclusion: 研究结论指出，AI-人类协作的有效性主要取决于任务结构而非具体行业背景，为组织战略提供了可迁移的分析视角。

Abstract: We develop an agent-based simulation to formalize AI-human collaboration as a
function of task structure, advancing a generalizable framework for strategic
decision-making in organizations. Distinguishing between heuristic-based human
adaptation and rule-based AI search, we model interactions across modular
(parallel) and sequenced (interdependent) tasks using an NK model. Our results
reveal that in modular tasks, AI often substitutes for humans - delivering
higher payoffs unless human expertise is very high, and the AI search space is
either narrowly focused or extremely broad. In sequenced tasks, interesting
complementarities emerge. When an expert human initiates the search and AI
subsequently refines it, aggregate performance is maximized. Conversely, when
AI leads, excessive heuristic refinement by the human can reduce payoffs. We
also show that even "hallucinatory" AI - lacking memory or structure - can
improve outcomes when augmenting low-capability humans by helping escape local
optima. These results yield a robust implication: the effectiveness of AI-human
collaboration depends less on context or industry, and more on the underlying
task structure. By elevating task decomposition as the central unit of
analysis, our model provides a transferable lens for strategic decision-making
involving humans and an agentic AI across diverse organizational settings.

</details>
