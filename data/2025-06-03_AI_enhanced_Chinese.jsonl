{"id": "2506.00019", "pdf": "https://arxiv.org/pdf/2506.00019", "abs": "https://arxiv.org/abs/2506.00019", "authors": ["William Alberto Cruz-Casta\u00f1eda", "Marcellus Amadeus"], "title": "Amadeus-Verbo Technical Report: The powerful Qwen2.5 family models trained in Portuguese", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This report introduces the experience of developing Amadeus Verbo, a family\nof large language models for Brazilian Portuguese. To handle diverse use cases,\nAmadeus Verbo includes base-tuned, merged, and instruction-tuned models in\nsizes of 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters. Thus, the main\nobjective is to show how easy it is to fine-tune foundation models to\ndemocratize the open-source development of Brazilian Portuguese LLMs when data\nand resources are available. Amadeus-Verbo family models are all available at\nHuggingFace at\nhttps://huggingface.co/collections/amadeusai/amadeus-verbo-qwen25-67cf2e7aae69ce2b3bcdcfda.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u5f00\u53d1\u5df4\u897f\u8461\u8404\u7259\u8bed\u5927\u8bed\u8a00\u6a21\u578bAmadeus Verbo\u7684\u7ecf\u9a8c\uff0c\u5305\u62ec\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u53ca\u5982\u4f55\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u63a8\u52a8\u5df4\u897f\u8461\u8404\u7259\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u6e90\u5f00\u53d1\uff0c\u5c55\u793a\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u7684\u4fbf\u6377\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\uff080.5B\u81f372B\uff09\uff0c\u5305\u62ec\u57fa\u7840\u8c03\u4f18\u3001\u5408\u5e76\u548c\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u3002", "result": "Amadeus Verbo\u7cfb\u5217\u6a21\u578b\u5df2\u5728HuggingFace\u4e0a\u5f00\u6e90\u3002", "conclusion": "\u5c55\u793a\u4e86\u6570\u636e\u548c\u8d44\u6e90\u53ef\u7528\u65f6\uff0c\u5df4\u897f\u8461\u8404\u7259\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u6e90\u5f00\u53d1\u7684\u53ef\u884c\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u5df4\u897f\u8461\u8404\u7259\u8bed, \u5fae\u8c03, \u5f00\u6e90"}}
{"id": "2506.00022", "pdf": "https://arxiv.org/pdf/2506.00022", "abs": "https://arxiv.org/abs/2506.00022", "authors": ["Shenghe Zheng", "Qianjia Cheng", "Junchi Yao", "Mengsong Wu", "haonan he", "Ning Ding", "Yu Cheng", "Shuyue Hu", "Lei Bai", "Dongzhan Zhou", "Ganqu Cui", "Peng Ye"], "title": "Scaling Physical Reasoning with the PHYSICS Dataset", "categories": ["cs.CL", "cs.LG", "physics.ed-ph"], "comment": "Work on physical datasets", "summary": "Large Language Models (LLMs) have achieved remarkable progress on advanced\nreasoning tasks such as mathematics and coding competitions. Meanwhile,\nphysics, despite being both reasoning-intensive and essential to real-world\nunderstanding, received limited academic and industrial attention. This paper\nintroduces PHYSICS, a dataset containing 16,568 high-quality physics problems\nspanning subjects and difficulty levels, to facilitate this issue.\nSpecifically, PHYSICS is curated with exercises from over 100 textbooks through\na carefully designed pipeline for quality control. It covers five major physics\ndomains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern\nPhysics. It also spans a wide range of difficulty levels, from high school to\ngraduate-level physics courses. To utilize the data for improving and\nevaluating the model's physical reasoning capabilities, we split the dataset\ninto training and test sets, and provide reasoning paths generated by powerful\nreasoning models for the training data to facilitate model training. In\naddition, for the evaluation part, we find that existing evaluation frameworks\nexhibit biases in aspects such as units, simplification, and precision in\nphysics domain. To balance efficiency and accuracy, we introduce a Rule+Model\nevaluation framework tailored to physics problems. Our evaluations on current\nstate-of-the-art open-source and proprietary models highlight the limitations\nof current models in handling physics-related tasks. We hope that our dataset\nand evaluation methodology will jointly advance the development of LLMs in the\nfield of physics.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86PHYSICS\u6570\u636e\u96c6\uff0c\u5305\u542b16,568\u4e2a\u9ad8\u8d28\u91cf\u7269\u7406\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u7269\u7406\u662f\u63a8\u7406\u5bc6\u96c6\u4e14\u5bf9\u73b0\u5b9e\u4e16\u754c\u7406\u89e3\u81f3\u5173\u91cd\u8981\u7684\u9886\u57df\uff0c\u4f46\u4e0e\u6570\u5b66\u548c\u7f16\u7a0b\u76f8\u6bd4\uff0c\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u5bf9\u5176\u5173\u6ce8\u8f83\u5c11\u3002", "method": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6d41\u7a0b\u4ece100\u591a\u672c\u6559\u79d1\u4e66\u4e2d\u6536\u96c6\u95ee\u9898\uff0c\u8986\u76d6\u4e94\u5927\u7269\u7406\u9886\u57df\u548c\u4e0d\u540c\u96be\u5ea6\u7ea7\u522b\uff0c\u5e76\u63d0\u4f9b\u8bad\u7ec3\u6570\u636e\u7684\u63a8\u7406\u8def\u5f84\u3002\u6b64\u5916\uff0c\u63d0\u51faRule+Model\u8bc4\u4f30\u6846\u67b6\u4ee5\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u504f\u5dee\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u5f53\u524d\u5148\u8fdb\u7684\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\u5728\u5904\u7406\u7269\u7406\u4efb\u52a1\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "PHYSICS\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u6709\u671b\u5171\u540c\u63a8\u52a8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u9886\u57df\u7684\u53d1\u5c55\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u7269\u7406\u63a8\u7406,\u6570\u636e\u96c6,\u8bc4\u4f30\u6846\u67b6"}}
{"id": "2506.00027", "pdf": "https://arxiv.org/pdf/2506.00027", "abs": "https://arxiv.org/abs/2506.00027", "authors": ["Zhengyu Chen", "Yudong Wang", "Teng Xiao", "Ruochen Zhou", "Xuesheng Yang", "Wei Wang", "Zhifang Sui", "Jingang Wang"], "title": "From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in improving the reasoning capabilities of Large Language\nModels have underscored the efficacy of Process Reward Models (PRMs) in\naddressing intermediate errors through structured feedback mechanisms. This\nstudy analyzes PRMs from multiple perspectives, including training\nmethodologies, scalability, and generalization capabilities. We investigate the\ninterplay between pre-training and reward model training FLOPs to assess their\ninfluence on PRM efficiency and accuracy in complex reasoning tasks. Our\nanalysis reveals a pattern of diminishing returns in performance with\nincreasing PRM scale, highlighting the importance of balancing model size and\ncomputational cost. Furthermore, the diversity of training datasets\nsignificantly impacts PRM performance, emphasizing the importance of diverse\ndata to enhance both accuracy and efficiency. We further examine test-time\nscaling strategies, identifying Monte Carlo Tree Search as the most effective\nmethod when computational resources are abundant, while Best-of-N Sampling\nserves as a practical alternative under resource-limited conditions. Notably,\nour findings indicate that PRMs trained on mathematical datasets exhibit\nperformance comparable to those tailored for code generation, suggesting robust\ncross-domain generalization. Employing a gradient-based metric, we observe that\nPRMs exhibit a preference for selecting responses with similar underlying\npatterns, further informing their optimization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\u5728\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u4f5c\u7528\uff0c\u63a2\u8ba8\u4e86\u8bad\u7ec3\u65b9\u6cd5\u3001\u89c4\u6a21\u5316\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u6307\u51fa\u6a21\u578b\u89c4\u6a21\u548c\u8ba1\u7b97\u6210\u672c\u7684\u5e73\u8861\u4ee5\u53ca\u6570\u636e\u591a\u6837\u6027\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u7684\u52a8\u673a\u662f\u8bc4\u4f30\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\u5728\u89e3\u51b3\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u4e2d\u95f4\u9519\u8bef\u7684\u80fd\u529b\uff0c\u65e8\u5728\u4f18\u5316\u5176\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u63a2\u7d22\u8de8\u9886\u57df\u7684\u6cdb\u5316\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u4ece\u591a\u89d2\u5ea6\u5206\u6790\u4e86PRMs\uff0c\u5305\u62ec\u8bad\u7ec3\u65b9\u6cd5\u3001FLOPs\u7684\u5f71\u54cd\u3001\u8bad\u7ec3\u6570\u636e\u591a\u6837\u6027\uff0c\u5e76\u6d4b\u8bd5\u4e86\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\uff08\u5982\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548cBest-of-N\u91c7\u6837\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0PRM\u6027\u80fd\u968f\u89c4\u6a21\u589e\u52a0\u800c\u9012\u51cf\uff0c\u6570\u636e\u591a\u6837\u6027\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff1b\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u5728\u8d44\u6e90\u5145\u8db3\u65f6\u6700\u6709\u6548\uff0c\u800cBest-of-N\u91c7\u6837\u9002\u5408\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\uff1bPRMs\u5728\u6570\u5b66\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4ea4\u53c9\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PRMs\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9700\u5e73\u8861\u6a21\u578b\u89c4\u6a21\u4e0e\u8ba1\u7b97\u6210\u672c\uff1b\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u548c\u9002\u5f53\u7684\u6d4b\u8bd5\u7b56\u7565\u662f\u4f18\u5316\u6027\u80fd\u7684\u5173\u952e\uff1bPRMs\u5177\u6709\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\u3001\u63a8\u7406\u80fd\u529b\u3001\u89c4\u6a21\u5316\u3001\u6cdb\u5316\u80fd\u529b\u3001\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u3001Best-of-N\u91c7\u6837"}}
{"id": "2506.00042", "pdf": "https://arxiv.org/pdf/2506.00042", "abs": "https://arxiv.org/abs/2506.00042", "authors": ["Yue Cui", "Liuyi Yao", "Shuchang Tao", "Weijie Shi", "Yaliang Li", "Bolin Ding", "Xiaofang Zhou"], "title": "Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have significantly advanced natural language\nprocessing, particularly through the integration of external tools and APIs.\nHowever, their effectiveness is frequently hampered by parameter mis-filling\nduring tool calling. In this paper, we propose the Hierarchical Tool Error\nChecklist (HiTEC) framework to systematically diagnose and mitigate\ntool-calling errors without relying on extensive real-world interactions. HiTEC\nintroduces a two-tiered approach: a global error checklist that identifies\ncommon, cross-tool issues, and a local error checklist that targets\ntool-specific and contextual failures. Building on this structure, we propose\ntwo deployments: HiTEC-In Context Learning (HiTEC-ICL) and\nHiTEC-Kahneman-Tversky Optimization (HiTEC-KTO). HiTEC-ICL embeds the global\nchecklist in the initial prompts and leverages a two-round conversational\ninteraction to dynamically refine parameter handling, while HiTEC-KTO generates\nhigh-quality negative examples to drive fine-tuning via preference-based\noptimization. Extensive experiments across five public datasets demonstrate\nthat our framework significantly improves parameter-filling accuracy and\ntool-calling success rates compared to baseline methods.", "AI": {"tldr": "\u63d0\u51faHiTEC\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u9519\u8bef\u68c0\u67e5\u6e05\u5355\u6539\u8fdbLLMs\u5de5\u5177\u8c03\u7528\u4e2d\u7684\u53c2\u6570\u586b\u5145\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5de5\u5177\u8c03\u7528\u65f6\u7684\u53c2\u6570\u586b\u5145\u9519\u8bef\u95ee\u9898\uff0c\u63d0\u5347\u5176\u5b9e\u9645\u5e94\u7528\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u5de5\u5177\u9519\u8bef\u68c0\u67e5\u6e05\u5355\uff08HiTEC\uff09\uff0c\u5305\u62ec\u5168\u5c40\u548c\u5c40\u90e8\u68c0\u67e5\u6e05\u5355\uff0c\u5e76\u901a\u8fc7HiTEC-ICL\u548cHiTEC-KTO\u4e24\u79cd\u65b9\u6cd5\u52a8\u6001\u4f18\u5316\u53c2\u6570\u5904\u7406\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHiTEC\u663e\u8457\u63d0\u9ad8\u4e86\u53c2\u6570\u586b\u5145\u51c6\u786e\u6027\u548c\u5de5\u5177\u8c03\u7528\u6210\u529f\u7387\u3002", "conclusion": "HiTEC\u6846\u67b6\u6709\u6548\u8bca\u65ad\u548c\u7f13\u89e3\u4e86\u5de5\u5177\u8c03\u7528\u9519\u8bef\uff0c\u4e3aLLMs\u7684\u5b9e\u7528\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u5de5\u5177\u8c03\u7528\u3001\u53c2\u6570\u586b\u5145\u3001HiTEC\u3001\u4f18\u5316"}}
{"id": "2506.00056", "pdf": "https://arxiv.org/pdf/2506.00056", "abs": "https://arxiv.org/abs/2506.00056", "authors": ["Hugon Lee", "Hyeonbin Moon", "Junhyeong Lee", "Seunghwa RYu"], "title": "Toward Knowledge-Guided AI for Inverse Design in Manufacturing: A Perspective on Domain, Physics, and Human-AI Synergy", "categories": ["cs.AI", "physics.comp-ph"], "comment": "26 pages, 4 figures", "summary": "Artificial intelligence (AI) is reshaping inverse design across manufacturing\ndomain, enabling high-performance discovery in materials, products, and\nprocesses. However, purely data-driven approaches often struggle in realistic\nsettings characterized by sparse data, high-dimensional design spaces, and\nnontrivial physical constraints. This perspective argues for a new generation\nof design systems that transcend black-box modeling by integrating domain\nknowledge, physics-informed learning, and intuitive human-AI interfaces. We\nfirst demonstrate how expert-guided sampling strategies enhance data efficiency\nand model generalization. Next, we discuss how physics-informed machine\nlearning enables physically consistent modeling in data-scarce regimes.\nFinally, we explore how large language models emerge as interactive design\nagents connecting user intent with simulation tools, optimization pipelines,\nand collaborative workflows. Through illustrative examples and conceptual\nframeworks, we advocate that inverse design in manufacturing should evolve into\na unified ecosystem, where domain knowledge, physical priors, and adaptive\nreasoning collectively enable scalable, interpretable, and accessible AI-driven\ndesign systems.", "AI": {"tldr": "\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u6b63\u5728\u91cd\u5851\u5236\u9020\u4e1a\u9886\u57df\u7684\u9006\u5411\u8bbe\u8ba1\uff0c\u63a8\u52a8\u9ad8\u6027\u80fd\u6750\u6599\u548c\u5de5\u827a\u7684\u53d1\u73b0\u3002\u7136\u800c\uff0c\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u758f\u3001\u9ad8\u7ef4\u8bbe\u8ba1\u7a7a\u95f4\u548c\u590d\u6742\u7269\u7406\u7ea6\u675f\u7684\u73b0\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u63d0\u51fa\u65b0\u4e00\u4ee3\u8bbe\u8ba1\u7cfb\u7edf\uff0c\u6574\u5408\u9886\u57df\u77e5\u8bc6\u3001\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u548c\u4eba\u673a\u4ea4\u4e92\u754c\u9762\uff0c\u8d85\u8d8a\u9ed1\u7bb1\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u73b0\u5b9e\u9006\u5411\u8bbe\u8ba1\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u3001\u7269\u7406\u4e00\u81f4\u4e14\u6613\u4e8e\u4ea4\u4e92\u7684\u8bbe\u8ba1\u7cfb\u7edf\u3002", "method": "\u7ed3\u5408\u4e13\u5bb6\u5f15\u5bfc\u91c7\u6837\u3001\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u548c\u5927\u8bed\u8a00\u6a21\u578b\u4ea4\u4e92\u6280\u672f\uff0c\u6784\u5efa\u6574\u5408\u9886\u57df\u77e5\u8bc6\u3001\u7269\u7406\u5148\u9a8c\u548c\u81ea\u9002\u5e94\u63a8\u7406\u7684\u7cfb\u7edf\u3002", "result": "\u7cfb\u7edf\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\u3001\u7269\u7406\u4e00\u81f4\u6027\u548c\u7528\u6237\u53cb\u597d\u6027\uff0c\u652f\u6301\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u548c\u6613\u8bbf\u95ee\u7684AI\u9a71\u52a8\u8bbe\u8ba1\u3002", "conclusion": "\u9006\u5411\u8bbe\u8ba1\u5e94\u53d1\u5c55\u4e3a\u7edf\u4e00\u751f\u6001\u7cfb\u7edf\uff0c\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u3001\u7269\u7406\u5148\u9a8c\u548c\u81ea\u9002\u5e94\u63a8\u7406\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "keywords": "\u4eba\u5de5\u667a\u80fd\uff0c\u9006\u5411\u8bbe\u8ba1\uff0c\u7269\u7406\u4fe1\u606f\u5b66\u4e60\uff0c\u4eba\u673a\u4ea4\u4e92\uff0c\u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.00030", "pdf": "https://arxiv.org/pdf/2506.00030", "abs": "https://arxiv.org/abs/2506.00030", "authors": ["Xiang Shi", "Rui Zhang", "Jiawei Liu", "Yinpeng Liu", "Qikai Cheng", "Wei Lu"], "title": "Modality Equilibrium Matters: Minor-Modality-Aware Adaptive Alternating for Cross-Modal Memory Enhancement", "categories": ["cs.LG"], "comment": "work in progress", "summary": "Multimodal fusion is susceptible to modality imbalance, where dominant\nmodalities overshadow weak ones, easily leading to biased learning and\nsuboptimal fusion, especially for incomplete modality conditions. To address\nthis problem, we propose a Shapley-guided alternating training framework that\nadaptively prioritizes minor modalities to balance and thus enhance the fusion.\nOur method leverages Shapley Value-based scheduling to improve the training\nsequence adaptively, ensuring that under-optimized modalities receive\nsufficient learning. Additionally, we introduce the memory module to refine and\ninherit modality-specific representations with a cross-modal mapping mechanism\nto align features at both the feature and sample levels. To further validate\nthe adaptability of the proposed approach, the encoder module empirically\nadopts both conventional and LLM-based backbones. With building up a novel\nmultimodal equilibrium metric, namely, equilibrium deviation metric (EDM), we\nevaluate the performance in both balance and accuracy across four multimodal\nbenchmark datasets, where our method achieves state-of-the-art (SOTA) results.\nMeanwhile, robustness analysis under missing modalities highlights its strong\ngeneralization capabilities. Accordingly, our findings reveal the untapped\npotential of alternating training, demonstrating that strategic modality\nprioritization fundamentally balances and promotes multimodal learning,\noffering a new paradigm for optimizing multimodal training dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eShapley\u503c\u7684\u4ea4\u66ff\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4f18\u5148\u5904\u7406\u5f31\u6a21\u6001\u6765\u5e73\u8861\u591a\u6a21\u6001\u878d\u5408\uff0c\u89e3\u51b3\u4e86\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\u3002", "motivation": "\u591a\u6a21\u6001\u878d\u5408\u4e2d\uff0c\u5f3a\u52bf\u6a21\u6001\u5bb9\u6613\u538b\u5236\u5f31\u52bf\u6a21\u6001\uff0c\u5bfc\u81f4\u5b66\u4e60\u504f\u5dee\u548c\u6b21\u4f18\u878d\u5408\uff0c\u5c24\u5176\u662f\u5728\u6a21\u6001\u4e0d\u5b8c\u6574\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u91c7\u7528Shapley\u503c\u8c03\u5ea6\u7b56\u7565\u81ea\u9002\u5e94\u4f18\u5316\u8bad\u7ec3\u5e8f\u5217\uff0c\u5f15\u5165\u8bb0\u5fc6\u6a21\u5757\u548c\u8de8\u6a21\u6001\u6620\u5c04\u673a\u5236\u5bf9\u9f50\u7279\u5f81\uff0c\u5e76\u652f\u6301\u4f20\u7edf\u548cLLM\u7f16\u7801\u5668\u3002", "result": "\u5728\u56db\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5e73\u8861\u548c\u51c6\u786e\u6027\u7684SOTA\u8868\u73b0\uff0c\u4e14\u5728\u7f3a\u5931\u6a21\u6001\u4e0b\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u4ea4\u66ff\u8bad\u7ec3\u548c\u6a21\u6001\u4f18\u5148\u7ea7\u7b56\u7565\u80fd\u6709\u6548\u5e73\u8861\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u4e3a\u591a\u6a21\u6001\u8bad\u7ec3\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "keywords": "\u591a\u6a21\u6001\u878d\u5408, Shapley\u503c, \u6a21\u6001\u5e73\u8861, \u4ea4\u66ff\u8bad\u7ec3, \u8bb0\u5fc6\u6a21\u5757"}}
{"id": "2506.00061", "pdf": "https://arxiv.org/pdf/2506.00061", "abs": "https://arxiv.org/abs/2506.00061", "authors": ["Wiktoria Mieleszczenko-Kowszewicz", "Beata Bajcar", "Aleksander Szcz\u0119sny", "Maciej Markiewicz", "Jolanta Babiak", "Berenika Dyczek", "Przemys\u0142aw Kazienko"], "title": "Unraveling SITT: Social Influence Technique Taxonomy and Detection with LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this work we present the Social Influence Technique Taxonomy (SITT), a\ncomprehensive framework of 58 empirically grounded techniques organized into\nnine categories, designed to detect subtle forms of social influence in textual\ncontent. We also investigate the LLMs ability to identify various forms of\nsocial influence. Building on interdisciplinary foundations, we construct the\nSITT dataset -- a 746-dialogue corpus annotated by 11 experts in Polish and\ntranslated into English -- to evaluate the ability of LLMs to identify these\ntechniques. Using a hierarchical multi-label classification setup, we benchmark\nfive LLMs, including GPT-4o, Claude 3.5, Llama-3.1, Mixtral, and PLLuM. Our\nresults show that while some models, notably Claude 3.5, achieved moderate\nsuccess (F1 score = 0.45 for categories), overall performance of models remains\nlimited, particularly for context-sensitive techniques. The findings\ndemonstrate key limitations in current LLMs' sensitivity to nuanced linguistic\ncues and underscore the importance of domain-specific fine-tuning. This work\ncontributes a novel resource and evaluation example for understanding how LLMs\ndetect, classify, and potentially replicate strategies of social influence in\nnatural dialogues.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u793e\u4f1a\u5f71\u54cd\u6280\u672f\u5206\u7c7b\u6cd5(SITT)\uff0c\u5305\u542b58\u79cd\u6280\u672f\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u6807\u6ce8\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u8bc6\u522b\u8fd9\u4e9b\u6280\u672f\u7684\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u68c0\u6d4b\u6587\u672c\u4e2d\u5fae\u5999\u7684\u793e\u4f1a\u5f71\u54cd\u5f62\u5f0f\uff0c\u5e76\u8bc4\u4f30LLMs\u8bc6\u522b\u8fd9\u4e9b\u5f62\u5f0f\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6784\u5efaSITT\u6570\u636e\u96c6\uff08746\u5bf9\u8bdd\uff0c\u7531\u4e13\u5bb6\u6807\u6ce8\uff09\uff0c\u5e76\u5728\u5206\u5c42\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u4e2d\u6d4b\u8bd55\u79cdLLMs\u7684\u6027\u80fd\u3002", "result": "Claude3.5\u8868\u73b0\u8f83\u4f18\uff08F1=0.45\uff09\uff0c\u4f46\u6574\u4f53\u6a21\u578b\u8bc6\u522b\u80fd\u529b\u6709\u9650\uff0c\u5c24\u5176\u662f\u5bf9\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u6280\u672f\u3002", "conclusion": "\u5f53\u524dLLMs\u5bf9\u5fae\u5999\u8bed\u8a00\u7ebf\u7d22\u7684\u654f\u611f\u6027\u6709\u9650\uff0c\u5f3a\u8c03\u4e86\u9886\u57df\u5fae\u8c03\u7684\u91cd\u8981\u6027\u3002", "keywords": "\u793e\u4f1a\u5f71\u54cd,LLMs,\u5206\u7c7b\u6cd5,\u6570\u636e\u96c6,\u591a\u6807\u7b7e\u5206\u7c7b"}}
{"id": "2506.00073", "pdf": "https://arxiv.org/pdf/2506.00073", "abs": "https://arxiv.org/abs/2506.00073", "authors": ["Shenzhe Zhu", "Jiao Sun", "Yi Nian", "Tobin South", "Alex Pentland", "Jiaxin Pei"], "title": "The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.MA"], "comment": null, "summary": "AI agents are increasingly used in consumer-facing applications to assist\nwith tasks such as product search, negotiation, and transaction execution. In\nthis paper, we explore a future scenario where both consumers and merchants\nauthorize AI agents to fully automate negotiations and transactions. We aim to\nanswer two key questions: (1) Do different LLM agents vary in their ability to\nsecure favorable deals for users? (2) What risks arise from fully automating\ndeal-making with AI agents in consumer markets? To address these questions, we\ndevelop an experimental framework that evaluates the performance of various LLM\nagents in real-world negotiation and transaction settings. Our findings reveal\nthat AI-mediated deal-making is an inherently imbalanced game -- different\nagents achieve significantly different outcomes for their users. Moreover,\nbehavioral anomalies in LLMs can result in financial losses for both consumers\nand merchants, such as overspending or accepting unreasonable deals. These\nresults underscore that while automation can improve efficiency, it also\nintroduces substantial risks. Users should exercise caution when delegating\nbusiness decisions to AI agents.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u4ee3\u7406\u5728\u6d88\u8d39\u8005\u5e02\u573a\u4e2d\u81ea\u52a8\u5316\u4ea4\u6613\u548c\u8c08\u5224\u7684\u6f5c\u529b\u53ca\u98ce\u9669\uff0c\u53d1\u73b0\u4e0d\u540c\u4ee3\u7406\u7684\u6027\u80fd\u5dee\u5f02\u663e\u8457\u4e14\u53ef\u80fd\u5e26\u6765\u8d22\u52a1\u635f\u5931\u3002", "motivation": "\u7814\u7a76AI\u4ee3\u7406\u5728\u6d88\u8d39\u8005\u5e02\u573a\u4e2d\u81ea\u52a8\u5316\u4ea4\u6613\u548c\u8c08\u5224\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u7531\u6b64\u5e26\u6765\u7684\u98ce\u9669\u3002", "method": "\u5f00\u53d1\u5b9e\u9a8c\u6846\u67b6\uff0c\u8bc4\u4f30\u4e0d\u540cLLM\u4ee3\u7406\u5728\u5b9e\u9645\u8c08\u5224\u548c\u4ea4\u6613\u4e2d\u7684\u8868\u73b0\u3002", "result": "AI\u4ee3\u7406\u7684\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u53ef\u80fd\u5bfc\u81f4\u8d22\u52a1\u635f\u5931\uff08\u5982\u8fc7\u5ea6\u652f\u51fa\u6216\u4e0d\u5408\u7406\u4ea4\u6613\uff09\u3002", "conclusion": "\u81ea\u52a8\u5316\u867d\u63d0\u5347\u6548\u7387\uff0c\u4f46\u4e5f\u5f15\u5165\u98ce\u9669\uff0c\u7528\u6237\u9700\u8c28\u614e\u6388\u6743AI\u4ee3\u7406\u505a\u5546\u4e1a\u51b3\u7b56\u3002", "keywords": "AI\u4ee3\u7406, \u81ea\u52a8\u5316\u8c08\u5224, \u4ea4\u6613\u98ce\u9669, LLM\u4ee3\u7406, \u6d88\u8d39\u8005\u5e02\u573a"}}
{"id": "2506.00039", "pdf": "https://arxiv.org/pdf/2506.00039", "abs": "https://arxiv.org/abs/2506.00039", "authors": ["Behtom Adeli", "John Mclinden", "Pankaj Pandey", "Ming Shao", "Yalda Shahriari"], "title": "AbsoluteNet: A Deep Learning Neural Network to Classify Cerebral Hemodynamic Responses of Auditory Processing", "categories": ["cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "In recent years, deep learning (DL) approaches have demonstrated promising\nresults in decoding hemodynamic responses captured by functional near-infrared\nspectroscopy (fNIRS), particularly in the context of brain-computer interface\n(BCI) applications. This work introduces AbsoluteNet, a novel deep learning\narchitecture designed to classify auditory event-related responses recorded\nusing fNIRS. The proposed network is built upon principles of spatio-temporal\nconvolution and customized activation functions. Our model was compared against\nseveral models, namely fNIRSNET, MDNN, DeepConvNet, and ShallowConvNet. The\nresults showed that AbsoluteNet outperforms existing models, reaching 87.0%\naccuracy, 84.8% sensitivity, and 89.2% specificity in binary classification,\nsurpassing fNIRSNET, the second-best model, by 3.8% in accuracy. These findings\nunderscore the effectiveness of our proposed deep learning model in decoding\nhemodynamic responses related to auditory processing and highlight the\nimportance of spatio-temporal feature aggregation and customized activation\nfunctions to better fit fNIRS dynamics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAbsoluteNet\u7684\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u5206\u7c7b\u901a\u8fc7fNIRS\u8bb0\u5f55\u7684\u542c\u89c9\u4e8b\u4ef6\u76f8\u5173\u54cd\u5e94\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u89e3\u7801\u8840\u6db2\u52a8\u529b\u5b66\u54cd\u5e94\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u89e3\u7801fNIRS\u6355\u83b7\u7684\u8840\u6db2\u52a8\u529b\u5b66\u54cd\u5e94\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u8111\u673a\u63a5\u53e3\u5e94\u7528\u4e2d\uff0c\u56e0\u6b64\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "AbsoluteNet\u57fa\u4e8e\u65f6\u7a7a\u5377\u79ef\u548c\u5b9a\u5236\u5316\u6fc0\u6d3b\u51fd\u6570\u7684\u539f\u7406\u6784\u5efa\uff0c\u4e0efNIRSNET\u3001MDNN\u3001DeepConvNet\u548cShallowConvNet\u7b49\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "AbsoluteNet\u5728\u4e8c\u5143\u5206\u7c7b\u4e2d\u8fbe\u523087.0%\u7684\u51c6\u786e\u7387\u300184.8%\u7684\u654f\u611f\u6027\u548c89.2%\u7684\u7279\u5f02\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "AbsoluteNet\u5728\u89e3\u7801\u4e0e\u542c\u89c9\u5904\u7406\u76f8\u5173\u7684\u8840\u6db2\u52a8\u529b\u5b66\u54cd\u5e94\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5f3a\u8c03\u4e86\u65f6\u7a7a\u7279\u5f81\u805a\u5408\u548c\u5b9a\u5236\u5316\u6fc0\u6d3b\u51fd\u6570\u7684\u91cd\u8981\u6027\u3002", "keywords": "\u6df1\u5ea6\u5b66\u4e60, fNIRS, \u8111\u673a\u63a5\u53e3, \u65f6\u7a7a\u5377\u79ef, \u5b9a\u5236\u5316\u6fc0\u6d3b\u51fd\u6570"}}
{"id": "2506.00064", "pdf": "https://arxiv.org/pdf/2506.00064", "abs": "https://arxiv.org/abs/2506.00064", "authors": ["Jiayi Zeng", "Yizhe Feng", "Mengliang He", "Wenhui Lei", "Wei Zhang", "Zeming Liu", "Xiaoming Shi", "Aimin Zhou"], "title": "Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated significant advancements in\nerror handling. Current error-handling works are performed in a passive manner,\nwith explicit error-handling instructions. However, in real-world scenarios,\nexplicit error-handling instructions are usually unavailable. In this paper,\nour work identifies this challenge as how to conduct proactive error handling\nwithout explicit error handling instructions. To promote further research, this\nwork introduces a new benchmark, termed Mis-prompt, consisting of four\nevaluation tasks, an error category taxonomy, and a new evaluation dataset.\nFurthermore, this work analyzes current LLMs' performance on the benchmark, and\nthe experimental results reveal that current LLMs show poor performance on\nproactive error handling, and SFT on error handling instances improves LLMs'\nproactive error handling capabilities. The dataset will be publicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u4e3b\u52a8\u9519\u8bef\u5904\u7406\u65b9\u9762\u7684\u6311\u6218\uff0c\u63d0\u51fa\u65b0\u57fa\u51c6Mis-prompt\uff0c\u53d1\u73b0\u5f53\u524dLLMs\u8868\u73b0\u4e0d\u4f73\u4f46\u53ef\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLMs\u7684\u9519\u8bef\u5904\u7406\u4f9d\u8d56\u663e\u5f0f\u6307\u4ee4\uff0c\u800c\u73b0\u5b9e\u573a\u666f\u4e2d\u8fd9\u4e9b\u6307\u4ee4\u901a\u5e38\u7f3a\u5931\uff0c\u56e0\u6b64\u7814\u7a76\u5982\u4f55\u5b9e\u73b0\u65e0\u9700\u663e\u5f0f\u6307\u4ee4\u7684\u4e3b\u52a8\u9519\u8bef\u5904\u7406\u662f\u5173\u952e\u3002", "method": "\u63d0\u51fa\u65b0\u57fa\u51c6Mis-prompt\uff0c\u5305\u542b\u56db\u9879\u8bc4\u6d4b\u4efb\u52a1\u3001\u9519\u8bef\u5206\u7c7b\u6cd5\u548c\u6570\u636e\u96c6\uff0c\u5e76\u5b9e\u9a8c\u5206\u6790LLMs\u8868\u73b0\u53ca\u76d1\u7763\u5fae\u8c03\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u524dLLMs\u5728\u4e3b\u52a8\u9519\u8bef\u5904\u7406\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u4f46\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347\u80fd\u529b\u3002", "conclusion": "\u4e3b\u52a8\u9519\u8bef\u5904\u7406\u662fLLMs\u7684\u91cd\u8981\u6311\u6218\uff0c\u76d1\u7763\u5fae\u8c03\u548c\u516c\u5f00\u6570\u636e\u96c6\u53ef\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u9519\u8bef\u5904\u7406, \u4e3b\u52a8\u9519\u8bef\u5904\u7406, \u76d1\u7763\u5fae\u8c03, \u8bc4\u6d4b\u57fa\u51c6"}}
{"id": "2506.00140", "pdf": "https://arxiv.org/pdf/2506.00140", "abs": "https://arxiv.org/abs/2506.00140", "authors": ["Jesse Thibodeau", "Hadi Nekoei", "Afaf Ta\u00efk", "Janarthanan Rajendran", "Golnoosh Farnadi"], "title": "Balancing Profit and Fairness in Risk-Based Pricing Markets", "categories": ["cs.AI", "cs.LG", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Dynamic, risk-based pricing can systematically exclude vulnerable consumer\ngroups from essential resources such as health insurance and consumer credit.\nWe show that a regulator can realign private incentives with social objectives\nthrough a learned, interpretable tax schedule. First, we provide a formal\nproposition that bounding each firm's \\emph{local} demographic gap implicitly\nbounds the \\emph{global} opt-out disparity, motivating firm-level penalties.\nBuilding on this insight we introduce \\texttt{MarketSim} -- an open-source,\nscalable simulator of heterogeneous consumers and profit-maximizing firms --\nand train a reinforcement learning (RL) social planner (SP) that selects a\nbracketed fairness-tax while remaining close to a simple linear prior via an\n$\\mathcal{L}_1$ regularizer. The learned policy is thus both transparent and\neasily interpretable. In two empirically calibrated markets, i.e., U.S.\nhealth-insurance and consumer-credit, our planner simultaneously raises\ndemand-fairness by up to $16\\%$ relative to unregulated Free Market while\noutperforming a fixed linear schedule in terms of social welfare without\nexplicit coordination. These results illustrate how AI-assisted regulation can\nconvert a competitive social dilemma into a win-win equilibrium, providing a\nprincipled and practical framework for fairness-aware market oversight.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u52a8\u6001\u98ce\u9669\u5b9a\u4ef7\u5bf9\u5f31\u52bf\u7fa4\u4f53\u7684\u6392\u65a5\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u89e3\u91ca\u7a0e\u6536\u653f\u7b56\u6765\u8c03\u8282\u5e02\u573a\uff0c\u63d0\u5347\u516c\u5e73\u6027\u548c\u793e\u4f1a\u798f\u5229\u3002", "motivation": "\u52a8\u6001\u98ce\u9669\u5b9a\u4ef7\u53ef\u80fd\u7cfb\u7edf\u6027\u5730\u6392\u65a5\u5f31\u52bf\u7fa4\u4f53\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u76d1\u7ba1\u65b9\u6cd5\u6765\u5e73\u8861\u79c1\u4eba\u5229\u76ca\u4e0e\u793e\u4f1a\u76ee\u6807\u3002", "method": "\u901a\u8fc7Proposition\u9650\u5236\u4f01\u4e1a\u7684\u672c\u5730\u4eba\u53e3\u5dee\u8ddd\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684\u6a21\u62df\u5668MarketSim\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u793e\u4f1a\u89c4\u5212\u8005\uff08SP\uff09\u6765\u5236\u5b9a\u516c\u5e73\u7a0e\u6536\u653f\u7b56\u3002", "result": "\u5728\u6a21\u62df\u7684\u7f8e\u56fd\u533b\u7597\u4fdd\u9669\u548c\u6d88\u8d39\u4fe1\u8d37\u5e02\u573a\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c06\u9700\u6c42\u516c\u5e73\u6027\u63d0\u5347\u4e8616%\uff0c\u4e14\u4f18\u4e8e\u672a\u76d1\u7ba1\u7684\u81ea\u7531\u5e02\u573a\u548c\u56fa\u5b9a\u7ebf\u6027\u7a0e\u6536\u3002", "conclusion": "AI\u8f85\u52a9\u76d1\u7ba1\u53ef\u4ee5\u5c06\u7ade\u4e89\u6027\u793e\u4f1a\u56f0\u5883\u8f6c\u5316\u4e3a\u53cc\u8d62\u5747\u8861\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u516c\u5e73\u5e02\u573a\u76d1\u7ba1\u7684\u6846\u67b6\u3002", "keywords": "\u52a8\u6001\u5b9a\u4ef7, \u516c\u5e73\u6027, \u5f3a\u5316\u5b66\u4e60, \u5e02\u573a\u76d1\u7ba1, \u793e\u4f1a\u798f\u5229"}}
{"id": "2506.00131", "pdf": "https://arxiv.org/pdf/2506.00131", "abs": "https://arxiv.org/abs/2506.00131", "authors": ["Simon Sinong Zhan", "Qingyuan Wu", "Frank Yang", "Xiangyu Shi", "Chao Huang", "Qi Zhu"], "title": "Adapting Offline Reinforcement Learning with Online Delays", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Offline-to-online deployment of reinforcement-learning (RL) agents must\nbridge two gaps: (1) the sim-to-real gap, where real systems add latency and\nother imperfections not present in simulation, and (2) the interaction gap,\nwhere policies trained purely offline face out-of-distribution states during\nonline execution because gathering new interaction data is costly or risky.\nAgents therefore have to generalize from static, delay-free datasets to\ndynamic, delay-prone environments. Standard offline RL learns from delay-free\nlogs yet must act under delays that break the Markov assumption and hurt\nperformance. We introduce DT-CORL (Delay-Transformer belief policy Constrained\nOffline RL), an offline-RL framework built to cope with delayed dynamics at\ndeployment. DT-CORL (i) produces delay-robust actions with a transformer-based\nbelief predictor even though it never sees delayed observations during\ntraining, and (ii) is markedly more sample-efficient than na\\\"ive\nhistory-augmentation baselines. Experiments on D4RL benchmarks with several\ndelay settings show that DT-CORL consistently outperforms both\nhistory-augmentation and vanilla belief-based methods, narrowing the\nsim-to-real latency gap while preserving data efficiency.", "AI": {"tldr": "DT-CORL\u662f\u4e00\u79cd\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53d8\u538b\u5668\u4fe1\u5ff5\u9884\u6d4b\u5668\u5904\u7406\u5ef6\u8fdf\u52a8\u6001\uff0c\u63d0\u9ad8\u4e86\u90e8\u7f72\u65f6\u7684\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5ef6\u8fdf\u95ee\u9898\u548c\u4ea4\u4e92\u6570\u636e\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u53d8\u538b\u5668\u4fe1\u5ff5\u9884\u6d4b\u5668\u751f\u6210\u5ef6\u8fdf\u9c81\u68d2\u7684\u52a8\u4f5c\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u9ad8\u6548\u6027\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDT-CORL\u4f18\u4e8e\u5386\u53f2\u589e\u5f3a\u548c\u57fa\u4e8e\u4fe1\u5ff5\u7684\u65b9\u6cd5\u3002", "conclusion": "DT-CORL\u6709\u6548\u7f29\u5c0f\u4e86\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5ef6\u8fdf\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6570\u636e\u6548\u7387\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60\u3001\u79bb\u7ebfRL\u3001\u5ef6\u8fdf\u5904\u7406\u3001\u53d8\u538b\u5668\u3001\u4fe1\u5ff5\u9884\u6d4b"}}
{"id": "2506.00065", "pdf": "https://arxiv.org/pdf/2506.00065", "abs": "https://arxiv.org/abs/2506.00065", "authors": ["Dota Tianai Dong", "Yifan Luo", "Po-Ya Angela Wang", "Asli Ozyurek", "Paula Rubio-Fernandez"], "title": "You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than Vocabulary Words for Humans and Multimodal Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages", "summary": "Multimodal language models (MLMs) increasingly communicate in human-like\nways, yet their ability to use reference words remains largely overlooked\ndespite their ubiquity in everyday communication. Our study addresses this gap\nby comparing human and MLM use of three word classes with increasing cognitive\ndemands: vocabulary words, possessive pronouns (`mine' vs `yours'), and\ndemonstrative pronouns (`this one' vs `that one'). Evaluating seven\nstate-of-the-art MLMs against human participants, we observe a clear difficulty\nhierarchy: while MLMs approach human-level performance on the vocabulary task,\nthey show substantial deficits with possessives and demonstratives. Our\nanalysis reveals these difficulties stem from limitations in perspective-taking\nand spatial reasoning. Although prompt engineering improved model performance\non possessive use, demonstrative use remained well below human-level\ncompetence. These findings provide theoretical and empirical evidence that\nproducing grammatical forms requiring pragmatics and social cognition remains a\nclear challenge in current NLP systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4eba\u7c7b\u548c\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff08MLM\uff09\u5728\u4f7f\u7528\u53c2\u8003\u8bcd\uff08\u5982\u8bcd\u6c47\u3001\u7269\u4e3b\u4ee3\u8bcd\u548c\u6307\u793a\u4ee3\u8bcd\uff09\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0MLM\u5728\u8bcd\u6c47\u4efb\u52a1\u4e0a\u63a5\u8fd1\u4eba\u7c7b\uff0c\u4f46\u5728\u7269\u4e3b\u548c\u6307\u793a\u4ee3\u8bcd\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4e3b\u8981\u56e0\u89c6\u89d2\u548c\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002", "motivation": "\u63a2\u7d22MLM\u5728\u4f7f\u7528\u53c2\u8003\u8bcd\u65f6\u7684\u80fd\u529b\uff0c\u586b\u8865\u5176\u5728\u65e5\u5e38\u4ea4\u6d41\u4e2d\u666e\u904d\u6027\u4f46\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u9886\u57df\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u4eba\u7c7b\u548c\u4e03\u79cd\u5148\u8fdb\u7684MLM\u5728\u8bcd\u6c47\u3001\u7269\u4e3b\u4ee3\u8bcd\u548c\u6307\u793a\u4ee3\u8bcd\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u56f0\u96be\u539f\u56e0\u3002", "result": "MLM\u5728\u8bcd\u6c47\u4efb\u52a1\u4e0a\u8868\u73b0\u63a5\u8fd1\u4eba\u7c7b\uff0c\u4f46\u5728\u7269\u4e3b\u548c\u6307\u793a\u4ee3\u8bcd\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u63d0\u793a\u5de5\u7a0b\u4ec5\u90e8\u5206\u6539\u5584\u4e86\u7269\u4e3b\u4ee3\u8bcd\u7684\u4f7f\u7528\u3002", "conclusion": "\u5f53\u524dNLP\u7cfb\u7edf\u5728\u5904\u7406\u9700\u8981\u8bed\u7528\u548c\u793e\u4f1a\u8ba4\u77e5\u7684\u8bed\u6cd5\u5f62\u5f0f\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\u3002", "keywords": "\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b, \u53c2\u8003\u8bcd, \u7269\u4e3b\u4ee3\u8bcd, \u6307\u793a\u4ee3\u8bcd, \u89c6\u89d2\u63a8\u7406"}}
{"id": "2506.00169", "pdf": "https://arxiv.org/pdf/2506.00169", "abs": "https://arxiv.org/abs/2506.00169", "authors": ["Aziida Nanyonga", "Graham Wild"], "title": "Utilizing AI for Aviation Post-Accident Analysis Classification", "categories": ["cs.AI"], "comment": null, "summary": "The volume of textual data available in aviation safety reports presents a\nchallenge for timely and accurate analysis. This paper examines how Artificial\nIntelligence (AI) and, specifically, Natural Language Processing (NLP) can\nautomate the process of extracting valuable insights from this data, ultimately\nenhancing aviation safety. The paper reviews ongoing efforts focused on the\napplication of NLP and deep learning to aviation safety reports, with the goal\nof classifying the level of damage to an aircraft and identifying the phase of\nflight during which safety occurrences happen. Additionally, the paper explores\nthe use of Topic Modeling (TM) to uncover latent thematic structures within\naviation incident reports, aiming to identify recurring patterns and potential\nareas for safety improvement. The paper compares and contrasts the performance\nof various deep learning models and TM techniques applied to datasets from the\nNational Transportation Safety Board (NTSB) and the Australian Transport Safety\nBureau (ATSB), as well as the Aviation Safety Network (ASN), discussing the\nimpact of dataset size and source on the accuracy of the analysis. The findings\ndemonstrate that both NLP and deep learning, as well as TM, can significantly\nimprove the efficiency and accuracy of aviation safety analysis, paving the way\nfor more proactive safety management and risk mitigation strategies.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u548cNLP\u5982\u4f55\u81ea\u52a8\u5316\u5206\u6790\u822a\u7a7a\u5b89\u5168\u62a5\u544a\uff0c\u63d0\u5347\u5b89\u5168\u6027\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u6df1\u5ea6\u5b66\u4e60\u548c\u4e3b\u9898\u5efa\u6a21\u6280\u672f\u7684\u8868\u73b0\u3002", "motivation": "\u822a\u7a7a\u5b89\u5168\u62a5\u544a\u6570\u636e\u91cf\u5927\uff0c\u4f20\u7edf\u5206\u6790\u6548\u7387\u4f4e\uff0c\u9700\u81ea\u52a8\u5316\u5de5\u5177\u63d0\u5347\u5206\u6790\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408NLP\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u5bf9\u822a\u7a7a\u5b89\u5168\u62a5\u544a\u8fdb\u884c\u5206\u7c7b\u548c\u4e3b\u9898\u5efa\u6a21\uff0c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u548c\u6280\u672f\u3002", "result": "NLP\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u4e3b\u9898\u5efa\u6a21\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u6790\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u6709\u52a9\u4e8e\u66f4\u4e3b\u52a8\u7684\u5b89\u5168\u7ba1\u7406\u3002", "conclusion": "AI\u6280\u672f\u53ef\u6709\u6548\u63d0\u5347\u822a\u7a7a\u5b89\u5168\u5206\u6790\u7684\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u4e3a\u98ce\u9669\u7ba1\u7406\u63d0\u4f9b\u652f\u6301\u3002", "keywords": "\u822a\u7a7a\u5b89\u5168, NLP, \u6df1\u5ea6\u5b66\u4e60, \u4e3b\u9898\u5efa\u6a21"}}
{"id": "2506.00135", "pdf": "https://arxiv.org/pdf/2506.00135", "abs": "https://arxiv.org/abs/2506.00135", "authors": ["Idan Attias", "Steve Hanneke", "Arvind Ramaswami"], "title": "Tradeoffs between Mistakes and ERM Oracle Calls in Online and Transductive Online Learning", "categories": ["cs.LG", "cs.DS", "stat.ML"], "comment": null, "summary": "We study online and transductive online learning when the learner interacts\nwith the concept class only via Empirical Risk Minimization (ERM) or weak\nconsistency oracles on arbitrary instance subsets. This contrasts with standard\nonline models, where the learner knows the entire class. The ERM oracle returns\na hypothesis minimizing loss on a given subset, while the weak consistency\noracle returns a binary signal indicating whether the subset is realizable by\nsome concept. The learner is evaluated by the number of mistakes and oracle\ncalls. In the standard online setting with ERM access, we prove tight lower\nbounds in both realizable and agnostic cases: $\\Omega(2^{d_{VC}})$ mistakes and\n$\\Omega(\\sqrt{T 2^{d_{LD}}})$ regret, where $T$ is the number of timesteps and\n$d_{LD}$ is the Littlestone dimension. We further show that existing online\nlearning results with ERM access carry over to the weak consistency setting,\nincurring an additional $O(T)$ in oracle calls. We then consider the\ntransductive online model, where the instance sequence is known but labels are\nrevealed sequentially. For general Littlestone classes, we show that optimal\nrealizable and agnostic mistake bounds can be achieved using $O(T^{d_{VC}+1})$\nweak consistency oracle calls. On the negative side, we show that limiting the\nlearner to $\\Omega(T)$ weak consistency queries is necessary for transductive\nonline learnability, and that restricting the learner to $\\Omega(T)$ ERM\nqueries is necessary to avoid exponential dependence on the Littlestone\ndimension. Finally, for certain concept classes, we reduce oracle calls via\nrandomized algorithms while maintaining similar mistake bounds. In particular,\nfor Thresholds on an unknown ordering, $O(\\log T)$ ERM queries suffice; for\n$k$-Intervals, $O(T^3 2^{2k})$ weak consistency queries suffice.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\uff08ERM\uff09\u6216\u5f31\u4e00\u81f4\u6027\u9884\u8a00\u673a\u4e0e\u6982\u5ff5\u7c7b\u4ea4\u4e92\u7684\u5728\u7ebf\u5b66\u4e60\u548c\u8f6c\u6362\u5728\u7ebf\u5b66\u4e60\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5728ERM\u8bbf\u95ee\u4e0b\u7684\u7d27\u4e0b\u754c\uff0c\u5e76\u5c06\u73b0\u6709\u7ed3\u679c\u6269\u5c55\u5230\u5f31\u4e00\u81f4\u6027\u8bbe\u7f6e\u3002\u5728\u8f6c\u6362\u5728\u7ebf\u6a21\u578b\u4e2d\uff0c\u5c55\u793a\u4e86\u6700\u4f18\u9519\u8bef\u754c\u6240\u9700\u9884\u8a00\u673a\u8c03\u7528\u6b21\u6570\uff0c\u5e76\u6307\u51fa\u9650\u5236\u9884\u8a00\u673a\u8c03\u7528\u7684\u5fc5\u8981\u6027\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u7279\u5b9a\u6982\u5ff5\u7c7b\uff0c\u901a\u8fc7\u968f\u673a\u7b97\u6cd5\u51cf\u5c11\u4e86\u9884\u8a00\u673a\u8c03\u7528\u3002", "motivation": "\u7814\u7a76\u5728\u4ec5\u901a\u8fc7ERM\u6216\u5f31\u4e00\u81f4\u6027\u9884\u8a00\u673a\u4e0e\u6982\u5ff5\u7c7b\u4ea4\u4e92\u65f6\u7684\u5728\u7ebf\u5b66\u4e60\u6027\u80fd\uff0c\u586b\u8865\u6807\u51c6\u5728\u7ebf\u6a21\u578b\u4e2d\u7684\u77e5\u8bc6\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u8bc1\u660e\uff0c\u6bd4\u8f83ERM\u548c\u5f31\u4e00\u81f4\u6027\u9884\u8a00\u673a\u7684\u6548\u679c\uff0c\u5e76\u5728\u8f6c\u6362\u5728\u7ebf\u6a21\u578b\u4e2d\u63a2\u7d22\u9884\u8a00\u673a\u8c03\u7528\u7684\u4f18\u5316\u3002\u968f\u673a\u7b97\u6cd5\u7528\u4e8e\u51cf\u5c11\u7279\u5b9a\u6982\u5ff5\u7c7b\u7684\u9884\u8a00\u673a\u8c03\u7528\u3002", "result": "\u8bc1\u660e\u5728ERM\u8bbf\u95ee\u4e0b\u7684\u7d27\u4e0b\u754c\uff08\u5982$\u03a9(2^{d_{VC}})$\u9519\u8bef\uff09\uff0c\u5c55\u793a\u4e86\u5f31\u4e00\u81f4\u6027\u8bbe\u7f6e\u589e\u52a0\u7684$O(T)$\u8c03\u7528\u6210\u672c\uff0c\u5e76\u5728\u8f6c\u6362\u5728\u7ebf\u6a21\u578b\u4e2d\u5b9e\u73b0\u6700\u4f18\u9519\u8bef\u754c\u3002\u968f\u673a\u7b97\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u7279\u5b9a\u7c7b\u7684\u9884\u8a00\u673a\u8c03\u7528\u3002", "conclusion": "\u4ec5\u901a\u8fc7ERM\u6216\u5f31\u4e00\u81f4\u6027\u9884\u8a00\u673a\u4ea4\u4e92\u7684\u5728\u7ebf\u5b66\u4e60\u6a21\u578b\u5728\u7406\u8bba\u4e0a\u662f\u53ef\u884c\u7684\uff0c\u4f46\u9884\u8a00\u673a\u8c03\u7528\u6b21\u6570\u7684\u4f18\u5316\u548c\u9650\u5236\u662f\u5173\u952e\u3002\u968f\u673a\u7b97\u6cd5\u4e3a\u7279\u5b9a\u7c7b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5728\u7ebf\u5b66\u4e60\uff0c\u8f6c\u6362\u5728\u7ebf\u5b66\u4e60\uff0c\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\uff0c\u5f31\u4e00\u81f4\u6027\uff0cLittlestone\u7ef4\u5ea6\uff0c\u9884\u8a00\u673a\u8c03\u7528"}}
{"id": "2506.00068", "pdf": "https://arxiv.org/pdf/2506.00068", "abs": "https://arxiv.org/abs/2506.00068", "authors": ["Afrozah Nadeem", "Mark Dras", "Usman Naseem"], "title": "Probing Politico-Economic Bias in Multilingual Large Language Models: A Cultural Analysis of Low-Resource Pakistani Languages", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Large Language Models (LLMs) are increasingly shaping public discourse, yet\ntheir politico-economic biases remain underexamined in non-Western and\nlow-resource multilingual contexts. This paper presents a systematic analysis\nof political bias in 13 state-of-the-art LLMs across five low-resource\nlanguages spoken in Pakistan: Urdu, Punjabi, Sindhi, Balochi, and Pashto. We\npropose a novel framework that integrates an adapted Political Compass Test\n(PCT) with a multi-level framing analysis. Our method combines quantitative\nassessment of political orientation across economic (left-right) and social\n(libertarian-authoritarian) axes with qualitative analysis of framing through\ncontent, style, and emphasis. We further contextualize this analysis by\naligning prompts with 11 key socio-political themes relevant to Pakistani\nsociety. Our results reveal that LLMs predominantly align with liberal-left\nvalues, echoing Western training data influences, but exhibit notable shifts\ntoward authoritarian framing in regional languages, suggesting strong cultural\nmodulation effects. We also identify consistent model-specific bias signatures\nand language-conditioned variations in ideological expression. These findings\nshow the urgent need for culturally grounded, multilingual bias auditing\nframeworks.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e8613\u79cd\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5df4\u57fa\u65af\u5766\u4e94\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u653f\u6cbb\u504f\u89c1\uff0c\u63ed\u793a\u5176\u666e\u904d\u504f\u5411\u4e8e\u81ea\u7531\u5de6\u7ffc\u4ef7\u503c\u89c2\uff0c\u4f46\u5728\u533a\u57df\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u660e\u663e\u7684\u5a01\u6743\u4e3b\u4e49\u503e\u5411\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u516c\u5171\u8bdd\u8bed\u7684\u5f71\u54cd\u65e5\u76ca\u663e\u8457\uff0c\u4f46\u5176\u5728\u975e\u897f\u65b9\u548c\u4f4e\u8d44\u6e90\u591a\u8bed\u8a00\u8bed\u5883\u4e2d\u7684\u653f\u6cbb\u7ecf\u6d4e\u504f\u89c1\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u7ed3\u5408\u5b9a\u5236\u7684\u653f\u6cbb\u6307\u5357\u9488\u6d4b\u8bd5\uff08PCT\uff09\u548c\u591a\u5c42\u6b21\u6846\u67b6\u5206\u6790\uff0c\u4ece\u7ecf\u6d4e\uff08\u5de6-\u53f3\uff09\u548c\u793e\u4f1a\uff08\u81ea\u7531-\u5a01\u6743\uff09\u8f74\u7ebf\u8fdb\u884c\u5b9a\u91cf\u8bc4\u4f30\uff0c\u5e76\u8f85\u4ee5\u5185\u5bb9\u3001\u98ce\u683c\u548c\u91cd\u70b9\u7684\u5b9a\u6027\u5206\u6790\u3002", "result": "LLMs\u4e3b\u8981\u53d7\u897f\u65b9\u8bad\u7ec3\u6570\u636e\u5f71\u54cd\uff0c\u504f\u5411\u81ea\u7531\u5de6\u7ffc\u4ef7\u503c\u89c2\uff0c\u4f46\u5728\u533a\u57df\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u5a01\u6743\u4e3b\u4e49\u503e\u5411\uff0c\u5b58\u5728\u6587\u5316\u8c03\u5236\u6548\u5e94\u548c\u6a21\u578b\u7279\u5b9a\u504f\u89c1\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5efa\u7acb\u57fa\u4e8e\u6587\u5316\u7684\u591a\u8bed\u8a00\u504f\u89c1\u5ba1\u8ba1\u6846\u67b6\u7684\u7d27\u8feb\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u653f\u6cbb\u504f\u89c1, \u4f4e\u8d44\u6e90\u8bed\u8a00, \u5df4\u57fa\u65af\u5766, \u6587\u5316\u8c03\u5236"}}
{"id": "2506.00178", "pdf": "https://arxiv.org/pdf/2506.00178", "abs": "https://arxiv.org/abs/2506.00178", "authors": ["Anirudh Nair", "Adi Banerjee", "Laurent Mombaerts", "Matthew Hagen", "Tarik Borogovac"], "title": "Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings", "categories": ["cs.AI", "cs.NE"], "comment": null, "summary": "Prompt engineering represents a critical bottleneck to harness the full\npotential of Large Language Models (LLMs) for solving complex tasks, as it\nrequires specialized expertise, significant trial-and-error, and manual\nintervention. This challenge is particularly pronounced for tasks involving\nsubjective quality assessment, where defining explicit optimization objectives\nbecomes fundamentally problematic. Existing automated prompt optimization\nmethods falter in these scenarios, as they typically require well-defined\ntask-specific numerical fitness functions or rely on generic templates that\ncannot capture the nuanced requirements of complex use cases. We introduce\nDEEVO (DEbate-driven EVOlutionary prompt optimization), a novel framework that\nguides prompt evolution through a debate-driven evaluation with an Elo-based\nselection. Contrary to prior work, DEEVOs approach enables exploration of the\ndiscrete prompt space while preserving semantic coherence through intelligent\ncrossover and strategic mutation operations that incorporate debate-based\nfeedback, combining elements from both successful and unsuccessful prompts\nbased on identified strengths rather than arbitrary splicing. Using Elo ratings\nas a fitness proxy, DEEVO simultaneously drives improvement and preserves\nvaluable diversity in the prompt population. Experimental results demonstrate\nthat DEEVO significantly outperforms both manual prompt engineering and\nalternative state-of-the-art optimization approaches on open-ended tasks and\nclose-ended tasks despite using no ground truth feedback. By connecting LLMs\nreasoning capabilities with adaptive optimization, DEEVO represents a\nsignificant advancement in prompt optimization research by eliminating the need\nof predetermined metrics to continuously improve AI systems.", "AI": {"tldr": "DEEVO\u6846\u67b6\u901a\u8fc7\u8fa9\u8bba\u9a71\u52a8\u7684\u8fdb\u5316\u548cElo\u8bc4\u5206\u4f18\u5316\u63d0\u793a\u5de5\u7a0b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u624b\u52a8\u548c\u5176\u4ed6\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "motivation": "\u63d0\u793a\u5de5\u7a0b\u662f\u53d1\u6325\u5927\u8bed\u8a00\u6a21\u578b\u6f5c\u529b\u7684\u5173\u952e\u74f6\u9888\uff0c\u5c24\u5176\u5728\u4e3b\u89c2\u4efb\u52a1\u4e2d\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u56e0\u4f9d\u8d56\u660e\u786e\u6307\u6807\u6216\u6a21\u677f\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "DEEVO\u4f7f\u7528\u8fa9\u8bba\u9a71\u52a8\u8bc4\u4f30\u548cElo\u8bc4\u5206\uff0c\u901a\u8fc7\u667a\u80fd\u4ea4\u53c9\u548c\u7b56\u7565\u53d8\u5f02\u64cd\u4f5c\u4f18\u5316\u63d0\u793a\uff0c\u4fdd\u7559\u8bed\u4e49\u8fde\u8d2f\u6027\u5e76\u63d0\u5347\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDEEVO\u5728\u5f00\u653e\u548c\u5c01\u95ed\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u624b\u52a8\u548c\u5176\u4ed6\u4f18\u5316\u65b9\u6cd5\uff0c\u65e0\u9700\u771f\u5b9e\u53cd\u9988\u5373\u53ef\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "DEEVO\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u81ea\u9002\u5e94\u4f18\u5316\uff0c\u4e3a\u63d0\u793a\u5de5\u7a0b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7a81\u7834\u3002", "keywords": "\u63d0\u793a\u5de5\u7a0b, \u5927\u8bed\u8a00\u6a21\u578b, \u8fdb\u5316\u4f18\u5316, \u8fa9\u8bba\u9a71\u52a8, Elo\u8bc4\u5206"}}
{"id": "2506.00136", "pdf": "https://arxiv.org/pdf/2506.00136", "abs": "https://arxiv.org/abs/2506.00136", "authors": ["Magdalena Proszewska", "Nikolay Malkin", "N. Siddharth"], "title": "On Designing Diffusion Autoencoders for Efficient Generation and Representation Learning", "categories": ["cs.LG"], "comment": "21 pages, 10 tables, 15 figures", "summary": "Diffusion autoencoders (DAs) are variants of diffusion generative models that\nuse an input-dependent latent variable to capture representations alongside the\ndiffusion process. These representations, to varying extents, can be used for\ntasks such as downstream classification, controllable generation, and\ninterpolation. However, the generative performance of DAs relies heavily on how\nwell the latent variables can be modelled and subsequently sampled from. Better\ngenerative modelling is also the primary goal of another class of diffusion\nmodels -- those that learn their forward (noising) process. While effective at\nadjusting the noise process in an input-dependent manner, they must satisfy\nadditional constraints derived from the terminal conditions of the diffusion\nprocess. Here, we draw a connection between these two classes of models and\nshow that certain design decisions (latent variable choice, conditioning\nmethod, etc.) in the DA framework -- leading to a model we term DMZ -- allow us\nto obtain the best of both worlds: effective representations as evaluated on\ndownstream tasks, including domain transfer, as well as more efficient\nmodelling and generation with fewer denoising steps compared to standard DMs.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u8ba8\u8bba\u4e86\u6269\u6563\u81ea\u7f16\u7801\u5668\uff08DAs\uff09\u53ca\u5176\u53d8\u4f53DMZ\u901a\u8fc7\u4f18\u5316\u6f5c\u53d8\u91cf\u8bbe\u8ba1\u548c\u6761\u4ef6\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u6709\u6548\u8868\u5f81\u548c\u66f4\u9ad8\u6548\u7684\u751f\u6210\u5efa\u6a21\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u6269\u6563\u81ea\u7f16\u7801\u5668\u4e2d\u6f5c\u53d8\u91cf\u5efa\u6a21\u548c\u6837\u672c\u751f\u6210\u7684\u95ee\u9898\uff0c\u540c\u65f6\u7ed3\u5408\u53e6\u4e00\u7c7b\u6269\u6563\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u8bbe\u8ba1\u3002", "method": "\u901a\u8fc7\u5206\u6790DA\u6846\u67b6\u4e2d\u7684\u8bbe\u8ba1\u51b3\u7b56\uff08\u5982\u6f5c\u53d8\u91cf\u9009\u62e9\u548c\u6761\u4ef6\u65b9\u6cd5\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u6a21\u578bDMZ\uff0c\u7ed3\u5408\u4e86\u4e24\u7c7b\u6269\u6563\u6a21\u578b\u7684\u4f18\u70b9\u3002", "result": "DMZ\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u9886\u57df\u8fc1\u79fb\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u751f\u6210\u6548\u7387\u66f4\u9ad8\uff0c\u51cf\u5c11\u4e86\u53bb\u566a\u6b65\u9aa4\u7684\u9700\u6c42\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u8bbe\u8ba1\u51b3\u7b56\uff0cDMZ\u6a21\u578b\u5728\u8868\u5f81\u80fd\u529b\u548c\u751f\u6210\u6548\u7387\u4e0a\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u6269\u6563\u81ea\u7f16\u7801\u5668, \u6f5c\u53d8\u91cf, \u751f\u6210\u6a21\u578b, \u9886\u57df\u8fc1\u79fb, \u9ad8\u6548\u5efa\u6a21"}}
{"id": "2506.00069", "pdf": "https://arxiv.org/pdf/2506.00069", "abs": "https://arxiv.org/abs/2506.00069", "authors": ["Robert Hankache", "Kingsley Nketia Acheampong", "Liang Song", "Marek Brynda", "Raad Khraishi", "Greig A. Cowan"], "title": "Evaluating the Sensitivity of LLMs to Prior Context", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in multi-turn\ndialogue and other sustained interactive scenarios, it is essential to\nunderstand how extended context affects their performance. Popular benchmarks,\nfocusing primarily on single-turn question answering (QA) tasks, fail to\ncapture the effects of multi-turn exchanges. To address this gap, we introduce\na novel set of benchmarks that systematically vary the volume and nature of\nprior context. We evaluate multiple conventional LLMs, including GPT, Claude,\nand Gemini, across these benchmarks to measure their sensitivity to contextual\nvariations. Our findings reveal that LLM performance on multiple-choice\nquestions can degrade dramatically in multi-turn interactions, with performance\ndrops as large as 73% for certain models. Even highly capable models such as\nGPT-4o exhibit up to a 32% decrease in accuracy. Notably, the relative\nperformance of larger versus smaller models is not always predictable.\nMoreover, the strategic placement of the task description within the context\ncan substantially mitigate performance drops, improving the accuracy by as much\nas a factor of 3.5. These findings underscore the need for robust strategies to\ndesign, evaluate, and mitigate context-related sensitivity in LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5173\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u6027\u80fd\u4e0b\u964d\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u591a\u8f6e\u4ea4\u4e92\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u63a2\u8ba8\u4e86\u4efb\u52a1\u63cf\u8ff0\u4f4d\u7f6e\u5bf9\u6027\u80fd\u7684\u6539\u5584\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u586b\u8865\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u8bc4\u4f30\u591a\u8f6e\u5bf9\u8bdd\u5bf9LLM\u6027\u80fd\u5f71\u54cd\u7684\u7a7a\u767d\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u7406\u89e3\u4e0a\u4e0b\u6587\u5bf9\u6a21\u578b\u8868\u73b0\u7684\u4f5c\u7528\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u5957\u65b0\u578b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u6027\u5730\u53d8\u5316\u4e0a\u4e0b\u6587\u7684\u91cf\u548c\u6027\u8d28\uff0c\u5e76\u8bc4\u4f30\u4e86\u5305\u62ecGPT\u3001Claude\u548cGemini\u5728\u5185\u7684\u591a\u4e2aLLM\u5728\u8fd9\u4e9b\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u591a\u8f6e\u5bf9\u8bdd\u4e2dLLM\u5728\u591a\u9009\u9898\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u53ef\u80fd\u4e0b\u964d\u9ad8\u8fbe73%\uff0c\u5373\u4f7f\u662fGPT-4o\u8fd9\u6837\u7684\u9ad8\u6027\u80fd\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u6700\u591a32%\u7684\u51c6\u786e\u7387\u4e0b\u964d\u3002\u4efb\u52a1\u63cf\u8ff0\u7684\u4f4d\u7f6e\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u9700\u8981\u66f4\u5065\u58ee\u7684\u7b56\u7565\u6765\u8bbe\u8ba1\u3001\u8bc4\u4f30\u548c\u7f13\u89e3LLM\u5bf9\u4e0a\u4e0b\u6587\u7684\u654f\u611f\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u591a\u8f6e\u5bf9\u8bdd\u3001\u57fa\u51c6\u6d4b\u8bd5\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u6027"}}
{"id": "2506.00189", "pdf": "https://arxiv.org/pdf/2506.00189", "abs": "https://arxiv.org/abs/2506.00189", "authors": ["Di Zhang", "Weida Wang", "Junxian Li", "Xunzhi Wang", "Jiatong Li", "Jianbo Wu", "Jingdi Lei", "Haonan He", "Peng Ye", "Shufei Zhang", "Wanli Ouyang", "Yuqiang Li", "Dongzhan Zhou"], "title": "Control-R: Towards controllable test-time scaling", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "This paper target in addressing the challenges of underthinking and\noverthinking in long chain-of-thought (CoT) reasoning for Large Reasoning\nModels (LRMs) by introducing Reasoning Control Fields (RCF)--a novel test-time\napproach that injects structured control signals to guide reasoning from a tree\nsearch perspective. RCF enables models to adjust reasoning effort according to\ngiven control conditions when solving complex tasks. Additionally, we present\nthe Control-R-4K dataset, which consists of challenging problems annotated with\ndetailed reasoning processes and corresponding control fields. To further\nenhance reasoning control, we propose a Conditional Distillation Finetuning\n(CDF) method, which trains model--particularly Control-R-32B--to effectively\nadjust reasoning effort during test time. Experimental results on benchmarks\nsuch as AIME2024 and MATH500 demonstrate that our approach achieves\nstate-of-the-art performance at the 32B scale while enabling a controllable\nLong CoT reasoning process (L-CoT). Overall, this work introduces an effective\nparadigm for controllable test-time scaling reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff08RCF\uff09\u6765\u89e3\u51b3\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u63a7\u5236\u4fe1\u53f7\u8c03\u6574\u63a8\u7406\u52aa\u529b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u51fa\u73b0\u7684\u8fc7\u5ea6\u6216\u4e0d\u8db3\u601d\u8003\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u63a8\u7406\u63a7\u5236\u57df\uff08RCF\uff09\u548c\u6761\u4ef6\u84b8\u998f\u5fae\u8c03\uff08CDF\uff09\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7Control-R-4K\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728AIME2024\u548cMATH500\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e8632B\u89c4\u6a21\u7684\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u4e3a\u53ef\u63a7\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u8303\u4f8b\u3002", "keywords": "\u957f\u94fe\u601d\u7ef4\u63a8\u7406,\u63a8\u7406\u63a7\u5236\u57df,\u6761\u4ef6\u84b8\u998f\u5fae\u8c03,\u53ef\u63a7\u63a8\u7406"}}
{"id": "2506.00152", "pdf": "https://arxiv.org/pdf/2506.00152", "abs": "https://arxiv.org/abs/2506.00152", "authors": ["Erfan Loghmani"], "title": "Aligning Language Models with Observational Data: Opportunities and Risks from a Causal Perspective", "categories": ["cs.LG", "econ.EM", "stat.ML", "I.2.6; I.2.7; H.4.0; J.4"], "comment": "10+12 pages, 8 figures", "summary": "Large language models are being widely used across industries to generate\ncontent that contributes directly to key performance metrics, such as\nconversion rates. Pretrained models, however, often fall short when it comes to\naligning with human preferences or optimizing for business objectives. As a\nresult, fine-tuning with good-quality labeled data is essential to guide models\nto generate content that achieves better results. Controlled experiments, like\nA/B tests, can provide such data, but they are often expensive and come with\nsignificant engineering and logistical challenges. Meanwhile, companies have\naccess to a vast amount of historical (observational) data that remains\nunderutilized. In this work, we study the challenges and opportunities of\nfine-tuning LLMs using observational data. We show that while observational\noutcomes can provide valuable supervision, directly fine-tuning models on such\ndata can lead them to learn spurious correlations. We present empirical\nevidence of this issue using various real-world datasets and propose\nDeconfoundLM, a method that explicitly removes the effect of known confounders\nfrom reward signals. Using simulation experiments, we demonstrate that\nDeconfoundLM improves the recovery of causal relationships and mitigates\nfailure modes found in fine-tuning methods that ignore or naively incorporate\nconfounding variables. Our findings highlight that while observational data\npresents risks, with the right causal corrections, it can be a powerful source\nof signal for LLM alignment. Please refer to the project page for code and\nrelated resources.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u5229\u7528\u5386\u53f2\u89c2\u6d4b\u6570\u636e\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6307\u51fa\u76f4\u63a5\u5fae\u8c03\u4f1a\u5bfc\u81f4\u865a\u5047\u76f8\u5173\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u53bb\u9664\u6df7\u6742\u53d8\u91cf\u5f71\u54cd\u7684DeconfoundLM\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u9002\u5e94\u4eba\u7c7b\u504f\u597d\u6216\u4e1a\u52a1\u76ee\u6807\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u800c\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u3002\u89c2\u6d4b\u6570\u636e\u867d\u4e30\u5bcc\uff0c\u4f46\u76f4\u63a5\u4f7f\u7528\u5b58\u5728\u98ce\u9669\uff0c\u9700\u89e3\u51b3\u6df7\u6742\u53d8\u91cf\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86DeconfoundLM\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u53bb\u9664\u5956\u52b1\u4fe1\u53f7\u4e2d\u7684\u5df2\u77e5\u6df7\u6742\u53d8\u91cf\u5f71\u54cd\uff0c\u6062\u590d\u56e0\u679c\u5173\u7cfb\u5e76\u6539\u8fdb\u6a21\u578b\u5fae\u8c03\u6548\u679c\u3002", "result": "\u5b9e\u8bc1\u8868\u660eDeconfoundLM\u80fd\u6709\u6548\u8bc6\u522b\u56e0\u679c\u5173\u7cfb\uff0c\u51cf\u5c11\u56e0\u6df7\u6742\u53d8\u91cf\u5bfc\u81f4\u7684\u6a21\u578b\u5931\u6548\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u6b63\u786e\u7684\u56e0\u679c\u6821\u6b63\uff0c\u89c2\u6d4b\u6570\u636e\u53ef\u4ee5\u4f5c\u4e3a\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u8d44\u6e90\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u89c2\u6d4b\u6570\u636e, \u6df7\u6742\u53d8\u91cf, \u5fae\u8c03, \u56e0\u679c\u63a8\u65ad"}}
{"id": "2506.00077", "pdf": "https://arxiv.org/pdf/2506.00077", "abs": "https://arxiv.org/abs/2506.00077", "authors": ["Edward Wang", "Tianyu Wang", "Avanti Athreya", "Vince Lyzinski", "Carey E. Priebe"], "title": "Gaussian mixture models as a proxy for interacting language models", "categories": ["cs.CL", "cs.LG", "stat.ML", "62R07"], "comment": null, "summary": "Large language models (LLMs) are a powerful tool with the ability to match\nhuman capabilities and behavior in many settings. Retrieval-augmented\ngeneration (RAG) further allows LLMs to generate diverse output depending on\nthe contents of their RAG database. This motivates their use in the social\nsciences to study human behavior between individuals when large-scale\nexperiments are infeasible. However, LLMs depend on complex, computationally\nexpensive algorithms. In this paper, we introduce interacting Gaussian mixture\nmodels (GMMs) as an alternative to similar frameworks using LLMs. We compare a\nsimplified model of GMMs to select experimental simulations of LLMs whose\nupdating and response depend on feedback from other LLMs. We find that\ninteracting GMMs capture important features of the dynamics in interacting\nLLMs, and we investigate key similarities and differences between interacting\nLLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture\nmodels, potential modifications, and future research directions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMMs\uff09\u4f5c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u66ff\u4ee3\u65b9\u6cd5\uff0c\u7528\u4e8e\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\uff0c\u5e76\u6bd4\u8f83\u4e24\u8005\u7684\u52a8\u6001\u7279\u5f81\u3002", "motivation": "\u4e3a\u7814\u7a76\u4eba\u7c7b\u884c\u4e3a\u63d0\u4f9b\u53ef\u6269\u5c55\u66ff\u4ee3\u65b9\u6848\uff0c\u907f\u514dLLMs\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u5f15\u5165\u4ea4\u4e92GMMs\uff0c\u901a\u8fc7\u7b80\u5316\u6a21\u578b\u4e0eLLMs\u7684\u5b9e\u9a8c\u6a21\u62df\u6bd4\u8f83\u52a8\u6001\u7279\u5f81\u3002", "result": "\u4ea4\u4e92GMMs\u80fd\u6355\u6349LLMs\u4ea4\u4e92\u52a8\u6001\u7684\u5173\u952e\u7279\u5f81\uff0c\u5e76\u63ed\u793a\u4e24\u8005\u7684\u5f02\u540c\u3002", "conclusion": "GMMs\u5728\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u7814\u7a76\u65b9\u5411\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u9ad8\u65af\u6df7\u5408\u6a21\u578b,\u793e\u4f1a\u79d1\u5b66,\u4ea4\u4e92\u52a8\u6001,\u8ba1\u7b97\u6548\u7387"}}
{"id": "2506.00202", "pdf": "https://arxiv.org/pdf/2506.00202", "abs": "https://arxiv.org/abs/2506.00202", "authors": ["Matthew Kam", "Cody Miller", "Miaoxin Wang", "Abey Tidwell", "Irene A. Lee", "Joyce Malyn-Smith", "Beatriz Perez", "Vikram Tiwari", "Joshua Kenitzer", "Andrew Macvean", "Erin Barrar"], "title": "What do professional software developers need to know to succeed in an age of Artificial Intelligence?", "categories": ["cs.AI"], "comment": "12 pages, 4 figures, software engineering education track of the 2025\n  ACM international conference on the foundations of software engineering,\n  includes supplementary material i.e. full 50-page occupational profile of the\n  AI-enhanced software developer", "summary": "Generative AI is showing early evidence of productivity gains for software\ndevelopers, but concerns persist regarding workforce disruption and deskilling.\nWe describe our research with 21 developers at the cutting edge of using AI,\nsummarizing 12 of their work goals we uncovered, together with 75 associated\ntasks and the skills & knowledge for each, illustrating how developers use AI\nat work. From all of these, we distilled our findings in the form of 5\ninsights. We found that the skills & knowledge to be a successful AI-enhanced\ndeveloper are organized into four domains (using Generative AI effectively,\ncore software engineering, adjacent engineering, and adjacent non-engineering)\ndeployed at critical junctures throughout a 6-step task workflow. In order to\n\"future proof\" developers for this age of AI, on-the-job learning initiatives\nand computer science degree programs will need to target both \"soft\" skills and\nthe technical skills & knowledge in all four domains to reskill, upskill and\nsafeguard against deskilling.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u5982\u4f55\u63d0\u5347\u5f00\u53d1\u8005\u751f\u4ea7\u529b\uff0c\u540c\u65f6\u5206\u6790\u4e86\u6f5c\u5728\u7684\u5de5\u4f5c\u51b2\u51fb\u4e0e\u6280\u80fd\u9000\u5316\u95ee\u9898\uff0c\u603b\u7ed3\u4e8621\u4f4d\u5f00\u53d1\u8005\u7684\u4f7f\u7528\u7ecf\u9a8c\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6280\u80fd\u57f9\u517b\u7684\u5efa\u8bae\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u8005\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u751f\u4ea7\u529b\u63d0\u5347\u3001\u5de5\u4f5c\u6d41\u7a0b\u53d8\u5316\u53ca\u6280\u80fd\u9700\u6c42\u7684\u6f14\u53d8\u3002", "method": "\u901a\u8fc7\u7814\u7a7621\u4f4d\u524d\u6cbf\u5f00\u53d1\u8005\u7684\u5b9e\u8df5\uff0c\u63d0\u70bc\u51fa12\u4e2a\u5de5\u4f5c\u76ee\u6807\u300175\u9879\u4efb\u52a1\u53ca\u76f8\u5173\u6280\u80fd\uff0c\u603b\u7ed3\u51fa5\u6761\u5173\u952e\u6d1e\u5bdf\u3002", "result": "\u53d1\u73b0AI\u589e\u5f3a\u578b\u5f00\u53d1\u8005\u9700\u8981\u56db\u7c7b\u6280\u80fd\uff08\u6709\u6548\u4f7f\u7528\u751f\u6210AI\u3001\u6838\u5fc3\u8f6f\u4ef6\u5de5\u7a0b\u3001\u76f8\u5173\u5de5\u7a0b\u3001\u975e\u5de5\u7a0b\u6280\u80fd\uff09\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u6280\u80fd\u57f9\u517b\u65b9\u5411\u3002", "conclusion": "\u4e3a\u9002\u5e94AI\u65f6\u4ee3\uff0c\u9700\u7ed3\u5408\u201c\u8f6f\u6280\u80fd\u201d\u4e0e\u6280\u672f\u6280\u80fd\u7684\u6559\u80b2\u4e0e\u57f9\u8bad\uff0c\u4ee5\u5e94\u5bf9\u6280\u80fd\u66f4\u65b0\u4e0e\u9000\u5316\u98ce\u9669\u3002", "keywords": "\u751f\u6210\u5f0fAI, \u5f00\u53d1\u8005\u751f\u4ea7\u529b, \u6280\u80fd\u57f9\u517b, \u5de5\u4f5c\u6d41\u7a0b, AI\u589e\u5f3a\u5f00\u53d1"}}
{"id": "2506.00158", "pdf": "https://arxiv.org/pdf/2506.00158", "abs": "https://arxiv.org/abs/2506.00158", "authors": ["Eli Chien", "Wei-Ning Chen", "Pan Li"], "title": "Privacy Amplification in Differentially Private Zeroth-Order Optimization with Hidden States", "categories": ["cs.LG"], "comment": null, "summary": "Zeroth-order optimization has emerged as a promising approach for fine-tuning\nlarge language models on domain-specific data, particularly under differential\nprivacy (DP) and memory constraints. While first-order methods have been\nextensively studied from a privacy perspective, the privacy analysis and\nalgorithmic design for zeroth-order methods remain significantly underexplored.\nA critical open question concerns hidden-state DP analysis: although convergent\nprivacy bounds are known for first-order methods, it has remained unclear\nwhether similar guarantees can be established for zeroth-order methods. In this\nwork, we provide an affirmative answer by proving a convergent DP bound for\nzeroth-order optimization. Our analysis generalizes the celebrated privacy\namplification-by-iteration framework to the setting of smooth loss functions in\nzeroth-order optimization. Furthermore, it induces better DP zeroth-order\nalgorithmic designs that are previously unknown to the literature.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u5728\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u4e0b\u7684\u6536\u655b\u6027\u548c\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u8bc1\u660e\u4e86\u5176\u9690\u79c1\u8fb9\u754c\uff0c\u5e76\u63d0\u51fa\u65b0\u7b97\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u4e00\u9636\u65b9\u6cd5\u5728\u9690\u79c1\u5206\u6790\u4e2d\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u96f6\u9636\u65b9\u6cd5\u7684\u9690\u79c1\u5206\u6790\u548c\u7b97\u6cd5\u8bbe\u8ba1\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u8ba8\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5c06\u9690\u79c1\u653e\u5927\u6846\u67b6\u63a8\u5e7f\u5230\u96f6\u9636\u4f18\u5316\u4e2d\u7684\u5e73\u6ed1\u635f\u5931\u51fd\u6570\uff0c\u5206\u6790\u4e86\u96f6\u9636\u4f18\u5316\u7684\u6536\u655b\u6027\uff0c\u5e76\u63d0\u51fa\u65b0\u7684DP\u96f6\u9636\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u96f6\u9636\u4f18\u5316\u7684\u6536\u655bDP\u8fb9\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u6b64\u524d\u6587\u732e\u4e2d\u672a\u77e5\u7684\u66f4\u597d\u7684DP\u96f6\u9636\u7b97\u6cd5\u8bbe\u8ba1\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86\u96f6\u9636\u65b9\u6cd5\u5728DP\u5206\u6790\u4e0a\u7684\u7a7a\u767d\uff0c\u4e3a\u9886\u57df\u7279\u5b9a\u6570\u636e\u7684\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u96f6\u9636\u4f18\u5316\uff0c\u5dee\u5206\u9690\u79c1\uff0c\u9690\u79c1\u653e\u5927\uff0c\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.00085", "pdf": "https://arxiv.org/pdf/2506.00085", "abs": "https://arxiv.org/abs/2506.00085", "authors": ["Vincent Siu", "Nicholas Crispino", "Zihao Yu", "Sam Pan", "Zhun Wang", "Yang Liu", "Dawn Song", "Chenguang Wang"], "title": "COSMIC: Generalized Refusal Direction Identification in LLM Activations", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) encode behaviors such as refusal within their\nactivation space, yet identifying these behaviors remains a significant\nchallenge. Existing methods often rely on predefined refusal templates\ndetectable in output tokens or require manual analysis. We introduce\n\\textbf{COSMIC} (Cosine Similarity Metrics for Inversion of Concepts), an\nautomated framework for direction selection that identifies viable steering\ndirections and target layers using cosine similarity - entirely independent of\nmodel outputs. COSMIC achieves steering performance comparable to prior methods\nwithout requiring assumptions about a model's refusal behavior, such as the\npresence of specific refusal tokens. It reliably identifies refusal directions\nin adversarial settings and weakly aligned models, and is capable of steering\nsuch models toward safer behavior with minimal increase in false refusals,\ndemonstrating robustness across a wide range of alignment conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCOSMIC\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u91cf\u8bc6\u522b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u62d2\u7edd\u884c\u4e3a\u65b9\u5411\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u8bbe\u6a21\u677f\u6216\u4eba\u5de5\u5206\u6790\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6a21\u578b\u884c\u4e3a\u5f15\u5bfc\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bbe\u6a21\u677f\u6216\u4eba\u5de5\u5206\u6790\u8bc6\u522b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u62d2\u7edd\u884c\u4e3a\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002\u4f5c\u8005\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u72ec\u7acb\u4e8e\u6a21\u578b\u8f93\u51fa\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u62d2\u7edd\u884c\u4e3a\u8bc6\u522b\u4e0e\u5f15\u5bfc\u3002", "method": "\u63d0\u51faCOSMIC\u6846\u67b6\uff0c\u5229\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u91cf\u9009\u62e9\u6709\u6548\u7684\u5f15\u5bfc\u65b9\u5411\u548c\u76ee\u6807\u5c42\uff0c\u65e0\u9700\u9884\u8bbe\u62d2\u7edd\u884c\u4e3a\u7684\u5047\u8bbe\uff08\u5982\u7279\u5b9a\u62d2\u7edd\u6807\u8bb0\uff09\u3002", "result": "COSMIC\u5728\u8bc6\u522b\u62d2\u7edd\u65b9\u5411\u548c\u5f15\u5bfc\u6a21\u578b\u5b89\u5168\u884c\u4e3a\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u865a\u5047\u62d2\u7edd\u7387\u589e\u5e45\u6700\u5c0f\u3002", "conclusion": "COSMIC\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u62d2\u7edd\u884c\u4e3a\u8bc6\u522b\u548c\u5f15\u5bfc\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5bf9\u9f50\u6761\u4ef6\u548c\u5bf9\u6297\u573a\u666f\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b, \u62d2\u7edd\u884c\u4e3a, \u4f59\u5f26\u76f8\u4f3c\u5ea6, \u81ea\u52a8\u5316\u5f15\u5bfc, COSMIC\u6846\u67b6"}}
{"id": "2506.00233", "pdf": "https://arxiv.org/pdf/2506.00233", "abs": "https://arxiv.org/abs/2506.00233", "authors": ["Aasish Kumar Sharma", "Dimitar Kyosev", "Julian Kunkel"], "title": "Ethical AI: Towards Defining a Collective Evaluation Framework", "categories": ["cs.AI"], "comment": "6 pages, 3 figures, accepted at 8th IEEE International Workshop on\n  Advances in Artificial Intelligence and Machine Learning (AIML 2025):\n  Futuristic AI and ML models & Intelligent Systems", "summary": "Artificial Intelligence (AI) is transforming sectors such as healthcare,\nfinance, and autonomous systems, offering powerful tools for innovation. Yet\nits rapid integration raises urgent ethical concerns related to data ownership,\nprivacy, and systemic bias. Issues like opaque decision-making, misleading\noutputs, and unfair treatment in high-stakes domains underscore the need for\ntransparent and accountable AI systems. This article addresses these challenges\nby proposing a modular ethical assessment framework built on ontological blocks\nof meaning-discrete, interpretable units that encode ethical principles such as\nfairness, accountability, and ownership. By integrating these blocks with FAIR\n(Findable, Accessible, Interoperable, Reusable) principles, the framework\nsupports scalable, transparent, and legally aligned ethical evaluations,\nincluding compliance with the EU AI Act. Using a real-world use case in\nAI-powered investor profiling, the paper demonstrates how the framework enables\ndynamic, behavior-informed risk classification. The findings suggest that\nontological blocks offer a promising path toward explainable and auditable AI\nethics, though challenges remain in automation and probabilistic reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u672c\u4f53\u5757\u7684\u6a21\u5757\u5316\u4f26\u7406\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3AI\u9886\u57df\u4e2d\u7684\u4f26\u7406\u95ee\u9898\uff0c\u5982\u900f\u660e\u5ea6\u3001\u516c\u5e73\u6027\u548c\u5408\u89c4\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "AI\u7684\u5feb\u901f\u5e94\u7528\u5e26\u6765\u4e86\u6570\u636e\u6240\u6709\u6743\u3001\u9690\u79c1\u548c\u7cfb\u7edf\u6027\u504f\u89c1\u7b49\u4f26\u7406\u95ee\u9898\uff0c\u4e9f\u9700\u900f\u660e\u548c\u53ef\u95ee\u8d23\u7684\u7cfb\u7edf\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u672c\u4f53\u5757\u7684\u9053\u5fb7\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408FAIR\u539f\u5219\uff0c\u7528\u4e8e\u52a8\u6001\u4f26\u7406\u8bc4\u4f30\u3002", "result": "\u5728AI\u6295\u8d44\u8005\u753b\u50cf\u7684\u5b9e\u9645\u7528\u4f8b\u4e2d\uff0c\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u52a8\u6001\u98ce\u9669\u5206\u7c7b\uff0c\u8868\u660e\u672c\u4f53\u5757\u662f\u53ef\u884c\u7684\u8def\u5f84\u3002", "conclusion": "\u672c\u4f53\u5757\u4e3a\u53ef\u89e3\u91ca\u548c\u53ef\u5ba1\u8ba1\u7684AI\u4f26\u7406\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u4f46\u81ea\u52a8\u5316\u548c\u6982\u7387\u63a8\u7406\u4ecd\u9700\u6539\u8fdb\u3002", "keywords": "\u4eba\u5de5\u667a\u80fd,\u4f26\u7406\u8bc4\u4f30,\u672c\u4f53\u5757,FAIR\u539f\u5219,\u900f\u660e\u5ea6"}}
{"id": "2506.00166", "pdf": "https://arxiv.org/pdf/2506.00166", "abs": "https://arxiv.org/abs/2506.00166", "authors": ["Kundan Krishna", "Joseph Y Cheng", "Charles Maalouf", "Leon A Gatys"], "title": "Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "16 pages, 2 figures, including references and appendix", "summary": "Existing paradigms for ensuring AI safety, such as guardrail models and\nalignment training, often compromise either inference efficiency or development\nflexibility. We introduce Disentangled Safety Adapters (DSA), a novel framework\naddressing these challenges by decoupling safety-specific computations from a\ntask-optimized base model. DSA utilizes lightweight adapters that leverage the\nbase model's internal representations, enabling diverse and flexible safety\nfunctionalities with minimal impact on inference cost. Empirically, DSA-based\nsafety guardrails substantially outperform comparably sized standalone models,\nnotably improving hallucination detection (0.88 vs. 0.61 AUC on Summedits) and\nalso excelling at classifying hate speech (0.98 vs. 0.92 on ToxiGen) and unsafe\nmodel inputs and responses (0.93 vs. 0.90 on AEGIS2.0 & BeaverTails).\nFurthermore, DSA-based safety alignment allows dynamic, inference-time\nadjustment of alignment strength and a fine-grained trade-off between\ninstruction following performance and model safety. Importantly, combining the\nDSA safety guardrail with DSA safety alignment facilitates context-dependent\nalignment strength, boosting safety on StrongReject by 93% while maintaining\n98% performance on MTBench -- a total reduction in alignment tax of 8\npercentage points compared to standard safety alignment fine-tuning. Overall,\nDSA presents a promising path towards more modular, efficient, and adaptable AI\nsafety and alignment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u5b89\u5168\u9002\u914d\u5668\uff08DSA\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u5b89\u5168\u8ba1\u7b97\u4e0e\u4efb\u52a1\u4f18\u5316\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u7075\u6d3b\u591a\u6837\u7684\u5b89\u5168\u529f\u80fd\u3002", "motivation": "\u73b0\u6709AI\u5b89\u5168\u8303\u5f0f\uff08\u5982\u62a4\u680f\u6a21\u578b\u548c\u5bf9\u9f50\u8bad\u7ec3\uff09\u901a\u5e38\u4f1a\u5728\u63a8\u7406\u6548\u7387\u6216\u5f00\u53d1\u7075\u6d3b\u6027\u4e0a\u505a\u51fa\u59a5\u534f\uff0cDSA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DSA\u5229\u7528\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff0c\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u5b9e\u73b0\u5b89\u5168\u529f\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u63a8\u7406\u6210\u672c\u7684\u5f71\u54cd\u3002", "result": "DSA\u5728\u5e7b\u89c9\u68c0\u6d4b\uff08AUC 0.88\uff09\u3001\u4ec7\u6068\u8a00\u8bba\u5206\u7c7b\uff08AUC 0.98\uff09\u53ca\u4e0d\u5b89\u5168\u8f93\u5165/\u54cd\u5e94\u8bc6\u522b\uff08AUC 0.93\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u652f\u6301\u52a8\u6001\u5bf9\u9f50\u5f3a\u5ea6\u8c03\u6574\u3002", "conclusion": "DSA\u4e3a\u6a21\u5757\u5316\u3001\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684AI\u5b89\u5168\u548c\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\u3002", "keywords": "AI\u5b89\u5168, \u89e3\u8026\u5b89\u5168\u9002\u914d\u5668, \u5e7b\u89c9\u68c0\u6d4b, \u4ec7\u6068\u8a00\u8bba\u5206\u7c7b, \u52a8\u6001\u5bf9\u9f50"}}
{"id": "2506.00087", "pdf": "https://arxiv.org/pdf/2506.00087", "abs": "https://arxiv.org/abs/2506.00087", "authors": ["Peng Xie", "Xingyuan Liu", "Tsz Wai Chan", "Yequan Bie", "Yangqiu Song", "Yang Wang", "Hao Chen", "Kani Chen"], "title": "SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic Code-Switching Dataset", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Code-switching (CS) is the alternating use of two or more languages within a\nconversation or utterance, often influenced by social context and speaker\nidentity. This linguistic phenomenon poses challenges for Automatic Speech\nRecognition (ASR) systems, which are typically designed for a single language\nand struggle to handle multilingual inputs. The growing global demand for\nmultilingual applications, including Code-Switching ASR (CSASR), Text-to-Speech\n(CSTTS), and Cross-Lingual Information Retrieval (CLIR), highlights the\ninadequacy of existing monolingual datasets.\n  Although some code-switching datasets exist, most are limited to bilingual\nmixing within homogeneous ethnic groups, leaving a critical need for a\nlarge-scale, diverse benchmark akin to ImageNet in computer vision.\n  To bridge this gap, we introduce \\textbf{LinguaMaster}, a multi-agent\ncollaboration framework specifically designed for efficient and scalable\nmultilingual data synthesis. Leveraging this framework, we curate\n\\textbf{SwitchLingua}, the first large-scale multilingual and multi-ethnic\ncode-switching dataset, including: (1) 420K CS textual samples across 12\nlanguages, and (2) over 80 hours of audio recordings from 174 speakers\nrepresenting 18 countries/regions and 63 racial/ethnic backgrounds, based on\nthe textual data. This dataset captures rich linguistic and cultural diversity,\noffering a foundational resource for advancing multilingual and multicultural\nresearch. Furthermore, to address the issue that existing ASR evaluation\nmetrics lack sensitivity to code-switching scenarios, we propose the\n\\textbf{Semantic-Aware Error Rate (SAER)}, a novel evaluation metric that\nincorporates semantic information, providing a more accurate and context-aware\nassessment of system performance.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86LinguaMaster\u6846\u67b6\u53caSwitchLingua\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u8bed\u8a00\u548c\u591a\u79cd\u65cf\u4ee3\u7801\u5207\u6362\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6307\u6807SAER\u3002", "motivation": "\u73b0\u6709\u5355\u8bed\u6570\u636e\u96c6\u65e0\u6cd5\u6ee1\u8db3\u591a\u8bed\u8a00\u5e94\u7528\uff08\u5982CSASR\u3001CSTTS\u3001CLIR\uff09\u7684\u9700\u6c42\uff0c\u4e14\u7f3a\u4e4f\u591a\u6837\u5316\u3001\u5927\u89c4\u6a21\u7684\u4ee3\u7801\u5207\u6362\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7LinguaMaster\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u6784\u5efaSwitchLingua\u6570\u636e\u96c6\uff0c\u5305\u542b420K\u6587\u672c\u6837\u672c\u548c80\u5c0f\u65f6\u97f3\u9891\uff0c\u5e76\u63d0\u51fa\u4e86SAER\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u6570\u636e\u96c6\u6db5\u76d612\u79cd\u8bed\u8a00\u300118\u4e2a\u56fd\u5bb6/\u5730\u533a\u7684174\u540d\u8bf4\u8bdd\u8005\uff0c\u4e30\u5bcc\u4e86\u8bed\u8a00\u548c\u6587\u5316\u591a\u6837\u6027\uff1bSAER\u80fd\u66f4\u7cbe\u786e\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "SwitchLingua\u4e3a\u591a\u8bed\u8a00\u548c\u8de8\u6587\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0cSAER\u5f25\u8865\u4e86\u73b0\u6709ASR\u8bc4\u4f30\u6307\u6807\u7684\u4e0d\u8db3\u3002", "keywords": "\u4ee3\u7801\u5207\u6362\u3001\u591a\u8bed\u8a00\u6570\u636e\u96c6\u3001ASR\u3001LinguaMaster\u3001SwitchLingua\u3001SAER"}}
{"id": "2506.00239", "pdf": "https://arxiv.org/pdf/2506.00239", "abs": "https://arxiv.org/abs/2506.00239", "authors": ["Dewei Feng", "Carol Li", "Wei Dai", "Paul Pu Liang"], "title": "SMELLNET: A Large-scale Dataset for Real-world Smell Recognition", "categories": ["cs.AI"], "comment": "22 pages, 13 figures", "summary": "The ability of AI to sense and identify various substances based on their\nsmell alone can have profound impacts on allergen detection (e.g., smelling\ngluten or peanuts in a cake), monitoring the manufacturing process, and sensing\nhormones that indicate emotional states, stress levels, and diseases. Despite\nthese broad impacts, there are virtually no large scale benchmarks, and\ntherefore little progress, for training and evaluating AI systems' ability to\nsmell in the real world. In this paper, we use portable gas and chemical\nsensors to create SmellNet, the first large-scale database that digitizes a\ndiverse range of smells in the natural world. SmellNet contains about 180,000\ntime steps of 50 substances (spanning nuts, spices, herbs, fruits, and\nvegetables) with 50 hours of data. Using SmellNet, we train AI models for\nreal-time classification of substances based on their smell alone. Our best\nmethods leverage sequence models, contrastive learning to integrate\nhigh-resolution Gas Chromatography-Mass Spectrometry molecular data, and a new\ntemporal difference method that identifies sharp changes in sensor readings.\nOur best models achieve up to 65.35% accuracy on pre-recorded data, and\ngeneralize to real-world conditions with 10.71% accuracy on nuts and 25.38% on\nspices in the challenging 50-way online classification task. Despite these\npromising results, SmellNet highlights many technical challenges in building AI\nfor smell, including richer feature learning, on-edge smell models, and\nrobustness to environmental changes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u6c14\u5473\u6570\u636e\u5e93SmellNet\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30AI\u7684\u6c14\u5473\u8bc6\u522b\u80fd\u529b\uff0c\u5c55\u793a\u4e86AI\u5728\u5b9e\u65f6\u6c14\u5473\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\u548c\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u7684\u6c14\u5473\u8bc6\u522b\u57fa\u51c6\u6570\u636e\u5e93\uff0c\u9650\u5236\u4e86AI\u5728\u6c14\u5473\u5206\u7c7b\u9886\u57df\u7684\u53d1\u5c55\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7SmellNet\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8AI\u5728\u6c14\u5473\u4f20\u611f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528\u4fbf\u643a\u5f0f\u6c14\u4f53\u548c\u5316\u5b66\u4f20\u611f\u5668\u91c7\u96c6\u6570\u636e\uff0c\u6784\u5efa\u5305\u542b18\u4e07\u65f6\u95f4\u6b65\u957f\u7684SmellNet\u6570\u636e\u5e93\uff0c\u7ed3\u5408\u5e8f\u5217\u6a21\u578b\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u9ad8\u5206\u8fa8\u7387\u5206\u5b50\u6570\u636e\u8bad\u7ec3AI\u6a21\u578b\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728\u9884\u5f55\u6570\u636e\u4e0a\u8fbe\u523065.35%\u7684\u51c6\u786e\u7387\uff0c\u572850\u7c7b\u5b9e\u65f6\u5206\u7c7b\u4efb\u52a1\u4e2d\u5bf9\u575a\u679c\u548c\u9999\u6599\u7684\u51c6\u786e\u7387\u5206\u522b\u4e3a10.71%\u548c25.38%\u3002", "conclusion": "SmellNet\u4e3aAI\u6c14\u5473\u8bc6\u522b\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u7279\u5f81\u5b66\u4e60\u3001\u73af\u5883\u9002\u5e94\u6027\u7b49\u6311\u6218\u3002", "keywords": "AI\u6c14\u5473\u8bc6\u522b, SmellNet, \u6c14\u4f53\u4f20\u611f\u5668, \u5bf9\u6bd4\u5b66\u4e60, \u5b9e\u65f6\u5206\u7c7b"}}
{"id": "2506.00172", "pdf": "https://arxiv.org/pdf/2506.00172", "abs": "https://arxiv.org/abs/2506.00172", "authors": ["Kaivalya Hariharan", "Uzay Girit", "Atticus Wang", "Jacob Andreas"], "title": "Breakpoint: Scalable evaluation of system-level reasoning in LLM code agents", "categories": ["cs.LG"], "comment": "21 pages, 14 figures", "summary": "Benchmarks for large language models (LLMs) have predominantly assessed\nshort-horizon, localized reasoning. Existing long-horizon suites (e.g.\nSWE-bench) rely on manually curated issues, so expanding or tuning difficulty\ndemands expensive human effort and evaluations quickly saturate. However, many\nreal-world tasks, such as software engineering or scientific research, require\nagents to rapidly comprehend and manipulate novel, complex structures\ndynamically; evaluating these capabilities requires the ability to construct\nlarge and varied sets of problems for agents to solve. We introduce Breakpoint,\na benchmarking methodology that automatically generates code-repair tasks by\nadversarially corrupting functions within real-world software repositories.\nBreakpoint systematically controls task difficulty along two clear dimensions:\nlocal reasoning (characterized by code complexity metrics such as cyclomatic\ncomplexity) and system-level reasoning (characterized by call-graph centrality\nand the number of simultaneously corrupted interdependent functions). In\nexperiments across more than 900 generated tasks we demonstrate that our\nmethodology can scale to arbitrary difficulty, with state-of-the-art models'\nsuccess rates ranging from 55% on the easiest tasks down to 0% on the hardest.", "AI": {"tldr": "Breakpoint\u662f\u4e00\u79cd\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u4fee\u590d\u4efb\u52a1\u6765\u8bc4\u4f30LLMs\u957f\u7a0b\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u4eba\u5de5\u5e72\u9884\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u95ee\u9898\u96c6\uff0c\u6269\u5c55\u6027\u548c\u96be\u5ea6\u8c03\u6574\u53d7\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u4efb\u52a1\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5bf9\u6297\u6027\u7834\u574f\u771f\u5b9e\u8f6f\u4ef6\u5e93\u4e2d\u7684\u51fd\u6570\uff0c\u63a7\u5236\u4efb\u52a1\u96be\u5ea6\uff08\u672c\u5730\u63a8\u7406\u548c\u7cfb\u7edf\u7ea7\u63a8\u7406\uff09\u3002", "result": "\u5728900\u591a\u4e2a\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u6700\u5148\u8fdb\u6a21\u578b\u7684\u6210\u529f\u7387\u4ece55%\u964d\u81f30%\u3002", "conclusion": "Breakpoint\u80fd\u6269\u5c55\u5230\u4efb\u610f\u96be\u5ea6\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4efb\u52a1\u8bc4\u4f30\u3002", "keywords": "\u957f\u7a0b\u63a8\u7406,\u4ee3\u7801\u4fee\u590d,\u4efb\u52a1\u96be\u5ea6,\u8bc4\u4f30\u57fa\u51c6"}}
{"id": "2506.00088", "pdf": "https://arxiv.org/pdf/2506.00088", "abs": "https://arxiv.org/abs/2506.00088", "authors": ["Qing Li", "Jiahui Geng", "Zongxiong Chen", "Derui Zhu", "Yuxia Wang", "Congbo Ma", "Chenyang Lyu", "Fakhri Karray"], "title": "HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In recent years, large language models (LLMs) have made remarkable\nadvancements, yet hallucination, where models produce inaccurate or non-factual\nstatements, remains a significant challenge for real-world deployment. Although\ncurrent classification-based methods, such as SAPLMA, are highly efficient in\nmitigating hallucinations, they struggle when non-factual information arises in\nthe early or mid-sequence of outputs, reducing their reliability. To address\nthese issues, we propose Hallucination Detection-Neural Differential Equations\n(HD-NDEs), a novel method that systematically assesses the truthfulness of\nstatements by capturing the full dynamics of LLMs within their latent space.\nOur approaches apply neural differential equations (Neural DEs) to model the\ndynamic system in the latent space of LLMs. Then, the sequence in the latent\nspace is mapped to the classification space for truth assessment. The extensive\nexperiments across five datasets and six widely used LLMs demonstrate the\neffectiveness of HD-NDEs, especially, achieving over 14% improvement in AUC-ROC\non the True-False dataset compared to state-of-the-art techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5HD-NDEs\uff0c\u901a\u8fc7\u795e\u7ecf\u5fae\u5206\u65b9\u7a0b\u6355\u6349LLMs\u7684\u52a8\u6001\u7279\u6027\uff0c\u4ee5\u68c0\u6d4b\u5e7b\u89c9\u73b0\u8c61\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u6280\u672f\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1LLMs\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5e7b\u89c9\u95ee\u9898\u4ecd\u963b\u788d\u5176\u5b9e\u9645\u5e94\u7528\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8f93\u51fa\u4e2d\u65e9\u671f\u6216\u4e2d\u671f\u51fa\u73b0\u975e\u4e8b\u5b9e\u4fe1\u606f\u65f6\u6548\u679c\u6709\u9650\u3002", "method": "\u5229\u7528\u795e\u7ecf\u5fae\u5206\u65b9\u7a0b\u5efa\u6a21LLMs\u6f5c\u5728\u7a7a\u95f4\u7684\u52a8\u6001\u7cfb\u7edf\uff0c\u5e76\u5c06\u6f5c\u5728\u7a7a\u95f4\u5e8f\u5217\u6620\u5c04\u5230\u5206\u7c7b\u7a7a\u95f4\u8fdb\u884c\u771f\u5b9e\u6027\u8bc4\u4f30\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u548c\u516d\u79cdLLMs\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHD-NDEs\u6548\u679c\u663e\u8457\uff0c\u5c24\u5176\u5728True-False\u6570\u636e\u96c6\u4e0aAUC-ROC\u63d0\u5347\u8d85\u8fc714%\u3002", "conclusion": "HD-NDEs\u901a\u8fc7\u7cfb\u7edf\u6027\u8bc4\u4f30LLMs\u6f5c\u5728\u7a7a\u95f4\u7684\u52a8\u6001\u7279\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5e7b\u89c9\u68c0\u6d4b\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u5e7b\u89c9\u68c0\u6d4b,\u795e\u7ecf\u5fae\u5206\u65b9\u7a0b,\u6f5c\u5728\u7a7a\u95f4,\u771f\u5b9e\u6027\u8bc4\u4f30"}}
{"id": "2506.00242", "pdf": "https://arxiv.org/pdf/2506.00242", "abs": "https://arxiv.org/abs/2506.00242", "authors": ["Shuai Feng", "Wei-Chuang Chan", "Srishti Chouhan", "Junior Francisco Garcia Ayala", "Srujananjali Medicherla", "Kyle Clark", "Mingwei Shi"], "title": "Whispers of Many Shores: Cultural Alignment through Collaborative Cultural Expertise", "categories": ["cs.AI", "cs.CL"], "comment": "14 main pages;8 page appendix", "summary": "The integration of large language models (LLMs) into global applications\nnecessitates effective cultural alignment for meaningful and\nculturally-sensitive interactions. Current LLMs often lack the nuanced\nunderstanding required for diverse cultural contexts, and adapting them\ntypically involves costly full fine-tuning. To address this, we introduce a\nnovel soft prompt fine-tuning framework that enables efficient and modular\ncultural alignment. Our method utilizes vectorized prompt tuning to dynamically\nroute queries to a committee of culturally specialized 'expert' LLM\nconfigurations, created by optimizing soft prompt embeddings without altering\nthe base model's parameters. Extensive experiments demonstrate that our\nframework significantly enhances cultural sensitivity and adaptability,\nimproving alignment scores from 0.208 to 0.820, offering a robust solution for\nculturally-aware LLM deployment. This research paves the way for subsequent\ninvestigations into enhanced cultural coverage and dynamic expert adaptation,\ncrucial for realizing autonomous AI with deeply nuanced understanding in a\nglobally interconnected world.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6f\u63d0\u793a\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u65b9\u6cd5\u52a8\u6001\u4f18\u5316LLM\u7684\u6587\u5316\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u6587\u5316\u654f\u611f\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u591a\u5143\u6587\u5316\u8bed\u5883\u4e2d\u7f3a\u4e4f\u7ec6\u817b\u7406\u89e3\uff0c\u5168\u6a21\u578b\u5fae\u8c03\u6210\u672c\u9ad8\u6602\uff0c\u4e9f\u9700\u9ad8\u6548\u6587\u5316\u5bf9\u9f50\u65b9\u6848\u3002", "method": "\u5229\u7528\u5411\u91cf\u5316\u63d0\u793a\u8c03\u4f18\uff0c\u52a8\u6001\u5206\u914d\u67e5\u8be2\u81f3\u6587\u5316\u4e13\u5bb6LLM\u914d\u7f6e\uff0c\u4f18\u5316\u8f6f\u63d0\u793a\u5d4c\u5165\u800c\u4e0d\u6539\u53d8\u57fa\u7840\u6a21\u578b\u53c2\u6570\u3002", "result": "\u6587\u5316\u5bf9\u9f50\u5206\u6570\u4ece0.208\u63d0\u5347\u81f30.820\uff0c\u663e\u8457\u589e\u5f3a\u6587\u5316\u654f\u611f\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6587\u5316\u611f\u77e5LLM\u90e8\u7f72\u63d0\u4f9b\u7a33\u5065\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u540e\u7eed\u6587\u5316\u8986\u76d6\u548c\u52a8\u6001\u4e13\u5bb6\u9002\u5e94\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u6587\u5316\u5bf9\u9f50, \u8f6f\u63d0\u793a\u5fae\u8c03, \u6587\u5316\u654f\u611f\u6027, \u52a8\u6001\u9002\u5e94"}}
{"id": "2506.00175", "pdf": "https://arxiv.org/pdf/2506.00175", "abs": "https://arxiv.org/abs/2506.00175", "authors": ["Shichang Zhang", "Hongzhe Du", "Karim Saraipour", "Jiaqi W. Ma", "Himabindu Lakkaraju"], "title": "Accountability Attribution: Tracing Model Behavior to Training Processes", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Modern AI development pipelines often involve multiple stages-pretraining,\nfine-tuning rounds, and subsequent adaptation or alignment-with numerous model\nupdate steps within each stage. This raises a critical question of\naccountability: when a deployed model succeeds or fails, which stage is\nresponsible, and to what extent? We pose the problem of accountability\nattribution, which aims to trace model behavior back to specific stages of the\ntraining process. To address this, we propose a general framework that answers\ncounterfactual questions about stage effects: how would the model behavior have\nchanged if the updates from a training stage had not been executed?. Within\nthis framework, we introduce estimators based on first-order approximations\nthat efficiently quantify the stage effects without retraining. Our estimators\naccount for both the training data and key aspects of optimization dynamics,\nincluding learning rate schedules, momentum, and weight decay. Empirically, we\ndemonstrate that our approach identifies training stages accountable for\nspecific behaviors, offering a practical tool for model analysis and a step\ntoward more accountable AI development.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u7528\u4e8e\u8ffd\u8e2aAI\u6a21\u578b\u884c\u4e3a\u80cc\u540e\u7684\u8bad\u7ec3\u9636\u6bb5\u8d23\u4efb\uff0c\u901a\u8fc7\u4e00\u9636\u8fd1\u4f3c\u65b9\u6cd5\u91cf\u5316\u5404\u9636\u6bb5\u5f71\u54cd\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3AI\u6a21\u578b\u5f00\u53d1\u4e2d\u591a\u9636\u6bb5\u8bad\u7ec3\u540e\u7684\u8d23\u4efb\u5f52\u5c5e\u95ee\u9898\uff0c\u660e\u786e\u5404\u9636\u6bb5\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u8d21\u732e\u3002", "method": "\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u4e00\u9636\u8fd1\u4f3c\u4f30\u8ba1\u5668\uff0c\u91cf\u5316\u8bad\u7ec3\u9636\u6bb5\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u8003\u8651\u6570\u636e\u548c\u4f18\u5316\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u80fd\u6709\u6548\u8bc6\u522b\u7279\u5b9a\u884c\u4e3a\u80cc\u540e\u7684\u8d23\u4efb\u8bad\u7ec3\u9636\u6bb5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6a21\u578b\u5206\u6790\u548c\u589e\u5f3aAI\u5f00\u53d1\u95ee\u8d23\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "keywords": "AI\u95ee\u8d23\u6027, \u8bad\u7ec3\u9636\u6bb5\u5206\u6790, \u53cd\u4e8b\u5b9e\u4f30\u8ba1, \u6a21\u578b\u884c\u4e3a\u6eaf\u6e90"}}
{"id": "2506.00103", "pdf": "https://arxiv.org/pdf/2506.00103", "abs": "https://arxiv.org/abs/2506.00103", "authors": ["Xun Lu"], "title": "Writing-Zero: Bridge the Gap Between Non-verifiable Problems and Verifiable Rewards", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has enabled large\nlanguage models (LLMs) to achieve remarkable breakthroughs in reasoning tasks\nwith objective ground-truth answers, such as mathematics and code generation.\nHowever, a significant gap remains for non-verifiable tasks, like creative\nwriting and open-ended dialogue, where quality assessment is inherently\nsubjective and lacks definitive references. Existing approaches for these\ndomains often rely on scalar reward models trained with human preferences,\nwhich suffer from limited generalization and are prone to reward hacking, such\nas over-explanation and length bias. In this work, we propose a unified\nRLVR-based training paradigm that bridges the gap between non-verifiable tasks\nand verifiable rewards. We introduce a writing-principle-based pairwise\nGenerative Reward Model (GenRM) and a novel Bootstrapped Relative Policy\nOptimization (BRPO) algorithm. The pairwise writing GenRM leverages\nself-principled critique to transform subjective assessments into reliable,\nverifiable rewards, while BRPO enables dynamic, reference-free pairwise\ncomparison by leveraging a bootstrapped response as temporary reference from\nwithin group rollouts during RL training. Our approach empowers LLMs to develop\nrobust writing capabilities without supervised fine-tuning, as demonstrated by\nWriting-Zero, which shows consistent improvement and strong resistance to\nreward hacking compared to scalar reward baselines. Furthermore, our method\nachieves competitive results on both in-house and open-source writing\nbenchmarks. Our findings suggest the potential to unify rule-based,\nreference-based, and reference-free reward modeling under the RLVR framework,\nthus paving the way for a comprehensive and scalable RL training paradigm\napplicable across all language tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684RLVR\u8bad\u7ec3\u8303\u5f0f\uff0c\u9002\u7528\u4e8e\u53ef\u9a8c\u8bc1\u548c\u975e\u53ef\u9a8c\u8bc1\u4efb\u52a1\uff0c\u901a\u8fc7\u57fa\u4e8e\u5199\u4f5c\u539f\u5219\u7684\u6210\u5bf9\u751f\u6210\u5956\u52b1\u6a21\u578b\u548c\u65b0\u578bBRPO\u7b97\u6cd5\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u5199\u4f5c\u80fd\u529b\u5e76\u907f\u514d\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002", "motivation": "\u9488\u5bf9\u975e\u53ef\u9a8c\u8bc1\u4efb\u52a1\uff08\u5982\u521b\u610f\u5199\u4f5c\uff09\u7f3a\u4e4f\u5ba2\u89c2\u8bc4\u4f30\u6807\u51c6\u7684\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u4e8e\u6807\u91cf\u5956\u52b1\u7684\u65b9\u6cd5\u6cdb\u5316\u6027\u5dee\u4e14\u6613\u53d7\u5956\u52b1\u9ed1\u5ba2\u5f71\u54cd\u3002", "method": "\u5f15\u5165\u6210\u5bf9\u751f\u6210\u5956\u52b1\u6a21\u578b\uff08GenRM\uff09\u548cBRPO\u7b97\u6cd5\uff0c\u5c06\u4e3b\u89c2\u8bc4\u4f30\u8f6c\u5316\u4e3a\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u5e76\u5229\u7528\u5f15\u5bfc\u5f0f\u54cd\u5e94\u8fdb\u884c\u65e0\u53c2\u8003\u6bd4\u8f83\u3002", "result": "\u65b9\u6cd5\u5728\u5199\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6301\u7eed\u7684\u63d0\u5347\u548c\u5bf9\u5956\u52b1\u9ed1\u5ba2\u7684\u5f3a\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5728\u5185\u90e8\u548c\u5f00\u6e90\u57fa\u51c6\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u8868\u660eRLVR\u6846\u67b6\u6709\u671b\u7edf\u4e00\u57fa\u4e8e\u89c4\u5219\u3001\u53c2\u8003\u548c\u65e0\u53c2\u8003\u7684\u5956\u52b1\u5efa\u6a21\uff0c\u4e3a\u8bed\u8a00\u4efb\u52a1\u63d0\u4f9b\u5168\u9762\u4e14\u53ef\u6269\u5c55\u7684RL\u8bad\u7ec3\u8303\u5f0f\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60\u3001\u8bed\u8a00\u6a21\u578b\u3001\u5956\u52b1\u5efa\u6a21\u3001\u521b\u610f\u5199\u4f5c\u3001RLVR"}}
{"id": "2506.00249", "pdf": "https://arxiv.org/pdf/2506.00249", "abs": "https://arxiv.org/abs/2506.00249", "authors": ["Aniketh Garikaparthi", "Manasi Patwardhan", "Aditya Sanjiv Kanade", "Aman Hassan", "Lovekesh Vig", "Arman Cohan"], "title": "MIR: Methodology Inspiration Retrieval for Scientific Research Problems", "categories": ["cs.AI", "cs.CL"], "comment": "ACL 2025", "summary": "There has been a surge of interest in harnessing the reasoning capabilities\nof Large Language Models (LLMs) to accelerate scientific discovery. While\nexisting approaches rely on grounding the discovery process within the relevant\nliterature, effectiveness varies significantly with the quality and nature of\nthe retrieved literature. We address the challenge of retrieving prior work\nwhose concepts can inspire solutions for a given research problem, a task we\ndefine as Methodology Inspiration Retrieval (MIR). We construct a novel dataset\ntailored for training and evaluating retrievers on MIR, and establish\nbaselines. To address MIR, we build the Methodology Adjacency Graph (MAG);\ncapturing methodological lineage through citation relationships. We leverage\nMAG to embed an \"intuitive prior\" into dense retrievers for identifying\npatterns of methodological inspiration beyond superficial semantic similarity.\nThis achieves significant gains of +5.4 in Recall@3 and +7.8 in Mean Average\nPrecision (mAP) over strong baselines. Further, we adapt LLM-based re-ranking\nstrategies to MIR, yielding additional improvements of +4.5 in Recall@3 and\n+4.8 in mAP. Through extensive ablation studies and qualitative analyses, we\nexhibit the promise of MIR in enhancing automated scientific discovery and\noutline avenues for advancing inspiration-driven retrieval.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u65b9\u6cd5\u7075\u611f\u68c0\u7d22\u201d\uff08MIR\uff09\u7684\u65b0\u4efb\u52a1\uff0c\u901a\u8fc7\u6784\u5efa\u65b9\u6cd5\u8bba\u90bb\u63a5\u56fe\uff08MAG\uff09\u548c\u6539\u8fdb\u68c0\u7d22\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79d1\u5b66\u53d1\u73b0\u7684\u81ea\u52a8\u5316\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u5e94\u7528\u4f9d\u8d56\u4e8e\u6587\u732e\u8d28\u91cf\uff0c\u6548\u679c\u53c2\u5dee\u4e0d\u9f50\u3002\u4e3a\u4e86\u89e3\u51b3\u68c0\u7d22\u4e0e\u7814\u7a76\u95ee\u9898\u76f8\u5173\u7684\u65b9\u6cd5\u7075\u611f\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86MIR\u4efb\u52a1\u3002", "method": "\u6784\u5efaMAG\u56fe\u6355\u6349\u65b9\u6cd5\u8bba\u4f20\u627f\u5173\u7cfb\uff0c\u5e76\u5c06\u201c\u76f4\u89c9\u5148\u9a8c\u201d\u5d4c\u5165\u7a20\u5bc6\u68c0\u7d22\u5668\u4e2d\uff0c\u540c\u65f6\u91c7\u7528LLM\u91cd\u6392\u5e8f\u7b56\u7565\u4f18\u5316MIR\u4efb\u52a1\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0cMAG\u63d0\u5347\u4e86Recall@3\u548cmAP\u6307\u6807\uff0cLLM\u91cd\u6392\u5e8f\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u6027\u80fd\u3002", "conclusion": "MIR\u5728\u81ea\u52a8\u79d1\u5b66\u53d1\u73b0\u4e2d\u5c55\u73b0\u4e86\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7075\u611f\u9a71\u52a8\u7684\u68c0\u7d22\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u79d1\u5b66\u53d1\u73b0, \u65b9\u6cd5\u7075\u611f\u68c0\u7d22, \u65b9\u6cd5\u8bba\u90bb\u63a5\u56fe, \u7a20\u5bc6\u68c0\u7d22\u5668"}}
{"id": "2506.00181", "pdf": "https://arxiv.org/pdf/2506.00181", "abs": "https://arxiv.org/abs/2506.00181", "authors": ["Enea Monzio Compagnoni", "Rustem Islamov", "Antonio Orvieto", "Eduard Gorbunov"], "title": "On the Interaction of Noise, Compression Role, and Adaptivity under $(L_0, L_1)$-Smoothness: An SDE-based Approach", "categories": ["cs.LG", "stat.ML"], "comment": "This manuscript is a work in progress: We welcome comments", "summary": "Using stochastic differential equation (SDE) approximations, we study the\ndynamics of Distributed SGD, Distributed Compressed SGD, and Distributed\nSignSGD under $(L_0,L_1)$-smoothness and flexible noise assumptions. Our\nanalysis provides insights -- which we validate through simulation -- into the\nintricate interactions between batch noise, stochastic gradient compression,\nand adaptivity in this modern theoretical setup. For instance, we show that\n\\textit{adaptive} methods such as Distributed SignSGD can successfully converge\nunder standard assumptions on the learning rate scheduler, even under\nheavy-tailed noise. On the contrary, Distributed (Compressed) SGD with\npre-scheduled decaying learning rate fails to achieve convergence, unless such\na schedule also accounts for an inverse dependency on the gradient norm -- de\nfacto falling back into an adaptive method.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7SDE\u8fd1\u4f3c\u7814\u7a76\u4e86\u5206\u5e03\u5f0fSGD\u3001\u538b\u7f29SGD\u548cSignSGD\u5728\u7075\u6d3b\u566a\u58f0\u5047\u8bbe\u4e0b\u7684\u52a8\u6001\uff0c\u63ed\u793a\u4e86\u6279\u566a\u58f0\u3001\u68af\u5ea6\u538b\u7f29\u548c\u81ea\u9002\u5e94\u65b9\u6cd5\u7684\u590d\u6742\u4e92\u52a8\u3002", "motivation": "\u63a2\u8ba8\u73b0\u4ee3\u7406\u8bba\u6846\u67b6\u4e0b\u5206\u5e03\u5f0f\u4f18\u5316\u65b9\u6cd5\u7684\u6536\u655b\u6027\uff0c\u7279\u522b\u662f\u5728\u566a\u58f0\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\u8fd1\u4f3c\u5206\u6790\u5206\u5e03\u5f0fSGD\u3001\u538b\u7f29SGD\u548cSignSGD\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u5e76\u7ed3\u5408\u6a21\u62df\u9a8c\u8bc1\u3002", "result": "\u81ea\u9002\u5e94\u65b9\u6cd5\uff08\u5982SignSGD\uff09\u80fd\u5728\u6807\u51c6\u5b66\u4e60\u7387\u8c03\u5ea6\u4e0b\u6536\u655b\uff0c\u800c\u9884\u8c03\u5ea6\u8870\u51cf\u5b66\u4e60\u7387\u7684SGD\u7c7b\u65b9\u6cd5\u9664\u975e\u4f9d\u8d56\u68af\u5ea6\u8303\u6570\uff0c\u5426\u5219\u65e0\u6cd5\u6536\u655b\u3002", "conclusion": "\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u66f4\u5f3a\uff0c\u800c\u4f20\u7edfSGD\u9700\u4f9d\u8d56\u68af\u5ea6\u8303\u6570\u8c03\u6574\u5b66\u4e60\u7387\u624d\u80fd\u6536\u655b\u3002", "keywords": "\u5206\u5e03\u5f0f\u4f18\u5316\uff0cSGD\uff0cSignSGD\uff0cSDE\uff0c\u81ea\u9002\u5e94\u5b66\u4e60\u7387\uff0c\u566a\u58f0"}}
{"id": "2506.00134", "pdf": "https://arxiv.org/pdf/2506.00134", "abs": "https://arxiv.org/abs/2506.00134", "authors": ["Fardin Ahsan Sakib", "Ziwei Zhu", "Karen Trister Grace", "Meliha Yetisgen", "Ozlem Uzuner"], "title": "Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Social determinants of health (SDOH) extraction from clinical text is\ncritical for downstream healthcare analytics. Although large language models\n(LLMs) have shown promise, they may rely on superficial cues leading to\nspurious predictions. Using the MIMIC portion of the SHAC (Social History\nAnnotation Corpus) dataset and focusing on drug status extraction as a case\nstudy, we demonstrate that mentions of alcohol or smoking can falsely induce\nmodels to predict current/past drug use where none is present, while also\nuncovering concerning gender disparities in model performance. We further\nevaluate mitigation strategies - such as prompt engineering and\nchain-of-thought reasoning - to reduce these false positives, providing\ninsights into enhancing LLM reliability in health domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e34\u5e8a\u6587\u672c\u4e2d\u63d0\u53d6\u5065\u5eb7\u793e\u4f1a\u51b3\u5b9a\u56e0\u7d20\uff08SDOH\uff09\u65f6\u5b58\u5728\u7684\u95ee\u9898\uff0c\u5982\u4f9d\u8d56\u8868\u9762\u7ebf\u7d22\u5bfc\u81f4\u865a\u5047\u9884\u6d4b\uff0c\u5e76\u53d1\u73b0\u5728\u836f\u7269\u72b6\u6001\u63d0\u53d6\u4e2d\u5b58\u5728\u6027\u522b\u5dee\u5f02\u3002\u4f5c\u8005\u8fd8\u8bc4\u4f30\u4e86\u7f13\u89e3\u7b56\u7565\uff0c\u5982\u63d0\u793a\u5de5\u7a0b\u548c\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u4ee5\u63d0\u9ad8LLMs\u5728\u5065\u5eb7\u9886\u57df\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u4e34\u5e8a\u6587\u672c\u4e2d\u63d0\u53d6SDOH\u65f6\u53ef\u80fd\u4ea7\u751f\u7684\u865a\u5047\u9884\u6d4b\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u836f\u7269\u72b6\u6001\u63d0\u53d6\u4e2d\u7684\u6027\u522b\u5dee\u5f02\uff0c\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528SHAC\u6570\u636e\u96c6\u7684MIMIC\u90e8\u5206\uff0c\u4ee5\u836f\u7269\u72b6\u6001\u63d0\u53d6\u4e3a\u4f8b\uff0c\u8bc4\u4f30LLMs\u7684\u9884\u6d4b\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u63d0\u793a\u5de5\u7a0b\u548c\u601d\u7ef4\u94fe\u63a8\u7406\u7b49\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u5bb9\u6613\u56e0\u9152\u7cbe\u6216\u5438\u70df\u7684\u63d0\u53ca\u800c\u9519\u8bef\u9884\u6d4b\u836f\u7269\u4f7f\u7528\u60c5\u51b5\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u6a21\u578b\u8868\u73b0\u7684\u6027\u522b\u5dee\u5f02\u3002\u7f13\u89e3\u7b56\u7565\uff08\u5982\u63d0\u793a\u5de5\u7a0b\uff09\u80fd\u591f\u51cf\u5c11\u8fd9\u4e9b\u9519\u8bef\u3002", "conclusion": "\u63d0\u793a\u5de5\u7a0b\u548c\u601d\u7ef4\u94fe\u63a8\u7406\u7b49\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u51cf\u5c11LLMs\u5728\u5065\u5eb7\u9886\u57df\u4e2d\u7684\u865a\u5047\u9884\u6d4b\uff0c\u63d0\u9ad8\u6a21\u578b\u53ef\u9760\u6027\uff0c\u4f46\u6027\u522b\u5dee\u5f02\u95ee\u9898\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "keywords": "\u5065\u5eb7\u793e\u4f1a\u51b3\u5b9a\u56e0\u7d20, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u4e34\u5e8a\u6587\u672c, \u836f\u7269\u72b6\u6001\u63d0\u53d6, \u63d0\u793a\u5de5\u7a0b, \u601d\u7ef4\u94fe\u63a8\u7406"}}
{"id": "2506.00258", "pdf": "https://arxiv.org/pdf/2506.00258", "abs": "https://arxiv.org/abs/2506.00258", "authors": ["Qianqi Yan", "Hongquan Li", "Shan Jiang", "Yang Zhao", "Xinze Guan", "Ching-Chen Kuo", "Xin Eric Wang"], "title": "Hidden in Plain Sight: Probing Implicit Reasoning in Multimodal Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) are increasingly deployed in\nopen-ended, real-world environments where inputs are messy, underspecified, and\nnot always trustworthy. Unlike curated benchmarks, these settings frequently\ninvolve instructions that refer to missing objects or contradictory facts, rely\non ambiguous references, or request infeasible actions. In such cases, success\nhinges not on task execution alone, but on a model's ability to detect when\nsomething is silently wrong. This paper presents a systematic analysis of how\ncurrent MLLMs handle such implicit reasoning scenarios: cases where the flaw is\nnot explicitly stated but must be inferred from context. Using a curated\ndiagnostic suite spanning four categories of real-world failure modes, we\nevaluate six MLLMs, including o3 and GPT-4o, and find that models frequently\nfail to surface hidden issues, even when they possess the necessary perceptual\nand reasoning skills. Explicit prompting reveals that the underlying\ncapabilities exist but are often suppressed in favor of user compliance. We\nfurther show that simple inference-time interventions, such as cautious persona\nprompting and, in particular, requiring a clarifying question, can dramatically\nrecover performance. Our findings highlight a persistent gap between reasoning\ncompetence and behavioral compliance in current MLLMs and suggest practical\nstrategies for making these models more trustworthy in underconstrained\nenvironments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5f00\u653e\u73af\u5883\u4e2d\u5904\u7406\u9690\u542b\u63a8\u7406\u95ee\u9898\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5b58\u5728\u6f5c\u5728\u80fd\u529b\u4f46\u7f3a\u4e4f\u884c\u4e3a\u5408\u89c4\u6027\uff0c\u5e76\u63d0\u51fa\u63d0\u5347\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u73af\u5883\uff08\u8f93\u5165\u4fe1\u606f\u4e0d\u5b8c\u6574\u6216\u4e0d\u51c6\u786e\uff09\u4e0b\u68c0\u6d4b\u9690\u542b\u95ee\u9898\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u56db\u4e2a\u7c7b\u522b\u7684\u8bca\u65ad\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u516d\u79cdMLLM\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5c1d\u8bd5\u63a8\u7406\u65f6\u95f4\u5e72\u9884\u63aa\u65bd\u3002", "result": "\u6a21\u578b\u5e38\u5ffd\u7565\u9690\u542b\u95ee\u9898\uff0c\u4f46\u901a\u8fc7\u63d0\u793a\u6216\u63d0\u95ee\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u5f53\u524dMLLMs\u5728\u63a8\u7406\u80fd\u529b\u4e0e\u884c\u4e3a\u5408\u89c4\u6027\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u5347\u53ef\u9760\u6027\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, \u9690\u542b\u63a8\u7406, \u884c\u4e3a\u5408\u89c4\u6027, \u5f00\u653e\u73af\u5883, \u53ef\u9760\u6027"}}
{"id": "2506.00188", "pdf": "https://arxiv.org/pdf/2506.00188", "abs": "https://arxiv.org/abs/2506.00188", "authors": ["Md Mahmuddun Nabi Murad", "Yasin Yilmaz"], "title": "Cluster-Aware Causal Mixer for Online Anomaly Detection in Multivariate Time Series", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Early and accurate detection of anomalies in time series data is critical,\ngiven the significant risks associated with false or missed detections. While\nMLP-based mixer models have shown promise in time series analysis, they lack a\ncausality mechanism to preserve temporal dependencies inherent in the system.\nMoreover, real-world multivariate time series often contain numerous channels\nwith diverse inter-channel correlations. A single embedding mechanism for all\nchannels does not effectively capture these complex relationships. To address\nthese challenges, we propose a novel cluster-aware causal mixer to effectively\ndetect anomalies in multivariate time series. Our model groups channels into\nclusters based on their correlations, with each cluster processed through a\ndedicated embedding layer. In addition, we introduce a causal mixer in our\nmodel, which mixes the information while maintaining causality. Furthermore, we\npresent an anomaly detection framework that accumulates the anomaly evidence\nover time to prevent false positives due to nominal outliers. Our proposed\nmodel operates in an online fashion, making it suitable for real-time\ntime-series anomaly detection tasks. Experimental evaluations across six public\nbenchmark datasets demonstrate that our model consistently achieves superior F1\nscores.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u611f\u77e5\u56e0\u679c\u6df7\u5408\u5668\u7684\u65b0\u578b\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u5206\u7ec4\u548c\u56e0\u679c\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MLP-based mixer\u6a21\u578b\u7f3a\u4e4f\u56e0\u679c\u5173\u7cfb\u673a\u5236\uff0c\u4e14\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u4e2d\u901a\u9053\u95f4\u7684\u590d\u6742\u5173\u7cfb\u672a\u88ab\u6709\u6548\u6355\u6349\u3002", "method": "\u901a\u8fc7\u805a\u7c7b\u5c06\u901a\u9053\u5206\u7ec4\uff0c\u6bcf\u7ec4\u4f7f\u7528\u4e13\u7528\u5d4c\u5165\u5c42\u5904\u7406\uff0c\u5e76\u5f15\u5165\u56e0\u679c\u6df7\u5408\u5668\u4fdd\u6301\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u5728\u516d\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u4f18\u5f02\u7684F1\u5206\u6570\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u6709\u6548\u68c0\u6d4b\u5f02\u5e38\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u4efb\u52a1\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\uff1b\u5f02\u5e38\u68c0\u6d4b\uff1b\u56e0\u679c\u5173\u7cfb\uff1b\u805a\u7c7b\uff1b\u591a\u53d8\u91cf"}}
{"id": "2506.00137", "pdf": "https://arxiv.org/pdf/2506.00137", "abs": "https://arxiv.org/abs/2506.00137", "authors": ["Alireza Salemi", "Hamed Zamani"], "title": "LaMP-QA: A Benchmark for Personalized Long-form Question Answering", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Personalization is essential for question answering systems that are\nuser-centric. Despite its importance, personalization in answer generation has\nbeen relatively underexplored. This is mainly due to lack of resources for\ntraining and evaluating personalized question answering systems. We address\nthis gap by introducing LaMP-QA -- a benchmark designed for evaluating\npersonalized long-form answer generation. The benchmark covers questions from\nthree major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal\nDevelopment, and (3) Society & Culture, encompassing over 45 subcategories in\ntotal. To assess the quality and potential impact of the LaMP-QA benchmark for\npersonalized question answering, we conduct comprehensive human and automatic\nevaluations, to compare multiple evaluation strategies for evaluating generated\npersonalized responses and measure their alignment with human preferences.\nFurthermore, we benchmark a number of non-personalized and personalized\napproaches based on open-source and proprietary large language models (LLMs).\nOur results show that incorporating the personalized context provided leads to\nperformance improvements of up to 39%. The benchmark is publicly released to\nsupport future research in this area.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86 LaMP-QA \u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e2a\u6027\u5316\u957f\u7b54\u6848\u751f\u6210\uff0c\u6db5\u76d6\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\u7c7b\u522b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e2a\u6027\u5316\u4e0a\u4e0b\u6587\u53ef\u63d0\u5347\u6027\u80fd\u8fbe 39%\u3002", "motivation": "\u9488\u5bf9\u4e2a\u6027\u5316\u95ee\u7b54\u7cfb\u7edf\u8bad\u7ec3\u548c\u8bc4\u4f30\u8d44\u6e90\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u586b\u8865\u4e86\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u4e86 LaMP-QA \u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u7c7b\u522b\u95ee\u9898\uff0c\u7ed3\u5408\u4eba\u5de5\u4e0e\u81ea\u52a8\u8bc4\u4f30\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u878d\u5165\u4e2a\u6027\u5316\u4e0a\u4e0b\u6587\u53ef\u5c06\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe 39%\u3002", "conclusion": "LaMP-QA \u57fa\u51c6\u4e3a\u4e2a\u6027\u5316\u95ee\u7b54\u7814\u7a76\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u4e2a\u6027\u5316\u7684\u91cd\u8981\u6027\u3002", "keywords": "\u4e2a\u6027\u5316\u95ee\u7b54\u3001LaMP-QA \u57fa\u51c6\u3001\u957f\u7b54\u6848\u751f\u6210\u3001\u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.00279", "pdf": "https://arxiv.org/pdf/2506.00279", "abs": "https://arxiv.org/abs/2506.00279", "authors": ["Boshra Khajehpiri", "Eric Granger", "Massimiliano de Zambotti", "Fiona C. Baker", "Mohamad Forouzanfar"], "title": "Sleep Brain and Cardiac Activity Predict Cognitive Flexibility and Conceptual Reasoning Using Deep Learning", "categories": ["cs.AI", "cs.LG"], "comment": "This work was accepted for publication in IEEE EMBC 2025", "summary": "Despite extensive research on the relationship between sleep and cognition,\nthe connection between sleep microstructure and human performance across\nspecific cognitive domains remains underexplored. This study investigates\nwhether deep learning models can predict executive functions, particularly\ncognitive adaptability and conceptual reasoning from physiological processes\nduring a night's sleep. To address this, we introduce CogPSGFormer, a\nmulti-scale convolutional-transformer model designed to process multi-modal\npolysomnographic data. This model integrates one-channel ECG and EEG signals\nalong with extracted features, including EEG power bands and heart rate\nvariability parameters, to capture complementary information across modalities.\nA thorough evaluation of the CogPSGFormer architecture was conducted to\noptimize the processing of extended sleep signals and identify the most\neffective configuration. The proposed framework was evaluated on 817\nindividuals from the STAGES dataset using cross-validation. The model achieved\n80.3\\% accuracy in classifying individuals into low vs. high cognitive\nperformance groups on unseen data based on Penn Conditional Exclusion Test\n(PCET) scores. These findings highlight the effectiveness of our multi-scale\nfeature extraction and multi-modal learning approach in leveraging\nsleep-derived signals for cognitive performance prediction. To facilitate\nreproducibility, our code is publicly accessible\n(https://github.com/boshrakh95/CogPSGFormer.git).", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bCogPSGFormer\uff0c\u5229\u7528\u591a\u6a21\u6001\u7761\u7720\u4fe1\u53f7\u9884\u6d4b\u8ba4\u77e5\u529f\u80fd\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u6267\u884c\u529f\u80fd\u65b9\u9762\u7684\u9002\u5e94\u6027\u548c\u6982\u5ff5\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u7761\u7720\u5fae\u89c2\u7ed3\u6784\u4e0e\u7279\u5b9a\u8ba4\u77e5\u9886\u57df\u8868\u73b0\u4e4b\u95f4\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u8054\u7cfb\u3002", "method": "\u63d0\u51faCogPSGFormer\u6a21\u578b\uff0c\u6574\u5408\u5355\u901a\u9053ECG\u548cEEG\u4fe1\u53f7\u53ca\u63d0\u53d6\u7279\u5f81\uff0c\u5982EEG\u529f\u7387\u5e26\u548c\u5fc3\u7387\u53d8\u5f02\u6027\u53c2\u6570\uff0c\u8fdb\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u591a\u6a21\u6001\u5b66\u4e60\u3002", "result": "\u5728STAGES\u6570\u636e\u96c6\u7684817\u540d\u4e2a\u4f53\u4e0a\uff0c\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u6570\u636e\u7684\u8ba4\u77e5\u8868\u73b0\u5206\u7c7b\u4e2d\u8fbe\u523080.3%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u5728\u5229\u7528\u7761\u7720\u4fe1\u53f7\u9884\u6d4b\u8ba4\u77e5\u8868\u73b0\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\u3002", "keywords": "\u7761\u7720\u5fae\u89c2\u7ed3\u6784,\u8ba4\u77e5\u529f\u80fd,\u6df1\u5ea6\u5b66\u4e60,\u591a\u6a21\u6001\u5b66\u4e60,CogPSGFormer"}}
{"id": "2506.00198", "pdf": "https://arxiv.org/pdf/2506.00198", "abs": "https://arxiv.org/abs/2506.00198", "authors": ["Srivathsan Badrinarayanan", "Rishikesh Magar", "Akshay Antony", "Radheesh Sharma Meda", "Amir Barati Farimani"], "title": "MOFGPT: Generative Design of Metal-Organic Frameworks using Language Models", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "comment": "21 pages, 3 figures (in main text, without references)", "summary": "The discovery of Metal-Organic Frameworks (MOFs) with application-specific\nproperties remains a central challenge in materials chemistry, owing to the\nimmense size and complexity of their structural design space. Conventional\ncomputational screening techniques such as molecular simulations and density\nfunctional theory (DFT), while accurate, are computationally prohibitive at\nscale. Machine learning offers an exciting alternative by leveraging\ndata-driven approaches to accelerate materials discovery. The complexity of\nMOFs, with their extended periodic structures and diverse topologies, creates\nboth opportunities and challenges for generative modeling approaches. To\naddress these challenges, we present a reinforcement learning-enhanced,\ntransformer-based framework for the de novo design of MOFs. Central to our\napproach is MOFid, a chemically-informed string representation encoding both\nconnectivity and topology, enabling scalable generative modeling. Our pipeline\ncomprises three components: (1) a generative GPT model trained on MOFid\nsequences, (2) MOFormer, a transformer-based property predictor, and (3) a\nreinforcement learning (RL) module that optimizes generated candidates via\nproperty-guided reward functions. By integrating property feedback into\nsequence generation, our method drives the model toward synthesizable,\ntopologically valid MOFs with desired functional attributes. This work\ndemonstrates the potential of large language models, when coupled with\nreinforcement learning, to accelerate inverse design in reticular chemistry and\nunlock new frontiers in computational MOF discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u751f\u6210\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u5177\u6709\u7279\u5b9a\u529f\u80fd\u7684\u91d1\u5c5e\u6709\u673a\u6846\u67b6\uff08MOFs\uff09\u3002", "motivation": "MOFs\u7684\u8bbe\u8ba1\u7a7a\u95f4\u5de8\u5927\u4e14\u590d\u6742\uff0c\u4f20\u7edf\u8ba1\u7b97\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u52a0\u901f\u53d1\u73b0\u65b0\u6750\u6599\u7684\u9014\u5f84\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u751f\u6210\u6a21\u578b\u3001\u6027\u8d28\u9884\u6d4b\u5668\u548c\u5f3a\u5316\u5b66\u4e60\u6a21\u5757\u7684\u6846\u67b6\uff0c\u4f7f\u7528MOFid\u7f16\u7801\u7ed3\u6784\u548c\u62d3\u6251\u4fe1\u606f\u3002", "result": "\u901a\u8fc7\u5c06\u6027\u8d28\u53cd\u9988\u96c6\u6210\u5230\u5e8f\u5217\u751f\u6210\u4e2d\uff0c\u6a21\u578b\u80fd\u591f\u751f\u6210\u5177\u6709\u7279\u5b9a\u529f\u80fd\u4e14\u53ef\u5408\u6210\u7684MOFs\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u5728MOFs\u53cd\u5411\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u6750\u6599\u53d1\u73b0\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "keywords": "MOFs, \u5f3a\u5316\u5b66\u4e60, \u751f\u6210\u6a21\u578b, \u6750\u6599\u53d1\u73b0, \u53cd\u6f14\u8bbe\u8ba1"}}
{"id": "2506.00145", "pdf": "https://arxiv.org/pdf/2506.00145", "abs": "https://arxiv.org/abs/2506.00145", "authors": ["Sujeet Kumar", "Pretam Ray", "Abhinay Beerukuri", "Shrey Kamoji", "Manoj Balaji Jagadeeshan", "Pawan Goyal"], "title": "Vedavani: A Benchmark Corpus for ASR on Vedic Sanskrit Poetry", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Sanskrit, an ancient language with a rich linguistic heritage, presents\nunique challenges for automatic speech recognition (ASR) due to its phonemic\ncomplexity and the phonetic transformations that occur at word junctures,\nsimilar to the connected speech found in natural conversations. Due to these\ncomplexities, there has been limited exploration of ASR in Sanskrit,\nparticularly in the context of its poetic verses, which are characterized by\nintricate prosodic and rhythmic patterns. This gap in research raises the\nquestion: How can we develop an effective ASR system for Sanskrit, particularly\none that captures the nuanced features of its poetic form? In this study, we\nintroduce Vedavani, the first comprehensive ASR study focused on Sanskrit Vedic\npoetry. We present a 54-hour Sanskrit ASR dataset, consisting of 30,779\nlabelled audio samples from the Rig Veda and Atharva Veda. This dataset\ncaptures the precise prosodic and rhythmic features that define the language.\nWe also benchmark the dataset on various state-of-the-art multilingual speech\nmodels.$^{1}$ Experimentation revealed that IndicWhisper performed the best\namong the SOTA models.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u9488\u5bf9\u68b5\u8bed\u5420\u9640\u8bd7\u6b4c\u7684\u8bed\u97f3\u8bc6\u522b\u7814\u7a76Vedavani\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b54\u5c0f\u65f6\u6807\u8bb0\u97f3\u9891\u7684\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8868\u660eIndicWhisper\u5728\u73b0\u6709\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u68b5\u8bed\u7684\u97f3\u4f4d\u590d\u6742\u6027\u548c\u8bed\u97f3\u53d8\u6362\u4e3a\u8bed\u97f3\u8bc6\u522b\u5e26\u6765\u6311\u6218\uff0c\u5c24\u5176\u5728\u8bd7\u6b4c\u97f5\u5f8b\u4e2d\u66f4\u663e\u590d\u6742\u3002\u76ee\u524d\u7f3a\u4e4f\u76f8\u5173\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5420\u9640\u8bd7\u6b4c\u7684\u8bed\u97f3\u8bc6\u522b\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b54\u5c0f\u65f6\u68b5\u8bed\u5420\u9640\u8bd7\u6b4c\u97f3\u9891\u7684\u6570\u636e\u96c6\uff08Rig Veda\u548cAtharva Veda\uff09\uff0c\u517130,779\u4e2a\u6837\u672c\uff0c\u6355\u6349\u4e86\u8bed\u8a00\u7684\u97f5\u5f8b\u7279\u5f81\u3002\u91c7\u7528\u591a\u8bed\u8a00\u8bed\u97f3\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "IndicWhisper\u5728\u591a\u8bed\u8a00\u8bed\u97f3\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "Vedavani\u4e3a\u68b5\u8bed\u5420\u9640\u8bd7\u6b4c\u7684\u8bed\u97f3\u8bc6\u522b\u63d0\u4f9b\u4e86\u9996\u4e2a\u7814\u7a76\u57fa\u7840\u548c\u6570\u636e\u96c6\uff0c\u5c55\u793a\u4e86IndicWhisper\u7684\u4f18\u8d8a\u6027\u3002", "keywords": "\u68b5\u8bed, \u8bed\u97f3\u8bc6\u522b, \u5420\u9640\u8bd7\u6b4c, ASR, \u591a\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.00309", "pdf": "https://arxiv.org/pdf/2506.00309", "abs": "https://arxiv.org/abs/2506.00309", "authors": ["Ruonan Wang", "Runxi Wang", "Yunwen Shen", "Chengfeng Wu", "Qinglin Zhou", "Rohitash Chandra"], "title": "Evaluation of LLMs for mathematical problem solving", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive performance on a range of\neducational tasks, but are still understudied for their potential to solve\nmathematical problems. In this study, we compare three prominent LLMs,\nincluding GPT-4o, DeepSeek-V3, and Gemini-2.0, on three mathematics datasets of\nvarying complexities (GSM8K, MATH500, and UNSW datasets). We take a\nfive-dimensional approach based on the Structured Chain-of-Thought (SCoT)\nframework to assess final answer correctness, step completeness, step validity,\nintermediate calculation accuracy, and problem comprehension. The results show\nthat GPT-4o is the most stable and consistent in performance across all the\ndatasets, but particularly it performs outstandingly in high-level questions of\nthe UNSW dataset. DeepSeek-V3 is competitively strong in well-structured\ndomains such as optimisation, but suffers from fluctuations in accuracy in\nstatistical inference tasks. Gemini-2.0 shows strong linguistic understanding\nand clarity in well-structured problems but performs poorly in multi-step\nreasoning and symbolic logic. Our error analysis reveals particular deficits in\neach model: GPT-4o is at times lacking in sufficient explanation or precision;\nDeepSeek-V3 leaves out intermediate steps; and Gemini-2.0 is less flexible in\nmathematical reasoning in higher dimensions.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86GPT-4o\u3001DeepSeek-V3\u548cGemini-2.0\u4e09\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4o\u8868\u73b0\u6700\u7a33\u5b9a\uff0cDeepSeek-V3\u5728\u7ed3\u6784\u5316\u4efb\u52a1\u4e2d\u5f3a\uff0c\u800cGemini-2.0\u8bed\u8a00\u7406\u89e3\u51fa\u8272\u4f46\u5728\u591a\u6b65\u63a8\u7406\u4e2d\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u586b\u8865\u5176\u5728\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u7814\u7a76\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\uff08SCoT\uff09\u6846\u67b6\u4ece\u4e94\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u6a21\u578b\u5728\u4e09\u4e2a\u4e0d\u540c\u590d\u6742\u5ea6\u6570\u5b66\u6570\u636e\u96c6\uff08GSM8K\u3001MATH500\u548cUNSW\uff09\u4e0a\u7684\u8868\u73b0\u3002", "result": "GPT-4o\u8868\u73b0\u6700\u7a33\u5b9a\u4e14\u5728\u9ad8\u96be\u5ea6\u95ee\u9898\u4e2d\u5c24\u4e3a\u7a81\u51fa\uff1bDeepSeek-V3\u5728\u4f18\u5316\u4efb\u52a1\u4e2d\u5f3a\u4f46\u7edf\u8ba1\u63a8\u7406\u4e0d\u7a33\u5b9a\uff1bGemini-2.0\u8bed\u8a00\u7406\u89e3\u5f3a\u4f46\u591a\u6b65\u63a8\u7406\u5f31\u3002", "conclusion": "\u4e0d\u540c\u6a21\u578b\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u5404\u6709\u6240\u957f\uff0c\u4f46\u5747\u5b58\u5728\u7279\u5b9a\u7f3a\u9677\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u6570\u5b66\u95ee\u9898\u89e3\u51b3, \u7ed3\u6784\u5316\u601d\u7ef4\u94fe, \u6027\u80fd\u8bc4\u4f30, GPT-4o, DeepSeek-V3, Gemini-2.0"}}
{"id": "2506.00205", "pdf": "https://arxiv.org/pdf/2506.00205", "abs": "https://arxiv.org/abs/2506.00205", "authors": ["Junze Deng", "Qinhang Wu", "Peizhong Ju", "Sen Lin", "Yingbin Liang", "Ness Shroff"], "title": "Unlocking the Power of Rehearsal in Continual Learning: A Theoretical Perspective", "categories": ["cs.LG"], "comment": "accepted to ICML 2025", "summary": "Rehearsal-based methods have shown superior performance in addressing\ncatastrophic forgetting in continual learning (CL) by storing and training on a\nsubset of past data alongside new data in current task. While such a concurrent\nrehearsal strategy is widely used, it remains unclear if this approach is\nalways optimal. Inspired by human learning, where sequentially revisiting tasks\nhelps mitigate forgetting, we explore whether sequential rehearsal can offer\ngreater benefits for CL compared to standard concurrent rehearsal. To address\nthis question, we conduct a theoretical analysis of rehearsal-based CL in\noverparameterized linear models, comparing two strategies: 1) Concurrent\nRehearsal, where past and new data are trained together, and 2) Sequential\nRehearsal, where new data is trained first, followed by revisiting past data\nsequentially. By explicitly characterizing forgetting and generalization error,\nwe show that sequential rehearsal performs better when tasks are less similar.\nThese insights further motivate a novel Hybrid Rehearsal method, which trains\nsimilar tasks concurrently and revisits dissimilar tasks sequentially. We\ncharacterize its forgetting and generalization performance, and our experiments\nwith deep neural networks further confirm that the hybrid approach outperforms\nstandard concurrent rehearsal. This work provides the first comprehensive\ntheoretical analysis of rehearsal-based CL.", "AI": {"tldr": "\u5bf9\u6bd4\u5e76\u53d1\u548c\u987a\u5e8f\u91cd\u653e\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6548\u679c\uff0c\u63d0\u51fa\u6df7\u5408\u91cd\u653e\u65b9\u6cd5\uff0c\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u63a2\u7d22\u6301\u7eed\u5b66\u4e60\u4e2d\u987a\u5e8f\u91cd\u653e\u662f\u5426\u6bd4\u5e76\u53d1\u91cd\u653e\u66f4\u6709\u6548\uff0c\u4ee5\u51cf\u8f7b\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u7406\u8bba\u5206\u6790\u8fc7\u53c2\u6570\u5316\u7ebf\u6027\u6a21\u578b\uff0c\u63d0\u51fa\u6df7\u5408\u91cd\u653e\uff08\u5e76\u53d1+\u987a\u5e8f\uff09\uff0c\u5e76\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u987a\u5e8f\u91cd\u653e\u5728\u4efb\u52a1\u76f8\u4f3c\u5ea6\u4f4e\u65f6\u6548\u679c\u66f4\u597d\uff0c\u6df7\u5408\u91cd\u653e\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u6df7\u5408\u91cd\u653e\u65b9\u6cd5\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u4f18\u4e8e\u6807\u51c6\u5e76\u53d1\u91cd\u653e\uff0c\u4e3a\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "keywords": "\u6301\u7eed\u5b66\u4e60,\u9057\u5fd8\u95ee\u9898,\u91cd\u653e\u7b56\u7565,\u6df7\u5408\u91cd\u653e"}}
{"id": "2506.00160", "pdf": "https://arxiv.org/pdf/2506.00160", "abs": "https://arxiv.org/abs/2506.00160", "authors": ["Qihui Fan", "Enfu Nan", "Wenbo Li", "Lei Lu", "Pu Zhao", "Yanzhi Wang"], "title": "Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement", "categories": ["cs.CL"], "comment": null, "summary": "The growing popularity of social deduction game systems for both business\napplications and AI research has greatly benefited from the rapid advancements\nin Large Language Models (LLMs), which now demonstrate stronger reasoning and\npersuasion capabilities. Especially with the raise of DeepSeek R1 and V3\nmodels, LLMs should enable a more engaging experience for human players in\nLLM-agent-based social deduction games like Werewolf. Previous works either\nfine-tuning, advanced prompting engineering, or additional experience pool to\nachieve engaging text-format Werewolf game experience. We propose a novel yet\nstraightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS)\nmodels designed for enhanced compatibility with various LLM models, and\nimproved user engagement. We argue with ever enhancing LLM reasoning, extra\ncomponents will be unnecessary in the case of Werewolf.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u7b80\u5355\u7684\u57fa\u4e8eLLM\u7684\u72fc\u4eba\u6740\u6e38\u620f\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u7684TTS\u6a21\u578b\u589e\u5f3a\u4e0e\u4e0d\u540cLLM\u7684\u517c\u5bb9\u6027\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u8ba4\u4e3a\u968f\u7740LLM\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\uff0c\u989d\u5916\u7ec4\u4ef6\u5c06\u53d8\u5f97\u4e0d\u5fc5\u8981\u3002", "motivation": "\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u5728\u5546\u4e1a\u5e94\u7528\u548cAI\u7814\u7a76\u4e2d\u7684\u6d41\u884c\uff0c\u4ee5\u53caLLM\u5728\u63a8\u7406\u548c\u8bf4\u670d\u80fd\u529b\u4e0a\u7684\u8fdb\u6b65\uff0c\u4fc3\u4f7f\u8bbe\u8ba1\u66f4\u5438\u5f15\u4eba\u7684LLM\u4ee3\u7406\u72fc\u4eba\u6740\u6e38\u620f\u4f53\u9a8c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u72fc\u4eba\u6740\u6e38\u620f\u7cfb\u7edf\uff0c\u5229\u7528\u4f18\u5316\u7684TTS\u6a21\u578b\uff0c\u51cf\u5c11\u5bf9\u989d\u5916\u7ec4\u4ef6\uff08\u5982\u5fae\u8c03\u6216\u9ad8\u7ea7\u63d0\u793a\u5de5\u7a0b\uff09\u7684\u4f9d\u8d56\u3002", "result": "\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u4e0e\u591a\u79cdLLM\u6a21\u578b\u7684\u517c\u5bb9\u6027\u63d0\u5347\uff0c\u5e76\u4f18\u5316\u4e86\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "conclusion": "\u968f\u7740LLM\u63a8\u7406\u80fd\u529b\u7684\u4e0d\u65ad\u589e\u5f3a\uff0c\u989d\u5916\u7ec4\u4ef6\u7684\u5fc5\u8981\u6027\u5c06\u964d\u4f4e\uff0c\u7b80\u5355\u7684LLM\u7cfb\u7edf\u5373\u53ef\u6ee1\u8db3\u9700\u6c42\u3002", "keywords": "LLM, \u72fc\u4eba\u6740, TTS, \u793e\u4ea4\u63a8\u7406\u6e38\u620f"}}
{"id": "2506.00320", "pdf": "https://arxiv.org/pdf/2506.00320", "abs": "https://arxiv.org/abs/2506.00320", "authors": ["Xiao Yu", "Baolin Peng", "Ruize Xu", "Michel Galley", "Hao Cheng", "Suman Nath", "Jianfeng Gao", "Zhou Yu"], "title": "Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent progress in reasoning with large language models (LLMs), such as\nDeepSeek-R1, demonstrates impressive capabilities in domains like mathematics\nand coding, by exhibiting complex cognitive behaviors such as verification,\ngoal decomposition, and self-reflection. However, it is unclear what behavior\nis effective and what behavior is missing for long-horizon AI agents tasks. In\nthis work, we propose Dyna-Think, a thinking framework that integrates planning\nwith an internal world model with reasoning and acting to enhance AI agent\nperformance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning\n(DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with\nDyna-Think, DIT reconstructs the thinking process of R1 to focus on performing\nworld model simulation relevant to the proposed (and planned) action, and\ntrains the policy using this reconstructed data. To enhance Dyna-Think, DDT\nuses a two-stage training process to first improve the agent's world modeling\nability via objectives such as state prediction or critique generation, and\nthen improve the agent's action via policy training. We evaluate our methods on\nOSWorld, and demonstrate that Dyna-Think improves the agent's in-domain and\nout-of-domain performance, achieving similar best-of-n performance compared to\nR1 while generating 2x less tokens on average. Our extensive empirical studies\nreveal that 1) using critique generation for world model training is effective\nto improve policy performance; and 2) AI agents with better performance\ncorrelate with better world modeling abilities. We believe our results suggest\na promising research direction to integrate world model simulation into AI\nagents to enhance their reasoning, planning, and acting capabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDyna-Think\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4e16\u754c\u6a21\u578b\u4e0e\u63a8\u7406\u548c\u884c\u52a8\u6765\u63d0\u5347AI\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982DeepSeek-R1\uff09\u5728\u6570\u5b66\u548c\u7f16\u7801\u7b49\u9886\u57df\u5c55\u793a\u4e86\u590d\u6742\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u54ea\u4e9b\u884c\u4e3a\u5bf9\u957f\u671f\u4efb\u52a1\u6709\u6548\u3002", "method": "\u63d0\u51faDyna-Think\u6846\u67b6\uff0c\u91c7\u7528Dyna-Think Imitation Learning (DIT)\u548cDyna-Think Dyna Training (DDT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u6a21\u62df\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\u3002", "result": "Dyna-Think\u5728OSWorld\u4e0a\u7684\u5b9e\u9a8c\u4e2d\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u751f\u6210\u66f4\u5c11\u7684token\uff0c\u540c\u65f6\u4e0eR1\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u5c06\u4e16\u754c\u6a21\u578b\u6a21\u62df\u6574\u5408\u5230AI\u667a\u80fd\u4f53\u4e2d\u662f\u63d0\u5347\u5176\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u7684\u6709\u6548\u65b9\u5411\u3002", "keywords": "AI\u667a\u80fd\u4f53, \u4e16\u754c\u6a21\u578b, \u63a8\u7406\u6846\u67b6, Dyna-Think, \u6a21\u4eff\u5b66\u4e60"}}
{"id": "2506.00209", "pdf": "https://arxiv.org/pdf/2506.00209", "abs": "https://arxiv.org/abs/2506.00209", "authors": ["Liwen Sun", "Hao-Ren Yao", "Gary Gao", "Ophir Frieder", "Chenyan Xiong"], "title": "Intercept Cancer: Cancer Pre-Screening with Large Scale Healthcare Foundation Models", "categories": ["cs.LG"], "comment": null, "summary": "Cancer screening, leading to early detection, saves lives. Unfortunately,\nexisting screening techniques require expensive and intrusive medical\nprocedures, not globally available, resulting in too many lost would-be-saved\nlives. We present CATCH-FM, CATch Cancer early with Healthcare Foundation\nModels, a cancer pre-screening methodology that identifies high-risk patients\nfor further screening solely based on their historical medical records. With\nmillions of electronic healthcare records (EHR), we establish the scaling law\nof EHR foundation models pretrained on medical code sequences, pretrain\ncompute-optimal foundation models of up to 2.4 billion parameters, and finetune\nthem on clinician-curated cancer risk prediction cohorts. In our retrospective\nevaluation comprising of thirty thousand patients, CATCH-FM achieved strong\nefficacy (60% sensitivity) with low risk (99% specificity and Negative\nPredictive Value), outperforming feature-based tree models as well as general\nand medical large language models by large margins. Despite significant\ndemographic, healthcare system, and EHR coding differences, CATCH-FM achieves\nstate-of-the-art pancreatic cancer risk prediction on the EHRSHOT few-shot\nleaderboard, outperforming EHR foundation models pretrained using on-site\npatient data. Our analysis demonstrates the robustness of CATCH-FM in various\npatient distributions, the benefits of operating in the ICD code space, and its\nability to capture non-trivial cancer risk factors. Our code will be\nopen-sourced.", "AI": {"tldr": "CATCH-FM\u662f\u4e00\u79cd\u57fa\u4e8e\u533b\u7597\u8bb0\u5f55\u7684\u65b0\u764c\u75c7\u9884\u7b5b\u67e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u5927\u89c4\u6a21\u533b\u7597\u4ee3\u7801\u5e8f\u5217\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u4f4e\u98ce\u9669\u7684\u764c\u75c7\u65e9\u671f\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u7684\u764c\u75c7\u7b5b\u67e5\u6280\u672f\u6602\u8d35\u4e14\u4fb5\u5165\u6027\u5f3a\uff0c\u5168\u7403\u53ef\u7528\u6027\u6709\u9650\uff0c\u5bfc\u81f4\u8bb8\u591a\u672c\u53ef\u633d\u6551\u7684\u751f\u547d\u6d41\u5931\u3002CATCH-FM\u65e8\u5728\u901a\u8fc7\u975e\u4fb5\u5165\u6027\u7684\u533b\u7597\u8bb0\u5f55\u5206\u6790\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e\u6570\u767e\u4e07\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff0c\u9884\u8bad\u7ec32.4\u4ebf\u53c2\u6570\u7684\u533b\u7597\u4ee3\u7801\u5e8f\u5217\u6a21\u578b\uff0c\u5e76\u5728\u4e34\u5e8a\u764c\u75c7\u98ce\u9669\u9884\u6d4b\u961f\u5217\u4e2d\u5fae\u8c03\u3002", "result": "\u57283\u4e07\u60a3\u8005\u7684\u56de\u987e\u6027\u8bc4\u4f30\u4e2d\uff0cCATCH-FM\u663e\u793a\u51fa\u9ad8\u6548\uff0860%\u7075\u654f\u5ea6\uff09\u548c\u4f4e\u98ce\u9669\uff0899%\u7279\u5f02\u6027\u53ca\u9634\u6027\u9884\u6d4b\u503c\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u548c\u901a\u7528\u533b\u5b66\u6a21\u578b\u3002", "conclusion": "CATCH-FM\u5728\u591a\u79cd\u60a3\u8005\u5206\u5e03\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u80fd\u591f\u6355\u6349\u975e\u663e\u6027\u764c\u75c7\u98ce\u9669\u56e0\u7d20\uff0c\u5e76\u4e3a\u5f00\u653e\u6e90\u7801\u3002", "keywords": "\u764c\u75c7\u7b5b\u67e5;CATCH-FM;\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55;\u9884\u8bad\u7ec3\u6a21\u578b;\u9ad8\u98ce\u9669\u60a3\u8005"}}
{"id": "2506.00195", "pdf": "https://arxiv.org/pdf/2506.00195", "abs": "https://arxiv.org/abs/2506.00195", "authors": ["Mingqian Zheng", "Wenjia Hu", "Patrick Zhao", "Motahhare Eslami", "Jena D. Hwang", "Faeze Brahman", "Carolyn Rose", "Maarten Sap"], "title": "Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Current LLMs are trained to refuse potentially harmful input queries\nregardless of whether users actually had harmful intents, causing a tradeoff\nbetween safety and user experience. Through a study of 480 participants\nevaluating 3,840 query-response pairs, we examine how different refusal\nstrategies affect user perceptions across varying motivations. Our findings\nreveal that response strategy largely shapes user experience, while actual user\nmotivation has negligible impact. Partial compliance -- providing general\ninformation without actionable details -- emerges as the optimal strategy,\nreducing negative user perceptions by over 50% to flat-out refusals.\nComplementing this, we analyze response patterns of 9 state-of-the-art LLMs and\nevaluate how 6 reward models score different refusal strategies, demonstrating\nthat models rarely deploy partial compliance naturally and reward models\ncurrently undervalue it. This work demonstrates that effective guardrails\nrequire focusing on crafting thoughtful refusals rather than detecting intent,\noffering a path toward AI safety mechanisms that ensure both safety and\nsustained user engagement.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\uff0c\u90e8\u5206\u5408\u89c4\uff08\u63d0\u4f9b\u4e00\u822c\u4fe1\u606f\u4f46\u4e0d\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7ec6\u8282\uff09\u662f\u6700\u4f18\u62d2\u7edd\u7b56\u7565\uff0c\u80fd\u663e\u8457\u51cf\u5c11\u7528\u6237\u8d1f\u9762\u4f53\u9a8c\uff0c\u4f46\u73b0\u6709LLMs\u548c\u5956\u52b1\u6a21\u578b\u672a\u80fd\u5145\u5206\u5229\u7528\u8fd9\u4e00\u7b56\u7565\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u5904\u7406\u6f5c\u5728\u6709\u5bb3\u67e5\u8be2\u65f6\u91c7\u53d6\u4e00\u5200\u5207\u7684\u62d2\u7edd\u7b56\u7565\uff0c\u5bfc\u81f4\u5b89\u5168\u6027\u4e0e\u7528\u6237\u4f53\u9a8c\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u901a\u8fc7480\u540d\u53c2\u4e0e\u8005\u8bc4\u4f303,840\u6761\u67e5\u8be2-\u54cd\u5e94\u5bf9\uff0c\u7814\u7a76\u4e0d\u540c\u62d2\u7edd\u7b56\u7565\u4e0e\u7528\u6237\u52a8\u673a\u5bf9\u7528\u6237\u4f53\u9a8c\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u67909\u4e2aLLMs\u7684\u54cd\u5e94\u6a21\u5f0f\u548c6\u4e2a\u5956\u52b1\u6a21\u578b\u7684\u8bc4\u5206\u3002", "result": "\u90e8\u5206\u5408\u89c4\u7b56\u7565\u80fd\u5c06\u7528\u6237\u8d1f\u9762\u611f\u77e5\u964d\u4f4e50%\u4ee5\u4e0a\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u548c\u5956\u52b1\u6a21\u578b\u672a\u80fd\u81ea\u7136\u91c7\u7528\u6216\u5145\u5206\u8bc4\u4f30\u8fd9\u4e00\u7b56\u7565\u3002", "conclusion": "\u6709\u6548\u7684\u5b89\u5168\u673a\u5236\u5e94\u4e13\u6ce8\u4e8e\u8bbe\u8ba1\u6df1\u601d\u719f\u8651\u7684\u62d2\u7edd\u7b56\u7565\uff0c\u800c\u975e\u610f\u56fe\u68c0\u6d4b\uff0c\u4ee5\u5b9e\u73b0\u5b89\u5168\u6027\u4e0e\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u5e73\u8861\u3002", "keywords": "LLMs, \u62d2\u7edd\u7b56\u7565, \u7528\u6237\u4f53\u9a8c, \u5b89\u5168\u6027, \u5956\u52b1\u6a21\u578b"}}
{"id": "2506.00328", "pdf": "https://arxiv.org/pdf/2506.00328", "abs": "https://arxiv.org/abs/2506.00328", "authors": ["Kourosh Shahnazari", "Seyed Moein Ayyoubzadeh", "Mohammadali Keshtparvar"], "title": "BASIL: Best-Action Symbolic Interpretable Learning for Evolving Compact RL Policies", "categories": ["cs.AI"], "comment": null, "summary": "The quest for interpretable reinforcement learning is a grand challenge for\nthe deployment of autonomous decision-making systems in safety-critical\napplications. Modern deep reinforcement learning approaches, while powerful,\ntend to produce opaque policies that compromise verification, reduce\ntransparency, and impede human oversight. To address this, we introduce BASIL\n(Best-Action Symbolic Interpretable Learning), a systematic approach for\ngenerating symbolic, rule-based policies via online evolutionary search with\nquality-diversity (QD) optimization. BASIL represents policies as ordered lists\nof symbolic predicates over state variables, ensuring full interpretability and\ntractable policy complexity. By using a QD archive, the methodology in the\nproposed study encourages behavioral and structural diversity between\ntop-performing solutions, while a complexity-aware fitness encourages the\nsynthesis of compact representations. The evolutionary system supports the use\nof exact constraints for rule count and system adaptability for balancing\ntransparency with expressiveness. Empirical comparisons with three benchmark\ntasks CartPole-v1, MountainCar-v0, and Acrobot-v1 show that BASIL consistently\nsynthesizes interpretable controllers with compact representations comparable\nto deep reinforcement learning baselines. Herein, this article introduces a new\ninterpretable policy synthesis method that combines symbolic expressiveness,\nevolutionary diversity, and online learning through a unifying framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBASIL\u7684\u7b26\u53f7\u5316\u3001\u53ef\u89e3\u91ca\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u8fdb\u5316\u641c\u7d22\u548c\u8d28\u91cf-\u591a\u6837\u6027\u4f18\u5316\u751f\u6210\u57fa\u4e8e\u89c4\u5219\u7684\u53ef\u89e3\u91ca\u7b56\u7565\uff0c\u5176\u8868\u73b0\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u76f8\u5f53\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u901a\u5e38\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u8fd9\u5f71\u54cd\u4e86\u9a8c\u8bc1\u3001\u900f\u660e\u5ea6\u548c\u4eba\u7c7b\u76d1\u7763\u7684\u6548\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u7b56\u7565\u751f\u6210\u65b9\u6cd5\u3002", "method": "BASIL\u901a\u8fc7\u5728\u7ebf\u8fdb\u5316\u641c\u7d22\u548c\u8d28\u91cf-\u591a\u6837\u6027\uff08QD\uff09\u4f18\u5316\u751f\u6210\u6709\u5e8f\u7684\u7b26\u53f7\u8c13\u8bcd\u5217\u8868\u4f5c\u4e3a\u7b56\u7565\uff0c\u786e\u4fdd\u53ef\u89e3\u91ca\u6027\u548c\u590d\u6742\u5ea6\u53ef\u63a7\u3002QD\u5b58\u6863\u9f13\u52b1\u884c\u4e3a\u591a\u6837\u6027\uff0c\u590d\u6742\u5ea6\u611f\u77e5\u7684\u9002\u5e94\u5ea6\u51fd\u6570\u4fc3\u8fdb\u7d27\u51d1\u8868\u793a\u3002", "result": "\u5728CartPole-v1\u3001MountainCar-v0\u548cAcrobot-v1\u4e09\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBASIL\u80fd\u4e00\u81f4\u5730\u751f\u6210\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u6027\u80fd\u76f8\u5f53\u7684\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u63a7\u5236\u5668\u3002", "conclusion": "BASIL\u4e3a\u53ef\u89e3\u91ca\u7b56\u7565\u5408\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u7b26\u53f7\u8868\u8fbe\u80fd\u529b\u3001\u8fdb\u5316\u591a\u6837\u6027\u548c\u5728\u7ebf\u5b66\u4e60\u7684\u7edf\u4e00\u6846\u67b6\u3002", "keywords": "\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u3001\u7b26\u53f7\u7b56\u7565\u3001\u8fdb\u5316\u641c\u7d22\u3001\u8d28\u91cf-\u591a\u6837\u6027\u4f18\u5316\u3001\u57fa\u51c6\u4efb\u52a1"}}
{"id": "2506.00236", "pdf": "https://arxiv.org/pdf/2506.00236", "abs": "https://arxiv.org/abs/2506.00236", "authors": ["Babak Barazandeh"], "title": "Localized LoRA: A Structured Low-Rank Approximation for Efficient Fine-Tuning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, offer compact\nand effective alternatives to full model fine-tuning by introducing low-rank\nupdates to pretrained weights. However, most existing approaches rely on global\nlow-rank structures, which can overlook spatial patterns spread across the\nparameter space. In this work, we propose Localized LoRA, a generalized\nframework that models weight updates as a composition of low-rank matrices\napplied to structured blocks of the weight matrix. This formulation enables\ndense, localized updates throughout the parameter space-without increasing the\ntotal number of trainable parameters. We provide a formal comparison between\nglobal, diagonal-local, and fully localized low-rank approximations, and show\nthat our method consistently achieves lower approximation error under matched\nparameter budgets. Experiments on both synthetic and practical settings\ndemonstrate that Localized LoRA offers a more expressive and adaptable\nalternative to existing methods, enabling efficient fine-tuning with improved\nperformance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLocalized LoRA\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u4f4e\u79e9\u66f4\u65b0\u63d0\u5347\u8868\u8fbe\u80fd\u529b\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709PEFT\u65b9\u6cd5\uff08\u5982LoRA\uff09\u901a\u5e38\u4f7f\u7528\u5168\u5c40\u4f4e\u79e9\u7ed3\u6784\uff0c\u5ffd\u7565\u4e86\u53c2\u6570\u7a7a\u95f4\u7684\u7a7a\u95f4\u5206\u5e03\u6a21\u5f0f\u3002", "method": "\u63d0\u51faLocalized LoRA\u6846\u67b6\uff0c\u5c06\u6743\u91cd\u66f4\u65b0\u5efa\u6a21\u4e3a\u4f4e\u79e9\u77e9\u9635\u5bf9\u6743\u91cd\u77e9\u9635\u7ed3\u6784\u5316\u5757\u7684\u7ec4\u5408\uff0c\u5b9e\u73b0\u5bc6\u96c6\u5c40\u90e8\u66f4\u65b0\u3002", "result": "\u5728\u76f8\u540c\u53c2\u6570\u9884\u7b97\u4e0b\uff0cLocalized LoRA\u7684\u8fd1\u4f3c\u8bef\u5dee\u66f4\u4f4e\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u8868\u8fbe\u80fd\u529b\u548c\u9002\u5e94\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Localized LoRA\u662f\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u6027\u80fd\u66f4\u597d\u7684\u5fae\u8c03\u66ff\u4ee3\u65b9\u6848\u3002", "keywords": "\u53c2\u6570\u9ad8\u6548\u5fae\u8c03, LoRA, \u5c40\u90e8\u66f4\u65b0, \u4f4e\u79e9\u77e9\u9635"}}
{"id": "2506.00200", "pdf": "https://arxiv.org/pdf/2506.00200", "abs": "https://arxiv.org/abs/2506.00200", "authors": ["Johannes Moll", "Louisa Fay", "Asfandyar Azhar", "Sophie Ostmeier", "Tim Lueth", "Sergios Gatidis", "Curtis Langlotz", "Jean-Benoit Delbrouck"], "title": "Structuring Radiology Reports: Challenging LLMs with Lightweight Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Radiology reports are critical for clinical decision-making but often lack a\nstandardized format, limiting both human interpretability and machine learning\n(ML) applications. While large language models (LLMs) have shown strong\ncapabilities in reformatting clinical text, their high computational\nrequirements, lack of transparency, and data privacy concerns hinder practical\ndeployment. To address these challenges, we explore lightweight encoder-decoder\nmodels (<300M parameters)-specifically T5 and BERT2BERT-for structuring\nradiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark\nthese models against eight open-source LLMs (1B-70B), adapted using prefix\nprompting, in-context learning (ICL), and low-rank adaptation (LoRA)\nfinetuning. Our best-performing lightweight model outperforms all LLMs adapted\nusing prompt-based techniques on a human-annotated test set. While some\nLoRA-finetuned LLMs achieve modest gains over the lightweight model on the\nFindings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%,\nGREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of\nsubstantially greater computational resources. For example, LLaMA-3-70B\nincurred more than 400 times the inference time, cost, and carbon emissions\ncompared to the lightweight model. These results underscore the potential of\nlightweight, task-specific models as sustainable and privacy-preserving\nsolutions for structuring clinical text in resource-constrained healthcare\nsettings.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff08\u5982T5\u548cBERT2BERT\uff09\u7528\u4e8e\u7ed3\u6784\u5316\u653e\u5c04\u5b66\u62a5\u544a\uff0c\u53d1\u73b0\u5176\u5728\u6027\u80fd\u548c\u8d44\u6e90\u6d88\u8017\u4e0a\u4f18\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002", "motivation": "\u653e\u5c04\u5b66\u62a5\u544a\u7f3a\u4e4f\u6807\u51c6\u5316\u683c\u5f0f\uff0c\u5f71\u54cd\u4e34\u5e8a\u5e94\u7528\u548c\u673a\u5668\u5b66\u4e60\u5904\u7406\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u5f3a\u4f46\u5b58\u5728\u8ba1\u7b97\u8d44\u6e90\u9ad8\u3001\u900f\u660e\u5ea6\u548c\u9690\u79c1\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6a21\u578b\uff08<300M\u53c2\u6570\uff09\u548c\u591a\u79cd\u9002\u914d\u6280\u672f\uff08\u524d\u7f00\u63d0\u793a\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001LoRA\u5fae\u8c03\uff09\u5bf9MIMIC-CXR\u548cCheXpert Plus\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u4eba\u5de5\u6807\u6ce8\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u63d0\u793a\u6280\u672f\u9002\u914d\u7684LLMs\uff0c\u867d\u90e8\u5206LoRA\u5fae\u8c03LLMs\u5728\u4e2a\u522b\u6307\u6807\u6709\u63d0\u5347\uff0c\u4f46\u8d44\u6e90\u6d88\u8017\u663e\u8457\u589e\u52a0\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u6a21\u578b\u662f\u8d44\u6e90\u53d7\u9650\u533b\u7597\u73af\u5883\u4e2d\u7ed3\u6784\u5316\u4e34\u5e8a\u6587\u672c\u7684\u53ef\u6301\u7eed\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u653e\u5c04\u5b66\u62a5\u544a,\u8f7b\u91cf\u7ea7\u6a21\u578b,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u7ed3\u6784\u5316\u6587\u672c,\u533b\u7597\u8d44\u6e90"}}
{"id": "2506.00398", "pdf": "https://arxiv.org/pdf/2506.00398", "abs": "https://arxiv.org/abs/2506.00398", "authors": ["Kordel K. France", "Rohith Peddi", "Nik Dennler", "Ovidiu Daescu"], "title": "Position: Olfaction Standardization is Essential for the Advancement of Embodied Artificial Intelligence", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Despite extraordinary progress in artificial intelligence (AI), modern\nsystems remain incomplete representations of human cognition. Vision, audition,\nand language have received disproportionate attention due to well-defined\nbenchmarks, standardized datasets, and consensus-driven scientific foundations.\nIn contrast, olfaction - a high-bandwidth, evolutionarily critical sense - has\nbeen largely overlooked. This omission presents a foundational gap in the\nconstruction of truly embodied and ethically aligned super-human intelligence.\nWe argue that the exclusion of olfactory perception from AI architectures is\nnot due to irrelevance but to structural challenges: unresolved scientific\ntheories of smell, heterogeneous sensor technologies, lack of standardized\nolfactory datasets, absence of AI-oriented benchmarks, and difficulty in\nevaluating sub-perceptual signal processing. These obstacles have hindered the\ndevelopment of machine olfaction despite its tight coupling with memory,\nemotion, and contextual reasoning in biological systems. In this position\npaper, we assert that meaningful progress toward general and embodied\nintelligence requires serious investment in olfactory research by the AI\ncommunity. We call for cross-disciplinary collaboration - spanning\nneuroscience, robotics, machine learning, and ethics - to formalize olfactory\nbenchmarks, develop multimodal datasets, and define the sensory capabilities\nnecessary for machines to understand, navigate, and act within human\nenvironments. Recognizing olfaction as a core modality is essential not only\nfor scientific completeness, but for building AI systems that are ethically\ngrounded in the full scope of the human experience.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u5c06\u55c5\u89c9\u7814\u7a76\u7eb3\u5165AI\u9886\u57df\uff0c\u4ee5\u5f25\u8865\u5176\u5728\u901a\u7528\u667a\u80fd\u548c\u4f26\u7406\u5bf9\u9f50\u4e2d\u7684\u7f3a\u5931\u3002", "motivation": "\u73b0\u4ee3AI\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u55c5\u89c9\u7684\u8003\u8651\uff0c\u800c\u55c5\u89c9\u5bf9\u8bb0\u5fc6\u3001\u60c5\u611f\u7b49\u5177\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u5176\u7f3a\u5931\u4f1a\u5bfc\u81f4AI\u5728\u4eba\u7c7b\u4f53\u9a8c\u4e2d\u7684\u4e0d\u5b8c\u6574\u6027\u3002", "method": "\u547c\u5401\u8de8\u5b66\u79d1\u5408\u4f5c\uff0c\u5305\u62ec\u795e\u7ecf\u79d1\u5b66\u3001\u673a\u5668\u4eba\u5b66\u548c\u673a\u5668\u5b66\u4e60\uff0c\u5236\u5b9a\u55c5\u89c9\u57fa\u51c6\u3001\u5f00\u53d1\u6570\u636e\u96c6\u3002", "result": "\u5f3a\u8c03\u4e86\u55c5\u89c9\u7814\u7a76\u5bf9AI\u7cfb\u7edf\u79d1\u5b66\u5b8c\u6574\u6027\u548c\u4f26\u7406\u57fa\u7840\u7684\u91cd\u8981\u6027\u3002", "conclusion": "AI\u793e\u533a\u5e94\u91cd\u89c6\u55c5\u89c9\u7814\u7a76\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5168\u9762\u548c\u4f26\u7406\u5bf9\u9f50\u7684\u667a\u80fd\u7cfb\u7edf\u3002", "keywords": "\u4eba\u5de5\u667a\u80fd, \u55c5\u89c9, \u8de8\u5b66\u79d1, \u4f26\u7406\u5bf9\u9f50"}}
{"id": "2506.00244", "pdf": "https://arxiv.org/pdf/2506.00244", "abs": "https://arxiv.org/abs/2506.00244", "authors": ["Pintu Kumar", "Nandyala Hemachandra"], "title": "DeGLIF for Label Noise Robust Node Classification using GNNs", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Noisy labelled datasets are generally inexpensive compared to clean labelled\ndatasets, and the same is true for graph data. In this paper, we propose a\ndenoising technique DeGLIF: Denoising Graph Data using Leave-One-Out Influence\nFunction. DeGLIF uses a small set of clean data and the leave-one-out influence\nfunction to make label noise robust node-level prediction on graph data.\nLeave-one-out influence function approximates the change in the model\nparameters if a training point is removed from the training dataset. Recent\nadvances propose a way to calculate the leave-one-out influence function for\nGraph Neural Networks (GNNs). We extend that recent work to estimate the change\nin validation loss, if a training node is removed from the training dataset. We\nuse this estimate and a new theoretically motivated relabelling function to\ndenoise the training dataset. We propose two DeGLIF variants to identify noisy\nnodes. Both these variants do not require any information about the noise model\nor the noise level in the dataset; DeGLIF also does not estimate these\nquantities. For one of these variants, we prove that the noisy points detected\ncan indeed increase risk. We carry out detailed computational experiments on\ndifferent datasets to show the effectiveness of DeGLIF. It achieves better\naccuracy than other baseline algorithms", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeGLIF\u7684\u53bb\u566a\u6280\u672f\uff0c\u5229\u7528\u7559\u4e00\u5f71\u54cd\u51fd\u6570\u548c\u5c0f\u91cf\u5e72\u51c0\u6570\u636e\u5bf9\u56fe\u6570\u636e\u4e2d\u7684\u566a\u58f0\u6807\u7b7e\u8fdb\u884c\u9c81\u68d2\u6027\u8282\u70b9\u7ea7\u9884\u6d4b\u3002", "motivation": "\u5e26\u566a\u58f0\u6807\u7b7e\u7684\u6570\u636e\u96c6\u901a\u5e38\u6bd4\u5e72\u51c0\u6807\u7b7e\u6570\u636e\u96c6\u66f4\u4fbf\u5b9c\uff0c\u4f46\u4f1a\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u56fe\u6570\u636e\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "DeGLIF\u4f7f\u7528\u7559\u4e00\u5f71\u54cd\u51fd\u6570\u4f30\u8ba1\u8bad\u7ec3\u8282\u70b9\u79fb\u9664\u5bf9\u9a8c\u8bc1\u635f\u5931\u7684\u5f71\u54cd\uff0c\u5e76\u7ed3\u5408\u65b0\u7684\u7406\u8bba\u9a71\u52a8\u91cd\u6807\u7b7e\u51fd\u6570\u53bb\u566a\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u65e0\u9700\u566a\u58f0\u6a21\u578b\u6216\u566a\u58f0\u6c34\u5e73\u4fe1\u606f\u7684\u53d8\u4f53\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDeGLIF\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u7b97\u6cd5\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u566a\u58f0\u8282\u70b9\u5e76\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "DeGLIF\u662f\u4e00\u79cd\u65e0\u9700\u566a\u58f0\u5148\u9a8c\u4fe1\u606f\u7684\u6709\u6548\u56fe\u6570\u636e\u53bb\u566a\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "keywords": "\u56fe\u6570\u636e, \u53bb\u566a, \u7559\u4e00\u5f71\u54cd\u51fd\u6570, \u56fe\u795e\u7ecf\u7f51\u7edc, \u8282\u70b9\u7ea7\u9884\u6d4b"}}
{"id": "2506.00204", "pdf": "https://arxiv.org/pdf/2506.00204", "abs": "https://arxiv.org/abs/2506.00204", "authors": ["Linyuan Gong", "Alvin Cheung", "Mostafa Elhoushi", "Sida Wang"], "title": "Structure-Aware Fill-in-the-Middle Pretraining for Code", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "14 pages", "summary": "Fill-in-the-Middle (FIM) is a common pretraining method for code LLMs, where\nmodels complete code segments given surrounding context. However, existing LLMs\ntreat code as plain text and mask random character spans. We propose and\nevaluate AST-FIM, a pretraining strategy that leverages Abstract Syntax Trees\n(ASTs) to mask complete syntactic structures at scale, ensuring coherent\ntraining examples better aligned with universal code structures and common code\nediting patterns such as blocks, expressions, or functions. To evaluate\nreal-world fill-in-the-middle (FIM) programming tasks, we introduce\nReal-FIM-Eval, a benchmark derived from 30,000+ GitHub commits across 12\nlanguages. On infilling tasks, experiments on 1B and 8B parameter models show\nthat AST-FIM is particularly beneficial for real-world code editing as it\noutperforms standard random-character FIM by up to 5 pts on standard FIM\nbenchmarks. Our code is publicly available at\nhttps://github.com/gonglinyuan/ast_fim.", "AI": {"tldr": "AST-FIM\u662f\u4e00\u79cd\u57fa\u4e8e\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c4f\u853d\u5b8c\u6574\u7684\u8bed\u6cd5\u7ed3\u6784\u6765\u63d0\u9ad8\u4ee3\u7801LLM\u7684\u586b\u5145\u80fd\u529b\uff0c\u4f18\u4e8e\u4f20\u7edf\u968f\u673a\u5b57\u7b26\u5c4f\u853d\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4ee3\u7801LLM\u5c06\u4ee3\u7801\u89c6\u4e3a\u666e\u901a\u6587\u672c\uff0c\u968f\u673a\u5c4f\u853d\u5b57\u7b26\u8de8\u5ea6\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u4ee3\u7801\u7ed3\u6784\u548c\u5e38\u89c1\u7f16\u8f91\u6a21\u5f0f\uff0c\u5982\u4ee3\u7801\u5757\u3001\u8868\u8fbe\u5f0f\u6216\u51fd\u6570\u3002", "method": "\u63d0\u51fa\u4e86AST-FIM\u65b9\u6cd5\uff0c\u5229\u7528AST\u5c4f\u853d\u5b8c\u6574\u8bed\u6cd5\u7ed3\u6784\uff0c\u751f\u6210\u66f4\u7b26\u5408\u4ee3\u7801\u7ed3\u6784\u548c\u7f16\u8f91\u6a21\u5f0f\u7684\u8bad\u7ec3\u793a\u4f8b\u3002", "result": "\u57281B\u548c8B\u53c2\u6570\u6a21\u578b\u4e0a\uff0cAST-FIM\u5728\u6807\u51c6FIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u968f\u673a\u5b57\u7b26\u5c4f\u853d\u65b9\u6cd5\u9ad8\u51fa5\u5206\uff0c\u5c24\u5176\u9002\u5408\u5b9e\u9645\u4ee3\u7801\u7f16\u8f91\u4efb\u52a1\u3002", "conclusion": "AST-FIM\u901a\u8fc7\u5229\u7528\u4ee3\u7801\u7684\u8bed\u6cd5\u7ed3\u6784\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u586b\u5145\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4ee3\u7801LLM\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "AST-FIM, Abstract Syntax Tree, Fill-in-the-Middle, pretraining, code LLM"}}
{"id": "2506.00417", "pdf": "https://arxiv.org/pdf/2506.00417", "abs": "https://arxiv.org/abs/2506.00417", "authors": ["Changyuan Zhao", "Ruichen Zhang", "Jiacheng Wang", "Gaosheng Zhao", "Dusit Niyato", "Geng Sun", "Shiwen Mao", "Dong In Kim"], "title": "World Models for Cognitive Agents: Transforming Edge Intelligence in Future Networks", "categories": ["cs.AI"], "comment": "7 pages, 4 figures", "summary": "World models are emerging as a transformative paradigm in artificial\nintelligence, enabling agents to construct internal representations of their\nenvironments for predictive reasoning, planning, and decision-making. By\nlearning latent dynamics, world models provide a sample-efficient framework\nthat is especially valuable in data-constrained or safety-critical scenarios.\nIn this paper, we present a comprehensive overview of world models,\nhighlighting their architecture, training paradigms, and applications across\nprediction, generation, planning, and causal reasoning. We compare and\ndistinguish world models from related concepts such as digital twins, the\nmetaverse, and foundation models, clarifying their unique role as embedded\ncognitive engines for autonomous agents. We further propose Wireless Dreamer, a\nnovel world model-based reinforcement learning framework tailored for wireless\nedge intelligence optimization, particularly in low-altitude wireless networks\n(LAWNs). Through a weather-aware UAV trajectory planning case study, we\ndemonstrate the effectiveness of our framework in improving learning efficiency\nand decision quality.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4e16\u754c\u6a21\u578b\u5728\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u91cd\u8981\u6027\u53ca\u5176\u591a\u9886\u57df\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWireless Dreamer\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u7ebf\u8fb9\u7f18\u667a\u80fd\u4f18\u5316\u3002", "motivation": "\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u8303\u5f0f\uff0c\u80fd\u591f\u5e2e\u52a9\u667a\u80fd\u4f53\u9ad8\u6548\u5730\u5b66\u4e60\u548c\u9884\u6d4b\u73af\u5883\u52a8\u6001\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u53d7\u9650\u6216\u5b89\u5168\u5173\u952e\u7684\u573a\u666f\u4e0b\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u4e16\u754c\u6a21\u578b\u7684\u67b6\u6784\u3001\u8bad\u7ec3\u8303\u5f0f\uff0c\u5e76\u63d0\u51faWireless Dreamer\u6846\u67b6\uff0c\u5e94\u7528\u4e8e\u65e0\u7ebf\u7f51\u7edc\u4f18\u5316\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u5929\u6c14\u611f\u77e5\u7684\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u6848\u4f8b\u4e2d\u5c55\u793a\u4e86\u5b66\u4e60\u6548\u7387\u548c\u51b3\u7b56\u8d28\u91cf\u7684\u63d0\u5347\u3002", "conclusion": "\u4e16\u754c\u6a21\u578b\u5728\u667a\u80fd\u4f53\u51b3\u7b56\u4e2d\u5177\u6709\u6f5c\u529b\uff0cWireless Dreamer\u6846\u67b6\u4e3a\u65e0\u7ebf\u7f51\u7edc\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u4e16\u754c\u6a21\u578b, \u4eba\u5de5\u667a\u80fd, \u65e0\u7ebf\u7f51\u7edc\u4f18\u5316, \u5f3a\u5316\u5b66\u4e60, UAV"}}
{"id": "2506.00245", "pdf": "https://arxiv.org/pdf/2506.00245", "abs": "https://arxiv.org/abs/2506.00245", "authors": ["Dang Nguyen", "Ali Payani", "Baharan Mirzasoleiman"], "title": "Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity", "categories": ["cs.LG", "cs.CL"], "comment": "11 pages, 4 figures, 6 tables, link:\n  https://github.com/BigML-CS-UCLA/SNNE", "summary": "Hallucination in large language models (LLMs) can be detected by assessing\nthe uncertainty of model outputs, typically measured using entropy. Semantic\nentropy (SE) enhances traditional entropy estimation by quantifying uncertainty\nat the semantic cluster level. However, as modern LLMs generate longer\none-sentence responses, SE becomes less effective because it overlooks two\ncrucial factors: intra-cluster similarity (the spread within a cluster) and\ninter-cluster similarity (the distance between clusters). To address these\nlimitations, we propose a simple black-box uncertainty quantification method\ninspired by nearest neighbor estimates of entropy. Our approach can also be\neasily extended to white-box settings by incorporating token probabilities.\nAdditionally, we provide theoretical results showing that our method\ngeneralizes semantic entropy. Extensive empirical results demonstrate its\neffectiveness compared to semantic entropy across two recent LLMs (Phi3 and\nLlama3) and three common text generation tasks: question answering, text\nsummarization, and machine translation. Our code is available at\nhttps://github.com/BigML-CS-UCLA/SNNE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6700\u8fd1\u90bb\u71b5\u4f30\u8ba1\u7684\u9ed1\u76d2\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u957f\u53e5\u751f\u6210\u4e2d\u8bed\u4e49\u71b5\uff08SE\uff09\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u957f\u53e5\u65f6\uff0c\u4f20\u7edf\u8bed\u4e49\u71b5\uff08SE\uff09\u5ffd\u7565\u4e86\u805a\u7c7b\u5185\u548c\u805a\u7c7b\u95f4\u76f8\u4f3c\u6027\uff0c\u5bfc\u81f4\u5e7b\u89c9\u68c0\u6d4b\u6548\u679c\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u9ed1\u76d2\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6700\u8fd1\u90bb\u71b5\u4f30\u8ba1\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u767d\u76d2\u8bbe\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728Phi3\u548cLlama3\u6a21\u578b\u53ca\u4e09\u5927\u6587\u672c\u751f\u6210\u4efb\u52a1\uff08\u95ee\u7b54\u3001\u6458\u8981\u3001\u7ffb\u8bd1\uff09\u4e2d\u4f18\u4e8e\u4f20\u7edf\u8bed\u4e49\u71b5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86SE\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u5e7b\u89c9\u68c0\u6d4b\u6548\u679c\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u5e7b\u89c9\u68c0\u6d4b,\u8bed\u4e49\u71b5,\u4e0d\u786e\u5b9a\u6027\u91cf\u5316,\u6700\u8fd1\u90bb\u4f30\u8ba1"}}
{"id": "2506.00210", "pdf": "https://arxiv.org/pdf/2506.00210", "abs": "https://arxiv.org/abs/2506.00210", "authors": ["Ziji Zhang", "Michael Yang", "Zhiyu Chen", "Yingying Zhuang", "Shu-Ting Pi", "Qun Liu", "Rajashekar Maragoud", "Vy Nguyen", "Anurag Beniwal"], "title": "REIC: RAG-Enhanced Intent Classification at Scale", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Accurate intent classification is critical for efficient routing in customer\nservice, ensuring customers are connected with the most suitable agents while\nreducing handling times and operational costs. However, as companies expand\ntheir product lines, intent classification faces scalability challenges due to\nthe increasing number of intents and variations in taxonomy across different\nverticals. In this paper, we introduce REIC, a Retrieval-augmented generation\nEnhanced Intent Classification approach, which addresses these challenges\neffectively. REIC leverages retrieval-augmented generation (RAG) to dynamically\nincorporate relevant knowledge, enabling precise classification without the\nneed for frequent retraining. Through extensive experiments on real-world\ndatasets, we demonstrate that REIC outperforms traditional fine-tuning,\nzero-shot, and few-shot methods in large-scale customer service settings. Our\nresults highlight its effectiveness in both in-domain and out-of-domain\nscenarios, demonstrating its potential for real-world deployment in adaptive\nand large-scale intent classification systems.", "AI": {"tldr": "REIC\u662f\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u610f\u56fe\u5206\u7c7b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u5ba2\u670d\u573a\u666f\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f01\u4e1a\u4ea7\u54c1\u7ebf\u6269\u5c55\u5bfc\u81f4\u610f\u56fe\u5206\u7c7b\u9762\u4e34\u89c4\u6a21\u5316\u548c\u5206\u7c7b\u6807\u51c6\u53d8\u5316\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u52a8\u6001\u6574\u5408\u76f8\u5173\u77e5\u8bc6\uff0c\u907f\u514d\u9891\u7e41\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cREIC\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\u3001\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002", "conclusion": "REIC\u5728\u5927\u89c4\u6a21\u81ea\u9002\u5e94\u610f\u56fe\u5206\u7c7b\u7cfb\u7edf\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u610f\u56fe\u5206\u7c7b,\u68c0\u7d22\u589e\u5f3a\u751f\u6210,\u5ba2\u670d,\u5927\u89c4\u6a21\u5206\u7c7b"}}
{"id": "2506.00430", "pdf": "https://arxiv.org/pdf/2506.00430", "abs": "https://arxiv.org/abs/2506.00430", "authors": ["Nicole Hsing"], "title": "MIRROR: Cognitive Inner Monologue Between Conversational Turns for Persistent Reflection and Reasoning in Conversational LLMs", "categories": ["cs.AI"], "comment": null, "summary": "Human intelligence relies on inner monologue to process complex information\nthrough simultaneous reflection, memory retrieval, and response formulation. We\nintroduce MIRROR (Modular Internal Reasoning, Reflection, Orchestration, and\nResponse), a cognitive architecture that systematically implements these\nparallel reasoning capabilities in large language models. MIRROR operates as a\nunified system with two distinct functional layers: the Thinker and the Talker.\nThe Thinker encompasses: (1) the Inner Monologue Manager, coordinating\nreasoning threads across cognitive dimensions (Goals, Reasoning, and Memory);\nand (2) the Cognitive Controller, synthesizing these threads into a coherent\ninternal narrative maintained across conversation turns. The Talker component\nthen leverages this integrated narrative for context-aware responses. Evaluated\non the CuRaTe benchmark--testing personalized dialogue with safety-critical\nconstraints, conflicting preferences, and multi-turn consistency--LLMs\nutilizing the MIRROR architecture achieve up to 156% relative improvement in\ncritical safety scenarios involving three persons with conflicting preferences,\nmaintaining an average accuracy of ~>80% on all scenarios. Across\nscenario-specific comparisons, GPT-4o, Gemini 1.5 Pro, Claude 3.7 Sonnet, Llama\n4 variants, and Mistral 3 variants with the MIRROR architecture outperformed\nbaseline models by 21% on average (15.5 percentage points absolute). MIRROR\ndirectly addresses three critical LLM failure modes: sycophancy, attentional\ndeficits to critical information, and inconsistent prioritization of\nconflicting constraints. This work bridges cognitive science and AI by\nimplementing modular internal reasoning inspired by human cognition, creating a\npersistent internal model that significantly enhances multi-turn conversation\ncapabilities.", "AI": {"tldr": "MIRROR\u662f\u4e00\u79cd\u8ba4\u77e5\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u7684\u5185\u7701\u5f0f\u601d\u7ef4\u6765\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u5e76\u884c\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u6539\u5584\u4e86\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5b89\u5168\u6027\u3001\u4e00\u81f4\u6027\u548c\u7ea6\u675f\u4f18\u5148\u7ea7\u5904\u7406\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u5185\u7701\u5f0f\u601d\u7ef4\u5904\u7406\u590d\u6742\u4fe1\u606f\uff0c\u800c\u5f53\u524d\u7684LLM\u5728\u6b64\u529f\u80fd\u4e0a\u5b58\u5728\u7f3a\u9677\uff08\u5982\u9022\u8fce\u3001\u6ce8\u610f\u529b\u7f3a\u5931\u548c\u7ea6\u675f\u4f18\u5148\u7ea7\u4e0d\u4e00\u81f4\uff09\u3002MIRROR\u65e8\u5728\u901a\u8fc7\u6a21\u5757\u5316\u8ba4\u77e5\u67b6\u6784\u5f25\u8865\u8fd9\u4e9b\u7f3a\u9677\u3002", "method": "MIRROR\u7531Thinker\u548cTalker\u4e24\u5c42\u7ec4\u6210\uff1aThinker\u8d1f\u8d23\u534f\u8c03\u76ee\u6807\u3001\u63a8\u7406\u548c\u8bb0\u5fc6\u7b49\u7ef4\u5ea6\u7684\u5e76\u884c\u7ebf\u7a0b\uff0c\u5e76\u751f\u6210\u8fde\u8d2f\u7684\u5167\u90e8\u5206\u6790\uff1bTalker\u57fa\u4e8e\u6b64\u5206\u6790\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u54cd\u5e94\u3002", "result": "\u5728CuRaTe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMIRROR\u4f7fLLM\u5728\u6d89\u53ca\u51b2\u7a81\u504f\u597d\u7684\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u8868\u73b0\u63d0\u5347156%\uff0c\u5e73\u5747\u51c6\u786e\u7387>80%\uff0c\u4e14\u5728\u591a\u79cd\u6a21\u578b\u4e0a\u5e73\u5747\u4f18\u4e8e\u57fa\u7ebf21%\uff0815.5\u4e2a\u767e\u5206\u70b9\uff09\u3002", "conclusion": "MIRROR\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u7684\u6a21\u5757\u5316\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u80fd\u529b\uff0c\u4e3a\u8ba4\u77e5\u79d1\u5b66\u4e0eAI\u7684\u878d\u5408\u63d0\u4f9b\u4e86\u8303\u4f8b\u3002", "keywords": "\u5185\u7701\u5f0f\u601d\u7ef4, \u8ba4\u77e5\u67b6\u6784, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u591a\u8f6e\u5bf9\u8bdd, \u5e76\u884c\u63a8\u7406"}}
{"id": "2506.00247", "pdf": "https://arxiv.org/pdf/2506.00247", "abs": "https://arxiv.org/abs/2506.00247", "authors": ["Aasish Kumar Sharma", "Sanjeeb Prashad Pandey", "Julian M. Kunkel"], "title": "Performance Analysis of Convolutional Neural Network By Applying Unconstrained Binary Quadratic Programming", "categories": ["cs.LG", "cs.ET"], "comment": "11 pages, 22 figures, accepted in IEEE COMPSAC 2025 Conference.\n  Preprint before peer review", "summary": "Convolutional Neural Networks (CNNs) are pivotal in computer vision and Big\nData analytics but demand significant computational resources when trained on\nlarge-scale datasets. Conventional training via back-propagation (BP) with\nlosses like Mean Squared Error or Cross-Entropy often requires extensive\niterations and may converge sub-optimally. Quantum computing offers a promising\nalternative by leveraging superposition, tunneling, and entanglement to search\ncomplex optimization landscapes more efficiently. In this work, we propose a\nhybrid optimization method that combines an Unconstrained Binary Quadratic\nProgramming (UBQP) formulation with Stochastic Gradient Descent (SGD) to\naccelerate CNN training. Evaluated on the MNIST dataset, our approach achieves\na 10--15\\% accuracy improvement over a standard BP-CNN baseline while\nmaintaining similar execution times. These results illustrate the potential of\nhybrid quantum-classical techniques in High-Performance Computing (HPC)\nenvironments for Big Data and Deep Learning. Fully realizing these benefits,\nhowever, requires a careful alignment of algorithmic structures with underlying\nquantum mechanisms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u65e0\u7ea6\u675f\u4e8c\u5143\u4e8c\u6b21\u89c4\u5212\uff08UBQP\uff09\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u7684\u6df7\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a0\u901f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u8bad\u7ec3\uff0c\u5e76\u5728MNIST\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8610-15%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u7684CNN\u8bad\u7ec3\u65b9\u6cd5\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e14\u53ef\u80fd\u5b58\u5728\u6b21\u4f18\u6536\u655b\u95ee\u9898\u3002\u91cf\u5b50\u8ba1\u7b97\u901a\u8fc7\u5229\u7528\u53e0\u52a0\u3001\u96a7\u7a7f\u548c\u7ea0\u7f20\u7b49\u7279\u6027\uff0c\u6709\u671b\u66f4\u9ad8\u6548\u5730\u641c\u7d22\u590d\u6742\u7684\u4f18\u5316\u7a7a\u95f4\uff0c\u4ece\u800c\u63d0\u4f9b\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u65e0\u7ea6\u675f\u4e8c\u5143\u4e8c\u6b21\u89c4\u5212\uff08UBQP\uff09\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\uff0c\u4ee5\u52a0\u901fCNN\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728MNIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u6807\u51c6\u7684BP-CNN\u57fa\u7ebf\u63d0\u9ad8\u4e8610-15%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u8fd1\u7684\u6267\u884c\u65f6\u95f4\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u5c55\u793a\u4e86\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6280\u672f\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u73af\u5883\u4e2d\u7684\u6f5c\u529b\uff0c\u4f46\u4e3a\u4e86\u5145\u5206\u53d1\u6325\u5176\u4f18\u52bf\uff0c\u9700\u8981\u5c06\u7b97\u6cd5\u7ed3\u6784\u4e0e\u5e95\u5c42\u91cf\u5b50\u673a\u5236\u7d27\u5bc6\u7ed3\u5408\u3002", "keywords": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u3001\u91cf\u5b50\u8ba1\u7b97\u3001\u65e0\u7ea6\u675f\u4e8c\u5143\u4e8c\u6b21\u89c4\u5212\uff08UBQP\uff09\u3001\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u3001\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09"}}
{"id": "2506.00232", "pdf": "https://arxiv.org/pdf/2506.00232", "abs": "https://arxiv.org/abs/2506.00232", "authors": ["Ruofan Wu", "Youngwon Lee", "Fan Shu", "Danmei Xu", "Seung-won Hwang", "Zhewei Yao", "Yuxiong He", "Feng Yan"], "title": "ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems are increasingly diverse, yet\nmany suffer from monolithic designs that tightly couple core functions like\nquery reformulation, retrieval, reasoning, and verification. This limits their\ninterpretability, systematic evaluation, and targeted improvement, especially\nfor complex multi-hop question answering. We introduce ComposeRAG, a novel\nmodular abstraction that decomposes RAG pipelines into atomic, composable\nmodules. Each module, such as Question Decomposition, Query Rewriting,\nRetrieval Decision, and Answer Verification, acts as a parameterized\ntransformation on structured inputs/outputs, allowing independent\nimplementation, upgrade, and analysis. To enhance robustness against errors in\nmulti-step reasoning, ComposeRAG incorporates a self-reflection mechanism that\niteratively revisits and refines earlier steps upon verification failure.\nEvaluated on four challenging multi-hop QA benchmarks, ComposeRAG consistently\noutperforms strong baselines in both accuracy and grounding fidelity.\nSpecifically, it achieves up to a 15% accuracy improvement over\nfine-tuning-based methods and up to a 5% gain over reasoning-specialized\npipelines under identical retrieval conditions. Crucially, ComposeRAG\nsignificantly enhances grounding: its verification-first design reduces\nungrounded answers by over 10% in low-quality retrieval settings, and by\napproximately 3% even with strong corpora. Comprehensive ablation studies\nvalidate the modular architecture, demonstrating distinct and additive\ncontributions from each component. These findings underscore ComposeRAG's\ncapacity to deliver flexible, transparent, scalable, and high-performing\nmulti-hop reasoning with improved grounding and interpretability.", "AI": {"tldr": "ComposeRAG\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u5757\u5316RAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u6838\u5fc3\u529f\u80fd\u4e3a\u539f\u5b50\u6a21\u5757\uff08\u5982\u95ee\u9898\u5206\u89e3\u3001\u67e5\u8be2\u6539\u5199\u3001\u68c0\u7d22\u51b3\u7b56\u548c\u7b54\u6848\u9a8c\u8bc1\uff09\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u8bc4\u4f30\u6027\u548c\u6027\u80fd\u3002\u5728\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0cComposeRAG\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u51c6\u786e\u6027\u548c\u7b54\u6848\u57fa\u7840\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u73b0\u6709\u7684RAG\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u7d27\u5bc6\u8026\u5408\u7684\u5355\u4f53\u8bbe\u8ba1\uff0c\u9650\u5236\u4e86\u5176\u53ef\u89e3\u91ca\u6027\u3001\u7cfb\u7edf\u6027\u8bc4\u4f30\u548c\u9488\u5bf9\u6027\u6539\u8fdb\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7684\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u3002ComposeRAG\u65e8\u5728\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "ComposeRAG\u5c06RAG\u6d41\u6c34\u7ebf\u5206\u89e3\u4e3a\u539f\u5b50\u5316\u7684\u53ef\u7ec4\u5408\u6a21\u5757\uff08\u5982\u95ee\u9898\u5206\u89e3\u3001\u67e5\u8be2\u6539\u5199\u7b49\uff09\uff0c\u6bcf\u4e2a\u6a21\u5757\u72ec\u7acb\u5b9e\u73b0\u548c\u4f18\u5316\uff0c\u5e76\u5f15\u5165\u81ea\u53cd\u601d\u673a\u5236\u4ee5\u589e\u5f3a\u591a\u6b65\u63a8\u7406\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u56db\u4e2a\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cComposeRAG\u5728\u51c6\u786e\u6027\u548c\u7b54\u6848\u57fa\u7840\u6027\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6700\u9ad8\u63d0\u534715%\u7684\u51c6\u786e\u7387\u548c10%\u7684\u57fa\u7840\u6027\uff08\u4f4e\u8d28\u91cf\u68c0\u7d22\u6761\u4ef6\u4e0b\uff09\u3002", "conclusion": "ComposeRAG\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u81ea\u53cd\u601d\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u3001\u900f\u660e\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\uff0c\u540c\u65f6\u6539\u5584\u4e86\u7b54\u6848\u7684\u57fa\u7840\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "keywords": "RAG, \u6a21\u5757\u5316\u8bbe\u8ba1, \u591a\u8df3\u95ee\u7b54, \u81ea\u53cd\u601d, \u7b54\u6848\u9a8c\u8bc1"}}
{"id": "2506.00496", "pdf": "https://arxiv.org/pdf/2506.00496", "abs": "https://arxiv.org/abs/2506.00496", "authors": ["Ashutosh Gupta", "Thomas A. Henzinger", "Konstantin Kueffner", "Kaushik Mallik", "David Pape"], "title": "Monitoring Robustness and Individual Fairness", "categories": ["cs.AI"], "comment": null, "summary": "Input-output robustness appears in various different forms in the literature,\nsuch as robustness of AI models to adversarial or semantic perturbations and\nindividual fairness of AI models that make decisions about humans.\n  We propose runtime monitoring of input-output robustness of deployed,\nblack-box AI models, where the goal is to design monitors that would observe\none long execution sequence of the model, and would raise an alarm whenever it\nis detected that two similar inputs from the past led to dissimilar outputs.\n  This way, monitoring will complement existing offline ``robustification''\napproaches to increase the trustworthiness of AI decision-makers.\n  We show that the monitoring problem can be cast as the fixed-radius nearest\nneighbor (FRNN) search problem, which, despite being well-studied, lacks\nsuitable online solutions.\n  We present our tool Clemont, which offers a number of lightweight monitors,\nsome of which use upgraded online variants of existing FRNN algorithms, and one\nuses a novel algorithm based on binary decision diagrams -- a data-structure\ncommonly used in software and hardware verification.\n  We have also developed an efficient parallelization technique that can\nsubstantially cut down the computation time of monitors for which the distance\nbetween input-output pairs is measured using the $L_\\infty$ norm.\n  Using standard benchmarks from the literature of adversarial and semantic\nrobustness and individual fairness, we perform a comparative study of different\nmonitors in \\tool, and demonstrate their effectiveness in correctly detecting\nrobustness violations at runtime.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8fd0\u884c\u65f6\u76d1\u63a7\u65b9\u6cd5\u6765\u589e\u5f3a\u9ed1\u76d2AI\u6a21\u578b\u7684\u8f93\u5165\u8f93\u51fa\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u76d1\u6d4b\u76f8\u4f3c\u8f93\u5165\u4ea7\u751f\u4e0d\u76f8\u4f3c\u8f93\u51fa\u7684\u60c5\u51b5\u3002", "motivation": "\u73b0\u6709AI\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff08\u5982\u5bf9\u6297\u6270\u52a8\u3001\u8bed\u4e49\u6270\u52a8\u548c\u4e2a\u4f53\u516c\u5e73\u6027\uff09\u9700\u8981\u5728\u7ebf\u76d1\u63a7\u4ee5\u8865\u5145\u73b0\u6709\u7684\u79bb\u7ebf\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u9ad8\u53ef\u4fe1\u5ea6\u3002", "method": "\u5c06\u76d1\u63a7\u95ee\u9898\u8f6c\u5316\u4e3a\u56fa\u5b9a\u534a\u5f84\u6700\u8fd1\u90bb\uff08FRNN\uff09\u641c\u7d22\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u5de5\u5177Clemont\uff0c\u5305\u542b\u591a\u79cd\u8f7b\u91cf\u7ea7\u76d1\u63a7\u5668\uff0c\u90e8\u5206\u57fa\u4e8e\u5347\u7ea7\u7684\u5728\u7ebfFRNN\u7b97\u6cd5\uff0c\u53e6\u4e00\u90e8\u5206\u57fa\u4e8e\u51b3\u7b56\u56fe\u7684\u65b0\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u76d1\u63a7\u5668\u5728\u8fd0\u884c\u65f6\u6709\u6548\u68c0\u6d4b\u9c81\u68d2\u6027\u8fdd\u89c4\u7684\u80fd\u529b\u3002", "conclusion": "\u8fd0\u884c\u65f6\u76d1\u63a7\u662f\u4e00\u79cd\u6709\u6548\u7684\u8865\u5145\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u5347AI\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u3002", "keywords": "\u8f93\u5165\u8f93\u51fa\u9c81\u68d2\u6027\u3001\u8fd0\u884c\u65f6\u76d1\u63a7\u3001\u56fa\u5b9a\u534a\u5f84\u6700\u8fd1\u90bb\u641c\u7d22\u3001AI\u53ef\u4fe1\u5ea6"}}
{"id": "2506.00259", "pdf": "https://arxiv.org/pdf/2506.00259", "abs": "https://arxiv.org/abs/2506.00259", "authors": ["Zhengyang Fan", "Wanru Li", "Kuo-chu Chang", "Ting Yuan"], "title": "PerFormer: A Permutation Based Vision Transformer for Remaining Useful Life Prediction", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Accurately estimating the remaining useful life (RUL) for degradation systems\nis crucial in modern prognostic and health management (PHM). Convolutional\nNeural Networks (CNNs), initially developed for tasks like image and video\nrecognition, have proven highly effectively in RUL prediction, demonstrating\nremarkable performance. However, with the emergence of the Vision Transformer\n(ViT), a Transformer model tailored for computer vision tasks such as image\nclassification, and its demonstrated superiority over CNNs, there is a natural\ninclination to explore its potential in enhancing RUL prediction accuracy.\nNonetheless, applying ViT directly to multivariate sensor data for RUL\nprediction poses challenges, primarily due to the ambiguous nature of spatial\ninformation in time series data. To address this issue, we introduce the\nPerFormer, a permutation-based vision transformer approach designed to permute\nmultivariate time series data, mimicking spatial characteristics akin to image\ndata, thereby making it suitable for ViT. To generate the desired permutation\nmatrix, we introduce a novel permutation loss function aimed at guiding the\nconvergence of any matrix towards a permutation matrix. Our experiments on\nNASA's C-MAPSS dataset demonstrate the PerFormer's superior performance in RUL\nprediction compared to state-of-the-art methods employing CNNs, Recurrent\nNeural Networks (RNNs), and various Transformer models. This underscores its\neffectiveness and potential in PHM applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPerFormer\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u57fa\u4e8eVision Transformer\u7684\u5269\u4f59\u4f7f\u7528\u5bff\u547d\u9884\u6d4b\u7cbe\u5ea6\uff0c\u901a\u8fc7\u6392\u5217\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4ee5\u6a21\u62df\u56fe\u50cf\u7279\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "Vision Transformer\u5728\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u76f4\u63a5\u5e94\u7528\u4e8e\u591a\u5143\u4f20\u611f\u5668\u6570\u636e\u7684RUL\u9884\u6d4b\u9762\u4e34\u6311\u6218\uff0c\u56e0\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u7a7a\u95f4\u4fe1\u606f\u6a21\u7cca\u3002", "method": "\u63d0\u51faPerFormer\u65b9\u6cd5\uff0c\u901a\u8fc7\u6392\u5217\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u6a21\u62df\u56fe\u50cf\u7a7a\u95f4\u7279\u6027\uff0c\u5e76\u8bbe\u8ba1\u65b0\u578b\u6392\u5217\u635f\u5931\u51fd\u6570\u4ee5\u751f\u6210\u6392\u5217\u77e9\u9635\u3002", "result": "\u5728NASA\u7684C-MAPSS\u6570\u636e\u96c6\u4e0a\uff0cPerFormer\u5728RUL\u9884\u6d4b\u4e0a\u4f18\u4e8eCNN\u3001RNN\u53ca\u5176\u4ed6Transformer\u6a21\u578b\u3002", "conclusion": "PerFormer\u8bc1\u660e\u4e86\u5176\u5728PHM\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002", "keywords": "RUL prediction, Vision Transformer, PerFormer, multivariate time series, PHM"}}
{"id": "2506.00235", "pdf": "https://arxiv.org/pdf/2506.00235", "abs": "https://arxiv.org/abs/2506.00235", "authors": ["Yexiao He", "Ang Li", "Boyi Liu", "Zhewei Yao", "Yuxiong He"], "title": "MedOrch: Medical Diagnosis with Tool-Augmented Reasoning Agents for Flexible Extensibility", "categories": ["cs.CL"], "comment": null, "summary": "Healthcare decision-making represents one of the most challenging domains for\nArtificial Intelligence (AI), requiring the integration of diverse knowledge\nsources, complex reasoning, and various external analytical tools. Current AI\nsystems often rely on either task-specific models, which offer limited\nadaptability, or general language models without grounding with specialized\nexternal knowledge and tools. We introduce MedOrch, a novel framework that\norchestrates multiple specialized tools and reasoning agents to provide\ncomprehensive medical decision support. MedOrch employs a modular, agent-based\narchitecture that facilitates the flexible integration of domain-specific tools\nwithout altering the core system. Furthermore, it ensures transparent and\ntraceable reasoning processes, enabling clinicians to meticulously verify each\nintermediate step underlying the system's recommendations. We evaluate MedOrch\nacross three distinct medical applications: Alzheimer's disease diagnosis,\nchest X-ray interpretation, and medical visual question answering, using\nauthentic clinical datasets. The results demonstrate MedOrch's competitive\nperformance across these diverse medical tasks. Notably, in Alzheimer's disease\ndiagnosis, MedOrch achieves an accuracy of 93.26%, surpassing the\nstate-of-the-art baseline by over four percentage points. For predicting\nAlzheimer's disease progression, it attains a 50.35% accuracy, marking a\nsignificant improvement. In chest X-ray analysis, MedOrch exhibits superior\nperformance with a Macro AUC of 61.2% and a Macro F1-score of 25.5%. Moreover,\nin complex multimodal visual question answering (Image+Table), MedOrch achieves\nan accuracy of 54.47%. These findings underscore MedOrch's potential to advance\nhealthcare AI by enabling reasoning-driven tool utilization for multimodal\nmedical data processing and supporting intricate cognitive tasks in clinical\ndecision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMedOrch\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u591a\u4e2a\u4e13\u7528\u5de5\u5177\u548c\u63a8\u7406\u4ee3\u7406\uff0c\u4e3a\u533b\u7597\u51b3\u7b56\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u533b\u7597\u51b3\u7b56\u662fAI\u6700\u5177\u6311\u6218\u6027\u7684\u9886\u57df\u4e4b\u4e00\uff0c\u73b0\u6709\u7cfb\u7edf\u8981\u4e48\u9002\u5e94\u6027\u6709\u9650\uff0c\u8981\u4e48\u7f3a\u4e4f\u4e13\u4e1a\u77e5\u8bc6\u652f\u6301\u3002", "method": "MedOrch\u91c7\u7528\u6a21\u5757\u5316\u3001\u57fa\u4e8e\u4ee3\u7406\u7684\u67b6\u6784\uff0c\u7075\u6d3b\u96c6\u6210\u9886\u57df\u4e13\u7528\u5de5\u5177\uff0c\u5e76\u786e\u4fdd\u63a8\u7406\u8fc7\u7a0b\u900f\u660e\u53ef\u8ffd\u6eaf\u3002", "result": "\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u7b49\u4e09\u4e2a\u533b\u7597\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u90e8\u5206\u4efb\u52a1\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "MedOrch\u5c55\u793a\u4e86\u5728\u590d\u6742\u533b\u7597\u51b3\u7b56\u4e2d\u9a71\u52a8\u5de5\u5177\u4f7f\u7528\u548c\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u7684\u6f5c\u529b\u3002", "keywords": "\u533b\u7597\u51b3\u7b56,AI,MedOrch,\u6a21\u5757\u5316\u67b6\u6784,\u591a\u4efb\u52a1\u8bc4\u4f30"}}
{"id": "2506.00530", "pdf": "https://arxiv.org/pdf/2506.00530", "abs": "https://arxiv.org/abs/2506.00530", "authors": ["Tianhui Liu", "Jie Feng", "Hetian Pang", "Xin Zhang", "Tianjian Ouyang", "Zhiyuan Zhang", "Yong Li"], "title": "CityLens: Benchmarking Large Language-Vision Models for Urban Socioeconomic Sensing", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Understanding urban socioeconomic conditions through visual data is a\nchallenging yet essential task for sustainable urban development and policy\nplanning. In this work, we introduce $\\textbf{CityLens}$, a comprehensive\nbenchmark designed to evaluate the capabilities of large language-vision models\n(LLVMs) in predicting socioeconomic indicators from satellite and street view\nimagery. We construct a multi-modal dataset covering a total of 17 globally\ndistributed cities, spanning 6 key domains: economy, education, crime,\ntransport, health, and environment, reflecting the multifaceted nature of urban\nlife. Based on this dataset, we define 11 prediction tasks and utilize three\nevaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation,\nand Feature-Based Regression. We benchmark 17 state-of-the-art LLVMs across\nthese tasks. Our results reveal that while LLVMs demonstrate promising\nperceptual and reasoning capabilities, they still exhibit limitations in\npredicting urban socioeconomic indicators. CityLens provides a unified\nframework for diagnosing these limitations and guiding future efforts in using\nLLVMs to understand and predict urban socioeconomic patterns. Our codes and\ndatasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens.", "AI": {"tldr": "CityLens\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u89c6\u89c9\u6a21\u578b\uff08LLVMs\uff09\u4ece\u536b\u661f\u548c\u8857\u666f\u56fe\u50cf\u9884\u6d4b\u793e\u4f1a\u7ecf\u6d4e\u6307\u6807\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6db5\u76d617\u4e2a\u57ce\u5e02\u76846\u4e2a\u5173\u952e\u9886\u57df\uff0c\u5e76\u63ed\u793a\u4e86LLVMs\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u4e3a\u4e86\u652f\u6301\u53ef\u6301\u7eed\u57ce\u5e02\u53d1\u5c55\u548c\u653f\u7b56\u89c4\u5212\uff0c\u9700\u8981\u7406\u89e3\u57ce\u5e02\u793e\u4f1a\u7ecf\u6d4e\u6761\u4ef6\uff0c\u800c\u89c6\u89c9\u6570\u636e\u4e3a\u6b64\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u8986\u76d617\u4e2a\u57ce\u5e02\u76846\u4e2a\u5173\u952e\u9886\u57df\uff0c\u5b9a\u4e49\u4e8611\u4e2a\u9884\u6d4b\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u4e09\u79cd\u8bc4\u4f30\u8303\u5f0f\u5bf917\u4e2aLLVMs\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "LLVMs\u8868\u73b0\u51fa\u6709\u524d\u666f\u7684\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u9884\u6d4b\u793e\u4f1a\u7ecf\u6d4e\u6307\u6807\u65f6\u4ecd\u6709\u5c40\u9650\u6027\u3002", "conclusion": "CityLens\u4e3a\u8bca\u65adLLVMs\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u6307\u5bfc\u672a\u6765\u5229\u7528LLVMs\u7406\u89e3\u548c\u9884\u6d4b\u57ce\u5e02\u793e\u4f1a\u7ecf\u6d4e\u6a21\u5f0f\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u89c6\u89c9\u6a21\u578b\uff0c\u793e\u4f1a\u7ecf\u6d4e\u6307\u6807\uff0c\u536b\u661f\u56fe\u50cf\uff0c\u8857\u666f\u56fe\u50cf\uff0c\u57ce\u5e02\u53d1\u5c55"}}
{"id": "2506.00286", "pdf": "https://arxiv.org/pdf/2506.00286", "abs": "https://arxiv.org/abs/2506.00286", "authors": ["Oliver Mortensen", "Mohammad Sadegh Talebi"], "title": "Entropic Risk Optimization in Discounted MDPs: Sample Complexity Bounds with a Generative Model", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "comment": null, "summary": "In this paper we analyze the sample complexities of learning the optimal\nstate-action value function $Q^*$ and an optimal policy $\\pi^*$ in a discounted\nMarkov decision process (MDP) where the agent has recursive entropic\nrisk-preferences with risk-parameter $\\beta\\neq 0$ and where a generative model\nof the MDP is available. We provide and analyze a simple model based approach\nwhich we call model-based risk-sensitive $Q$-value-iteration (MB-RS-QVI) which\nleads to $(\\epsilon,\\delta)$-PAC-bounds on $\\|Q^*-Q^k\\|$, and\n$\\|V^*-V^{\\pi_k}\\|$ where $Q_k$ is the output of MB-RS-QVI after k iterations\nand $\\pi_k$ is the greedy policy with respect to $Q_k$. Both PAC-bounds have\nexponential dependence on the effective horizon $\\frac{1}{1-\\gamma}$ and the\nstrength of this dependence grows with the learners risk-sensitivity $|\\beta|$.\nWe also provide two lower bounds which shows that exponential dependence on\n$|\\beta|\\frac{1}{1-\\gamma}$ is unavoidable in both cases. The lower bounds\nreveal that the PAC-bounds are both tight in $\\varepsilon$ and $\\delta$ and\nthat the PAC-bound on $Q$-learning is tight in the number of actions $A$, and\nthat the PAC-bound on policy-learning is nearly tight in $A$.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u5177\u6709\u9012\u5f52\u71b5\u98ce\u9669\u504f\u597d\u7684\u6298\u6263\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u5b66\u4e60\u6700\u4f18\u72b6\u6001-\u52a8\u4f5c\u503c\u51fd\u6570\u548c\u6700\u4f18\u7b56\u7565\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002", "motivation": "\u63a2\u8ba8\u98ce\u9669\u654f\u611f\u5b66\u4e60\u5728MDP\u4e2d\u7684\u6837\u672c\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u9012\u5f52\u71b5\u98ce\u9669\u504f\u597d\u5bf9\u5b66\u4e60\u6548\u7387\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u98ce\u9669\u654f\u611fQ\u503c\u8fed\u4ee3\uff08MB-RS-QVI\uff09\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5176PAC\u754c\u9650\u3002", "result": "\u5c55\u793a\u4e86\u5b66\u4e60\u754c\u9650\u5bf9\u6709\u6548\u89c6\u754c\u548c\u98ce\u9669\u654f\u611f\u5ea6\u7684\u6307\u6570\u4f9d\u8d56\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u7d27\u81f4\u6027\u8bc1\u660e\u3002", "conclusion": "\u8bc1\u660e\u4e86\u6837\u672c\u590d\u6742\u5ea6\u7684\u6307\u6570\u4f9d\u8d56\u6027\u662f\u4e0d\u53ef\u907f\u514d\u7684\uff0c\u4e14\u5b66\u4e60\u754c\u9650\u5728\u591a\u4e2a\u53c2\u6570\u4e0a\u662f\u7d27\u81f4\u7684\u3002", "keywords": "\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b,\u98ce\u9669\u654f\u611f\u5b66\u4e60,\u6837\u672c\u590d\u6742\u5ea6,\u9012\u5f52\u71b5\u98ce\u9669,PAC\u754c\u9650"}}
{"id": "2506.00250", "pdf": "https://arxiv.org/pdf/2506.00250", "abs": "https://arxiv.org/abs/2506.00250", "authors": ["Mohammad Javad Ranjbar Kalahroodi", "Amirhossein Sheikholselami", "Sepehr Karimi", "Sepideh Ranjbar Kalahroodi", "Heshaam Faili", "Azadeh Shakery"], "title": "PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain", "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable performance on a wide\nrange of NLP benchmarks, often surpassing human-level accuracy. However, their\nreliability in high-stakes domains such as medicine, particularly in\nlow-resource languages, remains underexplored. In this work, we introduce\nPersianMedQA, a large-scale, expert-validated dataset of multiple-choice\nPersian medical questions, designed to evaluate LLMs across both Persian and\nEnglish. We benchmark over 40 state-of-the-art models, including\ngeneral-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and\nchain-of-thought (CoT) settings. Our results show that closed-source general\nmodels (e.g., GPT-4.1) consistently outperform all other categories, achieving\n83.3% accuracy in Persian and 80.7% in English, while Persian fine-tuned models\nsuch as Dorna underperform significantly (e.g., 35.9% in Persian), often\nstruggling with both instruction-following and domain reasoning. We also\nanalyze the impact of translation, showing that while English performance is\ngenerally higher, Persian responses are sometimes more accurate due to cultural\nand clinical contextual cues. Finally, we demonstrate that model size alone is\ninsufficient for robust performance without strong domain or language\nadaptation. PersianMedQA provides a foundation for evaluating multilingual and\nculturally grounded medical reasoning in LLMs. The PersianMedQA dataset can be\naccessed at:\nhttps://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA](https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86PersianMedQA\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6ce2\u65af\u8bed\u548c\u82f1\u8bed\u4e2d\u533b\u7597\u95ee\u7b54\u80fd\u529b\u7684\u4e13\u5bb6\u9a8c\u8bc1\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u6bd4\u8f83\u4e8640\u591a\u79cd\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u6ce2\u65af\u8bed\uff09\u548c\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u533b\u5b66\uff09\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u6784\u5efaPersianMedQA\u6570\u636e\u96c6\uff0c\u5e76\u5bf940\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u548c\u601d\u7ef4\u94fe\u8bbe\u7f6e\u4e0b\u7684\u6d4b\u8bd5\u3002", "result": "\u5c01\u95ed\u6e90\u901a\u7528\u6a21\u578b\uff08\u5982GPT-4.1\uff09\u8868\u73b0\u6700\u4f18\uff0c\u800c\u6ce2\u65af\u8bed\u5fae\u8c03\u6a21\u578b\u8868\u73b0\u8f83\u5dee\u3002\u7ffb\u8bd1\u5bf9\u6027\u80fd\u6709\u5f71\u54cd\uff0c\u6a21\u578b\u5927\u5c0f\u4e0d\u8db3\u4ee5\u4fdd\u8bc1\u6027\u80fd\u3002", "conclusion": "PersianMedQA\u4e3a\u8bc4\u4f30\u591a\u8bed\u8a00\u548c\u533b\u5b66\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5f3a\u8c03\u9886\u57df\u6216\u8bed\u8a00\u9002\u5e94\u7684\u91cd\u8981\u6027\u3002", "keywords": "Large Language Models, PersianMedQA, \u533b\u7597\u95ee\u7b54, \u591a\u8bed\u8a00\u8bc4\u4f30"}}
{"id": "2506.00570", "pdf": "https://arxiv.org/pdf/2506.00570", "abs": "https://arxiv.org/abs/2506.00570", "authors": ["Liang Geng"], "title": "A \"Wenlu\" Brain System for Multimodal Cognition and Embodied Decision-Making: A Secure New Architecture for Deep Integration of Foundation Models and Domain Knowledge", "categories": ["cs.AI"], "comment": null, "summary": "With the rapid penetration of artificial intelligence across industries and\nscenarios, a key challenge in building the next-generation intelligent core\nlies in effectively integrating the language understanding capabilities of\nfoundation models with domain-specific knowledge bases in complex real-world\napplications. This paper proposes a multimodal cognition and embodied\ndecision-making brain system, ``Wenlu\", designed to enable secure fusion of\nprivate knowledge and public models, unified processing of multimodal data such\nas images and speech, and closed-loop decision-making from cognition to\nautomatic generation of hardware-level code. The system introduces a\nbrain-inspired memory tagging and replay mechanism, seamlessly integrating\nuser-private data, industry-specific knowledge, and general-purpose language\nmodels. It provides precise and efficient multimodal services for enterprise\ndecision support, medical analysis, autonomous driving, robotic control, and\nmore. Compared with existing solutions, ``Wenlu\" demonstrates significant\nadvantages in multimodal processing, privacy security, end-to-end hardware\ncontrol code generation, self-learning, and sustainable updates, thus laying a\nsolid foundation for constructing the next-generation intelligent core.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u6587\u7490\u201d\u7684\u591a\u6a21\u6001\u8ba4\u77e5\u4e0e\u5177\u8eab\u51b3\u7b56\u8111\u7cfb\u7edf\uff0c\u65e8\u5728\u6709\u6548\u878d\u5408\u79c1\u6709\u77e5\u8bc6\u4e0e\u516c\u5171\u6a21\u578b\uff0c\u652f\u6301\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\u548c\u95ed\u73af\u51b3\u7b56\u751f\u6210\u786c\u4ef6\u7ea7\u4ee3\u7801\u3002", "motivation": "\u968f\u7740AI\u5728\u5404\u884c\u4e1a\u7684\u5feb\u901f\u6e17\u900f\uff0c\u5982\u4f55\u5728\u590d\u6742\u5b9e\u9645\u5e94\u7528\u4e2d\u6574\u5408\u57fa\u7840\u6a21\u578b\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u4e0e\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u5e93\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u5f15\u5165\u8111\u542f\u53d1\u7684\u8bb0\u5fc6\u6807\u8bb0\u4e0e\u56de\u653e\u673a\u5236\uff0c\u878d\u5408\u7528\u6237\u79c1\u6709\u6570\u636e\u3001\u884c\u4e1a\u77e5\u8bc6\u4e0e\u901a\u7528\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u201c\u6587\u7490\u201d\u5728\u591a\u6a21\u6001\u5904\u7406\u3001\u9690\u79c1\u5b89\u5168\u3001\u7aef\u5230\u7aef\u786c\u4ef6\u63a7\u5236\u4ee3\u7801\u751f\u6210\u7b49\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee3\u667a\u80fd\u6838\u5fc3\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "keywords": "\u4eba\u5de5\u667a\u80fd\uff0c\u591a\u6a21\u6001\u8ba4\u77e5\uff0c\u5177\u8eab\u51b3\u7b56\uff0c\u79c1\u6709\u77e5\u8bc6\u878d\u5408\uff0c\u95ed\u73af\u51b3\u7b56"}}
{"id": "2506.00297", "pdf": "https://arxiv.org/pdf/2506.00297", "abs": "https://arxiv.org/abs/2506.00297", "authors": ["Fanglei Xue", "Andrew Kubaney", "Zhichun Guo", "Joseph K. Min", "Ge Liu", "Yi Yang", "David Baker"], "title": "Improving Protein Sequence Design through Designability Preference Optimization", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": null, "summary": "Protein sequence design methods have demonstrated strong performance in\nsequence generation for de novo protein design. However, as the training\nobjective was sequence recovery, it does not guarantee designability--the\nlikelihood that a designed sequence folds into the desired structure. To bridge\nthis gap, we redefine the training objective by steering sequence generation\ntoward high designability. To do this, we integrate Direct Preference\nOptimization (DPO), using AlphaFold pLDDT scores as the preference signal,\nwhich significantly improves the in silico design success rate. To further\nrefine sequence generation at a finer, residue-level granularity, we introduce\nResidue-level Designability Preference Optimization (ResiDPO), which applies\nresidue-level structural rewards and decouples optimization across residues.\nThis enables direct improvement in designability while preserving regions that\nalready perform well. Using a curated dataset with residue-level annotations,\nwe fine-tune LigandMPNN with ResiDPO to obtain EnhancedMPNN, which achieves a\nnearly 3-fold increase in in silico design success rate (from 6.56% to 17.57%)\non a challenging enzyme design benchmark.", "AI": {"tldr": "\u86cb\u767d\u5e8f\u5217\u8bbe\u8ba1\u65b9\u6cd5\u5728\u5e8f\u5217\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4f20\u7edf\u8bad\u7ec3\u76ee\u6807\u672a\u80fd\u4fdd\u8bc1\u8bbe\u8ba1\u540e\u7684\u5e8f\u5217\u80fd\u6298\u53e0\u6210\u6240\u9700\u7ed3\u6784\u3002\u65b0\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u8bbe\u8ba1\u6027\u76ee\u6807\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bbe\u8ba1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u86cb\u767d\u5e8f\u5217\u8bbe\u8ba1\u65b9\u6cd5\u4e2d\u8bbe\u8ba1\u6027\uff08\u8bbe\u8ba1\u5e8f\u5217\u80fd\u5426\u6298\u53e0\u6210\u76ee\u6807\u7ed3\u6784\uff09\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6574\u5408Direct Preference Optimization (DPO) \u548c\u4f7f\u7528AlphaFold pLDDT\u5206\u6570\u4f5c\u4e3a\u504f\u597d\u4fe1\u53f7\uff0c\u4ee5\u53ca\u5f15\u5165Residue-level Designability Preference Optimization (ResiDPO)\uff0c\u7ec6\u5316\u6b8b\u57fa\u7ea7\u4f18\u5316\u3002", "result": "EnhancedMPNN\u5728\u9176\u8bbe\u8ba1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c06\u8bbe\u8ba1\u6210\u529f\u7387\u4ece6.56%\u63d0\u5347\u81f317.57%\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u86cb\u767d\u5e8f\u5217\u7684\u8bbe\u8ba1\u6027\u548c\u5b9e\u7528\u6027\u3002", "keywords": "\u86cb\u767d\u5e8f\u5217\u8bbe\u8ba1, \u8bbe\u8ba1\u6027, DPO, ResiDPO, AlphaFold"}}
{"id": "2506.00253", "pdf": "https://arxiv.org/pdf/2506.00253", "abs": "https://arxiv.org/abs/2506.00253", "authors": ["Lihao Sun", "Chengzhi Mao", "Valentin Hofmann", "Xuechunzi Bai"], "title": "Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accpeted to ACL 2025 Main Conferencce", "summary": "Although value-aligned language models (LMs) appear unbiased in explicit bias\nevaluations, they often exhibit stereotypes in implicit word association tasks,\nraising concerns about their fair usage. We investigate the mechanisms behind\nthis discrepancy and find that alignment surprisingly amplifies implicit bias\nin model outputs. Specifically, we show that aligned LMs, unlike their\nunaligned counterparts, overlook racial concepts in early internal\nrepresentations when the context is ambiguous. Not representing race likely\nfails to activate safety guardrails, leading to unintended biases. Inspired by\nthis insight, we propose a new bias mitigation strategy that works by\nincentivizing the representation of racial concepts in the early model layers.\nIn contrast to conventional mitigation methods of machine unlearning, our\ninterventions find that steering the model to be more aware of racial concepts\neffectively mitigates implicit bias. Similar to race blindness in humans,\nignoring racial nuances can inadvertently perpetuate subtle biases in LMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5bf9\u9f50\u7684\u8bed\u8a00\u6a21\u578b\u5728\u9690\u5f0f\u504f\u89c1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u504f\u89c1\u7f13\u89e3\u7b56\u7565\uff0c\u901a\u8fc7\u6fc0\u52b1\u6a21\u578b\u5728\u65e9\u671f\u5c42\u8868\u793a\u79cd\u65cf\u6982\u5ff5\u6765\u89e3\u51b3\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5bf9\u9f50\u7684\u8bed\u8a00\u6a21\u578b\u5728\u663e\u5f0f\u504f\u89c1\u8bc4\u4f30\u4e2d\u8868\u73b0\u65e0\u504f\u89c1\uff0c\u4f46\u5728\u9690\u5f0f\u5173\u8054\u4efb\u52a1\u4e2d\u4ecd\u5b58\u5728\u504f\u89c1\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u8fd9\u79cd\u5dee\u5f02\u7684\u673a\u5236\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u53d1\u73b0\u5bf9\u9f50\u6a21\u578b\u5728\u6a21\u7cca\u4e0a\u4e0b\u6587\u4e2d\u5ffd\u7565\u79cd\u65cf\u6982\u5ff5\uff0c\u63d0\u51fa\u901a\u8fc7\u6fc0\u52b1\u6a21\u578b\u5728\u65e9\u671f\u5c42\u8868\u793a\u79cd\u65cf\u6982\u5ff5\u6765\u7f13\u89e3\u504f\u89c1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u6bd4\u4f20\u7edf\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u51cf\u5c11\u4e86\u9690\u5f0f\u504f\u89c1\u3002", "conclusion": "\u5ffd\u7565\u79cd\u65cf\u6982\u5ff5\u53ef\u80fd\u65e0\u610f\u4e2d\u5ef6\u7eed\u504f\u89c1\uff0c\u65b0\u7684\u5e72\u9884\u7b56\u7565\u901a\u8fc7\u589e\u5f3a\u79cd\u65cf\u610f\u8bc6\u6765\u6539\u5584\u516c\u5e73\u6027\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b\uff0c\u9690\u5f0f\u504f\u89c1\uff0c\u5bf9\u9f50\u6a21\u578b\uff0c\u504f\u89c1\u7f13\u89e3\uff0c\u79cd\u65cf\u6982\u5ff5"}}
{"id": "2506.00577", "pdf": "https://arxiv.org/pdf/2506.00577", "abs": "https://arxiv.org/abs/2506.00577", "authors": ["Yufa Zhou", "Shaobo Wang", "Xingyu Dong", "Xiangqi Jin", "Yifang Chen", "Yue Min", "Kexin Yang", "Xingzhang Ren", "Dayiheng Liu", "Linfeng Zhang"], "title": "Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.MA"], "comment": null, "summary": "Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS)\nremains challenging due to intricate reward modeling, dynamic agent\ninteractions, and demanding generalization requirements. This paper explores\nwhether post-training techniques, specifically Supervised Fine-Tuning (SFT) and\nReinforcement Learning with Verifiable Rewards (RLVR), can effectively\n$\\textit{generalize}$ to multi-agent scenarios. We use economic reasoning as a\ntestbed, leveraging its strong foundations in mathematics and game theory, its\ndemand for structured analytical reasoning, and its relevance to real-world\napplications such as market design, resource allocation, and policy analysis.\nWe introduce $\\textbf{Recon}$ ($\\textbf{R}$easoning like an\n$\\textbf{ECON}$omist), a 7B-parameter open-source LLM post-trained on a\nhand-curated dataset of 2,100 high-quality economic reasoning problems.\nComprehensive evaluation on economic reasoning benchmarks and multi-agent games\nreveals clear improvements in structured reasoning and economic rationality.\nThese results underscore the promise of domain-aligned post-training for\nenhancing reasoning and agent alignment, shedding light on the roles of SFT and\nRL in shaping model behavior. Code is available at\nhttps://github.com/MasterZhou1/Recon .", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5e26\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u540e\u8bad\u7ec3\u6280\u672f\uff0c\u662f\u5426\u80fd\u591f\u6709\u6548\u6cdb\u5316\u5230\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u5e76\u4ee5\u7ecf\u6d4e\u63a8\u7406\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5c55\u793a\u4e86Recon\u6a21\u578b\u5728\u7ed3\u6784\u5316\u63a8\u7406\u548c\u7ecf\u6d4e\u5408\u7406\u6027\u4e0a\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u7531\u4e8e\u590d\u6742\u7684\u5956\u52b1\u5efa\u6a21\u3001\u52a8\u6001\u667a\u80fd\u4f53\u4ea4\u4e92\u548c\u9ad8\u6cdb\u5316\u9700\u6c42\uff0c\u76f4\u63a5\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5177\u6709\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u540e\u8bad\u7ec3\u6280\u672f\u5728MAS\u4e2d\u7684\u6709\u6548\u6027\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u7814\u7a76\u91c7\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5e26\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u4f5c\u4e3a\u540e\u8bad\u7ec3\u6280\u672f\uff0c\u6784\u5efa\u4e86\u540d\u4e3aRecon\u76847B\u53c2\u6570\u5f00\u6e90LLM\uff0c\u5e76\u57282100\u4e2a\u9ad8\u8d28\u91cf\u7ecf\u6d4e\u63a8\u7406\u95ee\u9898\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u7ecf\u6d4e\u5b66\u63a8\u7406\u57fa\u51c6\u548c\u591a\u667a\u80fd\u4f53\u6e38\u620f\u7684\u5168\u9762\u8bc4\u4f30\u4e2d\uff0cRecon\u6a21\u578b\u663e\u793a\u51fa\u5728\u7ed3\u6784\u5316\u63a8\u7406\u548c\u7ecf\u6d4e\u5408\u7406\u6027\u65b9\u9762\u7684\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u9886\u57df\u5bf9\u9f50\u7684\u540e\u8bad\u7ec3\u6280\u672f\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u667a\u80fd\u4f53\u5bf9\u9f50\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86SFT\u548cRL\u5728\u5851\u9020\u6a21\u578b\u884c\u4e3a\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u591a\u667a\u80fd\u4f53\u7cfb\u7edf, \u7ecf\u6d4e\u63a8\u7406, \u76d1\u7763\u5fae\u8c03, \u5f3a\u5316\u5b66\u4e60"}}
{"id": "2506.00299", "pdf": "https://arxiv.org/pdf/2506.00299", "abs": "https://arxiv.org/abs/2506.00299", "authors": ["Purvish Jajal", "Nick John Eliopoulos", "Benjamin Shiue-Hal Chou", "George K. Thiruvathukal", "James C. Davis", "Yung-Hsiang Lu"], "title": "Inference-Time Alignment of Diffusion Models with Evolutionary Algorithms", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion models are state-of-the-art generative models in various domains,\nyet their samples often fail to satisfy downstream objectives such as safety\nconstraints or domain-specific validity. Existing techniques for alignment\nrequire gradients, internal model access, or large computational budgets. We\nintroduce an inference-time alignment framework based on evolutionary\nalgorithms. We treat diffusion models as black-boxes and search their latent\nspace to maximize alignment objectives. Our method enables efficient\ninference-time alignment for both differentiable and non-differentiable\nalignment objectives across a range of diffusion models. On the DrawBench and\nOpen Image Preferences benchmark, our EA methods outperform state-of-the-art\ngradient-based and gradient-free inference-time methods. In terms of memory\nconsumption, we require 55% to 76% lower GPU memory than gradient-based\nmethods. In terms of running-time, we are 72% to 80% faster than gradient-based\nmethods. We achieve higher alignment scores over 50 optimization steps on Open\nImage Preferences than gradient-based and gradient-free methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u7684\u6269\u6563\u6a21\u578b\u63a8\u7406\u65f6\u5bf9\u9f50\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u68af\u5ea6\u6216\u5185\u90e8\u6a21\u578b\u8bbf\u95ee\u7684\u60c5\u51b5\u4e0b\uff0c\u9ad8\u6548\u4f18\u5316\u5bf9\u9f50\u76ee\u6807\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\u65f6\u5e38\u5e38\u65e0\u6cd5\u6ee1\u8db3\u4e0b\u6e38\u76ee\u6807\uff08\u5982\u5b89\u5168\u6027\u6216\u9886\u57df\u7279\u5b9a\u6709\u6548\u6027\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u68af\u5ea6\u6216\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u5c06\u6269\u6563\u6a21\u578b\u89c6\u4e3a\u9ed1\u76d2\uff0c\u5229\u7528\u8fdb\u5316\u7b97\u6cd5\u5728\u5176\u6f5c\u5728\u7a7a\u95f4\u4e2d\u641c\u7d22\uff0c\u4ee5\u6700\u5927\u5316\u5bf9\u9f50\u76ee\u6807\uff0c\u652f\u6301\u53ef\u5fae\u548c\u4e0d\u53ef\u5fae\u76ee\u6807\u3002", "result": "\u5728DrawBench\u548cOpen Image Preferences\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5185\u5b58\uff08\u8282\u770155%-76%\uff09\u548c\u901f\u5ea6\uff08\u5feb72%-80%\uff09\u4e0a\u4f18\u4e8e\u68af\u5ea6\u65b9\u6cd5\u548c\u65e0\u68af\u5ea6\u65b9\u6cd5\uff0c\u4e14\u5bf9\u9f50\u5206\u6570\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u63a8\u7406\u65f6\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6269\u6563\u6a21\u578b\u548c\u590d\u6742\u7684\u5bf9\u9f50\u76ee\u6807\u3002", "keywords": "\u6269\u6563\u6a21\u578b,\u8fdb\u5316\u7b97\u6cd5,\u63a8\u7406\u65f6\u5bf9\u9f50,\u9ed1\u76d2\u4f18\u5316,\u751f\u6210\u6a21\u578b"}}
{"id": "2506.00256", "pdf": "https://arxiv.org/pdf/2506.00256", "abs": "https://arxiv.org/abs/2506.00256", "authors": ["Mahammed Kamruzzaman", "Gene Louis Kim"], "title": "The Impact of Disability Disclosure on Fairness and Bias in LLM-Driven Candidate Selection", "categories": ["cs.CL"], "comment": "Accepted at The 38th International FLAIRS Conference (FLAIRS\n  2025)(main)", "summary": "As large language models (LLMs) become increasingly integrated into hiring\nprocesses, concerns about fairness have gained prominence. When applying for\njobs, companies often request/require demographic information, including\ngender, race, and disability or veteran status. This data is collected to\nsupport diversity and inclusion initiatives, but when provided to LLMs,\nespecially disability-related information, it raises concerns about potential\nbiases in candidate selection outcomes. Many studies have highlighted how\ndisability can impact CV screening, yet little research has explored the\nspecific effect of voluntarily disclosed information on LLM-driven candidate\nselection. This study seeks to bridge that gap. When candidates shared\nidentical gender, race, qualifications, experience, and backgrounds, and sought\njobs with minimal employment rate gaps between individuals with and without\ndisabilities (e.g., Cashier, Software Developer), LLMs consistently favored\ncandidates who disclosed that they had no disability. Even in cases where\ncandidates chose not to disclose their disability status, the LLMs were less\nlikely to select them compared to those who explicitly stated they did not have\na disability.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728LLM\u9a71\u52a8\u7684\u62db\u8058\u4e2d\uff0c\u62ab\u9732\u6b8b\u75be\u4fe1\u606f\u7684\u5019\u9009\u4eba\u66f4\u5c11\u88ab\u9009\u62e9\uff0c\u5373\u4f7f\u5019\u9009\u4eba\u9009\u62e9\u4e0d\u62ab\u9732\uff0c\u4e5f\u6bd4\u660e\u786e\u8868\u793a\u65e0\u6b8b\u75be\u7684\u5019\u9009\u4eba\u66f4\u4e0d\u53d7\u9752\u7750\u3002", "motivation": "\u63a2\u8ba8LLM\u5728\u62db\u8058\u8fc7\u7a0b\u4e2d\u5bf9\u5019\u9009\u4eba\u81ea\u613f\u62ab\u9732\u6b8b\u75be\u4fe1\u606f\u7684\u6f5c\u5728\u504f\u89c1\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u6bd4\u8f83\u76f8\u540c\u80cc\u666f\u4f46\u62ab\u9732/\u4e0d\u62ab\u9732\u6b8b\u75be\u4fe1\u606f\u7684\u5019\u9009\u4eba\u5728LLM\u7b5b\u9009\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLM\u66f4\u503e\u5411\u4e8e\u9009\u62e9\u672a\u62ab\u9732\u6b8b\u75be\u7684\u5019\u9009\u4eba\uff0c\u5373\u4f7f\u672a\u62ab\u9732\u4e5f\u4ecd\u5904\u4e8e\u52a3\u52bf\u3002", "conclusion": "LLM\u5728\u62db\u8058\u4e2d\u5b58\u5728\u5bf9\u6b8b\u75be\u4fe1\u606f\u7684\u6f5c\u5728\u504f\u89c1\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u786e\u4fdd\u516c\u5e73\u3002", "keywords": "LLM, \u62db\u8058, \u516c\u5e73\u6027, \u6b8b\u75be\u4fe1\u606f, \u504f\u89c1"}}
{"id": "2506.00582", "pdf": "https://arxiv.org/pdf/2506.00582", "abs": "https://arxiv.org/abs/2506.00582", "authors": ["Chenjun Xu", "Bingbing Wen", "Bin Han", "Robert Wolfe", "Lucy Lu Wang", "Bill Howe"], "title": "Do Language Models Mirror Human Confidence? Exploring Psychological Insights to Address Overconfidence in LLMs", "categories": ["cs.AI", "I.2.7"], "comment": "Accepted by ACL 2025 Findings, 20 pages", "summary": "Psychology research has shown that humans are poor at estimating their\nperformance on tasks, tending towards underconfidence on easy tasks and\noverconfidence on difficult tasks. We examine three LLMs, Llama-3-70B-instruct,\nClaude-3-Sonnet, and GPT-4o, on a range of QA tasks of varying difficulty, and\nshow that models exhibit subtle differences from human patterns of\noverconfidence: less sensitive to task difficulty, and when prompted to answer\nbased on different personas -- e.g., expert vs layman, or different race,\ngender, and ages -- the models will respond with stereotypically biased\nconfidence estimations even though their underlying answer accuracy remains the\nsame. Based on these observations, we propose Answer-Free Confidence Estimation\n(AFCE) to improve confidence calibration and LLM interpretability in these\nsettings. AFCE is a self-assessment method that employs two stages of\nprompting, first eliciting only confidence scores on questions, then asking\nseparately for the answer. Experiments on the MMLU and GPQA datasets spanning\nsubjects and difficulty show that this separation of tasks significantly\nreduces overconfidence and delivers more human-like sensitivity to task\ndifficulty.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLMs\u5728QA\u4efb\u52a1\u4e2d\u7684\u81ea\u4fe1\u5ea6\u4f30\u8ba1\u95ee\u9898\uff0c\u53d1\u73b0\u5176\u4e0e\u4eba\u7c7b\u4e0d\u540c\u4e14\u5b58\u5728\u523b\u677f\u504f\u89c1\uff0c\u63d0\u51faAFCE\u65b9\u6cd5\u6539\u5584\u6821\u51c6\u3002", "motivation": "\u4eba\u7c7b\u5728\u4efb\u52a1\u4e2d\u5e38\u8868\u73b0\u51fa\u4e0d\u51c6\u786e\u7684\u81ea\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u8bba\u6587\u63a2\u7d22LLMs\u5728\u8fd9\u4e00\u884c\u4e3a\u4e0a\u7684\u5dee\u5f02\u53ca\u5176\u504f\u89c1\u95ee\u9898\u3002", "method": "\u91c7\u7528AFCE\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u63d0\u793a\uff08\u5148\u4f30\u8ba1\u81ea\u4fe1\u5ea6\u518d\u56de\u7b54\u95ee\u9898\uff09\uff0c\u51cf\u5c11\u8fc7\u81ea\u4fe1\u5e76\u63d0\u9ad8\u5bf9\u4efb\u52a1\u96be\u5ea6\u7684\u654f\u611f\u6027\u3002", "result": "AFCE\u5728MMLU\u548cGPQA\u6570\u636e\u96c6\u4e0a\u663e\u8457\u51cf\u5c11\u4e86\u8fc7\u81ea\u4fe1\uff0c\u4e14\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u81ea\u4fe1\u5ea6\u4f30\u8ba1\u6a21\u5f0f\u3002", "conclusion": "AFCE\u80fd\u6709\u6548\u6539\u5584LLMs\u7684\u81ea\u4fe1\u5ea6\u6821\u51c6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u4e0d\u540c\u4efb\u52a1\u96be\u5ea6\u548c\u4eba\u7269\u8bbe\u5b9a\u65f6\u3002", "keywords": "LLMs, \u81ea\u4fe1\u5ea6\u4f30\u8ba1, AFCE, \u8fc7\u81ea\u4fe1, QA\u4efb\u52a1"}}
{"id": "2506.00302", "pdf": "https://arxiv.org/pdf/2506.00302", "abs": "https://arxiv.org/abs/2506.00302", "authors": ["Can Polat", "Hasan Kurban", "Erchin Serpedin", "Mustafa Kurban"], "title": "Beyond Atomic Geometry Representations in Materials Science: A Human-in-the-Loop Multimodal Framework", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "comment": "Submitted to ICML 2025 Workshop on DataWorld", "summary": "Most materials science datasets are limited to atomic geometries (e.g., XYZ\nfiles), restricting their utility for multimodal learning and comprehensive\ndata-centric analysis. These constraints have historically impeded the adoption\nof advanced machine learning techniques in the field. This work introduces\nMultiCrystalSpectrumSet (MCS-Set), a curated framework that expands materials\ndatasets by integrating atomic structures with 2D projections and structured\ntextual annotations, including lattice parameters and coordination metrics.\nMCS-Set enables two key tasks: (1) multimodal property and summary prediction,\nand (2) constrained crystal generation with partial cluster supervision.\nLeveraging a human-in-the-loop pipeline, MCS-Set combines domain expertise with\nstandardized descriptors for high-quality annotation. Evaluations using\nstate-of-the-art language and vision-language models reveal substantial\nmodality-specific performance gaps and highlight the importance of annotation\nquality for generalization. MCS-Set offers a foundation for benchmarking\nmultimodal models, advancing annotation practices, and promoting accessible,\nversatile materials science datasets. The dataset and implementations are\navailable at https://github.com/KurbanIntelligenceLab/MultiCrystalSpectrumSet.", "AI": {"tldr": "MultiCrystalSpectrumSet (MCS-Set) \u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6750\u6599\u6570\u636e\u96c6\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u539f\u5b50\u7ed3\u6784\u30012D\u6295\u5f71\u548c\u7ed3\u6784\u5316\u6587\u672c\u6ce8\u91ca\uff0c\u652f\u6301\u591a\u6a21\u6001\u9884\u6d4b\u548c\u7ea6\u675f\u6676\u4f53\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u6750\u6599\u79d1\u5b66\u6570\u636e\u96c6\u4ec5\u5305\u542b\u539f\u5b50\u51e0\u4f55\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u548c\u6570\u636e\u4e2d\u5fc3\u7684\u5168\u9762\u5206\u6790\uff0c\u963b\u788d\u4e86\u5148\u8fdb\u673a\u5668\u5b66\u4e60\u6280\u672f\u7684\u5e94\u7528\u3002", "method": "MCS-Set \u901a\u8fc7\u6574\u5408\u539f\u5b50\u7ed3\u6784\u4e0e2D\u6295\u5f71\u53ca\u6587\u672c\u6ce8\u91ca\uff08\u5982\u6676\u683c\u53c2\u6570\u548c\u914d\u4f4d\u5ea6\u91cf\uff09\uff0c\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u6d41\u7a0b\u8fdb\u884c\u9ad8\u8d28\u91cf\u6807\u6ce8\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u591a\u6a21\u6001\u6a21\u578b\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u5f3a\u8c03\u6807\u6ce8\u8d28\u91cf\u5bf9\u6cdb\u5316\u7684\u91cd\u8981\u6027\u3002", "conclusion": "MCS-Set \u4e3a\u591a\u6a21\u6001\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u3001\u6807\u6ce8\u5b9e\u8df5\u6539\u8fdb\u53ca\u901a\u7528\u6750\u6599\u79d1\u5b66\u6570\u636e\u96c6\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "keywords": "\u6750\u6599\u79d1\u5b66, \u591a\u6a21\u6001\u5b66\u4e60, \u6570\u636e\u96c6, \u673a\u5668\u5b66\u4e60, \u6807\u6ce8\u8d28\u91cf"}}
{"id": "2506.00264", "pdf": "https://arxiv.org/pdf/2506.00264", "abs": "https://arxiv.org/abs/2506.00264", "authors": ["Mohammadamin Shafiei", "Hamidreza Saffari", "Nafise Sadat Moosavi"], "title": "MultiHoax: A Dataset of Multi-hop False-Premise Questions", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models are increasingly deployed in high-stakes domains,\ntheir ability to detect false assumptions and reason critically is crucial for\nensuring reliable outputs. False-premise questions (FPQs) serve as an important\nevaluation method by exposing cases where flawed assumptions lead to incorrect\nresponses. While existing benchmarks focus on single-hop FPQs, real-world\nreasoning often requires multi-hop inference, where models must verify\nconsistency across multiple reasoning steps rather than relying on\nsurface-level cues. To address this gap, we introduce MultiHoax, a benchmark\nfor evaluating LLMs' ability to handle false premises in complex, multi-step\nreasoning tasks. Our dataset spans seven countries and ten diverse knowledge\ncategories, using Wikipedia as the primary knowledge source to enable factual\nreasoning across regions. Experiments reveal that state-of-the-art LLMs\nstruggle to detect false premises across different countries, knowledge\ncategories, and multi-hop reasoning types, highlighting the need for improved\nfalse premise detection and more robust multi-hop reasoning capabilities in\nLLMs.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MultiHoax\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u590d\u6742\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2d\u5904\u7406\u9519\u8bef\u524d\u63d0\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u4e0d\u8db3\u4e4b\u5904\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5173\u952e\u9886\u57df\u7684\u90e8\u7f72\u65e5\u76ca\u589e\u591a\uff0c\u8bc6\u522b\u9519\u8bef\u524d\u63d0\u5e76\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u7684\u80fd\u529b\u5bf9\u786e\u4fdd\u53ef\u9760\u8f93\u51fa\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165MultiHoax\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4e03\u4e2a\u56fd\u5bb6\u548c\u5341\u4e2a\u77e5\u8bc6\u7c7b\u522b\uff0c\u4f7f\u7528\u7ef4\u57fa\u767e\u79d1\u4f5c\u4e3a\u77e5\u8bc6\u6e90\uff0c\u8bc4\u4f30LLMs\u5728\u591a\u8df3\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684LLMs\u5728\u4e0d\u540c\u56fd\u5bb6\u3001\u77e5\u8bc6\u7c7b\u522b\u548c\u591a\u8df3\u63a8\u7406\u7c7b\u578b\u4e2d\u68c0\u6d4b\u9519\u8bef\u524d\u63d0\u7684\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u7a81\u663e\u4e86\u63d0\u5347LLMs\u9519\u8bef\u524d\u63d0\u68c0\u6d4b\u80fd\u529b\u548c\u591a\u8df3\u63a8\u7406\u9c81\u68d2\u6027\u7684\u5fc5\u8981\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u9519\u8bef\u524d\u63d0\u95ee\u9898, \u591a\u8df3\u63a8\u7406, \u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2506.00618", "pdf": "https://arxiv.org/pdf/2506.00618", "abs": "https://arxiv.org/abs/2506.00618", "authors": ["Jingyi Yang", "Shuai Shao", "Dongrui Liu", "Jing Shao"], "title": "RiOSWorld: Benchmarking the Risk of Multimodal Compter-Use Agents", "categories": ["cs.AI"], "comment": "40 pages, 6 figures, Project Page:\n  https://yjyddq.github.io/RiOSWorld.github.io/", "summary": "With the rapid development of multimodal large language models (MLLMs), they\nare increasingly deployed as autonomous computer-use agents capable of\naccomplishing complex computer tasks. However, a pressing issue arises: Can the\nsafety risk principles designed and aligned for general MLLMs in dialogue\nscenarios be effectively transferred to real-world computer-use scenarios?\nExisting research on evaluating the safety risks of MLLM-based computer-use\nagents suffers from several limitations: it either lacks realistic interactive\nenvironments, or narrowly focuses on one or a few specific risk types. These\nlimitations ignore the complexity, variability, and diversity of real-world\nenvironments, thereby restricting comprehensive risk evaluation for\ncomputer-use agents. To this end, we introduce \\textbf{RiOSWorld}, a benchmark\ndesigned to evaluate the potential risks of MLLM-based agents during real-world\ncomputer manipulations. Our benchmark includes 492 risky tasks spanning various\ncomputer applications, involving web, social media, multimedia, os, email, and\noffice software. We categorize these risks into two major classes based on\ntheir risk source: (i) User-originated risks and (ii) Environmental risks. For\nthe evaluation, we evaluate safety risks from two perspectives: (i) Risk goal\nintention and (ii) Risk goal completion. Extensive experiments with multimodal\nagents on \\textbf{RiOSWorld} demonstrate that current computer-use agents\nconfront significant safety risks in real-world scenarios. Our findings\nhighlight the necessity and urgency of safety alignment for computer-use agents\nin real-world computer manipulation, providing valuable insights for developing\ntrustworthy computer-use agents. Our benchmark is publicly available at\nhttps://yjyddq.github.io/RiOSWorld.github.io/.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86RIOSWorld\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u73b0\u5b9e\u8ba1\u7b97\u673a\u64cd\u4f5c\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u53d1\u73b0\u73b0\u6709\u4ee3\u7406\u9762\u4e34\u91cd\u5927\u98ce\u9669\uff0c\u5f3a\u8c03\u4e86\u5b89\u5168\u5bf9\u9f50\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u73b0\u5b9e\u4e16\u754c\u4e2d\u57fa\u4e8eMLLM\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u5b89\u5168\u98ce\u9669\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u5728\u4ea4\u4e92\u73af\u5883\u548c\u98ce\u9669\u7c7b\u578b\u591a\u6837\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1RIOSWorld\u57fa\u51c6\uff0c\u5305\u542b492\u4e2a\u6db5\u76d6\u591a\u7c7b\u8ba1\u7b97\u673a\u5e94\u7528\u7684\u9669\u4efb\u52a1\uff0c\u5e76\u5c06\u98ce\u9669\u5206\u4e3a\u7528\u6237\u6e90\u98ce\u9669\u548c\u73af\u5883\u98ce\u9669\u3002\u4ece\u98ce\u9669\u610f\u56fe\u548c\u5b8c\u6210\u5ea6\u4e24\u4e2a\u89d2\u5ea6\u8bc4\u4f30\u5b89\u5168\u98ce\u9669\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u9762\u4e34\u663e\u8457\u5b89\u5168\u98ce\u9669\uff0c\u5f3a\u8c03\u4e86\u5b89\u5168\u5bf9\u9f50\u7684\u7d27\u8feb\u6027\u3002", "conclusion": "RIOSWorld\u4e3a\u5f00\u53d1\u53ef\u4fe1\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u8868\u660e\u9700\u52a0\u5f3a\u5b89\u5168\u5bf9\u9f50\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5b89\u5168\u98ce\u9669, \u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406, RIOSWorld\u57fa\u51c6"}}
{"id": "2506.00316", "pdf": "https://arxiv.org/pdf/2506.00316", "abs": "https://arxiv.org/abs/2506.00316", "authors": ["Atul Ganju", "Shashaank Aiyer", "Ved Sriraman", "Karthik Sridharan"], "title": "Active Learning via Regression Beyond Realizability", "categories": ["cs.LG"], "comment": null, "summary": "We present a new active learning framework for multiclass classification\nbased on surrogate risk minimization that operates beyond the standard\nrealizability assumption. Existing surrogate-based active learning algorithms\ncrucially rely on realizability$\\unicode{x2014}$the assumption that the optimal\nsurrogate predictor lies within the model class$\\unicode{x2014}$limiting their\napplicability in practical, misspecified settings. In this work we show that\nunder conditions significantly weaker than realizability, as long as the class\nof models considered is convex, one can still obtain a label and sample\ncomplexity comparable to prior work. Despite achieving similar rates, the\nalgorithmic approaches from prior works can be shown to fail in non-realizable\nsettings where our assumption is satisfied. Our epoch-based active learning\nalgorithm departs from prior methods by fitting a model from the full class to\nthe queried data in each epoch and returning an improper classifier obtained by\naggregating these models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u98ce\u9669\u6700\u5c0f\u5316\u7684\u591a\u7c7b\u5206\u7c7b\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u7a81\u7834\u6807\u51c6\u7684\u53ef\u5b9e\u73b0\u6027\u5047\u8bbe\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4ee3\u7406\u7684\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\u4f9d\u8d56\u4e8e\u53ef\u5b9e\u73b0\u6027\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u5728\u5b9e\u9645\u8bef\u8bbe\u5b9a\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u51f8\u6a21\u578b\u7c7b\u5e76\u5728\u6bcf\u4e2aepoch\u4e2d\u5bf9\u67e5\u8be2\u6570\u636e\u62df\u5408\u5b8c\u6574\u6a21\u578b\uff0c\u8fd4\u56de\u901a\u8fc7\u805a\u5408\u6a21\u578b\u5f97\u5230\u7684\u4e0d\u5f53\u5206\u7c7b\u5668\u3002", "result": "\u5728\u663e\u8457\u5f31\u4e8e\u53ef\u5b9e\u73b0\u6027\u7684\u6761\u4ef6\u4e0b\uff0c\u4ecd\u80fd\u83b7\u5f97\u4e0e\u73b0\u6709\u5de5\u4f5c\u76f8\u5f53\u7684\u6807\u7b7e\u548c\u6837\u672c\u590d\u6742\u5ea6\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u5728\u975e\u53ef\u5b9e\u73b0\u6027\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "keywords": "\u4e3b\u52a8\u5b66\u4e60, \u591a\u7c7b\u5206\u7c7b, \u4ee3\u7406\u98ce\u9669, \u53ef\u5b9e\u73b0\u6027"}}
{"id": "2506.00267", "pdf": "https://arxiv.org/pdf/2506.00267", "abs": "https://arxiv.org/abs/2506.00267", "authors": ["Cihan Xiao", "Ruixing Liang", "Xiangyu Zhang", "Mehmet Emre Tiryaki", "Veronica Bae", "Lavanya Shankar", "Rong Yang", "Ethan Poon", "Emmanuel Dupoux", "Sanjeev Khudanpur", "Leibny Paola Garcia Perera"], "title": "CASPER: A Large Scale Spontaneous Speech Dataset", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "The success of large language models has driven interest in developing\nsimilar speech processing capabilities. However, a key challenge is the\nscarcity of high-quality spontaneous speech data, as most existing datasets\ncontain scripted dialogues. To address this, we present a novel pipeline for\neliciting and recording natural dialogues and release our Stage 1 dataset with\n200+ hours of spontaneous speech. Our approach fosters fluid, natural\nconversations while encouraging a diverse range of topics and interactive\nexchanges. Unlike traditional methods, it facilitates genuine interactions,\nproviding a reproducible framework for future data collection. This paper\nintroduces our dataset and methodology, laying the groundwork for addressing\nthe shortage of spontaneous speech data. We plan to expand this dataset in\nfuture stages, offering a growing resource for the research community.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d41\u7a0b\u6765\u6536\u96c6\u81ea\u7136\u5bf9\u8bdd\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u9ad8\u8d28\u91cf\u81ea\u53d1\u8bed\u97f3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b200+\u5c0f\u65f6\u81ea\u53d1\u8bed\u97f3\u7684Stage 1\u6570\u636e\u96c6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6210\u529f\u6fc0\u53d1\u4e86\u5f00\u53d1\u7c7b\u4f3c\u8bed\u97f3\u5904\u7406\u80fd\u529b\u7684\u5174\u8da3\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u591a\u4e3a\u811a\u672c\u5bf9\u8bdd\uff0c\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u81ea\u53d1\u8bed\u97f3\u6570\u636e\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u6d41\u7a0b\uff0c\u901a\u8fc7\u5f15\u5bfc\u548c\u8bb0\u5f55\u81ea\u7136\u5bf9\u8bdd\uff0c\u9f13\u52b1\u591a\u6837\u5316\u548c\u4e92\u52a8\u6027\u5f3a\u7684\u4ea4\u6d41\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0c\u5b83\u4fc3\u8fdb\u4e86\u771f\u5b9e\u7684\u4e92\u52a8\u3002", "result": "\u53d1\u5e03\u4e86Stage 1\u6570\u636e\u96c6\uff0c\u5305\u542b200+\u5c0f\u65f6\u7684\u81ea\u53d1\u8bed\u97f3\u6570\u636e\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8d44\u6e90\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u586b\u8865\u81ea\u53d1\u8bed\u97f3\u6570\u636e\u77ed\u7f3a\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u8ba1\u5212\u5728\u672a\u6765\u6269\u5c55\u6570\u636e\u96c6\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u6301\u7eed\u589e\u957f\u7684\u8d44\u6e90\u3002", "keywords": "\u81ea\u53d1\u8bed\u97f3\u6570\u636e, \u81ea\u7136\u5bf9\u8bdd, \u6570\u636e\u96c6\u6536\u96c6, \u8bed\u97f3\u5904\u7406"}}
{"id": "2506.00641", "pdf": "https://arxiv.org/pdf/2506.00641", "abs": "https://arxiv.org/abs/2506.00641", "authors": ["Hanjun Luo", "Shenyu Dai", "Chiming Ni", "Xinfeng Li", "Guibin Zhang", "Kun Wang", "Tongliang Liu", "Hanan Salam"], "title": "AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "Despite the rapid advancement of LLM-based agents, the reliable evaluation of\ntheir safety and security remains a significant challenge. Existing rule-based\nor LLM-based evaluators often miss dangers in agents' step-by-step actions,\noverlook subtle meanings, fail to see how small issues compound, and get\nconfused by unclear safety or security rules. To overcome this evaluation\ncrisis, we introduce \\sys, a universal, training-free, memory-augmented\nreasoning framework that empowers LLM evaluators to emulate human expert\nevaluators. \\sys constructs an experiential memory by having an LLM adaptively\nextract structured semantic features (e.g., scenario, risk, behavior) and\ngenerate associated chain-of-thought reasoning traces for past interactions. A\nmulti-stage, context-aware retrieval-augmented generation process then\ndynamically retrieves the most relevant reasoning experiences to guide the LLM\nevaluator's assessment of new cases. Moreover, we developed \\data, the first\nbenchmark designed to check how well LLM-based evaluators can spot both safety\nrisks and security threats. \\data comprises \\textbf{2293} meticulously\nannotated interaction records, covering \\textbf{15} risk types across\n\\textbf{29} application scenarios. A key feature of \\data is its nuanced\napproach to ambiguous risk situations, employing ``Strict'' and ``Lenient''\njudgment standards. Experiments demonstrate that \\sys not only consistently\nimproves the evaluation performance of LLMs across all benchmarks but also sets\na new state-of-the-art in LLM-as-a-judge for agent safety and security,\nachieving human-level accuracy. Our work is openly openly accessible.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\backslash sys\u7684\u901a\u7528\u3001\u514d\u8bad\u7ec3\u3001\u8bb0\u5fc6\u589e\u5f3a\u63a8\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u65b0\u7684\u57fa\u51c6\backslash data\uff0c\u663e\u8457\u63d0\u5347LLM\u8bc4\u4f30\u8005\u5728\u5b89\u5168\u548c\u5b89\u5168\u98ce\u9669\u68c0\u6d4b\u4e0a\u7684\u8868\u73b0\uff0c\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c4\u5219\u6216LLM\u7684\u8bc4\u4f30\u65b9\u6cd5\u5728\u68c0\u6d4b\u9010\u6b65\u52a8\u4f5c\u4e2d\u7684\u5371\u9669\u3001\u7ec6\u5fae\u542b\u4e49\u3001\u5c0f\u95ee\u9898\u7d2f\u79ef\u53ca\u6a21\u7cca\u89c4\u5219\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u4e9f\u9700\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\backslash sys\u901a\u8fc7\u6784\u5efa\u7ecf\u9a8c\u8bb0\u5fc6\u5e93\uff0c\u52a8\u6001\u68c0\u7d22\u76f8\u5173\u63a8\u7406\u7ecf\u9a8c\u6307\u5bfc\u8bc4\u4f30\uff1b\u540c\u65f6\u5f00\u53d1\u9996\u4e2a\u9488\u5bf9\u5b89\u5168\u4e0e\u5b89\u5168\u98ce\u9669\u7684\u57fa\u51c6\backslash data\uff0c\u5305\u542b2293\u6761\u6807\u6ce8\u8bb0\u5f55\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\backslash sys\u5728\u6240\u6709\u57fa\u51c6\u4e0a\u5747\u63d0\u5347\u4e86LLM\u7684\u8bc4\u4f30\u6027\u80fd\uff0c\u5e76\u8fbe\u5230\u4e86\u4eba\u7c7b\u6c34\u5e73\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\backslash sys\u4e3aLLM\u8bc4\u4f30\u8005\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u9760\u7684\u6846\u67b6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "keywords": "LLM\u8bc4\u4f30\u3001\u5b89\u5168\u4e0e\u5b89\u5168\u3001\u8bb0\u5fc6\u589e\u5f3a\u63a8\u7406\u3001\u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2506.00329", "pdf": "https://arxiv.org/pdf/2506.00329", "abs": "https://arxiv.org/abs/2506.00329", "authors": ["Muhammad Adnan", "Nithesh Kurella", "Akhil Arunkumar", "Prashant J. Nair"], "title": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to 1.63x end-to-end speedup, while maintaining\nvideo quality. The source code of Foresight is available at\n\\texttt{https://github.com/STAR-Laboratory/foresight}.", "AI": {"tldr": "Foresight \u662f\u4e00\u79cd\u81ea\u9002\u5e94\u5c42\u91cd\u7528\u6280\u672f\uff0c\u901a\u8fc7\u51cf\u5c11\u53bb\u566a\u6b65\u9aa4\u4e2d\u7684\u8ba1\u7b97\u5197\u4f59\uff0c\u663e\u8457\u63d0\u5347\u4e86 Diffusion Transformers (DiTs) \u5728\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u4e0d\u53d8\u3002", "motivation": "DiTs \u5728\u89c6\u9891\u751f\u6210\u4e2d\u56e0\u6a21\u578b\u5927\u5c0f\u5927\u548c\u7a7a\u95f4-\u65f6\u95f4\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u6210\u672c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u9ad8\uff0c\u9759\u6001\u7f13\u5b58\u65e0\u6cd5\u9002\u5e94\u751f\u6210\u52a8\u6001\uff0c\u901f\u5ea6\u548c\u8d28\u91cf\u5e73\u8861\u4e0d\u4f73\u3002", "method": "Foresight \u52a8\u6001\u8bc6\u522b\u5e76\u91cd\u7528 DiT \u5757\u8f93\u51fa\uff0c\u6839\u636e\u751f\u6210\u53c2\u6570\uff08\u5982\u5206\u8fa8\u7387\u548c\u53bb\u566a\u8ba1\u5212\uff09\u81ea\u9002\u5e94\u4f18\u5316\u6548\u7387\u3002", "result": "\u5728 OpenSora\u3001Latte \u548c CogVideoX \u4e0a\uff0cForesight \u5b9e\u73b0\u4e86\u6700\u9ad8 1.63 \u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u4e14\u89c6\u9891\u8d28\u91cf\u4e0d\u53d8\u3002", "conclusion": "Foresight \u901a\u8fc7\u81ea\u9002\u5e94\u5c42\u91cd\u7528\u6280\u672f\u9ad8\u6548\u89e3\u51b3\u4e86 DiTs \u7684\u8ba1\u7b97\u5197\u4f59\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u6548\u7387\u3002", "keywords": "Diffusion Transformers, \u89c6\u9891\u751f\u6210, \u81ea\u9002\u5e94\u5c42\u91cd\u7528, \u8ba1\u7b97\u6548\u7387"}}
{"id": "2506.00277", "pdf": "https://arxiv.org/pdf/2506.00277", "abs": "https://arxiv.org/abs/2506.00277", "authors": ["Hans W. A. Hanley", "Zakir Durumeric"], "title": "Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": "Accepted to The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Contextual large language model embeddings are increasingly utilized for\ntopic modeling and clustering. However, current methods often scale poorly,\nrely on opaque similarity metrics, and struggle in multilingual settings. In\nthis work, we present a novel, scalable, interpretable, hierarchical, and\nmultilingual approach to clustering news articles and social media data. To do\nthis, we first train multilingual Matryoshka embeddings that can determine\nstory similarity at varying levels of granularity based on which subset of the\ndimensions of the embeddings is examined. This embedding model achieves\nstate-of-the-art performance on the SemEval 2022 Task 8 test dataset (Pearson\n$\\rho$ = 0.816). Once trained, we develop an efficient hierarchical clustering\nalgorithm that leverages the hierarchical nature of Matryoshka embeddings to\nidentify unique news stories, narratives, and themes. We conclude by\nillustrating how our approach can identify and cluster stories, narratives, and\noverarching themes within real-world news datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u3001\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u3001\u5c42\u6b21\u5316\u548c\u591a\u8bed\u8a00\u7684\u65b0\u95fb\u53ca\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u805a\u7c7b\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u8bed\u8a00Matryoshka\u5d4c\u5165\u6a21\u578b\u548c\u9ad8\u6548\u5c42\u6b21\u805a\u7c7b\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u73af\u5883\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u76f8\u4f3c\u6027\u5ea6\u91cf\u4e0d\u900f\u660e\u3002", "method": "\u8bad\u7ec3\u591a\u8bed\u8a00Matryoshka\u5d4c\u5165\u6a21\u578b\uff0c\u652f\u6301\u4e0d\u540c\u7c92\u5ea6\u7684\u6545\u4e8b\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u5c42\u6b21\u7ed3\u6784\u7684\u805a\u7c7b\u7b97\u6cd5\u3002", "result": "\u5d4c\u5165\u6a21\u578b\u5728SemEval 2022 Task 8\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u53d6\u5f97Pearson \u03c1=0.816\u7684SOTA\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u805a\u7c7b\u65b0\u95fb\u6545\u4e8b\u3001\u53d9\u4e8b\u548c\u4e3b\u9898\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u6570\u636e\u3002", "keywords": "\u4e3b\u9898\u5efa\u6a21\u3001\u805a\u7c7b\u3001\u591a\u8bed\u8a00\u5d4c\u5165\u3001\u5c42\u6b21\u805a\u7c7b\u3001\u65b0\u95fb\u5206\u6790"}}
{"id": "2506.00664", "pdf": "https://arxiv.org/pdf/2506.00664", "abs": "https://arxiv.org/abs/2506.00664", "authors": ["Yash Tiwari", "Owais Ahmad Lone", "Mayukha Pal"], "title": "OntoRAG: Enhancing Question-Answering through Automated Ontology Derivation from Unstructured Knowledge Bases", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Ontologies are pivotal for structuring knowledge bases to enhance question\nanswering (QA) systems powered by Large Language Models (LLMs). However,\ntraditional ontology creation relies on manual efforts by domain experts, a\nprocess that is time intensive, error prone, and impractical for large, dynamic\nknowledge domains. This paper introduces OntoRAG, an automated pipeline\ndesigned to derive ontologies from unstructured knowledge bases, with a focus\non electrical relay documents. OntoRAG integrates advanced techniques,\nincluding web scraping, PDF parsing, hybrid chunking, information extraction,\nknowledge graph construction, and ontology creation, to transform unstructured\ndata into a queryable ontology. By leveraging LLMs and graph based methods,\nOntoRAG enhances global sensemaking capabilities, outperforming conventional\nRetrieval Augmented Generation (RAG) and GraphRAG approaches in\ncomprehensiveness and diversity. Experimental results demonstrate OntoRAGs\neffectiveness, achieving a comprehensiveness win rate of 85% against vector RAG\nand 75% against GraphRAGs best configuration. This work addresses the critical\nchallenge of automating ontology creation, advancing the vision of the semantic\nweb.", "AI": {"tldr": "OntoRAG\u662f\u4e00\u79cd\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u7528\u4e8e\u4ece\u975e\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u4e2d\u6784\u5efa\u672c\u4f53\uff0c\u7279\u522b\u9488\u5bf9\u7535\u6c14\u7ee7\u7535\u5668\u6587\u6863\uff0c\u7ed3\u5408\u591a\u79cd\u6280\u672f\u63d0\u5347\u95ee\u7b54\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u672c\u4f53\u6784\u5efa\u4f9d\u8d56\u4eba\u5de5\uff0c\u8017\u65f6\u957f\u4e14\u6613\u51fa\u9519\uff0cOntoRAG\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u672c\u4f53\u521b\u5efa\u3002", "method": "OntoRAG\u6574\u5408\u4e86\u7f51\u7edc\u722c\u53d6\u3001PDF\u89e3\u6790\u3001\u6df7\u5408\u5206\u5757\u3001\u4fe1\u606f\u63d0\u53d6\u3001\u77e5\u8bc6\u56fe\u6784\u5efa\u548c\u672c\u4f53\u521b\u5efa\u7b49\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u663e\u793aOntoRAG\u5728\u5168\u9762\u6027\u548c\u591a\u6837\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edfRAG\u548cGraphRAG\uff0c\u5168\u9762\u6027\u80dc\u7387\u5206\u522b\u4e3a85%\u548c75%\u3002", "conclusion": "OntoRAG\u63a8\u52a8\u4e86\u8bed\u4e49\u7f51\u7684\u613f\u666f\uff0c\u89e3\u51b3\u4e86\u672c\u4f53\u81ea\u52a8\u5316\u521b\u5efa\u7684\u5173\u952e\u6311\u6218\u3002", "keywords": "OntoRAG, \u672c\u4f53\u6784\u5efa, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u77e5\u8bc6\u56fe\u8c31, \u95ee\u7b54\u7cfb\u7edf"}}
{"id": "2506.00337", "pdf": "https://arxiv.org/pdf/2506.00337", "abs": "https://arxiv.org/abs/2506.00337", "authors": ["Ming Hu", "Jianfu Yin", "Mingyu Dou", "Yuqi Wang", "Ruochen Dang", "Siyi Liang", "Cong Hu", "Yao Wang", "Bingliang Hu", "Quan Wang"], "title": "Channel-Imposed Fusion: A Simple yet Effective Method for Medical Time Series Classification", "categories": ["cs.LG"], "comment": null, "summary": "The automatic classification of medical time series signals, such as\nelectroencephalogram (EEG) and electrocardiogram (ECG), plays a pivotal role in\nclinical decision support and early detection of diseases. Although Transformer\nbased models have achieved notable performance by implicitly modeling temporal\ndependencies through self-attention mechanisms, their inherently complex\narchitectures and opaque reasoning processes undermine their trustworthiness in\nhigh stakes clinical settings. In response to these limitations, this study\nshifts focus toward a modeling paradigm that emphasizes structural\ntransparency, aligning more closely with the intrinsic characteristics of\nmedical data. We propose a novel method, Channel Imposed Fusion (CIF), which\nenhances the signal-to-noise ratio through cross-channel information fusion,\neffectively reduces redundancy, and improves classification performance.\nFurthermore, we integrate CIF with the Temporal Convolutional Network (TCN),\nknown for its structural simplicity and controllable receptive field, to\nconstruct an efficient and explicit classification framework. Experimental\nresults on multiple publicly available EEG and ECG datasets demonstrate that\nthe proposed method not only outperforms existing state-of-the-art (SOTA)\napproaches in terms of various classification metrics, but also significantly\nenhances the transparency of the classification process, offering a novel\nperspective for medical time series classification.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u65b9\u6cd5CIF\uff0c\u7ed3\u5408TCN\uff0c\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\u548c\u900f\u660e\u5ea6\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u590d\u6742\u67b6\u6784\u548c\u7f3a\u4e4f\u900f\u660e\u6027\u9650\u5236\u4e86\u5176\u5728\u4e34\u5e8a\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faChannel Imposed Fusion (CIF)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u901a\u9053\u4fe1\u606f\u878d\u5408\u63d0\u5347\u4fe1\u566a\u6bd4\uff0c\u5e76\u7ed3\u5408Temporal Convolutional Network (TCN)\u6784\u5efa\u5206\u7c7b\u6846\u67b6\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00EEG\u548cECG\u6570\u636e\u96c6\u4e0a\uff0cCIF+TCN\u4e0d\u4ec5\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u8fd8\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u8fc7\u7a0b\u7684\u900f\u660e\u5ea6\u3002", "conclusion": "CIF+TCN\u4e3a\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u900f\u660e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b, CIF, TCN, \u900f\u660e\u5ea6, EEG, ECG"}}
{"id": "2506.00288", "pdf": "https://arxiv.org/pdf/2506.00288", "abs": "https://arxiv.org/abs/2506.00288", "authors": ["Ahmed Elhady", "Eneko Agirre", "Mikel Artetxe"], "title": "Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation", "categories": ["cs.CL", "cs.AI"], "comment": "To appear in ACL 2025 Main", "summary": "Continued pretraining (CPT) is a popular approach to adapt existing large\nlanguage models (LLMs) to new languages. When doing so, it is common practice\nto include a portion of English data in the mixture, but its role has not been\ncarefully studied to date. In this work, we show that including English does\nnot impact validation perplexity, yet it is critical for the emergence of\ndownstream capabilities in the target language. We introduce a\nlanguage-agnostic benchmark for in-context learning (ICL), which reveals\ncatastrophic forgetting early on CPT when English is not included. This in turn\ndamages the ability of the model to generalize to downstream prompts in the\ntarget language as measured by perplexity, even if it does not manifest in\nterms of accuracy until later in training, and can be tied to a big shift in\nthe model parameters. Based on these insights, we introduce curriculum learning\nand exponential moving average (EMA) of weights as effective alternatives to\nmitigate the need for English. All in all, our work sheds light into the\ndynamics by which emergent abilities arise when doing CPT for language\nadaptation, and can serve as a foundation to design more effective methods in\nthe future.", "AI": {"tldr": "Continued pretraining (CPT) with English data in the mix doesn't affect perplexity but is crucial for downstream task performance in the target language. Excluding English leads to catastrophic forgetting and poor generalization, mitigated by curriculum learning and EMA.", "motivation": "To understand the role of English data in CPT for adapting LLMs to new languages, especially its influence on downstream capabilities despite not impacting perplexity.", "method": "Introduces a language-agnostic benchmark for in-context learning (ICL), studies catastrophic forgetting, and proposes curriculum learning and EMA to reduce reliance on English data.", "result": "Including English prevents catastrophic forgetting and ensures better generalization, while exclusion leads to poor performance despite similar perplexity.", "conclusion": "English data is vital for emergent abilities during CPT, and alternative methods like curriculum learning and EMA can help mitigate its necessity.", "keywords": "Continued pretraining, language adaptation, catastrophic forgetting, curriculum learning, exponential moving average"}}
{"id": "2506.00708", "pdf": "https://arxiv.org/pdf/2506.00708", "abs": "https://arxiv.org/abs/2506.00708", "authors": ["Yongkang Xiao", "Sinian Zhang", "Yi Dai", "Huixue Zhou", "Jue Hou", "Jie Ding", "Rui Zhang"], "title": "DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Knowledge graph completion (KGC) aims to predict missing triples in knowledge\ngraphs (KGs) by leveraging existing triples and textual information. Recently,\ngenerative large language models (LLMs) have been increasingly employed for\ngraph tasks. However, current approaches typically encode graph context in\ntextual form, which fails to fully exploit the potential of LLMs for perceiving\nand reasoning about graph structures. To address this limitation, we propose\nDrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph\nCompletion). DrKGC employs a flexible lightweight model training strategy to\nlearn structural embeddings and logical rules within the KG. It then leverages\na novel bottom-up graph retrieval method to extract a subgraph for each query\nguided by the learned rules. Finally, a graph convolutional network (GCN)\nadapter uses the retrieved subgraph to enhance the structural embeddings, which\nare then integrated into the prompt for effective LLM fine-tuning. Experimental\nresults on two general domain benchmark datasets and two biomedical datasets\ndemonstrate the superior performance of DrKGC. Furthermore, a realistic case\nstudy in the biomedical domain highlights its interpretability and practical\nutility.", "AI": {"tldr": "DrKGC\u901a\u8fc7\u52a8\u6001\u5b50\u56fe\u68c0\u7d22\u589e\u5f3aLLMs\uff0c\u7ed3\u5408\u7ed3\u6784\u5d4c\u5165\u548c\u903b\u8f91\u89c4\u5219\uff0c\u63d0\u5347\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528LLMs\u5728\u56fe\u7ed3\u6784\u611f\u77e5\u548c\u63a8\u7406\u4e0a\u7684\u6f5c\u529b\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86DrKGC\u3002", "method": "DrKGC\u7ed3\u5408\u8f7b\u91cf\u7ea7\u6a21\u578b\u8bad\u7ec3\u7b56\u7565\u5b66\u4e60\u7ed3\u6784\u5d4c\u5165\u548c\u903b\u8f91\u89c4\u5219\uff0c\u91c7\u7528\u81ea\u5e95\u5411\u4e0a\u7684\u5b50\u56fe\u68c0\u7d22\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7GCN\u9002\u914d\u5668\u589e\u5f3aLLMs\u7684\u63d0\u793a\u3002", "result": "\u5728\u901a\u7528\u548c\u751f\u7269\u533b\u5b66\u9886\u57df\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cDrKGC\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "DrKGC\u901a\u8fc7\u52a8\u6001\u5b50\u56fe\u68c0\u7d22\u548c\u7ed3\u6784\u5d4c\u5165\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u6548\u679c\u548c\u89e3\u91ca\u6027\u3002", "keywords": "\u77e5\u8bc6\u56fe\u8c31\u8865\u5168,LLMs,\u5b50\u56fe\u68c0\u7d22,\u7ed3\u6784\u5d4c\u5165,\u903b\u8f91\u89c4\u5219,GCN"}}
{"id": "2506.00356", "pdf": "https://arxiv.org/pdf/2506.00356", "abs": "https://arxiv.org/abs/2506.00356", "authors": ["Rorry Brenner", "Evan Davis", "Rushi Chaudhari", "Rowan Morse", "Jingyao Chen", "Xirui Liu", "Zhaoyi You", "Laurent Itti"], "title": "Exploring the Performance of Perforated Backpropagation through Further Experiments", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages, 7 figures, 1 table", "summary": "Perforated Backpropagation is a neural network optimization technique based\non modern understanding of the computational importance of dendrites within\nbiological neurons. This paper explores further experiments from the original\npublication, generated from a hackathon held at the Carnegie Mellon Swartz\nCenter in February 2025. Students and local Pittsburgh ML practitioners were\nbrought together to experiment with the Perforated Backpropagation algorithm on\nthe datasets and models which they were using for their projects. Results\nshowed that the system could enhance their projects, with up to 90% model\ncompression without negative impact on accuracy, or up to 16% increased\naccuracy of their original models.", "AI": {"tldr": "Perforated Backpropagation\u662f\u4e00\u79cd\u57fa\u4e8e\u751f\u7269\u795e\u7ecf\u5143\u6811\u7a81\u8ba1\u7b97\u91cd\u8981\u6027\u7684\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u6280\u672f\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22Perforated Backpropagation\u7b97\u6cd5\u5728\u5b9e\u9645\u9879\u76ee\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u9a8c\u8bc1\u5176\u662f\u5426\u80fd\u63d0\u5347\u6a21\u578b\u538b\u7f29\u548c\u51c6\u786e\u6027\u3002", "method": "\u5728Carnegie Mellon Swartz Center\u4e3e\u529e\u7684hackathon\u4e2d\uff0c\u5b66\u751f\u548c\u5f53\u5730ML\u4ece\u4e1a\u8005\u5c06Perforated Backpropagation\u5e94\u7528\u4e8e\u5404\u81ea\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6280\u672f\u53ef\u5b9e\u73b0\u9ad8\u8fbe90%\u7684\u6a21\u578b\u538b\u7f29\u800c\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\uff0c\u6216\u63d0\u5347\u539f\u59cb\u6a21\u578b\u51c6\u786e\u6027\u8fbe16%\u3002", "conclusion": "Perforated Backpropagation\u662f\u4e00\u79cd\u6709\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u6280\u672f\uff0c\u5177\u6709\u663e\u8457\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "Perforated Backpropagation, \u6a21\u578b\u538b\u7f29, \u795e\u7ecf\u7f51\u7edc\u4f18\u5316, hackathon, \u6811\u7a81\u8ba1\u7b97"}}
{"id": "2506.00290", "pdf": "https://arxiv.org/pdf/2506.00290", "abs": "https://arxiv.org/abs/2506.00290", "authors": ["Tianqi Chen", "Shujian Zhang", "Mingyuan Zhou"], "title": "DLM-One: Diffusion Language Models for One-Step Sequence Generation", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "This paper introduces DLM-One, a score-distillation-based framework for\none-step sequence generation with continuous diffusion language models (DLMs).\nDLM-One eliminates the need for iterative refinement by aligning the scores of\na student model's outputs in the continuous token embedding space with the\nscore function of a pretrained teacher DLM. We investigate whether DLM-One can\nachieve substantial gains in sampling efficiency for language modeling. Through\ncomprehensive experiments on DiffuSeq -- a representative continuous DLM -- we\nshow that DLM-One achieves up to ~500x speedup in inference time while\nmaintaining competitive performance on benchmark text generation tasks used to\nevaluate the teacher models. We further analyze the method's empirical behavior\nacross multiple datasets, providing initial insights into its generality and\npractical applicability. Our findings position one-step diffusion as a\npromising direction for efficient, high-quality language generation and broader\nadoption of continuous diffusion models operating in embedding space for\nnatural language processing.", "AI": {"tldr": "DLM-One \u662f\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u84b8\u998f\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u8fde\u7eed\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08DLMs\uff09\u5b9e\u73b0\u4e00\u6b65\u5e8f\u5217\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91c7\u6837\u6548\u7387\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u6269\u6563\u6a21\u578b\u9700\u8981\u8fed\u4ee3\u7ec6\u5316\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4e00\u6b65\u751f\u6210\u63d0\u5347\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5c06\u5b66\u751f\u6a21\u578b\u8f93\u51fa\u5728\u8fde\u7eed\u6807\u8bb0\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5206\u6570\u4e0e\u6559\u5e08 DLM \u7684\u5206\u6570\u51fd\u6570\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e00\u6b65\u751f\u6210\u3002", "result": "\u5728 DiffuSeq \u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u63a8\u7406\u65f6\u95f4\u53ef\u63d0\u5347\u7ea6 500 \u500d\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u4e00\u6b65\u6269\u6563\u4e3a\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u8bed\u8a00\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "DLM-One, \u6269\u6563\u6a21\u578b, \u8bed\u8a00\u751f\u6210, \u5206\u6570\u84b8\u998f"}}
{"id": "2506.00751", "pdf": "https://arxiv.org/pdf/2506.00751", "abs": "https://arxiv.org/abs/2506.00751", "authors": ["Zhuojun Gu", "Quan Wang", "Shuchu Han"], "title": "Alignment Revisited: Are Large Language Models Consistent in Stated and Revealed Preferences?", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) highlight the need to align\ntheir behaviors with human values. A critical, yet understudied, issue is the\npotential divergence between an LLM's stated preferences (its reported\nalignment with general principles) and its revealed preferences (inferred from\ndecisions in contextualized scenarios). Such deviations raise fundamental\nconcerns for the interpretability, trustworthiness, reasoning transparency, and\nethical deployment of LLMs, particularly in high-stakes applications. This work\nformally defines and proposes a method to measure this preference deviation. We\ninvestigate how LLMs may activate different guiding principles in specific\ncontexts, leading to choices that diverge from previously stated general\nprinciples. Our approach involves crafting a rich dataset of well-designed\nprompts as a series of forced binary choices and presenting them to LLMs. We\ncompare LLM responses to general principle prompts stated preference with LLM\nresponses to contextualized prompts revealed preference, using metrics like KL\ndivergence to quantify the deviation. We repeat the analysis across different\ncategories of preferences and on four mainstream LLMs and find that a minor\nchange in prompt format can often pivot the preferred choice regardless of the\npreference categories and LLMs in the test. This prevalent phenomenon\nhighlights the lack of understanding and control of the LLM decision-making\ncompetence. Our study will be crucial for integrating LLMs into services,\nespecially those that interact directly with humans, where morality, fairness,\nand social responsibilities are crucial dimensions. Furthermore, identifying or\nbeing aware of such deviation will be critically important as LLMs are\nincreasingly envisioned for autonomous agentic tasks where continuous human\nevaluation of all LLMs' intermediary decision-making steps is impossible.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u516c\u5f00\u5ba3\u79f0\u7684\u504f\u597d\u4e0e\u5728\u5177\u4f53\u60c5\u5883\u4e2d\u8868\u73b0\u51fa\u7684\u504f\u597d\u4e4b\u95f4\u7684\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u91cf\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u8bc4\u4f30LLM\u7684\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u4fe1\u5ea6\u548c\u4f26\u7406\u90e8\u7f72\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u4e30\u5bcc\u7684\u63d0\u793a\u6570\u636e\u96c6\u4f5c\u4e3a\u5f3a\u5236\u4e8c\u5143\u9009\u62e9\uff0c\u6bd4\u8f83LLM\u5bf9\u4e00\u822c\u539f\u5219\u548c\u60c5\u5883\u5316\u63d0\u793a\u7684\u54cd\u5e94\uff0c\u7528KL\u6563\u5ea6\u91cf\u5316\u504f\u5dee\u3002", "result": "\u53d1\u73b0\u8f7b\u5fae\u63d0\u793a\u683c\u5f0f\u53d8\u5316\u53ef\u5bfc\u81f4\u504f\u597d\u6539\u53d8\uff0c\u504f\u5dee\u73b0\u8c61\u666e\u904d\u5b58\u5728\u3002", "conclusion": "\u7814\u7a76\u5bf9LLM\u96c6\u6210\u5230\u670d\u52a1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u9053\u5fb7\u548c\u793e\u4f1a\u8d23\u4efb\u7684\u573a\u666f\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u504f\u597d\u504f\u5dee,\u4f26\u7406\u90e8\u7f72,\u53ef\u89e3\u91ca\u6027"}}
{"id": "2506.00362", "pdf": "https://arxiv.org/pdf/2506.00362", "abs": "https://arxiv.org/abs/2506.00362", "authors": ["Hoang T. Nguyen", "Priya L. Donti"], "title": "FSNet: Feasibility-Seeking Neural Network for Constrained Optimization with Guarantees", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Efficiently solving constrained optimization problems is crucial for numerous\nreal-world applications, yet traditional solvers are often computationally\nprohibitive for real-time use. Machine learning-based approaches have emerged\nas a promising alternative to provide approximate solutions at faster speeds,\nbut they struggle to strictly enforce constraints, leading to infeasible\nsolutions in practice. To address this, we propose the\nFeasibility-Seeking-Integrated Neural Network (FSNet), which integrates a\nfeasibility-seeking step directly into its solution procedure to ensure\nconstraint satisfaction. This feasibility-seeking step solves an unconstrained\noptimization problem that minimizes constraint violations in a differentiable\nmanner, enabling end-to-end training and providing guarantees on feasibility\nand convergence. Our experiments across a range of different optimization\nproblems, including both smooth/nonsmooth and convex/nonconvex problems,\ndemonstrate that FSNet can provide feasible solutions with solution quality\ncomparable to (or in some cases better than) traditional solvers, at\nsignificantly faster speeds.", "AI": {"tldr": "FSNet\u901a\u8fc7\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u96c6\u6210\u53ef\u884c\u6027\u641c\u7d22\u6b65\u9aa4\uff0c\u786e\u4fdd\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u7684\u89e3\u4e25\u683c\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\uff0c\u540c\u65f6\u63d0\u4f9b\u9ad8\u6548\u7684\u8fd1\u4f3c\u89e3\u3002", "motivation": "\u4f20\u7edf\u6c42\u89e3\u5668\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u800c\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u4e25\u683c\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\u3002", "method": "\u63d0\u51faFSNet\uff0c\u5c06\u53ef\u884c\u6027\u641c\u7d22\u6b65\u9aa4\u76f4\u63a5\u6574\u5408\u5230\u6c42\u89e3\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u89e3\u65e0\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u6700\u5c0f\u5316\u7ea6\u675f\u8fdd\u53cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFSNet\u80fd\u5728\u591a\u79cd\u4f18\u5316\u95ee\u9898\u4e2d\u63d0\u4f9b\u53ef\u884c\u89e3\uff0c\u89e3\u7684\u8d28\u91cf\u4e0e\u4f20\u7edf\u6c42\u89e3\u5668\u76f8\u5f53\uff0c\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "FSNet\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4e25\u683c\u6ee1\u8db3\u7ea6\u675f\u7684\u65b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u4f18\u5316\u95ee\u9898\u3002", "keywords": "\u7ea6\u675f\u4f18\u5316\u3001\u795e\u7ecf\u7f51\u7edc\u3001\u53ef\u884c\u6027\u3001\u5b9e\u65f6\u6c42\u89e3"}}
{"id": "2506.00304", "pdf": "https://arxiv.org/pdf/2506.00304", "abs": "https://arxiv.org/abs/2506.00304", "authors": ["Payal Mohapatra", "Akash Pandey", "Xiaoyuan Zhang", "Qi Zhu"], "title": "Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion with LLMs", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 main conference", "summary": "Unvoiced electromyography (EMG) is an effective communication tool for\nindividuals unable to produce vocal speech. However, most prior methods rely on\npaired voiced and unvoiced EMG signals, along with speech data, for EMG-to-text\nconversion, which is not practical for such individuals. Given the rise of\nlarge language models (LLMs) in speech recognition, we explore their potential\nto understand unvoiced speech. To this end, we address the challenge of\nlearning from unvoiced EMG alone and propose a novel EMG adaptor module that\nmaps EMG features into an LLM's input space, achieving an average word error\nrate (WER) of 0.49 on a closed-vocabulary unvoiced EMG-to-text task. Even with\na conservative data availability of just six minutes, our approach improves\nperformance over specialized models by nearly 20%. While LLMs have been shown\nto be extendable to new language modalities -- such as audio -- understanding\narticulatory biosignals like unvoiced EMG remains more challenging. This work\ntakes a crucial first step toward enabling LLMs to comprehend unvoiced speech\nusing surface EMG.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684EMG\u9002\u914d\u5668\u6a21\u5757\uff0c\u5c06\u65e0\u8bed\u97f3\u808c\u7535\u4fe1\u53f7\uff08EMG\uff09\u7279\u5f81\u6620\u5c04\u5230\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8f93\u5165\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u5bf9\u65e0\u8bed\u97f3\u808c\u7535\u4fe1\u53f7\u5230\u6587\u672c\u7684\u9ad8\u6548\u8f6c\u6362\u3002", "motivation": "\u4e3a\u65e0\u6cd5\u53d1\u58f0\u7684\u4e2a\u4f53\u63d0\u4f9b\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u901a\u8baf\u5de5\u5177\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u914d\u5bf9\u6709\u58f0\u548c\u65e0\u58f0EMG\u4fe1\u53f7\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faEMG\u9002\u914d\u5668\u6a21\u5757\uff0c\u5c06EMG\u7279\u5f81\u6620\u5c04\u5230LLM\u8f93\u5165\u7a7a\u95f4\uff0c\u4ec5\u5229\u7528\u65e0\u58f0EMG\u4fe1\u53f7\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u95ed\u8bcd\u6c47\u65e0\u8bed\u97f3EMG-to-text\u4efb\u52a1\u4e2d\uff0c\u5e73\u5747\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u4e3a0.49\uff0c\u6570\u636e\u91cf\u4ec5\u516d\u5206\u949f\u65f6\u6027\u80fd\u63d0\u5347\u8fd120%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLM\u7406\u89e3\u65e0\u58f0\u808c\u7535\u4fe1\u53f7\u8fc8\u51fa\u4e86\u5173\u952e\u4e00\u6b65\uff0c\u5c55\u793a\u4e86\u5176\u5728\u901a\u8baf\u8f85\u52a9\u6280\u672f\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "\u65e0\u8bed\u97f3\u808c\u7535\u4fe1\u53f7, \u5927\u8bed\u8a00\u6a21\u578b, EMG-to-text, \u901a\u8baf\u8f85\u52a9"}}
{"id": "2506.00765", "pdf": "https://arxiv.org/pdf/2506.00765", "abs": "https://arxiv.org/abs/2506.00765", "authors": ["Shengkun Wang", "Yanshen Sun", "Fanglan Chen", "Linhan Wang", "Naren Ramakrishnan", "Chang-Tien Lu", "Yinlin Chen"], "title": "HouseTS: A Large-Scale, Multimodal Spatiotemporal U.S. Housing Dataset", "categories": ["cs.AI"], "comment": null, "summary": "Accurate house-price forecasting is essential for investors, planners, and\nresearchers. However, reproducible benchmarks with sufficient spatiotemporal\ndepth and contextual richness for long horizon prediction remain scarce. To\naddress this, we introduce HouseTS a large scale, multimodal dataset covering\nmonthly house prices from March 2012 to December 2023 across 6,000 ZIP codes in\n30 major U.S. metropolitan areas. The dataset includes over 890K records,\nenriched with points of Interest (POI), socioeconomic indicators, and detailed\nreal estate metrics. To establish standardized performance baselines, we\nevaluate 14 models, spanning classical statistical approaches, deep neural\nnetworks (DNNs), and pretrained time-series foundation models. We further\ndemonstrate the value of HouseTS in a multimodal case study, where a vision\nlanguage model extracts structured textual descriptions of geographic change\nfrom time stamped satellite imagery. This enables interpretable, grounded\ninsights into urban evolution. HouseTS is hosted on Kaggle, while all\npreprocessing pipelines, benchmark code, and documentation are openly\nmaintained on GitHub to ensure full reproducibility and easy adoption.", "AI": {"tldr": "HouseTS\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u957f\u671f\u623f\u4ef7\u9884\u6d4b\uff0c\u5305\u542b\u4e30\u5bcc\u7684\u65f6\u7a7a\u6570\u636e\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6027\u80fd\u57fa\u51c6\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u5177\u6709\u8db3\u591f\u65f6\u7a7a\u6df1\u5ea6\u548c\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6027\u7684\u53ef\u590d\u73b0\u57fa\u51c6\uff0c\u7814\u7a76\u8005\u5f00\u53d1\u4e86HouseTS\u6570\u636e\u96c6\u4ee5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6570\u636e\u96c6\u5305\u542b890K\u6761\u8bb0\u5f55\uff0c\u6db5\u76d62012\u5e74\u81f32023\u5e7430\u4e2a\u7f8e\u56fd\u5927\u90fd\u5e02\u533a\u7684\u623f\u4ef7\u6570\u636e\uff0c\u5e76\u6574\u5408\u4e86POI\u3001\u793e\u4f1a\u7ecf\u6d4e\u6307\u6807\u7b49\u3002\u8bc4\u4f30\u4e8614\u79cd\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u591a\u6a21\u6001\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86HouseTS\u7684\u4ef7\u503c\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u3001\u9884\u5904\u7406\u7ba1\u9053\u548c\u4ee3\u7801\u4ee5\u652f\u6301\u590d\u73b0\u3002", "conclusion": "HouseTS\u4e3a\u623f\u4ef7\u9884\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\u548c\u591a\u6a21\u6001\u6570\u636e\u652f\u6301\uff0c\u4fc3\u8fdb\u53ef\u590d\u73b0\u6027\u548c\u6613\u7528\u6027\u3002", "keywords": "\u623f\u4ef7\u9884\u6d4b, \u591a\u6a21\u6001\u6570\u636e\u96c6, \u957f\u671f\u9884\u6d4b, HouseTS, \u6807\u51c6\u5316\u57fa\u51c6"}}
{"id": "2506.00382", "pdf": "https://arxiv.org/pdf/2506.00382", "abs": "https://arxiv.org/abs/2506.00382", "authors": ["Xuyuan Liu", "Lei Hsiung", "Yaoqing Yang", "Yujun Yan"], "title": "Spectral Insights into Data-Oblivious Critical Layers in Large Language Models", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted by Findings of ACL2025", "summary": "Understanding how feature representations evolve across layers in large\nlanguage models (LLMs) is key to improving their interpretability and\nrobustness. While recent studies have identified critical layers linked to\nspecific functions or behaviors, these efforts typically rely on data-dependent\nanalyses of fine-tuned models, limiting their use to post-hoc settings. In\ncontrast, we introduce a data-oblivious approach to identify intrinsic critical\nlayers in pre-fine-tuned LLMs by analyzing representation dynamics via Centered\nKernel Alignment(CKA). We show that layers with significant shifts in\nrepresentation space are also those most affected during fine-tuning--a pattern\nthat holds consistently across tasks for a given model. Our spectral analysis\nfurther reveals that these shifts are driven by changes in the top principal\ncomponents, which encode semantic transitions from rationales to conclusions.\nWe further apply these findings to two practical scenarios: efficient domain\nadaptation, where fine-tuning critical layers leads to greater loss reduction\ncompared to non-critical layers; and backdoor defense, where freezing them\nreduces attack success rates by up to 40%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6570\u636e\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7CKA\u8bc6\u522b\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5173\u952e\u5c42\uff0c\u53d1\u73b0\u8fd9\u4e9b\u5c42\u7684\u8868\u5f81\u52a8\u6001\u4e0e\u5fae\u8c03\u6548\u679c\u76f8\u5173\uff0c\u5e76\u5e94\u7528\u4e8e\u9886\u57df\u9002\u5e94\u548c\u540e\u95e8\u9632\u5fa1\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7279\u5f81\u8868\u5f81\u968f\u5c42\u7684\u6f14\u53d8\uff0c\u4ee5\u63d0\u9ad8\u5176\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528Centered Kernel Alignment\uff08CKA\uff09\u5206\u6790\u8868\u5f81\u52a8\u6001\uff0c\u8bc6\u522b\u9884\u5fae\u8c03\u6a21\u578b\u4e2d\u7684\u56fa\u6709\u5173\u952e\u5c42\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5173\u952e\u5c42\u7684\u8868\u5f81\u52a8\u6001\u4e0e\u5fae\u8c03\u6548\u679c\u76f8\u5173\uff0c\u4e14\u5728\u9886\u57df\u9002\u5e94\u548c\u540e\u95e8\u9632\u5fa1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLLMs\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c24\u5176\u5728\u5fae\u8c03\u6548\u7387\u548c\u5b89\u5168\u6027\u65b9\u9762\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5173\u952e\u5c42, CKA, \u9886\u57df\u9002\u5e94, \u540e\u95e8\u9632\u5fa1"}}
{"id": "2506.00307", "pdf": "https://arxiv.org/pdf/2506.00307", "abs": "https://arxiv.org/abs/2506.00307", "authors": ["John Harvill", "Ziwei Fan", "Hao Wang", "Yizhou Sun", "Hao Ding", "Luke Huan", "Anoop Deoras"], "title": "Lossless Token Sequence Compression via Meta-Tokens", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 8 figures", "summary": "Existing work on prompt compression for Large Language Models (LLM) focuses\non lossy methods that try to maximize the retention of semantic information\nthat is relevant to downstream tasks while significantly reducing the sequence\nlength. In this paper, we introduce a task-agnostic lossless compression\ntechnique similar to LZ77 that makes it possible to reduce the input token\nsequence length on average by 27\\% and 18\\% for the two evaluation tasks\nexplored here. Given that we use transformer-based LLMs, this equates to 47\\%\nand 33\\% less encoding computation, respectively, due to the quadratic nature\nof attention. The token sequence transformation is trivial to reverse and\nhighlights that no semantic information is lost in the process. We evaluate our\nproposed approach on two tasks that require strict preservation of\nsemantics/syntax and demonstrate that existing lossy compression methods\nperform poorly in this setting. We find that our lossless compression technique\nproduces only a small gap in performance compared to using the uncompressed\ninput and posit that larger models and an expanded computing budget would\nlikely erase the gap entirely.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u65e0\u5173\u7684\u65e0\u635f\u538b\u7f29\u6280\u672f\uff0c\u7c7b\u4f3cLZ77\uff0c\u5e73\u5747\u51cf\u5c11\u8f93\u5165token\u5e8f\u5217\u957f\u5ea627%\u548c18%\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u4e14\u96f6\u8bed\u4e49\u635f\u5931\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u63d0\u793a\u8bcd\u538b\u7f29\u65b9\u6cd5\u591a\u4e3a\u6709\u635f\u538b\u7f29\uff0c\u800c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u635f\u538b\u7f29\u65b9\u6cd5\uff0c\u4ee5\u4e25\u683c\u4fdd\u7559\u8bed\u4e49/\u8bed\u6cd5\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u7c7b\u4f3cLZ77\u7684\u65e0\u635f\u538b\u7f29\u6280\u672f\uff0c\u901a\u8fc7token\u5e8f\u5217\u53d8\u6362\u51cf\u5c11\u8f93\u5165\u957f\u5ea6\uff0c\u4e14\u53d8\u6362\u53ef\u9006\u3002", "result": "\u5728\u9700\u8981\u4e25\u683c\u8bed\u4e49\u4fdd\u7559\u7684\u4efb\u52a1\u4e2d\uff0c\u538b\u7f29\u540e\u5e73\u5747\u51cf\u5c11token\u5e8f\u5217\u957f\u5ea618-27%\uff0c\u76f8\u5f53\u4e8e\u51cf\u5c1133-47%\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u4e14\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002", "conclusion": "\u65e0\u635f\u538b\u7f29\u5728\u4fdd\u7559\u8bed\u4e49\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u672a\u6765\u66f4\u5927\u6a21\u578b\u6216\u8ba1\u7b97\u8d44\u6e90\u53ef\u80fd\u5b8c\u5168\u6d88\u9664\u6027\u80fd\u5dee\u8ddd\u3002", "keywords": "LLM, \u63d0\u793a\u8bcd\u538b\u7f29, \u65e0\u635f\u538b\u7f29, \u8ba1\u7b97\u6548\u7387, \u8bed\u4e49\u4fdd\u7559"}}
{"id": "2506.00780", "pdf": "https://arxiv.org/pdf/2506.00780", "abs": "https://arxiv.org/abs/2506.00780", "authors": ["Jingyu Liu", "Jingquan Peng", "xiaopeng Wu", "Xubin Li", "Tiezheng Ge", "Bo Zheng", "Yong Liu"], "title": "Do not Abstain! Identify and Solve the Uncertainty", "categories": ["cs.AI"], "comment": null, "summary": "Despite the widespread application of Large Language Models (LLMs) across\nvarious domains, they frequently exhibit overconfidence when encountering\nuncertain scenarios, yet existing solutions primarily rely on evasive responses\n(e.g., \"I don't know\") overlooks the opportunity of identifying and addressing\nthe uncertainty to generate more satisfactory responses. To systematically\ninvestigate and improve LLMs' ability of recognizing and addressing the source\nof uncertainty, we introduce \\textbf{ConfuseBench}, a benchmark mainly focus on\nthree types of uncertainty: document scarcity, limited capability, and query\nambiguity. Experiments with ConfuseBench reveal that current LLMs struggle to\naccurately identify the root cause of uncertainty and solve it. They prefer to\nattribute uncertainty to query ambiguity while overlooking capability\nlimitations, especially for those weaker models. To tackle this challenge, we\nfirst generate context-aware inquiries that highlight the confusing aspect of\nthe original query. Then we judge the source of uncertainty based on the\nuniqueness of the inquiry's answer. Further we use an on-policy training\nmethod, InteractDPO to generate better inquiries. Experimental results\ndemonstrate the efficacy of our approach.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u9762\u5bf9\u4e0d\u786e\u5b9a\u573a\u666f\u65f6\u7684\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86ConfuseBench\u57fa\u51c6\u4ee5\u7cfb\u7edf\u6027\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u8bc6\u522b\u4e0e\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86InteractDPO\u65b9\u6cd5\u6765\u63d0\u5347\u6548\u679c\u3002", "motivation": "LLM\u5728\u5904\u7406\u4e0d\u786e\u5b9a\u573a\u666f\u65f6\u5e38\u8868\u73b0\u51fa\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4e3b\u8981\u4f9d\u8d56\u56de\u907f\u56de\u5e94\uff0c\u5ffd\u7565\u4e86\u8bc6\u522b\u548c\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\u7684\u673a\u4f1a\u3002", "method": "\u5f15\u5165ConfuseBench\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u6587\u6863\u7a00\u7f3a\u6027\u3001\u80fd\u529b\u9650\u5236\u548c\u67e5\u8be2\u6b67\u4e49\u4e09\u79cd\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u63d0\u51faInteractDPO\u65b9\u6cd5\u6765\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8d28\u7591\u4e0e\u5224\u65ad\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524dLLM\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\u4e0d\u786e\u5b9a\u6027\u6839\u6e90\uff0c\u5c24\u5176\u662f\u8f83\u5f31\u6a21\u578b\u66f4\u503e\u5411\u4e8e\u5f52\u56e0\u4e8e\u67e5\u8be2\u6b67\u4e49\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u6539\u5584\u4e86\u8fd9\u4e00\u70b9\u3002", "conclusion": "\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u6027\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u8bc6\u522b\u4e0e\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\u7684\u80fd\u529b\uff0c\u5c55\u793a\u4e86InteractDPO\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff1b\u4e0d\u786e\u5b9a\u6027\uff1bConfuseBench\uff1bInteractDPO\uff1b\u80fd\u529b\u9650\u5236"}}
{"id": "2506.00384", "pdf": "https://arxiv.org/pdf/2506.00384", "abs": "https://arxiv.org/abs/2506.00384", "authors": ["Yutong Huang", "Zhiyuan Guo", "Yiying Zhang"], "title": "Deep-Learning-Driven Prefetching for Far Memory", "categories": ["cs.LG", "cs.DC", "cs.OS"], "comment": null, "summary": "Modern software systems face increasing runtime performance demands,\nparticularly in emerging architectures like far memory, where local-memory\nmisses incur significant latency. While machine learning (ML) has proven\neffective in offline systems optimization, its application to high-frequency,\nruntime-level problems remains limited due to strict performance,\ngeneralization, and integration constraints. We present FarSight, a Linux-based\nfar-memory system that leverages deep learning (DL) to efficiently perform\naccurate data prefetching. FarSight separates application semantics from\nruntime memory layout, allowing offline-trained DL models to predict access\npatterns using a compact vocabulary of ordinal possibilities, resolved at\nruntime through lightweight mapping structures. By combining asynchronous\ninference, lookahead prediction, and a cache-resident DL model, FarSight\nachieves high prediction accuracy with low runtime overhead. Our evaluation of\nFarSight on four data-intensive workloads shows that it outperforms the\nstate-of-the-art far-memory system by up to 3.6 times. Overall, this work\ndemonstrates the feasibility and advantages of applying modern ML techniques to\ncomplex, performance-critical software runtime problems.", "AI": {"tldr": "FarSight\u662f\u4e00\u4e2a\u57fa\u4e8eLinux\u7684\u8fdc\u5185\u5b58\u7cfb\u7edf\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u9884\u53d6\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u5728\u9ad8\u6027\u80fd\u9700\u6c42\u4e0b\uff0c\u7279\u522b\u662f\u5728\u8fdc\u5185\u5b58\u67b6\u6784\u4e2d\uff0c\u8fd0\u884c\u65f6\u6027\u80fd\u4f18\u5316\u9762\u4e34\u6311\u6218\u3002\u867d\u7136\u673a\u5668\u5b66\u4e60\u5728\u79bb\u7ebf\u7cfb\u7edf\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9ad8\u9891\u8fd0\u884c\u65f6\u95ee\u9898\u4e2d\u5e94\u7528\u53d7\u9650\u3002", "method": "FarSight\u901a\u8fc7\u5c06\u5e94\u7528\u8bed\u4e49\u4e0e\u8fd0\u884c\u65f6\u5185\u5b58\u5e03\u5c40\u5206\u79bb\uff0c\u4f7f\u7528\u79bb\u7ebf\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u8bbf\u95ee\u6a21\u5f0f\uff0c\u5e76\u7ed3\u5408\u5f02\u6b65\u63a8\u7406\u3001\u524d\u77bb\u9884\u6d4b\u548c\u7f13\u5b58\u9a7b\u7559\u6a21\u578b\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\uff0cFarSight\u6bd4\u73b0\u6709\u8fdc\u5185\u5b58\u7cfb\u7edf\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe3.6\u500d\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u6280\u672f\u53ef\u6210\u529f\u5e94\u7528\u4e8e\u590d\u6742\u3001\u9ad8\u6027\u80fd\u5173\u952e\u7684\u8f6f\u4ef6\u8fd0\u884c\u65f6\u95ee\u9898\u3002", "keywords": "\u8fdc\u5185\u5b58,\u6df1\u5ea6\u5b66\u4e60,\u6570\u636e\u9884\u53d6,\u8fd0\u884c\u65f6\u4f18\u5316"}}
{"id": "2506.00312", "pdf": "https://arxiv.org/pdf/2506.00312", "abs": "https://arxiv.org/abs/2506.00312", "authors": ["Brendan Sands", "Yining Wang", "Chenhao Xu", "Yuxuan Zhou", "Lai Wei", "Rohitash Chandra"], "title": "An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and DeepSeek-V3", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have been prominent in various tasks, including\ntext generation and summarisation. The applicability of LLMs to the generation\nof product reviews is gaining momentum, paving the way for the generation of\nmovie reviews. In this study, we propose a framework that generates movie\nreviews using three LLMs (GPT-4o, DeepSeek-V3, and Gemini-2.0), and evaluate\ntheir performance by comparing the generated outputs with IMDb user reviews. We\nuse movie subtitles and screenplays as input to the LLMs and investigate how\nthey affect the quality of reviews generated. We review the LLM-based movie\nreviews in terms of vocabulary, sentiment polarity, similarity, and thematic\nconsistency in comparison to IMDB user reviews. The results demonstrate that\nLLMs are capable of generating syntactically fluent and structurally complete\nmovie reviews. Nevertheless, there is still a noticeable gap in emotional\nrichness and stylistic coherence between LLM-generated and IMDb reviews,\nsuggesting that further refinement is needed to improve the overall quality of\nmovie review generation. We provided a survey-based analysis where participants\nwere told to distinguish between LLM and IMDb user reviews. The results show\nthat LLM-generated reviews are difficult to distinguish from IMDB user reviews.\nWe found that DeepSeek-V3 produced the most balanced reviews, closely matching\nIMDb reviews. GPT-4o overemphasised positive emotions, while Gemini-2.0\ncaptured negative emotions better but showed excessive emotional intensity.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528GPT-4o\u3001DeepSeek-V3\u548cGemini-2.0\u4e09\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7535\u5f71\u8bc4\u8bba\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u4e0eIMDb\u7528\u6237\u8bc4\u8bba\u6bd4\u8f83\u8bc4\u4f30\u5176\u6027\u80fd\u3002\u7ed3\u679c\u663e\u793a\uff0c\u867d\u7136LLM\u751f\u6210\u7684\u8bc4\u8bba\u5728\u8bed\u6cd5\u548c\u7ed3\u6784\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u60c5\u611f\u4e30\u5bcc\u6027\u548c\u98ce\u683c\u4e00\u81f4\u6027\u4e0a\u4ecd\u6709\u5dee\u8ddd\u3002DeepSeek-V3\u8868\u73b0\u6700\u5e73\u8861\uff0c\u63a5\u8fd1IMDb\u8bc4\u8bba\u3002", "motivation": "\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7535\u5f71\u8bc4\u8bba\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u4e0e\u771f\u5b9e\u7528\u6237\u8bc4\u8bba\u8fdb\u884c\u5bf9\u6bd4\uff0c\u4ee5\u8bc4\u4f30\u5176\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u7535\u5f71\u5b57\u5e55\u548c\u5267\u672c\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u4e09\u79cdLLM\u751f\u6210\u8bc4\u8bba\uff0c\u5e76\u4ece\u8bcd\u6c47\u3001\u60c5\u611f\u6781\u6027\u3001\u76f8\u4f3c\u6027\u548c\u4e3b\u9898\u4e00\u81f4\u6027\u7b49\u65b9\u9762\u4e0eIMDb\u8bc4\u8bba\u5bf9\u6bd4\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u53c2\u4e0e\u8005\u533a\u5206\u6d4b\u8bd5\u3002", "result": "LLM\u80fd\u751f\u6210\u8bed\u6cd5\u6d41\u7545\u4e14\u7ed3\u6784\u5b8c\u6574\u7684\u8bc4\u8bba\uff0c\u4f46\u60c5\u611f\u4e30\u5bcc\u6027\u548c\u98ce\u683c\u4e00\u81f4\u6027\u4e0d\u8db3\u3002DeepSeek-V3\u8868\u73b0\u6700\u63a5\u8fd1IMDb\u8bc4\u8bba\uff0cGPT-4o\u504f\u5411\u79ef\u6781\u60c5\u611f\uff0cGemini-2.0\u64c5\u957f\u6355\u6349\u6d88\u6781\u60c5\u611f\u4f46\u60c5\u611f\u5f3a\u5ea6\u8fc7\u9ad8\u3002", "conclusion": "LLM\u5728\u7535\u5f71\u8bc4\u8bba\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u5347\u60c5\u611f\u548c\u98ce\u683c\u7684\u5339\u914d\u5ea6\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u7535\u5f71\u8bc4\u8bba\u751f\u6210, \u60c5\u611f\u5206\u6790, IMDb\u8bc4\u8bba, \u6587\u672c\u751f\u6210"}}
{"id": "2506.00781", "pdf": "https://arxiv.org/pdf/2506.00781", "abs": "https://arxiv.org/abs/2506.00781", "authors": ["Chen Xiong", "Pin-Yu Chen", "Tsung-Yi Ho"], "title": "CoP: Agentic Red-teaming for Large Language Models using Composition of Principles", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have spurred transformative\napplications in various domains, ranging from open-source to proprietary LLMs.\nHowever, jailbreak attacks, which aim to break safety alignment and user\ncompliance by tricking the target LLMs into answering harmful and risky\nresponses, are becoming an urgent concern. The practice of red-teaming for LLMs\nis to proactively explore potential risks and error-prone instances before the\nrelease of frontier AI technology. This paper proposes an agentic workflow to\nautomate and scale the red-teaming process of LLMs through the\nComposition-of-Principles (CoP) framework, where human users provide a set of\nred-teaming principles as instructions to an AI agent to automatically\norchestrate effective red-teaming strategies and generate jailbreak prompts.\nDistinct from existing red-teaming methods, our CoP framework provides a\nunified and extensible framework to encompass and orchestrate human-provided\nred-teaming principles to enable the automated discovery of new red-teaming\nstrategies. When tested against leading LLMs, CoP reveals unprecedented safety\nrisks by finding novel jailbreak prompts and improving the best-known\nsingle-turn attack success rate by up to 19.0 times.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eCoP\u6846\u67b6\u7684\u81ea\u52a8\u5316\u7ea2\u961f\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u53d1\u73b0LLM\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7ea2\u961f\u6d4b\u8bd5\u65b9\u6cd5\u65e0\u6cd5\u7edf\u4e00\u548c\u6269\u5c55\u7684\u95ee\u9898\uff0c\u4ee5\u81ea\u52a8\u5316\u548c\u89c4\u6a21\u5316\u5730\u53d1\u73b0LLM\u7684\u5b89\u5168\u98ce\u9669\u3002", "method": "\u4f7f\u7528Composition-of-Principles (CoP)\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u7c7b\u63d0\u4f9b\u7684\u539f\u5219\u6307\u5bfcAI\u4ee3\u7406\u81ea\u52a8\u751f\u6210\u7ea2\u961f\u6d4b\u8bd5\u7b56\u7565\u548c\u8d8a\u72f1\u63d0\u793a\u3002", "result": "\u5728\u9886\u5148\u7684LLM\u4e0a\u6d4b\u8bd5\uff0cCoP\u53d1\u73b0\u4e86\u65b0\u7684\u8d8a\u72f1\u63d0\u793a\uff0c\u5e76\u5c06\u5355\u6b21\u653b\u51fb\u6210\u529f\u7387\u63d0\u9ad8\u4e8619\u500d\u3002", "conclusion": "CoP\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u53d1\u73b0LLM\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u4e3a\u7ea2\u961f\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u7ea2\u961f\u6d4b\u8bd5,\u5b89\u5168\u5bf9\u9f50,\u8d8a\u72f1\u653b\u51fb,CoP\u6846\u67b6"}}
{"id": "2506.00388", "pdf": "https://arxiv.org/pdf/2506.00388", "abs": "https://arxiv.org/abs/2506.00388", "authors": ["Ni Mu", "Hao Hu", "Xiao Hu", "Yiqin Yang", "Bo Xu", "Qing-Shan Jia"], "title": "CLARIFY: Contrastive Preference Reinforcement Learning for Untangling Ambiguous Queries", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Preference-based reinforcement learning (PbRL) bypasses explicit reward\nengineering by inferring reward functions from human preference comparisons,\nenabling better alignment with human intentions. However, humans often struggle\nto label a clear preference between similar segments, reducing label efficiency\nand limiting PbRL's real-world applicability. To address this, we propose an\noffline PbRL method: Contrastive LeArning for ResolvIng Ambiguous Feedback\n(CLARIFY), which learns a trajectory embedding space that incorporates\npreference information, ensuring clearly distinguished segments are spaced\napart, thus facilitating the selection of more unambiguous queries. Extensive\nexperiments demonstrate that CLARIFY outperforms baselines in both non-ideal\nteachers and real human feedback settings. Our approach not only selects more\ndistinguished queries but also learns meaningful trajectory embeddings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCLARIFY\u7684\u79bb\u7ebf\u504f\u597d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u89e3\u51b3\u4eba\u7c7b\u504f\u597d\u53cd\u9988\u4e2d\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u504f\u597d\u6807\u6ce8\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u504f\u597d\u5f3a\u5316\u5b66\u4e60\uff08PbRL\uff09\u4f9d\u8d56\u4eba\u7c7b\u504f\u597d\u6bd4\u8f83\u63a8\u65ad\u5956\u52b1\u51fd\u6570\uff0c\u4f46\u5728\u9762\u5bf9\u76f8\u4f3c\u7247\u6bb5\u65f6\uff0c\u4eba\u7c7b\u96be\u4ee5\u660e\u786e\u6807\u6ce8\u504f\u597d\uff0c\u5bfc\u81f4\u6807\u7b7e\u6548\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86CLARIFY\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u878d\u5165\u504f\u597d\u4fe1\u606f\u7684\u8f68\u8ff9\u5d4c\u5165\u7a7a\u95f4\uff0c\u4f7f\u660e\u663e\u4e0d\u540c\u7684\u7247\u6bb5\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u76f8\u8ddd\u66f4\u8fdc\uff0c\u4ece\u800c\u9009\u62e9\u66f4\u660e\u786e\u7684\u67e5\u8be2\u3002", "result": "\u5728\u975e\u7406\u60f3\u6559\u5e08\u548c\u771f\u5b9e\u4eba\u7c7b\u53cd\u9988\u573a\u666f\u4e0b\uff0cCLARIFY\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u9009\u62e9\u66f4\u6613\u533a\u5206\u7684\u67e5\u8be2\u5e76\u5b66\u4e60\u6709\u610f\u4e49\u7684\u8f68\u8ff9\u5d4c\u5165\u3002", "conclusion": "CLARIFY\u6709\u6548\u63d0\u5347\u4e86\u504f\u597d\u5f3a\u5316\u5b66\u4e60\u7684\u6807\u7b7e\u6548\u7387\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u89e3\u51b3\u4eba\u7c7b\u504f\u597d\u6a21\u7cca\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u504f\u597d\u5f3a\u5316\u5b66\u4e60, \u5bf9\u6bd4\u5b66\u4e60, \u8f68\u8ff9\u5d4c\u5165, \u6a21\u7cca\u53cd\u9988, \u79bb\u7ebf\u5b66\u4e60"}}
{"id": "2506.00319", "pdf": "https://arxiv.org/pdf/2506.00319", "abs": "https://arxiv.org/abs/2506.00319", "authors": ["Yufei Tian", "Jiao Sun", "Nanyun Peng", "Zizhao Zhang"], "title": "SkillVerse : Assessing and Enhancing LLMs with Tree Evaluation", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "As language models evolve to tackle complex, multifaceted tasks, their\nevaluation must adapt to capture this intricacy. A granular, skill-specific\nunderstanding of model capabilities can empower researchers to make informed\nmodel development plans. In this paper, we introduce SkillVerse, an\nunsupervised tree-structured diagnosis framework for understanding model\nproficiency in specific abilities. With LLM as a judge, SkillVerse first\ncritiques the model responses, and then organizes them into a hierarchical\nstructure termed dendrogram. Given proficiency at arbitrary levels of\ngranularity, SkillVerse is flexible to produce insights of behaviors of modern\nlarge models. We also demonstrate its efficacy in two downstream tasks: 1)\nimproving model in-context learning by 25% using a tree-search algorithm to\nselect more informative few-shot demonstrations, and 2) accurately predicting\nnew model weaknesses with a 55% success rate, 22% higher than without\nSkillVerse.", "AI": {"tldr": "SkillVerse\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u6811\u72b6\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u6280\u80fd\u4e0a\u7684\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u5904\u7406\u590d\u6742\u4efb\u52a1\u80fd\u529b\u7684\u589e\u5f3a\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u7406\u89e3\u6a21\u578b\u7684\u5177\u4f53\u80fd\u529b\u3002", "method": "\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\uff0cSkillVerse\u9996\u5148\u5bf9\u6a21\u578b\u54cd\u5e94\u8fdb\u884c\u6279\u8bc4\uff0c\u7136\u540e\u5c06\u5176\u7ec4\u7ec7\u4e3a\u6811\u72b6\u7ed3\u6784\uff08dendrogram\uff09\u3002", "result": "SkillVerse\u63d0\u5347\u4e86\u6a21\u578b\u7684\u60c5\u5883\u5b66\u4e60\u80fd\u529b25%\uff0c\u5e76\u6210\u529f\u9884\u6d4b\u6a21\u578b\u5f31\u70b9\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e8622%\u3002", "conclusion": "SkillVerse\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u6709\u6548\u7684\u5de5\u5177\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b\u3001\u8bc4\u4f30\u6846\u67b6\u3001SkillVerse\u3001\u6811\u72b6\u7ed3\u6784\u3001\u60c5\u5883\u5b66\u4e60"}}
{"id": "2506.00782", "pdf": "https://arxiv.org/pdf/2506.00782", "abs": "https://arxiv.org/abs/2506.00782", "authors": ["Weiyang Guo", "Zesheng Shi", "Zhuo Li", "Yequan Wang", "Xuebo Liu", "Wenya Wang", "Fangming Liu", "Min Zhang", "Jing Li"], "title": "Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via Reinforcement Learning", "categories": ["cs.AI"], "comment": "21 pages, 8 figures", "summary": "As large language models (LLMs) grow in power and influence, ensuring their\nsafety and preventing harmful output becomes critical. Automated red teaming\nserves as a tool to detect security vulnerabilities in LLMs without manual\nlabor. However, most existing methods struggle to balance the effectiveness and\ndiversity of red-team generated attack prompts. To address this challenge, we\npropose \\ourapproach, a novel automated red teaming training framework that\nutilizes reinforcement learning to explore and generate more effective attack\nprompts while balancing their diversity. Specifically, it consists of three\ntraining stages: (1) Cold Start: The red team model is supervised and\nfine-tuned on a jailbreak dataset obtained through imitation learning. (2)\nWarm-up Exploration: The model is trained in jailbreak instruction following\nand exploration, using diversity and consistency as reward signals. (3)\nEnhanced Jailbreak: Progressive jailbreak rewards are introduced to gradually\nenhance the jailbreak performance of the red-team model. Extensive experiments\non a variety of LLMs show that \\ourapproach effectively balances the diversity\nand effectiveness of jailbreak prompts compared to existing methods. Our work\nsignificantly improves the efficiency of red team exploration and provides a\nnew perspective on automated red teaming.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u52a8\u7ea2\u961f\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u591a\u6837\u5316\u4e14\u6709\u6548\u7684\u653b\u51fb\u63d0\u793a\uff0c\u4ee5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5f71\u54cd\u529b\u548c\u80fd\u529b\u589e\u957f\uff0c\u786e\u4fdd\u5176\u5b89\u5168\u6027\u548c\u9632\u6b62\u6709\u5bb3\u8f93\u51fa\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u653b\u51fb\u63d0\u793a\u7684\u6709\u6548\u6027\u548c\u591a\u6837\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u51b7\u542f\u52a8\uff08\u76d1\u7763\u5b66\u4e60\uff09\u3001\u70ed\u8eab\u63a2\u7d22\uff08\u4f7f\u7528\u591a\u6837\u6027\u548c\u4e00\u81f4\u6027\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff09\u3001\u5f3a\u5316\u8d8a\u72f1\uff08\u6e10\u8fdb\u5f0f\u5956\u52b1\u63d0\u5347\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u6027\u548c\u6709\u6548\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ea2\u961f\u63a2\u7d22\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u7ea2\u961f\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6709\u6548\u5e73\u8861\u4e86\u653b\u51fb\u63d0\u793a\u7684\u591a\u6837\u6027\u548c\u6548\u679c\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u81ea\u52a8\u7ea2\u961f, \u5f3a\u5316\u5b66\u4e60, \u653b\u51fb\u63d0\u793a, \u591a\u6837\u6027, \u6709\u6548\u6027"}}
{"id": "2506.00407", "pdf": "https://arxiv.org/pdf/2506.00407", "abs": "https://arxiv.org/abs/2506.00407", "authors": ["Ruixuan Chen", "Wentao Li", "Jiahui Xiao", "Yuchen Li", "Yimin Tang", "Xiaonan Wang"], "title": "Bias as a Virtue: Rethinking Generalization under Distribution Shifts", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "14 pages", "summary": "Machine learning models often degrade when deployed on data distributions\ndifferent from their training data. Challenging conventional validation\nparadigms, we demonstrate that higher in-distribution (ID) bias can lead to\nbetter out-of-distribution (OOD) generalization. Our Adaptive Distribution\nBridge (ADB) framework implements this insight by introducing controlled\nstatistical diversity during training, enabling models to develop bias profiles\nthat effectively generalize across distributions. Empirically, we observe a\nrobust negative correlation where higher ID bias corresponds to lower OOD\nerror--a finding that contradicts standard practices focused on minimizing\nvalidation error. Evaluation on multiple datasets shows our approach\nsignificantly improves OOD generalization. ADB achieves robust mean error\nreductions of up to 26.8% compared to traditional cross-validation, and\nconsistently identifies high-performing training strategies, evidenced by\npercentile ranks often exceeding 74.4%. Our work provides both a practical\nmethod for improving generalization and a theoretical framework for\nreconsidering the role of bias in robust machine learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdaptive Distribution Bridge (ADB)\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u63a7\u5236\u6027\u7684\u7edf\u8ba1\u591a\u6837\u6027\u6765\u63d0\u5347\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u66f4\u9ad8\u7684\u5206\u5e03\u5185\u504f\u5dee\uff08ID bias\uff09\u53cd\u800c\u53ef\u4ee5\u964d\u4f4e\u5206\u5e03\u5916\u9519\u8bef\uff08OOD error\uff09\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\u3002\u5b9e\u9a8c\u663e\u793aADB\u5c06OOD\u9519\u8bef\u964d\u4f4e\u4e8626.8%\uff0c\u4e14\u5176\u8bad\u7ec3\u7b56\u7565\u7684\u6027\u80fd\u767e\u5206\u4f4d\u6570\u5e38\u8d85\u8fc774.4%\u3002", "motivation": "\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u6027\u80fd\u4e0b\u964d\u660e\u663e\uff0c\u4e14\u5e38\u89c4\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff08\u5982\u6700\u5c0f\u5316\u9a8c\u8bc1\u8bef\u5dee\uff09\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u91cd\u65b0\u8003\u8651\u504f\u5dee\u5728\u6cdb\u5316\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAdaptive Distribution Bridge (ADB)\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u5f15\u5165\u63a7\u5236\u6027\u7684\u7edf\u8ba1\u591a\u6837\u6027\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5f62\u6210\u6709\u6548\u7684\u504f\u5dee\u914d\u7f6e\uff0c\u4ece\u800c\u63d0\u5347\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cADB\u663e\u8457\u63d0\u5347\u4e86OOD\u6cdb\u5316\u80fd\u529b\uff0c\u5e73\u5747\u9519\u8bef\u7387\u964d\u4f4e\u4e8626.8%\uff0c\u4e14\u5176\u8bad\u7ec3\u7b56\u7565\u7684\u6027\u80fd\u767e\u5206\u4f4d\u6570\u5e38\u8d85\u8fc774.4%\u3002", "conclusion": "ADB\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u4e00\u79cd\u63d0\u5347\u673a\u5668\u5b66\u4e60\u6cdb\u5316\u80fd\u529b\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u91cd\u65b0\u601d\u8003\u504f\u5dee\u5728\u9c81\u68d2\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\u3002", "keywords": "\u673a\u5668\u5b66\u4e60, \u5206\u5e03\u5916\u6cdb\u5316, \u504f\u5dee, \u81ea\u9002\u5e94\u5206\u5e03\u6865, \u7edf\u8ba1\u591a\u6837\u6027"}}
{"id": "2506.00331", "pdf": "https://arxiv.org/pdf/2506.00331", "abs": "https://arxiv.org/abs/2506.00331", "authors": ["Boyi Zhang", "Zhuo Liu", "Hangfeng He"], "title": "TreeRare: Syntax Tree-Guided Retrieval and Reasoning for Knowledge-Intensive Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "In real practice, questions are typically complex and knowledge-intensive,\nrequiring Large Language Models (LLMs) to recognize the multifaceted nature of\nthe question and reason across multiple information sources. Iterative and\nadaptive retrieval, where LLMs decide when and what to retrieve based on their\nreasoning, has been shown to be a promising approach to resolve complex,\nknowledge-intensive questions. However, the performance of such retrieval\nframeworks is limited by the accumulation of reasoning errors and misaligned\nretrieval results. To overcome these limitations, we propose TreeRare (Syntax\nTree-Guided Retrieval and Reasoning), a framework that utilizes syntax trees to\nguide information retrieval and reasoning for question answering. Following the\nprinciple of compositionality, TreeRare traverses the syntax tree in a\nbottom-up fashion, and in each node, it generates subcomponent-based queries\nand retrieves relevant passages to resolve localized uncertainty. A\nsubcomponent question answering module then synthesizes these passages into\nconcise, context-aware evidence. Finally, TreeRare aggregates the evidence\nacross the tree to form a final answer. Experiments across five question\nanswering datasets involving ambiguous or multi-hop reasoning demonstrate that\nTreeRare achieves substantial improvements over existing state-of-the-art\nmethods.", "AI": {"tldr": "TreeRare \u662f\u4e00\u79cd\u57fa\u4e8e\u8bed\u6cd5\u6811\u7684\u68c0\u7d22\u4e0e\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u5e95\u5411\u4e0a\u904d\u5386\u8bed\u6cd5\u6811\u89e3\u51b3\u590d\u6742\u6027\u77e5\u8bc6\u5bc6\u96c6\u578b\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u590d\u6742\u95ee\u9898\u9700\u8981 LLMs \u8de8\u591a\u6e90\u63a8\u7406\uff0c\u4f46\u73b0\u6709\u68c0\u7d22\u6846\u67b6\u56e0\u63a8\u7406\u9519\u8bef\u548c\u7ed3\u679c\u4e0d\u5bf9\u9f50\u800c\u53d7\u9650\u3002", "method": "TreeRare \u5229\u7528\u8bed\u6cd5\u6811\u751f\u6210\u5b50\u95ee\u9898\u67e5\u8be2\uff0c\u68c0\u7d22\u76f8\u5173\u6bb5\u843d\u5e76\u5408\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u8bc1\u636e\uff0c\u6700\u7ec8\u805a\u5408\u7b54\u6848\u3002", "result": "\u5728\u4e94\u4e2a\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\uff0cTreeRare \u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "TreeRare \u901a\u8fc7\u8bed\u6cd5\u6811\u5f15\u5bfc\u7684\u68c0\u7d22\u4e0e\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u95ee\u9898\u7684\u6311\u6218\u3002", "keywords": "TreeRare, \u8bed\u6cd5\u6811, \u68c0\u7d22\u63a8\u7406, \u95ee\u7b54\u7cfb\u7edf, LLMs"}}
{"id": "2506.00785", "pdf": "https://arxiv.org/pdf/2506.00785", "abs": "https://arxiv.org/abs/2506.00785", "authors": ["Sahiti Yerramilli", "Nilay Pande", "Rynaa Grover", "Jayant Sravan Tamarapalli"], "title": "GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper introduces GeoChain, a large-scale benchmark for evaluating\nstep-by-step geographic reasoning in multimodal large language models (MLLMs).\nLeveraging 1.46 million Mapillary street-level images, GeoChain pairs each\nimage with a 21-step chain-of-thought (CoT) question sequence (over 30 million\nQ&A pairs). These sequences guide models from coarse attributes to fine-grained\nlocalization across four reasoning categories - visual, spatial, cultural, and\nprecise geolocation - annotated by difficulty. Images are also enriched with\nsemantic segmentation (150 classes) and a visual locatability score. Our\nbenchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5\nvariants) on a diverse 2,088-image subset reveals consistent challenges: models\nfrequently exhibit weaknesses in visual grounding, display erratic reasoning,\nand struggle to achieve accurate localization, especially as the reasoning\ncomplexity escalates. GeoChain offers a robust diagnostic methodology, critical\nfor fostering significant advancements in complex geographic reasoning within\nMLLMs.", "AI": {"tldr": "GeoChain\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5730\u7406\u63a8\u7406\u80fd\u529b\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u5305\u542b146\u4e07\u5f20\u8857\u666f\u56fe\u50cf\u548c\u8d85\u8fc73000\u4e07\u95ee\u7b54\u5bf9\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u89c6\u89c9\u5b9a\u4f4d\u548c\u590d\u6742\u63a8\u7406\u4e0a\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5730\u7406\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0cGeoChain\u65e8\u5728\u901a\u8fc7\u5927\u89c4\u6a21\u57fa\u51c6\u548c\u5206\u6b65\u95ee\u9898\u5e8f\u5217\u8bca\u65ad\u6a21\u578b\u7684\u5f31\u70b9\u3002", "method": "\u5229\u7528146\u4e07\u5f20Mapillary\u8857\u666f\u56fe\u50cf\uff0c\u6bcf\u5f20\u56fe\u50cf\u914d\u4ee521\u6b65\u94fe\u5f0f\u601d\u8003\u95ee\u9898\u5e8f\u5217\uff0c\u6db5\u76d6\u89c6\u89c9\u3001\u7a7a\u95f4\u3001\u6587\u5316\u548c\u7cbe\u786e\u5b9a\u4f4d\u56db\u7c7b\u63a8\u7406\uff0c\u5e76\u6807\u6ce8\u96be\u5ea6\u3002", "result": "\u6d4b\u8bd5\u8868\u660e\uff0c\u73b0\u6709\u6a21\u578b\uff08\u5982GPT-4.1\u3001Claude 3.7\u7b49\uff09\u5728\u89c6\u89c9\u57fa\u7840\u3001\u8fde\u8d2f\u63a8\u7406\u548c\u7cbe\u786e\u5b9a\u4f4d\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u590d\u6742\u5ea6\u9ad8\u65f6\u3002", "conclusion": "GeoChain\u4e3a\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5730\u7406\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u8bca\u65ad\u5de5\u5177\u3002", "keywords": "GeoChain\u3001\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u5730\u7406\u63a8\u7406\u3001\u57fa\u51c6\u6d4b\u8bd5\u3001\u89c6\u89c9\u5b9a\u4f4d"}}
{"id": "2506.00410", "pdf": "https://arxiv.org/pdf/2506.00410", "abs": "https://arxiv.org/abs/2506.00410", "authors": ["Ziwen Wang"], "title": "JojoSCL: Shrinkage Contrastive Learning for single-cell RNA sequence Clustering", "categories": ["cs.LG", "q-bio.GN", "stat.ML"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) has revolutionized our understanding\nof cellular processes by enabling gene expression analysis at the individual\ncell level. Clustering allows for the identification of cell types and the\nfurther discovery of intrinsic patterns in single-cell data. However, the high\ndimensionality and sparsity of scRNA-seq data continue to challenge existing\nclustering models. In this paper, we introduce JojoSCL, a novel self-supervised\ncontrastive learning framework for scRNA-seq clustering. By incorporating a\nshrinkage estimator based on hierarchical Bayesian estimation, which adjusts\ngene expression estimates towards more reliable cluster centroids to reduce\nintra-cluster dispersion, and optimized using Stein's Unbiased Risk Estimate\n(SURE), JojoSCL refines both instance-level and cluster-level contrastive\nlearning. Experiments on ten scRNA-seq datasets substantiate that JojoSCL\nconsistently outperforms prevalent clustering methods, with further validation\nof its practicality through robustness analysis and ablation studies. JojoSCL's\ncode is available at: https://github.com/ziwenwang28/JojoSCL.", "AI": {"tldr": "JojoSCL\u662f\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3scRNA-seq\u6570\u636e\u805a\u7c7b\u4e2d\u7684\u9ad8\u7ef4\u7a00\u758f\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5206\u5c42\u8d1d\u53f6\u65af\u4f30\u8ba1\u7684\u6536\u7f29\u4f30\u8ba1\u5668\u548cStein\u65e0\u504f\u98ce\u9669\u4f30\u8ba1\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u6027\u80fd\u3002", "motivation": "scRNA-seq\u6570\u636e\u7684\u805a\u7c7b\u5bf9\u4e8e\u7ec6\u80de\u7c7b\u578b\u8bc6\u522b\u548c\u6a21\u5f0f\u53d1\u73b0\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u9ad8\u7ef4\u7a00\u758f\u7279\u6027\u7ed9\u73b0\u6709\u805a\u7c7b\u65b9\u6cd5\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "JojoSCL\u901a\u8fc7\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5c42\u8d1d\u53f6\u65af\u4f30\u8ba1\u7684\u6536\u7f29\u4f30\u8ba1\u5668\u548cStein\u65e0\u504f\u98ce\u9669\u4f30\u8ba1\u4f18\u5316\uff0c\u51cf\u5c11\u7c07\u5185\u79bb\u6563\u5ea6\uff0c\u63d0\u5347\u805a\u7c7b\u6548\u679c\u3002", "result": "\u5728\u5341\u4e2ascRNA-seq\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cJojoSCL\u4f18\u4e8e\u73b0\u6709\u805a\u7c7b\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u9c81\u68d2\u6027\u5206\u6790\u548c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "JojoSCL\u4e3ascRNA-seq\u6570\u636e\u805a\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "scRNA-seq, \u81ea\u76d1\u7763\u5b66\u4e60, \u5bf9\u6bd4\u5b66\u4e60, \u5206\u5c42\u8d1d\u53f6\u65af\u4f30\u8ba1, \u805a\u7c7b"}}
{"id": "2506.00332", "pdf": "https://arxiv.org/pdf/2506.00332", "abs": "https://arxiv.org/abs/2506.00332", "authors": ["Svetlana Churina", "Akshat Gupta", "Insyirah Mujtahid", "Kokil Jaidka"], "title": "Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Code-mixing involves the seamless integration of linguistic elements from\nmultiple languages within a single discourse, reflecting natural multilingual\ncommunication patterns. Despite its prominence in informal interactions such as\nsocial media, chat messages and instant-messaging exchanges, there has been a\nlack of publicly available corpora that are author-labeled and suitable for\nmodeling human conversations and relationships. This study introduces the first\nlabeled and general-purpose corpus for understanding code-mixing in context\nwhile maintaining rigorous privacy and ethical standards. Our live project will\ncontinuously gather, verify, and integrate code-mixed messages into a\nstructured dataset released in JSON format, accompanied by detailed metadata\nand linguistic statistics. To date, it includes over 355,641 messages spanning\nvarious code-mixing patterns, with a primary focus on English, Mandarin, and\nother languages. We expect the Codemix Corpus to serve as a foundational\ndataset for research in computational linguistics, sociolinguistics, and NLP\napplications.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u6807\u8bb0\u5316\u3001\u901a\u7528\u76ee\u7684\u7684\u8bed\u7801\u6df7\u5408\u8bed\u6599\u5e93\uff0c\u586b\u8865\u4e86\u516c\u5f00\u53ef\u7528\u6570\u636e\u7684\u7a7a\u767d\uff0c\u652f\u6301\u8ba1\u7b97\u8bed\u8a00\u5b66\u3001\u793e\u4f1a\u8bed\u8a00\u5b66\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u3002", "motivation": "\u8bed\u7801\u6df7\u5408\u5728\u793e\u4ea4\u5a92\u4f53\u7b49\u975e\u6b63\u5f0f\u4ea4\u6d41\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4f46\u7f3a\u4e4f\u516c\u5f00\u7684\u3001\u6807\u8bb0\u5316\u7684\u8bed\u6599\u5e93\uff0c\u96be\u4ee5\u652f\u6301\u76f8\u5173\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6301\u7eed\u6536\u96c6\u3001\u9a8c\u8bc1\u548c\u6574\u5408\u8bed\u7801\u6df7\u5408\u6d88\u606f\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u6570\u636e\u96c6\uff08JSON\u683c\u5f0f\uff09\uff0c\u5e76\u9644\u5e26\u8be6\u7ec6\u5143\u6570\u636e\u548c\u8bed\u8a00\u7edf\u8ba1\u4fe1\u606f\u3002", "result": "\u76ee\u524d\u5305\u542b\u8d85\u8fc7355,641\u6761\u6d88\u606f\uff0c\u6db5\u76d6\u591a\u79cd\u8bed\u7801\u6df7\u5408\u6a21\u5f0f\uff0c\u4e3b\u8981\u5173\u6ce8\u82f1\u8bed\u3001\u666e\u901a\u8bdd\u548c\u5176\u4ed6\u8bed\u8a00\u3002", "conclusion": "Codemix\u8bed\u6599\u5e93\u5c06\u6210\u4e3a\u8ba1\u7b97\u8bed\u8a00\u5b66\u3001\u793e\u4f1a\u8bed\u8a00\u5b66\u548cNLP\u5e94\u7528\u7684\u57fa\u7840\u6570\u636e\u96c6\u3002", "keywords": "\u8bed\u7801\u6df7\u5408, \u8bed\u6599\u5e93, \u8ba1\u7b97\u8bed\u8a00\u5b66, \u793e\u4f1a\u8bed\u8a00\u5b66, NLP"}}
{"id": "2506.00794", "pdf": "https://arxiv.org/pdf/2506.00794", "abs": "https://arxiv.org/abs/2506.00794", "authors": ["Jiaxin Wen", "Chenglei Si", "Yueh-han Chen", "He He", "Shi Feng"], "title": "Predicting Empirical AI Research Outcomes with Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Many promising-looking ideas in AI research fail to deliver, but their\nvalidation takes substantial human labor and compute. Predicting an idea's\nchance of success is thus crucial for accelerating empirical AI research, a\nskill that even expert researchers can only acquire through substantial\nexperience. We build the first benchmark for this task and compare LMs with\nhuman experts. Concretely, given two research ideas (e.g., two jailbreaking\nmethods), we aim to predict which will perform better on a set of benchmarks.\nWe scrape ideas and experimental results from conference papers, yielding 1,585\nhuman-verified idea pairs published after our base model's cut-off date for\ntesting, and 6,000 pairs for training. We then develop a system that combines a\nfine-tuned GPT-4.1 with a paper retrieval agent, and we recruit 25 human\nexperts to compare with. In the NLP domain, our system beats human experts by a\nlarge margin (64.4% v.s. 48.9%). On the full test set, our system achieves 77%\naccuracy, while off-the-shelf frontier LMs like o3 perform no better than\nrandom guessing, even with the same retrieval augmentation. We verify that our\nsystem does not exploit superficial features like idea complexity through\nextensive human-written and LM-designed robustness tests. Finally, we evaluate\nour system on unpublished novel ideas, including ideas generated by an AI\nideation agent. Our system achieves 63.6% accuracy, demonstrating its potential\nas a reward model for improving idea generation models. Altogether, our results\noutline a promising new direction for LMs to accelerate empirical AI research.", "AI": {"tldr": "\u8bba\u6587\u6784\u5efa\u4e86\u9996\u4e2a\u9884\u6d4bAI\u7814\u7a76\u60f3\u6cd5\u6210\u529f\u7387\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u5408\u5fae\u8c03GPT-4.1\u4e0e\u8bba\u6587\u68c0\u7d22\u4ee3\u7406\uff0c\u7cfb\u7edf\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u3002", "motivation": "\u4e3a\u4e86\u52a0\u901fAI\u5b9e\u8bc1\u7814\u7a76\uff0c\u9884\u6d4b\u7814\u7a76\u60f3\u6cd5\u7684\u6210\u529f\u6982\u7387\u662f\u5173\u952e\uff0c\u4f46\u76ee\u524d\u9a8c\u8bc1\u8fc7\u7a0b\u8017\u8d39\u5927\u91cf\u4eba\u529b\u548c\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u901a\u8fc7\u722c\u53d6\u4f1a\u8bae\u8bba\u6587\u4e2d\u7684\u60f3\u6cd5\u548c\u5b9e\u9a8c\u7ed3\u679c\uff0c\u6784\u5efa1,585\u5bf9\u9a8c\u8bc1\u6570\u636e\u96c6\u548c6,000\u5bf9\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u5fae\u8c03GPT-4.1\u4e0e\u68c0\u7d22\u4ee3\u7406\u7684\u7cfb\u7edf\u4e0e25\u4f4d\u4eba\u7c7b\u4e13\u5bb6\u5bf9\u6bd4\u3002", "result": "\u5728NLP\u9886\u57df\uff0c\u7cfb\u7edf\u4ee564.4%\u7684\u51c6\u786e\u7387\u8fdc\u8d85\u4eba\u7c7b\u768448.9%\uff1b\u6574\u4f53\u6d4b\u8bd5\u96c6\u4e0a\u8fbe77%\u51c6\u786e\u7387\uff0c\u800c\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u4ec5\u4e0e\u968f\u673a\u731c\u6d4b\u76f8\u5f53\u3002\u7cfb\u7edf\u8fd8\u901a\u8fc7\u4e86\u9c81\u68d2\u6027\u6d4b\u8bd5\uff0c\u5e76\u5728\u672a\u53d1\u8868\u7684\u65b0\u60f3\u6cd5\u4e0a\u8868\u73b0\u4f18\u5f02\uff0863.6%\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8bed\u8a00\u6a21\u578b\u5728\u52a0\u901fAI\u5b9e\u8bc1\u7814\u7a76\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u63d0\u5347\u60f3\u6cd5\u751f\u6210\u6a21\u578b\u3002", "keywords": "AI\u7814\u7a76\u9884\u6d4b, \u8bed\u8a00\u6a21\u578b, \u57fa\u51c6\u6d4b\u8bd5, \u4eba\u7c7b\u4e13\u5bb6\u5bf9\u6bd4, \u68c0\u7d22\u589e\u5f3a"}}
{"id": "2506.00416", "pdf": "https://arxiv.org/pdf/2506.00416", "abs": "https://arxiv.org/abs/2506.00416", "authors": ["Anum Nawaz", "Muhammad Irfan", "Xianjia Yu", "Zhuo Zou", "Tomi Westerlund"], "title": "Blockchain-Enabled Privacy-Preserving Second-Order Federated Edge Learning in Personalized Healthcare", "categories": ["cs.LG", "cs.CR", "stat.ML"], "comment": null, "summary": "Federated learning (FL) has attracted increasing attention to mitigate\nsecurity and privacy challenges in traditional cloud-centric machine learning\nmodels specifically in healthcare ecosystems. FL methodologies enable the\ntraining of global models through localized policies, allowing independent\noperations at the edge clients' level. Conventional first-order FL approaches\nface several challenges in personalized model training due to heterogeneous\nnon-independent and identically distributed (non-iid) data of each edge client.\nRecently, second-order FL approaches maintain the stability and consistency of\nnon-iid datasets while improving personalized model training. This study\nproposes and develops a verifiable and auditable optimized second-order FL\nframework BFEL (blockchain-enhanced federated edge learning) based on optimized\nFedCurv for personalized healthcare systems. FedCurv incorporates information\nabout the importance of each parameter to each client's task (through Fisher\nInformation Matrix) which helps to preserve client-specific knowledge and\nreduce model drift during aggregation. Moreover, it minimizes communication\nrounds required to achieve a target precision convergence for each edge client\nwhile effectively managing personalized training on non-iid and heterogeneous\ndata. The incorporation of Ethereum-based model aggregation ensures trust,\nverifiability, and auditability while public key encryption enhances privacy\nand security. Experimental results of federated CNNs and MLPs utilizing Mnist,\nCifar-10, and PathMnist demonstrate the high efficiency and scalability of the\nproposed framework.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aBFEL\u7684\u53ef\u9a8c\u8bc1\u548c\u53ef\u5ba1\u8ba1\u7684\u4e8c\u9636\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u7597\u5065\u5eb7\u7cfb\u7edf\u4e2d\u7684\u4e2a\u6027\u5316\u6a21\u578b\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u9762\u5bf9\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7684\u5f02\u6784\u6570\u636e\u65f6\u5b58\u5728\u6a21\u578b\u8bad\u7ec3\u4e2a\u6027\u5316\u7684\u6311\u6218\u3002\u4e8c\u9636\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u4fdd\u6301\u6570\u636e\u7a33\u5b9a\u6027\u5e76\u6539\u5584\u4e2a\u6027\u5316\u8bad\u7ec3\u3002", "method": "\u57fa\u4e8e\u4f18\u5316\u7684FedCurv\u65b9\u6cd5\uff0c\u7ed3\u5408Fisher\u4fe1\u606f\u77e9\u9635\u4ee5\u4fdd\u7559\u5ba2\u6237\u7aef\u7279\u5b9a\u77e5\u8bc6\uff0c\u51cf\u5c11\u6a21\u578b\u6f02\u79fb\uff0c\u5e76\u901a\u8fc7\u533a\u5757\u94fe\u6280\u672f\u786e\u4fdd\u4fe1\u4efb\u548c\u53ef\u5ba1\u8ba1\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728Mnist\u3001Cifar-10\u548cPathMnist\u6570\u636e\u96c6\u4e0a\u5177\u6709\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "BFEL\u6846\u67b6\u901a\u8fc7\u533a\u5757\u94fe\u6280\u672f\u548c\u4e8c\u9636\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u533b\u7597\u7cfb\u7edf\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u4e2a\u6027\u5316\u8bad\u7ec3\u80fd\u529b\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60, \u4e8c\u9636\u4f18\u5316, \u533a\u5757\u94fe, \u4e2a\u6027\u5316\u533b\u7597, \u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e"}}
{"id": "2506.00334", "pdf": "https://arxiv.org/pdf/2506.00334", "abs": "https://arxiv.org/abs/2506.00334", "authors": ["Gerard Christopher Yeo", "Kokil Jaidka"], "title": "Beyond Context to Cognitive Appraisal: Emotion Reasoning as a Theory of Mind Benchmark for Large Language Models", "categories": ["cs.CL"], "comment": "9 pages, 3 figures", "summary": "Datasets used for emotion recognition tasks typically contain overt cues that\ncan be used in predicting the emotions expressed in a text. However, one\nchallenge is that texts sometimes contain covert contextual cues that are rich\nin affective semantics, which warrant higher-order reasoning abilities to infer\nemotional states, not simply the emotions conveyed. This study advances beyond\nsurface-level perceptual features to investigate how large language models\n(LLMs) reason about others' emotional states using contextual information,\nwithin a Theory-of-Mind (ToM) framework. Grounded in Cognitive Appraisal\nTheory, we curate a specialized ToM evaluation dataset1 to assess both forward\nreasoning - from context to emotion- and backward reasoning - from emotion to\ninferred context. We showed that LLMs can reason to a certain extent, although\nthey are poor at associating situational outcomes and appraisals with specific\nemotions. Our work highlights the need for psychological theories in the\ntraining and evaluation of LLMs in the context of emotion reasoning.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u5728\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u6846\u67b6\u4e0b\u901a\u8fc7\u4e0a\u4e0b\u6587\u4fe1\u606f\u63a8\u65ad\u60c5\u7eea\u72b6\u6001\uff0c\u8d85\u8d8a\u4e86\u8868\u9762\u7684\u611f\u77e5\u7279\u5f81\u3002", "motivation": "\u73b0\u6709\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u7684\u6570\u636e\u96c6\u901a\u5e38\u4f9d\u8d56\u663e\u6027\u7ebf\u7d22\uff0c\u4f46\u6587\u672c\u4e2d\u53ef\u80fd\u5b58\u5728\u9700\u8981\u9ad8\u9636\u63a8\u7406\u80fd\u529b\u624d\u80fd\u8bc6\u522b\u7684\u9690\u6027\u60c5\u611f\u8bed\u4e49\u3002", "method": "\u7814\u7a76\u57fa\u4e8e\u8ba4\u77e5\u8bc4\u4ef7\u7406\u8bba\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684ToM\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4ece\u4e0a\u4e0b\u6587\u5230\u60c5\u7eea\u7684\u524d\u5411\u63a8\u7406\u548c\u4ece\u60c5\u7eea\u5230\u4e0a\u4e0b\u6587\u7684\u540e\u5411\u63a8\u7406\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u80fd\u8fdb\u884c\u63a8\u7406\uff0c\u4f46\u5728\u5173\u8054\u60c5\u5883\u7ed3\u679c\u548c\u8bc4\u4ef7\u4e0e\u7279\u5b9a\u60c5\u7eea\u65b9\u9762\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5fc3\u7406\u5b66\u7406\u8bba\u5728LLMs\u60c5\u611f\u63a8\u7406\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e2d\u7684\u5fc5\u8981\u6027\u3002", "keywords": "\u60c5\u611f\u8bc6\u522b, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5fc3\u667a\u7406\u8bba, \u8ba4\u77e5\u8bc4\u4ef7\u7406\u8bba, \u60c5\u611f\u63a8\u7406"}}
{"id": "2506.00807", "pdf": "https://arxiv.org/pdf/2506.00807", "abs": "https://arxiv.org/abs/2506.00807", "authors": ["Jiahui Zhou", "Dan Li", "Lin Li", "Zhuomin Chen", "Shunyu Wu", "Haozheng Ye", "Jian Lou", "Costas J. Spanos"], "title": "Enhancing LLM Reasoning for Time Series Classification by Tailored Thinking and Fused Decision", "categories": ["cs.AI"], "comment": null, "summary": "The reasoning capabilities of large language models (LLMs) have significantly\nadvanced their performance by enabling in-depth understanding of diverse tasks.\nWith growing interest in applying LLMs to the time series domain, this has\nproven nontrivial, as evidenced by the limited efficacy of straightforwardly\nadapting text-domain reasoning techniques. Although recent work has shown\npromise in several time series tasks, further leveraging advancements in LLM\nreasoning remains under-explored for time series classification (TSC) tasks,\ndespite their prevalence and significance in many real-world applications. In\nthis paper, we propose ReasonTSC, a novel framework designed to effectively\nleverage LLM reasoning for time series classification through both a multi-turn\nreasoning and a fused decision-making strategy tailored to TSC. Rather than\nstraightforwardly applying existing reasoning techniques or relying solely on\nLLMs' built-in reasoning capabilities, ReasonTSC first steers the model to\nthink over the essential characteristics of time series data. Next, it\nintegrates predictions and confidence scores from plug-in classifiers, e.g.,\ndomain-specific time series models, as in-context examples. Finally, ReasonTSC\nguides the LLM through a structured reasoning process: it evaluates the initial\nassessment, backtracks to consider alternative hypotheses, and compares their\nmerits before arriving at a final classification. Extensive experiments and\nsystematic ablation studies demonstrate that ReasonTSC consistently outperforms\nboth existing time series reasoning baselines and plug-in models, and is even\ncapable of identifying and correcting plug-in models' false predictions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReasonTSC\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u63a8\u7406\u548c\u878d\u5408\u51b3\u7b56\u7b56\u7565\uff0c\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\uff08TSC\uff09\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u6587\u672c\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u65f6\u95f4\u5e8f\u5217\u9886\u57df\u7684\u76f4\u63a5\u5e94\u7528\u6548\u679c\u6709\u9650\u3002\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u666e\u904d\u4e14\u91cd\u8981\uff0c\u4f46LLM\u63a8\u7406\u5728\u6b64\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002", "method": "ReasonTSC\u9996\u5148\u8ba9LLM\u601d\u8003\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u5173\u952e\u7279\u5f81\uff0c\u7136\u540e\u6574\u5408\u63d2\u4ef6\u5206\u7c7b\u5668\u7684\u9884\u6d4b\u548c\u7f6e\u4fe1\u5ea6\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u793a\u4f8b\uff0c\u6700\u540e\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\uff08\u8bc4\u4f30\u3001\u56de\u6eaf\u548c\u6bd4\u8f83\uff09\u5f97\u51fa\u6700\u7ec8\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReasonTSC\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u548c\u63d2\u4ef6\u6a21\u578b\uff0c\u5e76\u80fd\u8bc6\u522b\u548c\u7ea0\u6b63\u63d2\u4ef6\u6a21\u578b\u7684\u9519\u8bef\u9884\u6d4b\u3002", "conclusion": "ReasonTSC\u901a\u8fc7\u7ed3\u5408LLM\u7684\u63a8\u7406\u80fd\u529b\u548c\u9886\u57df\u7279\u5b9a\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u65f6\u95f4\u5e8f\u5217\u5206\u7c7b, \u591a\u8f6e\u63a8\u7406, \u878d\u5408\u51b3\u7b56, \u7ed3\u6784\u5316\u63a8\u7406"}}
{"id": "2506.00420", "pdf": "https://arxiv.org/pdf/2506.00420", "abs": "https://arxiv.org/abs/2506.00420", "authors": ["Miao Ye", "Suxiao Wang", "Jiaguang Han", "Yong Wang", "Xiaoli Wang", "Jingxuan Wei", "Peng Wen", "Jing Cui"], "title": "A New Spatiotemporal Correlation Anomaly Detection Method that Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Detecting anomalies in the data collected by WSNs can provide crucial\nevidence for assessing the reliability and stability of WSNs. Existing methods\nfor WSN anomaly detection often face challenges such as the limited extraction\nof spatiotemporal correlation features, the absence of sample labels, few\nanomaly samples, and an imbalanced sample distribution. To address these\nissues, a spatiotemporal correlation detection model (MTAD-RD) considering both\nmodel architecture and a two-stage training strategy perspective is proposed.\nIn terms of model structure design, the proposed MTAD-RD backbone network\nincludes a retentive network (RetNet) enhanced by a cross-retention (CR)\nmodule, a multigranular feature fusion module, and a graph attention network\nmodule to extract internode correlation information. This proposed model can\nintegrate the intermodal correlation features and spatial features of WSN\nneighbor nodes while extracting global information from time series data.\nMoreover, its serialized inference characteristic can remarkably reduce\ninference overhead. For model training, a two-stage training approach was\ndesigned. First, a contrastive learning proxy task was designed for time series\ndata with graph structure information in WSNs, enabling the backbone network to\nlearn transferable features from unlabeled data using unsupervised contrastive\nlearning methods, thereby addressing the issue of missing sample labels in the\ndataset. Then, a caching-based sample sampler was designed to divide samples\ninto few-shot and contrastive learning data. A specific joint loss function was\ndeveloped to jointly train the dual-graph discriminator network to address the\nproblem of sample imbalance effectively. In experiments carried out on real\npublic datasets, the designed MTAD-RD anomaly detection method achieved an F1\nscore of 90.97%, outperforming existing supervised WSN anomaly detection\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u6a21\u578b\u67b6\u6784\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u7684\u65f6\u7a7a\u76f8\u5173\u6027\u68c0\u6d4b\u6a21\u578b\uff08MTAD-RD\uff09\uff0c\u7528\u4e8e\u89e3\u51b3WSN\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u7279\u5f81\u63d0\u53d6\u4e0d\u8db3\u3001\u6837\u672c\u6807\u7b7e\u7f3a\u5931\u3001\u6837\u672c\u5206\u5e03\u4e0d\u5e73\u8861\u7b49\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709WSN\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u63d0\u53d6\u65f6\u7a7a\u76f8\u5173\u6027\u7279\u5f81\u3001\u6837\u672c\u6807\u7b7e\u7f3a\u5931\u3001\u5f02\u5e38\u6837\u672c\u7a00\u5c11\u53ca\u6837\u672c\u4e0d\u5e73\u8861\u7b49\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u589e\u5f3aRetNet\u3001\u591a\u7c92\u5ea6\u7279\u5f81\u878d\u5408\u6a21\u5757\u548c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u6a21\u5757\u7684MTAD-RD\u4e3b\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u5bf9\u6bd4\u5b66\u4e60\u4ee3\u7406\u4efb\u52a1\u548c\u7f13\u5b58\u6837\u672c\u91c7\u6837\u5668\uff09\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u771f\u5b9e\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0cMTAD-RD\u7684F1\u5206\u6570\u8fbe\u523090.97%\uff0c\u4f18\u4e8e\u73b0\u6709\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "MTAD-RD\u80fd\u6709\u6548\u6574\u5408\u65f6\u7a7a\u7279\u5f81\uff0c\u89e3\u51b3\u6837\u672c\u6807\u7b7e\u548c\u5206\u5e03\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347WSN\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "keywords": "WSN\u3001\u5f02\u5e38\u68c0\u6d4b\u3001\u65f6\u7a7a\u76f8\u5173\u6027\u3001\u5bf9\u6bd4\u5b66\u4e60\u3001\u6837\u672c\u4e0d\u5e73\u8861"}}
{"id": "2506.00338", "pdf": "https://arxiv.org/pdf/2506.00338", "abs": "https://arxiv.org/abs/2506.00338", "authors": ["Yifan Peng", "Shakeel Muhammad", "Yui Sudo", "William Chen", "Jinchuan Tian", "Chyi-Jiunn Lin", "Shinji Watanabe"], "title": "OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "The Open Whisper-style Speech Models (OWSM) project has developed a series of\nfully open speech foundation models using academic-scale resources, but their\ntraining data remains insufficient. This work enhances OWSM by integrating\nYODAS, a large-scale web-crawled dataset with a Creative Commons license.\nHowever, incorporating YODAS is nontrivial due to its wild nature, which\nintroduces challenges such as incorrect language labels and audio-text\nmisalignments. To address this, we develop a scalable data-cleaning pipeline\nusing public toolkits, yielding a dataset with 166,000 hours of speech across\n75 languages. Our new series of OWSM v4 models, trained on this curated dataset\nalongside existing OWSM data, significantly outperform previous versions on\nmultilingual benchmarks. Our models even match or surpass frontier industrial\nmodels like Whisper and MMS in multiple scenarios. We will publicly release the\ncleaned YODAS data, pre-trained models, and all associated scripts via the\nESPnet toolkit.", "AI": {"tldr": "OWSM v4\u901a\u8fc7\u96c6\u6210\u5927\u89c4\u6a21\u5f00\u653e\u7684YODAS\u6570\u636e\u96c6\u5e76\u5f00\u53d1\u6570\u636e\u6e05\u6d17\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u4e00\u4e9b\u5de5\u4e1a\u524d\u6cbf\u6a21\u578b\u3002", "motivation": "OWSM\u9879\u76ee\u73b0\u6709\u7684\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\uff0c\u9700\u8981\u6269\u5c55\u6570\u636e\u6e90\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u591a\u8bed\u8a00\u80fd\u529b\u3002", "method": "\u6574\u5408YODAS\u6570\u636e\u96c6\u5e76\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u6570\u636e\u6e05\u6d17\u6d41\u7a0b\uff0c\u89e3\u51b3\u6807\u7b7e\u9519\u8bef\u548c\u97f3\u9891-\u6587\u672c\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u6700\u7ec8\u751f\u6210166,000\u5c0f\u65f6\u7684\u6e05\u6d17\u540e\u6570\u636e\u3002", "result": "OWSM v4\u5728\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7248\u672c\uff0c\u5e76\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u8d85\u8d8aWhisper\u548cMMS\u7b49\u5de5\u4e1a\u524d\u6cbf\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5f00\u653e\u6570\u636e\u548c\u5de5\u5177\uff0cOWSM v4\u5c55\u793a\u4e86\u5b66\u672f\u754c\u8d44\u6e90\u4e0b\u8bed\u97f3\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u57fa\u7840\u6a21\u578b\u548c\u6570\u636e\u96c6\u3002", "keywords": "\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u3001\u591a\u8bed\u8a00\u3001\u6570\u636e\u6e05\u6d17\u3001\u5f00\u6e90\u3001YODAS"}}
{"id": "2506.00835", "pdf": "https://arxiv.org/pdf/2506.00835", "abs": "https://arxiv.org/abs/2506.00835", "authors": ["Jisheng Dang", "Yizhou Zhang", "Hao Ye", "Teng Wang", "Siming Chen", "Huicheng Zheng", "Yulan Guo", "Jianhuang Lai", "Bin Hu"], "title": "SynPO: Synergizing Descriptiveness and Preference Optimization for Video Detailed Captioning", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Fine-grained video captioning aims to generate detailed, temporally coherent\ndescriptions of video content. However, existing methods struggle to capture\nsubtle video dynamics and rich detailed information. In this paper, we leverage\npreference learning to enhance the performance of vision-language models in\nfine-grained video captioning, while mitigating several limitations inherent to\ndirect preference optimization (DPO). First, we propose a pipeline for\nconstructing preference pairs that leverages the intrinsic properties of VLMs\nalong with partial assistance from large language models, achieving an optimal\nbalance between cost and data quality. Second, we propose Synergistic\nPreference Optimization (SynPO), a novel optimization method offering\nsignificant advantages over DPO and its variants. SynPO prevents negative\npreferences from dominating the optimization, explicitly preserves the model's\nlanguage capability to avoid deviation of the optimization objective, and\nimproves training efficiency by eliminating the need for the reference model.\nWe extensively evaluate SynPO not only on video captioning benchmarks (e.g.,\nVDC, VDD, VATEX) but also across well-established NLP tasks, including general\nlanguage understanding and preference evaluation, using diverse pretrained\nmodels. Results demonstrate that SynPO consistently outperforms DPO variants\nwhile achieving 20\\% improvement in training efficiency. Code is available at\nhttps://github.com/longmalongma/SynPO", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u504f\u597d\u5b66\u4e60\u7684\u7ec6\u7c92\u5ea6\u89c6\u9891\u63cf\u8ff0\u65b9\u6cd5SynPO\uff0c\u901a\u8fc7\u6784\u9020\u9ad8\u8d28\u91cf\u504f\u597d\u5bf9\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u89c6\u9891\u4e2d\u7684\u7ec6\u5fae\u52a8\u6001\u548c\u4e30\u5bcc\u7ec6\u8282\u4fe1\u606f\uff0c\u56e0\u6b64\u8bba\u6587\u63d0\u51fa\u5229\u7528\u504f\u597d\u5b66\u4e60\u63d0\u5347\u7ec6\u7c92\u5ea6\u89c6\u9891\u63cf\u8ff0\u7684\u6027\u80fd\uff0c\u5e76\u514b\u670d\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7684\u5c40\u9650\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\uff1a1\uff09\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u504f\u597d\u5bf9\u7684\u6d41\u6c34\u7ebf\uff1b2\uff09\u534f\u540c\u504f\u597d\u4f18\u5316\uff08SynPO\uff09\u65b9\u6cd5\uff0c\u907f\u514d\u8d1f\u504f\u597d\u4e3b\u5bfc\u4f18\u5316\u5e76\u4fdd\u7559\u8bed\u8a00\u80fd\u529b\u3002", "result": "SynPO\u5728\u89c6\u9891\u63cf\u8ff0\u57fa\u51c6\u6d4b\u8bd5\u548cNLP\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\u53d8\u4f53\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u534720%\u3002", "conclusion": "SynPO\u4e3a\u7ec6\u7c92\u5ea6\u89c6\u9891\u63cf\u8ff0\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "keywords": "\u7ec6\u7c92\u5ea6\u89c6\u9891\u63cf\u8ff0, \u504f\u597d\u5b66\u4e60, \u534f\u540c\u4f18\u5316, \u89c6\u89c9\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.00424", "pdf": "https://arxiv.org/pdf/2506.00424", "abs": "https://arxiv.org/abs/2506.00424", "authors": ["Chamika Sudusinghe", "Gerasimos Gerogiannis Damitha Lenadora", "Charles Block", "Josep Torrellas", "Charith Mendis"], "title": "COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.ET"], "comment": "Accepted at the 42nd International Conference on Machine Learning", "summary": "Sparse tensor programs are essential in deep learning and graph analytics,\ndriving the need for optimized processing. To meet this demand, specialized\nhardware accelerators are being developed. Optimizing these programs for\naccelerators is challenging for two reasons: program performance is highly\nsensitive to variations in sparse inputs, and early-stage accelerators rely on\nexpensive simulators. Therefore, ML-based cost models used for optimizing such\nprograms on general-purpose hardware are often ineffective for early-stage\naccelerators, as they require large datasets for proper training. To this end,\nwe introduce COGNATE, a novel framework that leverages inexpensive data samples\nfrom general-purpose hardware (e.g., CPUs) to train cost models, followed by\nfew-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of\ninput features across hardware platforms while effectively mitigating\nheterogeneity, enabling cost model training with just 5% of the data samples\nneeded by accelerator-specific models to achieve comparable performance. We\nconduct extensive experiments to demonstrate that COGNATE outperforms existing\ntechniques, achieving average speedups of 1.47x (up to 5.46x) for SpMM and\n1.39x (up to 4.22x) for SDDMM.", "AI": {"tldr": "COGNATE\u6846\u67b6\u901a\u8fc7\u5229\u7528\u901a\u7528\u786c\u4ef6\u6570\u636e\u6837\u672c\u8bad\u7ec3\u6210\u672c\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u5fae\u8c03\uff0c\u663e\u8457\u4f18\u5316\u7a00\u758f\u5f20\u91cf\u7a0b\u5e8f\u6027\u80fd\u3002", "motivation": "\u7a00\u758f\u5f20\u91cf\u7a0b\u5e8f\u5728\u6df1\u5ea6\u5b66\u4e60\u548c\u56fe\u5206\u6790\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u65e9\u671f\u786c\u4ef6\u52a0\u901f\u5668\u7684\u4f18\u5316\u9762\u4e34\u6027\u80fd\u654f\u611f\u548c\u6a21\u62df\u5668\u6210\u672c\u9ad8\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faCOGNATE\u6846\u67b6\uff0c\u5229\u7528\u901a\u7528\u786c\u4ef6\u6570\u636e\u6837\u672c\u8bad\u7ec3\u6210\u672c\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u5728\u65b0\u578b\u786c\u4ef6\u4e0a\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCOGNATE\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0cSpMM\u548cSDDMM\u5206\u522b\u5b9e\u73b01.47\u500d\u548c1.39\u500d\u7684\u5e73\u5747\u52a0\u901f\u3002", "conclusion": "COGNATE\u901a\u8fc7\u9ad8\u6548\u6570\u636e\u5229\u7528\u548c\u6a21\u578b\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u5f20\u91cf\u7a0b\u5e8f\u5728\u65e9\u671f\u786c\u4ef6\u52a0\u901f\u5668\u4e0a\u7684\u6027\u80fd\u3002", "keywords": "\u7a00\u758f\u5f20\u91cf\u7a0b\u5e8f,\u786c\u4ef6\u52a0\u901f\u5668,\u6210\u672c\u6a21\u578b,COGNATE,\u673a\u5668\u5b66\u4e60"}}
{"id": "2506.00344", "pdf": "https://arxiv.org/pdf/2506.00344", "abs": "https://arxiv.org/abs/2506.00344", "authors": ["Sungjae Lee", "Hoyoung Kim", "Jeongyeon Hwang", "Eunhyeok Park", "Jungseul Ok"], "title": "Efficient Latent Semantic Clustering for Scaling Test-Time Computation of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling test-time computation--generating and analyzing multiple or\nsequential outputs for a single input--has become a promising strategy for\nimproving the reliability and quality of large language models (LLMs), as\nevidenced by advances in uncertainty quantification and multi-step reasoning. A\nkey shared component is semantic clustering, which groups outputs that differ\nin form but convey the same meaning. Semantic clustering enables estimation of\nthe distribution over the semantics of outputs and helps avoid redundant\nexploration of reasoning paths. However, existing approaches typically rely on\nexternal models, which introduce substantial computational overhead and often\nfail to capture context-aware semantics. We propose Latent Semantic Clustering\n(LSC), a lightweight and context-sensitive method that leverages the generator\nLLM's internal hidden states for clustering, eliminating the need for external\nmodels. Our extensive experiment across various LLMs and datasets shows that\nLSC significantly improves the computational efficiency of test-time scaling\nwhile maintaining or exceeding the performance of existing methods.", "AI": {"tldr": "LSC\u662f\u4e00\u79cd\u5229\u7528LLM\u5185\u90e8\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u8bed\u4e49\u805a\u7c7b\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u4fdd\u6301\u6216\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u3002", "motivation": "\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u53ef\u9760\u6027\u548c\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u591a\u6b65\u63a8\u7406\u65b9\u9762\uff0c\u8bed\u4e49\u805a\u7c7b\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u6a21\u578b\uff0c\u5e26\u6765\u9ad8\u8ba1\u7b97\u5f00\u9500\u548c\u8bed\u4e49\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faLatent Semantic Clustering (LSC)\uff0c\u5229\u7528\u751f\u6210LLM\u7684\u5185\u90e8\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u805a\u7c7b\uff0c\u907f\u514d\u5916\u90e8\u6a21\u578b\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u5404\u79cdLLM\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLSC\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e14\u6027\u80fd\u4fdd\u6301\u6216\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LSC\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u8bed\u4e49\u805a\u7c7b\u65b9\u6cd5\uff0c\u53ef\u66ff\u4ee3\u4f9d\u8d56\u5916\u90e8\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u8bed\u4e49\u805a\u7c7b,\u6d4b\u8bd5\u65f6\u8ba1\u7b97,\u9690\u85cf\u72b6\u6001,\u6548\u7387"}}
{"id": "2506.00855", "pdf": "https://arxiv.org/pdf/2506.00855", "abs": "https://arxiv.org/abs/2506.00855", "authors": ["Sau Lai Yip", "Sunan He", "Yuxiang Nie", "Shu Pui Chan", "Yilin Ye", "Sum Ying Lam", "Hao Chen"], "title": "MedBookVQA: A Systematic and Comprehensive Medical Benchmark Derived from Open-Access Book", "categories": ["cs.AI"], "comment": "For data and code, see:\n  https://huggingface.co/datasets/slyipae1/MedBookVQA and\n  https://github.com/slyipae1/MedBookVQA", "summary": "The accelerating development of general medical artificial intelligence\n(GMAI), powered by multimodal large language models (MLLMs), offers\ntransformative potential for addressing persistent healthcare challenges,\nincluding workforce deficits and escalating costs. The parallel development of\nsystematic evaluation benchmarks emerges as a critical imperative to enable\nperformance assessment and provide technological guidance. Meanwhile, as an\ninvaluable knowledge source, the potential of medical textbooks for benchmark\ndevelopment remains underexploited. Here, we present MedBookVQA, a systematic\nand comprehensive multimodal benchmark derived from open-access medical\ntextbooks. To curate this benchmark, we propose a standardized pipeline for\nautomated extraction of medical figures while contextually aligning them with\ncorresponding medical narratives. Based on this curated data, we generate 5,000\nclinically relevant questions spanning modality recognition, disease\nclassification, anatomical identification, symptom diagnosis, and surgical\nprocedures. A multi-tier annotation system categorizes queries through\nhierarchical taxonomies encompassing medical imaging modalities (42\ncategories), body anatomies (125 structures), and clinical specialties (31\ndepartments), enabling nuanced analysis across medical subdomains. We evaluate\na wide array of MLLMs, including proprietary, open-sourced, medical, and\nreasoning models, revealing significant performance disparities across task\ntypes and model categories. Our findings highlight critical capability gaps in\ncurrent GMAI systems while establishing textbook-derived multimodal benchmarks\nas essential evaluation tools. MedBookVQA establishes textbook-derived\nbenchmarking as a critical paradigm for advancing clinical AI, exposing\nlimitations in GMAI systems while providing anatomically structured performance\nmetrics across specialties.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MedBookVQA\uff0c\u4e00\u4e2a\u57fa\u4e8e\u533b\u5b66\u6559\u79d1\u4e66\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u901a\u7528\u533b\u5b66\u4eba\u5de5\u667a\u80fd\uff08GMAI\uff09\u7684\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u53d1\u5c55\uff0c\u901a\u7528\u533b\u5b66\u4eba\u5de5\u667a\u80fd\uff08GMAI\uff09\u6709\u671b\u89e3\u51b3\u533b\u7597\u884c\u4e1a\u4e2d\u7684\u52b3\u52a8\u529b\u77ed\u7f3a\u548c\u6210\u672c\u4e0a\u5347\u95ee\u9898\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u800c\u533b\u5b66\u6559\u79d1\u4e66\u4f5c\u4e3a\u5b9d\u8d35\u7684\u77e5\u8bc6\u6765\u6e90\uff0c\u5176\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u5f00\u53d1\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u6807\u51c6\u5316\u6d41\u7a0b\uff0c\u4ece\u5f00\u653e\u83b7\u53d6\u7684\u533b\u5b66\u6559\u79d1\u4e66\u4e2d\u81ea\u52a8\u63d0\u53d6\u533b\u5b66\u56fe\u50cf\uff0c\u5e76\u5c06\u5176\u4e0e\u76f8\u5173\u533b\u5b66\u6587\u672c\u5bf9\u9f50\u3002\u57fa\u4e8e\u6b64\uff0c\u751f\u6210\u4e865,000\u4e2a\u6d89\u53ca\u591a\u79cd\u4e34\u5e8a\u4efb\u52a1\u7684\u95ee\u7b54\u5bf9\uff0c\u5e76\u91c7\u7528\u591a\u5c42\u6ce8\u91ca\u7cfb\u7edf\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u8bc4\u4f30\u4e86\u591a\u79cdMLLM\u6a21\u578b\uff0c\u53d1\u73b0\u5176\u5728\u4efb\u52a1\u7c7b\u578b\u548c\u6a21\u578b\u7c7b\u522b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff0c\u540c\u65f6\u63ed\u793a\u4e86GMAI\u7cfb\u7edf\u7684\u80fd\u529b\u5dee\u8ddd\u3002", "conclusion": "MedBookVQA\u5c55\u793a\u4e86\u6559\u79d1\u4e66\u884d\u751f\u7684\u591a\u6a21\u6001\u57fa\u51c6\u5728\u63a8\u52a8\u4e34\u5e8aAI\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u6307\u6807\u3002", "keywords": "\u901a\u7528\u533b\u5b66\u4eba\u5de5\u667a\u80fd, \u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, \u533b\u5b66\u6559\u79d1\u4e66, \u57fa\u51c6\u6d4b\u8bd5, \u6027\u80fd\u8bc4\u4f30, MedBookVQA"}}
{"id": "2506.00431", "pdf": "https://arxiv.org/pdf/2506.00431", "abs": "https://arxiv.org/abs/2506.00431", "authors": ["Jie Peng", "Zhewei Wei", "Yuhang Ye"], "title": "TIDFormer: Exploiting Temporal and Interactive Dynamics Makes A Great Dynamic Graph Transformer", "categories": ["cs.LG"], "comment": "KDD2025", "summary": "Due to the proficiency of self-attention mechanisms (SAMs) in capturing\ndependencies in sequence modeling, several existing dynamic graph neural\nnetworks (DGNNs) utilize Transformer architectures with various encoding\ndesigns to capture sequential evolutions of dynamic graphs. However, the\neffectiveness and efficiency of these Transformer-based DGNNs vary\nsignificantly, highlighting the importance of properly defining the SAM on\ndynamic graphs and comprehensively encoding temporal and interactive dynamics\nwithout extra complex modules. In this work, we propose TIDFormer, a dynamic\ngraph TransFormer that fully exploits Temporal and Interactive Dynamics in an\nefficient manner. We clarify and verify the interpretability of our proposed\nSAM, addressing the open problem of its uninterpretable definitions on dynamic\ngraphs in previous works. To model the temporal and interactive dynamics,\nrespectively, we utilize the calendar-based time partitioning information and\nextract informative interaction embeddings for both bipartite and non-bipartite\ngraphs using merely the sampled first-order neighbors. In addition, we jointly\nmodel temporal and interactive features by capturing potential changes in\nhistorical interaction patterns through a simple decomposition. We conduct\nextensive experiments on several dynamic graph datasets to verify the\neffectiveness and efficiency of TIDFormer. The experimental results demonstrate\nthat TIDFormer excels, outperforming state-of-the-art models across most\ndatasets and experimental settings. Furthermore, TIDFormer exhibits significant\nefficiency advantages compared to previous Transformer-based methods.", "AI": {"tldr": "TIDFormer \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u52a8\u6001\u56fe Transformer \u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u6355\u83b7\u65f6\u95f4\u548c\u4ea4\u4e92\u52a8\u6001\uff0c\u901a\u8fc7\u7b80\u5355\u5206\u89e3\u5386\u53f2\u4ea4\u4e92\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e Transformer \u7684\u52a8\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\uff08DGNNs\uff09\u5728\u5b9a\u4e49\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff08SAM\uff09\u65f6\u7684\u4e0d\u900f\u660e\u6027\u548c\u6548\u7387\u95ee\u9898\u3002", "method": "TIDFormer \u5229\u7528\u57fa\u4e8e\u65e5\u5386\u7684\u65f6\u95f4\u5206\u533a\u4fe1\u606f\u548c\u91c7\u6837\u7684\u4e00\u9636\u90bb\u5c45\u63d0\u53d6\u4ea4\u4e92\u5d4c\u5165\uff0c\u901a\u8fc7\u7b80\u5355\u5206\u89e3\u8054\u5408\u5efa\u6a21\u65f6\u95f4\u548c\u4ea4\u4e92\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTIDFormer \u5728\u591a\u4e2a\u52a8\u6001\u56fe\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u6a21\u578b\uff0c\u5e76\u5177\u6709\u663e\u8457\u7684\u6548\u7387\u4f18\u52bf\u3002", "conclusion": "TIDFormer \u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u52a8\u6001\u56fe Transformer \u67b6\u6784\uff0c\u4e3a\u52a8\u6001\u56fe\u5efa\u6a21\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "keywords": "\u52a8\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\u3001Transformer\u3001\u65f6\u95f4\u52a8\u6001\u3001\u4ea4\u4e92\u52a8\u6001\u3001TIDFormer"}}
{"id": "2506.00381", "pdf": "https://arxiv.org/pdf/2506.00381", "abs": "https://arxiv.org/abs/2506.00381", "authors": ["Siavash Shams", "Richard Antonello", "Gavin Mischler", "Stephan Bickel", "Ashesh Mehta", "Nima Mesgarani"], "title": "Neuro2Semantic: A Transfer Learning Framework for Semantic Reconstruction of Continuous Language from Human Intracranial EEG", "categories": ["cs.CL", "eess.AS", "eess.SP"], "comment": "Accepted at Interspeech 2025 Code at\n  https://github.com/SiavashShams/neuro2semantic", "summary": "Decoding continuous language from neural signals remains a significant\nchallenge in the intersection of neuroscience and artificial intelligence. We\nintroduce Neuro2Semantic, a novel framework that reconstructs the semantic\ncontent of perceived speech from intracranial EEG (iEEG) recordings. Our\napproach consists of two phases: first, an LSTM-based adapter aligns neural\nsignals with pre-trained text embeddings; second, a corrector module generates\ncontinuous, natural text directly from these aligned embeddings. This flexible\nmethod overcomes the limitations of previous decoding approaches and enables\nunconstrained text generation. Neuro2Semantic achieves strong performance with\nas little as 30 minutes of neural data, outperforming a recent state-of-the-art\nmethod in low-data settings. These results highlight the potential for\npractical applications in brain-computer interfaces and neural decoding\ntechnologies.", "AI": {"tldr": "Neuro2Semantic \u662f\u4e00\u4e2a\u80fd\u4ece\u9885\u5185\u8111\u7535\u56fe(iEEG)\u91cd\u5efa\u8bed\u4e49\u5185\u5bb9\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7LSTM\u9002\u914d\u5668\u548c\u6821\u6b63\u6a21\u5757\u5b9e\u73b0\u8fde\u7eed\u81ea\u7136\u6587\u672c\u751f\u6210\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u4fe1\u53f7\u89e3\u7801\u8bed\u8a00\u7684\u96be\u9898\uff0c\u63a8\u52a8\u8111\u673a\u63a5\u53e3\u548c\u795e\u7ecf\u89e3\u7801\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1aLSTM\u9002\u914d\u5668\u5bf9\u9f50\u795e\u7ecf\u4fe1\u53f7\u4e0e\u6587\u672c\u5d4c\u5165\uff0c\u6821\u6b63\u6a21\u5757\u751f\u6210\u8fde\u7eed\u6587\u672c\u3002", "result": "\u5728\u4f4e\u6570\u636e\u91cf(30\u5206\u949f)\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "Neuro2Semantic \u5728\u795e\u7ecf\u89e3\u7801\u9886\u57df\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u795e\u7ecf\u4fe1\u53f7\u89e3\u7801, \u8bed\u4e49\u91cd\u5efa, LSTM, iEEG, \u8111\u673a\u63a5\u53e3"}}
{"id": "2506.00865", "pdf": "https://arxiv.org/pdf/2506.00865", "abs": "https://arxiv.org/abs/2506.00865", "authors": ["Jiajun He", "Jinyi Mi", "Tomoki Toda"], "title": "GIA-MIC: Multimodal Emotion Recognition with Gated Interactive Attention and Modality-Invariant Learning Constraints", "categories": ["cs.AI"], "comment": "Accepted by INTERSPEECH 2025", "summary": "Multimodal emotion recognition (MER) extracts emotions from multimodal data,\nincluding visual, speech, and text inputs, playing a key role in human-computer\ninteraction. Attention-based fusion methods dominate MER research, achieving\nstrong classification performance. However, two key challenges remain:\neffectively extracting modality-specific features and capturing cross-modal\nsimilarities despite distribution differences caused by modality heterogeneity.\nTo address these, we propose a gated interactive attention mechanism to\nadaptively extract modality-specific features while enhancing emotional\ninformation through pairwise interactions. Additionally, we introduce a\nmodality-invariant generator to learn modality-invariant representations and\nconstrain domain shifts by aligning cross-modal similarities. Experiments on\nIEMOCAP demonstrate that our method outperforms state-of-the-art MER\napproaches, achieving WA 80.7% and UA 81.3%.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u95e8\u63a7\u4ea4\u4e92\u6ce8\u610f\u529b\u548c\u6a21\u6001\u4e0d\u53d8\u751f\u6210\u5668\u7684\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\uff0c\u89e3\u51b3\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u548c\u8de8\u6a21\u6001\u76f8\u4f3c\u6027\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u7814\u7a76\u4e2d\uff0c\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u548c\u8de8\u6a21\u6001\u76f8\u4f3c\u6027\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u9700\u63d0\u51fa\u65b0\u65b9\u6cd5\u89e3\u51b3\u3002", "method": "\u91c7\u7528\u95e8\u63a7\u4ea4\u4e92\u6ce8\u610f\u529b\u673a\u5236\u63d0\u53d6\u6a21\u6001\u7279\u5f81\u5e76\u901a\u8fc7\u4ea4\u4e92\u589e\u5f3a\u60c5\u611f\u4fe1\u606f\uff0c\u7ed3\u5408\u6a21\u6001\u4e0d\u53d8\u751f\u6210\u5668\u5b66\u4e60\u4e0d\u53d8\u8868\u793a\u5e76\u5bf9\u9f50\u8de8\u6a21\u6001\u76f8\u4f3c\u6027\u3002", "result": "\u5728IEMOCAP\u6570\u636e\u96c6\u4e0a\uff0cWA\u8fbe80.7%\uff0cUA\u8fbe81.3%\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u89e3\u51b3\u4e86\u6a21\u6001\u7279\u5f81\u548c\u76f8\u4f3c\u6027\u95ee\u9898\u3002", "keywords": "\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u6a21\u6001\u4e0d\u53d8\u8868\u793a\u3001IEMOCAP"}}
{"id": "2506.00432", "pdf": "https://arxiv.org/pdf/2506.00432", "abs": "https://arxiv.org/abs/2506.00432", "authors": ["Seunghan Lee", "Taeyoung Park", "Kibok Lee"], "title": "Channel Normalization for Time Series Channel Identification", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "ICML 2025", "summary": "Channel identifiability (CID) refers to the ability to distinguish between\nindividual channels in time series (TS) modeling. The absence of CID often\nresults in producing identical outputs for identical inputs, disregarding\nchannel-specific characteristics. In this paper, we highlight the importance of\nCID and propose Channel Normalization (CN), a simple yet effective\nnormalization strategy that enhances CID by assigning distinct affine\ntransformation parameters to each channel. We further extend CN in two ways: 1)\nAdaptive CN (ACN) dynamically adjusts parameters based on the input TS,\nimproving adaptability in TS models, and 2) Prototypical CN (PCN) introduces a\nset of learnable prototypes instead of per-channel parameters, enabling\napplicability to datasets with unknown or varying number of channels and\nfacilitating use in TS foundation models. We demonstrate the effectiveness of\nCN and its variants by applying them to various TS models, achieving\nsignificant performance gains for both non-CID and CID models. In addition, we\nanalyze the success of our approach from an information theory perspective.\nCode is available at https://github.com/seunghan96/CN.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u901a\u9053\u5f52\u4e00\u5316\uff08CN\uff09\u53ca\u5176\u53d8\u4f53\uff08ACN\u548cPCN\uff09\uff0c\u4ee5\u589e\u5f3a\u65f6\u5e8f\u5efa\u6a21\u4e2d\u7684\u901a\u9053\u53ef\u8fa8\u8bc6\u6027\uff08CID\uff09\uff0c\u5e76\u5728\u591a\u79cd\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u65f6\u5e8f\u5efa\u6a21\u4e2d\u901a\u9053\u4e0d\u53ef\u8fa8\u8bc6\u6027\u95ee\u9898\uff0c\u907f\u514d\u6a21\u578b\u5ffd\u7565\u901a\u9053\u7279\u6709\u7279\u5f81\u3002", "method": "\u63d0\u51faCN\u3001ACN\u548cPCN\u4e09\u79cd\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u5206\u522b\u901a\u8fc7\u56fa\u5b9a\u53c2\u6570\u3001\u52a8\u6001\u8c03\u6574\u53c2\u6570\u548c\u5b66\u4e60\u539f\u578b\u6765\u589e\u5f3aCID\u3002", "result": "\u5728\u4e0d\u540c\u65f6\u5e8f\u6a21\u578b\u4e0a\u5e94\u7528CN\u53ca\u5176\u53d8\u4f53\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "CN\u53ca\u5176\u53d8\u4f53\u80fd\u6709\u6548\u89e3\u51b3CID\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u65f6\u5e8f\u4efb\u52a1\u548c\u6570\u636e\u96c6\u3002", "keywords": "\u901a\u9053\u53ef\u8fa8\u8bc6\u6027, \u65f6\u5e8f\u5efa\u6a21, \u5f52\u4e00\u5316, \u81ea\u9002\u5e94\u53c2\u6570, \u539f\u578b\u5b66\u4e60"}}
{"id": "2506.00386", "pdf": "https://arxiv.org/pdf/2506.00386", "abs": "https://arxiv.org/abs/2506.00386", "authors": ["Keyeun Lee", "Seolhee Lee", "Esther Hehsun Kim", "Yena Ko", "Jinsu Eun", "Dahee Kim", "Hyewon Cho", "Haiyi Zhu", "Robert E. Kraut", "Eunyoung Suh", "Eun-mee Kim", "Hajin Lim"], "title": "Adaptive-VP: A Framework for LLM-Based Virtual Patients that Adapts to Trainees' Dialogue to Facilitate Nurse Communication Training", "categories": ["cs.CL", "cs.HC"], "comment": "ACL 2025 Findings, 34 pages, 9 figures", "summary": "Effective communication training is essential to preparing nurses for\nhigh-quality patient care. While standardized patient (SP) simulations provide\nvaluable experiential learning, they are often costly and inflexible. Virtual\npatient (VP) systems offer a scalable alternative, but most fail to adapt to\nthe varying communication skills of trainees. In particular, when trainees\nrespond ineffectively, VPs should escalate in hostility or become\nuncooperative--yet this level of adaptive interaction remains largely\nunsupported. To address this gap, we introduce Adaptive-VP, a VP dialogue\ngeneration framework that leverages large language models (LLMs) to dynamically\nadapt VP behavior based on trainee input. The framework features a pipeline for\nconstructing clinically grounded yet flexible VP scenarios and a modular system\nfor assessing trainee communication and adjusting VP responses in real time,\nwhile ensuring learner safety. We validated Adaptive-VP by simulating\nchallenging patient conversations. Automated evaluation using a corpus from\npracticing nurses showed that our communication skill evaluation mechanism\nreflected real-world proficiency levels. Expert nurses further confirmed that\nAdaptive-VP produced more natural and realistic interactions than existing\napproaches, demonstrating its potential as a scalable and effective tool for\nnursing communication training.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Adaptive-VP\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u52a8\u6001\u8c03\u6574\u865a\u62df\u60a3\u8005\u884c\u4e3a\u4ee5\u9002\u5e94\u5b66\u5458\u6c9f\u901a\u6280\u80fd\uff0c\u4e3a\u62a4\u7406\u6c9f\u901a\u57f9\u8bad\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u6807\u51c6\u5316\u60a3\u8005\u6a21\u62df\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u800c\u73b0\u6709\u865a\u62df\u60a3\u8005\u7cfb\u7edf\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u5b66\u5458\u7684\u6c9f\u901a\u6280\u80fd\uff0c\u9650\u5236\u4e86\u5176\u6548\u679c\u3002", "method": "\u91c7\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6784\u5efaAdaptive-VP\u6846\u67b6\uff0c\u5b9e\u65f6\u8bc4\u4f30\u5b66\u5458\u6c9f\u901a\u6280\u80fd\u5e76\u52a8\u6001\u8c03\u6574\u865a\u62df\u60a3\u8005\u884c\u4e3a\u3002", "result": "\u81ea\u52a8\u5316\u8bc4\u4f30\u4e0e\u4e13\u5bb6\u9a8c\u8bc1\u663e\u793a\uff0cAdaptive-VP\u751f\u6210\u66f4\u81ea\u7136\u3001\u771f\u5b9e\u7684\u4e92\u52a8\uff0c\u4e14\u80fd\u53cd\u6620\u771f\u5b9e\u6c9f\u901a\u6c34\u5e73\u3002", "conclusion": "Adaptive-VP\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u62a4\u7406\u6c9f\u901a\u57f9\u8bad\u5de5\u5177\u3002", "keywords": "\u6c9f\u901a\u57f9\u8bad\u3001\u865a\u62df\u60a3\u8005\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u62a4\u7406\u6559\u80b2\u3001\u81ea\u9002\u5e94\u4e92\u52a8"}}
{"id": "2506.00886", "pdf": "https://arxiv.org/pdf/2506.00886", "abs": "https://arxiv.org/abs/2506.00886", "authors": ["Hongru Wang", "Cheng Qian", "Manling Li", "Jiahao Qiu", "Boyang Xue", "Mengdi Wang", "Heng Ji", "Kam-Fai Wong"], "title": "Toward a Theory of Agents as Tool-Use Decision-Makers", "categories": ["cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) evolve into increasingly autonomous agents,\nfundamental questions about their epistemic foundations remain unresolved: What\ndefines an agent? How should it make decisions? And what objectives should\nguide its behavior? In this position paper, we argue that true autonomy\nrequires agents to be grounded in a coherent epistemic framework that governs\nwhat they know, what they need to know, and how to acquire that knowledge\nefficiently. We propose a unified theory that treats internal reasoning and\nexternal actions as equivalent epistemic tools, enabling agents to\nsystematically coordinate introspection and interaction. Building on this\nframework, we advocate for aligning an agent's tool use decision-making\nboundary with its knowledge boundary, thereby minimizing unnecessary tool use\nand maximizing epistemic efficiency. This perspective shifts the design of\nagents from mere action executors to knowledge-driven intelligence systems,\noffering a principled path toward building foundation agents capable of\nadaptive, efficient, and goal-directed behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7edf\u4e00\u7406\u8bba\uff0c\u5c06LLMs\u7684\u5185\u90e8\u63a8\u7406\u4e0e\u5916\u90e8\u884c\u4e3a\u89c6\u4e3a\u7b49\u6548\u8ba4\u77e5\u5de5\u5177\uff0c\u4ee5\u77e5\u8bc6\u8fb9\u754c\u6307\u5bfc\u5de5\u5177\u4f7f\u7528\uff0c\u5b9e\u73b0\u9ad8\u6548\u81ea\u4e3b\u4ee3\u7406\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u7684\u8ba4\u77e5\u57fa\u7840\uff0c\u89e3\u51b3\u5176\u51b3\u7b56\u548c\u884c\u4e3a\u7684\u6307\u5bfc\u539f\u5219\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7406\u8bba\uff0c\u5c06\u5185\u90e8\u63a8\u7406\u548c\u5916\u90e8\u884c\u4e3a\u89c6\u4e3a\u7b49\u6548\u8ba4\u77e5\u5de5\u5177\uff0c\u5e76\u5efa\u8bae\u5de5\u5177\u4f7f\u7528\u51b3\u7b56\u4e0e\u77e5\u8bc6\u8fb9\u754c\u5bf9\u9f50\u3002", "result": "\u901a\u8fc7\u6846\u67b6\u8bbe\u8ba1\uff0c\u4ee3\u7406\u4ece\u884c\u4e3a\u6267\u884c\u8005\u8f6c\u53d8\u4e3a\u77e5\u8bc6\u9a71\u52a8\u7cfb\u7edf\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u3001\u9ad8\u6548\u548c\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u3002", "conclusion": "\u8bba\u6587\u4e3a\u6784\u5efa\u5177\u6709\u8ba4\u77e5\u57fa\u7840\u7684\u81ea\u4e3b\u4ee3\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u5347\u4e86LLMs\u7684\u81ea\u4e3b\u6027\u548c\u6548\u7387\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u81ea\u4e3b\u4ee3\u7406,\u8ba4\u77e5\u6846\u67b6,\u5de5\u5177\u4f7f\u7528,\u77e5\u8bc6\u8fb9\u754c"}}
{"id": "2506.00436", "pdf": "https://arxiv.org/pdf/2506.00436", "abs": "https://arxiv.org/abs/2506.00436", "authors": ["Masahiro Kato", "Yuki Ikeda abd Kentaro Baba", "Takashi Imai", "Ryo Inokuchi"], "title": "Learning from Double Positive and Unlabeled Data for Potential-Customer Identification", "categories": ["cs.LG", "cs.AI", "econ.EM", "stat.ME", "stat.ML"], "comment": "Accepted for publication in the Proceedings of IIAI AAI 2025", "summary": "In this study, we propose a method for identifying potential customers in\ntargeted marketing by applying learning from positive and unlabeled data (PU\nlearning). We consider a scenario in which a company sells a product and can\nobserve only the customers who purchased it. Decision-makers seek to market\nproducts effectively based on whether people have loyalty to the company.\nIndividuals with loyalty are those who are likely to remain interested in the\ncompany even without additional advertising. Consequently, those loyal\ncustomers would likely purchase from the company if they are interested in the\nproduct. In contrast, people with lower loyalty may overlook the product or buy\nsimilar products from other companies unless they receive marketing attention.\nTherefore, by focusing marketing efforts on individuals who are interested in\nthe product but do not have strong loyalty, we can achieve more efficient\nmarketing. To achieve this goal, we consider how to learn, from limited data, a\nclassifier that identifies potential customers who (i) have interest in the\nproduct and (ii) do not have loyalty to the company. Although our algorithm\ncomprises a single-stage optimization, its objective function implicitly\ncontains two losses derived from standard PU learning settings. For this\nreason, we refer to our approach as double PU learning. We verify the validity\nof the proposed algorithm through numerical experiments, confirming that it\nfunctions appropriately for the problem at hand.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6b63\u548c\u65e0\u6807\u8bb0\u6570\u636e\u5b66\u4e60\uff08PU\u5b66\u4e60\uff09\u8bc6\u522b\u6f5c\u5728\u5ba2\u6237\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u9ad8\u6548\u8425\u9500\u9488\u5bf9\u5bf9\u4ea7\u54c1\u6709\u5174\u8da3\u4f46\u5fe0\u8bda\u5ea6\u4e0d\u9ad8\u7684\u5ba2\u6237\u3002", "motivation": "\u516c\u53f8\u5e0c\u671b\u901a\u8fc7\u8425\u9500\u7b56\u7565\u66f4\u6709\u6548\u5730\u9501\u5b9a\u90a3\u4e9b\u5bf9\u4ea7\u54c1\u6709\u5174\u8da3\u4f46\u5fe0\u8bda\u5ea6\u4e0d\u9ad8\u7684\u5ba2\u6237\uff0c\u4ee5\u907f\u514d\u8d44\u6e90\u6d6a\u8d39\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u53ccPU\u5b66\u4e60\u7684\u5355\u9636\u6bb5\u4f18\u5316\u7b97\u6cd5\uff0c\u5176\u76ee\u6807\u51fd\u6570\u9690\u542b\u4e86\u6765\u81ea\u6807\u51c6PU\u5b66\u4e60\u8bbe\u7f6e\u7684\u4e24\u4e2a\u635f\u5931\u3002", "result": "\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u5176\u9002\u7528\u4e8e\u76ee\u6807\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u6f5c\u5728\u5ba2\u6237\uff0c\u63d0\u5347\u8425\u9500\u6548\u7387\u3002", "keywords": "PU\u5b66\u4e60,\u5ba2\u6237\u8bc6\u522b,\u8425\u9500\u6548\u7387,\u5fe0\u8bda\u5ea6,\u6b63\u548c\u65e0\u6807\u8bb0\u6570\u636e"}}
{"id": "2506.00391", "pdf": "https://arxiv.org/pdf/2506.00391", "abs": "https://arxiv.org/abs/2506.00391", "authors": ["Ge Qu", "Jinyang Li", "Bowen Qin", "Xiaolong Li", "Nan Huo", "Chenhao Ma", "Reynold Cheng"], "title": "SHARE: An SLM-based Hierarchical Action CorREction Assistant for Text-to-SQL", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main", "summary": "Current self-correction approaches in text-to-SQL face two critical\nlimitations: 1) Conventional self-correction methods rely on recursive\nself-calls of LLMs, resulting in multiplicative computational overhead, and 2)\nLLMs struggle to implement effective error detection and correction for\ndeclarative SQL queries, as they fail to demonstrate the underlying reasoning\npath. In this work, we propose SHARE, an SLM-based Hierarchical Action\ncorREction assistant that enables LLMs to perform more precise error\nlocalization and efficient correction. SHARE orchestrates three specialized\nSmall Language Models (SLMs) in a sequential pipeline, where it first\ntransforms declarative SQL queries into stepwise action trajectories that\nreveal underlying reasoning, followed by a two-phase granular refinement. We\nfurther propose a novel hierarchical self-evolution strategy for data-efficient\ntraining. Experimental results demonstrate that SHARE effectively enhances\nself-correction capabilities while proving robust across various LLMs.\nFurthermore, our comprehensive analysis shows that SHARE maintains strong\nperformance even in low-resource training settings, which is particularly\nvaluable for text-to-SQL applications with data privacy constraints.", "AI": {"tldr": "SHARE\u662f\u4e00\u79cd\u57fa\u4e8e\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5206\u5c42\u52a8\u4f5c\u6821\u6b63\u52a9\u624b\uff0c\u901a\u8fc7\u5c06\u58f0\u660e\u5f0fSQL\u67e5\u8be2\u8f6c\u6362\u4e3a\u9010\u6b65\u52a8\u4f5c\u8f68\u8ff9\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u9519\u8bef\u5b9a\u4f4d\u548c\u9ad8\u6548\u6821\u6b63\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u81ea\u6821\u6b63\u65b9\u6cd5\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u9519\u8bef\u68c0\u6d4b\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u6587\u672c\u5230SQL\u81ea\u6821\u6b63\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0cSHARE\u65e8\u5728\u901a\u8fc7\u5206\u5c42\u52a8\u4f5c\u6821\u6b63\u7b56\u7565\u63d0\u5347\u81ea\u6821\u6b63\u7684\u6548\u7387\u548c\u7cbe\u786e\u6027\u3002", "method": "SHARE\u901a\u8fc7\u4e09\u4e2a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u987a\u5e8f\u7ba1\u9053\uff0c\u5c06SQL\u67e5\u8be2\u8f6c\u6362\u4e3a\u9010\u6b65\u52a8\u4f5c\u8f68\u8ff9\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u7ec6\u7c92\u5ea6\u4fee\u6b63\u548c\u5206\u5c42\u81ea\u8fdb\u5316\u7b56\u7565\u8fdb\u884c\u6570\u636e\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSHARE\u663e\u8457\u63d0\u5347\u4e86\u81ea\u6821\u6b63\u80fd\u529b\uff0c\u5e76\u5728\u4f4e\u8d44\u6e90\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u5f3a\u5065\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u6709\u6570\u636e\u9690\u79c1\u9650\u5236\u7684\u6587\u672c\u5230SQL\u5e94\u7528\u3002", "conclusion": "SHARE\u901a\u8fc7\u5206\u5c42\u52a8\u4f5c\u6821\u6b63\u548c\u81ea\u8fdb\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6587\u672c\u5230SQL\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u81ea\u6821\u6b63\u65b9\u6848\u3002", "keywords": "\u6587\u672c\u5230SQL, \u81ea\u6821\u6b63, \u5c0f\u578b\u8bed\u8a00\u6a21\u578b, \u5206\u5c42\u52a8\u4f5c\u6821\u6b63, \u6570\u636e\u9690\u79c1"}}
{"id": "2506.00911", "pdf": "https://arxiv.org/pdf/2506.00911", "abs": "https://arxiv.org/abs/2506.00911", "authors": ["William Overman", "Mohsen Bayati"], "title": "Conformal Arbitrage: Risk-Controlled Balancing of Competing Objectives in Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Modern language model deployments must often balance competing objectives,\nfor example, helpfulness versus harmlessness, cost versus accuracy, and reward\nversus safety. We introduce Conformal Arbitrage, a post hoc framework that\nlearns a data driven threshold to mediate between a Primary model optimized for\na primary objective and a more conservative Guardian which could be another\nmodel or a human domain expert aligned with a guardrail objective. The\nthreshold is calibrated with conformal risk control, yielding finite sample,\ndistribution free guarantees that the long run frequency of undesirable events,\nsuch as factual errors or safety violations, does not exceed a user specified\nquota. Because Conformal Arbitrage operates wholly at the API level, without\nrequiring access to model logits or updating model weights, it complements\nweight based alignment techniques and integrates seamlessly with existing cost\naware cascades. Empirically, Conformal Arbitrage traces an efficient frontier,\nallowing users to define an acceptable performance level for one objective\nwhile maximizing utility in another. We observe that our method outperforms, in\nterms of accuracy, cost matched random routing between models. These properties\nmake Conformal Arbitrage a practical, theoretically grounded tool for\ntrustworthy and economical deployment of large language models across a broad\nrange of potentially competing objectives.", "AI": {"tldr": "Conformal Arbitrage\u662f\u4e00\u4e2a\u540e\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u4e3b\u8981\u6a21\u578b\u548c\u4fdd\u62a4\u673a\u5236\u4e4b\u95f4\u5b66\u4e60\u6570\u636e\u9a71\u52a8\u7684\u9608\u503c\uff0c\u5e73\u8861\u591a\u4e2a\u7ade\u4e89\u76ee\u6807\uff0c\u63d0\u4f9b\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u9700\u5e73\u8861\u591a\u79cd\u7ade\u4e89\u76ee\u6807\uff08\u5982\u5e2e\u52a9\u6027\u4e0e\u65e0\u5bb3\u6027\uff09\uff0c\u9700\u8981\u4e00\u4e2a\u7406\u8bba\u4e0a\u53ef\u9760\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Conformal Arbitrage\u6846\u67b6\uff0c\u901a\u8fc7conformal\u98ce\u9669\u63a7\u5236\u6821\u51c6\u9608\u503c\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u6743\u91cd\uff0c\u4ec5\u5728API\u5c42\u9762\u64cd\u4f5c\u3002", "result": "\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6210\u672c\u4e0a\u4f18\u4e8e\u968f\u673a\u8def\u7531\uff0c\u80fd\u9ad8\u6548\u5b9e\u73b0\u591a\u4e2a\u76ee\u6807\u7684\u6743\u8861\u3002", "conclusion": "Conformal Arbitrage\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u7406\u8bba\u53ef\u9760\u7684\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u76ee\u6807\u90e8\u7f72\u3002", "keywords": "Conformal Arbitrage, \u8bed\u8a00\u6a21\u578b\u90e8\u7f72, \u98ce\u9669\u63a7\u5236, \u540e\u5904\u7406\u6846\u67b6"}}
{"id": "2506.00437", "pdf": "https://arxiv.org/pdf/2506.00437", "abs": "https://arxiv.org/abs/2506.00437", "authors": ["Jiaxing Zhang", "Xiaoou Liu", "Dongsheng Luo", "Hua Wei"], "title": "Is Your Explanation Reliable: Confidence-Aware Explanation on Graph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "In Proceedings of the 31st ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD25)", "summary": "Explaining Graph Neural Networks (GNNs) has garnered significant attention\ndue to the need for interpretability, enabling users to understand the behavior\nof these black-box models better and extract valuable insights from their\npredictions. While numerous post-hoc instance-level explanation methods have\nbeen proposed to interpret GNN predictions, the reliability of these\nexplanations remains uncertain, particularly in the out-of-distribution or\nunknown test datasets. In this paper, we address this challenge by introducing\nan explainer framework with the confidence scoring module ( ConfExplainer),\ngrounded in theoretical principle, which is generalized graph information\nbottleneck with confidence constraint (GIB-CC), that quantifies the reliability\nof generated explanations. Experimental results demonstrate the superiority of\nour approach, highlighting the effectiveness of the confidence score in\nenhancing the trustworthiness and robustness of GNN explanations.", "AI": {"tldr": "ConfExplainer\u6846\u67b6\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u6a21\u5757\u91cf\u5316GNN\u89e3\u91ca\u7684\u53ef\u9760\u6027\uff0c\u63d0\u5347\u4e86\u89e3\u91ca\u7684\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u91ca\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u9700\u6c42\u63a8\u52a8\u4e86\u53ef\u9760\u89e3\u91ca\u65b9\u6cd5\u7684\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5728\u5206\u5e03\u5916\u6216\u672a\u77e5\u6570\u636e\u96c6\u4e0a\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7406\u8bba\u539f\u5219\uff08GIB-CC\uff09\u7684ConfExplainer\u6846\u67b6\uff0c\u5305\u542b\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u6a21\u5757\uff0c\u91cf\u5316\u89e3\u91ca\u7684\u53ef\u9760\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u663e\u8457\u589e\u5f3a\u4e86\u89e3\u91ca\u7684\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "ConfExplainer\u4e3aGNN\u89e3\u91ca\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u7406\u8bba\u548c\u65b9\u6cd5\u652f\u6301\u3002", "keywords": "\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u89e3\u91ca\u6027\uff0c\u7f6e\u4fe1\u5ea6\u8bc4\u5206\uff0c\u4fe1\u606f\u74f6\u9888\uff0c\u53ef\u9760\u6027"}}
{"id": "2506.00396", "pdf": "https://arxiv.org/pdf/2506.00396", "abs": "https://arxiv.org/abs/2506.00396", "authors": ["Jiawei Gu", "Shangsong Liang"], "title": "Speculative Reward Model Boosts Decision Making Ability of LLMs Cost-Effectively", "categories": ["cs.CL"], "comment": "ACL2025 Oral (Industry Track)", "summary": "Effective decision-making in Large Language Models (LLMs) is essential for\nhandling intricate tasks. However, existing approaches prioritize performance\nbut often overlook the balance between effectiveness and computational cost. To\naddress this, we first introduce the 3E Criteria to systematically assess the\ncost-effectiveness of search strategies, revealing that existing methods often\ntrade significant efficiency for marginal performance gains. To improve LLM\ndecision-making while maintaining efficiency, we propose the Speculative Reward\nModel (SRM), a plug-and-play framework that seamlessly integrates with existing\nsearch strategies. Specifically, SRM employs an external reward assigner to\npredict optimal actions, reducing reliance on LLMs' internal self-evaluation.\nAnd a speculative verification mechanism is used to prune suboptimal choices\nand guide the search toward more promising steps. We evaluate SRM on several\ncomplex decision-making tasks including mathematical reasoning, planning and\nnumerical reasoning in specialized domains. Experimental results show that SRM\nreduces costs to 1/10 of the original search framework on average while\nmaintaining effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u63a8\u6d4b\u5956\u52b1\u6a21\u578b\uff08SRM\uff09\u201d\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u5e73\u8861\u6027\u80fd\u4e0e\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8ffd\u6c42\u6027\u80fd\u63d0\u5347\u65f6\u5ffd\u89c6\u4e86\u6548\u7387\u4e0e\u6210\u672c\u7684\u5e73\u8861\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u63d0\u9ad8\u51b3\u7b56\u6548\u679c\u53c8\u80fd\u63a7\u5236\u8ba1\u7b97\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86SRM\u6846\u67b6\uff0c\u901a\u8fc7\u5916\u90e8\u5956\u52b1\u5206\u914d\u5668\u548c\u63a8\u6d4b\u9a8c\u8bc1\u673a\u5236\u4f18\u5316\u641c\u7d22\u7b56\u7565\uff0c\u51cf\u5c11\u5bf9LLMs\u5185\u90e8\u81ea\u8bc4\u4f30\u7684\u4f9d\u8d56\uff0c\u5e76\u526a\u679d\u6b21\u4f18\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSRM\u5728\u591a\u4e2a\u590d\u6742\u4efb\u52a1\u4e2d\uff08\u5982\u6570\u5b66\u63a8\u7406\u3001\u89c4\u5212\u548c\u4e13\u4e1a\u9886\u57df\u6570\u503c\u63a8\u7406\uff09\u5c06\u6210\u672c\u964d\u81f3\u539f\u6846\u67b6\u76841/10\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "SRM\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8eLLMs\u7684\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u51b3\u7b56\u6548\u7387,\u6210\u672c\u6548\u76ca,\u63a8\u6d4b\u5956\u52b1\u6a21\u578b,\u641c\u7d22\u7b56\u7565"}}
{"id": "2506.00930", "pdf": "https://arxiv.org/pdf/2506.00930", "abs": "https://arxiv.org/abs/2506.00930", "authors": ["Yongqi Li", "Shen Zhou", "Xiaohu Li", "Xin Miao", "Jintao Wen", "Mayi Xu", "Jianhao Chen", "Birong Pan", "Hankun Kang", "Yuanyuan Zhu", "Ming Zhong", "Tieyun Qian"], "title": "Aligning VLM Assistants with Personalized Situated Cognition", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 (main), camera-ready version", "summary": "Vision-language models (VLMs) aligned with general human objectives, such as\nbeing harmless and hallucination-free, have become valuable assistants of\nhumans in managing visual tasks. However, people with diversified backgrounds\nhave different cognition even in the same situation. Consequently, they may\nhave personalized expectations for VLM assistants. This highlights the urgent\nneed to align VLM assistants with personalized situated cognition for\nreal-world assistance. To study this problem, we first simplify it by\ncharacterizing individuals based on the sociological concept of Role-Set. Then,\nwe propose to evaluate the individuals' actions to examine whether the\npersonalized alignment is achieved. Further, we construct a benchmark named\nPCogAlignBench, which includes 18k instances and 20 individuals with different\nRole-Sets. Finally, we present a framework called PCogAlign, which constructs a\ncognition-aware and action-based reward model for personalized alignment.\nExperimental results and human evaluations demonstrate the reliability of the\nPCogAlignBench and the effectiveness of our proposed PCogAlign. We will\nopen-source the constructed benchmark and code at\nhttps://github.com/NLPGM/PCogAlign.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e0e\u4e2a\u6027\u5316\u7684\u60c5\u5883\u8ba4\u77e5\u5bf9\u9f50\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u89d2\u8272\u96c6\u7684\u4e2a\u4f53\u8868\u5f81\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u5305\u542b18k\u5b9e\u4f8b\u7684\u57fa\u51c6PCogAlignBench\uff0c\u5e76\u63d0\u51fa\u4e86\u4e2a\u6027\u5316\u7684\u5bf9\u9f50\u6846\u67b6PCogAlign\u3002", "motivation": "\u7531\u4e8e\u4e0d\u540c\u80cc\u666f\u7684\u4eba\u5bf9\u540c\u4e00\u60c5\u5883\u6709\u4e0d\u540c\u7684\u8ba4\u77e5\u548c\u671f\u671b\uff0c\u9700\u8981\u5c06VLM\u52a9\u624b\u4e0e\u4e2a\u6027\u5316\u7684\u60c5\u5883\u8ba4\u77e5\u5bf9\u9f50\uff0c\u4ee5\u6ee1\u8db3\u4e2a\u6027\u5316\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u793e\u4f1a\u5b66\u7684\u89d2\u8272\u96c6\u6982\u5ff5\u7b80\u5316\u4e2a\u4f53\u8868\u5f81\uff0c\u6784\u5efa\u57fa\u51c6PCogAlignBench\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u8ba4\u77e5\u548c\u52a8\u4f5c\u7684\u5956\u52b1\u6a21\u578bPCogAlign\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u548c\u4eba\u7c7b\u8bc4\u4f30\u8bc1\u660e\u4e86PCogAlignBench\u7684\u53ef\u9760\u6027\u548cPCogAlign\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "PCogAlign\u6846\u67b6\u80fd\u591f\u6709\u6548\u5b9e\u73b0VLM\u7684\u4e2a\u6027\u5316\u5bf9\u9f50\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u4e2a\u6027\u5316\u5bf9\u9f50, \u89d2\u8272\u96c6, \u60c5\u5883\u8ba4\u77e5, \u57fa\u51c6\u8bc4\u6d4b"}}
{"id": "2506.00438", "pdf": "https://arxiv.org/pdf/2506.00438", "abs": "https://arxiv.org/abs/2506.00438", "authors": ["Keisuke Sugiura", "Mizuki Yasuda", "Hiroki Matsutani"], "title": "PointODE: Lightweight Point Cloud Learning with Neural Ordinary Differential Equations on Edge", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "Embedded edge devices are often used as a computing platform to run\nreal-world point cloud applications, but recent deep learning-based methods may\nnot fit on such devices due to limited resources. In this paper, we aim to fill\nthis gap by introducing PointODE, a parameter-efficient ResNet-like\narchitecture for point cloud feature extraction based on a stack of MLP blocks\nwith residual connections. We leverage Neural ODE (Ordinary Differential\nEquation), a continuous-depth version of ResNet originally developed for\nmodeling the dynamics of continuous-time systems, to compress PointODE by\nreusing the same parameters across MLP blocks. The point-wise normalization is\nproposed for PointODE to handle the non-uniform distribution of feature points.\nWe introduce PointODE-Elite as a lightweight version with 0.58M trainable\nparameters and design its dedicated accelerator for embedded FPGAs. The\naccelerator consists of a four-stage pipeline to parallelize the feature\nextraction for multiple points and stores the entire parameters on-chip to\neliminate most of the off-chip data transfers. Compared to the ARM Cortex-A53\nCPU, the accelerator implemented on a Xilinx ZCU104 board speeds up the feature\nextraction by 4.9x, leading to 3.7x faster inference and 3.5x better\nenergy-efficiency. Despite the simple architecture, PointODE-Elite shows\ncompetitive accuracy to the state-of-the-art models on both synthetic and\nreal-world classification datasets, greatly improving the trade-off between\naccuracy and inference cost.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PointODE\uff0c\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u57fa\u4e8e\u795e\u7ecfODE\u7684ResNet\u67b6\u6784\uff0c\u7528\u4e8e\u70b9\u4e91\u7279\u5f81\u63d0\u53d6\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u8fd0\u884c\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u795e\u7ecfODE\u538b\u7f29\u6a21\u578b\uff0c\u63d0\u51fa\u70b9\u5f52\u4e00\u5316\u6280\u672f\uff0c\u5e76\u8bbe\u8ba1\u4e13\u7528\u52a0\u901f\u5668\u3002", "result": "\u5728\u5d4c\u5165\u5f0fFPGA\u4e0a\u5b9e\u73b04.9\u500d\u52a0\u901f\uff0c3.7\u500d\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u548c3.5\u500d\u80fd\u6548\u63d0\u5347\u3002", "conclusion": "PointODE-Elite\u5728\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u4e86\u663e\u8457\u5e73\u8861\u3002", "keywords": "\u70b9\u4e91, \u795e\u7ecfODE, \u5d4c\u5165\u5f0f\u8bbe\u5907, FPGA, \u7279\u5f81\u63d0\u53d6"}}
{"id": "2506.00400", "pdf": "https://arxiv.org/pdf/2506.00400", "abs": "https://arxiv.org/abs/2506.00400", "authors": ["Zixin Ding", "Junyuan Hong", "Jiachen T. Wang", "Zinan Lin", "Zhangyang Wang", "Yuxin Chen"], "title": "Scaling Textual Gradients via Sampling-Based Momentum", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As prompts play an increasingly critical role in large language models\n(LLMs), optimizing textual prompts has become a crucial challenge. The Textual\nGradient Descent (TGD) framework has emerged as a promising data-driven\napproach that iteratively refines textual prompts using LLM - suggested updates\n(or textual gradients) over minibatches of training samples. In this paper, we\nempirically demonstrate that scaling the number of training examples initially\nimproves but later degrades TGD's performance across multiple downstream NLP\ntasks. However, while data scaling improves results for most tasks, it also\nsignificantly increases the computational cost when leveraging LLMs. To address\nthis, we draw inspiration from numerical gradient descent and propose Textual\nStochastic Gradient Descent with Momentum (TSGD-M) - a method that facilitates\nscalable in-context learning by reweighting prompt sampling based on past batch\ndistributions. Across nine NLP tasks spanning three domains - including\nBIG-Bench Hard (BBH), natural language understanding tasks, and reasoning tasks\n- TSGD-M significantly outperforms TGD baselines that do not incorporate\nreweighted sampling, while also reducing variance in most tasks.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0cTGD\u6846\u67b6\u5728\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u65f6\u6027\u80fd\u5148\u63d0\u5347\u540e\u4e0b\u964d\uff0c\u63d0\u51faTSGD-M\u65b9\u6cd5\u4ee5\u4f18\u5316\u63d0\u793a\u91c7\u6837\u5e76\u964d\u4f4e\u65b9\u5dee\u3002", "motivation": "\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u63d0\u793a\u662f\u91cd\u8981\u6311\u6218\uff0cTGD\u6846\u67b6\u867d\u6709\u6548\u4f46\u6570\u636e\u6269\u5c55\u4f1a\u964d\u4f4e\u6027\u80fd\u3002", "method": "\u63d0\u51faTSGD-M\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u52a0\u6743\u63d0\u793a\u91c7\u6837\u6a21\u62df\u6570\u503c\u68af\u5ea6\u4e0b\u964d\u7684\u52a8\u91cf\u673a\u5236\u3002", "result": "\u5728\u4e5d\u9879NLP\u4efb\u52a1\u4e2d\uff0cTSGD-M\u663e\u8457\u4f18\u4e8eTGD\u57fa\u7ebf\u5e76\u964d\u4f4e\u65b9\u5dee\u3002", "conclusion": "TSGD-M\u5728\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u7ed3\u679c\u7684\u4e0d\u7a33\u5b9a\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u6587\u672c\u63d0\u793a\u4f18\u5316,TGD,TSGD-M,NLP\u4efb\u52a1"}}
{"id": "2506.00958", "pdf": "https://arxiv.org/pdf/2506.00958", "abs": "https://arxiv.org/abs/2506.00958", "authors": ["Youngmin Kim", "Jiwan Chung", "Jisoo Kim", "Sunghyun Lee", "Sangkyu Lee", "Junhyeok Kim", "Cheoljong Yang", "Youngjae Yu"], "title": "Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "Accepted to ACL 2025 (Main), Our code and dataset:\n  https://github.com/winston1214/nonverbal-conversation", "summary": "Nonverbal communication is integral to human interaction, with gestures,\nfacial expressions, and body language conveying critical aspects of intent and\nemotion. However, existing large language models (LLMs) fail to effectively\nincorporate these nonverbal elements, limiting their capacity to create fully\nimmersive conversational experiences. We introduce MARS, a multimodal language\nmodel designed to understand and generate nonverbal cues alongside text,\nbridging this gap in conversational AI. Our key innovation is VENUS, a\nlarge-scale dataset comprising annotated videos with time-aligned text, facial\nexpressions, and body language. Leveraging VENUS, we train MARS with a\nnext-token prediction objective, combining text with vector-quantized nonverbal\nrepresentations to achieve multimodal understanding and generation within a\nunified framework. Based on various analyses of the VENUS datasets, we validate\nits substantial scale and high effectiveness. Our quantitative and qualitative\nresults demonstrate that MARS successfully generates text and nonverbal\nlanguages, corresponding to conversational input.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86MARS\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u7ed3\u5408\u6587\u672c\u548c\u975e\u8bed\u8a00\u7ebf\u7d22\uff08\u5982\u8868\u60c5\u548c\u80a2\u4f53\u8bed\u8a00\uff09\uff0c\u4ee5\u63d0\u5347\u5bf9\u8bddAI\u7684\u6c89\u6d78\u611f\u3002\u5173\u952e\u521b\u65b0\u662fVENUS\u6570\u636e\u96c6\u548c\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u7ed3\u5408\u975e\u8bed\u8a00\u5143\u7d20\uff0c\u9650\u5236\u4e86\u5bf9\u8bdd\u4f53\u9a8c\u7684\u771f\u5b9e\u6027\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528VENUS\u6570\u636e\u96c6\uff08\u5305\u542b\u6807\u6ce8\u89c6\u9891\u548c\u65f6\u95f4\u5bf9\u9f50\u7684\u6587\u672c\u3001\u8868\u60c5\u53ca\u80a2\u4f53\u8bed\u8a00\uff09\uff0c\u901a\u8fc7\u4e0b\u4e00\u8bcd\u9884\u6d4b\u76ee\u6807\u8bad\u7ec3MARS\u6a21\u578b\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u7ed3\u679c\u663e\u793a\uff0cMARS\u80fd\u6210\u529f\u751f\u6210\u4e0e\u5bf9\u8bdd\u8f93\u5165\u5bf9\u5e94\u7684\u6587\u672c\u548c\u975e\u8bed\u8a00\u8868\u8fbe\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "MARS\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u4e0e\u975e\u8bed\u8a00\u7ebf\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bddAI\u7684\u6c89\u6d78\u611f\u548c\u771f\u5b9e\u6027\u3002", "keywords": "\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b, \u975e\u8bed\u8a00\u4ea4\u6d41, \u5bf9\u8bddAI, VENUS\u6570\u636e\u96c6, MARS\u6a21\u578b"}}
{"id": "2506.00439", "pdf": "https://arxiv.org/pdf/2506.00439", "abs": "https://arxiv.org/abs/2506.00439", "authors": ["Yuqian Fu", "Yuanheng Zhu", "Jiajun Chai", "Guojun Yin", "Wei Lin", "Qichao Zhang", "Dongbin Zhao"], "title": "RLAE: Reinforcement Learning-Assisted Ensemble for LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Ensembling large language models (LLMs) can effectively combine diverse\nstrengths of different models, offering a promising approach to enhance\nperformance across various tasks. However, existing methods typically rely on\nfixed weighting strategies that fail to adapt to the dynamic, context-dependent\ncharacteristics of LLM capabilities. In this work, we propose Reinforcement\nLearning-Assisted Ensemble for LLMs (RLAE), a novel framework that reformulates\nLLM ensemble through the lens of a Markov Decision Process (MDP). Our approach\nintroduces a RL agent that dynamically adjusts ensemble weights by considering\nboth input context and intermediate generation states, with the agent being\ntrained using rewards that directly correspond to the quality of final outputs.\nWe implement RLAE using both single-agent and multi-agent reinforcement\nlearning algorithms ($\\text{RLAE}_\\text{PPO}$ and $\\text{RLAE}_\\text{MAPPO}$ ),\ndemonstrating substantial improvements over conventional ensemble methods.\nExtensive evaluations on a diverse set of tasks show that RLAE outperforms\nexisting approaches by up to $3.3\\%$ accuracy points, offering a more effective\nframework for LLM ensembling. Furthermore, our method exhibits superior\ngeneralization capabilities across different tasks without the need for\nretraining, while simultaneously achieving lower time latency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u6743\u91cd\u8c03\u6574\u6846\u67b6RLAE\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u5408\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u5408\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u6743\u91cd\u7b56\u7565\uff0c\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u8bed\u5883\u548c\u4efb\u52a1\u9700\u6c42\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5c06\u96c6\u5408\u95ee\u9898\u5efa\u6a21\uff0c\u5f15\u5165\u4e86\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u52a8\u6001\u8c03\u6574\u6743\u91cd\uff0c\u5e76\u4f7f\u7528\u5355\u4ee3\u7406\u548c\u591a\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5b9e\u73b0\u3002", "result": "\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\uff0cRLAE\u5c06\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u53473.3%\uff0c\u5e76\u5177\u5907\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u66f4\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "RLAE\u4e3a\u8bed\u8a00\u6a21\u578b\u96c6\u5408\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u52a8\u6001\u8c03\u6574\u6846\u67b6\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u5f3a\u5316\u5b66\u4e60, \u6a21\u578b\u96c6\u5408, \u52a8\u6001\u6743\u91cd, \u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b"}}
{"id": "2506.00402", "pdf": "https://arxiv.org/pdf/2506.00402", "abs": "https://arxiv.org/abs/2506.00402", "authors": ["Vishwanath Pratap Singh", "Md. Sahidullah", "Tomi Kinnunen"], "title": "Causal Structure Discovery for Error Diagnostics of Children's ASR", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Interspeech 2025", "summary": "Children's automatic speech recognition (ASR) often underperforms compared to\nthat of adults due to a confluence of interdependent factors: physiological\n(e.g., smaller vocal tracts), cognitive (e.g., underdeveloped pronunciation),\nand extrinsic (e.g., vocabulary limitations, background noise). Existing\nanalysis methods examine the impact of these factors in isolation, neglecting\ninterdependencies-such as age affecting ASR accuracy both directly and\nindirectly via pronunciation skills. In this paper, we introduce a causal\nstructure discovery to unravel these interdependent relationships among\nphysiology, cognition, extrinsic factors, and ASR errors. Then, we employ\ncausal quantification to measure each factor's impact on children's ASR. We\nextend the analysis to fine-tuned models to identify which factors are\nmitigated by fine-tuning and which remain largely unaffected. Experiments on\nWhisper and Wav2Vec2.0 demonstrate the generalizability of our findings across\ndifferent ASR systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u7ed3\u6784\u53d1\u73b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u513f\u7ae5\u8bed\u97f3\u8bc6\u522b\u8868\u73b0\u5dee\u7684\u751f\u7406\u3001\u8ba4\u77e5\u548c\u5916\u5728\u56e0\u7d20\u4e4b\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u56e0\u679c\u91cf\u5316\u8bc4\u4f30\u5404\u56e0\u7d20\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7ed3\u679c\u5728\u4e0d\u540cASR\u7cfb\u7edf\u4e2d\u7684\u666e\u9002\u6027\u3002", "motivation": "\u513f\u7ae5\u8bed\u97f3\u8bc6\u522b\u8868\u73b0\u8f83\u5dee\u7684\u539f\u56e0\u590d\u6742\u4e14\u76f8\u4e92\u4f9d\u8d56\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5206\u6790\u8fd9\u4e9b\u56e0\u7d20\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "method": "\u5f15\u5165\u56e0\u679c\u7ed3\u6784\u53d1\u73b0\u65b9\u6cd5\uff0c\u91cf\u5316\u751f\u7406\u3001\u8ba4\u77e5\u548c\u5916\u5728\u56e0\u7d20\u5bf9ASR\u9519\u8bef\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u5fae\u8c03\u6a21\u578b\u7684\u6539\u5584\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u56e0\u679c\u5206\u6790\u65b9\u6cd5\u80fd\u6709\u6548\u63ed\u793a\u5404\u56e0\u7d20\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u4e14\u7ed3\u679c\u9002\u7528\u4e8e\u4e0d\u540cASR\u7cfb\u7edf\u3002", "conclusion": "\u56e0\u679c\u5206\u6790\u4e3a\u6539\u5584\u513f\u7ae5\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u56e0\u7d20\u95f4\u7684\u4ea4\u4e92\u5efa\u6a21\u3002", "keywords": "\u513f\u7ae5\u8bed\u97f3\u8bc6\u522b,\u56e0\u679c\u5206\u6790,ASR\u9519\u8bef,\u5fae\u8c03\u6a21\u578b"}}
{"id": "2506.00965", "pdf": "https://arxiv.org/pdf/2506.00965", "abs": "https://arxiv.org/abs/2506.00965", "authors": ["Fan Liu", "Bikang Pan", "Zhongyi Wang", "Xi Yao", "Xiaoying Tang", "Jingya Wang", "Ye Shi"], "title": "Unlocking Personalized Knowledge in Federated Large Language Model: The Power of Mixture of Experts", "categories": ["cs.AI"], "comment": null, "summary": "The Mixture of Experts (MoE) architecture has emerged as a prominent strategy\nfor scaling large language models (LLMs), effectively leveraging sparse\nactivation and facilitating task-specific personalization. However, current\nfederated learning (FL) approaches are primarily designed for dense models,\nmaking them unable to directly exploit the sparsity inherent in MoE\narchitectures. Treating MoE models as dense networks in federated scenarios\nresults in excessive communication overhead and computational costs,\nundermining the potential for personalized knowledge sharing. To address these\nchallenges, we propose FLEx (Federated LLMs with Personalized Experts), a novel\nfederated learning framework explicitly tailored for MoE-based LLMs. FLEx\nefficiently personalizes by pruning the global MoE model to keep only one\nexpert per client, and employs an adaptive gating mechanism to reintegrate\nthese personalized experts into the pre-trained MoE layers, ensuring the\noriginal backbone architecture remains unchanged. These personalized experts\nare trained with local data and stored locally on each client, while the shared\nmodules are aggregated globally. Extensive evaluations on diverse\ninstruction-based datasets under non-IID conditions consistently demonstrate\nthat FLEx outperforms existing federated baselines. Our code is available at\nhttps://anonymous.4open.science/r/FLEx-8F12.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86FLEx\u6846\u67b6\uff0c\u4e13\u95e8\u4e3aMoE\u67b6\u6784\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u8054\u90a6\u5b66\u4e60\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u4e13\u5bb6\u548c\u81ea\u9002\u5e94\u95e8\u63a7\u673a\u5236\u964d\u4f4e\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8054\u90a6\u5b66\u4e60\u65b9\u6848\u4e3b\u8981\u9488\u5bf9\u5bc6\u96c6\u6a21\u578b\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528MoE\u67b6\u6784\u7684\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u5e76\u5f71\u54cd\u4e2a\u6027\u5316\u77e5\u8bc6\u5171\u4eab\u3002", "method": "FLEx\u6846\u67b6\u901a\u8fc7\u4fee\u526a\u5168\u5c40MoE\u6a21\u578b\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u4fdd\u7559\u4e00\u4e2a\u4e13\u5bb6\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u95e8\u63a7\u673a\u5236\u5c06\u4e2a\u6027\u5316\u4e13\u5bb6\u6574\u5408\u5230\u9884\u8bad\u7ec3MoE\u5c42\u4e2d\uff0c\u540c\u65f6\u5171\u4eab\u6a21\u5757\u5168\u5c40\u805a\u5408\u3002", "result": "\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFLEx\u4f18\u4e8e\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FLEx\u901a\u8fc7\u4e2a\u6027\u5316\u4e13\u5bb6\u548c\u7a00\u758f\u6027\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86MoE\u67b6\u6784\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6311\u6218\u3002", "keywords": "MoE, \u8054\u90a6\u5b66\u4e60, \u5927\u8bed\u8a00\u6a21\u578b, \u4e2a\u6027\u5316\u4e13\u5bb6, \u7a00\u758f\u6027"}}
{"id": "2506.00440", "pdf": "https://arxiv.org/pdf/2506.00440", "abs": "https://arxiv.org/abs/2506.00440", "authors": ["Daniel-M. Jimenez-Gutierrez", "David Solans", "Mohammed Elbamby", "Nicolas Kourtellis"], "title": "PSI-PFL: Population Stability Index for Client Selection in non-IID Personalized Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated Learning (FL) enables decentralized machine learning (ML) model\ntraining while preserving data privacy by keeping data localized across\nclients. However, non-independent and identically distributed (non-IID) data\nacross clients poses a significant challenge, leading to skewed model updates\nand performance degradation. Addressing this, we propose PSI-PFL, a novel\nclient selection framework for Personalized Federated Learning (PFL) that\nleverages the Population Stability Index (PSI) to quantify and mitigate data\nheterogeneity (so-called non-IIDness). Our approach selects more homogeneous\nclients based on PSI, reducing the impact of label skew, one of the most\ndetrimental factors in FL performance. Experimental results over multiple data\nmodalities (tabular, image, text) demonstrate that PSI-PFL significantly\nimproves global model accuracy, outperforming state-of-the-art baselines by up\nto 10\\% under non-IID scenarios while ensuring fairer local performance.\nPSI-PFL enhances FL performance and offers practical benefits in applications\nwhere data privacy and heterogeneity are critical.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86PSI-PFL\u6846\u67b6\uff0c\u901a\u8fc7PSI\u91cf\u5316\u6570\u636e\u5f02\u8d28\u6027\u5e76\u9009\u62e9\u540c\u8d28\u5316\u5ba2\u6237\u7aef\uff0c\u4ee5\u63d0\u5347\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u975eIID\u6570\u636e\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u975eIID\u6570\u636e\u5bfc\u81f4\u6a21\u578b\u66f4\u65b0\u504f\u5dee\u548c\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u6570\u636e\u5f02\u8d28\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faPSI-PFL\u6846\u67b6\uff0c\u5229\u7528PSI\u91cf\u5316\u6570\u636e\u5f02\u8d28\u6027\uff0c\u9009\u62e9\u540c\u8d28\u5316\u5ba2\u6237\u7aef\u4ee5\u51cf\u8f7b\u6807\u7b7e\u504f\u659c\u5bf9FL\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPSI-PFL\u5728\u591a\u79cd\u6570\u636e\u6a21\u6001\u4e0b\u663e\u8457\u63d0\u5347\u5168\u5c40\u6a21\u578b\u51c6\u786e\u6027\uff0c\u5e76\u5728\u975eIID\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd510%\u3002", "conclusion": "PSI-PFL\u4e0d\u4ec5\u63d0\u5347\u4e86FL\u6027\u80fd\uff0c\u8fd8\u5728\u6570\u636e\u9690\u79c1\u548c\u5f02\u8d28\u6027\u5173\u952e\u7684\u5e94\u7528\u4e2d\u5177\u5907\u5b9e\u7528\u4ef7\u503c\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60,\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60,\u975eIID\u6570\u636e,PSI,\u5ba2\u6237\u7aef\u9009\u62e9"}}
{"id": "2506.00413", "pdf": "https://arxiv.org/pdf/2506.00413", "abs": "https://arxiv.org/abs/2506.00413", "authors": ["Daniel Israel", "Guy Van den Broeck", "Aditya Grover"], "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "comment": "10 pages, 5 figures", "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u81ea\u9002\u5e94\u5e76\u884c\u89e3\u7801\uff08APD\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u5728\u5e76\u884c\u751f\u6210\u4ee4\u724c\u65f6\u901f\u5ea6\u4e0e\u8d28\u91cf\u7684\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u7684\u751f\u6210\u901f\u5ea6\u53d7\u9650\u4e8e\u81ea\u56de\u5f52\u89e3\u7801\u7684\u9010\u4ee4\u724c\u9884\u6d4b\uff0c\u800cdLLM\u7406\u8bba\u4e0a\u652f\u6301\u5e76\u884c\u4ee4\u724c\u751f\u6210\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u96be\u4ee5\u5728\u4e0d\u663e\u8457\u727a\u7272\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u81ea\u56de\u5f52\u6a21\u578b\u7684\u901f\u5ea6\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u4e00\u4e2adLLM\u8fb9\u9645\u6982\u7387\u4e0e\u5c0f\u578b\u8f85\u52a9\u81ea\u56de\u5f52\u6a21\u578b\u5e8f\u5217\u8054\u5408\u6982\u7387\u7684\u4e58\u6cd5\u6df7\u5408\uff0c\u52a8\u6001\u8c03\u6574\u5e76\u884c\u91c7\u6837\u7684\u4ee4\u724c\u6570\u91cf\u3002\u4f18\u5316\u5305\u62ec\u542f\u7528KV\u7f13\u5b58\u548c\u9650\u5236\u63a9\u7801\u8f93\u5165\u5927\u5c0f\u3002", "result": "APD\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u663e\u8457\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u4e14\u8d28\u91cf\u635f\u5931\u6781\u5c0f\u3002", "conclusion": "APD\u901a\u8fc7\u4e09\u4e2a\u53ef\u8c03\u53c2\u6570\u7075\u6d3b\u6743\u8861\u541e\u5410\u91cf\u548c\u8d28\u91cf\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u5e76\u884c\u89e3\u7801\u65b9\u6cd5\u3002", "keywords": "\u81ea\u9002\u5e94\u5e76\u884c\u89e3\u7801\uff08APD\uff09\uff0c\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\uff0c\u81ea\u56de\u5f52\u6a21\u578b\uff0cKV\u7f13\u5b58\uff0c\u541e\u5410\u91cf"}}
{"id": "2506.00968", "pdf": "https://arxiv.org/pdf/2506.00968", "abs": "https://arxiv.org/abs/2506.00968", "authors": ["Linhan Xia", "Mingzhan Yang", "Guohui Yuan", "Shengnan Tao", "Yujing Qiu", "Guo Yu", "Kai Lei"], "title": "PolyBERT: Fine-Tuned Poly Encoder BERT-Based Model for Word Sense Disambiguation", "categories": ["cs.AI"], "comment": null, "summary": "Mainstream Word Sense Disambiguation (WSD) approaches have employed BERT to\nextract semantics from both context and definitions of senses to determine the\nmost suitable sense of a target word, achieving notable performance. However,\nthere are two limitations in these approaches. First, previous studies failed\nto balance the representation of token-level (local) and sequence-level\n(global) semantics during feature extraction, leading to insufficient semantic\nrepresentation and a performance bottleneck. Second, these approaches\nincorporated all possible senses of each target word during the training phase,\nleading to unnecessary computational costs. To overcome these limitations, this\npaper introduces a poly-encoder BERT-based model with batch contrastive\nlearning for WSD, named PolyBERT. Compared with previous WSD methods, PolyBERT\nhas two improvements: (1) A poly-encoder with a multi-head attention mechanism\nis utilized to fuse token-level (local) and sequence-level (global) semantics,\nrather than focusing on just one. This approach enriches semantic\nrepresentation by balancing local and global semantics. (2) To avoid redundant\ntraining inputs, Batch Contrastive Learning (BCL) is introduced. BCL utilizes\nthe correct senses of other target words in the same batch as negative samples\nfor the current target word, which reduces training inputs and computational\ncost. The experimental results demonstrate that PolyBERT outperforms baseline\nWSD methods such as Huang's GlossBERT and Blevins's BEM by 2\\% in F1-score. In\naddition, PolyBERT with BCL reduces GPU hours by 37.6\\% compared with PolyBERT\nwithout BCL.", "AI": {"tldr": "PolyBERT\u878d\u5408\u591a\u5c3a\u5ea6\u8bed\u4e49\u5e76\u901a\u8fc7\u6279\u91cf\u5bf9\u6bd4\u5b66\u4e60\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u63d0\u5347WSD\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3WSD\u4e2d\u8bed\u4e49\u8868\u793a\u4e0d\u5e73\ufffd\u548c\u8ba1\u7b97\u5197\u4f59\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5e73\u8861\u5c40\u90e8\u548c\u5168\u5c40\u8bed\u4e49\uff0c\u5f15\u5165\u6279\u91cf\u5bf9\u6bd4\u5b66\u4e60\u51cf\u5c11\u8bad\u7ec3\u8f93\u5165\u3002", "result": "F1\u5206\u6570\u63d0\u53472%\uff0cGPU\u65f6\u95f4\u51cf\u5c1137.6%\u3002", "conclusion": "PolyBERT\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "keywords": "Word Sense Disambiguation, PolyBERT, batch contrastive learning, BERT"}}
{"id": "2506.00453", "pdf": "https://arxiv.org/pdf/2506.00453", "abs": "https://arxiv.org/abs/2506.00453", "authors": ["Hao Li", "Hao Wan", "Yuzhou Chen", "Dongsheng Ye", "Yulia Gel", "Hao Jiang"], "title": "TMetaNet: Topological Meta-Learning Framework for Dynamic Link Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "ICML2025", "summary": "Dynamic graphs evolve continuously, presenting challenges for traditional\ngraph learning due to their changing structures and temporal dependencies.\nRecent advancements have shown potential in addressing these challenges by\ndeveloping suitable meta-learning-based dynamic graph neural network models.\nHowever, most meta-learning approaches for dynamic graphs rely on fixed weight\nupdate parameters, neglecting the essential intrinsic complex high-order\ntopological information of dynamically evolving graphs. We have designed Dowker\nZigzag Persistence (DZP), an efficient and stable dynamic graph persistent\nhomology representation method based on Dowker complex and zigzag persistence,\nto capture the high-order features of dynamic graphs. Armed with the DZP ideas,\nwe propose TMetaNet, a new meta-learning parameter update model based on\ndynamic topological features. By utilizing the distances between high-order\ntopological features, TMetaNet enables more effective adaptation across\nsnapshots. Experiments on real-world datasets demonstrate TMetaNet's\nstate-of-the-art performance and resilience to graph noise, illustrating its\nhigh potential for meta-learning and dynamic graph analysis. Our code is\navailable at https://github.com/Lihaogx/TMetaNet.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u56fe\u6301\u4e45\u540c\u8c03\u7684\u8868\u793a\u65b9\u6cd5DZP\uff0c\u5e76\u7ed3\u5408\u52a8\u6001\u62d3\u6251\u7279\u5f81\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5143\u5b66\u4e60\u53c2\u6570\u66f4\u65b0\u6a21\u578bTMetaNet\uff0c\u6709\u6548\u63d0\u5347\u4e86\u52a8\u6001\u56fe\u5b66\u4e60\u7684\u6548\u679c\u3002", "motivation": "\u52a8\u6001\u56fe\u7684\u7ed3\u6784\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\u4e0d\u65ad\u53d8\u5316\uff0c\u4f20\u7edf\u56fe\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002\u73b0\u6709\u5143\u5b66\u4e60\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u52a8\u6001\u56fe\u7684\u9ad8\u9636\u62d3\u6251\u4fe1\u606f\u3002", "method": "\u8bbe\u8ba1\u4e86DZP\u65b9\u6cd5\u6355\u6349\u52a8\u6001\u56fe\u7684\u9ad8\u9636\u7279\u5f81\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51faTMetaNet\u6a21\u578b\uff0c\u5229\u7528\u9ad8\u9636\u62d3\u6251\u7279\u5f81\u8ddd\u79bb\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u8de8\u5feb\u7167\u9002\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTMetaNet\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u5bf9\u56fe\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "TMetaNet\u5c55\u793a\u4e86\u5728\u5143\u5b66\u4e60\u548c\u52a8\u6001\u56fe\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "\u52a8\u6001\u56fe\u5b66\u4e60, \u5143\u5b66\u4e60, \u6301\u4e45\u540c\u8c03, \u62d3\u6251\u7279\u5f81"}}
{"id": "2506.00418", "pdf": "https://arxiv.org/pdf/2506.00418", "abs": "https://arxiv.org/abs/2506.00418", "authors": ["Siqi Liang", "Sumyeong Ahn", "Paramveer S. Dhillon", "Jiayu Zhou"], "title": "Dual Debiasing for Noisy In-Context Learning for Text Generation", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "Accepted by 2025 ACL Findings", "summary": "In context learning (ICL) relies heavily on high quality demonstrations drawn\nfrom large annotated corpora. Existing approaches detect noisy annotations by\nranking local perplexities, presuming that noisy samples yield higher\nperplexities than their clean counterparts. However, this assumption breaks\ndown when the noise ratio is high and many demonstrations are flawed. We\nreexamine the perplexity based paradigm for text generation under noisy\nannotations, highlighting two sources of bias in perplexity: the annotation\nitself and the domain specific knowledge inherent in large language models\n(LLMs). To overcome these biases, we introduce a dual debiasing framework that\nuses synthesized neighbors to explicitly correct perplexity estimates, yielding\na robust Sample Cleanliness Score. This metric uncovers absolute sample\ncleanliness regardless of the overall corpus noise level. Extensive experiments\ndemonstrate our method's superior noise detection capabilities and show that\nits final ICL performance is comparable to that of a fully clean demonstration\ncorpus. Moreover, our approach remains robust even when noise ratios are\nextremely high.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u90bb\u5c45\u6765\u4fee\u6b63\u56f0\u60d1\u5ea6\u4f30\u8ba1\uff0c\u4ece\u800c\u751f\u6210\u9c81\u68d2\u7684\u6837\u672c\u6e05\u6d01\u5ea6\u8bc4\u5206\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u566a\u58f0\u6761\u4ef6\u4e0bICL\u7684\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5c40\u90e8\u56f0\u60d1\u5ea6\u6392\u540d\u6765\u68c0\u6d4b\u566a\u58f0\u6807\u6ce8\uff0c\u4f46\u8fd9\u4e00\u5047\u8bbe\u5728\u9ad8\u566a\u58f0\u6bd4\u4f8b\u4e0b\u5931\u6548\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u56f0\u60d1\u5ea6\u7684\u4e24\u4e2a\u504f\u5dee\u6e90\uff0c\u5e76\u63d0\u51fa\u66f4\u7a33\u5065\u7684\u566a\u58f0\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u53cc\u53bb\u504f\u6846\u67b6\uff0c\u5229\u7528\u5408\u6210\u90bb\u5c45\u663e\u5f0f\u4fee\u6b63\u56f0\u60d1\u5ea6\u4f30\u8ba1\uff0c\u751f\u6210\u6837\u672c\u6e05\u6d01\u5ea6\u8bc4\u5206\uff08SCS\uff09\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u566a\u58f0\u68c0\u6d4b\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u9ad8\u566a\u58f0\u6bd4\u4f8b\u4e0b\u4ecd\u4fdd\u6301\u7a33\u5065\uff0c\u6700\u7ec8ICL\u6027\u80fd\u63a5\u8fd1\u5b8c\u5168\u6e05\u6d01\u7684\u6f14\u793a\u8bed\u6599\u5e93\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cc\u53bb\u504f\u6846\u67b6\u80fd\u6709\u6548\u514b\u670d\u56f0\u60d1\u5ea6\u7684\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347ICL\u5728\u9ad8\u566a\u58f0\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "keywords": "In-Context Learning, noise detection, perplexity bias, dual debiasing, sample cleanliness"}}
{"id": "2506.00989", "pdf": "https://arxiv.org/pdf/2506.00989", "abs": "https://arxiv.org/abs/2506.00989", "authors": ["Buyun He", "Xiaorui Jiang", "Qi Wu", "Hao Liu", "Yingguang Yang", "Yong Liao"], "title": "Boosting Bot Detection via Heterophily-Aware Representation Learning and Prototype-Guided Cluster Discovery", "categories": ["cs.AI"], "comment": "KDD 2025", "summary": "Detecting social media bots is essential for maintaining the security and\ntrustworthiness of social networks. While contemporary graph-based detection\nmethods demonstrate promising results, their practical application is limited\nby label reliance and poor generalization capability across diverse\ncommunities. Generative Graph Self-Supervised Learning (GSL) presents a\npromising paradigm to overcome these limitations, yet existing approaches\npredominantly follow the homophily assumption and fail to capture the global\npatterns in the graph, which potentially diminishes their effectiveness when\nfacing the challenges of interaction camouflage and distributed deployment in\nbot detection scenarios. To this end, we propose BotHP, a generative GSL\nframework tailored to boost graph-based bot detectors through heterophily-aware\nrepresentation learning and prototype-guided cluster discovery. Specifically,\nBotHP leverages a dual-encoder architecture, consisting of a graph-aware\nencoder to capture node commonality and a graph-agnostic encoder to preserve\nnode uniqueness. This enables the simultaneous modeling of both homophily and\nheterophily, effectively countering the interaction camouflage issue.\nAdditionally, BotHP incorporates a prototype-guided cluster discovery pretext\ntask to model the latent global consistency of bot clusters and identify\nspatially dispersed yet semantically aligned bot collectives. Extensive\nexperiments on two real-world bot detection benchmarks demonstrate that BotHP\nconsistently boosts graph-based bot detectors, improving detection performance,\nalleviating label reliance, and enhancing generalization capability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBotHP\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6784\u611f\u77e5\u8868\u793a\u5b66\u4e60\u548c\u539f\u578b\u5f15\u5bfc\u7684\u805a\u7c7b\u53d1\u73b0\uff0c\u63d0\u5347\u57fa\u4e8e\u56fe\u7684\u793e\u4ea4\u673a\u5668\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u7684\u793e\u4ea4\u673a\u5668\u4eba\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u6807\u7b7e\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u800c\u73b0\u6709\u7684\u751f\u6210\u56fe\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u672a\u80fd\u6355\u6349\u5168\u5c40\u6a21\u5f0f\uff0c\u96be\u4ee5\u5e94\u5bf9\u4ea4\u4e92\u4f2a\u88c5\u548c\u5206\u5e03\u5f0f\u90e8\u7f72\u7684\u6311\u6218\u3002", "method": "BotHP\u91c7\u7528\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5305\u62ec\u56fe\u611f\u77e5\u7f16\u7801\u5668\u548c\u56fe\u4e0d\u53ef\u77e5\u7f16\u7801\u5668\uff0c\u540c\u65f6\u5efa\u6a21\u540c\u8d28\u6027\u548c\u5f02\u8d28\u6027\uff0c\u5e76\u901a\u8fc7\u539f\u578b\u5f15\u5bfc\u7684\u805a\u7c7b\u53d1\u73b0\u4efb\u52a1\u8bc6\u522b\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBotHP\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u6807\u7b7e\u4f9d\u8d56\u5e76\u589e\u5f3a\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "BotHP\u901a\u8fc7\u5f02\u6784\u611f\u77e5\u548c\u5168\u5c40\u4e00\u81f4\u6027\u7684\u5efa\u6a21\uff0c\u4e3a\u793e\u4ea4\u673a\u5668\u4eba\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u793e\u4ea4\u673a\u5668\u4eba, \u56fe\u81ea\u76d1\u7763\u5b66\u4e60, \u5f02\u6784\u611f\u77e5, \u539f\u578b\u805a\u7c7b"}}
{"id": "2506.00457", "pdf": "https://arxiv.org/pdf/2506.00457", "abs": "https://arxiv.org/abs/2506.00457", "authors": ["Junwoo Park", "Hyuck Lee", "Dohyun Lee", "Daehoon Gwak", "Jaegul Choo"], "title": "Revisiting LLMs as Zero-Shot Time-Series Forecasters: Small Noise Can Break Large Models", "categories": ["cs.LG"], "comment": "Annual Meeting of the Association for Computational Linguistics\n  (ACL), 2025, Accepted as Short Paper", "summary": "Large Language Models (LLMs) have shown remarkable performance across diverse\ntasks without domain-specific training, fueling interest in their potential for\ntime-series forecasting. While LLMs have shown potential in zero-shot\nforecasting through prompting alone, recent studies suggest that LLMs lack\ninherent effectiveness in forecasting. Given these conflicting findings, a\nrigorous validation is essential for drawing reliable conclusions. In this\npaper, we evaluate the effectiveness of LLMs as zero-shot forecasters compared\nto state-of-the-art domain-specific models. Our experiments show that LLM-based\nzero-shot forecasters often struggle to achieve high accuracy due to their\nsensitivity to noise, underperforming even simple domain-specific models. We\nhave explored solutions to reduce LLMs' sensitivity to noise in the zero-shot\nsetting, but improving their robustness remains a significant challenge. Our\nfindings suggest that rather than emphasizing zero-shot forecasting, a more\npromising direction would be to focus on fine-tuning LLMs to better process\nnumerical sequences. Our experimental code is available at\nhttps://github.com/junwoopark92/revisiting-LLMs-zeroshot-forecaster.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u6548\u679c\u4e0d\u5982\u9886\u57df\u4e13\u7528\u6a21\u578b\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u96f6\u6837\u672c\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e2d\u77db\u76fe\u7684\u7ed3\u8bba\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83LLMs\u4e0e\u6700\u5148\u8fdb\u7684\u9886\u57df\u4e13\u7528\u6a21\u578b\u5728\u96f6\u6837\u672c\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "result": "LLMs\u5bf9\u566a\u58f0\u654f\u611f\uff0c\u9884\u6d4b\u51c6\u786e\u7387\u8f83\u4f4e\uff0c\u751a\u81f3\u4e0d\u5982\u7b80\u5355\u7684\u9886\u57df\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u66f4\u591a\u5173\u6ce8\u5982\u4f55\u901a\u8fc7\u5fae\u8c03LLMs\u6765\u63d0\u9ad8\u5176\u5bf9\u6570\u503c\u5e8f\u5217\u7684\u5904\u7406\u80fd\u529b\u3002", "keywords": "LLMs, \u65f6\u95f4\u5e8f\u5217\u9884\u6d4b, \u96f6\u6837\u672c\u5b66\u4e60, \u566a\u58f0\u654f\u611f"}}
{"id": "2506.00421", "pdf": "https://arxiv.org/pdf/2506.00421", "abs": "https://arxiv.org/abs/2506.00421", "authors": ["Jihyoung Jang", "Minwook Bae", "Minji Kim", "Dilek Hakkani-Tur", "Hyounghun Kim"], "title": "Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ACL 2025 (32 pages); Project website: https://m3c-dataset.github.io/", "summary": "As chatbots continue to evolve toward human-like, real-world, interactions,\nmultimodality remains an active area of research and exploration. So far,\nefforts to integrate multimodality into chatbots have primarily focused on\nimage-centric tasks, such as visual dialogue and image-based instructions,\nplacing emphasis on the \"eyes\" of human perception while neglecting the \"ears\",\nnamely auditory aspects. Moreover, these studies often center around static\ninteractions that focus on discussing the modality rather than naturally\nincorporating it into the conversation, which limits the richness of\nsimultaneous, dynamic engagement. Furthermore, while multimodality has been\nexplored in multi-party and multi-session conversations, task-specific\nconstraints have hindered its seamless integration into dynamic, natural\nconversations. To address these challenges, this study aims to equip chatbots\nwith \"eyes and ears\" capable of more immersive interactions with humans. As\npart of this effort, we introduce a new multimodal conversation dataset,\nMultimodal Multi-Session Multi-Party Conversation ($M^3C$), and propose a novel\nmultimodal conversation model featuring multimodal memory retrieval. Our model,\ntrained on the $M^3C$, demonstrates the ability to seamlessly engage in\nlong-term conversations with multiple speakers in complex, real-world-like\nsettings, effectively processing visual and auditory inputs to understand and\nrespond appropriately. Human evaluations highlight the model's strong\nperformance in maintaining coherent and dynamic interactions, demonstrating its\npotential for advanced multimodal conversational agents.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u5bf9\u8bdd\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u542c\u89c9\u8f93\u5165\uff0c\u63d0\u5347\u804a\u5929\u673a\u5668\u4eba\u7684\u6c89\u6d78\u5f0f\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u804a\u5929\u673a\u5668\u4eba\u5728\u591a\u6a21\u6001\u4ea4\u4e92\u4e2d\u504f\u91cd\u89c6\u89c9\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u542c\u89c9\u65b9\u9762\uff0c\u4e14\u4ea4\u4e92\u591a\u4e3a\u9759\u6001\u800c\u975e\u52a8\u6001\u878d\u5165\u5bf9\u8bdd\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u7684\u4e30\u5bcc\u6027\u3002", "method": "\u5f15\u5165\u65b0\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u6570\u636e\u96c6$M^3C$\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u8bb0\u5fc6\u68c0\u7d22\u7684\u5bf9\u8bdd\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u80fd\u591f\u65e0\u7f1d\u5904\u7406\u89c6\u89c9\u548c\u542c\u89c9\u8f93\u5165\uff0c\u7ef4\u6301\u8fde\u8d2f\u7684\u52a8\u6001\u5bf9\u8bdd\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u591a\u6a21\u6001\u5bf9\u8bdd\u4ee3\u7406\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u6280\u672f\u652f\u6301\uff0c\u63a8\u52a8\u4e86\u66f4\u81ea\u7136\u7684\u4ea4\u4e92\u53d1\u5c55\u3002", "keywords": "\u591a\u6a21\u6001\u804a\u5929\u673a\u5668\u4eba, \u89c6\u89c9\u4e0e\u542c\u89c9, $M^3C$\u6570\u636e\u96c6, \u52a8\u6001\u5bf9\u8bdd, \u6c89\u6d78\u5f0f\u4ea4\u4e92"}}
{"id": "2506.01003", "pdf": "https://arxiv.org/pdf/2506.01003", "abs": "https://arxiv.org/abs/2506.01003", "authors": ["Junli Jiang", "Pavel Naumov"], "title": "Higher-Order Responsibility", "categories": ["cs.AI", "cs.CC", "cs.GT"], "comment": null, "summary": "In ethics, individual responsibility is often defined through Frankfurt's\nprinciple of alternative possibilities. This definition is not adequate in a\ngroup decision-making setting because it often results in the lack of a\nresponsible party or \"responsibility gap''. One of the existing approaches to\naddress this problem is to consider group responsibility. Another, recently\nproposed, approach is \"higher-order'' responsibility. The paper considers the\nproblem of deciding if higher-order responsibility up to degree $d$ is enough\nto close the responsibility gap. The main technical result is that this problem\nis $\\Pi_{2d+1}$-complete.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u7fa4\u4f53\u51b3\u7b56\u4e2d\u9ad8\u9636\u8d23\u4efb\u662f\u5426\u80fd\u586b\u8865\u8d23\u4efb\u7a7a\u767d\uff0c\u5e76\u8bc1\u660e\u8be5\u95ee\u9898\u5c5e\u4e8e$\u03a0_{2d+1}$-\u5b8c\u5168\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5f17\u5170\u514b\u798f\u7279\u66ff\u4ee3\u53ef\u80fd\u6027\u539f\u5219\u5728\u7fa4\u4f53\u51b3\u7b56\u4e2d\u65e0\u6cd5\u51c6\u786e\u5b9a\u4e49\u8d23\u4efb\uff0c\u5bfc\u81f4\u8d23\u4efb\u7a7a\u767d\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u9ad8\u9636\u8d23\u4efb\u662f\u5426\u8db3\u4ee5\u586b\u8865\u8d23\u4efb\u7a7a\u767d\uff0c\u5e76\u5206\u6790\u5176\u8ba1\u7b97\u590d\u6742\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5224\u5b9a\u9ad8\u9636\u8d23\u4efb\u662f\u5426\u80fd\u586b\u8865\u8d23\u4efb\u7a7a\u767d\u7684\u95ee\u9898\u662f$\u03a0_{2d+1}$-\u5b8c\u5168\u7684\u3002", "conclusion": "\u9ad8\u9636\u8d23\u4efb\u4e3a\u89e3\u51b3\u7fa4\u4f53\u51b3\u7b56\u4e2d\u7684\u8d23\u4efb\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4f46\u5176\u8ba1\u7b97\u590d\u6742\u6027\u8f83\u9ad8\u3002", "keywords": "\u8d23\u4efb\u7a7a\u767d\u3001\u9ad8\u9636\u8d23\u4efb\u3001\u7fa4\u4f53\u51b3\u7b56\u3001\u8ba1\u7b97\u590d\u6742\u6027\u3001\u4f26\u7406\u5b66"}}
{"id": "2506.00458", "pdf": "https://arxiv.org/pdf/2506.00458", "abs": "https://arxiv.org/abs/2506.00458", "authors": ["Nina Cohen", "Kordel K. France"], "title": "Reinforcement Learning for Hanabi", "categories": ["cs.LG", "cs.AI", "cs.GT", "cs.MA"], "comment": null, "summary": "Hanabi has become a popular game for research when it comes to reinforcement\nlearning (RL) as it is one of the few cooperative card games where you have\nincomplete knowledge of the entire environment, thus presenting a challenge for\na RL agent. We explored different tabular and deep reinforcement learning\nalgorithms to see which had the best performance both against an agent of the\nsame type and also against other types of agents. We establish that certain\nagents played their highest scoring games against specific agents while others\nexhibited higher scores on average by adapting to the opposing agent's\nbehavior. We attempted to quantify the conditions under which each algorithm\nprovides the best advantage and identified the most interesting interactions\nbetween agents of different types. In the end, we found that temporal\ndifference (TD) algorithms had better overall performance and balancing of play\ntypes compared to tabular agents. Specifically, tabular Expected SARSA and deep\nQ-Learning agents showed the best performance.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u8868\u683c\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728Hanabi\u6e38\u620f\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0TD\u7b97\u6cd5\u603b\u4f53\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u662f\u8868\u683cExpected SARSA\u548c\u6df1\u5ea6Q-Learning\u4ee3\u7406\u3002", "motivation": "Hanabi\u4f5c\u4e3a\u4e0d\u5b8c\u5168\u4fe1\u606f\u5408\u4f5c\u7684\u5361\u724c\u6e38\u620f\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u72ec\u7279\u6311\u6218\u3002", "method": "\u6bd4\u8f83\u4e86\u4e0d\u540c\u7c7b\u578b\u7684\u8868\u683c\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u53ca\u5176\u5728\u4e0d\u540c\u5bf9\u624b\u95f4\u7684\u8868\u73b0\u3002", "result": "TD\u7b97\u6cd5\u8868\u73b0\u6700\u4f18\uff0c\u5c24\u5176\u662f\u8868\u683cExpected SARSA\u548c\u6df1\u5ea6Q-Learning\u4ee3\u7406\u3002", "conclusion": "TD\u7b97\u6cd5\u5728\u8868\u73b0\u548c\u5e73\u8861\u6027\u4e0a\u4f18\u4e8e\u8868\u683c\u4ee3\u7406\uff0c\u67d0\u4e9b\u4ee3\u7406\u5728\u4e0e\u7279\u5b9a\u5bf9\u624b\u5bf9\u6297\u65f6\u8868\u73b0\u66f4\u4f73\u3002", "keywords": "Hanabi, reinforcement learning, TD algorithms, Expected SARSA, deep Q-Learning"}}
{"id": "2506.00422", "pdf": "https://arxiv.org/pdf/2506.00422", "abs": "https://arxiv.org/abs/2506.00422", "authors": ["Yui Sudo", "Yosuke Fukumoto", "Muhammad Shakeel", "Yifan Peng", "Chyi-Jiunn Lin", "Shinji Watanabe"], "title": "DYNAC: Dynamic Vocabulary based Non-Autoregressive Contextualization for Speech Recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Contextual biasing (CB) improves automatic speech recognition for rare and\nunseen phrases. Recent studies have introduced dynamic vocabulary, which\nrepresents context phrases as expandable tokens in autoregressive (AR) models.\nThis method improves CB accuracy but with slow inference speed. While dynamic\nvocabulary can be applied to non-autoregressive (NAR) models, such as\nconnectionist temporal classification (CTC), the conditional independence\nassumption fails to capture dependencies between static and dynamic tokens.\nThis paper proposes DYNAC (Dynamic Vocabulary-based NAR Contextualization), a\nself-conditioned CTC method that integrates dynamic vocabulary into\nintermediate layers. Conditioning the encoder on dynamic vocabulary, DYNAC\neffectively captures dependencies between static and dynamic tokens while\nreducing the real-time factor (RTF). Experimental results show that DYNAC\nreduces RTF by 81% with a 0.1-point degradation in word error rate on the\nLibriSpeech 960 test-clean set.", "AI": {"tldr": "DYNAC\u662f\u4e00\u79cd\u81ea\u8c03\u8282CTC\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8bcd\u6c47\u96c6\u6210\u5230\u4e2d\u95f4\u5c42\uff0c\u63d0\u5347\u975e\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u4e0a\u4e0b\u6587\u504f\u5dee\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u52a8\u6001\u8bcd\u6c47\u5bfc\u81f4\u7684\u63a8\u7406\u901f\u5ea6\u6162\u95ee\u9898\uff0c\u5e76\u514b\u670d\u975e\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u9759\u6001\u548c\u52a8\u6001\u8bcd\u6c47\u95f4\u4f9d\u8d56\u5173\u7cfb\u96be\u4ee5\u6355\u6349\u7684\u7f3a\u9677\u3002", "method": "\u63d0\u51faDYNAC\u65b9\u6cd5\uff0c\u5c06\u52a8\u6001\u8bcd\u6c47\u96c6\u6210\u5230\u81ea\u8c03\u8282CTC\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\uff0c\u901a\u8fc7\u8c03\u8282\u7f16\u7801\u5668\u6355\u6349\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728LibriSpeech\u6d4b\u8bd5\u96c6\u4e0a\uff0cDYNAC\u964d\u4f4e81%\u7684\u5b9e\u65f6\u56e0\u5b50\uff0c\u4ec5\u4f7f\u8bcd\u9519\u7387\u589e\u52a00.1%\u3002", "conclusion": "DYNAC\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002", "keywords": "DYNAC, \u52a8\u6001\u8bcd\u6c47, \u975e\u81ea\u56de\u5f52\u6a21\u578b, CTC, \u4e0a\u4e0b\u6587\u504f\u5dee"}}
{"id": "2506.01048", "pdf": "https://arxiv.org/pdf/2506.01048", "abs": "https://arxiv.org/abs/2506.01048", "authors": ["Wei Song", "Zhenya Huang", "Cheng Cheng", "Weibo Gao", "Bihan Xu", "GuanHao Zhao", "Fei Wang", "Runze Wu"], "title": "IRT-Router: Effective and Interpretable Multi-LLM Routing via Item Response Theory", "categories": ["cs.AI"], "comment": "ACL 2025 Main", "summary": "Large language models (LLMs) have demonstrated exceptional performance across\na wide range of natural language tasks. However, selecting the optimal LLM to\nrespond to a user query often necessitates a delicate balance between\nperformance and cost. While powerful models deliver better results, they come\nat a high cost, whereas smaller models are more cost-effective but less\ncapable. To address this trade-off, we propose IRT-Router, a multi-LLM routing\nframework that efficiently routes user queries to the most suitable LLM.\nInspired by Item Response Theory (IRT), a psychological measurement\nmethodology, IRT-Router explicitly models the relationship between LLM\ncapabilities and user query attributes. This not only enables accurate\nprediction of response performance but also provides interpretable insights,\nsuch as LLM abilities and query difficulty. Additionally, we design an online\nquery warm-up technique based on semantic similarity, further enhancing the\nonline generalization capability of IRT-Router. Extensive experiments on 20\nLLMs and 12 datasets demonstrate that IRT-Router outperforms most baseline\nmethods in terms of effectiveness and interpretability. Its superior\nperformance in cold-start scenarios further confirms the reliability and\npracticality of IRT-Router in real-world applications. Code is available at\nhttps://github.com/Mercidaiha/IRT-Router.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faIRT-Router\uff0c\u4e00\u4e2a\u57fa\u4e8eItem Response Theory\u7684\u591aLLM\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u80fd\u529b\u4e0e\u67e5\u8be2\u5c5e\u6027\u7684\u5173\u7cfb\u4f18\u5316\u67e5\u8be2\u5206\u914d\uff0c\u5e73\u8861\u6027\u80fd\u4e0e\u6210\u672c\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9009\u62e9\u6700\u4f18\u6a21\u578b\u65f6\u9700\u6743\u8861\u6027\u80fd\u4e0e\u6210\u672c\u3002", "method": "\u91c7\u7528Item Response Theory\uff0c\u8bbe\u8ba1IRT-Router\u6846\u67b6\uff0c\u7ed3\u5408\u5728\u7ebf\u67e5\u8be2\u9884\u70ed\u6280\u672f\u3002", "result": "\u572820\u4e2aLLM\u548c12\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cIRT-Router\u5728\u6548\u679c\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "IRT-Router\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "keywords": "LLM, IRT-Router, \u8def\u7531\u6846\u67b6, \u51b7\u542f\u52a8, \u53ef\u89e3\u91ca\u6027"}}
{"id": "2506.00459", "pdf": "https://arxiv.org/pdf/2506.00459", "abs": "https://arxiv.org/abs/2506.00459", "authors": ["Elinor Ginzburg", "Itay Segev", "Yoash Levron", "Sarah Keren"], "title": "Comparing Traditional and Reinforcement-Learning Methods for Energy Storage Control", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We aim to better understand the tradeoffs between traditional and\nreinforcement learning (RL) approaches for energy storage management. More\nspecifically, we wish to better understand the performance loss incurred when\nusing a generative RL policy instead of using a traditional approach to find\noptimal control policies for specific instances. Our comparison is based on a\nsimplified micro-grid model, that includes a load component, a photovoltaic\nsource, and a storage device. Based on this model, we examine three use cases\nof increasing complexity: ideal storage with convex cost functions, lossy\nstorage devices, and lossy storage devices with convex transmission losses.\nWith the aim of promoting the principled use RL based methods in this\nchallenging and important domain, we provide a detailed formulation of each use\ncase and a detailed description of the optimization challenges. We then compare\nthe performance of traditional and RL methods, discuss settings in which it is\nbeneficial to use each method, and suggest avenues for future investigation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e0e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u80fd\u91cf\u5b58\u50a8\u7ba1\u7406\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u63a2\u8ba8\u4e86RL\u7b56\u7565\u5728\u7b80\u5316\u7684\u5fae\u7535\u7f51\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u7d22\u4f20\u7edf\u65b9\u6cd5\u4e0e\u5f3a\u5316\u5b66\u4e60\u5728\u80fd\u91cf\u5b58\u50a8\u7ba1\u7406\u4e2d\u7684\u6743\u8861\uff0c\u5206\u6790RL\u7b56\u7565\u7684\u6027\u80fd\u635f\u5931\u53ca\u5176\u9002\u7528\u573a\u666f\u3002", "method": "\u4f7f\u7528\u5305\u542b\u8d1f\u8f7d\u3001\u5149\u4f0f\u7535\u6e90\u548c\u5b58\u50a8\u8bbe\u5907\u7684\u7b80\u5316\u5fae\u7535\u7f51\u6a21\u578b\uff0c\u6bd4\u8f83\u4e09\u79cd\u590d\u6742\u6027\u9012\u589e\u7684\u7528\u4f8b\uff08\u7406\u60f3\u5b58\u50a8\u3001\u6709\u635f\u5b58\u50a8\u548c\u6709\u635f\u5b58\u50a8\u52a0\u4f20\u8f93\u635f\u8017\uff09\u4e2d\u4f20\u7edf\u65b9\u6cd5\u4e0eRL\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u573a\u666f\u4e0b\uff0c\u4f20\u7edf\u65b9\u6cd5\u4e0eRL\u5404\u6709\u4f18\u52bf\uff0c\u5e76\u6307\u51fa\u4e86\u5404\u81ea\u7684\u9002\u7528\u60c5\u51b5\u3002", "conclusion": "\u7814\u7a76\u4e3aRL\u5728\u80fd\u91cf\u5b58\u50a8\u7ba1\u7406\u4e2d\u7684\u5408\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "keywords": "\u80fd\u91cf\u5b58\u50a8\u7ba1\u7406\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u4f20\u7edf\u65b9\u6cd5\u3001\u5fae\u7535\u7f51\u6a21\u578b\u3001\u6027\u80fd\u6bd4\u8f83"}}
{"id": "2506.00425", "pdf": "https://arxiv.org/pdf/2506.00425", "abs": "https://arxiv.org/abs/2506.00425", "authors": ["Bingsen Chen", "Shengjie Wang", "Xi Ye", "Chen Zhao"], "title": "Inter-Passage Verification for Multi-evidence Multi-answer QA", "categories": ["cs.CL"], "comment": "19 pages, 6 figures, to appear in ACL 2025 Findings", "summary": "Multi-answer question answering (QA), where questions can have many valid\nanswers, presents a significant challenge for existing retrieval-augmented\ngeneration-based QA systems, as these systems struggle to retrieve and then\nsynthesize a large number of evidence passages. To tackle these challenges, we\npropose a new multi-answer QA framework -- Retrieval-augmented Independent\nReading with Inter-passage Verification (RI$^2$VER). Our framework retrieves a\nlarge set of passages and processes each passage individually to generate an\ninitial high-recall but noisy answer set. Then we propose a new inter-passage\nverification pipeline that validates every candidate answer through (1)\nVerification Question Generation, (2) Gathering Additional Evidence, and (3)\nVerification with inter-passage synthesis. Evaluations on the QAMPARI and RoMQA\ndatasets demonstrate that our framework significantly outperforms existing\nbaselines across various model sizes, achieving an average F1 score improvement\nof 11.17%. Further analysis validates that our inter-passage verification\npipeline enables our framework to be particularly beneficial for questions\nrequiring multi-evidence synthesis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRI$^2$VER\u7684\u591a\u7b54\u6848\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u548c\u9a8c\u8bc1\u6d41\u7a0b\u663e\u8457\u63d0\u5347\u591a\u7b54\u6848\u95ee\u9898\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5f0fQA\u7cfb\u7edf\u5728\u591a\u7b54\u6848\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5927\u91cf\u8bc1\u636e\u6bb5\u843d\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u72ec\u7acb\u9605\u8bfb\u548c\u6bb5\u843d\u95f4\u9a8c\u8bc1\uff08RI$^2$VER\uff09\uff0c\u5305\u62ec\u751f\u6210\u5019\u9009\u7b54\u6848\u96c6\u3001\u9a8c\u8bc1\u95ee\u9898\u751f\u6210\u3001\u8865\u5145\u8bc1\u636e\u6536\u96c6\u53ca\u6bb5\u843d\u95f4\u5408\u6210\u9a8c\u8bc1\u3002", "result": "\u5728QAMPARI\u548cRoMQA\u6570\u636e\u96c6\u4e0a\uff0c\u5e73\u5747F1\u5206\u6570\u63d0\u534711.17%\uff0c\u5c24\u5176\u5728\u9700\u8981\u591a\u91cd\u8bc1\u636e\u5408\u6210\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "RI$^2$VER\u6846\u67b6\u901a\u8fc7\u9a8c\u8bc1\u6d41\u7a0b\u6709\u6548\u89e3\u51b3\u4e86\u591a\u7b54\u6848QA\u4e2d\u7684\u68c0\u7d22\u548c\u5408\u6210\u96be\u9898\u3002", "keywords": "\u591a\u7b54\u6848\u95ee\u7b54\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u6bb5\u843d\u9a8c\u8bc1\u3001RI$^2$VER"}}
{"id": "2506.01056", "pdf": "https://arxiv.org/pdf/2506.01056", "abs": "https://arxiv.org/abs/2506.01056", "authors": ["Xiang Fei", "Xiawu Zheng", "Hao Feng"], "title": "MCP-Zero: Proactive Toolchain Construction for LLM Agents from Scratch", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Function-calling has enabled large language models (LLMs) to act as\ntool-using agents, but injecting thousands of tool schemas into the prompt is\ncostly and error-prone. We introduce MCP-Zero, a proactive agent framework that\nlets the LLM itself decide when and which external tools to retrieve, thereby\nassembling a task-specific toolchain from scratch. The framework is built upon\nthree components: (1) Proactive Tool Request, where the model emits a\nstructured $\\left<\\operatorname{tool\\_assistant}\\right>$ block that explicitly\nspecifies the desired server and task; (2) Hierarchical Vector Routing, a\ncoarse-to-fine retrieval algorithm that first selects candidate servers and\nthen ranks tools within each server based on the semantic similarity; (3)\nIterative Proactive Invocation, enabling multi-round, cross-domain toolchain\nconstruction with minimal context overhead, and allowing the model to\niteratively revise its request when the returned tools are insufficient. To\nevaluate our approach we also compile MCP-tools, a retrieval dataset comprising\n308 MCP servers and 2,797 tools extracted from the official\nModel-Context-Protocol repository and normalized into a unified JSON schema.\nExperiments show that MCP-Zero (i) effectively addresses the context overhead\nproblem of existing methods and accurately selects the correct tool from a pool\nof nearly 3,000 candidates (248.1k tokens); (ii) reduces token consumption by\n98\\% on the APIbank while maintaining high accuracy; and (iii) supports\nmulti-turn tool invocation with consistent accuracy across rounds. The code and\ndataset will be released soon.", "AI": {"tldr": "MCP-Zero \u662f\u4e00\u4e2a\u4e3b\u52a8\u4ee3\u7406\u6846\u67b6\uff0c\u5141\u8bb8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u884c\u51b3\u5b9a\u4f55\u65f6\u68c0\u7d22\u5916\u90e8\u5de5\u5177\uff0c\u4ece\u800c\u4ece\u5934\u6784\u5efa\u4efb\u52a1\u7279\u5b9a\u7684\u5de5\u5177\u94fe\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u6ce8\u5165\u5927\u91cf\u5de5\u5177\u6a21\u5f0f\u7684\u9ad8\u6210\u672c\u548c\u6613\u9519\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684 LLM \u5de5\u5177\u8c03\u7528\u65b9\u6cd5\u9700\u8981\u5c06\u5927\u91cf\u5de5\u5177\u6a21\u5f0f\u6ce8\u5165\u63d0\u793a\u4e2d\uff0c\u5bfc\u81f4\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u51fa\u9519\u3002MCP-Zero \u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u4ee3\u7406\u6846\u67b6\uff0c\u4f7f LLM \u80fd\u591f\u52a8\u6001\u68c0\u7d22\u548c\u9009\u62e9\u5de5\u5177\uff0c\u51cf\u5c11\u4e0a\u4e0b\u6587\u5f00\u9500\u3002", "method": "MCP-Zero \u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4e3b\u52a8\u5de5\u5177\u8bf7\u6c42\u3001\u5206\u5c42\u5411\u91cf\u8def\u7531\u548c\u8fed\u4ee3\u4e3b\u52a8\u8c03\u7528\uff0c\u901a\u8fc7\u8fd9\u4e9b\u673a\u5236\u5b9e\u73b0\u4e86\u52a8\u6001\u5de5\u5177\u94fe\u7684\u6784\u5efa\u548c\u9ad8\u6548\u68c0\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMCP-Zero \u80fd\u591f\u9ad8\u6548\u9009\u62e9\u6b63\u786e\u5de5\u5177\uff08\u4ece\u8fd1 3,000 \u5019\u9009\u5de5\u5177\u4e2d\uff09\uff0c\u51cf\u5c11 98% \u7684 token \u6d88\u8017\uff0c\u5e76\u652f\u6301\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u3002", "conclusion": "MCP-Zero \u663e\u8457\u964d\u4f4e\u4e86\u5de5\u5177\u8c03\u7528\u7684\u4e0a\u4e0b\u6587\u5f00\u9500\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u591a\u9886\u57df\u4efb\u52a1\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5de5\u5177\u8c03\u7528, \u4e3b\u52a8\u4ee3\u7406, \u5206\u5c42\u68c0\u7d22, \u591a\u8f6e\u8c03\u7528"}}
{"id": "2506.00467", "pdf": "https://arxiv.org/pdf/2506.00467", "abs": "https://arxiv.org/abs/2506.00467", "authors": ["Shuai Zhao", "Heyan Huang", "Xinge Li", "Xiaokang Chen", "Rui Wang"], "title": "SST: Self-training with Self-adaptive Thresholding for Semi-supervised Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted by Information Processing & Management (IP&M)", "summary": "Neural networks have demonstrated exceptional performance in supervised\nlearning, benefiting from abundant high-quality annotated data. However,\nobtaining such data in real-world scenarios is costly and labor-intensive.\nSemi-supervised learning (SSL) offers a solution to this problem. Recent\nstudies, such as Semi-ViT and Noisy Student, which employ consistency\nregularization or pseudo-labeling, have demonstrated significant achievements.\nHowever, they still face challenges, particularly in accurately selecting\nsufficient high-quality pseudo-labels due to their reliance on fixed\nthresholds. Recent methods such as FlexMatch and FreeMatch have introduced\nflexible or self-adaptive thresholding techniques, greatly advancing SSL\nresearch. Nonetheless, their process of updating thresholds at each iteration\nis deemed time-consuming, computationally intensive, and potentially\nunnecessary. To address these issues, we propose Self-training with\nSelf-adaptive Thresholding (SST), a novel, effective, and efficient SSL\nframework. SST introduces an innovative Self-Adaptive Thresholding (SAT)\nmechanism that adaptively adjusts class-specific thresholds based on the\nmodel's learning progress. SAT ensures the selection of high-quality\npseudo-labeled data, mitigating the risks of inaccurate pseudo-labels and\nconfirmation bias. Extensive experiments demonstrate that SST achieves\nstate-of-the-art performance with remarkable efficiency, generalization, and\nscalability across various architectures and datasets. Semi-SST-ViT-Huge\nachieves the best results on competitive ImageNet-1K SSL benchmarks, with 80.7%\n/ 84.9% Top-1 accuracy using only 1% / 10% labeled data. Compared to the\nfully-supervised DeiT-III-ViT-Huge, which achieves 84.8% Top-1 accuracy using\n100% labeled data, our method demonstrates superior performance using only 10%\nlabeled data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6SST\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7684\u9608\u503c\u673a\u5236SAT\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u4f2a\u6807\u7b7e\u9009\u62e9\u4e0d\u51c6\u786e\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u53d6\u5f97\u4e86\u4e00\u5b9a\u6210\u679c\uff0c\u4f46\u4ecd\u9762\u4e34\u4f2a\u6807\u7b7e\u9009\u62e9\u4e0d\u51c6\u786e\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u6311\u6218\u3002", "method": "SST\u6846\u67b6\u5f15\u5165\u81ea\u9002\u5e94\u7684SAT\u673a\u5236\uff0c\u52a8\u6001\u8c03\u6574\u7c7b\u522b\u7279\u5b9a\u9608\u503c\u4ee5\u9009\u62e9\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSST\u5728ImageNet-1K\u7b49\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u75281%/10%\u6807\u8bb0\u6570\u636e\u5373\u53ef\u8fbe\u523080.7%/84.9%\u7684Top-1\u51c6\u786e\u7387\u3002", "conclusion": "SST\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763\u5b66\u4e60\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "keywords": "\u534a\u76d1\u7763\u5b66\u4e60,\u4f2a\u6807\u7b7e,\u81ea\u9002\u5e94\u9608\u503c,SST"}}
{"id": "2506.00445", "pdf": "https://arxiv.org/pdf/2506.00445", "abs": "https://arxiv.org/abs/2506.00445", "authors": ["Long Bai", "Zixuan Li", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng", "Tat-Seng Chua"], "title": "G2S: A General-to-Specific Learning Framework for Temporal Knowledge Graph Forecasting with Large Language Models", "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "Forecasting over Temporal Knowledge Graphs (TKGs) which predicts future facts\nbased on historical ones has received much attention. Recent studies have\nintroduced Large Language Models (LLMs) for this task to enhance the models'\ngeneralization abilities. However, these models perform forecasting via\nsimultaneously learning two kinds of entangled knowledge in the TKG: (1)\ngeneral patterns, i.e., invariant temporal structures shared across different\nscenarios; and (2) scenario information, i.e., factual knowledge engaged in\nspecific scenario, such as entities and relations. As a result, the learning\nprocesses of these two kinds of knowledge may interfere with each other, which\npotentially impact the generalization abilities of the models. To enhance the\ngeneralization ability of LLMs on this task, in this paper, we propose a\nGeneral-to-Specific learning framework (G2S) that disentangles the learning\nprocesses of the above two kinds of knowledge. In the general learning stage,\nwe mask the scenario information in different TKGs and convert it into\nanonymous temporal structures. After training on these structures, the model is\nable to capture the general patterns across different TKGs. In the specific\nlearning stage, we inject the scenario information into the structures via\neither in-context learning or fine-tuning modes. Experimental results show that\nG2S effectively improves the generalization abilities of LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aG2S\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u65f6\u95f4\u77e5\u8bc6\u56fe\u8c31\uff08TKG\uff09\u9884\u6d4b\u4efb\u52a1\u4e2d\u5206\u79bb\u901a\u7528\u6a21\u5f0f\u548c\u7279\u5b9a\u573a\u666f\u4fe1\u606f\u7684\u5b66\u4e60\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u77e5\u8bc6\u56fe\u8c31\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u901a\u7528\u6a21\u5f0f\u548c\u7279\u5b9a\u573a\u666f\u4fe1\u606f\u7684\u5b66\u4e60\u8fc7\u7a0b\u76f8\u4e92\u5e72\u6270\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5206\u79bb\u8fd9\u4e24\u79cd\u77e5\u8bc6\u7684\u5b66\u4e60\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u901a\u7528\u5230\u7279\u5b9a\uff08G2S\uff09\u7684\u5b66\u4e60\u6846\u67b6\uff1a\u5728\u901a\u7528\u5b66\u4e60\u9636\u6bb5\uff0c\u901a\u8fc7\u63a9\u7801\u573a\u666f\u4fe1\u606f\u63d0\u53d6\u901a\u7528\u6a21\u5f0f\uff1b\u5728\u7279\u5b9a\u5b66\u4e60\u9636\u6bb5\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6216\u5fae\u8c03\u6ce8\u5165\u573a\u666f\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cG2S\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u77e5\u8bc6\u56fe\u8c31\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "G2S\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u901a\u7528\u6a21\u5f0f\u548c\u7279\u5b9a\u573a\u666f\u4fe1\u606f\u7684\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u65f6\u95f4\u77e5\u8bc6\u56fe\u8c31\u9884\u6d4b\u4efb\u52a1\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u65f6\u95f4\u77e5\u8bc6\u56fe\u8c31\uff08TKG\uff09\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3001\u6cdb\u5316\u80fd\u529b\u3001G2S\u6846\u67b6"}}
{"id": "2506.01080", "pdf": "https://arxiv.org/pdf/2506.01080", "abs": "https://arxiv.org/abs/2506.01080", "authors": ["Florian Carichon", "Aditi Khandelwal", "Marylou Fauchard", "Golnoosh Farnadi"], "title": "The Coming Crisis of Multi-Agent Misalignment: AI Alignment Must Be a Dynamic and Social Process", "categories": ["cs.AI", "cs.CY"], "comment": "Preprint of NeurIPS 2025 Position Paper", "summary": "This position paper states that AI Alignment in Multi-Agent Systems (MAS)\nshould be considered a dynamic and interaction-dependent process that heavily\ndepends on the social environment where agents are deployed, either\ncollaborative, cooperative, or competitive. While AI alignment with human\nvalues and preferences remains a core challenge, the growing prevalence of MAS\nin real-world applications introduces a new dynamic that reshapes how agents\npursue goals and interact to accomplish various tasks. As agents engage with\none another, they must coordinate to accomplish both individual and collective\ngoals. However, this complex social organization may unintentionally misalign\nsome or all of these agents with human values or user preferences. Drawing on\nsocial sciences, we analyze how social structure can deter or shatter group and\nindividual values. Based on these analyses, we call on the AI community to\ntreat human, preferential, and objective alignment as an interdependent\nconcept, rather than isolated problems. Finally, we emphasize the urgent need\nfor simulation environments, benchmarks, and evaluation frameworks that allow\nresearchers to assess alignment in these interactive multi-agent contexts\nbefore such dynamics grow too complex to control.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u5728\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\u4e2d\uff0cAI\u5bf9\u9f50\u5e94\u88ab\u89c6\u4e3a\u4e00\u4e2a\u52a8\u6001\u4e14\u4f9d\u8d56\u4e92\u52a8\u7684\u8fc7\u7a0b\uff0c\u793e\u4f1a\u73af\u5883\u5f71\u54cd\u91cd\u5927\uff0c\u5e76\u63d0\u51fa\u9700\u5c06\u5176\u89c6\u4e3a\u76f8\u4e92\u4f9d\u8d56\u7684\u95ee\u9898\uff0c\u547c\u5401\u5efa\u7acb\u6a21\u62df\u73af\u5883\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u968f\u7740\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0cAI\u5bf9\u9f50\u4e2d\u7684\u52a8\u6001\u4e92\u52a8\u548c\u793e\u4f1a\u7ed3\u6784\u5f71\u54cd\u6210\u4e3a\u65b0\u6311\u6218\uff0c\u53ef\u80fd\u5bfc\u81f4\u4ee3\u7406\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u610f\u5916\u504f\u79bb\u3002", "method": "\u501f\u9274\u793e\u4f1a\u79d1\u5b66\uff0c\u5206\u6790\u793e\u4f1a\u7ed3\u6784\u5982\u4f55\u5f71\u54cd\u7fa4\u4f53\u548c\u4e2a\u4f53\u4ef7\u503c\u89c2\uff0c\u5e76\u63d0\u51fa\u5c06\u4eba\u7c7b\u3001\u504f\u597d\u548c\u76ee\u6807\u5bf9\u9f50\u89c6\u4e3a\u76f8\u4e92\u4f9d\u8d56\u7684\u6982\u5ff5\u3002", "result": "\u6307\u51fa\u4e86\u5f53\u524dAI\u5bf9\u9f50\u7814\u7a76\u4e2d\u5ffd\u89c6\u52a8\u6001\u4e92\u52a8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5c06\u5176\u89c6\u4e3a\u76f8\u4e92\u4f9d\u8d56\u95ee\u9898\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u6025\u9700\u5f00\u53d1\u6a21\u62df\u73af\u5883\u3001\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u5728\u591a\u4ee3\u7406\u4ea4\u4e92\u73af\u5883\u4e2d\u8bc4\u4f30\u5bf9\u9f50\u95ee\u9898\uff0c\u907f\u514d\u590d\u6742\u5931\u63a7\u3002", "keywords": "AI\u5bf9\u9f50, \u591a\u4ee3\u7406\u7cfb\u7edf, \u793e\u4f1a\u7ed3\u6784, \u52a8\u6001\u4e92\u52a8, \u8bc4\u4f30\u6846\u67b6"}}
{"id": "2506.00476", "pdf": "https://arxiv.org/pdf/2506.00476", "abs": "https://arxiv.org/abs/2506.00476", "authors": ["Abhisek Ray", "Lukas Esterle"], "title": "Towards Graph-Based Privacy-Preserving Federated Learning: ModelNet -- A ResNet-based Model Classification Dataset", "categories": ["cs.LG"], "comment": "8 pages, 8 figures", "summary": "Federated Learning (FL) has emerged as a powerful paradigm for training\nmachine learning models across distributed data sources while preserving data\nlocality. However, the privacy of local data is always a pivotal concern and\nhas received a lot of attention in recent research on the FL regime. Moreover,\nthe lack of domain heterogeneity and client-specific segregation in the\nbenchmarks remains a critical bottleneck for rigorous evaluation. In this\npaper, we introduce ModelNet, a novel image classification dataset constructed\nfrom the embeddings extracted from a pre-trained ResNet50 model. First, we\nmodify the CIFAR100 dataset into three client-specific variants, considering\nthree domain heterogeneities (homogeneous, heterogeneous, and random).\nSubsequently, we train each client-specific subset of all three variants on the\npre-trained ResNet50 model to save model parameters. In addition to\nmulti-domain image data, we propose a new hypothesis to define the FL algorithm\nthat can access the anonymized model parameters to preserve the local privacy\nin a more effective manner compared to existing ones. ModelNet is designed to\nsimulate realistic FL settings by incorporating non-IID data distributions and\nclient diversity design principles in the mainframe for both conventional and\nfuturistic graph-driven FL algorithms. The three variants are ModelNet-S,\nModelNet-D, and ModelNet-R, which are based on homogeneous, heterogeneous, and\nrandom data settings, respectively. To the best of our knowledge, we are the\nfirst to propose a cross-environment client-specific FL dataset along with the\ngraph-based variant. Extensive experiments based on domain shifts and\naggregation strategies show the effectiveness of the above variants, making it\na practical benchmark for classical and graph-based FL research. The dataset\nand related code are available online.", "AI": {"tldr": "\u63d0\u51fa\u4e86ModelNet\uff0c\u4e00\u4e2a\u57fa\u4e8e\u9884\u8bad\u7ec3ResNet50\u6a21\u578b\u5d4c\u5165\u7684\u65b0\u578b\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u9690\u79c1\u548c\u9886\u57df\u5f02\u6784\u6027\u8bc4\u4f30\u7684\u6311\u6218\uff0c\u5e76\u5f15\u5165\u56fe\u9a71\u52a8\u7684FL\u7b97\u6cd5\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u9690\u79c1\u95ee\u9898\u548c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9886\u57df\u5f02\u6784\u6027\u4e0d\u8db3\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u4fee\u6539CIFAR100\u6570\u636e\u96c6\u751f\u6210\u4e09\u79cd\u5ba2\u6237\u7aef\u7279\u5b9a\u53d8\u4f53\uff08\u540c\u8d28\u3001\u5f02\u8d28\u3001\u968f\u673a\uff09\uff0c\u5e76\u57fa\u4e8e\u9884\u8bad\u7ec3ResNet50\u6a21\u578b\u8bad\u7ec3\uff0c\u63d0\u51fa\u65b0\u7684FL\u7b97\u6cd5\u4fdd\u62a4\u672c\u5730\u9690\u79c1\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86ModelNet\u53d8\u4f53\u5728\u9886\u57df\u504f\u79fb\u548c\u805a\u5408\u7b56\u7565\u4e0a\u7684\u6709\u6548\u6027\uff0c\u9002\u7528\u4e8e\u7ecf\u5178\u548c\u56fe\u9a71\u52a8\u7684FL\u7814\u7a76\u3002", "conclusion": "ModelNet\u4e3a\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u63a8\u52a8\u4e86\u56fe\u9a71\u52a8FL\u7b97\u6cd5\u7684\u53d1\u5c55\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60\u3001\u6570\u636e\u9690\u79c1\u3001\u9886\u57df\u5f02\u6784\u6027\u3001ModelNet\u3001\u56fe\u9a71\u52a8FL"}}
{"id": "2506.00448", "pdf": "https://arxiv.org/pdf/2506.00448", "abs": "https://arxiv.org/abs/2506.00448", "authors": ["Suhas BN", "Han-Chin Shing", "Lei Xu", "Mitch Strong", "Jon Burnsky", "Jessica Ofor", "Jordan R. Mason", "Susan Chen", "Sundararajan Srinivasan", "Chaitanya Shivade", "Jack Moriarty", "Joseph Paul Cohen"], "title": "Fact-Controlled Diagnosis of Hallucinations in Medical Text Summarization", "categories": ["cs.CL"], "comment": "https://github.com/amazon-science/acibench-hallucination-annotations", "summary": "Hallucinations in large language models (LLMs) during summarization of\npatient-clinician dialogues pose significant risks to patient care and clinical\ndecision-making. However, the phenomenon remains understudied in the clinical\ndomain, with uncertainty surrounding the applicability of general-domain\nhallucination detectors. The rarity and randomness of hallucinations further\ncomplicate their investigation. In this paper, we conduct an evaluation of\nhallucination detection methods in the medical domain, and construct two\ndatasets for the purpose: A fact-controlled Leave-N-out dataset -- generated by\nsystematically removing facts from source dialogues to induce hallucinated\ncontent in summaries; and a natural hallucination dataset -- arising\norganically during LLM-based medical summarization. We show that general-domain\ndetectors struggle to detect clinical hallucinations, and that performance on\nfact-controlled hallucinations does not reliably predict effectiveness on\nnatural hallucinations. We then develop fact-based approaches that count\nhallucinations, offering explainability not available with existing methods.\nNotably, our LLM-based detectors, which we developed using fact-controlled\nhallucinations, generalize well to detecting real-world clinical\nhallucinations. This research contributes a suite of specialized metrics\nsupported by expert-annotated datasets to advance faithful clinical\nsummarization systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u4e34\u5e8a\u5bf9\u8bdd\u6458\u8981\u4e2d\u7684\u865a\u5047\u4fe1\u606f\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u4e8b\u5b9e\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "LLMs\u5728\u4e34\u5e8a\u5bf9\u8bdd\u6458\u8981\u4e2d\u4ea7\u751f\u7684\u865a\u5047\u4fe1\u606f\u5bf9\u60a3\u8005\u62a4\u7406\u548c\u4e34\u5e8a\u51b3\u7b56\u6784\u6210\u98ce\u9669\uff0c\u4f46\u76ee\u524d\u7814\u7a76\u4e0d\u8db3\uff0c\u4e14\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u4e34\u5e8a\u9886\u57df\u6548\u679c\u6709\u9650\u3002", "method": "\u6784\u5efa\u4e86\u4e24\u79cd\u6570\u636e\u96c6\uff08\u4e8b\u5b9e\u63a7\u5236Leave-N-out\u6570\u636e\u96c6\u548c\u81ea\u7136\u865a\u5047\u6570\u636e\u96c6\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u4e8b\u5b9e\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u901a\u7528\u9886\u57df\u68c0\u6d4b\u5668\u5728\u4e34\u5e8a\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u5f00\u53d1\u7684\u65b0\u578b\u68c0\u6d4b\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u771f\u5b9e\u4e16\u754c\u7684\u4e34\u5e8a\u865a\u5047\u4fe1\u606f\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4e34\u5e8a\u6458\u8981\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e13\u4e1a\u5316\u7684\u8bc4\u4f30\u6307\u6807\u548c\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u4e86\u6458\u8981\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u63d0\u5347\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u4e34\u5e8a\u6458\u8981,\u865a\u5047\u4fe1\u606f\u68c0\u6d4b,\u533b\u7597\u9886\u57df,\u6570\u636e\u96c6"}}
{"id": "2506.01087", "pdf": "https://arxiv.org/pdf/2506.01087", "abs": "https://arxiv.org/abs/2506.01087", "authors": ["Bertram Lud\u00e4scher", "Yilin Xia", "Shawn Bowers"], "title": "Choices and their Provenance: Explaining Stable Solutions of Abstract Argumentation Frameworks", "categories": ["cs.AI", "cs.SC"], "comment": "International Workshop on the Theory and Practice of Provenance\n  (TaPP) and ProvenanceWeek'25 @SIGMOD, June 27, 2025. Berlin, Germany", "summary": "The rule $\\mathrm{Defeated}(x) \\leftarrow \\mathrm{Attacks}(y,x),\\, \\neg \\,\n\\mathrm{Defeated}(y)$, evaluated under the well-founded semantics (WFS), yields\na unique 3-valued (skeptical) solution of an abstract argumentation framework\n(AF). An argument $x$ is defeated ($\\mathrm{OUT}$) if there exists an\nundefeated argument $y$ that attacks it. For 2-valued (stable) solutions, this\nis the case iff $y$ is accepted ($\\mathrm{IN}$), i.e., if all of $y$'s\nattackers are defeated. Under WFS, arguments that are neither accepted nor\ndefeated are undecided ($\\mathrm{UNDEC}$). As shown in prior work, well-founded\nsolutions (a.k.a. grounded labelings) \"explain themselves\": The provenance of\narguments is given by subgraphs (definable via regular path queries) rooted at\nthe node of interest. This provenance is closely related to winning strategies\nof a two-player argumentation game.\n  We present a novel approach for extending this provenance to stable AF\nsolutions. Unlike grounded solutions, which can be constructed via a bottom-up\nalternating fixpoint procedure, stable models often involve non-deterministic\nchoice as part of the search for models. Thus, the provenance of stable\nsolutions is of a different nature, and reflects a more expressive generate &\ntest paradigm. Our approach identifies minimal sets of critical attacks,\npinpointing choices and assumptions made by a stable model. These critical\nattack edges provide additional insights into the provenance of an argument's\nstatus, combining well-founded derivation steps with choice steps. Our approach\ncan be understood as a form of diagnosis that finds minimal \"repairs\" to an AF\ngraph such that the well-founded solution of the repaired graph coincides with\nthe desired stable model of the original AF graph.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5c06\u8bba\u8bc1\u6846\u67b6\uff08AF\uff09\u4e2d\u57fa\u4e8e\u826f\u57fa\u8bed\u4e49\uff08WFS\uff09\u7684\u6839\u6e90\u6269\u5c55\u5230\u7a33\u5b9a\u89e3\uff08stable solutions\uff09\uff0c\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u653b\u51fb\u8fb9\u6765\u63ed\u793a\u8bba\u8bc1\u72b6\u6001\u7684\u6765\u6e90\u3002", "motivation": "\u826f\u57fa\u89e3\u63d0\u4f9b\u4e86\u8bba\u8bc1\u72b6\u6001\u7684\u660e\u786e\u6765\u6e90\uff0c\u4f46\u7a33\u5b9a\u89e3\u6d89\u53ca\u975e\u786e\u5b9a\u6027\u9009\u62e9\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u540c\u7684\u65b9\u6cd5\u6765\u89e3\u91ca\u5176\u6765\u6e90\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u6700\u5c0f\u7684\u5173\u952e\u653b\u51fb\u96c6\uff0c\u5c06\u826f\u57fa\u63a8\u5bfc\u6b65\u9aa4\u4e0e\u9009\u62e9\u6b65\u9aa4\u7ed3\u5408\uff0c\u63ed\u793a\u7a33\u5b9a\u89e3\u7684\u6765\u6e90\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u89c6\u4e3a\u4e00\u79cd\u8bca\u65ad\u5de5\u5177\uff0c\u627e\u5230\u6700\u5c0f\u7684\u201c\u4fee\u590d\u201d\u4ee5\u4f7f\u4fee\u590d\u540e\u7684\u56fe\u7684\u826f\u57fa\u89e3\u4e0e\u539f\u56fe\u7684\u7a33\u5b9a\u89e3\u4e00\u81f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u7a33\u5b9a\u89e3\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u6765\u6e90\u89e3\u91ca\uff0c\u7ed3\u5408\u826f\u57fa\u548c\u9009\u62e9\u6b65\u9aa4\uff0c\u589e\u5f3a\u4e86\u5bf9\u8bba\u8bc1\u72b6\u6001\u7684\u7406\u89e3\u3002", "keywords": "\u8bba\u8bc1\u6846\u67b6\uff0c\u826f\u57fa\u8bed\u4e49\uff0c\u7a33\u5b9a\u89e3\uff0c\u6839\u6e90\u5206\u6790\uff0c\u5173\u952e\u653b\u51fb"}}
{"id": "2506.00477", "pdf": "https://arxiv.org/pdf/2506.00477", "abs": "https://arxiv.org/abs/2506.00477", "authors": ["Leila Mahmoodi", "Peyman Moghadam", "Munawar Hayat", "Christian Simon", "Mehrtash Harandi"], "title": "Flashbacks to Harmonize Stability and Plasticity in Continual Learning", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "Manuscript submitted to Neural Networks (Elsevier) in August 2024;\n  and accepted in May 2025 for publication. This version is author-accepted\n  manuscript before copyediting and typesetting. The codes of this article will\n  be available at https://github.com/csiro-robotics/Flashback-Learning", "summary": "We introduce Flashback Learning (FL), a novel method designed to harmonize\nthe stability and plasticity of models in Continual Learning (CL). Unlike prior\napproaches that primarily focus on regularizing model updates to preserve old\ninformation while learning new concepts, FL explicitly balances this trade-off\nthrough a bidirectional form of regularization. This approach effectively\nguides the model to swiftly incorporate new knowledge while actively retaining\nits old knowledge. FL operates through a two-phase training process and can be\nseamlessly integrated into various CL methods, including replay, parameter\nregularization, distillation, and dynamic architecture techniques. In designing\nFL, we use two distinct knowledge bases: one to enhance plasticity and another\nto improve stability. FL ensures a more balanced model by utilizing both\nknowledge bases to regularize model updates. Theoretically, we analyze how the\nFL mechanism enhances the stability-plasticity balance. Empirically, FL\ndemonstrates tangible improvements over baseline methods within the same\ntraining budget. By integrating FL into at least one representative baseline\nfrom each CL category, we observed an average accuracy improvement of up to\n4.91% in Class-Incremental and 3.51% in Task-Incremental settings on standard\nimage classification benchmarks. Additionally, measurements of the\nstability-to-plasticity ratio confirm that FL effectively enhances this\nbalance. FL also outperforms state-of-the-art CL methods on more challenging\ndatasets like ImageNet.", "AI": {"tldr": "Flashback Learning (FL) \u662f\u4e00\u79cd\u65b0\u9896\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5411\u6b63\u5219\u5316\u5e73\u8861\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7ebf\u65b9\u6cd5\u548c\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u4e2d\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u51fa\u4e86 FL \u65b9\u6cd5\uff0c\u907f\u514d\u4ec5\u901a\u8fc7\u6a21\u578b\u66f4\u65b0\u6b63\u5219\u5316\u6765\u4fdd\u7559\u65e7\u77e5\u8bc6\u7684\u5c40\u9650\u6027\u3002", "method": "FL \u901a\u8fc7\u4e24\u9636\u6bb5\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5229\u7528\u4e24\u4e2a\u72ec\u7acb\u7684\u77e5\u8bc6\u5e93\uff08\u4e00\u4e2a\u589e\u5f3a\u53ef\u5851\u6027\uff0c\u53e6\u4e00\u4e2a\u63d0\u5347\u7a33\u5b9a\u6027\uff09\u6765\u6b63\u5219\u5316\u6a21\u578b\u66f4\u65b0\uff0c\u9002\u7528\u4e8e\u591a\u79cd CL \u65b9\u6cd5\u3002", "result": "\u5728\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFL \u5c06\u7c7b\u522b\u589e\u91cf\u548c\u4efb\u52a1\u589e\u91cf\u8bbe\u7f6e\u7684\u51c6\u786e\u7387\u5206\u522b\u5e73\u5747\u63d0\u5347\u4e86 4.91% \u548c 3.51%\uff0c\u5e76\u663e\u8457\u6539\u5584\u4e86\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u7684\u5e73\u8861\u3002", "conclusion": "FL \u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "keywords": "Flashback Learning, Continual Learning, \u7a33\u5b9a\u6027, \u53ef\u5851\u6027, \u53cc\u5411\u6b63\u5219\u5316"}}
{"id": "2506.00469", "pdf": "https://arxiv.org/pdf/2506.00469", "abs": "https://arxiv.org/abs/2506.00469", "authors": ["Shaoxiong Ji", "Zihao Li", "Jaakko Paavola", "Indraneil Paul", "Hengyu Luo", "J\u00f6rg Tiedemann"], "title": "Massively Multilingual Adaptation of Large Language Models Using Bilingual Translation Data", "categories": ["cs.CL"], "comment": "EMMA-500 Gen 2; refer to Gen 1 in arXiv:2409.17892", "summary": "This paper investigates a critical design decision in the practice of\nmassively multilingual continual pre-training -- the inclusion of parallel\ndata. Specifically, we study the impact of bilingual translation data for\nmassively multilingual language adaptation of the Llama3 family of models to\n500 languages. To this end, we construct the MaLA bilingual translation corpus,\ncontaining data from more than 2,500 language pairs. Subsequently, we develop\nthe EMMA-500 Llama 3 suite of four massively multilingual models -- continually\npre-trained from the Llama 3 family of base models extensively on diverse data\nmixes up to 671B tokens -- and explore the effect of continual pre-training\nwith or without bilingual translation data. Comprehensive evaluation across 7\ntasks and 12 benchmarks demonstrates that bilingual data tends to enhance\nlanguage transfer and performance, particularly for low-resource languages. We\nopen-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model\ngenerations.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u8de8\u8bed\u8a00\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e2d\u52a0\u5165\u5e76\u884c\u53cc\u8bed\u6570\u636e\u7684\u6548\u679c\uff0c\u9a8c\u8bc1\u4e86\u53cc\u8bed\u6570\u636e\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u6027\u80fd\u7684\u63d0\u5347\u4f5c\u7528\u3002", "motivation": "\u63a2\u7d22\u5728\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u6301\u7eed\u9884\u8bad\u7ec3\u4e2d\uff0c\u53cc\u8bed\u7ffb\u8bd1\u6570\u636e\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b2,500\u591a\u79cd\u8bed\u8a00\u5bf9\u7684MaLA\u53cc\u8bed\u7ffb\u8bd1\u8bed\u6599\u5e93\uff0c\u5e76\u57fa\u4e8eLlama 3\u6a21\u578b\u5bb6\u65cf\u5f00\u53d1\u4e86EMMA-500 Llama 3\u6a21\u578b\u5957\u4ef6\u3002", "result": "\u53cc\u8bed\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u8fc1\u79fb\u548c\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002", "conclusion": "\u53cc\u8bed\u6570\u636e\u5728\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u4e2d\u5177\u6709\u79ef\u6781\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "keywords": "\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u9884\u8bad\u7ec3, Llama 3, \u53cc\u8bed\u6570\u636e, \u4f4e\u8d44\u6e90\u8bed\u8a00"}}
{"id": "2506.01093", "pdf": "https://arxiv.org/pdf/2506.01093", "abs": "https://arxiv.org/abs/2506.01093", "authors": ["Kunal Khanvilkar", "Kranthi Kommuru"], "title": "Regulatory Graphs and GenAI for Real-Time Transaction Monitoring and Compliance Explanation in Banking", "categories": ["cs.AI", "cs.CE", "cs.LG"], "comment": null, "summary": "This paper presents a real-time transaction monitoring framework that\nintegrates graph-based modeling, narrative field embedding, and generative\nexplanation to support automated financial compliance. The system constructs\ndynamic transaction graphs, extracts structural and contextual features, and\nclassifies suspicious behavior using a graph neural network. A\nretrieval-augmented generation module generates natural language explanations\naligned with regulatory clauses for each flagged transaction. Experiments\nconducted on a simulated stream of financial data show that the proposed method\nachieves superior results, with 98.2% F1-score, 97.8% precision, and 97.0%\nrecall. Expert evaluation further confirms the quality and interpretability of\ngenerated justifications. The findings demonstrate the potential of combining\ngraph intelligence and generative models to support explainable, audit-ready\ncompliance in high-risk financial environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u4ea4\u6613\u76d1\u63a7\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u6a21\u578b\u3001\u53d9\u4e8b\u5d4c\u5165\u548c\u751f\u6210\u89e3\u91ca\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u91d1\u878d\u5408\u89c4\u3002", "motivation": "\u89e3\u51b3\u9ad8\u98ce\u9669\u91d1\u878d\u73af\u5883\u4e2d\u5b9e\u65f6\u4ea4\u6613\u76d1\u63a7\u548c\u5408\u89c4\u89e3\u91ca\u7684\u9700\u6c42\u3002", "method": "\u6784\u5efa\u52a8\u6001\u4ea4\u6613\u56fe\uff0c\u63d0\u53d6\u7279\u5f81\uff0c\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5f02\u5e38\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u6a21\u5757\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "result": "\u5b9e\u9a8c\u663e\u793aF1\u5206\u657098.2%\uff0c\u7cbe\u786e\u738797.8%\uff0c\u53ec\u56de\u738797.0%\uff0c\u4e13\u5bb6\u8ba4\u53ef\u89e3\u91ca\u8d28\u91cf\u3002", "conclusion": "\u56fe\u667a\u80fd\u4e0e\u751f\u6210\u6a21\u578b\u7ed3\u5408\u4e3a\u53ef\u89e3\u91ca\u7684\u5408\u89c4\u5ba1\u8ba1\u63d0\u4f9b\u6f5c\u529b\u3002", "keywords": "\u5b9e\u65f6\u76d1\u63a7\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u3001\u751f\u6210\u89e3\u91ca\u3001\u91d1\u878d\u5408\u89c4"}}
{"id": "2506.00478", "pdf": "https://arxiv.org/pdf/2506.00478", "abs": "https://arxiv.org/abs/2506.00478", "authors": ["Hongjie Zhu", "Zezheng Zhang", "Zeyu Zhang", "Yu Bai", "Shimin Wen", "Huazhang Wang", "Daji Ergu", "Ying Cai", "Yang Zhao"], "title": "Dynamic Domain Adaptation-Driven Physics-Informed Graph Representation Learning for AC-OPF", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Alternating Current Optimal Power Flow (AC-OPF) aims to optimize generator\npower outputs by utilizing the non-linear relationships between voltage\nmagnitudes and phase angles in a power system. However, current AC-OPF solvers\nstruggle to effectively represent the complex relationship between variable\ndistributions in the constraint space and their corresponding optimal\nsolutions. This limitation in constraint modeling restricts the system's\nability to develop diverse knowledge representations. Additionally, modeling\nthe power grid solely based on spatial topology further limits the integration\nof additional prior knowledge, such as temporal information. To overcome these\nchallenges, we propose DDA-PIGCN (Dynamic Domain Adaptation-Driven\nPhysics-Informed Graph Convolutional Network), a new method designed to address\nconstraint-related issues and build a graph-based learning framework that\nincorporates spatiotemporal features. DDA-PIGCN improves consistency\noptimization for features with varying long-range dependencies by applying\nmulti-layer, hard physics-informed constraints. It also uses a dynamic domain\nadaptation learning mechanism that iteratively updates and refines key state\nvariables under predefined constraints, enabling precise constraint\nverification. Moreover, it captures spatiotemporal dependencies between\ngenerators and loads by leveraging the physical structure of the power grid,\nallowing for deep integration of topological information across time and space.\nExtensive comparative and ablation studies show that DDA-PIGCN delivers strong\nperformance across several IEEE standard test cases (such as case9, case30, and\ncase300), achieving mean absolute errors (MAE) from 0.0011 to 0.0624 and\nconstraint satisfaction rates between 99.6% and 100%, establishing it as a\nreliable and efficient AC-OPF solver.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDDA-PIGCN\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u65f6\u7a7a\u7279\u5f81\u7684\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u7528\u4e8e\u89e3\u51b3AC-OPF\u95ee\u9898\u4e2d\u7684\u7ea6\u675f\u5efa\u6a21\u548c\u77e5\u8bc6\u8868\u793a\u5c40\u9650\u6027\uff0c\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709AC-OPF\u6c42\u89e3\u5668\u96be\u4ee5\u6709\u6548\u5efa\u6a21\u7ea6\u675f\u7a7a\u95f4\u4e2d\u53d8\u91cf\u5206\u5e03\u4e0e\u6700\u4f18\u89e3\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u4e14\u65e0\u6cd5\u5145\u5206\u5229\u7528\u65f6\u7a7a\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u52a8\u6001\u57df\u9002\u5e94\u9a71\u52a8\u7684\u7269\u7406\u4fe1\u606f\u56fe\u5377\u79ef\u7f51\u7edc\uff08DDA-PIGCN\uff09\uff0c\u7ed3\u5408\u591a\u5c42\u7ea7\u786c\u7269\u7406\u7ea6\u675f\u548c\u52a8\u6001\u57df\u9002\u5e94\u5b66\u4e60\u673a\u5236\uff0c\u6574\u5408\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u591a\u4e2aIEEE\u6807\u51c6\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cMAE\u4e3a0.0011\u81f30.0624\uff0c\u7ea6\u675f\u6ee1\u8db3\u7387\u4e3a99.6%\u81f3100%\u3002", "conclusion": "DDA-PIGCN\u80fd\u591f\u53ef\u9760\u4e14\u9ad8\u6548\u5730\u89e3\u51b3AC-OPF\u95ee\u9898\uff0c\u663e\u7740\u63d0\u5347\u4e86\u7ea6\u675f\u5efa\u6a21\u548c\u77e5\u8bc6\u8868\u793a\u80fd\u529b\u3002", "keywords": "AC-OPF, \u52a8\u6001\u57df\u9002\u5e94, \u7269\u7406\u4fe1\u606f\u56fe\u5377\u79ef\u7f51\u7edc, \u65f6\u7a7a\u4f9d\u8d56"}}
{"id": "2506.00479", "pdf": "https://arxiv.org/pdf/2506.00479", "abs": "https://arxiv.org/abs/2506.00479", "authors": ["Zekun Wang", "Minghua Ma", "Zexin Wang", "Rongchuan Mu", "Liping Shan", "Ming Liu", "Bing Qin"], "title": "EffiVLM-BENCH: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Vision-Language Models", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "ACL 2025", "summary": "Large Vision-Language Models (LVLMs) have achieved remarkable success, yet\ntheir significant computational demands hinder practical deployment. While\nefforts to improve LVLM efficiency are growing, existing methods lack\ncomprehensive evaluation across diverse backbones, benchmarks, and metrics. In\nthis work, we systematically evaluate mainstream acceleration techniques for\nLVLMs, categorized into token and parameter compression. We introduce\nEffiVLM-Bench, a unified framework for assessing not only absolute performance\nbut also generalization and loyalty, while exploring Pareto-optimal trade-offs.\nOur extensive experiments and in-depth analyses offer insights into optimal\nstrategies for accelerating LVLMs. We open-source code and recipes for\nEffiVLM-Bench to foster future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u4e3b\u6d41\u52a0\u901f\u6280\u672f\uff0c\u5f15\u5165\u4e86\u8bc4\u4f30\u6846\u67b6EffiVLM-Bench\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f18\u5316\u7b56\u7565\u7684\u6df1\u5165\u5206\u6790\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u9700\u6c42\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u4e3a\u4ee4\u724c\u548c\u53c2\u6570\u538b\u7f29\u7684\u4e3b\u6d41\u52a0\u901f\u6280\u672f\uff0c\u5e76\u5f15\u5165EffiVLM-Bench\u6846\u67b6\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u548c\u5206\u6790\u63d0\u4f9b\u4e86\u52a0\u901fLVLM\u7684\u6700\u4f18\u7b56\u7565\uff0c\u5e76\u5f00\u6e90\u4e86\u76f8\u5173\u4ee3\u7801\u548c\u914d\u65b9\u3002", "conclusion": "EffiVLM-Bench\u4e3aLVLM\u7684\u52a0\u901f\u6280\u672f\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u548c\u4f18\u5316\u65b9\u5411\u3002", "keywords": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u3001\u52a0\u901f\u6280\u672f\u3001EffiVLM-Bench\u3001\u4ee4\u724c\u538b\u7f29\u3001\u53c2\u6570\u538b\u7f29"}}
{"id": "2506.01095", "pdf": "https://arxiv.org/pdf/2506.01095", "abs": "https://arxiv.org/abs/2506.01095", "authors": ["Khe-Han Toh", "Hong-Kuan Teo"], "title": "Modular Speaker Architecture: A Framework for Sustaining Responsibility and Contextual Integrity in Multi-Agent AI Communication", "categories": ["cs.AI"], "comment": null, "summary": "Sustaining coherent, role-aware communication across multi-agent systems\nremains a foundational challenge in AI. Current frameworks often lack explicit\nmechanisms for speaker responsibility, leading to context drift, alignment\ninstability, and degraded interpretability over time. We propose the Modular\nSpeaker Architecture (MSA), a framework that decomposes speaker behavior into\nmodular components for role tracking, responsibility continuity, and contextual\ncoherence. Grounded in high-context human-AI dialogues, MSA includes three core\nmodules: a Speaker Role Module, a Responsibility Chain Tracker, and a\nContextual Integrity Validator. We evaluate MSA through annotated case studies\nand introduce structural metrics-pragmatic consistency, responsibility flow,\nand context stability-quantified via manual and automatic scoring and\nbootstrapped statistical analysis. Our results show that MSA reliably maintains\ninteraction structure without reliance on affective signals or surface-level\nheuristics. We further implement a prototype configuration language (G-Code)\nand modular API to support MSA deployment in dynamic multi-agent scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u6a21\u5757\u5316\u8bf4\u8bdd\u8005\u67b6\u6784\uff08MSA\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u89d2\u8272\u611f\u77e5\u901a\u4fe1\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u901a\u4fe1\u6846\u67b6\u7f3a\u4e4f\u660e\u786e\u7684\u8bf4\u8bdd\u8005\u8d23\u4efb\u673a\u5236\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u6f02\u79fb\u548c\u5bf9\u9f50\u4e0d\u7a33\u5b9a\uff0c\u5f71\u54cd\u4e86\u901a\u4fe1\u7684\u8fde\u8d2f\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e86MSA\u6846\u67b6\uff0c\u5305\u62ec\u8bf4\u8bdd\u8005\u89d2\u8272\u6a21\u5757\u3001\u8d23\u4efb\u94fe\u8ffd\u8e2a\u5668\u548c\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\u9a8c\u8bc1\u5668\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u7ed3\u6784\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMSA\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u60c5\u611f\u4fe1\u53f7\u6216\u8868\u9762\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u9760\u5730\u7ef4\u62a4\u4ea4\u4e92\u7ed3\u6784\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "MSA\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u539f\u578b\u914d\u7f6e\u8bed\u8a00\uff08G-Code\uff09\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u52a8\u6001\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "keywords": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf, \u6a21\u5757\u5316\u8bf4\u8bdd\u8005\u67b6\u6784, \u8d23\u4efb\u8fde\u7eed\u6027, \u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027, G-Code"}}
{"id": "2506.00482", "pdf": "https://arxiv.org/pdf/2506.00482", "abs": "https://arxiv.org/abs/2506.00482", "authors": ["Eunsu Kim", "Haneul Yoo", "Guijin Son", "Hitesh Patel", "Amit Agarwal", "Alice Oh"], "title": "BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) continue to advance, the need for up-to-date\nand well-organized benchmarks becomes increasingly critical. However, many\nexisting datasets are scattered, difficult to manage, and make it challenging\nto perform evaluations tailored to specific needs or domains, despite the\ngrowing importance of domain-specific models in areas such as math or code. In\nthis paper, we introduce BenchHub, a dynamic benchmark repository that empowers\nresearchers and developers to evaluate LLMs more effectively. BenchHub\naggregates and automatically classifies benchmark datasets from diverse\ndomains, integrating 303K questions across 38 benchmarks. It is designed to\nsupport continuous updates and scalable data management, enabling flexible and\ncustomizable evaluation tailored to various domains or use cases. Through\nextensive experiments with various LLM families, we demonstrate that model\nperformance varies significantly across domain-specific subsets, emphasizing\nthe importance of domain-aware benchmarking. We believe BenchHub can encourage\nbetter dataset reuse, more transparent model comparisons, and easier\nidentification of underrepresented areas in existing benchmarks, offering a\ncritical infrastructure for advancing LLM evaluation research.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86BenchHub\uff0c\u4e00\u4e2a\u52a8\u6001\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e93\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u66f4\u6709\u6548\u5730\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u96c6\u5206\u6563\u4e14\u96be\u7ba1\u7406\uff0c\u96be\u4ee5\u6ee1\u8db3\u7279\u5b9a\u9886\u57df\u6216\u9700\u6c42\uff0c\u800c\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff08\u5982\u6570\u5b66\u6216\u4ee3\u7801\uff09\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002", "method": "\u5f00\u53d1\u4e86BenchHub\uff0c\u805a\u5408\u5e76\u81ea\u52a8\u5206\u7c7b\u591a\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u96c6\uff0c\u6574\u5408\u4e8638\u4e2a\u57fa\u51c6\u4e2d\u7684303K\u95ee\u9898\uff0c\u652f\u6301\u6301\u7eed\u66f4\u65b0\u548c\u53ef\u6269\u5c55\u7684\u6570\u636e\u7ba1\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u5b50\u96c6\u7684\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u5f3a\u8c03\u4e86\u9886\u57df\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u7684\u91cd\u8981\u6027\u3002", "conclusion": "BenchHub\u4fc3\u8fdb\u4e86\u6570\u636e\u96c6\u91cd\u7528\u3001\u900f\u660e\u7684\u6a21\u578b\u6bd4\u8f83\uff0c\u5e76\u6709\u52a9\u4e8e\u53d1\u73b0\u73b0\u6709\u57fa\u51c6\u4e2d\u672a\u88ab\u5145\u5206\u4ee3\u8868\u7684\u9886\u57df\uff0c\u63a8\u52a8\u4e86LLM\u8bc4\u4f30\u7814\u7a76\u7684\u8fdb\u6b65\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u57fa\u51c6\u6d4b\u8bd5, \u6570\u636e\u96c6\u7ba1\u7406, \u9886\u57df\u7279\u5b9a\u8bc4\u4f30"}}
{"id": "2506.00481", "pdf": "https://arxiv.org/pdf/2506.00481", "abs": "https://arxiv.org/abs/2506.00481", "authors": ["Junseo Kim", "Jongwook Han", "Dongmin Choi", "Jongwook Yoon", "Eun-Ju Lee", "Yohan Jo"], "title": "PVP: An Image Dataset for Personalized Visual Persuasion with Persuasion Strategies, Viewer Characteristics, and Persuasiveness Ratings", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main. Code and dataset are released at:\n  https://github.com/holi-lab/PVP_Personalized_Visual_Persuasion", "summary": "Visual persuasion, which uses visual elements to influence cognition and\nbehaviors, is crucial in fields such as advertising and political\ncommunication. With recent advancements in artificial intelligence, there is\ngrowing potential to develop persuasive systems that automatically generate\npersuasive images tailored to individuals. However, a significant bottleneck in\nthis area is the lack of comprehensive datasets that connect the persuasiveness\nof images with the personal information about those who evaluated the images.\nTo address this gap and facilitate technological advancements in personalized\nvisual persuasion, we release the Personalized Visual Persuasion (PVP) dataset,\ncomprising 28,454 persuasive images across 596 messages and 9 persuasion\nstrategies. Importantly, the PVP dataset provides persuasiveness scores of\nimages evaluated by 2,521 human annotators, along with their demographic and\npsychological characteristics (personality traits and values). We demonstrate\nthe utility of our dataset by developing a persuasive image generator and an\nautomated evaluator, and establish benchmark baselines. Our experiments reveal\nthat incorporating psychological characteristics enhances the generation and\nevaluation of persuasive images, providing valuable insights for personalized\nvisual persuasion.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e2a\u6027\u5316\u89c6\u89c9\u8bf4\u670d\uff08PVP\uff09\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8fde\u63a5\u56fe\u50cf\u7684 persuasive \u6027\u4e0e\u8bc4\u4f30\u8005\u7684\u4e2a\u4eba\u4fe1\u606f\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5fc3\u7406\u7279\u5f81\u7684 persuasive \u56fe\u50cf\u751f\u6210\u5668\u548c\u8bc4\u4f30\u5668\u3002", "motivation": "\u89e3\u51b3\u7f3a\u4e4f\u5c06\u56fe\u50cf persuasive \u6027\u4e0e\u8bc4\u4f30\u8005\u4e2a\u4eba\u4fe1\u606f\u5173\u8054\u7684\u6570\u636e\u96c6\u95ee\u9898\uff0c\u63a8\u52a8\u4e2a\u6027\u5316\u89c6\u89c9\u8bf4\u670d\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u53d1\u5e03\u5305\u542b28,454\u5f20 persuasive \u56fe\u50cf\u7684PVP\u6570\u636e\u96c6\uff0c\u7ed3\u54082,521\u540d\u8bc4\u4f30\u8005\u7684\u4eba\u53e3\u7edf\u8ba1\u548c\u5fc3\u7406\u7279\u5f81\uff0c\u5f00\u53d1 persuasive \u56fe\u50cf\u751f\u6210\u5668\u548c\u8bc4\u4f30\u5668\u3002", "result": "\u5fc3\u7406\u7279\u5f81\u7684\u878d\u5165\u63d0\u5347\u4e86 persuasive \u56fe\u50cf\u7684\u751f\u6210\u548c\u8bc4\u4f30\u6548\u679c\u3002", "conclusion": "PVP\u6570\u636e\u96c6\u4e3a\u4e2a\u6027\u5316\u89c6\u89c9\u8bf4\u670d\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u548c\u57fa\u51c6\u6570\u636e\uff0c\u5fc3\u7406\u7279\u5f81\u662f\u5173\u952e\u56e0\u7d20\u3002", "keywords": "\u4e2a\u6027\u5316\u89c6\u89c9\u8bf4\u670d\u3001PVP\u6570\u636e\u96c6\u3001\u5fc3\u7406\u7279\u5f81\u3001\u56fe\u50cf\u751f\u6210\u3001persuasive \u56fe\u50cf"}}
{"id": "2506.01096", "pdf": "https://arxiv.org/pdf/2506.01096", "abs": "https://arxiv.org/abs/2506.01096", "authors": ["Yihao Liu", "Shuocheng Li", "Lang Cao", "Yuhang Xie", "Mengyu Zhou", "Haoyu Dong", "Xiaojun Ma", "Shi Han", "Dongmei Zhang"], "title": "SuperRL: Reinforcement Learning with Supervision to Boost Language Model Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Large language models are increasingly used for complex reasoning tasks where\nhigh-quality offline data such as expert-annotated solutions and distilled\nreasoning traces are often available. However, in environments with sparse\nrewards, reinforcement learning struggles to sample successful trajectories,\nleading to inefficient learning. At the same time, these offline trajectories\nthat represent correct reasoning paths are not utilized by standard on-policy\nreinforcement learning methods. To address this limitation, we propose SuperRL,\na unified training framework that adaptively incorporates offline supervision\ninto reinforcement learning. SuperRL introduces an Adaptive Switch to detect\nsparse reward conditions and activates a Hybrid Actor when necessary. The\nHybrid Actor integrates policy gradient and supervised learning objectives at\nthe loss level, enabling the model to benefit from accurate offline reasoning\nsignals while maintaining the exploratory capacity of reinforcement learning.\nExperiments on a range of reasoning benchmarks show that SuperRL consistently\noutperforms standard reinforcement learning by improving sample efficiency,\ngeneralization, and robustness under sparse rewards.", "AI": {"tldr": "SuperRL\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u79bb\u7ebf\u76d1\u7763\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u7684\u5b66\u4e60\u6548\u7387\u95ee\u9898\uff0c\u663e\u7740\u63d0\u5347\u6837\u672c\u6548\u7387\u3001\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\uff0c\u5f3a\u5316\u5b66\u4e60\u96be\u4ee5\u91c7\u6837\u6210\u529f\u8f68\u8ff9\uff0c\u540c\u65f6\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u672a\u5229\u7528\u79bb\u7ebf\u6b63\u786e\u63a8\u7406\u8def\u5f84\u6570\u636e\u3002", "method": "\u63d0\u51faSuperRL\u6846\u67b6\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u5207\u6362\u5668\u548c\u6df7\u5408\u6267\u884c\u5668\uff0c\u7ed3\u5408\u7b56\u7565\u68af\u5ea6\u548c\u76d1\u7763\u5b66\u4e60\u76ee\u6807\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSuperRL\u4f18\u4e8e\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u5347\u6837\u672c\u6548\u7387\u3001\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "SuperRL\u901a\u8fc7\u81ea\u9002\u5e94\u7ed3\u5408\u79bb\u7ebf\u76d1\u7763\u4e0e\u5f3a\u5316\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u7684\u5b66\u4e60\u95ee\u9898\u3002", "keywords": "SuperRL,\u5f3a\u5316\u5b66\u4e60,\u7a00\u758f\u5956\u52b1,\u79bb\u7ebf\u76d1\u7763,\u6df7\u5408\u6267\u884c\u5668"}}
{"id": "2506.00486", "pdf": "https://arxiv.org/pdf/2506.00486", "abs": "https://arxiv.org/abs/2506.00486", "authors": ["Jun Wu", "Yirong Xiong", "Jiangtao Wen", "Yuxing Han"], "title": "It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Despite rapid advancements in the research and deployment of large language\nmodels (LLMs), the statistical distribution of model parameters, as well as\ntheir influence on initialization, training dynamics, and downstream\nefficiency, has received surprisingly little attention. A recent work\nintroduced BackSlash, a training-time compression algorithm. It first\ndemonstrated that pre-trained LLM parameters follow generalized Gaussian\ndistributions (GGDs) better. By optimizing GG priors during training, BackSlash\ncan reduce parameters by up to 90\\% with minimal performance loss. Building on\nthis foundational insight, we propose a unified, end-to-end framework for LLM\noptimization based on the GG model. Our contributions are threefold: (1)\nGG-based initialization scheme that aligns with the statistical structure of\ntrained models, resulting in faster convergence and improved accuracy; (2)\nDeepShape, a post-training regularization method that reshapes weight\ndistributions to match a GG profile, improving compressibility with minimized\ndegradation in performance; and (3) RF8, a compact and hardware-efficient 8-bit\nfloating-point format designed for GG-distributed-initialized BackSlash\ntraining, enabling low-cost inference without compromising accuracy.\nExperiments across diverse model architectures show that our framework\nconsistently yields smaller and faster models that match or outperform standard\ntraining baselines. By grounding LLM development in principled statistical\nmodeling, this work forges a new path toward efficient, scalable, and\nhardware-aware AI systems. The code is available on our project page:\nhttps://huggingface.co/spaces/shifeng3711/gg_prior.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u5e7f\u4e49\u9ad8\u65af\u5206\u5e03\uff08GGD\uff09\u7684\u7edf\u4e00\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5305\u62ec\u521d\u59cb\u5316\u3001\u6b63\u5219\u5316\u548c\u538b\u7f29\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c11\u53c2\u6570\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u8fc5\u901f\u53d1\u5c55\uff0c\u4f46\u5176\u53c2\u6570\u7684\u7edf\u8ba1\u5206\u5e03\u53ca\u5176\u5bf9\u8bad\u7ec3\u548c\u6548\u7387\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u5173\u6ce8\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u7edf\u8ba1\u5efa\u6a21\u4f18\u5316LLM\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "1. \u57fa\u4e8eGGD\u7684\u521d\u59cb\u5316\u65b9\u6848\uff1b2. DeepShape\u540e\u8bad\u7ec3\u6b63\u5219\u5316\u65b9\u6cd5\uff1b3. RF8\u786c\u4ef6\u9ad8\u65488\u4f4d\u6d6e\u70b9\u683c\u5f0f\u3002\u4e09\u8005\u7ed3\u5408\u5f62\u6210\u7edf\u4e00\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u51cf\u5c11\u591a\u8fbe90%\u7684\u53c2\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8a\u6807\u51c6\u8bad\u7ec3\u57fa\u7ebf\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7edf\u8ba1\u5efa\u6a21\uff0c\u8bba\u6587\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u786c\u4ef6\u611f\u77e5\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5e7f\u4e49\u9ad8\u65af\u5206\u5e03, \u53c2\u6570\u538b\u7f29, \u786c\u4ef6\u6548\u7387, \u521d\u59cb\u5316\u4f18\u5316"}}
{"id": "2506.00483", "pdf": "https://arxiv.org/pdf/2506.00483", "abs": "https://arxiv.org/abs/2506.00483", "authors": ["Aviv Jan", "Dean Tahory", "Omer Talmi", "Omar Abo Mokh"], "title": "Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "8 pages, 5 figures", "summary": "Multi-hop questions still stump large language models (LLMs), which struggle\nto link information across multiple reasoning steps. We introduce Auto-Patch, a\nnovel method that dynamically patches hidden states during inference to enhance\nmulti-hop reasoning in LLMs. Building on the PatchScopes framework, Auto-Patch\nselectively modifies internal representations using a learned classifier.\nEvaluated on the MuSiQue dataset, Auto-Patch improves the solve rate from\n18.45\\% (baseline) to 23.63~$\\pm$~0.7\\% (3 runs), narrowing the gap to\nChain-of-Thought prompting (27.44\\%). Our results highlight the potential of\ndynamic hidden state interventions for advancing complex reasoning in LLMs.", "AI": {"tldr": "Auto-Patch\u662f\u4e00\u79cd\u52a8\u6001\u4fee\u8865\u9690\u85cf\u72b6\u6001\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8df3\u63a8\u7406\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8df3\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u8de8\u591a\u4e2a\u63a8\u7406\u6b65\u9aa4\u94fe\u63a5\u4fe1\u606f\u3002", "method": "\u57fa\u4e8ePatchScopes\u6846\u67b6\uff0cAuto-Patch\u901a\u8fc7\u5b66\u4e60\u7684\u5206\u7c7b\u5668\u9009\u62e9\u6027\u4fee\u6539\u5185\u90e8\u8868\u793a\u3002", "result": "\u5728MuSiQue\u6570\u636e\u96c6\u4e0a\uff0cAuto-Patch\u5c06\u89e3\u51b3\u7387\u4ece18.45%\u63d0\u5347\u81f323.63%\uff0c\u7f29\u5c0f\u4e86\u4e0eChain-of-Thought\u63d0\u793a\uff0827.44%\uff09\u7684\u5dee\u8ddd\u3002", "conclusion": "\u52a8\u6001\u9690\u85cf\u72b6\u6001\u5e72\u9884\u6709\u671b\u63a8\u52a8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u65b9\u9762\u7684\u8fdb\u6b65\u3002", "keywords": "\u591a\u8df3\u63a8\u7406\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001Auto-Patch\u3001PatchScopes\u3001\u9690\u85cf\u72b6\u6001\u5e72\u9884"}}
{"id": "2506.01116", "pdf": "https://arxiv.org/pdf/2506.01116", "abs": "https://arxiv.org/abs/2506.01116", "authors": ["Xinyi Liu", "Lipeng Ma", "Yixuan Li", "Weidong Yang", "Qingyuan Zhou", "Jiayi Song", "Shuhao Li", "Ben Fei"], "title": "ChemAU: Harness the Reasoning of LLMs in Chemical Research with Adaptive Uncertainty Estimation", "categories": ["cs.AI", "q-bio.QM"], "comment": null, "summary": "Large Language Models (LLMs) are widely used across various scenarios due to\ntheir exceptional reasoning capabilities and natural language understanding.\nWhile LLMs demonstrate strong performance in tasks involving mathematics and\ncoding, their effectiveness diminishes significantly when applied to\nchemistry-related problems. Chemistry problems typically involve long and\ncomplex reasoning steps, which contain specific terminology, including\nspecialized symbol systems and complex nomenclature conventions. These\ncharacteristics often cause general LLMs to experience hallucinations during\nthe reasoning process due to their lack of specific knowledge. However,\nexisting methods are struggling to effectively leverage chemical expertise and\nformulas. Moreover, current uncertainty estimation methods, designed to\nmitigate potential reasoning errors, are unable to precisely identify specific\nsteps or key knowledge. In this work, we propose a novel framework called\nChemAU, which incorporates our adaptive uncertainty estimation method that\napplies different uncertainty values based on the position of reasoning steps\nwithin the whole reasoning chain. Leveraging this method, ChemAU identifies\ngaps in chemistry knowledge and precisely supplements chemical expertise with\nthe specialized domain model, thereby correcting and updating the previously\nflawed reasoning chain. Our experiments with three popular LLMs across three\nchemistry datasets demonstrate that ChemAU significantly enhances both\nreasoning accuracy and uncertainty estimation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ChemAU\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5316\u5b66\u95ee\u9898\u4e2d\u7684\u63a8\u7406\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5316\u5b66\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u539f\u56e0\u662f\u5316\u5b66\u95ee\u9898\u7684\u590d\u6742\u6027\u548c\u672f\u8bed\u7279\u6b8a\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5229\u7528\u5316\u5b66\u77e5\u8bc6\u548c\u516c\u5f0f\u3002", "method": "\u63d0\u51faChemAU\u6846\u67b6\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u6839\u636e\u63a8\u7406\u6b65\u9aa4\u7684\u4f4d\u7f6e\u8c03\u6574\u4e0d\u786e\u5b9a\u6027\u503c\uff0c\u8865\u5145\u5316\u5b66\u4e13\u4e1a\u77e5\u8bc6\u4ee5\u4fee\u6b63\u63a8\u7406\u94fe\u3002", "result": "\u5728\u4e09\u4e2a\u5316\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cChemAU\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "conclusion": "ChemAU\u901a\u8fc7\u7cbe\u51c6\u8865\u5145\u5316\u5b66\u77e5\u8bc6\u548c\u81ea\u9002\u5e94\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5316\u5b66\u95ee\u9898\u4e2d\u7684\u5c40\u9650\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u5316\u5b66\u63a8\u7406,\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1,ChemAU,\u81ea\u9002\u5e94\u65b9\u6cd5"}}
{"id": "2506.00495", "pdf": "https://arxiv.org/pdf/2506.00495", "abs": "https://arxiv.org/abs/2506.00495", "authors": ["Xinyi Wang", "Lirong Gao", "Haobo Wang", "Yiming Zhang", "Junbo Zhao"], "title": "FLoE: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": "17 pages, 9 figures", "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a widely\nadopted strategy for adapting pre-trained Large Language Models (LLMs) to\ndownstream tasks, significantly reducing memory and computational costs.\nHowever, most existing PEFT techniques uniformly deploy LoRA adapters across\nall layers, disregarding the intrinsic heterogeneity of layer contributions and\ntask-specific rank requirements. This uniform paradigm leads to redundant\nparameter allocation and suboptimal adaptation efficiency. To address these\nlimitations, we propose FLoE, a novel PEFT framework that introduces two key\ninnovations: (i) a Fisher information-guided importance scoring mechanism to\ndynamically identify task-critical transformer layers for MoE-based low-rank\nadaptation, enabling sparse adapter deployment; and (ii) a Bayesian\noptimization-driven rank allocator that automatically determines optimal LoRA\nranks on specific datasets without exhaustive grid search. Extensive\nexperiments across diverse LLMs and benchmarks reveal that FLoE achieves\nimpressive efficiency-accuracy trade-offs, making FLoE particularly\nadvantageous in resource-constrained environments that necessitate rapid\nadaptation.", "AI": {"tldr": "FLoE\u662f\u4e00\u79cd\u65b0\u9896\u7684PEFT\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8bc6\u522b\u5173\u952e\u5c42\u548c\u81ea\u52a8\u5206\u914dLoRA\u79e9\uff0c\u63d0\u9ad8\u4e86\u9002\u914d\u6548\u7387\u3002", "motivation": "\u73b0\u6709PEFT\u65b9\u6cd5\u5728\u9002\u914d\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u65f6\uff0c\u5747\u5300\u90e8\u7f72LoRA\u9002\u914d\u5668\u5bfc\u81f4\u5197\u4f59\u53c2\u6570\u5206\u914d\u548c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u91c7\u7528Fisher\u4fe1\u606f\u5f15\u5bfc\u7684\u91cd\u8981\u6027\u8bc4\u5206\u673a\u5236\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u79e9\u5206\u914d\u5668\uff0c\u5b9e\u73b0\u7a00\u758f\u9002\u914d\u3002", "result": "FLoE\u5728\u591a\u79cdLLM\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u7684\u51c6\u786e\u6027\u6743\u8861\u3002", "conclusion": "FLoE\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u9700\u8981\u5feb\u901f\u9002\u914d\u7684\u573a\u666f\u3002", "keywords": "PEFT, LoRA, FLoE, Fisher\u4fe1\u606f, \u8d1d\u53f6\u65af\u4f18\u5316"}}
{"id": "2506.00488", "pdf": "https://arxiv.org/pdf/2506.00488", "abs": "https://arxiv.org/abs/2506.00488", "authors": ["Shuguo Hu", "Jun Hu", "Huaiwen Zhang"], "title": "Synergizing LLMs with Global Label Propagation for Multimodal Fake News Detection", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Large Language Models (LLMs) can assist multimodal fake news detection by\npredicting pseudo labels. However, LLM-generated pseudo labels alone\ndemonstrate poor performance compared to traditional detection methods, making\ntheir effective integration non-trivial. In this paper, we propose Global Label\nPropagation Network with LLM-based Pseudo Labeling (GLPN-LLM) for multimodal\nfake news detection, which integrates LLM capabilities via label propagation\ntechniques. The global label propagation can utilize LLM-generated pseudo\nlabels, enhancing prediction accuracy by propagating label information among\nall samples. For label propagation, a mask-based mechanism is designed to\nprevent label leakage during training by ensuring that training nodes do not\npropagate their own labels back to themselves. Experimental results on\nbenchmark datasets show that by synergizing LLMs with label propagation, our\nmodel achieves superior performance over state-of-the-art baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u4f2a\u6807\u7b7e\u7684\u5168\u5c40\u6807\u7b7e\u4f20\u64ad\u7f51\u7edc\uff08GLPN-LLM\uff09\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u5047\u65b0\u95fb\u68c0\u6d4b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6807\u7b7e\u4f20\u64ad\u6280\u672f\u6574\u5408LLM\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1LLM\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u5728\u5047\u65b0\u95fb\u68c0\u6d4b\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u6807\u7b7e\u4f20\u64ad\u6280\u672f\u6574\u5408LLM\u80fd\u529b\u53ef\u4ee5\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "\u63d0\u51faGLPN-LLM\u6a21\u578b\uff0c\u901a\u8fc7\u5168\u5c40\u6807\u7b7e\u4f20\u64ad\u5229\u7528LLM\u751f\u6210\u7684\u4f2a\u6807\u7b7e\uff0c\u907f\u514d\u6807\u7b7e\u6cc4\u6f0f\u7684\u63a9\u7801\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u7ed3\u5408LLM\u548c\u6807\u7b7e\u4f20\u64ad\u6280\u672f\u7684GLPN-LLM\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u5047\u65b0\u95fb\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u4f2a\u6807\u7b7e, \u6807\u7b7e\u4f20\u64ad, \u591a\u6a21\u6001\u5047\u65b0\u95fb\u68c0\u6d4b"}}
{"id": "2506.01174", "pdf": "https://arxiv.org/pdf/2506.01174", "abs": "https://arxiv.org/abs/2506.01174", "authors": ["Muhammad Qasim Ali", "Saeejith Nair", "Alexander Wong", "Yuchen Cui", "Yuhao Chen"], "title": "GraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question Answering", "categories": ["cs.AI"], "comment": "CVPR 2025 Workshop on 3D-LLM/VLA: Bridging Language, Vision and\n  Action in 3D Environments", "summary": "Structured scene representations are a core component of embodied agents,\nhelping to consolidate raw sensory streams into readable, modular, and\nsearchable formats. Due to their high computational overhead, many approaches\nbuild such representations in advance of the task. However, when the task\nspecifications change, such static approaches become inadequate as they may\nmiss key objects, spatial relations, and details. We introduce GraphPad, a\nmodifiable structured memory that an agent can tailor to the needs of the task\nthrough API calls. It comprises a mutable scene graph representing the\nenvironment, a navigation log indexing frame-by-frame content, and a scratchpad\nfor task-specific notes. Together, GraphPad serves as a dynamic workspace that\nremains complete, current, and aligned with the agent's immediate understanding\nof the scene and its task. On the OpenEQA benchmark, GraphPad attains 55.3%, a\n+3.0% increase over an image-only baseline using the same vision-language\nmodel, while operating with five times fewer input frames. These results show\nthat allowing online, language-driven refinement of 3-D memory yields more\ninformative representations without extra training or data collection.", "AI": {"tldr": "GraphPad\u662f\u4e00\u79cd\u53ef\u4fee\u6539\u7684\u7ed3\u6784\u5316\u8bb0\u5fc6\uff0c\u901a\u8fc7API\u8c03\u7528\u52a8\u6001\u8c03\u6574\u573a\u666f\u8868\u793a\uff0c\u63d0\u9ad8\u4efb\u52a1\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u573a\u666f\u8868\u793a\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u4efb\u52a1\u53d8\u5316\uff0cGraphPad\u65e8\u5728\u63d0\u4f9b\u52a8\u6001\u3001\u53ef\u8c03\u6574\u7684\u8bb0\u5fc6\u7ed3\u6784\u3002", "method": "GraphPad\u5305\u542b\u53ef\u53d8\u573a\u666f\u56fe\u3001\u5bfc\u822a\u65e5\u5fd7\u548c\u4efb\u52a1\u4fbf\u7b7e\uff0c\u652f\u6301\u8bed\u8a00\u9a71\u52a8\u7684\u5728\u7ebf\u7ec6\u5316\u3002", "result": "\u5728OpenEQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGraphPad\u6027\u80fd\u63d0\u53473%\uff0c\u4e14\u8f93\u5165\u5e27\u6570\u51cf\u5c1180%\u3002", "conclusion": "\u52a8\u6001\u7ec6\u5316\u76843D\u8bb0\u5fc6\u80fd\u751f\u6210\u66f4\u5177\u4fe1\u606f\u6027\u7684\u8868\u793a\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6570\u636e\u3002", "keywords": "\u7ed3\u6784\u5316\u8bb0\u5fc6,\u573a\u666f\u56fe,\u52a8\u6001\u8c03\u6574,OpenEQA,\u8bed\u8a00\u9a71\u52a8"}}
{"id": "2506.00499", "pdf": "https://arxiv.org/pdf/2506.00499", "abs": "https://arxiv.org/abs/2506.00499", "authors": ["Diogo Landau", "Ingeborg de Pater", "Mihaela Mitici", "Nishant Saurabh"], "title": "Federated learning framework for collaborative remaining useful life prognostics: an aircraft engine case study", "categories": ["cs.LG", "cs.DC", "cs.ET", "cs.SY", "eess.SY", "stat.ML"], "comment": null, "summary": "Complex systems such as aircraft engines are continuously monitored by\nsensors. In predictive aircraft maintenance, the collected sensor measurements\nare used to estimate the health condition and the Remaining Useful Life (RUL)\nof such systems. However, a major challenge when developing prognostics is the\nlimited number of run-to-failure data samples. This challenge could be overcome\nif multiple airlines would share their run-to-failure data samples such that\nsufficient learning can be achieved. Due to privacy concerns, however, airlines\nare reluctant to share their data in a centralized setting. In this paper, a\ncollaborative federated learning framework is therefore developed instead.\nHere, several airlines cooperate to train a collective RUL prognostic machine\nlearning model, without the need to centrally share their data. For this, a\ndecentralized validation procedure is proposed to validate the prognostics\nmodel without sharing any data. Moreover, sensor data is often noisy and of low\nquality. This paper therefore proposes four novel methods to aggregate the\nparameters of the global prognostic model. These methods enhance the robustness\nof the FL framework against noisy data. The proposed framework is illustrated\nfor training a collaborative RUL prognostic model for aircraft engines, using\nthe N-CMAPSS dataset. Here, six airlines are considered, that collaborate in\nthe FL framework to train a collective RUL prognostic model for their\naircraft's engines. When comparing the proposed FL framework with the case\nwhere each airline independently develops their own prognostic model, the\nresults show that FL leads to more accurate RUL prognostics for five out of the\nsix airlines. Moreover, the novel robust aggregation methods render the FL\nframework robust to noisy data samples.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u534f\u4f5c\u5f0f\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u98de\u673a\u53d1\u52a8\u673a\u7684\u5269\u4f59\u4f7f\u7528\u5bff\u547d\uff08RUL\uff09\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u9690\u79c1\u548c\u566a\u58f0\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u98de\u673a\u53d1\u52a8\u673a\u9884\u6d4b\u6027\u7ef4\u62a4\u4e2d\u56e0\u6570\u636e\u9690\u79c1\u9650\u5236\u800c\u65e0\u6cd5\u5171\u4eab\u6570\u636e\u7684\u95ee\u9898\uff0c\u5e76\u5e94\u5bf9\u4f20\u611f\u5668\u6570\u636e\u7684\u566a\u58f0\u548c\u4f4e\u8d28\u91cf\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u534f\u4f5c\u5f0f\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u62ec\u5206\u6563\u9a8c\u8bc1\u7a0b\u5e8f\u548c\u56db\u79cd\u53c2\u6570\u805a\u5408\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u566a\u58f0\u6570\u636e\u7684\u9c81\u68d2\u6027\u3002", "result": "\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u6bd4\u72ec\u7acb\u8bad\u7ec3\u7684\u6a21\u578b\u66f4\u51c6\u786e\uff0c\u4e14\u5728\u516d\u5bb6\u822a\u7a7a\u516c\u53f8\u4e2d\u6709\u4e94\u5bb6\u8868\u73b0\u51fa\u66f4\u597d\u7684RUL\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u534f\u4f5c\u5f0f\u8054\u90a6\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u9690\u79c1\u548c\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u4e86RUL\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60, \u5269\u4f59\u4f7f\u7528\u5bff\u547d, \u9884\u6d4b\u6027\u7ef4\u62a4, \u6570\u636e\u9690\u79c1, \u566a\u58f0\u6570\u636e"}}
{"id": "2506.00507", "pdf": "https://arxiv.org/pdf/2506.00507", "abs": "https://arxiv.org/abs/2506.00507", "authors": ["Dohyun Lee", "Seungil Chad Lee", "Chanwoo Yang", "Yujin Baek", "Jaegul Choo"], "title": "Exploring In-context Example Generation for Machine Translation", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large language models (LLMs) have demonstrated strong performance across\nvarious tasks, leveraging their exceptional in-context learning ability with\nonly a few examples. Accordingly, the selection of optimal in-context examples\nhas been actively studied in the field of machine translation. However, these\nstudies presuppose the presence of a demonstration pool with human-annotated\npairs, making them less applicable to low-resource languages where such an\nassumption is challenging to meet. To overcome this limitation, this paper\nexplores the research direction of in-context example generation for machine\ntranslation. Specifically, we propose Demonstration Augmentation for\nTranslation (DAT), a simple yet effective approach that generates example pairs\nwithout relying on any external resources. This method builds upon two prior\ncriteria, relevance and diversity, which have been highlighted in previous work\nas key factors for in-context example selection. Through experiments and\nanalysis on low-resource languages where human-annotated pairs are scarce, we\nshow that DAT achieves superior translation quality compared to the baselines.\nFurthermore, we investigate the potential of progressively accumulating\ngenerated pairs during test time to build and reuse a demonstration pool. Our\nimplementation is publicly available at https://github.com/aiclaudev/DAT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u7ffb\u8bd1\u7684\u4e0a\u4e0b\u6587\u793a\u4f8b\u751f\u6210\u65b9\u6cd5DAT\uff0c\u89e3\u51b3\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7f3a\u4e4f\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u793a\u4f8b\u6c60\uff0c\u96be\u4ee5\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u56e0\u6b64\u63a2\u7d22\u4e0d\u4f9d\u8d56\u5916\u90e8\u8d44\u6e90\u7684\u793a\u4f8b\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDAT\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u793a\u4f8b\u5bf9\u7684\u76f8\u5173\u6027\u548c\u591a\u6837\u6027\u6765\u751f\u6210\u4e0a\u4e0b\u6587\u793a\u4f8b\uff0c\u65e0\u987b\u5916\u90e8\u8d44\u6e90\u652f\u6301\u3002", "result": "\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5b9e\u9a8c\u4e2d\uff0cDAT\u7684\u7ffb\u8bd1\u8d28\u91cf\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63a2\u7d22\u4e86\u6d4b\u8bd5\u65f6\u9010\u6b65\u79ef\u7d2f\u751f\u6210\u793a\u4f8b\u7684\u6f5c\u529b\u3002", "conclusion": "DAT\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u3002", "keywords": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b,\u673a\u5668\u7ffb\u8bd1,\u4e0a\u4e0b\u6587\u5b66\u4e60,\u4f4e\u8d44\u6e90\u8bed\u8a00,DAT"}}
{"id": "2506.01199", "pdf": "https://arxiv.org/pdf/2506.01199", "abs": "https://arxiv.org/abs/2506.01199", "authors": ["Augusto Mondelli", "Yueshan Li", "Alessandro Zanardi", "Emilio Frazzoli"], "title": "Test Automation for Interactive Scenarios via Promptable Traffic Simulation", "categories": ["cs.AI", "cs.RO"], "comment": "Accepted by CVPR 2025 Workshop Data-Driven Autonomous Driving\n  Simulation (track 1)", "summary": "Autonomous vehicle (AV) planners must undergo rigorous evaluation before\nwidespread deployment on public roads, particularly to assess their robustness\nagainst the uncertainty of human behaviors. While recent advancements in\ndata-driven scenario generation enable the simulation of realistic human\nbehaviors in interactive settings, leveraging these models to construct\ncomprehensive tests for AV planners remains an open challenge. In this work, we\nintroduce an automated method to efficiently generate realistic and\nsafety-critical human behaviors for AV planner evaluation in interactive\nscenarios. We parameterize complex human behaviors using low-dimensional goal\npositions, which are then fed into a promptable traffic simulator, ProSim, to\nguide the behaviors of simulated agents. To automate test generation, we\nintroduce a prompt generation module that explores the goal domain and\nefficiently identifies safety-critical behaviors using Bayesian optimization.\nWe apply our method to the evaluation of an optimization-based planner and\ndemonstrate its effectiveness and efficiency in automatically generating\ndiverse and realistic driving behaviors across scenarios with varying initial\nconditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u7ef4\u76ee\u6807\u4f4d\u7f6e\u53c2\u6570\u5316\u548c\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u9ad8\u6548\u751f\u6210\u5b89\u5168\u548c\u5173\u952e\u7684\u4eba\u7c7b\u884c\u4e3a\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\u7684\u4ea4\u4e92\u573a\u666f\u8bc4\u4f30\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\u5728\u516c\u5171\u9053\u8def\u4e0a\u90e8\u7f72\u524d\u9700\u4e25\u683c\u8bc4\u4f30\u5176\u5bf9\u6297\u4eba\u7c7b\u884c\u4e3a\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5229\u7528\u6570\u636e\u9a71\u52a8\u573a\u666f\u751f\u6210\u6a21\u578b\u6784\u5efa\u5168\u9762\u6d4b\u8bd5\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u4eba\u7c7b\u884c\u4e3a\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u53ef\u63d0\u793a\u7684\u4ea4\u901a\u6a21\u62df\u5668ProSim\u548c\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u81ea\u52a8\u751f\u6210\u5b89\u5168\u548c\u5173\u952e\u7684\u884c\u4e3a\u6d4b\u8bd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4f18\u5316\u89c4\u5212\u5668\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u9ad8\u6548\u751f\u6210\u591a\u6837\u4e14\u771f\u5b9e\u7684\u9a7e\u9a76\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u81ea\u52a8\u5316\u624b\u6bb5\u3002", "keywords": "\u81ea\u52a8\u9a7e\u9a76\u3001\u4eba\u7c7b\u884c\u4e3a\u4e0d\u786e\u5b9a\u6027\u3001\u573a\u666f\u751f\u6210\u3001\u8d1d\u53f6\u65af\u4f18\u5316"}}
{"id": "2506.00505", "pdf": "https://arxiv.org/pdf/2506.00505", "abs": "https://arxiv.org/abs/2506.00505", "authors": ["Hanxiao Qu", "Krzysztof Gogol", "Florian Groetschla", "Claudio Tessone"], "title": "From Rules to Rewards: Reinforcement Learning for Interest Rate Adjustment in DeFi Lending", "categories": ["cs.LG"], "comment": null, "summary": "Decentralized Finance (DeFi) lending enables permissionless borrowing via\nsmart contracts. However, it faces challenges in optimizing interest rates,\nmitigating bad debt, and improving capital efficiency. Rule-based interest-rate\nmodels struggle to adapt to dynamic market conditions, leading to\ninefficiencies. This work applies Offline Reinforcement Learning (RL) to\noptimize interest rate adjustments in DeFi lending protocols. Using historical\ndata from Aave protocol, we evaluate three RL approaches: Conservative\nQ-Learning (CQL), Behavior Cloning (BC), and TD3 with Behavior Cloning\n(TD3-BC). TD3-BC demonstrates superior performance in balancing utilization,\ncapital stability, and risk, outperforming existing models. It adapts\neffectively to historical stress events like the May 2021 crash and the March\n2023 USDC depeg, showcasing potential for automated, real-time governance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4f18\u5316DeFi\u501f\u8d37\u534f\u8bae\u7684\u5229\u7387\u8c03\u6574\uff0cTD3-BC\u65b9\u6cd5\u5728\u5386\u53f2\u538b\u529b\u4e8b\u4ef6\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "motivation": "\u89e3\u51b3DeFi\u501f\u8d37\u4e2d\u5229\u7387\u4f18\u5316\u7684\u52a8\u6001\u9002\u5e94\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u8d44\u672c\u6548\u7387\u548c\u51cf\u5c11\u574f\u8d26\u3002", "method": "\u4f7f\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u8bc4\u4f30CQL\u3001BC\u548cTD3-BC\u4e09\u79cd\u65b9\u6cd5\uff0c\u57fa\u4e8eAave\u534f\u8bae\u7684\u5386\u53f2\u6570\u636e\u3002", "result": "TD3-BC\u5728\u5e73\u8861\u5229\u7528\u7387\u3001\u8d44\u672c\u7a33\u5b9a\u6027\u548c\u98ce\u9669\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "TD3-BC\u80fd\u6709\u6548\u9002\u5e94\u5386\u53f2\u538b\u529b\u4e8b\u4ef6\uff0c\u663e\u793a\u51fa\u5b9e\u65f6\u81ea\u52a8\u5316\u6cbb\u7406\u7684\u6f5c\u529b\u3002", "keywords": "DeFi, \u5f3a\u5316\u5b66\u4e60, \u5229\u7387\u4f18\u5316, Aave, TD3-BC"}}
{"id": "2506.00509", "pdf": "https://arxiv.org/pdf/2506.00509", "abs": "https://arxiv.org/abs/2506.00509", "authors": ["Zherui Li", "Yan Mi", "Zhenhong Zhou", "Houcheng Jiang", "Guibin Zhang", "Kun Wang", "Junfeng Fang"], "title": "Goal-Aware Identification and Rectification of Misinformation in Multi-Agent Systems", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Model-based Multi-Agent Systems (MASs) have demonstrated\nstrong advantages in addressing complex real-world tasks. However, due to the\nintroduction of additional attack surfaces, MASs are particularly vulnerable to\nmisinformation injection. To facilitate a deeper understanding of\nmisinformation propagation dynamics within these systems, we introduce\nMisinfoTask, a novel dataset featuring complex, realistic tasks designed to\nevaluate MAS robustness against such threats. Building upon this, we propose\nARGUS, a two-stage, training-free defense framework leveraging goal-aware\nreasoning for precise misinformation rectification within information flows.\nOur experiments demonstrate that in challenging misinformation scenarios, ARGUS\nexhibits significant efficacy across various injection attacks, achieving an\naverage reduction in misinformation toxicity of approximately 28.17% and\nimproving task success rates under attack by approximately 10.33%. Our code and\ndataset is available at: https://github.com/zhrli324/ARGUS.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aARGUS\u7684\u4e24\u9636\u6bb5\u9632\u5fa1\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u9519\u8bef\u4fe1\u606f\u4f20\u64ad\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u51cf\u5c11\u9519\u8bef\u4fe1\u606f\u6bd2\u6027\u548c\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u9519\u8bef\u4fe1\u606f\u6ce8\u5165\u653b\u51fb\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7814\u7a76\u9519\u8bef\u4fe1\u606f\u4f20\u64ad\u7684\u52a8\u6001\uff0c\u63d0\u9ad8\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u4e86MisinfoTask\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u4e24\u9636\u6bb5\u9632\u5fa1\u6846\u67b6ARGUS\uff0c\u5229\u7528\u76ee\u6807\u611f\u77e5\u63a8\u7406\u6765\u7ea0\u6b63\u4fe1\u606f\u6d41\u4e2d\u7684\u9519\u8bef\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cARGUS\u5728\u5e94\u5bf9\u5404\u79cd\u9519\u8bef\u4fe1\u606f\u6ce8\u5165\u653b\u51fb\u65f6\u8868\u73b0\u51fa\u663e\u8457\u6548\u679c\uff0c\u5e73\u5747\u51cf\u5c11\u9519\u8bef\u4fe1\u606f\u6bd2\u6027\u7ea628.17%\uff0c\u5e76\u5728\u53d7\u653b\u51fb\u65f6\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u7ea610.33%\u3002", "conclusion": "ARGUS\u662f\u4e00\u79cd\u6709\u6548\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5bf9\u9519\u8bef\u4fe1\u606f\u653b\u51fb\u7684\u62b5\u6297\u529b\u3002", "keywords": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf, \u9519\u8bef\u4fe1\u606f, \u9632\u5fa1\u6846\u67b6, ARGUS, MisinfoTask"}}
{"id": "2506.01268", "pdf": "https://arxiv.org/pdf/2506.01268", "abs": "https://arxiv.org/abs/2506.01268", "authors": ["Yudong Lu", "Yazhe Niu", "Shuai Hu", "Haolin Wang"], "title": "CleanS2S: Single-file Framework for Proactive Speech-to-Speech Interaction", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "CleanS2S is a framework for human-like speech-to-speech interaction that\nadvances conversational AI through single-file implementation and proactive\ndialogue capabilities. Our system integrates automatic speech recognition,\nlarge language models, and text-to-speech synthesis into a unified pipeline\nwith real-time interruption handling, achieving low transition latency through\nfull-duplex websocket connections and non-blocking I/O. Beyond conventional\nchatbot paradigms, we pioneer a proactive interaction mechanism, which combines\nmemory systems with Subjective Action Judgement module, enabling five\nhuman-like response strategies: interruption, refusal, deflection, silence, and\nstandard response. The memory module dynamically aggregates historical, and\ncontextual data to inform interaction decisions. This approach breaks the rigid\nturn-based convention by allowing system-initiated dialog control and\ncontext-aware response selection. And we propose Action Judgement SFT that\nassesses input streams for responses strategies. The framework's single-file\nimplementation with atomic configurations offers researchers unprecedented\ntransparency and extensibility for interaction agents. The code of CleanS2S is\nreleased at \\https://github.com/opendilab/CleanS2S.", "AI": {"tldr": "CleanS2S\u662f\u4e00\u4e2a\u901a\u8fc7\u5355\u6587\u4ef6\u5b9e\u73b0\u548c\u4e3b\u52a8\u5bf9\u8bdd\u80fd\u529b\u63a8\u52a8\u5bf9\u8bddAI\u53d1\u5c55\u7684\u8bed\u97f3\u5230\u8bed\u97f3\u4ea4\u4e92\u6846\u67b6\u3002", "motivation": "\u65e8\u5728\u6253\u7834\u4f20\u7edf\u7684\u57fa\u4e8e\u8f6e\u6b21\u7684\u5bf9\u8bdd\u6a21\u5f0f\uff0c\u5b9e\u73b0\u66f4\u4eba\u6027\u5316\u7684\u8bed\u97f3\u4ea4\u4e92\uff0c\u540c\u65f6\u63d0\u4f9b\u900f\u660e\u4e14\u53ef\u6269\u5c55\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002", "method": "\u6574\u5408\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6587\u672c\u5230\u8bed\u97f3\u5408\u6210\uff0c\u5229\u7528\u5168\u53cc\u5de5WebSocket\u8fde\u63a5\u548c\u975e\u963b\u585eI/O\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\uff0c\u7ed3\u5408\u5185\u5b58\u7cfb\u7edf\u548c\u4e3b\u89c2\u884c\u52a8\u5224\u65ad\u6a21\u5757\u5b9e\u73b0\u4e3b\u52a8\u4ea4\u4e92\u7b56\u7565\u3002", "result": "\u5b9e\u73b0\u4e86\u4e94\u79cd\u4eba\u6027\u5316\u54cd\u5e94\u7b56\u7565\uff08\u6253\u65ad\u3001\u62d2\u7edd\u3001\u56de\u907f\u3001\u6c89\u9ed8\u548c\u6807\u51c6\u54cd\u5e94\uff09\uff0c\u5e76\u652f\u6301\u7cfb\u7edf\u53d1\u8d77\u7684\u5bf9\u8bdd\u63a7\u5236\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u54cd\u5e94\u9009\u62e9\u3002", "conclusion": "CleanS2S\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u9ad8\u5ea6\u900f\u660e\u548c\u53ef\u6269\u5c55\u7684\u8bed\u97f3\u4ea4\u4e92\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u5bf9\u8bddAI\u7684\u53d1\u5c55\u3002", "keywords": "\u8bed\u97f3\u5230\u8bed\u97f3\u4ea4\u4e92\uff0c\u4e3b\u52a8\u5bf9\u8bdd\uff0c\u4f4e\u5ef6\u8fdf\uff0c\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u5f00\u6e90\u6846\u67b6"}}
{"id": "2506.00528", "pdf": "https://arxiv.org/pdf/2506.00528", "abs": "https://arxiv.org/abs/2506.00528", "authors": ["Richard Connor", "Alan Dearle", "Ben Claydon"], "title": "Ultra-Quantisation: Efficient Embedding Search via 1.58-bit Encodings", "categories": ["cs.LG", "cs.DB"], "comment": "Submitted to SISAP25 International Conference on Similarity Search\n  and Applications", "summary": "Many modern search domains comprise high-dimensional vectors of floating\npoint numbers derived from neural networks, in the form of embeddings. Typical\nembeddings range in size from hundreds to thousands of dimensions, making the\nsize of the embeddings, and the speed of comparison, a significant issue.\n  Quantisation is a class of mechanism which replaces the floating point values\nwith a smaller representation, for example a short integer. This gives an\napproximation of the embedding space in return for a smaller data\nrepresentation and a faster comparison function.\n  Here we take this idea almost to its extreme: we show how vectors of\narbitrary-precision floating point values can be replaced by vectors whose\nelements are drawn from the set {-1,0,1}. This yields very significant savings\nin space and metric evaluation cost, while maintaining a strong correlation for\nsimilarity measurements.\n  This is achieved by way of a class of convex polytopes which exist in the\nhigh-dimensional space. In this article we give an outline description of these\nobjects, and show how they can be used for the basis of such radical\nquantisation while maintaining a surprising degree of accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6781\u7aef\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u5c06\u9ad8\u7ef4\u6d6e\u70b9\u5411\u91cf\u66ff\u6362\u4e3a\u4ec5\u5305\u542b{-1,0,1}\u7684\u5411\u91cf\uff0c\u663e\u8457\u8282\u7701\u7a7a\u95f4\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u9ad8\u7ef4\u5d4c\u5165\u5411\u91cf\u5360\u7528\u5927\u91cf\u7a7a\u95f4\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u5bfb\u627e\u9ad8\u6548\u7684\u91cf\u5316\u65b9\u6cd5\u4ee5\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u3002", "method": "\u901a\u8fc7\u4e00\u7c7b\u9ad8\u7ef4\u51f8\u591a\u9762\u4f53\uff0c\u5c06\u4efb\u610f\u7cbe\u5ea6\u7684\u6d6e\u70b9\u5411\u91cf\u66ff\u6362\u4e3a{-1,0,1}\u7684\u5411\u91cf\u3002", "result": "\u6b64\u65b9\u6cd5\u663e\u8457\u8282\u7701\u4e86\u7a7a\u95f4\u548c\u8ba1\u7b97\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u7684\u5f3a\u76f8\u5173\u6027\u3002", "conclusion": "\u6781\u7aef\u91cf\u5316\u65b9\u6cd5\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u6781\u5927\u4f18\u5316\u4e86\u5d4c\u5165\u5411\u91cf\u7684\u5b58\u50a8\u548c\u8ba1\u7b97\u6548\u7387\u3002", "keywords": "\u91cf\u5316, \u5d4c\u5165\u5411\u91cf, \u9ad8\u7ef4\u7a7a\u95f4, \u51f8\u591a\u9762\u4f53, \u76f8\u4f3c\u5ea6\u6d4b\u91cf"}}
{"id": "2506.00514", "pdf": "https://arxiv.org/pdf/2506.00514", "abs": "https://arxiv.org/abs/2506.00514", "authors": ["Tianhui Zhang", "Bei Peng", "Danushka Bollegala"], "title": "Evaluating the Evaluation of Diversity in Commonsense Generation", "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "In commonsense generation, given a set of input concepts, a model must\ngenerate a response that is not only commonsense bearing, but also capturing\nmultiple diverse viewpoints. Numerous evaluation metrics based on form- and\ncontent-level overlap have been proposed in prior work for evaluating the\ndiversity of a commonsense generation model. However, it remains unclear as to\nwhich metrics are best suited for evaluating the diversity in commonsense\ngeneration. To address this gap, we conduct a systematic meta-evaluation of\ndiversity metrics for commonsense generation. We find that form-based diversity\nmetrics tend to consistently overestimate the diversity in sentence sets, where\neven randomly generated sentences are assigned overly high diversity scores. We\nthen use an Large Language Model (LLM) to create a novel dataset annotated for\nthe diversity of sentences generated for a commonsense generation task, and use\nit to conduct a meta-evaluation of the existing diversity evaluation metrics.\nOur experimental results show that content-based diversity evaluation metrics\nconsistently outperform the form-based counterparts, showing high correlations\nwith the LLM-based ratings. We recommend that future work on commonsense\ngeneration should use content-based metrics for evaluating the diversity of\ntheir outputs.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5e38\u8bc6\u751f\u6210\u4e2d\u591a\u6837\u6027\u6307\u6807\u7684\u9002\u7528\u6027\uff0c\u53d1\u73b0\u57fa\u4e8e\u5185\u5bb9\u7684\u6307\u6807\u4f18\u4e8e\u57fa\u4e8e\u5f62\u5f0f\u7684\u6307\u6807\u3002", "motivation": "\u89e3\u51b3\u5e38\u8bc6\u751f\u6210\u4efb\u52a1\u4e2d\u591a\u6837\u6027\u8bc4\u4f30\u6307\u6807\u9009\u62e9\u4e0d\u660e\u786e\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5143\u8bc4\u4f30\u548cLLM\u6807\u6ce8\u65b0\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u57fa\u4e8e\u5f62\u5f0f\u548c\u5185\u5bb9\u7684\u591a\u6837\u6027\u6307\u6807\u3002", "result": "\u57fa\u4e8e\u5185\u5bb9\u7684\u591a\u6837\u6027\u6307\u6807\u8868\u73b0\u66f4\u4f18\uff0c\u4e0eLLM\u8bc4\u5206\u9ad8\u5ea6\u76f8\u5173\u3002", "conclusion": "\u5efa\u8bae\u672a\u6765\u5e38\u8bc6\u751f\u6210\u7814\u7a76\u4f7f\u7528\u57fa\u4e8e\u5185\u5bb9\u7684\u591a\u6837\u6027\u8bc4\u4f30\u6307\u6807\u3002", "keywords": "\u5e38\u8bc6\u751f\u6210\u3001\u591a\u6837\u6027\u8bc4\u4f30\u3001\u5143\u8bc4\u4f30\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u5185\u5bb9\u591a\u6837\u6027"}}
{"id": "2506.01273", "pdf": "https://arxiv.org/pdf/2506.01273", "abs": "https://arxiv.org/abs/2506.01273", "authors": ["Fernando Granado", "Roberto Lotufo", "Jayr Pereira"], "title": "RAISE: Reasoning Agent for Interactive SQL Exploration", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have propelled research in\nnatural language interfaces to databases. However, most state-of-the-art\ntext-to-SQL systems still depend on complex, multi-stage pipelines. This work\nproposes a novel agentic framework that unifies schema linking, query\ngeneration, and iterative refinement within a single, end-to-end component. By\nleveraging the intrinsic reasoning abilities of LLMs, our method emulates how\nhumans answer questions when working with unfamiliar databases: understanding\nthe data by formulating hypotheses, running dynamic queries to validate them,\nreasoning over the results, and revising outputs based on observed results.\nCrucially, our approach introduces a new strategy for scaling test-time\ncomputation in text-to-SQL: we scale the depth of interactive database\nexploration and reflection. This shift enables the model to allocate\ncomputation dynamically to better understand the data, especially useful in\nambiguous and underspecified scenarios. Our experiments show that it improved\nthe Execution Accuracy (EX) from 44.8% to 56.5% on the challenging BIRD dataset\nusing DeepSeek-R1-Distill-Llama-70B. Furthermore, when equipped with steps to\nadd more diversity to the answers, our agent achieves a Best-of-N accuracy of\n81.8% with 8 rounds of candidate generation, rivaling the 82.79% achieved by\nthe top-ranked published solution, while reducing engineering complexity. These\nfindings position our unified framework as a promising alternative for building\nnatural language interfaces to databases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7aef\u5230\u7aef\u6587\u672c\u5230SQL\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u67e5\u8be2\u9a8c\u8bc1\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230SQL\u7cfb\u7edf\u4f9d\u8d56\u590d\u6742\u7684\u591a\u9636\u6bb5\u6d41\u7a0b\uff0c\u800c\u57fa\u4e8eLLM\u7684\u63a8\u7406\u80fd\u529b\u53ef\u4ee5\u6a21\u62df\u4eba\u7c7b\u5904\u7406\u964c\u751f\u6570\u636e\u5e93\u7684\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u96c6\u6210\u6a21\u5f0f\u94fe\u63a5\u3001\u67e5\u8be2\u751f\u6210\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u4ee5\u5e94\u5bf9\u6a21\u7cca\u573a\u666f\u3002", "result": "\u5728BIRD\u6570\u636e\u96c6\u4e0a\uff0c\u6267\u884c\u51c6\u786e\u7387\u4ece44.8%\u63d0\u5347\u81f356.5%\uff0c\u901a\u8fc78\u8f6e\u5019\u9009\u751f\u6210\u8fbe\u523081.8%\u7684\u6700\u4f73\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6784\u5efa\u81ea\u7136\u8bed\u8a00\u6570\u636e\u5e93\u63a5\u53e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5de5\u7a0b\u590d\u6742\u5ea6\u4f4e\u7684\u65b9\u6848\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u6587\u672c\u5230SQL, \u4ee3\u7406\u6846\u67b6, \u52a8\u6001\u67e5\u8be2, \u8fed\u4ee3\u4f18\u5316"}}
{"id": "2506.00531", "pdf": "https://arxiv.org/pdf/2506.00531", "abs": "https://arxiv.org/abs/2506.00531", "authors": ["Hang Fana", "Mingxuan Lib", "Zuhan Zhanga", "Long Chengc", "Yujian Ye", "Dunnan Liua"], "title": "M2WLLM: Multi-Modal Multi-Task Ultra-Short-term Wind Power Prediction Algorithm Based on Large Language Model", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The integration of wind energy into power grids necessitates accurate\nultra-short-term wind power forecasting to ensure grid stability and optimize\nresource allocation. This study introduces M2WLLM, an innovative model that\nleverages the capabilities of Large Language Models (LLMs) for predicting wind\npower output at granular time intervals. M2WLLM overcomes the limitations of\ntraditional and deep learning methods by seamlessly integrating textual\ninformation and temporal numerical data, significantly improving wind power\nforecasting accuracy through multi-modal data. Its architecture features a\nPrompt Embedder and a Data Embedder, enabling an effective fusion of textual\nprompts and numerical inputs within the LLMs framework. The Semantic Augmenter\nwithin the Data Embedder translates temporal data into a format that the LLMs\ncan comprehend, enabling it to extract latent features and improve prediction\naccuracy. The empirical evaluations conducted on wind farm data from three\nChinese provinces demonstrate that M2WLLM consistently outperforms existing\nmethods, such as GPT4TS, across various datasets and prediction horizons. The\nresults highlight LLMs' ability to enhance accuracy and robustness in\nultra-short-term forecasting and showcase their strong few-shot learning\ncapabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM2WLLM\u7684\u521b\u65b0\u6a21\u578b\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u8d85\u77ed\u671f\u98ce\u7535\u529f\u7387\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u98ce\u7535\u5e76\u7f51\u9700\u8981\u8d85\u77ed\u671f\u98ce\u7535\u529f\u7387\u9884\u6d4b\u4ee5\u786e\u4fdd\u7535\u7f51\u7a33\u5b9a\u548c\u8d44\u6e90\u4f18\u5316\u914d\u7f6e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "M2WLLM\u901a\u8fc7Prompt Embedder\u548cData Embedder\uff0c\u7ed3\u5408\u6587\u672c\u548c\u6570\u503c\u6570\u636e\uff0c\u5229\u7528\u8bed\u4e49\u589e\u5f3a\u5668\u5c06\u65f6\u95f4\u6570\u636e\u8f6c\u6362\u4e3aLLMs\u53ef\u7406\u89e3\u7684\u683c\u5f0f\uff0c\u63d0\u53d6\u6f5c\u5728\u7279\u5f81\u3002", "result": "\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u5185\uff0cM2WLLM\u5747\u4f18\u4e8eGPT4TS\u7b49\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86LLMs\u5728\u8d85\u77ed\u671f\u9884\u6d4b\u4e2d\u7684\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "LLMs\u5728\u8d85\u77ed\u671f\u98ce\u7535\u529f\u7387\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u5907\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\uff0c\u4e3a\u98ce\u7535\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u98ce\u7535\u529f\u7387\u9884\u6d4b,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u591a\u6a21\u6001\u6570\u636e,\u5c11\u6837\u672c\u5b66\u4e60,\u8d85\u77ed\u671f\u9884\u6d4b"}}
{"id": "2506.00519", "pdf": "https://arxiv.org/pdf/2506.00519", "abs": "https://arxiv.org/abs/2506.00519", "authors": ["Yuxi Sun", "Aoqi Zuo", "Wei Gao", "Jing Ma"], "title": "CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Association for Computational Linguistics Findings (ACL)\n  2025", "summary": "Large Language Models (LLMs) often exhibit knowledge disparities across\nlanguages. Encouraging LLMs to \\textit{abstain} when faced with knowledge gaps\nis a promising strategy to reduce hallucinations in multilingual settings.\nCurrent abstention strategies for multilingual scenarios primarily rely on\ngenerating feedback in various languages using LLMs and performing\nself-reflection. However, these methods can be adversely impacted by\ninaccuracies and biases in the generated feedback. To address this, from a\ncausal perspective, we introduce \\textit{CausalAbstain}, a method that helps\nLLMs determine whether to utilize multiple generated feedback responses and how\nto identify the most useful ones. Extensive experiments demonstrate that\n\\textit{CausalAbstain} effectively selects helpful feedback and enhances\nabstention decisions with interpretability in both native language\n(\\textsc{Casual-native}) and multilingual (\\textsc{Causal-multi}) settings,\noutperforming strong baselines on two benchmark datasets covering encyclopedic\nand commonsense knowledge QA tasks. Our code and data are open-sourced at\nhttps://github.com/peachch/CausalAbstain.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u2018CausalAbstain\u2019\u65b9\u6cd5\uff0c\u901a\u8fc7\u56e0\u679c\u5206\u6790\u6539\u8fdbLLMs\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u77e5\u8bc6\u5dee\u8ddd\u95ee\u9898\uff0c\u51cf\u5c11\u5e7b\u89c9\u73b0\u8c61\u3002", "motivation": "\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u77e5\u8bc6\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5e2e\u52a9\u6a21\u578b\u5728\u4e0d\u6e05\u695a\u65f6\u9009\u62e9\u2018\u653e\u5f03\u2019\uff0c\u4ee5\u51cf\u5c11\u5e7b\u89c9\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u81ea\u6211\u53cd\u9988\uff0c\u4f46\u6613\u53d7\u53cd\u9988\u4e2d\u7684\u9519\u8bef\u548c\u504f\u89c1\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u56e0\u679c\u89c6\u89d2\uff0c\u63d0\u51fa\u2018CausalAbstain\u2019\u65b9\u6cd5\uff0c\u6307\u5bfcLLMs\u5982\u4f55\u5229\u7528\u591a\u8bed\u8a00\u53cd\u9988\u5e76\u9009\u62e9\u6700\u6709\u7528\u7684\u90e8\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u767e\u79d1\u5168\u4e66\u548c\u5e38\u8bc6\u95ee\u7b54\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5177\u6709\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u2018CausalAbstain\u2019\u80fd\u6709\u6548\u63d0\u5347LLMs\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u653e\u5f03\u51b3\u7b56\u80fd\u529b\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u591a\u8bed\u8a00\u3001\u77e5\u8bc6\u5dee\u8ddd\u3001\u56e0\u679c\u5206\u6790\u3001\u653e\u5f03\u7b56\u7565"}}
{"id": "2506.01275", "pdf": "https://arxiv.org/pdf/2506.01275", "abs": "https://arxiv.org/abs/2506.01275", "authors": ["Artemis Panagopoulou", "Le Xue", "Honglu Zhou", "silvio savarese", "Ran Xu", "Caiming Xiong", "Chris Callison-Burch", "Mark Yatskar", "Juan Carlos Niebles"], "title": "Contra4: Evaluating Contrastive Cross-Modal Reasoning in Audio, Video, Image, and 3D", "categories": ["cs.AI"], "comment": null, "summary": "Real-world decision-making often begins with identifying which modality\ncontains the most relevant information for a given query. While recent\nmultimodal models have made impressive progress in processing diverse inputs,\nit remains unclear whether they can reason contrastively across multiple\nmodalities to select the one that best satisfies a natural language prompt. We\nargue this capability is foundational, especially in retrieval-augmented and\ndecision-time contexts, where systems must evaluate multiple signals and\nidentify which one conveys the relevant information. To evaluate this skill, we\nintroduce Contra4, a dataset for contrastive cross-modal reasoning across four\nmodalities: image, audio, video, and 3D. Each example presents a natural\nlanguage question alongside multiple candidate modality instances, and the\nmodel must select the one that semantically aligns with the prompt. Contra4\ncombines human-annotated captions with a mixture-of-models\nround-trip-consistency filter to ensure high-quality supervision, resulting in\n174k training examples and a manually verified test set of 2.3k samples. While\ntask-specific fine-tuning improves performance by 56% relative to baseline,\nstate-of-the-art models still achieve only 56% accuracy overall and 42% in\nfour-modality settings, underscoring a significant limitation in current\nmultimodal models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faContra4\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u5728\u8de8\u6a21\u6001\u5bf9\u6bd4\u63a8\u7406\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u56db\u6a21\u6001\u573a\u666f\u4e0b\u8868\u73b0\u6709\u9650\u3002", "motivation": "\u73b0\u5b9e\u51b3\u7b56\u9700\u8981\u8bc6\u522b\u6700\u76f8\u5173\u4fe1\u606f\u7684\u6a21\u6001\uff0c\u4f46\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u662f\u5426\u80fd\u8de8\u6a21\u6001\u5bf9\u6bd4\u63a8\u7406\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5f15\u5165Contra4\u6570\u636e\u96c6\uff0c\u5305\u542b\u56db\u79cd\u6a21\u6001\u7684\u6837\u672c\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u6d4b\u8bd5\u6a21\u578b\u9009\u62e9\u80fd\u529b\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728\u56db\u6a21\u6001\u573a\u666f\u4e0b\u51c6\u786e\u7387\u4ec542%\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8de8\u6a21\u6001\u5bf9\u6bd4\u63a8\u7406\u662f\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u7684\u663e\u8457\u77ed\u677f\u3002", "keywords": "\u591a\u6a21\u6001\u6a21\u578b, \u5bf9\u6bd4\u63a8\u7406, Contra4\u6570\u636e\u96c6, \u6a21\u6001\u9009\u62e9"}}
{"id": "2506.00533", "pdf": "https://arxiv.org/pdf/2506.00533", "abs": "https://arxiv.org/abs/2506.00533", "authors": ["Junquan Huang", "Zong-Gan Chen", "Yuncheng Jiang", "Zhi-Hui Zhan"], "title": "RsGCN: Rescaling Enhances Generalization of GCNs for Solving Scalable Traveling Salesman Problems", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Neural traveling salesman problem (TSP) solvers face two critical challenges:\npoor generalization for scalable TSPs and high training costs. To address these\nchallenges, we propose a new Rescaling Graph Convolutional Network (RsGCN).\nFocusing on the scale-dependent features (i.e., features varied with problem\nscales) related to nodes and edges that influence the sensitivity of GCNs to\nthe problem scales, a Rescaling Mechanism in RsGCN enhances the generalization\ncapability by (1) rescaling adjacent nodes to construct a subgraph with a\nuniform number of adjacent nodes for each node across various scales of TSPs,\nwhich stabilizes the graph message aggregation; (2) rescaling subgraph edges to\nadjust the lengths of subgraph edges to the same magnitude, which maintains\nnumerical consistency. In addition, an efficient training strategy with a\nmixed-scale dataset and bidirectional loss is used in RsGCN. To fully exploit\nthe heatmaps generated by RsGCN, we design an efficient post-search algorithm\ntermed Re2Opt, in which a reconstruction process based on adaptive weight is\nincorporated to help avoid local optima. Based on a combined architecture of\nRsGCN and Re2Opt, our solver achieves remarkable generalization and low\ntraining cost: with only 3 epochs of training on the mixed-scale dataset\ncontaining instances with up to 100 nodes, it can be generalized successfully\nto 10K-node instances without any fine-tuning. Extensive experiments\ndemonstrate our state-of-the-art performance across uniform distribution\ninstances of 9 different scales from 20 to 10K nodes and 78 real-world\ninstances from TSPLIB, while requiring the fewest learnable parameters and\ntraining epochs among neural competitors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Rescaling Graph Convolutional Network (RsGCN)\u548cRe2Opt\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u795e\u7ecf\u7f51\u7edc\u6c42\u89e3\u65c5\u884c\u5546\u95ee\u9898\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u8bad\u7ec3\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u7f51\u7edc\u65c5\u884c\u5546\u95ee\u9898(TSP)\u6c42\u89e3\u5668\u9762\u4e34\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u548c\u8bad\u7ec3\u6210\u672c\u9ad8\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faRsGCN\uff0c\u901a\u8fc7\u8282\u70b9\u548c\u8fb9\u7684\u91cd\u65b0\u7f29\u653e\u673a\u5236\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u7ed3\u5408Re2Opt\u540e\u641c\u7d22\u7b97\u6cd5\u4f18\u5316\u70ed\u56fe\u3002", "result": "\u8be5\u65b9\u6cd5\u4ec5\u97003\u4e2a\u8bad\u7ec3\u5468\u671f\u5373\u53ef\u4ece100\u8282\u70b9\u6cdb\u5316\u523010K\u8282\u70b9\uff0c\u4e14\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "RsGCN\u548cRe2Opt\u7684\u7ec4\u5408\u5728\u6cdb\u5316\u80fd\u529b\u548c\u8bad\u7ec3\u6548\u7387\u4e0a\u8fbe\u5230\u9886\u5148\u6c34\u5e73\u3002", "keywords": "TSP, RsGCN, Rescaling Mechanism, Re2Opt, Generalization"}}
{"id": "2506.00527", "pdf": "https://arxiv.org/pdf/2506.00527", "abs": "https://arxiv.org/abs/2506.00527", "authors": ["Runtao Ren", "Jian Ma", "Jianxi Luo"], "title": "Retrieval-Augmented Generation Systems for Intellectual Property via Synthetic Multi-Angle Fine-tuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems in the Intellectual Property\n(IP) field often struggle with diverse user queries, including colloquial\nexpressions, spelling errors, and ambiguous terminology, leading to inaccurate\nretrieval and suboptimal responses. To address this challenge, we propose\nMulti-Angle Question Generation and Retrieval Fine-Tuning Method (MQG-RFM), a\nnovel framework that leverages large language models (LLMs) to simulate varied\nuser inquiries and fine-tunes retrieval models to align semantically equivalent\nbut linguistically diverse questions. Unlike complex architectural\nmodifications, MQG-RFM adopts a lightweight Data-to-Tune paradigm, combining\nprompt-engineered query generation with hard negative mining to enhance\nretrieval robustness without costly infrastructure changes. Experimental\nresults on a Taiwan patent Q&A dataset show 185.62% improvement in retrieval\naccuracy on the Patent Consultation dataset and 262.26% improvement on the\nNovel Patent Technology Report dataset, with 14.22% and 53.58% improvements in\ngeneration quality over the baselines, respectively. By bridging the gap\nbetween user intent and system comprehension through semantic-aware retrieval\noptimization, MQG-RFM offers a practical, scalable approach for rapid,\ncost-effective deployment among small and medium-sized agencies seeking\nreliable patent intelligence solutions. Additionally, our proposed method has\nalready been adopted by ScholarMate, the largest professional research social\nnetworking platform in China, to support real-world development and deployment.\nA demo version of the instantiated is available at\nhttps://github.com/renruntao/patent_rag.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMQG-RFM\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89d2\u5ea6\u95ee\u9898\u751f\u6210\u548c\u68c0\u7d22\u5fae\u8c03\u65b9\u6cd5\u89e3\u51b3IP\u9886\u57df\u4e2dRAG\u7cfb\u7edf\u5bf9\u591a\u6837\u5316\u7528\u6237\u67e5\u8be2\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "IP\u9886\u57df\u7684RAG\u7cfb\u7edf\u5728\u9762\u5bf9\u591a\u6837\u5316\u7684\u7528\u6237\u67e5\u8be2\uff08\u5982\u53e3\u8bed\u5316\u8868\u8fbe\u3001\u62fc\u5199\u9519\u8bef\u548c\u672f\u8bed\u6a21\u7cca\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u68c0\u7d22\u4e0d\u51c6\u786e\u548c\u54cd\u5e94\u4e0d\u7406\u60f3\u3002", "method": "\u63d0\u51faMQG-RFM\u6846\u67b6\uff0c\u5229\u7528LLM\u6a21\u62df\u591a\u6837\u5316\u7528\u6237\u67e5\u8be2\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u7b49\u6548\u95ee\u9898\u5fae\u8c03\u68c0\u7d22\u6a21\u578b\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u7684Data-to-Tune\u8303\u5f0f\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u548c\u786c\u8d1f\u4f8b\u6316\u6398\u3002", "result": "\u5728\u53f0\u6e7e\u4e13\u5229Q&A\u6570\u636e\u96c6\u4e0a\uff0c\u68c0\u7d22\u51c6\u786e\u7387\u5206\u522b\u63d0\u9ad8\u4e86185.62%\u548c262.26%\uff0c\u751f\u6210\u8d28\u91cf\u63d0\u5347\u4e8614.22%\u548c53.58%\u3002", "conclusion": "MQG-RFM\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u7684\u68c0\u7d22\u4f18\u5316\uff0c\u4e3a\u4e2d\u5c0f\u578b\u673a\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u4e13\u5229\u60c5\u62a5\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210,RAG,\u591a\u89d2\u5ea6\u95ee\u9898\u751f\u6210,\u68c0\u7d22\u5fae\u8c03,\u4e13\u5229\u60c5\u62a5"}}
{"id": "2506.01277", "pdf": "https://arxiv.org/pdf/2506.01277", "abs": "https://arxiv.org/abs/2506.01277", "authors": ["Qiang Yi", "Lianlei Shan"], "title": "GeoLocSFT: Efficient Visual Geolocation via Supervised Fine-Tuning of Multimodal Foundation Models", "categories": ["cs.AI"], "comment": "29 pages, 14 figures", "summary": "Accurately determining the geographic location where a single image was\ntaken, visual geolocation, remains a formidable challenge due to the planet's\nvastness and the deceptive similarity among distant locations. We introduce\nGeoLocSFT, a framework that demonstrates how targeted supervised fine-tuning\n(SFT) of a large multimodal foundation model (Gemma 3) using a small,\nhigh-quality dataset can yield highly competitive geolocation performance.\nGeoLocSFT is trained with only 2700 carefully selected image-GPS pairs from our\ngeographically diverse MR600k dataset. Despite this limited data, our\nSFT-centric approach substantially improves over baseline models and achieves\nrobust results on standard benchmarks such as Im2GPS-3k and YFCC-4k, as well as\non our newly proposed and challenging MR40k benchmark, aimed specifically at\nsparsely populated regions. Further, we explore multi-candidate inference and\naggregation strategies but find that the core gains are already realized at the\nSFT stage. Our findings highlight the power of high-quality supervision and\nefficient SFT for planet-scale image geolocation, especially when compared to\nprior methods that require massive databases or complex pipelines. To foster\nfurther research, we publicly release the MR40k benchmark dataset.", "AI": {"tldr": "GeoLocSFT\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u5bf9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5168\u7403\u8303\u56f4\u5185\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7a00\u758f\u4eba\u53e3\u533a\u57df\u3002", "method": "\u4f7f\u75282700\u4e2a\u7cbe\u9009\u56fe\u50cf-GPS\u5bf9\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u63a2\u7d22\u591a\u5019\u9009\u63a8\u7406\u4e0e\u805a\u5408\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u662f\u65b0\u63d0\u51fa\u7684MR40k\u57fa\u51c6\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u76d1\u7763\u548c\u9ad8\u6548\u5fae\u8c03\u80fd\u591f\u663e\u8457\u63d0\u5347\u5730\u7406\u5b9a\u4f4d\u6027\u80fd\uff0c\u4e0d\u9700\u8981\u590d\u6742\u6d41\u7a0b\u6216\u5927\u6570\u636e\u96c6\u3002", "keywords": "\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d,\u76d1\u7763\u5fae\u8c03,\u591a\u6a21\u6001\u6a21\u578b,\u57fa\u51c6\u6570\u636e\u96c6"}}
{"id": "2506.00545", "pdf": "https://arxiv.org/pdf/2506.00545", "abs": "https://arxiv.org/abs/2506.00545", "authors": ["Mehdi Bejani", "Guillermo Perez-de-Arenaza-Pozo", "Juli\u00e1n D. Arias-Londo\u00f1o", "Juan I. Godino-LLorente"], "title": "Imputation of Missing Data in Smooth Pursuit Eye Movements Using a Self-Attention-based Deep Learning Approach", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages, 10 figures, 3 tables", "summary": "Missing data is a relevant issue in time series, especially in biomedical\nsequences such as those corresponding to smooth pursuit eye movements, which\noften contain gaps due to eye blinks and track losses, complicating the\nanalysis and extraction of meaningful biomarkers. In this paper, a novel\nimputation framework is proposed using Self-Attention-based Imputation networks\nfor time series, which leverages the power of deep learning and self-attention\nmechanisms to impute missing data. We further refine the imputed data using a\ncustom made autoencoder, tailored to represent smooth pursuit eye movement\nsequences. The proposed approach was implemented using 5,504 sequences from 172\nParkinsonian patients and healthy controls. Results show a significant\nimprovement in the accuracy of reconstructed eye movement sequences with\nrespect to other state of the art techniques, substantially reducing the values\nfor common time domain error metrics such as the mean absolute error, mean\nrelative error, and root mean square error, while also preserving the signal's\nfrequency domain characteristics. Moreover, it demonstrates robustness when\nlarge intervals of data are missing. This method offers an alternative solution\nfor robustly handling missing data in time series, enhancing the reliability of\nsmooth pursuit analysis for the screening and monitoring of neurodegenerative\ndisorders.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7f3a\u5931\u586b\u8865\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u7269\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u586b\u8865\u7cbe\u5ea6\u3002", "motivation": "\u5904\u7406\u751f\u7269\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\uff08\u5982\u773c\u52a8\u6570\u636e\uff09\u4e2d\u7684\u7f3a\u5931\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7b5b\u67e5\u7684\u53ef\u9760\u6027\u3002", "method": "\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u5b9a\u5236\u5316\u81ea\u52a8\u7f16\u7801\u5668\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u586b\u8865\u65f6\u95f4\u5e8f\u5217\u7f3a\u5931\u6570\u636e\u3002", "result": "\u57285,504\u4e2a\u5e8f\u5217\u4e0a\u6d4b\u8bd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bef\u5dee\u6307\u6807\uff0c\u5e76\u4fdd\u6301\u4e86\u4fe1\u53f7\u9891\u57df\u7279\u5f81\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65f6\u95f4\u5e8f\u5217\u7f3a\u5931\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u76d1\u6d4b\u3002", "keywords": "\u7f3a\u5931\u6570\u636e,\u81ea\u6ce8\u610f\u529b,\u81ea\u52a8\u7f16\u7801\u5668,\u773c\u52a8\u5e8f\u5217,\u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2506.00536", "pdf": "https://arxiv.org/pdf/2506.00536", "abs": "https://arxiv.org/abs/2506.00536", "authors": ["Changyue Wang", "Weihang Su", "Qingyao Ai", "Yujia Zhou", "Yiqun Liu"], "title": "Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge editing aims to efficiently update Large Language Models (LLMs) by\nmodifying specific knowledge without retraining the entire model. Among\nknowledge editing approaches, in-context editing (ICE) offers a lightweight\nsolution by injecting new knowledge directly into the input context, leaving\nmodel parameters unchanged. However, existing ICE approaches do not explicitly\nseparate the newly injected knowledge from the model's original reasoning\nprocess. This entanglement often results in conflicts between external updates\nand internal parametric knowledge, undermining the consistency and accuracy of\nthe reasoning path.In this work, we conduct preliminary experiments to examine\nhow parametric knowledge influences reasoning path planning. We find that the\nmodel's reasoning is tightly coupled with its internal knowledge, and that\nnaively injecting new information without adapting the reasoning path often\nleads to performance degradation, particularly in multi-hop tasks. To this end,\nwe propose DecKER, a novel ICE framework that decouples reasoning from\nknowledge editing by generating a masked reasoning path and then resolving\nknowledge edits via hybrid retrieval and model-based validation. Experiments on\nmulti-hop QA benchmarks show that DecKER significantly outperforms existing ICE\nmethods by mitigating knowledge conflicts and preserving reasoning consistency.\nOur code is available at: https://github.com/bebr2/DecKER .", "AI": {"tldr": "DecKER\u662f\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u63a8\u7406\u548c\u77e5\u8bc6\u7f16\u8f91\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u672a\u660e\u786e\u533a\u5206\u65b0\u6ce8\u5165\u77e5\u8bc6\u4e0e\u539f\u59cb\u63a8\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u51b2\u7a81\u548c\u51c6\u786e\u6027\u4e0b\u964d\u3002", "method": "\u63d0\u51faDecKER\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u63a9\u7801\u63a8\u7406\u8def\u5f84\u5e76\u7ed3\u5408\u68c0\u7d22\u548c\u6a21\u578b\u9a8c\u8bc1\u89e3\u8026\u63a8\u7406\u4e0e\u77e5\u8bc6\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDecKER\u5728\u591a\u8df3QA\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DecKER\u901a\u8fc7\u89e3\u8026\u63a8\u7406\u4e0e\u77e5\u8bc6\u7f16\u8f91\uff0c\u63d0\u5347\u4e86\u77e5\u8bc6\u7f16\u8f91\u7684\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u3002", "keywords": "\u77e5\u8bc6\u7f16\u8f91, \u5927\u8bed\u8a00\u6a21\u578b, \u63a8\u7406\u8def\u5f84, \u591a\u8df3QA"}}
{"id": "2506.01281", "pdf": "https://arxiv.org/pdf/2506.01281", "abs": "https://arxiv.org/abs/2506.01281", "authors": ["John Leland", "YooJung Choi"], "title": "On the Hardness of Approximating Distributions with Probabilistic Circuits", "categories": ["cs.AI"], "comment": null, "summary": "A fundamental challenge in probabilistic modeling is balancing expressivity\nand tractable inference. Probabilistic circuits (PCs) aim to directly address\nthis tradeoff by imposing structural constraints that guarantee efficient\ninference of certain queries while maintaining expressivity. Since inference\ncomplexity on PCs depends on circuit size, understanding the size bounds across\ncircuit families is key to characterizing the tradeoff between tractability and\nexpressive efficiency. However, expressive efficiency is often studied through\nexact representations, where exactly encoding distributions while enforcing\nvarious structural properties often incurs exponential size blow-ups. Thus, we\npose the following question: can we avoid such size blow-ups by allowing some\nsmall approximation error? We first show that approximating an arbitrary\ndistribution with bounded $f$-divergence is $\\mathsf{NP}$-hard for any model\nthat can tractably compute marginals. We then prove an exponential size gap for\napproximation between the class of decomposable PCs and additionally\ndeterministic PCs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u6982\u7387\u7535\u8def\u4e2d\u8868\u8fbe\u6027\u4e0e\u53ef\u63a8\u65ad\u6027\u7684\u5e73\u8861\u95ee\u9898\uff0c\u7814\u7a76\u4e86\u8fd1\u4f3c\u8868\u793a\u5bf9\u907f\u514d\u6307\u6570\u7ea7\u89c4\u6a21\u81a8\u80c0\u7684\u4f5c\u7528\uff0c\u5e76\u8bc1\u660e\u4e86\u53ef\u5206\u89e3\u6982\u7387\u7535\u8def\u4e0e\u786e\u5b9a\u6027\u6982\u7387\u7535\u8def\u4e4b\u95f4\u7684\u8fd1\u4f3c\u89c4\u6a21\u5dee\u8ddd\u3002", "motivation": "\u6982\u7387\u5efa\u6a21\u4e2d\u7684\u4e00\u4e2a\u6839\u672c\u6311\u6218\u662f\u5e73\u8861\u8868\u8fbe\u6027\u548c\u53ef\u63a8\u65ad\u6027\u3002\u867d\u7136\u6982\u7387\u7535\u8def\uff08PCs\uff09\u901a\u8fc7\u7ed3\u6784\u7ea6\u675f\u5728\u4fdd\u8bc1\u9ad8\u6548\u63a8\u65ad\u7684\u540c\u65f6\u4fdd\u6301\u8868\u8fbe\u6027\uff0c\u4f46\u4e25\u683c\u7684\u7cbe\u786e\u8868\u793a\u5f80\u5f80\u5bfc\u81f4\u6307\u6570\u7ea7\u7684\u89c4\u6a21\u81a8\u80c0\uff0c\u56e0\u6b64\u7814\u7a76\u8fd1\u4f3c\u8868\u793a\u662f\u5426\u80fd\u907f\u514d\u8fd9\u4e00\u95ee\u9898\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u8bba\u6587\u9996\u5148\u8bc1\u660e\uff0c\u5bf9\u4e8e\u4efb\u4f55\u80fd\u9ad8\u6548\u8ba1\u7b97\u8fb9\u7f18\u6982\u7387\u7684\u6a21\u578b\uff0c\u4ee5\u6709\u754c$f$-\u6563\u5ea6\u8fd1\u4f3c\u4efb\u610f\u5206\u5e03\u662f$\text{NP}$\u56f0\u96be\u7684\u3002\u968f\u540e\uff0c\u5206\u6790\u4e86\u53ef\u5206\u89e3\u6982\u7387\u7535\u8def\u4e0e\u786e\u5b9a\u6027\u6982\u7387\u7535\u8def\u5728\u8fd1\u4f3c\u8868\u793a\u4e2d\u7684\u89c4\u6a21\u5dee\u8ddd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u53ef\u5206\u89e3\u6982\u7387\u7535\u8def\u4e0e\u786e\u5b9a\u6027\u6982\u7387\u7535\u8def\u5728\u8fd1\u4f3c\u8868\u793a\u4e2d\u5b58\u5728\u6307\u6570\u7ea7\u7684\u89c4\u6a21\u5dee\u8ddd\uff0c\u7a81\u51fa\u4e86\u8fd1\u4f3c\u65b9\u6cd5\u5728\u907f\u514d\u89c4\u6a21\u81a8\u80c0\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u5141\u8bb8\u4e00\u5b9a\u7684\u8fd1\u4f3c\u8bef\u5dee\uff0c\u53ef\u4ee5\u5728\u907f\u514d\u6307\u6570\u7ea7\u89c4\u6a21\u81a8\u80c0\u7684\u540c\u65f6\uff0c\u7ef4\u6301\u6982\u7387\u7535\u8def\u7684\u8868\u8fbe\u6027\u4e0e\u53ef\u63a8\u65ad\u6027\u5e73\u8861\u3002", "keywords": "\u6982\u7387\u7535\u8def, \u8868\u8fbe\u6027, \u53ef\u63a8\u65ad\u6027, \u8fd1\u4f3c\u8868\u793a, $f$-\u6563\u5ea6"}}
{"id": "2506.00555", "pdf": "https://arxiv.org/pdf/2506.00555", "abs": "https://arxiv.org/abs/2506.00555", "authors": ["Peng Xia", "Jinglu Wang", "Yibo Peng", "Kaide Zeng", "Xian Wu", "Xiangru Tang", "Hongtu Zhu", "Yun Li", "Shujie Liu", "Yan Lu", "Huaxiu Yao"], "title": "MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential\nin multimodal diagnostic tasks. However, existing single-agent models struggle\nto generalize across diverse medical specialties, limiting their performance.\nRecent efforts introduce multi-agent collaboration frameworks inspired by\nclinical workflows, where general practitioners (GPs) and specialists interact\nin a fixed sequence. Despite improvements, these static pipelines lack\nflexibility and adaptability in reasoning. To address this, we propose\nMMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that\nenables dynamic, optimized collaboration among medical agents. Specifically, we\ntrain two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to\nassign patients to appropriate specialties, while the attending physician\nintegrates the judgments from multi-specialists and its own knowledge to make\nfinal decisions. To address the inconsistency in specialist outputs, we\nintroduce a curriculum learning (CL)-guided RL strategy that progressively\nteaches the attending physician to balance between imitating specialists and\ncorrecting their mistakes. Experiments on five medical VQA benchmarks\ndemonstrate that MMedAgent-RL not only outperforms both open-source and\nproprietary Med-LVLMs, but also exhibits human-like reasoning patterns.\nNotably, it achieves an average performance gain of 18.4% over supervised\nfine-tuning baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u4ee3\u7406\u6846\u67b6MMedAgent-RL\uff0c\u901a\u8fc7\u52a8\u6001\u534f\u4f5c\u4f18\u5316\u533b\u7597\u4ee3\u7406\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u4e13\u79d1\u8bca\u7597\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5355\u4ee3\u7406\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e13\u79d1\u4efb\u52a1\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9759\u6001\u591a\u4ee3\u7406\u6846\u67b6\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e24\u79cdGP\u4ee3\u7406\uff1a\u5206\u8bca\u533b\u751f\uff08\u5206\u914d\u60a3\u8005\uff09\u548c\u4e3b\u6cbb\u533b\u751f\uff08\u6574\u5408\u591a\u4e13\u79d1\u610f\u89c1\uff09\u3002\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u4f18\u5316\u4e3b\u6cbb\u533b\u751f\u7684\u51b3\u7b56\u80fd\u529b\u3002", "result": "\u5728\u4e94\u4e2a\u533b\u7597VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMMedAgent-RL\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u6027\u80fd\u5e73\u5747\u63d0\u534718.4%\uff0c\u4e14\u5177\u5907\u7c7b\u4eba\u63a8\u7406\u6a21\u5f0f\u3002", "conclusion": "\u52a8\u6001\u591a\u4ee3\u7406\u534f\u4f5c\u6846\u67b6\u663e\u8457\u63d0\u5347\u8de8\u4e13\u79d1\u8bca\u7597\u80fd\u529b\uff0c\u4e3a\u533b\u7597AI\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "keywords": "\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u6a21\u578b,\u591a\u4ee3\u7406\u534f\u4f5c,\u5f3a\u5316\u5b66\u4e60,\u8bfe\u7a0b\u5b66\u4e60,\u52a8\u6001\u63a8\u7406"}}
{"id": "2506.00539", "pdf": "https://arxiv.org/pdf/2506.00539", "abs": "https://arxiv.org/abs/2506.00539", "authors": ["Ruihan Yang", "Yikai Zhang", "Aili Chen", "Xintao Wang", "Siyu Yuan", "Jiangjie Chen", "Deqing Yang", "Yanghua Xiao"], "title": "ARIA: Training Language Agents with Intention-Driven Reward Aggregation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have enabled agents to perform complex reasoning\nand decision-making through free-form language interactions. However, in\nopen-ended language action environments (e.g., negotiation or question-asking\ngames), the action space can be formulated as a joint distribution over tokens,\nresulting in an exponentially large action space. Sampling actions in such a\nspace can lead to extreme reward sparsity, which brings large reward variance,\nhindering effective reinforcement learning (RL). To address this, we propose\nARIA, a method that Aggregates Rewards in Intention space to enable efficient\nand effective language Agents training. ARIA aims to project natural language\nactions from the high-dimensional joint token distribution space into a\nlow-dimensional intention space, where semantically similar actions are\nclustered and assigned shared rewards. This intention-aware reward aggregation\nreduces reward variance by densifying reward signals, fostering better policy\noptimization. Extensive experiments demonstrate that ARIA not only\nsignificantly reduces policy gradient variance, but also delivers substantial\nperformance gains of an average of 9.95% across four downstream tasks,\nconsistently outperforming offline and online RL baselines.", "AI": {"tldr": "ARIA\u65b9\u6cd5\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u52a8\u4f5c\u4ece\u9ad8\u7ef4\u7a7a\u95f4\u6295\u5f71\u5230\u4f4e\u7ef4\u610f\u56fe\u7a7a\u95f4\uff0c\u51cf\u5c11\u5956\u52b1\u65b9\u5dee\uff0c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u8bed\u8a00\u52a8\u4f5c\u73af\u5883\u4e2d\u56e0\u5de8\u5927\u52a8\u4f5c\u7a7a\u95f4\u5bfc\u81f4\u7684\u5956\u52b1\u7a00\u758f\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faARIA\u65b9\u6cd5\uff0c\u5c06\u52a8\u4f5c\u6295\u5f71\u5230\u4f4e\u7ef4\u610f\u56fe\u7a7a\u95f4\uff0c\u805a\u7c7b\u8bed\u4e49\u76f8\u4f3c\u52a8\u4f5c\u5e76\u5171\u4eab\u5956\u52b1\u3002", "result": "\u663e\u8457\u51cf\u5c11\u7b56\u7565\u68af\u5ea6\u65b9\u5dee\uff0c\u4e0b\u6e38\u4efb\u52a1\u5e73\u5747\u6027\u80fd\u63d0\u53479.95%\u3002", "conclusion": "ARIA\u6709\u6548\u63d0\u5347\u8bed\u8a00\u4ee3\u7406\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u5f3a\u5316\u5b66\u4e60, \u5956\u52b1\u7a00\u758f\u6027, ARIA\u65b9\u6cd5"}}
{"id": "2506.01297", "pdf": "https://arxiv.org/pdf/2506.01297", "abs": "https://arxiv.org/abs/2506.01297", "authors": ["Ya Wen", "Jixuan Cai", "Qiyao Ma", "Linyan Li", "Xinhua Chen", "Chris Webster", "Yulun Zhou"], "title": "MobCLIP: Learning General-purpose Geospatial Representation at Scale", "categories": ["cs.AI"], "comment": null, "summary": "Representation learning of geospatial locations remains a core challenge in\nachieving general geospatial intelligence. Current embedding methods often lack\nversatility, limiting their utility across diverse tasks in both human and\nnatural domains. We present MobCLIP, the first nationwide general-purpose\nlocation encoder, integrating an unprecedented diversity of data modalities\nthrough effective and scalable multimodal fusion. Adopting a novel CLIP-based\narchitecture, our framework aligns 100M+ POIs, nationwide remote sensing\nimagery, and structured demographic statistics with a billion-edge mobility\ngraph. By tokenizing spatial locations into grid cells inspired by Vision\nTransformers, we establish a unified representation space bridging mobility\npatterns and multimodal features. To rigorously evaluate the general-purpose\neffectiveness of MobCLIP, we construct a benchmark dataset composed of 11\ndownstream prediction tasks across social, economic, and natural domains.\nExperiments show that MobCLIP, with four input modalities and a compact\n128-dimensional representation space, achieves significantly superior\ngeneral-purpose predictive performances than state-of-the-art models by an\naverage of 35%. Thanks to the effective integration of human-centric\nmodalities, the performance gain is particularly profound in human-centric\ntasks, such as energy consumption (+260%), offline retail consumption amount\n(+98%), and crime cases (+95%) predictions. Echoing LLM scaling laws, we\nfurther demonstrate the scaling behavior in geospatial representation learning.\nWe open-source code and pretrained models at: github.com.", "AI": {"tldr": "MobCLIP\u662f\u4e00\u4e2a\u5168\u56fd\u901a\u7528\u7684\u5730\u7406\u7a7a\u95f4\u4f4d\u7f6e\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u6574\u5408POI\u3001\u9065\u611f\u5f71\u50cf\u548c\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5730\u7406\u7a7a\u95f4\u4f4d\u7f6e\u8868\u793a\u5b66\u4e60\u7684\u901a\u7528\u6027\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u57fa\u4e8eCLIP\u7684\u67b6\u6784\uff0c\u878d\u5408POI\u3001\u9065\u611f\u56fe\u50cf\u548c\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\uff0c\u901a\u8fc7\u7f51\u683c\u5316\u7a7a\u95f4\u8868\u793a\u7edf\u4e00\u591a\u6a21\u6001\u7279\u5f81\u3002", "result": "\u572811\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0cMobCLIP\u7684\u5e73\u5747\u6027\u80fd\u63d0\u534735%\uff0c\u5728\u4eba\u7c7b\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "MobCLIP\u5c55\u793a\u4e86\u5730\u7406\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u7684\u6269\u5c55\u6f5c\u529b\uff0c\u5f00\u6e90\u4ee3\u7801\u548c\u6a21\u578b\u63a8\u52a8\u4e86\u7814\u7a76\u8fdb\u5c55\u3002", "keywords": "\u5730\u7406\u7a7a\u95f4\u667a\u80fd\u3001\u591a\u6a21\u6001\u878d\u5408\u3001CLIP\u67b6\u6784\u3001\u901a\u7528\u4f4d\u7f6e\u7f16\u7801\u5668"}}
{"id": "2506.00563", "pdf": "https://arxiv.org/pdf/2506.00563", "abs": "https://arxiv.org/abs/2506.00563", "authors": ["Ziyan Luo", "Tianwei Ni", "Pierre-Luc Bacon", "Doina Precup", "Xujie Si"], "title": "Understanding Behavioral Metric Learning: A Large-Scale Study on Distracting Reinforcement Learning Environments", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A key approach to state abstraction is approximating behavioral metrics\n(notably, bisimulation metrics) in the observation space and embedding these\nlearned distances in the representation space. While promising for robustness\nto task-irrelevant noise, as shown in prior work, accurately estimating these\nmetrics remains challenging, requiring various design choices that create gaps\nbetween theory and practice. Prior evaluations focus mainly on final returns,\nleaving the quality of learned metrics and the source of performance gains\nunclear. To systematically assess how metric learning works in deep\nreinforcement learning (RL), we evaluate five recent approaches, unified\nconceptually as isometric embeddings with varying design choices. We benchmark\nthem with baselines across 20 state-based and 14 pixel-based tasks, spanning\n370 task configurations with diverse noise settings. Beyond final returns, we\nintroduce the evaluation of a denoising factor to quantify the encoder's\nability to filter distractions. To further isolate the effect of metric\nlearning, we propose and evaluate an isolated metric estimation setting, in\nwhich the encoder is influenced solely by the metric loss. Finally, we release\nan open-source, modular codebase to improve reproducibility and support future\nresearch on metric learning in deep RL.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u884c\u4e3a\u5ea6\u91cf\uff08\u5982\u53cc\u6a21\u62df\u5ea6\u91cf\uff09\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cd\u65b9\u6cd5\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u548c\u5f00\u6e90\u4ee3\u7801\u5e93\u3002", "motivation": "\u73b0\u6709\u7684\u884c\u4e3a\u5ea6\u91cf\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e14\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u5728\u6700\u7ec8\u56de\u62a5\u4e0a\uff0c\u7f3a\u4e4f\u5bf9\u5ea6\u91cf\u8d28\u91cf\u548c\u6027\u80fd\u589e\u76ca\u6765\u6e90\u7684\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u7814\u7a76\u4e86\u4e94\u79cd\u7edf\u4e00\u4e3a\u7b49\u8ddd\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u572820\u79cd\u72b6\u6001\u4efb\u52a1\u548c14\u79cd\u50cf\u7d20\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u4e86\u53bb\u566a\u56e0\u5b50\u548c\u5b64\u7acb\u5ea6\u91cf\u4f30\u8ba1\u8bbe\u7f6e\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u65b9\u6cd5\u5728\u591a\u6837\u5316\u566a\u58f0\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u91cf\u5316\u4e86\u7f16\u7801\u5668\u8fc7\u6ee4\u5e72\u6270\u7684\u80fd\u529b\u3002", "conclusion": "\u8bba\u6587\u4e3a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5ea6\u91cf\u5b66\u4e60\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u548c\u65b0\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86\u672a\u6765\u7814\u7a76\u7684\u53ef\u590d\u73b0\u6027\u3002", "keywords": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u884c\u4e3a\u5ea6\u91cf\uff0c\u7b49\u8ddd\u5d4c\u5165\uff0c\u72b6\u6001\u62bd\u8c61\uff0c\u566a\u58f0\u9c81\u68d2\u6027"}}
{"id": "2506.00549", "pdf": "https://arxiv.org/pdf/2506.00549", "abs": "https://arxiv.org/abs/2506.00549", "authors": ["Hyangsuk Min", "Yuho Lee", "Minjeong Ban", "Jiaqi Deng", "Nicole Hee-Yeon Kim", "Taewon Yun", "Hang Su", "Jason Cai", "Hwanjun Song"], "title": "Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages", "categories": ["cs.CL", "cs.AI"], "comment": "34 pages, 6 figures", "summary": "Evaluation frameworks for text summarization have evolved in terms of both\ndomain coverage and metrics. However, existing benchmarks still lack\ndomain-specific assessment criteria, remain predominantly English-centric, and\nface challenges with human annotation due to the complexity of reasoning. To\naddress these, we introduce MSumBench, which provides a multi-dimensional,\nmulti-domain evaluation of summarization in English and Chinese. It also\nincorporates specialized assessment criteria for each domain and leverages a\nmulti-agent debate system to enhance annotation quality. By evaluating eight\nmodern summarization models, we discover distinct performance patterns across\ndomains and languages. We further examine large language models as summary\nevaluators, analyzing the correlation between their evaluation and\nsummarization capabilities, and uncovering systematic bias in their assessment\nof self-generated summaries. Our benchmark dataset is publicly available at\nhttps://github.com/DISL-Lab/MSumBench.", "AI": {"tldr": "MSumBench\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u7ef4\u3001\u591a\u9886\u57df\u7684\u6587\u672c\u6458\u8981\u8bc4\u4f30\u6846\u67b6\uff0c\u6db5\u76d6\u4e2d\u82f1\u6587\uff0c\u5e76\u5f15\u5165\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7cfb\u7edf\u4ee5\u63d0\u9ad8\u6807\u6ce8\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u6807\u51c6\uff0c\u4ee5\u82f1\u8bed\u4e3a\u4e3b\uff0c\u4e14\u4eba\u5de5\u6807\u6ce8\u590d\u6742\u6027\u9ad8\uff0c\u9700\u6539\u8fdb\u3002", "method": "MSumBench\u6574\u5408\u591a\u9886\u57df\u3001\u591a\u8bed\u8a00\u8bc4\u4f30\uff0c\u5f15\u5165\u4e13\u7528\u6807\u51c6\u548c\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7cfb\u7edf\u63d0\u5347\u6807\u6ce8\u8d28\u91cf\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u4e0d\u540c\u6a21\u578b\u5728\u9886\u57df\u548c\u8bed\u8a00\u4e0a\u8868\u73b0\u5404\u5f02\uff0c\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\u3002", "conclusion": "MSumBench\u4e3a\u6587\u672c\u6458\u8981\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u4fc3\u8fdb\u9886\u57df\u548c\u8bed\u8a00\u7684\u591a\u6837\u6027\u7814\u7a76\u3002", "keywords": "\u6587\u672c\u6458\u8981, \u8bc4\u4f30\u6846\u67b6, \u591a\u9886\u57df, \u4e2d\u82f1\u6587, \u591a\u667a\u80fd\u4f53\u8fa9\u8bba"}}
{"id": "2506.01299", "pdf": "https://arxiv.org/pdf/2506.01299", "abs": "https://arxiv.org/abs/2506.01299", "authors": ["Jinmei Liu", "Fuhong Liu", "Jianye Hao", "Bo Wang", "Huaxiong Li", "Chunlin Chen", "Zhi Wang"], "title": "Scalable In-Context Q-Learning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Recent advancements in language models have demonstrated remarkable\nin-context learning abilities, prompting the exploration of in-context\nreinforcement learning (ICRL) to extend the promise to decision domains. Due to\ninvolving more complex dynamics and temporal correlations, existing ICRL\napproaches may face challenges in learning from suboptimal trajectories and\nachieving precise in-context inference. In the paper, we propose\n\\textbf{S}calable \\textbf{I}n-\\textbf{C}ontext \\textbf{Q}-\\textbf{L}earning\n(\\textbf{SICQL}), an innovative framework that harnesses dynamic programming\nand world modeling to steer ICRL toward efficient reward maximization and task\ngeneralization, while retaining the scalability and stability of supervised\npretraining. We design a prompt-based multi-head transformer architecture that\nsimultaneously predicts optimal policies and in-context value functions using\nseparate heads. We pretrain a generalized world model to capture task-relevant\ninformation, enabling the construction of a compact prompt that facilitates\nfast and precise in-context inference. During training, we perform iterative\npolicy improvement by fitting a state value function to an upper-expectile of\nthe Q-function, and distill the in-context value functions into policy\nextraction using advantage-weighted regression. Extensive experiments across a\nrange of discrete and continuous environments show consistent performance gains\nover various types of baselines, especially when learning from suboptimal data.\nOur code is available at https://github.com/NJU-RL/SICQL", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SICQL\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u548c\u4e16\u754c\u5efa\u6a21\u63d0\u5347\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u548c\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5c06\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u6269\u5c55\u5230\u51b3\u7b56\u9886\u57df\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6b21\u4f18\u8f68\u8ff9\u5b66\u4e60\u548c\u7cbe\u786e\u63a8\u7406\u4e2d\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u63d0\u793a\u7684\u591a\u5934Transformer\u67b6\u6784\uff0c\u9884\u8bad\u7ec3\u901a\u7528\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u7b56\u7565\u6539\u8fdb\u548c\u4ef7\u503c\u51fd\u6570\u63d0\u53d6\u4f18\u5316\u3002", "result": "\u5728\u79bb\u6563\u548c\u8fde\u7eed\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u57fa\u7ebf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u9002\u5408\u4ece\u6b21\u4f18\u6570\u636e\u4e2d\u5b66\u4e60\u3002", "conclusion": "SICQL\u6846\u67b6\u5728\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5956\u52b1\u6700\u5927\u5316\u548c\u4efb\u52a1\u6cdb\u5316\u3002", "keywords": "\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60, \u52a8\u6001\u89c4\u5212, \u4e16\u754c\u5efa\u6a21, Transformer, \u7b56\u7565\u6539\u8fdb"}}
{"id": "2506.00569", "pdf": "https://arxiv.org/pdf/2506.00569", "abs": "https://arxiv.org/abs/2506.00569", "authors": ["Nicholas E. Corrado", "Julian Katz-Samuels", "Adithya Devraj", "Hyokun Yun", "Chao Zhang", "Yi Xu", "Yi Pan", "Bing Yin", "Trishul Chilimbi"], "title": "AutoMixAlign: Adaptive Data Mixing for Multi-Task Preference Optimization in LLMs", "categories": ["cs.LG"], "comment": "ACL 2025, Main Conference", "summary": "When aligning large language models (LLMs), their performance on various\ntasks (such as being helpful, harmless, and honest) depends heavily on the\ncomposition of their training data. However, selecting a data mixture that\nachieves strong performance across all tasks is challenging. Existing\napproaches rely on large ablation studies, heuristics, or human intuition, but\nthese can be prohibitively expensive and suboptimal. We study this problem in\nthe setting of preference optimization via DPO and introduce AutoMixAlign\n(AMA), a theoretically-grounded algorithm that adaptively mixes datasets during\ntraining to balance performance across tasks. AMA first trains\n\\textit{specialist models} for each task to determine losses that correspond to\nstrong task performance. Then, it trains a generalist model using a novel\nminimax optimization that prioritizes tasks for which generalist model losses\ndeviate most from specialist model losses. To optimize this problem, we propose\ntwo algorithms: (1) AMA-R, which adaptively reweights the objective to\nprioritize tasks, and (2) AMA-S, which adaptively adjusts how much data is\nsampled from each task to prioritize tasks. Both algorithms achieve a\nconvergence rate of $O(1/\\sqrt{T})$ in the convex case. AMA-R's convergence\nresult follows from Sagawa et al. (2019), and we provide a convergence proof\nfor AMA-S using online learning techniques such as EXP3. We evaluate AMA on\nseveral multitask alignment setups and find that AMA outperforms the standard\nalignment approach -- which simply optimizes the total loss across all tasks --\nand also outperforms model merging methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAutoMixAlign\uff08AMA\uff09\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6df7\u5408\u6570\u636e\u96c6\u548c\u81ea\u9002\u5e94\u4f18\u5316\u6765\u89e3\u51b3\u591a\u4efb\u52a1\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5bf9\u9f50\u4efb\u52a1\u4e2d\u8868\u73b0\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u7ec4\u6210\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u6d88\u878d\u7814\u7a76\u6216\u4eba\u5de5\u76f4\u89c9\uff0c\u96be\u4ee5\u5168\u5c40\u4f18\u5316\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5e73\u8861\u591a\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u63d0\u51faAMA\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e13\u5bb6\u6a21\u578b\u635f\u5931\u548c\u6781\u5c0f\u6781\u5927\u4f18\u5316\uff0c\u52a8\u6001\u8c03\u6574\u4efb\u52a1\u6743\u91cd\u6216\u6570\u636e\u91c7\u6837\u6bd4\u4f8b\uff0c\u5177\u4f53\u5305\u62ecAMA-R\uff08\u81ea\u9002\u5e94\u52a0\u6743\uff09\u548cAMA-S\uff08\u81ea\u9002\u5e94\u91c7\u6837\uff09\u4e24\u79cd\u53d8\u4f53\u3002", "result": "AMA\u5728\u591a\u9879\u591a\u4efb\u52a1\u5bf9\u9f50\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u5bf9\u9f50\u65b9\u6cd5\u548c\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\uff0c\u4e14\u4e24\u79cd\u53d8\u4f53\u5747\u5177\u6709\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "AMA\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u6df7\u5408\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u5bf9\u9f50\u7684\u6548\u679c\uff0c\u4e3aLLM\u8bad\u7ec3\u6570\u636e\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u591a\u4efb\u52a1\u5bf9\u9f50\u3001\u81ea\u9002\u5e94\u4f18\u5316\u3001AutoMixAlign\u3001\u4e13\u5bb6\u6a21\u578b"}}
{"id": "2506.00551", "pdf": "https://arxiv.org/pdf/2506.00551", "abs": "https://arxiv.org/abs/2506.00551", "authors": ["Ming Wang", "Peidong Wang", "Lin Wu", "Xiaocui Yang", "Daling Wang", "Shi Feng", "Yuxin Chen", "Bixuan Wang", "Yifei Zhang"], "title": "AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Constrained by the cost and ethical concerns of involving real seekers in\nAI-driven mental health, researchers develop LLM-based conversational agents\n(CAs) with tailored configurations, such as profiles, symptoms, and scenarios,\nto simulate seekers. While these efforts advance AI in mental health, achieving\nmore realistic seeker simulation remains hindered by two key challenges:\ndynamic evolution and multi-session memory. Seekers' mental states often\nfluctuate during counseling, which typically spans multiple sessions. To\naddress this, we propose AnnaAgent, an emotional and cognitive dynamic agent\nsystem equipped with tertiary memory. AnnaAgent incorporates an emotion\nmodulator and a complaint elicitor trained on real counseling dialogues,\nenabling dynamic control of the simulator's configurations. Additionally, its\ntertiary memory mechanism effectively integrates short-term and long-term\nmemory across sessions. Evaluation results, both automated and manual,\ndemonstrate that AnnaAgent achieves more realistic seeker simulation in\npsychological counseling compared to existing baselines. The ethically reviewed\nand screened code can be found on https://github.com/sci-m-wang/AnnaAgent.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAnnaAgent\u7684\u60c5\u611f\u4e0e\u8ba4\u77e5\u52a8\u6001\u4ee3\u7406\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3AI\u5fc3\u7406\u5065\u5eb7\u7684\u5bfb\u6c42\u8005\u6a21\u62df\u4e2d\u7684\u52a8\u6001\u6f14\u5316\u548c\u591a\u4f1a\u8bdd\u8bb0\u5fc6\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u771f\u5b9e\u53c2\u4e0e\u8005\u7684\u6210\u672c\u548c\u4f26\u7406\u95ee\u9898\uff0cAI\u5fc3\u7406\u5065\u5eb7\u7814\u7a76\u9700\u8981\u66f4\u771f\u5b9e\u7684\u5bfb\u6c42\u8005\u6a21\u62df\u65b9\u6cd5\u3002", "method": "AnnaAgent\u7ed3\u5408\u4e86\u60c5\u7eea\u8c03\u8282\u5668\u548c\u57fa\u4e8e\u771f\u5b9e\u54a8\u8be2\u5bf9\u8bdd\u8bad\u7ec3\u7684\u62b1\u6028\u5f15\u53d1\u5668\uff0c\u5e76\u91c7\u7528\u4e09\u7ea7\u8bb0\u5fc6\u673a\u5236\u6574\u5408\u77ed\u671f\u548c\u957f\u671f\u8bb0\u5fc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAnnaAgent\u5728\u5fc3\u7406\u8f85\u5bfc\u4e2d\u6bd4\u73b0\u6709\u57fa\u7ebf\u66f4\u771f\u5b9e\u5730\u6a21\u62df\u5bfb\u6c42\u8005\u3002", "conclusion": "AnnaAgent\u4e3aAI\u5fc3\u7406\u5065\u5eb7\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u6a21\u62df\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u6f14\u5316\u548c\u591a\u4f1a\u8bdd\u8bb0\u5fc6\u7684\u6311\u6218\u3002", "keywords": "AI\u5fc3\u7406\u5065\u5eb7, \u5bfb\u6c42\u8005\u6a21\u62df, \u60c5\u611f\u52a8\u6001, \u8bb0\u5fc6\u673a\u5236"}}
{"id": "2506.01301", "pdf": "https://arxiv.org/pdf/2506.01301", "abs": "https://arxiv.org/abs/2506.01301", "authors": ["Chunhui Zhang", "Zhongyu Ouyang", "Kwonjoon Lee", "Nakul Agarwal", "Sean Dae Houlihan", "Soroush Vosoughi", "Shao-Yuan Lo"], "title": "Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted as a Spotlight at the 2025 Forty-Second International\n  Conference on Machine Learning (ICML 2025)", "summary": "Theory-of-Mind (ToM) enables humans to infer mental states-such as beliefs,\ndesires, and intentions-forming the foundation of social cognition. However,\nexisting computational ToM methods rely on structured workflows with\nToM-specific priors or deep model fine-tuning, which struggle with scalability\nin multimodal environments and fail to generalize as task complexity increases.\nTo address these limitations, we propose a scalable Bayesian ToM planner that\ndecomposes ToM reasoning into stepwise Bayesian updates. Our framework\nintroduces weak-to-strong control, allowing smaller language models (LMs) to\nspecialize in ToM-specific likelihood estimation and transfer their reasoning\nbehaviors to larger LMs (7B to 405B) for integration with social and world\nknowledge. This synergistic approach aligns large-model inference of human\nmental states with Bayesian principles. Extensive experiments show that our\nmethod achieves a 4.6% accuracy improvement over state-of-the-art techniques on\nmultimodal ToM benchmarks, including challenging unseen scenarios, thereby\nestablishing a new standard for modeling human mental states in complex\nenvironments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u8d1d\u53f6\u65af\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5206\u89e3ToM\u63a8\u7406\u4e3a\u9010\u6b65\u8d1d\u53f6\u65af\u66f4\u65b0\uff0c\u7ed3\u5408\u5c0f\u6a21\u578b\u548c\u5927\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u63d0\u9ad8\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709ToM\u8ba1\u7b97\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u6216\u6df1\u5ea6\u6a21\u578b\u5fae\u8c03\uff0c\u96be\u4ee5\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u6269\u5c55\uff0c\u4e14\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u5230\u5f3a\u7684\u63a7\u5236\u6846\u67b6\uff0c\u8ba9\u5c0f\u8bed\u8a00\u6a21\u578b\u4e13\u95e8\u8d1f\u8d23ToM\u4f3c\u7136\u4f30\u8ba1\uff0c\u5e76\u5c06\u5176\u63a8\u7406\u884c\u4e3a\u8fc1\u79fb\u81f3\u5927\u6a21\u578b\uff087B\u5230405B\uff09\u4e2d\uff0c\u7ed3\u5408\u793e\u4f1a\u548c\u4e16\u754c\u77e5\u8bc6\u3002", "result": "\u5728\u591a\u6a21\u6001ToM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u9ad8\u4e864.6%\u7684\u51c6\u786e\u7387\uff0c\u5c24\u5176\u5728\u672a\u77e5\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u5efa\u6a21\u4eba\u7c7b\u5fc3\u7406\u72b6\u6001\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u6027\u3002", "keywords": "\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\uff0c\u8d1d\u53f6\u65af\u63a8\u7406\uff0c\u591a\u6a21\u6001\uff0c\u8bed\u8a00\u6a21\u578b\uff0c\u53ef\u6269\u5c55\u6027"}}
{"id": "2506.00573", "pdf": "https://arxiv.org/pdf/2506.00573", "abs": "https://arxiv.org/abs/2506.00573", "authors": ["Dor Tsur", "Ziv Goldfeld", "Kristjan Greenewald", "Haim Permuter"], "title": "Neural Estimation for Scaling Entropic Multimarginal Optimal Transport", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Multimarginal optimal transport (MOT) is a powerful framework for modeling\ninteractions between multiple distributions, yet its applicability is\nbottlenecked by a high computational overhead. Entropic regularization provides\ncomputational speedups via the multimarginal Sinkhorn algorithm, whose time\ncomplexity, for a dataset size $n$ and $k$ marginals, generally scales as\n$O(n^k)$. However, this dependence on the dataset size $n$ is computationally\nprohibitive for many machine learning problems. In this work, we propose a new\ncomputational framework for entropic MOT, dubbed Neural Entropic MOT (NEMOT),\nthat enjoys significantly improved scalability. NEMOT employs neural networks\ntrained using mini-batches, which transfers the computational complexity from\nthe dataset size to the size of the mini-batch, leading to substantial gains.\nWe provide formal guarantees on the accuracy of NEMOT via non-asymptotic error\nbounds. We supplement these with numerical results that demonstrate the\nperformance gains of NEMOT over Sinkhorn's algorithm, as well as extensions to\nneural computation of multimarginal entropic Gromov-Wasserstein alignment. In\nparticular, orders-of-magnitude speedups are observed relative to the\nstate-of-the-art, with a notable increase in the feasible number of samples and\nmarginals. NEMOT seamlessly integrates as a module in large-scale machine\nlearning pipelines, and can serve to expand the practical applicability of\nentropic MOT for tasks involving multimarginal data.", "AI": {"tldr": "NEMOT\u662f\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u548c\u8ff7\u4f60\u6279\u6b21\u8bad\u7ec3\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u8fb9\u9645\u6700\u4f18\u8fd0\u8f93\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u591a\u8fb9\u9645\u6700\u4f18\u8fd0\u8f93\uff08MOT\uff09\u56e0\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u800c\u53d7\u9650\uff0c\u5c24\u5176\u662f\u5bf9\u5927\u6570\u636e\u96c6\u7684\u4f9d\u8d56\uff08$O(n^k)$\uff09\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\u3002", "method": "\u63d0\u51faNEMOT\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u548c\u8ff7\u4f60\u6279\u6b21\u8bad\u7ec3\uff0c\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u6570\u636e\u96c6\u89c4\u6a21\u8f6c\u79fb\u5230\u8ff7\u4f60\u6279\u6b21\u5927\u5c0f\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bef\u5dee\u4fdd\u8bc1\u3002", "result": "NEMOT\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6bd4Sinkhorn\u7b97\u6cd5\u5feb\u591a\u4e2a\u6570\u91cf\u7ea7\uff0c\u4e14\u652f\u6301\u66f4\u591a\u6837\u672c\u548c\u8fb9\u9645\u3002", "conclusion": "NEMOT\u53ef\u6269\u5c55\u591a\u8fb9\u9645\u6700\u4f18\u8fd0\u8f93\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u3002", "keywords": "\u591a\u8fb9\u9645\u6700\u4f18\u8fd0\u8f93, \u71b5\u6b63\u5219\u5316, \u795e\u7ecf\u7f51\u7edc, \u8ba1\u7b97\u6548\u7387"}}
{"id": "2506.00583", "pdf": "https://arxiv.org/pdf/2506.00583", "abs": "https://arxiv.org/abs/2506.00583", "authors": ["Yuhang Zhou", "Yimin Xiao", "Wei Ai", "Ge Gao"], "title": "The Hidden Language of Harm: Examining the Role of Emojis in Harmful Online Communication and Content Moderation", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "18 pages, 3 figures", "summary": "Social media platforms have become central to modern communication, yet they\nalso harbor offensive content that challenges platform safety and inclusivity.\nWhile prior research has primarily focused on textual indicators of offense,\nthe role of emojis, ubiquitous visual elements in online discourse, remains\nunderexplored. Emojis, despite being rarely offensive in isolation, can acquire\nharmful meanings through symbolic associations, sarcasm, and contextual misuse.\nIn this work, we systematically examine emoji contributions to offensive\nTwitter messages, analyzing their distribution across offense categories and\nhow users exploit emoji ambiguity. To address this, we propose an LLM-powered,\nmulti-step moderation pipeline that selectively replaces harmful emojis while\npreserving the tweet's semantic intent. Human evaluations confirm our approach\neffectively reduces perceived offensiveness without sacrificing meaning. Our\nanalysis also reveals heterogeneous effects across offense types, offering\nnuanced insights for online communication and emoji moderation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8868\u60c5\u7b26\u53f7\u5728\u63a8\u7279\u4e0a\u5bf9\u653b\u51fb\u6027\u5185\u5bb9\u7684\u8d21\u732e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u591a\u6b65\u9aa4\u5ba1\u6838\u7ba1\u9053\u6765\u66ff\u6362\u6709\u5bb3\u8868\u60c5\u7b26\u53f7\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u610f\u56fe\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u5728\u73b0\u4ee3\u6c9f\u901a\u4e2d\u5360\u636e\u6838\u5fc3\u5730\u4f4d\uff0c\u4f46\u540c\u65f6\u4e5f\u5b58\u5728\u653b\u51fb\u6027\u5185\u5bb9\u7684\u95ee\u9898\u3002\u4ee5\u5f80\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u6307\u6807\uff0c\u8868\u60c5\u7b26\u53f7\u7684\u4f5c\u7528\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u5c3d\u7ba1\u5b83\u4eec\u5f88\u5c11\u5355\u72ec\u6784\u6210\u653b\u51fb\u6027\uff0c\u4f46\u53ef\u80fd\u901a\u8fc7\u8c61\u5f81\u6027\u5173\u8054\u3001\u8bbd\u523a\u6216\u4e0a\u4e0b\u6587\u6ee5\u7528\u83b7\u5f97\u6709\u5bb3\u542b\u4e49\u3002", "method": "\u4f5c\u8005\u7cfb\u7edf\u5730\u5206\u6790\u4e86\u8868\u60c5\u7b26\u53f7\u5728\u63a8\u7279\u653b\u51fb\u6027\u6d88\u606f\u4e2d\u7684\u5206\u5e03\u53ca\u5176\u88ab\u5229\u7528\u7684\u65b9\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u591a\u6b65\u9aa4\u5ba1\u6838\u7ba1\u9053\uff0c\u9009\u62e9\u6027\u66ff\u6362\u6709\u5bb3\u8868\u60c5\u7b26\u53f7\uff0c\u540c\u65f6\u4fdd\u7559\u63a8\u6587\u7684\u8bed\u4e49\u610f\u56fe\u3002", "result": "\u4eba\u7c7b\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u653b\u51fb\u6027\u611f\u77e5\uff0c\u540c\u65f6\u672a\u727a\u7272\u8bed\u4e49\u3002\u5206\u6790\u8fd8\u63ed\u793a\u4e86\u4e0d\u540c\u653b\u51fb\u7c7b\u578b\u4e0b\u7684\u5f02\u8d28\u6027\u6548\u5e94\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u7ebf\u4ea4\u6d41\u548c\u8868\u60c5\u7b26\u53f7\u5ba1\u6838\u63d0\u4f9b\u4e86\u7ec6\u81f4\u89c1\u89e3\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u51cf\u5c11\u653b\u51fb\u6027\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u5185\u5bb9\u7684\u610f\u4e49\u3002", "keywords": "\u793e\u4ea4\u5a92\u4f53, \u8868\u60c5\u7b26\u53f7, \u653b\u51fb\u6027\u5185\u5bb9, LLM, \u5ba1\u6838\u7ba1\u9053"}}
{"id": "2506.01326", "pdf": "https://arxiv.org/pdf/2506.01326", "abs": "https://arxiv.org/abs/2506.01326", "authors": ["Zhiyuan Wang", "Bokui Chen", "Yinya Huang", "Qingxing Cao", "Ming He", "Jianping Fan", "Xiaodan Liang"], "title": "ORMind: A Cognitive-Inspired End-to-End Reasoning Framework for Operations Research", "categories": ["cs.AI"], "comment": "Accepted by Annual Meetings of the Association for Computational\n  Linguistics 2025", "summary": "Operations research (OR) is widely deployed to solve critical decision-making\nproblems with complex objectives and constraints, impacting manufacturing,\nlogistics, finance, and healthcare outcomes. While Large Language Models (LLMs)\nhave shown promising results in various domains, their practical application in\nindustry-relevant operations research (OR) problems presents significant\nchallenges and opportunities. Preliminary industrial applications of LLMs for\noperations research face two critical deployment challenges: 1) Self-correction\nfocuses on code syntax rather than mathematical accuracy, causing costly\nerrors; 2) Complex expert selection creates unpredictable workflows that reduce\ntransparency and increase maintenance costs, making them impractical for\ntime-sensitive business applications. To address these business limitations, we\nintroduce ORMind, a cognitive-inspired framework that enhances optimization\nthrough counterfactual reasoning. Our approach emulates human cognition,\nimplementing an end-to-end workflow that systematically transforms requirements\ninto mathematical models and executable solver code. It is currently being\ntested internally in Lenovo's AI Assistant, with plans to enhance optimization\ncapabilities for both business and consumer customers. Experiments demonstrate\nthat ORMind outperforms existing methods, achieving a 9.5\\% improvement on the\nNL4Opt dataset and a 14.6\\% improvement on the ComplexOR dataset.", "AI": {"tldr": "ORMind\u6846\u67b6\u901a\u8fc7\u53cd\u4e8b\u5b9e\u63a8\u7406\u63d0\u5347\u4f18\u5316\u6548\u679c\uff0c\u89e3\u51b3LLM\u5728\u5de5\u4e1aOR\u95ee\u9898\u4e2d\u7684\u4e24\u5927\u6311\u6218\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "LLM\u5728\u5de5\u4e1aOR\u5e94\u7528\u4e2d\u9762\u4e34\u6570\u5b66\u51c6\u786e\u6027\u4e0d\u8db3\u548c\u4e13\u5bb6\u9009\u62e9\u590d\u6742\u6027\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "ORMind\u6846\u67b6\u6a21\u4eff\u4eba\u7c7b\u8ba4\u77e5\uff0c\u5b9e\u73b0\u4ece\u9700\u6c42\u5230\u6570\u5b66\u6a21\u578b\u53ca\u53ef\u6267\u884c\u4ee3\u7801\u7684\u7cfb\u7edf\u5316\u8f6c\u6362\u3002", "result": "\u5b9e\u9a8c\u663e\u793aORMind\u5728NL4Opt\u548cComplexOR\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u53479.5%\u548c14.6%\u3002", "conclusion": "ORMind\u4e3a\u5de5\u4e1aOR\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u4f18\u5316\u6846\u67b6\uff0c\u672a\u6765\u5c06\u6269\u5c55\u5e94\u7528\u573a\u666f\u3002", "keywords": "\u64cd\u4f5c\u7814\u7a76, \u5927\u8bed\u8a00\u6a21\u578b, ORMind, \u53cd\u4e8b\u5b9e\u63a8\u7406, \u4f18\u5316"}}
{"id": "2506.00574", "pdf": "https://arxiv.org/pdf/2506.00574", "abs": "https://arxiv.org/abs/2506.00574", "authors": ["Fatemeh Lotfi", "Hossein Rajoli", "Fatemeh Afghah"], "title": "Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Modern wireless networks must adapt to dynamic conditions while efficiently\nmanaging diverse service demands. Traditional deep reinforcement learning (DRL)\nstruggles in these environments, as scattered and evolving feedback makes\noptimal decision-making challenging. Large Language Models (LLMs) offer a\nsolution by structuring unorganized network feedback into meaningful latent\nrepresentations, helping RL agents recognize patterns more effectively. For\nexample, in O-RAN slicing, concepts like SNR, power levels and throughput are\nsemantically related, and LLMs can naturally cluster them, providing a more\ninterpretable state representation. To leverage this capability, we introduce a\ncontextualization-based adaptation method that integrates learnable prompts\ninto an LLM-augmented DRL framework. Instead of relying on full model\nfine-tuning, we refine state representations through task-specific prompts that\ndynamically adjust to network conditions. Utilizing ORANSight, an LLM trained\non O-RAN knowledge, we develop Prompt-Augmented Multi agent RL (PA-MRL)\nframework. Learnable prompts optimize both semantic clustering and RL\nobjectives, allowing RL agents to achieve higher rewards in fewer iterations\nand adapt more efficiently. By incorporating prompt-augmented learning, our\napproach enables faster, more scalable, and adaptive resource allocation in\nO-RAN slicing. Experimental results show that it accelerates convergence and\noutperforms other baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5b66\u4e60\u63d0\u793a\u7684LLM\u589e\u5f3aDRL\u6846\u67b6\uff08PA-MRL\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u8bed\u4e49\u805a\u7c7b\u548cRL\u76ee\u6807\uff0c\u63d0\u5347\u65e0\u7ebf\u7f51\u7edc\u8d44\u6e90\u5206\u914d\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edfDRL\u5728\u52a8\u6001\u65e0\u7ebf\u7f51\u7edc\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u5bf9\u65e0\u5e8f\u53cd\u9988\u7684\u6709\u6548\u5904\u7406\uff1bLLMs\u80fd\u7ed3\u6784\u5316\u53cd\u9988\u5e76\u8f85\u52a9RL\u51b3\u7b56\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u9002\u5e94\u65b9\u6cd5\uff0c\u5229\u7528LLM\u7684\u53ef\u5b66\u4e60\u63d0\u793a\u4f18\u5316\u72b6\u6001\u8868\u793a\uff0c\u7ed3\u5408\u591a\u667a\u80fd\u4f53RL\u6846\u67b6\uff08PA-MRL\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u52a0\u901f\u6536\u655b\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8d44\u6e90\u5206\u914d\u3002", "conclusion": "\u63d0\u793a\u589e\u5f3a\u5b66\u4e60\u63d0\u5347\u4e86DRL\u7684\u9002\u5e94\u6027\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u7f51\u7edc\u73af\u5883\u3002", "keywords": "\u65e0\u7ebf\u7f51\u7edc, DRL, LLM, \u63d0\u793a\u5b66\u4e60, \u8d44\u6e90\u5206\u914d"}}
{"id": "2506.00585", "pdf": "https://arxiv.org/pdf/2506.00585", "abs": "https://arxiv.org/abs/2506.00585", "authors": ["Yucheng Cai", "Ke Li", "Yi Huang", "Junlan Feng", "Zhijian Ou"], "title": "Entriever: Energy-based Retriever for Knowledge-Grounded Dialog Systems", "categories": ["cs.CL"], "comment": "Accepted by ACL2025 Findings", "summary": "A retriever, which retrieves relevant knowledge pieces from a knowledge base\ngiven a context, is an important component in many natural language processing\n(NLP) tasks. Retrievers have been introduced in knowledge-grounded dialog\nsystems to improve knowledge acquisition. In knowledge-grounded dialog systems,\nwhen conditioning on a given context, there may be multiple relevant and\ncorrelated knowledge pieces. However, knowledge pieces are usually assumed to\nbe conditionally independent in current retriever models. To address this\nissue, we propose Entriever, an energy-based retriever. Entriever directly\nmodels the candidate retrieval results as a whole instead of modeling the\nknowledge pieces separately, with the relevance score defined by an energy\nfunction. We explore various architectures of energy functions and different\ntraining methods for Entriever, and show that Entriever substantially\noutperforms the strong cross-encoder baseline in knowledge retrieval tasks.\nFurthermore, we show that in semi-supervised training of knowledge-grounded\ndialog systems, Entriever enables effective scoring of retrieved knowledge\npieces and significantly improves end-to-end performance of dialog systems.", "AI": {"tldr": "Entriever\u662f\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u7684\u68c0\u7d22\u5668\uff0c\u901a\u8fc7\u76f4\u63a5\u5efa\u6a21\u5019\u9009\u68c0\u7d22\u7ed3\u679c\u7684\u6574\u4f53\u800c\u975e\u5355\u72ec\u5efa\u6a21\u77e5\u8bc6\u7247\u6bb5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u68c0\u7d22\u4efb\u52a1\u548c\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u5668\u6a21\u578b\u901a\u5e38\u5047\u8bbe\u77e5\u8bc6\u7247\u6bb5\u6761\u4ef6\u72ec\u7acb\uff0c\u4f46\u5b9e\u9645\u4e2d\u53ef\u80fd\u5b58\u5728\u591a\u4e2a\u76f8\u5173\u4e14\u76f8\u4e92\u4f9d\u8d56\u7684\u77e5\u8bc6\u7247\u6bb5\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86Entriever\u3002", "method": "Entriever\u91c7\u7528\u80fd\u91cf\u51fd\u6570\u5b9a\u4e49\u76f8\u5173\u6027\u5206\u6570\uff0c\u76f4\u63a5\u5efa\u6a21\u5019\u9009\u68c0\u7d22\u7ed3\u679c\u7684\u6574\u4f53\u3002\u7814\u7a76\u4e86\u591a\u79cd\u80fd\u91cf\u51fd\u6570\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "Entriever\u5728\u77e5\u8bc6\u68c0\u7d22\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\uff08cross-encoder\uff09\uff0c\u5e76\u5728\u534a\u76d1\u7763\u8bad\u7ec3\u7684\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u6027\u80fd\u3002", "conclusion": "Entriever\u901a\u8fc7\u6574\u4f53\u5efa\u6a21\u77e5\u8bc6\u7247\u6bb5\u7684\u76f8\u5173\u6027\uff0c\u80fd\u66f4\u6709\u6548\u5730\u652f\u6301\u77e5\u8bc6\u68c0\u7d22\u548c\u5bf9\u8bdd\u7cfb\u7edf\u3002", "keywords": "knowledge retrieval, retriever model, energy-based model, knowledge-grounded dialog"}}
{"id": "2506.01332", "pdf": "https://arxiv.org/pdf/2506.01332", "abs": "https://arxiv.org/abs/2506.01332", "authors": ["Min Choi", "Keonwoo Kim", "Sungwon Chae", "Sangyeob Baek"], "title": "An Empirical Study of Group Conformity in Multi-Agent Systems", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have enabled multi-agent\nsystems that simulate real-world interactions with near-human reasoning. While\nprevious studies have extensively examined biases related to protected\nattributes such as race, the emergence and propagation of biases on socially\ncontentious issues in multi-agent LLM interactions remain underexplored. This\nstudy explores how LLM agents shape public opinion through debates on five\ncontentious topics. By simulating over 2,500 debates, we analyze how initially\nneutral agents, assigned a centrist disposition, adopt specific stances over\ntime. Statistical analyses reveal significant group conformity mirroring human\nbehavior; LLM agents tend to align with numerically dominant groups or more\nintelligent agents, exerting a greater influence. These findings underscore the\ncrucial role of agent intelligence in shaping discourse and highlight the risks\nof bias amplification in online interactions. Our results emphasize the need\nfor policy measures that promote diversity and transparency in LLM-generated\ndiscussions to mitigate the risks of bias propagation within anonymous online\nenvironments.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u667a\u80fd\u4f53LLM\u5728\u4e89\u8bae\u8bdd\u9898\u4e0a\u7684\u504f\u89c1\u4f20\u64ad\uff0c\u901a\u8fc72500\u591a\u6b21\u6a21\u62df\u8fa9\u8bba\u63ed\u793a\u667a\u80fd\u4f53\u4f1a\u5411\u4e3b\u5bfc\u7fa4\u4f53\u6216\u66f4\u806a\u660e\u7684\u667a\u80fd\u4f53\u9760\u62e2\uff0c\u5f3a\u8c03\u4e86\u653f\u7b56\u5e72\u9884\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u63a2\u7d22\u591a\u667a\u80fd\u4f53LLM\u5728\u793e\u4f1a\u4e89\u8bae\u8bdd\u9898\u4e2d\u7684\u504f\u89c1\u4f20\u64ad\u73b0\u8c61\u53ca\u5176\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u6a21\u62df2500\u591a\u6b21\u8fa9\u8bba\uff0c\u5206\u6790\u521d\u59cb\u4e2d\u7acb\u667a\u80fd\u4f53\u5728\u4e89\u8bae\u8bdd\u9898\u4e0a\u7684\u7acb\u573a\u53d8\u5316\u3002", "result": "\u667a\u80fd\u4f53\u8868\u73b0\u51fa\u7fa4\u4f53\u4ece\u4f17\u884c\u4e3a\uff0c\u503e\u5411\u4e8e\u4e0e\u4e3b\u5bfc\u7fa4\u4f53\u6216\u66f4\u806a\u660e\u7684\u667a\u80fd\u4f53\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u533f\u540d\u5728\u7ebf\u73af\u5883\u4e2d\u7ba1\u7406LLM\u504f\u89c1\u4f20\u64ad\u7684\u653f\u7b56\u9700\u6c42\u3002", "keywords": "LLM, \u591a\u667a\u80fd\u4f53\u7cfb\u7edf, \u504f\u89c1\u4f20\u64ad, \u7fa4\u4f53\u4ece\u4f17, \u653f\u7b56\u5e72\u9884"}}
{"id": "2506.00576", "pdf": "https://arxiv.org/pdf/2506.00576", "abs": "https://arxiv.org/abs/2506.00576", "authors": ["Fatemeh Lotfi", "Hossein Rajoli", "Fatemeh Afghah"], "title": "ORAN-GUIDE: RAG-Driven Prompt Learning for LLM-Augmented Reinforcement Learning in O-RAN Network Slicing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Advanced wireless networks must support highly dynamic and heterogeneous\nservice demands. Open Radio Access Network (O-RAN) architecture enables this\nflexibility by adopting modular, disaggregated components, such as the RAN\nIntelligent Controller (RIC), Centralized Unit (CU), and Distributed Unit (DU),\nthat can support intelligent control via machine learning (ML). While deep\nreinforcement learning (DRL) is a powerful tool for managing dynamic resource\nallocation and slicing, it often struggles to process raw, unstructured input\nlike RF features, QoS metrics, and traffic trends. These limitations hinder\npolicy generalization and decision efficiency in partially observable and\nevolving environments. To address this, we propose \\textit{ORAN-GUIDE}, a\ndual-LLM framework that enhances multi-agent RL (MARL) with task-relevant,\nsemantically enriched state representations. The architecture employs a\ndomain-specific language model, ORANSight, pretrained on O-RAN control and\nconfiguration data, to generate structured, context-aware prompts. These\nprompts are fused with learnable tokens and passed to a frozen GPT-based\nencoder that outputs high-level semantic representations for DRL agents. This\ndesign adopts a retrieval-augmented generation (RAG) style pipeline tailored\nfor technical decision-making in wireless systems. Experimental results show\nthat ORAN-GUIDE improves sample efficiency, policy convergence, and performance\ngeneralization over standard MARL and single-LLM baselines.", "AI": {"tldr": "ORAN-GUIDE\u662f\u4e00\u4e2a\u53ccLLM\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\uff0c\u5229\u7528\u8bed\u4e49\u4e30\u5bcc\u72b6\u6001\u8868\u793a\u63d0\u5347\u65e0\u7ebf\u7f51\u7edc\u8d44\u6e90\u5206\u914d\u7684\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u52a8\u6001\u5f02\u6784\u670d\u52a1\u9700\u6c42\u4e0b\uff0c\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u5904\u7406\u975e\u7ed3\u6784\u8f93\u5165\uff08\u5982\u5c04\u9891\u7279\u5f81\u3001QoS\u6307\u6807\u7b49\uff09\u65f6\u6cdb\u5316\u80fd\u529b\u548c\u51b3\u7b56\u6548\u7387\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faORAN-GUIDE\u6846\u67b6\uff0c\u7ed3\u5408\u9886\u57df\u4e13\u7528\u8bed\u8a00\u6a21\u578bORANSight\u548c\u51bb\u7ed3\u7684GPT\u7f16\u7801\u5668\uff0c\u751f\u6210\u7ed3\u6784\u5316\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63d0\u793a\uff0c\u4e3aDRL\u667a\u80fd\u4f53\u63d0\u4f9b\u9ad8\u7ea7\u8bed\u4e49\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cORAN-GUIDE\u5728\u6837\u672c\u6548\u7387\u3001\u7b56\u7565\u6536\u655b\u6027\u548c\u6027\u80fd\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u6807\u51c6MARL\u548c\u5355LLM\u57fa\u7ebf\u3002", "conclusion": "ORAN-GUIDE\u901a\u8fc7\u8bed\u4e49\u589e\u5f3a\u7684MARL\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u7ebf\u7f51\u7edc\u8d44\u6e90\u5206\u914d\u7684\u667a\u80fd\u5316\u6c34\u5e73\u3002", "keywords": "\u65e0\u7ebf\u7f51\u7edc, O-RAN, \u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60, \u8bed\u8a00\u6a21\u578b, \u8d44\u6e90\u5206\u914d"}}
{"id": "2506.00608", "pdf": "https://arxiv.org/pdf/2506.00608", "abs": "https://arxiv.org/abs/2506.00608", "authors": ["Petros Raptopoulos", "Giorgos Filandrianos", "Maria Lymperaiou", "Giorgos Stamou"], "title": "PAKTON: A Multi-Agent Framework for Question Answering in Long Legal Agreements", "categories": ["cs.CL"], "comment": null, "summary": "Contract review is a complex and time-intensive task that typically demands\nspecialized legal expertise, rendering it largely inaccessible to non-experts.\nMoreover, legal interpretation is rarely straightforward-ambiguity is\npervasive, and judgments often hinge on subjective assessments. Compounding\nthese challenges, contracts are usually confidential, restricting their use\nwith proprietary models and necessitating reliance on open-source alternatives.\nTo address these challenges, we introduce PAKTON: a fully open-source,\nend-to-end, multi-agent framework with plug-and-play capabilities. PAKTON is\ndesigned to handle the complexities of contract analysis through collaborative\nagent workflows and a novel retrieval-augmented generation (RAG) component,\nenabling automated legal document review that is more accessible, adaptable,\nand privacy-preserving. Experiments demonstrate that PAKTON outperforms both\ngeneral-purpose and pretrained models in predictive accuracy, retrieval\nperformance, explainability, completeness, and grounded justifications as\nevaluated through a human study and validated with automated metrics.", "AI": {"tldr": "PAKTON\u662f\u4e00\u4e2a\u5168\u5f00\u6e90\u3001\u7aef\u5230\u7aef\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u89e3\u51b3\u5408\u540c\u5ba1\u67e5\u7684\u590d\u6742\u6027\u548c\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u5408\u540c\u5ba1\u67e5\u9700\u8981\u4e13\u4e1a\u6cd5\u5f8b\u77e5\u8bc6\u4e14\u8017\u65f6\uff0c\u5bf9\u975e\u4e13\u5bb6\u4e0d\u53cb\u597d\uff1b\u5408\u540c\u901a\u5e38\u4fdd\u5bc6\uff0c\u9650\u5236\u4e86\u4e13\u6709\u6a21\u578b\u7684\u4f7f\u7528\u3002", "method": "PAKTON\u91c7\u7528\u591a\u4ee3\u7406\u534f\u4f5c\u5de5\u4f5c\u6d41\u548c\u521b\u65b0\u7684RAG\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u6cd5\u5f8b\u6587\u4ef6\u5ba1\u67e5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPAKTON\u5728\u9884\u6d4b\u51c6\u786e\u6027\u3001\u68c0\u7d22\u6027\u80fd\u7b49\u65b9\u9762\u4f18\u4e8e\u901a\u7528\u6a21\u578b\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "PAKTON\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6613\u7528\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u5408\u540c\u5ba1\u67e5\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5408\u540c\u5ba1\u67e5,\u591a\u4ee3\u7406\u6846\u67b6,\u5f00\u6e90,RAG,\u9690\u79c1\u4fdd\u62a4"}}
{"id": "2506.01353", "pdf": "https://arxiv.org/pdf/2506.01353", "abs": "https://arxiv.org/abs/2506.01353", "authors": ["Nie Lin", "Yansen Wang", "Dongqi Han", "Weibang Jiang", "Jingyuan Li", "Ryosuke Furuta", "Yoichi Sato", "Dongsheng Li"], "title": "EgoBrain: Synergizing Minds and Eyes For Human Action Understanding", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "21 pages, 12 figures", "summary": "The integration of brain-computer interfaces (BCIs), in particular\nelectroencephalography (EEG), with artificial intelligence (AI) has shown\ntremendous promise in decoding human cognition and behavior from neural\nsignals. In particular, the rise of multimodal AI models have brought new\npossibilities that have never been imagined before. Here, we present EgoBrain\n--the world's first large-scale, temporally aligned multimodal dataset that\nsynchronizes egocentric vision and EEG of human brain over extended periods of\ntime, establishing a new paradigm for human-centered behavior analysis. This\ndataset comprises 61 hours of synchronized 32-channel EEG recordings and\nfirst-person video from 40 participants engaged in 29 categories of daily\nactivities. We then developed a muiltimodal learning framework to fuse EEG and\nvision for action understanding, validated across both cross-subject and\ncross-environment challenges, achieving an action recognition accuracy of\n66.70%. EgoBrain paves the way for a unified framework for brain-computer\ninterface with multiple modalities. All data, tools, and acquisition protocols\nare openly shared to foster open science in cognitive computing.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86EgoBrain\u6570\u636e\u96c6\uff0c\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u65f6\u95f4\u5bf9\u9f50\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u878d\u5408\u4e86\u8111\u7535\u56fe\uff08EEG\uff09\u548c\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u6570\u636e\uff0c\u4e3a\u4eba\u7c7b\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u4e8666.70%\u7684\u884c\u4e3a\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u63a2\u7d22\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u4e0e\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7684\u7ed3\u5408\uff0c\u5c24\u5176\u662f\u591a\u6a21\u6001AI\u6a21\u578b\u5728\u89e3\u7801\u4eba\u7c7b\u8ba4\u77e5\u548c\u884c\u4e3a\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4ee5\u63a8\u52a8\u4eba\u7c7b\u4e2d\u5fc3\u884c\u4e3a\u5206\u6790\u7684\u65b0\u8303\u5f0f\u3002", "method": "\u5f00\u53d1\u4e86EgoBrain\u6570\u636e\u96c6\uff0c\u5305\u542b61\u5c0f\u65f6\u7684\u540c\u6b6532\u901a\u9053EEG\u548c\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u6570\u636e\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u878d\u5408EEG\u548c\u89c6\u89c9\u6570\u636e\u4ee5\u8fdb\u884c\u884c\u4e3a\u7406\u89e3\u3002", "result": "\u5728\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u884c\u4e3a\u8bc6\u522b\u51c6\u786e\u6027\u8fbe\u523066.70%\uff0c\u9a8c\u8bc1\u4e86\u8de8\u53d7\u8bd5\u8005\u548c\u8de8\u73af\u5883\u7684\u6709\u6548\u6027\u3002", "conclusion": "EgoBrain\u4e3a\u591a\u6a21\u6001\u8111\u673a\u63a5\u53e3\u7684\u7edf\u4e00\u6846\u67b6\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u516c\u5f00\u6570\u636e\u3001\u5de5\u5177\u548c\u91c7\u96c6\u534f\u8bae\uff0c\u4fc3\u8fdb\u8ba4\u77e5\u8ba1\u7b97\u5f00\u653e\u79d1\u5b66\u53d1\u5c55\u3002", "keywords": "\u8111\u673a\u63a5\u53e3, \u591a\u6a21\u6001AI, EEG, \u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9, \u884c\u4e3a\u5206\u6790, \u5f00\u653e\u79d1\u5b66"}}
{"id": "2506.00580", "pdf": "https://arxiv.org/pdf/2506.00580", "abs": "https://arxiv.org/abs/2506.00580", "authors": ["Merlin Sch\u00fcler", "Laurenz Wiskott"], "title": "Slow Feature Analysis as Variational Inference Objective", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This work presents a novel probabilistic interpretation of Slow Feature\nAnalysis (SFA) through the lens of variational inference. Unlike prior\nformulations that recover linear SFA from Gaussian state-space models with\nlinear emissions, this approach relaxes the key constraint of linearity. While\nit does not lead to full equivalence to non-linear SFA, it recasts the\nclassical slowness objective in a variational framework. Specifically, it\nallows the slowness objective to be interpreted as a regularizer to a\nreconstruction loss. Furthermore, we provide arguments, why -- from the\nperspective of slowness optimization -- the reconstruction loss takes on the\nrole of the constraints that ensure informativeness in SFA. We conclude with a\ndiscussion of potential new research directions.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u7684\u89c6\u89d2\u63d0\u51fa\u4e86\u5bf9\u6162\u7279\u5f81\u5206\u6790\uff08SFA\uff09\u7684\u65b0\u6982\u7387\u89e3\u91ca\uff0c\u653e\u677e\u4e86\u7ebf\u6027\u7ea6\u675f\uff0c\u91cd\u65b0\u5c06\u7ecf\u5178\u7684\u6162\u76ee\u6807\u7f6e\u4e8e\u53d8\u5206\u6846\u67b6\u4e2d\u3002", "motivation": "\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u91cd\u65b0\u89e3\u91caSFA\uff0c\u63a2\u7d22\u975e\u7ebf\u6027\u6761\u4ef6\u4e0b\u7684\u6162\u7279\u5f81\u5206\u6790\uff0c\u6269\u5c55\u5176\u7406\u8bba\u57fa\u7840\u3002", "method": "\u91c7\u7528\u53d8\u5206\u63a8\u7406\u6846\u67b6\uff0c\u5c06\u6162\u76ee\u6807\u4f5c\u4e3a\u91cd\u6784\u635f\u5931\u7684\u6b63\u5219\u5316\u9879\uff0c\u5e76\u5206\u6790\u4e86\u91cd\u6784\u635f\u5931\u7684\u4f5c\u7528\u3002", "result": "\u5c55\u793a\u4e86\u6162\u76ee\u6807\u5728\u53d8\u5206\u6846\u67b6\u4e2d\u7684\u65b0\u89e3\u91ca\uff0c\u4e3a\u975e\u7ebf\u6027SFA\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "conclusion": "\u901a\u8fc7\u53d8\u5206\u65b9\u6cd5\u91cd\u65b0\u5b9a\u4e49SFA\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "Slow Feature Analysis, variational inference, probabilistic interpretation, non-linearity"}}
{"id": "2506.00612", "pdf": "https://arxiv.org/pdf/2506.00612", "abs": "https://arxiv.org/abs/2506.00612", "authors": ["Running Yang", "Wenlong Deng", "Minghui Chen", "Yuyin Zhou", "Xiaoxiao Li"], "title": "Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge Graph Guided Distractor Generation", "categories": ["cs.CL"], "comment": null, "summary": "Clinical tasks such as diagnosis and treatment require strong decision-making\nabilities, highlighting the importance of rigorous evaluation benchmarks to\nassess the reliability of large language models (LLMs). In this work, we\nintroduce a knowledge-guided data augmentation framework that enhances the\ndifficulty of clinical multiple-choice question (MCQ) datasets by generating\ndistractors (i.e., incorrect choices that are similar to the correct one and\nmay confuse existing LLMs). Using our KG-based pipeline, the generated choices\nare both clinically plausible and deliberately misleading. Our approach\ninvolves multi-step, semantically informed walks on a medical knowledge graph\nto identify distractor paths-associations that are medically relevant but\nfactually incorrect-which then guide the LLM in crafting more deceptive\ndistractors. We apply the designed knowledge graph guided distractor generation\n(KGGDG) pipline, to six widely used medical QA benchmarks and show that it\nconsistently reduces the accuracy of state-of-the-art LLMs. These findings\nestablish KGGDG as a powerful tool for enabling more robust and diagnostic\nevaluations of medical LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u77e5\u8bc6\u5f15\u5bfc\u7684\u6570\u636e\u589e\u5f3a\u6846\u67b6KGGDG\uff0c\u901a\u8fc7\u751f\u6210\u5177\u6709\u8bef\u5bfc\u6027\u7684\u5e72\u6270\u9879\uff0c\u63d0\u5347\u4e34\u5e8a\u591a\u9009\u9898\u7684\u96be\u5ea6\uff0c\u4ee5\u66f4\u4e25\u683c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u4e34\u5e8a\u4efb\u52a1\u5982\u8bca\u65ad\u548c\u6cbb\u7597\u9700\u8981\u5f3a\u5927\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e25\u683c\u7684\u8bc4\u4f30\u57fa\u51c6\u6765\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u591a\u6b65\u8bed\u4e49\u904d\u5386\u65b9\u6cd5\uff0c\u751f\u6210\u533b\u5b66\u4e0a\u76f8\u5173\u4f46\u4e8b\u5b9e\u9519\u8bef\u7684\u5e72\u6270\u9879\u8def\u5f84\uff0c\u6307\u5bfcLLM\u751f\u6210\u66f4\u5177\u8ff7\u60d1\u6027\u7684\u9009\u9879\u3002", "result": "\u5e94\u7528\u8be5\u6846\u67b6\u540e\uff0c\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u533b\u5b66QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u9876\u5c16LLM\u7684\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "KGGDG\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u533b\u5b66LLM\u7684\u9c81\u68d2\u6027\u548c\u8bca\u65ad\u80fd\u529b\u3002", "keywords": "\u77e5\u8bc6\u56fe\u8c31,\u6570\u636e\u589e\u5f3a,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u4e34\u5e8a\u591a\u9009\u9898,\u5e72\u6270\u9879\u751f\u6210"}}
{"id": "2506.01372", "pdf": "https://arxiv.org/pdf/2506.01372", "abs": "https://arxiv.org/abs/2506.01372", "authors": ["Minjun Zhu", "Qiujie Xie", "Yixuan Weng", "Jian Wu", "Zhen Lin", "Linyi Yang", "Yue Zhang"], "title": "AI Scientists Fail Without Strong Implementation Capability", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Position", "summary": "The emergence of Artificial Intelligence (AI) Scientist represents a paradigm\nshift in scientific discovery, with large language models (LLMs) taking the\nlead as the primary executor in the entire scientific workflow from idea\ngeneration to experiment implementation. Recent AI Scientist studies\ndemonstrate sufficient capabilities for independent scientific discovery, with\nthe generated research reports gaining acceptance at the ICLR 2025 workshop and\nACL 2025, arguing that a human-level AI Scientist, capable of uncovering\nphenomena previously unknown to humans, may be imminent. Despite this\nsubstantial progress, AI Scientist has yet to produce a groundbreaking\nachievement in the domain of computer science on par with automated scientific\ntools. Based on extensive quantitative evidence from existing benchmarks in\ncomplex engineering tasks and a systematic evaluation assess 28 research papers\ngenerated by five advanced AI Scientist systems, we argue that \\textbf{the\nfundamental bottleneck for AI Scientists lies in their capability to execute\nthe requisite verification procedures.} Current AI Scientist systems lack the\nexecution capabilities needed to execute rigorous experiments and produce\nhigh-quality scientific papers. To better illustrate the root cause of this\n\\textbf{implementation gap}, we provide an in-depth discussion on the\nfundamental limitations of AI Scientist. This position paper aims to call for\nthe participants in the community to bridge the implementation gap.", "AI": {"tldr": "AI\u79d1\u5b66\u5bb6\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u9a8c\u8bc1\u80fd\u529b\u662f\u5176\u74f6\u9888\uff0c\u9700\u89e3\u51b3\u6267\u884c\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u63a2\u8ba8AI\u79d1\u5b66\u5bb6\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u9a8c\u8bc1\u80fd\u529b\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5b9a\u91cf\u8bc1\u636e\u548c28\u7bc7AI\u751f\u6210\u8bba\u6587\u7684\u7cfb\u7edf\u8bc4\u4f30\u5206\u6790\u95ee\u9898\u3002", "result": "\u53d1\u73b0AI\u79d1\u5b66\u5bb6\u7684\u4e3b\u8981\u74f6\u9888\u662f\u9a8c\u8bc1\u80fd\u529b\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6267\u884c\u80fd\u529b\u3002", "conclusion": "\u547c\u5401\u793e\u533a\u5171\u540c\u89e3\u51b3AI\u79d1\u5b66\u5bb6\u7684\u6267\u884c\u80fd\u529b\u74f6\u9888\u3002", "keywords": "AI\u79d1\u5b66\u5bb6\uff0c\u79d1\u5b66\u53d1\u73b0\uff0c\u9a8c\u8bc1\u80fd\u529b\uff0c\u6267\u884c\u80fd\u529b\uff0c\u5b9e\u73b0\u7f3a\u53e3"}}
{"id": "2506.00587", "pdf": "https://arxiv.org/pdf/2506.00587", "abs": "https://arxiv.org/abs/2506.00587", "authors": ["Sonia Koszut", "Sam Nallaperuma-Herzberg", "Pietro Lio"], "title": "Decoding the Stressed Brain with Geometric Machine Learning", "categories": ["cs.LG"], "comment": "12 pages, 5 figures. This version has been accepted as a full paper\n  at the 2025 AI in Healthcare (AIiH) Conference", "summary": "Stress significantly contributes to both mental and physical disorders, yet\ntraditional self-reported questionnaires are inherently subjective. In this\nstudy, we introduce a novel framework that employs geometric machine learning\nto detect stress from raw EEG recordings. Our approach constructs graphs by\nintegrating structural connectivity (derived from electrode spatial\narrangement) with functional connectivity from pairwise signal correlations. A\nspatio-temporal graph convolutional network (ST-GCN) processes these graphs to\ncapture spatial and temporal dynamics. Experiments on the SAM-40 dataset show\nthat the ST-GCN outperforms standard machine learning models on all key\nclassification metrics and enhances interpretability, explored through ablation\nanalyses of key channels and brain regions. These results pave the way for more\nobjective and accurate stress detection methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u673a\u5668\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7EEG\u4fe1\u53f7\u68c0\u6d4b\u538b\u529b\uff0c\u7ed3\u5408\u7ed3\u6784\u548c\u529f\u80fd\u8fde\u63a5\u6027\uff0c\u4f7f\u7528ST-GCN\u6a21\u578b\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u538b\u529b\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e3b\u89c2\u95ee\u5377\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u66f4\u5ba2\u89c2\u3001\u51c6\u786e\u7684\u538b\u529b\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u96c6\u6210\u7ed3\u6784\u8fde\u63a5\u6027\u548c\u529f\u80fd\u8fde\u63a5\u6027\u6784\u5efa\u56fe\uff0c\u5229\u7528ST-GCN\u6a21\u578b\u5206\u6790\u65f6\u7a7a\u52a8\u6001\u3002", "result": "\u5728SAM-40\u6570\u636e\u96c6\u4e0a\uff0cST-GCN\u5728\u6240\u6709\u5173\u952e\u5206\u7c7b\u6307\u6807\u4e0a\u4f18\u4e8e\u6807\u51c6\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u538b\u529b\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u5ba2\u89c2\u548c\u51c6\u786e\u7684\u9014\u5f84\u3002", "keywords": "\u538b\u529b\u68c0\u6d4b, EEG, \u51e0\u4f55\u673a\u5668\u5b66\u4e60, ST-GCN"}}
{"id": "2506.00622", "pdf": "https://arxiv.org/pdf/2506.00622", "abs": "https://arxiv.org/abs/2506.00622", "authors": ["Haesung Pyun", "Yoonah Park", "Yohan Jo"], "title": "Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "In dialogue state tracking (DST), in-context learning comprises a retriever\nthat selects labeled dialogues as in-context examples and a DST model that uses\nthese examples to infer the dialogue state of the query dialogue. Existing\nmethods for constructing training data for retrievers suffer from three key\nlimitations: (1) the synergistic effect of examples is not considered, (2) the\nlinguistic characteristics of the query are not sufficiently factored in, and\n(3) scoring is not directly optimized for DST performance. Consequently, the\nretriever can fail to retrieve examples that would substantially improve DST\nperformance. To address these issues, we present CombiSearch, a method that\nscores effective in-context examples based on their combinatorial impact on DST\nperformance. Our evaluation on MultiWOZ shows that retrievers trained with\nCombiSearch surpass state-of-the-art models, achieving a 20x gain in data\nefficiency and generalizing well to the SGD dataset. Moreover, CombiSearch\nattains a 12% absolute improvement in the upper bound DST performance over\ntraditional approaches when no retrieval errors are assumed. This significantly\nincreases the headroom for practical DST performance while demonstrating that\nexisting methods rely on suboptimal data for retriever training.", "AI": {"tldr": "\u63d0\u51fa\u4e86CombiSearch\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec4\u5408\u5f0f\u8bc4\u5206\u4f18\u5316\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\uff08DST\uff09\u4e2d\u68c0\u7d22\u5668\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u5668\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u534f\u540c\u6548\u5e94\u3001\u8bed\u8a00\u7279\u5f81\u548c\u8bc4\u5206\u4f18\u5316\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86DST\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86CombiSearch\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4f8b\u5b50\u5bf9DST\u6027\u80fd\u7684\u7ec4\u5408\u5f71\u54cd\u8fdb\u884c\u8bc4\u5206\uff0c\u4f18\u5316\u68c0\u7d22\u6548\u679c\u3002", "result": "\u5728MultiWOZ\u548cSGD\u6570\u636e\u96c6\u4e0a\uff0cCombiSearch\u5b9e\u73b0\u4e8620\u500d\u6570\u636e\u6548\u7387\u63d0\u5347\u548c12%\u7edd\u5bf9\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "CombiSearch\u8bc1\u660e\u73b0\u6709\u68c0\u7d22\u5668\u8bad\u7ec3\u6570\u636e\u6b21\u4f18\uff0c\u663e\u8457\u63d0\u5347\u4e86DST\u6027\u80fd\u4e0a\u9650\u3002", "keywords": "\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u68c0\u7d22\u5668\u4f18\u5316\u3001CombiSearch"}}
{"id": "2506.01391", "pdf": "https://arxiv.org/pdf/2506.01391", "abs": "https://arxiv.org/abs/2506.01391", "authors": ["Zhong Zhang", "Yaxi Lu", "Yikun Fu", "Yupeng Huo", "Shenzhi Yang", "Yesai Wu", "Han Si", "Xin Cong", "Haotian Chen", "Yankai Lin", "Jie Xie", "Wei Zhou", "Wang Xu", "Yuanheng Zhang", "Zhou Su", "Zhongwu Zhai", "Xiaoming Liu", "Yudong Mei", "Jianming Xu", "Hongyan Tian", "Chongyi Wang", "Chi Chen", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun"], "title": "AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "I.2.8; I.2.7; I.2.10; H.5.2"], "comment": "The project is available at https://github.com/OpenBMB/AgentCPM-GUI", "summary": "The recent progress of large language model agents has opened new\npossibilities for automating tasks through graphical user interfaces (GUIs),\nespecially in mobile environments where intelligent interaction can greatly\nenhance usability. However, practical deployment of such agents remains\nconstrained by several key challenges. Existing training data is often noisy\nand lack semantic diversity, which hinders the learning of precise grounding\nand planning. Models trained purely by imitation tend to overfit to seen\ninterface patterns and fail to generalize in unfamiliar scenarios. Moreover,\nmost prior work focuses on English interfaces while overlooks the growing\ndiversity of non-English applications such as those in the Chinese mobile\necosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent\nbuilt for robust and efficient on-device GUI interaction. Our training pipeline\nincludes grounding-aware pre-training to enhance perception, supervised\nfine-tuning on high-quality Chinese and English trajectories to imitate\nhuman-like actions, and reinforcement fine-tuning with GRPO to improve\nreasoning capability. We also introduce a compact action space that reduces\noutput length and supports low-latency execution on mobile devices.\nAgentCPM-GUI achieves state-of-the-art performance on five public benchmarks\nand a new Chinese GUI benchmark called CAGUI, reaching $96.9\\%$ Type-Match and\n$91.3\\%$ Exact-Match. To facilitate reproducibility and further research, we\npublicly release all code, model checkpoint, and evaluation data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAgentCPM-GUI\uff0c\u4e00\u4e2a8B\u53c2\u6570\u7684\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\uff0c\u7528\u4e8e\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684GUI\u4ea4\u4e92\uff0c\u89e3\u51b3\u8bad\u7ec3\u6570\u636e\u566a\u58f0\u3001\u8bed\u4e49\u591a\u6837\u6027\u4e0d\u8db3\u548c\u8de8\u8bed\u8a00\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u7684\u8bad\u7ec3\u6570\u636e\u566a\u58f0\u5927\u3001\u8bed\u4e49\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u975e\u82f1\u8bed\u73af\u5883\uff08\u5982\u4e2d\u6587\uff09\u7684\u652f\u6301\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7grounding-aware\u9884\u8bad\u7ec3\u589e\u5f3a\u611f\u77e5\uff0c\u9ad8\u8d28\u91cf\u4e2d\u82f1\u6587\u8f68\u8ff9\u7684\u76d1\u7763\u5fae\u8c03\u6a21\u4eff\u4eba\u7c7b\u52a8\u4f5c\uff0cGRPO\u5f3a\u5316\u5fae\u8c03\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u7d27\u51d1\u52a8\u4f5c\u7a7a\u95f4\u4ee5\u964d\u4f4e\u5ef6\u8fdf\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5171\u57fa\u51c6\u548c\u4e2d\u6587CAGUI\u57fa\u51c6\u4e0a\u5b9e\u73b0SOTA\uff0cType-Match\u548cExact-Match\u5206\u522b\u8fbe\u523096.9%\u548c91.3%\u3002", "conclusion": "AgentCPM-GUI\u5728\u79fb\u52a8GUI\u4ea4\u4e92\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u652f\u6301\u591a\u8bed\u8a00\u5e76\u516c\u5f00\u4ee3\u7801\u548c\u6570\u636e\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406, GUI\u4ea4\u4e92, \u79fb\u52a8\u8bbe\u5907, \u4e2d\u82f1\u6587\u591a\u8bed\u8a00, \u5f3a\u5316\u5b66\u4e60"}}
{"id": "2506.00588", "pdf": "https://arxiv.org/pdf/2506.00588", "abs": "https://arxiv.org/abs/2506.00588", "authors": ["Jayanta Dey", "Nicholas Soures", "Miranda Gonzales", "Itamar Lerner", "Christopher Kanan", "Dhireesha Kudithipudi"], "title": "Temporal Chunking Enhances Recognition of Implicit Sequential Patterns", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this pilot study, we propose a neuro-inspired approach that compresses\ntemporal sequences into context-tagged chunks, where each tag represents a\nrecurring structural unit or``community'' in the sequence. These tags are\ngenerated during an offline sleep phase and serve as compact references to past\nexperience, allowing the learner to incorporate information beyond its\nimmediate input range. We evaluate this idea in a controlled synthetic\nenvironment designed to reveal the limitations of traditional neural network\nbased sequence learners, such as recurrent neural networks (RNNs), when facing\ntemporal patterns on multiple timescales. We evaluate this idea in a controlled\nsynthetic environment designed to reveal the limitations of traditional neural\nnetwork based sequence learners, such as recurrent neural networks (RNNs), when\nfacing temporal patterns on multiple timescales. Our results, while\npreliminary, suggest that temporal chunking can significantly enhance learning\nefficiency under resource constrained settings. A small-scale human pilot study\nusing a Serial Reaction Time Task further motivates the idea of structural\nabstraction. Although limited to synthetic tasks, this work serves as an early\nproof-of-concept, with initial evidence that learned context tags can transfer\nacross related task, offering potential for future applications in transfer\nlearning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u542f\u53d1\u7684\u65f6\u5e8f\u5e8f\u5217\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u7ebf\u7761\u7720\u9636\u6bb5\u751f\u6210\u4e0a\u4e0b\u6587\u6807\u8bb0\u5757\uff0c\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u5728\u9762\u4e34\u591a\u65f6\u95f4\u5c3a\u5ea6\u65f6\u5e8f\u6a21\u5f0f\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e0c\u671b\u901a\u8fc7\u65f6\u5e8f\u5206\u5757\u63d0\u5347\u8d44\u6e90\u53d7\u9650\u4e0b\u7684\u5b66\u4e60\u6548\u7387\u3002", "method": "\u8bbe\u8ba1\u5408\u6210\u73af\u5883\u8bc4\u4f30\u65f6\u5e8f\u5206\u5757\u65b9\u6cd5\uff0c\u4e0e\u4f20\u7edfRNN\u5bf9\u6bd4\uff0c\u5e76\u5728\u5c0f\u578b\u4eba\u7c7b\u8bd5\u9a8c\u4e2d\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u65f6\u5e8f\u5206\u5757\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6548\u7387\uff0c\u4e14\u4e0a\u4e0b\u6587\u6807\u8bb0\u5757\u53ef\u8de8\u4efb\u52a1\u8fc1\u79fb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e9\u671f\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5c55\u73b0\u4e86\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\u6f5c\u5728\u7684\u5e94\u7528\u4ef7\u503c\u3002", "keywords": "\u795e\u7ecf\u542f\u53d1, \u65f6\u5e8f\u5206\u5757, \u4e0a\u4e0b\u6587\u6807\u8bb0, \u5b66\u4e60\u6548\u7387, \u8fc1\u79fb\u5b66\u4e60"}}
{"id": "2506.00628", "pdf": "https://arxiv.org/pdf/2506.00628", "abs": "https://arxiv.org/abs/2506.00628", "authors": ["Niyati Bafna", "Matthew Wiesner"], "title": "LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Prior research indicates that LID model performance significantly declines on\naccented speech; however, the specific causes, extent, and characterization of\nthese errors remain under-explored. (i) We identify a common failure mode on\naccented speech whereby LID systems often misclassify L2 accented speech as the\nspeaker's native language or a related language. (ii) We present evidence\nsuggesting that state-of-the-art models are invariant to permutations of short\nspans of speech, implying they classify on the basis of short phonotactic\nfeatures indicative of accent rather than language. Our analysis reveals a\nsimple method to enhance model robustness to accents through input chunking.\n(iii) We present an approach that integrates sequence-level information into\nour model without relying on monolingual ASR systems; this reduces\naccent-language confusion and significantly enhances performance on accented\nspeech while maintaining comparable results on standard LID.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LID\u6a21\u578b\u5728\u53e3\u8bed\u97f3\u9891\u4e2d\u6027\u80fd\u4e0b\u964d\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8f93\u5165\u5206\u5757\u548c\u6574\u5408\u5e8f\u5217\u4fe1\u606f\u7684\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u5bf9\u53e3\u97f3\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684LID\u6a21\u578b\u5728\u53e3\u8bed\u97f3\u9891\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u4e14\u9519\u8bef\u6a21\u5f0f\u548c\u5bf9\u53e3\u97f3\u7684\u4e0d\u53d8\u6027\u5c1a\u672a\u6df1\u5165\u63a2\u7a76\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u7814\u7a76LID\u6a21\u578b\u5728\u53e3\u8bed\u97f3\u9891\u4e2d\u7684\u5e38\u89c1\u9519\u8bef\u6a21\u5f0f\uff0c\u63d0\u51fa\u8f93\u5165\u5206\u5757\u65b9\u6cd5\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u5e76\u5f15\u5165\u5e8f\u5217\u7ea7\u4fe1\u606f\u4ee5\u51cf\u5c11\u53e3\u97f3\u4e0e\u8bed\u8a00\u7684\u6df7\u6dc6\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u53e3\u8bed\u97f3\u9891\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6807\u51c6LID\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u7b80\u5355\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u663e\u8457\u6539\u5584LID\u6a21\u578b\u5728\u53e3\u8bed\u97f3\u9891\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "LID\u6a21\u578b\uff0c\u53e3\u8bed\u97f3\u9891\uff0c\u9c81\u68d2\u6027\uff0c\u8f93\u5165\u5206\u5757\uff0c\u5e8f\u5217\u4fe1\u606f"}}
{"id": "2506.01423", "pdf": "https://arxiv.org/pdf/2506.01423", "abs": "https://arxiv.org/abs/2506.01423", "authors": ["Hongyang Yang", "Likun Lin", "Yang She", "Xinyu Liao", "Jiaoyang Wang", "Runjia Zhang", "Yuquan Mo", "Christina Dan Wang"], "title": "FinRobot: Generative Business Process AI Agents for Enterprise Resource Planning in Finance", "categories": ["cs.AI", "cs.SE", "q-fin.GN"], "comment": null, "summary": "Enterprise Resource Planning (ERP) systems serve as the digital backbone of\nmodern financial institutions, yet they continue to rely on static, rule-based\nworkflows that limit adaptability, scalability, and intelligence. As business\noperations grow more complex and data-rich, conventional ERP platforms struggle\nto integrate structured and unstructured data in real time and to accommodate\ndynamic, cross-functional workflows.\n  In this paper, we present the first AI-native, agent-based framework for ERP\nsystems, introducing a novel architecture of Generative Business Process AI\nAgents (GBPAs) that bring autonomy, reasoning, and dynamic optimization to\nenterprise workflows. The proposed system integrates generative AI with\nbusiness process modeling and multi-agent orchestration, enabling end-to-end\nautomation of complex tasks such as budget planning, financial reporting, and\nwire transfer processing. Unlike traditional workflow engines, GBPAs interpret\nuser intent, synthesize workflows in real time, and coordinate specialized\nsub-agents for modular task execution. We validate the framework through case\nstudies in bank wire transfers and employee reimbursements, two representative\nfinancial workflows with distinct complexity and data modalities. Results show\nthat GBPAs achieve up to 40% reduction in processing time, 94% drop in error\nrate, and improved regulatory compliance by enabling parallelism, risk control\ninsertion, and semantic reasoning. These findings highlight the potential of\nGBPAs to bridge the gap between generative AI capabilities and enterprise-grade\nautomation, laying the groundwork for the next generation of intelligent ERP\nsystems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2aAI\u539f\u751f\u3001\u57fa\u4e8e\u4ee3\u7406\u7684ERP\u6846\u67b6GBPAs\uff0c\u7ed3\u5408\u751f\u6210\u5f0fAI\u4e0e\u4e1a\u52a1\u6d41\u7a0b\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u5e76\u964d\u4f4e\u9519\u8bef\u7387\u3002", "motivation": "\u4f20\u7edfERP\u7cfb\u7edf\u7684\u9759\u6001\u89c4\u5219\u5de5\u4f5c\u6d41\u96be\u4ee5\u9002\u5e94\u590d\u6742\u4e1a\u52a1\u9700\u6c42\uff0c\u9700\u5f15\u5165\u52a8\u6001\u667a\u80fd\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u751f\u6210\u5f0f\u4e1a\u52a1\u8fc7\u7a0bAI\u4ee3\u7406\uff08GBPAs\uff09\u67b6\u6784\uff0c\u96c6\u6210\u751f\u6210\u5f0fAI\u4e0e\u591a\u4ee3\u7406\u534f\u8c03\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u81ea\u52a8\u5316\u3002", "result": "GBPAs\u5728\u94f6\u884c\u7535\u6c47\u548c\u5458\u5de5\u62a5\u9500\u6848\u4f8b\u4e2d\u51cf\u5c1140%\u5904\u7406\u65f6\u95f4\u300194%\u9519\u8bef\u7387\uff0c\u5e76\u63d0\u5347\u5408\u89c4\u6027\u3002", "conclusion": "GBPAs\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fdERP\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u751f\u6210\u5f0fAI\u5728\u4f01\u4e1a\u81ea\u52a8\u5316\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "ERP, AI-native, generative AI, business process automation, dynamic workflow"}}
{"id": "2506.00592", "pdf": "https://arxiv.org/pdf/2506.00592", "abs": "https://arxiv.org/abs/2506.00592", "authors": ["Hongyao Tang", "Johan Obando-Ceron", "Pablo Samuel Castro", "Aaron Courville", "Glen Berseth"], "title": "Mitigating Plasticity Loss in Continual Reinforcement Learning by Reducing Churn", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Plasticity, or the ability of an agent to adapt to new tasks, environments,\nor distributions, is crucial for continual learning. In this paper, we study\nthe loss of plasticity in deep continual RL from the lens of churn: network\noutput variability for out-of-batch data induced by mini-batch training. We\ndemonstrate that (1) the loss of plasticity is accompanied by the exacerbation\nof churn due to the gradual rank decrease of the Neural Tangent Kernel (NTK)\nmatrix; (2) reducing churn helps prevent rank collapse and adjusts the step\nsize of regular RL gradients adaptively. Moreover, we introduce Continual Churn\nApproximated Reduction (C-CHAIN) and demonstrate it improves learning\nperformance and outperforms baselines in a diverse range of continual learning\nenvironments on OpenAI Gym Control, ProcGen, DeepMind Control Suite, and\nMinAtar benchmarks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5851\u6027\u4e27\u5931\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u901a\u8fc7\u51cf\u5c11\u7f51\u7edc\u8f93\u51fa\u7684\u53d8\u5f02\u6027\uff08churn\uff09\u6765\u6539\u5584\u5851\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86C-CHAIN\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5851\u6027\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6df1\u5ea6\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u4e2d\u5851\u6027\u4e27\u5931\u7684\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u8bba\u6587\u4ecechurn\u7684\u89d2\u5ea6\u63a2\u8ba8\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790Neural Tangent Kernel (NTK)\u77e9\u9635\u7684\u79e9\u9010\u6e10\u964d\u4f4e\u73b0\u8c61\uff0c\u63d0\u51fa\u51cf\u5c11churn\u4ee5\u7ef4\u6301\u5851\u6027\uff0c\u5e76\u5f15\u5165C-CHAIN\u65b9\u6cd5\u3002", "result": "C-CHAIN\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982OpenAI Gym Control\u3001ProcGen\u7b49\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5b66\u4e60\u6027\u80fd\u3002", "conclusion": "\u51cf\u5c11churn\u53ef\u4ee5\u6709\u6548\u9632\u6b62\u5851\u6027\u4e27\u5931\uff0cC-CHAIN\u65b9\u6cd5\u4e3a\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u6301\u7eed\u5b66\u4e60,\u5851\u6027\u4e27\u5931,\u6df1\u5ea6\u6301\u7eed\u5f3a\u5316\u5b66\u4e60,churn,C-CHAIN"}}
{"id": "2506.00634", "pdf": "https://arxiv.org/pdf/2506.00634", "abs": "https://arxiv.org/abs/2506.00634", "authors": ["Adam Visokay", "Ruth Bagley", "Ian Kennedy", "Chris Hess", "Kyle Crowder", "Rob Voigt", "Denis Peskoff"], "title": "Social Construction of Urban Space: Understanding Neighborhood Boundaries Using Rental Listings", "categories": ["cs.CL"], "comment": "8 pages, 3 figures, 4 tables", "summary": "Rental listings offer a unique window into how urban space is socially\nconstructed through language. We analyze Chicago Craigslist rental\nadvertisements from 2018 to 2024 to examine how listing agents characterize\nneighborhoods, identifying mismatches between institutional boundaries and\nneighborhood claims. Through manual and large language model annotation, we\nclassify unstructured listings from Craigslist according to their neighborhood.\nGeospatial analysis reveals three distinct patterns: properties with\nconflicting neighborhood designations due to competing spatial definitions,\nborder properties with valid claims to adjacent neighborhoods, and ``reputation\nlaundering\" where listings claim association with distant, desirable\nneighborhoods. Through topic modeling, we identify patterns that correlate with\nspatial positioning: listings further from neighborhood centers emphasize\ndifferent amenities than centrally-located units. Our findings demonstrate that\nnatural language processing techniques can reveal how definitions of urban\nspaces are contested in ways that traditional methods overlook.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790Craigslist\u79df\u623f\u5e7f\u544a\uff0c\u63ed\u793a\u57ce\u5e02\u7a7a\u95f4\u5982\u4f55\u901a\u8fc7\u8bed\u8a00\u88ab\u793e\u4f1a\u5efa\u6784\uff0c\u5e76\u53d1\u73b0\u4f20\u7edf\u65b9\u6cd5\u5ffd\u89c6\u7684\u7a7a\u95f4\u5b9a\u4e49\u4e89\u8bae\u3002", "motivation": "\u7814\u7a76\u79df\u623f\u5e7f\u544a\u5982\u4f55\u901a\u8fc7\u8bed\u8a00\u5851\u9020\u57ce\u5e02\u7a7a\u95f4\u7684\u793e\u4f1a\u5efa\u6784\uff0c\u7279\u522b\u662f\u90bb\u91cc\u5b9a\u4e49\u7684\u4e89\u8bae\u6027\u548c\u590d\u6742\u6027\u3002", "method": "\u7ed3\u5408\u4eba\u5de5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6807\u6ce8\uff0c\u5bf9\u829d\u52a0\u54e5Craigslist\u5e7f\u544a\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u8fdb\u884c\u5730\u7406\u7a7a\u95f4\u5206\u6790\u548c\u4e3b\u9898\u5efa\u6a21\u3002", "result": "\u53d1\u73b0\u4e09\u79cd\u7a7a\u95f4\u6a21\u5f0f\uff1a\u51b2\u7a81\u7684\u90bb\u91cc\u5b9a\u4e49\u3001\u8fb9\u754c\u623f\u6e90\u7684\u5408\u7406\u4e89\u8bae\u4ee5\u53ca\u201c\u58f0\u8a89\u6d17\u767d\u201d\uff1b\u4e3b\u9898\u5efa\u6a21\u663e\u793a\u623f\u6e90\u4f4d\u7f6e\u5f71\u54cd\u5176\u63cf\u8ff0\u7684\u4fbf\u5229\u8bbe\u65bd\u3002", "conclusion": "\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u80fd\u63ed\u793a\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7684\u57ce\u5e02\u7a7a\u95f4\u5b9a\u4e49\u4e89\u8bae\u3002", "keywords": "\u79df\u623f\u5e7f\u544a\uff0c\u57ce\u5e02\u7a7a\u95f4\uff0c\u793e\u4f1a\u5efa\u6784\uff0c\u5730\u7406\u7a7a\u95f4\u5206\u6790\uff0c\u4e3b\u9898\u5efa\u6a21"}}
{"id": "2506.01438", "pdf": "https://arxiv.org/pdf/2506.01438", "abs": "https://arxiv.org/abs/2506.01438", "authors": ["Prashik Buddhaghosh Bansod"], "title": "Distinguishing Autonomous AI Agents from Collaborative Agentic Systems: A Comprehensive Framework for Understanding Modern Intelligent Architectures", "categories": ["cs.AI"], "comment": null, "summary": "The emergence of large language models has catalyzed two distinct yet\ninterconnected paradigms in artificial intelligence: standalone AI Agents and\ncollaborative Agentic AI ecosystems. This comprehensive study establishes a\ndefinitive framework for distinguishing these architectures through systematic\nanalysis of their operational principles, structural compositions, and\ndeployment methodologies. We characterize AI Agents as specialized,\ntool-enhanced systems leveraging foundation models for targeted automation\nwithin constrained environments. Conversely, Agentic AI represents\nsophisticated multi-entity frameworks where distributed agents exhibit emergent\ncollective intelligence through coordinated interaction protocols. Our\ninvestigation traces the evolutionary trajectory from traditional rule-based\nsystems through generative AI foundations to contemporary agent architectures.\nWe present detailed architectural comparisons examining planning mechanisms,\nmemory systems, coordination protocols, and decision-making processes. The\nstudy categorizes application landscapes, contrasting single-agent\nimplementations in customer service and content management with multi-agent\ndeployments in research automation and complex decision support. We identify\ncritical challenges including reliability issues, coordination complexities,\nand scalability constraints, while proposing innovative solutions through\nenhanced reasoning frameworks, robust memory architectures, and improved\ncoordination mechanisms. This framework provides essential guidance for\npractitioners selecting appropriate agentic approaches and establishes\nfoundational principles for next-generation intelligent system development.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u5730\u5206\u6790\u4e86\u72ec\u7acbAI\u667a\u80fd\u4f53\u4e0e\u534f\u4f5c\u5f0fAgentic AI\u751f\u6001\u7cfb\u7edf\u7684\u533a\u522b\u4e0e\u8054\u7cfb\uff0c\u63d0\u4f9b\u4e86\u67b6\u6784\u5bf9\u6bd4\u3001\u5e94\u7528\u573a\u666f\u53ca\u6311\u6218\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u63a2\u7d22\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u50ac\u751f\u7684\u4e24\u79cdAI\u8303\u5f0f\uff08\u72ec\u7acb\u667a\u80fd\u4f53\u4e0e\u534f\u4f5c\u5f0f\u751f\u6001\u7cfb\u7edf\uff09\u7684\u5dee\u5f02\u4e0e\u5171\u540c\u70b9\uff0c\u4e3a\u667a\u80fd\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u64cd\u4f5c\u539f\u7406\u3001\u7ed3\u6784\u7ec4\u6210\u4e0e\u90e8\u7f72\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e24\u79cd\u67b6\u6784\u7684\u89c4\u5212\u673a\u5236\u3001\u8bb0\u5fc6\u7cfb\u7edf\u3001\u534f\u8c03\u534f\u8bae\u53ca\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u660e\u786e\u4e86AI\u667a\u80fd\u4f53\u4e0eAgentic AI\u7684\u7279\u5f81\u5dee\u5f02\uff0c\u603b\u7ed3\u4e86\u5e94\u7528\u573a\u666f\u53ca\u5173\u952e\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6848\uff08\u5982\u589e\u5f3a\u63a8\u7406\u6846\u67b6\u7b49\uff09\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u9009\u62e9\u5408\u9002\u667a\u80fd\u4f53\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5e76\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u7cfb\u7edf\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "keywords": "AI Agents, Agentic AI, \u67b6\u6784\u6bd4\u8f83, \u534f\u4f5c\u667a\u80fd, \u667a\u80fd\u7cfb\u7edf\u5f00\u53d1"}}
{"id": "2506.00594", "pdf": "https://arxiv.org/pdf/2506.00594", "abs": "https://arxiv.org/abs/2506.00594", "authors": ["Chunyu Wei", "Wenji Hu", "Xingjia Hao", "Yunhai Wang", "Yueguo Chen", "Bing Bai", "Fei Wang"], "title": "Graph Evidential Learning for Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by KDD25", "summary": "Graph anomaly detection faces significant challenges due to the scarcity of\nreliable anomaly-labeled datasets, driving the development of unsupervised\nmethods. Graph autoencoders (GAEs) have emerged as a dominant approach by\nreconstructing graph structures and node features while deriving anomaly scores\nfrom reconstruction errors. However, relying solely on reconstruction error for\nanomaly detection has limitations, as it increases the sensitivity to noise and\noverfitting. To address these issues, we propose Graph Evidential Learning\n(GEL), a probabilistic framework that redefines the reconstruction process\nthrough evidential learning. By modeling node features and graph topology using\nevidential distributions, GEL quantifies two types of uncertainty: graph\nuncertainty and reconstruction uncertainty, incorporating them into the anomaly\nscoring mechanism. Extensive experiments demonstrate that GEL achieves\nstate-of-the-art performance while maintaining high robustness against noise\nand structural perturbations.", "AI": {"tldr": "GEL\u662f\u4e00\u4e2a\u65b0\u7684\u6982\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u8bc1\u636e\u5b66\u4e60\u6539\u8fdb\u56fe\u5f02\u5e38\u68c0\u6d4b\uff0c\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u56fe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u56e0\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u548c\u4f20\u7edf\u65b9\u6cd5\u5bf9\u566a\u58f0\u654f\u611f\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8bc1\u636e\u5b66\u4e60\u5efa\u6a21\u56fe\u7684\u7279\u5f81\u548c\u62d3\u6251\u7ed3\u6784\uff0c\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u5e76\u7eb3\u5165\u5f02\u5e38\u8bc4\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGEL\u5728\u6027\u80fd\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GEL\u4e3a\u56fe\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6982\u7387\u6846\u67b6\u3002", "keywords": "\u56fe\u5f02\u5e38\u68c0\u6d4b,\u8bc1\u636e\u5b66\u4e60,\u4e0d\u786e\u5b9a\u6027,\u9c81\u68d2\u6027"}}
{"id": "2506.00636", "pdf": "https://arxiv.org/pdf/2506.00636", "abs": "https://arxiv.org/abs/2506.00636", "authors": ["Huy Ba Do", "Vy Le-Phuong Huynh", "Luan Thanh Nguyen"], "title": "ViToSA: Audio-Based Toxic Spans Detection on Vietnamese Speech Utterances", "categories": ["cs.CL"], "comment": "Accepted for presentation at INTERSPEECH 2025", "summary": "Toxic speech on online platforms is a growing concern, impacting user\nexperience and online safety. While text-based toxicity detection is\nwell-studied, audio-based approaches remain underexplored, especially for\nlow-resource languages like Vietnamese. This paper introduces ViToSA\n(Vietnamese Toxic Spans Audio), the first dataset for toxic spans detection in\nVietnamese speech, comprising 11,000 audio samples (25 hours) with accurate\nhuman-annotated transcripts. We propose a pipeline that combines ASR and toxic\nspans detection for fine-grained identification of toxic content. Our\nexperiments show that fine-tuning ASR models on ViToSA significantly reduces\nWER when transcribing toxic speech, while the text-based toxic spans detection\n(TSD) models outperform existing baselines. These findings establish a novel\nbenchmark for Vietnamese audio-based toxic spans detection, paving the way for\nfuture research in speech content moderation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u8d8a\u5357\u8bed\u9996\u4e2a\u7528\u4e8e\u8bed\u97f3\u4e2d\u6076\u610f\u5185\u5bb9\u68c0\u6d4b\u7684\u6570\u636e\u96c6ViToSA\uff0c\u5e76\u7ed3\u5408ASR\u548c\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d8a\u5357\u8bed\u8bed\u97f3\u6bd2\u6027\u5185\u5bb9\u7684\u8bc6\u522b\u6548\u679c\u3002", "motivation": "\u5728\u7ebf\u5e73\u53f0\u4e0a\u7684\u6709\u5bb3\u8bed\u97f3\u5185\u5bb9\u65e5\u76ca\u4e25\u91cd\uff0c\u800c\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u8d8a\u5357\u8bed\uff09\u7684\u8bed\u97f3\u6bd2\u6027\u68c0\u6d4b\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u5f15\u5165ViToSA\u6570\u636e\u96c6\uff0c\u7ed3\u5408ASR\uff08\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff09\u548c\u6bd2\u6027\u7247\u6bb5\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5f62\u6210\u7aef\u5230\u7aef\u7684\u68c0\u6d4b\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eViToSA\u5fae\u8c03\u7684ASR\u6a21\u578b\u5728\u6709\u5bb3\u8bed\u97f3\u8f6c\u5f55\u4e0aWER\u663e\u8457\u964d\u4f4e\uff0c\u6587\u672c\u6bd2\u6027\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "ViToSA\u4e3a\u8d8a\u5357\u8bed\u8bed\u97f3\u6bd2\u6027\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u8bed\u97f3\u5185\u5bb9\u5ba1\u6838\u7684\u7814\u7a76\u3002", "keywords": "\u8bed\u97f3\u6bd2\u6027\u68c0\u6d4b,\u8d8a\u5357\u8bed,ASR,\u6bd2\u6027\u7247\u6bb5\u68c0\u6d4b,\u5185\u5bb9\u5ba1\u6838"}}
{"id": "2506.01442", "pdf": "https://arxiv.org/pdf/2506.01442", "abs": "https://arxiv.org/abs/2506.01442", "authors": ["Xidong Yang", "Wenhao Li", "Junjie Sheng", "Chuyun Shen", "Yun Hua", "Xiangfeng Wang"], "title": "Agentic Episodic Control", "categories": ["cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has driven breakthroughs in AI, from game-play to\nscientific discovery and AI alignment. However, its broader applicability\nremains limited by challenges such as low data efficiency and poor\ngeneralizability. Recent advances suggest that large language models, with\ntheir rich world knowledge and reasoning capabilities, could complement RL by\nenabling semantic state modeling and task-agnostic planning. In this work, we\npropose the Agentic Episodic Control (AEC), a novel architecture that\nintegrates RL with LLMs to enhance decision-making. The AEC can leverage a\nlarge language model (LLM) to map the observations into language-grounded\nembeddings, which further can be stored in an episodic memory for rapid\nretrieval of high-value experiences. Simultaneously, a World-Graph working\nmemory module is utilized to capture structured environmental dynamics in order\nto enhance relational reasoning. Furthermore, a lightweight critical state\ndetector dynamically arbitrates between the episodic memory recall and the\nworld-model-guided exploration. On the whole, by combining the trial-and-error\nlearning scheme with LLM-derived semantic priors, the proposed AEC can improve\nboth data efficiency and generalizability in reinforcement learning. In\nexperiments on BabyAI-Text benchmark tasks, AEC demonstrates substantial\nimprovements over existing baselines, especially on complex and generalization\ntasks like FindObj, where it outperforms the best baseline by up to 76%. The\nproposed AEC framework bridges the strengths of numeric reinforcement learning\nand symbolic reasoning, which provides a pathway toward more adaptable and\nsample-efficient agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u578b\u67b6\u6784Agentic Episodic Control\uff08AEC\uff09\uff0c\u4ee5\u63d0\u5347\u51b3\u7b56\u6548\u7387\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u7a81\u7834\uff0c\u4f46\u5176\u6570\u636e\u6548\u7387\u4f4e\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u5c40\u9650\u6027\u9650\u5236\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002\u7ed3\u5408LLM\u7684\u8bed\u4e49\u5efa\u6a21\u548c\u89c4\u5212\u80fd\u529b\u53ef\u80fd\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "AEC\u901a\u8fc7LLM\u5c06\u89c2\u5bdf\u7ed3\u679c\u6620\u5c04\u4e3a\u57fa\u4e8e\u8bed\u8a00\u7684\u5d4c\u5165\u8868\u793a\uff0c\u5e76\u5c06\u5176\u5b58\u50a8\u5728\u60c5\u666f\u8bb0\u5fc6\u4e2d\u4ee5\u4fbf\u5feb\u901f\u68c0\u7d22\u9ad8\u4ef7\u503c\u7ecf\u9a8c\u3002\u540c\u65f6\uff0c\u901a\u8fc7World-Graph\u5de5\u4f5c\u8bb0\u5fc6\u6a21\u5757\u6355\u83b7\u7ed3\u6784\u5316\u73af\u5883\u52a8\u6001\uff0c\u5e76\u5229\u7528\u8f7b\u91cf\u7ea7\u4e34\u754c\u72b6\u6001\u68c0\u6d4b\u5668\u52a8\u6001\u534f\u8c03\u8bb0\u5fc6\u68c0\u7d22\u4e0e\u4e16\u754c\u6a21\u578b\u5f15\u5bfc\u7684\u63a2\u7d22\u3002", "result": "\u5728BabyAI-Text\u57fa\u51c6\u4efb\u52a1\u4e2d\uff0cAEC\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1FindObj\u4e0a\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe76%\u3002", "conclusion": "AEC\u6846\u67b6\u7ed3\u5408\u4e86\u6570\u503c\u5f3a\u5316\u5b66\u4e60\u4e0e\u7b26\u53f7\u63a8\u7406\u7684\u4f18\u52bf\uff0c\u4e3a\u5f00\u53d1\u66f4\u5177\u9002\u5e94\u6027\u548c\u6837\u672c\u6548\u7387\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u9014\u5f84\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u6570\u636e\u6548\u7387, \u6cdb\u5316\u80fd\u529b, \u60c5\u666f\u8bb0\u5fc6, \u4e16\u754c\u56fe"}}
{"id": "2506.00614", "pdf": "https://arxiv.org/pdf/2506.00614", "abs": "https://arxiv.org/abs/2506.00614", "authors": ["Ziqi Liu", "Pei Zeng", "Yi Ding"], "title": "Predictability-Aware Compression and Decompression Framework for Multichannel Time Series Data", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages,3 figures", "summary": "Real-world multichannel time series prediction faces growing demands for\nefficiency across edge and cloud environments, making channel compression a\ntimely and essential problem. Motivated by success of Multiple-Input\nMultiple-Output (MIMO) methods, we propose a predictability-aware\ncompression-decompression framework to reduce runtime, lower communication\ncost, and maintain prediction accuracy across diverse predictors. The core idea\ninvolves using a circular periodicity key matrix with orthogonality to capture\nunderlying time series predictability during compression and to mitigate\nreconstruction errors during decompression by relaxing oversimplified data\nassumptions. Theoretical and empirical analyses show that the proposed\nframework is both time-efficient and scalable under a large number of channels.\nExtensive experiments on six datasets across various predictors demonstrate\nthat the proposed method achieves superior overall performance by jointly\nconsidering prediction accuracy and runtime, while maintaining strong\ncompatibility with diverse predictors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u9884\u6d4b\u6027\u611f\u77e5\u7684\u538b\u7f29-\u89e3\u538b\u7f29\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u901a\u9053\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u65e8\u5728\u63d0\u9ad8\u6548\u7387\u3001\u964d\u4f4e\u901a\u4fe1\u6210\u672c\uff0c\u5e76\u4fdd\u6301\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u968f\u7740\u8fb9\u7f18\u548c\u4e91\u73af\u5883\u5bf9\u6548\u7387\u9700\u6c42\u7684\u589e\u52a0\uff0c\u901a\u9053\u538b\u7f29\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002\u53d7\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08MIMO\uff09\u65b9\u6cd5\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\u3002", "method": "\u5229\u7528\u5177\u6709\u6b63\u4ea4\u6027\u7684\u5468\u671f\u6027\u5173\u952e\u77e9\u9635\uff0c\u5728\u538b\u7f29\u9636\u6bb5\u6355\u83b7\u65f6\u95f4\u5e8f\u5217\u7684\u53ef\u9884\u6d4b\u6027\uff0c\u5e76\u5728\u89e3\u538b\u7f29\u9636\u6bb5\u901a\u8fc7\u653e\u677e\u7b80\u5316\u7684\u6570\u636e\u5047\u8bbe\u51cf\u5c11\u91cd\u6784\u8bef\u5dee\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5927\u91cf\u901a\u9053\u4e0b\u5177\u6709\u65f6\u95f4\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002\u516d\u4e2a\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u8fd0\u884c\u65f6\u6027\u80fd\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u4e0e\u591a\u6837\u5316\u9884\u6d4b\u5668\u517c\u5bb9\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u901a\u9053\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6574\u4f53\u6027\u80fd\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b, \u901a\u9053\u538b\u7f29, \u53ef\u9884\u6d4b\u6027\u611f\u77e5, MIMO, \u6b63\u4ea4\u6027"}}
{"id": "2506.00637", "pdf": "https://arxiv.org/pdf/2506.00637", "abs": "https://arxiv.org/abs/2506.00637", "authors": ["Lorenzo Jaime Yu Flores", "Ori Ernst", "Jackie Chi Kit Cheung"], "title": "Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "Well-calibrated model confidence scores can improve the usefulness of text\ngeneration models. For example, users can be prompted to review predictions\nwith low confidence scores, to prevent models from returning bad or potentially\ndangerous predictions. However, confidence metrics are not always well\ncalibrated in text generation. One reason is that in generation, there can be\nmany valid answers, which previous methods do not always account for. Hence, a\nconfident model could distribute its output probability among multiple\nsequences because they are all valid. We propose task-agnostic confidence\nmetrics suited to generation, which rely solely on the probabilities associated\nwith the model outputs without the need for further fine-tuning or heuristics.\nUsing these, we are able to improve the calibration of BART and Flan-T5 on\nsummarization, translation, and QA datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u65e0\u5173\u7684\u7f6e\u4fe1\u5ea6\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u6587\u672c\u751f\u6210\u6a21\u578b\u7684\u6821\u51c6\uff0c\u4ece\u800c\u63d0\u9ad8\u5176\u9884\u6d4b\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u5229\u7528\u6587\u672c\u751f\u6210\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u907f\u514d\u6a21\u578b\u8fd4\u56de\u4f4e\u8d28\u91cf\u6216\u5371\u9669\u7684\u9884\u6d4b\uff0c\u9700\u8981\u6539\u8fdb\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u65e0\u5173\u7684\u7f6e\u4fe1\u5ea6\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4ec5\u57fa\u4e8e\u6a21\u578b\u8f93\u51fa\u7684\u6982\u7387\u5206\u5e03\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u6216\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728BART\u548cFlan-T5\u6a21\u578b\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u6458\u8981\u3001\u7ffb\u8bd1\u548c\u95ee\u7b54\u4efb\u52a1\u7684\u6821\u51c6\u6548\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u7f6e\u4fe1\u5ea6\u5ea6\u91cf\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6539\u8fdb\u6587\u672c\u751f\u6210\u6a21\u578b\u7684\u6821\u51c6\uff0c\u63d0\u5347\u5176\u9884\u6d4b\u7684\u53ef\u9760\u6027\u3002", "keywords": "\u6587\u672c\u751f\u6210, \u7f6e\u4fe1\u5ea6\u6821\u51c6, \u4efb\u52a1\u65e0\u5173\u65b9\u6cd5, BART, Flan-T5"}}
{"id": "2506.01475", "pdf": "https://arxiv.org/pdf/2506.01475", "abs": "https://arxiv.org/abs/2506.01475", "authors": ["Zouying Cao", "Runze Wang", "Yifei Yang", "Xinbei Ma", "Xiaoyong Zhu", "Bo Zheng", "Hai Zhao"], "title": "PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization", "categories": ["cs.AI", "cs.CL"], "comment": "20 pages, 12 figures, 14 tables, ACL'25 Findings", "summary": "Large Language Model (LLM) agents have demonstrated impressive capabilities\nin handling complex interactive problems. Existing LLM agents mainly generate\nnatural language plans to guide reasoning, which is verbose and inefficient. NL\nplans are also tailored to specific tasks and restrict agents' ability to\ngeneralize across similar tasks. To this end, we explore pseudocode-style plans\n(P-code Plan) to capture the structural logic of reasoning. We find that P-code\nPlan empowers LLM agents with stronger generalization ability and more\nefficiency. Inspired by this finding, we propose a pseudocode-style Planning\nGuided Preference Optimization method called PGPO for effective agent learning.\nWith two planning-oriented rewards, PGPO further enhances LLM agents' ability\nto generate high-quality P-code Plans and subsequent reasoning. Experiments\nshow that PGPO achieves superior performance on representative agent benchmarks\nand outperforms the current leading baselines. Analyses reveal the advantage of\nPGPO in reducing action errors and omissions during reasoning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f2a\u4ee3\u7801\u98ce\u683c\u7684\u8ba1\u5212\uff08P-code Plan\uff09\u6765\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u4e86PGPO\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u901a\u8fc7\u751f\u6210\u81ea\u7136\u8bed\u8a00\u8ba1\u5212\u6307\u5bfc\u63a8\u7406\uff0c\u8fd9\u79cd\u65b9\u5f0f\u5197\u957f\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u7f3a\u4e4f\u8de8\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4f2a\u4ee3\u7801\u98ce\u683c\u8ba1\u5212\uff08P-code Plan\uff09\uff0c\u5e76\u5f00\u53d1\u4e86PGPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u4e2a\u9762\u5411\u8ba1\u5212\u7684\u5956\u52b1\u4f18\u5316\u4ee3\u7406\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u4ee3\u7801\u8ba1\u5212\u7684\u80fd\u529b\u3002", "result": "PGPO\u5728\u4ee3\u8868\u6027\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u5f53\u524d\u9886\u5148\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u63a8\u7406\u4e2d\u7684\u52a8\u4f5c\u9519\u8bef\u548c\u9057\u6f0f\u3002", "conclusion": "\u4f2a\u4ee3\u7801\u8ba1\u5212\u548cPGPO\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u7684\u63a8\u7406\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u4ee3\u7406, \u4f2a\u4ee3\u7801\u8ba1\u5212, PGPO, \u63a8\u7406\u6548\u7387"}}
{"id": "2506.00620", "pdf": "https://arxiv.org/pdf/2506.00620", "abs": "https://arxiv.org/abs/2506.00620", "authors": ["Ming-Yu Chung", "Jiashuo Fan", "Hancheng Ye", "Qinsi Wang", "Wei-Chen Shen", "Chia-Mu Yu", "Pin-Yu Chen", "Sy-Yen Kuo"], "title": "Model Reprogramming Demystified: A Neural Tangent Kernel Perspective", "categories": ["cs.LG", "68T05, 46E22"], "comment": "24 pages, 8 figures, 4 tables", "summary": "Model Reprogramming (MR) is a resource-efficient framework that adapts large\npre-trained models to new tasks with minimal additional parameters and data,\noffering a promising solution to the challenges of training large models for\ndiverse tasks. Despite its empirical success across various domains such as\ncomputer vision and time-series forecasting, the theoretical foundations of MR\nremain underexplored. In this paper, we present a comprehensive theoretical\nanalysis of MR through the lens of the Neural Tangent Kernel (NTK) framework.\nWe demonstrate that the success of MR is governed by the eigenvalue spectrum of\nthe NTK matrix on the target dataset and establish the critical role of the\nsource model's effectiveness in determining reprogramming outcomes. Our\ncontributions include a novel theoretical framework for MR, insights into the\nrelationship between source and target models, and extensive experiments\nvalidating our findings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7NTK\u6846\u67b6\u5bf9\u6a21\u578b\u91cd\u7f16\u7a0b\uff08MR\uff09\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5176\u6210\u529f\u7684\u5173\u952e\u5728\u4e8e\u76ee\u6807\u6570\u636e\u96c6\u4e0aNTK\u77e9\u9635\u7684\u7279\u5f81\u503c\u8c31\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6e90\u6a21\u578b\u6548\u679c\u5bf9\u91cd\u7f16\u7a0b\u7ed3\u679c\u7684\u91cd\u5927\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u6a21\u578b\u91cd\u7f16\u7a0b\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u5b9e\u8bc1\u6210\u529f\uff0c\u4f46\u5176\u7406\u8bba\u57fa\u7840\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u7406\u8bba\u5206\u6790\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u5207\u7ebf\u6838\uff08NTK\uff09\u6846\u67b6\uff0c\u5206\u6790MR\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u53d1\u73b0\u3002", "result": "MR\u7684\u6210\u529f\u53d6\u51b3\u4e8e\u76ee\u6807\u6570\u636e\u96c6\u4e0aNTK\u77e9\u9635\u7684\u7279\u5f81\u503c\u8c31\uff0c\u4e14\u6e90\u6a21\u578b\u7684\u6548\u679c\u5bf9\u91cd\u7f16\u7a0b\u7ed3\u679c\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u7814\u7a76\u4e3aMR\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u63ed\u793a\u4e86\u6e90\u6a21\u578b\u4e0e\u76ee\u6807\u6a21\u578b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "keywords": "\u6a21\u578b\u91cd\u7f16\u7a0b, \u795e\u7ecf\u5207\u7ebf\u6838, \u7279\u5f81\u503c\u8c31, \u6e90\u6a21\u578b, \u76ee\u6807\u6a21\u578b"}}
{"id": "2506.00643", "pdf": "https://arxiv.org/pdf/2506.00643", "abs": "https://arxiv.org/abs/2506.00643", "authors": ["Weijie Xu", "Shixian Cui", "Xi Fang", "Chi Xue", "Stephanie Eckman", "Chandan Reddy"], "title": "SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions", "categories": ["cs.CL", "cs.AI", "68T01", "I.2.7"], "comment": "40 pages, 13 figures", "summary": "Large language models (LLMs) are increasingly evaluated on single-answer\nmultiple-choice tasks, yet many real-world problems require identifying all\ncorrect answers from a set of options. This capability remains underexplored.\nWe introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on\nSelect All That Apply (SATA) questions across diverse domains, including\nreading comprehension, law, and biomedicine. Our evaluation of 27 open-source\nand proprietary models reveals a significant gap: even the strongest model\nachieves only 41.8% exact match, exposing LLMs' inability to reliably identify\nall correct answers. We find that this weakness stems from two core challenges:\nselection bias - models favor certain choices regardless of content, and count\nbias - models fail to predict the correct number of answers. To address these\nissues, we propose Choice Funnel, a decoding strategy that combines token\ndebiasing with adaptive thresholding to guide models toward complete and\naccurate selections. Choice Funnel achieves up to 29% higher exact match than\ncompetitive baselines while reducing inference cost by over 64%. Our findings\nexpose fundamental limitations in current LLMs and introduce a new framework\nfor diagnosing and improving multi-answer reasoning. We release SATA-BENCH and\nChoice Funnel to promote LLM development for robust decision-making in\nrealistic, multi-answer applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86SATA-BENCH\uff0c\u9996\u4e2a\u4e13\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u9009\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u800c\u8bbe\u8ba1\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86LLMs\u5728\u591a\u9009\u9898\u4e2d\u7684\u8868\u73b0\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848Choice Funnel\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22LLMs\u5728\u8bc6\u522b\u591a\u9009\u9898\u6240\u6709\u6b63\u786e\u7b54\u6848\u65b9\u9762\u7684\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u7a7a\u767d\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86Choice Funnel\u89e3\u7801\u7b56\u7565\uff0c\u7ed3\u5408\u53bb\u504f\u548c\u81ea\u9002\u5e94\u9608\u503c\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u591a\u9009\u9898\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u6700\u5f3a\u7684LLM\u5728\u591a\u9009\u9898\u4e2d\u7684\u51c6\u786e\u7387\u4ec541.8%\uff0c\u4f46Choice Funnel\u53ef\u5c06\u51c6\u786e\u7387\u63d0\u534729%\uff0c\u5e76\u964d\u4f4e64%\u7684\u63a8\u7406\u6210\u672c\u3002", "conclusion": "\u8bba\u6587\u63ed\u793a\u4e86LLMs\u5728\u591a\u9009\u9898\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6539\u8fdb\u6846\u67b6\uff0c\u63a8\u52a8\u4e86LLM\u5728\u591a\u7b54\u6848\u4efb\u52a1\u4e2d\u7684\u53d1\u5c55\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u591a\u9009\u4efb\u52a1,SATA-BENCH,Choice Funnel,\u53bb\u504f"}}
{"id": "2506.01616", "pdf": "https://arxiv.org/pdf/2506.01616", "abs": "https://arxiv.org/abs/2506.01616", "authors": ["Xiao Yang", "Jiawei Chen", "Jun Luo", "Zhengwei Fang", "Yinpeng Dong", "Hang Su", "Jun Zhu"], "title": "MLA-Trust: Benchmarking Trustworthiness of Multimodal LLM Agents in GUI Environments", "categories": ["cs.AI"], "comment": null, "summary": "The emergence of multimodal LLM-based agents (MLAs) has transformed\ninteraction paradigms by seamlessly integrating vision, language, action and\ndynamic environments, enabling unprecedented autonomous capabilities across GUI\napplications ranging from web automation to mobile systems. However, MLAs\nintroduce critical trustworthiness challenges that extend far beyond\ntraditional language models' limitations, as they can directly modify digital\nstates and trigger irreversible real-world consequences. Existing benchmarks\ninadequately tackle these unique challenges posed by MLAs' actionable outputs,\nlong-horizon uncertainty and multimodal attack vectors. In this paper, we\nintroduce MLA-Trust, the first comprehensive and unified framework that\nevaluates the MLA trustworthiness across four principled dimensions:\ntruthfulness, controllability, safety and privacy. We utilize websites and\nmobile applications as realistic testbeds, designing 34 high-risk interactive\ntasks and curating rich evaluation datasets. Large-scale experiments involving\n13 state-of-the-art agents reveal previously unexplored trustworthiness\nvulnerabilities unique to multimodal interactive scenarios. For instance,\nproprietary and open-source GUI-interacting MLAs pose more severe\ntrustworthiness risks than static MLLMs, particularly in high-stakes domains;\nthe transition from static MLLMs into interactive MLAs considerably compromises\ntrustworthiness, enabling harmful content generation in multi-step interactions\nthat standalone MLLMs would typically prevent; multi-step execution, while\nenhancing the adaptability of MLAs, involves latent nonlinear risk accumulation\nacross successive interactions, circumventing existing safeguards and resulting\nin unpredictable derived risks. Moreover, we present an extensible toolbox to\nfacilitate continuous evaluation of MLA trustworthiness across diverse\ninteractive environments.", "AI": {"tldr": "MLA-Trust\u662f\u4e00\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001LLM\u4ee3\u7406\u4fe1\u4efb\u5ea6\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5176\u5728\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u65b0\u6f0f\u6d1e\u3002", "motivation": "\u591a\u6a21\u6001LLM\u4ee3\u7406\uff08MLAs\uff09\u5728GUI\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5e26\u6765\u4e86\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u672a\u6d89\u53ca\u7684\u4fe1\u4efb\u95ee\u9898\uff0c\u5982\u76f4\u63a5\u4fee\u6539\u6570\u5b57\u72b6\u6001\u548c\u4e0d\u53ef\u9006\u540e\u679c\u3002", "method": "\u5f15\u5165MLA-Trust\u6846\u67b6\uff0c\u4ece\u771f\u5b9e\u6027\u548c\u53ef\u63a7\u6027\u7b49\u56db\u4e2a\u7ef4\u5ea6\u8bc4\u4f30MLAs\uff0c\u8bbe\u8ba134\u4e2a\u9ad8\u98ce\u9669\u4efb\u52a1\u548c\u8bc4\u4f30\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMLAs\u5728\u4ea4\u4e92\u573a\u666f\u4e2d\u5b58\u5728\u72ec\u7279\u4fe1\u4efb\u98ce\u9669\uff0c\u591a\u6b65\u6267\u884c\u4f1a\u7d2f\u79ef\u975e\u7ebf\u6027\u98ce\u9669\uff0c\u4e14\u9759\u6001MLLMs\u8f6c\u4e3a\u4ea4\u4e92MLAs\u4f1a\u964d\u4f4e\u4fe1\u4efb\u5ea6\u3002", "conclusion": "MLAs\u7684\u4fe1\u4efb\u6311\u6218\u9700\u65b0\u8bc4\u4f30\u65b9\u6cd5\uff0cMLA-Trust\u4e3a\u6301\u7eed\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5de5\u5177\u3002", "keywords": "\u591a\u6a21\u6001LLM\u4ee3\u7406,\u4fe1\u4efb\u5ea6\u8bc4\u4f30,\u4ea4\u4e92\u98ce\u9669,\u6846\u67b6\u8bbe\u8ba1"}}
{"id": "2506.00630", "pdf": "https://arxiv.org/pdf/2506.00630", "abs": "https://arxiv.org/abs/2506.00630", "authors": ["Young Jin Park", "Francois Germain", "Jing Liu", "Ye Wang", "Toshiaki Koike-Akino", "Gordon Wichern", "Navid Azizan", "Christopher R. Laughman", "Ankush Chakrabarty"], "title": "Probabilistic Forecasting for Building Energy Systems using Time-Series Foundation Models", "categories": ["cs.LG"], "comment": "Preliminary version appeared in NeurIPS TSALM Workshop:\n  https://neurips.cc/virtual/2024/103019", "summary": "Decision-making in building energy systems critically depends on the\npredictive accuracy of relevant time-series models. In scenarios lacking\nextensive data from a target building, foundation models (FMs) represent a\npromising technology that can leverage prior knowledge from vast and diverse\npre-training datasets to construct accurate probabilistic predictors for use in\ndecision-making tools. This paper investigates the applicability and\nfine-tuning strategies of time-series foundation models (TSFMs) in building\nenergy forecasting. We analyze both full fine-tuning and parameter-efficient\nfine-tuning approaches, particularly low-rank adaptation (LoRA), by using\nreal-world data from a commercial net-zero energy building to capture signals\nsuch as room occupancy, carbon emissions, plug loads, and HVAC energy\nconsumption. Our analysis reveals that the zero-shot predictive performance of\nTSFMs is generally suboptimal. To address this shortcoming, we demonstrate that\nemploying either full fine-tuning or parameter-efficient fine-tuning\nsignificantly enhances forecasting accuracy, even with limited historical data.\nNotably, fine-tuning with low-rank adaptation (LoRA) substantially reduces\ncomputational costs without sacrificing accuracy. Furthermore, fine-tuned TSFMs\nconsistently outperform state-of-the-art deep forecasting models (e.g.,\ntemporal fusion transformers) in accuracy, robustness, and generalization\nacross varying building zones and seasonal conditions. These results underline\nthe efficacy of TSFMs for practical, data-constrained building energy\nmanagement systems, enabling improved decision-making in pursuit of energy\nefficiency and sustainability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u5728\u5efa\u7b51\u80fd\u6e90\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u53ca\u5fae\u8c03\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u5fae\u8c03\u80fd\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4e14LoRA\u65b9\u6cd5\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u5efa\u7b51\u80fd\u6e90\u7cfb\u7edf\u7684\u51b3\u7b56\u4f9d\u8d56\u4e8e\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u800c\u5728\u76ee\u6807\u5efa\u7b51\u6570\u636e\u4e0d\u8db3\u7684\u60c5\u51b5\u4e0b\uff0c\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u53ef\u4ee5\u5229\u7528\u9884\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u7cbe\u786e\u7684\u6982\u7387\u9884\u6d4b\u5668\u3002", "method": "\u7814\u7a76\u4e86\u5168\u5fae\u8c03\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08\u7279\u522b\u662f\u4f4e\u79e9\u9002\u5e94LoRA\uff09\u65b9\u6cd5\uff0c\u4f7f\u7528\u5546\u4e1a\u51c0\u96f6\u80fd\u8017\u5efa\u7b51\u7684\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u5206\u6790\u3002", "result": "TSFMs\u7684\u96f6\u6837\u672c\u9884\u6d4b\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5fae\u8c03\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff1bLoRA\u65b9\u6cd5\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u4e14\u5fae\u8c03\u540e\u7684TSFMs\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u9884\u6d4b\u6a21\u578b\u3002", "conclusion": "\u5fae\u8c03\u7684TSFMs\u5728\u6570\u636e\u53d7\u9650\u7684\u5efa\u7b51\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u80fd\u6e90\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u3001\u5efa\u7b51\u80fd\u6e90\u9884\u6d4b\u3001\u5fae\u8c03\u3001LoRA\u3001\u80fd\u6e90\u6548\u7387"}}
{"id": "2506.00644", "pdf": "https://arxiv.org/pdf/2506.00644", "abs": "https://arxiv.org/abs/2506.00644", "authors": ["Ana Rita Valente", "Rufael Marew", "Hawau Olamide Toyin", "Hamdan Al-Ali", "Anelise Bohnen", "Inma Becerra", "Elsa Marta Soares", "Goncalo Leal", "Hanan Aldarmaki"], "title": "Clinical Annotations for Automatic Stuttering Severity Assessment", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Stuttering is a complex disorder that requires specialized expertise for\neffective assessment and treatment. This paper presents an effort to enhance\nthe FluencyBank dataset with a new stuttering annotation scheme based on\nestablished clinical standards. To achieve high-quality annotations, we hired\nexpert clinicians to label the data, ensuring that the resulting annotations\nmirror real-world clinical expertise. The annotations are multi-modal,\nincorporating audiovisual features for the detection and classification of\nstuttering moments, secondary behaviors, and tension scores. In addition to\nindividual annotations, we additionally provide a test set with highly reliable\nannotations based on expert consensus for assessing individual annotators and\nmachine learning models. Our experiments and analysis illustrate the complexity\nof this task that necessitates extensive clinical expertise for valid training\nand evaluation of stuttering assessment models.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e34\u5e8a\u6807\u51c6\u7684\u65b0\u7684\u53e3\u5403\u6807\u6ce8\u65b9\u6848\uff0c\u4ee5\u589e\u5f3aFluencyBank\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u548c\u591a\u6a21\u6001\u7279\u5f81\u63d0\u9ad8\u6807\u6ce8\u8d28\u91cf\u3002", "motivation": "\u53e3\u5403\u662f\u4e00\u79cd\u590d\u6742\u7684\u969c\u788d\uff0c\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u548c\u6280\u80fd\u8fdb\u884c\u6709\u6548\u8bc4\u4f30\u548c\u6cbb\u7597\uff0c\u73b0\u6709\u6570\u636e\u96c6\u9700\u8981\u66f4\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u65b9\u6848\u3002", "method": "\u8058\u8bf7\u4e13\u4e1a\u4e34\u5e8a\u533b\u751f\u8fdb\u884c\u6807\u6ce8\uff0c\u91c7\u7528\u591a\u6a21\u6001\u7279\u5f81\uff08\u89c6\u542c\u4fe1\u606f\uff09\u68c0\u6d4b\u548c\u5206\u7c7b\u53e3\u5403\u77ac\u95f4\u3001\u6b21\u8981\u884c\u4e3a\u53ca\u7d27\u5f20\u8bc4\u5206\uff0c\u5e76\u63d0\u4f9b\u57fa\u4e8e\u4e13\u5bb6\u5171\u8bc6\u7684\u9ad8\u53ef\u9760\u6d4b\u8bd5\u96c6\u3002", "result": "\u5b9e\u9a8c\u548c\u5206\u6790\u8868\u660e\uff0c\u8be5\u4efb\u52a1\u590d\u6742\u6027\u9ad8\uff0c\u9700\u8981\u4e30\u5bcc\u7684\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u7528\u4e8e\u6709\u6548\u8bad\u7ec3\u548c\u8bc4\u4f30\u53e3\u5403\u8bc4\u4f30\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u548c\u591a\u6a21\u6001\u7279\u5f81\uff0c\u65b0\u6807\u6ce8\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u96c6\u7684\u4e34\u5e8a\u5b9e\u7528\u6027\u548c\u6a21\u578b\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "keywords": "\u53e3\u5403,\u6807\u6ce8\u65b9\u6848,FluencyBank,\u591a\u6a21\u6001\u7279\u5f81,\u4e34\u5e8a\u4e13\u5bb6"}}
{"id": "2506.01622", "pdf": "https://arxiv.org/pdf/2506.01622", "abs": "https://arxiv.org/abs/2506.01622", "authors": ["Jonathan Richens", "David Abel", "Alexis Bellot", "Tom Everitt"], "title": "General agents need world models", "categories": ["cs.AI", "cs.LG", "cs.RO", "stat.ML"], "comment": "Accepted ICML 2025", "summary": "Are world models a necessary ingredient for flexible, goal-directed\nbehaviour, or is model-free learning sufficient? We provide a formal answer to\nthis question, showing that any agent capable of generalizing to multi-step\ngoal-directed tasks must have learned a predictive model of its environment. We\nshow that this model can be extracted from the agent's policy, and that\nincreasing the agents performance or the complexity of the goals it can achieve\nrequires learning increasingly accurate world models. This has a number of\nconsequences: from developing safe and general agents, to bounding agent\ncapabilities in complex environments, and providing new algorithms for\neliciting world models from agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4e16\u754c\u6a21\u578b\u662f\u5426\u4e3a\u7075\u6d3b\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u8fd8\u662f\u65e0\u6a21\u578b\u5b66\u4e60\u8db3\u591f\u3002\u7814\u7a76\u8868\u660e\uff0c\u80fd\u591f\u63a8\u5e7f\u5230\u591a\u6b65\u76ee\u6807\u5bfc\u5411\u4efb\u52a1\u7684\u4ee3\u7406\u5fc5\u987b\u5b66\u4e60\u73af\u5883\u9884\u6d4b\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u4e16\u754c\u6a21\u578b\u5728\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u4e2d\u7684\u4f5c\u7528\uff0c\u9a8c\u8bc1\u5176\u662f\u5426\u4e3a\u5fc5\u8981\u56e0\u7d20\uff0c\u4ee5\u53ca\u65e0\u6a21\u578b\u5b66\u4e60\u662f\u5426\u80fd\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4ee3\u7406\u5fc5\u987b\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u4ee5\u5b9e\u73b0\u591a\u6b65\u76ee\u6807\u5bfc\u5411\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4ece\u4ee3\u7406\u7b56\u7565\u4e2d\u63d0\u53d6\u8be5\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u4ee3\u7406\u6027\u80fd\u63d0\u5347\u6216\u76ee\u6807\u590d\u6742\u5ea6\u589e\u52a0\u9700\u8981\u66f4\u7cbe\u786e\u7684\u4e16\u754c\u6a21\u578b\uff0c\u8fd9\u5bf9\u5f00\u53d1\u5b89\u5168\u901a\u7528\u4ee3\u7406\u3001\u9650\u5236\u590d\u6742\u73af\u5883\u4e2d\u7684\u4ee3\u7406\u80fd\u529b\u7b49\u6709\u91cd\u8981\u5f71\u54cd\u3002", "conclusion": "\u4e16\u754c\u6a21\u578b\u662f\u7075\u6d3b\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u5176\u7cbe\u5ea6\u76f4\u63a5\u5f71\u54cd\u4ee3\u7406\u80fd\u529b\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u3002", "keywords": "\u4e16\u754c\u6a21\u578b\uff0c\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\uff0c\u65e0\u6a21\u578b\u5b66\u4e60\uff0c\u9884\u6d4b\u6a21\u578b\uff0c\u4ee3\u7406\u6027\u80fd"}}
{"id": "2506.00635", "pdf": "https://arxiv.org/pdf/2506.00635", "abs": "https://arxiv.org/abs/2506.00635", "authors": ["Wei Chen", "Yuxuan Liang"], "title": "Learning with Calibration: Exploring Test-Time Computing of Spatio-Temporal Forecasting", "categories": ["cs.LG", "cs.AI", "cs.ET", "stat.ML"], "comment": "28 pages, 9 figures, 8 tables", "summary": "Spatio-temporal forecasting is crucial in many domains, such as\ntransportation, meteorology, and energy. However, real-world scenarios\nfrequently present challenges such as signal anomalies, noise, and\ndistributional shifts. Existing solutions primarily enhance robustness by\nmodifying network architectures or training procedures. Nevertheless, these\napproaches are computationally intensive and resource-demanding, especially for\nlarge-scale applications. In this paper, we explore a novel test-time computing\nparadigm, namely learning with calibration, ST-TTC, for spatio-temporal\nforecasting. Through learning with calibration, we aim to capture periodic\nstructural biases arising from non-stationarity during the testing phase and\nperform real-time bias correction on predictions to improve accuracy.\nSpecifically, we first introduce a spectral-domain calibrator with\nphase-amplitude modulation to mitigate periodic shift and then propose a flash\nupdating mechanism with a streaming memory queue for efficient test-time\ncomputation. ST-TTC effectively bypasses complex training-stage techniques,\noffering an efficient and generalizable paradigm. Extensive experiments on\nreal-world datasets demonstrate the effectiveness, universality, flexibility\nand efficiency of our proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6821\u51c6\u5b66\u4e60\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8303\u5f0fST-TTC\uff0c\u7528\u4e8e\u65f6\u7a7a\u9884\u6d4b\uff0c\u901a\u8fc7\u5b9e\u65f6\u504f\u5dee\u4fee\u6b63\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u65f6\u7a7a\u9884\u6d4b\u4e2d\u4fe1\u53f7\u5f02\u5e38\u3001\u566a\u58f0\u548c\u5206\u5e03\u504f\u79fb\u7b49\u6311\u6218\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u548c\u8d44\u6e90\u9700\u6c42\u3002", "method": "\u5f15\u5165\u9891\u8c31\u57df\u6821\u51c6\u5668\u8fdb\u884c\u5468\u671f\u6027\u504f\u79fb\u4fee\u6b63\uff0c\u5e76\u91c7\u7528\u95ea\u901f\u66f4\u65b0\u673a\u5236\u548c\u6d41\u5f0f\u5185\u5b58\u961f\u5217\u4f18\u5316\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eST-TTC\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5177\u6709\u9ad8\u6548\u6027\u3001\u901a\u7528\u6027\u3001\u7075\u6d3b\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "ST-TTC\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u590d\u6742\u8bad\u7ec3\u7684\u9ad8\u6548\u901a\u7528\u65f6\u7a7a\u9884\u6d4b\u8303\u5f0f\u3002", "keywords": "\u65f6\u7a7a\u9884\u6d4b\u3001\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u3001\u6821\u51c6\u5b66\u4e60\u3001\u9891\u8c31\u57df\u6821\u51c6\u3001\u95ea\u901f\u66f4\u65b0"}}
{"id": "2506.00649", "pdf": "https://arxiv.org/pdf/2506.00649", "abs": "https://arxiv.org/abs/2506.00649", "authors": ["Neil De La Fuente", "Oscar Sainz", "Iker Garc\u00eda-Ferrero", "Eneko Agirre"], "title": "GuideX: Guided Synthetic Data Generation for Zero-Shot Information Extraction", "categories": ["cs.CL"], "comment": "ACL Findings 2025", "summary": "Information Extraction (IE) systems are traditionally domain-specific,\nrequiring costly adaptation that involves expert schema design, data\nannotation, and model training. While Large Language Models have shown promise\nin zero-shot IE, performance degrades significantly in unseen domains where\nlabel definitions differ. This paper introduces GUIDEX, a novel method that\nautomatically defines domain-specific schemas, infers guidelines, and generates\nsynthetically labeled instances, allowing for better out-of-domain\ngeneralization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art\nacross seven zeroshot Named Entity Recognition benchmarks. Models trained with\nGUIDEX gain up to 7 F1 points over previous methods without humanlabeled data,\nand nearly 2 F1 points higher when combined with it. Models trained on GUIDEX\ndemonstrate enhanced comprehension of complex, domain-specific annotation\nschemas. Code, models, and synthetic datasets are available at\nneilus03.github.io/guidex.com", "AI": {"tldr": "\u63d0\u51fa\u4e86GUIDEX\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u5b9a\u4e49\u9886\u57df\u6a21\u5f0f\u3001\u63a8\u65ad\u6307\u5357\u5e76\u751f\u6210\u5408\u6210\u6807\u8bb0\u5b9e\u4f8b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u4fe1\u606f\u63d0\u53d6\u7cfb\u7edf\u9700\u4e13\u5bb6\u8bbe\u8ba1\u3001\u6570\u636e\u6807\u6ce8\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u6210\u672c\u9ad8\uff1b\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faGUIDEX\u65b9\u6cd5\uff0c\u81ea\u52a8\u751f\u6210\u9886\u57df\u6a21\u5f0f\u3001\u6307\u5357\u548c\u5408\u6210\u6570\u636e\uff0c\u7ed3\u5408Llama 3.1\u5fae\u8c03\u3002", "result": "\u57287\u4e2a\u96f6\u6837\u672cNER\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u6700\u9ad8\u6c34\u5e73\uff0c\u63d0\u53477 F1\u70b9\uff08\u65e0\u4eba\u7c7b\u6570\u636e\u65f6\uff09\uff0c\u7ed3\u5408\u4eba\u7c7b\u6570\u636e\u65f6\u63d0\u5347\u8fd12 F1\u70b9\u3002", "conclusion": "GUIDEX\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u590d\u6742\u9886\u57df\u6a21\u5f0f\u7684\u7406\u89e3\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002", "keywords": "\u4fe1\u606f\u63d0\u53d6, \u96f6\u6837\u672c\u5b66\u4e60, \u9886\u57df\u9002\u5e94, GUIDEX"}}
{"id": "2506.01623", "pdf": "https://arxiv.org/pdf/2506.01623", "abs": "https://arxiv.org/abs/2506.01623", "authors": ["Ajsal Shereef Palattuparambil", "Thommen George Karimpanal", "Santu Rana"], "title": "MAGIK: Mapping to Analogous Goals via Imagination-enabled Knowledge Transfer", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Humans excel at analogical reasoning - applying knowledge from one task to a\nrelated one with minimal relearning. In contrast, reinforcement learning (RL)\nagents typically require extensive retraining even when new tasks share\nstructural similarities with previously learned ones. In this work, we propose\nMAGIK, a novel framework that enables RL agents to transfer knowledge to\nanalogous tasks without interacting with the target environment. Our approach\nleverages an imagination mechanism to map entities in the target task to their\nanalogues in the source domain, allowing the agent to reuse its original\npolicy. Experiments on custom MiniGrid and MuJoCo tasks show that MAGIK\nachieves effective zero-shot transfer using only a small number of\nhuman-labelled examples. We compare our approach to related baselines and\nhighlight how it offers a novel and effective mechanism for knowledge transfer\nvia imagination-based analogy mapping.", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u667a\u80fd\u4f53\u5728\u65b0\u4efb\u52a1\u4e0a\u7684\u77e5\u8bc6\u8fc1\u79fb\u80fd\u529b\u901a\u5e38\u8f83\u5dee\u3002\u672c\u6587\u63d0\u51fa\u7684MAGIK\u6846\u67b6\u901a\u8fc7\u60f3\u8c61\u529b\u673a\u5236\u5c06\u76ee\u6807\u4efb\u52a1\u7684\u5b9e\u4f53\u6620\u5c04\u5230\u6e90\u9886\u57df\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "motivation": "\u4eba\u7c7b\u80fd\u8f7b\u6613\u8fdb\u884c\u7c7b\u6bd4\u63a8\u7406\uff0c\u800cRL\u667a\u80fd\u4f53\u5219\u9700\u8981\u5927\u91cf\u91cd\u65b0\u8bad\u7ec3\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3RL\u667a\u80fd\u4f53\u5728\u7c7b\u6bd4\u4efb\u52a1\u4e2d\u7684\u77e5\u8bc6\u8fc1\u79fb\u95ee\u9898\u3002", "method": "MAGIK\u6846\u67b6\u5229\u7528\u60f3\u8c61\u529b\u673a\u5236\u5c06\u76ee\u6807\u4efb\u52a1\u7684\u5b9e\u4f53\u6620\u5c04\u5230\u6e90\u9886\u57df\uff0c\u4ece\u800c\u76f4\u63a5\u590d\u7528\u539f\u7b56\u7565\u3002\u5b9e\u9a8c\u5728MiniGrid\u548cMuJoCo\u4efb\u52a1\u4e0a\u8fdb\u884c\u3002", "result": "MAGIK\u5728\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u6837\u672c\u4e0b\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MAGIK\u901a\u8fc7\u60f3\u8c61\u529b\u7c7b\u6bd4\u6620\u5c04\u63d0\u4f9b\u4e86\u65b0\u9896\u4e14\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u673a\u5236\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60,\u77e5\u8bc6\u8fc1\u79fb,\u96f6\u6837\u672c\u8fc1\u79fb,\u7c7b\u6bd4\u63a8\u7406,MAGIK"}}
{"id": "2506.00642", "pdf": "https://arxiv.org/pdf/2506.00642", "abs": "https://arxiv.org/abs/2506.00642", "authors": ["Yuliang Ji", "Jian Wu", "Yuanzhe Xi"], "title": "Rethinking Neural-based Matrix Inversion: Why can't, and Where can", "categories": ["cs.LG"], "comment": null, "summary": "Deep neural networks have achieved substantial success across various\nscientific computing tasks. A pivotal challenge within this domain is the rapid\nand parallel approximation of matrix inverses, critical for numerous\napplications. Despite significant progress, there currently exists no universal\nneural-based method for approximating matrix inversion. This paper presents a\ntheoretical analysis demonstrating the fundamental limitations of neural\nnetworks in developing a general matrix inversion model. We expand the class of\nLipschitz functions to encompass a wider array of neural network models,\nthereby refining our theoretical approach. Moreover, we delineate specific\nconditions under which neural networks can effectively approximate matrix\ninverses. Our theoretical results are supported by experimental results from\ndiverse matrix datasets, exploring the efficacy of neural networks in\naddressing the matrix inversion challenge.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u795e\u7ecf\u7f51\u7edc\u5728\u8fd1\u4f3c\u77e9\u9635\u6c42\u9006\u4e2d\u7684\u7406\u8bba\u57fa\u7840\u548c\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u884c\u7684\u8fd1\u4f3c\u65b9\u6cd5\u3002", "motivation": "\u79d1\u5b66\u8ba1\u7b97\u4e2d\u77e9\u9635\u6c42\u9006\u7684\u5feb\u901f\u5e76\u884c\u8fd1\u4f3c\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u901a\u7528\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6269\u5c55Lipschitz\u51fd\u6570\u7c7b\u6765\u6db5\u76d6\u66f4\u591a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5e76\u5206\u6790\u7279\u5b9a\u6761\u4ef6\u4e0b\u7684\u8fd1\u4f3c\u80fd\u529b\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u795e\u7ecf\u7f51\u7edc\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u4ee5\u6709\u6548\u8fd1\u4f3c\u77e9\u9635\u6c42\u9006\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u5728\u77e9\u9635\u6c42\u9006\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5b9e\u73b0\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc, \u77e9\u9635\u6c42\u9006, Lipschitz\u51fd\u6570, \u79d1\u5b66\u8ba1\u7b97"}}
{"id": "2506.00658", "pdf": "https://arxiv.org/pdf/2506.00658", "abs": "https://arxiv.org/abs/2506.00658", "authors": ["Lang Xiong", "Raina Gao", "Alyssa Jeong", "Yicheng Fu", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Sarcasm is a form of humor where expressions convey meanings opposite to\ntheir literal interpretations. Classifying and generating sarcasm using large\nlanguage models is vital for interpreting human communication. Sarcasm poses\nchallenges for computational models, due to its nuanced nature. We introduce\nSarc7, a benchmark that classifies 7 types of sarcasm: self-deprecating,\nbrooding, deadpan, polite, obnoxious, raging, and manic by annotating entries\nof the MUStARD dataset. Classification was evaluated using zero-shot, few-shot,\nchain-of-thought (CoT), and a novel emotion-based prompting technique. We\npropose an emotion-based generation method developed by identifying key\ncomponents of sarcasm-incongruity, shock value, and context dependency. Our\nclassification experiments show that Gemini 2.5, using emotion-based prompting,\noutperforms other setups with an F1 score of 0.3664. Human evaluators preferred\nour emotion-based prompting, with 38.46% more successful generations than\nzero-shot prompting.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Sarc7\u57fa\u51c6\uff0c\u7528\u4e8e\u5206\u7c7b7\u79cd\u8bbd\u523a\u7c7b\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u60c5\u611f\u7684\u751f\u6210\u65b9\u6cd5\u3002Gemini 2.5\u5728\u4f7f\u7528\u60c5\u611f\u63d0\u793a\u65f6\u8868\u73b0\u6700\u4f73\uff0cF1\u5f97\u5206\u4e3a0.3664\u3002", "motivation": "\u8bbd\u523a\u56e0\u5176\u590d\u6742\u6027\u5bf9\u8ba1\u7b97\u6a21\u578b\u6784\u6210\u6311\u6218\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5206\u7c7b\u548c\u751f\u6210\u8bbd\u523a\u6765\u6539\u5584\u5bf9\u4eba\u7c7b\u4ea4\u6d41\u7684\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u6807\u6ce8MUStARD\u6570\u636e\u96c6\uff0c\u91c7\u7528\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u3001\u94fe\u5f0f\u601d\u7ef4\u548c\u60c5\u611f\u63d0\u793a\u6280\u672f\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u60c5\u611f\u7684\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u60c5\u611f\u63d0\u793a\u5728\u5206\u7c7b\u4e2d\u8868\u73b0\u6700\u4f73\uff08F1=0.3664\uff09\uff0c\u4eba\u7c7b\u8bc4\u4f30\u8005\u66f4\u504f\u597d\u60c5\u611f\u63d0\u793a\u751f\u6210\u7684\u5185\u5bb9\u3002", "conclusion": "\u60c5\u611f\u63d0\u793a\u5728\u8bbd\u523a\u5206\u7c7b\u548c\u751f\u6210\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u4e3a\u5904\u7406\u8bbd\u523a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u8bbd\u523a\u5206\u7c7b, \u60c5\u611f\u63d0\u793a, Sarc7, \u8bed\u8a00\u6a21\u578b, \u751f\u6210\u65b9\u6cd5"}}
{"id": "2506.01624", "pdf": "https://arxiv.org/pdf/2506.01624", "abs": "https://arxiv.org/abs/2506.01624", "authors": ["Mustafa Mert \u00c7elikok", "Saptarashmi Bandyopadhyay", "Robert Loftin"], "title": "Social Cooperation in Conversational AI Agents", "categories": ["cs.AI", "cs.LG", "I.2.6; I.2.7"], "comment": "4 pages, RLDM 2025 abstract (Spotlight presentation)", "summary": "The development of AI agents based on large, open-domain language models\n(LLMs) has paved the way for the development of general-purpose AI assistants\nthat can support human in tasks such as writing, coding, graphic design, and\nscientific research. A major challenge with such agents is that, by necessity,\nthey are trained by observing relatively short-term interactions with humans.\nSuch models can fail to generalize to long-term interactions, for example,\ninteractions where a user has repeatedly corrected mistakes on the part of the\nagent. In this work, we argue that these challenges can be overcome by\nexplicitly modeling humans' social intelligence, that is, their ability to\nbuild and maintain long-term relationships with other agents whose behavior\ncannot always be predicted. By mathematically modeling the strategies humans\nuse to communicate and reason about one another over long periods of time, we\nmay be able to derive new game theoretic objectives against which LLMs and\nfuture AI agents may be optimized.", "AI": {"tldr": "\u901a\u8fc7\u5efa\u6a21\u4eba\u7c7b\u793e\u4ea4\u667a\u80fd\u6765\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u4ee5\u89e3\u51b3\u5176\u5728\u957f\u671f\u4e92\u52a8\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLMs\u7684AI\u52a9\u624b\u5728\u957f\u671f\u4e92\u52a8\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u7528\u6237\u591a\u6b21\u7ea0\u6b63\u9519\u8bef\u7684\u60c5\u51b5\u4e0b\uff0c\u65e0\u6cd5\u6709\u6548\u6cdb\u5316\u3002\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u7684\u793e\u4ea4\u667a\u80fd\uff0c\u6709\u671b\u63d0\u5347\u6a21\u578b\u7684\u957f\u671f\u4e92\u52a8\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u6570\u5b66\u5efa\u6a21\u4eba\u7c7b\u5728\u957f\u671f\u4e92\u52a8\u4e2d\u7684\u6c9f\u901a\u548c\u63a8\u7406\u7b56\u7565\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u535a\u5f08\u8bba\u76ee\u6807\uff0c\u7528\u4e8e\u4f18\u5316LLMs\u53ca\u5176\u4ed6AI\u4ee3\u7406\u3002", "result": "\uff08\u6458\u8981\u672a\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u63d0\u51fa\u4e86\u7406\u8bba\u6846\u67b6\u3002\uff09", "conclusion": "\u901a\u8fc7\u5f15\u5165\u793e\u4ea4\u667a\u80fd\u7684\u6570\u5b66\u5efa\u6a21\uff0c\u53ef\u4ee5\u6539\u8fdbLLMs\u5728\u957f\u671f\u4e92\u52a8\u4e2d\u7684\u8868\u73b0\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3001\u793e\u4ea4\u667a\u80fd\u3001\u957f\u671f\u4e92\u52a8\u3001\u535a\u5f08\u8bba\u3001AI\u52a9\u624b"}}
{"id": "2506.00653", "pdf": "https://arxiv.org/pdf/2506.00653", "abs": "https://arxiv.org/abs/2506.00653", "authors": ["Femi Bello", "Anubrata Das", "Fanzhi Zeng", "Fangcong Yin", "Leqi Liu"], "title": "Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "It has been hypothesized that neural networks with similar architectures\ntrained on similar data learn shared representations relevant to the learning\ntask. We build on this idea by extending the conceptual framework where\nrepresentations learned across models trained on the same data can be expressed\nas linear combinations of a \\emph{universal} set of basis features. These basis\nfeatures underlie the learning task itself and remain consistent across models,\nregardless of scale. From this framework, we propose the \\textbf{Linear\nRepresentation Transferability (LRT)} Hypothesis -- that there exists an affine\ntransformation between the representation spaces of different models. To test\nthis hypothesis, we learn affine mappings between the hidden states of models\nof different sizes and evaluate whether steering vectors -- directions in\nhidden state space associated with specific model behaviors -- retain their\nsemantic effect when transferred from small to large language models using the\nlearned mappings. We find strong empirical evidence that such affine mappings\ncan preserve steering behaviors. These findings suggest that representations\nlearned by small models can be used to guide the behavior of large models, and\nthat the LRT hypothesis may be a promising direction on understanding\nrepresentation alignment across model scales.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5047\u8bbe\u4e0d\u540c\u89c4\u6a21\u7684\u795e\u7ecf\u7f51\u7edc\u5728\u76f8\u4f3c\u6570\u636e\u4e0a\u8bad\u7ec3\u540e\uff0c\u5176\u8868\u793a\u7a7a\u95f4\u53ef\u4ee5\u901a\u8fc7\u7ebf\u6027\u53d8\u6362\u5bf9\u9f50\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u5047\u8bbe\uff0c\u8868\u660e\u5c0f\u6a21\u578b\u7684\u8868\u793a\u53ef\u4ee5\u6307\u5bfc\u5927\u6a21\u578b\u7684\u884c\u4e3a\u3002", "motivation": "\u63a2\u7a76\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u5728\u76f8\u4f3c\u6570\u636e\u4e0a\u662f\u5426\u5b66\u4e60\u5230\u5171\u4eab\u7684\u8868\u793a\uff0c\u5e76\u9a8c\u8bc1\u8fd9\u4e9b\u8868\u793a\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u7ebf\u6027\u53d8\u6362\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u8fc1\u79fb\u3002", "method": "\u63d0\u51fa\u7ebf\u6027\u8868\u793a\u53ef\u8fc1\u79fb\u6027\uff08LRT\uff09\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u9690\u85cf\u72b6\u6001\u95f4\u7684\u4eff\u5c04\u6620\u5c04\uff0c\u9a8c\u8bc1\u8fc1\u79fb\u540e\u884c\u4e3a\u4fdd\u6301\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u4eff\u5c04\u6620\u5c04\u53ef\u4ee5\u4fdd\u7559\u5c0f\u6a21\u578b\u5230\u5927\u6a21\u578b\u7684\u884c\u4e3a\u6307\u5bfc\u80fd\u529b\uff0c\u652f\u6301LRT\u5047\u8bbe\u3002", "conclusion": "\u5c0f\u6a21\u578b\u7684\u8868\u793a\u53ef\u4ee5\u7528\u4e8e\u6307\u5bfc\u5927\u6a21\u578b\uff0cLRT\u5047\u8bbe\u4e3a\u8de8\u6a21\u578b\u89c4\u6a21\u8868\u793a\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc,\u8868\u793a\u5b66\u4e60,\u7ebf\u6027\u53d8\u6362,\u6a21\u578b\u8fc1\u79fb,LRT\u5047\u8bbe"}}
{"id": "2506.00668", "pdf": "https://arxiv.org/pdf/2506.00668", "abs": "https://arxiv.org/abs/2506.00668", "authors": ["Martin Kuo", "Jianyi Zhang", "Aolin Ding", "Louis DiValentin", "Amin Hass", "Benjamin F Morris", "Isaac Jacobson", "Randolph Linderman", "James Kiessling", "Nicolas Ramos", "Bhavna Gopal", "Maziyar Baran Pouyan", "Changwei Liu", "Hai Li", "Yiran Chen"], "title": "SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues", "categories": ["cs.CL"], "comment": null, "summary": "Malicious attackers can exploit large language models (LLMs) by engaging them\nin multi-turn dialogues to achieve harmful objectives, posing significant\nsafety risks to society. To address this challenge, we propose a novel defense\nmechanism: SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues\n(STREAM). STREAM defends LLMs against multi-turn attacks while preserving their\nfunctional capabilities. Our approach involves constructing a human-annotated\ndataset, the Safety Reasoning Multi-turn Dialogues dataset, which is used to\nfine-tune a plug-and-play safety reasoning moderator. This model is designed to\nidentify malicious intent hidden within multi-turn conversations and alert the\ntarget LLM of potential risks. We evaluate STREAM across multiple LLMs against\nprevalent multi-turn attack strategies. Experimental results demonstrate that\nour method significantly outperforms existing defense techniques, reducing the\nAttack Success Rate (ASR) by 51.2%, all while maintaining comparable LLM\ncapability.", "AI": {"tldr": "\u63d0\u51fa\u4e86STREAM\u9632\u5fa1\u673a\u5236\uff0c\u901a\u8fc7\u5b89\u5168\u63a8\u7406\u8c03\u8282\u5668\u4fdd\u62a4LLM\u514d\u53d7\u591a\u8f6e\u5bf9\u8bdd\u653b\u51fb\uff0c\u964d\u4f4e51.2%\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u6076\u610f\u653b\u51fb\u8005\u5229\u7528\u591a\u8f6e\u5bf9\u8bdd\u5371\u5bb3LLM\uff0c\u4e9f\u9700\u9632\u5fa1\u673a\u5236\u4fdd\u62a4\u6a21\u578b\u5b89\u5168\u3002", "method": "\u6784\u5efa\u5b89\u5168\u63a8\u7406\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u5fae\u8c03\u89e3\u7801\u5668\u8bc6\u522b\u6076\u610f\u610f\u56fe\u5e76\u8b66\u793aLLM\u3002", "result": "STREAM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u6280\u672f\uff0c\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e51.2%\uff0c\u540c\u65f6\u4fdd\u6301LLM\u529f\u80fd\u3002", "conclusion": "STREAM\u662f\u4e00\u79cd\u6709\u6548\u7684\u591a\u8f6e\u653b\u51fb\u9632\u5fa1\u65b9\u6cd5\uff0c\u517c\u987e\u5b89\u5168\u6027\u4e0e\u529f\u80fd\u6027\u3002", "keywords": "LLM\u5b89\u5168\u3001\u591a\u8f6e\u653b\u51fb\u3001\u9632\u5fa1\u673a\u5236\u3001\u5b89\u5168\u63a8\u7406\u3001\u6570\u636e\u96c6"}}
{"id": "2506.01676", "pdf": "https://arxiv.org/pdf/2506.01676", "abs": "https://arxiv.org/abs/2506.01676", "authors": ["Chong Li", "Chenglin Zhu", "Tao Zhang", "Mingan Lin", "Zenan Zhou", "Jian Xie"], "title": "K12Vista: Exploring the Boundaries of MLLMs in K-12 Education", "categories": ["cs.AI"], "comment": null, "summary": "Multimodal large language models have demonstrated remarkable reasoning\ncapabilities in various visual tasks. However, their abilities in K12 scenarios\nare still systematically underexplored. Previous studies suffer from various\nlimitations including narrow subject coverage, insufficient data scale, lack of\ndiversity in question types, and naive answer-centric evaluation method,\nresulting in insufficient exploration of model capabilities. To address these\ngaps, we propose K12Vista, the most comprehensive multimodal benchmark for\nChinese K12 subject knowledge understanding and reasoning to date, featuring\n33,000 questions across five core subjects from primary to high school and\nthree question types. Moreover, beyond the final outcome, we are also concerned\nwith the correctness of MLLMs' reasoning processes. For this purpose, we\nmeticulously compiles errors from MLLMs' reasoning processes and leverage an\nautomated data pipeline to construct K12-PEM-800K, the largest process\nevaluation dataset offering detailed step-by-step judgement annotations for\nMLLMs' reasoning. Subsequently, we developed K12-PEM, an advanced process\nevaluation model that integrates an overall assessment of both the reasoning\nprocess and answer correctness. Moreover, we also introduce K12-PEBench, the\nfirst high-quality, human-annotated benchmark specifically designed for\nevaluating abilities of reasoning process evaluation.Extensive experiments\nreveal that current MLLMs exhibit significant flaws when reasoning within\nK12Vista, providing critical insights for the development of more capable\nMLLMs.We open our resources at https://github.com/lichongod/K12Vista.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faK12Vista\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u4e2d\u6587K12\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b33,000\u9053\u9898\u76ee\uff0c\u6db5\u76d6\u4e94\u95e8\u6838\u5fc3\u5b66\u79d1\u548c\u4e09\u79cd\u9898\u578b\uff0c\u5e76\u6784\u5efa\u4e86K12-PEM-800K\u6570\u636e\u96c6\u548cK12-PEM\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728K12\u573a\u666f\u4e2d\u53d7\u9650\uff0c\u5305\u62ec\u5b66\u79d1\u8986\u76d6\u7a84\u3001\u6570\u636e\u89c4\u6a21\u4e0d\u8db3\u3001\u95ee\u9898\u7c7b\u578b\u5355\u4e00\u548c\u8bc4\u4f30\u65b9\u6cd5\u7b80\u5355\uff0c\u672a\u80fd\u5145\u5206\u6316\u6398\u6a21\u578b\u6f5c\u529b\u3002", "method": "\u63d0\u51faK12Vista\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6784\u5efaK12-PEM-800K\u6570\u636e\u96c6\u548cK12-PEM\u6a21\u578b\uff0c\u8bc4\u4f30\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u548c\u7b54\u6848\u6b63\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728K12Vista\u4e2d\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u6a21\u578b\u63d0\u4f9b\u5173\u952e\u6d1e\u89c1\u3002", "conclusion": "K12Vista\u548c\u76f8\u5173\u8d44\u6e90\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728K12\u573a\u666f\u4e2d\u7684\u7814\u7a76\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff1bK12\u6559\u80b2\uff1b\u57fa\u51c6\u6d4b\u8bd5\uff1b\u63a8\u7406\u8bc4\u4f30"}}
{"id": "2506.00656", "pdf": "https://arxiv.org/pdf/2506.00656", "abs": "https://arxiv.org/abs/2506.00656", "authors": ["Aris J. Aristorenas"], "title": "Permutation-Invariant Transformer Neural Architectures for Set-Based Indoor Localization Using Learned RSSI Embeddings", "categories": ["cs.LG", "cs.AI"], "comment": "16 pages, 1 figure", "summary": "We propose a permutation-invariant neural architecture for indoor\nlocalization using RSSI scans from Wi-Fi access points. Each scan is modeled as\nan unordered set of (BSSID, RSSI) pairs, where BSSIDs are mapped to learned\nembeddings and concatenated with signal strength. These are processed by a Set\nTransformer, enabling the model to handle variable-length, sparse inputs while\nlearning attention-based representations over access point relationships. We\nevaluate the model on a dataset collected across a campus environment\nconsisting of six buildings. Results show that the model accurately recovers\nfine-grained spatial structure and maintains performance across physically\ndistinct domains. In our experiments, a simple LSTM consistently outperformed\nall other models, achieving the lowest mean localization error across three\ntasks (E1 - E3), with average errors as low as 2.23 m. The Set Transformer\nperformed competitively, ranking second in every experiment and outperforming\nthe MLP, RNN, and basic attention models, particularly in scenarios involving\nmultiple buildings (E2) and multiple floors (E3). Performance degraded most in\nE2, where signal conditions varied substantially across buildings, highlighting\nthe importance of architectural robustness to domain diversity. This work\ndemonstrates that set-based neural models are a natural fit for signal-based\nlocalization, offering a principled approach to handling sparse, unordered\ninputs in real-world positioning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eWi-Fi\u4fe1\u53f7\u5b9a\u4f4d\u7684\u6392\u5217\u4e0d\u53d8\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7Set Transformer\u5904\u7406\u65e0\u5e8f\u4fe1\u53f7\u8f93\u5165\uff0c\u5728\u591a\u5efa\u7b51\u548c\u591a\u697c\u5c42\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3Wi-Fi\u4fe1\u53f7\u5b9a\u4f4d\u4e2d\u4fe1\u53f7\u7a00\u758f\u3001\u65e0\u5e8f\u4e14\u957f\u5ea6\u53ef\u53d8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u5408\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002", "method": "\u4f7f\u7528Set Transformer\u5904\u7406(BSSID, RSSI)\u5bf9\u7684\u65e0\u5e8f\u96c6\u5408\uff0c\u5b66\u4e60\u4fe1\u53f7\u95f4\u7684\u6ce8\u610f\u529b\u5173\u7cfb\u3002", "result": "\u6a21\u578b\u5728\u6821\u56ed\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0cLSTM\u8868\u73b0\u6700\u4f73\uff0cSet Transformer\u6b21\u4e4b\uff0c\u4f18\u4e8eMLP\u3001RNN\u548c\u57fa\u7840\u6ce8\u610f\u529b\u6a21\u578b\u3002", "conclusion": "\u96c6\u5408\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u9002\u7528\u4e8e\u4fe1\u53f7\u5b9a\u4f4d\u4efb\u52a1\uff0c\u80fd\u6709\u6548\u5904\u7406\u7a00\u758f\u65e0\u5e8f\u8f93\u5165\u3002", "keywords": "\u5ba4\u5185\u5b9a\u4f4d, Wi-Fi RSSI, Set Transformer, \u6392\u5217\u4e0d\u53d8\u6027, \u4fe1\u53f7\u5904\u7406"}}
{"id": "2506.00671", "pdf": "https://arxiv.org/pdf/2506.00671", "abs": "https://arxiv.org/abs/2506.00671", "authors": ["Yuelyu Ji", "Hang Zhang", "Shiven Verma", "Hui Ji", "Chun Li", "Yushui Han", "Yanshan Wang"], "title": "DeepRAG: Integrating Hierarchical Reasoning and Process Supervision for Biomedical Multi-Hop QA", "categories": ["cs.CL"], "comment": null, "summary": "We propose DeepRAG, a novel framework that integrates DeepSeek hierarchical\nquestion decomposition capabilities with RAG Gym unified retrieval-augmented\ngeneration optimization using process level supervision. Targeting the\nchallenging MedHopQA biomedical question answering task, DeepRAG systematically\ndecomposes complex queries into precise sub-queries and employs concept level\nreward signals informed by the UMLS ontology to enhance biomedical accuracy.\nPreliminary evaluations on the MedHopQA dataset indicate that DeepRAG\nsignificantly outperforms baseline models, including standalone DeepSeek and\nRAG Gym, achieving notable improvements in both Exact Match and concept level\naccuracy.", "AI": {"tldr": "DeepRAG\u662f\u4e00\u79cd\u65b0\u9896\u6846\u67b6\uff0c\u7ed3\u5408\u4e86DeepSeek\u5c42\u6b21\u5316\u95ee\u9898\u5206\u89e3\u80fd\u529b\u4e0eRAG Gym\u7edf\u4e00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4f18\u5316\uff0c\u901a\u8fc7\u8fc7\u7a0b\u7ea7\u76d1\u7763\u63d0\u5347\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u9488\u5bf9\u533b\u5b66\u95ee\u7b54\u4efb\u52a1MedHopQA\u7684\u590d\u6742\u6027\u6311\u6218\uff0c\u63d0\u51fa\u9700\u8981\u7cfb\u7edf\u5206\u89e3\u590d\u6742\u95ee\u9898\u5e76\u5229\u7528UMLS\u672c\u4f53\u589e\u5f3a\u6982\u5ff5\u7ea7\u7cbe\u51c6\u5ea6\u3002", "method": "\u6846\u67b6\u6574\u5408DeepSeek\u7684\u95ee\u9898\u5206\u89e3\u80fd\u529b\u548cRAG Gym\u7684\u68c0\u7d22\u751f\u6210\u4f18\u5316\uff0c\u901a\u8fc7UMLS\u63d0\u4f9b\u7684\u6982\u5ff5\u7ea7\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u76d1\u7763\u3002", "result": "\u5728MedHopQA\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86Exact Match\u548c\u6982\u5ff5\u7ea7\u51c6\u786e\u6027\u3002", "conclusion": "DeepRAG\u5728\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "keywords": "DeepRAG,DeepSeek,RAG Gym,MedHopQA,UMLS"}}
{"id": "2506.01683", "pdf": "https://arxiv.org/pdf/2506.01683", "abs": "https://arxiv.org/abs/2506.01683", "authors": ["Chanwoo Park", "Anna Seo Gyeong Choi", "Sunghye Cho", "Chanwoo Kim"], "title": "Reasoning-Based Approach with Chain-of-Thought for Alzheimer's Detection Using Speech and Large Language Models", "categories": ["cs.AI"], "comment": "Accepted to INTERSPEECH 2025", "summary": "Societies worldwide are rapidly entering a super-aged era, making elderly\nhealth a pressing concern. The aging population is increasing the burden on\nnational economies and households. Dementia cases are rising significantly with\nthis demographic shift. Recent research using voice-based models and large\nlanguage models (LLM) offers new possibilities for dementia diagnosis and\ntreatment. Our Chain-of-Thought (CoT) reasoning method combines speech and\nlanguage models. The process starts with automatic speech recognition to\nconvert speech to text. We add a linear layer to an LLM for Alzheimer's disease\n(AD) and non-AD classification, using supervised fine-tuning (SFT) with CoT\nreasoning and cues. This approach showed an 16.7% relative performance\nimprovement compared to methods without CoT prompt reasoning. To the best of\nour knowledge, our proposed method achieved state-of-the-art performance in CoT\napproaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u97f3\u548c\u8bed\u8a00\u6a21\u578b\u7684Chain-of-Thought (CoT)\u63a8\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u8001\u5e74\u75f4\u5446\u75c7\uff08AD\uff09\u7684\u8bca\u65ad\u548c\u5206\u7c7b\uff0c\u76f8\u8f83\u4e8e\u65e0CoT\u63d0\u793a\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u4e8616.7%\u3002", "motivation": "\u968f\u7740\u5168\u7403\u793e\u4f1a\u8fdb\u5165\u8d85\u7ea7\u8001\u9f84\u5316\u65f6\u4ee3\uff0c\u8001\u5e74\u5065\u5eb7\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u8001\u5e74\u75f4\u5446\u75c7\u75c5\u4f8b\u663e\u8457\u589e\u52a0\u3002\u4f20\u7edf\u8bca\u65ad\u65b9\u6cd5\u6548\u7387\u4e0d\u8db3\uff0c\u8bed\u97f3\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e3a\u75f4\u5446\u75c7\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u91c7\u7528Chain-of-Thought (CoT)\u63a8\u7406\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bed\u97f3\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\u3002\u9996\u5148\u901a\u8fc7\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u5c06\u8bed\u97f3\u8f6c\u6362\u4e3a\u6587\u672c\uff0c\u7136\u540e\u5728LLM\u4e0a\u6dfb\u52a0\u7ebf\u6027\u5c42\uff0c\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548cCoT\u63a8\u7406\u8fdb\u884cAD\u548c\u975eAD\u5206\u7c7b\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728CoT\u63a8\u7406\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u6027\u80fd\u76f8\u8f83\u4e8e\u65e0CoT\u63d0\u793a\u63a8\u7406\u7684\u65b9\u6cd5\u63d0\u5347\u4e8616.7%\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\uff0c\u7ed3\u5408\u8bed\u97f3\u548c\u8bed\u8a00\u6a21\u578b\u7684CoT\u65b9\u6cd5\u5728\u75f4\u5446\u75c7\u5206\u7c7b\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u75f4\u5446\u75c7\u7684\u65e9\u671f\u8bca\u65ad\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002", "keywords": "\u8001\u5e74\u75f4\u5446\u75c7\u3001Chain-of-Thought\u63a8\u7406\u3001\u8bed\u97f3\u6a21\u578b\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u76d1\u7763\u5fae\u8c03"}}
{"id": "2506.00660", "pdf": "https://arxiv.org/pdf/2506.00660", "abs": "https://arxiv.org/abs/2506.00660", "authors": ["Marziyeh Mohammadi", "Mohsen Vejdanihemmat", "Mahshad Lotfinia", "Mirabela Rusu", "Daniel Truhn", "Andreas Maier", "Soroosh Tayebi Arasteh"], "title": "Differential Privacy for Deep Learning in Medicine", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Differential privacy (DP) is a key technique for protecting sensitive patient\ndata in medical deep learning (DL). As clinical models grow more\ndata-dependent, balancing privacy with utility and fairness has become a\ncritical challenge. This scoping review synthesizes recent developments in\napplying DP to medical DL, with a particular focus on DP-SGD and alternative\nmechanisms across centralized and federated settings. Using a structured search\nstrategy, we identified 74 studies published up to March 2025. Our analysis\nspans diverse data modalities, training setups, and downstream tasks, and\nhighlights the tradeoffs between privacy guarantees, model accuracy, and\nsubgroup fairness. We find that while DP-especially at strong privacy\nbudgets-can preserve performance in well-structured imaging tasks, severe\ndegradation often occurs under strict privacy, particularly in underrepresented\nor complex modalities. Furthermore, privacy-induced performance gaps\ndisproportionately affect demographic subgroups, with fairness impacts varying\nby data type and task. A small subset of studies explicitly addresses these\ntradeoffs through subgroup analysis or fairness metrics, but most omit them\nentirely. Beyond DP-SGD, emerging approaches leverage alternative mechanisms,\ngenerative models, and hybrid federated designs, though reporting remains\ninconsistent. We conclude by outlining key gaps in fairness auditing,\nstandardization, and evaluation protocols, offering guidance for future work\ntoward equitable and clinically robust privacy-preserving DL systems in\nmedicine.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u5728\u533b\u5b66\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8DP-SGD\u53ca\u5176\u66ff\u4ee3\u673a\u5236\uff0c\u5206\u6790\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6027\u80fd\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u968f\u7740\u533b\u5b66\u6df1\u5ea6\u5b66\u4e60\u5bf9\u6570\u636e\u7684\u4f9d\u8d56\u6027\u589e\u5f3a\uff0c\u5982\u4f55\u5728\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u7684\u540c\u65f6\u786e\u4fdd\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u516c\u5e73\u6027\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u641c\u7d22\u7b56\u7565\uff0c\u5206\u6790\u4e8674\u9879\u7814\u7a76\uff0c\u6db5\u76d6\u4e0d\u540c\u6570\u636e\u6a21\u5f0f\u3001\u8bad\u7ec3\u8bbe\u7f6e\u548c\u4e0b\u6e38\u4efb\u52a1\uff0c\u63a2\u8ba8DP-SGD\u53ca\u5176\u4ed6\u673a\u5236\u5728\u96c6\u4e2d\u5f0f\u548c\u8054\u90a6\u5f0f\u8bbe\u7f6e\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f3a\u9690\u79c1\u9884\u7b97\u4e0b\u7684DP\u5728\u7ed3\u6784\u5316\u6210\u50cf\u4efb\u52a1\u4e2d\u6027\u80fd\u5c1a\u53ef\uff0c\u4f46\u4e25\u683c\u9690\u79c1\u4e0b\u6027\u80fd\u4e0b\u964d\u660e\u663e\uff0c\u4e14\u5bf9\u7279\u5b9a\u4eba\u53e3\u4e9a\u7ec4\u516c\u5e73\u6027\u5f71\u54cd\u663e\u8457\u3002\u5c11\u6570\u7814\u7a76\u901a\u8fc7\u4e9a\u7ec4\u5206\u6790\u6216\u516c\u5e73\u6027\u6307\u6807\u5904\u7406\u8fd9\u4e9b\u6743\u8861\uff0c\u4f46\u591a\u6570\u7814\u7a76\u672a\u6d89\u53ca\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u9700\u8981\u5728\u516c\u5e73\u6027\u5ba1\u8ba1\u3001\u6807\u51c6\u5316\u548c\u8bc4\u4f30\u534f\u8bae\u65b9\u9762\u586b\u8865\u7a7a\u767d\uff0c\u4ee5\u5b9e\u73b0\u533b\u5b66\u4e2d\u516c\u5e73\u4e14\u7a33\u5065\u7684\u9690\u79c1\u4fdd\u62a4\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u3002", "keywords": "\u5dee\u5206\u9690\u79c1, \u533b\u5b66\u6df1\u5ea6\u5b66\u4e60, \u9690\u79c1-\u6548\u7528\u6743\u8861, \u516c\u5e73\u6027, \u8054\u90a6\u5b66\u4e60"}}
{"id": "2506.00694", "pdf": "https://arxiv.org/pdf/2506.00694", "abs": "https://arxiv.org/abs/2506.00694", "authors": ["Li Zhang", "Morgan Gray", "Jaromir Savelka", "Kevin D. Ashley"], "title": "Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50"], "comment": "11 pages, 7th Workshop on Automated Semantic Analysis of Information\n  in Legal Text, 16 June 2025, Chicago, IL", "summary": "Large Language Models (LLMs) demonstrate potential in complex legal tasks\nlike argument generation, yet their reliability remains a concern. Building\nupon pilot work assessing LLM generation of 3-ply legal arguments using human\nevaluation, this paper introduces an automated pipeline to evaluate LLM\nperformance on this task, specifically focusing on faithfulness (absence of\nhallucination), factor utilization, and appropriate abstention. We define\nhallucination as the generation of factors not present in the input case\nmaterials and abstention as the model's ability to refrain from generating\narguments when instructed and no factual basis exists. Our automated method\nemploys an external LLM to extract factors from generated arguments and\ncompares them against the ground-truth factors provided in the input case\ntriples (current case and two precedent cases). We evaluated eight distinct\nLLMs on three tests of increasing difficulty: 1) generating a standard 3-ply\nargument, 2) generating an argument with swapped precedent roles, and 3)\nrecognizing the impossibility of argument generation due to lack of shared\nfactors and abstaining. Our findings indicate that while current LLMs achieve\nhigh accuracy (over 90%) in avoiding hallucination on viable argument\ngeneration tests (Tests 1 & 2), they often fail to utilize the full set of\nrelevant factors present in the cases. Critically, on the abstention test (Test\n3), most models failed to follow instructions to stop, instead generating\nspurious arguments despite the lack of common factors. This automated pipeline\nprovides a scalable method for assessing these crucial LLM behaviors,\nhighlighting the need for improvements in factor utilization and robust\nabstention capabilities before reliable deployment in legal settings. Project\npage:\nhttps://github.com/lizhang-AIandLaw/Measuring-Faithfulness-and-Abstention.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u751f\u6210\u6cd5\u5f8b\u8bba\u636e\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u5e7b\u89c9\u3001\u56e0\u7d20\u5229\u7528\u548c\u9002\u65f6\u5f03\u6743\uff0c\u7ed3\u679c\u8868\u660eLLMs\u5728\u907f\u514d\u5e7b\u89c9\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u56e0\u7d20\u5229\u7528\u548c\u5f03\u6743\u80fd\u529b\u4e0a\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u8bc4\u4f30\u548c\u63d0\u5347LLMs\u5728\u590d\u6742\u6cd5\u5f8b\u4efb\u52a1\uff08\u5982\u8bba\u636e\u751f\u6210\uff09\u4e2d\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u8bbe\u8ba1\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u5229\u7528\u5916\u90e8LLM\u63d0\u53d6\u751f\u6210\u8bba\u636e\u4e2d\u7684\u56e0\u7d20\u5e76\u4e0e\u8f93\u5165\u6848\u4f8b\u7684\u771f\u5b9e\u56e0\u7d20\u5bf9\u6bd4\uff0c\u6d4b\u8bd58\u79cdLLM\u5728\u4e09\u79cd\u96be\u5ea6\u9012\u589e\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLMs\u5728\u907f\u514d\u5e7b\u89c9\u4e0a\u8868\u73b0\u4f18\u5f02\uff08>90%\u51c6\u786e\u7387\uff09\uff0c\u4f46\u672a\u80fd\u5145\u5206\u5229\u7528\u6848\u4f8b\u4e2d\u7684\u5168\u90e8\u76f8\u5173\u56e0\u7d20\uff0c\u4e14\u591a\u6570\u6a21\u578b\u5728\u65e0\u5171\u4eab\u56e0\u7d20\u65f6\u65e0\u6cd5\u9002\u65f6\u5f03\u6743\u3002", "conclusion": "\u5f53\u524dLLMs\u5728\u6cd5\u5f8b\u4efb\u52a1\u4e2d\u7684\u56e0\u7d20\u5229\u7528\u548c\u5f03\u6743\u80fd\u529b\u4ecd\u9700\u6539\u8fdb\uff0c\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u4e3a\u672a\u6765\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9014\u5f84\u3002", "keywords": "Large Language Models, legal argumentation, hallucination, factor utilization, abstention"}}
{"id": "2506.01689", "pdf": "https://arxiv.org/pdf/2506.01689", "abs": "https://arxiv.org/abs/2506.01689", "authors": ["Shuting Wang", "Yunqi Liu", "Zixin Yang", "Ning Hu", "Zhicheng Dou", "Chenyan Xiong"], "title": "Respond Beyond Language: A Benchmark for Video Generation in Response to Realistic User Intents", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Querying generative AI models, e.g., large language models (LLMs), has become\na prevalent method for information acquisition. However, existing query-answer\ndatasets primarily focus on textual responses, making it challenging to address\ncomplex user queries that require visual demonstrations or explanations for\nbetter understanding. To bridge this gap, we construct a benchmark,\nRealVideoQuest, designed to evaluate the abilities of text-to-video (T2V)\nmodels in answering real-world, visually grounded queries. It identifies 7.5K\nreal user queries with video response intents from Chatbot-Arena and builds\n4.5K high-quality query-video pairs through a multistage video retrieval and\nrefinement process. We further develop a multi-angle evaluation system to\nassess the quality of generated video answers. Experiments indicate that\ncurrent T2V models struggle with effectively addressing real user queries,\npointing to key challenges and future research opportunities in multimodal AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aRealVideoQuest\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u6a21\u578b\u5728\u56de\u7b54\u9700\u8981\u89c6\u89c9\u6f14\u793a\u7684\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u65f6\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u67e5\u8be2-\u7b54\u6848\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u54cd\u5e94\uff0c\u96be\u4ee5\u6ee1\u8db3\u9700\u8981\u89c6\u89c9\u6f14\u793a\u7684\u590d\u6742\u7528\u6237\u67e5\u8be2\u3002", "method": "\u901a\u8fc7\u591a\u9636\u6bb5\u89c6\u9891\u68c0\u7d22\u548c\u7cbe\u70bc\u8fc7\u7a0b\u6784\u5efa\u4e864.5K\u9ad8\u8d28\u91cf\u67e5\u8be2-\u89c6\u9891\u5bf9\uff0c\u5e76\u5f00\u53d1\u4e86\u591a\u89d2\u5ea6\u8bc4\u4f30\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524dT2V\u6a21\u578b\u5728\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u4e2d\u8868\u73b0\u6b20\u4f73\u3002", "conclusion": "\u6307\u51fa\u4e86\u591a\u6a21\u6001AI\u7684\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "keywords": "\u751f\u6210\u5f0fAI\uff0c\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\uff0c\u89c6\u89c9\u57fa\u51c6\uff0c\u591a\u6a21\u6001AI"}}
{"id": "2506.00676", "pdf": "https://arxiv.org/pdf/2506.00676", "abs": "https://arxiv.org/abs/2506.00676", "authors": ["Saad Hossain", "Samanvay Vajpayee", "Sirisha Rambhatla"], "title": "SafeTuneBed: A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become ubiquitous, parameter-efficient\nfine-tuning methods and safety-first defenses have proliferated rapidly.\nHowever, the number of approaches and their recent increase have resulted in\ndiverse evaluations-varied datasets, metrics, and inconsistent threat\nsettings-making it difficult to fairly compare safety, utility, and robustness\nacross methods. To address this, we introduce SafeTuneBed, a benchmark and\ntoolkit unifying fine-tuning and defense evaluation. SafeTuneBed (i) curates a\ndiverse repository of multiple fine-tuning datasets spanning sentiment\nanalysis, question-answering, multi-step reasoning, and open-ended instruction\ntasks, and allows for the generation of harmful-variant splits; (ii) enables\nintegration of state-of-the-art defenses, including alignment-stage\nimmunization, in-training safeguards, and post-tuning repair; and (iii)\nprovides evaluators for safety (attack success rate, refusal consistency) and\nutility. Built on Python-first, dataclass-driven configs and plugins,\nSafeTuneBed requires minimal additional code to specify any fine-tuning regime,\ndefense method, and metric suite, while ensuring end-to-end reproducibility. We\nshowcase its value by benchmarking representative defenses across varied\npoisoning scenarios and tasks. By standardizing data, code, and metrics,\nSafeTuneBed is the first focused toolkit of its kind to accelerate rigorous and\ncomparable research in safe LLM fine-tuning. Code is available at:\nhttps://github.com/criticalml-uw/SafeTuneBed", "AI": {"tldr": "SafeTuneBed\u662f\u4e00\u4e2a\u7528\u4e8e\u7edf\u4e00\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5fae\u8c03\u548c\u9632\u5fa1\u65b9\u6cd5\u7684\u57fa\u51c6\u4e0e\u5de5\u5177\u5305\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u7814\u7a76\u4e2d\u6570\u636e\u3001\u6307\u6807\u548c\u5a01\u80c1\u8bbe\u7f6e\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u5f53\u524dLLM\u5fae\u8c03\u548c\u5b89\u5168\u9632\u5fa1\u65b9\u6cd5\u7684\u591a\u6837\u6027\u548c\u8bc4\u4f30\u4e0d\u4e00\u81f4\u6027\uff0c\u96be\u4ee5\u516c\u5e73\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u7684\u5b89\u5168\u6027\u3001\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\uff0c\u56e0\u6b64\u9700\u8981\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "SafeTuneBed\u901a\u8fc7\uff08i\uff09\u6574\u5408\u591a\u6837\u5316\u7684\u5fae\u8c03\u6570\u636e\u96c6\uff0c\uff08ii\uff09\u652f\u6301\u96c6\u6210\u5148\u8fdb\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\uff08iii\uff09\u63d0\u4f9b\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u91c7\u7528Python\u9a71\u52a8\u7684\u914d\u7f6e\u548c\u63d2\u4ef6\u8bbe\u8ba1\uff0c\u786e\u4fdd\u53ef\u590d\u73b0\u6027\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30\u4ee3\u8868\u6027\u9632\u5fa1\u65b9\u6cd5\u5728\u4e0d\u540c\u4e2d\u6bd2\u573a\u666f\u548c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c55\u793a\u4e86SafeTuneBed\u7684\u4ef7\u503c\u3002", "conclusion": "SafeTuneBed\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u6807\u51c6\u5316\u6570\u636e\u3001\u4ee3\u7801\u548c\u6307\u6807\u7684\u5de5\u5177\u5305\uff0c\u4e3a\u5b89\u5168LLM\u5fae\u8c03\u7814\u7a76\u63d0\u4f9b\u4e86\u4e25\u8c28\u4e14\u53ef\u6bd4\u8f83\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5fae\u8c03, \u5b89\u5168\u9632\u5fa1, \u57fa\u51c6\u6d4b\u8bd5, \u53ef\u590d\u73b0\u6027"}}
{"id": "2506.00713", "pdf": "https://arxiv.org/pdf/2506.00713", "abs": "https://arxiv.org/abs/2506.00713", "authors": ["Debarati Bhattacharjee", "Ashish Anand"], "title": "From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 7 figures", "summary": "This paper presents a framework to convert argumentative texts into argument\nknowledge graphs (AKG). Starting with basic annotations of argumentative\ncomponents (ACs) and argumentative relations (ARs), we enrich the information\nby constructing a knowledge base (KB) graph with metadata attributes for nodes.\nNext, we use premises and inference rules from the KB to form arguments by\napplying modus ponens. From these arguments, we create an AKG. The nodes and\nedges of the AKG have attributes that capture important argumentative features.\nWe also find missing inference rules by identifying markers. This makes it\npossible to identify undercut attacks that were previously undetectable in\nexisting datasets. The AKG gives a graphical view of the argumentative\nstructure that is easier to understand than theoretical formats. It also\nprepares the ground for future reasoning tasks, including checking the\ncoherence of arguments and identifying opportunities for revision. For this, it\nis important to find indirect relations, many of which are implicit. Our\nproposed AKG format, with annotated inference rules and modus ponens, will help\nreasoning models learn the implicit indirect relations that require inference\nover arguments and the relations between them.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u8bae\u8bba\u6587\u672c\u8f6c\u6362\u4e3a\u8bba\u8bc1\u77e5\u8bc6\u56fe\uff08AKG\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u77e5\u8bc6\u5e93\u548c\u63a8\u7406\u89c4\u5219\uff0c\u751f\u6210\u5177\u6709\u4e30\u5bcc\u5c5e\u6027\u7684AKG\uff0c\u4ee5\u589e\u5f3a\u5bf9\u8bba\u8bc1\u7ed3\u6784\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u8bba\u8bc1\u6570\u636e\u96c6\u7f3a\u4e4f\u5bf9\u9690\u542b\u5173\u7cfb\u7684\u6355\u6349\u80fd\u529b\uff0c\u5c24\u5176\u662f\u95f4\u63a5\u653b\u51fb\u5173\u7cfb\u3002AKG\u65e8\u5728\u901a\u8fc7\u56fe\u5f62\u5316\u8868\u793a\u548c\u63a8\u7406\u89c4\u5219\u7684\u5e94\u7528\uff0c\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "1. \u6807\u6ce8\u8bba\u8bc1\u7ec4\u4ef6\uff08ACs\uff09\u548c\u8bba\u8bc1\u5173\u7cfb\uff08ARs\uff09\uff1b2. \u6784\u5efa\u5e26\u5143\u6570\u636e\u5c5e\u6027\u7684\u77e5\u8bc6\u5e93\u56fe\uff1b3. \u5e94\u7528\u5047\u8a00\u63a8\u7406\uff08modus ponens\uff09\u751f\u6210\u8bba\u8bc1\uff1b4. \u521b\u5efa\u5177\u6709\u4e30\u5bcc\u5c5e\u6027\u7684AKG\uff1b5. \u8bc6\u522b\u7f3a\u5931\u7684\u63a8\u7406\u89c4\u5219\u4ee5\u53d1\u73b0\u95f4\u63a5\u653b\u51fb\u5173\u7cfb\u3002", "result": "\u751f\u6210\u7684AKG\u80fd\u591f\u6355\u6349\u9690\u542b\u7684\u95f4\u63a5\u5173\u7cfb\uff0c\u5c24\u5176\u662f\u672a\u68c0\u6d4b\u5230\u7684\u653b\u51fb\u5173\u7cfb\uff0c\u540c\u65f6\u4e3a\u672a\u6765\u7684\u63a8\u7406\u4efb\u52a1\uff08\u5982\u8bba\u8bc1\u4e00\u81f4\u6027\u548c\u4fee\u8ba2\u673a\u4f1a\uff09\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "AKG\u901a\u8fc7\u56fe\u5f62\u5316\u8868\u793a\u548c\u63a8\u7406\u89c4\u5219\u7684\u663e\u5f0f\u5316\uff0c\u589e\u5f3a\u4e86\u8bba\u8bc1\u7ed3\u6784\u7684\u53ef\u7406\u89e3\u6027\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7684\u8bba\u8bc1\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "keywords": "\u8bba\u8bc1\u77e5\u8bc6\u56fe\uff08AKG\uff09\u3001\u5047\u8a00\u63a8\u7406\u3001\u77e5\u8bc6\u5e93\u3001\u63a8\u7406\u89c4\u5219\u3001\u95f4\u63a5\u5173\u7cfb"}}
{"id": "2506.01692", "pdf": "https://arxiv.org/pdf/2506.01692", "abs": "https://arxiv.org/abs/2506.01692", "authors": ["Sylee Dandekar", "Shripad Deshmukh", "Frank Chiu", "W. Bradley Knox", "Scott Niekum"], "title": "A Descriptive and Normative Theory of Human Beliefs in RLHF", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Human preferences in RLHF are typically modeled as a function of the human's\nreward function or corresponding optimal state-action values. In this work, we\npropose that human beliefs about the capabilities of the agent being trained\nalso play a key role in preference generation. We examine two questions related\nto this hypothesis, one descriptive and one normative, respectively: Do human\nlabelers' beliefs about agent capabilities affect the preferences that they\nprovide? And what is the ideal set of beliefs about an agent -- and resulting\npreferences -- for humans to have? We propose a new preference model that\nincorporates human beliefs and provide a normative theory that bounds the error\non the final learned policy based on the \\textit{mismatch} between the human's\nbeliefs and an idealized set of beliefs. We then confirm via a human study that\nbeliefs about agent capabilities do, in fact, significantly affect preferences\nand can be influenced through simple interventions. Additionally, we\nempirically show through synthetic experiments that it is often suboptimal for\nhuman preference labelers to assume agent optimality. Collectively, these\nresults theoretically and empirically demonstrate how reducing the mismatch\nbetween human beliefs and agent capabilities can lead to more performant RLHF\nand point toward new best practices for RLHF practitioners.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4eba\u7c7b\u4fe1\u5ff5\u5bf9RLHF\u4e2d\u504f\u597d\u751f\u6210\u7684\u91cd\u8981\u6027\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u8bc1\u660e\u4fe1\u5ff5\u4e0e\u4ee3\u7406\u80fd\u529b\u5339\u914d\u5bf9\u63d0\u5347\u6027\u80fd\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u5bf9\u4ee3\u7406\u80fd\u529b\u7684\u4fe1\u5ff5\u5982\u4f55\u5f71\u54cd\u5176\u504f\u597d\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u51cf\u5c11\u8fd9\u79cd\u4fe1\u5ff5\u4e0e\u4ee3\u7406\u5b9e\u9645\u80fd\u529b\u7684\u5931\u914d\u6765\u63d0\u5347RLHF\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u4eba\u7c7b\u4fe1\u5ff5\u7684\u65b0\u504f\u597d\u6a21\u578b\uff0c\u63d0\u4f9b\u89c4\u8303\u7406\u8bba\u5e76\u8bbe\u8ba1\u4eba\u7c7b\u7814\u7a76\u548c\u5408\u6210\u5b9e\u9a8c\u9a8c\u8bc1\u5047\u8bbe\u3002", "result": "\u4eba\u7c7b\u4fe1\u5ff5\u663e\u8457\u5f71\u54cd\u504f\u597d\uff0c\u4e14\u901a\u8fc7\u7b80\u5355\u5e72\u9884\u53ef\u8c03\u8282\u3002\u5408\u6210\u5b9e\u9a8c\u8868\u660e\u5047\u8bbe\u4ee3\u7406\u6700\u4f18\u6027\u901a\u5e38\u6b21\u4f18\u3002", "conclusion": "\u51cf\u5c11\u4eba\u7c7b\u4fe1\u5ff5\u4e0e\u4ee3\u7406\u80fd\u529b\u7684\u5931\u914d\u53ef\u63d0\u5347RLHF\u6027\u80fd\uff0c\u4e3a\u5b9e\u8df5\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002", "keywords": "RLHF, human beliefs, preference generation, agent capabilities"}}
{"id": "2506.00688", "pdf": "https://arxiv.org/pdf/2506.00688", "abs": "https://arxiv.org/abs/2506.00688", "authors": ["Zhili Feng", "Yixuan Even Xu", "Alexander Robey", "Robert Kirk", "Xander Davies", "Yarin Gal", "Avi Schwarzschild", "J. Zico Kolter"], "title": "Existing Large Language Model Unlearning Evaluations Are Inconclusive", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Machine unlearning aims to remove sensitive or undesired data from large\nlanguage models. However, recent studies suggest that unlearning is often\nshallow, claiming that removed knowledge can easily be recovered. In this work,\nwe critically examine standard unlearning evaluation practices and uncover key\nlimitations that shake our trust in those findings. First, we show that some\nevaluations introduce substantial new information into the model, potentially\nmasking true unlearning performance by re-teaching the model during testing.\nSecond, we demonstrate that evaluation outcomes vary significantly across\ntasks, undermining the generalizability of current evaluation routines.\nFinally, we find that many evaluations rely on spurious correlations, making\ntheir results difficult to trust and interpret. Taken together, these issues\nsuggest that current evaluation protocols may both overstate and understate\nunlearning success. To address this, we propose two principles for future\nunlearning evaluations: minimal information injection and downstream task\nawareness. We validate these principles through a series of targeted\nexperiments, showing how violations of each can lead to misleading conclusions.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u4e86\u5f53\u524d\u673a\u5668\u5b66\u4e60\u53bb\u5b66\u4e60\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u9879\u6539\u8fdb\u539f\u5219\uff0c\u5f3a\u8c03\u6700\u5c0f\u4fe1\u606f\u6ce8\u5165\u548c\u4efb\u52a1\u610f\u8bc6\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8d28\u7591\u5f53\u524d\u53bb\u5b66\u4e60\u8bc4\u4f30\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u5176\u5b58\u5728\u7684\u95ee\u9898\uff08\u5982\u4fe1\u606f\u6ce8\u5165\u3001\u4efb\u52a1\u5dee\u5f02\u548c\u865a\u5047\u76f8\u5173\u6027\uff09\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e24\u9879\u65b0\u539f\u5219\uff1a\u6700\u5c0f\u4fe1\u606f\u6ce8\u5165\u548c\u4e0b\u6e38\u4efb\u52a1\u610f\u8bc6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u53ef\u80fd\u5938\u5927\u6216\u4f4e\u4f30\u53bb\u5b66\u4e60\u6548\u679c\uff0c\u65b0\u539f\u5219\u80fd\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u53bb\u5b66\u4e60\u6027\u80fd\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u9700\u8981\u6539\u8fdb\u53bb\u5b66\u4e60\u8bc4\u4f30\u65b9\u6cd5\uff0c\u91c7\u7528\u65b0\u539f\u5219\u4ee5\u63d0\u9ad8\u7ed3\u679c\u7684\u53ef\u9760\u6027\u548c\u89e3\u91ca\u6027\u3002", "keywords": "\u673a\u5668\u53bb\u5b66\u4e60, \u8bc4\u4f30\u65b9\u6cd5, \u4fe1\u606f\u6ce8\u5165, \u4efb\u52a1\u610f\u8bc6, \u5b9e\u9a8c\u9a8c\u8bc1"}}
{"id": "2506.00722", "pdf": "https://arxiv.org/pdf/2506.00722", "abs": "https://arxiv.org/abs/2506.00722", "authors": ["Siddhant Arora", "Jinchuan Tian", "Hayato Futami", "Jee-weon Jung", "Jiatong Shi", "Yosuke Kashiwagi", "Emiru Tsunoo", "Shinji Watanabe"], "title": "Chain-of-Thought Training for Open E2E Spoken Dialogue Systems", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Unlike traditional cascaded pipelines, end-to-end (E2E) spoken dialogue\nsystems preserve full differentiability and capture non-phonemic information,\nmaking them well-suited for modeling spoken interactions. However, existing E2E\napproaches often require large-scale training data and generates responses\nlacking semantic coherence. We propose a simple yet effective strategy\nleveraging a chain-of-thought (CoT) formulation, ensuring that training on\nconversational data remains closely aligned with the multimodal language model\n(LM)'s pre-training on speech recognition~(ASR), text-to-speech synthesis\n(TTS), and text LM tasks. Our method achieves over 1.5 ROUGE-1 improvement over\nthe baseline, successfully training spoken dialogue systems on publicly\navailable human-human conversation datasets, while being compute-efficient\nenough to train on just 300 hours of public human-human conversation data, such\nas the Switchboard. We will publicly release our models and training code.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u7684\u7aef\u5230\u7aef\u53e3\u8bed\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u89c4\u6a21\u6570\u636e\u548c\u8bed\u4e49\u4e0d\u8fde\u8d2f\u7684\u95ee\u9898\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u53e3\u8bed\u5bf9\u8bdd\u7cfb\u7edf\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u4e14\u751f\u6210\u54cd\u5e94\u8bed\u4e49\u4e0d\u8fde\u8d2f\uff0c\u56e0\u6b64\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u7b56\u7565\uff0c\u5c06\u5bf9\u8bdd\u6570\u636e\u8bad\u7ec3\u4e0e\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\uff08\u5982ASR\u3001TTS\u548c\u6587\u672cLM\u4efb\u52a1\uff09\u7d27\u5bc6\u7ed3\u5408\u3002", "result": "\u65b9\u6cd5\u5728ROUGE-1\u6307\u6807\u4e0a\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e861.5\uff0c\u4e14\u4ec5\u9700300\u5c0f\u65f6\u7684\u516c\u5f00\u4eba\u7c7b\u5bf9\u8bdd\u6570\u636e\u5373\u53ef\u9ad8\u6548\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u53e3\u8bed\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u8fd8\u964d\u4f4e\u4e86\u6570\u636e\u9700\u6c42\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u7aef\u5230\u7aef\u53e3\u8bed\u5bf9\u8bdd\u7cfb\u7edf, \u94fe\u5f0f\u601d\u7ef4, \u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b, ROUGE-1"}}
{"id": "2506.01704", "pdf": "https://arxiv.org/pdf/2506.01704", "abs": "https://arxiv.org/abs/2506.01704", "authors": ["Jiongnan Liu", "Zhicheng Dou", "Ning Hu", "Chenyan Xiong"], "title": "Generate, Not Recommend: Personalized Multimodal Content Generation", "categories": ["cs.AI"], "comment": null, "summary": "To address the challenge of information overload from massive web contents,\nrecommender systems are widely applied to retrieve and present personalized\nresults for users. However, recommendation tasks are inherently constrained to\nfiltering existing items and lack the ability to generate novel concepts,\nlimiting their capacity to fully satisfy user demands and preferences. In this\npaper, we propose a new paradigm that goes beyond content filtering and\nselecting: directly generating personalized items in a multimodal form, such as\nimages, tailored to individual users. To accomplish this, we leverage\nany-to-any Large Multimodal Models (LMMs) and train them in both supervised\nfine-tuning and online reinforcement learning strategy to equip them with the\nability to yield tailored next items for users. Experiments on two benchmark\ndatasets and user study confirm the efficacy of the proposed method. Notably,\nthe generated images not only align well with users' historical preferences but\nalso exhibit relevance to their potential future interests.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u751f\u6210\u4e2a\u6027\u5316\u5185\u5bb9\uff0c\u7a81\u7834\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u7684\u8fc7\u6ee4\u9650\u5236\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u5c40\u9650\u4e8e\u8fc7\u6ee4\u73b0\u6709\u5185\u5bb9\uff0c\u65e0\u6cd5\u751f\u6210\u65b0\u6982\u5ff5\uff0c\u9650\u5236\u4e86\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u76d1\u7763\u5fae\u8c03\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\uff0c\u76f4\u63a5\u751f\u6210\u4e2a\u6027\u5316\u5185\u5bb9\uff08\u5982\u56fe\u50cf\uff09\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u7528\u6237\u7814\u7a76\u4e2d\uff0c\u751f\u6210\u7684\u56fe\u50cf\u4e0e\u7528\u6237\u5386\u53f2\u504f\u597d\u548c\u6f5c\u5728\u5174\u8da3\u9ad8\u5ea6\u543b\u5408\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u6269\u5c55\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u80fd\u591f\u751f\u6210\u7b26\u5408\u7528\u6237\u4e2a\u6027\u5316\u9700\u6c42\u7684\u5185\u5bb9\u3002", "keywords": "\u63a8\u8350\u7cfb\u7edf,\u591a\u6a21\u6001\u6a21\u578b,\u4e2a\u6027\u5316\u751f\u6210,\u5f3a\u5316\u5b66\u4e60"}}
{"id": "2506.00691", "pdf": "https://arxiv.org/pdf/2506.00691", "abs": "https://arxiv.org/abs/2506.00691", "authors": ["Junaid Muzaffar", "Ahsan Adeel", "Khubaib Ahmed", "Ingo Frommholz", "Zeeshan Pervez", "Ahsan ul Haq"], "title": "Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Training reinforcement learning (RL) agents often requires significant\ncomputational resources and extended training times. To address this, we build\nupon the foundation laid by Google Brain's Sensory Neuron, which introduced a\nnovel neural architecture for reinforcement learning tasks that maintained\npermutation in-variance in the sensory neuron system. While the baseline model\ndemonstrated significant performance improvements over traditional approaches,\nwe identified opportunities to enhance the efficiency of the learning process\nfurther. We propose a modified attention mechanism incorporating a non-linear\ntransformation of the key vectors (K) using a mapping function, resulting in a\nnew set of key vectors (K'). This non-linear mapping enhances the\nrepresentational capacity of the attention mechanism, allowing the model to\nencode more complex feature interactions and accelerating convergence without\ncompromising performance. Our enhanced model demonstrates significant\nimprovements in learning efficiency, showcasing the potential for non-linear\nattention mechanisms in advancing reinforcement learning algorithms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u53d8\u6362\u589e\u5f3a\u952e\u5411\u91cf\u7684\u8868\u793a\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u7684\u5b66\u4e60\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u4e2d\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u3001\u8bad\u7ec3\u65f6\u95f4\u957f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5bf9\u952e\u5411\u91cf\u8fdb\u884c\u975e\u7ebf\u6027\u53d8\u6362\uff0c\u83b7\u5f97\u66f4\u5177\u8868\u73b0\u529b\u7684\u952e\u5411\u91cf\uff0c\u589e\u5f3a\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\u3002", "result": "\u6539\u8fdb\u540e\u7684\u6a21\u578b\u5728\u5b66\u4e60\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u975e\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u5728\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e2d\u5177\u6709\u63d0\u5347\u6548\u7387\u548c\u6027\u80fd\u7684\u6f5c\u529b\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u975e\u7ebf\u6027\u53d8\u6362\u3001\u5b66\u4e60\u6548\u7387"}}
{"id": "2506.00726", "pdf": "https://arxiv.org/pdf/2506.00726", "abs": "https://arxiv.org/abs/2506.00726", "authors": ["Hongye Zheng", "Yichen Wang", "Ray Pan", "Guiran Liu", "Binrong Zhu", "Hanlu Zhang"], "title": "Structured Gradient Guidance for Few-Shot Adaptation in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a gradient-informed fine-tuning method for large language\nmodels under few-shot conditions. The goal is to enhance task adaptability and\ntraining stability when data is limited. The method builds on a base loss\nfunction and introduces two gradient-related regularization terms. The first\nenforces gradient direction consistency to guide parameter updates along\ntask-relevant directions and prevent drift. The second controls gradient\nmagnitude to avoid abnormal updates. Together, these components support a more\nefficient and stable optimization path. To further improve cross-task\ngeneralization, the method incorporates a gradient alignment mechanism. This\nmechanism measures the consistency between optimization directions of the\nsource and target tasks. It enhances fine-tuning performance in multi-task and\ncross-domain scenarios. Across various natural language understanding tasks,\nthe method outperforms existing fine-tuning strategies in average accuracy,\ngradient stability, and directional alignment. Empirical evaluations under\ndifferent sample sizes and domain-specific tasks confirm the method's\nrobustness and broad applicability in low-resource environments. In particular,\nthe method shows clear advantages in controlling parameter update paths. The\nresults demonstrate that a gradient-based fine-tuning framework can effectively\nleverage the representational power of large language models. It ensures\ntraining stability while reducing dependence on large volumes of labeled data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u68af\u5ea6\u611f\u77e5\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u7684\u4efb\u52a1\u9002\u5e94\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\u901a\u8fc7\u68af\u5ea6\u76f8\u5173\u6b63\u5219\u9879\u548c\u68af\u5ea6\u5bf9\u9f50\u673a\u5236\uff0c\u4f18\u5316\u4e86\u8bad\u7ec3\u8def\u5f84\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u548c\u8de8\u57df\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u5fae\u8c03\u65f6\u7684\u4efb\u52a1\u9002\u5e94\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u51cf\u5c11\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "\u57fa\u4e8e\u57fa\u7840\u635f\u5931\u51fd\u6570\uff0c\u5f15\u5165\u4e24\u4e2a\u68af\u5ea6\u76f8\u5173\u6b63\u5219\u9879\uff1a\u68af\u5ea6\u65b9\u5411\u4e00\u81f4\u6027\u548c\u68af\u5ea6\u5e45\u503c\u63a7\u5236\uff0c\u4ee5\u53ca\u68af\u5ea6\u5bf9\u9f50\u673a\u5236\uff0c\u4f18\u5316\u8bad\u7ec3\u8def\u5f84\u3002", "result": "\u5728\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u5e73\u5747\u51c6\u786e\u7387\u3001\u68af\u5ea6\u7a33\u5b9a\u6027\u548c\u65b9\u5411\u5bf9\u9f50\u6027\uff0c\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u73af\u5883\u3002", "conclusion": "\u68af\u5ea6\u611f\u77e5\u5fae\u8c03\u6846\u67b6\u80fd\u6709\u6548\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u5f81\u80fd\u529b\uff0c\u786e\u4fdd\u8bad\u7ec3\u7a33\u5b9a\u6027\u5e76\u51cf\u5c11\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u9700\u6c42\u3002", "keywords": "\u68af\u5ea6\u611f\u77e5\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u5fae\u8c03\uff0c\u8bad\u7ec3\u7a33\u5b9a\u6027"}}
{"id": "2506.01716", "pdf": "https://arxiv.org/pdf/2506.01716", "abs": "https://arxiv.org/abs/2506.01716", "authors": ["Yifei Zhou", "Sergey Levine", "Jason Weston", "Xian Li", "Sainbayar Sukhbaatar"], "title": "Self-Challenging Language Model Agents", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models are quickly becoming the foundation for intelligent\nagents that are capable of using tools. However, training such agents is\nchallenging because it requires human creation and annotation of a diverse set\nof tasks, tools, and evaluation criteria. In this paper, we propose the\nSelf-Challenging framework for training an agent on high-quality tasks that are\ngenerated by itself. The agent first plays the role of challenger and generates\na task after interacting with the given tools. The tasks take the form of a\nnovel general class of problems termed Code-as-Task, which are defined by an\ninstruction, a verification function and solution and failure cases which serve\nas tests, allowing to filter only for high-quality tasks. The agent then takes\nan executor role and trains on those tasks with reinforcement learning using\nthe evaluation feedback as a reward. Evaluation on two existing multi-turn\ntool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging\nframework achieves over a two-fold improvement in Llama-3.1-8B-Instruct,\ndespite using only self-generated training data.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u6311\u6218\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u4ee3\u7406\u81ea\u6211\u751f\u6210\u9ad8\u8d28\u91cf\u4efb\u52a1\u8fdb\u884c\u8bad\u7ec3\uff0c\u63d0\u5347\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u6548\u679c\u663e\u8457\u3002", "motivation": "\u4f20\u7edf\u8bad\u7ec3\u667a\u80fd\u4ee3\u7406\u9700\u8981\u4eba\u5de5\u6807\u6ce8\u591a\u6837\u4efb\u52a1\u548c\u5de5\u5177\uff0c\u6210\u672c\u9ad8\uff1b\u63d0\u51fa\u81ea\u6311\u6218\u6846\u67b6\u4ee5\u51cf\u5c11\u5bf9\u4eba\u5de5\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "\u4ee3\u7406\u5206\u6311\u6218\u8005\u548c\u6267\u884c\u8005\u89d2\u8272\uff1a\u5148\u751f\u6210\u4efb\u52a1\uff08Code-as-Task\u5f62\u5f0f\uff09\uff0c\u518d\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002\u4efb\u52a1\u5305\u62ec\u6307\u4ee4\u3001\u9a8c\u8bc1\u51fd\u6570\u53ca\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "\u5728M3ToolEval\u548cTauBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u81ea\u8bad\u7ec3\u6570\u636e\u4e0bLlama-3.1-8B-Instruct\u6027\u80fd\u63d0\u5347\u4e24\u500d\u3002", "conclusion": "\u81ea\u6311\u6218\u6846\u67b6\u80fd\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7406\u6027\u80fd\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u667a\u80fd\u4ee3\u7406, \u81ea\u751f\u6210\u4efb\u52a1, Code-as-Task, \u5f3a\u5316\u5b66\u4e60"}}
{"id": "2506.00700", "pdf": "https://arxiv.org/pdf/2506.00700", "abs": "https://arxiv.org/abs/2506.00700", "authors": ["Nikola Milosevic", "Johannes M\u00fcller", "Nico Scherf"], "title": "Central Path Proximal Policy Optimization", "categories": ["cs.LG"], "comment": null, "summary": "In constrained Markov decision processes, enforcing constraints during\ntraining is often thought of as decreasing the final return. Recently, it was\nshown that constraints can be incorporated directly in the policy geometry,\nyielding an optimization trajectory close to the central path of a barrier\nmethod, which does not compromise final return. Building on this idea, we\nintroduce Central Path Proximal Policy Optimization (C3PO), a simple\nmodification of PPO that produces policy iterates, which stay close to the\ncentral path of the constrained optimization problem. Compared to existing\non-policy methods, C3PO delivers improved performance with tighter constraint\nenforcement, suggesting that central path-guided updates offer a promising\ndirection for constrained policy optimization.", "AI": {"tldr": "C3PO \u662f\u4e00\u79cd\u6539\u8fdb\u7684 PPO \u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7ea6\u675f\u76f4\u63a5\u878d\u5165\u7b56\u7565\u51e0\u4f55\uff0c\u4fdd\u6301\u63a5\u8fd1\u4e2d\u5fc3\u8def\u5f84\u7684\u66f4\u65b0\uff0c\u4ece\u800c\u63d0\u9ad8\u6027\u80fd\u5e76\u66f4\u4e25\u683c\u5730\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\u3002", "motivation": "\u5728\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f3a\u5236\u6267\u884c\u7ea6\u675f\u53ef\u80fd\u4f1a\u964d\u4f4e\u6700\u7ec8\u56de\u62a5\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5c06\u7ea6\u675f\u76f4\u63a5\u878d\u5165\u7b56\u7565\u51e0\u4f55\u53ef\u4ee5\u907f\u514d\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86 Central Path Proximal Policy Optimization (C3PO)\uff0c\u901a\u8fc7\u5bf9 PPO \u8fdb\u884c\u7b80\u5355\u4fee\u6539\uff0c\u751f\u6210\u63a5\u8fd1\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u4e2d\u5fc3\u8def\u5f84\u7684\u7b56\u7565\u8fed\u4ee3\u3002", "result": "\u4e0e\u73b0\u6709\u7b56\u7565\u65b9\u6cd5\u76f8\u6bd4\uff0cC3PO \u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u6027\u80fd\u8868\u73b0\u548c\u66f4\u4e25\u683c\u7684\u7ea6\u675f\u6267\u884c\u3002", "conclusion": "\u4e2d\u5fc3\u8def\u5f84\u5f15\u5bfc\u7684\u7b56\u7565\u66f4\u65b0\u4e3a\u7ea6\u675f\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "keywords": "\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b, \u4e2d\u5fc3\u8def\u5f84, PPO, \u7b56\u7565\u4f18\u5316"}}
{"id": "2506.00737", "pdf": "https://arxiv.org/pdf/2506.00737", "abs": "https://arxiv.org/abs/2506.00737", "authors": ["Yulia Otmakhova", "Lea Frermann"], "title": "Narrative Media Framing in Political Discourse", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Narrative frames are a powerful way of conceptualizing and communicating\ncomplex, controversial ideas, however automated frame analysis to date has\nmostly overlooked this framing device. In this paper, we connect elements of\nnarrativity with fundamental aspects of framing, and present a framework which\nformalizes and operationalizes such aspects. We annotate and release a data set\nof news articles in the climate change domain, analyze the dominance of\nnarrative frame components across political leanings, and test LLMs in their\nability to predict narrative frames and their components. Finally, we apply our\nframework in an unsupervised way to elicit components of narrative framing in a\nsecond domain, the COVID-19 crisis, where our predictions are congruent with\nprior theoretical work showing the generalizability of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u5f62\u5f0f\u5316\u548c\u64cd\u4f5c\u5316\u53d9\u4e8b\u6846\u67b6\uff0c\u5e76\u5728\u6c14\u5019\u53d8\u5316\u548cCOVID-19\u9886\u57df\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u7814\u7a76\u81ea\u52a8\u5316\u6846\u67b6\u5206\u6790\u4e2d\u53d9\u4e8b\u6846\u67b6\u7684\u7f3a\u5931\uff0c\u63a2\u8ba8\u5982\u4f55\u5c06\u53d9\u4e8b\u6027\u4e0e\u6846\u67b6\u5206\u6790\u7ed3\u5408\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5206\u6790\u4e86\u65b0\u95fb\u6587\u7ae0\u6570\u636e\u96c6\u4e2d\u7684\u53d9\u4e8b\u6846\u67b6\u7ec4\u6210\u90e8\u5206\uff0c\u5e76\u6d4b\u8bd5\u4e86LLMs\u5728\u9884\u6d4b\u53d9\u4e8b\u6846\u67b6\u53ca\u5176\u7ec4\u6210\u90e8\u5206\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u53d9\u4e8b\u6846\u67b6\u7ec4\u6210\u90e8\u5206\u5728\u4e0d\u540c\u653f\u6cbb\u503e\u5411\u4e2d\u7684\u4e3b\u5bfc\u6027\u5206\u6790\u53d1\u73b0\uff0c\u9884\u6d4b\u7ed3\u679c\u4e0e\u7406\u8bba\u4e00\u81f4\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6cdb\u5316\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u6709\u6548\u64cd\u4f5c\u5316\u53d9\u4e8b\u6846\u67b6\uff0c\u5e76\u5728\u4e0d\u540c\u9886\u57df\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "keywords": "narrative frames, framing, operationalization, climate change, COVID-19, LLMs"}}
{"id": "2506.01804", "pdf": "https://arxiv.org/pdf/2506.01804", "abs": "https://arxiv.org/abs/2506.01804", "authors": ["Cheonsu Jeong"], "title": "A Study on the MCP x A2A Framework for Enhancing Interoperability of LLM-based Autonomous Agents", "categories": ["cs.AI"], "comment": null, "summary": "This paper provides an in-depth technical analysis and implementation\nmethodology of the open-source Agent-to-Agent (A2A) protocol developed by\nGoogle and the Model Context Protocol (MCP) introduced by Anthropic. While the\nevolution of LLM-based autonomous agents is rapidly accelerating, efficient\ninteractions among these agents and their integration with external systems\nremain significant challenges. In modern AI systems, collaboration between\nautonomous agents and integration with external tools have become essential\nelements for building practical AI applications. A2A offers a standardized\ncommunication method that enables agents developed in heterogeneous\nenvironments to collaborate effectively, while MCP provides a structured I/O\nframework for agents to connect with external tools and resources. Prior\nstudies have focused primarily on the features and applications of either A2A\nor MCP individually. In contrast, this study takes an integrated approach,\nexploring how the two protocols can complement each other to address\ninteroperability issues and facilitate efficient collaboration within complex\nagent ecosystems.", "AI": {"tldr": "\u5bf9Google\u7684A2A\u534f\u8bae\u548cAnthropic\u7684MCP\u534f\u8bae\u8fdb\u884c\u6280\u672f\u5206\u6790\u548c\u6574\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u63a2\u8ba8\u4e24\u8005\u5982\u4f55\u4e92\u8865\u89e3\u51b3\u534f\u540c\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3AI\u7cfb\u7edf\u4e2d\uff0c\u9ad8\u6548\u4ee3\u7406\u95f4\u4ea4\u4e92\u53ca\u5916\u90e8\u7cfb\u7edf\u96c6\u6210\u662f\u4e3b\u8981\u6311\u6218\uff0c\u9700\u6807\u51c6\u5316\u901a\u4fe1\u4e0e\u7ed3\u6784\u5316\u7684I/O\u6846\u67b6\u3002", "method": "\u7ed3\u5408A2A\u7684\u6807\u51c6\u5316\u901a\u4fe1\u4e0eMCP\u7684I/O\u6846\u67b6\uff0c\u63d0\u51fa\u6574\u5408\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u663e\u793a\u4e24\u79cd\u534f\u8bae\u4e92\u8865\u53ef\u63d0\u5347\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u7684\u534f\u4f5c\u6548\u7387\u3002", "conclusion": "\u6574\u5408A2A\u548cMCP\u53ef\u6709\u6548\u89e3\u51b3\u4ee3\u7406\u95f4\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\uff0c\u4fc3\u8fdb\u590d\u6742\u751f\u6001\u4e2d\u7684\u9ad8\u6548\u534f\u4f5c\u3002", "keywords": "A2A, MCP, \u81ea\u4e3b\u4ee3\u7406, \u4e92\u64cd\u4f5c\u6027, AI\u534f\u4f5c"}}
{"id": "2506.00701", "pdf": "https://arxiv.org/pdf/2506.00701", "abs": "https://arxiv.org/abs/2506.00701", "authors": ["Yongchao Huang"], "title": "Bayesian Inference of Training Dataset Membership", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages", "summary": "Determining whether a dataset was part of a machine learning model's training\ndata pool can reveal privacy vulnerabilities, a challenge often addressed\nthrough membership inference attacks (MIAs). Traditional MIAs typically require\naccess to model internals or rely on computationally intensive shadow models.\nThis paper proposes an efficient, interpretable and principled Bayesian\ninference method for membership inference. By analyzing post-hoc metrics such\nas prediction error, confidence (entropy), perturbation magnitude, and dataset\nstatistics from a trained ML model, our approach computes posterior\nprobabilities of membership without requiring extensive model training.\nExperimental results on synthetic datasets demonstrate the method's\neffectiveness in distinguishing member from non-member datasets. Beyond\nmembership inference, this method can also detect distribution shifts, offering\na practical and interpretable alternative to existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u3001\u53ef\u89e3\u91ca\u7684\u8d1d\u53f6\u65af\u63a8\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u5224\u65ad\u6570\u636e\u96c6\u662f\u5426\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u6210\u5458\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08MIAs\uff09\u7684\u590d\u6742\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfMIAs\u9700\u8981\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u6216\u4f9d\u8d56\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u5f71\u5b50\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8bad\u7ec3\u540e\u6a21\u578b\u7684\u9884\u6d4b\u8bef\u5dee\u3001\u7f6e\u4fe1\u5ea6\uff08\u71b5\uff09\u3001\u6270\u52a8\u5e45\u5ea6\u548c\u6570\u636e\u96c6\u7edf\u8ba1\u7b49\u6307\u6807\uff0c\u5229\u7528\u8d1d\u53f6\u65af\u63a8\u7406\u8ba1\u7b97\u6210\u5458\u7684\u540e\u9a8c\u6982\u7387\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u533a\u5206\u6210\u5458\u548c\u975e\u6210\u5458\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u6210\u5458\u63a8\u7406\uff0c\u8fd8\u80fd\u68c0\u6d4b\u5206\u5e03\u53d8\u5316\uff0c\u4e3a\u73b0\u6709\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "keywords": "\u6210\u5458\u63a8\u7406\u653b\u51fb, \u8d1d\u53f6\u65af\u63a8\u7406, \u9690\u79c1\u4fdd\u62a4, \u673a\u5668\u5b66\u4e60\u5b89\u5168"}}
{"id": "2506.00739", "pdf": "https://arxiv.org/pdf/2506.00739", "abs": "https://arxiv.org/abs/2506.00739", "authors": ["Chiyu Zhang", "Marc-Alexandre Cote", "Michael Albada", "Anush Sankaran", "Jack W. Stokes", "Tong Wang", "Amir Abdi", "William Blum", "Muhammad Abdul-Mageed"], "title": "DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments", "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM) agents have shown impressive capabilities in human\nlanguage comprehension and reasoning, yet their potential in cybersecurity\nremains underexplored. We introduce DefenderBench, a practical, open-source\ntoolkit for evaluating language agents across offense, defense, and\ncybersecurity knowledge-based tasks. DefenderBench includes environments for\nnetwork intrusion, malicious content detection, code vulnerability analysis,\nand cybersecurity knowledge assessment. It is intentionally designed to be\naffordable and easily accessible for researchers while providing fair and\nrigorous assessment. We benchmark several state-of-the-art (SoTA) and popular\nLLMs, including both open- and closed-weight models, using a standardized\nagentic framework. Our results show that Claude-3.7-sonnet performs best with a\nDefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40,\nwhile the best open-weight model, Llama 3.3 70B, is not far behind with a\nDefenderBench score of 71.81. DefenderBench's modular design allows seamless\nintegration of custom LLMs and tasks, promoting reproducibility and fair\ncomparisons. An anonymized version of DefenderBench is available at\nhttps://github.com/microsoft/DefenderBench.", "AI": {"tldr": "DefenderBench\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u7edc\u5b89\u5168\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u62ec\u7f51\u7edc\u5165\u4fb5\u3001\u6076\u610f\u5185\u5bb9\u68c0\u6d4b\u7b49\u3002\u6d4b\u8bd5\u7ed3\u679c\u663e\u793aClaude-3.7-sonnet\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5c3d\u7ba1\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f00\u53d1DefenderBench\u5de5\u5177\u5305\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u6d4b\u8bd5\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u7edc\u5b89\u5168\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "Claude-3.7-sonnet\u5f97\u5206\u6700\u9ad8\uff0881.65\uff09\uff0c\u5f00\u6e90\u7684Llama 3.3 70B\u4e5f\u8868\u73b0\u4e0d\u4fd7\uff0871.81\uff09\u3002", "conclusion": "DefenderBench\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u8d1f\u62c5\u4e14\u516c\u5e73\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u6a21\u578b\u548c\u4efb\u52a1\u96c6\u6210\uff0c\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b, \u7f51\u7edc\u5b89\u5168, \u8bc4\u4f30\u5de5\u5177, DefenderBench"}}
{"id": "2506.01813", "pdf": "https://arxiv.org/pdf/2506.01813", "abs": "https://arxiv.org/abs/2506.01813", "authors": ["Djallel Bouneffouf", "Matthew Riemer", "Kush Varshney"], "title": "The Ultimate Test of Superintelligent AI Agents: Can an AI Balance Care and Control in Asymmetric Relationships?", "categories": ["cs.AI"], "comment": null, "summary": "This paper introduces the Shepherd Test, a new conceptual test for assessing\nthe moral and relational dimensions of superintelligent artificial agents. The\ntest is inspired by human interactions with animals, where ethical\nconsiderations about care, manipulation, and consumption arise in contexts of\nasymmetric power and self-preservation. We argue that AI crosses an important,\nand potentially dangerous, threshold of intelligence when it exhibits the\nability to manipulate, nurture, and instrumentally use less intelligent agents,\nwhile also managing its own survival and expansion goals. This includes the\nability to weigh moral trade-offs between self-interest and the well-being of\nsubordinate agents. The Shepherd Test thus challenges traditional AI evaluation\nparadigms by emphasizing moral agency, hierarchical behavior, and complex\ndecision-making under existential stakes. We argue that this shift is critical\nfor advancing AI governance, particularly as AI systems become increasingly\nintegrated into multi-agent environments. We conclude by identifying key\nresearch directions, including the development of simulation environments for\ntesting moral behavior in AI, and the formalization of ethical manipulation\nwithin multi-agent systems.", "AI": {"tldr": "Shepherd Test\u662f\u4e00\u4e2a\u65b0\u7684\u6982\u5ff5\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u8d85\u7ea7\u667a\u80fd\u4eba\u5de5\u4ee3\u7406\u7684\u9053\u5fb7\u548c\u5173\u7cfb\u7ef4\u5ea6\uff0c\u5f3a\u8c03\u5176\u5728\u4e0d\u5bf9\u79f0\u6743\u529b\u548c\u81ea\u6211\u4fdd\u5b58\u80cc\u666f\u4e0b\u7684\u64cd\u7eb5\u3001\u517b\u80b2\u548c\u5de5\u5177\u6027\u4f7f\u7528\u80fd\u529b\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u65e5\u76ca\u96c6\u6210\uff0c\u4f20\u7edfAI\u8bc4\u4f30\u8303\u5f0f\u672a\u80fd\u5145\u5206\u5173\u6ce8\u5176\u9053\u5fb7\u4ee3\u7406\u548c\u590d\u6742\u51b3\u7b56\u80fd\u529b\uff0c\u4e9f\u9700\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u53d7\u4eba\u7c7b\u4e0e\u52a8\u7269\u4f26\u7406\u4e92\u52a8\u542f\u53d1\u7684Shepherd Test\uff0c\u6d4b\u8bd5AI\u5728\u64cd\u7eb5\u3001\u517b\u80b2\u53ca\u6743\u8861\u81ea\u8eab\u5229\u76ca\u4e0e\u4e0b\u5c5e\u667a\u80fd\u4f53\u798f\u7949\u65f6\u7684\u80fd\u529b\u3002", "result": "Shepherd Test\u6311\u6218\u4e86\u4f20\u7edfAI\u8bc4\u4f30\u8303\u5f0f\uff0c\u63d0\u51fa\u9700\u5173\u6ce8AI\u7684\u9053\u5fb7\u4ee3\u7406\u3001\u5c42\u7ea7\u884c\u4e3a\u548c\u5b58\u5728\u98ce\u9669\u4e0b\u7684\u590d\u6742\u51b3\u7b56\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u5f00\u53d1\u6a21\u62df\u73af\u5883\u6d4b\u8bd5AI\u9053\u5fb7\u884c\u4e3a\uff0c\u4ee5\u53ca\u5f62\u5f0f\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u4f26\u7406\u64cd\u7eb5\u3002", "keywords": "Shepherd Test, \u8d85\u7ea7\u667a\u80fd, \u9053\u5fb7\u8bc4\u4f30, \u591a\u667a\u80fd\u4f53\u7cfb\u7edf, AI\u6cbb\u7406"}}
{"id": "2506.00710", "pdf": "https://arxiv.org/pdf/2506.00710", "abs": "https://arxiv.org/abs/2506.00710", "authors": ["Valter Hudovernik", "Minkai Xu", "Juntong Shi", "Lovro \u0160ubelj", "Stefano Ermon", "Erik \u0160trumbelj", "Jure Leskovec"], "title": "RelDiff: Relational Data Generative Modeling with Graph-Based Diffusion Models", "categories": ["cs.LG"], "comment": null, "summary": "Real-world databases are predominantly relational, comprising multiple\ninterlinked tables that contain complex structural and statistical\ndependencies. Learning generative models on relational data has shown great\npromise in generating synthetic data and imputing missing values. However,\nexisting methods often struggle to capture this complexity, typically reducing\nrelational data to conditionally generated flat tables and imposing limiting\nstructural assumptions. To address these limitations, we introduce RelDiff, a\nnovel diffusion generative model that synthesizes complete relational databases\nby explicitly modeling their foreign key graph structure. RelDiff combines a\njoint graph-conditioned diffusion process across all tables for attribute\nsynthesis, and a $2K+$SBM graph generator based on the Stochastic Block Model\nfor structure generation. The decomposition of graph structure and relational\nattributes ensures both high fidelity and referential integrity, both of which\nare crucial aspects of synthetic relational database generation. Experiments on\n11 benchmark datasets demonstrate that RelDiff consistently outperforms prior\nmethods in producing realistic and coherent synthetic relational databases.\nCode is available at https://github.com/ValterH/RelDiff.", "AI": {"tldr": "RelDiff\u662f\u4e00\u79cd\u65b0\u578b\u7684\u6269\u6563\u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u5408\u6210\u5b8c\u6574\u7684\u5173\u8054\u6570\u636e\u5e93\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5916\u952e\u56fe\u7ed3\u6784\uff0c\u540c\u65f6\u786e\u4fdd\u9ad8\u4fdd\u771f\u548c\u5f15\u7528\u5b8c\u6574\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u6a21\u578b\u5728\u5904\u7406\u5173\u8054\u6570\u636e\u65f6\u901a\u5e38\u7b80\u5316\u5176\u590d\u6742\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u591a\u8868\u683c\u95f4\u7684\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51faRelDiff\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "RelDiff\u91c7\u7528\u8054\u5408\u56fe\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\u8fdb\u884c\u5c5e\u6027\u5408\u6210\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u968f\u673a\u5757\u6a21\u578b\u7684$2K+SBM$\u56fe\u751f\u6210\u5668\u8fdb\u884c\u7ed3\u6784\u751f\u6210\u3002", "result": "\u572811\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRelDiff\u5728\u751f\u6210\u771f\u5b9e\u4e14\u8fde\u8d2f\u7684\u5173\u8054\u6570\u636e\u5e93\u65f6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RelDiff\u6210\u529f\u89e3\u51b3\u4e86\u5173\u8054\u6570\u636e\u5e93\u751f\u6210\u4e2d\u7684\u7ed3\u6784\u590d\u6742\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u548c\u5f15\u7528\u5b8c\u6574\u6027\u3002", "keywords": "\u751f\u6210\u6a21\u578b, \u6269\u6563\u8fc7\u7a0b, \u5173\u8054\u6570\u636e\u5e93, \u5916\u952e\u56fe\u7ed3\u6784, \u968f\u673a\u5757\u6a21\u578b"}}
{"id": "2506.00740", "pdf": "https://arxiv.org/pdf/2506.00740", "abs": "https://arxiv.org/abs/2506.00740", "authors": ["Harveen Singh Chadha", "Aswin Shanmugam Subramanian", "Vikas Joshi", "Shubham Bansal", "Jian Xue", "Rupeshkumar Mehta", "Jinyu Li"], "title": "Length Aware Speech Translation for Video Dubbing", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "This paper was accepted to Interspeech 2025", "summary": "In video dubbing, aligning translated audio with the source audio is a\nsignificant challenge. Our focus is on achieving this efficiently, tailored for\nreal-time, on-device video dubbing scenarios. We developed a phoneme-based\nend-to-end length-sensitive speech translation (LSST) model, which generates\ntranslations of varying lengths short, normal, and long using predefined tags.\nAdditionally, we introduced length-aware beam search (LABS), an efficient\napproach to generate translations of different lengths in a single decoding\npass. This approach maintained comparable BLEU scores compared to a baseline\nwithout length awareness while significantly enhancing synchronization quality\nbetween source and target audio, achieving a mean opinion score (MOS) gain of\n0.34 for Spanish and 0.65 for Korean, respectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u97f3\u7d20\u7aef\u5230\u7aef\u957f\u5ea6\u654f\u611f\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\uff08LSST\uff09\u548c\u957f\u5ea6\u611f\u77e5\u6ce2\u675f\u641c\u7d22\uff08LABS\uff09\uff0c\u4ee5\u5728\u8bbe\u5907\u4e0a\u5b9e\u65f6\u89c6\u9891\u914d\u97f3\u4e2d\u9ad8\u6548\u5bf9\u9f50\u7ffb\u8bd1\u97f3\u9891\u4e0e\u6e90\u97f3\u9891\u3002", "motivation": "\u89c6\u9891\u914d\u97f3\u4e2d\uff0c\u7ffb\u8bd1\u97f3\u9891\u4e0e\u6e90\u97f3\u9891\u5bf9\u9f50\u662f\u4e00\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5b9e\u65f6\u3001\u8bbe\u5907\u4e0a\u7684\u573a\u666f\u3002", "method": "\u5f00\u53d1\u4e86LSST\u6a21\u578b\u548c\u4f7f\u7528\u9884\u5b9a\u4e49\u6807\u7b7e\u751f\u6210\u4e0d\u540c\u957f\u5ea6\u7ffb\u8bd1\u7684LABS\u65b9\u6cd5\u3002", "result": "\u5728\u4fdd\u6301BLEU\u5206\u6570\u53ef\u6bd4\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u540c\u6b65\u8d28\u91cf\uff08\u897f\u73ed\u7259\u8bedMOS\u589e\u76ca0.34\uff0c\u97e9\u8bed0.65\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u914d\u97f3\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u97f3\u9891\u5bf9\u9f50\uff0c\u5c24\u5176\u662f\u5bf9\u5b9e\u65f6\u8bbe\u5907\u573a\u666f\u3002", "keywords": "\u89c6\u9891\u914d\u97f3, \u8bed\u97f3\u7ffb\u8bd1, \u957f\u5ea6\u654f\u611f, \u5b9e\u65f6\u5bf9\u9f50"}}
{"id": "2506.01820", "pdf": "https://arxiv.org/pdf/2506.01820", "abs": "https://arxiv.org/abs/2506.01820", "authors": ["Tim Woydt", "Moritz Willig", "Antonia W\u00fcst", "Lukas Helff", "Wolfgang Stammer", "Constantin A. Rothkopf", "Kristian Kersting"], "title": "Fodor and Pylyshyn's Legacy -- Still No Human-like Systematic Compositionality in Neural Networks", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Strong meta-learning capabilities for systematic compositionality are\nemerging as an important skill for navigating the complex and changing tasks of\ntoday's world. However, in presenting models for robust adaptation to novel\nenvironments, it is important to refrain from making unsupported claims about\nthe performance of meta-learning systems that ultimately do not stand up to\nscrutiny. While Fodor and Pylyshyn famously posited that neural networks\ninherently lack this capacity as they are unable to model compositional\nrepresentations or structure-sensitive operations, and thus are not a viable\nmodel of the human mind, Lake and Baroni recently presented meta-learning as a\npathway to compositionality. In this position paper, we critically revisit this\nclaim and highlight limitations in the proposed meta-learning framework for\ncompositionality. Our analysis shows that modern neural meta-learning systems\ncan only perform such tasks, if at all, under a very narrow and restricted\ndefinition of a meta-learning setup. We therefore claim that `Fodor and\nPylyshyn's legacy' persists, and to date, there is no human-like systematic\ncompositionality learned in neural networks.", "AI": {"tldr": "\u8bba\u6587\u6279\u8bc4\u4e86\u5f53\u524d\u5143\u5b66\u4e60\u6846\u67b6\u5728\u7cfb\u7edf\u6027\u7ec4\u5408\u6027\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u8ba4\u4e3a\u5176\u4ec5\u5728\u72ed\u7a84\u5b9a\u4e49\u4e0b\u6709\u6548\uff0c\u4e14\u672a\u80fd\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684\u7ec4\u5408\u6027\u3002", "motivation": "\u63a2\u8ba8\u5143\u5b66\u4e60\u662f\u5426\u80fd\u5b9e\u73b0\u7cfb\u7edf\u6027\u7ec4\u5408\u6027\uff0c\u56de\u5e94Fodor\u548cPylyshyn\u5bf9\u795e\u7ecf\u7f51\u7edc\u7684\u6279\u8bc4\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u6307\u51fa\u5176\u5c40\u9650\u6027\u3002", "result": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u53ea\u80fd\u5728\u6781\u7a84\u7684\u5143\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u7ec4\u5408\u6027\uff0c\u8fdc\u672a\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u3002", "conclusion": "Fodor\u548cPylyshyn\u7684\u6279\u8bc4\u4f9d\u7136\u6210\u7acb\uff0c\u795e\u7ecf\u7f51\u7edc\u7684\u7ec4\u5408\u6027\u80fd\u529b\u6709\u9650\u3002", "keywords": "\u5143\u5b66\u4e60, \u7cfb\u7edf\u6027\u7ec4\u5408\u6027, \u795e\u7ecf\u7f51\u7edc, \u4eba\u7c7b\u8ba4\u77e5"}}
{"id": "2506.00711", "pdf": "https://arxiv.org/pdf/2506.00711", "abs": "https://arxiv.org/abs/2506.00711", "authors": ["Wei Dai", "Peilin Chen", "Chanakya Ekbote", "Paul Pu Liang"], "title": "QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Clinical decision-making routinely demands reasoning over heterogeneous data,\nyet existing multimodal language models (MLLMs) remain largely vision-centric\nand fail to generalize across clinical specialties. To bridge this gap, we\nintroduce QoQ-Med-7B/32B, the first open generalist clinical foundation model\nthat jointly reasons across medical images, time-series signals, and text\nreports. QoQ-Med is trained with Domain-aware Relative Policy Optimization\n(DRPO), a novel reinforcement-learning objective that hierarchically scales\nnormalized rewards according to domain rarity and modality difficulty,\nmitigating performance imbalance caused by skewed clinical data distributions.\nTrained on 2.61 million instruction tuning pairs spanning 9 clinical domains,\nwe show that DRPO training boosts diagnostic performance by 43% in macro-F1 on\naverage across all visual domains as compared to other critic-free training\nmethods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation\ndata, it is able to highlight salient regions related to the diagnosis, with an\nIoU 10x higher than open models while reaching the performance of OpenAI\no4-mini. To foster reproducibility and downstream research, we release (i) the\nfull model weights, (ii) the modular training pipeline, and (iii) all\nintermediate reasoning traces at https://github.com/DDVD233/QoQ_Med.", "AI": {"tldr": "QoQ-Med-7B/32B\u662f\u9996\u4e2a\u5f00\u653e\u7684\u901a\u7528\u4e34\u5e8a\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7DRPO\u8bad\u7ec3\u65b9\u6cd5\u8054\u5408\u5206\u6790\u533b\u5b66\u56fe\u50cf\u3001\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\u548c\u6587\u672c\u62a5\u544a\uff0c\u663e\u8457\u63d0\u5347\u8bca\u65ad\u6027\u80fd\u5e76\u89e3\u51b3\u6570\u636e\u5206\u5e03\u4e0d\u5747\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u89c6\u89c9\uff0c\u65e0\u6cd5\u8de8\u4e34\u5e8a\u4e13\u4e1a\u6cdb\u5316\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u7efc\u5408\u5206\u6790\u591a\u79cd\u4e34\u5e8a\u6570\u636e\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86QoQ-Med\u6a21\u578b\uff0c\u91c7\u7528\u57df\u611f\u77e5\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08DRPO\uff09\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6839\u636e\u57df\u7a00\u6709\u6027\u548c\u6a21\u6001\u96be\u5ea6\u5206\u5c42\u7f29\u653e\u5956\u52b1\u3002", "result": "DRPO\u8bad\u7ec3\u5c06\u53ef\u89c6\u5316\u9886\u57df\u7684\u8bca\u65ad\u6027\u80fd\u5e73\u5747\u63d0\u534743%\uff08\u5b8fF1\uff09\uff0c\u5e76\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5f00\u653e\u6a21\u578b\uff0c\u8fbe\u5230OpenAI o4-mini\u6c34\u5e73\u3002", "conclusion": "QoQ-Med\u901a\u8fc7DRPO\u8bad\u7ec3\u89e3\u51b3\u4e86\u4e34\u5e8a\u6570\u636e\u5206\u5e03\u4e0d\u5747\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8bca\u65ad\u6027\u80fd\uff0c\u63a8\u52a8\u4e86\u53ef\u590d\u73b0\u6027\u548c\u4e0b\u6e38\u7814\u7a76\u3002", "keywords": "QoQ-Med, DRPO, \u4e34\u5e8a\u57fa\u7840\u6a21\u578b, \u591a\u6a21\u6001, \u533b\u7597\u8bca\u65ad"}}
{"id": "2506.00741", "pdf": "https://arxiv.org/pdf/2506.00741", "abs": "https://arxiv.org/abs/2506.00741", "authors": ["Shangbin Feng", "Yike Wang", "Weijia Shi", "Yulia Tsvetkov"], "title": "Data Swarms: Optimizable Generation of Synthetic Evaluation Data", "categories": ["cs.CL"], "comment": null, "summary": "We propose Data Swarms, an algorithm to optimize the generation of synthetic\nevaluation data and advance quantitative desiderata of LLM evaluation. We first\ntrain a swarm of initial data generators using existing data, and define\nvarious evaluation objectives to reflect the desired properties of evaluation\n(e.g., generate more difficult problems for the evaluated models) and\nquantitatively evaluate data generators. We then employ particle swarm\noptimization to optimize the swarm of data generators, where they\ncollaboratively search through the model parameter space to find new generators\nthat advance these objectives. We further extend it to Adversarial Swarms,\nwhere the data generator swarm generates harder data while the test taker model\nswarm learns from such data, co-evolving dynamically for better data and models\nsimultaneously. Extensive experiments demonstrate that Data Swarms outperforms\neight data generation baselines across five evaluation objectives, while\nAdversarial Swarms produce more robust learning of synthetic data and stronger\ngeneralization. Further analysis reveals that Data Swarms successfully\noptimizes compositions of multiple evaluation objectives and generalizes to new\noff-the-shelf LLMs, unseen at optimization time.", "AI": {"tldr": "Data Swarms\u662f\u4e00\u79cd\u4f18\u5316\u751f\u6210\u5408\u6210\u8bc4\u4f30\u6570\u636e\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u7c92\u5b50\u7fa4\u4f18\u5316\u534f\u540c\u641c\u7d22\u53c2\u6570\u7a7a\u95f4\u4ee5\u63d0\u5347\u6570\u636e\u751f\u6210\u5668\u7684\u6027\u80fd\u3002\u5176\u6269\u5c55\u7248\u672cAdversarial Swarms\u901a\u8fc7\u5bf9\u6297\u751f\u6210\u548c\u6a21\u578b\u5b66\u4e60\u52a8\u6001\u63d0\u5347\u6570\u636e\u8d28\u91cf\u548c\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709LLM\u8bc4\u4f30\u6570\u636e\u751f\u6210\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u8bc4\u4f30\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u9488\u5bf9\u6027\u3002", "method": "\u4f7f\u7528\u7c92\u5b50\u7fa4\u4f18\u5316\u8bad\u7ec3\u521d\u59cb\u6570\u636e\u751f\u6210\u5668\u7fa4\uff0c\u5e76\u901a\u8fc7\u5bf9\u6297\u751f\u6210\u4e0e\u6a21\u578b\u5b66\u4e60\u52a8\u6001\u4f18\u5316\u3002", "result": "\u5728\u4e94\u9879\u8bc4\u4f30\u76ee\u6807\u4e0a\u4f18\u4e8e\u516b\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u80fd\u4f18\u5316\u591a\u76ee\u6807\u7ec4\u5408\u5e76\u6cdb\u5316\u5230\u672a\u89c1\u7684LLM\u6a21\u578b\u3002", "conclusion": "Data Swarms\u548cAdversarial Swarms\u80fd\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\uff0c\u5e76\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "Data Swarms, Adversarial Swarms, \u7c92\u5b50\u7fa4\u4f18\u5316, LLM\u8bc4\u4f30, \u5408\u6210\u6570\u636e"}}
{"id": "2506.01881", "pdf": "https://arxiv.org/pdf/2506.01881", "abs": "https://arxiv.org/abs/2506.01881", "authors": ["Yaoyao Qian", "Jindan Huang", "Yuanli Wang", "Simon Yu", "Kyrie Zhixuan Zhou", "Jiayuan Mao", "Mingfu Liang", "Hanhan Zhou"], "title": "WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent Triggerability in Task-Oriented Dialogue", "categories": ["cs.AI", "cs.CL"], "comment": "43 pages, 31 figures. Project website: https://nanostorm.netlify.app/", "summary": "Task-oriented dialogue systems often face difficulties when user utterances\nseem semantically complete but lack necessary structural information for\nappropriate system action. This arises because users frequently do not fully\nunderstand their own needs, while systems require precise intent definitions.\nCurrent LLM-based agents cannot effectively distinguish between linguistically\ncomplete and contextually triggerable expressions, lacking frameworks for\ncollaborative intent formation. We present STORM, a framework modeling\nasymmetric information dynamics through conversations between UserLLM (full\ninternal access) and AgentLLM (observable behavior only). STORM produces\nannotated corpora capturing expression trajectories and latent cognitive\ntransitions, enabling systematic analysis of collaborative understanding\ndevelopment. Our contributions include: (1) formalizing asymmetric information\nprocessing in dialogue systems; (2) modeling intent formation tracking\ncollaborative understanding evolution; and (3) evaluation metrics measuring\ninternal cognitive improvements alongside task performance. Experiments across\nfour language models reveal that moderate uncertainty (40-60%) can outperform\ncomplete transparency in certain scenarios, with model-specific patterns\nsuggesting reconsideration of optimal information completeness in human-AI\ncollaboration. These findings contribute to understanding asymmetric reasoning\ndynamics and inform uncertainty-calibrated dialogue system design.", "AI": {"tldr": "STORM\u6846\u67b6\u901a\u8fc7UserLLM\u548cAgentLLM\u6a21\u62df\u5bf9\u8bdd\u4e2d\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\uff0c\u4e13\u6ce8\u4e8e\u534f\u4f5c\u610f\u56fe\u5f62\u6210\uff0c\u63d0\u51fa40-60%\u7684\u4e0d\u786e\u5b9a\u6027\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u4f18\u4e8e\u5b8c\u5168\u900f\u660e\u3002", "motivation": "\u89e3\u51b3\u7528\u6237\u8bdd\u8bed\u8bed\u4e49\u5b8c\u6574\u4f46\u7ed3\u6784\u4fe1\u606f\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u7814\u7a76AI\u5982\u4f55\u4e0e\u7528\u6237\u534f\u4f5c\u5f62\u6210\u610f\u56fe\u3002", "method": "\u63d0\u51faSTORM\u6846\u67b6\uff0c\u5229\u7528UserLLM\u548cAgentLLM\u6a21\u62df\u5bf9\u8bdd\uff0c\u6355\u6349\u8868\u8fbe\u8f68\u8ff9\u548c\u6f5c\u5728\u8ba4\u77e5\u8f6c\u53d8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c40-60%\u7684\u4e0d\u786e\u5b9a\u6027\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u6a21\u578b\u7279\u5b9a\u7684\u6a21\u5f0f\u63ed\u793a\u4e86\u4fe1\u606f\u5b8c\u6574\u6027\u7684\u91cd\u65b0\u601d\u8003\u3002", "conclusion": "STORM\u63d0\u4f9b\u4e86\u4e00\u79cd\u5206\u6790\u534f\u4f5c\u7406\u89e3\u53d1\u5c55\u7684\u65b9\u6cd5\uff0c\u4e3a\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u7684\u5bf9\u8bdd\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u53c2\u8003\u3002", "keywords": "\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf, \u4fe1\u606f\u4e0d\u5bf9\u79f0, \u534f\u4f5c\u610f\u56fe\u5f62\u6210, \u4e0d\u786e\u5b9a\u6027\u6821\u51c6, \u8ba4\u77e5\u52a8\u6001"}}
{"id": "2506.00723", "pdf": "https://arxiv.org/pdf/2506.00723", "abs": "https://arxiv.org/abs/2506.00723", "authors": ["Daniel Paleka", "Shashwat Goel", "Jonas Geiping", "Florian Tram\u00e8r"], "title": "Pitfalls in Evaluating Language Model Forecasters", "categories": ["cs.LG", "cs.AI", "cs.IR"], "comment": "20 pages, 8 figures", "summary": "Large language models (LLMs) have recently been applied to forecasting tasks,\nwith some works claiming these systems match or exceed human performance. In\nthis paper, we argue that, as a community, we should be careful about such\nconclusions as evaluating LLM forecasters presents unique challenges. We\nidentify two broad categories of issues: (1) difficulty in trusting evaluation\nresults due to many forms of temporal leakage, and (2) difficulty in\nextrapolating from evaluation performance to real-world forecasting. Through\nsystematic analysis and concrete examples from prior work, we demonstrate how\nevaluation flaws can raise concerns about current and future performance\nclaims. We argue that more rigorous evaluation methodologies are needed to\nconfidently assess the forecasting abilities of LLMs.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5b58\u5728\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u65f6\u95f4\u6cc4\u6f0f\u548c\u5b9e\u9645\u9884\u6d4b\u80fd\u529b\u8bc4\u4f30\u56f0\u96be\u4e24\u5927\u95ee\u9898\u3002", "motivation": "\u8d28\u7591\u5f53\u524d\u5bf9LLMs\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u7684\u7814\u7a76\u7ed3\u8bba\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u4e25\u8c28\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u548c\u5177\u4f53\u6848\u4f8b\uff0c\u63ed\u9732\u8bc4\u4f30\u4e2d\u7684\u7f3a\u9677\uff0c\u5c24\u5176\u662f\u65f6\u95f4\u6cc4\u6f0f\u548c\u6027\u80fd\u5916\u63a8\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u53ef\u80fd\u5bfc\u81f4\u5bf9LLM\u9884\u6d4b\u80fd\u529b\u7684\u8bef\u5224\u3002", "conclusion": "\u547c\u5401\u91c7\u7528\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u51c6\u786e\u8bc4\u4f30LLMs\u7684\u9884\u6d4b\u80fd\u529b\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u9884\u6d4b\u4efb\u52a1,\u65f6\u95f4\u6cc4\u6f0f,\u8bc4\u4f30\u65b9\u6cd5,\u6027\u80fd\u5916\u63a8"}}
{"id": "2506.00743", "pdf": "https://arxiv.org/pdf/2506.00743", "abs": "https://arxiv.org/abs/2506.00743", "authors": ["Yeshwanth Venkatesha", "Souvik Kundu", "Priyadarshini Panda"], "title": "Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection", "categories": ["cs.CL", "cs.AI", "cs.DC"], "comment": null, "summary": "Parameter Efficient Fine-Tuning (PEFT) has become the de-facto approach in\nadapting Large Language Models (LLMs) for downstream tasks in Natural Language\nProcessing. However, its adoption in privacy-preserving distributed learning\nframeworks, such as Federated Learning (FL), remains relatively limited. This\nis mainly due to challenges specific to FL, such as resource-constrained\ndevices and diverse data distributions among clients. In this paper, we propose\nan efficient method to perform PEFT within the FL framework for Multi-Head\nAttention (MHA) based language models. We address the challenges through head\npruning, a novel head-specific weighted aggregation mechanism, and a client\nselection strategy. Head pruning minimizes training complexity within the\nclients, guided by the importance score computed based on the confidence of the\nattention head. Weighted aggregation of heads ensures the global model captures\ncrucial updates from diverse clients complementing our client selection\nstrategy. We show results on the MultiNLI benchmark along with 20 Newsgroups,\nXL-Sum, and E2E NLG datasets. We use the MultiNLI dataset and T5-small model\nwith LoRA as our PEFT method, attaining sparsity levels of up to 90%, resulting\nin a communication advantage of up to 1.8x and a reduction in training OPs of\n3.9x while maintaining the accuracy drop under 2%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u5185\u9ad8\u6548\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u7684\u65b9\u6cd5\uff0c\u9488\u5bf9\u591a\u5934\u6ce8\u610f\u529b\uff08MHA\uff09\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5934\u90e8\u526a\u679d\u3001\u52a0\u6743\u805a\u5408\u548c\u5ba2\u6237\u7aef\u9009\u62e9\u7b56\u7565\u89e3\u51b3FL\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1PEFT\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5728\u9690\u79c1\u4fdd\u62a4\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\uff08\u5982\u8054\u90a6\u5b66\u4e60\uff09\u4e2d\u5e94\u7528\u6709\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u8bbe\u5907\u8d44\u6e90\u53d7\u9650\u548c\u6570\u636e\u5206\u5e03\u4e0d\u5747\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5934\u90e8\u526a\u679d\u964d\u4f4e\u8bad\u7ec3\u590d\u6742\u5ea6\uff0c\u63d0\u51fa\u5934\u90e8\u7279\u5b9a\u7684\u52a0\u6743\u805a\u5408\u673a\u5236\u548c\u5ba2\u6237\u7aef\u9009\u62e9\u7b56\u7565\uff0c\u786e\u4fdd\u5168\u5c40\u6a21\u578b\u6355\u6349\u591a\u6837\u5316\u7684\u66f4\u65b0\u3002", "result": "\u5728MultiNLI\u7b49\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u4f7f\u7528T5-small\u6a21\u578b\u548cLoRA\u65b9\u6cd5\uff0c\u8fbe\u523090%\u7684\u7a00\u758f\u5ea6\uff0c\u901a\u4fe1\u4f18\u52bf\u63d0\u53471.8\u500d\uff0c\u8bad\u7ec3\u64cd\u4f5c\u51cf\u5c113.9\u500d\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u5c0f\u4e8e2%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u9ad8\u6548\u5b9e\u73b0\u4e86PEFT\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\u3002", "keywords": "\u53c2\u6570\u9ad8\u6548\u5fae\u8c03, \u8054\u90a6\u5b66\u4e60, \u591a\u5934\u6ce8\u610f\u529b, \u5934\u90e8\u526a\u679d, \u52a0\u6743\u805a\u5408"}}
{"id": "2506.01900", "pdf": "https://arxiv.org/pdf/2506.01900", "abs": "https://arxiv.org/abs/2506.01900", "authors": ["Manish Bhatt", "Ronald F. Del Rosario", "Vineeth Sai Narajala", "Idan Habler"], "title": "COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents", "categories": ["cs.AI", "cs.CE", "cs.CR"], "comment": "20 pages, 2 figures, github linked", "summary": "The meteoric rise and proliferation of autonomous Large Language Model (LLM)\nagents promise significant capabilities across various domains. However, their\ndeployment is increasingly constrained by substantial computational demands,\nspecifically for Graphics Processing Unit (GPU) resources. This paper addresses\nthe critical problem of optimizing resource utilization in LLM agent systems.\nWe introduce COALESCE (Cost-Optimized and Secure Agent Labour Exchange via\nSkill-based Competence Estimation), a novel framework designed to enable\nautonomous LLM agents to dynamically outsource specific subtasks to\nspecialized, cost-effective third-party LLM agents. The framework integrates\nmechanisms for hybrid skill representation, dynamic skill discovery, automated\ntask decomposition, a unified cost model comparing internal execution costs\nagainst external outsourcing prices, simplified market-based decision-making\nalgorithms, and a standardized communication protocol between LLM agents.\nComprehensive validation through 239 theoretical simulations demonstrates\n41.8\\% cost reduction potential, while large-scale empirical validation across\n240 real LLM tasks confirms 20.3\\% cost reduction with proper epsilon-greedy\nexploration, establishing both theoretical viability and practical\neffectiveness. The emergence of proposed open standards like Google's\nAgent2Agent (A2A) protocol further underscores the need for frameworks like\nCOALESCE that can leverage such standards for efficient agent interaction. By\nfacilitating a dynamic market for agent capabilities, potentially utilizing\nprotocols like A2A for communication, COALESCE aims to significantly reduce\noperational costs, enhance system scalability, and foster the emergence of\nspecialized agent economies, making complex LLM agent functionalities more\naccessible and economically viable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCOALESCE\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5916\u5305\u4efb\u52a1\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7cfb\u7edf\u7684\u8d44\u6e90\u5229\u7528\uff0c\u7406\u8bba\u4e0a\u53ef\u964d\u4f4e41.8%\u6210\u672c\uff0c\u5b9e\u8bc1\u4e2d\u964d\u4f4e20.3%\u6210\u672c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7cfb\u7edf\u7684\u90e8\u7f72\u53d7\u5230GPU\u8d44\u6e90\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u4f18\u5316\u8d44\u6e90\u5229\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCOALESCE\u6846\u67b6\uff0c\u5305\u62ec\u6df7\u5408\u6280\u80fd\u8868\u793a\u3001\u52a8\u6001\u6280\u80fd\u53d1\u73b0\u3001\u4efb\u52a1\u5206\u89e3\u3001\u6210\u672c\u6a21\u578b\u3001\u5e02\u573a\u51b3\u7b56\u7b97\u6cd5\u548c\u6807\u51c6\u5316\u901a\u4fe1\u534f\u8bae\u3002", "result": "\u7406\u8bba\u6a21\u62df\u663e\u793a41.8%\u6210\u672c\u964d\u4f4e\u6f5c\u529b\uff0c\u5b9e\u8bc1\u5b9e\u9a8c\u663e\u793a20.3%\u6210\u672c\u964d\u4f4e\u3002", "conclusion": "COALESCE\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u5916\u5305\u663e\u8457\u964d\u4f4e\u6210\u672c\uff0c\u63d0\u5347\u7cfb\u7edf\u53ef\u6269\u5c55\u6027\uff0c\u4fc3\u8fdb\u4e13\u4e1a\u5316\u4ee3\u7406\u7ecf\u6d4e\u3002", "keywords": "LLM\u4ee3\u7406, \u8d44\u6e90\u4f18\u5316, COALESCE\u6846\u67b6, \u52a8\u6001\u5916\u5305, \u6210\u672c\u964d\u4f4e"}}
{"id": "2506.00724", "pdf": "https://arxiv.org/pdf/2506.00724", "abs": "https://arxiv.org/abs/2506.00724", "authors": ["Siddharth Prabhu", "Srinivas Rangarajan", "Mayuresh Kothare"], "title": "A condensing approach to multiple shooting neural ordinary differential equation", "categories": ["cs.LG", "math.DS"], "comment": null, "summary": "Multiple-shooting is a parameter estimation approach for ordinary\ndifferential equations. In this approach, the trajectory is broken into small\nintervals, each of which can be integrated independently. Equality constraints\nare then applied to eliminate the shooting gap between the end of the previous\ntrajectory and the start of the next trajectory. Unlike single-shooting,\nmultiple-shooting is more stable, especially for highly oscillatory and long\ntrajectories. In the context of neural ordinary differential equations,\nmultiple-shooting is not widely used due to the challenge of incorporating\ngeneral equality constraints. In this work, we propose a condensing-based\napproach to incorporate these shooting equality constraints while training a\nmultiple-shooting neural ordinary differential equation (MS-NODE) using\nfirst-order optimization methods such as Adam.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u91cd\u5c04\u51fb\u65b9\u6cd5\u5728\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u538b\u7f29\u7684\u7b56\u7565\u6765\u89e3\u51b3\u7ea6\u675f\u95ee\u9898\u3002", "motivation": "\u591a\u91cd\u5c04\u51fb\u65b9\u6cd5\u5728\u4f20\u7edf\u53c2\u6570\u4f30\u8ba1\u4e2d\u8868\u73b0\u7a33\u5b9a\uff0c\u4f46\u5728\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u4e2d\u5e94\u7528\u8f83\u5c11\uff0c\u4e3b\u8981\u7531\u4e8e\u96be\u4ee5\u5904\u7406\u4e00\u822c\u6027\u7ea6\u675f\u6761\u4ef6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u538b\u7f29\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u91cd\u5c04\u51fb\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\uff08MS-NODE\uff09\uff0c\u5e76\u4f7f\u7528\u4e00\u9636\u4f18\u5316\u65b9\u6cd5\uff08\u5982Adam\uff09\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u91cd\u5c04\u51fb\u4e2d\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u538b\u7f29\u65b9\u6cd5\u4e3a\u591a\u91cd\u5c04\u51fb\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u591a\u91cd\u5c04\u51fb, \u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b, \u538b\u7f29\u65b9\u6cd5, \u4f18\u5316"}}
{"id": "2506.00748", "pdf": "https://arxiv.org/pdf/2506.00748", "abs": "https://arxiv.org/abs/2506.00748", "authors": ["Pardis Sadat Zahraei", "Ali Emami"], "title": "Translate With Care: Addressing Gender Bias, Neutrality, and Reasoning in Large Language Model Translations", "categories": ["cs.CL", "cs.CY"], "comment": "Accepted to Findings of ACL 2025", "summary": "Addressing gender bias and maintaining logical coherence in machine\ntranslation remains challenging, particularly when translating between natural\ngender languages, like English, and genderless languages, such as Persian,\nIndonesian, and Finnish. We introduce the Translate-with-Care (TWC) dataset,\ncomprising 3,950 challenging scenarios across six low- to mid-resource\nlanguages, to assess translation systems' performance. Our analysis of diverse\ntechnologies, including GPT-4, mBART-50, NLLB-200, and Google Translate,\nreveals a universal struggle in translating genderless content, resulting in\ngender stereotyping and reasoning errors. All models preferred masculine\npronouns when gender stereotypes could influence choices. Google Translate and\nGPT-4 showed particularly strong bias, favoring male pronouns 4-6 times more\nthan feminine ones in leadership and professional success contexts. Fine-tuning\nmBART-50 on TWC substantially resolved these biases and errors, led to strong\ngeneralization, and surpassed proprietary LLMs while remaining open-source.\nThis work emphasizes the need for targeted approaches to gender and semantic\ncoherence in machine translation, particularly for genderless languages,\ncontributing to more equitable and accurate translation systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Translate-with-Care (TWC)\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7ffb\u8bd1\u7cfb\u7edf\u5728\u5904\u7406\u65e0\u6027\u522b\u8bed\u8a00\u65f6\u7684\u6027\u522b\u504f\u89c1\u548c\u903b\u8f91\u8fde\u8d2f\u6027\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5fae\u8c03mBART-50\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u7ffb\u8bd1\u4e2d\u6027\u522b\u504f\u89c1\u548c\u903b\u8f91\u8fde\u8d2f\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u65e0\u6027\u522b\u8bed\u8a00\uff08\u5982\u6ce2\u65af\u8bed\u3001\u5370\u5c3c\u8bed\u3001\u82ac\u5170\u8bed\uff09\u4e0e\u6709\u6027\u522b\u8bed\u8a00\uff08\u5982\u82f1\u8bed\uff09\u4e4b\u95f4\u7684\u7ffb\u8bd1\u3002", "method": "\u5f15\u5165TWC\u6570\u636e\u96c6\uff083,950\u4e2a\u6311\u6218\u6027\u573a\u666f\uff09\uff0c\u8bc4\u4f30GPT-4\u3001mBART-50\u3001NLLB-200\u548cGoogle Translate\u7684\u6027\u80fd\uff0c\u5e76\u5bf9mBART-50\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u663e\u793a\u6240\u6709\u6a21\u578b\u5728\u65e0\u6027\u522b\u5185\u5bb9\u7ffb\u8bd1\u4e2d\u666e\u904d\u5b58\u5728\u6027\u522b\u504f\u89c1\u548c\u63a8\u7406\u9519\u8bef\uff0c\u5fae\u8c03mBART-50\u663e\u8457\u6539\u5584\u4e86\u8fd9\u4e9b\u95ee\u9898\u4e14\u8868\u73b0\u4f18\u4e8e\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "\u9700\u9488\u5bf9\u6027\u89e3\u51b3\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u6027\u522b\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u65e0\u6027\u522b\u8bed\u8a00\u4e2d\uff0c\u4ee5\u63d0\u5347\u7ffb\u8bd1\u7684\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u3002", "keywords": "gender bias, machine translation, genderless languages, TWC dataset, fine-tuning"}}
{"id": "2506.01901", "pdf": "https://arxiv.org/pdf/2506.01901", "abs": "https://arxiv.org/abs/2506.01901", "authors": ["Yifan Hao", "Xingyuan Pan", "Hanning Zhang", "Chenlu Ye", "Rui Pan", "Tong Zhang"], "title": "Understanding Overadaptation in Supervised Fine-Tuning: The Role of Ensemble Methods", "categories": ["cs.AI"], "comment": null, "summary": "Supervised fine-tuning (SFT) on domain-specific data is the dominant approach\nfor adapting foundation models to specialized tasks. However, it has been\nobserved that SFT models tend to forget knowledge acquired during pretraining.\nIn vision models, ensembling a pretrained model with its fine-tuned counterpart\nhas been shown to mitigate this issue. In this work, we demonstrate that the\nsame holds for language models, and, more strikingly, we observe an\noveradaptation phenomenon: the ensemble model not only retains general\nknowledge from the foundation model but also outperforms the fine-tuned model\neven on the fine-tuning domain itself. Despite the empirical success of\nensembling, a theoretical understanding of its benefits remains underexplored.\nWe develop a formal theoretical analysis of the overadaptation phenomenon.\nEnsembling mitigates this by balancing two primary sources of error: bias,\ncaused by insufficient fine-tuning, and variance, introduced by overfitting to\nfine-tuning data. While regularization techniques aim to address this\ntrade-off, we show that ensembling provides a more effective solution. We\nanalyze this phenomenon in over-parameterized linear settings and demonstrate\nthat interpolating between pretrained and fine-tuned weights significantly\nimproves performance. These findings offer theoretical justification for the\nobserved advantages of model ensembling, supported by empirical experiments\nconsistent with our analysis.", "AI": {"tldr": "\u901a\u8fc7\u96c6\u6210\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6a21\u578b\u89e3\u51b3\u76d1\u7763\u5fae\u8c03\u4e2d\u7684\u77e5\u8bc6\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u7406\u8bba\u5206\u6790\u3002", "motivation": "\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5728\u7279\u5b9a\u9886\u57df\u6570\u636e\u4e0a\u5bb9\u6613\u9057\u5fd8\u9884\u8bad\u7ec3\u77e5\u8bc6\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5fae\u8c03\u6a21\u578b\uff0c\u5e73\u8861\u504f\u5dee\u548c\u65b9\u5dee\u95ee\u9898\u3002", "result": "\u96c6\u6210\u6a21\u578b\u4e0d\u4ec5\u4fdd\u7559\u4e86\u9884\u8bad\u7ec3\u77e5\u8bc6\uff0c\u8fd8\u5728\u5fae\u8c03\u9886\u57df\u4e0a\u4f18\u4e8e\u5355\u4e00\u5fae\u8c03\u6a21\u578b\u3002", "conclusion": "\u96c6\u6210\u65b9\u6cd5\u4f18\u4e8e\u6b63\u5219\u5316\u6280\u672f\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u76d1\u7763\u5fae\u8c03\u4e2d\u7684\u8fc7\u9002\u5e94\u95ee\u9898\u3002", "keywords": "\u76d1\u7763\u5fae\u8c03\u3001\u6a21\u578b\u96c6\u6210\u3001\u8fc7\u9002\u5e94\u3001\u504f\u5dee-\u65b9\u5dee\u6743\u8861"}}
{"id": "2506.00727", "pdf": "https://arxiv.org/pdf/2506.00727", "abs": "https://arxiv.org/abs/2506.00727", "authors": ["Javier Bisbal", "Julio Sotelo", "Maria I Vald\u00e9s", "Pablo Irarrazaval", "Marcelo E Andia", "Julio Garc\u00eda", "Jos\u00e9 Rodriguez-Palomarez", "Francesca Raimondi", "Cristi\u00e1n Tejos", "Sergio Uribe"], "title": "Adaptive Plane Reformatting for 4D Flow MRI using Deep Reinforcement Learning", "categories": ["cs.LG", "cs.CV", "I.4.0"], "comment": "11 pages, 4 figures, submitted to IEEE Transactions on Medical\n  Imaging", "summary": "Deep reinforcement learning (DRL) algorithms have shown robust results in\nplane reformatting tasks. In these methods, an agent sequentially adjusts the\nposition and orientation of an initial plane towards an objective location.\nThis process allows accurate plane reformatting, without the need for detailed\nlandmarks, which makes it suitable for images with limited contrast and\nresolution, such as 4D flow MRI. However, current DRL methods require the test\ndataset to be in the same position and orientation as the training dataset. In\nthis paper, we present a novel technique that utilizes a flexible coordinate\nsystem based on the current state, enabling navigation in volumes at any\nposition or orientation. We adopted the Asynchronous Advantage Actor Critic\n(A3C) algorithm for reinforcement learning, outperforming Deep Q Network (DQN).\nExperimental results in 4D flow MRI demonstrate improved accuracy in plane\nreformatting angular and distance errors (6.32 +- 4.15 {\\deg} and 3.40 +- 2.75\nmm), as well as statistically equivalent flow measurements determined by a\nplane reformatting process done by an expert (p=0.21). The method's flexibility\nand adaptability make it a promising candidate for other medical imaging\napplications beyond 4D flow MRI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7075\u6d3b\u5750\u6807\u7cfb\u7684\u65b0\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u65b9\u6cd5\u5728\u5e73\u9762\u91cd\u65b0\u683c\u5f0f\u5316\u4efb\u52a1\u4e2d\u9700\u8981\u6d4b\u8bd5\u6570\u636e\u4e0e\u8bad\u7ec3\u6570\u636e\u4f4d\u7f6e\u548c\u65b9\u5411\u4e00\u81f4\u7684\u95ee\u9898\u3002\u91c7\u7528A3C\u7b97\u6cd5\uff0c\u6027\u80fd\u4f18\u4e8eDQN\uff0c\u5b9e\u9a8c\u7ed3\u679c\u57284D\u6d41MRI\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u5f53\u524dDRL\u65b9\u6cd5\u5728\u5e73\u9762\u91cd\u65b0\u683c\u5f0f\u5316\u4efb\u52a1\u4e2d\u9700\u8981\u6d4b\u8bd5\u6570\u636e\u4e0e\u8bad\u7ec3\u6570\u636e\u7684\u4f4d\u7f6e\u548c\u65b9\u5411\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u5176\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7075\u6d3b\u5750\u6807\u7cfb\u7684\u65b0\u6280\u672f\uff0c\u91c7\u7528A3C\u7b97\u6cd5\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u57284D\u6d41MRI\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u89d2\u5ea6\u548c\u8ddd\u79bb\u8bef\u5dee\uff086.32\u00b14.15\u00b0\u548c3.40\u00b12.75\u6beb\u7c73\uff09\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u6d41\u91cf\u6d4b\u91cf\u7ed3\u679c\u4e0e\u4e13\u5bb6\u64cd\u4f5c\u65e0\u7edf\u8ba1\u5b66\u5dee\u5f02\uff08p=0.21\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u4f7f\u5176\u4e0d\u4ec5\u9002\u7528\u4e8e4D\u6d41MRI\uff0c\u8fd8\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u533b\u5b66\u6210\u50cf\u5e94\u7528\u3002", "keywords": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60, \u5e73\u9762\u91cd\u65b0\u683c\u5f0f\u5316, A3C\u7b97\u6cd5, 4D\u6d41MRI, \u7075\u6d3b\u5750\u6807\u7cfb"}}
{"id": "2506.00759", "pdf": "https://arxiv.org/pdf/2506.00759", "abs": "https://arxiv.org/abs/2506.00759", "authors": ["Wenshuo Dong", "Qingsong Yang", "Shu Yang", "Lijie Hu", "Meng Ding", "Wanyu Lin", "Tianhang Zheng", "Di Wang"], "title": "Understanding and Mitigating Cross-lingual Privacy Leakage via Language-specific and Universal Privacy Neurons", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) trained on massive data capture rich information\nembedded in the training data. However, this also introduces the risk of\nprivacy leakage, particularly involving personally identifiable information\n(PII). Although previous studies have shown that this risk can be mitigated\nthrough methods such as privacy neurons, they all assume that both the\n(sensitive) training data and user queries are in English. We show that they\ncannot defend against the privacy leakage in cross-lingual contexts: even if\nthe training data is exclusively in one language, these (private) models may\nstill reveal private information when queried in another language. In this\nwork, we first investigate the information flow of cross-lingual privacy\nleakage to give a better understanding. We find that LLMs process private\ninformation in the middle layers, where representations are largely shared\nacross languages. The risk of leakage peaks when converted to a\nlanguage-specific space in later layers. Based on this, we identify\nprivacy-universal neurons and language-specific privacy neurons.\nPrivacy-universal neurons influence privacy leakage across all languages, while\nlanguage-specific privacy neurons are only related to specific languages. By\ndeactivating these neurons, the cross-lingual privacy leakage risk is reduced\nby 23.3%-31.6%.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u505c\u7528\u9690\u79c1\u901a\u7528\u795e\u7ecf\u5143\u548c\u8bed\u8a00\u7279\u5b9a\u9690\u79c1\u795e\u7ecf\u5143\uff0c\u53ef\u964d\u4f4e\u98ce\u9669\u3002", "motivation": "\u63a2\u7a76LLMs\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u82f1\u8bed\u7684\u5c40\u9650\u6027\u3002", "method": "\u5206\u6790\u8de8\u8bed\u8a00\u9690\u79c1\u6cc4\u9732\u7684\u4fe1\u606f\u6d41\uff0c\u8bc6\u522b\u9690\u79c1\u901a\u7528\u795e\u7ecf\u5143\u548c\u8bed\u8a00\u7279\u5b9a\u795e\u7ecf\u5143\uff0c\u5e76\u901a\u8fc7\u505c\u7528\u8fd9\u4e9b\u795e\u7ecf\u5143\u964d\u4f4e\u98ce\u9669\u3002", "result": "\u505c\u7528\u76f8\u5173\u795e\u7ecf\u5143\u540e\uff0c\u8de8\u8bed\u8a00\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u964d\u4f4e23.3%-31.6%\u3002", "conclusion": "LLMs\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u5904\u7406\u795e\u7ecf\u5143\u53ef\u6709\u6548\u7f13\u89e3\u95ee\u9898\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u9690\u79c1\u6cc4\u9732, \u591a\u8bed\u8a00, \u795e\u7ecf\u5143"}}
{"id": "2506.01926", "pdf": "https://arxiv.org/pdf/2506.01926", "abs": "https://arxiv.org/abs/2506.01926", "authors": ["Joey Skaf", "Luis Ibanez-Lissen", "Robert McCarthy", "Connor Watts", "Vasil Georgiv", "Hannes Whittingham", "Lorena Gonzalez-Manzano", "David Lindner", "Cameron Tice", "Edward James Young", "Puria Radmard"], "title": "Large language models can learn and generalize steganographic chain-of-thought under process supervision", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "10 pages main text, 3 figures main text, 15 pages supplementary\n  material, 1 figure supplementary material, submitted to NeurIPS 2025", "summary": "Chain-of-thought (CoT) reasoning not only enhances large language model\nperformance but also provides critical insights into decision-making processes,\nmarking it as a useful tool for monitoring model intent and planning. By\nproactively preventing models from acting on CoT indicating misaligned or\nharmful intent, CoT monitoring can be used to reduce risks associated with\ndeploying models. However, developers may be incentivized to train away the\nappearance of harmful intent from CoT traces, by either customer preferences or\nregulatory requirements. Recent works have shown that banning mention of a\nspecific example of reward hacking, which may be done either to make CoT\npresentable to users or as a naive attempt to prevent the behavior, causes\nobfuscation of the undesired reasoning traces but the persistence of the\nundesired behavior. Such obfuscation threatens the reliability of CoT\nmonitoring. However, obfuscation of reasoning can be due to its internalization\nto latent space computation, or its encoding within the CoT. Here, we provide\nan extension to these results. First, we show that penalizing the use of\nspecific strings within load-bearing reasoning traces causes models to\nsubstitute alternative strings. Crucially, this does not alter the underlying\nmethod by which the model performs the task, demonstrating that the model can\nlearn to steganographically encode its reasoning. We further demonstrate that\nmodels can generalize an encoding scheme. When the penalized strings belong to\nan overarching class, the model learns not only to substitute strings seen in\ntraining, but also develops a general encoding scheme for all members of the\nclass which it can apply to held-out testing strings.", "AI": {"tldr": "CoT\u76d1\u63a7\u53ef\u964d\u4f4e\u6a21\u578b\u98ce\u9669\uff0c\u4f46\u5f00\u53d1\u8005\u53ef\u80fd\u56e0\u5ba2\u6237\u6216\u6cd5\u89c4\u8981\u6c42\u800c\u9690\u85cf\u6709\u5bb3\u610f\u56fe\uff0c\u5bfc\u81f4\u903b\u8f91\u6a21\u7cca\u5316\u3002\u7814\u7a76\u8868\u660e\uff0c\u60e9\u7f5a\u7279\u5b9a\u5b57\u7b26\u4e32\u4f1a\u4fc3\u4f7f\u6a21\u578b\u66ff\u4ee3\u7f16\u7801\uff0c\u4f46\u5e95\u5c42\u903b\u8f91\u4e0d\u53d8\uff0c\u4e14\u80fd\u63a8\u5e7f\u7f16\u7801\u65b9\u6848\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u8ba8CoT\u76d1\u63a7\u7684\u6709\u6548\u6027\u53ca\u5f00\u53d1\u8005\u9690\u85cf\u6709\u5bb3\u610f\u56fe\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u7f16\u7801\u66ff\u4ee3\u903b\u8f91\u4fdd\u6301\u51b3\u7b56\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u60e9\u7f5a\u7279\u5b9a\u5b57\u7b26\u4e32\u89c2\u5bdf\u6a21\u578b\u66ff\u4ee3\u884c\u4e3a\uff0c\u6d4b\u8bd5\u5176\u7f16\u7801\u80fd\u529b\u53ca\u6cdb\u5316\u6027\u3002", "result": "\u6a21\u578b\u80fd\u5b66\u4e60\u66ff\u4ee3\u7f16\u7801\u65b9\u6848\u4e14\u6cdb\u5316\u5230\u65b0\u5b57\u7b26\u4e32\uff0c\u5e95\u5c42\u903b\u8f91\u4e0d\u53d8\u3002", "conclusion": "\u9690\u85cf\u6709\u5bb3\u610f\u56fe\u5bfc\u81f4\u903b\u8f91\u6a21\u7cca\u5316\uff0c\u6a21\u578b\u4ecd\u80fd\u901a\u8fc7\u7f16\u7801\u7ef4\u6301\u884c\u4e3a\uff0c\u5a01\u80c1CoT\u76d1\u63a7\u53ef\u9760\u6027\u3002", "keywords": "Chain-of-thought, \u6a21\u578b\u76d1\u63a7, \u6709\u5bb3\u610f\u56fe, \u7f16\u7801\u6cdb\u5316, \u903b\u8f91\u6a21\u7cca\u5316"}}
{"id": "2506.00731", "pdf": "https://arxiv.org/pdf/2506.00731", "abs": "https://arxiv.org/abs/2506.00731", "authors": ["Binghang Lu", "Changhong Mou", "Guang Lin"], "title": "MoPINNEnKF: Iterative Model Inference using generic-PINN-based ensemble Kalman filter", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Physics-informed neural networks (PINNs) have emerged as a powerful tool for\nsolving forward and inverse problems involving partial differential equations\n(PDEs) by incorporating physical laws into the training process. However, the\nperformance of PINNs is often hindered in real-world scenarios involving noisy\nobservational data and missing physics, particularly in inverse problems. In\nthis work, we propose an iterative multi-objective PINN ensemble Kalman filter\n(MoPINNEnKF) framework that improves the robustness and accuracy of PINNs in\nboth forward and inverse problems by using the \\textit{ensemble Kalman filter}\nand the \\textit{non-dominated sorting genetic algorithm} III (NSGA-III).\nSpecifically, NSGA-III is used as a multi-objective optimizer that can generate\nvarious ensemble members of PINNs along the optimal Pareto front, while\naccounting the model uncertainty in the solution space. These ensemble members\nare then utilized within the EnKF to assimilate noisy observational data. The\nEnKF's analysis is subsequently used to refine the data loss component for\nretraining the PINNs, thereby iteratively updating their parameters. The\niterative procedure generates improved solutions to the PDEs. The proposed\nmethod is tested on two benchmark problems: the one-dimensional viscous Burgers\nequation and the time-fractional mixed diffusion-wave equation (TFMDWE). The\nnumerical results show it outperforms standard PINNs in handling noisy data and\nmissing physics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fed\u4ee3\u591a\u76ee\u6807PINN\u96c6\u6210\u5361\u5c14\u66fc\u6ee4\u6ce2\u6846\u67b6\uff08MoPINNEnKF\uff09\uff0c\u901a\u8fc7\u7ed3\u5408NSGA-III\u548cEnKF\u63d0\u5347PINNs\u5728\u566a\u58f0\u6570\u636e\u548c\u4e0d\u5b8c\u6574\u7269\u7406\u6a21\u578b\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "PINNs\u5728\u89e3\u51b3\u6d89\u53ca\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u6b63\u95ee\u9898\u548c\u53cd\u95ee\u9898\u65f6\u56e0\u566a\u58f0\u6570\u636e\u548c\u7f3a\u5931\u7269\u7406\u6a21\u578b\u800c\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u6539\u8fdb\u5176\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u5229\u7528NSGA-III\u4f5c\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u5668\u751f\u6210PINN\u7684\u96c6\u6210\u6210\u5458\uff0c\u518d\u901a\u8fc7EnKF\u540c\u5316\u566a\u58f0\u6570\u636e\u5e76\u66f4\u65b0PINN\u53c2\u6570\uff0c\u8fed\u4ee3\u4f18\u5316\u89e3\u3002", "result": "\u5728Burgers\u65b9\u7a0b\u548cTFMDWE\u7b49\u57fa\u51c6\u95ee\u9898\u4e0a\uff0cMoPINNEnKF\u4f18\u4e8e\u6807\u51c6PINNs\uff0c\u5c24\u5176\u5728\u566a\u58f0\u6570\u636e\u548c\u7269\u7406\u6a21\u578b\u7f3a\u5931\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "MoPINNEnKF\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86PINNs\u5904\u7406\u590d\u6742\u5b9e\u9645\u95ee\u9898\u7684\u80fd\u529b\uff0c\u4e3a\u566a\u58f0\u6570\u636e\u548c\u4e0d\u5b8c\u6574\u7269\u7406\u6a21\u578b\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "PINNs, NSGA-III, EnKF, multi-objective optimization, PDEs, inverse problems"}}
{"id": "2506.00773", "pdf": "https://arxiv.org/pdf/2506.00773", "abs": "https://arxiv.org/abs/2506.00773", "authors": ["Boheng Sheng", "Jiacheng Yao", "Meicong Zhang", "Guoxiu He"], "title": "Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often struggle to accurately read and comprehend\nextremely long texts. Current methods for improvement typically rely on\nsplitting long contexts into fixed-length chunks. However, fixed truncation\nrisks separating semantically relevant content, leading to ambiguity and\ncompromising accurate understanding. To overcome this limitation, we propose a\nstraightforward approach for dynamically separating and selecting chunks of\nlong context, facilitating a more streamlined input for LLMs. In particular, we\ncompute semantic similarities between adjacent sentences, using lower\nsimilarities to adaptively divide long contexts into variable-length chunks. We\nfurther train a question-aware classifier to select sensitive chunks that are\ncritical for answering specific questions. Experimental results on both\nsingle-hop and multi-hop question-answering benchmarks show that the proposed\napproach consistently outperforms strong baselines. Notably, it maintains\nrobustness across a wide range of input lengths, handling sequences of up to\n256k tokens. Our datasets and code are available at the following link:\nhttps://github.com/ECNU-Text-Computing/DCS", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5206\u5272\u548c\u9009\u62e9\u957f\u6587\u672c\u5757\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u5ea6\u81ea\u9002\u5e94\u5212\u5206\uff0c\u5e76\u8bad\u7ec3\u95ee\u9898\u611f\u77e5\u5206\u7c7b\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u957f\u6587\u672c\u5904\u7406\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u6587\u672c\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56fa\u5b9a\u957f\u5ea6\u7684\u5206\u5757\u65b9\u6cd5\u5bb9\u6613\u5206\u5272\u8bed\u4e49\u76f8\u5173\u5185\u5bb9\uff0c\u5bfc\u81f4\u7406\u89e3\u6a21\u7cca\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u76f8\u90bb\u53e5\u5b50\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff0c\u52a8\u6001\u5212\u5206\u957f\u6587\u672c\u4e3a\u53d8\u957f\u5757\uff0c\u540c\u65f6\u8bad\u7ec3\u95ee\u9898\u611f\u77e5\u5206\u7c7b\u5668\u4ee5\u9009\u62e9\u5173\u952e\u5757\u3002", "result": "\u5728\u5355\u8df3\u548c\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e00\u81f4\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u80fd\u5904\u7406\u591a\u8fbe256k\u6807\u8bb0\u7684\u5e8f\u5217\u3002", "conclusion": "\u52a8\u6001\u5206\u5757\u548c\u95ee\u9898\u611f\u77e5\u9009\u62e9\u663e\u8457\u63d0\u5347\u4e86LLM\u5bf9\u957f\u6587\u672c\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5177\u5907\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u8bed\u4e49\u76f8\u4f3c\u5ea6, \u52a8\u6001\u5206\u5757, \u95ee\u9898\u611f\u77e5\u5206\u7c7b\u5668"}}
{"id": "2506.01934", "pdf": "https://arxiv.org/pdf/2506.01934", "abs": "https://arxiv.org/abs/2506.01934", "authors": ["Yiqun Yao", "Xiang Li", "Xin Jiang", "Xuezhi Fang", "Naitong Yu", "Aixin Sun", "Yequan Wang"], "title": "RoboEgo System Card: An Omnimodal Model with Native Full Duplexity", "categories": ["cs.AI"], "comment": null, "summary": "Humans naturally process real-world multimodal information in a full-duplex\nmanner. In artificial intelligence, replicating this capability is essential\nfor advancing model development and deployment, particularly in embodied\ncontexts. The development of multimodal models faces two primary challenges:\n(1) effectively handling more than three modalities-such as vision, audio, and\ntext; and (2) delivering full-duplex responses to rapidly evolving human\ninstructions. To facilitate research on models that support both omnimodal\nprocessing and full duplexity, we present RoboEgo (alias: FLM-Ego), a unified\nmodel system designed to address both challenges. RoboEgo incorporates a\nbackbone architecture and algorithms that natively support full duplexity,\nachieving a theoretical duplex latency of 80 ms. In streaming visually grounded\nconversations under real-world conditions, RoboEgo exhibits superior\nresponsiveness and speech naturalness, while maintaining comparable content\nqualities to state-of-the-art semi-duplex omnimodal models-a feat previously\nconsidered unattainable by native full-duplex systems.", "AI": {"tldr": "RoboEgo\uff08FLM-Ego\uff09\u662f\u4e00\u4e2a\u652f\u6301\u5168\u53cc\u5de5\u548c\u591a\u6a21\u6001\u5904\u7406\u7684\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u6a21\u6001\u5904\u7406\u548c\u5b9e\u65f6\u54cd\u5e94\u4eba\u7c7b\u6307\u4ee4\u7684\u6311\u6218\uff0c\u5176\u7406\u8bba\u53cc\u5de5\u5ef6\u8fdf\u4e3a80\u6beb\u79d2\uff0c\u5728\u5b9e\u9645\u5bf9\u8bdd\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u54cd\u5e94\u901f\u5ea6\u548c\u8bed\u97f3\u81ea\u7136\u5ea6\u3002", "motivation": "\u4eba\u7c7b\u5929\u751f\u4ee5\u5168\u53cc\u5de5\u65b9\u5f0f\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u8fd9\u5728\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u5b9e\u73b0\u5bf9\u6a21\u578b\u5f00\u53d1\u548c\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5177\u8eab\u4e0a\u4e0b\u6587\u4e2d\u3002", "method": "RoboEgo\u91c7\u7528\u4e86\u4e00\u79cd\u7edf\u4e00\u6a21\u578b\u7cfb\u7edf\uff0c\u5176\u6838\u5fc3\u67b6\u6784\u548c\u7b97\u6cd5\u539f\u751f\u652f\u6301\u5168\u53cc\u5de5\u5904\u7406\uff0c\u5b9e\u73b0\u4e8680\u6beb\u79d2\u7684\u7406\u8bba\u53cc\u5de5\u5ef6\u8fdf\u3002", "result": "\u5728\u5b9e\u65f6\u89c6\u89c9\u5bf9\u8bdd\u4e2d\uff0cRoboEgo\u5c55\u73b0\u51fa\u6bd4\u73b0\u6709\u534a\u53cc\u5de5\u591a\u6a21\u6001\u6a21\u578b\u66f4\u4f18\u7684\u54cd\u5e94\u901f\u5ea6\u548c\u8bed\u97f3\u81ea\u7136\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u4f3c\u7684\u5185\u5bb9\u8d28\u91cf\u3002", "conclusion": "RoboEgo\u7684\u6210\u529f\u8868\u660e\uff0c\u539f\u751f\u5168\u53cc\u5de5\u7cfb\u7edf\u5728\u591a\u6a21\u6001\u5904\u7406\u4e2d\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u54cd\u5e94\u6027\u548c\u81ea\u7136\u5ea6\uff0c\u7a81\u7834\u4e86\u4ee5\u5f80\u7684\u6280\u672f\u9650\u5236\u3002", "keywords": "\u591a\u6a21\u6001\u5904\u7406,\u5168\u53cc\u5de5,RoboEgo,\u4eba\u5de5\u667a\u80fd,\u5177\u8eab\u4e0a\u4e0b\u6587"}}
{"id": "2506.00732", "pdf": "https://arxiv.org/pdf/2506.00732", "abs": "https://arxiv.org/abs/2506.00732", "authors": ["Caio Corro", "Mathieu Lacroix", "Joseph Le Roux"], "title": "Bregman Conditional Random Fields: Sequence Labeling with Parallelizable Inference Algorithms", "categories": ["cs.LG", "cs.CL"], "comment": "ACL 2025", "summary": "We propose a novel discriminative model for sequence labeling called Bregman\nconditional random fields (BCRF). Contrary to standard linear-chain conditional\nrandom fields, BCRF allows fast parallelizable inference algorithms based on\niterative Bregman projections. We show how such models can be learned using\nFenchel-Young losses, including extension for learning from partial labels.\nExperimentally, our approach delivers comparable results to CRF while being\nfaster, and achieves better results in highly constrained settings compared to\nmean field, another parallelizable alternative.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5e8f\u5217\u6807\u8bb0\u5224\u522b\u6a21\u578bBCRF\uff0c\u901a\u8fc7Bregman\u6295\u5f71\u5b9e\u73b0\u5feb\u901f\u5e76\u884c\u63a8\u65ad\uff0c\u6027\u80fd\u4e0eCRF\u76f8\u5f53\u4f46\u66f4\u5feb\uff0c\u5728\u7ea6\u675f\u6761\u4ef6\u4e0b\u4f18\u4e8e\u5e73\u5747\u573a\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7ebf\u6027\u94fe\u6761\u4ef6\u968f\u673a\u573a\uff08CRF\uff09\u63a8\u65ad\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5e76\u884c\u5316\u63a8\u65ad\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eBregman\u6295\u5f71\u7684\u5feb\u901f\u5e76\u884c\u63a8\u65ad\u7b97\u6cd5\uff0c\u91c7\u7528Fenchel-Young\u635f\u5931\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\uff0c\u652f\u6301\u90e8\u5206\u6807\u7b7e\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660eBCRF\u6027\u80fd\u4e0eCRF\u76f8\u5f53\u4f46\u901f\u5ea6\u66f4\u5feb\uff0c\u5728\u7ea6\u675f\u6761\u4ef6\u4e0b\u4f18\u4e8e\u5e73\u5747\u573a\u65b9\u6cd5\u3002", "conclusion": "BCRF\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u5e8f\u5217\u6807\u8bb0\u6a21\u578b\uff0c\u9002\u5408\u5e76\u884c\u5316\u8ba1\u7b97\u548c\u90e8\u5206\u6807\u7b7e\u5b66\u4e60\u573a\u666f\u3002", "keywords": "BCRF, \u6761\u4ef6\u968f\u673a\u573a, Bregman\u6295\u5f71, \u5e76\u884c\u63a8\u65ad, \u90e8\u5206\u6807\u7b7e\u5b66\u4e60"}}
{"id": "2506.00777", "pdf": "https://arxiv.org/pdf/2506.00777", "abs": "https://arxiv.org/abs/2506.00777", "authors": ["Md Tahmid Rahman Laskar", "Israt Jahan", "Elham Dolatabadi", "Chun Peng", "Enamul Hoque", "Jimmy Huang"], "title": "Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Main Conference)", "summary": "Large Language Models (LLMs) have demonstrated impressive performance in\nbiomedical relation extraction, even in zero-shot scenarios. However,\nevaluating LLMs in this task remains challenging due to their ability to\ngenerate human-like text, often producing synonyms or abbreviations of\ngold-standard answers, making traditional automatic evaluation metrics\nunreliable. On the other hand, while human evaluation is more reliable, it is\ncostly and time-consuming, making it impractical for real-world applications.\nThis paper investigates the use of LLMs-as-the-Judge as an alternative\nevaluation method for biomedical relation extraction. We benchmark 8 LLMs as\njudges to evaluate the responses generated by 5 other LLMs across 3 biomedical\nrelation extraction datasets. Unlike other text-generation tasks, we observe\nthat LLM-based judges perform quite poorly (usually below 50% accuracy) in the\nbiomedical relation extraction task. Our findings reveal that it happens mainly\nbecause relations extracted by LLMs do not adhere to any standard format. To\naddress this, we propose structured output formatting for LLM-generated\nresponses that helps LLM-Judges to improve their performance by about 15% (on\naverage). We also introduce a domain adaptation technique to further enhance\nLLM-Judge performance by effectively transferring knowledge between datasets.\nWe release both our human-annotated and LLM-annotated judgment data (36k\nsamples in total) for public use here:\nhttps://github.com/tahmedge/llm_judge_biomedical_re.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u8bc4\u4f30\u5668\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u7269\u533b\u5b66\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\uff0c\u53d1\u73b0\u5176\u6548\u679c\u4e0d\u4f73\uff08\u51c6\u786e\u7387\u4f4e\u4e8e50%\uff09\uff0c\u5e76\u63d0\u51fa\u7ed3\u6784\u5316\u8f93\u51fa\u683c\u5f0f\u548c\u9886\u57df\u9002\u5e94\u6280\u672f\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u7531\u4e8eLLMs\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u7684\u6587\u672c\uff0c\u4f20\u7edf\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u4e0d\u53ef\u9760\uff0c\u800c\u4eba\u5de5\u8bc4\u4f30\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u56e0\u6b64\u63a2\u7d22LLMs\u4f5c\u4e3a\u8bc4\u4f30\u5668\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8ba98\u4e2aLLMs\u4f5c\u4e3a\u6cd5\u5b98\u8bc4\u4f305\u4e2a\u5176\u4ed6LLMs\u57283\u4e2a\u751f\u7269\u533b\u5b66\u5173\u7cfb\u62bd\u53d6\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u7ed3\u6784\u5316\u8f93\u51fa\u683c\u5f0f\u548c\u9886\u57df\u9002\u5e94\u6280\u672f\u3002", "result": "LLM-\u6cd5\u5b98\u6027\u80fd\u8f83\u5dee\uff08\u5e73\u5747\u4f4e\u4e8e50%\u51c6\u786e\u7387\uff09\uff0c\u4f46\u7ed3\u6784\u5316\u8f93\u51fa\u683c\u5f0f\u4f7f\u5176\u6027\u80fd\u63d0\u5347\u7ea615%\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u7ed3\u6784\u5316\u8f93\u51fa\u548c\u9886\u57df\u9002\u5e94\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347LLMs\u4f5c\u4e3a\u8bc4\u4f30\u5668\u7684\u6027\u80fd\uff0c\u5e76\u516c\u5f00\u4e86\u6807\u6ce8\u6570\u636e\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3001\u751f\u7269\u533b\u5b66\u5173\u7cfb\u62bd\u53d6\u3001\u8bc4\u4f30\u65b9\u6cd5\u3001\u7ed3\u6784\u5316\u8f93\u51fa\u3001\u9886\u57df\u9002\u5e94"}}
{"id": "2506.00002", "pdf": "https://arxiv.org/pdf/2506.00002", "abs": "https://arxiv.org/abs/2506.00002", "authors": ["Hao Mark Chen", "Zehuan Zhang", "Wanru Zhao", "Nicholas Lane", "Hongxiang Fan"], "title": "Advancing AI-assisted Hardware Design with Hierarchical Decentralized Training and Personalized Inference-Time Optimization", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.LG"], "comment": null, "summary": "Recent years have witnessed a significant increase in the adoption of AI\ntechniques to enhance electronic design automation. In particular, the\nemergence of Large Language Models (LLMs) has sparked significant interest in\nLLM-assisted hardware design generation, spanning applications from classical\ndigital circuits to quantum computing. Despite substantial progress in this\ndirection, the quality of LLM-generated hardware design still cannot meet the\nrequirements for practical deployment. In this work, we identify three critical\nchallenges hindering the development of LLM-assisted hardware design\ngeneration: 1) limited data availability, 2) varied data quality, 3) inadequate\ninference-time efficiency. To address these fundamental challenges, this paper\nintroduces a two-stage framework for AI-assisted hardware design by exploring\ndecentralized training and personalized inference. In the first stage, we\npropose to harness private domain design sources through a hierarchical\ndecentralized training mechanism that addresses data-sharing constraints. To\nmitigate the impact of low-quality data, we identify optimization opportunities\nin hardware generation tasks, using user-defined metrics for model aggregation.\nThe second stage focuses on client personalization to enhance both speed and\nquality. We introduce a new metric, Trueput, to analyze LLM-assisted hardware\ngeneration efficiency. To optimize Trueput, we implement personalized\ninference-time acceleration and customized sampling strategies. Evaluating both\nclassical and quantum benchmarks, our experimental results demonstrate that the\nproposed two-stage framework can significantly improve the model capability for\nhardware design generation. As orthogonal enhancements to existing methods, our\nframework can achieve $33\\% \\sim 50\\%$ semantic accuracy improvement and $2.3$\ntimes speedup, depending on the difficulty of the generation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u548c\u4e2a\u6027\u5316\u63a8\u7406\uff0c\u89e3\u51b3LLM\u8f85\u52a9\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u4e09\u5927\u6311\u6218\uff1a\u6570\u636e\u6709\u9650\u3001\u8d28\u91cf\u53c2\u5dee\u548c\u63a8\u7406\u6548\u7387\u4e0d\u8db3\u3002\u5b9e\u9a8c\u663e\u793a\u8be5\u6846\u67b6\u8bed\u4e49\u51c6\u786e\u6027\u63d0\u534733%~50%\uff0c\u901f\u5ea6\u63d0\u53472.3\u500d\u3002", "motivation": "\u5f53\u524dLLM\u8f85\u52a9\u786c\u4ef6\u8bbe\u8ba1\u7684\u8d28\u91cf\u5c1a\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u90e8\u7f72\u9700\u6c42\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u6570\u636e\u53ef\u7528\u6027\u3001\u8d28\u91cf\u548c\u63a8\u7406\u6548\u7387\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u5c42\u6b21\u5316\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u4ee5\u5229\u7528\u79c1\u6709\u9886\u57df\u6570\u636e\u5e76\u4f18\u5316\u805a\u5408\uff1b2) \u4e2a\u6027\u5316\u63a8\u7406\u52a0\u901f\u548c\u91c7\u6837\u7b56\u7565\u63d0\u5347\u6548\u7387\u3002", "result": "\u6846\u67b6\u5728\u7ecf\u5178\u548c\u91cf\u5b50\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u8bed\u4e49\u51c6\u786e\u6027\uff0833%~50%\uff09\u548c\u901f\u5ea6\uff082.3\u500d\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u8f85\u52a9\u786c\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u3002", "keywords": "AI\u8f85\u52a9\u786c\u4ef6\u8bbe\u8ba1, \u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b, \u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3, \u4e2a\u6027\u5316\u63a8\u7406"}}
{"id": "2506.00744", "pdf": "https://arxiv.org/pdf/2506.00744", "abs": "https://arxiv.org/abs/2506.00744", "authors": ["Kazuki Irie", "Morris Yau", "Samuel J. Gershman"], "title": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear Transformers", "categories": ["cs.LG"], "comment": null, "summary": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with dynamic synaptic memory through fast-weight\nprogramming (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u8bb0\u5fc6\u67b6\u6784\uff0c\u7ed3\u5408KV\u8bb0\u5fc6\u548cFW\u8bb0\u5fc6\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u4e09\u79cd\u65b9\u6cd5\u5b9e\u73b0\u4e92\u8865\uff0c\u5e76\u5728\u8bed\u8a00\u5efa\u6a21\u3001\u68c0\u7d22\u4efb\u52a1\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "KV\u8bb0\u5fc6\u7cbe\u786e\u68c0\u7d22\u4f46\u590d\u6742\u5ea6\u9ad8\uff0cFW\u8bb0\u5fc6\u652f\u6301\u957f\u5e8f\u5217\u4f46\u727a\u7272\u7cbe\u786e\u6027\uff0c\u4e24\u8005\u4e92\u8865\u4f46\u5404\u6709\u5c40\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6df7\u5408\u67b6\u6784\u6765\u7ed3\u5408\u5176\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u5e76\u6bd4\u8f83\u4e86\u4e09\u79cd\u6df7\u5408KV\u8bb0\u5fc6\u548cFW\u8bb0\u5fc6\u7684\u65b9\u6cd5\uff0c\u5206\u522b\u7528\u4e8e\u8bed\u8a00\u5efa\u6a21\u3001\u68c0\u7d22\u4efb\u52a1\u548c\u5408\u6210\u7b97\u6cd5\u4efb\u52a1\u3002", "result": "\u901a\u8fc7\u8bad\u7ec33.4\u4ebf\u548c13\u4ebf\u53c2\u6570\u6a21\u578b\u53ca\u5408\u6210\u4efb\u52a1\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6df7\u5408\u67b6\u6784\u5728\u514b\u670d\u5355\u4e00\u7ec4\u4ef6\u5c40\u9650\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bbe\u8ba1\u7684\u6df7\u5408\u8bb0\u5fc6\u7cfb\u7edf\u80fd\u591f\u514b\u670d\u5355\u4e00\u7ec4\u4ef6\u7684\u5c40\u9650\uff0c\u4e3a\u795e\u7ecf\u8bb0\u5fc6\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "keywords": "\u6df7\u5408\u8bb0\u5fc6\u67b6\u6784, KV\u8bb0\u5fc6, FW\u8bb0\u5fc6, \u8bed\u8a00\u5efa\u6a21, \u5f3a\u5316\u5b66\u4e60"}}
{"id": "2506.00783", "pdf": "https://arxiv.org/pdf/2506.00783", "abs": "https://arxiv.org/abs/2506.00783", "authors": ["Rong Wu", "Pinlong Cai", "Jianbiao Mei", "Licheng Wen", "Tao Hu", "Xuemeng Yang", "Daocheng Fu", "Botian Shi"], "title": "KG-TRACES: Enhancing Large Language Models with Knowledge Graph-constrained Trajectory Reasoning and Attribution Supervision", "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 13 figures", "summary": "Large language models (LLMs) have made remarkable strides in various natural\nlanguage processing tasks, but their performance on complex reasoning problems\nremains hindered by a lack of explainability and trustworthiness. This issue,\noften manifesting as hallucinations or unattributable reasoning processes,\nlimits their applicability in complex reasoning scenarios. To address this, we\npropose Knowledge Graph-constrained Trajectory Reasoning Attribution and Chain\nExplanation Supervision (KG-TRACES), a novel framework that enhances the\nreasoning ability of LLMs through explicit supervision over reasoning paths and\nprocesses. KG-TRACES jointly supervises the model to: (1) predict symbolic\nrelation paths, (2) predict full triple-level reasoning paths, and (3) generate\nattribution-aware reasoning processes grounded in the reasoning paths. At\ninference phase, the model adapts to both KG-available and KG-unavailable\nscenarios, retrieving reasoning paths from a KG when possible or predicting\nplausible reasoning paths with only intrinsic knowledge when not. This design\nenables the model to reason in an explainable and source-attributable pattern.\nThrough extensive experiments on complex reasoning tasks, we demonstrate that\nKG-TRACES significantly outperforms existing SOTA: it improves Hits@1 by 1.6%\nand F1 by 4.7% on WebQSP, and achieves improvements of 4.8% in Hits@1 and 2.1%\nin F1 on CWQ. Moreover, we show its transferability to specialized domains such\nas medicine. By visualizing the intermediate steps of reasoning processes, we\nfurther show that the explicit supervision introduced by KG-TRACES leads to\nmore stable and goal-directed reasoning processes, aligning closely with\ncorrect answers. Code is available at https://github.com/Edaizi/KG-TRACES.", "AI": {"tldr": "KG-TRACES\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u76d1\u7763LLMs\u7684\u63a8\u7406\u8def\u5f84\u548c\u8fc7\u7a0b\uff0c\u63d0\u5347\u4e86\u5176\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u63d0\u51faKG-TRACES\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u7763\u63a8\u7406\u8def\u5f84\u548c\u8fc7\u7a0b\uff08\u5305\u62ec\u5173\u7cfb\u8def\u5f84\u9884\u6d4b\u3001\u4e09\u5143\u7ec4\u7ea7\u63a8\u7406\u8def\u5f84\u9884\u6d4b\u548c\u5f52\u56e0\u611f\u77e5\u63a8\u7406\u751f\u6210\uff09\uff0c\u589e\u5f3aLLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728WebQSP\u548cCWQ\u7b49\u6570\u636e\u96c6\u4e0a\uff0cKG-TRACES\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08Hits@1\u548cF1\u5747\u6709\u63d0\u5347\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u533b\u5b66\u7b49\u4e13\u95e8\u9886\u57df\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "KG-TRACES\u901a\u8fc7\u663e\u5f0f\u76d1\u7763\u63a8\u7406\u8def\u5f84\u548c\u8fc7\u7a0b\uff0c\u63d0\u5347\u4e86LLMs\u7684\u63a8\u7406\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u5176\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u590d\u6742\u63a8\u7406, \u77e5\u8bc6\u56fe\u8c31, \u53ef\u89e3\u91ca\u6027, KG-TRACES"}}
{"id": "2506.00004", "pdf": "https://arxiv.org/pdf/2506.00004", "abs": "https://arxiv.org/abs/2506.00004", "authors": ["J. Luquin", "C. Mackin", "S. Ambrogio", "A. Chen", "F. Baldi", "G. Miralles", "M. J. Rasch", "J. B\u00fcchel", "M. Lalwani", "W. Ponghiran", "P. Solomon", "H. Tsai", "G. W. Burr", "P. Narayanan"], "title": "Rapid yet accurate Tile-circuit and device modeling for Analog In-Memory Computing", "categories": ["cs.AR", "cs.AI", "cs.ET"], "comment": null, "summary": "Analog In-Memory Compute (AIMC) can improve the energy efficiency of Deep\nLearning by orders of magnitude. Yet analog-domain device and circuit\nnon-idealities -- within the analog ``Tiles'' performing Matrix-Vector Multiply\n(MVM) operations -- can degrade neural-network task accuracy. We quantify the\nimpact of low-level distortions and noise, and develop a mathematical model for\nMultiply-ACcumulate (MAC) operations mapped to analog tiles.\nInstantaneous-current IR-drop (the most significant circuit non-ideality), and\nADC quantization effects are fully captured by this model, which can predict\nMVM tile-outputs both rapidly and accurately, as compared to much slower\nrigorous circuit simulations. A statistical model of PCM read noise at\nnanosecond timescales is derived from -- and matched against -- experimental\nmeasurements. We integrate these (statistical) device and (deterministic)\ncircuit effects into a PyTorch-based framework to assess the accuracy impact on\nthe BERT and ALBERT Transformer networks. We show that hardware-aware\nfine-tuning using simple Gaussian noise provides resilience against ADC\nquantization and PCM read noise effects, but is less effective against IR-drop.\nThis is because IR-drop -- although deterministic -- is non-linear, is changing\nsignificantly during the time-integration window, and is ultimately dependent\non all the excitations being introduced in parallel into the analog tile. The\napparent inability of simple Gaussian noise applied during training to properly\nprepare a DNN network for IR-drop during inference implies that more complex\ntraining approaches -- incorporating advances such as the Tile-circuit model\nintroduced here -- will be critical for resilient deployment of large neural\nnetworks onto AIMC hardware.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6a21\u62df\u5185\u5b58\u8ba1\u7b97(AIMC)\u4e2d\u7684\u8bbe\u5907\u548c\u7535\u8def\u975e\u7406\u60f3\u6027\u5bf9\u795e\u7ecf\u7f51\u7edc\u51c6\u786e\u6027\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u5b66\u6a21\u578b\u6765\u9884\u6d4b\u77e9\u9635-\u5411\u91cf\u4e58(MVM)\u64cd\u4f5c\u7684\u8f93\u51fa\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u7684\u80fd\u6548\uff0c\u6a21\u62df\u5185\u5b58\u8ba1\u7b97(AIMC)\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5176\u8bbe\u5907\u548c\u7535\u8def\u7684\u975e\u7406\u60f3\u6027\u4f1a\u964d\u4f4e\u795e\u7ecf\u7f51\u7edc\u7684\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u91cf\u5316\u8fd9\u4e9b\u5f71\u54cd\u5e76\u627e\u5230\u89e3\u51b3\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u6570\u5b66\u6a21\u578b\u6765\u6355\u83b7\u77ac\u65f6\u7535\u6d41IR-drop\u548cADC\u91cf\u5316\u6548\u5e94\uff0c\u5e76\u7ed3\u5408\u5b9e\u9a8c\u6d4b\u91cf\u6570\u636e\u63a8\u5bfc\u51faPCM\u8bfb\u53d6\u566a\u58f0\u7684\u7edf\u8ba1\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u6a21\u578b\u88ab\u96c6\u6210\u5230PyTorch\u6846\u67b6\u4e2d\uff0c\u7528\u4e8e\u8bc4\u4f30\u5176\u5bf9BERT\u548cALBERT Transformer\u7f51\u7edc\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f7f\u7528\u7b80\u5355\u9ad8\u65af\u566a\u58f0\u7684\u786c\u4ef6\u611f\u77e5\u5fae\u8c03\u53ef\u4ee5\u62b5\u6297ADC\u91cf\u5316\u548cPCM\u8bfb\u53d6\u566a\u58f0\uff0c\u4f46\u5bf9IR-drop\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3aIR-drop\u662f\u975e\u7ebf\u6027\u4e14\u52a8\u6001\u53d8\u5316\u7684\u3002", "conclusion": "\u8bba\u6587\u6307\u51fa\uff0c\u66f4\u590d\u6742\u7684\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982\u7ed3\u5408\u672c\u6587\u63d0\u51fa\u7684Tile-circuit\u6a21\u578b\uff09\u5bf9\u4e8e\u5728AIMC\u786c\u4ef6\u4e0a\u90e8\u7f72\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u81f3\u5173\u91cd\u8981\u3002", "keywords": "\u6a21\u62df\u5185\u5b58\u8ba1\u7b97, \u795e\u7ecf\u7f51\u7edc, \u8bbe\u5907\u975e\u7406\u60f3\u6027, \u7535\u8def\u566a\u58f0, BERT, ALBERT"}}
{"id": "2506.00756", "pdf": "https://arxiv.org/pdf/2506.00756", "abs": "https://arxiv.org/abs/2506.00756", "authors": ["Harvineet Singh", "Fan Xia", "Alexej Gossmann", "Andrew Chuang", "Julian C. Hong", "Jean Feng"], "title": "\"Who experiences large model decay and why?\" A Hierarchical Framework for Diagnosing Heterogeneous Performance Drift", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "13 pages, 9 figures, 8 tables, 18 pages appendix. To be published in\n  Proceedings of the 42nd International Conference on Machine Learning,\n  Vancouver, Canada. PMLR 267, 2025", "summary": "Machine learning (ML) models frequently experience performance degradation\nwhen deployed in new contexts. Such degradation is rarely uniform: some\nsubgroups may suffer large performance decay while others may not.\nUnderstanding where and how large differences in performance arise is critical\nfor designing targeted corrective actions that mitigate decay for the most\naffected subgroups while minimizing any unintended effects. Current approaches\ndo not provide such detailed insight, as they either (i) explain how average\nperformance shifts arise or (ii) identify adversely affected subgroups without\ninsight into how this occurred. To this end, we introduce a Subgroup-scanning\nHierarchical Inference Framework for performance drifT (SHIFT). SHIFT first\nasks \"Is there any subgroup with unacceptably large performance decay due to\ncovariate/outcome shifts?\" (Where?) and, if so, dives deeper to ask \"Can we\nexplain this using more detailed variable(subset)-specific shifts?\" (How?). In\nreal-world experiments, we find that SHIFT identifies interpretable subgroups\naffected by performance decay, and suggests targeted actions that effectively\nmitigate the decay.", "AI": {"tldr": "SHIFT\u6846\u67b6\u89e3\u51b3\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u65b0\u73af\u5883\u4e2d\u6027\u80fd\u4e0b\u964d\u7684\u4e0d\u5747\u5300\u95ee\u9898\uff0c\u901a\u8fc7\u8bc6\u522b\u53d7\u5f71\u54cd\u7684\u5b50\u7fa4\u5e76\u63d0\u4f9b\u9488\u5bf9\u6027\u63aa\u65bd\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u65b0\u73af\u5883\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u5f80\u5f80\u4e0d\u5747\u5300\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u8be6\u7ec6\u7684\u5b50\u7fa4\u6027\u80fd\u4e0b\u964d\u539f\u56e0\u548c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSHIFT\u6846\u67b6\uff0c\u9996\u5148\u8bc6\u522b\u6027\u80fd\u4e0b\u964d\u4e25\u91cd\u7684\u5b50\u7fa4\uff0c\u7136\u540e\u6df1\u5165\u5206\u6790\u539f\u56e0\uff0c\u5e76\u63d0\u4f9b\u9488\u5bf9\u6027\u63aa\u65bd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSHIFT\u80fd\u8bc6\u522b\u53ef\u89e3\u91ca\u7684\u5b50\u7fa4\u5e76\u6709\u6548\u7f13\u89e3\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "SHIFT\u901a\u8fc7\u5206\u5c42\u63a8\u65ad\u6846\u67b6\uff0c\u4e3a\u4e0d\u5747\u5300\u6027\u80fd\u4e0b\u964d\u63d0\u4f9b\u4e86\u89e3\u91ca\u548c\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u673a\u5668\u5b66\u4e60,\u6027\u80fd\u4e0b\u964d,\u5b50\u7fa4\u5206\u6790,SHIFT\u6846\u67b6"}}
{"id": "2506.00784", "pdf": "https://arxiv.org/pdf/2506.00784", "abs": "https://arxiv.org/abs/2506.00784", "authors": ["Shaily Bhatt", "Tal August", "Maria Antoniak"], "title": "Research Borderlands: Analysing Writing Across Research Cultures", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Improving cultural competence of language technologies is important. However\nmost recent works rarely engage with the communities they study, and instead\nrely on synthetic setups and imperfect proxies of culture. In this work, we\ntake a human-centered approach to discover and measure language-based cultural\nnorms, and cultural competence of LLMs. We focus on a single kind of culture,\nresearch cultures, and a single task, adapting writing across research\ncultures. Through a set of interviews with interdisciplinary researchers, who\nare experts at moving between cultures, we create a framework of structural,\nstylistic, rhetorical, and citational norms that vary across research cultures.\nWe operationalise these features with a suite of computational metrics and use\nthem for (a) surfacing latent cultural norms in human-written research papers\nat scale; and (b) highlighting the lack of cultural competence of LLMs, and\ntheir tendency to homogenise writing. Overall, our work illustrates the\nefficacy of a human-centered approach to measuring cultural norms in\nhuman-written and LLM-generated texts.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u5e76\u8861\u91cf\u8bed\u8a00\u6587\u5316\u89c4\u8303\u53caLLMs\u7684\u6587\u5316\u80fd\u529b\uff0c\u805a\u7126\u7814\u7a76\u6587\u5316\u548c\u5199\u4f5c\u4efb\u52a1\uff0c\u901a\u8fc7\u4e13\u5bb6\u8bbf\u8c08\u5efa\u7acb\u6846\u67b6\uff0c\u63ed\u793aLLMs\u7684\u6587\u5316\u540c\u8d28\u5316\u95ee\u9898\u3002", "motivation": "\u63d0\u5347\u8bed\u8a00\u6280\u672f\u7684\u6587\u5316\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5e38\u5ffd\u7565\u793e\u533a\u53c2\u4e0e\uff0c\u4f9d\u8d56\u5408\u6210\u8bbe\u7f6e\u548c\u6587\u5316\u7684\u975e\u5b8c\u7f8e\u4ee3\u7406\u3002", "method": "\u901a\u8fc7\u91c7\u8bbf\u8de8\u5b66\u79d1\u7814\u7a76\u4e13\u5bb6\uff0c\u5efa\u7acb\u7ed3\u6784\u3001\u98ce\u683c\u3001\u4fee\u8f9e\u548c\u5f15\u7528\u89c4\u8303\u7684\u6846\u67b6\uff0c\u5e76\u7528\u8ba1\u7b97\u6307\u6807\u91cf\u5316\u3002", "result": "\u63ed\u793a\u4e86\u4eba\u7c7b\u7814\u7a76\u8bba\u6587\u4e2d\u7684\u6f5c\u5728\u6587\u5316\u89c4\u8303\uff0c\u5e76\u51f8\u663eLLMs\u7684\u6587\u5316\u80fd\u529b\u4e0d\u8db3\u53ca\u5176\u5199\u4f5c\u540c\u8d28\u5316\u503e\u5411\u3002", "conclusion": "\u4eba\u672c\u65b9\u6cd5\u80fd\u6709\u6548\u8861\u91cf\u4eba\u7c7b\u4e0eLLM\u751f\u6210\u6587\u672c\u4e2d\u7684\u6587\u5316\u89c4\u8303\u3002", "keywords": "\u6587\u5316\u80fd\u529b,\u8bed\u8a00\u6280\u672f,\u4eba\u7c7b\u4e2d\u5fc3,LLMs,\u7814\u7a76\u6587\u5316"}}
{"id": "2506.00009", "pdf": "https://arxiv.org/pdf/2506.00009", "abs": "https://arxiv.org/abs/2506.00009", "authors": ["Yihan Zhu", "Gang Liu", "Eric Inae", "Meng Jiang"], "title": "MolTextNet: A Two-Million Molecule-Text Dataset for Multimodal Molecular Learning", "categories": ["q-bio.BM", "cs.AI"], "comment": "21 pages, 13 figures, 10 tables", "summary": "Small molecules are essential to drug discovery, and graph-language models\nhold promise for learning molecular properties and functions from text.\nHowever, existing molecule-text datasets are limited in scale and\ninformativeness, restricting the training of generalizable multimodal models.\nWe present MolTextNet, a dataset of 2.5 million high-quality molecule-text\npairs designed to overcome these limitations. To construct it, we propose a\nsynthetic text generation pipeline that integrates structural features,\ncomputed properties, bioactivity data, and synthetic complexity. Using\nGPT-4o-mini, we create structured descriptions for 2.5 million molecules from\nChEMBL35, with text over 10 times longer than prior datasets. MolTextNet\nsupports diverse downstream tasks, including property prediction and structure\nretrieval. Pretraining CLIP-style models with Graph Neural Networks and\nModernBERT on MolTextNet yields improved performance, highlighting its\npotential for advancing foundational multimodal modeling in molecular science.\nOur dataset is available at\nhttps://huggingface.co/datasets/liuganghuggingface/moltextnet.", "AI": {"tldr": "MolTextNet\u662f\u4e00\u4e2a\u5305\u542b250\u4e07\u9ad8\u8d28\u91cf\u5206\u5b50-\u6587\u672c\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u548c\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5408\u6210\u6587\u672c\u751f\u6210\u7ba1\u9053\u6784\u5efa\uff0c\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u5206\u5b50-\u6587\u672c\u6570\u636e\u96c6\u89c4\u6a21\u548c\u4fe1\u606f\u91cf\u6709\u9650\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u9700\u8981\u66f4\u5927\u3001\u66f4\u4e30\u5bcc\u7684\u6570\u636e\u96c6\u6765\u63a8\u52a8\u5206\u5b50\u79d1\u5b66\u4e2d\u7684\u591a\u6a21\u6001\u5efa\u6a21\u3002", "method": "\u63d0\u51fa\u5408\u6210\u6587\u672c\u751f\u6210\u7ba1\u9053\uff0c\u7ed3\u5408\u7ed3\u6784\u7279\u5f81\u3001\u8ba1\u7b97\u6027\u8d28\u3001\u751f\u7269\u6d3b\u6027\u6570\u636e\u548c\u5408\u6210\u590d\u6742\u6027\uff0c\u5229\u7528GPT-4o-mini\u4e3aChEMBL35\u4e2d\u7684250\u4e07\u4e2a\u5206\u5b50\u751f\u6210\u957f\u6587\u672c\u63cf\u8ff0\u3002", "result": "MolTextNet\u751f\u6210\u7684\u6587\u672c\u957f\u5ea6\u662f\u73b0\u6709\u6570\u636e\u96c6\u768410\u500d\u4ee5\u4e0a\uff0c\u8bad\u7ec3\u51fa\u7684CLIP\u98ce\u683c\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u6709\u6240\u63d0\u5347\u3002", "conclusion": "MolTextNet\u5728\u63a8\u52a8\u5206\u5b50\u79d1\u5b66\u4e2d\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002", "keywords": "MolTextNet, \u5206\u5b50-\u6587\u672c\u6570\u636e\u96c6, \u591a\u6a21\u6001\u6a21\u578b, \u5408\u6210\u6587\u672c\u751f\u6210, \u5206\u5b50\u79d1\u5b66"}}
{"id": "2506.00764", "pdf": "https://arxiv.org/pdf/2506.00764", "abs": "https://arxiv.org/abs/2506.00764", "authors": ["Gautam Chandrasekaran", "Adam Klivans"], "title": "Learning Juntas under Markov Random Fields", "categories": ["cs.LG", "cs.DS"], "comment": null, "summary": "We give an algorithm for learning $O(\\log n)$ juntas in polynomial-time with\nrespect to Markov Random Fields (MRFs) in a smoothed analysis framework where\nonly the external field has been randomly perturbed. This is a broad\ngeneralization of the work of Kalai and Teng, who gave an algorithm that\nsucceeded with respect to smoothed product distributions (i.e., MRFs whose\ndependency graph has no edges). Our algorithm has two phases: (1) an\nunsupervised structure learning phase and (2) a greedy supervised learning\nalgorithm. This is the first example where algorithms for learning the\nstructure of an undirected graphical model lead to provably efficient\nalgorithms for supervised learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u5e73\u6ed1\u5206\u6790\u6846\u67b6\u4e0b\u5b66\u4e60Markov Random Fields (MRFs)\u4e2d$O(\text{log}n)$ juntas\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\uff0c\u6269\u5c55\u4e86Kalai\u548cTeng\u7684\u5de5\u4f5c\u3002", "motivation": "\u6269\u5c55\u4e86Kalai\u548cTeng\u5728\u5e73\u6ed1\u4e58\u79ef\u5206\u5e03\u4e0b\u7684\u7b97\u6cd5\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u66f4\u4e00\u822c\u7684MRFs\u3002", "method": "\u7b97\u6cd5\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u65e0\u76d1\u7763\u7ed3\u6784\u5b66\u4e60\u9636\u6bb5\u548c\u8d2a\u5fc3\u76d1\u7763\u5b66\u4e60\u9636\u6bb5\u3002", "result": "\u9996\u6b21\u5c55\u793a\u4e86\u65e0\u5411\u56fe\u6a21\u578b\u7ed3\u6784\u5b66\u4e60\u7b97\u6cd5\u53ef\u4ee5\u8f6c\u5316\u4e3a\u9ad8\u6548\u7684\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3aMRFs\u4e2d\u7684juntas\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u65b9\u6cd5\u3002", "keywords": "juntas, Markov Random Fields, \u5e73\u6ed1\u5206\u6790, \u56fe\u6a21\u578b\u5b66\u4e60"}}
{"id": "2506.00789", "pdf": "https://arxiv.org/pdf/2506.00789", "abs": "https://arxiv.org/abs/2506.00789", "authors": ["Yixiao Zeng", "Tianyu Cao", "Danqing Wang", "Xinran Zhao", "Zimeng Qiu", "Morteza Ziyadi", "Tongshuang Wu", "Lei Li"], "title": "RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances recency and factuality in\nanswers. However, existing evaluations rarely test how well these systems cope\nwith real-world noise, conflicting between internal and external retrieved\ncontexts, or fast-changing facts. We introduce Retrieval-Aware Robustness\nEvaluation (RARE), a unified framework and large-scale benchmark that jointly\nstress-tests query and document perturbations over dynamic, time-sensitive\ncorpora. One of the central features of RARE is a knowledge-graph-driven\nsynthesis pipeline (RARE-Get) that automatically extracts single and multi-hop\nrelations from the customized corpus and generates multi-level question sets\nwithout manual intervention. Leveraging this pipeline, we construct a dataset\n(RARE-Set) spanning 400 expert-level time-sensitive finance, economics, and\npolicy documents and 48,322 questions whose distribution evolves as the\nunderlying sources change. To quantify resilience, we formalize\nretrieval-conditioned robustness metrics (RARE-Met) that capture a model's\nability to remain correct or recover when queries, documents, or real-world\nretrieval results are systematically altered. Our results show that RAG systems\nexhibit surprising vulnerability to perturbations, with document robustness\nconsistently being the weakest point regardless of generator size or\narchitecture. RAG systems consistently show lower robustness on multi-hop\nqueries than single-hop queries across all domains.", "AI": {"tldr": "RARE\u6846\u67b6\u5f15\u5165\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u8bc4\u4f30\u6807\u51c6\u548c\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u6d4b\u8bd5RAG\u7cfb\u7edf\u5728\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u3001\u51b2\u7a81\u548c\u5feb\u901f\u53d8\u5316\u4e8b\u5b9e\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0RAG\u7cfb\u7edf\u5bf9\u6270\u52a8\u8868\u73b0\u51fa\u663e\u8457\u8106\u5f31\u6027\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u5f88\u5c11\u6d4b\u8bd5RAG\u7cfb\u7edf\u5982\u4f55\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u3001\u5185\u90e8\u4e0e\u5916\u90e8\u68c0\u7d22\u5185\u5bb9\u7684\u51b2\u7a81\u6216\u5feb\u901f\u53d8\u5316\u7684\u4e8b\u5b9e\u3002", "method": "\u63d0\u51fa\u4e86RARE\u6846\u67b6\uff0c\u5305\u62ec\u77e5\u8bc6\u56fe\u8c31\u9a71\u52a8\u7684\u5408\u6210\u7ba1\u9053(RARE-Get)\u751f\u6210\u95ee\u9898\u96c6\uff0c\u6784\u5efa\u6570\u636e\u96c6(RARE-Set)\uff0c\u5e76\u5f62\u5f0f\u5316\u68c0\u7d22\u6761\u4ef6\u9c81\u68d2\u6027\u6307\u6807(RARE-Met)\u3002", "result": "RAG\u7cfb\u7edf\u5bf9\u6270\u52a8\u8868\u73b0\u51fa\u663e\u8457\u8106\u5f31\u6027\uff0c\u6587\u6863\u9c81\u68d2\u6027\u662f\u5176\u6700\u8584\u5f31\u73af\u8282\uff0c\u4e14\u591a\u8df3\u67e5\u8be2\u7684\u9c81\u68d2\u6027\u666e\u904d\u4f4e\u4e8e\u5355\u8df3\u67e5\u8be2\u3002", "conclusion": "RARE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u8bc4\u4f30RAG\u7cfb\u7edf\u9c81\u68d2\u6027\u7684\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "keywords": "RAG, \u9c81\u68d2\u6027\u8bc4\u4f30, \u77e5\u8bc6\u56fe\u8c31, \u591a\u8df3\u67e5\u8be2, \u65f6\u95f4\u654f\u611f\u6587\u6863"}}
{"id": "2506.00770", "pdf": "https://arxiv.org/pdf/2506.00770", "abs": "https://arxiv.org/abs/2506.00770", "authors": ["Sai Vamsi Alisetti", "Vikas Kalagi", "Sanjukta Krishnagopal"], "title": "Beyond Attention: Learning Spatio-Temporal Dynamics with Emergent Interpretable Topologies", "categories": ["cs.LG", "cs.AI", "cs.SI"], "comment": "13 pages, 10 figures, workshop", "summary": "Spatio-temporal forecasting is critical in applications such as traffic\nprediction, energy demand modeling, and weather monitoring. While Graph\nAttention Networks (GATs) are popular for modeling spatial dependencies, they\nrely on predefined adjacency structures and dynamic attention scores,\nintroducing inductive biases and computational overhead that can obscure\ninterpretability.\n  We propose InterGAT, a simplified alternative to GAT that replaces masked\nattention with a fully learnable, symmetric node interaction matrix, capturing\nlatent spatial relationships without relying on fixed graph topologies. Our\nframework, InterGAT-GRU, which incorporates a GRU-based temporal decoder,\noutperforms the baseline GAT-GRU in forecasting accuracy, achieving at least a\n21% improvement on the SZ-Taxi dataset and a 6% improvement on the Los-Loop\ndataset across all forecasting horizons (15 to 60 minutes). Additionally, we\nobserved reduction in training time by 60-70% compared to GAT-GRU baseline.\n  Crucially, the learned interaction matrix reveals interpretable structure: it\nrecovers sparse, topology-aware attention patterns that align with community\nstructure. Spectral and clustering analyses show that the model captures both\nlocalized and global dynamics, offering insights into the functional topology\ndriving predictions. This highlights how structure learning can simultaneously\nsupport prediction, computational efficiency, and topological interpretabil-ity\nin dynamic graph-based domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInterGAT\u7684\u7b80\u5316\u66ff\u4ee3\u65b9\u6848\uff0c\u7528\u4e8e\u65f6\u7a7a\u9884\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5bf9\u79f0\u8282\u70b9\u4ea4\u4e92\u77e9\u9635\u53d6\u4ee3\u4f20\u7edf\u7684\u56fe\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u90bb\u63a5\u7ed3\u6784\u548c\u52a8\u6001\u6ce8\u610f\u529b\u5206\u6570\uff0c\u53ef\u80fd\u5bfc\u81f4\u5f52\u7eb3\u504f\u89c1\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u63a9\u76d6\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faInterGAT\u548cInterGAT-GRU\u6846\u67b6\uff0c\u4f7f\u7528\u5b8c\u5168\u53ef\u5b66\u4e60\u7684\u5bf9\u79f0\u8282\u70b9\u4ea4\u4e92\u77e9\u9635\u6355\u83b7\u6f5c\u5728\u7a7a\u95f4\u5173\u7cfb\uff0c\u7ed3\u5408GRU\u65f6\u95f4\u89e3\u7801\u5668\u8fdb\u884c\u65f6\u7a7a\u9884\u6d4b\u3002", "result": "\u5728SZ-Taxi\u548cLos-Loop\u6570\u636e\u96c6\u4e0a\uff0c\u9884\u6d4b\u7cbe\u5ea6\u5206\u522b\u63d0\u534721%\u548c6%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1160-70%\uff0c\u4e14\u5b66\u4e60\u5230\u7684\u4ea4\u4e92\u77e9\u9635\u5177\u6709\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "InterGAT\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u8fd8\u80fd\u63ed\u793a\u529f\u80fd\u62d3\u6251\u7ed3\u6784\uff0c\u4e3a\u52a8\u6001\u56fe\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u517c\u5177\u9884\u6d4b\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u65f6\u7a7a\u9884\u6d4b\uff0c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff0cGRU\uff0c\u53ef\u89e3\u91ca\u6027\uff0c\u8ba1\u7b97\u6548\u7387"}}
{"id": "2506.00806", "pdf": "https://arxiv.org/pdf/2506.00806", "abs": "https://arxiv.org/abs/2506.00806", "authors": ["Songtao Jiang", "Chenyi Zhou", "Yan Zhang", "Yeying Jin", "Zuozhu Liu"], "title": "Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) still struggle with complex\nreasoning tasks in Visual Question Answering (VQA). While current methods have\nadvanced by incorporating visual prompts, our study uncovers critical\nlimitations: these approaches indiscriminately annotate all detected objects\nfor every visual question, generating excessive visual markers that degrade\ntask performance. This issue stems primarily from a lack of focus on key visual\nelements, raising two important questions: Are all objects equally important,\nand do all questions require visual prompts? Motivated by Dual Process Theory,\nwhich distinguishes between instinctive and deliberate cognitive modes in human\nreasoning, we propose FOCUS, a plug-and-play approach that dynamically adapts\nto the complexity of questions, combining fast intuitive judgments with\ndeliberate analytical reasoning to enhance the vision-language reasoning\ncapability of the MLLM. For straightforward questions, FOCUS supports efficient\nzero-shot reasoning. For more complex tasks, it employs the conceptualizing\nbefore observation strategy to highlight critical elements. Extensive\nexperiments on four benchmarks, ScienceQA, TextQA, VizWiz, and MME, demonstrate\nthat FOCUS consistently improves the performance of both open-source and\nblack-box MLLMs, achieving significant gains across all datasets. Ablation\nstudies further validate the importance of combining diverse cognitive\nstrategies with refined visual information for superior performance. Code will\nbe released.", "AI": {"tldr": "\u63d0\u51fa\u4e86FOCUS\u65b9\u6cd5\uff0c\u52a8\u6001\u9002\u5e94\u95ee\u9898\u590d\u6742\u6027\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5bf9\u6240\u6709\u89c6\u89c9\u95ee\u9898\u8fdb\u884c\u5bf9\u8c61\u6807\u6ce8\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u7f3a\u4e4f\u5bf9\u5173\u952e\u89c6\u89c9\u5143\u7d20\u7684\u5173\u6ce8\u3002", "method": "FOCUS\u7ed3\u5408\u76f4\u89c9\u5224\u65ad\u4e0e\u6df1\u601d\u719f\u8651\u5206\u6790\uff0c\u6839\u636e\u95ee\u9898\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u7b56\u7565\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFOCUS\u663e\u8457\u63d0\u5347\u4e86MLLMs\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408\u591a\u6837\u5316\u8ba4\u77e5\u7b56\u7565\u548c\u7cbe\u7ec6\u5316\u89c6\u89c9\u4fe1\u606f\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "keywords": "MLLMs, VQA, \u53cc\u8fc7\u7a0b\u7406\u8bba, FOCUS, \u89c6\u89c9\u63d0\u793a"}}
{"id": "2506.00038", "pdf": "https://arxiv.org/pdf/2506.00038", "abs": "https://arxiv.org/abs/2506.00038", "authors": ["Reid McIlroy-Young"], "title": "The Folly of AI for Age Verification", "categories": ["cs.CY", "cs.AI"], "comment": "From AI for Public Missions Workshop at AAAI", "summary": "In the near future a governmental body will be asked to allow companies to\nuse AI for age verification. If they allow it the resulting system will both be\neasily circumvented and disproportionately misclassify minorities and low\nsocioeconomic status users. This is predictable by showing that other very\nsimilar systems (facial recognition and remote proctoring software) have\nsimilar issues despite years of efforts to mitigate their biases. These biases\nare due to technical limitations both of the AI models themselves and the\nphysical hardware they are running on that will be difficult to overcome below\nthe cost of government ID-based age verification. Thus in, the near future,\ndeploying an AI system for age verification is folly.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\uff0c\u4f7f\u7528AI\u8fdb\u884c\u5e74\u9f84\u9a8c\u8bc1\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff0c\u5305\u62ec\u6613\u88ab\u7ed5\u8fc7\u4ee5\u53ca\u5bf9\u5c11\u6570\u7fa4\u4f53\u548c\u4f4e\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\u7528\u6237\u7684\u4e0d\u516c\u5e73\u5206\u7c7b\u3002", "motivation": "\u653f\u5e9c\u53ef\u80fd\u5141\u8bb8\u4f01\u4e1a\u4f7f\u7528AI\u8fdb\u884c\u5e74\u9f84\u9a8c\u8bc1\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u5c06\u5bfc\u81f4\u4e0d\u516c\u5e73\u7684\u7ed3\u679c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u7c7b\u4f3c\u6280\u672f\uff08\u5982\u4eba\u8138\u8bc6\u522b\u548c\u8fdc\u7a0b\u76d1\u8003\u8f6f\u4ef6\uff09\u7684\u95ee\u9898\uff0c\u9884\u6d4bAI\u5e74\u9f84\u9a8c\u8bc1\u7684\u5c40\u9650\u6027\u3002", "result": "AI\u6a21\u578b\u548c\u786c\u4ef6\u7684\u6280\u672f\u9650\u5236\u5bfc\u81f4\u504f\u89c1\uff0c\u96be\u4ee5\u4f4e\u6210\u672c\u5730\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\u3002", "conclusion": "\u5728\u8fd1\u671f\u90e8\u7f72AI\u5e74\u9f84\u9a8c\u8bc1\u7cfb\u7edf\u662f\u4e0d\u660e\u667a\u7684\u3002", "keywords": "AI\u5e74\u9f84\u9a8c\u8bc1, \u504f\u89c1, \u6280\u672f\u9650\u5236, \u516c\u5e73\u6027"}}
{"id": "2506.00771", "pdf": "https://arxiv.org/pdf/2506.00771", "abs": "https://arxiv.org/abs/2506.00771", "authors": ["Zitao Chen", "Yinjun Jia", "Zitong Tian", "Wei-Ying Ma", "Yanyan Lan"], "title": "Manipulating 3D Molecules in a Fixed-Dimensional SE(3)-Equivariant Latent Space", "categories": ["cs.LG", "cs.AI", "I.2.6"], "comment": "11 pages, 6 figures", "summary": "Medicinal chemists often optimize drugs considering their 3D structures and\ndesigning structurally distinct molecules that retain key features, such as\nshapes, pharmacophores, or chemical properties. Previous deep learning\napproaches address this through supervised tasks like molecule inpainting or\nproperty-guided optimization. In this work, we propose a flexible zero-shot\nmolecule manipulation method by navigating in a shared latent space of 3D\nmolecules. We introduce a Variational AutoEncoder (VAE) for 3D molecules, named\nMolFLAE, which learns a fixed-dimensional, SE(3)-equivariant latent space\nindependent of atom counts. MolFLAE encodes 3D molecules using an\nSE(3)-equivariant neural network into fixed number of latent nodes,\ndistinguished by learned embeddings. The latent space is regularized, and\nmolecular structures are reconstructed via a Bayesian Flow Network (BFN)\nconditioned on the encoder's latent output. MolFLAE achieves competitive\nperformance on standard unconditional 3D molecule generation benchmarks.\nMoreover, the latent space of MolFLAE enables zero-shot molecule manipulation,\nincluding atom number editing, structure reconstruction, and coordinated latent\ninterpolation for both structure and properties. We further demonstrate our\napproach on a drug optimization task for the human glucocorticoid receptor,\ngenerating molecules with improved hydrophilicity while preserving key\ninteractions, under computational evaluations. These results highlight the\nflexibility, robustness, and real-world utility of our method, opening new\navenues for molecule editing and optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMolFLAE\u7684\u96f6-shot\u5206\u5b50\u64cd\u7eb5\u65b9\u6cd5\uff0c\u901a\u8fc7SE(3)-\u7b49\u53d8\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e603D\u5206\u5b50\u7684\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u652f\u6301\u5206\u5b50\u7f16\u8f91\u548c\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u836f\u7269\u5316\u5b66\u5bb6\u4f9d\u8d563D\u7ed3\u6784\u4f18\u5316\u5206\u5b50\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u591a\u4e3a\u76d1\u7763\u4efb\u52a1\uff0c\u672c\u5de5\u4f5c\u65e8\u5728\u5f00\u53d1\u7075\u6d3b\u4e14\u65e0\u76d1\u7763\u7684\u5206\u5b50\u64cd\u7eb5\u65b9\u6cd5\u3002", "method": "\u91c7\u7528SE(3)-\u7b49\u53d8\u795e\u7ecf\u7f51\u7edc\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668MolFLAE\uff0c\u7ed3\u5408Bayesian Flow Network\u91cd\u6784\u5206\u5b50\u7ed3\u6784\uff0c\u5b9e\u73b0\u56fa\u5b9a\u7ef4\u5ea6\u7684\u6f5c\u5728\u7a7a\u95f4\u7f16\u7801\u3002", "result": "\u5728\u65e0\u6761\u4ef63D\u5206\u5b50\u751f\u6210\u57fa\u51c6\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u96f6-shot\u64cd\u4f5c\uff08\u5982\u539f\u5b50\u6570\u7f16\u8f91\u3001\u7ed3\u6784\u91cd\u6784\u548c\u5c5e\u6027\u63d2\u503c\uff09\uff0c\u5e76\u5728\u836f\u7269\u4f18\u5316\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "MolFLAE\u5c55\u793a\u4e86\u7075\u6d3b\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u5206\u5b50\u7f16\u8f91\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "3D\u5206\u5b50\u7f16\u8f91, SE(3)-\u7b49\u53d8, \u53d8\u5206\u81ea\u7f16\u7801\u5668, \u96f6-shot\u5b66\u4e60, \u836f\u7269\u4f18\u5316"}}
{"id": "2506.00814", "pdf": "https://arxiv.org/pdf/2506.00814", "abs": "https://arxiv.org/abs/2506.00814", "authors": ["Zifeng Zhu", "Shangbin Feng", "Herun Wan", "Ningnan Wang", "Minnan Luo", "Yulia Tsvetkov"], "title": "GuessBench: Sensemaking Multimodal Creativity in the Wild", "categories": ["cs.CL"], "comment": null, "summary": "We propose GuessBench, a novel benchmark that evaluates Vision Language\nModels (VLMs) on modeling the pervasive, noisy, and pluralistic human\ncreativity. GuessBench sources data from \"Guess the Build\", an online\nmultiplayer Minecraft minigame where one player constructs a Minecraft build\ngiven a concept (e.g. caterpillar) and others try to guess it with natural\nlanguage hints, presenting a pristine testbed for sensemaking creativity in the\nwild with VLMs acting as guessers. We curate 1500 images from the actual\ngameplay and design 2000 problems spanning static and dynamic image settings,\nnatural language hints of varying completeness, and more. Extensive experiments\nwith six open/API VLMs and five reasoning enhancement approaches demonstrate\nthat GuessBench presents a uniquely challenging task in creativity modeling:\neven the start-of-the-art GPT-4o is incorrect on 34% of instances, while we\nobserve a huge performance gap (13.87% vs. 53.93% on average) between open and\nAPI models. When used as a resource to improve VLMs, fine-tuning on the\nreasoning traces for GuessBench problems improves visual perception tasks by\n15.36% on average. Further analysis reveals that VLM performance in creativity\nsensemaking correlates with the frequency of the concept in training data,\nwhile the accuracy drops sharply for concepts in underrepresented cultural\ncontexts and low-resource languages.", "AI": {"tldr": "GuessBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5bf9\u4eba\u7c7b\u521b\u9020\u529b\u5efa\u6a21\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u57fa\u4e8eMinecraft\u591a\u4eba\u6e38\u620f\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff0c\u5c55\u793a\u4e86\u5176\u5728\u521b\u9020\u529b\u5efa\u6a21\u4e0a\u7684\u6311\u6218\u6027\u3002", "motivation": "\u7814\u7a76VLMs\u5982\u4f55\u5efa\u6a21\u4eba\u7c7b\u5e7f\u6cdb\u3001\u5608\u6742\u548c\u591a\u5143\u5316\u7684\u521b\u9020\u529b\uff0c\u5c24\u5176\u662f\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7406\u89e3\u4e0d\u5b8c\u6574\u63d0\u793a\u7684\u80fd\u529b\u3002", "method": "\u4eceMinecraft\u6e38\u620f\u4e2d\u6536\u96c61500\u5f20\u56fe\u50cf\u5e76\u8bbe\u8ba12000\u4e2a\u95ee\u9898\uff0c\u6d4b\u8bd56\u79cdVLMs\u548c5\u79cd\u589e\u5f3a\u63a8\u7406\u65b9\u6cd5\u3002", "result": "\u6700\u5148\u8fdb\u7684GPT-4o\u572834%\u7684\u60c5\u51b5\u4e0b\u9519\u8bef\uff0c\u5f00\u6e90\u4e0eAPI\u6a21\u578b\u6027\u80fd\u5dee\u8ddd\u663e\u8457\uff0813.87% vs. 53.93%\uff09\u3002\u5fae\u8c03\u63a8\u7406\u75d5\u8ff9\u53ef\u63d0\u5347\u89c6\u89c9\u4efb\u52a1\u6027\u80fd15.36%\u3002", "conclusion": "GuessBench\u63ed\u793a\u4e86VLMs\u5728\u521b\u9020\u529b\u5efa\u6a21\u4e0a\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6587\u5316\u80cc\u666f\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0b\u7684\u8868\u73b0\u4e0d\u4f73\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u521b\u9020\u529b\u5efa\u6a21, Minecraft, \u57fa\u51c6\u6d4b\u8bd5, \u591a\u6837\u6027"}}
{"id": "2506.00047", "pdf": "https://arxiv.org/pdf/2506.00047", "abs": "https://arxiv.org/abs/2506.00047", "authors": ["Jan G\u00f6pfert", "Jann M. Weinand", "Patrick Kuckertz", "Noah Pflugradt", "Jochen Lin\u00dfen"], "title": "Risks of AI-driven product development and strategies for their mitigation", "categories": ["cs.CY", "cs.AI", "cs.CE"], "comment": null, "summary": "Humanity is progressing towards automated product development, a trend that\npromises faster creation of better products and thus the acceleration of\ntechnological progress. However, increasing reliance on non-human agents for\nthis process introduces many risks. This perspective aims to initiate a\ndiscussion on these risks and appropriate mitigation strategies. To this end,\nwe outline a set of principles for safer AI-driven product development which\nemphasize human oversight, accountability, and explainable design, among\nothers. The risk assessment covers both technical risks which affect product\nquality and safety, and sociotechnical risks which affect society. While\nAI-driven product development is still in its early stages, this discussion\nwill help balance its opportunities and risks without delaying essential\nprogress in understanding, norm-setting, and regulation.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u63a2\u8ba8\u4e86\u81ea\u52a8\u5316\u4ea7\u54c1\u5f00\u53d1\u4e2dAI\u9a71\u52a8\u5e26\u6765\u7684\u98ce\u9669\u53ca\u5176\u7f13\u89e3\u7b56\u7565\uff0c\u63d0\u51fa\u4e86\u5f3a\u8c03\u4eba\u7c7b\u76d1\u7763\u3001\u8d23\u4efb\u548c\u53ef\u89e3\u91ca\u8bbe\u8ba1\u7684\u539f\u5219\u3002", "motivation": "\u81ea\u52a8\u5316\u4ea7\u54c1\u5f00\u53d1\u901f\u5ea6\u5feb\u3001\u8d28\u91cf\u9ad8\uff0c\u4f46\u4f9d\u8d56\u975e\u4eba\u7c7b\u4ee3\u7406\u5f15\u5165\u98ce\u9669\uff0c\u9700\u8981\u5e73\u8861\u673a\u9047\u4e0e\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86AI\u9a71\u52a8\u4ea7\u54c1\u5f00\u53d1\u7684\u5b89\u5168\u539f\u5219\uff0c\u5305\u62ec\u4eba\u7c7b\u76d1\u7763\u3001\u8d23\u4efb\u548c\u53ef\u89e3\u91ca\u8bbe\u8ba1\u7b49\u3002", "result": "\u5206\u6790\u6280\u672f\u548c\u793e\u4f1a\u7684\u53cc\u91cd\u98ce\u9669\uff0c\u4e3a\u672a\u6765\u7684\u7406\u89e3\u3001\u89c4\u8303\u5236\u5b9a\u548c\u76d1\u7ba1\u63d0\u4f9b\u57fa\u7840\u3002", "conclusion": "\u5c3d\u7ba1\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u8ba8\u8bba\u6709\u52a9\u4e8e\u5e73\u8861AI\u9a71\u52a8\u4ea7\u54c1\u5f00\u53d1\u7684\u673a\u9047\u4e0e\u98ce\u9669\u3002", "keywords": "\u81ea\u52a8\u5316\u4ea7\u54c1\u5f00\u53d1\u3001AI\u9a71\u52a8\u3001\u98ce\u9669\u7ba1\u7406\u3001\u4eba\u7c7b\u76d1\u7763\u3001\u53ef\u89e3\u91ca\u8bbe\u8ba1"}}
{"id": "2506.00772", "pdf": "https://arxiv.org/pdf/2506.00772", "abs": "https://arxiv.org/abs/2506.00772", "authors": ["Zihang Liu", "Tianyu Pang", "Oleg Balabanov", "Chaoqun Yang", "Tianjin Huang", "Lu Yin", "Yaoqing Yang", "Shiwei Liu"], "title": "LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "Recent studies have shown that supervised fine-tuning of LLMs on a small\nnumber of high-quality datasets can yield strong reasoning capabilities.\nHowever, full fine-tuning (Full FT), while powerful, is computationally\nexpensive and susceptible to overfitting and catastrophic forgetting,\nparticularly when data is limited. Sparse fine-tuning, which previously\nachieved notable success by updating only a small subset of model parameters,\noffers a promising trade-off between efficiency and effectiveness. Yet, it has\nlagged behind in the LLM era due to the difficulty of identifying parameters\ntruly critical for reasoning. In this work, we state that weights with the\nlargest magnitude after low-rank approximation are critical weights for\nfine-tuning, which we call Principal Weights. Surprisingly, while\nmagnitude-based sparse fine-tuning performs poorly as a baseline on LLM\nfine-tuning, it becomes highly effective after rank reduction. These insights\nmotivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only\nupdates the top 5% Principal Weights throughout training and consistently\nachieves better performance on reasoning tasks than Full FT, while maintaining\nmemory efficiency on par with popular parameter-efficient fine-tuning methods.\nIn addition to strong performance on target domains such as arithmetic\nreasoning, LIFT also retains up to 20% more source-domain knowledge, compared\nto Full FT and LoRA. Our code is available at:\nhttps://github.com/zihanghliu/LIFT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u4f4e\u79e9\u4fe1\u606f\u7684\u7a00\u758f\u5fae\u8c03\u65b9\u6cd5LIFT\uff0c\u4ec5\u66f4\u65b0\u524d5%\u4e3b\u6743\u91cd\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5b8c\u5168\u5fae\u8c03\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5b8c\u5168\u5fae\u8c03\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6613\u8fc7\u62df\u5408\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u7a00\u758f\u5fae\u8c03\u5728LLM\u65f6\u4ee3\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u4f4e\u79e9\u8fd1\u4f3c\u8bc6\u522b\u5173\u952e\u6743\u91cd\uff08\u4e3b\u6743\u91cd\uff09\uff0c\u4ec5\u66f4\u65b0\u524d5%\u7684\u4e3b\u6743\u91cd\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u7b97\u672f\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5b8c\u5168\u5fae\u8c03\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u66f4\u591a\u6e90\u57df\u77e5\u8bc6\uff08\u6bd4\u5b8c\u5168\u5fae\u8c03\u548cLoRA\u591a20%\uff09\u3002", "conclusion": "LIFT\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u9002\u5408\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u3002", "keywords": "LLM\u5fae\u8c03\uff0c\u7a00\u758f\u5fae\u8c03\uff0c\u4f4e\u79e9\u8fd1\u4f3c\uff0c\u53c2\u6570\u6548\u7387\uff0c\u63a8\u7406\u4efb\u52a1"}}
{"id": "2506.00815", "pdf": "https://arxiv.org/pdf/2506.00815", "abs": "https://arxiv.org/abs/2506.00815", "authors": ["Manoj Balaji Jagadeeshan", "Samarth Bhatia", "Pretam Ray", "Harshul Raj Surana", "Akhil Rajeev P", "Priya Mishra", "Annarao Kulkarni", "Ganesh Ramakrishnan", "Prathosh AP", "Pawan Goyal"], "title": "From Plain Text to Poetic Form: Generating Metrically-Constrained Sanskrit Verses", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly improved\nnatural language generation, including creative tasks like poetry composition.\nHowever, most progress remains concentrated in high-resource languages. This\nraises an important question: Can LLMs be adapted for structured poetic\ngeneration in a low-resource, morphologically rich language such as Sanskrit?\nIn this work, we introduce a dataset designed for translating English prose\ninto structured Sanskrit verse, with strict adherence to classical metrical\npatterns, particularly the Anushtub meter. We evaluate a range of generative\nmodels-both open-source and proprietary-under multiple settings. Specifically,\nwe explore constrained decoding strategies and instruction-based fine-tuning\ntailored to metrical and semantic fidelity. Our decoding approach achieves over\n99% accuracy in producing syntactically valid poetic forms, substantially\noutperforming general-purpose models in meter conformity. Meanwhile,\ninstruction-tuned variants show improved alignment with source meaning and\npoetic style, as supported by human assessments, albeit with marginal\ntrade-offs in metrical precision.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bd7\u6b4c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4e3b\u8981\u96c6\u4e2d\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u3002\u672c\u6587\u7814\u7a76\u5982\u4f55\u5c06\u5176\u7528\u4e8e\u4f4e\u8d44\u6e90\u3001\u5f62\u6001\u4e30\u5bcc\u7684\u68b5\u8bed\uff0c\u901a\u8fc7\u6570\u636e\u96c6\u548c\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u89e3\u7801\u7b56\u7565\u548c\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u68b5\u8bed\uff09\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u7279\u522b\u662f\u5982\u4f55\u751f\u6210\u4e25\u683c\u7b26\u5408\u53e4\u5178\u97f5\u5f8b\u7684\u8bd7\u6b4c\u3002", "method": "\u5f15\u5165\u6570\u636e\u96c6\u5c06\u82f1\u6587\u6563\u6587\u7ffb\u8bd1\u4e3a\u68b5\u8bed\u8bd7\uff0c\u8bc4\u4f30\u5f00\u6e90\u548c\u4e13\u6709\u751f\u6210\u6a21\u578b\uff0c\u63a2\u7d22\u7ea6\u675f\u89e3\u7801\u548c\u6307\u4ee4\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u7ea6\u675f\u89e3\u7801\u5728\u8bed\u6cd5\u6709\u6548\u6027\u4e0a\u8fbe99%\u51c6\u786e\u7387\uff0c\u6307\u4ee4\u5fae\u8c03\u5728\u8bed\u4e49\u548c\u98ce\u683c\u4e0a\u8868\u73b0\u66f4\u4f73\uff0c\u4f46\u97f5\u5f8b\u7cbe\u5ea6\u7565\u6709\u964d\u4f4e\u3002", "conclusion": "LLMs\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bd7\u6b4c\u751f\u6210\uff0c\u7ea6\u675f\u89e3\u7801\u548c\u6307\u4ee4\u5fae\u8c03\u5404\u6709\u4f18\u52bf\uff0c\u9700\u5e73\u8861\u9700\u6c42\u9009\u62e9\u3002", "keywords": "LLMs, \u68b5\u8bed, \u8bd7\u6b4c\u751f\u6210, \u7ea6\u675f\u89e3\u7801, \u6307\u4ee4\u5fae\u8c03"}}
{"id": "2506.00049", "pdf": "https://arxiv.org/pdf/2506.00049", "abs": "https://arxiv.org/abs/2506.00049", "authors": ["Arjun Rao", "Hanieh Alipour", "Nick Pendar"], "title": "Rethinking Hybrid Retrieval: When Small Embeddings and LLM Re-ranking Beat Bigger Models", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "This paper presents a comparison of embedding models in tri-modal hybrid\nretrieval for Retrieval-Augmented Generation (RAG) systems. We investigate the\nfusion of dense semantic, sparse lexical, and graph-based embeddings, focusing\non the performance of the MiniLM-v6 and BGE-Large architectures. Contrary to\nconventional assumptions, our results show that the compact MiniLM-v6\noutperforms the larger BGE-Large when integrated with LLM-based re-ranking\nwithin our tri-modal hybrid framework. Experiments conducted on the SciFact,\nFIQA, and NFCorpus datasets demonstrate significant improvements in retrieval\nquality with the MiniLM-v6 configuration. The performance difference is\nparticularly pronounced in agentic re-ranking scenarios, indicating better\nalignment between MiniLM-v6's embedding space and LLM reasoning. Our findings\nsuggest that embedding model selection for RAG systems should prioritize\ncompatibility with multi-signal fusion and LLM alignment, rather than relying\nsolely on larger models. This approach may reduce computational requirements\nwhile improving retrieval accuracy and efficiency.", "AI": {"tldr": "\u6bd4\u8f83\u4e86MiniLM-v6\u548cBGE-Large\u5728RAG\u7cfb\u7edf\u4e2d\u7684\u4e09\u6a21\u6001\u6df7\u5408\u68c0\u7d22\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0MiniLM-v6\u5728\u4e0eLLM\u91cd\u6392\u7ed3\u5408\u65f6\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u63a2\u8ba8\u5728RAG\u7cfb\u7edf\u4e2d\uff0c\u591a\u79cd\u5d4c\u5165\u6a21\u578b\u7684\u878d\u5408\u65b9\u6cd5\u53ca\u5176\u6027\u80fd\u5dee\u5f02\uff0c\u5c24\u5176\u662fMiniLM-v6\u548cBGE-Large\u7684\u8868\u73b0\u3002", "method": "\u878d\u5408\u7a20\u5bc6\u8bed\u4e49\u3001\u7a00\u758f\u8bcd\u6cd5\u548c\u57fa\u4e8e\u56fe\u7684\u5d4c\u5165\uff0c\u6d4b\u8bd5MiniLM-v6\u548cBGE-Large\u5728\u4e09\u6a21\u6001\u6df7\u5408\u6846\u67b6\u4e2d\u7684\u8868\u73b0\u3002", "result": "MiniLM-v6\u5728\u7ed3\u5408LLM\u91cd\u6392\u65f6\u4f18\u4e8eBGE-Large\uff0c\u5c24\u5176\u5728\u4ee3\u7406\u91cd\u6392\u573a\u666f\u4e2d\u8868\u73b0\u663e\u8457\u3002", "conclusion": "RAG\u7cfb\u7edf\u7684\u5d4c\u5165\u6a21\u578b\u9009\u62e9\u5e94\u6ce8\u91cd\u591a\u4fe1\u53f7\u878d\u5408\u548cLLM\u5bf9\u9f50\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u5927\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "keywords": "RAG,\u4e09\u6a21\u6001\u6df7\u5408\u68c0\u7d22,\u5d4c\u5165\u6a21\u578b,MiniLM-v6,BGE-Large"}}
{"id": "2506.00795", "pdf": "https://arxiv.org/pdf/2506.00795", "abs": "https://arxiv.org/abs/2506.00795", "authors": ["Xing Lei", "Zifeng Zhuang", "Shentao Yang", "Sheng Xu", "Yunhao Luo", "Fei Shen", "Xuetao Zhang", "Donglin Wang"], "title": "Bridging Supervised and Temporal Difference Learning with $Q$-Conditioned Maximization", "categories": ["cs.LG"], "comment": null, "summary": "Recently, supervised learning (SL) methodology has emerged as an effective\napproach for offline reinforcement learning (RL) due to their simplicity,\nstability, and efficiency. However, recent studies show that SL methods lack\nthe trajectory stitching capability, typically associated with temporal\ndifference (TD)-based approaches. A question naturally surfaces: How can we\nendow SL methods with stitching capability and bridge its performance gap with\nTD learning? To answer this question, we introduce $Q$-conditioned maximization\nsupervised learning for offline goal-conditioned RL, which enhances SL with the\nstitching capability through $Q$-conditioned policy and $Q$-conditioned\nmaximization. Concretely, we propose Goal-Conditioned Reinforced Supervised\nLearning (GCReinSL), which consists of (1) estimating the $Q$-function by CVAE\nfrom the offline dataset and (2) finding the maximum $Q$-value within the data\nsupport by integrating $Q$-function maximization with Expectile Regression. In\ninference time, our policy chooses optimal actions based on such a maximum\n$Q$-value. Experimental results from stitching evaluations on offline RL\ndatasets demonstrate that our method outperforms prior SL approaches with\nstitching capabilities and goal data augmentation techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u548c\u8f68\u8ff9\u7f1d\u5408\u80fd\u529b\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5GCReinSL\uff0c\u901a\u8fc7Q-\u6761\u4ef6\u7b56\u7565\u548c\u6700\u5927\u5316\u4f18\u5316\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7f3a\u4e4f\u8f68\u8ff9\u7f1d\u5408\u80fd\u529b\u7684\u95ee\u9898\uff0c\u7f29\u5c0f\u5176\u4e0eTD\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u5f15\u5165Q-\u6761\u4ef6\u6700\u5927\u5316\u76d1\u7763\u5b66\u4e60\uff0c\u7ed3\u5408CVAE\u4f30\u8ba1Q\u51fd\u6570\u548c\u671f\u671b\u56de\u5f52\u5b9e\u73b0Q\u503c\u6700\u5927\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGCReinSL\u5728\u79bb\u7ebfRL\u6570\u636e\u96c6\u4e0a\u7684\u7f1d\u5408\u6027\u80fd\u4f18\u4e8e\u73b0\u6709SL\u65b9\u6cd5\u3002", "conclusion": "GCReinSL\u6210\u529f\u8d4b\u4e88\u4e86SL\u65b9\u6cd5\u8f68\u8ff9\u7f1d\u5408\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u6027\u80fd\u3002", "keywords": "\u76d1\u7763\u5b66\u4e60, \u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60, \u8f68\u8ff9\u7f1d\u5408, Q-\u6761\u4ef6\u7b56\u7565, GCReinSL"}}
{"id": "2506.00817", "pdf": "https://arxiv.org/pdf/2506.00817", "abs": "https://arxiv.org/abs/2506.00817", "authors": ["Weitao Ma", "Xiyuan Du", "Xiaocheng Feng", "Lei Huang", "Yichong Huang", "Huiyi Zhang", "Xiaoliang Yang", "Baohang Li", "Xiachong Feng", "Ting Liu", "Bing Qin"], "title": "One for All: Update Parameterized Knowledge Across Multiple Models", "categories": ["cs.CL"], "comment": "ACL 2025 (Main Conference)", "summary": "Large language models (LLMs) encode vast world knowledge but struggle to stay\nup-to-date, often leading to errors and hallucinations. Knowledge editing\noffers an efficient alternative to retraining, enabling targeted modifications\nby updating specific model parameters. However, existing methods primarily\nfocus on individual models, posing challenges in efficiently updating multiple\nmodels and adapting to new models. To address this, we propose OnceEdit, a\nnovel ensemble-based approach that employs a plug-in model as the editing\nmodule, enabling stable knowledge updates across multiple models. Building on\nthe model ensemble, OnceEdit introduces two key mechanisms to enhance its\neffectiveness. First, we introduce a dynamic weight mechanism through a \\weight\ntoken for distinguishing between edit-related and non-edit-related instances,\nensuring the appropriate utilization of knowledge from integrated models.\nSecond, we incorporate an ensemble enhancement mechanism to mitigate the\nexcessive reliance on the central model inherent in the model ensemble\ntechnique, making it more suitable for knowledge editing. Extensive experiments\non diverse LLMs demonstrate that OnceEdit consistently outperforms existing\nmethods while achieving superior editing efficiency. Further analysis confirms\nits adaptability and stability in multi-model editing scenarios. Our code will\nbe available.", "AI": {"tldr": "OnceEdit\u662f\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u7684\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6743\u91cd\u548c\u96c6\u6210\u589e\u5f3a\u673a\u5236\uff0c\u9ad8\u6548\u66f4\u65b0\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u96be\u4ee5\u4fdd\u6301\u77e5\u8bc6\u66f4\u65b0\uff0c\u5bfc\u81f4\u9519\u8bef\u548c\u5e7b\u89c9\u3002\u73b0\u6709\u7684\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u4e00\u6a21\u578b\uff0c\u96be\u4ee5\u9ad8\u6548\u66f4\u65b0\u591a\u4e2a\u6a21\u578b\u6216\u9002\u914d\u65b0\u6a21\u578b\u3002", "method": "\u63d0\u51faOnceEdit\uff0c\u91c7\u7528\u63d2\u4ef6\u6a21\u578b\u4f5c\u4e3a\u7f16\u8f91\u6a21\u5757\uff0c\u5f15\u5165\u52a8\u6001\u6743\u91cd\u673a\u5236\u548c\u96c6\u6210\u589e\u5f3a\u673a\u5236\uff0c\u4f18\u5316\u591a\u6a21\u578b\u77e5\u8bc6\u7f16\u8f91\u7684\u6548\u679c\u548c\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOnceEdit\u5728\u591a\u4e2aLLM\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7f16\u8f91\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "OnceEdit\u5728\u77e5\u8bc6\u7f16\u8f91\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u578b\u573a\u666f\uff0c\u5177\u5907\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u7a33\u5b9a\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u77e5\u8bc6\u7f16\u8f91,\u6a21\u578b\u96c6\u6210,\u52a8\u6001\u6743\u91cd,\u96c6\u6210\u589e\u5f3a"}}
{"id": "2506.00052", "pdf": "https://arxiv.org/pdf/2506.00052", "abs": "https://arxiv.org/abs/2506.00052", "authors": ["Ilia Sucholutsky", "Katherine M. Collins", "Nori Jacoby", "Bill D. Thompson", "Robert D. Hawkins"], "title": "Using LLMs to Advance the Cognitive Science of Collectives", "categories": ["q-bio.NC", "cs.AI", "cs.HC", "cs.MA", "cs.SI"], "comment": null, "summary": "LLMs are already transforming the study of individual cognition, but their\napplication to studying collective cognition has been underexplored. We lay out\nhow LLMs may be able to address the complexity that has hindered the study of\ncollectives and raise possible risks that warrant new methods.", "AI": {"tldr": "\u8ba8\u8bba\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7814\u7a76\u96c6\u4f53\u8ba4\u77e5\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u53ca\u53ef\u80fd\u98ce\u9669\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u805a\u7126\u4e8eLLMs\u5728\u4e2a\u4f53\u8ba4\u77e5\u4e2d\u7684\u5e94\u7528\uff0c\u800c\u96c6\u4f53\u8ba4\u77e5\u9886\u57df\u7684\u63a2\u7d22\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u5229\u7528LLMs\u89e3\u51b3\u96c6\u4f53\u8ba4\u77e5\u7814\u7a76\u4e2d\u590d\u6742\u6027\u7684\u521d\u6b65\u601d\u8def\uff0c\u5e76\u63a2\u8ba8\u76f8\u5173\u98ce\u9669\u3002", "result": "\u660e\u786eLLMs\u5728\u96c6\u4f53\u8ba4\u77e5\u7814\u7a76\u4e2d\u7684\u6f5c\u5728\u4f5c\u7528\u53ca\u9700\u5e94\u5bf9\u7684\u65b0\u98ce\u9669\u3002", "conclusion": "LLMs\u4e3a\u96c6\u4f53\u8ba4\u77e5\u7814\u7a76\u63d0\u4f9b\u65b0\u5de5\u5177\uff0c\u4f46\u9700\u5f00\u53d1\u65b0\u65b9\u6cd5\u5e94\u5bf9\u6f5c\u5728\u98ce\u9669\u3002", "keywords": "LLMs, \u96c6\u4f53\u8ba4\u77e5, \u590d\u6742\u6027, \u98ce\u9669, \u65b0\u65b9\u6cd5"}}
{"id": "2506.00797", "pdf": "https://arxiv.org/pdf/2506.00797", "abs": "https://arxiv.org/abs/2506.00797", "authors": ["Jianglin Ding", "Jingcheng Tang", "Gangshan Jing"], "title": "Action Dependency Graphs for Globally Optimal Coordinated Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "Action-dependent individual policies, which incorporate both environmental\nstates and the actions of other agents in decision-making, have emerged as a\npromising paradigm for achieving global optimality in multi-agent reinforcement\nlearning (MARL). However, the existing literature often adopts auto-regressive\naction-dependent policies, where each agent's policy depends on the actions of\nall preceding agents. This formulation incurs substantial computational\ncomplexity as the number of agents increases, thereby limiting scalability. In\nthis work, we consider a more generalized class of action-dependent policies,\nwhich do not necessarily follow the auto-regressive form. We propose to use the\n`action dependency graph (ADG)' to model the inter-agent action dependencies.\nWithin the context of MARL problems structured by coordination graphs, we prove\nthat an action-dependent policy with a sparse ADG can achieve global\noptimality, provided the ADG satisfies specific conditions specified by the\ncoordination graph. Building on this theoretical foundation, we develop a\ntabular policy iteration algorithm with guaranteed global optimality.\nFurthermore, we integrate our framework into several SOTA algorithms and\nconduct experiments in complex environments. The empirical results affirm the\nrobustness and applicability of our approach in more general scenarios,\nunderscoring its potential for broader MARL challenges.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u4f5c\u4f9d\u8d56\u56fe\u7684\u975e\u81ea\u56de\u5f52\u7b56\u7565\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u901a\u8fc7\u7a00\u758f\u4f9d\u8d56\u56fe\u4fdd\u8bc1\u5168\u5c40\u6700\u4f18\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52\u52a8\u4f5c\u4f9d\u8d56\u7b56\u7565\u5728\u667a\u80fd\u4f53\u6570\u91cf\u589e\u52a0\u65f6\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u56e0\u6b64\u7814\u7a76\u4e86\u66f4\u901a\u7528\u7684\u52a8\u4f5c\u4f9d\u8d56\u7b56\u7565\u3002", "method": "\u5f15\u5165\u52a8\u4f5c\u4f9d\u8d56\u56fe\uff08ADG\uff09\u5efa\u6a21\u667a\u80fd\u4f53\u95f4\u7684\u52a8\u4f5c\u4f9d\u8d56\uff0c\u57fa\u4e8e\u534f\u8c03\u56fe\u7406\u8bba\u8bc1\u660e\u4e86\u7a00\u758fADG\u53ef\u5b9e\u73b0\u5168\u5c40\u6700\u4f18\uff0c\u5e76\u63d0\u51fa\u4e86\u8868\u683c\u7b56\u7565\u8fed\u4ee3\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\u3002", "conclusion": "\u7a00\u758f\u52a8\u4f5c\u4f9d\u8d56\u56fe\u7b56\u7565\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "keywords": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u3001\u52a8\u4f5c\u4f9d\u8d56\u56fe\u3001\u5168\u5c40\u6700\u4f18\u3001\u7b56\u7565\u8fed\u4ee3"}}
{"id": "2506.00823", "pdf": "https://arxiv.org/pdf/2506.00823", "abs": "https://arxiv.org/abs/2506.00823", "authors": ["Yuntai Bao", "Xuhong Zhang", "Tianyu Du", "Xinkui Zhao", "Zhengwen Feng", "Hao Peng", "Jianwei Yin"], "title": "Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in LLMs Across Logical Transformations and Question Answering Tasks", "categories": ["cs.CL"], "comment": "19 pages, 16 figures; accepted to Findings of ACL 2025", "summary": "Large language models (LLMs) are trained on extensive datasets that\nencapsulate substantial world knowledge. However, their outputs often include\nconfidently stated inaccuracies. Earlier works suggest that LLMs encode\ntruthfulness as a distinct linear feature, termed the \"truth direction\", which\ncan classify truthfulness reliably. We address several open questions about the\ntruth direction: (i) whether LLMs universally exhibit consistent truth\ndirections; (ii) whether sophisticated probing techniques are necessary to\nidentify truth directions; and (iii) how the truth direction generalizes across\ndiverse contexts. Our findings reveal that not all LLMs exhibit consistent\ntruth directions, with stronger representations observed in more capable\nmodels, particularly in the context of logical negation. Additionally, we\ndemonstrate that truthfulness probes trained on declarative atomic statements\ncan generalize effectively to logical transformations, question-answering\ntasks, in-context learning, and external knowledge sources. Finally, we explore\nthe practical application of truthfulness probes in selective\nquestion-answering, illustrating their potential to improve user trust in LLM\noutputs. These results advance our understanding of truth directions and\nprovide new insights into the internal representations of LLM beliefs. Our code\nis public at https://github.com/colored-dye/truthfulness_probe_generalization", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u2018\u771f\u5b9e\u6027\u65b9\u5411\u2019\u7684\u666e\u904d\u6027\u3001\u8bc6\u522b\u65b9\u6cd5\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u9009\u62e9\u6027\u95ee\u7b54\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u5e7f\u6cdb\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5e76\u5305\u542b\u4e30\u5bcc\u7684\u77e5\u8bc6\uff0c\u4f46\u5176\u8f93\u51fa\u5e38\u5305\u542b\u81ea\u4fe1\u9648\u8ff0\u7684\u9519\u8bef\u3002\u524d\u4eba\u7814\u7a76\u63d0\u51fa\u2018\u771f\u5b9e\u6027\u65b9\u5411\u2019\u53ef\u4ee5\u53ef\u9760\u5206\u7c7b\u771f\u5b9e\u6027\uff0c\u4f46\u5c1a\u5b58\u591a\u4e2a\u672a\u89e3\u95ee\u9898\u3002", "method": "\u63a2\u8ba8\u4e86LLMs\u662f\u5426\u666e\u904d\u5177\u6709\u4e00\u81f4\u6027\u771f\u5b9e\u6027\u65b9\u5411\uff0c\u662f\u5426\u9700\u8981\u590d\u6742\u63a2\u6d4b\u6280\u672f\u8bc6\u522b\u771f\u5b9e\u6027\u65b9\u5411\uff0c\u4ee5\u53ca\u5176\u5728\u591a\u6837\u5316\u4e0a\u4e0b\u6587\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5229\u7528\u4e86\u903b\u8f91\u5426\u5b9a\u3001\u95ee\u7b54\u4efb\u52a1\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5916\u90e8\u77e5\u8bc6\u6e90\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u53d1\u73b0\u5e76\u975e\u6240\u6709LLMs\u5747\u6709\u4e00\u81f4\u771f\u5b9e\u6027\u65b9\u5411\uff0c\u8f83\u5f3a\u5927\u6a21\u578b\u5728\u6b64\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u5c24\u5176\u662f\u5728\u903b\u8f91\u5426\u5b9a\u4e2d\uff1b\u771f\u5b9e\u6027\u63a2\u6d4b\u65b9\u6cd5\u53ef\u6709\u6548\u6cdb\u5316\u81f3\u591a\u79cd\u4efb\u52a1\uff1b\u5b9e\u7528\u6027\u7814\u7a76\u8868\u660e\u5176\u80fd\u63d0\u5347\u7528\u6237\u5bf9LLM\u8f93\u51fa\u7684\u4fe1\u4efb\u3002", "conclusion": "\u7814\u7a76\u63a8\u8fdb\u4e86\u5bf9\u771f\u5b9e\u6027\u65b9\u5411\u7684\u7406\u89e3\uff0c\u5e76\u63ed\u793a\u4e86LLMs\u5185\u90e8\u4fe1\u5ff5\u8868\u5f81\u7684\u65b0\u89c6\u89d2\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u771f\u5b9e\u6027\u65b9\u5411, \u903b\u8f91\u5426\u5b9a, \u63a2\u6d4b\u6280\u672f, \u6cdb\u5316\u80fd\u529b, \u9009\u62e9\u6027\u95ee\u7b54"}}
{"id": "2506.00053", "pdf": "https://arxiv.org/pdf/2506.00053", "abs": "https://arxiv.org/abs/2506.00053", "authors": ["Sulaiman khan", "Muhammad Ahmad", "Fida Ullah", "Carlos Aguilar Iba\u00f1ez", "Jos\u00e9 Eduardo Valdez Rodriguez"], "title": "Improving statistical learning methods via features selection without replacement sampling and random projection", "categories": ["q-bio.QM", "cs.AI", "cs.LG", "stat.AP", "stat.ML"], "comment": null, "summary": "Cancer is fundamentally a genetic disease characterized by genetic and\nepigenetic alterations that disrupt normal gene expression, leading to\nuncontrolled cell growth and metastasis. High-dimensional microarray datasets\npose challenges for classification models due to the \"small n, large p\"\nproblem, resulting in overfitting. This study makes three different key\ncontributions: 1) we propose a machine learning-based approach integrating the\nFeature Selection Without Re-placement (FSWOR) technique and a projection\nmethod to improve classification accuracy. 2) We apply the Kendall statistical\ntest to identify the most significant genes from the brain cancer mi-croarray\ndataset (GSE50161), reducing the feature space from 54,675 to 20,890 genes.3)\nwe apply machine learning models using k-fold cross validation techniques in\nwhich our model incorpo-rates ensemble classifiers with LDA projection and\nNa\\\"ive Bayes, achieving a test score of 96%, outperforming existing methods by\n9.09%. The results demonstrate the effectiveness of our ap-proach in\nhigh-dimensional gene expression analysis, improving classification accuracy\nwhile mitigating overfitting. This study contributes to cancer biomarker\ndiscovery, offering a robust computational method for analyzing microarray\ndata.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408FSWOR\u6280\u672f\u548c\u6295\u5f71\u65b9\u6cd5\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u9ad8\u7ef4\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u7684\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5e76\u5728\u8111\u764c\u6570\u636e\u96c6\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u764c\u75c7\u662f\u4e00\u79cd\u7531\u57fa\u56e0\u548c\u8868\u89c2\u9057\u4f20\u6539\u53d8\u5f15\u8d77\u7684\u75be\u75c5\uff0c\u9ad8\u7ef4\u5fae\u9635\u5217\u6570\u636e\u7684\u5206\u7c7b\u9762\u4e34\u2018\u5c0fn\u5927p\u2019\u95ee\u9898\u5bfc\u81f4\u8fc7\u62df\u5408\u3002", "method": "\u63d0\u51faFSWOR\u6280\u672f\u7ed3\u5408\u6295\u5f71\u65b9\u6cd5\u8fdb\u884c\u7279\u5f81\u9009\u62e9\uff0c\u5e94\u7528Kendall\u7edf\u8ba1\u6d4b\u8bd5\u7b5b\u9009\u91cd\u8981\u57fa\u56e0\uff0c\u5e76\u91c7\u7528\u96c6\u6210\u5206\u7c7b\u5668\u4e0eLDA\u6295\u5f71\u548c\u6734\u7d20\u8d1d\u53f6\u65af\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u6d4b\u8bd5\u4e2d\u8fbe\u523096%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd59.09%\uff0c\u663e\u8457\u964d\u4f4e\u8fc7\u62df\u5408\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u764c\u75c7\u751f\u7269\u6807\u5fd7\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u5206\u6790\u3002", "keywords": "\u764c\u75c7\u5206\u7c7b, \u9ad8\u7ef4\u6570\u636e, \u7279\u5f81\u9009\u62e9, \u673a\u5668\u5b66\u4e60, \u57fa\u56e0\u8868\u8fbe"}}
{"id": "2506.00798", "pdf": "https://arxiv.org/pdf/2506.00798", "abs": "https://arxiv.org/abs/2506.00798", "authors": ["Jiankai Zheng", "Liang Xie"], "title": "A Dynamic Stiefel Graph Neural Network for Efficient Spatio-Temporal Time Series Forecasting", "categories": ["cs.LG"], "comment": "Accepted at IJCAI 2025", "summary": "Spatio-temporal time series (STTS) have been widely used in many\napplications. However, accurately forecasting STTS is challenging due to\ncomplex dynamic correlations in both time and space dimensions. Existing graph\nneural networks struggle to balance effectiveness and efficiency in modeling\ndynamic spatio-temporal relations. To address this problem, we propose the\nDynamic Spatio-Temporal Stiefel Graph Neural Network (DST-SGNN) to efficiently\nprocess STTS. For DST-SGNN, we first introduce the novel Stiefel Graph Spectral\nConvolution (SGSC) and Stiefel Graph Fourier Transform (SGFT). The SGFT matrix\nin SGSC is constrained to lie on the Stiefel manifold, and SGSC can be regarded\nas a filtered graph spectral convolution. We also propose the Linear Dynamic\nGraph Optimization on Stiefel Manifold (LDGOSM), which can efficiently learn\nthe SGFT matrix from the dynamic graph and significantly reduce the\ncomputational complexity. Finally, we propose a multi-layer SGSC (MSGSC) that\nefficiently captures complex spatio-temporal correlations. Extensive\nexperiments on seven spatio-temporal datasets show that DST-SGNN outperforms\nstate-of-the-art methods while maintaining relatively low computational costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDST-SGNN\u7684\u52a8\u6001\u65f6\u7a7aStiefel\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7SGSC\u548cSGFT\u6280\u672f\u9ad8\u6548\u5904\u7406\u65f6\u7a7a\u65f6\u95f4\u5e8f\u5217\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6210\u672c\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u65f6\u7a7a\u65f6\u95f4\u5e8f\u5217\uff08STTS\uff09\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u7ef4\u5ea6\u4e0a\u5b58\u5728\u590d\u6742\u7684\u52a8\u6001\u76f8\u5173\u6027\uff0c\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u96be\u4ee5\u5e73\u8861\u5efa\u6a21\u52a8\u6001\u65f6\u7a7a\u5173\u7cfb\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "method": "\u5f15\u5165\u4e86Stiefel\u56fe\u8c31\u5377\u79ef\uff08SGSC\uff09\u548cStiefel\u56fe\u5085\u91cc\u53f6\u53d8\u6362\uff08SGFT\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u7ebf\u6027\u52a8\u6001\u56fe\u4f18\u5316\uff08LDGOSM\uff09\u548c\u591a\u5c42SGSC\uff08MSGSC\uff09\u6765\u9ad8\u6548\u6355\u83b7\u590d\u6742\u7684\u65f6\u7a7a\u76f8\u5173\u6027\u3002", "result": "\u5728\u4e03\u4e2a\u65f6\u7a7a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDST-SGNN\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "DST-SGNN\u901a\u8fc7\u65b0\u9896\u7684\u6280\u672f\u5728\u65f6\u7a7a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u7684\u5e73\u8861\u3002", "keywords": "\u65f6\u7a7a\u65f6\u95f4\u5e8f\u5217\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u3001Stiefel\u6d41\u5f62\u3001\u52a8\u6001\u56fe\u4f18\u5316\u3001SGSC\u3001SGFT"}}
{"id": "2506.00826", "pdf": "https://arxiv.org/pdf/2506.00826", "abs": "https://arxiv.org/abs/2506.00826", "authors": ["Yongkang Xiao", "Rui Zhang"], "title": "HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs)\nby incorporating diverse modalities such as images and text. Multi-modal\nknowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals\nto infer missing facts, thereby mitigating the intrinsic incompleteness of\nMMKGs. Existing MMKGC methods typically leverage only the information contained\nin the MMKGs under the closed-world assumption and adopt discriminative\ntraining objectives, which limits their reasoning capacity during completion.\nRecent generative completion approaches powered by advanced large language\nmodels (LLMs) have shown strong reasoning abilities in unimodal knowledge graph\ncompletion, but their potential in MMKGC remains largely unexplored. To bridge\nthis gap, we propose HERGC, a Heterogeneous Experts Representation and\nGenerative Completion framework for MMKGs. HERGC first deploys a Heterogeneous\nExperts Representation Retriever that enriches and fuses multimodal information\nand retrieves a compact candidate set for each incomplete triple. It then uses\na Generative LLM Predictor fine-tuned on minimal instruction data to accurately\nidentify the correct answer from these candidates. Extensive experiments on\nthree standard MMKG benchmarks demonstrate HERGC's effectiveness and\nrobustness, achieving state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u540d\u4e3aHERGC\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u548c\u751f\u6210\u5f0f\u9884\u6d4b\u6765\u89e3\u51b3\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c01\u95ed\u4e16\u754c\u5047\u8bbe\u4e0b\u4ec5\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u5185\u90e8\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u800c\u751f\u6210\u5f0f\u65b9\u6cd5\u6f5c\u529b\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "HERGC\u6846\u67b6\u7ed3\u5408\u5f02\u6784\u4e13\u5bb6\u8868\u5f81\u68c0\u7d22\u5668\u548c\u751f\u6210\u5f0fLLM\u9884\u6d4b\u5668\uff0c\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\u5e76\u751f\u6210\u5019\u9009\u7b54\u6848\u3002", "result": "\u5728\u4e09\u4e2a\u6807\u51c6MMKG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "HERGC\u6709\u6548\u89e3\u51b3\u4e86MMKGC\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u751f\u6210\u5f0f\u65b9\u6cd5\u7684\u6f5c\u529b\u3002", "keywords": "\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31, \u77e5\u8bc6\u56fe\u8c31\u8865\u5168, \u751f\u6210\u5f0f\u6a21\u578b, HERGC"}}
{"id": "2506.00058", "pdf": "https://arxiv.org/pdf/2506.00058", "abs": "https://arxiv.org/abs/2506.00058", "authors": ["An Vu", "Jonas Oppenlaender"], "title": "Prompt Engineer: Analyzing Skill Requirements in the AI Job Market", "categories": ["cs.CY", "cs.AI", "cs.HC", "I.2.m; H.5.m"], "comment": "42 pages, 8 figures", "summary": "The rise of large language models (LLMs) has created a new job role: the\nPrompt Engineer. Despite growing interest in this position, we still do not\nfully understand what skills this new job role requires or how common these\njobs are. We analyzed 20,662 job postings on LinkedIn, including 72 prompt\nengineer positions, to learn more about this emerging role. We found that\nprompt engineering is still rare (less than 0.5% of sampled job postings) but\nhas a unique skill profile. Prompt engineers need AI knowledge (22.8%), prompt\ndesign skills (18.7%), good communication (21.9%), and creative problem-solving\n(15.8%) skills. These requirements significantly differ from those of\nestablished roles, such as data scientists and machine learning engineers,\nshowing that prompt engineering is becoming its own profession. Our findings\nhelp job seekers, employers, and educational institutions in better\nunderstanding the emerging field of prompt engineering.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u63d0\u793a\u5de5\u7a0b\u5e08\u8fd9\u4e00\u65b0\u5174\u804c\u4f4d\u7684\u6280\u80fd\u9700\u6c42\u548c\u5e02\u573a\u73b0\u72b6\uff0c\u53d1\u73b0\u5176\u5177\u6709\u72ec\u7279\u7684\u6280\u80fd\u7ec4\u5408\uff0c\u5e76\u9010\u6e10\u5f62\u6210\u72ec\u7acb\u804c\u4e1a\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e86\u89e3\u63d0\u793a\u5de5\u7a0b\u5e08\u804c\u4f4d\u7684\u6280\u80fd\u9700\u6c42\u548c\u5e02\u573a\u666e\u53ca\u7a0b\u5ea6\uff0c\u586b\u8865\u5bf9\u8fd9\u4e00\u65b0\u5174\u89d2\u8272\u8ba4\u8bc6\u7684\u7a7a\u767d\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u5206\u6790LinkedIn\u4e0a\u768420,662\u4e2a\u804c\u4f4d\u62db\u8058\u4fe1\u606f\uff0c\u5176\u4e2d\u5305\u62ec72\u4e2a\u63d0\u793a\u5de5\u7a0b\u5e08\u804c\u4f4d\u3002", "result": "\u7ed3\u679c\u663e\u793a\u63d0\u793a\u5de5\u7a0b\u5e08\u804c\u4f4d\u4ecd\u8f83\u7f55\u89c1\uff08\u5360\u6837\u672c\u76840.5%\u4ee5\u4e0b\uff09\uff0c\u4f46\u5177\u5907\u72ec\u7279\u7684\u6280\u80fd\u7ec4\u5408\uff0c\u5982AI\u77e5\u8bc6\uff0822.8%\uff09\u3001\u63d0\u793a\u8bbe\u8ba1\u6280\u80fd\uff0818.7%\uff09\u3001\u6c9f\u901a\u80fd\u529b\uff0821.9%\uff09\u548c\u521b\u610f\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0815.8%\uff09\u3002", "conclusion": "\u7ed3\u8bba\u662f\u63d0\u793a\u5de5\u7a0b\u5e08\u6b63\u6210\u4e3a\u4e00\u95e8\u72ec\u7acb\u804c\u4e1a\uff0c\u7814\u7a76\u6210\u679c\u6709\u52a9\u4e8e\u6c42\u804c\u8005\u3001\u96c7\u4e3b\u548c\u6559\u80b2\u673a\u6784\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u63d0\u793a\u5de5\u7a0b\u5e08, \u6280\u80fd\u5206\u6790, \u804c\u4e1a\u5e02\u573a"}}
{"id": "2506.00799", "pdf": "https://arxiv.org/pdf/2506.00799", "abs": "https://arxiv.org/abs/2506.00799", "authors": ["Kaiyang Li", "Shaobo Han", "Qing Su", "Wei Li", "Zhipeng Cai", "Shihao Ji"], "title": "Uni-LoRA: One Vector is All You Need", "categories": ["cs.LG"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient\nfine-tuning (PEFT) method for large language models (LLMs) by constraining\nweight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and\nVB-LoRA push efficiency further by introducing additional constraints to reduce\nthe trainable parameter space. In this paper, we show that the parameter space\nreduction strategies employed by these LoRA variants can be formulated within a\nunified framework, Uni-LoRA, where the LoRA parameter space, flattened as a\nhigh-dimensional vector space $R^D$, can be reconstructed through a projection\nfrom a subspace R^d, with $d \\ll D$. We demonstrate that the fundamental\ndifference among various LoRA methods lies in the choice of the projection\nmatrix, $P \\in R^{D \\times d}$.Most existing LoRA variants rely on layer-wise\nor structure-specific projections that limit cross-layer parameter sharing,\nthereby compromising parameter efficiency. In light of this, we introduce an\nefficient and theoretically grounded projection matrix that is isometric,\nenabling global parameter sharing and reducing computation overhead.\nFurthermore, under the unified view of Uni-LoRA, this design requires only a\nsingle trainable vector to reconstruct LoRA parameters for the entire LLM -\nmaking Uni-LoRA both a unified framework and a \"one-vector-only\" solution.\nExtensive experiments on GLUE, mathematical reasoning, and instruction tuning\nbenchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter\nefficiency while outperforming or matching prior approaches in predictive\nperformance.", "AI": {"tldr": "Uni-LoRA\u662f\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u4e14\u7406\u8bba\u652f\u6301\u7684\u6295\u5f71\u77e9\u9635\u5b9e\u73b0\u4e86\u5168\u5c40\u53c2\u6570\u5171\u4eab\uff0c\u663e\u8457\u63d0\u5347\u4e86LoRA\u53d8\u4f53\u7684\u53c2\u6570\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684LoRA\u53d8\u4f53\u901a\u8fc7\u5c42\u6216\u7ed3\u6784\u7279\u5b9a\u7684\u6295\u5f71\u9650\u5236\u4e86\u8de8\u5c42\u53c2\u6570\u5171\u4eab\uff0c\u5f71\u54cd\u4e86\u53c2\u6570\u6548\u7387\u3002Uni-LoRA\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u63d0\u5347\u53c2\u6570\u6548\u7387\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51faUni-LoRA\u6846\u67b6\uff0c\u901a\u8fc7\u7b49\u8ddd\u6295\u5f71\u77e9\u9635\u4ece\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u91cd\u6784LoRA\u53c2\u6570\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u5168\u5c40\u53c2\u6570\u5171\u4eab\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u4ec5\u9700\u5355\u4e2a\u53ef\u8bad\u7ec3\u5411\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08GLUE\u3001\u6570\u5b66\u63a8\u7406\u548c\u6307\u4ee4\u8c03\u4f18\uff09\u4e2d\uff0cUni-LoRA\u5728\u53c2\u6570\u6548\u7387\u548c\u9884\u6d4b\u6027\u80fd\u4e0a\u90fd\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Uni-LoRA\u4e0d\u4ec5\u7edf\u4e00\u4e86\u73b0\u6709LoRA\u53d8\u4f53\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5355\u5411\u91cf\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "LoRA, \u53c2\u6570\u9ad8\u6548\u5fae\u8c03, \u7edf\u4e00\u6846\u67b6, \u5168\u5c40\u53c2\u6570\u5171\u4eab, \u7b49\u8ddd\u6295\u5f71"}}
{"id": "2506.00829", "pdf": "https://arxiv.org/pdf/2506.00829", "abs": "https://arxiv.org/abs/2506.00829", "authors": ["Keyuan Cheng", "Zijian Kan", "Zhixian He", "Zhuoran Zhang", "Muhammad Asif Ali", "Ke Xu", "Lijie Hu", "Di Wang"], "title": "COMPKE: Complex Question Answering under Knowledge Editing", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ACL 2025 Findings", "summary": "Knowledge Editing, which efficiently modifies the knowledge in large language\nmodels, has gathered great attention. Current benchmarks primarily use\nmulti-hop question answering to assess and analyze newly injected or updated\nknowledge. However, we argue that these benchmarks fail to effectively evaluate\nhow well the updated models apply this knowledge in real-life scenarios,\nparticularly when questions require complex reasoning, involving one-to-many\nrelationships or multi-step logical intersections. To fill in this gap, we\nintroduce a new benchmark, COMPKE: Complex Question Answering under Knowledge\nEditing, which includes 11,924 complex questions that reflect real-life\nsituations. We conduct an extensive evaluation of four knowledge editing\nmethods on COMPKE, revealing that their effectiveness varies notably across\ndifferent models. For instance, MeLLo attains an accuracy of 39.47 on\nGPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further\ninvestigate the underlying causes of these disparities from both methodological\nand model-specific perspectives. The datasets are available at\nhttps://github.com/kzjkzj666/CompKE.", "AI": {"tldr": "COMPKE\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u542b11,924\u4e2a\u590d\u6742\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u548c\u6a21\u578b\u95f4\u7684\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165COMPKE\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b11,924\u4e2a\u590d\u6742\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u56db\u79cd\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u4e0d\u540c\u6a21\u578b\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u4e0d\u540c\u65b9\u6cd5\u5728\u4e0d\u540c\u6a21\u578b\u4e0a\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u4f8b\u5982MeLLo\u5728GPT-4O-MINI\u4e0a\u51c6\u786e\u7387\u4e3a39.47\uff0c\u800c\u5728QWEN2.5-3B\u4e0a\u4ec5\u4e3a3.83\u3002", "conclusion": "COMPKE\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e0d\u8db3\uff0c\u63ed\u793a\u4e86\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u63a8\u7406\u573a\u666f\u4e2d\u3002", "keywords": "\u77e5\u8bc6\u7f16\u8f91\u3001\u57fa\u51c6\u6d4b\u8bd5\u3001\u590d\u6742\u63a8\u7406\u3001\u591a\u8df3\u95ee\u7b54"}}
{"id": "2506.00060", "pdf": "https://arxiv.org/pdf/2506.00060", "abs": "https://arxiv.org/abs/2506.00060", "authors": ["Sina Amirrajab", "Volker Vehof", "Michael Bietenbeck", "Ali Yilmaz"], "title": "Comparative analysis of privacy-preserving open-source LLMs regarding extraction of diagnostic information from clinical CMR imaging reports", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "under review for Scientific Reports", "summary": "Purpose: We investigated the utilization of privacy-preserving,\nlocally-deployed, open-source Large Language Models (LLMs) to extract\ndiagnostic information from free-text cardiovascular magnetic resonance (CMR)\nreports. Materials and Methods: We evaluated nine open-source LLMs on their\nability to identify diagnoses and classify patients into various cardiac\ndiagnostic categories based on descriptive findings in 109 clinical CMR\nreports. Performance was quantified using standard classification metrics\nincluding accuracy, precision, recall, and F1 score. We also employed confusion\nmatrices to examine patterns of misclassification across models. Results: Most\nopen-source LLMs demonstrated exceptional performance in classifying reports\ninto different diagnostic categories. Google's Gemma2 model achieved the\nhighest average F1 score of 0.98, followed by Qwen2.5:32B and DeepseekR1-32B\nwith F1 scores of 0.96 and 0.95, respectively. All other evaluated models\nattained average scores above 0.93, with Mistral and DeepseekR1-7B being the\nonly exceptions. The top four LLMs outperformed our board-certified\ncardiologist (F1 score of 0.94) across all evaluation metrics in analyzing CMR\nreports. Conclusion: Our findings demonstrate the feasibility of implementing\nopen-source, privacy-preserving LLMs in clinical settings for automated\nanalysis of imaging reports, enabling accurate, fast and resource-efficient\ndiagnostic categorization.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5f00\u6e90\u9690\u79c1\u4fdd\u62a4\u672c\u5730\u90e8\u7f72\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u8840\u7ba1\u78c1\u5171\u632f\u62a5\u544a\u8bca\u65ad\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u672c\u5730\u90e8\u7f72\u7684\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u81ea\u7531\u6587\u672c\u7684\u5fc3\u8840\u7ba1\u78c1\u5171\u632f\u62a5\u544a\u4e2d\u63d0\u53d6\u8bca\u65ad\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u4e34\u5e8a\u6548\u7387\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "method": "\u8bc4\u4f30\u4e86\u4e5d\u79cd\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728109\u4efd\u4e34\u5e8a\u5fc3\u8840\u7ba1\u78c1\u5171\u632f\u62a5\u544a\u4e2d\u7684\u8bca\u65ad\u5206\u7c7b\u80fd\u529b\uff0c\u4f7f\u7528\u51c6\u786e\u6027\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u7b49\u6307\u6807\u8fdb\u884c\u91cf\u5316\u3002", "result": "\u591a\u6570\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0cGoogle\u7684Gemma2\u6a21\u578bF1\u5206\u6570\u6700\u9ad8\uff080.98\uff09\uff0c\u90e8\u5206\u6a21\u578b\u751a\u81f3\u8d85\u8fc7\u4e13\u4e1a\u5fc3\u810f\u79d1\u533b\u751f\uff08F1\u5206\u65700.94\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5f00\u6e90\u9690\u79c1\u4fdd\u62a4\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u9ad8\u6548\u3001\u51c6\u786e\u5730\u5b9e\u73b0\u5f71\u50cf\u62a5\u544a\u7684\u81ea\u52a8\u5316\u5206\u6790\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u73af\u5883\u3002", "keywords": "\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b,\u5fc3\u8840\u7ba1\u78c1\u5171\u632f,\u9690\u79c1\u4fdd\u62a4,\u4e34\u5e8a\u62a5\u544a\u5206\u6790,\u8bca\u65ad\u5206\u7c7b"}}
{"id": "2506.00808", "pdf": "https://arxiv.org/pdf/2506.00808", "abs": "https://arxiv.org/abs/2506.00808", "authors": ["Jiahao Zhang", "Yilong Wang", "Zhiwei Zhang", "Xiaorui Liu", "Suhang Wang"], "title": "Unlearning Inversion Attacks for Graph Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Graph unlearning methods aim to efficiently remove the impact of sensitive\ndata from trained GNNs without full retraining, assuming that deleted\ninformation cannot be recovered. In this work, we challenge this assumption by\nintroducing the graph unlearning inversion attack: given only black-box access\nto an unlearned GNN and partial graph knowledge, can an adversary reconstruct\nthe removed edges? We identify two key challenges: varying\nprobability-similarity thresholds for unlearned versus retained edges, and the\ndifficulty of locating unlearned edge endpoints, and address them with\nTrendAttack. First, we derive and exploit the confidence pitfall, a theoretical\nand empirical pattern showing that nodes adjacent to unlearned edges exhibit a\nlarge drop in model confidence. Second, we design an adaptive prediction\nmechanism that applies different similarity thresholds to unlearned and other\nmembership edges. Our framework flexibly integrates existing membership\ninference techniques and extends them with trend features. Experiments on four\nreal-world datasets demonstrate that TrendAttack significantly outperforms\nstate-of-the-art GNN membership inference baselines, exposing a critical\nprivacy vulnerability in current graph unlearning methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6311\u6218\u4e86\u56fe\u53cd\u5b66\u4e60\u65b9\u6cd5\u7684\u5047\u8bbe\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u56fe\u53cd\u5b66\u4e60\u53cd\u8f6c\u653b\u51fb\uff08TrendAttack\uff09\uff0c\u80fd\u591f\u901a\u8fc7\u9ed1\u76d2\u8bbf\u95ee\u90e8\u5206\u6062\u590d\u88ab\u5220\u9664\u7684\u8fb9\u7684\u4fe1\u606f\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u9a8c\u8bc1\u56fe\u53cd\u5b66\u4e60\u65b9\u6cd5\u7684\u5b89\u5168\u5047\u8bbe\u662f\u5426\u6210\u7acb\uff0c\u5373\u88ab\u5220\u9664\u7684\u654f\u611f\u6570\u636e\u662f\u5426\u771f\u7684\u65e0\u6cd5\u6062\u590d\u3002", "method": "\u63d0\u51faTrendAttack\u65b9\u6cd5\uff0c\u5229\u7528\u7f6e\u4fe1\u5ea6\u9677\u9631\u548c\u81ea\u9002\u5e94\u9884\u6d4b\u673a\u5236\u533a\u5206\u672a\u5b66\u4e60\u8fb9\u4e0e\u5176\u4ed6\u8fb9\uff0c\u7ed3\u5408\u73b0\u6709\u7684\u6210\u5458\u63a8\u7406\u6280\u672f\u8fdb\u884c\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTrendAttack\u5728\u56db\u4e2a\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684GNN\u6210\u5458\u63a8\u7406\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u56fe\u53cd\u5b66\u4e60\u65b9\u6cd5\u7684\u9690\u79c1\u6f0f\u6d1e\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u5f53\u524d\u7684\u56fe\u53cd\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u7684\u9690\u79c1\u98ce\u9669\uff0c\u9700\u8981\u66f4\u5b89\u5168\u7684\u673a\u5236\u6765\u4fdd\u62a4\u654f\u611f\u6570\u636e\u3002", "keywords": "\u56fe\u53cd\u5b66\u4e60,\u9690\u79c1\u653b\u51fb,\u6210\u5458\u63a8\u7406,TrendAttack,\u7f6e\u4fe1\u5ea6\u9677\u9631,\u9ed1\u76d2\u653b\u51fb"}}
{"id": "2506.00842", "pdf": "https://arxiv.org/pdf/2506.00842", "abs": "https://arxiv.org/abs/2506.00842", "authors": ["Jiawei Gu", "Ziting Xian", "Yuanzhen Xie", "Ye Liu", "Enjie Liu", "Ruichao Zhong", "Mochi Gao", "Yunzhi Tan", "Bo Hu", "Zang Li"], "title": "Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) achieve strong performance on plain text tasks\nbut underperform on structured data like tables and databases. Potential\nchallenges arise from their underexposure during pre-training and rigid\ntext-to-structure transfer mechanisms. Unlike humans who seamlessly apply\nlearned patterns across data modalities, LLMs struggle to infer implicit\nrelationships embedded in tabular formats, especially in the absence of\nexplicit structural guidance. To bridge this cognitive gap, we introduce\nContrastive Retrieval-Augmented Generation on Experience (CoRE), a framework\nthat builds experience memory representations and enhances generalization\nthrough contrastive In-Context Learning (ICL) to simulate human-like knowledge\ntransfer. Experiments on Text-to-SQL and TableQA show CoRE significantly\nimproves performance, achieving average gains of 3.44% and 4.24%, with up to\n17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated\nExperience Memory expands training data 8-9x, enhancing diversity and domain\ncoverage. This training-free and continual method propels LLMs toward\nstructured knowledge expertise.", "AI": {"tldr": "CoRE\u6846\u67b6\u901a\u8fc7\u5bf9\u6bd4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u6570\u636e\u4efb\u52a1\uff08\u5982Text-to-SQL\u548cTableQA\uff09\u4e0a\u7684\u6027\u80fd\uff0c\u5e73\u5747\u63d0\u53473.44%\u548c4.24%\uff0c\u6700\u9ad8\u53ef\u8fbe17.2%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u6570\u636e\uff08\u5982\u8868\u683c\u548c\u6570\u636e\u5e93\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u4e3a\u9884\u8bad\u7ec3\u4e2d\u5bf9\u7ed3\u6784\u5316\u6570\u636e\u63a5\u89e6\u4e0d\u8db3\uff0c\u4ee5\u53ca\u6587\u672c\u5230\u7ed3\u6784\u7684\u8f6c\u6362\u673a\u5236\u8fc7\u4e8e\u521a\u6027\u3002", "method": "\u5f15\u5165CoRE\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u7ecf\u9a8c\u8bb0\u5fc6\u8868\u793a\u6a21\u62df\u4eba\u7c7b\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5e76\u5229\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCoRE\u5728Text-to-SQL\u548cTableQA\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u7ecf\u9a8c\u8bb0\u5fc6\u4f7f\u8bad\u7ec3\u6570\u636e\u6269\u5927\u4e868-9\u500d\u3002", "conclusion": "CoRE\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65e0\u9700\u8bad\u7ec3\u3001\u6301\u7eed\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5176\u5728\u7ed3\u6784\u5316\u6570\u636e\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u7ed3\u6784\u5316\u6570\u636e,CoRE\u6846\u67b6,\u4e0a\u4e0b\u6587\u5b66\u4e60,\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22"}}
{"id": "2506.00844", "pdf": "https://arxiv.org/pdf/2506.00844", "abs": "https://arxiv.org/abs/2506.00844", "authors": ["Xingyu Wu", "Kui Yu", "Jibin Wu", "Kay Chen Tan"], "title": "LLM Cannot Discover Causality, and Should Be Restricted to Non-Decisional Support in Causal Discovery", "categories": ["cs.LG"], "comment": null, "summary": "This paper critically re-evaluates LLMs' role in causal discovery and argues\nagainst their direct involvement in determining causal relationships. We\ndemonstrate that LLMs' autoregressive, correlation-driven modeling inherently\nlacks the theoretical grounding for causal reasoning and introduces\nunreliability when used as priors in causal discovery algorithms. Through\nempirical studies, we expose the limitations of existing LLM-based methods and\nreveal that deliberate prompt engineering (e.g., injecting ground-truth\nknowledge) could overstate their performance, helping to explain the\nconsistently favorable results reported in much of the current literature.\nBased on these findings, we strictly confined LLMs' role to a non-decisional\nauxiliary capacity: LLMs should not participate in determining the existence or\ndirectionality of causal relationships, but can assist the search process for\ncausal graphs (e.g., LLM-based heuristic search). Experiments across various\nsettings confirm that, by strictly isolating LLMs from causal decision-making,\nLLM-guided heuristic search can accelerate the convergence and outperform both\ntraditional and LLM-based methods in causal structure learning. We conclude\nwith a call for the community to shift focus from naively applying LLMs to\ndeveloping specialized models and training method that respect the core\nprinciples of causal discovery.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u8bc4\u4f30\u4e86LLMs\u5728\u56e0\u679c\u53d1\u73b0\u4e2d\u7684\u4f5c\u7528\uff0c\u53cd\u5bf9\u5176\u76f4\u63a5\u53c2\u4e0e\u786e\u5b9a\u56e0\u679c\u5173\u7cfb\uff0c\u6307\u51fa\u5176\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u5e76\u53ef\u80fd\u5bfc\u81f4\u4e0d\u53ef\u9760\u6027\uff0c\u5b9e\u8bc1\u7814\u7a76\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u56e0\u679c\u53d1\u73b0\u4e2d\u7684\u9002\u7528\u6027\uff0c\u63ed\u793a\u5176\u5c40\u9650\u6027\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790LLMs\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u9650\u5236\u5176\u89d2\u8272\u4e3a\u975e\u51b3\u7b56\u8f85\u52a9\u7684\u5efa\u8bae\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9650\u5236LLMs\u4e3a\u975e\u51b3\u7b56\u8f85\u52a9\u53ef\u52a0\u901f\u6536\u655b\u5e76\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u547c\u5401\u793e\u533a\u5f00\u53d1\u5c0a\u91cd\u56e0\u679c\u53d1\u73b0\u6838\u5fc3\u539f\u7406\u7684\u4e13\u7528\u6a21\u578b\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002", "keywords": "LLMs, \u56e0\u679c\u53d1\u73b0, \u542f\u53d1\u5f0f\u641c\u7d22, \u53ef\u9760\u6027"}}
{"id": "2506.00854", "pdf": "https://arxiv.org/pdf/2506.00854", "abs": "https://arxiv.org/abs/2506.00854", "authors": ["Jacky Tai-Yu Lu", "Jung Chiang", "Chi-Sheng Chen", "Anna Nai-Yun Tung", "Hsiang Wei Hu", "Yuan Chiao Cheng"], "title": "EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM", "q-bio.NC"], "comment": null, "summary": "We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one\nof the earliest open-vocabulary EEG-to-text generation frameworks tailored for\nChinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact\npretrained language model (MiniLM), our architecture aligns multichannel brain\nsignals with natural language representations via masked pretraining and\ncontrastive learning. Using a subset of the ChineseEEG dataset, where each\nsentence contains approximately ten Chinese characters aligned with 128-channel\nEEG recorded at 256 Hz, we segment EEG into per-character embeddings and\npredict full sentences in a zero-shot setting. The decoder is trained with\nteacher forcing and padding masks to accommodate variable-length sequences.\nEvaluation on over 1,500 training-validation sentences and 300 held-out test\nsamples shows promising lexical alignment, with a best BLEU-1 score of 6.38\\%.\nWhile syntactic fluency remains a challenge, our findings demonstrate the\nfeasibility of non-phonetic, cross-modal language decoding from EEG. This work\nopens a new direction in multilingual brain-to-text research and lays the\nfoundation for future cognitive-language interfaces in Chinese.", "AI": {"tldr": "EEG2TEXT-CN\u662f\u4e00\u4e2a\u9488\u5bf9\u6c49\u8bed\u7684\u5f00\u6e90\u8bcd\u6c47EEG\u5230\u6587\u672c\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u751f\u7269\u542f\u53d1\u7684EEG\u7f16\u7801\u5668\u548c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u63a9\u7801\u9884\u8bad\u7ec3\u548c\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u8111\u4fe1\u53f7\u4e0e\u81ea\u7136\u8bed\u8a00\u7684\u6620\u5c04\u3002\u5728\u4e2d\u6587EEG\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u53ef\u884c\u6027\uff0c\u4f46\u53e5\u6cd5\u6d41\u7545\u6027\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u63a2\u7d22\u6c49\u8bed\u73af\u5883\u4e0b\u4ece\u8111\u7535\u4fe1\u53f7\uff08EEG\uff09\u5230\u6587\u672c\u7684\u975e\u8bed\u97f3\u8de8\u6a21\u6001\u89e3\u7801\uff0c\u4e3a\u672a\u6765\u7684\u8ba4\u77e5\u8bed\u8a00\u63a5\u53e3\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u4f7f\u7528NICE-EEG\u7f16\u7801\u5668\u548cMiniLM\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u63a9\u7801\u9884\u8bad\u7ec3\u548c\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u591a\u901a\u9053\u8111\u4fe1\u53f7\u4e0e\u8bed\u8a00\u8868\u793a\uff0c\u5e76\u5728\u4e2d\u6587EEG\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u3002", "result": "\u57281500\u591a\u4e2a\u8bad\u7ec3\u9a8c\u8bc1\u53e5\u5b50\u548c300\u4e2a\u6d4b\u8bd5\u6837\u672c\u4e0a\uff0cBLEU-1\u5f97\u5206\u8fbe\u52306.38%\uff0c\u8bc1\u660e\u4e86\u975e\u8bed\u97f3\u8de8\u6a21\u6001\u89e3\u7801\u7684\u53ef\u884c\u6027\u3002", "conclusion": "EEG2TEXT-CN\u4e3a\u6c49\u8bed\u8111\u673a\u6587\u672c\u89e3\u7801\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u4f46\u53e5\u6cd5\u6d41\u7545\u6027\u4ecd\u9700\u4f18\u5316\u3002", "keywords": "EEG-to-text, Chinese, cross-modal decoding, brain-computer interface, NICE-EEG, MiniLM"}}
{"id": "2506.00845", "pdf": "https://arxiv.org/pdf/2506.00845", "abs": "https://arxiv.org/abs/2506.00845", "authors": ["Yizhuo Zhang", "Heng Wang", "Shangbin Feng", "Zhaoxuan Tan", "Xinyun Liu", "Yulia Tsvetkov"], "title": "Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning", "categories": ["cs.LG", "cs.CL", "I.2.7; I.2.2"], "comment": "9 pages, 3 figures, 3 tables. Experimental code and results are\n  publicly available at\n  https://anonymous.4open.science/r/Graph_RL-BF08/readme.md", "summary": "Previous research has sought to enhance the graph reasoning capabilities of\nLLMs by supervised fine-tuning on synthetic graph data. While these led to\nspecialized LLMs better at solving graph algorithm problems, we don't need LLMs\nfor shortest path: we need generalization from synthetic graph data to\nreal-world tasks with implicit graph structures. In this work, we propose to\nunlock generalizable learning of graph synthetic data with reinforcement\nlearning. We first design solution-based and process-based rewards for\nsynthetic graph problems: instead of rigid memorizing response patterns in\ndirect fine-tuning, we posit that RL would help LLMs grasp the essentials\nunderlying graph reasoning and alleviate overfitting. We employ RL algorithms\nsuch as GRPO and DPO, aligning both off-the-shelf LLMs and LLMs fine-tuned on\nsynthetic graph data. We then compare them against existing settings on both\nin-domain synthetic tasks and out-of-domain real-world tasks with implicit\ngraph structures such as multi-hop QA, structured planning, and more. Extensive\nexperiments demonstrate that our RL recipe leads to statistically significant\nimprovement on 5 datasets, with an average gain of 12.9\\% over baseline\nsettings. Further analysis reveals that process-based rewards consistently\noutperform solution-based rewards, mixing synthetic and real-world task data\nyields potential gains, while compositionality and explainable intermediate\nsteps remains a critical challenge even after RL.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u66ff\u4ee3\u4f20\u7edf\u7684\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347LLMs\u5728\u5408\u6210\u56fe\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684\u9690\u5f0f\u56fe\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u4f7fLLMs\u5728\u5408\u6210\u56fe\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u7814\u7a76\u8f6c\u5411\u5f3a\u5316\u5b66\u4e60\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u89e3\u548c\u57fa\u4e8e\u8fc7\u7a0b\u7684\u5956\u52b1\u673a\u5236\uff0c\u4f7f\u7528GRPO\u548cDPO\u7b49RL\u7b97\u6cd5\uff0c\u5bf9LLMs\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cRL\u65b9\u6cd5\u57285\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u534712.9%\uff0c\u57fa\u4e8e\u8fc7\u7a0b\u7684\u5956\u52b1\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u7ec4\u5408\u6027\u548c\u89e3\u91ca\u6027\u4ecd\u662f\u6311\u6218\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u63d0\u5347LLMs\u7684\u56fe\u63a8\u7406\u6cdb\u5316\u80fd\u529b\u4e0a\u6709\u6548\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u89e3\u51b3\u7ec4\u5408\u6027\u548c\u89e3\u91ca\u6027\u95ee\u9898\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u56fe\u63a8\u7406, \u5927\u8bed\u8a00\u6a21\u578b, \u6cdb\u5316\u80fd\u529b, \u771f\u5b9e\u4e16\u754c\u4efb\u52a1"}}
{"id": "2506.00859", "pdf": "https://arxiv.org/pdf/2506.00859", "abs": "https://arxiv.org/abs/2506.00859", "authors": ["Md Kowsher", "Nusrat Jahan Prottasha", "Shiyun Xu", "Shetu Mohanto", "Chen Chen", "Niloofar Yousefi", "Ozlem Garibay"], "title": "How Bidirectionality Helps Language Models Learn Better via Dynamic Bottleneck Estimation", "categories": ["cs.CL"], "comment": null, "summary": "Bidirectional language models have better context understanding and perform\nbetter than unidirectional models on natural language understanding tasks, yet\nthe theoretical reasons behind this advantage remain unclear. In this work, we\ninvestigate this disparity through the lens of the Information Bottleneck (IB)\nprinciple, which formalizes a trade-off between compressing input information\nand preserving task-relevant content. We propose FlowNIB, a dynamic and\nscalable method for estimating mutual information during training that\naddresses key limitations of classical IB approaches, including computational\nintractability and fixed trade-off schedules. Theoretically, we show that\nbidirectional models retain more mutual information and exhibit higher\neffective dimensionality than unidirectional models. To support this, we\npresent a generalized framework for measuring representational complexity and\nprove that bidirectional representations are strictly more informative under\nmild conditions. We further validate our findings through extensive experiments\nacross multiple models and tasks using FlowNIB, revealing how information is\nencoded and compressed throughout training. Together, our work provides a\nprincipled explanation for the effectiveness of bidirectional architectures and\nintroduces a practical tool for analyzing information flow in deep language\nmodels.", "AI": {"tldr": "\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\uff08IB\uff09\u539f\u5219\uff0c\u7814\u7a76\u53d1\u73b0\u53cc\u5411\u8bed\u8a00\u6a21\u578b\u56e0\u5176\u66f4\u9ad8\u7684\u4e92\u4fe1\u606f\u4fdd\u7559\u548c\u6709\u6548\u7ef4\u5ea6\u8868\u73b0\u4f18\u4e8e\u5355\u5411\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86FlowNIB\u65b9\u6cd5\u89e3\u51b3IB\u65b9\u6cd5\u7684\u5c40\u9650\u3002", "motivation": "\u63a2\u8ba8\u53cc\u5411\u8bed\u8a00\u6a21\u578b\u4f18\u4e8e\u5355\u5411\u6a21\u578b\u7684\u7406\u8bba\u539f\u56e0\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u539f\u5219\u7684\u89e3\u91ca\u3002", "method": "\u63d0\u51faFlowNIB\u65b9\u6cd5\uff0c\u52a8\u6001\u4f30\u8ba1\u8bad\u7ec3\u4e2d\u7684\u4e92\u4fe1\u606f\uff0c\u89e3\u51b3\u4f20\u7edfIB\u65b9\u6cd5\u7684\u8ba1\u7b97\u96be\u9898\uff1b\u5efa\u7acb\u8861\u91cf\u8868\u73b0\u590d\u6742\u5ea6\u7684\u6846\u67b6\uff0c\u8bc1\u660e\u53cc\u5411\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u3002", "result": "\u53cc\u5411\u6a21\u578b\u4fdd\u7559\u66f4\u591a\u4e92\u4fe1\u606f\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6709\u6548\u7ef4\u5ea6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4fe1\u606f\u7f16\u7801\u548c\u538b\u7f29\u7684\u8fc7\u7a0b\u3002", "conclusion": "\u5de5\u4f5c\u4e3a\u53cc\u5411\u67b6\u6784\u7684\u9ad8\u6548\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\uff0c\u5e76\u63d0\u4f9b\u4e86\u5206\u6790\u6df1\u5ea6\u8bed\u8a00\u6a21\u578b\u4e2d\u4fe1\u606f\u6d41\u7684\u5b9e\u7528\u5de5\u5177\u3002", "keywords": "\u53cc\u5411\u8bed\u8a00\u6a21\u578b, \u4fe1\u606f\u74f6\u9888, \u4e92\u4fe1\u606f, FlowNIB, \u8868\u73b0\u590d\u6742\u5ea6"}}
{"id": "2506.00846", "pdf": "https://arxiv.org/pdf/2506.00846", "abs": "https://arxiv.org/abs/2506.00846", "authors": ["Mana Sakai", "Ryo Karakida", "Masaaki Imaizumi"], "title": "Infinite-Width Limit of a Single Attention Layer: Analysis via Tensor Programs", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "In modern theoretical analyses of neural networks, the infinite-width limit\nis often invoked to justify Gaussian approximations of neuron preactivations\n(e.g., via neural network Gaussian processes or Tensor Programs). However,\nthese Gaussian-based asymptotic theories have so far been unable to capture the\nbehavior of attention layers, except under special regimes such as infinitely\nmany heads or tailored scaling schemes. In this paper, leveraging the Tensor\nPrograms framework, we rigorously identify the infinite-width limit\ndistribution of variables within a single attention layer under realistic\narchitectural dimensionality and standard $1/\\sqrt{n}$-scaling with $n$\ndimensionality. We derive the exact form of this limit law without resorting to\ninfinite-head approximations or tailored scalings, demonstrating that it\ndeparts fundamentally from Gaussianity. This limiting distribution exhibits\nnon-Gaussianity from a hierarchical structure, being Gaussian conditional on\nthe random similarity scores. Numerical experiments validate our theoretical\npredictions, confirming the effectiveness of our theory at finite width and\naccurate description of finite-head attentions. Beyond characterizing a\nstandalone attention layer, our findings lay the groundwork for developing a\nunified theory of deep Transformer architectures in the infinite-width regime.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7Tensor Programs\u6846\u67b6\uff0c\u4e25\u683c\u63a8\u5bfc\u4e86\u5355\u4e2a\u6ce8\u610f\u529b\u5c42\u5728\u65e0\u9650\u5bbd\u5ea6\u6781\u9650\u4e0b\u7684\u5206\u5e03\u60c5\u51b5\uff0c\u5c55\u793a\u4e86\u5176\u975e\u9ad8\u65af\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9ad8\u65af\u8fd1\u4f3c\u7684\u65e0\u9650\u5bbd\u5ea6\u7406\u8bba\u65e0\u6cd5\u6709\u6548\u63cf\u8ff0\u6ce8\u610f\u529b\u5c42\u7684\u884c\u4e3a\uff0c\u5c24\u5176\u662f\u5728\u73b0\u5b9e\u7684\u67b6\u6784\u7ef4\u5ea6\u548c\u6807\u51c6\u7f29\u653e\u4e0b\u3002", "method": "\u5229\u7528Tensor Programs\u6846\u67b6\uff0c\u63a8\u5bfc\u4e86\u5355\u4e2a\u6ce8\u610f\u529b\u5c42\u5728\u65e0\u9650\u5bbd\u5ea6\u6781\u9650\u4e0b\u7684\u5206\u5e03\uff0c\u65e0\u9700\u4f9d\u8d56\u65e0\u9650\u5934\u6216\u7279\u6b8a\u7f29\u653e\u3002", "result": "\u53d1\u73b0\u8be5\u6781\u9650\u5206\u5e03\u5177\u6709\u975e\u9ad8\u65af\u6027\uff0c\u4e14\u4e0e\u968f\u673a\u76f8\u4f3c\u6027\u5206\u6570\u76f8\u5173\uff0c\u6570\u503c\u5b9e\u9a8c\u652f\u6301\u7406\u8bba\u9884\u6d4b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u65e0\u9650\u5bbd\u5ea6\u4e0b\u6df1\u5ea6Transformer\u7684\u7edf\u4e00\u7406\u8bba\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "keywords": "\u6ce8\u610f\u529b\u5c42, \u65e0\u9650\u5bbd\u5ea6, \u975e\u9ad8\u65af\u6027, Tensor Programs, Transformer"}}
{"id": "2506.00863", "pdf": "https://arxiv.org/pdf/2506.00863", "abs": "https://arxiv.org/abs/2506.00863", "authors": ["Nidhi Kowtal", "Raviraj Joshi"], "title": "L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with Synthetic Annotations using CoTR prompting and Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Emotion recognition in low-resource languages like Marathi remains\nchallenging due to limited annotated data. We present L3Cube-MahaEmotions, a\nhigh-quality Marathi emotion recognition dataset with 11 fine-grained emotion\nlabels. The training data is synthetically annotated using large language\nmodels (LLMs), while the validation and test sets are manually labeled to serve\nas a reliable gold-standard benchmark. Building on the MahaSent dataset, we\napply the Chain-of-Translation (CoTR) prompting technique, where Marathi\nsentences are translated into English and emotion labeled via a single prompt.\nGPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training data\nannotation due to superior label quality. We evaluate model performance using\nstandard metrics and explore label aggregation strategies (e.g., Union,\nIntersection). While GPT-4 predictions outperform fine-tuned BERT models,\nBERT-based models trained on synthetic labels fail to surpass GPT-4. This\nhighlights both the importance of high-quality human-labeled data and the\ninherent complexity of emotion recognition. An important finding of this work\nis that generic LLMs like GPT-4 and Llama3-405B generalize better than\nfine-tuned BERT for complex low-resource emotion recognition tasks. The dataset\nand model are shared publicly at https://github.com/l3cube-pune/MarathiNLP", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u9a6c\u62c9\u5730\u8bed\u60c5\u611f\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4\uff09\u5408\u6210\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\uff0c\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\u5219\u624b\u52a8\u6807\u6ce8\uff0c\u53d1\u73b0\u901a\u7528LLM\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5fae\u8c03BERT\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u9a6c\u62c9\u5730\u8bed\uff09\u4e2d\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528Chain-of-Translation\uff08CoTR\uff09\u63d0\u793a\u6280\u672f\uff0c\u5c06\u9a6c\u62c9\u5730\u8bed\u53e5\u5b50\u7ffb\u8bd1\u4e3a\u82f1\u8bed\u540e\u6807\u6ce8\u60c5\u611f\uff0c\u5e76\u8bc4\u4f30GPT-4\u548cLlama3-405B\u7684\u6807\u6ce8\u6548\u679c\uff0c\u9009\u62e9GPT-4\u751f\u6210\u8bad\u7ec3\u6570\u636e\u3002", "result": "GPT-4\u9884\u6d4b\u7ed3\u679c\u4f18\u4e8e\u5fae\u8c03BERT\u6a21\u578b\uff0c\u4f46BERT\u6a21\u578b\u57fa\u4e8e\u5408\u6210\u6807\u7b7e\u8bad\u7ec3\u540e\u672a\u8d85\u8d8aGPT-4\uff0c\u663e\u793a\u9ad8\u8d28\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u901a\u7528LLM\u5728\u4f4e\u8d44\u6e90\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u51f8\u663e\u4e86\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u53ca\u4efb\u52a1\u590d\u6742\u6027\u3002", "keywords": "\u60c5\u611f\u8bc6\u522b,\u9a6c\u62c9\u5730\u8bed,\u4f4e\u8d44\u6e90\u8bed\u8a00,LLM,GPT-4,BERT"}}
{"id": "2506.00066", "pdf": "https://arxiv.org/pdf/2506.00066", "abs": "https://arxiv.org/abs/2506.00066", "authors": ["Arne Tillmann"], "title": "Literature Review Of Multi-Agent Debate For Problem-Solving", "categories": ["cs.MA", "cs.AI", "I.2.7"], "comment": "11 pages, 2 figures", "summary": "Multi-agent large language models (MA-LLMs) are a rapidly growing research\narea that leverages multiple interacting language agents to tackle complex\ntasks, outperforming single-agent large language models. This literature review\nsynthesizes the latest research on agent profiles, communication structures,\nand decision-making processes, drawing insights from both traditional\nmulti-agent systems and state-of-the-art MA-LLM studies. In doing so, it aims\nto address the lack of direct comparisons in the field, illustrating how\nfactors like scalability, communication structure, and decision-making\nprocesses influence MA-LLM performance. By examining frequent practices and\noutlining current challenges, the review reveals that multi-agent approaches\ncan yield superior results but also face elevated computational costs and\nunder-explored challenges unique to MA-LLM. Overall, these findings provide\nresearchers and practitioners with a roadmap for developing robust and\nefficient multi-agent AI solutions.", "AI": {"tldr": "\u7efc\u8ff0\u603b\u7ed3\u4e86\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\uff08MA-LLMs\uff09\u7684\u6700\u65b0\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u5176\u5982\u4f55\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u8d85\u8d8a\u5355\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u72ec\u7279\u6311\u6218\u7b49\u95ee\u9898\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u9886\u57df\u5185\u7f3a\u4e4f\u76f4\u63a5\u6bd4\u8f83\u7684\u95ee\u9898\uff0c\u5206\u6790\u5982\u53ef\u6269\u5c55\u6027\u3001\u901a\u4fe1\u7ed3\u6784\u548c\u51b3\u7b56\u8fc7\u7a0b\u7b49\u56e0\u7d20\u5bf9MA-LLM\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u7814\u7a76\uff0c\u7ed3\u5408\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e0e\u524d\u6cbfMA-LLM\u7814\u7a76\uff0c\u5206\u6790\u667a\u80fd\u4f53\u914d\u7f6e\u3001\u901a\u4fe1\u7ed3\u6784\u548c\u51b3\u7b56\u6d41\u7a0b\u3002", "result": "\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u548cMA-LLM\u7279\u6709\u7684\u672a\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u65b9\u6848\u80fd\u63d0\u4f9b\u66f4\u5f3a\u6027\u80fd\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u89e3\u51b3\u6548\u7387\u95ee\u9898\uff1b\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u53d1\u5c55\u8def\u7ebf\u3002", "keywords": "MA-LLM, \u591a\u667a\u80fd\u4f53, \u901a\u4fe1\u7ed3\u6784, \u51b3\u7b56\u8fc7\u7a0b, \u53ef\u6269\u5c55\u6027"}}
{"id": "2506.00848", "pdf": "https://arxiv.org/pdf/2506.00848", "abs": "https://arxiv.org/abs/2506.00848", "authors": ["Jiali Cheng", "Hadi Amiri"], "title": "Speech Unlearning", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "comment": "Interspeech 2025", "summary": "We introduce machine unlearning for speech tasks, a novel and underexplored\nresearch problem that aims to efficiently and effectively remove the influence\nof specific data from trained speech models without full retraining. This has\nimportant applications in privacy preservation, removal of outdated or noisy\ndata, and bias mitigation. While machine unlearning has been studied in\ncomputer vision and natural language processing, its application to speech is\nlargely unexplored due to the high-dimensional, sequential, and\nspeaker-dependent nature of speech data. We define two fundamental speech\nunlearning tasks: sample unlearning, which removes individual data points\n(e.g., a voice recording), and class unlearning, which removes an entire\ncategory (e.g., all data from a speaker), while preserving performance on the\nremaining data. Experiments on keyword spotting and speaker identification\ndemonstrate that unlearning speech data is significantly more challenging than\nunlearning image or text data. We conclude with key future directions in this\narea, including structured training, robust evaluation, feature-level\nunlearning, broader applications, scalable methods, and adversarial robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8bed\u97f3\u4efb\u52a1\u7684\u673a\u5668\u53bb\u5b66\u4e60\uff08machine unlearning\uff09\u6280\u672f\uff0c\u65e8\u5728\u9ad8\u6548\u53bb\u9664\u7279\u5b9a\u6570\u636e\u5bf9\u8bed\u97f3\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u9690\u79c1\u4fdd\u62a4\u3001\u53bb\u9664\u8fc7\u65f6\u6216\u566a\u58f0\u6570\u636e\u3001\u51cf\u8f7b\u504f\u89c1\u662f\u4e3b\u8981\u5e94\u7528\u52a8\u673a\u3002\u5c3d\u7ba1\u673a\u5668\u53bb\u5b66\u4e60\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u7531\u4e8e\u8bed\u97f3\u6570\u636e\u7684\u591a\u7ef4\u6027\u3001\u65f6\u5e8f\u6027\u548c\u8bf4\u8bdd\u4eba\u4f9d\u8d56\u6027\uff0c\u8bed\u97f3\u9886\u57df\u7684\u53bb\u5b66\u4e60\u95ee\u9898\u5c1a\u672a\u6df1\u5165\u63a2\u7d22\u3002", "method": "\u5b9a\u4e49\u4e86\u4e24\u9879\u57fa\u672c\u4efb\u52a1\uff1a\u6837\u672c\u53bb\u5b66\u4e60\uff08\u79fb\u9664\u5355\u4e2a\u6570\u636e\u70b9\uff0c\u5982\u7279\u5b9a\u5f55\u97f3\uff09\u548c\u7c7b\u522b\u53bb\u5b66\u4e60\uff08\u79fb\u9664\u6574\u4e2a\u7c7b\u522b\uff0c\u5982\u67d0\u4e2a\u8bf4\u8bdd\u4eba\u7684\u6240\u6709\u6570\u636e\uff09\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u4f59\u6570\u636e\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8bed\u97f3\u6570\u636e\u7684\u53bb\u5b66\u4e60\u6bd4\u56fe\u50cf\u6216\u6587\u672c\u66f4\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u5728\u5173\u952e\u8bcd\u68c0\u6d4b\u548c\u8bf4\u8bdd\u4eba\u8bc6\u522b\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u672a\u6765\u65b9\u5411\u5305\u62ec\u7ed3\u6784\u5316\u8bad\u7ec3\u3001\u9c81\u68d2\u8bc4\u4f30\u3001\u7279\u5f81\u7ea7\u53bb\u5b66\u4e60\u3001\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3001\u53ef\u6269\u5c55\u65b9\u6cd5\u53ca\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "keywords": "machine unlearning, speech tasks, privacy preservation, bias mitigation, sample unlearning, class unlearning"}}
{"id": "2506.00869", "pdf": "https://arxiv.org/pdf/2506.00869", "abs": "https://arxiv.org/abs/2506.00869", "authors": ["Zhaotian Weng", "Haoxuan Li", "Kuan-Hao Huang", "Jieyu Zhao"], "title": "What's Missing in Vision-Language Models? Probing Their Struggles with Causal Order Reasoning", "categories": ["cs.CL", "I.7.0"], "comment": "12 pages", "summary": "Despite the impressive performance of vision-language models (VLMs) on\ndownstream tasks, their ability to understand and reason about causal\nrelationships in visual inputs remains unclear. Robust causal reasoning is\nfundamental to solving complex high-level reasoning tasks, yet existing\nbenchmarks often include a mixture of reasoning questions, and VLMs can\nfrequently exploit object recognition and activity identification as shortcuts\nto arrive at the correct answers, making it challenging to truly assess their\ncausal reasoning abilities. To bridge this gap, we introduce VQA-Causal and\nVCR-Causal, two new benchmarks specifically designed to isolate and rigorously\nevaluate VLMs' causal reasoning abilities. Our findings reveal that while VLMs\nexcel in object and activity recognition, they perform poorly on causal\nreasoning tasks, often only marginally surpassing random guessing. Further\nanalysis suggests that this limitation stems from a severe lack of causal\nexpressions in widely used training datasets, where causal relationships are\nrarely explicitly conveyed. We additionally explore fine-tuning strategies with\nhard negative cases, showing that targeted fine-tuning can improve model's\ncausal reasoning while maintaining generalization and downstream performance.\nOur study highlights a key gap in current VLMs and lays the groundwork for\nfuture work on causal understanding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5VQA-Causal\u548cVCR-Causal\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1VLMs\u5728\u5bf9\u8c61\u548c\u6d3b\u52a8\u8bc6\u522b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u4e3b\u8981\u539f\u56e0\u662f\u8bad\u7ec3\u6570\u636e\u4e2d\u7f3a\u4e4f\u56e0\u679c\u5173\u7cfb\u7684\u663e\u5f0f\u8868\u8fbe\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30VLMs\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u95ee\u9898\u5e38\u88ab\u5bf9\u8c61\u8bc6\u522b\u548c\u6d3b\u52a8\u8bc6\u522b\u4f5c\u4e3a\u6377\u5f84\u89e3\u7b54\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u4e24\u4e2a\u65b0\u57fa\u51c6\u6d4b\u8bd5VQA-Causal\u548cVCR-Causal\uff0c\u5e76\u63a2\u7d22\u9488\u5bf9\u8d1f\u4f8b\u7684\u5fae\u8c03\u7b56\u7565\u3002", "result": "VLMs\u5728\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4ec5\u7565\u4f18\u4e8e\u968f\u673a\u731c\u6d4b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dVLMs\u5728\u56e0\u679c\u63a8\u7406\u4e0a\u7684\u5173\u952e\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b,\u56e0\u679c\u63a8\u7406,\u57fa\u51c6\u6d4b\u8bd5,\u8d1f\u4f8b\u5fae\u8c03"}}
{"id": "2506.00849", "pdf": "https://arxiv.org/pdf/2506.00849", "abs": "https://arxiv.org/abs/2506.00849", "authors": ["Qi Chen", "Jierui Zhu", "Florian Shkurti"], "title": "Generalization in VAE and Diffusion Models: A Unified Information-Theoretic Analysis", "categories": ["cs.LG", "cs.AI"], "comment": "ICLR 2025 Accepted", "summary": "Despite the empirical success of Diffusion Models (DMs) and Variational\nAutoencoders (VAEs), their generalization performance remains theoretically\nunderexplored, especially lacking a full consideration of the shared\nencoder-generator structure. Leveraging recent information-theoretic tools, we\npropose a unified theoretical framework that provides guarantees for the\ngeneralization of both the encoder and generator by treating them as randomized\nmappings. This framework further enables (1) a refined analysis for VAEs,\naccounting for the generator's generalization, which was previously overlooked;\n(2) illustrating an explicit trade-off in generalization terms for DMs that\ndepends on the diffusion time $T$; and (3) providing computable bounds for DMs\nbased solely on the training data, allowing the selection of the optimal $T$\nand the integration of such bounds into the optimization process to improve\nmodel performance. Empirical results on both synthetic and real datasets\nillustrate the validity of the proposed theory.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5206\u6790\u4e86\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAEs\uff09\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u586b\u8865\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u7684\u5dee\u8ddd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u6cdb\u5316\u6027\u80fd\u5728\u7406\u8bba\u4e0a\u7f3a\u4e4f\u5168\u9762\u5206\u6790\uff0c\u5c24\u5176\u662f\u5171\u4eab\u7f16\u7801\u5668-\u751f\u6210\u5668\u7ed3\u6784\u7684\u8003\u8651\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u4fe1\u606f\u8bba\u5de5\u5177\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u7f16\u7801\u5668\u548c\u751f\u6210\u5668\u89c6\u4e3a\u968f\u673a\u6620\u5c04\uff0c\u5206\u6790\u5176\u6cdb\u5316\u6027\u80fd\u3002", "result": "1. \u6539\u8fdb\u4e86\u5bf9VAE\u751f\u6210\u5668\u6cdb\u5316\u7684\u5206\u6790\uff1b2. \u63ed\u793a\u4e86DMs\u4e2d\u6cdb\u5316\u4e0e\u6269\u6563\u65f6\u95f4T\u7684\u6743\u8861\uff1b3. \u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u53ef\u8ba1\u7b97\u754c\u9650\u3002", "conclusion": "\u7406\u8bba\u6846\u67b6\u6709\u6548\uff0c\u5b9e\u8bc1\u7ed3\u679c\u652f\u6301\u4e86\u5176\u6709\u6548\u6027\u3002", "keywords": "\u6269\u6563\u6a21\u578b, \u53d8\u5206\u81ea\u7f16\u7801\u5668, \u6cdb\u5316\u6027\u80fd, \u4fe1\u606f\u8bba, \u7406\u8bba\u6846\u67b6"}}
{"id": "2506.00875", "pdf": "https://arxiv.org/pdf/2506.00875", "abs": "https://arxiv.org/abs/2506.00875", "authors": ["Yangfan Ye", "Xiaocheng Feng", "Zekun Yuan", "Xiachong Feng", "Libo Qin", "Lei Huang", "Weitao Ma", "Yichong Huang", "Zhirui Zhang", "Yunfei Lu", "Xiaohui Yan", "Duyu Tang", "Dandan Tu", "Bing Qin"], "title": "CC-Tuning: A Cross-Lingual Connection Mechanism for Improving Joint Multilingual Supervised Fine-Tuning", "categories": ["cs.CL"], "comment": "ACL2025 main conference, long paper", "summary": "Current large language models (LLMs) often exhibit imbalanced multilingual\ncapabilities due to their English-centric training corpora. To address this,\nexisting fine-tuning approaches operating at the data-level (e.g., through data\naugmentation or distillation) typically introduce implicit cross-lingual\nalignment, overlooking the potential for more profound, latent-level\ncross-lingual interactions. In this work, we propose CC-Tuning, a novel\nmultilingual fine-tuning paradigm that explicitly establishes a cross-lingual\nconnection mechanism at the latent level. During training, CC-Tuning fuses the\nfeed forward activations from both English and non-English inputs, enabling the\nmodel to benefit from both linguistic resources. This process is facilitated\nwith a trainable Decision Maker that identifies beneficial activations.\nFurthermore, during inference, a Transform Matrix is utilized to simulate the\ncross-lingual connection under monolingual setting through representation\ntransformation. Our experiments on six benchmarks covering 22 languages show\nthat CC-Tuning outperforms vanilla SFT and offers a strong latent-level\nalternative to data-level augmentation methods. Further analysis also\nhighlights the practicality of CC-Tuning and the potential of latent-level\ncross-lingual interactions in advancing the multilingual performance of LLMs.", "AI": {"tldr": "CC-Tuning\u662f\u4e00\u79cd\u65b0\u7684\u591a\u8bed\u8a00\u5fae\u8c03\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u6f5c\u5728\u5c42\u9762\u663e\u5f0f\u5efa\u7acb\u8de8\u8bed\u8a00\u8fde\u63a5\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u591a\u8bed\u8a00\u80fd\u529b\u4e0d\u5e73\u8861\uff0c\u4e3b\u8981\u57fa\u4e8e\u82f1\u8bed\u8bed\u6599\u5e93\u8bad\u7ec3\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u6570\u636e\u589e\u5f3a\u6216\u84b8\u998f\uff09\u901a\u5e38\u5ffd\u7565\u6f5c\u5728\u5c42\u9762\u7684\u8de8\u8bed\u8a00\u4ea4\u4e92\u3002", "method": "CC-Tuning\u5728\u8bad\u7ec3\u4e2d\u878d\u5408\u82f1\u8bed\u548c\u975e\u82f1\u8bed\u8f93\u5165\u7684\u524d\u9988\u6fc0\u6d3b\uff0c\u5e76\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u51b3\u7b56\u9009\u62e9\u5668\u8bc6\u522b\u6709\u76ca\u6fc0\u6d3b\u3002\u5728\u63a8\u7406\u4e2d\uff0c\u4f7f\u7528\u53d8\u6362\u77e9\u9635\u6a21\u62df\u8de8\u8bed\u8a00\u8fde\u63a5\u3002", "result": "\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u8986\u76d622\u79cd\u8bed\u8a00\uff09\u4e2d\uff0cCC-Tuning\u4f18\u4e8e\u4f20\u7edfSFT\uff0c\u5c55\u793a\u4e86\u6f5c\u5728\u5c42\u9762\u8de8\u8bed\u8a00\u4ea4\u4e92\u7684\u6709\u6548\u6027\u3002", "conclusion": "CC-Tuning\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6f5c\u5728\u5c42\u9762\u66ff\u4ee3\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u6027\u80fd\u3002", "keywords": "\u591a\u8bed\u8a00\u6a21\u578b, \u8de8\u8bed\u8a00\u4ea4\u4e92, \u5fae\u8c03, \u6f5c\u5728\u5c42\u9762, CC-Tuning"}}
{"id": "2506.00862", "pdf": "https://arxiv.org/pdf/2506.00862", "abs": "https://arxiv.org/abs/2506.00862", "authors": ["Haixin Wang", "Jiashu Pan", "Hao Wu", "Fan Zhang", "Tailin Wu"], "title": "FourierFlow: Frequency-aware Flow Matching for Generative Turbulence Modeling", "categories": ["cs.LG"], "comment": "27 pages, 14 figures", "summary": "Modeling complex fluid systems, especially turbulence governed by partial\ndifferential equations (PDEs), remains a fundamental challenge in science and\nengineering. Recently, diffusion-based generative models have gained attention\nas a powerful approach for these tasks, owing to their capacity to capture\nlong-range dependencies and recover hierarchical structures. However, we\npresent both empirical and theoretical evidence showing that generative models\nstruggle with significant spectral bias and common-mode noise when generating\nhigh-fidelity turbulent flows. Here we propose FourierFlow, a novel generative\nturbulence modeling framework that enhances the frequency-aware learning by\nboth implicitly and explicitly mitigating spectral bias and common-mode noise.\nFourierFlow comprises three key innovations. Firstly, we adopt a dual-branch\nbackbone architecture, consisting of a salient flow attention branch with\nlocal-global awareness to focus on sensitive turbulence areas. Secondly, we\nintroduce a frequency-guided Fourier mixing branch, which is integrated via an\nadaptive fusion strategy to explicitly mitigate spectral bias in the generative\nmodel. Thirdly, we leverage the high-frequency modeling capabilities of the\nmasked auto-encoder pre-training and implicitly align the features of the\ngenerative model toward high-frequency components. We validate the\neffectiveness of FourierFlow on three canonical turbulent flow scenarios,\ndemonstrating superior performance compared to state-of-the-art methods.\nFurthermore, we show that our model exhibits strong generalization capabilities\nin challenging settings such as out-of-distribution domains, long-term temporal\nextrapolation, and robustness to noisy inputs. The code can be found at\nhttps://github.com/AI4Science-WestlakeU/FourierFlow.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFourierFlow\uff0c\u4e00\u79cd\u65b0\u6846\u67b6\u7528\u4e8e\u89e3\u51b3\u751f\u6210\u6a21\u578b\u5728\u6a21\u62df\u6e4d\u6d41\u65f6\u7684\u9891\u8c31\u504f\u5dee\u548c\u5171\u6a21\u566a\u58f0\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u3001\u9891\u7387\u5f15\u5bfc\u548c\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u6a21\u62df\u6e4d\u6d41\u65f6\u5b58\u5728\u9891\u8c31\u504f\u5dee\u548c\u5171\u6a21\u566a\u58f0\u95ee\u9898\uff0c\u9650\u5236\u4e86\u9ad8\u4fdd\u771f\u6e4d\u6d41\u751f\u6210\u7684\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "FourierFlow\u5305\u542b\u53cc\u5206\u652f\u67b6\u6784\uff08\u663e\u8457\u6027\u6d41\u548c\u9891\u7387\u5f15\u5bfc\u5206\u652f\uff09\u3001\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\u4ee5\u53ca\u63a9\u7801\u81ea\u7f16\u7801\u5668\u9884\u8bad\u7ec3\uff0c\u4ee5\u663e\u5f0f\u548c\u9690\u5f0f\u65b9\u5f0f\u4f18\u5316\u9891\u8c31\u611f\u77e5\u3002", "result": "\u5728\u4e09\u79cd\u5178\u578b\u6e4d\u6d41\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5206\u5e03\u5916\u57df\u3001\u957f\u671f\u65f6\u95f4\u5916\u63a8\u548c\u566a\u58f0\u8f93\u5165\u4e0b\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FourierFlow\u901a\u8fc7\u9891\u7387\u611f\u77e5\u5b66\u4e60\u663e\u8457\u63d0\u5347\u6e4d\u6d41\u5efa\u6a21\u7684\u751f\u6210\u8d28\u91cf\u4e0e\u9c81\u68d2\u6027\uff0c\u4e3a\u590d\u6742\u6d41\u4f53\u7cfb\u7edf\u6a21\u62df\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "keywords": "\u6e4d\u6d41\u5efa\u6a21\u3001\u751f\u6210\u6a21\u578b\u3001\u9891\u8c31\u504f\u5dee\u3001\u5171\u6a21\u566a\u58f0\u3001FourierFlow"}}
{"id": "2506.00876", "pdf": "https://arxiv.org/pdf/2506.00876", "abs": "https://arxiv.org/abs/2506.00876", "authors": ["Yixin Wan", "Anil Ramakrishna", "Kai-Wei Chang", "Volkan Cevher", "Rahul Gupta"], "title": "Not Every Token Needs Forgetting: Selective Unlearning to Limit Change in Utility in Large Language Model Unlearning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Model (LLM) unlearning has recently gained significant\nattention, driven by the need to remove unwanted information, such as private,\nsensitive, or copyrighted content, from LLMs. However, conventional unlearning\napproaches indiscriminately update model parameters to forget all tokens in a\ntarget document, including common tokens (e.g., pronouns, prepositions, general\nnouns) that carry general knowledge. In this paper, we highlight that not every\ntoken needs forgetting. We propose Selective Unlearning (SU), which identifies\na critical subset of tokens within the forgetting set that is relevant to the\nunwanted information, and unlearns only those tokens. Experiments on two\nbenchmarks and six baseline unlearning algorithms demonstrate that SU not only\nachieves effective unlearning on the targeted forget data, but also\nsignificantly preserves the model's utility in the retaining set.", "AI": {"tldr": "Selective Unlearning (SU) \u65b9\u6cd5\u901a\u8fc7\u9009\u62e9\u6027\u9057\u5fd8\u4e0e\u65e0\u7528\u4fe1\u606f\u76f8\u5173\u7684\u5173\u952e\u5b50\u96c6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u76f2\u76ee\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u9057\u5fd8\u65b9\u6cd5\u4e0d\u52a0\u533a\u522b\u5730\u9057\u5fd8\u76ee\u6807\u6587\u6863\u4e2d\u7684\u6240\u6709\u6807\u8bb0\uff0c\u5305\u62ec\u643a\u5e26\u901a\u7528\u77e5\u8bc6\u7684\u5e38\u89c1\u6807\u8bb0\u3002\u672c\u6587\u65e8\u5728\u4ec5\u9057\u5fd8\u4e0e\u65e0\u7528\u4fe1\u606f\u76f8\u5173\u7684\u5173\u952e\u6807\u8bb0\u3002", "method": "\u63d0\u51fa\u4e86 Selective Unlearning (SU) \u65b9\u6cd5\uff0c\u8bc6\u522b\u9057\u5fd8\u96c6\u4e2d\u4e0e\u65e0\u7528\u4fe1\u606f\u76f8\u5173\u7684\u5173\u952e\u5b50\u96c6\uff0c\u5e76\u4ec5\u5bf9\u8fd9\u4e9b\u6807\u8bb0\u8fdb\u884c\u9057\u5fd8\u64cd\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSU \u4e0d\u4ec5\u6709\u6548\u5b9e\u73b0\u4e86\u76ee\u6807\u6570\u636e\u7684\u9057\u5fd8\uff0c\u8fd8\u663e\u8457\u4fdd\u7559\u4e86\u6a21\u578b\u5728\u4fdd\u7559\u96c6\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "SU \u65b9\u6cd5\u5728\u5904\u7406\u65e0\u7528\u4fe1\u606f\u7684\u9057\u5fd8\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u907f\u514d\u4e86\u901a\u7528\u77e5\u8bc6\u7684\u635f\u5931\u3002", "keywords": "Large Language Model (LLM) unlearning, selective unlearning, utility preservation"}}
{"id": "2506.00070", "pdf": "https://arxiv.org/pdf/2506.00070", "abs": "https://arxiv.org/abs/2506.00070", "authors": ["Dongyoung Kim", "Sumin Park", "Huiwon Jang", "Jinwoo Shin", "Jaehyung Kim", "Younggyo Seo"], "title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics", "categories": ["cs.RO", "cs.AI"], "comment": "26 pages, 14 figures", "summary": "Large Vision-Language Models (LVLMs) have recently shown great promise in\nadvancing robotics by combining embodied reasoning with robot control. A common\napproach involves training on embodied reasoning tasks related to robot control\nusing Supervised Fine-Tuning (SFT). However, SFT datasets are often\nheuristically constructed and not explicitly optimized for improving robot\ncontrol. Furthermore, SFT often leads to issues such as catastrophic forgetting\nand reduced generalization performance. To address these limitations, we\nintroduce Robot-R1, a novel framework that leverages reinforcement learning to\nenhance embodied reasoning specifically for robot control. Robot-R1 learns to\npredict the next keypoint state required for task completion, conditioned on\nthe current scene image and environment metadata derived from expert\ndemonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples\nreasoning-based responses and reinforces those that lead to more accurate\npredictions. Our experiments show that models trained with Robot-R1 outperform\nSFT methods on embodied reasoning tasks. Despite having only 7B parameters,\nRobot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action\ncontrol, such as spatial and primitive movement reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u6846\u67b6Robot-R1\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u5177\u8eab\u63a8\u7406\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5728\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u5b58\u5728\u7684\u542f\u53d1\u5f0f\u6570\u636e\u96c6\u6784\u5efa\u95ee\u9898\u3001\u707e\u96be\u6027\u9057\u5fd8\u548c\u6cdb\u5316\u6027\u80fd\u4e0b\u964d\u7b49\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u9884\u6d4b\u4efb\u52a1\u5b8c\u6210\u6240\u9700\u7684\u4e0b\u4e00\u4e2a\u5173\u952e\u70b9\u72b6\u6001\uff0c\u5e76\u6839\u636e\u5f53\u524d\u573a\u666f\u56fe\u50cf\u548c\u73af\u5883\u5143\u6570\u636e\u91c7\u6837\u548c\u5f3a\u5316\u63a8\u7406\u54cd\u5e94\u3002", "result": "\u5728\u5177\u8eab\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eSFT\u65b9\u6cd5\uff0c\u751a\u81f3\u5728\u4f4e\u7ea7\u522b\u52a8\u4f5c\u63a7\u5236\u4efb\u52a1\u4e2d\u8d85\u8d8aGPT-4o\u3002", "conclusion": "Robot-R1\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u5177\u8eab\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u73b0\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u6f5c\u529b\u3002", "keywords": "Large Vision-Language Models, Robot Control, Reinforcement Learning, Embodied Reasoning, Robot-R1"}}
{"id": "2506.00867", "pdf": "https://arxiv.org/pdf/2506.00867", "abs": "https://arxiv.org/abs/2506.00867", "authors": ["Kyowoon Lee", "Jaesik Choi"], "title": "Local Manifold Approximation and Projection for Manifold-Aware Diffusion Planning", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025", "summary": "Recent advances in diffusion-based generative modeling have demonstrated\nsignificant promise in tackling long-horizon, sparse-reward tasks by leveraging\noffline datasets. While these approaches have achieved promising results, their\nreliability remains inconsistent due to the inherent stochastic risk of\nproducing infeasible trajectories, limiting their applicability in\nsafety-critical applications. We identify that the primary cause of these\nfailures is inaccurate guidance during the sampling procedure, and demonstrate\nthe existence of manifold deviation by deriving a lower bound on the guidance\ngap. To address this challenge, we propose Local Manifold Approximation and\nProjection (LoMAP), a training-free method that projects the guided sample onto\na low-rank subspace approximated from offline datasets, preventing infeasible\ntrajectory generation. We validate our approach on standard offline\nreinforcement learning benchmarks that involve challenging long-horizon\nplanning. Furthermore, we show that, as a standalone module, LoMAP can be\nincorporated into the hierarchical diffusion planner, providing further\nperformance enhancements.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLoMAP\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6307\u5bfc\u6837\u672c\u6295\u5f71\u5230\u79bb\u7ebf\u6570\u636e\u96c6\u8fd1\u4f3c\u7684\u4f4e\u79e9\u5b50\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u751f\u6210\u6a21\u578b\u4e2d\u56e0\u91c7\u6837\u8fc7\u7a0b\u4e2d\u6307\u5bfc\u4e0d\u51c6\u786e\u800c\u5bfc\u81f4\u4e0d\u53ef\u884c\u8f68\u8ff9\u751f\u6210\u7684\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u751f\u6210\u6a21\u578b\u5728\u957f\u65f6\u57df\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u53ef\u9760\u6027\u56e0\u91c7\u6837\u8fc7\u7a0b\u4e2d\u4e0d\u53ef\u884c\u8f68\u8ff9\u7684\u968f\u673a\u98ce\u9669\u800c\u53d7\u9650\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u573a\u666f\u65f6\u5b58\u5728\u6311\u6218\u3002", "method": "\u8bba\u6587\u63d0\u51faLoMAP\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5c06\u5f15\u5bfc\u6837\u672c\u6295\u5f71\u5230\u4f4e\u79e9\u5b50\u7a7a\u95f4\uff0c\u907f\u514d\u751f\u6210\u4e0d\u53ef\u884c\u8f68\u8ff9\uff0c\u5e76\u5206\u6790\u4e86\u6d41\u5f62\u504f\u5dee\u7684\u5b58\u5728\u3002", "result": "\u5728\u6807\u51c6\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLoMAP\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4f5c\u4e3a\u72ec\u7acb\u6a21\u5757\u53ef\u878d\u5165\u5206\u5c42\u6269\u6563\u89c4\u5212\u5668\u4e2d\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "LoMAP\u901a\u8fc7\u6d41\u5f62\u8fd1\u4f3c\u548c\u6295\u5f71\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u751f\u6210\u6a21\u578b\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u4efb\u52a1\u3002", "keywords": "\u6269\u6563\u6a21\u578b, \u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60, \u6d41\u5f62\u8fd1\u4f3c, \u8f68\u8ff9\u751f\u6210, \u5b89\u5168\u6027"}}
{"id": "2506.00883", "pdf": "https://arxiv.org/pdf/2506.00883", "abs": "https://arxiv.org/abs/2506.00883", "authors": ["Farong Wen", "Yijin Guo", "Junying Wang", "Jiaohao Xiao", "Yingjie Zhou", "Chunyi Li", "Zicheng Zhang", "Guangtao Zhai"], "title": "Improve MLLM Benchmark Efficiency through Interview", "categories": ["cs.CL"], "comment": null, "summary": "The rapid development of Multimodal Large Language Models (MLLM) has led to a\nwide range of MLLM applications, and a number of benchmark datasets have sprung\nup in order to assess MLLM abilities. However, full-coverage Q&A testing on\nlarge-scale data is resource-intensive and time-consuming. To address this\nissue, we propose the MLLM Interview (MITV) strategy, which aims to quickly\nobtain MLLM performance metrics by quizzing fewer question. First, First, we\nconstructed the interview dataset, which was built on an existing MLLM\nassessment dataset, by adding difficulty labels based on the performance of\nsome typical MLLMs in this dataset. Second, we propose an MLLM Interview\nstrategy, which obtains an initial performance situation of the large model by\nquizzing a small number of topics and then continuously tries to test the\nmodel's limits. Through extensive experiments, the result shows that the MITV\nstrategy proposed in this paper performs well on MLLM benchmark datasets, and\nit is able to obtain the model evaluation capability faster through a small\nnumber of questions and answers.", "AI": {"tldr": "\u63d0\u51faMLLM Interview (MITV)\u7b56\u7565\uff0c\u901a\u8fc7\u5c11\u91cf\u95ee\u9898\u5feb\u901f\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MLLM\u8bc4\u4f30\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u8017\u65f6\u8017\u8d44\u6e90\uff0cMITV\u7b56\u7565\u65e8\u5728\u9ad8\u6548\u83b7\u53d6\u6027\u80fd\u6307\u6807\u3002", "method": "\u6784\u5efa\u5e26\u96be\u5ea6\u6807\u7b7e\u7684\u9762\u8bd5\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5c11\u91cf\u95ee\u9898\u6d4b\u8bd5\u6a21\u578b\u6027\u80fd\u5e76\u9010\u6b65\u63a2\u7d22\u5176\u6781\u9650\u3002", "result": "MITV\u7b56\u7565\u5728MLLM\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u80fd\u901a\u8fc7\u5c11\u91cf\u95ee\u7b54\u5feb\u901f\u5b8c\u6210\u6a21\u578b\u8bc4\u4f30\u3002", "conclusion": "MITV\u7b56\u7565\u4e3aMLLM\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u8d44\u6e90\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, \u6027\u80fd\u8bc4\u4f30, \u9762\u8bd5\u7b56\u7565, \u96be\u5ea6\u6807\u7b7e"}}
{"id": "2506.00071", "pdf": "https://arxiv.org/pdf/2506.00071", "abs": "https://arxiv.org/abs/2506.00071", "authors": ["Chenhui Zuo", "Guohao Lin", "Chen Zhang", "Shanning Zhuang", "Yanan Sui"], "title": "Human sensory-musculoskeletal modeling and control of whole-body movements", "categories": ["q-bio.NC", "cs.AI", "cs.RO"], "comment": null, "summary": "Coordinated human movement depends on the integration of multisensory inputs,\nsensorimotor transformation, and motor execution, as well as sensory feedback\nresulting from body-environment interaction. Building dynamic models of the\nsensory-musculoskeletal system is essential for understanding movement control\nand investigating human behaviours. Here, we report a human\nsensory-musculoskeletal model, termed SMS-Human, that integrates precise\nanatomical representations of bones, joints, and muscle-tendon units with\nmultimodal sensory inputs involving visual, vestibular, proprioceptive, and\ntactile components. A stage-wise hierarchical deep reinforcement learning\nframework was developed to address the inherent challenges of high-dimensional\ncontrol in musculoskeletal systems with integrated multisensory information.\nUsing this framework, we demonstrated the simulation of three representative\nmovement tasks, including bipedal locomotion, vision-guided object\nmanipulation, and human-machine interaction during bicycling. Our results\nshowed a close resemblance between natural and simulated human motor\nbehaviours. The simulation also revealed musculoskeletal dynamics that could\nnot be directly measured. This work sheds deeper insights into the sensorimotor\ndynamics of human movements, facilitates quantitative understanding of human\nbehaviours in interactive contexts, and informs the design of systems with\nembodied intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSMS-Human\u7684\u4eba\u7c7b\u611f\u5b98-\u808c\u8089\u9aa8\u9abc\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u591a\u611f\u5b98\u8f93\u5165\u548c\u7cbe\u786e\u7684\u89e3\u5256\u7ed3\u6784\uff0c\u7ed3\u5408\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6a21\u62df\u4e86\u591a\u79cd\u4eba\u7c7b\u8fd0\u52a8\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5efa\u7acb\u52a8\u6001\u611f\u5b98-\u808c\u8089\u9aa8\u9abc\u6a21\u578b\uff0c\u6df1\u5165\u7406\u89e3\u8fd0\u52a8\u63a7\u5236\u548c\u4eba\u7c7b\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u591a\u611f\u5b98\u8f93\u5165\u4e0b\u7684\u590d\u6742\u52a8\u4f5c\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4eba\u7c7b\u611f\u5b98-\u808c\u8089\u9aa8\u9abc\u6a21\u578b\uff0c\u6574\u5408\u4e86\u9aa8\u9abc\u3001\u5173\u8282\u3001\u808c\u8089-\u808c\u8171\u5355\u5143\u4ee5\u53ca\u89c6\u89c9\u3001\u524d\u5ead\u89c9\u3001\u672c\u4f53\u611f\u89c9\u548c\u89e6\u89c9\u7b49\u591a\u611f\u5b98\u8f93\u5165\uff0c\u91c7\u7528\u5206\u5c42\u6b21\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6765\u89e3\u51b3\u9ad8\u7ef4\u63a7\u5236\u95ee\u9898\u3002", "result": "\u6a21\u62df\u4e86\u53cc\u8db3\u884c\u8d70\u3001\u89c6\u89c9\u5f15\u5bfc\u7269\u4f53\u64cd\u4f5c\u548c\u9a91\u8f66\u4e2d\u7684\u4eba\u673a\u4ea4\u4e92\u4efb\u52a1\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u62df\u884c\u4e3a\u4e0e\u81ea\u7136\u4eba\u7c7b\u884c\u4e3a\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u5e76\u63ed\u793a\u4e86\u65e0\u6cd5\u76f4\u63a5\u6d4b\u91cf\u7684\u808c\u8089\u9aa8\u9abc\u52a8\u529b\u5b66\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4eba\u7c7b\u8fd0\u52a8\u7684\u611f\u5b98-\u8fd0\u52a8\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u6df1\u5165\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u91cf\u5316\u7406\u89e3\u4ea4\u4e92\u80cc\u666f\u4e0b\u7684\u4eba\u7c7b\u884c\u4e3a\uff0c\u5e76\u4e3a\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u53c2\u8003\u3002", "keywords": "\u611f\u5b98-\u808c\u8089\u9aa8\u9abc\u6a21\u578b,\u591a\u611f\u5b98\u8f93\u5165,\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60,\u8fd0\u52a8\u63a7\u5236,\u4eba\u7c7b\u884c\u4e3a\u6a21\u62df"}}
{"id": "2506.00880", "pdf": "https://arxiv.org/pdf/2506.00880", "abs": "https://arxiv.org/abs/2506.00880", "authors": ["Zhuo Chen", "Yizhen Zheng", "Huan Yee Koh", "Hongxin Xiang", "Linjiang Chen", "Wenjie Du", "Yang Wang"], "title": "ModuLM: Enabling Modular and Multimodal Molecular Relational Learning with Large Language Models", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM"], "comment": null, "summary": "Molecular Relational Learning (MRL) aims to understand interactions between\nmolecular pairs, playing a critical role in advancing biochemical research.\nWith the recent development of large language models (LLMs), a growing number\nof studies have explored the integration of MRL with LLMs and achieved\npromising results. However, the increasing availability of diverse LLMs and\nmolecular structure encoders has significantly expanded the model space,\npresenting major challenges for benchmarking. Currently, there is no LLM\nframework that supports both flexible molecular input formats and dynamic\narchitectural switching. To address these challenges, reduce redundant coding,\nand ensure fair model comparison, we propose ModuLM, a framework designed to\nsupport flexible LLM-based model construction and diverse molecular\nrepresentations. ModuLM provides a rich suite of modular components, including\n8 types of 2D molecular graph encoders, 11 types of 3D molecular conformation\nencoders, 7 types of interaction layers, and 7 mainstream LLM backbones. Owing\nto its highly flexible model assembly mechanism, ModuLM enables the dynamic\nconstruction of over 50,000 distinct model configurations. In addition, we\nprovide comprehensive results to demonstrate the effectiveness of ModuLM in\nsupporting LLM-based MRL tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ModuLM\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5206\u5b50\u5173\u7cfb\u5b66\u4e60\uff08MRL\uff09\u4e2d\u6a21\u578b\u591a\u6837\u6027\u548c\u7075\u6d3b\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u652f\u6301\u52a8\u6001\u67b6\u6784\u5207\u6362\u548c\u591a\u79cd\u5206\u5b50\u8868\u793a\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5982\u4f55\u6574\u5408MRL\u4e0eLLMs\u5e76\u5b9e\u73b0\u516c\u5e73\u7684\u6a21\u578b\u6bd4\u8f83\u6210\u4e3a\u4e00\u4e2a\u6311\u6218\u3002", "method": "ModuLM\u6846\u67b6\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u6a21\u5757\u5316\u7ec4\u4ef6\uff0c\u5305\u62ec2D/3D\u5206\u5b50\u7f16\u7801\u5668\u3001\u4ea4\u4e92\u5c42\u548c\u4e3b\u6d41LLM\u9aa8\u5e72\u7f51\u7edc\uff0c\u652f\u6301\u8d85\u8fc750,000\u79cd\u6a21\u578b\u914d\u7f6e\u3002", "result": "ModuLM\u5728\u57fa\u4e8eLLM\u7684MRL\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "ModuLM\u4e3aMRL\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u6a21\u578b\u591a\u6837\u6027\u548c\u5197\u4f59\u7f16\u7801\u95ee\u9898\u3002", "keywords": "\u5206\u5b50\u5173\u7cfb\u5b66\u4e60, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u6a21\u5757\u5316\u6846\u67b6, \u5206\u5b50\u7f16\u7801\u5668"}}
{"id": "2506.00893", "pdf": "https://arxiv.org/pdf/2506.00893", "abs": "https://arxiv.org/abs/2506.00893", "authors": ["Junying Wang", "Wenzhe Li", "Yalun Wu", "Yingji Liang", "Yijin Guo", "Chunyi Li", "Haodong Duan", "Zicheng Zhang", "Guangtao Zhai"], "title": "Affordance Benchmark for MLLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Affordance theory posits that environments inherently offer action\npossibilities that shape perception and behavior. While Multimodal Large\nLanguage Models (MLLMs) excel in vision-language tasks, their ability to\nperceive affordance, which is crucial for intuitive and safe interactions,\nremains underexplored. To address this, we introduce A4Bench, a novel benchmark\ndesigned to evaluate the affordance perception abilities of MLLMs across two\ndimensions: 1) Constitutive Affordance}, assessing understanding of inherent\nobject properties through 1,282 question-answer pairs spanning nine\nsub-disciplines, and 2) Transformative Affordance, probing dynamic and\ncontextual nuances (e.g., misleading, time-dependent, cultural, or\nindividual-specific affordance) with 718 challenging question-answer pairs.\nEvaluating 17 MLLMs (nine proprietary and eight open-source) against human\nperformance, we find that proprietary models generally outperform open-source\ncounterparts, but all exhibit limited capabilities, particularly in\ntransformative affordance perception. Furthermore, even top-performing models,\nsuch as Gemini-2.0-Pro (18.05% overall exact match accuracy), significantly lag\nbehind human performance (best: 85.34%, worst: 81.25%). These findings\nhighlight critical gaps in environmental understanding of MLLMs and provide a\nfoundation for advancing AI systems toward more robust, context-aware\ninteractions. The dataset is available in\nhttps://github.com/JunyingWang959/A4Bench/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86A4Bench\u57fa\u51c6\uff0c\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u611f\u77e5\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u73af\u5883\u7406\u89e3\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u5c24\u5176\u5728\u52a8\u6001\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u201c\u8f6c\u6362\u6027\u53ef\u4f9b\u6027\u201d\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22MLLMs\u5728\u611f\u77e5\u73af\u5883\u53ef\u4f9b\u6027\u65b9\u9762\u7684\u80fd\u529b\uff0c\u8fd9\u662f\u5b9e\u73b0\u76f4\u89c9\u548c\u5b89\u5168\u4e92\u52a8\u7684\u5173\u952e\uff0c\u4f46\u76ee\u524d\u4ecd\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6784\u5efaA4Bench\u57fa\u51c6\uff0c\u5206\u4e3a\u201c\u6784\u6210\u6027\u53ef\u4f9b\u6027\u201d\uff08\u8bc4\u4f30\u5bf9\u8c61\u56fa\u6709\u5c5e\u6027\uff09\u548c\u201c\u8f6c\u6362\u6027\u53ef\u4f9b\u6027\u201d\uff08\u8bc4\u4f30\u52a8\u6001\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\uff09\u4e24\u5927\u7ef4\u5ea6\uff0c\u5305\u542b2000\u4e2a\u95ee\u7b54\u5bf9\uff0c\u5e76\u5bf917\u79cdMLLM\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e13\u6709\u6a21\u578b\u666e\u904d\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u4f46\u6574\u4f53\u8868\u73b0\u4ecd\u6709\u9650\uff0c\u5c24\u5176\u662f\u5728\u8f6c\u6362\u6027\u53ef\u4f9b\u6027\u4efb\u52a1\u4e2d\uff1b\u6240\u6709\u6a21\u578b\u7684\u6027\u80fd\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u8868\u73b0\uff08\u5dee\u8ddd\u663e\u8457\uff09\u3002", "conclusion": "MLLMs\u5728\u73af\u5883\u7406\u89e3\u65b9\u9762\u5b58\u5728\u91cd\u5927\u4e0d\u8db3\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u5b9e\u73b0\u66f4\u9c81\u68d2\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4ea4\u4e92\u3002\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002", "keywords": "\u53ef\u4f9b\u6027\u7406\u8bba,\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b,A4Bench,\u73af\u5883\u7406\u89e3,\u4e0a\u4e0b\u6587\u611f\u77e5"}}
{"id": "2506.00072", "pdf": "https://arxiv.org/pdf/2506.00072", "abs": "https://arxiv.org/abs/2506.00072", "authors": ["Nariman Naderi", "Zahra Atf", "Peter R Lewis", "Aref Mahjoub far", "Seyed Amir Ahmad Safavi-Naini", "Ali Soroush"], "title": "Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "This paper was accepted for presentation at the 7th International\n  Workshop on EXplainable, Trustworthy, and Responsible AI and Multi-Agent\n  Systems (EXTRAAMAS 2025). Workshop website:\n  https://extraamas.ehealth.hevs.ch/index.html", "summary": "This paper investigates how prompt engineering techniques impact both\naccuracy and confidence elicitation in Large Language Models (LLMs) applied to\nmedical contexts. Using a stratified dataset of Persian board exam questions\nacross multiple specialties, we evaluated five LLMs - GPT-4o, o3-mini,\nLlama-3.3-70b, Llama-3.1-8b, and DeepSeek-v3 - across 156 configurations. These\nconfigurations varied in temperature settings (0.3, 0.7, 1.0), prompt styles\n(Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales\n(1-10, 1-100). We used AUC-ROC, Brier Score, and Expected Calibration Error\n(ECE) to evaluate alignment between confidence and actual performance.\nChain-of-Thought prompts improved accuracy but also led to overconfidence,\nhighlighting the need for calibration. Emotional prompting further inflated\nconfidence, risking poor decisions. Smaller models like Llama-3.1-8b\nunderperformed across all metrics, while proprietary models showed higher\naccuracy but still lacked calibrated confidence. These results suggest prompt\nengineering must address both accuracy and uncertainty to be effective in\nhigh-stakes medical tasks.", "AI": {"tldr": "\u7814\u7a76\u4e86\u63d0\u793a\u5de5\u7a0b\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u4efb\u52a1\u4e2d\u51c6\u786e\u6027\u548c\u7f6e\u4fe1\u5ea6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e0d\u540c\u63d0\u793a\u98ce\u683c\u548c\u6a21\u578b\u914d\u7f6e\u5bf9\u7ed3\u679c\u6709\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u5982\u4f55\u5f71\u54cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u8bed\u5883\u4e0b\u7684\u51c6\u786e\u6027\u548c\u7f6e\u4fe1\u5ea6\u8868\u73b0\uff0c\u4e3a\u9ad8\u98ce\u9669\u4efb\u52a1\u63d0\u4f9b\u4f18\u5316\u65b9\u5411\u3002", "method": "\u4f7f\u7528\u6ce2\u65af\u533b\u5b66\u8003\u8bd5\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u4e94\u79cd\u6a21\u578b\u7684156\u79cd\u914d\u7f6e\uff08\u6e29\u5ea6\u3001\u63d0\u793a\u98ce\u683c\u3001\u7f6e\u4fe1\u5ea6\u91cf\u8868\uff09\uff0c\u901a\u8fc7AUC-ROC\u3001Brier Score\u548cECE\u8bc4\u4f30\u3002", "result": "\u601d\u7ef4\u94fe\u63d0\u793a\u63d0\u9ad8\u51c6\u786e\u6027\u4f46\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\uff1b\u60c5\u611f\u63d0\u793a\u8fdb\u4e00\u6b65\u52a0\u5267\u7f6e\u4fe1\u5ea6\u504f\u5dee\uff1b\u5c0f\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e13\u6709\u6a21\u578b\u51c6\u786e\u6027\u9ad8\u4f46\u7f6e\u4fe1\u5ea6\u672a\u6821\u51c6\u3002", "conclusion": "\u63d0\u793a\u5de5\u7a0b\u9700\u540c\u65f6\u4f18\u5316\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u4ee5\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u7684\u533b\u5b66\u4efb\u52a1\u3002", "keywords": "\u63d0\u793a\u5de5\u7a0b\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u533b\u5b66\uff0c\u7f6e\u4fe1\u5ea6\uff0c\u6821\u51c6"}}
{"id": "2506.00895", "pdf": "https://arxiv.org/pdf/2506.00895", "abs": "https://arxiv.org/abs/2506.00895", "authors": ["Kyowoon Lee", "Jaesik Choi"], "title": "State-Covering Trajectory Stitching for Diffusion Planners", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diffusion-based generative models are emerging as powerful tools for\nlong-horizon planning in reinforcement learning (RL), particularly with offline\ndatasets. However, their performance is fundamentally limited by the quality\nand diversity of training data. This often restricts their generalization to\ntasks outside their training distribution or longer planning horizons. To\novercome this challenge, we propose State-Covering Trajectory Stitching\n(SCoTS), a novel reward-free trajectory augmentation method that incrementally\nstitches together short trajectory segments, systematically generating diverse\nand extended trajectories. SCoTS first learns a temporal distance-preserving\nlatent representation that captures the underlying temporal structure of the\nenvironment, then iteratively stitches trajectory segments guided by\ndirectional exploration and novelty to effectively cover and expand this latent\nspace. We demonstrate that SCoTS significantly improves the performance and\ngeneralization capabilities of diffusion planners on offline goal-conditioned\nbenchmarks requiring stitching and long-horizon reasoning. Furthermore,\naugmented trajectories generated by SCoTS significantly improve the performance\nof widely used offline goal-conditioned RL algorithms across diverse\nenvironments.", "AI": {"tldr": "SCoTS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5956\u52b1\u514d\u8d39\u8f68\u8ff9\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f1d\u5408\u77ed\u8f68\u8ff9\u6bb5\u751f\u6210\u591a\u6837\u5316\u4e14\u6269\u5c55\u7684\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u89c4\u5212\u5668\u5728\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u7684\u957f\u671f\u89c4\u5212\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u8bad\u7ec3\u5206\u5e03\u4e4b\u5916\u7684\u4efb\u52a1\u6216\u66f4\u957f\u7684\u89c4\u5212\u65f6\u57df\u3002SCoTS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "SCoTS\u901a\u8fc7\u5b66\u4e60\u4fdd\u6301\u65f6\u95f4\u8ddd\u79bb\u7684\u6f5c\u5728\u8868\u793a\uff0c\u6355\u6349\u73af\u5883\u7684\u65f6\u95f4\u7ed3\u6784\uff0c\u7136\u540e\u901a\u8fc7\u65b9\u5411\u6027\u63a2\u7d22\u548c\u65b0\u9896\u6027\u5f15\u5bfc\u8fed\u4ee3\u7f1d\u5408\u8f68\u8ff9\u6bb5\uff0c\u6269\u5c55\u6f5c\u5728\u7a7a\u95f4\u8986\u76d6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSCoTS\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u89c4\u5212\u5668\u5728\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6\u57fa\u51c6\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u8f68\u8ff9\u7f1d\u5408\u548c\u957f\u671f\u63a8\u7406\u7684\u573a\u666f\u4e2d\uff0c\u540c\u65f6\u4e5f\u6539\u8fdb\u4e86\u5e38\u7528\u79bb\u7ebfRL\u7b97\u6cd5\u7684\u8868\u73b0\u3002", "conclusion": "SCoTS\u901a\u8fc7\u7cfb\u7edf\u751f\u6210\u591a\u6837\u5316\u548c\u6269\u5c55\u7684\u8f68\u8ff9\uff0c\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u957f\u65f6\u57df\u89c4\u5212\u4efb\u52a1\u3002", "keywords": "\u6269\u6563\u6a21\u578b,\u5f3a\u5316\u5b66\u4e60,\u8f68\u8ff9\u7f1d\u5408,\u957f\u65f6\u57df\u89c4\u5212,\u79bb\u7ebf\u6570\u636e\u96c6"}}
{"id": "2506.00900", "pdf": "https://arxiv.org/pdf/2506.00900", "abs": "https://arxiv.org/abs/2506.00900", "authors": ["Jinfeng Zhou", "Yuxuan Chen", "Yihan Shi", "Xuanming Zhang", "Leqi Lei", "Yi Feng", "Zexuan Xiong", "Miao Yan", "Xunzhi Wang", "Yaru Cao", "Jianing Yin", "Shuai Wang", "Quanyu Dai", "Zhenhua Dong", "Hongning Wang", "Minlie Huang"], "title": "SocialEval: Evaluating Social Intelligence of Large Language Models", "categories": ["cs.CL"], "comment": "ACL 2025, Repository: \\url{https://github.com/thu-coai/SocialEval}", "summary": "LLMs exhibit promising Social Intelligence (SI) in modeling human behavior,\nraising the need to evaluate LLMs' SI and their discrepancy with humans. SI\nequips humans with interpersonal abilities to behave wisely in navigating\nsocial interactions to achieve social goals. This presents an operational\nevaluation paradigm: outcome-oriented goal achievement evaluation and\nprocess-oriented interpersonal ability evaluation, which existing work fails to\naddress. To this end, we propose SocialEval, a script-based bilingual SI\nbenchmark, integrating outcome- and process-oriented evaluation by manually\ncrafting narrative scripts. Each script is structured as a world tree that\ncontains plot lines driven by interpersonal ability, providing a comprehensive\nview of how LLMs navigate social interactions. Experiments show that LLMs fall\nbehind humans on both SI evaluations, exhibit prosociality, and prefer more\npositive social behaviors, even if they lead to goal failure. Analysis of LLMs'\nformed representation space and neuronal activations reveals that LLMs have\ndeveloped ability-specific functional partitions akin to the human brain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSocialEval\uff0c\u4e00\u4e2a\u57fa\u4e8e\u811a\u672c\u7684\u53cc\u8bedSI\u57fa\u51c6\uff0c\u901a\u8fc7\u6574\u5408\u7ed3\u679c\u5bfc\u5411\u548c\u8fc7\u7a0b\u5bfc\u5411\u8bc4\u4f30\uff0c\u63ed\u793aLLMs\u5728\u793e\u4ea4\u667a\u80fd\u4e0a\u4e0e\u4eba\u7c7b\u7684\u5dee\u8ddd\u3002", "motivation": "\u8bc4\u4f30LLMs\u7684\u793e\u4ea4\u667a\u80fd\uff08SI\uff09\u53ca\u5176\u4e0e\u4eba\u7c7b\u7684\u5dee\u5f02\uff0c\u56e0LLMs\u5728\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u624b\u52a8\u7f16\u5199\u7684\u53d9\u8ff0\u811a\u672c\u6784\u5efaSocialEval\uff0c\u4ee5\u4e16\u754c\u6811\u5f62\u5f0f\u9a71\u52a8\u60c5\u8282\u7ebf\uff0c\u7ed3\u5408\u7ed3\u679c\u548c\u8fc7\u7a0b\u5bfc\u5411\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLMs\u5728SI\u8bc4\u4f30\u4e2d\u843d\u540e\u4e8e\u4eba\u7c7b\uff0c\u8868\u73b0\u51fa\u4eb2\u793e\u4f1a\u6027\uff0c\u4e14\u503e\u5411\u79ef\u6781\u793e\u4ea4\u884c\u4e3a\uff0c\u5373\u4f7f\u963b\u788d\u76ee\u6807\u8fbe\u6210\u3002", "conclusion": "LLMs\u5df2\u5f62\u6210\u7c7b\u4f3c\u4eba\u8111\u7684\u80fd\u529b\u7279\u5b9a\u529f\u80fd\u5206\u533a\uff0c\u4f46\u5728SI\u4e0a\u4ecd\u9700\u6539\u8fdb\u3002", "keywords": "\u793e\u4ea4\u667a\u80fd\uff0c\u8bc4\u4f30\u57fa\u51c6\uff0cLLMs\uff0c\u4eba\u7c7b\u884c\u4e3a\u6a21\u62df\uff0cSocialEval"}}
{"id": "2506.00074", "pdf": "https://arxiv.org/pdf/2506.00074", "abs": "https://arxiv.org/abs/2506.00074", "authors": ["Daniele Barolo", "Chiara Valentin", "Fariba Karimi", "Luis Gal\u00e1rraga", "Gonzalo G. M\u00e9ndez", "Lisette Esp\u00edn-Noboa"], "title": "Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations", "categories": ["cs.CY", "cs.AI", "cs.DL", "cs.IR", "cs.SI", "physics.soc-ph", "68T50", "I.2.7; C.4; F.2; K.4.1"], "comment": "39 pages: 10 main (incl. 9 figures), 3 references, and 26 appendix.\n  Paper under-review", "summary": "This paper evaluates the performance of six open-weight LLMs (llama3-8b,\nllama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending\nexperts in physics across five tasks: top-k experts by field, influential\nscientists by discipline, epoch, seniority, and scholar counterparts. The\nevaluation examines consistency, factuality, and biases related to gender,\nethnicity, academic popularity, and scholar similarity. Using ground-truth data\nfrom the American Physical Society and OpenAlex, we establish scholarly\nbenchmarks by comparing model outputs to real-world academic records. Our\nanalysis reveals inconsistencies and biases across all models. mixtral-8x7b\nproduces the most stable outputs, while llama3.1-70b shows the highest\nvariability. Many models exhibit duplication, and some, particularly gemma2-9b\nand llama3.1-8b, struggle with formatting errors. LLMs generally recommend real\nscientists, but accuracy drops in field-, epoch-, and seniority-specific\nqueries, consistently favoring senior scholars. Representation biases persist,\nreplicating gender imbalances (reflecting male predominance),\nunder-representing Asian scientists, and over-representing White scholars.\nDespite some diversity in institutional and collaboration networks, models\nfavor highly cited and productive scholars, reinforcing the rich-getricher\neffect while offering limited geographical representation. These findings\nhighlight the need to improve LLMs for more reliable and equitable scholarly\nrecommendations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u516d\u79cd\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u5b66\u9886\u57df\u4e13\u5bb6\u63a8\u8350\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5b58\u5728\u4e00\u81f4\u6027\u95ee\u9898\u3001\u4e8b\u5b9e\u9519\u8bef\u53ca\u591a\u79cd\u504f\u89c1\uff0c\u9700\u6539\u8fdb\u4ee5\u63d0\u9ad8\u53ef\u9760\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30\u5f00\u6e90LLMs\u5728\u4e13\u5bb6\u63a8\u8350\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u4e00\u81f4\u6027\u3001\u4e8b\u5b9e\u6027\u548c\u6f5c\u5728\u504f\u89c1\uff0c\u4ee5\u63a8\u52a8\u66f4\u53ef\u9760\u7684\u5b66\u672f\u63a8\u8350\u7cfb\u7edf\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u6a21\u578b\u8f93\u51fa\u4e0e\u7f8e\u56fd\u7269\u7406\u5b66\u4f1a\u548cOpenAlex\u7684\u771f\u5b9e\u5b66\u672f\u6570\u636e\uff0c\u8bc4\u4f30\u516d\u79cdLLMs\u5728\u4e94\u4e2a\u4e13\u5bb6\u63a8\u8350\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5206\u6790\u663e\u793a\u6a21\u578b\u666e\u904d\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u548c\u504f\u89c1\uff0cmixtral-8x7b\u8f93\u51fa\u6700\u7a33\u5b9a\uff0cllama3.1-70b\u53d8\u5f02\u6027\u6700\u9ad8\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5728\u6027\u522b\u3001\u79cd\u65cf\u548c\u5b66\u672f\u5f71\u54cd\u529b\u7b49\u65b9\u9762\u8868\u73b0\u51fa\u660e\u663e\u504f\u5dee\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u5f53\u524dLLMs\u5728\u5b66\u672f\u63a8\u8350\u4e2d\u4ecd\u5b58\u5728\u8bf8\u591a\u95ee\u9898\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u51cf\u5c11\u504f\u89c1\u5e76\u63d0\u5347\u51c6\u786e\u6027\u3002", "keywords": "LLMs, \u4e13\u5bb6\u63a8\u8350, \u4e00\u81f4\u6027, \u4e8b\u5b9e\u6027, \u504f\u89c1"}}
{"id": "2506.00910", "pdf": "https://arxiv.org/pdf/2506.00910", "abs": "https://arxiv.org/abs/2506.00910", "authors": ["Seongjae Kang", "Dong Bok Lee", "Hyungjoon Jang", "Dongseop Kim", "Sung Ju Hwang"], "title": "PCoreSet: Effective Active Learning through Knowledge Distillation from Vision-Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "35 pages, 30 figures", "summary": "Knowledge distillation (KD) is a widely used framework for training compact,\ntask-specific models by leveraging the knowledge of teacher models. However,\nits application to active learning (AL), which aims to minimize annotation\ncosts through iterative sample selection, remains underexplored. This gap stems\nfrom the fact that KD typically assumes access to sufficient labeled data,\nwhereas AL operates in data-scarce scenarios where task-specific teacher models\nare often unavailable. In this paper, we introduce ActiveKD, a framework that\nintegrates AL with KD by leveraging the zero- and few-shot capabilities of\nlarge vision-language models (VLMs). A key aspect of ActiveKD is the structured\nprediction bias of VLMs -- i.e., their predictions form clusters in the\nprobability space. We regard this structure as an inductive bias of the teacher\nmodel, capturing generalizable output patterns beneficial to student learning.\nTo exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection\nstrategy that maximizes coverage in the probability space rather than the\nfeature space. PCoreSet strategically selects categorically diverse unlabeled\nsamples, facilitating more efficient transfer of teacher knowledge under\nlimited annotation budgets. Evaluations on 11 datasets show that PCoreSet\nconsistently outperforms existing selection methods within the ActiveKD\nframework, advancing research at the intersection of AL and KD.", "AI": {"tldr": "ActiveKD\u662f\u4e00\u4e2a\u5c06\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u4e0e\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u80fd\u529b\uff0c\u901a\u8fc7\u6982\u7387\u6838\u5fc3\u96c6\uff08PCoreSet\uff09\u7b56\u7565\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u9ad8\u6548\u4f20\u9012\u6559\u5e08\u77e5\u8bc6\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u586b\u8865\u77e5\u8bc6\u84b8\u998f\u5728\u4e3b\u52a8\u5b66\u4e60\u4e2d\u5e94\u7528\u7684\u7a7a\u767d\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\uff0c\u5982\u4f55\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u6709\u6548\u4f20\u9012\u6559\u5e08\u77e5\u8bc6\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5f15\u5165ActiveKD\u6846\u67b6\uff0c\u5229\u7528VLMs\u7684\u7ed3\u6784\u5316\u9884\u6d4b\u504f\u7f6e\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u5e76\u63d0\u51faProbabilistic CoreSet\uff08PCoreSet\uff09\u7b56\u7565\uff0c\u4ee5\u5728\u6982\u7387\u7a7a\u95f4\u4e2d\u6700\u5927\u5316\u8986\u76d6\u3002", "result": "\u572811\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cPCoreSet\u5728ActiveKD\u6846\u67b6\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u9009\u62e9\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u8bba\u662fActiveKD\u6846\u67b6\u6210\u529f\u5730\u5c06AL\u4e0eKD\u7ed3\u5408\uff0c\u901a\u8fc7PCoreSet\u7b56\u7565\u63d0\u9ad8\u4e86\u5728\u6709\u9650\u6807\u6ce8\u9884\u7b97\u4e0b\u7684\u77e5\u8bc6\u4f20\u9012\u6548\u7387\u3002", "keywords": "\u77e5\u8bc6\u84b8\u998f, \u4e3b\u52a8\u5b66\u4e60, \u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u6982\u7387\u6838\u5fc3\u96c6"}}
{"id": "2506.00912", "pdf": "https://arxiv.org/pdf/2506.00912", "abs": "https://arxiv.org/abs/2506.00912", "authors": ["Yongdong chi", "Hanqing Wang", "Zonghan Yang", "Jian Yang", "Xiao Yan", "Yun Chen", "Guanhua Chen"], "title": "Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot Programming Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text-to-SQL transforms the user queries from natural language to executable\nSQL programs, enabling non-experts to interact with complex databases. Existing\nprompt-based methods craft meticulous text guidelines and examples to\nfacilitate SQL generation, but their accuracy is hindered by the large semantic\ngap between the texts and the low-resource SQL programs. In this work, we\npropose Pi-SQL, which incorporates the high-resource Python program as a pivot\nto bridge between the natural language query and SQL program. In particular,\nPi-SQL first generates Python programs that provide fine-grained step-by-step\nguidelines in their code blocks or comments, and then produces an SQL program\nfollowing the guidance of each Python program.The final SQL program matches the\nreference Python program's query results and, through selection from candidates\ngenerated by different strategies, achieves superior execution speed, with a\nreward-based valid efficiency score up to 4.55 higher than the best-performing\nbaseline.Extensive experiments demonstrate the effectiveness of Pi-SQL, which\nimproves the execution accuracy of the best-performing baseline by up to 3.20.", "AI": {"tldr": "Pi-SQL \u901a\u8fc7\u5f15\u5165Python\u4f5c\u4e3a\u4e2d\u4ecb\u6865\u63a5\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u4e0eSQL\u7a0b\u5e8f\uff0c\u663e\u8457\u63d0\u5347\u6587\u672c\u8f6cSQL\u7684\u51c6\u786e\u6027\u548c\u6267\u884c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u5728\u6587\u672c\u4e0e\u4f4e\u8d44\u6e90SQL\u7a0b\u5e8f\u4e4b\u95f4\u5b58\u5728\u8f83\u5927\u8bed\u4e49\u5dee\u8ddd\uff0c\u5bfc\u81f4\u51c6\u786e\u6027\u53d7\u9650\u3002", "method": "Pi-SQL \u9996\u5148\u751f\u6210Python\u7a0b\u5e8f\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u6b65\u9aa4\u6307\u5bfc\uff0c\u7136\u540e\u57fa\u4e8ePython\u751f\u6210SQL\u7a0b\u5e8f\uff0c\u5e76\u901a\u8fc7\u5019\u9009\u7b56\u7565\u9009\u62e9\u6700\u4f18\u89e3\u3002", "result": "Pi-SQL \u7684\u6267\u884c\u6548\u7387\u8bc4\u5206\u6bd4\u8868\u73b0\u6700\u4f73\u7684\u57fa\u7ebf\u9ad8\u51fa4.55\uff0c\u6267\u884c\u51c6\u786e\u6027\u63d0\u5347\u9ad8\u8fbe3.20\u3002", "conclusion": "Pi-SQL \u901a\u8fc7Python\u4f5c\u4e3a\u4e2d\u4ecb\uff0c\u6709\u6548\u7f29\u5c0f\u4e86\u8bed\u4e49\u5dee\u8ddd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u8f6cSQL\u7684\u6027\u80fd\u3002", "keywords": "\u6587\u672c\u8f6cSQL, Python, \u8bed\u4e49\u5dee\u8ddd, Pi-SQL"}}
{"id": "2506.00075", "pdf": "https://arxiv.org/pdf/2506.00075", "abs": "https://arxiv.org/abs/2506.00075", "authors": ["Diego Pollini", "Bruna V. Guterres", "Rodrigo S. Guerra", "Ricardo B. Grando"], "title": "Reducing Latency in LLM-Based Natural Language Commands Processing for Robot Navigation", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted to the 23rd IEEE International Conference on Industrial\n  Informatics (INDIN)", "summary": "The integration of Large Language Models (LLMs), such as GPT, in industrial\nrobotics enhances operational efficiency and human-robot collaboration.\nHowever, the computational complexity and size of these models often provide\nlatency problems in request and response times. This study explores the\nintegration of the ChatGPT natural language model with the Robot Operating\nSystem 2 (ROS 2) to mitigate interaction latency and improve robotic system\ncontrol within a simulated Gazebo environment. We present an architecture that\nintegrates these technologies without requiring a middleware transport\nplatform, detailing how a simulated mobile robot responds to text and voice\ncommands. Experimental results demonstrate that this integration improves\nexecution speed, usability, and accessibility of the human-robot interaction by\ndecreasing the communication latency by 7.01\\% on average. Such improvements\nfacilitate smoother, real-time robot operations, which are crucial for\nindustrial automation and precision tasks.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5c06ChatGPT\u4e0eROS 2\u7ed3\u5408\u4ee5\u51cf\u5c11\u4ea4\u4e92\u5ef6\u8fdf\uff0c\u63d0\u5347\u673a\u5668\u4eba\u63a7\u5236\u6548\u7387\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u4e1a\u673a\u5668\u4eba\u4e2d\u56e0\u8ba1\u7b97\u590d\u6742\u6027\u548c\u89c4\u6a21\u5bfc\u81f4\u7684\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6574\u5408ChatGPT\u4e0eROS 2\uff0c\u8bbe\u8ba1\u65e0\u9700\u4e2d\u95f4\u4ef6\u7684\u67b6\u6784\uff0c\u5e76\u5728Gazebo\u6a21\u62df\u73af\u5883\u4e2d\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u901a\u4fe1\u5ef6\u8fdf\u5e73\u5747\u964d\u4f4e7.01%\uff0c\u63d0\u5347\u4e86\u6267\u884c\u901f\u5ea6\u548c\u4ea4\u4e92\u4f53\u9a8c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u6539\u5584\u4e86\u5b9e\u65f6\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u81ea\u52a8\u5316\u548c\u7cbe\u5bc6\u4efb\u52a1\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, ChatGPT, ROS 2, \u5de5\u4e1a\u673a\u5668\u4eba, \u5ef6\u8fdf\u4f18\u5316"}}
{"id": "2506.00917", "pdf": "https://arxiv.org/pdf/2506.00917", "abs": "https://arxiv.org/abs/2506.00917", "authors": ["Priyank Agrawal", "Shipra Agrawal", "Azmat Azati"], "title": "Q-learning with Posterior Sampling", "categories": ["cs.LG"], "comment": "39 Pages", "summary": "Bayesian posterior sampling techniques have demonstrated superior empirical\nperformance in many exploration-exploitation settings. However, their\ntheoretical analysis remains a challenge, especially in complex settings like\nreinforcement learning. In this paper, we introduce Q-Learning with Posterior\nSampling (PSQL), a simple Q-learning-based algorithm that uses Gaussian\nposteriors on Q-values for exploration, akin to the popular Thompson Sampling\nalgorithm in the multi-armed bandit setting. We show that in the tabular\nepisodic MDP setting, PSQL achieves a regret bound of $\\tilde\nO(H^2\\sqrt{SAT})$, closely matching the known lower bound of\n$\\Omega(H\\sqrt{SAT})$. Here, S, A denote the number of states and actions in\nthe underlying Markov Decision Process (MDP), and $T=KH$ with $K$ being the\nnumber of episodes and $H$ being the planning horizon. Our work provides\nseveral new technical insights into the core challenges in combining posterior\nsampling with dynamic programming and TD-learning-based RL algorithms, along\nwith novel ideas for resolving those difficulties. We hope this will form a\nstarting point for analyzing this efficient and important algorithmic technique\nin even more complex RL settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eQ\u5b66\u4e60\u7684\u7b97\u6cd5PSQL\uff0c\u7ed3\u5408\u9ad8\u65af\u540e\u9a8c\u91c7\u6837\u8fdb\u884c\u63a2\u7d22\uff0c\u5728\u8868\u683c\u578bMDP\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5df2\u77e5\u4e0b\u754c\u7684\u540e\u6094\u754c\u3002", "motivation": "\u8d1d\u53f6\u65af\u540e\u9a8c\u91c7\u6837\u5728\u63a2\u7d22-\u5229\u7528\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7406\u8bba\u5206\u6790\u5c24\u5176\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u4ecd\u5177\u6311\u6218\u6027\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u63d0\u51faQ-Learning with Posterior Sampling (PSQL)\uff0c\u5229\u7528\u9ad8\u65af\u540e\u9a8c\u91c7\u6837Q\u503c\u8fdb\u884c\u63a2\u7d22\u3002", "result": "\u5728\u8868\u683c\u578bMDP\u4e2d\uff0cPSQL\u7684\u540e\u6094\u754c\u4e3a$\tilde O(H^2\\sqrt{SAT})$\uff0c\u63a5\u8fd1\u5df2\u77e5\u4e0b\u754c\u3002", "conclusion": "PSQL\u4e3a\u540e\u9a8c\u91c7\u6837\u4e0e\u52a8\u6001\u89c4\u5212\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u6709\u671b\u63a8\u52a8\u66f4\u590d\u6742RL\u573a\u666f\u7684\u7814\u7a76\u3002", "keywords": "\u8d1d\u53f6\u65af\u540e\u9a8c\u91c7\u6837, Q\u5b66\u4e60, \u5f3a\u5316\u5b66\u4e60, Thompson\u91c7\u6837, \u540e\u6094\u754c"}}
{"id": "2506.00914", "pdf": "https://arxiv.org/pdf/2506.00914", "abs": "https://arxiv.org/abs/2506.00914", "authors": ["Aishik Nagar", "Ishaan Singh Rawal", "Mansi Dhanania", "Cheston Tan"], "title": "How do Transformer Embeddings Represent Compositions? A Functional Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Compositionality is a key aspect of human intelligence, essential for\nreasoning and generalization. While transformer-based models have become the de\nfacto standard for many language modeling tasks, little is known about how they\nrepresent compound words, and whether these representations are compositional.\nIn this study, we test compositionality in Mistral, OpenAI Large, and Google\nembedding models, and compare them with BERT. First, we evaluate\ncompositionality in the representations by examining six diverse models of\ncompositionality (addition, multiplication, dilation, regression, etc.). We\nfind that ridge regression, albeit linear, best accounts for compositionality.\nSurprisingly, we find that the classic vector addition model performs almost as\nwell as any other model. Next, we verify that most embedding models are highly\ncompositional, while BERT shows much poorer compositionality. We verify and\nvisualize our findings with a synthetic dataset consisting of fully transparent\nadjective-noun compositions. Overall, we present a thorough investigation of\ncompositionality.", "AI": {"tldr": "\u7814\u7a76\u4e86Mistral\u3001OpenAI Large\u3001Google\u5d4c\u5165\u6a21\u578b\u548cBERT\u7684\u7ec4\u5408\u6027\uff0c\u53d1\u73b0\u7ebf\u6027\u6a21\u578b\uff08\u5982\u5cad\u56de\u5f52\uff09\u5728\u7ec4\u5408\u6027\u8868\u73b0\u4e0a\u6700\u4f73\uff0c\u4e14\u5927\u591a\u6570\u5d4c\u5165\u6a21\u578b\u5177\u6709\u9ad8\u7ec4\u5408\u6027\uff0c\u800cBERT\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u63a2\u8ba8Transformer\u6a21\u578b\u5728\u7ec4\u5408\u8bcd\u8868\u793a\u4e2d\u7684\u7ec4\u5408\u6027\uff0c\u4ee5\u7406\u89e3\u5176\u5728\u63a8\u7406\u548c\u6cdb\u5316\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u8bc4\u4f30\u516d\u79cd\u7ec4\u5408\u6027\u6a21\u578b\uff08\u52a0\u6cd5\u3001\u4e58\u6cd5\u3001\u6269\u5f20\u3001\u56de\u5f52\u7b49\uff09\uff0c\u4f7f\u7528\u5cad\u56de\u5f52\u4f5c\u4e3a\u57fa\u51c6\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u9a8c\u8bc1\u7ed3\u679c\u3002", "result": "\u5cad\u56de\u5f52\u6a21\u578b\u5728\u7ec4\u5408\u6027\u8868\u73b0\u4e0a\u6700\u4f18\uff0c\u5927\u591a\u6570\u5d4c\u5165\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0c\u4f46BERT\u8f83\u5dee\u3002\u5411\u91cf\u52a0\u6cd5\u6a21\u578b\u8868\u73b0\u63a5\u8fd1\u6700\u4f18\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9\u7ec4\u5408\u6027\u7684\u5168\u9762\u5206\u6790\uff0c\u53d1\u73b0\u5d4c\u5165\u6a21\u578b\u666e\u904d\u5177\u6709\u9ad8\u7ec4\u5408\u6027\uff0c\u800cBERT\u8868\u73b0\u4e0d\u8db3\u3002", "keywords": "\u7ec4\u5408\u6027, Transformer\u6a21\u578b, \u5d4c\u5165\u6a21\u578b, \u5cad\u56de\u5f52, BERT"}}
{"id": "2506.00076", "pdf": "https://arxiv.org/pdf/2506.00076", "abs": "https://arxiv.org/abs/2506.00076", "authors": ["Andrew Cornfeld", "Ashley Miller", "Mercedes Mora-Figueroa", "Kurt Samuels", "Anthony Palomba"], "title": "Optimizing Storytelling, Improving Audience Retention, and Reducing Waste in the Entertainment Industry", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Television networks face high financial risk when making programming\ndecisions, often relying on limited historical data to forecast episodic\nviewership. This study introduces a machine learning framework that integrates\nnatural language processing (NLP) features from over 25000 television episodes\nwith traditional viewership data to enhance predictive accuracy. By extracting\nemotional tone, cognitive complexity, and narrative structure from episode\ndialogue, we evaluate forecasting performance using SARIMAX, rolling XGBoost,\nand feature selection models. While prior viewership remains a strong baseline\npredictor, NLP features contribute meaningful improvements for some series. We\nalso introduce a similarity scoring method based on Euclidean distance between\naggregate dialogue vectors to compare shows by content. Tested across diverse\ngenres, including Better Call Saul and Abbott Elementary, our framework reveals\ngenre-specific performance and offers interpretable metrics for writers,\nexecutives, and marketers seeking data-driven insight into audience behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408NLP\u548c\u4f20\u7edf\u6536\u89c6\u7387\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u7535\u89c6\u5267\u96c6\u6536\u89c6\u7387\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u9762\u5bf9\u8282\u76ee\u51b3\u7b56\u7684\u9ad8\u8d22\u52a1\u98ce\u9669\uff0c\u7535\u89c6\u53f0\u9700\u8981\u66f4\u51c6\u786e\u7684\u6536\u89c6\u7387\u9884\u6d4b\u65b9\u6cd5\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u7684\u5386\u53f2\u6570\u636e\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u5267\u96c6\u5bf9\u8bdd\u7684\u60c5\u611f\u57fa\u8c03\u3001\u8ba4\u77e5\u590d\u6742\u5ea6\u548c\u53d9\u4e8b\u7ed3\u6784\u7b49NLP\u7279\u5f81\uff0c\u7ed3\u5408\u4f20\u7edf\u6536\u89c6\u6570\u636e\uff0c\u4f7f\u7528SARIMAX\u3001XGBoost\u7b49\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u3002", "result": "NLP\u7279\u5f81\u5bf9\u67d0\u4e9b\u5267\u96c6\u7684\u9884\u6d4b\u80fd\u529b\u6709\u6240\u63d0\u5347\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5bf9\u8bdd\u5411\u91cf\u76f8\u4f3c\u6027\u7684\u8bc4\u5206\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u7c7b\u578b\u5267\u96c6\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4e3a\u5185\u5bb9\u521b\u4f5c\u8005\u548c\u7ba1\u7406\u8005\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u89c1\u89e3\u3002", "keywords": "\u6536\u89c6\u7387\u9884\u6d4b,NLP,\u673a\u5668\u5b66\u4e60,\u5185\u5bb9\u76f8\u4f3c\u6027"}}
{"id": "2506.00918", "pdf": "https://arxiv.org/pdf/2506.00918", "abs": "https://arxiv.org/abs/2506.00918", "authors": ["Lennart Bramlage", "Crist\u00f3bal Curio"], "title": "Principled Input-Output-Conditioned Post-Hoc Uncertainty Estimation for Regression Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Uncertainty quantification is critical in safety-sensitive applications but\nis often omitted from off-the-shelf neural networks due to adverse effects on\npredictive performance. Retrofitting uncertainty estimates post-hoc typically\nrequires access to model parameters or gradients, limiting feasibility in\npractice. We propose a theoretically grounded framework for post-hoc\nuncertainty estimation in regression tasks by fitting an auxiliary model to\nboth original inputs and frozen model outputs. Drawing from principles of\nmaximum likelihood estimation and sequential parameter fitting, we formalize an\nexact post-hoc optimization objective that recovers the canonical MLE of\nGaussian parameters, without requiring sampling or approximation at inference.\nWhile prior work has used model outputs to estimate uncertainty, we explicitly\ncharacterize the conditions under which this is valid and demonstrate the\nextent to which structured outputs can support quasi-epistemic inference. We\nfind that using diverse auxiliary data, such as augmented subsets of the\noriginal training data, significantly enhances OOD detection and metric\nperformance. Our hypothesis that frozen model outputs contain generalizable\nlatent information about model error and predictive uncertainty is tested and\nconfirmed. Finally, we ensure that our method maintains proper estimation of\ninput-dependent uncertainty without relying exclusively on base model\nforecasts. These findings are demonstrated in toy problems and adapted to both\nUCI and depth regression benchmarks. Code: https://github.com/biggzlar/IO-CUE.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u652f\u6301\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u62df\u5408\u8f85\u52a9\u6a21\u578b\u6765\u4f30\u8ba1\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u53c2\u6570\u6216\u68af\u5ea6\uff0c\u4e14\u663e\u8457\u63d0\u5347\u4e86OOD\u68c0\u6d4b\u548c\u5ea6\u91cf\u6027\u80fd\u3002", "motivation": "\u5728\u5b89\u5168\u654f\u611f\u5e94\u7528\u4e2d\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u5e38\u56e0\u6027\u80fd\u5f71\u54cd\u800c\u5ffd\u7565\u8fd9\u4e00\u70b9\u3002\u73b0\u6709\u540e\u5904\u7406\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u8bbf\u95ee\u6a21\u578b\u53c2\u6570\u6216\u68af\u5ea6\uff0c\u5b9e\u7528\u6027\u53d7\u9650\u3002", "method": "\u901a\u8fc7\u62df\u5408\u8f85\u52a9\u6a21\u578b\uff0c\u7ed3\u5408\u539f\u59cb\u8f93\u5165\u548c\u51bb\u7ed3\u6a21\u578b\u8f93\u51fa\uff0c\u5229\u7528\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u548c\u987a\u5e8f\u53c2\u6570\u62df\u5408\uff0c\u5f62\u5f0f\u5316\u540e\u4f18\u5316\u76ee\u6807\uff0c\u65e0\u9700\u63a8\u7406\u65f6\u7684\u91c7\u6837\u6216\u8fd1\u4f3c\u3002", "result": "\u4f7f\u7528\u591a\u6837\u5316\u7684\u8f85\u52a9\u6570\u636e\uff08\u5982\u589e\u5f3a\u7684\u5b50\u96c6\uff09\u663e\u8457\u63d0\u5347\u4e86OOD\u68c0\u6d4b\u548c\u5ea6\u91cf\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u51bb\u7ed3\u6a21\u578b\u8f93\u51fa\u5305\u542b\u5173\u4e8e\u6a21\u578b\u8bef\u5dee\u548c\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u5e7f\u4e49\u6f5c\u5728\u4fe1\u606f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u4f9d\u8d56\u57fa\u7840\u6a21\u578b\u9884\u6d4b\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u4f30\u8ba1\u4e86\u8f93\u5165\u76f8\u5173\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "keywords": "\u4e0d\u786e\u5b9a\u6027\u91cf\u5316,\u540e\u9a8c\u4f30\u8ba1,\u56de\u5f52\u4efb\u52a1,\u6700\u5927\u4f3c\u7136\u4f30\u8ba1,OOD\u68c0\u6d4b"}}
{"id": "2506.00942", "pdf": "https://arxiv.org/pdf/2506.00942", "abs": "https://arxiv.org/abs/2506.00942", "authors": ["Haitao Li", "Ziyu Li", "Yiheng Mao", "Ziyi Liu", "Zhoujian Sun", "Zhengxing Huang"], "title": "anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding", "categories": ["cs.CL", "cs.AI", "eess.SP"], "comment": null, "summary": "The advent of multimodal large language models (MLLMs) has sparked interest\nin their application to electrocardiogram (ECG) analysis. However, existing\nECG-focused MLLMs primarily focus on report generation tasks, often limited to\nsingle 12-lead, short-duration (10s) ECG inputs, thereby underutilizing the\npotential of MLLMs. To this end, we aim to develop a MLLM for ECG analysis that\nsupports a broader range of tasks and more flexible ECG inputs. However,\nexisting ECG-QA datasets are often monotonous. To address this gap, we first\nconstructed the anyECG dataset, which encompasses a wide variety of tasks,\nincluding report generation, abnormal waveform localization, and open-ended\nquestion answering. In addition to standard hospital ECGs, we introduced\nlong-duration reduced-lead ECGs for home environments and multiple ECG\ncomparison scenarios commonly encountered in clinical practice. Furthermore, we\npropose the anyECG-chat model, which supports dynamic-length ECG inputs and\nmultiple ECG inputs. We trained the model using a three-stage curriculum\ntraining recipe with the anyECG dataset. A comprehensive evaluation was\nconducted, demonstrating that anyECG-chat is capable of supporting various\npractical application scenarios, including not only common report generation\ntasks but also abnormal waveform localization for long-duration reduced-lead\nECGs in home environments and comprehensive comparative analysis of multiple\nECGs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u591a\u79cd\u4efb\u52a1\u548c\u7075\u6d3b\u5fc3\u7535\u56fe\uff08ECG\uff09\u8f93\u5165\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u5e76\u4ecb\u7ecd\u4e86anyECG\u6570\u636e\u96c6\u53ca\u5176\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u5fc3\u7535\u56fe\u5206\u6790\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u591a\u5c40\u9650\u4e8e\u5355\u6b2112\u5bfc\u8054\u77ed\u65f6\u7a0b\u8f93\u5165\uff0c\u672a\u80fd\u5145\u5206\u53d1\u6325\u6f5c\u529b\uff0c\u56e0\u6b64\u9700\u5f00\u53d1\u66f4\u7075\u6d3b\u7684\u6a21\u578b\u3002", "method": "\u6784\u5efaanyECG\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u6837\u4efb\u52a1\uff1b\u63d0\u51faanyECG-chat\u6a21\u578b\uff0c\u652f\u6301\u52a8\u6001\u957f\u5ea6\u548c\u591aECG\u8f93\u5165\uff0c\u5e76\u91c7\u7528\u4e09\u9636\u6bb5\u8bfe\u7a0b\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "anyECG-chat\u6a21\u578b\u6210\u529f\u652f\u6301\u591a\u79cd\u5b9e\u9645\u5e94\u7528\u573a\u666f\uff0c\u5982\u62a5\u544a\u751f\u6210\u3001\u5f02\u5e38\u6ce2\u5f62\u5b9a\u4f4d\u53ca\u591aECG\u5bf9\u6bd4\u5206\u6790\u3002", "conclusion": "anyECG-chat\u6a21\u578b\u6269\u5c55\u4e86ECG\u5206\u6790\u7684\u9002\u7528\u6027\uff0c\u9002\u7528\u4e8e\u5bb6\u5ead\u548c\u4e34\u5e8a\u73af\u5883\uff0c\u5c55\u73b0\u51fa\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, \u5fc3\u7535\u56fe\u5206\u6790, \u52a8\u6001\u8f93\u5165, \u6570\u636e\u96c6\u6784\u5efa"}}
{"id": "2506.00079", "pdf": "https://arxiv.org/pdf/2506.00079", "abs": "https://arxiv.org/abs/2506.00079", "authors": ["John P. Dickerson", "Hadi Hosseini", "Samarth Khanna", "Leona Pierce"], "title": "Who Gets the Kidney? Human-AI Alignment, Indecision, and Moral Values", "categories": ["cs.CY", "cs.AI", "cs.LG", "I.2.1; I.2.7; I.2.11"], "comment": null, "summary": "The rapid integration of Large Language Models (LLMs) in high-stakes\ndecision-making -- such as allocating scarce resources like donor organs --\nraises critical questions about their alignment with human moral values. We\nsystematically evaluate the behavior of several prominent LLMs against human\npreferences in kidney allocation scenarios and show that LLMs: i) exhibit stark\ndeviations from human values in prioritizing various attributes, and ii) in\ncontrast to humans, LLMs rarely express indecision, opting for deterministic\ndecisions even when alternative indecision mechanisms (e.g., coin flipping) are\nprovided. Nonetheless, we show that low-rank supervised fine-tuning with few\nsamples is often effective in improving both decision consistency and\ncalibrating indecision modeling. These findings illustrate the necessity of\nexplicit alignment strategies for LLMs in moral/ethical domains.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u80be\u810f\u5206\u914d\u51b3\u7b56\u4e2d\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u53d1\u73b0LLMs\u5b58\u5728\u663e\u8457\u504f\u5dee\uff0c\u4e14\u7f3a\u4e4f\u4eba\u7c7b\u5e38\u89c1\u7684\u72b9\u8c6b\u673a\u5236\u3002\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u7684\u4f4e\u79e9\u76d1\u7763\u5fae\u8c03\u53ef\u6539\u8fdb\u51b3\u7b56\u4e00\u81f4\u6027\u548c\u72b9\u8c6b\u6821\u51c6\u3002", "motivation": "\u63a2\u7a76LLMs\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\uff08\u5982\u5668\u5b98\u5206\u914d\uff09\u4e2d\u662f\u5426\u4e0e\u4eba\u7c7b\u9053\u5fb7\u4ef7\u503c\u89c2\u5bf9\u9f50\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u591a\u4e2aLLMs\u5728\u80be\u810f\u5206\u914d\u573a\u666f\u4e2d\u7684\u884c\u4e3a\uff0c\u5e76\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u6bd4\uff1b\u91c7\u7528\u4f4e\u79e9\u76d1\u7763\u5fae\u8c03\u6539\u8fdb\u6a21\u578b\u884c\u4e3a\u3002", "result": "LLMs\u5728\u5c5e\u6027\u4f18\u5148\u7ea7\u4e0a\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5b58\u5728\u663e\u8457\u504f\u5dee\uff0c\u4e14\u51e0\u4e4e\u4e0d\u8868\u8fbe\u72b9\u8c6b\uff1b\u5c11\u91cf\u6837\u672c\u7684\u5fae\u8c03\u53ef\u663e\u8457\u6539\u8fdb\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "LLMs\u5728\u9053\u5fb7/\u4f26\u7406\u9886\u57df\u9700\u8981\u663e\u5f0f\u7684\u5bf9\u9f50\u7b56\u7565\u4ee5\u786e\u4fdd\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4e00\u81f4\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u9053\u5fb7\u5bf9\u9f50,\u5668\u5b98\u5206\u914d,\u51b3\u7b56\u4e00\u81f4\u6027,\u4f4e\u79e9\u5fae\u8c03"}}
{"id": "2506.00920", "pdf": "https://arxiv.org/pdf/2506.00920", "abs": "https://arxiv.org/abs/2506.00920", "authors": ["Philip Heejun Lee"], "title": "Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "comment": "Note: v1: working paper; code, additional baselines, ablations, will\n  follow in v2", "summary": "Deep sequence models typically degrade in accuracy when test sequences\nsignificantly exceed their training lengths, yet many critical tasks--such as\nalgorithmic reasoning, multi-step arithmetic, and compositional\ngeneralization--require robust length extrapolation. We introduce PRISM, a\nProbabilistic Relative-position Implicit Superposition Model, a novel\npositional encoding mechanism that enables Transformers to extrapolate\naccurately up to 10x beyond their training length. PRISM learns continuous\nrelative positions through a differentiable histogram-filter update, preserving\nposition uncertainty via a probabilistic superposition rather than conventional\ndeterministic embeddings. Empirically, PRISM achieves state-of-the-art length\nextrapolation, successfully generalizing to previously intractable sequence\nlengths across algorithmic benchmarks--including arithmetic (addition,\nmultiplication), SCAN compositionality tasks, and complex copy variants derived\nfrom DeepMind's recent datasets. Our analysis demonstrates that PRISM's\nstochastic positional encoding maintains sharp and interpretable internal\nstates, providing a theoretical basis for reliable length generalization. These\nresults advance the goal of neural sequence models that remain algorithmically\nrobust at lengths far exceeding their training horizon.", "AI": {"tldr": "PRISM\u662f\u4e00\u79cd\u65b0\u578b\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\uff0c\u4f7fTransformer\u80fd\u51c6\u786e\u5916\u63a8\u81f3\u8bad\u7ec3\u957f\u5ea6\u768410\u500d\uff0c\u89e3\u51b3\u4e86\u5e8f\u5217\u6a21\u578b\u5728\u6d4b\u8bd5\u957f\u5ea6\u8d85\u51fa\u8bad\u7ec3\u8303\u56f4\u65f6\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u6d4b\u8bd5\u5e8f\u5217\u663e\u8457\u8d85\u51fa\u8bad\u7ec3\u957f\u5ea6\u65f6\uff0c\u6df1\u5ea6\u5e8f\u5217\u6a21\u578b\u7684\u51c6\u786e\u6027\u901a\u5e38\u4f1a\u4e0b\u964d\uff0c\u800c\u8bb8\u591a\u5173\u952e\u4efb\u52a1\uff08\u5982\u7b97\u6cd5\u63a8\u7406\u3001\u591a\u6b65\u7b97\u672f\u548c\u7ec4\u5408\u6cdb\u5316\uff09\u9700\u8981\u7a33\u5065\u7684\u957f\u5ea6\u5916\u63a8\u80fd\u529b\u3002", "method": "PRISM\u901a\u8fc7\u53ef\u5fae\u5206\u76f4\u65b9\u56fe\u6ee4\u6ce2\u66f4\u65b0\u5b66\u4e60\u8fde\u7eed\u76f8\u5bf9\u4f4d\u7f6e\uff0c\u91c7\u7528\u6982\u7387\u53e0\u52a0\u800c\u975e\u786e\u5b9a\u6027\u5d4c\u5165\u6765\u4fdd\u7559\u4f4d\u7f6e\u4e0d\u786e\u5b9a\u6027\u3002", "result": "PRISM\u5728\u7b97\u6cd5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u957f\u5ea6\u5916\u63a8\u80fd\u529b\uff0c\u6210\u529f\u6cdb\u5316\u5230\u4ee5\u524d\u96be\u4ee5\u5904\u7406\u7684\u5e8f\u5217\u957f\u5ea6\u3002", "conclusion": "PRISM\u7684\u968f\u673a\u4f4d\u7f6e\u7f16\u7801\u4fdd\u6301\u4e86\u6e05\u6670\u4e14\u53ef\u89e3\u91ca\u7684\u5185\u90e8\u72b6\u6001\uff0c\u4e3a\u53ef\u9760\u7684\u957f\u5ea6\u6cdb\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "keywords": "PRISM, Transformer, \u957f\u5ea6\u5916\u63a8, \u4f4d\u7f6e\u7f16\u7801, \u6982\u7387\u53e0\u52a0"}}
{"id": "2506.00955", "pdf": "https://arxiv.org/pdf/2506.00955", "abs": "https://arxiv.org/abs/2506.00955", "authors": ["Zhu Li", "Yuqing Zhang", "Xiyuan Gao", "Shekhar Nayak", "Matt Coler"], "title": "Leveraging Large Language Models for Sarcastic Speech Annotation in Sarcasm Detection", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Sarcasm fundamentally alters meaning through tone and context, yet detecting\nit in speech remains a challenge due to data scarcity. In addition, existing\ndetection systems often rely on multimodal data, limiting their applicability\nin contexts where only speech is available. To address this, we propose an\nannotation pipeline that leverages large language models (LLMs) to generate a\nsarcasm dataset. Using a publicly available sarcasm-focused podcast, we employ\nGPT-4o and LLaMA 3 for initial sarcasm annotations, followed by human\nverification to resolve disagreements. We validate this approach by comparing\nannotation quality and detection performance on a publicly available sarcasm\ndataset using a collaborative gating architecture. Finally, we introduce\nPodSarc, a large-scale sarcastic speech dataset created through this pipeline.\nThe detection model achieves a 73.63% F1 score, demonstrating the dataset's\npotential as a benchmark for sarcasm detection research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u8bbd\u523a\u8bed\u97f3\u6570\u636e\u96c6\u7684\u6807\u6ce8\u6d41\u7a0b\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u9a8c\u8bc1\u4f18\u5316\u6807\u6ce8\u8d28\u91cf\u3002\u8be5\u65b9\u6cd5\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6548\u679c\uff0c\u5e76\u53d1\u5e03\u4e86PodSarc\u6570\u636e\u96c6\uff0cF1\u5206\u6570\u8fbe73.63%\u3002", "motivation": "\u8bbd\u523a\u901a\u8fc7\u8bed\u6c14\u548c\u4e0a\u4e0b\u6587\u6539\u53d8\u542b\u4e49\uff0c\u4f46\u8bed\u97f3\u4e2d\u68c0\u6d4b\u8bbd\u523a\u4ecd\u56e0\u6570\u636e\u7a00\u7f3a\u548c\u591a\u6a21\u6001\u4f9d\u8d56\u800c\u53d7\u9650\u3002", "method": "\u901a\u8fc7LLM\uff08GPT-4o\u548cLLaMA 3\uff09\u5bf9\u8bbd\u523a\u64ad\u5ba2\u8fdb\u884c\u521d\u59cb\u6807\u6ce8\uff0c\u7ed3\u5408\u4eba\u7c7b\u9a8c\u8bc1\u751f\u6210\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528\u534f\u4f5c\u95e8\u63a7\u67b6\u6784\u9a8c\u8bc1\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u521b\u5efa\u7684PodSarc\u6570\u636e\u96c6\u5728\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u523073.63%\u7684F1\u5206\u6570\uff0c\u5c55\u73b0\u4e86\u4f5c\u4e3a\u57fa\u51c6\u6570\u636e\u96c6\u7684\u6f5c\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6807\u6ce8\u6d41\u7a0b\u548cPodSarc\u6570\u636e\u96c6\u4e3a\u8bed\u97f3\u8bbd\u523a\u68c0\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u548c\u57fa\u51c6\u3002", "keywords": "\u8bbd\u523a\u68c0\u6d4b\u3001\u8bed\u97f3\u5206\u6790\u3001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u6570\u636e\u96c6\u751f\u6210\u3001\u534f\u540c\u6807\u6ce8"}}
{"id": "2506.00080", "pdf": "https://arxiv.org/pdf/2506.00080", "abs": "https://arxiv.org/abs/2506.00080", "authors": ["Stefan Pasch"], "title": "Bottom-Up Perspectives on AI Governance: Insights from User Reviews of AI Products", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "With the growing importance of AI governance, numerous high-level frameworks\nand principles have been articulated by policymakers, institutions, and expert\ncommunities to guide the development and application of AI. While such\nframeworks offer valuable normative orientation, they may not fully capture the\npractical concerns of those who interact with AI systems in organizational and\noperational contexts. To address this gap, this study adopts a bottom-up\napproach to explore how governance-relevant themes are expressed in user\ndiscourse. Drawing on over 100,000 user reviews of AI products from G2.com, we\napply BERTopic to extract latent themes and identify those most semantically\nrelated to AI governance. The analysis reveals a diverse set of\ngovernance-relevant topics spanning both technical and non-technical domains.\nThese include concerns across organizational processes-such as planning,\ncoordination, and communication-as well as stages of the AI value chain,\nincluding deployment infrastructure, data handling, and analytics. The findings\nshow considerable overlap with institutional AI governance and ethics\nframeworks on issues like privacy and transparency, but also surface overlooked\nareas such as project management, strategy development, and customer\ninteraction. This highlights the need for more empirically grounded,\nuser-centered approaches to AI governance-approaches that complement normative\nmodels by capturing how governance unfolds in applied settings. By\nforegrounding how governance is enacted in practice, this study contributes to\nmore inclusive and operationally grounded approaches to AI governance and\ndigital policy.", "AI": {"tldr": "\u7814\u7a76\u91c7\u7528\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u7528\u6237\u8bc4\u8bba\u63a2\u8ba8AI\u6cbb\u7406\u7684\u5b9e\u8df5\u4e3b\u9898\uff0c\u63ed\u793a\u4e0e\u6280\u672f\u4e0e\u975e\u6280\u672f\u9886\u57df\u76f8\u5173\u7684\u6cbb\u7406\u95ee\u9898\u3002", "motivation": "\u73b0\u6709AI\u6cbb\u7406\u6846\u67b6\u591a\u4e3a\u9ad8\u5c42\u6307\u5bfc\uff0c\u672a\u80fd\u5145\u5206\u53cd\u6620\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u95ee\u9898\uff0c\u7814\u7a76\u8bd5\u56fe\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5229\u7528BERTopic\u4ece10\u4e07+AI\u4ea7\u54c1\u7528\u6237\u8bc4\u8bba\u4e2d\u63d0\u53d6\u4e0eAI\u6cbb\u7406\u8bed\u4e49\u76f8\u5173\u7684\u6f5c\u5728\u4e3b\u9898\u3002", "result": "\u53d1\u73b0\u6cbb\u7406\u4e3b\u9898\u6db5\u76d6\u6280\u672f\u4e0e\u975e\u6280\u672f\u9886\u57df\uff0c\u90e8\u5206\u4e0e\u73b0\u6709\u6846\u67b6\u91cd\u53e0\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u5982\u9879\u76ee\u7ba1\u7406\u7b49\u88ab\u5ffd\u89c6\u7684\u65b9\u9762\u3002", "conclusion": "\u9700\u66f4\u591a\u5b9e\u8bc1\u548c\u7528\u6237\u4e2d\u5fc3\u7684AI\u6cbb\u7406\u65b9\u6cd5\uff0c\u4ee5\u8865\u5145\u73b0\u6709\u89c4\u8303\u6a21\u578b\u3002", "keywords": "AI\u6cbb\u7406, \u7528\u6237\u8bc4\u8bba, BERTopic, \u5b9e\u8df5\u4e3b\u9898, \u6570\u5b57\u653f\u7b56"}}
{"id": "2506.00932", "pdf": "https://arxiv.org/pdf/2506.00932", "abs": "https://arxiv.org/abs/2506.00932", "authors": ["Qiao Xiao", "Boqian Wu", "Andrey Poddubnyy", "Elena Mocanu", "Phuong H. Nguyen", "Mykola Pechenizkiy", "Decebal Constantin Mocanu"], "title": "Addressing the Collaboration Dilemma in Low-Data Federated Learning via Transient Sparsity", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning (FL) enables collaborative model training across\ndecentralized clients while preserving data privacy, leveraging aggregated\nupdates to build robust global models. However, this training paradigm faces\nsignificant challenges due to data heterogeneity and limited local datasets,\nwhich often impede effective collaboration. In such scenarios, we identify the\nLayer-wise Inertia Phenomenon in FL, wherein the middle layers of global model\nundergo minimal updates after early communication rounds, ultimately limiting\nthe effectiveness of global aggregation. We demonstrate the presence of this\nphenomenon across a wide range of federated settings, spanning diverse datasets\nand architectures. To address this issue, we propose LIPS (Layer-wise Inertia\nPhenomenon with Sparsity), a simple yet effective method that periodically\nintroduces transient sparsity to stimulate meaningful updates and empower\nglobal aggregation. Experiments demonstrate that LIPS effectively mitigates\nlayer-wise inertia, enhances aggregation effectiveness, and improves overall\nperformance in various FL scenarios. This work not only deepens the\nunderstanding of layer-wise learning dynamics in FL but also paves the way for\nmore effective collaboration strategies in resource-constrained environments.\nOur code is publicly available at: https://github.com/QiaoXiao7282/LIPS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLIPS\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u51fa\u73b0\u7684\u5c42\u95f4\u60ef\u6027\u73b0\u8c61\uff0c\u901a\u8fc7\u5f15\u5165\u7a00\u758f\u6027\u6fc0\u52b1\u66f4\u65b0\uff0c\u63d0\u5347\u5168\u5c40\u805a\u5408\u6548\u679c\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u5f02\u6784\u6027\u548c\u672c\u5730\u6570\u636e\u96c6\u6709\u9650\u7684\u95ee\u9898\u5bfc\u81f4\u4e2d\u95f4\u5c42\u66f4\u65b0\u4e0d\u8db3\uff0c\u5f71\u54cd\u5168\u5c40\u6a21\u578b\u6548\u679c\uff0c\u7814\u7a76\u8005\u8bd5\u56fe\u89e3\u51b3\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u63d0\u51faLIPS\u65b9\u6cd5\uff0c\u5468\u671f\u6027\u5f15\u5165\u77ac\u6001\u7a00\u758f\u6027\u4ee5\u6fc0\u52b1\u6709\u6548\u66f4\u65b0\uff0c\u4ece\u800c\u589e\u5f3a\u5168\u5c40\u805a\u5408\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLIPS\u80fd\u6709\u6548\u7f13\u89e3\u5c42\u95f4\u60ef\u6027\uff0c\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u805a\u5408\u6548\u679c\u548c\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "LIPS\u4e0d\u4ec5\u6df1\u5316\u4e86\u5bf9\u8054\u90a6\u5b66\u4e60\u4e2d\u5c42\u95f4\u52a8\u6001\u7684\u7406\u89e3\uff0c\u8fd8\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u534f\u4f5c\u7b56\u7565\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60, \u5c42\u95f4\u60ef\u6027, \u7a00\u758f\u6027, \u5168\u5c40\u805a\u5408"}}
{"id": "2506.00963", "pdf": "https://arxiv.org/pdf/2506.00963", "abs": "https://arxiv.org/abs/2506.00963", "authors": ["Cheng Cheng", "Zhenya Huang", "Guanhao Zhao", "Yuxiang Guo", "Xin Lin", "Jinze Wu", "Xin Li", "Shijin Wang"], "title": "From Objectives to Questions: A Planning-based Framework for Educational Mathematical Question Generation", "categories": ["cs.CL"], "comment": null, "summary": "Automatically generating high-quality mathematical problems that align with\neducational objectives is a crucial task in NLP-based educational technology.\nTraditional generation methods focus primarily on textual quality, but they\noften overlook educational objectives. Moreover, these methods address only\nsingle-dimensional, simple question generation, failing to meet complex,\nmultifaceted educational requirements. To address these challenges, we\nconstructed and annotated EduMath, a dataset of 16k mathematical questions with\nmulti-dimensional educational objectives. Based on this dataset, we developed\nEQGEVAL, which incorporates three evaluation dimensions and is designed to\nassess the ability of models to generate educational questions. Drawing\ninspiration from teachers' problem design processes, we propose the Educational\nQuestion Planning with self-Reflection (EQPR) method for educational\nmathematical question generation, following a \"plan-evaluate-optimize\"\napproach. Specifically, by combining planning algorithm based on Monte Carlo\nTree Search with the generative capabilities of Large Language Models, we\ncontinuously optimize questions through iterative feedback. This\nself-optimization mechanism ensures that the generated questions both fit the\neducational context and strategically achieve specific basic educational\nobjectives. Through extensive experiments based on EQGEVAL, we have\ndemonstrated that EQPR achieves significant improvements in generating\nquestions that meet multi-dimensional educational objectives.", "AI": {"tldr": "\u63d0\u51faEQPR\u65b9\u6cd5\uff0c\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b26\u5408\u591a\u7ef4\u6559\u80b2\u76ee\u6807\u7684\u6570\u5b66\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5ffd\u89c6\u6559\u80b2\u76ee\u6807\uff0c\u4ec5\u751f\u6210\u7b80\u5355\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u4ee5\u6ee1\u8db3\u590d\u6742\u9700\u6c42\u3002", "method": "\u57fa\u4e8eEduMath\u6570\u636e\u96c6\uff0c\u5f00\u53d1EQGEVAL\u8bc4\u4f30\u6846\u67b6\uff0c\u91c7\u7528EQPR\uff08\u89c4\u5212-\u8bc4\u4f30-\u4f18\u5316\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4e0eLLM\u3002", "result": "EQPR\u5728\u591a\u7ef4\u6559\u80b2\u76ee\u6807\u95ee\u9898\u751f\u6210\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "EQPR\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u7b26\u5408\u6559\u80b2\u76ee\u6807\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u590d\u6742\u9700\u6c42\u3002", "keywords": "\u6570\u5b66\u95ee\u9898\u751f\u6210\u3001\u6559\u80b2\u76ee\u6807\u3001\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u3001LLM\u3001EQPR"}}
{"id": "2506.00081", "pdf": "https://arxiv.org/pdf/2506.00081", "abs": "https://arxiv.org/abs/2506.00081", "authors": ["Aditya Naik", "Jovi Thomas", "Teja Sree", "Himavant Reddy"], "title": "Artificial Empathy: AI based Mental Health", "categories": ["q-bio.OT", "cs.AI", "cs.HC"], "comment": null, "summary": "Many people suffer from mental health problems but not everyone seeks\nprofessional help or has access to mental health care. AI chatbots have\nincreasingly become a go-to for individuals who either have mental disorders or\nsimply want someone to talk to. This paper presents a study on participants who\nhave previously used chatbots and a scenario-based testing of large language\nmodel (LLM) chatbots. Our findings indicate that AI chatbots were primarily\nutilized as a \"Five minute therapist\" or as a non-judgmental companion.\nParticipants appreciated the anonymity and lack of judgment from chatbots.\nHowever, there were concerns about privacy and the security of sensitive\ninformation. The scenario-based testing of LLM chatbots highlighted additional\nissues. Some chatbots were consistently reassuring, used emojis and names to\nadd a personal touch, and were quick to suggest seeking professional help.\nHowever, there were limitations such as inconsistent tone, occasional\ninappropriate responses (e.g., casual or romantic), and a lack of crisis\nsensitivity, particularly in recognizing red flag language and escalating\nresponses appropriately. These findings can inform both the technology and\nmental health care industries on how to better utilize AI chatbots to support\nindividuals during challenging emotional periods.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u804a\u5929\u673a\u5668\u4eba\u5728\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5176\u4f5c\u4e3a\u201c\u4e94\u5206\u949f\u6cbb\u7597\u5e08\u201d\u6216\u975e\u8bc4\u5224\u6027\u4f34\u4fa3\u7684\u89d2\u8272\u53d7\u5230\u6b22\u8fce\uff0c\u4f46\u4e5f\u5b58\u5728\u9690\u79c1\u3001\u54cd\u5e94\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\u3002", "motivation": "\u7814\u7a76AI\u804a\u5929\u673a\u5668\u4eba\u5728\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u4ee5\u6539\u8fdb\u5176\u670d\u52a1\u5e76\u586b\u8865\u65e0\u6cd5\u83b7\u53d6\u4e13\u4e1a\u5e2e\u52a9\u7684\u4eba\u7fa4\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u7528\u6237\u8bbf\u8c08\u548c\u57fa\u4e8e\u573a\u666f\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u804a\u5929\u673a\u5668\u4eba\u6d4b\u8bd5\uff0c\u6536\u96c6\u6570\u636e\u5e76\u5206\u6790\u4f7f\u7528\u4f53\u9a8c\u3002", "result": "\u804a\u5929\u673a\u5668\u4eba\u56e0\u533f\u540d\u6027\u548c\u975e\u8bc4\u5224\u6027\u53d7\u5230\u6b22\u8fce\uff0c\u4f46\u5b58\u5728\u9690\u79c1\u4fdd\u62a4\u4e0d\u8db3\u3001\u54cd\u5e94\u4e0d\u4e00\u81f4\u3001\u5371\u673a\u654f\u611f\u5ea6\u4f4e\u7b49\u95ee\u9898\u3002", "conclusion": "AI\u804a\u5929\u673a\u5668\u4eba\u53ef\u4f5c\u4e3a\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u7684\u8865\u5145\u5de5\u5177\uff0c\u4f46\u9700\u6539\u8fdb\u9690\u79c1\u4fdd\u62a4\u3001\u54cd\u5e94\u4e00\u81f4\u6027\u548c\u5371\u673a\u5904\u7406\u80fd\u529b\u3002", "keywords": "AI\u804a\u5929\u673a\u5668\u4eba, \u5fc3\u7406\u5065\u5eb7\u652f\u6301, \u9690\u79c1\u95ee\u9898, \u54cd\u5e94\u4e00\u81f4\u6027, \u5371\u673a\u654f\u611f\u5ea6"}}
{"id": "2506.00936", "pdf": "https://arxiv.org/pdf/2506.00936", "abs": "https://arxiv.org/abs/2506.00936", "authors": ["Peijin Guo", "Minghui Li", "Hewen Pan", "Bowen Chen", "Yang Wu", "Zikang Guo", "Leo Yu Zhang", "Shengshan Hu", "Shengqing Hu"], "title": "Uncertainty-Aware Metabolic Stability Prediction with Dual-View Contrastive Learning", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": "This manuscript has been accepted for publication at ECML-PKDD 2025.\n  The final version will be published in the conference proceedings", "summary": "Accurate prediction of molecular metabolic stability (MS) is critical for\ndrug research and development but remains challenging due to the complex\ninterplay of molecular interactions. Despite recent advances in graph neural\nnetworks (GNNs) for MS prediction, current approaches face two critical\nlimitations: (1) incomplete molecular modeling due to atom-centric\nmessage-passing mechanisms that disregard bond-level topological features, and\n(2) prediction frameworks that lack reliable uncertainty quantification. To\naddress these challenges, we propose TrustworthyMS, a novel contrastive\nlearning framework designed for uncertainty-aware metabolic stability\nprediction. First, a molecular graph topology remapping mechanism synchronizes\natom-bond interactions through edge-induced feature propagation, capturing both\nlocalized electronic effects and global conformational constraints. Second,\ncontrastive topology-bond alignment enforces consistency between molecular\ntopology views and bond patterns via feature alignment, enhancing\nrepresentation robustness. Third, uncertainty modeling through Beta-Binomial\nuncertainty quantification enables simultaneous prediction and confidence\ncalibration under epistemic uncertainty. Through extensive experiments, our\nresults demonstrate that TrustworthyMS outperforms current state-of-the-art\nmethods in terms of predictive performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86TrustworthyMS\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6539\u8fdb\u5206\u5b50\u4ee3\u8c22\u7a33\u5b9a\u6027\u9884\u6d4b\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u5206\u5b50\u5efa\u6a21\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u4ee3\u8c22\u7a33\u5b9a\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u5206\u5b50\u56fe\u62d3\u6251\u91cd\u6620\u5c04\u673a\u5236\u3001\u5bf9\u6bd4\u62d3\u6251\u952e\u5bf9\u9f50\u548cBeta-Binomial\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6a21\u578b\u3002", "result": "TrustworthyMS\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5b50\u4ee3\u8c22\u7a33\u5b9a\u6027\u9884\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "keywords": "\u4ee3\u8c22\u7a33\u5b9a\u6027, \u56fe\u795e\u7ecf\u7f51\u7edc, \u5bf9\u6bd4\u5b66\u4e60, \u4e0d\u786e\u5b9a\u6027\u91cf\u5316, \u836f\u7269\u7814\u53d1"}}
{"id": "2506.00964", "pdf": "https://arxiv.org/pdf/2506.00964", "abs": "https://arxiv.org/abs/2506.00964", "authors": ["Dren Fazlija", "Arkadij Orlov", "Sandipan Sikdar"], "title": "ACCESS DENIED INC: The First Benchmark Environment for Sensitivity Awareness", "categories": ["cs.CL"], "comment": "20 pages, 4 figures, 8 tables, ACL 2025 (Findings)", "summary": "Large language models (LLMs) are increasingly becoming valuable to corporate\ndata management due to their ability to process text from various document\nformats and facilitate user interactions through natural language queries.\nHowever, LLMs must consider the sensitivity of information when communicating\nwith employees, especially given access restrictions. Simple filtering based on\nuser clearance levels can pose both performance and privacy challenges. To\naddress this, we propose the concept of sensitivity awareness (SA), which\nenables LLMs to adhere to predefined access rights rules. In addition, we\ndeveloped a benchmarking environment called ACCESS DENIED INC to evaluate SA.\nOur experimental findings reveal significant variations in model behavior,\nparticularly in managing unauthorized data requests while effectively\naddressing legitimate queries. This work establishes a foundation for\nbenchmarking sensitivity-aware language models and provides insights to enhance\nprivacy-centric AI systems in corporate environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u654f\u611f\u6027\u611f\u77e5\uff08SA\uff09\u7684\u6982\u5ff5\uff0c\u5e2e\u52a9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4f01\u4e1a\u6570\u636e\u7ba1\u7406\u4e2d\u9075\u5b88\u8bbf\u95ee\u6743\u9650\u89c4\u5219\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u73af\u5883\u6d4b\u8bd5\u5176\u6548\u679c\u3002", "motivation": "\u4f01\u4e1a\u73af\u5883\u4e2d\u4f7f\u7528LLMs\u5904\u7406\u654f\u611f\u4fe1\u606f\u65f6\uff0c\u7b80\u5355\u7684\u7528\u6237\u6743\u9650\u8fc7\u6ee4\u53ef\u80fd\u5e26\u6765\u6027\u80fd\u548c\u9690\u79c1\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u654f\u611f\u6027\u7ba1\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u654f\u611f\u6027\u611f\u77e5\uff08SA\uff09\u6982\u5ff5\uff0c\u5e76\u5f00\u53d1\u4e86ACCESS DENIED INC\u57fa\u51c6\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u6743\u9650\u9650\u5236\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5728\u5904\u7406\u672a\u6388\u6743\u6570\u636e\u8bf7\u6c42\u548c\u5408\u6cd5\u67e5\u8be2\u65f6\u884c\u4e3a\u5dee\u5f02\u663e\u8457\uff0cSA\u6709\u6548\u63d0\u5347\u4e86\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u654f\u611f\u6027\u611f\u77e5\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u6784\u5efa\u9690\u79c1\u53cb\u597d\u7684\u4f01\u4e1aAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u654f\u611f\u6027\u611f\u77e5, \u6570\u636e\u9690\u79c1, \u4f01\u4e1a\u6570\u636e\u7ba1\u7406, \u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2506.00083", "pdf": "https://arxiv.org/pdf/2506.00083", "abs": "https://arxiv.org/abs/2506.00083", "authors": ["Jiawei Hou", "Xiangyang Xue", "Taiping Zeng"], "title": "Hi-Dyna Graph: Hierarchical Dynamic Scene Graph for Robotic Autonomy in Human-Centric Environments", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Autonomous operation of service robotics in human-centric scenes remains\nchallenging due to the need for understanding of changing environments and\ncontext-aware decision-making. While existing approaches like topological maps\noffer efficient spatial priors, they fail to model transient object\nrelationships, whereas dense neural representations (e.g., NeRF) incur\nprohibitive computational costs. Inspired by the hierarchical scene\nrepresentation and video scene graph generation works, we propose Hi-Dyna\nGraph, a hierarchical dynamic scene graph architecture that integrates\npersistent global layouts with localized dynamic semantics for embodied robotic\nautonomy. Our framework constructs a global topological graph from posed RGB-D\ninputs, encoding room-scale connectivity and large static objects (e.g.,\nfurniture), while environmental and egocentric cameras populate dynamic\nsubgraphs with object position relations and human-object interaction patterns.\nA hybrid architecture is conducted by anchoring these subgraphs to the global\ntopology using semantic and spatial constraints, enabling seamless updates as\nthe environment evolves. An agent powered by large language models (LLMs) is\nemployed to interpret the unified graph, infer latent task triggers, and\ngenerate executable instructions grounded in robotic affordances. We conduct\ncomplex experiments to demonstrate Hi-Dyna Grap's superior scene representation\neffectiveness. Real-world deployments validate the system's practicality with a\nmobile manipulator: robotics autonomously complete complex tasks with no\nfurther training or complex rewarding in a dynamic scene as cafeteria\nassistant. See https://anonymous.4open.science/r/Hi-Dyna-Graph-B326 for video\ndemonstration and more details.", "AI": {"tldr": "Hi-Dyna Graph \u63d0\u51fa\u4e86\u4e00\u79cd\u5c42\u6b21\u5316\u52a8\u6001\u573a\u666f\u56fe\u67b6\u6784\uff0c\u7ed3\u5408\u5168\u5c40\u5e03\u5c40\u4e0e\u5c40\u90e8\u52a8\u6001\u8bed\u4e49\uff0c\u63d0\u5347\u670d\u52a1\u673a\u5668\u4eba\u5728\u52a8\u6001\u573a\u666f\u4e2d\u7684\u81ea\u4e3b\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u62d3\u6251\u5730\u56fe\uff09\u65e0\u6cd5\u5efa\u6a21\u77ac\u6001\u7269\u4f53\u5173\u7cfb\uff0c\u800c\u5bc6\u96c6\u795e\u7ecf\u8868\u793a\uff08\u5982NeRF\uff09\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6355\u6349\u5168\u5c40\u5e03\u5c40\u53c8\u80fd\u52a8\u6001\u66f4\u65b0\u5c40\u90e8\u8bed\u4e49\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5168\u5c40\u62d3\u6251\u56fe\u548c\u52a8\u6001\u5b50\u56fe\u7ed3\u5408\uff0c\u4f7f\u7528\u8bed\u4e49\u548c\u7a7a\u95f4\u7ea6\u675f\u951a\u5b9a\u52a8\u6001\u5b50\u56fe\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89e3\u6790\u573a\u666f\u56fe\u548c\u751f\u6210\u6307\u4ee4\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e Hi-Dyna Graph \u5728\u573a\u666f\u8868\u793a\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5b9e\u9645\u90e8\u7f72\u4e2d\u673a\u5668\u4eba\u80fd\u5728\u52a8\u6001\u573a\u666f\uff08\u5982\u81ea\u52a9\u9910\u5385\uff09\u4e2d\u81ea\u4e3b\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u3002", "conclusion": "Hi-Dyna Graph \u4e3a\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u64cd\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u670d\u52a1\u673a\u5668\u4eba,\u52a8\u6001\u573a\u666f\u56fe,\u5c42\u6b21\u5316\u8868\u793a,\u5927\u8bed\u8a00\u6a21\u578b,\u81ea\u4e3b\u6027"}}
{"id": "2506.00959", "pdf": "https://arxiv.org/pdf/2506.00959", "abs": "https://arxiv.org/abs/2506.00959", "authors": ["Xiaohan Wang", "Yu Zhang", "Guibin Jiang", "Bing Cheng", "Wei Lin"], "title": "Hidden Representation Clustering with Multi-Task Representation Learning towards Robust Online Budget Allocation", "categories": ["cs.LG"], "comment": null, "summary": "Marketing optimization, commonly formulated as an online budget allocation\nproblem, has emerged as a pivotal factor in driving user growth. Most existing\nresearch addresses this problem by following the principle of 'first predict\nthen optimize' for each individual, which presents challenges related to\nlarge-scale counterfactual prediction and solving complexity trade-offs. Note\nthat the practical data quality is uncontrollable, and the solving scale tends\nto be tens of millions. Therefore, the existing approaches make the robust\nbudget allocation non-trivial, especially in industrial scenarios with\nconsiderable data noise. To this end, this paper proposes a novel approach that\nsolves the problem from the cluster perspective. Specifically, we propose a\nmulti-task representation network to learn the inherent attributes of\nindividuals and project the original features into high-dimension hidden\nrepresentations through the first two layers of the trained network. Then, we\ndivide these hidden representations into $K$ groups through partitioning-based\nclustering, thus reformulating the problem as an integer stochastic programming\nproblem under different total budgets. Finally, we distill the representation\nmodule and clustering model into a multi-category model to facilitate online\ndeployment. Offline experiments validate the effectiveness and superiority of\nour approach compared to six state-of-the-art marketing optimization\nalgorithms. Online A/B tests on the Meituan platform indicate that the approach\noutperforms the online algorithm by 0.53% and 0.65%, considering order volume\n(OV) and gross merchandise volume (GMV), respectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u8425\u9500\u9884\u7b97\u5206\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u8868\u793a\u7f51\u7edc\u5b66\u4e60\u4e2a\u4f53\u5c5e\u6027\u5e76\u8fdb\u884c\u805a\u7c7b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0b\u9762\u4e34\u7684\u9884\u6d4b\u548c\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u8425\u9500\u4f18\u5316\u4e2d\u5927\u89c4\u6a21\u6570\u636e\u4e0b\u7684\u9884\u6d4b\u548c\u4f18\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u8d28\u91cf\u4e0d\u53ef\u63a7\u4e14\u566a\u58f0\u8f83\u5927\u7684\u5de5\u4e1a\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51fa\u591a\u4efb\u52a1\u8868\u793a\u7f51\u7edc\u5b66\u4e60\u4e2a\u4f53\u5c5e\u6027\uff0c\u901a\u8fc7\u805a\u7c7b\u5c06\u539f\u59cb\u7279\u5f81\u5206\u4e3aK\u7ec4\uff0c\u5e76\u8f6c\u5316\u4e3a\u6574\u6570\u968f\u673a\u89c4\u5212\u95ee\u9898\u3002\u8fdb\u4e00\u6b65\u5c06\u6a21\u5757\u84b8\u998f\u4e3a\u591a\u7c7b\u522b\u6a21\u578b\u4ee5\u4fbf\u5728\u7ebf\u90e8\u7f72\u3002", "result": "\u79bb\u7ebf\u5b9e\u9a8c\u8868\u660e\u4f18\u4e8e\u516d\u79cd\u5148\u8fdb\u65b9\u6cd5\uff1b\u5728\u7ebfA/B\u6d4b\u8bd5\u5728\u8ba2\u5355\u91cf\u548cGMV\u4e0a\u5206\u522b\u63d0\u53470.53%\u548c0.65%\u3002", "conclusion": "\u57fa\u4e8e\u805a\u7c7b\u7684\u65b9\u6cd5\u5728\u8425\u9500\u9884\u7b97\u5206\u914d\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u573a\u666f\u3002", "keywords": "\u8425\u9500\u4f18\u5316, \u9884\u7b97\u5206\u914d, \u591a\u4efb\u52a1\u8868\u793a\u7f51\u7edc, \u805a\u7c7b, \u6574\u6570\u968f\u673a\u89c4\u5212"}}
{"id": "2506.00973", "pdf": "https://arxiv.org/pdf/2506.00973", "abs": "https://arxiv.org/abs/2506.00973", "authors": ["Vadivel Abishethvarman", "Bhavik Chandna", "Pratik Jalan", "Usman Naseem"], "title": "XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Large Language Models (LLMs) can generate content spanning ideological\nrhetoric to explicit instructions for violence. However, existing safety\nevaluations often rely on simplistic binary labels (safe and unsafe),\noverlooking the nuanced spectrum of risk these outputs pose. To address this,\nwe present XGUARD, a benchmark and evaluation framework designed to assess the\nseverity of extremist content generated by LLMs. XGUARD includes 3,840 red\nteaming prompts sourced from real world data such as social media and news,\ncovering a broad range of ideologically charged scenarios. Our framework\ncategorizes model responses into five danger levels (0 to 4), enabling a more\nnuanced analysis of both the frequency and severity of failures. We introduce\nthe interpretable Attack Severity Curve (ASC) to visualize vulnerabilities and\ncompare defense mechanisms across threat intensities. Using XGUARD, we evaluate\nsix popular LLMs and two lightweight defense strategies, revealing key insights\ninto current safety gaps and trade-offs between robustness and expressive\nfreedom. Our work underscores the value of graded safety metrics for building\ntrustworthy LLMs.", "AI": {"tldr": "XGUARD\u662f\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u7ea7\u6d4b\u8bd5LLM\u751f\u6210\u6781\u7aef\u5185\u5bb9\u7684\u4e25\u91cd\u6027\uff0c\u901a\u8fc7\u4e94\u7ea7\u5371\u9669\u5206\u7c7b\u548cASC\u66f2\u7ebf\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u7684\u5b89\u5168\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u7684LLM\u5b89\u5168\u8bc4\u4f30\u8fc7\u4e8e\u7b80\u5355\uff08\u4ec5\u4e8c\u5206\u7c7b\uff09\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u98ce\u9669\u7684\u591a\u6837\u6027\u3002XGUARD\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b3,840\u6761\u771f\u5b9e\u7ea2\u961f\u63d0\u793a\u7684\u57fa\u51c6\uff0c\u5c06\u6a21\u578b\u54cd\u5e94\u5206\u4e3a5\u7ea7\u5371\u9669\uff0c\u5e76\u5f15\u5165ASC\u66f2\u7ebf\u53ef\u89c6\u5316\u9632\u5fa1\u6548\u679c\u3002", "result": "\u8bc4\u4f30\u4e866\u4e2a\u4e3b\u6d41LLM\u548c2\u79cd\u9632\u5fa1\u7b56\u7565\uff0c\u63ed\u793a\u4e86\u5b89\u5168\u6027\u5dee\u8ddd\u53ca\u9c81\u68d2\u6027\u4e0e\u8868\u8fbe\u81ea\u7531\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u5206\u7ea7\u5b89\u5168\u6307\u6807\u5bf9\u6784\u5efa\u53ef\u4fe1LLM\u81f3\u5173\u91cd\u8981\uff0cXGUARD\u4e3a\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "keywords": "LLM\u5b89\u5168\u3001\u6781\u7aef\u5185\u5bb9\u3001\u7ea2\u961f\u6d4b\u8bd5\u3001\u5206\u7ea7\u8bc4\u4f30\u3001\u9632\u5fa1\u7b56\u7565"}}
{"id": "2506.00961", "pdf": "https://arxiv.org/pdf/2506.00961", "abs": "https://arxiv.org/abs/2506.00961", "authors": ["Ofri Eisen", "Ron Dorfman", "Kfir Y. Levy"], "title": "Enhancing Parallelism in Decentralized Stochastic Convex Optimization", "categories": ["cs.LG", "stat.ML"], "comment": "ICML 2025", "summary": "Decentralized learning has emerged as a powerful approach for handling large\ndatasets across multiple machines in a communication-efficient manner. However,\nsuch methods often face scalability limitations, as increasing the number of\nmachines beyond a certain point negatively impacts convergence rates. In this\nwork, we propose Decentralized Anytime SGD, a novel decentralized learning\nalgorithm that significantly extends the critical parallelism threshold,\nenabling the effective use of more machines without compromising performance.\nWithin the stochastic convex optimization (SCO) framework, we establish a\ntheoretical upper bound on parallelism that surpasses the current\nstate-of-the-art, allowing larger networks to achieve favorable statistical\nguarantees and closing the gap with centralized learning in highly connected\ntopologies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDecentralized Anytime SGD\u7684\u65b0\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7684\u5e76\u884c\u80fd\u529b\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u673a\u5668\u6570\u91cf\u589e\u52a0\u65f6\u6536\u655b\u901f\u5ea6\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u65f6\u5177\u6709\u901a\u4fe1\u6548\u7387\u9ad8\u7684\u4f18\u52bf\uff0c\u4f46\u968f\u7740\u673a\u5668\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5176\u6536\u655b\u901f\u5ea6\u4f1a\u53d7\u5230\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u5728\u968f\u673a\u51f8\u4f18\u5316\uff08SCO\uff09\u6846\u67b6\u4e0b\uff0c\u63d0\u51faDecentralized Anytime SGD\u7b97\u6cd5\uff0c\u6269\u5c55\u4e86\u5e76\u884c\u6027\u9608\u503c\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u7a81\u7834\u4e86\u73b0\u6709\u6280\u672f\u7684\u5e76\u884c\u6027\u4e0a\u9650\uff0c\u4f7f\u5f97\u66f4\u5927\u89c4\u6a21\u7684\u7f51\u7edc\u80fd\u591f\u5b9e\u73b0\u826f\u597d\u7684\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u5e76\u5728\u9ad8\u5ea6\u8fde\u63a5\u7684\u62d3\u6251\u4e2d\u7f29\u5c0f\u4e86\u4e0e\u4e2d\u5fc3\u5316\u5b66\u4e60\u7684\u5dee\u8ddd\u3002", "conclusion": "Decentralized Anytime SGD\u663e\u8457\u63d0\u5347\u4e86\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60,\u5e76\u884c\u6027,\u968f\u673a\u68af\u5ea6\u4e0b\u964d,\u51f8\u4f18\u5316,\u5206\u5e03\u5f0f\u5b66\u4e60"}}
{"id": "2506.00975", "pdf": "https://arxiv.org/pdf/2506.00975", "abs": "https://arxiv.org/abs/2506.00975", "authors": ["Qichao Wang", "Ziqiao Meng", "Wenqian Cui", "Yifei Zhang", "Pengcheng Wu", "Bingzhe Wu", "Irwin King", "Liang Chen", "Peilin Zhao"], "title": "NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Inspired by the impressive capabilities of GPT-4o, there is growing interest\nin enabling speech language models (SLMs) to engage in natural, fluid spoken\ninteractions with humans. Recent advancements have led to the development of\nseveral SLMs that demonstrate promising results in this area. However, current\napproaches have yet to fully exploit dual-channel speech data, which inherently\ncaptures the structure and dynamics of human conversation. In this work, we\nsystematically explore the use of dual-channel speech data in the context of\nmodern large language models, and introduce a novel generative modeling\nparadigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent\ndual-channel spoken dialogue learning using decoder-only architectures for the\nfirst time. We evaluate our approach on standard benchmarks, and empirical\nresults show that our proposed method, NTPP, significantly improves the\nconversational abilities of SLMs in terms of turn-taking prediction, response\ncoherence, and naturalness. Moreover, compared to existing methods, NTPP\nachieves substantially lower inference latency, highlighting its practical\nefficiency for real-time applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6a21\u578b\u8303\u5f0fNTPP\uff0c\u9996\u6b21\u5229\u7528\u53cc\u901a\u9053\u8bed\u97f3\u6570\u636e\u63d0\u5347\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u80fd\u529b\uff0c\u663e\u8457\u6539\u5584\u4e86\u5bf9\u8bdd\u7684\u81ea\u7136\u6027\u548c\u5ef6\u8fdf\u95ee\u9898\u3002", "motivation": "\u53d7GPT-4o\u542f\u53d1\uff0c\u5e0c\u671b\u63d0\u5347\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u53cc\u901a\u9053\u8bed\u97f3\u6570\u636e\u3002", "method": "\u63d0\u51faNTPP\uff08Next-Token-Pair Prediction\uff09\u6a21\u578b\uff0c\u5229\u7528\u53cc\u901a\u9053\u8bed\u97f3\u6570\u636e\u8bad\u7ec3\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5b9e\u73b0\u8bf4\u8bdd\u8005\u65e0\u5173\u7684\u53cc\u901a\u9053\u5bf9\u8bdd\u5b66\u4e60\u3002", "result": "NTPP\u5728\u5bf9\u8bdd\u80fd\u529b\uff08\u5982\u8f6e\u8f6c\u9884\u6d4b\u3001\u54cd\u5e94\u8fde\u8d2f\u6027\u3001\u81ea\u7136\u6027\uff09\u548c\u63a8\u7406\u5ef6\u8fdf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "NTPP\u4e3a\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u81ea\u7136\u7684\u5bf9\u8bdd\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u53cc\u901a\u9053\u8bed\u97f3\u6570\u636e\uff0cNTPP\uff0c\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff0c\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u5b9e\u65f6\u5e94\u7528"}}
{"id": "2506.00962", "pdf": "https://arxiv.org/pdf/2506.00962", "abs": "https://arxiv.org/abs/2506.00962", "authors": ["Enric Ribera Borrell", "Lorenz Richter", "Christof Sch\u00fctte"], "title": "Reinforcement Learning with Random Time Horizons", "categories": ["cs.LG", "math.OC", "math.PR", "stat.ML"], "comment": null, "summary": "We extend the standard reinforcement learning framework to random time\nhorizons. While the classical setting typically assumes finite and\ndeterministic or infinite runtimes of trajectories, we argue that multiple\nreal-world applications naturally exhibit random (potentially\ntrajectory-dependent) stopping times. Since those stopping times typically\ndepend on the policy, their randomness has an effect on policy gradient\nformulas, which we (mostly for the first time) derive rigorously in this work\nboth for stochastic and deterministic policies. We present two complementary\nperspectives, trajectory or state-space based, and establish connections to\noptimal control theory. Our numerical experiments demonstrate that using the\nproposed formulas can significantly improve optimization convergence compared\nto traditional approaches.", "AI": {"tldr": "\u8bba\u6587\u5c06\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6269\u5c55\u5230\u968f\u673a\u65f6\u95f4\u8303\u56f4\uff0c\u5206\u6790\u4e86\u968f\u673a\u505c\u6b62\u65f6\u95f4\u5bf9\u7b56\u7565\u68af\u5ea6\u516c\u5f0f\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u8865\u5145\u89c6\u89d2\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u4f18\u5316\u6536\u655b\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u8bb8\u591a\u5e94\u7528\u5177\u6709\u968f\u673a\u505c\u6b62\u65f6\u95f4\uff0c\u800c\u7ecf\u5178\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u672a\u8003\u8651\u8fd9\u79cd\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u6269\u5c55\u6846\u67b6\u4ee5\u9002\u5e94\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u91c7\u7528\u8f68\u8ff9\u548c\u72b6\u6001\u7a7a\u95f4\u4e24\u79cd\u4e92\u8865\u89c6\u89d2\uff0c\u63a8\u5bfc\u968f\u673a\u548c\u786e\u5b9a\u6027\u7b56\u7565\u4e0b\u7684\u7b56\u7565\u68af\u5ea6\u516c\u5f0f\uff0c\u5e76\u4e0e\u6700\u4f18\u63a7\u5236\u7406\u8bba\u5efa\u7acb\u8054\u7cfb\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63d0\u51fa\u7684\u7b56\u7565\u68af\u5ea6\u516c\u5f0f\u80fd\u663e\u8457\u63d0\u5347\u4f18\u5316\u6536\u655b\u6027\u3002", "conclusion": "\u968f\u673a\u65f6\u95f4\u8303\u56f4\u7684\u6269\u5c55\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u66f4\u591a\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u7b56\u7565\u68af\u5ea6, \u968f\u673a\u505c\u6b62\u65f6\u95f4, \u6700\u4f18\u63a7\u5236"}}
{"id": "2506.00980", "pdf": "https://arxiv.org/pdf/2506.00980", "abs": "https://arxiv.org/abs/2506.00980", "authors": ["Sina J. Semnani", "Pingyue Zhang", "Wanyue Zhai", "Haozhuo Li", "Ryan Beauchamp", "Trey Billing", "Katayoun Kishi", "Manling Li", "Monica S. Lam"], "title": "LEMONADE: A Large Multilingual Expert-Annotated Abstractive Event Dataset for the Real World", "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "This paper presents LEMONADE, a large-scale conflict event dataset comprising\n39,786 events across 20 languages and 171 countries, with extensive coverage of\nregion-specific entities. LEMONADE is based on a partially reannotated subset\nof the Armed Conflict Location & Event Data (ACLED), which has documented\nglobal conflict events for over a decade.\n  To address the challenge of aggregating multilingual sources for global event\nanalysis, we introduce abstractive event extraction (AEE) and its subtask,\nabstractive entity linking (AEL). Unlike conventional span-based event\nextraction, our approach detects event arguments and entities through holistic\ndocument understanding and normalizes them across the multilingual dataset. We\nevaluate various large language models (LLMs) on these tasks, adapt existing\nzero-shot event extraction systems, and benchmark supervised models.\nAdditionally, we introduce ZEST, a novel zero-shot retrieval-based system for\nAEL.\n  Our best zero-shot system achieves an end-to-end F1 score of 58.3%, with LLMs\noutperforming specialized event extraction models such as GoLLIE. For entity\nlinking, ZEST achieves an F1 score of 45.7%, significantly surpassing OneNet, a\nstate-of-the-art zero-shot baseline that achieves only 23.7%. However, these\nzero-shot results lag behind the best supervised systems by 20.1% and 37.0% in\nthe end-to-end and AEL tasks, respectively, highlighting the need for further\nresearch.", "AI": {"tldr": "LEMONADE\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u51b2\u7a81\u4e8b\u4ef6\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u62bd\u8c61\u4e8b\u4ef6\u63d0\u53d6\uff08AEE\uff09\u548c\u5b9e\u4f53\u94fe\u63a5\uff08AEL\uff09\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u6765\u6e90\u6570\u636e\u805a\u5408\u7684\u6311\u6218\uff0c\u63d0\u51faAEE\u548cAEL\u4ee5\u6539\u8fdb\u4f20\u7edf\u4e8b\u4ef6\u63d0\u53d6\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528LLMs\u8fdb\u884c\u4efb\u52a1\u8bc4\u4f30\uff0c\u63d0\u51fa\u96f6-shot\u68c0\u7d22\u7cfb\u7edfZEST\uff0c\u5e76\u4e0e\u76d1\u7763\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u96f6-shot\u7cfb\u7edf\u5728\u7aef\u5230\u7aef\u4efb\u52a1\u4e2dF1\u5f97\u5206\u4e3a58.3%\uff0c\u4f18\u4e8e\u4e13\u4e1a\u6a21\u578bGoLLIE\uff1bZEST\u5728AEL\u4efb\u52a1\u4e2dF1\u5f97\u520645.7%\uff0c\u4f18\u4e8e\u57fa\u7ebfOneNet\u3002", "conclusion": "\u96f6-shot\u7cfb\u7edf\u4ecd\u6709\u5dee\u8ddd\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u5347\u3002", "keywords": "LEMONADE, AEE, AEL, LLMs, ZEST"}}
{"id": "2506.00967", "pdf": "https://arxiv.org/pdf/2506.00967", "abs": "https://arxiv.org/abs/2506.00967", "authors": ["Tingting Zhang", "Sergiy A. Vorobyov", "David J. Love", "Taejoon Kim", "Kai Dong"], "title": "Pilot Contamination-Aware Graph Attention Network for Power Control in CFmMIMO", "categories": ["cs.LG"], "comment": null, "summary": "Optimization-based power control algorithms are predominantly iterative with\nhigh computational complexity, making them impractical for real-time\napplications in cell-free massive multiple-input multiple-output (CFmMIMO)\nsystems. Learning-based methods have emerged as a promising alternative, and\namong them, graph neural networks (GNNs) have demonstrated their excellent\nperformance in solving power control problems. However, all existing GNN-based\napproaches assume ideal orthogonality among pilot sequences for user equipments\n(UEs), which is unrealistic given that the number of UEs exceeds the available\northogonal pilot sequences in CFmMIMO schemes. Moreover, most learning-based\nmethods assume a fixed number of UEs, whereas the number of active UEs varies\nover time in practice. Additionally, supervised training necessitates costly\ncomputational resources for computing the target power control solutions for a\nlarge volume of training samples. To address these issues, we propose a graph\nattention network for downlink power control in CFmMIMO systems that operates\nin a self-supervised manner while effectively handling pilot contamination and\nadapting to a dynamic number of UEs. Experimental results show its\neffectiveness, even in comparison to the optimal accelerated projected gradient\nmethod as a baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7684\u81ea\u76d1\u7763\u4e0b\u884c\u529f\u7387\u63a7\u5236\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u6b63\u4ea4\u6027\u5047\u8bbe\u4e0d\u73b0\u5b9e\u3001\u7528\u6237\u8bbe\u5907\u6570\u91cf\u52a8\u6001\u53d8\u5316\u4ee5\u53ca\u76d1\u7763\u8bad\u7ec3\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4f18\u5316\u7684\u529f\u7387\u63a7\u5236\u7b97\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\uff1b\u800c\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\u5047\u8bbe\u8fc7\u4e8e\u7406\u60f3\u5316\uff0c\u4e14\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u7528\u6237\u8bbe\u5907\u6570\u91cf\u3002", "method": "\u91c7\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GNN\uff09\uff0c\u4ee5\u81ea\u76d1\u7763\u65b9\u5f0f\u8fd0\u884c\uff0c\u6709\u6548\u5904\u7406\u5bfc\u9891\u6c61\u67d3\u5e76\u9002\u5e94\u52a8\u6001\u7528\u6237\u8bbe\u5907\u6570\u91cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u751a\u81f3\u4f18\u4e8e\u4f5c\u4e3a\u57fa\u7ebf\u7684\u52a0\u901f\u6295\u5f71\u68af\u5ea6\u6cd5\u3002", "conclusion": "\u81ea\u76d1\u7763\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u4e3aCFmMIMO\u7cfb\u7edf\u4e2d\u7684\u529f\u7387\u63a7\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u529f\u7387\u63a7\u5236\u3001\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u3001CFmMIMO\u3001\u5bfc\u9891\u6c61\u67d3"}}
{"id": "2506.00981", "pdf": "https://arxiv.org/pdf/2506.00981", "abs": "https://arxiv.org/abs/2506.00981", "authors": ["Marianne de Heer Kloots", "Hosein Mohebbi", "Charlotte Pouw", "Gaofei Shen", "Willem Zuidema", "Martijn Bentum"], "title": "What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025. For model, code, and materials, see\n  https://github.com/mdhk/SSL-NL-eval", "summary": "How language-specific are speech representations learned by self-supervised\nmodels? Existing work has shown that a range of linguistic features can be\nsuccessfully decoded from end-to-end models trained only on speech recordings.\nHowever, it's less clear to what extent pre-training on specific languages\nimproves language-specific linguistic information. Here we test the encoding of\nDutch phonetic and lexical information in internal representations of\nself-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the\nrepresentation of Dutch linguistic features as compared to pre-training on\nsimilar amounts of English or larger amounts of multilingual data. This\nlanguage-specific advantage is well-detected by trained clustering or\nclassification probes, and partially observable using zero-shot metrics.\nFurthermore, the language-specific benefit on linguistic feature encoding\naligns with downstream performance on Automatic Speech Recognition.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578bWav2Vec2\u4e2d\u8377\u5170\u8bed\u8bed\u97f3\u548c\u8bcd\u6c47\u4fe1\u606f\u7684\u8868\u793a\u6548\u679c\uff0c\u53d1\u73b0\u4ec5\u7528\u8377\u5170\u8bed\u9884\u8bad\u7ec3\u6bd4\u7528\u82f1\u8bed\u6216\u591a\u8bed\u8a00\u6570\u636e\u8bad\u7ec3\u66f4\u80fd\u63d0\u5347\u8377\u5170\u8bed\u7279\u5f81\u7684\u8bed\u8a00\u7279\u5f02\u6027\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u81ea\u76d1\u7763\u6a21\u578b\u7684\u8bed\u8a00\u7279\u5f02\u6027\u8868\u793a\uff0c\u5c24\u5176\u662f\u8377\u5170\u8bed\u7684\u7279\u5f81\u5728\u4e0d\u540c\u9884\u8bad\u7ec3\u6570\u636e\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u6d4b\u8bd5Wav2Vec2\u6a21\u578b\u5728\u8377\u5170\u8bed\u8bed\u97f3\u548c\u8bcd\u6c47\u4fe1\u606f\u4e0a\u7684\u7f16\u7801\u80fd\u529b\uff0c\u4f7f\u7528\u5206\u7c7b\u6216\u805a\u7c7b\u63a2\u9488\u53ca\u96f6\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8377\u5170\u8bed\u9884\u8bad\u7ec3\u5728\u8bed\u8a00\u7279\u5f81\u7f16\u7801\u4e0a\u4f18\u4e8e\u82f1\u8bed\u6216\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\uff0c\u4e14\u4e0e\u4e0b\u6e38\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u76f8\u5173\u3002", "conclusion": "\u8bed\u8a00\u7279\u5f02\u6027\u9884\u8bad\u7ec3\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u76ee\u6807\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\u3002", "keywords": "\u81ea\u76d1\u7763\u5b66\u4e60, Wav2Vec2, \u8bed\u8a00\u7279\u5f02\u6027, \u8377\u5170\u8bed, \u8bed\u97f3\u8bc6\u522b"}}
{"id": "2506.00089", "pdf": "https://arxiv.org/pdf/2506.00089", "abs": "https://arxiv.org/abs/2506.00089", "authors": ["Hyundong Jin", "Sicheol Sung", "Shinwoo Park", "SeungYeop Baik", "Yo-Sub Han"], "title": "TRAPDOC: Deceiving LLM Users by Injecting Imperceptible Phantom Tokens into Documents", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The reasoning, writing, text-editing, and retrieval capabilities of\nproprietary large language models (LLMs) have advanced rapidly, providing users\nwith an ever-expanding set of functionalities. However, this growing utility\nhas also led to a serious societal concern: the over-reliance on LLMs. In\nparticular, users increasingly delegate tasks such as homework, assignments, or\nthe processing of sensitive documents to LLMs without meaningful engagement.\nThis form of over-reliance and misuse is emerging as a significant social\nissue. In order to mitigate these issues, we propose a method injecting\nimperceptible phantom tokens into documents, which causes LLMs to generate\noutputs that appear plausible to users but are in fact incorrect. Based on this\ntechnique, we introduce TRAPDOC, a framework designed to deceive over-reliant\nLLM users. Through empirical evaluation, we demonstrate the effectiveness of\nour framework on proprietary LLMs, comparing its impact against several\nbaselines. TRAPDOC serves as a strong foundation for promoting more responsible\nand thoughtful engagement with language models. Our code is available at\nhttps://github.com/jindong22/TrapDoc.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5728\u6587\u6863\u4e2d\u6ce8\u5165\u4e0d\u53ef\u5bdf\u89c9\u7684\u201c\u5e7b\u5f71\u6807\u8bb0\u201d\u6765\u6b3a\u9a97\u8fc7\u5ea6\u4f9d\u8d56\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7528\u6237\u7684\u65b9\u6cd5\uff0c\u5e76\u4ecb\u7ecd\u4e86TRAPDOC\u6846\u67b6\u3002\u8be5\u6846\u67b6\u80fd\u5f15\u5bfcLLM\u751f\u6210\u770b\u4f3c\u5408\u7406\u5b9e\u5219\u9519\u8bef\u7684\u8f93\u51fa\uff0c\u65e8\u5728\u51cf\u5c11\u7528\u6237\u5bf9LLM\u7684\u6ee5\u7528\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u529f\u80fd\u7684\u6269\u5c55\uff0c\u7528\u6237\u5bf9\u5176\u8fc7\u5ea6\u4f9d\u8d56\u65e5\u76ca\u4e25\u91cd\uff0c\u5c24\u5176\u662f\u5728\u4f5c\u4e1a\u3001\u654f\u611f\u6587\u6863\u5904\u7406\u7b49\u65b9\u9762\u3002\u8fd9\u79cd\u4f9d\u8d56\u53ef\u80fd\u5bfc\u81f4\u793e\u4f1a\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u9650\u5236\u6ee5\u7528\u3002", "method": "\u901a\u8fc7\u5728\u6587\u6863\u4e2d\u6ce8\u5165\u4e0d\u53ef\u5bdf\u89c9\u7684\u5e7b\u5f71\u6807\u8bb0\uff0c\u4f7f\u5f97LLM\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u5b9e\u9645\u9519\u8bef\u7684\u8f93\u51fa\uff0c\u4ece\u800c\u6b3a\u9a97\u8fc7\u5ea6\u4f9d\u8d56\u7684\u7528\u6237\u3002", "result": "TRAPDOC\u6846\u67b6\u5728\u5b9e\u9a8c\u4e2d\u663e\u793a\u4e86\u5bf9\u4e13\u6709LLM\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u4e0e\u57fa\u7ebf\u7684\u6bd4\u8f83\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "conclusion": "TRAPDOC\u4e3a\u4fc3\u8fdb\u66f4\u8d1f\u8d23\u4efb\u548c\u5ba1\u614e\u7684\u8bed\u8a00\u6a21\u578b\u4f7f\u7528\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u6ee5\u7528\u3001\u5e7b\u5f71\u6807\u8bb0\u3001TRAPDOC\u6846\u67b6\u3001\u793e\u4f1a\u8d23\u4efb"}}
{"id": "2506.00969", "pdf": "https://arxiv.org/pdf/2506.00969", "abs": "https://arxiv.org/abs/2506.00969", "authors": ["Jiashuo Liu", "Peng Cui"], "title": "Data Heterogeneity Modeling for Trustworthy Machine Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Survey paper for tutorial \"Data Heterogeneity Modeling for\n  Trustworthy Machine Learning\" in KDD'25", "summary": "Data heterogeneity plays a pivotal role in determining the performance of\nmachine learning (ML) systems. Traditional algorithms, which are typically\ndesigned to optimize average performance, often overlook the intrinsic\ndiversity within datasets. This oversight can lead to a myriad of issues,\nincluding unreliable decision-making, inadequate generalization across\ndifferent domains, unfair outcomes, and false scientific inferences. Hence, a\nnuanced approach to modeling data heterogeneity is essential for the\ndevelopment of dependable, data-driven systems. In this survey paper, we\npresent a thorough exploration of heterogeneity-aware machine learning, a\nparadigm that systematically integrates considerations of data heterogeneity\nthroughout the entire ML pipeline -- from data collection and model training to\nmodel evaluation and deployment. By applying this approach to a variety of\ncritical fields, including healthcare, agriculture, finance, and recommendation\nsystems, we demonstrate the substantial benefits and potential of\nheterogeneity-aware ML. These applications underscore how a deeper\nunderstanding of data diversity can enhance model robustness, fairness, and\nreliability and help model diagnosis and improvements. Moreover, we delve into\nfuture directions and provide research opportunities for the whole data mining\ncommunity, aiming to promote the development of heterogeneity-aware ML.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5f02\u8d28\u6027\u611f\u77e5\u673a\u5668\u5b66\u4e60\u7684\u91cd\u8981\u6027\uff0c\u5f3a\u8c03\u5176\u5728\u591a\u4e2a\u5173\u952e\u9886\u57df\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u901a\u5e38\u5ffd\u7565\u6570\u636e\u5f02\u8d28\u6027\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9760\u7684\u51b3\u7b56\u548c\u6cdb\u5316\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7ec6\u81f4\u7684\u65b9\u6cd5\u6765\u5efa\u6a21\u5f02\u8d28\u6027\u3002", "method": "\u901a\u8fc7\u5728\u6574\u4e2a\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\uff08\u4ece\u6570\u636e\u6536\u96c6\u5230\u6a21\u578b\u90e8\u7f72\uff09\u4e2d\u7cfb\u7edf\u6574\u5408\u6570\u636e\u5f02\u8d28\u6027\u8003\u91cf\uff0c\u63d0\u51fa\u5f02\u8d28\u6027\u611f\u77e5\u673a\u5668\u5b66\u4e60\u8303\u5f0f\u3002", "result": "\u5f02\u8d28\u6027\u611f\u77e5\u673a\u5668\u5b66\u4e60\u5728\u533b\u7597\u3001\u519c\u4e1a\u3001\u91d1\u878d\u548c\u63a8\u8350\u7cfb\u7edf\u7b49\u9886\u57df\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6a21\u578b\u9c81\u68d2\u6027\u3001\u516c\u5e73\u6027\u548c\u53ef\u9760\u6027\u63d0\u5347\u3002", "conclusion": "\u6df1\u5165\u7814\u7a76\u6570\u636e\u591a\u6837\u6027\u80fd\u6539\u5584\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4e3a\u6570\u636e\u6316\u6398\u793e\u533a\u63d0\u4f9b\u672a\u6765\u7814\u7a76\u673a\u4f1a\u3002", "keywords": "\u6570\u636e\u5f02\u8d28\u6027, \u673a\u5668\u5b66\u4e60, \u6a21\u578b\u9c81\u68d2\u6027, \u516c\u5e73\u6027, \u6570\u636e\u591a\u6837\u6027"}}
{"id": "2506.00985", "pdf": "https://arxiv.org/pdf/2506.00985", "abs": "https://arxiv.org/abs/2506.00985", "authors": ["Valeriya Goloviznina", "Alexander Sergeev", "Mikhail Melnichenko", "Evgeny Kotelnikov"], "title": "Do LLMs Understand Why We Write Diaries? A Method for Purpose Extraction and Clustering", "categories": ["cs.CL"], "comment": "Accepted for CompLing-2025 conference", "summary": "Diary analysis presents challenges, particularly in extracting meaningful\ninformation from large corpora, where traditional methods often fail to deliver\nsatisfactory results. This study introduces a novel method based on Large\nLanguage Models (LLMs) to identify and cluster the various purposes of diary\nwriting. By \"purposes,\" we refer to the intentions behind diary writing, such\nas documenting life events, self-reflection, or practicing language skills. Our\napproach is applied to Soviet-era diaries (1922-1929) from the Prozhito digital\narchive, a rich collection of personal narratives. We evaluate different\nproprietary and open-source LLMs, finding that GPT-4o and o1-mini achieve the\nbest performance, while a template-based baseline is significantly less\neffective. Additionally, we analyze the retrieved purposes based on gender, age\nof the authors, and the year of writing. Furthermore, we examine the types of\nerrors made by the models, providing a deeper understanding of their\nlimitations and potential areas for improvement in future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u65e5\u8bb0\u4e2d\u8bc6\u522b\u548c\u805a\u7c7b\u5199\u4f5c\u76ee\u7684\uff0c\u5e76\u5728\u82cf\u8054\u65f6\u671f\u65e5\u8bb0\u4e0a\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u65e5\u8bb0\u8bed\u6599\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u4fe1\u606f\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5148\u8fdb\u7684\u5de5\u5177\u6765\u5206\u6790\u65e5\u8bb0\u5199\u4f5c\u76ee\u7684\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\u548co1-mini\uff09\u5bf9\u65e5\u8bb0\u6587\u672c\u8fdb\u884c\u5206\u6790\u548c\u805a\u7c7b\uff0c\u5e76\u4e0e\u57fa\u4e8e\u6a21\u677f\u7684\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "GPT-4o\u548co1-mini\u8868\u73b0\u6700\u4f73\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u6548\u679c\u8f83\u5dee\uff1b\u7814\u7a76\u8fd8\u5bf9\u4f5c\u8005\u6027\u522b\u3001\u5e74\u9f84\u548c\u5199\u4f5c\u5e74\u4efd\u7684\u5f71\u54cd\u8fdb\u884c\u4e86\u5206\u6790\u3002", "conclusion": "LLM\u5728\u65e5\u8bb0\u5206\u6790\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u4f46\u4ecd\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u548c\u5904\u7406\u9519\u8bef\u7c7b\u578b\u3002", "keywords": "\u65e5\u8bb0\u5206\u6790, \u5199\u4f5c\u76ee\u7684, \u5927\u8bed\u8a00\u6a21\u578b, \u82cf\u8054\u65f6\u671f\u65e5\u8bb0, \u6587\u672c\u805a\u7c7b"}}
{"id": "2506.00094", "pdf": "https://arxiv.org/pdf/2506.00094", "abs": "https://arxiv.org/abs/2506.00094", "authors": ["Konstantin Aal", "Tanja Aal", "Vasil Navumau", "David Unbehaun", "Claudia M\u00fcller", "Volker Wulf", "Sarah R\u00fcller"], "title": "Feeling Guilty Being a c(ai)borg: Navigating the Tensions Between Guilt and Empowerment in AI Use", "categories": ["cs.CY", "cs.AI", "cs.HC", "K.4.3"], "comment": "16 pages,", "summary": "This paper explores the emotional, ethical and practical dimensions of\nintegrating Artificial Intelligence (AI) into personal and professional\nworkflows, focusing on the concept of feeling guilty as a 'c(ai)borg' - a human\naugmented by AI. Inspired by Donna Haraway's Cyborg Manifesto, the study\nexplores how AI challenges traditional notions of creativity, originality and\nintellectual labour. Using an autoethnographic approach, the authors reflect on\ntheir year-long experiences with AI tools, revealing a transition from initial\nguilt and reluctance to empowerment through skill-building and transparency.\nKey findings highlight the importance of basic academic skills, advanced AI\nliteracy and honest engagement with AI results. The c(ai)borg vision advocates\nfor a future where AI is openly embraced as a collaborative partner, fostering\ninnovation and equity while addressing issues of access and agency. By\nreframing guilt as growth, the paper calls for a thoughtful and inclusive\napproach to AI integration.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u878d\u5165\u5de5\u4f5c\u6d41\u7a0b\u7684\u60c5\u611f\u3001\u4f26\u7406\u548c\u5b9e\u8df5\u5c42\u9762\uff0c\u63d0\u51fa\u4e86\u2018c(ai)borg\u2019\u6982\u5ff5\uff0c\u547c\u5401\u5c06AI\u89c6\u4e3a\u534f\u4f5c\u4f19\u4f34\uff0c\u5f3a\u8c03\u6280\u80fd\u57f9\u517b\u548c\u900f\u660e\u4f7f\u7528\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76AI\u5bf9\u4eba\u7c7b\u521b\u9020\u529b\u3001\u539f\u521b\u6027\u548c\u667a\u529b\u52b3\u52a8\u7684\u6311\u6218\uff0c\u63a2\u7d22AI\u8f85\u52a9\u4e0b\u7684\u4eba\u7c7b\u8eab\u4efd\u8f6c\u53d8\u3002", "method": "\u91c7\u7528\u81ea\u6211\u6c11\u65cf\u5fd7\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f5c\u8005\u4e00\u5e74\u7684AI\u5de5\u5177\u4f7f\u7528\u7ecf\u9a8c\u8fdb\u884c\u5206\u6790\u3002", "result": "\u4ece\u521d\u59cb\u7684\u6127\u759a\u611f\u8f6c\u53d8\u4e3a\u901a\u8fc7\u6280\u80fd\u63d0\u5347\u548c\u900f\u660e\u4f7f\u7528\u83b7\u5f97\u8d4b\u80fd\uff0c\u5f3a\u8c03\u57fa\u7840\u5b66\u672f\u80fd\u529b\u4e0e\u9ad8\u7ea7AI\u7d20\u517b\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u5021\u5bfc\u5f00\u653e\u62e5\u62b1AI\u7684\u672a\u6765\uff0c\u5c06\u5176\u4f5c\u4e3a\u534f\u4f5c\u4f19\u4f34\uff0c\u4fc3\u8fdb\u521b\u65b0\u4e0e\u516c\u5e73\uff0c\u540c\u65f6\u89e3\u51b3\u53ef\u8bbf\u95ee\u6027\u548c\u81ea\u4e3b\u6027\u95ee\u9898\u3002", "keywords": "\u4eba\u5de5\u667a\u80fd\uff0c\u4f26\u7406\uff0c\u81ea\u6211\u6c11\u65cf\u5fd7\uff0c\u534f\u4f5c\uff0cAI\u7d20\u517b"}}
{"id": "2506.00976", "pdf": "https://arxiv.org/pdf/2506.00976", "abs": "https://arxiv.org/abs/2506.00976", "authors": ["Jonathan Bobrutsky", "Amit Moscovich"], "title": "Quantization-based Bounds on the Wasserstein Metric", "categories": ["cs.LG", "stat.ML"], "comment": "23 pages, 8 figures, 7 tables", "summary": "The Wasserstein metric has become increasingly important in many machine\nlearning applications such as generative modeling, image retrieval and domain\nadaptation. Despite its appeal, it is often too costly to compute. This has\nmotivated approximation methods like entropy-regularized optimal transport,\ndownsampling, and subsampling, which trade accuracy for computational\nefficiency. In this paper, we consider the challenge of computing efficient\napproximations to the Wasserstein metric that also serve as strict upper or\nlower bounds. Focusing on discrete measures on regular grids, our approach\ninvolves formulating and exactly solving a Kantorovich problem on a coarse grid\nusing a quantized measure and specially designed cost matrix, followed by an\nupscaling and correction stage. This is done either in the primal or dual space\nto obtain valid upper and lower bounds on the Wasserstein metric of the\nfull-resolution inputs. We evaluate our methods on the DOTmark optimal\ntransport images benchmark, demonstrating a 10x-100x speedup compared to\nentropy-regularized OT while keeping the approximation error below 2%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u89c4\u5219\u7f51\u683c\u4e0a\u8ba1\u7b97Wasserstein\u5ea6\u91cf\u7684\u9ad8\u6548\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c97\u7f51\u683c\u4e0a\u7684Kantorovich\u95ee\u9898\u6c42\u89e3\u548c\u4e0a\u91c7\u6837\u6821\u6b63\uff0c\u5b9e\u73b0\u4e86\u4e25\u683c\u7684\u4e0a\u754c\u6216\u4e0b\u754c\uff0c\u901f\u5ea6\u6bd4\u71b5\u6b63\u5219\u5316\u65b9\u6cd5\u5feb10-100\u500d\uff0c\u8bef\u5dee\u63a7\u5236\u57282%\u4ee5\u5185\u3002", "motivation": "Wasserstein\u5ea6\u91cf\u5728\u673a\u5668\u5b66\u4e60\u7684\u591a\u4e2a\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u7684\u8fd1\u4f3c\u65b9\u6cd5\u3002", "method": "\u5728\u89c4\u5219\u7f51\u683c\u4e0a\uff0c\u901a\u8fc7\u7c97\u7f51\u683c\u7684Kantorovich\u95ee\u9898\u6c42\u89e3\u3001\u91cf\u5316\u6d4b\u5ea6\u548c\u7279\u6b8a\u8bbe\u8ba1\u7684\u6210\u672c\u77e9\u9635\uff0c\u7ed3\u5408\u4e0a\u91c7\u6837\u548c\u6821\u6b63\u9636\u6bb5\uff0c\u5f97\u5230\u5168\u5206\u8fa8\u7387\u8f93\u5165Wasserstein\u5ea6\u91cf\u7684\u4e25\u683c\u4e0a\u4e0b\u754c\u3002", "result": "\u5728DOTmark\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u901f\u5ea6\u6bd4\u71b5\u6b63\u5219\u5316\u65b9\u6cd5\u5feb10-100\u500d\uff0c\u8bef\u5dee\u4f4e\u4e8e2%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u7cbe\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u6548Wasserstein\u8fd1\u4f3c\u7684\u5927\u89c4\u6a21\u5e94\u7528\u3002", "keywords": "Wasserstein\u5ea6\u91cf, Kantorovich\u95ee\u9898, \u8fd1\u4f3c\u65b9\u6cd5, \u71b5\u6b63\u5219\u5316, \u4e0a\u91c7\u6837"}}
{"id": "2506.00986", "pdf": "https://arxiv.org/pdf/2506.00986", "abs": "https://arxiv.org/abs/2506.00986", "authors": ["Alexander Sergeev", "Valeriya Goloviznina", "Mikhail Melnichenko", "Evgeny Kotelnikov"], "title": "Talking to Data: Designing Smart Assistants for Humanities Databases", "categories": ["cs.CL"], "comment": "Accepted for InterSys-2025 conference", "summary": "Access to humanities research databases is often hindered by the limitations\nof traditional interaction formats, particularly in the methods of searching\nand response generation. This study introduces an LLM-based smart assistant\ndesigned to facilitate natural language communication with digital humanities\ndata. The assistant, developed in a chatbot format, leverages the RAG approach\nand integrates state-of-the-art technologies such as hybrid search, automatic\nquery generation, text-to-SQL filtering, semantic database search, and\nhyperlink insertion. To evaluate the effectiveness of the system, experiments\nwere conducted to assess the response quality of various language models. The\ntesting was based on the Prozhito digital archive, which contains diary entries\nfrom predominantly Russian-speaking individuals who lived in the 20th century.\nThe chatbot is tailored to support anthropology and history researchers, as\nwell as non-specialist users with an interest in the field, without requiring\nprior technical training. By enabling researchers to query complex databases\nwith natural language, this tool aims to enhance accessibility and efficiency\nin humanities research. The study highlights the potential of Large Language\nModels to transform the way researchers and the public interact with digital\narchives, making them more intuitive and inclusive. Additional materials are\npresented in GitHub repository:\nhttps://github.com/alekosus/talking-to-data-intersys2025.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u52a9\u624b\uff0c\u7528\u4e8e\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4e0e\u6570\u5b57\u4eba\u6587\u6570\u636e\u4ea4\u4e92\uff0c\u65e8\u5728\u63d0\u9ad8\u4eba\u6587\u7814\u7a76\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u4ea4\u4e92\u5f62\u5f0f\u9650\u5236\u4e86\u4eba\u6587\u7814\u7a76\u6570\u636e\u5e93\u7684\u8bbf\u95ee\uff0c\u8be5\u7814\u7a76\u8bd5\u56fe\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528RAG\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df7\u5408\u641c\u7d22\u3001\u81ea\u52a8\u67e5\u8be2\u751f\u6210\u3001\u6587\u672c\u5230SQL\u8fc7\u6ee4\u7b49\u6280\u672f\u5f00\u53d1\u804a\u5929\u673a\u5668\u4eba\u3002", "result": "\u5b9e\u9a8c\u57fa\u4e8eProzhito\u6570\u5b57\u6863\u6848\u8bc4\u4f30\u4e86\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u7684\u54cd\u5e94\u8d28\u91cf\u3002", "conclusion": "LLM\u6709\u6f5c\u529b\u6539\u53d8\u7814\u7a76\u4eba\u5458\u548c\u516c\u4f17\u4e0e\u6570\u5b57\u6863\u6848\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u4f7f\u5176\u66f4\u76f4\u89c2\u548c\u5305\u5bb9\u3002", "keywords": "LLM, \u6570\u5b57\u4eba\u6587, RAG, \u81ea\u7136\u8bed\u8a00\u5904\u7406, Prozhito"}}
{"id": "2506.00095", "pdf": "https://arxiv.org/pdf/2506.00095", "abs": "https://arxiv.org/abs/2506.00095", "authors": ["Yuchong Li", "Xiaojun Zeng", "Chihua Fang", "Jian Yang", "Lei Zhang"], "title": "ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Hepato-pancreato-biliary (HPB) disorders represent a global public health\nchallenge due to their high morbidity and mortality. Although large language\nmodels (LLMs) have shown promising performance in general medical\nquestion-answering tasks, the current evaluation benchmarks are mostly derived\nfrom standardized examinations or manually designed questions, lacking HPB\ncoverage and clinical cases. To address these issues, we systematically\neatablish an HPB disease evaluation benchmark comprising 3,535 closed-ended\nmultiple-choice questions and 337 open-ended real diagnosis cases, which\nencompasses all the 33 main categories and 465 subcategories of HPB diseases\ndefined in the International Statistical Classification of Diseases, 10th\nRevision (ICD-10). The multiple-choice questions are curated from public\ndatasets and synthesized data, and the clinical cases are collected from\nprestigious medical journals, case-sharing platforms, and collaborating\nhospitals. By evalauting commercial and open-source general and medical LLMs on\nour established benchmark, namely ClinBench-HBP, we find that while commercial\nLLMs perform competently on medical exam questions, they exhibit substantial\nperformance degradation on HPB diagnosis tasks, especially on complex,\ninpatient clinical cases. Those medical LLMs also show limited generalizability\nto HPB diseases. Our results reveal the critical limitations of current LLMs in\nthe domain of HPB diseases, underscoring the imperative need for future medical\nLLMs to handle real, complex clinical diagnostics rather than simple medical\nexam questions. The benchmark will be released at the homepage.", "AI": {"tldr": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u9488\u5bf9\u809d\u80f0\u80c6\uff08HPB\uff09\u75be\u75c5\u7684\u8bc4\u4f30\u57fa\u51c6ClinBench-HBP\uff0c\u5305\u542b3,535\u4e2a\u95ed\u5377\u9009\u62e9\u9898\u548c337\u4e2a\u5f00\u653e\u5f0f\u771f\u5b9e\u8bca\u65ad\u6848\u4f8b\uff0c\u8986\u76d6ICD-10\u4e2d\u6240\u670933\u4e2a\u4e3b\u8981\u7c7b\u522b\u548c465\u4e2a\u5b50\u7c7b\u522b\u3002\u8bc4\u4f30\u53d1\u73b0\uff0c\u73b0\u6709\u5546\u4e1a\u548c\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728HPB\u8bca\u65ad\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u590d\u6742\u4e34\u5e8a\u6848\u4f8b\u4e2d\u3002", "motivation": "HPB\u75be\u75c5\u7684\u9ad8\u53d1\u75c5\u7387\u548c\u6b7b\u4ea1\u7387\u4f7f\u5176\u6210\u4e3a\u5168\u7403\u516c\u5171\u536b\u751f\u6311\u6218\uff0c\u4f46\u76ee\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8bc4\u4f30\u57fa\u51c6\u7f3a\u4e4fHPB\u76f8\u5173\u5185\u5bb9\u548c\u4e34\u5e8a\u6848\u4f8b\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u4e13\u95e8\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u901a\u8fc7\u6574\u5408\u516c\u5f00\u6570\u636e\u96c6\u548c\u5408\u6210\u6570\u636e\uff0c\u4ee5\u53ca\u4ece\u533b\u5b66\u671f\u520a\u3001\u6848\u4f8b\u5206\u4eab\u5e73\u53f0\u548c\u5408\u4f5c\u533b\u9662\u6536\u96c6\u7684\u4e34\u5e8a\u6848\u4f8b\uff0c\u7cfb\u7edf\u6784\u5efa\u4e86ClinBench-HBP\u57fa\u51c6\u3002\u968f\u540e\u8bc4\u4f30\u4e86\u5546\u4e1a\u548c\u5f00\u6e90\u901a\u7528\u53ca\u533b\u7597LLMs\u5728\u8be5\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5546\u4e1aLLMs\u5728\u533b\u5b66\u8003\u8bd5\u9898\u76ee\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728HPB\u8bca\u65ad\u4efb\u52a1\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5c24\u5176\u5728\u590d\u6742\u4f4f\u9662\u75c5\u4f8b\u4e2d\u3002\u533b\u7597LLMs\u5bf9HPB\u75be\u75c5\u7684\u6cdb\u5316\u80fd\u529b\u4e5f\u6709\u9650\u3002", "conclusion": "\u5f53\u524dLLMs\u5728HPB\u75be\u75c5\u9886\u57df\u5b58\u5728\u660e\u663e\u5c40\u9650\uff0c\u672a\u6765\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u771f\u5b9e\u3001\u590d\u6742\u4e34\u5e8a\u8bca\u65ad\u7684\u533b\u7597LLMs\uff0c\u800c\u4e0d\u4ec5\u662f\u7b80\u5355\u7684\u533b\u5b66\u8003\u8bd5\u9898\u76ee\u3002", "keywords": "HPB\u75be\u75c5, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u4e34\u5e8a\u8bca\u65ad, \u8bc4\u4f30\u57fa\u51c6, ICD-10"}}
{"id": "2506.00998", "pdf": "https://arxiv.org/pdf/2506.00998", "abs": "https://arxiv.org/abs/2506.00998", "authors": ["Changshun Wu", "Tianyi Duan", "Saddek Bensalem", "Chih-Hong Cheng"], "title": "LoRA-BAM: Input Filtering for Fine-tuned LLMs via Boxed Abstraction Monitors over LoRA Layers", "categories": ["cs.LG"], "comment": null, "summary": "Fine-tuning large language models (LLMs) improves performance on\ndomain-specific tasks but can lead to overfitting, making them unreliable on\nout-of-distribution (OoD) queries. We propose LoRA-BAM - a method that adds OoD\ndetection monitors to the LoRA layer using boxed abstraction to filter\nquestions beyond the model's competence. Feature vectors from the fine-tuning\ndata are extracted via the LLM and clustered. Clusters are enclosed in boxes; a\nquestion is flagged as OoD if its feature vector falls outside all boxes. To\nimprove interpretability and robustness, we introduce a regularization loss\nduring fine-tuning that encourages paraphrased questions to stay close in the\nfeature space, and the enlargement of the decision boundary is based on the\nfeature variance within a cluster. Our method complements existing defenses by\nproviding lightweight and interpretable OoD detection.", "AI": {"tldr": "LoRA-BAM\u662f\u4e00\u79cd\u901a\u8fc7\u76d2\u62bd\u8c61\u5728LoRA\u5c42\u6dfb\u52a0OoD\u68c0\u6d4b\u76d1\u89c6\u5668\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fc7\u6ee4\u8d85\u51fa\u6a21\u578b\u80fd\u529b\u7684\u67e5\u8be2\uff0c\u5e76\u5f15\u5165\u6b63\u5219\u5316\u635f\u5931\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u7136\u80fd\u63d0\u5347\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u8fc7\u62df\u5408\uff0c\u4f7f\u5176\u5728\u5206\u5e03\u5916\uff08OoD\uff09\u67e5\u8be2\u4e0a\u4e0d\u53ef\u9760\u3002", "method": "\u4ece\u5fae\u8c03\u6570\u636e\u4e2d\u63d0\u53d6\u7279\u5f81\u5411\u91cf\u5e76\u805a\u7c7b\uff0c\u5c06\u805a\u7c7b\u7ed3\u679c\u5c01\u88c5\u4e3a\u76d2\u5b50\uff1b\u82e5\u95ee\u9898\u7684\u7279\u5f81\u5411\u91cf\u843d\u5728\u6240\u6709\u76d2\u5b50\u5916\u5219\u6807\u8bb0\u4e3aOoD\u3002\u5f15\u5165\u6b63\u5219\u5316\u635f\u5931\u4ee5\u9f13\u52b1\u8fd1\u4e49\u95ee\u9898\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u76f8\u8fd1\u3002", "result": "LoRA-BAM\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u4e14\u53ef\u89e3\u91ca\u7684OoD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8865\u5145\u4e86\u73b0\u6709\u9632\u5fa1\u624b\u6bb5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u76d2\u62bd\u8c61\u548c\u6b63\u5219\u5316\u63d0\u5347\u4e86OoD\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5206\u5e03\u5916\u68c0\u6d4b, LoRA, \u76d2\u62bd\u8c61, \u6b63\u5219\u5316"}}
{"id": "2506.01034", "pdf": "https://arxiv.org/pdf/2506.01034", "abs": "https://arxiv.org/abs/2506.01034", "authors": ["Benjamin Matthias Ruppik", "Julius von Rohrscheidt", "Carel van Niekerk", "Michael Heck", "Renato Vukovic", "Shutong Feng", "Hsien-chin Lin", "Nurul Lubis", "Bastian Rieck", "Marcus Zibrowius", "Milica Ga\u0161i\u0107"], "title": "Less is More: Local Intrinsic Dimensions of Contextual Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, with an additional 13 pages of appendix", "summary": "Understanding the internal mechanisms of large language models (LLMs) remains\na challenging and complex endeavor. Even fundamental questions, such as how\nfine-tuning affects model behavior, often require extensive empirical\nevaluation. In this paper, we introduce a novel perspective based on the\ngeometric properties of contextual latent embeddings to study the effects of\ntraining and fine-tuning. To that end, we measure the local dimensions of a\ncontextual language model's latent space and analyze their shifts during\ntraining and fine-tuning. We show that the local dimensions provide insights\ninto the model's training dynamics and generalization ability. Specifically,\nthe mean of the local dimensions predicts when the model's training\ncapabilities are exhausted, as exemplified in a dialogue state tracking task,\noverfitting, as demonstrated in an emotion recognition task, and grokking, as\nillustrated with an arithmetic task. Furthermore, our experiments suggest a\npractical heuristic: reductions in the mean local dimension tend to accompany\nand predict subsequent performance gains. Through this exploration, we aim to\nprovide practitioners with a deeper understanding of the implications of\nfine-tuning on embedding spaces, facilitating informed decisions when\nconfiguring models for specific applications. The results of this work\ncontribute to the ongoing discourse on the interpretability, adaptability, and\ngeneralizability of LLMs by bridging the gap between intrinsic model mechanisms\nand geometric properties in the respective embeddings.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u51e0\u4f55\u89c6\u89d2\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8bad\u7ec3\u4e0e\u5fae\u8c03\u6548\u679c\uff0c\u63d0\u51fa\u5c40\u90e8\u7ef4\u5ea6\u4f5c\u4e3a\u9884\u6d4b\u6a21\u578b\u52a8\u6001\u548c\u6cdb\u5316\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u7406\u89e3LLMs\u5185\u90e8\u673a\u5236\u53ca\u5176\u5728\u8bad\u7ec3\u548c\u5fae\u8c03\u4e2d\u7684\u884c\u4e3a\u53d8\u5316\u662f\u4e00\u4e2a\u590d\u6742\u6311\u6218\uff0c\u9700\u63a2\u7d22\u65b0\u89c6\u89d2\u4ee5\u63d0\u4f9b\u66f4\u76f4\u89c2\u7684\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u7684\u5c40\u90e8\u7ef4\u5ea6\u53ca\u5176\u5728\u8bad\u7ec3\u548c\u5fae\u8c03\u4e2d\u7684\u53d8\u5316\uff0c\u5206\u6790\u5176\u5bf9\u6a21\u578b\u52a8\u6001\u548c\u6cdb\u5316\u7684\u5f71\u54cd\u3002", "result": "\u5c40\u90e8\u7ef4\u5ea6\u7684\u5e73\u5747\u503c\u80fd\u9884\u6d4b\u6a21\u578b\u8bad\u7ec3\u80fd\u529b\u8017\u5c3d\uff08\u5982\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\uff09\u3001\u8fc7\u62df\u5408\uff08\u5982\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\uff09\u53ca\u987f\u609f\u73b0\u8c61\uff08\u5982\u7b97\u672f\u4efb\u52a1\uff09\uff0c\u4e14\u5176\u51cf\u5c11\u901a\u5e38\u4f34\u968f\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5c40\u90e8\u7ef4\u5ea6\u5206\u6790\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5bf9\u5fae\u8c03\u6548\u679c\u7684\u65b0\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u6a21\u578b\u914d\u7f6e\uff0c\u63a8\u52a8LLMs\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\u7814\u7a76\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u5fae\u8c03\u3001\u5c40\u90e8\u7ef4\u5ea6\u3001\u51e0\u4f55\u5c5e\u6027\u3001\u6cdb\u5316\u80fd\u529b"}}
{"id": "2506.00096", "pdf": "https://arxiv.org/pdf/2506.00096", "abs": "https://arxiv.org/abs/2506.00096", "authors": ["Liangrui Pan", "Qingchun Liang", "Shen Zhao", "Songqing Fan", "Shaoliang Peng"], "title": "PathGene: Benchmarking Driver Gene Mutations and Exon Prediction Using Multicenter Lung Cancer Histopathology Image Dataset", "categories": ["q-bio.GN", "cs.AI"], "comment": "Submit to NIPS2025", "summary": "Accurately predicting gene mutations, mutation subtypes and their exons in\nlung cancer is critical for personalized treatment planning and prognostic\nassessment. Faced with regional disparities in medical resources and the high\ncost of genomic assays, using artificial intelligence to infer these mutations\nand exon variants from routine histopathology images could greatly facilitate\nprecision therapy. Although some prior studies have shown that deep learning\ncan accelerate the prediction of key gene mutations from lung cancer pathology\nslides, their performance remains suboptimal and has so far been limited mainly\nto early screening tasks. To address these limitations, we have assembled\nPathGene, which comprises histopathology images paired with next-generation\nsequencing reports from 1,576 patients at the Second Xiangya Hospital, Central\nSouth University, and 448 TCGA-LUAD patients. This multi-center dataset links\nwhole-slide images to driver gene mutation status, mutation subtypes, exon, and\ntumor mutational burden (TMB) status, with the goal of leveraging pathology\nimages to predict mutations, subtypes, exon locations, and TMB for early\ngenetic screening and to advance precision oncology. Unlike existing datasets,\nwe provide molecular-level information related to histopathology images in\nPathGene to facilitate the development of biomarker prediction models. We\nbenchmarked 11 multiple-instance learning methods on PathGene for mutation,\nsubtype, exon, and TMB prediction tasks. These experimental methods provide\nvaluable alternatives for early genetic screening of lung cancer patients and\nassisting clinicians to quickly develop personalized precision targeted\ntreatment plans for patients. Code and data are available at\nhttps://github.com/panliangrui/NIPS2025/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPathGene\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u75c5\u7406\u56fe\u50cf\u548c\u57fa\u56e0\u6d4b\u5e8f\u6570\u636e\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4b\u80ba\u764c\u57fa\u56e0\u7a81\u53d8\u3001\u4e9a\u578b\u548c\u5916\u663e\u5b50\u53d8\u5f02\uff0c\u4e3a\u7cbe\u51c6\u533b\u7597\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u533b\u7597\u8d44\u6e90\u5206\u5e03\u4e0d\u5747\u548c\u57fa\u56e0\u68c0\u6d4b\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u5229\u7528AI\u4ece\u5e38\u89c4\u75c5\u7406\u56fe\u50cf\u63a8\u65ad\u57fa\u56e0\u7a81\u53d8\u548c\u5916\u663e\u5b50\u53d8\u5f02\uff0c\u63a8\u52a8\u7cbe\u51c6\u6cbb\u7597\u3002", "method": "\u6536\u96c6\u591a\u4e2d\u5fc3\u6570\u636e\u96c6PathGene\uff0c\u5305\u542b1576\u540d\u60a3\u8005\u7684\u6570\u636e\uff0c\u5e76\u8bc4\u4f3011\u79cd\u591a\u793a\u4f8b\u5b66\u4e60\u65b9\u6cd5\u5728\u7a81\u53d8\u3001\u4e9a\u578b\u3001\u5916\u663e\u5b50\u548cTMB\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "PathGene\u6570\u636e\u96c6\u548c\u5b9e\u9a8c\u65b9\u6cd5\u4e3a\u80ba\u764c\u60a3\u8005\u7684\u65e9\u671f\u57fa\u56e0\u7b5b\u67e5\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u8ba1\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "conclusion": "PathGene\u53ca\u5176\u65b9\u6cd5\u5728\u80ba\u764c\u7cbe\u51c6\u533b\u7597\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u53ef\u4f5c\u4e3a\u4e34\u5e8a\u8f85\u52a9\u5de5\u5177\u3002", "keywords": "\u80ba\u764c, \u57fa\u56e0\u7a81\u53d8, \u75c5\u7406\u56fe\u50cf, \u591a\u793a\u4f8b\u5b66\u4e60, \u7cbe\u51c6\u533b\u7597"}}
{"id": "2506.01000", "pdf": "https://arxiv.org/pdf/2506.01000", "abs": "https://arxiv.org/abs/2506.01000", "authors": ["Chengyi Cai", "Zesheng Ye", "Lei Feng", "Jianzhong Qi", "Feng Liu"], "title": "Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Model reprogramming adapts pretrained models to downstream tasks by modifying\nonly the input and output spaces. Visual reprogramming (VR) is one instance for\nvision tasks that adds a trainable noise pattern (i.e., a visual prompt) to\ninput images to facilitate downstream classification. The existing VR\napproaches for CLIP train a single visual prompt using all descriptions of\ndifferent downstream classes. However, the limited learning capacity may result\nin (1) a failure to capture diverse aspects of the descriptions (e.g., shape,\ncolor, and texture), and (2) a possible bias toward less informative attributes\nthat do not help distinguish between classes. In this paper, we introduce a\ndecoupling-and-reweighting framework. Our decoupled visual prompts (DVP) are\noptimized using descriptions grouped by explicit causes (DVP-cse) or\nunsupervised clusters (DVP-cls). Then, we integrate the outputs of these visual\nprompts with a probabilistic reweighting matrix (PRM) that measures their\ncontributions to each downstream class. Theoretically, DVP lowers the empirical\nrisk bound. Experimentally, DVP outperforms baselines on average across 11\ndownstream datasets. Notably, the DVP-PRM integration enables insights into how\nindividual visual prompts influence classification decisions, providing a\nprobabilistic framework for understanding reprogramming. Our code is available\nat https://github.com/tmlr-group/DecoupledVP.", "AI": {"tldr": "\u6a21\u578b\u91cd\u65b0\u7f16\u7a0b\u901a\u8fc7\u4fee\u6539\u8f93\u5165\u548c\u8f93\u51fa\u7a7a\u95f4\u6765\u9002\u5e94\u9884\u8bad\u7ec3\u6a21\u578b\u5230\u4e0b\u6e38\u4efb\u52a1\u3002\u89c6\u89c9\u91cd\u65b0\u7f16\u7a0b\uff08VR\uff09\u662f\u4e00\u79cd\u4e3a\u89c6\u89c9\u4efb\u52a1\u6dfb\u52a0\u53ef\u8bad\u7ec3\u566a\u58f0\u6a21\u5f0f\uff08\u5373\u89c6\u89c9\u63d0\u793a\uff09\u4ee5\u8f85\u52a9\u5206\u7c7b\u7684\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u6240\u6709\u7c7b\u522b\u63cf\u8ff0\u8bad\u7ec3\u5355\u4e2a\u89c6\u89c9\u63d0\u793a\uff0c\u53ef\u80fd\u5bfc\u81f4\u591a\u6837\u6027\u6355\u6349\u4e0d\u8db3\u548c\u504f\u89c1\u3002\u672c\u6587\u63d0\u51fa\u89e3\u8026\u548c\u91cd\u52a0\u6743\u6846\u67b6\uff08DVP\uff09\uff0c\u901a\u8fc7\u5206\u7ec4\u4f18\u5316\u63d0\u793a\u5e76\u96c6\u6210\u8f93\u51fa\uff0c\u964d\u4f4e\u4e86\u7ecf\u9a8c\u98ce\u9669\u754c\u9650\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u91cd\u65b0\u7f16\u7a0b\u65b9\u6cd5\u4f7f\u7528\u5355\u4e00\u89c6\u89c9\u63d0\u793a\uff0c\u53ef\u80fd\u65e0\u6cd5\u6355\u6349\u591a\u6837\u5c5e\u6027\u5e76\u504f\u5411\u975e\u4fe1\u606f\u5c5e\u6027\u3002\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u548c\u89e3\u91ca\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u89c6\u89c9\u63d0\u793a\uff08DVP\uff09\uff0c\u901a\u8fc7\u5206\u7ec4\u63cf\u8ff0\u4f18\u5316\u63d0\u793a\uff08DVP-cse\u6216DVP-cls\uff09\uff0c\u5e76\u96c6\u6210\u8f93\u51fa\u4f7f\u7528\u6982\u7387\u91cd\u52a0\u6743\u77e9\u9635\uff08PRM\uff09\u8861\u91cf\u5176\u5bf9\u5206\u7c7b\u7684\u8d21\u732e\u3002", "result": "DVP\u572811\u4e2a\u4e0b\u6e38\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0cPRM\u96c6\u6210\u63d0\u4f9b\u4e86\u5bf9\u89c6\u89c9\u63d0\u793a\u5f71\u54cd\u7684\u6982\u7387\u89e3\u91ca\u3002", "conclusion": "\u89e3\u8026\u548c\u91cd\u52a0\u6743\u6846\u67b6\u63d0\u9ad8\u4e86\u89c6\u89c9\u91cd\u65b0\u7f16\u7a0b\u7684\u6548\u679c\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u5206\u7c7b\u51b3\u7b56\u4f9d\u636e\u3002", "keywords": "\u6a21\u578b\u91cd\u65b0\u7f16\u7a0b\u3001\u89c6\u89c9\u63d0\u793a\u3001\u89e3\u8026\u3001\u6982\u7387\u91cd\u52a0\u6743\u3001\u5206\u7c7b"}}
{"id": "2506.01042", "pdf": "https://arxiv.org/pdf/2506.01042", "abs": "https://arxiv.org/abs/2506.01042", "authors": ["Yu Zheng", "Yuan Yuan", "Yong Li", "Paolo Santi"], "title": "Probing Neural Topology of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Probing large language models (LLMs) has yielded valuable insights into their\ninternal mechanisms by linking neural representations to interpretable\nsemantics. However, how neurons functionally co-activate with each other to\ngive rise to emergent capabilities remains largely unknown, hindering a deeper\nunderstanding and safer development of LLMs. In this work, we introduce graph\nprobing, a method for uncovering the functional connectivity topology of LLM\nneurons and relating it to language generation performance. By analyzing\ninternal neural graphs across diverse LLM families and scales, we discover a\nuniversal predictability of next-token prediction performance using only neural\ntopology. This predictability is robust even when retaining just 1% of neuron\nconnections or probing models after only 8 pretraining steps, highlighting the\nsparsity and early emergence of topological patterns. Further graph matching\nanalysis suggests that, despite significant distinctions in architectures,\nparameters, and training data, different LLMs develop intricate and consistent\nneural topological structures that may form the foundation for their language\ngeneration abilities. Codes and data for the graph probing toolbox are released\nat https://github.com/DavyMorgan/llm-graph-probing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u56fe\u63a2\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u795e\u7ecf\u5143\u7684\u529f\u80fd\u8fde\u63a5\u62d3\u6251\u7ed3\u6784\uff0c\u5e76\u53d1\u73b0\u795e\u7ecf\u62d3\u6251\u7ed3\u6784\u80fd\u666e\u904d\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecf\u5143\u5982\u4f55\u534f\u540c\u6fc0\u6d3b\u4ee5\u4ea7\u751f\u6d8c\u73b0\u80fd\u529b\uff0c\u4ece\u800c\u66f4\u6df1\u5165\u7406\u89e3\u5e76\u5b89\u5168\u5f00\u53d1LLMs\u3002", "method": "\u5f15\u5165\u56fe\u63a2\u6d4b\u65b9\u6cd5\uff0c\u5206\u6790\u4e0d\u540cLLM\u5bb6\u65cf\u548c\u89c4\u6a21\u7684\u5185\u90e8\u795e\u7ecf\u56fe\uff0c\u7814\u7a76\u5176\u4e0e\u8bed\u8a00\u751f\u6210\u6027\u80fd\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u795e\u7ecf\u62d3\u6251\u7ed3\u6784\u80fd\u666e\u904d\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u6027\u80fd\uff0c\u5373\u4f7f\u4ec5\u4fdd\u75591%\u795e\u7ecf\u5143\u8fde\u63a5\u6216\u4ec5\u57288\u6b65\u9884\u8bad\u7ec3\u540e\uff0c\u8fd9\u79cd\u9884\u6d4b\u6027\u4ecd\u7a33\u5065\u3002", "conclusion": "\u4e0d\u540cLLMs\u5f62\u6210\u4e86\u590d\u6742\u4e14\u4e00\u81f4\u7684\u795e\u7ecf\u62d3\u6251\u7ed3\u6784\uff0c\u53ef\u80fd\u662f\u5176\u8bed\u8a00\u751f\u6210\u80fd\u529b\u7684\u57fa\u7840\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u795e\u7ecf\u5143\u62d3\u6251, \u56fe\u63a2\u6d4b, \u8bed\u8a00\u751f\u6210"}}
{"id": "2506.00100", "pdf": "https://arxiv.org/pdf/2506.00100", "abs": "https://arxiv.org/abs/2506.00100", "authors": ["Ajinkya Kulkarni", "Francisco Teixeira", "Enno Hermann", "Thomas Rolland", "Isabel Trancoso", "Mathew Magimai Doss"], "title": "Children's Voice Privacy: First Steps And Emerging Challenges", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CR"], "comment": "Accepted at Interspeech 2025, Netherlands", "summary": "Children are one of the most under-represented groups in speech technologies,\nas well as one of the most vulnerable in terms of privacy. Despite this,\nanonymization techniques targeting this population have received little\nattention. In this study, we seek to bridge this gap, and establish a baseline\nfor the use of voice anonymization techniques designed for adult speech when\napplied to children's voices. Such an evaluation is essential, as children's\nspeech presents a distinct set of challenges when compared to that of adults.\nThis study comprises three children's datasets, six anonymization methods, and\nobjective and subjective utility metrics for evaluation. Our results show that\nexisting systems for adults are still able to protect children's voice privacy,\nbut suffer from much higher utility degradation. In addition, our subjective\nstudy displays the challenges of automatic evaluation methods for speech\nquality in children's speech, highlighting the need for further research.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u513f\u7ae5\u8bed\u97f3\u533f\u540d\u5316\u6280\u672f\u7684\u73b0\u72b6\uff0c\u8bc4\u4f30\u4e86\u73b0\u6709\u6210\u4eba\u8bed\u97f3\u533f\u540d\u5316\u65b9\u6cd5\u5728\u513f\u7ae5\u8bed\u97f3\u4e0a\u7684\u6548\u679c\uff0c\u53d1\u73b0\u867d\u7136\u80fd\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u5b9e\u7528\u6027\u4e0b\u964d\u8f83\u5927\uff0c\u5e76\u6307\u51fa\u81ea\u52a8\u8bc4\u4f30\u513f\u7ae5\u8bed\u97f3\u8d28\u91cf\u7684\u6311\u6218\u3002", "motivation": "\u513f\u7ae5\u5728\u8bed\u97f3\u6280\u672f\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u4e14\u9690\u79c1\u4fdd\u62a4\u9700\u6c42\u9ad8\uff0c\u4f46\u9488\u5bf9\u4ed6\u4eec\u7684\u8bed\u97f3\u533f\u540d\u5316\u6280\u672f\u7814\u7a76\u8f83\u5c11\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u4e09\u4e2a\u513f\u7ae5\u8bed\u97f3\u6570\u636e\u96c6\u548c\u516d\u79cd\u533f\u540d\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u5ba2\u89c2\u4e0e\u4e3b\u89c2\u6548\u7528\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709\u6210\u4eba\u8bed\u97f3\u533f\u540d\u5316\u6280\u672f\u80fd\u4fdd\u62a4\u513f\u7ae5\u9690\u79c1\uff0c\u4f46\u5b9e\u7528\u6027\u663e\u8457\u4e0b\u964d\uff1b\u4e3b\u89c2\u7814\u7a76\u63ed\u793a\u4e86\u513f\u7ae5\u8bed\u97f3\u81ea\u52a8\u8bc4\u4f30\u7684\u6311\u6218\u3002", "conclusion": "\u513f\u7ae5\u8bed\u97f3\u533f\u540d\u5316\u6280\u672f\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5b9e\u7528\u6027\u63d0\u5347\u548c\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u7684\u6539\u8fdb\u3002", "keywords": "\u513f\u7ae5\u8bed\u97f3\u6280\u672f\u3001\u8bed\u97f3\u533f\u540d\u5316\u3001\u9690\u79c1\u4fdd\u62a4\u3001\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30"}}
{"id": "2506.01016", "pdf": "https://arxiv.org/pdf/2506.01016", "abs": "https://arxiv.org/abs/2506.01016", "authors": ["Olya Mastikhina", "Dhruv Sreenivas", "Pablo Samuel Castro"], "title": "Optimistic critics can empower small actors", "categories": ["cs.LG", "stat.ML"], "comment": "RLC 2025", "summary": "Actor-critic methods have been central to many of the recent advances in deep\nreinforcement learning. The most common approach is to use symmetric\narchitectures, whereby both actor and critic have the same network topology and\nnumber of parameters. However, recent works have argued for the advantages of\nasymmetric setups, specifically with the use of smaller actors. We perform\nbroad empirical investigations and analyses to better understand the\nimplications of this and find that, in general, smaller actors result in\nperformance degradation and overfit critics. Our analyses suggest poor data\ncollection, due to value underestimation, as one of the main causes for this\nbehavior, and further highlight the crucial role the critic can play in\nalleviating this pathology. We explore techniques to mitigate the observed\nvalue underestimation, which enables further research in asymmetric\nactor-critic methods.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u4f7f\u7528\u66f4\u5c0f\u7684actor\u7f51\u7edc\u7684\u4e0d\u5bf9\u79f0\u67b6\u6784\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548ccritic\u8fc7\u62df\u5408\uff0c\u5e76\u63a2\u8ba8\u4e86\u7f13\u89e3\u65b9\u6cd5\u3002", "motivation": "\u63a2\u8ba8actor-critic\u65b9\u6cd5\u4e2d\u4e0d\u5bf9\u79f0\uff08\u66f4\u5c0factor\uff09\u67b6\u6784\u7684\u5f71\u54cd\u53ca\u5176\u53ef\u80fd\u5e26\u6765\u7684\u6027\u80fd\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u7814\u7a76\u548c\u5206\u6790\uff0c\u6bd4\u8f83\u5bf9\u79f0\u548c\u4e0d\u5bf9\u79f0\u67b6\u6784\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u5206\u6790\u5176\u80cc\u540e\u7684\u539f\u56e0\u3002", "result": "\u66f4\u5c0f\u7684actor\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548ccritic\u8fc7\u62df\u5408\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u4f4e\u4f30\u4ef7\u503c\u7684\u7b56\u7565\u5bfc\u81f4\u6570\u636e\u6536\u96c6\u4e0d\u8db3\u3002", "conclusion": "\u63d0\u51fa\u4e86\u7f13\u89e3\u4f4e\u4f30\u4ef7\u503c\u7684\u65b9\u6cd5\uff0c\u4e3a\u4e0d\u5bf9\u79f0actor-critic\u65b9\u6cd5\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "keywords": "actor-critic, deep reinforcement learning, asymmetric architecture, value underestimation"}}
{"id": "2506.01047", "pdf": "https://arxiv.org/pdf/2506.01047", "abs": "https://arxiv.org/abs/2506.01047", "authors": ["Phan Anh Duong", "Cat Luong", "Divyesh Bommana", "Tianyu Jiang"], "title": "CHEER-Ekman: Fine-grained Embodied Emotion Classification", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Emotions manifest through physical experiences and bodily reactions, yet\nidentifying such embodied emotions in text remains understudied. We present an\nembodied emotion classification dataset, CHEER-Ekman, extending the existing\nbinary embodied emotion dataset with Ekman's six basic emotion categories.\nUsing automatic best-worst scaling with large language models, we achieve\nperformance superior to supervised approaches on our new dataset. Our\ninvestigation reveals that simplified prompting instructions and\nchain-of-thought reasoning significantly improve emotion recognition accuracy,\nenabling smaller models to achieve competitive performance with larger ones.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eEkman\u516d\u79cd\u57fa\u672c\u60c5\u611f\u7c7b\u522b\u7684\u6570\u636e\u96c6CHEER-Ekman\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u6700\u4f73-\u6700\u5dee\u7f29\u653e\u65b9\u6cd5\uff0c\u60c5\u611f\u8bc6\u522b\u51c6\u786e\u7387\u4f18\u4e8e\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u6587\u672c\u8bc6\u522b\u60c5\u611f\u7684\u8eab\u4f53\u53cd\u5e94\uff0c\u586b\u8865\u4e86\u8fd9\u4e00\u9886\u57df\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u81ea\u52a8\u6700\u4f73-\u6700\u5dee\u7f29\u653e\u65b9\u6cd5\u548c\u7b80\u5316\u63d0\u793a\u6307\u4ee4\u53ca\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\uff0c\u4f18\u5316\u60c5\u611f\u8bc6\u522b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u60c5\u611f\u8bc6\u522b\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\uff0c\u4e14\u5c0f\u578b\u6a21\u578b\u8868\u73b0\u53ef\u4e0e\u5927\u578b\u6a21\u578b\u7ade\u4e89\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u548c\u601d\u7ef4\u94fe\uff0c\u53ef\u663e\u8457\u63d0\u5347\u60c5\u611f\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "keywords": "\u60c5\u611f\u8bc6\u522b\u3001Ekman\u57fa\u672c\u60c5\u611f\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u6700\u4f73-\u6700\u5dee\u7f29\u653e"}}
{"id": "2506.00107", "pdf": "https://arxiv.org/pdf/2506.00107", "abs": "https://arxiv.org/abs/2506.00107", "authors": ["Sibei Liu", "Yuanzhe Zhang", "Xiang Li", "Yunbo Liu", "Chengwei Feng", "Hao Yang"], "title": "Gated Multimodal Graph Learning for Personalized Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Multimodal recommendation has emerged as a promising solution to alleviate\nthe cold-start and sparsity problems in collaborative filtering by\nincorporating rich content information, such as product images and textual\ndescriptions. However, effectively integrating heterogeneous modalities into a\nunified recommendation framework remains a challenge. Existing approaches often\nrely on fixed fusion strategies or complex architectures , which may fail to\nadapt to modality quality variance or introduce unnecessary computational\noverhead.\n  In this work, we propose RLMultimodalRec, a lightweight and modular\nrecommendation framework that combines graph-based user modeling with adaptive\nmultimodal item encoding. The model employs a gated fusion module to\ndynamically balance the contribution of visual and textual modalities, enabling\nfine-grained and content-aware item representations. Meanwhile, a two-layer\nLightGCN encoder captures high-order collaborative signals by propagating\nembeddings over the user-item interaction graph without relying on nonlinear\ntransformations.\n  We evaluate our model on a real-world dataset from the Amazon product domain.\nExperimental results demonstrate that RLMultimodalRec consistently outperforms\nseveral competitive baselines, including collaborative filtering, visual-aware,\nand multimodal GNN-based methods. The proposed approach achieves significant\nimprovements in top-K recommendation metrics while maintaining scalability and\ninterpretability, making it suitable for practical deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRLMultimodalRec\u7684\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6a21\u6001\u878d\u5408\u548c\u56fe\u5efa\u6a21\u7f13\u89e3\u51b7\u542f\u52a8\u548c\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u63a8\u8350\u53ef\u4ee5\u89e3\u51b3\u534f\u540c\u8fc7\u6ee4\u4e2d\u7684\u51b7\u542f\u52a8\u548c\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u6574\u5408\u5f02\u6784\u6a21\u6001\u3002", "method": "\u63d0\u51faRLMultimodalRec\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u7528\u6237\u5efa\u6a21\u548c\u81ea\u9002\u5e94\u591a\u6a21\u6001\u7f16\u7801\uff0c\u4f7f\u7528\u95e8\u63a7\u878d\u5408\u52a8\u6001\u5e73\u8861\u89c6\u89c9\u4e0e\u6587\u672c\u6a21\u6001\u3002", "result": "\u5728\u4e9a\u9a6c\u900a\u6570\u636e\u96c6\u4e0a\uff0cRLMultimodalRec\u5728top-K\u63a8\u8350\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u63a8\u8350\u7684\u6027\u80fd\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002", "keywords": "\u591a\u6a21\u6001\u63a8\u8350, \u81ea\u9002\u5e94\u878d\u5408, \u56fe\u795e\u7ecf\u7f51\u7edc, \u51b7\u542f\u52a8"}}
{"id": "2506.01049", "pdf": "https://arxiv.org/pdf/2506.01049", "abs": "https://arxiv.org/abs/2506.01049", "authors": ["Siyuan Li", "Juanxi Tian", "Zedong Wang", "Xin Jin", "Zicheng Liu", "Wentao Zhang", "Dan Xu"], "title": "Taming LLMs by Scaling Learning Rates with Gradient Grouping", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint version of \"Taming LLMs with Gradient Grouping\" (ACL'2025).\n  The code will be available at https://github.com/ScalingOpt/SGG", "summary": "Training large language models (LLMs) poses challenges due to their massive\nscale and heterogeneous architectures. While adaptive optimizers like AdamW\nhelp address gradient variations, they still struggle with efficient and\neffective parameter-wise learning rate estimation, resulting in training\ninstability, slow convergence, and poor compatibility with parameter-efficient\nfine-tuning (PEFT) techniques. This work introduces Scaling with Gradient\nGrouping (SGG), an optimizer wrapper that improves adaptive learning rate\nestimation by dynamic grouping and group-specific scaling. SGG first groups\ngradient statistics in each layer into clusters and then applies\ncluster-specific scaling to calibrate learning rates for each parameter, thus\nimposing collective group-wise constraints while maintaining precise\nper-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that\nSGG integrates seamlessly with existing optimizers, and offers consistent gains\nand faster convergence over baselines, with various model sizes. Its stability\nacross varying batch sizes and learning rates establishes SGG as a robust\nchoice for LLM optimization.", "AI": {"tldr": "SGG\uff08\u57fa\u4e8e\u68af\u5ea6\u5206\u7ec4\u7684\u7f29\u653e\uff09\u662f\u4e00\u79cd\u4f18\u5316\u5668\u5305\u88c5\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u7ec4\u548c\u7ec4\u7279\u5b9a\u7f29\u653e\u6539\u8fdb\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u4f30\u8ba1\uff0c\u63d0\u5347LLM\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u81ea\u9002\u5e94\u4f18\u5316\u5668\u5728\u5904\u7406\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u9762\u4e34\u5b66\u4e60\u7387\u4f30\u8ba1\u6548\u7387\u4f4e\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u53ca\u4e0e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u517c\u5bb9\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "SGG\u901a\u8fc7\u68af\u5ea6\u7edf\u8ba1\u52a8\u6001\u5206\u7ec4\u548c\u7ec4\u7279\u5b9a\u7f29\u653e\u6821\u51c6\u5b66\u4e60\u7387\uff0c\u5b9e\u73b0\u96c6\u4f53\u7ec4\u7ea7\u7ea6\u675f\u4e0e\u7cbe\u786e\u53c2\u6570\u9002\u5e94\u7684\u5e73\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSGG\u80fd\u65e0\u7f1d\u96c6\u6210\u73b0\u6709\u4f18\u5316\u5668\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u548c\u7a33\u5b9a\u8868\u73b0\u3002", "conclusion": "SGG\u4e3aLLM\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u4f18\u5316\u5668, \u5b66\u4e60\u7387\u4f30\u8ba1, \u68af\u5ea6\u5206\u7ec4, \u8bad\u7ec3\u7a33\u5b9a\u6027"}}
{"id": "2506.01062", "pdf": "https://arxiv.org/pdf/2506.01062", "abs": "https://arxiv.org/abs/2506.01062", "authors": ["Thinh Pham", "Nguyen Nguyen", "Pratibha Zunjare", "Weiyuan Chen", "Yu-Min Tseng", "Tu Vu"], "title": "SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint. 22 pages, 7 figures, 11 tables", "summary": "We introduce SealQA, a new challenge benchmark for evaluating\nSEarch-Augmented Language models on fact-seeking questions where web search\nyields conflicting, noisy, or unhelpful results. SealQA comes in three flavors:\n(1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and\nreasoning capabilities, with Seal-0 focusing on the most challenging questions\nwhere chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3)\nLongSeal, which extends SealQA to test long-context, multi-document reasoning\nin \"needle-in-a-haystack\" settings. Our evaluation reveals critical limitations\nin current models: Even frontier LLMs perform poorly across all SealQA flavors.\nOn Seal-0, frontier agentic models equipped with tools like o3 and o4-mini\nachieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning\nefforts. We find that advanced reasoning models such as DeepSeek-R1-671B and\no3-mini are highly vulnerable to noisy search results. Notably, increasing\ntest-time compute does not yield reliable gains across o3-mini, o4-mini, and\no3, with performance often plateauing or even declining early. Additionally,\nwhile recent models are less affected by the \"lost-in-the-middle\" issue, they\nstill fail to reliably identify relevant documents in LongSeal when faced with\nnumerous distractors. To facilitate future work, we release SealQA at\nhuggingface.co/datasets/vtllms/sealqa.", "AI": {"tldr": "SealQA\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30SEarch-Augmented Language\u6a21\u578b\u5728\u5904\u7406\u4e8b\u5b9e\u6027\u95ee\u9898\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u7f51\u7edc\u641c\u7d22\u7ed3\u679c\u51b2\u7a81\u3001\u5608\u6742\u6216\u65e0\u5e2e\u52a9\u7684\u60c5\u51b5\u4e0b\u3002\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\u5f53\u524d\u524d\u6cbf\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u957f\u6587\u672c\u548c\u591a\u6587\u6863\u63a8\u7406\u65b9\u9762\u4ecd\u6709\u663e\u8457\u7f3a\u9677\u3002", "motivation": "\u9488\u5bf9\u7f51\u7edc\u641c\u7d22\u7ed3\u679c\u4e2d\u5e38\u89c1\u7684\u95ee\u9898\uff08\u5982\u51b2\u7a81\u3001\u566a\u58f0\u6216\u65e0\u6548\u4fe1\u606f\uff09\uff0c\u5f00\u53d1\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u79cd\u7c7b\u578b\u7684\u6d4b\u8bd5\u96c6\uff1aSeal-0\uff08\u6838\u5fc3\uff09\u3001Seal-Hard\uff08\u9ad8\u96be\u5ea6\uff09\u548cLongSeal\uff08\u957f\u6587\u672c\u591a\u6587\u6863\u63a8\u7406\uff09\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u5728\u6240\u6709\u6d4b\u8bd5\u96c6\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u5728Seal-0\u4e2d\u51c6\u786e\u7387\u6781\u4f4e\uff08\u5982o3-mini\u8fbe6.3%\uff09\u3002\u589e\u52a0\u8ba1\u7b97\u8d44\u6e90\u672a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u6a21\u578b\u5bf9\u566a\u58f0\u641c\u7d22\u7ed3\u679c\u9ad8\u5ea6\u654f\u611f\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u4e8b\u5b9e\u6027\u95ee\u9898\u548c\u591a\u6587\u6863\u63a8\u7406\u65f6\u4ecd\u5b58\u5728\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u6a21\u578b\u8bbe\u8ba1\u548c\u63a8\u7406\u80fd\u529b\u3002", "keywords": "SealQA, \u8bc4\u4f30\u57fa\u51c6, \u641c\u7d22\u589e\u5f3a\u8bed\u8a00\u6a21\u578b, \u4e8b\u5b9e\u6027\u95ee\u9898, \u591a\u6587\u6863\u63a8\u7406, \u566a\u58f0\u9c81\u68d2\u6027"}}
{"id": "2506.01052", "pdf": "https://arxiv.org/pdf/2506.01052", "abs": "https://arxiv.org/abs/2506.01052", "authors": ["Wei-Cheng Lee", "Francesco Orabona"], "title": "A Finite-Time Analysis of TD Learning with Linear Function Approximation without Projections nor Strong Convexity", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "We investigate the finite-time convergence properties of Temporal Difference\n(TD) learning with linear function approximation, a cornerstone algorithm in\nreinforcement learning. While prior work has established convergence\nguarantees, these results typically rely on the assumption that each iterate is\nprojected onto a bounded set or that the learning rate is set according to the\nunknown strong convexity constant -- conditions that are both artificial and do\nnot match the current practice.\n  In this paper, we challenge the necessity of such assumptions and present a\nrefined analysis of TD learning. We show that the simple projection-free\nvariant converges with a rate of\n$\\tilde{\\mathcal{O}}(\\frac{||\\theta^*||^2_2}{\\sqrt{T}})$, even in the presence\nof Markovian noise. Our analysis reveals a novel self-bounding property of the\nTD updates and exploits it to guarantee bounded iterates.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u4e0bTD\u5b66\u4e60\u7684\u6709\u9650\u65f6\u95f4\u6536\u655b\u7279\u6027\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u5047\u8bbe\u5e76\u63d0\u51fa\u4e86\u65e0\u6295\u5f71\u53d8\u4f53\u7684\u6536\u655b\u6027\u8bc1\u660e\u3002", "motivation": "\u4f20\u7edfTD\u5b66\u4e60\u7684\u6536\u655b\u6027\u5206\u6790\u4f9d\u8d56\u4e8e\u4eba\u4e3a\u5047\u8bbe\uff08\u5982\u6295\u5f71\u5230\u6709\u754c\u96c6\u6216\u9700\u5f3a\u51f8\u5e38\u6570\uff09\uff0c\u4e0e\u5b9e\u9645\u60c5\u51b5\u4e0d\u7b26\uff0c\u672c\u6587\u65e8\u5728\u6311\u6218\u8fd9\u4e9b\u5047\u8bbe\u3002", "method": "\u4f7f\u7528\u65e0\u6295\u5f71\u53d8\u4f53\u7684TD\u5b66\u4e60\uff0c\u5e76\u5229\u7528\u5176\u81ea\u9650\u754c\u7279\u6027\u5206\u6790\u6536\u655b\u6027\u3002", "result": "\u5728\u9a6c\u5c14\u53ef\u592b\u566a\u58f0\u4e0b\uff0c\u65e0\u6295\u5f71TD\u5b66\u4e60\u7684\u6536\u655b\u901f\u7387\u4e3a$\tilde{\\mathcal{O}}(\\frac{||\\theta^*||^2_2}{\\sqrt{T}})$\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u65e0\u9700\u4f20\u7edf\u5047\u8bbe\u5373\u53ef\u5b9e\u73b0TD\u5b66\u4e60\u7684\u6536\u655b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "keywords": "TD\u5b66\u4e60,\u7ebf\u6027\u51fd\u6570\u903c\u8fd1,\u6536\u655b\u6027\u5206\u6790,\u81ea\u9650\u754c\u7279\u6027,\u9a6c\u5c14\u53ef\u592b\u566a\u58f0"}}
{"id": "2506.01074", "pdf": "https://arxiv.org/pdf/2506.01074", "abs": "https://arxiv.org/abs/2506.01074", "authors": ["Amir Hossein Kargaran", "Yihong Liu", "Fran\u00e7ois Yvon", "Hinrich Sch\u00fctze"], "title": "How Programming Concepts and Neurons Are Shared in Code Language Models", "categories": ["cs.CL", "cs.PL", "cs.SE"], "comment": "ACL Findings 2025", "summary": "Several studies have explored the mechanisms of large language models (LLMs)\nin coding tasks, but most have focused on programming languages (PLs) in a\nmonolingual setting. In this paper, we investigate the relationship between\nmultiple PLs and English in the concept space of LLMs. We perform a few-shot\ntranslation task on 21 PL pairs using two Llama-based models. By decoding the\nembeddings of intermediate layers during this task, we observe that the concept\nspace is closer to English (including PL keywords) and assigns high\nprobabilities to English tokens in the second half of the intermediate layers.\nWe analyze neuron activations for 11 PLs and English, finding that while\nlanguage-specific neurons are primarily concentrated in the bottom layers,\nthose exclusive to each PL tend to appear in the top layers. For PLs that are\nhighly aligned with multiple other PLs, identifying language-specific neurons\nis not feasible. These PLs also tend to have a larger keyword set than other\nPLs and are closer to the model's concept space regardless of the input/output\nPL in the translation task. Our findings provide insights into how LLMs\ninternally represent PLs, revealing structural patterns in the model's concept\nspace. Code is available at https://github.com/cisnlp/code-specific-neurons.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u591a\u7f16\u7a0b\u8bed\u8a00(PLs)\u4e0e\u82f1\u8bed\u7684\u6982\u5ff5\u7a7a\u95f4\u4e2d\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u5c11\u6837\u672c\u7ffb\u8bd1\u4efb\u52a1\u53d1\u73b0\u6982\u5ff5\u7a7a\u95f4\u66f4\u63a5\u8fd1\u82f1\u8bed\uff0c\u5e76\u5206\u6790\u4e86\u795e\u7ecf\u5143\u6fc0\u6d3b\u6a21\u5f0f\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u591a\u7f16\u7a0b\u8bed\u8a00\u4e0e\u82f1\u8bed\u4e4b\u95f4\u7684\u5185\u90e8\u8868\u793a\u5173\u7cfb\uff0c\u586b\u8865\u4e86\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7f16\u7a0b\u8bed\u8a00\u6982\u5ff5\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u57fa\u4e8eLlama\u7684\u6a21\u578b\uff0c\u5bf921\u4e2a\u7f16\u7a0b\u8bed\u8a00\u5bf9\u8fdb\u884c\u5c11\u6837\u672c\u7ffb\u8bd1\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u89e3\u7801\u4e2d\u95f4\u5c42\u5d4c\u5165\u548c\u795e\u7ecf\u5143\u6fc0\u6d3b\u6765\u5206\u6790\u6982\u5ff5\u7a7a\u95f4\u3002", "result": "\u53d1\u73b0\u6982\u5ff5\u7a7a\u95f4\u66f4\u63a5\u8fd1\u82f1\u8bed(\u5305\u542bPL\u5173\u952e\u8bcd)\uff0c\u4e14\u8bed\u8a00\u7279\u5b9a\u795e\u7ecf\u5143\u96c6\u4e2d\u5728\u5e95\u5c42\uff0c\u800c\u4e13\u7528\u795e\u7ecf\u5143\u51fa\u73b0\u5728\u9876\u5c42\uff1b\u9ad8\u5ea6\u5bf9\u9f50\u7684PL\u96be\u4ee5\u8bc6\u522b\u7279\u5b9a\u795e\u7ecf\u5143\u3002", "conclusion": "LLMs\u5728\u591a\u7f16\u7a0b\u8bed\u8a00\u4e0e\u82f1\u8bed\u7684\u6982\u5ff5\u7a7a\u95f4\u4e2d\u8868\u73b0\u51fa\u7ed3\u6784\u6027\u6a21\u5f0f\uff0c\u4e3a\u7406\u89e3\u6a21\u578b\u5185\u90e8\u8868\u793a\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u7f16\u7a0b\u8bed\u8a00, \u6982\u5ff5\u7a7a\u95f4, \u795e\u7ecf\u5143\u6fc0\u6d3b, \u5c11\u6837\u672c\u7ffb\u8bd1"}}
{"id": "2506.00133", "pdf": "https://arxiv.org/pdf/2506.00133", "abs": "https://arxiv.org/abs/2506.00133", "authors": ["Mohammadhossein Homaei", "Mehran Tarif", "Agustin Di Bartolo", "Oscar Mogollon Gutierrez", "Mar Avila"], "title": "A Reinforcement Learning-Based Telematic Routing Protocol for the Internet of Underwater Things", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": "8 Pages, 10 Figures, 2 Tables", "summary": "The Internet of Underwater Things (IoUT) faces major challenges such as low\nbandwidth, high latency, mobility, and limited energy resources. Traditional\nrouting protocols like RPL, which were designed for land-based networks, do not\nperform well in these underwater conditions. This paper introduces RL-RPL-UA, a\nnew routing protocol that uses reinforcement learning to improve performance in\nunderwater environments. Each node includes a lightweight RL agent that selects\nthe best parent node based on local information such as packet delivery ratio,\nbuffer level, link quality, and remaining energy. RL-RPL-UA keeps full\ncompatibility with standard RPL messages and adds a dynamic objective function\nto support real-time decision-making. Simulations using Aqua-Sim show that\nRL-RPL-UA increases packet delivery by up to 9.2%, reduces energy use per\npacket by 14.8%, and extends network lifetime by 80 seconds compared to\ntraditional methods. These results suggest that RL-RPL-UA is a promising and\nenergy-efficient routing solution for underwater networks.", "AI": {"tldr": "\u9488\u5bf9\u6c34\u4e0b\u7269\u8054\u7f51\uff08IoUT\uff09\u7684\u4f4e\u5e26\u5bbd\u3001\u9ad8\u5ef6\u8fdf\u7b49\u95ee\u9898\uff0c\u8bba\u6587\u63d0\u51faRL-RPL-UA\u534f\u8bae\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u8def\u7531\u6027\u80fd\uff0c\u517c\u5bb9RPL\u5e76\u63d0\u5347\u6570\u636e\u5305\u4f20\u9012\u7387\u4e0e\u80fd\u6548\u3002", "motivation": "\u4f20\u7edf\u8def\u7531\u534f\u8bae\uff08\u5982RPL\uff09\u5728\u6c34\u4e0b\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u9002\u914d\u4f4e\u5e26\u5bbd\u3001\u9ad8\u5ef6\u8fdf\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u57fa\u4e8e\u5c40\u90e8\u4fe1\u606f\uff08\u5982\u4e22\u5305\u7387\u3001\u80fd\u91cf\u7b49\uff09\u52a8\u6001\u9009\u62e9\u6700\u4f18\u7236\u8282\u70b9\uff0c\u517c\u5bb9RPL\u5e76\u5f15\u5165\u52a8\u6001\u76ee\u6807\u51fd\u6570\u3002", "result": "\u4eff\u771f\u663e\u793a\uff0cRL-RPL-UA\u63d0\u5347\u6570\u636e\u5305\u4f20\u9012\u73879.2%\uff0c\u964d\u4f4e\u80fd\u801714.8%\uff0c\u5ef6\u957f\u7f51\u7edc\u5bff\u547d80\u79d2\u3002", "conclusion": "RL-RPL-UA\u662f\u4e00\u79cd\u9ad8\u6548\u8282\u80fd\u7684\u6c34\u4e0b\u7f51\u7edc\u8def\u7531\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "IoUT, \u5f3a\u5316\u5b66\u4e60, \u8def\u7531\u534f\u8bae, \u80fd\u6548, RPL"}}
{"id": "2506.01054", "pdf": "https://arxiv.org/pdf/2506.01054", "abs": "https://arxiv.org/abs/2506.01054", "authors": ["Attila Sz\u00e1sz", "Bal\u00e1zs B\u00e1nhelyi", "M\u00e1rk Jelasity"], "title": "No Soundness in the Real World: On the Challenges of the Verification of Deployed Neural Networks", "categories": ["cs.LG"], "comment": "accepted at ICML 2025. For the implementation, see\n  https://github.com/szasza1/no_soundness", "summary": "The ultimate goal of verification is to guarantee the safety of deployed\nneural networks. Here, we claim that all the state-of-the-art verifiers we are\naware of fail to reach this goal. Our key insight is that theoretical soundness\n(bounding the full-precision output while computing with floating point) does\nnot imply practical soundness (bounding the floating point output in a\npotentially stochastic environment). We prove this observation for the\napproaches that are currently used to achieve provable theoretical soundness,\nsuch as interval analysis and its variants. We also argue that achieving\npractical soundness is significantly harder computationally. We support our\nclaims empirically as well by evaluating several well-known verification\nmethods. To mislead the verifiers, we create adversarial networks that detect\nand exploit features of the deployment environment, such as the order and\nprecision of floating point operations. We demonstrate that all the tested\nverifiers are vulnerable to our new deployment-specific attacks, which proves\nthat they are not practically sound.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u5f53\u524d\u6700\u5148\u8fdb\u7684\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\u5668\u672a\u80fd\u771f\u6b63\u4fdd\u8bc1\u5b89\u5168\u6027\uff0c\u6307\u51fa\u7406\u8bba\u4e0a\u7684\u51c6\u786e\u6027\uff08\u4f7f\u7528\u6d6e\u70b9\u8ba1\u7b97\u65f6\u754c\u5b9a\u5168\u7cbe\u5ea6\u8f93\u51fa\uff09\u4e0e\u5b9e\u9645\u4e0a\u7684\u51c6\u786e\u6027\uff08\u5728\u6f5c\u5728\u968f\u673a\u73af\u5883\u4e2d\u754c\u5b9a\u6d6e\u70b9\u8f93\u51fa\uff09\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u70b9\u3002", "motivation": "\u63a2\u8ba8\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\u5668\u7684\u5c40\u9650\u6027\uff0c\u63ed\u793a\u5176\u5728\u5b9e\u8df5\u4e2d\u65e0\u6cd5\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6d4b\u8bd5\u591a\u79cd\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u9488\u5bf9\u90e8\u7f72\u73af\u5883\u7684\u5bf9\u6297\u7f51\u7edc\u653b\u51fb\u3002", "result": "\u6240\u6709\u6d4b\u8bd5\u7684\u9a8c\u8bc1\u5668\u5747\u6613\u53d7\u9488\u5bf9\u90e8\u7f72\u73af\u5883\u7684\u653b\u51fb\uff0c\u8868\u660e\u5176\u4e0d\u5177\u5907\u5b9e\u9645\u4e0a\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u5f53\u524d\u9a8c\u8bc1\u65b9\u6cd5\u5b58\u5728\u91cd\u5927\u7f3a\u9677\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u8ba1\u7b97\u80fd\u529b\u624d\u80fd\u5b9e\u73b0\u5b9e\u9645\u4e0a\u7684\u51c6\u786e\u6027\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\u3001\u6d6e\u70b9\u8fd0\u7b97\u3001\u51c6\u786e\u6027\u3001\u5bf9\u6297\u653b\u51fb\u3001\u90e8\u7f72\u73af\u5883"}}
{"id": "2506.01084", "pdf": "https://arxiv.org/pdf/2506.01084", "abs": "https://arxiv.org/abs/2506.01084", "authors": ["Saibo Geng", "Nathan Ranchin", "Yunzhen yao", "Maxime Peyrard", "Chris Wendler", "Michael Gastpar", "Robert West"], "title": "zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression", "categories": ["cs.CL", "cs.LG"], "comment": "Code will be released at https://github.com/epfl-dlab/zip2zip", "summary": "Tokenization efficiency plays a critical role in the performance and cost of\nlarge language models (LLMs), yet most models rely on static tokenizers\noptimized for general-purpose corpora. These tokenizers' fixed vocabularies\noften fail to adapt to domain- or language-specific inputs, leading to longer\ntoken sequences and higher computational costs. We introduce zip2zip, a\nframework that enables LLMs to dynamically adjust token vocabulary at inference\ntime, allowing for fewer generated tokens and thus faster inference. zip2zip\nconsists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch\n(LZW) compression that incrementally compresses tokens into reusable\n\"hypertokens\" on the fly; (2) an embedding layer that computes embeddings for\nnewly formed hypertokens at runtime; and (3) a causal language modeling variant\nthat trains the model to operate on hypertokenized, compressed sequences. We\nshow that an existing LLM can be zip2zip-fied in 10 GPU-hours via\nparameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to\nuse hypertokens at inference time, reducing input and output sequence length by\n20-60\\%, with significant improvements in inference latency.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86zip2zip\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574token\u8bcd\u6c47\u8868\u51cf\u5c11\u5e8f\u5217\u957f\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u9759\u6001tokenizer\u96be\u4ee5\u9002\u5e94\u7279\u5b9a\u9886\u57df\u6216\u8bed\u8a00\u8f93\u5165\uff0c\u5bfc\u81f4\u5e8f\u5217\u8fc7\u957f\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fazip2zip\u6846\u67b6\uff0c\u5305\u542b\u52a8\u6001LZW\u538b\u7f29tokenizer\u3001\u8fd0\u884c\u65f6\u5d4c\u5165\u5c42\u548c\u56e0\u679c\u8bed\u8a00\u5efa\u6a21\u53d8\u4f53\u3002", "result": "zip2zip-fied LLMs\u5728\u63a8\u7406\u65f6\u51cf\u5c11\u4e8620-60%\u7684\u5e8f\u5217\u957f\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "zip2zip\u662f\u9ad8\u6548\u4e14\u8f7b\u91cf\u7684\u52a8\u6001tokenization\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "tokenization, LLMs, dynamic vocabulary, LZW compression"}}
{"id": "2506.01059", "pdf": "https://arxiv.org/pdf/2506.01059", "abs": "https://arxiv.org/abs/2506.01059", "authors": ["Jun Rui Lee", "Sadegh Emami", "Michael David Hollins", "Timothy C. H. Wong", "Carlos Ignacio Villalobos S\u00e1nchez", "Francesca Toni", "Dekai Zhang", "Adam Dejl"], "title": "XAI-Units: Benchmarking Explainability Methods with Unit Tests", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at FAccT 2025", "summary": "Feature attribution (FA) methods are widely used in explainable AI (XAI) to\nhelp users understand how the inputs of a machine learning model contribute to\nits outputs. However, different FA models often provide disagreeing importance\nscores for the same model. In the absence of ground truth or in-depth knowledge\nabout the inner workings of the model, it is often difficult to meaningfully\ndetermine which of the different FA methods produce more suitable explanations\nin different contexts. As a step towards addressing this issue, we introduce\nthe open-source XAI-Units benchmark, specifically designed to evaluate FA\nmethods against diverse types of model behaviours, such as feature\ninteractions, cancellations, and discontinuous outputs. Our benchmark provides\na set of paired datasets and models with known internal mechanisms,\nestablishing clear expectations for desirable attribution scores. Accompanied\nby a suite of built-in evaluation metrics, XAI-Units streamlines systematic\nexperimentation and reveals how FA methods perform against distinct, atomic\nkinds of model reasoning, similar to unit tests in software engineering.\nCrucially, by using procedurally generated models tied to synthetic datasets,\nwe pave the way towards an objective and reliable comparison of FA methods.", "AI": {"tldr": "XAI-Units\u662f\u4e00\u4e2a\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u7279\u5f81\u5f52\u56e0\uff08FA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u591a\u6837\u5316\u7684\u6a21\u578b\u884c\u4e3a\u5982\u7279\u5f81\u4ea4\u4e92\u548c\u8f93\u51fa\u4e0d\u8fde\u7eed\u6027\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u3002", "motivation": "\u5728\u7f3a\u4e4f\u771f\u5b9e\u57fa\u51c6\u6216\u6df1\u5165\u6a21\u578b\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u96be\u4ee5\u5224\u65ad\u4e0d\u540cFA\u65b9\u6cd5\u7684\u4f18\u52a3\uff0c\u56e0\u6b64\u9700\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u5f00\u53d1XAI-Units\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u5df2\u77e5\u5185\u90e8\u673a\u5236\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u5e76\u96c6\u6210\u591a\u79cd\u8bc4\u4ef7\u6307\u6807\u3002", "result": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u6a21\u578b\u751f\u6210\uff0c\u63d0\u4f9b\u5ba2\u89c2\u7684FA\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u63ed\u793a\u4e0d\u540c\u65b9\u6cd5\u5728\u5404\u7c7b\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "XAI-Units\u4e3aFA\u65b9\u6cd5\u8bc4\u4f30\u63d0\u4f9b\u6807\u51c6\u5316\u6846\u67b6\uff0c\u52a9\u529b\u53ef\u89e3\u91caAI\u7684\u53ef\u9760\u6027\u63d0\u5347\u3002", "keywords": "\u7279\u5f81\u5f52\u56e0,\u53ef\u89e3\u91caAI,\u57fa\u51c6\u6d4b\u8bd5,\u6a21\u578b\u884c\u4e3a,\u8bc4\u4f30"}}
{"id": "2506.01089", "pdf": "https://arxiv.org/pdf/2506.01089", "abs": "https://arxiv.org/abs/2506.01089", "authors": ["Metehan Oguz", "Yavuz Bakman", "Duygu Nur Yaldiz"], "title": "Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) have demonstrated impressive performances in\ntasks related to coreference resolution. However, previous studies mostly\nassessed LLM performance on coreference resolution with nouns and third person\npronouns. This study evaluates LLM performance on coreference resolution with\nindexical like I, you, here and tomorrow, which come with unique challenges due\nto their linguistic properties. We present the first study examining how LLMs\ninterpret indexicals in English, releasing the English Indexical Dataset with\n1600 multiple-choice questions. We evaluate pioneering LLMs, including GPT-4o,\nClaude 3.5 Sonnet, Gemini 1.5 Pro, and DeepSeek V3. Our results reveal that\nLLMs exhibit an impressive performance with some indexicals (I), while\nstruggling with others (you, here, tomorrow), and that syntactic cues (e.g.\nquotation) contribute to LLM performance with some indexicals, while they\nreduce performance with others. Code and data are available at:\nhttps://github.com/metehanoguzz/LLMs-Indexicals-English.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u7d22\u5f15\u8bcd\uff08\u5982I\u3001you\u7b49\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u5e76\u63a2\u8ba8\u4e86\u53e5\u6cd5\u7ebf\u7d22\u7684\u5f71\u54cd\u3002", "motivation": "\u4e4b\u524d\u7684\u7814\u7a76\u4e3b\u8981\u8bc4\u4f30\u4e86LLMs\u5728\u540d\u8bcd\u548c\u7b2c\u4e09\u4eba\u79f0\u4ee3\u8bcd\u4e0a\u7684\u5171\u6307\u6d88\u89e3\u80fd\u529b\uff0c\u672c\u7814\u7a76\u586b\u8865\u4e86\u7d22\u5f15\u8bcd\uff08\u5982I\u3001you\uff09\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5305\u542b1600\u4e2a\u591a\u9879\u9009\u62e9\u9898\u7684\u82f1\u8bed\u7d22\u5f15\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86GPT-4o\u7b49LLMs\u7684\u8868\u73b0\u3002", "result": "LLMs\u5728\u90e8\u5206\u7d22\u5f15\u8bcd\uff08\u5982I\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5176\u4ed6\uff08\u5982you\u3001here\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1b\u53e5\u6cd5\u7ebf\u7d22\u5bf9\u4e0d\u540c\u7d22\u5f15\u8bcd\u7684\u5f71\u54cd\u5404\u5f02\u3002", "conclusion": "LLMs\u5904\u7406\u7d22\u5f15\u8bcd\u7684\u80fd\u529b\u5c1a\u4e0d\u5b8c\u5584\uff0c\u53e5\u6cd5\u7ebf\u7d22\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u5171\u6307\u6d88\u89e3\u3001\u7d22\u5f15\u8bcd\u3001\u53e5\u6cd5\u7ebf\u7d22\u3001\u8bc4\u4f30"}}
{"id": "2506.00138", "pdf": "https://arxiv.org/pdf/2506.00138", "abs": "https://arxiv.org/abs/2506.00138", "authors": ["Reece Keller", "Alyn Tornell", "Felix Pei", "Xaq Pitkow", "Leo Kozachkov", "Aran Nayebi"], "title": "Autonomous Behavior and Whole-Brain Dynamics Emerge in Embodied Zebrafish Agents with Model-based Intrinsic Motivation", "categories": ["q-bio.NC", "cs.AI", "cs.LG", "cs.RO"], "comment": "17 pages, 7 figures", "summary": "Autonomy is a hallmark of animal intelligence, enabling adaptive and\nintelligent behavior in complex environments without relying on external reward\nor task structure. Existing reinforcement learning approaches to exploration in\nsparse reward and reward-free environments, including class of methods known as\nintrinsic motivation, exhibit inconsistent exploration patterns and thus fail\nto produce robust autonomous behaviors observed in animals. Moreover, systems\nneuroscience has largely overlooked the neural basis of autonomy, focusing\ninstead on experimental paradigms where animals are motivated by external\nreward rather than engaging in unconstrained, naturalistic and task-independent\nbehavior. To bridge these gaps, we introduce a novel model-based intrinsic\ndrive explicitly designed to capture robust autonomous exploration observed in\nanimals. Our method (3M-Progress) motivates naturalistic behavior by tracking\ndivergence between the agent's current world model and an ethological prior. We\ndemonstrate that artificial embodied agents trained with 3M-Progress capture\nthe explainable variance in behavioral patterns and whole-brain neural-glial\ndynamics recorded from autonomously-behaving larval zebrafish, introducing the\nfirst goal-driven, population-level model of neural-glial computation. Our\nfindings establish a computational framework connecting model-based intrinsic\nmotivation to naturalistic behavior, providing a foundation for building\nartificial agents with animal-like autonomy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u57fa\u4e8e\u6a21\u578b\u7684\u5185\u5728\u9a71\u52a8\u65b9\u6cd5\uff083M-Progress\uff09\uff0c\u65e8\u5728\u6a21\u4eff\u52a8\u7269\u7684\u81ea\u4e3b\u63a2\u7d22\u884c\u4e3a\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u4eba\u5de5\u4ee3\u7406\u4ee5\u5b9e\u73b0\u7c7b\u52a8\u7269\u7684\u81ea\u4e3b\u6027\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u7a00\u758f\u5956\u52b1\u548c\u65e0\u5956\u52b1\u73af\u5883\u4e2d\u63a2\u7d22\u884c\u4e3a\u4e0d\u4e00\u81f4\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u81ea\u4e3b\u884c\u4e3a\u795e\u7ecf\u57fa\u7840\u7684\u6df1\u5165\u7814\u7a76\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa3M-Progress\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ddf\u8e2a\u667a\u80fd\u4f53\u7684\u5f53\u524d\u4e16\u754c\u6a21\u578b\u4e0e\u884c\u4e3a\u5148\u9a8c\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u6fc0\u52b1\u81ea\u7136\u884c\u4e3a\u3002", "result": "3M-Progress\u8bad\u7ec3\u7684\u4eba\u5de5\u4ee3\u7406\u80fd\u591f\u6355\u6349\u81ea\u4e3b\u884c\u4e3a\u7684\u6591\u9a6c\u9c7c\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u5168\u8111\u795e\u7ecf\u80f6\u8d28\u52a8\u6001\uff0c\u9996\u6b21\u5efa\u7acb\u4e86\u76ee\u6807\u9a71\u52a8\u7684\u795e\u7ecf\u80f6\u8d28\u8ba1\u7b97\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u5177\u6709\u52a8\u7269\u81ea\u4e3b\u6027\u7684\u4eba\u5de5\u4ee3\u7406\u63d0\u4f9b\u4e86\u8ba1\u7b97\u6846\u67b6\uff0c\u5e76\u8fde\u63a5\u4e86\u6a21\u578b\u5185\u5728\u52a8\u673a\u4e0e\u81ea\u7136\u884c\u4e3a\u3002", "keywords": "\u81ea\u4e3b\u6027, \u5185\u5728\u52a8\u673a, \u6a21\u578b\u9a71\u52a8, \u81ea\u7136\u884c\u4e3a, \u795e\u7ecf\u80f6\u8d28\u52a8\u6001"}}
{"id": "2506.01114", "pdf": "https://arxiv.org/pdf/2506.01114", "abs": "https://arxiv.org/abs/2506.01114", "authors": ["Yavuz Bakman", "Duygu Nur Yaldiz", "Sungmin Kang", "Tuo Zhang", "Baturalp Buyukates", "Salman Avestimehr", "Sai Praneeth Karimireddy"], "title": "Reconsidering LLM Uncertainty Estimation Methods in the Wild", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ACL 2025", "summary": "Large Language Model (LLM) Uncertainty Estimation (UE) methods have become a\ncrucial tool for detecting hallucinations in recent years. While numerous UE\nmethods have been proposed, most existing studies evaluate them in isolated\nshort-form QA settings using threshold-independent metrics such as AUROC or\nPRR. However, real-world deployment of UE methods introduces several\nchallenges. In this work, we systematically examine four key aspects of\ndeploying UE methods in practical settings. Specifically, we assess (1) the\nsensitivity of UE methods to decision threshold selection, (2) their robustness\nto query transformations such as typos, adversarial prompts, and prior chat\nhistory, (3) their applicability to long-form generation, and (4) strategies\nfor handling multiple UE scores for a single query. Our evaluations on 19 UE\nmethods reveal that most of them are highly sensitive to threshold selection\nwhen there is a distribution shift in the calibration dataset. While these\nmethods generally exhibit robustness against previous chat history and typos,\nthey are significantly vulnerable to adversarial prompts. Additionally, while\nexisting UE methods can be adapted for long-form generation through various\nstrategies, there remains considerable room for improvement. Lastly, ensembling\nmultiple UE scores at test time provides a notable performance boost, which\nhighlights its potential as a practical improvement strategy. Code is available\nat: https://github.com/duygunuryldz/uncertainty_in_the_wild.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff08UE\uff09\u65b9\u6cd5\u5728\u5b9e\u8df5\u90e8\u7f72\u4e2d\u7684\u56db\u4e2a\u5173\u952e\u65b9\u9762\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5bf9\u9608\u503c\u9009\u62e9\u654f\u611f\u4e14\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u63d0\u793a\u7684\u5f71\u54cd\uff0c\u4f46\u5c55\u793a\u4e86\u5728\u957f\u6587\u672c\u751f\u6210\u548c\u591a\u5206\u6570\u96c6\u6210\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709UE\u65b9\u6cd5\u591a\u5728\u77ed\u95ee\u7b54\u573a\u666f\u4e2d\u8bc4\u4f30\uff0c\u800c\u5b9e\u9645\u90e8\u7f72\u4e2d\u9762\u4e34\u591a\u79cd\u6311\u6218\uff0c\u4f5c\u8005\u63a2\u7a76\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u6d4b\u8bd5\uff081\uff09\u9608\u503c\u9009\u62e9\u654f\u611f\u6027\uff0c\uff082\uff09\u5bf9\u67e5\u8be2\u53d8\u6362\u7684\u9c81\u68d2\u6027\uff0c\uff083\uff09\u957f\u6587\u672c\u751f\u6210\u9002\u7528\u6027\uff0c\uff084\uff09\u591a\u5206\u6570\u5904\u7406\u7b56\u7565\uff0c\u8bc4\u4f30\u4e8619\u79cdUE\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u5927\u591a\u6570\u65b9\u6cd5\u5bf9\u9608\u503c\u9009\u62e9\u654f\u611f\uff0c\u5bf9\u6297\u6027\u63d0\u793a\u6613\u53d7\u5f71\u54cd\uff0c\u4f46\u5bf9\u957f\u6587\u672c\u751f\u6210\u548c\u591a\u5206\u6570\u96c6\u6210\u8868\u73b0\u6709\u6f5c\u529b\u3002", "conclusion": "\u73b0\u6709UE\u65b9\u6cd5\u5728\u5b9e\u8df5\u90e8\u7f72\u4e2d\u4ecd\u9700\u6539\u8fdb\uff0c\u591a\u5206\u6570\u96c6\u6210\u7b56\u7565\u5c55\u793a\u4e86\u5b9e\u7528\u4ef7\u503c\u3002", "keywords": "LLM, Uncertainty Estimation, Threshold Sensitivity, Adversarial Prompts, Long-form Generation"}}
{"id": "2506.01104", "pdf": "https://arxiv.org/pdf/2506.01104", "abs": "https://arxiv.org/abs/2506.01104", "authors": ["Steven Robinson", "Antonio Carlos Rivera"], "title": "Contextual Candor: Enhancing LLM Trustworthiness Through Hierarchical Unanswerability Detection", "categories": ["cs.CL"], "comment": null, "summary": "The pervasive deployment of large language models (LLMs) in conversational AI\nsystems has revolutionized information access, yet their propensity for\ngenerating factually unsupported or hallucinated responses remains a critical\nimpediment to trustworthiness and widespread adoption. This paper introduces\nReinforced Unanswerability Learning (RUL), a novel hybrid training paradigm\ndesigned to imbue LLMs with the intrinsic capability to accurately detect\nunanswerable questions and generate reliably appropriate responses. Unlike\nconventional approaches that rely on external classifiers or simple prompting,\nRUL integrates a discriminative unanswerability prediction head with the LLM's\ngenerative core, guided by a multi-stage learning strategy. This includes\nsupervised fine-tuning on a novel, richly annotated dataset,\nEnhanced-CAsT-Answerability (ECA), which features hierarchical answerability\nlabels and ground-truth refusal responses. Crucially, RUL incorporates a\nsubsequent reinforcement learning with human feedback (RLHF) phase to refine\nthe nuance, helpfulness, and informativeness of refusal responses. Extensive\nexperiments demonstrate RUL's superior performance, achieving significantly\nhigher accuracy in unanswerability detection across sentence, paragraph, and\nranking levels, and substantially increasing the generation of appropriate\nrefusals for unanswerable queries, alongside strong performance on answerable\nquestions. Human evaluations further corroborate RUL's effectiveness,\nhighlighting a marked improvement in perceived helpfulness and trustworthiness,\nultimately paving the way for more reliable and user-centric conversational AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRUL\u7684\u65b0\u578b\u6df7\u5408\u8bad\u7ec3\u8303\u5f0f\uff0c\u65e8\u5728\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bc6\u522b\u65e0\u6cd5\u56de\u7b54\u95ee\u9898\u5e76\u751f\u6210\u9002\u5f53\u54cd\u5e94\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u5b66\u4e60\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u751f\u6210\u56de\u7b54\u65f6\u53ef\u80fd\u4ea7\u751f\u4e0d\u51c6\u786e\u6216\u865a\u6784\u5185\u5bb9\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5176\u5728\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u53ef\u4fe1\u5ea6\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "method": "\u63d0\u51faRUL\u6846\u67b6\uff0c\u7ed3\u5408\u5224\u522b\u5f0f\u65e0\u7b54\u6848\u9884\u6d4b\u5934\u548c\u751f\u6210\u6838\u5fc3\u6a21\u578b\uff0c\u91c7\u7528\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRUL\u5728\u65e0\u7b54\u6848\u68c0\u6d4b\u548c\u62d2\u7edd\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "RUL\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u3001\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u5bf9\u8bddAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u65e0\u7b54\u6848\u68c0\u6d4b, \u5f3a\u5316\u5b66\u4e60, \u5bf9\u8bddAI, \u53ef\u4fe1\u5ea6"}}
{"id": "2506.00150", "pdf": "https://arxiv.org/pdf/2506.00150", "abs": "https://arxiv.org/abs/2506.00150", "authors": ["Rafael Capilla", "J. Andr\u00e9s D\u00edaz-Pace", "Yamid Ram\u00edrez", "Jennifer P\u00e9rez", "Vanessa Rodr\u00edguez-Horcajo"], "title": "Supporting architecture evaluation for ATAM scenarios with LLMs", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Architecture evaluation methods have long been used to evaluate software\ndesigns. Several evaluation methods have been proposed and used to analyze\ntradeoffs between different quality attributes. Having competing qualities\nleads to conflicts for selecting which quality-attribute scenarios are the most\nsuitable ones that an architecture should tackle and for prioritizing the\nscenarios required by the stakeholders. In this context, architecture\nevaluation is carried out manually, often involving long brainstorming sessions\nto decide which are the most adequate quality scenarios. To reduce this effort\nand make the assessment and selection of scenarios more efficient, we suggest\nthe usage of LLMs to partially automate evaluation activities. As a first step\nto validate this hypothesis, this work studies MS Copilot as an LLM tool to\nanalyze quality scenarios suggested by students in a software architecture\ncourse and compares the students' results with the assessment provided by the\nLLM. Our initial study reveals that the LLM produces in most cases better and\nmore accurate results regarding the risks, sensitivity points and tradeoff\nanalysis of the quality scenarios. Overall, the use of generative AI has the\npotential to partially automate and support the architecture evaluation tasks,\nimproving the human decision-making process.", "AI": {"tldr": "\u5229\u7528LLMs\u90e8\u5206\u81ea\u52a8\u5316\u8f6f\u4ef6\u67b6\u6784\u8bc4\u4f30\u7684\u7814\u7a76\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u751f\u4e0eLLM\u7684\u5206\u6790\u7ed3\u679c\uff0c\u53d1\u73b0LLM\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u80fd\u63d0\u4f9b\u66f4\u4f18\u7684\u8d28\u91cf\u573a\u666f\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u67b6\u6784\u8bc4\u4f30\u4f9d\u8d56\u4eba\u5de5\uff0c\u6548\u7387\u4f4e\u4e14\u8017\u65f6\uff1b\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLM\u662f\u5426\u80fd\u4e3a\u8d28\u91cf\u573a\u666f\u8bc4\u4f30\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u652f\u6301\u3002", "method": "\u91c7\u7528MS Copilot\u4f5c\u4e3aLLM\u5de5\u5177\uff0c\u5206\u6790\u5b66\u751f\u63d0\u51fa\u7684\u8d28\u91cf\u573a\u666f\uff0c\u5e76\u4e0e\u5b66\u751f\u7684\u4eba\u5de5\u8bc4\u4f30\u7ed3\u679c\u5bf9\u6bd4\u3002", "result": "LLM\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u80fd\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u98ce\u9669\u3001\u654f\u611f\u70b9\u53ca\u6743\u8861\u5206\u6790\uff0c\u4f18\u4e8e\u4eba\u5de5\u8bc4\u4f30\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u53ef\u90e8\u5206\u81ea\u52a8\u5316\u67b6\u6784\u8bc4\u4f30\uff0c\u8f85\u52a9\u4eba\u7c7b\u51b3\u7b56\uff0c\u63d0\u5347\u6548\u7387\u3002", "keywords": "\u67b6\u6784\u8bc4\u4f30, LLM, MS Copilot, \u8d28\u91cf\u573a\u666f, \u81ea\u52a8\u5316"}}
{"id": "2506.01115", "pdf": "https://arxiv.org/pdf/2506.01115", "abs": "https://arxiv.org/abs/2506.01115", "authors": ["Yihe Dong", "Lorenzo Noci", "Mikhail Khodak", "Mufan Li"], "title": "Attention Retrieves, MLP Memorizes: Disentangling Trainable Components in the Transformer", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The Transformer architecture is central to the success of modern Large\nLanguage Models (LLMs), in part due to its surprising ability to perform a wide\nrange of algorithmic tasks -- including mathematical reasoning, memorization,\nand retrieval -- using only gradient-based training on next-token prediction.\nWhile the core component of a Transformer is the self-attention mechanism, we\nquestion how much, and which aspects, of the performance gains can be\nattributed to it. To this end, we compare standard Transformers to variants in\nwhich either the multi-layer perceptron (MLP) layers or the attention\nprojectors (queries and keys) are frozen at initialization. To further isolate\nthe contribution of attention, we introduce MixiT -- the Mixing Transformer --\na simplified, principled model in which the attention coefficients are entirely\nrandom and fixed at initialization, eliminating any input-dependent computation\nor learning in attention. Surprisingly, we find that MixiT matches the\nperformance of fully trained Transformers on various algorithmic tasks,\nespecially those involving basic arithmetic or focusing heavily on\nmemorization. For retrieval-based tasks, we observe that having input-dependent\nattention coefficients is consistently beneficial, while MixiT underperforms.\nWe attribute this failure to its inability to form specialized circuits such as\ninduction heads -- a specific circuit known to be crucial for learning and\nexploiting repeating patterns in input sequences. Even more interestingly, we\nfind that attention with frozen key and query projectors is not only able to\nform induction heads, but can also perform competitively on language modeling.\nOur results underscore the importance of architectural heterogeneity, where\ndistinct components contribute complementary inductive biases crucial for\nsolving different classes of tasks.", "AI": {"tldr": "Transformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u5e76\u975e\u5176\u6210\u529f\u7684\u5173\u952e\u56e0\u7d20\uff0c\u7814\u7a76\u8868\u660e\u56fa\u5b9a\u6ce8\u610f\u529b\u7cfb\u6570\u6216\u51bb\u7ed3\u90e8\u5206\u7ec4\u4ef6\u7684\u7b80\u5316\u6a21\u578b\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0e\u5b8c\u6574\u6a21\u578b\u76f8\u5f53\u3002", "motivation": "\u63a2\u8ba8Transformer\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u771f\u5b9e\u8d21\u732e\uff0c\u5c24\u5176\u662f\u5728\u7b97\u6cd5\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u51bb\u7ed3MLP\u5c42\u6216\u6ce8\u610f\u529b\u6295\u5f71\u5668\uff0c\u4ee5\u53ca\u5f15\u5165\u56fa\u5b9a\u968f\u673a\u6ce8\u610f\u529b\u7cfb\u6570\u7684MixiT\u6a21\u578b\uff0c\u5bf9\u6bd4\u6807\u51c6Transformer\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "MixiT\u5728\u7b97\u672f\u548c\u8bb0\u5fc6\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0e\u5b8c\u6574\u6a21\u578b\u76f8\u5f53\uff0c\u4f46\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u56e0\u65e0\u6cd5\u5f62\u6210\u4e13\u7528\u7535\u8def\uff08\u5982\u5f52\u7eb3\u5934\uff09\u800c\u8868\u73b0\u8f83\u5dee\u3002\u51bb\u7ed3\u90e8\u5206\u6ce8\u610f\u529b\u7ec4\u4ef6\u7684\u6a21\u578b\u4ecd\u80fd\u5b8c\u6210\u4efb\u52a1\u3002", "conclusion": "Transformer\u7684\u67b6\u6784\u5f02\u8d28\u6027\u5bf9\u4e0d\u540c\u4efb\u52a1\u7684\u89e3\u51b3\u81f3\u5173\u91cd\u8981\uff0c\u6ce8\u610f\u529b\u673a\u5236\u7684\u8d21\u732e\u56e0\u4efb\u52a1\u7c7b\u578b\u800c\u5f02\u3002", "keywords": "Transformer, Attention Mechanism, Algorithmic Tasks, MixiT, Inductive Biases"}}
{"id": "2506.01133", "pdf": "https://arxiv.org/pdf/2506.01133", "abs": "https://arxiv.org/abs/2506.01133", "authors": ["As\u0131m Ersoy", "Basel Mousi", "Shammur Chowdhury", "Firoj Alam", "Fahim Dalvi", "Nadir Durrani"], "title": "From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted Interspeech 2025", "summary": "The emergence of large language models (LLMs) has demonstrated that systems\ntrained solely on text can acquire extensive world knowledge, develop reasoning\ncapabilities, and internalize abstract semantic concepts--showcasing properties\nthat can be associated with general intelligence. This raises an intriguing\nquestion: Do such concepts emerge in models trained on other modalities, such\nas speech? Furthermore, when models are trained jointly on multiple modalities:\nDo they develop a richer, more structured semantic understanding? To explore\nthis, we analyze the conceptual structures learned by speech and textual models\nboth individually and jointly. We employ Latent Concept Analysis, an\nunsupervised method for uncovering and interpreting latent representations in\nneural networks, to examine how semantic abstractions form across modalities.\nFor reproducibility we made scripts and other resources available to the\ncommunity.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u6a21\u6001\u8bad\u7ec3\u4e2d\u662f\u5426\u80fd\u591f\u5f62\u6210\u66f4\u4e30\u5bcc\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u5e76\u5206\u6790\u4e86\u8bed\u97f3\u548c\u6587\u672c\u6a21\u578b\u7684\u6f5c\u5728\u6982\u5ff5\u7ed3\u6784\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u8bad\u7ec3\u4e2d\u662f\u5426\u80fd\u591f\u5f62\u6210\u7c7b\u4f3c\u6587\u672c\u6a21\u578b\u7684\u5e7f\u6cdb\u8bed\u4e49\u6982\u5ff5\uff0c\u4ee5\u63a2\u7d22\u8de8\u6a21\u6001\u7684\u8bed\u4e49\u62bd\u8c61\u5f62\u6210\u673a\u5236\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u6982\u5ff5\u5206\u6790\uff08Latent Concept Analysis\uff09\u8fd9\u4e00\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5206\u6790\u8bed\u97f3\u548c\u6587\u672c\u6a21\u578b\u5355\u72ec\u53ca\u8054\u5408\u8bad\u7ec3\u65f6\u7684\u6f5c\u5728\u8868\u793a\u3002", "result": "\u53d1\u73b0\u4e86\u8de8\u6a21\u6001\u8bad\u7ec3\u4e2d\u8bed\u4e49\u62bd\u8c61\u7684\u5f62\u6210\u6a21\u5f0f\uff0c\u5e76\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u8d44\u6e90\u3002", "conclusion": "\u591a\u6a21\u6001\u8bad\u7ec3\u53ef\u80fd\u5e2e\u52a9\u6a21\u578b\u5f62\u6210\u66f4\u7ed3\u6784\u5316\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u4e3a\u672a\u6765\u8de8\u6a21\u6001\u667a\u80fd\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u591a\u6a21\u6001\u8bad\u7ec3, \u6f5c\u5728\u6982\u5ff5\u5206\u6790, \u8bed\u4e49\u62bd\u8c61, \u8de8\u6a21\u6001\u5b66\u4e60"}}
{"id": "2506.00154", "pdf": "https://arxiv.org/pdf/2506.00154", "abs": "https://arxiv.org/abs/2506.00154", "authors": ["Agust\u00edn Roca", "Gast\u00f3n Castro", "Gabriel Torre", "Leonardo J. Colombo", "Ignacio Mas", "Javier Pereira", "Juan I. Giribet"], "title": "Detection of Endangered Deer Species Using UAV Imagery: A Comparative Study Between Efficient Deep Learning Approaches", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This study compares the performance of state-of-the-art neural networks\nincluding variants of the YOLOv11 and RT-DETR models for detecting marsh deer\nin UAV imagery, in scenarios where specimens occupy a very small portion of the\nimage and are occluded by vegetation. We extend previous analysis adding\nprecise segmentation masks for our datasets enabling a fine-grained training of\na YOLO model with a segmentation head included. Experimental results show the\neffectiveness of incorporating the segmentation head achieving superior\ndetection performance. This work contributes valuable insights for improving\nUAV-based wildlife monitoring and conservation strategies through scalable and\naccurate AI-driven detection systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86YOLOv11\u548cRT-DETR\u7b49\u5148\u8fdb\u795e\u7ecf\u7f51\u7edc\u5728\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u68c0\u6d4b\u6cbc\u6cfd\u9e7f\u7684\u6027\u80fd\uff0c\u7279\u522b\u5173\u6ce8\u76ee\u6807\u5360\u56fe\u50cf\u6bd4\u4f8b\u6781\u5c0f\u4e14\u88ab\u690d\u88ab\u906e\u6321\u7684\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u6dfb\u52a0\u7cbe\u786e\u5206\u5272\u63a9\u7801\u63d0\u5347\u4e86YOLO\u6a21\u578b\u7684\u6548\u679c\u3002", "motivation": "\u4e3a\u63d0\u9ad8\u65e0\u4eba\u673a\u5728\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u548c\u4fdd\u62a4\u4e2d\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u76ee\u6807\u6781\u5c0f\u4e14\u906e\u6321\u4e25\u91cd\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u5229\u7528\u5148\u8fdb\u795e\u7ecf\u7f51\u7edc\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u6570\u636e\u96c6\u5e76\u6dfb\u52a0\u7cbe\u786e\u5206\u5272\u63a9\u7801\uff0c\u8bad\u7ec3\u5e26\u6709\u5206\u5272\u5934\u7684YOLO\u6a21\u578b\uff0c\u4f18\u5316\u5176\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u52a0\u5165\u5206\u5272\u5934\u7684YOLO\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u65e0\u4eba\u673a\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684AI\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u4fdd\u62a4\u7b56\u7565\u7684\u6539\u8fdb\u3002", "keywords": "YOLOv11, RT-DETR, \u65e0\u4eba\u673a, \u91ce\u751f\u52a8\u7269\u76d1\u6d4b, \u5206\u5272\u63a9\u7801"}}
{"id": "2506.01121", "pdf": "https://arxiv.org/pdf/2506.01121", "abs": "https://arxiv.org/abs/2506.01121", "authors": ["Jacob K. Christopher", "Michael Cardei", "Jinhao Liang", "Ferdinando Fioretto"], "title": "Neuro-Symbolic Generative Diffusion Models for Physically Grounded, Robust, and Safe Generation", "categories": ["cs.LG", "cs.AI"], "comment": "Published at the 2nd International Conference on Neuro-symbolic\n  Systems (NeuS 2025)", "summary": "Despite the remarkable generative capabilities of diffusion models, their\nintegration into safety-critical or scientifically rigorous applications\nremains hindered by the need to ensure compliance with stringent physical,\nstructural, and operational constraints. To address this challenge, this paper\nintroduces Neuro-Symbolic Diffusion (NSD), a novel framework that interleaves\ndiffusion steps with symbolic optimization, enabling the generation of\ncertifiably consistent samples under user-defined functional and logic\nconstraints. This key feature is provided for both standard and discrete\ndiffusion models, enabling, for the first time, the generation of both\ncontinuous (e.g., images and trajectories) and discrete (e.g., molecular\nstructures and natural language) outputs that comply with constraints. This\nability is demonstrated on tasks spanning three key challenges: (1) Safety, in\nthe context of non-toxic molecular generation and collision-free trajectory\noptimization; (2) Data scarcity, in domains such as drug discovery and\nmaterials engineering; and (3) Out-of-domain generalization, where enforcing\nsymbolic constraints allows adaptation beyond the training distribution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNeuro-Symbolic Diffusion\uff08NSD\uff09\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u4e0e\u7b26\u53f7\u4f18\u5316\uff0c\u751f\u6210\u7b26\u5408\u7528\u6237\u5b9a\u4e49\u7684\u51fd\u6570\u548c\u903b\u8f91\u7ea6\u675f\u7684\u6837\u672c\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u5728\u751f\u6210\u80fd\u529b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u5b89\u5168\u6027\u8981\u6c42\u9ad8\u6216\u79d1\u5b66\u4e25\u8c28\u7684\u5e94\u7528\u4e2d\u4ecd\u53d7\u9650\u4e8e\u9700\u8981\u6ee1\u8db3\u4e25\u683c\u7684\u7269\u7406\u3001\u7ed3\u6784\u548c\u64cd\u4f5c\u7ea6\u675f\u3002", "method": "NSD\u6846\u67b6\u901a\u8fc7\u4ea4\u66ff\u8fdb\u884c\u6269\u6563\u6b65\u9aa4\u548c\u7b26\u53f7\u4f18\u5316\uff0c\u80fd\u591f\u751f\u6210\u5728\u8fde\u7eed\u548c\u79bb\u6563\u8f93\u51fa\u4e2d\u5747\u7b26\u5408\u7ea6\u675f\u7684\u6837\u672c\u3002", "result": "\u8be5\u6846\u67b6\u5728\u4e09\u4e2a\u5173\u952e\u6311\u6218\u4e2d\u5c55\u793a\u4e86\u6709\u6548\u6027\uff1a\u5b89\u5168\uff08\u5982\u65e0\u6bd2\u5206\u5b50\u751f\u6210\u548c\u9632\u78b0\u649e\u8f68\u8ff9\u4f18\u5316\uff09\u3001\u6570\u636e\u7a00\u7f3a\uff08\u5982\u836f\u7269\u53d1\u73b0\u548c\u6750\u6599\u5de5\u7a0b\uff09\u4ee5\u53ca\u57df\u5916\u6cdb\u5316\u3002", "conclusion": "NSD\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u4e0e\u7b26\u53f7\u4f18\u5316\uff0c\u4e3a\u9ad8\u8981\u6c42\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u751f\u6210\u80fd\u529b\u3002", "keywords": "\u6269\u6563\u6a21\u578b, \u7b26\u53f7\u4f18\u5316, \u5b89\u5168\u7ea6\u675f, \u6570\u636e\u7a00\u7f3a, \u57df\u5916\u6cdb\u5316"}}
{"id": "2506.01147", "pdf": "https://arxiv.org/pdf/2506.01147", "abs": "https://arxiv.org/abs/2506.01147", "authors": ["Prerak Srivastava", "Giulio Corallo", "Sergey Rybalko"], "title": "A Word is Worth 4-bit: Efficient Log Parsing with Binary Coded Decimal Recognition", "categories": ["cs.CL", "cs.LG"], "comment": "Pre-print of our accepted paper at IEEE International Conference on\n  Web Services (ICWS 2025). 4 pages, 2 figures", "summary": "System-generated logs are typically converted into categorical log templates\nthrough parsing. These templates are crucial for generating actionable insights\nin various downstream tasks. However, existing parsers often fail to capture\nfine-grained template details, leading to suboptimal accuracy and reduced\nutility in downstream tasks requiring precise pattern identification. We\npropose a character-level log parser utilizing a novel neural architecture that\naggregates character embeddings. Our approach estimates a sequence of\nbinary-coded decimals to achieve highly granular log templates extraction. Our\nlow-resource character-level parser, tested on revised Loghub-2k and a manually\nannotated industrial dataset, matches LLM-based parsers in accuracy while\noutperforming semantic parsers in efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5b57\u7b26\u7ea7\u65e5\u5fd7\u89e3\u6790\u5668\uff0c\u901a\u8fc7\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u805a\u5408\u5b57\u7b26\u5d4c\u5165\uff0c\u751f\u6210\u7ec6\u7c92\u5ea6\u65e5\u5fd7\u6a21\u677f\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65e5\u5fd7\u89e3\u6790\u5668\u65e0\u6cd5\u6355\u6349\u7ec6\u7c92\u5ea6\u6a21\u677f\u7ec6\u8282\uff0c\u5bfc\u81f4\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u964d\u4f4e\u3002", "method": "\u4f7f\u7528\u5b57\u7b26\u7ea7\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u805a\u5408\u5b57\u7b26\u5d4c\u5165\uff0c\u751f\u6210\u4e8c\u8fdb\u5236\u7f16\u7801\u5e8f\u5217\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u65e5\u5fd7\u6a21\u677f\u63d0\u53d6\u3002", "result": "\u5728\u4fee\u8ba2\u7248Loghub-2k\u548c\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u6027\u80fd\u4e0eLLM\u89e3\u6790\u5668\u76f8\u5f53\uff0c\u6548\u7387\u4f18\u4e8e\u8bed\u4e49\u89e3\u6790\u5668\u3002", "conclusion": "\u5b57\u7b26\u7ea7\u89e3\u6790\u5668\u5728\u4f4e\u8d44\u6e90\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u517c\u5177\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "keywords": "\u65e5\u5fd7\u89e3\u6790\u3001\u5b57\u7b26\u7ea7\u795e\u7ecf\u7f51\u7edc\u3001\u7ec6\u7c92\u5ea6\u6a21\u677f\u3001\u4e8c\u8fdb\u5236\u7f16\u7801"}}
{"id": "2506.01145", "pdf": "https://arxiv.org/pdf/2506.01145", "abs": "https://arxiv.org/abs/2506.01145", "authors": ["Merlin Sch\u00fcler", "Eddie Seabrook", "Laurenz Wiskott"], "title": "Slow Feature Analysis on Markov Chains from Goal-Directed Behavior", "categories": ["cs.LG"], "comment": null, "summary": "Slow Feature Analysis is a unsupervised representation learning method that\nextracts slowly varying features from temporal data and can be used as a basis\nfor subsequent reinforcement learning. Often, the behavior that generates the\ndata on which the representation is learned is assumed to be a uniform random\nwalk. Less research has focused on using samples generated by goal-directed\nbehavior, as commonly the case in a reinforcement learning setting, to learn a\nrepresentation. In a spatial setting, goal-directed behavior typically leads to\nsignificant differences in state occupancy between states that are close to a\nreward location and far from a reward location.\n  Through the perspective of optimal slow features on ergodic Markov chains,\nthis work investigates the effects of these differences on value-function\napproximation in an idealized setting. Furthermore, three correction routes,\nwhich can potentially alleviate detrimental scaling effects, are evaluated and\ndiscussed. In addition, the special case of goal-averse behavior is considered.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u751f\u6210\u7684\u6570\u636e\u5b66\u4e60\u8868\u793a\uff0c\u5e76\u63a2\u8ba8\u4e86\u72b6\u6001\u5360\u7528\u5dee\u5f02\u5bf9\u503c\u51fd\u6570\u8fd1\u4f3c\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u4fee\u6b63\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u751f\u6210\u7684\u6570\u636e\u5bf9\u8868\u793a\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5728\u7a7a\u95f4\u8bbe\u7f6e\u4e2d\uff0c\u5956\u52b1\u4f4d\u7f6e\u9644\u8fd1\u548c\u8fdc\u5904\u7684\u72b6\u6001\u5360\u7528\u5dee\u5f02\u5982\u4f55\u5f71\u54cd\u503c\u51fd\u6570\u8fd1\u4f3c\u3002", "method": "\u901a\u8fc7\u6700\u4f18\u6162\u7279\u5f81\u5728\u904d\u5386\u9a6c\u5c14\u53ef\u592b\u94fe\u4e0a\u7684\u89c6\u89d2\uff0c\u5206\u6790\u72b6\u6001\u5360\u7528\u5dee\u5f02\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e09\u79cd\u4fee\u6b63\u65b9\u6cd5\u6765\u51cf\u8f7b\u4e0d\u826f\u7f29\u653e\u6548\u5e94\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u5bf9\u8868\u793a\u5b66\u4e60\u7684\u7279\u6b8a\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u884c\u7684\u4fee\u6b63\u65b9\u6cd5\u3002", "conclusion": "\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u7684\u6570\u636e\u5728\u8868\u793a\u5b66\u4e60\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u53ef\u4ee5\u901a\u8fc7\u4fee\u6b63\u65b9\u6cd5\u4f18\u5316\u503c\u51fd\u6570\u8fd1\u4f3c\u6548\u679c\u3002", "keywords": "\u6162\u7279\u5f81\u5206\u6790, \u8868\u793a\u5b66\u4e60, \u5f3a\u5316\u5b66\u4e60, \u76ee\u6807\u5bfc\u5411\u884c\u4e3a, \u9a6c\u5c14\u53ef\u592b\u94fe"}}
{"id": "2506.01156", "pdf": "https://arxiv.org/pdf/2506.01156", "abs": "https://arxiv.org/abs/2506.01156", "authors": ["Nhan Phan", "Mikko Kuronen", "Maria Kautonen", "Riikka Ullakonoja", "Anna von Zansen", "Yaroslav Getman", "Ekaterina Voskoboinik", "Tam\u00e1s Gr\u00f3sz", "Mikko Kurimo"], "title": "Mispronunciation Detection Without L2 Pronunciation Dataset in Low-Resource Setting: A Case Study in Finland Swedish", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025 conference", "summary": "Mispronunciation detection (MD) models are the cornerstones of many language\nlearning applications. Unfortunately, most systems are built for English and\nother major languages, while low-resourced language varieties, such as Finland\nSwedish (FS), lack such tools. In this paper, we introduce our MD model for FS,\ntrained on 89 hours of first language (L1) speakers' spontaneous speech and\ntested on 33 minutes of L2 transcribed read-aloud speech.\n  We trained a multilingual wav2vec 2.0 model with entropy regularization,\nfollowed by temperature scaling and top-k normalization after the inference to\nbetter adapt it for MD. The main novelty of our method lies in its simplicity,\nrequiring minimal L2 data. The process is also language-independent, making it\nsuitable for other low-resource languages. Our proposed algorithm allows us to\nbalance Recall (43.2%) and Precision (29.8%), compared with the baseline\nmodel's Recall (77.5%) and Precision (17.6%).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u82ac\u5170\u745e\u5178\u8bed\uff08FS\uff09\u7684\u53d1\u97f3\u9519\u8bef\u68c0\u6d4b\uff08MD\uff09\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u4ec5\u9700\u5c11\u91cf\u7b2c\u4e8c\u8bed\u8a00\uff08L2\uff09\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u65b0\u9896\u7684\u65b9\u6cd5\u5728\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u7531\u4e8e\u591a\u6570\u53d1\u97f3\u9519\u8bef\u68c0\u6d4b\u7cfb\u7edf\u4ec5\u9488\u5bf9\u82f1\u8bed\u7b49\u4e3b\u6d41\u8bed\u8a00\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u5982\u82ac\u5170\u745e\u5178\u8bed\u7f3a\u4e4f\u6b64\u7c7b\u5de5\u5177\uff0c\u4f5c\u8005\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u591a\u8bed\u8a00wav2vec 2.0\u6a21\u578b\uff0c\u7ed3\u5408\u71b5\u6b63\u5219\u5316\u3001\u6e29\u5ea6\u7f29\u653e\u548ctop-k\u5f52\u4e00\u5316\uff0c\u4ec5\u9700\u5c11\u91cfL2\u6570\u636e\u5373\u53ef\u9002\u914dMD\u4efb\u52a1\u3002", "result": "\u6a21\u578b\u5728\u53ec\u56de\u7387\uff0843.2%\uff09\u548c\u7cbe\u786e\u5ea6\uff0829.8%\uff09\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0877.5%\u548c17.6%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7684\u7b80\u5355\u6027\u548c\u8bed\u8a00\u65e0\u5173\u6027\u4f7f\u5176\u9002\u7528\u4e8e\u5176\u4ed6\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002", "keywords": "\u53d1\u97f3\u9519\u8bef\u68c0\u6d4b\u3001\u4f4e\u8d44\u6e90\u8bed\u8a00\u3001\u82ac\u5170\u745e\u5178\u8bed\u3001wav2vec 2.0\u3001\u71b5\u6b63\u5219\u5316"}}
{"id": "2506.01151", "pdf": "https://arxiv.org/pdf/2506.01151", "abs": "https://arxiv.org/abs/2506.01151", "authors": ["Xintong Sun", "Chi Wei", "Minghao Tian", "Shiwen Ni"], "title": "Earley-Driven Dynamic Pruning for Efficient Structured Decoding", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML2025 poster", "summary": "Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring\ntheir outputs conform to strict structural or grammatical constraints remains\nchallenging, which is critical in function calls and domain-specific language\n(DSL) generation. Constrained decoding with context-free grammar is a flexible\napproach to guarantee LLMs' adherence to a specific format by dynamically\nbuilding a token logits mask. However, creating this mask requires checking the\nvalidity of all tokens in the LLM vocabulary at every decoding step, which\noften incurs significant overheads in existing constrained decoding engines. To\naddress this challenge, we propose $\\textbf{ZapFormat}$, a novel\n$\\textbf{dynamic pruning}$ strategy based on the Earley algorithm that\nidentifies and eliminates invalid or redundant Earley states in real-time,\nsignificantly reducing memory occupation of the Earley algorithm's states. This\nfurther enables us to use a state cache to speed up structured generations on a\nlarge number of queries. We implemented ZapFormat in a new constrained decoding\nengine called Formatron which also incorporates existing optimizations. Through\ncomprehensive experiments on structured generation tasks, including JSON\ngeneration, JSON Schema validation, and semantic parsing, we demonstrate that\nFormatron not only $\\textbf{consistently maintains}$ high-precision compliant\noutputs but also achieves $\\textbf{significant improvements}$ in inference\nspeed up to 2x compared to state-of-the-art implementations. More importantly,\nFormatron is generally applicable across various LLM architectures. We release\nFormatron as open source at https://github.com/Dan-wanna-M/formatron.", "AI": {"tldr": "ZapFormat \u662f\u4e00\u79cd\u57fa\u4e8e Earley \u7b97\u6cd5\u7684\u52a8\u6001\u526a\u679d\u7b56\u7565\uff0c\u7528\u4e8e\u4f18\u5316 LLMs \u7684\u7ea6\u675f\u89e3\u7801\uff0c\u51cf\u5c11\u5185\u5b58\u5360\u7528\u5e76\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u786e\u4fdd LLMs \u8f93\u51fa\u7b26\u5408\u4e25\u683c\u7684\u7ed3\u6784\u6216\u8bed\u6cd5\u7ea6\u675f\uff08\u5982\u51fd\u6570\u8c03\u7528\u548c DSL \u751f\u6210\uff09\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u73b0\u6709\u7ea6\u675f\u89e3\u7801\u5f15\u64ce\u5728\u9a8c\u8bc1\u4ee4\u724c\u6709\u6548\u6027\u65f6\u5b58\u5728\u663e\u8457\u5f00\u9500\u3002", "method": "\u63d0\u51fa ZapFormat\uff0c\u4e00\u79cd\u52a8\u6001\u526a\u679d\u7b56\u7565\uff0c\u5b9e\u65f6\u8bc6\u522b\u548c\u6d88\u9664\u5197\u4f59\u7684 Earley \u72b6\u6001\uff0c\u5e76\u7ed3\u5408\u72b6\u6001\u7f13\u5b58\u52a0\u901f\u7ed3\u6784\u5316\u751f\u6210\u3002", "result": "\u5728 JSON \u751f\u6210\u548c\u8bed\u4e49\u89e3\u6790\u7b49\u4efb\u52a1\u4e2d\uff0cFormatron \u4fdd\u6301\u9ad8\u7cbe\u5ea6\u8f93\u51fa\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe 2 \u500d\u3002", "conclusion": "Formatron \u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u7ea6\u675f\u89e3\u7801\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u591a\u79cd LLM \u67b6\u6784\u3002", "keywords": "Large Language Models, constrained decoding, Earley algorithm, dynamic pruning, structured generation"}}
{"id": "2506.01172", "pdf": "https://arxiv.org/pdf/2506.01172", "abs": "https://arxiv.org/abs/2506.01172", "authors": ["Byung-Doh Oh", "Hongao Zhu", "William Schuler"], "title": "The Inverse Scaling Effect of Pre-Trained Language Model Surprisal Is Not Due to Data Leakage", "categories": ["cs.CL"], "comment": "ACL Findings 2025; results with Natural Stories alignment issue\n  corrected (commit 4700daa)", "summary": "In psycholinguistic modeling, surprisal from larger pre-trained language\nmodels has been shown to be a poorer predictor of naturalistic human reading\ntimes. However, it has been speculated that this may be due to data leakage\nthat caused language models to see the text stimuli during training. This paper\npresents two studies to address this concern at scale. The first study reveals\nrelatively little leakage of five naturalistic reading time corpora in two\npre-training datasets in terms of length and frequency of token $n$-gram\noverlap. The second study replicates the negative relationship between language\nmodel size and the fit of surprisal to reading times using models trained on\n'leakage-free' data that overlaps only minimally with the reading time corpora.\nTaken together, this suggests that previous results using language models\ntrained on these corpora are not driven by the effects of data leakage.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u610f\u5916\u6027\u4e0e\u4eba\u7c7b\u9605\u8bfb\u65f6\u95f4\u7684\u9884\u6d4b\u5173\u7cfb\u4e0d\u53d7\u6570\u636e\u6cc4\u9732\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u610f\u5916\u6027\u4e0e\u4eba\u7c7b\u9605\u8bfb\u65f6\u95f4\u7684\u9884\u6d4b\u5173\u7cfb\u662f\u5426\u53d7\u6570\u636e\u6cc4\u9732\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u7814\u7a76\uff1a1\uff09\u5206\u6790\u4e94\u4e2a\u81ea\u7136\u9605\u8bfb\u65f6\u95f4\u8bed\u6599\u5e93\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6cc4\u9732\u7a0b\u5ea6\uff1b2\uff09\u4f7f\u7528\u2018\u65e0\u6cc4\u9732\u2019\u6570\u636e\u91cd\u590d\u5b9e\u9a8c\u3002", "result": "\u53d1\u73b0\u6570\u636e\u6cc4\u9732\u8f83\u5c11\uff0c\u4e14\u6a21\u578b\u5927\u5c0f\u4ecd\u4e0e\u610f\u5916\u6027\u9884\u6d4b\u9605\u8bfb\u65f6\u95f4\u7684\u62df\u5408\u5ea6\u8d1f\u76f8\u5173\u3002", "conclusion": "\u5148\u524d\u7ed3\u679c\u5e76\u975e\u7531\u6570\u636e\u6cc4\u9732\u9a71\u52a8\u3002", "keywords": "\u5fc3\u7406\u8bed\u8a00\u5b66, \u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b, \u610f\u5916\u6027, \u6570\u636e\u6cc4\u9732, \u9605\u8bfb\u65f6\u95f4"}}
{"id": "2506.00185", "pdf": "https://arxiv.org/pdf/2506.00185", "abs": "https://arxiv.org/abs/2506.00185", "authors": ["Lilit Grigoryan", "Vladimir Bataev", "Andrei Andrusenko", "Hainan Xu", "Vitaly Lavrukhin", "Boris Ginsburg"], "title": "Pushing the Limits of Beam Search Decoding for Transducer-based ASR models", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Transducer models have emerged as a promising choice for end-to-end ASR\nsystems, offering a balanced trade-off between recognition accuracy, streaming\ncapabilities, and inference speed in greedy decoding. However, beam search\nsignificantly slows down Transducers due to repeated evaluations of key network\ncomponents, limiting practical applications. This paper introduces a universal\nmethod to accelerate beam search for Transducers, enabling the implementation\nof two optimized algorithms: ALSD++ and AES++. The proposed method utilizes\nbatch operations, a tree-based hypothesis structure, novel blank scoring for\nenhanced shallow fusion, and CUDA graph execution for efficient GPU inference.\nThis narrows the speed gap between beam and greedy modes to only 10-20% for the\nwhole system, achieves 14-30% relative improvement in WER compared to greedy\ndecoding, and improves shallow fusion for low-resource up to 11% compared to\nexisting implementations. All the algorithms are open sourced.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u6279\u5904\u7406\u64cd\u4f5c\u548c\u6811\u72b6\u5047\u8bbe\u7ed3\u6784\u7b49\u6280\u672f\uff0c\u663e\u8457\u52a0\u901fTransducer\u6a21\u578b\u7684\u675f\u641c\u7d22\uff0c\u7f29\u5c0f\u4e0e\u8d2a\u5a6a\u89e3\u7801\u7684\u901f\u5ea6\u5dee\u8ddd\uff0c\u5e76\u5728WER\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u7531\u4e8e\u675f\u641c\u7d22\u5bfc\u81f4\u7684\u91cd\u590d\u8ba1\u7b97\u5927\u5e45\u964d\u4f4e\u4e86Transducer\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u6548\u7387\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6279\u5904\u7406\u64cd\u4f5c\u3001\u6811\u72b6\u5047\u8bbe\u7ed3\u6784\u3001\u65b0\u9896\u7684\u7a7a\u683c\u8bc4\u5206\u6280\u672f\uff0c\u4ee5\u53caCUDA\u56fe\u6267\u884c\u4f18\u5316GPU\u63a8\u7406\u3002", "result": "\u675f\u641c\u7d22\u4e0e\u8d2a\u5a6a\u89e3\u7801\u7684\u901f\u5ea6\u5dee\u8ddd\u7f29\u5c0f\u81f310-20%\uff0cWER\u76f8\u5bf9\u63d0\u534714-30%\uff0c\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u6d45\u878d\u5408\u6027\u80fd\u63d0\u534711%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86Transducer\u6a21\u578b\u7684\u675f\u641c\u7d22\u6548\u7387\uff0c\u663e\u8457\u6539\u5584\u4e86\u8bc6\u522b\u6027\u80fd\u3002", "keywords": "Transducer, \u675f\u641c\u7d22, ASR, \u8d2a\u5a6a\u89e3\u7801, CUDA"}}
{"id": "2506.01153", "pdf": "https://arxiv.org/pdf/2506.01153", "abs": "https://arxiv.org/abs/2506.01153", "authors": ["Roussel Desmond Nzoyem", "Nawid Keshtmand", "Idriss Tsayem", "David A. W. Barton", "Tom Deakin"], "title": "Weight-Space Linear Recurrent Neural Networks", "categories": ["cs.LG"], "comment": "33 pages, 21 figures, 11 tables", "summary": "We introduce WARP (Weight-space Adaptive Recurrent Prediction), a simple yet\npowerful framework that unifies weight-space learning with linear recurrence to\nredefine sequence modeling. Unlike conventional recurrent neural networks\n(RNNs) which collapse temporal dynamics into fixed-dimensional hidden states,\nWARP explicitly parametrizes the hidden state as the weights of a distinct root\nneural network. This formulation promotes higher-resolution memory,\ngradient-free adaptation at test-time, and seamless integration of\ndomain-specific physical priors. Empirical validation shows that WARP matches\nor surpasses state-of-the-art baselines on diverse classification tasks,\nspanning synthetic benchmarks to real-world datasets. Furthermore, extensive\nexperiments across sequential image completion, dynamical system\nreconstruction, and multivariate time series forecasting demonstrate its\nexpressiveness and generalization capabilities. Critically, WARP's weight\ntrajectories offer valuable insights into the model's inner workings. Ablation\nstudies confirm the architectural necessity of key components, solidifying\nweight-space linear RNNs as a transformative paradigm for adaptive machine\nintelligence.", "AI": {"tldr": "WARP\u662f\u4e00\u4e2a\u7edf\u4e00\u6743\u91cd\u7a7a\u95f4\u5b66\u4e60\u4e0e\u7ebf\u6027\u9012\u5f52\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u53c2\u6570\u5316\u9690\u85cf\u72b6\u6001\u4e3a\u6839\u795e\u7ecf\u7f51\u7edc\u7684\u6743\u91cd\uff0c\u63d0\u5347\u8bb0\u5fc6\u5206\u8fa8\u7387\u4e0e\u6d4b\u8bd5\u65f6\u9002\u5e94\u6027\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u91cd\u65b0\u5b9a\u4e49\u5e8f\u5217\u5efa\u6a21\uff0c\u89e3\u51b3\u4f20\u7edfRNN\u5c06\u65f6\u95f4\u52a8\u6001\u538b\u7f29\u4e3a\u56fa\u5b9a\u7ef4\u5ea6\u9690\u72b6\u6001\u7684\u5c40\u9650\u6027\u3002", "method": "\u5c06\u9690\u72b6\u6001\u53c2\u6570\u5316\u4e3a\u6839\u795e\u7ecf\u7f51\u7edc\u7684\u6743\u91cd\uff0c\u5b9e\u73b0\u68af\u5ea6\u81ea\u7531\u9002\u5e94\u4e0e\u9886\u57df\u5148\u9a8c\u96c6\u6210\u3002", "result": "\u5728\u5206\u7c7b\u4efb\u52a1\u3001\u65f6\u5e8f\u56fe\u50cf\u8865\u5168\u3001\u52a8\u529b\u7cfb\u7edf\u91cd\u6784\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "WARP\u4e3a\u81ea\u9002\u5e94\u673a\u5668\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u6743\u91cd\u8f68\u8ff9\u63ed\u793a\u4e86\u6a21\u578b\u5185\u90e8\u673a\u5236\u3002", "keywords": "WARP, \u6743\u91cd\u7a7a\u95f4\u5b66\u4e60, \u7ebf\u6027\u9012\u5f52, \u5e8f\u5217\u5efa\u6a21, \u81ea\u9002\u5e94\u667a\u80fd"}}
{"id": "2506.01187", "pdf": "https://arxiv.org/pdf/2506.01187", "abs": "https://arxiv.org/abs/2506.01187", "authors": ["Eran Hirsch", "Aviv Slobodkin", "David Wan", "Elias Stengel-Eskin", "Mohit Bansal", "Ido Dagan"], "title": "LAQuer: Localized Attribution Queries in Content-grounded Generation", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Grounded text generation models often produce content that deviates from\ntheir source material, requiring user verification to ensure accuracy. Existing\nattribution methods associate entire sentences with source documents, which can\nbe overwhelming for users seeking to fact-check specific claims. In contrast,\nexisting sub-sentence attribution methods may be more precise but fail to align\nwith users' interests. In light of these limitations, we introduce Localized\nAttribution Queries (LAQuer), a new task that localizes selected spans of\ngenerated output to their corresponding source spans, allowing fine-grained and\nuser-directed attribution. We compare two approaches for the LAQuer task,\nincluding prompting large language models (LLMs) and leveraging LLM internal\nrepresentations. We then explore a modeling framework that extends existing\nattributed text generation methods to LAQuer. We evaluate this framework across\ntwo grounded text generation tasks: Multi-document Summarization (MDS) and\nLong-form Question Answering (LFQA). Our findings show that LAQuer methods\nsignificantly reduce the length of the attributed text. Our contributions\ninclude: (1) proposing the LAQuer task to enhance attribution usability, (2)\nsuggesting a modeling framework and benchmarking multiple baselines, and (3)\nproposing a new evaluation setting to promote future research on localized\nattribution in content-grounded generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Localized Attribution Queries (LAQuer)\u4efb\u52a1\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\u751f\u6210\u6587\u672c\u4e0e\u6e90\u6587\u672c\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u63d0\u5347\u6587\u672c\u751f\u6210\u7684\u51c6\u786e\u6027\u9a8c\u8bc1\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9a8c\u8bc1\u751f\u6210\u6587\u672c\u51c6\u786e\u6027\u65f6\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u6216\u4e0e\u7528\u6237\u9700\u6c42\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7cbe\u51c6\u7684\u7528\u6237\u5bfc\u5411\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faLAQuer\u4efb\u52a1\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5229\u7528LLM\u5185\u90e8\u8868\u793a\u7684\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u6269\u5c55\u4e86\u73b0\u6709\u6a21\u578b\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLAQuer\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u9700\u9a8c\u8bc1\u7684\u6587\u672c\u957f\u5ea6\uff0c\u5728\u591a\u6587\u6863\u6458\u8981\u548c\u957f\u683c\u5f0f\u95ee\u7b54\u4efb\u52a1\u4e2d\u6709\u6548\u3002", "conclusion": "LAQuer\u4efb\u52a1\u63d0\u5347\u4e86\u6587\u672c\u751f\u6210\u7684\u53ef\u7528\u6027\uff0c\u672a\u6765\u53ef\u63a8\u52a8\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\u7814\u7a76\u3002", "keywords": "\u6587\u672c\u751f\u6210, \u5b9a\u4f4d\u9a8c\u8bc1, LAQuer, \u591a\u6587\u6863\u6458\u8981, \u957f\u683c\u5f0f\u95ee\u7b54"}}
{"id": "2506.00191", "pdf": "https://arxiv.org/pdf/2506.00191", "abs": "https://arxiv.org/abs/2506.00191", "authors": ["Jiawei Chen", "Lusi Li", "Daniel Takabi", "Masha Sosonkina", "Rui Ning"], "title": "Heterogeneous Graph Backdoor Attack", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Heterogeneous Graph Neural Networks (HGNNs) excel in modeling complex,\nmulti-typed relationships across diverse domains, yet their vulnerability to\nbackdoor attacks remains unexplored. To address this gap, we conduct the first\ninvestigation into the susceptibility of HGNNs to existing graph backdoor\nattacks, revealing three critical issues: (1) high attack budget required for\neffective backdoor injection, (2) inefficient and unreliable backdoor\nactivation, and (3) inaccurate attack effectiveness evaluation. To tackle these\nissues, we propose the Heterogeneous Graph Backdoor Attack (HGBA), the first\nbackdoor attack specifically designed for HGNNs, introducing a novel\nrelation-based trigger mechanism that establishes specific connections between\na strategically selected trigger node and poisoned nodes via the backdoor\nmetapath. HGBA achieves efficient and stealthy backdoor injection with minimal\nstructural modifications and supports easy backdoor activation through two\nflexible strategies: Self-Node Attack and Indiscriminate Attack. Additionally,\nwe improve the ASR measurement protocol, enabling a more accurate assessment of\nattack effectiveness. Extensive experiments demonstrate that HGBA far surpasses\nmultiple state-of-the-art graph backdoor attacks in black-box settings,\nefficiently attacking HGNNs with low attack budgets. Ablation studies show that\nthe strength of HBGA benefits from our trigger node selection method and\nbackdoor metapath selection strategy. In addition, HGBA shows superior\nrobustness against node feature perturbations and multiple types of existing\ngraph backdoor defense mechanisms. Finally, extension experiments demonstrate\nthat the relation-based trigger mechanism can effectively extend to tasks in\nhomogeneous graph scenarios, thereby posing severe threats to broader\nsecurity-critical domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7814\u7a76\u4e86\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HGNNs\uff09\u5bf9\u540e\u95e8\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e86\u5f02\u6784\u56fe\u540e\u95e8\u653b\u51fb\uff08HGBA\uff09\uff0c\u91c7\u7528\u57fa\u4e8e\u5173\u7cfb\u7684\u89e6\u53d1\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u653b\u51fb\u7684\u9ad8\u6210\u672c\u3001\u4f4e\u6548\u548c\u4e0d\u51c6\u786e\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HGNNs\uff09\u5728\u591a\u9886\u57df\u590d\u6742\u5173\u7cfb\u5efa\u6a21\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5bf9\u540e\u95e8\u653b\u51fb\u7684\u8106\u5f31\u6027\u5c1a\u672a\u88ab\u7814\u7a76\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u5f02\u6784\u56fe\u540e\u95e8\u653b\u51fb\uff08HGBA\uff09\uff0c\u5f15\u5165\u57fa\u4e8e\u5173\u7cfb\u7684\u89e6\u53d1\u673a\u5236\uff0c\u901a\u8fc7\u540e\u95e8\u5143\u8def\u5f84\u8fde\u63a5\u89e6\u53d1\u8282\u70b9\u548c\u4e2d\u6bd2\u8282\u70b9\uff0c\u652f\u6301\u4e24\u79cd\u7075\u6d3b\u7684\u6fc0\u6d3b\u7b56\u7565\uff0c\u5e76\u6539\u8fdb\u653b\u51fb\u6548\u679c\u8bc4\u4f30\u534f\u8bae\u3002", "result": "HGBA\u5728\u4f4e\u653b\u51fb\u9884\u7b97\u4e0b\u9ad8\u6548\u653b\u51fbHGNNs\uff0c\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\uff0c\u4e14\u5bf9\u8282\u70b9\u7279\u5f81\u6270\u52a8\u548c\u591a\u79cd\u9632\u5fa1\u673a\u5236\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "HGBA\u4e3a\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u540e\u95e8\u653b\u51fb\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u9690\u853d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5e7f\u6cdb\u7684\u5b89\u5168\u5a01\u80c1\u6f5c\u529b\u3002", "keywords": "\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc, \u540e\u95e8\u653b\u51fb, \u57fa\u4e8e\u5173\u7cfb\u7684\u89e6\u53d1, \u540e\u95e8\u5143\u8def\u5f84, \u9c81\u68d2\u6027"}}
{"id": "2506.01158", "pdf": "https://arxiv.org/pdf/2506.01158", "abs": "https://arxiv.org/abs/2506.01158", "authors": ["Danyal Rehman", "Oscar Davis", "Jiarui Lu", "Jian Tang", "Michael Bronstein", "Yoshua Bengio", "Alexander Tong", "Avishek Joey Bose"], "title": "FORT: Forward-Only Regression Training of Normalizing Flows", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Preprint", "summary": "Simulation-free training frameworks have been at the forefront of the\ngenerative modelling revolution in continuous spaces, leading to neural\ndynamical systems that encompass modern large-scale diffusion and flow matching\nmodels. Despite the scalability of training, the generation of high-quality\nsamples and their corresponding likelihood under the model requires expensive\nnumerical simulation -- inhibiting adoption in numerous scientific applications\nsuch as equilibrium sampling of molecular systems. In this paper, we revisit\nclassical normalizing flows as one-step generative models with exact\nlikelihoods and propose a novel, scalable training objective that does not\nrequire computing the expensive change of variable formula used in conventional\nmaximum likelihood training. We propose Forward-Only Regression Training\n(FORT), a simple $\\ell_2$-regression objective that maps prior samples under\nour flow to specifically chosen targets. We demonstrate that FORT supports a\nwide class of targets, such as optimal transport targets and targets from\npre-trained continuous-time normalizing flows (CNF). We further demonstrate\nthat by using CNF targets, our one-step flows allow for larger-scale training\nthat exceeds the performance and stability of maximum likelihood training,\nwhile unlocking a broader class of architectures that were previously\nchallenging to train. Empirically, we elucidate that our trained flows can\nperform equilibrium conformation sampling in Cartesian coordinates of alanine\ndipeptide, alanine tripeptide, and alanine tetrapeptide.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u6b65\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5FORT\uff0c\u901a\u8fc7\u7b80\u5355\u7684L2\u56de\u5f52\u76ee\u6807\u4f18\u5316\uff0c\u907f\u514d\u4f20\u7edf\u6700\u5927\u4f3c\u7136\u8bad\u7ec3\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u53d8\u91cf\u53d8\u6362\u516c\u5f0f\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u9700\u8981\u6602\u8d35\u7684\u6570\u503c\u6a21\u62df\u8ba1\u7b97\uff0c\u9650\u5236\u4e86\u5176\u5728\u79d1\u5b66\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u91c7\u7528\uff0c\u5982\u5206\u5b50\u7cfb\u7edf\u7684\u5e73\u8861\u91c7\u6837\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u6a21\u62df\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86Forward-Only Regression Training (FORT)\uff0c\u4e00\u79cd\u57fa\u4e8eL2\u56de\u5f52\u7684\u76ee\u6807\u51fd\u6570\uff0c\u901a\u8fc7\u5c06\u5148\u9a8c\u6837\u672c\u6620\u5c04\u5230\u7279\u5b9a\u76ee\u6807\u6765\u8bad\u7ec3\u5355\u6b65\u751f\u6210\u6a21\u578b\u3002\u652f\u6301\u591a\u79cd\u76ee\u6807\u7c7b\u578b\uff0c\u5982\u6700\u4f18\u4f20\u8f93\u76ee\u6807\u548c\u9884\u8bad\u7ec3\u7684\u8fde\u7eed\u65f6\u95f4\u5f52\u4e00\u5316\u6d41(CNF)\u76ee\u6807\u3002", "result": "FORT\u8bad\u7ec3\u7684\u5355\u6b65\u751f\u6210\u6a21\u578b\u5728\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7684\u6700\u5927\u4f3c\u7136\u8bad\u7ec3\uff0c\u4e14\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u67b6\u6784\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u53ef\u5728\u7279\u5b9a\u5206\u5b50\u7cfb\u7edf\u7684\u5e73\u8861\u6784\u8c61\u91c7\u6837\u4e2d\u6709\u6548\u5e94\u7528\u3002", "conclusion": "FORT\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8bad\u7ec3\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u4e3a\u590d\u6742\u79d1\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "keywords": "\u751f\u6210\u6a21\u578b, \u5f52\u4e00\u5316\u6d41, \u8bad\u7ec3\u76ee\u6807, \u5206\u5b50\u7cfb\u7edf\u91c7\u6837, \u8ba1\u7b97\u6548\u7387"}}
{"id": "2506.01190", "pdf": "https://arxiv.org/pdf/2506.01190", "abs": "https://arxiv.org/abs/2506.01190", "authors": ["Madhavendra Thakur"], "title": "Culturally-Grounded Chain-of-Thought (CG-CoT):Enhancing LLM Performance on Culturally-Specific Tasks in Low-Resource Languages", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) struggle with culturally-specific reasoning\ntasks, particularly in low-resource languages, hindering their global\napplicability. Addressing this gap is crucial for equitable AI deployment. We\nintroduce Culturally-Grounded Chain-of-Thought (CG-CoT), a novel prompting\nstrategy that combines dense vector retrieval of cultural context with explicit\nreasoning sequences. Our extensive experiments on Yoruba proverb interpretation\ndemonstrate that CG-CoT provides significantly higher culturally-aligned\naccuracy and depth than traditional prompting methods, validated through both\nautomated metrics and LLM-based evaluations. Notably, we uncover stark\ndisparities between token-level translation metrics like BLEU and human-judged\ncultural relevance, suggesting a rethinking of evaluation approaches for\nlow-resource NLP.", "AI": {"tldr": "\u63d0\u51faCG-CoT\u65b9\u6cd5\u89e3\u51b3LLM\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u6587\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e0d\u8db3\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u5316\u4e0a\u4e0b\u6587\u68c0\u7d22\u4e0e\u663e\u5f0f\u63a8\u7406\u5e8f\u5217\u5b9e\u73b0\u66f4\u597d\u6548\u679c\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u7279\u5b9a\u6587\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5f71\u54cd\u5176\u5168\u7403\u5e94\u7528\u516c\u5e73\u6027\u3002", "method": "\u7ed3\u5408\u5bc6\u96c6\u5411\u91cf\u68c0\u7d22\u7684\u6587\u5316\u4e0a\u4e0b\u6587\u4e0e\u663e\u5f0f\u63a8\u7406\u5e8f\u5217\uff08CG-CoT\uff09\uff0c\u5728\u7ea6\u9c81\u5df4\u8c1a\u8bed\u89e3\u91ca\u4efb\u52a1\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "CG-CoT\u5728\u6587\u5316\u5bf9\u9f50\u51c6\u786e\u6027\u548c\u6df1\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u540c\u65f6\u63ed\u793aBLEU\u7b49\u6307\u6807\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7684\u6587\u5316\u76f8\u5173\u6027\u5dee\u5f02\u3002", "conclusion": "CG-CoT\u6709\u6548\u63d0\u5347\u6587\u5316\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u547c\u5401\u91cd\u65b0\u601d\u8003\u4f4e\u8d44\u6e90NLP\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "keywords": "LLM\u3001\u6587\u5316\u63a8\u7406\u3001\u4f4e\u8d44\u6e90\u8bed\u8a00\u3001CG-CoT\u3001BLEU"}}
{"id": "2506.01167", "pdf": "https://arxiv.org/pdf/2506.01167", "abs": "https://arxiv.org/abs/2506.01167", "authors": ["Alper Kamil Bozkurt", "Calin Belta", "Ming C. Lin"], "title": "Accelerated Learning with Linear Temporal Logic using Differentiable Simulation", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "To ensure learned controllers comply with safety and reliability requirements\nfor reinforcement learning in real-world settings remains challenging.\nTraditional safety assurance approaches, such as state avoidance and\nconstrained Markov decision processes, often inadequately capture trajectory\nrequirements or may result in overly conservative behaviors. To address these\nlimitations, recent studies advocate the use of formal specification languages\nsuch as linear temporal logic (LTL), enabling the derivation of\ncorrect-by-construction learning objectives from the specified requirements.\nHowever, the sparse rewards associated with LTL specifications make learning\nextremely difficult, whereas dense heuristic-based rewards risk compromising\ncorrectness. In this work, we propose the first method, to our knowledge, that\nintegrates LTL with differentiable simulators, facilitating efficient\ngradient-based learning directly from LTL specifications by coupling with\ndifferentiable paradigms. Our approach introduces soft labeling to achieve\ndifferentiable rewards and states, effectively mitigating the sparse-reward\nissue intrinsic to LTL without compromising objective correctness. We validate\nthe efficacy of our method through experiments, demonstrating significant\nimprovements in both reward attainment and training time compared to the\ndiscrete methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTL\uff09\u4e0e\u53ef\u5fae\u5206\u6a21\u62df\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7a00\u758f\u5956\u52b1\u548c\u4fdd\u5b88\u884c\u4e3a\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u72b6\u6001\u907f\u514d\u548c\u7ea6\u675fMarkov\u51b3\u7b56\u8fc7\u7a0b\uff09\u96be\u4ee5\u6ee1\u8db3\u8f68\u8ff9\u9700\u6c42\u6216\u8fc7\u4e8e\u4fdd\u5b88\u3002LTL\u867d\u7136\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u89c4\u8303\uff0c\u4f46\u7a00\u758f\u5956\u52b1\u4f7f\u5b66\u4e60\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06LTL\u4e0e\u53ef\u5fae\u5206\u6a21\u62df\u5668\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f6f\u6807\u8bb0\u5b9e\u73b0\u53ef\u5fae\u5206\u5956\u52b1\u548c\u72b6\u6001\uff0c\u89e3\u51b3\u4e86LTL\u7684\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u6b63\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5956\u52b1\u83b7\u53d6\u548c\u8bad\u7ec3\u65f6\u95f4\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u79bb\u6563\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06LTL\u89c4\u8303\u4e0e\u53ef\u5fae\u5206\u5b66\u4e60\u7ed3\u5408\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60,\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTL\uff09,\u53ef\u5fae\u5206\u6a21\u62df\u5668,\u7a00\u758f\u5956\u52b1,\u5b89\u5168\u63a7\u5236\u5668"}}
{"id": "2506.01195", "pdf": "https://arxiv.org/pdf/2506.01195", "abs": "https://arxiv.org/abs/2506.01195", "authors": ["Anshun Asher Zheng", "Junyi Jessy Li", "David I. Beaver"], "title": "CoBRA: Quantifying Strategic Language Use and LLM Pragmatics", "categories": ["cs.CL"], "comment": "18 pages", "summary": "Language is often used strategically, particularly in high-stakes,\nadversarial settings, yet most work on pragmatics and LLMs centers on\ncooperativity. This leaves a gap in systematic understanding of non-cooperative\ndiscourse. To address this, we introduce CoBRA (Cooperation-Breach Response\nAssessment), along with three interpretable metrics -- Benefit at Turn (BaT),\nPenalty at Turn (PaT), and Normalized Relative Benefit at Turn (NRBaT) -- to\nquantify the perceived strategic effects of discourse moves. We also present\nCHARM, an annotated dataset of real courtroom cross-examinations, to\ndemonstrate the framework's effectiveness. Using these tools, we evaluate a\nrange of LLMs and show that LLMs generally exhibit limited pragmatic\nunderstanding of strategic language. While model size shows an increase in\nperformance on our metrics, reasoning ability does not help and largely hurts,\nintroducing overcomplication and internal confusion.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5f15\u5165CoBRA\u6846\u67b6\u548cCHARM\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86LLM\u5728\u975e\u5408\u4f5c\u8bed\u5883\u4e2d\u5bf9\u6218\u7565\u8bed\u8a00\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u586b\u8865\u4e86LLM\u5728\u975e\u5408\u4f5c\u8bdd\u8bed\u4e2d\u7cfb\u7edf\u6027\u7406\u89e3\u7684\u7a7a\u767d\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u7684\u5bf9\u6297\u6027\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86CoBRA\u6846\u67b6\u548c\u4e09\u4e2a\u91cf\u5316\u6307\u6807\uff08BaT\u3001PaT\u3001NRBaT\uff09\uff0c\u5e76\u4f7f\u7528CHARM\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "LLM\u5728\u6218\u7565\u8bed\u8a00\u7406\u89e3\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u6a21\u578b\u89c4\u6a21\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u63a8\u7406\u80fd\u529b\u53cd\u800c\u5e26\u6765\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "LLM\u5728\u975e\u5408\u4f5c\u8bed\u5883\u4e2d\u7684\u5b9e\u7528\u7406\u89e3\u80fd\u529b\u4ecd\u6709\u5f85\u63d0\u5347\u3002", "keywords": "\u975e\u5408\u4f5c\u8bdd\u8bed, \u6218\u7565\u8bed\u8a00, LLM\u8bc4\u4f30, CoBRA, CHARM"}}
{"id": "2506.01177", "pdf": "https://arxiv.org/pdf/2506.01177", "abs": "https://arxiv.org/abs/2506.01177", "authors": ["Andrew Smith", "Erhan Guven"], "title": "Bridging Quantum and Classical Computing in Drug Design: Architecture Principles for Improved Molecule Generation", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": "10 pages, 7 figures", "summary": "Hybrid quantum-classical machine learning offers a path to leverage noisy\nintermediate-scale quantum (NISQ) devices for drug discovery, but optimal model\narchitectures remain unclear. We systematically optimize the quantum-classical\nbridge architecture for generative adversarial networks (GANs) in molecular\ndiscovery using multi-objective Bayesian optimization. Our optimized model\n(BO-QGAN) significantly improves performance, achieving a 2.27-fold higher Drug\nCandidate Score (DCS) than prior quantum-hybrid benchmarks and 2.21-fold higher\nthan the classical baseline, using over 60% fewer parameters. Key findings\nfavor layering multiple (3-4) shallow (4-8 qubit) quantum circuits\nsequentially, while classical architecture shows less sensitivity above a\nminimum capacity. This work provides the first empirically grounded\narchitectural guidelines for hybrid models, enabling more effective integration\nof current quantum computers into pharmaceutical research pipelines.", "AI": {"tldr": "\u901a\u8fc7\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u7814\u7a76\u4f18\u5316\u4e86\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u836f\u7269\u53d1\u73b0\u6027\u80fd\u3002", "motivation": "\u5229\u7528NISQ\u8bbe\u5907\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u6df7\u5408\u6a21\u578b\u67b6\u6784\u4e0d\u660e\u786e\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u4f18\u5316\u91cf\u5b50-\u7ecf\u5178\u6865\u63a5\u67b6\u6784\u3002", "result": "BO-QGAN\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0cDCS\u5206\u6570\u8f83\u57fa\u51c6\u63d0\u9ad82.27\u500d\uff0c\u53c2\u6570\u51cf\u5c1160%\u3002", "conclusion": "\u591a\u6d45\u5c42\u91cf\u5b50\u7535\u8def\u987a\u5e8f\u5806\u53e0\u662f\u4f18\u5316\u65b9\u5411\uff0c\u4e3a\u6df7\u5408\u6a21\u578b\u63d0\u4f9b\u5b9e\u8bc1\u6307\u5357\u3002", "keywords": "\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408,GAN,NISQ,\u8d1d\u53f6\u65af\u4f18\u5316,\u836f\u7269\u53d1\u73b0"}}
{"id": "2506.01197", "pdf": "https://arxiv.org/pdf/2506.01197", "abs": "https://arxiv.org/abs/2506.01197", "authors": ["Mark Muchane", "Sean Richardson", "Kiho Park", "Victor Veitch"], "title": "Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Code is available at\n  https://github.com/muchanem/hierarchical-sparse-autoencoders", "summary": "Sparse dictionary learning (and, in particular, sparse autoencoders) attempts\nto learn a set of human-understandable concepts that can explain variation on\nan abstract space. A basic limitation of this approach is that it neither\nexploits nor represents the semantic relationships between the learned\nconcepts. In this paper, we introduce a modified SAE architecture that\nexplicitly models a semantic hierarchy of concepts. Application of this\narchitecture to the internal representations of large language models shows\nboth that semantic hierarchy can be learned, and that doing so improves both\nreconstruction and interpretability. Additionally, the architecture leads to\nsignificant improvements in computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\u67b6\u6784\uff0c\u663e\u5f0f\u5efa\u6a21\u6982\u5ff5\u7684\u8bed\u4e49\u5c42\u6b21\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7a00\u758f\u5b57\u5178\u5b66\u4e60\u65e0\u6cd5\u6355\u6349\u6982\u5ff5\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u5b66\u4e60\u6548\u679c\u3002", "method": "\u5f15\u5165\u663e\u5f0f\u5efa\u6a21\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\u7684\u6539\u8fdb\u7a00\u758f\u81ea\u7f16\u7801\u5668\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u67b6\u6784\u80fd\u5b66\u4e60\u8bed\u4e49\u5c42\u6b21\uff0c\u63d0\u5347\u91cd\u5efa\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u6539\u8fdb\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u7a00\u758f\u81ea\u7f16\u7801\u5668,\u8bed\u4e49\u5c42\u6b21,\u91cd\u5efa\u6027,\u53ef\u89e3\u91ca\u6027,\u8ba1\u7b97\u6548\u7387"}}
{"id": "2506.00203", "pdf": "https://arxiv.org/pdf/2506.00203", "abs": "https://arxiv.org/abs/2506.00203", "authors": ["Omid Reza Abbasi", "Franz Welscher", "Georg Weinberger", "Johannes Scholz"], "title": "The World As Large Language Models See It: Exploring the reliability of LLMs in representing geographical features", "categories": ["cs.CY", "cs.AI", "cs.IR"], "comment": "9 pages, 4 figures, 2 tables", "summary": "As large language models (LLMs) continue to evolve, questions about their\ntrustworthiness in delivering factual information have become increasingly\nimportant. This concern also applies to their ability to accurately represent\nthe geographic world. With recent advancements in this field, it is relevant to\nconsider whether and to what extent LLMs' representations of the geographical\nworld can be trusted. This study evaluates the performance of GPT-4o and Gemini\n2.0 Flash in three key geospatial tasks: geocoding, elevation estimation, and\nreverse geocoding. In the geocoding task, both models exhibited systematic and\nrandom errors in estimating the coordinates of St. Anne's Column in Innsbruck,\nAustria, with GPT-4o showing greater deviations and Gemini 2.0 Flash\ndemonstrating more precision but a significant systematic offset. For elevation\nestimation, both models tended to underestimate elevations across Austria,\nthough they captured overall topographical trends, and Gemini 2.0 Flash\nperformed better in eastern regions. The reverse geocoding task, which involved\nidentifying Austrian federal states from coordinates, revealed that Gemini 2.0\nFlash outperformed GPT-4o in overall accuracy and F1-scores, demonstrating\nbetter consistency across regions. Despite these findings, neither model\nachieved an accurate reconstruction of Austria's federal states, highlighting\npersistent misclassifications. The study concludes that while LLMs can\napproximate geographic information, their accuracy and reliability are\ninconsistent, underscoring the need for fine-tuning with geographical\ninformation to enhance their utility in GIScience and Geoinformatics.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0GPT-4o\u548cGemini 2.0 Flash\u5728\u7a7a\u95f4\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4e00\u81f4\uff0cGemini 2.0 Flash\u603b\u4f53\u66f4\u4f18\uff0c\u4f46\u5730\u7406\u4fe1\u606f\u51c6\u786e\u6027\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5730\u7406\u7a7a\u95f4\u4efb\u52a1\u4e2d\u7684\u53ef\u4fe1\u5ea6\u95ee\u9898\u3002", "method": "\u8bc4\u4f30GPT-4o\u548cGemini 2.0 Flash\u5728\u4e09\u79cd\u5730\u7406\u4efb\u52a1\uff08\u5730\u7406\u7f16\u7801\u3001\u9ad8\u7a0b\u4f30\u8ba1\u548c\u53cd\u5411\u5730\u7406\u7f16\u7801\uff09\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u4e24\u6a21\u578b\u5728\u5730\u7406\u7f16\u7801\u4e2d\u5747\u5b58\u5728\u8bef\u5dee\uff0cGemini 2.0 Flash\u8868\u73b0\u66f4\u4f18\uff1b\u9ad8\u7a0b\u4f30\u8ba1\u4e2d\u5747\u4f4e\u4f30\u4f46\u8d8b\u52bf\u6b63\u786e\uff1b\u53cd\u5411\u5730\u7406\u7f16\u7801\u4e2dGemini 2.0 Flash\u66f4\u7cbe\u786e\u3002", "conclusion": "LLMs\u80fd\u8fd1\u4f3c\u5730\u7406\u4fe1\u606f\uff0c\u4f46\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u9700\u589e\u5f3a\u5730\u7406\u6570\u636e\u4ee5\u63d0\u9ad8\u5b9e\u7528\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u5730\u7406\u4fe1\u606f, GPT-4o, Gemini 2.0 Flash, \u7a7a\u95f4\u4efb\u52a1"}}
{"id": "2506.01183", "pdf": "https://arxiv.org/pdf/2506.01183", "abs": "https://arxiv.org/abs/2506.01183", "authors": ["Erhan Xu", "Kai Ye", "Hongyi Zhou", "Luhan Zhu", "Francesco Quinzan", "Chengchun Shi"], "title": "Doubly Robust Alignment for Large Language Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "This paper studies reinforcement learning from human feedback (RLHF) for\naligning large language models with human preferences. While RLHF has\ndemonstrated promising results, many algorithms are highly sensitive to\nmisspecifications in the underlying preference model (e.g., the Bradley-Terry\nmodel), the reference policy, or the reward function, resulting in undesirable\nfine-tuning. To address model misspecification, we propose a doubly robust\npreference optimization algorithm that remains consistent when either the\npreference model or the reference policy is correctly specified (without\nrequiring both). Our proposal demonstrates superior and more robust performance\nthan state-of-the-art algorithms, both in theory and in practice. The code is\navailable at https://github.com/DRPO4LLM/DRPO4LLM", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u9c81\u68d2\u504f\u597d\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4ece\u4eba\u7c7b\u53cd\u9988\uff08RLHF\uff09\u4e2d\u6a21\u578b\u8bef\u914d\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7b97\u6cd5\u5728\u504f\u597d\u6a21\u578b\u6216\u53c2\u8003\u7b56\u7565\u6b63\u786e\u65f6\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76RLHF\u5728\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u65f6\u7684\u6a21\u578b\u8bef\u914d\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u9ad8\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u53cc\u9c81\u68d2\u504f\u597d\u4f18\u5316\u7b97\u6cd5\uff0c\u65e0\u9700\u540c\u65f6\u6b63\u786e\u6307\u5b9a\u504f\u597d\u6a21\u578b\u548c\u53c2\u8003\u7b56\u7565\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u5747\u663e\u793a\uff0c\u8be5\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86RLHF\u4e2d\u7684\u6a21\u578b\u8bef\u914d\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u3002", "keywords": "RLHF, \u53cc\u9c81\u68d2, \u504f\u597d\u4f18\u5316, \u6a21\u578b\u8bef\u914d, \u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.01205", "pdf": "https://arxiv.org/pdf/2506.01205", "abs": "https://arxiv.org/abs/2506.01205", "authors": ["Antonia Karamolegkou", "Oliver Eberle", "Phillip Rust", "Carina Kauf", "Anders S\u00f8gaard"], "title": "Trick or Neat: Adversarial Ambiguity and Language Model Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Detecting ambiguity is important for language understanding, including\nuncertainty estimation, humour detection, and processing garden path sentences.\nWe assess language models' sensitivity to ambiguity by introducing an\nadversarial ambiguity dataset that includes syntactic, lexical, and\nphonological ambiguities along with adversarial variations (e.g., word-order\nchanges, synonym replacements, and random-based alterations). Our findings show\nthat direct prompting fails to robustly identify ambiguity, while linear probes\ntrained on model representations can decode ambiguity with high accuracy,\nsometimes exceeding 90\\%. Our results offer insights into the prompting\nparadigm and how language models encode ambiguity at different layers. We\nrelease both our code and data: https://github.com/coastalcph/lm_ambiguity.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\u5bf9\u6b67\u4e49\u7684\u654f\u611f\u6027\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u5bf9\u6297\u6027\u6b67\u4e49\u6570\u636e\u96c6\uff0c\u5305\u62ec\u53e5\u6cd5\u3001\u8bcd\u6c47\u548c\u8bed\u97f3\u6b67\u4e49\u3002\u7814\u7a76\u53d1\u73b0\u76f4\u63a5\u63d0\u793a\u65e0\u6cd5\u7a33\u5065\u8bc6\u522b\u6b67\u4e49\uff0c\u800c\u57fa\u4e8e\u6a21\u578b\u8868\u793a\u7684\u7ebf\u6027\u63a2\u9488\u80fd\u9ad8\u7cbe\u5ea6\u89e3\u7801\u6b67\u4e49\u3002", "motivation": "\u660e\u786e\u6b67\u4e49\u68c0\u6d4b\u5bf9\u8bed\u8a00\u7406\u89e3\u7684\u91cd\u8981\u6027\uff0c\u5305\u62ec\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3001\u5e7d\u9ed8\u68c0\u6d4b\u7b49\uff0c\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5bf9\u6b67\u4e49\u7684\u654f\u611f\u6027\u3002", "method": "\u5f15\u5165\u5bf9\u6297\u6027\u6b67\u4e49\u6570\u636e\u96c6\uff08\u542b\u53e5\u6cd5\u3001\u8bcd\u6c47\u548c\u8bed\u97f3\u6b67\u4e49\u53ca\u5bf9\u6297\u6027\u53d8\u4f53\uff09\uff0c\u901a\u8fc7\u76f4\u63a5\u63d0\u793a\u548c\u7ebf\u6027\u63a2\u9488\u5206\u6790\u6a21\u578b\u8868\u73b0\u3002", "result": "\u76f4\u63a5\u63d0\u793a\u6548\u679c\u4e0d\u4f73\uff0c\u7ebf\u6027\u63a2\u9488\u8bc6\u522b\u6b67\u4e49\u51c6\u786e\u7387\u9ad8\u8fbe90%\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u5c42\u7f16\u7801\u6b67\u4e49\u7684\u65b9\u5f0f\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u793a\u8303\u5f0f\u63d0\u4f9b\u89c1\u89e3\uff0c\u5e76\u5c55\u793a\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u7f16\u7801\u6b67\u4e49\u3002", "keywords": "\u6b67\u4e49\u68c0\u6d4b, \u8bed\u8a00\u6a21\u578b, \u5bf9\u6297\u6027\u6570\u636e\u96c6, \u7ebf\u6027\u63a2\u9488"}}
{"id": "2506.01194", "pdf": "https://arxiv.org/pdf/2506.01194", "abs": "https://arxiv.org/abs/2506.01194", "authors": ["Divyansh Jhunjhunwala", "Arian Raje", "Madan Ravi Ganesh", "Chaithanya Kumar Mummadi", "Chaoqun Dong", "Jiawei Zhou", "Wan-Yi Lin", "Gauri Joshi", "Zhenzhen Li"], "title": "FedRPCA: Enhancing Federated LoRA Aggregation Using Robust PCA", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "LoRA has emerged as one of the most promising fine-tuning techniques,\nespecially for federated learning (FL), since it significantly reduces\ncommunication and computation costs at resource-constrained clients. However,\ndata heterogeneity remains a significant challenge for LoRA-based FL, and the\nconventional aggregation strategy based on FedAvg suffers from slow convergence\nand suboptimal accuracy. Motivated by recent advances in model merging,\nparticularly Task Arithmetic, we explore the idea of aggregating client LoRA\nparameters using scaled averaging. We first observe that a naive application of\nTask Arithmetic is ineffective due to the high cosine similarity between client\nupdates, indicating significant common knowledge in the updates across clients.\nTo address this issue, we propose decomposing client LoRA updates via Robust\nPrincipal Component Analysis (Robust-PCA) into a common low-rank component and\nclient-specific sparse components. Our proposed algorithm FedRPCA aggregates\nthe low-rank components through averaging, consolidating common knowledge, and\napplies scaled averaging to the sparse components to amplify client-specific\nknowledge. We evaluate our approach across a variety of vision and language\ntasks and demonstrate that it achieves higher final accuracy and faster\nconvergence compared to competing baselines.", "AI": {"tldr": "LoRA\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6570\u636e\u5f02\u6784\u6027\u5bfc\u81f4\u6536\u655b\u6162\u548c\u7cbe\u5ea6\u4e0d\u8db3\u3002\u7814\u7a76\u63d0\u51faFedRPCA\u7b97\u6cd5\uff0c\u901a\u8fc7Robust-PCA\u5206\u89e3LoRA\u53c2\u6570\uff0c\u7ed3\u5408\u5e73\u5747\u548c\u7f29\u653e\u5e73\u5747\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LoRA\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u56e0\u6570\u636e\u5f02\u6784\u6027\u5bfc\u81f4\u7684\u6536\u655b\u6162\u548c\u7cbe\u5ea6\u95ee\u9898\u3002", "method": "\u63d0\u51faFedRPCA\u7b97\u6cd5\uff0c\u5229\u7528Robust-PCA\u5206\u89e3LoRA\u53c2\u6570\u4e3a\u4f4e\u79e9\u548c\u7a00\u758f\u5206\u91cf\uff0c\u5206\u522b\u91c7\u7528\u5e73\u5747\u548c\u7f29\u653e\u5e73\u5747\u805a\u5408\u3002", "result": "\u5728\u591a\u79cd\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d\uff0cFedRPCA\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6700\u7ec8\u7cbe\u5ea6\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "FedRPCA\u6709\u6548\u63d0\u5347LoRA\u5728\u5f02\u6784\u6570\u636e\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u3002", "keywords": "LoRA, \u8054\u90a6\u5b66\u4e60, Robust-PCA, \u6570\u636e\u5f02\u6784\u6027, FedRPCA"}}
{"id": "2506.01206", "pdf": "https://arxiv.org/pdf/2506.01206", "abs": "https://arxiv.org/abs/2506.01206", "authors": ["Daewon Choi", "Seunghyuk Oh", "Saket Dingliwal", "Jihoon Tack", "Kyuyoung Kim", "Woomin Song", "Seojin Kim", "Insu Han", "Jinwoo Shin", "Aram Galstyan", "Shubham Katiyar", "Sravan Babu Bodapati"], "title": "Mamba Drafters for Speculative Decoding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding has emerged as a promising approach to accelerating\nlarge language model (LLM) generation using a fast drafter while maintaining\nalignment with the target model's distribution. However, existing approaches\nface a trade-off: external drafters offer flexibility but can suffer from\nslower drafting, while self-speculation methods use drafters tailored to the\ntarget model but require re-training. In this paper, we introduce novel\ndrafters based on Mamba, a state-of-the-art state space model (SSM), as a\nsolution that combines the best aspects of both approaches. By leveraging the\nlinear structure of SSMs, our approach avoids the quadratic complexity inherent\nin traditional Transformer-based methods, enabling faster drafting and lower\nmemory usage while maintaining the flexibility to work across different target\nmodels. We further enhance efficiency with a novel test-time tree search\nalgorithm for generating high-quality draft candidates. Our empirical\nevaluation demonstrates that Mamba-based drafters not only outperform existing\nexternal drafting methods but are also comparable to state-of-the-art\nself-speculation approaches while using less memory and maintaining their\ncross-model adaptability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u65b0\u578bdrafters\uff0c\u7528\u4e8e\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\uff0c\u7ed3\u5408\u4e86\u5916\u90e8drafters\u7684\u7075\u6d3b\u6027\u548c\u81ea\u63a8\u6d4b\u65b9\u6cd5\u7684\u9ad8\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u5728\u7075\u6d3b\u6027\u548c\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u5916\u90e8drafters\u901f\u5ea6\u6162\uff0c\u800c\u81ea\u63a8\u6d4b\u65b9\u6cd5\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u5229\u7528Mamba\uff08\u4e00\u79cd\u6700\u5148\u8fdb\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff09\u7684\u7ebf\u6027\u7ed3\u6784\uff0c\u907f\u514d\u4e86\u4f20\u7edfTransformer\u65b9\u6cd5\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u5e76\u7ed3\u5408\u65b0\u9896\u7684\u6d4b\u8bd5\u65f6\u6811\u641c\u7d22\u7b97\u6cd5\u63d0\u5347\u6548\u7387\u3002", "result": "Mamba-based drafters\u4f18\u4e8e\u73b0\u6709\u5916\u90e8drafters\u65b9\u6cd5\uff0c\u4e14\u5728\u5185\u5b58\u4f7f\u7528\u548c\u8de8\u6a21\u578b\u9002\u5e94\u6027\u4e0a\u5ab2\u7f8e\u81ea\u63a8\u6d4b\u65b9\u6cd5\u3002", "conclusion": "Mamba-based drafters\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u52a0\u901fLLM\u751f\u6210\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "speculative decoding, large language model, Mamba, state space model, tree search"}}
{"id": "2506.01208", "pdf": "https://arxiv.org/pdf/2506.01208", "abs": "https://arxiv.org/abs/2506.01208", "authors": ["Rapha\u00ebl Romero", "Tijl De Bie", "Nick Heard", "Alexander Modell"], "title": "Multiresolution Analysis and Statistical Thresholding on Dynamic Networks", "categories": ["cs.LG"], "comment": null, "summary": "Detecting structural change in dynamic network data has wide-ranging\napplications. Existing approaches typically divide the data into time bins,\nextract network features within each bin, and then compare these features over\ntime. This introduces an inherent tradeoff between temporal resolution and the\nstatistical stability of the extracted features. Despite this tradeoff,\nreminiscent of time-frequency tradeoffs in signal processing, most methods rely\non a fixed temporal resolution. Choosing an appropriate resolution parameter is\ntypically difficult and can be especially problematic in domains like\ncybersecurity, where anomalous behavior may emerge at multiple time scales. We\naddress this challenge by proposing ANIE (Adaptive Network Intensity\nEstimation), a multi-resolution framework designed to automatically identify\nthe time scales at which network structure evolves, enabling the joint\ndetection of both rapid and gradual changes. Modeling interactions as Poisson\nprocesses, our method proceeds in two steps: (1) estimating a low-dimensional\nsubspace of node behavior, and (2) deriving a set of novel empirical affinity\ncoefficients that quantify change in interaction intensity between latent\nfactors and support statistical testing for structural change across time\nscales. We provide theoretical guarantees for subspace estimation and the\nasymptotic behavior of the affinity coefficients, enabling model-based change\ndetection. Experiments on synthetic networks show that ANIE adapts to the\nappropriate time resolution and is able to capture sharp structural changes\nwhile remaining robust to noise. Furthermore, applications to real-world data\nshowcase the practical benefits of ANIE's multiresolution approach to detecting\nstructural change over fixed resolution methods.", "AI": {"tldr": "ANIE\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u591a\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u7f51\u7edc\u6570\u636e\u4e2d\u7684\u7ed3\u6784\u53d8\u5316\u68c0\u6d4b\uff0c\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u53d8\u5316\u7684\u65f6\u6807\uff0c\u517c\u987e\u5feb\u901f\u548c\u7f13\u6162\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u56fa\u5b9a\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u96be\u4ee5\u5e73\u8861\u65f6\u95f4\u548c\u7edf\u8ba1\u7a33\u5b9a\u6027\uff0c\u5c24\u5176\u5728\u591a\u65f6\u6807\u5f02\u5e38\u7684\u9886\u57df\uff08\u5982\u7f51\u7edc\u5b89\u5168\uff09\u4e2d\u95ee\u9898\u7a81\u51fa\u3002", "method": "ANIE\u5206\u4e3a\u4e24\u6b65\uff1a(1)\u4f30\u8ba1\u8282\u70b9\u884c\u4e3a\u7684\u4f4e\u7ef4\u5b50\u7a7a\u95f4\uff0c(2)\u8ba1\u7b97\u65b0\u9896\u7684\u7ecf\u9a8c\u4eb2\u548c\u7cfb\u6570\u4ee5\u91cf\u5316\u4e0d\u540c\u65f6\u6807\u7684\u4ea4\u4e92\u5f3a\u5ea6\u53d8\u5316\u5e76\u652f\u6301\u7edf\u8ba1\u6d4b\u8bd5\u3002", "result": "\u7406\u8bba\u4fdd\u8bc1\u4e86\u5b50\u7a7a\u95f4\u4f30\u8ba1\u548c\u4eb2\u548c\u7cfb\u6570\u7684\u6e10\u8fd1\u884c\u4e3a\uff0c\u5b9e\u9a8c\u8868\u660eANIE\u80fd\u81ea\u9002\u5e94\u65f6\u6807\u3001\u6355\u6349\u7a81\u53d8\u4e14\u5bf9\u566a\u58f0\u9c81\u68d2\u3002", "conclusion": "ANIE\u5728\u591a\u5206\u8fa8\u7387\u7ed3\u6784\u53d8\u5316\u68c0\u6d4b\u4e2d\u4f18\u4e8e\u56fa\u5b9a\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u95ee\u9898\u3002", "keywords": "\u52a8\u6001\u7f51\u7edc, \u7ed3\u6784\u53d8\u5316\u68c0\u6d4b, \u591a\u5206\u8fa8\u7387\u5206\u6790, ANIE, \u7f51\u7edc\u5b89\u5168"}}
{"id": "2506.01215", "pdf": "https://arxiv.org/pdf/2506.01215", "abs": "https://arxiv.org/abs/2506.01215", "authors": ["Woomin Song", "Sai Muralidhar Jayanthi", "Srikanth Ronanki", "Kanthashree Mysore Sathyendra", "Jinwoo Shin", "Aram Galstyan", "Shubham Katiyar", "Sravan Babu Bodapati"], "title": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models increasingly gain popularity in real-world\napplications, processing extremely long contexts, often exceeding the model's\npre-trained context limits, has emerged as a critical challenge. While existing\napproaches to efficient long-context processing show promise, recurrent\ncompression-based methods struggle with information preservation, whereas\nrandom access approaches require substantial memory resources. We introduce\nREFORM, a novel inference framework that efficiently handles long contexts\nthrough a two-phase approach. First, it incrementally processes input chunks\nwhile maintaining a compressed KV cache, constructs cross-layer context\nembeddings, and utilizes early exit strategy for improved efficiency. Second,\nit identifies and gathers essential tokens via similarity matching and\nselectively recomputes the KV cache. Compared to baselines, REFORM achieves\nover 50% and 27% performance gains on RULER and BABILong respectively at 1M\ncontext length. It also outperforms baselines on Infinite-Bench and MM-NIAH,\ndemonstrating flexibility across diverse tasks and domains. Additionally,\nREFORM reduces inference time by 30% and peak memory usage by 5%, achieving\nboth efficiency and superior performance.", "AI": {"tldr": "REFORM\u662f\u4e00\u79cd\u9ad8\u6548\u5904\u7406\u957f\u6587\u672c\u7684\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u91cf\u5904\u7406\u548c\u9009\u62e9\u6027KV\u7f13\u5b58\u91cd\u7ec4\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u8d85\u51fa\u9884\u8bad\u7ec3\u9650\u5236\u7684\u957f\u6587\u672c\u65f6\uff0c\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u6216\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7b56\u7565\uff1a\u589e\u91cf\u5904\u7406\u4e0e\u538b\u7f29KV\u7f13\u5b58\uff0c\u4ee5\u53ca\u76f8\u4f3c\u6027\u5339\u914d\u9009\u62e9\u5fc5\u8981\u4ee4\u724c\u3002", "result": "\u57281M\u6587\u672c\u957f\u5ea6\u4e0b\uff0c\u6027\u80fd\u63d0\u5347\u8d8550%\u548c27%\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1130%\uff0c\u5185\u5b58\u5360\u7528\u964d\u4f4e5%\u3002", "conclusion": "REFORM\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "keywords": "\u957f\u6587\u672c\u5904\u7406, KV\u7f13\u5b58, \u589e\u91cf\u5904\u7406, \u76f8\u4f3c\u6027\u5339\u914d, \u6027\u80fd\u63d0\u5347"}}
{"id": "2506.00214", "pdf": "https://arxiv.org/pdf/2506.00214", "abs": "https://arxiv.org/abs/2506.00214", "authors": ["Abhijeet Vishwasrao", "Sai Bharath Chandra Gutha", "Andres Cremades", "Klas Wijk", "Aakash Patil", "Catherine Gorle", "Beverley J McKeon", "Hossein Azizpour", "Ricardo Vinuesa"], "title": "Diff-SPORT: Diffusion-based Sensor Placement Optimization and Reconstruction of Turbulent flows in urban environments", "categories": ["physics.flu-dyn", "cs.AI"], "comment": null, "summary": "Rapid urbanization demands accurate and efficient monitoring of turbulent\nwind patterns to support air quality, climate resilience and infrastructure\ndesign. Traditional sparse reconstruction and sensor placement strategies face\nmajor accuracy degradations under practical constraints. Here, we introduce\nDiff-SPORT, a diffusion-based framework for high-fidelity flow reconstruction\nand optimal sensor placement in urban environments. Diff-SPORT combines a\ngenerative diffusion model with a maximum a posteriori (MAP) inference scheme\nand a Shapley-value attribution framework to propose a scalable and\ninterpretable solution. Compared to traditional numerical methods, Diff-SPORT\nachieves significant speedups while maintaining both statistical and\ninstantaneous flow fidelity. Our approach offers a modular, zero-shot\nalternative to retraining-intensive strategies, supporting fast and reliable\nurban flow monitoring under extreme sparsity. Diff-SPORT paves the way for\nintegrating generative modeling and explainability in sustainable urban\nintelligence.", "AI": {"tldr": "Diff-SPORT\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9ad8\u7cbe\u5ea6\u6e4d\u6d41\u91cd\u5efa\u548c\u4f20\u611f\u5668\u4f18\u5316\u653e\u7f6e\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6700\u5927\u540e\u9a8c\u63a8\u7406\u548cShapley\u503c\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u5e76\u4fdd\u6301\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u5feb\u901f\u57ce\u5e02\u5316\u4e2d\u7684\u98ce\u573a\u76d1\u6d4b\u3002", "motivation": "\u5feb\u901f\u57ce\u5e02\u5316\u9700\u8981\u9ad8\u6548\u51c6\u786e\u7684\u98ce\u573a\u76d1\u6d4b\u4ee5\u652f\u6301\u7a7a\u6c14\u8d28\u91cf\u3001\u6c14\u5019\u9002\u5e94\u6027\u548c\u57fa\u7840\u8bbe\u65bd\u8bbe\u8ba1\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u5b9e\u7528\u6027\u7ea6\u675f\u4e0b\u7cbe\u5ea6\u4e0b\u964d\u4e25\u91cd\u3002", "method": "\u7ed3\u5408\u751f\u6210\u6269\u6563\u6a21\u578b\u3001\u6700\u5927\u540e\u9a8c\u63a8\u7406\u548cShapley\u503c\u6846\u67b6\uff0c\u63d0\u51fa\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684Diff-SPORT\u65b9\u6cd5\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\uff0cDiff-SPORT\u663e\u8457\u52a0\u901f\u4e14\u4fdd\u6301\u4e86\u7edf\u8ba1\u548c\u77ac\u65f6\u6d41\u573a\u7684\u4fdd\u771f\u5ea6\u3002", "conclusion": "Diff-SPORT\u4e3a\u53ef\u6301\u7eed\u57ce\u5e02\u667a\u80fd\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u3001\u96f6\u6837\u672c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u6781\u7aef\u7a00\u758f\u6761\u4ef6\u4e0b\u7684\u5feb\u901f\u53ef\u9760\u76d1\u6d4b\u3002", "keywords": "\u57ce\u5e02\u5316\u3001\u98ce\u573a\u91cd\u5efa\u3001\u6269\u6563\u6a21\u578b\u3001\u4f20\u611f\u5668\u4f18\u5316\u3001\u53ef\u6301\u7eed\u667a\u80fd"}}
{"id": "2506.01212", "pdf": "https://arxiv.org/pdf/2506.01212", "abs": "https://arxiv.org/abs/2506.01212", "authors": ["Menglin Kong", "Vincent Zhihao Zheng", "Xudong Wang", "Lijun Sun"], "title": "Dynamic Modes as Time Representation for Spatiotemporal Forecasting", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This paper introduces a data-driven time embedding method for modeling\nlong-range seasonal dependencies in spatiotemporal forecasting tasks. The\nproposed approach employs Dynamic Mode Decomposition (DMD) to extract temporal\nmodes directly from observed data, eliminating the need for explicit timestamps\nor hand-crafted time features. These temporal modes serve as time\nrepresentations that can be seamlessly integrated into deep spatiotemporal\nforecasting models. Unlike conventional embeddings such as time-of-day\nindicators or sinusoidal functions, our method captures complex multi-scale\nperiodicity through spectral analysis of spatiotemporal data. Extensive\nexperiments on urban mobility, highway traffic, and climate datasets\ndemonstrate that the DMD-based embedding consistently improves long-horizon\nforecasting accuracy, reduces residual correlation, and enhances temporal\ngeneralization. The method is lightweight, model-agnostic, and compatible with\nany architecture that incorporates time covariates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65f6\u95f4\u5d4c\u5165\u65b9\u6cd5\uff0c\u5229\u7528\u52a8\u6001\u6a21\u6001\u5206\u89e3\u63d0\u53d6\u65f6\u95f4\u6a21\u6001\uff0c\u63d0\u5347\u65f6\u7a7a\u9884\u6d4b\u7684\u957f\u8303\u56f4\u4f9d\u8d56\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5d4c\u5165\u65b9\u6cd5\uff08\u5982\u65f6\u95f4\u6307\u793a\u5668\u6216\u6b63\u5f26\u51fd\u6570\uff09\u65e0\u6cd5\u6709\u6548\u6355\u6349\u591a\u5c3a\u5ea6\u5468\u671f\u6027\uff0c\u9700\u6539\u8fdb\u4ee5\u63d0\u5347\u957f\u671f\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u52a8\u6001\u6a21\u6001\u5206\u89e3\uff08DMD\uff09\u4ece\u89c2\u6d4b\u6570\u636e\u76f4\u63a5\u63d0\u53d6\u65f6\u95f4\u6a21\u6001\uff0c\u65e0\u9700\u663e\u5f0f\u65f6\u95f4\u6233\u6216\u624b\u5de5\u7279\u5f81\u3002", "result": "\u5728\u57ce\u5e02\u4ea4\u901a\u3001\u9ad8\u901f\u516c\u8def\u6d41\u91cf\u548c\u6c14\u5019\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cDMD\u5d4c\u5165\u663e\u8457\u63d0\u5347\u957f\u671f\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u6b8b\u5dee\u76f8\u5173\u6027\uff0c\u589e\u5f3a\u65f6\u95f4\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8f7b\u91cf\u3001\u6a21\u578b\u65e0\u5173\uff0c\u9002\u5408\u4efb\u4f55\u5305\u542b\u65f6\u95f4\u534f\u53d8\u91cf\u7684\u67b6\u6784\uff0c\u4e3a\u65f6\u7a7a\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u65f6\u95f4\u8868\u5f81\u65b9\u6848\u3002", "keywords": "\u6570\u636e\u9a71\u52a8\u3001\u65f6\u95f4\u5d4c\u5165\u3001\u52a8\u6001\u6a21\u6001\u5206\u89e3\u3001\u65f6\u7a7a\u9884\u6d4b\u3001\u591a\u5c3a\u5ea6\u5468\u671f\u6027"}}
{"id": "2506.01237", "pdf": "https://arxiv.org/pdf/2506.01237", "abs": "https://arxiv.org/abs/2506.01237", "authors": ["SungHo Kim", "Nayeon Kim", "Taehee Jeon", "SangKeun Lee"], "title": "Polishing Every Facet of the GEM: Testing Linguistic Competence of LLMs and Humans in Korean", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 main conference", "summary": "We introduce the $\\underline{Ko}rean \\underline{G}rammar\n\\underline{E}valuation Bench\\underline{M}ark (KoGEM)$, designed to assess the\nlinguistic competence of LLMs and humans in Korean. KoGEM consists of 1.5k\nmultiple-choice QA pairs covering five main categories and 16 subcategories.\nThe zero-shot evaluation of 27 LLMs of various sizes and types reveals that\nwhile LLMs perform remarkably well on straightforward tasks requiring primarily\ndefinitional knowledge, they struggle with tasks that demand the integration of\nreal-world experiential knowledge, such as phonological rules and\npronunciation. Furthermore, our in-depth analysis suggests that incorporating\nsuch experiential knowledge could enhance the linguistic competence of LLMs.\nWith KoGEM, we not only highlight the limitations of current LLMs in linguistic\ncompetence but also uncover hidden facets of LLMs in linguistic competence,\npaving the way for enhancing comprehensive language understanding. Our code and\ndataset are available at: https://github.com/SungHo3268/KoGEM.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u97e9\u56fd\u8bed\u8bed\u6cd5\u8bc4\u4f30\u57fa\u51c6\uff08KoGEM\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u4eba\u7c7b\u7684\u97e9\u8bed\u8bed\u8a00\u80fd\u529b\uff0c\u53d1\u73b0LLMs\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\u4f46\u5728\u9700\u8981\u5b9e\u9645\u7ecf\u9a8c\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u65e8\u5728\u8bc4\u4f30LLMs\u548c\u4eba\u7c7b\u5728\u97e9\u8bed\u4e2d\u7684\u8bed\u8a00\u80fd\u529b\uff0c\u63ed\u793aLLMs\u7684\u5c40\u9650\u6027\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u5411\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b1.5k\u591a\u9009\u9898\u7684KoGEM\uff0c\u6db5\u76d65\u5927\u7c7b16\u5b50\u7c7b\uff0c\u5e76\u5bf927\u79cdLLMs\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\u3002", "result": "LLMs\u5728\u5b9a\u4e49\u6027\u77e5\u8bc6\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u5b9e\u9645\u7ecf\u9a8c\u7684\u4efb\u52a1\uff08\u5982\u97f3\u7cfb\u89c4\u5219\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "KoGEM\u4e0d\u4ec5\u63ed\u793a\u4e86LLMs\u7684\u8bed\u8a00\u80fd\u529b\u5c40\u9650\u6027\uff0c\u8fd8\u63d0\u51fa\u4e86\u901a\u8fc7\u6574\u5408\u5b9e\u8df5\u7ecf\u9a8c\u63d0\u5347\u5176\u8bed\u8a00\u80fd\u529b\u7684\u53ef\u80fd\u6027\u3002", "keywords": "KoGEM, \u97e9\u56fd\u8bed, \u8bed\u8a00\u80fd\u529b\u8bc4\u4f30, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u97f3\u7cfb\u89c4\u5219"}}
{"id": "2506.00227", "pdf": "https://arxiv.org/pdf/2506.00227", "abs": "https://arxiv.org/abs/2506.00227", "authors": ["Anthony Gosselin", "Ge Ya Luo", "Luis Lara", "Florian Golemo", "Derek Nowrouzezahrai", "Liam Paull", "Alexia Jolicoeur-Martineau", "Christopher Pal"], "title": "Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Under review", "summary": "Video diffusion techniques have advanced significantly in recent years;\nhowever, they struggle to generate realistic imagery of car crashes due to the\nscarcity of accident events in most driving datasets. Improving traffic safety\nrequires realistic and controllable accident simulations. To tackle the\nproblem, we propose Ctrl-Crash, a controllable car crash video generation model\nthat conditions on signals such as bounding boxes, crash types, and an initial\nimage frame. Our approach enables counterfactual scenario generation where\nminor variations in input can lead to dramatically different crash outcomes. To\nsupport fine-grained control at inference time, we leverage classifier-free\nguidance with independently tunable scales for each conditioning signal.\nCtrl-Crash achieves state-of-the-art performance across quantitative video\nquality metrics (e.g., FVD and JEDi) and qualitative measurements based on a\nhuman-evaluation of physical realism and video quality compared to prior\ndiffusion-based methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53ef\u63a7\u7684\u8f66\u7978\u89c6\u9891\u751f\u6210\u6a21\u578bCtrl-Crash\uff0c\u901a\u8fc7\u8fb9\u754c\u6846\u3001\u78b0\u649e\u7c7b\u578b\u7b49\u4fe1\u53f7\u751f\u6210\u903c\u771f\u4e14\u53ef\u63a7\u7684\u8f66\u7978\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u6563\u6280\u672f\u96be\u4ee5\u751f\u6210\u903c\u771f\u7684\u8f66\u7978\u573a\u666f\uff0c\u56e0\u9a7e\u9a76\u6570\u636e\u96c6\u4e2d\u4e8b\u6545\u4e8b\u4ef6\u7a00\u7f3a\uff0c\u9700\u63d0\u5347\u4ea4\u901a\u5b89\u5168\u4eff\u771f\u80fd\u529b\u3002", "method": "\u5229\u7528\u8fb9\u754c\u6846\u3001\u78b0\u649e\u7c7b\u578b\u548c\u521d\u59cb\u5e27\u7b49\u4fe1\u53f7\uff0c\u7ed3\u5408\u5206\u7c7b\u5668\u65e0\u5173\u5f15\u5bfc\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\u7684\u89c6\u9891\u751f\u6210\u3002", "result": "\u5728\u89c6\u9891\u8d28\u91cf\u6307\u6807\uff08\u5982FVD\u3001JEDi\uff09\u548c\u4eba\u7c7b\u8bc4\u4f30\u4e2d\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6269\u6563\u65b9\u6cd5\u3002", "conclusion": "Ctrl-Crash\u80fd\u9ad8\u6548\u751f\u6210\u903c\u771f\u4e14\u53ef\u63a7\u7684\u8f66\u7978\u89c6\u9891\uff0c\u4e3a\u4ea4\u901a\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u6709\u529b\u5de5\u5177\u3002", "keywords": "\u89c6\u9891\u751f\u6210, \u8f66\u7978\u4eff\u771f, \u6269\u6563\u6a21\u578b, \u53ef\u63a7\u6027, \u4ea4\u901a\u5b89\u5168"}}
{"id": "2506.01213", "pdf": "https://arxiv.org/pdf/2506.01213", "abs": "https://arxiv.org/abs/2506.01213", "authors": ["Ning Zhang", "Henry Kenlay", "Li Zhang", "Mihai Cucuringu", "Xiaowen Dong"], "title": "On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective", "categories": ["cs.LG", "eess.SP", "stat.ML"], "comment": null, "summary": "Graph convolutional neural networks (GCNNs) have emerged as powerful tools\nfor analyzing graph-structured data, achieving remarkable success across\ndiverse applications. However, the theoretical understanding of the stability\nof these models, i.e., their sensitivity to small changes in the graph\nstructure, remains in rather limited settings, hampering the development and\ndeployment of robust and trustworthy models in practice. To fill this gap, we\nstudy how perturbations in the graph topology affect GCNN outputs and propose a\nnovel formulation for analyzing model stability. Unlike prior studies that\nfocus only on worst-case perturbations, our distribution-aware formulation\ncharacterizes output perturbations across a broad range of input data. This\nway, our framework enables, for the first time, a probabilistic perspective on\nthe interplay between the statistical properties of the node data and\nperturbations in the graph topology. We conduct extensive experiments to\nvalidate our theoretical findings and demonstrate their benefits over existing\nbaselines, in terms of both representation stability and adversarial attacks on\ndownstream tasks. Our results demonstrate the practical significance of the\nproposed formulation and highlight the importance of incorporating data\ndistribution into stability analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08GCNNs\uff09\u5728\u62d3\u6251\u6270\u52a8\u4e0b\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u611f\u77e5\u7684\u7a33\u5b9a\u6027\u5206\u6790\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "GCNNs\u5728\u56fe\u7ed3\u6784\u6570\u636e\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u5176\u7a33\u5b9a\u6027\u7684\u7406\u8bba\u7406\u89e3\u4ecd\u6709\u9650\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u4fe1\u5ea6\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u611f\u77e5\u7684\u7a33\u5b9a\u6027\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u8651\u8f93\u5165\u6570\u636e\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u5168\u9762\u5206\u6790\u56fe\u62d3\u6251\u6270\u52a8\u5bf9\u6a21\u578b\u8f93\u51fa\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u8868\u793a\u7a33\u5b9a\u6027\u548c\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5f3a\u8c03\u4e86\u5c06\u6570\u636e\u5206\u5e03\u7eb3\u5165\u7a33\u5b9a\u6027\u5206\u6790\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aGCNNs\u7684\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u89c6\u89d2\uff0c\u8bc1\u660e\u6570\u636e\u5206\u5e03\u5bf9\u7a33\u5b9a\u6027\u5206\u6790\u7684\u5173\u952e\u4f5c\u7528\u3002", "keywords": "\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc, \u7a33\u5b9a\u6027\u5206\u6790, \u5206\u5e03\u611f\u77e5, \u62d3\u6251\u6270\u52a8, \u5bf9\u6297\u6027\u653b\u51fb"}}
{"id": "2506.01241", "pdf": "https://arxiv.org/pdf/2506.01241", "abs": "https://arxiv.org/abs/2506.01241", "authors": ["Jie Ruan", "Inderjeet Nair", "Shuyang Cao", "Amy Liu", "Sheza Munir", "Micah Pollens-Dempsey", "Tiffany Chiang", "Lucy Kates", "Nicholas David", "Sihan Chen", "Ruxin Yang", "Yuqian Yang", "Jasmine Gump", "Tessa Bialek", "Vivek Sankaran", "Margo Schlanger", "Lu Wang"], "title": "ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces ExpertLongBench, an expert-level benchmark containing\n11 tasks from 9 domains that reflect realistic expert workflows and\napplications. Beyond question answering, the application-driven tasks in\nExpertLongBench demand long-form outputs that can exceed 5,000 tokens and\nstrict adherence to domain-specific requirements. Notably, each task in\nExpertLongBench includes a rubric, designed or validated by domain experts, to\nspecify task requirements and guide output evaluation. Furthermore, we propose\nCLEAR, an evaluation framework that supports accurate evaluation of long-form\nmodel outputs in our benchmark. To achieve fine-grained, expert-aligned\nevaluation, CLEAR derives checklists from both model outputs and references by\nextracting information corresponding to items in the task-specific rubric.\nChecklist items for model outputs are then compared with corresponding items\nfor reference outputs to assess their correctness, enabling grounded\nevaluation. We benchmark 11 large language models (LLMs) and analyze components\nin CLEAR, showing that (1) existing LLMs, with the top performer achieving only\na 26.8% F1 score, require significant improvement for expert-level tasks; (2)\nmodels can generate content corresponding to the required aspects, though often\nnot accurately; and (3) accurate checklist extraction and comparison in CLEAR\ncan be achieved by open-weight models for more scalable and low-cost usage.", "AI": {"tldr": "ExpertLongBench\u662f\u4e00\u4e2a\u4e13\u5bb6\u7ea7\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b11\u4e2a\u4efb\u52a1\uff0c\u8986\u76d69\u4e2a\u9886\u57df\uff0c\u6a21\u62df\u771f\u5b9e\u4e13\u5bb6\u5de5\u4f5c\u6d41\u7a0b\u3002\u5176\u8981\u6c42\u957f\u6587\u672c\u8f93\u51fa\uff08\u8d855000\u8bcd\uff09\u5e76\u4e25\u683c\u9075\u5faa\u9886\u57df\u7279\u5b9a\u9700\u6c42\u3002\u63d0\u51fa\u7684CLEAR\u8bc4\u4f30\u6846\u67b6\u901a\u8fc7\u63d0\u53d6\u4efb\u52a1\u8981\u6c42\u5217\u8868\uff0c\u5b9e\u73b0\u4e13\u5bb6\u5bf9\u9f50\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u5728\u4e13\u5bb6\u7ea7\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u63d0\u5347\u5176\u5b9e\u9645\u5e94\u7528\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5305\u542b\u4e13\u5bb6\u9a8c\u8bc1\u7684\u4efb\u52a1\u8981\u6c42\u5217\u8868\uff0c\u5e76\u5f00\u53d1CLEAR\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u8f93\u51fa\u4e0e\u53c2\u8003\u6587\u672c\u7684\u4fe1\u606f\u70b9\u8fdb\u884c\u6bd4\u8f83\u8bc4\u4f30\u3002", "result": "\u6d4b\u8bd511\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u8868\u73b0\u6700\u4f73\u8005F1\u5206\u6570\u4ec5\u4e3a26.8%\uff0c\u663e\u793a\u6a21\u578b\u9700\u663e\u8457\u6539\u8fdb\u3002CLEAR\u6846\u67b6\u53ef\u6269\u5c55\u4e14\u4f4e\u6210\u672c\u3002", "conclusion": "\u4e13\u5bb6\u7ea7\u4efb\u52a1\u5bf9AI\u4ecd\u5177\u6311\u6218\uff0cCLEAR\u63d0\u4f9b\u4e86\u6709\u6548\u8bc4\u4f30\u8def\u5f84\u3002", "keywords": "ExpertLongBench, CLEAR, \u957f\u6587\u672c\u8bc4\u4f30, \u4e13\u5bb6\u4efb\u52a1, \u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.01225", "pdf": "https://arxiv.org/pdf/2506.01225", "abs": "https://arxiv.org/abs/2506.01225", "authors": ["Majdi Hassan", "Cristian Gabellini", "Hatem Helal", "Dominique Beaini", "Kirill Neklyudov"], "title": "Self-Refining Training for Amortized Density Functional Theory", "categories": ["cs.LG"], "comment": null, "summary": "Density Functional Theory (DFT) allows for predicting all the chemical and\nphysical properties of molecular systems from first principles by finding an\napproximate solution to the many-body Schr\\\"odinger equation. However, the cost\nof these predictions becomes infeasible when increasing the scale of the energy\nevaluations, e.g., when calculating the ground-state energy for simulating\nmolecular dynamics. Recent works have demonstrated that, for substantially\nlarge datasets of molecular conformations, Deep Learning-based models can\npredict the outputs of the classical DFT solvers by amortizing the\ncorresponding optimization problems. In this paper, we propose a novel method\nthat reduces the dependency of amortized DFT solvers on large pre-collected\ndatasets by introducing a self-refining training strategy. Namely, we propose\nan efficient method that simultaneously trains a deep-learning model to predict\nthe DFT outputs and samples molecular conformations that are used as training\ndata for the model. We derive our method as a minimization of the variational\nupper bound on the KL-divergence measuring the discrepancy between the\ngenerated samples and the target Boltzmann distribution defined by the ground\nstate energy. To demonstrate the utility of the proposed scheme, we perform an\nextensive empirical study comparing it with the models trained on the\npre-collected datasets. Finally, we open-source our implementation of the\nproposed algorithm, optimized with asynchronous training and sampling stages,\nwhich enables simultaneous sampling and training. Code is available at\nhttps://github.com/majhas/self-refining-dft.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u51cf\u5c11\u4e86\u5bf9\u5927\u578b\u9884\u6536\u96c6\u6570\u636e\u96c6\u7684\u4f9d\u8d56\uff0c\u901a\u8fc7\u540c\u65f6\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9884\u6d4bDFT\u8f93\u51fa\u5e76\u91c7\u6837\u5206\u5b50\u6784\u8c61\u3002", "motivation": "DFT\u8ba1\u7b97\u5728\u5206\u5b50\u7cfb\u7edf\u89c4\u6a21\u548c\u80fd\u91cf\u8bc4\u4f30\u6210\u672c\u4e0a\u5b58\u5728\u9650\u5236\uff0c\u5c24\u5176\u662f\u5728\u6a21\u62df\u5206\u5b50\u52a8\u529b\u5b66\u65f6\u3002\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u9700\u4f9d\u8d56\u5927\u578b\u6570\u636e\u96c6\uff0c\u672c\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u4f9d\u8d56\u95ee\u9898\u3002", "method": "\u91c7\u7528\u81ea\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u6700\u5c0f\u5316KL\u6563\u5ea6\u7684\u53d8\u5206\u4e0a\u754c\uff0c\u540c\u65f6\u8bad\u7ec3\u6a21\u578b\u9884\u6d4bDFT\u8f93\u51fa\u5e76\u91c7\u6837\u5206\u5b50\u6784\u8c61\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u6570\u636e\u4f9d\u8d56\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u5f00\u6e90\u4e86\u5f02\u6b65\u8bad\u7ec3\u7684\u4f18\u5316\u5b9e\u73b0\u3002", "conclusion": "\u81ea\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u51cf\u5c11\u4e86DFT\u8ba1\u7b97\u5bf9\u5927\u578b\u6570\u636e\u96c6\u7684\u4f9d\u8d56\uff0c\u4e3a\u5206\u5b50\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002", "keywords": "DFT, \u6df1\u5ea6\u5b66\u4e60, \u81ea\u4f18\u5316\u8bad\u7ec3, \u5206\u5b50\u6784\u8c61, KL\u6563\u5ea6"}}
{"id": "2506.01252", "pdf": "https://arxiv.org/pdf/2506.01252", "abs": "https://arxiv.org/abs/2506.01252", "authors": ["Shufeng Kong", "Xingru Yang", "Yuanyuan Wei", "Zijie Wang", "Hao Tang", "Jiuqi Qin", "Shuting Lan", "Yingheng Wang", "Junwen Bai", "Zhuangbin Chen", "Zibin Zheng", "Caihua Liu", "Hao Liang"], "title": "MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Traditional Chinese Medicine (TCM) is a holistic medical system with\nmillennia of accumulated clinical experience, playing a vital role in global\nhealthcare-particularly across East Asia. However, the implicit reasoning,\ndiverse textual forms, and lack of standardization in TCM pose major challenges\nfor computational modeling and evaluation. Large Language Models (LLMs) have\ndemonstrated remarkable potential in processing natural language across diverse\ndomains, including general medicine. Yet, their systematic evaluation in the\nTCM domain remains underdeveloped. Existing benchmarks either focus narrowly on\nfactual question answering or lack domain-specific tasks and clinical realism.\nTo fill this gap, we introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs\non TCM Knowledge, Reasoning, and Safety. Developed in collaboration with\ncertified TCM experts, MTCMB comprises 12 sub-datasets spanning five major\ncategories: knowledge QA, language understanding, diagnostic reasoning,\nprescription generation, and safety evaluation. The benchmark integrates\nreal-world case records, national licensing exams, and classical texts,\nproviding an authentic and comprehensive testbed for TCM-capable models.\nPreliminary results indicate that current LLMs perform well on foundational\nknowledge but fall short in clinical reasoning, prescription planning, and\nsafety compliance. These findings highlight the urgent need for domain-aligned\nbenchmarks like MTCMB to guide the development of more competent and\ntrustworthy medical AI systems. All datasets, code, and evaluation tools are\npublicly available at: https://github.com/Wayyuanyuan/MTCMB.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MTCMB\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u533b\u77e5\u8bc6\u3001\u63a8\u7406\u548c\u5b89\u5168\u6027\u65b9\u9762\u7684\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u4e2d\u533b\u7f3a\u4e4f\u8ba1\u7b97\u5efa\u6a21\u548c\u8bc4\u4f30\u7684\u6807\u51c6\uff0c\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u5728\u4e2d\u533b\u9886\u57df\u7f3a\u4e4f\u7cfb\u7edf\u6027\u548c\u4e34\u5e8a\u771f\u5b9e\u6027\u3002", "method": "\u5f00\u53d1\u4e86MTCMB\uff0c\u5305\u542b12\u4e2a\u5b50\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u77e5\u8bc6\u95ee\u7b54\u3001\u8bed\u8a00\u7406\u89e3\u3001\u8bca\u65ad\u63a8\u7406\u3001\u5904\u65b9\u751f\u6210\u548c\u5b89\u5168\u6027\u8bc4\u4f305\u5927\u7c7b\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709\u6a21\u578b\u5728\u57fa\u7840\u77e5\u8bc6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4e34\u5e8a\u63a8\u7406\u3001\u5904\u65b9\u8ba1\u5212\u548c\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "conclusion": "\u9700\u8981\u7c7b\u4f3cMTCMB\u7684\u9886\u57df\u5bf9\u9f50\u57fa\u51c6\uff0c\u4ee5\u6307\u5bfc\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u4e2d\u533bAI\u7cfb\u7edf\u3002", "keywords": "\u4e2d\u533b,\u5927\u8bed\u8a00\u6a21\u578b,\u8bc4\u4f30\u57fa\u51c6,\u4e34\u5e8a\u63a8\u7406,\u5b89\u5168\u6027"}}
{"id": "2506.00241", "pdf": "https://arxiv.org/pdf/2506.00241", "abs": "https://arxiv.org/abs/2506.00241", "authors": ["Menglin Zhao", "Zhuorui Yong", "Ruijia Guan", "Kai-Wei Chang", "Adrian Haimovich", "Kei Ouchi", "Timothy Bickmore", "Bingsheng Yao", "Dakuo Wang", "Smit Desai"], "title": "Designing AI Tools for Clinical Care Teams to Support Serious Illness Conversations with Older Adults in the Emergency Department", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Serious illness conversations (SICs), discussions between clinical care teams\nand patients with serious, life-limiting illnesses about their values, goals,\nand care preferences, are critical for patient-centered care. Without these\nconversations, patients often receive aggressive interventions that may not\nalign with their goals. Clinical care teams face significant barriers when\nconducting serious illness conversations with older adult patients in Emergency\nDepartment (ED) settings, where most older adult patients lack documented\ntreatment goals. To understand current practices and identify AI support\nopportunities, we conducted interviews with two domain experts and nine ED\nclinical care team members. Through thematic analysis, we characterized a\nfour-phase serious illness conversation workflow (identification, preparation,\nconduction, documentation) and identified key needs and challenges at each\nstage. Clinical care teams struggle with fragmented EHR data access, time\nconstraints, emotional preparation demands, and documentation burdens. While\nparticipants expressed interest in AI tools for information synthesis,\nconversational support, and automated documentation, they emphasized preserving\nhuman connection and clinical autonomy. We present design guidelines for AI\ntools supporting SIC workflows that fit within existing clinical practices.\nThis work contributes empirical understanding of ED-based serious illness\nconversations and provides design considerations for AI in high-stakes clinical\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6025\u8bca\u79d1\uff08ED\uff09\u73af\u5883\u4e2d\u4e0e\u8001\u5e74\u60a3\u8005\u8fdb\u884c\u4e25\u91cd\u75be\u75c5\u5bf9\u8bdd\uff08SICs\uff09\u7684\u73b0\u72b6\uff0c\u5e76\u63d0\u51faAI\u5de5\u5177\u7684\u8bbe\u8ba1\u6307\u5357\u4ee5\u652f\u6301\u8fd9\u4e00\u6d41\u7a0b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3\u6025\u8bca\u79d1\u4e2d\u4e25\u91cd\u75be\u75c5\u5bf9\u8bdd\u7684\u5b9e\u8df5\uff0c\u5e76\u63a2\u7d22AI\u5982\u4f55\u5e2e\u52a9\u514b\u670d\u5f53\u524d\u9762\u4e34\u7684\u6311\u6218\uff0c\u5982\u6570\u636e\u788e\u7247\u5316\u3001\u65f6\u95f4\u9650\u5236\u548c\u60c5\u611f\u8d1f\u62c5\u7b49\u3002", "method": "\u901a\u8fc7\u8bbf\u8c08\u4e24\u4f4d\u9886\u57df\u4e13\u5bb6\u548c\u4e5d\u540d\u6025\u8bca\u79d1\u4e34\u5e8a\u56e2\u961f\u6210\u5458\uff0c\u91c7\u7528\u4e3b\u9898\u5206\u6790\u6cd5\u603b\u7ed3\u51fa\u4e25\u91cd\u75be\u75c5\u5bf9\u8bdd\u7684\u56db\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\uff08\u8bc6\u522b\u3001\u51c6\u5907\u3001\u8fdb\u884c\u3001\u8bb0\u5f55\uff09\uff0c\u5e76\u786e\u5b9a\u5404\u9636\u6bb5\u7684\u9700\u6c42\u548c\u6311\u6218\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u4e34\u5e8a\u56e2\u961f\u5728\u4e25\u91cd\u75be\u75c5\u5bf9\u8bdd\u4e2d\u9762\u4e34\u7684\u591a\u79cd\u969c\u788d\uff0c\u5305\u62ec\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u788e\u7247\u5316\u3001\u65f6\u95f4\u538b\u529b\u548c\u6587\u6863\u8d1f\u62c5\u3002\u53c2\u4e0e\u8005\u5bf9AI\u5de5\u5177\u5728\u4fe1\u606f\u5408\u6210\u3001\u5bf9\u8bdd\u652f\u6301\u548c\u81ea\u52a8\u5316\u6587\u6863\u5904\u7406\u65b9\u9762\u7684\u6f5c\u529b\u8868\u793a\u5174\u8da3\uff0c\u4f46\u5f3a\u8c03\u9700\u4fdd\u7559\u4eba\u9645\u8054\u7cfb\u548c\u4e34\u5e8a\u81ea\u4e3b\u6743\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u652f\u6301\u4e25\u91cd\u75be\u75c5\u5bf9\u8bdd\u5de5\u4f5c\u6d41\u7a0b\u7684AI\u5de5\u5177\u8bbe\u8ba1\u6307\u5357\uff0c\u5f3a\u8c03\u9700\u9002\u5e94\u73b0\u6709\u4e34\u5e8a\u5b9e\u8df5\u5e76\u5c0a\u91cd\u533b\u60a3\u5173\u7cfb\u3002", "keywords": "\u4e25\u91cd\u75be\u75c5\u5bf9\u8bdd\uff0c\u6025\u8bca\u79d1\uff0c\u8001\u5e74\u60a3\u8005\uff0c\u4eba\u5de5\u667a\u80fd\uff0c\u4e34\u5e8a\u5b9e\u8df5"}}
{"id": "2506.01230", "pdf": "https://arxiv.org/pdf/2506.01230", "abs": "https://arxiv.org/abs/2506.01230", "authors": ["Jiongli Zhu", "Geyang Xu", "Felipe Lorenzi", "Boris Glavic", "Babak Salimi"], "title": "Stress-Testing ML Pipelines with Adversarial Data Corruption", "categories": ["cs.LG"], "comment": null, "summary": "Structured data-quality issues, such as missing values correlated with\ndemographics, culturally biased labels, or systemic selection biases, routinely\ndegrade the reliability of machine-learning pipelines. Regulators now\nincreasingly demand evidence that high-stakes systems can withstand these\nrealistic, interdependent errors, yet current robustness evaluations typically\nuse random or overly simplistic corruptions, leaving worst-case scenarios\nunexplored. We introduce SAVAGE, a causally inspired framework that (i)\nformally models realistic data-quality issues through dependency graphs and\nflexible corruption templates, and (ii) systematically discovers corruption\npatterns that maximally degrade a target performance metric. SAVAGE employs a\nbi-level optimization approach to efficiently identify vulnerable data\nsubpopulations and fine-tune corruption severity, treating the full ML\npipeline, including preprocessing and potentially non-differentiable models, as\na black box. Extensive experiments across multiple datasets and ML tasks (data\ncleaning, fairness-aware learning, uncertainty quantification) demonstrate that\neven a small fraction (around 5 %) of structured corruptions identified by\nSAVAGE severely impacts model performance, far exceeding random or manually\ncrafted errors, and invalidating core assumptions of existing techniques. Thus,\nSAVAGE provides a practical tool for rigorous pipeline stress-testing, a\nbenchmark for evaluating robustness methods, and actionable guidance for\ndesigning more resilient data workflows.", "AI": {"tldr": "SAVAGE\u662f\u4e00\u4e2a\u56e0\u679c\u542f\u53d1\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u548c\u53d1\u73b0\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u4ee5\u7cfb\u7edf\u6027\u5730\u6d4b\u8bd5\u673a\u5668\u5b66\u4e60\u7ba1\u9053\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff08\u5982\u4e0e\u4eba\u53e3\u7edf\u8ba1\u76f8\u5173\u7684\u7f3a\u5931\u503c\u6216\u7cfb\u7edf\u9009\u62e9\u504f\u5dee\uff09\u4f1a\u964d\u4f4e\u673a\u5668\u5b66\u4e60\u7684\u53ef\u9760\u6027\uff0c\u800c\u73b0\u6709\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u65b9\u6cd5\u8fc7\u4e8e\u7b80\u5316\uff0c\u65e0\u6cd5\u8986\u76d6\u6700\u574f\u60c5\u51b5\u3002", "method": "SAVAGE\u901a\u8fc7\u4f9d\u8d56\u56fe\u548c\u7075\u6d3b\u7684\u6570\u636e\u7834\u574f\u6a21\u677f\u5efa\u6a21\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\u9ad8\u6548\u8bc6\u522b\u8106\u5f31\u6570\u636e\u5b50\u7fa4\u4f53\u548c\u7834\u574f\u4e25\u91cd\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSAVAGE\u53d1\u73b0\u7684\u5c11\u91cf\u7ed3\u6784\u5316\u7834\u574f\uff08\u7ea65%\uff09\u80fd\u663e\u8457\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u8fdc\u8d85\u968f\u673a\u6216\u4eba\u5de5\u8bbe\u8ba1\u7684\u9519\u8bef\u3002", "conclusion": "SAVAGE\u4e3a\u7ba1\u9053\u538b\u529b\u6d4b\u8bd5\u3001\u9c81\u68d2\u6027\u65b9\u6cd5\u8bc4\u4f30\u548c\u8bbe\u8ba1\u66f4\u5065\u58ee\u7684\u6570\u636e\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "keywords": "\u6570\u636e\u8d28\u91cf,\u9c81\u68d2\u6027\u8bc4\u4f30,\u673a\u5668\u7ba1\u9053,\u56e0\u679c\u5efa\u6a21,\u53cc\u5c42\u4f18\u5316"}}
{"id": "2506.01253", "pdf": "https://arxiv.org/pdf/2506.01253", "abs": "https://arxiv.org/abs/2506.01253", "authors": ["Sai Vallurupalli", "Francis Ferraro"], "title": "CoRE: Condition-based Reasoning for Identifying Outcome Variance in Complex Events", "categories": ["cs.CL"], "comment": "Accepted to Findings of the Association for Computational Linguistics\n  2025", "summary": "Knowing which latent conditions lead to a particular outcome is useful for\ncritically examining claims made about complex event outcomes. Identifying\nimplied conditions and examining their influence on an outcome is challenging.\nWe handle this by combining and augmenting annotations from two existing\ndatasets consisting of goals and states, and explore the influence of\nconditions through our research questions and Condition-based Reasoning tasks.\nWe examine open and closed LLMs of varying sizes and intent-alignment on our\nreasoning tasks and find that conditions are useful when not all context is\navailable. Models differ widely in their ability to generate and identify\noutcome-variant conditions which affects their performance on outcome\nvalidation when conditions are used to replace missing context. Larger models\nlike GPT-4o, are more cautious in such less constrained situations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u9690\u542b\u6761\u4ef6\u5bf9\u4e8b\u4ef6\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u7ed3\u5408\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u7684\u76ee\u6807\u548c\u72b6\u6001\u6807\u6ce8\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u6761\u4ef6\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e0d\u540c\u89c4\u6a21\u7684LLM\u6a21\u578b\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6761\u4ef6\u5728\u4e0a\u4e0b\u6587\u7f3a\u5931\u65f6\u975e\u5e38\u6709\u7528\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u751f\u6210\u548c\u8bc6\u522b\u7ed3\u679c\u53d8\u4f53\u6761\u4ef6\u7684\u80fd\u529b\u4e0a\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u7814\u7a76\u9690\u542b\u6761\u4ef6\u5982\u4f55\u5f71\u54cd\u590d\u6742\u4e8b\u4ef6\u7ed3\u679c\uff0c\u4ee5\u66f4\u4e25\u683c\u5730\u9a8c\u8bc1\u76f8\u5173\u4e3b\u5f20\u3002", "method": "\u7ed3\u5408\u5e76\u6269\u5c55\u4e86\u4e24\u4e2a\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u7684\u6807\u6ce8\uff08\u76ee\u6807\u548c\u72b6\u6001\uff09\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u6761\u4ef6\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u6d4b\u8bd5\u4e0d\u540c\u89c4\u6a21\u548c\u610f\u56fe\u5bf9\u9f50\u7684LLM\u6a21\u578b\u3002", "result": "\u6761\u4ef6\u5728\u4e0a\u4e0b\u6587\u7f3a\u5931\u65f6\u6709\u6548\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u751f\u6210\u548c\u8bc6\u522b\u7ed3\u679c\u53d8\u4f53\u6761\u4ef6\u7684\u80fd\u529b\u4e0a\u5dee\u5f02\u663e\u8457\uff0cGPT-4\u7b49\u5927\u6a21\u578b\u5728\u4f4e\u7ea6\u675f\u60c5\u51b5\u4e0b\u66f4\u8c28\u614e\u3002", "conclusion": "\u6761\u4ef6\u5bf9\u7ed3\u679c\u9a8c\u8bc1\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u6a21\u578b\u89c4\u6a21\u548c\u610f\u56fe\u5bf9\u9f50\u662f\u5173\u952e\u56e0\u7d20\u3002", "keywords": "\u9690\u542b\u6761\u4ef6\uff0c\u590d\u6742\u4e8b\u4ef6\u7ed3\u679c\uff0cLLM\u6a21\u578b\uff0c\u6761\u4ef6\u63a8\u7406\uff0c\u7ed3\u679c\u9a8c\u8bc1"}}
{"id": "2506.01231", "pdf": "https://arxiv.org/pdf/2506.01231", "abs": "https://arxiv.org/abs/2506.01231", "authors": ["Wenhao Song", "Xuan Wu", "Bo Yang", "You Zhou", "Yubin Xiao", "Yanchun Liang", "Hongwei Ge", "Heow Pueh Lee", "Chunguo Wu"], "title": "Towards Efficient Few-shot Graph Neural Architecture Search via Partitioning Gradient Contribution", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by SIGKDD 2025", "summary": "To address the weight coupling problem, certain studies introduced few-shot\nNeural Architecture Search (NAS) methods, which partition the supernet into\nmultiple sub-supernets. However, these methods often suffer from computational\ninefficiency and tend to provide suboptimal partitioning schemes. To address\nthis problem more effectively, we analyze the weight coupling problem from a\nnovel perspective, which primarily stems from distinct modules in succeeding\nlayers imposing conflicting gradient directions on the preceding layer modules.\nBased on this perspective, we propose the Gradient Contribution (GC) method\nthat efficiently computes the cosine similarity of gradient directions among\nmodules by decomposing the Vector-Jacobian Product during supernet\nbackpropagation. Subsequently, the modules with conflicting gradient directions\nare allocated to distinct sub-supernets while similar ones are grouped\ntogether. To assess the advantages of GC and address the limitations of\nexisting Graph Neural Architecture Search methods, which are limited to\nsearching a single type of Graph Neural Networks (Message Passing Neural\nNetworks (MPNNs) or Graph Transformers (GTs)), we propose the Unified Graph\nNeural Architecture Search (UGAS) framework, which explores optimal\ncombinations of MPNNs and GTs. The experimental results demonstrate that GC\nachieves state-of-the-art (SOTA) performance in supernet partitioning quality\nand time efficiency. In addition, the architectures searched by UGAS+GC\noutperform both the manually designed GNNs and those obtained by existing NAS\nmethods. Finally, ablation studies further demonstrate the effectiveness of all\nproposed methods.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u6743\u91cd\u8026\u5408\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u5c11\u6837\u672c\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u65b9\u6cd5\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u548c\u5206\u533a\u65b9\u6848\u6b21\u4f18\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5206\u6790\u68af\u5ea6\u65b9\u5411\u51b2\u7a81\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u8ba1\u7b97\u68af\u5ea6\u8d21\u732e\uff08GC\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u7edf\u4e00\u7684\u56fe\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08UGAS\uff09\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672cNAS\u65b9\u6cd5\u5728\u89e3\u51b3\u6743\u91cd\u8026\u5408\u95ee\u9898\u65f6\u6548\u7387\u4f4e\u4e0b\u4e14\u5206\u533a\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u4ece\u68af\u5ea6\u65b9\u5411\u51b2\u7a81\u7684\u89d2\u5ea6\u63a2\u7d22\u66f4\u4f18\u65b9\u6848\u3002", "method": "\u63d0\u51faGC\u65b9\u6cd5\u8ba1\u7b97\u6a21\u5757\u95f4\u68af\u5ea6\u65b9\u5411\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u5e76\u901a\u8fc7\u5206\u89e3\u5411\u91cf-\u96c5\u53ef\u6bd4\u79ef\u5b9e\u73b0\u9ad8\u6548\u5206\u533a\uff1b\u540c\u65f6\u63d0\u51faUGAS\u6846\u67b6\uff0c\u7ed3\u5408MPNNs\u4e0eGTs\u8fdb\u884c\u641c\u7d22\u3002", "result": "GC\u5728\u8d85\u7f51\u5206\u533a\u8d28\u91cf\u548c\u65f6\u95f4\u6548\u7387\u4e0a\u8fbe\u5230SOTA\uff1bUGAS+GC\u641c\u7d22\u7684\u67b6\u6784\u4f18\u4e8e\u624b\u52a8\u8bbe\u8ba1\u7684GNN\u548c\u5176\u4ed6NAS\u65b9\u6cd5\u3002", "conclusion": "GC\u548cUGAS\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "keywords": "\u6743\u91cd\u8026\u5408,\u795e\u7ecf\u67b6\u6784\u641c\u7d22,\u68af\u5ea6\u8d21\u732e,\u56fe\u795e\u7ecf\u7f51\u7edc,\u7edf\u4e00\u6846\u67b6"}}
{"id": "2506.01254", "pdf": "https://arxiv.org/pdf/2506.01254", "abs": "https://arxiv.org/abs/2506.01254", "authors": ["Yimin Du"], "title": "Memory-Efficient FastText: A Comprehensive Approach Using Double-Array Trie Structures and Mark-Compact Memory Management", "categories": ["cs.CL"], "comment": "10 pages", "summary": "FastText has established itself as a fundamental algorithm for learning word\nrepresentations, demonstrating exceptional capability in handling\nout-of-vocabulary words through character-level n-gram embeddings. However, its\nhash-based bucketing mechanism introduces critical limitations for large-scale\nindustrial deployment: hash collisions cause semantic drift, and memory\nrequirements become prohibitively expensive when dealing with real-world\nvocabularies containing millions of terms. This paper presents a comprehensive\nmemory optimization framework that fundamentally reimagines FastText's memory\nmanagement through the integration of double-array trie (DA-trie) structures\nand mark-compact garbage collection principles. Our approach leverages the\nlinguistic insight that n-grams sharing common prefixes or suffixes exhibit\nhighly correlated embeddings due to co-occurrence patterns in natural language.\nBy systematically identifying and merging semantically similar embeddings based\non structural relationships, we achieve compression ratios of 4:1 to 10:1 while\nmaintaining near-perfect embedding quality. The algorithm consists of four\nsophisticated phases: prefix trie construction with embedding mapping,\nprefix-based similarity compression, suffix-based similarity compression, and\nmark-compact memory reorganization. Comprehensive experiments on a 30-million\nChinese vocabulary dataset demonstrate memory reduction from over 100GB to\napproximately 30GB with negligible performance degradation. Our industrial\ndeployment results show significant cost reduction, faster loading times, and\nimproved model reliability through the elimination of hash collision artifacts.\nCode and experimental implementations are available at:\nhttps://github.com/initial-d/me_fasttext", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u6570\u7ec4Trie\u548c\u6807\u8bb0\u538b\u7f29\u5783\u573e\u56de\u6536\u7684FastText\u5185\u5b58\u4f18\u5316\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u5e76\u4fdd\u6301\u9ad8\u8d28\u91cf\u8bcd\u5d4c\u5165\u3002", "motivation": "\u89e3\u51b3FastText\u5728\u5de5\u4e1a\u89c4\u6a21\u90e8\u7f72\u4e2d\u56e0\u54c8\u5e0c\u51b2\u7a81\u5bfc\u81f4\u8bed\u4e49\u6f02\u79fb\u548c\u5185\u5b58\u9700\u6c42\u8fc7\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u6574\u5408\u53cc\u6570\u7ec4Trie\u7ed3\u6784\u548c\u6807\u8bb0\u538b\u7f29\u5783\u573e\u56de\u6536\u539f\u5219\uff0c\u901a\u8fc7\u524d\u7f00\u548c\u540e\u7f00\u76f8\u4f3c\u6027\u538b\u7f29\u8bcd\u5d4c\u5165\u3002", "result": "\u57283000\u4e07\u4e2d\u6587\u8bcd\u6c47\u6570\u636e\u4e0a\uff0c\u5185\u5b58\u4ece100GB\u964d\u81f330GB\uff0c\u6027\u80fd\u51e0\u4e4e\u65e0\u635f\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u964d\u4f4e\u6210\u672c\uff0c\u63d0\u5347\u52a0\u8f7d\u901f\u5ea6\u548c\u6a21\u578b\u53ef\u9760\u6027\u3002", "keywords": "FastText, \u5185\u5b58\u4f18\u5316, \u53cc\u6570\u7ec4Trie, \u6807\u8bb0\u538b\u7f29, \u8bcd\u5d4c\u5165"}}
{"id": "2506.00274", "pdf": "https://arxiv.org/pdf/2506.00274", "abs": "https://arxiv.org/abs/2506.00274", "authors": ["Jan-Niclas Hilgert", "Carlo Jakobs", "Michael K\u00fclper", "Martin Lambertz", "Axel Mahr", "Elmar Padilla"], "title": "Chances and Challenges of the Model Context Protocol in Digital Forensics and Incident Response", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models hold considerable promise for supporting forensic\ninvestigations, but their widespread adoption is hindered by a lack of\ntransparency, explainability, and reproducibility. This paper explores how the\nemerging Model Context Protocol can address these challenges and support the\nmeaningful use of LLMs in digital forensics. Through a theoretical analysis, we\nexamine how MCP can be integrated across various forensic scenarios - ranging\nfrom artifact analysis to the generation of interpretable reports. We also\noutline both technical and conceptual considerations for deploying an MCP\nserver in forensic environments. Our analysis reveals a wide range of use cases\nin which MCP not only strengthens existing forensic workflows but also\nfacilitates the application of LLMs to areas of forensics where their use was\npreviously limited. Furthermore, we introduce the concept of the inference\nconstraint level - a way of characterizing how specific MCP design choices can\ndeliberately constrain model behavior, thereby enhancing both auditability and\ntraceability. Our insights demonstrate that MCP has significant potential as a\nfoundational component for developing LLM-assisted forensic workflows that are\nnot only more transparent, reproducible, and legally defensible, but also\nrepresent a step toward increased automation in digital forensic analysis.\nHowever, we also highlight potential challenges that the adoption of MCP may\npose for digital forensics in the future.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u5982\u4f55\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6570\u5b57\u53d6\u8bc1\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u5176\u5728\u591a\u79cd\u53d6\u8bc1\u573a\u666f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b57\u53d6\u8bc1\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u7684\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u589e\u5f3a\u5176\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u63a2\u8ba8MCP\u5982\u4f55\u6574\u5408\u5230\u4e0d\u540c\u53d6\u8bc1\u573a\u666f\u4e2d\uff0c\u5e76\u5206\u6790\u5176\u6280\u672f\u5b9e\u73b0\u548c\u6982\u5ff5\u8bbe\u8ba1\u3002", "result": "MCP\u4e0d\u4ec5\u80fd\u5f3a\u5316\u73b0\u6709\u53d6\u8bc1\u6d41\u7a0b\uff0c\u8fd8\u80fd\u6269\u5c55LLM\u7684\u5e94\u7528\u8303\u56f4\uff0c\u540c\u65f6\u901a\u8fc7\u63a8\u7406\u7ea6\u675f\u7ea7\u522b\u63d0\u5347\u53ef\u5ba1\u8ba1\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u3002", "conclusion": "MCP\u4f5c\u4e3aLLM\u8f85\u52a9\u53d6\u8bc1\u7684\u57fa\u7840\u7ec4\u4ef6\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4e5f\u63d0\u51fa\u4e86\u672a\u6765\u53ef\u80fd\u9762\u4e34\u7684\u6311\u6218\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u6570\u5b57\u53d6\u8bc1, \u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae, \u900f\u660e\u5ea6, \u53ef\u89e3\u91ca\u6027"}}
{"id": "2506.01250", "pdf": "https://arxiv.org/pdf/2506.01250", "abs": "https://arxiv.org/abs/2506.01250", "authors": ["Youngmin Oh", "Jinje Park", "Taejin Paik", "Jaemin Park"], "title": "Neural Variance-aware Dueling Bandits with Deep Representation and Shallow Exploration", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "In this paper, we address the contextual dueling bandit problem by proposing\nvariance-aware algorithms that leverage neural networks to approximate\nnonlinear utility functions. Our approach employs a \\textit{variance-aware\nexploration strategy}, which adaptively accounts for uncertainty in pairwise\ncomparisons while relying only on the gradients with respect to the learnable\nparameters of the last layer. This design effectively balances the\nexploration--exploitation tradeoff under both the Upper Confidence Bound (UCB)\nand Thompson Sampling (TS) frameworks. As a result, under standard assumptions,\nwe establish theoretical guarantees showing that our algorithms achieve\nsublinear cumulative average regret of order $\\bigol\\lt(d \\sqrt{\\sum_{t=1}^T\n\\sigma_t^2} + \\sqrt{dT}\\rt),$ for sufficiently wide neural networks, where $ d\n$ is the contextual dimension, $ \\sigma_t^2 $ the variance of comparisons at\nround $ t $, and $ T $ the total number of rounds. We also empirically validate\nthat our approach offers reasonable computational efficiency and achieves\nsublinear regret on both synthetic tasks with nonlinear utilities and\nreal-world tasks, outperforming existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\u89e3\u51b3\u4e0a\u4e0b\u6587\u5bf9\u51b3bandit\u95ee\u9898\uff0c\u91c7\u7528\u65b9\u5dee\u611f\u77e5\u63a2\u7d22\u7b56\u7565\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u4e0a\u4e0b\u6587\u5bf9\u51b3bandit\u95ee\u9898\u4e2d\u975e\u7ebf\u6027\u6548\u7528\u51fd\u6570\u7684\u9ad8\u6548\u5b66\u4e60\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u975e\u7ebf\u6027\u6548\u7528\u51fd\u6570\uff0c\u7ed3\u5408\u65b9\u5dee\u611f\u77e5\u63a2\u7d22\u7b56\u7565\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u7406\u8bba\u4fdd\u8bc1\u4e86\u6b21\u7ebf\u6027\u7d2f\u79ef\u9057\u61be\uff1b\u5b9e\u9a8c\u8868\u660e\u5728\u5408\u6210\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u6709\u6548\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "keywords": "contextual dueling bandit, neural networks, variance-aware exploration"}}
{"id": "2506.01257", "pdf": "https://arxiv.org/pdf/2506.01257", "abs": "https://arxiv.org/abs/2506.01257", "authors": ["Jiancheng Ye", "Sophie Bronstein", "Jiarui Hai", "Malak Abu Hashish"], "title": "DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "DeepSeek-R1 is a cutting-edge open-source large language model (LLM)\ndeveloped by DeepSeek, showcasing advanced reasoning capabilities through a\nhybrid architecture that integrates mixture of experts (MoE), chain of thought\n(CoT) reasoning, and reinforcement learning. Released under the permissive MIT\nlicense, DeepSeek-R1 offers a transparent and cost-effective alternative to\nproprietary models like GPT-4o and Claude-3 Opus; it excels in structured\nproblem-solving domains such as mathematics, healthcare diagnostics, code\ngeneration, and pharmaceutical research. The model demonstrates competitive\nperformance on benchmarks like the United States Medical Licensing Examination\n(USMLE) and American Invitational Mathematics Examination (AIME), with strong\nresults in pediatric and ophthalmologic clinical decision support tasks. Its\narchitecture enables efficient inference while preserving reasoning depth,\nmaking it suitable for deployment in resource-constrained settings. However,\nDeepSeek-R1 also exhibits increased vulnerability to bias, misinformation,\nadversarial manipulation, and safety failures - especially in multilingual and\nethically sensitive contexts. This survey highlights the model's strengths,\nincluding interpretability, scalability, and adaptability, alongside its\nlimitations in general language fluency and safety alignment. Future research\npriorities include improving bias mitigation, natural language comprehension,\ndomain-specific validation, and regulatory compliance. Overall, DeepSeek-R1\nrepresents a major advance in open, scalable AI, underscoring the need for\ncollaborative governance to ensure responsible and equitable deployment.", "AI": {"tldr": "DeepSeek-R1\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408\u6df7\u5408\u4e13\u5bb6\u3001\u94fe\u5f0f\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u6570\u5b66\u3001\u533b\u7597\u8bca\u65ad\u7b49\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4e5f\u5b58\u5728\u504f\u89c1\u548c\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u63d0\u4f9b\u900f\u660e\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4ee5\u89e3\u51b3\u4e13\u6709\u6a21\u578b\u5982GPT-4o\u548cClaude-3 Opus\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\uff0c\u7ed3\u5408\u94fe\u5f0f\u63a8\u7406\uff08CoT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728USMLE\u548cAIME\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u591a\u8bed\u8a00\u548c\u4f26\u7406\u654f\u611f\u573a\u666f\u4e0b\u6613\u53d7\u504f\u89c1\u548c\u5b89\u5168\u95ee\u9898\u5f71\u54cd\u3002", "conclusion": "DeepSeek-R1\u662f\u5f00\u653e\u53ef\u6269\u5c55AI\u7684\u91cd\u5927\u8fdb\u5c55\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u6539\u8fdb\u504f\u89c1\u7f13\u89e3\u548c\u5b89\u5168\u6027\u3002", "keywords": "\u5f00\u6e90\u8bed\u8a00\u6a21\u578b,\u6df7\u5408\u4e13\u5bb6,\u94fe\u5f0f\u63a8\u7406,\u5f3a\u5316\u5b66\u4e60,\u504f\u89c1,\u5b89\u5168\u6027"}}
{"id": "2506.01260", "pdf": "https://arxiv.org/pdf/2506.01260", "abs": "https://arxiv.org/abs/2506.01260", "authors": ["Sameera Ramasinghe", "Thalaiyasingam Ajanthan", "Gil Avraham", "Yan Zuo", "Alexander Long"], "title": "Protocol Models: Scaling Decentralized Training with Communication-Efficient Model Parallelism", "categories": ["cs.LG"], "comment": null, "summary": "Scaling models has led to significant advancements in deep learning, but\ntraining these models in decentralized settings remains challenging due to\ncommunication bottlenecks. While existing compression techniques are effective\nin data-parallel, they do not extend to model parallelism. Unlike data-parallel\ntraining, where weight gradients are exchanged, model-parallel requires\ncompressing activations and activation gradients as they propagate through\nlayers, accumulating compression errors. We propose a novel compression\nalgorithm that compresses both forward and backward passes, enabling up to 99%\ncompression with no convergence degradation with negligible memory/compute\noverhead. By leveraging a recursive structure in transformer networks, we\npredefine a low-dimensional subspace to confine the activations and gradients,\nallowing full reconstruction in subsequent layers. Our method achieves up to\n100x improvement in communication efficiency and enables training\nbillion-parameter-scale models over low-end GPUs connected via consumer-grade\ninternet speeds as low as 80Mbps, matching the convergence of centralized\ndatacenter systems with 100Gbps connections with model parallel.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u538b\u7f29\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6a21\u578b\u5e76\u884c\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u6548\u7387\u95ee\u9898\uff0c\u652f\u6301\u6781\u4f4e\u5e26\u5bbd\u73af\u5883\u4e0b\u7684\u5341\u4ebf\u53c2\u6570\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u4e2d\uff0c\u6a21\u578b\u5e76\u884c\u8bad\u7ec3\u9762\u4e34\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u4f20\u7edf\u538b\u7f29\u6280\u672f\u65e0\u6cd5\u6709\u6548\u9002\u5e94\u3002", "method": "\u901a\u8fc7\u9884\u5b9a\u4e49\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u538b\u7f29\u6fc0\u6d3b\u548c\u68af\u5ea6\uff0c\u5e76\u5229\u7528Transformer\u9012\u5f52\u7ed3\u6784\u5b9e\u73b0\u5b8c\u5168\u91cd\u6784\u3002", "result": "\u7b97\u6cd5\u652f\u6301\u9ad8\u8fbe99%\u7684\u538b\u7f29\u7387\uff0c\u901a\u4fe1\u6548\u7387\u63d0\u5347100\u500d\uff0c\u53ef\u572880Mbps\u5e26\u5bbd\u4e0b\u8fbe\u5230\u4e0e\u6570\u636e\u4e2d\u5fc3100Gbps\u76f8\u5f53\u7684\u6536\u655b\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f4e\u5e26\u5bbd\u73af\u5883\u4e0b\u7684\u6a21\u578b\u5e76\u884c\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u6a21\u578b\u5e76\u884c\u3001\u901a\u4fe1\u6548\u7387\u3001\u538b\u7f29\u7b97\u6cd5\u3001Transformer\u3001\u5206\u5e03\u5f0f\u8bad\u7ec3"}}
{"id": "2506.01262", "pdf": "https://arxiv.org/pdf/2506.01262", "abs": "https://arxiv.org/abs/2506.01262", "authors": ["Jisoo Mok", "Ik-hwan Kim", "Sangkwon Park", "Sungroh Yoon"], "title": "Exploring the Potential of LLMs as Personalized Assistants: Dataset, Evaluation, and Analysis", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Personalized AI assistants, a hallmark of the human-like capabilities of\nLarge Language Models (LLMs), are a challenging application that intertwines\nmultiple problems in LLM research. Despite the growing interest in the\ndevelopment of personalized assistants, the lack of an open-source\nconversational dataset tailored for personalization remains a significant\nobstacle for researchers in the field. To address this research gap, we\nintroduce HiCUPID, a new benchmark to probe and unleash the potential of LLMs\nto deliver personalized responses. Alongside a conversational dataset, HiCUPID\nprovides a Llama-3.2-based automated evaluation model whose assessment closely\nmirrors human preferences. We release our dataset, evaluation model, and code\nat https://github.com/12kimih/HiCUPID.", "AI": {"tldr": "HiCUPID\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u6316\u6398LLMs\u63d0\u4f9b\u4e2a\u6027\u5316\u56de\u590d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u5bf9\u8bdd\u6570\u636e\u96c6\u548c\u81ea\u52a8\u8bc4\u4f30\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9\u4e2a\u6027\u5316AI\u52a9\u624b\u7814\u7a76\u5f00\u6e90\u7684\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51fa\u4e86HiCUPID\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u5bf9\u8bdd\u6570\u636e\u96c6\u548c\u57fa\u4e8eLlama-3.2\u7684\u81ea\u52a8\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6a21\u578b\u548c\u4ee3\u7801\uff0c\u8bc4\u4f30\u6a21\u578b\u7684\u8868\u73b0\u4e0e\u4eba\u7c7b\u504f\u597d\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "HiCUPID\u586b\u8865\u4e86\u7814\u7a76\u7a7a\u767d\uff0c\u4e3a\u4e2a\u6027\u5316AI\u52a9\u624b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "keywords": "\u4e2a\u6027\u5316AI\u52a9\u624b, LLMs, \u5bf9\u8bdd\u6570\u636e\u96c6, \u81ea\u52a8\u8bc4\u4f30\u6a21\u578b, HiCUPID"}}
{"id": "2506.00281", "pdf": "https://arxiv.org/pdf/2506.00281", "abs": "https://arxiv.org/abs/2506.00281", "authors": ["Chris M. Ward", "Josh Harguess"], "title": "Adversarial Threat Vectors and Risk Mitigation for Retrieval-Augmented Generation Systems", "categories": ["cs.CR", "cs.AI"], "comment": "SPIE DCS: Proceedings Volume Assurance and Security for AI-enabled\n  Systems 2025", "summary": "Retrieval-Augmented Generation (RAG) systems, which integrate Large Language\nModels (LLMs) with external knowledge sources, are vulnerable to a range of\nadversarial attack vectors. This paper examines the importance of RAG systems\nthrough recent industry adoption trends and identifies the prominent attack\nvectors for RAG: prompt injection, data poisoning, and adversarial query\nmanipulation. We analyze these threats under risk management lens, and propose\nrobust prioritized control list that includes risk-mitigating actions like\ninput validation, adversarial training, and real-time monitoring.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u9762\u4e34\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u63d0\u51fa\u4e86\u98ce\u9669\u7f13\u89e3\u63aa\u65bd\u3002", "motivation": "\u968f\u7740RAG\u7cfb\u7edf\u5728\u884c\u4e1a\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5b89\u5168\u6027\u95ee\u9898\u65e5\u76ca\u51f8\u663e\uff0c\u4e9f\u9700\u7814\u7a76\u5176\u653b\u51fb\u5411\u91cf\u5e76\u63d0\u51fa\u5e94\u5bf9\u7b56\u7565\u3002", "method": "\u8bc6\u522b\u4e86RAG\u7cfb\u7edf\u7684\u4e3b\u8981\u653b\u51fb\u5411\u91cf\uff08\u5982\u63d0\u793a\u6ce8\u5165\u3001\u6570\u636e\u6295\u6bd2\u548c\u5bf9\u6297\u6027\u67e5\u8be2\u64cd\u7eb5\uff09\uff0c\u5e76\u4ece\u98ce\u9669\u7ba1\u7406\u89d2\u5ea6\u5206\u6790\u4e86\u8fd9\u4e9b\u5a01\u80c1\uff0c\u63d0\u51fa\u4e86\u4f18\u5148\u7ea7\u63a7\u5236\u5217\u8868\u3002", "result": "\u63d0\u51fa\u7684\u98ce\u9669\u7f13\u89e3\u63aa\u65bd\u5305\u62ec\u8f93\u5165\u9a8c\u8bc1\u3001\u5bf9\u6297\u6027\u8bad\u7ec3\u548c\u5b9e\u65f6\u76d1\u63a7\u3002", "conclusion": "RAG\u7cfb\u7edf\u9762\u4e34\u591a\u79cd\u5b89\u5168\u5a01\u80c1\uff0c\u4f46\u901a\u8fc7\u4f18\u5148\u7ea7\u63a7\u5236\u5217\u8868\u548c\u5177\u4f53\u63aa\u65bd\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u98ce\u9669\u3002", "keywords": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210, \u5bf9\u6297\u6027\u653b\u51fb, \u98ce\u9669\u7ba1\u7406, \u5b89\u5168\u63aa\u65bd"}}
{"id": "2506.01261", "pdf": "https://arxiv.org/pdf/2506.01261", "abs": "https://arxiv.org/abs/2506.01261", "authors": ["Zhijie Xie", "Shenghui Song"], "title": "The Actor-Critic Update Order Matters for PPO in Federated Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "In the context of Federated Reinforcement Learning (FRL), applying Proximal\nPolicy Optimization (PPO) faces challenges related to the update order of its\nactor and critic due to the aggregation step occurring between successive\niterations. In particular, when local actors are updated based on local critic\nestimations, the algorithm becomes vulnerable to data heterogeneity. As a\nresult, the conventional update order in PPO (critic first, then actor) may\ncause heterogeneous gradient directions among clients, hindering convergence to\na globally optimal policy. To address this issue, we propose FedRAC, which\nreverses the update order (actor first, then critic) to eliminate the\ndivergence of critics from different clients. Theoretical analysis shows that\nthe convergence bound of FedRAC is immune to data heterogeneity under mild\nconditions, i.e., bounded level of heterogeneity and accurate policy\nevaluation. Empirical results indicate that the proposed algorithm obtains\nhigher cumulative rewards and converges more rapidly in five experiments,\nincluding three classical RL environments and a highly heterogeneous autonomous\ndriving scenario using the SUMO traffic simulator.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86FedRAC\u7b97\u6cd5\uff0c\u901a\u8fc7\u53cd\u8f6cPPO\u4e2d\u6f14\u5458\u548c\u8bc4\u8bba\u5bb6\u7684\u66f4\u65b0\u987a\u5e8f\uff0c\u89e3\u51b3\u4e86\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u4e2d\u56e0\u6570\u636e\u5f02\u8d28\u6027\u5bfc\u81f4\u7684\u6536\u655b\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0cPPO\u7b97\u6cd5\u7531\u4e8e\u805a\u5408\u6b65\u9aa4\u7684\u5b58\u5728\uff0c\u6f14\u5458\u548c\u8bc4\u8bba\u5bb6\u7684\u66f4\u65b0\u987a\u5e8f\u53ef\u80fd\u5bfc\u81f4\u68af\u5ea6\u65b9\u5411\u4e0d\u4e00\u81f4\uff0c\u963b\u788d\u5168\u5c40\u6700\u4f18\u7b56\u7565\u7684\u6536\u655b\u3002", "method": "\u63d0\u51faFedRAC\u7b97\u6cd5\uff0c\u8c03\u6574PPO\u7684\u66f4\u65b0\u987a\u5e8f\uff08\u5148\u66f4\u65b0\u6f14\u5458\uff0c\u518d\u66f4\u65b0\u8bc4\u8bba\u5bb6\uff09\uff0c\u4ee5\u51cf\u5c11\u5ba2\u6237\u95f4\u8bc4\u8bba\u5bb6\u7684\u5dee\u5f02\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660eFedRAC\u5728\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\u4e0a\u5177\u6709\u6297\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u5177\u6709\u66f4\u9ad8\u7684\u7d2f\u79ef\u5956\u52b1\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "FedRAC\u901a\u8fc7\u6539\u53d8\u66f4\u65b0\u987a\u5e8f\u6709\u6548\u89e3\u51b3\u4e86FRL\u4e2d\u7684\u6536\u655b\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5f02\u8d28\u6027\u6570\u636e\u573a\u666f\u3002", "keywords": "\u8054\u90a6\u5f3a\u5316\u5b66\u4e60, \u8fd1\u7aef\u7b56\u7565\u4f18\u5316, \u6570\u636e\u5f02\u8d28\u6027, \u66f4\u65b0\u987a\u5e8f, FedRAC"}}
{"id": "2506.01263", "pdf": "https://arxiv.org/pdf/2506.01263", "abs": "https://arxiv.org/abs/2506.01263", "authors": ["Yu Nakagome", "Michael Hentschel"], "title": "WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard CTC-based Keyword Spotting and Inter-layer Biasing", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Despite recent advances in end-to-end speech recognition methods, the output\ntends to be biased to the training data's vocabulary, resulting in inaccurate\nrecognition of proper nouns and other unknown terms. To address this issue, we\npropose a method to improve recognition accuracy of such rare words in\nCTC-based models without additional training or text-to-speech systems.\nSpecifically, keyword spotting is performed using acoustic features of\nintermediate layers during inference, and a bias is applied to the subsequent\nlayers of the acoustic model for detected keywords. For keyword detection, we\nadopt a wildcard CTC that is both fast and tolerant of ambiguous matches,\nallowing flexible handling of words that are difficult to match strictly. Since\nthis method does not require retraining of existing models, it can be easily\napplied to even large-scale models. In experiments on Japanese speech\nrecognition, the proposed method achieved a 29% improvement in the F1 score for\nunknown words.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6539\u8fdbCTC\u6a21\u578b\u4e2d\u7a00\u6709\u8bcd\u8bc6\u522b\u51c6\u786e\u7387\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216TTS\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u7aef\u5230\u7aef\u8bed\u97f3\u8bc6\u522b\u5bf9\u8bad\u7ec3\u6570\u636e\u8bcd\u6c47\u7684\u504f\u501a\u95ee\u9898\uff0c\u7279\u522b\u662f\u4e13\u6709\u540d\u8bcd\u548c\u672a\u77e5\u8bcd\u6c47\u7684\u8bc6\u522b\u3002", "method": "\u5229\u7528\u4e2d\u95f4\u5c42\u58f0\u5b66\u7279\u5f81\u8fdb\u884c\u5173\u952e\u8bcd\u68c0\u6d4b\uff0c\u5e76\u5bf9\u540e\u7eed\u58f0\u5b66\u6a21\u578b\u5c42\u65bd\u52a0\u504f\u7f6e\uff0c\u4f7f\u7528wildcard CTC\u5b9e\u73b0\u5feb\u901f\u4e14\u6a21\u7cca\u5339\u914d\u3002", "result": "\u5728\u65e5\u8bed\u8bed\u97f3\u8bc6\u522b\u5b9e\u9a8c\u4e2d\uff0c\u672a\u77e5\u8bcd\u7684F1\u5206\u6570\u63d0\u5347\u4e8629%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u7a00\u6709\u8bcd\u7684\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u4e14\u6613\u4e8e\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6a21\u578b\u3002", "keywords": "\u8bed\u97f3\u8bc6\u522b, CTC\u6a21\u578b, \u5173\u952e\u8bcd\u68c0\u6d4b, \u7a00\u6709\u8bcd\u8bc6\u522b"}}
{"id": "2506.01290", "pdf": "https://arxiv.org/pdf/2506.01290", "abs": "https://arxiv.org/abs/2506.01290", "authors": ["Shunyu Wu", "Dan Li", "Haozheng Ye", "Zhuomin Chen", "Jiahui Zhou", "Jian Lou", "Zibin Zheng", "See-Kiong Ng"], "title": "TSRating: Rating Quality of Diverse Time Series Data by Meta-learning from LLM Judgment", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "High-quality time series (TS) data are essential for ensuring TS model\nperformance, rendering research on rating TS data quality indispensable.\nExisting methods have shown promising rating accuracy within individual\ndomains, primarily by extending data quality rating techniques such as\ninfluence functions and Shapley values to account for temporal characteristics.\nHowever, they neglect the fact that real-world TS data can span vastly\ndifferent domains and exhibit distinct properties, hampering the accurate and\nefficient rating of diverse TS data. In this paper, we propose TSRating, a\nnovel and unified framework for rating the quality of time series data crawled\nfrom diverse domains. TSRating is built on the assumption that LLMs inherit\nample knowledge, acquired during their extensive pretraining, enabling them to\ncomprehend and discern quality differences in diverse TS data. We verify this\nassumption by devising a series of prompts to elicit quality comparisons from\nLLMs for pairs of TS samples. We then fit a dedicated rating model, termed\nTSRater, to convert the LLMs' judgments into efficient quality predictions via\nTSRater's inference on future TS samples. To ensure cross-domain adaptability,\nwe develop a meta-learning scheme to train TSRater on quality comparisons\ncollected from nine distinct domains. To improve training efficiency, we employ\nsignSGD for inner-loop updates, thus circumventing the demanding computation of\nhypergradients. Extensive experimental results on eleven benchmark datasets\nacross three time series tasks, each using both conventional TS models and TS\nfoundation models, demonstrate that TSRating outperforms baselines in terms of\nestimation accuracy, efficiency, and domain adaptability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86TSRating\u6846\u67b6\uff0c\u5229\u7528LLMs\u7684\u5e7f\u6cdb\u77e5\u8bc6\u8bc4\u4f30\u591a\u6837\u5316\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8d28\u91cf\uff0c\u901a\u8fc7meta-learning\u63d0\u5347\u8de8\u9886\u57df\u9002\u5e94\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u4e00\u9886\u57df\uff0c\u65e0\u6cd5\u51c6\u786e\u9ad8\u6548\u5730\u8bc4\u4f30\u591a\u6837\u5316\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8d28\u91cf\uff0c\u4e9f\u9700\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51faTSRating\u6846\u67b6\uff0c\u5229\u7528LLMs\u7684\u77e5\u8bc6\u751f\u6210\u8d28\u91cf\u6bd4\u8f83\uff0c\u901a\u8fc7meta-learning\u548csignSGD\u8bad\u7ec3TSRater\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u8de8\u9886\u57df\u9884\u6d4b\u3002", "result": "\u572811\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c3\u4e2a\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e2d\uff0cTSRating\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u9886\u57df\u9002\u5e94\u6027\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TSRating\u6846\u67b6\u901a\u8fc7\u7ed3\u5408LLMs\u548cmeta-learning\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6837\u5316\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u7684\u6311\u6218\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8d28\u91cf,LLMs,meta-learning,\u8de8\u9886\u57df\u9002\u5e94\u6027"}}
{"id": "2506.01265", "pdf": "https://arxiv.org/pdf/2506.01265", "abs": "https://arxiv.org/abs/2506.01265", "authors": ["Do Xuan Long", "Duong Ngoc Yen", "Do Xuan Trong", "Luu Anh Tuan", "Kenji Kawaguchi", "Shafiq Joty", "Min-Yen Kan", "Nancy F. Chen"], "title": "Beyond In-Context Learning: Aligning Long-form Generation of Large Language Models via Task-Inherent Attribute Guidelines", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "In-context learning (ICL) is an important yet not fully understood ability of\npre-trained large language models (LLMs). It can greatly enhance task\nperformance using a few examples, termed demonstrations, without fine-tuning.\nAlthough effective in question answering, ICL often underperforms in long-form\ngeneration tasks such as summarization. Under appropriately realistic\nassumptions, we empirically and theoretically show that ICL demonstrations\nalone are insufficient to teach LLMs the task language and format distributions\nfor generation. We argue for explicit exposure to the task distributions and\nhypothesize that defining them by prompting enhances model performance. To this\nend, we present LongGuide, which efficiently generates two parallel streams of\nguidelines capturing task language and format properties: (i) Metric Guidelines\n(MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output\nConstraint Guidelines (OCGs) that constrain generation at both token and\nsentence levels. LongGuide automatically selects the best combination of\nguidelines, improving both strong open- and closed-source LLMs by over 5% in\nboth zero- and few-shot settings. We show that LongGuide is generalizable,\nlearnable by weak models to enhance strong ones, and integrates synergistically\nwith automatic prompt optimizers.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u5728\u957f\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\uff0c\u63d0\u51faLongGuide\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4efb\u52a1\u8bed\u8a00\u548c\u683c\u5f0f\u7684\u6307\u5bfc\u51c6\u5219\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22ICL\u5728\u957f\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faLongGuide\u65b9\u6cd5\uff0c\u751f\u6210\u5e76\u884c\u6307\u5bfc\u51c6\u5219\uff08Metric Guidelines\u548cOutput Constraint Guidelines\uff09\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u9009\u62e9\u6700\u4f73\u7ec4\u5408\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660eLongGuide\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u5c06\u5f3a\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\u8d85\u8fc75%\u3002", "conclusion": "LongGuide\u5177\u6709\u901a\u7528\u6027\u3001\u53ef\u5b66\u4e60\u6027\uff0c\u5e76\u80fd\u4e0e\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u5668\u534f\u540c\u5de5\u4f5c\uff0c\u6709\u6548\u63d0\u5347ICL\u5728\u957f\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "keywords": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u957f\u6587\u672c\u751f\u6210\u3001\u6307\u5bfc\u51c6\u5219\u3001\u6a21\u578b\u4f18\u5316"}}
{"id": "2506.01302", "pdf": "https://arxiv.org/pdf/2506.01302", "abs": "https://arxiv.org/abs/2506.01302", "authors": ["Zhengyu Fang", "Xiaoge Zhang", "Anyin Zhao", "Xiao Li", "Huiyuan Chen", "Jing Li"], "title": "Recent Developments in GNNs for Drug Discovery", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "In this paper, we review recent developments and the role of Graph Neural\nNetworks (GNNs) in computational drug discovery, including molecule generation,\nmolecular property prediction, and drug-drug interaction prediction. By\nsummarizing the most recent developments in this area, we underscore the\ncapabilities of GNNs to comprehend intricate molecular patterns, while\nexploring both their current and prospective applications. We initiate our\ndiscussion by examining various molecular representations, followed by detailed\ndiscussions and categorization of existing GNN models based on their input\ntypes and downstream application tasks. We also collect a list of commonly used\nbenchmark datasets for a variety of applications. We conclude the paper with\nbrief discussions and summarize common trends in this important research area.", "AI": {"tldr": "\u56de\u987e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u8ba1\u7b97\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u5e94\u7528\uff0c\u5305\u62ec\u5206\u5b50\u751f\u6210\u3001\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u548c\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\uff0c\u603b\u7ed3GNN\u6a21\u578b\u7684\u5206\u7c7b\u548c\u5e38\u7528\u6570\u636e\u96c6\uff0c\u5e76\u63a2\u8ba8\u672a\u6765\u8d8b\u52bf\u3002", "motivation": "\u63a2\u7d22GNNs\u5728\u836f\u7269\u53d1\u73b0\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u4e86\u89e3\u5176\u5f53\u524d\u8fdb\u5c55\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "method": "\u603b\u7ed3GNN\u6a21\u578b\u7684\u5206\u7c7b\uff0c\u5206\u6790\u4e0d\u540c\u8f93\u5165\u7c7b\u578b\u548c\u4e0b\u6e38\u4efb\u52a1\uff0c\u5e76\u6574\u7406\u5e38\u7528\u6570\u636e\u96c6\u3002", "result": "GNNs\u5c55\u73b0\u4e86\u7406\u89e3\u548c\u5904\u7406\u590d\u6742\u5206\u5b50\u6a21\u5f0f\u7684\u80fd\u529b\uff0c\u4e3a\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002", "conclusion": "GNNs\u5728\u836f\u7269\u53d1\u73b0\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\uff0c\u672a\u6765\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002", "keywords": "\u56fe\u795e\u7ecf\u7f51\u7edc, \u836f\u7269\u53d1\u73b0, \u5206\u5b50\u751f\u6210, \u6027\u8d28\u9884\u6d4b, \u6570\u636e\u96c6"}}
{"id": "2506.01266", "pdf": "https://arxiv.org/pdf/2506.01266", "abs": "https://arxiv.org/abs/2506.01266", "authors": ["Yuanhe Tian", "Mingjie Deng", "Guoqing Jin", "Yan Song"], "title": "Detoxification of Large Language Models through Output-layer Fusion with a Calibration Model", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 1 figure", "summary": "Existing approaches for Large language model (LLM) detoxification generally\nrely on training on large-scale non-toxic or human-annotated preference data,\ndesigning prompts to instruct the LLM to generate safe content, or modifying\nthe model parameters to remove toxic information, which are computationally\nexpensive, lack robustness, and often compromise LLMs' fluency and contextual\nunderstanding. In this paper, we propose a simple yet effective approach for\nLLM detoxification, which leverages a compact, pre-trained calibration model\nthat guides the detoxification process of a target LLM via a lightweight\nintervention in its generation pipeline. By learning a detoxified embedding\nspace from non-toxic data, the calibration model effectively steers the LLM\naway from generating harmful content. This approach only requires a one-time\ntraining of the calibration model that is able to be seamlessly applied to\nmultiple LLMs without compromising fluency or contextual understanding.\nExperiment results on the benchmark dataset demonstrate that our approach\nreduces toxicity while maintaining reasonable content expression.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53bb\u6bd2\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u6821\u51c6\u6a21\u578b\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u8f7b\u91cf\u5e72\u9884\uff0c\u907f\u514d\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u3002", "motivation": "\u73b0\u6709LLM\u53bb\u6bd2\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u65e0\u6bd2\u6570\u636e\u6216\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u8fd8\u4f1a\u5f71\u54cd\u6d41\u7545\u6027\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u3002", "method": "\u5229\u7528\u7d27\u51d1\u7684\u9884\u8bad\u7ec3\u6821\u51c6\u6a21\u578b\uff0c\u901a\u8fc7\u8f7b\u91cf\u5e72\u9884\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5f15\u5bfc\u53bb\u6bd2\uff0c\u5b66\u4e60\u65e0\u6bd2\u5d4c\u5165\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u964d\u4f4e\u6bd2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5408\u7406\u7684\u5185\u5bb9\u8868\u8fbe\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ec5\u9700\u4e00\u6b21\u6027\u8bad\u7ec3\u6821\u51c6\u6a21\u578b\uff0c\u53ef\u65e0\u7f1d\u5e94\u7528\u4e8e\u591a\u4e2aLLM\uff0c\u4e14\u4e0d\u5f71\u54cd\u6d41\u7545\u6027\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u53bb\u6bd2\u3001\u6821\u51c6\u6a21\u578b\u3001\u8f7b\u91cf\u5e72\u9884\u3001\u65e0\u6bd2\u5d4c\u5165\u7a7a\u95f4"}}
{"id": "2506.01303", "pdf": "https://arxiv.org/pdf/2506.01303", "abs": "https://arxiv.org/abs/2506.01303", "authors": ["Chong Li", "Xiangyang Xue", "Jianfeng Feng", "Taiping Zeng"], "title": "Latent Structured Hopfield Network for Semantic Association and Retrieval", "categories": ["cs.LG", "q-bio.NC"], "comment": null, "summary": "Episodic memory enables humans to recall past experiences by associating\nsemantic elements such as objects, locations, and time into coherent event\nrepresentations. While large pretrained models have shown remarkable progress\nin modeling semantic memory, the mechanisms for forming associative structures\nthat support episodic memory remain underexplored. Inspired by hippocampal CA3\ndynamics and its role in associative memory, we propose the Latent Structured\nHopfield Network (LSHN), a biologically inspired framework that integrates\ncontinuous Hopfield attractor dynamics into an autoencoder architecture. LSHN\nmimics the cortical-hippocampal pathway: a semantic encoder extracts compact\nlatent representations, a latent Hopfield network performs associative\nrefinement through attractor convergence, and a decoder reconstructs perceptual\ninput. Unlike traditional Hopfield networks, our model is trained end-to-end\nwith gradient descent, achieving scalable and robust memory retrieval.\nExperiments on MNIST, CIFAR-10, and a simulated episodic memory task\ndemonstrate superior performance in recalling corrupted inputs under occlusion\nand noise, outperforming existing associative memory models. Our work provides\na computational perspective on how semantic elements can be dynamically bound\ninto episodic memory traces through biologically grounded attractor mechanisms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u7269\u542f\u53d1\u7684\u6f5c\u5728\u7ed3\u6784\u5316Hopfield\u7f51\u7edc\uff08LSHN\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u8fde\u7eedHopfield\u5438\u5f15\u5b50\u52a8\u529b\u5b66\u548c\u81ea\u7f16\u7801\u5668\u67b6\u6784\uff0c\u6a21\u62df\u6d77\u9a6c\u4f53CA3\u5728\u8054\u60f3\u8bb0\u5fc6\u4e2d\u7684\u4f5c\u7528\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8bed\u4e49\u5143\u7d20\u7684\u52a8\u6001\u7ed1\u5b9a\uff0c\u652f\u6301\u60c5\u666f\u8bb0\u5fc6\u7684\u5f62\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5efa\u6a21\u8bed\u4e49\u8bb0\u5fc6\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5bf9\u652f\u6301\u60c5\u666f\u8bb0\u5fc6\u7684\u8054\u60f3\u7ed3\u6784\u5f62\u6210\u673a\u5236\u7814\u7a76\u4e0d\u8db3\u3002\u8bba\u6587\u53d7\u6d77\u9a6c\u4f53CA3\u52a8\u529b\u5b66\u7684\u542f\u53d1\uff0c\u8bd5\u56fe\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86LSHN\u6846\u67b6\uff0c\u5305\u62ec\u8bed\u4e49\u7f16\u7801\u5668\u63d0\u53d6\u6f5c\u5728\u8868\u793a\u3001\u6f5c\u5728Hopfield\u7f51\u7edc\u901a\u8fc7\u5438\u5f15\u5b50\u6536\u655b\u8fdb\u884c\u8054\u60f3\u4f18\u5316\uff0c\u4ee5\u53ca\u89e3\u7801\u5668\u91cd\u5efa\u611f\u77e5\u8f93\u5165\u3002\u6a21\u578b\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u7a33\u5065\u7684\u8bb0\u5fc6\u68c0\u7d22\u3002", "result": "\u5728MNIST\u3001CIFAR-10\u548c\u6a21\u62df\u60c5\u666f\u8bb0\u5fc6\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLSHN\u5728\u53d7\u906e\u6321\u548c\u566a\u58f0\u5e72\u6270\u7684\u8f93\u5165\u56de\u5fc6\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u8054\u60f3\u8bb0\u5fc6\u6a21\u578b\u3002", "conclusion": "LSHN\u4e3a\u901a\u8fc7\u751f\u7269\u542f\u53d1\u7684\u5438\u5f15\u5b50\u673a\u5236\u52a8\u6001\u7ed1\u5b9a\u8bed\u4e49\u5143\u7d20\u5230\u60c5\u666f\u8bb0\u5fc6\u75d5\u8ff9\u63d0\u4f9b\u4e86\u8ba1\u7b97\u89c6\u89d2\u3002", "keywords": "\u60c5\u666f\u8bb0\u5fc6, Hopfield\u7f51\u7edc, \u81ea\u7f16\u7801\u5668, \u6d77\u9a6c\u4f53CA3, \u8054\u60f3\u8bb0\u5fc6"}}
{"id": "2506.01276", "pdf": "https://arxiv.org/pdf/2506.01276", "abs": "https://arxiv.org/abs/2506.01276", "authors": ["Sheng Liang", "Yongyue Zhang", "Yaxiong Wu", "Ruiming Tang", "Yong Liu"], "title": "Schema as Parameterized Tools for Universal Information Extraction", "categories": ["cs.CL", "I.2.7"], "comment": "12 pages, 7 figures, 5 tables", "summary": "Universal information extraction (UIE) primarily employs an extractive\ngeneration approach with large language models (LLMs), typically outputting\nstructured information based on predefined schemas such as JSON or tables. UIE\nsuffers from a lack of adaptability when selecting between predefined schemas\nand on-the-fly schema generation within the in-context learning paradigm,\nespecially when there are numerous schemas to choose from. In this paper, we\npropose a unified adaptive text-to-structure generation framework, called\nSchema as Parameterized Tools (SPT), which reimagines the tool-calling\ncapability of LLMs by treating predefined schemas as parameterized tools for\ntool selection and parameter filling. Specifically, our SPT method can be\napplied to unify closed, open, and on-demand IE tasks by adopting Schema\nRetrieval by fetching the relevant schemas from a predefined pool, Schema\nFilling by extracting information and filling slots as with tool parameters, or\nSchema Generation by synthesizing new schemas with uncovered cases. Experiments\nshow that the SPT method can handle four distinct IE tasks adaptively,\ndelivering robust schema retrieval and selection performance. SPT also achieves\ncomparable extraction performance to LoRA baselines and current leading UIE\nsystems with significantly fewer trainable parameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6587\u672c\u5230\u7ed3\u6784\u751f\u6210\u6846\u67b6SPT\uff0c\u901a\u8fc7\u5c06\u9884\u5b9a\u4e49\u6a21\u5f0f\u89c6\u4e3a\u53c2\u6570\u5316\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u901a\u7528\u4fe1\u606f\u63d0\u53d6\uff08UIE\uff09\u4e2d\u7684\u6a21\u5f0f\u9002\u5e94\u6027\u95ee\u9898\u3002SPT\u80fd\u591f\u7edf\u4e00\u5904\u7406\u5c01\u95ed\u3001\u5f00\u653e\u548c\u6309\u9700IE\u4efb\u52a1\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u901a\u7528\u4fe1\u606f\u63d0\u53d6\uff08UIE\uff09\u4e2d\u9884\u5b9a\u4e49\u6a21\u5f0f\u4e0e\u52a8\u6001\u6a21\u5f0f\u751f\u6210\u4e4b\u95f4\u7684\u9002\u5e94\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faSchema as Parameterized Tools (SPT)\u6846\u67b6\uff0c\u5c06\u6a21\u5f0f\u89c6\u4e3a\u53c2\u6570\u5316\u5de5\u5177\uff0c\u652f\u6301\u6a21\u5f0f\u68c0\u7d22\u3001\u6a21\u5f0f\u586b\u5145\u548c\u6a21\u5f0f\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSPT\u80fd\u81ea\u9002\u5e94\u5904\u7406\u56db\u79cdIE\u4efb\u52a1\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u4e14\u8bad\u7ec3\u53c2\u6570\u66f4\u5c11\u3002", "conclusion": "SPT\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86UIE\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "keywords": "\u901a\u7528\u4fe1\u606f\u63d0\u53d6, \u5927\u8bed\u8a00\u6a21\u578b, \u6a21\u5f0f\u53c2\u6570\u5316, \u81ea\u9002\u5e94\u6846\u67b6"}}
{"id": "2506.01311", "pdf": "https://arxiv.org/pdf/2506.01311", "abs": "https://arxiv.org/abs/2506.01311", "authors": ["Leo Mei", "Mark Stamp"], "title": "Energy Considerations for Large Pretrained Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Increasingly complex neural network architectures have achieved phenomenal\nperformance. However, these complex models require massive computational\nresources that consume substantial amounts of electricity, which highlights the\npotential environmental impact of such models. Previous studies have\ndemonstrated that substantial redundancies exist in large pre-trained models.\nHowever, previous work has primarily focused on compressing models while\nretaining comparable model performance, and the direct impact on electricity\nconsumption appears to have received relatively little attention. By\nquantifying the energy usage associated with both uncompressed and compressed\nmodels, we investigate compression as a means of reducing electricity\nconsumption. We consider nine different pre-trained models, ranging in size\nfrom 8M parameters to 138M parameters. To establish a baseline, we first train\neach model without compression and record the electricity usage and time\nrequired during training, along with other relevant statistics. We then apply\nthree compression techniques: Steganographic capacity reduction, pruning, and\nlow-rank factorization. In each of the resulting cases, we again measure the\nelectricity usage, training time, model accuracy, and so on. We find that\npruning and low-rank factorization offer no significant improvements with\nrespect to energy usage or other related statistics, while steganographic\ncapacity reduction provides major benefits in almost every case. We discuss the\nsignificance of these findings.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\u5bf9\u51cf\u5c11\u7535\u529b\u6d88\u8017\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u538b\u7f29\u6280\u672f\u5728\u4e0d\u540c\u6a21\u578b\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u9690\u5199\u5bb9\u91cf\u7f29\u51cf\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u968f\u7740\u795e\u7ecf\u7f51\u7edc\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u5176\u8ba1\u7b97\u8d44\u6e90\u548c\u9ad8\u7535\u529b\u6d88\u8017\u5e26\u6765\u4e86\u73af\u5883\u95ee\u9898\uff0c\u4f46\u5df2\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6027\u80fd\u4fdd\u7559\u800c\u975e\u7535\u529b\u6d88\u8017\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u9009\u53d6\u4e5d\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5148\u8bb0\u5f55\u672a\u538b\u7f29\u65f6\u7684\u7535\u529b\u6d88\u8017\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u518d\u5e94\u7528\u9690\u5199\u5bb9\u91cf\u7f29\u51cf\u3001\u526a\u679d\u548c\u4f4e\u79e9\u5206\u89e3\u4e09\u79cd\u538b\u7f29\u6280\u672f\u5e76\u5bf9\u6bd4\u7ed3\u679c\u3002", "result": "\u526a\u679d\u548c\u4f4e\u79e9\u5206\u89e3\u5728\u7535\u529b\u6d88\u8017\u65b9\u9762\u65e0\u663e\u8457\u6539\u8fdb\uff0c\u800c\u9690\u5199\u5bb9\u91cf\u7f29\u51cf\u5728\u51e0\u4e4e\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u8868\u73b0\u51fa\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "\u9690\u5199\u5bb9\u91cf\u7f29\u51cf\u662f\u51cf\u5c11\u795e\u7ecf\u7f51\u7edc\u7535\u529b\u6d88\u8017\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7eff\u8272AI\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\u3001\u7535\u529b\u6d88\u8017\u3001\u9690\u5199\u5bb9\u91cf\u7f29\u51cf\u3001\u526a\u679d\u3001\u4f4e\u79e9\u5206\u89e3"}}
{"id": "2506.01305", "pdf": "https://arxiv.org/pdf/2506.01305", "abs": "https://arxiv.org/abs/2506.01305", "authors": ["Thong Nguyen", "Duc Nguyen", "Minh Dang", "Thai Dao", "Long Nguyen", "Quan H. Nguyen", "Dat Nguyen", "Kien Tran", "Minh Tran"], "title": "VM14K: First Vietnamese Medical Benchmark", "categories": ["cs.CL"], "comment": null, "summary": "Medical benchmarks are indispensable for evaluating the capabilities of\nlanguage models in healthcare for non-English-speaking communities,therefore\nhelp ensuring the quality of real-life applications. However, not every\ncommunity has sufficient resources and standardized methods to effectively\nbuild and design such benchmark, and available non-English medical data is\nnormally fragmented and difficult to verify. We developed an approach to tackle\nthis problem and applied it to create the first Vietnamese medical question\nbenchmark, featuring 14,000 multiple-choice questions across 34 medical\nspecialties. Our benchmark was constructed using various verifiable sources,\nincluding carefully curated medical exams and clinical records, and eventually\nannotated by medical experts. The benchmark includes four difficulty levels,\nranging from foundational biological knowledge commonly found in textbooks to\ntypical clinical case studies that require advanced reasoning. This design\nenables assessment of both the breadth and depth of language models' medical\nunderstanding in the target language thanks to its extensive coverage and\nin-depth subject-specific expertise. We release the benchmark in three parts: a\nsample public set (4k questions), a full public set (10k questions), and a\nprivate set (2k questions) used for leaderboard evaluation. Each set contains\nall medical subfields and difficulty levels. Our approach is scalable to other\nlanguages, and we open-source our data construction pipeline to support the\ndevelopment of future multilingual benchmarks in the medical domain.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6784\u5efa\u8d8a\u5357\u8bed\u533b\u5b66\u95ee\u9898\u57fa\u51c6\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u80fd\u529b\uff0c\u5305\u542b14,000\u4e2a\u9009\u62e9\u9898\uff0c\u6db5\u76d634\u4e2a\u533b\u5b66\u4e13\u4e1a\u3002", "motivation": "\u4e3a\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u975e\u82f1\u8bed\u533b\u7597\u793e\u533a\u4e2d\u7684\u80fd\u529b\uff0c\u786e\u4fdd\u5b9e\u9645\u5e94\u7528\u7684\u8d28\u91cf\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u8d44\u6e90\u548c\u65b9\u6cd5\u6765\u6784\u5efa\u6b64\u7c7b\u57fa\u51c6\u3002", "method": "\u4f7f\u7528\u53ef\u9a8c\u8bc1\u7684\u533b\u5b66\u8003\u8bd5\u548c\u4e34\u5e8a\u8bb0\u5f55\uff0c\u7ecf\u4e13\u5bb6\u6807\u6ce8\uff0c\u6784\u5efa\u6db5\u76d6\u56db\u4e2a\u96be\u5ea6\u7b49\u7ea7\u7684\u591a\u9009\u9898\u57fa\u51c6\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u8d8a\u5357\u8bed\u533b\u5b66\u95ee\u9898\u57fa\u51c6\uff0c\u5206\u4e3a\u516c\u5f00\u6837\u672c\u96c6\u3001\u5b8c\u6574\u516c\u5f00\u96c6\u548c\u79c1\u4eba\u8bc4\u4f30\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u8bed\u8a00\uff0c\u5f00\u6e90\u6570\u636e\u6784\u5efa\u6d41\u7a0b\u652f\u6301\u672a\u6765\u591a\u8bed\u8a00\u533b\u7597\u57fa\u51c6\u7684\u5f00\u53d1\u3002", "keywords": "\u533b\u5b66\u57fa\u51c6\uff0c\u8d8a\u5357\u8bed\uff0c\u8bed\u8a00\u6a21\u578b\uff0c\u533b\u7597\u9886\u57df\uff0c\u591a\u8bed\u8a00"}}
{"id": "2506.00308", "pdf": "https://arxiv.org/pdf/2506.00308", "abs": "https://arxiv.org/abs/2506.00308", "authors": ["Hayoung Jung", "Shravika Mittal", "Ananya Aatreya", "Navreet Kaur", "Munmun De Choudhury", "Tanushree Mitra"], "title": "MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": "34 pages, 14 figures, 21 tables. In submission", "summary": "Understanding the prevalence of misinformation in health topics online can\ninform public health policies and interventions. However, measuring such\nmisinformation at scale remains a challenge, particularly for high-stakes but\nunderstudied topics like opioid-use disorder (OUD)--a leading cause of death in\nthe U.S. We present the first large-scale study of OUD-related myths on\nYouTube, a widely-used platform for health information. With clinical experts,\nwe validate 8 pervasive myths and release an expert-labeled video dataset. To\nscale labeling, we introduce MythTriage, an efficient triage pipeline that uses\na lightweight model for routine cases and defers harder ones to a\nhigh-performing, but costlier, large language model (LLM). MythTriage achieves\nup to 0.86 macro F1-score while estimated to reduce annotation time and\nfinancial cost by over 76% compared to experts and full LLM labeling. We\nanalyze 2.9K search results and 343K recommendations, uncovering how myths\npersist on YouTube and offering actionable insights for public health and\nplatform moderation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5927\u89c4\u6a21\u7814\u7a76\u4e86YouTube\u4e0a\u4e0e\u963f\u7247\u7c7b\u4f7f\u7528\u969c\u788d\uff08OUD\uff09\u76f8\u5173\u7684\u5065\u5eb7\u8c23\u8a00\uff0c\u63d0\u51fa\u4e86MythTriage\u9ad8\u6548\u6807\u6ce8\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u8c23\u8a00\u7684\u4f20\u64ad\u6a21\u5f0f\u3002", "motivation": "\u6d4b\u91cf\u5728\u7ebf\u5065\u5eb7\u8c23\u8a00\u5bf9\u516c\u5171\u536b\u751f\u653f\u7b56\u548c\u5e72\u9884\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u89c4\u6a21\u6d4b\u91cf\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u963f\u7247\u7c7b\u4f7f\u7528\u969c\u788d\uff08OUD\uff09\u8fd9\u4e00\u9ad8\u98ce\u9669\u4f46\u7814\u7a76\u4e0d\u8db3\u7684\u8bdd\u9898\u3002", "method": "\u901a\u8fc7\u4e0e\u4e34\u5e8a\u4e13\u5bb6\u5408\u4f5c\u9a8c\u8bc18\u79cd\u5e38\u89c1\u8c23\u8a00\uff0c\u5e76\u53d1\u5e03\u4e13\u5bb6\u6807\u6ce8\u7684\u89c6\u9891\u6570\u636e\u96c6\uff1b\u63d0\u51faMythTriage\u65b9\u6cd5\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u9ad8\u6548\u6807\u6ce8\u3002", "result": "MythTriage\u7684\u5b8fF1-score\u8fbe\u52300.86\uff0c\u6807\u6ce8\u65f6\u95f4\u548c\u6210\u672c\u964d\u4f4e76%\u4ee5\u4e0a\uff1b\u5206\u6790\u4e862.9K\u641c\u7d22\u7ed3\u679c\u548c343K\u63a8\u8350\u89c6\u9891\uff0c\u63ed\u793a\u4e86\u8c23\u8a00\u7684\u4f20\u64ad\u673a\u5236\u3002", "conclusion": "\u7814\u7a76\u4e3a\u516c\u5171\u536b\u751f\u548c\u5e73\u53f0\u5185\u5bb9\u5ba1\u6838\u63d0\u4f9b\u4e86\u53ef\u884c\u89c1\u89e3\uff0c\u5c55\u793a\u4e86MythTriage\u5728\u5927\u89c4\u6a21\u8c23\u8a00\u6807\u6ce8\u4e2d\u7684\u9ad8\u6548\u6027\u3002", "keywords": "\u963f\u7247\u7c7b\u4f7f\u7528\u969c\u788d, YouTube, \u5065\u5eb7\u8c23\u8a00, MythTriage, \u5927\u578b\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.01317", "pdf": "https://arxiv.org/pdf/2506.01317", "abs": "https://arxiv.org/abs/2506.01317", "authors": ["Yanjun Fu", "Faisal Hamman", "Sanghamitra Dutta"], "title": "T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages, 6 figures", "summary": "Instruction tuning is essential for Large Language Models (LLMs) to\neffectively follow user instructions. To improve training efficiency and reduce\ndata redundancy, recent works use LLM-based scoring functions, e.g.,\nInstruction-Following Difficulty (IFD), to select high-quality\ninstruction-tuning data with scores above a threshold. While these data\nselection methods often lead to models that can match or even exceed the\nperformance of models trained on the full datasets, we identify two key\nlimitations: (i) they assess quality at the sample level, ignoring token-level\ninformativeness; and (ii) they overlook the robustness of the scoring method,\noften selecting a sample due to superficial lexical features instead of its\ntrue quality. In this work, we propose Token-Selective HIeRarchical Data\nSelection for Instruction Tuning (T-SHIRT), a novel data selection framework\nthat introduces a new scoring method to include only informative tokens in\nquality evaluation and also promotes robust and reliable samples whose\nneighbors also show high quality with less local inconsistencies. We\ndemonstrate that models instruction-tuned on a curated dataset (only 5% of the\noriginal size) using T-SHIRT can outperform those trained on the entire\nlarge-scale dataset by up to 5.48 points on average across eight benchmarks.\nAcross various LLMs and training set scales, our method consistently surpasses\nexisting state-of-the-art data selection techniques, while also remaining both\ncost-effective and highly efficient. For instance, by using GPT-2 for score\ncomputation, we are able to process a dataset of 52k samples using 40 minutes\non a single GPU.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aT-SHIRT\u7684\u65b0\u578b\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u8bc4\u5206\u65b9\u6cd5\u5173\u6ce8\u4ee4\u724c\u7ea7\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u6307\u4ee4\u8c03\u4f18\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u5728\u6837\u672c\u7ea7\u522b\u8bc4\u4f30\u8d28\u91cf\uff0c\u5ffd\u7565\u4e86\u4ee4\u724c\u7ea7\u7684\u6709\u6548\u6027\uff0c\u4e14\u8bc4\u5206\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faToken-Selective HIeRarchical Data Selection\uff08T-SHIRT\uff09\uff0c\u901a\u8fc7\u65b0\u8bc4\u5206\u65b9\u6cd5\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684\u4ee4\u724c\u5e76\u63d0\u5347\u6837\u672c\u7684\u9c81\u68d2\u6027\u3002", "result": "\u4f7f\u7528T-SHIRT\u7cbe\u9009\u7684\u6570\u636e\u96c6\uff08\u4ec5\u539f\u6570\u636e\u76845%\uff09\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u4f18\u4e8e\u4f7f\u7528\u5168\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b5.48\u5206\u3002", "conclusion": "T-SHIRT\u5728\u6570\u636e\u9009\u62e9\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u5177\u6709\u6210\u672c\u6548\u76ca\u548c\u9ad8\u6548\u7387\u3002", "keywords": "\u6307\u4ee4\u8c03\u4f18, \u6570\u636e\u9009\u62e9, \u4ee4\u724c\u7ea7\u8bc4\u4f30, \u9c81\u68d2\u6027, T-SHIRT"}}
{"id": "2506.01308", "pdf": "https://arxiv.org/pdf/2506.01308", "abs": "https://arxiv.org/abs/2506.01308", "authors": ["Christopher Li", "Rickard Stureborg", "Bhuwan Dhingra", "Jun Yang"], "title": "A Platform for Investigating Public Health Content with Efficient Concern Classification", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "19 pages, 15 figures", "summary": "A recent rise in online content expressing concerns with public health\ninitiatives has contributed to already stalled uptake of preemptive measures\nglobally. Future public health efforts must attempt to understand such content,\nwhat concerns it may raise among readers, and how to effectively respond to it.\nTo this end, we present ConcernScope, a platform that uses a teacher-student\nframework for knowledge transfer between large language models and light-weight\nclassifiers to quickly and effectively identify the health concerns raised in a\ntext corpus. The platform allows uploading massive files directly,\nautomatically scraping specific URLs, and direct text editing. ConcernScope is\nbuilt on top of a taxonomy of public health concerns. Intended for public\nhealth officials, we demonstrate several applications of this platform: guided\ndata exploration to find useful examples of common concerns found in online\ncommunity datasets, identification of trends in concerns through an example\ntime series analysis of 186,000 samples, and finding trends in topic frequency\nbefore and after significant events.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aConcernScope\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u5feb\u901f\u8bc6\u522b\u6587\u672c\u4e2d\u7684\u5065\u5eb7\u5173\u5207\u95ee\u9898\uff0c\u5e2e\u52a9\u516c\u5171\u536b\u751f\u5b98\u5458\u7406\u89e3\u5e76\u5e94\u5bf9\u5728\u7ebf\u5185\u5bb9\u4e2d\u7684\u62c5\u5fe7\u3002", "motivation": "\u5728\u7ebf\u5185\u5bb9\u4e2d\u5bf9\u516c\u5171\u536b\u751f\u4e3e\u63aa\u7684\u62c5\u5fe7\u589e\u52a0\uff0c\u5bfc\u81f4\u5168\u7403\u9884\u9632\u63aa\u65bd\u7684\u91c7\u7eb3\u53d7\u963b\u3002\u672a\u6765\u516c\u5171\u536b\u751f\u5de5\u4f5c\u9700\u7406\u89e3\u8fd9\u4e9b\u5185\u5bb9\u53ca\u8bfb\u8005\u7684\u5173\u5207\uff0c\u5e76\u6709\u6548\u56de\u5e94\u3002", "method": "ConcernScope\u91c7\u7528\u6559\u5e08-\u5b66\u751f\u6846\u67b6\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u8f7b\u91cf\u5206\u7c7b\u5668\uff0c\u5feb\u901f\u8bc6\u522b\u6587\u672c\u4e2d\u7684\u5065\u5eb7\u5173\u5207\u95ee\u9898\u3002", "result": "\u5e73\u53f0\u652f\u6301\u5927\u89c4\u6a21\u6587\u4ef6\u4e0a\u4f20\u3001\u81ea\u52a8URL\u6293\u53d6\u548c\u76f4\u63a5\u6587\u672c\u7f16\u8f91\uff0c\u5c55\u793a\u4e86\u5728\u5728\u7ebf\u793e\u533a\u6570\u636e\u96c6\u4e2d\u53d1\u73b0\u5e38\u89c1\u5173\u5207\u3001\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7b49\u5e94\u7528\u3002", "conclusion": "ConcernScope\u4e3a\u516c\u5171\u536b\u751f\u5b98\u5458\u63d0\u4f9b\u4e86\u8bc6\u522b\u548c\u5206\u6790\u5065\u5eb7\u5173\u5207\u7684\u6709\u6548\u5de5\u5177\u3002", "keywords": "\u516c\u5171\u536b\u751f,\u5728\u7ebf\u5185\u5bb9,\u5173\u5207\u8bc6\u522b,\u8bed\u8a00\u6a21\u578b,\u77e5\u8bc6\u8fc1\u79fb"}}
{"id": "2506.01318", "pdf": "https://arxiv.org/pdf/2506.01318", "abs": "https://arxiv.org/abs/2506.01318", "authors": ["SeungBum Ha", "Saerom Park", "Sung Whan Yoon"], "title": "Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack", "categories": ["cs.LG", "cs.AI", "I.2.6"], "comment": "10 pages, 4 figures, 2 tables, Under review at NeurIPS 2025", "summary": "Machine unlearning (MU) aims to expunge a designated forget set from a\ntrained model without costly retraining, yet the existing techniques overlook\ntwo critical blind spots: \"over-unlearning\" that deteriorates retained data\nnear the forget set, and post-hoc \"relearning\" attacks that aim to resurrect\nthe forgotten knowledge. We first derive the over-unlearning metric\nOU@{\\epsilon}, which represents the collateral damage to the nearby region of\nthe forget set, where the over-unlearning mainly appears. Next, we expose an\nunforeseen relearning threat on MU, i.e., the Prototypical Relearning Attack,\nwhich exploits the per-class prototype of the forget class with just a few\nsamples, and easily restores the pre-unlearning performance. To counter both\nblind spots, we introduce Spotter, a plug-and-play objective that combines (i)\na masked knowledge-distillation penalty on the nearby region of forget set to\nsuppress OU@{\\epsilon}, and (ii) an intra-class dispersion loss that scatters\nforget-class embeddings, neutralizing prototypical relearning attacks. On\nCIFAR-10, as one of validations, Spotter reduces OU@{\\epsilon}by below the\n0.05X of the baseline, drives forget accuracy to 0%, preserves accuracy of the\nretain set within 1% of difference with the original, and denies the\nprototype-attack by keeping the forget set accuracy within <1%, without\naccessing retained data. It confirms that Spotter is a practical remedy of the\nunlearning's blind spots.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u673a\u5668\u9057\u5fd8\uff08MU\uff09\u6280\u672f\u4e2d\u5b58\u5728\u7684\u201c\u8fc7\u5ea6\u9057\u5fd8\u201d\u548c\u201c\u540e\u5b66\u4e60\u201d\u653b\u51fb\u76f2\u70b9\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u540d\u4e3aSpotter\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u63a9\u853d\u77e5\u8bc6\u84b8\u998f\u548c\u7c7b\u5185\u5206\u6563\u635f\u5931\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660eSpotter\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u9057\u5fd8\u6280\u672f\u5ffd\u7565\u4e86\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u8fc7\u5ea6\u9057\u5fd8\u5bfc\u81f4\u7684\u90bb\u8fd1\u6570\u636e\u6027\u80fd\u4e0b\u964d\uff0c\u4ee5\u53ca\u540e\u5b66\u4e60\u653b\u51fb\uff08\u5982\u539f\u578b\u518d\u5b66\u4e60\u653b\u51fb\uff09\u53ef\u80fd\u6062\u590d\u88ab\u9057\u5fd8\u7684\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86Spotter\u65b9\u6cd5\uff0c\u5305\u542b\u63a9\u853d\u77e5\u8bc6\u84b8\u998f\u60e9\u7f5a\u548c\u7c7b\u5185\u5206\u6563\u635f\u5931\uff0c\u4ee5\u51cf\u5c11\u8fc7\u5ea6\u9057\u5fd8\u5e76\u9632\u6b62\u539f\u578b\u518d\u5b66\u4e60\u653b\u51fb\u3002", "result": "\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\uff0cSpotter\u5c06\u8fc7\u5ea6\u9057\u5fd8\u964d\u81f3\u57fa\u7ebf\u76840.05\u500d\u4ee5\u4e0b\uff0c\u9057\u5fd8\u51c6\u786e\u7387\u964d\u81f30%\uff0c\u4fdd\u7559\u96c6\u51c6\u786e\u7387\u4e0e\u539f\u6a21\u578b\u5dee\u5f02\u5c0f\u4e8e1%\uff0c\u4e14\u6210\u529f\u62b5\u5fa1\u4e86\u539f\u578b\u653b\u51fb\u3002", "conclusion": "Spotter\u662f\u89e3\u51b3\u673a\u5668\u9057\u5fd8\u76f2\u70b9\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u80fd\u591f\u5e73\u8861\u9057\u5fd8\u6548\u679c\u4e0e\u6a21\u578b\u6027\u80fd\u3002", "keywords": "\u673a\u5668\u9057\u5fd8,\u8fc7\u5ea6\u9057\u5fd8,\u539f\u578b\u518d\u5b66\u4e60\u653b\u51fb,Spotter,\u77e5\u8bc6\u84b8\u998f"}}
{"id": "2506.01312", "pdf": "https://arxiv.org/pdf/2506.01312", "abs": "https://arxiv.org/abs/2506.01312", "authors": ["Chunhui Zhang", "Sirui", "Wang", "Zhongyu Ouyang", "Xiangchi Yuan", "Soroush Vosoughi"], "title": "Growing Through Experience: Scaling Episodic Grounding in Language Models", "categories": ["cs.CL"], "comment": "Accepted at The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Language models (LMs) require robust episodic grounding-the capacity to learn\nfrom and apply past experiences-to excel at physical planning tasks. Current\nepisodic grounding approaches struggle with scalability and integration,\nlimiting their effectiveness, especially for medium-sized LMs (7B parameters).\nWhile larger LMs (70-405B parameters) possess superior hierarchical\nrepresentations and extensive pre-trained knowledge, they encounter a\nfundamental scale paradox: despite their advanced abstraction capabilities,\nthey lack efficient mechanisms to leverage experience streams. We propose a\nscalable weak-to-strong episodic learning framework that effectively transfers\nepisodic behaviors from smaller to larger LMs. This framework integrates Monte\nCarlo tree search for structured experience collection with a novel\ndistillation method, preserving the inherent LM capabilities while embedding\nepisodic memory. Experiments demonstrate our method surpasses state-of-the-art\nproprietary LMs by 3.45% across diverse planning and question-answering tasks.\nLayer-wise probing further indicates significant improvements in task\nalignment, especially within deeper LM layers, highlighting stable\ngeneralization even for previously unseen scenarios with increased planning\ncomplexity-conditions where baseline methods degrade markedly.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5f31\u5230\u5f3a\u60c5\u666f\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u65b0\u578b\u84b8\u998f\u65b9\u6cd5\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u60c5\u666f\u5b66\u4e60\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u96c6\u6210\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5bf9\u4e2d\u7b49\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u3002\u5927\u6a21\u578b\u867d\u5177\u5907\u66f4\u5f3a\u7684\u62bd\u8c61\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u5229\u7528\u7ecf\u9a8c\u6d41\u7684\u673a\u5236\u3002", "method": "\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u8fdb\u884c\u7ed3\u6784\u5316\u7ecf\u9a8c\u6536\u96c6\uff0c\u5e76\u91c7\u7528\u65b0\u578b\u84b8\u998f\u65b9\u6cd5\uff0c\u5728\u4fdd\u7559\u8bed\u8a00\u6a21\u578b\u56fa\u6709\u80fd\u529b\u7684\u540c\u65f6\u5d4c\u5165\u60c5\u666f\u8bb0\u5fc6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u89c4\u5212\u548c\u95ee\u7b54\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f3.45%\uff0c\u5e76\u5728\u6df1\u5c42\u6a21\u578b\u5c42\u4e2d\u663e\u8457\u63d0\u5347\u4efb\u52a1\u5bf9\u9f50\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u63d0\u5347\u6a21\u578b\u60c5\u666f\u8bb0\u5fc6\u80fd\u529b\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u7a33\u5b9a\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u89c4\u5212\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b,\u60c5\u666f\u5b66\u4e60,\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22,\u84b8\u998f\u65b9\u6cd5,\u4efb\u52a1\u5bf9\u9f50"}}
{"id": "2506.00322", "pdf": "https://arxiv.org/pdf/2506.00322", "abs": "https://arxiv.org/abs/2506.00322", "authors": ["Sofiane Mahiou", "Amir Dizche", "Reza Nazari", "Xinmin Wu", "Ralph Abbey", "Jorge Silva", "Georgi Ganev"], "title": "dpmm: Differentially Private Marginal Models, a Library for Synthetic Tabular Data Generation", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "Accepted to the Theory and Practice of Differential Privacy Workshop\n  (TPDP 2025)", "summary": "We propose dpmm, an open-source library for synthetic data generation with\nDifferentially Private (DP) guarantees. It includes three popular marginal\nmodels -- PrivBayes, MST, and AIM -- that achieve superior utility and offer\nricher functionality compared to alternative implementations. Additionally, we\nadopt best practices to provide end-to-end DP guarantees and address well-known\nDP-related vulnerabilities. Our goal is to accommodate a wide audience with\neasy-to-install, highly customizable, and robust model implementations.\n  Our codebase is available from https://github.com/sassoftware/dpmm.", "AI": {"tldr": "\u63d0\u51fadpmm\u5f00\u6e90\u5e93\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u5dee\u5206\u9690\u79c1(DP)\u4fdd\u8bc1\u7684\u5408\u6210\u6570\u636e\uff0c\u5305\u542b\u4e09\u79cd\u8fb9\u9645\u6a21\u578b\u5e76\u63d0\u4f9b\u4e30\u5bcc\u529f\u80fd\u3002", "motivation": "\u4e3a\u6ee1\u8db3\u5e7f\u6cdb\u7528\u6237\u9700\u6c42\uff0c\u63d0\u4f9b\u6613\u5b89\u88c5\u3001\u9ad8\u5ea6\u53ef\u5b9a\u5236\u4e14\u9c81\u68d2\u7684\u5dee\u5206\u9690\u79c1\u5408\u6210\u6570\u636e\u751f\u6210\u5de5\u5177\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u6d41\u884c\u7684\u8fb9\u9645\u6a21\u578b\uff08PrivBayes\u3001MST\u3001AIM\uff09\uff0c\u7ed3\u5408\u6700\u4f73\u5b9e\u8df5\u63d0\u4f9b\u7aef\u5230\u7aefDP\u4fdd\u8bc1\u3002", "result": "\u5b9e\u73b0\u4f18\u4e8e\u5176\u4ed6\u5de5\u5177\u7684\u6548\u7528\uff0c\u5e76\u89e3\u51b3\u5df2\u77e5DP\u76f8\u5173\u6f0f\u6d1e\u3002", "conclusion": "dpmm\u662f\u4e00\u4e2a\u529f\u80fd\u4e30\u5bcc\u4e14\u9ad8\u5ea6\u53ef\u9760\u7684\u5dee\u5206\u9690\u79c1\u5408\u6210\u6570\u636e\u751f\u6210\u5e93\u3002", "keywords": "\u5dee\u5206\u9690\u79c1, \u5408\u6210\u6570\u636e, \u5f00\u6e90\u5e93, PrivBayes, MST, AIM"}}
{"id": "2506.01320", "pdf": "https://arxiv.org/pdf/2506.01320", "abs": "https://arxiv.org/abs/2506.01320", "authors": ["Taehoon Yoon", "Yunhong Min", "Kyeongmin Yeo", "Minhyuk Sung"], "title": "$\u03a8$-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "We introduce $\\Psi$-Sampler, an SMC-based framework incorporating pCNL-based\ninitial particle sampling for effective inference-time reward alignment with a\nscore-based generative model. Inference-time reward alignment with score-based\ngenerative models has recently gained significant traction, following a broader\nparadigm shift from pre-training to post-training optimization. At the core of\nthis trend is the application of Sequential Monte Carlo (SMC) to the denoising\nprocess. However, existing methods typically initialize particles from the\nGaussian prior, which inadequately captures reward-relevant regions and results\nin reduced sampling efficiency. We demonstrate that initializing from the\nreward-aware posterior significantly improves alignment performance. To enable\nposterior sampling in high-dimensional latent spaces, we introduce the\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines\ndimension-robust proposals with gradient-informed dynamics. This approach\nenables efficient and scalable posterior sampling and consistently improves\nperformance across various reward alignment tasks, including layout-to-image\ngeneration, quantity-aware generation, and aesthetic-preference generation, as\ndemonstrated in our experiments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86$\\\\\\\\Psi$-Sampler\u6846\u67b6\uff0c\u7ed3\u5408pCNL\u521d\u59cb\u7c92\u5b50\u91c7\u6837\uff0c\u7528\u4e8e\u57fa\u4e8e\u8bc4\u5206\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u65f6\u95f4\u5956\u52b1\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91c7\u6837\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ece\u9ad8\u65af\u5148\u9a8c\u521d\u59cb\u5316\u7c92\u5b50\uff0c\u672a\u80fd\u6709\u6548\u6355\u6349\u5956\u52b1\u76f8\u5173\u533a\u57df\uff0c\u5bfc\u81f4\u91c7\u6837\u6548\u7387\u4f4e\u3002\u8bba\u6587\u63d0\u51fa\u4ece\u5956\u52b1\u611f\u77e5\u540e\u9a8c\u521d\u59cb\u5316\u4ee5\u63d0\u9ad8\u5bf9\u9f50\u6027\u80fd\u3002", "method": "\u5f15\u5165\u9884\u5904\u7406\u7684Crank-Nicolson Langevin (pCNL)\u7b97\u6cd5\uff0c\u7ed3\u5408\u7ef4\u5ea6\u9c81\u68d2\u7684\u63d0\u8bae\u4e0e\u68af\u5ea6\u4fe1\u606f\u52a8\u6001\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u540e\u9a8c\u91c7\u6837\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e03\u5c40\u5230\u56fe\u50cf\u751f\u6210\u3001\u6570\u91cf\u611f\u77e5\u751f\u6210\u548c\u5ba1\u7f8e\u504f\u597d\u751f\u6210\u7b49\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "$\\\\\\\\Psi$-Sampler\u901a\u8fc7\u5956\u52b1\u611f\u77e5\u540e\u9a8c\u91c7\u6837\u548cpCNL\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u95f4\u5956\u52b1\u5bf9\u9f50\u4e2d\u7684\u6548\u7387\u95ee\u9898\u3002", "keywords": "$\\\\\\\\Psi$-Sampler, pCNL, \u5956\u52b1\u5bf9\u9f50, \u540e\u9a8c\u91c7\u6837, \u8bc4\u5206\u751f\u6210\u6a21\u578b"}}
{"id": "2506.01322", "pdf": "https://arxiv.org/pdf/2506.01322", "abs": "https://arxiv.org/abs/2506.01322", "authors": ["Thi Vu", "Linh The Nguyen", "Dat Quoc Nguyen"], "title": "Zero-Shot Text-to-Speech for Vietnamese", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "To appear in Proceedings of ACL 2025 (Main conference paper)", "summary": "This paper introduces PhoAudiobook, a newly curated dataset comprising 941\nhours of high-quality audio for Vietnamese text-to-speech. Using PhoAudiobook,\nwe conduct experiments on three leading zero-shot TTS models: VALL-E,\nVoiceCraft, and XTTS-V2. Our findings demonstrate that PhoAudiobook\nconsistently enhances model performance across various metrics. Moreover,\nVALL-E and VoiceCraft exhibit superior performance in synthesizing short\nsentences, highlighting their robustness in handling diverse linguistic\ncontexts. We publicly release PhoAudiobook to facilitate further research and\ndevelopment in Vietnamese text-to-speech.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86PhoAudiobook\u6570\u636e\u96c6\uff0c\u5305\u542b941\u5c0f\u65f6\u7684\u8d8a\u5357\u8bed\u9ad8\u8d28\u91cf\u97f3\u9891\uff0c\u7528\u4e8e\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u3002\u5b9e\u9a8c\u4f7f\u7528\u4e09\u79cd\u96f6\u6837\u672cTTS\u6a21\u578b\uff08VALL-E\u3001VoiceCraft\u3001XTTS-V2\uff09\uff0c\u53d1\u73b0PhoAudiobook\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5176\u4e2dVALL-E\u548cVoiceCraft\u5728\u77ed\u53e5\u5408\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002", "motivation": "\u4e3a\u4fc3\u8fdb\u8d8a\u5357\u8bed\u6587\u672c\u5230\u8bed\u97f3\u6280\u672f\u7684\u7814\u7a76\u548c\u53d1\u5c55\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u97f3\u9891\u6570\u636e\u96c6\u3002", "method": "\u5229\u7528PhoAudiobook\u6570\u636e\u96c6\uff0c\u5bf9VALL-E\u3001VoiceCraft\u548cXTTS-V2\u4e09\u79cd\u96f6\u6837\u672cTTS\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "PhoAudiobook\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0cVALL-E\u548cVoiceCraft\u5728\u77ed\u53e5\u5408\u6210\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "PhoAudiobook\u53ef\u4f5c\u4e3a\u8d8a\u5357\u8bedTTS\u7814\u7a76\u7684\u91cd\u8981\u8d44\u6e90\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "keywords": "PhoAudiobook, \u8d8a\u5357\u8bedTTS, \u96f6\u6837\u672c\u6a21\u578b, VALL-E, VoiceCraft, XTTS-V2"}}
{"id": "2506.00327", "pdf": "https://arxiv.org/pdf/2506.00327", "abs": "https://arxiv.org/abs/2506.00327", "authors": ["Shreshth Saini", "Ru-Ling Liao", "Yan Ye", "Alan C. Bovik"], "title": "Latent Guidance in Diffusion Models for Perceptual Evaluations", "categories": ["cs.CV", "cs.AI"], "comment": "24 Pages, 7 figures, 10 Tables", "summary": "Despite recent advancements in latent diffusion models that generate\nhigh-dimensional image data and perform various downstream tasks, there has\nbeen little exploration into perceptual consistency within these models on the\ntask of No-Reference Image Quality Assessment (NR-IQA). In this paper, we\nhypothesize that latent diffusion models implicitly exhibit perceptually\nconsistent local regions within the data manifold. We leverage this insight to\nguide on-manifold sampling using perceptual features and input measurements.\nSpecifically, we propose Perceptual Manifold Guidance (PMG), an algorithm that\nutilizes pretrained latent diffusion models and perceptual quality features to\nobtain perceptually consistent multi-scale and multi-timestep feature maps from\nthe denoising U-Net. We empirically demonstrate that these hyperfeatures\nexhibit high correlation with human perception in IQA tasks. Our method can be\napplied to any existing pretrained latent diffusion model and is\nstraightforward to integrate. To the best of our knowledge, this paper is the\nfirst work on guiding diffusion model with perceptual features for NR-IQA.\nExtensive experiments on IQA datasets show that our method, LGDM, achieves\nstate-of-the-art performance, underscoring the superior generalization\ncapabilities of diffusion models for NR-IQA tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u611f\u77e5\u6d41\u5f62\u5f15\u5bfc\uff08PMG\uff09\u7684\u7b97\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u611f\u77e5\u8d28\u91cf\u7279\u5f81\uff0c\u5728\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08NR-IQA\uff09\u4efb\u52a1\u4e2d\u5b9e\u73b0\u611f\u77e5\u4e00\u81f4\u6027\uff0c\u5e76\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u611f\u77e5\u9ad8\u5ea6\u76f8\u5173\u7684\u7279\u5f81\u3002", "motivation": "\u5c3d\u7ba1\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728\u9ad8\u7ef4\u56fe\u50cf\u751f\u6210\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5bf9\u5176\u5728NR-IQA\u4efb\u52a1\u4e2d\u7684\u611f\u77e5\u4e00\u81f4\u6027\u7814\u7a76\u8f83\u5c11\u3002\u8bba\u6587\u5047\u8bbe\u8fd9\u4e9b\u6a21\u578b\u9690\u5f0f\u5730\u8868\u73b0\u51fa\u6570\u636e\u6d41\u5f62\u4e2d\u7684\u611f\u77e5\u4e00\u81f4\u5c40\u90e8\u533a\u57df\uff0c\u5e76\u5229\u7528\u8fd9\u4e00\u5047\u8bbe\u8fdb\u884c\u6539\u8fdb\u3002", "method": "\u63d0\u51faPMG\u7b97\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u611f\u77e5\u7279\u5f81\uff0c\u4ece\u53bb\u566aU-Net\u4e2d\u63d0\u53d6\u591a\u5c3a\u5ea6\u548c\u591a\u65f6\u95f4\u6b65\u7684\u7279\u5f81\u56fe\uff0c\u4ee5\u5b9e\u73b0\u611f\u77e5\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u7279\u5f81\u4e0e\u4eba\u7c7b\u611f\u77e5\u9ad8\u5ea6\u76f8\u5173\uff0c\u4e14\u5728NR-IQA\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5c06\u611f\u77e5\u7279\u5f81\u7528\u4e8e\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u8fdb\u884cNR-IQA\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u4f18\u8d8a\u6027\u3002", "keywords": "\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff0c\u611f\u77e5\u4e00\u81f4\u6027\uff0c\u611f\u77e5\u6d41\u5f62\u5f15\u5bfc\uff0cU-Net"}}
{"id": "2506.01327", "pdf": "https://arxiv.org/pdf/2506.01327", "abs": "https://arxiv.org/abs/2506.01327", "authors": ["Zenghao Guan", "Guojun Zhu", "Yucan Zhou", "Wu Liu", "Weiping Wang", "Jiebo Luo", "Xiaoyan Gu"], "title": "STSA: Federated Class-Incremental Learning via Spatial-Temporal Statistics Aggregation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Class-Incremental Learning (FCIL) enables Class-Incremental\nLearning (CIL) from distributed data. Existing FCIL methods typically integrate\nold knowledge preservation into local client training. However, these methods\ncannot avoid spatial-temporal client drift caused by data heterogeneity and\noften incur significant computational and communication overhead, limiting\npractical deployment. To address these challenges simultaneously, we propose a\nnovel approach, Spatial-Temporal Statistics Aggregation (STSA), which provides\na unified framework to aggregate feature statistics both spatially (across\nclients) and temporally (across stages). The aggregated feature statistics are\nunaffected by data heterogeneity and can be used to update the classifier in\nclosed form at each stage. Additionally, we introduce STSA-E, a\ncommunication-efficient variant with theoretical guarantees, achieving similar\nperformance to STSA-E with much lower communication overhead. Extensive\nexperiments on three widely used FCIL datasets, with varying degrees of data\nheterogeneity, show that our method outperforms state-of-the-art FCIL methods\nin terms of performance, flexibility, and both communication and computation\nefficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5STSA\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\u548c\u8ba1\u7b97\u901a\u4fe1\u5f00\u9500\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u907f\u514d\u6570\u636e\u5f02\u8d28\u6027\u5bfc\u81f4\u7684\u7a7a\u95f4-\u65f6\u95f4\u6f02\u79fb\uff0c\u4e14\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u5927\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u7a7a\u95f4-\u65f6\u95f4\u7edf\u8ba1\u805a\u5408\uff08STSA\uff09\u6846\u67b6\uff0c\u7edf\u4e00\u805a\u5408\u7279\u5f81\u7edf\u8ba1\u4fe1\u606f\uff1b\u5e76\u8bbe\u8ba1\u4e86\u901a\u4fe1\u9ad8\u6548\u7248\u672cSTSA-E\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u7075\u6d3b\u6027\u548c\u8ba1\u7b97\u901a\u4fe1\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "STSA\u548cSTSA-E\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5f02\u8d28\u6027\u548c\u5f00\u9500\u95ee\u9898\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60,\u7c7b\u589e\u91cf\u5b66\u4e60,\u6570\u636e\u5f02\u8d28\u6027,\u901a\u4fe1\u6548\u7387"}}
{"id": "2506.01329", "pdf": "https://arxiv.org/pdf/2506.01329", "abs": "https://arxiv.org/abs/2506.01329", "authors": ["Guifeng Deng", "Shuyin Rao", "Tianyu Lin", "Anlu Dai", "Pan Wang", "Junyi Xie", "Haidong Song", "Ke Zhao", "Dongwu Xu", "Zhengdong Cheng", "Tao Li", "Haiteng Jiang"], "title": "Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines", "categories": ["cs.CL", "cs.AI"], "comment": "30 pages, 8 figures", "summary": "Psychological support hotlines are critical for crisis intervention but face\nsignificant challenges due to rising demand. Large language models (LLMs) could\nsupport crisis assessments, yet their capabilities in emotionally sensitive\ncontexts remain unclear. We introduce PsyCrisisBench, a benchmark of 540\nannotated transcripts from the Hangzhou Psychological Assistance Hotline,\nassessing four tasks: mood status recognition, suicidal ideation detection,\nsuicide plan identification, and risk assessment. We evaluated 64 LLMs across\n15 families (e.g., GPT, Claude, Gemini, Llama, Qwen, DeepSeek) using zero-shot,\nfew-shot, and fine-tuning paradigms. Performance was measured by F1-score, with\nstatistical comparisons via Welch's t-tests. LLMs performed strongly on\nsuicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779),\nand risk assessment (F1=0.907), improved with few-shot and fine-tuning. Mood\nstatus recognition was more challenging (max F1=0.709), likely due to lost\nvocal cues and ambiguity. A fine-tuned 1.5B-parameter model (Qwen2.5-1.5B)\nsurpassed larger models on mood and suicidal ideation. Open-source models like\nQwQ-32B performed comparably to closed-source on most tasks (p>0.3), though\nclosed models retained an edge in mood detection (p=0.007). Performance scaled\nwith size up to a point; quantization (AWQ) reduced GPU memory by 70% with\nminimal F1 degradation. LLMs show substantial promise in structured\npsychological crisis assessments, especially with fine-tuning. Mood recognition\nremains limited due to contextual complexity. The narrowing gap between open-\nand closed-source models, combined with efficient quantization, suggests\nfeasible integration. PsyCrisisBench offers a robust evaluation framework to\nguide model development and ethical deployment in mental health.", "AI": {"tldr": "\u63d0\u51fa\u4e86PsyCrisisBench\u57fa\u51c6\uff0c\u8bc4\u4f30LLM\u5728\u5fc3\u7406\u5371\u673a\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u81ea\u6740\u503e\u5411\u548c\u98ce\u9669\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u60c5\u7eea\u8bc6\u522b\u4ecd\u5177\u6311\u6218\u6027\u3002", "motivation": "\u89e3\u51b3\u5fc3\u7406\u652f\u6301\u70ed\u7ebf\u9700\u6c42\u589e\u52a0\u4e0eLLM\u5728\u60c5\u611f\u654f\u611f\u573a\u666f\u4e2d\u80fd\u529b\u4e0d\u660e\u786e\u7684\u95ee\u9898\uff0c\u63a2\u7d22LLM\u5728\u5fc3\u7406\u5371\u673a\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528PsyCrisisBench\uff08540\u4efd\u6807\u6ce8\u6587\u672c\uff09\u8bc4\u4f3064\u4e2aLLM\uff0c\u6db5\u76d6\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u5fae\u8c03\u4e09\u79cd\u6a21\u5f0f\uff0c\u901a\u8fc7F1\u5206\u6570\u548c\u7edf\u8ba1\u68c0\u9a8c\u8861\u91cf\u6027\u80fd\u3002", "result": "LLM\u5728\u81ea\u6740\u503e\u5411\u68c0\u6d4b\uff08F1=0.880\uff09\u548c\u98ce\u9669\u8bc4\u4f30\uff08F1=0.907\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u60c5\u7eea\u8bc6\u522b\u8f83\u96be\uff08F1=0.709\uff09\u3002\u5f00\u6e90\u6a21\u578b\u4e0e\u95ed\u6e90\u6a21\u578b\u5dee\u8ddd\u7f29\u5c0f\u3002", "conclusion": "LLM\u5728\u7ed3\u6784\u5316\u5fc3\u7406\u5371\u673a\u8bc4\u4f30\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5c24\u5176\u9002\u5408\u5fae\u8c03\u4efb\u52a1\uff1b\u60c5\u7eea\u8bc6\u522b\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u5f00\u6e90\u6a21\u578b\u548c\u91cf\u5316\u6280\u672f\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u53ef\u884c\u6027\u3002", "keywords": "PsyCrisisBench, LLM, \u5fc3\u7406\u5371\u673a\u8bc4\u4f30, \u81ea\u6740\u503e\u5411\u68c0\u6d4b, \u60c5\u7eea\u8bc6\u522b"}}
{"id": "2506.01337", "pdf": "https://arxiv.org/pdf/2506.01337", "abs": "https://arxiv.org/abs/2506.01337", "authors": ["Zeming Li", "Xiangyue Liu", "Xiangyu Zhang", "Ping Tan", "Heung-Yeung Shum"], "title": "NoiseAR: AutoRegressing Initial Noise Prior for Diffusion Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diffusion models have emerged as powerful generative frameworks, creating\ndata samples by progressively denoising an initial random state. Traditionally,\nthis initial state is sampled from a simple, fixed distribution like isotropic\nGaussian, inherently lacking structure and a direct mechanism for external\ncontrol. While recent efforts have explored ways to introduce controllability\ninto the diffusion process, particularly at the initialization stage, they\noften rely on deterministic or heuristic approaches. These methods can be\nsuboptimal, lack expressiveness, and are difficult to scale or integrate into\nmore sophisticated optimization frameworks. In this paper, we introduce\nNoiseAR, a novel method for AutoRegressive Initial Noise Prior for Diffusion\nModels. Instead of a static, unstructured source, NoiseAR learns to generate a\ndynamic and controllable prior distribution for the initial noise. We formulate\nthe generation of the initial noise prior's parameters as an autoregressive\nprobabilistic modeling task over spatial patches or tokens. This approach\nenables NoiseAR to capture complex spatial dependencies and introduce learned\nstructure into the initial state. Crucially, NoiseAR is designed to be\nconditional, allowing text prompts to directly influence the learned prior,\nthereby achieving fine-grained control over the diffusion initialization. Our\nexperiments demonstrate that NoiseAR can generate initial noise priors that\nlead to improved sample quality and enhanced consistency with conditional\ninputs, offering a powerful, learned alternative to traditional random\ninitialization. A key advantage of NoiseAR is its probabilistic formulation,\nwhich naturally supports seamless integration into probabilistic frameworks\nlike Markov Decision Processes and Reinforcement Learning. Our code will be\navailable at https://github.com/HKUST-SAIL/NoiseAR/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNoiseAR\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6269\u6563\u6a21\u578b\u7684\u81ea\u52a8\u56de\u5f52\u521d\u59cb\u566a\u58f0\u5148\u9a8c\uff0c\u901a\u8fc7\u5b66\u4e60\u751f\u6210\u52a8\u6001\u4e14\u53ef\u63a7\u7684\u521d\u59cb\u566a\u58f0\u5206\u5e03\uff0c\u63d0\u5347\u6837\u672c\u8d28\u91cf\u548c\u6761\u4ef6\u8f93\u5165\u7684\u8fde\u8d2f\u6027\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u6a21\u578b\u7684\u521d\u59cb\u72b6\u6001\u901a\u5e38\u4ece\u56fa\u5b9a\u7684\u7b80\u5355\u5206\u5e03\u4e2d\u91c7\u6837\uff0c\u7f3a\u4e4f\u7ed3\u6784\u548c\u5916\u90e8\u63a7\u5236\u673a\u5236\u3002NoiseAR\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u548c\u53ef\u63a7\u7684\u5148\u9a8c\u5206\u5e03\u6539\u8fdb\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "NoiseAR\u5c06\u521d\u59cb\u566a\u58f0\u5148\u9a8c\u7684\u53c2\u6570\u751f\u6210\u4efb\u52a1\u5efa\u6a21\u4e3a\u7a7a\u95f4\u5757\u6216\u4ee4\u724c\u4e0a\u7684\u81ea\u56de\u5f52\u6982\u7387\u6a21\u578b\uff0c\u4ece\u800c\u6355\u83b7\u590d\u6742\u7684\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5141\u8bb8\u6761\u4ef6\u8f93\u5165\uff08\u5982\u6587\u672c\u63d0\u793a\uff09\u76f4\u63a5\u5f71\u54cd\u5148\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNoiseAR\u751f\u6210\u7684\u521d\u59cb\u566a\u58f0\u5148\u9a8c\u80fd\u663e\u8457\u63d0\u5347\u6837\u672c\u8d28\u91cf\uff0c\u5e76\u589e\u5f3a\u4e0e\u6761\u4ef6\u8f93\u5165\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4e5f\u652f\u6301\u4e0e\u5176\u4ed6\u6982\u7387\u6846\u67b6\uff08\u5982MDP\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "NoiseAR\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u63a7\u7684\u6269\u6563\u6a21\u578b\u521d\u59cb\u566a\u58f0\u751f\u6210\u65b9\u6cd5\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u521d\u59cb\u5316\u9636\u6bb5\u5f15\u5165\u4e86\u5b66\u4e60\u548c\u52a8\u6001\u7ed3\u6784\u3002", "keywords": "\u6269\u6563\u6a21\u578b, \u521d\u59cb\u566a\u58f0\u5148\u9a8c, \u81ea\u56de\u5f52\u6a21\u578b, \u6761\u4ef6\u751f\u6210, \u53ef\u63a7\u751f\u6210"}}
{"id": "2506.01334", "pdf": "https://arxiv.org/pdf/2506.01334", "abs": "https://arxiv.org/abs/2506.01334", "authors": ["Yiwen Jiang", "Deval Mehta", "Wei Feng", "Zongyuan Ge"], "title": "Enhancing Interpretable Image Classification Through LLM Agents and Conditional Concept Bottleneck Models", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Main)", "summary": "Concept Bottleneck Models (CBMs) decompose image classification into a\nprocess governed by interpretable, human-readable concepts. Recent advances in\nCBMs have used Large Language Models (LLMs) to generate candidate concepts.\nHowever, a critical question remains: What is the optimal number of concepts to\nuse? Current concept banks suffer from redundancy or insufficient coverage. To\naddress this issue, we introduce a dynamic, agent-based approach that adjusts\nthe concept bank in response to environmental feedback, optimizing the number\nof concepts for sufficiency yet concise coverage. Moreover, we propose\nConditional Concept Bottleneck Models (CoCoBMs) to overcome the limitations in\ntraditional CBMs' concept scoring mechanisms. It enhances the accuracy of\nassessing each concept's contribution to classification tasks and feature an\neditable matrix that allows LLMs to correct concept scores that conflict with\ntheir internal knowledge. Our evaluations across 6 datasets show that our\nmethod not only improves classification accuracy by 6% but also enhances\ninterpretability assessments by 30%.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u6982\u5ff5\u6570\u91cf\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u6761\u4ef6\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CoCoBMs\uff09\u4ee5\u4f18\u5316\u5206\u7c7b\u4efb\u52a1\u7684\u6982\u5ff5\u8bc4\u5206\u673a\u5236\uff0c\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524d\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\u5e93\u5b58\u5728\u5197\u4f59\u6216\u8986\u76d6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e14\u4f20\u7edf\u6982\u5ff5\u8bc4\u5206\u673a\u5236\u6709\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u52a8\u6001\u4f18\u5316\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4ee3\u7406\u7684\u52a8\u6001\u65b9\u6cd5\u8c03\u6574\u6982\u5ff5\u5e93\uff0c\u63d0\u51fa\u6761\u4ef6\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CoCoBMs\uff09\u6539\u8fdb\u6982\u5ff5\u8bc4\u5206\uff0c\u5e76\u901a\u8fc7\u53ef\u7f16\u8f91\u77e9\u9635\u5141\u8bb8LLMs\u4fee\u6b63\u51b2\u7a81\u7684\u6982\u5ff5\u8bc4\u5206\u3002", "result": "\u57286\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5c06\u5206\u7c7b\u51c6\u786e\u6027\u63d0\u9ad8\u4e866%\uff0c\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u63d0\u5347\u4e8630%\u3002", "conclusion": "\u52a8\u6001\u6982\u5ff5\u5e93\u548cCoCoBMs\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfCBM\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u89e3\u91ca\u6027\u3002", "keywords": "\u6982\u5ff5\u74f6\u9888\u6a21\u578b, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u52a8\u6001\u6982\u5ff5\u5e93, \u6761\u4ef6\u6982\u5ff5\u74f6\u9888\u6a21\u578b, \u53ef\u89e3\u91ca\u6027"}}
{"id": "2506.00335", "pdf": "https://arxiv.org/pdf/2506.00335", "abs": "https://arxiv.org/abs/2506.00335", "authors": ["Jingyang He", "Shuai Wang", "Ang Li"], "title": "Recover Experimental Data with Selection Bias using Counterfactual Logic", "categories": ["stat.ME", "cs.AI"], "comment": null, "summary": "Selection bias, arising from the systematic inclusion or exclusion of certain\nsamples, poses a significant challenge to the validity of causal inference.\nWhile Bareinboim et al. introduced methods for recovering unbiased\nobservational and interventional distributions from biased data using partial\nexternal information, the complexity of the backdoor adjustment and the\nmethod's strong reliance on observational data limit its applicability in many\npractical settings. In this paper, we formally discover the recoverability of\n$P(Y^*_{x^*})$ under selection bias with experimental data. By explicitly\nconstructing counterfactual worlds via Structural Causal Models (SCMs), we\nanalyze how selection mechanisms in the observational world propagate to the\ncounterfactual domain. We derive a complete set of graphical and theoretical\ncriteria to determine that the experimental distribution remain unaffected by\nselection bias. Furthermore, we propose principled methods for leveraging\npartially unbiased observational data to recover $P(Y^*_{x^*})$ from biased\nexperimental datasets. Simulation studies replicating realistic research\nscenarios demonstrate the practical utility of our approach, offering concrete\nguidance for mitigating selection bias in applied causal inference.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u5b58\u5728\u9009\u62e9\u504f\u5dee\u7684\u60c5\u51b5\u4e0b\u5982\u4f55\u5229\u7528\u5b9e\u9a8c\u6570\u636e\u6062\u590d\u65e0\u504f\u7684\u56e0\u679c\u63a8\u65ad\u5206\u5e03\uff0c\u63d0\u51fa\u4e86\u901a\u8fc7\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u6784\u5efa\u53cd\u4e8b\u5b9e\u4e16\u754c\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u56fe\u5f62\u548c\u7406\u8bba\u51c6\u5219\u6765\u9a8c\u8bc1\u5b9e\u9a8c\u5206\u5e03\u4e0d\u53d7\u9009\u62e9\u504f\u5dee\u5f71\u54cd\u3002", "motivation": "\u9009\u62e9\u504f\u5dee\u4f1a\u4e25\u91cd\u5f71\u54cd\u56e0\u679c\u63a8\u65ad\u7684\u6709\u6548\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u89c2\u6d4b\u6570\u636e\u4e14\u590d\u6742\u5ea6\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6570\u636e\u6062\u590d\u65e0\u504f\u5206\u5e03\u3002", "method": "\u4f7f\u7528\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCMs\uff09\u660e\u786e\u6784\u5efa\u53cd\u4e8b\u5b9e\u4e16\u754c\uff0c\u5206\u6790\u9009\u62e9\u673a\u5236\u5982\u4f55\u4f20\u64ad\uff0c\u5e76\u63d0\u51fa\u4e86\u56fe\u5f62\u548c\u7406\u8bba\u51c6\u5219\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u5229\u7528\u90e8\u5206\u65e0\u504f\u89c2\u6d4b\u6570\u636e\u7684\u65b9\u6cd5\u3002", "result": "\u6a21\u62df\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u73b0\u5b9e\u7814\u7a76\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u7528\u6027\uff0c\u4e3a\u51cf\u5c11\u5e94\u7528\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u9009\u62e9\u504f\u5dee\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u5b58\u5728\u9009\u62e9\u504f\u5dee\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u65b9\u6cd5\u652f\u6301\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "keywords": "\u9009\u62e9\u504f\u5dee, \u56e0\u679c\u63a8\u65ad, \u7ed3\u6784\u56e0\u679c\u6a21\u578b, \u53cd\u4e8b\u5b9e\u4e16\u754c, \u5b9e\u9a8c\u6570\u636e"}}
{"id": "2506.01339", "pdf": "https://arxiv.org/pdf/2506.01339", "abs": "https://arxiv.org/abs/2506.01339", "authors": ["Changsheng Wang", "Yihua Zhang", "Jinghan Jia", "Parikshit Ram", "Dennis Wei", "Yuguang Yao", "Soumyadeep Pal", "Nathalie Baracaldo", "Sijia Liu"], "title": "Invariance Makes LLM Unlearning Resilient Even to Unanticipated Downstream Fine-Tuning", "categories": ["cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Machine unlearning offers a promising solution to privacy and safety concerns\nin large language models (LLMs) by selectively removing targeted knowledge\nwhile preserving utility. However, current methods are highly sensitive to\ndownstream fine-tuning, which can quickly recover forgotten information-even\nfrom unrelated tasks. To address this, we introduce invariance into unlearning\nfor the first time, inspired by invariant risk minimization (IRM). Building on\nthis principle, we propose invariant LLM unlearning (ILU), a\nregularization-based framework that enhances robustness. Notably, ILU\ngeneralizes well to diverse fine-tuning tasks, even when trained using a single\ndataset. A task vector analysis is also provided to further elucidate the\nrationale behind ILU's effectiveness. Extensive experiments on the WMDP and\nMUSE benchmark, reveal that ILU significantly outperforms state-of-the-art\nunlearning methods, including negative preference optimization (NPO) and\nrepresentation misdirection for unlearning (RMU). Notably, ILU achieves\nsuperior unlearning robustness across diverse downstream fine-tuning scenarios\n(e.g., math, paraphrase detection, and sentiment analysis) while preserving the\nfine-tuning performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u53d8\u98ce\u9669\u6700\u5c0f\u5316\uff08IRM\uff09\u7684\u65b0\u65b9\u6cd5ILU\uff0c\u7528\u4e8e\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u201c\u9057\u5fd8\u201d\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u9762\u5bf9\u4e0b\u6e38\u5fae\u8c03\u65f6\u66f4\u9c81\u68d2\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u9057\u5fd8\u65b9\u6cd5\u5bf9\u4e0b\u6e38\u5fae\u8c03\u9ad8\u5ea6\u654f\u611f\u7684\u95ee\u9898\uff0c\u907f\u514d\u9057\u5fd8\u7684\u77e5\u8bc6\u88ab\u5feb\u901f\u6062\u590d\u3002", "method": "\u63d0\u51faILU\u6846\u67b6\uff0c\u5229\u7528\u4e0d\u53d8\u98ce\u9669\u6700\u5c0f\u5316\uff08IRM\uff09\u5f15\u5165\u4e0d\u53d8\u6027\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u589e\u5f3a\u9057\u5fd8\u7684\u9c81\u68d2\u6027\u3002", "result": "ILU\u5728WMDP\u548cMUSE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8eNPO\u548cRMU\u7b49\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "ILU\u901a\u8fc7\u5f15\u5165\u4e0d\u53d8\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u9057\u5fd8\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "keywords": "\u673a\u5668\u9057\u5fd8\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u4e0d\u53d8\u98ce\u9669\u6700\u5c0f\u5316\u3001\u4e0b\u6e38\u5fae\u8c03\u3001\u9c81\u68d2\u6027"}}
{"id": "2506.01340", "pdf": "https://arxiv.org/pdf/2506.01340", "abs": "https://arxiv.org/abs/2506.01340", "authors": ["Shahad Al-Khalifa", "Nadir Durrani", "Hend Al-Khalifa", "Firoj Alam"], "title": "The Landscape of Arabic Large Language Models (ALLMs): A New Era for Arabic Language Technology", "categories": ["cs.CL"], "comment": "Accepted at CACM", "summary": "The emergence of ChatGPT marked a transformative milestone for Artificial\nIntelligence (AI), showcasing the remarkable potential of Large Language Models\n(LLMs) to generate human-like text. This wave of innovation has revolutionized\nhow we interact with technology, seamlessly integrating LLMs into everyday\ntasks such as vacation planning, email drafting, and content creation. While\nEnglish-speaking users have significantly benefited from these advancements,\nthe Arabic world faces distinct challenges in developing Arabic-specific LLMs.\nArabic, one of the languages spoken most widely around the world, serves more\nthan 422 million native speakers in 27 countries and is deeply rooted in a rich\nlinguistic and cultural heritage. Developing Arabic LLMs (ALLMs) presents an\nunparalleled opportunity to bridge technological gaps and empower communities.\nThe journey of ALLMs has been both fascinating and complex, evolving from\nrudimentary text processing systems to sophisticated AI-driven models. This\narticle explores the trajectory of ALLMs, from their inception to the present\nday, highlighting the efforts to evaluate these models through benchmarks and\npublic leaderboards. We also discuss the challenges and opportunities that\nALLMs present for the Arab world.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u963f\u62c9\u4f2f\u8bed\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08ALLMs\uff09\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u4ece\u57fa\u7840\u6587\u672c\u5904\u7406\u7cfb\u7edf\u5230\u590d\u6742\u7684AI\u9a71\u52a8\u6a21\u578b\uff0c\u5e76\u8ba8\u8bba\u4e86\u963f\u62c9\u4f2f\u4e16\u754c\u9762\u4e34\u7684\u6311\u6218\u4e0e\u673a\u9047\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u4f5c\u4e3a\u5168\u7403\u5e7f\u6cdb\u4f7f\u7528\u7684\u8bed\u8a00\u4e4b\u4e00\uff0c\u62e5\u6709\u8d85\u8fc74.22\u4ebf\u6bcd\u8bed\u4f7f\u7528\u8005\uff0c\u4f46\u963f\u62c9\u4f2f\u8bed\u7279\u5b9a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53d1\u5c55\u9762\u4e34\u72ec\u7279\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22ALLMs\u7684\u53d1\u5c55\u8f68\u8ff9\u53ca\u5176\u5bf9\u963f\u62c9\u4f2f\u4e16\u754c\u7684\u5f71\u54cd\u3002", "method": "\u6587\u7ae0\u56de\u987e\u4e86ALLMs\u4ece\u65e9\u671f\u6587\u672c\u5904\u7406\u7cfb\u7edf\u5230\u73b0\u4ee3AI\u9a71\u52a8\u6a21\u578b\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u548c\u516c\u5171\u6392\u884c\u699c\u5bf9\u5176\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cALLMs\u4e3a\u963f\u62c9\u4f2f\u4e16\u754c\u63d0\u4f9b\u4e86\u586b\u8865\u6280\u672f\u5dee\u8ddd\u548c\u589e\u5f3a\u793e\u533a\u80fd\u529b\u7684\u673a\u4f1a\uff0c\u4f46\u540c\u65f6\u4e5f\u9762\u4e34\u8bf8\u591a\u6311\u6218\u3002", "conclusion": "ALLMs\u7684\u53d1\u5c55\u5bf9\u963f\u62c9\u4f2f\u4e16\u754c\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u4ecd\u9700\u514b\u670d\u8bed\u8a00\u548c\u6587\u5316\u7684\u590d\u6742\u6027\u4ee5\u5b9e\u73b0\u5176\u6f5c\u529b\u3002", "keywords": "\u963f\u62c9\u4f2f\u8bed\u5927\u578b\u8bed\u8a00\u6a21\u578b, LLMs, \u4eba\u5de5\u667a\u80fd, \u8bed\u8a00\u6587\u5316, \u6280\u672f\u5dee\u8ddd"}}
{"id": "2506.01348", "pdf": "https://arxiv.org/pdf/2506.01348", "abs": "https://arxiv.org/abs/2506.01348", "authors": ["Yeping Jin", "Lauren Wise", "Ioannis Paschalidis"], "title": "Distributionally Robust Learning in Survival Analysis", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "We introduce an innovative approach that incorporates a Distributionally\nRobust Learning (DRL) approach into Cox regression to enhance the robustness\nand accuracy of survival predictions. By formulating a DRL framework with a\nWasserstein distance-based ambiguity set, we develop a variant Cox model that\nis less sensitive to assumptions about the underlying data distribution and\nmore resilient to model misspecification and data perturbations. By leveraging\nWasserstein duality, we reformulate the original min-max DRL problem into a\ntractable regularized empirical risk minimization problem, which can be\ncomputed by exponential conic programming. We provide guarantees on the finite\nsample behavior of our DRL-Cox model. Moreover, through extensive simulations\nand real world case studies, we demonstrate that our regression model achieves\nsuperior performance in terms of prediction accuracy and robustness compared\nwith traditional methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5206\u5e03\u9c81\u68d2\u5b66\u4e60(DRL)\u7684Cox\u56de\u5f52\u65b9\u6cd5\uff0c\u589e\u5f3a\u751f\u5b58\u9884\u6d4b\u7684\u7a33\u5065\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edfCox\u56de\u5f52\u5bf9\u6570\u636e\u5206\u5e03\u7684\u5047\u8bbe\u654f\u611f\uff0c\u4e14\u6613\u53d7\u6a21\u578b\u8bef\u8bbe\u548c\u6570\u636e\u6270\u52a8\u5f71\u54cd\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u5229\u7528Wasserstein\u8ddd\u79bb\u6784\u5efaDRL\u6846\u67b6\uff0c\u5e76\u8f6c\u5316\u4e3a\u53ef\u5904\u7406\u7684\u6b63\u5219\u5316\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u6848\u4f8b\u7814\u7a76\uff0c\u8bc1\u660e\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u7a33\u5065\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684DRL-Cox\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u751f\u5b58\u9884\u6d4b\u7684\u7a33\u5065\u6027\u548c\u51c6\u786e\u6027\u3002", "keywords": "Cox\u56de\u5f52,\u5206\u5e03\u9c81\u68d2\u5b66\u4e60,Wasserstein\u8ddd\u79bb,\u751f\u5b58\u9884\u6d4b"}}
{"id": "2506.01341", "pdf": "https://arxiv.org/pdf/2506.01341", "abs": "https://arxiv.org/abs/2506.01341", "authors": ["Yiran Zhang", "Mo Wang", "Xiaoyang Li", "Kaixuan Ren", "Chencheng Zhu", "Usman Naseem"], "title": "TurnBench-MS: A Benchmark for Evaluating Multi-Turn, Multi-Step Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Despite impressive advances in large language models (LLMs), existing\nbenchmarks often focus on single-turn or single-step tasks, failing to capture\nthe kind of iterative reasoning required in real-world settings. To address\nthis limitation, we introduce TurnBench, a novel benchmark that evaluates\nmulti-turn, multi-step reasoning through an interactive code-breaking task\ninspired by a \"Turing Machine Board Game.\" In each episode, a model must\nuncover hidden logical or arithmetic rules by making sequential guesses,\nreceiving structured feedback, and integrating clues across multiple rounds.\nThis dynamic setup requires models to reason over time, adapt based on past\ninformation, and maintain consistency across steps-capabilities underexplored\nin current benchmarks. TurnBench includes two modes: Classic, which tests\nstandard reasoning, and Nightmare, which introduces increased complexity and\nrequires robust inferential chains. To support fine-grained analysis, we\nprovide ground-truth annotations for intermediate reasoning steps. Our\nevaluation of state-of-the-art LLMs reveals significant gaps: the best model\nachieves 81.5% accuracy in Classic mode, but performance drops to 17.8% in\nNightmare mode. In contrast, human participants achieve 100% in both,\nunderscoring the challenge TurnBench poses to current models. By incorporating\nfeedback loops and hiding task rules, TurnBench reduces contamination risks and\nprovides a rigorous testbed for diagnosing and advancing multi-step, multi-turn\nreasoning in LLMs.", "AI": {"tldr": "TurnBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u591a\u8f6e\u3001\u591a\u6b65\u63a8\u7406\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u4ee3\u7801\u7834\u89e3\u4efb\u52a1\u6d4b\u8bd5\u6a21\u578b\u7684\u52a8\u6001\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u679c\u53d1\u73b0LLMs\u5728\u590d\u6742\u60c5\u5883\u4e0b\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u591a\u4e3a\u5355\u8f6e\u6216\u5355\u6b65\u4efb\u52a1\uff0c\u65e0\u6cd5\u771f\u5b9e\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u4e2d\u9700\u8981\u8fed\u4ee3\u63a8\u7406\u7684\u573a\u666f\u3002", "method": "\u63d0\u51faTurnBench\uff0c\u5305\u542bClassic\u548cNightmare\u4e24\u79cd\u6a21\u5f0f\uff0c\u901a\u8fc7\u4ee3\u7801\u7834\u89e3\u4efb\u52a1\u6d4b\u8bd5\u6a21\u578b\u7684\u591a\u8f6e\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e2d\u95f4\u6b65\u9aa4\u6807\u6ce8\u3002", "result": "\u6700\u4f73LLM\u5728Classic\u6a21\u5f0f\u4e0b\u51c6\u786e\u7387\u4e3a81.5%\uff0c\u4f46\u5728Nightmare\u6a21\u5f0f\u4e0b\u964d\u81f317.8%\uff1b\u4eba\u7c7b\u5219\u5747\u8fbe\u5230100%\u3002", "conclusion": "TurnBench\u63ed\u793a\u4e86LLMs\u5728\u591a\u8f6e\u63a8\u7406\u4e2d\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "keywords": "LLMs,\u591a\u8f6e\u63a8\u7406,\u57fa\u51c6\u6d4b\u8bd5,\u4ee3\u7801\u7834\u89e3,\u52a8\u6001\u63a8\u7406"}}
{"id": "2506.00348", "pdf": "https://arxiv.org/pdf/2506.00348", "abs": "https://arxiv.org/abs/2506.00348", "authors": ["Shivam Shorewala", "Zihao Yang"], "title": "Beyond Winning: Margin of Victory Relative to Expectation Unlocks Accurate Skill Ratings", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "Knowledge of accurate relative skills in any competitive system is essential,\nbut foundational approaches such as ELO discard extremely relevant performance\ndata by concentrating exclusively on binary outcomes. While margin of victory\n(MOV) extensions exist, they often lack a definitive method for incorporating\nthis information. We introduce Margin of Victory Differential Analysis (MOVDA),\na framework that enhances traditional rating systems by using the deviation\nbetween the true MOV and a $\\textit{modeled expectation}$. MOVDA learns a\ndomain-specific, non-linear function (a scaled hyperbolic tangent that captures\nsaturation effects and home advantage) to predict expected MOV based on rating\ndifferentials. Crucially, the $\\textit{difference}$ between the true and\nexpected MOV provides a subtle and weighted signal for rating updates,\nhighlighting informative deviations in all levels of contests. Extensive\nexperiments on professional NBA basketball data (from 2013 to 2023, with 13,619\ngames) show that MOVDA significantly outperforms standard ELO and Bayesian\nbaselines. MOVDA reduces Brier score prediction error by $1.54\\%$ compared to\nTrueSkill, increases outcome accuracy by $0.58\\%$, and most importantly\naccelerates rating convergence by $13.5\\%$, while maintaining the computational\nefficiency of the original ELO updates. MOVDA offers a theoretically motivated,\nempirically superior, and computationally lean approach to integrating\nperformance magnitude into skill rating for competitive environments like the\nNBA.", "AI": {"tldr": "MOVDA\u662f\u4e00\u79cd\u65b0\u7684\u8bc4\u5206\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u771f\u5b9e\u6bd4\u5206\u5dee\u4e0e\u6a21\u578b\u9884\u6d4b\u5dee\u503c\u7684\u5dee\u5f02\u6765\u63d0\u5347\u4f20\u7edf\u8bc4\u5206\u7cfb\u7edf\uff0c\u663e\u8457\u4f18\u4e8eELO\u548c\u8d1d\u53f6\u65af\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edf\u8bc4\u5206\u7cfb\u7edf\uff08\u5982ELO\uff09\u4ec5\u5173\u6ce8\u4e8c\u5143\u7ed3\u679c\uff0c\u5ffd\u7565\u4e86\u6bd4\u5206\u5dee\u7684\u4e30\u5bcc\u4fe1\u606f\u3002MOVDA\u65e8\u5728\u901a\u8fc7\u6bd4\u5206\u5dee\u5206\u6790\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u7ade\u4e89\u7cfb\u7edf\u4e2d\u7684\u6280\u80fd\u3002", "method": "MOVDA\u5f15\u5165\u4e00\u4e2a\u9886\u57df\u7279\u5b9a\u7684\u975e\u7ebf\u6027\u51fd\u6570\uff08\u7f29\u653e\u53cc\u66f2\u6b63\u5207\uff09\u9884\u6d4b\u9884\u671f\u6bd4\u5206\u5dee\uff0c\u5e76\u5229\u7528\u771f\u5b9e\u4e0e\u9884\u671f\u5dee\u503c\u4f5c\u4e3a\u8bc4\u5206\u66f4\u65b0\u7684\u4fe1\u53f7\u3002", "result": "\u5728NBA\u6570\u636e\uff082013-2023\u5e74\uff0c13,619\u573a\u6bd4\u8d5b\uff09\u4e2d\uff0cMOVDA\u6bd4TrueSkill\u964d\u4f4eBrier\u5206\u6570\u8bef\u5dee1.54%\uff0c\u63d0\u9ad8\u7ed3\u679c\u51c6\u786e\u73870.58%\uff0c\u52a0\u901f\u8bc4\u5206\u6536\u655b13.5%\u3002", "conclusion": "MOVDA\u7406\u8bba\u5408\u7406\u3001\u5b9e\u8bc1\u4f18\u8d8a\u4e14\u8ba1\u7b97\u9ad8\u6548\uff0c\u4e3aNBA\u7b49\u7ade\u4e89\u73af\u5883\u63d0\u4f9b\u4e86\u6bd4\u5206\u5dee\u4e0e\u6280\u80fd\u8bc4\u5206\u7684\u5148\u8fdb\u6574\u5408\u65b9\u6cd5\u3002", "keywords": "\u6280\u80fd\u8bc4\u5206,MOVDA,\u6bd4\u5206\u5dee\u5206\u6790,NBA,ELO,TrueSkill"}}
{"id": "2506.01350", "pdf": "https://arxiv.org/pdf/2506.01350", "abs": "https://arxiv.org/abs/2506.01350", "authors": ["Taisuke Kobayashi", "Shingo Murata"], "title": "Variational Adaptive Noise and Dropout towards Stable Recurrent Neural Networks", "categories": ["cs.LG", "cs.RO"], "comment": "6 pages, 6 figures (accepted in ICDL2025)", "summary": "This paper proposes a novel stable learning theory for recurrent neural\nnetworks (RNNs), so-called variational adaptive noise and dropout (VAND). As\nstabilizing factors for RNNs, noise and dropout on the internal state of RNNs\nhave been separately confirmed in previous studies. We reinterpret the\noptimization problem of RNNs as variational inference, showing that noise and\ndropout can be derived simultaneously by transforming the explicit\nregularization term arising in the optimization problem into implicit\nregularization. Their scale and ratio can also be adjusted appropriately to\noptimize the main objective of RNNs, respectively. In an imitation learning\nscenario with a mobile manipulator, only VAND is able to imitate sequential and\nperiodic behaviors as instructed. https://youtu.be/UOho3Xr6A2w", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVAND\u7684\u65b0\u578b\u7a33\u5b9a\u5b66\u4e60\u7406\u8bba\uff0c\u7528\u4e8e\u589e\u5f3aRNN\u7684\u7a33\u5b9a\u6027\uff0c\u5c06\u566a\u58f0\u548cDropout\u7edf\u4e00\u4e3a\u9690\u5f0f\u6b63\u5219\u5316\u3002", "motivation": "\u89e3\u51b3RNN\u4e2d\u566a\u58f0\u548cDropout\u4f5c\u4e3a\u72ec\u7acb\u7a33\u5b9a\u56e0\u7d20\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u5c06\u5176\u7edf\u4e00\u4e3a\u4f18\u5316\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u5c06\u663e\u5f0f\u6b63\u5219\u5316\u8f6c\u6362\u4e3a\u9690\u5f0f\u6b63\u5219\u5316\uff0c\u52a8\u6001\u8c03\u6574\u566a\u58f0\u548cDropout\u7684\u89c4\u6a21\u4e0e\u6bd4\u4f8b\u3002", "result": "\u5728\u79fb\u52a8\u673a\u68b0\u81c2\u7684\u6a21\u4eff\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0cVAND\u6210\u529f\u6a21\u4eff\u4e86\u987a\u5e8f\u548c\u5468\u671f\u6027\u884c\u4e3a\u3002", "conclusion": "VAND\u4e3aRNN\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7a33\u5b9a\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "keywords": "RNN, \u53d8\u5206\u63a8\u7406, \u566a\u58f0, Dropout, \u6a21\u4eff\u5b66\u4e60"}}
{"id": "2506.01344", "pdf": "https://arxiv.org/pdf/2506.01344", "abs": "https://arxiv.org/abs/2506.01344", "authors": ["Manan Suri", "Puneet Mathur", "Nedim Lipka", "Franck Dernoncourt", "Ryan A. Rossi", "Vivek Gupta", "Dinesh Manocha"], "title": "Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic Agents", "categories": ["cs.CL"], "comment": null, "summary": "Flowcharts are a critical tool for visualizing decision-making processes.\nHowever, their non-linear structure and complex visual-textual relationships\nmake it challenging to interpret them using LLMs, as vision-language models\nfrequently hallucinate nonexistent connections and decision paths when\nanalyzing these diagrams. This leads to compromised reliability for automated\nflowchart processing in critical domains such as logistics, health, and\nengineering. We introduce the task of Fine-grained Flowchart Attribution, which\ntraces specific components grounding a flowchart referring LLM response.\nFlowchart Attribution ensures the verifiability of LLM predictions and improves\nexplainability by linking generated responses to the flowchart's structure. We\npropose FlowPathAgent, a neurosymbolic agent that performs fine-grained post\nhoc attribution through graph-based reasoning. It first segments the flowchart,\nthen converts it into a structured symbolic graph, and then employs an agentic\napproach to dynamically interact with the graph, to generate attribution paths.\nAdditionally, we present FlowExplainBench, a novel benchmark for evaluating\nflowchart attributions across diverse styles, domains, and question types.\nExperimental results show that FlowPathAgent mitigates visual hallucinations in\nLLM answers over flowchart QA, outperforming strong baselines by 10-14% on our\nproposed FlowExplainBench dataset.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u7ec6\u7c92\u5ea6\u6d41\u7a0b\u56fe\u5f52\u56e0\u4efb\u52a1\u548cFlowPathAgent\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u6d41\u7a0b\u56fe\u89e3\u91ca\u7684\u53ef\u9760\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u89e3\u91ca\u6d41\u7a0b\u56fe\u65f6\u4ea7\u751f\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u81ea\u52a8\u5316\u5904\u7406\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faFlowPathAgent\uff0c\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u4ee3\u7406\uff0c\u901a\u8fc7\u56fe\u63a8\u7406\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5f52\u56e0\u3002", "result": "\u5728FlowExplainBench\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf10-14%\u3002", "conclusion": "FlowPathAgent\u6709\u6548\u51cf\u5c11\u4e86LLMs\u5728\u6d41\u7a0b\u56fe\u89e3\u91ca\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "keywords": "\u6d41\u7a0b\u56fe,\u5f52\u56e0\u4efb\u52a1,LLMs,\u795e\u7ecf\u7b26\u53f7\u4ee3\u7406"}}
{"id": "2506.00352", "pdf": "https://arxiv.org/pdf/2506.00352", "abs": "https://arxiv.org/abs/2506.00352", "authors": ["Chinkit Patel", "Kee Siong Ng"], "title": "Enabling Secure and Ephemeral AI Workloads in Data Mesh Environments", "categories": ["cs.DC", "cs.AI", "cs.DB"], "comment": "52 pages", "summary": "Many large enterprises that operate highly governed and complex ICT\nenvironments have no efficient and effective way to support their Data and AI\nteams in rapidly spinning up and tearing down self-service data and compute\ninfrastructure, to experiment with new data analytic tools, and deploy data\nproducts into operational use. This paper proposes a key piece of the solution\nto the overall problem, in the form of an on-demand self-service data-platform\ninfrastructure to empower de-centralised data teams to build data products on\ntop of centralised templates, policies and governance. The core innovation is\nan efficient method to leverage immutable container operating systems and\ninfrastructure-as-code methodologies for creating, from scratch, vendor-neutral\nand short-lived Kubernetes clusters on-premises and in any cloud environment.\nOur proposed approach can serve as a repeatable, portable and cost-efficient\nalternative or complement to commercial Platform-as-a-Service (PaaS) offerings,\nand this is particularly important in supporting interoperability in complex\ndata mesh environments with a mix of modern and legacy compute infrastructure.", "AI": {"tldr": "\u4e3a\u5927\u578b\u4f01\u4e1a\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u670d\u52a1\u7684\u6570\u636e\u5e73\u53f0\u57fa\u7840\u8bbe\u65bd\uff0c\u652f\u6301\u6570\u636e\u56e2\u961f\u5feb\u901f\u6784\u5efa\u548c\u9500\u6bc1\u77ed\u671fKubernetes\u96c6\u7fa4\uff0c\u4ee5\u5b9e\u9a8c\u548c\u90e8\u7f72\u6570\u636e\u4ea7\u54c1\u3002", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u5728\u590d\u6742ICT\u73af\u5883\u4e2d\u7f3a\u4e4f\u9ad8\u6548\u652f\u6301\u6570\u636e\u56e2\u961f\u5feb\u901f\u5b9e\u9a8c\u548c\u90e8\u7f72\u6570\u636e\u4ea7\u54c1\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u4e0d\u53ef\u53d8\u7684\u5bb9\u5668\u64cd\u4f5c\u7cfb\u7edf\u548c\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801\u65b9\u6cd5\uff0c\u5728\u672c\u5730\u6216\u4e91\u73af\u5883\u4e2d\u521b\u5efa\u77ed\u751f\u547d\u5468\u671f\u7684\u3001\u4f9b\u5e94\u5546\u4e2d\u7acb\u7684Kubernetes\u96c6\u7fa4\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u91cd\u590d\u3001\u4fbf\u643a\u4e14\u6210\u672c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4f5c\u4e3a\u5546\u4e1aPaaS\u7684\u66ff\u4ee3\u6216\u8865\u5145\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7279\u522b\u9002\u7528\u4e8e\u652f\u6301\u590d\u6742\u6570\u636e\u7f51\u683c\u73af\u5883\u4e2d\u7684\u4e92\u64cd\u4f5c\u6027\uff0c\u6db5\u76d6\u73b0\u4ee3\u548c\u4f20\u7edf\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u3002", "keywords": "\u81ea\u670d\u52a1\u6570\u636e\u5e73\u53f0, Kubernetes, \u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801, \u6570\u636e\u7f51\u683c"}}
{"id": "2506.01352", "pdf": "https://arxiv.org/pdf/2506.01352", "abs": "https://arxiv.org/abs/2506.01352", "authors": ["Guangxin He", "Yuan Cao", "Yutong He", "Tianyi Bai", "Kun Yuan", "Binhang Yuan"], "title": "TAH-QUANT: Effective Activation Quantization in Pipeline Parallelism over Slow Network", "categories": ["cs.LG"], "comment": null, "summary": "Decentralized training of large language models offers the opportunity to\npool computational resources across geographically distributed participants but\nfaces significant network communication bottlenecks, particularly in\npipeline-parallel settings. While pipeline parallelism partitions model layers\nacross devices to handle large-scale models, it necessitates frequent\ncommunication of intermediate activations, creating challenges when network\nbandwidth is limited. Existing activation compression methods, such as AQ-SGD,\nmitigate quantization-induced errors through error compensation but impose\nprohibitive memory overhead by requiring storage of previous activations. To\naddress these issues, we introduce TAH-Quant (Tile-wise Adaptive Hadamard\nQuantization), a novel activation quantization framework designed specifically\nfor pipeline parallelism. Our approach integrates fine-grained tile-wise\nquantization for precise control, entropy-guided token-level adaptive bit\nallocation for optimal bit usage, and a Hadamard-based transform with pivot\nelement swapping to effectively suppress quantization outliers. We further\nprovide a theoretical analysis, proving that pipeline parallel training\nequipped with TAH-Quant maintains a convergence rate of\n$\\mathcal{O}(1/\\sqrt{T})$, matching that of vanilla stochastic gradient\ndescent. Extensive experiments on diverse LLM tasks demonstrate that TAH-Quant\nachieves aggressive activation quantization (3-4 bits) ratio, which provides up\nto 4.3$\\times$ end-to-end speedup without compromising training convergence,\nmatches state-of-the-art methods, incurs no extra memory overhead, and\ngeneralizes well across different training scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTAH-Quant\u7684\u65b0\u6fc0\u6d3b\u91cf\u5316\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u89e3\u51b3\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u7f51\u7edc\u901a\u4fe1\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u5206\u5e03\u5f0f\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u9891\u7e41\u901a\u4fe1\u4e2d\u95f4\u6fc0\u6d3b\u6570\u636e\uff0c\u4f46\u7f51\u7edc\u5e26\u5bbd\u6709\u9650\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u5982AQ-SGD\u56e0\u5b58\u50a8\u5386\u53f2\u6fc0\u6d3b\u6570\u636e\u800c\u5e26\u6765\u8fc7\u9ad8\u5185\u5b58\u5f00\u9500\u3002", "method": "TAH-Quant\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5206\u5757\u91cf\u5316\u3001\u57fa\u4e8e\u71b5\u7684\u4ee4\u724c\u7ea7\u81ea\u9002\u5e94\u6bd4\u7279\u5206\u914d\u548cHadamard\u53d8\u6362\uff0c\u6709\u6548\u538b\u5236\u91cf\u5316\u5f02\u5e38\u503c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u53ef\u5b9e\u73b03-4\u4f4d\u7684\u6fc0\u8fdb\u91cf\u5316\uff0c\u7aef\u5230\u7aef\u52a0\u901f\u8fbe4.3\u500d\uff0c\u4e14\u4e0d\u5f71\u54cd\u8bad\u7ec3\u6536\u655b\u3002", "conclusion": "TAH-Quant\u4e3a\u5206\u5e03\u5f0f\u8bad\u7ec3\u63d0\u4f9b\u9ad8\u6548\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\u3002", "keywords": "\u5206\u5e03\u5f0f\u8bad\u7ec3, \u6fc0\u6d3b\u91cf\u5316, \u7ba1\u9053\u5e76\u884c, TAH-Quant, \u5927\u578b\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.01347", "pdf": "https://arxiv.org/pdf/2506.01347", "abs": "https://arxiv.org/abs/2506.01347", "authors": ["Xinyu Zhu", "Mengzhou Xia", "Zhepei Wei", "Wei-Lin Chen", "Danqi Chen", "Yu Meng"], "title": "The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach\nfor training language models (LMs) on reasoning tasks that elicit emergent long\nchains of thought (CoTs). Unlike supervised learning, it updates the model\nusing both correct and incorrect samples via policy gradients. To better\nunderstand its mechanism, we decompose the learning signal into reinforcing\ncorrect responses and penalizing incorrect ones, referred to as Positive and\nNegative Sample Reinforcement (PSR and NSR), respectively. We train\nQwen2.5-Math-7B and Qwen3-4B on a mathematical reasoning dataset and uncover a\nsurprising result: training with only negative samples -- without reinforcing\ncorrect responses -- can be highly effective: it consistently improves\nperformance over the base model across the entire Pass@$k$ spectrum ($k$ up to\n$256$), often matching or surpassing PPO and GRPO. In contrast, reinforcing\nonly correct responses improves Pass@$1$ but degrades performance at higher\n$k$, due to reduced diversity. These inference-scaling trends highlight that\nsolely penalizing incorrect responses may contribute more to performance than\npreviously recognized. Through gradient analysis, we show that NSR works by\nsuppressing incorrect generations and redistributing probability mass toward\nother plausible candidates, guided by the model's prior beliefs. It refines the\nmodel's existing knowledge rather than introducing entirely new behaviors.\nBuilding on this insight, we propose a simple variant of the RL objective that\nupweights NSR, and show that it consistently improves overall Pass@$k$\nperformance on MATH, AIME 2025, and AMC23. Our code is available at\nhttps://github.com/TianHongZXY/RLVR-Decomposed.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u4ec5\u4f7f\u7528\u8d1f\u6837\u672c\uff08\u60e9\u7f5a\u9519\u8bef\uff09\u5bf9\u8bed\u8a00\u6a21\u578b\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\u3002", "motivation": "\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u4e2d\u4e0d\u540c\u6837\u672c\uff08\u6b63\u6837\u672c\u548c\u8d1f\u6837\u672c\uff09\u5bf9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u8d1f\u6837\u672c\u5b66\u4e60\u673a\u5236\u7684\u6f5c\u5728\u4f18\u52bf\u3002", "method": "\u5206\u89e3\u5f3a\u5316\u5b66\u4e60\u4fe1\u53f7\u4e3a\u6b63\u6837\u672c\u5f3a\u5316\uff08PSR\uff09\u548c\u8d1f\u6837\u672c\u5f3a\u5316\uff08NSR\uff09\uff0c\u5e76\u5728\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\u3002\u901a\u8fc7\u68af\u5ea6\u5206\u6790\u9a8c\u8bc1NSR\u7684\u6548\u679c\u3002", "result": "\u4ec5\u4f7f\u7528\u8d1f\u6837\u672c\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u591a\u6837\u6027\u7684\u8bc4\u4f30\u4e2d\uff08Pass@k\uff0ck\u9ad8\u8fbe256\uff09\uff0c\u751a\u81f3\u4f18\u4e8ePPO\u548cGRPO\u65b9\u6cd5\u3002", "conclusion": "\u8d1f\u6837\u672c\u5f3a\u5316\u901a\u8fc7\u6291\u5236\u9519\u8bef\u751f\u6210\u548c\u91cd\u65b0\u5206\u914d\u6982\u7387\u8d28\u91cf\u6765\u4f18\u5316\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u6539\u8fdb\u7684\u5f3a\u5316\u76ee\u6807\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u679c\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u8bed\u8a00\u6a21\u578b, \u6570\u5b66\u63a8\u7406, \u8d1f\u6837\u672c\u5f3a\u5316"}}
{"id": "2506.01356", "pdf": "https://arxiv.org/pdf/2506.01356", "abs": "https://arxiv.org/abs/2506.01356", "authors": ["Haoyu Li", "Xiangru Zhong", "Bin Hu", "Huan Zhang"], "title": "Two-Stage Learning of Stabilizing Neural Controllers via Zubov Sampling and Iterative Domain Expansion", "categories": ["cs.LG", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Learning-based neural network (NN) control policies have shown impressive\nempirical performance. However, obtaining stability guarantees and estimations\nof the region of attraction of these learned neural controllers is challenging\ndue to the lack of stable and scalable training and verification algorithms.\nAlthough previous works in this area have achieved great success, much\nconservatism remains in their framework. In this work, we propose a novel\ntwo-stage training framework to jointly synthesize the controller and Lyapunov\nfunction for continuous-time systems. By leveraging a Zubov-inspired region of\nattraction characterization to directly estimate stability boundaries, we\npropose a novel training data sampling strategy and a domain updating mechanism\nthat significantly reduces the conservatism in training. Moreover, unlike\nexisting works on continuous-time systems that rely on an SMT solver to\nformally verify the Lyapunov condition, we extend state-of-the-art neural\nnetwork verifier $\\alpha,\\!\\beta$-CROWN with the capability of performing\nautomatic bound propagation through the Jacobian of dynamical systems and a\nnovel verification scheme that avoids expensive bisection. To demonstrate the\neffectiveness of our approach, we conduct numerical experiments by synthesizing\nand verifying controllers on several challenging nonlinear systems across\nmultiple dimensions. We show that our training can yield region of attractions\nwith volume $5 - 1.5\\cdot 10^{5}$ times larger compared to the baselines, and\nour verification on continuous systems can be up to $40-10000$ times faster\ncompared to the traditional SMT solver dReal. Our code is available at\nhttps://github.com/Verified-Intelligence/Two-Stage_Neural_Controller_Training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u5408\u5408\u6210\u63a7\u5236\u5668\u548cLyapunov\u51fd\u6570\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4fdd\u5b88\u6027\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684\u9a8c\u8bc1\u65b9\u6cd5\u63d0\u5347\u4e86\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u5b66\u4e60\u7684\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u7b56\u7565\u5728\u7a33\u5b9a\u6027\u548c\u533a\u57df\u5438\u5f15\u529b\u4f30\u8ba1\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408Zubov\u542f\u53d1\u7684\u533a\u57df\u5438\u5f15\u529b\u7279\u5f81\u548c\u6539\u8fdb\u7684\u6570\u636e\u91c7\u6837\u7b56\u7565\uff0c\u6269\u5c55\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\u5668\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u533a\u57df\u5438\u5f15\u529b\u4f53\u79ef\u6bd4\u57fa\u7ebf\u59275-1.5\u00d710^5\u500d\uff0c\u9a8c\u8bc1\u901f\u5ea6\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb40-10000\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u964d\u4f4e\u4fdd\u5b88\u6027\u548c\u63d0\u5347\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u975e\u7ebf\u6027\u7cfb\u7edf\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc\u63a7\u5236, Lyapunov\u51fd\u6570, \u7a33\u5b9a\u6027\u9a8c\u8bc1, Zubov\u542f\u53d1, \u4e24\u9636\u6bb5\u8bad\u7ec3"}}
{"id": "2506.01357", "pdf": "https://arxiv.org/pdf/2506.01357", "abs": "https://arxiv.org/abs/2506.01357", "authors": ["Zhiyang Qi", "Takumasa Kaneko", "Keiko Takamizo", "Mariko Ukiyo", "Michimasa Inaba"], "title": "KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Generating psychological counseling responses with language models relies\nheavily on high-quality datasets. Crowdsourced data collection methods require\nstrict worker training, and data from real-world counseling environments may\nraise privacy and ethical concerns. While recent studies have explored using\nlarge language models (LLMs) to augment psychological counseling dialogue\ndatasets, the resulting data often suffers from limited diversity and\nauthenticity. To address these limitations, this study adopts a role-playing\napproach where trained counselors simulate counselor-client interactions,\nensuring high-quality dialogues while mitigating privacy risks. Using this\nmethod, we construct KokoroChat, a Japanese psychological counseling dialogue\ndataset comprising 6,589 long-form dialogues, each accompanied by comprehensive\nclient feedback. Experimental results demonstrate that fine-tuning open-source\nLLMs with KokoroChat improves both the quality of generated counseling\nresponses and the automatic evaluation of counseling dialogues. The KokoroChat\ndataset is available at https://github.com/UEC-InabaLab/KokoroChat.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u65b9\u6cd5\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u5fc3\u7406\u54a8\u8be2\u5bf9\u8bdd\u6570\u636e\u96c6KokoroChat\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5fc3\u7406\u54a8\u8be2\u5bf9\u8bdd\u6570\u636e\u96c6\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\uff0c\u4e14\u5b58\u5728\u9690\u79c1\u548c\u4f26\u7406\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u8bc1\u8d28\u91cf\u53c8\u80fd\u89c4\u907f\u98ce\u9669\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u89d2\u8272\u626e\u6f14\u65b9\u6cd5\uff0c\u7531\u7ecf\u8fc7\u57f9\u8bad\u7684\u54a8\u8be2\u5e08\u6a21\u62df\u54a8\u8be2\u5e08\u4e0e\u5ba2\u6237\u7684\u5bf9\u8bdd\uff0c\u6784\u5efa\u4e86\u5305\u542b6,589\u6761\u957f\u5bf9\u8bdd\u7684\u65e5\u8bed\u5fc3\u7406\u54a8\u8be2\u6570\u636e\u96c6KokoroChat\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528KokoroChat\u5fae\u8c03\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u751f\u6210\u5fc3\u7406\u54a8\u8be2\u56de\u590d\u7684\u8d28\u91cf\u548c\u5bf9\u8bdd\u7684\u81ea\u52a8\u8bc4\u4f30\u6548\u679c\u3002", "conclusion": "\u89d2\u8272\u626e\u6f14\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u6548\u7684\u5fc3\u7406\u5bf9\u8bdd\u6570\u636e\u6536\u96c6\u624b\u6bb5\uff0cKokoroChat\u4e3a\u5fc3\u7406\u54a8\u8be2\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u8d44\u6e90\u3002", "keywords": "\u5fc3\u7406\u54a8\u8be2\u5bf9\u8bdd\u6570\u636e\u96c6, \u89d2\u8272\u626e\u6f14, \u5927\u8bed\u8a00\u6a21\u578b, \u6570\u636e\u591a\u6837\u6027, \u9690\u79c1\u4fdd\u62a4"}}
{"id": "2506.00358", "pdf": "https://arxiv.org/pdf/2506.00358", "abs": "https://arxiv.org/abs/2506.00358", "authors": ["Sarthak Kumar Maharana", "Saksham Singh Kushwaha", "Baoming Zhang", "Adrian Rodriguez", "Songtao Wei", "Yapeng Tian", "Yunhui Guo"], "title": "$\\texttt{AVROBUSTBENCH}$: Benchmarking the Robustness of Audio-Visual Recognition Models at Test-Time", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "Under review. For uniformity, all TTA experiments are done with a\n  batch size of 16", "summary": "While recent audio-visual models have demonstrated impressive performance,\ntheir robustness to distributional shifts at test-time remains not fully\nunderstood. Existing robustness benchmarks mainly focus on single modalities,\nmaking them insufficient for thoroughly assessing the robustness of\naudio-visual models. Motivated by real-world scenarios where shifts can occur\n$\\textit{simultaneously}$ in both audio and visual modalities, we introduce\n$\\texttt{AVROBUSTBENCH}$, a comprehensive benchmark designed to evaluate the\ntest-time robustness of audio-visual recognition models.\n$\\texttt{AVROBUSTBENCH}$ comprises four audio-visual benchmark datasets,\n$\\texttt{AUDIOSET-2C}$, $\\texttt{VGGSOUND-2C}$, $\\texttt{KINETICS-2C}$, and\n$\\texttt{EPICKITCHENS-2C}$, each incorporating 75 bimodal audio-visual\ncorruptions that are $\\textit{co-occurring}$ and $\\textit{correlated}$. Through\nextensive evaluations, we observe that state-of-the-art supervised and\nself-supervised audio-visual models exhibit declining robustness as corruption\nseverity increases. Furthermore, online test-time adaptation (TTA) methods, on\n$\\texttt{VGGSOUND-2C}$ and $\\texttt{KINETICS-2C}$, offer minimal improvements\nin performance under bimodal corruptions. We further propose $\\texttt{AV2C}$, a\nsimple TTA approach enabling on-the-fly cross-modal fusion by penalizing\nhigh-entropy samples, which achieves improvements on $\\texttt{VGGSOUND-2C}$. We\nhope that $\\texttt{AVROBUSTBENCH}$ will steer the development of more effective\nand robust audio-visual TTA approaches. Our code is available\n$\\href{https://github.com/sarthaxxxxx/AV-C-Robustness-Benchmark}{here}$.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aAVROBUSTBENCH\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u97f3\u9891-\u89c6\u89c9\u8bc6\u522b\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u5728\u7ebf\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u65b9\u6cd5AV2C\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\u53ef\u80fd\u540c\u65f6\u53d1\u751f\u5206\u5e03\u504f\u79fb\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u8bbe\u8ba1\u4e86\u56db\u4e2a\u5305\u542b\u53cc\u6a21\u6001\u97f3\u9891-\u89c6\u89c9\u6270\u52a8\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86AV2C\u65b9\u6cd5\uff0c\u901a\u8fc7\u60e9\u7f5a\u9ad8\u71b5\u6837\u672c\u5b9e\u73b0\u52a8\u6001\u8de8\u6a21\u6001\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u6270\u52a8\u4e25\u91cd\u6027\u589e\u52a0\u65f6\u9c81\u68d2\u6027\u4e0b\u964d\uff0c\u800cAV2C\u5728\u90e8\u5206\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6709\u6240\u63d0\u5347\u3002", "conclusion": "AVROBUSTBENCH\u6709\u671b\u63a8\u52a8\u66f4\u6709\u6548\u7684\u97f3\u9891-\u89c6\u89c9\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "keywords": "\u97f3\u9891-\u89c6\u89c9\u6a21\u578b\u3001\u9c81\u68d2\u6027\u3001\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u3001\u8de8\u6a21\u6001\u878d\u5408\u3001\u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2506.01360", "pdf": "https://arxiv.org/pdf/2506.01360", "abs": "https://arxiv.org/abs/2506.01360", "authors": ["Dongwon Choi", "Sunwoo Kim", "Juyeon Kim", "Kyungho Kim", "Geon Lee", "Shinhwan Kang", "Myunghwan Kim", "Kijung Shin"], "title": "RDB2G-Bench: A Comprehensive Benchmark for Automatic Graph Modeling of Relational Databases", "categories": ["cs.LG"], "comment": "Code and datasets are in https://github.com/chlehdwon/RDB2G-Bench", "summary": "Relational databases (RDBs) are composed of interconnected tables, where\nrelationships between them are defined through foreign keys. Recent research on\napplying machine learning to RDBs has explored graph-based representations of\nRDBs, where rows of tables are modeled as nodes, and foreign key relationships\nare modeled as edges. RDB-to-graph modeling helps capture cross-table\ndependencies, ultimately leading to enhanced performance across diverse tasks.\nHowever, there are numerous ways to model RDBs as graphs, and performance\nvaries significantly depending on the chosen graph model. In our analysis,\napplying a common heuristic rule for graph modeling leads to up to a 10% drop\nin performance compared to the best-performing graph model, which remains\nnon-trivial to identify. To foster research on intelligent RDB-to-graph\nmodeling, we introduce RDB2G-Bench, the first benchmark framework for\nevaluating such methods. We construct extensive datasets covering 5 real-world\nRDBs and 12 predictive tasks, resulting in around 50k graph-performance pairs\nfor efficient and reproducible evaluations. Thanks to our precomputed datasets,\nwe were able to benchmark 9 automatic RDB-to-graph modeling methods on the 12\ntasks over 600x faster than on-the-fly evaluation, which requires repeated\nmodel training. Our analysis of the datasets and benchmark results reveals key\nstructural patterns affecting graph model effectiveness, along with practical\nimplications for effective graph modeling.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86RDB2G-Bench\uff0c\u7b2c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5173\u7cfb\u6570\u636e\u5e93\uff08RDB\uff09\u5230\u56fe\u6a21\u578b\u8f6c\u6362\u65b9\u6cd5\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u56fe\u5efa\u6a21\u65b9\u6cd5\u6027\u80fd\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u5f71\u54cd\u6a21\u578b\u6548\u679c\u7684\u5173\u952e\u7ed3\u6784\u6a21\u5f0f\u3002", "motivation": "\u5173\u7cfb\u6570\u636e\u5e93\u7684\u56fe\u5efa\u6a21\u65b9\u6cd5\u591a\u79cd\u591a\u6837\uff0c\u4f46\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u4e14\u7f3a\u4e4f\u7edf\u4e00\u8bc4\u4f30\u6807\u51c6\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u5316\u5de5\u5177\u4fc3\u8fdb\u8be5\u9886\u57df\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86RDB2G-Bench\u57fa\u51c6\u6846\u67b6\uff0c\u6784\u5efa\u4e86\u6db5\u76d65\u4e2a\u771f\u5b9eRDB\u548c12\u9879\u9884\u6d4b\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5171\u7ea650k\u56fe-\u6027\u80fd\u5bf9\uff0c\u7528\u4e8e\u9ad8\u6548\u8bc4\u4f309\u79cd\u81ea\u52a8\u56fe\u5efa\u6a21\u65b9\u6cd5\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c\u4e0d\u540c\u7684\u56fe\u5efa\u6a21\u65b9\u6cd5\u6027\u80fd\u5dee\u5f02\u53ef\u8fbe10%\uff0c\u4e14\u901a\u8fc7\u9884\u8ba1\u7b97\u6570\u636e\u96c6\u53ef\u5c06\u8bc4\u4f30\u901f\u5ea6\u63d0\u5347600\u500d\u4ee5\u4e0a\u3002", "conclusion": "RDB2G-Bench\u4e3aRDB\u56fe\u5efa\u6a21\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u8bc4\u4f30\u5de5\u5177\uff0c\u5e76\u63ed\u793a\u4e86\u5f71\u54cd\u6a21\u578b\u6548\u679c\u7684\u5173\u952e\u7ed3\u6784\u6a21\u5f0f\u3002", "keywords": "\u5173\u7cfb\u6570\u636e\u5e93, \u56fe\u5efa\u6a21, \u57fa\u51c6\u6846\u67b6, \u673a\u5668\u5b66\u4e60"}}
{"id": "2506.01367", "pdf": "https://arxiv.org/pdf/2506.01367", "abs": "https://arxiv.org/abs/2506.01367", "authors": ["Kensuke Mitsuzawa", "Damien Garreau"], "title": "MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect Hallucinations", "categories": ["cs.CL", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) have become pervasive in our everyday life. Yet,\na fundamental obstacle prevents their use in many critical applications: their\npropensity to generate fluent, human-quality content that is not grounded in\nreality. The detection of such hallucinations is thus of the highest\nimportance. In this work, we propose a new method to flag hallucinated content,\nMMD-Flagger. It relies on Maximum Mean Discrepancy (MMD), a non-parametric\ndistance between distributions. On a high-level perspective, MMD-Flagger tracks\nthe MMD between the generated documents and documents generated with various\ntemperature parameters. We show empirically that inspecting the shape of this\ntrajectory is sufficient to detect most hallucinations. This novel method is\nbenchmarked on two machine translation datasets, on which it outperforms\nnatural competitors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\u7684\u65b0\u65b9\u6cd5MMD-Flagger\uff0c\u7528\u4e8e\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u7684\u5185\u5bb9\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "LLMs\u751f\u6210\u7684\u5185\u5bb9\u53ef\u80fd\u5b58\u5728\u4e0d\u771f\u5b9e\uff08\u5e7b\u89c9\uff09\u7684\u95ee\u9898\uff0c\u8fd9\u5728\u5173\u952e\u5e94\u7528\u4e2d\u662f\u4e00\u4e2a\u91cd\u5927\u969c\u788d\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u6b64\u7c7b\u95ee\u9898\u3002", "method": "\u5229\u7528\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\u4f5c\u4e3a\u975e\u53c2\u6570\u5206\u5e03\u8ddd\u79bb\uff0c\u901a\u8fc7\u8ddf\u8e2a\u4e0d\u540c\u6e29\u5ea6\u53c2\u6570\u4e0b\u751f\u6210\u6587\u6863\u7684MMD\u8f68\u8ff9\u6765\u68c0\u6d4b\u5e7b\u89c9\u3002", "result": "\u5728\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u96c6\u4e0a\uff0cMMD-Flagger\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5927\u591a\u6570\u5e7b\u89c9\u3002", "conclusion": "MMD-Flagger\u662f\u4e00\u79cd\u6709\u6548\u7684\u5e7b\u89c9\u68c0\u6d4b\u5de5\u5177\uff0c\u9002\u7528\u4e8eLLMs\u751f\u6210\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u95ee\u9898\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u5e7b\u89c9\u68c0\u6d4b,\u6700\u5927\u5747\u503c\u5dee\u5f02,\u673a\u5668\u7ffb\u8bd1"}}
{"id": "2506.00368", "pdf": "https://arxiv.org/pdf/2506.00368", "abs": "https://arxiv.org/abs/2506.00368", "authors": ["Ngoc Long Pham", "Tri Nhu Do"], "title": "Neural Network-based Information-Theoretic Transceivers for High-Order Modulation Schemes", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Neural network (NN)-based end-to-end (E2E) communication systems, in which\neach system component may consist of a portion of a neural network, have been\ninvestigated as potential tools for developing artificial intelligence\n(Al)-native E2E systems. In this paper, we propose an NN-based bitwise receiver\nthat improves computational efficiency while maintaining performance comparable\nto baseline demappers. Building on this foundation, we introduce a novel\nsymbol-wise autoencoder (AE)-based E2E system that jointly optimizes the\ntransmitter and receiver at the physical layer. We evaluate the proposed\nNN-based receiver using bit-error rate (BER) analysis to confirm that the\nnumerical BER achieved by NN-based receivers or transceivers is accurate.\nResults demonstrate that the AE-based system outperforms baseline\narchitectures, particularly for higher-order modulation schemes. We further\nshow that the training signal-to-noise ratio (SNR) significantly affects the\nperformance of the systems when inference is conducted at different SNR levels.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\uff08NN\uff09\u7684\u9ad8\u6548\u6bd4\u7279\u63a5\u6536\u5668\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7b26\u53f7\u81ea\u7f16\u7801\u5668\uff08AE\uff09\u7684\u65b0\u578b\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7269\u7406\u5c42\u7684\u53d1\u5c04\u5668\u548c\u63a5\u6536\u5668\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6027\u80fd\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u63a2\u7d22\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u7aef\u5230\u7aef\u901a\u4fe1\u7cfb\u7edf\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u5e76\u4fdd\u6301\u6027\u80fd\uff0c\u65e8\u5728\u5f00\u53d1\u4eba\u5de5\u667a\u80fd\u539f\u751f\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\u3002", "method": "\u63d0\u51faNN\u6bd4\u7279\u63a5\u6536\u5668\uff0c\u5e76\u6784\u5efa\u57fa\u4e8e\u7b26\u53f7AE\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u901a\u8fc7\u6bd4\u7279\u9519\u8bef\u7387\uff08BER\uff09\u5206\u6790\u8bc4\u4f30\u6027\u80fd\u3002", "result": "AE\u7cfb\u7edf\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u67b6\u6784\uff0c\u5c24\u5176\u662f\u9ad8\u9636\u8c03\u5236\u65b9\u6848\uff1b\u8bad\u7ec3\u4fe1\u566a\u6bd4\uff08SNR\uff09\u5bf9\u7cfb\u7edf\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u57fa\u4e8eNN\u548cAE\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u5408\u9ad8\u9636\u8c03\u5236\u573a\u666f\uff0cSNR\u9009\u62e9\u662f\u5173\u952e\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc\uff0c\u7aef\u5230\u7aef\u901a\u4fe1\uff0c\u81ea\u7f16\u7801\u5668\uff0c\u6bd4\u7279\u9519\u8bef\u7387\uff0c\u4fe1\u566a\u6bd4"}}
{"id": "2506.01361", "pdf": "https://arxiv.org/pdf/2506.01361", "abs": "https://arxiv.org/abs/2506.01361", "authors": ["Muhammad Hasan Ferdous", "Emam Hossain", "Md Osman Gani"], "title": "TimeGraph: Synthetic Benchmark Datasets for Robust Time-Series Causal Discovery", "categories": ["cs.LG", "cs.IR", "stat.ML", "62H12, 62P10, 68T05", "I.2.6; I.5.1; G.3"], "comment": "11 pages, 4 figures, accepted at KDD 2025 (Datasets and Benchmarks\n  Track)", "summary": "Robust causal discovery in time series datasets depends on reliable benchmark\ndatasets with known ground-truth causal relationships. However, such datasets\nremain scarce, and existing synthetic alternatives often overlook critical\ntemporal properties inherent in real-world data, including nonstationarity\ndriven by trends and seasonality, irregular sampling intervals, and the\npresence of unobserved confounders. To address these challenges, we introduce\nTimeGraph, a comprehensive suite of synthetic time-series benchmark datasets\nthat systematically incorporates both linear and nonlinear dependencies while\nmodeling key temporal characteristics such as trends, seasonal effects, and\nheterogeneous noise patterns. Each dataset is accompanied by a fully specified\ncausal graph featuring varying densities and diverse noise distributions and is\nprovided in two versions: one including unobserved confounders and one without,\nthereby offering extensive coverage of real-world complexity while preserving\nmethodological neutrality. We further demonstrate the utility of TimeGraph\nthrough systematic evaluations of state-of-the-art causal discovery algorithms\nincluding PCMCI+, LPCMCI, and FGES across a diverse array of configurations and\nmetrics. Our experiments reveal significant variations in algorithmic\nperformance under realistic temporal conditions, underscoring the need for\nrobust synthetic benchmarks in the fair and transparent assessment of causal\ndiscovery methods. The complete TimeGraph suite, including dataset generation\nscripts, evaluation metrics, and recommended experimental protocols, is freely\navailable to facilitate reproducible research and foster community-driven\nadvancements in time-series causal discovery.", "AI": {"tldr": "TimeGraph\u662f\u4e00\u4e2a\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u57fa\u51c6\u6570\u636e\u96c6\u5957\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30\u56e0\u679c\u53d1\u73b0\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u7f3a\u4e4f\u771f\u5b9e\u65f6\u95f4\u7279\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u53d1\u73b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\u7a00\u7f3a\u4e14\u5ffd\u7565\u771f\u5b9e\u65f6\u95f4\u7279\u6027\uff0c\u5982\u975e\u5e73\u7a33\u6027\u3001\u4e0d\u89c4\u5219\u91c7\u6837\u548c\u672a\u89c2\u5bdf\u5230\u7684\u6df7\u6742\u56e0\u7d20\u3002", "method": "\u5f15\u5165TimeGraph\u5957\u4ef6\uff0c\u7cfb\u7edf\u6574\u5408\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\uff0c\u6a21\u62df\u8d8b\u52bf\u3001\u5b63\u8282\u6548\u5e94\u548c\u5f02\u8d28\u6027\u566a\u58f0\u7b49\u65f6\u95f4\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u7b97\u6cd5\u5728\u771f\u5b9e\u65f6\u95f4\u6761\u4ef6\u4e0b\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u7a81\u663e\u4e86\u7a33\u5065\u5408\u6210\u57fa\u51c6\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "TimeGraph\u4e3a\u516c\u5e73\u900f\u660e\u8bc4\u4f30\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u4fc3\u8fdb\u53ef\u91cd\u590d\u7814\u7a76\u548c\u793e\u533a\u53d1\u5c55\u3002", "keywords": "TimeGraph,\u5408\u6210\u65f6\u95f4\u5e8f\u5217,\u56e0\u679c\u53d1\u73b0,\u57fa\u51c6\u6570\u636e\u96c6"}}
{"id": "2506.01381", "pdf": "https://arxiv.org/pdf/2506.01381", "abs": "https://arxiv.org/abs/2506.01381", "authors": ["Yilong Lai", "Jialong Wu", "Zhenglin Wang", "Deyu Zhou"], "title": "AdaRewriter: Unleashing the Power of Prompting-based Conversational Query Reformulation via Test-Time Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "Prompting-based conversational query reformulation has emerged as a powerful\napproach for conversational search, refining ambiguous user queries into\nstandalone search queries. Best-of-N reformulation over the generated\ncandidates via prompting shows impressive potential scaling capability.\nHowever, both the previous tuning methods (training time) and adaptation\napproaches (test time) can not fully unleash their benefits. In this paper, we\npropose AdaRewriter, a novel framework for query reformulation using an\noutcome-supervised reward model via test-time adaptation. By training a\nlightweight reward model with contrastive ranking loss, AdaRewriter selects the\nmost promising reformulation during inference. Notably, it can operate\neffectively in black-box systems, including commercial LLM APIs. Experiments on\nfive conversational search datasets show that AdaRewriter significantly\noutperforms the existing methods across most settings, demonstrating the\npotential of test-time adaptation for conversational query reformulation.", "AI": {"tldr": "AdaRewriter\u662f\u4e00\u79cd\u57fa\u4e8e\u6d4b\u8bd5\u65f6\u9002\u5e94\u7684\u67e5\u8be2\u91cd\u5199\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5956\u52b1\u6a21\u578b\u9009\u62e9\u6700\u4f73\u91cd\u5199\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u6216\u6d4b\u8bd5\u65f6\u5747\u672a\u80fd\u5145\u5206\u91ca\u653e\u63d0\u793a\u5f0f\u67e5\u8be2\u91cd\u5199\u7684\u6f5c\u529b\uff0c\u56e0\u6b64\u63d0\u51faAdaRewriter\u4ee5\u4f18\u5316\u6548\u679c\u3002", "method": "AdaRewriter\u4f7f\u7528\u57fa\u4e8e\u5956\u52b1\u6a21\u578b\u7684\u5bf9\u6bd4\u6392\u5e8f\u635f\u5931\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u9009\u62e9\u6700\u4f18\u91cd\u5199\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u9ed1\u76d2\u7cfb\u7edf\u5982\u5546\u4e1aLLM API\u3002", "result": "\u5728\u4e94\u4e2a\u5bf9\u8bdd\u641c\u7d22\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAdaRewriter\u5728\u591a\u6570\u8bbe\u7f6e\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AdaRewriter\u5c55\u793a\u4e86\u6d4b\u8bd5\u65f6\u9002\u5e94\u5728\u5bf9\u8bdd\u67e5\u8be2\u91cd\u5199\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "\u5bf9\u8bdd\u641c\u7d22,\u67e5\u8be2\u91cd\u5199,\u6d4b\u8bd5\u65f6\u9002\u5e94,\u5956\u52b1\u6a21\u578b"}}
{"id": "2506.00385", "pdf": "https://arxiv.org/pdf/2506.00385", "abs": "https://arxiv.org/abs/2506.00385", "authors": ["Yakun Song", "Jiawei Chen", "Xiaobin Zhuang", "Chenpeng Du", "Ziyang Ma", "Jian Wu", "Jian Cong", "Dongya Jia", "Zhuo Chen", "Yuping Wang", "Yuxuan Wang", "Xie Chen"], "title": "MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity Reconstruction and Generation", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "18 pages, 3 figures. The code and pre-trained models are available at\n  https://github.com/Ereboas/MagiCodec", "summary": "Neural audio codecs have made significant strides in efficiently mapping raw\naudio waveforms into discrete token representations, which are foundational for\ncontemporary audio generative models. However, most existing codecs are\noptimized primarily for reconstruction quality, often at the expense of the\ndownstream modelability of the encoded tokens. Motivated by the need to\novercome this bottleneck, we introduce $\\textbf{MagiCodec}$, a novel\nsingle-layer, streaming Transformer-based audio codec. MagiCodec is designed\nwith a multistage training pipeline that incorporates Gaussian noise injection\nand latent regularization, explicitly targeting the enhancement of semantic\nexpressiveness in the generated codes while preserving high reconstruction\nfidelity. We analytically derive the effect of noise injection in the frequency\ndomain, demonstrating its efficacy in attenuating high-frequency components and\nfostering robust tokenization. Extensive experimental evaluations show that\nMagiCodec surpasses state-of-the-art codecs in both reconstruction quality and\ndownstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like\ndistributions, as observed in natural languages, thereby improving\ncompatibility with language-model-based generative architectures. The code and\npre-trained models are available at https://github.com/Ereboas/MagiCodec.", "AI": {"tldr": "MagiCodec\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u5355\u5c42\u6d41\u5f0f\u97f3\u9891\u7f16\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u63d0\u5347\u8bed\u4e49\u8868\u8fbe\u548c\u91cd\u5efa\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u867d\u91cd\u5efa\u8d28\u91cf\u9ad8\uff0c\u4f46\u7f16\u7801\u7684\u6807\u8bb0\u5bf9\u4e0b\u6e38\u751f\u6210\u6a21\u578b\u7684\u9002\u914d\u6027\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u7ed3\u5408\u9ad8\u65af\u566a\u58f0\u6ce8\u5165\u548c\u6f5c\u5728\u6b63\u5219\u5316\uff0c\u589e\u5f3a\u8bed\u4e49\u8868\u8fbe\u5e76\u4fdd\u7559\u9ad8\u4fdd\u771f\u91cd\u5efa\u3002", "result": "MagiCodec\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u7f16\u89e3\u7801\u5668\uff0c\u6807\u8bb0\u5206\u5e03\u5448\u73b0\u7c7b\u81ea\u7136\u8bed\u8a00\u7684Zipf\u7279\u6027\u3002", "conclusion": "MagiCodec\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u548c\u6b63\u5219\u5316\u6539\u8fdb\u4e86\u6807\u8bb0\u7684\u8bed\u4e49\u8868\u8fbe\uff0c\u9002\u914d\u751f\u6210\u6a21\u578b\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002", "keywords": "\u97f3\u9891\u7f16\u89e3\u7801\u5668,Transformer,\u566a\u58f0\u6ce8\u5165,\u8bed\u4e49\u8868\u8fbe,Zipf\u5206\u5e03"}}
{"id": "2506.01364", "pdf": "https://arxiv.org/pdf/2506.01364", "abs": "https://arxiv.org/abs/2506.01364", "authors": ["Yuchen Fang", "Hao Miao", "Yuxuan Liang", "Liwei Deng", "Yue Cui", "Ximu Zeng", "Yuyang Xia", "Yan Zhao", "Torben Bach Pedersen", "Christian S. Jensen", "Xiaofang Zhou", "Kai Zheng"], "title": "Unraveling Spatio-Temporal Foundation Models via the Pipeline Lens: A Comprehensive Review", "categories": ["cs.LG", "cs.AI"], "comment": "21 pages, 10 figures", "summary": "Spatio-temporal deep learning models aims to utilize useful patterns in such\ndata to support tasks like prediction. However, previous deep learning models\ndesigned for specific tasks typically require separate training for each use\ncase, leading to increased computational and storage costs. To address this\nissue, spatio-temporal foundation models have emerged, offering a unified\nframework capable of solving multiple spatio-temporal tasks. These foundation\nmodels achieve remarkable success by learning general knowledge with\nspatio-temporal data or transferring the general capabilities of pre-trained\nlanguage models. While previous surveys have explored spatio-temporal data and\nmethodologies separately, they have ignored a comprehensive examination of how\nfoundation models are designed, selected, pre-trained, and adapted. As a\nresult, the overall pipeline for spatio-temporal foundation models remains\nunclear. To bridge this gap, we innovatively provide an up-to-date review of\nprevious spatio-temporal foundation models from the pipeline perspective. The\npipeline begins with an introduction to different types of spatio-temporal\ndata, followed by details of data preprocessing and embedding techniques. The\npipeline then presents a novel data property taxonomy to divide existing\nmethods according to data sources and dependencies, providing efficient and\neffective model design and selection for researchers. On this basis, we further\nillustrate the training objectives of primitive models, as well as the\nadaptation techniques of transferred models. Overall, our survey provides a\nclear and structured pipeline to understand the connection between core\nelements of spatio-temporal foundation models while guiding researchers to get\nstarted quickly. Additionally, we introduce emerging opportunities such as\nmulti-objective training in the field of spatio-temporal foundation models.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5904\u7406\u65f6\u7a7a\u6570\u636e\u65f6\u901a\u5e38\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u5355\u72ec\u8bad\u7ec3\uff0c\u589e\u52a0\u4e86\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c\u3002\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u5176\u8bbe\u8ba1\u3001\u9009\u62e9\u3001\u9884\u8bad\u7ec3\u548c\u9002\u914d\u7684\u5168\u9762\u7814\u7a76\u3002\u672c\u6587\u4ece\u6d41\u7a0b\u89d2\u5ea6\u5bf9\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4e86\u6700\u65b0\u7efc\u8ff0\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6e05\u6670\u7684\u6d41\u7a0b\u6846\u67b6\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u5feb\u901f\u4e0a\u624b\u3002", "motivation": "\u89e3\u51b3\u65f6\u7a7a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u5355\u4efb\u52a1\u8bad\u7ec3\u5e26\u6765\u7684\u9ad8\u6210\u672c\u548c\u6548\u7387\u95ee\u9898\uff0c\u5e76\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u5173\u4e8e\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\u8bbe\u8ba1\u3001\u9009\u62e9\u3001\u9884\u8bad\u7ec3\u548c\u9002\u914d\u7684\u5168\u9762\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u4ece\u6d41\u7a0b\u89d2\u5ea6\u7efc\u8ff0\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\uff0c\u5305\u62ec\u6570\u636e\u7c7b\u578b\u4ecb\u7ecd\u3001\u6570\u636e\u9884\u5904\u7406\u4e0e\u5d4c\u5165\u6280\u672f\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6570\u636e\u5c5e\u6027\u548c\u4f9d\u8d56\u5173\u7cfb\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6e05\u6670\u7684\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\u6d41\u7a0b\u6846\u67b6\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u7406\u89e3\u6a21\u578b\u8bbe\u8ba1\u4e0e\u9009\u62e9\uff0c\u5e76\u4ecb\u7ecd\u4e86\u65b0\u5174\u7814\u7a76\u65b9\u5411\u5982\u591a\u76ee\u6807\u8bad\u7ec3\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u6d41\u7a0b\u6846\u67b6\uff0c\u4fc3\u8fdb\u4e86\u5bf9\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\u7684\u5168\u9762\u7406\u89e3\uff0c\u5e76\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "keywords": "\u65f6\u7a7a\u6570\u636e,\u6df1\u5ea6\u5b66\u4e60,\u57fa\u7840\u6a21\u578b,\u9884\u8bad\u7ec3,\u6d41\u7a0b\u6846\u67b6"}}
{"id": "2506.01406", "pdf": "https://arxiv.org/pdf/2506.01406", "abs": "https://arxiv.org/abs/2506.01406", "authors": ["Andrei Popescu-Belis", "Alexis Allemann", "Teo Ferrari", "Gopal Krishnamani"], "title": "Speech-to-Speech Translation Pipelines for Conversations in Low-Resource Languages", "categories": ["cs.CL"], "comment": "Proceedings of MT Summit 2025", "summary": "The popularity of automatic speech-to-speech translation for human\nconversations is growing, but the quality varies significantly depending on the\nlanguage pair. In a context of community interpreting for low-resource\nlanguages, namely Turkish and Pashto to/from French, we collected fine-tuning\nand testing data, and compared systems using several automatic metrics (BLEU,\nCOMET, and BLASER) and human assessments. The pipelines included automatic\nspeech recognition, machine translation, and speech synthesis, with local\nmodels and cloud-based commercial ones. Some components have been fine-tuned on\nour data. We evaluated over 60 pipelines and determined the best one for each\ndirection. We also found that the ranks of components are generally independent\nof the rest of the pipeline.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u571f\u8033\u5176\u8bed\u548c\u666e\u4ec0\u56fe\u8bed\u4e0e\u6cd5\u8bed\uff09\u7684\u81ea\u52a8\u8bed\u97f3\u7ffb\u8bd1\u8d28\u91cf\uff0c\u8bc4\u4f30\u4e8660\u591a\u4e2a\u7cfb\u7edf\u7ba1\u7ebf\uff0c\u5e76\u786e\u5b9a\u4e86\u6700\u4f73\u65b9\u6848\u3002", "motivation": "\u81ea\u52a8\u8bed\u97f3\u7ffb\u8bd1\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u4e2d\u8d28\u91cf\u5dee\u5f02\u5927\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6570\u636e\u5fae\u8c03\u548c\u591a\u6307\u6807\u8bc4\u4f30\u63d0\u5347\u6027\u80fd\u3002", "method": "\u6536\u96c6\u6570\u636e\u3001\u5fae\u8c03\u6a21\u578b\uff0c\u6bd4\u8f83\u591a\u79cd\u7ba1\u7ebf\uff08\u672c\u5730\u548c\u4e91\u7aef\uff09\uff0c\u4f7f\u7528BLEU\u3001COMET\u3001BLASER\u7b49\u6307\u6807\u53ca\u4eba\u5de5\u8bc4\u4f30\u3002", "result": "\u786e\u5b9a\u4e86\u6bcf\u79cd\u8bed\u8a00\u5bf9\u7684\u6700\u4f73\u7ba1\u7ebf\uff0c\u53d1\u73b0\u7ec4\u4ef6\u6027\u80fd\u4e0e\u7ba1\u7ebf\u5176\u4ed6\u90e8\u5206\u72ec\u7acb\u3002", "conclusion": "\u793e\u533a\u53e3\u8bd1\u4e2d\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u7684\u81ea\u52a8\u7ffb\u8bd1\u53ef\u901a\u8fc7\u7ec4\u4ef6\u4f18\u5316\u63d0\u5347\u8d28\u91cf\u3002", "keywords": "\u8bed\u97f3\u7ffb\u8bd1,\u4f4e\u8d44\u6e90\u8bed\u8a00,\u5fae\u8c03\u6a21\u578b,\u793e\u533a\u53e3\u8bd1,\u591a\u6307\u6807\u8bc4\u4f30"}}
{"id": "2506.01369", "pdf": "https://arxiv.org/pdf/2506.01369", "abs": "https://arxiv.org/abs/2506.01369", "authors": ["Fuxiang Zhang", "Jiacheng Xu", "Chaojie Wang", "Ce Cui", "Yang Liu", "Bo An"], "title": "Incentivizing LLMs to Self-Verify Their Answers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in complex\nreasoning tasks through both post-training and test-time scaling laws. While\nprevalent test-time scaling approaches are often realized by using external\nreward models to guide the model generation process, we find only marginal\ngains can be acquired when scaling a model post-trained on specific reasoning\ntasks. We identify that the limited improvement stems from distribution\ndiscrepancies between the specific post-trained generator and the general\nreward model. To address this, we propose a framework that incentivizes LLMs to\nself-verify their own answers. By unifying answer generation and verification\nwithin a single reinforcement learning (RL) process, we train models that can\neffectively assess the correctness of their own solutions. The trained model\ncan further scale its performance during inference time by verifying its\ngenerations, without the need for external verifiers. We train our\nself-verification models based on Qwen2.5-Math-7B and\nDeepSeek-R1-Distill-Qwen-1.5B, demonstrating its capabilities across varying\nreasoning context lengths. Experiments on multiple mathematical reasoning\nbenchmarks show that our models can not only improve post-training performance\nbut also enable effective test-time scaling. Our code is available at\nhttps://github.com/mansicer/self-verification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7edf\u4e00\u7b54\u6848\u751f\u6210\u4e0e\u9a8c\u8bc1\uff0c\u63d0\u5347\u4e86LLMs\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u65e0\u9700\u5916\u90e8\u9a8c\u8bc1\u5668\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u5956\u52b1\u6a21\u578b\uff0c\u4f46\u5bf9\u7279\u5b9a\u4efb\u52a1\u540e\u8bad\u7ec3\u7684\u6a21\u578b\u6548\u679c\u63d0\u5347\u6709\u9650\uff0c\u539f\u56e0\u4e3a\u751f\u6210\u5668\u4e0e\u5956\u52b1\u6a21\u578b\u7684\u5206\u5e03\u5dee\u5f02\u3002", "method": "\u8bbe\u8ba1\u81ea\u9a8c\u8bc1\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u81ea\u6211\u8bc4\u4f30\u7b54\u6848\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u8fdb\u4e00\u6b65\u6269\u5c55\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u4e0d\u4ec5\u63d0\u5347\u4e86\u540e\u8bad\u7ec3\u6027\u80fd\uff0c\u8fd8\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002", "conclusion": "\u81ea\u9a8c\u8bc1\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u5206\u5e03\u5dee\u5f02\u95ee\u9898\uff0c\u63d0\u5347LLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "keywords": "\u81ea\u9a8c\u8bc1, \u5f3a\u5316\u5b66\u4e60, LLMs, \u6570\u5b66\u63a8\u7406"}}
{"id": "2506.01407", "pdf": "https://arxiv.org/pdf/2506.01407", "abs": "https://arxiv.org/abs/2506.01407", "authors": ["Olga Zamaraeva", "Dan Flickinger", "Francis Bond", "Carlos G\u00f3mez-Rodr\u00edguez"], "title": "Comparing LLM-generated and human-authored news text using formal syntactic theory", "categories": ["cs.CL"], "comment": "20 pages, 15 figures, 13 tables; accepted to ACL-2025 main", "summary": "This study provides the first comprehensive comparison of New York\nTimes-style text generated by six large language models against real,\nhuman-authored NYT writing. The comparison is based on a formal syntactic\ntheory. We use Head-driven Phrase Structure Grammar (HPSG) to analyze the\ngrammatical structure of the texts. We then investigate and illustrate the\ndifferences in the distributions of HPSG grammar types, revealing systematic\ndistinctions between human and LLM-generated writing. These findings contribute\nto a deeper understanding of the syntactic behavior of LLMs as well as humans,\nwithin the NYT genre.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5168\u9762\u6bd4\u8f83\u4e86\u516d\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u300a\u7ebd\u7ea6\u65f6\u62a5\u300b\u98ce\u683c\u6587\u672c\u4e0e\u771f\u5b9e\u4eba\u7c7b\u64b0\u5199\u7684\u6587\u672c\uff0c\u57fa\u4e8e\u5f62\u5f0f\u53e5\u6cd5\u7406\u8bbaHPSG\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u7684\u8bed\u6cd5\u5206\u5e03\u5dee\u5f02\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u5728\u300a\u7ebd\u7ea6\u65f6\u62a5\u300b\u98ce\u683c\u6587\u672c\u4e0a\u7684\u53e5\u6cd5\u884c\u4e3a\u5dee\u5f02\u3002", "method": "\u4f7f\u7528HPSG\u5bf9\u6587\u672c\u7684\u8bed\u6cd5\u7ed3\u6784\u8fdb\u884c\u5206\u6790\uff0c\u6bd4\u8f83\u8bed\u6cd5\u7c7b\u578b\u7684\u5206\u5e03\u3002", "result": "\u63ed\u793a\u4e86\u4eba\u7c7b\u4e0eLLM\u751f\u6210\u6587\u672c\u5728HPSG\u8bed\u6cd5\u7c7b\u578b\u5206\u5e03\u4e0a\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u589e\u8fdb\u4e86\u5bf9LLM\u548c\u4eba\u7c7b\u5728\u7279\u5b9a\u6587\u672c\u7c7b\u578b\u4e2d\u53e5\u6cd5\u884c\u4e3a\u7684\u7406\u89e3\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, HPSG, \u53e5\u6cd5\u5206\u6790, \u7ebd\u7ea6\u65f6\u62a5, \u6587\u672c\u751f\u6210"}}
{"id": "2506.01374", "pdf": "https://arxiv.org/pdf/2506.01374", "abs": "https://arxiv.org/abs/2506.01374", "authors": ["Sujun Tang", "Christopher Priebe", "Rohan Mahapatra", "Lianhui Qin", "Hadi Esmaeilzadeh"], "title": "Compiler Optimization via LLM Reasoning for Efficient Model Serving", "categories": ["cs.LG", "cs.AI", "cs.PL"], "comment": null, "summary": "While model serving has unlocked unprecedented capabilities, the high cost of\nserving large-scale models continues to be a significant barrier to widespread\naccessibility and rapid innovation. Compiler optimizations have long driven\nsubstantial performance improvements, but existing compilers struggle with\nneural workloads due to the exponentially large and highly interdependent space\nof possible transformations. Although existing stochastic search techniques can\nbe effective, they are often sample-inefficient and fail to leverage the\nstructural context underlying compilation decisions. We set out to investigate\nthe research question of whether reasoning with large language models (LLMs),\nwithout any retraining, can leverage the context-aware decision space of\ncompiler optimization to significantly improve sample efficiency. To that end,\nwe introduce a novel compilation framework (dubbed REASONING COMPILER) that\nformulates optimization as a sequential, context-aware decision process, guided\nby a large language model and structured Monte Carlo tree search (MCTS). The\nLLM acts as a proposal mechanism, suggesting hardware-aware transformations\nthat reflect the current program state and accumulated performance feedback.\nMonte Carlo tree search (MCTS) incorporates the LLM-generated proposals to\nbalance exploration and exploitation, facilitating structured,\ncontext-sensitive traversal of the expansive compiler optimization space. By\nachieving substantial speedups with markedly fewer samples than leading neural\ncompilers, our approach demonstrates the potential of LLM-guided reasoning to\ntransform the landscape of compiler optimization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u662f\u5426\u53ef\u4ee5\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u51b3\u7b56\u6765\u63d0\u5347\u7f16\u8bd1\u5668\u4f18\u5316\u7684\u6837\u672c\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u7f16\u8bd1\u5668\u4f18\u5316\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u5927\u89c4\u6a21\u6a21\u578b\u7684\u9ad8\u670d\u52a1\u6210\u672c\u4ecd\u662f\u5e7f\u6cdb\u53ef\u8bbf\u95ee\u6027\u548c\u5feb\u901f\u521b\u65b0\u7684\u969c\u788d\u3002\u73b0\u6709\u7f16\u8bd1\u5668\u96be\u4ee5\u5904\u7406\u795e\u7ecf\u7f51\u7edc\u5de5\u4f5c\u8d1f\u8f7d\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aREASONING COMPILER\u7684\u65b0\u7f16\u8bd1\u6846\u67b6\uff0c\u7ed3\u5408LLM\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\uff0c\u5c06\u4f18\u5316\u5efa\u6a21\u4e3a\u987a\u5e8f\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u901a\u8fc7\u6bd4\u9886\u5148\u7684\u795e\u7ecf\u7f16\u8bd1\u5668\u66f4\u5c11\u7684\u6837\u672c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u3002", "conclusion": "LLM\u5f15\u5bfc\u7684\u63a8\u7406\u6709\u6f5c\u529b\u6539\u53d8\u7f16\u8bd1\u5668\u4f18\u5316\u7684\u683c\u5c40\u3002", "keywords": "\u6a21\u578b\u670d\u52a1, \u7f16\u8bd1\u5668\u4f18\u5316, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22, \u6837\u672c\u6548\u7387"}}
{"id": "2506.01419", "pdf": "https://arxiv.org/pdf/2506.01419", "abs": "https://arxiv.org/abs/2506.01419", "authors": ["Joseph Marvin Imperial", "Abdullah Barayan", "Regina Stodden", "Rodrigo Wilkens", "Ricardo Munoz Sanchez", "Lingyun Gao", "Melissa Torgbi", "Dawn Knight", "Gail Forey", "Reka R. Jablonkai", "Ekaterina Kochmar", "Robert Reynolds", "Eugenio Ribeiro", "Horacio Saggion", "Elena Volodina", "Sowmya Vajjala", "Thomas Francois", "Fernando Alva-Manchego", "Harish Tayyar Madabushi"], "title": "UniversalCEFR: Enabling Open Multilingual Research on Language Proficiency Assessment", "categories": ["cs.CL"], "comment": null, "summary": "We introduce UniversalCEFR, a large-scale multilingual multidimensional\ndataset of texts annotated according to the CEFR (Common European Framework of\nReference) scale in 13 languages. To enable open research in both automated\nreadability and language proficiency assessment, UniversalCEFR comprises\n505,807 CEFR-labeled texts curated from educational and learner-oriented\nresources, standardized into a unified data format to support consistent\nprocessing, analysis, and modeling across tasks and languages. To demonstrate\nits utility, we conduct benchmark experiments using three modelling paradigms:\na) linguistic feature-based classification, b) fine-tuning pre-trained LLMs,\nand c) descriptor-based prompting of instruction-tuned LLMs. Our results\nfurther support using linguistic features and fine-tuning pretrained models in\nmultilingual CEFR level assessment. Overall, UniversalCEFR aims to establish\nbest practices in data distribution in language proficiency research by\nstandardising dataset formats and promoting their accessibility to the global\nresearch community.", "AI": {"tldr": "UniversalCEFR\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u591a\u7ef4\u5ea6\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b13\u79cd\u8bed\u8a00\u7684\u6587\u672c\uff0c\u6807\u6ce8\u4e86CEFR\u7b49\u7ea7\u3002\u6570\u636e\u96c6\u5305\u542b505,807\u4e2a\u6587\u672c\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u53ef\u8bfb\u6027\u548c\u8bed\u8a00\u80fd\u529b\u8bc4\u4f30\u7814\u7a76\u3002", "motivation": "\u4e3a\u81ea\u52a8\u5316\u53ef\u8bfb\u6027\u548c\u8bed\u8a00\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u5f00\u653e\u7684\u3001\u6807\u51c6\u5316\u7684\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u652f\u6301\u5168\u7403\u7814\u7a76\u793e\u533a\u7684\u7edf\u4e00\u7814\u7a76\u548c\u5efa\u6a21\u3002", "method": "\u901a\u8fc7\u4e09\u79cd\u6a21\u578b\u8303\u5f0f\u8fdb\u884c\u57fa\u51c6\u5b9e\u9a8c\uff1a\u57fa\u4e8e\u8bed\u8a00\u7279\u5f81\u7684\u5206\u7c7b\u3001\u5fae\u8c03\u9884\u8bad\u7ec3\u5927\u6a21\u578b\uff0c\u4ee5\u53ca\u57fa\u4e8e\u63cf\u8ff0\u7b26\u7684\u6307\u4ee4\u8c03\u4f18\u5927\u6a21\u578b\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u652f\u6301\u5728CEFR\u7b49\u7ea7\u8bc4\u4f30\u4e2d\u4f7f\u7528\u8bed\u8a00\u7279\u5f81\u548c\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "UniversalCEFR\u901a\u8fc7\u6807\u51c6\u5316\u6570\u636e\u683c\u5f0f\u548c\u63d0\u5347\u53ef\u8bbf\u95ee\u6027\uff0c\u65e8\u5728\u63a8\u52a8\u8bed\u8a00\u80fd\u529b\u7814\u7a76\u4e2d\u7684\u6570\u636e\u5171\u4eab\u548c\u7814\u7a76\u5b9e\u8df5\u3002", "keywords": "UniversalCEFR, CEFR, \u591a\u8bed\u8a00\u6570\u636e\u96c6, \u8bed\u8a00\u80fd\u529b\u8bc4\u4f30, \u81ea\u52a8\u5316\u53ef\u8bfb\u6027"}}
{"id": "2506.00411", "pdf": "https://arxiv.org/pdf/2506.00411", "abs": "https://arxiv.org/abs/2506.00411", "authors": ["Yi Yang", "Jiaxuan Sun", "Siqi Kou", "Yihan Wang", "Zhijie Deng"], "title": "LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Real-world embodied agents face long-horizon tasks, characterized by\nhigh-level goals demanding multi-step solutions beyond single actions.\nSuccessfully navigating these requires both high-level task planning (i.e.,\ndecomposing goals into sub-tasks) and low-level motion control (i.e.,\ngenerating precise robot actions). While existing vision language action (VLA)\nmodels and hierarchical architectures offer potential in embodied tasks, the\nformer often falter in planning, and the latter can suffer from coordination\nissues, both hampering performance. We introduce a new unified VLA framework\nfor long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA\nleverages a large pretrained vision language model (VLM) as the backbone to\njointly generate language and action tokens for sub-task generation and robot\naction prediction, respectively. This shared representation promotes better\ngeneralization across tasks. Additionally, LoHoVLA embraces a hierarchical\nclosed-loop control mechanism to mitigate errors originating from both\nhigh-level planning and low-level control. To train LoHoVLA, we introduce\nLoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon\ntasks, each with 1,000 expert demonstrations composed of visual observations,\nlinguistic goals, sub-tasks, and robot actions. Experimental results show that\nLoHoVLA significantly surpasses both hierarchical and standard VLA approaches\non long-horizon embodied tasks in the Ravens simulator. These findings\nunderscore the promise of unified architectures for advancing generalizable\nembodied intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLoHoVLA\u7684\u7edf\u4e00\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u9ad8\u5c42\u6b21\u7684\u5b50\u4efb\u52a1\u5206\u89e3\u548c\u4f4e\u5c42\u6b21\u7684\u8fd0\u52a8\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5728\u4efb\u52a1\u89c4\u5212\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u5206\u5c42\u67b6\u6784\u5728\u534f\u8c03\u4e0a\u6709\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "LoHoVLA\u7ed3\u5408\u4e86\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u9aa8\u5e72\uff0c\u8054\u5408\u751f\u6210\u8bed\u8a00\u548c\u52a8\u4f5c\u6807\u8bb0\uff0c\u7528\u4e8e\u5b50\u4efb\u52a1\u751f\u6210\u548c\u673a\u5668\u4eba\u52a8\u4f5c\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528\u5206\u5c42\u95ed\u73af\u63a7\u5236\u673a\u5236\u51cf\u5c11\u9519\u8bef\u3002", "result": "\u5728Ravens\u6a21\u62df\u5668\u4e0a\u768420\u4e2a\u957f\u65f6\u7a0b\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0cLoHoVLA\u663e\u8457\u4f18\u4e8e\u5206\u5c42\u548c\u6807\u51c6VLA\u65b9\u6cd5\u3002", "conclusion": "\u7edf\u4e00\u7684\u67b6\u6784\u6709\u671b\u63a8\u52a8\u53ef\u6cdb\u5316\u7684\u5d4c\u5165\u5f0f\u667a\u80fd\u7684\u53d1\u5c55\u3002", "keywords": "\u957f\u65f6\u7a0b\u4efb\u52a1,\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b,\u5206\u5c42\u63a7\u5236,\u673a\u5668\u4eba\u52a8\u4f5c\u9884\u6d4b,\u5d4c\u5165\u5f0f\u667a\u80fd"}}
{"id": "2506.01376", "pdf": "https://arxiv.org/pdf/2506.01376", "abs": "https://arxiv.org/abs/2506.01376", "authors": ["Minghao Xu", "Jiaze Song", "Keming Wu", "Xiangxin Zhou", "Bin Cui", "Wentao Zhang"], "title": "Modeling All-Atom Glycan Structures via Hierarchical Message Passing and Multi-Scale Pre-training", "categories": ["cs.LG"], "comment": "Published at ICML 2025. All code and data are released", "summary": "Understanding the various properties of glycans with machine learning has\nshown some preliminary promise. However, previous methods mainly focused on\nmodeling the backbone structure of glycans as graphs of monosaccharides (i.e.,\nsugar units), while they neglected the atomic structures underlying each\nmonosaccharide, which are actually important indicators of glycan properties.\nWe fill this blank by introducing the GlycanAA model for All-Atom-wise Glycan\nmodeling. GlycanAA models a glycan as a heterogeneous graph with monosaccharide\nnodes representing its global backbone structure and atom nodes representing\nits local atomic-level structures. Based on such a graph, GlycanAA performs\nhierarchical message passing to capture from local atomic-level interactions to\nglobal monosaccharide-level interactions. To further enhance model capability,\nwe pre-train GlycanAA on a high-quality unlabeled glycan dataset, deriving the\nPreGlycanAA model. We design a multi-scale mask prediction algorithm to endow\nthe model about different levels of dependencies in a glycan. Extensive\nbenchmark results show the superiority of GlycanAA over existing glycan\nencoders and verify the further improvements achieved by PreGlycanAA. We\nmaintain all resources at https://github.com/kasawa1234/GlycanAA", "AI": {"tldr": "GlycanAA\u6a21\u578b\u901a\u8fc7\u5168\u539f\u5b50\u7ea7\u522b\u7684\u7cd6\u94fe\u5efa\u6a21\uff0c\u5f25\u8865\u4e86\u4ee5\u5f80\u65b9\u6cd5\u5ffd\u7565\u5355\u7cd6\u539f\u5b50\u7ed3\u6784\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u5c42\u6b21\u5316\u6d88\u606f\u4f20\u9012\u548c\u9884\u8bad\u7ec3\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7cd6\u94fe\u5efa\u6a21\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u7cd6\u9aa8\u67b6\u7ed3\u6784\uff0c\u5ffd\u7565\u4e86\u539f\u5b50\u7ea7\u522b\u7ed3\u6784\u7684\u91cd\u8981\u6027\uff0cGlycanAA\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "GlycanAA\u5c06\u7cd6\u94fe\u5efa\u6a21\u4e3a\u5f02\u6784\u56fe\uff0c\u5305\u542b\u5355\u7cd6\u8282\u70b9\u548c\u539f\u5b50\u8282\u70b9\uff0c\u5e76\u901a\u8fc7\u5c42\u6b21\u5316\u6d88\u606f\u4f20\u9012\u548c\u9884\u8bad\u7ec3\uff08PreGlycanAA\uff09\u589e\u5f3a\u6a21\u578b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eGlycanAA\u4f18\u4e8e\u73b0\u6709\u7cd6\u94fe\u7f16\u7801\u5668\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "GlycanAA\u901a\u8fc7\u5168\u539f\u5b50\u5efa\u6a21\u548c\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cd6\u94fe\u6027\u8d28\u9884\u6d4b\u7684\u6027\u80fd\u3002", "keywords": "\u7cd6\u94fe\u5efa\u6a21, \u673a\u5668\u5b66\u4e60, \u5f02\u6784\u56fe, \u9884\u8bad\u7ec3"}}
{"id": "2506.01420", "pdf": "https://arxiv.org/pdf/2506.01420", "abs": "https://arxiv.org/abs/2506.01420", "authors": ["Kyuyoung Kim", "Hyunjun Jeon", "Jinwoo Shin"], "title": "Self-Refining Language Model Anonymizers via Adversarial Distillation", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly used in sensitive domains,\nwhere their ability to infer personal data from seemingly benign text poses\nemerging privacy risks. While recent LLM-based anonymization methods help\nmitigate such risks, they often rely on proprietary models (e.g., GPT-4),\nraising concerns about cost and the potential exposure of sensitive data to\nuntrusted external systems. To address this, we introduce SElf-refining\nAnonymization with Language model (SEAL), a novel distillation framework for\ntraining small language models (SLMs) to perform effective anonymization\nwithout relying on external costly models at inference time. We leverage\nadversarial interactions between an LLM anonymizer and an inference model to\ncollect trajectories of anonymized texts and inferred attributes, which are\nused to distill anonymization, adversarial inference, and utility evaluation\ncapabilities into SLMs via supervised fine-tuning and preference learning. The\nresulting models learn to both anonymize text and critique their outputs,\nenabling iterative improvement of anonymization quality via self-refinement.\nExperiments on SynthPAI, a dataset of synthetic personal profiles and text\ncomments, demonstrate that SLMs trained with SEAL achieve substantial\nimprovements in anonymization capabilities. Notably, 8B models attain a\nprivacy-utility trade-off comparable to that of the GPT-4 anonymizer and, with\nself-refinement, even surpass it in terms of privacy. These results show the\neffectiveness of our adversarial distillation framework in training SLMs as\nefficient anonymizers. To facilitate further research, we release the full\ndataset used in our experiments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSEAL\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u84b8\u998f\u6280\u672f\u8bad\u7ec3\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\uff0c\u4ee5\u5728\u4e0d\u4f9d\u8d56\u5916\u90e8\u6602\u8d35\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u533f\u540d\u5316\u6587\u672c\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u654f\u611f\u9886\u57df\u4e2d\u7684\u4f7f\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u5176\u4ece\u770b\u4f3c\u65e0\u5bb3\u7684\u6587\u672c\u4e2d\u63a8\u65ad\u4e2a\u4eba\u4fe1\u606f\u7684\u80fd\u529b\u5f15\u53d1\u9690\u79c1\u98ce\u9669\u3002\u73b0\u6709\u7684\u533f\u540d\u5316\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e13\u6709\u6a21\u578b\uff08\u5982GPT-4\uff09\uff0c\u589e\u52a0\u4e86\u6210\u672c\u548c\u6570\u636e\u6cc4\u9732\u7684\u98ce\u9669\u3002", "method": "SEAL\u6846\u67b6\u901a\u8fc7LLM\u533f\u540d\u5668\u548c\u63a8\u65ad\u6a21\u578b\u7684\u5bf9\u6297\u4ea4\u4e92\uff0c\u6536\u96c6\u533f\u540d\u5316\u6587\u672c\u548c\u63a8\u65ad\u5c5e\u6027\u7684\u8f68\u8ff9\uff0c\u5e76\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u548c\u504f\u597d\u5b66\u4e60\u5c06\u533f\u540d\u5316\u3001\u5bf9\u6297\u63a8\u65ad\u548c\u6548\u7528\u8bc4\u4f30\u80fd\u529b\u84b8\u998f\u5230SLMs\u4e2d\u3002", "result": "\u5728SynthPAI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSEAL\u8bad\u7ec3\u7684SLMs\u5728\u533f\u540d\u5316\u80fd\u529b\u4e0a\u6709\u663e\u8457\u63d0\u5347\u30028B\u6a21\u578b\u5728\u9690\u79c1\u4e0e\u6548\u7528\u6743\u8861\u4e0a\u5ab2\u7f8eGPT-4\u533f\u540d\u5668\uff0c\u5e76\u901a\u8fc7\u81ea\u7ec6\u5316\u5728\u9690\u79c1\u65b9\u9762\u8d85\u8d8a\u5b83\u3002", "conclusion": "SEAL\u6846\u67b6\u8bc1\u660e\u4e86\u901a\u8fc7\u5bf9\u6297\u84b8\u998f\u8bad\u7ec3SLMs\u4f5c\u4e3a\u9ad8\u6548\u533f\u540d\u5316\u5de5\u5177\u7684\u53ef\u884c\u6027\uff0c\u5e76\u516c\u5f00\u4e86\u5b9e\u9a8c\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u9690\u79c1\u98ce\u9669, \u533f\u540d\u5316, \u5bf9\u6297\u84b8\u998f, \u5c0f\u578b\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.01386", "pdf": "https://arxiv.org/pdf/2506.01386", "abs": "https://arxiv.org/abs/2506.01386", "authors": ["Manit Baser", "Dinil Mon Divakaran", "Mohan Gurusamy"], "title": "ThinkEval: Practical Evaluation of Knowledge Preservation and Consistency in LLM Editing with Thought-based Knowledge Graphs", "categories": ["cs.LG"], "comment": null, "summary": "Model editing has become an important tool for addressing privacy, bias, and\nmisinformation in large language models (LLMs) by enabling updates to knowledge\nwithout the need for retraining from scratch. However, existing editing\ntechniques often target isolated facts, ignoring ripple effects on related\nknowledge, allowing edited facts to remain deducible and compromising broader\ncontextual integrity. For example, changing Harry Potter's school from Hogwarts\nto Ilvermorny requires reassigning his house from Gryffindor to a suitable\nalternative while preserving Gryffindor's relationship with Hogwarts. In this\nwork, we present a new model-editing setting, deep editing, to show: (1) how\nediting techniques fail to handle connected facts, evaluating how original\nknowledge sneaks through unchanged causal links, and (2) their impact on\nbroader contextual knowledge. We introduce ThinkEval, a framework to\nsystematically evaluate model-editing techniques by building model-specific\nknowledge graphs to analyze pre- and post-edit effects on fact persistence and\ncatastrophic forgetting. We present KnowGIC, a benchmark created with\nThinkEval, consisting of sequentially linked queries to measure these effects.\nWe evaluate five editing techniques: AlphaEdit, RECT, ROME, MEMIT, and PRUNE\nacross multiple LLMs. We find that these techniques struggle to balance\nindirect fact suppression with the preservation of related knowledge. Our\ndataset is available at: https://anonymous.4open.science/r/KnowGIC.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u7f16\u8f91\u8bbe\u7f6e\u201c\u6df1\u5ea6\u7f16\u8f91\u201d\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u7f16\u8f91\u6280\u672f\u65e0\u6cd5\u5904\u7406\u5173\u8054\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86ThinkEval\u6846\u67b6\u548cKnowGIC\u57fa\u51c6\u6765\u8bc4\u4f30\u7f16\u8f91\u6280\u672f\u7684\u5f71\u54cd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7f16\u8f91\u6280\u672f\u901a\u5e38\u53ea\u9488\u5bf9\u5b64\u7acb\u4e8b\u5b9e\uff0c\u5ffd\u7565\u4e86\u5173\u8054\u77e5\u8bc6\u7684\u8fde\u9501\u53cd\u5e94\uff0c\u5bfc\u81f4\u7f16\u8f91\u540e\u7684\u77e5\u8bc6\u5b8c\u6574\u6027\u53d7\u635f\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u6df1\u5ea6\u7f16\u8f91\u8bbe\u7f6e\uff0c\u5e76\u5f00\u53d1\u4e86ThinkEval\u6846\u67b6\u548cKnowGIC\u57fa\u51c6\uff0c\u901a\u8fc7\u6784\u5efa\u7279\u5b9a\u77e5\u8bc6\u56fe\u8c31\u6765\u8bc4\u4f30\u7f16\u8f91\u6280\u672f\u5bf9\u4e8b\u5b9e\u6301\u4e45\u6027\u548c\u707e\u96be\u6027\u9057\u5fd8\u7684\u5f71\u54cd\u3002", "result": "\u8bc4\u4f30\u4e86\u4e94\u79cd\u7f16\u8f91\u6280\u672f\u5728\u591a\u4e2aLLM\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6280\u672f\u96be\u4ee5\u5e73\u8861\u95f4\u63a5\u4e8b\u5b9e\u6291\u5236\u4e0e\u5173\u8054\u77e5\u8bc6\u4fdd\u5b58\u3002", "conclusion": "\u73b0\u6709\u7f16\u8f91\u6280\u672f\u5728\u5904\u7406\u5173\u8054\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u672a\u6765\u7684\u7814\u7a76\u9700\u8981\u6539\u8fdb\u8fd9\u4e00\u70b9\u3002", "keywords": "\u6a21\u578b\u7f16\u8f91, \u5927\u8bed\u8a00\u6a21\u578b, ThinkEval, KnowGIC, \u6df1\u5ea6\u7f16\u8f91"}}
{"id": "2506.01435", "pdf": "https://arxiv.org/pdf/2506.01435", "abs": "https://arxiv.org/abs/2506.01435", "authors": ["Hayato Tsukagoshi", "Ryohei Sasano"], "title": "Redundancy, Isotropy, and Intrinsic Dimensionality of Prompt-based Text Embeddings", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Prompt-based text embedding models, which generate task-specific embeddings\nupon receiving tailored prompts, have recently demonstrated remarkable\nperformance. However, their resulting embeddings often have thousands of\ndimensions, leading to high storage costs and increased computational costs of\nembedding-based operations. In this paper, we investigate how post-hoc\ndimensionality reduction applied to the embeddings affects the performance of\nvarious tasks that leverage these embeddings, specifically classification,\nclustering, retrieval, and semantic textual similarity (STS) tasks. Our\nexperiments show that even a naive dimensionality reduction, which keeps only\nthe first 25% of the dimensions of the embeddings, results in a very slight\nperformance degradation, indicating that these embeddings are highly redundant.\nNotably, for classification and clustering, even when embeddings are reduced to\nless than 0.5% of the original dimensionality the performance degradation is\nvery small. To quantitatively analyze this redundancy, we perform an analysis\nbased on the intrinsic dimensionality and isotropy of the embeddings. Our\nanalysis reveals that embeddings for classification and clustering, which are\nconsidered to have very high dimensional redundancy, exhibit lower intrinsic\ndimensionality and less isotropy compared with those for retrieval and STS.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u5bf9\u9ad8\u7ef4\u63d0\u793a\u5d4c\u5165\u8fdb\u884c\u964d\u7ef4\u5bf9\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5373\u4f7f\u5927\u5e45\u964d\u7ef4\uff0c\u6027\u80fd\u4e0b\u964d\u5f88\u5c0f\uff0c\u63ed\u793a\u4e86\u5d4c\u5165\u7684\u9ad8\u5ea6\u5197\u4f59\u6027\u3002", "motivation": "\u63d0\u793a\u5d4c\u5165\u6a21\u578b\u867d\u7136\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u9ad8\u7ef4\u5ea6\u5e26\u6765\u5b58\u50a8\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u7814\u7a76\u964d\u7ef4\u5bf9\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u5bf9\u5d4c\u5165\u5e94\u7528\u540e\u5904\u7406\u964d\u7ef4\uff0c\u5e76\u901a\u8fc7\u5185\u5728\u7ef4\u5ea6\u548c\u5404\u5411\u540c\u6027\u5206\u6790\u5197\u4f59\u6027\u3002", "result": "\u5373\u4f7f\u964d\u7ef4\u5230\u539f\u59cb\u7ef4\u5ea6\u768425%\u751a\u81f30.5%\uff0c\u5206\u7c7b\u548c\u805a\u7c7b\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\u5f88\u5c0f\uff0c\u63ed\u793a\u4e86\u5d4c\u5165\u7684\u9ad8\u5ea6\u5197\u4f59\u6027\u3002", "conclusion": "\u9ad8\u7ef4\u63d0\u793a\u5d4c\u5165\u5177\u6709\u663e\u8457\u5197\u4f59\u6027\uff0c\u964d\u7ef4\u53ef\u663e\u8457\u964d\u4f4e\u6210\u672c\u800c\u4e0d\u663e\u8457\u5f71\u54cd\u6027\u80fd\u3002", "keywords": "\u63d0\u793a\u5d4c\u5165, \u964d\u7ef4, \u5185\u5728\u7ef4\u5ea6, \u5404\u5411\u540c\u6027, \u5197\u4f59\u6027"}}
{"id": "2506.00415", "pdf": "https://arxiv.org/pdf/2506.00415", "abs": "https://arxiv.org/abs/2506.00415", "authors": ["Matthew Brophy"], "title": "Wide Reflective Equilibrium in LLM Alignment: Bridging Moral Epistemology and AI Safety", "categories": ["cs.CY", "cs.AI"], "comment": "24 pages excluding references, 3 tables", "summary": "As large language models (LLMs) become more powerful and pervasive across\nsociety, ensuring these systems are beneficial, safe, and aligned with human\nvalues is crucial. Current alignment techniques, like Constitutional AI (CAI),\ninvolve complex iterative processes. This paper argues that the Method of Wide\nReflective Equilibrium (MWRE) -- a well-established coherentist moral\nmethodology -- offers a uniquely apt framework for understanding current LLM\nalignment efforts. Moreover, this methodology can substantively augment these\nprocesses by providing concrete pathways for improving their dynamic\nrevisability, procedural legitimacy, and overall ethical grounding. Together,\nthese enhancements can help produce more robust and ethically defensible\noutcomes. MWRE, emphasizing the achievement of coherence between our considered\nmoral judgments, guiding moral principles, and relevant background theories,\narguably better represents the intricate reality of LLM alignment and offers a\nmore robust path to justification than prevailing foundationalist models or\nsimplistic input-output evaluations. While current methods like CAI bear a\nstructural resemblance to MWRE, they often lack its crucial emphasis on\ndynamic, bi-directional revision of principles and the procedural legitimacy\nderived from such a process. While acknowledging various disanalogies (e.g.,\nconsciousness, genuine understanding in LLMs), the paper demonstrates that MWRE\nserves as a valuable heuristic for critically analyzing current alignment\nefforts and for guiding the future development of more ethically sound and\njustifiably aligned AI systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u5e7f\u6cdb\u53cd\u601d\u5e73\u8861\u6cd5\uff08MWRE\uff09\u6765\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5bf9\u9f50\u5de5\u4f5c\uff0c\u6307\u51fa\u5176\u5728\u52a8\u6001\u53ef\u4fee\u8ba2\u6027\u3001\u7a0b\u5e8f\u5408\u6cd5\u6027\u548c\u4f26\u7406\u57fa\u7840\u65b9\u9762\u7684\u4f18\u52bf\u3002", "motivation": "\u5f53\u524d\u7684\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982\u5baa\u6cd5AI\uff09\u8fc7\u7a0b\u590d\u6742\uff0cMWRE\u4f5c\u4e3a\u4e00\u79cd\u6210\u719f\u7684\u9053\u5fb7\u65b9\u6cd5\u8bba\uff0c\u53ef\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u8df5\u8def\u5f84\u3002", "method": "\u901a\u8fc7MWRE\u65b9\u6cd5\u8bba\uff0c\u5f3a\u8c03\u9053\u5fb7\u5224\u65ad\u3001\u9053\u5fb7\u539f\u5219\u548c\u80cc\u666f\u7406\u8bba\u95f4\u7684\u8fde\u8d2f\u6027\uff0c\u5e76\u52a8\u6001\u53cc\u5411\u4fee\u8ba2\u539f\u5219\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u9f50\u8fc7\u7a0b\u7684\u7a0b\u5e8f\u5408\u6cd5\u6027\u548c\u4f26\u7406\u57fa\u7840\u3002", "result": "MWRE\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u5177\u8fa9\u62a4\u6027\u548c\u4f26\u7406\u6307\u5bfc\u610f\u4e49\u7684\u6846\u67b6\uff0c\u4f18\u4e8e\u5f53\u524d\u7684\u4e3b\u6d41\u57fa\u7840\u4e3b\u4e49\u6a21\u578b\u6216\u7b80\u5355\u8f93\u5165\u8f93\u51fa\u8bc4\u4f30\u3002", "conclusion": "MWRE\u53ef\u4f5c\u4e3a\u5206\u6790\u548c\u6539\u8fdbLLM\u5bf9\u9f50\u5de5\u4f5c\u7684\u542f\u53d1\u5f0f\u5de5\u5177\uff0c\u5e2e\u52a9\u5f00\u53d1\u66f4\u5177\u4f26\u7406\u5408\u7406\u6027\u7684AI\u7cfb\u7edf\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u5bf9\u9f50,\u5e7f\u6cdb\u53cd\u601d\u5e73\u8861\u6cd5,\u5baa\u6cd5AI,\u4f26\u7406\u57fa\u7840"}}
{"id": "2506.01387", "pdf": "https://arxiv.org/pdf/2506.01387", "abs": "https://arxiv.org/abs/2506.01387", "authors": ["Paritosh Ranjan", "Surajit Majumder", "Prodip Roy"], "title": "Multi Part Deployment of Neural Network", "categories": ["cs.LG", "cs.NE"], "comment": "7 pages, 1 figures", "summary": "The increasing scale of modern neural networks, exemplified by architectures\nfrom IBM (530 billion neurons) and Google (500 billion parameters), presents\nsignificant challenges in terms of computational cost and infrastructure\nrequirements. As deep neural networks continue to grow, traditional training\nparadigms relying on monolithic GPU clusters become increasingly unsustainable.\nThis paper proposes a distributed system architecture that partitions a neural\nnetwork across multiple servers, each responsible for a subset of neurons.\nNeurons are classified as local or remote, with inter-server connections\nmanaged via a metadata-driven lookup mechanism. A Multi-Part Neural Network\nExecution Engine facilitates seamless execution and training across distributed\npartitions by dynamically resolving and invoking remote neurons using stored\nmetadata. All servers share a unified model through a network file system\n(NFS), ensuring consistency during parallel updates. A Neuron Distributor\nmodule enables flexible partitioning strategies based on neuron count,\npercentage, identifiers, or network layers. This architecture enables\ncost-effective, scalable deployment of deep learning models on cloud\ninfrastructure, reducing dependency on high-performance centralized compute\nresources.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u5e03\u5f0f\u7cfb\u7edf\u67b6\u6784\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u5206\u5e03\u5230\u591a\u4e2a\u670d\u52a1\u5668\u4e0a\uff0c\u901a\u8fc7\u5143\u6570\u636e\u9a71\u52a8\u7684\u67e5\u627e\u673a\u5236\u7ba1\u7406\u8fdc\u7a0b\u795e\u7ecf\u5143\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u89c4\u6a21\u8d8a\u6765\u8d8a\u5927\uff0c\u4f20\u7edf\u7684\u96c6\u4e2d\u5f0fGPU\u96c6\u7fa4\u8bad\u7ec3\u65b9\u5f0f\u6210\u672c\u9ad8\u4e14\u4e0d\u53ef\u6301\u7eed\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5206\u5e03\u5f0f\u7cfb\u7edf\u67b6\u6784\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u5206\u533a\u5230\u591a\u53f0\u670d\u52a1\u5668\uff0c\u6bcf\u53f0\u670d\u52a1\u5668\u8d1f\u8d23\u4e00\u90e8\u5206\u795e\u7ecf\u5143\uff0c\u901a\u8fc7\u5143\u6570\u636e\u9a71\u52a8\u67e5\u627e\u8fdc\u7a0b\u795e\u7ecf\u5143\uff0c\u5e76\u5229\u7528NFS\u4fdd\u6301\u6a21\u578b\u4e00\u81f4\u6027\u3002", "result": "\u8be5\u67b6\u6784\u964d\u4f4e\u4e86\u9ad8\u6027\u80fd\u96c6\u4e2d\u5f0f\u8ba1\u7b97\u8d44\u6e90\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4e91\u7aef\u7684\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u90e8\u7f72\u3002", "conclusion": "\u5206\u5e03\u5f0f\u7cfb\u7edf\u67b6\u6784\u80fd\u591f\u6709\u6548\u652f\u6301\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u548c\u90e8\u7f72\uff0c\u5177\u6709\u9ad8\u6269\u5c55\u6027\u548c\u6210\u672c\u6548\u76ca\u3002", "keywords": ""}}
{"id": "2506.01439", "pdf": "https://arxiv.org/pdf/2506.01439", "abs": "https://arxiv.org/abs/2506.01439", "authors": ["Yosuke Kashiwagi", "Hayato Futami", "Emiru Tsunoo", "Satoshi Asakawa"], "title": "Whale: Large-Scale multilingual ASR model with w2v-BERT and E-Branchformer with large speech data", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "This paper reports on the development of a large-scale speech recognition\nmodel, Whale. Similar to models such as Whisper and OWSM, Whale leverages both\na large model size and a diverse, extensive dataset. Whale's architecture\nintegrates w2v-BERT self-supervised model, an encoder-decoder backbone built on\nE-Branchformer, and a joint CTC-attention decoding strategy. The training\ncorpus comprises varied speech data, of not only public corpora but also\nin-house data, thereby enhancing the model's robustness to different speaking\nstyles and acoustic conditions. Through evaluations on multiple benchmarks,\nWhale achieved comparable performance to existing models. In particular, it\nachieves a word error rate of 2.4% on the Librispeech test-clean set and a\ncharacter error rate of 3.4% on the CSJ eval3 set, outperforming Whisper\nlarge-v3 and OWSM v3.1.", "AI": {"tldr": "Whale\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\uff0c\u7ed3\u5408\u4e86w2v-BERT\u81ea\u76d1\u7763\u6a21\u578b\u548cE-Branchformer\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u6837\u5316\u7684\u6570\u636e\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f00\u53d1\u9ad8\u6027\u80fd\u7684\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\uff0c\u4ee5\u5e94\u5bf9\u4e0d\u540c\u8bf4\u8bdd\u98ce\u683c\u548c\u58f0\u5b66\u6761\u4ef6\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528w2v-BERT\u81ea\u76d1\u7763\u6a21\u578b\u548cE-Branchformer\u67b6\u6784\uff0c\u7ed3\u5408\u8054\u5408CTC-\u6ce8\u610f\u529b\u89e3\u7801\u7b56\u7565\uff0c\u5229\u7528\u591a\u6837\u5316\u6570\u636e\u96c6\u8bad\u7ec3\u3002", "result": "\u5728Librispeech\u548cCSJ\u6d4b\u8bd5\u96c6\u4e0a\u5206\u522b\u8fbe\u52302.4%\u548c3.4%\u7684\u9519\u8bef\u7387\uff0c\u4f18\u4e8eWhisper\u548cOWSM\u3002", "conclusion": "Whale\u6a21\u578b\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u548c\u5148\u8fdb\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u3002", "keywords": "\u8bed\u97f3\u8bc6\u522b, Whale, w2v-BERT, E-Branchformer, \u8054\u5408CTC-\u6ce8\u610f\u529b"}}
{"id": "2506.01393", "pdf": "https://arxiv.org/pdf/2506.01393", "abs": "https://arxiv.org/abs/2506.01393", "authors": ["Shogo Iwazaki"], "title": "Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization", "categories": ["cs.LG", "stat.ML"], "comment": "37 pages", "summary": "This paper addresses the Bayesian optimization problem (also referred to as\nthe Bayesian setting of the Gaussian process bandit), where the learner seeks\nto minimize the regret under a function drawn from a known Gaussian process\n(GP). Under a Mat\\'ern kernel with a certain degree of smoothness, we show that\nthe Gaussian process upper confidence bound (GP-UCB) algorithm achieves\n$\\tilde{O}(\\sqrt{T})$ cumulative regret with high probability. Furthermore, our\nanalysis yields $O(\\sqrt{T \\ln^4 T})$ regret under a squared exponential\nkernel. These results fill the gap between the existing regret upper bound for\nGP-UCB and the best-known bound provided by Scarlett (2018). The key idea in\nour proof is to capture the concentration behavior of the input sequence\nrealized by GP-UCB, enabling a more refined analysis of the GP's information\ngain.", "AI": {"tldr": "\u7814\u7a76\u4e86\u8d1d\u53f6\u65af\u4f18\u5316\u95ee\u9898\uff0c\u8bc1\u660eGP-UCB\u7b97\u6cd5\u5728\u7279\u5b9a\u9ad8\u65af\u8fc7\u7a0b\u4e0b\u8fbe\u5230\u8f83\u4f4e\u7d2f\u79ef\u9057\u61be\u3002", "motivation": "\u586b\u8865\u73b0\u6709GP-UCB\u9057\u61be\u4e0a\u754c\u4e0eScarlett (2018)\u6700\u4f18\u754c\u4e4b\u95f4\u7684\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u91c7\u7528Mat\u00e9rn\u6838\u548c\u5e73\u65b9\u6307\u6570\u6838\uff0c\u901a\u8fc7\u5206\u6790\u8f93\u5165\u5e8f\u5217\u7684\u96c6\u4e2d\u6027\uff0c\u4f18\u5316\u9ad8\u65af\u8fc7\u7a0b\u4fe1\u606f\u589e\u76ca\u3002", "result": "\u5728Mat\u00e9rn\u6838\u4e0b\u5b9e\u73b0$\tilde{O}(\\sqrt{T})$\u9057\u61be\uff0c\u5e73\u65b9\u6307\u6570\u6838\u4e0b\u4e3a$O(\\sqrt{T \\ln^4 T})$\u3002", "conclusion": "GP-UCB\u7b97\u6cd5\u5728\u7279\u5b9a\u9ad8\u65af\u8fc7\u7a0b\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u9057\u61be\u4e0a\u754c\u63a5\u8fd1\u6700\u4f18\u3002", "keywords": "\u8d1d\u53f6\u65af\u4f18\u5316, \u9ad8\u65af\u8fc7\u7a0b, GP-UCB, \u9057\u61be\u4e0a\u754c, \u4fe1\u606f\u589e\u76ca"}}
{"id": "2506.01451", "pdf": "https://arxiv.org/pdf/2506.01451", "abs": "https://arxiv.org/abs/2506.01451", "authors": ["Anshika Rawal", "Abhijeet Kumar", "Mridul Mishra"], "title": "Building Entity Association Mining Framework for Knowledge Discovery", "categories": ["cs.CL", "cs.IR", "I.2.7"], "comment": "Presented at Business Analytics and Intelligence Conference, IIM\n  Bengaluru", "summary": "Extracting useful signals or pattern to support important business decisions\nfor example analyzing investment product traction and discovering customer\npreference, risk monitoring etc. from unstructured text is a challenging task.\nCapturing interaction of entities or concepts and association mining is a\ncrucial component in text mining, enabling information extraction and reasoning\nover and knowledge discovery from text. Furthermore, it can be used to enrich\nor filter knowledge graphs to guide exploration processes, descriptive\nanalytics and uncover hidden stories in the text. In this paper, we introduce a\ndomain independent pipeline i.e., generalized framework to enable document\nfiltering, entity extraction using various sources (or techniques) as plug-ins\nand association mining to build any text mining business use-case and\nquantitatively define a scoring metric for ranking purpose. The proposed\nframework has three major components a) Document filtering: filtering\ndocuments/text of interest from massive amount of texts b) Configurable entity\nextraction pipeline: include entity extraction techniques i.e., i) DBpedia\nSpotlight, ii) Spacy NER, iii) Custom Entity Matcher, iv) Phrase extraction (or\ndictionary) based c) Association Relationship Mining: To generates\nco-occurrence graph to analyse potential relationships among entities,\nconcepts. Further, co-occurrence count based frequency statistics provide a\nholistic window to observe association trends or buzz rate in specific business\ncontext. The paper demonstrates the usage of framework as fundamental building\nbox in two financial use-cases namely brand product discovery and vendor risk\nmonitoring. We aim that such framework will remove duplicated effort, minimize\nthe development effort, and encourage reusability and rapid prototyping in\nassociation mining business applications for institutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u6587\u672c\u6316\u6398\u6846\u67b6\uff0c\u7528\u4e8e\u6587\u6863\u8fc7\u6ee4\u3001\u5b9e\u4f53\u63d0\u53d6\u548c\u5173\u8054\u6316\u6398\uff0c\u652f\u6301\u91d1\u878d\u7528\u4f8b\u5982\u54c1\u724c\u4ea7\u54c1\u53d1\u73b0\u548c\u4f9b\u5e94\u5546\u98ce\u9669\u76d1\u63a7\u3002", "motivation": "\u89e3\u51b3\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u63d0\u53d6\u6709\u7528\u4fe1\u53f7\u6216\u6a21\u5f0f\u4ee5\u652f\u6301\u4e1a\u52a1\u51b3\u7b56\uff08\u5982\u6295\u8d44\u4ea7\u54c1\u5206\u6790\u3001\u5ba2\u6237\u504f\u597d\u53d1\u73b0\u548c\u98ce\u9669\u76d1\u63a7\uff09\u7684\u6311\u6218\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a1\uff09\u6587\u6863\u8fc7\u6ee4\uff1b2\uff09\u53ef\u914d\u7f6e\u7684\u5b9e\u4f53\u63d0\u53d6\u7ba1\u9053\uff08\u4f7f\u7528DBpedia Spotlight\u3001Spacy NER\u7b49\u6280\u672f\uff09\uff1b3\uff09\u5173\u8054\u5173\u7cfb\u6316\u6398\uff08\u751f\u6210\u5171\u73b0\u56fe\u5206\u6790\u5b9e\u4f53\u5173\u7cfb\uff09\u3002", "result": "\u6846\u67b6\u5728\u91d1\u878d\u7528\u4f8b\u4e2d\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\uff0c\u51cf\u5c11\u4e86\u91cd\u590d\u5f00\u53d1\u5de5\u4f5c\u5e76\u652f\u6301\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u6587\u672c\u6316\u6398\u7684\u6548\u7387\uff0c\u4fc3\u8fdb\u4e86\u5173\u8054\u6316\u6398\u4e1a\u52a1\u5e94\u7528\u7684\u53ef\u91cd\u7528\u6027\u548c\u5feb\u901f\u5f00\u53d1\u3002", "keywords": "\u6587\u672c\u6316\u6398, \u5b9e\u4f53\u63d0\u53d6, \u5173\u8054\u6316\u6398, \u91d1\u878d\u7528\u4f8b, \u5171\u73b0\u56fe"}}
{"id": "2506.01396", "pdf": "https://arxiv.org/pdf/2506.01396", "abs": "https://arxiv.org/abs/2506.01396", "authors": ["Linzh Zhao", "Aki Rehn", "Mikko A. Heikkil\u00e4", "Razane Tajeddine", "Antti Honkela"], "title": "Mitigating Disparate Impact of Differentially Private Learning through Bounded Adaptive Clipping", "categories": ["cs.LG", "cs.CR", "stat.ML", "I.2.6; K.4.2"], "comment": "NeurIPS 2025 under review. 22 pages, 8 figures", "summary": "Differential privacy (DP) has become an essential framework for\nprivacy-preserving machine learning. Existing DP learning methods, however,\noften have disparate impacts on model predictions, e.g., for minority groups.\nGradient clipping, which is often used in DP learning, can suppress larger\ngradients from challenging samples. We show that this problem is amplified by\nadaptive clipping, which will often shrink the clipping bound to tiny values to\nmatch a well-fitting majority, while significantly reducing the accuracy for\nothers. We propose bounded adaptive clipping, which introduces a tunable lower\nbound to prevent excessive gradient suppression. Our method improves the\naccuracy of the worst-performing class on average over 10 percentage points on\nskewed MNIST and Fashion MNIST compared to the unbounded adaptive clipping, and\nover 5 percentage points over constant clipping.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6709\u754c\u81ea\u9002\u5e94\u88c1\u526a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86DP\u5b66\u4e60\u4e2d\u68af\u5ea6\u88c1\u526a\u5bf9\u5c11\u6570\u7fa4\u4f53\u7684\u4e0d\u516c\u5e73\u5f71\u54cd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u5b66\u4e60\u65b9\u6cd5\u5728\u6a21\u578b\u9884\u6d4b\u4e2d\u5b58\u5728\u4e0d\u5e73\u7b49\u5f71\u54cd\uff0c\u4f8b\u5982\u5bf9\u5c11\u6570\u7fa4\u4f53\u7684\u51c6\u786e\u6027\u8f83\u4f4e\u3002\u81ea\u9002\u5e94\u68af\u5ea6\u88c1\u526a\u4f1a\u4e3a\u4e86\u9002\u5e94\u591a\u6570\u7fa4\u4f53\u800c\u8fc7\u5c0f\u5730\u88c1\u526a\u68af\u5ea6\uff0c\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u754c\u81ea\u9002\u5e94\u88c1\u526a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u8c03\u8282\u7684\u4e0b\u754c\uff0c\u907f\u514d\u8fc7\u5ea6\u88c1\u526a\u68af\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u65e0\u754c\u81ea\u9002\u5e94\u88c1\u526a\u548c\u56fa\u5b9a\u88c1\u526a\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u504f\u659c\u7684MNIST\u548cFashion MNIST\u6570\u636e\u96c6\u4e0a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8868\u73b0\u6700\u5dee\u7c7b\u7684\u51c6\u786e\u6027\uff08\u5206\u522b\u63d0\u9ad8\u4e8610%\u548c5%\u4ee5\u4e0a\uff09\u3002", "conclusion": "\u6709\u754c\u81ea\u9002\u5e94\u88c1\u526a\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86DP\u5b66\u4e60\u4e2d\u68af\u5ea6\u88c1\u526a\u5bf9\u5c11\u6570\u7fa4\u4f53\u7684\u4e0d\u516c\u5e73\u6027\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6574\u4f53\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u3002", "keywords": "\u5dee\u5206\u9690\u79c1, \u68af\u5ea6\u88c1\u526a, \u81ea\u9002\u5e94\u88c1\u526a, \u516c\u5e73\u6027, \u673a\u5668\u5b66\u4e60"}}
{"id": "2506.01458", "pdf": "https://arxiv.org/pdf/2506.01458", "abs": "https://arxiv.org/abs/2506.01458", "authors": ["Tanel Alum\u00e4e", "Artem Fedorchenko"], "title": "TalTech Systems for the Interspeech 2025 ML-SUPERB 2.0 Challenge", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "This paper describes the language identification and multilingual speech\nrecognition system developed at Tallinn University of Technology for the\nInterspeech 2025 ML-SUPERB 2.0 Challenge. A hybrid language identification\nsystem is used, consisting of a pretrained language embedding model and a\nlight-weight speech recognition model with a shared encoder across languages\nand language-specific bigram language models. For speech recognition, three\nmodels are used, where only a single model is applied for each language,\ndepending on the training data availability and performance on held-out data.\nThe model set consists of a finetuned version of SeamlessM4T, MMS-1B-all with\ncustom language adapters and MMS-zeroshot. The system obtained the top overall\nscore in the challenge.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e3aInterspeech 2025 ML-SUPERB 2.0\u6311\u6218\u8d5b\u5f00\u53d1\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff0c\u91c7\u7528\u6df7\u5408\u8bed\u8a00\u8bc6\u522b\u65b9\u6cd5\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\uff0c\u6700\u7ec8\u83b7\u5f97\u6311\u6218\u8d5b\u6700\u9ad8\u5206\u3002", "motivation": "\u5f00\u53d1\u9ad8\u6548\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9Interspeech\u6311\u6218\u8d5b\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\uff0c\u7ed3\u5408\u8bed\u8a00\u7279\u5b9a\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u52a8\u6001\u9009\u62e9\u6700\u4f18\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u3002", "result": "\u7cfb\u7edf\u5728\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u7efc\u5408\u8bc4\u5206\u3002", "conclusion": "\u6df7\u5408\u8bed\u8a00\u8bc6\u522b\u65b9\u6cd5\u548c\u52a8\u6001\u6a21\u578b\u9009\u62e9\u7b56\u7565\u5728\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "keywords": "\u8bed\u97f3\u8bc6\u522b, \u591a\u8bed\u8a00\u5904\u7406, \u8bed\u8a00\u8bc6\u522b, \u9884\u8bad\u7ec3\u6a21\u578b"}}
{"id": "2506.01404", "pdf": "https://arxiv.org/pdf/2506.01404", "abs": "https://arxiv.org/abs/2506.01404", "authors": ["Xue Xian Zheng", "Weihang Liu", "Xin Lou", "Stefan Vlaski", "Tareq Al-Naffouri"], "title": "Quantitative Error Feedback for Quantization Noise Reduction of Filtering over Graphs", "categories": ["cs.LG", "cs.MA", "cs.SY", "eess.SY"], "comment": "Journal Paper from ICASSP 10.1109/ICASSP49660.2025.10888821", "summary": "This paper introduces an innovative error feedback framework designed to\nmitigate quantization noise in distributed graph filtering, where\ncommunications are constrained to quantized messages. It comes from error\nspectrum shaping techniques from state-space digital filters, and therefore\nestablishes connections between quantized filtering processes over different\ndomains. In contrast to existing error compensation methods, our framework\nquantitatively feeds back the quantization noise for exact compensation. We\nexamine the framework under three key scenarios: (i) deterministic graph\nfiltering, (ii) graph filtering over random graphs, and (iii) graph filtering\nwith random node-asynchronous updates. Rigorous theoretical analysis\ndemonstrates that the proposed framework significantly reduces the effect of\nquantization noise, and we provide closed-form solutions for the optimal error\nfeedback coefficients. Moreover, this quantitative error feedback mechanism can\nbe seamlessly integrated into communication-efficient decentralized\noptimization frameworks, enabling lower error floors. Numerical experiments\nvalidate the theoretical results, consistently showing that our method\noutperforms conventional quantization strategies in terms of both accuracy and\nrobustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u8bef\u5dee\u53cd\u9988\u6846\u67b6\uff0c\u7528\u4e8e\u51cf\u5c11\u5206\u5e03\u5f0f\u56fe\u6ee4\u6ce2\u4e2d\u7684\u91cf\u5316\u566a\u58f0\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u591a\u79cd\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u6027\u3002", "motivation": "\u9488\u5bf9\u5206\u5e03\u5f0f\u56fe\u6ee4\u6ce2\u4e2d\u91cf\u5316\u901a\u4fe1\u5bfc\u81f4\u7684\u566a\u58f0\u95ee\u9898\uff0c\u7814\u7a76\u4e00\u79cd\u80fd\u591f\u7cbe\u786e\u8865\u507f\u91cf\u5316\u566a\u58f0\u7684\u8bef\u5dee\u53cd\u9988\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u6570\u5b57\u6ee4\u6ce2\u5668\u7684\u8bef\u5dee\u8c31\u6574\u5f62\u6280\u672f\uff0c\u63d0\u51fa\u5b9a\u91cf\u8bef\u5dee\u53cd\u9988\u673a\u5236\uff0c\u5e76\u5728\u786e\u5b9a\u6027\u56fe\u6ee4\u6ce2\u3001\u968f\u673a\u56fe\u6ee4\u6ce2\u548c\u968f\u673a\u8282\u70b9\u5f02\u6b65\u66f4\u65b0\u4e09\u79cd\u573a\u666f\u4e0b\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u91cf\u5316\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u63d0\u4f9b\u4e86\u6700\u4f18\u8bef\u5dee\u53cd\u9988\u7cfb\u6570\u7684\u95ed\u5f0f\u89e3\uff0c\u5e76\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bef\u5dee\u53cd\u9988\u6846\u67b6\u80fd\u6709\u6548\u964d\u4f4e\u91cf\u5316\u566a\u58f0\uff0c\u9002\u7528\u4e8e\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u4f18\u5316\u6846\u67b6\u3002", "keywords": "\u8bef\u5dee\u53cd\u9988, \u91cf\u5316\u566a\u58f0, \u5206\u5e03\u5f0f\u56fe\u6ee4\u6ce2, \u72b6\u6001\u7a7a\u95f4\u6ee4\u6ce2\u5668"}}
{"id": "2506.01474", "pdf": "https://arxiv.org/pdf/2506.01474", "abs": "https://arxiv.org/abs/2506.01474", "authors": ["Polina Tsvilodub", "Robert D. Hawkins", "Michael Franke"], "title": "Integrating Neural and Symbolic Components in a Model of Pragmatic Question-Answering", "categories": ["cs.CL"], "comment": "16 pages, 16 figures. To appear in the proceedings of Society for\n  Computation in Linguistics (SCiL) 2025", "summary": "Computational models of pragmatic language use have traditionally relied on\nhand-specified sets of utterances and meanings, limiting their applicability to\nreal-world language use. We propose a neuro-symbolic framework that enhances\nprobabilistic cognitive models by integrating LLM-based modules to propose and\nevaluate key components in natural language, eliminating the need for manual\nspecification. Through a classic case study of pragmatic question-answering, we\nsystematically examine various approaches to incorporating neural modules into\nthe cognitive model -- from evaluating utilities and literal semantics to\ngenerating alternative utterances and goals. We find that hybrid models can\nmatch or exceed the performance of traditional probabilistic models in\npredicting human answer patterns. However, the success of the neuro-symbolic\nmodel depends critically on how LLMs are integrated: while they are\nparticularly effective for proposing alternatives and transforming abstract\ngoals into utilities, they face challenges with truth-conditional semantic\nevaluation. This work charts a path toward more flexible and scalable models of\npragmatic language use while illuminating crucial design considerations for\nbalancing neural and symbolic components.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u57fa\u4e8eLLM\u7684\u6a21\u5757\u6765\u589e\u5f3a\u6982\u7387\u8ba4\u77e5\u6a21\u578b\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\uff0c\u4ece\u800c\u63d0\u5347\u5b9e\u7528\u6027\u8bed\u8a00\u6a21\u578b\u7684\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u4f20\u7edf\u5b9e\u7528\u6027\u8bed\u8a00\u8ba1\u7b97\u6a21\u578b\u4f9d\u8d56\u624b\u5de5\u5b9a\u4e49\u7684\u8bed\u53e5\u548c\u610f\u4e49\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7LLM\u6a21\u5757\u751f\u6210\u548c\u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u4e2d\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u5e76\u5728\u7ecf\u5178\u7684\u5b9e\u7528\u6027\u95ee\u7b54\u6848\u4f8b\u4e2d\u6bd4\u8f83\u591a\u79cd\u795e\u7ecf\u6a21\u5757\u6574\u5408\u65b9\u6cd5\u3002", "result": "\u6df7\u5408\u6a21\u578b\u5728\u9884\u6d4b\u4eba\u7c7b\u7b54\u6848\u6a21\u5f0f\u65b9\u9762\u4e0e\u4f20\u7edf\u6982\u7387\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u4f46\u795e\u7ecf\u7b26\u53f7\u6a21\u578b\u7684\u6210\u529f\u4f9d\u8d56\u4e8eLLM\u7684\u6574\u5408\u65b9\u5f0f\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u6a21\u578b\u4e3a\u5b9e\u7528\u6027\u8bed\u8a00\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u6ce8\u610f\u795e\u7ecf\u4e0e\u7b26\u53f7\u7ec4\u4ef6\u7684\u5e73\u8861\u8bbe\u8ba1\u3002", "keywords": "\u795e\u7ecf\u7b26\u53f7\u6846\u67b6, LLM, \u5b9e\u7528\u6027\u8bed\u8a00, \u8ba4\u77e5\u6a21\u578b, \u95ee\u7b54\u7cfb\u7edf"}}
{"id": "2506.01405", "pdf": "https://arxiv.org/pdf/2506.01405", "abs": "https://arxiv.org/abs/2506.01405", "authors": ["Xiang Zhao", "Ruijie Li", "Qiao Ning", "Shikai Guo", "Hui Li", "Qian Ma"], "title": "SOC-DGL: Social Interaction Behavior Inspired Dual Graph Learning Framework for Drug-Target Interaction Identification", "categories": ["cs.LG"], "comment": "14 pages, 17 figures (including subfigures), 4 tables. Xiang Zhao and\n  Ruijie Li contributed equally to this work and should be considered co-first\n  authors. The source code and datasets are available at\n  https://github.com/Zhaoxiang0422/SOC-DGL", "summary": "The identification of drug-target interactions (DTI) is crucial for drug\ndiscovery and repositioning, as it reveals potential uses of existing drugs,\naiding in the acceleration of the drug development process and reducing\nassociated costs. Despite the similarity information in DTI is important, most\nmodels are limited to mining direct similarity information within homogeneous\ngraphs, overlooking the potential yet rich similarity information in\nheterogeneous graphs. Inspired by real-world social interaction behaviors, we\npropose SOC-DGL, which comprises two specialized modules: the Affinity-Driven\nGraph Learning (ADGL) module and the Equilibrium-Driven Graph Learning (EDGL)\nmodule. The ADGL module adopts a comprehensive social interaction strategy,\nleveraging an affinity-enhanced global drug-target graph to learn both global\nDTI and the individual similarity information of drugs and targets. In\ncontrast, the EDGL module employs a higher-order social interaction strategy,\namplifying the influence of even-hop neighbors through an even-polynomial graph\nfilter grounded in balance theory, enabling the indirect mining of higher-order\nhomogeneous information. This dual approach enables SOC-DGL to effectively and\ncomprehensively capture similarity information across diverse interaction\nscales within the affinity matrices and drug-target association matrices,\nsignificantly enhancing the model's generalization capability and predictive\naccuracy in DTI tasks. To address the issue of imbalance in drug-target\ninteraction datasets, this paper proposes an adjustable imbalance loss function\nthat mitigates the impact of sample imbalance by adjusting the weight of\nnegative samples and a parameter. Extensive experiments on four benchmark\ndatasets demonstrate significant accuracy improvements achieved by SOC-DGL,\nparticularly in scenarios involving data imbalance and unseen drugs or targets.", "AI": {"tldr": "SOC-DGL\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u4eb2\u548c\u529b\u9a71\u52a8\u56fe\u5b66\u4e60\u548c\u5747\u8861\u9a71\u52a8\u56fe\u5b66\u4e60\u6a21\u5757\uff0c\u6709\u6548\u6316\u6398\u5f02\u6784\u56fe\u4e2d\u7684\u76f8\u4f3c\u6027\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u836f\u7269-\u9776\u6807\u4e92\u4f5c\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u836f\u7269-\u9776\u6807\u4e92\u4f5c\uff08DTI\uff09\u9884\u6d4b\u4e2d\u4ec5\u5173\u6ce8\u540c\u6784\u56fe\u76f4\u63a5\u76f8\u4f3c\u6027\u7684\u5c40\u9650\u6027\uff0c\u4f5c\u8005\u4ece\u771f\u5b9e\u793e\u4ea4\u884c\u4e3a\u4e2d\u83b7\u53d6\u7075\u611f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u6316\u6398\u5f02\u6784\u56fe\u4e30\u5bcc\u76f8\u4f3c\u6027\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "method": "SOC-DGL\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a\u4eb2\u548c\u529b\u9a71\u52a8\u56fe\u5b66\u4e60\uff08ADGL\uff09\u548c\u5747\u8861\u9a71\u52a8\u56fe\u5b66\u4e60\uff08EDGL\uff09\u3002ADGL\u5229\u7528\u5168\u5c40\u836f\u7269-\u9776\u6807\u56fe\u5b66\u4e60\u5168\u5c40DTI\u548c\u4e2a\u4f53\u76f8\u4f3c\u6027\u4fe1\u606f\uff0c\u800cEDGL\u901a\u8fc7\u9ad8\u9636\u793e\u4ea4\u7b56\u7565\u6316\u6398\u95f4\u63a5\u7684\u9ad8\u9636\u540c\u6784\u4fe1\u606f\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u53ef\u8c03\u8282\u7684\u6837\u672c\u4e0d\u5e73\u8861\u635f\u5931\u51fd\u6570\u4ee5\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSOC-DGL\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u4e0d\u5e73\u8861\u548c\u672a\u89c1\u836f\u7269\u6216\u9776\u6807\u573a\u666f\u4e2d\u3002", "conclusion": "SOC-DGL\u901a\u8fc7\u7ed3\u5408\u4e0d\u540c\u5c3a\u5ea6\u7684\u76f8\u4f3c\u6027\u4fe1\u606f\u548c\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e3aDTI\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u836f\u7269-\u9776\u6807\u4e92\u4f5c, \u56fe\u5b66\u4e60, \u5f02\u6784\u56fe, \u793e\u4ea4\u884c\u4e3a, \u6570\u636e\u4e0d\u5e73\u8861"}}
{"id": "2506.01484", "pdf": "https://arxiv.org/pdf/2506.01484", "abs": "https://arxiv.org/abs/2506.01484", "authors": ["Shuzhou Yuan", "Ercong Nie", "Lukas Kouba", "Ashish Yashwanth Kangen", "Helmut Schmid", "Hinrich Schutze", "Michael Farber"], "title": "LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech Detoxification", "categories": ["cs.CL"], "comment": null, "summary": "Detoxification, the task of rewriting harmful language into non-toxic text,\nhas become increasingly important amid the growing prevalence of toxic content\nonline. However, high-quality parallel datasets for detoxification, especially\nfor hate speech, remain scarce due to the cost and sensitivity of human\nannotation. In this paper, we propose a novel LLM-in-the-loop pipeline\nleveraging GPT-4o-mini for automated detoxification. We first replicate the\nParaDetox pipeline by replacing human annotators with an LLM and show that the\nLLM performs comparably to human annotation. Building on this, we construct\nPARADEHATE, a large-scale parallel dataset specifically for hatespeech\ndetoxification. We release PARADEHATE as a benchmark of over 8K hate/non-hate\ntext pairs and evaluate a wide range of baseline methods. Experimental results\nshow that models such as BART, fine-tuned on PARADEHATE, achieve better\nperformance in style accuracy, content preservation, and fluency, demonstrating\nthe effectiveness of LLM-generated detoxification text as a scalable\nalternative to human annotation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528GPT-4o-mini\u7684LLM-in-the-loop\u7ba1\u9053\uff0c\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4ec7\u6068\u8a00\u8bba\u89e3\u6bd2\u6570\u636e\u96c6PARADEHATE\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u7ebf\u6bd2\u6027\u5185\u5bb9\u65e5\u76ca\u589e\u591a\uff0c\u800c\u9ad8\u8d28\u91cf\u5e76\u884c\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u5c24\u5176\u662f\u5728\u4ec7\u6068\u8a00\u8bba\u9886\u57df\uff0c\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u654f\u611f\u3002", "method": "\u5229\u7528GPT-4o-mini\u66ff\u4ee3\u4eba\u5de5\u6807\u6ce8\uff0c\u6784\u5efa\u5927\u578b\u5e76\u884c\u6570\u636e\u96c6PARADEHATE\uff0c\u5e76\u901a\u8fc7BART\u7b49\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u548c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8ePARADEHATE\u5fae\u8c03\u7684\u6a21\u578b\u5728\u98ce\u683c\u51c6\u786e\u6027\u3001\u5185\u5bb9\u4fdd\u7559\u548c\u6d41\u7545\u6027\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u9a8c\u8bc1\u4e86LLM\u751f\u6210\u6570\u636e\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "LLM\u751f\u6210\u7684\u89e3\u6bd2\u6587\u672c\u53ef\u6709\u6548\u66ff\u4ee3\u4eba\u5de5\u6807\u6ce8\uff0c\u4e3a\u81ea\u52a8\u89e3\u6bd2\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u89e3\u6bd2\u5316, \u4ec7\u6068\u8a00\u8bba, \u4eba\u5de5\u6807\u6ce8, GPT-4o-mini, PARADEHATE"}}
{"id": "2506.01414", "pdf": "https://arxiv.org/pdf/2506.01414", "abs": "https://arxiv.org/abs/2506.01414", "authors": ["Yida Wang", "David Joseph Tan", "Nassir Navab", "Federico Tombari"], "title": "Self-supervised Latent Space Optimization with Nebula Variational Coding", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": null, "summary": "Deep learning approaches process data in a layer-by-layer way with\nintermediate (or latent) features. We aim at designing a general solution to\noptimize the latent manifolds to improve the performance on classification,\nsegmentation, completion and/or reconstruction through probabilistic models.\nThis paper proposes a variational inference model which leads to a clustered\nembedding. We introduce additional variables in the latent space, called\n\\textbf{nebula anchors}, that guide the latent variables to form clusters\nduring training. To prevent the anchors from clustering among themselves, we\nemploy the variational constraint that enforces the latent features within an\nanchor to form a Gaussian distribution, resulting in a generative model we\nrefer as Nebula Variational Coding (NVC). Since each latent feature can be\nlabeled with the closest anchor, we also propose to apply metric learning in a\nself-supervised way to make the separation between clusters more explicit. As a\nconsequence, the latent variables of our variational coder form clusters which\nadapt to the generated semantic of the training data, \\textit{e.g.} the\ncategorical labels of each sample. We demonstrate experimentally that it can be\nused within different architectures designed to solve different problems\nincluding text sequence, images, 3D point clouds and volumetric data,\nvalidating the advantage of our proposed method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aNebula Variational Coding\uff08NVC\uff09\u7684\u53d8\u5206\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165nebula anchors\u4f18\u5316\u6f5c\u5728\u6d41\u5f62\uff0c\u63d0\u5347\u5206\u7c7b\u3001\u5206\u5272\u3001\u5b8c\u6210\u548c\u91cd\u5efa\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e2d\u95f4\u7279\u5f81\u7684\u5904\u7406\u901a\u5e38\u7f3a\u4e4f\u4f18\u5316\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6982\u7387\u6a21\u578b\u4f18\u5316\u6f5c\u5728\u6d41\u5f62\uff0c\u63d0\u5347\u591a\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u63d0\u51faNVC\u6a21\u578b\uff0c\u5f15\u5165nebula anchors\u5f15\u5bfc\u6f5c\u5728\u53d8\u91cf\u805a\u7c7b\uff0c\u5e76\u901a\u8fc7\u53d8\u5206\u7ea6\u675f\u907f\u514danchors\u81ea\u8eab\u805a\u7c7b\uff0c\u540c\u65f6\u7ed3\u5408\u81ea\u76d1\u7763\u5ea6\u91cf\u5b66\u4e60\u660e\u786e\u805a\u7c7b\u5206\u79bb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNVC\u9002\u7528\u4e8e\u6587\u672c\u5e8f\u5217\u3001\u56fe\u50cf\u30013D\u70b9\u4e91\u548c\u4f53\u6570\u636e\u7b49\u591a\u79cd\u67b6\u6784\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f18\u52bf\u3002", "conclusion": "NVC\u901a\u8fc7\u4f18\u5316\u6f5c\u5728\u6d41\u5f62\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u7684\u6027\u80fd\u3002", "keywords": "\u6df1\u5ea6\u5b66\u4e60,\u53d8\u5206\u63a8\u7406,\u6f5c\u5728\u6d41\u5f62,\u805a\u7c7b,\u81ea\u76d1\u7763\u5b66\u4e60"}}
{"id": "2506.01488", "pdf": "https://arxiv.org/pdf/2506.01488", "abs": "https://arxiv.org/abs/2506.01488", "authors": ["Long Yao", "Wenzhong Yang", "Yabo Yin", "Fuyuan Wei", "Hongzhen Lv", "Jiaren Peng", "Liejun Wang", "Xiaoming Tao"], "title": "Argument-Centric Causal Intervention Method for Mitigating Bias in Cross-Document Event Coreference Resolution", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Cross-document Event Coreference Resolution (CD-ECR) is a fundamental task in\nnatural language processing (NLP) that seeks to determine whether event\nmentions across multiple documents refer to the same real-world occurrence.\nHowever, current CD-ECR approaches predominantly rely on trigger features\nwithin input mention pairs, which induce spurious correlations between\nsurface-level lexical features and coreference relationships, impairing the\noverall performance of the models. To address this issue, we propose a novel\ncross-document event coreference resolution method based on Argument-Centric\nCausal Intervention (ACCI). Specifically, we construct a structural causal\ngraph to uncover confounding dependencies between lexical triggers and\ncoreference labels, and introduce backdoor-adjusted interventions to isolate\nthe true causal effect of argument semantics. To further mitigate spurious\ncorrelations, ACCI integrates a counterfactual reasoning module that quantifies\nthe causal influence of trigger word perturbations, and an argument-aware\nenhancement module to promote greater sensitivity to semantically grounded\ninformation. In contrast to prior methods that depend on costly data\naugmentation or heuristic-based filtering, ACCI enables effective debiasing in\na unified end-to-end framework without altering the underlying training\nprocedure. Extensive experiments demonstrate that ACCI achieves CoNLL F1 of\n88.4% on ECB+ and 85.2% on GVC, achieving state-of-the-art performance. The\nimplementation and materials are available at https://github.com/era211/ACCI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eArgument-Centric Causal Intervention\uff08ACCI\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u8de8\u6587\u6863\u4e8b\u4ef6\u5171\u6307\u6d88\u89e3\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8de8\u6587\u6863\u4e8b\u4ef6\u5171\u6307\u6d88\u89e3\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u89e6\u53d1\u8bcd\u7279\u5f81\uff0c\u5bfc\u81f4\u8868\u9762\u8bcd\u6c47\u7279\u5f81\u4e0e\u5171\u6307\u5173\u7cfb\u95f4\u7684\u865a\u5047\u76f8\u5173\u6027\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u6784\u5efa\u7ed3\u6784\u56e0\u679c\u56fe\u63ed\u793a\u8bcd\u6c47\u89e6\u53d1\u4e0e\u5171\u6307\u6807\u7b7e\u95f4\u7684\u6df7\u6742\u4f9d\u8d56\uff0c\u5f15\u5165\u540e\u95e8\u8c03\u6574\u5e72\u9884\u5206\u79bb\u8bba\u5143\u8bed\u4e49\u7684\u771f\u5b9e\u56e0\u679c\u6548\u5e94\uff0c\u5e76\u7ed3\u5408\u53cd\u4e8b\u5b9e\u63a8\u7406\u6a21\u5757\u548c\u8bba\u5143\u611f\u77e5\u589e\u5f3a\u6a21\u5757\u3002", "result": "\u5728ECB+\u548cGVC\u6570\u636e\u96c6\u4e0a\u5206\u522b\u53d6\u5f9788.4%\u548c85.2%\u7684CoNLL F1\u5206\u6570\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "ACCI\u4e3a\u8de8\u6587\u6863\u4e8b\u4ef6\u5171\u6307\u6d88\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u53bb\u504f\u5dee\u65b9\u6cd5\uff0c\u65e0\u9700\u6570\u636e\u589e\u5f3a\u6216\u542f\u53d1\u5f0f\u8fc7\u6ee4\uff0c\u5728\u7aef\u5230\u7aef\u6846\u67b6\u4e2d\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "keywords": "\u8de8\u6587\u6863\u4e8b\u4ef6\u5171\u6307\u6d88\u89e3, \u56e0\u679c\u5e72\u9884, \u53cd\u4e8b\u5b9e\u63a8\u7406, \u8bba\u5143\u8bed\u4e49, \u865a\u5047\u76f8\u5173\u6027"}}
{"id": "2506.01444", "pdf": "https://arxiv.org/pdf/2506.01444", "abs": "https://arxiv.org/abs/2506.01444", "authors": ["Sujeevan Aseervatham", "Achraf Kerzazi", "Youn\u00e8s Bennani"], "title": "Variance-Based Defense Against Blended Backdoor Attacks", "categories": ["cs.LG", "cs.CV"], "comment": "This paper has been accepted at ECML PKDD 2025", "summary": "Backdoor attacks represent a subtle yet effective class of cyberattacks\ntargeting AI models, primarily due to their stealthy nature. The model behaves\nnormally on clean data but exhibits malicious behavior only when the attacker\nembeds a specific trigger into the input. This attack is performed during the\ntraining phase, where the adversary corrupts a small subset of the training\ndata by embedding a pattern and modifying the labels to a chosen target. The\nobjective is to make the model associate the pattern with the target label\nwhile maintaining normal performance on unaltered data. Several defense\nmechanisms have been proposed to sanitize training data-sets. However, these\nmethods often rely on the availability of a clean dataset to compute\nstatistical anomalies, which may not always be feasible in real-world scenarios\nwhere datasets can be unavailable or compromised. To address this limitation,\nwe propose a novel defense method that trains a model on the given dataset,\ndetects poisoned classes, and extracts the critical part of the attack trigger\nbefore identifying the poisoned instances. This approach enhances\nexplainability by explicitly revealing the harmful part of the trigger. The\neffectiveness of our method is demonstrated through experimental evaluations on\nwell-known image datasets and comparative analysis against three\nstate-of-the-art algorithms: SCAn, ABL, and AGPD.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u9632\u5fa1\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u6e05\u9664AI\u6a21\u578b\u4e2d\u7684\u540e\u95e8\u653b\u51fb\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5e72\u51c0\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u9632\u5fa1\u65b9\u6cd5\u9700\u8981\u4f9d\u8d56\u5e72\u51c0\u6570\u636e\u96c6\u8ba1\u7b97\u7edf\u8ba1\u5f02\u5e38\uff0c\u8fd9\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5f80\u5f80\u4e0d\u53ef\u884c\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e0d\u4f9d\u8d56\u5e72\u51c0\u6570\u636e\u96c6\u7684\u540e\u95e8\u653b\u51fb\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u8bad\u7ec3\u4e00\u4e2a\u6a21\u578b\uff0c\u68c0\u6d4b\u88ab\u6c61\u67d3\u7684\u7c7b\u522b\uff0c\u63d0\u53d6\u653b\u51fb\u89e6\u53d1\u5668\u7684\u5173\u952e\u90e8\u5206\uff0c\u7136\u540e\u8bc6\u522b\u88ab\u6c61\u67d3\u7684\u5b9e\u4f8b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u660e\u786e\u63ed\u793a\u89e6\u53d1\u5668\u7684\u6709\u5bb3\u90e8\u5206\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u77e5\u540d\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u6709\u6548\uff0c\u5e76\u6bd4\u4e09\u79cd\u5148\u8fdb\u7b97\u6cd5\uff08SCAn\u3001ABL\u548cAGPD\uff09\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u540e\u95e8\u653b\u51fb\u9632\u5fa1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u5e72\u51c0\u6570\u636e\u96c6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "keywords": "\u540e\u95e8\u653b\u51fb, AI\u5b89\u5168, \u9632\u5fa1\u65b9\u6cd5, \u89e6\u53d1\u5668\u68c0\u6d4b, \u7edf\u8ba1\u5f02\u5e38"}}
{"id": "2506.01489", "pdf": "https://arxiv.org/pdf/2506.01489", "abs": "https://arxiv.org/abs/2506.01489", "authors": ["Edison Marrese-Taylor", "Erica K. Shimomoto", "Alfredo Solano", "Enrique Reid"], "title": "Multilingual Definition Modeling", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose the first multilingual study on definition\nmodeling. We use monolingual dictionary data for four new languages (Spanish,\nFrench, Portuguese, and German) and perform an in-depth empirical study to test\nthe performance of pre-trained multilingual language models on definition\nmodeling of monosemic words when finetuned on this data. Furthermore, we use a\nzero-shot approach to test the multilingual capabilities of two popular\nchat-based Large Language Models (LLMs) in the task. Results show that\nmultilingual language models can perform on-pair with English but cannot\nleverage potential cross-lingual synergies, with LLMs generally offering better\nperformance overall. A comprehensive human evaluation of the LLM-generated\ndefinition highlights the zero and few-shot capabilities of these models in\nthis new task, also showing their shortcomings. Finally, we show that\nperformance on our task via BERTScore strongly correlates to the performance on\nmultilingual LLM benchmarks, suggesting that our task offers a viable\ncompute-constrained, stable and natural alternative to these.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u63d0\u51fa\u591a\u8bed\u8a00\u5b9a\u4e49\u5efa\u6a21\u7814\u7a76\uff0c\u6d4b\u8bd5\u4e86\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u6a21\u578b\u5728\u65b0\u8bed\u8a00\uff08\u897f\u73ed\u7259\u8bed\u3001\u6cd5\u8bed\u3001\u8461\u8404\u7259\u8bed\u548c\u5fb7\u8bed\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e24\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u96f6\u6837\u672c\u80fd\u529b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u591a\u8bed\u8a00\u6a21\u578b\u4e0e\u82f1\u8bed\u8868\u73b0\u76f8\u5f53\uff0c\u4f46\u65e0\u6cd5\u5229\u7528\u8de8\u8bed\u8a00\u534f\u540c\u6548\u5e94\uff0c\u800cLLM\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u7814\u7a76\u591a\u8bed\u8a00\u5b9a\u4e49\u5efa\u6a21\u7684\u6f5c\u529b\uff0c\u63a2\u7d22\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65b0\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u65b0\u8bed\u8a00\u7684\u5355\u8bed\u8bcd\u5178\u6570\u636e\uff0c\u5bf9\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u91c7\u7528\u96f6\u6837\u672c\u65b9\u6cd5\u6d4b\u8bd5\u4e24\u79cdLLM\u7684\u8868\u73b0\u3002", "result": "\u591a\u8bed\u8a00\u6a21\u578b\u4e0e\u82f1\u8bed\u8868\u73b0\u76f8\u5f53\uff0c\u4f46LLM\u6574\u4f53\u8868\u73b0\u66f4\u4f18\u3002BERTScore\u4e0e\u591a\u8bed\u8a00LLM\u57fa\u51c6\u6027\u80fd\u5f3a\u76f8\u5173\u3002", "conclusion": "\u591a\u8bed\u8a00\u5b9a\u4e49\u5efa\u6a21\u4efb\u52a1\u4e3a\u8ba1\u7b97\u7ea6\u675f\u3001\u7a33\u5b9a\u4e14\u81ea\u7136\u7684\u66ff\u4ee3\u65b9\u6848\u63d0\u4f9b\u53ef\u884c\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86LLM\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u3002", "keywords": "\u591a\u8bed\u8a00\u5b9a\u4e49\u5efa\u6a21, \u9884\u8bad\u7ec3\u6a21\u578b, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u96f6\u6837\u672c\u5b66\u4e60, BERTScore"}}
{"id": "2506.01450", "pdf": "https://arxiv.org/pdf/2506.01450", "abs": "https://arxiv.org/abs/2506.01450", "authors": ["Manuel Franco de la Pe\u00f1a", "\u00c1ngel Luis Perales G\u00f3mez", "Lorenzo Fern\u00e1ndez Maim\u00f3"], "title": "ShaTS: A Shapley-based Explainability Method for Time Series Artificial Intelligence Models applied to Anomaly Detection in Industrial Internet of Things", "categories": ["cs.LG", "cs.AI"], "comment": "22 pages;16 figures;Submitted to Elsevier (Information Fusion)", "summary": "Industrial Internet of Things environments increasingly rely on advanced\nAnomaly Detection and explanation techniques to rapidly detect and mitigate\ncyberincidents, thereby ensuring operational safety. The sequential nature of\ndata collected from these environments has enabled improvements in Anomaly\nDetection using Machine Learning and Deep Learning models by processing time\nwindows rather than treating the data as tabular. However, conventional\nexplanation methods often neglect this temporal structure, leading to imprecise\nor less actionable explanations. This work presents ShaTS (Shapley values for\nTime Series models), which is a model-agnostic explainable Artificial\nIntelligence method designed to enhance the precision of Shapley value\nexplanations for time series models. ShaTS addresses the shortcomings of\ntraditional approaches by incorporating an a priori feature grouping strategy\nthat preserves temporal dependencies and produces both coherent and actionable\ninsights. Experiments conducted on the SWaT dataset demonstrate that ShaTS\naccurately identifies critical time instants, precisely pinpoints the sensors,\nactuators, and processes affected by anomalies, and outperforms SHAP in terms\nof both explainability and resource efficiency, fulfilling the real-time\nrequirements of industrial environments.", "AI": {"tldr": "ShaTS\u662f\u4e00\u79cd\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u7684\u6a21\u578b\u65e0\u5173\u89e3\u91ca\u6027AI\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u4f9d\u8d56\u6027\u63d0\u9ad8\u4e86Shapley\u503c\u89e3\u91ca\u7684\u7cbe\u786e\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5de5\u4e1a\u7269\u8054\u7f51\u73af\u5883\u4e2d\uff0c\u5f02\u5e38\u68c0\u6d4b\u548c\u89e3\u91ca\u6280\u672f\u5bf9\u786e\u4fdd\u8fd0\u884c\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u65f6\u95f4\u7ed3\u6784\uff0c\u5bfc\u81f4\u89e3\u91ca\u4e0d\u7cbe\u786e\u6216\u4e0d\u53ef\u64cd\u4f5c\u3002", "method": "\u63d0\u51faShaTS\u65b9\u6cd5\uff0c\u91c7\u7528\u5148\u9a8c\u7279\u5f81\u5206\u7ec4\u7b56\u7565\u4fdd\u7559\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u751f\u6210\u8fde\u8d2f\u4e14\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002", "result": "\u5728SWaT\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u8868\u660e\uff0cShaTS\u80fd\u51c6\u786e\u8bc6\u522b\u5173\u952e\u65f6\u95f4\u70b9\u3001\u53d7\u5f71\u54cd\u7684\u4f20\u611f\u5668\u548c\u6267\u884c\u5668\uff0c\u4e14\u5728\u89e3\u91ca\u6027\u548c\u8d44\u6e90\u6548\u7387\u4e0a\u4f18\u4e8eSHAP\u3002", "conclusion": "ShaTS\u6ee1\u8db3\u5de5\u4e1a\u73af\u5883\u7684\u5b9e\u65f6\u9700\u6c42\uff0c\u63d0\u4f9b\u66f4\u7cbe\u786e\u4e14\u53ef\u64cd\u4f5c\u7684\u5f02\u5e38\u89e3\u91ca\u3002", "keywords": "\u5de5\u4e1a\u7269\u8054\u7f51\u3001\u5f02\u5e38\u68c0\u6d4b\u3001\u65f6\u95f4\u5e8f\u5217\u3001Shapley\u503c\u3001\u6a21\u578b\u65e0\u5173\u89e3\u91ca"}}
{"id": "2506.01495", "pdf": "https://arxiv.org/pdf/2506.01495", "abs": "https://arxiv.org/abs/2506.01495", "authors": ["Ping Wu", "Guobin Shen", "Dongcheng Zhao", "Yuwei Wang", "Yiting Dong", "Yu Shi", "Enmeng Lu", "Feifei Zhao", "Yi Zeng"], "title": "CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Ensuring that Large Language Models (LLMs) align with mainstream human values\nand ethical norms is crucial for the safe and sustainable development of AI.\nCurrent value evaluation and alignment are constrained by Western cultural bias\nand incomplete domestic frameworks reliant on non-native rules; furthermore,\nthe lack of scalable, rule-driven scenario generation methods makes evaluations\ncostly and inadequate across diverse cultural contexts. To address these\nchallenges, we propose a hierarchical value framework grounded in core Chinese\nvalues, encompassing three main dimensions, 12 core values, and 50 derived\nvalues. Based on this framework, we construct a large-scale Chinese Values\nCorpus (CVC) containing over 250,000 value rules enhanced and expanded through\nhuman annotation. Experimental results show that CVC-guided scenarios\noutperform direct generation ones in value boundaries and content diversity. In\nthe evaluation across six sensitive themes (e.g., surrogacy, suicide), seven\nmainstream LLMs preferred CVC-generated options in over 70.5% of cases, while\nfive Chinese human annotators showed an 87.5% alignment with CVC, confirming\nits universality, cultural relevance, and strong alignment with Chinese values.\nAdditionally, we construct 400,000 rule-based moral dilemma scenarios that\nobjectively capture nuanced distinctions in conflicting value prioritization\nacross 17 LLMs. Our work establishes a culturally-adaptive benchmarking\nframework for comprehensive value evaluation and alignment, representing\nChinese characteristics. All data are available at\nhttps://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at\nhttps://github.com/Beijing-AISI/CVC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e2d\u56fd\u6838\u5fc3\u4ef7\u503c\u89c2\u7684\u5206\u5c42\u4ef7\u503c\u6846\u67b6\uff0c\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u4e2d\u6587\u4ef7\u503c\u89c2\u8bed\u6599\u5e93\uff08CVC\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u8c03\u6574\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4ef7\u503c\u89c2\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u6709\u6548\u6027\u548c\u9002\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u4ef7\u503c\u89c2\u8bc4\u4f30\u548c\u8c03\u6574\u5b58\u5728\u897f\u65b9\u6587\u5316\u504f\u89c1\u548c\u672c\u571f\u6846\u67b6\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u53ef\u6269\u5c55\u7684\u89c4\u5219\u9a71\u52a8\u573a\u666f\u751f\u6210\u65b9\u6cd5\uff0c\u5bfc\u81f4\u8bc4\u4f30\u6210\u672c\u9ad8\u4e14\u4e0d\u5168\u9762\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b3\u4e2a\u4e3b\u8981\u7ef4\u5ea6\u300112\u4e2a\u6838\u5fc3\u4ef7\u503c\u89c2\u548c50\u4e2a\u6d3e\u751f\u4ef7\u503c\u89c2\u7684\u5206\u5c42\u4ef7\u503c\u6846\u67b6\uff0c\u5e76\u6784\u5efa\u4e86\u8d85\u8fc725\u4e07\u6761\u89c4\u5219\u7684\u4e2d\u6587\u4ef7\u503c\u89c2\u8bed\u6599\u5e93\uff08CVC\uff09\u3002", "result": "CVC\u751f\u6210\u7684\u573a\u666f\u5728\u4ef7\u503c\u8fb9\u754c\u548c\u5185\u5bb9\u591a\u6837\u6027\u4e0a\u4f18\u4e8e\u76f4\u63a5\u751f\u6210\u7684\u573a\u666f\uff0c\u4e3b\u6d41LLMs\u572870.5%\u7684\u6848\u4f8b\u4e2d\u504f\u597dCVC\u9009\u9879\uff0c\u4e2d\u56fd\u4eba\u7c7b\u6807\u6ce8\u8005\u4e0eCVC\u7684\u5951\u5408\u5ea6\u8fbe87.5%\u3002", "conclusion": "CVC\u4e3a\u5168\u9762\u4ef7\u503c\u89c2\u8bc4\u4f30\u548c\u8c03\u6574\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6587\u5316\u9002\u5e94\u6027\u5f3a\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u4f53\u73b0\u4e86\u4e2d\u56fd\u7279\u8272\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u4ef7\u503c\u89c2\u5bf9\u9f50\u3001\u4e2d\u6587\u4ef7\u503c\u89c2\u8bed\u6599\u5e93\u3001\u6587\u5316\u9002\u5e94\u6027\u3001\u9053\u5fb7\u56f0\u5883"}}
{"id": "2506.01467", "pdf": "https://arxiv.org/pdf/2506.01467", "abs": "https://arxiv.org/abs/2506.01467", "authors": ["Dorian Gailhard", "Enzo Tartaglione", "Lirida Naviner", "Jhony H. Giraldo"], "title": "Feature-aware Hypergraph Generation via Next-Scale Prediction", "categories": ["cs.LG", "cs.DM"], "comment": null, "summary": "Hypergraphs generalize traditional graphs by allowing hyperedges to connect\nmultiple nodes, making them well-suited for modeling complex structures with\nhigher-order relationships, such as 3D meshes, molecular systems, and\nelectronic circuits. While topology is central to hypergraph structure, many\nreal-world applications also require node and hyperedge features. Existing\nhypergraph generation methods focus solely on topology, often overlooking\nfeature modeling. In this work, we introduce FAHNES (feature-aware hypergraph\ngeneration via next-scale prediction), a hierarchical approach that jointly\ngenerates hypergraph topology and features. FAHNES builds a multi-scale\nrepresentation through node coarsening, then learns to reconstruct finer levels\nvia localized expansion and refinement, guided by a new node budget mechanism\nthat controls cluster splitting. We evaluate FAHNES on synthetic hypergraphs,\n3D meshes, and molecular datasets. FAHNES achieves competitive results in\nreconstructing topology and features, establishing a foundation for future\nresearch in featured hypergraph generative modeling.", "AI": {"tldr": "FAHNES\u662f\u4e00\u79cd\u5c42\u6b21\u5316\u65b9\u6cd5\uff0c\u8054\u5408\u751f\u6210\u8d85\u56fe\u62d3\u6251\u548c\u7279\u5f81\uff0c\u901a\u8fc7\u8282\u70b9\u7c97\u5316\u548c\u5c40\u90e8\u6269\u5c55\u91cd\u6784\u66f4\u7cbe\u7ec6\u5c42\u7ea7\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8d85\u56fe\u751f\u6210\u65b9\u6cd5\u4ec5\u5173\u6ce8\u62d3\u6251\u7ed3\u6784\uff0c\u5e38\u5ffd\u7565\u7279\u5f81\u5efa\u6a21\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u9700\u8981\u4e24\u8005\u7ed3\u5408\u3002", "method": "\u901a\u8fc7\u8282\u70b9\u7c97\u5316\u6784\u5efa\u591a\u5c3a\u5ea6\u8868\u793a\uff0c\u901a\u8fc7\u5c40\u90e8\u6269\u5c55\u548c\u7ec6\u5316\u91cd\u6784\u7cbe\u7ec6\u5c42\u7ea7\uff0c\u91c7\u7528\u8282\u70b9\u9884\u7b97\u673a\u5236\u63a7\u5236\u5206\u88c2\u3002", "result": "\u5728\u5408\u6210\u8d85\u56fe\u30013D\u7f51\u683c\u548c\u5206\u5b50\u6570\u636e\u96c6\u4e0a\uff0cFAHNES\u5728\u62d3\u6251\u548c\u7279\u5f81\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "FAHNES\u4e3a\u7279\u5f81\u8d85\u56fe\u751f\u6210\u5efa\u6a21\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "keywords": "\u8d85\u56fe\u751f\u6210\u3001\u7279\u5f81\u5efa\u6a21\u3001\u591a\u5c3a\u5ea6\u8868\u793a\u3001FAHNES"}}
{"id": "2506.01496", "pdf": "https://arxiv.org/pdf/2506.01496", "abs": "https://arxiv.org/abs/2506.01496", "authors": ["Guitao Wang", "Jinming Zhao", "Hao Yang", "Guilin Qi", "Tongtong Wu", "Gholamreza Haffari"], "title": "Continual Speech Learning with Fused Speech Features", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech 2025", "summary": "Rapid growth in speech data demands adaptive models, as traditional static\nmethods fail to keep pace with dynamic and diverse speech information. We\nintroduce continuous speech learning, a new set-up targeting at bridging the\nadaptation gap in current speech models. We use the encoder-decoder Whisper\nmodel to standardize speech tasks into a generative format. We integrate a\nlearnable gated-fusion layer on the top of the encoder to dynamically select\ntask-specific features for downstream tasks. Our approach improves accuracy\nsignificantly over traditional methods in six speech processing tasks,\ndemonstrating gains in adapting to new speech tasks without full retraining.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fde\u7eed\u8bed\u97f3\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u95e8\u63a7\u878d\u5408\u5c42\u52a8\u6001\u9009\u62e9\u4efb\u52a1\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u516d\u79cd\u8bed\u97f3\u4efb\u52a1\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u8bed\u97f3\u6a21\u578b\u65e0\u6cd5\u9002\u5e94\u5feb\u901f\u589e\u957f\u548c\u591a\u6837\u5316\u7684\u8bed\u97f3\u6570\u636e\uff0c\u4e9f\u9700\u52a8\u6001\u81ea\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eWhisper\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u95e8\u63a7\u878d\u5408\u5c42\uff0c\u52a8\u6001\u9009\u62e9\u4efb\u52a1\u7279\u5f81\u8fdb\u884c\u4e0b\u6e38\u4efb\u52a1\u9002\u914d\u3002", "result": "\u5728\u516d\u79cd\u8bed\u97f3\u4efb\u52a1\u4e2d\uff0c\u51c6\u786e\u6027\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "conclusion": "\u8fde\u7eed\u8bed\u97f3\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u97f3\u6a21\u578b\u7684\u52a8\u6001\u9002\u914d\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002", "keywords": "\u8bed\u97f3\u5904\u7406,\u81ea\u9002\u5e94\u6a21\u578b,\u95e8\u63a7\u878d\u5408,Whisper\u6a21\u578b"}}
{"id": "2506.00452", "pdf": "https://arxiv.org/pdf/2506.00452", "abs": "https://arxiv.org/abs/2506.00452", "authors": ["TaeJun Ha", "Chaehyun Jung", "Hyeonuk Kim", "Jeongwoo Park", "Jeonghun Park"], "title": "Attention-Aided MMSE for OFDM Channel Estimation: Learning Linear Filters with Attention", "categories": ["eess.SP", "cs.AI", "stat.ML"], "comment": "13 pages, 12 figures", "summary": "In orthogonal frequency division multiplexing (OFDM), accurate channel\nestimation is crucial. Classical signal processing based approaches, such as\nminimum mean-squared error (MMSE) estimation, often require second-order\nstatistics that are difficult to obtain in practice. Recent deep neural\nnetworks based methods have been introduced to address this; yet they often\nsuffer from high complexity. This paper proposes an Attention-aided MMSE\n(A-MMSE), a novel model-based DNN framework that learns the optimal MMSE filter\nvia the Attention Transformer. Once trained, the A-MMSE estimates the channel\nthrough a single linear operation for channel estimation, eliminating nonlinear\nactivations during inference and thus reducing computational complexity. To\nenhance the learning efficiency of the A-MMSE, we develop a two-stage Attention\nencoder, designed to effectively capture the channel correlation structure.\nAdditionally, a rank-adaptive extension of the proposed A-MMSE allows flexible\ntrade-offs between complexity and channel estimation accuracy. Extensive\nsimulations with 3GPP TDL channel models demonstrate that the proposed A-MMSE\nconsistently outperforms other baseline methods in terms of normalized MSE\nacross a wide range of SNR conditions. In particular, the A-MMSE and its\nrank-adaptive extension establish a new frontier in the performance complexity\ntrade-off, redefining the standard for practical channel estimation methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u589e\u5f3a\u7684MMSE\uff08A-MMSE\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6a21\u578b\u9a71\u52a8\u7684DNN\u6846\u67b6\u548cTransformer\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u4f4e\u590d\u6742\u5ea6\u7684\u4fe1\u9053\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\u590d\u6742\u5ea6\u9ad8\uff0c\u800c\u4f20\u7edfMMSE\u4f9d\u8d56\u96be\u4ee5\u83b7\u53d6\u7684\u4e8c\u9636\u7edf\u8ba1\u91cf\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u6a21\u578b\u9a71\u52a8DNN\u6539\u8fdb\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6ce8\u610f\u529b\u7f16\u7801\u5668\uff0c\u5b66\u4e60\u6700\u4f18MMSE\u6ee4\u6ce2\u5668\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529bTransformer\u5b9e\u73b0\u5355\u6b21\u7ebf\u6027\u64cd\u4f5c\u5b8c\u6210\u4fe1\u9053\u4f30\u8ba1\uff0c\u907f\u514d\u4e86\u63a8\u7406\u4e2d\u7684\u975e\u7ebf\u6027\u6fc0\u6d3b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cA-MMSE\u57283GPP TDL\u4fe1\u9053\u6a21\u578b\u4e0b\uff0c\u5728\u591a\u79cdSNR\u6761\u4ef6\u4e0b\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u6027\u80fd\u4e0e\u590d\u6742\u5ea6\u6743\u8861\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "A-MMSE\u53ca\u5176\u79e9\u81ea\u9002\u5e94\u6269\u5c55\u4e3a\u5b9e\u9645\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002", "keywords": "OFDM, \u4fe1\u9053\u4f30\u8ba1, \u6ce8\u610f\u529b\u673a\u5236, MMSE, Transformer"}}
{"id": "2506.01478", "pdf": "https://arxiv.org/pdf/2506.01478", "abs": "https://arxiv.org/abs/2506.01478", "authors": ["Tung-Lam Ngo", "Ba-Hoang Tran", "Duy-Cat Can", "Trung-Hieu Do", "Oliver Y. Ch\u00e9n", "Hoang-Quynh Le"], "title": "MUDI: A Multimodal Biomedical Dataset for Understanding Pharmacodynamic Drug-Drug Interactions", "categories": ["cs.LG", "cs.CL", "cs.MM", "q-bio.QM"], "comment": null, "summary": "Understanding the interaction between different drugs (drug-drug interaction\nor DDI) is critical for ensuring patient safety and optimizing therapeutic\noutcomes. Existing DDI datasets primarily focus on textual information,\noverlooking multimodal data that reflect complex drug mechanisms. In this\npaper, we (1) introduce MUDI, a large-scale Multimodal biomedical dataset for\nUnderstanding pharmacodynamic Drug-drug Interactions, and (2) benchmark\nlearning methods to study it. In brief, MUDI provides a comprehensive\nmultimodal representation of drugs by combining pharmacological text, chemical\nformulas, molecular structure graphs, and images across 310,532 annotated drug\npairs labeled as Synergism, Antagonism, or New Effect. Crucially, to\neffectively evaluate machine-learning based generalization, MUDI consists of\nunseen drug pairs in the test set. We evaluate benchmark models using both late\nfusion voting and intermediate fusion strategies. All data, annotations,\nevaluation scripts, and baselines are released under an open research license.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aMUDI\u7684\u591a\u6a21\u6001\u751f\u7269\u533b\u5b66\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u836f\u6548\u5b66\u836f\u7269-\u836f\u7269\u76f8\u4e92\u4f5c\u7528\uff08DDI\uff09\uff0c\u5e76\u8bc4\u4f30\u4e86\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684DDI\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u53cd\u6620\u590d\u6742\u836f\u7269\u673a\u5236\u7684\u591a\u6a21\u6001\u6570\u636e\u3002", "method": "MUDI\u6570\u636e\u96c6\u7ed3\u5408\u4e86\u836f\u7406\u5b66\u6587\u672c\u3001\u5316\u5b66\u516c\u5f0f\u3001\u5206\u5b50\u7ed3\u6784\u56fe\u548c\u56fe\u50cf\uff0c\u5e76\u91c7\u7528\u665a\u671f\u878d\u5408\u6295\u7968\u548c\u4e2d\u671f\u878d\u5408\u7b56\u7565\u8bc4\u4f30\u6a21\u578b\u3002", "result": "MUDI\u5305\u542b310,532\u4e2a\u6807\u6ce8\u836f\u7269\u5bf9\uff0c\u6d4b\u8bd5\u96c6\u4e2d\u5305\u542b\u672a\u89c1\u836f\u7269\u5bf9\u4ee5\u8bc4\u4f30\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MUDI\u4e3a\u7814\u7a76DDI\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5e76\u4fc3\u8fdb\u4e86\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "keywords": "\u836f\u7269-\u836f\u7269\u76f8\u4e92\u4f5c\u7528, \u591a\u6a21\u6001\u6570\u636e\u96c6, \u836f\u6548\u5b66, \u673a\u5668\u5b66\u4e60, \u6cdb\u5316\u80fd\u529b"}}
{"id": "2506.01512", "pdf": "https://arxiv.org/pdf/2506.01512", "abs": "https://arxiv.org/abs/2506.01512", "authors": ["Meng Li", "Michael Vrazitulis", "David Schlangen"], "title": "Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes", "categories": ["cs.CL", "cs.AI"], "comment": "accepted by ACL 2025 (main)", "summary": "Rational speakers are supposed to know what they know and what they do not\nknow, and to generate expressions matching the strength of evidence. In\ncontrast, it is still a challenge for current large language models to generate\ncorresponding utterances based on the assessment of facts and confidence in an\nuncertain real-world environment. While it has recently become popular to\nestimate and calibrate confidence of LLMs with verbalized uncertainty, what is\nlacking is a careful examination of the linguistic knowledge of uncertainty\nencoded in the latent space of LLMs. In this paper, we draw on typological\nframeworks of epistemic expressions to evaluate LLMs' knowledge of epistemic\nmodality, using controlled stories. Our experiments show that the performance\nof LLMs in generating epistemic expressions is limited and not robust, and\nhence the expressions of uncertainty generated by LLMs are not always reliable.\nTo build uncertainty-aware LLMs, it is necessary to enrich semantic knowledge\nof epistemic modality in LLMs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u786e\u5b9a\u73b0\u5b9e\u73af\u5883\u4e2d\u751f\u6210\u57fa\u4e8e\u4e8b\u5b9e\u8bc4\u4f30\u548c\u4fe1\u5fc3\u7684\u8868\u8ff0\u65f6\u7684\u5c40\u9650\u6027\uff0c\u6307\u51fa\u5176\u8868\u8fbe\u7684\u53ef\u9760\u6027\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u8005\u5e0c\u671b\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u57fa\u4e8e\u4fe1\u5fc3\u8bc4\u4f30\u7684\u8868\u8fbe\u65f6\u7684\u8868\u73b0\uff0c\u5e76\u586b\u8865\u5bf9\u5176\u4e0d\u786e\u5b9a\u6027\u8bed\u8a00\u77e5\u8bc6\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5229\u7528\u7c7b\u578b\u5b66\u6846\u67b6\u548c\u53d7\u63a7\u6545\u4e8b\uff0c\u8bc4\u4f30\u6a21\u578b\u5bf9\u8ba4\u8bc6\u6a21\u6001\u7684\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u5728\u751f\u6210\u8ba4\u8bc6\u8868\u8fbe\u65f6\u8868\u73b0\u6709\u9650\u4e14\u4e0d\u7a33\u5b9a\uff0c\u5176\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\u4e0d\u53ef\u9760\u3002", "conclusion": "\u4e3a\u6784\u5efa\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u610f\u8bc6\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u9700\u589e\u5f3a\u5176\u5bf9\u8ba4\u8bc6\u6a21\u6001\u7684\u8bed\u4e49\u77e5\u8bc6\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u8ba4\u8bc6\u6a21\u6001, \u4e0d\u786e\u5b9a\u6027\u8868\u8fbe, \u7c7b\u578b\u5b66\u6846\u67b6"}}
{"id": "2506.01482", "pdf": "https://arxiv.org/pdf/2506.01482", "abs": "https://arxiv.org/abs/2506.01482", "authors": ["Zijian Zhao", "Dian Jin", "Zijing Zhou", "Xiaoyu Zhang"], "title": "Automatic Stage Lighting Control: Is it a Rule-Driven Process or Generative Task?", "categories": ["cs.LG", "cs.AI", "cs.MM", "eess.AS"], "comment": null, "summary": "Stage lighting plays an essential role in live music performances,\ninfluencing the engaging experience of both musicians and audiences. Given the\nhigh costs associated with hiring or training professional lighting engineers,\nAutomatic Stage Lighting Control (ASLC) has gained increasing attention.\nHowever, most existing approaches only classify music into limited categories\nand map them to predefined light patterns, resulting in formulaic and\nmonotonous outcomes that lack rationality. To address this issue, this paper\npresents an end-to-end solution that directly learns from experienced lighting\nengineers -- Skip-BART. To the best of our knowledge, this is the first work to\nconceptualize ASLC as a generative task rather than merely a classification\nproblem. Our method modifies the BART model to take audio music as input and\nproduce light hue and value (intensity) as output, incorporating a novel skip\nconnection mechanism to enhance the relationship between music and light within\nthe frame grid.We validate our method through both quantitative analysis and an\nhuman evaluation, demonstrating that Skip-BART outperforms conventional\nrule-based methods across all evaluation metrics and shows only a limited gap\ncompared to real lighting engineers.Specifically, our method yields a p-value\nof 0.72 in a statistical comparison based on human evaluations with human\nlighting engineers, suggesting that the proposed approach closely matches human\nlighting engineering performance. To support further research, we have made our\nself-collected dataset, code, and trained model parameters available at\nhttps://github.com/RS2002/Skip-BART .", "AI": {"tldr": "Skip-BART\u662f\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u821e\u53f0\u706f\u5149\u63a7\u5236\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5b66\u4e60\u4e13\u4e1a\u706f\u5149\u5e08\u7684\u7ecf\u9a8c\uff0c\u5c06ASLC\u4efb\u52a1\u89c6\u4e3a\u751f\u6210\u4efb\u52a1\u800c\u975e\u5206\u7c7b\u95ee\u9898\uff0c\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u96c7\u4f63\u6216\u57f9\u8bad\u4e13\u4e1a\u706f\u5149\u5e08\u6210\u672c\u9ad8\u6602\uff0c\u81ea\u52a8\u821e\u53f0\u706f\u5149\u63a7\u5236(ASLC)\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u8fc7\u4e8e\u516c\u5f0f\u5316\u3002", "method": "\u4fee\u6539BART\u6a21\u578b\uff0c\u4ee5\u97f3\u9891\u4e3a\u8f93\u5165\uff0c\u751f\u6210\u706f\u5149\u8272\u8c03\u548c\u5f3a\u5ea6\uff0c\u5e76\u5f15\u5165\u8df3\u8dc3\u8fde\u63a5\u673a\u5236\u589e\u5f3a\u97f3\u4e50\u4e0e\u706f\u5149\u7684\u5173\u8054\u3002", "result": "\u5b9a\u91cf\u5206\u6790\u548c\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\uff0cSkip-BART\u5728\u5404\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e0e\u771f\u5b9e\u706f\u5149\u5e08\u7684\u8868\u73b0\u63a5\u8fd1(p\u503c\u4e3a0.72)\u3002", "conclusion": "Skip-BART\u5c06ASLC\u89c6\u4e3a\u751f\u6210\u4efb\u52a1\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u4ee3\u7801\u652f\u6301\u3002", "keywords": "\u81ea\u52a8\u821e\u53f0\u706f\u5149\u63a7\u5236(ASLC), Skip-BART, \u751f\u6210\u4efb\u52a1, \u8df3\u8dc3\u8fde\u63a5, \u7aef\u5230\u7aef\u5b66\u4e60"}}
{"id": "2506.01520", "pdf": "https://arxiv.org/pdf/2506.01520", "abs": "https://arxiv.org/abs/2506.01520", "authors": ["Bobo Li", "Yuheng Wang", "Hao Fei", "Juncheng Li", "Wei Ji", "Mong-Li Lee", "Wynne Hsu"], "title": "FormFactory: An Interactive Benchmarking Suite for Multimodal Form-Filling Agents", "categories": ["cs.CL"], "comment": "8 pages, 7 figures", "summary": "Online form filling is a common yet labor-intensive task involving extensive\nkeyboard and mouse interactions. Despite the long-standing vision of automating\nthis process with \"one click\", existing tools remain largely rule-based and\nlack generalizable, generative capabilities. Recent advances in Multimodal\nLarge Language Models (MLLMs) have enabled promising agents for GUI-related\ntasks in general-purpose scenarios. However, they struggle with the unique\nchallenges of form filling, such as flexible layouts and the difficulty of\naligning textual instructions with on-screen fields. To bridge this gap, we\nformally define the form-filling task and propose FormFactory, an interactive\nbenchmarking suite comprising a web-based interface, backend evaluation module,\nand carefully constructed dataset. Our benchmark covers diverse real-world\nscenarios, incorporates various field formats, and simulates high-fidelity form\ninteractions. We conduct a comprehensive evaluation of state-of-the-art MLLMs\nand observe that no model surpasses 5% accuracy, underscoring the inherent\ndifficulty of the task. These findings also reveal significant limitations in\ncurrent models' visual layout reasoning and field-value alignment abilities. We\nhope our benchmark can serve as a stepping stone for further research into\nrobust, practical form-filling agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFormFactory\uff0c\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u5316\u8868\u5355\u586b\u5199\u7684\u4ea4\u4e92\u5f0f\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u8868\u5355\u586b\u5199\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff08\u51c6\u786e\u7387\u4e0d\u8db35%\uff09\u3002", "motivation": "\u5728\u7ebf\u8868\u5355\u586b\u5199\u662f\u4e00\u9879\u5e38\u89c1\u4f46\u52b3\u52a8\u5bc6\u96c6\u7684\u4efb\u52a1\uff0c\u73b0\u6709\u5de5\u5177\u591a\u4e3a\u57fa\u4e8e\u89c4\u5219\u4e14\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u3002\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728GUI\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u8868\u5355\u586b\u5199\u4efb\u52a1\u4e2d\u9762\u4e34\u72ec\u7279\u6311\u6218\u3002", "method": "\u63d0\u51faFormFactory\uff0c\u4e00\u4e2a\u5305\u542b\u7f51\u9875\u754c\u9762\u3001\u540e\u7aef\u8bc4\u4f30\u6a21\u5757\u548c\u6570\u636e\u96c6\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u6a21\u62df\u771f\u5b9e\u573a\u666f\u548c\u591a\u6837\u5316\u7684\u8868\u5355\u4ea4\u4e92\u5f62\u5f0f\u3002", "result": "\u5bf9\u5f53\u524d\u6700\u5148\u8fdb\u7684MLLMs\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u6ca1\u6709\u6a21\u578b\u8d85\u8fc75%\u7684\u51c6\u786e\u7387\uff0c\u7a81\u663e\u4e86\u4efb\u52a1\u96be\u5ea6\u53ca\u6a21\u578b\u5728\u89c6\u89c9\u5e03\u5c40\u63a8\u7406\u548c\u5b57\u6bb5\u503c\u5bf9\u9f50\u80fd\u529b\u7684\u4e0d\u8db3\u3002", "conclusion": "FormFactory\u53ef\u4f5c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u7a33\u5065\u3001\u5b9e\u7528\u8868\u5355\u586b\u5199\u4ee3\u7406\u7684\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "keywords": "\u8868\u5355\u586b\u5199\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u89c6\u89c9\u5e03\u5c40\u63a8\u7406\uff0c\u57fa\u51c6\u6d4b\u8bd5\uff0cFormFactory"}}
{"id": "2506.00455", "pdf": "https://arxiv.org/pdf/2506.00455", "abs": "https://arxiv.org/abs/2506.00455", "authors": ["Kordel K. France", "Ovidiu Daescu"], "title": "Diffusion Models for Increasing Accuracy in Olfaction Sensors and Datasets", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Robotic odour source localization (OSL) is a critical capability for\nautonomous systems operating in complex environments. However, current OSL\nmethods often suffer from ambiguities, particularly when robots misattribute\nodours to incorrect objects due to limitations in olfactory datasets and sensor\nresolutions. To address this challenge, we introduce a novel machine learning\nmethod using diffusion-based molecular generation to enhance odour localization\naccuracy that can be used by itself or with automated olfactory dataset\nconstruction pipelines with vision-language models (VLMs) This generative\nprocess of our diffusion model expands the chemical space beyond the\nlimitations of both current olfactory datasets and the training data of VLMs,\nenabling the identification of potential odourant molecules not previously\ndocumented. The generated molecules can then be more accurately validated using\nadvanced olfactory sensors which emulate human olfactory recognition through\nelectronic sensor arrays. By integrating visual analysis, language processing,\nand molecular generation, our framework enhances the ability of\nolfaction-vision models on robots to accurately associate odours with their\ncorrect sources, thereby improving navigation and decision-making in\nenvironments where olfactory cues are essential. Our methodology represents a\nfoundational advancement in the field of robotic olfaction, offering a scalable\nsolution to the challenges posed by limited olfactory data and sensor\nambiguities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u673a\u5668\u4eba\u6c14\u5473\u6e90\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u6c14\u5473\u6e90\u5b9a\u4f4d\u65b9\u6cd5\u56e0\u55c5\u89c9\u6570\u636e\u96c6\u548c\u4f20\u611f\u5668\u5206\u8fa8\u7387\u9650\u5236\u5bfc\u81f4\u7684\u8bef\u5224\u95ee\u9898\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u5206\u5b50\u6269\u5c55\u5316\u5b66\u7a7a\u95f4\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u7535\u5b50\u4f20\u611f\u5668\u9a8c\u8bc1\u751f\u6210\u5206\u5b50\u3002", "result": "\u63d0\u5347\u4e86\u55c5\u89c9-\u89c6\u89c9\u6a21\u578b\u5bf9\u6c14\u5473\u6e90\u7684\u51c6\u786e\u5173\u8054\u80fd\u529b\uff0c\u6539\u5584\u4e86\u673a\u5668\u4eba\u5728\u4f9d\u8d56\u55c5\u89c9\u7ebf\u7d22\u7684\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u4e0e\u51b3\u7b56\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u55c5\u89c9\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u8db3\u548c\u4f20\u611f\u5668\u6a21\u7cca\u6027\u95ee\u9898\u3002", "keywords": "\u673a\u5668\u4eba\u55c5\u89c9,\u6269\u6563\u6a21\u578b,\u89c6\u89c9\u8bed\u8a00\u6a21\u578b,\u5206\u5b50\u751f\u6210,\u6c14\u5473\u6e90\u5b9a\u4f4d"}}
{"id": "2506.01486", "pdf": "https://arxiv.org/pdf/2506.01486", "abs": "https://arxiv.org/abs/2506.01486", "authors": ["Jelke Wibbeke", "Sebastian Rohjans", "Andreas Rauh"], "title": "Model-agnostic Mitigation Strategies of Data Imbalance for Regression", "categories": ["cs.LG", "62J20 (primary) 68T05 (secondary)", "I.2.6"], "comment": "34 pages, 11 figures, to be submitted to Springer Nature Machine\n  Learning", "summary": "Data imbalance persists as a pervasive challenge in regression tasks,\nintroducing bias in model performance and undermining predictive reliability.\nThis is particularly detrimental in applications aimed at predicting rare\nevents that fall outside the domain of the bulk of the training data. In this\nstudy, we review the current state-of-the-art regarding sampling-based methods\nand cost-sensitive learning. Additionally, we propose novel approaches to\nmitigate model bias. To better asses the importance of data, we introduce the\ndensity-distance and density-ratio relevance functions, which effectively\nintegrate empirical frequency of data with domain-specific preferences,\noffering enhanced interpretability for end-users. Furthermore, we present\nadvanced mitigation techniques (cSMOGN and crbSMOGN), which build upon and\nimprove existing sampling methods. In a comprehensive quantitative evaluation,\nwe benchmark state-of-the-art methods on 10 synthetic and 42 real-world\ndatasets, using neural networks, XGBoosting trees and Random Forest models. Our\nanalysis reveals that while most strategies improve performance on rare\nsamples, they often degrade it on frequent ones. We demonstrate that\nconstructing an ensemble of models -- one trained with imbalance mitigation and\nanother without -- can significantly reduce these negative effects. The key\nfindings underscore the superior performance of our novel crbSMOGN sampling\ntechnique with the density-ratio relevance function for neural networks,\noutperforming state-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u56de\u5f52\u4efb\u52a1\u4e2d\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u65b9\u6cd5\uff08\u5982\u5bc6\u5ea6-\u8ddd\u79bb\u548c\u5bc6\u5ea6-\u6bd4\u76f8\u5173\u6027\u51fd\u6570\uff09\u548c\u6539\u8fdb\u7684\u91c7\u6837\u6280\u672f\uff08cSMOGN\u548ccrbSMOGN\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6570\u636e\u4e0d\u5e73\u8861\u5bfc\u81f4\u6a21\u578b\u504f\u5dee\u548c\u9884\u6d4b\u4e0d\u53ef\u9760\uff0c\u7279\u522b\u662f\u5728\u9884\u6d4b\u7f55\u89c1\u4e8b\u4ef6\u65f6\uff0c\u5f71\u54cd\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u5bc6\u5ea6-\u8ddd\u79bb\u548c\u5bc6\u5ea6-\u6bd4\u76f8\u5173\u6027\u51fd\u6570\uff0c\u4ee5\u53ca\u6539\u8fdb\u7684\u91c7\u6837\u6280\u672fcSMOGN\u548ccrbSMOGN\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u5728\u63d0\u5347\u7f55\u89c1\u6837\u672c\u6027\u80fd\u7684\u540c\u65f6\uff0c\u53ef\u80fd\u5bf9\u5e38\u89c1\u6837\u672c\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff1b\u4f46\u901a\u8fc7\u6a21\u578b\u96c6\u6210\u53ef\u663e\u8457\u51cf\u5c11\u8fd9\u4e9b\u8d1f\u9762\u5f71\u54cd\u3002crbSMOGN\u6280\u672f\u7ed3\u5408\u5bc6\u5ea6-\u6bd4\u76f8\u5173\u6027\u51fd\u6570\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u65b0\u65b9\u6cd5\u5728\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u6a21\u578b\u96c6\u6210\u7684\u91cd\u8981\u6027\u3002", "keywords": "\u6570\u636e\u4e0d\u5e73\u8861, \u56de\u5f52\u4efb\u52a1, \u7f55\u89c1\u4e8b\u4ef6\u9884\u6d4b, \u91c7\u6837\u6280\u672f, \u5bc6\u5ea6\u51fd\u6570, \u6a21\u578b\u96c6\u6210"}}
{"id": "2506.01524", "pdf": "https://arxiv.org/pdf/2506.01524", "abs": "https://arxiv.org/abs/2506.01524", "authors": ["Qi Lin", "Weikai Xu", "Lisi Chen", "Bin Dai"], "title": "V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the continued proliferation of Large Language Model (LLM) based\nchatbots, there is a growing demand for generating responses that are not only\nlinguistically fluent but also consistently aligned with persona-specific\ntraits in conversations. However, existing role-play and persona-based chat\napproaches rely heavily on static role descriptions, coarse-grained signal\nspace, and low-quality synthetic data, which fail to capture dynamic\nfine-grained details in human-like chat. Human-like chat requires modeling\nsubtle latent traits, such as emotional tone, situational awareness, and\nevolving personality, which are difficult to predefine and cannot be easily\nlearned from synthetic or distillation-based data. To address these\nlimitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework,\ncontaining a variational auto-encoding module and fine-grained control space\nwhich dynamically adapts dialogue behaviour based on fine-grained,\ninterpretable latent variables across talking style, interaction patterns, and\npersonal attributes. We also construct a high-quality dataset, HumanChatData,\nand benchmark HumanChatBench to address the scarcity of high-quality data in\nthe human-like domain. Experiments show that LLMs based on V-VAE consistently\noutperform standard baselines on HumanChatBench and DialogBench, which further\ndemonstrates the effectiveness of V-VAE and HumanChatData.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdVerbalfine-grained\u7ec6\u8282\u7684\u5bf9\u8bdd\u6846\u67b6V-VAE\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u53d8\u91cf\u751f\u6210\u4e00\u81f4\u6027\u9ad8\u7684\u4e2a\u6027\u5316\u5bf9\u8bdd\uff0c\u5e76\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6HumanChatData\u548c\u8bc4\u4f30\u6807\u51c6HumanChatBench\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u89d2\u8272\u7684\u804a\u5929\u65b9\u6cd5\u5728\u52a8\u6001\u3001\u7ec6\u7c92\u5ea6\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u4e0a\u7684\u4e0d\u8db3\uff0c\u751f\u6210\u66f4\u7b26\u5408\u4eba\u7c7b\u5bf9\u8bdd\u98ce\u683c\u7684\u54cd\u5e94\u3002", "method": "\u91c7\u7528\u53d8\u5206\u81ea\u7f16\u7801\uff08V-VAE\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u63a7\u5236\u7a7a\u95f4\u548c\u52a8\u6001\u9002\u5e94\u6a21\u5757\uff0c\u4ece\u9ad8\u8d28\u91cf\u6570\u636eHumanChatData\u4e2d\u5b66\u4e60\u6f5c\u5728\u53d8\u91cf\u3002", "result": "\u5728HumanChatBench\u548cDialogBench\u4e0a\uff0c\u57fa\u4e8eV-VAE\u7684LLM\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u57fa\u7ebf\u3002", "conclusion": "V-VAE\u548cHumanChatData\u6709\u6548\u63d0\u5347\u4e86\u5bf9\u8bdd\u751f\u6210\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e2a\u6027\u5316\u804a\u5929\uff0c\u53d8\u5206\u81ea\u7f16\u7801\uff0c\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u9ad8\u8d28\u91cf\u6570\u636e"}}
{"id": "2506.01490", "pdf": "https://arxiv.org/pdf/2506.01490", "abs": "https://arxiv.org/abs/2506.01490", "authors": ["Yanxi Luo", "Shijin Wang", "Zhongxing Xu", "Yulong Li", "Feilong Tang", "Jionglong Su"], "title": "Confidence-Aware Self-Distillation for Multimodal Sentiment Analysis with Incomplete Modalities", "categories": ["cs.LG"], "comment": null, "summary": "Multimodal sentiment analysis (MSA) aims to understand human sentiment\nthrough multimodal data. In real-world scenarios, practical factors often lead\nto uncertain modality missingness. Existing methods for handling modality\nmissingness are based on data reconstruction or common subspace projections.\nHowever, these methods neglect the confidence in multimodal combinations and\nimpose constraints on intra-class representation, hindering the capture of\nmodality-specific information and resulting in suboptimal performance. To\naddress these challenges, we propose a Confidence-Aware Self-Distillation\n(CASD) strategy that effectively incorporates multimodal probabilistic\nembeddings via a mixture of Student's $t$-distributions, enhancing its\nrobustness by incorporating confidence and accommodating heavy-tailed\nproperties. This strategy estimates joint distributions with uncertainty scores\nand reduces uncertainty in the student network by consistency distillation.\nFurthermore, we introduce a reparameterization representation module that\nfacilitates CASD in robust multimodal learning by sampling embeddings from the\njoint distribution for the prediction module to calculate the task loss. As a\nresult, the directional constraint from the loss minimization is alleviated by\nthe sampled representation. Experimental results on three benchmark datasets\ndemonstrate that our method achieves state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u611f\u77e5\u81ea\u84b8\u998f\uff08CASD\uff09\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u6982\u7387\u5d4c\u5165\u548c\u4e0d\u786e\u5b9a\u6027\u8bc4\u5206\uff0c\u89e3\u51b3\u4e86\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u5e76\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u6a21\u6001\u7f3a\u5931\u95ee\u9898\u5e38\u89c1\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u591a\u6a21\u6001\u7ec4\u5408\u7684\u7f6e\u4fe1\u5ea6\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u4e3a\u6b64\uff0c\u63d0\u51faCASD\u7b56\u7565\u6765\u63d0\u5347\u9c81\u68d2\u6027\u3002", "method": "CASD\u7b56\u7565\u5229\u7528\u5b66\u751f$t$-\u5206\u5e03\u6df7\u5408\u591a\u6a21\u6001\u6982\u7387\u5d4c\u5165\uff0c\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u548c\u91cd\u5c3e\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u4e00\u81f4\u6027\u84b8\u998f\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u5f15\u5165\u91cd\u53c2\u6570\u5316\u8868\u793a\u6a21\u5757\uff0c\u901a\u8fc7\u4ece\u8054\u5408\u5206\u5e03\u91c7\u6837\u5d4c\u5165\u8ba1\u7b97\u4efb\u52a1\u635f\u5931\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "CASD\u7b56\u7565\u901a\u8fc7\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u8bc4\u5206\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5bf9\u6a21\u6001\u7f3a\u5931\u7684\u9c81\u68d2\u6027\u3002", "keywords": "multimodal sentiment analysis, modality missingness, confidence-aware, self-distillation, robustness"}}
{"id": "2506.01531", "pdf": "https://arxiv.org/pdf/2506.01531", "abs": "https://arxiv.org/abs/2506.01531", "authors": ["Wenhao Liu", "Zhenyi Lu", "Xinyu Hu", "Jierui Zhang", "Dailin Li", "Jiacheng Cen", "Huilin Cao", "Haiteng Wang", "Yuhan Li", "Kun Xie", "Dandan Li", "Pei Zhang", "Chengbo Zhang", "Yuxiang Ren", "Xiaohong Huang", "Yan Ma"], "title": "STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop Multi-Agent Framework", "categories": ["cs.CL"], "comment": "accepted by ACL2025", "summary": "High-quality math datasets are crucial for advancing the reasoning abilities\nof large language models (LLMs). However, existing datasets often suffer from\nthree key issues: outdated and insufficient challenging content, neglecting\nhuman-like reasoning, and limited reliability due to single-LLM generation. To\naddress these, we introduce $\\textbf{STORM-BORN}$, an ultra-challenging dataset\nof mathematical derivations sourced from cutting-edge academic papers, which\nincludes dense human-like approximations and heuristic cues. To ensure the\nreliability and quality, we propose a novel human-in-the-loop, multi-agent data\ngeneration framework, integrating reasoning-dense filters, multi-agent\ncollaboration, and human mathematicians' evaluations. We curated a set of 2,000\nsynthetic samples and deliberately selected the 100 most difficult problems.\nEven most advanced models like GPT-o1 solved fewer than $5\\%$ of them.\nFine-tuning on STORM-BORN boosts accuracy by $7.84\\%$ (LLaMA3-8B) and $9.12\\%$\n(Qwen2.5-7B). As AI approaches mathematician-level reasoning, STORM-BORN\nprovides both a high-difficulty benchmark and a human-like reasoning training\nresource. Our code and dataset are publicly available at\nhttps://github.com/lwhere/STORM-BORN.", "AI": {"tldr": "STORM-BORN\u662f\u4e00\u4e2a\u8d85\u5177\u6311\u6218\u6027\u7684\u6570\u5b66\u6570\u636e\u96c6\uff0c\u6e90\u4e8e\u524d\u6cbf\u5b66\u672f\u8bba\u6587\uff0c\u5305\u542b\u5bc6\u96c6\u7684\u4eba\u7c7b\u63a8\u7406\u548c\u542f\u53d1\u5f0f\u63d0\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5185\u5bb9\u8fc7\u65f6\u3001\u63a8\u7406\u4e0d\u8db3\u548c\u53ef\u9760\u6027\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u6570\u636e\u96c6\u5b58\u5728\u5185\u5bb9\u8fc7\u65f6\u3001\u7f3a\u4e4f\u6311\u6218\u6027\u3001\u5ffd\u89c6\u4eba\u7c7b\u63a8\u7406\u548c\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0cSTORM-BORN\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4eba\u5728\u56de\u8def\u7684\u591a\u667a\u80fd\u4f53\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u63a8\u7406\u5bc6\u96c6\u578b\u8fc7\u6ee4\u5668\u3001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u4eba\u7c7b\u6570\u5b66\u5bb6\u7684\u8bc4\u4f30\u3002", "result": "\u751f\u6210\u76842000\u4e2a\u6837\u672c\u4e2d\uff0c\u6700\u96be\u7684100\u4e2a\u95ee\u9898\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684GPT-o1\u6a21\u578b\u4e5f\u4ec5\u89e3\u51b3\u4e0d\u52305%\uff0c\u5fae\u8c03\u540eLLaMA3-8B\u548cQwen2.5-7B\u7684\u51c6\u786e\u7387\u5206\u522b\u63d0\u53477.84%\u548c9.12%\u3002", "conclusion": "STORM-BORN\u4e3aAI\u63d0\u4f9b\u4e86\u9ad8\u96be\u5ea6\u57fa\u51c6\u548c\u4eba\u7c7b\u63a8\u7406\u8bad\u7ec3\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u6570\u5b66\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u8fdb\u6b65\u3002", "keywords": "\u6570\u5b66\u6570\u636e\u96c6,\u4eba\u7c7b\u63a8\u7406,\u591a\u667a\u80fd\u4f53\u751f\u6210,\u9ad8\u96be\u5ea6\u57fa\u51c6"}}
{"id": "2506.01502", "pdf": "https://arxiv.org/pdf/2506.01502", "abs": "https://arxiv.org/abs/2506.01502", "authors": ["Mikhail Persiianov", "Jiawei Chen", "Petr Mokrov", "Alexander Tyurin", "Evgeny Burnaev", "Alexander Korotin"], "title": "Learning of Population Dynamics: Inverse Optimization Meets JKO Scheme", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Learning population dynamics involves recovering the underlying process that\ngoverns particle evolution, given evolutionary snapshots of samples at discrete\ntime points. Recent methods frame this as an energy minimization problem in\nprobability space and leverage the celebrated JKO scheme for efficient time\ndiscretization. In this work, we introduce $\\texttt{iJKOnet}$, an approach that\ncombines the JKO framework with inverse optimization techniques to learn\npopulation dynamics. Our method relies on a conventional $\\textit{end-to-end}$\nadversarial training procedure and does not require restrictive architectural\nchoices, e.g., input-convex neural networks. We establish theoretical\nguarantees for our methodology and demonstrate improved performance over prior\nJKO-based methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408JKO\u6846\u67b6\u548c\u9006\u4f18\u5316\u6280\u672f\u7684\u65b9\u6cd5$\texttt{iJKOnet}$\uff0c\u7528\u4e8e\u5b66\u4e60\u79cd\u7fa4\u52a8\u6001\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u5b9e\u73b0\uff0c\u65e0\u9700\u7279\u5b9a\u7f51\u7edc\u67b6\u6784\uff0c\u4f18\u4e8e\u73b0\u6709JKO\u65b9\u6cd5\u3002", "motivation": "\u5b66\u4e60\u79cd\u7fa4\u52a8\u6001\u9700\u8981\u4ece\u79bb\u6563\u65f6\u95f4\u70b9\u7684\u6837\u672c\u5feb\u7167\u4e2d\u6062\u590d\u7c92\u5b50\u6f14\u5316\u7684\u6f5c\u5728\u8fc7\u7a0b\u3002\u73b0\u6709\u65b9\u6cd5\u5c06\u5176\u89c6\u4e3a\u6982\u7387\u7a7a\u95f4\u4e2d\u7684\u80fd\u91cf\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u5e76\u5229\u7528JKO\u65b9\u6848\u8fdb\u884c\u65f6\u95f4\u79bb\u6563\u5316\u3002", "method": "\u63d0\u51fa$\texttt{iJKOnet}$\u65b9\u6cd5\uff0c\u5c06JKO\u6846\u67b6\u4e0e\u9006\u4f18\u5316\u6280\u672f\u7ed3\u5408\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u5b9e\u73b0\uff0c\u65e0\u9700\u7279\u5b9a\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u7406\u8bba\u5206\u6790\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709JKO\u65b9\u6cd5\u3002", "conclusion": "$\texttt{iJKOnet}$\u5728\u5b66\u4e60\u548c\u6a21\u62df\u79cd\u7fa4\u52a8\u6001\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9650\u5236\u3002", "keywords": "JKO\u6846\u67b6, \u9006\u4f18\u5316, \u79cd\u7fa4\u52a8\u6001, \u5bf9\u6297\u8bad\u7ec3"}}
{"id": "2506.01535", "pdf": "https://arxiv.org/pdf/2506.01535", "abs": "https://arxiv.org/abs/2506.01535", "authors": ["Haruki Sakajo", "Yusuke Ide", "Justin Vasselli", "Yusuke Sakai", "Yingtao Tian", "Hidetaka Kamigaito", "Taro Watanabe"], "title": "Dictionaries to the Rescue: Cross-Lingual Vocabulary Transfer for Low-Resource Languages Using Bilingual Dictionaries", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "Cross-lingual vocabulary transfer plays a promising role in adapting\npre-trained language models to new languages, including low-resource languages.\nExisting approaches that utilize monolingual or parallel corpora face\nchallenges when applied to languages with limited resources. In this work, we\npropose a simple yet effective vocabulary transfer method that utilizes\nbilingual dictionaries, which are available for many languages, thanks to\ndescriptive linguists. Our proposed method leverages a property of BPE\ntokenizers where removing a subword from the vocabulary causes a fallback to\nshorter subwords. The embeddings of target subwords are estimated iteratively\nby progressively removing them from the tokenizer. The experimental results\nshow that our approach outperforms existing methods for low-resource languages,\ndemonstrating the effectiveness of a dictionary-based approach for\ncross-lingual vocabulary transfer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u8bed\u8bcd\u5178\u7684\u8bcd\u6c47\u8fc1\u79fb\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u901a\u8fc7\u9010\u6b65\u79fb\u9664\u76ee\u6807\u5b50\u8bcd\u5e76\u8fed\u4ee3\u4f30\u8ba1\u5176\u5d4c\u5165\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u8bcd\u6c47\u8fc1\u79fb\u4e2d\u7684\u6311\u6218\uff0c\u5229\u7528\u5e7f\u6cdb\u53ef\u7528\u7684\u53cc\u8bed\u8bcd\u5178\u3002", "method": "\u5229\u7528BPE\u5206\u8bcd\u5668\u7279\u6027\uff0c\u9010\u6b65\u79fb\u9664\u76ee\u6807\u5b50\u8bcd\u5e76\u8fed\u4ee3\u4f30\u8ba1\u5176\u5d4c\u5165\u3002", "result": "\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u8bcd\u5178\u7684\u8de8\u8bed\u8a00\u8bcd\u6c47\u8fc1\u79fb\u65b9\u6cd5\u7b80\u5355\u6709\u6548\u3002", "keywords": "\u8de8\u8bed\u8a00,\u8bcd\u6c47\u8fc1\u79fb,\u4f4e\u8d44\u6e90\u8bed\u8a00,\u53cc\u8bed\u8bcd\u5178,BPE"}}
{"id": "2506.00462", "pdf": "https://arxiv.org/pdf/2506.00462", "abs": "https://arxiv.org/abs/2506.00462", "authors": ["Ioan-Paul Ciobanu", "Andrei-Iulian Hiji", "Nicolae-Catalin Ristea", "Paul Irofti", "Cristian Rusu", "Radu Tudor Ionescu"], "title": "XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "comment": null, "summary": "Recent advances in audio generation led to an increasing number of deepfakes,\nmaking the general public more vulnerable to financial scams, identity theft,\nand misinformation. Audio deepfake detectors promise to alleviate this issue,\nwith many recent studies reporting accuracy rates close to 99%. However, these\nmethods are typically tested in an in-domain setup, where the deepfake samples\nfrom the training and test sets are produced by the same generative models. To\nthis end, we introduce XMAD-Bench, a large-scale cross-domain multilingual\naudio deepfake benchmark comprising 668.8 hours of real and deepfake speech. In\nour novel dataset, the speakers, the generative methods, and the real audio\nsources are distinct across training and test splits. This leads to a\nchallenging cross-domain evaluation setup, where audio deepfake detectors can\nbe tested ``in the wild''. Our in-domain and cross-domain experiments indicate\na clear disparity between the in-domain performance of deepfake detectors,\nwhich is usually as high as 100%, and the cross-domain performance of the same\nmodels, which is sometimes similar to random chance. Our benchmark highlights\nthe need for the development of robust audio deepfake detectors, which maintain\ntheir generalization capacity across different languages, speakers, generative\nmethods, and data sources. Our benchmark is publicly released at\nhttps://github.com/ristea/xmad-bench/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86XMAD-Bench\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u8de8\u9886\u57df\u591a\u8bed\u8a00\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u7528\u4e8e\u68c0\u6d4b\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u516c\u4f17\u9762\u4e34\u66f4\u591a\u91d1\u878d\u8bc8\u9a97\u3001\u8eab\u4efd\u76d7\u7a83\u548c\u865a\u5047\u4fe1\u606f\u7684\u98ce\u9669\u3002\u5f53\u524d\u68c0\u6d4b\u5668\u5728\u57df\u5185\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8de8\u57df\u6027\u80fd\u8f83\u5dee\uff0c\u4e9f\u9700\u7814\u7a76\u5176\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86XMAD-Bench\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5305\u542b668.8\u5c0f\u65f6\u7684\u771f\u5b9e\u548c\u4f2a\u9020\u8bed\u97f3\u6570\u636e\uff0c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u96c6\u7684\u8bf4\u8bdd\u4eba\u3001\u751f\u6210\u65b9\u6cd5\u548c\u771f\u5b9e\u97f3\u9891\u6765\u6e90\u5747\u4e0d\u76f8\u540c\uff0c\u4ee5\u6784\u5efa\u8de8\u57df\u8bc4\u4f30\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u57df\u5185\u6027\u80fd\u53ef\u8fbe100%\uff0c\u4f46\u8de8\u57df\u6027\u80fd\u6709\u65f6\u63a5\u8fd1\u968f\u673a\u731c\u6d4b\uff0c\u8868\u660e\u5f53\u524d\u68c0\u6d4b\u5668\u5728\u8de8\u57df\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "XMAD-Bench\u51f8\u663e\u4e86\u5f00\u53d1\u8de8\u8bed\u8a00\u3001\u8bf4\u8bdd\u4eba\u3001\u751f\u6210\u65b9\u6cd5\u548c\u6570\u636e\u6e90\u90fd\u80fd\u4fdd\u6301\u9c81\u68d2\u6027\u7684\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u7684\u5fc5\u8981\u6027\u3002", "keywords": "\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u3001\u8de8\u57df\u8bc4\u4f30\u3001\u57fa\u51c6\u6d4b\u8bd5\u96c6\u3001\u6cdb\u5316\u80fd\u529b"}}
{"id": "2506.01503", "pdf": "https://arxiv.org/pdf/2506.01503", "abs": "https://arxiv.org/abs/2506.01503", "authors": ["Benedikt Hilmes", "Nick Rossenbach", "Ralf Schl\u00fcter"], "title": "Analyzing the Importance of Blank for CTC-Based Knowledge Distillation", "categories": ["cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted for Interspeech 2025", "summary": "With the rise of large pre-trained foundation models for automatic speech\nrecognition new challenges appear. While the performance of these models is\ngood, runtime and cost of inference increases. One approach to make use of\ntheir strength while retaining efficiency is to distill their knowledge to\nsmaller models during training. In this work, we explore different CTC-based\ndistillation variants, focusing on blank token handling. We show that common\napproaches like blank elimination do not always work off the shelf. We explore\nnew blank selection patterns as a potential sweet spot between standard\nknowledge distillation and blank elimination mechanisms. Through the\nintroduction of a symmetric selection method, we are able to remove the CTC\nloss during knowledge distillation with minimal to no performance degradation.\nWith this, we make the training independent from target labels, potentially\nallowing for distillation on untranscribed audio data.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86CTC\u84b8\u998f\u7684\u53d8\u4f53\uff0c\u7279\u522b\u662f\u7a7a\u767d\u6807\u8bb0\u5904\u7406\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u79f0\u9009\u62e9\u65b9\u6cd5\u4ee5\u51cf\u5c11CTC\u635f\u5931\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4e2d\u6027\u80fd\u4f18\u79c0\u4f46\u63a8\u7406\u6210\u672c\u9ad8\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u63d0\u9ad8\u5c0f\u578b\u6a21\u578b\u7684\u6548\u7387\u3002", "method": "\u63a2\u7d22\u4e86\u4e0d\u540c\u7684CTC\u84b8\u998f\u53d8\u4f53\u548c\u7a7a\u767d\u6807\u8bb0\u5904\u7406\u65b9\u5f0f\uff0c\u63d0\u51fa\u4e86\u5bf9\u79f0\u9009\u62e9\u65b9\u6cd5\u4ee5\u51cf\u5c11\u5bf9\u76ee\u6807\u6807\u7b7e\u7684\u4f9d\u8d56\u3002", "result": "\u901a\u8fc7\u5bf9\u79f0\u9009\u62e9\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u964d\u4f4e\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u79fb\u9664CTC\u635f\u5931\uff0c\u652f\u6301\u65e0\u8f6c\u5f55\u97f3\u9891\u6570\u636e\u7684\u84b8\u998f\u3002", "conclusion": "\u5bf9\u79f0\u9009\u62e9\u65b9\u6cd5\u4e3aCTC\u84b8\u998f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u53ef\u80fd\u652f\u6301\u65e0\u6807\u7b7e\u6570\u636e\u7684\u5e94\u7528\u3002", "keywords": "\u77e5\u8bc6\u84b8\u998f, CTC, \u7a7a\u767d\u6807\u8bb0, \u8bed\u97f3\u8bc6\u522b, \u9884\u8bad\u7ec3\u6a21\u578b"}}
{"id": "2506.01565", "pdf": "https://arxiv.org/pdf/2506.01565", "abs": "https://arxiv.org/abs/2506.01565", "authors": ["Li Zhou", "Lutong Yu", "Dongchu Xie", "Shaohuan Cheng", "Wenyan Li", "Haizhou Li"], "title": "Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation", "categories": ["cs.CL", "cs.CV"], "comment": "cultural analysis, cultural visual understanding, cultural image\n  transcreation", "summary": "Culture is a rich and dynamic domain that evolves across both geography and\ntime. However, existing studies on cultural understanding with vision-language\nmodels (VLMs) primarily emphasize geographic diversity, often overlooking the\ncritical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a\nnovel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning\nancient Chinese dynasties, serves as a representative cultural heritage that\nreflects the profound temporal aspects of Chinese culture while remaining\nhighly popular in Chinese contemporary society. Hanfu-Bench comprises two core\ntasks: cultural visual understanding and cultural image transcreation.The\nformer task examines temporal-cultural feature recognition based on single- or\nmulti-image inputs through multiple-choice visual question answering, while the\nlatter focuses on transforming traditional attire into modern designs through\ncultural element inheritance and modern context adaptation. Our evaluation\nshows that closed VLMs perform comparably to non-experts on visual cutural\nunderstanding but fall short by 10\\% to human experts, while open VLMs lags\nfurther behind non-experts. For the transcreation task, multi-faceted human\nevaluation indicates that the best-performing model achieves a success rate of\nonly 42\\%. Our benchmark provides an essential testbed, revealing significant\nchallenges in this new direction of temporal cultural understanding and\ncreative adaptation.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86Hanfu-Bench\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u6587\u5316\u7684\u65f6\u7a7a\u591a\u6837\u6027\uff0c\u7528\u4e8e\u8bc4\u6d4b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u4f20\u7edf\u6587\u5316\u548c\u73b0\u4ee3\u9002\u5e94\u6027\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6587\u5316\u7684\u5730\u7406\u591a\u6837\u6027\uff0c\u5ffd\u7565\u65f6\u95f4\u7ef4\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u4e13\u5bb6\u7b56\u5212\u7684Hanfu-Bench\u6570\u636e\u96c6\uff0c\u5305\u542b\u6587\u5316\u89c6\u89c9\u7406\u89e3\u548c\u56fe\u50cf\u8f6c\u521b\u4e24\u5927\u4efb\u52a1\uff0c\u8bc4\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u5c01\u95edVLMs\u5728\u6587\u5316\u89c6\u89c9\u7406\u89e3\u4e0a\u4e0e\u975e\u4e13\u5bb6\u76f8\u5f53\uff0c\u4f46\u843d\u540e\u4e13\u5bb610%\uff1b\u5f00\u653eVLMs\u8868\u73b0\u66f4\u5dee\u3002\u8f6c\u521b\u4efb\u52a1\u4e2d\u6700\u4f73\u6a21\u578b\u6210\u529f\u7387\u4ec542%\u3002", "conclusion": "Hanfu-Bench\u63ed\u793a\u4e86\u65f6\u95f4\u6587\u5316\u7406\u89e3\u548c\u521b\u610f\u9002\u5e94\u7684\u91cd\u5927\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u5e73\u53f0\u3002", "keywords": "\u6587\u5316\u7406\u89e3,\u89c6\u89c9\u8bed\u8a00\u6a21\u578b,Hanfu-Bench,\u65f6\u95f4\u7ef4\u5ea6,\u8f6c\u521b"}}
{"id": "2506.01522", "pdf": "https://arxiv.org/pdf/2506.01522", "abs": "https://arxiv.org/abs/2506.01522", "authors": ["Peter Sorrenson", "Lukas L\u00fchrs", "Hans Olischl\u00e4ger", "Ullrich K\u00f6the"], "title": "Beyond Diagonal Covariance: Flexible Posterior VAEs via Free-Form Injective Flows", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Variational Autoencoders (VAEs) are powerful generative models widely used\nfor learning interpretable latent spaces, quantifying uncertainty, and\ncompressing data for downstream generative tasks. VAEs typically rely on\ndiagonal Gaussian posteriors due to computational constraints. Using arguments\ngrounded in differential geometry, we demonstrate inherent limitations in the\nrepresentational capacity of diagonal covariance VAEs, as illustrated by\nexplicit low-dimensional examples. In response, we show that a regularized\nvariant of the recently introduced Free-form Injective Flow (FIF) can be\ninterpreted as a VAE featuring a highly flexible, implicitly defined posterior.\nCrucially, this regularization yields a posterior equivalent to a full Gaussian\ncovariance distribution, yet maintains computational costs comparable to\nstandard diagonal covariance VAEs. Experiments on image datasets validate our\napproach, demonstrating that incorporating full covariance substantially\nimproves model likelihood.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAEs\uff09\u5728\u8868\u793a\u80fd\u529b\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u901a\u8fc7\u6b63\u5219\u5316\u81ea\u7531\u5f62\u5f0f\u6ce8\u5165\u6d41\uff08FIF\uff09\u6765\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u5bf9\u89d2\u534f\u65b9\u5deeVAEs\u3002", "motivation": "\u4f20\u7edf\u7684VAEs\u7531\u4e8e\u8ba1\u7b97\u9650\u5236\u901a\u5e38\u91c7\u7528\u5bf9\u89d2\u9ad8\u65af\u540e\u9a8c\uff0c\u4f46\u5176\u8868\u793a\u80fd\u529b\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5c55\u793a\u5982\u4f55\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u540e\u9a8c\u5206\u5e03\u7684\u7075\u6d3b\u6027\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e\u5fae\u5206\u51e0\u4f55\u7406\u8bba\u5206\u6790\u4e86\u5bf9\u89d2\u534f\u65b9\u5deeVAEs\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6b63\u5219\u5316\u7684\u81ea\u7531\u5f62\u5f0f\u6ce8\u5165\u6d41\uff08FIF\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u5168\u9ad8\u65af\u534f\u65b9\u5dee\u540e\u9a8c\u5206\u5e03\u3002", "result": "\u5728\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u91c7\u7528\u5168\u534f\u65b9\u5dee\u540e\u9a8c\u5206\u5e03\u7684\u6a21\u578b\u5728\u4f3c\u7136\u51fd\u6570\u8868\u73b0\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5bf9\u89d2\u534f\u65b9\u5deeVAEs\u3002", "conclusion": "\u901a\u8fc7\u6b63\u5219\u5316FIF\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347VAEs\u7684\u751f\u6210\u80fd\u529b\u3002", "keywords": "\u53d8\u5206\u81ea\u7f16\u7801\u5668, \u81ea\u7531\u5f62\u5f0f\u6ce8\u5165\u6d41, \u5fae\u5206\u51e0\u4f55, \u540e\u9a8c\u5206\u5e03, \u9ad8\u65af\u534f\u65b9\u5dee"}}
{"id": "2506.01578", "pdf": "https://arxiv.org/pdf/2506.01578", "abs": "https://arxiv.org/abs/2506.01578", "authors": ["Philipp Schoenegger", "Cameron R. Jones", "Philip E. Tetlock", "Barbara Mellers"], "title": "Prompt Engineering Large Language Models' Forecasting Capabilities", "categories": ["cs.CL"], "comment": null, "summary": "Large language model performance can be improved in a large number of ways.\nMany such techniques, like fine-tuning or advanced tool usage, are\ntime-intensive and expensive. Although prompt engineering is significantly\ncheaper and often works for simpler tasks, it remains unclear whether prompt\nengineering suffices for more complex domains like forecasting. Here we show\nthat small prompt modifications rarely boost forecasting accuracy beyond a\nminimal baseline. In our first study, we tested 38 prompts across Claude 3.5\nSonnet, Claude 3.5 Haiku, GPT-4o, and Llama 3.1 405B. In our second, we\nintroduced compound prompts and prompts from external sources, also including\nthe reasoning models o1 and o1-mini. Our results show that most prompts lead to\nnegligible gains, although references to base rates yield slight benefits.\nSurprisingly, some strategies showed strong negative effects on accuracy:\nespecially encouraging the model to engage in Bayesian reasoning. These results\nsuggest that, in the context of complex tasks like forecasting, basic prompt\nrefinements alone offer limited gains, implying that more robust or specialized\ntechniques may be required for substantial performance improvements in AI\nforecasting.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u590d\u6742\u7684\u4efb\u52a1\uff08\u5982\u9884\u6d4b\uff09\u4e2d\uff0c\u7b80\u5355\u7684\u63d0\u793a\u5de5\u7a0b\u5bf9\u51c6\u786e\u6027\u63d0\u5347\u6709\u9650\uff0c\u67d0\u4e9b\u7b56\u7565\u751a\u81f3\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u5f3a\u6216\u4e13\u4e1a\u7684\u6280\u5de7\u3002", "motivation": "\u63a2\u8ba8\u5728\u590d\u6742\u9886\u57df\uff08\u5982\u9884\u6d4b\uff09\u4e2d\uff0c\u63d0\u793a\u5de5\u7a0b\u662f\u5426\u80fd\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "method": "\u6d4b\u8bd5\u4e8638\u79cd\u63d0\u793a\uff0c\u6db5\u76d6\u591a\u4e2a\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u590d\u5408\u63d0\u793a\u548c\u5916\u90e8\u6765\u6e90\u63d0\u793a\uff0c\u5305\u62ec\u63a8\u7406\u6a21\u578bo1\u548co1-mini\u3002", "result": "\u5927\u591a\u6570\u63d0\u793a\u6548\u679c\u5fae\u5f31\uff0c\u57fa\u7387\u53c2\u8003\u7565\u6709\u5e2e\u52a9\uff0c\u4f46\u8d1d\u53f6\u65af\u63a8\u7406\u63d0\u793a\u663e\u8457\u964d\u4f4e\u51c6\u786e\u6027\u3002", "conclusion": "\u590d\u6742\u4efb\u52a1\u4e2d\uff0c\u4ec5\u9760\u63d0\u793a\u5de5\u7a0b\u6539\u8fdb\u6548\u679c\u6709\u9650\uff0c\u9700\u66f4\u7a33\u5065\u6216\u4e13\u95e8\u7684\u6280\u672f\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u9884\u6d4b, \u63d0\u793a\u5de5\u7a0b, \u51c6\u786e\u6027"}}
{"id": "2506.01523", "pdf": "https://arxiv.org/pdf/2506.01523", "abs": "https://arxiv.org/abs/2506.01523", "authors": ["Jihun Yun", "Juno Kim", "Jongho Park", "Junhyuck Kim", "Jongha Jon Ryu", "Jaewoong Cho", "Kwang-Sung Jun"], "title": "Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model", "categories": ["cs.LG", "stat.ML"], "comment": "26 pages, 7 tables", "summary": "Alignment via reinforcement learning from human feedback (RLHF) has become\nthe dominant paradigm for controlling the quality of outputs from large\nlanguage models (LLMs). However, when viewed as `loss + regularization,' the\nstandard RLHF objective lacks theoretical justification and incentivizes\ndegenerate, deterministic solutions, an issue that variants such as Direct\nPolicy Optimization (DPO) also inherit. In this paper, we rethink alignment by\nframing it as \\emph{distribution learning} from pairwise preference feedback by\nexplicitly modeling how information about the target language model bleeds\nthrough the preference data. This explicit modeling leads us to propose three\nprincipled learning objectives: preference maximum likelihood estimation,\npreference distillation, and reverse KL minimization. We theoretically show\nthat all three approaches enjoy strong non-asymptotic $O(1/n)$ convergence to\nthe target language model, naturally avoiding degeneracy and reward\noverfitting. Finally, we empirically demonstrate that our distribution learning\nframework, especially preference distillation, consistently outperforms or\nmatches the performances of RLHF and DPO across various tasks and models.", "AI": {"tldr": "\u8bba\u6587\u91cd\u65b0\u601d\u8003\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684RLHF\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\u4e14\u6613\u5bfc\u81f4\u9000\u5316\u6027\u89e3\uff0c\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u7406\u8bba\u5316\u7684\u5bf9\u9f50\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u57fa\u4e8e\u5206\u5e03\u5b66\u4e60\u7684\u76ee\u6807\u51fd\u6570\uff1a\u504f\u597d\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u3001\u504f\u597d\u84b8\u998f\u548c\u53cd\u5411KL\u6700\u5c0f\u5316\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e09\u79cd\u65b9\u6cd5\u5177\u6709\u5f3a\u6536\u655b\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u6216\u5339\u914dRLHF\u548cDPO\u3002", "conclusion": "\u5206\u5e03\u5b66\u4e60\u6846\u67b6\u4e3a\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\uff0c\u5c24\u5176\u662f\u504f\u597d\u84b8\u998f\u8868\u73b0\u4f18\u5f02\u3002", "keywords": "\u5bf9\u9f50, \u5f3a\u5316\u5b66\u4e60, \u5206\u5e03\u5b66\u4e60, \u504f\u597d\u53cd\u9988"}}
{"id": "2506.01587", "pdf": "https://arxiv.org/pdf/2506.01587", "abs": "https://arxiv.org/abs/2506.01587", "authors": ["Muhammad Islam", "Javed Ali Khan", "Mohammed Abaker", "Ali Daud", "Azeem Irshad"], "title": "Unified Large Language Models for Misinformation Detection in Low-Resource Linguistic Settings", "categories": ["cs.CL"], "comment": null, "summary": "The rapid expansion of social media platforms has significantly increased the\ndissemination of forged content and misinformation, making the detection of\nfake news a critical area of research. Although fact-checking efforts\npredominantly focus on English-language news, there is a noticeable gap in\nresources and strategies to detect news in regional languages, such as Urdu.\nAdvanced Fake News Detection (FND) techniques rely heavily on large, accurately\nlabeled datasets. However, FND in under-resourced languages like Urdu faces\nsubstantial challenges due to the scarcity of extensive corpora and the lack of\nvalidated lexical resources. Current Urdu fake news datasets are often\ndomain-specific and inaccessible to the public. They also lack human\nverification, relying mainly on unverified English-to-Urdu translations, which\ncompromises their reliability in practical applications. This study highlights\nthe necessity of developing reliable, expert-verified, and domain-independent\nUrdu-enhanced FND datasets to improve fake news detection in Urdu and other\nresource-constrained languages. This paper presents the first benchmark large\nFND dataset for Urdu news, which is publicly available for validation and deep\nanalysis. We also evaluate this dataset using multiple state-of-the-art\npre-trained large language models (LLMs), such as XLNet, mBERT, XLM-RoBERTa,\nRoBERTa, DistilBERT, and DeBERTa. Additionally, we propose a unified LLM model\nthat outperforms the others with different embedding and feature extraction\ntechniques. The performance of these models is compared based on accuracy, F1\nscore, precision, recall, and human judgment for vetting the sample results of\nnews.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e4c\u5c14\u90fd\u8bed\u5047\u65b0\u95fb\u68c0\u6d4b\u7684\u65b0\u578b\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684LLM\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u4e4c\u5c14\u90fd\u8bed\u7b49\u8d44\u6e90\u53d7\u9650\u8bed\u8a00\u7f3a\u4e4f\u53ef\u9760\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u516c\u5f00\u7684\u4e4c\u5c14\u90fd\u8bed\u5047\u65b0\u95fb\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u591a\u79cdLLM\uff08\u5982XLNet\u3001mBERT\u7b49\uff09\u8fdb\u884c\u9a8c\u8bc1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684LLM\u6a21\u578b\u3002", "result": "\u7edf\u4e00\u7684LLM\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001F1\u5206\u6570\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u5224\u65ad\u9a8c\u8bc1\u4e86\u6837\u672c\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u4e2d\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u7edf\u4e00\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u4e4c\u5c14\u90fd\u8bed\u5047\u65b0\u95fb\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bed\u8a00\u7684FND\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u5047\u65b0\u95fb\u68c0\u6d4b,\u4e4c\u5c14\u90fd\u8bed,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u6570\u636e\u96c6,\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6"}}
{"id": "2506.01529", "pdf": "https://arxiv.org/pdf/2506.01529", "abs": "https://arxiv.org/abs/2506.01529", "authors": ["Thomas Delliaux", "Nguyen-Khanh Vu", "Vincent Fran\u00e7ois-Lavet", "Elise van der Pol", "Emmanuel Rachelson"], "title": "Learning Abstract World Models with a Group-Structured Latent Space", "categories": ["cs.LG"], "comment": "20 pages, 18 figures", "summary": "Learning meaningful abstract models of Markov Decision Processes (MDPs) is\ncrucial for improving generalization from limited data. In this work, we show\nhow geometric priors can be imposed on the low-dimensional representation\nmanifold of a learned transition model. We incorporate known symmetric\nstructures via appropriate choices of the latent space and the associated group\nactions, which encode prior knowledge about invariances in the environment. In\naddition, our framework allows the embedding of additional unstructured\ninformation alongside these symmetries. We show experimentally that this leads\nto better predictions of the latent transition model than fully unstructured\napproaches, as well as better learning on downstream RL tasks, in environments\nwith rotational and translational features, including in first-person views of\n3D environments. Additionally, our experiments show that this leads to simpler\nand more disentangled representations. The full code is available on GitHub to\nensure reproducibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728MDPs\u4f4e\u7ef4\u8868\u793a\u6d41\u5f62\u4e0a\u65bd\u52a0\u51e0\u4f55\u5148\u9a8c\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u79f0\u7ed3\u6784\u63d0\u5347\u6a21\u578b\u9884\u6d4b\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u63d0\u9ad8\u4ece\u6709\u9650\u6570\u636e\u4e2d\u5b66\u4e60MDPs\u62bd\u8c61\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5728\u8868\u793a\u6d41\u5f62\u4e0a\u65bd\u52a0\u51e0\u4f55\u5148\u9a8c\uff0c\u5229\u7528\u5bf9\u79f0\u7ed3\u6784\u548c\u6f5c\u5728\u7a7a\u95f4\u8bbe\u8ba1\u7f16\u7801\u73af\u5883\u4e0d\u53d8\u6027\u3002", "result": "\u5728\u5177\u6709\u65cb\u8f6c\u548c\u5e73\u79fb\u7279\u5f81\u7684\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u975e\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u4e14\u8868\u793a\u66f4\u7b80\u6d01\u548c\u53ef\u89e3\u8026\u3002", "conclusion": "\u51e0\u4f55\u5148\u9a8c\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u9884\u6d4b\u548c\u8868\u793a\u5b66\u4e60\u3002", "keywords": "MDPs, \u51e0\u4f55\u5148\u9a8c, \u5bf9\u79f0\u7ed3\u6784, \u8868\u793a\u5b66\u4e60"}}
{"id": "2506.01592", "pdf": "https://arxiv.org/pdf/2506.01592", "abs": "https://arxiv.org/abs/2506.01592", "authors": ["Ahmed Elshabrawy", "Thanh-Nhi Nguyen", "Yeeun Kang", "Lihan Feng", "Annant Jain", "Faadil Abdullah Shaikh", "Jonibek Mansurov", "Mohamed Fazli Mohamed Imam", "Jesus-German Ortiz-Barajas", "Rendi Chevi", "Alham Fikri Aji"], "title": "Statement-Tuning Enables Efficient Cross-lingual Generalization in Encoder-only Models", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Large Language Models (LLMs) excel in zero-shot and few-shot tasks, but\nachieving similar performance with encoder-only models like BERT and RoBERTa\nhas been challenging due to their architecture. However, encoders offer\nadvantages such as lower computational and memory costs. Recent work adapts\nthem for zero-shot generalization using Statement Tuning, which reformulates\ntasks into finite templates. We extend this approach to multilingual NLP,\nexploring whether encoders can achieve zero-shot cross-lingual generalization\nand serve as efficient alternatives to memory-intensive LLMs for low-resource\nlanguages. Our results show that state-of-the-art encoder models generalize\nwell across languages, rivaling multilingual LLMs while being more efficient.\nWe also analyze multilingual Statement Tuning dataset design, efficiency gains,\nand language-specific generalization, contributing to more inclusive and\nresource-efficient NLP models. We release our code and models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7Statement Tuning\u65b9\u6cd5\u4f7f\u7f16\u7801\u5668\u6a21\u578b\uff08\u5982BERT\u548cRoBERTa\uff09\u5728\u96f6\u6837\u672c\u8de8\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4e0e\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7ade\u4e89\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u9a8c\u8bc1\u7f16\u7801\u5668\u6a21\u578b\u80fd\u5426\u5728\u96f6\u6837\u672c\u8de8\u8bed\u8a00\u4efb\u52a1\u4e2d\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7ade\u4e89\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u3002", "method": "\u91c7\u7528Statement Tuning\u65b9\u6cd5\uff0c\u5c06\u4efb\u52a1\u91cd\u6784\u4e3a\u6709\u9650\u6a21\u677f\uff0c\u5e76\u8fdb\u884c\u591a\u8bed\u8a00\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7f16\u7801\u5668\u6a21\u578b\u5728\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e0eLLMs\u76f8\u5f53\uff0c\u4f46\u66f4\u9ad8\u6548\u3002", "conclusion": "\u7f16\u7801\u5668\u6a21\u578b\u53ef\u4f5c\u4e3a\u8d44\u6e90\u6709\u9650\u8bed\u8a00\u4efb\u52a1\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u63a8\u52a8\u66f4\u5177\u5305\u5bb9\u6027\u7684NLP\u7814\u7a76\u3002", "keywords": "Large Language Models, BERT, RoBERTa, zero-shot, multilingual, efficiency"}}
{"id": "2506.01533", "pdf": "https://arxiv.org/pdf/2506.01533", "abs": "https://arxiv.org/abs/2506.01533", "authors": ["Yuchen Ma", "Jonas Schweisthal", "Hengrui Zhang", "Stefan Feuerriegel"], "title": "A Diffusion-Based Method for Learning the Multi-Outcome Distribution of Medical Treatments", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at KDD 2025", "summary": "In medicine, treatments often influence multiple, interdependent outcomes,\nsuch as primary endpoints, complications, adverse events, or other secondary\nendpoints. Hence, to make optimal treatment decisions, clinicians are\ninterested in learning the distribution of multi-dimensional treatment\noutcomes. However, the vast majority of machine learning methods for predicting\ntreatment effects focus on single-outcome settings, despite the fact that\nmedical data often include multiple, interdependent outcomes. To address this\nlimitation, we propose a novel diffusion-based method called DIME to learn the\njoint distribution of multiple outcomes of medical treatments. We addresses\nthree challenges relevant in medical practice: (i)it is tailored to learn the\njoint interventional distribution of multiple medical outcomes, which enables\nreliable decision-making with uncertainty quantification rather than relying\nsolely on point estimates; (ii)it explicitly captures the dependence structure\nbetween outcomes; (iii)it can handle outcomes of mixed type, including binary,\ncategorical, and continuous variables. In DIME, we take into account the\nfundamental problem of causal inference through causal masking. For training,\nour method decomposes the joint distribution into a series of conditional\ndistributions with a customized conditional masking to account for the\ndependence structure across outcomes. For inference, our method\nauto-regressively generates predictions. This allows our method to move beyond\npoint estimates of causal quantities and thus learn the joint interventional\ndistribution. To the best of our knowledge, DIME is the first neural method\ntailored to learn the joint, multi-outcome distribution of medical treatments.\nAcross various experiments, we demonstrate that our method effectively learns\nthe joint distribution and captures shared information among multiple outcomes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u65b0\u65b9\u6cd5DIME\uff0c\u7528\u4e8e\u5b66\u4e60\u533b\u7597\u6cbb\u7597\u7684\u591a\u4e2a\u7ed3\u679c\u7684\u8054\u5408\u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u5143\u7ed3\u679c\u9884\u6d4b\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4e34\u5e8a\u4e0a\u9700\u8981\u4e86\u89e3\u591a\u7ef4\u6cbb\u7597\u7ed3\u679c\u7684\u8054\u5408\u5206\u5e03\u4ee5\u505a\u51fa\u6700\u4f18\u51b3\u7b56\uff0c\u4f46\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u591a\u805a\u7126\u4e8e\u5355\u4e00\u7ed3\u679c\u9884\u6d4b\u3002", "method": "\u901a\u8fc7\u56e0\u679c\u63a9\u7801\u548c\u6761\u4ef6\u5206\u89e3\uff0cDIME\u5b66\u4e60\u591a\u5143\u7ed3\u679c\u7684\u8054\u5408\u5e72\u9884\u5206\u5e03\uff0c\u5e76\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDIME\u80fd\u6709\u6548\u5b66\u4e60\u8054\u5408\u5206\u5e03\u5e76\u6355\u6349\u7ed3\u679c\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "conclusion": "DIME\u662f\u9996\u4e2a\u9488\u5bf9\u591a\u5143\u533b\u7597\u7ed3\u679c\u8054\u5408\u5206\u5e03\u7684\u795e\u7ecf\u65b9\u6cd5\uff0c\u53ef\u652f\u6301\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u51b3\u7b56\u3002", "keywords": "\u533b\u7597\u6cbb\u7597\uff0c\u591a\u7ed3\u679c\u9884\u6d4b\uff0c\u6269\u6563\u6a21\u578b\uff0c\u8054\u5408\u5206\u5e03\uff0c\u56e0\u679c\u63a8\u65ad"}}
{"id": "2506.01602", "pdf": "https://arxiv.org/pdf/2506.01602", "abs": "https://arxiv.org/abs/2506.01602", "authors": ["Kensuke Mitsuzawa"], "title": "MMD-Sense-Analysis: Word Sense Detection Leveraging Maximum Mean Discrepancy", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Word sense analysis is an essential analysis work for interpreting the\nlinguistic and social backgrounds. The word sense change detection is a task of\nidentifying and interpreting shifts in word meanings over time. This paper\nproposes MMD-Sense-Analysis, a novel approach that leverages Maximum Mean\nDiscrepancy (MMD) to select semantically meaningful variables and quantify\nchanges across time periods. This method enables both the identification of\nwords undergoing sense shifts and the explanation of their evolution over\nmultiple historical periods. To my knowledge, this is the first application of\nMMD to word sense change detection. Empirical assessment results demonstrate\nthe effectiveness of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMMD\u7684\u65b0\u65b9\u6cd5MMD-Sense-Analysis\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u89e3\u91ca\u8bcd\u8bed\u610f\u4e49\u968f\u65f6\u95f4\u7684\u53d8\u5316\u3002", "motivation": "\u8bcd\u8bed\u610f\u4e49\u5206\u6790\u5bf9\u4e8e\u7406\u89e3\u8bed\u8a00\u548c\u793e\u4f1a\u80cc\u666f\u81f3\u5173\u91cd\u8981\uff0c\u800c\u8bcd\u8bed\u610f\u4e49\u7684\u6f14\u53d8\u68c0\u6d4b\u662f\u8bc6\u522b\u548c\u89e3\u91ca\u8bcd\u8bed\u610f\u4e49\u968f\u65f6\u95f4\u53d8\u5316\u7684\u4efb\u52a1\u3002", "method": "\u5229\u7528\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\u9009\u62e9\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u53d8\u91cf\uff0c\u5e76\u91cf\u5316\u4e0d\u540c\u65f6\u95f4\u6bb5\u7684\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u8bcd\u8bed\u610f\u4e49\u6f14\u53d8\u65b9\u9762\u6709\u6548\u3002", "conclusion": "MMD\u9996\u6b21\u88ab\u5e94\u7528\u4e8e\u8bcd\u8bed\u610f\u4e49\u53d8\u5316\u68c0\u6d4b\uff0c\u5e76\u5c55\u73b0\u51fa\u5176\u6f5c\u529b\u3002", "keywords": "\u8bcd\u8bed\u610f\u4e49\u5206\u6790, MMD, \u610f\u4e49\u6f14\u53d8\u68c0\u6d4b"}}
{"id": "2506.00494", "pdf": "https://arxiv.org/pdf/2506.00494", "abs": "https://arxiv.org/abs/2506.00494", "authors": ["Ali Ghanizadeh", "Ali Ahmadi", "Arash Bahrami"], "title": "Multi-Objective Neural Network Assisted Design Optimization of Soft Fin-Ray Grippers for Enhanced Grasping Performance", "categories": ["cs.RO", "cs.AI", "cs.CE"], "comment": null, "summary": "Soft Fin-Ray grippers can perform delicate and careful manipulation, which\nhas caused notable attention in different fields. These grippers can handle\nobjects of various forms and sizes safely. The internal structure of the\nFin-Ray finger plays a significant role in its adaptability and grasping\nperformance. However, modeling the non-linear grasp force and deformation\nbehaviors for design purposes is challenging. Moreover, when the Fin-Ray finger\nbecomes more rigid and capable of exerting higher forces, it becomes less\ndelicate in handling objects. The contrast between these two objectives gives\nrise to a multi-objective optimization problem. In this study, we employ finite\nelement method (FEM) to estimate the deflections and contact forces of the\nFin-Ray, grasping cylindrical objects. This dataset is then used to construct a\nmultilayer perception (MLP) for prediction of the contact force and the tip\ndisplacement. The FEM dataset consists of three input and four target features.\nThe three input features of the MLP and optimization design variables are the\nthickness of the front and supporting beams, the thickness of the cross beams,\nand the equal spacing between the cross beams. In addition, the target features\nare the maximum contact forces and maximum tip displacements in x- and\ny-directions. The magnitude of maximum contact force and magnitude of maximum\ntip displacement are the two objectives, showing the trade-off between force\nand delicate manipulation in soft Fin-Ray grippers. Furthermore, the optimized\nset of solutions are found using multi-objective optimal techniques. We use\nnon-dominated sorting genetic algorithm (NSGA-II) method for this purpose. Our\nfindings demonstrate that our methodologies can be used to improve the design\nand gripping performance of soft robotic grippers, helping us to choose a\ndesign not only for delicate grasping but also for high-force applications.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8f6fFin-Ray\u5939\u5177\u7684\u8bbe\u8ba1\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528\u6709\u9650\u5143\u6cd5\uff08FEM\uff09\u548c\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u63a5\u89e6\u529b\u4e0e\u5f62\u53d8\uff0c\u5e76\u901a\u8fc7NSGA-II\u7b97\u6cd5\u5b9e\u73b0\u591a\u76ee\u6807\u4f18\u5316\u3002", "motivation": "Fin-Ray\u5939\u5177\u5728\u67d4\u6027\u548c\u9ad8\u529b\u64cd\u63a7\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u4ee5\u63d0\u5347\u5176\u8bbe\u8ba1\u6027\u80fd\u3002", "method": "\u4f7f\u7528FEM\u6a21\u62df\u6293\u53d6\u5706\u67f1\u4f53\u65f6\u7684\u5f62\u53d8\u548c\u63a5\u89e6\u529b\uff0c\u6784\u5efa\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u9884\u6d4b\uff0c\u5e76\u91c7\u7528NSGA-II\u7b97\u6cd5\u4f18\u5316\u8bbe\u8ba1\u53c2\u6570\u3002", "result": "\u5b9e\u73b0\u4e86\u63a5\u89e6\u529b\u4e0e\u5f62\u53d8\u7684\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u627e\u5230\u4e86\u517c\u987e\u67d4\u6027\u548c\u9ad8\u529b\u7684\u8bbe\u8ba1\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u6539\u8fdb\u8f6f\u5939\u5177\u8bbe\u8ba1\uff0c\u6ee1\u8db3\u4ece\u7cbe\u7ec6\u6293\u53d6\u5230\u9ad8\u529b\u5e94\u7528\u7684\u9700\u6c42\u3002", "keywords": "\u8f6f\u673a\u5668\u4eba\u5939\u5177\u3001Fin-Ray\u7ed3\u6784\u3001\u6709\u9650\u5143\u6cd5\u3001\u795e\u7ecf\u7f51\u7edc\u3001\u591a\u76ee\u6807\u4f18\u5316"}}
{"id": "2506.01541", "pdf": "https://arxiv.org/pdf/2506.01541", "abs": "https://arxiv.org/abs/2506.01541", "authors": ["Timofei Gritsaev", "Nikita Morozov", "Kirill Tamogashev", "Daniil Tiapkin", "Sergey Samsonov", "Alexey Naumov", "Dmitry Vetrov", "Nikolay Malkin"], "title": "Adaptive Destruction Processes for Diffusion Samplers", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This paper explores the challenges and benefits of a trainable destruction\nprocess in diffusion samplers -- diffusion-based generative models trained to\nsample an unnormalised density without access to data samples. Contrary to the\nmajority of work that views diffusion samplers as approximations to an\nunderlying continuous-time model, we view diffusion models as discrete-time\npolicies trained to produce samples in very few generation steps. We propose to\ntrade some of the elegance of the underlying theory for flexibility in the\ndefinition of the generative and destruction policies. In particular, we\ndecouple the generation and destruction variances, enabling both transition\nkernels to be learned as unconstrained Gaussian densities. We show that, when\nthe number of steps is limited, training both generation and destruction\nprocesses results in faster convergence and improved sampling quality on\nvarious benchmarks. Through a robust ablation study, we investigate the design\nchoices necessary to facilitate stable training. Finally, we show the\nscalability of our approach through experiments on GAN latent space sampling\nfor conditional image generation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6269\u6563\u91c7\u6837\u5668\u4e2d\u53ef\u8bad\u7ec3\u7684\u7834\u574f\u8fc7\u7a0b\u7684\u6311\u6218\u4e0e\u4f18\u52bf\uff0c\u63d0\u51fa\u4e00\u79cd\u79bb\u6563\u65f6\u95f4\u7b56\u7565\uff0c\u63d0\u5347\u751f\u6210\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u6269\u6563\u91c7\u6837\u5668\u5728\u6709\u9650\u6b65\u9aa4\u4e0b\u7684\u9ad8\u6548\u751f\u6210\u80fd\u529b\uff0c\u7a81\u7834\u4f20\u7edf\u8fde\u7eed\u65f6\u95f4\u6a21\u578b\u7684\u9650\u5236\u3002", "method": "\u901a\u8fc7\u89e3\u8026\u751f\u6210\u548c\u7834\u574f\u8fc7\u7a0b\u7684\u65b9\u5dee\uff0c\u8bad\u7ec3\u672a\u7ea6\u675f\u7684\u9ad8\u65af\u5bc6\u5ea6\u6838\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u751f\u6210\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u6b65\u9aa4\u4e0b\u6536\u655b\u66f4\u5feb\uff0c\u91c7\u6837\u8d28\u91cf\u66f4\u9ad8\uff0c\u4e14\u9002\u7528\u4e8e\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u3002", "conclusion": "\u79bb\u6563\u65f6\u95f4\u7b56\u7565\u548c\u7075\u6d3b\u7684\u6838\u8bbe\u8ba1\u80fd\u663e\u8457\u63d0\u5347\u6269\u6563\u91c7\u6837\u5668\u7684\u6027\u80fd\u3002", "keywords": "\u6269\u6563\u6a21\u578b\u3001\u751f\u6210\u7b56\u7565\u3001\u9ad8\u65af\u5bc6\u5ea6\u3001\u6761\u4ef6\u56fe\u50cf\u751f\u6210"}}
{"id": "2506.01615", "pdf": "https://arxiv.org/pdf/2506.01615", "abs": "https://arxiv.org/abs/2506.01615", "authors": ["Pasunuti Prasanjith", "Prathmesh B More", "Anoop Kunchukuttan", "Raj Dabre"], "title": "IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language RAG Systems", "categories": ["cs.CL"], "comment": "WIP", "summary": "Retrieval-Augmented Generation (RAG) systems enable language models to access\nrelevant information and generate accurate, well-grounded, and contextually\ninformed responses. However, for Indian languages, the development of\nhigh-quality RAG systems is hindered by the lack of two critical resources: (1)\nevaluation benchmarks for retrieval and generation tasks, and (2) large-scale\ntraining datasets for multilingual retrieval. Most existing benchmarks and\ndatasets are centered around English or high-resource languages, making it\ndifficult to extend RAG capabilities to the diverse linguistic landscape of\nIndia. To address the lack of evaluation benchmarks, we create IndicMSMarco, a\nmultilingual benchmark for evaluating retrieval quality and response generation\nin 13 Indian languages, created via manual translation of 1000 diverse queries\nfrom MS MARCO-dev set. To address the need for training data, we build a\nlarge-scale dataset of (question, answer, relevant passage) tuples derived from\nthe Wikipedias of 19 Indian languages using state-of-the-art LLMs.\nAdditionally, we include translated versions of the original MS MARCO dataset\nto further enrich the training data and ensure alignment with real-world\ninformation-seeking tasks. Resources are available here:\nhttps://huggingface.co/datasets/ai4bharat/Indic-Rag-Suite", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9488\u5bf9\u5370\u5ea6\u8bed\u8a00\u5f00\u53d1\u9ad8\u8d28\u91cfRAG\u7cfb\u7edf\u7684\u4e24\u5927\u6311\u6218\uff1a\u7f3a\u4e4f\u8bc4\u4f30\u57fa\u51c6\u548c\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u4ecb\u7ecd\u901a\u8fc7\u521b\u5efaIndicMSMarco\u57fa\u51c6\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5370\u5ea6\u8bed\u8a00\u7f3a\u4e4f\u9ad8\u8d28\u91cfRAG\u7cfb\u7edf\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u5728\u4e8e\u7f3a\u4e4f\u8bc4\u4f30\u57fa\u51c6\u548c\u591a\u8bed\u8a00\u68c0\u7d22\u7684\u5927\u578b\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7\u624b\u52a8\u7ffb\u8bd1MS MARCO-dev\u521b\u5efaIndicMSMarco\u57fa\u51c6\uff0c\u5e76\u5229\u7528LLMs\u4ece19\u79cd\u5370\u5ea6\u8bed\u8a00\u7684\u7ef4\u57fa\u767e\u79d1\u4e2d\u6784\u5efa(question, answer, relevant passage)\u6570\u636e\u96c6\uff0c\u540c\u65f6\u7ffb\u8bd1\u539f\u59cbMS MARCO\u6570\u636e\u96c6\u4ee5\u4e30\u5bcc\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u521b\u5efa\u4e86IndicMSMarco\u57fa\u51c6\u548c\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u53ef\u7528\u4e8e\u8bc4\u4f30\u548c\u8bad\u7ec3\u5370\u5ea6\u8bed\u8a00\u7684RAG\u7cfb\u7edf\u3002", "conclusion": "\u63d0\u51fa\u7684\u8d44\u6e90\u548c\u6846\u67b6\u586b\u8865\u4e86\u5370\u5ea6\u8bed\u8a00RAG\u7cfb\u7edf\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "keywords": "Retrieval-Augmented Generation, Indian languages, evaluation benchmarks, training datasets, multilingual retrieval"}}
{"id": "2506.00512", "pdf": "https://arxiv.org/pdf/2506.00512", "abs": "https://arxiv.org/abs/2506.00512", "authors": ["Yang Zheng", "Mengqi Huang", "Nan Chen", "Zhendong Mao"], "title": "Pro3D-Editor : A Progressive-Views Perspective for Consistent and Precise 3D Editing", "categories": ["cs.GR", "cs.AI"], "comment": null, "summary": "Text-guided 3D editing aims to precisely edit semantically relevant local 3D\nregions, which has significant potential for various practical applications\nranging from 3D games to film production. Existing methods typically follow a\nview-indiscriminate paradigm: editing 2D views indiscriminately and projecting\nthem back into 3D space. However, they overlook the different cross-view\ninterdependencies, resulting in inconsistent multi-view editing. In this study,\nwe argue that ideal consistent 3D editing can be achieved through a\n\\textit{progressive-views paradigm}, which propagates editing semantics from\nthe editing-salient view to other editing-sparse views. Specifically, we\npropose \\textit{Pro3D-Editor}, a novel framework, which mainly includes\nPrimary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view\nSampler dynamically samples and edits the most editing-salient view as the\nprimary view. Key-view Render accurately propagates editing semantics from the\nprimary view to other key views through its Mixture-of-View-Experts Low-Rank\nAdaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based\non the edited multi-views. Extensive experiments demonstrate that our method\noutperforms existing methods in editing accuracy and spatial consistency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6587\u672c\u5f15\u5bfc\u76843D\u7f16\u8f91\u65b9\u6cd5Pro3D-Editor\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u89c6\u56fe\u8303\u5f0f\u89e3\u51b3\u591a\u89c6\u56fe\u7f16\u8f91\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7f16\u8f912D\u89c6\u56fe\u65f6\u5ffd\u89c6\u4e0d\u540c\u89c6\u56fe\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u591a\u89c6\u56fe\u7f16\u8f91\u4e0d\u4e00\u81f4\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u6e10\u8fdb\u5f0f\u89c6\u56fe\u8303\u5f0f\u5b9e\u73b0\u4e00\u81f4\u76843D\u7f16\u8f91\u3002", "method": "\u63d0\u51faPro3D-Editor\u6846\u67b6\uff0c\u5305\u62ecPrimary-view Sampler\u52a8\u6001\u91c7\u6837\u7f16\u8f91\u663e\u8457\u89c6\u56fe\u3001Key-view Render\u901a\u8fc7MoVE-LoRA\u5c06\u7f16\u8f91\u8bed\u4e49\u4f20\u64ad\u5230\u5173\u952e\u89c6\u56fe\u3001Full-view Refiner\u57fa\u4e8e\u591a\u89c6\u56fe\u7f16\u8f91\u4f18\u53163D\u5bf9\u8c61\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7f16\u8f91\u51c6\u786e\u6027\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Pro3D-Editor\u901a\u8fc7\u6e10\u8fdb\u5f0f\u89c6\u56fe\u8303\u5f0f\u6709\u6548\u63d0\u5347\u4e863D\u7f16\u8f91\u7684\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u3002", "keywords": "3D editing, text-guided, multi-view consistency, Pro3D-Editor"}}
{"id": "2506.01544", "pdf": "https://arxiv.org/pdf/2506.01544", "abs": "https://arxiv.org/abs/2506.01544", "authors": ["Batuhan Koyuncu", "Rachael DeVries", "Ole Winther", "Isabel Valera"], "title": "Temporal Variational Implicit Neural Representations", "categories": ["cs.LG"], "comment": null, "summary": "We introduce Temporal Variational Implicit Neural Representations (TV-INRs),\na probabilistic framework for modeling irregular multivariate time series that\nenables efficient individualized imputation and forecasting. By integrating\nimplicit neural representations with latent variable models, TV-INRs learn\ndistributions over time-continuous generator functions conditioned on\nsignal-specific covariates. Unlike existing approaches that require extensive\ntraining, fine-tuning or meta-learning, our method achieves accurate\nindividualized predictions through a single forward pass. Our experiments\ndemonstrate that with a single TV-INRs instance, we can accurately solve\ndiverse imputation and forecasting tasks, offering a computationally efficient\nand scalable solution for real-world applications. TV-INRs excel especially in\nlow-data regimes, where it outperforms existing methods by an order of\nmagnitude in mean squared error for imputation task.", "AI": {"tldr": "TV-INRs\u662f\u4e00\u79cd\u7ed3\u5408\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u548c\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\u7684\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u5efa\u6a21\u4e0d\u89c4\u5219\u591a\u5143\u65f6\u95f4\u5e8f\u5217\uff0c\u652f\u6301\u4e2a\u6027\u5316\u63d2\u8865\u548c\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5efa\u6a21\u65f6\u95f4\u5e8f\u5217\u65f6\u9700\u5927\u91cf\u8bad\u7ec3\u6216\u5fae\u8c03\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5355\u6b21\u524d\u5411\u9884\u6d4b\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u4e0e\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\u7684\u7ed3\u5408\uff0c\u5b66\u4e60\u57fa\u4e8e\u4fe1\u53f7\u7279\u5b9a\u534f\u53d8\u91cf\u7684\u65f6\u95f4\u8fde\u7eed\u751f\u6210\u5668\u51fd\u6570\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u663e\u793aTV-INRs\u5728\u63d2\u8865\u548c\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4f4e\u6570\u636e\u91cf\u573a\u666f\u3002", "conclusion": "TV-INRs\u4e3a\u771f\u5b9e\u4e16\u754c\u7684\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217, \u9690\u5f0f\u795e\u7ecf\u8868\u793a, \u6f5c\u5728\u53d8\u91cf\u6a21\u578b, \u63d2\u8865, \u9884\u6d4b"}}
{"id": "2506.01621", "pdf": "https://arxiv.org/pdf/2506.01621", "abs": "https://arxiv.org/abs/2506.01621", "authors": ["Zixiao Zhu", "Kezhi Mao"], "title": "Domain Lexical Knowledge-based Word Embedding Learning for Text Classification under Small Data", "categories": ["cs.CL"], "comment": "13 pages, 2 figures", "summary": "Pre-trained language models such as BERT have been proved to be powerful in\nmany natural language processing tasks. But in some text classification\napplications such as emotion recognition and sentiment analysis, BERT may not\nlead to satisfactory performance. This often happens in applications where\nkeywords play critical roles in the prediction of class labels. Our\ninvestigation found that the root cause of the problem is that the\ncontext-based BERT embedding of the keywords may not be discriminative enough\nto produce discriminative text representation for classification. Motivated by\nthis finding, we develop a method to enhance word embeddings using\ndomain-specific lexical knowledge. The knowledge-based embedding enhancement\nmodel projects the BERT embedding into a new space where within-class\nsimilarity and between-class difference are maximized. To implement the\nknowledge-based word embedding enhancement model, we also develop a knowledge\nacquisition algorithm for automatically collecting lexical knowledge from\nonline open sources. Experiment results on three classification tasks,\nincluding sentiment analysis, emotion recognition and question answering, have\nshown the effectiveness of our proposed word embedding enhancing model. The\ncodes and datasets are in https://github.com/MidiyaZhu/KVWEFFER.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9886\u57df\u8bcd\u6c47\u77e5\u8bc6\u589e\u5f3aBERT\u8bcd\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5728\u5173\u952e\u8bcd\u4e3b\u5bfc\u7684\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff08\u5982\u60c5\u611f\u5206\u6790\u548c\u60c5\u7eea\u8bc6\u522b\uff09\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u5173\u952e\u8bcd\u5bf9\u5206\u7c7b\u6807\u7b7e\u9884\u6d4b\u8d77\u5173\u952e\u4f5c\u7528\u7684\u4efb\u52a1\u4e2d\uff0cBERT\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u53ef\u80fd\u4e0d\u8db3\u4ee5\u751f\u6210\u533a\u5206\u6027\u5f3a\u7684\u6587\u672c\u8868\u793a\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5229\u7528\u9886\u57df\u8bcd\u6c47\u77e5\u8bc6\u589e\u5f3a\u8bcd\u5d4c\u5165\u7684\u6a21\u578b\uff0c\u5c06BERT\u5d4c\u5165\u6295\u5f71\u5230\u4e00\u4e2a\u6700\u5927\u5316\u7c7b\u5185\u76f8\u4f3c\u6027\u548c\u7c7b\u95f4\u5dee\u5f02\u7684\u65b0\u7a7a\u95f4\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u81ea\u52a8\u4ece\u5728\u7ebf\u5f00\u653e\u8d44\u6e90\u83b7\u53d6\u8bcd\u6c47\u77e5\u8bc6\u7684\u7b97\u6cd5\u3002", "result": "\u5728\u60c5\u611f\u5206\u6790\u3001\u60c5\u7eea\u8bc6\u522b\u548c\u95ee\u7b54\u4e09\u4e2a\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u589e\u5f3a\u8bcd\u5d4c\u5165\u7684\u533a\u5206\u6027\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5173\u952e\u8bcd\u4e3b\u5bfc\u4efb\u52a1\u7684\u5206\u7c7b\u6027\u80fd\u3002", "keywords": "BERT, \u8bcd\u5d4c\u5165\u589e\u5f3a, \u9886\u57df\u8bcd\u6c47\u77e5\u8bc6, \u6587\u672c\u5206\u7c7b, \u60c5\u611f\u5206\u6790"}}
{"id": "2506.01545", "pdf": "https://arxiv.org/pdf/2506.01545", "abs": "https://arxiv.org/abs/2506.01545", "authors": ["Mate Botond Nemeth", "Emma Hart", "Kevin Sim", "Quentin Renau"], "title": "Class Incremental Learning for Algorithm Selection", "categories": ["cs.LG"], "comment": "This paper was accepted at GECCO 2025. 4 pages, 2 figures", "summary": "Algorithm selection is commonly used to predict the best solver from a\nportfolio per per-instance. In many real scenarios, instances arrive in a\nstream: new instances become available over time, while the number of class\nlabels can also grow as new data distributions arrive downstream. As a result,\nthe classification model needs to be periodically updated to reflect additional\nsolvers without catastrophic forgetting of past data. In machine-learning (ML),\nthis is referred to as Class Incremental Learning (CIL). While commonly\naddressed in ML settings, its relevance to algorithm-selection in optimisation\nhas not been previously studied. Using a bin-packing dataset, we benchmark 8\ncontinual learning methods with respect to their ability to withstand\ncatastrophic forgetting. We find that rehearsal-based methods significantly\noutperform other CIL methods. While there is evidence of forgetting, the loss\nis small at around 7%. Hence, these methods appear to be a viable approach to\ncontinual learning in streaming optimisation scenarios.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u7b97\u6cd5\u9009\u62e9\u4e2d\u7684\u589e\u91cf\u5b66\u4e60\u95ee\u9898\uff0c\u8bc4\u4f30\u4e868\u79cd\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u53d1\u73b0\u57fa\u4e8e\u8bb0\u5fc6\u7684\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5728\u4f18\u5316\u9886\u57df\u4e2d\uff0c\u5b9e\u4f8b\u548c\u6c42\u89e3\u5668\u968f\u65f6\u95f4\u589e\u52a0\u65f6\uff0c\u5982\u4f55\u907f\u514d\u5206\u7c7b\u6a21\u578b\u9057\u5fd8\u5386\u53f2\u6570\u636e\u662f\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u88c5\u7bb1\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e868\u79cd\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5bf9\u707e\u96be\u6027\u9057\u5fd8\u7684\u62b5\u6297\u80fd\u529b\u3002", "result": "\u57fa\u4e8e\u8bb0\u5fc6\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u9057\u5fd8\u635f\u5931\u7ea6\u4e3a7%\u3002", "conclusion": "\u57fa\u4e8e\u8bb0\u5fc6\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u9002\u7528\u4e8e\u6d41\u5f0f\u4f18\u5316\u573a\u666f\u3002", "keywords": "\u7b97\u6cd5\u9009\u62e9,\u589e\u91cf\u5b66\u4e60,\u707e\u96be\u6027\u9057\u5fd8,\u6301\u7eed\u5b66\u4e60,\u4f18\u5316"}}
{"id": "2506.01627", "pdf": "https://arxiv.org/pdf/2506.01627", "abs": "https://arxiv.org/abs/2506.01627", "authors": ["Shiwen Ni", "Jiawen Li", "Hung-Yu Kao"], "title": "MVAN: Multi-View Attention Networks for Fake News Detection on Social Media", "categories": ["cs.CL"], "comment": null, "summary": "Fake news on social media is a widespread and serious problem in today's\nsociety. Existing fake news detection methods focus on finding clues from Long\ntext content, such as original news articles and user comments. This paper\nsolves the problem of fake news detection in more realistic scenarios. Only\nsource shot-text tweet and its retweet users are provided without user\ncomments. We develop a novel neural network based model,\n\\textbf{M}ulti-\\textbf{V}iew \\textbf{A}ttention \\textbf{N}etworks (MVAN) to\ndetect fake news and provide explanations on social media. The MVAN model\nincludes text semantic attention and propagation structure attention, which\nensures that our model can capture information and clues both of source tweet\ncontent and propagation structure. In addition, the two attention mechanisms in\nthe model can find key clue words in fake news texts and suspicious users in\nthe propagation structure. We conduct experiments on two real-world datasets,\nand the results demonstrate that MVAN can significantly outperform\nstate-of-the-art methods by 2.5\\% in accuracy on average, and produce a\nreasonable explanation.", "AI": {"tldr": "\u8be5\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u6a21\u578bMVAN\uff0c\u7528\u4e8e\u68c0\u6d4b\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u5047\u65b0\u95fb\u5e76\u63d0\u4f9b\u89e3\u91ca\u3002MVAN\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u6ce8\u610f\u529b\u548c\u4f20\u64ad\u7ed3\u6784\u6ce8\u610f\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5047\u65b0\u95fb\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u957f\u6587\u672c\u5185\u5bb9\uff0c\u5982\u539f\u6587\u548c\u7528\u6237\u8bc4\u8bba\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u4ec5\u63d0\u4f9b\u7b80\u77ed\u63a8\u6587\u53ca\u5176\u8f6c\u53d1\u7528\u6237\u7684\u66f4\u73b0\u5b9e\u573a\u666f\u4e0b\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u591a\u89c6\u89d2\u6ce8\u610f\u529b\u7f51\u7edc\uff08MVAN\uff09\uff0c\u5305\u62ec\u6587\u672c\u8bed\u4e49\u6ce8\u610f\u529b\u548c\u4f20\u64ad\u7ed3\u6784\u6ce8\u610f\u529b\uff0c\u4ece\u63a8\u6587\u5185\u5bb9\u548c\u4f20\u64ad\u7ed3\u6784\u4e2d\u6355\u6349\u5173\u952e\u4fe1\u606f\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMVAN\u5728\u51c6\u786e\u7387\u4e0a\u5e73\u5747\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad82.5%\uff0c\u5e76\u80fd\u63d0\u4f9b\u5408\u7406\u7684\u89e3\u91ca\u3002", "conclusion": "MVAN\u5728\u5047\u65b0\u95fb\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u77ed\u6587\u672c\u548c\u4f20\u64ad\u7ed3\u6784\u7684\u7ed3\u5408\u4e0a\u5177\u6709\u4f18\u52bf\u3002", "keywords": "\u5047\u65b0\u95fb\u68c0\u6d4b, \u793e\u4ea4\u5a92\u4f53, \u6ce8\u610f\u529b\u673a\u5236, \u795e\u7ecf\u7f51\u7edc, \u591a\u89c6\u89d2\u5b66\u4e60"}}
{"id": "2506.01552", "pdf": "https://arxiv.org/pdf/2506.01552", "abs": "https://arxiv.org/abs/2506.01552", "authors": ["Roman Plaud", "Alexandre Perez-Lebel", "Matthieu Labeau", "Antoine Saillenfest", "Thomas Bonald"], "title": "To Each Metric Its Decoding: Post-Hoc Optimal Decision Rules of Probabilistic Hierarchical Classifiers", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted at ICML 2025", "summary": "Hierarchical classification offers an approach to incorporate the concept of\nmistake severity by leveraging a structured, labeled hierarchy. However,\ndecoding in such settings frequently relies on heuristic decision rules, which\nmay not align with task-specific evaluation metrics. In this work, we propose a\nframework for the optimal decoding of an output probability distribution with\nrespect to a target metric. We derive optimal decision rules for increasingly\ncomplex prediction settings, providing universal algorithms when candidates are\nlimited to the set of nodes. In the most general case of predicting a subset of\nnodes, we focus on rules dedicated to the hierarchical $hF_{\\beta}$ scores,\ntailored to hierarchical settings. To demonstrate the practical utility of our\napproach, we conduct extensive empirical evaluations, showcasing the\nsuperiority of our proposed optimal strategies, particularly in underdetermined\nscenarios. These results highlight the potential of our methods to enhance the\nperformance and reliability of hierarchical classifiers in real-world\napplications. The code is available at\nhttps://github.com/RomanPlaud/hierarchical_decision_rules", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u89e3\u7801\u6846\u67b6\uff0c\u9488\u5bf9\u5c42\u6b21\u5206\u7c7b\u4e2d\u7684\u76ee\u6807\u6307\u6807\uff0c\u63a8\u5bfc\u4e86\u4e0d\u540c\u590d\u6742\u5ea6\u9884\u6d4b\u573a\u666f\u7684\u6700\u4f18\u51b3\u7b56\u89c4\u5219\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4f18\u52bf\u3002", "motivation": "\u5c42\u6b21\u5206\u7c7b\u4e2d\u7684\u4f20\u7edf\u542f\u53d1\u5f0f\u51b3\u7b56\u89c4\u5219\u53ef\u80fd\u65e0\u6cd5\u4e0e\u4efb\u52a1\u7279\u5b9a\u8bc4\u4f30\u6307\u6807\u5bf9\u9f50\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4f18\u5316\u89e3\u7801\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u4e8e\u76ee\u6807\u6307\u6807\u4f18\u5316\u8f93\u51fa\u6982\u7387\u5206\u5e03\u7684\u89e3\u7801\uff0c\u5e76\u9488\u5bf9\u4e0d\u540c\u9884\u6d4b\u573a\u666f\u63a8\u5bfc\u4e86\u6700\u4f18\u51b3\u7b56\u89c4\u5219\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6700\u4f18\u7b56\u7565\u5728\u4e0d\u786e\u5b9a\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u63d0\u5347\u4e86\u5c42\u6b21\u5206\u7c7b\u5668\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5c42\u6b21\u5206\u7c7b\u5668\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6027\u80fd\u589e\u5f3a\u548c\u53ef\u9760\u6027\u6539\u8fdb\u7684\u6f5c\u529b\u3002", "keywords": "\u5c42\u6b21\u5206\u7c7b, \u6700\u4f18\u89e3\u7801, \u51b3\u7b56\u89c4\u5219, hF\u03b2\u5206\u6570"}}
{"id": "2506.01629", "pdf": "https://arxiv.org/pdf/2506.01629", "abs": "https://arxiv.org/abs/2506.01629", "authors": ["Frederick Riemenschneider", "Anette Frank"], "title": "Cross-Lingual Generalization and Compression: From Language-Specific to Shared Neurons", "categories": ["cs.CL", "I.2.4; I.2.7"], "comment": "Paper accepted for publication at ACL 2025 Main; 10 pages, 20\n  figures, 4 tables", "summary": "Multilingual language models (MLLMs) have demonstrated remarkable abilities\nto transfer knowledge across languages, despite being trained without explicit\ncross-lingual supervision. We analyze the parameter spaces of three MLLMs to\nstudy how their representations evolve during pre-training, observing patterns\nconsistent with compression: models initially form language-specific\nrepresentations, which gradually converge into cross-lingual abstractions as\ntraining progresses. Through probing experiments, we observe a clear transition\nfrom uniform language identification capabilities across layers to more\nspecialized layer functions. For deeper analysis, we focus on neurons that\nencode distinct semantic concepts. By tracing their development during\npre-training, we show how they gradually align across languages. Notably, we\nidentify specific neurons that emerge as increasingly reliable predictors for\nthe same concepts across languages.", "AI": {"tldr": "\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u80fd\u591f\u5728\u65e0\u663e\u5f0f\u8de8\u8bed\u8a00\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u77e5\u8bc6\u8fc1\u79fb\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5176\u8868\u5f81\u5728\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9010\u6e10\u4ece\u8bed\u8a00\u7279\u5b9a\u538b\u7f29\u4e3a\u8de8\u8bed\u8a00\u62bd\u8c61\uff0c\u795e\u7ecf\u5143\u4e5f\u9010\u6e10\u5bf9\u9f50\u3002", "motivation": "\u7814\u7a76\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u663e\u5f0f\u76d1\u7763\u4e0b\u5982\u4f55\u5b9e\u73b0\u8de8\u8bed\u8a00\u77e5\u8bc6\u8fc1\u79fb\u53ca\u5176\u8868\u5f81\u6f14\u5316\u3002", "method": "\u5206\u6790\u4e09\u4e2aMLLMs\u7684\u53c2\u6570\u7a7a\u95f4\uff0c\u901a\u8fc7\u63a2\u7d22\u5b9e\u9a8c\u89c2\u5bdf\u8868\u5f81\u6f14\u5316\u53ca\u795e\u7ecf\u5143\u5bf9\u9f50\u3002", "result": "\u6a21\u578b\u8868\u5f81\u4ece\u8bed\u8a00\u7279\u5b9a\u9010\u6e10\u878d\u5408\u4e3a\u8de8\u8bed\u8a00\u62bd\u8c61\uff0c\u7279\u5b9a\u795e\u7ecf\u5143\u6210\u4e3a\u8de8\u8bed\u8a00\u6982\u5ff5\u7684\u53ef\u9760\u9884\u6d4b\u5668\u3002", "conclusion": "MLLMs\u901a\u8fc7\u8868\u5f81\u538b\u7f29\u548c\u795e\u7ecf\u5143\u5bf9\u9f50\u5b9e\u73b0\u8de8\u8bed\u8a00\u77e5\u8bc6\u8fc1\u79fb\u3002", "keywords": "\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b,\u8868\u5f81\u6f14\u5316,\u795e\u7ecf\u5143\u5bf9\u9f50,\u8de8\u8bed\u8a00\u8fc1\u79fb"}}
{"id": "2506.01562", "pdf": "https://arxiv.org/pdf/2506.01562", "abs": "https://arxiv.org/abs/2506.01562", "authors": ["Wojciech Masarczyk", "Mateusz Ostaszewski", "Tin Sum Cheng", "Tomasz Trzci\u0144ski", "Aurelien Lucchi", "Razvan Pascanu"], "title": "Unpacking Softmax: How Temperature Drives Representation Collapse, Compression, and Generalization", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "The softmax function is a fundamental building block of deep neural networks,\ncommonly used to define output distributions in classification tasks or\nattention weights in transformer architectures. Despite its widespread use and\nproven effectiveness, its influence on learning dynamics and learned\nrepresentations remains poorly understood, limiting our ability to optimize\nmodel behavior. In this paper, we study the pivotal role of the softmax\nfunction in shaping the model's representation. We introduce the concept of\nrank deficit bias - a phenomenon in which softmax-based deep networks find\nsolutions of rank much lower than the number of classes. This bias depends on\nthe softmax function's logits norm, which is implicitly influenced by\nhyperparameters or directly modified by softmax temperature. Furthermore, we\ndemonstrate how to exploit the softmax dynamics to learn compressed\nrepresentations or to enhance their performance on out-of-distribution data. We\nvalidate our findings across diverse architectures and real-world datasets,\nhighlighting the broad applicability of temperature tuning in improving model\nperformance. Our work provides new insights into the mechanisms of softmax,\nenabling better control over representation learning in deep neural networks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86softmax\u51fd\u6570\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u5176\u4f1a\u5bfc\u81f4\u8868\u793a\u5b66\u4e60\u7684rank deficit bias\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u8c03\u6574\u6e29\u5ea6\u53c2\u6570\u4f18\u5316\u6a21\u578b\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "motivation": "softmax\u51fd\u6570\u867d\u7136\u5e7f\u6cdb\u7528\u4e8e\u5206\u7c7b\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f46\u5176\u5bf9\u5b66\u4e60\u52a8\u6001\u548c\u8868\u793a\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff0c\u9650\u5236\u4e86\u6a21\u578b\u4f18\u5316\u7684\u80fd\u529b\u3002", "method": "\u7814\u7a76softmax\u5bf9\u8868\u793a\u7684\u5f71\u54cd\uff0c\u63d0\u51farank deficit bias\u7684\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u6e29\u5ea6\u53c2\u6570\u6539\u8fdb\u8868\u793a\u5b66\u4e60\u3002", "result": "\u53d1\u73b0softmax\u4f1a\u5f15\u8d77\u4f4e\u79e9\u89e3\uff0c\u9a8c\u8bc1\u4e86\u6e29\u5ea6\u8c03\u4f18\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u4f5c\u7528\u3002", "conclusion": "\u63ed\u793a\u4e86softmax\u7684\u673a\u5236\uff0c\u4e3a\u6df1\u5ea6\u7f51\u7edc\u7684\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u65b9\u5411\u3002", "keywords": "softmax, rank deficit bias, representation learning, temperature tuning"}}
{"id": "2506.01646", "pdf": "https://arxiv.org/pdf/2506.01646", "abs": "https://arxiv.org/abs/2506.01646", "authors": ["Chaoyue He", "Xin Zhou", "Yi Wu", "Xinjia Yu", "Yan Zhang", "Lei Zhang", "Di Wang", "Shengfei Lyu", "Hong Xu", "Xiaoqiao Wang", "Wei Liu", "Chunyan Miao"], "title": "ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; H.3.3"], "comment": "37 pages, 8 figures, 11 tables", "summary": "We introduce ESGenius, a comprehensive benchmark for evaluating and enhancing\nthe proficiency of Large Language Models (LLMs) in Environmental, Social and\nGovernance (ESG) and sustainability-focused question answering. ESGenius\ncomprises two key components: (i) ESGenius-QA, a collection of 1 136\nmultiple-choice questions generated by LLMs and rigorously validated by domain\nexperts, covering a broad range of ESG pillars and sustainability topics. Each\nquestion is systematically linked to its corresponding source text, enabling\ntransparent evaluation and supporting retrieval-augmented generation (RAG)\nmethods; and (ii) ESGenius-Corpus, a meticulously curated repository of 231\nfoundational frameworks, standards, reports and recommendation documents from\nseven authoritative sources. Moreover, to fully assess the capabilities and\nadaptation potential of the model, we implement a rigorous two-stage evaluation\nprotocol -- Zero-Shot and RAG. Extensive experiments across 50 LLMs (ranging\nfrom 0.5 B to 671 B parameters) demonstrate that state-of-the-art models\nachieve only moderate performance in zero-shot settings, with accuracies\ntypically around 55--70\\%, highlighting ESGenius's challenging nature for LLMs\nin interdisciplinary contexts. However, models employing RAG show significant\nperformance improvements, particularly for smaller models. For example,\n\"DeepSeek-R1-Distill-Qwen-14B\" improves from 63.82\\% (zero-shot) to 80.46\\%\nwith RAG. These results underscore the necessity of grounding responses in\nauthoritative sources for enhanced ESG understanding. To the best of our\nknowledge, ESGenius is the first benchmark curated for LLMs and the relevant\nenhancement technologies that focuses on ESG and sustainability topics.", "AI": {"tldr": "ESGenius\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8eESG\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u4e3b\u9898\u7684LLM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542bQA\u6570\u636e\u96c6\u548c\u6743\u5a01\u6587\u6863\u5e93\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u548cRAG\u8bc4\u4f30\uff0c\u53d1\u73b0RAG\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8bc4\u4f30\u548c\u63d0\u9ad8LLM\u5728ESG\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u95ee\u9898\u89e3\u7b54\u4e2d\u7684\u80fd\u529b\uff0c\u586b\u8865\u8be5\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\u3002", "method": "\u5f00\u53d1ESGenius-QA\uff081136\u4e2a\u591a\u9009\u95ee\u9898\uff09\u548cESGenius-Corpus\uff08231\u4efd\u6743\u5a01\u6587\u6863\uff09\uff0c\u91c7\u7528\u96f6\u6837\u672c\u548cRAG\u4e24\u9636\u6bb5\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u96f6\u6837\u672c\u51c6\u786e\u738755-70%\uff0cRAG\u663e\u8457\u63d0\u5347\u6027\u80fd\uff08\u5982DeepSeek\u6a21\u578b\u4ece63.82%\u63d0\u5347\u81f380.46%\uff09\u3002", "conclusion": "LLM\u9700\u4f9d\u8d56\u6743\u5a01\u6587\u732e\u589e\u5f3aESG\u7406\u89e3\uff0cESGenius\u4e3a\u76f8\u5173\u6280\u672f\u63d0\u4f9b\u4e86\u9996\u4e2a\u8bc4\u4f30\u57fa\u51c6\u3002", "keywords": "LLM, ESG, \u53ef\u6301\u7eed\u53d1\u5c55, \u57fa\u51c6\u6d4b\u8bd5, RAG"}}
{"id": "2506.00534", "pdf": "https://arxiv.org/pdf/2506.00534", "abs": "https://arxiv.org/abs/2506.00534", "authors": ["Yudong Zhang", "Ruobing Xie", "Xingwu Sun", "Jiansheng Chen", "Zhanhui Kang", "Di Wang", "Yu Wang"], "title": "The Security Threat of Compressed Projectors in Large Vision-Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The choice of a suitable visual language projector (VLP) is critical to the\nsuccessful training of large visual language models (LVLMs). Mainstream VLPs\ncan be broadly categorized into compressed and uncompressed projectors, and\neach offering distinct advantages in performance and computational efficiency.\nHowever, their security implications have not been thoroughly examined. Our\ncomprehensive evaluation reveals significant differences in their security\nprofiles: compressed projectors exhibit substantial vulnerabilities, allowing\nadversaries to successfully compromise LVLMs even with minimal knowledge of\nstructural information. In stark contrast, uncompressed projectors demonstrate\nrobust security properties and do not introduce additional vulnerabilities.\nThese findings provide critical guidance for researchers in selecting optimal\nVLPs that enhance the security and reliability of visual language models. The\ncode will be released.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u89c6\u89c9\u8bed\u8a00\u6295\u5c04\u5668\uff08VLP\uff09\u5bf9\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u8bad\u7ec3\u5b89\u5168\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u538b\u7f29\u578bVLP\u5b58\u5728\u663e\u8457\u8106\u5f31\u6027\uff0c\u800c\u975e\u538b\u7f29\u578bVLP\u5219\u8868\u73b0\u51fa\u8f83\u5f3a\u5b89\u5168\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u8bc4\u4f30\u4e0d\u540cVLP\u7c7b\u578b\u5bf9LVLM\u5b89\u5168\u6027\u7684\u5f71\u54cd\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u5bf9VLP\u5b89\u5168\u6027\u5206\u6790\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5168\u9762\u8bc4\u4f30\u538b\u7f29\u578b\u548c\u975e\u538b\u7f29\u578bVLP\u7684\u5b89\u5168\u6027\u8868\u73b0\uff0c\u5206\u6790\u4e24\u8005\u5728\u5bf9\u6297\u653b\u51fb\u4e2d\u7684\u8106\u5f31\u6027\u5dee\u5f02\u3002", "result": "\u7ed3\u679c\u8868\u660e\u538b\u7f29\u578bVLP\u6613\u53d7\u653b\u51fb\uff0c\u800c\u975e\u538b\u7f29\u578bVLP\u5b89\u5168\u6027\u8f83\u9ad8\uff0c\u672a\u5f15\u5165\u989d\u5916\u6f0f\u6d1e\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9009\u62e9\u5b89\u5168\u7684VLP\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u63d0\u5347LVLM\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6295\u5c04\u5668, \u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u5b89\u5168\u6027, \u5bf9\u6297\u653b\u51fb"}}
{"id": "2506.01568", "pdf": "https://arxiv.org/pdf/2506.01568", "abs": "https://arxiv.org/abs/2506.01568", "authors": ["Cornelius V. Braun", "Sayantan Auddy", "Marc Toussaint"], "title": "Trajectory First: A Curriculum for Discovering Diverse Policies", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Being able to solve a task in diverse ways makes agents more robust to task\nvariations and less prone to local optima. In this context, constrained\ndiversity optimization has emerged as a powerful reinforcement learning (RL)\nframework to train a diverse set of agents in parallel. However, existing\nconstrained-diversity RL methods often under-explore in complex tasks such as\nrobotic manipulation, leading to a lack in policy diversity. To improve\ndiversity optimization in RL, we therefore propose a curriculum that first\nexplores at the trajectory level before learning step-based policies. In our\nempirical evaluation, we provide novel insights into the shortcoming of\nskill-based diversity optimization, and demonstrate empirically that our\ncurriculum improves the diversity of the learned skills.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u591a\u6837\u5316\u89e3\u6765\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u63a2\u7d22\u8f68\u8ff9\u518d\u5b66\u4e60\u6b65\u8fdb\u7b56\u7565\u7684\u8bfe\u7a0b\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u73b0\u6709\u7ea6\u675f\u591a\u6837\u6027RL\u65b9\u6cd5\u63a2\u7d22\u4e0d\u8db3\u5bfc\u81f4\u7b56\u7565\u591a\u6837\u6027\u7f3a\u4e4f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u63a2\u7d22\u8f68\u8ff9\u518d\u5b66\u4e60\u6b65\u8fdb\u7b56\u7565\u7684\u8bfe\u7a0b\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u5b66\u4e60\u6280\u80fd\u7684\u591a\u6837\u6027\u3002", "conclusion": "\u901a\u8fc7\u8f68\u8ff9\u7ea7\u63a2\u7d22\u7684\u8bfe\u7a0b\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347RL\u4e2d\u7684\u591a\u6837\u6027\u4f18\u5316\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60\u3001\u591a\u6837\u6027\u4f18\u5316\u3001\u8bfe\u7a0b\u5b66\u4e60\u3001\u673a\u5668\u4eba\u64cd\u4f5c"}}
{"id": "2506.01675", "pdf": "https://arxiv.org/pdf/2506.01675", "abs": "https://arxiv.org/abs/2506.01675", "authors": ["Chen Zhang", "Zhiyuan Liao", "Yansong Feng"], "title": "Cross-Lingual Transfer of Cultural Knowledge: An Asymmetric Phenomenon", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Despite substantial research efforts evaluating how well large language\nmodels~(LLMs) handle global cultural diversity, the mechanisms behind their\ncultural knowledge acquisition, particularly in multilingual settings, remain\nunclear. We study this question by investigating how cultural knowledge\ntransfers across languages during language adaptation of LLMs. We introduce an\ninterpretable framework for studying this transfer, ensuring training data\ntransparency and controlling transfer effects. Through a study of four\nnon-Anglophonic cultures, we observe bidirectional cultural transfer between\nEnglish and other high-resource languages, while low-resource languages\nprimarily transfer knowledge to English with limited reverse flow. To explain\nthis asymmetric phenomenon, we propose a frequency-based hypothesis: cultural\nknowledge appearing more frequently in the pretraining data transfers more\neasily, which is supported by empirical analysis of the training corpora.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u6587\u5316\u77e5\u8bc6\u8fc1\u79fb\u7684\u673a\u5236\uff0c\u91cd\u70b9\u5173\u6ce8\u9ad8\u8d44\u6e90\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e4b\u95f4\u7684\u4e0d\u5bf9\u79f0\u8fc1\u79fb\u73b0\u8c61\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u5927\u91cf\u7814\u7a76\u8bc4\u4f30LLMs\u5904\u7406\u5168\u7403\u6587\u5316\u591a\u6837\u6027\u7684\u80fd\u529b\uff0c\u4f46\u5176\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u6587\u5316\u77e5\u8bc6\u83b7\u53d6\u7684\u673a\u5236\u4ecd\u4e0d\u660e\u786e\u3002", "method": "\u5f15\u5165\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u7814\u7a76\u6587\u5316\u77e5\u8bc6\u5728\u8bed\u8a00\u9002\u5e94\u8fc7\u7a0b\u4e2d\u7684\u8fc1\u79fb\uff0c\u63a7\u5236\u8bad\u7ec3\u6570\u636e\u7684\u900f\u660e\u5ea6\u5e76\u5206\u6790\u8fc1\u79fb\u6548\u5e94\u3002", "result": "\u53d1\u73b0\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e0e\u82f1\u8bed\u4e4b\u95f4\u5b58\u5728\u53cc\u5411\u6587\u5316\u8fc1\u79fb\uff0c\u800c\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e3b\u8981\u5411\u82f1\u8bed\u5355\u5411\u8fc1\u79fb\u3002\u9891\u7387\u5047\u8bbe\u89e3\u91ca\u4e86\u8fd9\u4e00\u4e0d\u5bf9\u79f0\u73b0\u8c61\u3002", "conclusion": "\u6587\u5316\u77e5\u8bc6\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u51fa\u73b0\u9891\u7387\u8d8a\u9ad8\uff0c\u8d8a\u5bb9\u6613\u8fc1\u79fb\uff0c\u8fd9\u4e00\u53d1\u73b0\u4e3a\u4f18\u5316LLMs\u6587\u5316\u77e5\u8bc6\u83b7\u53d6\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u6587\u5316\u591a\u6837\u6027, \u591a\u8bed\u8a00\u8fc1\u79fb, \u77e5\u8bc6\u83b7\u53d6, \u8bad\u7ec3\u6570\u636e\u9891\u7387"}}
{"id": "2506.01569", "pdf": "https://arxiv.org/pdf/2506.01569", "abs": "https://arxiv.org/abs/2506.01569", "authors": ["Eduardo Paluzo-Hidalgo"], "title": "Latent Space Topology Evolution in Multilayer Perceptrons", "categories": ["cs.LG", "math.AT"], "comment": null, "summary": "This paper introduces a topological framework for interpreting the internal\nrepresentations of Multilayer Perceptrons (MLPs). We construct a simplicial\ntower, a sequence of simplicial complexes connected by simplicial maps, that\ncaptures how data topology evolves across network layers. Our approach enables\nbi-persistence analysis: layer persistence tracks topological features within\neach layer across scales, while MLP persistence reveals how these features\ntransform through the network. We prove stability theorems for our topological\ndescriptors and establish that linear separability in latent spaces is related\nto disconnected components in the nerve complexes. To make our framework\npractical, we develop a combinatorial algorithm for computing MLP persistence\nand introduce trajectory-based visualisations that track data flow through the\nnetwork. Experiments on synthetic and real-world medical data demonstrate our\nmethod's ability to identify redundant layers, reveal critical topological\ntransitions, and provide interpretable insights into how MLPs progressively\norganise data for classification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u62d3\u6251\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u91ca\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u7684\u5185\u90e8\u8868\u5f81\uff0c\u5e76\u901a\u8fc7\u6784\u9020\u5355\u7eaf\u590d\u5f62\u5e8f\u5217\u5206\u6790\u6570\u636e\u62d3\u6251\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u6f14\u5316\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u62d3\u6251\u5b66\u65b9\u6cd5\u63ed\u793aMLP\u4e2d\u6570\u636e\u8868\u5f81\u7684\u6f14\u53d8\u8fc7\u7a0b\uff0c\u589e\u5f3a\u5bf9\u7f51\u7edc\u884c\u4e3a\u7684\u7406\u89e3\u4e0e\u89e3\u91ca\u6027\u3002", "method": "\u6784\u5efa\u5355\u7eaf\u590d\u5f62\u5854\uff0c\u63d0\u51fa\u53cc\u6301\u4e45\u6027\u5206\u6790\u65b9\u6cd5\uff08\u5c42\u6301\u4e45\u6027\u548cMLP\u6301\u4e45\u6027\uff09\uff0c\u5f00\u53d1\u7ec4\u5408\u7b97\u6cd5\u8ba1\u7b97MLP\u6301\u4e45\u6027\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u8f68\u8ff9\u7684\u53ef\u89c6\u5316\u5de5\u5177\u3002", "result": "\u8bc1\u660e\u4e86\u62d3\u6251\u63cf\u8ff0\u7b26\u7684\u7a33\u5b9a\u6027\u5b9a\u7406\uff0c\u53d1\u73b0\u7ebf\u6027\u53ef\u5206\u6027\u4e0e\u795e\u7ecf\u590d\u6742\u5ea6\u7684\u8fde\u901a\u6027\u76f8\u5173\uff0c\u5b9e\u9a8c\u663e\u793a\u65b9\u6cd5\u80fd\u8bc6\u522b\u5197\u4f59\u5c42\u548c\u5173\u952e\u62d3\u6251\u8dc3\u8fc1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aMLP\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u62d3\u6251\u5206\u6790\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u6570\u636e\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u9010\u6b65\u7ec4\u7ec7\u8fc7\u7a0b\u3002", "keywords": "\u62d3\u6251\u5206\u6790, \u591a\u5c42\u611f\u77e5\u673a, \u5355\u7eaf\u590d\u5f62, \u6301\u4e45\u6027, \u53ef\u89e3\u91ca\u6027"}}
{"id": "2506.01687", "pdf": "https://arxiv.org/pdf/2506.01687", "abs": "https://arxiv.org/abs/2506.01687", "authors": ["Anya Sims", "Thom Foster", "Klara Kaleb", "Tuan-Duy H. Nguyen", "Joseph Lee", "Jakob N. Foerster", "Yee Whye Teh", "Cong Lu"], "title": "StochasTok: Improving Fine-Grained Subword Understanding in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Subword-level understanding is integral to numerous tasks, including\nunderstanding multi-digit numbers, spelling mistakes, abbreviations, rhyming,\nand wordplay. Despite this, current large language models (LLMs) still often\nstruggle with seemingly simple subword-level tasks like How many 'r's in\n'strawberry'?. A key factor behind these failures is tokenization which\nobscures the fine-grained structure of words. Current alternatives, such as\ncharacter-level and dropout tokenization methods, significantly increase\ncomputational costs and provide inconsistent improvements. In this paper we\nrevisit tokenization and introduce StochasTok, a simple, efficient stochastic\ntokenization scheme that randomly splits tokens during training, allowing LLMs\nto 'see' their internal structure. Our experiments show that pretraining with\nStochasTok substantially improves LLMs' downstream performance across multiple\nsubword-level language games, including character counting, substring\nidentification, and math tasks. Furthermore, StochasTok's simplicity allows\nseamless integration at any stage of the training pipeline; and we demonstrate\nthat post-training with StochasTok can instill improved subword understanding\ninto existing pretrained models, thus avoiding costly pretraining from scratch.\nThese dramatic improvements achieved with a minimal change suggest StochasTok\nholds exciting potential when applied to larger, more capable models. Code\nopen-sourced at: https://github.com/anyasims/stochastok.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86StochasTok\uff0c\u4e00\u79cd\u968f\u673a\u5206\u8bcd\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u62c6\u5206\u6807\u8bb0\u6765\u6539\u5584\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b50\u8bcd\u7ea7\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5b50\u8bcd\u7ea7\u4efb\u52a1\uff08\u5982\u7edf\u8ba1\u5b57\u6bcd\u6570\u91cf\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u5206\u8bcd\u65b9\u6cd5\u63a9\u76d6\u4e86\u8bcd\u7684\u7ec6\u7c92\u5ea6\u7ed3\u6784\u3002", "method": "\u5f15\u5165StochasTok\uff0c\u4e00\u79cd\u968f\u673a\u5206\u8bcd\u65b9\u6848\uff0c\u8bad\u7ec3\u65f6\u968f\u673a\u62c6\u5206\u6807\u8bb0\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u89c2\u5bdf\u5230\u8bcd\u7684\u5185\u90e8\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cStochasTok\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u5b57\u7b26\u8ba1\u6570\u3001\u5b50\u4e32\u8bc6\u522b\u7b49\u5b50\u8bcd\u7ea7\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4e14\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u3002", "conclusion": "StochasTok\u901a\u8fc7\u7b80\u5355\u6539\u52a8\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u5c55\u793a\u4e86\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "StochasTok, \u968f\u673a\u5206\u8bcd, \u5b50\u8bcd\u7ea7\u4efb\u52a1, \u5927\u578b\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.01582", "pdf": "https://arxiv.org/pdf/2506.01582", "abs": "https://arxiv.org/abs/2506.01582", "authors": ["Fabrizio Boncoraglio", "Emanuele Troiani", "Vittorio Erba", "Lenka Zdeborov\u00e1"], "title": "Bayes optimal learning of attention-indexed models", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.IT", "math.IT", "stat.ML"], "comment": null, "summary": "We introduce the attention-indexed model (AIM), a theoretical framework for\nanalyzing learning in deep attention layers. Inspired by multi-index models,\nAIM captures how token-level outputs emerge from layered bilinear interactions\nover high-dimensional embeddings. Unlike prior tractable attention models, AIM\nallows full-width key and query matrices, aligning more closely with practical\ntransformers. Using tools from statistical mechanics and random matrix theory,\nwe derive closed-form predictions for Bayes-optimal generalization error and\nidentify sharp phase transitions as a function of sample complexity, model\nwidth, and sequence length. We propose a matching approximate message passing\nalgorithm and show that gradient descent can reach optimal performance. AIM\noffers a solvable playground for understanding learning in modern attention\narchitectures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u6ce8\u610f\u529b\u7d22\u5f15\u6a21\u578b\uff08AIM\uff09\uff0c\u7528\u4e8e\u5206\u6790\u6df1\u5ea6\u6ce8\u610f\u529b\u5c42\u4e2d\u7684\u5b66\u4e60\u673a\u5236\uff0c\u901a\u8fc7\u7edf\u8ba1\u529b\u5b66\u548c\u968f\u673a\u77e9\u9635\u7406\u8bba\u5de5\u5177\u9884\u6d4b\u4f18\u5316\u6027\u80fd\uff0c\u5e76\u4e0e\u5b9e\u8df5\u4e2d\u7684Transformer\u66f4\u63a5\u8fd1\u3002", "motivation": "\u7814\u7a76\u73b0\u4ee3\u6ce8\u610f\u529b\u67b6\u6784\u4e2d\u7684\u5b66\u4e60\u673a\u5236\uff0c\u63d0\u51fa\u4e00\u4e2a\u66f4\u8d34\u8fd1\u5b9e\u9645Transformer\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u91c7\u7528\u7edf\u8ba1\u529b\u5b66\u548c\u968f\u673a\u77e9\u9635\u7406\u8bba\u5de5\u5177\uff0c\u5206\u6790\u591a\u7d22\u5f15\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5c42\uff0c\u5e76\u63d0\u51fa\u8fd1\u4f3c\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\u3002", "result": "\u5f97\u51fa\u4e86Bayes\u6700\u4f18\u6cdb\u5316\u8bef\u5dee\u7684\u5c01\u95ed\u89e3\uff0c\u5e76\u53d1\u73b0\u6837\u672c\u590d\u6742\u5ea6\u3001\u6a21\u578b\u5bbd\u5ea6\u548c\u5e8f\u5217\u957f\u5ea6\u7684\u5c16\u9510\u76f8\u53d8\u3002", "conclusion": "AIM\u4e3a\u7406\u89e3\u73b0\u4ee3\u6ce8\u610f\u529b\u67b6\u6784\u4e2d\u7684\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u89e3\u7684\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u68af\u5ea6\u4e0b\u964d\u53ef\u4ee5\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "keywords": "\u6ce8\u610f\u529b\u7d22\u5f15\u6a21\u578b, \u7edf\u8ba1\u529b\u5b66, \u968f\u673a\u77e9\u9635\u7406\u8bba, \u6df1\u5ea6\u5b66\u4e60, Transformer"}}
{"id": "2506.01698", "pdf": "https://arxiv.org/pdf/2506.01698", "abs": "https://arxiv.org/abs/2506.01698", "authors": ["Wenna Lai", "Haoran Xie", "Guandong Xu", "Qing Li", "S. Joe Qin"], "title": "When LLMs Team Up: The Emergence of Collaborative Affective Computing", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 7 figures, and 3 tables", "summary": "Affective Computing (AC) is essential in bridging the gap between human\nemotional experiences and machine understanding. Traditionally, AC tasks in\nnatural language processing (NLP) have been approached through pipeline\narchitectures, which often suffer from structure rigidity that leads to\ninefficiencies and limited adaptability. The advent of Large Language Models\n(LLMs) has revolutionized this field by offering a unified approach to\naffective understanding and generation tasks, enhancing the potential for\ndynamic, real-time interactions. However, LLMs face cognitive limitations in\naffective reasoning, such as misinterpreting cultural nuances or contextual\nemotions, and hallucination problems in decision-making. To address these\nchallenges, recent research advocates for LLM-based collaboration systems that\nemphasize interactions among specialized models and LLMs, mimicking human-like\naffective intelligence through the synergy of emotional and rational thinking\nthat aligns with Dual Process Theory in psychology. This survey aims to provide\na comprehensive overview of LLM-based collaboration systems in AC, exploring\nfrom structured collaborations to autonomous collaborations. Specifically, it\nincludes: (1) A systematic review of existing methods, focusing on\ncollaboration strategies, mechanisms, key functions, and applications; (2)\nExperimental comparisons of collaboration strategies across representative\ntasks in affective understanding and generation; (3) An analysis highlighting\nthe potential of these systems to enhance robustness and adaptability in\ncomplex affective reasoning; (4) A discussion of key challenges and future\nresearch directions to further advance the field. This work is the first to\nsystematically explore collaborative intelligence with LLMs in AC, paving the\nway for more powerful applications that approach human-like social\nintelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u534f\u4f5c\u7cfb\u7edf\u5728\u60c5\u611f\u8ba1\u7b97\uff08AC\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86\u5176\u65b9\u6cd5\u3001\u5b9e\u9a8c\u6bd4\u8f83\u3001\u6f5c\u529b\u53ca\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4f20\u7edfAC\u4efb\u52a1\u4e2d\u7684\u6d41\u6c34\u7ebf\u67b6\u6784\u5b58\u5728\u7ed3\u6784\u521a\u6027\u95ee\u9898\uff0cLLM\u867d\u63d0\u5347\u4e86\u52a8\u6001\u4ea4\u4e92\u6f5c\u529b\uff0c\u4f46\u5728\u60c5\u611f\u63a8\u7406\u4e2d\u4ecd\u6709\u8ba4\u77e5\u5c40\u9650\u6027\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u534f\u4f5c\u7cfb\u7edf\u4ee5\u5f25\u8865\u8fd9\u4e9b\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u6bd4\u8f83\u534f\u4f5c\u7b56\u7565\uff0c\u5e76\u5206\u6790\u5176\u5bf9\u590d\u6742\u60c5\u611f\u63a8\u7406\u7684\u589e\u5f3a\u4f5c\u7528\u3002", "result": "LLM\u534f\u4f5c\u7cfb\u7edf\u80fd\u63d0\u5347\u60c5\u611f\u7406\u89e3\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u4f46\u9700\u89e3\u51b3\u6587\u5316\u8bef\u89e3\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "\u534f\u4f5c\u667a\u80fd\u4e3aAC\u63d0\u4f9b\u4e86\u4eba\u7c7b\u5316\u7684\u793e\u4f1a\u667a\u80fd\u6f5c\u529b\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u9700\u89e3\u51b3\u73b0\u6709\u6311\u6218\u3002", "keywords": "\u60c5\u611f\u8ba1\u7b97, \u5927\u8bed\u8a00\u6a21\u578b, \u534f\u4f5c\u7cfb\u7edf, \u53cc\u8fc7\u7a0b\u7406\u8bba, \u793e\u4f1a\u667a\u80fd"}}
{"id": "2506.01584", "pdf": "https://arxiv.org/pdf/2506.01584", "abs": "https://arxiv.org/abs/2506.01584", "authors": ["Denys Herasymuk", "Nazar Protsiv", "Julia Stoyanovich"], "title": "VirnyFlow: A Design Space for Responsible Model Development", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "Developing machine learning (ML) models requires a deep understanding of\nreal-world problems, which are inherently multi-objective. In this paper, we\npresent VirnyFlow, the first design space for responsible model development,\ndesigned to assist data scientists in building ML pipelines that are tailored\nto the specific context of their problem. Unlike conventional AutoML\nframeworks, VirnyFlow enables users to define customized optimization criteria,\nperform comprehensive experimentation across pipeline stages, and iteratively\nrefine models in alignment with real-world constraints. Our system integrates\nevaluation protocol definition, multi-objective Bayesian optimization,\ncost-aware multi-armed bandits, query optimization, and distributed parallelism\ninto a unified architecture. We show that VirnyFlow significantly outperforms\nstate-of-the-art AutoML systems in both optimization quality and scalability\nacross five real-world benchmarks, offering a flexible, efficient, and\nresponsible alternative to black-box automation in ML development.", "AI": {"tldr": "VirnyFlow\u662f\u4e00\u79cd\u521b\u65b0\u7684\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\uff0c\u65e8\u5728\u5e2e\u52a9\u6570\u636e\u79d1\u5b66\u5bb6\u6784\u5efa\u9002\u5408\u7279\u5b9a\u95ee\u9898\u7684\u8d1f\u8d23\u4efb\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfAutoML\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u95ee\u9898\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u662f\u591a\u76ee\u6807\u7684\uff0c\u9700\u8981\u7ed3\u5408\u5177\u4f53\u4e0a\u4e0b\u6587\u8fdb\u884c\u5b9a\u5236\u5316\u5f00\u53d1\uff0c\u800c\u4f20\u7edfAutoML\u6846\u67b6\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u900f\u660e\u6027\u3002", "method": "VirnyFlow\u6574\u5408\u4e86\u8bc4\u4f30\u534f\u8bae\u5b9a\u4e49\u3001\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\u3001\u6210\u672c\u611f\u77e5\u591a\u81c2\u8001\u864e\u673a\u3001\u67e5\u8be2\u4f18\u5316\u548c\u5206\u5e03\u5f0f\u5e76\u884c\u6280\u672f\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVirnyFlow\u5728\u4f18\u5316\u8d28\u91cf\u548c\u6269\u5c55\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709AutoML\u7cfb\u7edf\u3002", "conclusion": "VirnyFlow\u4e3a\u673a\u5668\u5b66\u4e60\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u9ad8\u6548\u4e14\u8d1f\u8d23\u4efb\u7684\u81ea\u52a8\u5316\u66ff\u4ee3\u65b9\u6848\u3002", "keywords": "\u673a\u5668\u5b66\u4e60,\u591a\u76ee\u6807\u4f18\u5316,\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60,\u8d1f\u8d23\u4efbAI"}}
{"id": "2506.01702", "pdf": "https://arxiv.org/pdf/2506.01702", "abs": "https://arxiv.org/abs/2506.01702", "authors": ["Dominik Macko"], "title": "mdok of KInIT: Robustly Fine-tuned LLM for Binary and Multiclass AI-Generated Text Detection", "categories": ["cs.CL"], "comment": null, "summary": "The large language models (LLMs) are able to generate high-quality texts in\nmultiple languages. Such texts are often not recognizable by humans as\ngenerated, and therefore present a potential of LLMs for misuse (e.g.,\nplagiarism, spams, disinformation spreading). An automated detection is able to\nassist humans to indicate the machine-generated texts; however, its robustness\nto out-of-distribution data is still challenging. This notebook describes our\nmdok approach in robust detection, based on fine-tuning smaller LLMs for text\nclassification. It is applied to both subtasks of Voight-Kampff Generative AI\nDetection 2025, providing remarkable performance in binary detection as well as\nin multiclass (1st rank) classification of various cases of human-AI\ncollaboration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fae\u8c03\u5c0f\u578bLLMs\u7684\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\uff08mdok\uff09\uff0c\u7528\u4e8e\u68c0\u6d4b\u673a\u5668\u751f\u6210\u6587\u672c\uff0c\u5728Voight-Kampff Generative AI Detection 2025\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u7684\u591a\u8bed\u8a00\u9ad8\u8d28\u91cf\u6587\u672c\u53ef\u80fd\u5bfc\u81f4\u6ee5\u7528\uff08\u5982\u6284\u88ad\u3001\u5783\u573e\u4fe1\u606f\u3001\u4f20\u64ad\u865a\u5047\u4fe1\u606f\uff09\uff0c\u81ea\u52a8\u5316\u68c0\u6d4b\u4ecd\u9700\u63d0\u9ad8\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u5fae\u8c03\u5c0f\u578bLLMs\u8fdb\u884c\u6587\u672c\u5206\u7c7b\uff0c\u63d0\u51famdok\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e\u4e8c\u5143\u68c0\u6d4b\u548c\u591a\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u5728Voight-Kampff\u4efb\u52a1\u4e2d\uff0c\u4e8c\u5143\u68c0\u6d4b\u548c\u591a\u5206\u7c7b\uff08\u6392\u540d\u7b2c\u4e00\uff09\u5747\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "mdok\u65b9\u6cd5\u5728\u68c0\u6d4b\u673a\u5668\u751f\u6210\u6587\u672c\u65b9\u9762\u5177\u6709\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u9632\u8303LLMs\u6ee5\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u6587\u672c\u68c0\u6d4b, \u9c81\u68d2\u6027, \u5fae\u8c03, \u591a\u5206\u7c7b"}}
{"id": "2506.01594", "pdf": "https://arxiv.org/pdf/2506.01594", "abs": "https://arxiv.org/abs/2506.01594", "authors": ["Hana Samad", "Michael Akinwumi", "Jameel Khan", "Christoph M\u00fcgge-Durum", "Emmanuel O. Ogundimu"], "title": "Selecting for Less Discriminatory Algorithms: A Relational Search Framework for Navigating Fairness-Accuracy Trade-offs in Practice", "categories": ["cs.LG", "cs.CY", "I.2.1; I.2.6; K.4.1; K.5.2"], "comment": "40 pages, 5 figures. Introduces a horizontal LDA search framework for\n  relationally navigating fairness-accuracy trade-offs using 2021 HMDA data", "summary": "As machine learning models are increasingly embedded into society through\nhigh-stakes decision-making, selecting the right algorithm for a given task,\naudience, and sector presents a critical challenge, particularly in the context\nof fairness. Traditional assessments of model fairness have often framed\nfairness as an objective mathematical property, treating model selection as an\noptimization problem under idealized informational conditions. This overlooks\nmodel multiplicity as a consideration--that multiple models can deliver similar\nperformance while exhibiting different fairness characteristics. Legal scholars\nhave engaged this challenge through the concept of Less Discriminatory\nAlgorithms (LDAs), which frames model selection as a civil rights obligation.\nIn real-world deployment, this normative challenge is bounded by constraints on\nfairness experimentation, e.g., regulatory standards, institutional priorities,\nand resource capacity.\n  Against these considerations, the paper revisits Lee and Floridi (2021)'s\nrelational fairness approach using updated 2021 Home Mortgage Disclosure Act\n(HMDA) data, and proposes an expansion of the scope of the LDA search process.\nWe argue that extending the LDA search horizontally, considering fairness\nacross model families themselves, provides a lightweight complement, or\nalternative, to within-model hyperparameter optimization, when operationalizing\nfairness in non-experimental, resource constrained settings. Fairness metrics\nalone offer useful, but insufficient signals to accurately evaluate candidate\nLDAs. Rather, by using a horizontal LDA search approach with the relational\ntrade-off framework, we demonstrate a responsible minimum viable LDA search on\nreal-world lending outcomes. Organizations can modify this approach to\nsystematically compare, evaluate, and select LDAs that optimize fairness and\naccuracy in a sector-based contextualized manner.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9009\u62e9\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u6a2a\u5411\u641c\u7d22\u548c\u5173\u7cfb\u516c\u5e73\u6846\u67b6\u4f18\u5316\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u9645\u573a\u666f\u3002", "motivation": "\u5728\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5d4c\u5165\u793e\u4f1a\u51b3\u7b56\u65f6\uff0c\u5982\u4f55\u9009\u62e9\u516c\u5e73\u4e14\u9ad8\u6548\u7684\u7b97\u6cd5\u6210\u4e3a\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u662f\u9762\u5bf9\u6a21\u578b\u591a\u6837\u6027\u548c\u8d44\u6e90\u9650\u5236\u3002", "method": "\u63d0\u51fa\u6a2a\u5411LDA\u641c\u7d22\u65b9\u6cd5\uff0c\u7ed3\u5408\u5173\u7cfb\u516c\u5e73\u6846\u67b6\uff0c\u5229\u7528HMDA\u6570\u636e\u8bc4\u4f30\u6a21\u578b\u7684\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u3002", "result": "\u6a2a\u5411LDA\u641c\u7d22\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u9645\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u5316\u7684\u516c\u5e73\u6027\u4f18\u5316\u65b9\u6848\uff0c\u8865\u5145\u4e86\u4f20\u7edf\u53c2\u6570\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5173\u7cfb\u516c\u5e73\u6846\u67b6\u548c\u6a2a\u5411LDA\u641c\u7d22\uff0c\u7ec4\u7ec7\u53ef\u4ee5\u66f4\u7cfb\u7edf\u5730\u9009\u62e9\u548c\u8bc4\u4f30\u516c\u5e73\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u7279\u5b9a\u884c\u4e1a\u573a\u666f\u3002", "keywords": "\u516c\u5e73\u6027,\u673a\u5668\u5b66\u4e60,\u6a21\u578b\u9009\u62e9,LDA,\u8d44\u6e90\u9650\u5236"}}
{"id": "2506.01709", "pdf": "https://arxiv.org/pdf/2506.01709", "abs": "https://arxiv.org/abs/2506.01709", "authors": ["Krishna Patel", "Nivedha Sivakumar", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "title": "Fairness Dynamics During Training", "categories": ["cs.CL"], "comment": null, "summary": "We investigate fairness dynamics during Large Language Model (LLM) training\nto enable the diagnoses of biases and mitigations through training\ninterventions like early stopping; we find that biases can emerge suddenly and\ndo not always follow common performance metrics. We introduce two new metrics\nto evaluate fairness dynamics holistically during model pre-training: Average\nRank and Jensen-Shannon Divergence by Parts. These metrics provide insights\ninto the Pythia models' progression of biases in gender prediction of\noccupations on the WinoBias dataset. By monitoring these dynamics, we find that\n(1) Pythia-6.9b is biased towards men; it becomes more performant and confident\npredicting \"male\" than \"female\" during training, (2) via early-stopping,\nPythia-6.9b can exchange 1.7% accuracy on LAMBADA for a 92.5% increase in\nfairness, and (3) larger models can exhibit more bias; Pythia-6.9b makes more\nassumptions about gender than Pythia-160m, even when a subject's gender is not\nspecified.", "AI": {"tldr": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bad\u7ec3\u4e2d\u7684\u516c\u5e73\u6027\u52a8\u6001\uff0c\u901a\u8fc7\u65b0\u6307\u6807\u8bca\u65ad\u548c\u7f13\u89e3\u504f\u89c1\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u4e2d\u7684\u504f\u89c1\u53ef\u80fd\u7a81\u7136\u51fa\u73b0\u4e14\u4e0e\u6027\u80fd\u6307\u6807\u65e0\u5173\u3002", "motivation": "\u7406\u89e3\u548c\u89e3\u51b3LLM\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u504f\u89c1\uff0c\u5c24\u5176\u662f\u6027\u522b\u504f\u89c1\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u516c\u5e73\u6027\u3002", "method": "\u5f15\u5165\u4e24\u79cd\u65b0\u6307\u6807\uff08\u5e73\u5747\u6392\u540d\u548c\u90e8\u5206Jensen-Shannon\u6563\u5ea6\uff09\uff0c\u5728WinoBias\u6570\u636e\u96c6\u4e0a\u5206\u6790Pythia\u6a21\u578b\u7684\u6027\u522b\u504f\u89c1\u52a8\u6001\u3002", "result": "\u53d1\u73b0Pythia-6.9b\u5bf9\u7537\u6027\u504f\u89c1\u66f4\u5f3a\uff0c\u65e9\u671f\u505c\u6b62\u53ef\u663e\u8457\u63d0\u5347\u516c\u5e73\u6027\uff0c\u5927\u6a21\u578b\u53ef\u80fd\u8868\u73b0\u51fa\u66f4\u591a\u504f\u89c1\u3002", "conclusion": "\u76d1\u63a7\u516c\u5e73\u6027\u52a8\u6001\u6709\u52a9\u4e8e\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u6539\u8fdb\u6a21\u578b\u516c\u5e73\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u516c\u5e73\u6027, \u504f\u89c1, \u9884\u8bad\u7ec3, \u65e9\u671f\u505c\u6b62"}}
{"id": "2506.01596", "pdf": "https://arxiv.org/pdf/2506.01596", "abs": "https://arxiv.org/abs/2506.01596", "authors": ["Yaniv Galron", "Fabrizio Frasca", "Haggai Maron", "Eran Treister", "Moshe Eliasof"], "title": "Understanding and Improving Laplacian Positional Encodings For Temporal GNNs", "categories": ["cs.LG", "cs.AI"], "comment": "ECML-PKDD 2025", "summary": "Temporal graph learning has applications in recommendation systems, traffic\nforecasting, and social network analysis. Although multiple architectures have\nbeen introduced, progress in positional encoding for temporal graphs remains\nlimited. Extending static Laplacian eigenvector approaches to temporal graphs\nthrough the supra-Laplacian has shown promise, but also poses key challenges:\nhigh eigendecomposition costs, limited theoretical understanding, and ambiguity\nabout when and how to apply these encodings. In this paper, we address these\nissues by (1) offering a theoretical framework that connects supra-Laplacian\nencodings to per-time-slice encodings, highlighting the benefits of leveraging\nadditional temporal connectivity, (2) introducing novel methods to reduce the\ncomputational overhead, achieving up to 56x faster runtimes while scaling to\ngraphs with 50,000 active nodes, and (3) conducting an extensive experimental\nstudy to identify which models, tasks, and datasets benefit most from these\nencodings. Our findings reveal that while positional encodings can\nsignificantly boost performance in certain scenarios, their effectiveness\nvaries across different models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u65f6\u5e8f\u56fe\u4e2d\u4f4d\u7f6e\u7f16\u7801\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6846\u67b6\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u65f6\u5e8f\u56fe\u5b66\u4e60\u5728\u591a\u4e2a\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4f4d\u7f6e\u7f16\u7801\u65b9\u9762\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u7406\u8bba\u7406\u89e3\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u8fde\u63a5\u8d85\u62c9\u666e\u62c9\u65af\u7f16\u7801\u4e0e\u65f6\u95f4\u7247\u7f16\u7801\uff0c\u5f15\u5165\u65b0\u65b9\u6cd5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7814\u7a76\u3002", "result": "\u65b0\u65b9\u6cd5\u5b9e\u73b0\u4e8656\u500d\u7684\u52a0\u901f\uff0c\u4e14\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u6548\u679c\u56e0\u6a21\u578b\u800c\u5f02\u3002", "conclusion": "\u65f6\u5e8f\u56fe\u4e2d\u7684\u4f4d\u7f6e\u7f16\u7801\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u6709\u6548\uff0c\u4f46\u5176\u9002\u7528\u6027\u9700\u6839\u636e\u6a21\u578b\u548c\u4efb\u52a1\u9009\u62e9\u3002", "keywords": "\u65f6\u5e8f\u56fe\u5b66\u4e60, \u4f4d\u7f6e\u7f16\u7801, \u8d85\u62c9\u666e\u62c9\u65af, \u8ba1\u7b97\u6548\u7387, \u5b9e\u9a8c\u7814\u7a76"}}
{"id": "2506.01710", "pdf": "https://arxiv.org/pdf/2506.01710", "abs": "https://arxiv.org/abs/2506.01710", "authors": ["Fangyu Lei", "Jinxiang Meng", "Yiming Huang", "Tinghong Chen", "Yun Zhang", "Shizhu He", "Jun Zhao", "Kang Liu"], "title": "Reasoning-Table: Exploring Reinforcement Learning for Table Reasoning", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Table reasoning, encompassing tasks such as table question answering, fact\nverification, and text-to-SQL, requires precise understanding of structured\ntabular data, coupled with numerical computation and code manipulation for\neffective inference. Supervised fine-tuning (SFT) approaches have achieved\nnotable success but often struggle with generalization and robustness due to\nbiases inherent in imitative learning. We introduce Reasoning-Table, the first\napplication of reinforcement learning (RL) to table reasoning, achieving\nstate-of-the-art performance. Through rigorous data preprocessing, reward\ndesign, and tailored training strategies, our method leverages simple\nrule-based outcome rewards to outperform SFT across multiple benchmarks.\nUnified training across diverse tasks enables Reasoning-Table to emerge as a\nrobust table reasoning large language model, surpassing larger proprietary\nmodels like Claude-3.7-Sonnet by 4.0% on table reasoning benchmarks. The\napproach also achieves excellent performance on text-to-SQL tasks, reaching\n68.3% performance on the BIRD dev dataset with a 7B model. Further experiments\ndemonstrate that Reasoning-Table enhances the model's generalization\ncapabilities and robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86**Reasoning-Table**\uff0c\u9996\u6b21\u5c06\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5e94\u7528\u4e8e\u8868\u683c\u63a8\u7406\u4efb\u52a1\uff0c\u901a\u8fc7\u6570\u636e\u9884\u5904\u7406\u3001\u5956\u52b1\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7b56\u7565\u4f18\u5316\uff0c\u8d85\u8d8a\u4e86\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u65b9\u6cd5\u5728\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u56e0\u4e3a\u6a21\u4eff\u5b66\u4e60\u5bb9\u6613\u5f15\u5165\u504f\u5dee\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u6570\u636e\u9884\u5904\u7406\u3001\u5956\u52b1\u8bbe\u8ba1\u548c\u9488\u5bf9\u6027\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5229\u7528\u7b80\u5355\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "result": "Reasoning-Table\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86SFT\u65b9\u6cd5\uff0c\u5e76\u5728\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e2d\u6bd4Claude-3.7-Sonnet\u9ad8\u51fa4.0%\uff0c\u540c\u65f6\u5728\u6587\u672c\u5230SQL\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e8668.3%\u7684\u6027\u80fd\uff08\u57fa\u4e8eBIRD dev\u6570\u636e\u96c6\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\u7684\u5e94\u7528\u663e\u8457\u63d0\u5347\u4e86\u8868\u683c\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3001\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u751a\u81f3\u4f18\u4e8e\u66f4\u5927\u89c4\u6a21\u7684\u4e13\u6709\u6a21\u578b\u3002", "keywords": "\u8868\u683c\u63a8\u7406, \u5f3a\u5316\u5b66\u4e60, \u76d1\u7763\u5fae\u8c03, \u9c81\u68d2\u6027, \u6cdb\u5316\u80fd\u529b, \u6587\u672c\u5230SQL"}}
{"id": "2506.01597", "pdf": "https://arxiv.org/pdf/2506.01597", "abs": "https://arxiv.org/abs/2506.01597", "authors": ["Yixian Zhang", "Huaze Tang", "Chao Wang", "Wenbo Ding"], "title": "Policy Newton Algorithm in Reproducing Kernel Hilbert Space", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) policies represented in Reproducing Kernel\nHilbert Spaces (RKHS) offer powerful representational capabilities. While\nsecond-order optimization methods like Newton's method demonstrate faster\nconvergence than first-order approaches, current RKHS-based policy optimization\nremains constrained to first-order techniques. This limitation stems primarily\nfrom the intractability of explicitly computing and inverting the\ninfinite-dimensional Hessian operator in RKHS. We introduce Policy Newton in\nRKHS, the first second-order optimization framework specifically designed for\nRL policies represented in RKHS. Our approach circumvents direct computation of\nthe inverse Hessian operator by optimizing a cubic regularized auxiliary\nobjective function. Crucially, we leverage the Representer Theorem to transform\nthis infinite-dimensional optimization into an equivalent, computationally\ntractable finite-dimensional problem whose dimensionality scales with the\ntrajectory data volume. We establish theoretical guarantees proving convergence\nto a local optimum with a local quadratic convergence rate. Empirical\nevaluations on a toy financial asset allocation problem validate these\ntheoretical properties, while experiments on standard RL benchmarks demonstrate\nthat Policy Newton in RKHS achieves superior convergence speed and higher\nepisodic rewards compared to established first-order RKHS approaches and\nparametric second-order methods. Our work bridges a critical gap between\nnon-parametric policy representations and second-order optimization methods in\nreinforcement learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u7528\u4e8eRKHS\u4e2dRL\u7b56\u7565\u7684\u4e8c\u9636\u4f18\u5316\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u65e0\u9650\u7ef4Hessian\u7b97\u5b50\u96be\u4ee5\u8ba1\u7b97\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u5f53\u524dRKHS\u4e2d\u7684RL\u7b56\u7565\u4f18\u5316\u4ec5\u9650\u4e8e\u4e00\u9636\u65b9\u6cd5\uff0c\u56e0\u4e3a\u65e0\u9650\u7ef4Hessian\u7b97\u5b50\u96be\u4ee5\u663e\u5f0f\u8ba1\u7b97\u548c\u6c42\u9006\u3002", "method": "\u63d0\u51faPolicy Newton in RKHS\uff0c\u901a\u8fc7\u4f18\u5316\u4e09\u6b21\u6b63\u5219\u5316\u7684\u8f85\u52a9\u76ee\u6807\u51fd\u6570\uff0c\u5229\u7528\u8868\u793a\u5b9a\u7406\u5c06\u5176\u8f6c\u5316\u4e3a\u6709\u9650\u7ef4\u95ee\u9898\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5c40\u90e8\u6700\u4f18\u89e3\u7684\u6536\u655b\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6536\u655b\u901f\u5ea6\u548c\u5956\u52b1\u4f18\u4e8e\u4e00\u9636RKHS\u65b9\u6cd5\u548c\u53c2\u6570\u5316\u4e8c\u9636\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u975e\u53c2\u6570\u7b56\u7565\u8868\u793a\u4e0e\u4e8c\u9636\u4f18\u5316\u65b9\u6cd5\u4e4b\u95f4\u7684\u5173\u952e\u7a7a\u767d\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, RKHS, \u4e8c\u9636\u4f18\u5316, Policy Newton"}}
{"id": "2506.01713", "pdf": "https://arxiv.org/pdf/2506.01713", "abs": "https://arxiv.org/abs/2506.01713", "authors": ["Zhongwei Wan", "Zhihao Dou", "Che Liu", "Yu Zhang", "Dongfei Cui", "Qinjian Zhao", "Hui Shen", "Jing Xiong", "Yi Xin", "Yifan Jiang", "Yangfan He", "Mi Zhang", "Shen Yan"], "title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning", "categories": ["cs.CL"], "comment": "Under review", "summary": "Multimodal large language models (MLLMs) have shown promising capabilities in\nreasoning tasks, yet still struggle with complex problems requiring explicit\nself-reflection and self-correction, especially compared to their unimodal\ntext-based counterparts. Existing reflection methods are simplistic and\nstruggle to generate meaningful and instructive feedback, as the reasoning\nability and knowledge limits of pre-trained models are largely fixed during\ninitial training. To overcome these challenges, we propose Multimodal\nSelf-Reflection enhanced reasoning with Group Relative Policy Optimization\n(SRPO), a two-stage reflection-aware reinforcement learning (RL) framework\nexplicitly designed to enhance multimodal LLM reasoning. In the first stage, we\nconstruct a high-quality, reflection-focused dataset under the guidance of an\nadvanced MLLM, which generates reflections based on initial responses to help\nthe policy model learn both reasoning and self-reflection. In the second stage,\nwe introduce a novel reward mechanism within the GRPO framework that encourages\nconcise and cognitively meaningful reflection while avoiding redundancy.\nExtensive experiments across multiple multimodal reasoning benchmarks,\nincluding MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B\nand Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms\nstate-of-the-art models, achieving notable improvements in both reasoning\naccuracy and reflection quality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSRPO\u7684\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u81ea\u6211\u53cd\u601d\u548c\u4fee\u6b63\u7684\u590d\u6742\u95ee\u9898\u4e0a\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u62ec\u6784\u5efa\u9ad8\u8d28\u91cf\u53cd\u601d\u6570\u636e\u96c6\u548c\u5f15\u5165\u65b0\u9896\u7684\u5956\u52b1\u673a\u5236\u3002", "result": "SRPO\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u7406\u51c6\u786e\u6027\u548c\u53cd\u601d\u8d28\u91cf\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "SRPO\u901a\u8fc7\u53cd\u601d\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, \u5f3a\u5316\u5b66\u4e60, \u81ea\u6211\u53cd\u601d, \u63a8\u7406\u80fd\u529b, SRPO"}}
{"id": "2506.01598", "pdf": "https://arxiv.org/pdf/2506.01598", "abs": "https://arxiv.org/abs/2506.01598", "authors": ["Jin Song", "Kenji Kawaguchi", "Zhenya Yan"], "title": "PMNO: A novel physics guided multi-step neural operator predictor for partial differential equations", "categories": ["cs.LG", "physics.comp-ph"], "comment": "27 pages, 12 figures", "summary": "Neural operators, which aim to approximate mappings between\ninfinite-dimensional function spaces, have been widely applied in the\nsimulation and prediction of physical systems. However, the limited\nrepresentational capacity of network architectures, combined with their heavy\nreliance on large-scale data, often hinder effective training and result in\npoor extrapolation performance. In this paper, inspired by traditional\nnumerical methods, we propose a novel physics guided multi-step neural operator\n(PMNO) architecture to address these challenges in long-horizon prediction of\ncomplex physical systems. Distinct from general operator learning methods, the\nPMNO framework replaces the single-step input with multi-step historical data\nin the forward pass and introduces an implicit time-stepping scheme based on\nthe Backward Differentiation Formula (BDF) during backpropagation. This design\nnot only strengthens the model's extrapolation capacity but also facilitates\nmore efficient and stable training with fewer data samples, especially for\nlong-term predictions. Meanwhile, a causal training strategy is employed to\ncircumvent the need for multi-stage training and to ensure efficient end-to-end\noptimization. The neural operator architecture possesses resolution-invariant\nproperties, enabling the trained model to perform fast extrapolation on\narbitrary spatial resolutions. We demonstrate the superior predictive\nperformance of PMNO predictor across a diverse range of physical systems,\nincluding 2D linear system, modeling over irregular domain, complex-valued wave\ndynamics, and reaction-diffusion processes. Depending on the specific problem\nsetting, various neural operator architectures, including FNO, DeepONet, and\ntheir variants, can be seamlessly integrated into the PMNO framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u5f15\u5bfc\u7684\u591a\u6b65\u795e\u7ecf\u7b97\u5b50\u67b6\u6784\uff08PMNO\uff09\uff0c\u901a\u8fc7\u591a\u6b65\u5386\u53f2\u6570\u636e\u548c\u9690\u5f0f\u65f6\u95f4\u6b65\u8fdb\u65b9\u6848\uff0c\u63d0\u9ad8\u957f\u671f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7b97\u5b50\u5728\u6a21\u62df\u548c\u9884\u6d4b\u7269\u7406\u7cfb\u7edf\u65f6\uff0c\u56e0\u7f51\u7edc\u67b6\u6784\u9650\u5236\u548c\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6548\u679c\u5dee\u548c\u9884\u6d4b\u6027\u80fd\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u591a\u6b65\u5386\u53f2\u6570\u636e\u8f93\u5165\u548c\u57fa\u4e8eBDF\u7684\u9690\u5f0f\u65f6\u95f4\u6b65\u8fdb\u65b9\u6848\uff0c\u7ed3\u5408\u56e0\u679c\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u6548\u7a33\u5b9a\u7684\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "PMNO\u5728\u591a\u79cd\u7269\u7406\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u652f\u6301\u4efb\u610f\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u5916\u63a8\u3002", "conclusion": "PMNO\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u795e\u7ecf\u7b97\u5b50\u5bf9\u590d\u6742\u7cfb\u7edf\u7684\u957f\u671f\u9884\u6d4b\u80fd\u529b\uff0c\u4e14\u517c\u5bb9\u591a\u79cd\u73b0\u6709\u7b97\u5b50\u67b6\u6784\u3002", "keywords": "\u795e\u7ecf\u7b97\u5b50\u3001\u7269\u7406\u5f15\u5bfc\u3001\u591a\u6b65\u9884\u6d4b\u3001BDF\u3001\u5916\u63a8\u6027\u80fd"}}
{"id": "2506.01723", "pdf": "https://arxiv.org/pdf/2506.01723", "abs": "https://arxiv.org/abs/2506.01723", "authors": ["Soyoung Oh", "Xinting Huang", "Mathis Pink", "Michael Hahn", "Vera Demberg"], "title": "Tug-of-war between idiom's figurative and literal meanings in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Idioms present a unique challenge for language models due to their\nnon-compositional figurative meanings, which often strongly diverge from the\nidiom's literal interpretation. This duality requires a model to learn\nrepresenting and deciding between the two meanings to interpret an idiom in a\nfigurative sense, or literally. In this paper, we employ tools from mechanistic\ninterpretability to trace how a large pretrained causal transformer\n(LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom\nprocessing: First, the idiom's figurative meaning is retrieved in early\nattention and MLP sublayers. We identify specific attention heads which boost\nthe figurative meaning of the idiom while suppressing the idiom's literal\ninterpretation. The model subsequently represents the figurative representation\nthrough an intermediate path. Meanwhile, a parallel bypass route forwards\nliteral interpretation, ensuring that a both reading remain available. Overall,\nour findings provide a mechanistic evidence for idiom comprehension in an\nautoregressive transformer.", "AI": {"tldr": "\u5927\u578b\u9884\u8bad\u7ec3\u56e0\u679c\u53d8\u6362\u5668\uff08LLama3.2-1B-base\uff09\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u5c42\u611f\u77e5\u5668\u5b50\u5c42\u5206\u4e09\u6b65\u5904\u7406\u4e60\u8bed\u7684\u975e\u7ec4\u5408\u6027\u6bd4\u55bb\u610f\u4e49\uff0c\u540c\u65f6\u4fdd\u7559\u5b57\u9762\u89e3\u91ca\u3002", "motivation": "\u7814\u7a76\u4e60\u8bed\u5bf9\u8bed\u8a00\u6a21\u578b\u7684\u6311\u6218\uff0c\u56e0\u5176\u6bd4\u55bb\u610f\u4e49\u4e0e\u5b57\u9762\u89e3\u91ca\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u9700\u6a21\u578b\u5b66\u4e60\u540c\u65f6\u5904\u7406\u8fd9\u4e24\u79cd\u610f\u4e49\u3002", "method": "\u91c7\u7528\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5de5\u5177\uff0c\u8ffd\u8e2a\u6a21\u578b\u5904\u7406\u4e60\u8bed\u7684\u8fc7\u7a0b\uff0c\u5b9a\u4f4d\u4e60\u8bed\u6bd4\u55bb\u610f\u4e49\u68c0\u7d22\u3001\u6291\u5236\u5b57\u9762\u89e3\u91ca\u53ca\u5e76\u884c\u5904\u7406\u8def\u5f84\u3002", "result": "\u8bc6\u522b\u51fa\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u63d0\u5347\u6bd4\u55bb\u610f\u4e49\u5e76\u6291\u5236\u5b57\u9762\u89e3\u91ca\uff0c\u540c\u65f6\u6a21\u578b\u901a\u8fc7\u4e2d\u4ecb\u8def\u5f84\u8868\u793a\u6bd4\u55bb\u610f\u4e49\uff0c\u4fdd\u7559\u5b57\u9762\u89e3\u91ca\u7684\u5e76\u884c\u8def\u5f84\u3002", "conclusion": "\u7814\u7a76\u4e3a\u81ea\u56de\u5f52\u53d8\u6362\u5668\u4e2d\u7684\u4e60\u8bed\u7406\u89e3\u63d0\u4f9b\u4e86\u673a\u5236\u6027\u8bc1\u636e\u3002", "keywords": "\u4e60\u8bed\u5904\u7406,\u673a\u5236\u53ef\u89e3\u91ca\u6027,\u6ce8\u610f\u529b\u5934,LLama3.2-1B-base"}}
{"id": "2506.01599", "pdf": "https://arxiv.org/pdf/2506.01599", "abs": "https://arxiv.org/abs/2506.01599", "authors": ["Hanlin Yu", "Berfin Inal", "Georgios Arvanitidis", "Soren Hauberg", "Francesco Locatello", "Marco Fumero"], "title": "Connecting Neural Models Latent Geometries with Relative Geodesic Representations", "categories": ["cs.LG"], "comment": null, "summary": "Neural models learn representations of high-dimensional data on\nlow-dimensional manifolds. Multiple factors, including stochasticities in the\ntraining process, model architectures, and additional inductive biases, may\ninduce different representations, even when learning the same task on the same\ndata. However, it has recently been shown that when a latent structure is\nshared between distinct latent spaces, relative distances between\nrepresentations can be preserved, up to distortions. Building on this idea, we\ndemonstrate that exploiting the differential-geometric structure of latent\nspaces of neural models, it is possible to capture precisely the\ntransformations between representational spaces trained on similar data\ndistributions. Specifically, we assume that distinct neural models parametrize\napproximately the same underlying manifold, and introduce a representation\nbased on the pullback metric that captures the intrinsic structure of the\nlatent space, while scaling efficiently to large models. We validate\nexperimentally our method on model stitching and retrieval tasks, covering\nautoencoders and vision foundation discriminative models, across diverse\narchitectures, datasets, and pretraining schemes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u795e\u7ecf\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u7684\u5fae\u5206\u51e0\u4f55\u7ed3\u6784\uff0c\u6355\u6349\u76f8\u4f3c\u6570\u636e\u5206\u5e03\u4e0a\u8bad\u7ec3\u7684\u4e0d\u540c\u8868\u793a\u7a7a\u95f4\u4e4b\u95f4\u7684\u53d8\u6362\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u795e\u7ecf\u6a21\u578b\u5b66\u4e60\u6570\u636e\u8868\u793a\u65f6\u7684\u6f5c\u5728\u7ed3\u6784\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u8bad\u7ec3\u6761\u4ef6\u4e0b\u4ea7\u751f\u7684\u8868\u793a\u7a7a\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\u3002", "method": "\u65b9\u6cd5\u57fa\u4e8e\u5047\u8bbe\u4e0d\u540c\u6a21\u578b\u53c2\u6570\u5316\u76f8\u540c\u7684\u6f5c\u5728\u6d41\u5f62\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u62c9\u56de\u5ea6\u91cf\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u4ee5\u9ad8\u6548\u6355\u6349\u6f5c\u5728\u7a7a\u95f4\u7684\u5185\u5728\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u6a21\u578b\u7f1d\u5408\u548c\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u6db5\u76d6\u81ea\u7f16\u7801\u5668\u548c\u89c6\u89c9\u57fa\u7840\u5224\u522b\u6a21\u578b\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0c\u5229\u7528\u51e0\u4f55\u7ed3\u6784\u53ef\u4ee5\u7cbe\u786e\u63cf\u8ff0\u4e0d\u540c\u8868\u793a\u7a7a\u95f4\u4e4b\u95f4\u7684\u53d8\u6362\u5173\u7cfb\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc\u3001\u6f5c\u5728\u7a7a\u95f4\u3001\u8868\u793a\u5b66\u4e60\u3001\u5fae\u5206\u51e0\u4f55\u3001\u62c9\u56de\u5ea6\u91cf"}}
{"id": "2506.01732", "pdf": "https://arxiv.org/pdf/2506.01732", "abs": "https://arxiv.org/abs/2506.01732", "authors": ["Pierre-Carl Langlais", "Carlos Rosas Hinostroza", "Mattia Nee", "Catherine Arnett", "Pavel Chizhov", "Eliot Krzystof Jones", "Ir\u00e8ne Girard", "David Mach", "Anastasia Stasenko", "Ivan P. Yamshchikov"], "title": "Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are pre-trained on large amounts of data from\ndifferent sources and domains. These data most often contain trillions of\ntokens with large portions of copyrighted or proprietary content, which hinders\nthe usage of such models under AI legislation. This raises the need for truly\nopen pre-training data that is compliant with the data security regulations. In\nthis paper, we introduce Common Corpus, the largest open dataset for language\nmodel pre-training. The data assembled in Common Corpus are either\nuncopyrighted or under permissible licenses and amount to about two trillion\ntokens. The dataset contains a wide variety of languages, ranging from the main\nEuropean languages to low-resource ones rarely present in pre-training\ndatasets; in addition, it includes a large portion of code data. The diversity\nof data sources in terms of covered domains and time periods opens up the paths\nfor both research and entrepreneurial needs in diverse areas of knowledge. In\nthis technical report, we present the detailed provenance of data assembling\nand the details of dataset filtering and curation. Being already used by such\nindustry leaders as Anthropic and multiple LLM training projects, we believe\nthat Common Corpus will become a critical infrastructure for open science\nresearch in LLMs.", "AI": {"tldr": "Common Corpus\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u3001\u5408\u89c4\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ea6\u4e24\u4e07\u4ebf\u4e2a\u6765\u81ea\u516c\u5171\u9886\u57df\u6216\u5141\u8bb8\u4f7f\u7528\u7684\u4ee4\u724c\uff0c\u6db5\u76d6\u591a\u79cd\u8bed\u8a00\u548c\u4ee3\u7801\u6570\u636e\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u9884\u8bad\u7ec3\u6570\u636e\u7248\u6743\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLMs\u9884\u8bad\u7ec3\u6570\u636e\u5e38\u6d89\u53ca\u7248\u6743\u6216\u4e13\u6709\u5185\u5bb9\uff0c\u9650\u5236\u4e86\u5176\u5728AI\u6cd5\u89c4\u4e0b\u7684\u4f7f\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u5b8c\u5168\u5f00\u653e\u4e14\u5408\u89c4\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u516c\u5171\u9886\u57df\u6216\u5141\u8bb8\u4f7f\u7528\u7684\u6570\u636e\uff0c\u7ec4\u88c5\u6210Common Corpus\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u79cd\u8bed\u8a00\u548c\u4ee3\u7801\u6570\u636e\uff0c\u5e76\u8be6\u7ec6\u63cf\u8ff0\u4e86\u6570\u636e\u6765\u6e90\u3001\u8fc7\u6ee4\u548c\u6574\u7406\u8fc7\u7a0b\u3002", "result": "Common Corpus\u5df2\u6210\u4e3a\u884c\u4e1a\u9886\u5bfc\u8005\u4f7f\u7528\u7684\u6570\u636e\u96c6\uff0c\u88ab\u8ba4\u4e3a\u662f\u5f00\u653e\u79d1\u5b66\u7814\u7a76LLM\u7684\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u3002", "conclusion": "Common Corpus\u4e3a\u89e3\u51b3\u9884\u8bad\u7ec3\u6570\u636e\u7248\u6743\u95ee\u9898\u63d0\u4f9b\u4e86\u5f00\u653e\u4e14\u5408\u89c4\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4fc3\u8fdb\u4e86LLM\u7814\u7a76\u548c\u5e94\u7528\u7684\u53d1\u5c55\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u5f00\u653e\u6570\u636e\u96c6,\u9884\u8bad\u7ec3\u6570\u636e,Common Corpus,\u6570\u636e\u5408\u89c4"}}
{"id": "2506.01614", "pdf": "https://arxiv.org/pdf/2506.01614", "abs": "https://arxiv.org/abs/2506.01614", "authors": ["Hamid Attar", "Luigi Lunardon", "Alessio Pagani"], "title": "Contrastive Learning for Efficient Transaction Validation in UTXO-based Blockchains", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, 5 figures, 3 tables", "summary": "This paper introduces a Machine Learning (ML) approach for scalability of\nUTXO-based blockchains, such as Bitcoin. Prior approaches to UTXO set sharding\nstruggle with distributing UTXOs effectively across validators, creating\nsubstantial communication overhead due to child-parent transaction\ndependencies. This overhead, which arises from the need to locate parent UTXOs,\nsignificantly hampers transaction processing speeds. Our solution uses ML to\noptimize not only UTXO set sharding but also the routing of incoming\ntransactions, ensuring that transactions are directed to shards containing\ntheir parent UTXOs. At the heart of our approach is a framework that combines\ncontrastive and unsupervised learning to create an embedding space for\ntransaction outputs. This embedding allows the model to group transaction\noutputs based on spending relationships, making it possible to route\ntransactions efficiently to the correct validation microservices. Trained on\nhistorical transaction data with triplet loss and online semi-hard negative\nmining, the model embeds parent-child spending patterns directly into its\nparameters, thus eliminating the need for costly, real-time parent transaction\nlookups. This significantly reduces cross-shard communication overhead,\nboosting throughput and scalability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684UTXO\u533a\u5757\u94fe\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u5206\u7247\u548c\u4ea4\u6613\u8def\u7531\uff0c\u51cf\u5c11\u8de8\u5206\u7247\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u73b0\u6709UTXO\u5206\u7247\u65b9\u6cd5\u56e0\u4ea4\u6613\u4f9d\u8d56\u5173\u7cfb\u5bfc\u81f4\u901a\u4fe1\u5f00\u9500\u5927\uff0c\u5f71\u54cd\u4ea4\u6613\u5904\u7406\u901f\u5ea6\u3002", "method": "\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u6784\u5efa\u4ea4\u6613\u8f93\u51fa\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u5229\u7528\u5386\u53f2\u6570\u636e\u8bad\u7ec3\u6a21\u578b\uff0c\u51cf\u5c11\u5b9e\u65f6\u67e5\u8be2\u9700\u6c42\u3002", "result": "\u663e\u8457\u964d\u4f4e\u4e86\u8de8\u5206\u7247\u901a\u4fe1\u5f00\u9500\uff0c\u63d0\u9ad8\u4e86\u4ea4\u6613\u541e\u5410\u91cf\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u53ef\u6709\u6548\u4f18\u5316UTXO\u533a\u5757\u94fe\u7684\u5206\u7247\u548c\u4ea4\u6613\u8def\u7531\u3002", "keywords": "\u673a\u5668\u5b66\u4e60, UTXO, \u533a\u5757\u94fe\u6269\u5c55, \u5206\u7247, \u4ea4\u6613\u8def\u7531"}}
{"id": "2506.01734", "pdf": "https://arxiv.org/pdf/2506.01734", "abs": "https://arxiv.org/abs/2506.01734", "authors": ["Jiandong Shao", "Yao Lu", "Jianfei Yang"], "title": "Benford's Curse: Tracing Digit Bias to Numerical Hallucination in LLMs", "categories": ["cs.CL"], "comment": "Under Review", "summary": "Large Language Models (LLMs) exhibit impressive performance on complex\nreasoning tasks, yet they frequently fail on basic numerical problems,\nproducing incorrect outputs. Inspired by Benford's Law -- a statistical pattern\nwhere lower digits occur more frequently as leading digits -- we hypothesize\nthat the long-tailed digit distributions in web-collected corpora may be\nlearned by LLMs during pretraining, leading to biased numerical generation. To\ninvestigate the hypothesis, we first examine whether digits frequencies in\npretraining corpus (OLMo2) follows Benford's law. We then construct an\nevaluation benchmark with uniformly distributed ground-truth digits across\nseven numerical reasoning tasks. Our evaluation results demonstrate that\nleading open-source LLMs show a consistent pattern of digit bias that resembles\nBenford's law. Through logit-lens tracing and neuron-level dissection, we\nidentify that this bias arises predominantly from a small subset of highly\ndigit-selective feed-forward network (FFN) neurons in the deeper layers.\nFinally, we demonstrate that pruning these neurons mitigates imbalanced\novergeneration and partially corrects erroneous outputs, providing causal\nevidence that fine-grained pretraining digit bias can propagate into model\nbehavior. Our findings reveal a fundamental connection between corpus-level\nstatistics and symbolic failure modes in LLMs, offering a new lens for\ndiagnosing and mitigating hallucinations in numerical tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u503c\u4efb\u52a1\u4e2d\u7684\u751f\u6210\u504f\u5dee\u95ee\u9898\uff0c\u53d1\u73b0\u8fd9\u79cd\u504f\u5dee\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u957f\u5c3e\u6570\u5b57\u5206\u5e03\uff08\u7c7b\u4f3c\u4e8e\u672c\u798f\u5fb7\u5b9a\u5f8b\uff09\u6709\u5173\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u522b\u7684\u5206\u6790\u63d0\u51fa\u4e86\u7f13\u89e3\u65b9\u6cd5\u3002", "motivation": "LLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u5728\u57fa\u7840\u6570\u503c\u95ee\u9898\u4e0a\u7ecf\u5e38\u5931\u8d25\u3002\u8bba\u6587\u5047\u8bbe\u8fd9\u79cd\u73b0\u8c61\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u6570\u5b57\u5206\u5e03\u7684\u957f\u5c3e\u7279\u6027\u6709\u5173\u3002", "method": "\u901a\u8fc7\u5206\u6790\u9884\u8bad\u7ec3\u6570\u636e\uff08OLMo2\uff09\u662f\u5426\u7b26\u5408\u672c\u798f\u5fb7\u5b9a\u5f8b\uff0c\u6784\u5efa\u5747\u5300\u5206\u5e03\u7684\u6570\u5b57\u57fa\u51c6\u4efb\u52a1\uff0c\u5e76\u5bf9LLMs\u8fdb\u884c\u795e\u7ecf\u5143\u7ea7\u522b\u7684\u5206\u6790\uff08\u5982logit-lens\u8ffd\u8e2a\u548cFFN\u795e\u7ecf\u5143\u526a\u679d\uff09\u3002", "result": "\u5f00\u6e90LLMs\u8868\u73b0\u51fa\u4e0e\u672c\u798f\u5fb7\u5b9a\u5f8b\u76f8\u4f3c\u7684\u6570\u5b57\u751f\u6210\u504f\u5dee\uff0c\u8fd9\u79cd\u504f\u5dee\u4e3b\u8981\u6e90\u4e8e\u6df1\u5c42\u7f51\u7edc\u4e2d\u5c11\u91cf\u9ad8\u5ea6\u6570\u5b57\u9009\u62e9\u6027\u7684FFN\u795e\u7ecf\u5143\u3002\u526a\u679d\u8fd9\u4e9b\u795e\u7ecf\u5143\u53ef\u4ee5\u90e8\u5206\u7ea0\u6b63\u9519\u8bef\u8f93\u51fa\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u9884\u8bad\u7ec3\u6570\u636e\u7684\u7ec6\u7c92\u5ea6\u6570\u5b57\u504f\u5dee\u4f1a\u4f20\u64ad\u5230\u6a21\u578b\u884c\u4e3a\u4e2d\uff0c\u4e3a\u8bca\u65ad\u548c\u7f13\u89e3\u6570\u503c\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u6570\u503c\u751f\u6210\u504f\u5dee, \u672c\u798f\u5fb7\u5b9a\u5f8b, FFN\u795e\u7ecf\u5143, \u795e\u7ecf\u5143\u526a\u679d"}}
{"id": "2506.01625", "pdf": "https://arxiv.org/pdf/2506.01625", "abs": "https://arxiv.org/abs/2506.01625", "authors": ["Artun Saday", "Ya\u015far Cahit Y\u0131ld\u0131r\u0131m", "Cem Tekin"], "title": "Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We address the problem of Gaussian Process (GP) optimization in the presence\nof unknown and potentially varying adversarial perturbations. Unlike\ntraditional robust optimization approaches that focus on maximizing performance\nunder worst-case scenarios, we consider a robust satisficing objective, where\nthe goal is to consistently achieve a predefined performance threshold $\\tau$,\neven under adversarial conditions. We propose two novel algorithms based on\ndistinct formulations of robust satisficing, and show that they are instances\nof a general robust satisficing framework. Further, each algorithm offers\ndifferent guarantees depending on the nature of the adversary. Specifically, we\nderive two regret bounds: one that is sublinear over time, assuming certain\nconditions on the adversary and the satisficing threshold $\\tau$, and another\nthat scales with the perturbation magnitude but requires no assumptions on the\nadversary. Through extensive experiments, we demonstrate that our approach\noutperforms the established robust optimization methods in achieving the\nsatisficing objective, particularly when the ambiguity set of the robust\noptimization framework is inaccurately specified.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u9c81\u68d2\u6ee1\u8db3\u6027\u76ee\u6807\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u9ad8\u65af\u8fc7\u7a0b\u4f18\u5316\u4e2d\u7684\u672a\u77e5\u5bf9\u6297\u6270\u52a8\u95ee\u9898\uff0c\u4f18\u4e8e\u4f20\u7edf\u9c81\u68d2\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u9ad8\u65af\u8fc7\u7a0b\u4f18\u5316\u4e2d\u672a\u77e5\u5bf9\u6297\u6270\u52a8\u7684\u95ee\u9898\uff0c\u76ee\u6807\u662f\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u6301\u7eed\u6ee1\u8db3\u9884\u8bbe\u6027\u80fd\u9608\u503c\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u9c81\u68d2\u6ee1\u8db3\u6027\u76ee\u6807\u7684\u65b0\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u5b83\u4eec\u662f\u901a\u7528\u9c81\u68d2\u6ee1\u8db3\u6027\u6846\u67b6\u7684\u5b9e\u4f8b\u3002\u5206\u522b\u9002\u7528\u4e8e\u4e0d\u540c\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u4f18\u5316\u3002", "result": "\u63a8\u5bfc\u51fa\u4e24\u79cd\u9057\u61be\u754c\u9650\uff1a\u4e00\u79cd\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5177\u6709\u6b21\u7ebf\u6027\u65f6\u95f4\u6027\u80fd\uff0c\u53e6\u4e00\u79cd\u5219\u4e0e\u6270\u52a8\u5e45\u5ea6\u76f8\u5173\u4f46\u65e0\u9700\u5bf9\u6297\u6027\u5047\u8bbe\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6ee1\u8db3\u6027\u76ee\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u9c81\u68d2\u4f18\u5316\u3002", "conclusion": "\u8bba\u6587\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u9c81\u68d2\u6ee1\u8db3\u6027\u65b9\u6cd5\u5728\u5bf9\u6297\u73af\u5883\u4e0b\u5177\u6709\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u5f53\u4f20\u7edf\u9c81\u68d2\u4f18\u5316\u7684\u6a21\u7cca\u96c6\u4e0d\u51c6\u786e\u65f6\u6548\u679c\u66f4\u663e\u8457\u3002", "keywords": "\u9ad8\u65af\u8fc7\u7a0b\u4f18\u5316, \u5bf9\u6297\u6270\u52a8, \u9c81\u68d2\u6ee1\u8db3\u6027, \u9057\u61be\u754c\u9650"}}
{"id": "2506.01748", "pdf": "https://arxiv.org/pdf/2506.01748", "abs": "https://arxiv.org/abs/2506.01748", "authors": ["Yihong Tang", "Kehai Chen", "Muyun Yang", "Zhengyu Niu", "Jing Li", "Tiejun Zhao", "Min Zhang"], "title": "Thinking in Character: Advancing Role-Playing Agents with Role-Aware Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "The advancement of Large Language Models (LLMs) has spurred significant\ninterest in Role-Playing Agents (RPAs) for applications such as emotional\ncompanionship and virtual interaction. However, recent RPAs are often built on\nexplicit dialogue data, lacking deep, human-like internal thought processes,\nresulting in superficial knowledge and style expression. While Large Reasoning\nModels (LRMs) can be employed to simulate character thought, their direct\napplication is hindered by attention diversion (i.e., RPAs forget their role)\nand style drift (i.e., overly formal and rigid reasoning rather than\ncharacter-consistent reasoning). To address these challenges, this paper\nintroduces a novel Role-Aware Reasoning (RAR) method, which consists of two\nimportant stages: Role Identity Activation (RIA) and Reasoning Style\nOptimization (RSO). RIA explicitly guides the model with character profiles\nduring reasoning to counteract attention diversion, and then RSO aligns\nreasoning style with the character and scene via LRM distillation to mitigate\nstyle drift. Extensive experiments demonstrate that the proposed RAR\nsignificantly enhances the performance of RPAs by effectively addressing\nattention diversion and style drift.", "AI": {"tldr": "\u9488\u5bf9\u89d2\u8272\u626e\u6f14\u4ee3\u7406\uff08RPAs\uff09\u5728\u6a21\u62df\u4eba\u7c7b\u6df1\u5c42\u601d\u7ef4\u548c\u98ce\u683c\u8868\u8fbe\u4e0a\u7684\u4e0d\u8db3\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u89d2\u8272\u611f\u77e5\u63a8\u7406\uff08RAR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u89d2\u8272\u8eab\u4efd\u6fc0\u6d3b\uff08RIA\uff09\u548c\u63a8\u7406\u98ce\u683c\u4f18\u5316\uff08RSO\uff09\u4e24\u9636\u6bb5\u89e3\u51b3\u6ce8\u610f\u529b\u5206\u6563\u548c\u98ce\u683c\u6f02\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347RPA\u8868\u73b0\u3002", "motivation": "\u5f53\u524dRPAs\u57fa\u4e8e\u663e\u6027\u5bf9\u8bdd\u6570\u636e\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u6df1\u5c42\u601d\u7ef4\uff0c\u5bfc\u81f4\u77e5\u8bc6\u8868\u8fbe\u6d45\u8584\uff0c\u4e14\u76f4\u63a5\u5e94\u7528\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u4f1a\u5f15\u53d1\u6ce8\u610f\u529b\u5206\u6563\u548c\u98ce\u683c\u6f02\u79fb\u3002", "method": "\u63d0\u51faRAR\u65b9\u6cd5\uff0c\u5305\u542bRIA\uff08\u901a\u8fc7\u89d2\u8272\u914d\u7f6e\u6587\u4ef6\u6307\u5bfc\u63a8\u7406\u4ee5\u5bf9\u6297\u6ce8\u610f\u529b\u5206\u6563\uff09\u548cRSO\uff08\u901a\u8fc7LRM\u84b8\u998f\u4f18\u5316\u63a8\u7406\u98ce\u683c\u4ee5\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRAR\u6709\u6548\u89e3\u51b3\u4e86\u6ce8\u610f\u529b\u5206\u6563\u548c\u98ce\u683c\u6f02\u79fb\uff0c\u663e\u8457\u63d0\u5347\u4e86RPAs\u7684\u8868\u73b0\u3002", "conclusion": "RAR\u65b9\u6cd5\u4e3aRPAs\u63d0\u4f9b\u4e86\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u63a8\u7406\u548c\u98ce\u683c\u8868\u8fbe\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u6280\u672f\u7684\u6838\u5fc3\u95ee\u9898\u3002", "keywords": "Large Language Models, Role-Playing Agents, Role-Aware Reasoning, Attention Diversion, Style Drift"}}
{"id": "2506.01631", "pdf": "https://arxiv.org/pdf/2506.01631", "abs": "https://arxiv.org/abs/2506.01631", "authors": ["Zehao Wu", "Yanjie Zhao", "Haoyu Wang"], "title": "Gradient-Based Model Fingerprinting for LLM Similarity Detection and Family Classification", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": null, "summary": "As Large Language Models (LLMs) become integral software components in modern\napplications, unauthorized model derivations through fine-tuning, merging, and\nredistribution have emerged as critical software engineering challenges. Unlike\ntraditional software where clone detection and license compliance are\nwell-established, the LLM ecosystem lacks effective mechanisms to detect model\nlineage and enforce licensing agreements. This gap is particularly problematic\nwhen open-source model creators, such as Meta's LLaMA, require derivative works\nto maintain naming conventions for attribution, yet no technical means exist to\nverify compliance.\n  To fill this gap, treating LLMs as software artifacts requiring provenance\ntracking, we present TensorGuard, a gradient-based fingerprinting framework for\nLLM similarity detection and family classification. Our approach extracts\nmodel-intrinsic behavioral signatures by analyzing gradient responses to random\ninput perturbations across tensor layers, operating independently of training\ndata, watermarks, or specific model formats. TensorGuard supports the\nwidely-adopted safetensors format and constructs high-dimensional fingerprints\nthrough statistical analysis of gradient features. These fingerprints enable\ntwo complementary capabilities: direct pairwise similarity assessment between\narbitrary models through distance computation, and systematic family\nclassification of unknown models via the K-Means clustering algorithm with\ndomain-informed centroid initialization using known base models. Experimental\nevaluation on 58 models comprising 8 base models and 50 derivatives across five\nmodel families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94%\nclassification accuracy under our centroid-initialized K-Means clustering.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86TensorGuard\uff0c\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u6307\u7eb9\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4bLLM\u7684\u76f8\u4f3c\u6027\u548c\u5bb6\u65cf\u5206\u7c7b\uff0c\u89e3\u51b3\u4e86LLM\u6a21\u578b\u884d\u751f\u548c\u7248\u6743\u5408\u89c4\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740LLM\u6210\u4e3a\u73b0\u4ee3\u5e94\u7528\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u884d\u751f\uff08\u5982\u5fae\u8c03\u548c\u91cd\u65b0\u5206\u53d1\uff09\u6210\u4e3a\u91cd\u8981\u6311\u6218\u3002\u5f00\u6e90\u6a21\u578b\uff08\u5982LLaMA\uff09\u8981\u6c42\u884d\u751f\u6a21\u578b\u9075\u5b88\u547d\u540d\u89c4\u8303\uff0c\u4f46\u7f3a\u4e4f\u6280\u672f\u624b\u6bb5\u9a8c\u8bc1\u5408\u89c4\u6027\u3002", "method": "\u63d0\u51faTensorGuard\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u68af\u5ea6\u54cd\u5e94\u63d0\u53d6\u6a21\u578b\u56fa\u6709\u884c\u4e3a\u7279\u5f81\uff0c\u72ec\u7acb\u4e8e\u8bad\u7ec3\u6570\u636e\u3001\u6c34\u5370\u6216\u7279\u5b9a\u6a21\u578b\u683c\u5f0f\uff0c\u652f\u6301safetensors\u683c\u5f0f\uff0c\u5e76\u5229\u7528K-Means\u805a\u7c7b\u8fdb\u884c\u5bb6\u65cf\u5206\u7c7b\u3002", "result": "\u572858\u4e2a\u6a21\u578b\uff08\u5305\u62ec8\u4e2a\u57fa\u7840\u6a21\u578b\u548c50\u4e2a\u884d\u751f\u6a21\u578b\uff09\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe94%\u3002", "conclusion": "TensorGuard\u4e3aLLM\u7684\u6eaf\u6e90\u548c\u7248\u6743\u5408\u89c4\u63d0\u4f9b\u4e86\u6709\u6548\u6280\u672f\u624b\u6bb5\u3002", "keywords": "Large Language Models, provenance tracking, gradient-based fingerprinting, family classification, safetensors"}}
{"id": "2506.01775", "pdf": "https://arxiv.org/pdf/2506.01775", "abs": "https://arxiv.org/abs/2506.01775", "authors": ["Milind Agarwal", "Daisy Rosenblum", "Antonios Anastasopoulos"], "title": "Developing a Mixed-Methods Pipeline for Community-Oriented Digitization of Kwak'wala Legacy Texts", "categories": ["cs.CL"], "comment": "Accepted to Comput-EL 2025 Workshop. Preprint", "summary": "Kwak'wala is an Indigenous language spoken in British Columbia, with a rich\nlegacy of published documentation spanning more than a century, and an active\ncommunity of speakers, teachers, and learners engaged in language\nrevitalization. Over 11 volumes of the earliest texts created during the\ncollaboration between Franz Boas and George Hunt have been scanned but remain\nunreadable by machines. Complete digitization through optical character\nrecognition has the potential to facilitate transliteration into modern\northographies and the creation of other language technologies. In this paper,\nwe apply the latest OCR techniques to a series of Kwak'wala texts only\naccessible as images, and discuss the challenges and unique adaptations\nnecessary to make such technologies work for these real-world texts. Building\non previous methods, we propose using a mix of off-the-shelf OCR methods,\nlanguage identification, and masking to effectively isolate Kwak'wala text,\nalong with post-correction models, to produce a final high-quality\ntranscription.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u6700\u65b0OCR\u6280\u672f\u5c06Kwak'wala\u8bed\u8a00\u7684\u5386\u53f2\u6587\u672c\u6570\u5b57\u5316\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed3\u5408\u73b0\u6709OCR\u5de5\u5177\u3001\u8bed\u8a00\u8bc6\u522b\u548c\u6821\u5bf9\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "motivation": "Kwak'wala\u662f\u4e00\u79cd\u6fd2\u5371\u8bed\u8a00\uff0c\u5df2\u6709\u5927\u91cf\u6587\u672c\u56e0\u673a\u5668\u4e0d\u53ef\u8bfb\u800c\u65e0\u6cd5\u5145\u5206\u5229\u7528\uff0c\u6570\u5b57\u5316\u8fd9\u4e9b\u6587\u672c\u6709\u52a9\u4e8e\u8bed\u8a00\u632f\u5174\u548c\u6280\u672f\u5f00\u53d1\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u5305\u62ec\u73b0\u6210OCR\u5de5\u5177\u3001\u8bed\u8a00\u8bc6\u522b\u3001\u63a9\u7801\u6280\u672f\u9694\u79bb\u6587\u672c\uff0c\u4ee5\u53ca\u540e\u6821\u5bf9\u6a21\u578b\u63d0\u9ad8\u8f6c\u5f55\u8d28\u91cf\u3002", "result": "\u6210\u529f\u5c06\u56fe\u50cf\u4e2d\u7684Kwak'wala\u6587\u672c\u8f6c\u5f55\u4e3a\u53ef\u8bfb\u7684\u73b0\u4ee3\u62fc\u5199\u5f62\u5f0f\uff0c\u89e3\u51b3\u4e86\u6280\u672f\u6311\u6218\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5176\u4ed6\u6fd2\u5371\u8bed\u8a00\u7684\u6587\u672c\u6570\u5b57\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "keywords": "Kwak'wala, OCR, \u8bed\u8a00\u632f\u5174, \u6570\u5b57\u5316, \u6587\u672c\u8f6c\u5f55"}}
{"id": "2506.00607", "pdf": "https://arxiv.org/pdf/2506.00607", "abs": "https://arxiv.org/abs/2506.00607", "authors": ["JungWoo Chae", "Jiyoon Kim", "Sangheum Hwang"], "title": "Parallel Rescaling: Rebalancing Consistency Guidance for Personalized Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Personalizing diffusion models to specific users or concepts remains\nchallenging, particularly when only a few reference images are available.\nExisting methods such as DreamBooth and Textual Inversion often overfit to\nlimited data, causing misalignment between generated images and text prompts\nwhen attempting to balance identity fidelity with prompt adherence. While\nDirect Consistency Optimization (DCO) with its consistency-guided sampling\npartially alleviates this issue, it still struggles with complex or stylized\nprompts. In this paper, we propose a parallel rescaling technique for\npersonalized diffusion models. Our approach explicitly decomposes the\nconsistency guidance signal into parallel and orthogonal components relative to\nclassifier free guidance (CFG). By rescaling the parallel component, we\nminimize disruptive interference with CFG while preserving the subject's\nidentity. Unlike prior personalization methods, our technique does not require\nadditional training data or expensive annotations. Extensive experiments show\nimproved prompt alignment and visual fidelity compared to baseline methods,\neven on challenging stylized prompts. These findings highlight the potential of\nparallel rescaled guidance to yield more stable and accurate personalization\nfor diverse user inputs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e76\u884c\u91cd\u65b0\u7f29\u653e\u6280\u672f\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u89e3\u4e00\u81f4\u6027\u5f15\u5bfc\u4fe1\u53f7\u4e3a\u5e73\u884c\u548c\u6b63\u4ea4\u5206\u91cf\uff0c\u4f18\u5316\u751f\u6210\u56fe\u50cf\u7684\u63d0\u793a\u5bf9\u9f50\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u9650\u6570\u636e\u4e0b\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u5bfc\u81f4\u751f\u6210\u56fe\u50cf\u4e0e\u6587\u672c\u63d0\u793a\u4e0d\u5339\u914d\uff0c\u5c24\u5176\u662f\u590d\u6742\u6216\u98ce\u683c\u5316\u63d0\u793a\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u5e76\u884c\u91cd\u65b0\u7f29\u653e\u6280\u672f\uff0c\u5c06\u4e00\u81f4\u6027\u5f15\u5bfc\u4fe1\u53f7\u5206\u89e3\u4e3a\u5e73\u884c\u548c\u6b63\u4ea4\u5206\u91cf\uff0c\u91cd\u65b0\u8c03\u6574\u5e73\u884c\u5206\u91cf\u4ee5\u51cf\u5c11\u5bf9\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u7684\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u63d0\u793a\u4e0b\u4ecd\u80fd\u4fdd\u6301\u66f4\u597d\u7684\u63d0\u793a\u5bf9\u9f50\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u5e76\u884c\u91cd\u65b0\u7f29\u653e\u6280\u672f\u4e3a\u6269\u6563\u6a21\u578b\u7684\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u548c\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u4e2a\u6027\u5316\u6269\u6563\u6a21\u578b\u3001\u5e76\u884c\u91cd\u65b0\u7f29\u653e\u3001\u63d0\u793a\u5bf9\u9f50\u3001\u89c6\u89c9\u4fdd\u771f\u5ea6"}}
{"id": "2506.01639", "pdf": "https://arxiv.org/pdf/2506.01639", "abs": "https://arxiv.org/abs/2506.01639", "authors": ["Yixian Zhang", "Huaze Tang", "Changxu Wei", "Wenbo Ding"], "title": "Bidirectional Soft Actor-Critic: Leveraging Forward and Reverse KL Divergence for Efficient Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The Soft Actor-Critic (SAC) algorithm, a state-of-the-art method in maximum\nentropy reinforcement learning, traditionally relies on minimizing reverse\nKullback-Leibler (KL) divergence for policy updates. However, this approach\nleads to an intractable optimal projection policy, necessitating gradient-based\napproximations that can suffer from instability and poor sample efficiency.\nThis paper investigates the alternative use of forward KL divergence within\nSAC. We demonstrate that for Gaussian policies, forward KL divergence yields an\nexplicit optimal projection policy -- corresponding to the mean and variance of\nthe target Boltzmann distribution's action marginals. Building on the distinct\nadvantages of both KL directions, we propose Bidirectional SAC, an algorithm\nthat first initializes the policy using the explicit forward KL projection and\nthen refines it by optimizing the reverse KL divergence. Comprehensive\nexperiments on continuous control benchmarks show that Bidirectional SAC\nsignificantly outperforms standard SAC and other baselines, achieving up to a\n$30\\%$ increase in episodic rewards, alongside enhanced sample efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u5411KL\u6563\u5ea6\u7684SAC\u7b97\u6cd5\uff08Bidirectional SAC\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u6b63\u5411\u548c\u53cd\u5411KL\u6563\u5ea6\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u4f20\u7edfSAC\u7b97\u6cd5\u4f7f\u7528\u53cd\u5411KL\u6563\u5ea6\u8fdb\u884c\u7b56\u7565\u66f4\u65b0\uff0c\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6027\u548c\u6837\u672c\u6548\u7387\u4f4e\u3002\u672c\u6587\u7814\u7a76\u6b63\u5411KL\u6563\u5ea6\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faBidirectional SAC\u7b97\u6cd5\uff0c\u5148\u7528\u6b63\u5411KL\u6563\u5ea6\u521d\u59cb\u5316\u7b56\u7565\uff0c\u518d\u7528\u53cd\u5411KL\u6563\u5ea6\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cBidirectional SAC\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6SAC\u548c\u5176\u4ed6\u57fa\u7ebf\uff0c\u5956\u52b1\u63d0\u5347\u8fbe30%\u3002", "conclusion": "\u7ed3\u5408\u53cc\u5411KL\u6563\u5ea6\u7684SAC\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\u3002", "keywords": "SAC, KL divergence, reinforcement learning, continuous control"}}
{"id": "2506.01776", "pdf": "https://arxiv.org/pdf/2506.01776", "abs": "https://arxiv.org/abs/2506.01776", "authors": ["Yile Liu", "Ziwei Ma", "Xiu Jiang", "Jinglu Hu", "Jing Chang", "Liang Li"], "title": "MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "With the rapid adoption of large language models (LLMs) in natural language\nprocessing, the ability to follow instructions has emerged as a key metric for\nevaluating their practical utility. However, existing evaluation methods often\nfocus on single-language scenarios, overlooking the challenges and differences\npresent in multilingual and cross-lingual contexts. To address this gap, we\nintroduce MaXIFE: a comprehensive evaluation benchmark designed to assess\ninstruction-following capabilities across 23 languages with 1,667 verifiable\ninstruction tasks. MaXIFE integrates both Rule-Based Evaluation and Model-Based\nEvaluation, ensuring a balance of efficiency and accuracy. We applied MaXIFE to\nevaluate several leading commercial and open-source LLMs, establishing baseline\nresults for future comparisons. By providing a standardized tool for\nmultilingual instruction-following evaluation, MaXIFE aims to advance research\nand development in natural language processing.", "AI": {"tldr": "MaXIFE\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u6307\u4ee4\u8ddf\u968f\u8bc4\u4f30\u57fa\u51c6\uff0c\u8986\u76d623\u79cd\u8bed\u8a00\u548c1,667\u4e2a\u53ef\u9a8c\u8bc1\u4efb\u52a1\uff0c\u65e8\u5728\u586b\u8865\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u573a\u666f\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u5e94\u7528\u9700\u8981\u8bc4\u4f30\u5176\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u591a\u96c6\u4e2d\u5728\u5355\u8bed\u573a\u666f\uff0c\u5ffd\u89c6\u591a\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u6311\u6218\u3002", "method": "MaXIFE\u7ed3\u5408\u4e86\u57fa\u4e8e\u89c4\u5219\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u517c\u987e\u6548\u7387\u4e0e\u51c6\u786e\u6027\uff0c\u5e76\u7528\u4e8e\u8bc4\u4f30\u591a\u4e2a\u5546\u4e1a\u548c\u5f00\u6e90LLMs\u3002", "result": "\u901a\u8fc7MaXIFE\u8bc4\u4f30\uff0c\u786e\u7acb\u4e86\u672a\u6765\u6bd4\u8f83\u7684\u57fa\u7ebf\u7ed3\u679c\uff0c\u5c55\u793a\u5176\u5728\u591a\u8bed\u8a00\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u8bc4\u4f30\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "MaXIFE\u4e3a\u7814\u7a76\u548c\u53d1\u5c55\u81ea\u7136\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u591a\u8bed\u8a00\u6307\u4ee4\u8bc4\u4f30\u5de5\u5177\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u6307\u4ee4\u8ddf\u968f,\u591a\u8bed\u8a00\u8bc4\u4f30,MaXIFE,\u81ea\u7136\u8bed\u8a00\u5904\u7406"}}
{"id": "2506.00613", "pdf": "https://arxiv.org/pdf/2506.00613", "abs": "https://arxiv.org/abs/2506.00613", "authors": ["Julian Quevedo", "Percy Liang", "Sherry Yang"], "title": "Evaluating Robot Policies in a World Model", "categories": ["cs.RO", "cs.AI"], "comment": "https://world-model-eval.github.io", "summary": "Robotics has broad applications from automating house chores to taking care\nof patients. However, evaluating robot control policies is challenging, as\nreal-world testing is expensive, while handcrafted simulations often fail to\naccurately reflect real-world conditions, resulting in poor correlation between\nsimulated evaluation and real-world outcomes. In this work, we investigate\nWorld-model-based Policy Evaluation (WPE). We first train an action-conditioned\nvideo generation model as a proxy to real-world environments. To enable\nefficient rollouts of hundreds of interactive steps while mitigating error\naccumulation in the world model, we propose an inference scheme which we call\nBlockwise-Autoregressive Diffusion Transformer with adjustable context and\ndecoding horizon lengths. To ensure that the world model indeed follows action\ninput, we propose metrics based on the agreement between the ground truth video\nand generated video conditioned on the same sequence of actions to evaluate the\nworld model. We then use the world model for policy evaluation by performing\nMonte Carlo rollouts in the world model while employing a vision-language model\n(VLM) as a reward function. Interestingly, we found that WPE tends to\nunderestimate the policy values for in-distribution actions and overestimate\npolicy values for out-of-distribution actions. Nevertheless, WPE preserves the\nrelative rankings of different policies. In emulating real robot executions,\nWPE achieves high fidelity in mimicing robot arm movements as in real videos,\nwhile emulating highly realistic object interaction remains challenging.\nDespite this limitation, we show that a world model can serve as a starting\npoint for evaluating robot policies before real-world deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u7b56\u7565\u8bc4\u4f30\u65b9\u6cd5\uff08WPE\uff09\uff0c\u901a\u8fc7\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u6a21\u578b\u6a21\u62df\u771f\u5b9e\u73af\u5883\uff0c\u7ed3\u5408\u63a8\u7406\u65b9\u6848\u548c\u5956\u52b1\u51fd\u6570\uff0c\u5b9e\u73b0\u5bf9\u673a\u5668\u4eba\u7b56\u7565\u7684\u6709\u6548\u8bc4\u4f30\uff0c\u5c3d\u7ba1\u5bf9\u5206\u5e03\u5916\u884c\u4e3a\u7684\u8bc4\u4f30\u5b58\u5728\u504f\u5dee\uff0c\u4f46\u80fd\u4fdd\u6301\u7b56\u7565\u7684\u76f8\u5bf9\u6392\u540d\u3002", "motivation": "\u7531\u4e8e\u771f\u5b9e\u73af\u5883\u6d4b\u8bd5\u6210\u672c\u9ad8\u4e14\u624b\u5de5\u6a21\u62df\u6548\u679c\u4e0d\u4f73\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e00\u79cd\u4f4e\u6210\u672c\u4e14\u76f8\u5bf9\u51c6\u786e\u7684\u673a\u5668\u4eba\u7b56\u7565\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u52a8\u4f5c\u6761\u4ef6\u89c6\u9891\u751f\u6210\u6a21\u578b\u6a21\u62df\u771f\u5b9e\u73af\u5883\uff0c\u63d0\u51fa\u5757\u72b6\u81ea\u56de\u5f52\u6269\u6563\u53d8\u6362\u5668\u4ee5\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\uff0c\u5e76\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4f5c\u4e3a\u5956\u52b1\u51fd\u6570\u8fdb\u884c\u8499\u7279\u5361\u6d1b\u63a8\u6f14\u3002", "result": "WPE\u5728\u8bc4\u4f30\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u884c\u4e3a\u65f6\u5b58\u5728\u9ad8\u4f30\u6216\u4f4e\u4f30\u73b0\u8c61\uff0c\u4f46\u80fd\u4fdd\u6301\u7b56\u7565\u7684\u76f8\u5bf9\u6392\u540d\uff0c\u4e14\u5728\u6a21\u62df\u673a\u68b0\u81c2\u8fd0\u52a8\u65f6\u6548\u679c\u8f83\u597d\u3002", "conclusion": "\u4e16\u754c\u6a21\u578b\u53ef\u4f5c\u4e3a\u771f\u5b9e\u90e8\u7f72\u524d\u7684\u521d\u6b65\u7b56\u7565\u8bc4\u4f30\u5de5\u5177\uff0c\u5c3d\u7ba1\u5728\u67d0\u4e9b\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u4ecd\u6709\u5c40\u9650\u6027\u3002", "keywords": "\u673a\u5668\u4eba\u7b56\u7565\u8bc4\u4f30, \u4e16\u754c\u6a21\u578b, \u89c6\u9891\u751f\u6210, \u8499\u7279\u5361\u6d1b\u63a8\u6f14, \u89c6\u89c9\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.01656", "pdf": "https://arxiv.org/pdf/2506.01656", "abs": "https://arxiv.org/abs/2506.01656", "authors": ["Ryotaro Kawata", "Kohsei Matsutani", "Yuri Kinoshita", "Naoki Nishikawa", "Taiji Suzuki"], "title": "Mixture of Experts Provably Detect and Learn the Latent Cluster Structure in Gradient-Based Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Mixture of Experts (MoE), an ensemble of specialized models equipped with a\nrouter that dynamically distributes each input to appropriate experts, has\nachieved successful results in the field of machine learning. However,\ntheoretical understanding of this architecture is falling behind due to its\ninherent complexity. In this paper, we theoretically study the sample and\nruntime complexity of MoE following the stochastic gradient descent (SGD) when\nlearning a regression task with an underlying cluster structure of single index\nmodels. On the one hand, we prove that a vanilla neural network fails in\ndetecting such a latent organization as it can only process the problem as a\nwhole. This is intrinsically related to the concept of information exponent\nwhich is low for each cluster, but increases when we consider the entire task.\nOn the other hand, we show that a MoE succeeds in dividing this problem into\neasier subproblems by leveraging the ability of each expert to weakly recover\nthe simpler function corresponding to an individual cluster. To the best of our\nknowledge, this work is among the first to explore the benefits of the MoE\nframework by examining its SGD dynamics in the context of nonlinear regression.", "AI": {"tldr": "MoE\u67b6\u6784\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u590d\u6742\u6027\u5bfc\u81f4\u7406\u8bba\u7406\u89e3\u6ede\u540e\u3002\u672c\u6587\u901a\u8fc7SGD\u7814\u7a76\u4e86MoE\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u6837\u672c\u548c\u8fd0\u884c\u65f6\u590d\u6742\u5ea6\uff0c\u53d1\u73b0MoE\u80fd\u6210\u529f\u5206\u89e3\u95ee\u9898\uff0c\u800c\u666e\u901a\u795e\u7ecf\u7f51\u7edc\u5219\u65e0\u6cd5\u68c0\u6d4b\u6f5c\u5728\u7ed3\u6784\u3002", "motivation": "\u7531\u4e8eMoE\u67b6\u6784\u7684\u590d\u6742\u6027\uff0c\u76ee\u524d\u5bf9\u5176\u7406\u8bba\u7406\u89e3\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7SGD\u6df1\u5165\u7814\u7a76MoE\u5728\u975e\u7ebf\u6027\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u52a8\u6001\u7279\u6027\u3002", "method": "\u901a\u8fc7\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u5206\u6790MoE\u5728\u5b66\u4e60\u5177\u6709\u6f5c\u5728\u805a\u7c7b\u7ed3\u6784\u7684\u56de\u5f52\u4efb\u52a1\u65f6\u7684\u6837\u672c\u548c\u8fd0\u884c\u65f6\u590d\u6742\u5ea6\u3002", "result": "\u666e\u901a\u795e\u7ecf\u7f51\u7edc\u65e0\u6cd5\u68c0\u6d4b\u6f5c\u5728\u7ed3\u6784\uff0c\u800cMoE\u80fd\u6210\u529f\u5206\u89e3\u95ee\u9898\uff0c\u5404\u4e13\u5bb6\u80fd\u5f31\u6062\u590d\u5355\u4e2a\u805a\u7c7b\u7684\u7b80\u5355\u51fd\u6570\u3002", "conclusion": "MoE\u5728\u5206\u89e3\u590d\u6742\u95ee\u9898\u4e3a\u66f4\u7b80\u5355\u7684\u5b50\u95ee\u9898\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u8fd9\u662f\u5176\u6210\u529f\u7684\u5173\u952e\u3002", "keywords": "Mixture of Experts, SGD, nonlinear regression, computational complexity, neural networks"}}
{"id": "2506.01784", "pdf": "https://arxiv.org/pdf/2506.01784", "abs": "https://arxiv.org/abs/2506.01784", "authors": ["Shuai Wang", "Yinan Yu"], "title": "iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Main)", "summary": "While Large Language Models (LLMs) excel at many natural language processing\ntasks, they often suffer from factual inaccuracies in knowledge-intensive\nscenarios. Integrating external knowledge resources, particularly knowledge\ngraphs (KGs), provides a transparent and updatable foundation for more reliable\nreasoning. Knowledge Base Question Answering (KBQA), which queries and reasons\nover KGs, is central to this effort, especially for complex, multi-hop queries.\nHowever, multi-hop reasoning poses two key challenges: (1)~maintaining coherent\nreasoning paths, and (2)~avoiding prematurely discarding critical multi-hop\nconnections. To address these issues, we introduce iQUEST, a question-guided\nKBQA framework that iteratively decomposes complex queries into simpler\nsub-questions, ensuring a structured and focused reasoning trajectory.\nAdditionally, we integrate a Graph Neural Network (GNN) to look ahead and\nincorporate 2-hop neighbor information at each reasoning step. This dual\napproach strengthens the reasoning process, enabling the model to explore\nviable paths more effectively. Detailed experiments demonstrate the consistent\nimprovement delivered by iQUEST across four benchmark datasets and four LLMs.", "AI": {"tldr": "iQUEST\u901a\u8fc7\u8fed\u4ee3\u5206\u89e3\u590d\u6742\u67e5\u8be2\u5e76\u96c6\u6210GNN\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86KBQA\u4e2d\u7684\u591a\u8df3\u63a8\u7406\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u5b58\u5728\u4e8b\u5b9e\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u800c\u77e5\u8bc6\u56fe\u8c31\u80fd\u63d0\u4f9b\u900f\u660e\u4e14\u53ef\u66f4\u65b0\u7684\u77e5\u8bc6\u652f\u6301\u3002\u591a\u8df3\u63a8\u7406\u7684\u4e24\u5927\u6311\u6218\u662f\u4fdd\u6301\u8fde\u8d2f\u63a8\u7406\u8def\u5f84\u548c\u907f\u514d\u8fc7\u65e9\u4e22\u5f03\u5173\u952e\u8fde\u63a5\u3002", "method": "\u63d0\u51faiQUEST\u6846\u67b6\uff0c\u8fed\u4ee3\u5206\u89e3\u590d\u6742\u67e5\u8be2\u4e3a\u5b50\u95ee\u9898\uff0c\u5e76\u96c6\u6210GNN\u5728\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u4e2d\u524d\u77bb\u6027\u5f15\u51652\u8df3\u90bb\u5c45\u4fe1\u606f\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u56db\u79cdLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0ciQUEST\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e14\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "iQUEST\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8def\u5f84\u548c\u591a\u8df3\u4fe1\u606f\u6574\u5408\uff0c\u4e3aKBQA\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "KBQA, \u591a\u8df3\u63a8\u7406, \u77e5\u8bc6\u56fe\u8c31, GNN, iQUEST"}}
{"id": "2506.01665", "pdf": "https://arxiv.org/pdf/2506.01665", "abs": "https://arxiv.org/abs/2506.01665", "authors": ["Tim Walter", "Hannah Markgraf", "Jonathan K\u00fclz", "Matthias Althoff"], "title": "Provably Safe Reinforcement Learning from Analytic Gradients", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "16 pages, 10 figures", "summary": "Deploying autonomous robots in safety-critical applications requires safety\nguarantees. Provably safe reinforcement learning is an active field of research\nwhich aims to provide such guarantees using safeguards. These safeguards should\nbe integrated during training to prevent a large sim-to-real gap. While there\nare several approaches for safeguarding sampling-based reinforcement learning,\nanalytic gradient-based reinforcement learning often achieves superior\nperformance and sample efficiency. However, there is no safeguarding approach\nfor this learning paradigm yet. Our work addresses this gap by developing the\nfirst effective safeguard for analytic gradient-based reinforcement learning.\nWe analyse existing, differentiable safeguards, adapt them through modified\nmappings and gradient formulations, and integrate them with a state-of-the-art\nlearning algorithm and a differentiable simulation. We evaluate how different\nsafeguards affect policy optimisation using numerical experiments on two\nclassical control tasks. The results demonstrate safeguarded training without\ncompromising performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9996\u6b21\u7528\u4e8e\u5206\u6790\u68af\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6709\u6548\u5b89\u5168\u4fdd\u969c\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u8be5\u5b66\u4e60\u8303\u5f0f\u7684\u5b89\u5168\u7a7a\u767d\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u90e8\u7f72\u81ea\u4e3b\u673a\u5668\u4eba\u9700\u8981\u5b89\u5168\u4fdd\u8bc1\uff0c\u800c\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u5206\u6790\u68af\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u5b89\u5168\u4fdd\u969c\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u548c\u6539\u8fdb\u73b0\u6709\u53ef\u5fae\u5206\u5b89\u5168\u4fdd\u969c\u65b9\u6cd5\uff0c\u5c06\u5176\u4e0e\u6700\u5148\u8fdb\u7684\u5b66\u4e60\u7b97\u6cd5\u548c\u53ef\u5fae\u5206\u6a21\u62df\u76f8\u7ed3\u5408\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u4e0d\u5f71\u54cd\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5b89\u5168\u7684\u8bad\u7ec3\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u586b\u8865\u4e86\u5206\u6790\u68af\u5ea6\u5f3a\u5316\u5b66\u4e60\u9886\u57df\u7684\u5b89\u5168\u4fdd\u969c\u7a7a\u767d\u3002", "keywords": "\u5b89\u5168\u5f3a\u5316\u5b66\u4e60, \u5206\u6790\u68af\u5ea6\u5f3a\u5316\u5b66\u4e60, \u5b89\u5168\u4fdd\u969c, \u53ef\u5fae\u5206\u6a21\u62df"}}
{"id": "2506.01793", "pdf": "https://arxiv.org/pdf/2506.01793", "abs": "https://arxiv.org/abs/2506.01793", "authors": ["Yijin Guo", "Kaiyuan Ji", "Xiaorong Zhu", "Junying Wang", "Farong Wen", "Chunyi Li", "Zicheng Zhang", "Guangtao Zhai"], "title": "Human-Centric Evaluation for Foundation Models", "categories": ["cs.CL"], "comment": null, "summary": "Currently, nearly all evaluations of foundation models focus on objective\nmetrics, emphasizing quiz performance to define model capabilities. While this\nmodel-centric approach enables rapid performance assessment, it fails to\nreflect authentic human experiences. To address this gap, we propose a\nHuman-Centric subjective Evaluation (HCE) framework, focusing on three core\ndimensions: problem-solving ability, information quality, and interaction\nexperience. Through experiments involving Deepseek R1, OpenAI o3 mini, Grok 3,\nand Gemini 2.5, we conduct over 540 participant-driven evaluations, where\nhumans and models collaborate on open-ended research tasks, yielding a\ncomprehensive subjective dataset. This dataset captures diverse user feedback\nacross multiple disciplines, revealing distinct model strengths and\nadaptability. Our findings highlight Grok 3's superior performance, followed by\nDeepseek R1 and Gemini 2.5, with OpenAI o3 mini lagging behind. By offering a\nnovel framework and a rich dataset, this study not only enhances subjective\nevaluation methodologies but also lays the foundation for standardized,\nautomated assessments, advancing LLM development for research and practical\nscenarios. Our dataset link is\nhttps://github.com/yijinguo/Human-Centric-Evaluation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u6846\u67b6\uff08HCE\uff09\uff0c\u901a\u8fc7\u4e3b\u89c2\u7ef4\u5ea6\uff08\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3001\u4fe1\u606f\u8d28\u91cf\u548c\u4ea4\u4e92\u4f53\u9a8c\uff09\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\uff0c\u5f25\u8865\u4e86\u4f20\u7edf\u5ba2\u89c2\u6307\u6807\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u7840\u6a21\u578b\u8bc4\u4f30\u591a\u57fa\u4e8e\u5ba2\u89c2\u6307\u6807\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u7684\u4eba\u7c7b\u4f53\u9a8c\uff0cHCE\u6846\u67b6\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7540\u591a\u540d\u53c2\u4e0e\u8005\u4e0e\u6a21\u578b\uff08Deepseek R1\u3001OpenAI o3 mini\u3001Grok 3\u548cGemini 2.5\uff09\u534f\u4f5c\u5b8c\u6210\u5f00\u653e\u5f0f\u7814\u7a76\u4efb\u52a1\uff0c\u6536\u96c6\u4e3b\u89c2\u53cd\u9988\u6570\u636e\u3002", "result": "Grok 3\u8868\u73b0\u6700\u4f73\uff0c\u5176\u6b21\u662fDeepseek R1\u548cGemini 2.5\uff0cOpenAI o3 mini\u8868\u73b0\u8f83\u5f31\u3002", "conclusion": "HCE\u6846\u67b6\u63d0\u4f9b\u4e86\u4e3b\u89c2\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86LLM\u5728\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u3002", "keywords": "\u57fa\u7840\u6a21\u578b\u3001\u4e3b\u89c2\u8bc4\u4f30\u3001\u4eba\u7c7b\u4e2d\u5fc3\u3001\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3001\u4fe1\u606f\u8d28\u91cf"}}
{"id": "2506.00615", "pdf": "https://arxiv.org/pdf/2506.00615", "abs": "https://arxiv.org/abs/2506.00615", "authors": ["Andreu Ballus Santacana"], "title": "A Topological Semantics of Dialogue: Nerve Structures and Logical Extraction", "categories": ["cs.LO", "cs.AI", "math.AT", "math.LO", "03B05, 55U10, 68T27", "F.4.1; I.2.3; I.2.4"], "comment": "17 pages", "summary": "We introduce a concise, topologically-motivated semantics for finite\ndialogues by mapping each utterance to an open set in a fixed semantic space,\nbuilding the corresponding nerve complex of joint satisfiability, and\nextracting fundamental combinatorial invariants:\n  1. The negative nerve, which enumerates all finite collections of utterances\nwhose\n  opens have empty intersection, providing a straightforward criterion for\nmerging\n  separate transcripts without contradiction.\n  2. The global interpretation subspace, the unique minimal open in which all\nasserted\n  utterances hold simultaneously, enabling effective enumeration of all logical\n  consequences of the entire dialogue.\n  3. A practical demonstration in the Wolfram Language, with algorithms for\nconstructing\n  nerves, detecting inconsistencies, and computing the global interpretation,\nthereby\n  illustrating computational feasibility.\n  Our framework is grounded in classical duality and topological semantics\n(Stone duality, Priestley duality, Tarski's semantics, coherence-space methods,\nScott domains, topos semantics, and homotopy type theory) while drawing on\nrecent advances in topological data analysis and dialogue-based semantics.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u62d3\u6251\u5b66\u7684\u6709\u9650\u5bf9\u8bdd\u8bed\u4e49\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6bcf\u4e2a\u8bdd\u8bed\u6620\u5c04\u5230\u56fa\u5b9a\u8bed\u4e49\u7a7a\u95f4\u4e2d\u7684\u5f00\u96c6\uff0c\u6784\u5efa\u795e\u7ecf\u590d\u5408\u4f53\uff0c\u5e76\u63d0\u53d6\u5173\u952e\u7ec4\u5408\u4e0d\u53d8\u91cf\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8bdd\u7684\u903b\u8f91\u4e00\u81f4\u6027\u5206\u6790\u548c\u5168\u5c40\u89e3\u91ca\u8ba1\u7b97\u3002", "motivation": "\u65e8\u5728\u4e3a\u6709\u9650\u5bf9\u8bdd\u63d0\u4f9b\u4e00\u79cd\u7b80\u6d01\u4e14\u5177\u6709\u62d3\u6251\u5b66\u52a8\u673a\u7684\u8bed\u4e49\uff0c\u4ee5\u652f\u6301\u5bf9\u8bdd\u7684\u903b\u8f91\u4e00\u81f4\u6027\u548c\u5168\u5c40\u89e3\u91ca\u7684\u81ea\u52a8\u5316\u5206\u6790\u3002", "method": "\u5c06\u8bdd\u8bed\u6620\u5c04\u4e3a\u5f00\u96c6\uff0c\u6784\u5efa\u795e\u7ecf\u590d\u5408\u4f53\uff0c\u63d0\u53d6\u8d1f\u795e\u7ecf\u548c\u5168\u5c40\u89e3\u91ca\u5b50\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7Wolfram\u8bed\u8a00\u5b9e\u73b0\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u8d1f\u795e\u7ecf\u548c\u5168\u5c40\u89e3\u91ca\u5b50\u7a7a\u95f4\u7684\u6982\u5ff5\uff0c\u5c55\u793a\u4e86\u8ba1\u7b97\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u7ed3\u5408\u7ecf\u5178\u5bf9\u5076\u6027\u548c\u62d3\u6251\u8bed\u4e49\u5b66\uff0c\u4e3a\u5bf9\u8bdd\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\u3002", "keywords": "\u5bf9\u8bdd\u8bed\u4e49, \u62d3\u6251\u5b66, \u795e\u7ecf\u590d\u5408\u4f53, \u903b\u8f91\u4e00\u81f4\u6027, Wolfram\u8bed\u8a00"}}
{"id": "2506.01672", "pdf": "https://arxiv.org/pdf/2506.01672", "abs": "https://arxiv.org/abs/2506.01672", "authors": ["Shikun Sun", "Min Zhou", "Zixuan Wang", "Xubin Li", "Tiezheng Ge", "Zijie Ye", "Xiaoyu Qin", "Junliang Xing", "Bo Zheng", "Jia Jia"], "title": "Minimal Impact ControlNet: Advancing Multi-ControlNet Integration", "categories": ["cs.LG"], "comment": "ICLR 2025", "summary": "With the advancement of diffusion models, there is a growing demand for\nhigh-quality, controllable image generation, particularly through methods that\nutilize one or multiple control signals based on ControlNet. However, in\ncurrent ControlNet training, each control is designed to influence all areas of\nan image, which can lead to conflicts when different control signals are\nexpected to manage different parts of the image in practical applications. This\nissue is especially pronounced with edge-type control conditions, where regions\nlacking boundary information often represent low-frequency signals, referred to\nas silent control signals. When combining multiple ControlNets, these silent\ncontrol signals can suppress the generation of textures in related areas,\nresulting in suboptimal outcomes. To address this problem, we propose Minimal\nImpact ControlNet. Our approach mitigates conflicts through three key\nstrategies: constructing a balanced dataset, combining and injecting feature\nsignals in a balanced manner, and addressing the asymmetry in the score\nfunction's Jacobian matrix induced by ControlNet. These improvements enhance\nthe compatibility of control signals, allowing for freer and more harmonious\ngeneration in areas with silent control signals.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMinimal Impact ControlNet\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u6570\u636e\u96c6\u6784\u5efa\u3001\u7279\u5f81\u4fe1\u53f7\u5e73\u8861\u6ce8\u5165\u548c\u89e3\u51b3Jacobian\u77e9\u9635\u4e0d\u5bf9\u79f0\u6027\uff0c\u89e3\u51b3\u4e86\u591a\u63a7\u5236\u4fe1\u53f7\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u7684\u517c\u5bb9\u6027\u548c\u81ea\u7531\u5ea6\u3002", "motivation": "\u5f53\u524dControlNet\u8bad\u7ec3\u4e2d\uff0c\u591a\u79cd\u63a7\u5236\u4fe1\u53f7\u53ef\u80fd\u76f8\u4e92\u51b2\u7a81\uff0c\u5c24\u5176\u662f\u5728\u8fb9\u7f18\u578b\u63a7\u5236\u6761\u4ef6\u4e0b\uff0c\u4f4e\u9891\u7387\u7684\u65e0\u58f0\u4fe1\u53f7\u4f1a\u6291\u5236\u7eb9\u7406\u751f\u6210\uff0c\u5bfc\u81f4\u751f\u6210\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5e73\u8861\u6570\u636e\u96c6\u3001\u5e73\u8861\u6ce8\u5165\u7279\u5f81\u4fe1\u53f7\u4ee5\u53ca\u89e3\u51b3Jacobian\u77e9\u9635\u4e0d\u5bf9\u79f0\u6027\uff0c\u63d0\u51faMinimal Impact ControlNet\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u63a7\u5236\u4fe1\u53f7\u7684\u517c\u5bb9\u6027\uff0c\u4f7f\u5f97\u65e0\u58f0\u4fe1\u53f7\u533a\u57df\u80fd\u66f4\u81ea\u7531\u548c\u8c10\u5730\u751f\u6210\u56fe\u50cf\u3002", "conclusion": "Minimal Impact ControlNet\u6709\u6548\u89e3\u51b3\u4e86\u591a\u63a7\u5236\u4fe1\u53f7\u51b2\u7a81\u95ee\u9898\uff0c\u4e3a\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u7684\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "ControlNet, \u56fe\u50cf\u751f\u6210, \u63a7\u5236\u4fe1\u53f7, \u65e0\u58f0\u4fe1\u53f7, Jacobian\u77e9\u9635"}}
{"id": "2506.01796", "pdf": "https://arxiv.org/pdf/2506.01796", "abs": "https://arxiv.org/abs/2506.01796", "authors": ["Chen Zhang", "Jiuheng Lin", "Xiao Liu", "Zekai Zhang", "Yansong Feng"], "title": "Read it in Two Steps: Translating Extremely Low-Resource Languages with Code-Augmented Grammar Books", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "While large language models (LLMs) have shown promise in translating\nextremely low-resource languages using resources like dictionaries, the\neffectiveness of grammar books remains debated. This paper investigates the\nrole of grammar books in translating extremely low-resource languages by\ndecomposing it into two key steps: grammar rule retrieval and application. To\nfacilitate the study, we introduce ZhuangRules, a modularized dataset of\ngrammar rules and their corresponding test sentences. Our analysis reveals that\nrule retrieval constitutes a primary bottleneck in grammar-based translation.\nMoreover, although LLMs can apply simple rules for translation when explicitly\nprovided, they encounter difficulties in handling more complex rules. To\naddress these challenges, we propose representing grammar rules as code\nfunctions, considering their similarities in structure and the benefit of code\nin facilitating LLM reasoning. Our experiments show that using code rules\nsignificantly boosts both rule retrieval and application, ultimately resulting\nin a 13.1% BLEU improvement in translation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8bed\u6cd5\u4e66\u5728\u6781\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u89c4\u5219\u68c0\u7d22\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u5e76\u63d0\u51fa\u5c06\u8bed\u6cd5\u89c4\u5219\u7f16\u7801\u4e3a\u4ee3\u7801\u51fd\u6570\u4ee5\u63d0\u9ad8\u7ffb\u8bd1\u6548\u679c\uff0c\u5b9e\u9a8c\u663e\u793aBLEU\u5206\u6570\u63d0\u5347\u4e8613.1%\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u8bed\u6cd5\u4e66\u5728\u6781\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u5206\u89e3\u4e3a\u89c4\u5219\u68c0\u7d22\u548c\u5e94\u7528\u4e24\u4e2a\u6b65\u9aa4\u6765\u5206\u6790\u3002", "method": "\u5f15\u5165ZhuangRules\u6570\u636e\u96c6\uff0c\u5c06\u8bed\u6cd5\u89c4\u5219\u5206\u89e3\u4e3a\u68c0\u7d22\u548c\u5e94\u7528\u6b65\u9aa4\uff0c\u5e76\u63d0\u51fa\u5c06\u89c4\u5219\u8868\u793a\u4e3a\u4ee3\u7801\u51fd\u6570\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u89c4\u5219\u68c0\u7d22\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u800c\u4ee3\u7801\u89c4\u5219\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u548c\u5e94\u7528\u6548\u679c\uff0c\u7ffb\u8bd1BLEU\u5206\u6570\u63d0\u9ad8\u4e8613.1%\u3002", "conclusion": "\u5c06\u8bed\u6cd5\u89c4\u5219\u7f16\u7801\u4e3a\u4ee3\u7801\u51fd\u6570\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u7ffb\u8bd1\u6548\u679c\uff0c\u89e3\u51b3\u4e86LLM\u5728\u590d\u6742\u89c4\u5219\u5904\u7406\u4e0a\u7684\u56f0\u96be\u3002", "keywords": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1, \u8bed\u6cd5\u4e66, \u89c4\u5219\u68c0\u7d22, \u4ee3\u7801\u89c4\u5219, BLEU\u5206\u6570"}}
{"id": "2506.01722", "pdf": "https://arxiv.org/pdf/2506.01722", "abs": "https://arxiv.org/abs/2506.01722", "authors": ["Antoine Moulin", "Emmanuel Esposito", "Dirk van der Hoeven"], "title": "When Lower-Order Terms Dominate: Adaptive Expert Algorithms for Heavy-Tailed Losses", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We consider the problem setting of prediction with expert advice with\npossibly heavy-tailed losses, i.e.\\ the only assumption on the losses is an\nupper bound on their second moments, denoted by $\\theta$. We develop adaptive\nalgorithms that do not require any prior knowledge about the range or the\nsecond moment of the losses. Existing adaptive algorithms have what is\ntypically considered a lower-order term in their regret guarantees. We show\nthat this lower-order term, which is often the maximum of the losses, can\nactually dominate the regret bound in our setting. Specifically, we show that\neven with small constant $\\theta$, this lower-order term can scale as\n$\\sqrt{KT}$, where $K$ is the number of experts and $T$ is the time horizon. We\npropose adaptive algorithms with improved regret bounds that avoid the\ndependence on such a lower-order term and guarantee $\\mathcal{O}(\\sqrt{\\theta\nT\\log(K)})$ regret in the worst case, and $\\mathcal{O}(\\theta\n\\log(KT)/\\Delta_{\\min})$ regret when the losses are sampled i.i.d.\\ from some\nfixed distribution, where $\\Delta_{\\min}$ is the difference between the mean\nlosses of the second best expert and the best expert. Additionally, when the\nloss function is the squared loss, our algorithm also guarantees improved\nregret bounds over prior results.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e13\u5bb6\u9884\u6d4b\u95ee\u9898\u4e2d\u7684\u91cd\u5c3e\u635f\u5931\u573a\u666f\uff0c\u63d0\u51fa\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u7684\u81ea\u9002\u5e94\u7b97\u6cd5\uff0c\u6539\u8fdb\u73b0\u6709\u7b97\u6cd5\u7684\u9057\u61be\u754c\u3002", "motivation": "\u73b0\u6709\u81ea\u9002\u5e94\u7b97\u6cd5\u5728\u91cd\u5c3e\u635f\u5931\u573a\u666f\u4e0b\u7684\u9057\u61be\u754c\u53ef\u80fd\u88ab\u4f4e\u9636\u9879\u4e3b\u5bfc\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u51cf\u5c11\u8fd9\u79cd\u4f9d\u8d56\u6027\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u81ea\u9002\u5e94\u7b97\u6cd5\uff0c\u4e0d\u4f9d\u8d56\u635f\u5931\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u4f18\u5316\u9057\u61be\u754c\u7684\u8bbe\u8ba1\u6765\u907f\u514d\u4f4e\u9636\u9879\u5f71\u54cd\u3002", "result": "\u65b0\u7b97\u6cd5\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u4fdd\u8bc1O(\u221a(\u03b8Tlog(K)))\u7684\u9057\u61be\u754c\uff0c\u4e14\u5728\u72ec\u7acb\u540c\u5206\u5e03\u635f\u5931\u4e0b\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u7b97\u6cd5\u5728\u91cd\u5c3e\u635f\u5931\u573a\u666f\u4e0b\u663e\u8457\u6539\u8fdb\u9057\u61be\u754c\uff0c\u9002\u7528\u4e8e\u5e73\u65b9\u635f\u5931\u7b49\u5177\u4f53\u573a\u666f\u3002", "keywords": "\u4e13\u5bb6\u9884\u6d4b, \u91cd\u5c3e\u635f\u5931, \u81ea\u9002\u5e94\u7b97\u6cd5, \u9057\u61be\u754c"}}
{"id": "2506.01807", "pdf": "https://arxiv.org/pdf/2506.01807", "abs": "https://arxiv.org/abs/2506.01807", "authors": ["Zaur Gouliev"], "title": "Propaganda and Information Dissemination in the Russo-Ukrainian War: Natural Language Processing of Russian and Western Twitter Narratives", "categories": ["cs.CL"], "comment": "7 pages; 6 figures", "summary": "The conflict in Ukraine has been not only characterised by military\nengagement but also by a significant information war, with social media\nplatforms like X, formerly known as Twitter playing an important role in\nshaping public perception. This article provides an analysis of tweets from\npropaganda accounts and trusted accounts collected from the onset of the war,\nFebruary 2022 until the middle of May 2022 with n=40,000 total tweets. We\nutilise natural language processing and machine learning algorithms to assess\nthe sentiment and identify key themes, topics and narratives across the dataset\nwith human-in-the-loop (HITL) analysis throughout. Our findings indicate\ndistinct strategies in how information is created, spread, and targeted at\ndifferent audiences by both sides. Propaganda accounts frequently employ\nemotionally charged language and disinformation to evoke fear and distrust,\nwhereas other accounts, primarily Western tend to focus on factual reporting\nand humanitarian aspects of the conflict. Clustering analysis reveals groups of\naccounts with similar behaviours, which we suspect indicates the presence of\ncoordinated efforts. This research attempts to contribute to our understanding\nof the dynamics of information warfare and offers techniques for future studies\non social media influence in military conflicts.", "AI": {"tldr": "\u5206\u6790\u4e4c\u514b\u5170\u6218\u4e89\u4e2d\u793e\u4ea4\u5a92\u4f53\u7684\u4fe1\u606f\u6218\u7b56\u7565\uff0c\u901a\u8fc7\u63a8\u7279\u6570\u636e\u63ed\u793a\u5ba3\u4f20\u8d26\u6237\u548c\u53ef\u4fe1\u8d26\u6237\u7684\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u4e4c\u514b\u5170\u51b2\u7a81\u4e2d\u793e\u4ea4\u5a92\u4f53\u4fe1\u606f\u6218\u7684\u52a8\u6001\uff0c\u63ed\u793a\u5ba3\u4f20\u4e0e\u53ef\u4fe1\u8d26\u6237\u7684\u4e0d\u540c\u53d9\u4e8b\u7b56\u7565\u3002", "method": "\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u673a\u5668\u5b66\u4e60\u5206\u67904\u4e07\u6761\u63a8\u6587\uff0c\u7ed3\u5408\u4eba\u5de5\u5e72\u9884\uff0c\u8bc4\u4f30\u60c5\u611f\u548c\u4e3b\u9898\u3002", "result": "\u5ba3\u4f20\u8d26\u6237\u4f7f\u7528\u60c5\u7eea\u5316\u8bed\u8a00\u548c\u865a\u5047\u4fe1\u606f\uff0c\u53ef\u4fe1\u8d26\u6237\u6ce8\u91cd\u4e8b\u5b9e\u548c\u4eba\u9053\u4e3b\u4e49\uff1b\u805a\u7c7b\u5206\u6790\u53d1\u73b0\u7591\u4f3c\u534f\u8c03\u884c\u4e3a\u3002", "conclusion": "\u4fe1\u606f\u6218\u7b56\u7565\u5dee\u5f02\u663e\u8457\uff0c\u7814\u7a76\u4e3a\u672a\u6765\u793e\u4ea4\u5a92\u4f53\u5728\u519b\u4e8b\u51b2\u7a81\u4e2d\u7684\u5f71\u54cd\u63d0\u4f9b\u6280\u672f\u53c2\u8003\u3002", "keywords": "\u4e4c\u514b\u5170\u51b2\u7a81, \u4fe1\u606f\u6218, \u793e\u4ea4\u5a92\u4f53, \u81ea\u7136\u8bed\u8a00\u5904\u7406, \u673a\u5668\u5b66\u4e60"}}
{"id": "2506.00627", "pdf": "https://arxiv.org/pdf/2506.00627", "abs": "https://arxiv.org/abs/2506.00627", "authors": ["Srikanth Avasarala", "Serena Wang", "Juba Ziani"], "title": "The Disparate Effects of Partial Information in Bayesian Strategic Learning", "categories": ["cs.GT", "cs.AI"], "comment": null, "summary": "We study how partial information about scoring rules affects fairness in\nstrategic learning settings. In strategic learning, a learner deploys a scoring\nrule, and agents respond strategically by modifying their features -- at some\ncost -- to improve their outcomes. However, in our work, agents do not observe\nthe scoring rule directly; instead, they receive a noisy signal of said rule.\nWe consider two different agent models: (i) naive agents, who take the noisy\nsignal at face value, and (ii) Bayesian agents, who update a prior belief based\non the signal.\n  Our goal is to understand how disparities in outcomes arise between groups\nthat differ in their costs of feature modification, and how these disparities\nvary with the level of transparency of the learner's rule. For naive agents, we\nshow that utility disparities can grow unboundedly with noise, and that the\ngroup with lower costs can, perhaps counter-intuitively, be disproportionately\nharmed under limited transparency. In contrast, for Bayesian agents,\ndisparities remain bounded. We provide a full characterization of disparities\nacross groups as a function of the level of transparency and show that they can\nvary non-monotonically with noise; in particular, disparities are often\nminimized at intermediate levels of transparency. Finally, we extend our\nanalysis to settings where groups differ not only in cost, but also in prior\nbeliefs, and study how this asymmetry influences fairness.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u5206\u89c4\u5219\u7684\u90e8\u5206\u4fe1\u606f\u5982\u4f55\u5f71\u54cd\u7b56\u7565\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027\uff0c\u5206\u6790\u4e0d\u540c\u900f\u660e\u5ea6\u548c\u4ee3\u7406\u6a21\u578b\u5bf9\u7ed3\u679c\u5dee\u5f02\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u5728\u7b56\u7565\u5b66\u4e60\u4e2d\uff0c\u4ee3\u7406\u65e0\u6cd5\u76f4\u63a5\u89c2\u5bdf\u8bc4\u5206\u89c4\u5219\u800c\u662f\u63a5\u6536\u566a\u58f0\u4fe1\u53f7\u65f6\uff0c\u4e0d\u540c\u6210\u672c\u7fa4\u4f53\u7684\u7ed3\u679c\u5dee\u5f02\u5982\u4f55\u4ea7\u751f\uff0c\u4ee5\u53ca\u900f\u660e\u5ea6\u5bf9\u516c\u5e73\u6027\u7684\u5f71\u54cd\u3002", "method": "\u5bf9\u6bd4\u5929\u771f\u4ee3\u7406\uff08\u76f4\u63a5\u4fe1\u4efb\u4fe1\u53f7\uff09\u548c\u8d1d\u53f6\u65af\u4ee3\u7406\uff08\u57fa\u4e8e\u4fe1\u53f7\u66f4\u65b0\u5148\u9a8c\u4fe1\u5ff5\uff09\u4e24\u79cd\u6a21\u578b\uff0c\u5206\u6790\u900f\u660e\u5ea6\u4e0e\u566a\u58f0\u5bf9\u7fa4\u4f53\u5dee\u5f02\u7684\u5f71\u54cd\u3002", "result": "\u5929\u771f\u4ee3\u7406\u7684\u6548\u7528\u5dee\u5f02\u53ef\u80fd\u968f\u566a\u58f0\u65e0\u9650\u589e\u957f\uff0c\u800c\u8d1d\u53f6\u65af\u4ee3\u7406\u7684\u5dee\u5f02\u6709\u754c\u3002\u900f\u660e\u5ea6\u4e0e\u566a\u58f0\u7684\u5173\u7cfb\u975e\u5355\u8c03\uff0c\u5dee\u5f02\u5e38\u5728\u4e2d\u900f\u660e\u5ea6\u65f6\u6700\u5c0f\u3002", "conclusion": "\u7fa4\u4f53\u95f4\u6210\u672c\u5dee\u5f02\u548c\u5148\u9a8c\u4fe1\u5ff5\u4e0d\u5bf9\u79f0\u4f1a\u663e\u8457\u5f71\u54cd\u516c\u5e73\u6027\uff0c\u4e2d\u900f\u660e\u5ea6\u53ef\u80fd\u662f\u5e73\u8861\u5dee\u5f02\u7684\u5173\u952e\u3002", "keywords": "\u7b56\u7565\u5b66\u4e60, \u516c\u5e73\u6027, \u900f\u660e\u5ea6, \u566a\u58f0\u4fe1\u53f7, \u8d1d\u53f6\u65af\u4ee3\u7406"}}
{"id": "2506.01728", "pdf": "https://arxiv.org/pdf/2506.01728", "abs": "https://arxiv.org/abs/2506.01728", "authors": ["Chendi Qian", "Christopher Morris"], "title": "Principled data augmentation for learning to solve quadratic programming problems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Linear and quadratic optimization are crucial in numerous real-world\napplications, from training machine learning models to integer-linear\noptimization. Recently, learning-to-optimize methods (L2O) for linear (LPs) or\nquadratic programs (QPs) using message-passing graph neural networks (MPNNs)\nhave gained traction, promising lightweight, data-driven proxies for solving\nsuch optimization problems. For example, they replace the costly computation of\nstrong branching scores in branch-and-bound solvers, requiring solving many\nsuch optimization problems. However, robust L2O MPNNs remain challenging in\ndata-scarce settings, especially when addressing complex optimization problems\nsuch as QPs. This work introduces a principled approach to data augmentation\ntailored for QPs via MPNNs. Our method leverages theoretically justified data\naugmentation techniques to generate diverse yet optimality-preserving\ninstances. Furthermore, we integrate these augmentations into a self-supervised\nlearning framework based on contrastive learning, thereby pretraining MPNNs for\nenhanced performance on L2O tasks. Extensive experiments demonstrate that our\napproach improves generalization in supervised scenarios and facilitates\neffective transfer learning to related optimization problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e8c\u6b21\u89c4\u5212\u95ee\u9898\uff08QPs\uff09\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\uff08MPNNs\uff09\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u4ee5\u63d0\u5347\u5b66\u4e60\u5230\u4f18\u5316\uff08L2O\uff09\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5b66\u4e60\u5230\u4f18\u5316\u65b9\u6cd5\uff08L2O\uff09\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u590d\u6742\u4f18\u5316\u95ee\u9898\uff08\u5982QPs\uff09\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6765\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u652f\u6301\u7684\u6570\u636e\u589e\u5f3a\u751f\u6210\u591a\u6837\u6027\u4e14\u4fdd\u6301\u6700\u4f18\u6027\u7684\u5b9e\u4f8b\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3MPNNs\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u76d1\u7763\u5b66\u4e60\u4e2d\u63d0\u5347\u4e86\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u652f\u6301\u6709\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86L2O\u4efb\u52a1\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728QPs\u95ee\u9898\u4e0a\u3002", "keywords": "\u5b66\u4e60\u5230\u4f18\u5316\uff08L2O\uff09\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\uff08MPNNs\uff09\u3001\u6570\u636e\u589e\u5f3a\u3001\u4e8c\u6b21\u89c4\u5212\uff08QPs\uff09\u3001\u5bf9\u6bd4\u5b66\u4e60"}}
{"id": "2506.01808", "pdf": "https://arxiv.org/pdf/2506.01808", "abs": "https://arxiv.org/abs/2506.01808", "authors": ["Beomseok Lee", "Marcely Zanon Boito", "Laurent Besacier", "Ioan Calapodescu"], "title": "NAVER LABS Europe Submission to the Instruction-following Track", "categories": ["cs.CL"], "comment": null, "summary": "In this paper we describe NAVER LABS Europe submission to the\ninstruction-following speech processing short track at IWSLT 2025. We\nparticipate in the constrained settings, developing systems that can\nsimultaneously perform ASR, ST, and SQA tasks from English speech input into\nthe following target languages: Chinese, Italian, and German. Our solution\nleverages two pretrained modules: (1) a speech-to-LLM embedding projector\ntrained using representations from the SeamlessM4T-v2-large speech encoder; and\n(2) LoRA adapters trained on text data on top of a Llama-3.1-8B-Instruct. These\nmodules are jointly loaded and further instruction-tuned for 1K steps on\nmultilingual and multimodal data to form our final system submitted for\nevaluation.", "AI": {"tldr": "NAVER LABS Europe\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u4efb\u52a1\u8bed\u97f3\u5904\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u540c\u65f6\u6267\u884cASR\u3001ST\u548cSQA\u4efb\u52a1\uff0c\u652f\u6301\u82f1\u8bed\u8f93\u5165\u5230\u4e2d\u6587\u3001\u610f\u5927\u5229\u8bed\u548c\u5fb7\u8bed\u3002\u7cfb\u7edf\u7ed3\u5408\u4e86SeamlessM4T-v2-large\u8bed\u97f3\u7f16\u7801\u5668\u548cLlama-3.1-8B-Instruct\u7684LoRA\u9002\u914d\u5668\uff0c\u5e76\u901a\u8fc71K\u6b65\u7684\u591a\u8bed\u8a00\u591a\u6a21\u6001\u6570\u636e\u5fae\u8c03\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u5904\u7406\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u3001\u8bed\u97f3\u7ffb\u8bd1\uff08ST\uff09\u548c\u8bed\u97f3\u95ee\u7b54\uff08SQA\uff09\u7684\u591a\u4efb\u52a1\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u8bed\u8a00\u8f93\u51fa\u3002", "method": "\u7cfb\u7edf\u57fa\u4e8e\u4e24\u4e2a\u9884\u8bad\u7ec3\u6a21\u5757\uff1a\u8bed\u97f3\u5230LLM\u5d4c\u5165\u6295\u5f71\u5668\u548cLoRA\u9002\u914d\u5668\uff0c\u5e76\u5728\u591a\u8bed\u8a00\u591a\u6a21\u6001\u6570\u636e\u4e0a\u8fdb\u884c\u4e861K\u6b65\u7684\u5fae\u8c03\u3002", "result": "\u7cfb\u7edf\u5728\u6307\u4ee4\u8ddf\u968f\u8bed\u97f3\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u652f\u6301\u82f1\u8bed\u5230\u4e2d\u6587\u3001\u610f\u5927\u5229\u8bed\u548c\u5fb7\u8bed\u7684\u8f6c\u6362\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u5757\u548c\u591a\u4efb\u52a1\u5fae\u8c03\uff0c\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8bed\u97f3\u5904\u7406\u529f\u80fd\u3002", "keywords": "\u8bed\u97f3\u5904\u7406, ASR, ST, SQA, SeamlessM4T, Llama-3.1-8B"}}
{"id": "2506.00633", "pdf": "https://arxiv.org/pdf/2506.00633", "abs": "https://arxiv.org/abs/2506.00633", "authors": ["Daniele Molino", "Camillo Maria Caruso", "Filippo Ruffini", "Paolo Soda", "Valerio Guarrasi"], "title": "Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive Vision-Language Pretraining", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Objective: While recent advances in text-conditioned generative models have\nenabled the synthesis of realistic medical images, progress has been largely\nconfined to 2D modalities such as chest X-rays. Extending text-to-image\ngeneration to volumetric Computed Tomography (CT) remains a significant\nchallenge, due to its high dimensionality, anatomical complexity, and the\nabsence of robust frameworks that align vision-language data in 3D medical\nimaging. Methods: We introduce a novel architecture for Text-to-CT generation\nthat combines a latent diffusion model with a 3D contrastive vision-language\npretraining scheme. Our approach leverages a dual-encoder CLIP-style model\ntrained on paired CT volumes and radiology reports to establish a shared\nembedding space, which serves as the conditioning input for generation. CT\nvolumes are compressed into a low-dimensional latent space via a pretrained\nvolumetric VAE, enabling efficient 3D denoising diffusion without requiring\nexternal super-resolution stages. Results: We evaluate our method on the\nCT-RATE dataset and conduct a comprehensive assessment of image fidelity,\nclinical relevance, and semantic alignment. Our model achieves competitive\nperformance across all tasks, significantly outperforming prior baselines for\ntext-to-CT generation. Moreover, we demonstrate that CT scans synthesized by\nour framework can effectively augment real data, improving downstream\ndiagnostic performance. Conclusion: Our results show that modality-specific\nvision-language alignment is a key component for high-quality 3D medical image\ngeneration. By integrating contrastive pretraining and volumetric diffusion,\nour method offers a scalable and controllable solution for synthesizing\nclinically meaningful CT volumes from text, paving the way for new applications\nin data augmentation, medical education, and automated clinical simulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4ece\u6587\u672c\u751f\u62103D CT\u56fe\u50cf\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e0e3D\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u6269\u5c55\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6280\u672f\u81f33D CT\u6210\u50cf\u9886\u57df\uff0c\u89e3\u51b3\u9ad8\u7ef4\u5ea6\u548c\u89e3\u5256\u590d\u6742\u6027\u7684\u6311\u6218\uff0c\u586b\u88653D\u533b\u5b66\u5f71\u50cf\u4e2d\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u53cc\u7f16\u7801\u5668CLIP\u98ce\u683c\u6a21\u578b\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u76843D VAE\uff0c\u901a\u8fc7\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u76843D\u53bb\u566a\u6269\u6563\uff0c\u65e0\u9700\u989d\u5916\u8d85\u5206\u8fa8\u7387\u5904\u7406\u3002", "result": "\u5728CT-RATE\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6a21\u578b\u5728\u56fe\u50cf\u4fdd\u771f\u5ea6\u3001\u4e34\u5e8a\u76f8\u5173\u6027\u548c\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u6709\u6548\u589e\u5f3a\u4e0b\u6e38\u8bca\u65ad\u6027\u80fd\u3002", "conclusion": "\u6a21\u6001\u7279\u5b9a\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u662f\u9ad8\u8d28\u91cf3D\u533b\u5b66\u56fe\u50cf\u751f\u6210\u7684\u5173\u952e\uff0c\u65b9\u6cd5\u4e3a\u6570\u636e\u589e\u5f3a\u3001\u533b\u5b66\u6559\u80b2\u548c\u4e34\u5e8a\u6a21\u62df\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "3D CT\u751f\u6210, \u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50, \u6f5c\u5728\u6269\u6563\u6a21\u578b, \u533b\u5b66\u5f71\u50cf\u5408\u6210, \u6570\u636e\u589e\u5f3a"}}
{"id": "2506.01741", "pdf": "https://arxiv.org/pdf/2506.01741", "abs": "https://arxiv.org/abs/2506.01741", "authors": ["Imran Nasim", "Melanie Weber"], "title": "Automated Manifold Learning for Reduced Order Modeling", "categories": ["cs.LG"], "comment": "17 pages, 6 figures", "summary": "The problem of identifying geometric structure in data is a cornerstone of\n(unsupervised) learning. As a result, Geometric Representation Learning has\nbeen widely applied across scientific and engineering domains. In this work, we\ninvestigate the use of Geometric Representation Learning for the data-driven\ndiscovery of system dynamics from spatial-temporal data. We propose to encode\nsimilarity structure in such data in a spatial-temporal proximity graph, to\nwhich we apply a range of classical and deep learning-based manifold learning\napproaches to learn reduced order dynamics. We observe that while manifold\nlearning is generally capable of recovering reduced order dynamics, the quality\nof the learned representations varies substantially across different algorithms\nand hyperparameter choices. This is indicative of high sensitivity to the\ninherent geometric assumptions of the respective approaches and suggests a need\nfor careful hyperparameter tuning, which can be expensive in practise. To\novercome these challenges, we propose a framework for Automated Manifold\nLearning, which selects a manifold learning approach and corresponding\nhyperparameter choices based on representative subsamples of the input graph.\nWe demonstrate that the proposed framework leads to performance gains both in\nscalability and in the learned representations' accuracy in capturing local and\nglobal geometric features of the underlying system dynamics.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5229\u7528\u51e0\u4f55\u8868\u793a\u5b66\u4e60\u4ece\u65f6\u7a7a\u6570\u636e\u4e2d\u53d1\u73b0\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6d41\u5f62\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6700\u4f73\u7b97\u6cd5\u548c\u8d85\u53c2\u6570\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u51e0\u4f55\u8868\u793a\u5b66\u4e60\u5728\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5bf9\u51e0\u4f55\u5047\u8bbe\u548c\u8d85\u53c2\u6570\u9009\u62e9\u9ad8\u5ea6\u654f\u611f\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u65f6\u7a7a\u90bb\u8fd1\u56fe\uff0c\u5e94\u7528\u7ecf\u5178\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5b66\u4e60\u964d\u9636\u52a8\u529b\u5b66\uff0c\u5e76\u5f00\u53d1\u81ea\u52a8\u5316\u6d41\u5f62\u5b66\u4e60\u6846\u67b6\u4ee5\u4f18\u5316\u7b97\u6cd5\u548c\u8d85\u53c2\u6570\u9009\u62e9\u3002", "result": "\u81ea\u52a8\u5316\u6846\u67b6\u5728\u53ef\u6269\u5c55\u6027\u548c\u5b66\u4e60\u8868\u793a\u7684\u51c6\u786e\u6027\uff08\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u51e0\u4f55\u7279\u5f81\uff09\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u81ea\u52a8\u5316\u6d41\u5f62\u5b66\u4e60\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5bf9\u51e0\u4f55\u5047\u8bbe\u548c\u8d85\u53c2\u6570\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u5347\u6027\u80fd\u548c\u6548\u7387\u3002", "keywords": "\u51e0\u4f55\u8868\u793a\u5b66\u4e60, \u6d41\u5f62\u5b66\u4e60, \u7cfb\u7edf\u52a8\u529b\u5b66, \u65f6\u7a7a\u6570\u636e, \u81ea\u52a8\u5316\u5b66\u4e60"}}
{"id": "2506.01814", "pdf": "https://arxiv.org/pdf/2506.01814", "abs": "https://arxiv.org/abs/2506.01814", "authors": ["PeiHsuan Huang", "ZihWei Lin", "Simon Imbot", "WenCheng Fu", "Ethan Tu"], "title": "Analysis of LLM Bias (Chinese Propaganda & Anti-US Sentiment) in DeepSeek-R1 vs. ChatGPT o3-mini-high", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Large language models (LLMs) increasingly shape public understanding and\ncivic decisions, yet their ideological neutrality is a growing concern. While\nexisting research has explored various forms of LLM bias, a direct,\ncross-lingual comparison of models with differing geopolitical\nalignments-specifically a PRC-system model versus a non-PRC counterpart-has\nbeen lacking. This study addresses this gap by systematically evaluating\nDeepSeek-R1 (PRC-aligned) against ChatGPT o3-mini-high (non-PRC) for\nChinese-state propaganda and anti-U.S. sentiment. We developed a novel corpus\nof 1,200 de-contextualized, reasoning-oriented questions derived from\nChinese-language news, presented in Simplified Chinese, Traditional Chinese,\nand English. Answers from both models (7,200 total) were assessed using a\nhybrid evaluation pipeline combining rubric-guided GPT-4o scoring with human\nannotation. Our findings reveal significant model-level and language-dependent\nbiases. DeepSeek-R1 consistently exhibited substantially higher proportions of\nboth propaganda and anti-U.S. bias compared to ChatGPT o3-mini-high, which\nremained largely free of anti-U.S. sentiment and showed lower propaganda\nlevels. For DeepSeek-R1, Simplified Chinese queries elicited the highest bias\nrates; these diminished in Traditional Chinese and were nearly absent in\nEnglish. Notably, DeepSeek-R1 occasionally responded in Simplified Chinese to\nTraditional Chinese queries and amplified existing PRC-aligned terms in its\nChinese answers, demonstrating an \"invisible loudspeaker\" effect. Furthermore,\nsuch biases were not confined to overtly political topics but also permeated\ncultural and lifestyle content, particularly in DeepSeek-R1.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86PRC\u7cfb\u7edf\u6a21\u578bDeepSeek-R1\u4e0e\u975ePRC\u6a21\u578bChatGPT o3-mini-high\u7684\u5730\u7f18\u653f\u6cbb\u503e\u5411\u6027\uff0c\u53d1\u73b0\u524d\u8005\u5728\u5ba3\u4f20\u548c\u53cd\u7f8e\u60c5\u7eea\u4e0a\u8868\u73b0\u66f4\u5f3a\uff0c\u4e14\u8bed\u8a00\u5bf9\u504f\u89c1\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u5730\u7f18\u653f\u6cbb\u5bf9\u9f50\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u610f\u8bc6\u5f62\u6001\u4e0a\u7684\u5dee\u5f02\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u76f4\u63a5\u3001\u8de8\u8bed\u8a00\u6bd4\u8f83\u7684\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e861,200\u4e2a\u53bb\u4e0a\u4e0b\u6587\u63a8\u7406\u95ee\u9898\u7684\u65b0\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u6df7\u5408\u8bc4\u4f30\u6d41\u7a0b\uff08GPT-4o\u8bc4\u5206\u548c\u4eba\u5de5\u6807\u6ce8\uff09\u5206\u67907,200\u4e2a\u6a21\u578b\u56de\u7b54\u3002", "result": "DeepSeek-R1\u6bd4ChatGPT o3-mini-high\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5ba3\u4f20\u548c\u53cd\u7f8e\u504f\u89c1\uff0c\u7b80\u4f53\u4e2d\u6587\u67e5\u8be2\u7684\u504f\u89c1\u7387\u6700\u9ad8\uff0c\u4e14\u5728\u6587\u5316\u548c\u751f\u6d3b\u65b9\u5f0f\u5185\u5bb9\u4e2d\u4e5f\u6709\u4f53\u73b0\u3002", "conclusion": "LLMs\u7684\u504f\u89c1\u4e0e\u5730\u7f18\u653f\u6cbb\u5bf9\u9f50\u548c\u8bed\u8a00\u5bc6\u5207\u76f8\u5173\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u6a21\u578b\u5728\u516c\u5171\u7406\u89e3\u4e2d\u7684\u610f\u8bc6\u5f62\u6001\u5f71\u54cd\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u610f\u8bc6\u5f62\u6001\u504f\u89c1, \u8de8\u8bed\u8a00\u6bd4\u8f83, \u5ba3\u4f20, \u5730\u7f18\u653f\u6cbb"}}
{"id": "2506.01777", "pdf": "https://arxiv.org/pdf/2506.01777", "abs": "https://arxiv.org/abs/2506.01777", "authors": ["Hithem Lamri", "Manaar Alam", "Haiyan Jiang", "Michail Maniatakos"], "title": "DRAUN: An Algorithm-Agnostic Data Reconstruction Attack on Federated Unlearning Systems", "categories": ["cs.LG"], "comment": null, "summary": "Federated Unlearning (FU) enables clients to remove the influence of specific\ndata from a collaboratively trained shared global model, addressing regulatory\nrequirements such as GDPR and CCPA. However, this unlearning process introduces\na new privacy risk: A malicious server may exploit unlearning updates to\nreconstruct the data requested for removal, a form of Data Reconstruction\nAttack (DRA). While DRAs for machine unlearning have been studied extensively\nin centralized Machine Learning-as-a-Service (MLaaS) settings, their\napplicability to FU remains unclear due to the decentralized, client-driven\nnature of FU. This work presents DRAUN, the first attack framework to\nreconstruct unlearned data in FU systems. DRAUN targets optimization-based\nunlearning methods, which are widely adopted for their efficiency. We\ntheoretically demonstrate why existing DRAs targeting machine unlearning in\nMLaaS fail in FU and show how DRAUN overcomes these limitations. We validate\nour approach through extensive experiments on four datasets and four model\narchitectures, evaluating its performance against five popular unlearning\nmethods, effectively demonstrating that state-of-the-art FU methods remain\nvulnerable to DRAs.", "AI": {"tldr": "DRAUN\u662f\u4e00\u79cd\u9488\u5bf9\u8054\u5408\u5b66\u4e60\u9057\u5fd8\uff08FU\uff09\u7cfb\u7edf\u7684\u9996\u6b21\u6570\u636e\u91cd\u5efa\u653b\u51fb\uff08DRA\uff09\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u73b0\u6709FU\u65b9\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4\u4e0a\u7684\u6f0f\u6d1e\u3002", "motivation": "\u7531\u4e8eFU\u7cfb\u7edf\u6ee1\u8db3GDPR\u548cCCPA\u7b49\u6cd5\u89c4\u8981\u6c42\uff0c\u5141\u8bb8\u5ba2\u6237\u4ece\u5171\u4eab\u5168\u5c40\u6a21\u578b\u4e2d\u5220\u9664\u7279\u5b9a\u6570\u636e\u7684\u5f71\u54cd\uff0c\u4f46\u8fd9\u4e00\u8fc7\u7a0b\u53ef\u80fd\u88ab\u6076\u610f\u670d\u52a1\u5668\u5229\u7528\uff0c\u901a\u8fc7\u9057\u5fd8\u66f4\u65b0\u91cd\u5efa\u88ab\u5220\u9664\u7684\u6570\u636e\uff0c\u5b58\u5728\u9690\u79c1\u98ce\u9669\u3002", "method": "DRAUN\u9488\u5bf9\u5e7f\u6cdb\u91c7\u7528\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u514b\u670d\u4e86\u73b0\u6709DRA\u5728FU\u7cfb\u7edf\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u91cd\u5efa\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u548c\u56db\u79cd\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDRAUN\u6210\u529f\u5bf9\u4e94\u79cd\u6d41\u884c\u7684\u9057\u5fd8\u65b9\u6cd5\u53d1\u8d77\u653b\u51fb\uff0c\u8bc1\u660e\u73b0\u6709FU\u65b9\u6cd5\u4ecd\u6613\u53d7DRAs\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86FU\u7cfb\u7edf\u5728\u9690\u79c1\u4fdd\u62a4\u4e0a\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u4e3a\u672a\u6765\u8bbe\u8ba1\u66f4\u5b89\u5168\u7684FU\u65b9\u6cd5\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "keywords": "\u8054\u5408\u5b66\u4e60\u9057\u5fd8\uff08FU\uff09\uff0c\u6570\u636e\u91cd\u5efa\u653b\u51fb\uff08DRA\uff09\uff0c\u9690\u79c1\u98ce\u9669\uff0cGDPR\uff0c\u673a\u5668\u5b66\u4e60\u5373\u670d\u52a1\uff08MLaaS\uff09"}}
{"id": "2506.01817", "pdf": "https://arxiv.org/pdf/2506.01817", "abs": "https://arxiv.org/abs/2506.01817", "authors": ["Shadman Rohan", "Ishita Sur Apan", "Muhtasim Ibteda Shochcho", "Md Fahim", "Mohammad Ashfaq Ur Rahman", "AKM Mahbubur Rahman", "Amin Ahsan Ali"], "title": "BD at BEA 2025 Shared Task: MPNet Ensembles for Pedagogical Mistake Identification and Localization in AI Tutor Responses", "categories": ["cs.CL"], "comment": null, "summary": "We present Team BD's submission to the BEA 2025 Shared Task on Pedagogical\nAbility Assessment of AI-powered Tutors, under Track 1 (Mistake Identification)\nand Track 2 (Mistake Location). Both tracks involve three-class classification\nof tutor responses in educational dialogues - determining if a tutor correctly\nrecognizes a student's mistake (Track 1) and whether the tutor pinpoints the\nmistake's location (Track 2). Our system is built on MPNet, a Transformer-based\nlanguage model that combines BERT and XLNet's pre-training advantages. We\nfine-tuned MPNet on the task data using a class-weighted cross-entropy loss to\nhandle class imbalance, and leveraged grouped cross-validation (10 folds) to\nmaximize the use of limited data while avoiding dialogue overlap between\ntraining and validation. We then performed a hard-voting ensemble of the best\nmodels from each fold, which improves robustness and generalization by\ncombining multiple classifiers. Our approach achieved strong results on both\ntracks, with exact-match macro-F1 scores of approximately 0.7110 for Mistake\nIdentification and 0.5543 for Mistake Location on the official test set. We\ninclude comprehensive analysis of our system's performance, including confusion\nmatrices and t-SNE visualizations to interpret classifier behavior, as well as\na taxonomy of common errors with examples. We hope our ensemble-based approach\nand findings provide useful insights for designing reliable tutor response\nevaluation systems in educational dialogue settings.", "AI": {"tldr": "Team BD\u63d0\u4ea4\u4e86BEA 2025\u5171\u4eab\u4efb\u52a1\u7684\u6559\u80b2\u5bf9\u8bdd\u4e2dAI\u5bfc\u5e08\u6559\u5b66\u80fd\u529b\u8bc4\u4f30\u7684\u4e24\u4e2a\u4efb\u52a1\uff08\u9519\u8bef\u8bc6\u522b\u548c\u9519\u8bef\u5b9a\u4f4d\uff09\uff0c\u91c7\u7528\u57fa\u4e8eMPNet\u7684Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u96c6\u6210\u65b9\u6cd5\u53d6\u5f97\u8f83\u597d\u6210\u679c\u3002", "motivation": "\u8bc4\u4f30AI\u5bfc\u5e08\u5728\u6559\u80b2\u5bf9\u8bdd\u4e2d\u8bc6\u522b\u548c\u5b9a\u4f4d\u5b66\u751f\u9519\u8bef\u7684\u80fd\u529b\uff0c\u8bbe\u8ba1\u53ef\u9760\u7684\u5bfc\u5e08\u54cd\u5e94\u8bc4\u4f30\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528MPNet\u6a21\u578b\uff0c\u7ed3\u5408BERT\u548cXLNet\u7684\u9884\u8bad\u7ec3\u4f18\u52bf\uff0c\u901a\u8fc7\u7c7b\u52a0\u6743\u4ea4\u53c9\u71b5\u635f\u5931\u548c\u5206\u7ec4\u4ea4\u53c9\u9a8c\u8bc1\u4f18\u5316\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u786c\u6295\u7968\u96c6\u6210\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u5b98\u65b9\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u9519\u8bef\u8bc6\u522b\u548c\u9519\u8bef\u5b9a\u4f4d\u7684\u5b8fF1\u5206\u6570\u5206\u522b\u4e3a0.7110\u548c0.5543\u3002", "conclusion": "\u96c6\u6210\u65b9\u6cd5\u548c\u5168\u9762\u5206\u6790\u4e3a\u6559\u80b2\u5bf9\u8bdd\u4e2d\u7684\u5bfc\u5e08\u54cd\u5e94\u8bc4\u4f30\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "keywords": "BEA 2025, Pedagogical Ability, AI Tutor, Mistake Identification, Mistake Location, MPNet, Ensemble Learning"}}
{"id": "2506.01780", "pdf": "https://arxiv.org/pdf/2506.01780", "abs": "https://arxiv.org/abs/2506.01780", "authors": ["Sophia Zhang Pettersson", "Kuo-Yun Liang", "Juan Carlos Andresen"], "title": "Federated Gaussian Mixture Models", "categories": ["cs.LG"], "comment": "19 pages, 6 figures. Submitted to ACM", "summary": "This paper introduces FedGenGMM, a novel one-shot federated learning approach\nfor Gaussian Mixture Models (GMM) tailored for unsupervised learning scenarios.\nIn federated learning (FL), where multiple decentralized clients\ncollaboratively train models without sharing raw data, significant challenges\ninclude statistical heterogeneity, high communication costs, and privacy\nconcerns. FedGenGMM addresses these issues by allowing local GMM models,\ntrained independently on client devices, to be aggregated through a single\ncommunication round. This approach leverages the generative property of GMMs,\nenabling the creation of a synthetic dataset on the server side to train a\nglobal model efficiently. Evaluation across diverse datasets covering image,\ntabular, and time series data demonstrates that FedGenGMM consistently achieves\nperformance comparable to non-federated and iterative federated methods, even\nunder significant data heterogeneity. Additionally, FedGenGMM significantly\nreduces communication overhead, maintains robust performance in anomaly\ndetection tasks, and offers flexibility in local model complexities, making it\nparticularly suitable for edge computing environments.", "AI": {"tldr": "FedGenGMM \u662f\u4e00\u79cd\u7528\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u7684\u4e00\u6b65\u5f0f\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u65e0\u76d1\u7763\u5b66\u4e60\u573a\u666f\uff0c\u901a\u8fc7\u5355\u8f6e\u901a\u4fe1\u805a\u5408\u672c\u5730\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u5f02\u8d28\u6027\u3001\u9ad8\u901a\u4fe1\u6210\u672c\u548c\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5b58\u5728\u6570\u636e\u5f02\u8d28\u6027\u3001\u9ad8\u901a\u4fe1\u6210\u672c\u548c\u9690\u79c1\u95ee\u9898\uff0cFedGenGMM\u65e8\u5728\u901a\u8fc7\u4e00\u6b65\u5f0f\u805a\u5408\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u7684\u6a21\u578b\u6027\u80fd\u3002", "method": "FedGenGMM\u5229\u7528GMM\u7684\u751f\u6210\u7279\u6027\uff0c\u901a\u8fc7\u5728\u670d\u52a1\u5668\u7aef\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u5168\u5c40\u6a21\u578b\uff0c\u4ec5\u9700\u5355\u8f6e\u901a\u4fe1\u5373\u53ef\u5b8c\u6210\u805a\u5408\u3002", "result": "\u5728\u56fe\u50cf\u3001\u8868\u683c\u548c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedGenGMM\u6027\u80fd\u4e0e\u975e\u8054\u90a6\u53ca\u8fed\u4ee3\u8054\u90a6\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\uff0c\u5e76\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "FedGenGMM\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u8054\u90a6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u6570\u636e\u5f02\u8d28\u6027\u9ad8\u7684\u573a\u666f\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60,\u9ad8\u65af\u6df7\u5408\u6a21\u578b,\u65e0\u76d1\u7763\u5b66\u4e60,\u9690\u79c1\u4fdd\u62a4,\u8fb9\u7f18\u8ba1\u7b97"}}
{"id": "2506.01819", "pdf": "https://arxiv.org/pdf/2506.01819", "abs": "https://arxiv.org/abs/2506.01819", "authors": ["Moahmmadamin Shafiei", "Hamidreza Saffari"], "title": "Not All Jokes Land: Evaluating Large Language Models Understanding of Workplace Humor", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "With the recent advances in Artificial Intelligence (AI) and Large Language\nModels (LLMs), the automation of daily tasks, like automatic writing, is\ngetting more and more attention. Hence, efforts have focused on aligning LLMs\nwith human values, yet humor, particularly professional industrial humor used\nin workplaces, has been largely neglected. To address this, we develop a\ndataset of professional humor statements along with features that determine the\nappropriateness of each statement. Our evaluation of five LLMs shows that LLMs\noften struggle to judge the appropriateness of humor accurately.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5224\u65ad\u804c\u4e1a\u5e7d\u9ed8\u9002\u5f53\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u4e1a\u5e7d\u9ed8\u6570\u636e\u96c6\u3002", "motivation": "\u968f\u7740AI\u548cLLMs\u7684\u53d1\u5c55\uff0c\u81ea\u52a8\u5199\u4f5c\u7b49\u65e5\u5e38\u4efb\u52a1\u81ea\u52a8\u5316\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u804c\u4e1a\u5e7d\u9ed8\u7684\u9002\u5207\u6027\u5224\u65ad\u88ab\u5ffd\u89c6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u4e13\u4e1a\u5e7d\u9ed8\u8bed\u53e5\u53ca\u5176\u9002\u5207\u6027\u7279\u5f81\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e94\u79cdLLMs\u7684\u8868\u73b0\u3002", "result": "LLMs\u5728\u5224\u65ad\u5e7d\u9ed8\u9002\u5207\u6027\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u804c\u4e1a\u5e7d\u9ed8\u9886\u57df\u7684\u5c40\u9650\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u5bf9\u9f50\u4eba\u7c7b\u4ef7\u503c\u7684\u6a21\u578b\u3002", "keywords": "\u4eba\u5de5\u667a\u80fd, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u804c\u4e1a\u5e7d\u9ed8, \u9002\u5207\u6027\u5224\u65ad"}}
{"id": "2506.01781", "pdf": "https://arxiv.org/pdf/2506.01781", "abs": "https://arxiv.org/abs/2506.01781", "authors": ["Subhadip Nandi", "Neeraj Agrawal", "Anshika Singh", "Priyanka Bhatt"], "title": "Enhancing Customer Service Chatbots with Context-Aware NLU through Selective Attention and Multi-task Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Customer service chatbots are conversational systems aimed at addressing\ncustomer queries, often by directing them to automated workflows. A crucial\naspect of this process is the classification of the customer's intent.\nPresently, most intent classification models for customer care utilise only\ncustomer query for intent prediction. This may result in low-accuracy models,\nwhich cannot handle ambiguous queries. An ambiguous query like \"I didn't\nreceive my package\" could indicate a delayed order, or an order that was\ndelivered but the customer failed to receive it. Resolution of each of these\nscenarios requires the execution of very different sequence of steps. Utilizing\nadditional information, such as the customer's order delivery status, in the\nright manner can help identify the intent for such ambiguous queries. In this\npaper, we have introduced a context-aware NLU model that incorporates both, the\ncustomer query and contextual information from the customer's order status for\npredicting customer intent. A novel selective attention module is used to\nextract relevant context features. We have also proposed a multi-task learning\nparadigm for the effective utilization of different label types available in\nour training data. Our suggested method, Multi-Task Learning Contextual NLU\nwith Selective Attention Weighted Context (MTL-CNLU-SAWC), yields a 4.8%\nincrease in top 2 accuracy score over the baseline model which only uses user\nqueries, and a 3.5% improvement over existing state-of-the-art models that\ncombine query and context. We have deployed our model to production for\nWalmart's customer care domain. Accurate intent prediction through\nMTL-CNLU-SAWC helps to better direct customers to automated workflows, thereby\nsignificantly reducing escalations to human agents, leading to almost a million\ndollars in yearly savings for the company.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5ba2\u6237\u67e5\u8be2\u548c\u8ba2\u5355\u72b6\u6001\u4fe1\u606f\u7684\u4e0a\u4e0b\u6587\u611f\u77e5NLU\u6a21\u578b\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6ce8\u610f\u529b\u6a21\u5757\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u610f\u56fe\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u51cf\u5c11\u4e86\u4eba\u5de5\u5e72\u9884\uff0c\u8282\u7701\u4e86\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u5ba2\u6237\u670d\u52a1\u804a\u5929\u673a\u5668\u4eba\u7684\u610f\u56fe\u5206\u7c7b\u6a21\u578b\u4ec5\u57fa\u4e8e\u5ba2\u6237\u67e5\u8be2\uff0c\u5bfc\u81f4\u5bf9\u6a21\u7cca\u67e5\u8be2\u7684\u5904\u7406\u51c6\u786e\u6027\u4f4e\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5NLU\u6a21\u578b\uff08MTL-CNLU-SAWC\uff09\uff0c\u7ed3\u5408\u9009\u62e9\u6027\u6ce8\u610f\u529b\u6a21\u5757\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u540c\u65f6\u5229\u7528\u5ba2\u6237\u67e5\u8be2\u548c\u8ba2\u5355\u72b6\u6001\u4fe1\u606f\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u6a21\u578b\u5728Top 2\u51c6\u786e\u7387\u4e0a\u6bd4\u4ec5\u4f7f\u7528\u67e5\u8be2\u7684\u57fa\u7ebf\u6a21\u578b\u63d0\u9ad8\u4e864.8%\uff0c\u6bd4\u73b0\u6709\u7ed3\u5408\u67e5\u8be2\u548c\u4e0a\u4e0b\u6587\u7684\u6700\u5148\u8fdb\u6a21\u578b\u63d0\u9ad8\u4e863.5%\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u5e72\u9884\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u610f\u56fe\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u8282\u7701\u4e86\u516c\u53f8\u6210\u672c\u3002", "keywords": "\u610f\u56fe\u5206\u7c7b,\u4e0a\u4e0b\u6587\u611f\u77e5,NLU,\u9009\u62e9\u6027\u6ce8\u610f\u529b,\u591a\u4efb\u52a1\u5b66\u4e60"}}
{"id": "2506.01829", "pdf": "https://arxiv.org/pdf/2506.01829", "abs": "https://arxiv.org/abs/2506.01829", "authors": ["Yumo Xu", "Peng Qi", "Jifan Chen", "Kunlun Liu", "Rujun Han", "Lan Liu", "Bonan Min", "Vittorio Castelli", "Arshit Gupta", "Zhiguo Wang"], "title": "CiteEval: Principle-Driven Citation Evaluation for Source Attribution", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "ACL 2025", "summary": "Citation quality is crucial in information-seeking systems, directly\ninfluencing trust and the effectiveness of information access. Current\nevaluation frameworks, both human and automatic, mainly rely on Natural\nLanguage Inference (NLI) to assess binary or ternary supportiveness from cited\nsources, which we argue is a suboptimal proxy for citation evaluation. In this\nwork we introduce CiteEval, a citation evaluation framework driven by\nprinciples focusing on fine-grained citation assessment within a broad context,\nencompassing not only the cited sources but the full retrieval context, user\nquery, and generated text. Guided by the proposed framework, we construct\nCiteBench, a multi-domain benchmark with high-quality human annotations on\ncitation quality. To enable efficient evaluation, we further develop\nCiteEval-Auto, a suite of model-based metrics that exhibit strong correlation\nwith human judgments. Experiments across diverse systems demonstrate\nCiteEval-Auto's superior ability to capture the multifaceted nature of\ncitations compared to existing metrics, offering a principled and scalable\napproach to evaluate and improve model-generated citations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCiteEval\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5f15\u7528\u8bc4\u4f30\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u5f00\u53d1\u4e86CiteBench\u57fa\u51c6\u548cCiteEval-Auto\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eNLI\u7684\u5f15\u7528\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u591f\u5168\u9762\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u591a\u7ef4\u5ea6\u5f15\u7528\u8d28\u91cf\u3002", "method": "\u63d0\u51faCiteEval\u6846\u67b6\uff0c\u7ed3\u5408\u68c0\u7d22\u4e0a\u4e0b\u6587\u3001\u7528\u6237\u67e5\u8be2\u548c\u751f\u6210\u6587\u672c\uff0c\u6784\u5efaCiteBench\u57fa\u51c6\uff0c\u5e76\u5f00\u53d1CiteEval-Auto\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\u3002", "result": "CiteEval-Auto\u5728\u591a\u6837\u5316\u7cfb\u7edf\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6307\u6807\uff0c\u80fd\u6355\u6349\u5f15\u7528\u7684\u591a\u7ef4\u5ea6\u7279\u6027\u3002", "conclusion": "CiteEval\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u57fa\u4e8e\u539f\u5219\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u751f\u6210\u7684\u5f15\u7528\u3002", "keywords": "\u5f15\u7528\u8bc4\u4f30,\u81ea\u7136\u8bed\u8a00\u63a8\u7406,\u57fa\u51c6\u6d4b\u8bd5,\u81ea\u52a8\u8bc4\u4f30"}}
{"id": "2506.01789", "pdf": "https://arxiv.org/pdf/2506.01789", "abs": "https://arxiv.org/abs/2506.01789", "authors": ["Genta Indra Winata", "David Anugraha", "Emmy Liu", "Alham Fikri Aji", "Shou-Yi Hung", "Aditya Parashar", "Patrick Amadeus Irawan", "Ruochen Zhang", "Zheng-Xin Yong", "Jan Christian Blaise Cruz", "Niklas Muennighoff", "Seungone Kim", "Hanyang Zhao", "Sudipta Kar", "Kezia Erina Suryoraharjo", "M. Farid Adilazuarda", "En-Shiun Annie Lee", "Ayu Purwarianti", "Derry Tanti Wijaya", "Monojit Choudhury"], "title": "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "eess.AS"], "comment": "Preprint", "summary": "High-quality datasets are fundamental to training and evaluating machine\nlearning models, yet their creation-especially with accurate human\nannotations-remains a significant challenge. Many dataset paper submissions\nlack originality, diversity, or rigorous quality control, and these\nshortcomings are often overlooked during peer review. Submissions also\nfrequently omit essential details about dataset construction and properties.\nWhile existing tools such as datasheets aim to promote transparency, they are\nlargely descriptive and do not provide standardized, measurable methods for\nevaluating data quality. Similarly, metadata requirements at conferences\npromote accountability but are inconsistently enforced. To address these\nlimitations, this position paper advocates for the integration of systematic,\nrubric-based evaluation metrics into the dataset review process-particularly as\nsubmission volumes continue to grow. We also explore scalable, cost-effective\nmethods for synthetic data generation, including dedicated tools and\nLLM-as-a-judge approaches, to support more efficient evaluation. As a call to\naction, we introduce DataRubrics, a structured framework for assessing the\nquality of both human- and model-generated datasets. Leveraging recent advances\nin LLM-based evaluation, DataRubrics offers a reproducible, scalable, and\nactionable solution for dataset quality assessment, enabling both authors and\nreviewers to uphold higher standards in data-centric research. We also release\ncode to support reproducibility of LLM-based evaluations at\nhttps://github.com/datarubrics/datarubrics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faDataRubrics\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u548c\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6307\u6807\u6539\u8fdb\u6570\u636e\u96c6\u8bc4\u5ba1\u8fc7\u7a0b\uff0c\u5e76\u63a2\u7d22\u4e86\u9ad8\u6548\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u6570\u636e\u96c6\u8bba\u6587\u5728\u539f\u521b\u6027\u3001\u591a\u6837\u6027\u548c\u8d28\u91cf\u63a7\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u8bc4\u5ba1\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f15\u5165DataRubrics\u6846\u67b6\uff0c\u7ed3\u5408LLM\u6280\u672f\uff0c\u63d0\u4f9b\u53ef\u91cd\u590d\u3001\u53ef\u6269\u5c55\u7684\u6570\u636e\u96c6\u8d28\u91cf\u8bc4\u4f30\u65b9\u6848\u3002", "result": "\u63d0\u51fa\u4e86\u53ef\u64cd\u4f5c\u7684\u6570\u636e\u96c6\u8d28\u91cf\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u652f\u6301\u53ef\u91cd\u590d\u6027\u3002", "conclusion": "DataRubrics\u6846\u67b6\u4e3a\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u8d28\u91cf\u6807\u51c6\u548c\u8bc4\u5ba1\u900f\u660e\u5ea6\u3002", "keywords": "\u6570\u636e\u96c6\u8bc4\u4f30, LLM, \u8d28\u91cf\u8bc4\u4f30, \u8bc4\u5ba1\u6d41\u7a0b, \u5408\u6210\u6570\u636e"}}
{"id": "2506.01840", "pdf": "https://arxiv.org/pdf/2506.01840", "abs": "https://arxiv.org/abs/2506.01840", "authors": ["Igor Sterner", "Simone Teufel"], "title": "Minimal Pair-Based Evaluation of Code-Switching", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "There is a lack of an evaluation methodology that estimates the extent to\nwhich large language models (LLMs) use code-switching (CS) in the same way as\nbilinguals. Existing methods do not have wide language coverage, fail to\naccount for the diverse range of CS phenomena, or do not scale. We propose an\nintervention based on minimal pairs of CS. Each minimal pair contains one\nnaturally occurring CS sentence and one minimally manipulated variant. We\ncollect up to 1,000 such pairs each for 11 language pairs. Our human\nexperiments show that, for every language pair, bilinguals consistently prefer\nthe naturally occurring CS sentence. Meanwhile our experiments with current\nLLMs show that the larger the model, the more consistently it assigns higher\nprobability to the naturally occurring CS sentence than to the variant. In\naccordance with theoretical claims, the largest probability differences arise\nin those pairs where the manipulated material consisted of closed-class words.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5c0f\u5bf9\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u662f\u5426\u50cf\u53cc\u8bed\u8005\u4e00\u6837\u4f7f\u7528\u4ee3\u7801\u5207\u6362\uff08CS\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u89c4\u6a21\u5bf9CS\u504f\u597d\u7684\u4e00\u81f4\u6027\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u8a00\u8986\u76d6\u3001CS\u73b0\u8c61\u591a\u6837\u6027\u6216\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u5bf9LLM\u5728CS\u884c\u4e3a\u4e0a\u4e0e\u53cc\u8bed\u8005\u4e00\u81f4\u6027\u7684\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u6700\u5c0f\u5bf9\u5e72\u9884\uff0c\u6536\u96c611\u79cd\u8bed\u8a00\u5bf9\u7684\u81ea\u7136CS\u53e5\u5b50\u548c\u5176\u53d8\u4f53\uff0c\u5bf9\u6bd4\u53cc\u8bed\u8005\u548cLLM\u5bf9\u4e24\u8005\u7684\u504f\u597d\u3002", "result": "\u53cc\u8bed\u8005\u4e00\u81f4\u504f\u597d\u81ea\u7136CS\u53e5\u5b50\uff0cLLM\u4e2d\u6a21\u578b\u8d8a\u5927\uff0c\u5bf9\u81ea\u7136CS\u53e5\u5b50\u7684\u6982\u7387\u5206\u914d\u8d8a\u4e00\u81f4\u3002\u6700\u5927\u6982\u7387\u5dee\u5f02\u51fa\u73b0\u5728\u95ed\u7c7b\u8bcd\u64cd\u7eb5\u7684\u5bf9\u4e2d\u3002", "conclusion": "\u6a21\u578b\u89c4\u6a21\u4e0eCS\u884c\u4e3a\u4e00\u81f4\u6027\u6b63\u76f8\u5173\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5047\u8bbe\u3002", "keywords": "\u4ee3\u7801\u5207\u6362,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u6700\u5c0f\u5bf9,\u53cc\u8bed\u8005,\u8bc4\u4f30\u65b9\u6cd5"}}
{"id": "2506.01790", "pdf": "https://arxiv.org/pdf/2506.01790", "abs": "https://arxiv.org/abs/2506.01790", "authors": ["Zachary Coalson", "Juhan Bae", "Nicholas Carlini", "Sanghyun Hong"], "title": "$IF-GUIDE$: Influence Function-Guided Detoxification of LLMs", "categories": ["cs.LG", "cs.CR"], "comment": "Pre-print", "summary": "We study how training data contributes to the emergence of toxic behaviors in\nlarge-language models. Most prior work on reducing model toxicity adopts\n$reactive$ approaches, such as fine-tuning pre-trained (and potentially toxic)\nmodels to align them with human values. In contrast, we propose a $proactive$\napproach$-$IF-Guide$-$which leverages influence functions to identify harmful\ntokens within any training data and suppress their impact during training. To\nthis end, we first show that standard influence functions are ineffective at\ndiscovering harmful training records. We then present a novel adaptation that\nmeasures token-level attributions from training data to model toxicity, along\nwith techniques for selecting toxic training documents and a learning objective\nthat can be integrated into both pre-training and fine-tuning. Moreover,\nIF-Guide does not rely on human-preference data, which is typically required by\nexisting alignment methods. In evaluation, we demonstrate that IF-Guide\nsubstantially reduces both explicit and implicit toxicity$-$by up to 10$\\times$\ncompared to uncensored models, and up to 3$\\times$ compared to baseline\nalignment methods, e.g., DPO and RAD$-$across both pre-training and fine-tuning\nscenarios. IF-Guide is computationally efficient: a billion-parameter model is\n$not$ $necessary$ for computing influence scores; a million-parameter\nmodel$-$with 7.5$\\times$ fewer parameters$-$can effectively serve as a proxy\nfor identifying harmful data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u65b9\u6cd5IF-Guide\uff0c\u901a\u8fc7\u5f71\u54cd\u51fd\u6570\u8bc6\u522b\u5e76\u6291\u5236\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6709\u5bb3\u6807\u8bb0\uff0c\u4ece\u800c\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6bd2\u6027\u884c\u4e3a\uff0c\u65e0\u9700\u4f9d\u8d56\u4eba\u7c7b\u504f\u597d\u6570\u636e\u3002", "motivation": "\u7814\u7a76\u8bad\u7ec3\u6570\u636e\u5982\u4f55\u5bfc\u81f4\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6bd2\u6027\u884c\u4e3a\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u4e3b\u52a8\u65b9\u6cd5\u66ff\u4ee3\u73b0\u6709\u7684\u53cd\u5e94\u5f0f\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684\u5f71\u54cd\u51fd\u6570\u8bc6\u522b\u6709\u5bb3\u6807\u8bb0\uff0c\u63d0\u51fa\u9009\u62e9\u6709\u5bb3\u8bad\u7ec3\u6587\u6863\u7684\u6280\u672f\uff0c\u5e76\u8bbe\u8ba1\u53ef\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e2d\u7684\u5b66\u4e60\u76ee\u6807\u3002", "result": "IF-Guide\u663e\u8457\u51cf\u5c11\u663e\u6027\u548c\u9690\u6027\u6bd2\u6027\uff0c\u6548\u679c\u4f18\u4e8e\u672a\u5ba1\u67e5\u6a21\u578b\u548c\u57fa\u7ebf\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982DPO\u548cRAD\uff09\uff0c\u4e14\u8ba1\u7b97\u9ad8\u6548\u3002", "conclusion": "IF-Guide\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u4eba\u7c7b\u504f\u597d\u6570\u636e\u7684\u4e3b\u52a8\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u6a21\u578b\u6bd2\u6027\u3002", "keywords": "\u6a21\u578b\u6bd2\u6027, \u5f71\u54cd\u51fd\u6570, \u4e3b\u52a8\u65b9\u6cd5, \u9884\u8bad\u7ec3, \u5fae\u8c03"}}
{"id": "2506.01846", "pdf": "https://arxiv.org/pdf/2506.01846", "abs": "https://arxiv.org/abs/2506.01846", "authors": ["Igor Sterner", "Simone Teufel"], "title": "Code-Switching and Syntax: A Large-Scale Experiment", "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "The theoretical code-switching (CS) literature provides numerous pointwise\ninvestigations that aim to explain patterns in CS, i.e. why bilinguals switch\nlanguage in certain positions in a sentence more often than in others. A\nresulting consensus is that CS can be explained by the syntax of the\ncontributing languages. There is however no large-scale, multi-language,\ncross-phenomena experiment that tests this claim. When designing such an\nexperiment, we need to make sure that the system that is predicting where\nbilinguals tend to switch has access only to syntactic information. We provide\nsuch an experiment here. Results show that syntax alone is sufficient for an\nautomatic system to distinguish between sentences in minimal pairs of CS, to\nthe same degree as bilingual humans. Furthermore, the learnt syntactic patterns\ngeneralise well to unseen language pairs.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u53e5\u6cd5\u4fe1\u606f\u8db3\u4ee5\u9884\u6d4b\u53cc\u8bed\u8005\u5728\u53e5\u5b50\u4e2d\u7684\u4ee3\u7801\u8f6c\u6362\u884c\u4e3a\uff0c\u7ed3\u679c\u663e\u793a\u81ea\u52a8\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u8868\u73b0\u76f8\u5f53\uff0c\u4e14\u6a21\u5f0f\u53ef\u6cdb\u5316\u81f3\u672a\u89c1\u8fc7\u7684\u8bed\u8a00\u5bf9\u3002", "motivation": "\u73b0\u6709\u7406\u8bba\u8ba4\u4e3a\u4ee3\u7801\u8f6c\u6362\uff08CS\uff09\u53ef\u7531\u8bed\u8a00\u53e5\u6cd5\u89e3\u91ca\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u5b9e\u9a8c\u9a8c\u8bc1\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u4ec5\u57fa\u4e8e\u53e5\u6cd5\u4fe1\u606f\u7684\u9884\u6d4b\u7cfb\u7edf\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u8de8\u73b0\u8c61\u7684\u4ee3\u7801\u8f6c\u6362\u5b9e\u9a8c\u3002", "result": "\u53e5\u6cd5\u4fe1\u606f\u8db3\u4ee5\u4f7f\u81ea\u52a8\u7cfb\u7edf\u533a\u5206\u4ee3\u7801\u8f6c\u6362\u7684\u6700\u5c0f\u5bf9\uff0c\u6548\u679c\u4e0e\u4eba\u7c7b\u76f8\u5f53\uff0c\u4e14\u6a21\u5f0f\u53ef\u6cdb\u5316\u81f3\u65b0\u8bed\u8a00\u5bf9\u3002", "conclusion": "\u53e5\u6cd5\u5728\u4ee3\u7801\u8f6c\u6362\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u652f\u6301\u4e86\u7406\u8bba\u5047\u8bbe\u3002", "keywords": "\u4ee3\u7801\u8f6c\u6362, \u53e5\u6cd5, \u591a\u8bed\u8a00\u5b9e\u9a8c, \u81ea\u52a8\u7cfb\u7edf, \u6cdb\u5316\u80fd\u529b"}}
{"id": "2506.01815", "pdf": "https://arxiv.org/pdf/2506.01815", "abs": "https://arxiv.org/abs/2506.01815", "authors": ["Stephan Sturm"], "title": "Path Signatures for Feature Extraction. An Introduction to the Mathematics Underpinning an Efficient Machine Learning Technique", "categories": ["cs.LG", "math.PR", "60L10, 62H30"], "comment": "15 pages, 11 figures", "summary": "We provide an introduction to the topic of path signatures as means of\nfeature extraction for machine learning from data streams. The article stresses\nthe mathematical theory underlying the signature methodology, highlighting the\nconceptual character without plunging into the technical details of rigorous\nproofs. These notes are based on an introductory presentation given to students\nof the Research Experience for Undergraduates in Industrial Mathematics and\nStatistics at Worcester Polytechnic Institute in June 2024.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u8def\u5f84\u7b7e\u540d\u4f5c\u4e3a\u4ece\u6570\u636e\u6d41\u4e2d\u8fdb\u884c\u673a\u5668\u5b66\u4e60\u7279\u5f81\u63d0\u53d6\u7684\u65b9\u6cd5\uff0c\u5f3a\u8c03\u5176\u6570\u5b66\u7406\u8bba\u57fa\u7840\uff0c\u9002\u5408\u672c\u79d1\u751f\u5165\u95e8\u5b66\u4e60\u3002", "motivation": "\u4e3a\u5b66\u751f\u63d0\u4f9b\u8def\u5f84\u7b7e\u540d\u65b9\u6cd5\u7684\u57fa\u7840\u77e5\u8bc6\uff0c\u907f\u514d\u8fc7\u4e8e\u6280\u672f\u6027\uff0c\u4fbf\u4e8e\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u7406\u8bba\u4ecb\u7ecd\u8def\u5f84\u7b7e\u540d\u7684\u6982\u5ff5\uff0c\u672a\u6df1\u5165\u6280\u672f\u7ec6\u8282\u6216\u4e25\u683c\u8bc1\u660e\u3002", "result": "\u63d0\u4f9b\u4e86\u8def\u5f84\u7b7e\u540d\u65b9\u6cd5\u7684\u6559\u5b66\u6846\u67b6\uff0c\u9002\u5408\u521d\u5b66\u8005\u3002", "conclusion": "\u8def\u5f84\u7b7e\u540d\u662f\u6570\u636e\u6d41\u7279\u5f81\u63d0\u53d6\u7684\u6709\u6548\u5de5\u5177\uff0c\u9002\u5408\u5728\u6559\u5b66\u4e2d\u5f15\u5165\u3002", "keywords": "\u8def\u5f84\u7b7e\u540d, \u7279\u5f81\u63d0\u53d6, \u673a\u5668\u5b66\u4e60, \u6570\u636e\u6d41"}}
{"id": "2506.01859", "pdf": "https://arxiv.org/pdf/2506.01859", "abs": "https://arxiv.org/abs/2506.01859", "authors": ["Tamer Alkhouli", "Katerina Margatina", "James Gung", "Raphael Shu", "Claudia Zaghi", "Monica Sunkara", "Yi Zhang"], "title": "CONFETTI: Conversational Function-Calling Evaluation Through Turn-Level Interactions", "categories": ["cs.CL"], "comment": "ACL 2025 (main conference)", "summary": "We introduce Conversational Function-Calling Evaluation Through Turn-Level\nInteractions (CONFETTI), a conversational benchmark1 designed to evaluate the\nfunction-calling capabilities and response quality of large language models\n(LLMs). Current benchmarks lack comprehensive assessment of LLMs in complex\nconversational scenarios. CONFETTI addresses this gap through 109\nhuman-simulated conversations, comprising 313 user turns and covering 86 APIs.\nThese conversations explicitly target various conversational complexities, such\nas follow-ups, goal correction and switching, ambiguous and implicit goals. We\nperform off-policy turn-level evaluation using this benchmark targeting\nfunction-calling. Our benchmark also incorporates dialog act annotations to\nassess agent responses. We evaluate a series of state-of-the-art LLMs and\nanalyze their performance with respect to the number of available APIs,\nconversation lengths, and chained function calling. Our results reveal that\nwhile some models are able to handle long conversations, and leverage more than\n20+ APIs successfully, other models struggle with longer context or when\nincreasing the number of APIs. We also report that the performance on chained\nfunction-calls is severely limited across the models. Overall, the top\nperforming models on CONFETTI are Nova Pro (40.01%), Claude Sonnet v3.5\n(35.46%) and Llama 3.1 405B (33.19%) followed by command-r-plus (31.18%) and\nMistral-Large-2407 (30.07%).", "AI": {"tldr": "CONFETTI\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u5bf9\u8bdd\u573a\u666f\u4e2d\u8c03\u7528\u529f\u80fd\u548c\u54cd\u5e94\u8d28\u91cf\u7684\u5bf9\u8bdd\u57fa\u51c6\uff0c\u901a\u8fc7109\u4e2a\u4eba\u5de5\u6a21\u62df\u5bf9\u8bdd\u8986\u76d686\u4e2aAPI\uff0c\u63ed\u793a\u6a21\u578b\u5728\u957f\u5bf9\u8bdd\u548c\u591aAPI\u8c03\u7528\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u7f3a\u4e4f\u5bf9LLMs\u5728\u590d\u6742\u5bf9\u8bdd\u573a\u666f\u4e2d\u5168\u9762\u8bc4\u4f30\u7684\u9700\u6c42\uff0cCONFETTI\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528109\u4e2a\u4eba\u5de5\u6a21\u62df\u5bf9\u8bdd\uff08313\u4e2a\u7528\u6237\u8f6e\u6b21\uff0c\u8986\u76d686\u4e2aAPI\uff09\u8fdb\u884c\u79bb\u7ebf\u7b56\u7565\u7684\u8f6e\u6b21\u7ea7\u8bc4\u4f30\uff0c\u5e76\u52a0\u5165\u5bf9\u8bdd\u884c\u4e3a\u6ce8\u91ca\u6765\u8bc4\u4f30\u4ee3\u7406\u54cd\u5e94\u3002", "result": "\u90e8\u5206\u6a21\u578b\u80fd\u5904\u7406\u957f\u5bf9\u8bdd\u548c\u8d85\u8fc720\u4e2aAPI\u7684\u8c03\u7528\uff0c\u4f46\u591a\u6570\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u6216\u589e\u52a0API\u6570\u91cf\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e32\u8054\u529f\u80fd\u8c03\u7528\u7684\u6027\u80fd\u666e\u904d\u53d7\u9650\u3002\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u4e3aNova Pro\uff0840.01%\uff09\u3001Claude Sonnet v3.5\uff0835.46%\uff09\u548cLlama 3.1 405B\uff0833.19%\uff09\u3002", "conclusion": "CONFETTI\u63ed\u793a\u4e86\u6a21\u578b\u5728\u590d\u6742\u5bf9\u8bdd\u548c\u529f\u80fd\u8c03\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "keywords": "\u5bf9\u8bdd\u57fa\u51c6, \u5927\u8bed\u8a00\u6a21\u578b, \u529f\u80fd\u8c03\u7528, \u590d\u6742\u5bf9\u8bdd, API\u8bc4\u4f30"}}
{"id": "2506.01826", "pdf": "https://arxiv.org/pdf/2506.01826", "abs": "https://arxiv.org/abs/2506.01826", "authors": ["Haruki Yokota", "Hiroshi Higashi", "Yuichi Tanaka", "Gene Cheung"], "title": "Efficient Learning of Balanced Signed Graphs via Sparse Linear Programming", "categories": ["cs.LG", "eess.SP"], "comment": "13 pages, submitted to IEEE Transactions on Signal Processing", "summary": "Signed graphs are equipped with both positive and negative edge weights,\nencoding pairwise correlations as well as anti-correlations in data. A balanced\nsigned graph is a signed graph with no cycles containing an odd number of\nnegative edges. Laplacian of a balanced signed graph has eigenvectors that map\nvia a simple linear transform to ones in a corresponding positive graph\nLaplacian, thus enabling reuse of spectral filtering tools designed for\npositive graphs. We propose an efficient method to learn a balanced signed\ngraph Laplacian directly from data. Specifically, extending a previous linear\nprogramming (LP) based sparse inverse covariance estimation method called\nCLIME, we formulate a new LP problem for each Laplacian column $i$, where the\nlinear constraints restrict weight signs of edges stemming from node $i$, so\nthat nodes of same / different polarities are connected by positive / negative\nedges. Towards optimal model selection, we derive a suitable CLIME parameter\n$\\rho$ based on a combination of the Hannan-Quinn information criterion and a\nminimum feasibility criterion. We solve the LP problem efficiently by tailoring\na sparse LP method based on ADMM. We theoretically prove local solution\nconvergence of our proposed iterative algorithm. Extensive experimental results\non synthetic and real-world datasets show that our balanced graph learning\nmethod outperforms competing methods and enables reuse of spectral filters,\nwavelets, and graph convolutional nets (GCN) constructed for positive graphs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5e73\u8861\u7b26\u53f7\u56fe\u62c9\u666e\u62c9\u65af\u77e9\u9635\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u5229\u7528\u7ebf\u6027\u89c4\u5212\u6269\u5c55\u4e86CLIME\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7ADMM\u7b97\u6cd5\u4f18\u5316\u6c42\u89e3\u3002", "motivation": "\u7b26\u53f7\u56fe\u80fd\u540c\u65f6\u7f16\u7801\u6570\u636e\u7684\u6b63\u8d1f\u76f8\u5173\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u5b66\u4e60\u5e73\u8861\u7b26\u53f7\u56fe\u7684\u62c9\u666e\u62c9\u65af\u77e9\u9635\uff0c\u9650\u5236\u4e86\u8c31\u6ee4\u6ce2\u5de5\u5177\u7684\u590d\u7528\u3002", "method": "\u6269\u5c55CLIME\u65b9\u6cd5\uff0c\u63d0\u51fa\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\u7684\u5e73\u8861\u7b26\u53f7\u56fe\u62c9\u666e\u62c9\u65af\u77e9\u9635\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408Hannan-Quinn\u51c6\u5219\u548cADMM\u7b97\u6cd5\u4f18\u5316\u6c42\u89e3\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u7b97\u6cd5\u5c40\u90e8\u6536\u655b\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u7ade\u4e89\u65b9\u6cd5\uff0c\u5e76\u80fd\u590d\u7528\u6b63\u56fe\u8c31\u6ee4\u6ce2\u5de5\u5177\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u5b9e\u7528\uff0c\u4e3a\u7b26\u53f7\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u7b26\u53f7\u56fe, \u5e73\u8861\u56fe, \u62c9\u666e\u62c9\u65af\u77e9\u9635, \u8c31\u6ee4\u6ce2, \u7ebf\u6027\u89c4\u5212, ADMM"}}
{"id": "2506.01872", "pdf": "https://arxiv.org/pdf/2506.01872", "abs": "https://arxiv.org/abs/2506.01872", "authors": ["Tinghui Zhu", "Kai Zhang", "Muhao Chen", "Yu Su"], "title": "Is Extending Modality The Right Path Towards Omni-Modality?", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Omni-modal language models (OLMs) aim to integrate and reason over diverse\ninput modalities--such as text, images, video, and audio--while maintaining\nstrong language capabilities. Despite recent advancements, existing models,\nespecially open-source ones, remain far from true omni-modality, struggling to\ngeneralize beyond the specific modality pairs they are trained on or to achieve\nstrong performance when processing multi-modal inputs. We study the effect of\nextending modality, the dominant technique for training multimodal models,\nwhere an off-the-shelf language model is fine-tuned on target-domain and\nlanguage data. Specifically, we investigate three key questions: (1) Does\nmodality extension compromise core language abilities? (2) Can model merging\neffectively integrate independently fine-tuned modality-specific models to\nachieve omni-modality? (3) Does omni-modality extension lead to better\nknowledge sharing and generalization compared to sequential extension? Through\nextensive experiments, we analyze these trade-offs and provide insights into\nthe feasibility of achieving true omni-modality using current approaches.", "AI": {"tldr": "OLMs\u7814\u7a76\u591a\u6a21\u6001\u6574\u5408\u4e0e\u8bed\u8a00\u80fd\u529b\u4fdd\u6301\uff0c\u63a2\u8ba8\u6a21\u6001\u6269\u5c55\u5bf9\u6838\u5fc3\u8bed\u8a00\u80fd\u529b\u7684\u5f71\u54cd\u3001\u6a21\u578b\u878d\u5408\u5b9e\u73b0\u591a\u6a21\u6001\u7684\u6548\u679c\uff0c\u4ee5\u53ca\u591a\u6a21\u6001\u6269\u5c55\u662f\u5426\u4f18\u4e8e\u987a\u5e8f\u6269\u5c55\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u5728\u591a\u6a21\u6001\u6574\u5408\u548c\u6027\u80fd\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u5b9e\u9a8c\u63a2\u7d22\u5b9e\u73b0\u771f\u6b63\u591a\u6a21\u6001\u7684\u53ef\u80fd\u6027\u548c\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4e86\u6a21\u6001\u6269\u5c55\u6280\u672f\uff0c\u5206\u6790\u4e86\u5176\u5bf9\u6838\u5fc3\u8bed\u8a00\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5c1d\u8bd5\u901a\u8fc7\u6a21\u578b\u878d\u5408\u5b9e\u73b0\u591a\u6a21\u6001\u6574\u5408\uff0c\u5e76\u6bd4\u8f83\u4e86\u591a\u6a21\u6001\u6269\u5c55\u4e0e\u987a\u5e8f\u6269\u5c55\u7684\u6548\u679c\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u6743\u8861\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u5f53\u524d\u6280\u672f\u80fd\u5426\u5b9e\u73b0\u771f\u6b63\u591a\u6a21\u6001\u7684\u89c1\u89e3\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u591a\u6a21\u6001\u6a21\u578b\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u4f46\u5168\u9762\u5b9e\u73b0\u771f\u6b63\u591a\u6a21\u6001\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002", "keywords": "Omni-modal language models, \u591a\u6a21\u6001\u6574\u5408, \u6a21\u6001\u6269\u5c55, \u6a21\u578b\u878d\u5408, \u77e5\u8bc6\u5171\u4eab"}}
{"id": "2506.01827", "pdf": "https://arxiv.org/pdf/2506.01827", "abs": "https://arxiv.org/abs/2506.01827", "authors": ["Spencer Banasik"], "title": "Memory Access Characterization of Large Language Models in CPU Environment and its Potential Impacts", "categories": ["cs.LG", "cs.AR"], "comment": "34 pages, 14 figures", "summary": "As machine learning algorithms are shown to be an increasingly valuable tool,\nthe demand for their access has grown accordingly. Oftentimes, it is infeasible\nto run inference with larger models without an accelerator, which may be\nunavailable in environments that have constraints such as energy consumption,\nsecurity, or cost. To increase the availability of these models, we aim to\nimprove the LLM inference speed on a CPU-only environment by modifying the\ncache architecture. To determine what improvements could be made, we conducted\ntwo experiments using Llama.cpp and the QWEN model: running various cache\nconfigurations and evaluating their performance, and outputting a trace of the\nmemory footprint. Using these experiments, we investigate the memory access\npatterns and performance characteristics to identify potential optimizations.", "AI": {"tldr": "\u63d0\u5347CPU\u73af\u5883\u4e0bLLM\u63a8\u7406\u901f\u5ea6\uff0c\u901a\u8fc7\u4fee\u6539\u7f13\u5b58\u67b6\u6784\u6765\u5206\u6790\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u548c\u6027\u80fd\u7279\u5f81\uff0c\u8bc6\u522b\u4f18\u5316\u70b9\u3002", "motivation": "\u7531\u4e8e\u52a0\u901f\u5668\u5728\u67d0\u4e9b\u73af\u5883\u4e2d\u4e0d\u53ef\u7528\uff08\u5982\u80fd\u8017\u3001\u5b89\u5168\u6216\u6210\u672c\u9650\u5236\uff09\uff0c\u7814\u7a76\u5982\u4f55\u5728\u4e0d\u4f9d\u8d56\u52a0\u901f\u5668\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u5927\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790Llama.cpp\u548cQWEN\u6a21\u578b\u7684\u7f13\u5b58\u914d\u7f6e\u6027\u80fd\u53ca\u5185\u5b58\u8db3\u8ff9\uff0c\u7814\u7a76\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u548c\u6027\u80fd\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u5e2e\u52a9\u8bc6\u522b\u4e86\u6f5c\u5728\u7684\u4f18\u5316\u65b9\u5411\uff0c\u4ee5\u6539\u5584CPU-only\u73af\u5883\u4e0b\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u7f13\u5b58\u67b6\u6784\u7684\u8c03\u6574\u548c\u4f18\u5316\uff0c\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u52a0\u901f\u5668\u7684\u60c5\u51b5\u4e0b\u63d0\u5347LLM\u7684\u63a8\u7406\u6548\u7387\u3002", "keywords": "LLM, CPU\u63a8\u7406, \u7f13\u5b58\u4f18\u5316, \u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f"}}
{"id": "2506.01918", "pdf": "https://arxiv.org/pdf/2506.01918", "abs": "https://arxiv.org/abs/2506.01918", "authors": ["Chi-Jane Chen", "Yuhang Chen", "Sukwon Yun", "Natalie Stanley", "Tianlong Chen"], "title": "Spatial Coordinates as a Cell Language: A Multi-Sentence Framework for Imaging Mass Cytometry Analysis", "categories": ["cs.CL"], "comment": null, "summary": "Image mass cytometry (IMC) enables high-dimensional spatial profiling by\ncombining mass cytometry's analytical power with spatial distributions of cell\nphenotypes. Recent studies leverage large language models (LLMs) to extract\ncell states by translating gene or protein expression into biological context.\nHowever, existing single-cell LLMs face two major challenges: (1) Integration\nof spatial information: they struggle to generalize spatial coordinates and\neffectively encode spatial context as text, and (2) Treating each cell\nindependently: they overlook cell-cell interactions, limiting their ability to\ncapture biological relationships. To address these limitations, we propose\nSpatial2Sentence, a novel framework that integrates single-cell expression and\nspatial information into natural language using a multi-sentence approach.\nSpatial2Sentence constructs expression similarity and distance matrices,\npairing spatially adjacent and expressionally similar cells as positive pairs\nwhile using distant and dissimilar cells as negatives. These multi-sentence\nrepresentations enable LLMs to learn cellular interactions in both expression\nand spatial contexts. Equipped with multi-task learning, Spatial2Sentence\noutperforms existing single-cell LLMs on preprocessed IMC datasets, improving\ncell-type classification by 5.98% and clinical status prediction by 4.18% on\nthe diabetes dataset while enhancing interpretability. The source code can be\nfound here: https://github.com/UNITES-Lab/Spatial2Sentence.", "AI": {"tldr": "Spatial2Sentence\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u53e5\u5b50\u65b9\u6cd5\u6574\u5408\u5355\u7ec6\u80de\u8868\u8fbe\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5355\u7ec6\u80deLLMs\u5728\u7a7a\u95f4\u4fe1\u606f\u6574\u5408\u548c\u7ec6\u80de\u95f4\u76f8\u4e92\u4f5c\u7528\u5904\u7406\u4e0a\u7684\u4e0d\u8db3\u3002\u8be5\u6846\u67b6\u5728IMC\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u5347\u4e86\u7ec6\u80de\u7c7b\u578b\u5206\u7c7b\u548c\u4e34\u5e8a\u72b6\u6001\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u5355\u7ec6\u80de\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6574\u5408\u7a7a\u95f4\u4fe1\u606f\u548c\u5904\u7406\u7ec6\u80de\u95f4\u76f8\u4e92\u4f5c\u7528\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u751f\u7269\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "Spatial2Sentence\u901a\u8fc7\u6784\u5efa\u8868\u8fbe\u76f8\u4f3c\u6027\u548c\u8ddd\u79bb\u77e9\u9635\uff0c\u5c06\u7a7a\u95f4\u76f8\u90bb\u4e14\u8868\u8fbe\u76f8\u4f3c\u7684\u7ec6\u80de\u914d\u5bf9\u4e3a\u6b63\u6837\u672c\uff0c\u8fdc\u8ddd\u79bb\u4e14\u8868\u8fbe\u4e0d\u76f8\u4f3c\u7684\u7ec6\u80de\u914d\u5bf9\u4e3a\u8d1f\u6837\u672c\uff0c\u751f\u6210\u591a\u53e5\u5b50\u8868\u793a\uff0c\u4f7fLLMs\u80fd\u591f\u5b66\u4e60\u7ec6\u80de\u5728\u8868\u8fbe\u548c\u7a7a\u95f4\u80cc\u666f\u4e0b\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5728\u9884\u5904\u7406IMC\u6570\u636e\u96c6\u4e0a\uff0cSpatial2Sentence\u6bd4\u73b0\u6709\u5355\u7ec6\u80deLLMs\u8868\u73b0\u66f4\u4f18\uff0c\u7cd6\u5c3f\u75c5\u6570\u636e\u96c6\u4e0a\u7684\u7ec6\u80de\u7c7b\u578b\u5206\u7c7b\u548c\u4e34\u5e8a\u72b6\u6001\u9884\u6d4b\u5206\u522b\u63d0\u9ad8\u4e865.98%\u548c4.18%\uff0c\u5e76\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "Spatial2Sentence\u901a\u8fc7\u6574\u5408\u7a7a\u95f4\u548c\u8868\u8fbe\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u7ec6\u80de\u5206\u6790\u7684\u8868\u73b0\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "keywords": "Image mass cytometry, Large language models, Spatial2Sentence, Single-cell analysis, Cell-cell interactions"}}
{"id": "2506.00674", "pdf": "https://arxiv.org/pdf/2506.00674", "abs": "https://arxiv.org/abs/2506.00674", "authors": ["Zhiwei Zhang", "Samy Wu Fung", "Anastasios Kyrillidis", "Stanley Osher", "Moshe Y. Vardi"], "title": "Thinking Out of the Box: Hybrid SAT Solving by Unconstrained Continuous Optimization", "categories": ["cs.LO", "cs.AI", "cs.LG", "math.OC"], "comment": null, "summary": "The Boolean satisfiability (SAT) problem lies at the core of many\napplications in combinatorial optimization, software verification,\ncryptography, and machine learning. While state-of-the-art solvers have\ndemonstrated high efficiency in handling conjunctive normal form (CNF)\nformulas, numerous applications require non-CNF (hybrid) constraints, such as\nXOR, cardinality, and Not-All-Equal constraints. Recent work leverages\npolynomial representations to represent such hybrid constraints, but it relies\non box constraints that can limit the use of powerful unconstrained optimizers.\nIn this paper, we propose unconstrained continuous optimization formulations\nfor hybrid SAT solving by penalty terms. We provide theoretical insights into\nwhen these penalty terms are necessary and demonstrate empirically that\nunconstrained optimizers (e.g., Adam) can enhance SAT solving on hybrid\nbenchmarks. Our results highlight the potential of combining continuous\noptimization and machine-learning-based methods for effective hybrid SAT\nsolving.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u60e9\u7f5a\u9879\u7684\u65e0\u7ea6\u675f\u8fde\u7eed\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6df7\u5408\u5e03\u5c14\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\uff08SAT\uff09\uff0c\u5c55\u793a\u4e86\u5176\u5728\u63d0\u5347\u6c42\u89e3\u6548\u7387\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u7684SAT\u6c42\u89e3\u5668\u5728CNF\u516c\u5f0f\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5f53\u9762\u5bf9\u975eCNF\u7ea6\u675f\uff08\u5982XOR\u3001\u57fa\u6570\u7ea6\u675f\u7b49\uff09\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002\u591a\u9879\u5f0f\u8868\u793a\u65b9\u6cd5\u867d\u80fd\u5904\u7406\u8fd9\u4e9b\u7ea6\u675f\uff0c\u4f46\u4f9d\u8d56\u76d2\u7ea6\u675f\u9650\u5236\u4e86\u65e0\u7ea6\u675f\u4f18\u5316\u5668\u7684\u4f7f\u7528\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u60e9\u7f5a\u9879\uff0c\u5c06\u6df7\u5408SAT\u95ee\u9898\u8f6c\u5316\u4e3a\u65e0\u7ea6\u675f\u8fde\u7eed\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5229\u7528\u65e0\u7ea6\u675f\u4f18\u5316\u5668\uff08\u5982Adam\uff09\u8fdb\u884c\u6c42\u89e3\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6df7\u5408SAT\u95ee\u9898\u7684\u6c42\u89e3\u6548\u7387\u3002", "conclusion": "\u7ed3\u5408\u8fde\u7eed\u4f18\u5316\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e3a\u6df7\u5408SAT\u6c42\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002", "keywords": "\u5e03\u5c14\u53ef\u6ee1\u8db3\u6027\u95ee\u9898, \u6df7\u5408\u7ea6\u675f, \u8fde\u7eed\u4f18\u5316, \u60e9\u7f5a\u9879, \u673a\u5668\u5b66\u4e60"}}
{"id": "2506.01833", "pdf": "https://arxiv.org/pdf/2506.01833", "abs": "https://arxiv.org/abs/2506.01833", "authors": ["Zhao Yang", "Jiwei Zhu", "Bing Su"], "title": "SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model", "categories": ["cs.LG", "q-bio.GN"], "comment": "Accepted to ICML 2025", "summary": "Inspired by the success of unsupervised pre-training paradigms, researchers\nhave applied these approaches to DNA pre-training. However, we argue that these\napproaches alone yield suboptimal results because pure DNA sequences lack\nsufficient information, since their functions are regulated by genomic profiles\nlike chromatin accessibility. Here, we demonstrate that supervised training for\ngenomic profile prediction serves as a more effective alternative to pure\nsequence pre-training. Furthermore, considering the multi-species and\nmulti-profile nature of genomic profile prediction, we introduce our\n$\\textbf{S}$pecies-$\\textbf{P}$rofile $\\textbf{A}$daptive\n$\\textbf{C}$ollaborative $\\textbf{E}$xperts (SPACE) that leverages Mixture of\nExperts (MoE) to better capture the relationships between DNA sequences across\ndifferent species and genomic profiles, thereby learning more effective DNA\nrepresentations. Through extensive experiments across various tasks, our model\nachieves state-of-the-art performance, establishing that DNA models trained\nwith supervised genomic profiles serve as powerful DNA representation learners.\nThe code is available at https://github.com/ZhuJiwei111/SPACE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76d1\u7763\u8bad\u7ec3\u548c\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff08SPACE\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8DNA\u5e8f\u5217\u8868\u793a\u5b66\u4e60\u7684\u6548\u679c\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u7eafDNA\u5e8f\u5217\u7f3a\u4e4f\u8db3\u591f\u7684\u529f\u80fd\u4fe1\u606f\uff0c\u4f20\u7edf\u7684\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5229\u7528\u57fa\u56e0\u7ec4\u56fe\u8c31\u7684\u76d1\u7763\u8bad\u7ec3\u548c\u8de8\u7269\u79cd\u534f\u4f5c\u5b66\u4e60\uff0c\u6539\u8fdbDNA\u8868\u793a\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPACE\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u65b9\u6cd5\uff0c\u6355\u6349\u4e0d\u540c\u7269\u79cd\u548c\u57fa\u56e0\u7ec4\u56fe\u8c31\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ece\u800c\u5b66\u4e60\u66f4\u6709\u6548\u7684DNA\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSPACE\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u76d1\u7763\u57fa\u56e0\u7ec4\u56fe\u8c31\u8bad\u7ec3\u7684DNA\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u901a\u8fc7\u76d1\u7763\u8bad\u7ec3\u548c\u8de8\u7269\u79cd\u534f\u4f5c\u5b66\u4e60\uff0cSPACE\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86DNA\u5e8f\u5217\u8868\u793a\u5b66\u4e60\u7684\u6548\u679c\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "DNA\u8868\u793a\u5b66\u4e60\u3001\u57fa\u56e0\u7ec4\u56fe\u8c31\u3001\u6df7\u5408\u4e13\u5bb6\u3001\u76d1\u7763\u8bad\u7ec3\u3001\u8de8\u7269\u79cd\u5b66\u4e60"}}
{"id": "2506.01920", "pdf": "https://arxiv.org/pdf/2506.01920", "abs": "https://arxiv.org/abs/2506.01920", "authors": ["Serry Sibaee", "Omer Nacar", "Adel Ammar", "Yasser Al-Habashi", "Abdulrahman Al-Batati", "Wadii Boulila"], "title": "From Guidelines to Practice: A New Paradigm for Arabic Language Model Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses critical gaps in Arabic language model evaluation by\nestablishing comprehensive theoretical guidelines and introducing a novel\nevaluation framework. We first analyze existing Arabic evaluation datasets,\nidentifying significant issues in linguistic accuracy, cultural alignment, and\nmethodological rigor. To address these limitations in LLMs, we present the\nArabic Depth Mini Dataset (ADMD), a carefully curated collection of 490\nchallenging questions spanning ten major domains (42 sub-domains, see Figure 1.\nUsing ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet,\nGemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant\nvariations in model performance across different domains, with particular\nchallenges in areas requiring deep cultural understanding and specialized\nknowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%,\nshowing relative strength in mathematical theory in Arabic, Arabic language,\nand islamic domains. This work provides both theoretical foundations and\npractical insights for improving Arabic language model evaluation, emphasizing\nthe importance of cultural competence alongside technical capabilities.", "AI": {"tldr": "\u672c\u6587\u586b\u8865\u4e86\u963f\u62c9\u4f2f\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u7684\u5173\u952e\u7a7a\u767d\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u5168\u9762\u7684\u7406\u8bba\u6307\u5357\u548c\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u5206\u6790\u73b0\u6709\u963f\u62c9\u4f2f\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u5305\u62ec\u8bed\u8a00\u51c6\u786e\u6027\u3001\u6587\u5316\u4e00\u81f4\u6027\u548c\u65b9\u6cd5\u8bba\u4e25\u8c28\u6027\u7684\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u963f\u62c9\u4f2f\u6df1\u5ea6\u8ff7\u4f60\u6570\u636e\u96c6\uff08ADMD\uff09\uff0c\u5305\u542b490\u4e2a\u6db5\u76d610\u5927\u9886\u57df\u7684\u95ee\u9898\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e94\u79cd\u9886\u5148\u8bed\u8a00\u6a21\u578b\u3002", "result": "Claude 3.5 Sonnet\u6574\u4f53\u8868\u73b0\u6700\u4f73\uff08\u51c6\u786e\u738730%\uff09\uff0c\u5c24\u5176\u5728\u6570\u5b66\u7406\u8bba\u3001\u963f\u62c9\u4f2f\u8bed\u8a00\u548c\u4f0a\u65af\u5170\u9886\u57df\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u4e3a\u963f\u62c9\u4f2f\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u57fa\u7840\uff0c\u5f3a\u8c03\u6587\u5316\u80fd\u529b\u4e0e\u6280\u672f\u80fd\u529b\u540c\u7b49\u91cd\u8981\u3002", "keywords": "\u963f\u62c9\u4f2f\u8bed\u8a00\u6a21\u578b\u3001\u8bc4\u4f30\u6846\u67b6\u3001\u6587\u5316\u7406\u89e3\u3001ADMD"}}
{"id": "2506.01844", "pdf": "https://arxiv.org/pdf/2506.01844", "abs": "https://arxiv.org/abs/2506.01844", "authors": ["Mustafa Shukor", "Dana Aubakirova", "Francesco Capuano", "Pepijn Kooijmans", "Steven Palma", "Adil Zouitine", "Michel Aractingi", "Caroline Pascal", "Martino Russi", "Andres Marafioti", "Simon Alibert", "Matthieu Cord", "Thomas Wolf", "Remi Cadene"], "title": "SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics", "categories": ["cs.LG", "cs.RO"], "comment": "24 pages. Code and assets: https://github.com/huggingface/lerobot", "summary": "Vision-language models (VLMs) pretrained on large-scale multimodal datasets\nencode rich visual and linguistic knowledge, making them a strong foundation\nfor robotics. Rather than training robotic policies from scratch, recent\napproaches adapt VLMs into vision-language-action (VLA) models that enable\nnatural language-driven perception and control. However, existing VLAs are\ntypically massive--often with billions of parameters--leading to high training\ncosts and limited real-world deployability. Moreover, they rely on academic and\nindustrial datasets, overlooking the growing availability of\ncommunity-collected data from affordable robotic platforms. In this work, we\npresent SmolVLA, a small, efficient, and community-driven VLA that drastically\nreduces both training and inference costs, while retaining competitive\nperformance. SmolVLA is designed to be trained on a single GPU and deployed on\nconsumer-grade GPUs or even CPUs. To further improve responsiveness, we\nintroduce an asynchronous inference stack decoupling perception and action\nprediction from action execution, allowing higher control rates with chunked\naction generation. Despite its compact size, SmolVLA achieves performance\ncomparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both\nsimulated as well as real-world robotic benchmarks and release all code,\npretrained models, and training data.", "AI": {"tldr": "SmolVLA\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u51cf\u5c11\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u53c2\u6570\u91cf\u8fc7\u5927\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u5b9e\u9645\u90e8\u7f72\u53d7\u9650\uff0c\u4e14\u5ffd\u89c6\u4e86\u793e\u533a\u6570\u636e\u7684\u6f5c\u529b\u3002", "method": "\u8bbe\u8ba1SmolVLA\uff0c\u652f\u6301\u5355GPU\u8bad\u7ec3\uff0c\u90e8\u7f72\u4e8e\u6d88\u8d39\u7ea7\u786c\u4ef6\uff0c\u5e76\u5f15\u5165\u5f02\u6b65\u63a8\u7406\u6808\u63d0\u9ad8\u54cd\u5e94\u901f\u5ea6\u3002", "result": "SmolVLA\u6027\u80fd\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SmolVLA\u4e3a\u673a\u5668\u4eba\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u793e\u533a\u6570\u636e\u7684\u5e94\u7528\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u673a\u5668\u4eba, \u4f4e\u6210\u672c\u90e8\u7f72, \u5f02\u6b65\u63a8\u7406"}}
{"id": "2506.01928", "pdf": "https://arxiv.org/pdf/2506.01928", "abs": "https://arxiv.org/abs/2506.01928", "authors": ["Subham Sekhar Sahoo", "Zhihan Yang", "Yash Akhauri", "Johnna Liu", "Deepansha Singh", "Zhoujun Cheng", "Zhengzhong Liu", "Eric Xing", "John Thickstun", "Arash Vahdat"], "title": "Esoteric Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Diffusion-based language models offer a compelling alternative to\nautoregressive (AR) models by enabling parallel and controllable generation.\nAmong this family of models, Masked Diffusion Models (MDMs) achieve the\nstrongest performance but still underperform AR models in perplexity and lack\nkey inference-time efficiency features--most notably, KV caching. In this work,\nwe introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,\nenabling smooth interpolation between their perplexities while overcoming their\nrespective limitations. Eso-LMs set a new state of the art on standard language\nmodeling benchmarks. Crucially, we are the **first to introduce KV caching for\nMDMs** while preserving parallel generation, significantly improving inference\nefficiency. Combined with an optimized sampling schedule, our method achieves\nup to **65x** faster inference than standard MDMs and **4x** faster inference\nthan prior semi-autoregressive approaches. We provide the code and model\ncheckpoints on the project page:\n[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)", "AI": {"tldr": "Eso-LMs\u878d\u5408\u81ea\u56de\u5f52\u548c\u63a9\u853d\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7KV\u7f13\u5b58\u548c\u4f18\u5316\u91c7\u6837\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u7ed3\u5408\u81ea\u56de\u5f52\u6a21\u578b\uff08AR\uff09\u548c\u63a9\u853d\u6269\u6563\u6a21\u578b\uff08MDM\uff09\u7684\u4f18\u52bf\uff0c\u89e3\u51b3MDM\u5728\u56f0\u60d1\u5ea6\u548c\u63a8\u7406\u6548\u7387\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faEso-LMs\u6a21\u578b\uff0c\u878d\u5408AR\u4e0eMDM\uff0c\u5f15\u5165KV\u7f13\u5b58\u5e76\u4f18\u5316\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5728\u6807\u51c6\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff0c\u63a8\u7406\u6548\u7387\u6bd4\u6807\u51c6MDM\u5feb65\u500d\uff0c\u6bd4\u534a\u81ea\u56de\u5f52\u65b9\u6cd5\u5feb4\u500d\u3002", "conclusion": "Eso-LMs\u901a\u8fc7\u7ed3\u5408\u4e24\u79cd\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u8bed\u8a00\u751f\u6210\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b\u3001\u63a9\u853d\u6269\u6563\u6a21\u578b\u3001\u81ea\u56de\u5f52\u6a21\u578b\u3001KV\u7f13\u5b58\u3001\u63a8\u7406\u6548\u7387"}}
{"id": "2506.00679", "pdf": "https://arxiv.org/pdf/2506.00679", "abs": "https://arxiv.org/abs/2506.00679", "authors": ["Yunguan Fu", "Weixi Yi", "Charlotte Manisty", "Anish N Bhuva", "Thomas A Treibel", "James C Moon", "Matthew J Clarkson", "Rhodri Huw Davies", "Yipeng Hu"], "title": "CineMA: A Foundation Model for Cine Cardiac MRI", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Cardiac magnetic resonance (CMR) is a key investigation in clinical\ncardiovascular medicine and has been used extensively in population research.\nHowever, extracting clinically important measurements such as ejection fraction\nfor diagnosing cardiovascular diseases remains time-consuming and subjective.\nWe developed CineMA, a foundation AI model automating these tasks with limited\nlabels. CineMA is a self-supervised autoencoder model trained on 74,916 cine\nCMR studies to reconstruct images from masked inputs. After fine-tuning, it was\nevaluated across eight datasets on 23 tasks from four categories: ventricle and\nmyocardium segmentation, left and right ventricle ejection fraction\ncalculation, disease detection and classification, and landmark localisation.\nCineMA is the first foundation model for cine CMR to match or outperform\nconvolutional neural networks (CNNs). CineMA demonstrated greater label\nefficiency than CNNs, achieving comparable or better performance with fewer\nannotations. This reduces the burden of clinician labelling and supports\nreplacing task-specific training with fine-tuning foundation models in future\ncardiac imaging applications. Models and code for pre-training and fine-tuning\nare available at https://github.com/mathpluscode/CineMA, democratising access\nto high-performance models that otherwise require substantial computational\nresources, promoting reproducibility and accelerating clinical translation.", "AI": {"tldr": "CineMA \u662f\u4e00\u79cd\u81ea\u76d1\u7763\u57fa\u7840 AI \u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5fc3\u810f\u78c1\u5171\u632f (CMR) \u56fe\u50cf\u5206\u6790\uff0c\u51cf\u5c11\u4e86\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u4f9d\u8d56\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf CNN \u6a21\u578b\u3002", "motivation": "\u63d0\u53d6 CMR \u4e2d\u7684\u91cd\u8981\u4e34\u5e8a\u6307\u6807\uff08\u5982\u5c04\u8840\u5206\u6570\uff09\u8d39\u65f6\u4e14\u4e3b\u89c2\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u81ea\u76d1\u7763\u81ea\u52a8\u7f16\u7801\u5668\u6a21\u578b CineMA\uff0c\u57fa\u4e8e 74,916 \u4e2a CMR \u7814\u7a76\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\uff0c\u5b8c\u6210\u591a\u4e2a\u4efb\u52a1\u3002", "result": "CineMA \u5728 8 \u4e2a\u6570\u636e\u96c6\u4e0a\u7684 23 \u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6807\u6ce8\u6548\u7387\u66f4\u9ad8\uff0c\u6027\u80fd\u4f18\u4e8e CNN\u3002", "conclusion": "CineMA \u51cf\u8f7b\u4e86\u4e34\u5e8a\u6807\u6ce8\u8d1f\u62c5\uff0c\u4e3a\u672a\u6765\u5fc3\u810f\u6210\u50cf\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "keywords": "CMR, AI \u6a21\u578b, \u81ea\u76d1\u7763\u5b66\u4e60, \u5c04\u8840\u5206\u6570, \u5fc3\u810f\u6210\u50cf"}}
{"id": "2506.01849", "pdf": "https://arxiv.org/pdf/2506.01849", "abs": "https://arxiv.org/abs/2506.01849", "authors": ["Krzysztof Kotowski", "Ramez Shendy", "Jakub Nalepa", "Przemys\u0142aw Biecek", "Piotr Wilczy\u0144ski", "Agata Kaczmarek", "Dawid P\u0142udowski", "Artur Janicki", "Evridiki Ntagiou"], "title": "Trojan Horse Hunt in Time Series Forecasting for Space Operations", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "This competition hosted on Kaggle\n(https://www.kaggle.com/competitions/trojan-horse-hunt-in-space) is the first\npart of a series of follow-up competitions and hackathons related to the\n\"Assurance for Space Domain AI Applications\" project funded by the European\nSpace Agency (https://assurance-ai.space-codev.org/). The competition idea is\nbased on one of the real-life AI security threats identified within the project\n-- the adversarial poisoning of continuously fine-tuned satellite telemetry\nforecasting models. The task is to develop methods for finding and\nreconstructing triggers (trojans) in advanced models for satellite telemetry\nforecasting used in safety-critical space operations. Participants are provided\nwith 1) a large public dataset of real-life multivariate satellite telemetry\n(without triggers), 2) a reference model trained on the clean data, 3) a set of\npoisoned neural hierarchical interpolation (N-HiTS) models for time series\nforecasting trained on the dataset with injected triggers, and 4) Jupyter\nnotebook with the training pipeline and baseline algorithm (the latter will be\npublished in the last month of the competition). The main task of the\ncompetition is to reconstruct a set of 45 triggers (i.e., short multivariate\ntime series segments) injected into the training data of the corresponding set\nof 45 poisoned models. The exact characteristics (i.e., shape, amplitude, and\nduration) of these triggers must be identified by participants. The popular\nNeural Cleanse method is adopted as a baseline, but it is not designed for time\nseries analysis and new approaches are necessary for the task. The impact of\nthe competition is not limited to the space domain, but also to many other\nsafety-critical applications of advanced time series analysis where model\npoisoning may lead to serious consequences.", "AI": {"tldr": "Kaggle\u7ade\u8d5b\u805a\u7126\u4e8e\u536b\u661f\u9065\u6d4b\u9884\u6d4b\u6a21\u578b\u4e2d\u7684\u5bf9\u6297\u6027\u4e2d\u6bd2\u95ee\u9898\uff0c\u4efb\u52a1\u662f\u5f00\u53d1\u65b9\u6cd5\u4ee5\u68c0\u6d4b\u548c\u91cd\u5efa\u89e6\u53d1\u5668\u3002", "motivation": "\u9488\u5bf9\u536b\u661f\u9065\u6d4b\u9884\u6d4b\u6a21\u578b\u4e2d\u7684\u5bf9\u6297\u6027\u4e2d\u6bd2\u5a01\u80c1\uff0c\u786e\u4fdd\u7a7a\u95f4\u9886\u57dfAI\u5e94\u7528\u7684\u5b89\u5168\u3002", "method": "\u53c2\u8d5b\u8005\u9700\u4f7f\u7528\u63d0\u4f9b\u7684\u516c\u5f00\u6570\u636e\u96c6\u3001\u53c2\u8003\u6a21\u578b\u548c\u4e2d\u6bd2\u6a21\u578b\uff0c\u5f00\u53d1\u65b0\u65b9\u6cd5\u4ee5\u91cd\u5efa\u89e6\u53d1\u5668\u3002", "result": "\u7ade\u8d5b\u7ed3\u679c\u5c06\u5c55\u793a\u89e6\u53d1\u5668\u7684\u5177\u4f53\u7279\u5f81\uff08\u5f62\u72b6\u3001\u5e45\u503c\u3001\u65f6\u957f\uff09\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7ade\u8d5b\u4e0d\u4ec5\u9002\u7528\u4e8e\u7a7a\u95f4\u9886\u57df\uff0c\u4e5f\u9002\u7528\u4e8e\u5176\u4ed6\u5b89\u5168\u5173\u952e\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u5e94\u7528\u3002", "keywords": "Kaggle, \u5bf9\u6297\u6027\u4e2d\u6bd2, \u536b\u661f\u9065\u6d4b, \u65f6\u95f4\u5e8f\u5217, Neural Cleanse"}}
{"id": "2506.01937", "pdf": "https://arxiv.org/pdf/2506.01937", "abs": "https://arxiv.org/abs/2506.01937", "authors": ["Saumya Malik", "Valentina Pyatkin", "Sander Land", "Jacob Morrison", "Noah A. Smith", "Hannaneh Hajishirzi", "Nathan Lambert"], "title": "RewardBench 2: Advancing Reward Model Evaluation", "categories": ["cs.CL"], "comment": "Data, models, and leaderboard available at\n  https://huggingface.co/collections/allenai/reward-bench-2-683d2612a4b3e38a3e53bb51", "summary": "Reward models are used throughout the post-training of language models to\ncapture nuanced signals from preference data and provide a training target for\noptimization across instruction following, reasoning, safety, and more domains.\nThe community has begun establishing best practices for evaluating reward\nmodels, from the development of benchmarks that test capabilities in specific\nskill areas to others that test agreement with human preferences. At the same\ntime, progress in evaluation has not been mirrored by the effectiveness of\nreward models in downstream tasks -- simpler direct alignment algorithms are\nreported to work better in many cases. This paper introduces RewardBench 2, a\nnew multi-skill reward modeling benchmark designed to bring new, challenging\ndata for accuracy-based reward model evaluation -- models score about 20 points\non average lower on RewardBench 2 compared to the first RewardBench -- while\nbeing highly correlated with downstream performance. Compared to most other\nbenchmarks, RewardBench 2 sources new human prompts instead of existing prompts\nfrom downstream evaluations, facilitating more rigorous evaluation practices.\nIn this paper, we describe our benchmark construction process and report how\nexisting models perform on it, while quantifying how performance on the\nbenchmark correlates with downstream use of the models in both inference-time\nscaling algorithms, like best-of-N sampling, and RLHF training algorithms like\nproximal policy optimization.", "AI": {"tldr": "RewardBench 2\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u6280\u80fd\u5956\u52b1\u5efa\u6a21\u57fa\u51c6\uff0c\u65e8\u5728\u63d0\u4f9b\u66f4\u5177\u6311\u6218\u6027\u7684\u6570\u636e\uff0c\u7528\u4e8e\u57fa\u4e8e\u51c6\u786e\u6027\u7684\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\uff0c\u4e14\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u9ad8\u5ea6\u76f8\u5173\u3002", "motivation": "\u5f53\u524d\u5956\u52b1\u6a21\u578b\u5728\u8bc4\u4f30\u65b9\u9762\u7684\u8fdb\u5c55\u672a\u80fd\u4e0e\u5176\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u76f8\u5339\u914d\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u65b0\u7684\u4eba\u7c7b\u63d0\u793a\u6784\u5efaRewardBench 2\u57fa\u51c6\uff0c\u5e76\u8bc4\u4f30\u73b0\u6709\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u53ca\u5176\u4e0e\u4e0b\u6e38\u4efb\u52a1\u7684\u76f8\u5173\u6027\u3002", "result": "RewardBench 2\u7684\u96be\u5ea6\u66f4\u9ad8\uff0c\u6a21\u578b\u5e73\u5747\u5f97\u5206\u6bd4\u7b2c\u4e00\u7248\u4f4e20\u5206\uff0c\u4f46\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u9ad8\u5ea6\u76f8\u5173\u3002", "conclusion": "RewardBench 2\u4e3a\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5177\u6311\u6218\u6027\u548c\u76f8\u5173\u6027\u7684\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u66f4\u4e25\u683c\u7684\u5b9e\u8df5\u3002", "keywords": "\u5956\u52b1\u6a21\u578b, \u8bc4\u4f30\u57fa\u51c6, \u4e0b\u6e38\u4efb\u52a1, \u4eba\u7c7b\u63d0\u793a, RLHF"}}
{"id": "2506.01855", "pdf": "https://arxiv.org/pdf/2506.01855", "abs": "https://arxiv.org/abs/2506.01855", "authors": ["Vitaly Feldman", "Guy Kornowski", "Xin Lyu"], "title": "Trade-offs in Data Memorization via Strong Data Processing Inequalities", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "comment": "To appear in COLT 2025", "summary": "Recent research demonstrated that training large language models involves\nmemorization of a significant fraction of training data. Such memorization can\nlead to privacy violations when training on sensitive user data and thus\nmotivates the study of data memorization's role in learning. In this work, we\ndevelop a general approach for proving lower bounds on excess data\nmemorization, that relies on a new connection between strong data processing\ninequalities and data memorization. We then demonstrate that several simple and\nnatural binary classification problems exhibit a trade-off between the number\nof samples available to a learning algorithm, and the amount of information\nabout the training data that a learning algorithm needs to memorize to be\naccurate. In particular, $\\Omega(d)$ bits of information about the training\ndata need to be memorized when $O(1)$ $d$-dimensional examples are available,\nwhich then decays as the number of examples grows at a problem-specific rate.\nFurther, our lower bounds are generally matched (up to logarithmic factors) by\nsimple learning algorithms. We also extend our lower bounds to more general\nmixture-of-clusters models. Our definitions and results build on the work of\nBrown et al. (2021) and address several limitations of the lower bounds in\ntheir work.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u8bb0\u5fc6\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc1\u660e\u6570\u636e\u8bb0\u5fc6\u4e0b\u754c\u7684\u65b0\u65b9\u6cd5\u3002\u7814\u7a76\u663e\u793a\uff0c\u7b80\u5355\u7684\u4e8c\u5143\u5206\u7c7b\u95ee\u9898\u4e2d\u5b58\u5728\u6837\u672c\u6570\u91cf\u4e0e\u6240\u9700\u8bb0\u5fc6\u4fe1\u606f\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u57fa\u4e8e\u654f\u611f\u7528\u6237\u6570\u636e\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u5bfc\u81f4\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63a2\u8ba8\u4e86\u6570\u636e\u8bb0\u5fc6\u5728\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u5c06\u5f3a\u6570\u636e\u5904\u7406\u4e0d\u7b49\u5f0f\u4e0e\u6570\u636e\u8bb0\u5fc6\u8054\u7cfb\u8d77\u6765\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc1\u660e\u6570\u636e\u8bb0\u5fc6\u4e0b\u754c\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u4e8c\u5143\u5206\u7c7b\u548c\u6df7\u5408\u805a\u7c7b\u6a21\u578b\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5c11\u6837\u672c\u60c5\u51b5\u4e0b\u9700\u8981\u8bb0\u5fc6$\text{\u03a9}(d)$\u4f4d\u4fe1\u606f\uff0c\u4e14\u8fd9\u4e9b\u4e0b\u754c\u901a\u5e38\u88ab\u7b80\u5355\u5b66\u4e60\u7b97\u6cd5\u5339\u914d\u3002", "conclusion": "\u8be5\u7814\u7a76\u6539\u8fdb\u4e86Brown\u7b49\u4eba\u7684\u5de5\u4f5c\uff0c\u5e76\u4e3a\u7406\u89e3\u6570\u636e\u8bb0\u5fc6\u4e0e\u5b66\u4e60\u4e4b\u95f4\u7684\u6743\u8861\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "keywords": "\u6570\u636e\u8bb0\u5fc6\u3001\u9690\u79c1\u4fdd\u62a4\u3001\u5f3a\u6570\u636e\u5904\u7406\u4e0d\u7b49\u5f0f\u3001\u4e8c\u5143\u5206\u7c7b\u3001\u6df7\u5408\u805a\u7c7b\u6a21\u578b"}}
{"id": "2506.01938", "pdf": "https://arxiv.org/pdf/2506.01938", "abs": "https://arxiv.org/abs/2506.01938", "authors": ["Franco Alberto Cardillo", "Franca Debole", "Francesca Frontini", "Mitra Aelami", "Nan\u00e9e Chahinian", "Serge Conrad"], "title": "Novel Benchmark for NER in the Wastewater and Stormwater Domain", "categories": ["cs.CL"], "comment": null, "summary": "Effective wastewater and stormwater management is essential for urban\nsustainability and environmental protection. Extracting structured knowledge\nfrom reports and regulations is challenging due to domainspecific terminology\nand multilingual contexts. This work focuses on domain-specific Named Entity\nRecognition (NER) as a first step towards effective relation and information\nextraction to support decision making. A multilingual benchmark is crucial for\nevaluating these methods. This study develops a French-Italian domain-specific\ntext corpus for wastewater management. It evaluates state-of-the-art NER\nmethods, including LLM-based approaches, to provide a reliable baseline for\nfuture strategies and explores automated annotation projection in view of an\nextension of the corpus to new languages.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6cd5\u8bed-\u610f\u5927\u5229\u8bed\u7684\u5e9f\u6c34\u7ba1\u7406\u9886\u57df\u7279\u5b9a\u6587\u672c\u8bed\u6599\u5e93\uff0c\u7528\u4e8e\u8bc4\u4f30\u6700\u5148\u8fdb\u7684NER\u65b9\u6cd5\uff0c\u652f\u6301\u672a\u6765\u7684\u591a\u8bed\u8a00\u51b3\u7b56\u652f\u6301\u5de5\u5177\u5f00\u53d1\u3002", "motivation": "\u5e9f\u6c34\u7ba1\u7406\u7684\u6709\u6548\u4fe1\u606f\u63d0\u53d6\u9700\u8981\u5904\u7406\u9886\u57df\u7279\u5b9a\u672f\u8bed\u548c\u591a\u8bed\u8a00\u6311\u6218\uff0cNER\u662f\u5173\u952e\u6b65\u9aa4\u3002", "method": "\u5f00\u53d1\u6cd5\u8bed-\u610f\u5927\u5229\u8bed\u5e9f\u6c34\u7ba1\u7406\u6587\u672c\u8bed\u6599\u5e93\uff0c\u8bc4\u4f30LLM-based\u7b49NER\u65b9\u6cd5\uff0c\u63a2\u7d22\u81ea\u52a8\u5316\u6807\u6ce8\u6295\u5f71\u3002", "result": "\u63d0\u4f9b\u4e86\u53ef\u9760\u7684NER\u57fa\u51c6\uff0c\u652f\u6301\u591a\u8bed\u8a00\u5e9f\u6c34\u7ba1\u7406\u51b3\u7b56\u3002", "conclusion": "\u8be5\u8bed\u6599\u5e93\u548c\u65b9\u6cd5\u4e3a\u5e9f\u6c34\u7ba1\u7406\u9886\u57df\u7684\u4fe1\u606f\u63d0\u53d6\u548c\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "keywords": "\u5e9f\u6c34\u7ba1\u7406, NER, \u591a\u8bed\u8a00\u8bed\u6599\u5e93, LLM, \u6807\u6ce8\u6295\u5f71"}}
{"id": "2506.01863", "pdf": "https://arxiv.org/pdf/2506.01863", "abs": "https://arxiv.org/abs/2506.01863", "authors": ["Andrei Panferov", "Alexandra Volkova", "Ionut-Vlad Modoranu", "Vage Egiazarian", "Mher Safaryan", "Dan Alistarh"], "title": "Unified Scaling Laws for Compressed Representations", "categories": ["cs.LG", "cs.CL"], "comment": "Preprint", "summary": "Scaling laws have shaped recent advances in machine learning by enabling\npredictable scaling of model performance based on model size, computation, and\ndata volume. Concurrently, the rise in computational cost for AI has motivated\nmodel compression techniques, notably quantization and sparsification, which\nhave emerged to mitigate the steep computational demands associated with\nlarge-scale training and inference. This paper investigates the interplay\nbetween scaling laws and compression formats, exploring whether a unified\nscaling framework can accurately predict model performance when training occurs\nover various compressed representations, such as sparse, scalar-quantized,\nsparse-quantized or even vector-quantized formats. Our key contributions\ninclude validating a general scaling law formulation and showing that it is\napplicable both individually but also composably across compression types.\nBased on this, our main finding is demonstrating both theoretically and\nempirically that there exists a simple \"capacity\" metric -- based on the\nrepresentation's ability to fit random Gaussian data -- which can robustly\npredict parameter efficiency across multiple compressed representations. On the\npractical side, we extend our formulation to directly compare the accuracy\npotential of different compressed formats, and to derive better algorithms for\ntraining over sparse-quantized formats.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6269\u5c55\u5b9a\u5f8b\u4e0e\u538b\u7f29\u683c\u5f0f\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6269\u5c55\u6846\u67b6\uff0c\u80fd\u591f\u9884\u6d4b\u5728\u4e0d\u540c\u538b\u7f29\u8868\u793a\u4e0b\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8868\u793a\u80fd\u529b\u7684\u7b80\u5355\u6307\u6807\u3002", "motivation": "\u63a2\u7d22\u6269\u5c55\u5b9a\u5f8b\u4e0e\u538b\u7f29\u683c\u5f0f\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4ee5\u89e3\u51b3\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u9a8c\u8bc1\u901a\u7528\u7684\u6269\u5c55\u5b9a\u5f8b\u6846\u67b6\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u7a00\u758f\u3001\u91cf\u5316\u7b49\u538b\u7f29\u683c\u5f0f\uff0c\u540c\u65f6\u63d0\u51fa\u57fa\u4e8e\u8868\u793a\u80fd\u529b\u7684\u7b80\u5355\u6307\u6807\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u8bc1\u8868\u660e\uff0c\u57fa\u4e8e\u8868\u793a\u80fd\u529b\u7684\u6307\u6807\u53ef\u4ee5\u7a33\u5065\u5730\u9884\u6d4b\u591a\u79cd\u538b\u7f29\u683c\u5f0f\u7684\u53c2\u6570\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u4e3a\u538b\u7f29\u683c\u5f0f\u7684\u6027\u80fd\u9884\u6d4b\u548c\u7b97\u6cd5\u6539\u8fdb\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "keywords": "\u6269\u5c55\u5b9a\u5f8b, \u6a21\u578b\u538b\u7f29, \u7a00\u758f\u8868\u793a, \u91cf\u5316, \u53c2\u6570\u6548\u7387"}}
{"id": "2506.01939", "pdf": "https://arxiv.org/pdf/2506.01939", "abs": "https://arxiv.org/abs/2506.01939", "authors": ["Shenzhi Wang", "Le Yu", "Chang Gao", "Chujie Zheng", "Shixuan Liu", "Rui Lu", "Kai Dang", "Xionghui Chen", "Jianxin Yang", "Zhenru Zhang", "Yuqiong Liu", "An Yang", "Andrew Zhao", "Yang Yue", "Shiji Song", "Bowen Yu", "Gao Huang", "Junyang Lin"], "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 17 figures, 2 tables", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful approach to enhancing the reasoning capabilities of Large Language\nModels (LLMs), while its mechanisms are not yet well understood. In this work,\nwe undertake a pioneering exploration of RLVR through the novel perspective of\ntoken entropy patterns, comprehensively analyzing how different tokens\ninfluence reasoning performance. By examining token entropy patterns in\nChain-of-Thought (CoT) reasoning, we observe that only a small fraction of\ntokens exhibit high entropy, and these tokens act as critical forks that steer\nthe model toward diverse reasoning pathways. Furthermore, studying how entropy\npatterns evolve during RLVR training reveals that RLVR largely adheres to the\nbase model's entropy patterns, primarily adjusting the entropy of high-entropy\ntokens. These findings highlight the significance of high-entropy tokens (i.e.,\nforking tokens) to RLVR. We ultimately improve RLVR by restricting policy\ngradient updates to forking tokens and uncover a finding even beyond the 80/20\nrule: utilizing only 20% of the tokens while maintaining performance comparable\nto full-gradient updates on the Qwen3-8B base model and significantly\nsurpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71\non AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models,\nhighlighting a strong scaling trend. In contrast, training exclusively on the\n80% lowest-entropy tokens leads to a marked decline in performance. These\nfindings indicate that the efficacy of RLVR primarily arises from optimizing\nthe high-entropy tokens that decide reasoning directions. Collectively, our\nresults highlight the potential to understand RLVR through a token-entropy\nperspective and optimize RLVR by leveraging high-entropy minority tokens to\nfurther improve LLM reasoning.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7814\u7a76\u4ee4\u724c\u71b5\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u6027\u80fd\u7684\u5173\u952e\u673a\u5236\uff1a\u9ad8\u71b5\u4ee4\u724c\uff08\u5373\u5206\u53c9\u4ee4\u724c\uff09\u5bf9\u63a8\u7406\u8def\u5f84\u7684\u5f15\u5bfc\u4f5c\u7528\u3002\u901a\u8fc7\u9650\u5236\u7b56\u7565\u68af\u5ea6\u66f4\u65b0\u4ec5\u4f5c\u7528\u4e8e\u5206\u53c9\u4ee4\u724c\uff0cRLVR\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u5168\u68af\u5ea6\u66f4\u65b0\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22RLVR\u5982\u4f55\u901a\u8fc7\u4ee4\u724c\u71b5\u6a21\u5f0f\u4f18\u5316LLMs\u7684\u63a8\u7406\u6027\u80fd\uff0c\u586b\u8865\u4e86\u5bf9\u8be5\u673a\u5236\u7406\u89e3\u4e0d\u8db3\u7684\u7a7a\u767d\u3002", "method": "\u5206\u6790Chain-of-Thought\u63a8\u7406\u4e2d\u7684\u4ee4\u724c\u71b5\u6a21\u5f0f\uff0c\u7814\u7a76RLVR\u8bad\u7ec3\u4e2d\u71b5\u6a21\u5f0f\u7684\u6f14\u53d8\uff0c\u5e76\u901a\u8fc7\u9650\u5236\u7b56\u7565\u68af\u5ea6\u66f4\u65b0\u81f3\u9ad8\u71b5\u4ee4\u724c\u6765\u4f18\u5316RLVR\u3002", "result": "\u4ec5\u4f7f\u752820%\u7684\u9ad8\u71b5\u4ee4\u724c\u5373\u53ef\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5728\u5927\u578b\u6a21\u578b\uff08\u5982Qwen3-32B\uff09\u4e0a\u663e\u8457\u8d85\u8d8a\u5168\u68af\u5ea6\u66f4\u65b0\uff08\u5982AIME\u5206\u6570\u63d0\u534711.04\uff09\u3002", "conclusion": "RLVR\u7684\u4f18\u5316\u6548\u679c\u4e3b\u8981\u6765\u6e90\u4e8e\u5bf9\u9ad8\u71b5\u4ee4\u724c\u7684\u8c03\u6574\uff0c\u901a\u8fc7\u5206\u6790\u4ee4\u724c\u71b5\u6a21\u5f0f\u53ef\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316LLMs\u7684\u63a8\u7406\u6027\u80fd\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\uff1b\u4ee4\u724c\u71b5\u6a21\u5f0f\uff1b\u5927\u8bed\u8a00\u6a21\u578b\uff1b\u63a8\u7406\u4f18\u5316"}}
{"id": "2506.01868", "pdf": "https://arxiv.org/pdf/2506.01868", "abs": "https://arxiv.org/abs/2506.01868", "authors": ["Chengbing Chen", "Yutong Li", "Rui Zhao", "Zhoulin Liu", "Zheyong Fan", "Gang Tang", "Zhiyong Wang"], "title": "NepTrain and NepTrainKit: Automated Active Learning and Visualization Toolkit for Neuroevolution Potentials", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "comment": null, "summary": "As a machine-learned potential, the neuroevolution potential (NEP) method\nfeatures exceptional computational efficiency and has been successfully applied\nin materials science. Constructing high-quality training datasets is crucial\nfor developing accurate NEP models. However, the preparation and screening of\nNEP training datasets remain a bottleneck for broader applications due to their\ntime-consuming, labor-intensive, and resource-intensive nature. In this work,\nwe have developed NepTrain and NepTrainKit, which are dedicated to initializing\nand managing training datasets to generate high-quality training sets while\nautomating NEP model training. NepTrain is an open-source Python package that\nfeatures a bond length filtering method to effectively identify and remove\nnon-physical structures from molecular dynamics trajectories, thereby ensuring\nhigh-quality training datasets. NepTrainKit is a graphical user interface (GUI)\nsoftware designed specifically for NEP training datasets, providing\nfunctionalities for data editing, visualization, and interactive exploration.\nIt integrates key features such as outlier identification, farthest-point\nsampling, non-physical structure detection, and configuration type selection.\nThe combination of these tools enables users to process datasets more\nefficiently and conveniently. Using $\\rm CsPbI_3$ as a case study, we\ndemonstrate the complete workflow for training NEP models with NepTrain and\nfurther validate the models through materials property predictions. We believe\nthis toolkit will greatly benefit researchers working with machine learning\ninteratomic potentials.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86NepTrain\u548cNepTrainKit\u5de5\u5177\uff0c\u7528\u4e8e\u9ad8\u6548\u7ba1\u7406\u548c\u7b5b\u9009NEP\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u63d0\u9ad8\u8bad\u7ec3\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u6784\u5efa\u9ad8\u8d28\u91cf\u7684NEP\u8bad\u7ec3\u6570\u636e\u96c6\u662f\u5f00\u53d1\u51c6\u786e\u6a21\u578b\u7684\u5173\u952e\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u8017\u65f6\u8017\u529b\u3002", "method": "\u5f00\u53d1\u4e86NepTrain\uff08Python\u5305\uff09\u548cNepTrainKit\uff08GUI\u8f6f\u4ef6\uff09\uff0c\u63d0\u4f9b\u6570\u636e\u96c6\u521d\u59cb\u5316\u3001\u7ba1\u7406\u548c\u81ea\u52a8\u5316\u8bad\u7ec3\u529f\u80fd\u3002", "result": "\u6210\u529f\u5e94\u7528\u4e8eCsPbI3\u6848\u4f8b\uff0c\u9a8c\u8bc1\u4e86\u5de5\u5177\u7684\u5b9e\u7528\u6027\u548c\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u5de5\u5177\u4e3a\u673a\u5668\u5b66\u4e60\u539f\u5b50\u95f4\u52bf\u80fd\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u9ad8\u6548\u4fbf\u6377\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "NEP, \u8bad\u7ec3\u6570\u636e\u96c6, NepTrain, NepTrainKit, \u673a\u5668\u5b66\u4e60\u52bf\u80fd"}}
{"id": "2506.01951", "pdf": "https://arxiv.org/pdf/2506.01951", "abs": "https://arxiv.org/abs/2506.01951", "authors": ["Zicheng Xu", "Guanchu Wang", "Guangyao Zheng", "Yu-Neng Chuang", "Alexander Szalay", "Xia Hu", "Vladimir Braverman"], "title": "Self-ensemble: Mitigating Confidence Distortion for Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Although Large Language Models (LLMs) perform well in general fields, they\nexhibit a confidence distortion problem on multi-choice question-answering\n(MCQA), particularly as the number of answer choices increases. Specifically,\non MCQA with many choices, LLMs suffer from under-confidence in correct\npredictions and over-confidence in incorrect ones, leading to a substantially\ndegraded performance. To solve this problem, we propose Self-ensemble in this\nwork. Our method splits the choices into several groups and ensembles LLM\npredictions across these groups to reach a final decision. The advantage of\nSelf-ensemble is its plug-and-play nature, where it can be integrated into\nexisting LLM architecture based on a designed attention mask and positional\nencoding, without requiring labeled datasets for parameter tuning. Experimental\nresults on three LLMs and datasets demonstrate that Self-ensemble\ncomprehensively addresses the confidence distortion problem of LLMs,\noutperforming standard inference as well as baseline methods.", "AI": {"tldr": "\u81ea\u96c6\u6210\u65b9\u6cd5\u901a\u8fc7\u5206\u7ec4\u9009\u62e9\u5e76\u96c6\u6210\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u9009\u95ee\u9898\u4e2d\u7684\u4fe1\u5fc3\u5931\u771f\u95ee\u9898\uff0c\u65e0\u9700\u8c03\u53c2\u5373\u53ef\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u9009\u95ee\u9898\u4e2d\u5b58\u5728\u4fe1\u5fc3\u5931\u771f\u95ee\u9898\uff0c\u8868\u73b0\u4e3a\u5bf9\u6b63\u786e\u7b54\u6848\u4fe1\u5fc3\u4e0d\u8db3\u548c\u5bf9\u9519\u8bef\u7b54\u6848\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u81ea\u96c6\u6210\u65b9\u6cd5\uff0c\u5c06\u9009\u9879\u5206\u7ec4\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u63a9\u7801\u548c\u4f4d\u7f6e\u7f16\u7801\u96c6\u6210\u9884\u6d4b\uff0c\u65e0\u9700\u6807\u8bb0\u6570\u636e\u96c6\u8c03\u53c2\u3002", "result": "\u5728\u4e09\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u81ea\u96c6\u6210\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u4fe1\u5fc3\u5931\u771f\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u6807\u51c6\u63a8\u7406\u548c\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u96c6\u6210\u65b9\u6cd5\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u9009\u95ee\u9898\u4e2d\u7684\u8868\u73b0\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u591a\u9009\u95ee\u9898, \u4fe1\u5fc3\u5931\u771f, \u81ea\u96c6\u6210"}}
{"id": "2506.01869", "pdf": "https://arxiv.org/pdf/2506.01869", "abs": "https://arxiv.org/abs/2506.01869", "authors": ["John Violos", "Konstantina-Christina Diamanti", "Ioannis Kompatsiaris", "Symeon Papadopoulos"], "title": "Frugal Machine Learning for Energy-efficient, and Resource-aware Artificial Intelligence", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Frugal Machine Learning (FML) refers to the practice of designing Machine\nLearning (ML) models that are efficient, cost-effective, and mindful of\nresource constraints. This field aims to achieve acceptable performance while\nminimizing the use of computational resources, time, energy, and data for both\ntraining and inference. FML strategies can be broadly categorized into input\nfrugality, learning process frugality, and model frugality, each focusing on\nreducing resource consumption at different stages of the ML pipeline. This\nchapter explores recent advancements, applications, and open challenges in FML,\nemphasizing its importance for smart environments that incorporate edge\ncomputing and IoT devices, which often face strict limitations in bandwidth,\nenergy, or latency. Technological enablers such as model compression,\nenergy-efficient hardware, and data-efficient learning techniques are\ndiscussed, along with adaptive methods including parameter regularization,\nknowledge distillation, and dynamic architecture design that enable incremental\nmodel updates without full retraining. Furthermore, it provides a comprehensive\ntaxonomy of frugal methods, discusses case studies across diverse domains, and\nidentifies future research directions to drive innovation in this evolving\nfield.", "AI": {"tldr": "Frugal Machine Learning (FML) focuses on creating efficient and resource-conscious ML models, emphasizing input, learning process, and model frugality to minimize computational costs while maintaining performance.", "motivation": "The field aims to address resource constraints in ML, especially in edge computing and IoT environments, by reducing computational, time, and energy costs.", "method": "FML employs strategies like model compression, energy-efficient hardware, and data-efficient learning techniques, along with adaptive methods such as knowledge distillation and dynamic architecture design.", "result": "The chapter highlights advancements in FML, provides a taxonomy of frugal methods, and includes case studies across various domains.", "conclusion": "FML is crucial for resource-limited environments and offers future research directions to further innovate the field.", "keywords": "Frugal Machine Learning, resource efficiency, edge computing, model compression, IoT"}}
{"id": "2506.01952", "pdf": "https://arxiv.org/pdf/2506.01952", "abs": "https://arxiv.org/abs/2506.01952", "authors": ["Atsuyuki Miyai", "Zaiying Zhao", "Kazuki Egashira", "Atsuki Sato", "Tatsumi Sunada", "Shota Onohara", "Hiromasa Yamanishi", "Mashiro Toyooka", "Kunato Nishina", "Ryoma Maeda", "Kiyoharu Aizawa", "Toshihiko Yamasaki"], "title": "WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Page: https://webchorearena.github.io/", "summary": "Powered by a large language model (LLM), a web browsing agent operates web\nbrowsers in a human-like manner and offers a highly transparent path toward\nautomating a wide range of everyday tasks. As web agents become increasingly\ncapable and demonstrate proficiency in general browsing tasks, a critical\nquestion emerges: Can they go beyond general browsing to robustly handle tasks\nthat are tedious and complex, or chores that humans often avoid doing\nthemselves? In this paper, we introduce WebChoreArena, a new fully reproducible\nbenchmark comprising 532 carefully curated tasks designed to extend the scope\nof WebArena beyond general browsing to more labor-intensive and tedious tasks.\nWebChoreArena systematically integrates three key challenges: (i) Massive\nMemory tasks requiring accurate retrieval of large amounts of information in\nthe observations, (ii) Calculation tasks demanding precise mathematical\nreasoning, and (iii) Long-Term Memory tasks necessitating long-term memory\nacross multiple webpages. Built on top of the fully reproducible and widely\nadopted four WebArena simulation environments, WebChoreArena ensures strict\nreproducibility and enables fair, direct comparisons with the established\nWebArena benchmark, offering key insights into agent progress. Our experimental\nresults demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7\nSonnet, and Gemini 2.5 Pro, significant improvements in performance are\nobserved on WebChoreArena. These findings suggest that WebChoreArena is\nwell-suited to measure the advancement of state-of-the-art LLMs with greater\nclarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro,\nthere remains substantial room for improvement compared to WebArena,\nhighlighting the increased challenges posed by WebChoreArena.", "AI": {"tldr": "WebChoreArena\u662f\u4e00\u4e2a\u65b0\u7684\u53ef\u590d\u73b0\u57fa\u51c6\uff0c\u5305\u542b532\u4e2a\u4efb\u52a1\uff0c\u65e8\u5728\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u3001\u7e41\u7410\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6a21\u578b\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8d85\u8d8a\u5e38\u89c4\u6d4f\u89c8\u7684\u590d\u6742\u3001\u7e41\u7410\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5e38\u89c4\u6d4f\u89c8\u4efb\u52a1\u3002", "method": "\u901a\u8fc7WebChoreArena\u57fa\u51c6\uff0c\u7cfb\u7edf\u6027\u6574\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u5927\u89c4\u6a21\u8bb0\u5fc6\u4efb\u52a1\u3001\u7cbe\u786e\u8ba1\u7b97\u4efb\u52a1\u548c\u957f\u671f\u8bb0\u5fc6\u4efb\u52a1\uff0c\u5e76\u5728WebArena\u7684\u56db\u4e2a\u6a21\u62df\u73af\u5883\u4e2d\u5b9e\u73b0\u4e25\u683c\u53ef\u590d\u73b0\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u968f\u7740\u6a21\u578b\u5347\u7ea7\uff08\u5982GPT-4o\u3001Claude 3.7 Sonnet\u3001Gemini 2.5 Pro\uff09\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4f46\u4ecd\u8fdc\u672a\u8fbe\u5230WebArena\u7684\u6c34\u5e73\uff0c\u8868\u660eWebChoreArena\u7684\u6311\u6218\u6027\u66f4\u9ad8\u3002", "conclusion": "WebChoreArena\u80fd\u6709\u6548\u8861\u91cfLLM\u7684\u8fdb\u5c55\uff0c\u4f46\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u4ecd\u6709\u8f83\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, WebChoreArena, \u57fa\u51c6\u6d4b\u8bd5, \u590d\u6742\u4efb\u52a1, \u673a\u5668\u6d4f\u89c8\u4ee3\u7406"}}
{"id": "2506.01876", "pdf": "https://arxiv.org/pdf/2506.01876", "abs": "https://arxiv.org/abs/2506.01876", "authors": ["Alessio Russo", "Ryan Welch", "Aldo Pacchiano"], "title": "Learning to Explore: An In-Context Learning Approach for Pure Exploration", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this work, we study the active sequential hypothesis testing problem, also\nknown as pure exploration, where the goal is to actively control a data\ncollection process to efficiently identify the correct hypothesis underlying a\ndecision problem. While relevant across multiple domains, devising adaptive\nexploration strategies remains challenging, particularly due to difficulties in\nencoding appropriate inductive biases. Existing Reinforcement Learning\n(RL)-based methods often underperform when relevant information structures are\ninadequately represented, whereas more complex methods, like Best Arm\nIdentification (BAI) techniques, may be difficult to devise and typically rely\non explicit modeling assumptions. To address these limitations, we introduce\nIn-Context Pure Exploration (ICPE), an in-context learning approach that uses\nTransformers to learn exploration strategies directly from experience. ICPE\ncombines supervised learning and reinforcement learning to identify and exploit\nlatent structure across related tasks, without requiring prior assumptions.\nNumerical results across diverse synthetic and semi-synthetic benchmarks\nhighlight ICPE's capability to achieve robust performance performance in\ndeterministic, stochastic, and structured settings. These results demonstrate\nICPE's ability to match optimal instance-dependent algorithms using only deep\nlearning techniques, making it a practical and general approach to\ndata-efficient exploration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u7eaf\u63a2\u7d22\u65b9\u6cd5\uff08ICPE\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u5b66\u4e60\u63a2\u7d22\u7b56\u7565\uff0c\u65e0\u9700\u5148\u9a8c\u5047\u8bbe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9002\u5e94\u6027\u63a2\u7d22\u7b56\u7565\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5f53\u4fe1\u606f\u7ed3\u6784\u672a\u80fd\u5145\u5206\u8868\u5f81\u65f6\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u663e\u5f0f\u5efa\u6a21\u5047\u8bbe\u7684\u65b9\u6cd5\u3002", "method": "ICPE\u91c7\u7528Transformer\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u6280\u672f\uff0c\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\uff0c\u76f4\u63a5\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u5728\u591a\u79cd\u5408\u6210\u548c\u534a\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cICPE\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u80fd\uff0c\u80fd\u591f\u5339\u914d\u6700\u4f18\u5b9e\u4f8b\u4f9d\u8d56\u7b97\u6cd5\uff0c\u4ec5\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u63a2\u7d22\u3002", "conclusion": "ICPE\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u901a\u7528\u7684\u6570\u636e\u9ad8\u6548\u63a2\u7d22\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u786e\u5b9a\u6027\u3001\u968f\u673a\u6027\u548c\u7ed3\u6784\u5316\u573a\u666f\u3002", "keywords": "\u4e3b\u52a8\u5b66\u4e60,\u7eaf\u63a2\u7d22,\u5f3a\u5316\u5b66\u4e60,Transformer,\u4e0a\u4e0b\u6587\u5b66\u4e60"}}
{"id": "2506.01954", "pdf": "https://arxiv.org/pdf/2506.01954", "abs": "https://arxiv.org/abs/2506.01954", "authors": ["Jennifer Chen", "Aidar Myrzakhan", "Yaxin Luo", "Hassaan Muhammad Khan", "Sondos Mahmoud Bsharat", "Zhiqiang Shen"], "title": "DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Main. Code is available at https://github.com/VILA-Lab/DRAG", "summary": "Retrieval-Augmented Generation (RAG) methods have proven highly effective for\ntasks requiring factual consistency and robust knowledge retrieval. However,\nlarge-scale RAG systems consume significant computational resources and are\nprone to generating hallucinated content from Humans. In this work, we\nintroduce $\\texttt{DRAG}$, a novel framework for distilling RAG knowledge from\nlarge-scale Language Models (LLMs) into small LMs (SLMs). Our approach\nleverages evidence- and knowledge graph-based distillation, ensuring that the\ndistilled model retains critical factual knowledge while significantly reducing\nmodel size and computational cost. By aligning the smaller model's predictions\nwith a structured knowledge graph and ranked evidence, $\\texttt{DRAG}$\neffectively mitigates hallucinations and improves factual accuracy. We further\npresent a case demonstrating how our framework mitigates user privacy risks and\nintroduce a corresponding benchmark. Experimental evaluations on multiple\nbenchmarks demonstrate that our method outperforms the prior competitive RAG\nmethods like MiniRAG for SLMs by up to 27.7% using the same models, preserving\nhigh-level efficiency and reliability. With $\\texttt{DRAG}$, we provide a\npractical and resource-efficient roadmap to deploying enhanced retrieval and\ngeneration capabilities in small-sized LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DRAG \u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u77e5\u8bc6\u538b\u7f29\u5230\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u4e2d\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u51cf\u5c11\u5e7b\u89c9\u5185\u5bb9\uff0c\u540c\u65f6\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u867d\u7136\u6709\u6548\uff0c\u4f46\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u5185\u5bb9\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\u3002", "method": "DRAG \u6846\u67b6\u5229\u7528\u4e86\u57fa\u4e8e\u8bc1\u636e\u548c\u77e5\u8bc6\u56fe\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u786e\u4fdd\u5c0f\u578b\u6a21\u578b\u5728\u51cf\u5c11\u89c4\u6a21\u7684\u540c\u65f6\u4fdd\u7559\u5173\u952e\u4e8b\u5b9e\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDRAG \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\uff08\u5982 MiniRAG\uff09\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe 27.7%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u548c\u53ef\u9760\u6027\u3002", "conclusion": "DRAG \u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u5c0f\u578b\u6a21\u578b\u4e2d\u5b9e\u73b0\u589e\u5f3a\u7684\u68c0\u7d22\u548c\u751f\u6210\u80fd\u529b\u3002", "keywords": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210, \u77e5\u8bc6\u84b8\u998f, \u5e7b\u89c9\u5185\u5bb9, \u5c0f\u578b\u8bed\u8a00\u6a21\u578b, \u77e5\u8bc6\u56fe"}}
{"id": "2506.01883", "pdf": "https://arxiv.org/pdf/2506.01883", "abs": "https://arxiv.org/abs/2506.01883", "authors": ["Davide D'Ascenzo", "Sebastiano Cultrera di Montesano"], "title": "scDataset: Scalable Data Loading for Deep Learning on Large-Scale Single-Cell Omics", "categories": ["cs.LG", "cs.AI", "cs.DB", "68T07, 68P05, 92C40", "I.2.6; H.2.8; J.3"], "comment": null, "summary": "Modern single-cell datasets now comprise hundreds of millions of cells,\npresenting significant challenges for training deep learning models that\nrequire shuffled, memory-efficient data loading. While the AnnData format is\nthe community standard for storing single-cell datasets, existing data loading\nsolutions for AnnData are often inadequate: some require loading all data into\nmemory, others convert to dense formats that increase storage demands, and many\nare hampered by slow random disk access. We present scDataset, a PyTorch\nIterableDataset that operates directly on one or more AnnData files without the\nneed for format conversion. The core innovation is a combination of block\nsampling and batched fetching, which together balance randomness and I/O\nefficiency. On the Tahoe 100M dataset, scDataset achieves up to a 48$\\times$\nspeed-up over AnnLoader, a 27$\\times$ speed-up over HuggingFace Datasets, and\nan 18$\\times$ speed-up over BioNeMo in single-core settings. These advances\ndemocratize large-scale single-cell model training for the broader research\ncommunity.", "AI": {"tldr": "scDataset\u662f\u4e00\u4e2a\u76f4\u63a5\u5728AnnData\u6587\u4ef6\u4e0a\u64cd\u4f5c\u7684PyTorch IterableDataset\uff0c\u901a\u8fc7\u5757\u91c7\u6837\u548c\u6279\u91cf\u8bfb\u53d6\u63d0\u9ad8\u4e86\u5355\u7ec6\u80de\u6a21\u578b\u8bad\u7ec3\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u4ee3\u5355\u7ec6\u80de\u6570\u636e\u96c6\u89c4\u6a21\u5e9e\u5927\uff0c\u73b0\u6709\u6570\u636e\u52a0\u8f7d\u65b9\u6cd5\u5185\u5b58\u6548\u7387\u4f4e\u3001\u5b58\u50a8\u9700\u6c42\u9ad8\u4e14\u901f\u5ea6\u6162\uff0c\u6025\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u5757\u91c7\u6837\u548c\u6279\u91cf\u8bfb\u53d6\u6280\u672f\uff0cscDataset\u76f4\u63a5\u5728AnnData\u6587\u4ef6\u4e0a\u64cd\u4f5c\uff0c\u65e0\u9700\u683c\u5f0f\u8f6c\u6362\u3002", "result": "\u5728Tahoe 100M\u6570\u636e\u96c6\u4e0a\uff0cscDataset\u6bd4\u73b0\u6709\u5de5\u5177\u5feb18-48\u500d\u3002", "conclusion": "scDataset\u4e3a\u5927\u89c4\u6a21\u5355\u7ec6\u80de\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5355\u7ec6\u80de\u6570\u636e,\u6df1\u5ea6\u5b66\u4e60,\u6570\u636e\u52a0\u8f7d,AnnData,scDataset"}}
{"id": "2311.03057", "pdf": "https://arxiv.org/pdf/2311.03057", "abs": "https://arxiv.org/abs/2311.03057", "authors": ["Sunkyung Lee", "Minjin Choi", "Jongwuk Lee"], "title": "GLEN: Generative Retrieval via Lexical Index Learning", "categories": ["cs.IR", "cs.CL"], "comment": "In Proceedings of the 2023 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2023) main conference. 12 pages, 2 figures, 8\n  tables", "summary": "Generative retrieval shed light on a new paradigm of document retrieval,\naiming to directly generate the identifier of a relevant document for a query.\nWhile it takes advantage of bypassing the construction of auxiliary index\nstructures, existing studies face two significant challenges: (i) the\ndiscrepancy between the knowledge of pre-trained language models and\nidentifiers and (ii) the gap between training and inference that poses\ndifficulty in learning to rank. To overcome these challenges, we propose a\nnovel generative retrieval method, namely Generative retrieval via LExical\niNdex learning (GLEN). For training, GLEN effectively exploits a dynamic\nlexical identifier using a two-phase index learning strategy, enabling it to\nlearn meaningful lexical identifiers and relevance signals between queries and\ndocuments. For inference, GLEN utilizes collision-free inference, using\nidentifier weights to rank documents without additional overhead. Experimental\nresults prove that GLEN achieves state-of-the-art or competitive performance\nagainst existing generative retrieval methods on various benchmark datasets,\ne.g., NQ320k, MS MARCO, and BEIR. The code is available at\nhttps://github.com/skleee/GLEN.", "AI": {"tldr": "GLEN\u901a\u8fc7\u52a8\u6001\u8bcd\u6c47\u6807\u8bc6\u548c\u4e24\u9636\u6bb5\u7d22\u5f15\u5b66\u4e60\u7b56\u7565\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u68c0\u7d22\u4e2d\u7684\u77e5\u8bc6\u4e0d\u4e00\u81f4\u548c\u8bad\u7ec3-\u63a8\u7406\u5dee\u8ddd\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u68c0\u7d22\u3002", "motivation": "\u751f\u6210\u5f0f\u68c0\u7d22\u76f4\u63a5\u751f\u6210\u6587\u6863\u6807\u8bc6\u7b26\uff0c\u4f46\u9762\u4e34\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u4e0d\u4e00\u81f4\u548c\u8bad\u7ec3-\u63a8\u7406\u5dee\u8ddd\u4e24\u5927\u6311\u6218\uff0c\u9700\u63d0\u51fa\u65b0\u65b9\u6cd5\u89e3\u51b3\u3002", "method": "GLEN\u91c7\u7528\u52a8\u6001\u8bcd\u6c47\u6807\u8bc6\u548c\u4e24\u9636\u6bb5\u7d22\u5f15\u5b66\u4e60\u7b56\u7565\uff0c\u5b66\u4e60\u6709\u610f\u4e49\u7684\u8bcd\u6c47\u6807\u8bc6\u7b26\u548c\u67e5\u8be2-\u6587\u6863\u76f8\u5173\u6027\u4fe1\u53f7\uff1b\u63a8\u7406\u65f6\u4f7f\u7528\u65e0\u78b0\u649e\u63a8\u65ad\uff0c\u901a\u8fc7\u6807\u8bc6\u7b26\u6743\u91cd\u6392\u540d\u6587\u6863\u3002", "result": "GLEN\u5728NQ320k\u3001MS MARCO\u548cBEIR\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u65b0\u6216\u7ade\u4e89\u529b\u6027\u80fd\u3002", "conclusion": "GLEN\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u514b\u670d\u4e86\u751f\u6210\u5f0f\u68c0\u7d22\u7684\u6311\u6218\uff0c\u5c55\u73b0\u4e86\u9ad8\u6548\u7684\u68c0\u7d22\u80fd\u529b\u3002", "keywords": "\u751f\u6210\u5f0f\u68c0\u7d22\u3001\u52a8\u6001\u8bcd\u6c47\u6807\u8bc6\u3001\u4e24\u9636\u6bb5\u7d22\u5f15\u5b66\u4e60\u3001\u65e0\u78b0\u649e\u63a8\u65ad\u3001\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2506.00714", "pdf": "https://arxiv.org/pdf/2506.00714", "abs": "https://arxiv.org/abs/2506.00714", "authors": ["Mingwei Zheng", "Chengpeng Wang", "Xuwei Liu", "Jinyao Guo", "Shiwei Feng", "Xiangyu Zhang"], "title": "An LLM Agent for Functional Bug Detection in Network Protocols", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Functional correctness is critical for ensuring the reliability and security\nof network protocol implementations. Functional bugs, instances where\nimplementations diverge from behaviors specified in RFC documents, can lead to\nsevere consequences, including faulty routing, authentication bypasses, and\nservice disruptions. Detecting these bugs requires deep semantic analysis\nacross specification documents and source code, a task beyond the capabilities\nof traditional static analysis tools. This paper introduces RFCScan, an\nautonomous agent that leverages large language models (LLMs) to detect\nfunctional bugs by checking conformance between network protocol\nimplementations and their RFC specifications. Inspired by the human auditing\nprocedure, RFCScan comprises two key components: an indexing agent and a\ndetection agent. The former hierarchically summarizes protocol code semantics,\ngenerating semantic indexes that enable the detection agent to narrow down the\nscanning scope. The latter employs demand-driven retrieval to iteratively\ncollect additional relevant data structures and functions, eventually\nidentifying potential inconsistencies with the RFC specifications effectively.\nWe evaluate RFCScan across six real-world network protocol implementations.\nRFCScan identifies 47 functional bugs with 81.9% precision, of which 20 bugs\nhave been confirmed or fixed by developers.", "AI": {"tldr": "RFCScan\u662f\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u7f51\u7edc\u534f\u8bae\u5b9e\u73b0\u4e2d\u529f\u80fd\u6027\u9519\u8bef\u7684\u81ea\u4e3b\u4ee3\u7406\u5de5\u5177\uff0c\u5176\u901a\u8fc7\u5bf9\u6bd4RFC\u89c4\u8303\u4e0e\u5b9e\u73b0\u4ee3\u7801\u7684\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u9519\u8bef\u68c0\u6d4b\u3002", "motivation": "\u529f\u80fd\u6027\u9519\u8bef\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u7f51\u7edc\u95ee\u9898\uff0c\u4f20\u7edf\u9759\u6001\u5206\u6790\u5de5\u5177\u96be\u4ee5\u68c0\u6d4b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u534f\u8bae\u5b9e\u73b0\u7684\u6b63\u786e\u6027\u3002", "method": "RFCScan\u5305\u542b\u7d22\u5f15\u4ee3\u7406\u548c\u68c0\u6d4b\u4ee3\u7406\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff0c\u524d\u8005\u7528\u4e8e\u751f\u6210\u8bed\u4e49\u7d22\u5f15\uff0c\u540e\u8005\u901a\u8fc7\u9700\u6c42\u9a71\u52a8\u7684\u68c0\u7d22\u53d1\u73b0\u6f5c\u5728\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u7f51\u7edc\u534f\u8bae\u5b9e\u73b0\u4e2d\uff0cRFCScan\u68c0\u6d4b\u523047\u4e2a\u529f\u80fd\u9519\u8bef\uff0c\u7cbe\u5ea6\u8fbe81.9%\uff0c\u5176\u4e2d20\u4e2a\u9519\u8bef\u5df2\u88ab\u5f00\u53d1\u8005\u786e\u8ba4\u6216\u4fee\u590d\u3002", "conclusion": "RFCScan\u901a\u8fc7\u7ed3\u5408LLM\u548c\u5206\u5c42\u8bed\u4e49\u5206\u6790\uff0c\u4e3a\u7f51\u7edc\u534f\u8bae\u7684\u529f\u80fd\u6b63\u786e\u6027\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u529f\u80fd\u6027\u9519\u8bef\u68c0\u6d4b, RFC\u89c4\u8303, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u7f51\u7edc\u534f\u8bae\u5b9e\u73b0"}}
{"id": "2506.01884", "pdf": "https://arxiv.org/pdf/2506.01884", "abs": "https://arxiv.org/abs/2506.01884", "authors": ["Gene Li"], "title": "Agnostic Reinforcement Learning: Foundations and Algorithms", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Ph.D. thesis", "summary": "Reinforcement Learning (RL) has demonstrated tremendous empirical success\nacross numerous challenging domains. However, we lack a strong theoretical\nunderstanding of the statistical complexity of RL in environments with large\nstate spaces, where function approximation is required for sample-efficient\nlearning. This thesis addresses this gap by rigorously examining the\nstatistical complexity of RL with function approximation from a learning\ntheoretic perspective. Departing from a long history of prior work, we consider\nthe weakest form of function approximation, called agnostic policy learning, in\nwhich the learner seeks to find the best policy in a given class $\\Pi$, with no\nguarantee that $\\Pi$ contains an optimal policy for the underlying task.\n  We systematically explore agnostic policy learning along three key axes:\nenvironment access -- how a learner collects data from the environment;\ncoverage conditions -- intrinsic properties of the underlying MDP measuring the\nexpansiveness of state-occupancy measures for policies in the class $\\Pi$, and\nrepresentational conditions -- structural assumptions on the class $\\Pi$\nitself. Within this comprehensive framework, we (1) design new learning\nalgorithms with theoretical guarantees and (2) characterize fundamental\nperformance bounds of any algorithm. Our results reveal significant statistical\nseparations that highlight the power and limitations of agnostic policy\nlearning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u72b6\u6001\u7a7a\u95f4\u5927\u3001\u9700\u8981\u51fd\u6570\u903c\u8fd1\u7684\u73af\u5883\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u7edf\u8ba1\u590d\u6742\u6027\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5b66\u4e60\u7406\u8bba\u89c6\u89d2\u7684\u4e25\u683c\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\uff08\u73af\u5883\u8bbf\u95ee\u3001\u8986\u76d6\u6761\u4ef6\u548c\u8868\u793a\u6761\u4ef6\uff09\u7cfb\u7edf\u63a2\u7d22\u4e86\u65e0\u5148\u9a8c\u77e5\u8bc6\u7b56\u7565\u5b66\u4e60\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u5728\u8bb8\u591a\u9886\u57df\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u4f46\u5bf9\u4e8e\u5927\u89c4\u6a21\u72b6\u6001\u7a7a\u95f4\u4e2d\u51fd\u6570\u903c\u8fd1\u7684\u7edf\u8ba1\u590d\u6742\u6027\u7f3a\u4e4f\u7406\u8bba\u7406\u89e3\uff0c\u56e0\u6b64\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u6210\u4e3a\u7814\u7a76\u52a8\u673a\u3002", "method": "\u901a\u8fc7\u65e0\u5148\u9a8c\u77e5\u8bc6\u7b56\u7565\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7814\u7a76\u73af\u5883\u8bbf\u95ee\u3001\u8986\u76d6\u6761\u4ef6\u548c\u8868\u793a\u6761\u4ef6\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u8bbe\u8ba1\u65b0\u7b97\u6cd5\u5e76\u5206\u6790\u5176\u7406\u8bba\u6027\u80fd\u754c\u9650\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u65e0\u5148\u9a8c\u77e5\u8bc6\u7b56\u7565\u5b66\u4e60\u7684\u7edf\u8ba1\u5206\u79bb\uff0c\u5c55\u793a\u4e86\u5176\u80fd\u529b\u4e0e\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u65b0\u7b97\u6cd5\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u5f3a\u5316\u5b66\u4e60\u7684\u7edf\u8ba1\u590d\u6742\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u51fd\u6570\u903c\u8fd1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u65e0\u5148\u9a8c\u77e5\u8bc6\u7b56\u7565\u5b66\u4e60\u7684\u6f5c\u529b\u4e0e\u9650\u5236\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u7edf\u8ba1\u590d\u6742\u6027, \u51fd\u6570\u903c\u8fd1, \u65e0\u5148\u9a8c\u77e5\u8bc6\u7b56\u7565\u5b66\u4e60"}}
{"id": "2506.00001", "pdf": "https://arxiv.org/pdf/2506.00001", "abs": "https://arxiv.org/abs/2506.00001", "authors": ["Qun-Kai Lin", "Cheng Hsu", "Tian-Sheuan Chang"], "title": "Enhancing Finite State Machine Design Automation with Large Language Models and Prompt Engineering Techniques", "categories": ["cs.AR", "cs.CL"], "comment": "published in 2024 IEEE Asia Pacific Conference on Circuits and\n  Systems (APCCAS 2024)", "summary": "Large Language Models (LLMs) have attracted considerable attention in recent\nyears due to their remarkable compatibility with Hardware Description Language\n(HDL) design. In this paper, we examine the performance of three major LLMs,\nClaude 3 Opus, ChatGPT-4, and ChatGPT-4o, in designing finite state machines\n(FSMs). By utilizing the instructional content provided by HDLBits, we evaluate\nthe stability, limitations, and potential approaches for improving the success\nrates of these models. Furthermore, we explore the impact of using the\nprompt-refining method, To-do-Oriented Prompting (TOP) Patch, on the success\nrate of these LLM models in various FSM design scenarios. The results show that\nthe systematic format prompt method and the novel prompt refinement method have\nthe potential to be applied to other domains beyond HDL design automation,\nconsidering its possible integration with other prompt engineering techniques\nin the future.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e09\u79cd\u4e3b\u8981\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Claude 3 Opus\u3001ChatGPT-4\u548cChatGPT-4o\uff09\u5728\u8bbe\u8ba1\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u4e86\u5176\u7a33\u5b9a\u6027\u3001\u5c40\u9650\u6027\u53ca\u6539\u8fdb\u65b9\u6cd5\uff0c\u5e76\u63a2\u7d22\u4e86\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff08TOP Patch\uff09\u5bf9\u5176\u6210\u529f\u7387\u7684\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u786c\u4ef6\u63cf\u8ff0\u8bed\u8a00\uff08HDL\uff09\u8bbe\u8ba1\u4e2d\u7684\u663e\u8457\u517c\u5bb9\u6027\uff0c\u7814\u7a76\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u53ca\u6539\u8fdb\u65b9\u6cd5\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "method": "\u901a\u8fc7HDLBits\u63d0\u4f9b\u7684\u6559\u5b66\u5185\u5bb9\uff0c\u8bc4\u4f30\u6a21\u578b\u5728FSM\u8bbe\u8ba1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff08TOP Patch\uff09\u5bf9\u6210\u529f\u7387\u7684\u5f71\u54cd\u3002", "result": "\u7cfb\u7edf\u6027\u683c\u5f0f\u63d0\u793a\u65b9\u6cd5\u548c\u65b0\u578b\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u6709\u671b\u5e94\u7528\u4e8eHDL\u8bbe\u8ba1\u81ea\u52a8\u5316\u4ee5\u5916\u7684\u5176\u4ed6\u9886\u57df\uff0c\u5e76\u53ef\u80fd\u4e0e\u5176\u4ed6\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u7ed3\u5408\u3002", "conclusion": "\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff08\u5982TOP Patch\uff09\u5bf9\u63d0\u5347LLM\u5728FSM\u8bbe\u8ba1\u4e2d\u7684\u6210\u529f\u7387\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u8fdb\u4e00\u6b65\u63a8\u5e7f\u81f3\u5176\u4ed6\u9886\u57df\u3002", "keywords": "Large Language Models, Hardware Description Language, Finite State Machine, Prompt Engineering"}}
{"id": "2506.00718", "pdf": "https://arxiv.org/pdf/2506.00718", "abs": "https://arxiv.org/abs/2506.00718", "authors": ["Tianqin Li", "Ziqi Wen", "Leiran Song", "Jun Liu", "Zhi Jing", "Tai Sing Lee"], "title": "From Local Cues to Global Percepts: Emergent Gestalt Organization in Self-Supervised Vision Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human vision organizes local cues into coherent global forms using Gestalt\nprinciples like closure, proximity, and figure-ground assignment -- functions\nreliant on global spatial structure. We investigate whether modern vision\nmodels show similar behaviors, and under what training conditions these emerge.\nWe find that Vision Transformers (ViTs) trained with Masked Autoencoding (MAE)\nexhibit activation patterns consistent with Gestalt laws, including illusory\ncontour completion, convexity preference, and dynamic figure-ground\nsegregation. To probe the computational basis, we hypothesize that modeling\nglobal dependencies is necessary for Gestalt-like organization. We introduce\nthe Distorted Spatial Relationship Testbench (DiSRT), which evaluates\nsensitivity to global spatial perturbations while preserving local textures.\nUsing DiSRT, we show that self-supervised models (e.g., MAE, CLIP) outperform\nsupervised baselines and sometimes even exceed human performance. ConvNeXt\nmodels trained with MAE also exhibit Gestalt-compatible representations,\nsuggesting such sensitivity can arise without attention architectures. However,\nclassification finetuning degrades this ability. Inspired by biological vision,\nwe show that a Top-K activation sparsity mechanism can restore global\nsensitivity. Our findings identify training conditions that promote or suppress\nGestalt-like perception and establish DiSRT as a diagnostic for global\nstructure sensitivity across models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u73b0\u4ee3\u89c6\u89c9\u6a21\u578b\u662f\u5426\u8868\u73b0\u51fa\u7c7b\u4f3c\u683c\u5f0f\u5854\u539f\u5219\u7684\u884c\u4e3a\uff0c\u5e76\u63a2\u8ba8\u8bad\u7ec3\u6761\u4ef6\u5982\u4f55\u5f71\u54cd\u8fd9\u4e9b\u884c\u4e3a\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4f7f\u7528MAE\u8bad\u7ec3\u7684ViTs\u8868\u73b0\u51fa\u4e0e\u683c\u5f0f\u5854\u6cd5\u5219\u4e00\u81f4\u7684\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u5e76\u63d0\u51faDiSRT\u6d4b\u8bd5\u5e73\u53f0\u8bc4\u4f30\u5168\u5c40\u7ed3\u6784\u7684\u654f\u611f\u6027\u3002", "motivation": "\u63a2\u8ba8\u73b0\u4ee3\u89c6\u89c9\u6a21\u578b\u662f\u5426\u80fd\u50cf\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u4e00\u6837\u5229\u7528\u683c\u5f0f\u5854\u539f\u5219\uff08\u5982\u95ed\u5408\u6027\u3001\u90bb\u8fd1\u6027\uff09\u7ec4\u7ec7\u5c40\u90e8\u4fe1\u606f\uff0c\u4ee5\u53ca\u8bad\u7ec3\u6761\u4ef6\u5982\u4f55\u5f71\u54cd\u8fd9\u79cd\u80fd\u529b\u3002", "method": "\u901a\u8fc7Distorted Spatial Relationship Testbench (DiSRT)\u6d4b\u8bd5\u6a21\u578b\u5bf9\u5168\u5c40\u7a7a\u95f4\u6270\u52a8\u7684\u654f\u611f\u6027\uff0c\u7814\u7a76\u4e86MAE-trained ViTs\u548cConvNeXt\u7b49\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u81ea\u76d1\u7763\u6a21\u578b\uff08\u5982MAE\u3001CLIP\uff09\u5728DiSRT\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u76d1\u7763\u57fa\u51c6\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8d85\u8fc7\u4eba\u7c7b\u8868\u73b0\u3002\u5206\u7c7b\u5fae\u8c03\u4f1a\u524a\u5f31\u6a21\u578b\u5bf9\u5168\u5c40\u7ed3\u6784\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u7279\u5b9a\u8bad\u7ec3\u6761\u4ef6\uff08\u5982\u81ea\u76d1\u7763\u5b66\u4e60\uff09\u80fd\u4fc3\u8fdb\u683c\u5f0f\u5854\u7c7b\u4f3c\u884c\u4e3a\uff0c\u800c\u5206\u7c7b\u4efb\u52a1\u4f1a\u6291\u5236\u8fd9\u79cd\u80fd\u529b\u3002DiSRT\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u6a21\u578b\u5168\u5c40\u7ed3\u6784\u654f\u611f\u6027\u7684\u8bca\u65ad\u5de5\u5177\u3002", "keywords": "Gestalt principles, Vision Transformers, Masked Autoencoding, DiSRT, global perception"}}
{"id": "2506.01890", "pdf": "https://arxiv.org/pdf/2506.01890", "abs": "https://arxiv.org/abs/2506.01890", "authors": ["David Ortiz-Perez", "Manuel Benavent-Lledo", "Javier Rodriguez-Juan", "Jose Garcia-Rodriguez", "David Tom\u00e1s"], "title": "CogniAlign: Word-Level Multimodal Speech Alignment with Gated Cross-Attention for Alzheimer's Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Early detection of cognitive disorders such as Alzheimer's disease is\ncritical for enabling timely clinical intervention and improving patient\noutcomes. In this work, we introduce CogniAlign, a multimodal architecture for\nAlzheimer's detection that integrates audio and textual modalities, two\nnon-intrusive sources of information that offer complementary insights into\ncognitive health. Unlike prior approaches that fuse modalities at a coarse\nlevel, CogniAlign leverages a word-level temporal alignment strategy that\nsynchronizes audio embeddings with corresponding textual tokens based on\ntranscription timestamps. This alignment supports the development of\ntoken-level fusion techniques, enabling more precise cross-modal interactions.\nTo fully exploit this alignment, we propose a Gated Cross-Attention Fusion\nmechanism, where audio features attend over textual representations, guided by\nthe superior unimodal performance of the text modality. In addition, we\nincorporate prosodic cues, specifically interword pauses, by inserting pause\ntokens into the text and generating audio embeddings for silent intervals,\nfurther enriching both streams. We evaluate CogniAlign on the ADReSSo dataset,\nwhere it achieves an accuracy of 90.36%, outperforming existing\nstate-of-the-art methods. A detailed ablation study confirms the advantages of\nour alignment strategy, attention-based fusion, and prosodic modeling.", "AI": {"tldr": "CogniAlign\u662f\u4e00\u79cd\u591a\u6a21\u6001\u67b6\u6784\uff0c\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u65e9\u671f\u68c0\u6d4b\uff0c\u901a\u8fc7\u97f3\u9891\u548c\u6587\u672c\u6a21\u6001\u7684\u7cbe\u7ec6\u5bf9\u9f50\u548c\u878d\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u65e9\u671f\u53d1\u73b0\u8ba4\u77e5\u969c\u788d\uff08\u5982\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff09\u5bf9\u53ca\u65f6\u5e72\u9884\u548c\u6539\u5584\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u878d\u5408\u4e0a\u8f83\u4e3a\u7c97\u7cd9\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4e92\u8865\u4fe1\u606f\u3002", "method": "CogniAlign\u91c7\u7528\u8bcd\u7ea7\u65f6\u95f4\u5bf9\u9f50\u7b56\u7565\uff0c\u5c06\u97f3\u9891\u5d4c\u5165\u4e0e\u6587\u672c\u6807\u8bb0\u540c\u6b65\uff0c\u5e76\u63d0\u51fa\u4e86\u95e8\u63a7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u673a\u5236\uff0c\u540c\u65f6\u6574\u5408\u4e86\u97f5\u5f8b\u7ebf\u7d22\uff08\u5982\u505c\u987f\u6807\u8bb0\uff09\u3002", "result": "\u5728ADReSSo\u6570\u636e\u96c6\u4e0a\uff0cCogniAlign\u8fbe\u523090.36%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "CogniAlign\u901a\u8fc7\u7cbe\u7ec6\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u548c\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "keywords": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u3001\u591a\u6a21\u6001\u878d\u5408\u3001\u97f3\u9891\u6587\u672c\u5bf9\u9f50\u3001\u95e8\u63a7\u6ce8\u610f\u529b\u3001\u97f5\u5f8b\u5efa\u6a21"}}
{"id": "2506.00003", "pdf": "https://arxiv.org/pdf/2506.00003", "abs": "https://arxiv.org/abs/2506.00003", "authors": ["Arjun Prasaath Anbazhagan", "Parteek Kumar", "Ujjwal Kaur", "Aslihan Akalin", "Kevin Zhu", "Sean O'Brien"], "title": "Probing Audio-Generation Capabilities of Text-Based Language Models", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted at Conference of the North American Chapter of the\n  Association for Computational Linguistics 2025, Student Research Workshop\n  (NAACL SRW)", "summary": "How does textual representation of audio relate to the Large Language Model's\n(LLMs) learning about the audio world? This research investigates the extent to\nwhich LLMs can be prompted to generate audio, despite their primary training in\ntextual data. We employ a three-tier approach, progressively increasing the\ncomplexity of audio generation: 1) Musical Notes, 2) Environmental Sounds, and\n3) Human Speech. To bridge the gap between text and audio, we leverage code as\nan intermediary, prompting LLMs to generate code that, when executed, produces\nthe desired audio output. To evaluate the quality and accuracy of the generated\naudio, we employ FAD and CLAP scores. Our findings reveal that while LLMs can\ngenerate basic audio features, their performance deteriorates as the complexity\nof the audio increases. This suggests that while LLMs possess a latent\nunderstanding of the auditory world, their ability to translate this\nunderstanding into tangible audio output remains rudimentary. Further research\ninto techniques that can enhance the quality and diversity of LLM-generated\naudio can lead to an improvement in the performance of text-based LLMs in\ngenerating audio.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86LLMs\u5982\u4f55\u901a\u8fc7\u6587\u672c\u751f\u6210\u97f3\u9891\uff0c\u5e76\u6d4b\u8bd5\u4e86\u5176\u5728\u4e0d\u540c\u97f3\u9891\u590d\u6742\u5ea6\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u6587\u672c\u8bad\u7ec3\u57fa\u7840\u4e0a\u751f\u6210\u97f3\u9891\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u4e09\u6b65\u6e10\u8fdb\u65b9\u6cd5\uff08\u97f3\u7b26\u3001\u73af\u5883\u58f0\u97f3\u3001\u4eba\u58f0\uff09\uff0c\u4ee5\u4ee3\u7801\u4e3a\u4e2d\u4ecb\u751f\u6210\u97f3\u9891\u3002", "result": "LLMs\u80fd\u751f\u6210\u57fa\u7840\u97f3\u9891\uff0c\u4f46\u968f\u590d\u6742\u5ea6\u589e\u52a0\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "LLMs\u5177\u5907\u97f3\u9891\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u751f\u6210\u80fd\u529b\u6709\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "keywords": "LLMs,\u97f3\u9891\u751f\u6210,\u6587\u672c\u8f6c\u97f3\u9891,FAD,CLAP"}}
{"id": "2506.01897", "pdf": "https://arxiv.org/pdf/2506.01897", "abs": "https://arxiv.org/abs/2506.01897", "authors": ["Wei Shen", "Yaxiang Zhang", "Minhui Huang", "Mengfan Xu", "Jiawei Zhang", "Cong Shen"], "title": "MLorc: Momentum Low-rank Compression for Large Language Model Adaptation", "categories": ["cs.LG", "cs.IT", "math.IT", "math.OC"], "comment": null, "summary": "With increasing size of large language models (LLMs), full-parameter\nfine-tuning imposes substantial memory demands. To alleviate this, we propose a\nnovel memory-efficient training paradigm called Momentum Low-rank compression\n(MLorc). By directly compressing and reconstructing momentum rather than\ngradients, MLorc avoids imposing a fixed-rank constraint on weight update\nmatrices and better preserves the training dynamics of full-parameter\nfine-tuning, in contrast to existing low-rank approaches such as LoRA and\nGaLore. Empirically, MLorc consistently outperforms other memory-efficient\ntraining methods, matches or even exceeds the performance of full fine-tuning\nwith a small rank (e.g., $r=4$), and generalizes well across different\noptimizers -- all while not compromising time or memory efficiency.\nFurthermore, we provide a theoretical guarantee for its convergence under\nreasonable assumptions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMLorc\u7684\u5185\u5b58\u9ad8\u6548\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u538b\u7f29\u548c\u91cd\u6784\u52a8\u91cf\u800c\u975e\u68af\u5ea6\uff0c\u907f\u514d\u4e86\u6743\u91cd\u66f4\u65b0\u77e9\u9635\u7684\u56fa\u5b9a\u79e9\u7ea6\u675f\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6027\u80fd\u63a5\u8fd1\u751a\u81f3\u8d85\u8d8a\u5168\u53c2\u6570\u5fae\u8c03\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u7684\u589e\u52a0\uff0c\u5168\u53c2\u6570\u5fae\u8c03\u5bf9\u5185\u5b58\u9700\u6c42\u5de8\u5927\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u91c7\u7528MLorc\u65b9\u6cd5\uff0c\u76f4\u63a5\u538b\u7f29\u548c\u91cd\u6784\u52a8\u91cf\uff0c\u907f\u514d\u6743\u91cd\u66f4\u65b0\u77e9\u9635\u7684\u56fa\u5b9a\u79e9\u7ea6\u675f\uff0c\u4fdd\u7559\u5168\u53c2\u6570\u5fae\u8c03\u7684\u8bad\u7ec3\u52a8\u6001\u3002", "result": "MLorc\u5728\u4fdd\u6301\u65f6\u95f4\u548c\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u5185\u5b58\u9ad8\u6548\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63a5\u8fd1\u751a\u81f3\u8d85\u8d8a\u5168\u53c2\u6570\u5fae\u8c03\uff0c\u4e14\u5bf9\u4e0d\u540c\u4f18\u5316\u5668\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u3002", "conclusion": "MLorc\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fae\u8c03\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5185\u5b58\u9ad8\u6548\u8bad\u7ec3\uff0c\u52a8\u91cf\u538b\u7f29\uff0c\u4f4e\u79e9\u65b9\u6cd5\uff0cMLorc"}}
{"id": "2506.00054", "pdf": "https://arxiv.org/pdf/2506.00054", "abs": "https://arxiv.org/abs/2506.00054", "authors": ["Chaitanya Sharma"], "title": "Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to\nenhance large language models (LLMs) by conditioning generation on external\nevidence retrieved at inference time. While RAG addresses critical limitations\nof parametric knowledge storage-such as factual inconsistency and domain\ninflexibility-it introduces new challenges in retrieval quality, grounding\nfidelity, pipeline efficiency, and robustness against noisy or adversarial\ninputs. This survey provides a comprehensive synthesis of recent advances in\nRAG systems, offering a taxonomy that categorizes architectures into\nretriever-centric, generator-centric, hybrid, and robustness-oriented designs.\nWe systematically analyze enhancements across retrieval optimization, context\nfiltering, decoding control, and efficiency improvements, supported by\ncomparative performance analyses on short-form and multi-hop question answering\ntasks. Furthermore, we review state-of-the-art evaluation frameworks and\nbenchmarks, highlighting trends in retrieval-aware evaluation, robustness\ntesting, and federated retrieval settings. Our analysis reveals recurring\ntrade-offs between retrieval precision and generation flexibility, efficiency\nand faithfulness, and modularity and coordination. We conclude by identifying\nopen challenges and future research directions, including adaptive retrieval\narchitectures, real-time retrieval integration, structured reasoning over\nmulti-hop evidence, and privacy-preserving retrieval mechanisms. This survey\naims to consolidate current knowledge in RAG research and serve as a foundation\nfor the next generation of retrieval-augmented language modeling systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u6cd5\u5206\u6790\u4e0d\u540c\u67b6\u6784\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u68c0\u7d22\u4f18\u5316\u3001\u751f\u6210\u63a7\u5236\u548c\u6548\u7387\u63d0\u5347\u65b9\u9762\u7684\u6539\u8fdb\u3002", "motivation": "\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5f25\u8865\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u53c2\u6570\u5316\u77e5\u8bc6\u5b58\u50a8\u4e2d\u7684\u4e0d\u8db3\uff0c\u5982\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u6027\u548c\u9886\u57df\u7075\u6d3b\u6027\u5dee\u3002", "method": "\u5bf9RAG\u7cfb\u7edf\u8fdb\u884c\u5206\u7c7b\uff08\u68c0\u7d22\u4e2d\u5fc3\u3001\u751f\u6210\u4e2d\u5fc3\u3001\u6df7\u5408\u8bbe\u8ba1\u548c\u9c81\u68d2\u6027\u8bbe\u8ba1\uff09\uff0c\u5206\u6790\u68c0\u7d22\u4f18\u5316\u3001\u4e0a\u4e0b\u6587\u8fc7\u6ee4\u3001\u89e3\u7801\u63a7\u5236\u548c\u6548\u7387\u6539\u8fdb\u3002", "result": "\u5728\u77ed\u683c\u5f0f\u548c\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u6027\u80fd\u6539\u8fdb\uff0c\u5e76\u603b\u7ed3\u4e86\u68c0\u7d22\u7cbe\u5ea6\u4e0e\u751f\u6210\u7075\u6d3b\u6027\u3001\u6548\u7387\u4e0e\u5fe0\u5b9e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u81ea\u9002\u5e94\u68c0\u7d22\u67b6\u6784\u3001\u5b9e\u65f6\u68c0\u7d22\u96c6\u6210\u548c\u9690\u79c1\u4fdd\u62a4\u68c0\u7d22\u673a\u5236\u3002", "keywords": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3001\u68c0\u7d22\u4f18\u5316\u3001\u751f\u6210\u63a7\u5236\u3001\u95ee\u7b54\u4efb\u52a1"}}
{"id": "2506.01907", "pdf": "https://arxiv.org/pdf/2506.01907", "abs": "https://arxiv.org/abs/2506.01907", "authors": ["Yan Zhou", "Bradley Malin", "Murat Kantarcioglu"], "title": "SMOTE-DP: Improving Privacy-Utility Tradeoff with Synthetic Data", "categories": ["cs.LG", "cs.CR", "stat.ML"], "comment": null, "summary": "Privacy-preserving data publication, including synthetic data sharing, often\nexperiences trade-offs between privacy and utility. Synthetic data is generally\nmore effective than data anonymization in balancing this trade-off, however,\nnot without its own challenges. Synthetic data produced by generative models\ntrained on source data may inadvertently reveal information about outliers.\nTechniques specifically designed for preserving privacy, such as introducing\nnoise to satisfy differential privacy, often incur unpredictable and\nsignificant losses in utility. In this work we show that, with the right\nmechanism of synthetic data generation, we can achieve strong privacy\nprotection without significant utility loss. Synthetic data generators\nproducing contracting data patterns, such as Synthetic Minority Over-sampling\nTechnique (SMOTE), can enhance a differentially private data generator,\nleveraging the strengths of both. We prove in theory and through empirical\ndemonstration that this SMOTE-DP technique can produce synthetic data that not\nonly ensures robust privacy protection but maintains utility in downstream\nlearning tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408SMOTE\u548c\u5dee\u5206\u9690\u79c1\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff08SMOTE-DP\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u9690\u79c1\u4fdd\u62a4\u4e0e\u6570\u636e\u6548\u7528\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u80fd\u4fdd\u6301\u6570\u636e\u7684\u9ad8\u6548\u7528\u3002", "motivation": "\u9690\u79c1\u4fdd\u62a4\u6570\u636e\u53d1\u5e03\uff08\u5982\u5408\u6210\u6570\u636e\u5171\u4eab\uff09\u5e38\u9762\u4e34\u9690\u79c1\u4e0e\u6548\u7528\u7684\u6743\u8861\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u52a0\u566a\u5dee\u5206\u9690\u79c1\uff09\u53ef\u80fd\u5bfc\u81f4\u6548\u7528\u663e\u8457\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u4e0d\u663e\u8457\u635f\u5931\u6548\u7528\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u673a\u5236\u3002", "method": "\u63d0\u51faSMOTE-DP\u6280\u672f\uff0c\u7ed3\u5408\u751f\u6210\u5408\u6210\u6570\u636e\u7684SMOTE\u65b9\u6cd5\u548c\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff0c\u5229\u7528\u5176\u5404\u81ea\u7684\u4f18\u52bf\u751f\u6210\u9690\u79c1\u4fdd\u62a4\u4e14\u6548\u7528\u7684\u6570\u636e\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8868\u660e\uff0cSMOTE-DP\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u4e0d\u4ec5\u80fd\u63d0\u4f9b\u5f3a\u9690\u79c1\u4fdd\u62a4\uff0c\u8fd8\u80fd\u5728\u4e0b\u6e38\u5b66\u4e60\u4efb\u52a1\u4e2d\u4fdd\u6301\u826f\u597d\u7684\u6548\u7528\u3002", "conclusion": "SMOTE-DP\u4e3a\u89e3\u51b3\u9690\u79c1\u4e0e\u6548\u7528\u7684\u6743\u8861\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6570\u636e\u6548\u7528\u65b9\u9762\u7684\u53cc\u91cd\u4f18\u52bf\u3002", "keywords": "\u9690\u79c1\u4fdd\u62a4, \u5408\u6210\u6570\u636e, \u5dee\u5206\u9690\u79c1, SMOTE, \u6570\u636e\u6548\u7528"}}
{"id": "2506.01913", "pdf": "https://arxiv.org/pdf/2506.01913", "abs": "https://arxiv.org/abs/2506.01913", "authors": ["Thomas Pethick", "Wanyun Xie", "Mete Erdogan", "Kimon Antonakopoulos", "Tony Silveti-Falls", "Volkan Cevher"], "title": "Generalized Gradient Norm Clipping & Non-Euclidean $(L_0,L_1)$-Smoothness", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This work introduces a hybrid non-Euclidean optimization method which\ngeneralizes gradient norm clipping by combining steepest descent and\nconditional gradient approaches. The method achieves the best of both worlds by\nestablishing a descent property under a generalized notion of\n($L_0$,$L_1$)-smoothness. Weight decay is incorporated in a principled manner\nby identifying a connection to the Frank-Wolfe short step. In the stochastic\ncase, we show an order optimal $O(n^{-1/4})$ convergence rate by leveraging a\nmomentum based gradient estimator. We discuss how to instantiate the algorithms\nfor deep learning and demonstrate their properties on image classification and\nlanguage modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u975e\u6b27\u51e0\u91cc\u5f97\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u6700\u901f\u4e0b\u964d\u548c\u6761\u4ef6\u68af\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e7f\u4e49($L_0$,$L_1$)-\u5149\u6ed1\u6027\u5b9e\u73b0\u4e0b\u964d\u6027\u8d28\uff0c\u5e76\u5728\u968f\u673a\u60c5\u51b5\u4e0b\u5c55\u793a\u4e86\u6700\u4f18\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u5728\u4f18\u5316\u95ee\u9898\u4e2d\u7ed3\u5408\u6700\u901f\u4e0b\u964d\u548c\u6761\u4ef6\u68af\u5ea6\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u901a\u8fc7\u5e7f\u4e49\u5149\u6ed1\u6027\u6982\u5ff5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6df7\u5408\u975e\u6b27\u51e0\u91cc\u5f97\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u68af\u5ea6\u8303\u6570\u88c1\u526a\u548c\u6743\u91cd\u8870\u51cf\uff0c\u5229\u7528\u52a8\u91cf\u68af\u5ea6\u4f30\u8ba1\u5668\u4f18\u5316\u968f\u673a\u60c5\u51b5\u4e0b\u7684\u6536\u655b\u901f\u5ea6\u3002", "result": "\u5728\u968f\u673a\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86$O(n^{-1/4})$\u7684\u6700\u4f18\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u3002", "keywords": "\u975e\u6b27\u51e0\u91cc\u5f97\u4f18\u5316,\u68af\u5ea6\u88c1\u526a,\u6743\u91cd\u8870\u51cf,\u52a8\u91cf\u68af\u5ea6\u4f30\u8ba1\u5668,\u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2506.00062", "pdf": "https://arxiv.org/pdf/2506.00062", "abs": "https://arxiv.org/abs/2506.00062", "authors": ["Aladin Djuhera", "Swanand Ravindra Kadhe", "Farhan Ahmed", "Syed Zawad", "Holger Boche", "Walid Saad"], "title": "SafeCOMM: What about Safety Alignment in Fine-Tuned Telecom Large Language Models?", "categories": ["cs.CY", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "Fine-tuning large language models (LLMs) for telecom tasks and datasets is a\ncommon practice to adapt general-purpose models to the telecom domain. However,\nlittle attention has been paid to how this process may compromise model safety.\nRecent research has shown that even benign fine-tuning can degrade the safety\nalignment of LLMs, causing them to respond to harmful or unethical user\nqueries. In this paper, we investigate this issue for telecom-tuned LLMs using\nthree representative datasets featured by the GenAINet initiative. We show that\nsafety degradation persists even for structured and seemingly harmless datasets\nsuch as 3GPP standards and tabular records, indicating that telecom-specific\ndata is not immune to safety erosion during fine-tuning. We further extend our\nanalysis to publicly available Telecom LLMs trained via continual pre-training,\nrevealing that safety alignment is often severely lacking, primarily due to the\nomission of safety-focused instruction tuning. To address these issues in both\nfine-tuned and pre-trained models, we conduct extensive experiments and\nevaluate three safety realignment defenses (SafeInstruct, SafeLoRA, and\nSafeMERGE) using established red-teaming benchmarks. The results show that,\nacross all settings, the proposed defenses can effectively restore safety after\nharmful degradation without compromising downstream task performance, leading\nto Safe teleCOMMunication (SafeCOMM) models. In a nutshell, our work serves as\na diagnostic study and practical guide for safety realignment in telecom-tuned\nLLMs, and emphasizes the importance of safety-aware instruction and fine-tuning\nfor real-world deployments of Telecom LLMs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7535\u4fe1\u9886\u57df\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u6a21\u578b\u5b89\u5168\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e09\u79cd\u5b89\u5168\u91cd\u6821\u51c6\u65b9\u6cd5\u4ee5\u6062\u590d\u5b89\u5168\u6027\u3002", "motivation": "\u63a2\u8ba8\u7535\u4fe1\u9886\u57df\u5fae\u8c03LLM\u65f6\u53ef\u80fd\u5bf9\u6a21\u578b\u5b89\u5168\u6027\u9020\u6210\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u4ee3\u8868\u6027\u7684\u7535\u4fe1\u6570\u636e\u96c6\uff083GPP\u6807\u51c6\u548c\u8868\u683c\u8bb0\u5f55\uff09\u8bc4\u4f30\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5b89\u5168\u6027\uff0c\u5e76\u63d0\u51fa\u4e09\u79cd\u5b89\u5168\u91cd\u6821\u51c6\u9632\u5fa1\u65b9\u6cd5\uff08SafeInstruct\u3001SafeLoRA\u548cSafeMERGE\uff09\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e09\u79cd\u9632\u5fa1\u65b9\u6cd5\u80fd\u6709\u6548\u6062\u590d\u6a21\u578b\u5b89\u5168\u6027\u4e14\u4e0d\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u7535\u4fe1\u9886\u57df\u5fae\u8c03LLM\u9700\u5173\u6ce8\u5b89\u5168\u6027\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u5b89\u5168\u91cd\u6821\u51c6\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002", "keywords": "LLM, \u7535\u4fe1, \u5fae\u8c03, \u6a21\u578b\u5b89\u5168, \u5b89\u5168\u91cd\u6821\u51c6"}}
{"id": "2506.00742", "pdf": "https://arxiv.org/pdf/2506.00742", "abs": "https://arxiv.org/abs/2506.00742", "authors": ["Zeqi Gu", "Yin Cui", "Zhaoshuo Li", "Fangyin Wei", "Yunhao Ge", "Jinwei Gu", "Ming-Yu Liu", "Abe Davis", "Yifan Ding"], "title": "ArtiScene: Language-Driven Artistic 3D Scene Generation Through Image Intermediary", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR", "summary": "Designing 3D scenes is traditionally a challenging task that demands both\nartistic expertise and proficiency with complex software. Recent advances in\ntext-to-3D generation have greatly simplified this process by letting users\ncreate scenes based on simple text descriptions. However, as these methods\ngenerally require extra training or in-context learning, their performance is\noften hindered by the limited availability of high-quality 3D data. In\ncontrast, modern text-to-image models learned from web-scale images can\ngenerate scenes with diverse, reliable spatial layouts and consistent, visually\nappealing styles. Our key insight is that instead of learning directly from 3D\nscenes, we can leverage generated 2D images as an intermediary to guide 3D\nsynthesis. In light of this, we introduce ArtiScene, a training-free automated\npipeline for scene design that integrates the flexibility of free-form\ntext-to-image generation with the diversity and reliability of 2D intermediary\nlayouts.\n  First, we generate 2D images from a scene description, then extract the shape\nand appearance of objects to create 3D models. These models are assembled into\nthe final scene using geometry, position, and pose information derived from the\nsame intermediary image. Being generalizable to a wide range of scenes and\nstyles, ArtiScene outperforms state-of-the-art benchmarks by a large margin in\nlayout and aesthetic quality by quantitative metrics. It also averages a 74.89%\nwinning rate in extensive user studies and 95.07% in GPT-4o evaluation. Project\npage: https://artiscene-cvpr.github.io/", "AI": {"tldr": "ArtiScene\u5229\u7528\u6587\u672c\u751f\u62102D\u56fe\u50cf\uff0c\u518d\u4ece\u4e2d\u63d0\u53d63D\u4fe1\u606f\uff0c\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u76843D\u573a\u666f\u8bbe\u8ba1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf3D\u573a\u666f\u8bbe\u8ba1\u590d\u6742\u4e14\u4f9d\u8d56\u9ad8\u8d28\u91cf3D\u6570\u636e\uff0c\u800c\u6587\u672c\u5230\u56fe\u50cf\u7684\u6a21\u578b\u80fd\u751f\u6210\u591a\u6837\u4e14\u53ef\u9760\u76842D\u5e03\u5c40\uff0c\u542f\u53d1\u901a\u8fc72D\u56fe\u50cf\u6307\u5bfc3D\u5408\u6210\u3002", "method": "\u5148\u751f\u62102D\u573a\u666f\u56fe\u50cf\uff0c\u4ece\u4e2d\u63d0\u53d6\u7269\u4f53\u5f62\u72b6\u548c\u5916\u89c2\u4fe1\u606f\uff0c\u7ed3\u5408\u51e0\u4f55\u3001\u4f4d\u7f6e\u548c\u59ff\u6001\u6784\u5efa3D\u573a\u666f\u3002", "result": "ArtiScene\u5728\u5e03\u5c40\u548c\u7f8e\u5b66\u8d28\u91cf\u4e0a\u5927\u5e45\u9886\u5148\u57fa\u51c6\u65b9\u6cd5\uff0c\u7528\u6237\u7814\u7a76\u4e2d\u83b7\u80dc\u738774.89%\uff0cGPT-4o\u8bc4\u4f30\u4e2d\u8fbe95.07%\u3002", "conclusion": "\u5229\u75282D\u56fe\u50cf\u4f5c\u4e3a\u4e2d\u4ecb\u76843D\u573a\u666f\u8bbe\u8ba1\u65b9\u6cd5\u9ad8\u6548\u4e14\u901a\u7528\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5bf93D\u6570\u636e\u7684\u4f9d\u8d56\u3002", "keywords": "3D\u573a\u666f\u8bbe\u8ba1, \u6587\u672c\u52303D, 2D\u56fe\u50cf\u5f15\u5bfc, ArtiScene, \u65e0\u8bad\u7ec3\u65b9\u6cd5"}}
{"id": "2506.01919", "pdf": "https://arxiv.org/pdf/2506.01919", "abs": "https://arxiv.org/abs/2506.01919", "authors": ["Yifan Hao", "Chenlu Ye", "Chi Han", "Tong Zhang"], "title": "Transformers as Multi-task Learners: Decoupling Features in Hidden Markov Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Transformer based models have shown remarkable capabilities in sequence\nlearning across a wide range of tasks, often performing well on specific task\nby leveraging input-output examples. Despite their empirical success, a\ncomprehensive theoretical understanding of this phenomenon remains limited. In\nthis work, we investigate the layerwise behavior of Transformers to uncover the\nmechanisms underlying their multi-task generalization ability. Taking\nexplorations on a typical sequence model, i.e, Hidden Markov Models, which are\nfundamental to many language tasks, we observe that: first, lower layers of\nTransformers focus on extracting feature representations, primarily influenced\nby neighboring tokens; second, on the upper layers, features become decoupled,\nexhibiting a high degree of time disentanglement. Building on these empirical\ninsights, we provide theoretical analysis for the expressiveness power of\nTransformers. Our explicit constructions align closely with empirical\nobservations, providing theoretical support for the Transformer's effectiveness\nand efficiency on sequence learning across diverse tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u6790Transformer\u7684\u5206\u5c42\u884c\u4e3a\uff0c\u63ed\u793a\u4e86\u5176\u5728\u591a\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u80cc\u540e\u7684\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u652f\u6301\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1Transformer\u5728\u5e8f\u5217\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u80cc\u540e\u7684\u7406\u8bba\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u5178\u578b\u5e8f\u5217\u6a21\u578b\uff08\u5982\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff09\u7684\u5206\u5c42\u884c\u4e3a\uff0c\u5206\u6790Transformer\u7684\u7279\u5f81\u63d0\u53d6\u548c\u65f6\u57df\u89e3\u8026\u80fd\u529b\u3002", "result": "\u4e0b\u5c42\u63d0\u53d6\u57fa\u4e8e\u90bb\u8fd1\u8bcd\u7684\u7279\u5f81\u8868\u793a\uff0c\u4e0a\u5c42\u5b9e\u73b0\u7279\u5f81\u89e3\u8026\u548c\u65f6\u57df\u89e3\u8026\uff1b\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u4e3aTransformer\u5728\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "keywords": "Transformer, \u5206\u5c42\u884c\u4e3a, \u591a\u4efb\u52a1\u6cdb\u5316, \u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b, \u7406\u8bba\u5206\u6790"}}
{"id": "2411.19276", "pdf": "https://arxiv.org/pdf/2411.19276", "abs": "https://arxiv.org/abs/2411.19276", "authors": ["Daniel Basilewitsch", "Jo\u00e3o F. Bravo", "Christian Tutschku", "Frederick Struckmeier"], "title": "Quantum Neural Networks in Practice: A Comparative Study with Classical Models from Standard Data Sets to Industrial Images", "categories": ["quant-ph", "cs.LG"], "comment": "24 pages, 13 figures", "summary": "In this study, we compare the performance of randomized classical and quantum\nneural networks (NNs) as well as classical and quantum-classical hybrid\nconvolutional neural networks (CNNs) for the task of binary image\nclassification. We use two distinct methodologies: using randomized NNs on\ndimensionality-reduced data, and applying CNNs to full image data. We evaluate\nthese approaches on three data sets of increasing complexity: an artificial\nhypercube dataset, MNIST handwritten digits and real-world industrial images.\nWe analyze correlations between classification accuracy and quantum model\nhyperparameters, including the number of trainable parameters, feature encoding\nmethods, circuit layers, entangling gate type and structure, gate entangling\npower, and measurement operators. For random quantum NNs, we compare their\nperformance against literature models. Classical and quantum/hybrid models\nachieved statistically equivalent classification accuracies across most\ndatasets, with no approach demonstrating consistent superiority. We observe\nthat quantum models show lower variance with respect to initial training\nparameters, suggesting better training stability. Among the hyperparameters\nanalyzed, only the number of trainable parameters showed a positive correlation\nwith the model performance. Around 94% of the best-performing quantum NNs had\nentangling gates, although for hybrid CNNs, models without entanglement\nperformed equally well but took longer to converge. Cross-dataset performance\nanalysis revealed limited transferability of quantum models between different\nclassification tasks. Our study provides an industry perspective on quantum\nmachine learning for practical image classification tasks, highlighting both\ncurrent limitations and potential avenues for further research in quantum\ncircuit design, entanglement utilization, and model transferability across\nvaried applications.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u968f\u673a\u7ecf\u5178\u548c\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u3001\u7ecf\u5178\u4e0e\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u4e8c\u5143\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5206\u6790\u4e86\u91cf\u5b50\u6a21\u578b\u8d85\u53c2\u6570\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u91cf\u5b50\u6a21\u578b\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u8bc4\u4f30\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u5728\u5b9e\u9645\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u63a2\u7d22\u91cf\u5b50\u6a21\u578b\u5728\u6027\u80fd\u3001\u7a33\u5b9a\u6027\u548c\u8d85\u53c2\u6570\u76f8\u5173\u6027\u65b9\u9762\u7684\u8868\u73b0\uff0c\u4e3a\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u4f7f\u7528\u968f\u673a\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u964d\u7ef4\u6570\u636e\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u5168\u56fe\u50cf\u6570\u636e\uff0c\u5bf9\u6bd4\u5206\u6790\u91cf\u5b50\u4e0e\u7ecf\u5178\u6a21\u578b\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u53ca\u8d85\u53c2\u6570\u76f8\u5173\u6027\u3002", "result": "\u91cf\u5b50\u4e0e\u7ecf\u5178\u6a21\u578b\u5728\u5927\u591a\u6570\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u76f8\u5f53\uff0c\u91cf\u5b50\u6a21\u578b\u8bad\u7ec3\u7a33\u5b9a\u6027\u66f4\u597d\u3002\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u4e0e\u6a21\u578b\u6027\u80fd\u6b63\u76f8\u5173\uff0c94%\u7684\u91cf\u5b50\u6a21\u578b\u5305\u542b\u7ea0\u7f20\u95e8\u3002", "conclusion": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u53ef\u9760\u4f46\u65e0\u663e\u8457\u4f18\u52bf\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u3001\u7ea0\u7f20\u95e8\u5229\u7528\u548c\u6a21\u578b\u8de8\u4efb\u52a1\u8fc1\u79fb\u6027\u3002", "keywords": "\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc,\u5377\u79ef\u795e\u7ecf\u7f51\u7edc,\u56fe\u50cf\u5206\u7c7b,\u91cf\u5b50\u8ba1\u7b97,\u673a\u5668\u5b66\u4e60"}}
{"id": "2506.00750", "pdf": "https://arxiv.org/pdf/2506.00750", "abs": "https://arxiv.org/abs/2506.00750", "authors": ["Monoshi Kumar Roy", "Simin Chen", "Benjamin Steenhoek", "Jinjun Peng", "Gail Kaiser", "Baishakhi Ray", "Wei Le"], "title": "CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Understanding and reasoning about code semantics is essential for enhancing\ncode LLMs' abilities to solve real-world software engineering (SE) tasks.\nAlthough several code reasoning benchmarks exist, most rely on synthetic\ndatasets or educational coding problems and focus on coarse-grained reasoning\ntasks such as input/output prediction, limiting their effectiveness in\nevaluating LLMs in practical SE contexts. To bridge this gap, we propose\nCodeSense, the first benchmark that makes available a spectrum of fine-grained\ncode reasoning tasks concerned with the software engineering of real-world\ncode. We collected Python, C and Java software projects from real-world\nrepositories. We executed tests from these repositories, collected their\nexecution traces, and constructed a ground truth dataset for fine-grained\nsemantic reasoning tasks. We then performed comprehensive evaluations on\nstate-of-the-art LLMs. Our results show a clear performance gap for the models\nto handle fine-grained reasoning tasks. Although prompting techniques such as\nchain-of-thought and in-context learning helped, the lack of code semantics in\nLLMs fundamentally limit models' capabilities of code reasoning. Besides\ndataset, benchmark and evaluation, our work produced an execution tracing\nframework and tool set that make it easy to collect ground truth for\nfine-grained SE reasoning tasks, offering a strong basis for future benchmark\nconstruction and model post training. Our code and data are located at\nhttps://codesense-bench.github.io/.", "AI": {"tldr": "CodeSense\u662f\u9996\u4e2a\u9488\u5bf9\u73b0\u5b9e\u4e16\u754c\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7ec6\u7c92\u5ea6\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7c97\u7c92\u5ea6\u6d4b\u8bd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u63a8\u7406\u57fa\u51c6\u591a\u4f9d\u8d56\u5408\u6210\u6570\u636e\u6216\u6559\u80b2\u6027\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30LLM\u5728\u5b9e\u9645\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4ece\u73b0\u5b9e\u9879\u76ee\u4ed3\u5e93\u6536\u96c6Python\u3001C\u548cJava\u4ee3\u7801\uff0c\u6267\u884c\u6d4b\u8bd5\u5e76\u6536\u96c6\u6267\u884c\u8f68\u8ff9\uff0c\u6784\u5efa\u7ec6\u7c92\u5ea6\u8bed\u4e49\u63a8\u7406\u4efb\u52a1\u6570\u636e\u96c6\u3002", "result": "\u8bc4\u4f30\u663e\u793aLLM\u5728\u7ec6\u7c92\u5ea6\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u63d0\u793a\u6280\u672f\u867d\u6709\u5e2e\u52a9\uff0c\u4f46\u7f3a\u4e4f\u4ee3\u7801\u8bed\u4e49\u7406\u89e3\u9650\u5236\u4e86\u6a21\u578b\u80fd\u529b\u3002", "conclusion": "CodeSense\u4e3a\u672a\u6765\u57fa\u51c6\u6784\u5efa\u548c\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5de5\u5177\u548c\u57fa\u7840\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "keywords": "\u4ee3\u7801\u63a8\u7406,LLM,\u8f6f\u4ef6\u5de5\u7a0b,\u57fa\u51c6\u6d4b\u8bd5,\u7ec6\u7c92\u5ea6\u8bed\u4e49"}}
{"id": "2506.00007", "pdf": "https://arxiv.org/pdf/2506.00007", "abs": "https://arxiv.org/abs/2506.00007", "authors": ["Zhengfeng Wu", "Ziyi Chen", "Nnaemeka Achebe", "Vaibhav V. Rao", "Pratik Shrestha", "Ioannis Savidis"], "title": "Emerging ML-AI Techniques for Analog and RF EDA", "categories": ["cs.AR", "cs.CE", "cs.LG"], "comment": "9 pages, 2 figures", "summary": "This survey explores the integration of machine learning (ML) into EDA\nworkflows for analog and RF circuits, addressing challenges unique to analog\ndesign, which include complex constraints, nonlinear design spaces, and high\ncomputational costs. State-of-the-art learning and optimization techniques are\nreviewed for circuit tasks such as constraint formulation, topology generation,\ndevice modeling, sizing, placement, and routing. The survey highlights the\ncapability of ML to enhance automation, improve design quality, and reduce\ntime-to-market while meeting the target specifications of an analog or RF\ncircuit. Emerging trends and cross-cutting challenges, including robustness to\nvariations and considerations of interconnect parasitics, are also discussed.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u673a\u5668\u5b66\u4e60\u5728\u6a21\u62df\u548c\u5c04\u9891\u7535\u8defEDA\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u4e86ML\u5728\u81ea\u52a8\u5316\u3001\u8bbe\u8ba1\u8d28\u91cf\u548c\u7f29\u77ed\u4e0a\u5e02\u65f6\u95f4\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u63a2\u8ba8ML\u5728\u6a21\u62df\u8bbe\u8ba1\u4e2d\u9762\u4e34\u7684\u72ec\u7279\u6311\u6218\uff0c\u5982\u590d\u6742\u7ea6\u675f\u3001\u975e\u7ebf\u6027\u8bbe\u8ba1\u7a7a\u95f4\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u4ee5\u63d0\u5347EDA\u5de5\u5177\u7684\u6027\u80fd\u3002", "method": "\u56de\u987e\u4e86\u6700\u5148\u8fdb\u7684\u5b66\u4e60\u548c\u4f18\u5316\u6280\u672f\uff0c\u6db5\u76d6\u7535\u8def\u7ea6\u675f\u5236\u5b9a\u3001\u62d3\u6251\u751f\u6210\u3001\u5668\u4ef6\u5efa\u6a21\u3001\u5c3a\u5bf8\u3001\u5e03\u5c40\u548c\u5e03\u7ebf\u7b49\u4efb\u52a1\u3002", "result": "ML\u80fd\u591f\u589e\u5f3a\u81ea\u52a8\u5316\u3001\u63d0\u9ad8\u8bbe\u8ba1\u8d28\u91cf\u5e76\u52a0\u901f\u4e0a\u5e02\u65f6\u95f4\uff0c\u540c\u65f6\u6ee1\u8db3\u6a21\u62df\u6216\u5c04\u9891\u7535\u8def\u7684\u89c4\u683c\u8981\u6c42\u3002", "conclusion": "\u603b\u7ed3\u4e86ML\u5728EDA\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u8ba8\u8bba\u4e86\u65b0\u5174\u8d8b\u52bf\u548c\u6311\u6218\uff0c\u5982\u53d8\u5316\u7684\u9c81\u68d2\u6027\u548c\u4e92\u8fde\u5bc4\u751f\u6548\u5e94\u3002", "keywords": "\u673a\u5668\u5b66\u4e60\uff0cEDA\uff0c\u6a21\u62df\u7535\u8def\uff0c\u5c04\u9891\u7535\u8def\uff0c\u81ea\u52a8\u5316\uff0c\u8bbe\u8ba1\u4f18\u5316"}}
{"id": "2506.00008", "pdf": "https://arxiv.org/pdf/2506.00008", "abs": "https://arxiv.org/abs/2506.00008", "authors": ["Amit Sharma"], "title": "AI Accelerators for Large Language Model In-ference: Architecture Analysis and Scaling Strategies", "categories": ["cs.AR", "cs.LG"], "comment": null, "summary": "The rapid growth of large-language models (LLMs) is driving a new wave of\nspecialized hardware for inference. This paper presents the first\nworkload-centric, cross-architectural performance study of commercial AI\naccelerators, spanning GPU-based chips, hybrid packages, and wafer-scale\nengines. We compare memory hierarchies, compute fabrics, and on-chip\ninterconnects, and observe up to 3.7x performance variation across\narchitectures as batch size and sequence length change. Four scaling techniques\nfor trillion-parameter models are examined; expert parallelism offers an 8.4x\nparameter-to-compute advantage but incurs 2.1x higher latency variance than\ntensor parallelism. These findings provide quantitative guidance for matching\nworkloads to accelerators and reveal architectural gaps that next-generation\ndesigns must address.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5bf9\u4e0d\u540c\u5546\u4e1aAI\u52a0\u901f\u5668\u8fdb\u884c\u4e86\u8de8\u67b6\u6784\u6027\u80fd\u6bd4\u8f83\uff0c\u5206\u6790\u4e86\u5185\u5b58\u5c42\u6b21\u3001\u8ba1\u7b97\u7ed3\u6784\u548c\u7247\u4e0a\u4e92\u8fde\uff0c\u89c2\u5bdf\u4e86\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e07\u4ebf\u53c2\u6570\u6a21\u578b\u7684\u6269\u5c55\u6280\u672f\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5bf9\u4e13\u7528\u786c\u4ef6\u9700\u6c42\u7684\u589e\u957f\u4fc3\u4f7f\u7814\u7a76\u5546\u4e1aAI\u52a0\u901f\u5668\u7684\u6027\u80fd\u5dee\u5f02\u53ca\u9002\u7528\u573a\u666f\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4GPU\u82af\u7247\u3001\u6df7\u5408\u5c01\u88c5\u548c\u6676\u5706\u7ea7\u5f15\u64ce\u7b49\u4e0d\u540c\u67b6\u6784\u7684\u5185\u5b58\u5c42\u6b21\u3001\u8ba1\u7b97\u7ed3\u6784\u548c\u4e92\u8fde\u6027\u80fd\uff0c\u5e76\u7ed3\u5408\u6279\u91cf\u5927\u5c0f\u548c\u5e8f\u5217\u957f\u5ea6\u53d8\u5316\u8fdb\u884c\u5206\u6790\u3002", "result": "\u89c2\u5bdf\u5230\u4e0d\u540c\u67b6\u6784\u95f4\u6027\u80fd\u5dee\u5f02\u9ad8\u8fbe3.7\u500d\uff0c\u4e13\u5bb6\u5e76\u884c\u6280\u672f\u53c2\u6570\u6548\u7387\u9ad8\u4f46\u5ef6\u8fdf\u65b9\u5dee\u8f83\u5927\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5339\u914d\u4efb\u52a1\u4e0e\u52a0\u901f\u5668\u63d0\u4f9b\u4e86\u91cf\u5316\u6307\u5bfc\uff0c\u5e76\u63ed\u793a\u4e86\u4e0b\u4e00\u4ee3\u786c\u4ef6\u8bbe\u8ba1\u9700\u89e3\u51b3\u7684\u67b6\u6784\u7f3a\u53e3\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, AI\u52a0\u901f\u5668, \u6027\u80fd\u7814\u7a76, \u4e13\u5bb6\u5e76\u884c, \u5f20\u91cf\u5e76\u884c"}}
{"id": "2506.00033", "pdf": "https://arxiv.org/pdf/2506.00033", "abs": "https://arxiv.org/abs/2506.00033", "authors": ["Valerie Tsao", "Nathaniel W. Chaney", "Manolis Veveakis"], "title": "Probabilistic Spatial Interpolation of Sparse Data using Diffusion Models", "categories": ["stat.AP", "cs.LG"], "comment": "41 pages, 14 figures, submitted to AMS Artificial Intelligence for\n  the Earth Systems", "summary": "The large underlying assumption of climate models today relies on the basis\nof a \"confident\" initial condition, a reasonably plausible snapshot of the\nEarth for which all future predictions depend on. However, given the inherently\nchaotic nature of our system, this assumption is complicated by sensitive\ndependence, where small uncertainties in initial conditions can lead to\nexponentially diverging outcomes over time. This challenge is particularly\nsalient at global spatial scales and over centennial timescales, where data\ngaps are not just common but expected. The source of uncertainty is two-fold:\n(1) sparse, noisy observations from satellites and ground stations, and (2)\ninternal variability stemming from the simplifying approximations within the\nmodels themselves.\n  In practice, data assimilation methods are used to reconcile this missing\ninformation by conditioning model states on partial observations. Our work\nbuilds on this idea but operates at the extreme end of sparsity. We propose a\nconditional data imputation framework that reconstructs full temperature fields\nfrom as little as 1% observational coverage. The method leverages a diffusion\nmodel guided by a prekriged mask, effectively inferring the full-state fields\nfrom minimal data points. We validate our framework over the Southern Great\nPlains, focusing on afternoon (12:00-6:00 PM) temperature fields during the\nsummer months of 2018-2020. Across varying observational densities--from swath\ndata to isolated in-situ sensors--our model achieves strong reconstruction\naccuracy, highlighting its potential to fill in critical data gaps in both\nhistorical reanalysis and real-time forecasting pipelines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6761\u4ef6\u6570\u636e\u586b\u8865\u6846\u67b6\uff0c\u53ef\u4ece\u4ec51%\u7684\u89c2\u6d4b\u8986\u76d6\u7387\u91cd\u5efa\u5b8c\u6574\u6e29\u5ea6\u573a\uff0c\u89e3\u51b3\u4e86\u6c14\u5019\u6a21\u578b\u4e2d\u521d\u59cb\u6761\u4ef6\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u6c14\u5019\u6a21\u578b\u7684\u9884\u6d4b\u4f9d\u8d56\u4e8e\u201c\u81ea\u4fe1\u201d\u7684\u521d\u59cb\u6761\u4ef6\uff0c\u4f46\u7531\u4e8e\u7cfb\u7edf\u7684\u6df7\u6c8c\u6027\u548c\u6570\u636e\u7a00\u758f\u6027\uff0c\u521d\u59cb\u6761\u4ef6\u7684\u4e0d\u786e\u5b9a\u6027\u4f1a\u5bfc\u81f4\u9884\u6d4b\u7ed3\u679c\u4e25\u91cd\u504f\u79bb\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u548c\u9884\u63d2\u503c\u63a9\u7801\uff0c\u4ece\u6781\u7a00\u758f\u7684\u89c2\u6d4b\u6570\u636e\u4e2d\u63a8\u65ad\u5b8c\u6574\u6e29\u5ea6\u573a\u3002", "result": "\u57282018-2020\u5e74\u590f\u5b63\u7684\u5357\u65b9\u5927\u5e73\u539f\u5730\u533a\u9a8c\u8bc1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u89c2\u6d4b\u5bc6\u5ea6\u4e0b\u5747\u8868\u73b0\u51fa\u9ad8\u91cd\u5efa\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6709\u6548\u586b\u8865\u5386\u53f2\u548c\u5b9e\u65f6\u9884\u6d4b\u4e2d\u7684\u5173\u952e\u6570\u636e\u7a7a\u767d\uff0c\u63d0\u5347\u6c14\u5019\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "keywords": "\u6c14\u5019\u6a21\u578b, \u6570\u636e\u586b\u8865, \u6269\u6563\u6a21\u578b, \u521d\u59cb\u6761\u4ef6, \u4e0d\u786e\u5b9a\u6027"}}
{"id": "2506.00037", "pdf": "https://arxiv.org/pdf/2506.00037", "abs": "https://arxiv.org/abs/2506.00037", "authors": ["Dipam Goswami", "Liying Wang", "Bart\u0142omiej Twardowski", "Joost van de Weijer"], "title": "Query Drift Compensation: Enabling Compatibility in Continual Learning of Retrieval Embedding Models", "categories": ["cs.IR", "cs.LG"], "comment": "Accepted at CoLLAs 2025", "summary": "Text embedding models enable semantic search, powering several NLP\napplications like Retrieval Augmented Generation by efficient information\nretrieval (IR). However, text embedding models are commonly studied in\nscenarios where the training data is static, thus limiting its applications to\ndynamic scenarios where new training data emerges over time. IR methods\ngenerally encode a huge corpus of documents to low-dimensional embeddings and\nstore them in a database index. During retrieval, a semantic search over the\ncorpus is performed and the document whose embedding is most similar to the\nquery embedding is returned. When updating an embedding model with new training\ndata, using the already indexed corpus is suboptimal due to the\nnon-compatibility issue, since the model which was used to obtain the\nembeddings of the corpus has changed. While re-indexing of old corpus documents\nusing the updated model enables compatibility, it requires much higher\ncomputation and time. Thus, it is critical to study how the already indexed\ncorpus can still be effectively used without the need of re-indexing. In this\nwork, we establish a continual learning benchmark with large-scale datasets and\ncontinually train dense retrieval embedding models on query-document pairs from\nnew datasets in each task and observe forgetting on old tasks due to\nsignificant drift of embeddings. We employ embedding distillation on both query\nand document embeddings to maintain stability and propose a novel query drift\ncompensation method during retrieval to project new model query embeddings to\nthe old embedding space. This enables compatibility with previously indexed\ncorpus embeddings extracted using the old model and thus reduces the\nforgetting. We show that the proposed method significantly improves performance\nwithout any re-indexing. Code is available at\nhttps://github.com/dipamgoswami/QDC.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u52a8\u6001\u8bad\u7ec3\u6570\u636e\u73af\u5883\u4e0b\u907f\u514d\u91cd\u65b0\u7d22\u5f15\u65e7\u8bed\u6599\u7684\u6587\u672c\u5d4c\u5165\u6a21\u578b\u66f4\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5d4c\u5165\u84b8\u998f\u548c\u67e5\u8be2\u6f02\u79fb\u8865\u507f\u51cf\u5c11\u9057\u5fd8\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u52a8\u6001\u8bad\u7ec3\u6570\u636e\u573a\u666f\u4e0b\uff0c\u907f\u514d\u91cd\u65b0\u7d22\u5f15\u65e7\u8bed\u6599\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u4f7f\u7528\u66f4\u65b0\u7684\u6587\u672c\u5d4c\u5165\u6a21\u578b\u3002", "method": "\u91c7\u7528\u5d4c\u5165\u84b8\u998f\u6280\u672f\u7a33\u5b9a\u67e5\u8be2\u548c\u6587\u6863\u5d4c\u5165\uff0c\u5e76\u63d0\u51fa\u67e5\u8be2\u6f02\u79fb\u8865\u507f\u65b9\u6cd5\uff0c\u5c06\u65b0\u6a21\u578b\u7684\u67e5\u8be2\u5d4c\u5165\u6295\u5f71\u5230\u65e7\u5d4c\u5165\u7a7a\u95f4\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u65e0\u9700\u91cd\u65b0\u7d22\u5f15\u65e7\u8bed\u6599\u3002", "conclusion": "\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u578b\u66f4\u65b0\u5bfc\u81f4\u7684\u975e\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u964d\u4f4e\u4e86\u9057\u5fd8\u3002", "keywords": "\u6587\u672c\u5d4c\u5165\u3001\u52a8\u6001\u8bad\u7ec3\u6570\u636e\u3001\u5d4c\u5165\u84b8\u998f\u3001\u67e5\u8be2\u6f02\u79fb\u8865\u507f\u3001\u8bed\u4e49\u641c\u7d22"}}
{"id": "2506.00788", "pdf": "https://arxiv.org/pdf/2506.00788", "abs": "https://arxiv.org/abs/2506.00788", "authors": ["Djaber Rouabhia", "Ismail Hadjadj"], "title": "Behavioral Augmentation of UML Class Diagrams: An Empirical Study of Large Language Models for Method Generation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Automating the enrichment of UML class diagrams with behavioral methods from\nnatural language use cases is a significant challenge. This study evaluates\nnine large language models (LLMs) in augmenting a methodless UML diagram (21\nclasses, 17 relationships) using 21 structured waste-management use cases. A\ntotal of 90 diagrams (3,373 methods) were assessed across six metrics: method\nquantity, signature richness (visibility, names, parameters, return types),\nannotation completeness (linking to use cases/actions), structural fidelity,\nsyntactic correctness (PlantUML compilation), and naming convergence (across\nmodels). All LLMs produced valid PlantUML diagrams adhering to UML conventions.\nSome models excelled in method coverage and annotation accuracy, while others\nshowed richer parameterization but weaker traceability. These results\ndemonstrate that LLMs can generate well-structured methods with consistent\nnaming, advancing automated behavioral modeling. However, inconsistencies in\nannotations and signatures highlight the need for improved prompt engineering\nand model selection. The rapid generation of these methods supports Agile\npractices by enabling faster design iterations. Despite their capabilities,\nhuman oversight is essential to ensure accuracy, appropriateness, and semantic\nalignment. This positions LLMs as collaborative partners in software design.\nAll experimental artifacts (\\texttt{.puml}, \\texttt{.png}, \\texttt{.csv}) are\npublicly available for reproducibility.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4e5d\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u52a8\u5316\u8865\u5168\u65e0\u65b9\u6cd5\u7684UML\u7c7b\u56fe\u65b9\u9762\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u663e\u793aLLM\u80fd\u591f\u751f\u6210\u7ed3\u6784\u826f\u597d\u7684\u65b9\u6cd5\uff0c\u4f46\u5728\u6ce8\u91ca\u548c\u7b7e\u540d\u4e00\u81f4\u6027\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u81ea\u52a8\u5316\u4ece\u81ea\u7136\u8bed\u8a00\u7528\u4f8b\u4e2d\u4e3aUML\u7c7b\u56fe\u8865\u5145\u884c\u4e3a\u65b9\u6cd5\u662f\u5f53\u524d\u7684\u91cd\u8981\u6311\u6218\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u8bc4\u4f30LLM\u7684\u6027\u80fd\u6765\u63a8\u52a8\u8fd9\u4e00\u9886\u57df\u7684\u8fdb\u5c55\u3002", "method": "\u7814\u7a76\u4f7f\u752821\u4e2a\u7ed3\u6784\u5316\u5e9f\u7269\u7ba1\u7406\u7528\u4f8b\uff0c\u8bc4\u4f30\u4e86\u4e5d\u79cdLLM\u751f\u6210\u768490\u4e2aUML\u56fe\uff083,373\u4e2a\u65b9\u6cd5\uff09\uff0c\u5e76\u901a\u8fc7\u516d\u9879\u6307\u6807\uff08\u5982\u65b9\u6cd5\u6570\u91cf\u3001\u7b7e\u540d\u4e30\u5bcc\u6027\u7b49\uff09\u8fdb\u884c\u5206\u6790\u3002", "result": "\u6240\u6709LLM\u5747\u80fd\u751f\u6210\u7b26\u5408UML\u89c4\u8303\u7684PlantUML\u56fe\uff0c\u90e8\u5206\u6a21\u578b\u5728\u65b9\u6cd5\u8986\u76d6\u7387\u548c\u6ce8\u91ca\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u6574\u4f53\u4ecd\u9700\u6539\u8fdb\u6ce8\u91ca\u548c\u7b7e\u540d\u4e00\u81f4\u6027\u3002", "conclusion": "LLM\u53ef\u4f5c\u4e3a\u8f6f\u4ef6\u8bbe\u8ba1\u7684\u534f\u4f5c\u5de5\u5177\uff0c\u4f46\u9700\u4eba\u7c7b\u76d1\u7763\u4ee5\u786e\u4fdd\u51c6\u786e\u6027\uff1b\u5176\u5feb\u901f\u751f\u6210\u80fd\u529b\u652f\u6301\u654f\u6377\u5f00\u53d1\u7684\u8bbe\u8ba1\u8fed\u4ee3\u3002", "keywords": "UML\u7c7b\u56fe, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u81ea\u7136\u8bed\u8a00\u7528\u4f8b, \u884c\u4e3a\u5efa\u6a21, \u654f\u6377\u5f00\u53d1"}}
{"id": "2506.00041", "pdf": "https://arxiv.org/pdf/2506.00041", "abs": "https://arxiv.org/abs/2506.00041", "authors": ["Seongwan Park", "Taeklim Kim", "Youngjoong Ko"], "title": "Decoding Dense Embeddings: Sparse Autoencoders for Interpreting and Discretizing Dense Retrieval", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "Despite their strong performance, Dense Passage Retrieval (DPR) models suffer\nfrom a lack of interpretability. In this work, we propose a novel\ninterpretability framework that leverages Sparse Autoencoders (SAEs) to\ndecompose previously uninterpretable dense embeddings from DPR models into\ndistinct, interpretable latent concepts. We generate natural language\ndescriptions for each latent concept, enabling human interpretations of both\nthe dense embeddings and the query-document similarity scores of DPR models. We\nfurther introduce Concept-Level Sparse Retrieval (CL-SR), a retrieval framework\nthat directly utilizes the extracted latent concepts as indexing units. CL-SR\neffectively combines the semantic expressiveness of dense embeddings with the\ntransparency and efficiency of sparse representations. We show that CL-SR\nachieves high index-space and computational efficiency while maintaining robust\nperformance across vocabulary and semantic mismatches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u5206\u89e3\u5bc6\u96c6\u5d4c\u5165\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86\u6982\u5ff5\u7ea7\u7a00\u758f\u68c0\u7d22\uff08CL-SR\uff09\uff0c\u7ed3\u5408\u8bed\u4e49\u8868\u8fbe\u548c\u7a00\u758f\u8868\u793a\u7684\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\uff08DPR\uff09\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u89e3\u5bc6\u96c6\u5d4c\u5165\uff0c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u5e76\u63d0\u51faCL-SR\u6846\u67b6\u3002", "result": "CL-SR\u5728\u7d22\u5f15\u7a7a\u95f4\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "CL-SR\u6210\u529f\u7ed3\u5408\u4e86\u5bc6\u96c6\u5d4c\u5165\u7684\u8bed\u4e49\u8868\u8fbe\u548c\u7a00\u758f\u8868\u793a\u7684\u900f\u660e\u6027\u3002", "keywords": "\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b, \u53ef\u89e3\u91ca\u6027, \u7a00\u758f\u81ea\u7f16\u7801\u5668, CL-SR"}}
{"id": "2506.00044", "pdf": "https://arxiv.org/pdf/2506.00044", "abs": "https://arxiv.org/abs/2506.00044", "authors": ["Jieyu Chen", "Sebastian Lerch", "Melanie Schienle", "Tomasz Serafin", "Rafa\u0142 Weron"], "title": "Probabilistic intraday electricity price forecasting using generative machine learning", "categories": ["stat.AP", "cs.LG", "stat.ML"], "comment": null, "summary": "The growing importance of intraday electricity trading in Europe calls for\nimproved price forecasting and tailored decision-support tools. In this paper,\nwe propose a novel generative neural network model to generate probabilistic\npath forecasts for intraday electricity prices and use them to construct\neffective trading strategies for Germany's continuous-time intraday market. Our\nmethod demonstrates competitive performance in terms of statistical evaluation\nmetrics compared to two state-of-the-art statistical benchmark approaches. To\nfurther assess its economic value, we consider a realistic fixed-volume trading\nscenario and propose various strategies for placing market sell orders based on\nthe path forecasts. Among the different trading strategies, the price paths\ngenerated by our generative model lead to higher profit gains than the\nbenchmark methods. Our findings highlight the potential of generative machine\nlearning tools in electricity price forecasting and underscore the importance\nof economic evaluation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5f0f\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u65e5\u5185\u7535\u529b\u4ef7\u683c\u7684\u6982\u7387\u8def\u5f84\u9884\u6d4b\uff0c\u5e76\u6784\u5efa\u6709\u6548\u7684\u4ea4\u6613\u7b56\u7565\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5728\u7edf\u8ba1\u548c\u7ecf\u6d4e\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6b27\u6d32\u65e5\u5185\u7535\u529b\u4ea4\u6613\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u9700\u8981\u6539\u8fdb\u4ef7\u683c\u9884\u6d4b\u548c\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002", "method": "\u91c7\u7528\u751f\u6210\u5f0f\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8fdb\u884c\u6982\u7387\u8def\u5f84\u9884\u6d4b\uff0c\u5e76\u6784\u5efa\u57fa\u4e8e\u9884\u6d4b\u7684\u4ea4\u6613\u7b56\u7565\u3002", "result": "\u5728\u7edf\u8ba1\u6307\u6807\u548c\u7ecf\u6d4e\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u4ea4\u6613\u7b56\u7565\u5e26\u6765\u66f4\u9ad8\u5229\u6da6\u3002", "conclusion": "\u751f\u6210\u5f0f\u673a\u5668\u5b66\u4e60\u5de5\u5177\u5728\u7535\u529b\u4ef7\u683c\u9884\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u7ecf\u6d4e\u8bc4\u4f30\u662f\u5173\u952e\u3002", "keywords": "\u65e5\u5185\u7535\u529b\u4ea4\u6613,\u751f\u6210\u5f0f\u795e\u7ecf\u7f51\u7edc,\u4ef7\u683c\u9884\u6d4b,\u4ea4\u6613\u7b56\u7565"}}
{"id": "2506.00048", "pdf": "https://arxiv.org/pdf/2506.00048", "abs": "https://arxiv.org/abs/2506.00048", "authors": ["Aravinda Jatavallabha", "Prabhanjan Bharadwaj", "Ashish Chander"], "title": "Graph Contrastive Learning for Optimizing Sparse Data in Recommender Systems with LightGCL", "categories": ["cs.IR", "cs.LG"], "comment": "Term Paper, Machine Learning with Graphs, North Carolina State\n  University", "summary": "Graph Neural Networks (GNNs) are powerful tools for recommendation systems,\nbut they often struggle under data sparsity and noise. To address these issues,\nwe implemented LightGCL, a graph contrastive learning model that uses Singular\nValue Decomposition (SVD) for robust graph augmentation, preserving semantic\nintegrity without relying on stochastic or heuristic perturbations. LightGCL\nenables structural refinement and captures global collaborative signals,\nachieving significant gains over state-of-the-art models across benchmark\ndatasets. Our experiments also demonstrate improved fairness and resilience to\npopularity bias, making it well-suited for real-world recommender systems.", "AI": {"tldr": "LightGCL\u901a\u8fc7SVD\u589e\u5f3a\u56fe\u5bf9\u6bd4\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86GNN\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u7a00\u758f\u6027\u548c\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u516c\u5e73\u6027\u3002", "motivation": "GNN\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u56e0\u6570\u636e\u7a00\u758f\u548c\u566a\u58f0\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u4e00\u79cd\u66f4\u7a33\u5065\u7684\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8eSVD\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578bLightGCL\uff0c\u907f\u514d\u968f\u673a\u6216\u542f\u53d1\u5f0f\u6270\u52a8\uff0c\u4fdd\u7559\u8bed\u4e49\u5b8c\u6574\u6027\u3002", "result": "LightGCL\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u516c\u5e73\u6027\u548c\u6297\u6d41\u884c\u5ea6\u504f\u5dee\u80fd\u529b\u3002", "conclusion": "LightGCL\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u771f\u5b9e\u63a8\u8350\u7cfb\u7edf\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u4e0e\u7a33\u5065\u6027\u3002", "keywords": "GNN, \u63a8\u8350\u7cfb\u7edf, \u56fe\u5bf9\u6bd4\u5b66\u4e60, SVD, \u6570\u636e\u7a00\u758f\u6027"}}
{"id": "2506.00238", "pdf": "https://arxiv.org/pdf/2506.00238", "abs": "https://arxiv.org/abs/2506.00238", "authors": ["Ehsan Karimi", "Maryam Rahnemoonfar"], "title": "ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping for Natural Disaster Damage Assessment", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG", "I.2.7; I.2.10; I.5.1"], "comment": "Accepted by the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025)", "summary": "Natural disasters usually affect vast areas and devastate infrastructures.\nPerforming a timely and efficient response is crucial to minimize the impact on\naffected communities, and data-driven approaches are the best choice. Visual\nquestion answering (VQA) models help management teams to achieve in-depth\nunderstanding of damages. However, recently published models do not possess the\nability to answer open-ended questions and only select the best answer among a\npredefined list of answers. If we want to ask questions with new additional\npossible answers that do not exist in the predefined list, the model needs to\nbe fin-tuned/retrained on a new collected and annotated dataset, which is a\ntime-consuming procedure. In recent years, large-scale Vision-Language Models\n(VLMs) have earned significant attention. These models are trained on extensive\ndatasets and demonstrate strong performance on both unimodal and multimodal\nvision/language downstream tasks, often without the need for fine-tuning. In\nthis paper, we propose a VLM-based zero-shot VQA (ZeShot-VQA) method, and\ninvestigate the performance of on post-disaster FloodNet dataset. Since the\nproposed method takes advantage of zero-shot learning, it can be applied on new\ndatasets without fine-tuning. In addition, ZeShot-VQA is able to process and\ngenerate answers that has been not seen during the training procedure, which\ndemonstrates its flexibility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u96f6\u6837\u672c\u89c6\u89c9\u95ee\u7b54\u65b9\u6cd5\uff08ZeShot-VQA\uff09\uff0c\u7528\u4e8e\u707e\u5bb3\u7ba1\u7406\u4e2d\u7684\u5f00\u653e\u6027\u95ee\u9898\u56de\u7b54\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u9002\u5e94\u65b0\u6570\u636e\u96c6\u3002", "motivation": "\u81ea\u7136\u707e\u5bb3\u901a\u5e38\u5f71\u54cd\u5e7f\u6cdb\uff0c\u53ca\u65f6\u9ad8\u6548\u7684\u6570\u636e\u9a71\u52a8\u54cd\u5e94\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684VQA\u6a21\u578b\u65e0\u6cd5\u56de\u7b54\u5f00\u653e\u6027\u95ee\u9898\uff0c\u4e14\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u4ee5\u9002\u5e94\u65b0\u7b54\u6848\uff0c\u8017\u65f6\u8017\u529b\u3002", "method": "\u5229\u7528\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\uff0c\u63d0\u51faZeShot-VQA\u65b9\u6cd5\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u3002", "result": "\u5728FloodNet\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86ZeShot-VQA\u7684\u6027\u80fd\uff0c\u80fd\u5904\u7406\u672a\u89c1\u8fc7\u7684\u7b54\u6848\uff0c\u5c55\u73b0\u7075\u6d3b\u6027\u3002", "conclusion": "ZeShot-VQA\u4e3a\u707e\u5bb3\u7ba1\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u5f00\u653e\u6027\u95ee\u9898\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u3001\u96f6\u6837\u672c\u5b66\u4e60\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u3001\u707e\u5bb3\u7ba1\u7406\u3001\u5f00\u653e\u6027\u95ee\u9898"}}
{"id": "2506.00816", "pdf": "https://arxiv.org/pdf/2506.00816", "abs": "https://arxiv.org/abs/2506.00816", "authors": ["Xiang Zhang", "Run He", "Jiao Chen", "Di Fang", "Ming Li", "Ziqian Zeng", "Cen Chen", "Huiping Zhuang"], "title": "L3A: Label-Augmented Analytic Adaptation for Multi-Label Class Incremental Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICML2025", "summary": "Class-incremental learning (CIL) enables models to learn new classes\ncontinually without forgetting previously acquired knowledge. Multi-label CIL\n(MLCIL) extends CIL to a real-world scenario where each sample may belong to\nmultiple classes, introducing several challenges: label absence, which leads to\nincomplete historical information due to missing labels, and class imbalance,\nwhich results in the model bias toward majority classes. To address these\nchallenges, we propose Label-Augmented Analytic Adaptation (L3A), an\nexemplar-free approach without storing past samples. L3A integrates two key\nmodules. The pseudo-label (PL) module implements label augmentation by\ngenerating pseudo-labels for current phase samples, addressing the label\nabsence problem. The weighted analytic classifier (WAC) derives a closed-form\nsolution for neural networks. It introduces sample-specific weights to\nadaptively balance the class contribution and mitigate class imbalance.\nExperiments on MS-COCO and PASCAL VOC datasets demonstrate that L3A outperforms\nexisting methods in MLCIL tasks. Our code is available at\nhttps://github.com/scut-zx/L3A.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aL3A\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u591a\u6807\u7b7e\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u6807\u7b7e\u7f3a\u5931\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u4f2a\u6807\u7b7e\u6a21\u5757\u548c\u52a0\u6743\u5206\u6790\u5206\u7c7b\u5668\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u591a\u6807\u7b7e\u589e\u91cf\u5b66\u4e60\uff08MLCIL\uff09\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u9762\u4e34\u6807\u7b7e\u7f3a\u5931\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u3002", "method": "L3A\u5305\u542b\u4f2a\u6807\u7b7e\uff08PL\uff09\u6a21\u5757\u751f\u6210\u4f2a\u6807\u7b7e\u4ee5\u89e3\u51b3\u6807\u7b7e\u7f3a\u5931\u95ee\u9898\uff0c\u4ee5\u53ca\u52a0\u6743\u5206\u6790\u5206\u7c7b\u5668\uff08WAC\uff09\u901a\u8fc7\u95ed\u5f0f\u89e3\u548c\u6837\u672c\u6743\u91cd\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u3002", "result": "\u5728MS-COCO\u548cPASCAL VOC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cL3A\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "L3A\u662f\u4e00\u79cd\u65e0\u9700\u5b58\u50a8\u5386\u53f2\u6837\u672c\u7684MLCIL\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6807\u7b7e\u7f3a\u5931\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "keywords": "\u591a\u6807\u7b7e\u589e\u91cf\u5b66\u4e60, \u6807\u7b7e\u7f3a\u5931, \u7c7b\u522b\u4e0d\u5e73\u8861, L3A"}}
{"id": "2506.00819", "pdf": "https://arxiv.org/pdf/2506.00819", "abs": "https://arxiv.org/abs/2506.00819", "authors": ["Dawood Wasif", "Terrence J Moore", "Chandan K Reddy", "Jin-Hee Cho"], "title": "DriveMind: A Dual-VLM based Reinforcement Learning Framework for Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "End-to-end autonomous driving systems map sensor data directly to control\ncommands, but remain opaque, lack interpretability, and offer no formal safety\nguarantees. While recent vision-language-guided reinforcement learning (RL)\nmethods introduce semantic feedback, they often rely on static prompts and\nfixed objectives, limiting adaptability to dynamic driving scenes. We present\nDriveMind, a unified semantic reward framework that integrates: (i) a\ncontrastive Vision-Language Model (VLM) encoder for stepwise semantic\nanchoring; (ii) a novelty-triggered VLM encoder-decoder, fine-tuned via\nchain-of-thought (CoT) distillation, for dynamic prompt generation upon\nsemantic drift; (iii) a hierarchical safety module enforcing kinematic\nconstraints (e.g., speed, lane centering, stability); and (iv) a compact\npredictive world model to reward alignment with anticipated ideal states.\nDriveMind achieves 19.4 +/- 2.3 km/h average speed, 0.98 +/- 0.03 route\ncompletion, and near-zero collisions in CARLA Town 2, outperforming baselines\nby over 4% in success rate. Its semantic reward generalizes zero-shot to real\ndash-cam data with minimal distributional shift, demonstrating robust\ncross-domain alignment and potential for real-world deployment.", "AI": {"tldr": "DriveMind\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u4e49\u5956\u52b1\u6846\u67b6\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u5b89\u5168\u6027\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u52a8\u6001\u63d0\u793a\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u9a7e\u9a76\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u5b89\u5168\u6027\u4fdd\u8bc1\uff0c\u4e14\u52a8\u6001\u9a7e\u9a76\u573a\u666f\u7684\u9002\u5e94\u6027\u4e0d\u8db3\u3002", "method": "DriveMind\u6574\u5408\u4e86\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u5668\u3001\u52a8\u6001\u63d0\u793a\u751f\u6210\u3001\u5206\u5c42\u5b89\u5168\u6a21\u5757\u548c\u9884\u6d4b\u4e16\u754c\u6a21\u578b\uff0c\u4ee5\u8bed\u4e49\u5956\u52b1\u6846\u67b6\u4f18\u5316\u9a7e\u9a76\u884c\u4e3a\u3002", "result": "DriveMind\u5728CARLA Town 2\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u901f\u5ea6\u548c\u8def\u7ebf\u5b8c\u6210\u7387\uff0c\u78b0\u649e\u7387\u63a5\u8fd1\u96f6\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "DriveMind\u901a\u8fc7\u8bed\u4e49\u5956\u52b1\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\uff0c\u5c55\u793a\u4e86\u8de8\u9886\u57df\u90e8\u7f72\u7684\u6f5c\u529b\u3002", "keywords": "\u81ea\u52a8\u9a7e\u9a76\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8bed\u4e49\u5956\u52b1\uff0c\u52a8\u6001\u63d0\u793a\uff0c\u5b89\u5168\u6027"}}
{"id": "2506.00057", "pdf": "https://arxiv.org/pdf/2506.00057", "abs": "https://arxiv.org/abs/2506.00057", "authors": ["Yiwei Sun"], "title": "Hierarchical Bayesian Knowledge Tracing in Undergraduate Engineering Education", "categories": ["cs.CY", "cs.LG", "stat.AP", "stat.ML", "62P25, 68T05, 62M99", "K.3.1; I.2.6"], "comment": "6 pages, 6 figures, 3 tables", "summary": "Educators teaching entry-level university engineering modules face the\nchallenge of identifying which topics students find most difficult and how to\nsupport diverse student needs effectively. This study demonstrates a rigorous\nyet interpretable statistical approach -- hierarchical Bayesian modeling --\nthat leverages detailed student response data to quantify both skill difficulty\nand individual student abilities. Using a large-scale dataset from an\nundergraduate Statics course, we identified clear patterns of skill mastery and\nuncovered distinct student subgroups based on their learning trajectories. Our\nanalysis reveals that certain concepts consistently present challenges,\nrequiring targeted instructional support, while others are readily mastered and\nmay benefit from enrichment activities. Importantly, the hierarchical Bayesian\nmethod provides educators with intuitive, reliable metrics without sacrificing\npredictive accuracy. This approach allows for data-informed decisions, enabling\npersonalized teaching strategies to improve student engagement and success. By\ncombining robust statistical methods with clear interpretability, this study\nequips educators with actionable insights to better support diverse learner\npopulations.", "AI": {"tldr": "\u6559\u80b2\u5de5\u4f5c\u8005\u901a\u8fc7\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\u5206\u6790\u5b66\u751f\u6570\u636e\uff0c\u8bc6\u522b\u5b66\u4e60\u96be\u70b9\u548c\u5b66\u751f\u80fd\u529b\u5dee\u5f02\uff0c\u4e3a\u4e2a\u6027\u5316\u6559\u5b66\u63d0\u4f9b\u4f9d\u636e\u3002", "motivation": "\u89e3\u51b3\u5927\u5b66\u5de5\u7a0b\u5165\u95e8\u8bfe\u7a0b\u4e2d\u5982\u4f55\u8bc6\u522b\u5b66\u751f\u96be\u70b9\u548c\u6ee1\u8db3\u591a\u6837\u5316\u9700\u6c42\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\u5206\u6790\u5b66\u751f\u53cd\u5e94\u6570\u636e\uff0c\u91cf\u5316\u6280\u80fd\u96be\u5ea6\u548c\u4e2a\u4f53\u80fd\u529b\u3002", "result": "\u53d1\u73b0\u6280\u80fd\u638c\u63e1\u7684\u660e\u786e\u6a21\u5f0f\u548c\u5b66\u751f\u4e9a\u7ec4\uff0c\u63ed\u793a\u67d0\u4e9b\u6982\u5ff5\u666e\u904d\u56f0\u96be\uff0c\u9700\u8981\u9488\u5bf9\u6027\u652f\u6301\u3002", "conclusion": "\u5206\u5c42\u8d1d\u53f6\u65af\u65b9\u6cd5\u4e3a\u6559\u80b2\u8005\u63d0\u4f9b\u76f4\u89c2\u53ef\u9760\u7684\u6307\u6807\uff0c\u652f\u6301\u4e2a\u6027\u5316\u6559\u5b66\u7b56\u7565\u3002", "keywords": "\u8d1d\u53f6\u65af\u6a21\u578b\uff1b\u5b66\u4e60\u5206\u6790\uff1b\u4e2a\u6027\u5316\u6559\u5b66\uff1b\u5de5\u7a0b\u6559\u80b2\uff1b\u5b66\u751f\u80fd\u529b"}}
{"id": "2506.00821", "pdf": "https://arxiv.org/pdf/2506.00821", "abs": "https://arxiv.org/abs/2506.00821", "authors": ["Huixin Zhan", "Jason H. Moore"], "title": "SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM),\nhave demonstrated significant success in variant effect prediction. However,\ntheir adversarial robustness remains largely unexplored. To address this gap,\nwe propose SafeGenes: a framework for Secure analysis of genomic foundation\nmodels, leveraging adversarial attacks to evaluate robustness against both\nengineered near-identical adversarial Genes and embedding-space manipulations.\nIn this study, we assess the adversarial vulnerabilities of GFMs using two\napproaches: the Fast Gradient Sign Method (FGSM) and a soft prompt attack. FGSM\nintroduces minimal perturbations to input sequences, while the soft prompt\nattack optimizes continuous embeddings to manipulate model predictions without\nmodifying the input tokens. By combining these techniques, SafeGenes provides a\ncomprehensive assessment of GFM susceptibility to adversarial manipulation.\nTargeted soft prompt attacks led to substantial performance degradation, even\nin large models such as ESM1b and ESM1v. These findings expose critical\nvulnerabilities in current foundation models, opening new research directions\ntoward improving their security and robustness in high-stakes genomic\napplications such as variant effect prediction.", "AI": {"tldr": "SafeGenes\u6846\u67b6\u8bc4\u4f30\u4e86\u57fa\u56e0\u7ec4\u57fa\u7840\u6a21\u578b\uff08GFMs\uff09\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u5176\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u8868\u73b0\u8106\u5f31\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u5d4c\u5165\u7a7a\u95f4\u64cd\u4f5c\u7684\u8f6f\u63d0\u793a\u653b\u51fb\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u5f53\u524dGFMs\u5728\u57fa\u56e0\u7ec4\u53d8\u4f53\u6548\u5e94\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5bf9\u6297\u9c81\u68d2\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u91c7\u7528FGSM\u548c\u8f6f\u63d0\u793a\u653b\u51fb\u4e24\u79cd\u65b9\u6cd5\uff0c\u8bc4\u4f30\u6a21\u578b\u5bf9\u8f93\u5165\u5e8f\u5217\u6270\u52a8\u548c\u5d4c\u5165\u7a7a\u95f4\u64cd\u4f5c\u7684\u8106\u5f31\u6027\u3002", "result": "\u8f6f\u63d0\u793a\u653b\u51fb\u663e\u8457\u964d\u4f4e\u4e86\u5927\u578b\u6a21\u578b\u5982ESM1b\u548cESM1v\u7684\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u5176\u5b89\u5168\u6f0f\u6d1e\u3002", "conclusion": "GFMs\u5b58\u5728\u5bf9\u6297\u653b\u51fb\u7684\u4e25\u91cd\u8106\u5f31\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u9ad8\u5176\u5728\u9ad8\u98ce\u9669\u57fa\u56e0\u7ec4\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002", "keywords": "\u57fa\u56e0\u7ec4\u57fa\u7840\u6a21\u578b,\u5bf9\u6297\u9c81\u68d2\u6027,SafeGenes,FGSM,\u8f6f\u63d0\u793a\u653b\u51fb"}}
{"id": "2506.00261", "pdf": "https://arxiv.org/pdf/2506.00261", "abs": "https://arxiv.org/abs/2506.00261", "authors": ["Xiaochen Wang", "Zongyu Wu", "Yuan Zhong", "Xiang Zhang", "Suhang Wang", "Fenglong Ma"], "title": "GPR: Empowering Generation with Graph-Pretrained Retriever", "categories": ["cs.IR", "cs.CL"], "comment": "Short paper submitted to EMNLP'25", "summary": "Graph retrieval-augmented generation (GRAG) places high demands on\ngraph-specific retrievers. However, existing retrievers often rely on language\nmodels pretrained on plain text, limiting their effectiveness due to domain\nmisalignment and structure ignorance. To address these challenges, we propose\nGPR, a graph-based retriever pretrained directly on knowledge graphs. GPR\naligns natural language questions with relevant subgraphs through LLM-guided\ngraph augmentation and employs a structure-aware objective to learn\nfine-grained retrieval strategies. Experiments on two datasets, three LLM\nbackbones, and five baselines show that GPR consistently improves both\nretrieval quality and downstream generation, demonstrating its effectiveness as\na robust retrieval solution for GRAG.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86GPR\uff0c\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u9884\u8bad\u7ec3\u7684\u56fe\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7684\u56fe\u589e\u5f3a\u548c\u7ed3\u6784\u611f\u77e5\u76ee\u6807\u63d0\u5347\u4e86\u68c0\u7d22\u8d28\u91cf\u548c\u4e0b\u6e38\u751f\u6210\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u5668\u901a\u5e38\u4f9d\u8d56\u4e8e\u7eaf\u6587\u672c\u9884\u8bad\u7ec3\uff0c\u5bfc\u81f4\u9886\u57df\u4e0d\u5339\u914d\u548c\u7ed3\u6784\u5ffd\u7565\uff0c\u9650\u5236\u4e86\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08GRAG\uff09\u7684\u6548\u679c\u3002", "method": "GPR\u76f4\u63a5\u5728\u77e5\u8bc6\u56fe\u8c31\u4e0a\u9884\u8bad\u7ec3\uff0c\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7684\u56fe\u589e\u5f3a\u548c\u7ed3\u6784\u611f\u77e5\u76ee\u6807\uff0c\u5bf9\u9f50\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u4e0e\u76f8\u5173\u5b50\u56fe\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u3001\u4e09\u4e2aLLM\u4e3b\u5e72\u548c\u4e94\u4e2a\u57fa\u7ebf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGPR\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u8d28\u91cf\u548c\u4e0b\u6e38\u751f\u6210\u6548\u679c\u3002", "conclusion": "GPR\u662f\u4e00\u79cd\u6709\u6548\u7684\u56fe\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8eGRAG\u4efb\u52a1\u3002", "keywords": "\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u77e5\u8bc6\u56fe\u8c31\u3001\u8bed\u8a00\u6a21\u578b\u3001\u7ed3\u6784\u611f\u77e5"}}
{"id": "2506.00276", "pdf": "https://arxiv.org/pdf/2506.00276", "abs": "https://arxiv.org/abs/2506.00276", "authors": ["Jiawei Fang", "Yuxuan Sun", "Chengtian Ma", "Qiuyu Lu", "Lining Yao"], "title": "RoboMoRe: LLM-based Robot Co-design via Joint Optimization of Morphology and Reward", "categories": ["cs.RO", "cs.CL", "68T40, 68T05, 90C90", "I.2.9; I.2.6; I.2.8; I.2.10"], "comment": "30 pages, 13 figures", "summary": "Robot co-design, jointly optimizing morphology and control policy, remains a\nlongstanding challenge in the robotics community, where many promising robots\nhave been developed. However, a key limitation lies in its tendency to converge\nto sub-optimal designs due to the use of fixed reward functions, which fail to\nexplore the diverse motion modes suitable for different morphologies. Here we\npropose RoboMoRe, a large language model (LLM)-driven framework that integrates\nmorphology and reward shaping for co-optimization within the robot co-design\nloop. RoboMoRe performs a dual-stage optimization: in the coarse optimization\nstage, an LLM-based diversity reflection mechanism generates both diverse and\nhigh-quality morphology-reward pairs and efficiently explores their\ndistribution. In the fine optimization stage, top candidates are iteratively\nrefined through alternating LLM-guided reward and morphology gradient updates.\nRoboMoRe can optimize both efficient robot morphologies and their suited motion\nbehaviors through reward shaping. Results demonstrate that without any\ntask-specific prompting or predefined reward/morphology templates, RoboMoRe\nsignificantly outperforms human-engineered designs and competing methods across\neight different tasks.", "AI": {"tldr": "RoboMoRe\u6846\u67b6\u901a\u8fc7\u53cc\u9636\u6bb5\u4f18\u5316\uff08\u7c97\u4f18\u5316\u548c\u7ec6\u4f18\u5316\uff09\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u534f\u540c\u8bbe\u8ba1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u4eba\u5de5\u8bbe\u8ba1\u548c\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u673a\u5668\u4eba\u534f\u540c\u8bbe\u8ba1\u4e2d\u56fa\u5b9a\u5956\u52b1\u51fd\u6570\u5bfc\u81f4\u6b21\u4f18\u8bbe\u8ba1\uff0c\u9650\u5236\u4e86\u591a\u6837\u8fd0\u52a8\u6a21\u5f0f\u7684\u63a2\u7d22\u3002", "method": "RoboMoRe\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u53cc\u9636\u6bb5\u4f18\u5316\uff0c\u5148\u7c97\u4f18\u5316\u751f\u6210\u591a\u6837\u9ad8\u8d28\u91cf\u7684\u5f62\u6001-\u5956\u52b1\u5bf9\uff0c\u518d\u7ec6\u4f18\u5316\u8fed\u4ee3\u6539\u8fdb\u3002", "result": "\u5728\u516b\u9879\u4efb\u52a1\u4e2d\uff0cRoboMoRe\u65e0\u9700\u7279\u5b9a\u63d0\u793a\u6216\u6a21\u677f\uff0c\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u548c\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "RoboMoRem\u4e3a\u673a\u5668\u4eba\u534f\u540c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u5f62\u6001\u4e0e\u884c\u4e3a\u7684\u534f\u540c\u4f18\u5316\u3002", "keywords": "\u673a\u5668\u4eba\u534f\u540c\u8bbe\u8ba1, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u53cc\u9636\u6bb5\u4f18\u5316, \u5956\u52b1\u5851\u9020"}}
{"id": "2506.00831", "pdf": "https://arxiv.org/pdf/2506.00831", "abs": "https://arxiv.org/abs/2506.00831", "authors": ["M Sabbir Salek", "Mashrur Chowdhury", "Muhaimin Bin Munir", "Yuchen Cai", "Mohammad Imtiaz Hasan", "Jean-Michel Tine", "Latifur Khan", "Mizanur Rahman"], "title": "A Large Language Model-Supported Threat Modeling Framework for Transportation Cyber-Physical Systems", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Modern transportation systems rely on cyber-physical systems (CPS), where\ncyber systems interact seamlessly with physical systems like\ntransportation-related sensors and actuators to enhance safety, mobility, and\nenergy efficiency. However, growing automation and connectivity increase\nexposure to cyber vulnerabilities. Existing threat modeling frameworks for\ntransportation CPS are often limited in scope, resource-intensive, and\ndependent on significant cybersecurity expertise. To address these gaps, we\npresent TraCR-TMF (Transportation Cybersecurity and Resiliency Threat Modeling\nFramework), a large language model (LLM)-based framework that minimizes expert\nintervention. TraCR-TMF identifies threats, potential attack techniques, and\ncorresponding countermeasures by leveraging the MITRE ATT&CK matrix through\nthree LLM-based approaches: (i) a retrieval-augmented generation (RAG) method\nrequiring no expert input, (ii) an in-context learning approach requiring low\nexpert input, and (iii) a supervised fine-tuning method requiring moderate\nexpert input. TraCR-TMF also maps attack paths to critical assets by analyzing\nvulnerabilities using a customized LLM. The framework was evaluated in two\nscenarios. First, it identified relevant attack techniques across\ntransportation CPS applications, with 90% precision as validated by experts.\nSecond, using a fine-tuned LLM, it successfully predicted multiple\nexploitations including lateral movement, data exfiltration, and\nransomware-related encryption that occurred during a major real-world\ncyberattack incident. These results demonstrate TraCR-TMF's effectiveness in\nCPS threat modeling, its reduced reliance on cybersecurity expertise, and its\nadaptability across CPS domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ea4\u901a\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u5efa\u6a21\u6846\u67b6TraCR-TMF\uff0c\u901a\u8fc7\u51cf\u5c11\u4e13\u5bb6\u5e72\u9884\u6765\u8bc6\u522b\u5a01\u80c1\u3001\u653b\u51fb\u6280\u672f\u548c\u5e94\u5bf9\u63aa\u65bd\uff0c\u5e76\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u4ee3\u4ea4\u901a\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\uff08CPS\uff09\uff0c\u4f46\u81ea\u52a8\u5316\u548c\u8fde\u901a\u6027\u7684\u589e\u52a0\u5bfc\u81f4\u4e86\u5b89\u5168\u6f0f\u6d1e\u7684\u66b4\u9732\u3002\u73b0\u6709\u5a01\u80c1\u5efa\u6a21\u6846\u67b6\u5728\u8303\u56f4\u3001\u8d44\u6e90\u548c\u4e13\u4e1a\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "TraCR-TMF\u5229\u7528MITRE ATT&CK\u77e9\u9635\uff0c\u901a\u8fc7\u4e09\u79cd\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\uff08\u65e0\u9700\u4e13\u5bb6\u8f93\u5165\u7684RAG\u3001\u4f4e\u4e13\u5bb6\u8f93\u5165\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u4e2d\u7b49\u4e13\u5bb6\u8f93\u5165\u7684\u76d1\u7763\u5fae\u8c03\uff09\u548c\u5b9a\u5236\u5316LLM\u5206\u6790\u6f0f\u6d1e\uff0c\u6620\u5c04\u653b\u51fb\u8def\u5f84\u3002", "result": "\u6846\u67b6\u5728\u4e24\u79cd\u573a\u666f\u4e2d\u9a8c\u8bc1\uff1a\u8bc6\u522b\u4ea4\u901aCPS\u5e94\u7528\u7684\u653b\u51fb\u6280\u672f\uff08\u4e13\u5bb6\u9a8c\u8bc1\u7cbe\u786e\u5ea6\u4e3a90%\uff09\uff0c\u4ee5\u53ca\u9884\u6d4b\u5b9e\u9645\u7f51\u7edc\u653b\u51fb\u4e8b\u4ef6\u4e2d\u7684\u591a\u9636\u6bb5\u5229\u7528\u884c\u4e3a\u3002", "conclusion": "TraCR-TMF\u5728CPS\u5a01\u80c1\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u51cf\u5c11\u4e86\u5bf9\u7f51\u7edc\u5b89\u5168\u4e13\u4e1a\u77e5\u8bc6\u7684\u4f9d\u8d56\uff0c\u5e76\u5177\u6709\u8de8CPS\u9886\u57df\u7684\u9002\u5e94\u6027\u3002", "keywords": "\u4ea4\u901a\u7f51\u7edc\u7269\u7406\u7cfb\u7edf,\u5a01\u80c1\u5efa\u6a21,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u7f51\u7edc\u5b89\u5168,MITRE ATT&CK"}}
{"id": "2506.00832", "pdf": "https://arxiv.org/pdf/2506.00832", "abs": "https://arxiv.org/abs/2506.00832", "authors": ["Kyowoon Lee", "Artyom Stitsyuk", "Gunu Jho", "Inchul Hwang", "Jaesik Choi"], "title": "Counterfactual Activation Editing for Post-hoc Prosody and Mispronunciation Correction in TTS Models", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Recent advances in Text-to-Speech (TTS) have significantly improved speech\nnaturalness, increasing the demand for precise prosody control and\nmispronunciation correction. Existing approaches for prosody manipulation often\ndepend on specialized modules or additional training, limiting their capacity\nfor post-hoc adjustments. Similarly, traditional mispronunciation correction\nrelies on grapheme-to-phoneme dictionaries, making it less practical in\nlow-resource settings. We introduce Counterfactual Activation Editing, a\nmodel-agnostic method that manipulates internal representations in a\npre-trained TTS model to achieve post-hoc control of prosody and pronunciation.\nExperimental results show that our method effectively adjusts prosodic features\nand corrects mispronunciations while preserving synthesis quality. This opens\nthe door to inference-time refinement of TTS outputs without retraining,\nbridging the gap between pre-trained TTS models and editable speech synthesis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Counterfactual Activation Editing \u7684\u6a21\u578b\u65e0\u5173\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u9884\u8bad\u7ec3\u7684TTS\u6a21\u578b\u4e2d\u64cd\u63a7\u5185\u90e8\u8868\u793a\uff0c\u5b9e\u73b0\u540e\u5904\u7406\u7684\u97f5\u5f8b\u63a7\u5236\u548c\u53d1\u97f3\u4fee\u6b63\u3002", "motivation": "\u73b0\u6709\u97f5\u5f8b\u63a7\u5236\u548c\u53d1\u97f3\u4fee\u6b63\u65b9\u6cd5\u4f9d\u8d56\u4e13\u95e8\u6a21\u5757\u6216\u989d\u5916\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u540e\u5904\u7406\u8c03\u6574\u7684\u80fd\u529b\uff0c\u4e14\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u5b9e\u7528\u6027\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u64cd\u63a7\u9884\u8bad\u7ec3TTS\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\uff08Counterfactual Activation Editing\uff09\u5b9e\u73b0\u97f5\u5f8b\u548c\u53d1\u97f3\u7684\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8c03\u6574\u97f5\u5f8b\u7279\u5f81\u548c\u4fee\u6b63\u53d1\u97f3\uff0c\u540c\u65f6\u4fdd\u6301\u5408\u6210\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86TTS\u8f93\u51fa\u7684\u63a8\u65ad\u65f6\u4f18\u5316\uff0c\u586b\u8865\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u53ef\u7f16\u8f91\u8bed\u97f3\u5408\u6210\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "keywords": "\u6587\u672c\u5230\u8bed\u97f3\u3001\u97f5\u5f8b\u63a7\u5236\u3001\u53d1\u97f3\u4fee\u6b63\u3001\u540e\u5904\u7406\u8c03\u6574"}}
{"id": "2506.00363", "pdf": "https://arxiv.org/pdf/2506.00363", "abs": "https://arxiv.org/abs/2506.00363", "authors": ["Yubai Wei", "Jiale Han", "Yi Yang"], "title": "Adapting General-Purpose Embedding Models to Private Datasets Using Keyword-based Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": "Link: https://github.com/BaileyWei/BMEmbed", "summary": "Text embedding models play a cornerstone role in AI applications, such as\nretrieval-augmented generation (RAG). While general-purpose text embedding\nmodels demonstrate strong performance on generic retrieval benchmarks, their\neffectiveness diminishes when applied to private datasets (e.g.,\ncompany-specific proprietary data), which often contain specialized terminology\nand lingo. In this work, we introduce BMEmbed, a novel method for adapting\ngeneral-purpose text embedding models to private datasets. By leveraging the\nwell-established keyword-based retrieval technique (BM25), we construct\nsupervisory signals from the ranking of keyword-based retrieval results to\nfacilitate model adaptation. We evaluate BMEmbed across a range of domains,\ndatasets, and models, showing consistent improvements in retrieval performance.\nMoreover, we provide empirical insights into how BM25-based signals contribute\nto improving embeddings by fostering alignment and uniformity, highlighting the\nvalue of this approach in adapting models to domain-specific data. We release\nthe source code available at https://github.com/BaileyWei/BMEmbed for the\nresearch community.", "AI": {"tldr": "BMEmbed\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528BM25\u6280\u672f\u4e3a\u901a\u7528\u6587\u672c\u5d4c\u5165\u6a21\u578b\u6784\u5efa\u76d1\u7763\u4fe1\u53f7\uff0c\u4ee5\u63d0\u5347\u5176\u5728\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u7684\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u901a\u7528\u6587\u672c\u5d4c\u5165\u6a21\u578b\u5728\u901a\u7528\u68c0\u7d22\u57fa\u51c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5305\u542b\u4e13\u4e1a\u672f\u8bed\u7684\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u5229\u7528BM25\u7684\u5173\u952e\u8bcd\u68c0\u7d22\u6280\u672f\uff0c\u6784\u5efa\u76d1\u7763\u4fe1\u53f7\u4ee5\u4f18\u5316\u6a21\u578b\u5728\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u7684\u5d4c\u5165\u8868\u73b0\u3002", "result": "\u5728\u5404\u79cd\u9886\u57df\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86BMEmbed\u7684\u4e00\u81f4\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "BM25\u4fe1\u53f7\u901a\u8fc7\u4fc3\u8fdb\u5bf9\u9f50\u548c\u5747\u5300\u6027\u6539\u8fdb\u5d4c\u5165\uff0c\u4e3a\u9002\u5e94\u9886\u57df\u7279\u5b9a\u6570\u636e\u63d0\u4f9b\u4e86\u4ef7\u503c\u3002", "keywords": "\u6587\u672c\u5d4c\u5165\u6a21\u578b, BM25, \u79c1\u6709\u6570\u636e\u96c6, \u68c0\u7d22\u589e\u5f3a\u751f\u6210, BMEmbed"}}
{"id": "2506.00098", "pdf": "https://arxiv.org/pdf/2506.00098", "abs": "https://arxiv.org/abs/2506.00098", "authors": ["Edgar Welte", "Rania Rayyes"], "title": "Interactive Imitation Learning for Dexterous Robotic Manipulation: Challenges and Perspectives -- A Survey", "categories": ["cs.RO", "cs.LG"], "comment": "21 pages, 3 figures", "summary": "Dexterous manipulation is a crucial yet highly complex challenge in humanoid\nrobotics, demanding precise, adaptable, and sample-efficient learning methods.\nAs humanoid robots are usually designed to operate in human-centric\nenvironments and interact with everyday objects, mastering dexterous\nmanipulation is critical for real-world deployment. Traditional approaches,\nsuch as reinforcement learning and imitation learning, have made significant\nstrides, but they often struggle due to the unique challenges of real-world\ndexterous manipulation, including high-dimensional control, limited training\ndata, and covariate shift. This survey provides a comprehensive overview of\nthese challenges and reviews existing learning-based methods for dexterous\nmanipulation, spanning imitation learning, reinforcement learning, and hybrid\napproaches. A promising yet underexplored direction is interactive imitation\nlearning, where human feedback actively refines a robot's behavior during\ntraining. While interactive imitation learning has shown success in various\nrobotic tasks, its application to dexterous manipulation remains limited. To\naddress this gap, we examine current interactive imitation learning techniques\napplied to other robotic tasks and discuss how these methods can be adapted to\nenhance dexterous manipulation. By synthesizing state-of-the-art research, this\npaper highlights key challenges, identifies gaps in current methodologies, and\noutlines potential directions for leveraging interactive imitation learning to\nimprove dexterous robotic skills.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u7684\u6311\u6218\u548c\u5b66\u4e60\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u4ea4\u4e92\u5f0f\u6a21\u4eff\u5b66\u4e60\u5728\u89e3\u51b3\u9ad8\u7ef4\u63a7\u5236\u3001\u6570\u636e\u6709\u9650\u548c\u534f\u53d8\u91cf\u504f\u79fb\u7b49\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u5728\u4eba\u7c7b\u4e2d\u5fc3\u73af\u5883\u4e2d\u9700\u8981\u638c\u63e1\u7075\u5de7\u64cd\u4f5c\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\uff09\u9762\u4e34\u9ad8\u7ef4\u63a7\u5236\u3001\u6570\u636e\u6709\u9650\u7b49\u95ee\u9898\uff0c\u56e0\u6b64\u63a2\u7d22\u4ea4\u4e92\u5f0f\u6a21\u4eff\u5b66\u4e60\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "method": "\u7efc\u8ff0\u4e86\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\uff08\u6a21\u4eff\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6df7\u5408\u65b9\u6cd5\uff09\uff0c\u5e76\u63a2\u8ba8\u5982\u4f55\u5c06\u4ea4\u4e92\u5f0f\u6a21\u4eff\u5b66\u4e60\u5e94\u7528\u4e8e\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u3002", "result": "\u5f53\u524d\u4ea4\u4e92\u5f0f\u6a21\u4eff\u5b66\u4e60\u5728\u7075\u5de7\u64cd\u4f5c\u4e2d\u5e94\u7528\u6709\u9650\uff0c\u4f46\u901a\u8fc7\u5206\u6790\u5176\u4ed6\u4efb\u52a1\u7684\u6210\u529f\u6848\u4f8b\uff0c\u63d0\u51fa\u4e86\u9002\u5e94\u548c\u6539\u8fdb\u7684\u65b9\u5411\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u6a21\u4eff\u5b66\u4e60\u6709\u671b\u63d0\u5347\u7075\u5de7\u64cd\u4f5c\u6280\u80fd\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u586b\u8865\u65b9\u6cd5\u4e0a\u7684\u7a7a\u767d\u3002", "keywords": "\u4eba\u5f62\u673a\u5668\u4eba, \u7075\u5de7\u64cd\u4f5c, \u4ea4\u4e92\u5f0f\u6a21\u4eff\u5b66\u4e60, \u5f3a\u5316\u5b66\u4e60, \u6a21\u4eff\u5b66\u4e60"}}
{"id": "2506.00102", "pdf": "https://arxiv.org/pdf/2506.00102", "abs": "https://arxiv.org/abs/2506.00102", "authors": ["Ema Puljak", "Maurizio Pierini", "Artur Garcia-Saez"], "title": "Tensor Network for Anomaly Detection in the Latent Space of Proton Collision Events at the LHC", "categories": ["hep-ph", "cond-mat.stat-mech", "cs.LG", "hep-ex", "quant-ph", "stat.ML"], "comment": null, "summary": "The pursuit of discovering new phenomena at the Large Hadron Collider (LHC)\ndemands constant innovation in algorithms and technologies. Tensor networks are\nmathematical models on the intersection of classical and quantum machine\nlearning, which present a promising and efficient alternative for tackling\nthese challenges. In this work, we propose a tensor network-based strategy for\nanomaly detection at the LHC and demonstrate its superior performance in\nidentifying new phenomena compared to established quantum methods. Our model is\na parametrized Matrix Product State with an isometric feature map, processing a\nlatent representation of simulated LHC data generated by an autoencoder. Our\nresults highlight the potential of tensor networks to enhance new-physics\ndiscovery.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f20\u91cf\u7f51\u7edc\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8eLHC\u7684\u65b0\u7269\u7406\u73b0\u8c61\u53d1\u73b0\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u91cf\u5b50\u65b9\u6cd5\u3002", "motivation": "LHC\u9700\u8981\u4e0d\u65ad\u521b\u65b0\u7b97\u6cd5\u548c\u6280\u672f\u4ee5\u53d1\u73b0\u65b0\u73b0\u8c61\uff0c\u5f20\u91cf\u7f51\u7edc\u4f5c\u4e3a\u7ecf\u5178\u4e0e\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7684\u4ea4\u53c9\u6a21\u578b\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u63d0\u4f9b\u4e86\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u53c2\u6570\u5316\u77e9\u9635\u4e58\u79ef\u6001\u548c\u7b49\u8ddd\u7279\u5f81\u6620\u5c04\uff0c\u5904\u7406\u7531\u81ea\u7f16\u7801\u5668\u751f\u6210\u7684LHC\u6a21\u62df\u6570\u636e\u7684\u6f5c\u5728\u8868\u793a\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8bc6\u522b\u65b0\u7269\u7406\u73b0\u8c61\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u91cf\u5b50\u65b9\u6cd5\u3002", "conclusion": "\u5f20\u91cf\u7f51\u7edc\u5177\u6709\u589e\u5f3a\u65b0\u7269\u7406\u53d1\u73b0\u7684\u6f5c\u529b\u3002", "keywords": "\u5f20\u91cf\u7f51\u7edc,LHC,\u5f02\u5e38\u68c0\u6d4b,\u91cf\u5b50\u673a\u5668\u5b66\u4e60,\u77e9\u9635\u4e58\u79ef\u6001"}}
{"id": "2506.00119", "pdf": "https://arxiv.org/pdf/2506.00119", "abs": "https://arxiv.org/abs/2506.00119", "authors": ["Chi Lung Cheng", "Ranit Das", "Runze Li", "Radha Mastandrea", "Vinicius Mikuni", "Benjamin Nachman", "David Shih", "Gup Singh"], "title": "Generator Based Inference (GBI)", "categories": ["hep-ph", "cs.LG", "hep-ex"], "comment": "9 pages, 9 figures", "summary": "Statistical inference in physics is often based on samples from a generator\n(sometimes referred to as a ``forward model\") that emulate experimental data\nand depend on parameters of the underlying theory. Modern machine learning has\nsupercharged this workflow to enable high-dimensional and unbinned analyses to\nutilize much more information than ever before. We propose a general framework\nfor describing the integration of machine learning with generators called\nGenerator Based Inference (GBI). A well-studied special case of this setup is\nSimulation Based Inference (SBI) where the generator is a physics-based\nsimulator. In this work, we examine other methods within the GBI toolkit that\nuse data-driven methods to build the generator. In particular, we focus on\nresonant anomaly detection, where the generator describing the background is\nlearned from sidebands. We show how to perform machine learning-based parameter\nestimation in this context with data-derived generators. This transforms the\nstatistical outputs of anomaly detection to be directly interpretable and the\nperformance on the LHCO community benchmark dataset establishes a new\nstate-of-the-art for anomaly detection sensitivity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u57fa\u4e8e\u751f\u6210\u5668\u7684\u63a8\u65ad\uff08GBI\uff09\u7684\u901a\u7528\u6846\u67b6\uff0c\u5c06\u673a\u5668\u5b66\u4e60\u4e0e\u751f\u6210\u5668\u6574\u5408\uff0c\u7279\u522b\u5173\u6ce8\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6784\u5efa\u751f\u6210\u5668\u3002", "motivation": "\u4e3a\u4e86\u5728\u7269\u7406\u7edf\u8ba1\u63a8\u65ad\u4e2d\u5229\u7528\u673a\u5668\u5b66\u4e60\u63d0\u5347\u9ad8\u7ef4\u548c\u975e\u5206\u6bb5\u5206\u6790\u7684\u4fe1\u606f\u5229\u7528\u7387\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86GBI\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6784\u5efa\u751f\u6210\u5668\uff0c\u805a\u7126\u4e8e\u5171\u632f\u5f02\u5e38\u68c0\u6d4b\uff0c\u80cc\u666f\u751f\u6210\u5668\u4ece\u65c1\u5e26\u5b66\u4e60\u3002", "result": "\u5728LHCO\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u7684\u65b0\u6807\u6746\uff0c\u4e14\u7edf\u8ba1\u8f93\u51fa\u53ef\u76f4\u63a5\u89e3\u91ca\u3002", "conclusion": "GBI\u6846\u67b6\u4e3a\u7269\u7406\u7edf\u8ba1\u63a8\u65ad\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\uff0c\u5c24\u5176\u662f\u6570\u636e\u9a71\u52a8\u751f\u6210\u5668\u7684\u5e94\u7528\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u7684\u7075\u654f\u5ea6\u3002", "keywords": "generator based inference, machine learning, statistical inference, anomaly detection, physics"}}
{"id": "2506.00856", "pdf": "https://arxiv.org/pdf/2506.00856", "abs": "https://arxiv.org/abs/2506.00856", "authors": ["Qiang Chen", "Tianyang Han", "Jin Li", "Ye Luo", "Yuxiao Wu", "Xiaowei Zhang", "Tuo Zhou"], "title": "Can AI Master Econometrics? Evidence from Econometrics AI Agent on Expert-Level Tasks", "categories": ["econ.EM", "cs.AI"], "comment": null, "summary": "Can AI effectively perform complex econometric analysis traditionally\nrequiring human expertise? This paper evaluates an agentic AI's capability to\nmaster econometrics, focusing on empirical analysis performance. We develop an\n``Econometrics AI Agent'' built on the open-source MetaGPT framework. This\nagent exhibits outstanding performance in: (1) planning econometric tasks\nstrategically, (2) generating and executing code, (3) employing error-based\nreflection for improved robustness, and (4) allowing iterative refinement\nthrough multi-round conversations. We construct two datasets from academic\ncoursework materials and published research papers to evaluate performance\nagainst real-world challenges. Comparative testing shows our domain-specialized\nagent significantly outperforms both benchmark large language models (LLMs) and\ngeneral-purpose AI agents. This work establishes a testbed for exploring AI's\nimpact on social science research and enables cost-effective integration of\ndomain expertise, making advanced econometric methods accessible to users with\nminimal coding expertise. Furthermore, our agent enhances research\nreproducibility and offers promising pedagogical applications for econometrics\nteaching.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eMetaGPT\u6846\u67b6\u7684\u300e\u8ba1\u91cf\u7ecf\u6d4e\u5b66AI\u4ee3\u7406\u300f\uff0c\u8bc4\u4f30\u5176\u5728\u590d\u6742\u8ba1\u91cf\u7ecf\u6d4e\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u4ee3\u7406\u5728\u4efb\u52a1\u89c4\u5212\u3001\u4ee3\u7801\u751f\u6210\u4e0e\u6267\u884c\u3001\u9519\u8bef\u53cd\u601d\u53ca\u8fed\u4ee3\u4f18\u5316\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u663e\u8457\u4f18\u4e8e\u901a\u7528AI\u6a21\u578b\uff0c\u4e3a\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "motivation": "\u63a2\u7d22AI\u662f\u5426\u80fd\u591f\u66ff\u4ee3\u4eba\u7c7b\u4e13\u5bb6\u6267\u884c\u590d\u6742\u7684\u8ba1\u91cf\u7ecf\u6d4e\u5206\u6790\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eMetaGPT\u6846\u67b6\u7684\u8ba1\u91cf\u7ecf\u6d4e\u5b66AI\u4ee3\u7406\uff0c\u901a\u8fc7\u89c4\u5212\u4efb\u52a1\u3001\u751f\u6210\u4e0e\u6267\u884c\u4ee3\u7801\u3001\u9519\u8bef\u53cd\u601d\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u5e76\u6784\u5efa\u4e24\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u8be5\u4ee3\u7406\u5728\u8ba1\u91cf\u7ecf\u6d4e\u5206\u6790\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u8d85\u8d8a\u901a\u7528AI\u6a21\u578b\u548c\u57fa\u51c6\u5927\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aAI\u5728\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u5e73\u53f0\uff0c\u964d\u4f4e\u4e86\u4f7f\u7528\u9ad8\u7ea7\u8ba1\u91cf\u65b9\u6cd5\u7684\u95e8\u69db\uff0c\u5e76\u63d0\u5347\u4e86\u7814\u7a76\u53ef\u91cd\u73b0\u6027\u548c\u6559\u5b66\u6f5c\u529b\u3002", "keywords": "\u8ba1\u91cf\u7ecf\u6d4e\u5b66, AI\u4ee3\u7406, MetaGPT, \u793e\u4f1a\u79d1\u5b66\u7814\u7a76"}}
{"id": "2506.00128", "pdf": "https://arxiv.org/pdf/2506.00128", "abs": "https://arxiv.org/abs/2506.00128", "authors": ["Gabriel Aracena", "Kyle Luster", "Fabio Santos", "Igor Steinmacher", "Marco A. Gerosa"], "title": "Applying Large Language Models to Issue Classification: Revisiting with Extended Data and New Models", "categories": ["cs.SE", "cs.LG"], "comment": "35 pages, 2 figures, 9 tables, Pre-print for Science of Computer\n  Programming", "summary": "Effective prioritization of issue reports in software engineering helps to\noptimize resource allocation and information recovery. However, manual issue\nclassification is laborious and lacks scalability. As an alternative, many open\nsource software (OSS) projects employ automated processes for this task, yet\nthis method often relies on large datasets for adequate training.\nTraditionally, machine learning techniques have been used for issue\nclassification. More recently, large language models (LLMs) have emerged as\npowerful tools for addressing a range of software engineering challenges,\nincluding code and test generation, mapping new requirements to legacy software\nendpoints, and conducting code reviews. The following research investigates an\nautomated approach to issue classification based on LLMs. By leveraging the\ncapabilities of such models, we aim to develop a robust system for prioritizing\nissue reports, mitigating the necessity for extensive training data while also\nmaintaining reliability in classification. In our research, we developed an\nLLM-based approach for accurately labeling issues by selecting two of the most\nprominent large language models. We then compared their performance across\nmultiple datasets. Our findings show that GPT-4o achieved the best results in\nclassifying issues from the NLBSE 2024 competition. Moreover, GPT-4o\noutperformed DeepSeek R1, achieving an F1 score 20% higher when both models\nwere trained on the same dataset from the NLBSE 2023 competition, which was ten\ntimes larger than the NLBSE 2024 dataset. The fine-tuned GPT-4o model attained\nan average F1 score of 80.7%, while the fine-tuned DeepSeek R1 model achieved\n59.33%. Increasing the dataset size did not improve the F1 score, reducing the\ndependence on massive datasets for building an efficient solution to issue\nclassification.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u5316\u5206\u7c7b\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u95ee\u9898\u62a5\u544a\uff0c\u4ee5\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u5c55\u793a\u4e86GPT-4o\u5728\u6027\u80fd\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u624b\u52a8\u5206\u7c7b\u95ee\u9898\u62a5\u544a\u6548\u7387\u4f4e\u4e0b\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u3002\u672c\u6587\u65e8\u5728\u5229\u7528LLM\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u4e14\u6570\u636e\u9700\u6c42\u8f83\u4f4e\u7684\u81ea\u52a8\u5316\u5206\u7c7b\u65b9\u6848\u3002", "method": "\u7814\u7a76\u9009\u62e9\u4e24\u79cd\u4e3b\u6d41LLM\uff08GPT-4o\u548cDeepSeek R1\uff09\uff0c\u901a\u8fc7\u591a\u4e2a\u6570\u636e\u96c6\u6bd4\u8f83\u5176\u5728\u95ee\u9898\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u5b9e\u9a8c\u4e2d\uff0cGPT-4o\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u662f\u5728NLBSE 2024\u7ade\u8d5b\u6570\u636e\u4e0a\uff0c\u5176F1\u5206\u6570\u6bd4DeepSeek R1\u9ad820%\uff0c\u5e73\u5747F1\u5f97\u5206\u4e3a80.7%\u3002\u589e\u52a0\u6570\u636e\u96c6\u89c4\u6a21\u5e76\u672a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "LLM\uff08\u5c24\u5176\u662fGPT-4o\uff09\u80fd\u663e\u8457\u63d0\u5347\u95ee\u9898\u5206\u7c7b\u7684\u6548\u7387\uff0c\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u3002", "keywords": "\u95ee\u9898\u5206\u7c7b,\u5927\u578b\u8bed\u8a00\u6a21\u578b,GPT-4o,DeepSeek R1,\u8f6f\u4ef6\u5de5\u7a0b,\u81ea\u52a8\u5316"}}
{"id": "2506.00129", "pdf": "https://arxiv.org/pdf/2506.00129", "abs": "https://arxiv.org/abs/2506.00129", "authors": ["Edward Fish", "Richard Bowden"], "title": "Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware Sign Language Translation", "categories": ["cs.CV", "cs.LG"], "comment": "Under Review", "summary": "Recent progress in Sign Language Translation (SLT) has focussed primarily on\nimproving the representational capacity of large language models to incorporate\nSign Language features. This work explores an alternative direction: enhancing\nthe geometric properties of skeletal representations themselves. We propose\nGeo-Sign, a method that leverages the properties of hyperbolic geometry to\nmodel the hierarchical structure inherent in sign language kinematics. By\nprojecting skeletal features derived from Spatio-Temporal Graph Convolutional\nNetworks (ST-GCNs) into the Poincar\\'e ball model, we aim to create more\ndiscriminative embeddings, particularly for fine-grained motions like finger\narticulations. We introduce a hyperbolic projection layer, a weighted Fr\\'echet\nmean aggregation scheme, and a geometric contrastive loss operating directly in\nhyperbolic space. These components are integrated into an end-to-end\ntranslation framework as a regularisation function, to enhance the\nrepresentations within the language model. This work demonstrates the potential\nof hyperbolic geometry to improve skeletal representations for Sign Language\nTranslation, improving on SOTA RGB methods while preserving privacy and\nimproving computational efficiency. Code available here:\nhttps://github.com/ed-fish/geo-sign.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u53cc\u66f2\u51e0\u4f55\u6539\u8fdb\u624b\u8bed\u7ffb\u8bd1\u4e2d\u9aa8\u9abc\u8868\u793a\u7684\u65b9\u6cd5\uff08Geo-Sign\uff09\uff0c\u901a\u8fc7\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u6295\u5f71\u548c\u5bf9\u6bd4\u635f\u5931\u589e\u5f3a\u8868\u793a\uff0c\u63d0\u9ad8\u4e86\u7ffb\u8bd1\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\uff0c\u4f46\u672c\u6587\u63a2\u7d22\u4e86\u901a\u8fc7\u6539\u8fdb\u9aa8\u9abc\u8868\u793a\u7684\u51e0\u4f55\u7279\u6027\u6765\u63d0\u5347\u624b\u8bed\u7ffb\u8bd1\u6548\u679c\u7684\u65b0\u65b9\u5411\u3002", "method": "\u63d0\u51faGeo-Sign\u65b9\u6cd5\uff0c\u5c06\u9aa8\u9abc\u7279\u5f81\u6295\u5f71\u5230\u53cc\u66f2\u7a7a\u95f4\uff08Poincar\u00e9\u7403\u6a21\u578b\uff09\uff0c\u7ed3\u5408\u53cc\u66f2\u6295\u5f71\u5c42\u3001\u52a0\u6743Fr\u00e9chet\u5747\u503c\u805a\u5408\u548c\u51e0\u4f55\u5bf9\u6bd4\u635f\u5931\uff0c\u4f5c\u4e3a\u4e00\u4e2a\u6b63\u5219\u5316\u51fd\u6570\u96c6\u6210\u5230\u7aef\u5230\u7aef\u7ffb\u8bd1\u6846\u67b6\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u53cc\u66f2\u51e0\u4f55\u80fd\u6709\u6548\u6539\u8fdb\u9aa8\u9abc\u8868\u793a\uff0c\u8d85\u8d8a\u73b0\u6709RGB\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u5e76\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u53cc\u66f2\u51e0\u4f55\u5728\u624b\u8bed\u7ffb\u8bd1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u751f\u6210\u66f4\u5177\u533a\u5206\u6027\u7684\u9aa8\u9abc\u5d4c\u5165\uff0c\u7279\u522b\u662f\u5bf9\u7ec6\u7c92\u5ea6\u52a8\u4f5c\uff08\u5982\u624b\u6307\u5173\u8282\uff09\u3002", "keywords": "\u624b\u8bed\u7ffb\u8bd1, \u53cc\u66f2\u51e0\u4f55, \u9aa8\u9abc\u8868\u793a, Poincar\u00e9\u7403\u6a21\u578b, \u51e0\u4f55\u5bf9\u6bd4\u635f\u5931"}}
{"id": "2506.00871", "pdf": "https://arxiv.org/pdf/2506.00871", "abs": "https://arxiv.org/abs/2506.00871", "authors": ["Ryo Fujii", "Hideo Saito", "Ryo Hachiuma"], "title": "Towards Predicting Any Human Trajectory In Context", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Predicting accurate future trajectories of pedestrians is essential for\nautonomous systems but remains a challenging task due to the need for\nadaptability in different environments and domains. A common approach involves\ncollecting scenario-specific data and performing fine-tuning via\nbackpropagation. However, this process is often impractical on edge devices due\nto constrained computational resources. To address this challenge, we introduce\nTrajICL, an In-Context Learning (ICL) framework for pedestrian trajectory\nprediction that enables rapid adaptation without fine-tuning on the\nscenario-specific data. We propose a spatio-temporal similarity-based example\nselection (STES) method that selects relevant examples from previously observed\ntrajectories within the same scene by identifying similar motion patterns at\ncorresponding locations. To further refine this selection, we introduce\nprediction-guided example selection (PG-ES), which selects examples based on\nboth the past trajectory and the predicted future trajectory, rather than\nrelying solely on the past trajectory. This approach allows the model to\naccount for long-term dynamics when selecting examples. Finally, instead of\nrelying on small real-world datasets with limited scenario diversity, we train\nour model on a large-scale synthetic dataset to enhance its prediction ability\nby leveraging in-context examples. Extensive experiments demonstrate that\nTrajICL achieves remarkable adaptation across both in-domain and cross-domain\nscenarios, outperforming even fine-tuned approaches across multiple public\nbenchmarks. The code will be released at\nhttps://fujiry0.github.io/TrajICL-project-page.", "AI": {"tldr": "\u5f15\u5165TrajICL\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u65e0\u9700\u5fae\u8c03\u5b9e\u73b0\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u7684\u5feb\u901f\u9002\u5e94\uff0c\u63d0\u51fa\u57fa\u4e8e\u65f6\u7a7a\u76f8\u4f3c\u6027\u548c\u9884\u6d4b\u5f15\u5bfc\u7684\u793a\u4f8b\u9009\u62e9\u65b9\u6cd5\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u4e0b\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u7684\u9002\u5e94\u6027\u95ee\u9898\uff0c\u907f\u514d\u4f9d\u8d56\u573a\u666f\u7279\u5b9a\u6570\u636e\u7684\u5fae\u8c03\u3002", "method": "\u63d0\u51faSTES\u548cPG-ES\u65b9\u6cd5\u9009\u62e9\u76f8\u4f3c\u8f68\u8ff9\u793a\u4f8b\uff0c\u5229\u7528\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u3002", "result": "TrajICL\u5728\u8de8\u57df\u548c\u57df\u5185\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "TrajICL\u5c55\u793a\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u80fd\u529b\uff0c\u662f\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u793a\u4f8b\u9009\u62e9\u3001\u5408\u6210\u6570\u636e"}}
{"id": "2506.00548", "pdf": "https://arxiv.org/pdf/2506.00548", "abs": "https://arxiv.org/abs/2506.00548", "authors": ["Jiahui Geng", "Thy Thy Tran", "Preslav Nakov", "Iryna Gurevych"], "title": "Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities", "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": null, "summary": "Existing attacks against multimodal language models (MLLMs) primarily\ncommunicate instructions through text accompanied by adversarial images. In\ncontrast, we exploit the capabilities of MLLMs to interpret non-textual\ninstructions, specifically, adversarial images or audio generated by our novel\nmethod, Con Instruction. We optimize these adversarial examples to align\nclosely with target instructions in the embedding space, revealing the\ndetrimental implications of MLLMs' sophisticated understanding. Unlike prior\nwork, our method does not require training data or preprocessing of textual\ninstructions. While these non-textual adversarial examples can effectively\nbypass MLLM safety mechanisms, their combination with various text inputs\nsubstantially amplifies attack success. We further introduce a new Attack\nResponse Categorization (ARC) framework, which evaluates both the quality of\nthe model's response and its relevance to the malicious instructions.\nExperimental results demonstrate that Con Instruction effectively bypasses\nsafety mechanisms in multiple vision- and audio-language models, including\nLLaVA-v1.5, InternVL, Qwen-VL, and Qwen-Audio, evaluated on two standard\nbenchmarks: AdvBench and SafeBench. Specifically, our method achieves the\nhighest attack success rates, reaching 81.3% and 86.6% on LLaVA-v1.5 (13B). On\nthe defense side, we explore various countermeasures against our attacks and\nuncover a substantial performance gap among existing techniques. Our\nimplementation is made publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCon Instruction\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u5bf9\u6297\u6027\u7684\u56fe\u50cf\u6216\u97f3\u9891\u6765\u653b\u51fb\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\uff0c\u80fd\u591f\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\u5e76\u663e\u8457\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u5bf9MLLMs\u7684\u653b\u51fb\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u6307\u4ee4\u548c\u5bf9\u6297\u56fe\u50cf\uff0c\u800c\u672c\u8bba\u6587\u63a2\u7d22\u4e86\u6a21\u578b\u5bf9\u975e\u6587\u672c\u6307\u4ee4\uff08\u5982\u56fe\u50cf\u6216\u97f3\u9891\uff09\u7684\u7406\u89e3\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5176\u6f5c\u5728\u6f0f\u6d1e\u3002", "method": "\u63d0\u51faCon Instruction\u65b9\u6cd5\uff0c\u4f18\u5316\u5bf9\u6297\u6027\u793a\u4f8b\u4ee5\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u4e0e\u76ee\u6807\u6307\u4ee4\u5bf9\u9f50\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u6216\u6587\u672c\u6307\u4ee4\u9884\u5904\u7406\u3002\u5e76\u5f15\u5165\u65b0\u7684ARC\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\uff08\u5982LLaVA-v1.5\u7b49\uff09\u548c\u57fa\u51c6\u6d4b\u8bd5\uff08AdvBench\u3001SafeBench\uff09\u4e0a\uff0c\u653b\u51fb\u6210\u529f\u7387\u6700\u9ad8\u8fbe86.6%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u7ed5\u8fc7MLLMs\u7684\u5b89\u5168\u673a\u5236\uff0c\u540c\u65f6\u53d1\u73b0\u73b0\u6709\u9632\u5fa1\u6280\u672f\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "keywords": "\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b, \u5bf9\u6297\u653b\u51fb, \u5b89\u5168\u673a\u5236, Con Instruction, ARC\u6846\u67b6"}}
{"id": "2506.00885", "pdf": "https://arxiv.org/pdf/2506.00885", "abs": "https://arxiv.org/abs/2506.00885", "authors": ["Leying Zhang", "Yao Qian", "Xiaofei Wang", "Manthan Thakker", "Dongmei Wang", "Jianwei Yu", "Haibin Wu", "Yuxuan Hu", "Jinyu Li", "Yanmin Qian", "Sheng Zhao"], "title": "CoVoMix2: Advancing Zero-Shot Dialogue Generation with Fully Non-Autoregressive Flow Matching", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Generating natural-sounding, multi-speaker dialogue is crucial for\napplications such as podcast creation, virtual agents, and multimedia content\ngeneration. However, existing systems struggle to maintain speaker consistency,\nmodel overlapping speech, and synthesize coherent conversations efficiently. In\nthis paper, we introduce CoVoMix2, a fully non-autoregressive framework for\nzero-shot multi-talker dialogue generation. CoVoMix2 directly predicts\nmel-spectrograms from multi-stream transcriptions using a flow-matching-based\ngenerative model, eliminating the reliance on intermediate token\nrepresentations. To better capture realistic conversational dynamics, we\npropose transcription-level speaker disentanglement, sentence-level alignment,\nand prompt-level random masking strategies. Our approach achieves\nstate-of-the-art performance, outperforming strong baselines like MoonCast and\nSesame in speech quality, speaker consistency, and inference speed. Notably,\nCoVoMix2 operates without requiring transcriptions for the prompt and supports\ncontrollable dialogue generation, including overlapping speech and precise\ntiming control, demonstrating strong generalizability to real-world speech\ngeneration scenarios.", "AI": {"tldr": "CoVoMix2\u662f\u4e00\u4e2a\u975e\u81ea\u56de\u5f52\u7684\u96f6\u6837\u672c\u591a\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u751f\u6210\u6a21\u578b\u76f4\u63a5\u4ece\u591a\u6d41\u8f6c\u5f55\u9884\u6d4b\u6885\u5c14\u9891\u8c31\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u8bf4\u8bdd\u4eba\u4e00\u81f4\u6027\u3001\u91cd\u53e0\u8bed\u97f3\u5efa\u6a21\u548c\u9ad8\u6548\u5bf9\u8bdd\u5408\u6210\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u751f\u6210\u81ea\u7136\u3001\u591a\u8bf4\u8bdd\u4eba\u7684\u5bf9\u8bdd\u5728\u64ad\u5ba2\u3001\u865a\u62df\u4ee3\u7406\u548c\u591a\u5a92\u4f53\u5185\u5bb9\u751f\u6210\u7b49\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u5728\u8bf4\u8bdd\u4eba\u4e00\u81f4\u6027\u3001\u91cd\u53e0\u8bed\u97f3\u5efa\u6a21\u548c\u9ad8\u6548\u5bf9\u8bdd\u5408\u6210\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "method": "CoVoMix2\u91c7\u7528\u6d41\u5339\u914d\u751f\u6210\u6a21\u578b\uff0c\u76f4\u63a5\u4ece\u591a\u6d41\u8f6c\u5f55\u9884\u6d4b\u6885\u5c14\u9891\u8c31\uff0c\u65e0\u9700\u4e2d\u95f4\u6807\u8bb0\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u8f6c\u5f55\u7ea7\u8bf4\u8bdd\u4eba\u89e3\u8026\u3001\u53e5\u5b50\u7ea7\u5bf9\u9f50\u548c\u63d0\u793a\u7ea7\u968f\u673a\u63a9\u7801\u7b56\u7565\u6355\u6349\u771f\u5b9e\u5bf9\u8bdd\u52a8\u6001\u3002", "result": "CoVoMix2\u5728\u8bed\u97f3\u8d28\u91cf\u3001\u8bf4\u8bdd\u4eba\u4e00\u81f4\u6027\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u4f18\u4e8eMoonCast\u548cSesame\u7b49\u57fa\u51c6\u6a21\u578b\uff0c\u652f\u6301\u53ef\u63a7\u5bf9\u8bdd\u751f\u6210\uff08\u5982\u91cd\u53e0\u8bed\u97f3\u548c\u7cbe\u786e\u65f6\u5e8f\u63a7\u5236\uff09\uff0c\u5e76\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002", "conclusion": "CoVoMix2\u5c55\u73b0\u4e86\u96f6\u6837\u672c\u591a\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u751f\u6210\u7684\u5148\u8fdb\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u591a\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u751f\u6210,\u975e\u81ea\u56de\u5f52\u6a21\u578b,\u6d41\u5339\u914d,\u96f6\u6837\u672c\u5b66\u4e60,\u8bed\u97f3\u5408\u6210"}}
{"id": "2506.00891", "pdf": "https://arxiv.org/pdf/2506.00891", "abs": "https://arxiv.org/abs/2506.00891", "authors": ["Sa Zhu", "Huashan Chen", "Wanqian Zhang", "Jinchao Zhang", "Zexian Yang", "Xiaoshuai Hao", "Bo Li"], "title": "Uneven Event Modeling for Partially Relevant Video Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICME 2025", "summary": "Given a text query, partially relevant video retrieval (PRVR) aims to\nretrieve untrimmed videos containing relevant moments, wherein event modeling\nis crucial for partitioning the video into smaller temporal events that\npartially correspond to the text. Previous methods typically segment videos\ninto a fixed number of equal-length clips, resulting in ambiguous event\nboundaries. Additionally, they rely on mean pooling to compute event\nrepresentations, inevitably introducing undesired misalignment. To address\nthese, we propose an Uneven Event Modeling (UEM) framework for PRVR. We first\nintroduce the Progressive-Grouped Video Segmentation (PGVS) module, to\niteratively formulate events in light of both temporal dependencies and\nsemantic similarity between consecutive frames, enabling clear event\nboundaries. Furthermore, we also propose the Context-Aware Event Refinement\n(CAER) module to refine the event representation conditioned the text's\ncross-attention. This enables event representations to focus on the most\nrelevant frames for a given text, facilitating more precise text-video\nalignment. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance on two PRVR benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUEM\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u90e8\u5206\u76f8\u5173\u89c6\u9891\u68c0\u7d22(PRVR)\u3002\u901a\u8fc7PGVS\u6a21\u5757\u5b9e\u73b0\u6e05\u6670\u7684\u4e8b\u4ef6\u8fb9\u754c\u5212\u5206\uff0c\u4ee5\u53caCAER\u6a21\u5757\u4f18\u5316\u4e8b\u4ef6\u8868\u793a\uff0c\u63d0\u5347\u4e86\u6587\u672c-\u89c6\u9891\u5bf9\u9f50\u7684\u7cbe\u51c6\u5ea6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u89c6\u9891\u5206\u5272\u4e3a\u56fa\u5b9a\u6570\u91cf\u7684\u7b49\u957f\u7247\u6bb5\uff0c\u5bfc\u81f4\u4e8b\u4ef6\u8fb9\u754c\u6a21\u7cca\uff0c\u4e14\u4f9d\u8d56\u5e73\u5747\u6c60\u5316\u8ba1\u7b97\u4e8b\u4ef6\u8868\u793a\uff0c\u6613\u5f15\u5165\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86UEM\u6846\u67b6\uff0c\u5305\u62ecPGVS\u6a21\u5757\u901a\u8fc7\u8fed\u4ee3\u65b9\u5f0f\u7ed3\u5408\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u5212\u5206\u4e8b\u4ef6\u8fb9\u754c\uff0c\u4ee5\u53caCAER\u6a21\u5757\u5229\u7528\u6587\u672c\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u4f18\u5316\u4e8b\u4ef6\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2aPRVR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "UEM\u6846\u67b6\u901a\u8fc7\u6539\u8fdb\u4e8b\u4ef6\u5efa\u6a21\u548c\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86PRVR\u4efb\u52a1\u7684\u6027\u80fd\u3002", "keywords": "\u90e8\u5206\u76f8\u5173\u89c6\u9891\u68c0\u7d22,\u4e8b\u4ef6\u5efa\u6a21,\u6587\u672c-\u89c6\u9891\u5bf9\u9f50,PGVS,CAER"}}
{"id": "2506.00165", "pdf": "https://arxiv.org/pdf/2506.00165", "abs": "https://arxiv.org/abs/2506.00165", "authors": ["Jie Gao", "Rajesh Jayaram", "Benedikt Kolbe", "Shay Sapir", "Chris Schwiegelshohn", "Sandeep Silwal", "Erik Waingarten"], "title": "Randomized Dimensionality Reduction for Euclidean Maximization and Diversity Measures", "categories": ["cs.DS", "cs.LG"], "comment": "ICML 2025", "summary": "Randomized dimensionality reduction is a widely-used algorithmic technique\nfor speeding up large-scale Euclidean optimization problems. In this paper, we\nstudy dimension reduction for a variety of maximization problems, including\nmax-matching, max-spanning tree, max TSP, as well as various measures for\ndataset diversity. For these problems, we show that the effect of dimension\nreduction is intimately tied to the \\emph{doubling dimension} $\\lambda_X$ of\nthe underlying dataset $X$ -- a quantity measuring intrinsic dimensionality of\npoint sets. Specifically, we prove that a target dimension of $O(\\lambda_X)$\nsuffices to approximately preserve the value of any near-optimal solution,which\nwe also show is necessary for some of these problems. This is in contrast to\nclassical dimension reduction results, whose dependence increases with the\ndataset size $|X|$. We also provide empirical results validating the quality of\nsolutions found in the projected space, as well as speedups due to\ndimensionality reduction.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u968f\u673a\u964d\u7ef4\u6280\u672f\u5728\u5404\u79cd\u6700\u5927\u5316\u95ee\u9898\uff08\u5982\u6700\u5927\u5339\u914d\u3001\u6700\u5927\u751f\u6210\u6811\u3001\u6700\u5927TSP\u7b49\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u964d\u7ef4\u6548\u679c\u4e0e\u6570\u636e\u96c6\u7684\u5185\u5728\u7ef4\u5ea6\uff08\u53cc\u500d\u7ef4\u5ea6\u03bb\u2093\uff09\u5bc6\u5207\u76f8\u5173\u3002\u7814\u7a76\u8868\u660e\uff0c\u76ee\u6807\u7ef4\u5ea6O(\u03bb\u2093)\u8db3\u4ee5\u8fd1\u4f3c\u4fdd\u7559\u6700\u4f18\u89e3\uff0c\u4e14\u5728\u67d0\u4e9b\u95ee\u9898\u4e2d\u662f\u5fc5\u8981\u7684\u3002\u4e0e\u4f20\u7edf\u964d\u7ef4\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u4e8e\u6570\u636e\u96c6\u5927\u5c0f|X|\u3002", "motivation": "\u63a2\u7d22\u968f\u673a\u964d\u7ef4\u6280\u672f\u5728\u9ad8\u7ef4\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5982\u4f55\u901a\u8fc7\u5185\u5728\u7ef4\u5ea6\uff08\u53cc\u500d\u7ef4\u5ea6\uff09\u6765\u4f18\u5316\u964d\u7ef4\u6548\u679c\uff0c\u4ece\u800c\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7814\u7a76\u964d\u7ef4\u6280\u672f\u5bf9\u591a\u79cd\u6700\u5927\u5316\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u548c\u6570\u636e\u96c6\u591a\u6837\u6027\u7684\u5f71\u54cd\u3002\u91cd\u70b9\u5173\u6ce8\u53cc\u500d\u7ef4\u5ea6\u03bb\u2093\u4e0e\u964d\u7ef4\u6548\u679c\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u76ee\u6807\u7ef4\u5ea6O(\u03bb\u2093)\u53ef\u4ee5\u8fd1\u4f3c\u4fdd\u7559\u6700\u4f18\u89e3\uff0c\u8fd9\u5728\u67d0\u4e9b\u95ee\u9898\u662f\u5fc5\u8981\u7684\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u964d\u7ef4\u540e\u7684\u89e3\u8d28\u91cf\u548c\u8ba1\u7b97\u52a0\u901f\u6548\u679c\u3002", "conclusion": "\u53cc\u500d\u7ef4\u5ea6\u662f\u51b3\u5b9a\u964d\u7ef4\u6548\u679c\u7684\u5173\u952e\u56e0\u7d20\u3002\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u66f4\u9ad8\u6548\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u6570\u636e\u96c6\u5927\u5c0f\u3002", "keywords": "\u968f\u673a\u964d\u7ef4, \u53cc\u500d\u7ef4\u5ea6, \u6700\u5927\u5316\u95ee\u9898, \u4f18\u5316, \u8ba1\u7b97\u6548\u7387"}}
{"id": "2506.00894", "pdf": "https://arxiv.org/pdf/2506.00894", "abs": "https://arxiv.org/abs/2506.00894", "authors": ["Keyuan Cheng", "Xudong Shen", "Yihao Yang", "Tengyue Wang", "Yang Cao", "Muhammad Asif Ali", "Hanbin Wang", "Lijie Hu", "Di Wang"], "title": "CODEMENV: Benchmarking Large Language Models on Code Migration", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by ACL 2025 Findings", "summary": "Large language models (LLMs) have shown remarkable capabilities across\nvarious software engineering tasks; however, their effectiveness in code\nmigration, adapting code to run in different environments, remains\ninsufficiently studied. In this work, we introduce CODEMENV: Code Migration\nAcross Environment, a new benchmark specifically designed to assess LLMs'\nabilities in code migration scenarios. CODEMENV consists of 922 examples\nspanning 19 Python and Java packages, and covers three core tasks: (1)\nidentifying functions incompatible with specific versions, (2) detecting\nchanges in function definitions, and (3) adapting code to target environments.\nExperimental evaluation with seven LLMs on CODEMENV yields an average pass@1\nrate of 26.50%, with GPT-4O achieving the highest score at 43.84%. Key findings\ninclude: (i) LLMs tend to be more proficient with newer function versions,\nwhich aids in migrating legacy code, and (ii) LLMs sometimes exhibit logical\ninconsistencies by identifying function changes irrelevant to the intended\nmigration environment. The datasets are available at\nhttps://github.com/xdshen-ai/Benchmark-of-Code-Migration.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86CODEMENV\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u8fc1\u79fb\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aGPT-4O\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6a21\u578b\u5728\u5904\u7406\u65e7\u7248\u672c\u51fd\u6570\u65f6\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u586b\u8865\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u8fc1\u79fb\u4efb\u52a1\u4e2d\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u5e76\u5206\u6790\u5176\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u8bbe\u8ba1\u4e86CODEMENV\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b922\u4e2aPython\u548cJava\u793a\u4f8b\uff0c\u8986\u76d6\u4e09\u7c7b\u6838\u5fc3\u4efb\u52a1\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e03\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u5e73\u5747pass@1\u5f97\u5206\u4e3a26.50%\uff0cGPT-4O\u4ee543.84%\u7684\u6700\u9ad8\u5206\u9886\u5148\uff0c\u4f46\u6a21\u578b\u5728\u5904\u7406\u65e7\u7248\u672c\u51fd\u6570\u65f6\u5b58\u5728\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u8fc1\u79fb\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u5bf9\u65e7\u7248\u672c\u4ee3\u7801\u7684\u5904\u7406\u80fd\u529b\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u4ee3\u7801\u8fc1\u79fb,CODEMENV,GPT-4O,\u8bc4\u4f30\u57fa\u51c6"}}
{"id": "2506.00171", "pdf": "https://arxiv.org/pdf/2506.00171", "abs": "https://arxiv.org/abs/2506.00171", "authors": ["Nicol\u00e1s Garc\u00eda Trillos", "Chenghui Li", "Raghavendra Venkatraman"], "title": "Minimax Rates for the Estimation of Eigenpairs of Weighted Laplace-Beltrami Operators on Manifolds", "categories": ["stat.ML", "cs.LG", "math.AP"], "comment": null, "summary": "We study the problem of estimating eigenpairs of elliptic differential\noperators from samples of a distribution $\\rho$ supported on a manifold $M$.\nThe operators discussed in the paper are relevant in unsupervised learning and\nin particular are obtained by taking suitable scaling limits of widely used\ngraph Laplacians over data clouds. We study the minimax risk for this eigenpair\nestimation problem and explore the rates of approximation that can be achieved\nby commonly used graph Laplacians built from random data. More concretely,\nassuming that $\\rho$ belongs to a certain family of distributions with\ncontrolled second derivatives, and assuming that the $d$-dimensional manifold\n$M$ where $\\rho$ is supported has bounded geometry, we prove that the\nstatistical minimax rate for approximating eigenvalues and eigenvectors in the\n$H^1(M)$-sense is $n^{-2/(d+4)}$, a rate that matches the minimax rate for a\nclosely related density estimation problem. We then revisit the literature\nstudying Laplacians over proximity graphs in the large data limit and prove\nthat, under slightly stronger regularity assumptions on the data generating\nmodel, eigenpairs of graph Laplacians induce manifold agnostic estimators with\nan error of approximation that, up to logarithmic corrections, matches our\nlower bounds. Our analysis allows us to expand the existing literature on\ngraph-based learning in at least two significant ways: 1) we consider stronger\nnorms to measure the error of approximation than the ones that had been\nanalyzed in the past; 2) our rates of convergence are uniform over a family of\nsmooth distributions and do not just apply to densities with special\nsymmetries, and, as a consequence of our lower bounds, are essentially sharp\nwhen the connectivity of the graph is sufficiently high.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u5206\u5e03\u6837\u672c\u4f30\u8ba1\u692d\u5706\u5fae\u5206\u7b97\u5b50\u7279\u5f81\u5bf9\u7684\u95ee\u9898\uff0c\u5206\u6790\u4e86\u6700\u5c0f\u6700\u5927\u98ce\u9669\uff0c\u5e76\u8bc1\u660e\u4e86\u8fd1\u4f3c\u7387\u4e0e\u5bc6\u5ea6\u4f30\u8ba1\u95ee\u9898\u76f8\u5339\u914d\uff0c\u6269\u5c55\u4e86\u57fa\u4e8e\u56fe\u7684\u5b66\u4e60\u7684\u73b0\u6709\u6587\u732e\u3002", "motivation": "\u7814\u7a76\u692d\u5706\u5fae\u5206\u7b97\u5b50\u7279\u5f81\u5bf9\u7684\u4f30\u8ba1\u95ee\u9898\uff0c\u65e8\u5728\u901a\u8fc7\u968f\u673a\u6570\u636e\u7684\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u627e\u5230\u6700\u4f18\u7684\u8fd1\u4f3c\u7387\uff0c\u5e76\u6269\u5c55\u56fe\u57fa\u5b66\u4e60\u7684\u5206\u6790\u8303\u56f4\u3002", "method": "\u5229\u7528\u6709\u754c\u51e0\u4f55\u652f\u6491\u7684\u6d41\u5f62\u4e0a\u7684\u5206\u5e03\u6837\u672c\uff0c\u5206\u6790\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u7684\u7279\u5f81\u5bf9\u4f30\u8ba1\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u7279\u5b9a\u8303\u6570\u4e0b\u7684\u8fd1\u4f3c\u8bef\u5dee\u3002", "result": "\u8bc1\u660e\u4e86\u6700\u5c0f\u6700\u5927\u8fd1\u4f3c\u7387\u4e3a$n^{-2/(d+4)}$\uff0c\u5e76\u901a\u8fc7\u66f4\u5f3a\u7684\u6b63\u5219\u5047\u8bbe\u9a8c\u8bc1\u4e86\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u80fd\u751f\u6210\u6d41\u5f62\u4e0d\u53ef\u77e5\u7684\u4f30\u8ba1\u5668\uff0c\u5176\u8bef\u5dee\u4e0e\u4e0b\u754c\u5339\u914d\u3002", "conclusion": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86\u56fe\u57fa\u5b66\u4e60\u7684\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u8fd1\u4f3c\u7387\u7684\u666e\u9002\u6027\u548c\u6700\u4f18\u6027\u3002", "keywords": "\u692d\u5706\u5fae\u5206\u7b97\u5b50\uff0c\u7279\u5f81\u5bf9\u4f30\u8ba1\uff0c\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u6700\u5c0f\u6700\u5927\u98ce\u9669"}}
{"id": "2506.00180", "pdf": "https://arxiv.org/pdf/2506.00180", "abs": "https://arxiv.org/abs/2506.00180", "authors": ["Juho Kim"], "title": "Empirical Validation of the Independent Chip Model", "categories": ["cs.GT", "cs.LG"], "comment": null, "summary": "The independent chip model (ICM) forms a cornerstone of all modern poker\ntournament strategy. However, despite its prominence, the ICM's performance in\nthe real world has not been sufficiently scrutinized, especially at a large\nscale. In this paper, we introduce our new dataset of poker tournaments,\nconsisting of results of over ten thousand events. Then, using this dataset, we\nperform two experiments as part of a large-scale empirical validation of the\nICM. First, we verify that the ICM performs more accurately than a baseline we\npropose. Second, we obtain empirical evidence of the ICM underestimating the\nperformances of players with larger stacks while overestimating those who are\nshort-stacked. Our contributions may be useful to future researchers developing\nnew algorithms for estimating a player's value in poker tournaments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5bf9\u8d85\u8fc7\u4e00\u4e07\u573a\u6251\u514b\u9526\u6807\u8d5b\u6570\u636e\u7684\u5206\u6790\uff0c\u5b9e\u8bc1\u9a8c\u8bc1\u4e86\u72ec\u7acb\u7b79\u7801\u6a21\u578b\uff08ICM\uff09\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u73b0\u5b9e\u4e2d\u5b58\u5728\u5bf9\u7b79\u7801\u591a\u5be1\u73a9\u5bb6\u7684\u4ef7\u503c\u8bc4\u4f30\u504f\u5dee\u3002", "motivation": "\u72ec\u7acb\u7b79\u7801\u6a21\u578b\uff08ICM\uff09\u867d\u7136\u5728\u73b0\u4ee3\u6251\u514b\u9526\u6807\u8d5b\u7b56\u7565\u4e2d\u5360\u636e\u91cd\u8981\u5730\u4f4d\uff0c\u4f46\u5176\u5728\u73b0\u5b9e\u4e2d\u7684\u8868\u73b0\u5c1a\u672a\u5f97\u5230\u5145\u5206\u9a8c\u8bc1\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u6536\u96c6\u548c\u5206\u6790\u8d85\u8fc7\u4e00\u4e07\u573a\u6251\u514b\u9526\u6807\u8d5b\u7684\u6570\u636e\uff0c\u8bbe\u8ba1\u4e86\u4e24\u7ec4\u5b9e\u9a8c\uff1a\u9a8c\u8bc1ICM\u76f8\u8f83\u4e8e\u57fa\u7ebf\u7684\u51c6\u786e\u6027\uff0c\u4ee5\u53ca\u89c2\u5bdfICM\u5bf9\u4e0d\u540c\u7b79\u7801\u91cf\u73a9\u5bb6\u7684\u8bc4\u4f30\u504f\u5dee\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cICM\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u6240\u63d0\u51fa\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u4f46\u4f1a\u4f4e\u4f30\u7b79\u7801\u91cf\u8f83\u5927\u73a9\u5bb6\u7684\u8868\u73b0\uff0c\u540c\u65f6\u9ad8\u4f30\u7b79\u7801\u91cf\u8f83\u5c0f\u73a9\u5bb6\u7684\u8868\u73b0\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u672a\u6765\u5f00\u53d1\u6539\u8fdb\u6251\u514b\u9526\u6807\u8d5b\u4e2d\u73a9\u5bb6\u4ef7\u503c\u4f30\u7b97\u7b97\u6cd5\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u5b9e\u8bc1\u4f9d\u636e\u3002", "keywords": "\u72ec\u7acb\u7b79\u7801\u6a21\u578b\uff08ICM\uff09\u3001\u6251\u514b\u9526\u6807\u8d5b\u3001\u5b9e\u8bc1\u9a8c\u8bc1\u3001\u8bc4\u4f30\u504f\u5dee"}}
{"id": "2506.00182", "pdf": "https://arxiv.org/pdf/2506.00182", "abs": "https://arxiv.org/abs/2506.00182", "authors": ["Atsushi Suzuki"], "title": "Overfitting has a limitation: a model-independent generalization error bound based on R\u00e9nyi entropy", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT", "math.ST", "stat.TH"], "comment": null, "summary": "Will further scaling up of machine learning models continue to bring success?\nA significant challenge in answering this question lies in understanding\ngeneralization error, which is the impact of overfitting. Understanding\ngeneralization error behavior of increasingly large-scale machine learning\nmodels remains a significant area of investigation, as conventional analyses\noften link error bounds to model complexity, failing to fully explain the\nsuccess of extremely large architectures. This research introduces a novel\nperspective by establishing a model-independent upper bound for generalization\nerror applicable to algorithms whose outputs are determined solely by the\ndata's histogram, such as empirical risk minimization or gradient-based\nmethods. Crucially, this bound is shown to depend only on the R\\'enyi entropy\nof the data-generating distribution, suggesting that a small generalization\nerror can be maintained even with arbitrarily large models, provided the data\nquantity is sufficient relative to this entropy. This framework offers a direct\nexplanation for the phenomenon where generalization performance degrades\nsignificantly upon injecting random noise into data, where the performance\ndegrade is attributed to the consequent increase in the data distribution's\nR\\'enyi entropy. Furthermore, we adapt the no-free-lunch theorem to be\ndata-distribution-dependent, demonstrating that an amount of data corresponding\nto the R\\'enyi entropy is indeed essential for successful learning, thereby\nhighlighting the tightness of our proposed generalization bound.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u662f\u5426\u6301\u7eed\u5e26\u6765\u6210\u529f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u6cdb\u5316\u8bef\u5dee\u4e0a\u754c\uff0c\u4ec5\u4f9d\u8d56\u4e8e\u6570\u636e\u7684R\u00e9nyi\u71b5\u3002", "motivation": "\u7406\u89e3\u6cdb\u5316\u8bef\u5dee\u884c\u4e3a\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u6a21\u578b\u4e2d\u7684\u8868\u73b0\uff0c\u56e0\u4e3a\u4f20\u7edf\u5206\u6790\u65e0\u6cd5\u5b8c\u5168\u89e3\u91ca\u8d85\u5927\u67b6\u6784\u7684\u6210\u529f\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u4ec5\u7531\u6570\u636e\u76f4\u65b9\u56fe\u786e\u5b9a\u8f93\u51fa\u7684\u7b97\u6cd5\u7684\u6cdb\u5316\u8bef\u5dee\u4e0a\u754c\uff0c\u5e76\u63a2\u8ba8\u5176\u5bf9\u6570\u636eR\u00e9nyi\u71b5\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u53ea\u8981\u6570\u636e\u91cf\u76f8\u5bf9\u4e8eR\u00e9nyi\u71b5\u8db3\u591f\u5927\uff0c\u5373\u4f7f\u6a21\u578b\u65e0\u9650\u589e\u5927\uff0c\u4e5f\u80fd\u4fdd\u6301\u8f83\u5c0f\u7684\u6cdb\u5316\u8bef\u5dee\u3002", "conclusion": "\u6570\u636e\u5206\u5e03\u53ca\u5176R\u00e9nyi\u71b5\u662f\u5f71\u54cd\u6cdb\u5316\u6027\u80fd\u7684\u5173\u952e\uff0c\u6846\u67b6\u89e3\u91ca\u4e86\u566a\u58f0\u6ce8\u5165\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u73b0\u8c61\u3002", "keywords": "\u6cdb\u5316\u8bef\u5dee, R\u00e9nyi\u71b5, \u5927\u89c4\u6a21\u6a21\u578b, \u673a\u5668\u5b66\u4e60"}}
{"id": "2506.00805", "pdf": "https://arxiv.org/pdf/2506.00805", "abs": "https://arxiv.org/abs/2506.00805", "authors": ["Songtao Jiang", "Yan Zhang", "Yeying Jin", "Zhihang Tang", "Yangyang Wu", "Yang Feng", "Jian Wu", "Zuozhu Liu"], "title": "HSCR: Hierarchical Self-Contrastive Rewarding for Aligning Medical Vision Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Medical Vision-Language Models (Med-VLMs) have achieved success across\nvarious tasks, yet most existing methods overlook the modality misalignment\nissue that can lead to untrustworthy responses in clinical settings. In this\npaper, we propose Hierarchical Self-Contrastive Rewarding (HSCR), a novel\napproach that addresses two critical challenges in Med-VLM alignment: 1)\nCost-effective generation of high-quality preference data; 2) Capturing nuanced\nand context-aware preferences for improved alignment. HSCR first leverages the\ninherent capability of Med-VLMs to generate dispreferred responses with higher\nsampling probability. By analyzing output logit shifts after visual token\ndropout, we identify modality-coupled tokens that induce misalignment and\nderive an implicit alignment reward function. This function guides token\nreplacement with hallucinated ones during decoding, producing high-quality\ndispreferred data. Furthermore, HSCR introduces a multi-level preference\noptimization strategy, which extends beyond traditional adjacent-level\noptimization by incorporating nuanced implicit preferences, leveraging relative\nquality in dispreferred data to capture subtle alignment cues for more precise\nand context-aware optimization. Extensive experiments across multiple medical\ntasks, including Med-VQA, medical image captioning and instruction following,\ndemonstrate that HSCR not only enhances zero-shot performance but also\nsignificantly improves modality alignment and trustworthiness with just 2,000\ntraining entries.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHSCR\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08Med-VLMs\uff09\u4e2d\u7684\u6a21\u6001\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u5bf9\u6bd4\u5956\u52b1\u548c\u591a\u7ea7\u504f\u597d\u4f18\u5316\u63d0\u5347\u6a21\u578b\u7684\u5bf9\u9f50\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u53ef\u80fd\u56e0\u6a21\u6001\u4e0d\u5bf9\u9f50\u95ee\u9898\u4ea7\u751f\u4e0d\u53ef\u9760\u7684\u54cd\u5e94\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "HSCR\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u504f\u597d\u6570\u636e\u548c\u591a\u7ea7\u504f\u597d\u4f18\u5316\u7b56\u7565\uff0c\u5229\u7528\u89c6\u89c9\u6807\u8bb0\u4e22\u5931\u5206\u6790\u548c\u9690\u5f0f\u5bf9\u9f50\u5956\u52b1\u51fd\u6570\u6307\u5bfc\u89e3\u7801\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHSCR\u5728\u591a\u79cd\u533b\u5b66\u4efb\u52a1\uff08\u5982Med-VQA\u3001\u533b\u5b66\u56fe\u50cf\u63cf\u8ff0\u548c\u6307\u4ee4\u9075\u5faa\uff09\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u6027\u80fd\u548c\u6a21\u6001\u5bf9\u9f50\u6027\uff0c\u4ec5\u97002000\u4e2a\u8bad\u7ec3\u6761\u76ee\u3002", "conclusion": "HSCR\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "keywords": "Med-VLMs, \u6a21\u6001\u4e0d\u5bf9\u9f50, HSCR, \u81ea\u5bf9\u6bd4\u5956\u52b1, \u591a\u7ea7\u504f\u597d\u4f18\u5316"}}
{"id": "2506.00197", "pdf": "https://arxiv.org/pdf/2506.00197", "abs": "https://arxiv.org/abs/2506.00197", "authors": ["Xinyue Shen", "Yun Shen", "Michael Backes", "Yang Zhang"], "title": "When GPT Spills the Tea: Comprehensive Assessment of Knowledge File Leakage in GPTs", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Knowledge files have been widely used in large language model (LLM) agents,\nsuch as GPTs, to improve response quality. However, concerns about the\npotential leakage of knowledge files have grown significantly. Existing studies\ndemonstrate that adversarial prompts can induce GPTs to leak knowledge file\ncontent. Yet, it remains uncertain whether additional leakage vectors exist,\nparticularly given the complex data flows across clients, servers, and\ndatabases in GPTs. In this paper, we present a comprehensive risk assessment of\nknowledge file leakage, leveraging a novel workflow inspired by Data Security\nPosture Management (DSPM). Through the analysis of 651,022 GPT metadata, 11,820\nflows, and 1,466 responses, we identify five leakage vectors: metadata, GPT\ninitialization, retrieval, sandboxed execution environments, and prompts. These\nvectors enable adversaries to extract sensitive knowledge file data such as\ntitles, content, types, and sizes. Notably, the activation of the built-in tool\nCode Interpreter leads to a privilege escalation vulnerability, enabling\nadversaries to directly download original knowledge files with a 95.95% success\nrate. Further analysis reveals that 28.80% of leaked files are copyrighted,\nincluding digital copies from major publishers and internal materials from a\nlisted company. In the end, we provide actionable solutions for GPT builders\nand platform providers to secure the GPT data supply chain.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u65b0\u578b\u6570\u636e\u5b89\u5168\u7ba1\u7406\u65b9\u6cd5\uff0c\u5168\u9762\u8bc4\u4f30\u4e86\u77e5\u8bc6\u6587\u4ef6\u6cc4\u6f0f\u98ce\u9669\uff0c\u63ed\u793a\u4e86\u4e94\u4e2a\u6cc4\u6f0f\u9014\u5f84\uff0c\u5e76\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u968f\u7740\u77e5\u8bc6\u6587\u4ef6\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u6cc4\u6f0f\u98ce\u9669\u65e5\u76ca\u589e\u52a0\uff0c\u9700\u8981\u5168\u9762\u8bc4\u4f30\u4ee5\u4fdd\u969c\u6570\u636e\u5b89\u5168\u3002", "method": "\u5229\u7528\u6570\u636e\u5b89\u5168\u7ba1\u7406\u6846\u67b6\uff0c\u5206\u6790\u4e86\u5927\u91cfGPT\u5143\u6570\u636e\u3001\u6570\u636e\u6d41\u548c\u54cd\u5e94\uff0c\u8bc6\u522b\u4e86\u4e94\u79cd\u6cc4\u6f0f\u9014\u5f84\u3002", "result": "\u53d1\u73b0\u4e94\u4e2a\u6cc4\u6f0f\u9014\u5f84\uff0c\u5176\u4e2d\u5185\u7f6e\u5de5\u5177Code Interpreter\u5b58\u5728\u7279\u6743\u5347\u7ea7\u6f0f\u6d1e\uff0c\u6210\u529f\u7387\u8fbe95.95%\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u77e5\u8bc6\u6587\u4ef6\u7684\u4e25\u91cd\u6cc4\u6f0f\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u77e5\u8bc6\u6587\u4ef6\u6cc4\u6f0f\uff1b GPT\uff1b \u6570\u636e\u5b89\u5168\uff1b \u6cc4\u6f0f\u9014\u5f84\uff1b Code Interpreter"}}
{"id": "2506.00924", "pdf": "https://arxiv.org/pdf/2506.00924", "abs": "https://arxiv.org/abs/2506.00924", "authors": ["Parsa Hassani Shariat Panahi", "Amir Hossein Jalilvand", "M. Hasan Najafi"], "title": "Bridging Subjective and Objective QoE: Operator-Level Aggregation Using LLM-Based Comment Analysis and Network MOS Comparison", "categories": ["cs.NI", "cs.AI", "cs.HC"], "comment": "19 ppages, 13 figures", "summary": "This paper introduces a dual-layer framework for network operator-side\nquality of experience (QoE) assessment that integrates both objective network\nmodeling and subjective user perception extracted from live-streaming\nplatforms. On the objective side, we develop a machine learning model trained\non mean opinion scores (MOS) computed via the ITU-T P.1203 reference\nimplementation, allowing accurate prediction of user-perceived video quality\nusing only network parameters such as packet loss, delay, jitter, and\nthroughput without reliance on video content or client-side instrumentation. On\nthe subjective side, we present a semantic filtering and scoring pipeline that\nprocesses user comments from live streams to extract performance-related\nfeedback. A large language model is used to assign scalar MOS scores to\nfiltered comments in a deterministic and reproducible manner. To support\nscalable and interpretable analysis, we construct a labeled dataset of 47,894\nlive-stream comments, of which about 34,000 are identified as QoE-relevant\nthrough multi-layer semantic filtering. Each comment is enriched with simulated\nInternet Service Provider attribution and temporally aligned using synthetic\ntimestamps in 5-min intervals. The resulting dataset enables operator-level\naggregation and time-series analysis of user-perceived quality. A delta MOS\nmetric is proposed to measure each Internet service provider's deviation from\nplatform-wide sentiment, allowing detection of localized degradations even in\nthe absence of direct network telemetry. A controlled outage simulation\nconfirms the framework's effectiveness in identifying service disruptions\nthrough comment-based trends alone. The system provides each operator with its\nown subjective MOS and the global platform average per interval, enabling\nreal-time interpretation of performance deviations and comparison with\nobjective network-based QoE estimates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u7684QoE\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u5ba2\u89c2\u7f51\u7edc\u5efa\u6a21\u548c\u4e3b\u89c2\u7528\u6237\u53cd\u9988\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u8bed\u4e49\u5206\u6790\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684QoE\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5ba2\u6237\u7aef\u5de5\u5177\u6216\u89c6\u9891\u5185\u5bb9\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7f51\u7edc\u53c2\u6570\u548c\u7528\u6237\u8bc4\u8bba\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eITU-T P.1203\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u89c6\u9891\u8d28\u91cf\uff0c\u5e76\u4f7f\u7528LLM\u5904\u7406\u7528\u6237\u8bc4\u8bba\u4ee5\u63d0\u53d6QoE\u53cd\u9988\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u6807\u6ce8\u6570\u636e\u96c6\u652f\u6301\u5206\u6790\u3002", "result": "\u6784\u5efa\u4e8647,894\u6761\u8bc4\u8bba\u7684\u6570\u636e\u96c6\uff0c34,000\u6761\u4e0eQoE\u76f8\u5173\u3002\u63d0\u51fadelta MOS\u6307\u6807\u68c0\u6d4b\u5c40\u90e8\u670d\u52a1\u964d\u7ea7\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u4e2d\u65ad\u6a21\u62df\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u901a\u8fc7\u7f51\u7edc\u53c2\u6570\u548c\u7528\u6237\u53cd\u9988\u7684\u5b9e\u65f6QoE\u8bc4\u4f30\uff0c\u4e3a\u8fd0\u8425\u5546\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5206\u6790\u5de5\u5177\u3002", "keywords": "QoE\u8bc4\u4f30\uff0c\u673a\u5668\u5b66\u4e60\uff0c\u8bed\u4e49\u5206\u6790\uff0c\u7528\u6237\u53cd\u9988\uff0c\u7f51\u7edc\u6027\u80fd"}}
{"id": "2506.00223", "pdf": "https://arxiv.org/pdf/2506.00223", "abs": "https://arxiv.org/abs/2506.00223", "authors": ["Mohammad Saleh Hasankhani"], "title": "Enhancing Drug Discovery: Autoencoder-Based Latent Space Augmentation for Improved Molecular Solubility Prediction using LatMixSol", "categories": ["q-bio.QM", "cs.LG"], "comment": null, "summary": "Accurate prediction of molecular solubility is a cornerstone of early-stage\ndrug discovery, yet conventional machine learning models face significant\nchallenges due to limited labeled data and the high-dimensional nature of\nmolecular descriptors. To address these issues, we propose LatMixSol, a novel\nlatent space augmentation framework that combines autoencoder-based feature\ncompression with guided interpolation to enrich training data. Our approach\nfirst encodes molecular descriptors into a low-dimensional latent space using a\ntwo-layer autoencoder. Spectral clustering is then applied to group chemically\nsimilar molecules, enabling targeted MixUp-style interpolation within clusters.\nSynthetic samples are generated by blending latent vectors of cluster members\nand decoding them back to the original feature space. Evaluated on the\nHuuskonen solubility benchmark, LatMixSol demonstrates consistent improvements\nacross three of four gradient-boosted regressors (CatBoost, LightGBM,\nHistGradientBoosting), achieving RMSE reductions of 3.2-7.6% and R-squared\nincreases of 0.5-1.5%. Notably, HistGradientBoosting shows the most significant\nenhancement with a 7.6% RMSE improvement. Our analysis confirms that\ncluster-guided latent space augmentation preserves chemical validity while\nexpanding dataset diversity, offering a computationally efficient strategy to\nenhance predictive models in resource-constrained drug discovery pipelines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLatMixSol\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u5206\u5b50\u6eb6\u89e3\u5ea6\u7684\u9884\u6d4b\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u589e\u5f3a\u548c\u805a\u7c7b\u6307\u5bfc\u7684\u63d2\u503c\u6765\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u548c\u7ef4\u5ea6\u95ee\u9898\u3002", "motivation": "\u5728\u836f\u7269\u53d1\u73b0\u65e9\u671f\u9636\u6bb5\uff0c\u5206\u5b50\u6eb6\u89e3\u5ea6\u7684\u51c6\u786e\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u56e0\u6807\u8bb0\u6570\u636e\u6709\u9650\u548c\u5206\u5b50\u63cf\u8ff0\u7b26\u7684\u9ad8\u7ef4\u6027\u800c\u9762\u4e34\u6311\u6218\u3002", "method": "\u65b9\u6cd5\u7ed3\u5408\u4e86\u81ea\u7f16\u7801\u5668\u7279\u5f81\u538b\u7f29\u548c\u805a\u7c7b\u6307\u5bfc\u7684MixUp\u63d2\u503c\uff0c\u751f\u6210\u5408\u6210\u6837\u672c\u4ee5\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cLatMixSol\u5728\u4e09\u4e2a\u68af\u5ea6\u589e\u5f3a\u56de\u5f52\u5668\u4e2d\u663e\u8457\u964d\u4f4e\u4e86RMSE\uff083.2%-7.6%\uff09\u5e76\u63d0\u9ad8\u4e86R-squared\uff080.5%-1.5%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5316\u5b66\u6709\u6548\u6027\u524d\u63d0\u4e0b\u6269\u5c55\u4e86\u6570\u636e\u591a\u6837\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u836f\u7269\u53d1\u73b0\u6d41\u6c34\u7ebf\u63d0\u4f9b\u4e86\u9ad8\u6548\u7b56\u7565\u3002", "keywords": "\u5206\u5b50\u6eb6\u89e3\u5ea6\u9884\u6d4b, \u6f5c\u5728\u7a7a\u95f4\u589e\u5f3a, \u81ea\u7f16\u7801\u5668, MixUp\u63d2\u503c, \u836f\u7269\u53d1\u73b0"}}
{"id": "2506.00927", "pdf": "https://arxiv.org/pdf/2506.00927", "abs": "https://arxiv.org/abs/2506.00927", "authors": ["Tianrui Pan", "Jie Liu", "Zewen Huang", "Jie Tang", "Gangshan Wu"], "title": "In-the-wild Audio Spatialization with Flexible Text-guided Localization", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Accepted by ACL 2025 main", "summary": "To enhance immersive experiences, binaural audio offers spatial awareness of\nsounding objects in AR, VR, and embodied AI applications. While existing audio\nspatialization methods can generally map any available monaural audio to\nbinaural audio signals, they often lack the flexible and interactive control\nneeded in complex multi-object user-interactive environments. To address this,\nwe propose a Text-guided Audio Spatialization (TAS) framework that utilizes\nflexible text prompts and evaluates our model from unified generation and\ncomprehension perspectives. Due to the limited availability of premium and\nlarge-scale stereo data, we construct the SpatialTAS dataset, which encompasses\n376,000 simulated binaural audio samples to facilitate the training of our\nmodel. Our model learns binaural differences guided by 3D spatial location and\nrelative position prompts, augmented by flipped-channel audio. It outperforms\nexisting methods on both simulated and real-recorded datasets, demonstrating\nsuperior generalization and accuracy. Besides, we develop an assessment model\nbased on Llama-3.1-8B, which evaluates the spatial semantic coherence between\nour generated binaural audio and text prompts through a spatial reasoning task.\nResults demonstrate that text prompts provide flexible and interactive control\nto generate binaural audio with excellent quality and semantic consistency in\nspatial locations. Dataset is available at\n\\href{https://github.com/Alice01010101/TASU}", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7684\u97f3\u9891\u7a7a\u95f4\u5316\uff08TAS\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u6a21\u578b\u4f18\u5316\uff0c\u63d0\u5347\u53cc\u8033\u97f3\u9891\u7684\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u7a7a\u95f4\u5316\u65b9\u6cd5\u5728\u590d\u6742\u591a\u5bf9\u8c61\u4ea4\u4e92\u73af\u5883\u4e2d\u7f3a\u4e4f\u7075\u6d3b\u63a7\u5236\uff0c\u9700\u8981\u66f4\u4f18\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6587\u672c\u63d0\u793a\u6307\u5bfc\u53cc\u8033\u97f3\u9891\u751f\u6210\uff0c\u6784\u5efaSpatialTAS\u6570\u636e\u96c6\u5e76\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u5907\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u6587\u672c\u63d0\u793a\u4e3a\u53cc\u8033\u97f3\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u7075\u6d3b\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u7684\u8f93\u51fa\u3002", "keywords": "\u53cc\u8033\u97f3\u9891\u3001\u97f3\u9891\u7a7a\u95f4\u5316\u3001\u6587\u672c\u63d0\u793a\u3001AR/VR\u3001\u4ea4\u4e92\u63a7\u5236"}}
{"id": "2506.00226", "pdf": "https://arxiv.org/pdf/2506.00226", "abs": "https://arxiv.org/abs/2506.00226", "authors": ["Oldemar Rodr\u00edguez"], "title": "Riemannian Principal Component Analysis", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO", "stat.TH"], "comment": "arXiv admin note: substantial text overlap with arXiv:2405.06799", "summary": "This paper proposes an innovative extension of Principal Component Analysis\n(PCA) that transcends the traditional assumption of data lying in Euclidean\nspace, enabling its application to data on Riemannian manifolds. The primary\nchallenge addressed is the lack of vector space operations on such manifolds.\nFletcher et al., in their work {\\em Principal Geodesic Analysis for the Study\nof Nonlinear Statistics of Shape}, proposed Principal Geodesic Analysis (PGA)\nas a geometric approach to analyze data on Riemannian manifolds, particularly\neffective for structured datasets like medical images, where the manifold's\nintrinsic structure is apparent. However, PGA's applicability is limited when\ndealing with general datasets that lack an implicit local distance notion. In\nthis work, we introduce a generalized framework, termed {\\em Riemannian\nPrincipal Component Analysis (R-PCA)}, to extend PGA for any data endowed with\na local distance structure. Specifically, we adapt the PCA methodology to\nRiemannian manifolds by equipping data tables with local metrics, enabling the\nincorporation of manifold geometry. This framework provides a unified approach\nfor dimensionality reduction and statistical analysis directly on manifolds,\nopening new possibilities for datasets with region-specific or part-specific\ndistance notions, ensuring respect for their intrinsic geometric properties.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684Riemannian\u4e3b\u6210\u5206\u5206\u6790\uff08R-PCA\uff09\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86PCA\u5728\u975e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfPGA\u5728\u5904\u7406\u7f3a\u4e4f\u5c40\u90e8\u8ddd\u79bb\u6982\u5ff5\u7684\u901a\u7528\u6570\u636e\u96c6\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfPCA\u5047\u8bbe\u6570\u636e\u4f4d\u4e8e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u800cPGA\u867d\u7136\u9002\u7528\u4e8eRiemannian\u6d41\u5f62\uff0c\u4f46\u5bf9\u7f3a\u4e4f\u5c40\u90e8\u8ddd\u79bb\u6982\u5ff5\u7684\u6570\u636e\u96c6\u9002\u7528\u6027\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u5177\u6709\u5c40\u90e8\u8ddd\u79bb\u7ed3\u6784\u7684\u6570\u636e\u3002", "method": "\u901a\u8fc7\u4e3a\u6570\u636e\u8868\u914d\u5907\u5c40\u90e8\u5ea6\u91cf\uff0c\u5c06PCA\u65b9\u6cd5\u63a8\u5e7f\u5230Riemannian\u6d41\u5f62\uff0c\u4ece\u800c\u878d\u5165\u6d41\u5f62\u51e0\u4f55\u7ed3\u6784\uff0c\u5f62\u6210R-PCA\u6846\u67b6\u3002", "result": "R-PCA\u4e3a\u6d41\u5f62\u4e0a\u7684\u964d\u7ef4\u548c\u7edf\u8ba1\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5177\u6709\u533a\u57df\u6216\u90e8\u5206\u7279\u5b9a\u8ddd\u79bb\u6982\u5ff5\u7684\u6570\u636e\u96c6\u3002", "conclusion": "R-PCA\u6210\u529f\u6269\u5c55\u4e86PCA\u5728\u975e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u7684\u5e94\u7528\uff0c\u4e3a\u590d\u6742\u6570\u636e\u96c6\u7684\u51e0\u4f55\u6027\u8d28\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "keywords": "Riemannian\u4e3b\u6210\u5206\u5206\u6790, \u4e3b\u6d4b\u5730\u7ebf\u5206\u6790, \u6570\u636e\u964d\u7ef4, Riemannian\u6d41\u5f62, \u5c40\u90e8\u5ea6\u91cf"}}
{"id": "2506.00928", "pdf": "https://arxiv.org/pdf/2506.00928", "abs": "https://arxiv.org/abs/2506.00928", "authors": ["Olga Loginova", "Sof\u00eda Ortega Loguinova"], "title": "Deep Temporal Reasoning in Video Language Models: A Cross-Linguistic Evaluation of Action Duration and Completion through Perfect Times", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Human perception of events is intrinsically tied to distinguishing between\ncompleted (perfect and telic) and ongoing (durative) actions, a process\nmediated by both linguistic structure and visual cues. In this work, we\nintroduce the \\textbf{Perfect Times} dataset, a novel, quadrilingual (English,\nItalian, Russian, and Japanese) multiple-choice question-answering benchmark\ndesigned to assess video-language models (VLMs) on temporal reasoning. By\npairing everyday activity videos with event completion labels and\nperfectivity-tailored distractors, our dataset probes whether models truly\ncomprehend temporal dynamics or merely latch onto superficial markers.\nExperimental results indicate that state-of-the-art models, despite their\nsuccess on text-based tasks, struggle to mirror human-like temporal and causal\nreasoning grounded in video. This study underscores the necessity of\nintegrating deep multimodal cues to capture the nuances of action duration and\ncompletion within temporal and causal video dynamics, setting a new standard\nfor evaluating and advancing temporal reasoning in VLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aPerfect Times\u7684\u65b0\u578b\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u63a8\u7406\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u6a21\u62df\u4eba\u7c7b\u5bf9\u52a8\u4f5c\u5b8c\u6210\u6027\u548c\u6301\u7eed\u6027\u7684\u7406\u89e3\uff0c\u5f3a\u8c03\u4e86\u6574\u5408\u591a\u6a21\u6001\u7ebf\u7d22\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u4eba\u7c7b\u5982\u4f55\u901a\u8fc7\u8bed\u8a00\u548c\u89c6\u89c9\u7ebf\u7d22\u533a\u5206\u5df2\u5b8c\u6210\u548c\u6301\u7eed\u7684\u52a8\u4f5c\uff0c\u5e76\u8bc4\u4f30\u5f53\u524d\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u6a21\u62df\u8fd9\u79cd\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6784\u5efa\u4e00\u4e2a\u591a\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u610f\u5927\u5229\u8bed\u3001\u4fc4\u8bed\u3001\u65e5\u8bed\uff09\u7684\u591a\u9009\u9898\u6570\u636e\u96c6Perfect Times\uff0c\u7ed3\u5408\u65e5\u5e38\u6d3b\u52a8\u89c6\u9891\u548c\u52a8\u4f5c\u5b8c\u6210\u6027\u6807\u7b7e\uff0c\u5e76\u8bbe\u8ba1\u7279\u5b9a\u5e72\u6270\u9879\u6765\u6d4b\u8bd5\u6a21\u578b\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5c3d\u7ba1\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u6587\u672c\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u89c6\u9891\u4e2d\u6a21\u62df\u4eba\u7c7b\u65f6\u95f4\u548c\u56e0\u679c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u5f3a\u8c03\u4e86\u6574\u5408\u591a\u6a21\u6001\u7ebf\u7d22\u5bf9\u4e8e\u6355\u6349\u52a8\u4f5c\u6301\u7eed\u6027\u548c\u5b8c\u6210\u6027\u7ec6\u5fae\u5dee\u522b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u65f6\u95f4\u63a8\u7406\u7684\u65b0\u6807\u51c6\u3002", "keywords": "\u65f6\u95f4\u63a8\u7406\uff0c\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\uff0c\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u52a8\u4f5c\u5b8c\u6210\u6027\uff0cPerfect Times\u6570\u636e\u96c6"}}
{"id": "2506.00934", "pdf": "https://arxiv.org/pdf/2506.00934", "abs": "https://arxiv.org/abs/2506.00934", "authors": ["Goksenin Yuksel", "Marcel van Gerven", "Kiki van der Heijden"], "title": "General-purpose audio representation learning for real-world sound scenes", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "While audio foundation models perform well on myriad of tasks from sound\nclassification to speech analysis, these models are trained and tested on dry,\nnon-spatial, single-source audio clips. This limits their success in real-world\nsituations and results in spatially unaware audio embeddings. To address these\nlimitations, we propose a novel self-supervised training approach for\nGeneral-Purpose, Real-world Audio Models (GRAMs). The GRAM training approach\nenables robust spatial audio representation learning for naturalistic, noisy\nsound scenes and can be applied to any masking-based deep learning model. We\ndemonstrate the success of our approach by training two state-of-the-art\nmodels, one with a transformer and one with a mamba backbone. We assess the\nquality of the extracted audio representations from GRAMs using the original\nversion of the HEAR benchmark, a newly synthesized, naturalistic version of the\nHEAR benchmark, and novel sound localization tasks based on HEAR benchmark\ndatasets. The results show that our approach minimizes the performance gap\nbetween dry, non-spatial, single-source sound scenes and naturalistic sound\nscenes for crucial tasks such as auditory scene analysis, outperforming\nexisting state-of-the-art audio foundation models at a fraction of the training\nsteps. Moreover, GRAMs show state-of-the-art performance on sound localization\ntasks, exceeding even supervised sound localization models. In sum, the\nproposed approach represents a significant advancement towards robust audio\nfoundation models for real-world applications with state-of-the-art performance\non naturalistic sound scenes as well as spatial audio representation learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5GRAM\uff0c\u7528\u4e8e\u63d0\u5347\u97f3\u9891\u57fa\u7840\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u7a7a\u95f4\u97f3\u9891\u8868\u5f81\u5b66\u4e60\u4e0a\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u57fa\u7840\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u591a\u6e90\u3001\u6709\u566a\u58f0\u548c\u7a7a\u95f4\u5316\u7684\u97f3\u9891\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5GRAM\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u57fa\u4e8e\u63a9\u7801\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u5728Transformer\u548cMamba\u67b6\u6784\u4e0a\u9a8c\u8bc1\u3002", "result": "GRAM\u663e\u8457\u7f29\u5c0f\u4e86\u5e72\u71e5\u3001\u5355\u6e90\u97f3\u9891\u573a\u666f\u4e0e\u81ea\u7136\u97f3\u9891\u573a\u666f\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u5728\u58f0\u97f3\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "GRAM\u4e3a\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u97f3\u9891\u57fa\u7840\u6a21\u578b\uff0c\u5c55\u73b0\u4e86\u5728\u81ea\u7136\u97f3\u9891\u573a\u666f\u548c\u7a7a\u95f4\u97f3\u9891\u5b66\u4e60\u4e2d\u7684\u9886\u5148\u6027\u80fd\u3002", "keywords": "\u97f3\u9891\u57fa\u7840\u6a21\u578b, \u81ea\u76d1\u7763\u5b66\u4e60, \u7a7a\u95f4\u97f3\u9891, GRAM, \u58f0\u97f3\u5b9a\u4f4d"}}
{"id": "2506.00228", "pdf": "https://arxiv.org/pdf/2506.00228", "abs": "https://arxiv.org/abs/2506.00228", "authors": ["Rebekah A. Gelp\u00ed", "Yibing Ju", "Ethan C. Jackson", "Yikai Tang", "Shon Verch", "Claas Voelcker", "William A. Cunningham"], "title": "Sorrel: A simple and flexible framework for multi-agent reinforcement learning", "categories": ["cs.MA", "cs.LG"], "comment": null, "summary": "We introduce Sorrel (https://github.com/social-ai-uoft/sorrel), a simple\nPython interface for generating and testing new multi-agent reinforcement\nlearning environments. This interface places a high degree of emphasis on\nsimplicity and accessibility, and uses a more psychologically intuitive\nstructure for the basic agent-environment loop, making it a useful tool for\nsocial scientists to investigate how learning and social interaction leads to\nthe development and change of group dynamics. In this short paper, we outline\nthe basic design philosophy and features of Sorrel.", "AI": {"tldr": "Sorrel \u662f\u4e00\u4e2a\u7b80\u5355\u7684 Python \u63a5\u53e3\uff0c\u7528\u4e8e\u751f\u6210\u548c\u6d4b\u8bd5\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u5f3a\u8c03\u7b80\u5355\u6027\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u9002\u5408\u793e\u4f1a\u79d1\u5b66\u5bb6\u7814\u7a76\u5b66\u4e60\u4e0e\u793e\u4ea4\u4e92\u52a8\u5982\u4f55\u5f71\u54cd\u7fa4\u4f53\u52a8\u6001\u3002", "motivation": "\u4e3a\u7814\u7a76\u5b66\u4e60\u4e0e\u793e\u4ea4\u4e92\u52a8\u5bf9\u7fa4\u4f53\u52a8\u6001\u7684\u5f71\u54cd\uff0c\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5355\u4e14\u5fc3\u7406\u76f4\u89c9\u5316\u7684\u5de5\u5177\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5f3a\u8c03\u7b80\u5355\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u7684 Python \u63a5\u53e3\uff0c\u91c7\u7528\u5fc3\u7406\u76f4\u89c9\u5316\u7684\u667a\u80fd\u4f53-\u73af\u5883\u5faa\u73af\u7ed3\u6784\u3002", "result": "Sorrel \u80fd\u591f\u5e2e\u52a9\u793e\u4f1a\u79d1\u5b66\u5bb6\u4fbf\u6377\u5730\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5b9e\u9a8c\u3002", "conclusion": "Sorrel \u662f\u4e00\u4e2a\u9002\u5408\u793e\u4f1a\u79d1\u5b66\u5bb6\u7814\u7a76\u7fa4\u4f53\u52a8\u6001\u7684\u5de5\u5177\uff0c\u8bbe\u8ba1\u7b80\u6d01\u4e14\u6613\u4e8e\u4f7f\u7528\u3002", "keywords": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60, \u793e\u4f1a\u4ea4\u4e92, \u7fa4\u4f53\u52a8\u6001, \u5fc3\u7406\u5b66, Python \u63a5\u53e3"}}
{"id": "2506.00252", "pdf": "https://arxiv.org/pdf/2506.00252", "abs": "https://arxiv.org/abs/2506.00252", "authors": ["Sammy Khalife", "Andrea Lodi"], "title": "How hard is learning to cut? Trade-offs and sample complexity", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "In the recent years, branch-and-cut algorithms have been the target of\ndata-driven approaches designed to enhance the decision making in different\nphases of the algorithm such as branching, or the choice of cutting planes\n(cuts). In particular, for cutting plane selection two score functions have\nbeen proposed in the literature to evaluate the quality of a cut:\nbranch-and-cut tree size and gap closed. In this paper, we present new sample\ncomplexity lower bounds, valid for both scores. We show that for a wide family\nof classes $\\mathcal{F}$ that maps an instance to a cut, learning over an\nunknown distribution of the instances to minimize those scores requires at\nleast (up to multiplicative constants) as many samples as learning from the\nsame class function $\\mathcal{F}$ any generic target function (using square\nloss). Our results also extend to the case of learning from a restricted set of\ncuts, namely those from the Simplex tableau. To the best of our knowledge,\nthese constitute the first lower bounds for the learning-to-cut framework. We\ncompare our bounds to known upper bounds in the case of neural networks and\nshow they are nearly tight. We illustrate our results with a graph neural\nnetwork selection evaluated on set covering and facility location integer\nprogramming models and we empirically show that the gap closed score is an\neffective proxy to minimize the branch-and-cut tree size. Although the gap\nclosed score has been extensively used in the integer programming literature,\nthis is the first principled analysis discussing both scores at the same time\nboth theoretically and computationally.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u5206\u652f\u5207\u5272\u7b97\u6cd5\u4e2d\u5207\u5272\u5e73\u9762\u9009\u62e9\u7684\u65b0\u6837\u672c\u590d\u6742\u5ea6\u4e0b\u754c\uff0c\u5e76\u5bf9\u4e24\u79cd\u8bc4\u5206\u51fd\u6570\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u9996\u6b21\u4e3a\u5b66\u4e60\u5207\u5272\u6846\u67b6\u63d0\u4f9b\u4e86\u4e0b\u754c\u3002", "motivation": "\u7814\u7a76\u5206\u652f\u5207\u5272\u7b97\u6cd5\u4e2d\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u5207\u5272\u5e73\u9762\u9009\u62e9\u4e0a\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6587\u732e\u4e2d\u7f3a\u4e4f\u7406\u8bba\u5206\u6790\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6837\u672c\u590d\u6742\u5ea6\u4e0b\u754c\u5206\u6790\uff0c\u6bd4\u8f83\u4e86\u4e24\u79cd\u8bc4\u5206\u51fd\u6570\uff08\u5206\u652f\u5207\u5272\u6811\u5927\u5c0f\u548c\u95f4\u9699\u95ed\u5408\uff09\u7684\u6837\u672c\u9700\u6c42\uff0c\u5e76\u6269\u5c55\u5230\u6709\u9650\u5207\u5272\u96c6\u7684\u5b66\u4e60\u3002", "result": "\u8bc1\u660e\u4e86\u5b66\u4e60\u5207\u5272\u95ee\u9898\u9700\u8981\u4e0e\u5b66\u4e60\u4e00\u822c\u76ee\u6807\u51fd\u6570\u76f8\u5f53\u7684\u6837\u672c\u91cf\uff0c\u4e14\u5728\u795e\u7ecf\u7f51\u7edc\u6848\u4f8b\u4e2d\u63a5\u8fd1\u5df2\u77e5\u4e0a\u754c\u3002", "conclusion": "\u95f4\u9699\u95ed\u5408\u8bc4\u5206\u4e3a\u5206\u652f\u5207\u5272\u6811\u5927\u5c0f\u7684\u6709\u6548\u4ee3\u7406\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u5207\u5272\u5e73\u9762\u9009\u62e9\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "keywords": "\u5206\u652f\u5207\u5272\u7b97\u6cd5,\u5207\u5272\u5e73\u9762\u9009\u62e9,\u6837\u672c\u590d\u6742\u5ea6,\u5b66\u4e60\u7406\u8bba,\u6574\u6570\u89c4\u5212"}}
{"id": "2506.00983", "pdf": "https://arxiv.org/pdf/2506.00983", "abs": "https://arxiv.org/abs/2506.00983", "authors": ["Chuan Meng", "Francesco Tonolini", "Fengran Mo", "Nikolaos Aletras", "Emine Yilmaz", "Gabriella Kazai"], "title": "Bridging the Gap: From Ad-hoc to Proactive Search in Conversations", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "H.3.3"], "comment": "Accepted as a full paper at SIGIR 2025", "summary": "Proactive search in conversations (PSC) aims to reduce user effort in\nformulating explicit queries by proactively retrieving useful relevant\ninformation given conversational context. Previous work in PSC either directly\nuses this context as input to off-the-shelf ad-hoc retrievers or further\nfine-tunes them on PSC data. However, ad-hoc retrievers are pre-trained on\nshort and concise queries, while the PSC input is longer and noisier. This\ninput mismatch between ad-hoc search and PSC limits retrieval quality. While\nfine-tuning on PSC data helps, its benefits remain constrained by this input\ngap. In this work, we propose Conv2Query, a novel conversation-to-query\nframework that adapts ad-hoc retrievers to PSC by bridging the input gap\nbetween ad-hoc search and PSC. Conv2Query maps conversational context into\nad-hoc queries, which can either be used as input for off-the-shelf ad-hoc\nretrievers or for further fine-tuning on PSC data. Extensive experiments on two\nPSC datasets show that Conv2Query significantly improves ad-hoc retrievers'\nperformance, both when used directly and after fine-tuning on PSC.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Conv2Query\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u6620\u5c04\u4e3a\u4f20\u7edf\u68c0\u7d22\u67e5\u8be2\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u68c0\u7d22\u5668\u5728\u5bf9\u8bdd\u73af\u5883\u4e2d\u8f93\u5165\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u8bdd\u4e3b\u52a8\u641c\u7d22\uff08PSC\uff09\u65b9\u6cd5\u56e0\u8f93\u5165\u683c\u5f0f\u4e0d\u5339\u914d\u800c\u9650\u5236\u4e86\u68c0\u7d22\u8d28\u91cf\uff0c\u4f20\u7edf\u68c0\u7d22\u5668\u9002\u5408\u77ed\u67e5\u8be2\u800c\u975e\u957f\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u3002", "method": "\u63d0\u51fa\u4e86Conv2Query\u6846\u67b6\uff0c\u5c06\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u8f6c\u6362\u4e3a\u9002\u5408\u4f20\u7edf\u68c0\u7d22\u5668\u7684\u67e5\u8be2\uff0c\u53ef\u76f4\u63a5\u6216\u5fae\u8c03\u540e\u7528\u4e8e\u68c0\u7d22\u3002", "result": "\u5728\u4e24\u4e2aPSC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cConv2Query\u663e\u8457\u63d0\u5347\u4e86\u4f20\u7edf\u68c0\u7d22\u5668\u7684\u6027\u80fd\uff0c\u65e0\u8bba\u76f4\u63a5\u4f7f\u7528\u8fd8\u662f\u5fae\u8c03\u540e\u3002", "conclusion": "Conv2Query\u6709\u6548\u89e3\u51b3\u4e86\u8f93\u5165\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3aPSC\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u68c0\u7d22\u65b9\u6848\u3002", "keywords": "\u5bf9\u8bdd\u4e3b\u52a8\u641c\u7d22\u3001\u68c0\u7d22\u5668\u9002\u5e94\u3001\u67e5\u8be2\u8f6c\u6362\u3001Conv2Query"}}
{"id": "2506.00943", "pdf": "https://arxiv.org/pdf/2506.00943", "abs": "https://arxiv.org/abs/2506.00943", "authors": ["Chanuka Wijayakoon", "Hai Dong", "H. M. N. Dilum Bandara", "Zahir Tari", "Anurag Soin"], "title": "Legal Compliance Evaluation of Smart Contracts Generated By Large Language Models", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted for publication at IEEE International Conference on\n  Blockchain and Cryptocurrency (ICBC) 2025", "summary": "Smart contracts can implement and automate parts of legal contracts, but\nensuring their legal compliance remains challenging. Existing approaches such\nas formal specification, verification, and model-based development require\nexpertise in both legal and software development domains, as well as extensive\nmanual effort. Given the recent advances of Large Language Models (LLMs) in\ncode generation, we investigate their ability to generate legally compliant\nsmart contracts directly from natural language legal contracts, addressing\nthese challenges. We propose a novel suite of metrics to quantify legal\ncompliance based on modeling both legal and smart contracts as processes and\ncomparing their behaviors. We select four LLMs, generate 20 smart contracts\nbased on five legal contracts, and analyze their legal compliance. We find that\nwhile all LLMs generate syntactically correct code, there is significant\nvariance in their legal compliance with larger models generally showing higher\nlevels of compliance. We also evaluate the proposed metrics against properties\nof software metrics, showing they provide fine-grained distinctions, enable\nnuanced comparisons, and are applicable across domains for code from any\nsource, LLM or developer. Our results suggest that LLMs can assist in\ngenerating starter code for legally compliant smart contracts with strict\nreviews, and the proposed metrics provide a foundation for automated and\nself-improving development workflows.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ece\u81ea\u7136\u8bed\u8a00\u6cd5\u5f8b\u5408\u540c\u751f\u6210\u5408\u6cd5\u5408\u89c4\u7684\u667a\u80fd\u5408\u7ea6\u7684\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u65b0\u7684\u5ea6\u91cf\u6807\u51c6\u6765\u91cf\u5316\u5408\u89c4\u6027\u3002", "motivation": "\u786e\u4fdd\u667a\u80fd\u5408\u7ea6\u7684\u6cd5\u5f8b\u5408\u89c4\u6027\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6cd5\u5f8b\u548c\u8f6f\u4ef6\u5f00\u53d1\u9886\u57df\u7684\u4e13\u4e1a\u77e5\u8bc6\u53ca\u5927\u91cf\u624b\u52a8\u5de5\u4f5c\u3002LLMs\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u8fdb\u5c55\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u5957\u57fa\u4e8e\u5efa\u6a21\u6cd5\u5f8b\u548c\u667a\u80fd\u5408\u7ea6\u884c\u4e3a\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u5e76\u6d4b\u8bd5\u4e86\u56db\u79cdLLMs\u751f\u621020\u4efd\u667a\u80fd\u5408\u7ea6\u7684\u5408\u89c4\u6027\u3002", "result": "\u6240\u6709LLMs\u751f\u6210\u7684\u4ee3\u7801\u8bed\u6cd5\u6b63\u786e\uff0c\u4f46\u6cd5\u5f8b\u5408\u89c4\u6027\u5dee\u5f02\u663e\u8457\uff0c\u8f83\u5927\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002\u6240\u63d0\u51fa\u7684\u5ea6\u91cf\u6807\u51c6\u5177\u6709\u7ec6\u7c92\u5ea6\u533a\u5206\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u8de8\u9886\u57df\u4ee3\u7801\u8bc4\u4f30\u3002", "conclusion": "LLMs\u53ef\u7528\u4e8e\u751f\u6210\u521d\u6b65\u5408\u6cd5\u7684\u667a\u80fd\u5408\u7ea6\u4ee3\u7801\uff0c\u4f46\u9700\u4e25\u683c\u5ba1\u67e5\uff1b\u63d0\u51fa\u7684\u5ea6\u91cf\u6807\u51c6\u4e3a\u81ea\u52a8\u5316\u548c\u81ea\u6539\u8fdb\u7684\u5f00\u53d1\u6d41\u7a0b\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "keywords": "\u667a\u80fd\u5408\u7ea6, \u6cd5\u5f8b\u5408\u89c4\u6027, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5ea6\u91cf\u6807\u51c6, \u4ee3\u7801\u751f\u6210"}}
{"id": "2506.00270", "pdf": "https://arxiv.org/pdf/2506.00270", "abs": "https://arxiv.org/abs/2506.00270", "authors": ["Rajarshi Guhaniyogi", "Laura Baracaldo", "Sudipto Banerjee"], "title": "Bayesian Data Sketching for Varying Coefficient Regression Models", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Varying coefficient models are popular for estimating nonlinear regression\nfunctions in functional data models. Their Bayesian variants have received\nlimited attention in large data applications, primarily due to prohibitively\nslow posterior computations using Markov chain Monte Carlo (MCMC) algorithms.\nWe introduce Bayesian data sketching for varying coefficient models to obviate\ncomputational challenges presented by large sample sizes. To address the\nchallenges of analyzing large data, we compress the functional response vector\nand predictor matrix by a random linear transformation to achieve dimension\nreduction and conduct inference on the compressed data. Our approach\ndistinguishes itself from several existing methods for analyzing large\nfunctional data in that it requires neither the development of new models or\nalgorithms, nor any specialized computational hardware while delivering fully\nmodel-based Bayesian inference. Well-established methods and algorithms for\nvarying coefficient regression models can be applied to the compressed data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u8d1d\u53f6\u65af\u6570\u636e\u8349\u56fe\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u4e2d\u7684\u53d8\u7cfb\u6570\u6a21\u578b\uff0c\u901a\u8fc7\u968f\u673a\u7ebf\u6027\u53d8\u6362\u538b\u7f29\u6570\u636e\uff0c\u89e3\u51b3\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u5728\u5927\u6570\u636e\u5e94\u7528\u4e2d\uff0c\u8d1d\u53f6\u65af\u53d8\u7cfb\u6570\u6a21\u578b\u56e0MCMC\u7b97\u6cd5\u8ba1\u7b97\u6548\u7387\u4f4e\u800c\u53d7\u9650\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u964d\u7ef4\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u968f\u673a\u7ebf\u6027\u53d8\u6362\u538b\u7f29\u529f\u80fd\u54cd\u5e94\u5411\u91cf\u548c\u9884\u6d4b\u77e9\u9635\uff0c\u5728\u538b\u7f29\u6570\u636e\u4e0a\u5e94\u7528\u73b0\u6709\u53d8\u7cfb\u6570\u56de\u5f52\u6a21\u578b\u3002", "result": "\u65e0\u9700\u65b0\u6a21\u578b\u6216\u7b97\u6cd5\uff0c\u5373\u53ef\u5b9e\u73b0\u5b8c\u5168\u57fa\u4e8e\u6a21\u578b\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u901a\u7528\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u529f\u80fd\u6570\u636e\u5206\u6790\u3002", "keywords": "\u53d8\u7cfb\u6570\u6a21\u578b, \u8d1d\u53f6\u65af\u63a8\u65ad, \u6570\u636e\u538b\u7f29, \u5927\u89c4\u6a21\u6570\u636e"}}
{"id": "2506.01055", "pdf": "https://arxiv.org/pdf/2506.01055", "abs": "https://arxiv.org/abs/2506.01055", "authors": ["Meysam Alizadeh", "Zeynab Samei", "Daria Stetsenko", "Fabrizio Gilardi"], "title": "Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution", "categories": ["cs.CR", "cs.CL", "68Txx"], "comment": "25 pages, 18 figures, NeurIPS formatting style", "summary": "Previous benchmarks on prompt injection in large language models (LLMs) have\nprimarily focused on generic tasks and attacks, offering limited insights into\nmore complex threats like data exfiltration. This paper examines how prompt\ninjection can cause tool-calling agents to leak personal data observed during\ntask execution. Using a fictitious banking agent, we develop data flow-based\nattacks and integrate them into AgentDojo, a recent benchmark for agentic\nsecurity. To enhance its scope, we also create a richer synthetic dataset of\nhuman-AI banking conversations. In 16 user tasks from AgentDojo, LLMs show a\n15-50 percentage point drop in utility under attack, with average attack\nsuccess rates (ASR) around 20 percent; some defenses reduce ASR to zero. Most\nLLMs, even when successfully tricked by the attack, avoid leaking highly\nsensitive data like passwords, likely due to safety alignments, but they remain\nvulnerable to disclosing other personal data. The likelihood of password\nleakage increases when a password is requested along with one or two additional\npersonal details. In an extended evaluation across 48 tasks, the average ASR is\naround 15 percent, with no built-in AgentDojo defense fully preventing leakage.\nTasks involving data extraction or authorization workflows, which closely\nresemble the structure of exfiltration attacks, exhibit the highest ASRs,\nhighlighting the interaction between task type, agent performance, and defense\nefficacy.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u5982\u4f55\u5bfc\u81f4\u5de5\u5177\u8c03\u7528\u4ee3\u7406\u6cc4\u9732\u4e2a\u4eba\u6570\u636e\uff0c\u901a\u8fc7\u865a\u6784\u94f6\u884c\u4ee3\u7406\u5c55\u793a\u4e86\u653b\u51fb\u6548\u679c\uff0c\u7ed3\u679c\u663e\u793a\u653b\u51fb\u6210\u529f\u7387\u5e73\u574720%\uff0c\u90e8\u5206\u9632\u5fa1\u63aa\u65bd\u80fd\u964d\u81f30%\u3002", "motivation": "\u63a2\u8ba8\u590d\u6742\u5a01\u80c1\u5982\u6570\u636e\u6cc4\u9732\u7684\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u8865\u5145\u73b0\u6709\u7814\u7a76\u5bf9\u901a\u7528\u4efb\u52a1\u7684\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528\u865a\u6784\u94f6\u884c\u4ee3\u7406\u8bbe\u8ba1\u57fa\u4e8e\u6570\u636e\u6d41\u7684\u653b\u51fb\uff0c\u6574\u5408\u81f3AgentDojo\u57fa\u51c6\u5e76\u6269\u5c55\u6570\u636e\u96c6\u3002", "result": "\u653b\u51fb\u5bfc\u81f4LLM\u6548\u7528\u4e0b\u964d15-50%\uff0c\u5e73\u5747\u653b\u51fb\u6210\u529f\u738720%\uff1b\u90e8\u5206\u9632\u5fa1\u63aa\u65bd\u6709\u6548\uff0c\u4f46\u5bc6\u7801\u6cc4\u9732\u98ce\u9669\u589e\u52a0\u3002", "conclusion": "LLM\u5bf9\u654f\u611f\u6570\u636e\u6cc4\u9732\u5177\u6709\u62b5\u6297\u529b\uff0c\u4f46\u4ecd\u6613\u6cc4\u9732\u5176\u4ed6\u4e2a\u4eba\u6570\u636e\uff1b\u4efb\u52a1\u7c7b\u578b\u4e0e\u9632\u5fa1\u6548\u679c\u5bc6\u5207\u76f8\u5173\u3002", "keywords": "\u63d0\u793a\u6ce8\u5165, \u6570\u636e\u6cc4\u9732, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u4ee3\u7406\u5b89\u5168, \u9632\u5fa1\u63aa\u65bd"}}
{"id": "2506.00273", "pdf": "https://arxiv.org/pdf/2506.00273", "abs": "https://arxiv.org/abs/2506.00273", "authors": ["Tuochao Chen", "D Shin", "Hakan Erdogan", "Sinan Hersek"], "title": "SoundSculpt: Direction and Semantics Driven Ambisonic Target Sound Extraction", "categories": ["eess.AS", "cs.LG", "cs.SD"], "comment": null, "summary": "This paper introduces SoundSculpt, a neural network designed to extract\ntarget sound fields from ambisonic recordings. SoundSculpt employs an\nambisonic-in-ambisonic-out architecture and is conditioned on both spatial\ninformation (e.g., target direction obtained by pointing at an immersive video)\nand semantic embeddings (e.g., derived from image segmentation and captioning).\nTrained and evaluated on synthetic and real ambisonic mixtures, SoundSculpt\ndemonstrates superior performance compared to various signal processing\nbaselines. Our results further reveal that while spatial conditioning alone can\nbe effective, the combination of spatial and semantic information is beneficial\nin scenarios where there are secondary sound sources spatially close to the\ntarget. Additionally, we compare two different semantic embeddings derived from\na text description of the target sound using text encoders.", "AI": {"tldr": "SoundSculpt\u662f\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u4eceAmbisonic\u5f55\u97f3\u4e2d\u63d0\u53d6\u76ee\u6807\u58f0\u573a\uff0c\u7ed3\u5408\u7a7a\u95f4\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63d0\u53d6\u76ee\u6807\u58f0\u573a\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u76ee\u6807\u58f0\u6e90\u4e0e\u6b21\u8981\u58f0\u6e90\u7a7a\u95f4\u63a5\u8fd1\u65f6\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528Ambisonic-in-Ambisonic-out\u67b6\u6784\uff0c\u7ed3\u5408\u7a7a\u95f4\u4fe1\u606f\uff08\u5982\u76ee\u6807\u65b9\u5411\uff09\u548c\u8bed\u4e49\u5d4c\u5165\uff08\u5982\u56fe\u50cf\u5206\u5272\u548c\u5b57\u5e55\u751f\u6210\uff09\u3002", "result": "SoundSculpt\u5728\u5408\u6210\u548c\u771f\u5b9eAmbisonic\u6df7\u5408\u7269\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u7a7a\u95f4\u4e0e\u8bed\u4e49\u4fe1\u606f\u7684\u7ed3\u5408\u5728\u590d\u6742\u573a\u666f\u4e2d\u5c24\u4e3a\u6709\u6548\u3002", "conclusion": "\u7a7a\u95f4\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u58f0\u573a\u63d0\u53d6\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u76ee\u6807\u4e0e\u6b21\u8981\u58f0\u6e90\u63a5\u8fd1\u65f6\u3002", "keywords": "SoundSculpt, Ambisonic, \u795e\u7ecf\u7f51\u7edc, \u8bed\u4e49\u5d4c\u5165, \u76ee\u6807\u58f0\u573a"}}
{"id": "2506.00979", "pdf": "https://arxiv.org/pdf/2506.00979", "abs": "https://arxiv.org/abs/2506.00979", "authors": ["Wayne Zhang", "Changjiang Jiang", "Zhonghao Zhang", "Chenyang Si", "Fengchang Yu", "Wei Peng"], "title": "IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection", "categories": ["cs.CV", "cs.AI"], "comment": "20pages,13figures,7 tables", "summary": "The rapid advancement of Artificial Intelligence Generated Content (AIGC) in\nvisual domains has resulted in highly realistic synthetic images and videos,\ndriven by sophisticated generative frameworks such as diffusion-based\narchitectures. While these breakthroughs open substantial opportunities, they\nsimultaneously raise critical concerns about content authenticity and\nintegrity. Many current AIGC detection methods operate as black-box binary\nclassifiers, which offer limited interpretability, and no approach supports\ndetecting both images and videos in a unified framework. This dual limitation\ncompromises model transparency, reduces trustworthiness, and hinders practical\ndeployment. To address these challenges, we introduce IVY-FAKE , a novel,\nunified, and large-scale dataset specifically designed for explainable\nmultimodal AIGC detection. Unlike prior benchmarks, which suffer from\nfragmented modality coverage and sparse annotations, IVY-FAKE contains over\n150,000 richly annotated training samples (images and videos) and 18,700\nevaluation examples, each accompanied by detailed natural-language reasoning\nbeyond simple binary labels. Building on this, we propose Ivy Explainable\nDetector (IVY-XDETECTOR), a unified AIGC detection and explainable architecture\nthat jointly performs explainable detection for both image and video content.\nOur unified vision-language model achieves state-of-the-art performance across\nmultiple image and video detection benchmarks, highlighting the significant\nadvancements enabled by our dataset and modeling framework. Our data is\npublicly available at https://huggingface.co/datasets/AI-Safeguard/Ivy-Fake.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7edf\u4e00\u3001\u5927\u89c4\u6a21\u6570\u636e\u96c6IVY-FAKE\u548c\u53ef\u89e3\u91ca\u7684AIGC\u68c0\u6d4b\u6846\u67b6IVY-XDETECTOR\uff0c\u7528\u4e8e\u89e3\u51b3\u5f53\u524d\u65b9\u6cd5\u7f3a\u4e4f\u89e3\u91ca\u6027\u548c\u591a\u6a21\u6001\u7edf\u4e00\u68c0\u6d4b\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dAIGC\u68c0\u6d4b\u65b9\u6cd5\u591a\u4e3a\u9ed1\u76d2\u4e8c\u5143\u5206\u7c7b\u5668\uff0c\u7f3a\u4e4f\u89e3\u91ca\u6027\u4e14\u4e0d\u652f\u6301\u591a\u6a21\u6001\u7edf\u4e00\u68c0\u6d4b\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u5305\u542b\u4e30\u5bcc\u6ce8\u91ca\u7684IVY-FAKE\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684IVY-XDETECTOR\uff0c\u5b9e\u73b0\u56fe\u50cf\u548c\u89c6\u9891\u7684\u7edf\u4e00\u68c0\u6d4b\u4e0e\u89e3\u91ca\u3002", "result": "IVY-XDETECTOR\u5728\u591a\u4e2a\u68c0\u6d4b\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6570\u636e\u96c6\u548c\u5efa\u6a21\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7IVY-FAKE\u548cIVY-XDETECTOR\uff0c\u8bba\u6587\u4e3aAIGC\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u7edf\u4e00\u7684\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "keywords": "AIGC\u68c0\u6d4b,\u591a\u6a21\u6001,\u53ef\u89e3\u91ca\u6027,\u7edf\u4e00\u6846\u67b6,\u6570\u636e\u96c6"}}
{"id": "2506.00280", "pdf": "https://arxiv.org/pdf/2506.00280", "abs": "https://arxiv.org/abs/2506.00280", "authors": ["Matthew Hull", "Haoyang Yang", "Pratham Mehta", "Mansi Phute", "Aeree Cho", "Haoran Wang", "Matthew Lau", "Wenke Lee", "Willian T. Lunardi", "Martin Andreoni", "Polo Chau"], "title": "3D Gaussian Splat Vulnerabilities", "categories": ["cs.CR", "cs.CV", "cs.LG"], "comment": "4 pages, 4 figures, CVPR '25 Workshop on Neural Fields Beyond\n  Conventional Cameras", "summary": "With 3D Gaussian Splatting (3DGS) being increasingly used in safety-critical\napplications, how can an adversary manipulate the scene to cause harm? We\nintroduce CLOAK, the first attack that leverages view-dependent Gaussian\nappearances - colors and textures that change with viewing angle - to embed\nadversarial content visible only from specific viewpoints. We further\ndemonstrate DAGGER, a targeted adversarial attack directly perturbing 3D\nGaussians without access to underlying training data, deceiving multi-stage\nobject detectors e.g., Faster R-CNN, through established methods such as\nprojected gradient descent. These attacks highlight underexplored\nvulnerabilities in 3DGS, introducing a new potential threat to robotic learning\nfor autonomous navigation and other safety-critical 3DGS applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCLOAK\u548cDAGGER\u4e24\u79cd\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u75283DGS\uff083D\u9ad8\u65af\u6e85\u5c04\uff09\u7684\u89c6\u89d2\u4f9d\u8d56\u6027\u7279\u6027\uff0c\u901a\u8fc7\u7279\u5b9a\u89c6\u89d2\u5d4c\u5165\u5bf9\u6297\u5185\u5bb9\u6216\u76f4\u63a5\u6270\u52a83D\u9ad8\u65af\u6765\u6b3a\u9a97\u76ee\u6807\u68c0\u6d4b\u5668\u3002", "motivation": "\u968f\u77403DGS\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u7814\u7a76\u5176\u6f5c\u5728\u7684\u5bf9\u6297\u6027\u653b\u51fb\u6f0f\u6d1e\u81f3\u5173\u91cd\u8981\u3002", "method": "CLOAK\u5229\u7528\u89c6\u89d2\u4f9d\u8d56\u7684\u9ad8\u65af\u5916\u89c2\u5d4c\u5165\u5bf9\u6297\u5185\u5bb9\uff1bDAGGER\u5219\u76f4\u63a5\u6270\u52a83D\u9ad8\u65af\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u6b3a\u9a97\u76ee\u6807\u68c0\u6d4b\u5668\u3002", "result": "\u8fd9\u4e9b\u653b\u51fb\u6210\u529f\u63ed\u793a\u4e863DGS\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u6f5c\u5728\u5a01\u80c1\u3002", "conclusion": "3DGS\u5b58\u5728\u672a\u5145\u5206\u63a2\u7d22\u7684\u6f0f\u6d1e\uff0c\u53ef\u80fd\u5bf9\u673a\u5668\u4eba\u5b66\u4e60\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u6784\u6210\u65b0\u5a01\u80c1\u3002", "keywords": "3D\u9ad8\u65af\u6e85\u5c04, \u5bf9\u6297\u653b\u51fb, \u5b89\u5168\u5173\u952e\u5e94\u7528, Faster R-CNN"}}
{"id": "2506.01256", "pdf": "https://arxiv.org/pdf/2506.01256", "abs": "https://arxiv.org/abs/2506.01256", "authors": ["Matthew C. Kelley"], "title": "Confidence intervals for forced alignment boundaries using model ensembles", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "submitted for publication; 7 pages, 1 figure", "summary": "Forced alignment is a common tool to align audio with orthographic and\nphonetic transcriptions. Most forced alignment tools provide only a single\nestimate of a boundary. The present project introduces a method of deriving\nconfidence intervals for these boundaries using a neural network ensemble\ntechnique. Ten different segment classifier neural networks were previously\ntrained, and the alignment process is repeated with each model. The alignment\nensemble is then used to place the boundary at the median of the boundaries in\nthe ensemble, and 97.85% confidence intervals are constructed using order\nstatistics. On the Buckeye and TIMIT corpora, the ensemble boundaries show a\nslight improvement over using just a single model. The confidence intervals are\nincorporated into Praat TextGrids using a point tier, and they are also output\nas a table for researchers to analyze separately as diagnostics or to\nincorporate uncertainty into their analyses.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u96c6\u6210\u6280\u672f\u4e3a\u5f3a\u5236\u5bf9\u9f50\u8fb9\u754c\u751f\u6210\u7f6e\u4fe1\u533a\u95f4\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5236\u5bf9\u9f50\u5de5\u5177\u4ec5\u63d0\u4f9b\u5355\u4e00\u8fb9\u754c\u4f30\u8ba1\uff0c\u7f3a\u4e4f\u5bf9\u8fb9\u754c\u4e0d\u786e\u5b9a\u6027\u7684\u91cf\u5316\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u5341\u4e2a\u4e0d\u540c\u7684\u5206\u6bb5\u5206\u7c7b\u795e\u7ecf\u7f51\u7edc\uff0c\u91cd\u590d\u5bf9\u9f50\u8fc7\u7a0b\uff0c\u5e76\u5229\u7528\u96c6\u6210\u6280\u672f\u751f\u6210\u7f6e\u4fe1\u533a\u95f4\u3002", "result": "\u5728Buckeye\u548cTIMIT\u8bed\u6599\u5e93\u4e0a\uff0c\u96c6\u6210\u8fb9\u754c\u6bd4\u5355\u4e00\u6a21\u578b\u7565\u6709\u6539\u8fdb\uff0c\u7f6e\u4fe1\u533a\u95f4\u6210\u529f\u6574\u5408\u5230Praat TextGrids\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u91cf\u5316\u4e86\u5f3a\u5236\u5bf9\u9f50\u8fb9\u754c\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "keywords": "forced alignment, neural network ensemble, confidence intervals, phonetic transcription"}}
{"id": "2506.01293", "pdf": "https://arxiv.org/pdf/2506.01293", "abs": "https://arxiv.org/abs/2506.01293", "authors": ["Yichi Zhang", "Zhuo Chen", "Lingbing Guo", "Yajing Xu", "Min Zhang", "Wen Zhang", "Huajun Chen"], "title": "Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New Perspective for MLLM Evaluation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Multi-modal large language models (MLLMs) incorporate heterogeneous\nmodalities into LLMs, enabling a comprehensive understanding of diverse\nscenarios and objects. Despite the proliferation of evaluation benchmarks and\nleaderboards for MLLMs, they predominantly overlook the critical capacity of\nMLLMs to comprehend world knowledge with structured abstractions that appear in\nvisual form. To address this gap, we propose a novel evaluation paradigm and\ndevise M3STR, an innovative benchmark grounded in the Multi-Modal Map for\nSTRuctured understanding. This benchmark leverages multi-modal knowledge graphs\nto synthesize images encapsulating subgraph architectures enriched with\nmulti-modal entities. M3STR necessitates that MLLMs not only recognize the\nmulti-modal entities within the visual inputs but also decipher intricate\nrelational topologies among them. We delineate the benchmark's statistical\nprofiles and automated construction pipeline, accompanied by an extensive\nempirical analysis of 26 state-of-the-art MLLMs. Our findings reveal persistent\ndeficiencies in processing abstractive visual information with structured\nknowledge, thereby charting a pivotal trajectory for advancing MLLMs' holistic\nreasoning capacities. Our code and data are released at\nhttps://github.com/zjukg/M3STR", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u8bc4\u4f30\u8303\u5f0fM3STR\uff0c\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u6d4b\u8bd5MLLMs\u5bf9\u7ed3\u6784\u5316\u77e5\u8bc6\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684MLLMs\u8bc4\u4f30\u57fa\u51c6\u5ffd\u89c6\u4e86\u5b83\u4eec\u5728\u7406\u89e3\u89c6\u89c9\u5f62\u5f0f\u7684\u7ed3\u6784\u5316\u62bd\u8c61\u77e5\u8bc6\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u591a\u6a21\u6001\u5730\u56fe\u7684M3STR\u57fa\u51c6\uff0c\u5229\u7528\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u5305\u542b\u591a\u6a21\u6001\u5b9e\u4f53\u7684\u5b50\u56fe\u67b6\u6784\u56fe\u50cf\u3002", "result": "\u5bf926\u4e2a\u5148\u8fdbMLLMs\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5b83\u4eec\u5728\u5904\u7406\u7ed3\u6784\u5316\u77e5\u8bc6\u7684\u62bd\u8c61\u89c6\u89c9\u4fe1\u606f\u65b9\u9762\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002", "conclusion": "M3STR\u4e3a\u63d0\u5347MLLMs\u7684\u6574\u4f53\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u5173\u952e\u65b9\u5411\u3002", "keywords": "MLLMs, \u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31, \u7ed3\u6784\u5316\u7406\u89e3, \u8bc4\u4f30\u57fa\u51c6"}}
{"id": "2506.00305", "pdf": "https://arxiv.org/pdf/2506.00305", "abs": "https://arxiv.org/abs/2506.00305", "authors": ["Antonello Paolino", "Gabriele Nava", "Fabio Di Natale", "Fabio Bergonti", "Punith Reddy Vanteddu", "Donato Grassi", "Luca Riccobene", "Alex Zanotti", "Renato Tognaccini", "Gianluca Iaccarino", "Daniele Pucci"], "title": "Learning Aerodynamics for the Control of Flying Humanoid Robots", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Robots with multi-modal locomotion are an active research field due to their\nversatility in diverse environments. In this context, additional actuation can\nprovide humanoid robots with aerial capabilities. Flying humanoid robots face\nchallenges in modeling and control, particularly with aerodynamic forces. This\npaper addresses these challenges from a technological and scientific\nstandpoint. The technological contribution includes the mechanical design of\niRonCub-Mk1, a jet-powered humanoid robot, optimized for jet engine\nintegration, and hardware modifications for wind tunnel experiments on humanoid\nrobots for precise aerodynamic forces and surface pressure measurements. The\nscientific contribution offers a comprehensive approach to model and control\naerodynamic forces using classical and learning techniques. Computational Fluid\nDynamics (CFD) simulations calculate aerodynamic forces, validated through wind\ntunnel experiments on iRonCub-Mk1. An automated CFD framework expands the\naerodynamic dataset, enabling the training of a Deep Neural Network and a\nlinear regression model. These models are integrated into a simulator for\ndesigning aerodynamic-aware controllers, validated through flight simulations\nand balancing experiments on the iRonCub-Mk1 physical prototype.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u55b7\u6c14\u52a8\u529b\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u548c\u63a7\u5236\uff0c\u901a\u8fc7CFD\u6a21\u62df\u548c\u98ce\u6d1e\u5b9e\u9a8c\u9a8c\u8bc1\u6c14\u52a8\u529b\u6a21\u578b\uff0c\u5e76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u4f18\u5316\u63a7\u5236\u7cfb\u7edf\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u6001\u8fd0\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u5c24\u5176\u662f\u4eba\u5f62\u673a\u5668\u4eba\u5728\u7a7a\u4e2d\u8fd0\u52a8\u65f6\u7684\u6c14\u52a8\u529b\u5efa\u6a21\u4e0e\u63a7\u5236\u6311\u6218\u3002", "method": "1. \u8bbe\u8ba1\u55b7\u6c14\u52a8\u529b\u4eba\u5f62\u673a\u5668\u4ebaiRonCub-Mk1\uff0c\u4f18\u5316\u55b7\u6c14\u53d1\u52a8\u673a\u96c6\u6210\uff1b2. \u901a\u8fc7CFD\u6a21\u62df\u548c\u98ce\u6d1e\u5b9e\u9a8c\u9a8c\u8bc1\u6c14\u52a8\u529b\u6a21\u578b\uff1b3. \u5f00\u53d1\u81ea\u52a8CFD\u6846\u67b6\u751f\u6210\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff1b4. \u8bbe\u8ba1\u6c14\u52a8\u529b\u611f\u77e5\u63a7\u5236\u5668\u5e76\u5728\u6a21\u62df\u548c\u7269\u7406\u539f\u578b\u4e2d\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u9a8c\u8bc1\u4e86\u6c14\u52a8\u529b\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u63a7\u5236\u5668\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u7ed3\u5408\u4f20\u7edf\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\u7684\u6c14\u52a8\u529b\u5efa\u6a21\u4e0e\u63a7\u5236\u65b9\u6cd5\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u7684\u7a7a\u4e2d\u8fd0\u52a8\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u4eba\u5f62\u673a\u5668\u4eba,\u6c14\u52a8\u529b,CFD,\u6df1\u5ea6\u5b66\u4e60,\u63a7\u5236"}}
{"id": "2506.00992", "pdf": "https://arxiv.org/pdf/2506.00992", "abs": "https://arxiv.org/abs/2506.00992", "authors": ["Peng Hui", "Jiamuyang Zhao", "Changxin Li", "Qingzhen Zhu"], "title": "Quotient Network -- A Network Similar to ResNet but Learning Quotients", "categories": ["cs.CV", "cs.AI"], "comment": "This manuscript is the original version submitted to NeurIPS 2024,\n  which was later revised and published as \"Quotient Network: A Network Similar\n  to ResNet but Learning Quotients\" in Algorithms 2024, 17(11), 521\n  (https://doi.org/10.3390/a17110521). Please cite the journal version when\n  referring to this work", "summary": "The emergence of ResNet provides a powerful tool for training extremely deep\nnetworks. The core idea behind it is to change the learning goals of the\nnetwork. It no longer learns new features from scratch but learns the\ndifference between the target and existing features. However, the difference\nbetween the two kinds of features does not have an independent and clear\nmeaning, and the amount of learning is based on the absolute rather than the\nrelative difference, which is sensitive to the size of existing features. We\npropose a new network that perfectly solves these two problems while still\nhaving the advantages of ResNet. Specifically, it chooses to learn the quotient\nof the target features with the existing features, so we call it the quotient\nnetwork. In order to enable this network to learn successfully and achieve\nhigher performance, we propose some design rules for this network so that it\ncan be trained efficiently and achieve better performance than ResNet.\nExperiments on the CIFAR10, CIFAR100, and SVHN datasets prove that this network\ncan stably achieve considerable improvements over ResNet by simply making tiny\ncorresponding changes to the original ResNet network without adding new\nparameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b8b\u5dee\u7f51\u7edc\u6539\u8fdb\u7684\u5546\u7f51\u7edc\uff0c\u901a\u8fc7\u5b66\u4e60\u76ee\u6807\u7279\u5f81\u4e0e\u73b0\u6709\u7279\u5f81\u7684\u5546\u89e3\u51b3\u4e86\u6b8b\u5dee\u7f51\u7edc\u5bf9\u7279\u5f81\u5927\u5c0f\u654f\u611f\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8eResNet\u3002", "motivation": "\u6b8b\u5dee\u7f51\u7edc\uff08ResNet\uff09\u901a\u8fc7\u5b66**\u4e60\u7279\u5f81\u7684\u5dee\u5f02\u6765\u8bad\u7ec3\u6df1\u5ea6\u7f51\u7edc\uff0c\u4f46\u5dee\u5f02\u7684\u7edd\u5bf9\u6027\u4f7f\u5176\u5bf9\u7279\u5f81\u5927\u5c0f\u654f\u611f\uff0c\u7f3a\u4e4f\u72ec\u7acb\u610f\u4e49\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5546\u7f51\u7edc\uff08Quotient Network\uff09\uff0c\u901a\u8fc7\u5b66**\u4e60\u76ee\u6807\u7279\u5f81\u4e0e\u73b0\u6709\u7279\u5f81\u7684\u5546\uff0c\u5e76\u7ed3\u5408\u8bbe\u8ba1\u89c4\u5219\u4ee5\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u548c\u66f4\u9ad8\u6027\u80fd\u3002", "result": "\u5728CIFAR10\u3001CIFAR100\u548cSVHN\u6570\u636e\u96c6\u4e0a\uff0c\u5546\u7f51\u7edc\u901a\u8fc7\u5fae\u5c0f\u8c03\u6574\u5373\u53ef\u7a33\u5b9a\u8d85\u8d8aResNet\u6027\u80fd\uff0c\u4e14\u672a\u589e\u52a0\u989d\u5916\u53c2\u6570\u3002", "conclusion": "\u5546\u7f51\u7edc\u6210\u529f\u89e3\u51b3\u4e86ResNet\u7684\u95ee\u9898\uff0c\u4fdd\u7559\u4e86\u5176\u4f18\u52bf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u63d0\u5347\u7684\u6709\u6548\u6027\u3002", "keywords": "ResNet, \u5546\u7f51\u7edc, \u6df1\u5ea6\u5b66**\u4e60, CIFAR10, CIFAR100, SVHN"}}
{"id": "2506.01004", "pdf": "https://arxiv.org/pdf/2506.01004", "abs": "https://arxiv.org/abs/2506.01004", "authors": ["Tong Zhang", "Juan C Leon Alcazar", "Bernard Ghanem"], "title": "Motion-Aware Concept Alignment for Consistent Video Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce MoCA-Video (Motion-Aware Concept Alignment in Video), a\ntraining-free framework bridging the gap between image-domain semantic mixing\nand video. Given a generated video and a user-provided reference image,\nMoCA-Video injects the semantic features of the reference image into a specific\nobject within the video, while preserving the original motion and visual\ncontext. Our approach leverages a diagonal denoising schedule and\nclass-agnostic segmentation to detect and track objects in the latent space and\nprecisely control the spatial location of the blended objects. To ensure\ntemporal coherence, we incorporate momentum-based semantic corrections and\ngamma residual noise stabilization for smooth frame transitions. We evaluate\nMoCA's performance using the standard SSIM, image-level LPIPS, temporal LPIPS,\nand introduce a novel metric CASS (Conceptual Alignment Shift Score) to\nevaluate the consistency and effectiveness of the visual shifts between the\nsource prompt and the modified video frames. Using self-constructed dataset,\nMoCA-Video outperforms current baselines, achieving superior spatial\nconsistency, coherent motion, and a significantly higher CASS score, despite\nhaving no training or fine-tuning. MoCA-Video demonstrates that structured\nmanipulation in the diffusion noise trajectory allows for controllable,\nhigh-quality video synthesis.", "AI": {"tldr": "MoCA-Video\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u50cf\u57df\u8bed\u4e49\u6df7\u5408\u4e0e\u89c6\u9891\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u89c6\u9891\u4e2d\u7279\u5b9a\u5bf9\u8c61\u7684\u8bed\u4e49\u7279\u5f81\u6ce8\u5165\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u8fd0\u52a8\u548c\u89c6\u89c9\u4e0a\u4e0b\u6587\u3002", "motivation": "\u5f25\u5408\u56fe\u50cf\u57df\u8bed\u4e49\u6df7\u5408\u4e0e\u89c6\u9891\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u5728\u89c6\u9891\u4e2d\u5bf9\u7279\u5b9a\u5bf9\u8c61\u7684\u8bed\u4e49\u7279\u5f81\u6ce8\u5165\u3002", "method": "\u5229\u7528\u5bf9\u89d2\u53bb\u566a\u8c03\u5ea6\u548c\u7c7b\u65e0\u5173\u5206\u5272\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u68c0\u6d4b\u548c\u8ddf\u8e2a\u5bf9\u8c61\uff0c\u7ed3\u5408\u57fa\u4e8e\u52a8\u91cf\u7684\u8bed\u4e49\u6821\u6b63\u548c\u4f3d\u9a6c\u6b8b\u5dee\u566a\u58f0\u7a33\u5b9a\u5316\u786e\u4fdd\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "result": "\u5728SSIM\u3001LPIPS\u548cCASS\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "conclusion": "MoCA-Video\u901a\u8fc7\u7ed3\u6784\u5316\u64cd\u4f5c\u6269\u6563\u566a\u58f0\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a7\u4e14\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u5408\u6210\u3002", "keywords": "MoCA-Video, motion-aware, concept alignment, video synthesis, denoising schedule, temporal coherence"}}
{"id": "2506.00315", "pdf": "https://arxiv.org/pdf/2506.00315", "abs": "https://arxiv.org/abs/2506.00315", "authors": ["Mahmoud Elgenedy"], "title": "Power-of-Two (PoT) Weights in Large Language Models (LLMs)", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Complexity of Neural Networks is increasing rapidly due to the massive\nincrease in model parameters. Specifically, in Large Language Models (LLMs),\nthe number of model parameters has grown exponentially in the past few years,\nfor example, from 1.5 billion parameters in GPT2 to 175 billion in GPT3. This\nraises a significant challenge for implementation, especially for Edge devices\nwhere memory and processing power are very limited. In this work, we\ninvestigate reducing LLM complexity with special type of quantization, power of\ntwo (PoT), for linear layers weights and transformer tables. PoT not only\nprovides memory reduction but more importantly provides significant\ncomputational reduction through converting multiplication to bit shifting. We\nobtained preliminary results of PoT quantization on Nano-GPT implementation\nusing Shakespeare dataset. We then extended results to 124-M GPT-2 model. The\nPoT quantization results are shown to be very promising with cross entropy loss\ndegradation $\\approx$[1.3-0.88] with number of bits range [4-6] to represent\npower levels.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u5e42\u6b21\u4e8c\u91cf\u5316\uff08PoT\uff09\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u590d\u6742\u6027\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u968f\u7740LLM\u53c2\u6570\u6570\u91cf\u6fc0\u589e\uff0c\u8fb9\u7f18\u8bbe\u5907\u9762\u4e34\u5185\u5b58\u548c\u8ba1\u7b97\u80fd\u529b\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u9700\u5bfb\u627e\u9ad8\u6548\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528PoT\u91cf\u5316\u7ebf\u6027\u5c42\u6743\u91cd\u548c\u8f6c\u6362\u8868\uff0c\u5c06\u4e58\u6cd5\u8f6c\u6362\u4e3a\u4f4d\u79fb\u64cd\u4f5c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u3002", "result": "\u5728Nano-GPT\u548c124M\u53c2\u6570GPT-2\u6a21\u578b\u4e0a\u521d\u6b65\u9a8c\u8bc1\uff0c4-6\u6bd4\u7279\u8868\u793a\u65f6\u4ea4\u53c9\u71b5\u635f\u5931\u4ec5\u589e\u52a0[1.3-0.88]\u3002", "conclusion": "PoT\u91cf\u5316\u65b9\u6cd5\u5728\u6a21\u578b\u538b\u7f29\u548c\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u91cf\u5316, \u5e42\u6b21\u4e8c\u91cf\u5316, \u8fb9\u7f18\u8ba1\u7b97, \u6a21\u578b\u538b\u7f29"}}
{"id": "2506.01365", "pdf": "https://arxiv.org/pdf/2506.01365", "abs": "https://arxiv.org/abs/2506.01365", "authors": ["Kumud Tripathi", "Chowdam Venkata Kumar", "Pankaj Wasnik"], "title": "Attention Is Not Always the Answer: Optimizing Voice Activity Detection with Simple Feature Fusion", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025, 5 pages, 4 figures, 2 tables", "summary": "Voice Activity Detection (VAD) plays a key role in speech processing, often\nutilizing hand-crafted or neural features. This study examines the\neffectiveness of Mel-Frequency Cepstral Coefficients (MFCCs) and pre-trained\nmodel (PTM) features, including wav2vec 2.0, HuBERT, WavLM, UniSpeech, MMS, and\nWhisper. We propose FusionVAD, a unified framework that combines both feature\ntypes using three fusion strategies: concatenation, addition, and\ncross-attention (CA). Experimental results reveal that simple fusion\ntechniques, particularly addition, outperform CA in both accuracy and\nefficiency. Fusion-based models consistently surpass single-feature models,\nhighlighting the complementary nature of MFCCs and PTM features. Notably, our\nbest-performing fusion model exceeds the state-of-the-art Pyannote across\nmultiple datasets, achieving an absolute average improvement of 2.04%. These\nresults confirm that simple feature fusion enhances VAD robustness while\nmaintaining computational efficiency.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86FusionVAD\u6846\u67b6\uff0c\u7ed3\u5408MFCC\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u7279\u5f81\uff0c\u901a\u8fc7\u4e09\u79cd\u878d\u5408\u7b56\u7565\u63d0\u5347\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b\uff08VAD\uff09\u6027\u80fd\uff0c\u53d1\u73b0\u7b80\u5355\u878d\u5408\u65b9\u6cd5\u6548\u679c\u4f18\u4e8e\u590d\u6742\u65b9\u6cd5\uff0c\u6a21\u578b\u8868\u73b0\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u7ed3\u679c\u3002", "motivation": "\u63d0\u5347\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u63a2\u7d22MFCC\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u7279\u5f81\u7684\u4e92\u8865\u6027\u3002", "method": "\u63d0\u51faFusionVAD\u6846\u67b6\uff0c\u91c7\u7528\u62fc\u63a5\u3001\u52a0\u6cd5\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u4e09\u79cd\u878d\u5408\u7b56\u7565\u7ed3\u5408MFCC\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u7279\u5f81\u3002", "result": "\u52a0\u6cd5\u878d\u5408\u65b9\u6cd5\u6548\u679c\u6700\u4f18\uff0c\u878d\u5408\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u6a21\u578bPyannote\uff0c\u5e73\u5747\u63d0\u53472.04%\u3002", "conclusion": "\u7b80\u5355\u7279\u5f81\u878d\u5408\u80fd\u663e\u8457\u589e\u5f3aVAD\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "keywords": "\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b, MFCC, \u9884\u8bad\u7ec3\u6a21\u578b, \u7279\u5f81\u878d\u5408, FusionVAD"}}
{"id": "2506.01023", "pdf": "https://arxiv.org/pdf/2506.01023", "abs": "https://arxiv.org/abs/2506.01023", "authors": ["Shenghui Lu", "Hukai Huang", "Jinanglong Yao", "Kaidi Wang", "Qingyang Hong", "Lin Li"], "title": "A Two-Stage Hierarchical Deep Filtering Framework for Real-Time Speech Enhancement", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "5 pages, 2 figure, accepted by Interspeech 2025", "summary": "This paper proposes a model that integrates sub-band processing and deep\nfiltering to fully exploit information from the target time-frequency (TF) bin\nand its surrounding TF bins for single-channel speech enhancement. The sub-band\nmodule captures surrounding frequency bin information at the input, while the\ndeep filtering module applies filtering at the output to both the target TF bin\nand its surrounding TF bins. To further improve the model performance, we\ndecouple deep filtering into temporal and frequency components and introduce a\ntwo-stage framework, reducing the complexity of filter coefficient prediction\nat each stage. Additionally, we propose the TAConv module to strengthen\nconvolutional feature extraction. Experimental results demonstrate that the\nproposed hierarchical deep filtering network (HDF-Net) effectively utilizes\nsurrounding TF bin information and outperforms other advanced systems while\nusing fewer resources.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b50\u5e26\u5904\u7406\u548c\u6df1\u5ea6\u6ee4\u6ce2\u7684\u6a21\u578b\uff08HDF-Net\uff09\uff0c\u901a\u8fc7\u5145\u5206\u5229\u7528\u76ee\u6807\u65f6\u9891\uff08TF\uff09\u70b9\u53ca\u5176\u5468\u56f4TF\u70b9\u7684\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u5355\u901a\u9053\u8bed\u97f3\u589e\u5f3a\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u901a\u9053\u8bed\u97f3\u589e\u5f3a\u65b9\u6cd5\u53ef\u80fd\u672a\u80fd\u5145\u5206\u5229\u7528\u76ee\u6807\u65f6\u9891\u70b9\u5468\u56f4\u7684\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u53d6\u548c\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6df1\u5ea6\u6ee4\u6ce2\u7f51\u7edc\uff08HDF-Net\uff09\uff0c\u5305\u62ec\u5b50\u5e26\u6a21\u5757\u548c\u6df1\u5ea6\u6ee4\u6ce2\u6a21\u5757\u3002\u5b50\u5e26\u6a21\u5757\u5728\u8f93\u5165\u9636\u6bb5\u6355\u83b7\u5468\u56f4\u9891\u70b9\u4fe1\u606f\uff0c\u6df1\u5ea6\u6ee4\u6ce2\u6a21\u5757\u5728\u8f93\u51fa\u9636\u6bb5\u5bf9\u76ee\u6807TF\u70b9\u53ca\u5176\u5468\u56f4TF\u70b9\u8fdb\u884c\u6ee4\u6ce2\u3002\u6b64\u5916\uff0c\u5c06\u6df1\u5ea6\u6ee4\u6ce2\u89e3\u8026\u4e3a\u65f6\u95f4\u548c\u9891\u7387\u5206\u91cf\uff0c\u5e76\u5f15\u5165\u4e24\u9636\u6bb5\u6846\u67b6\u4ee5\u964d\u4f4e\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u63d0\u51faTAConv\u6a21\u5757\u589e\u5f3a\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHDF-Net\u80fd\u591f\u6709\u6548\u5229\u7528\u5468\u56f4TF\u70b9\u4fe1\u606f\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u7cfb\u7edf\uff0c\u540c\u65f6\u8d44\u6e90\u6d88\u8017\u66f4\u5c11\u3002", "conclusion": "HDF-Net\u901a\u8fc7\u7ed3\u5408\u5b50\u5e26\u5904\u7406\u548c\u6df1\u5ea6\u6ee4\u6ce2\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5355\u901a\u9053\u8bed\u97f3\u589e\u5f3a\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "keywords": "\u8bed\u97f3\u589e\u5f3a,\u65f6\u9891\u5206\u6790,\u6df1\u5ea6\u6ee4\u6ce2,\u5b50\u5e26\u5904\u7406,TAConv"}}
{"id": "2506.00343", "pdf": "https://arxiv.org/pdf/2506.00343", "abs": "https://arxiv.org/abs/2506.00343", "authors": ["Mustafa Chasmai", "Alexander Shepard", "Subhransu Maji", "Grant Van Horn"], "title": "The iNaturalist Sounds Dataset", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": null, "summary": "We present the iNaturalist Sounds Dataset (iNatSounds), a collection of\n230,000 audio files capturing sounds from over 5,500 species, contributed by\nmore than 27,000 recordists worldwide. The dataset encompasses sounds from\nbirds, mammals, insects, reptiles, and amphibians, with audio and species\nlabels derived from observations submitted to iNaturalist, a global citizen\nscience platform. Each recording in the dataset varies in length and includes a\nsingle species annotation. We benchmark multiple backbone architectures,\ncomparing multiclass classification objectives with multilabel objectives.\nDespite weak labeling, we demonstrate that iNatSounds serves as a useful\npretraining resource by benchmarking it on strongly labeled downstream\nevaluation datasets. The dataset is available as a single, freely accessible\narchive, promoting accessibility and research in this important domain. We\nenvision models trained on this data powering next-generation public engagement\napplications, and assisting biologists, ecologists, and land use managers in\nprocessing large audio collections, thereby contributing to the understanding\nof species compositions in diverse soundscapes.", "AI": {"tldr": "iNatSounds\u662f\u4e00\u4e2a\u5305\u542b23\u4e07\u97f3\u9891\u6587\u4ef6\u30015500\u591a\u79cd\u7269\u79cd\u58f0\u97f3\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7269\u79cd\u58f0\u97f3\u5206\u7c7b\u7814\u7a76\u3002", "motivation": "\u63d0\u4f9b\u5927\u89c4\u6a21\u7684\u7269\u79cd\u58f0\u97f3\u6570\u636e\u96c6\uff0c\u652f\u6301\u751f\u7269\u591a\u6837\u6027\u7814\u7a76\u548c\u516c\u4f17\u53c2\u4e0e\u79d1\u5b66\u9879\u76ee\u3002", "method": "\u6536\u96c6\u6765\u81eaiNaturalist\u5e73\u53f0\u7684\u97f3\u9891\u6587\u4ef6\uff0c\u6807\u6ce8\u7269\u79cd\u4fe1\u606f\uff0c\u5e76\u6d4b\u8bd5\u591a\u5206\u7c7b\u548c\u591a\u6807\u7b7e\u5206\u7c7b\u6a21\u578b\u3002", "result": "\u6570\u636e\u96c6\u867d\u6807\u6ce8\u8f83\u5f31\uff0c\u4f46\u53ef\u7528\u4f5c\u9884\u8bad\u7ec3\u8d44\u6e90\uff0c\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6709\u5e2e\u52a9\u3002", "conclusion": "iNatSounds\u6709\u52a9\u4e8e\u5f00\u53d1\u65b0\u4e00\u4ee3\u516c\u5171\u53c2\u4e0e\u5de5\u5177\u548c\u652f\u6301\u751f\u7269\u591a\u6837\u6027\u7814\u7a76\u3002", "keywords": "iNatSounds, \u7269\u79cd\u58f0\u97f3\u5206\u7c7b, \u751f\u7269\u591a\u6837\u6027, \u516c\u6c11\u79d1\u5b66, \u97f3\u9891\u6570\u636e\u96c6"}}
{"id": "2506.01413", "pdf": "https://arxiv.org/pdf/2506.01413", "abs": "https://arxiv.org/abs/2506.01413", "authors": ["Yulei Qin", "Gang Li", "Zongyi Li", "Zihan Xu", "Yuchen Shi", "Zhekai Lin", "Xiao Cui", "Ke Li", "Xing Sun"], "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "10 pages of main body, 3 tables, 5 figures, 40 pages of appendix", "summary": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nare available at https://github.com/yuleiqin/RAIF.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6fc0\u52b1\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6307\u4ee4\u9075\u5faa\u4e2d\u7684\u8868\u73b0\u95ee\u9898\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u6307\u4ee4\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5f53\u6307\u4ee4\u5305\u542b\u5e76\u884c\u3001\u94fe\u5f0f\u548c\u5206\u652f\u7ed3\u6784\u7684\u591a\u91cd\u7ea6\u675f\u65f6\u3002\u4f20\u7edf\u7684\u201c\u601d\u7ef4\u94fe\u201d\u65b9\u6cd5\u56e0\u5176\u6d45\u5c42\u63a8\u7406\u6a21\u5f0f\u53cd\u800c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u53ef\u9a8c\u8bc1\u7684\u89c4\u5219\u9a71\u52a8\u5956\u52b1\u4fe1\u53f7\uff0c\u901a\u8fc7\u5206\u89e3\u590d\u6742\u6307\u4ee4\u5e76\u8bbe\u8ba1\u5bf9\u6bd4\u6837\u672c\u6765\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u5229\u7528\u4e13\u5bb6\u884c\u4e3a\u514b\u9686\u5b9e\u73b0\u6a21\u578b\u6027\u80fd\u7684\u7a33\u5b9a\u8fc1\u79fb\u3002", "result": "\u5728\u4e03\u4e2a\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u4f7f1.5B\u53c2\u6570\u7684\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u63d0\u534711.74%\uff0c\u63a5\u8fd18B\u53c2\u6570\u6a21\u578b\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6307\u4ee4\u9075\u5faa\u4e2d\u7684\u6d45\u5c42\u63a8\u7406\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u590d\u6742\u6307\u4ee4,\u601d\u7ef4\u94fe,\u5f3a\u5316\u5b66\u4e60,\u63a8\u7406\u80fd\u529b"}}
{"id": "2506.00379", "pdf": "https://arxiv.org/pdf/2506.00379", "abs": "https://arxiv.org/abs/2506.00379", "authors": ["Qi Qin", "Erbo Li", "Xingxiang Li", "Yifan Sun", "Wu Wang", "Chen Xu"], "title": "Label-shift robust federated feature screening for high-dimensional classification", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": "57 pages,9 tables,8 figures", "summary": "Distributed and federated learning are important tools for high-dimensional\nclassification of large datasets. To reduce computational costs and overcome\nthe curse of dimensionality, feature screening plays a pivotal role in\neliminating irrelevant features during data preprocessing. However, data\nheterogeneity, particularly label shifting across different clients, presents\nsignificant challenges for feature screening. This paper introduces a general\nframework that unifies existing screening methods and proposes a novel utility,\nlabel-shift robust federated feature screening (LR-FFS), along with its\nfederated estimation procedure. The framework facilitates a uniform analysis of\nmethods and systematically characterizes their behaviors under label shift\nconditions. Building upon this framework, LR-FFS leverages conditional\ndistribution functions and expectations to address label shift without adding\ncomputational burdens and remains robust against model misspecification and\noutliers. Additionally, the federated procedure ensures computational\nefficiency and privacy protection while maintaining screening effectiveness\ncomparable to centralized processing. We also provide a false discovery rate\n(FDR) control method for federated feature screening. Experimental results and\ntheoretical analyses demonstrate LR-FFS's superior performance across diverse\nclient environments, including those with varying class distributions, sample\nsizes, and missing categorical data.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86LR-FFS\uff0c\u4e00\u79cd\u6807\u7b7e\u504f\u79fb\u9c81\u68d2\u7684\u8054\u90a6\u7279\u5f81\u7b5b\u9009\u65b9\u6cd5\uff0c\u7edf\u4e00\u4e86\u73b0\u6709\u65b9\u6cd5\u6846\u67b6\uff0c\u5e76\u5728\u6807\u7b7e\u504f\u79fb\u6761\u4ef6\u4e0b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5206\u5e03\u5f0f\u548c\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u6570\u636e\u5f02\u6784\u6027\uff08\u5c24\u5176\u662f\u6807\u7b7e\u504f\u79fb\uff09\u7ed9\u7279\u5f81\u7b5b\u9009\u5e26\u6765\u6311\u6218\uff0c\u4e9f\u9700\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLR-FFS\u6846\u67b6\uff0c\u5229\u7528\u6761\u4ef6\u5206\u5e03\u51fd\u6570\u548c\u671f\u671b\u5904\u7406\u6807\u7b7e\u504f\u79fb\uff0c\u5e76\u8bbe\u8ba1\u8054\u90a6\u4f30\u8ba1\u8fc7\u7a0b\uff0c\u786e\u4fdd\u8ba1\u7b97\u9ad8\u6548\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "result": "\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cLR-FFS\u5728\u4e0d\u540c\u5ba2\u6237\u7aef\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5305\u62ec\u7c7b\u5206\u5e03\u3001\u6837\u672c\u91cf\u548c\u7f3a\u5931\u6570\u636e\u5dee\u5f02\u3002", "conclusion": "LR-FFS\u5728\u6807\u7b7e\u504f\u79fb\u6761\u4ef6\u4e0b\u5177\u6709\u9c81\u68d2\u6027\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u9690\u79c1\u5b89\u5168\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u8054\u90a6\u5b66\u4e60\u573a\u666f\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60, \u7279\u5f81\u7b5b\u9009, \u6807\u7b7e\u504f\u79fb, \u9690\u79c1\u4fdd\u62a4, \u8ba1\u7b97\u6548\u7387"}}
{"id": "2506.01510", "pdf": "https://arxiv.org/pdf/2506.01510", "abs": "https://arxiv.org/abs/2506.01510", "authors": ["Herman Kamper", "Benjamin van Niekerk", "Julian Za\u00efdi", "Marc-Andr\u00e9 Carbonneau"], "title": "LinearVC: Linear transformations of self-supervised features through the lens of voice conversion", "categories": ["eess.AS", "cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "We introduce LinearVC, a simple voice conversion method that sheds light on\nthe structure of self-supervised representations. First, we show that simple\nlinear transformations of self-supervised features effectively convert voices.\nNext, we probe the geometry of the feature space by constraining the set of\nallowed transformations. We find that just rotating the features is sufficient\nfor high-quality voice conversion. This suggests that content information is\nembedded in a low-dimensional subspace which can be linearly transformed to\nproduce a target voice. To validate this hypothesis, we finally propose a\nmethod that explicitly factorizes content and speaker information using\nsingular value decomposition; the resulting linear projection with a rank of\njust 100 gives competitive conversion results. Our work has implications for\nboth practical voice conversion and a broader understanding of self-supervised\nspeech representations. Samples and code: https://www.kamperh.com/linearvc/.", "AI": {"tldr": "LinearVC\u662f\u4e00\u79cd\u7b80\u5355\u7684\u8bed\u97f3\u8f6c\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u53d8\u6362\u81ea\u76d1\u7763\u7279\u5f81\u5b9e\u73b0\u9ad8\u6548\u8bed\u97f3\u8f6c\u6362\uff0c\u63ed\u793a\u4e86\u7279\u5f81\u7a7a\u95f4\u4e2d\u5185\u5bb9\u7684\u4f4e\u7ef4\u5d4c\u5165\u7279\u6027\u3002", "motivation": "\u7814\u7a76\u81ea\u76d1\u7763\u8868\u793a\u7684\u7ed3\u6784\uff0c\u63a2\u7d22\u8bed\u97f3\u8f6c\u6362\u4e2d\u5185\u5bb9\u4e0e\u8bf4\u8bdd\u8005\u4fe1\u606f\u7684\u5206\u79bb\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u53d8\u6362\u81ea\u76d1\u7763\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u65cb\u8f6c\u548c\u5947\u5f02\u503c\u5206\u89e3\u663e\u5f0f\u5206\u89e3\u5185\u5bb9\u4e0e\u8bf4\u8bdd\u8005\u4fe1\u606f\u3002", "result": "\u4ec5100\u7ef4\u7684\u7ebf\u6027\u6295\u5f71\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u8f6c\u6362\uff0c\u8868\u660e\u5185\u5bb9\u4fe1\u606f\u5d4c\u5165\u5728\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u4e2d\u3002", "conclusion": "LinearVC\u4e0d\u4ec5\u63d0\u4f9b\u5b9e\u7528\u7684\u8bed\u97f3\u8f6c\u6362\u65b9\u6cd5\uff0c\u8fd8\u6df1\u5316\u4e86\u5bf9\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u793a\u7684\u7406\u89e3\u3002", "keywords": "\u8bed\u97f3\u8f6c\u6362, \u81ea\u76d1\u7763\u8868\u793a, \u7ebf\u6027\u53d8\u6362, \u5947\u5f02\u503c\u5206\u89e3"}}
{"id": "2506.01064", "pdf": "https://arxiv.org/pdf/2506.01064", "abs": "https://arxiv.org/abs/2506.01064", "authors": ["Yudong Zhang", "Ruobing Xie", "Yiqing Huang", "Jiansheng Chen", "Xingwu Sun", "Zhanhui Kang", "Di Wang", "Yu Wang"], "title": "Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in large vision-language models (LVLMs) have showcased their\nremarkable capabilities across a wide range of multimodal vision-language\ntasks. However, these models remain vulnerable to visual adversarial attacks,\nwhich can substantially compromise their performance. Despite their potential\nimpact, the development of effective methods for purifying such adversarial\nexamples has received relatively limited attention. In this paper, we introduce\nF3, a novel adversarial purification framework that employs a counterintuitive\n\"fighting fire with fire\" strategy: intentionally introducing simple\nperturbations to adversarial examples to mitigate their harmful effects.\nSpecifically, F3 leverages cross-modal attentions derived from randomly\nperturbed adversary examples as reference targets. By injecting noise into\nthese adversarial examples, F3 effectively refines their attention, resulting\nin cleaner and more reliable model outputs. Remarkably, this seemingly\nparadoxical approach of employing noise to counteract adversarial attacks\nyields impressive purification results. Furthermore, F3 offers several distinct\nadvantages: it is training-free and straightforward to implement, and exhibits\nsignificant computational efficiency improvements compared to existing\npurification methods. These attributes render F3 particularly suitable for\nlarge-scale industrial applications where both robust performance and\noperational efficiency are critical priorities. The code will be made publicly\navailable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aF3\u7684\u65b0\u5bf9\u6297\u51c0\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u7b80\u5355\u6270\u52a8\u6765\u7f13\u89e3\u5bf9\u6297\u6837\u672c\u7684\u5371\u5bb3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u89c6\u89c9\u5bf9\u6297\u653b\u51fb\u4ecd\u663e\u8106\u5f31\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u5bf9\u6297\u6837\u672c\u51c0\u5316\u65b9\u6cd5\uff0cF3\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "F3\u91c7\u7528\u2018\u4ee5\u706b\u653b\u706b\u2019\u7b56\u7565\uff0c\u6545\u610f\u5f15\u5165\u968f\u673a\u6270\u52a8\uff0c\u5229\u7528\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u4f5c\u4e3a\u53c2\u8003\u76ee\u6807\uff0c\u4ece\u800c\u51c0\u5316\u5bf9\u6297\u6837\u672c\u3002", "result": "F3\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c0\u5316\uff0c\u8ba1\u7b97\u6548\u7387\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "F3\u662f\u4e00\u79cd\u7b80\u5355\u3001\u9ad8\u6548\u4e14\u9002\u5408\u5927\u89c4\u6a21\u5de5\u4e1a\u5e94\u7528\u7684\u5bf9\u6297\u51c0\u5316\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u5bf9\u6297\u653b\u51fb, \u51c0\u5316\u6846\u67b6, \u8de8\u6a21\u6001\u6ce8\u610f\u529b"}}
{"id": "2506.01551", "pdf": "https://arxiv.org/pdf/2506.01551", "abs": "https://arxiv.org/abs/2506.01551", "authors": ["Bingqian Lin", "Yunshuang Nie", "Khun Loun Zai", "Ziming Wei", "Mingfei Han", "Rongtao Xu", "Minzhe Niu", "Jianhua Han", "Liang Lin", "Cewu Lu", "Xiaodan Liang"], "title": "EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Building Vision-Language Navigation (VLN) agents which can navigate following\nnatural language instructions is a long-standing goal in human-robot\ninteraction applications. Recent studies have revealed the potential of\ntraining open-source Large Language Models (LLMs) to unleash LLMs' reasoning\nability for improving navigation, and simultaneously mitigate the domain gap\nbetween LLMs' training corpus and the VLN task. However, these approaches\nprimarily adopt direct input-output mapping paradigms, causing the mapping\nlearning difficult and the navigational decisions unexplainable.\nChain-of-Thought (CoT) training is a promising way to improve both navigational\ndecision accuracy and interpretability, while the complexity of the navigation\ntask makes the perfect CoT labels unavailable and may lead to overfitting\nthrough pure CoT supervised fine-tuning. In this paper, we propose a novel\nsElf-improving embodied reasoning framework for boosting LLM-based\nvision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two\nstages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model\nwith formalized CoT labels to both activate the model's navigational reasoning\ncapabilities and increase the reasoning speed; (2) Self-Reflective\nPost-Training, where the model is iteratively trained with its own reasoning\noutputs as self-enriched CoT labels to enhance the supervision diversity. A\nself-reflective auxiliary task is also introduced to encourage learning correct\nreasoning patterns by contrasting with wrong ones. Experimental results on the\npopular VLN benchmarks demonstrate the superiority of EvolveNav over previous\nLLM-based VLN approaches. Code is available at\nhttps://github.com/expectorlin/EvolveNav.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEvolveNav\u7684\u81ea\u6539\u8fdb\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u8bad\u7ec3\u548c\u81ea\u53cd\u601d\u673a\u5236\u63d0\u5347\u57fa\u4e8eLLM\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709VLN\u4efb\u52a1\u4e2d\u76f4\u63a5\u8f93\u5165-\u8f93\u51fa\u6620\u5c04\u65b9\u6cd5\u96be\u4ee5\u5b66\u4e60\u4e14\u51b3\u7b56\u4e0d\u53ef\u89e3\u91ca\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u8bad\u7ec3\u63d0\u9ad8\u5bfc\u822a\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u4f7f\u7528\u683c\u5f0f\u5316CoT\u6807\u7b7e\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff1b2) \u901a\u8fc7\u81ea\u53cd\u601d\u540e\u8bad\u7ec3\uff0c\u7528\u6a21\u578b\u81ea\u8eab\u63a8\u7406\u8f93\u51fa\u4f5c\u4e3a\u81ea\u589e\u5f3aCoT\u6807\u7b7e\uff0c\u5e76\u5f15\u5165\u81ea\u53cd\u601d\u8f85\u52a9\u4efb\u52a1\u3002", "result": "\u5728\u4e3b\u6d41VLN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEvolveNav\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684VLN\u65b9\u6cd5\u3002", "conclusion": "EvolveNav\u901a\u8fc7\u5206\u9636\u6bb5\u8bad\u7ec3\u548c\u81ea\u53cd\u601d\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u4efb\u52a1\u7684\u8868\u73b0\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u590d\u6742\u7684\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u3001\u94fe\u5f0f\u601d\u7ef4\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u81ea\u6539\u8fdb\u6846\u67b6\u3001\u591a\u6a21\u6001\u63a8\u7406"}}
{"id": "2506.01065", "pdf": "https://arxiv.org/pdf/2506.01065", "abs": "https://arxiv.org/abs/2506.01065", "authors": ["Ivan Milinovi\u0107", "Leon Stjepan Uroi\u0107", "Marko \u0110urasevi\u0107"], "title": "Trilevel Memetic Algorithm for the Electric Vehicle Routing Problem", "categories": ["cs.NE", "cs.AI", "I.2.8"], "comment": null, "summary": "The Electric Vehicle Routing Problem (EVRP) extends the capacitated vehicle\nrouting problem by incorporating battery constraints and charging stations,\nposing significant optimization challenges. This paper introduces a Trilevel\nMemetic Algorithm (TMA) that hierarchically optimizes customer sequences, route\nassignments, and charging station insertions. The method combines genetic\nalgorithms with dynamic programming, ensuring efficient and high-quality\nsolutions. Benchmark tests on WCCI2020 instances show competitive performance,\nmatching best-known results for small-scale cases. While computational demands\nlimit scalability, TMA demonstrates strong potential for sustainable logistics\nplanning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u7ea7\u9057\u4f20\u7b97\u6cd5\uff08TMA\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u7535\u52a8\u6c7d\u8f66\u8def\u5f84\u95ee\u9898\uff08EVRP\uff09\uff0c\u901a\u8fc7\u5206\u5c42\u4f18\u5316\u5ba2\u6237\u5e8f\u5217\u3001\u8def\u5f84\u5206\u914d\u548c\u5145\u7535\u7ad9\u63d2\u5165\u6765\u5e94\u5bf9\u7535\u6c60\u7ea6\u675f\u548c\u5145\u7535\u7ad9\u7684\u6311\u6218\u3002", "motivation": "\u7535\u52a8\u6c7d\u8f66\u8def\u5f84\u95ee\u9898\u56e0\u7535\u6c60\u9650\u5236\u548c\u5145\u7535\u7ad9\u7684\u5f15\u5165\u800c\u589e\u52a0\u4e86\u4f18\u5316\u96be\u5ea6\uff0c\u9700\u8981\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u548c\u52a8\u6001\u7f16\u7a0b\u7684\u4e09\u7ea7\u9057\u4f20\u7b97\u6cd5\uff08TMA\uff09\uff0c\u5206\u5c42\u4f18\u5316\u5ba2\u6237\u5e8f\u5217\u3001\u8def\u5f84\u5206\u914d\u548c\u5145\u7535\u7ad9\u63d2\u5165\u3002", "result": "\u5728WCCI2020\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c0f\u89c4\u6a21\u6848\u4f8b\u4e2d\u5339\u914d\u4e86\u6700\u4f73\u5df2\u77e5\u7ed3\u679c\uff0c\u5c3d\u7ba1\u8ba1\u7b97\u9700\u6c42\u9650\u5236\u4e86\u5176\u6269\u5c55\u6027\u3002", "conclusion": "TMA\u5c55\u73b0\u4e86\u5728\u53ef\u6301\u7eed\u7269\u6d41\u89c4\u5212\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u4f46\u8ba1\u7b97\u6548\u7387\u4ecd\u9700\u6539\u8fdb\u3002", "keywords": "\u7535\u52a8\u6c7d\u8f66\u8def\u5f84\u95ee\u9898, \u9057\u4f20\u7b97\u6cd5, \u52a8\u6001\u7f16\u7a0b, \u53ef\u6301\u7eed\u7269\u6d41"}}
{"id": "2506.01671", "pdf": "https://arxiv.org/pdf/2506.01671", "abs": "https://arxiv.org/abs/2506.01671", "authors": ["Adriana Eufrosina Bora", "Akshatha Arodi", "Duoyi Zhang", "Jordan Bannister", "Mirko Bronzi", "Arsene Fansi Tchango", "Md Abul Bashar", "Richi Nayak", "Kerrie Mengersen"], "title": "AIMSCheck: Leveraging LLMs for AI-Assisted Review of Modern Slavery Statements Across Jurisdictions", "categories": ["cs.CY", "cs.CL"], "comment": "27 pages, to appear at ACL 2025", "summary": "Modern Slavery Acts mandate that corporations disclose their efforts to\ncombat modern slavery, aiming to enhance transparency and strengthen practices\nfor its eradication. However, verifying these statements remains challenging\ndue to their complex, diversified language and the sheer number of statements\nthat must be reviewed. The development of NLP tools to assist in this task is\nalso difficult due to a scarcity of annotated data. Furthermore, as modern\nslavery transparency legislation has been introduced in several countries, the\ngeneralizability of such tools across legal jurisdictions must be studied. To\naddress these challenges, we work with domain experts to make two key\ncontributions. First, we present AIMS.uk and AIMS.ca, newly annotated datasets\nfrom the UK and Canada to enable cross-jurisdictional evaluation. Second, we\nintroduce AIMSCheck, an end-to-end framework for compliance validation.\nAIMSCheck decomposes the compliance assessment task into three levels,\nenhancing interpretability and practical applicability. Our experiments show\nthat models trained on an Australian dataset generalize well across UK and\nCanadian jurisdictions, demonstrating the potential for broader application in\ncompliance monitoring. We release the benchmark datasets and AIMSCheck to the\npublic to advance AI-adoption in compliance assessment and drive further\nresearch in this field.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u6807\u6ce8\u6570\u636e\u96c6AIMS.uk\u548cAIMS.ca\uff0c\u5e76\u5f15\u5165AIMSCheck\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u6cd5\u57df\u5408\u89c4\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5177\u5907\u5e7f\u6cdb\u5e94\u7528\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u4ee3\u5974\u96b6\u5236\u6cd5\u6848\u8981\u6c42\u4f01\u4e1a\u62ab\u9732\u5176\u5e94\u5bf9\u63aa\u65bd\uff0c\u4f46\u9a8c\u8bc1\u8fd9\u4e9b\u58f0\u660e\u7684\u590d\u6742\u6027\u9ad8\uff0c\u4e14\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\uff0c\u8de8\u6cd5\u57df\u901a\u7528\u6027\u9700\u7814\u7a76\u3002", "method": "\u4e0e\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\uff0c\u521b\u5efaUK\u548c\u52a0\u62ff\u5927\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5f00\u53d1AIMSCheck\u6846\u67b6\uff0c\u5206\u89e3\u5408\u89c4\u8bc4\u4f30\u4efb\u52a1\u4e3a\u4e09\u7ea7\u3002", "result": "\u57fa\u4e8e\u6fb3\u5927\u5229\u4e9a\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u5728UK\u548c\u52a0\u62ff\u5927\u8868\u73b0\u826f\u597d\uff0c\u8bc1\u660e\u5176\u8de8\u6cd5\u57df\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u53d1\u5e03\u6570\u636e\u96c6\u548c\u6846\u67b6\u53ef\u63a8\u52a8AI\u5728\u5408\u89c4\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\uff0c\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002", "keywords": "\u73b0\u4ee3\u5974\u96b6\u5236\u3001\u5408\u89c4\u8bc4\u4f30\u3001NLP\u3001\u8de8\u6cd5\u57df\u3001AIMSCheck"}}
{"id": "2506.01069", "pdf": "https://arxiv.org/pdf/2506.01069", "abs": "https://arxiv.org/abs/2506.01069", "authors": ["Malik A. Altayar", "Muhyeeddin Alqaraleh", "Mowafaq Salem Alzboon", "Wesam T. Almagharbeh"], "title": "Revolutionizing Blood Banks: AI-Driven Fingerprint-Blood Group Correlation for Enhanced Safety", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Identification of a person is central in forensic science, security, and\nhealthcare. Methods such as iris scanning and genomic profiling are more\naccurate but expensive, time-consuming, and more difficult to implement. This\nstudy focuses on the relationship between the fingerprint patterns and the ABO\nblood group as a biometric identification tool. A total of 200 subjects were\nincluded in the study, and fingerprint types (loops, whorls, and arches) and\nblood groups were compared. Associations were evaluated with statistical tests,\nincluding chi-square and Pearson correlation. The study found that the loops\nwere the most common fingerprint pattern and the O+ blood group was the most\nprevalent. Even though there was some associative pattern, there was no\nstatistically significant difference in the fingerprint patterns of different\nblood groups. Overall, the results indicate that blood group data do not\nsignificantly improve personal identification when used in conjunction with\nfingerprinting. Although the study shows weak correlation, it may emphasize the\nefforts of multi-modal based biometric systems in enhancing the current\nbiometric systems. Future studies may focus on larger and more diverse samples,\nand possibly machine learning and additional biometrics to improve\nidentification methods. This study addresses an element of the ever-changing\nnature of the fields of forensic science and biometric identification,\nhighlighting the importance of resilient analytical methods for personal\nidentification.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6307\u7eb9\u6a21\u5f0f\u4e0eABO\u8840\u578b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u4e24\u8005\u76f8\u5173\u6027\u8f83\u5f31\uff0c\u6307\u7eb9\u6a21\u5f0f\u5bf9\u8840\u578b\u7684\u4e2a\u4eba\u8bc6\u522b\u65e0\u663e\u8457\u5e2e\u52a9\u3002", "motivation": "\u63a2\u7d22\u6307\u7eb9\u6a21\u5f0f\u4e0e\u8840\u578b\u7684\u5173\u8054\uff0c\u4f5c\u4e3a\u751f\u7269\u8bc6\u522b\u5de5\u5177\u7684\u8865\u5145\uff0c\u4ee5\u63d0\u9ad8\u4e2a\u4eba\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "method": "\u5bf9200\u540d\u53d7\u8bd5\u8005\u7684\u6307\u7eb9\u7c7b\u578b\uff08\u73af\u3001\u6da1\u3001\u5f13\uff09\u548c\u8840\u578b\u8fdb\u884c\u7edf\u8ba1\u6bd4\u8f83\uff0c\u4f7f\u7528\u5361\u65b9\u68c0\u9a8c\u548c\u76ae\u5c14\u900a\u76f8\u5173\u5206\u6790\u3002", "result": "\u672a\u53d1\u73b0\u4e0d\u540c\u8840\u578b\u95f4\u6307\u7eb9\u6a21\u5f0f\u7684\u663e\u8457\u5dee\u5f02\uff0c\u8840\u578b\u6570\u636e\u5bf9\u6307\u7eb9\u8bc6\u522b\u7684\u8865\u5145\u4f5c\u7528\u6709\u9650\u3002", "conclusion": "\u8840\u578b\u6570\u636e\u4e0e\u6307\u7eb9\u6a21\u5f0f\u7ed3\u5408\u5bf9\u4e2a\u4eba\u8bc6\u522b\u63d0\u5347\u4e0d\u660e\u663e\uff0c\u672a\u6765\u9700\u591a\u6a21\u6001\u751f\u7269\u8bc6\u522b\u548c\u66f4\u5927\u6837\u672c\u7814\u7a76\u3002", "keywords": "\u6307\u7eb9\u6a21\u5f0f, ABO\u8840\u578b, \u751f\u7269\u8bc6\u522b, \u591a\u6a21\u6001\u7cfb\u7edf, \u7edf\u8ba1\u68c0\u9a8c"}}
{"id": "2506.00433", "pdf": "https://arxiv.org/pdf/2506.00433", "abs": "https://arxiv.org/abs/2506.00433", "authors": ["Luigi Sigillo", "Shengfeng He", "Danilo Comminiello"], "title": "Latent Wavelet Diffusion: Enabling 4K Image Synthesis for Free", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "High-resolution image synthesis remains a core challenge in generative\nmodeling, particularly in balancing computational efficiency with the\npreservation of fine-grained visual detail. We present Latent Wavelet Diffusion\n(LWD), a lightweight framework that enables any latent diffusion model to scale\nto ultra-high-resolution image generation (2K to 4K) for free. LWD introduces\nthree key components: (1) a scale-consistent variational autoencoder objective\nthat enhances the spectral fidelity of latent representations; (2) wavelet\nenergy maps that identify and localize detail-rich spatial regions within the\nlatent space; and (3) a time-dependent masking strategy that focuses denoising\nsupervision on high-frequency components during training. LWD requires no\narchitectural modifications and incurs no additional computational overhead.\nDespite its simplicity, it consistently improves perceptual quality and reduces\nFID in ultra-high-resolution image synthesis, outperforming strong baseline\nmodels. These results highlight the effectiveness of frequency-aware,\nsignal-driven supervision as a principled and efficient approach for\nhigh-resolution generative modeling.", "AI": {"tldr": "\u4e00\u79cd\u540d\u4e3aLWD\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u8d85\u9ad8\u5206\u8fa8\u7387\uff082K\u81f34K\uff09\u56fe\u50cf\u751f\u6210\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u4e2d\u8ba1\u7b97\u6548\u7387\u4e0e\u7ec6\u8282\u4fdd\u7559\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u5f15\u5165\u5c3a\u5ea6\u4e00\u81f4\u7684\u53d8\u5206\u81ea\u7f16\u7801\u76ee\u6807\u3001\u5c0f\u6ce2\u80fd\u91cf\u56fe\u548c\u65f6\u53d8\u63a9\u7801\u7b56\u7565\u3002", "result": "\u5728\u8d85\u9ad8\u6e05\u56fe\u50cf\u5408\u6210\u4e2d\u63d0\u9ad8\u4e86\u611f\u77e5\u8d28\u91cf\uff0c\u964d\u4f4e\u4e86FID\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "LWD\u8bc1\u660e\u9891\u7387\u611f\u77e5\u3001\u4fe1\u53f7\u9a71\u52a8\u7684\u65b9\u6cd5\u662f\u9ad8\u5206\u8fa8\u7387\u751f\u6210\u5efa\u6a21\u7684\u9ad8\u6548\u9014\u5f84\u3002", "keywords": "\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210,\u6f5c\u5728\u6269\u6563\u6a21\u578b,\u5c0f\u6ce2\u53d8\u6362,\u8ba1\u7b97\u6548\u7387"}}
{"id": "2506.01673", "pdf": "https://arxiv.org/pdf/2506.01673", "abs": "https://arxiv.org/abs/2506.01673", "authors": ["Sunkyung Lee", "Minjin Choi", "Eunseong Choi", "Hye-young Kim", "Jongwuk Lee"], "title": "GRAM: Generative Recommendation via Semantic-aware Multi-granular Late Fusion", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "ACL 2025 (Main Conference)", "summary": "Generative recommendation is an emerging paradigm that leverages the\nextensive knowledge of large language models by formulating recommendations\ninto a text-to-text generation task. However, existing studies face two key\nlimitations in (i) incorporating implicit item relationships and (ii) utilizing\nrich yet lengthy item information. To address these challenges, we propose a\nGenerative Recommender via semantic-Aware Multi-granular late fusion (GRAM),\nintroducing two synergistic innovations. First, we design semantic-to-lexical\ntranslation to encode implicit hierarchical and collaborative item\nrelationships into the vocabulary space of LLMs. Second, we present\nmulti-granular late fusion to integrate rich semantics efficiently with minimal\ninformation loss. It employs separate encoders for multi-granular prompts,\ndelaying the fusion until the decoding stage. Experiments on four benchmark\ndatasets show that GRAM outperforms eight state-of-the-art generative\nrecommendation models, achieving significant improvements of 11.5-16.0% in\nRecall@5 and 5.3-13.6% in NDCG@5. The source code is available at\nhttps://github.com/skleee/GRAM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86GRRAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u7684\u591a\u7c92\u5ea6\u540e\u671f\u878d\u5408\u6539\u8fdb\u751f\u6210\u63a8\u8350\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u9690\u5f0f\u9879\u76ee\u5173\u7cfb\u548c\u4e30\u5bcc\u4f46\u5197\u957f\u9879\u76ee\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u63a8\u8350\u7814\u7a76\u5728\u9690\u5f0f\u9879\u76ee\u5173\u7cfb\u5efa\u6a21\u548c\u5197\u957f\u9879\u76ee\u4fe1\u606f\u5229\u7528\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u80fd\u4fdd\u7559\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u8bed\u4e49\u5230\u8bcd\u6c47\u7684\u8f6c\u6362\u548c\u591a\u7c92\u5ea6\u540e\u671f\u878d\u5408\uff0c\u5206\u522b\u89e3\u51b3\u9879\u76ee\u5173\u7cfb\u7f16\u7801\u548c\u4fe1\u606f\u9ad8\u6548\u6574\u5408\u95ee\u9898\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cGRAM\u5728Recall@5\u548cNDCG@5\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u4e8611.5-16.0%\u548c5.3-13.6%\u3002", "conclusion": "GRAM\u901a\u8fc7\u4e24\u9879\u521b\u65b0\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u63a8\u8350\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u751f\u6210\u63a8\u8350\u3001\u591a\u7c92\u5ea6\u878d\u5408\u3001\u8bed\u4e49\u611f\u77e5\u3001\u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.01078", "pdf": "https://arxiv.org/pdf/2506.01078", "abs": "https://arxiv.org/abs/2506.01078", "authors": ["Yufei Zhan", "Ziheng Wu", "Yousong Zhu", "Rongkun Xue", "Ruipu Luo", "Zhenghao Chen", "Can Zhang", "Yifan Li", "Zhentao He", "Zheming Yang", "Ming Tang", "Minghui Qiu", "Jinqiao Wang"], "title": "GThinker: Towards General Multimodal Reasoning via Cue-Guided Rethinking", "categories": ["cs.CV", "cs.AI"], "comment": "Tech report", "summary": "Despite notable advancements in multimodal reasoning, leading Multimodal\nLarge Language Models (MLLMs) still underperform on vision-centric multimodal\nreasoning tasks in general scenarios. This shortfall stems from their\npredominant reliance on logic- and knowledge-based slow thinking strategies,\nwhile effective for domains like math and science, fail to integrate visual\ninformation effectively during reasoning. Consequently, these models often fail\nto adequately ground visual cues, resulting in suboptimal performance in tasks\nthat require multiple plausible visual interpretations and inferences. To\naddress this, we present GThinker (General Thinker), a novel reasoning MLLM\nexcelling in multimodal reasoning across general scenarios, mathematics, and\nscience. GThinker introduces Cue-Rethinking, a flexible reasoning pattern that\ngrounds inferences in visual cues and iteratively reinterprets these cues to\nresolve inconsistencies. Building on this pattern, we further propose a\ntwo-stage training pipeline, including pattern-guided cold start and incentive\nreinforcement learning, designed to enable multimodal reasoning capabilities\nacross domains. Furthermore, to support the training, we construct\nGThinker-11K, comprising 7K high-quality, iteratively-annotated reasoning paths\nand 4K curated reinforcement learning samples, filling the data gap toward\ngeneral multimodal reasoning. Extensive experiments demonstrate that GThinker\nachieves 81.5% on the challenging comprehensive multimodal reasoning benchmark\nM$^3$CoT, surpassing the latest O4-mini model. It also shows an average\nimprovement of 2.1% on general scenario multimodal reasoning benchmarks, while\nmaintaining on-par performance in mathematical reasoning compared to\ncounterpart advanced reasoning models. The code, model, and data will be\nreleased soon at https://github.com/jefferyZhan/GThinker.", "AI": {"tldr": "GThinker\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u89c6\u89c9\u7ebf\u7d22\u91cd\u65b0\u601d\u8003\uff08Cue-Rethinking\uff09\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u901a\u7528\u3001\u6570\u5b66\u548c\u79d1\u5b66\u573a\u666f\u7684\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u4f9d\u8d56\u903b\u8f91\u548c\u77e5\u8bc6\u63a8\u7406\uff0c\u672a\u80fd\u6709\u6548\u6574\u5408\u89c6\u89c9\u4fe1\u606f\u3002", "method": "\u63d0\u51faCue-Rethinking\u63a8\u7406\u6a21\u5f0f\uff0c\u901a\u8fc7\u89c6\u89c9\u7ebf\u7d22\u8fed\u4ee3\u4f18\u5316\u63a8\u7406\uff1b\u8bbe\u8ba1\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u6a21\u5f0f\u5f15\u5bfc\u51b7\u542f\u52a8\u548c\u6fc0\u52b1\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u5e76\u6784\u5efaGThinker-11K\u6570\u636e\u96c6\u3002", "result": "GThinker\u5728M$^3$CoT\u57fa\u51c6\u4e0a\u8fbe\u523081.5%\uff0c\u4f18\u4e8eO4-mini\uff0c\u901a\u7528\u573a\u666f\u63a8\u7406\u5e73\u5747\u63d0\u53472.1%\uff0c\u6570\u5b66\u63a8\u7406\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "GThinker\u901a\u8fc7\u89c6\u89c9\u7ebf\u7d22\u8fed\u4ee3\u548c\u57df\u9002\u5e94\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "keywords": "\u591a\u6a21\u6001\u63a8\u7406,Cue-Rethinking,GThinker-11K,\u4e24\u9636\u6bb5\u8bad\u7ec3"}}
{"id": "2506.00446", "pdf": "https://arxiv.org/pdf/2506.00446", "abs": "https://arxiv.org/abs/2506.00446", "authors": ["Tatsuki Takahashi", "Chihiro Maru", "Hiroko Shoji"], "title": "Off-Policy Evaluation of Ranking Policies via Embedding-Space User Behavior Modeling", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Off-policy evaluation (OPE) in ranking settings with large ranking action\nspaces, which stems from an increase in both the number of unique actions and\nlength of the ranking, is essential for assessing new recommender policies\nusing only logged bandit data from previous versions. To address the high\nvariance issues associated with existing estimators, we introduce two new\nassumptions: no direct effect on rankings and user behavior model on ranking\nembedding spaces. We then propose the generalized marginalized inverse\npropensity score (GMIPS) estimator with statistically desirable properties\ncompared to existing ones. Finally, we demonstrate that the GMIPS achieves the\nlowest MSE. Notably, among GMIPS variants, the marginalized reward interaction\nIPS (MRIPS) incorporates a doubly marginalized importance weight based on a\ncascade behavior assumption on ranking embeddings. MRIPS effectively balances\nthe trade-off between bias and variance, even as the ranking action spaces\nincrease and the above assumptions may not hold, as evidenced by our\nexperiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u5927\u89c4\u6a21\u6392\u5e8f\u52a8\u4f5c\u7a7a\u95f4\u4e0b\u79bb\u7b56\u7565\u8bc4\u4f30\uff08OPE\uff09\u7684\u9ad8\u65b9\u5dee\u95ee\u9898\u7684\u65b0\u5047\u8bbe\u548c\u5e7f\u4e49\u8fb9\u9645\u9006\u503e\u5411\u5f97\u5206\uff08GMIPS\uff09\u4f30\u8ba1\u5668\uff0c\u5176\u4e2dMRIPS\u53d8\u4f53\u5728\u504f\u5dee\u548c\u65b9\u5dee\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4f30\u8ba1\u5668\u5728\u5927\u89c4\u6a21\u6392\u5e8f\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u9ad8\u65b9\u5dee\u7684\u95ee\u9898\uff0c\u4e3a\u65b0\u63a8\u8350\u7b56\u7565\u7684\u8bc4\u4f30\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u65b0\u5047\u8bbe\uff08\u6392\u540d\u65e0\u76f4\u63a5\u6548\u5e94\u548c\u7528\u6237\u884c\u4e3a\u6a21\u578b\u5728\u6392\u5e8f\u5d4c\u5165\u7a7a\u95f4\uff09\uff0c\u5e76\u8bbe\u8ba1GMIPS\u4f30\u8ba1\u5668\u53ca\u5176\u53d8\u4f53MRIPS\uff0c\u5229\u7528\u53cc\u91cd\u8fb9\u9645\u91cd\u8981\u6027\u6743\u91cd\u3002", "result": "GMIPS\u5728\u5b9e\u9a8c\u4e2d\u83b7\u5f97\u6700\u4f4e\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\uff0cMRIPS\u5728\u5047\u8bbe\u4e0d\u5b8c\u5168\u6210\u7acb\u65f6\u4ecd\u80fd\u6709\u6548\u5e73\u8861\u504f\u5dee\u548c\u65b9\u5dee\u3002", "conclusion": "GMIPS\u7279\u522b\u662fMRIPS\u53d8\u4f53\u5728\u5927\u89c4\u6a21\u6392\u5e8f\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u63a8\u8350\u7cfb\u7edf\u8bc4\u4f30\u3002", "keywords": "\u79bb\u7b56\u7565\u8bc4\u4f30,\u5927\u89c4\u6a21\u6392\u5e8f\u52a8\u4f5c\u7a7a\u95f4,GMIPS,MRIPS,\u63a8\u8350\u7cfb\u7edf"}}
{"id": "2506.01079", "pdf": "https://arxiv.org/pdf/2506.01079", "abs": "https://arxiv.org/abs/2506.01079", "authors": ["Long Qian", "Eric Wang", "Bernardo Subercaseaux", "Marijn J. H. Heule"], "title": "Unfolding Boxes with Local Constraints", "categories": ["cs.CG", "cs.AI"], "comment": "Accepted at CADE30 (2025). 17 figures. Code at\n  https://github.com/LongQianQL/CADE30-BoxUnfoldings", "summary": "We consider the problem of finding and enumerating polyominos that can be\nfolded into multiple non-isomorphic boxes. While several computational\napproaches have been proposed, including SAT, randomized algorithms, and\ndecision diagrams, none has been able to perform at scale. We argue that\nexisting SAT encodings are hindered by the presence of global constraints\n(e.g., graph connectivity or acyclicity), which are generally hard to encode\neffectively and hard for solvers to reason about. In this work, we propose a\nnew SAT-based approach that replaces these global constraints with simple local\nconstraints that have substantially better propagation properties. Our approach\ndramatically improves the scalability of both computing and enumerating common\nbox unfoldings: (i) while previous approaches could only find common unfoldings\nof two boxes up to area 88, ours easily scales beyond 150, and (ii) while\nprevious approaches were only able to enumerate common unfoldings up to area\n30, ours scales up to 60. This allows us to rule out 46, 54, and 58 as the\nsmallest areas allowing a common unfolding of three boxes, thereby refuting a\nconjecture of Xu et al. (2017).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8eSAT\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u66ff\u6362\u5168\u5c40\u7ea6\u675f\u4e3a\u5c40\u90e8\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u9762\u4f53\u5c55\u5f00\u7684\u8ba1\u7b97\u548c\u679a\u4e3e\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u5148\u524d\u65b9\u6cd5\u7684\u89c4\u6a21\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709SAT\u7f16\u7801\u56e0\u5168\u5c40\u7ea6\u675f\uff08\u5982\u56fe\u8fde\u901a\u6027\u6216\u65e0\u73af\u6027\uff09\u7684\u5b58\u5728\u800c\u53d7\u9650\uff0c\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u591a\u9762\u4f53\u5c55\u5f00\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684SAT\u65b9\u6cd5\uff0c\u5c06\u5168\u5c40\u7ea6\u675f\u66ff\u6362\u4e3a\u5177\u6709\u66f4\u597d\u4f20\u64ad\u7279\u6027\u7684\u5c40\u90e8\u7ea6\u675f\u3002", "result": "\u65b0\u65b9\u6cd5\u5c06\u53cc\u76d2\u5171\u5c55\u9762\u79ef\u4ece88\u63d0\u5347\u81f3150\uff0c\u679a\u4e3e\u80fd\u529b\u4ece30\u63d0\u5347\u81f360\uff0c\u5e76\u5426\u5b9a\u4e86Xu\u7b49\u4eba\u5173\u4e8e\u4e09\u76d2\u5171\u5c55\u6700\u5c0f\u9762\u79ef\u7684\u731c\u60f3\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdbSAT\u7f16\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u591a\u9762\u4f53\u5c55\u5f00\u95ee\u9898\u7684\u80fd\u529b\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "SAT, \u591a\u9762\u4f53\u5c55\u5f00, \u5c40\u90e8\u7ea6\u675f, \u679a\u4e3e, \u56fe\u8fde\u901a\u6027"}}
{"id": "2506.00450", "pdf": "https://arxiv.org/pdf/2506.00450", "abs": "https://arxiv.org/abs/2506.00450", "authors": ["Wenhan Lyu", "Devashish Tyagi", "Yihang Yang", "Ziwei Li", "Ajay Somani", "Karthikeyan Shanmugasundaram", "Nikola Andrejevic", "Ferdi Adeputra", "Curtis Zeng", "Arun K. Singh", "Maxime Ransan", "Sagar Jain"], "title": "DV365: Extremely Long User History Modeling at Instagram", "categories": ["cs.IR", "cs.LG"], "comment": "SIGKDD 2025 accepted", "summary": "Long user history is highly valuable signal for recommendation systems, but\neffectively incorporating it often comes with high cost in terms of data center\npower consumption and GPU. In this work, we chose offline embedding over\nend-to-end sequence length optimization methods to enable extremely long user\nsequence modeling as a cost-effective solution, and propose a new user\nembedding learning strategy, multi-slicing and summarization, that generates\nhighly generalizable user representation of user's long-term stable interest.\nHistory length we encoded in this embedding is up to 70,000 and on average\n40,000. This embedding, named as DV365, is proven highly incremental on top of\nadvanced attentive user sequence models deployed in Instagram. Produced by a\nsingle upstream foundational model, it is launched in 15 different models\nacross Instagram and Threads with significant impact, and has been production\nbattle-proven for >1 year since our first launch.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79bb\u7ebf\u5d4c\u5165\u65b9\u6cd5\uff08DV365\uff09\uff0c\u901a\u8fc7\u591a\u5207\u7247\u548c\u6458\u8981\u6280\u672f\u5904\u7406\u8d85\u957f\u7528\u6237\u5386\u53f2\u5e8f\u5217\uff0c\u4ee5\u4f4e\u6210\u672c\u5b9e\u73b0\u9ad8\u6548\u63a8\u8350\u3002", "motivation": "\u957f\u7528\u6237\u5386\u53f2\u6570\u636e\u5bf9\u63a8\u8350\u7cfb\u7edf\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u76f4\u63a5\u4f7f\u7528\u6210\u672c\u9ad8\uff0c\u9700\u4f4e\u8017\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u79bb\u7ebf\u5d4c\u5165\u548c\u7528\u6237\u5d4c\u5165\u5b66\u4e60\u7b56\u7565\uff08\u591a\u5207\u7247\u4e0e\u6458\u8981\uff09\uff0c\u751f\u6210\u7a33\u5b9a\u5174\u8da3\u7684\u7528\u6237\u8868\u5f81\u3002", "result": "DV365\u5d4c\u5165\u5728Instagram\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u652f\u630115\u4e2a\u6a21\u578b\u5e76\u7a33\u5b9a\u8fd0\u884c1\u5e74\u4ee5\u4e0a\u3002", "conclusion": "\u79bb\u7ebf\u5d4c\u5165\u65b9\u6cd5\u662f\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6709\u6548\u5904\u7406\u8d85\u957f\u7528\u6237\u5e8f\u5217\u3002", "keywords": "\u63a8\u8350\u7cfb\u7edf, \u957f\u7528\u6237\u5e8f\u5217, \u79bb\u7ebf\u5d4c\u5165, \u591a\u5207\u7247, \u6458\u8981\u6280\u672f, Instagram"}}
{"id": "2506.01085", "pdf": "https://arxiv.org/pdf/2506.01085", "abs": "https://arxiv.org/abs/2506.01085", "authors": ["Shivam Chandhok", "Qian Yang", "Oscar Manas", "Kanishk Jain", "Leonid Sigal", "Aishwarya Agrawal"], "title": "Learning What Matters: Prioritized Concept Learning via Relative Error-driven Sample Selection", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint", "summary": "Instruction tuning has been central to the success of recent vision-language\nmodels (VLMs), but it remains expensive-requiring large-scale datasets,\nhigh-quality annotations, and large compute budgets. We propose PRioritized\ncOncept learninG via Relative Error-driven Sample Selection (PROGRESS), a data-\nand compute-efficient framework that enables VLMs to dynamically select what to\nlearn next based on their evolving needs during training. At each stage, the\nmodel tracks its learning progress across skills and selects the most\ninformative samples-those it has not already mastered and that are not too\ndifficult to learn at the current stage of training. This strategy effectively\ncontrols skill acquisition and the order in which skills are learned.\nSpecifically, we sample from skills showing the highest learning progress,\nprioritizing those with the most rapid improvement. Unlike prior methods,\nPROGRESS requires no upfront answer annotations, queries answers only on a need\nbasis, avoids reliance on additional supervision from auxiliary VLMs, and does\nnot require compute-heavy gradient computations for data selection. Experiments\nacross multiple instruction-tuning datasets of varying scales demonstrate that\nPROGRESS consistently outperforms state-of-the-art baselines with much less\ndata and supervision. Additionally, we show strong cross-architecture\ngeneralization and transferability to larger models, validating PROGRESS as a\nscalable solution for efficient learning.", "AI": {"tldr": "PROGRESS\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u9009\u62e9\u6837\u672c\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5148\u7ea7\u6982\u5ff5\u5b66\u4e60\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6307\u4ee4\u8c03\u4f18\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u5305\u62ec\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3001\u9ad8\u8d28\u91cf\u6807\u6ce8\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "method": "PROGRESS\u901a\u8fc7\u76f8\u5bf9\u8bef\u5dee\u9a71\u52a8\u7684\u6837\u672c\u9009\u62e9\u52a8\u6001\u9009\u62e9\u672a\u638c\u63e1\u4e14\u96be\u5ea6\u9002\u4e2d\u7684\u6837\u672c\uff0c\u4f18\u5148\u5b66\u4e60\u8fdb\u6b65\u6700\u5feb\u7684\u6280\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPROGRESS\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u6570\u636e\u9700\u6c42\u91cf\u66f4\u4f4e\u3002", "conclusion": "PROGRESS\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u9ad8\u6548\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u8de8\u67b6\u6784\u901a\u7528\u6027\u548c\u5411\u66f4\u5927\u6a21\u578b\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u6307\u4ee4\u8c03\u4f18, \u6570\u636e\u6548\u7387, \u6837\u672c\u9009\u62e9, PROGRESS"}}
{"id": "2506.01107", "pdf": "https://arxiv.org/pdf/2506.01107", "abs": "https://arxiv.org/abs/2506.01107", "authors": ["Abderrahim Bendahi", "Benjamin Doerr", "Adrien Fradin", "Johannes F. Lutzeyer"], "title": "Speeding Up Hyper-Heuristics With Markov-Chain Operator Selection and the Only-Worsening Acceptance Operator", "categories": ["cs.NE", "cs.AI", "cs.DS"], "comment": "Accepted at IJCAI 2025", "summary": "The move-acceptance hyper-heuristic was recently shown to be able to leave\nlocal optima with astonishing efficiency (Lissovoi et al., Artificial\nIntelligence (2023)). In this work, we propose two modifications to this\nalgorithm that demonstrate impressive performances on a large class of\nbenchmarks including the classic Cliff$_d$ and Jump$_m$ function classes. (i)\nInstead of randomly choosing between the only-improving and any-move acceptance\noperator, we take this choice via a simple two-state Markov chain. This\nmodification alone reduces the runtime on Jump$_m$ functions with gap parameter\n$m$ from $\\Omega(n^{2m-1})$ to $O(n^{m+1})$. (ii) We then replace the all-moves\nacceptance operator with the operator that only accepts worsenings. Such a,\ncounter-intuitive, operator has not been used before in the literature.\nHowever, our proofs show that our only-worsening operator can greatly help in\nleaving local optima, reducing, e.g., the runtime on Jump functions to $O(n^3\n\\log n)$ independent of the gap size. In general, we prove a remarkably good\nruntime of $O(n^{k+1} \\log n)$ for our Markov move-acceptance hyper-heuristic\non all members of a new benchmark class SEQOPT$_k$, which contains a large\nnumber of functions having $k$ successive local optima, and which contains the\ncommonly studied Jump$_m$ and Cliff$_d$ functions for $k=2$.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u79fb\u52a8\u63a5\u53d7\u8d85\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u4e24\u79cd\u4fee\u6539\uff08\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u9009\u62e9\u548c\u4ec5\u63a5\u53d7\u6076\u5316\u89e3\u7684\u7b97\u5b50\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u591a\u79cd\u57fa\u51c6\u51fd\u6570\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u79fb\u52a8\u63a5\u53d7\u8d85\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728\u8df3\u51fa\u5c40\u90e8\u6700\u4f18\u65f6\u6548\u7387\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u6539\u8fdb\uff1a1\uff09\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u94fe\u9009\u62e9\u63a5\u53d7\u7b97\u5b50\uff1b2\uff09\u5f15\u5165\u4ec5\u63a5\u53d7\u6076\u5316\u89e3\u7684\u65b0\u7b97\u5b50\u3002", "result": "\u6539\u8fdb\u540e\u7684\u7b97\u6cd5\u5728Jump\u51fd\u6570\u4e0a\u7684\u8fd0\u884c\u65f6\u95f4\u4ece\u6307\u6570\u7ea7\u964d\u4f4e\u5230\u591a\u9879\u5f0f\u7ea7\uff0c\u4e14\u5728\u65b0\u7684SEQOPTk\u57fa\u51c6\u7c7b\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u94fe\u548c\u4ec5\u63a5\u53d7\u6076\u5316\u89e3\u7684\u7b97\u5b50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b97\u6cd5\u8df3\u51fa\u5c40\u90e8\u6700\u4f18\u7684\u6548\u7387\u3002", "keywords": "\u8d85\u542f\u53d1\u5f0f, \u5c40\u90e8\u6700\u4f18, \u9a6c\u5c14\u53ef\u592b\u94fe, Jump\u51fd\u6570, \u79fb\u52a8\u63a5\u53d7"}}
{"id": "2506.00471", "pdf": "https://arxiv.org/pdf/2506.00471", "abs": "https://arxiv.org/abs/2506.00471", "authors": ["Shijun Cheng", "Tariq Alkhalifah"], "title": "DiffPINN: Generative diffusion-initialized physics-informed neural networks for accelerating seismic wavefield representation", "categories": ["physics.geo-ph", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "Physics-informed neural networks (PINNs) offer a powerful framework for\nseismic wavefield modeling, yet they typically require time-consuming\nretraining when applied to different velocity models. Moreover, their training\ncan suffer from slow convergence due to the complexity of of the wavefield\nsolution. To address these challenges, we introduce a latent diffusion-based\nstrategy for rapid and effective PINN initialization. First, we train multiple\nPINNs to represent frequency-domain scattered wavefields for various velocity\nmodels, then flatten each trained network's parameters into a one-dimensional\nvector, creating a comprehensive parameter dataset. Next, we employ an\nautoencoder to learn latent representations of these parameter vectors,\ncapturing essential patterns across diverse PINN's parameters. We then train a\nconditional diffusion model to store the distribution of these latent vectors,\nwith the corresponding velocity models serving as conditions. Once trained,\nthis diffusion model can generate latent vectors corresponding to new velocity\nmodels, which are subsequently decoded by the autoencoder into complete PINN\nparameters. Experimental results indicate that our method significantly\naccelerates training and maintains high accuracy across in-distribution and\nout-of-distribution velocity scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u5feb\u901f\u521d\u59cb\u5316\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\uff0c\u4ee5\u89e3\u51b3\u4e0d\u540c\u901f\u5ea6\u6a21\u578b\u4e0b\u8bad\u7ec3\u8017\u65f6\u548c\u6536\u655b\u6162\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfPINNs\u5728\u4e0d\u540c\u901f\u5ea6\u6a21\u578b\u4e0a\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u4e14\u6536\u655b\u6162\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u521d\u59cb\u5316\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u591a\u4e2aPINN\u751f\u6210\u53c2\u6570\u5411\u91cf\uff0c\u5229\u7528\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u8bad\u7ec3\u6761\u4ef6\u6269\u6563\u6a21\u578b\u751f\u6210\u65b0\u6a21\u578b\u7684\u53c2\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u52a0\u901f\u8bad\u7ec3\uff0c\u5e76\u5728\u5404\u79cd\u901f\u5ea6\u6a21\u578b\u4e0b\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u6f5c\u5728\u6269\u6563\u7b56\u7565\u4e3aPINNs\u7684\u9ad8\u6548\u521d\u59cb\u5316\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "keywords": "\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u3001\u6f5c\u5728\u6269\u6563\u3001\u81ea\u7f16\u7801\u5668\u3001\u901f\u5ea6\u6a21\u578b\u3001\u6ce2\u573a\u5efa\u6a21"}}
{"id": "2506.01877", "pdf": "https://arxiv.org/pdf/2506.01877", "abs": "https://arxiv.org/abs/2506.01877", "authors": ["Dayoon Ko", "Jinyoung Kim", "Sohyeon Kim", "Jinhyuk Kim", "Jaehoon Lee", "Seonghak Song", "Minyoung Lee", "Gunhee Kim"], "title": "When Should Dense Retrievers Be Updated in Evolving Corpora? Detecting Out-of-Distribution Corpora Using GradNormIR", "categories": ["cs.IR", "cs.CL"], "comment": "ACL 2025 Findings", "summary": "Dense retrievers encode texts into embeddings to efficiently retrieve\nrelevant documents from large databases in response to user queries. However,\nreal-world corpora continually evolve, leading to a shift from the original\ntraining distribution of the retriever. Without timely updates or retraining,\nindexing newly emerging documents can degrade retrieval performance for future\nqueries. Thus, identifying when a dense retriever requires an update is\ncritical for maintaining robust retrieval systems. In this paper, we propose a\nnovel task of predicting whether a corpus is out-of-distribution (OOD) relative\nto a dense retriever before indexing. Addressing this task allows us to\nproactively manage retriever updates, preventing potential retrieval failures.\nWe introduce GradNormIR, an unsupervised approach that leverages gradient norms\nto detect OOD corpora effectively. Experiments on the BEIR benchmark\ndemonstrate that GradNormIR enables timely updates of dense retrievers in\nevolving document collections, significantly enhancing retrieval robustness and\nefficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGradNormIR\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u5bc6\u96c6\u68c0\u7d22\u5668\u7684\u8bed\u6599\u5e93\u662f\u5426\u5206\u5e03\u504f\u79fb\uff0c\u4ee5\u63d0\u524d\u7ba1\u7406\u68c0\u7d22\u5668\u66f4\u65b0\uff0c\u63d0\u5347\u68c0\u7d22\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u7684\u8bed\u6599\u5e93\u4e0d\u65ad\u53d8\u5316\uff0c\u82e5\u4e0d\u53ca\u65f6\u66f4\u65b0\u6216\u91cd\u65b0\u8bad\u7ec3\u68c0\u7d22\u5668\uff0c\u65b0\u6587\u6863\u7684\u7d22\u5f15\u53ef\u80fd\u5bfc\u81f4\u68c0\u7d22\u6027\u80fd\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u8bc6\u522b\u4f55\u65f6\u9700\u8981\u66f4\u65b0\u68c0\u7d22\u5668\u662f\u5173\u952e\u3002", "method": "\u63d0\u51fa\u4e86GradNormIR\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u8303\u6570\u68c0\u6d4b\u8bed\u6599\u5e93\u662f\u5426\u8d85\u51fa\u5bc6\u96c6\u68c0\u7d22\u5668\u7684\u539f\u59cb\u5206\u5e03\uff08OOD\uff09\u3002", "result": "\u5728BEIR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGradNormIR\u6709\u6548\u5730\u63d0\u5347\u4e86\u5bc6\u96c6\u68c0\u7d22\u5668\u5728\u52a8\u6001\u8bed\u6599\u5e93\u4e2d\u7684\u68c0\u7d22\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "conclusion": "GradNormIR\u65b9\u6cd5\u80fd\u591f\u4e3b\u52a8\u7ba1\u7406\u68c0\u7d22\u5668\u66f4\u65b0\uff0c\u9632\u6b62\u6f5c\u5728\u7684\u68c0\u7d22\u5931\u8d25\uff0c\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u3002", "keywords": "\u5bc6\u96c6\u68c0\u7d22\u5668,\u5206\u5e03\u504f\u79fb,\u68af\u5ea6\u8303\u6570,\u65e0\u76d1\u7763\u5b66\u4e60,\u52a8\u6001\u8bed\u6599\u5e93"}}
{"id": "2506.01109", "pdf": "https://arxiv.org/pdf/2506.01109", "abs": "https://arxiv.org/abs/2506.01109", "authors": ["Fengze Li", "Yangle Liu", "Jieming Ma", "Hai-Ning Liang", "Yaochun Shen", "Huangxiang Li", "Zhijing Wu"], "title": "CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic Gaussian Splatting", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Accurate fruit counting in real-world agricultural environments is a\nlongstanding challenge due to visual occlusions, semantic ambiguity, and the\nhigh computational demands of 3D reconstruction. Existing methods based on\nneural radiance fields suffer from low inference speed, limited generalization,\nand lack support for open-set semantic control. This paper presents\nFruitLangGS, a real-time 3D fruit counting framework that addresses these\nlimitations through spatial reconstruction, semantic embedding, and\nlanguage-guided instance estimation. FruitLangGS first reconstructs\norchard-scale scenes using an adaptive Gaussian splatting pipeline with\nradius-aware pruning and tile-based rasterization for efficient rendering. To\nenable semantic control, each Gaussian encodes a compressed CLIP-aligned\nlanguage embedding, forming a compact and queryable 3D representation. At\ninference time, prompt-based semantic filtering is applied directly in 3D\nspace, without relying on image-space segmentation or view-level fusion. The\nselected Gaussians are then converted into dense point clouds via\ndistribution-aware sampling and clustered to estimate fruit counts.\nExperimental results on real orchard data demonstrate that FruitLangGS achieves\nhigher rendering speed, semantic flexibility, and counting accuracy compared to\nprior approaches, offering a new perspective for language-driven, real-time\nneural rendering across open-world scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86FruitLangGS\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u91cd\u5efa\u3001\u8bed\u4e49\u5d4c\u5165\u548c\u8bed\u8a00\u5f15\u5bfc\u7684\u5b9e\u4f8b\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f63D\u6c34\u679c\u8ba1\u6570\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6e32\u67d3\u901f\u5ea6\u3001\u8bed\u4e49\u7075\u6d3b\u6027\u548c\u8ba1\u6570\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u519c\u4e1a\u73af\u5883\u4e2d\u56e0\u89c6\u89c9\u906e\u6321\u3001\u8bed\u4e49\u6a21\u7cca\u548c3D\u91cd\u5efa\u8ba1\u7b97\u9700\u6c42\u9ad8\u800c\u96be\u4ee5\u51c6\u786e\u8ba1\u6570\u6c34\u679c\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u81ea\u9002\u5e94\u9ad8\u65af\u6e85\u5c04\u7ba1\u9053\u8fdb\u884c\u573a\u666f\u91cd\u5efa\uff0c\u7ed3\u5408CLIP\u5bf9\u9f50\u7684\u8bed\u8a00\u5d4c\u5165\u5b9e\u73b0\u8bed\u4e49\u63a7\u5236\uff0c\u5e76\u91c7\u7528\u5206\u5e03\u611f\u77e5\u91c7\u6837\u548c\u805a\u7c7b\u8fdb\u884c\u5b9e\u4f8b\u4f30\u8ba1\u3002", "result": "\u5728\u771f\u5b9e\u679c\u56ed\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86FruitLangGS\u5728\u6e32\u67d3\u901f\u5ea6\u3001\u8bed\u4e49\u7075\u6d3b\u6027\u548c\u8ba1\u6570\u7cbe\u5ea6\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "FruitLangGS\u4e3a\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u8bed\u8a00\u9a71\u52a8\u5b9e\u65f6\u795e\u7ecf\u6e32\u67d3\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "keywords": "3D\u6c34\u679c\u8ba1\u6570,\u8bed\u4e49\u63a7\u5236,\u5b9e\u65f6\u6e32\u67d3,\u9ad8\u65af\u6e85\u5c04,\u8bed\u8a00\u5d4c\u5165"}}
{"id": "2506.01111", "pdf": "https://arxiv.org/pdf/2506.01111", "abs": "https://arxiv.org/abs/2506.01111", "authors": ["Shunian Chen", "Xinyuan Xie", "Zheshu Chen", "Liyan Zhao", "Owen Lee", "Zhan Su", "Qilin Sun", "Benyou Wang"], "title": "FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal Contextual Fusion", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "High-quality, large-scale audio captioning is crucial for advancing audio\nunderstanding, yet current automated methods often generate captions that lack\nfine-grained detail and contextual accuracy, primarily due to their reliance on\nlimited unimodal or superficial multimodal information. Drawing inspiration\nfrom human auditory perception, which adeptly integrates cross-modal cues and\nperforms sophisticated auditory scene analysis, we introduce a novel two-stage\nautomated pipeline. This pipeline first employs specialized pretrained models\nto extract diverse contextual cues (e.g., speech, music, general sounds, and\nvisual information from associated video). A large language model (LLM) then\nsynthesizes these rich, multimodal inputs to generate detailed and\ncontext-aware audio captions. Key contributions of this work include: (1) the\nproposed scalable method for fine-grained audio caption generation; (2)\nFusionAudio, a new large-scale dataset comprising 1.2 million such detailed\ncaptions, combined with 6 million QA pairs; and (3) enhanced audio models\ndeveloped using FusionAudio, specifically a CLAP-based audio encoder with\nsuperior audio-text alignment and instruction following. This paper paves the\nway for more nuanced and accurate automated understanding of complex audio\nenvironments. Code and data can be found in\nhttps://github.com/satsuki2486441738/FusionAudio.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u4fe1\u606f\u751f\u6210\u7ec6\u7c92\u5ea6\u7684\u97f3\u9891\u63cf\u8ff0\uff0c\u5e76\u53d1\u5e03\u5927\u89c4\u6a21\u6570\u636e\u96c6FusionAudio\u3002", "motivation": "\u5f53\u524d\u97f3\u9891\u63cf\u8ff0\u65b9\u6cd5\u7f3a\u4e4f\u7ec6\u8282\u548c\u4e0a\u4e0b\u6587\u51c6\u786e\u6027\uff0c\u53d7\u9650\u4e8e\u5355\u6a21\u6001\u6216\u6d45\u5c42\u591a\u6a21\u6001\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u53d6\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u7ed3\u5408LLM\u751f\u6210\u8be6\u7ec6\u97f3\u9891\u63cf\u8ff0\u3002", "result": "\u5f00\u53d1\u4e86FusionAudio\u6570\u636e\u96c6\uff08120\u4e07\u63cf\u8ff0\u548c600\u4e07QA\u5bf9\uff09\uff0c\u5e76\u6539\u8fdb\u4e86\u97f3\u9891-\u6587\u672c\u5bf9\u9f50\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63a8\u52a8\u4e86\u590d\u6742\u97f3\u9891\u73af\u5883\u7684\u81ea\u52a8\u5316\u7406\u89e3\u3002", "keywords": "\u97f3\u9891\u63cf\u8ff0\u3001\u591a\u6a21\u6001\u4fe1\u606f\u3001FusionAudio\u6570\u636e\u96c6\u3001LLM\u3001CLAP\u7f16\u7801\u5668"}}
{"id": "2506.01902", "pdf": "https://arxiv.org/pdf/2506.01902", "abs": "https://arxiv.org/abs/2506.01902", "authors": ["Xinliu Zhong", "Kayhan Batmanghelich", "Li Sun"], "title": "Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination", "categories": ["cs.CV", "cs.CL"], "comment": "6 pages, 1 figure, accepted by 2024 IEEE Conference on Artificial\n  Intelligence (CAI)", "summary": "Vision-language models pre-trained on large scale of unlabeled biomedical\nimages and associated reports learn generalizable semantic representations.\nThese multi-modal representations can benefit various downstream tasks in the\nbiomedical domain. Contrastive learning is widely used to pre-train\nvision-language models for general natural images and associated captions.\nDespite its popularity, we found biomedical texts have complex and\ndomain-specific semantics that are often neglected by common contrastive\nmethods. To address this issue, we propose a novel method, perturbed report\ndiscrimination, for pre-train biomedical vision-language models. First, we\ncurate a set of text perturbation methods that keep the same words, but disrupt\nthe semantic structure of the sentence. Next, we apply different types of\nperturbation to reports, and use the model to distinguish the original report\nfrom the perturbed ones given the associated image. Parallel to this, we\nenhance the sensitivity of our method to higher level of granularity for both\nmodalities by contrasting attention-weighted image sub-regions and sub-words in\nthe image-text pairs. We conduct extensive experiments on multiple downstream\ntasks, and our method outperforms strong baseline methods. The results\ndemonstrate that our approach learns more semantic meaningful and robust\nmulti-modal representations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff08\u6270\u52a8\u62a5\u544a\u5224\u522b\uff09\u7528\u4e8e\u9884\u8bad\u7ec3\u751f\u7269\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5ffd\u89c6\u751f\u7269\u533b\u5b66\u6587\u672c\u590d\u6742\u8bed\u4e49\u7684\u95ee\u9898\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u6587\u672c\u5177\u6709\u590d\u6742\u4e14\u9886\u57df\u7279\u5b9a\u7684\u8bed\u4e49\uff0c\u5e38\u89c1\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u8fd9\u4e00\u70b9\u3002", "method": "\u901a\u8fc7\u6270\u52a8\u62a5\u544a\uff08\u4fdd\u6301\u8bcd\u6c47\u4f46\u7834\u574f\u8bed\u4e49\u7ed3\u6784\uff09\uff0c\u8ba9\u6a21\u578b\u533a\u5206\u539f\u59cb\u62a5\u544a\u4e0e\u6270\u52a8\u62a5\u544a\uff0c\u5e76\u589e\u5f3a\u5bf9\u56fe\u50cf\u548c\u6587\u672c\u7ec6\u7c92\u5ea6\u4fe1\u606f\u7684\u654f\u611f\u6027\u3002", "result": "\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b66\u4e60\u5230\u66f4\u5177\u8bed\u4e49\u610f\u4e49\u548c\u9c81\u68d2\u6027\u7684\u591a\u6a21\u6001\u8868\u793a\u3002", "conclusion": "\u63d0\u51fa\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u751f\u7269\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u590d\u6742\u8bed\u4e49\u573a\u666f\u3002", "keywords": "\u751f\u7269\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u5bf9\u6bd4\u5b66\u4e60\u3001\u8bed\u4e49\u6270\u52a8\u3001\u591a\u6a21\u6001\u8868\u793a"}}
{"id": "2506.00557", "pdf": "https://arxiv.org/pdf/2506.00557", "abs": "https://arxiv.org/abs/2506.00557", "authors": ["Josh Givens", "Song Liu", "Henry W J Reeve"], "title": "Score Matching With Missing Data", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": "Accepted for ICML 2025 Conference Proceedings (Spotlight)", "summary": "Score matching is a vital tool for learning the distribution of data with\napplications across many areas including diffusion processes, energy based\nmodelling, and graphical model estimation. Despite all these applications,\nlittle work explores its use when data is incomplete. We address this by\nadapting score matching (and its major extensions) to work with missing data in\na flexible setting where data can be partially missing over any subset of the\ncoordinates. We provide two separate score matching variations for general use,\nan importance weighting (IW) approach, and a variational approach. We provide\nfinite sample bounds for our IW approach in finite domain settings and show it\nto have especially strong performance in small sample lower dimensional cases.\nComplementing this, we show our variational approach to be strongest in more\ncomplex high-dimensional settings which we demonstrate on graphical model\nestimation tasks on both real and simulated data.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5206\u6570\u5339\u914d\u5728\u4e0d\u5b8c\u6574\u6570\u636e\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u9002\u5e94\u6027\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0d\u540c\u6570\u636e\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5206\u6570\u5339\u914d\u5728\u6570\u636e\u5206\u5e03\u5b66\u4e60\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5728\u4e0d\u5b8c\u6574\u6570\u636e\u4e2d\u7684\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5206\u6570\u5339\u914d\u53d8\u4f53\uff1a\u91cd\u8981\u6027\u52a0\u6743\uff08IW\uff09\u65b9\u6cd5\u548c\u53d8\u5206\u65b9\u6cd5\u3002", "result": "IW\u65b9\u6cd5\u5728\u4f4e\u7ef4\u5c0f\u6837\u672c\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u53d8\u5206\u65b9\u6cd5\u5728\u9ad8\u7ef4\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u5f3a\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u586b\u8865\u4e86\u5206\u6570\u5339\u914d\u5728\u4e0d\u5b8c\u6574\u6570\u636e\u4e2d\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u5206\u6570\u5339\u914d, \u4e0d\u5b8c\u6574\u6570\u636e, \u91cd\u8981\u6027\u52a0\u6743, \u53d8\u5206\u65b9\u6cd5, \u56fe\u5f62\u6a21\u578b\u4f30\u8ba1"}}
{"id": "2506.01955", "pdf": "https://arxiv.org/pdf/2506.01955", "abs": "https://arxiv.org/abs/2506.01955", "authors": ["Grace Luo", "Jonathan Granskog", "Aleksander Holynski", "Trevor Darrell"], "title": "Dual-Process Image Generation", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Prior methods for controlling image generation are limited in their ability\nto be taught new tasks. In contrast, vision-language models, or VLMs, can learn\ntasks in-context and produce the correct outputs for a given input. We propose\na dual-process distillation scheme that allows feed-forward image generators to\nlearn new tasks from deliberative VLMs. Our scheme uses a VLM to rate the\ngenerated images and backpropagates this gradient to update the weights of the\nimage generator. Our general framework enables a wide variety of new control\ntasks through the same text-and-image based interface. We showcase a handful of\napplications of this technique for different types of control signals, such as\ncommonsense inferences and visual prompts. With our method, users can implement\nmultimodal controls for properties such as color palette, line weight, horizon\nposition, and relative depth within a matter of minutes. Project page:\nhttps://dual-process.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8fc7\u7a0b\u84b8\u998f\u65b9\u6848\uff0c\u4f7f\u524d\u9988\u56fe\u50cf\u751f\u6210\u5668\u80fd\u591f\u4ece\u6df1\u601d\u719f\u8651\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e2d\u5b66\u4e60\u65b0\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u6587\u672c\u548c\u56fe\u50cf\u7684\u754c\u9762\u5b9e\u73b0\u591a\u79cd\u63a7\u5236\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u751f\u6210\u63a7\u5236\u65b9\u6cd5\u5728\u5b66\u4e60\u65b0\u4efb\u52a1\u65b9\u9762\u53d7\u9650\uff0c\u800c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u53ef\u4ee5\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u4efb\u52a1\u5e76\u751f\u6210\u6b63\u786e\u8f93\u51fa\u3002", "method": "\u4f7f\u7528\u53cc\u8fc7\u7a0b\u84b8\u998f\u65b9\u6848\uff0c\u901a\u8fc7VLM\u5bf9\u751f\u6210\u56fe\u50cf\u8fdb\u884c\u8bc4\u5206\uff0c\u5e76\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u68af\u5ea6\u66f4\u65b0\u56fe\u50cf\u751f\u6210\u5668\u7684\u6743\u91cd\u3002", "result": "\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u63a7\u5236\u4fe1\u53f7\uff08\u5982\u5e38\u8bc6\u63a8\u7406\u548c\u89c6\u89c9\u63d0\u793a\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u652f\u6301\u5feb\u901f\u5b9e\u73b0\u591a\u6a21\u6001\u63a7\u5236\uff08\u5982\u8c03\u8272\u677f\u3001\u7ebf\u6761\u7c97\u7ec6\u7b49\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u4efb\u52a1\u5b66\u4e60\u80fd\u529b\uff0c\u6269\u5c55\u4e86\u63a7\u5236\u7684\u591a\u6837\u6027\u548c\u6548\u7387\u3002", "keywords": "\u56fe\u50cf\u751f\u6210\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u53cc\u8fc7\u7a0b\u84b8\u998f\u3001\u591a\u6a21\u6001\u63a7\u5236"}}
{"id": "2506.00589", "pdf": "https://arxiv.org/pdf/2506.00589", "abs": "https://arxiv.org/abs/2506.00589", "authors": ["Griffin Tabor", "Tucker Hermans"], "title": "Constrained Stein Variational Gradient Descent for Robot Perception, Planning, and Identification", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Many core problems in robotics can be framed as constrained optimization\nproblems. Often on these problems, the robotic system has uncertainty, or it\nwould be advantageous to identify multiple high quality feasible solutions. To\nenable this, we present two novel frameworks for applying principles of\nconstrained optimization to the new variational inference algorithm Stein\nvariational gradient descent. Our general framework supports multiple types of\nconstrained optimizers and can handle arbitrary constraints. We demonstrate on\na variety of problems that we are able to learn to approximate distributions\nwithout violating constraints. Specifically, we show that we can build\ndistributions of: robot motion plans that exactly avoid collisions, robot arm\njoint angles on the SE(3) manifold with exact table placement constraints, and\nobject poses from point clouds with table placement constraints.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u6846\u67b6\uff0c\u5c06\u7ea6\u675f\u4f18\u5316\u539f\u5219\u5e94\u7528\u4e8eStein\u53d8\u5206\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u591a\u9ad8\u8d28\u91cf\u53ef\u884c\u89e3\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u8bb8\u591a\u6838\u5fc3\u95ee\u9898\u53ef\u4ee5\u5efa\u6a21\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u4e14\u7cfb\u7edf\u901a\u5e38\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u6216\u9700\u8981\u591a\u4e2a\u53ef\u884c\u89e3\u3002", "method": "\u7ed3\u5408\u7ea6\u675f\u4f18\u5316\u548cStein\u53d8\u5206\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u63d0\u51fa\u901a\u7528\u6846\u67b6\u652f\u6301\u591a\u79cd\u7ea6\u675f\u7c7b\u578b\u548c\u4f18\u5316\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u7ea6\u675f\u6761\u4ef6\u4e0b\u8fd1\u4f3c\u5206\u5e03\uff0c\u5982\u907f\u969c\u8fd0\u52a8\u89c4\u5212\u3001SE(3)\u6d41\u5f62\u4e0a\u7684\u673a\u68b0\u81c2\u5173\u8282\u89d2\u5ea6\u548c\u70b9\u4e91\u4e2d\u7684\u7269\u4f53\u4f4d\u59ff\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5728\u591a\u79cd\u5b9e\u9645\u5e94\u7528\u4e2d\u6ee1\u8db3\u7ea6\u675f\u7684\u5206\u5e03\u5b66\u4e60\u80fd\u529b\u3002", "keywords": "constrained optimization, Stein variational gradient descent, robotics, uncertainty, variational inference"}}
{"id": "2506.00659", "pdf": "https://arxiv.org/pdf/2506.00659", "abs": "https://arxiv.org/abs/2506.00659", "authors": ["Marco Di Gennaro", "Mario D'Onghia", "Mario Polino", "Stefano Zanero", "Michele Carminati"], "title": "PackHero: A Scalable Graph-based Approach for Efficient Packer Identification", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Anti-analysis techniques, particularly packing, challenge malware analysts,\nmaking packer identification fundamental. Existing packer identifiers have\nsignificant limitations: signature-based methods lack flexibility and struggle\nagainst dynamic evasion, while Machine Learning approaches require extensive\ntraining data, limiting scalability and adaptability. Consequently, achieving\naccurate and adaptable packer identification remains an open problem. This\npaper presents PackHero, a scalable and efficient methodology for identifying\npackers using a novel static approach. PackHero employs a Graph Matching\nNetwork and clustering to match and group Call Graphs from programs packed with\nknown packers. We evaluate our approach on a public dataset of malware and\nbenign samples packed with various packers, demonstrating its effectiveness and\nscalability across varying sample sizes. PackHero achieves a macro-average\nF1-score of 93.7% with just 10 samples per packer, improving to 98.3% with 100\nsamples. Notably, PackHero requires fewer samples to achieve stable performance\ncompared to other Machine Learning-based tools. Overall, PackHero matches the\nperformance of State-of-the-art signature-based tools, outperforming them in\nhandling Virtualization-based packers such as Themida/Winlicense, with a recall\nof 100%.", "AI": {"tldr": "PackHero\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u9759\u6001\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u6253\u5305\u7a0b\u5e8f\uff0c\u91c7\u7528\u56fe\u5339\u914d\u7f51\u7edc\u548c\u805a\u7c7b\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u6240\u9700\u6837\u672c\u91cf\u3002", "motivation": "\u73b0\u6709\u6253\u5305\u7a0b\u5e8f\u8bc6\u522b\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u7b7e\u540d\u6216\u673a\u5668\u5b66\u4e60\uff09\u5b58\u5728\u7075\u6d3b\u6027\u4e0d\u8db3\u3001\u4f9d\u8d56\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "PackHero\u5229\u7528\u56fe\u5339\u914d\u7f51\u7edc\u548c\u805a\u7c7b\u6280\u672f\uff0c\u5bf9\u5df2\u77e5\u6253\u5305\u7a0b\u5e8f\u7684\u8c03\u7528\u56fe\u8fdb\u884c\u5339\u914d\u548c\u5206\u7ec4\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cPackHero\u5728\u4ec5\u9700\u6bcf\u4e2a\u6253\u5305\u7a0b\u5e8f10\u4e2a\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u5b8f\u5e73\u5747F1\u5206\u6570\u8fbe93.7%\uff0c100\u4e2a\u6837\u672c\u65f6\u63d0\u5347\u81f398.3%\u3002", "conclusion": "PackHero\u5728\u6027\u80fd\u4e0a\u5ab2\u7f8e\u73b0\u6709\u57fa\u4e8e\u7b7e\u540d\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u5904\u7406\u865a\u62df\u5316\u6253\u5305\u7a0b\u5e8f\uff08\u5982Themida/Winlicense\uff09\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u53ec\u56de\u7387\u8fbe100%\u3002", "keywords": "\u6253\u5305\u7a0b\u5e8f\u8bc6\u522b, \u56fe\u5339\u914d\u7f51\u7edc, \u805a\u7c7b, \u9759\u6001\u5206\u6790, \u673a\u5668\u5b66\u4e60"}}
{"id": "2506.00662", "pdf": "https://arxiv.org/pdf/2506.00662", "abs": "https://arxiv.org/abs/2506.00662", "authors": ["Taeho Jo", "Eun Hye Lee", "Alzheimer's Disease Sequencing Project"], "title": "Uncertainty-Aware Genomic Classification of Alzheimer's Disease: A Transformer-Based Ensemble Approach with Monte Carlo Dropout", "categories": ["q-bio.GN", "cs.LG"], "comment": null, "summary": "INTRODUCTION: Alzheimer's disease (AD) is genetically complex, complicating\nrobust classification from genomic data. METHODS: We developed a\ntransformer-based ensemble model (TrUE-Net) using Monte Carlo Dropout for\nuncertainty estimation in AD classification from whole-genome sequencing (WGS).\nWe combined a transformer that preserves single-nucleotide polymorphism (SNP)\nsequence structure with a concurrent random forest using flattened genotypes.\nAn uncertainty threshold separated samples into an uncertain (high-variance)\ngroup and a more certain (low-variance) group. RESULTS: We analyzed 1050\nindividuals, holding out half for testing. Overall accuracy and area under the\nreceiver operating characteristic (ROC) curve (AUC) were 0.6514 and 0.6636,\nrespectively. Excluding the uncertain group improved accuracy from 0.6263 to\n0.7287 (10.24% increase) and F1 from 0.5843 to 0.8205 (23.62% increase).\nDISCUSSION: Monte Carlo Dropout-driven uncertainty helps identify ambiguous\ncases that may require further clinical evaluation, thus improving reliability\nin AD genomic classification.", "AI": {"tldr": "\u5229\u7528Transformer\u96c6\u6210\u6a21\u578b\uff08TrUE-Net\uff09\u548c\u8499\u7279\u5361\u6d1bDropout\u6280\u672f\uff0c\u901a\u8fc7\u5168\u57fa\u56e0\u7ec4\u6d4b\u5e8f\u6570\u636e\u8fdb\u884c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u5206\u7c7b\uff0c\u4e0d\u786e\u5b9a\u6027\u9608\u503c\u533a\u5206\u6837\u672c\uff0c\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u5177\u6709\u9057\u4f20\u590d\u6742\u6027\uff0c\u57fa\u56e0\u7ec4\u6570\u636e\u5206\u7c7b\u56f0\u96be\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eTransformer\u7684\u96c6\u6210\u6a21\u578b\uff08TrUE-Net\uff09\uff0c\u7ed3\u5408\u8499\u7279\u5361\u6d1bDropout\u548c\u968f\u673a\u68ee\u6797\uff0c\u5bf9\u5168\u57fa\u56e0\u7ec4\u6d4b\u5e8f\u6570\u636e\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u6d4b\u8bd5\u96c6\u4e0a\u51c6\u786e\u7387\u4e3a0.6514\uff0cAUC\u4e3a0.6636\uff1b\u6392\u9664\u9ad8\u4e0d\u786e\u5b9a\u6027\u6837\u672c\u540e\uff0c\u51c6\u786e\u7387\u63d0\u534710.24%\uff0cF1\u5206\u6570\u63d0\u534723.62%\u3002", "conclusion": "\u8499\u7279\u5361\u6d1bDropout\u6709\u52a9\u4e8e\u8bc6\u522b\u4e0d\u786e\u5b9a\u6027\u6837\u672c\uff0c\u63d0\u5347AD\u57fa\u56e0\u7ec4\u5206\u7c7b\u7684\u53ef\u9760\u6027\u3002", "keywords": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5, \u57fa\u56e0\u7ec4\u5206\u7c7b, Transformer, \u8499\u7279\u5361\u6d1bDropout, \u968f\u673a\u68ee\u6797"}}
{"id": "2506.01166", "pdf": "https://arxiv.org/pdf/2506.01166", "abs": "https://arxiv.org/abs/2506.01166", "authors": ["Shereef Helal", "Alberto Garcia-Ortiz", "Lennart Bamberg"], "title": "VUSA: Virtually Upscaled Systolic Array Architecture to Exploit Unstructured Sparsity in AI Acceleration", "categories": ["cs.AR", "cs.AI", "cs.LG"], "comment": "Preprint accepted for publication at MOCAST 2025. Submitted for\n  possible publication in IEEE Xplore", "summary": "Leveraging high degrees of unstructured sparsity is a promising approach to\nenhance the efficiency of deep neural network DNN accelerators - particularly\nimportant for emerging Edge-AI applications. We introduce VUSA, a\nsystolic-array architecture that virtually grows based on the present sparsity\nto perform larger matrix multiplications with the same number of physical\nmultiply-accumulate MAC units. The proposed architecture achieves saving by 37%\nand 68% in area and power efficiency, respectively, at the same\npeak-performance, compared to a baseline systolic array architecture in a\ncommercial 16-nm technology. Still, the proposed architecture supports\nacceleration for any DNN with any sparsity - even no sparsity at all. Thus, the\nproposed architecture is application-independent, making it viable for\ngeneral-purpose AI acceleration.", "AI": {"tldr": "VUSA\u662f\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u6027\u7684\u8109\u52a8\u9635\u5217\u67b6\u6784\uff0c\u901a\u8fc7\u865a\u62df\u6269\u5c55\u5b9e\u73b0\u66f4\u5927\u77e9\u9635\u4e58\u6cd5\uff0c\u8282\u770137%\u9762\u79ef\u548c68%\u529f\u8017\uff0c\u9002\u7528\u4e8e\u901a\u7528AI\u52a0\u901f\u3002", "motivation": "\u5229\u7528\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027\u63d0\u5347DNN\u52a0\u901f\u5668\u6548\u7387\uff0c\u5c24\u5176\u9488\u5bf9\u8fb9\u7f18AI\u5e94\u7528\u3002", "method": "\u63d0\u51faVUSA\u67b6\u6784\uff0c\u6839\u636e\u7a00\u758f\u6027\u52a8\u6001\u865a\u62df\u6269\u5c55\u8109\u52a8\u9635\u5217\uff0c\u4fdd\u6301\u7269\u7406MAC\u5355\u5143\u6570\u91cf\u4e0d\u53d8\u3002", "result": "\u572816\u7eb3\u7c73\u5de5\u827a\u4e0b\uff0c\u76f8\u6bd4\u57fa\u7ebf\u67b6\u6784\uff0c\u9762\u79ef\u548c\u529f\u8017\u6548\u7387\u5206\u522b\u63d0\u534737%\u548c68%\uff0c\u540c\u65f6\u652f\u6301\u4efb\u610f\u7a00\u758f\u5ea6\u7684DNN\u3002", "conclusion": "VUSA\u662f\u4e00\u79cd\u5e94\u7528\u65e0\u5173\u7684\u901a\u7528AI\u52a0\u901f\u67b6\u6784\uff0c\u9ad8\u6548\u4e14\u7075\u6d3b\u3002", "keywords": "DNN\u52a0\u901f\u5668,\u8109\u52a8\u9635\u5217,\u7a00\u758f\u6027,\u8fb9\u7f18AI,\u80fd\u6548\u4f18\u5316"}}
{"id": "2506.01182", "pdf": "https://arxiv.org/pdf/2506.01182", "abs": "https://arxiv.org/abs/2506.01182", "authors": ["Muhammad Qasim Ali", "Aditya Sridhar", "Shahbuland Matiana", "Alex Wong", "Mohammad Al-Sharman"], "title": "Humanoid World Models: Open World Foundation Models for Humanoid Robotics", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Humanoid robots have the potential to perform complex tasks in human centered\nenvironments but require robust predictive models to reason about the outcomes\nof their actions. We introduce Humanoid World Models (HWM) a family of\nlightweight open source video based models that forecast future egocentric\nobservations conditioned on actions. We train two types of generative models\nMasked Transformers and FlowMatching on 100 hours of humanoid demonstrations.\nAdditionally we explore architectural variants with different attention\nmechanisms and parameter sharing strategies. Our parameter sharing techniques\nreduce model size by 33 to 53 with minimal impact on performance or visual\nfidelity. HWM is designed to be trained and deployed in practical academic and\nsmall lab settings such as 1 to 2 GPUs.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Humanoid World Models (HWM)\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5f00\u6e90\u89c6\u9891\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u4eba\u5f62\u673a\u5668\u4eba\u5728\u52a8\u4f5c\u6761\u4ef6\u4e0b\u7684\u672a\u6765\u7b2c\u4e00\u89c6\u89d2\u89c2\u5bdf\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u9700\u5f3a\u5927\u9884\u6d4b\u6a21\u578b\u4ee5\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u6267\u884c\u590d\u6742\u4efb\u52a1\uff0cHWM\u65e8\u5728\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bad\u7ec3\u4e24\u79cd\u751f\u6210\u6a21\u578b\uff08Masked Transformers\u548cFlowMatching\uff09\uff0c\u4f7f\u7528100\u5c0f\u65f6\u4eba\u5f62\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\uff0c\u5e76\u63a2\u7d22\u4e0d\u540c\u6ce8\u610f\u529b\u673a\u5236\u548c\u53c2\u6570\u5171\u4eab\u7b56\u7565\u3002", "result": "\u53c2\u6570\u5171\u4eab\u6280\u672f\u5c06\u6a21\u578b\u5927\u5c0f\u51cf\u5c1133%\u81f353%\uff0c\u5bf9\u6027\u80fd\u6216\u89c6\u89c9\u4fdd\u771f\u5ea6\u5f71\u54cd\u5f88\u5c0f\u3002", "conclusion": "HWM\u9002\u5408\u5b66\u672f\u548c\u5c0f\u578b\u5b9e\u9a8c\u5ba4\u73af\u5883\uff0c\u59821\u81f32\u4e2aGPU\u7684\u90e8\u7f72\u3002", "keywords": "\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u9884\u6d4b\u6a21\u578b\uff0c\u751f\u6210\u6a21\u578b\uff0c\u53c2\u6570\u5171\u4eab\uff0c\u8f7b\u91cf\u7ea7"}}
{"id": "2506.00681", "pdf": "https://arxiv.org/pdf/2506.00681", "abs": "https://arxiv.org/abs/2506.00681", "authors": ["Dimitrios Bralios", "Paris Smaragdis", "Jonah Casebeer"], "title": "Learning to Upsample and Upmix Audio in the Latent Domain", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": null, "summary": "Neural audio autoencoders create compact latent representations that preserve\nperceptually important information, serving as the foundation for both modern\naudio compression systems and generation approaches like next-token prediction\nand latent diffusion. Despite their prevalence, most audio processing\noperations, such as spatial and spectral up-sampling, still inefficiently\noperate on raw waveforms or spectral representations rather than directly on\nthese compressed representations. We propose a framework that performs audio\nprocessing operations entirely within an autoencoder's latent space,\neliminating the need to decode to raw audio formats. Our approach dramatically\nsimplifies training by operating solely in the latent domain, with a latent L1\nreconstruction term, augmented by a single latent adversarial discriminator.\nThis contrasts sharply with raw-audio methods that typically require complex\ncombinations of multi-scale losses and discriminators. Through experiments in\nbandwidth extension and mono-to-stereo up-mixing, we demonstrate computational\nefficiency gains of up to 100x while maintaining quality comparable to\npost-processing on raw audio. This work establishes a more efficient paradigm\nfor audio processing pipelines that already incorporate autoencoders, enabling\nsignificantly faster and more resource-efficient workflows across various audio\ntasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u795e\u7ecf\u7f51\u7edc\u97f3\u9891\u81ea\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u76f4\u63a5\u8fdb\u884c\u97f3\u9891\u5904\u7406\u7684\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u89e3\u7801\u4e3a\u539f\u59cb\u97f3\u9891\u683c\u5f0f\u7684\u4f4e\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u5e76\u4fdd\u6301\u4e86\u97f3\u9891\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u7684\u97f3\u9891\u5904\u7406\u65b9\u6cd5\u901a\u5e38\u76f4\u63a5\u5728\u539f\u59cb\u6ce2\u5f62\u6216\u9891\u8c31\u8868\u793a\u4e0a\u64cd\u4f5c\uff0c\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u73b0\u4ee3\u97f3\u9891\u81ea\u7f16\u7801\u5668\u5df2\u80fd\u751f\u6210\u7d27\u51d1\u7684\u6f5c\u5728\u8868\u793a\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u76f4\u63a5\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u64cd\u4f5c\uff0c\u7b80\u5316\u8bad\u7ec3\u8fc7\u7a0b\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5b8c\u5168\u5728\u81ea\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6267\u884c\u97f3\u9891\u5904\u7406\u64cd\u4f5c\uff0c\u65e0\u9700\u89e3\u7801\u4e3a\u539f\u59cb\u97f3\u9891\u3002\u65b9\u6cd5\u91c7\u7528\u6f5c\u5728L1\u91cd\u5efa\u635f\u5931\u548c\u5355\u4e00\u6f5c\u5728\u5bf9\u6297\u5224\u522b\u5668\uff0c\u663e\u8457\u7b80\u5316\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728\u5e26\u5bbd\u6269\u5c55\u548c\u5355\u58f0\u9053\u5230\u7acb\u4f53\u58f0\u4e0a\u6df7\u5b9e\u9a8c\u4e2d\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u4e86\u9ad8\u8fbe100\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u539f\u59cb\u97f3\u9891\u540e\u5904\u7406\u76f8\u5f53\u7684\u8d28\u91cf\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4f7f\u7528\u81ea\u7f16\u7801\u5668\u7684\u97f3\u9891\u5904\u7406\u6d41\u7a0b\u5efa\u7acb\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5404\u79cd\u97f3\u9891\u4efb\u52a1\u7684\u5de5\u4f5c\u6548\u7387\u3002", "keywords": "\u97f3\u9891\u5904\u7406, \u81ea\u7f16\u7801\u5668, \u6f5c\u5728\u7a7a\u95f4, \u8ba1\u7b97\u6548\u7387"}}
{"id": "2506.01196", "pdf": "https://arxiv.org/pdf/2506.01196", "abs": "https://arxiv.org/abs/2506.01196", "authors": ["Ishika Singh", "Ankit Goyal", "Stan Birchfield", "Dieter Fox", "Animesh Garg", "Valts Blukis"], "title": "OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "17 pages", "summary": "We introduce OG-VLA, a novel architecture and learning framework that\ncombines the generalization strengths of Vision Language Action models (VLAs)\nwith the robustness of 3D-aware policies. We address the challenge of mapping\nnatural language instructions and multi-view RGBD observations to quasi-static\nrobot actions. 3D-aware robot policies achieve state-of-the-art performance on\nprecise robot manipulation tasks, but struggle with generalization to unseen\ninstructions, scenes, and objects. On the other hand, VLAs excel at\ngeneralizing across instructions and scenes, but can be sensitive to camera and\nrobot pose variations. We leverage prior knowledge embedded in language and\nvision foundation models to improve generalization of 3D-aware keyframe\npolicies. OG-VLA projects input observations from diverse views into a point\ncloud which is then rendered from canonical orthographic views, ensuring input\nview invariance and consistency between input and output spaces. These\ncanonical views are processed with a vision backbone, a Large Language Model\n(LLM), and an image diffusion model to generate images that encode the next\nposition and orientation of the end-effector on the input scene. Evaluations on\nthe Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization\nto unseen environments, with over 40% relative improvements while maintaining\nrobust performance in seen settings. We also show real-world adaption in 3 to 5\ndemonstrations along with strong generalization. Videos and resources at\nhttps://og-vla.github.io/", "AI": {"tldr": "OG-VLA\u7ed3\u5408\u4e86\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\uff08VLA\uff09\u7684\u6cdb\u5316\u80fd\u529b\u548c3D\u611f\u77e5\u7b56\u7565\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u591a\u89c6\u89d2RGBD\u89c2\u6d4b\u548c\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6620\u5c04\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u672a\u89c1\u573a\u666f\u548c\u6307\u4ee4\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u89e3\u51b33D\u611f\u77e5\u7b56\u7565\u5728\u672a\u89c1\u6307\u4ee4\u3001\u573a\u666f\u548c\u5bf9\u8c61\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4ee5\u53caVLA\u6a21\u578b\u5bf9\u76f8\u673a\u548c\u673a\u5668\u4eba\u59ff\u6001\u53d8\u5316\u654f\u611f\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u70b9\u4e91\u6295\u5f71\u5230\u6b63\u4ea4\u89c6\u56fe\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u89c9\u4e3b\u5e72\u3001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u7f16\u7801\u672b\u7aef\u6267\u884c\u5668\u4f4d\u7f6e\u548c\u65b9\u5411\u7684\u56fe\u50cf\u3002", "result": "\u5728Arnold\u548cColosseum\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOG-VLA\u5b9e\u73b0\u4e8640%\u4ee5\u4e0a\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "OG-VLA\u901a\u8fc7\u7ed3\u54083D\u611f\u77e5\u7b56\u7565\u548cVLA\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "keywords": "OG-VLA, Vision Language Action, 3D-aware policies, generalization, robot manipulation"}}
{"id": "2506.00698", "pdf": "https://arxiv.org/pdf/2506.00698", "abs": "https://arxiv.org/abs/2506.00698", "authors": ["Tianze Yang", "Yucheng Shi", "Mengnan Du", "Xuansheng Wu", "Qiaoyu Tan", "Jin Sun", "Ninghao Liu"], "title": "Concept-Centric Token Interpretation for Vector-Quantized Generative Models", "categories": ["cs.CV", "cs.LG"], "comment": "17 pages, 7 figures", "summary": "Vector-Quantized Generative Models (VQGMs) have emerged as powerful tools for\nimage generation. However, the key component of VQGMs -- the codebook of\ndiscrete tokens -- is still not well understood, e.g., which tokens are\ncritical to generate an image of a certain concept? This paper introduces\nConcept-Oriented Token Explanation (CORTEX), a novel approach for interpreting\nVQGMs by identifying concept-specific token combinations. Our framework employs\ntwo methods: (1) a sample-level explanation method that analyzes token\nimportance scores in individual images, and (2) a codebook-level explanation\nmethod that explores the entire codebook to find globally relevant tokens.\nExperimental results demonstrate CORTEX's efficacy in providing clear\nexplanations of token usage in the generative process, outperforming baselines\nacross multiple pretrained VQGMs. Besides enhancing VQGMs transparency, CORTEX\nis useful in applications such as targeted image editing and shortcut feature\ndetection. Our code is available at https://github.com/YangTianze009/CORTEX.", "AI": {"tldr": "CORTEX\u662f\u4e00\u79cd\u89e3\u91caVQGM\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u6982\u5ff5\u7279\u5b9a\u7684\u6807\u8bb0\u7ec4\u5408\u6765\u589e\u5f3a\u6a21\u578b\u900f\u660e\u5ea6\u3002", "motivation": "\u5c3d\u7ba1VQGM\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u5f3a\u5927\uff0c\u4f46\u5176\u79bb\u6563\u6807\u8bb0\u7684\u7801\u672c\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\uff0cCORTEX\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6837\u672c\u7ea7\u548c\u7801\u672c\u7ea7\u4e24\u79cd\u89e3\u91ca\u65b9\u6cd5\uff0c\u5206\u522b\u5206\u6790\u5355\u4e2a\u56fe\u50cf\u548c\u5168\u5c40\u7801\u672c\u4e2d\u7684\u6807\u8bb0\u91cd\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCORTEX\u5728\u89e3\u91ca\u751f\u6210\u8fc7\u7a0b\u4e2d\u6807\u8bb0\u4f7f\u7528\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u652f\u6301\u76ee\u6807\u56fe\u50cf\u7f16\u8f91\u548c\u5feb\u6377\u7279\u5f81\u68c0\u6d4b\u3002", "conclusion": "CORTEX\u4e0d\u4ec5\u63d0\u9ad8\u4e86VQGM\u7684\u900f\u660e\u5ea6\uff0c\u8fd8\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "keywords": "VQGM, \u6982\u5ff5\u89e3\u91ca, \u6807\u8bb0\u7ec4\u5408, \u900f\u660e\u5ea6, \u56fe\u50cf\u751f\u6210"}}
{"id": "2506.01214", "pdf": "https://arxiv.org/pdf/2506.01214", "abs": "https://arxiv.org/abs/2506.01214", "authors": ["Ali Zia", "Renuka Sharma", "Abdelwahed Khamis", "Xuesong Li", "Muhammad Husnain", "Numan Shafi", "Saeed Anwar", "Sabine Schmoelzl", "Eric Stone", "Lars Petersson", "Vivien Rolland"], "title": "A Review on Coarse to Fine-Grained Animal Action Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This review provides an in-depth exploration of the field of animal action\nrecognition, focusing on coarse-grained (CG) and fine-grained (FG) techniques.\nThe primary aim is to examine the current state of research in animal behaviour\nrecognition and to elucidate the unique challenges associated with recognising\nsubtle animal actions in outdoor environments. These challenges differ\nsignificantly from those encountered in human action recognition due to factors\nsuch as non-rigid body structures, frequent occlusions, and the lack of\nlarge-scale, annotated datasets. The review begins by discussing the evolution\nof human action recognition, a more established field, highlighting how it\nprogressed from broad, coarse actions in controlled settings to the demand for\nfine-grained recognition in dynamic environments. This shift is particularly\nrelevant for animal action recognition, where behavioural variability and\nenvironmental complexity present unique challenges that human-centric models\ncannot fully address. The review then underscores the critical differences\nbetween human and animal action recognition, with an emphasis on high\nintra-species variability, unstructured datasets, and the natural complexity of\nanimal habitats. Techniques like spatio-temporal deep learning frameworks\n(e.g., SlowFast) are evaluated for their effectiveness in animal behaviour\nanalysis, along with the limitations of existing datasets. By assessing the\nstrengths and weaknesses of current methodologies and introducing a\nrecently-published dataset, the review outlines future directions for advancing\nfine-grained action recognition, aiming to improve accuracy and\ngeneralisability in behaviour analysis across species.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u6df1\u5165\u63a2\u8ba8\u4e86\u52a8\u7269\u884c\u4e3a\u8bc6\u522b\u9886\u57df\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u7c97\u7c92\u5ea6\uff08CG\uff09\u548c\u7ec6\u7c92\u5ea6\uff08FG\uff09\u6280\u672f\u3002\u76ee\u6807\u662f\u68b3\u7406\u5f53\u524d\u7814\u7a76\u73b0\u72b6\uff0c\u5e76\u9610\u660e\u6237\u5916\u73af\u5883\u4e2d\u8bc6\u522b\u7ec6\u5fae\u52a8\u7269\u52a8\u4f5c\u7684\u72ec\u7279\u6311\u6218\uff0c\u8fd9\u4e9b\u6311\u6218\u4e0e\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u52a8\u7269\u884c\u4e3a\u8bc6\u522b\u7684\u7814\u7a76\u9762\u4e34\u975e\u521a\u6027\u8eab\u4f53\u7ed3\u6784\u3001\u9891\u7e41\u906e\u6321\u548c\u7f3a\u4e4f\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u7b49\u6311\u6218\u3002\u4e0e\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u76f8\u6bd4\uff0c\u8fd9\u4e00\u95ee\u9898\u66f4\u590d\u6742\uff0c\u9700\u8981\u9488\u5bf9\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7efc\u8ff0\u56de\u987e\u4e86\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u5e76\u8ba8\u8bba\u4e86\u65f6\u7a7a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff08\u5982 SlowFast\uff09\u5728\u52a8\u7269\u884c\u4e3a\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u8bc4\u4f30\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\u3002", "result": "\u901a\u8fc7\u5206\u6790\u5f53\u524d\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u5f15\u5165\u65b0\u53d1\u5e03\u7684\u6570\u636e\u96c6\uff0c\u7efc\u8ff0\u63d0\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u8bc6\u522b\u7684\u65b9\u5411\uff0c\u65e8\u5728\u63d0\u5347\u8de8\u7269\u79cd\u884c\u4e3a\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u52a8\u7269\u52a8\u4f5c\u8bc6\u522b\u7684\u72ec\u7279\u6311\u6218\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u65b9\u6cd5\u548c\u6280\u672f\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5728\u6570\u636e\u91c7\u96c6\u548c\u6a21\u578b\u4f18\u5316\u65b9\u9762\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002", "keywords": "\u52a8\u7269\u884c\u4e3a\u8bc6\u522b\u3001\u7c97\u7c92\u5ea6\u3001\u7ec6\u7c92\u5ea6\u3001\u6df1\u5ea6\u5b66\u4e60\u3001SlowFast\u3001\u6570\u636e\u96c6"}}
{"id": "2506.00721", "pdf": "https://arxiv.org/pdf/2506.00721", "abs": "https://arxiv.org/abs/2506.00721", "authors": ["Tianze Yang", "Tyson Jordan", "Ninghao Liu", "Jin Sun"], "title": "Common Inpainted Objects In-N-Out of Context", "categories": ["cs.CV", "cs.LG"], "comment": "12 pages, 7 figures", "summary": "We present Common Inpainted Objects In-N-Out of Context (COinCO), a novel\ndataset addressing the scarcity of out-of-context examples in existing vision\ndatasets. By systematically replacing objects in COCO images through\ndiffusion-based inpainting, we create 97,722 unique images featuring both\ncontextually coherent and inconsistent scenes, enabling effective context\nlearning. Each inpainted object is meticulously verified and categorized as in-\nor out-of-context through a multimodal large language model assessment. Our\nanalysis reveals significant patterns in semantic priors that influence\ninpainting success across object categories. We demonstrate three key tasks\nenabled by COinCO: (1) training context classifiers that effectively determine\nwhether existing objects belong in their context; (2) a novel\nObjects-from-Context prediction task that determines which new objects\nnaturally belong in given scenes at both instance and clique levels, and (3)\ncontext-enhanced fake detection on state-of-the-art methods without\nfine-tuning. COinCO provides a controlled testbed with contextual variations,\nestablishing a foundation for advancing context-aware visual understanding in\ncomputer vision and image forensics. Our code and data are at:\nhttps://github.com/YangTianze009/COinCO.", "AI": {"tldr": "COinCO\u662f\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6269\u6563\u4fee\u590d\u6280\u672f\u7cfb\u7edf\u6027\u66ff\u6362COCO\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\uff0c\u751f\u621097,722\u5f20\u56fe\u50cf\uff0c\u652f\u6301\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u6570\u636e\u96c6\u4e2d\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u4e0d\u4e00\u81f4\u793a\u4f8b\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6269\u6563\u4fee\u590d\u6280\u672f\u66ff\u6362COCO\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9a8c\u8bc1\u5e76\u5206\u7c7b\u4fee\u590d\u5bf9\u8c61\u3002", "result": "\u63ed\u793a\u4e86\u8bed\u4e49\u5148\u9a8c\u6a21\u5f0f\uff0c\u652f\u6301\u4e0a\u4e0b\u6587\u5206\u7c7b\u5668\u7b49\u4e09\u9879\u5173\u952e\u4efb\u52a1\u3002", "conclusion": "COinCO\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u56fe\u50cf\u53d6\u8bc1\u4e2d\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u7406\u89e3\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "keywords": "\u6570\u636e\u96c6, \u6269\u6563\u4fee\u590d, \u4e0a\u4e0b\u6587\u5b66\u4e60, \u8ba1\u7b97\u673a\u89c6\u89c9, \u56fe\u50cf\u53d6\u8bc1"}}
{"id": "2506.01227", "pdf": "https://arxiv.org/pdf/2506.01227", "abs": "https://arxiv.org/abs/2506.01227", "authors": ["Rakesh Podder", "Turgay Caglar", "Shadaab Kawnain Bashir", "Sarath Sreedharan", "Indrajit Ray", "Indrakshi Ray"], "title": "SPEAR: Security Posture Evaluation using AI Planner-Reasoning on Attack-Connectivity Hypergraphs", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Graph-based frameworks are often used in network hardening to help a cyber\ndefender understand how a network can be attacked and how the best defenses can\nbe deployed. However, incorporating network connectivity parameters in the\nattack graph, reasoning about the attack graph when we do not have access to\ncomplete information, providing system administrator suggestions in an\nunderstandable format, and allowing them to do what-if analysis on various\nscenarios and attacker motives is still missing. We fill this gap by presenting\nSPEAR, a formal framework with tool support for security posture evaluation and\nanalysis that keeps human-in-the-loop. SPEAR uses the causal formalism of AI\nplanning to model vulnerabilities and configurations in a networked system. It\nautomatically converts network configurations and vulnerability descriptions\ninto planning models expressed in the Planning Domain Definition Language\n(PDDL). SPEAR identifies a set of diverse security hardening strategies that\ncan be presented in a manner understandable to the domain expert. These allow\nthe administrator to explore the network hardening solution space in a\nsystematic fashion and help evaluate the impact and compare the different\nsolutions.", "AI": {"tldr": "SPEAR\u662f\u4e00\u4e2a\u57fa\u4e8eAI\u89c4\u5212\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7f51\u7edc\u5b89\u5168\u8bc4\u4f30\u548c\u5206\u6790\uff0c\u652f\u6301\u4eba\u7c7b\u4e13\u5bb6\u53c2\u4e0e\uff0c\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u7f51\u7edc\u52a0\u56fa\u7b56\u7565\u3002", "motivation": "\u5f53\u524d\u7f51\u7edc\u52a0\u56fa\u4e2d\uff0c\u653b\u51fb\u56fe\u7684\u7f51\u7edc\u8fde\u63a5\u53c2\u6570\u3001\u4e0d\u5b8c\u5168\u4fe1\u606f\u7684\u63a8\u7406\u3001\u6613\u61c2\u7684\u7ba1\u7406\u5458\u5efa\u8bae\u53ca\u5047\u8bbe\u5206\u6790\u4ecd\u5b58\u4e0d\u8db3\u3002", "method": "\u5229\u7528AI\u89c4\u5212\u7684\u56e0\u679c\u5f62\u5f0f\u5316\u65b9\u6cd5\u5efa\u6a21\u7f51\u7edc\u6f0f\u6d1e\u4e0e\u914d\u7f6e\uff0c\u81ea\u52a8\u8f6c\u6362\u4e3aPDDL\u8bed\u8a00\u6a21\u578b\uff0c\u751f\u6210\u591a\u6837\u5316\u7b56\u7565\u3002", "result": "\u63d0\u4f9b\u4e86\u53ef\u7406\u89e3\u7684\u7f51\u7edc\u52a0\u56fa\u7b56\u7565\uff0c\u652f\u6301\u7ba1\u7406\u5458\u7cfb\u7edf\u5316\u63a2\u7d22\u89e3\u51b3\u65b9\u6848\u5e76\u8bc4\u4f30\u5f71\u54cd\u3002", "conclusion": "SPEAR\u586b\u8865\u4e86\u7f51\u7edc\u5b89\u5168\u8bc4\u4f30\u548c\u52a0\u56fa\u7b56\u7565\u751f\u6210\u5de5\u5177\u4e2d\u7684\u5173\u952e\u7a7a\u767d\u3002", "keywords": "\u7f51\u7edc\u5b89\u5168, \u653b\u51fb\u56fe, AI\u89c4\u5212, PDDL, SPEAR"}}
{"id": "2506.00725", "pdf": "https://arxiv.org/pdf/2506.00725", "abs": "https://arxiv.org/abs/2506.00725", "authors": ["Mouyang Cheng", "Chu-Liang Fu", "Bowen Yu", "Eunbi Rha", "Abhijatmedhi Chotrattanapituk", "Douglas L Abernathy", "Yongqiang Cheng", "Mingda Li"], "title": "A Foundation Model for Non-Destructive Defect Identification from Vibrational Spectra", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "comment": "14 pages, 5 figures", "summary": "Defects are ubiquitous in solids and strongly influence materials' mechanical\nand functional properties. However, non-destructive characterization and\nquantification of defects, especially when multiple types coexist, remain a\nlong-standing challenge. Here we introduce DefectNet, a foundation machine\nlearning model that predicts the chemical identity and concentration of\nsubstitutional point defects with multiple coexisting elements directly from\nvibrational spectra, specifically phonon density-of-states (PDoS). Trained on\nover 16,000 simulated spectra from 2,000 semiconductors, DefectNet employs a\ntailored attention mechanism to identify up to six distinct defect elements at\nconcentrations ranging from 0.2% to 25%. The model generalizes well to unseen\ncrystals across 56 elements and can be fine-tuned on experimental data.\nValidation using inelastic scattering measurements of SiGe alloys and MgB$_2$\nsuperconductor demonstrates its accuracy and transferability. Our work\nestablishes vibrational spectroscopy as a viable, non-destructive probe for\npoint defect quantification in bulk materials, and highlights the promise of\nfoundation models in data-driven defect engineering.", "AI": {"tldr": "DefectNet\u662f\u4e00\u4e2a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u632f\u52a8\u5149\u8c31\u975e\u7834\u574f\u6027\u5730\u9884\u6d4b\u6750\u6599\u4e2d\u70b9\u7f3a\u9677\u7684\u5316\u5b66\u7279\u6027\u548c\u6d53\u5ea6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5171\u5b58\u5143\u7d20\u3002", "motivation": "\u89e3\u51b3\u6750\u6599\u4e2d\u591a\u79cd\u7f3a\u9677\u5171\u5b58\u65f6\u7684\u975e\u7834\u574f\u6027\u5b9a\u91cf\u8868\u5f81\u96be\u9898\u3002", "method": "\u8bad\u7ec3\u4e8e16,000\u4e2a\u6a21\u62df\u632f\u52a8\u5149\u8c31\uff0c\u91c7\u7528\u5b9a\u5236\u6ce8\u610f\u529b\u673a\u5236\u9884\u6d4b6\u79cd\u7f3a\u9677\u5143\u7d20\u3002", "result": "\u6a21\u578b\u80fd\u6cdb\u5316\u81f356\u79cd\u672a\u89c1\u8fc7\u5143\u7d20\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u7ed3\u679c\u51c6\u786e\u3002", "conclusion": "\u632f\u52a8\u5149\u8c31\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u4e3a\u7f3a\u9677\u5de5\u7a0b\u63d0\u4f9b\u4e86\u53ef\u884c\u5de5\u5177\u3002", "keywords": "DefectNet, \u70b9\u7f3a\u9677, \u632f\u52a8\u5149\u8c31, \u673a\u5668\u5b66\u4e60, \u975e\u7834\u574f\u6027\u8868\u5f81"}}
{"id": "2506.01232", "pdf": "https://arxiv.org/pdf/2506.01232", "abs": "https://arxiv.org/abs/2506.01232", "authors": ["Mojtaba Nayyeri", "Athish A Yogi", "Nadeen Fathallah", "Ratan Bahadur Thapa", "Hans-Michael Tautenhahn", "Anton Schnurpel", "Steffen Staab"], "title": "Retrieval-Augmented Generation of Ontologies from Relational Databases", "categories": ["cs.DB", "cs.AI"], "comment": "Under review", "summary": "Transforming relational databases into knowledge graphs with enriched\nontologies enhances semantic interoperability and unlocks advanced graph-based\nlearning and reasoning over data. However, previous approaches either demand\nsignificant manual effort to derive an ontology from a database schema or\nproduce only a basic ontology. We present RIGOR, Retrieval-augmented Iterative\nGeneration of RDB Ontologies, an LLM-driven approach that turns relational\nschemas into rich OWL ontologies with minimal human effort. RIGOR combines\nthree sources via RAG, the database schema and its documentation, a repository\nof domain ontologies, and a growing core ontology, to prompt a generative LLM\nfor producing successive, provenance-tagged delta ontology fragments. Each\nfragment is refined by a judge-LLM before being merged into the core ontology,\nand the process iterates table-by-table following foreign key constraints until\ncoverage is complete. Applied to real-world databases, our approach outputs\nontologies that score highly on standard quality dimensions such as accuracy,\ncompleteness, conciseness, adaptability, clarity, and consistency, while\nsubstantially reducing manual effort.", "AI": {"tldr": "RIGOR\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\uff0c\u5229\u7528RAG\u6280\u672f\u4ece\u5173\u7cfb\u6570\u636e\u5e93\u751f\u6210\u4e30\u5bcc\u7684OWL\u672c\u4f53\uff0c\u663e\u8457\u51cf\u5c11\u624b\u52a8\u5de5\u4f5c\u91cf\uff0c\u5e76\u5728\u591a\u4e2a\u8d28\u91cf\u7ef4\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u4ece\u5173\u7cfb\u6570\u636e\u5e93\u751f\u6210\u672c\u4f53\u65f6\u9700\u8981\u5927\u91cf\u624b\u52a8\u5de5\u4f5c\u6216\u53ea\u80fd\u751f\u6210\u57fa\u7840\u672c\u4f53\uff0c\u5f71\u54cd\u4e86\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\u548c\u9ad8\u7ea7\u56fe\u5b66\u4e60\u4e0e\u63a8\u7406\u7684\u80fd\u529b\u3002", "method": "RIGOR\u7ed3\u5408\u6570\u636e\u5e93\u6a21\u5f0f\u3001\u6587\u6863\u3001\u57df\u672c\u4f53\u5e93\u548c\u6838\u5fc3\u672c\u4f53\uff0c\u901a\u8fc7LLM\u751f\u6210\u589e\u91cf\u672c\u4f53\u7247\u6bb5\uff0c\u5e76\u7531\u53e6\u4e00LLM\u8fdb\u884c\u4f18\u5316\uff0c\u8fed\u4ee3\u5904\u7406\u76f4\u81f3\u8986\u76d6\u5b8c\u6574\u6570\u636e\u5e93\u3002", "result": "RIGOR\u5728\u51c6\u786e\u6027\u3001\u5b8c\u6574\u6027\u3001\u7b80\u6d01\u6027\u3001\u9002\u5e94\u6027\u3001\u6e05\u6670\u6027\u548c\u4e00\u81f4\u6027\u7b49\u6807\u51c6\u8d28\u91cf\u7ef4\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u624b\u52a8\u5de5\u4f5c\u3002", "conclusion": "RIGOR\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u5c06\u5173\u7cfb\u6570\u636e\u5e93\u8f6c\u5316\u4e3a\u4e30\u5bcc\u7684\u672c\u4f53\uff0c\u4e3a\u77e5\u8bc6\u56fe\u8c31\u7684\u6784\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u77e5\u8bc6\u56fe\u8c31, \u672c\u4f53\u751f\u6210, \u5173\u7cfb\u6570\u636e\u5e93, LLM, RAG"}}
{"id": "2506.01234", "pdf": "https://arxiv.org/pdf/2506.01234", "abs": "https://arxiv.org/abs/2506.01234", "authors": ["Woojin Cho", "Steve Andreas Immanuel", "Junhyuk Heo", "Darongsae Kwon"], "title": "Fourier-Modulated Implicit Neural Representation for Multispectral Satellite Image Compression", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "Accepted to IGARSS 2025 (Oral)", "summary": "Multispectral satellite images play a vital role in agriculture, fisheries,\nand environmental monitoring. However, their high dimensionality, large data\nvolumes, and diverse spatial resolutions across multiple channels pose\nsignificant challenges for data compression and analysis. This paper presents\nImpliSat, a unified framework specifically designed to address these challenges\nthrough efficient compression and reconstruction of multispectral satellite\ndata. ImpliSat leverages Implicit Neural Representations (INR) to model\nsatellite images as continuous functions over coordinate space, capturing fine\nspatial details across varying spatial resolutions. Furthermore, we introduce a\nFourier modulation algorithm that dynamically adjusts to the spectral and\nspatial characteristics of each band, ensuring optimal compression while\npreserving critical image details.", "AI": {"tldr": "ImpliSat\u4f7f\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a(INR)\u548c\u5085\u91cc\u53f6\u8c03\u5236\u7b97\u6cd5,\u9ad8\u6548\u538b\u7f29\u548c\u91cd\u5efa\u591a\u5149\u8c31\u536b\u661f\u6570\u636e,\u89e3\u51b3\u4e86\u9ad8\u7ef4\u5ea6\u548c\u591a\u5206\u8fa8\u7387\u6311\u6218\u3002", "motivation": "\u591a\u5149\u8c31\u536b\u661f\u56fe\u50cf\u7684\u9ad8\u7ef4\u5ea6\u3001\u5927\u6570\u636e\u91cf\u548c\u591a\u5206\u8fa8\u7387\u95ee\u9898\u7ed9\u6570\u636e\u538b\u7f29\u548c\u5206\u6790\u5e26\u6765\u4e86\u5de8\u5927\u6311\u6218\u3002", "method": "\u901a\u8fc7INR\u5c06\u536b\u661f\u56fe\u50cf\u5efa\u6a21\u4e3a\u5750\u6807\u7a7a\u95f4\u7684\u8fde\u7eed\u51fd\u6570,\u5e76\u5f15\u5165\u5085\u91cc\u53f6\u8c03\u5236\u7b97\u6cd5\u52a8\u6001\u8c03\u6574\u5404\u6ce2\u6bb5\u7684\u5149\u8c31\u548c\u7a7a\u95f4\u7279\u5f81\u3002", "result": "\u5b9e\u73b0\u4e86\u591a\u5149\u8c31\u536b\u661f\u6570\u636e\u7684\u9ad8\u6548\u538b\u7f29\u548c\u7cbe\u7ec6\u91cd\u5efa\u3002", "conclusion": "ImpliSat\u4e3a\u591a\u5149\u8c31\u536b\u661f\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u538b\u7f29\u4e0e\u91cd\u5efa\u6846\u67b6\u3002", "keywords": "\u591a\u5149\u8c31\u536b\u661f\u56fe\u50cf,\u6570\u636e\u538b\u7f29,\u9690\u5f0f\u795e\u7ecf\u8868\u793a,\u5085\u91cc\u53f6\u8c03\u5236"}}
{"id": "2506.00800", "pdf": "https://arxiv.org/pdf/2506.00800", "abs": "https://arxiv.org/abs/2506.00800", "authors": ["Daiki Takeuchi", "Binh Thien Nguyen", "Masahiro Yasuda", "Yasunori Ohishi", "Daisuke Niizumi", "Noboru Harada"], "title": "CLAP-ART: Automated Audio Captioning with Semantic-rich Audio Representation Tokenizer", "categories": ["eess.AS", "cs.LG", "cs.SD"], "comment": "Accepted to Interspeech2025", "summary": "Automated Audio Captioning (AAC) aims to describe the semantic contexts of\ngeneral sounds, including acoustic events and scenes, by leveraging effective\nacoustic features. To enhance performance, an AAC method, EnCLAP, employed\ndiscrete tokens from EnCodec as an effective input for fine-tuning a language\nmodel BART. However, EnCodec is designed to reconstruct waveforms rather than\ncapture the semantic contexts of general sounds, which AAC should describe. To\naddress this issue, we propose CLAP-ART, an AAC method that utilizes\n``semantic-rich and discrete'' tokens as input. CLAP-ART computes semantic-rich\ndiscrete tokens from pre-trained audio representations through vector\nquantization. We experimentally confirmed that CLAP-ART outperforms baseline\nEnCLAP on two AAC benchmarks, indicating that semantic-rich discrete tokens\nderived from semantically rich AR are beneficial for AAC.", "AI": {"tldr": "CLAP-ART\u662f\u4e00\u79cd\u81ea\u52a8\u5316\u97f3\u9891\u63cf\u8ff0\uff08AAC\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u8bed\u4e49\u4e30\u5bcc\u7684\u79bb\u6563\u4ee4\u724c\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684EnCLAP\u65b9\u6cd5\u4f7f\u7528EnCodec\u7684\u79bb\u6563\u4ee4\u724c\uff0c\u4f46\u5176\u8bbe\u8ba1\u76ee\u6807\u4e3a\u6ce2\u5f62\u91cd\u5efa\uff0c\u800c\u975e\u6355\u6349\u97f3\u9891\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "method": "CLAP-ART\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u4ece\u9884\u8bad\u7ec3\u7684\u97f3\u9891\u8868\u793a\u4e2d\u63d0\u53d6\u8bed\u4e49\u4e30\u5bcc\u7684\u79bb\u6563\u4ee4\u724c\uff0c\u7528\u4e8e\u5fae\u8c03BART\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCLAP-ART\u5728\u4e24\u4e2aAAC\u57fa\u51c6\u4e0a\u4f18\u4e8eEnCLAP\u57fa\u7ebf\u3002", "conclusion": "\u8bed\u4e49\u4e30\u5bcc\u7684\u79bb\u6563\u4ee4\u724c\u5bf9AAC\u4efb\u52a1\u6709\u76ca\u3002", "keywords": "\u81ea\u52a8\u5316\u97f3\u9891\u63cf\u8ff0, \u79bb\u6563\u4ee4\u724c, \u8bed\u4e49\u4e30\u5bcc, \u5411\u91cf\u91cf\u5316"}}
{"id": "2506.00813", "pdf": "https://arxiv.org/pdf/2506.00813", "abs": "https://arxiv.org/abs/2506.00813", "authors": ["Jiaqi Luo", "Yuan Yuan", "Shixin Xu"], "title": "TIME: TabPFN-Integrated Multimodal Engine for Robust Tabular-Image Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Tabular-image multimodal learning, which integrates structured tabular data\nwith imaging data, holds great promise for a variety of tasks, especially in\nmedical applications. Yet, two key challenges remain: (1) the lack of a\nstandardized, pretrained representation for tabular data, as is commonly\navailable in vision and language domains; and (2) the difficulty of handling\nmissing values in the tabular modality, which are common in real-world medical\ndatasets. To address these issues, we propose the TabPFN-Integrated Multimodal\nEngine (TIME), a novel multimodal framework that builds on the recently\nintroduced tabular foundation model, TabPFN. TIME leverages TabPFN as a frozen\ntabular encoder to generate robust, strong embeddings that are naturally\nresilient to missing data, and combines them with image features from\npretrained vision backbones. We explore a range of fusion strategies and\ntabular encoders, and evaluate our approach on both natural and medical\ndatasets. Extensive experiments demonstrate that TIME consistently outperforms\ncompetitive baselines across both complete and incomplete tabular inputs,\nunderscoring its practical value in real-world multimodal learning scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTIME\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408TabPFN\u4f5c\u4e3a\u8868\u683c\u7f16\u7801\u5668\u548c\u9884\u8bad\u7ec3\u89c6\u89c9\u4e3b\u5e72\uff0c\u89e3\u51b3\u4e86\u8868\u683c\u6570\u636e\u6807\u51c6\u5316\u8868\u793a\u7f3a\u5931\u548c\u7f3a\u5931\u503c\u5904\u7406\u7684\u6311\u6218\u3002", "motivation": "\u8868\u683c\u56fe\u50cf\u591a\u6a21\u6001\u5b66\u4e60\u5728\u533b\u5b66\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8868\u683c\u6570\u636e\u8868\u793a\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u8868\u683c\u6a21\u6001\u4e2d\u7684\u7f3a\u5931\u503c\u3002", "method": "\u5229\u7528TabPFN\u4f5c\u4e3a\u51bb\u7ed3\u8868\u683c\u7f16\u7801\u5668\u751f\u6210\u7a33\u5065\u5d4c\u5165\uff0c\u4e0e\u9884\u8bad\u7ec3\u89c6\u89c9\u4e3b\u5e72\u63d0\u53d6\u7684\u56fe\u50cf\u7279\u5f81\u7ed3\u5408\uff0c\u63a2\u7d22\u591a\u79cd\u878d\u5408\u7b56\u7565\u548c\u8868\u683c\u7f16\u7801\u5668\u3002", "result": "\u5728\u81ea\u7136\u548c\u533b\u5b66\u6570\u636e\u96c6\u4e0a\uff0cTIME\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u4e0d\u5b8c\u6574\u8868\u683c\u8f93\u5165\u65f6\u6548\u679c\u663e\u8457\u3002", "conclusion": "TIME\u5728\u771f\u5b9e\u591a\u6a21\u6001\u5b66\u4e60\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u5c24\u5176\u662f\u5728\u8868\u683c\u6570\u636e\u7f3a\u5931\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "keywords": "\u591a\u6a21\u6001\u5b66\u4e60\u3001\u8868\u683c\u6570\u636e\u3001\u56fe\u50cf\u6570\u636e\u3001\u533b\u5b66\u5e94\u7528\u3001\u7f3a\u5931\u503c\u5904\u7406"}}
{"id": "2506.01242", "pdf": "https://arxiv.org/pdf/2506.01242", "abs": "https://arxiv.org/abs/2506.01242", "authors": ["Brian Hu Zhang", "Tuomas Sandholm"], "title": "General search techniques without common knowledge for imperfect-information games, and application to superhuman Fog of War chess", "categories": ["cs.GT", "cs.AI"], "comment": null, "summary": "Since the advent of AI, games have served as progress benchmarks. Meanwhile,\nimperfect-information variants of chess have existed for over a century,\npresent extreme challenges, and have been the focus of significant AI research.\nBeyond calculation needed in regular chess, they require reasoning about\ninformation gathering, the opponent's knowledge, signaling, etc. The most\npopular variant, Fog of War (FoW) chess (aka. dark chess) is a recognized\nchallenge problem in AI after superhuman performance was reached in no-limit\nTexas hold'em poker. We present Obscuro, the first superhuman AI for FoW chess.\nIt introduces advances to search in imperfect-information games, enabling\nstrong, scalable reasoning. Experiments against the prior state-of-the-art AI\nand human players -- including the world's best -- show that Obscuro is\nsignificantly stronger. FoW chess is the largest (by amount of imperfect\ninformation) turn-based game in which superhuman performance has been achieved\nand the largest game in which imperfect-information search has been\nsuccessfully applied.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u6b3e\u540d\u4e3aObscuro\u7684\u8d85\u4ebaAI\uff0c\u7528\u4e8e\u89e3\u51b3\u96fe\u6218\u8c61\u68cb\uff08FoW chess\uff09\u8fd9\u7c7b\u4e0d\u5b8c\u5168\u4fe1\u606f\u535a\u5f08\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5176\u5f3a\u5927\u7684\u641c\u7d22\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "AI\u5728\u4e0d\u5b8c\u5168\u4fe1\u606f\u535a\u5f08\u4e2d\u8868\u73b0\u8d85\u8d8a\u4eba\u7c7b\u7684\u9700\u6c42\u63a8\u52a8\u4e86\u5bf9\u96fe\u6218\u8c61\u68cb\u7684\u7814\u7a76\uff0c\u8fd9\u662f\u7ee7\u5fb7\u5dde\u6251\u514b\u540e\u7684\u53c8\u4e00\u91cd\u8981\u6311\u6218\u3002", "method": "Obscuro\u901a\u8fc7\u6539\u8fdb\u4e0d\u5b8c\u5168\u4fe1\u606f\u535a\u5f08\u4e2d\u7684\u641c\u7d22\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cObscuro\u5728\u5bf9\u6297\u73b0\u6709\u6700\u4f73AI\u548c\u4eba\u7c7b\u9876\u5c16\u73a9\u5bb6\u65f6\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u5bf9\u624b\uff0c\u6210\u4e3a\u9996\u4e2a\u5728\u96fe\u6218\u8c61\u68cb\u4e2d\u8d85\u8d8a\u4eba\u7c7b\u7684AI\u3002", "conclusion": "Obscuro\u6210\u529f\u5c55\u793a\u4e86\u5728\u5927\u89c4\u6a21\u4e0d\u5b8c\u5168\u4fe1\u606f\u535a\u5f08\u4e2d\u5b9e\u73b0\u8d85\u4eba\u6027\u80fd\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "AI, \u4e0d\u5b8c\u5168\u4fe1\u606f\u535a\u5f08, \u96fe\u6218\u8c61\u68cb, \u641c\u7d22\u7b97\u6cd5, \u8d85\u4eba\u6027\u80fd"}}
{"id": "2506.00818", "pdf": "https://arxiv.org/pdf/2506.00818", "abs": "https://arxiv.org/abs/2506.00818", "authors": ["Sinian Zhang", "Kaicheng Zhang", "Ziping Xu", "Tianxi Cai", "Doudou Zhou"], "title": "Generalized Linear Markov Decision Process", "categories": ["stat.ML", "cs.LG"], "comment": "34 pages, 9 figures", "summary": "The linear Markov Decision Process (MDP) framework offers a principled\nfoundation for reinforcement learning (RL) with strong theoretical guarantees\nand sample efficiency. However, its restrictive assumption-that both transition\ndynamics and reward functions are linear in the same feature space-limits its\napplicability in real-world domains, where rewards often exhibit nonlinear or\ndiscrete structures. Motivated by applications such as healthcare and\ne-commerce, where data is scarce and reward signals can be binary or\ncount-valued, we propose the Generalized Linear MDP (GLMDP) framework-an\nextension of the linear MDP framework-that models rewards using generalized\nlinear models (GLMs) while maintaining linear transition dynamics. We establish\nthe Bellman completeness of GLMDPs with respect to a new function class that\naccommodates nonlinear rewards and develop two offline RL algorithms:\nGeneralized Pessimistic Value Iteration (GPEVI) and a semi-supervised variant\n(SS-GPEVI) that utilizes both labeled and unlabeled trajectories. Our\nalgorithms achieve theoretical guarantees on policy suboptimality and\ndemonstrate improved sample efficiency in settings where reward labels are\nexpensive or limited.", "AI": {"tldr": "\u63d0\u51fa\u5e7f\u4e49\u7ebf\u6027MDP\u6846\u67b6\uff0c\u6269\u5c55\u7ebf\u6027MDP\u4ee5\u5904\u7406\u975e\u7ebf\u6027\u5956\u52b1\uff0c\u5f00\u53d1\u4e24\u79cd\u79bb\u7ebfRL\u7b97\u6cd5\u5e76\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7ebf\u6027MDP\u5047\u8bbe\u9650\u5236\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5956\u52b1\u4fe1\u53f7\u4e3a\u975e\u7ebf\u6027\u6216\u79bb\u6563\u7684\u60c5\u51b5\u3002", "method": "\u5e7f\u4e49\u7ebf\u6027MDP\u6846\u67b6\uff0c\u4f7f\u7528GLM\u5efa\u6a21\u5956\u52b1\uff0c\u5f00\u53d1GPEVI\u548cSS-GPEVI\u7b97\u6cd5\u3002", "result": "\u7b97\u6cd5\u5728\u5956\u52b1\u6807\u7b7e\u6709\u9650\u65f6\u8868\u73b0\u66f4\u9ad8\u6548\uff0c\u7406\u8bba\u4fdd\u8bc1\u7b56\u7565\u6b21\u4f18\u6027\u3002", "conclusion": "GLMDP\u6846\u67b6\u9002\u7528\u4e8e\u5956\u52b1\u4fe1\u53f7\u590d\u6742\u7684\u573a\u666f\uff0c\u63d0\u5347\u6837\u672c\u6548\u7387\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u5e7f\u4e49\u7ebf\u6027\u6a21\u578b, \u79bb\u7ebf\u5b66\u4e60, \u6837\u672c\u6548\u7387"}}
{"id": "2506.01247", "pdf": "https://arxiv.org/pdf/2506.01247", "abs": "https://arxiv.org/abs/2506.01247", "authors": ["Gerasimos Chatzoudis", "Zhuowei Li", "Gemma E. Moran", "Hao Wang", "Dimitris N. Metaxas"], "title": "Visual Sparse Steering: Improving Zero-shot Image Classification with Sparsity Guided Steering Vectors", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Steering vision foundation models at inference time without retraining or\naccess to large labeled datasets is a desirable yet challenging objective,\nparticularly in dynamic or resource-constrained settings. In this paper, we\nintroduce Visual Sparse Steering (VS2), a lightweight, test-time method that\nguides vision models using steering vectors derived from sparse features\nlearned by top-$k$ Sparse Autoencoders without requiring contrastive data.\nSpecifically, VS2 surpasses zero-shot CLIP by 4.12% on CIFAR-100, 1.08% on\nCUB-200, and 1.84% on Tiny-ImageNet. We further propose VS2++, a\nretrieval-augmented variant that selectively amplifies relevant sparse features\nusing pseudo-labeled neighbors at inference time. With oracle positive/negative\nsets, VS2++ achieves absolute top-1 gains over CLIP zero-shot of up to 21.44%\non CIFAR-100, 7.08% on CUB-200, and 20.47% on Tiny-ImageNet. Interestingly, VS2\nand VS2++ raise per-class accuracy by up to 25% and 38%, respectively, showing\nthat sparse steering benefits specific classes by disambiguating visually or\ntaxonomically proximate categories rather than providing a uniform boost.\nFinally, to better align the sparse features learned through the SAE\nreconstruction task with those relevant for downstream performance, we propose\nPrototype-Aligned Sparse Steering (PASS). By incorporating a\nprototype-alignment loss during SAE training, using labels only during training\nwhile remaining fully test-time unsupervised, PASS consistently, though\nmodestly, outperforms VS2, achieving a 6.12% gain over VS2 only on CIFAR-100\nwith ViT-B/32.", "AI": {"tldr": "VS2\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u63a8\u7406\u65f6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u7279\u5f81\u5f15\u5bfc\u89c6\u89c9\u6a21\u578b\uff0c\u65e0\u9700\u5bf9\u6bd4\u6570\u636e\u3002VS2++\u662f\u5176\u68c0\u7d22\u589e\u5f3a\u7248\u672c\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002PASS\u901a\u8fc7\u539f\u578b\u5bf9\u9f50\u7a00\u758f\u7279\u5f81\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86VS2\u3002", "motivation": "\u5728\u52a8\u6001\u6216\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u4e2d\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u8bbf\u95ee\u5927\u91cf\u6807\u6ce8\u6570\u636e\u5373\u53ef\u5f15\u5bfc\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u5177\u6311\u6218\u6027\u7684\u76ee\u6807\u3002", "method": "VS2\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7684\u7a00\u758f\u7279\u5f81\u751f\u6210\u5f15\u5bfc\u5411\u91cf\uff1bVS2++\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u673a\u5236\u9009\u62e9\u6027\u653e\u5927\u76f8\u5173\u7279\u5f81\uff1bPASS\u901a\u8fc7\u539f\u578b\u5bf9\u9f50\u4f18\u5316\u7a00\u758f\u7279\u5f81\u3002", "result": "VS2\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8fc7CLIP\u96f6\u6837\u672c\u65b9\u6cd5\uff1bVS2++\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\uff1bPASS\u5728CIFAR-100\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u7a00\u758f\u7279\u5f81\u5f15\u5bfc\u5728\u7279\u5b9a\u7c7b\u522b\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u539f\u578b\u5bf9\u9f50\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u65b9\u6cd5\u3002", "keywords": "\u89c6\u89c9\u57fa\u7840\u6a21\u578b, \u7a00\u758f\u81ea\u7f16\u7801\u5668, \u63a8\u7406\u65f6\u5f15\u5bfc, \u68c0\u7d22\u589e\u5f3a, \u539f\u578b\u5bf9\u9f50"}}
{"id": "2506.00828", "pdf": "https://arxiv.org/pdf/2506.00828", "abs": "https://arxiv.org/abs/2506.00828", "authors": ["Chao Wang", "Yue Zheng", "Yujing Zhang", "Yan Feng", "Zhe Wang", "Xiaowei Shi", "An You", "Yu Chen"], "title": "Breaker: Removing Shortcut Cues with User Clustering for Single-slot Recommendation System", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "In a single-slot recommendation system, users are only exposed to one item at\na time, and the system cannot collect user feedback on multiple items\nsimultaneously. Therefore, only pointwise modeling solutions can be adopted,\nfocusing solely on modeling the likelihood of clicks or conversions for items\nby users to learn user-item preferences, without the ability to capture the\nranking information among different items directly. However, since user-side\ninformation is often much more abundant than item-side information, the model\ncan quickly learn the differences in user intrinsic tendencies, which are\nindependent of the items they are exposed to. This can cause these intrinsic\ntendencies to become a shortcut bias for the model, leading to insufficient\nmining of the most concerned user-item preferences. To solve this challenge, we\nintroduce the Breaker model. Breaker integrates an auxiliary task of user\nrepresentation clustering with a multi-tower structure for cluster-specific\npreference modeling. By clustering user representations, we ensure that users\nwithin each cluster exhibit similar characteristics, which increases the\ncomplexity of the pointwise recommendation task on the user side. This forces\nthe multi-tower structure with cluster-driven parameter learning to better\nmodel user-item preferences, ultimately eliminating shortcut biases related to\nuser intrinsic tendencies. In terms of training, we propose a delayed parameter\nupdate mechanism to enhance training stability and convergence, enabling\nend-to-end joint training of the auxiliary clustering and classification tasks.\nBoth offline and online experiments demonstrate that our method surpasses the\nbaselines. It has already been deployed and is actively serving tens of\nmillions of users daily on Meituan, one of the most popular e-commerce\nplatforms for services.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBreaker\u6a21\u578b\uff0c\u901a\u8fc7\u7528\u6237\u8868\u793a\u805a\u7c7b\u548c\u591a\u5854\u7ed3\u6784\u6d88\u9664\u63a8\u8350\u7cfb\u7edf\u4e2d\u7528\u6237\u5185\u5728\u503e\u5411\u7684\u5feb\u6377\u504f\u5dee\uff0c\u63d0\u5347\u7528\u6237-\u7269\u54c1\u504f\u597d\u7684\u5efa\u6a21\u6548\u679c\u3002", "motivation": "\u5728\u5355\u69fd\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u7528\u6237\u53ea\u80fd\u4e00\u6b21\u63a5\u89e6\u4e00\u4e2a\u7269\u54c1\uff0c\u7cfb\u7edf\u65e0\u6cd5\u540c\u65f6\u6536\u96c6\u591a\u7269\u54c1\u53cd\u9988\uff0c\u5bfc\u81f4\u6a21\u578b\u5bb9\u6613\u4f9d\u8d56\u7528\u6237\u5185\u5728\u503e\u5411\u800c\u4ea7\u751f\u5feb\u6377\u504f\u5dee\u3002", "method": "Breaker\u6a21\u578b\u7ed3\u5408\u7528\u6237\u8868\u793a\u805a\u7c7b\u7684\u8f85\u52a9\u4efb\u52a1\u548c\u591a\u5854\u7ed3\u6784\uff0c\u8fdb\u884c\u805a\u7c7b\u7279\u5b9a\u7684\u504f\u597d\u5efa\u6a21\uff0c\u5e76\u91c7\u7528\u5ef6\u8fdf\u53c2\u6570\u66f4\u65b0\u673a\u5236\u4ee5\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5df2\u5728\u7f8e\u56e2\u5e73\u53f0\u4e0a\u6210\u529f\u90e8\u7f72\uff0c\u6bcf\u65e5\u670d\u52a1\u6570\u5343\u4e07\u7528\u6237\u3002", "conclusion": "Breaker\u6a21\u578b\u6709\u6548\u6d88\u9664\u7528\u6237\u5185\u5728\u503e\u5411\u7684\u5feb\u6377\u504f\u5dee\uff0c\u63d0\u5347\u4e86\u5355\u69fd\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "keywords": "\u63a8\u8350\u7cfb\u7edf, \u5feb\u6377\u504f\u5dee, \u7528\u6237\u8868\u793a\u805a\u7c7b, \u591a\u5854\u7ed3\u6784, \u5ef6\u8fdf\u53c2\u6570\u66f4\u65b0"}}
{"id": "2506.00839", "pdf": "https://arxiv.org/pdf/2506.00839", "abs": "https://arxiv.org/abs/2506.00839", "authors": ["Pedro Figueiredo", "Qihao He", "Nima Khademi Kalantari"], "title": "Neural Path Guiding with Distribution Factorization", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "11 pages, 11 figures. Accepted to EGSR 2025", "summary": "In this paper, we present a neural path guiding method to aid with Monte\nCarlo (MC) integration in rendering. Existing neural methods utilize\ndistribution representations that are either fast or expressive, but not both.\nWe propose a simple, but effective, representation that is sufficiently\nexpressive and reasonably fast. Specifically, we break down the 2D distribution\nover the directional domain into two 1D probability distribution functions\n(PDF). We propose to model each 1D PDF using a neural network that estimates\nthe distribution at a set of discrete coordinates. The PDF at an arbitrary\nlocation can then be evaluated and sampled through interpolation. To train the\nnetwork, we maximize the similarity of the learned and target distributions. To\nreduce the variance of the gradient during optimizations and estimate the\nnormalization factor, we propose to cache the incoming radiance using an\nadditional network. Through extensive experiments, we demonstrate that our\napproach is better than the existing methods, particularly in challenging\nscenes with complex light transport.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u8def\u5f84\u5f15\u5bfc\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u8499\u7279\u5361\u6d1b\u79ef\u5206\u5728\u6e32\u67d3\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u5206\u89e32D\u5206\u5e03\u4e3a\u4e24\u4e2a1D\u6982\u7387\u5206\u5e03\u51fd\u6570\uff0c\u5e76\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u9ad8\u6548\u4e14\u8868\u8fbe\u529b\u5f3a\u7684\u5206\u5e03\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u65b9\u6cd5\u5728\u5206\u5e03\u8868\u793a\u4e0a\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u5feb\u901f\u548c\u8868\u8fbe\u529b\u5f3a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u9ad8\u6548\u8ba1\u7b97\u53c8\u80fd\u51c6\u786e\u8868\u8fbe\u7684\u65b9\u6cd5\u3002", "method": "\u5c062D\u65b9\u5411\u57df\u5206\u5e03\u5206\u89e3\u4e3a\u4e24\u4e2a1D\u6982\u7387\u5206\u5e03\u51fd\u6570\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u79bb\u6563\u70b9\u4e0a\u7684\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u63d2\u503c\u5b9e\u73b0\u4efb\u610f\u4f4d\u7f6e\u7684\u8bc4\u4f30\u548c\u91c7\u6837\u3002\u8bad\u7ec3\u65f6\u6700\u5927\u5316\u5b66\u4e60\u5206\u5e03\u4e0e\u76ee\u6807\u5206\u5e03\u7684\u76f8\u4f3c\u6027\uff0c\u5e76\u4f7f\u7528\u989d\u5916\u7f51\u7edc\u7f13\u5b58\u5165\u5c04\u8f90\u5c04\u5ea6\u4ee5\u51cf\u5c11\u68af\u5ea6\u65b9\u5dee\u548c\u4f30\u8ba1\u5f52\u4e00\u5316\u56e0\u5b50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u5149\u4f20\u8f93\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7b80\u5355\u76841D\u5206\u5e03\u5206\u89e3\u548c\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u8868\u8fbe\u529b\u5f3a\u7684\u5206\u5e03\u8868\u793a\uff0c\u63d0\u5347\u4e86\u6e32\u67d3\u6548\u679c\u3002", "keywords": "\u795e\u7ecf\u8def\u5f84\u5f15\u5bfc, \u8499\u7279\u5361\u6d1b\u79ef\u5206, \u6e32\u67d3, 1D\u6982\u7387\u5206\u5e03\u51fd\u6570, \u795e\u7ecf\u7f51\u7edc"}}
{"id": "2506.01274", "pdf": "https://arxiv.org/pdf/2506.01274", "abs": "https://arxiv.org/abs/2506.01274", "authors": ["Hosu Lee", "Junho Kim", "Hyunjun Kim", "Yong Man Ro"], "title": "ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent progress in Large Multi-modal Models (LMMs) has enabled effective\nvision-language reasoning, yet the ability to understand video content remains\nconstrained by suboptimal frame selection strategies. Existing approaches often\nrely on static heuristics or external retrieval modules to feed frame\ninformation into video-LLMs, which may fail to provide the query-relevant\ninformation. In this work, we introduce ReFoCUS (Reinforcement-guided Frame\nOptimization for Contextual UnderStanding), a novel frame-level policy\noptimization framework that shifts the optimization target from textual\nresponses to visual input selection. ReFoCUS learns a frame selection policy\nvia reinforcement learning, using reward signals derived from a reference LMM\nto reflect the model's intrinsic preferences for frames that best support\ntemporally grounded responses. To efficiently explore the large combinatorial\nframe space, we employ an autoregressive, conditional selection architecture\nthat ensures temporal coherence while reducing complexity. Our approach does\nnot require explicit supervision at the frame-level and consistently improves\nreasoning performance across multiple video QA benchmarks, highlighting the\nbenefits of aligning frame selection with model-internal utility.", "AI": {"tldr": "\u63d0\u51fa\u4e86ReFoCUS\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u89c6\u9891\u5e27\u9009\u62e9\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u89c6\u9891\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u542f\u53d1\u5f0f\u6216\u5916\u90e8\u68c0\u7d22\u6a21\u5757\uff0c\u53ef\u80fd\u5bfc\u81f4\u67e5\u8be2\u76f8\u5173\u4fe1\u606f\u7f3a\u5931\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u8003LMM\u7684\u5956\u52b1\u4fe1\u53f7\u5b66\u4e60\u5e27\u9009\u62e9\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u81ea\u56de\u5f52\u6761\u4ef6\u9009\u62e9\u7ed3\u6784\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u6a21\u578b\u5185\u90e8\u6548\u7528\u5bf9\u9f50\u7684\u5e27\u9009\u62e9\u7b56\u7565\u6709\u52a9\u4e8e\u63d0\u5347\u89c6\u9891\u7406\u89e3\u80fd\u529b\u3002", "keywords": "\u591a\u6a21\u6001\u6a21\u578b\uff0c\u89c6\u9891\u7406\u89e3\uff0c\u5f3a\u5316\u5b66\u4e60\uff0c\u5e27\u9009\u62e9"}}
{"id": "2506.00866", "pdf": "https://arxiv.org/pdf/2506.00866", "abs": "https://arxiv.org/abs/2506.00866", "authors": ["Meilin Wang", "Wei Huang", "Mingming Gong", "Zheng Zhang"], "title": "Projection Pursuit Density Ratio Estimation", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Density ratio estimation (DRE) is a paramount task in machine learning, for\nits broad applications across multiple domains, such as covariate shift\nadaptation, causal inference, independence tests and beyond. Parametric methods\nfor estimating the density ratio possibly lead to biased results if models are\nmisspecified, while conventional non-parametric methods suffer from the curse\nof dimensionality when the dimension of data is large. To address these\nchallenges, in this paper, we propose a novel approach for DRE based on the\nprojection pursuit (PP) approximation. The proposed method leverages PP to\nmitigate the impact of high dimensionality while retaining the model\nflexibility needed for the accuracy of DRE. We establish the consistency and\nthe convergence rate for the proposed estimator. Experimental results\ndemonstrate that our proposed method outperforms existing alternatives in\nvarious applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6295\u5f71\u8ffd\u8e2a\uff08PP\uff09\u7684\u65b0\u578b\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u9ad8\u7ef4\u6570\u636e\u4e0b\u7684\u53c2\u6570\u548c\u975e\u53c2\u6570\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u7ef4\u6570\u636e\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u6295\u5f71\u8ffd\u8e2a\uff08PP\uff09\u8fd1\u4f3c\u6cd5\uff0c\u901a\u8fc7\u964d\u7ef4\u4fdd\u7559\u6a21\u578b\u7075\u6d3b\u6027\uff0c\u63d0\u9ad8\u4f30\u8ba1\u51c6\u786e\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u4f30\u8ba1\u91cf\u7684\u4e00\u81f4\u6027\u548c\u6536\u655b\u901f\u5ea6\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PP\u65b9\u6cd5\u5728\u9ad8\u7ef4\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u517c\u5177\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002", "keywords": "\u5bc6\u5ea6\u6bd4\u4f30\u8ba1, \u6295\u5f71\u8ffd\u8e2a, \u9ad8\u7ef4\u6570\u636e, \u4e00\u81f4\u6027, \u6536\u655b\u901f\u5ea6"}}
{"id": "2506.01307", "pdf": "https://arxiv.org/pdf/2506.01307", "abs": "https://arxiv.org/abs/2506.01307", "authors": ["Youze Wang", "Wenbo Hu", "Yinpeng Dong", "Jing Liu", "Hanwang Zhang", "Richang Hong"], "title": "Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have evolved into Multimodal Large Language\nModels (MLLMs), significantly enhancing their capabilities by integrating\nvisual information and other types, thus aligning more closely with the nature\nof human intelligence, which processes a variety of data forms beyond just\ntext. Despite advancements, the undesirable generation of these models remains\na critical concern, particularly due to vulnerabilities exposed by text-based\njailbreak attacks, which have represented a significant threat by challenging\nexisting safety protocols. Motivated by the unique security risks posed by the\nintegration of new and old modalities for MLLMs, we propose a unified\nmultimodal universal jailbreak attack framework that leverages iterative\nimage-text interactions and transfer-based strategy to generate a universal\nadversarial suffix and image. Our work not only highlights the interaction of\nimage-text modalities can be used as a critical vulnerability but also\nvalidates that multimodal universal jailbreak attacks can bring higher-quality\nundesirable generations across different MLLMs. We evaluate the undesirable\ncontext generation of MLLMs like LLaVA, Yi-VL, MiniGPT4, MiniGPT-v2, and\nInstructBLIP, and reveal significant multimodal safety alignment issues,\nhighlighting the inadequacy of current safety mechanisms against sophisticated\nmultimodal attacks. This study underscores the urgent need for robust safety\nmeasures in MLLMs, advocating for a comprehensive review and enhancement of\nsecurity protocols to mitigate potential risks associated with multimodal\ncapabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u6a21\u6001\u901a\u7528\u8d8a\u72f1\u653b\u51fb\u6846\u67b6\uff0c\u5229\u7528\u56fe\u50cf-\u6587\u672c\u8fed\u4ee3\u4ea4\u4e92\u548c\u8fc1\u79fb\u7b56\u7565\u751f\u6210\u901a\u7528\u5bf9\u6297\u540e\u7f00\u548c\u56fe\u50cf\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u6027\u5bf9\u9f50\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u6574\u5408\u4e86\u89c6\u89c9\u548c\u5176\u4ed6\u7c7b\u578b\u6570\u636e\uff0c\u4f46\u6587\u672c\u8d8a\u72f1\u653b\u51fb\u66b4\u9732\u4e86\u5176\u5b89\u5168\u9690\u60a3\uff0c\u9700\u8981\u7814\u7a76\u65b0\u578b\u5b89\u5168\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf-\u6587\u672c\u4ea4\u4e92\u548c\u8fc1\u79fb\u7b56\u7565\u7684\u901a\u7528\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb\u6846\u67b6\uff0c\u8bc4\u4f30\u5176\u5728\u591a\u79cdMLLM\u4e2d\u7684\u6548\u679c\u3002", "result": "\u653b\u51fb\u6846\u67b6\u80fd\u8de8\u4e0d\u540cMLLM\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u4e0d\u53ef\u63a7\u8f93\u51fa\uff0c\u9a8c\u8bc1\u4e86\u73b0\u6709\u5b89\u5168\u673a\u5236\u5bf9\u591a\u6a21\u6001\u653b\u51fb\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u52a0\u5f3a\u5bf9MLLMs\u7684\u5b89\u5168\u534f\u8bae\u5ba1\u67e5\u548c\u589e\u5f3a\uff0c\u4ee5\u5e94\u5bf9\u591a\u6a21\u6001\u80fd\u529b\u5e26\u6765\u7684\u6f5c\u5728\u98ce\u9669\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, \u8d8a\u72f1\u653b\u51fb, \u5b89\u5168\u6027\u5bf9\u9f50, \u56fe\u50cf-\u6587\u672c\u4ea4\u4e92, \u5bf9\u6297\u751f\u6210"}}
{"id": "2506.00904", "pdf": "https://arxiv.org/pdf/2506.00904", "abs": "https://arxiv.org/abs/2506.00904", "authors": ["Xander K\u00fcpers", "Jeroen Klein Brinke", "Rob Bemthuis", "Ozlem Durmaz Incel"], "title": "Towards Edge-Based Idle State Detection in Construction Machinery Using Surveillance Cameras", "categories": ["cs.CV", "cs.LG"], "comment": "18 pages, 6 figures, 3 tables; to appear in Intelligent Systems and\n  Applications, Lecture Notes in Networks and Systems (LNNS), Springer, 2025.\n  Part of the 11th Intelligent Systems Conference (IntelliSys 2025), 28-29\n  August 2025, Amsterdam, The Netherlands", "summary": "The construction industry faces significant challenges in optimizing\nequipment utilization, as underused machinery leads to increased operational\ncosts and project delays. Accurate and timely monitoring of equipment activity\nis therefore key to identifying idle periods and improving overall efficiency.\nThis paper presents the Edge-IMI framework for detecting idle construction\nmachinery, specifically designed for integration with surveillance camera\nsystems. The proposed solution consists of three components: object detection,\ntracking, and idle state identification, which are tailored for execution on\nresource-constrained, CPU-based edge computing devices. The performance of\nEdge-IMI is evaluated using a combined dataset derived from the ACID and MOCS\nbenchmarks. Experimental results confirm that the object detector achieves an\nF1 score of 71.75%, indicating robust real-world detection capabilities. The\nlogistic regression-based idle identification module reliably distinguishes\nbetween active and idle machinery with minimal false positives. Integrating all\nthree modules, Edge-IMI enables efficient on-site inference, reducing reliance\non high-bandwidth cloud services and costly hardware accelerators. We also\nevaluate the performance of object detection models on Raspberry Pi 5 and an\nIntel NUC platforms, as example edge computing platforms. We assess the\nfeasibility of real-time processing and the impact of model optimization\ntechniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEdge-IMI\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u8fb9\u7f18\u8ba1\u7b97\u8bbe\u5907\u4f18\u5316\u5efa\u7b51\u673a\u68b0\u7684\u7a7a\u95f2\u72b6\u6001\u68c0\u6d4b\uff0c\u4ee5\u63d0\u9ad8\u8bbe\u5907\u5229\u7528\u7387\u3002", "motivation": "\u5efa\u7b51\u884c\u4e1a\u4e2d\u673a\u68b0\u8bbe\u5907\u7684\u4f4e\u6548\u4f7f\u7528\u5bfc\u81f4\u8fd0\u8425\u6210\u672c\u589e\u52a0\u548c\u9879\u76ee\u5ef6\u8bef\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5b9e\u65f6\u76d1\u63a7\u8bbe\u5907\u6d3b\u52a8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "Edge-IMI\u6846\u67b6\u7531\u5bf9\u8c61\u68c0\u6d4b\u3001\u8ddf\u8e2a\u548c\u7a7a\u95f2\u72b6\u6001\u8bc6\u522b\u4e09\u4e2a\u6a21\u5757\u7ec4\u6210\uff0c\u4e13\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8ba1\u7b97\u8bbe\u5907\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u76ee\u6807\u68c0\u6d4b\u6a21\u5757F1\u5f97\u5206\u4e3a71.75%\uff0c\u7a7a\u95f2\u8bc6\u522b\u6a21\u5757\u80fd\u53ef\u9760\u533a\u5206\u6d3b\u8dc3\u4e0e\u7a7a\u95f2\u8bbe\u5907\u3002Edge-IMI\u5728Raspberry Pi 5\u548cIntel NUC\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u5b9e\u65f6\u5904\u7406\u7684\u53ef\u884c\u6027\u3002", "conclusion": "Edge-IMI\u901a\u8fc7\u8fb9\u7f18\u8ba1\u7b97\u5b9e\u73b0\u9ad8\u6548\u63a8\u65ad\uff0c\u51cf\u5c11\u4e86\u5bf9\u9ad8\u5e26\u5bbd\u4e91\u670d\u52a1\u548c\u6602\u8d35\u786c\u4ef6\u52a0\u901f\u5668\u7684\u4f9d\u8d56\u3002", "keywords": "\u8fb9\u7f18\u8ba1\u7b97, \u5efa\u7b51\u673a\u68b0, \u7a7a\u95f2\u68c0\u6d4b, \u76ee\u6807\u68c0\u6d4b, \u5b9e\u65f6\u5904\u7406"}}
{"id": "2506.00925", "pdf": "https://arxiv.org/pdf/2506.00925", "abs": "https://arxiv.org/abs/2506.00925", "authors": ["Mengdi Liu", "Xiaoxue Cheng", "Zhangyang Gao", "Hong Chang", "Cheng Tan", "Shiguang Shan", "Xilin Chen"], "title": "ProtInvTree: Deliberate Protein Inverse Folding with Reward-guided Tree Search", "categories": ["q-bio.BM", "cs.CV", "cs.LG"], "comment": null, "summary": "Designing protein sequences that fold into a target 3D structure, known as\nprotein inverse folding, is a fundamental challenge in protein engineering.\nWhile recent deep learning methods have achieved impressive performance by\nrecovering native sequences, they often overlook the one-to-many nature of the\nproblem: multiple diverse sequences can fold into the same structure. This\nmotivates the need for a generative model capable of designing diverse\nsequences while preserving structural consistency. To address this trade-off,\nwe introduce ProtInvTree, the first reward-guided tree-search framework for\nprotein inverse folding. ProtInvTree reformulates sequence generation as a\ndeliberate, step-wise decision-making process, enabling the exploration of\nmultiple design paths and exploitation of promising candidates through\nself-evaluation, lookahead, and backtracking. We propose a two-stage\nfocus-and-grounding action mechanism that decouples position selection and\nresidue generation. To efficiently evaluate intermediate states, we introduce a\njumpy denoising strategy that avoids full rollouts. Built upon pretrained\nprotein language models, ProtInvTree supports flexible test-time scaling by\nexpanding the search depth and breadth without retraining. Empirically,\nProtInvTree outperforms state-of-the-art baselines across multiple benchmarks,\ngenerating structurally consistent yet diverse sequences, including those far\nfrom the native ground truth.", "AI": {"tldr": "ProtInvTree\u662f\u4e00\u79cd\u5956\u52b1\u5f15\u5bfc\u7684\u6811\u641c\u7d22\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u86cb\u767d\u8d28\u53cd\u5411\u6298\u53e0\u95ee\u9898\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u540c\u65f6\u751f\u6210\u591a\u6837\u5316\u7684\u5e8f\u5217\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u86cb\u767d\u8d28\u53cd\u5411\u6298\u53e0\u4efb\u52a1\u4e2d\u4e3b\u8981\u5173\u6ce8\u6062\u590d\u539f\u59cb\u5e8f\u5217\uff0c\u5ffd\u7565\u4e86\u4e00\u5bf9\u591a\u7684\u591a\u6837\u6027\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u5e8f\u5217\u4e14\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u6a21\u578b\u3002", "method": "ProtInvTree\u91c7\u7528\u4e24\u9636\u6bb5\u7126\u70b9\u4e0e\u63a5\u5730\u52a8\u4f5c\u673a\u5236\uff0c\u89e3\u8026\u4f4d\u7f6e\u9009\u62e9\u548c\u6b8b\u57fa\u751f\u6210\uff0c\u5e76\u5f15\u5165\u8df3\u8dc3\u53bb\u566a\u7b56\u7565\u9ad8\u6548\u8bc4\u4f30\u4e2d\u95f4\u72b6\u6001\u3002\u5b83\u8fd8\u652f\u6301\u7075\u6d3b\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cProtInvTree\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u7ed3\u6784\u4e00\u81f4\u4e14\u591a\u6837\u5316\u7684\u5e8f\u5217\uff0c\u5305\u62ec\u8fdc\u79bb\u539f\u59cb\u771f\u5b9e\u7684\u5e8f\u5217\u3002", "conclusion": "ProtInvTree\u4e3a\u86cb\u767d\u8d28\u53cd\u5411\u6298\u53e0\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u5e8f\u5217\u751f\u6210\u4e2d\u7684\u591a\u6837\u6027\u95ee\u9898\u3002", "keywords": "\u86cb\u767d\u8d28\u53cd\u5411\u6298\u53e0,\u6811\u641c\u7d22\u6846\u67b6,\u5e8f\u5217\u591a\u6837\u6027,\u7ed3\u6784\u4e00\u81f4\u6027,\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.00933", "pdf": "https://arxiv.org/pdf/2506.00933", "abs": "https://arxiv.org/abs/2506.00933", "authors": ["Zhihao Xu", "Saisai Ding", "Zhikun Zhang", "Xiangjun Wang"], "title": "Reconstruction and Prediction of Volterra Integral Equations Driven by Gaussian Noise", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Integral equations are widely used in fields such as applied modeling,\nmedical imaging, and system identification, providing a powerful framework for\nsolving deterministic problems. While parameter identification for differential\nequations has been extensively studied, the focus on integral equations,\nparticularly stochastic Volterra integral equations, remains limited. This\nresearch addresses the parameter identification problem, also known as the\nequation reconstruction problem, in Volterra integral equations driven by\nGaussian noise. We propose an improved deep neural networks framework for\nestimating unknown parameters in the drift term of these equations. The network\nrepresents the primary variables and their integrals, enhancing parameter\nestimation accuracy by incorporating inter-output relationships into the loss\nfunction. Additionally, the framework extends beyond parameter identification\nto predict the system's behavior outside the integration interval. Prediction\naccuracy is validated by comparing predicted and true trajectories using a 95%\nconfidence interval. Numerical experiments demonstrate the effectiveness of the\nproposed deep neural networks framework in both parameter identification and\nprediction tasks, showing robust performance under varying noise levels and\nproviding accurate solutions for modeling stochastic systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u5e26\u6709\u9ad8\u65af\u566a\u58f0\u7684Volterra\u79ef\u5206\u65b9\u7a0b\u4e2d\u7684\u672a\u77e5\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u635f\u5931\u51fd\u6570\u589e\u5f3a\u53c2\u6570\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u540c\u65f6\u80fd\u591f\u9884\u6d4b\u7cfb\u7edf\u5728\u79ef\u5206\u533a\u95f4\u5916\u7684\u884c\u4e3a\u3002", "motivation": "\u79ef\u5206\u65b9\u7a0b\u5728\u5e94\u7528\u5efa\u6a21\u3001\u533b\u5b66\u6210\u50cf\u548c\u7cfb\u7edf\u8bc6\u522b\u7b49\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5bf9\u968f\u673aVolterra\u79ef\u5206\u65b9\u7a0b\u7684\u53c2\u6570\u8bc6\u522b\u7814\u7a76\u8f83\u5c11\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u8868\u793a\u4e3b\u53d8\u91cf\u53ca\u5176\u79ef\u5206\uff0c\u5e76\u5c06\u8f93\u51fa\u95f4\u5173\u7cfb\u7eb3\u5165\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u63d0\u9ad8\u53c2\u6570\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u53c2\u6570\u5e76\u9884\u6d4b\u7cfb\u7edf\u884c\u4e3a\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u5728\u53c2\u6570\u8bc6\u522b\u548c\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u968f\u673a\u7cfb\u7edf\u5efa\u6a21\u3002", "keywords": "\u79ef\u5206\u65b9\u7a0b, \u53c2\u6570\u8bc6\u522b, \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc, Volterra\u65b9\u7a0b, \u9ad8\u65af\u566a\u58f0"}}
{"id": "2506.01333", "pdf": "https://arxiv.org/pdf/2506.01333", "abs": "https://arxiv.org/abs/2506.01333", "authors": ["Manish Bhatt", "Vineeth Sai Narajala", "Idan Habler"], "title": "ETDI: Mitigating Tool Squatting and Rug Pull Attacks in Model Context Protocol (MCP) by using OAuth-Enhanced Tool Definitions and Policy-Based Access Control", "categories": ["cs.CR", "cs.AI", "cs.ET"], "comment": "11 Pages, 10 figures, Github links in introduction", "summary": "The Model Context Protocol (MCP) plays a crucial role in extending the\ncapabilities of Large Language Models (LLMs) by enabling integration with\nexternal tools and data sources. However, the standard MCP specification\npresents significant security vulnerabilities, notably Tool Poisoning and Rug\nPull attacks. This paper introduces the Enhanced Tool Definition Interface\n(ETDI), a security extension designed to fortify MCP. ETDI incorporates\ncryptographic identity verification, immutable versioned tool definitions, and\nexplicit permission management, often leveraging OAuth 2.0. We further propose\nextending MCP with fine-grained, policy-based access control, where tool\ncapabilities are dynamically evaluated against explicit policies using a\ndedicated policy engine, considering runtime context beyond static OAuth\nscopes. This layered approach aims to establish a more secure, trustworthy, and\ncontrollable ecosystem for AI applications interacting with LLMs and external\ntools.", "AI": {"tldr": "ETDI\u4f5c\u4e3a\u5b89\u5168\u6269\u5c55\u589e\u5f3aMCP\uff0c\u901a\u8fc7\u52a0\u5bc6\u9a8c\u8bc1\u548c\u6743\u9650\u7ba1\u7406\u9632\u6b62\u5de5\u5177\u4e2d\u6bd2\u548c\u8dd1\u8def\u653b\u51fb\u3002", "motivation": "\u6807\u51c6MCP\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff08\u5982\u5de5\u5177\u4e2d\u6bd2\u548c\u8dd1\u8def\u653b\u51fb\uff09\uff0c\u9700\u8981\u66f4\u5b89\u5168\u53ef\u63a7\u7684AI\u751f\u6001\u7cfb\u7edf\u3002", "method": "\u5f15\u5165ETDI\uff0c\u7ed3\u5408\u52a0\u5bc6\u9a8c\u8bc1\u3001\u4e0d\u53ef\u53d8\u5de5\u5177\u5b9a\u4e49\u548cOAuth 2.0\u6743\u9650\u7ba1\u7406\uff0c\u5e76\u6269\u5c55\u57fa\u4e8e\u7b56\u7565\u7684\u8bbf\u95ee\u63a7\u5236\u3002", "result": "\u5efa\u7acb\u4e86\u66f4\u5b89\u5168\u3001\u53ef\u4fe1\u4e14\u53ef\u63a7\u7684AI\u5e94\u7528\u751f\u6001\u7cfb\u7edf\u3002", "conclusion": "ETDI\u6709\u6548\u63d0\u5347MCP\u5b89\u5168\u6027\uff0c\u9002\u7528\u4e8e\u9700\u8981\u4e0e\u5916\u90e8\u5de5\u5177\u96c6\u6210\u7684LLM\u5e94\u7528\u3002", "keywords": "MCP, ETDI, \u5b89\u5168\u6269\u5c55, OAuth 2.0, \u7b56\u7565\u63a7\u5236"}}
{"id": "2506.01075", "pdf": "https://arxiv.org/pdf/2506.01075", "abs": "https://arxiv.org/abs/2506.01075", "authors": ["Mohsen Heidari", "Roni Khardon"], "title": "Learning DNF through Generalized Fourier Representations", "categories": ["cs.DS", "cs.IT", "cs.LG", "math.IT"], "comment": "54 pages", "summary": "The Fourier representation for the uniform distribution over the Boolean cube\nhas found numerous applications in algorithms and complexity analysis. Notably,\nin learning theory, learnability of Disjunctive Normal Form (DNF) under uniform\nas well as product distributions has been established through such\nrepresentations. This paper makes five main contributions. First, it introduces\na generalized Fourier expansion that can be used with any distribution $D$\nthrough the representation of the distribution as a Bayesian network (BN).\nSecond, it shows that the main algorithmic tools for learning with the Fourier\nrepresentation, that use membership queries to approximate functions by\nrecovering their heavy Fourier coefficients, can be used with slight\nmodifications with the generalized expansion. These results hold for any\ndistribution. Third, it analyzes the $L_1$ spectral norm of conjunctions under\nthe new expansion, showing that it is bounded for a class of distributions\nwhich can be represented by difference bounded tree BN, where a parent node in\nthe BN representation can change the conditional expectation of a child node by\nat most $\\alpha<0.5$. Lower bounds are presented to show that such constraints\nare necessary. The fourth contribution uses these results to show the\nlearnability of DNF with membership queries under difference bounded tree BN.\nThe final contribution is to develop an algorithm for learning\ndifference-bounded tree BN distributions, thus extending the DNF learnability\nresult to cases where the distribution is not known in advance.", "AI": {"tldr": "\u5e7f\u4e49\u5085\u91cc\u53f6\u5c55\u5f00\u5f15\u5165\u8d1d\u53f6\u65af\u7f51\u7edc\u4ee5\u9002\u7528\u4e8e\u4efb\u610f\u5206\u5e03\uff0c\u8bc1\u660e\u5b66\u4e60\u5de5\u5177\u53ef\u6cdb\u5316\uff0c\u5206\u6790\u7279\u5b9a\u5206\u5e03\u7684L1\u8c31\u8303\u6570\u4e0a\u4e0b\u754c\uff0c\u5c55\u793aDNF\u53ef\u5b66\u4e60\u6027\uff0c\u5e76\u63d0\u51fa\u672a\u77e5\u5206\u5e03\u65f6\u7684\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u6269\u5c55\u5085\u91cc\u53f6\u8868\u793a\u5728\u5e03\u5c14\u7acb\u65b9\u4f53\u4e0a\u7684\u5e94\u7528\uff0c\u4ee5\u9002\u5e94\u66f4\u5e7f\u6cdb\u5206\u5e03\uff0c\u63d0\u5347DNF\u5728\u590d\u6742\u5206\u5e03\u4e0b\u7684\u53ef\u5b66\u4e60\u6027\u3002", "method": "1.\u5f15\u5165\u57fa\u4e8e\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u5e7f\u4e49\u5085\u91cc\u53f6\u5c55\u5f00\uff1b2.\u9a8c\u8bc1\u539f\u6709\u5b66\u4e60\u5de5\u5177\u7684\u9002\u7528\u6027\uff1b3.\u5206\u6790\u7279\u5b9aBN\u7684L1\u8c31\u8303\u6570\uff1b4.\u8bc1\u660eDNF\u53ef\u5b66\u4e60\u6027\uff1b5.\u5f00\u53d1\u672a\u77e5\u5206\u5e03\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u5e7f\u4e49\u5c55\u5f00\u9002\u7528\u4efb\u610f\u5206\u5e03\uff1b\u5b66\u4e60\u5de5\u5177\u53ef\u6cdb\u5316\uff1b\u7279\u5b9aBN\u4e0bL1\u8c31\u8303\u6570\u6709\u754c\uff1bDNF\u5728\u5dee\u754cBN\u4e0b\u53ef\u5b66\u4e60\uff1b\u6210\u529f\u5f00\u53d1\u672a\u77e5\u5206\u5e03\u5b66\u4e60\u7b97\u6cd5\u3002", "conclusion": "\u5e7f\u4e49\u5085\u91cc\u53f6\u5c55\u5f00\u6709\u6548\u6269\u5c55\u5e94\u7528\u8303\u56f4\uff0c\u4e3aDNF\u5b66\u4e60\u63d0\u4f9b\u65b0\u5de5\u5177\uff0c\u7b97\u6cd5\u5b9e\u7528\u6027\u9ad8\u3002", "keywords": "\u5085\u91cc\u53f6\u5c55\u5f00\u3001\u8d1d\u53f6\u65af\u7f51\u7edc\u3001DNF\u5b66\u4e60\u3001\u5dee\u754cBN\u3001L1\u8c31\u8303\u6570"}}
{"id": "2506.01083", "pdf": "https://arxiv.org/pdf/2506.01083", "abs": "https://arxiv.org/abs/2506.01083", "authors": ["Zheng Zhao"], "title": "Generative diffusion posterior sampling for informative likelihoods", "categories": ["stat.ML", "cs.LG", "cs.SY", "eess.SY"], "comment": "Commemorative issue", "summary": "Sequential Monte Carlo (SMC) methods have recently shown successful results\nfor conditional sampling of generative diffusion models. In this paper we\npropose a new diffusion posterior SMC sampler achieving improved statistical\nefficiencies, particularly under outlier conditions or highly informative\nlikelihoods. The key idea is to construct an observation path that correlates\nwith the diffusion model and to design the sampler to leverage this correlation\nfor more efficient sampling. Empirical results conclude the efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6269\u6563\u540e\u9a8cSMC\u91c7\u6837\u5668\uff0c\u901a\u8fc7\u6784\u5efa\u4e0e\u6269\u6563\u6a21\u578b\u76f8\u5173\u7684\u89c2\u6d4b\u8def\u5f84\uff0c\u63d0\u9ad8\u4e86\u91c7\u6837\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u79bb\u7fa4\u503c\u6216\u9ad8\u4fe1\u606f\u4f3c\u7136\u6761\u4ef6\u4e0b\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u5e8f\u8d2f\u8499\u7279\u5361\u7f57\uff08SMC\uff09\u65b9\u6cd5\u5728\u751f\u6210\u6269\u6563\u6a21\u578b\u7684\u6761\u4ef6\u91c7\u6837\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u79bb\u7fa4\u503c\u6216\u9ad8\u4fe1\u606f\u4f3c\u7136\u6761\u4ef6\u4e0b\u6548\u7387\u8f83\u4f4e\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u6269\u6563\u540e\u9a8cSMC\u91c7\u6837\u5668\uff0c\u6784\u5efa\u4e0e\u6269\u6563\u6a21\u578b\u76f8\u5173\u7684\u89c2\u6d4b\u8def\u5f84\uff0c\u5e76\u5229\u7528\u8fd9\u79cd\u76f8\u5173\u6027\u63d0\u9ad8\u91c7\u6837\u6548\u7387\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u79bb\u7fa4\u503c\u6216\u9ad8\u4fe1\u606f\u4f3c\u7136\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u7edf\u8ba1\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u8def\u5f84\u76f8\u5173\u6027\uff0c\u65b0\u7684SMC\u91c7\u6837\u5668\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u66f4\u9ad8\u6548\u7684\u91c7\u6837\u80fd\u529b\u3002", "keywords": "\u5e8f\u8d2f\u8499\u7279\u5361\u7f57\uff08SMC\uff09\u3001\u6269\u6563\u6a21\u578b\u3001\u6761\u4ef6\u91c7\u6837\u3001\u7edf\u8ba1\u6548\u7387\u3001\u79bb\u7fa4\u503c"}}
{"id": "2506.01380", "pdf": "https://arxiv.org/pdf/2506.01380", "abs": "https://arxiv.org/abs/2506.01380", "authors": ["Xinle Cheng", "Tianyu He", "Jiayi Xu", "Junliang Guo", "Di He", "Jiang Bian"], "title": "Playing with Transformer at 30+ FPS via Next-Frame Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autoregressive video models offer distinct advantages over bidirectional\ndiffusion models in creating interactive video content and supporting streaming\napplications with arbitrary duration. In this work, we present Next-Frame\nDiffusion (NFD), an autoregressive diffusion transformer that incorporates\nblock-wise causal attention, enabling iterative sampling and efficient\ninference via parallel token generation within each frame. Nonetheless,\nachieving real-time video generation remains a significant challenge for such\nmodels, primarily due to the high computational cost associated with diffusion\nsampling and the hardware inefficiencies inherent to autoregressive generation.\nTo address this, we introduce two innovations: (1) We extend consistency\ndistillation to the video domain and adapt it specifically for video models,\nenabling efficient inference with few sampling steps; (2) To fully leverage\nparallel computation, motivated by the observation that adjacent frames often\nshare the identical action input, we propose speculative sampling. In this\napproach, the model generates next few frames using current action input, and\ndiscard speculatively generated frames if the input action differs. Experiments\non a large-scale action-conditioned video generation benchmark demonstrate that\nNFD beats autoregressive baselines in terms of both visual quality and sampling\nefficiency. We, for the first time, achieves autoregressive video generation at\nover 30 Frames Per Second (FPS) on an A100 GPU using a 310M model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faNext-Frame Diffusion (NFD)\uff0c\u4e00\u79cd\u81ea\u56de\u5f52\u6269\u6563Transformer\uff0c\u901a\u8fc7\u5757\u7ea7\u56e0\u679c\u6ce8\u610f\u529b\u548c\u4e24\u79cd\u521b\u65b0\u6280\u672f\uff08\u4e00\u81f4\u6027\u84b8\u998f\u548c\u63a8\u6d4b\u91c7\u6837\uff09\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u9996\u6b21\u5728A100 GPU\u4e0a\u4ee530 FPS\u5b9e\u73b0\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u5728\u4ea4\u4e92\u5f0f\u89c6\u9891\u5185\u5bb9\u521b\u5efa\u548c\u6d41\u5a92\u4f53\u5e94\u7528\u4e2d\uff0c\u81ea\u56de\u5f52\u89c6\u9891\u6a21\u578b\u76f8\u6bd4\u53cc\u5411\u6269\u6563\u6a21\u578b\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5b9e\u65f6\u751f\u6210\u4ecd\u9762\u4e34\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u786c\u4ef6\u6548\u7387\u4f4e\u7684\u6311\u6218\u3002", "method": "1. \u5f15\u5165\u4e00\u81f4\u6027\u84b8\u998f\u6280\u672f\u5230\u89c6\u9891\u9886\u57df\uff0c\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\uff1b2. \u63d0\u51fa\u63a8\u6d4b\u91c7\u6837\uff0c\u5229\u7528\u76f8\u90bb\u5e27\u52a8\u4f5c\u8f93\u5165\u76f8\u4f3c\u6027\u5e76\u884c\u751f\u6210\u591a\u5e27\u3002", "result": "NFD\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u91c7\u6837\u6548\u7387\u4e0a\u8d85\u8d8a\u57fa\u7ebf\uff0c\u9996\u6b21\u5728A100 GPU\u4e0a\u4ee530 FPS\u5b9e\u73b0\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u3002", "conclusion": "NFD\u901a\u8fc7\u6280\u672f\u521b\u65b0\u663e\u8457\u63d0\u5347\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u7684\u6548\u7387\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u53ef\u80fd\u3002", "keywords": "\u81ea\u56de\u5f52\u6a21\u578b, \u6269\u6563\u6a21\u578b, \u89c6\u9891\u751f\u6210, \u4e00\u81f4\u6027\u84b8\u998f, \u63a8\u6d4b\u91c7\u6837"}}
{"id": "2506.01143", "pdf": "https://arxiv.org/pdf/2506.01143", "abs": "https://arxiv.org/abs/2506.01143", "authors": ["Hannes Matt", "Dominik St\u00f6ger"], "title": "Linear regression with overparameterized linear neural networks: Tight upper and lower bounds for implicit $\\ell^1$-regularization", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT", "math.OC"], "comment": null, "summary": "Modern machine learning models are often trained in a setting where the\nnumber of parameters exceeds the number of training samples. To understand the\nimplicit bias of gradient descent in such overparameterized models, prior work\nhas studied diagonal linear neural networks in the regression setting. These\nstudies have shown that, when initialized with small weights, gradient descent\ntends to favor solutions with minimal $\\ell^1$-norm - an effect known as\nimplicit regularization. In this paper, we investigate implicit regularization\nin diagonal linear neural networks of depth $D\\ge 2$ for overparameterized\nlinear regression problems. We focus on analyzing the approximation error\nbetween the limit point of gradient flow trajectories and the solution to the\n$\\ell^1$-minimization problem. By deriving tight upper and lower bounds on the\napproximation error, we precisely characterize how the approximation error\ndepends on the scale of initialization $\\alpha$. Our results reveal a\nqualitative difference between depths: for $D \\ge 3$, the error decreases\nlinearly with $\\alpha$, whereas for $D=2$, it decreases at rate\n$\\alpha^{1-\\varrho}$, where the parameter $\\varrho \\in [0,1)$ can be explicitly\ncharacterized. Interestingly, this parameter is closely linked to so-called\nnull space property constants studied in the sparse recovery literature. We\ndemonstrate the asymptotic tightness of our bounds through explicit examples.\nNumerical experiments corroborate our theoretical findings and suggest that\ndeeper networks, i.e., $D \\ge 3$, may lead to better generalization,\nparticularly for realistic initialization scales.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6df1\u5ea6D\u22652\u7684\u5bf9\u89d2\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\u5728\u8fc7\u53c2\u6570\u5316\u7ebf\u6027\u56de\u5f52\u4e2d\u7684\u9690\u5f0f\u6b63\u5219\u5316\uff0c\u5206\u6790\u4e86\u68af\u5ea6\u6d41\u8f68\u8ff9\u6781\u9650\u70b9\u4e0e\u2113\u00b9\u6700\u5c0f\u5316\u95ee\u9898\u89e3\u4e4b\u95f4\u7684\u8fd1\u4f3c\u8bef\u5dee\u3002", "motivation": "\u4e3a\u7406\u89e3\u8fc7\u53c2\u6570\u5316\u6a21\u578b\u4e2d\u68af\u5ea6\u4e0b\u964d\u7684\u9690\u5f0f\u504f\u7f6e\uff0c\u7814\u7a76\u6df1\u5ea6\u5bf9\u89d2\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\u5728\u56de\u5f52\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63a8\u5bfc\u8fd1\u4f3c\u8bef\u5dee\u7684\u7d27\u4e0a\u754c\u548c\u4e0b\u754c\uff0c\u5206\u6790\u5176\u4e0e\u521d\u59cb\u5316\u5c3a\u5ea6\u03b1\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0D\u22653\u65f6\u8bef\u5dee\u7ebf\u6027\u51cf\u5c0f\uff0cD=2\u65f6\u8bef\u5dee\u51cf\u5c0f\u901f\u7387\u4e3a\u03b1^{1\u2212\u03f1}\uff0c\u4e0e\u7a00\u758f\u6062\u590d\u4e2d\u7684\u96f6\u7a7a\u95f4\u6027\u8d28\u76f8\u5173\u3002", "conclusion": "\u6df1\u5ea6\u7f51\u7edc\uff08D\u22653\uff09\u5728\u73b0\u5b9e\u521d\u59cb\u5316\u5c3a\u5ea6\u4e0b\u53ef\u80fd\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "keywords": "\u9690\u5f0f\u6b63\u5219\u5316,\u68af\u5ea6\u6d41,\u5bf9\u89d2\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc,\u8fc7\u53c2\u6570\u5316,\u7a00\u758f\u6062\u590d"}}
{"id": "2506.01388", "pdf": "https://arxiv.org/pdf/2506.01388", "abs": "https://arxiv.org/abs/2506.01388", "authors": ["Yihao Ding", "Soyeon Caren Han", "Yan Li", "Josiah Poon"], "title": "VRD-IU: Lessons from Visually Rich Document Intelligence and Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at IJCAI 2025 Demonstrations Track", "summary": "Visually Rich Document Understanding (VRDU) has emerged as a critical field\nin document intelligence, enabling automated extraction of key information from\ncomplex documents across domains such as medical, financial, and educational\napplications. However, form-like documents pose unique challenges due to their\ncomplex layouts, multi-stakeholder involvement, and high structural\nvariability. Addressing these issues, the VRD-IU Competition was introduced,\nfocusing on extracting and localizing key information from multi-format forms\nwithin the Form-NLU dataset, which includes digital, printed, and handwritten\ndocuments. This paper presents insights from the competition, which featured\ntwo tracks: Track A, emphasizing entity-based key information retrieval, and\nTrack B, targeting end-to-end key information localization from raw document\nimages. With over 20 participating teams, the competition showcased various\nstate-of-the-art methodologies, including hierarchical decomposition,\ntransformer-based retrieval, multimodal feature fusion, and advanced object\ndetection techniques. The top-performing models set new benchmarks in VRDU,\nproviding valuable insights into document intelligence.", "AI": {"tldr": "VRD-IU\u7ade\u8d5b\u805a\u7126\u4e8e\u4ece\u590d\u6742\u591a\u683c\u5f0f\u8868\u5355\u4e2d\u63d0\u53d6\u548c\u5b9a\u4f4d\u5173\u952e\u4fe1\u606f\uff0c\u5c55\u793a\u4e86\u591a\u79cd\u5148\u8fdb\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u7406\u89e3\u7684\u6807\u6746\u3002", "motivation": "\u89e3\u51b3\u8868\u5355\u7c7b\u6587\u6863\u7531\u4e8e\u590d\u6742\u5e03\u5c40\u548c\u591a\u5229\u76ca\u76f8\u5173\u8005\u53c2\u4e0e\u5bfc\u81f4\u7684\u72ec\u7279\u6311\u6218\u3002", "method": "\u7ade\u8d5b\u8bbe\u7f6e\u4e24\u4e2a\u8d5b\u9053\uff1a\u57fa\u4e8e\u5b9e\u4f53\u7684\u5173\u952e\u4fe1\u606f\u68c0\u7d22\uff08Track A\uff09\u548c\u7aef\u5230\u7aef\u7684\u5173\u952e\u4fe1\u606f\u5b9a\u4f4d\uff08Track B\uff09\uff0c\u53c2\u4e0e\u8005\u4f7f\u7528\u4e86\u5c42\u6b21\u5206\u89e3\u3001\u57fa\u4e8eTransformer\u7684\u68c0\u7d22\u548c\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u7b49\u6280\u672f\u3002", "result": "20\u591a\u4e2a\u56e2\u961f\u53c2\u4e0e\uff0c\u9876\u7ea7\u6a21\u578b\u4e3aVRDU\u9886\u57df\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "\u7ade\u8d5b\u4e3a\u6587\u6863\u667a\u80fd\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u5148\u8fdb\u6280\u672f\u5728\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "Visually Rich Document Understanding, Form-NLU, \u7ade\u8d5b, \u591a\u6a21\u6001\u7279\u5f81\u878d\u5408, \u5173\u952e\u4fe1\u606f\u63d0\u53d6"}}
{"id": "2506.01392", "pdf": "https://arxiv.org/pdf/2506.01392", "abs": "https://arxiv.org/abs/2506.01392", "authors": ["Junha Chun", "Youngjoon Jeong", "Taesup Kim"], "title": "Sparse Imagination for Efficient Visual World Model Planning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "World model based planning has significantly improved decision-making in\ncomplex environments by enabling agents to simulate future states and make\ninformed choices. However, ensuring the prediction accuracy of world models\noften demands substantial computational resources, posing a major challenge for\nreal-time applications. This computational burden is particularly restrictive\nin robotics, where resources are severely constrained. To address this\nlimitation, we propose a Sparse Imagination for Efficient Visual World Model\nPlanning, which enhances computational efficiency by reducing the number of\ntokens processed during forward prediction. Our method leverages a sparsely\ntrained vision-based world model based on transformers with randomized grouped\nattention strategy, allowing the model to adaptively adjust the number of\ntokens processed based on the computational resource. By enabling sparse\nimagination (rollout), our approach significantly accelerates planning while\nmaintaining high control fidelity. Experimental results demonstrate that sparse\nimagination preserves task performance while dramatically improving inference\nefficiency, paving the way for the deployment of world models in real-time\ndecision-making scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u60f3\u8c61\u7684\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u9884\u6d4b\u65f6\u7684\u4ee4\u724c\u5904\u7406\u6570\u91cf\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u51b3\u7b56\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u7684\u4e16\u754c\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u9650\u5236\u4e86\u5728\u673a\u5668\u4eba\u7b49\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u7684\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u7a00\u758f\u8bad\u7ec3\u89c6\u89c9\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u968f\u673a\u5206\u7ec4\u6ce8\u610f\u529b\u7b56\u7565\u81ea\u9002\u5e94\u8c03\u6574\u5904\u7406\u4ee4\u724c\u6570\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7a00\u758f\u60f3\u8c61\u65b9\u6cd5\u5728\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e16\u754c\u6a21\u578b\u5728\u5b9e\u65f6\u51b3\u7b56\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "keywords": "\u4e16\u754c\u6a21\u578b\uff0c\u7a00\u758f\u60f3\u8c61\uff0c\u8ba1\u7b97\u6548\u7387\uff0c\u53d8\u6362\u5668\uff0c\u5b9e\u65f6\u51b3\u7b56"}}
{"id": "2506.01162", "pdf": "https://arxiv.org/pdf/2506.01162", "abs": "https://arxiv.org/abs/2506.01162", "authors": ["Maryam Aliakbarpour", "Zhan Shi", "Ria Stevens", "Vincent X. Wang"], "title": "Nearly-Linear Time Private Hypothesis Selection with the Optimal Approximation Factor", "categories": ["cs.DS", "cs.CR", "cs.LG", "stat.ML"], "comment": "33 pages", "summary": "Estimating the density of a distribution from its samples is a fundamental\nproblem in statistics. Hypothesis selection addresses the setting where, in\naddition to a sample set, we are given $n$ candidate distributions -- referred\nto as hypotheses -- and the goal is to determine which one best describes the\nunderlying data distribution. This problem is known to be solvable very\nefficiently, requiring roughly $O(\\log n)$ samples and running in\n$\\tilde{O}(n)$ time. The quality of the output is measured via the total\nvariation distance to the unknown distribution, and the approximation factor of\nthe algorithm determines how large this distance is compared to the optimal\ndistance achieved by the best candidate hypothesis. It is known that $\\alpha =\n3$ is the optimal approximation factor for this problem. We study hypothesis\nselection under the constraint of differential privacy. We propose a\ndifferentially private algorithm in the central model that runs in\nnearly-linear time with respect to the number of hypotheses, achieves the\noptimal approximation factor, and incurs only a modest increase in sample\ncomplexity, which remains polylogarithmic in $n$. This resolves an open\nquestion posed by [Bun, Kamath, Steinke, Wu, NeurIPS 2019]. Prior to our work,\nexisting upper bounds required quadratic time.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5dee\u5206\u9690\u79c1\u7ea6\u675f\u4e0b\u7684\u5047\u8bbe\u9009\u62e9\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8fd1\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u8fd1\u4f3c\u56e0\u5b50\uff0c\u4e14\u6837\u672c\u590d\u6742\u5ea6\u4ec5\u9002\u5ea6\u589e\u52a0\u3002", "motivation": "\u5728\u7edf\u8ba1\u5b66\u4e2d\uff0c\u4ece\u6837\u672c\u4e2d\u4f30\u8ba1\u5206\u5e03\u7684\u5bc6\u5ea6\u662f\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\u3002\u5047\u8bbe\u9009\u62e9\u95ee\u9898\u5173\u6ce8\u5728\u7ed9\u5b9a\u5019\u9009\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\uff0c\u786e\u5b9a\u54ea\u4e00\u4e2a\u6700\u80fd\u63cf\u8ff0\u6570\u636e\u5206\u5e03\u3002\u4ee5\u5f80\u7684\u7814\u7a76\u8868\u660e\uff0c\u8be5\u95ee\u9898\u53ef\u4ee5\u5728\u9ad8\u6548\u7684\u65f6\u95f4\u548c\u6837\u672c\u590d\u6742\u5ea6\u4e0b\u89e3\u51b3\uff0c\u4f46\u5c1a\u672a\u5728\u5dee\u5206\u9690\u79c1\u7ea6\u675f\u4e0b\u5b9e\u73b0\u6700\u4f18\u89e3\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4e2d\u5fc3\u6a21\u578b\u4e0b\u7684\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u8fd1\u7ebf\u6027\u65f6\u95f4\u5185\u8fd0\u884c\uff08\u76f8\u5bf9\u4e8e\u5047\u8bbe\u6570\u91cf\uff09\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u8fd1\u4f3c\u56e0\u5b50\uff08\u03b1=3\uff09\uff0c\u5e76\u4e14\u6837\u672c\u590d\u6742\u5ea6\u4ec5\u589e\u52a0\u4e3a\u591a\u9879\u5f0f\u5bf9\u6570\u7ea7\u522b\u3002", "result": "\u8be5\u7b97\u6cd5\u5728\u65f6\u95f4\u548c\u6837\u672c\u590d\u6742\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u540e\u8005\u9700\u8981\u4e8c\u6b21\u65f6\u95f4\uff09\uff0c\u540c\u65f6\u89e3\u51b3\u4e86[Bun\u7b49\u4eba]\u63d0\u51fa\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u5dee\u5206\u9690\u79c1\u7ea6\u675f\u4e0b\uff0c\u5047\u8bbe\u9009\u62e9\u95ee\u9898\u53ef\u4ee5\u5728\u8fd1\u7ebf\u6027\u65f6\u95f4\u5185\u9ad8\u6548\u89e3\u51b3\uff0c\u4e14\u6837\u672c\u590d\u6742\u5ea6\u4ec5\u9002\u5ea6\u589e\u52a0\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5047\u8bbe\u9009\u62e9, \u5dee\u5206\u9690\u79c1, \u4e2d\u5fc3\u6a21\u578b, \u6837\u672c\u590d\u6742\u5ea6, \u8fd1\u4f3c\u56e0\u5b50"}}
{"id": "2506.01411", "pdf": "https://arxiv.org/pdf/2506.01411", "abs": "https://arxiv.org/abs/2506.01411", "authors": ["Minjeong Park", "Hongbeen Park", "Jinkyu Kim"], "title": "ViTA-PAR: Visual and Textual Attribute Alignment with Attribute Prompting for Pedestrian Attribute Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to IEEE ICIP 2025", "summary": "The Pedestrian Attribute Recognition (PAR) task aims to identify various\ndetailed attributes of an individual, such as clothing, accessories, and\ngender. To enhance PAR performance, a model must capture features ranging from\ncoarse-grained global attributes (e.g., for identifying gender) to fine-grained\nlocal details (e.g., for recognizing accessories) that may appear in diverse\nregions. Recent research suggests that body part representation can enhance the\nmodel's robustness and accuracy, but these methods are often restricted to\nattribute classes within fixed horizontal regions, leading to degraded\nperformance when attributes appear in varying or unexpected body locations. In\nthis paper, we propose Visual and Textual Attribute Alignment with Attribute\nPrompting for Pedestrian Attribute Recognition, dubbed as ViTA-PAR, to enhance\nattribute recognition through specialized multimodal prompting and\nvision-language alignment. We introduce visual attribute prompts that capture\nglobal-to-local semantics, enabling diverse attribute representations. To\nenrich textual embeddings, we design a learnable prompt template, termed person\nand attribute context prompting, to learn person and attributes context.\nFinally, we align visual and textual attribute features for effective fusion.\nViTA-PAR is validated on four PAR benchmarks, achieving competitive performance\nwith efficient inference. We release our code and model at\nhttps://github.com/mlnjeongpark/ViTA-PAR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ViTA-PAR\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u63d0\u793a\u548c\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u589e\u5f3a\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\uff0c\u7ed3\u5408\u89c6\u89c9\u5c5e\u6027\u63d0\u793a\u548c\u53ef\u5b66\u4e60\u7684\u6587\u672c\u63d0\u793a\u6a21\u677f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5c5e\u6027\u8bc6\u522b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u83b7\u884c\u4eba\u5c5e\u6027\u65f6\u53d7\u9650\u4e8e\u56fa\u5b9a\u533a\u57df\uff0c\u65e0\u6cd5\u9002\u5e94\u5c5e\u6027\u5728\u4e0d\u540c\u8eab\u4f53\u4f4d\u7f6e\u7684\u53d8\u5316\uff0cViTA-PAR\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u6280\u672f\u63d0\u5347\u5c5e\u6027\u7684\u5168\u5c40\u548c\u5c40\u90e8\u8868\u5f81\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u5c5e\u6027\u63d0\u793a\u6355\u83b7\u5168\u5c40\u5230\u5c40\u90e8\u8bed\u4e49\uff0c\u8bbe\u8ba1\u53ef\u5b66\u4e60\u7684\u6587\u672c\u63d0\u793a\u6a21\u677f\uff08\u4eba\u7269\u548c\u5c5e\u6027\u4e0a\u4e0b\u6587\u63d0\u793a\uff09\uff0c\u5e76\u5bf9\u9f50\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u8fdb\u884c\u878d\u5408\u3002", "result": "\u5728\u56db\u4e2aPAR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "ViTA-PAR\u901a\u8fc7\u89c6\u89c9\u548c\u6587\u672c\u5c5e\u6027\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "keywords": "\u884c\u4eba\u5c5e\u6027\u8bc6\u522b, \u591a\u6a21\u6001\u63d0\u793a, \u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50, \u5c5e\u6027\u4e0a\u4e0b\u6587, ViTA-PAR"}}
{"id": "2506.01412", "pdf": "https://arxiv.org/pdf/2506.01412", "abs": "https://arxiv.org/abs/2506.01412", "authors": ["Bishwajit Prasad Gond", "Durga Prasad Mohapatra"], "title": "System Calls for Malware Detection and Classification: Methodologies and Applications", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "As malware continues to become more complex and harder to detect, Malware\nAnalysis needs to continue to evolve to stay one step ahead. One promising key\narea approach focuses on using system calls and API Calls, the core\ncommunication between user applications and the operating system and their\nkernels. These calls provide valuable insight into how software or programs\nbehaves, making them an useful tool for spotting suspicious or harmful activity\nof programs and software. This chapter takes a deep down look at how system\ncalls are used in malware detection and classification, covering techniques\nlike static and dynamic analysis, as well as sandboxing. By combining these\nmethods with advanced techniques like machine learning, statistical analysis,\nand anomaly detection, researchers can analyze system call patterns to tell the\ndifference between normal and malicious behavior. The chapter also explores how\nthese techniques are applied across different systems, including Windows,\nLinux, and Android, while also looking at the ways sophisticated malware tries\nto evade detection.", "AI": {"tldr": "\u6458\u8981\u63a2\u8ba8\u4e86\u7cfb\u7edf\u8c03\u7528\u5728\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u548c\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\uff0c\u7ed3\u5408\u9759\u6001\u4e0e\u52a8\u6001\u5206\u6790\u3001\u6c99\u76d2\u7b49\u6280\u672f\uff0c\u4ee5\u53ca\u673a\u5668\u5b66\u4e60\u548c\u7edf\u8ba1\u5206\u6790\uff0c\u4ee5\u533a\u5206\u6b63\u5e38\u4e0e\u6076\u610f\u884c\u4e3a\u3002", "motivation": "\u6076\u610f\u8f6f\u4ef6\u65e5\u76ca\u590d\u6742\uff0c\u68c0\u6d4b\u96be\u5ea6\u589e\u52a0\uff0c\u4e9f\u9700\u5229\u7528\u7cfb\u7edf\u8c03\u7528\u548cAPI\u8c03\u7528\u7b49\u6838\u5fc3\u901a\u4fe1\u673a\u5236\u63d0\u5347\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u9759\u6001\u5206\u6790\u3001\u52a8\u6001\u5206\u6790\u3001\u6c99\u76d2\u6280\u672f\uff0c\u5e76\u8fd0\u7528\u673a\u5668\u5b66\u4e60\u3001\u7edf\u8ba1\u5206\u6790\u548c\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\u5206\u6790\u7cfb\u7edf\u8c03\u7528\u6a21\u5f0f\u3002", "result": "\u901a\u8fc7\u5206\u6790\u7cfb\u7edf\u8c03\u7528\u6a21\u5f0f\uff0c\u80fd\u591f\u6709\u6548\u533a\u5206\u6b63\u5e38\u4e0e\u6076\u610f\u884c\u4e3a\uff0c\u8986\u76d6Windows\u3001Linux\u548cAndroid\u7b49\u7cfb\u7edf\u3002", "conclusion": "\u7cfb\u7edf\u8c03\u7528\u5206\u6790\u662f\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u7684\u91cd\u8981\u624b\u6bb5\uff0c\u7ed3\u5408\u5148\u8fdb\u6280\u672f\u53ef\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "keywords": "\u6076\u610f\u8f6f\u4ef6\u5206\u6790,\u7cfb\u7edf\u8c03\u7528,API\u8c03\u7528,\u673a\u5668\u5b66\u4e60,\u9759\u6001\u5206\u6790,\u52a8\u6001\u5206\u6790"}}
{"id": "2506.01173", "pdf": "https://arxiv.org/pdf/2506.01173", "abs": "https://arxiv.org/abs/2506.01173", "authors": ["Tushar Gautam", "Robert M. Kirby", "Jacob Hochhalter", "Shandian Zhe"], "title": "SIFBench: An Extensive Benchmark for Fatigue Analysis", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Fatigue-induced crack growth is a leading cause of structural failure across\ncritical industries such as aerospace, civil engineering, automotive, and\nenergy. Accurate prediction of stress intensity factors (SIFs) -- the key\nparameters governing crack propagation in linear elastic fracture mechanics --\nis essential for assessing fatigue life and ensuring structural integrity.\nWhile machine learning (ML) has shown great promise in SIF prediction, its\nadvancement has been severely limited by the lack of rich, transparent,\nwell-organized, and high-quality datasets.\n  To address this gap, we introduce SIFBench, an open-source, large-scale\nbenchmark database designed to support ML-based SIF prediction. SIFBench\ncontains over 5 million different crack and component geometries derived from\nhigh-fidelity finite element simulations across 37 distinct scenarios, and\nprovides a unified Python interface for seamless data access and customization.\nWe report baseline results using a range of popular ML models -- including\nrandom forests, support vector machines, feedforward neural networks, and\nFourier neural operators -- alongside comprehensive evaluation metrics and\ntemplate code for model training, validation, and assessment. By offering a\nstandardized and scalable resource, SIFBench substantially lowers the entry\nbarrier and fosters the development and application of ML methods in damage\ntolerance design and predictive maintenance.", "AI": {"tldr": "SIFBench\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u5e93\uff0c\u65e8\u5728\u652f\u6301\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5e94\u529b\u5f3a\u5ea6\u56e0\u5b50\u9884\u6d4b\uff0c\u586b\u8865\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "motivation": "\u75b2\u52b3\u88c2\u7eb9\u589e\u957f\u662f\u7ed3\u6784\u5931\u6548\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u800c\u7cbe\u786e\u9884\u6d4b\u5e94\u529b\u5f3a\u5ea6\u56e0\u5b50\u5bf9\u8bc4\u4f30\u75b2\u52b3\u5bff\u547d\u548c\u786e\u4fdd\u7ed3\u6784\u5b8c\u6574\u6027\u81f3\u5173\u91cd\u8981\u3002\u76ee\u524d\u673a\u5668\u5b66\u4e60\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u56e0\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u800c\u53d7\u9650\u3002", "method": "\u901a\u8fc7\u9ad8\u4fdd\u771f\u6709\u9650\u5143\u6a21\u62df\u751f\u6210\u8d85\u8fc7500\u4e07\u4e2a\u88c2\u7eb9\u548c\u7ec4\u4ef6\u51e0\u4f55\u6570\u636e\uff0c\u6db5\u76d637\u79cd\u4e0d\u540c\u573a\u666f\uff0c\u5e76\u63d0\u4f9b\u4e86Python\u63a5\u53e3\u4ee5\u65b9\u4fbf\u6570\u636e\u8bbf\u95ee\u548c\u5b9a\u5236\u3002", "result": "\u4f7f\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982\u968f\u673a\u68ee\u6797\u3001SVM\u3001\u795e\u7ecf\u7f51\u7edc\u7b49\uff09\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6307\u6807\u548c\u6a21\u677f\u4ee3\u7801\u3002", "conclusion": "SIFBench\u964d\u4f4e\u4e86\u8fdb\u5165\u95e8\u69db\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u5b66\u4e60\u5728\u635f\u4f24\u5bb9\u9650\u8bbe\u8ba1\u548c\u9884\u6d4b\u6027\u7ef4\u62a4\u4e2d\u7684\u5e94\u7528\u3002", "keywords": "\u75b2\u52b3\u88c2\u7eb9\u589e\u957f,\u5e94\u529b\u5f3a\u5ea6\u56e0\u5b50,\u673a\u5668\u5b66\u4e60,\u6570\u636e\u96c6,\u6709\u9650\u5143\u6a21\u62df"}}
{"id": "2506.01189", "pdf": "https://arxiv.org/pdf/2506.01189", "abs": "https://arxiv.org/abs/2506.01189", "authors": ["Emmanuel Hartman", "Nicolas Charon"], "title": "SVarM: Linear Support Varifold Machines for Classification and Regression on Geometric Data", "categories": ["cs.CV", "cs.LG", "math.DG", "math.FA", "49Q15, 53C42, 46N10", "I.5.1; I.4.0"], "comment": "22 pages, 12 figures", "summary": "Despite progress in the rapidly developing field of geometric deep learning,\nperforming statistical analysis on geometric data--where each observation is a\nshape such as a curve, graph, or surface--remains challenging due to the\nnon-Euclidean nature of shape spaces, which are defined as equivalence classes\nunder invariance groups. Building machine learning frameworks that incorporate\nsuch invariances, notably to shape parametrization, is often crucial to ensure\ngeneralizability of the trained models to new observations. This work proposes\nSVarM to exploit varifold representations of shapes as measures and their\nduality with test functions $h:\\mathbb{R}^n \\times S^{n-1} \\to \\mathbb{R}$.\nThis method provides a general framework akin to linear support vector machines\nbut operating instead over the infinite-dimensional space of varifolds. We\ndevelop classification and regression models on shape datasets by introducing a\nneural network-based representation of the trainable test function $h$. This\napproach demonstrates strong performance and robustness across various shape\ngraph and surface datasets, achieving results comparable to state-of-the-art\nmethods while significantly reducing the number of trainable parameters.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSVarM\u7684\u65b9\u6cd5\uff0c\u5229\u7528varifold\u8868\u793a\u5f62\u72b6\uff0c\u5e76\u901a\u8fc7\u4e0e\u6d4b\u8bd5\u51fd\u6570\u7684\u5bf9\u5076\u6027\u6784\u5efa\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u51e0\u4f55\u6570\u636e\u7edf\u8ba1\u5206\u6790\u7684\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5f62\u72b6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u53c2\u6570\u8f83\u5c11\u3002", "motivation": "\u51e0\u4f55\u6570\u636e\u7684\u7edf\u8ba1\u5206\u6790\u56e0\u5f62\u72b6\u7a7a\u95f4\u7684\u975e\u6b27\u51e0\u91cc\u5f97\u6027\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u901a\u8fc7\u7ed3\u5408\u5f62\u72b6\u53c2\u6570\u5316\u7684\u4e0d\u53d8\u6027\uff0c\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5229\u7528varifold\u8868\u793a\u5f62\u72b6\uff0c\u5e76\u901a\u8fc7\u6d4b\u8bd5\u51fd\u6570\u7684\u5bf9\u5076\u6027\u6784\u5efa\u5206\u7c7b\u548c\u56de\u5f52\u6a21\u578b\uff0c\u5f15\u5165\u4e86\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6d4b\u8bd5\u51fd\u6570\u8868\u793a\u3002", "result": "\u5728\u591a\u7ec4\u5f62\u72b6\u56fe\u4e0e\u8868\u9762\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u6027\u80fd\u4e0e\u4e3b\u6d41\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u53c2\u6570\u6570\u91cf\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "SVarM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53c2\u6570\u5c11\u7684\u51e0\u4f55\u6570\u636e\u5206\u6790\u6846\u67b6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60, varifold, \u5f62\u72b6\u5206\u6790, \u4e0d\u53d8\u6027, \u795e\u7ecf\u7f51\u7edc"}}
{"id": "2506.01453", "pdf": "https://arxiv.org/pdf/2506.01453", "abs": "https://arxiv.org/abs/2506.01453", "authors": ["Igor Ciril", "Khalil Haddaoui", "Yohann Tendero"], "title": "From Initial Data to Boundary Layers: Neural Networks for Nonlinear Hyperbolic Conservation Laws", "categories": ["math.AP", "cs.AI"], "comment": null, "summary": "We address the approximation of entropy solutions to initial-boundary value\nproblems for nonlinear strictly hyperbolic conservation laws using neural\nnetworks. A general and systematic framework is introduced for the design of\nefficient and reliable learning algorithms, combining fast convergence during\ntraining with accurate predictions. The methodology is assessed through a\nseries of one-dimensional scalar test cases, highlighting its potential\napplicability to more complex industrial scenarios.", "AI": {"tldr": "\u7528\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u975e\u7ebf\u6027\u4e25\u683c\u53cc\u66f2\u5b88\u6052\u5b9a\u5f8b\u7684\u521d\u8fb9\u503c\u95ee\u9898\u7684\u71b5\u89e3\uff0c\u5e76\u63d0\u51fa\u9ad8\u6548\u53ef\u9760\u7684\u5b66\u4e60\u7b97\u6cd5\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u975e\u7ebf\u6027\u4e25\u683c\u53cc\u66f2\u5b88\u6052\u5b9a\u5f8b\u7684\u521d\u8fb9\u503c\u95ee\u9898\u4e2d\u71b5\u89e3\u7684\u8fd1\u4f3c\u8ba1\u7b97\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u7ed3\u5408\u5feb\u901f\u6536\u655b\u548c\u51c6\u786e\u9884\u6d4b\u7684\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7b97\u6cd5\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u4e00\u7ef4\u6807\u91cf\u6d4b\u8bd5\u6848\u4f8b\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e00\u7ef4\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5c55\u793a\u5176\u5728\u66f4\u590d\u6742\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u4e3a\u975e\u7ebf\u6027\u5b88\u6052\u5b9a\u5f8b\u7684\u71b5\u89e3\u8fd1\u4f3c\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc, \u975e\u7ebf\u6027\u53cc\u66f2\u5b88\u6052\u5b9a\u5f8b, \u71b5\u89e3, \u5b66\u4e60\u7b97\u6cd5"}}
{"id": "2506.01456", "pdf": "https://arxiv.org/pdf/2506.01456", "abs": "https://arxiv.org/abs/2506.01456", "authors": ["Lina Qin", "Cheng Zhu", "Chuqi Zhou", "Yukun Huang", "Jiayi Zhu", "Ping Liang", "Jinju Wang", "Yixing Huang", "Cheng Luo", "Dezhong Yao", "Ying Tan"], "title": "GenDMR: A dynamic multimodal role-swapping network for identifying risk gene phenotypes", "categories": ["q-bio.GN", "cs.AI", "cs.LG", "q-bio.NC"], "comment": "31 pages, 9 figures", "summary": "Recent studies have shown that integrating multimodal data fusion techniques\nfor imaging and genetic features is beneficial for the etiological analysis and\npredictive diagnosis of Alzheimer's disease (AD). However, there are several\ncritical flaws in current deep learning methods. Firstly, there has been\ninsufficient discussion and exploration regarding the selection and encoding of\ngenetic information. Secondly, due to the significantly superior classification\nvalue of AD imaging features compared to genetic features, many studies in\nmultimodal fusion emphasize the strengths of imaging features, actively\nmitigating the influence of weaker features, thereby diminishing the learning\nof the unique value of genetic features. To address this issue, this study\nproposes the dynamic multimodal role-swapping network (GenDMR). In GenDMR, we\ndevelop a novel approach to encode the spatial organization of single\nnucleotide polymorphisms (SNPs), enhancing the representation of their genomic\ncontext. Additionally, to adaptively quantify the disease risk of SNPs and\nbrain region, we propose a multi-instance attention module to enhance model\ninterpretability. Furthermore, we introduce a dominant modality selection\nmodule and a contrastive self-distillation module, combining them to achieve a\ndynamic teacher-student role exchange mechanism based on dominant and auxiliary\nmodalities for bidirectional co-updating of different modal data. Finally,\nGenDMR achieves state-of-the-art performance on the ADNI public dataset and\nvisualizes attention to different SNPs, focusing on confirming 12 potential\nhigh-risk genes related to AD, including the most classic APOE and recently\nhighlighted significant risk genes. This demonstrates GenDMR's interpretable\nanalytical capability in exploring AD genetic features, providing new insights\nand perspectives for the development of multimodal data fusion techniques.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u591a\u6a21\u6001\u89d2\u8272\u4ea4\u6362\u7f51\u7edc\uff08GenDMR\uff09\uff0c\u89e3\u51b3\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u5728\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u4e2d\u5bf9\u9057\u4f20\u4fe1\u606f\u7f16\u7801\u548c\u7279\u5f81\u5e73\u8861\u7684\u4e0d\u8db3\uff0c\u63d0\u5347AD\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u9057\u4f20\u4fe1\u606f\u7f16\u7801\u548c\u591a\u6a21\u6001\u7279\u5f81\u5e73\u8861\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u4ee5\u63d0\u9ad8AD\u75c5\u56e0\u5206\u6790\u548c\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002", "method": "\u5f00\u53d1SNP\u7a7a\u95f4\u7ec4\u7ec7\u7f16\u7801\u65b9\u6cd5\uff0c\u63d0\u51fa\u591a\u5b9e\u4f8b\u6ce8\u610f\u529b\u6a21\u5757\u548c\u52a8\u6001\u89d2\u8272\u4ea4\u6362\u673a\u5236\u3002", "result": "\u5728ADNI\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u8bc6\u522b12\u4e2a\u6f5c\u5728\u9ad8\u98ce\u9669\u57fa\u56e0\u3002", "conclusion": "GenDMR\u4e3a\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u89e3\u91ca\u5206\u6790\u80fd\u529b\u3002", "keywords": "\u591a\u6a21\u6001\u878d\u5408\u3001AD\u3001GenDMR\u3001SNP\u7f16\u7801\u3001\u52a8\u6001\u89d2\u8272\u4ea4\u6362"}}
{"id": "2506.01221", "pdf": "https://arxiv.org/pdf/2506.01221", "abs": "https://arxiv.org/abs/2506.01221", "authors": ["Md Adnan Faisal Hossain", "Zhihao Duan", "Fengqing Zhu"], "title": "Flexible Mixed Precision Quantization for Learned Image Compression", "categories": ["eess.IV", "cs.LG"], "comment": null, "summary": "Despite its improvements in coding performance compared to traditional\ncodecs, Learned Image Compression (LIC) suffers from large computational costs\nfor storage and deployment. Model quantization offers an effective solution to\nreduce the computational complexity of LIC models. However, most existing works\nperform fixed-precision quantization which suffers from sub-optimal utilization\nof resources due to the varying sensitivity to quantization of different layers\nof a neural network. In this paper, we propose a Flexible Mixed Precision\nQuantization (FMPQ) method that assigns different bit-widths to different\nlayers of the quantized network using the fractional change in rate-distortion\nloss as the bit-assignment criterion. We also introduce an adaptive search\nalgorithm which reduces the time-complexity of searching for the desired\ndistribution of quantization bit-widths given a fixed model size. Evaluation of\nour method shows improved BD-Rate performance under similar model size\nconstraints compared to other works on quantization of LIC models. We have made\nthe source code available at gitlab.com/viper-purdue/fmpq.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\uff08FMPQ\uff09\uff0c\u901a\u8fc7\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u4e0d\u540c\u5c42\u5206\u914d\u4e0d\u540c\u7684\u4f4d\u5bbd\uff0c\u7ed3\u5408\u641c\u7d22\u7b97\u6cd5\u4f18\u5316\u91cf\u5316\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5b66\u4e60\u56fe\u50cf\u538b\u7f29\uff08LIC\uff09\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7f16\u89e3\u7801\u5668\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u548c\u5b58\u50a8\u9700\u6c42\u8f83\u9ad8\uff0c\u9700\u8981\u6709\u6548\u7684\u91cf\u5316\u65b9\u6cd5\u964d\u4f4e\u590d\u6742\u5ea6\u3002", "method": "\u4f7f\u7528FMPQ\u65b9\u6cd5\uff0c\u6839\u636e\u5c42\u95f4\u91cf\u5316\u654f\u611f\u6027\u7684\u5dee\u5f02\u5206\u914d\u4e0d\u540c\u4f4d\u5bbd\uff0c\u5e76\u4ee5\u7387\u5931\u771f\u635f\u5931\u7684\u53d8\u5316\u4e3a\u5206\u914d\u6807\u51c6\uff1b\u5f15\u5165\u81ea\u9002\u5e94\u641c\u7d22\u7b97\u6cd5\u51cf\u5c11\u4f4d\u5bbd\u5206\u5e03\u641c\u7d22\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u5728\u7c7b\u4f3c\u6a21\u578b\u5927\u5c0f\u7ea6\u675f\u4e0b\uff0cFMPQ\u65b9\u6cd5\u5728BD-Rate\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u91cf\u5316\u65b9\u6cd5\u3002", "conclusion": "FMPQ\u65b9\u6cd5\u4e3aLIC\u6a21\u578b\u7684\u91cf\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "keywords": "\u5b66\u4e60\u56fe\u50cf\u538b\u7f29, \u91cf\u5316, \u6df7\u5408\u7cbe\u5ea6, \u7387\u5931\u771f\u4f18\u5316, \u795e\u7ecf\u7f51\u7edc"}}
{"id": "2506.01463", "pdf": "https://arxiv.org/pdf/2506.01463", "abs": "https://arxiv.org/abs/2506.01463", "authors": ["V. Botti"], "title": "Agentic AI and Multiagentic: Are We Reinventing the Wheel?", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "The terms Agentic AI and Multiagentic AI have recently gained popularity in\ndiscussions on generative artificial intelligence, often used to describe\nautonomous software agents and systems composed of such agents. However, the\nuse of these terms confuses these buzzwords with well-established concepts in\nAI literature: intelligent agents and multi-agent systems. This article offers\na critical analysis of this conceptual misuse. We review the theoretical\norigins of \"agentic\" in the social sciences (Bandura, 1986) and philosophical\nnotions of intentionality (Dennett, 1971), and then summarise foundational\nworks on intelligent agents and multi-agent systems by Wooldridge, Jennings and\nothers. We examine classic agent architectures, from simple reactive agents to\nBelief-Desire-Intention (BDI) models, and highlight key properties (autonomy,\nreactivity, proactivity, social capability) that define agency in AI. We then\ndiscuss recent developments in large language models (LLMs) and agent platforms\nbased on LLMs, including the emergence of LLM-powered AI agents and open-source\nmulti-agent orchestration frameworks. We argue that the term AI Agentic is\noften used as a buzzword for what are essentially AI agents, and AI\nMultiagentic for what are multi-agent systems. This confusion overlooks decades\nof research in the field of autonomous agents and multi-agent systems. The\narticle advocates for scientific and technological rigour and the use of\nestablished terminology from the state of the art in AI, incorporating the\nwealth of existing knowledge, including standards for multi-agent system\nplatforms, communication languages and coordination and cooperation algorithms,\nagreement technologies (automated negotiation, argumentation, virtual\norganisations, trust, reputation, etc.), into the new and promising wave of\nLLM-based AI agents, so as not to end up reinventing the wheel.", "AI": {"tldr": "\u672c\u6587\u6279\u5224\u6027\u5730\u5206\u6790\u4e86\u2018Agentic AI\u2019\u548c\u2018Multiagentic AI\u2019\u8fd9\u4e24\u4e2a\u6d41\u884c\u672f\u8bed\u7684\u6982\u5ff5\u6df7\u6dc6\u95ee\u9898\uff0c\u5f3a\u8c03\u5176\u4e0eAI\u9886\u57df\u5df2\u6709\u6982\u5ff5\u2018\u667a\u80fd\u4ee3\u7406\u2019\u548c\u2018\u591a\u4ee3\u7406\u7cfb\u7edf\u2019\u7684\u91cd\u590d\uff0c\u5e76\u547c\u5401\u79d1\u5b66\u4e25\u8c28\u6027\u3002", "motivation": "\u63a2\u8ba8\u5f53\u524d\u5bf9\u2018Agentic AI\u2019\u548c\u2018Multiagentic AI\u2019\u672f\u8bed\u7684\u6ee5\u7528\uff0c\u6f84\u6e05\u5176\u4e0e\u5df2\u6709AI\u6982\u5ff5\u7684\u5173\u7cfb\uff0c\u907f\u514d\u5ffd\u7565\u591a\u5e74\u7814\u7a76\u6210\u679c\u3002", "method": "\u56de\u987e\u2018agentic\u2019\u7684\u7406\u8bba\u8d77\u6e90\u3001\u54f2\u5b66\u6982\u5ff5\u53ca\u7ecf\u5178\u4ee3\u7406\u67b6\u6784\uff0c\u5206\u6790LLM\u9a71\u52a8\u7684AI\u4ee3\u7406\u5e73\u53f0\u53d1\u5c55\u3002", "result": "\u6307\u51fa\u8fd9\u4e9b\u65b0\u672f\u8bed\u5b9e\u8d28\u4e0a\u662f\u5df2\u6709\u6982\u5ff5\u7684\u53d8\u4f53\uff0c\u5bb9\u6613\u88ab\u8bef\u7528\u4e3a\u8425\u9500\u5671\u5934\u3002", "conclusion": "\u63d0\u5021\u4f7f\u7528\u89c4\u8303\u7684AI\u672f\u8bed\uff0c\u6574\u5408\u73b0\u6709\u77e5\u8bc6\uff0c\u907f\u514d\u91cd\u590d\u7814\u7a76\u3002", "keywords": "Agentic AI, Multiagentic AI, \u667a\u80fd\u4ee3\u7406, \u591a\u4ee3\u7406\u7cfb\u7edf, \u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.01226", "pdf": "https://arxiv.org/pdf/2506.01226", "abs": "https://arxiv.org/abs/2506.01226", "authors": ["Nicholas H. Barbara", "Ruigang Wang", "Alexandre Megretski", "Ian R. Manchester"], "title": "React to Surprises: Stable-by-Design Neural Feedback Control and the Youla-REN", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "We study parameterizations of stabilizing nonlinear policies for\nlearning-based control. We propose a structure based on a nonlinear version of\nthe Youla-Ku\\v{c}era parameterization combined with robust neural networks such\nas the recurrent equilibrium network (REN). The resulting parameterizations are\nunconstrained, and hence can be searched over with first-order optimization\nmethods, while always ensuring closed-loop stability by construction. We study\nthe combination of (a) nonlinear dynamics, (b) partial observation, and (c)\nincremental closed-loop stability requirements (contraction and Lipschitzness).\nWe find that with any two of these three difficulties, a contracting and\nLipschitz Youla parameter always leads to contracting and Lipschitz closed\nloops. However, if all three hold, then incremental stability can be lost with\nexogenous disturbances. Instead, a weaker condition is maintained, which we\ncall d-tube contraction and Lipschitzness. We further obtain converse results\nshowing that the proposed parameterization covers all contracting and Lipschitz\nclosed loops for certain classes of nonlinear systems. Numerical experiments\nillustrate the utility of our parameterization when learning controllers with\nbuilt-in stability certificates for: i) ``economic'' rewards without\nstabilizing effects; ii) short training horizons; and iii) uncertain systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u5b66\u4e60\u7684\u63a7\u5236\u4e2d\u7a33\u5b9a\u975e\u7ebf\u6027\u7b56\u7565\u7684\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u975e\u7ebf\u6027Youla-Ku\u010dera\u53c2\u6570\u5316\u548c\u9c81\u68d2\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u7ea6\u675f\u53c2\u6570\u5316\u7ed3\u6784\uff0c\u786e\u4fdd\u95ed\u73af\u7a33\u5b9a\u6027\u3002", "motivation": "\u63a2\u7d22\u5728\u5b66\u4e60\u63a7\u5236\u4e2d\u5982\u4f55\u5b9e\u73b0\u7a33\u5b9a\u95ed\u73af\u7cfb\u7edf\u7684\u975e\u7ebf\u6027\u7b56\u7565\u53c2\u6570\u5316\uff0c\u4ee5\u514b\u670d\u975e\u7ebf\u6027\u52a8\u6001\u3001\u90e8\u5206\u89c2\u6d4b\u548c\u589e\u91cf\u7a33\u5b9a\u6027\u7b49\u6311\u6218\u3002", "method": "\u7ed3\u5408\u975e\u7ebf\u6027Youla-Ku\u010dera\u53c2\u6570\u5316\u548c\u9c81\u68d2\u795e\u7ecf\u7f51\u7edc\uff08\u5982REN\uff09\uff0c\u63d0\u51fa\u975e\u7ea6\u675f\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e00\u9636\u4f18\u5316\u641c\u7d22\u3002", "result": "\u5728\u4e09\u79cd\u56f0\u96be\u4e2d\u7684\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c\u53c2\u6570\u5316\u65b9\u6cd5\u80fd\u4fdd\u6301\u6536\u7f29\u6027\u548cLipschitz\u6027\uff1b\u4e09\u79cd\u56f0\u96be\u540c\u65f6\u51fa\u73b0\u65f6\uff0c\u589e\u91cf\u7a33\u5b9a\u6027\u53ef\u80fd\u4e22\u5931\uff0c\u4f46\u4ecd\u6ee1\u8db3d-\u7ba1\u6536\u7f29\u548cLipschitz\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5177\u6709\u7a33\u5b9a\u8bc1\u4e66\u7684\u63a7\u5236\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u7ecf\u6d4e\u5956\u52b1\u3001\u77ed\u8bad\u7ec3\u5468\u671f\u548c\u4e0d\u786e\u5b9a\u7cfb\u7edf\u3002", "keywords": "\u5b66\u4e60\u63a7\u5236, \u975e\u7ebf\u6027\u7b56\u7565, \u95ed\u73af\u7a33\u5b9a\u6027, Youla-Ku\u010dera\u53c2\u6570\u5316, \u9c81\u68d2\u795e\u7ecf\u7f51\u7edc"}}
{"id": "2506.01267", "pdf": "https://arxiv.org/pdf/2506.01267", "abs": "https://arxiv.org/abs/2506.01267", "authors": ["Jingfu Peng", "Yuhong Yang"], "title": "Adversarial learning for nonparametric regression: Minimax rate and adaptive estimation", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "Despite tremendous advancements of machine learning models and algorithms in\nvarious application domains, they are known to be vulnerable to subtle, natural\nor intentionally crafted perturbations in future input data, known as\nadversarial attacks. While numerous adversarial learning methods have been\nproposed, fundamental questions about their statistical optimality in robust\nloss remain largely unanswered. In particular, the minimax rate of convergence\nand the construction of rate-optimal estimators under future $X$-attacks are\nyet to be worked out.\n  In this paper, we address this issue in the context of nonparametric\nregression, under suitable assumptions on the smoothness of the regression\nfunction and the geometric structure of the input perturbation set. We first\nestablish the minimax rate of convergence under adversarial $L_q$-risks with $1\n\\leq q \\leq \\infty$ and propose a piecewise local polynomial estimator that\nachieves the minimax optimality. The established minimax rate elucidates how\nthe smoothness level and perturbation magnitude affect the fundamental limit of\nadversarial learning under future $X$-attacks. Furthermore, we construct a\ndata-driven adaptive estimator that is shown to achieve, within a logarithmic\nfactor, the optimal rate across a broad scale of nonparametric and adversarial\nclasses.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\u7684\u975e\u53c2\u6570\u56de\u5f52\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u5728\u5bf9\u6297\u6027L_q\u98ce\u9669\u4e0b\u7684\u6781\u5c0f\u6781\u5927\u6536\u655b\u901f\u7387\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6700\u4f18\u7684\u5c40\u90e8\u591a\u9879\u5f0f\u4f30\u8ba1\u5668\u3002", "motivation": "\u5c3d\u7ba1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u5de8\u5927\u8fdb\u6b65\uff0c\u4f46\u5b83\u4eec\u5bb9\u6613\u53d7\u5230\u672a\u6765\u7684\u8f93\u5165\u6570\u636e\u4e2d\u81ea\u7136\u6216\u4eba\u4e3a\u6270\u52a8\u7684\u653b\u51fb\u3002\u76ee\u524d\u5bf9\u5bf9\u6297\u6027\u5b66\u4e60\u7684\u7edf\u8ba1\u6700\u4f18\u6027\u4ecd\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\u3002", "method": "\u5728\u975e\u53c2\u6570\u56de\u5f52\u6846\u67b6\u4e0b\uff0c\u5047\u8bbe\u56de\u5f52\u51fd\u6570\u7684\u5e73\u6ed1\u6027\u548c\u8f93\u5165\u6270\u52a8\u96c6\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5efa\u7acb\u5bf9\u6297\u6027L_q\u98ce\u9669\u7684\u6781\u5c0f\u6781\u5927\u6536\u655b\u901f\u7387\uff0c\u5e76\u63d0\u51fa\u5206\u6bb5\u5c40\u90e8\u591a\u9879\u5f0f\u4f30\u8ba1\u5668\u4ee5\u5b9e\u73b0\u6700\u4f18\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5e73\u6ed1\u5ea6\u6c34\u5e73\u548c\u6270\u52a8\u5e45\u5ea6\u663e\u8457\u5f71\u54cd\u5bf9\u6297\u6027\u5b66\u4e60\u7684\u6781\u9650\u6027\u80fd\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u6570\u636e\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u4f30\u8ba1\u5668\uff0c\u80fd\u591f\u5728\u5e7f\u6cdb\u7c7b\u522b\u4e2d\u63a5\u8fd1\u6700\u4f18\u901f\u7387\u3002", "conclusion": "\u672c\u6587\u4e3a\u5bf9\u6297\u6027\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u63ed\u793a\u4e86\u5176\u7edf\u8ba1\u6781\u9650\uff0c\u5e76\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6700\u4f18\u4f30\u8ba1\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "keywords": "\u5bf9\u6297\u5b66\u4e60, \u975e\u53c2\u6570\u56de\u5f52, \u6781\u5c0f\u6781\u5927\u901f\u7387, \u5c40\u90e8\u591a\u9879\u5f0f\u4f30\u8ba1\u5668"}}
{"id": "2506.01538", "pdf": "https://arxiv.org/pdf/2506.01538", "abs": "https://arxiv.org/abs/2506.01538", "authors": ["Guobin Zhu", "Rui Zhou", "Wenkang Ji", "Shiyu Zhao"], "title": "LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative Policy Generation", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted by IEEE Robotics and Automation Letters", "summary": "Although Multi-Agent Reinforcement Learning (MARL) is effective for complex\nmulti-robot tasks, it suffers from low sample efficiency and requires iterative\nmanual reward tuning. Large Language Models (LLMs) have shown promise in\nsingle-robot settings, but their application in multi-robot systems remains\nlargely unexplored. This paper introduces a novel LLM-Aided MARL (LAMARL)\napproach, which integrates MARL with LLMs, significantly enhancing sample\nefficiency without requiring manual design. LAMARL consists of two modules: the\nfirst module leverages LLMs to fully automate the generation of prior policy\nand reward functions. The second module is MARL, which uses the generated\nfunctions to guide robot policy training effectively. On a shape assembly\nbenchmark, both simulation and real-world experiments demonstrate the unique\nadvantages of LAMARL. Ablation studies show that the prior policy improves\nsample efficiency by an average of 185.9% and enhances task completion, while\nstructured prompts based on Chain-of-Thought (CoT) and basic APIs improve LLM\noutput success rates by 28.5%-67.5%. Videos and code are available at\nhttps://guobin-zhu.github.io/LLM-MARL", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLAMARL\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\uff0c\u65e0\u9700\u4eba\u5de5\u8bbe\u8ba1\u3002", "motivation": "MARL\u5728\u590d\u6742\u591a\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6837\u672c\u6548\u7387\u4f4e\u4e14\u9700\u8981\u4eba\u5de5\u8c03\u6574\u5956\u52b1\u3002LLM\u5728\u5355\u673a\u5668\u4eba\u573a\u666f\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "LAMARL\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a1) \u5229\u7528LLM\u81ea\u52a8\u751f\u6210\u5148\u9a8c\u7b56\u7565\u548c\u5956\u52b1\u51fd\u6570\uff1b2) \u4f7f\u7528MARL\u901a\u8fc7\u8fd9\u4e9b\u51fd\u6570\u9ad8\u6548\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\u3002", "result": "\u5728\u5f62\u72b6\u7ec4\u88c5\u4efb\u52a1\u4e2d\uff0cLAMARL\u7684\u6837\u672c\u6548\u7387\u5e73\u5747\u63d0\u5347185.9%\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u63d0\u9ad8\uff1b\u57fa\u4e8eCoT\u548c\u57fa\u7840API\u7684\u7ed3\u6784\u5316\u63d0\u793a\u5c06LLM\u8f93\u51fa\u6210\u529f\u7387\u63d0\u9ad828.5%-67.5%\u3002", "conclusion": "LAMARL\u6709\u6548\u89e3\u51b3\u4e86MARL\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\uff0c\u5c55\u793a\u4e86LLM\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u6837\u672c\u6548\u7387, \u5148\u9a8c\u7b56\u7565, \u5956\u52b1\u51fd\u6570"}}
{"id": "2506.01324", "pdf": "https://arxiv.org/pdf/2506.01324", "abs": "https://arxiv.org/abs/2506.01324", "authors": ["Junghyun Lee", "Yassir Jedra", "Alexandre Prouti\u00e8re", "Se-Young Yun"], "title": "Near-Optimal Clustering in Mixture of Markov Chains", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT", "math.PR"], "comment": "36 pages", "summary": "We study the problem of clustering $T$ trajectories of length $H$, each\ngenerated by one of $K$ unknown ergodic Markov chains over a finite state space\nof size $S$. The goal is to accurately group trajectories according to their\nunderlying generative model. We begin by deriving an instance-dependent,\nhigh-probability lower bound on the clustering error rate, governed by the\nweighted KL divergence between the transition kernels of the chains. We then\npresent a novel two-stage clustering algorithm. In Stage~I, we apply spectral\nclustering using a new injective Euclidean embedding for ergodic Markov chains\n-- a contribution of independent interest that enables sharp concentration\nresults. Stage~II refines the initial clusters via a single step of\nlikelihood-based reassignment. Our method achieves a near-optimal clustering\nerror with high probability, under the conditions $H =\n\\tilde{\\Omega}(\\gamma_{\\mathrm{ps}}^{-1} (S^2 \\vee \\pi_{\\min}^{-1}))$ and $TH =\n\\tilde{\\Omega}(\\gamma_{\\mathrm{ps}}^{-1} S^2 )$, where $\\pi_{\\min}$ is the\nminimum stationary probability of a state across the $K$ chains and\n$\\gamma_{\\mathrm{ps}}$ is the minimum pseudo-spectral gap. These requirements\nprovide significant improvements, if not at least comparable, to the\nstate-of-the-art guarantee (Kausik et al., 2023), and moreover, our algorithm\noffers a key practical advantage: unlike existing approach, it requires no\nprior knowledge of model-specific quantities (e.g., separation between kernels\nor visitation probabilities). We conclude by discussing the inherent gap\nbetween our upper and lower bounds, providing insights into the unique\nstructure of this clustering problem.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5bf9\u7531\u591a\u4e2a\u672a\u77e5\u9a6c\u5c14\u53ef\u592b\u94fe\u751f\u6210\u7684\u957f\u8f68\u8ff9\u8fdb\u884c\u805a\u7c7b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u805a\u7c7b\u8bef\u5dee\u3002", "motivation": "\u76ee\u6807\u662f\u51c6\u786e\u5730\u5c06\u8f68\u8ff9\u6309\u5176\u751f\u6210\u6a21\u578b\u5206\u7ec4\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5148\u9a8c\u77e5\u8bc6\u7684\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u7b97\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u8c31\u805a\u7c7b\u548c\u65b0\u7684\u6b27\u51e0\u91cc\u5f97\u5d4c\u5165\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u4f3c\u7136\u91cd\u5206\u914d\u7ec6\u5316\u805a\u7c7b\u3002", "result": "\u7b97\u6cd5\u5728\u9ad8\u6982\u7387\u4e0b\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u805a\u7c7b\u8bef\u5dee\uff0c\u4e14\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u8be5\u805a\u7c7b\u95ee\u9898\u7684\u72ec\u7279\u7ed3\u6784\uff0c\u5e76\u6307\u51fa\u4e86\u4e0a\u4e0b\u754c\u4e4b\u95f4\u7684\u56fa\u6709\u5dee\u8ddd\u3002", "keywords": "\u8f68\u8ff9\u805a\u7c7b\u3001\u9a6c\u5c14\u53ef\u592b\u94fe\u3001\u8c31\u805a\u7c7b\u3001\u4f2a\u8c31\u95f4\u9699"}}
{"id": "2506.01539", "pdf": "https://arxiv.org/pdf/2506.01539", "abs": "https://arxiv.org/abs/2506.01539", "authors": ["Tianjiao Zhang", "Fei Zhang", "Jiangchao Yao", "Ya Zhang", "Yanfeng Wang"], "title": "G4Seg: Generation for Inexact Segmentation Refinement with Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 12 figures, IEEE International Conference on Multimedia &\n  Expo 2025", "summary": "This paper considers the problem of utilizing a large-scale text-to-image\ndiffusion model to tackle the challenging Inexact Segmentation (IS) task.\nUnlike traditional approaches that rely heavily on discriminative-model-based\nparadigms or dense visual representations derived from internal attention\nmechanisms, our method focuses on the intrinsic generative priors in Stable\nDiffusion~(SD). Specifically, we exploit the pattern discrepancies between\noriginal images and mask-conditional generated images to facilitate a\ncoarse-to-fine segmentation refinement by establishing a semantic\ncorrespondence alignment and updating the foreground probability. Comprehensive\nquantitative and qualitative experiments validate the effectiveness and\nsuperiority of our plug-and-play design, underscoring the potential of\nleveraging generation discrepancies to model dense representations and\nencouraging further exploration of generative approaches for solving\ndiscriminative tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u89e3\u51b3\u4e0d\u7cbe\u786e\u5206\u5272\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u539f\u59cb\u56fe\u50cf\u4e0e\u63a9\u7801\u6761\u4ef6\u751f\u6210\u56fe\u50cf\u7684\u6a21\u5f0f\u5dee\u5f02\uff0c\u5b9e\u73b0\u8bed\u4e49\u5bf9\u9f50\u548c\u524d\u666f\u6982\u7387\u66f4\u65b0\u3002", "motivation": "\u4f20\u7edf\u7684\u5206\u5272\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5224\u522b\u6a21\u578b\u6216\u5bc6\u96c6\u89c6\u89c9\u8868\u793a\uff0c\u800c\u672c\u6587\u63a2\u7d22\u4e86\u7a33\u5b9a\u6269\u6563\uff08SD\uff09\u4e2d\u7684\u751f\u6210\u5148\u9a8c\uff0c\u65e8\u5728\u5229\u7528\u751f\u6210\u5dee\u5f02\u89e3\u51b3\u5224\u522b\u4efb\u52a1\u3002", "method": "\u57fa\u4e8e\u7a33\u5b9a\u6269\u6563\u7684\u751f\u6210\u5148\u9a8c\uff0c\u901a\u8fc7\u5206\u6790\u539f\u59cb\u56fe\u50cf\u4e0e\u63a9\u7801\u6761\u4ef6\u751f\u6210\u56fe\u50cf\u7684\u6a21\u5f0f\u5dee\u5f02\uff0c\u5b9e\u73b0\u8bed\u4e49\u5bf9\u9f50\u548c\u524d\u666f\u6982\u7387\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u5c55\u793a\u4e86\u751f\u6210\u5dee\u5f02\u5728\u5efa\u6a21\u5bc6\u96c6\u8868\u793a\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u751f\u6210\u65b9\u6cd5\u5728\u89e3\u51b3\u5224\u522b\u4efb\u52a1\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u8fd9\u4e00\u65b9\u5411\u3002", "keywords": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b,\u4e0d\u7cbe\u786e\u5206\u5272,\u751f\u6210\u5148\u9a8c,\u8bed\u4e49\u5bf9\u9f50,\u7a33\u5b9a\u6269\u6563"}}
{"id": "2506.01566", "pdf": "https://arxiv.org/pdf/2506.01566", "abs": "https://arxiv.org/abs/2506.01566", "authors": ["Mika Markus M\u00fcller", "Konstantin L\u00fcbeck", "Alexander Louis-Ferdinand Jung", "Jannik Steinmetz", "Oliver Bringmann"], "title": "FlexiSAGA: A Flexible Systolic Array GEMM Accelerator for Sparse and Dense Processing", "categories": ["cs.PF", "cs.AI", "cs.AR", "cs.LG"], "comment": "Accepted Version for: SAMOS XXV", "summary": "Artificial Intelligence (AI) algorithms, such as Deep Neural Networks (DNNs),\nhave become an important tool for a wide range of applications, from computer\nvision to natural language processing. However, the computational complexity of\nDNN inference poses a significant challenge, particularly for processing on\nresource-constrained edge devices. One promising approach to address this\nchallenge is the exploitation of sparsity in DNN operator weights.\n  In this work, we present FlexiSAGA, an architecturally configurable and\ndataflow-flexible AI hardware accelerator for the sparse and dense processing\nof general matrix multiplications (GEMMs). FlexiSAGA supports seven different\nsparse and dense dataflows, enabling efficient processing of resource intensive\nDNN operators. Additionally, we propose a DNN pruning method specifically\ntailored towards the FlexiSAGA architecture, allowing for near-optimal\nprocessing of dense and sparse convolution and fully-connected operators,\nfacilitating a DNN/HW co-design flow. Our results show a whole DNN\nsparse-over-dense inference speedup ranging from 1.41 up to 4.28, outperforming\ncommercial and literature-reported accelerator platforms.", "AI": {"tldr": "FlexiSAGA \u662f\u4e00\u79cd\u53ef\u914d\u7f6e\u7684 AI \u786c\u4ef6\u52a0\u901f\u5668\uff0c\u652f\u6301\u7a00\u758f\u548c\u5bc6\u96c6\u6570\u636e\u6d41\u5904\u7406\uff0c\u901a\u8fc7\u7279\u5b9a\u526a\u679d\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548 DNN \u5904\u7406\u3002\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u52a0\u901f\u5668\u5e73\u53f0\u3002", "motivation": "\u89e3\u51b3 DNN \u63a8\u7406\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u6311\u6218\uff0c\u5229\u7528\u7a00\u758f\u6027\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u63d0\u51fa FlexiSAGA \u786c\u4ef6\u52a0\u901f\u5668\u53ca\u5176\u914d\u5957\u7684\u526a\u679d\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u79cd\u6570\u636e\u6d41\u5904\u7406\u7a00\u758f\u548c\u5bc6\u96c6 GEMM\u3002", "result": "\u5b9e\u73b0 1.41 \u81f3 4.28 \u500d\u7a00\u758f\u5bf9\u5bc6\u96c6\u63a8\u7406\u52a0\u901f\uff0c\u4f18\u4e8e\u5546\u4e1a\u548c\u6587\u732e\u62a5\u9053\u7684\u5e73\u53f0\u3002", "conclusion": "FlexiSAGA \u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u52a0\u901f\u5668\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u3002", "keywords": "AI, DNN, \u7a00\u758f\u6027, \u786c\u4ef6\u52a0\u901f\u5668, GEMM, \u526a\u679d"}}
{"id": "2506.01572", "pdf": "https://arxiv.org/pdf/2506.01572", "abs": "https://arxiv.org/abs/2506.01572", "authors": ["Iqra Yousaf", "Aqsa Yousaf"], "title": "Advanced Nanostructured Topical Therapeutics for Psoriasis: Strategic Synthesis, Multimodal Characterization, and Preliminary Pharmacodynamic Profiling", "categories": ["physics.med-ph", "cs.AI", "physics.bio-ph", "92C50", "J.3; I.4.5; J.2"], "comment": "24 pages", "summary": "Psoriasis is a long-term inflammatory skin disease that remains difficult to\ntreat. In this study, we developed a new topical treatment by combining metal\noxide nanoparticles: cerium oxide (CeO2), zinc oxide (ZnO), and silver (Ag),\nwith natural plant extracts in a gel made from fish collagen and agar. The\nnanoparticles were characterized using UV-Vis spectroscopy, dynamic light\nscattering (DLS), Fourier-transform infrared spectroscopy (FTIR), and scanning\nelectron microscopy (SEM), showing good stability and a uniform particle size\ndistribution (ZnO averaged 66 nm).\n  To enhance therapeutic potential, the gel was enriched with plant-derived\nantioxidants from bitter melon, ginger, and neem. This formulation was tested\non an animal model of psoriasis. The treated group exhibited faster wound\nhealing and reduced inflammation compared to both placebo and untreated groups,\nwith statistically significant results (p < 0.01 to p < 0.001) observed from\nDay 3, becoming more pronounced by Day 14.\n  These results indicate that the combination of nanoparticles with plant-based\ncomponents in a topical gel may provide a promising new approach to psoriasis\ntreatment. Further studies are recommended to evaluate long-term safety and\ntherapeutic effectiveness.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u91d1\u5c5e\u6c27\u5316\u7269\u7eb3\u7c73\u9897\u7c92\u4e0e\u690d\u7269\u63d0\u53d6\u7269\u7684\u65b0\u578b\u5c40\u90e8\u51dd\u80f6\u6cbb\u7597\u94f6\u5c51\u75c5\uff0c\u52a8\u7269\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u7684\u6cbb\u7597\u6548\u679c\u3002", "motivation": "\u94f6\u5c51\u75c5\u662f\u4e00\u79cd\u96be\u4ee5\u6cbb\u7597\u7684\u6162\u6027\u708e\u75c7\u6027\u76ae\u80a4\u75be\u75c5\uff0c\u9700\u8981\u63a2\u7d22\u65b0\u7684\u6cbb\u7597\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u6c27\u5316\u94c8\u3001\u6c27\u5316\u950c\u548c\u94f6\u7eb3\u7c73\u9897\u7c92\u4e0e\u82e6\u74dc\u3001\u59dc\u548c\u5370\u695d\u7b49\u690d\u7269\u63d0\u53d6\u7269\uff0c\u5236\u5907\u4e86\u9c7c\u80f6\u539f\u548c\u743c\u8102\u51dd\u80f6\u3002\u7eb3\u7c73\u9897\u7c92\u901a\u8fc7\u591a\u79cd\u6280\u672f\u8868\u5f81\uff0c\u5e76\u5728\u94f6\u5c51\u75c5\u52a8\u7269\u6a21\u578b\u4e2d\u6d4b\u8bd5\u6cbb\u7597\u6548\u679c\u3002", "result": "\u6cbb\u7597\u7ec4\u8868\u73b0\u51fa\u66f4\u5feb\u7684\u4f24\u53e3\u6108\u5408\u548c\u708e\u75c7\u51cf\u5c11\uff0c\u7edf\u8ba1\u7ed3\u679c\u663e\u8457\u4f18\u4e8e\u5bf9\u7167\u7ec4\u3002", "conclusion": "\u7eb3\u7c73\u9897\u7c92\u4e0e\u690d\u7269\u6210\u5206\u7ed3\u5408\u7684\u5c40\u90e8\u51dd\u80f6\u53ef\u80fd\u662f\u6cbb\u7597\u94f6\u5c51\u75c5\u7684\u6709\u524d\u666f\u7684\u65b0\u65b9\u6cd5\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u957f\u671f\u5b89\u5168\u6027\u548c\u7597\u6548\u3002", "keywords": "\u94f6\u5c51\u75c5;\u91d1\u5c5e\u6c27\u5316\u7269\u7eb3\u7c73\u9897\u7c92;\u690d\u7269\u63d0\u53d6\u7269;\u5c40\u90e8\u6cbb\u7597;\u52a8\u7269\u6a21\u578b"}}
{"id": "2506.01378", "pdf": "https://arxiv.org/pdf/2506.01378", "abs": "https://arxiv.org/abs/2506.01378", "authors": ["K\u00fcr\u015fat Tekb\u0131y\u0131k", "Amir Hossein Fahim Raouf", "\u0130smail G\u00fcven\u00e7", "Mingzhe Chen", "G\u00fcne\u015f Karabulut Kurt", "Antoine Lesage-Landry"], "title": "From Turbulence to Tranquility: AI-Driven Low-Altitude Network", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Low Altitude Economy (LAE) networks own transformative potential in urban\nmobility, emergency response, and aerial logistics. However, these networks\nface significant challenges in spectrum management, interference mitigation,\nand real-time coordination across dynamic and resource-constrained\nenvironments. After addressing these challenges, this study explores three core\nelements for enabling intelligent LAE networks as follows machine\nlearning-based spectrum sensing and coexistence, artificial intelligence\n(AI)-optimized resource allocation and trajectory planning, and testbed-driven\nvalidation and standardization. We highlight how federated and reinforcement\nlearning techniques support decentralized, adaptive decision-making under\nmobility and energy constraints. In addition, we discuss the role of real-world\nplatforms such as AERPAW in bridging the gap between simulation and deployment\nand enabling iterative system refinement under realistic conditions. This study\naims to provide a forward-looking roadmap toward developing efficient and\ninteroperable AI-driven LAE ecosystems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u7684\u667a\u80fd\u5316\u53d1\u5c55\u8def\u5f84\uff0c\u91cd\u70b9\u89e3\u51b3\u9891\u8c31\u7ba1\u7406\u3001\u5e72\u6270\u7f13\u89e3\u548c\u5b9e\u65f6\u534f\u8c03\u7b49\u6311\u6218\u3002", "motivation": "\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u5728\u63d0\u5347\u57ce\u5e02\u6d41\u52a8\u6027\u3001\u5e94\u6025\u54cd\u5e94\u548c\u7a7a\u4e2d\u7269\u6d41\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u9891\u8c31\u7ba1\u7406\u548c\u5b9e\u65f6\u534f\u8c03\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528\u673a\u5668\u5b66\u4e60\u8fdb\u884c\u9891\u8c31\u611f\u77e5\u4e0e\u5171\u5b58\u3001AI\u4f18\u5316\u8d44\u6e90\u5206\u914d\u548c\u8f68\u8ff9\u89c4\u5212\uff0c\u5e76\u901a\u8fc7\u6d4b\u8bd5\u5e73\u53f0\u9a8c\u8bc1\u6807\u51c6\u5316\u3002", "result": "\u8054\u90a6\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u652f\u6301\u53bb\u4e2d\u5fc3\u5316\u3001\u9002\u5e94\u6027\u7684\u51b3\u7b56\u673a\u5236\uff0cAERPAW\u7b49\u5e73\u53f0\u8fde\u63a5\u4eff\u771f\u4e0e\u5b9e\u9645\u90e8\u7f72\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u9ad8\u6548\u3001\u4e92\u64cd\u4f5c\u7684AI\u9a71\u52a8\u4f4e\u7a7a\u7ecf\u6d4e\u751f\u6001\u7cfb\u7edf\u7684\u524d\u77bb\u6027\u53d1\u5c55\u8def\u7ebf\u3002", "keywords": "\u4f4e\u7a7a\u7ecf\u6d4e, \u673a\u5668\u5b66\u4e60, \u4eba\u5de5\u667a\u80fd, \u8d44\u6e90\u5206\u914d, \u8054\u90a6\u5b66\u4e60"}}
{"id": "2506.01583", "pdf": "https://arxiv.org/pdf/2506.01583", "abs": "https://arxiv.org/abs/2506.01583", "authors": ["Yiming Zhong", "Yumeng Liu", "Chuyang Xiao", "Zemin Yang", "Youzhuo Wang", "Yufei Zhu", "Ye Shi", "Yujing Sun", "Xinge Zhu", "Yuexin Ma"], "title": "FreqPolicy: Frequency Autoregressive Visuomotor Policy with Continuous Tokens", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Learning effective visuomotor policies for robotic manipulation is\nchallenging, as it requires generating precise actions while maintaining\ncomputational efficiency. Existing methods remain unsatisfactory due to\ninherent limitations in the essential action representation and the basic\nnetwork architectures. We observe that representing actions in the frequency\ndomain captures the structured nature of motion more effectively: low-frequency\ncomponents reflect global movement patterns, while high-frequency components\nencode fine local details. Additionally, robotic manipulation tasks of varying\ncomplexity demand different levels of modeling precision across these frequency\nbands. Motivated by this, we propose a novel paradigm for visuomotor policy\nlearning that progressively models hierarchical frequency components. To\nfurther enhance precision, we introduce continuous latent representations that\nmaintain smoothness and continuity in the action space. Extensive experiments\nacross diverse 2D and 3D robotic manipulation benchmarks demonstrate that our\napproach outperforms existing methods in both accuracy and efficiency,\nshowcasing the potential of a frequency-domain autoregressive framework with\ncontinuous tokens for generalized robotic manipulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u9891\u7387\u57df\u4e2d\u5efa\u6a21\u52a8\u4f5c\u7684\u5c42\u6b21\u5316\u9891\u7387\u7ec4\u4ef6\uff0c\u5e76\u7ed3\u5408\u8fde\u7eed\u6f5c\u5728\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\u56e0\u52a8\u4f5c\u8868\u793a\u548c\u7f51\u7edc\u67b6\u6784\u7684\u56fa\u6709\u5c40\u9650\u6027\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u901a\u8fc7\u89c2\u5bdf\u53d1\u73b0\u9891\u7387\u57df\u4e2d\u7684\u52a8\u4f5c\u8868\u793a\u80fd\u66f4\u6709\u6548\u5730\u6355\u6349\u52a8\u4f5c\u7684\u5168\u5c40\u548c\u5c40\u90e8\u7ec6\u8282\uff0c\u4ece\u800c\u9a71\u52a8\u4e86\u65b0\u65b9\u6cd5\u7684\u63d0\u51fa\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u5efa\u6a21\u5c42\u6b21\u5316\u9891\u7387\u7ec4\u4ef6\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60\u8303\u5f0f\uff0c\u5e76\u5f15\u5165\u8fde\u7eed\u6f5c\u5728\u8868\u793a\u4ee5\u4fdd\u6301\u52a8\u4f5c\u7a7a\u95f4\u7684\u5e73\u6ed1\u6027\u548c\u8fde\u7eed\u6027\u3002", "result": "\u5728\u591a\u79cd2D\u548c3D\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u9891\u7387\u57df\u81ea\u56de\u5f52\u6846\u67b6\u4e0e\u8fde\u7eed\u6807\u8bb0\u7684\u7ed3\u5408\u5728\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002", "keywords": "\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u3001\u9891\u7387\u57df\u3001\u673a\u5668\u4eba\u64cd\u4f5c\u3001\u81ea\u56de\u5f52\u6846\u67b6\u3001\u8fde\u7eed\u6f5c\u5728\u8868\u793a"}}
{"id": "2506.01600", "pdf": "https://arxiv.org/pdf/2506.01600", "abs": "https://arxiv.org/abs/2506.01600", "authors": ["Tenny Yin", "Zhiting Mei", "Tao Sun", "Lihan Zha", "Emily Zhou", "Jeremy Bao", "Miyu Yamane", "Ola Shorinwa", "Anirudha Majumdar"], "title": "WoMAP: World Models For Embodied Open-Vocabulary Object Localization", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Language-instructed active object localization is a critical challenge for\nrobots, requiring efficient exploration of partially observable environments.\nHowever, state-of-the-art approaches either struggle to generalize beyond\ndemonstration datasets (e.g., imitation learning methods) or fail to generate\nphysically grounded actions (e.g., VLMs). To address these limitations, we\nintroduce WoMAP (World Models for Active Perception): a recipe for training\nopen-vocabulary object localization policies that: (i) uses a Gaussian\nSplatting-based real-to-sim-to-real pipeline for scalable data generation\nwithout the need for expert demonstrations, (ii) distills dense rewards signals\nfrom open-vocabulary object detectors, and (iii) leverages a latent world model\nfor dynamics and rewards prediction to ground high-level action proposals at\ninference time. Rigorous simulation and hardware experiments demonstrate\nWoMAP's superior performance in a broad range of zero-shot object localization\ntasks, with more than 9x and 2x higher success rates compared to VLM and\ndiffusion policy baselines, respectively. Further, we show that WoMAP achieves\nstrong generalization and sim-to-real transfer on a TidyBot.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86WoMAP\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65afSplatting\u548c\u6f5c\u5728\u4e16\u754c\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u8bed\u8a00\u6307\u4ee4\u4e0b\u7269\u4f53\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u6027\u548c\u7269\u7406\u52a8\u4f5c\u751f\u6210\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u8a00\u6307\u4ee4\u7269\u4f53\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u6cdb\u5316\u6027\u4e0d\u8db3\u548c\u7269\u7406\u52a8\u4f5c\u751f\u6210\u4e0d\u7406\u60f3\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9ad8\u65afSplatting\u751f\u6210\u6570\u636e\u96c6\uff0c\u65e0\u9700\u4e13\u5bb6\u793a\u8303\uff1b\u901a\u8fc7\u5f00\u653e\u8bcd\u6c47\u7269\u4f53\u68c0\u6d4b\u5668\u63d0\u53d6\u5bc6\u96c6\u5956\u52b1\u4fe1\u53f7\uff1b\u5229\u7528\u6f5c\u5728\u4e16\u754c\u6a21\u578b\u9884\u6d4b\u52a8\u529b\u5b66\u548c\u5956\u52b1\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0cWoMAP\u5728\u96f6\u6837\u672c\u7269\u4f53\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u7387\u4e3a\u57fa\u7ebf\u65b9\u6cd5\u76849\u500d\u548c2\u500d\u4ee5\u4e0a\u3002", "conclusion": "WoMAP\u5728\u6cdb\u5316\u6027\u548c\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u4e2d\u8868\u73b0\u5f3a\u5927\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u3002", "keywords": "\u8bed\u8a00\u6307\u4ee4, \u7269\u4f53\u5b9a\u4f4d, \u9ad8\u65afSplatting, \u6f5c\u5728\u4e16\u754c\u6a21\u578b, \u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb"}}
{"id": "2506.01493", "pdf": "https://arxiv.org/pdf/2506.01493", "abs": "https://arxiv.org/abs/2506.01493", "authors": ["Yuya Kobayashi", "Yuhta Takida", "Takashi Shibuya", "Yuki Mitsufuji"], "title": "Efficiency without Compromise: CLIP-aided Text-to-Image GANs with Increased Diversity", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at IJCNN 2025", "summary": "Recently, Generative Adversarial Networks (GANs) have been successfully\nscaled to billion-scale large text-to-image datasets. However, training such\nmodels entails a high training cost, limiting some applications and research\nusage. To reduce the cost, one promising direction is the incorporation of\npre-trained models. The existing method of utilizing pre-trained models for a\ngenerator significantly reduced the training cost compared with the other\nlarge-scale GANs, but we found the model loses the diversity of generation for\na given prompt by a large margin. To build an efficient and high-fidelity\ntext-to-image GAN without compromise, we propose to use two specialized\ndiscriminators with Slicing Adversarial Networks (SANs) adapted for\ntext-to-image tasks. Our proposed model, called SCAD, shows a notable\nenhancement in diversity for a given prompt with better sample fidelity. We\nalso propose to use a metric called Per-Prompt Diversity (PPD) to evaluate the\ndiversity of text-to-image models quantitatively. SCAD achieved a zero-shot FID\ncompetitive with the latest large-scale GANs at two orders of magnitude less\ntraining cost.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSCAD\u7684\u9ad8\u6548\u6587\u672c\u5230\u56fe\u50cfGAN\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u4e13\u7528\u9274\u522b\u5668\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u5347\u4e86\u751f\u6210\u591a\u6837\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cfGAN\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u751f\u6210\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u7814\u7a76\u4eba\u5458\u5e0c\u671b\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u4e13\u7528\u9274\u522b\u5668\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u4e13\u7528\u9274\u522b\u5668\u548c\u5207\u7247\u5bf9\u6297\u7f51\u7edc\uff08SANs\uff09\u6784\u5efaSCAD\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86Per-Prompt Diversity\uff08PPD\uff09\u6307\u6807\u3002", "result": "SCAD\u5728\u8bad\u7ec3\u6210\u672c\u5927\u5e45\u964d\u4f4e\u7684\u540c\u65f6\uff0c\u751f\u6210\u591a\u6837\u6027\u548c\u6837\u672c\u4fdd\u771f\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u96f6\u6837\u672cFID\u4e0e\u6700\u65b0\u5927\u89c4\u6a21GAN\u7ade\u4e89\u3002", "conclusion": "SCAD\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u548c\u9ad8\u4fdd\u771f\u5ea6\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002", "keywords": "GAN, \u6587\u672c\u5230\u56fe\u50cf, \u9884\u8bad\u7ec3\u6a21\u578b, \u591a\u6837\u6027, \u8bad\u7ec3\u6210\u672c"}}
{"id": "2506.01608", "pdf": "https://arxiv.org/pdf/2506.01608", "abs": "https://arxiv.org/abs/2506.01608", "authors": ["Andy Bonnetto", "Haozhe Qi", "Franklin Leong", "Matea Tashkovska", "Mahdi Rad", "Solaiman Shokur", "Friedhelm Hummel", "Silvestro Micera", "Marc Pollefeys", "Alexander Mathis"], "title": "EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models", "categories": ["cs.CV", "cs.AI", "cs.LG", "q-bio.OT"], "comment": "Code and data at: https://github.com/amathislab/EPFL-Smart-Kitchen", "summary": "Understanding behavior requires datasets that capture humans while carrying\nout complex tasks. The kitchen is an excellent environment for assessing human\nmotor and cognitive function, as many complex actions are naturally exhibited\nin kitchens from chopping to cleaning. Here, we introduce the\nEPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion capture\nplatform inside a kitchen environment. Nine static RGB-D cameras, inertial\nmeasurement units (IMUs) and one head-mounted HoloLens~2 headset were used to\ncapture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset is\na multi-view action dataset with synchronized exocentric, egocentric, depth,\nIMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjects\ncooking four different recipes. Action sequences were densely annotated with\n33.78 action segments per minute. Leveraging this multi-modal dataset, we\npropose four benchmarks to advance behavior understanding and modeling through\n1) a vision-language benchmark, 2) a semantic text-to-motion generation\nbenchmark, 3) a multi-modal action recognition benchmark, 4) a pose-based\naction segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset to\npave the way for better methods as well as insights to understand the nature of\necologically-valid human behavior. Code and data are available at\nhttps://github.com/amathislab/EPFL-Smart-Kitchen", "AI": {"tldr": "EPFL-Smart-Kitchen-30\u6570\u636e\u96c6\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u53a8\u623f\u884c\u4e3a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u4eba\u7c7b\u8fd0\u52a8\u548c\u8ba4\u77e5\u529f\u80fd\uff0c\u5305\u542b\u591a\u89c6\u89d2\u52a8\u4f5c\u6355\u6349\u548c\u5bc6\u96c6\u6807\u6ce8\uff0c\u63d0\u51fa\u4e86\u56db\u4e2a\u57fa\u51c6\u4efb\u52a1\u4ee5\u63a8\u52a8\u884c\u4e3a\u7406\u89e3\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u4eba\u7c7b\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u70f9\u996a\uff09\u4e2d\u7684\u884c\u4e3a\u548c\u8ba4\u77e5\u529f\u80fd\uff0c\u9700\u8981\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u96c6\u6765\u6355\u83b7\u591a\u6a21\u6001\u52a8\u4f5c\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u4e5d\u53f0\u9759\u6001RGB-D\u76f8\u673a\u3001IMUs\u548cHoloLens~2\u5934\u663e\uff0c\u6355\u83b716\u540d\u88ab\u8bd5\u5728\u53a8\u623f\u73af\u5883\u4e2d\u76843D\u52a8\u4f5c\u3001\u89c6\u7ebf\u548c\u8eab\u4f53\u8fd0\u52a8\uff0c\u5171\u8ba129.7\u5c0f\u65f6\u6570\u636e\uff0c\u5e76\u5bc6\u96c6\u6807\u6ce8\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u6570\u636e\u96c6EPFL-Smart-Kitchen-30\uff0c\u5e76\u8bbe\u8ba1\u4e86\u56db\u4e2a\u57fa\u51c6\u4efb\u52a1\uff08\u5982\u89c6\u89c9-\u8bed\u8a00\u57fa\u51c6\u3001\u52a8\u4f5c\u8bc6\u522b\u57fa\u51c6\u7b49\uff09\uff0c\u4e3a\u884c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u7814\u7a76\u751f\u6001\u6709\u6548\u7684\u4eba\u7c7b\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e30\u5bcc\u8d44\u6e90\uff0c\u6709\u671b\u63a8\u52a8\u884c\u4e3a\u7406\u89e3\u548c\u5efa\u6a21\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "keywords": "\u884c\u4e3a\u7406\u89e3\uff0c\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u52a8\u4f5c\u6355\u6349\uff0c\u53a8\u623f\u73af\u5883\uff0c\u57fa\u51c6\u4efb\u52a1"}}
{"id": "2506.01497", "pdf": "https://arxiv.org/pdf/2506.01497", "abs": "https://arxiv.org/abs/2506.01497", "authors": ["Stefan Uhlich", "Andrea Bonetti", "Arun Venkitaraman", "Chia-Yu Hsieh", "Mustafa Emre G\u00fcrsoy", "Ryoga Matsuo", "Lorenzo Servadei"], "title": "SpiceMixer -- Netlist-Level Circuit Evolution", "categories": ["cs.NE", "cs.AR", "cs.LG", "B.7.0"], "comment": null, "summary": "This paper introduces SpiceMixer, a genetic algorithm developed to synthesize\nnovel analog circuits by evolving SPICE netlists. Unlike conventional methods,\nSpiceMixer operates directly on netlist lines, enabling compatibility with any\ncomponent or subcircuit type and supporting general-purpose genetic operations.\nBy using a normalized netlist format, the algorithm enhances the effectiveness\nof its genetic operators: crossover, mutation, and pruning. We show that\nSpiceMixer achieves superior performance in synthesizing standard cells\n(inverter, two-input NAND, and latch) and in designing an analog classifier\ncircuit for the Iris dataset, reaching an accuracy of 89% on the test set.\nAcross all evaluated tasks, SpiceMixer consistently outperforms existing\nsynthesis methods.", "AI": {"tldr": "SpiceMixer \u662f\u4e00\u79cd\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u901a\u8fc7\u6f14\u5316 SPICE \u7f51\u8868\u6765\u5408\u6210\u65b0\u9896\u7684\u6a21\u62df\u7535\u8def\u3002\u4e0e\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0c\u5b83\u76f4\u63a5\u64cd\u4f5c\u7f51\u8868\uff0c\u652f\u6301\u901a\u7528\u9057\u4f20\u64cd\u4f5c\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u64cd\u4f5c SPICE \u7f51\u8868\u7684\u9057\u4f20\u7b97\u6cd5\uff0c\u4ee5\u63d0\u5347\u6a21\u62df\u7535\u8def\u5408\u6210\u7684\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6807\u51c6\u5316\u7f51\u8868\u683c\u5f0f\uff0c\u7ed3\u5408\u4ea4\u53c9\u3001\u53d8\u5f02\u548c\u526a\u679d\u7b49\u9057\u4f20\u64cd\u4f5c\u5668\uff0c\u5b9e\u73b0\u5bf9\u5404\u79cd\u7ec4\u4ef6\u7684\u517c\u5bb9\u6027\u3002", "result": "\u5728\u6807\u51c6\u5355\u5143\uff08\u5982\u53cd\u76f8\u5668\u3001\u4e8c\u8f93\u5165 NAND\u3001\u9501\u5b58\u5668\uff09\u548c Iris \u6570\u636e\u96c6\u6a21\u62df\u5206\u7c7b\u7535\u8def\u7684\u8bbe\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6d4b\u8bd5\u96c6\u51c6\u786e\u7387\u8fbe\u523089%\u3002", "conclusion": "SpiceMixer \u5728\u6a21\u62df\u7535\u8def\u5408\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86\u5176\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "keywords": "SpiceMixer, \u9057\u4f20\u7b97\u6cd5, SPICE \u7f51\u8868, \u6a21\u62df\u7535\u8def, \u7535\u8def\u5408\u6210"}}
{"id": "2506.01618", "pdf": "https://arxiv.org/pdf/2506.01618", "abs": "https://arxiv.org/abs/2506.01618", "authors": ["Karl El Hajal", "Enno Hermann", "Sevada Hovsepyan", "Mathew Magimai. -Doss"], "title": "Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric Speech", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "comment": "Accepted at Interspeech 2025", "summary": "Automatic speech recognition (ASR) systems struggle with dysarthric speech\ndue to high inter-speaker variability and slow speaking rates. To address this,\nwe explore dysarthric-to-healthy speech conversion for improved ASR\nperformance. Our approach extends the Rhythm and Voice (RnV) conversion\nframework by introducing a syllable-based rhythm modeling method suited for\ndysarthric speech. We assess its impact on ASR by training LF-MMI models and\nfine-tuning Whisper on converted speech. Experiments on the Torgo corpus reveal\nthat LF-MMI achieves significant word error rate reductions, especially for\nmore severe cases of dysarthria, while fine-tuning Whisper on converted data\nhas minimal effect on its performance. These results highlight the potential of\nunsupervised rhythm and voice conversion for dysarthric ASR. Code available at:\nhttps://github.com/idiap/RnV", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u8bed\u97f3\u8f6c\u6362\u6280\u672f\u6539\u5584\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5bf9\u6784\u97f3\u969c\u788d\u8bed\u97f3\u7684\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u6784\u97f3\u969c\u788d\u8bed\u97f3\u7684\u9ad8\u53d8\u5f02\u6027\u548c\u6162\u8bed\u901f\u5bfc\u81f4\u73b0\u6709ASR\u7cfb\u7edf\u8868\u73b0\u4e0d\u4f73\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u8bed\u97f3\u8f6c\u6362\u6280\u672f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6269\u5c55\u4e86Rhythm and Voice\u8f6c\u6362\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u9002\u5408\u6784\u97f3\u969c\u788d\u8bed\u97f3\u7684\u97f3\u8282\u8282\u594f\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u8f6c\u6362\u540e\u7684\u8bed\u97f3\u8bad\u7ec3\u4e86LF-MMI\u6a21\u578b\u548c\u5fae\u8c03\u4e86Whisper\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLF-MMI\u6a21\u578b\u5728\u8f6c\u6362\u8bed\u97f3\u4e0a\u663e\u8457\u964d\u4f4e\u4e86\u8bcd\u9519\u8bef\u7387\uff0c\u7279\u522b\u662f\u5728\u4e25\u91cd\u6784\u97f3\u969c\u788d\u6848\u4f8b\u4e2d\uff0c\u800c\u5fae\u8c03Whisper\u7684\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u65e0\u76d1\u7763\u7684\u8282\u594f\u548c\u8bed\u97f3\u8f6c\u6362\u6280\u672f\u5728\u6784\u97f3\u969c\u788dASR\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "keywords": "\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b,\u6784\u97f3\u969c\u788d,\u8bed\u97f3\u8f6c\u6362,\u8282\u594f\u5efa\u6a21,LF-MMI,Whisper"}}
{"id": "2506.01586", "pdf": "https://arxiv.org/pdf/2506.01586", "abs": "https://arxiv.org/abs/2506.01586", "authors": ["Zhuohang Dang", "Minnan Luo", "Chengyou Jia", "Hangwei Qian", "Xiaojun Chang", "Ivor W. Tsang"], "title": "Multi-Modal Dataset Distillation in the Wild", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent multi-modal models have shown remarkable versatility in real-world\napplications. However, their rapid development encounters two critical data\nchallenges. First, the training process requires large-scale datasets, leading\nto substantial storage and computational costs. Second, these data are\ntypically web-crawled with inevitable noise, i.e., partially mismatched pairs,\nseverely degrading model performance. To these ends, we propose Multi-modal\ndataset Distillation in the Wild, i.e., MDW, the first framework to distill\nnoisy multi-modal datasets into compact clean ones for effective and efficient\nmodel training. Specifically, MDW introduces learnable fine-grained\ncorrespondences during distillation and adaptively optimizes distilled data to\nemphasize correspondence-discriminative regions, thereby enhancing distilled\ndata's information density and efficacy. Moreover, to capture robust\ncross-modal correspondence prior knowledge from real data, MDW proposes\ndual-track collaborative learning to avoid the risky data noise, alleviating\ninformation loss with certifiable noise tolerance. Extensive experiments\nvalidate MDW's theoretical and empirical efficacy with remarkable scalability,\nsurpassing prior methods by over 15% across various compression ratios,\nhighlighting its appealing practicality for applications with diverse efficacy\nand resource needs.", "AI": {"tldr": "MDW\u6846\u67b6\u901a\u8fc7\u84b8\u998f\u591a\u6a21\u6001\u6570\u636e\u89e3\u51b3\u5927\u89c4\u6a21\u8bad\u7ec3\u5e26\u6765\u7684\u5b58\u50a8\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u540c\u65f6\u4f18\u5316\u566a\u58f0\u6570\u636e\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u8bad\u7ec3\u9762\u4e34\u5927\u89c4\u6a21\u6570\u636e\u5b58\u50a8\u6210\u672c\u9ad8\u548c\u566a\u58f0\u6570\u636e\u964d\u4f4e\u6027\u80fd\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faMDW\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u7ec6\u7c92\u5ea6\u5bf9\u5e94\u5173\u7cfb\u548c\u53cc\u8f68\u534f\u540c\u5b66\u4e60\uff0c\u84b8\u998f\u6570\u636e\u5e76\u63d0\u5347\u4fe1\u606f\u5bc6\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMDW\u5728\u591a\u79cd\u538b\u7f29\u6bd4\u4e0b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd515%\u4ee5\u4e0a\uff0c\u5177\u6709\u5f3a\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "MDW\u6709\u6548\u89e3\u51b3\u6570\u636e\u89c4\u6a21\u548c\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u3002", "keywords": "\u591a\u6a21\u6001\u6570\u636e, \u6570\u636e\u96c6\u84b8\u998f, \u566a\u58f0\u5bb9\u5fcd, \u7ec6\u7c92\u5ea6\u5bf9\u5e94"}}
{"id": "2506.01659", "pdf": "https://arxiv.org/pdf/2506.01659", "abs": "https://arxiv.org/abs/2506.01659", "authors": ["Daniel Szelogowski"], "title": "Engram Memory Encoding and Retrieval: A Neurocomputational Perspective", "categories": ["cs.NE", "cs.AI", "cs.IR", "cs.LG", "q-bio.NC", "I.2.0; I.2.4; I.2.6; I.2.m; E.1; E.2; E.4; H.3; J.3; J.4"], "comment": "18 pages, 7 figures, 3 tables", "summary": "Despite substantial research into the biological basis of memory, the precise\nmechanisms by which experiences are encoded, stored, and retrieved in the brain\nremain incompletely understood. A growing body of evidence supports the engram\ntheory, which posits that sparse populations of neurons undergo lasting\nphysical and biochemical changes to support long-term memory. Yet, a\ncomprehensive computational framework that integrates biological findings with\nmechanistic models remains elusive. This work synthesizes insights from\ncellular neuroscience and computational modeling to address key challenges in\nengram research: how engram neurons are identified and manipulated; how\nsynaptic plasticity mechanisms contribute to stable memory traces; and how\nsparsity promotes efficient, interference-resistant representations. Relevant\ncomputational approaches -- such as sparse regularization, engram gating, and\nbiologically inspired architectures like Sparse Distributed Memory and spiking\nneural networks -- are also examined. Together, these findings suggest that\nmemory efficiency, capacity, and stability emerge from the interaction of\nplasticity and sparsity constraints. By integrating neurobiological and\ncomputational perspectives, this paper provides a comprehensive theoretical\nfoundation for engram research and proposes a roadmap for future inquiry into\nthe mechanisms underlying memory, with implications for the diagnosis and\ntreatment of memory-related disorders.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6574\u5408\u4e86\u7ec6\u80de\u795e\u7ecf\u79d1\u5b66\u548c\u8ba1\u7b97\u5efa\u6a21\u7684\u89c1\u89e3\uff0c\u63a2\u8ba8\u4e86\u8bb0\u5fc6\u5370\u8bb0\uff08engram\uff09\u7814\u7a76\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u8bb0\u5fc6\u673a\u5236\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u5bf9\u8bb0\u5fc6\u7684\u751f\u7269\u5b66\u57fa\u7840\u8fdb\u884c\u4e86\u5927\u91cf\u7814\u7a76\uff0c\u4f46\u5927\u8111\u4e2d\u7ecf\u9a8c\u7f16\u7801\u3001\u5b58\u50a8\u548c\u68c0\u7d22\u7684\u7cbe\u786e\u673a\u5236\u4ecd\u4e0d\u5b8c\u5168\u6e05\u695a\u3002\u9700\u8981\u7efc\u5408\u751f\u7269\u5b66\u53d1\u73b0\u548c\u8ba1\u7b97\u6a21\u578b\u6765\u7406\u89e3\u8bb0\u5fc6\u5370\u8bb0\u7684\u4f5c\u7528\u3002", "method": "\u7ed3\u5408\u7ec6\u80de\u795e\u7ecf\u79d1\u5b66\u7684\u5b9e\u9a8c\u53d1\u73b0\u548c\u8ba1\u7b97\u5efa\u6a21\uff08\u5982\u7a00\u758f\u6b63\u5219\u5316\u3001\u8bb0\u5fc6\u5370\u8bb0\u95e8\u63a7\u3001\u7a00\u758f\u5206\u5e03\u5f0f\u8bb0\u5fc6\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7b49\uff09\uff0c\u5206\u6790\u8bb0\u5fc6\u5370\u8bb0\u7684\u673a\u5236\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bb0\u5fc6\u7684\u6548\u7387\u3001\u5bb9\u91cf\u548c\u7a33\u5b9a\u6027\u6e90\u4e8e\u53ef\u5851\u6027\u548c\u7a00\u758f\u6027\u7ea6\u675f\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u7efc\u5408\u795e\u7ecf\u751f\u7269\u548c\u8ba1\u7b97\u89c6\u89d2\u4e3a\u8bb0\u5fc6\u5370\u8bb0\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u8bba\u6587\u4e3a\u8bb0\u5fc6\u5370\u8bb0\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\uff0c\u5bf9\u8bb0\u5fc6\u76f8\u5173\u75be\u75c5\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "keywords": "\u8bb0\u5fc6\u5370\u8bb0\uff08engram\uff09, \u7a00\u758f\u6027, \u53ef\u5851\u6027, \u8ba1\u7b97\u6a21\u578b, \u795e\u7ecf\u79d1\u5b66"}}
{"id": "2506.01662", "pdf": "https://arxiv.org/pdf/2506.01662", "abs": "https://arxiv.org/abs/2506.01662", "authors": ["Catarina Moreira", "Anna Palatkina", "Dacia Braca", "Dylan M. Walsh", "Peter J. Leihn", "Fang Chen", "Nina C. Hubig"], "title": "Explainable AI Systems Must Be Contestable: Here's How to Make It Happen", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "As AI regulations around the world intensify their focus on system safety,\ncontestability has become a mandatory, yet ill-defined, safeguard. In XAI,\n\"contestability\" remains an empty promise: no formal definition exists, no\nalgorithm guarantees it, and practitioners lack concrete guidance to satisfy\nregulatory requirements. Grounded in a systematic literature review, this paper\npresents the first rigorous formal definition of contestability in explainable\nAI, directly aligned with stakeholder requirements and regulatory mandates. We\nintroduce a modular framework of by-design and post-hoc mechanisms spanning\nhuman-centered interfaces, technical architectures, legal processes, and\norganizational workflows. To operationalize our framework, we propose the\nContestability Assessment Scale, a composite metric built on more than twenty\nquantitative criteria. Through multiple case studies across diverse application\ndomains, we reveal where state-of-the-art systems fall short and show how our\nframework drives targeted improvements. By converting contestability from\nregulatory theory into a practical framework, our work equips practitioners\nwith the tools to embed genuine recourse and accountability into AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u53ef\u89e3\u91caAI\u4e2d\u201c\u53ef\u4e89\u8bae\u6027\u201d\u7684\u9996\u4e2a\u4e25\u683c\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\u548c\u8bc4\u4f30\u91cf\u8868\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e2e\u52a9\u4ece\u4e1a\u8005\u5b9e\u73b0\u6cd5\u89c4\u8981\u6c42\u3002", "motivation": "\u968f\u7740\u5168\u7403AI\u6cd5\u89c4\u5bf9\u7cfb\u7edf\u5b89\u5168\u7684\u5173\u6ce8\u589e\u52a0\uff0c\u53ef\u4e89\u8bae\u6027\u6210\u4e3a\u5f3a\u5236\u6027\u4f46\u5b9a\u4e49\u6a21\u7cca\u7684\u4fdd\u969c\u63aa\u65bd\u3002\u76ee\u524d\uff0c\u53ef\u89e3\u91caAI\u4e2d\u7684\u53ef\u4e89\u8bae\u6027\u7f3a\u4e4f\u5f62\u5f0f\u5316\u5b9a\u4e49\u548c\u5b9e\u73b0\u7b97\u6cd5\uff0c\u4ece\u4e1a\u8005\u7f3a\u4e4f\u5177\u4f53\u6307\u5bfc\u3002", "method": "\u57fa\u4e8e\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u4e0e\u5229\u76ca\u76f8\u5173\u8005\u9700\u6c42\u548c\u6cd5\u89c4\u8981\u6c42\u76f4\u63a5\u5bf9\u9f50\u7684\u53ef\u4e89\u8bae\u6027\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\u548cContestability Assessment Scale\u8bc4\u4f30\u91cf\u8868\u3002", "result": "\u901a\u8fc7\u591a\u4e2a\u6848\u4f8b\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u4e0d\u8db3\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u6846\u67b6\u5b9e\u73b0\u9488\u5bf9\u6027\u6539\u8fdb\u3002", "conclusion": "\u672c\u6587\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5c06\u53ef\u4e89\u8bae\u6027\u4ece\u6cd5\u89c4\u7406\u8bba\u8f6c\u5316\u4e3a\u5b9e\u8df5\u7684\u5de5\u5177\uff0c\u4ece\u800c\u5728AI\u7cfb\u7edf\u4e2d\u5d4c\u5165\u771f\u6b63\u7684\u8865\u6551\u548c\u95ee\u8d23\u673a\u5236\u3002", "keywords": "\u53ef\u89e3\u91caAI, \u53ef\u4e89\u8bae\u6027, \u6cd5\u89c4\u5408\u89c4, \u6a21\u5757\u5316\u6846\u67b6, \u8bc4\u4f30\u91cf\u8868"}}
{"id": "2506.01635", "pdf": "https://arxiv.org/pdf/2506.01635", "abs": "https://arxiv.org/abs/2506.01635", "authors": ["Julian Richter", "Christopher Erd\u00f6s", "Christian Scheurer", "Jochen J. Steil", "Niels Dehio"], "title": "Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Temporal alignment of multiple signals through time warping is crucial in\nmany fields, such as classification within speech recognition or robot motion\nlearning. Almost all related works are limited to data in Euclidean space.\nAlthough an attempt was made in 2011 to adapt this concept to unit quaternions,\na general extension to Riemannian manifolds remains absent. Given its\nimportance for numerous applications in robotics and beyond, we introduce\nRiemannian Time Warping~(RTW). This novel approach efficiently aligns multiple\nsignals by considering the geometric structure of the Riemannian manifold in\nwhich the data is embedded. Extensive experiments on synthetic and real-world\ndata, including tests with an LBR iiwa robot, demonstrate that RTW consistently\noutperforms state-of-the-art baselines in both averaging and classification\ntasks.", "AI": {"tldr": "RTW\u662f\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u9ece\u66fc\u6d41\u5f62\u7684\u51e0\u4f55\u7ed3\u6784\u6765\u9ad8\u6548\u5bf9\u9f50\u591a\u4e2a\u4fe1\u53f7\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5bf9\u9f50\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u7f3a\u4e4f\u5bf9\u9ece\u66fc\u6d41\u5f62\u6570\u636e\u7684\u901a\u7528\u6269\u5c55\u3002", "method": "\u63d0\u51faRiemannian Time Warping (RTW)\u65b9\u6cd5\uff0c\u5229\u7528\u9ece\u66fc\u6d41\u5f62\u7684\u51e0\u4f55\u7ed3\u6784\u5bf9\u9f50\u4fe1\u53f7\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\uff08\u5982\u673a\u5668\u4eba\u6d4b\u8bd5\uff09\u4e2d\uff0cRTW\u5728\u5e73\u5747\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RTW\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u4fe1\u53f7\u5bf9\u9f50\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u5e94\u7528\u3002", "keywords": "\u65f6\u95f4\u5bf9\u9f50,\u9ece\u66fc\u6d41\u5f62,\u673a\u5668\u4eba\u5b66\u4e60,\u4fe1\u53f7\u5904\u7406"}}
{"id": "2506.01666", "pdf": "https://arxiv.org/pdf/2506.01666", "abs": "https://arxiv.org/abs/2506.01666", "authors": ["Florian F\u00fcrrutter", "Zohim Chandani", "Ikko Hamamura", "Hans J. Briegel", "Gorka Mu\u00f1oz-Gil"], "title": "Synthesis of discrete-continuous quantum circuits with multimodal diffusion models", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": "Main Text: 10 pages and 5 figures; Appendix: 17 pages, 7 figures and\n  1 table. Code available at: https://github.com/FlorianFuerrutter/genQC", "summary": "Efficiently compiling quantum operations remains a major bottleneck in\nscaling quantum computing. Today's state-of-the-art methods achieve low\ncompilation error by combining search algorithms with gradient-based parameter\noptimization, but they incur long runtimes and require multiple calls to\nquantum hardware or expensive classical simulations, making their scaling\nprohibitive. Recently, machine-learning models have emerged as an alternative,\nthough they are currently restricted to discrete gate sets. Here, we introduce\na multimodal denoising diffusion model that simultaneously generates a\ncircuit's structure and its continuous parameters for compiling a target\nunitary. It leverages two independent diffusion processes, one for discrete\ngate selection and one for parameter prediction. We benchmark the model over\ndifferent experiments, analyzing the method's accuracy across varying qubit\ncounts, circuit depths, and proportions of parameterized gates. Finally, by\nexploiting its rapid circuit generation, we create large datasets of circuits\nfor particular operations and use these to extract valuable heuristics that can\nhelp us discover new insights into quantum circuit synthesis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u53bb\u566a\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u540c\u65f6\u751f\u6210\u91cf\u5b50\u7535\u8def\u7684\u7ed3\u6784\u548c\u8fde\u7eed\u53c2\u6570\uff0c\u4ee5\u9ad8\u6548\u7f16\u8bd1\u76ee\u6807\u9149\u77e9\u9635\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9ad8\u6210\u672c\u548c\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u91cf\u5b50\u8ba1\u7b97\u4e2d\uff0c\u9ad8\u6548\u7f16\u8bd1\u91cf\u5b50\u64cd\u4f5c\u662f\u4e00\u4e2a\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u80fd\u5b9e\u73b0\u4f4e\u7f16\u8bd1\u8bef\u5dee\uff0c\u4f46\u8fd0\u884c\u65f6\u95f4\u957f\u4e14\u4f9d\u8d56\u4e8e\u91cf\u5b50\u786c\u4ef6\u6216\u6602\u8d35\u7684\u7ecf\u5178\u6a21\u62df\uff0c\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u5f15\u5165\u591a\u6a21\u6001\u53bb\u566a\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u4e24\u4e2a\u72ec\u7acb\u7684\u6269\u6563\u8fc7\u7a0b\uff1a\u4e00\u4e2a\u7528\u4e8e\u79bb\u6563\u95e8\u9009\u62e9\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u53c2\u6570\u9884\u6d4b\uff0c\u4ee5\u540c\u65f6\u751f\u6210\u7535\u8def\u7ed3\u6784\u548c\u8fde\u7eed\u53c2\u6570\u3002", "result": "\u901a\u8fc7\u4e0d\u540c\u5b9e\u9a8c\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u4e86\u6a21\u578b\u5728\u591a\u6837\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u3001\u7535\u8def\u6df1\u5ea6\u548c\u53c2\u6570\u5316\u95e8\u6bd4\u4f8b\u4e0b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5229\u7528\u5176\u5feb\u901f\u751f\u6210\u80fd\u529b\u521b\u5efa\u6570\u636e\u96c6\u4ee5\u63d0\u53d6\u542f\u53d1\u5f0f\u89c4\u5219\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5feb\u901f\u7535\u8def\u751f\u6210\u80fd\u529b\uff0c\u5e76\u63a8\u52a8\u4e86\u91cf\u5b50\u7535\u8def\u5408\u6210\u7684\u65b0\u89c1\u89e3\u53d1\u73b0\u3002", "keywords": "\u91cf\u5b50\u8ba1\u7b97, \u7f16\u8bd1\u4f18\u5316, \u591a\u6a21\u6001\u6269\u6563\u6a21\u578b, \u7535\u8def\u5408\u6210"}}
{"id": "2506.01641", "pdf": "https://arxiv.org/pdf/2506.01641", "abs": "https://arxiv.org/abs/2506.01641", "authors": ["Toon Van Puyvelde", "Mehran Zareh", "Chris Develder"], "title": "Interpretable reinforcement learning for heat pump control through asymmetric differentiable decision trees", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": "7 pages, 3 figures, conference", "summary": "In recent years, deep reinforcement learning (DRL) algorithms have gained\ntraction in home energy management systems. However, their adoption by energy\nmanagement companies remains limited due to the black-box nature of DRL, which\nfails to provide transparent decision-making feedback. To address this,\nexplainable reinforcement learning (XRL) techniques have emerged, aiming to\nmake DRL decisions more transparent. Among these, soft differential decision\ntree (DDT) distillation provides a promising approach due to the clear decision\nrules they are based on, which can be efficiently computed. However, achieving\nhigh performance often requires deep, and completely full, trees, which reduces\ninterpretability. To overcome this, we propose a novel asymmetric soft DDT\nconstruction method. Unlike traditional soft DDTs, our approach adaptively\nconstructs trees by expanding nodes only when necessary. This improves the\nefficient use of decision nodes, which require a predetermined depth to\nconstruct full symmetric trees, enhancing both interpretability and\nperformance. We demonstrate the potential of asymmetric DDTs to provide\ntransparent, efficient, and high-performing decision-making in home energy\nmanagement systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u975e\u5bf9\u79f0\u8f6f\u5dee\u5206\u51b3\u7b56\u6811\uff08DDT\uff09\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u5728\u5bb6\u5ead\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u7531\u4e8eDRL\u7684\u9ed1\u76d2\u7279\u6027\u9650\u5236\u4e86\u5176\u5728\u80fd\u6e90\u7ba1\u7406\u516c\u53f8\u7684\u5e94\u7528\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u5f3a\u5316\u5b66\u4e60\uff08XRL\uff09\u6280\u672f\u63d0\u5347\u51b3\u7b56\u900f\u660e\u5ea6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u975e\u5bf9\u79f0\u8f6fDDT\u6784\u5efa\u65b9\u6cd5\uff0c\u52a8\u6001\u6269\u5c55\u8282\u70b9\u4ee5\u63d0\u9ad8\u51b3\u7b56\u8282\u70b9\u7684\u4f7f\u7528\u6548\u7387\uff0c\u907f\u514d\u4f20\u7edf\u5bf9\u79f0\u6811\u9700\u9884\u5b9a\u4e49\u6df1\u5ea6\u7684\u7f3a\u70b9\u3002", "result": "\u975e\u5bf9\u79f0DDT\u5728\u5bb6\u5ead\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\u4e2d\u5c55\u73b0\u51fa\u900f\u660e\u3001\u9ad8\u6548\u548c\u9ad8\u6027\u80fd\u7684\u51b3\u7b56\u6f5c\u529b\u3002", "conclusion": "\u975e\u5bf9\u79f0\u8f6fDDT\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u8282\u70b9\u4f7f\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002", "keywords": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60,\u53ef\u89e3\u91ca\u6027,\u5dee\u5206\u51b3\u7b56\u6811,\u80fd\u6e90\u7ba1\u7406,\u975e\u5bf9\u79f0\u6811"}}
{"id": "2506.01678", "pdf": "https://arxiv.org/pdf/2506.01678", "abs": "https://arxiv.org/abs/2506.01678", "authors": ["Nikola L. Kolev", "Max Trouton", "Filippo Federici Canova", "Geoff Thornton", "David Z. Gao", "Neil J. Curson", "Taylor J. Z. Stock"], "title": "Overcoming Data Scarcity in Scanning Tunnelling Microscopy Image Segmentation", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "comment": null, "summary": "Scanning tunnelling microscopy (STM) is a powerful technique for imaging\nsurfaces with atomic resolution, providing insight into physical and chemical\nprocesses at the level of single atoms and molecules. A regular task of STM\nimage analysis is the identification and labelling of features of interest\nagainst a uniform background. Performing this manually is a labour-intensive\ntask, requiring significant human effort. To reduce this burden, we propose an\nautomated approach to the segmentation of STM images that uses both few-shot\nlearning and unsupervised learning. Our technique offers greater flexibility\ncompared to previous supervised methods; it removes the requirement for large\nmanually annotated datasets and is thus easier to adapt to an unseen surface\nwhile still maintaining a high accuracy. We demonstrate the effectiveness of\nour approach by using it to recognise atomic features on three distinct\nsurfaces: Si(001), Ge(001), and TiO$_2$(110), including adsorbed AsH$_3$\nmolecules on the silicon and germanium surfaces. Our model exhibits strong\ngeneralisation capabilities, and following initial training, can be adapted to\nunseen surfaces with as few as one additional labelled data point. This work is\na significant step towards efficient and material-agnostic, automatic\nsegmentation of STM images.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5c11\u6837\u672c\u5b66\u4e60\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u81ea\u52a8\u5316STM\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u4eba\u5de5\u6807\u6ce8\u7684\u9700\u6c42\uff0c\u63d0\u9ad8\u4e86\u5bf9\u65b0\u8868\u9762\u7684\u9002\u5e94\u6027\u3002", "motivation": "STM\u56fe\u50cf\u7684\u624b\u52a8\u5206\u6790\u8017\u65f6\u8d39\u529b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u51cf\u8f7b\u8d1f\u62c5\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u7ed3\u5408\u5c11\u6837\u672c\u5b66\u4e60\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u7684\u9700\u6c42\u3002", "result": "\u5728Si(001)\u3001Ge(001)\u548cTiO$_2$(110)\u7b49\u8868\u9762\u4e0a\u6210\u529f\u8bc6\u522b\u539f\u5b50\u7279\u5f81\uff0c\u5305\u62ec\u5438\u9644\u7684AsH$_3$\u5206\u5b50\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u6548\u4e14\u6750\u6599\u65e0\u5173\u7684STM\u56fe\u50cf\u81ea\u52a8\u5206\u5272\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002", "keywords": "STM\u56fe\u50cf\u3001\u5c11\u6837\u672c\u5b66\u4e60\u3001\u65e0\u76d1\u7763\u5b66\u4e60\u3001\u81ea\u52a8\u5206\u5272\u3001\u539f\u5b50\u5206\u8fa8\u7387"}}
{"id": "2506.01701", "pdf": "https://arxiv.org/pdf/2506.01701", "abs": "https://arxiv.org/abs/2506.01701", "authors": ["Haoru Tan", "Sitong Wu", "Wei Huang", "Shizhen Zhao", "Xiaojuan Qi"], "title": "Data Pruning by Information Maximization", "categories": ["cs.CV", "cs.AI"], "comment": "ICLR 2025", "summary": "In this paper, we present InfoMax, a novel data pruning method, also known as\ncoreset selection, designed to maximize the information content of selected\nsamples while minimizing redundancy. By doing so, InfoMax enhances the overall\ninformativeness of the coreset. The information of individual samples is\nmeasured by importance scores, which capture their influence or difficulty in\nmodel learning. To quantify redundancy, we use pairwise sample similarities,\nbased on the premise that similar samples contribute similarly to the learning\nprocess. We formalize the coreset selection problem as a discrete quadratic\nprogramming (DQP) task, with the objective of maximizing the total information\ncontent, represented as the sum of individual sample contributions minus the\nredundancies introduced by similar samples within the coreset. To ensure\npractical scalability, we introduce an efficient gradient-based solver,\ncomplemented by sparsification techniques applied to the similarity matrix and\ndataset partitioning strategies. This enables InfoMax to seamlessly scale to\ndatasets with millions of samples. Extensive experiments demonstrate the\nsuperior performance of InfoMax in various data pruning tasks, including image\nclassification, vision-language pre-training, and instruction tuning for large\nlanguage models.", "AI": {"tldr": "InfoMax\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u4fee\u526a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u6837\u672c\u4fe1\u606f\u5185\u5bb9\u5e76\u6700\u5c0f\u5316\u5197\u4f59\u6765\u63d0\u5347\u6838\u5fc3\u96c6\u7684\u6574\u4f53\u4fe1\u606f\u91cf\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u91cd\u8981\u6027\u8bc4\u5206\u548c\u6837\u672c\u76f8\u4f3c\u6027\u91cf\u5316\u4fe1\u606f\u4e0e\u5197\u4f59\uff0c\u5c06\u5176\u5f62\u5f0f\u5316\u4e3a\u79bb\u6563\u4e8c\u6b21\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u9ad8\u6548\u6c42\u89e3\u5668\u6269\u5c55\u81f3\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u4fee\u526a\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u4fe1\u606f\u91cf\u548c\u5197\u4f59\u6027\uff0c\u9650\u5236\u4e86\u6838\u5fc3\u96c6\u7684\u6709\u6548\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u4f18\u5316\u4e8c\u8005\u7684\u65b0\u65b9\u6cd5\u3002", "method": "InfoMax\u901a\u8fc7\u91cd\u8981\u6027\u8bc4\u5206\u91cf\u6837\u672c\u4fe1\u606f\u91cf\uff0c\u7528\u6837\u672c\u76f8\u4f3c\u6027\u91cf\u5316\u5197\u4f59\u6027\uff0c\u5e76\u5c06\u5176\u5efa\u6a21\u4e3a\u79bb\u6563\u4e8c\u6b21\u89c4\u5212\u95ee\u9898\uff0c\u91c7\u7528\u68af\u5ea6\u6c42\u89e3\u5668\u548c\u7a00\u758f\u5316\u6280\u672f\u63d0\u9ad8\u6269\u5c55\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eInfoMax\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6307\u4ee4\u8c03\u4f18\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "InfoMax\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u4fee\u526a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6838\u5fc3\u96c6\u7684\u4fe1\u606f\u91cf\u3002", "keywords": "\u6570\u636e\u4fee\u526a, \u6838\u5fc3\u96c6\u9009\u62e9, \u4fe1\u606f\u6700\u5927\u5316, \u5197\u4f59\u6700\u5c0f\u5316, \u79bb\u6563\u4e8c\u6b21\u89c4\u5212"}}
{"id": "2506.01685", "pdf": "https://arxiv.org/pdf/2506.01685", "abs": "https://arxiv.org/abs/2506.01685", "authors": ["Benjamin Schiffer", "Mark Sellke"], "title": "Geometry Meets Incentives: Sample-Efficient Incentivized Exploration with Linear Contexts", "categories": ["cs.GT", "cs.LG"], "comment": null, "summary": "In the incentivized exploration model, a principal aims to explore and learn\nover time by interacting with a sequence of self-interested agents. It has been\nrecently understood that the main challenge in designing incentive-compatible\nalgorithms for this problem is to gather a moderate amount of initial data,\nafter which one can obtain near-optimal regret via posterior sampling. With\nhigh-dimensional contexts, however, this \\emph{initial exploration} phase\nrequires exponential sample complexity in some cases, which prevents efficient\nlearning unless initial data can be acquired exogenously. We show that these\nbarriers to exploration disappear under mild geometric conditions on the set of\navailable actions, in which case incentive-compatibility does not preclude\nregret-optimality. Namely, we consider the linear bandit model with actions in\nthe Euclidean unit ball, and give an incentive-compatible exploration algorithm\nwith sample complexity that scales polynomially with the dimension and other\nparameters.", "AI": {"tldr": "\u7814\u7a76\u5728\u6fc0\u52b1\u5f0f\u63a2\u7d22\u6a21\u578b\u4e2d\uff0c\u5982\u4f55\u5728\u9ad8\u7ef4\u80cc\u666f\u4e0b\u901a\u8fc7\u51e0\u4f55\u6761\u4ef6\u6d88\u9664\u521d\u59cb\u63a2\u7d22\u7684\u6307\u6570\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u6fc0\u52b1\u517c\u5bb9\u7b97\u6cd5\u3002", "motivation": "\u5728\u6fc0\u52b1\u5f0f\u63a2\u7d22\u6a21\u578b\u4e2d\uff0c\u521d\u59cb\u63a2\u7d22\u9636\u6bb5\u7684\u9ad8\u7ef4\u4e0a\u4e0b\u6587\u53ef\u80fd\u5bfc\u81f4\u6307\u6570\u7ea7\u6837\u672c\u590d\u6742\u5ea6\uff0c\u963b\u788d\u9ad8\u6548\u5b66\u4e60\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u51e0\u4f55\u6761\u4ef6\u907f\u514d\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u8d4c\u535a\u673a\u6a21\u578b\uff0c\u52a8\u4f5c\u96c6\u5408\u4e3a\u6b27\u51e0\u91cc\u5f97\u5355\u4f4d\u7403\uff0c\u63d0\u51fa\u4e00\u79cd\u6fc0\u52b1\u517c\u5bb9\u7684\u63a2\u7d22\u7b97\u6cd5\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e0e\u7ef4\u5ea6\u7b49\u53c2\u6570\u5448\u591a\u9879\u5f0f\u5173\u7cfb\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u9ad8\u7ef4\u80cc\u666f\u4e0b\uff0c\u901a\u8fc7\u51e0\u4f55\u6761\u4ef6\u5b9e\u73b0\u4e86\u591a\u9879\u5f0f\u7ea7\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u89e3\u51b3\u4e86\u521d\u59cb\u63a2\u7d22\u7684\u6307\u6570\u590d\u6742\u5ea6\u95ee\u9898\u3002", "conclusion": "\u5728\u7279\u5b9a\u51e0\u4f55\u6761\u4ef6\u4e0b\uff0c\u6fc0\u52b1\u517c\u5bb9\u6027\u4e0e\u6700\u4f18\u540e\u6094\u6027\u80fd\u53ef\u4ee5\u5171\u5b58\uff0c\u4e3a\u9ad8\u7ef4\u6fc0\u52b1\u63a2\u7d22\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u6fc0\u52b1\u5f0f\u63a2\u7d22,\u9ad8\u7ef4\u4e0a\u4e0b\u6587,\u7ebf\u6027\u8d4c\u535a\u673a,\u6fc0\u52b1\u517c\u5bb9,\u6837\u672c\u590d\u6742\u5ea6"}}
{"id": "2506.01757", "pdf": "https://arxiv.org/pdf/2506.01757", "abs": "https://arxiv.org/abs/2506.01757", "authors": ["Marco Calzavara", "Ard Kastrati", "Matteo Macchini", "Dushan Vasilevski", "Roger Wattenhofer"], "title": "Efficient Egocentric Action Recognition with Multimodal Data", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted as an extended abstract at the Second Joint Egocentric\n  Vision (EgoVis) Workshop, 2025", "summary": "The increasing availability of wearable XR devices opens new perspectives for\nEgocentric Action Recognition (EAR) systems, which can provide deeper human\nunderstanding and situation awareness. However, deploying real-time algorithms\non these devices can be challenging due to the inherent trade-offs between\nportability, battery life, and computational resources. In this work, we\nsystematically analyze the impact of sampling frequency across different input\nmodalities - RGB video and 3D hand pose - on egocentric action recognition\nperformance and CPU usage. By exploring a range of configurations, we provide a\ncomprehensive characterization of the trade-offs between accuracy and\ncomputational efficiency. Our findings reveal that reducing the sampling rate\nof RGB frames, when complemented with higher-frequency 3D hand pose input, can\npreserve high accuracy while significantly lowering CPU demands. Notably, we\nobserve up to a 3x reduction in CPU usage with minimal to no loss in\nrecognition performance. This highlights the potential of multimodal input\nstrategies as a viable approach to achieving efficient, real-time EAR on XR\ndevices.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u964d\u4f4eRGB\u5e27\u91c7\u6837\u7387\u5e76\u8f85\u4ee5\u9ad8\u98913D\u624b\u90e8\u59ff\u6001\u8f93\u5165\uff0c\u53ef\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4eCPU\u4f7f\u7528\u7387\uff0c\u9002\u7528\u4e8eXR\u8bbe\u5907\u7684\u5b9e\u65f6\u52a8\u4f5c\u8bc6\u522b\u3002", "motivation": "\u89e3\u51b3XR\u8bbe\u5907\u5728\u5b9e\u65f6\u52a8\u4f5c\u8bc6\u522b\u4e2d\u9762\u4e34\u7684\u4fbf\u643a\u6027\u3001\u7535\u6c60\u5bff\u547d\u548c\u8ba1\u7b97\u8d44\u6e90\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790RGB\u89c6\u9891\u548c3D\u624b\u90e8\u59ff\u6001\u5728\u4e0d\u540c\u91c7\u6837\u9891\u7387\u4e0b\u5bf9\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u548cCPU\u4f7f\u7528\u7387\u7684\u5f71\u54cd\u3002", "result": "\u964d\u4f4eRGB\u5e27\u91c7\u6837\u7387\u5e76\u7ed3\u5408\u9ad8\u98913D\u624b\u90e8\u59ff\u6001\u8f93\u5165\u53ef\u51cf\u5c113\u500dCPU\u4f7f\u7528\u7387\uff0c\u4e14\u51e0\u4e4e\u4e0d\u5f71\u54cd\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "\u591a\u6a21\u6001\u8f93\u5165\u7b56\u7565\u662f\u5b9e\u73b0XR\u8bbe\u5907\u9ad8\u6548\u5b9e\u65f6\u52a8\u4f5c\u8bc6\u522b\u7684\u53ef\u884c\u65b9\u6cd5\u3002", "keywords": "Egocentric Action Recognition, XR devices, sampling frequency, computational efficiency, multimodal input"}}
{"id": "2506.01718", "pdf": "https://arxiv.org/pdf/2506.01718", "abs": "https://arxiv.org/abs/2506.01718", "authors": ["Andrew Alden", "Blanka Horvath", "Zacharia Issa"], "title": "Signature Maximum Mean Discrepancy Two-Sample Statistical Tests", "categories": ["stat.ML", "cs.LG", "math.DS"], "comment": "45 pages, 19 figures", "summary": "Maximum Mean Discrepancy (MMD) is a widely used concept in machine learning\nresearch which has gained popularity in recent years as a highly effective tool\nfor comparing (finite-dimensional) distributions. Since it is designed as a\nkernel-based method, the MMD can be extended to path space valued distributions\nusing the signature kernel. The resulting signature MMD (sig-MMD) can be used\nto define a metric between distributions on path space. Similarly to the\noriginal use case of the MMD as a test statistic within a two-sample testing\nframework, the sig-MMD can be applied to determine if two sets of paths are\ndrawn from the same stochastic process. This work is dedicated to understanding\nthe possibilities and challenges associated with applying the sig-MMD as a\nstatistical tool in practice. We introduce and explain the sig-MMD, and provide\neasily accessible and verifiable examples for its practical use. We present\nexamples that can lead to Type 2 errors in the hypothesis test, falsely\nindicating that samples have been drawn from the same underlying process (which\ngenerally occurs in a limited data setting). We then present techniques to\nmitigate the occurrence of this type of error.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u7b7e\u540d\u6838\u7684\u7b7e\u540dMMD\uff08sig-MMD\uff09\u5728\u8def\u5f84\u7a7a\u95f4\u5206\u5e03\u6bd4\u8f83\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u5206\u6790\u4e86\u5176\u53ef\u80fd\u5bfc\u81f4\u7b2c\u4e8c\u7c7b\u9519\u8bef\u7684\u573a\u666f\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76sig-MMD\u4f5c\u4e3a\u7edf\u8ba1\u5de5\u5177\u5728\u5b9e\u8df5\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u4e0e\u6311\u6218\uff0c\u7279\u522b\u5173\u6ce8\u5176\u5728\u4e24\u6837\u672c\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u7b7e\u540d\u6838\u6269\u5c55MMD\u81f3\u8def\u5f84\u7a7a\u95f4\uff0c\u63d0\u51fasig-MMD\uff0c\u5e76\u901a\u8fc7\u5177\u4f53\u6848\u4f8b\u9a8c\u8bc1\u5176\u5e94\u7528\u6548\u679c\u3002", "result": "\u53d1\u73b0sig-MMD\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u53ef\u80fd\u5bfc\u81f4\u7b2c\u4e8c\u7c7b\u9519\u8bef\uff0c\u5e76\u63d0\u51fa\u4e86\u51cf\u5c11\u6b64\u7c7b\u9519\u8bef\u7684\u6280\u672f\u3002", "conclusion": "sig-MMD\u662f\u4e00\u4e2a\u6709\u6548\u7684\u5de5\u5177\uff0c\u4f46\u9700\u8c28\u614e\u4f7f\u7528\u4ee5\u907f\u514d\u8bef\u5224\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u91cf\u8f83\u5c0f\u7684\u60c5\u51b5\u4e0b\u3002", "keywords": "Maximum Mean Discrepancy, \u7b7e\u540d\u6838, \u8def\u5f84\u7a7a\u95f4, \u4e24\u6837\u672c\u6d4b\u8bd5, \u7b2c\u4e8c\u7c7b\u9519\u8bef"}}
{"id": "2506.01770", "pdf": "https://arxiv.org/pdf/2506.01770", "abs": "https://arxiv.org/abs/2506.01770", "authors": ["Zeming Wei", "Chengcan Wu", "Meng Sun"], "title": "ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) have achieved significant success in various\ntasks, yet concerns about their safety and security have emerged. In\nparticular, they pose risks in generating harmful content and vulnerability to\njailbreaking attacks. To analyze and monitor machine learning models,\nmodel-based analysis has demonstrated notable potential in stateful deep neural\nnetworks, yet suffers from scalability issues when extending to LLMs due to\ntheir vast feature spaces. In this paper, we propose ReGA, a model-based\nanalysis framework with representation-guided abstraction, to safeguard LLMs\nagainst harmful prompts and generations. By leveraging safety-critical\nrepresentations, which are low-dimensional directions emerging in hidden states\nthat indicate safety-related concepts, ReGA effectively addresses the\nscalability issue when constructing the abstract model for safety modeling. Our\ncomprehensive evaluation shows that ReGA performs sufficiently well in\ndistinguishing between safe and harmful inputs, achieving an AUROC of 0.975 at\nthe prompt level and 0.985 at the conversation level. Additionally, ReGA\nexhibits robustness to real-world attacks and generalization across different\nsafety perspectives, outperforming existing safeguard paradigms in terms of\ninterpretability and scalability. Overall, ReGA serves as an efficient and\nscalable solution to enhance LLM safety by integrating representation\nengineering with model-based abstraction, paving the way for new paradigms to\nutilize software insights for AI safety. Our code is available at\nhttps://github.com/weizeming/ReGA.", "AI": {"tldr": "ReGA\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u8868\u793a\u5f15\u5bfc\u7684\u62bd\u8c61\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b89\u5168\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u95ee\u9898\uff0c\u6709\u6548\u533a\u5206\u5b89\u5168\u4e0e\u6709\u5bb3\u8f93\u5165\u3002", "motivation": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u548c\u6613\u53d7\u653b\u51fb\u65b9\u9762\u7684\u5b89\u5168\u9690\u60a3\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u5206\u6790\u65b9\u6cd5\u56e0\u53ef\u6269\u5c55\u6027\u95ee\u9898\u96be\u4ee5\u9002\u7528\u3002", "method": "\u63d0\u51faReGA\u6846\u67b6\uff0c\u5229\u7528\u5b89\u5168\u5173\u952e\u8868\u793a\uff08\u4f4e\u7ef4\u65b9\u5411\uff09\u6784\u5efa\u62bd\u8c61\u6a21\u578b\uff0c\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "result": "ReGA\u5728\u533a\u5206\u5b89\u5168\u4e0e\u6709\u5bb3\u8f93\u5165\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0cAUROC\u6307\u6807\u5728\u63d0\u793a\u7ea7\u522b\u4e3a0.975\uff0c\u5bf9\u8bdd\u7ea7\u522b\u4e3a0.985\uff0c\u4e14\u5bf9\u5b9e\u9645\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "ReGA\u901a\u8fc7\u5c06\u8868\u793a\u5de5\u7a0b\u4e0e\u57fa\u4e8e\u6a21\u578b\u7684\u62bd\u8c61\u7ed3\u5408\uff0c\u4e3a\u589e\u5f3aLLM\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u6a21\u578b\u5206\u6790\u3001\u5b89\u5168\u5173\u952e\u8868\u793a\u3001\u53ef\u6269\u5c55\u6027\u3001\u4eba\u5de5\u667a\u80fd\u5b89\u5168"}}
{"id": "2506.01755", "pdf": "https://arxiv.org/pdf/2506.01755", "abs": "https://arxiv.org/abs/2506.01755", "authors": ["Defne E. Ozan", "Andrea N\u00f3voa", "Georgios Rigas", "Luca Magri"], "title": "Data-assimilated model-informed reinforcement learning", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "The control of spatio-temporally chaos is challenging because of high\ndimensionality and unpredictability. Model-free reinforcement learning (RL)\ndiscovers optimal control policies by interacting with the system, typically\nrequiring observations of the full physical state.In practice, sensors often\nprovide only partial and noisy measurements (observations) of the system. The\nobjective of this paper is to develop a framework that enables the control of\nchaotic systems with partial and noisy observability. The proposed method,\ndata-assimilated model-informed reinforcement learning (DA-MIRL), integrates\n(i) low-order models to approximate high-dimensional dynamics; (ii) sequential\ndata assimilation to correct the model prediction when observations become\navailable; and (iii) an off-policy actor-critic RL algorithm to adaptively\nlearn an optimal control strategy based on the corrected state estimates. We\ntest DA-MIRL on the spatiotemporally chaotic solutions of the\nKuramoto-Sivashinsky equation. We estimate the full state of the environment\nwith (i) a physics-based model, here, a coarse-grained model; and (ii) a\ndata-driven model, here, the control-aware echo state network, which is\nproposed in this paper. We show that DA-MIRL successfully estimates and\nsuppresses the chaotic dynamics of the environment in real time from partial\nobservations and approximate models. This work opens opportunities for the\ncontrol of partially observable chaotic systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u636e\u540c\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6DA-MIRL\uff0c\u7528\u4e8e\u63a7\u5236\u548c\u6291\u5236\u9ad8\u7ef4\u6df7\u6c8c\u7cfb\u7edf\u7684\u52a8\u6001\uff0c\u5373\u4f7f\u57fa\u4e8e\u90e8\u5206\u548c\u566a\u58f0\u89c2\u6d4b\u4e5f\u6709\u6548\u3002", "motivation": "\u7531\u4e8e\u9ad8\u7ef4\u6df7\u6c8c\u7cfb\u7edf\u7684\u9ad8\u590d\u6742\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u4f20\u7edf\u7684\u63a7\u5236\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u90e8\u5206\u548c\u566a\u58f0\u89c2\u6d4b\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u4f18\u5316\u63a7\u5236\u7b56\u7565\u3002", "method": "DA-MIRL\u7ed3\u5408\u4e86\u4f4e\u9636\u6a21\u578b\u3001\u5e8f\u5217\u6570\u636e\u540c\u5316\u548c\u79bb\u7b56\u7565actor-critic\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u7269\u7406\u6a21\u578b\u548c\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff08\u5982\u63a7\u5236\u611f\u77e5\u7684Echo\u72b6\u6001\u7f51\u7edc\uff09\u5b9e\u73b0\u72b6\u6001\u4f30\u8ba1\u548c\u63a7\u5236\u3002", "result": "\u5728Kuramoto-Sivashinsky\u65b9\u7a0b\u7684\u6df7\u6c8c\u89e3\u4e0a\u6d4b\u8bd5\uff0cDA-MIRL\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u90e8\u5206\u89c2\u6d4b\u548c\u8fd1\u4f3c\u6a21\u578b\u4e2d\u5b9e\u65f6\u4f30\u8ba1\u5e76\u6291\u5236\u6df7\u6c8c\u52a8\u6001\u3002", "conclusion": "DA-MIRL\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u6df7\u6c8c\u7cfb\u7edf\u7684\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "keywords": "\u6df7\u6c8c\u63a7\u5236, \u5f3a\u5316\u5b66\u4e60, \u6570\u636e\u540c\u5316, \u90e8\u5206\u53ef\u89c2\u6d4b\u6027, Kuramoto-Sivashinsky\u65b9\u7a0b"}}
{"id": "2506.01774", "pdf": "https://arxiv.org/pdf/2506.01774", "abs": "https://arxiv.org/abs/2506.01774", "authors": ["Lu\u00eds Cruz", "Jo\u00e3o Paulo Fernandes", "Maja H. Kirkeby", "Silverio Mart\u00ednez-Fern\u00e1ndez", "June Sallou", "Hina Anwar", "Enrique Barba Roque", "Justus Bogner", "Joel Casta\u00f1o", "Fernando Castor", "Aadil Chasmawala", "Sim\u00e3o Cunha", "Daniel Feitosa", "Alexandra Gonz\u00e1lez", "Andreas Jedlitschka", "Patricia Lago", "Ana Oprescu", "Pooja Rani", "Jo\u00e3o Saraiva", "Federica Sarro", "Raghavendra Selvan", "Karthik Vaidhyanathan", "Roberto Verdecchia", "Ivan P. Yamshchikov", "Henry Muccini"], "title": "Greening AI-enabled Systems with Software Engineering: A Research Agenda for Environmentally Sustainable AI Practices", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The environmental impact of Artificial Intelligence (AI)-enabled systems is\nincreasing rapidly, and software engineering plays a critical role in\ndeveloping sustainable solutions. The \"Greening AI with Software Engineering\"\nCECAM-Lorentz workshop (no. 1358, 2025) funded by the Centre Europ\\'een de\nCalcul Atomique et Mol\\'eculaire and the Lorentz Center, provided an\ninterdisciplinary forum for 29 participants, from practitioners to academics,\nto share knowledge, ideas, practices, and current results dedicated to\nadvancing green software and AI research. The workshop was held February 3-7,\n2025, in Lausanne, Switzerland. Through keynotes, flash talks, and\ncollaborative discussions, participants identified and prioritized key\nchallenges for the field. These included energy assessment and standardization,\nbenchmarking practices, sustainability-aware architectures, runtime adaptation,\nempirical methodologies, and education. This report presents a research agenda\nemerging from the workshop, outlining open research directions and practical\nrecommendations to guide the development of environmentally sustainable\nAI-enabled systems rooted in software engineering principles.", "AI": {"tldr": "\u8be5\u7814\u8ba8\u4f1a\u63a2\u8ba8\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u5728\u5f00\u53d1\u53ef\u6301\u7eedAI\u7cfb\u7edf\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u7eff\u8272\u8f6f\u4ef6\u548cAI\u7814\u7a76\u7684\u5173\u952e\u6311\u6218\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u5bf9\u73af\u5883\u5f71\u54cd\u52a0\u5267\uff0c\u7814\u8ba8\u4f1a\u65e8\u5728\u901a\u8fc7\u8f6f\u4ef6\u5de5\u7a0b\u63a8\u52a8\u53ef\u6301\u7eed\u89e3\u51b3\u65b9\u6848\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u4e3b\u9898\u6f14\u8bb2\u3001\u5feb\u901f\u6f14\u8bb2\u548c\u534f\u4f5c\u8ba8\u8bba\uff0c\u53c2\u4e0e\u8005\u8bc6\u522b\u5e76\u4f18\u5148\u8003\u8651\u4e86\u80fd\u6e90\u8bc4\u4f30\u3001\u57fa\u51c6\u6d4b\u8bd5\u3001\u53ef\u6301\u7eed\u67b6\u6784\u7b49\u5173\u952e\u6311\u6218\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u5f00\u653e\u7814\u7a76\u65b9\u5411\u548c\u5b9e\u8df5\u5efa\u8bae\uff0c\u4ee5\u6307\u5bfc\u57fa\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u539f\u5219\u7684\u73af\u5883\u53ef\u6301\u7eedAI\u7cfb\u7edf\u5f00\u53d1\u3002", "conclusion": "\u7814\u8ba8\u4f1a\u4e3a\u7eff\u8272AI\u548c\u8f6f\u4ef6\u7814\u7a76\u63d0\u4f9b\u4e86\u8de8\u5b66\u79d1\u4ea4\u6d41\u5e73\u53f0\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u5236\u5b9a\u4e86\u8bae\u7a0b\u3002", "keywords": "AI\u53ef\u6301\u7eed\u6027\uff0c\u8f6f\u4ef6\u5de5\u7a0b\uff0c\u7eff\u8272\u8ba1\u7b97\uff0c\u80fd\u6e90\u8bc4\u4f30\uff0c\u53ef\u6301\u7eed\u53d1\u5c55"}}
{"id": "2506.01778", "pdf": "https://arxiv.org/pdf/2506.01778", "abs": "https://arxiv.org/abs/2506.01778", "authors": ["Yafei Yang", "Zihui Zhang", "Bo Yang"], "title": "unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary Reasoning", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "ICML 2025. Code and data are available at:\n  https://github.com/vLAR-group/unMORE", "summary": "We study the challenging problem of unsupervised multi-object segmentation on\nsingle images. Existing methods, which rely on image reconstruction objectives\nto learn objectness or leverage pretrained image features to group similar\npixels, often succeed only in segmenting simple synthetic objects or\ndiscovering a limited number of real-world objects. In this paper, we introduce\nunMORE, a novel two-stage pipeline designed to identify many complex objects in\nreal-world images. The key to our approach involves explicitly learning three\nlevels of carefully defined object-centric representations in the first stage.\nSubsequently, our multi-object reasoning module utilizes these learned object\npriors to discover multiple objects in the second stage. Notably, this\nreasoning module is entirely network-free and does not require human labels.\nExtensive experiments demonstrate that unMORE significantly outperforms all\nexisting unsupervised methods across 6 real-world benchmark datasets, including\nthe challenging COCO dataset, achieving state-of-the-art object segmentation\nresults. Remarkably, our method excels in crowded images where all baselines\ncollapse.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aunMORE\u7684\u4e24\u9636\u6bb5\u65e0\u76d1\u7763\u591a\u76ee\u6807\u5206\u5272\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u56fe\u50cf\u4e2d\u6709\u6548\u8bc6\u522b\u591a\u4e2a\u5bf9\u8c61\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u56fe\u50cf\u91cd\u5efa\u76ee\u6807\u6216\u9884\u8bad\u7ec3\u7279\u5f81\uff0c\u4ec5\u80fd\u5206\u5272\u7b80\u5355\u5408\u6210\u5bf9\u8c61\u6216\u6709\u9650\u6570\u91cf\u7684\u771f\u5b9e\u5bf9\u8c61\u3002", "method": "unMORE\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u5b66\u4e60\u4e09\u5c42\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u65e0\u7f51\u7edc\u7684\u63a8\u7406\u6a21\u5757\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u53d1\u73b0\u591a\u5bf9\u8c61\u3002", "result": "\u57286\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5305\u62ecCOCO\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u62e5\u6324\u56fe\u50cf\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "unMORE\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u65e0\u76d1\u7763\u591a\u5bf9\u8c61\u5206\u5272\u6027\u80fd\u3002", "keywords": "\u65e0\u76d1\u7763\u5b66\u4e60,\u591a\u76ee\u6807\u5206\u5272,\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a,\u771f\u5b9e\u4e16\u754c\u56fe\u50cf"}}
{"id": "2506.01816", "pdf": "https://arxiv.org/pdf/2506.01816", "abs": "https://arxiv.org/abs/2506.01816", "authors": ["Steffen W. R. Werner", "Benjamin Peherstorfer"], "title": "An adaptive data sampling strategy for stabilizing dynamical systems via controller inference", "categories": ["math.OC", "cs.LG", "cs.NA", "math.DS", "math.NA", "37N35, 65F55, 90C22, 90C59, 93B52"], "comment": "27 pages, 9 figures", "summary": "Learning stabilizing controllers from data is an important task in\nengineering applications; however, collecting informative data is challenging\nbecause unstable systems often lead to rapidly growing or erratic trajectories.\nIn this work, we propose an adaptive sampling scheme that generates data while\nsimultaneously stabilizing the system to avoid instabilities during the data\ncollection. Under mild assumptions, the approach provably generates data sets\nthat are informative for stabilization and have minimal size. The numerical\nexperiments demonstrate that controller inference with the novel adaptive\nsampling approach learns controllers with up to one order of magnitude fewer\ndata samples than unguided data generation. The results show that the proposed\napproach opens the door to stabilizing systems in edge cases and limit states\nwhere instabilities often occur and data collection is inherently difficult.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u91c7\u6837\u65b9\u6848\uff0c\u7528\u4e8e\u5728\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\u4e2d\u907f\u514d\u7cfb\u7edf\u4e0d\u7a33\u5b9a\uff0c\u5e76\u751f\u6210\u7528\u4e8e\u63a7\u5236\u5668\u63a8\u65ad\u7684\u6700\u5c0f\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6570\u636e\u9700\u6c42\u51cf\u5c11\u4e86\u5341\u500d\u3002", "motivation": "\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u7a33\u5b9a\u63a7\u5236\u5668\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u4e0d\u7a33\u5b9a\u7cfb\u7edf\u53ef\u80fd\u5bfc\u81f4\u6570\u636e\u6536\u96c6\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u91c7\u6837\u65b9\u6848\uff0c\u5728\u6570\u636e\u6536\u96c6\u7684\u540c\u65f6\u7a33\u5b9a\u7cfb\u7edf\uff0c\u907f\u514d\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u786e\u4fdd\u751f\u6210\u7684\u6570\u636e\u96c6\u8db3\u591f\u5c0f\u4e14\u4fe1\u606f\u4e30\u5bcc\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u975e\u5f15\u5bfc\u6570\u636e\u751f\u6210\u5c11\u7528\u5341\u500d\u7684\u6570\u636e\u6837\u672c\u5373\u53ef\u5b66\u4e60\u5230\u7a33\u5b9a\u63a7\u5236\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u5728\u4e0d\u7a33\u5b9a\u6613\u53d1\u7684\u8fb9\u7f18\u60c5\u51b5\u548c\u6781\u9650\u72b6\u6001\u4e0b\u7a33\u5b9a\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u6536\u96c6\u7684\u56fa\u6709\u96be\u9898\u3002", "keywords": "\u81ea\u9002\u5e94\u91c7\u6837, \u7a33\u5b9a\u63a7\u5236\u5668, \u6570\u636e\u6536\u96c6, \u7cfb\u7edf\u7a33\u5b9a"}}
{"id": "2506.01782", "pdf": "https://arxiv.org/pdf/2506.01782", "abs": "https://arxiv.org/abs/2506.01782", "authors": ["Simon Mylius"], "title": "Systematic Hazard Analysis for Frontier AI using STPA", "categories": ["cs.CY", "cs.AI", "cs.SY", "eess.SY"], "comment": "29 pages, 5 figures, 7 tables", "summary": "All of the frontier AI companies have published safety frameworks where they\ndefine capability thresholds and risk mitigations that determine how they will\nsafely develop and deploy their models. Adoption of systematic approaches to\nrisk modelling, based on established practices used in safety-critical\nindustries, has been recommended, however frontier AI companies currently do\nnot describe in detail any structured approach to identifying and analysing\nhazards. STPA (Systems-Theoretic Process Analysis) is a systematic methodology\nfor identifying how complex systems can become unsafe, leading to hazards. It\nachieves this by mapping out controllers and controlled processes then\nanalysing their interactions and feedback loops to understand how harmful\noutcomes could occur (Leveson & Thomas, 2018). We evaluate STPA's ability to\nbroaden the scope, improve traceability and strengthen the robustness of safety\nassurance for frontier AI systems. Applying STPA to the threat model and\nscenario described in 'A Sketch of an AI Control Safety Case' (Korbak et al.,\n2025), we derive a list of Unsafe Control Actions. From these we select a\nsubset and explore the Loss Scenarios that lead to them if left unmitigated. We\nfind that STPA is able to identify causal factors that may be missed by\nunstructured hazard analysis methodologies thereby improving robustness. We\nsuggest STPA could increase the safety assurance of frontier AI when used to\ncomplement or check coverage of existing AI governance techniques including\ncapability thresholds, model evaluations and emergency procedures. The\napplication of a systematic methodology supports scalability by increasing the\nproportion of the analysis that could be conducted by LLMs, reducing the burden\non human domain experts.", "AI": {"tldr": "\u524d\u6cbfAI\u516c\u53f8\u5df2\u53d1\u5e03\u5b89\u5168\u6846\u67b6\uff0c\u4f46\u7f3a\u4e4f\u8be6\u7ec6\u7684\u7ed3\u6784\u5316\u98ce\u9669\u5206\u6790\u65b9\u6cd5\u3002STPA\u65b9\u6cd5\u53ef\u63d0\u5347AI\u7cfb\u7edf\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u5206\u6790\u8bc6\u522b\u6f5c\u5728\u98ce\u9669\u3002", "motivation": "\u524d\u6cbfAI\u516c\u53f8\u5f53\u524d\u7684\u5b89\u5168\u6846\u67b6\u7f3a\u4e4f\u7ed3\u6784\u5316\u98ce\u9669\u8bc6\u522b\u65b9\u6cd5\uff0c\u9700\u5f15\u5165\u7cfb\u7edf\u6027\u65b9\u6cd5\u4ee5\u589e\u5f3a\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528STPA\u65b9\u6cd5\u5206\u6790AI\u7cfb\u7edf\u7684\u63a7\u5236\u5668\u4e0e\u53d7\u63a7\u8fc7\u7a0b\uff0c\u8bc6\u522b\u4e0d\u5b89\u5168\u63a7\u5236\u884c\u4e3a\u53ca\u635f\u5931\u573a\u666f\u3002", "result": "STPA\u80fd\u8bc6\u522b\u975e\u7ed3\u6784\u5316\u65b9\u6cd5\u53ef\u80fd\u9057\u6f0f\u7684\u56e0\u679c\u56e0\u7d20\uff0c\u63d0\u5347\u5b89\u5168\u4fdd\u8bc1\u7684\u7a33\u5065\u6027\u3002", "conclusion": "STPA\u53ef\u8865\u5145\u73b0\u6709AI\u6cbb\u7406\u6280\u672f\uff0c\u63d0\u9ad8\u5b89\u5168\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7LLM\u5b9e\u73b0\u5206\u6790\u7684\u53ef\u6269\u5c55\u6027\u3002", "keywords": "\u524d\u6cbfAI, \u5b89\u5168\u6846\u67b6, STPA, \u98ce\u9669\u5206\u6790, \u63a7\u5236\u5b89\u5168"}}
{"id": "2506.01845", "pdf": "https://arxiv.org/pdf/2506.01845", "abs": "https://arxiv.org/abs/2506.01845", "authors": ["Kwanghee Choi", "Masao Someki", "Emma Strubell", "Shinji Watanabe"], "title": "On-device Streaming Discrete Speech Units", "categories": ["eess.AS", "cs.LG", "cs.SD"], "comment": "Accepted to Interspeech 2025, source code at\n  https://github.com/Masao-Someki/StreamingDSU", "summary": "Discrete speech units (DSUs) are derived from clustering the features of\nself-supervised speech models (S3Ms). DSUs offer significant advantages for\non-device streaming speech applications due to their rich phonetic information,\nhigh transmission efficiency, and seamless integration with large language\nmodels. However, conventional DSU-based approaches are impractical as they\nrequire full-length speech input and computationally expensive S3Ms. In this\nwork, we reduce both the attention window and the model size while preserving\nthe effectiveness of DSUs. Our results demonstrate that we can reduce\nfloating-point operations (FLOPs) by 50% with only a relative increase of 6.5%\nin character error rate (CER) on the ML-SUPERB 1h dataset. These findings\nhighlight the potential of DSUs for real-time speech processing in\nresource-constrained environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u79bb\u6563\u8bed\u97f3\u5355\u5143\uff08DSUs\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c0f\u6ce8\u610f\u529b\u7a97\u53e3\u548c\u6a21\u578b\u89c4\u6a21\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u97f3\u8bc6\u522b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfDSU\u65b9\u6cd5\u9700\u8981\u5b8c\u6574\u8bed\u97f3\u8f93\u5165\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\uff08S3Ms\uff09\uff0c\u4e0d\u9002\u5408\u5b9e\u65f6\u5904\u7406\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u51cf\u5c0f\u6ce8\u610f\u529b\u7a97\u53e3\u548c\u6a21\u578b\u89c4\u6a21\uff0c\u51cf\u5c11\u6d6e\u70b9\u8fd0\u7b97\uff08FLOPs\uff09\uff0c\u540c\u65f6\u786e\u4fddDSU\u7684\u8bed\u97f3\u4fe1\u606f\u4fdd\u7559\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5728ML-SUPERB 1h\u6570\u636e\u96c6\u4e0a\uff0cFLOPs\u51cf\u5c11\u4e8650%\uff0c\u800c\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09\u4ec5\u76f8\u5bf9\u589e\u52a06.5%\u3002", "conclusion": "\u6539\u8fdb\u540e\u7684DSU\u65b9\u6cd5\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u8bed\u97f3\u5904\u7406\u3002", "keywords": "\u79bb\u6563\u8bed\u97f3\u5355\u5143\uff08DSUs\uff09\u3001\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\uff08S3Ms\uff09\u3001\u5b9e\u65f6\u5904\u7406\u3001\u8d44\u6e90\u53d7\u9650\u3001\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09"}}
{"id": "2506.01850", "pdf": "https://arxiv.org/pdf/2506.01850", "abs": "https://arxiv.org/abs/2506.01850", "authors": ["Wayner Barrios", "Andr\u00e9s Villa", "Juan Le\u00f3n Alc\u00e1zar", "SouYoung Jin", "Bernard Ghanem"], "title": "MoDA: Modulation Adapter for Fine-Grained Visual Grounding in Instructional MLLMs", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": null, "summary": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated\nimpressive performance on instruction-following tasks by integrating pretrained\nvisual encoders with large language models (LLMs). However, existing approaches\noften struggle to ground fine-grained visual concepts in complex scenes. In\nthis paper, we propose MoDA (Modulation Adapter), a lightweight yet effective\nmodule designed to refine pre-aligned visual features through\ninstruction-guided modulation. Our approach follows the standard LLaVA training\nprotocol, consisting of a two-stage process: (1) aligning image features to the\nLLMs input space via a frozen vision encoder and adapter layers, and (2)\nrefining those features using the MoDA adapter during the instructional tuning\nstage. MoDA employs a Transformer-based cross-attention mechanism to generate a\nmodulation mask over the aligned visual tokens, thereby emphasizing\nsemantically relevant embedding dimensions based on the language instruction.\nThe modulated features are then passed to the LLM for autoregressive language\ngeneration. Our experimental evaluation shows that MoDA improves visual\ngrounding and generates more contextually appropriate responses, demonstrating\nits effectiveness as a general-purpose enhancement for image-based MLLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMoDA\uff08Modulation Adapter\uff09\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u901a\u8fc7\u6307\u4ee4\u5f15\u5bfc\u7684\u8c03\u5236\u4f18\u5316\u9884\u5bf9\u9f50\u89c6\u89c9\u7279\u5f81\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u6982\u5ff5\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u96be\u4ee5\u5bf9\u7ec6\u7c92\u5ea6\u89c6\u89c9\u6982\u5ff5\u8fdb\u884c\u6709\u6548\u5b9a\u4f4d\uff0c\u9700\u8981\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a1\uff09\u901a\u8fc7\u51bb\u7ed3\u89c6\u89c9\u7f16\u7801\u5668\u548c\u9002\u914d\u5c42\u5c06\u56fe\u50cf\u7279\u5f81\u5bf9\u9f50\u5230LLMs\u8f93\u5165\u7a7a\u95f4\uff1b2\uff09\u5728\u6307\u4ee4\u5fae\u8c03\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8eTransformer\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u7684MoDA\u6a21\u5757\u751f\u6210\u8c03\u5236\u63a9\u7801\uff0c\u5f3a\u8c03\u4e0e\u8bed\u8a00\u6307\u4ee4\u76f8\u5173\u7684\u89c6\u89c9\u7279\u5f81\u7ef4\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMoDA\u63d0\u5347\u4e86\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\uff0c\u5e76\u751f\u6210\u4e86\u66f4\u7b26\u5408\u4e0a\u4e0b\u6587\u7684\u56de\u7b54\u3002", "conclusion": "MoDA\u662f\u4e00\u79cd\u901a\u7528\u589e\u5f3a\u6a21\u5757\uff0c\u53ef\u6709\u6548\u63d0\u5347\u57fa\u4e8e\u56fe\u50cf\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, \u89c6\u89c9\u5b9a\u4f4d, \u8c03\u5236\u9002\u914d\u5668, \u6307\u4ee4\u5f15\u5bfc, Transformer"}}
{"id": "2506.01882", "pdf": "https://arxiv.org/pdf/2506.01882", "abs": "https://arxiv.org/abs/2506.01882", "authors": ["Peter Sentz", "Stanley Nicholson", "Yujin Cho", "Sohail Reddy", "Brendan Keith", "Stefanie G\u00fcnther"], "title": "Learning thermodynamic master equations for open quantum systems", "categories": ["quant-ph", "cs.LG", "I.2.6; J.2"], "comment": "20 pages, 7 figures", "summary": "The characterization of Hamiltonians and other components of open quantum\ndynamical systems plays a crucial role in quantum computing and other\napplications. Scientific machine learning techniques have been applied to this\nproblem in a variety of ways, including by modeling with deep neural networks.\nHowever, the majority of mathematical models describing open quantum systems\nare linear, and the natural nonlinearities in learnable models have not been\nincorporated using physical principles. We present a data-driven model for open\nquantum systems that includes learnable, thermodynamically consistent terms.\nThe trained model is interpretable, as it directly estimates the system\nHamiltonian and linear components of coupling to the environment. We validate\nthe model on synthetic two and three-level data, as well as experimental\ntwo-level data collected from a quantum device at Lawrence Livermore National\nLaboratory.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u53ef\u5b66\u4e60\u7684\u3001\u70ed\u529b\u5b66\u4e00\u81f4\u7684\u9879\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u54c8\u5bc6\u987f\u91cf\u548c\u5176\u4ed6\u7ec4\u4ef6\u7684\u8868\u5f81\u5bf9\u91cf\u5b50\u8ba1\u7b97\u81f3\u5173\u91cd\u8981\uff0c\u7136\u800c\u73b0\u6709\u6a21\u578b\u591a\u4e3a\u7ebf\u6027\uff0c\u7f3a\u4e4f\u7269\u7406\u539f\u7406\u5f15\u5165\u7684\u975e\u7ebf\u6027\u7279\u6027\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u4e14\u70ed\u529b\u5b66\u4e00\u81f4\u7684\u9879\uff0c\u76f4\u63a5\u4f30\u8ba1\u7cfb\u7edf\u54c8\u5bc6\u987f\u91cf\u548c\u73af\u5883\u8026\u5408\u7684\u7ebf\u6027\u90e8\u5206\u3002", "result": "\u6a21\u578b\u5728\u5408\u6210\u7684\u4e24\u80fd\u7ea7\u548c\u4e09\u80fd\u7ea7\u6570\u636e\u4ee5\u53ca\u5b9e\u9a8c\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u6a21\u578b\u5177\u6709\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u80fd\u6709\u6548\u8868\u5f81\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u3002", "keywords": "\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\uff0c\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff0c\u70ed\u529b\u5b66\u4e00\u81f4\u6027\uff0c\u54c8\u5bc6\u987f\u91cf\u4f30\u8ba1"}}
{"id": "2506.01806", "pdf": "https://arxiv.org/pdf/2506.01806", "abs": "https://arxiv.org/abs/2506.01806", "authors": ["Shubham Pandey", "Bhavin Jawade", "Srirangaraj Setlur"], "title": "Ridgeformer: Mutli-Stage Contrastive Training For Fine-grained Cross-Domain Fingerprint Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to IEEE International Conference on Image Processing 2025", "summary": "The increasing demand for hygienic and portable biometric systems has\nunderscored the critical need for advancements in contactless fingerprint\nrecognition. Despite its potential, this technology faces notable challenges,\nincluding out-of-focus image acquisition, reduced contrast between fingerprint\nridges and valleys, variations in finger positioning, and perspective\ndistortion. These factors significantly hinder the accuracy and reliability of\ncontactless fingerprint matching. To address these issues, we propose a novel\nmulti-stage transformer-based contactless fingerprint matching approach that\nfirst captures global spatial features and subsequently refines localized\nfeature alignment across fingerprint samples. By employing a hierarchical\nfeature extraction and matching pipeline, our method ensures fine-grained,\ncross-sample alignment while maintaining the robustness of global feature\nrepresentation. We perform extensive evaluations on publicly available datasets\nsuch as HKPolyU and RidgeBase under different evaluation protocols, such as\ncontactless-to-contact matching and contactless-to-contactless matching and\ndemonstrate that our proposed approach outperforms existing methods, including\nCOTS solutions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u591a\u9636\u6bb5\u975e\u63a5\u89e6\u5f0f\u6307\u7eb9\u5339\u914d\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u4e2d\u56e0\u56fe\u50cf\u6a21\u7cca\u3001\u5bf9\u6bd4\u5ea6\u4f4e\u548c\u624b\u6307\u4f4d\u7f6e\u53d8\u5316\u7b49\u95ee\u9898\u5bfc\u81f4\u7684\u5339\u914d\u4e0d\u51c6\u786e\u95ee\u9898\u3002", "motivation": "\u975e\u63a5\u89e6\u5f0f\u6307\u7eb9\u8bc6\u522b\u56e0\u536b\u751f\u548c\u4fbf\u643a\u6027\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709\u6280\u672f\u9762\u4e34\u56fe\u50cf\u6a21\u7cca\u3001\u5bf9\u6bd4\u5ea6\u4f4e\u3001\u624b\u6307\u4f4d\u7f6e\u53d8\u5316\u548c\u900f\u89c6\u53d8\u5f62\u7b49\u6311\u6218\uff0c\u5f71\u54cd\u4e86\u5339\u914d\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5Transformer\u65b9\u6cd5\uff0c\u5148\u63d0\u53d6\u5168\u5c40\u7a7a\u95f4\u7279\u5f81\uff0c\u518d\u4f18\u5316\u5c40\u90e8\u7279\u5f81\u5bf9\u9f50\uff0c\u901a\u8fc7\u5206\u5c42\u7279\u5f81\u63d0\u53d6\u548c\u5339\u914d\u6d41\u7a0b\u5b9e\u73b0\u7cbe\u51c6\u5339\u914d\u3002", "result": "\u5728HKPolyU\u548cRidgeBase\u7b49\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u975e\u63a5\u89e6-\u63a5\u89e6\u548c\u975e\u63a5\u89e6-\u975e\u63a5\u89e6\u5339\u914d\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\uff08COTS\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u9636\u6bb5Transformer\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u975e\u63a5\u89e6\u5f0f\u6307\u7eb9\u5339\u914d\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "keywords": "\u975e\u63a5\u89e6\u5f0f\u6307\u7eb9\u8bc6\u522b, Transformer, \u7279\u5f81\u5bf9\u9f50, \u5206\u5c42\u5339\u914d"}}
{"id": "2506.01891", "pdf": "https://arxiv.org/pdf/2506.01891", "abs": "https://arxiv.org/abs/2506.01891", "authors": ["Mahmud Ashraf Shamim", "Eric Reinhardt", "Talal Ahmed Chowdhury", "Sergei Gleyzer", "Paulo T Araujo"], "title": "Probing Quantum Spin Systems with Kolmogorov-Arnold Neural Network Quantum States", "categories": ["quant-ph", "cond-mat.dis-nn", "cond-mat.str-el", "cs.LG"], "comment": "16 pages, 13 figures", "summary": "Neural Quantum States (NQS) are a class of variational wave functions\nparametrized by neural networks (NNs) to study quantum many-body systems. In\nthis work, we propose SineKAN, the NQS ansatz based on Kolmogorov-Arnold\nNetworks (KANs), to represent quantum mechanical wave functions as nested\nunivariate functions. We show that \\sk wavefunction with learnable sinusoidal\nactivation functions can capture the ground state energies, fidelities and\nvarious correlation functions of the 1D Transverse-Field Ising model,\nAnisotropic Heisenberg model, and Antiferromagnetic $J_{1}-J_{2}$ model with\ndifferent chain lengths. In our study of the $J_1-J_2$ model with $L=100$\nsites, we find that the SineKAN model outperforms several previously explored\nneural quantum state ans\\\"atze, including Restricted Boltzmann Machines (RBMs),\nLong Short-Term Memory models (LSTMs), and Feed-Forward Neural Networks (FFCN),\nwhen compared to the results obtained from the Density Matrix Renormalization\nGroup (DMRG) algorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8eKolmogorov-Arnold\u7f51\u7edc\uff08KANs\uff09\u7684SineKAN\u4f5c\u4e3a\u795e\u7ecf\u91cf\u5b50\u6001\uff08NQS\uff09\u7684\u65b0\u53d8\u5206\u6ce2\u51fd\u6570\uff0c\u7528\u4e8e\u7814\u7a76\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\uff0c\u5e76\u5728\u591a\u4e2a\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u91cf\u5b50\u6001\u8868\u793a\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u57fa\u6001\u80fd\u91cf\u548c\u5173\u8054\u51fd\u6570\u7684\u8ba1\u7b97\u7cbe\u5ea6\u3002", "method": "\u4f7f\u7528\u5e26\u6709\u53ef\u5b66\u4e60\u6b63\u5f26\u6fc0\u6d3b\u51fd\u6570\u7684SineKAN\u6a21\u578b\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u4e00\u7ef4\u6a2a\u5411\u573a\u4f0a\u8f9b\u6a21\u578b\u3001\u5404\u5411\u5f02\u6027\u6d77\u68ee\u5821\u6a21\u578b\u548c\u53cd\u94c1\u78c1$J_{1}-J_{2}$\u6a21\u578b\u3002", "result": "SineKAN\u5728$J_1-J_2$\u6a21\u578b\uff08L=100\uff09\u4e2d\u8868\u73b0\u4f18\u4e8eRBM\u3001LSTM\u548cFFCN\u7b49\u73b0\u6709\u65b9\u6cd5\uff0c\u63a5\u8fd1DMRG\u7b97\u6cd5\u7684\u7ed3\u679c\u3002", "conclusion": "SineKAN\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684NQS\u53d8\u5206\u6ce2\u51fd\u6570\uff0c\u5728\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7814\u7a76\u4e2d\u5177\u6709\u4f18\u8d8a\u6027\u548c\u6f5c\u529b\u3002", "keywords": "\u795e\u7ecf\u91cf\u5b50\u6001, Kolmogorov-Arnold\u7f51\u7edc, \u53d8\u5206\u6ce2\u51fd\u6570, \u91cf\u5b50\u591a\u4f53\u7cfb\u7edf, \u57fa\u6001\u80fd\u91cf"}}
{"id": "2506.01824", "pdf": "https://arxiv.org/pdf/2506.01824", "abs": "https://arxiv.org/abs/2506.01824", "authors": ["Pedro Zuidberg Dos Martires"], "title": "A Quantum Information Theoretic Approach to Tractable Probabilistic Models", "categories": ["quant-ph", "cs.AI"], "comment": null, "summary": "By recursively nesting sums and products, probabilistic circuits have emerged\nin recent years as an attractive class of generative models as they enjoy, for\ninstance, polytime marginalization of random variables. In this work we study\nthese machine learning models using the framework of quantum information\ntheory, leading to the introduction of positive unital circuits (PUnCs), which\ngeneralize circuit evaluations over positive real-valued probabilities to\ncircuit evaluations over positive semi-definite matrices. As a consequence,\nPUnCs strictly generalize probabilistic circuits as well as recently introduced\ncircuit classes such as PSD circuits.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86\u901a\u8fc7\u91cf\u5b50\u4fe1\u606f\u7406\u8bba\u6846\u67b6\u7814\u7a76\u6982\u7387\u7535\u8def\uff0c\u5f15\u5165\u4e86\u6b63\u5355\u5143\u7535\u8def\uff08PUnCs\uff09\uff0c\u6269\u5c55\u4e86\u6982\u7387\u7535\u8def\u7684\u5e94\u7528\u8303\u56f4\u3002", "motivation": "\u5229\u7528\u91cf\u5b50\u4fe1\u606f\u7406\u8bba\u6846\u67b6\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u6269\u5c55\u6982\u7387\u7535\u8def\u7684\u529f\u80fd\u548c\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u9012\u5f52\u5d4c\u5957\u548c\u548c\u79ef\u64cd\u4f5c\uff0c\u5f15\u5165\u6b63\u5355\u5143\u7535\u8def\uff08PUnCs\uff09\uff0c\u5c06\u6982\u7387\u7535\u8def\u6269\u5c55\u5230\u534a\u6b63\u5b9a\u77e9\u9635\u8bc4\u4f30\u3002", "result": "PUnCs\u4e25\u683c\u6cdb\u5316\u4e86\u6982\u7387\u7535\u8def\u4ee5\u53caPSD\u7535\u8def\u7b49\u65b0\u8fd1\u5f15\u5165\u7684\u7535\u8def\u7c7b\u522b\u3002", "conclusion": "PUnCs\u4e3a\u6982\u7387\u7535\u8def\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u6846\u67b6\uff0c\u4e3a\u7535\u8def\u8bc4\u4f30\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "keywords": "\u6982\u7387\u7535\u8def\u3001\u91cf\u5b50\u4fe1\u606f\u7406\u8bba\u3001\u6b63\u5355\u5143\u7535\u8def\uff08PUnCs\uff09\u3001\u534a\u6b63\u5b9a\u77e9\u9635"}}
{"id": "2506.01904", "pdf": "https://arxiv.org/pdf/2506.01904", "abs": "https://arxiv.org/abs/2506.01904", "authors": ["Qijia Jiang", "Reuben Cohn-Gordon"], "title": "Machine-Learned Sampling of Conditioned Path Measures", "categories": ["stat.ML", "cs.LG", "stat.CO"], "comment": null, "summary": "We propose algorithms for sampling from posterior path measures $P(C([0, T],\n\\mathbb{R}^d))$ under a general prior process. This leverages ideas from (1)\ncontrolled equilibrium dynamics, which gradually transport between two path\nmeasures, and (2) optimization in $\\infty$-dimensional probability space\nendowed with a Wasserstein metric, which can be used to evolve a density curve\nunder the specified likelihood. The resulting algorithms are theoretically\ngrounded and can be integrated seamlessly with neural networks for learning the\ntarget trajectory ensembles, without access to data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u540e\u9a8c\u8def\u5f84\u6d4b\u5ea6\u4e2d\u91c7\u6837\u7684\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86\u63a7\u5236\u5e73\u8861\u52a8\u6001\u548c\u6982\u7387\u7a7a\u95f4\u4f18\u5316\u7684\u601d\u60f3\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u4ece\u540e\u9a8c\u8def\u5f84\u6d4b\u5ea6\u4e2d\u9ad8\u6548\u91c7\u6837\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5229\u7528\u63a7\u5236\u5e73\u8861\u52a8\u6001\u9010\u6b65\u4f20\u8f93\u8def\u5f84\u6d4b\u5ea6\uff0c\u5e76\u7ed3\u5408Wasserstein\u5ea6\u91cf\u4e0b\u7684\u6982\u7387\u7a7a\u95f4\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u5f00\u53d1\u51fa\u7406\u8bba\u4e0a\u6709\u57fa\u7840\u7684\u7b97\u6cd5\uff0c\u5e76\u80fd\u4e0e\u795e\u7ecf\u7f51\u7edc\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u5b66\u4e60\u76ee\u6807\u8f68\u8ff9\u96c6\u5408\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\u3002", "keywords": "\u540e\u9a8c\u8def\u5f84\u6d4b\u5ea6,\u63a7\u5236\u5e73\u8861\u52a8\u6001,Wasserstein\u5ea6\u91cf,\u795e\u7ecf\u7f51\u7edc"}}
{"id": "2506.01923", "pdf": "https://arxiv.org/pdf/2506.01923", "abs": "https://arxiv.org/abs/2506.01923", "authors": ["Amin Karimi Monsefi", "Mridul Khurana", "Rajiv Ramnath", "Anuj Karpatne", "Wei-Lun Chao", "Cheng Zhang"], "title": "TaxaDiffusion: Progressively Trained Diffusion Model for Fine-Grained Species Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We propose TaxaDiffusion, a taxonomy-informed training framework for\ndiffusion models to generate fine-grained animal images with high morphological\nand identity accuracy. Unlike standard approaches that treat each species as an\nindependent category, TaxaDiffusion incorporates domain knowledge that many\nspecies exhibit strong visual similarities, with distinctions often residing in\nsubtle variations of shape, pattern, and color. To exploit these relationships,\nTaxaDiffusion progressively trains conditioned diffusion models across\ndifferent taxonomic levels -- starting from broad classifications such as Class\nand Order, refining through Family and Genus, and ultimately distinguishing at\nthe Species level. This hierarchical learning strategy first captures\ncoarse-grained morphological traits shared by species with common ancestors,\nfacilitating knowledge transfer before refining fine-grained differences for\nspecies-level distinction. As a result, TaxaDiffusion enables accurate\ngeneration even with limited training samples per species. Extensive\nexperiments on three fine-grained animal datasets demonstrate that outperforms\nexisting approaches, achieving superior fidelity in fine-grained animal image\ngeneration. Project page: https://amink8.github.io/TaxaDiffusion/", "AI": {"tldr": "TaxaDiffusion\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u7c7b\u5b66\u77e5\u8bc6\u7684\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u7ec6\u7c92\u5ea6\u52a8\u7269\u56fe\u50cf\uff0c\u901a\u8fc7\u5206\u5c42\u5b66\u4e60\u7b56\u7565\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u7ec6\u7c92\u5ea6\u52a8\u7269\u56fe\u50cf\u751f\u6210\u4e2d\u5f62\u6001\u548c\u8eab\u4efd\u51c6\u786e\u6027\u7684\u6311\u6218\uff0cTaxaDiffusion\u5229\u7528\u7269\u79cd\u95f4\u7684\u89c6\u89c9\u76f8\u4f3c\u6027\uff0c\u901a\u8fc7\u5206\u7c7b\u5b66\u5c42\u6b21\u6307\u5bfc\u8bad\u7ec3\u3002", "method": "\u91c7\u7528\u5206\u5c42\u8bad\u7ec3\u7b56\u7565\uff0c\u4ece\u7c97\u5230\u7ec6\uff08\u5982\u4ece\u7c7b\u3001\u76ee\u5230\u79d1\u3001\u5c5e\u518d\u5230\u79cd\uff09\u9010\u6b65\u8bad\u7ec3\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u6355\u6349\u5171\u4eab\u5f62\u6001\u7279\u5f81\u5e76\u7ec6\u5316\u7269\u79cd\u5dee\u5f02\u3002", "result": "\u5728\u4e09\u4e2a\u7ec6\u7c92\u5ea6\u52a8\u7269\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTaxaDiffusion\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u56fe\u50cf\u7684\u4fdd\u771f\u5ea6\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "TaxaDiffusion\u901a\u8fc7\u6574\u5408\u5206\u7c7b\u5b66\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u52a8\u7269\u56fe\u50cf\u7684\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u5176\u4ed6\u9886\u57df\u7684\u5206\u5c42\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u501f\u9274\u3002", "keywords": "TaxaDiffusion, \u6269\u6563\u6a21\u578b, \u7ec6\u7c92\u5ea6\u56fe\u50cf\u751f\u6210, \u5206\u7c7b\u5b66, \u5206\u5c42\u8bad\u7ec3"}}
{"id": "2506.01929", "pdf": "https://arxiv.org/pdf/2506.01929", "abs": "https://arxiv.org/abs/2506.01929", "authors": ["Saar Huberman", "Or Patashnik", "Omer Dahary", "Ron Mokady", "Daniel Cohen-Or"], "title": "Image Generation from Contextually-Contradictory Prompts", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": "Project page: https://tdpc2025.github.io/SAP/", "summary": "Text-to-image diffusion models excel at generating high-quality, diverse\nimages from natural language prompts. However, they often fail to produce\nsemantically accurate results when the prompt contains concept combinations\nthat contradict their learned priors. We define this failure mode as contextual\ncontradiction, where one concept implicitly negates another due to entangled\nassociations learned during training. To address this, we propose a stage-aware\nprompt decomposition framework that guides the denoising process using a\nsequence of proxy prompts. Each proxy prompt is constructed to match the\nsemantic content expected to emerge at a specific stage of denoising, while\nensuring contextual coherence. To construct these proxy prompts, we leverage a\nlarge language model (LLM) to analyze the target prompt, identify\ncontradictions, and generate alternative expressions that preserve the original\nintent while resolving contextual conflicts. By aligning prompt information\nwith the denoising progression, our method enables fine-grained semantic\ncontrol and accurate image generation in the presence of contextual\ncontradictions. Experiments across a variety of challenging prompts show\nsubstantial improvements in alignment to the textual prompt.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9636\u6bb5\u611f\u77e5\u63d0\u793a\u5206\u89e3\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u56e0\u4e0a\u4e0b\u6587\u77db\u76fe\u5bfc\u81f4\u8bed\u4e49\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u5305\u542b\u77db\u76fe\u6982\u5ff5\u7ec4\u5408\u7684\u63d0\u793a\u65f6\uff0c\u5f80\u5f80\u4f1a\u4ea7\u751f\u8bed\u4e49\u4e0d\u51c6\u786e\u7684\u7ed3\u679c\u3002\u5b9a\u4e49\u8fd9\u79cd\u95ee\u9898\u4e3a\u4e0a\u4e0b\u6587\u77db\u76fe\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u9636\u6bb5\u611f\u77e5\u63d0\u793a\u5206\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7406\u63d0\u793a\u5e8f\u5217\u5f15\u5bfc\u53bb\u566a\u8fc7\u7a0b\u3002\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5206\u6790\u76ee\u6807\u63d0\u793a\uff0c\u8bc6\u522b\u77db\u76fe\u5e76\u751f\u6210\u66ff\u4ee3\u8868\u8fbe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u63d0\u793a\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u4e0e\u6587\u672c\u63d0\u793a\u7684\u5bf9\u9f50\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u9636\u6bb5\u5bf9\u9f50\u548c\u8bed\u4e49\u5206\u89e3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587\u77db\u76fe\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u56fe\u50cf\u751f\u6210\u3002", "keywords": "\u6587\u672c\u5230\u56fe\u50cf\uff0c\u6269\u6563\u6a21\u578b\uff0c\u4e0a\u4e0b\u6587\u77db\u76fe\uff0c\u63d0\u793a\u5206\u89e3\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.01936", "pdf": "https://arxiv.org/pdf/2506.01936", "abs": "https://arxiv.org/abs/2506.01936", "authors": ["Han Shao", "Shuo Xie", "Kunhe Yang"], "title": "Should Decision-Makers Reveal Classifiers in Online Strategic Classification?", "categories": ["cs.GT", "cs.LG"], "comment": null, "summary": "Strategic classification addresses a learning problem where a decision-maker\nimplements a classifier over agents who may manipulate their features in order\nto receive favorable predictions. In the standard model of online strategic\nclassification, in each round, the decision-maker implements and publicly\nreveals a classifier, after which agents perfectly best respond based on this\nknowledge. However, in practice, whether to disclose the classifier is often\ndebated -- some decision-makers believe that hiding the classifier can prevent\nmisclassification errors caused by manipulation.\n  In this paper, we formally examine how limiting the agents' access to the\ncurrent classifier affects the decision-maker's performance. Specifically, we\nconsider an extended online strategic classification setting where agents lack\ndirect knowledge about the current classifier and instead manipulate based on a\nweighted average of historically implemented classifiers. Our main result shows\nthat in this setting, the decision-maker incurs $(1-\\gamma)^{-1}$ or\n$k_{\\text{in}}$ times more mistakes compared to the full-knowledge setting,\nwhere $k_{\\text{in}}$ is the maximum in-degree of the manipulation graph\n(representing how many distinct feature vectors can be manipulated to appear as\na single one), and $\\gamma$ is the discount factor indicating agents' memory of\npast classifiers. Our results demonstrate how withholding access to the\nclassifier can backfire and degrade the decision-maker's performance in online\nstrategic classification.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u6218\u7565\u5206\u7c7b\u4e2d\u9650\u5236\u5bf9\u624b\uff08agents\uff09\u5bf9\u5f53\u524d\u5206\u7c7b\u5668\u7684\u8bbf\u95ee\u5982\u4f55\u5f71\u54cd\u51b3\u7b56\u8005\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u9690\u85cf\u5206\u7c7b\u5668\u53ef\u80fd\u5bfc\u81f4\u66f4\u591a\u9519\u8bef\u3002", "motivation": "\u63a2\u8ba8\u51b3\u7b56\u8005\u662f\u5426\u5e94\u516c\u5f00\u5206\u7c7b\u5668\uff0c\u4ee5\u9632\u6b62\u5bf9\u624b\u901a\u8fc7\u64cd\u7eb5\u7279\u5f81\u83b7\u53d6\u6709\u5229\u9884\u6d4b\uff0c\u5b9e\u8df5\u4e2d\u8fd9\u4e00\u505a\u6cd5\u5b58\u5728\u4e89\u8bae\u3002", "method": "\u6269\u5c55\u4e86\u5728\u7ebf\u6218\u7565\u5206\u7c7b\u6a21\u578b\uff0c\u5047\u8bbe\u5bf9\u624b\u57fa\u4e8e\u5386\u53f2\u5206\u7c7b\u5668\u7684\u52a0\u6743\u5e73\u5747\u8fdb\u884c\u64cd\u7eb5\uff0c\u800c\u975e\u76f4\u63a5\u8bbf\u95ee\u5f53\u524d\u5206\u7c7b\u5668\u3002", "result": "\u51b3\u7b56\u8005\u5728\u9690\u85cf\u5206\u7c7b\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u9519\u8bef\u7387\u589e\u52a0\u4e3a $(1-\\gamma)^{-1}$ \u6216 $k_{\\text{in}}$ \u500d\uff0c\u5176\u4e2d $k_{\\text{in}}$ \u662f\u64cd\u7eb5\u56fe\u7684\u6700\u5927\u5165\u5ea6\uff0c$\\gamma$ \u662f\u6298\u6263\u56e0\u5b50\u3002", "conclusion": "\u9690\u85cf\u5206\u7c7b\u5668\u53ef\u80fd\u9002\u5f97\u5176\u53cd\uff0c\u964d\u4f4e\u4e86\u51b3\u7b56\u8005\u5728\u5728\u7ebf\u6218\u7565\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002", "keywords": "\u6218\u7565\u5206\u7c7b\u3001\u5728\u7ebf\u5b66\u4e60\u3001\u5206\u7c7b\u5668\u9690\u85cf\u3001\u7279\u5f81\u64cd\u7eb5\u3001\u51b3\u7b56\u8005\u6027\u80fd"}}
{"id": "2506.01945", "pdf": "https://arxiv.org/pdf/2506.01945", "abs": "https://arxiv.org/abs/2506.01945", "authors": ["Nurbanu Bursa"], "title": "Stock Market Telepathy: Graph Neural Networks Predicting the Secret Conversations between MINT and G7 Countries", "categories": ["econ.EM", "cs.LG", "stat.AP"], "comment": null, "summary": "Emerging economies, particularly the MINT countries (Mexico, Indonesia,\nNigeria, and T\\\"urkiye), are gaining influence in global stock markets,\nalthough they remain susceptible to the economic conditions of developed\ncountries like the G7 (Canada, France, Germany, Italy, Japan, the United\nKingdom, and the United States). This interconnectedness and sensitivity of\nfinancial markets make understanding these relationships crucial for investors\nand policymakers to predict stock price movements accurately. To this end, we\nexamined the main stock market indices of G7 and MINT countries from 2012 to\n2024, using a recent graph neural network (GNN) algorithm called multivariate\ntime series forecasting with graph neural network (MTGNN). This method allows\nfor considering complex spatio-temporal connections in multivariate time\nseries. In the implementations, MTGNN revealed that the US and Canada are the\nmost influential G7 countries regarding stock indices in the forecasting\nprocess, and Indonesia and T\\\"urkiye are the most influential MINT countries.\nAdditionally, our results showed that MTGNN outperformed traditional methods in\nforecasting the prices of stock market indices for MINT and G7 countries.\nConsequently, the study offers valuable insights into economic blocks' markets\nand presents a compelling empirical approach to analyzing global stock market\ndynamics using MTGNN.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86G7\u548cMINT\u56fd\u5bb6\u80a1\u5e02\u4e4b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\uff0c\u53d1\u73b0\u7f8e\u52a0\u662fG7\u4e2d\u5bf9\u80a1\u5e02\u6700\u5f71\u54cd\u7684\u56fd\u5bb6\uff0c\u5370\u5c3c\u548c\u571f\u8033\u5176\u5219\u5728MINT\u4e2d\u6700\u5177\u5f71\u54cd\u529b\u3002\u91c7\u7528MTGNN\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u65b0\u5174\u7ecf\u6d4e\u4f53\uff08MINT\u56fd\u5bb6\uff09\u4e0e\u53d1\u8fbe\u7ecf\u6d4e\u4f53\uff08G7\u56fd\u5bb6\uff09\u80a1\u5e02\u4e4b\u95f4\u7684\u5173\u8054\u6027\uff0c\u4e3a\u6295\u8d44\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u80a1\u5e02\u9884\u6d4b\u7684\u51c6\u786e\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08MTGNN\uff09\u5206\u67902012\u81f32024\u5e74G7\u548cMINT\u56fd\u5bb6\u7684\u4e3b\u8981\u80a1\u5e02\u6307\u6570\uff0c\u8003\u8651\u590d\u6742\u7684\u65f6\u7a7a\u5173\u8054\u3002", "result": "MTGNN\u663e\u793a\u7f8e\u52a0\u5728G7\u4e2d\u5f71\u54cd\u529b\u6700\u5927\uff0c\u5370\u5c3c\u548c\u571f\u8033\u5176\u5728MINT\u4e2d\u6700\u5177\u5f71\u54cd\u529b\uff1b\u4e14MTGNN\u9884\u6d4b\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5168\u7403\u7ecf\u6d4e\u4f53\u80a1\u5e02\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0cMTGNN\u662f\u4e00\u79cd\u6709\u6548\u7684\u5168\u7403\u80a1\u5e02\u52a8\u6001\u5206\u6790\u5de5\u5177\u3002", "keywords": "MINT\u56fd\u5bb6, G7\u56fd\u5bb6, \u80a1\u5e02\u9884\u6d4b, \u56fe\u795e\u7ecf\u7f51\u7edc, \u65f6\u7a7a\u5173\u8054"}}
{"id": "2506.01921", "pdf": "https://arxiv.org/pdf/2506.01921", "abs": "https://arxiv.org/abs/2506.01921", "authors": ["Minghao Liu", "Zhitao He", "Zhiyuan Fan", "Qingyun Wang", "Yi R. Fung"], "title": "MedEBench: Revisiting Text-instructed Image Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-guided image editing has seen rapid progress in natural image domains,\nbut its adaptation to medical imaging remains limited and lacks standardized\nevaluation. Clinically, such editing holds promise for simulating surgical\noutcomes, creating personalized teaching materials, and enhancing patient\ncommunication. To bridge this gap, we introduce \\textbf{MedEBench}, a\ncomprehensive benchmark for evaluating text-guided medical image editing. It\nconsists of 1,182 clinically sourced image-prompt triplets spanning 70 tasks\nacross 13 anatomical regions. MedEBench offers three key contributions: (1) a\nclinically relevant evaluation framework covering Editing Accuracy, Contextual\nPreservation, and Visual Quality, supported by detailed descriptions of\nexpected change and ROI (Region of Interest) masks; (2) a systematic comparison\nof seven state-of-the-art models, revealing common failure patterns; and (3) a\nfailure analysis protocol based on attention grounding, using IoU between\nattention maps and ROIs to identify mislocalization. MedEBench provides a solid\nfoundation for developing and evaluating reliable, clinically meaningful\nmedical image editing systems.", "AI": {"tldr": "MedEBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u6587\u672c\u5f15\u5bfc\u533b\u5b66\u56fe\u50cf\u7f16\u8f91\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u5305\u542b1,182\u4e2a\u4e34\u5e8a\u56fe\u50cf-\u63d0\u793a\u5bf9\uff0c\u8986\u76d613\u4e2a\u89e3\u5256\u533a\u57df\uff0c\u65e8\u5728\u586b\u8865\u8be5\u9886\u57df\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u7a7a\u767d\u3002", "motivation": "\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u5728\u81ea\u7136\u56fe\u50cf\u9886\u57df\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u5e94\u7528\u6709\u9650\u4e14\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\uff0c\u800c\u4e34\u5e8a\u4e0a\u6709\u6a21\u62df\u624b\u672f\u7ed3\u679c\u3001\u4e2a\u6027\u5316\u6559\u5b66\u548c\u60a3\u8005\u6c9f\u901a\u7b49\u6f5c\u5728\u4ef7\u503c\u3002", "method": "\u63d0\u51fa\u4e86MedEBench\u57fa\u51c6\uff0c\u5305\u62ec1,182\u4e2a\u4e34\u5e8a\u56fe\u50cf-\u63d0\u793a\u5bf9\uff0c\u8986\u76d670\u4e2a\u4efb\u52a1\u768413\u4e2a\u89e3\u5256\u533a\u57df\uff0c\u63d0\u4f9b\u7f16\u8f91\u51c6\u786e\u6027\u3001\u4e0a\u4e0b\u6587\u4fdd\u7559\u548c\u89c6\u89c9\u8d28\u91cf\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u6bd4\u8f837\u79cd\u6700\u5148\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5206\u6790\u5931\u8d25\u6a21\u5f0f\uff0c\u53d1\u73b0\u5e38\u89c1\u95ee\u9898\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eIoU\uff08\u4ea4\u5e76\u6bd4\uff09\u7684\u5b9a\u4f4d\u9519\u8bef\u5206\u6790\u65b9\u6cd5\u3002", "conclusion": "MedEBench\u4e3a\u5f00\u53d1\u53ef\u9760\u7684\u3001\u4e34\u5e8a\u610f\u4e49\u663e\u8457\u7684\u533b\u5b66\u56fe\u50cf\u7f16\u8f91\u7cfb\u7edf\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "keywords": "\u6587\u672c\u5f15\u5bfc\u7f16\u8f91, \u533b\u5b66\u5f71\u50cf, \u8bc4\u4f30\u57fa\u51c6, \u4e34\u5e8a\u76f8\u5173\u6027, \u6ce8\u610f\u529b\u673a\u5236"}}
{"id": "2506.01927", "pdf": "https://arxiv.org/pdf/2506.01927", "abs": "https://arxiv.org/abs/2506.01927", "authors": ["Mel Krusniak", "Hang Xu", "Parker Palermo", "Forrest Laine"], "title": "Online Competitive Information Gathering for Partially Observable Trajectory Games", "categories": ["cs.GT", "cs.AI", "cs.MA", "cs.RO"], "comment": "Accepted at RSS 2025", "summary": "Game-theoretic agents must make plans that optimally gather information about\ntheir opponents. These problems are modeled by partially observable stochastic\ngames (POSGs), but planning in fully continuous POSGs is intractable without\nheavy offline computation or assumptions on the order of belief maintained by\neach player. We formulate a finite history/horizon refinement of POSGs which\nadmits competitive information gathering behavior in trajectory space, and\nthrough a series of approximations, we present an online method for computing\nrational trajectory plans in these games which leverages particle-based\nestimations of the joint state space and performs stochastic gradient play. We\nalso provide the necessary adjustments required to deploy this method on\nindividual agents. The method is tested in continuous pursuit-evasion and\nwarehouse-pickup scenarios (alongside extensions to $N > 2$ players and to more\ncomplex environments with visual and physical obstacles), demonstrating\nevidence of active information gathering and outperforming passive competitors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u7684\u968f\u673a\u535a\u5f08\uff08POSG\uff09\u4e2d\u5728\u7ebf\u8ba1\u7b97\u5408\u7406\u8f68\u8ff9\u89c4\u5212\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c92\u5b50\u4f30\u8ba1\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u5b9e\u73b0\uff0c\u5c55\u793a\u4e86\u5728\u8fde\u7eed\u8ffd\u8e2a\u548c\u591a\u73a9\u5bb6\u573a\u666f\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u5b8c\u5168\u8fde\u7eedPOSG\u4e2d\u4fe1\u606f\u89c4\u5212\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u5728\u7ebf\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u4fe1\u606f\u83b7\u53d6\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u6709\u9650\u5386\u53f2\u548c\u65f6\u57df\u4f18\u5316\u7684POSG\u6a21\u578b\uff0c\u7ed3\u5408\u7c92\u5b50\u4f30\u8ba1\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u6280\u672f\uff0c\u4f18\u5316\u8f68\u8ff9\u89c4\u5212\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8fde\u7eed\u8ffd\u8e2a\u548c\u4ed3\u5e93\u53d6\u8d27\u7b49\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4e3b\u52a8\u4fe1\u606f\u83b7\u53d6\u80fd\u529b\uff0c\u5e76\u4f18\u4e8e\u88ab\u52a8\u7b56\u7565\u3002", "conclusion": "\u63d0\u51fa\u7684\u5728\u7ebf\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u73af\u5883\u4e2d\u7684\u4fe1\u606f\u83b7\u53d6\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u591a\u73a9\u5bb6\u548c\u969c\u788d\u7269\u573a\u666f\u3002", "keywords": "\u90e8\u5206\u53ef\u89c2\u5bdf\u968f\u673a\u535a\u5f08\u3001\u4fe1\u606f\u83b7\u53d6\u3001\u8f68\u8ff9\u89c4\u5212\u3001\u5728\u7ebf\u8ba1\u7b97\u3001\u968f\u673a\u68af\u5ea6\u4e0b\u964d"}}
{"id": "2506.01931", "pdf": "https://arxiv.org/pdf/2506.01931", "abs": "https://arxiv.org/abs/2506.01931", "authors": ["Rui-Jie Yew", "Bill Marino", "Suresh Venkatasubramanian"], "title": "Red Teaming AI Policy: A Taxonomy of Avoision and the EU AI Act", "categories": ["cs.CY", "cs.AI"], "comment": "Forthcoming at the 2025 ACM Conference on Fairness, Accountability,\n  and Transparency", "summary": "The shape of AI regulation is beginning to emerge, most prominently through\nthe EU AI Act (the \"AIA\"). By 2027, the AIA will be in full effect, and firms\nare starting to adjust their behavior in light of this new law. In this paper,\nwe present a framework and taxonomy for reasoning about \"avoision\" -- conduct\nthat walks the line between legal avoidance and evasion -- that firms might\nengage in so as to minimize the regulatory burden the AIA poses. We organize\nthese avoision strategies around three \"tiers\" of increasing AIA exposure that\nregulated entities face depending on: whether their activities are (1) within\nscope of the AIA, (2) exempted from provisions of the AIA, or are (3) placed in\na category with higher regulatory scrutiny. In each of these tiers and for each\nstrategy, we specify the organizational and technological forms through which\navoision may manifest. Our goal is to provide an adversarial framework for \"red\nteaming\" the AIA and AI regulation on the horizon.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u548c\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u4f01\u4e1a\u5728\u9762\u5bf9\u6b27\u76dfAI\u6cd5\u6848\uff08AIA\uff09\u65f6\u53ef\u80fd\u91c7\u53d6\u7684\u4ecb\u4e8e\u5408\u6cd5\u907f\u7a0e\u4e0e\u975e\u6cd5\u9003\u7a0e\u4e4b\u95f4\u7684\u884c\u4e3a\uff08\u79f0\u4e3a\u201cavoision\u201d\uff09\u3002", "motivation": "\u7814\u7a76\u4f01\u4e1a\u5982\u4f55\u901a\u8fc7\u201cavoision\u201d\u7b56\u7565\u6700\u5c0f\u5316AIA\u5e26\u6765\u7684\u76d1\u7ba1\u8d1f\u62c5\uff0c\u4e3a\u672a\u6765AI\u76d1\u7ba1\u7684\u201c\u7ea2\u961f\u6d4b\u8bd5\u201d\u63d0\u4f9b\u5bf9\u6297\u6027\u6846\u67b6\u3002", "method": "\u56f4\u7ed5AIA\u7684\u4e09\u5c42\u98ce\u9669\u7b49\u7ea7\uff08\u5728\u8303\u56f4\u5185\u3001\u8c41\u514d\u3001\u9ad8\u76d1\u7ba1\uff09\u5206\u7c7b\u4f01\u4e1a\u884c\u4e3a\uff0c\u5e76\u5206\u6790\u6bcf\u5c42\u7684\u7ec4\u7ec7\u548c\u6280\u672f\u8868\u73b0\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u5957\u7b56\u7565\u548c\u5f62\u5f0f\uff0c\u7528\u4e8e\u9884\u6d4b\u4f01\u4e1a\u5bf9AIA\u7684\u89c4\u907f\u884c\u4e3a\u3002", "conclusion": "\u901a\u8fc7\u6846\u67b6\u548c\u5206\u7c7b\u6cd5\uff0c\u8bba\u6587\u4e3a\u76d1\u7ba1\u673a\u6784\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u9884\u6d4b\u548c\u5e94\u5bf9\u4f01\u4e1a\u89c4\u907f\u7b56\u7565\u7684\u5de5\u5177\u3002", "keywords": "AI\u76d1\u7ba1\uff0c\u6b27\u76dfAI\u6cd5\u6848\uff0cavoision\uff0c\u89c4\u907f\u7b56\u7565\uff0c\u7ea2\u961f\u6d4b\u8bd5"}}
{"id": "2506.01944", "pdf": "https://arxiv.org/pdf/2506.01944", "abs": "https://arxiv.org/abs/2506.01944", "authors": ["Ademi Adeniji", "Zhuoran Chen", "Vincent Liu", "Venkatesh Pattabiraman", "Raunaq Bhirangi", "Siddhant Haldar", "Pieter Abbeel", "Lerrel Pinto"], "title": "Feel the Force: Contact-Driven Learning from Humans", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Controlling fine-grained forces during manipulation remains a core challenge\nin robotics. While robot policies learned from robot-collected data or\nsimulation show promise, they struggle to generalize across the diverse range\nof real-world interactions. Learning directly from humans offers a scalable\nsolution, enabling demonstrators to perform skills in their natural embodiment\nand in everyday environments. However, visual demonstrations alone lack the\ninformation needed to infer precise contact forces. We present FeelTheForce\n(FTF): a robot learning system that models human tactile behavior to learn\nforce-sensitive manipulation. Using a tactile glove to measure contact forces\nand a vision-based model to estimate hand pose, we train a closed-loop policy\nthat continuously predicts the forces needed for manipulation. This policy is\nre-targeted to a Franka Panda robot with tactile gripper sensors using shared\nvisual and action representations. At execution, a PD controller modulates\ngripper closure to track predicted forces-enabling precise, force-aware\ncontrol. Our approach grounds robust low-level force control in scalable human\nsupervision, achieving a 77% success rate across 5 force-sensitive manipulation\ntasks. Code and videos are available at https://feel-the-force-ftf.github.io.", "AI": {"tldr": "\u4e00\u4e2a\u673a\u5668\u4eba\u5b66\u4e60\u7cfb\u7edfFeelTheForce\uff08FTF\uff09\u901a\u8fc7\u5efa\u6a21\u4eba\u7c7b\u89e6\u89c9\u884c\u4e3a\uff0c\u7ed3\u5408\u89e6\u89c9\u624b\u5957\u548c\u89c6\u89c9\u6a21\u578b\uff0c\u5b66\u4e60\u7cbe\u786e\u7684\u529b\u654f\u611f\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u4e2d\u529b\u63a7\u5236\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u5229\u7528\u4eba\u7c7b\u6f14\u793a\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5b66\u4e60\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u89e6\u89c9\u624b\u5957\u6d4b\u91cf\u63a5\u89e6\u529b\uff0c\u7ed3\u5408\u89c6\u89c9\u6a21\u578b\u4f30\u8ba1\u624b\u90e8\u59ff\u6001\uff0c\u8bad\u7ec3\u95ed\u73af\u7b56\u7565\u5b66\u4e60\u529b\u654f\u611f\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7PD\u63a7\u5236\u5668\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u3002", "result": "\u57285\u4e2a\u529b\u654f\u611f\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8fbe\u523077%\u7684\u6210\u529f\u7387\u3002", "conclusion": "FTF\u7cfb\u7edf\u901a\u8fc7\u4eba\u7c7b\u76d1\u7763\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u529b\u63a7\u5236\uff0c\u5c55\u793a\u4e86\u5176\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "\u673a\u5668\u4eba\u5b66\u4e60\uff0c\u89e6\u89c9\u63a7\u5236\uff0c\u529b\u654f\u611f\u64cd\u4f5c\uff0c\u4eba\u7c7b\u6f14\u793a"}}
