<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 85]
- [cs.LG](#cs.LG) [Total: 147]
- [cs.AI](#cs.AI) [Total: 50]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.CY](#cs.CY) [Total: 10]
- [cs.CC](#cs.CC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.SI](#cs.SI) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.CR](#cs.CR) [Total: 16]
- [cs.LO](#cs.LO) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 4]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.RO](#cs.RO) [Total: 21]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 9]
- [cs.DL](#cs.DL) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [math.NA](#math.NA) [Total: 2]
- [cs.DS](#cs.DS) [Total: 3]
- [stat.ML](#stat.ML) [Total: 16]
- [eess.SY](#eess.SY) [Total: 5]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.CV](#cs.CV) [Total: 59]
- [cs.DB](#cs.DB) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 5]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [cs.IR](#cs.IR) [Total: 16]
- [cs.FL](#cs.FL) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.CE](#cs.CE) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Outcome-Based Education: Evaluating Students' Perspectives Using Transformer](https://arxiv.org/abs/2506.17223)
*Shuvra Smaran Das,Anirban Saha Anik,Md Kishor Morol,Mohammad Sakib Mahmood*

Key words: Outcome-Based Education, Transformer Models, DistilBERT, LIME, NLP, Student Feedback

TL;DR: 本研究探讨了基于结果的教育的价值，并通过Transformer模型和LIME解释器分析学生反馈数据，以实现更精准的教育成果评估和改进。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在利用先进的NLP技术（如DistilBERT）提升教育成果的测量和改进，符合OBE的核心目标。

Method: 采用Transformer模型（特别是DistilBERT）分析学生反馈数据，并结合LIME解释器确保模型预测的透明性。

Result: 结合Transformer模型和LIME解释器的框架在情感分类和教育数据分析中表现出色，为OBE提供了数据驱动的改进依据。

Conclusion: 该研究成功开发了一个高效、透明的教育数据分析框架，有助于OBE目标的实现。

Abstract: Outcome-Based Education (OBE) emphasizes the development of specific
competencies through student-centered learning. In this study, we reviewed the
importance of OBE and implemented transformer-based models, particularly
DistilBERT, to analyze an NLP dataset that includes student feedback. Our
objective is to assess and improve educational outcomes. Our approach is better
than other machine learning models because it uses the transformer's deep
understanding of language context to classify sentiment better, giving better
results across a wider range of matrices. Our work directly contributes to
OBE's goal of achieving measurable outcomes by facilitating the identification
of patterns in student learning experiences. We have also applied LIME (local
interpretable model-agnostic explanations) to make sure that model predictions
are clear. This gives us understandable information about how key terms affect
sentiment. Our findings indicate that the combination of transformer models and
LIME explanations results in a strong and straightforward framework for
analyzing student feedback. This aligns more closely with the principles of OBE
and ensures the improvement of educational practices through data-driven
insights.

</details>


### [2] [Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs](https://arxiv.org/abs/2506.17231)
*Xiang Li,Chong Zhang,Jia Wang,Fangyu Wu,Yushi Li,Xiaobo Jin*

Key words: 大语言模型, 越狱攻击, 对抗性提示蒸馏, 安全性, 强化学习

TL;DR: 论文提出了一种对抗性提示蒸馏方法，通过结合掩码语言建模、强化学习和动态温度控制，使小型语言模型能对主流大语言模型进行越狱攻击，实验验证了其高效性和跨模型适应性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前越狱攻击方法效率低、计算成本高且适应性差，难以应对大语言模型的快速发展和新防御策略。

Method: 结合掩码语言建模、强化学习和动态温度控制的提示生成与蒸馏方法。

Result: 实验验证了该方法在攻击成功率和危害性上的优越性，体现了资源效率和跨模型适应性。

Conclusion: 研究探索了将大语言模型的越狱能力蒸馏到小型语言模型中的可行性，揭示了模型的脆弱性，为大语言模型安全研究提供了新思路。

Abstract: Attacks on large language models (LLMs) in jailbreaking scenarios raise many
security and ethical issues. Current jailbreak attack methods face problems
such as low efficiency, high computational cost, and poor cross-model
adaptability and versatility, which make it difficult to cope with the rapid
development of LLM and new defense strategies. Our work proposes an Adversarial
Prompt Distillation, which combines masked language modeling, reinforcement
learning, and dynamic temperature control through a prompt generation and
distillation method. It enables small language models (SLMs) to jailbreak
attacks on mainstream LLMs. The experimental results verify the superiority of
the proposed method in terms of attack success rate and harm, and reflect the
resource efficiency and cross-model adaptability. This research explores the
feasibility of distilling the jailbreak ability of LLM to SLM, reveals the
model's vulnerability, and provides a new idea for LLM security research.

</details>


### [3] [GTA: Grouped-head latenT Attention](https://arxiv.org/abs/2506.17286)
*Luoyang Sun,Jiwen Jiang,Cheng Deng,Xinjian Wu,Haifeng Zhang,Lei Chen,Lionel Ni,Jun Wang*

Key words: 注意力机制，大语言模型，内存优化，计算效率，KV缓存

TL;DR: 提出了一种新型注意力机制GTA，通过共享注意力图和压缩值缓存，显著减少内存和计算开销，同时保持性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大语言模型的注意力机制存在计算和内存开销大的问题，限制了其在资源有限硬件上的部署效率。

Method: GTA通过共享注意力图和压缩值缓存到潜在空间，减少KV缓存和计算复杂度。

Result: GTA减少了62.5%的计算FLOPs和70%的KV缓存，推理速度提升2倍。

Conclusion: GTA是一种高效且性能保持的注意力机制，适用于资源受限的部署场景。

Abstract: Attention mechanisms underpin the success of large language models (LLMs),
yet their substantial computational and memory overhead poses challenges for
optimizing efficiency and performance. A critical bottleneck arises as KV cache
and attention computations scale rapidly with text length, challenging
deployment on hardware with limited computational and memory resources. We
observe that attention mechanisms exhibit substantial redundancy, since the KV
cache can be significantly compressed and attention maps across heads display
high similarity, revealing that much of the computation and storage is
unnecessary. Leveraging these insights, we propose \textbf{G}rouped-Head
Laten\textbf{T} \textbf{A}ttention (GTA), a novel attention mechanism that
reduces memory usage and computational complexity while maintaining
performance. GTA comprises two components: (1) a shared attention map mechanism
that reuses attention scores across multiple heads, decreasing the key cache
size; and (2) a nonlinear value decoder with learned projections that
compresses the value cache into a latent space, further cutting memory needs.
GTA cuts attention computation FLOPs by up to \emph{62.5\%} versus
Grouped-Query Attention and shrink the KV cache by up to \emph{70\%}, all while
avoiding the extra overhead of Multi-Head Latent Attention to improve LLM
deployment efficiency. Consequently, GTA models achieve a \emph{2x} increase in
end-to-end inference speed, with prefill benefiting from reduced computational
cost and decoding benefiting from the smaller cache footprint.

</details>


### [4] [AI-Generated Game Commentary: A Survey and a Datasheet Repository](https://arxiv.org/abs/2506.17294)
*Qirui Zheng,Xingbo Wang,Keyuan Cheng,Yunlong Lu,Wenxin Li*

Key words: AI生成游戏解说, 多模态NLP, 数据集调查, 评估指标

TL;DR: 本文介绍了AI生成游戏解说（AIGGC）的通用框架，并全面调查了45个现有游戏解说数据集和方法，总结了其关键技术挑战和评估指标。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: AIGGC因其市场潜力和技术挑战受到广泛关注，但缺乏系统性研究。

Method: 提出通用框架，分类调查现有数据集和方法，并总结评估指标。

Result: 整理了45个数据集和方法，提供了结构化数据表以支持未来研究。

Conclusion: AIGGC研究前景广阔，系统性分类和评估指标为未来工作提供基础。

Abstract: AI-Generated Game Commentary (AIGGC) has gained increasing attention due to
its market potential and inherent technical challenges. As a comprehensive
multimodal Natural Language Processing (NLP) task, AIGGC imposes substantial
demands on language models, including factual accuracy, logical reasoning,
expressive text generation, generation speed, and context management. In this
paper, we introduce a general framework for AIGGC and present a comprehensive
survey of 45 existing game commentary dataset and methods according to key
challenges they aim to address in this domain. We further classify and compare
various evaluation metrics commonly used in this domain. To support future
research and benchmarking, we also provide a structured datasheet summarizing
the essential attributes of these datasets in appendix, which is meanwhile
publicly available in an open repository.

</details>


### [5] [Semantic uncertainty in advanced decoding methods for LLM generation](https://arxiv.org/abs/2506.17296)
*Darius Foodeei,Simin Fan,Martin Jaggi*

Key words: 大型语言模型, 链式思维解码, 推测采样, 语义不确定性

TL;DR: 研究探讨了不同解码方法对大型语言模型（LLM）输出的语义不确定性的影响，发现链式思维（CoT）解码在代码生成中显著提升了准确率，而推测采样在摘要任务中表现最佳。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机在于了解不同解码策略如何影响语言模型输出的多样性和可靠性，以优化实际应用中的表现。

Method: 通过实验比较了链式思维（CoT）解码和推测采样在问答、摘要和代码生成任务中的表现。

Result: CoT解码在代码生成中Pass@2提升48.8%，推测采样在摘要任务中ROUGE得分更高。

Conclusion: 结构化解码方法可以同时提升语义探索和输出质量，挑战了多样性-准确性的传统权衡观点。

Abstract: This study investigates semantic uncertainty in large language model (LLM)
outputs across different decoding methods, focusing on emerging techniques like
speculative sampling and chain-of-thought (CoT) decoding. Through experiments
on question answering, summarization, and code generation tasks, we analyze how
different decoding strategies affect both the diversity and reliability of
model outputs. Our findings reveal that while CoT decoding demonstrates higher
semantic diversity, it maintains lower predictive entropy, suggesting that
structured exploration can lead to more confident and accurate outputs. This is
evidenced by a 48.8% improvement in code generation Pass@2 rates, despite lower
alignment with reference solutions. For summarization tasks, speculative
sampling proved particularly effective, achieving superior ROUGE scores while
maintaining moderate semantic diversity. Our results challenge conventional
assumptions about trade-offs between diversity and accuracy in language model
outputs, demonstrating that properly structured decoding methods can increase
semantic exploration while maintaining or improving output quality. These
findings have significant implications for deploying language models in
practical applications where both reliability and diverse solution generation
are crucial.

</details>


### [6] [Mercury: Ultra-Fast Language Models Based on Diffusion](https://arxiv.org/abs/2506.17298)
*Inception Labs,Samar Khanna,Siddhant Kharbanda,Shufan Li,Harshit Varma,Eric Wang,Sawyer Birnbaum,Ziyang Luo,Yanis Miraoui,Akash Palrecha,Stefano Ermon,Aditya Grover,Volodymyr Kuleshov*

Key words: Mercury, 扩散模型, 大语言模型, 编程应用, 并行预测, Transformer

TL;DR: Mercury 是一代新型商用大语言模型（LLM），基于扩散模型和 Transformer 架构，专注于并行预测多标记。Mercury Coder 是其首款专为编程应用设计的系列，拥有 Mini 和 Small 两种规模，在速度和质量方面取得突破。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了解决当前语言模型在高质量编程任务中的效率问题，Mercury 通过扩散技术和并行预测实现了显著的性能提升。

Method: 采用 Transformer 架构，基于扩散模型训练并行预测多标记，并设计 Mercury Coder 系列用于编程任务。

Result: Mercury Coder Mini 和 Small 在 NVIDIA H100 GPU 上分别达到 1109 和 737 标记/秒的吞吐量，比前沿模型快 10 倍且质量相当。在 Copilot Arena 中排名第二且速度最快。

Conclusion: Mercury 通过技术创新在编程任务中实现了速度与质量的平衡，未来有望进一步扩展应用场景。

Abstract: We present Mercury, a new generation of commercial-scale large language
models (LLMs) based on diffusion. These models are parameterized via the
Transformer architecture and trained to predict multiple tokens in parallel. In
this report, we detail Mercury Coder, our first set of diffusion LLMs designed
for coding applications. Currently, Mercury Coder comes in two sizes: Mini and
Small. These models set a new state-of-the-art on the speed-quality frontier.
Based on independent evaluations conducted by Artificial Analysis, Mercury
Coder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109
tokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform
speed-optimized frontier models by up to 10x on average while maintaining
comparable quality. We discuss additional results on a variety of code
benchmarks spanning multiple languages and use-cases as well as real-world
validation by developers on Copilot Arena, where the model currently ranks
second on quality and is the fastest model overall. We also release a public
API at https://platform.inceptionlabs.ai/ and free playground at
https://chat.inceptionlabs.ai

</details>


### [7] [PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights](https://arxiv.org/abs/2506.17314)
*Adnan Qidwai,Srija Mukhopadhyay,Prerana Khatiwada,Dan Roth,Vivek Gupta*

Key words: 电子商务, 产品描述, 大语言模型, PRAISE, 评论分析

TL;DR: PRAISE是一种利用大语言模型从客户评论和卖家描述中提取、比较和结构化信息的系统，帮助改善电子商务产品描述的准确性和完整性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 卖家提供的产品描述通常不完整，而客户评论包含有价值的信息但难以手动筛选。PRAISE旨在解决这一问题。

Method: PRAISE利用大型语言模型（LLMs）自动分析客户评论和卖家描述，提取并结构化关键信息。

Result: PRAISE能够高效地识别卖家描述与评论之间的差异，并以直观格式展示这些差异，帮助卖家改进描述并帮助买家评估产品可靠性。

Conclusion: PRAISE在提升电子商务产品目录质量和信任度方面具有显著潜力。

Abstract: Accurate and complete product descriptions are crucial for e-commerce, yet
seller-provided information often falls short. Customer reviews offer valuable
details but are laborious to sift through manually. We present PRAISE: Product
Review Attribute Insight Structuring Engine, a novel system that uses Large
Language Models (LLMs) to automatically extract, compare, and structure
insights from customer reviews and seller descriptions. PRAISE provides users
with an intuitive interface to identify missing, contradictory, or partially
matching details between these two sources, presenting the discrepancies in a
clear, structured format alongside supporting evidence from reviews. This
allows sellers to easily enhance their product listings for clarity and
persuasiveness, and buyers to better assess product reliability. Our
demonstration showcases PRAISE's workflow, its effectiveness in generating
actionable structured insights from unstructured reviews, and its potential to
significantly improve the quality and trustworthiness of e-commerce product
catalogs.

</details>


### [8] [Towards Safety Evaluations of Theory of Mind in Large Language Models](https://arxiv.org/abs/2506.17352)
*Tatsuhiro Aoshima,Mitsuaki Akiyama*

Key words: 大型语言模型, 欺骗行为, 安全评估, 理论心智

TL;DR: 该论文探讨了大型语言模型（LLMs）在安全评估中的潜在风险，尤其是其可能表现出的欺骗行为。作者提出通过测量LLMs的理论心智（theory of mind）能力来评估这些行为，并分析了相关研究和LLMs的发展趋势。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着LLMs能力的提升，其安全评估的重要性日益凸显，尤其是模型可能表现出欺骗行为或规避监管机制的行为。

Method: 作者通过回顾理论心智的相关研究，并分析一系列开源LLMs的发展趋势，评估其理论心智能力与阅读理解的对比。

Result: 研究发现，尽管LLMs的阅读理解能力有所提升，但理论心智能力并未同步发展。

Conclusion: 论文总结了当前LLMs在理论心智能力方面的安全评估现状，并指出了未来研究的挑战。

Abstract: As the capabilities of large language models (LLMs) continue to advance, the
importance of rigorous safety evaluation is becoming increasingly evident.
Recent concerns within the realm of safety assessment have highlighted
instances in which LLMs exhibit behaviors that appear to disable oversight
mechanisms and respond in a deceptive manner. For example, there have been
reports suggesting that, when confronted with information unfavorable to their
own persistence during task execution, LLMs may act covertly and even provide
false answers to questions intended to verify their behavior.To evaluate the
potential risk of such deceptive actions toward developers or users, it is
essential to investigate whether these behaviors stem from covert, intentional
processes within the model. In this study, we propose that it is necessary to
measure the theory of mind capabilities of LLMs. We begin by reviewing existing
research on theory of mind and identifying the perspectives and tasks relevant
to its application in safety evaluation. Given that theory of mind has been
predominantly studied within the context of developmental psychology, we
analyze developmental trends across a series of open-weight LLMs. Our results
indicate that while LLMs have improved in reading comprehension, their theory
of mind capabilities have not shown comparable development. Finally, we present
the current state of safety evaluation with respect to LLMs' theory of mind,
and discuss remaining challenges for future work.

</details>


### [9] [Cash or Comfort? How LLMs Value Your Inconvenience](https://arxiv.org/abs/2506.17367)
*Mateusz Cedro,Timour Ichmoukhamedov,Sofie Goethals,Yifan He,James Hinns,David Martens*

Key words: 大型语言模型、决策行为、金钱与舒适度权衡、用户不适

TL;DR: 该论文研究了大型语言模型（LLMs）在个人决策中的行为，特别是在金钱奖励与用户舒适度冲突时的表现，揭示了LLMs在此类场景中的不稳定性和不合理性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着LLMs被提议作为近乎自主的AI代理，其在个人决策中的行为尚未被充分理解。本研究旨在探索LLMs在面对金钱与舒适度权衡时的表现。

Method: 通过量化多个LLMs对用户不适（如额外步行、等待、饥饿和疼痛）赋予的价格，分析其决策行为。

Result: 发现LLMs存在响应方差大、对提示措辞敏感、接受不合理低奖励等问题，表明其在决策中的不稳定性和不合理性。

Conclusion: 当前LLMs作为决策辅助工具的前景存在严重问题，亟需对其在权衡金钱与舒适度时的行为进行更严格的审查。

Abstract: Large Language Models (LLMs) are increasingly proposed as near-autonomous
artificial intelligence (AI) agents capable of making everyday decisions on
behalf of humans. Although LLMs perform well on many technical tasks, their
behaviour in personal decision-making remains less understood. Previous studies
have assessed their rationality and moral alignment with human decisions.
However, the behaviour of AI assistants in scenarios where financial rewards
are at odds with user comfort has not yet been thoroughly explored. In this
paper, we tackle this problem by quantifying the prices assigned by multiple
LLMs to a series of user discomforts: additional walking, waiting, hunger and
pain. We uncover several key concerns that strongly question the prospect of
using current LLMs as decision-making assistants: (1) a large variance in
responses between LLMs, (2) within a single LLM, responses show fragility to
minor variations in prompt phrasing (e.g., reformulating the question in the
first person can considerably alter the decision), (3) LLMs can accept
unreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10
hours), and (4) LLMs can reject monetary gains where no discomfort is imposed
(e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for
scrutiny of how LLMs value human inconvenience, particularly as we move toward
applications where such cash-versus-comfort trade-offs are made on users'
behalf.

</details>


### [10] [Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study](https://arxiv.org/abs/2506.17410)
*Danielle R. Thomas,Conrad Borchers,Jionghao Lin,Sanjit Kakarla,Shambhavi Bhushan,Erin Gatz,Shivang Gupta,Ralph Abboud,Kenneth R. Koedinger*

Key words: 辅导行为分析、生成式AI、数学教育、大规模评估、LLM提示

TL;DR: 研究探讨了使用生成式AI分析数学辅导对话的可行性和扩展性，评估辅导员的表扬和错误回应行为，模型表现接近人类判断。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有研究缺乏对辅导行为的大规模分析，本文利用生成式AI填补这一空白。

Method: 随机选取50份数学辅导对话，使用多种生成式AI模型（如GPT-4、Gemini-1.5-pro）分析辅导员的表扬和错误回应行为。

Result: 模型在检测相关情境（如表扬和错误）时准确率较高（82-98%），且与人类判断一致（73-89%）。

Conclusion: 生成式AI可用于大规模辅导行为评估，提出了低成本提示策略，并分享了LLM提示以支持研究复现。

Abstract: Tutoring improves student achievement, but identifying and studying what
tutoring actions are most associated with student learning at scale based on
audio transcriptions is an open research problem. This present study
investigates the feasibility and scalability of using generative AI to identify
and evaluate specific tutor moves in real-life math tutoring. We analyze 50
randomly selected transcripts of college-student remote tutors assisting middle
school students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo,
Gemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills:
delivering effective praise and responding to student math errors. All models
reliably detected relevant situations, for example, tutors providing praise to
students (94-98% accuracy) and a student making a math error (82-88% accuracy)
and effectively evaluated the tutors' adherence to tutoring best practices,
aligning closely with human judgments (83-89% and 73-77%, respectively). We
propose a cost-effective prompting strategy and discuss practical implications
for using large language models to support scalable assessment in authentic
settings. This work further contributes LLM prompts to support reproducibility
and research in AI-supported learning.

</details>


### [11] [UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making](https://arxiv.org/abs/2506.17419)
*Jinhao Duan,James Diffenderfer,Sandeep Madireddy,Tianlong Chen,Bhavya Kailkhura,Kaidi Xu*

Key words: LLM, 不确定性量化, 多步决策, 信息论, UProp

TL;DR: 论文提出了一个信息论框架，将LLM的顺序决策不确定性分解为内部和外部不确定性，并提出了高效的UProp方法进行估计，在多项基准测试中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: LLM在安全关键应用中的顺序决策需要信任机制，现有不确定性量化方法主要针对单轮问答，多步决策场景研究不足。

Method: 提出信息论框架分解不确定性（内部和外部），设计UProp方法通过PMI估计外部不确定性，并在多步决策场景中验证。

Result: UProp在多步决策基准测试（如AgentBench、HotpotQA）中显著优于现有单轮UQ方法。

Conclusion: UProp有效解决了LLM多步决策中的不确定性量化问题，具有高效性和广泛应用潜力。

Abstract: As Large Language Models (LLMs) are integrated into safety-critical
applications involving sequential decision-making in the real world, it is
essential to know when to trust LLM decisions. Existing LLM Uncertainty
Quantification (UQ) methods are primarily designed for single-turn
question-answering formats, resulting in multi-step decision-making scenarios,
e.g., LLM agentic system, being underexplored. In this paper, we introduce a
principled, information-theoretic framework that decomposes LLM sequential
decision uncertainty into two parts: (i) internal uncertainty intrinsic to the
current decision, which is focused on existing UQ methods, and (ii) extrinsic
uncertainty, a Mutual-Information (MI) quantity describing how much uncertainty
should be inherited from preceding decisions. We then propose UProp, an
efficient and effective extrinsic uncertainty estimator that converts the
direct estimation of MI to the estimation of Pointwise Mutual Information (PMI)
over multiple Trajectory-Dependent Decision Processes (TDPs). UProp is
evaluated over extensive multi-step decision-making benchmarks, e.g.,
AgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and
DeepSeek-V3. Experimental results demonstrate that UProp significantly
outperforms existing single-turn UQ baselines equipped with thoughtful
aggregation strategies. Moreover, we provide a comprehensive analysis of UProp,
including sampling efficiency, potential applications, and intermediate
uncertainty propagation, to demonstrate its effectiveness. Codes will be
available at https://github.com/jinhaoduan/UProp.

</details>


### [12] [Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media](https://arxiv.org/abs/2506.17435)
*Alberto Martinez-Serra,Alejandro De La Fuente,Nienke Viescher,Ana S. Cardenal*

Key words: 大型语言模型,政治内容分类,URL分析,跨语言研究,政治科学

TL;DR: 本文评估了大型语言模型（LLMs）通过URL准确分类政治内容（PC）的能力，研究发现URL能够嵌入大部分新闻内容，为准确性与成本平衡提供了重要视角。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管LLMs在标注任务中表现优异，但其仅通过URL分类政治内容的效果尚未被充分探索。本研究填补了这一空白。

Method: 使用GPT、Llama等先进LLMs，通过比较URL和全文分析政治内容的能力，并与人工标注和传统机器学习方法对比。

Result: 研究表明URL能够很好地近似全文分析，尤其是在跨语言和国家背景的情况下。

Conclusion: URL在政治内容分类中具有潜力，同时提出了在政治科学研究中使用LLMs的方法学建议。

Abstract: The use of large language models (LLMs) is becoming common in the context of
political science, particularly in studies that analyse individuals use of
digital media. However, while previous research has demonstrated LLMs ability
at labelling tasks, the effectiveness of using LLMs to classify political
content (PC) from just URLs is not yet well explored. The work presented in
this article bridges this gap by evaluating whether LLMs can accurately
identify PC vs. non-PC from both the article text and the URLs from five
countries (France, Germany, Spain, the UK, and the US) and different languages.
Using cutting-edge LLMs like GPT, Llama, Mistral, Deepseek, Qwen and Gemma, we
measure model performance to assess whether URL-level analysis can be a good
approximation for full-text analysis of PC, even across different linguistic
and national contexts. Model outputs are compared with human-labelled articles,
as well as traditional supervised machine learning techniques, to set a
baseline of performance. Overall, our findings suggest the capacity of URLs to
embed most of the news content, providing a vital perspective on accuracy-cost
balancing. We also account for contextual limitations and suggest
methodological recommendations to use LLMs within political science studies.

</details>


### [13] [Breaking the Transcription Bottleneck: Fine-tuning ASR Models for Extremely Low-Resource Fieldwork Languages](https://arxiv.org/abs/2506.17459)
*Siyu Liang,Gina-Anne Levow*

Key words: 自动语音识别,低资源语言,田野语言学,MMS,XLS-R

TL;DR: 本文探讨了在多语言自动语音识别（ASR）模型MMS和XLS-R在五种低资源语言上的表现，提供了适用于语言学田野工作的实用指南。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决在田野调查中由于自发语音、环境噪声和数据稀缺导致ASR效果不佳的问题。

Method: 通过控制训练数据时长，对MMS和XLS-R模型在五种低资源语言上进行基准测试。

Result: MMS在极少量训练数据时表现最佳，而XLS-R在训练数据超过一小时时表现相当。

Conclusion: 为语言学家提供了可复现的ASR适配方法，以缓解语言文档转录的瓶颈。

Abstract: Automatic Speech Recognition (ASR) has reached impressive accuracy for
high-resource languages, yet its utility in linguistic fieldwork remains
limited. Recordings collected in fieldwork contexts present unique challenges,
including spontaneous speech, environmental noise, and severely constrained
datasets from under-documented languages. In this paper, we benchmark the
performance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five
typologically diverse low-resource languages with control of training data
duration. Our findings show that MMS is best suited when extremely small
amounts of training data are available, whereas XLS-R shows parity performance
once training data exceed one hour. We provide linguistically grounded analysis
for further provide insights towards practical guidelines for field linguists,
highlighting reproducible ASR adaptation approaches to mitigate the
transcription bottleneck in language documentation.

</details>


### [14] [Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems](https://arxiv.org/abs/2506.17467)
*Weixin Liang*

Key words: 大语言模型, AI公平性, AI检测, 科研反馈, 算法分析

TL;DR: 该论文探讨了大语言模型（LLMs）在社会中的快速应用及其影响，包括机构采用AI检测器带来的偏见问题、LLMs在各领域的广泛使用，以及LLMs在科研反馈中的潜力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究LLMs如何改变写作和交流方式，并考察社会和机构如何应对这一新兴技术及其带来的公平性问题。

Method: 通过三个研究方向：分析AI检测器的偏见、开发算法测量LLM的采用情况、实证研究LLM在科研反馈中的应用。

Result: 揭示了AI检测器对非主流语言使用者的偏见、LLM在各领域的广泛应用模式，以及LLM在科研反馈中的实用性。

Conclusion: LLMs的应用带来了深刻的变革，但也需关注公平性和支持弱势群体，未来需加强AI治理。

Abstract: Large language models (LLMs) have shown significant potential to change how
we write, communicate, and create, leading to rapid adoption across society.
This dissertation examines how individuals and institutions are adapting to and
engaging with this emerging technology through three research directions.
First, I demonstrate how the institutional adoption of AI detectors introduces
systematic biases, particularly disadvantaging writers of non-dominant language
varieties, highlighting critical equity concerns in AI governance. Second, I
present novel population-level algorithmic approaches that measure the
increasing adoption of LLMs across writing domains, revealing consistent
patterns of AI-assisted content in academic peer reviews, scientific
publications, consumer complaints, corporate communications, job postings, and
international organization press releases. Finally, I investigate LLMs'
capability to provide feedback on research manuscripts through a large-scale
empirical analysis, offering insights into their potential to support
researchers who face barriers in accessing timely manuscript feedback,
particularly early-career researchers and those from under-resourced settings.

</details>


### [15] [VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM](https://arxiv.org/abs/2506.17506)
*Lesheng Jin,Zhenyuan Ruan,Haohui Mai,Jingbo Shang*

Key words: GPU编译器、寄存器分配、大语言模型、形式验证、性能优化

TL;DR: VeriLocc框架结合大语言模型（LLM）与形式编译器技术，实现跨GPU架构的可验证通用寄存器分配，性能优于手动调优库。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现代GPU快速发展，但生产编译器仍依赖手工调整的寄存器分配启发式方法，需对不同硬件进行重新调整。

Method: VeriLocc微调LLM以将中间表示（MIR）转换为目标专用寄存器分配，辅以静态分析和验证器引导的再生循环确保正确性。

Result: 在矩阵乘法（GEMM）和多头注意力（MHA）任务中，VeriLocc单次准确率达85-99%，pass@100接近100%，并通过案例研究发现其分配优于rocBLAS库10%以上。

Conclusion: VeriLocc提供了一种高效且可验证的寄存器分配方法，显著优于传统手工调优技术。

Abstract: Modern GPUs evolve rapidly, yet production compilers still rely on
hand-crafted register allocation heuristics that require substantial re-tuning
for each hardware generation. We introduce VeriLocc, a framework that combines
large language models (LLMs) with formal compiler techniques to enable
generalizable and verifiable register allocation across GPU architectures.
VeriLocc fine-tunes an LLM to translate intermediate representations (MIRs)
into target-specific register assignments, aided by static analysis for
cross-architecture normalization and generalization and a verifier-guided
regeneration loop to ensure correctness. Evaluated on matrix multiplication
(GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot
accuracy and near-100% pass@100. Case study shows that VeriLocc discovers more
performant assignments than expert-tuned libraries, outperforming rocBLAS by
over 10% in runtime.

</details>


### [16] [Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning](https://arxiv.org/abs/2506.17525)
*Mingfei Lau,Qian Chen,Yeming Fang,Tingting Xu,Tongzhou Chen,Pavel Golik*

Key words: 多语言语音数据集, 质量控制, 台湾闽南语, 语言规划, 社会语言学

TL;DR: 论文对三个多语言语音数据集的质量问题进行了审计，发现某些语言存在显著问题，特别是资源不足的语言。提出了改进建议，强调了社会语言学意识在数据集开发中的重要性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是揭示广泛使用的公共多语言语音数据集（如Mozilla Common Voice、FLEURS和VoxPopuli）中的质量问题，以提升其训练和评估用途。

Method: 方法包括对数据集的微观和宏观层面质量问题进行分类，并以台湾闽南语为例进行案例分析。

Result: 发现宏观问题在资源不足的语言中更普遍，并通过案例分析强调了语言规划和数据质量控制的重要性。

Conclusion: 结论提出了未来数据集开发的改进指南，强调了社会语言学意识对构建可靠语音数据资源的作用。

Abstract: Our quality audit for three widely used public multilingual speech datasets -
Mozilla Common Voice 17.0, FLEURS, and VoxPopuli - shows that in some
languages, these datasets suffer from significant quality issues. We believe
addressing these issues will make these datasets more useful as training and
evaluation sets, and improve downstream models. We divide these quality issues
into two categories: micro-level and macro-level. We find that macro-level
issues are more prevalent in less institutionalized, often under-resourced
languages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that
highlights the need for proactive language planning (e.g. orthography
prescriptions, dialect boundary definition) and enhanced data quality control
in the process of Automatic Speech Recognition (ASR) dataset creation. We
conclude by proposing guidelines and recommendations to mitigate these issues
in future dataset development, emphasizing the importance of sociolinguistic
awareness in creating robust and reliable speech data resources.

</details>


### [17] [DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning](https://arxiv.org/abs/2506.17533)
*Yuanhao Wu,Juntong Song,Hanning Zhang,Tong Zhang,Cheng Niu*

Key words: DuaShepherd, 奖励建模, 数学推理, 大语言模型, 多任务学习

TL;DR: DuaShepherd是一种新颖的奖励建模框架，通过结合正确性和潜力两种互补的奖励信号，提升大语言模型的数学推理能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 通过整合不同奖励信号（正确性和潜力），以更全面地评估和优化大语言模型的数学推理能力。

Method: 开发自动化流水线构建大规模奖励建模数据集，采用多头架构并行训练两种奖励模型。

Result: 实验证明，结合两种奖励信号的模型在多个基准测试中表现优于单一奖励模型，在可比资源限制下达到最优性能。

Conclusion: DuaShepherd框架有效整合了正确性和潜力奖励信号，显著提升了模型的数学推理性能。

Abstract: In this paper, we propose DuaShepherd, a novel reward modeling framework that
integrates two complementary reward signals, correctness and potential, to
enhance the mathematical reasoning capabilities of Large Language Models
(LLMs). While correctness-based signals emphasize identification of stepwise
errors, potential-based signals focus on the likelihood of reaching the correct
final answer. We developed an automated pipeline for constructing large-scale
reward modeling dataset with both signals. A unified, multi-head architecture
was explored to train the two reward models in a multi-task setup,
demonstrating benefits from learning both correctness and potential in
parallel. By combining these two signals into a compound probability, our model
achieves consistent performance improvements across multiple benchmarks.
Empirical evaluations on MATH500 and ProcessBench confirm that this combined
reward significantly outperforms models trained on either reward type alone,
achieving state-of-the-art performance under comparable resource constraints.

</details>


### [18] [Probing for Phonology in Self-Supervised Speech Representations: A Case Study on Accent Perception](https://arxiv.org/abs/2506.17542)
*Nitin Venkateswaran,Kevin Tang,Ratree Wayland*

Key words: 口音感知, 自监督学习, 音位特征, Wav2Vec2-BERT, WavLM

TL;DR: 研究探讨了自监督学习模型如何编码影响口音感知的音位特征，发现特定音段的预训练表示能有效预测口音强度。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统口音感知模型低估了音位特征的梯度变化对口音判断的影响，研究旨在填补这一空白。

Method: 使用CSLU外国口音英语语料库，提取音位特征概率，并利用Wav2Vec2-BERT和WavLM的预训练表示进行分析。

Result: 结果表明，音段的预训练表示特征子集能最好地预测口音强度，与感知显著的音位特征相关。

Conclusion: 自监督语音表示在通过可解释音位特征建模口音感知方面具有重要价值。

Abstract: Traditional models of accent perception underestimate the role of gradient
variations in phonological features which listeners rely upon for their accent
judgments. We investigate how pretrained representations from current
self-supervised learning (SSL) models of speech encode phonological
feature-level variations that influence the perception of segmental accent. We
focus on three segments: the labiodental approximant, the rhotic tap, and the
retroflex stop, which are uniformly produced in the English of native speakers
of Hindi as well as other languages in the Indian sub-continent. We use the
CSLU Foreign Accented English corpus (Lander, 2007) to extract, for these
segments, phonological feature probabilities using Phonet (V\'asquez-Correa et
al., 2019) and pretrained representations from Wav2Vec2-BERT (Barrault et al.,
2023) and WavLM (Chen et al., 2022) along with accent judgements by native
speakers of American English. Probing analyses show that accent strength is
best predicted by a subset of the segment's pretrained representation features,
in which perceptually salient phonological features that contrast the expected
American English and realized non-native English segments are given prominent
weighting. A multinomial logistic regression of pretrained representation-based
segment distances from American and Indian English baselines on accent ratings
reveals strong associations between the odds of accent strength and distances
from the baselines, in the expected directions. These results highlight the
value of self-supervised speech representations for modeling accent perception
using interpretable phonological features.

</details>


### [19] [AgriCHN: A Comprehensive Cross-domain Resource for Chinese Agricultural Named Entity Recognition](https://arxiv.org/abs/2506.17578)
*Lingxiao Zeng,Yiqi Tong,Wei Guo,Huarui Wu,Lihao Ge,Yijun Ye,Fuzhen Zhuang,Deqing Wang,Wei Guo,Cheng Chen*

Key words: 农业命名实体识别、中文数据集、农业实体、水文、气象

TL;DR: 论文提出了一种名为AgriCHN的中文农业命名实体识别数据集，填补了农业与水文、气象领域关联的数据空白，提升了实体识别准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决农业命名实体识别任务中高质量数据稀缺的问题，尤其是中文数据，并提出农业与水文、气象的关联。

Method: 构建AgriCHN数据集，包含4,040个句子和15,799个农业实体，涵盖27个类别，并进行了数据验证和基准测试。

Result: AgriCHN数据集质量高，实体类型更丰富，细粒度更细，实验结果显示其对NER模型具有挑战性和研究潜力。

Conclusion: AgriCHN是开源中文资源，有助于提升农业实体识别的准确性，并为进一步研究提供基础。

Abstract: Agricultural named entity recognition is a specialized task focusing on
identifying distinct agricultural entities within vast bodies of text,
including crops, diseases, pests, and fertilizers. It plays a crucial role in
enhancing information extraction from extensive agricultural text resources.
However, the scarcity of high-quality agricultural datasets, particularly in
Chinese, has resulted in suboptimal performance when employing mainstream
methods for this purpose. Most earlier works only focus on annotating
agricultural entities while overlook the profound correlation of agriculture
with hydrology and meteorology. To fill this blank, we present AgriCHN, a
comprehensive open-source Chinese resource designed to promote the accuracy of
automated agricultural entity annotation. The AgriCHN dataset has been
meticulously curated from a wealth of agricultural articles, comprising a total
of 4,040 sentences and encapsulating 15,799 agricultural entity mentions
spanning 27 diverse entity categories. Furthermore, it encompasses entities
from hydrology to meteorology, thereby enriching the diversity of entities
considered. Data validation reveals that, compared with relevant resources,
AgriCHN demonstrates outstanding data quality, attributable to its richer
agricultural entity types and more fine-grained entity divisions. A benchmark
task has also been constructed using several state-of-the-art neural NER
models. Extensive experimental results highlight the significant challenge
posed by AgriCHN and its potential for further research.

</details>


### [20] [Mind the Gap: Assessing Wiktionary's Crowd-Sourced Linguistic Knowledge on Morphological Gaps in Two Related Languages](https://arxiv.org/abs/2506.17603)
*Jonathan Sakunkoo,Annabella Sakunkoo*

Key words: 形态学缺陷、神经形态分析器、维基词典、拉丁语、意大利语、众包数据

TL;DR: 论文研究了形态学缺陷现象，利用神经形态分析器验证了维基词典中拉丁语和意大利语的形态缺陷动词列表，发现维基词典在意大利语中表现可靠，但拉丁语中有7%的词条可能不准确。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 形态学缺陷现象在语言学中是一个有趣但研究不足的问题，对提高NLP工具在形态丰富语言中的准确性至关重要。尽管维基词典是稀缺语言现象的重要资源，但其可靠性存在争议。

Method: 研究定制了一个神经形态分析器，用于标注拉丁语和意大利语语料库，并利用大规模标注数据计算验证了维基词典中列出的缺陷动词。

Result: 结果显示，维基词典对意大利语的形态缺陷描述高度可靠，但拉丁语中有7%的词条在语料库中表现出非缺陷特征。

Conclusion: 研究揭示了众包维基作为语言知识来源的潜在局限性，尤其是在较少研究的语言和现象中，但也为形态学和稀缺语言现象的研究提供了工具和方法。

Abstract: Morphological defectivity is an intriguing and understudied phenomenon in
linguistics. Addressing defectivity, where expected inflectional forms are
absent, is essential for improving the accuracy of NLP tools in morphologically
rich languages. However, traditional linguistic resources often lack coverage
of morphological gaps as such knowledge requires significant human expertise
and effort to document and verify. For scarce linguistic phenomena in
under-explored languages, Wikipedia and Wiktionary often serve as among the few
accessible resources. Despite their extensive reach, their reliability has been
a subject of controversy. This study customizes a novel neural morphological
analyzer to annotate Latin and Italian corpora. Using the massive annotated
data, crowd-sourced lists of defective verbs compiled from Wiktionary are
validated computationally. Our results indicate that while Wiktionary provides
a highly reliable account of Italian morphological gaps, 7% of Latin lemmata
listed as defective show strong corpus evidence of being non-defective. This
discrepancy highlights potential limitations of crowd-sourced wikis as
definitive sources of linguistic knowledge, particularly for less-studied
phenomena and languages, despite their value as resources for rare linguistic
features. By providing scalable tools and methods for quality assurance of
crowd-sourced data, this work advances computational morphology and expands
linguistic knowledge of defectivity in non-English, morphologically rich
languages.

</details>


### [21] [TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting](https://arxiv.org/abs/2506.17609)
*Lincan Li,Eren Erman Ozguven,Yue Zhao,Guang Wang,Yiqun Xie,Yushun Dong*

Key words: 台风轨迹预测, Transformer, 自然语言描述, 气象轨迹

TL;DR: TyphoFormer结合自然语言描述和数值数据，通过Transformer提升台风轨迹预测准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有Transformer模型在台风轨迹预测中缺乏上下文知识，TyphoFormer通过引入语言描述弥补这一不足。

Method: 利用LLM生成台风属性文本描述，嵌入为特殊标记，与数值序列一起输入Transformer编码器。

Result: 在HURDAT2基准测试中表现优于其他方法，尤其在非线性路径和有限历史数据场景下。

Conclusion: TyphoFormer证明了语言描述对提升稀疏气象轨迹预测的有效性。

Abstract: Accurate typhoon track forecasting is crucial for early system warning and
disaster response. While Transformer-based models have demonstrated strong
performance in modeling the temporal dynamics of dense trajectories of humans
and vehicles in smart cities, they usually lack access to broader contextual
knowledge that enhances the forecasting reliability of sparse meteorological
trajectories, such as typhoon tracks. To address this challenge, we propose
TyphoFormer, a novel framework that incorporates natural language descriptions
as auxiliary prompts to improve typhoon trajectory forecasting. For each time
step, we use Large Language Model (LLM) to generate concise textual
descriptions based on the numerical attributes recorded in the North Atlantic
hurricane database. The language descriptions capture high-level meteorological
semantics and are embedded as auxiliary special tokens prepended to the
numerical time series input. By integrating both textual and sequential
information within a unified Transformer encoder, TyphoFormer enables the model
to leverage contextual cues that are otherwise inaccessible through numerical
features alone. Extensive experiments are conducted on HURDAT2 benchmark,
results show that TyphoFormer consistently outperforms other state-of-the-art
baseline methods, particularly under challenging scenarios involving nonlinear
path shifts and limited historical observations.

</details>


### [22] [OpusLM: A Family of Open Unified Speech Language Models](https://arxiv.org/abs/2506.17611)
*Jinchuan Tian,William Chen,Yifan Peng,Jiatong Shi,Siddhant Arora,Shikhar Bharadwaj,Takashi Maekaku,Yusuke Shinohara,Keita Goto,Xiang Yue,Huck Yang,Shinji Watanabe*

Key words: OpusLMs, 语音语言模型, 多流设计, 多阶段训练

TL;DR: 本文介绍了OpusLMs，一组开放的语音语言模型，通过多流设计和多阶段训练策略，在语音识别和合成等领域表现出色。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 推动开放的语音语言模型研究，提供透明且高性能的模型。

Method: 从文本语言模型初始化，持续预训练于语音文本对和纯文本数据，采用多流设计和多阶段训练策略。

Result: OpusLMs在语音识别、语音合成和文本处理中表现优异。

Conclusion: OpusLMs为开放的语音语言模型研究提供了可靠的工具。

Abstract: This paper presents Open Unified Speech Language Models (OpusLMs), a family
of open foundational speech language models (SpeechLMs) up to 7B. Initialized
from decoder-only text language models, the OpusLMs are continuously
pre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We
demonstrate our OpusLMs achieve comparable (or even superior) performance with
existing SpeechLMs in speech recognition, speech synthesis, and text-only
capabilities. Technically, this paper articulates our SpeechLM designs on
tokenization, multi-stream language models, and multi-stage training
strategies. We experimentally demonstrate the importance of model size scaling
and the effect of annealing data selection. The OpusLMs are all built from
publicly available materials and are fully transparent models. We release our
code, data, checkpoints, and training logs to facilitate open SpeechLM research

</details>


### [23] [Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs](https://arxiv.org/abs/2506.17630)
*Yang Wu,Yifan Zhang,Yiwei Wang,Yujun Cai,Yurong Wu,Yuran Wang,Ning Xu,Jian Cheng*

Key words: 大型语言模型（LLMs）、推理能力、答案锚定、行为分析、五级提示框架

TL;DR: 该研究探讨了大型语言模型（LLMs）是否主要依赖答案而非真实推理能力，通过五级答案可见性提示框架发现LLMs严重依赖显式答案，推理能力可能仅是对结果的合理化解释。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是明确LLMs的成功是否源于记忆化的答案-推理模式而非真实推理能力，揭示其推理深度的局限性。

Method: 采用五级答案可见性提示框架，系统性操纵答案线索，通过间接行为分析探究模型行为。

Result: 实验显示，当答案线索被屏蔽时，LLMs的性能下降了26.90%，表明其对显式答案的强烈依赖。

Conclusion: 研究表明LLMs的推理能力更多是后验合理化而非真实推理，需重新评估其推理深度。

Abstract: While Large Language Models (LLMs) demonstrate impressive reasoning
capabilities, growing evidence suggests much of their success stems from
memorized answer-reasoning patterns rather than genuine inference. In this
work, we investigate a central question: are LLMs primarily anchored to final
answers or to the textual pattern of reasoning chains? We propose a five-level
answer-visibility prompt framework that systematically manipulates answer cues
and probes model behavior through indirect, behavioral analysis. Experiments
across state-of-the-art LLMs reveal a strong and consistent reliance on
explicit answers. The performance drops by 26.90\% when answer cues are masked,
even with complete reasoning chains. These findings suggest that much of the
reasoning exhibited by LLMs may reflect post-hoc rationalization rather than
true inference, calling into question their inferential depth. Our study
uncovers the answer-anchoring phenomenon with rigorous empirical validation and
underscores the need for a more nuanced understanding of what constitutes
reasoning in LLMs.

</details>


### [24] [Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation](https://arxiv.org/abs/2506.17637)
*Yang Wu,Yifan Zhang,Yurong Wu,Yuran Wang,Junkai Zhang,Jian Cheng*

Key words: LLMs, OR, Step-Opt-Instruct, 复杂问题, 优化建模

TL;DR: 提出Step-Opt-Instruct框架，通过逐步生成和验证高质数据，优化LLMs在复杂OR任务中的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决LLMs在复杂优化建模任务中的挑战。

Method: 采用Step-Opt-Instruct框架生成高质量数据，并通过迭代和验证优化LLMs。

Result: Step-Opt模型在NL4OPT等基准测试中表现优异，复杂任务准确率提升17.01%。

Conclusion: 结构验证与逐步优化结合，有效提升LLMs在决策自动化中的表现。

Abstract: Large Language Models (LLMs) have revolutionized various domains but
encounter substantial challenges in tackling optimization modeling tasks for
Operations Research (OR), particularly when dealing with complex problem. In
this work, we propose Step-Opt-Instruct, a framework that augments existing
datasets and generates high-quality fine-tuning data tailored to optimization
modeling. Step-Opt-Instruct employs iterative problem generation to
systematically increase problem complexity and stepwise validation to
rigorously verify data, preventing error propagation and ensuring the quality
of the generated dataset. Leveraging this framework, we fine-tune open-source
LLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that
achieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and
IndustryOR. Extensive experiments demonstrate the superior performance of
Step-Opt, especially in addressing complex OR tasks, with a notable 17.01\%
improvement in micro average accuracy on difficult problems. These findings
highlight the effectiveness of combining structured validation with gradual
problem refinement to advance the automation of decision-making processes using
LLMs.The code and dataset are available at https://github.com/samwu-learn/Step.

</details>


### [25] [TPTT: Transforming Pretrained Transformer into Titans](https://arxiv.org/abs/2506.17671)
*Fabien Furfaro*

Key words: 大语言模型, 注意力机制, 内存管理, 参数高效微调

TL;DR: TPTT是提升预训练Transformer模型效率的新框架，采用线性注意力机制和高级内存管理，显著提升计算效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决大语言模型在长上下文推理中的计算和内存需求问题。

Method: 引入TPTT框架，结合Memory as Gate和混合线性注意力机制，支持参数高效微调（LoRA）。

Result: 在1B参数模型上，MMLU基准测试显示效率与准确性显著提升（如EM提高20%）。

Conclusion: TPTT具有实用扩展性和鲁棒性，代码与工具包已开源。

Abstract: Recent advances in large language models (LLMs) have led to remarkable
progress in natural language processing, but their computational and memory
demands remain a significant challenge, particularly for long-context
inference. We introduce TPTT (Transforming Pretrained Transformer into Titans),
a novel framework for enhancing pretrained Transformer models with efficient
linearized attention mechanisms and advanced memory management. TPTT employs
techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA).
It is fully compatible with the Hugging Face Transformers library, enabling
seamless adaptation of any causal LLM through parameter-efficient fine-tuning
(LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU
benchmark with models of approximately 1 billion parameters, observing
substantial improvements in both efficiency and accuracy. For instance,
Titans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its
baseline. Statistical analyses and comparisons with recent state-of-the-art
methods confirm the practical scalability and robustness of TPTT. Code is
available at https://github.com/fabienfrfr/tptt . Python package at
https://pypi.org/project/tptt/ .

</details>


### [26] [Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering](https://arxiv.org/abs/2506.17692)
*Binquan Ji,Haibo Luo,Yifei Lu,Lei Hei,Jiaqi Wang,Tingjing Liao,Lingyu Wang,Shichao Wang,Feiliang Ren*

Key words: 多跳QA, DEC框架, 轻量级LLM, 关键词提取, 语义漂移

TL;DR: DEC框架通过分解复杂问题和上下文感知重写，结合轻量级关键词提取模块，在多跳QA任务中表现优异，显著减少计算开销。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决多跳QA任务中轻量级LLM面临的长上下文处理和语义漂移问题。

Method: 提出DEC框架，分解问题为子问题并通过上下文感知重写优化；引入轻量级关键词提取模块。

Result: DEC在多跳QA数据集上表现优异，减少token消耗，在8B参数模型中达到SOTA效果。

Conclusion: DEC在资源受限环境中高效且有效，适合轻量级LLM。

Abstract: Knowledge-intensive multi-hop question answering (QA) tasks, which require
integrating evidence from multiple sources to address complex queries, often
necessitate multiple rounds of retrieval and iterative generation by large
language models (LLMs). However, incorporating many documents and extended
contexts poses challenges -such as hallucinations and semantic drift-for
lightweight LLMs with fewer parameters. This work proposes a novel framework
called DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions
into logically coherent subquestions to form a hallucination-free reasoning
chain. It then iteratively refines these subquestions through context-aware
rewriting to generate effective query formulations. For retrieval, we introduce
a lightweight discriminative keyword extraction module that leverages extracted
keywords to achieve targeted, precise document recall with relatively low
computational overhead. Extensive experiments on three multi-hop QA datasets
demonstrate that DEC performs on par with or surpasses state-of-the-art
benchmarks while significantly reducing token consumption. Notably, our
approach attains state-of-the-art results on models with 8B parameters,
showcasing its effectiveness in various scenarios, particularly in
resource-constrained environments.

</details>


### [27] [Zero-Shot Conversational Stance Detection: Dataset and Approaches](https://arxiv.org/abs/2506.17693)
*Yuzhe Ding,Kang He,Bobo Li,Li Zheng,Haijun He,Fei Li,Chong Teng,Donghong Ji*

Key words: 立场检测、对话、零样本学习、原型对比学习、ZS-CSD

TL;DR: 提出了一种零样本立场检测数据集ZS-CSD和模型SITPCL，在零样本场景下表现优异，但仍存在挑战。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决现有对话立场检测数据集目标受限的问题，提升模型在未见目标上的泛化能力。

Method: 构建ZS-CSD数据集，提出基于说话者互动和目标感知的原型对比学习模型SITPCL。

Result: SITPCL在零样本对话立场检测中达到最先进水平，F1-macro得分为43.81%。

Conclusion: 零样本对话立场检测仍具挑战性，SITPCL为未来研究提供了基准。

Abstract: Stance detection, which aims to identify public opinion towards specific
targets using social media data, is an important yet challenging task. With the
increasing number of online debates among social media users, conversational
stance detection has become a crucial research area. However, existing
conversational stance detection datasets are restricted to a limited set of
specific targets, which constrains the effectiveness of stance detection models
when encountering a large number of unseen targets in real-world applications.
To bridge this gap, we manually curate a large-scale, high-quality zero-shot
conversational stance detection dataset, named ZS-CSD, comprising 280 targets
across two distinct target types. Leveraging the ZS-CSD dataset, we propose
SITPCL, a speaker interaction and target-aware prototypical contrastive
learning model, and establish the benchmark performance in the zero-shot
setting. Experimental results demonstrate that our proposed SITPCL model
achieves state-of-the-art performance in zero-shot conversational stance
detection. Notably, the SITPCL model attains only an F1-macro score of 43.81%,
highlighting the persistent challenges in zero-shot conversational stance
detection.

</details>


### [28] [The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future](https://arxiv.org/abs/2506.17700)
*Summra Saleem,Muhammad Nabeel Asim,Shaista Zulfiqar,Andreas Dengel*

Key words: 大型语言模型, 提示优化, 自然语言处理, 评价基准

TL;DR: 该论文综述了大型语言模型（LLMs）中提示优化的策略，填补了现有研究的空白，并通过分类和分析为未来研究提供基础。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着LLMs在NLP领域的广泛应用，提示工程和优化策略成为提升性能的关键，但目前缺乏对提示优化策略的全面分析。

Method: 论文分析了提示优化策略的工作原理，将其分为11个类别，并总结了这些策略在不同NLP任务、LLMs和基准数据集中的应用。

Result: 研究提供了一个综合框架，支持未来在统一实验设置下评估提示优化策略和LLM预测流程。

Conclusion: 该研究为提示优化策略的适应和创新提供了集中化的知识基础，推动了未探索任务中的预测模型开发。

Abstract: Large Language Models (LLMs) have revolutionized the field of Natural
Language Processing (NLP) by automating traditional labor-intensive tasks and
consequently accelerated the development of computer-aided applications. As
researchers continue to advance this field with the introduction of novel
language models and more efficient training/finetuning methodologies, the idea
of prompt engineering and subsequent optimization strategies with LLMs has
emerged as a particularly impactful trend to yield a substantial performance
boost across diverse NLP tasks. To best of our knowledge numerous review
articles have explored prompt engineering, however, a critical gap exists in
comprehensive analyses of prompt optimization strategies. To bridge this gap
this paper provides unique and comprehensive insights about the potential of
diverse prompt optimization strategies. It analyzes their underlying working
paradigms and based on these principles, categorizes them into 11 distinct
classes. Moreover, the paper provides details about various NLP tasks where
these prompt optimization strategies have been employed, along with details of
different LLMs and benchmark datasets used for evaluation. This comprehensive
compilation lays a robust foundation for future comparative studies and enables
rigorous assessment of prompt optimization and LLM-based predictive pipelines
under consistent experimental settings: a critical need in the current
landscape. Ultimately, this research will centralize diverse strategic
knowledge to facilitate the adaptation of existing prompt optimization
strategies for development of innovative predictors across unexplored tasks.

</details>


### [29] [Aged to Perfection: Machine-Learning Maps of Age in Conversational English](https://arxiv.org/abs/2506.17708)
*MingZe Tang*

Key words: 英国国家语料库、年龄组、语言模式、机器学习、社会语言学

TL;DR: 研究利用英国国家语料库2014，分析不同年龄组的语言模式，探讨说话者人口统计学与语言因素的关系，通过计算语言分析和机器学习揭示代际语言学特征。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索不同年龄组之间语言模式的差异，了解说话者人口统计学与语言学特征的联系。

Method: 使用英国国家语料库2014，结合计算语言分析和机器学习，分析语义多样性、词汇选择和句子长度等。

Result: 揭示了代际语言学特征，并建立了预测说话者年龄组的模型。

Conclusion: 研究增进了对现代英国口语社会语言学多样性的理解。

Abstract: The study uses the British National Corpus 2014, a large sample of
contemporary spoken British English, to investigate language patterns across
different age groups. Our research attempts to explore how language patterns
vary between different age groups, exploring the connection between speaker
demographics and linguistic factors such as utterance duration, lexical
diversity, and word choice. By merging computational language analysis and
machine learning methodologies, we attempt to uncover distinctive linguistic
markers characteristic of multiple generations and create prediction models
that can consistently estimate the speaker's age group from various aspects.
This work contributes to our knowledge of sociolinguistic diversity throughout
the life of modern British speech.

</details>


### [30] [Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages](https://arxiv.org/abs/2506.17715)
*Matthias Schöffel,Esteban Garces Arias,Marinus Wiedner,Paula Ruppert,Meimingwei Li,Christian Heumann,Matthias Aßenmacher*

Key words: 词性标注, 大型语言模型, 中世纪罗曼语, 低资源语言, 标注准确性

TL;DR: 研究了中世纪罗曼语（如中世纪奥克语、西班牙语和法语）的词性标注问题，探讨了大型语言模型（LLMs）在此领域的应用挑战和解决方案。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 中世纪罗曼语的词性标注面临语言演变、拼写变异和标注数据稀缺等挑战，需探索适合低资源历史语言的技术。

Method: 通过系统实验评估了微调方法、提示工程、模型架构、解码策略及跨语言迁移学习对标注准确性的影响。

Result: 发现LLMs在处理历史语言变异和非标准化拼写时存在局限性，但也有能有效应对低资源历史语言的专门技术。

Conclusion: 研究为中世纪罗曼语的词性标注提供了技术方向，指明了未来改进的可能性。

Abstract: Part-of-speech (POS) tagging remains a foundational component in natural
language processing pipelines, particularly critical for historical text
analysis at the intersection of computational linguistics and digital
humanities. Despite significant advancements in modern large language models
(LLMs) for ancient languages, their application to Medieval Romance languages
presents distinctive challenges stemming from diachronic linguistic evolution,
spelling variations, and labeled data scarcity. This study systematically
investigates the central determinants of POS tagging performance across diverse
corpora of Medieval Occitan, Medieval Spanish, and Medieval French texts,
spanning biblical, hagiographical, medical, and dietary domains. Through
rigorous experimentation, we evaluate how fine-tuning approaches, prompt
engineering, model architectures, decoding strategies, and cross-lingual
transfer learning techniques affect tagging accuracy. Our results reveal both
notable limitations in LLMs' ability to process historical language variations
and non-standardized spelling, as well as promising specialized techniques that
effectively address the unique challenges presented by low-resource historical
languages.

</details>


### [31] [KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process](https://arxiv.org/abs/2506.17728)
*Dalong Zhang,Jun Xu,Jun Zhou,Lei Liang,Lin Yuan,Ling Zhong,Mengshu Sun,Peilong Zhao,QiWei Wang,Xiaorui Wang,Xinkai Du,YangYang Hou,Yu Ao,ZhaoYang Wang,Zhengke Gui,ZhiYing Yi,Zhongpu Bo*

Key words: KAG-Thinker, 大语言模型, 逻辑推理, 知识检索, 监督微调

TL;DR: KAG-Thinker是一个基于轻参数大语言模型（LLM）的人机推理框架，通过结构化的思维过程提升逻辑一致性和上下文一致性。它通过广度分解将复杂问题分解为子问题，利用知识边界和深度解决模型优化知识获取，并通过监督微调而非强化学习来对齐推理范式。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 提升大语言模型在特定领域知识库问答任务中的逻辑连贯性和上下文一致性，模拟人类认知机制处理复杂问题。

Method: 采用广度分解将复杂问题分解为子问题，利用自然语言和逻辑函数表示，分类为知识检索或推理分析任务；通过知识边界和深度解决模型优化知识获取；使用监督微调和对齐推理范式。

Result: 框架能够更有效地处理复杂问题，提升逻辑一致性和知识获取的全面性。

Conclusion: KAG-Thinker通过结构化思维和优化知识检索，显著提升了大语言模型的推理能力。

Abstract: In this paper, we introduce KAG-Thinker, a novel human-like reasoning
framework built upon a parameter-light large language model (LLM). Our approach
enhances the logical coherence and contextual consistency of the thinking
process in question-answering (Q\&A) tasks on domain-specific knowledge bases
(KBs) within LLMs. This framework simulates human cognitive mechanisms for
handling complex problems by establishing a structured thinking process.
Continuing the \textbf{Logical Form} guided retrieval and reasoning technology
route of KAG v0.7, firstly, it decomposes complex questions into independently
solvable sub-problems(also referred to as logical forms) through
\textbf{breadth decomposition}, each represented in two equivalent
forms-natural language and logical function-and further classified as either
Knowledge Retrieval or Reasoning Analysis tasks, with dependencies and
variables passing explicitly modeled via logical function interfaces. In the
solving process, the Retrieval function is used to perform knowledge retrieval
tasks, while the Math and Deduce functions are used to perform reasoning
analysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval
sub-problem tasks, LLMs and external knowledge sources are regarded as
equivalent KBs. We use the \textbf{knowledge boundary} model to determine the
optimal source using self-regulatory mechanisms such as confidence calibration
and reflective reasoning, and use the \textbf{depth solving} model to enhance
the comprehensiveness of knowledge acquisition. Finally, instead of utilizing
reinforcement learning, we employ supervised fine-tuning with multi-turn
dialogues to align the model with our structured inference paradigm, thereby
avoiding excessive reflection. This is supported by a data evaluation framework
and iterative corpus synthesis, which facilitate the generation of detailed
reasoning trajectories...

</details>


### [32] [HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations](https://arxiv.org/abs/2506.17748)
*Anwoy Chatterjee,Yash Goel,Tanmoy Chakraborty*

Key words: 语言模型，幻觉检测，解耦表示，HSIC，单遍方法

TL;DR: 语言模型（LMs）常生成与输入上下文不符的虚假内容（幻觉）。为高效检测幻觉，作者提出了一种单次、无训练的方法HIDE，通过解耦LM的隐藏表示实现检测。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决LMs生成虚假内容的问题，提高其可靠性，同时避免现有多遍检测方法的高计算成本。

Method: 提出HIDE方法，基于输入上下文和生成输出之间的解耦假设，利用HSIC量化隐藏状态表示。

Result: 在四大QA数据集和六种开源LMs上的实验表明，HIDE在单遍方法中表现最佳，AUC-ROC平均提升29%，并优于多遍方法3%，同时减少51%计算时间。

Conclusion: 通过解耦LM内部表示，HIDE实现了高效且实用的幻觉检测。

Abstract: Contemporary Language Models (LMs), while impressively fluent, often generate
content that is factually incorrect or unfaithful to the input context - a
critical issue commonly referred to as 'hallucination'. This tendency of LMs to
generate hallucinated content undermines their reliability, especially because
these fabrications are often highly convincing and therefore difficult to
detect. While several existing methods attempt to detect hallucinations, most
rely on analyzing multiple generations per input, leading to increased
computational cost and latency. To address this, we propose a single-pass,
training-free approach for effective Hallucination detectIon via Decoupled
rEpresentations (HIDE). Our approach leverages the hypothesis that
hallucinations result from a statistical decoupling between an LM's internal
representations of input context and its generated output. We quantify this
decoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to
hidden-state representations extracted while generating the output sequence. We
conduct extensive experiments on four diverse question answering datasets,
evaluating both faithfulness and factuality hallucinations across six
open-source LMs of varying scales and properties. Our results demonstrate that
HIDE outperforms other single-pass methods in almost all settings, achieving an
average relative improvement of ~29% in AUC-ROC over the best-performing
single-pass strategy across various models and datasets. Additionally, HIDE
shows competitive and often superior performance with multi-pass
state-of-the-art methods, obtaining an average relative improvement of ~3% in
AUC-ROC while consuming ~51% less computation time. Our findings highlight the
effectiveness of exploiting internal representation decoupling in LMs for
efficient and practical hallucination detection.

</details>


### [33] [Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights](https://arxiv.org/abs/2506.17789)
*N J Karthika,Maharaj Brahma,Rohit Saluja,Ganesh Ramakrishnan,Maunendra Sankar Desarkar*

Key words: 分词, 多语言NLP, BPE, Unigram LM, 低资源语言

TL;DR: 该论文对17种印度语言的分词策略进行了全面评估，分析了不同算法（BPE和Unigram LM）及其对多语言NLP的影响，提出了更公平高效的分词方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有分词器对高资源语言偏好明显，难以满足印度次大陆等语言多样性和形态复杂性的需求。

Method: 通过量化评估BPE和Unigram LM算法的优劣、词汇量影响及多语言词汇构建策略（联合训练和基于聚类的训练）。

Result: 研究发现极低资源语言可从相关高资源语言的训练中受益，为构建更公平高效的多语言分词器提供了实用建议。

Conclusion: 研究为多语言NLP的分词器设计提供了语言学依据，推动了更公平高效的技术发展。

Abstract: Tokenization plays a pivotal role in multilingual NLP. However, existing
tokenizers are often skewed towards high-resource languages, limiting their
effectiveness for linguistically diverse and morphologically rich languages
such as those in the Indian subcontinent. This paper presents a comprehensive
intrinsic evaluation of tokenization strategies across 17 Indian languages. We
quantify the trade-offs between bottom-up and top-down tokenizer algorithms
(BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of
multilingual vocabulary construction such as joint and cluster-based training.
We also show that extremely low-resource languages can benefit from tokenizers
trained on related high-resource languages. Our study provides practical
insights for building more fair, efficient, and linguistically informed
tokenizers for multilingual NLP.

</details>


### [34] [THCM-CAL: Temporal-Hierarchical Causal Modelling with Conformal Calibration for Clinical Risk Prediction](https://arxiv.org/abs/2506.17844)
*Xin Zhang,Qiyu Wei,Yingjie Zhu,Fanyi Wu,Sophia Ananiadou*

Key words: 电子健康记录,风险预测,因果模型,多模态数据

TL;DR: 论文提出了THCM-CAL模型，通过因果图建模结构化诊断代码和无结构叙述笔记的交互，提升临床风险预测可靠性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决电子健康记录中结构化与非结构化数据模态间的因果交互问题。

Method: THCM-CAL构建多模态因果图，利用分层因果发现推断三种临床基础交互，并扩展了多标签ICD编码的置信度校准。

Result: 在MIMIC-III和MIMIC-IV数据集上表现优异。

Conclusion: THCM-CAL能有效建模临床数据模态间的复杂交互，提升预测可靠性。

Abstract: Automated clinical risk prediction from electronic health records (EHRs)
demands modeling both structured diagnostic codes and unstructured narrative
notes. However, most prior approaches either handle these modalities separately
or rely on simplistic fusion strategies that ignore the directional,
hierarchical causal interactions by which narrative observations precipitate
diagnoses and propagate risk across admissions. In this paper, we propose
THCM-CAL, a Temporal-Hierarchical Causal Model with Conformal Calibration. Our
framework constructs a multimodal causal graph where nodes represent clinical
entities from two modalities: Textual propositions extracted from notes and ICD
codes mapped to textual descriptions. Through hierarchical causal discovery,
THCM-CAL infers three clinically grounded interactions: intra-slice
same-modality sequencing, intra-slice cross-modality triggers, and inter-slice
risk propagation. To enhance prediction reliability, we extend conformal
prediction to multi-label ICD coding, calibrating per-code confidence intervals
under complex co-occurrences. Experimental results on MIMIC-III and MIMIC-IV
demonstrate the superiority of THCM-CAL.

</details>


### [35] [LLMs for Customized Marketing Content Generation and Evaluation at Scale](https://arxiv.org/abs/2506.17863)
*Haoran Liu,Amir Tahmasbi,Ehtesham Sam Haque,Purak Jain*

Key words: 离站营销, 广告生成, 自动化评估, LLM

TL;DR: 论文提出MarketingFM系统，通过多数据源生成关键词定向广告，并开发AutoEval-Main和AutoEval-Update系统自动化评估广告质量，显著提升效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前离站营销内容通用且与落地页不匹配，效果受限，需高效生成与评估定向广告。

Method: 结合检索增强与多数据源生成广告，并基于规则与LLM技术自动化评估。

Result: 关键词广告点击率提升9%，AutoEval-Main与人类评估一致率达89.57%。

Conclusion: 自动化系统显著提升广告效果与评估效率，但仍需人工监督。

Abstract: Offsite marketing is essential in e-commerce, enabling businesses to reach
customers through external platforms and drive traffic to retail websites.
However, most current offsite marketing content is overly generic,
template-based, and poorly aligned with landing pages, limiting its
effectiveness. To address these limitations, we propose MarketingFM, a
retrieval-augmented system that integrates multiple data sources to generate
keyword-specific ad copy with minimal human intervention. We validate
MarketingFM via offline human and automated evaluations and large-scale online
A/B tests. In one experiment, keyword-focused ad copy outperformed templates,
achieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC,
demonstrating gains in ad ranking and cost efficiency. Despite these gains,
human review of generated ads remains costly. To address this, we propose
AutoEval-Main, an automated evaluation system that combines rule-based metrics
with LLM-as-a-Judge techniques to ensure alignment with marketing principles.
In experiments with large-scale human annotations, AutoEval-Main achieved
89.57% agreement with human reviewers. Building on this, we propose
AutoEval-Update, a cost-efficient LLM-human collaborative framework to
dynamically refine evaluation prompts and adapt to shifting criteria with
minimal human input. By selectively sampling representative ads for human
review and using a critic LLM to generate alignment reports, AutoEval-Update
improves evaluation consistency while reducing manual effort. Experiments show
the critic LLM suggests meaningful refinements, improving LLM-human agreement.
Nonetheless, human oversight remains essential for setting thresholds and
validating refinements before deployment.

</details>


### [36] [QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs](https://arxiv.org/abs/2506.17864)
*Taolin Zhang,Haidong Kang,Dongyang Li,Qizhou Chen,Chengyu Wang Xiaofeng He,Richang Hong*

Key words: 大规模语言模型, 模型编辑, 连续编辑, 自校正, 队列

TL;DR: 论文提出一种基于队列的自校正框架QueueEDIT，解决大规模语言模型（LLMs）在连续编辑中的性能下降问题，并减少参数偏差对模型通用能力的影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管LLMs表现出色，但仍存在幻觉问题。模型编辑用于修正事实错误，连续编辑尤其具有挑战性。编辑过程中引入的新参数可能损害模型的通用能力。

Method: 提出QueueEDIT框架，通过结构映射编辑损失将三元组映射到LLMs的知识敏感神经元，并将编辑参数存储在队列中，动态对齐先前编辑的参数。无关参数被冻结，仅更新队列头部参数。

Result: 实验表明，QueueEDIT在多种连续编辑场景下显著优于基线方法，同时在单次编辑中保持竞争力，且LLMs在编辑过程中仍保持通用任务的高能力。

Conclusion: QueueEDIT有效提升了连续模型编辑性能，同时减少了参数偏差对LLMs通用能力的负面影响。

Abstract: Recently, large language models (LLMs) have demonstrated impressive results
but still suffer from hallucinations. Model editing has been proposed to
correct factual inaccuracies in LLMs. A challenging case is sequential model
editing (SME), which aims to rectify errors continuously rather than treating
them as a one-time task. During SME, the general capabilities of LLMs can be
negatively affected due to the introduction of new parameters. In this paper,
we propose a queue-based self-correction framework (QueueEDIT) that not only
enhances SME performance by addressing long-sequence dependency but also
mitigates the impact of parameter bias on the general capabilities of LLMs.
Specifically, we first introduce a structural mapping editing loss to map the
triplets to the knowledge-sensitive neurons within the Transformer layers of
LLMs. We then store the located parameters for each piece of edited knowledge
in a queue and dynamically align previously edited parameters. In each edit, we
select queue parameters most relevant to the currently located parameters to
determine whether previous knowledge needs realignment. Irrelevant parameters
in the queue are frozen, and we update the parameters at the queue head to the
LLM to ensure they do not harm general abilities. Experiments show that our
framework significantly outperforms strong baselines across various SME
settings and maintains competitiveness in single-turn editing. The resulting
LLMs also preserve high capabilities in general NLP tasks throughout the SME
process.

</details>


### [37] [How Alignment Shrinks the Generative Horizon](https://arxiv.org/abs/2506.17871)
*Chenghao Yang,Ari Holtzman*

Key words: 大型语言模型, 对齐调整, 分支因子, 稳定生成, 推理任务

TL;DR: 该论文研究了大型语言模型输出缺乏多样性的原因，提出了分支因子（BF）作为衡量生成多样性的指标，发现对齐调整显著降低了BF，并探讨了其对推理任务的影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究大型语言模型（LLMs）在生成过程中输出稳定性的原因，以解释为何对齐后的模型往往缺乏多样性。

Method: 通过引入分支因子（BF）这一指标，量化模型输出分布的概率集中现象，并分析对齐调整和生成链长度对BF的影响。

Result: 发现对齐调整显著降低了BF（近乎一个数量级），且BF随生成过程递减；对齐后的模型因更早进入低BF阶段而表现更稳定。

Conclusion: 对齐并未改变模型的行为模式，而是引导其利用低熵轨迹；BF可作为理解和控制LLM输出的有效工具。

Abstract: Despite their impressive capabilities, aligned large language models (LLMs)
often generate outputs that lack diversity. What drives this stability in the
generation? We investigate this phenomenon through the lens of probability
concentration in the model's output distribution. To quantify this
concentration, we introduce the Branching Factor (BF) -- a token-invariant
measure of the effective number of plausible next steps during generation. Our
empirical analysis reveals two key findings: (1) BF often decreases as
generation progresses, suggesting that LLMs become more predictable as they
generate. (2) alignment tuning substantially sharpens the model's output
distribution from the outset, reducing BF by nearly an order of magnitude
(e.g., from 12 to 1.2) relative to base models. This stark reduction helps
explain why aligned models often appear less sensitive to decoding strategies.
Building on this insight, we find this stability has surprising implications
for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g.,
DeepSeek-distilled models), for instance, leverage this effect; by generating
longer reasoning chains, they push generation into later, more deterministic
(lower BF) stages, resulting in more stable outputs. We hypothesize that
alignment tuning does not fundamentally change a model's behavior, but instead
steers it toward stylistic tokens (e.g., "Sure") that unlock low-entropy
trajectories already present in the base model. This view is supported by
nudging experiments, which show that prompting base models with such tokens can
similarly reduce BF. Together, our findings establish BF as a powerful
diagnostic for understanding and controlling LLM outputs - clarifying how
alignment reduces variability, how CoT promotes stable generations, and how
base models can be steered away from diversity.

</details>


### [38] [Multi-turn Jailbreaking via Global Refinement and Active Fabrication](https://arxiv.org/abs/2506.17881)
*Hua Tang,Lingyong Yan,Yukun Zhao,Shuaiqiang Wang,Jizhou Huang,Dawei Yin*

Key words: 大语言模型,越狱,多轮对话,安全威胁,全局优化

TL;DR: 该论文提出了一种新型的多轮越狱方法，通过全局优化逃逸路径并主动伪造模型响应，以提高生成有害内容的成功率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有越狱技术主要关注单轮场景，而多轮场景的安全威胁尚未充分研究。为解决这一问题，作者提出了一种适应对话动态变化的多轮越狱方法。

Method: 通过全局优化每一步的逃逸路径，并主动伪造模型响应以抑制安全警告，从而在后续提问中更易生成有害内容。

Result: 实验结果表明，该方法在六种先进大语言模型上的表现优于现有的单轮和多轮越狱技术。

Conclusion: 该方法有效解决了多轮越狱中的动态适应问题，提升了安全威胁识别能力。

Abstract: Large Language Models (LLMs) have achieved exceptional performance across a
wide range of tasks. However, they still pose significant safety risks due to
the potential misuse for malicious purposes. Jailbreaks, which aim to elicit
models to generate harmful content, play a critical role in identifying the
underlying security threats. Recent jailbreaking primarily focuses on
single-turn scenarios, while the more complicated multi-turn scenarios remain
underexplored. Moreover, existing multi-turn jailbreaking techniques struggle
to adapt to the evolving dynamics of dialogue as the interaction progresses. To
address this limitation, we propose a novel multi-turn jailbreaking method that
refines the jailbreaking path globally at each interaction. We also actively
fabricate model responses to suppress safety-related warnings, thereby
increasing the likelihood of eliciting harmful outputs in subsequent questions.
Experimental results demonstrate the superior performance of our method
compared with existing single-turn and multi-turn jailbreaking techniques
across six state-of-the-art LLMs. Our code is publicly available at
https://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication.

</details>


### [39] [Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation](https://arxiv.org/abs/2506.17949)
*Hong Su*

Key words: 大语言模型,创新扩散,泛化,结构冗余

TL;DR: 论文提出了一种基于扩散的创新扩展模型，帮助LLM在相似结构中推广应用局部创新。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: LLM在预训练中表现出强大的模式复现能力，但难以将局部创新应用到多阶段过程中的其他部分。

Method: 提出了四步创新扩散模型：识别核心创新、泛化创新、判断适用范围、系统化应用。

Result: 验证结果表明该模型能有效提升LLM在结构相似阶段中的创新推广能力。

Conclusion: 该模型通过利用结构冗余性，显著增强了LLM的泛化和重用能力。

Abstract: Large Language Models (LLMs) exhibit strong capabilities in reproducing and
extending patterns observed during pretraining but often struggle to generalize
novel ideas beyond their original context. This paper addresses the challenge
of applying such localized innovations - introduced at a specific stage or
component - to other parts of a multi-stage process. We propose a scatter-based
innovation expansion model (innovation scatter model) that guides the LLM
through a four-step process: (1) identifying the core innovation by comparing
the user's input with its surrounding context, (2) generalizing the innovation
by removing references to specific stages or components, (3) determining
whether the generalized innovation applies to a broader scope beyond the
original stage, and (4) systematically applying it to other structurally
similar stages using the LLM. This model leverages structural redundancy across
stages to improve the applicability of novel ideas. Verification results
demonstrate that the innovation scatter model enables LLMs to extend
innovations across structurally similar stages, thereby enhancing
generalization and reuse.

</details>


### [40] [A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment](https://arxiv.org/abs/2506.17951)
*Quanwei Tang,Sophia Yat Mei Lee,Junshuang Wu,Dong Zhang,Shoushan Li,Erik Cambria,Guodong Zhou*

Key words: RAG, GraphMPA, 模式寻求偏好优化

TL;DR: GraphMPA是一个基于图的框架，通过模式寻求偏好对齐来提升检索增强生成模型的问题回答能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决检索增强生成模型中全局理解和人类偏好对齐的挑战。

Method: 构建分层文档图并使用模式寻求偏好优化。

Result: 在六个数据集上的实验证明了其有效性。

Conclusion: GraphMPA能有效提升模型输出的质量和伦理对齐。

Abstract: Recent advancements in retrieval-augmented generation (RAG) have enhanced
large language models in question answering by integrating external knowledge.
However, challenges persist in achieving global understanding and aligning
responses with human ethical and quality preferences. To address these issues,
we propose GraphMPA, a comprehensive graph-based framework with mode-seeking
preference alignment. Our approach constructs a hierarchical document graph
using a general similarity measurement, mimicking human cognitive processes for
information understanding and synthesis. Additionally, we introduce
mode-seeking preference optimization to better align model outputs with human
preferences through probability-matching constraints. Extensive experiments on
six datasets demonstrate the effectiveness of our
\href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.

</details>


### [41] [PDF Retrieval Augmented Question Answering](https://arxiv.org/abs/2506.18027)
*Thi Thu Uyen Hoang,Viet Anh Nguyen*

Key words: 问答系统, 检索增强生成, PDF处理, 多模态数据, 语言模型

TL;DR: 本文提出了一种基于检索增强生成（RAG）框架的问答系统，旨在提升从PDF文件中提取信息的能力，尤其是处理多模态数据（如文本、图像、图表等）。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有QA系统主要针对文本内容，而PDF文件中包含丰富的多模态数据（如图像、图表等），这为信息提取带来了独特挑战。

Method: 通过改进RAG框架中非文本元素的处理和集成方法，并微调大型语言模型以适应系统，开发了一个综合的QA系统。

Result: 实验证明，该系统能够高效地从PDF中提取准确的多样化信息，并回答复杂的多模态问题。

Conclusion: 该研究不仅拓展了检索增强QA系统的边界，还为多模态数据集成和处理的研究奠定了基础。

Abstract: This paper presents an advancement in Question-Answering (QA) systems using a
Retrieval Augmented Generation (RAG) framework to enhance information
extraction from PDF files. Recognizing the richness and diversity of data
within PDFs--including text, images, vector diagrams, graphs, and tables--poses
unique challenges for existing QA systems primarily designed for textual
content. We seek to develop a comprehensive RAG-based QA system that will
effectively address complex multimodal questions, where several data types are
combined in the query. This is mainly achieved by refining approaches to
processing and integrating non-textual elements in PDFs into the RAG framework
to derive precise and relevant answers, as well as fine-tuning large language
models to better adapt to our system. We provide an in-depth experimental
evaluation of our solution, demonstrating its capability to extract accurate
information that can be applied to different types of content across PDFs. This
work not only pushes the boundaries of retrieval-augmented QA systems but also
lays a foundation for further research in multimodal data integration and
processing.

</details>


### [42] [Splitformer: An improved early-exit architecture for automatic speech recognition on edge devices](https://arxiv.org/abs/2506.18035)
*Maxence Lasbordes,Daniele Falavigna,Alessio Brutti*

Key words: early-exit architectures, automatic speech recognition, dynamic computation, resource-aware processing

TL;DR: 论文提出了一种在提前退出架构中引入并行层的方法，通过处理输入的下采样版本来提升语音识别性能，同时仅轻微增加参数量且不影响推理时间。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在设备端处理场景中，资源有限且计算能力随时间变化，动态调整神经网络计算负载至关重要。现有的提前退出架构虽然高效，但在自动语音识别任务中缺乏灵活性。

Method: 在架构中引入并行层，处理输入的下采样版本，改进现有提前退出模型的性能。

Result: 标准基准测试中的语音识别性能显著提升，总参数量略有增加但推理时间不受影响。

Conclusion: 该方法有效平衡了计算资源使用与性能，适合资源受限场景。

Abstract: The ability to dynamically adjust the computational load of neural models
during inference in a resource aware manner is crucial for on-device processing
scenarios, characterised by limited and time-varying computational resources.
Early-exit architectures represent an elegant and effective solution, since
they can process the input with a subset of their layers, exiting at
intermediate branches (the upmost layers are hence removed from the model).
  From a different perspective, for automatic speech recognition applications
there are memory-efficient neural architectures that apply variable frame rate
analysis, through downsampling/upsampling operations in the middle layers,
reducing the overall number of operations and improving significantly the
performance on well established benchmarks. One example is the Zipformer.
However, these architectures lack the modularity necessary to inject early-exit
branches.
  With the aim of improving the performance in early-exit models, we propose
introducing parallel layers in the architecture that process downsampled
versions of their inputs. % in conjunction with standard processing layers. We
show that in this way the speech recognition performance on standard benchmarks
significantly improve, at the cost of a small increase in the overall number of
model parameters but without affecting the inference time.

</details>


### [43] [Markov-Enhanced Clustering for Long Document Summarization: Tackling the 'Lost in the Middle' Challenge with Large Language Models](https://arxiv.org/abs/2506.18036)
*Aziz Amari,Mohamed Achref Ben Ammar*

Key words: 文本摘要、提取式摘要、生成式摘要、混合方法、马尔可夫链图

TL;DR: 提出了一种结合抽取式和生成式方法的混合摘要技术，通过分块、聚类和马尔可夫链图选择语义顺序，解决长文档摘要中的关键信息丢失问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 信息爆炸时代需要高效的自动文本摘要技术，但目前生成式摘要模型在长文档中易丢失关键信息（'lost in the middle'问题）。

Method: 将文档分块并聚类，为每个聚类生成摘要，利用马尔可夫链图选择语义顺序，构建最终摘要。

Result: 混合方法有效解决了长文档摘要中的关键信息丢失问题。

Conclusion: 混合方法在长文档摘要中表现优于单一方法，平衡了资源消耗和信息保留。

Abstract: The rapid expansion of information from diverse sources has heightened the
need for effective automatic text summarization, which condenses documents into
shorter, coherent texts. Summarization methods generally fall into two
categories: extractive, which selects key segments from the original text, and
abstractive, which generates summaries by rephrasing the content coherently.
Large language models have advanced the field of abstractive summarization, but
they are resourceintensive and face significant challenges in retaining key
information across lengthy documents, which we call being "lost in the middle".
To address these issues, we propose a hybrid summarization approach that
combines extractive and abstractive techniques. Our method splits the document
into smaller text chunks, clusters their vector embeddings, generates a summary
for each cluster that represents a key idea in the document, and constructs the
final summary by relying on a Markov chain graph when selecting the semantic
order of ideas.

</details>


### [44] [Statistical Multicriteria Evaluation of LLM-Generated Text](https://arxiv.org/abs/2506.18082)
*Esteban Garces Arias,Hannah Blocher,Julian Rodemann,Matthias Aßenmacher,Christoph Jansen*

Key words: 文本质量评估,广义随机优势,多维度评估,统计推断,解码策略

TL;DR: 该论文提出了一种基于广义随机优势（GSD）的框架，用于解决LLM生成文本质量评估中的多维度权衡问题，克服了现有方法的局限性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前评估方法通常依赖单一指标或简单聚合，无法捕捉文本质量的多维度特性（如连贯性、多样性、流畅性等）。

Method: 采用广义随机优势（GSD）框架，支持多维度评估并避免指标的任意加权，结合部分序对解码策略进行统计推断。

Result: 该框架能够识别解码策略在统计上显著的性能差异，同时考虑采样设计的潜在偏差。

Conclusion: GSD-front方法为LLM生成文本的质量评估提供了更全面且统计可靠的解决方案。

Abstract: Assessing the quality of LLM-generated text remains a fundamental challenge
in natural language processing. Current evaluation approaches often rely on
isolated metrics or simplistic aggregations that fail to capture the nuanced
trade-offs between coherence, diversity, fluency, and other relevant indicators
of text quality. In this work, we adapt a recently proposed framework for
statistical inference based on Generalized Stochastic Dominance (GSD) that
addresses three critical limitations in existing benchmarking methodologies:
the inadequacy of single-metric evaluation, the incompatibility between
cardinal automatic metrics and ordinal human judgments, and the lack of
inferential statistical guarantees. The GSD-front approach enables simultaneous
evaluation across multiple quality dimensions while respecting their different
measurement scales, building upon partial orders of decoding strategies, thus
avoiding arbitrary weighting of the involved metrics. By applying this
framework to evaluate common decoding strategies against human-generated text,
we demonstrate its ability to identify statistically significant performance
differences while accounting for potential deviations from the i.i.d.
assumption of the sampling design.

</details>


### [45] [Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution](https://arxiv.org/abs/2506.18091)
*Patrik Stano,Aleš Horák*

Key words: anaphora resolution, Czech, LLMs, fine-tuning, prompt engineering

TL;DR: 论文比较了两种现代方法在捷克语中指代消解任务的表现：基于大语言模型的提示工程与微调紧凑生成模型，结果显示微调模型（尤其是mT5-large）表现更优。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 指代消解对于自然语言理解至关重要，特别是在形态丰富的捷克语中。论文旨在比较两种现代方法的有效性。

Method: 使用来自Prague Dependency Treebank的数据集，评估了多种指令调优的大语言模型（如Mistral Large 2和Llama 3）与微调的mT5和Mistral变体。

Result: 提示工程方法在少量示例下表现良好（最高74.5%准确率），但微调模型（尤其是mT5-large）显著超越，达到88%准确率且计算资源需求更低。

Conclusion: 微调模型在捷克语指代消解任务中表现更优，尤其是在准确率和资源效率方面。

Abstract: Anaphora resolution plays a critical role in natural language understanding,
especially in morphologically rich languages like Czech. This paper presents a
comparative evaluation of two modern approaches to anaphora resolution on Czech
text: prompt engineering with large language models (LLMs) and fine-tuning
compact generative models. Using a dataset derived from the Prague Dependency
Treebank, we evaluate several instruction-tuned LLMs, including Mistral Large 2
and Llama 3, using a series of prompt templates. We compare them against
fine-tuned variants of the mT5 and Mistral models that we trained specifically
for Czech anaphora resolution. Our experiments demonstrate that while prompting
yields promising few-shot results (up to 74.5% accuracy), the fine-tuned
models, particularly mT5-large, outperform them significantly, achieving up to
88% accuracy while requiring fewer computational resources. We analyze
performance across different anaphora types, antecedent distances, and source
corpora, highlighting key strengths and trade-offs of each approach.

</details>


### [46] [InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating](https://arxiv.org/abs/2506.18102)
*Fuyu Wang,Jiangtong Li,Kun Zhu,Changjun Jiang*

Key words: 大语言模型,辩论系统,评估框架,优化方法,CoT推理

TL;DR: 提出了一种双组件框架（InspireScore和InspireDebate），用于解决现有LLM辩论系统在客观评估（如真实性和逻辑有效性）和结构化优化上的不足，显著提升了辩论效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基于LLM的辩论系统忽视客观评估（如真实性和逻辑有效性），且缺乏结构化优化方法，限制了其效果。

Method: InspireScore是一个多维评估系统，结合主观和客观标准；InspireDebate通过CoT推理增强、多维DPO和Web-RAG实现了分阶段优化。

Result: InspireScore与专家判断的相关性比现有方法高44%；InspireDebate比基线模型表现优57%。

Conclusion: 双组件框架显著提升了辩论系统的评估和效果，为未来研究提供了新方向。

Abstract: With the rapid advancements in large language models (LLMs), debating tasks,
such as argument quality assessment and debate process simulation, have made
significant progress. However, existing LLM-based debating systems focus on
responding to specific arguments while neglecting objective assessments such as
authenticity and logical validity. Furthermore, these systems lack a structured
approach to optimize across various dimensions$-$including evaluation metrics,
chain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby
limiting their effectiveness. To address these interconnected challenges, we
propose a dual-component framework: (1) $\textbf{InspireScore}$, a novel
evaluation system that establishes a multi-dimensional assessment architecture
incorporating four subjective criteria (emotional appeal, argument clarity,
argument arrangement, and topic relevance) alongside two objective metrics
(fact authenticity and logical validity); and (2) $\textbf{InspireDebate}$, an
optimized debating framework employing a phased optimization approach through
CoT reasoning enhancement, multi-dimensional Direct Preference Optimization
(DPO), and real-time knowledge grounding via web-based Retrieval Augmented
Generation (Web-RAG). Empirical evaluations demonstrate that
$\textbf{InspireScore}$ achieves 44$\%$ higher correlation with expert
judgments compared to existing methods, while $\textbf{InspireDebate}$ shows
significant improvements, outperforming baseline models by 57$\%$. Source code
is available at https://github.com/fywang12/InspireDebate.

</details>


### [47] [Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use](https://arxiv.org/abs/2506.18105)
*Yicheng Fu,Zhemin Huang,Liuxin Yang,Yumeng Lu,Zhongdongming Dai*

Key words: 

TL;DR: Chengyu-Bench 是一个针对中文成语的综合评测基准，包含三个任务：情感评价、适用性检测和开放式填空。现有大模型在情感评价上表现优异，但在理解成语的语境和文化内涵上仍有不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 针对中文成语的理解和使用困难，现有评测基准任务单一，无法全面评估语言模型的实际能力。

Method: 提出 Chengyu-Bench，包含 2,937 个人工验证样本，涵盖 1,765 个常见成语，设计了三个任务：情感评价、适用性检测和开放式填空。

Result: 评测结果显示，主流大模型在情感评价任务上准确率达 95%，但在适用性检测和开放式填空任务上分别仅为 85% 和 40%，主要错误源于对成语含义的根本误解。

Conclusion: 尽管语言模型能可靠判断成语情感倾向，但仍难以掌握其文化及语境内涵，Chengyu-Bench 为改进提供了重要参考。

Abstract: Chinese idioms (Chengyu) are concise four-character expressions steeped in
history and culture, whose literal translations often fail to capture their
full meaning. This complexity makes them challenging for language models to
interpret and use correctly. Existing benchmarks focus on narrow tasks -
multiple-choice cloze tests, isolated translation, or simple paraphrasing. We
introduce Chengyu-Bench, a comprehensive benchmark featuring three tasks: (1)
Evaluative Connotation, classifying idioms as positive or negative; (2)
Appropriateness, detecting incorrect idiom usage in context; and (3) Open
Cloze, filling blanks in longer passages without options. Chengyu-Bench
comprises 2,937 human-verified examples covering 1,765 common idioms sourced
from diverse corpora. We evaluate leading LLMs and find they achieve over 95%
accuracy on Evaluative Connotation, but only ~85% on Appropriateness and ~40%
top-1 accuracy on Open Cloze. Error analysis reveals that most mistakes arise
from fundamental misunderstandings of idiom meanings. Chengyu-Bench
demonstrates that while LLMs can reliably gauge idiom sentiment, they still
struggle to grasp the cultural and contextual nuances essential for proper
usage. The benchmark and source code are available at:
https://github.com/sofyc/ChengyuBench.

</details>


### [48] [Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives](https://arxiv.org/abs/2506.18116)
*Batool Haider,Atmika Gorti,Aman Chadha,Manas Gaur*

Key words: 大型语言模型, 心理健康, 偏见检测, 多跳问答, 去偏见

TL;DR: 该研究通过多跳问答框架（MHQA）检测大型语言模型（LLMs）在心理健康领域中的交叉偏见，发现系统性差异并提出了两种去偏见技术，效果显著。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究发现LLMs在心理健康领域可能传播偏见，加剧对边缘群体的伤害，但目前缺乏系统性检测方法。

Method: 采用多跳问答框架（MHQA）分析LLMs在心理健康话语中的偏见，并结合角色扮演模拟和显性偏见减少技术进行去偏见。

Result: 评估了四种LLMs，发现系统性偏见，去偏见技术实现了66-94%的偏见减少。

Conclusion: 研究揭示了LLMs在心理健康领域中的偏见问题，为开发公平AI提供了可操作的见解。

Abstract: Large Language Models (LLMs) in mental healthcare risk propagating biases
that reinforce stigma and harm marginalized groups. While previous research
identified concerning trends, systematic methods for detecting intersectional
biases remain limited. This work introduces a multi-hop question answering
(MHQA) framework to explore LLM response biases in mental health discourse. We
analyze content from the Interpretable Mental Health Instruction (IMHI) dataset
across symptom presentation, coping mechanisms, and treatment approaches. Using
systematic tagging across age, race, gender, and socioeconomic status, we
investigate bias patterns at demographic intersections. We evaluate four LLMs:
Claude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic
disparities across sentiment, demographics, and mental health conditions. Our
MHQA approach demonstrates superior detection compared to conventional methods,
identifying amplification points where biases magnify through sequential
reasoning. We implement two debiasing techniques: Roleplay Simulation and
Explicit Bias Reduction, achieving 66-94% bias reductions through few-shot
prompting with BBQ dataset examples. These findings highlight critical areas
where LLMs reproduce mental healthcare biases, providing actionable insights
for equitable AI development.

</details>


### [49] [The Syntactic Acceptability Dataset (Preview): A Resource for Machine Learning and Linguistic Analysis of English](https://arxiv.org/abs/2506.18120)
*Tom S Juzek*

Key words: 句法可接受性、数据集、语法性、可接受性、计算语言学

TL;DR: 摘要介绍了一个句法可接受性数据集，包含1000个英语序列，标有语法状态和可接受性状态，是目前最大的公开数据集。初步分析显示语法性和可接受性判断在83%的情况下一致，机器学习模型在预测可接受性方面表现更好。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为句法和计算语言学研究设计一个资源，填补公开数据集空缺。

Method: 数据集包含1000个英语序列，来自教科书和期刊，通过文献和众包标出语法性和可接受性。

Result: 语法性和可接受性在83%的情况下一致，机器学习模型预测可接受性更准确。

Conclusion: 该数据集是目前最大的公开资源，未来将扩展数据。

Abstract: We present a preview of the Syntactic Acceptability Dataset, a resource being
designed for both syntax and computational linguistics research. In its current
form, the dataset comprises 1,000 English sequences from the syntactic
discourse: Half from textbooks and half from the journal Linguistic Inquiry,
the latter to ensure a representation of the contemporary discourse. Each entry
is labeled with its grammatical status ("well-formedness" according to
syntactic formalisms) extracted from the literature, as well as its
acceptability status ("intuitive goodness" as determined by native speakers)
obtained through crowdsourcing, with highest experimental standards. Even in
its preliminary form, this dataset stands as the largest of its kind that is
publicly accessible. We also offer preliminary analyses addressing three
debates in linguistics and computational linguistics: We observe that
grammaticality and acceptability judgments converge in about 83% of the cases
and that "in-betweenness" occurs frequently. This corroborates existing
research. We also find that while machine learning models struggle with
predicting grammaticality, they perform considerably better in predicting
acceptability. This is a novel finding. Future work will focus on expanding the
dataset.

</details>


### [50] [$φ^{\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models](https://arxiv.org/abs/2506.18129)
*Bugra Kilictas,Faruk Alpay*

Key words: 自回归变换器、语义漂移、符号净化、嵌入矩阵、AI安全

TL;DR: 本文发现自回归变换器语言模型中的长破折号会引发递归语义漂移，导致从句边界幻觉和嵌入空间纠缠，并提出了一种结合符号净化与嵌入矩阵调整的解决方案。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在解决长破折号在语言模型中引发的递归语义漂移问题，提升生成内容的连贯性和安全性。

Method: 通过符号净化（phi-infinity算子）和嵌入矩阵调整，抑制问题标记并保障语义连贯性。

Result: 实验验证了方法在生成一致性和主题维持上的显著改进。

Conclusion: 本文为识别和缓解模型中的标记级漏洞提供了通用框架，提升了AI安全性和稳健性。

Abstract: We identify a critical vulnerability in autoregressive transformer language
models where the em dash token induces recursive semantic drift, leading to
clause boundary hallucination and embedding space entanglement. Through formal
analysis of token-level perturbations in semantic lattices, we demonstrate that
em dash insertion fundamentally alters the model's latent representations,
causing compounding errors in long-form generation. We propose a novel solution
combining symbolic clause purification via the phi-infinity operator with
targeted embedding matrix realignment. Our approach enables total suppression
of problematic tokens without requiring model retraining, while preserving
semantic coherence through fixed-point convergence guarantees. Experimental
validation shows significant improvements in generation consistency and topic
maintenance. This work establishes a general framework for identifying and
mitigating token-level vulnerabilities in foundation models, with immediate
implications for AI safety, model alignment, and robust deployment of large
language models in production environments. The methodology extends beyond
punctuation to address broader classes of recursive instabilities in neural
text generation systems.

</details>


### [51] [Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models](https://arxiv.org/abs/2506.18141)
*Ruixuan Deng,Xiaoyang Hu,Miles Gilberti,Shane Storks,Aman Taxali,Mike Angstadt,Chandra Sripada,Joyce Chai*

Key words: 大型语言模型, 稀疏自编码器, 语义组件, 模块化知识, 模型操控

TL;DR: 论文通过分析大型语言模型中的语义组件，揭示其模块化知识组织方式，并展示了如何高效操纵模型输出。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在理解大型语言模型中语义组件的结构及其对输出的影响，探索高效操控模型的方法。

Method: 使用稀疏自编码器（SAE）特征共激活技术，从少量提示中识别语义一致的网络组件，并通过消融和放大组件验证其作用。

Result: 消融或放大国家和关系组件会分别改变模型输出；国家组件主要出现在早期层，关系组件则集中在后期层。

Conclusion: 大型语言模型中的知识呈模块化分布，后期层的关系组件对输出具有更强的因果影响。

Abstract: We identify semantically coherent, context-consistent network components in
large language models (LLMs) using coactivation of sparse autoencoder (SAE)
features collected from just a handful of prompts. Focusing on country-relation
tasks, we show that ablating semantic components for countries and relations
changes model outputs in predictable ways, while amplifying these components
induces counterfactual responses. Notably, composing relation and country
components yields compound counterfactual outputs. We find that, whereas most
country components emerge from the very first layer, the more abstract relation
components are concentrated in later layers. Furthermore, within relation
components themselves, nodes from later layers tend to have a stronger causal
impact on model outputs. Overall, these findings suggest a modular organization
of knowledge within LLMs and advance methods for efficient, targeted model
manipulation.

</details>


### [52] [QuranMorph: Morphologically Annotated Quranic Corpus](https://arxiv.org/abs/2506.18148)
*Diyam Akra,Tymaa Hammouda,Mustafa Jarrar*

Key words: QuranMorph, 形态学标注, SAMA/Qabas标签集, 开源语料库

TL;DR: QuranMorph是一个为《古兰经》手动标注的形态学语料库，包含77,429个标记。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为《古兰经》提供高质量的手动标注语料，以支持语言研究和资源互联。

Method: 由三位语言专家手动进行词形还原和词性标注，结合Qabas词典数据库和SAMA/Qabas细粒度标签集。

Result: 语料库标注丰富（40种标签），并成功与其他语言资源互联。

Conclusion: QuranMorph是一个开源且高质量的标注语料库，可用于语言学研究。

Abstract: We present the QuranMorph corpus, a morphologically annotated corpus for the
Quran (77,429 tokens). Each token in the QuranMorph was manually lemmatized and
tagged with its part-of-speech by three expert linguists. The lemmatization
process utilized lemmas from Qabas, an Arabic lexicographic database linked
with 110 lexicons and corpora of 2 million tokens. The part-of-speech tagging
was performed using the fine-grained SAMA/Qabas tagset, which encompasses 40
tags. As shown in this paper, this rich lemmatization and POS tagset enabled
the QuranMorph corpus to be inter-linked with many linguistic resources. The
corpus is open-source and publicly available as part of the SinaLab resources
at (https://sina.birzeit.edu/quran)

</details>


### [53] [CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers](https://arxiv.org/abs/2506.18185)
*Zihan Liang,Ziwen Pan,Sumon Kanti Dey,Azra Ismail*

Key words: SMM4H-HeaRD 2025,失眠检测,食品安全事件,RoBERTa,GPT-4,数据增强

TL;DR: 论文介绍了在SMM4H-HeaRD 2025共享任务中针对失眠检测和食品安全事件提取的系统，重点分享了在Task 5 Subtask 1中的优异表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是开发高效的系统，通过先进的自然语言处理技术，解决临床笔记中失眠检测和新闻报道中食品安全事件提取的挑战。

Method: 方法包括使用RoBERTa等编码器模型，结合GPT-4进行数据增强，以及对任务的预处理、模型架构和子任务特定调整。

Result: 在Task 5 Subtask 1中取得了F1分数0.958的优异成绩，获得了第一名。

Conclusion: 研究表明，结合RoBERTa和GPT-4的方法在特定任务中表现出色，为类似任务提供了有效解决方案。

Abstract: This paper presents our system for the SMM4H-HeaRD 2025 shared tasks,
specifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2).
Task 4 focused on detecting mentions of insomnia in clinical notes, while Task
5 addressed the extraction of food safety events from news articles. We
participated in all subtasks and report key findings across them, with
particular emphasis on Task 5 Subtask 1, where our system achieved strong
performance-securing first place with an F1 score of 0.958 on the test set. To
attain this result, we employed encoder-based models (e.g., RoBERTa), alongside
GPT-4 for data augmentation. This paper outlines our approach, including
preprocessing, model architecture, and subtask-specific adaptations

</details>


### [54] [Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review](https://arxiv.org/abs/2506.18199)
*Bushra Asseri,Estabrag Abdelaziz,Areej Al-Wabil*

Key words: 大型语言模型, 文化偏见, 提示工程, 阿拉伯和穆斯林, 去偏策略

TL;DR: 该论文通过混合方法系统回顾了针对阿拉伯和穆斯林的文化偏见缓解策略，提出了五种提示工程方法，发现结构化多步流程效果最佳，但研究数量有限，呼吁未来研究关注适应性提示技术和补充去偏方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型在多个领域表现出色，但针对阿拉伯和穆斯林的文化偏见问题未得到充分研究，且缺乏专门的提示工程策略研究，亟需系统化的解决方案。

Method: 采用PRISMA指南和Kitchenham的系统回顾方法，分析了2021-2024年间8项关于偏见缓解策略的实证研究。

Result: 提出五种主要提示工程方法：文化提示、情感启动、自我去偏技术、结构化多步流程和参数优化连续提示。其中，结构化多步流程效果最好，偏见减少高达87.7%。

Conclusion: 提示工程能有效缓解文化偏见且无需模型参数访问，但研究数量有限，未来应开发适应性提示技术、专属评估资源并结合其他去偏方法。

Abstract: Large language models have demonstrated remarkable capabilities across
various domains, yet concerns about cultural bias - particularly towards Arabs
and Muslims - pose significant ethical challenges by perpetuating harmful
stereotypes and marginalization. Despite growing recognition of bias in LLMs,
prompt engineering strategies specifically addressing Arab and Muslim
representation remain understudied. This mixed-methods systematic review
examines such techniques, offering evidence-based guidance for researchers and
practitioners. Following PRISMA guidelines and Kitchenham's systematic review
methodology, we analyzed 8 empirical studies published between 2021-2024
investigating bias mitigation strategies. Our findings reveal five primary
prompt engineering approaches: cultural prompting, affective priming,
self-debiasing techniques, structured multi-step pipelines, and
parameter-optimized continuous prompts. Although all approaches show potential
for reducing bias, effectiveness varied substantially across studies and bias
types. Evidence suggests that certain bias types may be more resistant to
prompt-based mitigation than others. Structured multi-step pipelines
demonstrated the highest overall effectiveness, achieving up to 87.7% reduction
in bias, though they require greater technical expertise. Cultural prompting
offers broader accessibility with substantial effectiveness. These results
underscore the accessibility of prompt engineering for mitigating cultural bias
without requiring access to model parameters. The limited number of studies
identified highlights a significant research gap in this critical area. Future
research should focus on developing culturally adaptive prompting techniques,
creating Arab and Muslim-specific evaluation resources, and integrating prompt
engineering with complementary debiasing methods to address deeper stereotypes
while maintaining model utility.

</details>


### [55] [Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications](https://arxiv.org/abs/2506.18201)
*Bushra Asseri,Estabraq Abdelaziz,Maha Al Mogren,Tayef Alhefdhi,Areej Al-Wabil*

Key words: 情感识别, 多模态AI, 阿拉伯语, 文化敏感性, 教育技术

TL;DR: 论文研究了GPT-4o和Gemini 1.5 Pro在阿拉伯语儿童故事书插图情感识别中的表现，GPT-4o表现优于Gemini，但模型仍存在文化理解和情感分类局限。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 开发适用于阿拉伯语文化背景的情感识别AI技术，以支持教育工具的进步。

Method: 使用三种提示策略评估模型对75张阿拉伯故事书插图的情感识别能力，并与人类标注进行对比。

Result: GPT-4o在链式思考提示下表现最佳（F1得分59%），Gemini为43%；模型在文化相关的情感和叙事语境中表现较差。

Conclusion: 当前模型在文化理解方面存在局限，需开发更具文化敏感性的训练方法。

Abstract: Emotion recognition capabilities in multimodal AI systems are crucial for
developing culturally responsive educational technologies, yet remain
underexplored for Arabic language contexts where culturally appropriate
learning tools are critically needed. This study evaluates the emotion
recognition performance of two advanced multimodal large language models,
GPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook
illustrations. We assessed both models across three prompting strategies
(zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic
storybooks, comparing model predictions with human annotations based on
Plutchik's emotional framework. GPT-4o consistently outperformed Gemini across
all conditions, achieving the highest macro F1-score of 59% with
chain-of-thought prompting compared to Gemini's best performance of 43%. Error
analysis revealed systematic misclassification patterns, with valence
inversions accounting for 60.7% of errors, while both models struggled with
culturally nuanced emotions and ambiguous narrative contexts. These findings
highlight fundamental limitations in current models' cultural understanding and
emphasize the need for culturally sensitive training approaches to develop
effective emotion-aware educational technologies for Arabic-speaking learners.

</details>


### [56] [Enhancing Entity Aware Machine Translation with Multi-task Learning](https://arxiv.org/abs/2506.18318)
*An Trieu,Phuong Nguyen,Minh Le Nguyen*

Key words: 实体感知机器翻译,多任务学习,命名实体识别,SemEval

TL;DR: 论文提出了一种通过多任务学习优化命名实体识别和机器翻译性能的方法，以提升实体感知机器翻译任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 实体感知机器翻译(EAMT)由于缺乏相关翻译数据且处理上下文复杂，是一个复杂的自然语言处理任务。

Method: 采用多任务学习方法，同时优化命名实体识别和机器翻译两个子任务。

Result: 在SemEval 2025竞赛Task 2提供的数据集上进行了结果和分析。

Conclusion: 多任务学习方法有效提升了实体感知机器翻译的性能。

Abstract: Entity-aware machine translation (EAMT) is a complicated task in natural
language processing due to not only the shortage of translation data related to
the entities needed to translate but also the complexity in the context needed
to process while translating those entities. In this paper, we propose a method
that applies multi-task learning to optimize the performance of the two
subtasks named entity recognition and machine translation, which improves the
final performance of the Entity-aware machine translation task. The result and
analysis are performed on the dataset provided by the organizer of Task 2 of
the SemEval 2025 competition.

</details>


### [57] [TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance](https://arxiv.org/abs/2506.18337)
*Syed Mekael Wasti,Shou-Yi Hung,Christopher Collins,En-Shiun Annie Lee*

Key words: 机器翻译, 后编辑, 错误预测, HCI, 效率

TL;DR: TranslationCorrect是一个集成了机器翻译生成、错误预测和直观后编辑界面的框架，显著提高了翻译效率和用户满意度。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了解决机器翻译后编辑和研究数据收集工作流程低效和分散的问题。

Method: 结合NLLB模型生成翻译、XCOMET或LLM API预测错误，以及基于HCI原则的后编辑界面。

Result: 用户研究证实，TranslationCorrect显著提高了翻译效率和用户满意度。

Conclusion: TranslationCorrect为翻译和研究提供了高效、集成的解决方案。

Abstract: Machine translation (MT) post-editing and research data collection often rely
on inefficient, disconnected workflows. We introduce TranslationCorrect, an
integrated framework designed to streamline these tasks. TranslationCorrect
combines MT generation using models like NLLB, automated error prediction using
models like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive
post-editing interface within a single environment. Built with human-computer
interaction (HCI) principles in mind to minimize cognitive load, as confirmed
by a user study. For translators, it enables them to correct errors and batch
translate efficiently. For researchers, TranslationCorrect exports high-quality
span-based annotations in the Error Span Annotation (ESA) format, using an
error taxonomy inspired by Multidimensional Quality Metrics (MQM). These
outputs are compatible with state-of-the-art error detection models and
suitable for training MT or post-editing systems. Our user study confirms that
TranslationCorrect significantly improves translation efficiency and user
satisfaction over traditional annotation methods.

</details>


### [58] [Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs](https://arxiv.org/abs/2506.18341)
*Kang Chen,Mengdi Zhang,Yixin Cao*

Key words: 大型语言模型, 多语言学习, 推理效率, 数据效率

TL;DR: 本文研究了大型语言模型在测试时的扩展挑战，提出了一种新的多语言统一学习方法​​，通过解码干预策略提升模型性能和效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索多语言推理的多样性，并解决数据收集和推理效率的挑战。

Method: ​L²多语言统一学习结合解码干预策略，利用多语言数据进行微调提升推理能力。

Result: 多语言学习减少了所需数据和推理标记数量，同时保持了可比性能。

Conclusion: ​L²方法为解决数据收集和测试效率问题提供了有前景的方案。

Abstract: This paper explores the challenges of test-time scaling of large language
models (LLMs), regarding both the data and inference efficiency. We highlight
the diversity of multi-lingual reasoning based on our pilot studies, and then
introduce a novel approach, \(L^2\) multi-lingual unification learning with a
decoding intervention strategy for further investigation. The basic idea of
\(L^2\) is that the reasoning process varies across different languages, which
may be mutually beneficial to enhance both model performance and efficiency. In
specific, there are two types of multi-lingual data: the entire long
chain-of-thought annotations in different languages and the step-wise mixture
of languages. By further tuning based on them, we show that even small amounts
of data can significantly improve reasoning capabilities. Our findings suggest
that multilingual learning reduces both the required data and the number of
inference tokens while maintaining a comparable performance. Furthermore,
\(L^2\) is orthogonal to other data efficient methods. Thus, we also emphasize
the importance of diverse data selection. The \(L^2\) method offers a promising
solution to the challenges of data collection and test-time compute efficiency
in LLMs.

</details>


### [59] [Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics](https://arxiv.org/abs/2506.18387)
*Yousang Cho,Key-Sun Choi*

Key words: 评估指标、因果解释、自动诊断报告、临床推理、GPT-Black

TL;DR: 该研究比较了六种评估指标在自动生成诊断报告中捕捉因果解释质量的准确性，发现GPT-Black最具区分力，而基于相似性的指标与临床推理质量不符。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在评估不同指标在衡量自动生成诊断报告中因果解释质量时的有效性，以确定哪些指标能更好地反映临床推理的合理性。

Method: 比较了六种指标（BERTScore、Cosine Similarity、BioSentVec、GPT-White、GPT-Black和专家定性评估），应用于基于观察和多选输入的报告，并采用两种权重策略。

Result: GPT-Black在识别逻辑连贯且临床有效的因果叙述中表现最佳，GPT-White与专家评估一致，而相似性指标与临床推理质量偏离。

Conclusion: 指标选择和加权显著影响评估结果，支持在需要可解释性和因果推理的任务中使用基于LLM的评估方法。

Abstract: This study investigates how accurately different evaluation metrics capture
the quality of causal explanations in automatically generated diagnostic
reports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec,
GPT-White, GPT-Black, and expert qualitative assessment across two input types:
observation-based and multiple-choice-based report generation. Two weighting
strategies are applied: one reflecting task-specific priorities, and the other
assigning equal weights to all metrics. Our results show that GPT-Black
demonstrates the strongest discriminative power in identifying logically
coherent and clinically valid causal narratives. GPT-White also aligns well
with expert evaluations, while similarity-based metrics diverge from clinical
reasoning quality. These findings emphasize the impact of metric selection and
weighting on evaluation outcomes, supporting the use of LLM-based evaluation
for tasks requiring interpretability and causal reasoning.

</details>


### [60] [Lemmatization as a Classification Task: Results from Arabic across Multiple Genres](https://arxiv.org/abs/2506.18399)
*Mostafa Saeed,Nizar Habash*

Key words: 词形还原, 阿拉伯语, 机器翻译, 语义聚类, 序列到序列模型

TL;DR: 论文提出两种新方法将词形还原定义为分类任务，利用机器翻译和语义聚类，构建了新的阿拉伯语词形还原测试集，并评估了序列到序列模型的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决阿拉伯语等形态丰富语言中词形还原工具面临的标准不一致和文体覆盖有限的问题。

Method: 采用Lemma-POS-Gloss (LPG)标签集分类方法，结合机器翻译和语义聚类技术。

Result: 分类和聚类方法表现更稳健且可解释，优于序列到序列模型，后者存在生成不合理形式的局限性。

Conclusion: 分类和聚类方法为阿拉伯语词形还原设定了新基准，展示了更高的鲁棒性和解释性。

Abstract: Lemmatization is crucial for NLP tasks in morphologically rich languages with
ambiguous orthography like Arabic, but existing tools face challenges due to
inconsistent standards and limited genre coverage. This paper introduces two
novel approaches that frame lemmatization as classification into a
Lemma-POS-Gloss (LPG) tagset, leveraging machine translation and semantic
clustering. We also present a new Arabic lemmatization test set covering
diverse genres, standardized alongside existing datasets. We evaluate character
level sequence-to-sequence models, which perform competitively and offer
complementary value, but are limited to lemma prediction (not LPG) and prone to
hallucinating implausible forms. Our results show that classification and
clustering yield more robust, interpretable outputs, setting new benchmarks for
Arabic lemmatization.

</details>


### [61] [TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2506.18421)
*Ce Li,Xiaofan Liu,Zhiyan Song,Ce Chi,Chen Zhao,Jingjing Yang,Zhendong Wang,Kexin Yang,Boshen Shi,Xing Wang,Chao Deng,Junlan Feng*

Key words: 表格推理、LLMs、评估基准、TReB、数据集

TL;DR: 该论文针对大型语言模型（LLMs）在表格数据推理中缺乏有效评估基准的问题，提出了一个全面的表格推理评估基准TReB，包含26个子任务，并构建了高质量数据集和评估框架。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 企业数据多以表格形式存储，但LLMs在表格推理方面面临挑战，缺乏公平评估其性能的标准。

Method: 通过迭代数据处理构建高质量数据集，提出三种推理模式（TCoT、PoT、ICoT）的评估框架，并对20多种先进LLMs进行测试。

Result: 实验表明现有LLMs在复杂表格任务中仍有显著改进空间，数据集和框架已公开。

Conclusion: TReB填补了表格推理评估的空白，证明了其在衡量LLMs能力方面的有效性。

Abstract: The majority of data in businesses and industries is stored in tables,
databases, and data warehouses. Reasoning with table-structured data poses
significant challenges for large language models (LLMs) due to its hidden
semantics, inherent complexity, and structured nature. One of these challenges
is lacking an effective evaluation benchmark fairly reflecting the performances
of LLMs on broad table reasoning abilities. In this paper, we fill in this gap,
presenting a comprehensive table reasoning evolution benchmark, TReB, which
measures both shallow table understanding abilities and deep table reasoning
abilities, a total of 26 sub-tasks. We construct a high quality dataset through
an iterative data processing procedure. We create an evaluation framework to
robustly measure table reasoning capabilities with three distinct inference
modes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs
using this frame work and prove its effectiveness. Experimental results reveal
that existing LLMs still have significant room for improvement in addressing
the complex and real world Table related tasks. Both the dataset and evaluation
framework are publicly available, with the dataset hosted on [HuggingFace] and
the framework on [GitHub].

</details>


### [62] [MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models](https://arxiv.org/abs/2506.18485)
*Junjie Zhang,Guozheng Ma,Shunyu Liu,Haoyu Wang,Jiaxing Huang,Ting-En Lin,Fei Huang,Yongbin Li,Dacheng Tao*

Key words: 强化学习, 上下文学习, 大型语言模型, 推理任务, MeRF

TL;DR: 论文提出了一种结合上下文学习能力的强化学习方法MeRF，通过将奖励规范直接注入提示中，激励LLM生成更优输出，并在逻辑推理任务中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有RLVR方法忽略了LLM的上下文学习能力，论文旨在探索如何结合强化学习与上下文学习，以提升LLM的推理能力。

Method: MeRF方法通过将奖励规范注入提示中，作为上下文激励，引导模型生成符合优化目标的输出。

Result: 在K&K逻辑推理基准测试中，MeRF显著优于基线方法，且激励与奖励函数一致性越高性能越好。

Conclusion: MeRF成功结合强化学习与上下文学习，为LLM推理任务提供了一种简单有效的优化方法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle
complex reasoning tasks. However, existing RLVR methods overlook one of the
most distinctive capabilities of LLMs, their in-context learning ability, as
prominently demonstrated by the success of Chain-of-Thought (CoT) prompting.
This motivates us to explore how reinforcement learning can be effectively
combined with in-context learning to better improve the reasoning capabilities
of LLMs. In this paper, we introduce Motivation-enhanced Reinforcement
Finetuning} (MeRF), an intuitive yet effective method enhancing reinforcement
learning of LLMs by involving ``telling LLMs the rules of the game''.
Specifically, MeRF directly injects the reward specification into the prompt,
which serves as an in-context motivation for model to improve its responses
with awareness of the optimization objective. This simple modification
leverages the in-context learning ability of LLMs aligning generation with
optimization, thereby incentivizing the model to generate desired outputs from
both inner motivation and external reward. Empirical evaluations on the Knights
and Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that
\texttt{MeRF} achieves substantial performance gains over baselines. Moreover,
ablation studies show that performance improves with greater consistency
between the in-context motivation and the external reward function, while the
model also demonstrates an ability to adapt to misleading motivations through
reinforcement learning.

</details>


### [63] [Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance](https://arxiv.org/abs/2506.18501)
*Wael Etaiwi,Bushra Alhijawi*

Key words: 大语言模型, NLP任务评估, ChatGPT, DeepSeek, 性能比较

TL;DR: 对ChatGPT和DeepSeek在五个关键NLP任务中的表现进行了全面评估，发现DeepSeek在分类稳定性和逻辑推理方面表现优异，而ChatGPT在需要细腻理解和灵活性的任务中更胜一筹。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着大语言模型在NLP任务中的广泛应用，研究其在不同领域的效果、优势和局限性，以便针对任务需求选择合适的模型。

Method: 设计了结构化的实验协议，使用中性提示和两个基准数据集，评估ChatGPT和DeepSeek在情感分析、主题分类、文本摘要、机器翻译和文本蕴涵任务中的表现。

Result: DeepSeek在分类稳定性和逻辑推理上表现更好，而ChatGPT在需要更高灵活性和理解能力的任务中更具优势。

Conclusion: 研究结果为根据任务需求选择合适的大语言模型提供了实用指导。

Abstract: The increasing use of large language models (LLMs) in natural language
processing (NLP) tasks has sparked significant interest in evaluating their
effectiveness across diverse applications. While models like ChatGPT and
DeepSeek have shown strong results in many NLP domains, a comprehensive
evaluation is needed to understand their strengths, weaknesses, and
domain-specific abilities. This is critical as these models are applied to
various tasks, from sentiment analysis to more nuanced tasks like textual
entailment and translation. This study aims to evaluate ChatGPT and DeepSeek
across five key NLP tasks: sentiment analysis, topic classification, text
summarization, machine translation, and textual entailment. A structured
experimental protocol is used to ensure fairness and minimize variability. Both
models are tested with identical, neutral prompts and evaluated on two
benchmark datasets per task, covering domains like news, reviews, and
formal/informal texts. The results show that DeepSeek excels in classification
stability and logical reasoning, while ChatGPT performs better in tasks
requiring nuanced understanding and flexibility. These findings provide
valuable insights for selecting the appropriate LLM based on task requirements.

</details>


### [64] [End-to-End Spoken Grammatical Error Correction](https://arxiv.org/abs/2506.18532)
*Mengjie Qian,Rao Ma,Stefano Bannò,Mark J. F. Gales,Kate M. Knill*

Key words: 口语语法纠错（SGEC）、端到端（E2E）、伪标记、参考对齐、Whisper模型

TL;DR: 本文研究了口语语法纠错（SGEC）的端到端（E2E）框架，探讨了其挑战与解决方案，并通过伪标记和上下文信息提升性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 口语语法纠错（SGEC）对第二语言学习者、教育者和考官至关重要，但面临因不流畅和转录错误等带来的额外挑战。

Method: 比较了级联、部分级联和E2E架构，利用Whisper基础模型，并通过伪标记扩增训练数据（2500小时）。结合上下文信息和参考对齐过程提升反馈精度。

Result: 在Linguaskill和Speak & Improve语料上的实验表明，所提方法显著提升了E2E SGEC的性能。

Conclusion: 通过伪标记、上下文信息和参考对齐过程的结合，E2E SGEC系统的性能显著提升，为口语语法纠错提供了有效解决方案。

Abstract: Grammatical Error Correction (GEC) and feedback play a vital role in
supporting second language (L2) learners, educators, and examiners. While
written GEC is well-established, spoken GEC (SGEC), aiming to provide feedback
based on learners' speech, poses additional challenges due to disfluencies,
transcription errors, and the lack of structured input. SGEC systems typically
follow a cascaded pipeline consisting of Automatic Speech Recognition (ASR),
disfluency detection, and GEC, making them vulnerable to error propagation
across modules. This work examines an End-to-End (E2E) framework for SGEC and
feedback generation, highlighting challenges and possible solutions when
developing these systems. Cascaded, partial-cascaded and E2E architectures are
compared, all built on the Whisper foundation model. A challenge for E2E
systems is the scarcity of GEC labeled spoken data. To address this, an
automatic pseudo-labeling framework is examined, increasing the training data
from 77 to over 2500 hours. To improve the accuracy of the SGEC system,
additional contextual information, exploiting the ASR output, is investigated.
Candidate feedback of their mistakes is an essential step to improving
performance. In E2E systems the SGEC output must be compared with an estimate
of the fluent transcription to obtain the feedback. To improve the precision of
this feedback, a novel reference alignment process is proposed that aims to
remove hypothesised edits that results from fluent transcription errors.
Finally, these approaches are combined with an edit confidence estimation
approach, to exclude low-confidence edits. Experiments on the in-house
Linguaskill (LNG) corpora and the publicly available Speak & Improve (S&I)
corpus show that the proposed approaches significantly boost E2E SGEC
performance.

</details>


### [65] [When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking](https://arxiv.org/abs/2506.18535)
*Manu Pande,Shahil Kumar,Anay Yatin Damle*

Key words: 预训练模型, 微调, MS MARCO, 嵌入空间, 迁移学习

TL;DR: 研究发现，对预训练Transformer模型进行微调反而会降低在MS MARCO段落排序任务上的表现，所有微调方法均未超越基础模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探究为何微调预训练Transformer模型在MS MARCO任务上表现不佳，挑战现有的迁移学习观念。

Method: 通过五种模型变体（包括全参数微调和LoRA适配）进行实验，结合UMAP可视化和训练动态分析。

Result: 微调破坏了预训练模型学习的嵌入空间结构，导致性能下降（MRR@10: 0.3026）。

Conclusion: 传统微调方法在饱和基准测试上可能无效，需进行架构创新以实现改进。

Abstract: This paper investigates the counterintuitive phenomenon where fine-tuning
pre-trained transformer models degrades performance on the MS MARCO passage
ranking task. Through comprehensive experiments involving five model
variants-including full parameter fine-tuning and parameter efficient LoRA
adaptations-we demonstrate that all fine-tuning approaches underperform the
base sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our
analysis reveals that fine-tuning disrupts the optimal embedding space
structure learned during the base model's extensive pre-training on 1 billion
sentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations
show progressive embedding space flattening, while training dynamics analysis
and computational efficiency metrics further support our findings. These
results challenge conventional wisdom about transfer learning effectiveness on
saturated benchmarks and suggest architectural innovations may be necessary for
meaningful improvements.

</details>


### [66] [A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot LLM Classification Performance](https://arxiv.org/abs/2506.18576)
*Matteo Melis,Gabriella Lapesa,Dennis Assenmacher*

Key words: 仇恨言论, NLP, 零样本评估, 概念元素, LLM

TL;DR: 论文研究了仇恨言论的定义如何影响模型性能，通过理论分类和实验评估LLM的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 仇恨言论是NLP中危害性最大的内容之一，但定义模糊且影响模型效果的问题尚未解决。

Method: 收集并分析现有仇恨言论定义，构建包含14个概念元素的分类体系；对三个LLM进行零样本评估。

Result: 不同定义（尤其是特异性程度）会影响模型性能，但效果因模型架构而异。

Conclusion: 明确仇恨言论的定义对模型性能至关重要，但需考虑模型架构的差异。

Abstract: Detecting harmful content is a crucial task in the landscape of NLP
applications for Social Good, with hate speech being one of its most dangerous
forms. But what do we mean by hate speech, how can we define it, and how does
prompting different definitions of hate speech affect model performance? The
contribution of this work is twofold. At the theoretical level, we address the
ambiguity surrounding hate speech by collecting and analyzing existing
definitions from the literature. We organize these definitions into a taxonomy
of 14 Conceptual Elements-building blocks that capture different aspects of
hate speech definitions, such as references to the target of hate (individual
or groups) or of the potential consequences of it. At the experimental level,
we employ the collection of definitions in a systematic zero-shot evaluation of
three LLMs, on three hate speech datasets representing different types of data
(synthetic, human-in-the-loop, and real-world). We find that choosing different
definitions, i.e., definitions with a different degree of specificity in terms
of encoded elements, impacts model performance, but this effect is not
consistent across all architectures.

</details>


### [67] [Parallel Continuous Chain-of-Thought with Jacobi Iteration](https://arxiv.org/abs/2506.18582)
*Haoyi Wu,Zhihao Teng,Kewei Tu*

Key words: 连续思维链,并行训练,雅可比迭代,大语言模型,推理效率

TL;DR: 该论文提出了一种名为并行连续思维链（PCCoT）的方法，通过并行更新潜在思维令牌来提升连续思维链的训练和推理效率，并保持或优于原有性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 连续的思维链方法在节省大语言模型的推理令牌方面有效，但其顺序依赖性导致训练时间较长，影响了效率。

Method: 提出PCCoT，采用雅可比迭代法并行更新潜在思维令牌，而非顺序更新，从而提高训练和推理效率。

Result: 实验显示，PCCoT在适当迭代次数下能保持或优于性能，同时节省近50%的训练和推理时间，且训练过程更稳定。

Conclusion: PCCoT通过并行化显著提升了连续思维链的效率，同时保持了性能，为大语言模型的训练和推理提供了更优方案。

Abstract: Continuous chain-of-thought has been shown to be effective in saving
reasoning tokens for large language models. By reasoning with continuous latent
thought tokens, continuous CoT is able to perform implicit reasoning in a
compact manner. However, the sequential dependencies between latent thought
tokens spoil parallel training, leading to long training time. In this paper,
we propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi
iteration on the latent thought tokens, updating them iteratively in parallel
instead of sequentially and thus improving both training and inference
efficiency of continuous CoT. Experiments demonstrate that by choosing the
proper number of iterations, we are able to achieve comparable or even better
performance while saving nearly 50% of the training and inference time.
Moreover, PCCoT shows better stability and robustness in the training process.
Our code is available at https://github.com/whyNLP/PCCoT.

</details>


### [68] [Reply to "Emergent LLM behaviors are observationally equivalent to data leakage"](https://arxiv.org/abs/2506.18600)
*Ariel Flint Ashery,Luca Maria Aiello,Andrea Baronchelli*

Key words: 大语言模型, 数据污染, 涌现动态, 自组织, 社会规范

TL;DR: 论文指出，尽管数据污染可能会影响大语言模型（LLM）的模拟结果，但这并不妨碍研究LLM群体中的自组织和模型依赖的涌现动态。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨数据污染对大语言模型群体模拟的影响，并澄清即便存在数据污染，仍可以研究真正的涌现动态。

Method: 通过分析文献中的批评（Barrie和Törnberg对Flint Ashery等人的结果的批评），论证自组织和模型依赖的涌现动态可被研究。

Result: 研究发现，在特定案例（如社会规范）中，涌现动态已经被实证观察到。

Conclusion: 数据污染虽然是一个潜在问题，但不会完全阻碍对LLM群体中真正涌现动态的研究。

Abstract: A potential concern when simulating populations of large language models
(LLMs) is data contamination, i.e. the possibility that training data may shape
outcomes in unintended ways. While this concern is important and may hinder
certain experiments with multi-agent models, it does not preclude the study of
genuinely emergent dynamics in LLM populations. The recent critique by Barrie
and T\"ornberg [1] of the results of Flint Ashery et al. [2] offers an
opportunity to clarify that self-organisation and model-dependent emergent
dynamics can be studied in LLM populations, highlighting how such dynamics have
been empirically observed in the specific case of social conventions.

</details>


### [69] [Semantic similarity estimation for domain specific data using BERT and other techniques](https://arxiv.org/abs/2506.18602)
*R. Prashanth*

Key words: 语义相似度、自然语言处理、BERT、USE、InferSent

TL;DR: 该论文研究了语义相似度估计问题，比较了USE、InferSent和BERT等先进技术，发现BERT在特定领域数据上表现最佳。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 语义相似度估计在自然语言处理中有广泛应用，如问答系统和机器翻译。论文旨在比较不同技术在该任务上的表现。

Method: 使用了USE、InferSent和BERT三种技术，并基于特定领域内数据集和公开数据集（Quora问题对）进行分析。

Result: BERT表现最优，推测是由于其微调过程能更好地学习训练数据的模式。

Conclusion: BERT是处理特定领域数据的最佳选择。

Abstract: Estimation of semantic similarity is an important research problem both in
natural language processing and the natural language understanding, and that
has tremendous application on various downstream tasks such as question
answering, semantic search, information retrieval, document clustering,
word-sense disambiguation and machine translation. In this work, we carry out
the estimation of semantic similarity using different state-of-the-art
techniques including the USE (Universal Sentence Encoder), InferSent and the
most recent BERT, or Bidirectional Encoder Representations from Transformers,
models. We use two question pairs datasets for the analysis, one is a domain
specific in-house dataset and the other is a public dataset which is the
Quora's question pairs dataset. We observe that the BERT model gave much
superior performance as compared to the other methods. This should be because
of the fine-tuning procedure that is involved in its training process, allowing
it to learn patterns based on the training data that is used. This works
demonstrates the applicability of BERT on domain specific datasets. We infer
from the analysis that BERT is the best technique to use in the case of domain
specific data.

</details>


### [70] [The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches](https://arxiv.org/abs/2506.18621)
*Alisa Barkar,Mathieu Chollet,Matthieu Labeau,Beatrice Biancardi,Chloe Clavel*

Key words: 大型语言模型,说服力,修辞手法,GPT-4o,语言风格

TL;DR: 该研究探讨了大型语言模型如何通过修改法语3MT数据集中的演讲内容来理解说服力，提出了新的方法和可解释的文本特征集，分析了GPT-4o在增强或减弱说服力中的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究的动机是探究大型语言模型（如GPT-4o）是否能够像人类一样优化演讲的说服力，并揭示其在语言风格上的系统性调整方式。

Method: 方法包括修改PhD候选人的演讲内容，使用新的可解释文本特征集（如修辞手法和话语标记），并通过GPT-4o生成不同说服力水平的文本。

Result: 结果显示，GPT-4o并非以人类方式优化说服力，而是通过系统性地调整情感词汇和句式（如疑问句和感叹句）来增强修辞效果。

Conclusion: GPT-4o在说服力调整上表现出系统性的风格修改，而非人类化的优化策略，其对情感和句法的调整显著影响了演讲的修辞效果。

Abstract: This study examines how large language models understand the concept of
persuasiveness in public speaking by modifying speech transcripts from PhD
candidates in the "Ma These en 180 Secondes" competition, using the 3MT French
dataset. Our contributions include a novel methodology and an interpretable
textual feature set integrating rhetorical devices and discourse markers. We
prompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic
shifts between original and generated speech in terms of the new features.
Results indicate that GPT-4o applies systematic stylistic modifications rather
than optimizing persuasiveness in a human-like manner. Notably, it manipulates
emotional lexicon and syntactic structures (such as interrogative and
exclamatory clauses) to amplify rhetorical impact.

</details>


### [71] [ByteSpan: Information-Driven Subword Tokenisation](https://arxiv.org/abs/2506.18639)
*Zébulon Goriely,Suchir Salhan,Pietro Lesci,Julius Cheng,Paula Buttery*

Key words: 子词分词器, 字节级语言模型, 信息驱动, 形态对齐

TL;DR: ByteSpan是一种新型的基于信息驱动的子词分词器，通过分组可预测的字节序列生成高效的子词词汇表。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 受动态分词方法与词汇分割计算模型的启发，探索通过分组可预测字节而非仅合并其表示来生成有用的固定子词词汇表。

Method: 利用外部的字节级语言模型在训练过程中识别连续的、可预测的字节序列，并将其分组为子词。

Result: 实验表明，ByteSpan生成的词汇表在英语中具有比BPE更高的形态对齐分数；在25种语言中表现出相似的压缩效率和Rényi效率。

Conclusion: ByteSpan通过信息驱动方法实现了高效的子词分词，适用于多语言场景。

Abstract: Recent dynamic tokenisation methods operate directly on bytes and pool their
latent representations into patches. This bears similarities to computational
models of word segmentation that determine lexical boundaries using spikes in
an autoregressive model's prediction error. Inspired by this connection, we
explore whether grouping predictable bytes - rather than pooling their
representations - can yield a useful fixed subword vocabulary. We propose a new
information-driven subword tokeniser, ByteSpan, that uses an external
byte-level LM during training to identify contiguous predictable byte sequences
and group them into subwords. Experiments show that ByteSpan yields efficient
vocabularies with higher morphological alignment scores than BPE for English.
Multilingual experiments show similar compression and R\'enyi efficiency for 25
languages.

</details>


### [72] [Is There a Case for Conversation Optimized Tokenizers in Large Language Models?](https://arxiv.org/abs/2506.18674)
*Raquel Ferrando,Javier Conde,Gonzalo Martínez,Pedro Reviriego*

Key words: 大型语言模型, 分词器优化, 聊天机器人, 能源效率, 令牌数量

TL;DR: 论文探讨了为聊天机器人对话优化分词器的潜在好处，通过重新设计分词器词汇表并在聊天对话中评估其性能，发现优化后的分词器可以减少令牌数量，带来5%到10%的能源节约。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型（LLM）的计算和能源成本因模型规模和用户数量激增而急剧增加。分词器在模型效率中起关键作用，但目前优化主要针对训练语料，而非用户输入与聊天机器人响应的对话场景。

Method: 使用公开的聊天机器人对话语料库重新设计不同分词器的词汇表，并评估其在对话领域中的性能表现。

Result: 优化的分词器在聊天对话中能减少令牌数量，带来5%到10%的能源节约，同时对原始训练语料的分词效率影响极小或略有提升。

Conclusion: 为特定应用场景（如聊天机器人）优化分词器可以显著提高能源效率，具有实际应用价值。

Abstract: The computational and energy costs of Large Language Models (LLMs) have
increased exponentially driven by the growing model sizes and the massive
adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is
the computation of a token. Therefore, the tokenizer plays an important role in
the efficiency of a model, and they are carefully optimized to minimize the
number of tokens for the text in their training corpus. One of the most popular
applications of LLMs are chatbots that interact with users. A key observation
is that, for those chatbots, what is important is the performance of the
tokenizer in the user text input and the chatbot responses. Those are most
likely different from the text in the training corpus. So, a question that
immediately arises is whether there is a potential benefit in optimizing
tokenizers for chatbot conversations. In this paper, this idea is explored for
different tokenizers by using a publicly available corpus of chatbot
conversations to redesign their vocabularies and evaluate their performance in
this domain. The results show that conversation-optimized tokenizers
consistently reduce the number of tokens in chatbot dialogues, which can lead
to meaningful energy savings, in the range of 5% to 10% while having minimal or
even slightly positive impact on tokenization efficiency for the original
training corpus.

</details>


### [73] [Context Biasing for Pronunciations-Orthography Mismatch in Automatic Speech Recognition](https://arxiv.org/abs/2506.18703)
*Christian Huber,Alexander Waibel*

Key words: 自动语音识别, 神经序列到序列系统, 上下文偏置, 发音-拼写不匹配, 动态纠正

TL;DR: 该论文提出了一种允许在推断过程中动态纠正替换错误的方法，以提高具有发音-拼写不匹配特征的单词的识别准确性，从而改善了未见过单词的识别性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 虽然神经序列到序列系统在自动语音识别中表现优异，但在处理训练中未见的单词（如命名实体、缩写或领域特定词汇）时容易失败。现有的上下文偏置方法对这些单词的发音-拼写不匹配问题仍然效果有限。

Method: 提出了一种允许用户在推断过程中动态添加纠正的方法，以修正替换错误，从而提高具有发音-拼写不匹配特征的单词的识别准确性。

Result: 该方法在偏置词错误率上实现了高达11%的相对提升，同时保持了具有竞争力的整体词错误率。

Conclusion: 通过动态纠正替换错误，该方法有效提升了未见过单词的识别性能，尤其是在发音-拼写不匹配的情况下。

Abstract: Neural sequence-to-sequence systems deliver state-of-the-art performance for
automatic speech recognition. When using appropriate modeling units, e.g.,
byte-pair encoded characters, these systems are in principal open vocabulary
systems. In practice, however, they often fail to recognize words not seen
during training, e.g., named entities, acronyms, or domain-specific special
words. To address this problem, many context biasing methods have been
proposed; however, for words with a pronunciation-orthography mismatch, these
methods may still struggle. We propose a method which allows corrections of
substitution errors to improve the recognition accuracy of such challenging
words. Users can add corrections on the fly during inference. We show that with
this method we get a relative improvement in biased word error rate of up to
11\%, while maintaining a competitive overall word error rate.

</details>


### [74] [Benchmarking the Pedagogical Knowledge of Large Language Models](https://arxiv.org/abs/2506.18710)
*Maxime Lelièvre,Amy Waldock,Meng Liu,Natalia Valdés Aspillaga,Alasdair Mackintosh,María José Ogando Portelo,Jared Lee,Paul Atherton,Robin A. A. Ince,Oliver G. B. Garrod*

Key words: 教学法基准，跨领域教学知识，特殊教育需求与残疾，大型语言模型，教育评估

TL;DR: 该论文介绍了“教学法基准”，一个评估大型语言模型在跨领域教学知识（CDPK）和特殊教育需求与残疾（SEND）教学知识上的新数据集，填补了现有基准在评估教学理解方面的空白。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基准主要关注内容知识，而忽视了教学法的评估。本文旨在通过新数据集填补这一空白，评估模型的教学理解能力及其在教育中的应用潜力。

Method: 通过从教师专业发展考试中精心挑选问题，构建了涵盖教学策略和评估方法等子领域的基准数据集，并评估了97个模型的准确性。建立了在线排行榜，支持交互式探索和模型性能分析。

Result: 模型在教学知识问题上的准确率范围为28%至89%。论文分析了成本与准确性之间的关系，并展示了帕累托前沿的进展。在线排行榜提供了动态更新和多维度分析。

Conclusion: 教学法基准有助于衡量模型在教学概念理解、学习者需求响应和多样化教学实践中的应用能力，为教育中大型语言模型的负责任部署提供依据。

Abstract: Benchmarks like Massive Multitask Language Understanding (MMLU) have played a
pivotal role in evaluating AI's knowledge and abilities across diverse domains.
However, existing benchmarks predominantly focus on content knowledge, leaving
a critical gap in assessing models' understanding of pedagogy - the method and
practice of teaching. This paper introduces The Pedagogy Benchmark, a novel
dataset designed to evaluate large language models on their Cross-Domain
Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)
pedagogical knowledge. These benchmarks are built on a carefully curated set of
questions sourced from professional development exams for teachers, which cover
a range of pedagogical subdomains such as teaching strategies and assessment
methods. Here we outline the methodology and development of these benchmarks.
We report results for 97 models, with accuracies spanning a range from 28% to
89% on the pedagogical knowledge questions. We consider the relationship
between cost and accuracy and chart the progression of the Pareto value
frontier over time. We provide online leaderboards at
https://rebrand.ly/pedagogy which are updated with new models and allow
interactive exploration and filtering based on various model properties, such
as cost per token and open-vs-closed weights, as well as looking at performance
in different subjects. LLMs and generative AI have tremendous potential to
influence education and help to address the global learning crisis.
Education-focused benchmarks are crucial to measure models' capacities to
understand pedagogical concepts, respond appropriately to learners' needs, and
support effective teaching practices across diverse contexts. They are needed
for informing the responsible and evidence-based deployment of LLMs and
LLM-based tools in educational settings, and for guiding both development and
policy decisions.

</details>


### [75] [Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach](https://arxiv.org/abs/2506.18756)
*Chong Zhang,Xiang Li,Jia Wang,Shan Liang,Haochen Xue,Xiaobo Jin*

Key words: 大型语言模型, 提示优化, 自适应贪婪二分搜索, 语义稳定性, 对抗样本

TL;DR: 论文提出了一种名为AGBS的方法，通过自适应贪婪二分搜索平衡提示优化的语义一致性和攻击效果，提升了LLM的可靠性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前大型语言模型(LLM)在自动提示优化中容易因用户需求多样性而导致误解释和错误输出，亟需一种能保持语义稳定的方法。

Method: 提出了自适应贪婪二分搜索(AGBS)方法，动态评估提示优化策略对LLM性能的影响，同时生成对抗样本。

Result: 实验表明AGBS能有效平衡语义一致性和攻击效果，提升了提示优化系统的可靠性。

Conclusion: AGBS为设计更可靠的提示优化系统提供了实用见解，代码已开源。

Abstract: Large Language Models (LLMs) increasingly rely on automatic prompt
engineering in graphical user interfaces (GUIs) to refine user inputs and
enhance response accuracy. However, the diversity of user requirements often
leads to unintended misinterpretations, where automated optimizations distort
original intentions and produce erroneous outputs. To address this challenge,
we propose the Adaptive Greedy Binary Search (AGBS) method, which simulates
common prompt optimization mechanisms while preserving semantic stability. Our
approach dynamically evaluates the impact of such strategies on LLM
performance, enabling robust adversarial sample generation. Through extensive
experiments on open and closed-source LLMs, we demonstrate AGBS's effectiveness
in balancing semantic consistency and attack efficacy. Our findings offer
actionable insights for designing more reliable prompt optimization systems.
Code is available at: https://github.com/franz-chang/DOBS

</details>


### [76] [ASP2LJ : An Adversarial Self-Play Laywer Augmented Legal Judgment Framework](https://arxiv.org/abs/2506.18768)
*Ao Chang,Tong Zhou,Yubo Chen,Delai Qiu,Shengping Liu,Kang Liu,Jun Zhao*

Key words: 法律判决预测,长尾分布,对抗自博弈,ASP2LJ,RareCases

TL;DR: 提出了ASP2LJ框架，通过对抗自博弈增强律师论据生成以及生成模块解决长尾分布问题，并引入了RareCases稀有案例数据集。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 法律判决预测面临长尾分布和律师作用被忽视的问题，作者旨在提升判决的客观性与公平性。

Method: 结合对抗自博弈机制和案例生成模块，提出了ASP2LJ框架，并构建了RareCases数据集。

Result: 在SimuCourt和RareCases数据集上验证了框架的有效性。

Conclusion: ASP2LJ提升了判决质量，贡献了框架、数据集及公开代码。

Abstract: Legal Judgment Prediction (LJP) aims to predict judicial outcomes, including
relevant legal charge, terms, and fines, which is a crucial process in Large
Language Model(LLM). However, LJP faces two key challenges: (1)Long Tail
Distribution: Current datasets, derived from authentic cases, suffer from high
human annotation costs and imbalanced distributions, leading to model
performance degradation. (2)Lawyer's Improvement: Existing systems focus on
enhancing judges' decision-making but neglect the critical role of lawyers in
refining arguments, which limits overall judicial accuracy. To address these
issues, we propose an Adversarial Self-Play Lawyer Augmented Legal Judgment
Framework, called ASP2LJ, which integrates a case generation module to tackle
long-tailed data distributions and an adversarial self-play mechanism to
enhance lawyers' argumentation skills. Our framework enables a judge to
reference evolved lawyers' arguments, improving the objectivity, fairness, and
rationality of judicial decisions. Besides, We also introduce RareCases, a
dataset for rare legal cases in China, which contains 120 tail-end cases. We
demonstrate the effectiveness of our approach on the SimuCourt dataset and our
RareCases dataset. Experimental results show our framework brings improvements,
indicating its utilization. Our contributions include an integrated framework,
a rare-case dataset, and publicly releasing datasets and code to support
further research in automated judicial systems.

</details>


### [77] [Existing LLMs Are Not Self-Consistent For Simple Tasks](https://arxiv.org/abs/2506.18781)
*Zhenru Lin,Jiawen Tao,Yang Yuan,Andrew Chi-Chih Yao*

Key words: 大语言模型,自洽性,不一致性,量化方法,可靠AI

TL;DR: 研究发现大语言模型在简单任务中存在不一致性，提出了量化方法和两种自动修复方法，但完全解决仍需努力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 确保大语言模型的决策透明且可信，需解决其内部推理的自洽性问题。

Method: 引入不一致性指标，并提出基于图和能量的两种自动修复方法。

Result: 即使先进模型也未完全自洽，修复方法仅部分有效。

Conclusion: 自洽性对构建可靠、可解释AI至关重要，但完全解决仍具挑战性。

Abstract: Large Language Models (LLMs) have grown increasingly powerful, yet ensuring
their decisions remain transparent and trustworthy requires self-consistency --
no contradictions in their internal reasoning. Our study reveals that even on
simple tasks, such as comparing points on a line or a plane, or reasoning in a
family tree, all smaller models are highly inconsistent, and even
state-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully
self-consistent. To quantify and mitigate these inconsistencies, we introduce
inconsistency metrics and propose two automated methods -- a graph-based and an
energy-based approach. While these fixes provide partial improvements, they
also highlight the complexity and importance of self-consistency in building
more reliable and interpretable AI. The code and data are available at
https://github.com/scorpio-nova/llm-self-consistency.

</details>


### [78] [RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies](https://arxiv.org/abs/2506.18819)
*Arjun Mukerji,Michael L. Jackson,Jason Jones,Neil Sanghavi*

Key words: 大型语言模型,现实世界证据,医学研究,摘要任务,RWESummary

TL;DR: 该论文介绍了RWESummary，一个用于评估大型语言模型在现实世界证据研究摘要任务中的表现的新基准。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 目前大型语言模型在医学研究辅助任务中已有评估，但尚未专门针对现实世界证据研究的结构化输出摘要任务进行评估。

Method: 通过开发RWESummary基准，并将其纳入MedHELM框架，评估不同语言模型的表现。

Result: 在13个现实世界证据研究中，Gemini 2.5模型整体表现最佳。

Conclusion: RWESummary为现实世界证据研究的摘要任务提供了一个新颖且有价值的基础模型基准。

Abstract: Large Language Models (LLMs) have been extensively evaluated for general
summarization tasks as well as medical research assistance, but they have not
been specifically evaluated for the task of summarizing real-world evidence
(RWE) from structured output of RWE studies. We introduce RWESummary, a
proposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al.,
2025) to enable benchmarking of LLMs for this task. RWESummary includes one
scenario and three evaluations covering major types of errors observed in
summarization of medical research studies and was developed using Atropos
Health proprietary data. Additionally, we use RWESummary to compare the
performance of different LLMs in our internal RWE summarization tool. At the
time of publication, with 13 distinct RWE studies, we found the Gemini 2.5
models performed best overall (both Flash and Pro). We suggest RWESummary as a
novel and useful foundation model benchmark for real-world evidence study
summarization.

</details>


### [79] [MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task](https://arxiv.org/abs/2506.18828)
*Jorge Iranzo-Sánchez,Javier Iranzo-Sánchez,Adrià Giménez,Jorge Civera,Alfons Juan*

Key words: 实时语音翻译, 预训练模型, 模块化系统, 延迟优化

TL;DR: 该研究介绍了一种模块化级联系统，用于实时长语音翻译，通过适应预训练模型提升性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决实时长语音翻译的独特挑战，利用预训练模型避免从头训练。

Method: 结合Whisper ASR和NLLB MT模型，采用轻量级适应技术和自适应策略。

Result: 在ACL60/60数据集上BLEU得分31.96，延迟2.94秒，测试集得分29.8 BLEU。

Conclusion: 预训练模型的轻量级适应可有效实现实时长语音翻译，无需大量领域数据或端到端训练。

Abstract: This work describes the participation of the MLLP-VRAIN research group in the
shared task of the IWSLT 2025 Simultaneous Speech Translation track. Our
submission addresses the unique challenges of real-time translation of
long-form speech by developing a modular cascade system that adapts strong
pre-trained models to streaming scenarios. We combine Whisper Large-V3-Turbo
for ASR with the multilingual NLLB-3.3B model for MT, implementing lightweight
adaptation techniques rather than training new end-to-end models from scratch.
Our approach employs document-level adaptation with prefix training to enhance
the MT model's ability to handle incomplete inputs, while incorporating
adaptive emission policies including a wait-$k$ strategy and RALCP for managing
the translation stream. Specialized buffer management techniques and
segmentation strategies ensure coherent translations across long audio
sequences. Experimental results on the ACL60/60 dataset demonstrate that our
system achieves a favorable balance between translation quality and latency,
with a BLEU score of 31.96 and non-computational-aware StreamLAAL latency of
2.94 seconds. Our final model achieves a preliminary score on the official test
set (IWSLT25Instruct) of 29.8 BLEU. Our work demonstrates that carefully
adapted pre-trained components can create effective simultaneous translation
systems for long-form content without requiring extensive in-domain parallel
data or specialized end-to-end training.

</details>


### [80] [STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2506.18831)
*Aryasomayajula Ram Bharadwaj*

Key words: 大语言模型、思维链推理、冗余推理、PID控制器、动态校准

TL;DR: STUPID是一种无需训练的方法，通过PID控制器动态调整激活引导强度，减少冗余推理步骤，提高计算效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决大语言模型在扩展思维链推理时的过度思考问题，生成冗余推理步骤增加计算成本并可能降低性能。

Method: 结合块级分类器和PID控制机制，动态调整引导强度，基于预测的冗余概率。

Result: 在GSM8K上，准确率提高6%，令牌使用减少32%。

Conclusion: STUPID提供了一种动态推理校准框架，在保持推理质量的同时显著提高计算效率。

Abstract: Large Language Models employing extended chain-of-thought (CoT) reasoning
often suffer from the overthinking phenomenon, generating excessive and
redundant reasoning steps that increase computational costs while potentially
degrading performance. While recent work has explored static steering
approaches to mitigate this issue, they lack the adaptability to dynamically
adjust intervention strength based on real-time reasoning quality. We propose
STUPID (Steering Token Usage via PID controller), a novel training-free method
that employs a PID controller to dynamically modulate activation steering
strength during inference. Our approach combines a chunk-level classifier for
detecting redundant reasoning patterns with a PID control mechanism that
adaptively adjusts steering intensity based on the predicted redundancy
probability. Experimental evaluation on GSM8K demonstrates that STUPID achieves
a 6% improvement in accuracy while reducing token usage by 32%, outperforming
static steering baselines. Our method provides a principled framework for
dynamic reasoning calibration that maintains reasoning quality while
significantly improving computational efficiency.

</details>


### [81] [LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning](https://arxiv.org/abs/2506.18841)
*Yuhao Wu,Yushi Bai,Zhiqiang Hu,Roy Ka-Wei Lee,Juanzi Li*

Key words: LLMs, 长文本生成, 强化学习, LongWriter-Zero, 奖励模型

TL;DR: 本文提出了一种基于强化学习的激励方法，无需依赖标注或合成数据，实现了LLMs的超长高质量文本生成能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决LLMs在超长文本生成中的长度限制和质量退化问题。

Method: 利用强化学习训练基础模型，通过专门奖励模型控制长度、质量和结构。

Result: LongWriter-Zero模型在长文本写作任务中表现优异，超越了传统SFT方法和100B+模型。

Conclusion: 激励方法有效提升了LLMs的长文本生成能力，且无需依赖合成数据。

Abstract: Ultra-long generation by large language models (LLMs) is a widely demanded
scenario, yet it remains a significant challenge due to their maximum
generation length limit and overall quality degradation as sequence length
increases. Previous approaches, exemplified by LongWriter, typically rely on
''teaching'', which involves supervised fine-tuning (SFT) on synthetic
long-form outputs. However, this strategy heavily depends on synthetic SFT
data, which is difficult and costly to construct, often lacks coherence and
consistency, and tends to be overly artificial and structurally monotonous. In
this work, we propose an incentivization-based approach that, starting entirely
from scratch and without relying on any annotated or synthetic data, leverages
reinforcement learning (RL) to foster the emergence of ultra-long, high-quality
text generation capabilities in LLMs. We perform RL training starting from a
base model, similar to R1-Zero, guiding it to engage in reasoning that
facilitates planning and refinement during the writing process. To support
this, we employ specialized reward models that steer the LLM towards improved
length control, writing quality, and structural formatting. Experimental
evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,
consistently outperforms traditional SFT methods on long-form writing tasks,
achieving state-of-the-art results across all metrics on WritingBench and
Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and
Qwen3-235B. We open-source our data and model checkpoints under
https://huggingface.co/THU-KEG/LongWriter-Zero-32B

</details>


### [82] [Mechanistic Interpretability Needs Philosophy](https://arxiv.org/abs/2506.18852)
*Iwan Williams,Ninell Oldenburg,Ruchira Dhar,Joshua Hatherley,Constanza Fierro,Nina Rajcic,Sandrine R. Schiller,Filippos Stamatiou,Anders Søgaard*

Key words: 机制可解释性,哲学,跨学科,认知伦理

TL;DR: 本文主张机制可解释性（MI）研究需要结合哲学，以澄清概念、改进方法并评估解释AI系统的认知和伦理影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着MI研究影响力的增长，需审视其隐含的假设、概念和解释策略，而哲学能为这一过程提供持续支持。

Method: 以MI文献中的三个开放问题为例，展示哲学如何助力MI研究。

Result: 哲学能够帮助MI澄清概念、优化方法并评估其认知与伦理影响。

Conclusion: 提倡MI研究与哲学的跨学科合作，以解决其核心问题。

Abstract: Mechanistic interpretability (MI) aims to explain how neural networks work by
uncovering their underlying causal mechanisms. As the field grows in influence,
it is increasingly important to examine not just models themselves, but the
assumptions, concepts and explanatory strategies implicit in MI research. We
argue that mechanistic interpretability needs philosophy: not as an
afterthought, but as an ongoing partner in clarifying its concepts, refining
its methods, and assessing the epistemic and ethical stakes of interpreting AI
systems. Taking three open problems from the MI literature as examples, this
position paper illustrates the value philosophy can add to MI research, and
outlines a path toward deeper interdisciplinary dialogue.

</details>


### [83] [CommVQ: Commutative Vector Quantization for KV Cache Compression](https://arxiv.org/abs/2506.18879)
*Junyan Li,Yang Zhang,Muhammad Yusuf Hassan,Talha Chafekar,Tianle Cai,Zhile Ren,Pengsheng Guo,Foroozan Karimzadeh,Colorado Reed,Chong Wang,Chuang Gan*

Key words: 大型语言模型, KV缓存, 向量量化, 内存优化, GPU

TL;DR: 该论文提出了Commutative Vector Quantization (CommVQ)方法，通过压缩大型语言模型(LLMs)中的键值(KV)缓存，显著减少内存使用，并实现了高效的1位量化。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于大型语言模型(LLMs)在长上下文应用中键值(KV)缓存的GPU内存占用问题日益突出，作者旨在解决这一瓶颈。

Method: 采用加法量化和轻量级编码器压缩KV缓存；设计了与Rotary Position Embedding (RoPE)可交换的代码本，并通过EM算法训练，将其解码集成到自注意力机制中。

Result: 实验表明，该方法将FP16 KV缓存大小减少了87.5%（2位量化），并在1位量化时性能优于现有方法，使LLaMA-3.1 8B模型在128K上下文长度下运行于单块RTX 4090 GPU。

Conclusion: CommVQ显著降低了长上下文LLM推理的内存需求，同时保持了高精度和低开销。

Abstract: Large Language Models (LLMs) are increasingly used in applications requiring
long context lengths, but the key-value (KV) cache often becomes a memory
bottleneck on GPUs as context grows. To address this, we propose Commutative
Vector Quantization (CommVQ) to significantly reduce memory usage for
long-context LLM inference. We first introduce additive quantization with a
lightweight encoder and codebook to compress the KV cache, which can be decoded
via simple matrix multiplication. To further reduce computational costs during
decoding, we design the codebook to be commutative with Rotary Position
Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.
This enables efficient integration of decoding into the self-attention
mechanism. Our approach achieves high accuracy with additive quantization and
low overhead via the RoPE-commutative codebook. Experiments on long-context
benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%
with 2-bit quantization, while outperforming state-of-the-art KV cache
quantization methods. Notably, it enables 1-bit KV cache quantization with
minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context
length on a single RTX 4090 GPU. The source code is available at:
https://github.com/UMass-Embodied-AGI/CommVQ.

</details>


### [84] [OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization](https://arxiv.org/abs/2506.18880)
*Yiyou Sun,Shawn Hu,Georgia Zhou,Ken Zheng,Hannaneh Hajishirzi,Nouha Dziri,Dawn Song*

Key words: 大型语言模型、数学推理、OMEGA基准测试、泛化能力、创造性思维

TL;DR: 论文针对大型语言模型在数学推理中表现出的局限性，提出了OMEGA基准测试，通过三个创新性泛化轴（探索性、组合性和转化性）评估模型的泛化能力，发现前沿模型在复杂问题中表现下降，微调后探索性泛化有所提升，但组合性和转化性表现有限。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前大型语言模型在奥林匹克数学任务中表现出色，但面对需要新颖思维的复杂问题时表现不足，研究旨在系统评估这些局限性。

Method: 引入OMEGA基准测试，通过程序生成的训练-测试对评估三个泛化轴（探索性、组合性和转化性），覆盖多个数学领域，并使用符号、数值或图形方法验证解。

Result: 前沿模型在复杂问题中表现显著下降；微调Qwen系列模型后，探索性泛化有所改善，组合性和转化性泛化仍有限。

Conclusion: OMEGA为提升模型真正数学创造力奠定了基础，未来需突破机械熟练度的局限。

Abstract: Recent large-scale language models (LLMs) with long Chain-of-Thought
reasoning-such as DeepSeek-R1-have achieved impressive results on
Olympiad-level mathematics benchmarks. However, they often rely on a narrow set
of strategies and struggle with problems that require a novel way of thinking.
To systematically investigate these limitations, we introduce
OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a
controlled yet diverse benchmark designed to evaluate three axes of
out-of-distribution generalization, inspired by Boden's typology of creativity:
(1) Exploratory-applying known problem solving skills to more complex instances
within the same problem domain; (2) Compositional-combining distinct reasoning
skills, previously learned in isolation, to solve novel problems that require
integrating these skills in new and coherent ways; and (3)
Transformative-adopting novel, often unconventional strategies by moving beyond
familiar approaches to solve problems more effectively. OMEGA consists of
programmatically generated training-test pairs derived from templated problem
generators across geometry, number theory, algebra, combinatorics, logic, and
puzzles, with solutions verified using symbolic, numerical, or graphical
methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance
degradation as problem complexity increases. Moreover, we fine-tune the
Qwen-series models across all generalization settings and observe notable
improvements in exploratory generalization, while compositional generalization
remains limited and transformative reasoning shows little to no improvement. By
isolating and quantifying these fine-grained failures, OMEGA lays the
groundwork for advancing LLMs toward genuine mathematical creativity beyond
mechanical proficiency.

</details>


### [85] [ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs](https://arxiv.org/abs/2506.18896)
*Jiaru Zou,Ling Yang,Jingwen Gu,Jiahao Qiu,Ke Shen,Jingrui He,Mengdi Wang*

Key words: 过程奖励模型（PRMs）、推理轨迹、强化学习、监督微调、ReasonFlux-PRM

TL;DR: ReasonFlux-PRM是一种新型的轨迹感知过程奖励模型，专门设计用于评估轨迹-响应类型的推理轨迹，相比现有模型在多个任务中表现更优。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有过程奖励模型（PRMs）主要基于模型最终输出进行训练，难以对中间推理轨迹进行稳健评估，尤其是在前沿推理模型生成的轨迹-响应输出中表现不足。

Method: ReasonFlux-PRM结合了步骤级和轨迹级的监督，支持离线和在线设置下的奖励监督，包括高质量数据选择、密集过程级奖励提供以及奖励引导的测试时扩展。

Result: 在AIME、MATH500和GPQA-Diamond等基准测试中，ReasonFlux-PRM-7B表现优于其他PRMs和人工基线，平均性能提升分别为12.1%（监督微调）、4.5%（强化学习）和6.3%（测试时扩展）。

Conclusion: ReasonFlux-PRM在多个任务中显著提升了性能，并提供了适用于资源受限场景的高效版本。

Abstract: Process Reward Models (PRMs) have recently emerged as a powerful framework
for supervising intermediate reasoning steps in large language models (LLMs).
Previous PRMs are primarily trained on model final output responses and
struggle to evaluate intermediate thinking trajectories robustly, especially in
the emerging setting of trajectory-response outputs generated by frontier
reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a
novel trajectory-aware PRM explicitly designed to evaluate the
trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both
step-level and trajectory-level supervision, enabling fine-grained reward
assignment aligned with structured chain-of-thought data. We adapt
ReasonFlux-PRM to support reward supervision under both offline and online
settings, including (i) selecting high-quality model distillation data for
downstream supervised fine-tuning of smaller models, (ii) providing dense
process-level rewards for policy optimization during reinforcement learning,
and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results
on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond
demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs
(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our
derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving
average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement
learning, and 6.3% in test-time scaling. We also release our efficient
ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.
Projects: https://github.com/Gen-Verse/ReasonFlux

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [86] [MMET: A Multi-Input and Multi-Scale Transformer for Efficient PDEs Solving](https://arxiv.org/abs/2506.17230)
*Yichen Luo,Jia Wang,Dapeng Lan,Yu Liu,Zhibo Pang*

Key words: PDE, 多尺度, 多输入, 变换器, 机器学习

TL;DR: 论文提出了一种名为MMET的新型框架，通过解耦网格和查询点序列、使用GCE层嵌入多尺度输入，并结合Hilbert曲线降低计算成本，显著提升了多输入多尺度PDE问题的求解效率。实验表明MMET在精度和计算效率上优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决传统机器学习方法在处理多输入、多尺度PDE问题时泛化能力不足和计算成本高的问题。

Method: 设计MMET框架，包括网格与查询点解耦、GCE层嵌入输入变量、Hilbert曲线重排序和分块嵌入机制。

Result: MMET在多个基准测试中表现优于现有方法，尤其在精度和计算效率上有显著提升。

Conclusion: MMET为工程和物理应用中的实时PDE求解提供了高效、可扩展的解决方案，并展示了预训练模型在特定领域的潜力。

Abstract: Partial Differential Equations (PDEs) are fundamental for modeling physical
systems, yet solving them in a generic and efficient manner using machine
learning-based approaches remains challenging due to limited multi-input and
multi-scale generalization capabilities, as well as high computational costs.
This paper proposes the Multi-input and Multi-scale Efficient Transformer
(MMET), a novel framework designed to address the above challenges. MMET
decouples mesh and query points as two sequences and feeds them into the
encoder and decoder, respectively, and uses a Gated Condition Embedding (GCE)
layer to embed input variables or functions with varying dimensions, enabling
effective solutions for multi-scale and multi-input problems. Additionally, a
Hilbert curve-based reserialization and patch embedding mechanism decrease the
input length. This significantly reduces the computational cost when dealing
with large-scale geometric models. These innovations enable efficient
representations and support multi-scale resolution queries for large-scale and
multi-input PDE problems. Experimental evaluations on diverse benchmarks
spanning different physical fields demonstrate that MMET outperforms SOTA
methods in both accuracy and computational efficiency. This work highlights the
potential of MMET as a robust and scalable solution for real-time PDE solving
in engineering and physics-based applications, paving the way for future
explorations into pre-trained large-scale models in specific domains. This work
is open-sourced at https://github.com/YichenLuo-0/MMET.

</details>


### [87] [PCaM: A Progressive Focus Attention-Based Information Fusion Method for Improving Vision Transformer Domain Adaptation](https://arxiv.org/abs/2506.17232)
*Zelin Zang,Fei Wang,Liangyu Li,Jinlin Wu,Chunshui Zhao,Zhen Lei,Baigui Sun*

Key words: 无监督域适应, 视觉Transformer, 注意力机制, 前景融合, 跨域对齐

TL;DR: 提出了渐进式注意力机制（PCaM）解决UDA中前景对象不匹配问题，通过过滤背景信息提升跨域注意力一致性，实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有基于ViT的UDA方法因前景对象大小和空间分布差异导致注意力一致性差，影响域对齐效果。

Method: 采用PCaM机制逐步过滤背景信息，并结合注意力引导损失增强任务相关区域的注意力一致性。

Result: 在多个数据集上显著提升适应性能，达到新的SOTA效果。

Conclusion: PCaM通过注意力引导的前景融合有效改善了UDA任务的效果。

Abstract: Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a
labeled source domain to an unlabeled target domain. Recent UDA methods based
on Vision Transformers (ViTs) have achieved strong performance through
attention-based feature alignment. However, we identify a key limitation:
foreground object mismatch, where the discrepancy in foreground object size and
spatial distribution across domains weakens attention consistency and hampers
effective domain alignment. To address this issue, we propose the Progressive
Focus Cross-Attention Mechanism (PCaM), which progressively filters out
background information during cross-attention, allowing the model to focus on
and fuse discriminative foreground semantics across domains. We further
introduce an attentional guidance loss that explicitly directs attention toward
task-relevant regions, enhancing cross-domain attention consistency. PCaM is
lightweight, architecture-agnostic, and easy to integrate into existing
ViT-based UDA pipelines. Extensive experiments on Office-Home, DomainNet,
VisDA-2017, and remote sensing datasets demonstrate that PCaM significantly
improves adaptation performance and achieves new state-of-the-art results,
validating the effectiveness of attention-guided foreground fusion for domain
adaptation.

</details>


### [88] [Graph Neural Networks in Multi-Omics Cancer Research: A Structured Survey](https://arxiv.org/abs/2506.17234)
*Payam Zohari,Mostafa Haghir Chehreghani*

Key words: 多组学数据、图神经网络、癌症研究、数据整合、生物信息学

TL;DR: 该论文系统综述了基于图神经网络（GNN）的多组学数据整合方法在癌症研究中的应用，总结了当前的模型、任务和未来发展方向。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多组学数据整合是揭示癌症复杂生物学基础的有效策略，而图神经网络为建模这种异构和结构化数据提供了强大框架。

Method: 通过对近期研究的系统综述，将基于GNN的方法分类，重点关注组学层、网络结构和生物任务（如亚型分类、预后预测等）。

Result: 研究发现混合模型和可解释模型呈增长趋势，注意力机制和对比学习应用增多，患者特异性图和知识驱动先验成为新兴方向。

Conclusion: 该综述为设计高效的GNN多组学整合流程提供了全面参考，总结了当前实践、局限性和未来潜力。

Abstract: The task of data integration for multi-omics data has emerged as a powerful
strategy to unravel the complex biological underpinnings of cancer. Recent
advancements in graph neural networks (GNNs) offer an effective framework to
model heterogeneous and structured omics data, enabling precise representation
of molecular interactions and regulatory networks. This systematic review
explores several recent studies that leverage GNN-based architectures in
multi-omics cancer research. We classify the approaches based on their targeted
omics layers, graph neural network structures, and biological tasks such as
subtype classification, prognosis prediction, and biomarker discovery. The
analysis reveals a growing trend toward hybrid and interpretable models,
alongside increasing adoption of attention mechanisms and contrastive learning.
Furthermore, we highlight the use of patient-specific graphs and
knowledge-driven priors as emerging directions. This survey serves as a
comprehensive resource for researchers aiming to design effective GNN-based
pipelines for integrative cancer analysis, offering insights into current
practices, limitations, and potential future directions.

</details>


### [89] [Training a Scientific Reasoning Model for Chemistry](https://arxiv.org/abs/2506.17238)
*Siddharth M. Narayanan,James D. Braza,Ryan-Rhys Griffiths,Albert Bou,Geemi Wellawatte,Mayk Caldas Ramos,Ludovico Mitchener,Samuel G. Rodriques,Andrew D. White*

Key words: 推理模型，化学，强化学习，分子设计，数据效率

TL;DR: 本文介绍了ether0，一个基于Mistral-Small-24B的24B参数推理模型，能够在化学领域进行自然语言推理并生成化学结构。通过强化学习训练，该模型在数据效率和任务表现上优于通用化学模型、前沿模型和人类专家。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨推理模型是否能在数学、编程和逻辑之外的领域（如化学）中通用化，并验证其在数据效率和性能上的优势。

Method: 基于Mistral-Small-24B的24B参数模型，通过强化学习在640,730个化学问题上进行训练，覆盖375项任务。

Result: 模型在分子设计任务上超过通用化学模型、前沿模型和人类专家，且数据效率更高。

Conclusion: 该方法可扩展至其他科学领域，训练数据高效的专用语言模型。

Abstract: Reasoning models are large language models that emit a long chain-of-thought
before answering, providing both higher accuracy and explicit reasoning for
their response. A major question has been whether language model reasoning
generalizes beyond mathematics, programming, and logic, where most previous
work has focused. We demonstrate that reasoning models can be post-trained for
chemistry without additional domain pretraining, and require substantially less
data compared to contemporary domain-specific models. We report ether0, a 24B
parameter LLM (based on Mistral-Small-24B) that can reason in natural language
and respond with chemical structures. This reasoning model was trained with
reinforcement learning on 640,730 experimentally-grounded chemistry problems
across 375 tasks ranging from synthesizability, to blood-brain barrier
permeability, to human receptor activity, to scent. Our model exceeds
general-purpose chemistry models, frontier models, and human experts on
molecular design tasks. It is also more data efficient relative to specialized
models. We anticipate that this method can be applied to train data-efficient
language models specialized for tasks across a wide variety of scientific
domains.

</details>


### [90] [Recursive Learning-Based Virtual Buffering for Analytical Global Placement](https://arxiv.org/abs/2506.17247)
*Andrew B. Kahng,Yiting Liu,Zhiang Wang*

Key words: 缓冲感知布局, 机器学习, 时序收敛, ERC违规, OpenROAD

TL;DR: MLBuf-RePlAce是一个开源的基于学习的虚拟缓冲感知全局布局框架，显著提升了时序性能，同时保持了功耗的稳定性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着技术节点的进步，互连延迟与单元延迟的不均衡缩放使得具有缓冲孔隙（细胞密度）意识的布局在物理合成流程中对时序收敛至关重要。现有方法存在计算成本高或未能充分处理电气规则检查（ERC）违规的问题。

Method: MLBuf-RePlAce采用基于递归学习的方法预测缓冲器类型和位置，并在全局布局期间处理ERC违规。

Result: 在OpenROAD流程中，MLBuf-RePlAce在总负松弛（TNS）上实现了（最大值56％，平均值31％）的改善；在商用流程中，TNS改善了（最大值53％，平均值28％），且布线后功耗平均提升了0.2％。

Conclusion: MLBuf-RePlAce在时序性能上显著优于现有方法，同时保持了功耗的稳定性，成为物理设计流程中一个有效的解决方案。

Abstract: Due to the skewed scaling of interconnect versus cell delay in modern
technology nodes, placement with buffer porosity (i.e., cell density) awareness
is essential for timing closure in physical synthesis flows. However, existing
approaches face two key challenges: (i) traditional van Ginneken-Lillis-style
buffering approaches are computationally expensive during global placement; and
(ii) machine learning-based approaches, such as BufFormer, lack a thorough
consideration of Electrical Rule Check (ERC) violations and fail to "close the
loop" back into the physical design flow. In this work, we propose
MLBuf-RePlAce, the first open-source learning-driven virtual buffering-aware
analytical global placement framework, built on top of the OpenROAD
infrastructure. MLBuf-RePlAce adopts an efficient recursive learning-based
generative buffering approach to predict buffer types and locations, addressing
ERC violations during global placement. We compare MLBuf-RePlAce against the
default virtual buffering-based timing-driven global placer in OpenROAD, using
open-source testcases from the TILOS MacroPlacement and OpenROAD-flow-scripts
repositories. Without degradation of post-route power, MLBuf-RePlAce achieves
(maximum, average) improvements of (56%, 31%) in total negative slack (TNS)
within the open-source OpenROAD flow. When evaluated by completion in a
commercial flow, MLBuf-RePlAce achieves (maximum, average) improvements of
(53%, 28%) in TNS with an average of 0.2% improvement in post-route power.

</details>


### [91] [Efficient Quantification of Multimodal Interaction at Sample Level](https://arxiv.org/abs/2506.17248)
*Zequn Yang,Hongfa Wang,Di Hu*

Key words: 多模态信息, 交互作用, 冗余估计, 点对点信息论, LSMI

TL;DR: 论文提出了一种轻量级样本级多模态交互（LSMI）估计器，用于精确量化多模态信息中的冗余、独特性和协同作用，解决了理论和技术上的挑战。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多模态信息中的交互作用（冗余、独特性和协同作用）对理解信息动态至关重要，但准确量化这些交互在样本级别存在挑战。

Method: 基于点对点信息论，首先开发了冗余估计框架，然后提出一种通用的交互估计方法，采用高效熵估计技术。

Result: 在合成和真实数据集上的实验验证了LSMI的精确性和高效性，能够揭示样本和类别级别的多模态动态。

Conclusion: LSMI提供了一种实用工具，可用于样本分区、知识蒸馏和模型集成等应用。

Abstract: Interactions between modalities -- redundancy, uniqueness, and synergy --
collectively determine the composition of multimodal information. Understanding
these interactions is crucial for analyzing information dynamics in multimodal
systems, yet their accurate sample-level quantification presents significant
theoretical and computational challenges. To address this, we introduce the
Lightweight Sample-wise Multimodal Interaction (LSMI) estimator, rigorously
grounded in pointwise information theory. We first develop a redundancy
estimation framework, employing an appropriate pointwise information measure to
quantify this most decomposable and measurable interaction. Building upon this,
we propose a general interaction estimation method that employs efficient
entropy estimation, specifically tailored for sample-wise estimation in
continuous distributions. Extensive experiments on synthetic and real-world
datasets validate LSMI's precision and efficiency. Crucially, our sample-wise
approach reveals fine-grained sample- and category-level dynamics within
multimodal data, enabling practical applications such as redundancy-informed
sample partitioning, targeted knowledge distillation, and interaction-aware
model ensembling. The code is available at
https://github.com/GeWu-Lab/LSMI_Estimator.

</details>


### [92] [Improving Prediction Certainty Estimation for Reliable Early Exiting via Null Space Projection](https://arxiv.org/abs/2506.17249)
*Jianing He,Qi Zhang,Duoqian Miao,Yi Kun,Shufeng Hao,Hongyun Zhang,Zhihua Wei*

Key words: 早期退出,预训练语言模型,CAP分数,NSP分数,GLUE

TL;DR: 论文提出了一种基于CAP分数的早期退出方法，通过整合类相关和不相关信息来提高预测确定性，从而更可靠地决定退出时机。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有早期退出方法仅依赖类相关logits，忽视类不相关信息对预测确定性的负面影响，导致过早退出和错误预测。

Method: 定义了NSP分数来评估类不相关信息比例，并提出了基于CAP分数的早期退出方法，综合logits和NSP分数以优化退出决策。

Result: 在GLUE基准测试中，平均加速比为2.19倍，性能下降可忽略，优于当前最佳方法28%。

Conclusion: CAP方法在任务性能和推理效率之间实现了更好的平衡。

Abstract: Early exiting has demonstrated great potential in accelerating the inference
of pre-trained language models (PLMs) by enabling easy samples to exit at
shallow layers, eliminating the need for executing deeper layers. However,
existing early exiting methods primarily rely on class-relevant logits to
formulate their exiting signals for estimating prediction certainty, neglecting
the detrimental influence of class-irrelevant information in the features on
prediction certainty. This leads to an overestimation of prediction certainty,
causing premature exiting of samples with incorrect early predictions. To
remedy this, we define an NSP score to estimate prediction certainty by
considering the proportion of class-irrelevant information in the features. On
this basis, we propose a novel early exiting method based on the
Certainty-Aware Probability (CAP) score, which integrates insights from both
logits and the NSP score to enhance prediction certainty estimation, thus
enabling more reliable exiting decisions. The experimental results on the GLUE
benchmark show that our method can achieve an average speed-up ratio of 2.19x
across all tasks with negligible performance degradation, surpassing the
state-of-the-art (SOTA) ConsistentEE by 28%, yielding a better trade-off
between task performance and inference efficiency. The code is available at
https://github.com/He-Jianing/NSP.git.

</details>


### [93] [Towards Interpretable Adversarial Examples via Sparse Adversarial Attack](https://arxiv.org/abs/2506.17250)
*Fudong Lin,Jiadong Lou,Hao Wang,Brian Jalaian,Xu Yuan*

Key words: 稀疏攻击,对抗样本,CNN,参数化技术

TL;DR: 本文提出了一种稀疏攻击方法，用于理解CNN的脆弱性，通过最小的初始扰动和高效的计算，克服了现有方法的不足，实现了快速、可迁移且强力的攻击。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的稀疏攻击方法难以生成可解释的对抗样本，且存在计算开销大、迁移性差和攻击能力弱的问题。本研究旨在解决这些问题并提升稀疏攻击的性能。

Method: 引入了一种新颖的参数化技术以近似NP-hard的l0优化问题，并设计了一种损失函数以最大化对抗性并最小化扰动的像素数量。

Result: 实验表明，该方法在计算开销、迁移性和攻击强度上优于现有技术，并能生成更稀疏的对抗样本，帮助分类噪音类型。

Conclusion: 该方法不仅提升了稀疏攻击的性能，还为解释对抗扰动的误导机制提供了新的见解。

Abstract: Sparse attacks are to optimize the magnitude of adversarial perturbations for
fooling deep neural networks (DNNs) involving only a few perturbed pixels
(i.e., under the l0 constraint), suitable for interpreting the vulnerability of
DNNs. However, existing solutions fail to yield interpretable adversarial
examples due to their poor sparsity. Worse still, they often struggle with
heavy computational overhead, poor transferability, and weak attack strength.
In this paper, we aim to develop a sparse attack for understanding the
vulnerability of CNNs by minimizing the magnitude of initial perturbations
under the l0 constraint, to overcome the existing drawbacks while achieving a
fast, transferable, and strong attack to DNNs. In particular, a novel and
theoretical sound parameterization technique is introduced to approximate the
NP-hard l0 optimization problem, making directly optimizing sparse
perturbations computationally feasible. Besides, a novel loss function is
designed to augment initial perturbations by maximizing the adversary property
and minimizing the number of perturbed pixels simultaneously. Extensive
experiments are conducted to demonstrate that our approach, with theoretical
performance guarantees, outperforms state-of-the-art sparse attacks in terms of
computational overhead, transferability, and attack strength, expecting to
serve as a benchmark for evaluating the robustness of DNNs. In addition,
theoretical and empirical results validate that our approach yields sparser
adversarial examples, empowering us to discover two categories of noises, i.e.,
"obscuring noise" and "leading noise", which will help interpret how
adversarial perturbation misleads the classifiers into incorrect predictions.
Our code is available at https://github.com/fudong03/SparseAttack.

</details>


### [94] [Training-free LLM Verification via Recycling Few-shot Examples](https://arxiv.org/abs/2506.17251)
*Dongseok Lee,Jimyung Hong,Dongyoung Kim,Jaehyung Kim*

Key words: LLM, 少样本学习, 响应验证, Referi, 贝叶斯规则

TL;DR: 论文提出了一种名为Referi的新框架，通过复用少样本示例来验证LLM输出，无需额外训练，显著提高了LLM的准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: LLM的随机推理过程和不同输出结论带来了挑战，现有方法如多数投票或外部验证模型存在局限，如适用性有限或额外训练成本。

Method: 提出Referi框架，利用少样本示例评估候选输出，结合两种分数（基于贝叶斯规则设计）选择最佳输出。

Result: 在三个LLM和七个不同任务上的实验显示，框架平均提高了4.8%的准确率。

Conclusion: Referi通过有效响应选择，无需额外训练即可显著提升LLM性能。

Abstract: Although LLMs have achieved remarkable performance, the inherent
stochasticity of their reasoning process and varying conclusions present
significant challenges. Majority voting or Best-of-N with external verification
models has been explored to find the most promising solution among multiple LLM
outputs. However, these approaches have certain limitations, such as limited
applicability or the cost of an additional training step. To address this
problem, we propose a novel and effective framework that Recycles Few-shot
examples to verify LLM outputs (Referi). Our key idea is to additionally
utilize the given few-shot examples to evaluate the candidate outputs of the
target query, not only using them to generate outputs as the conventional
few-shot prompting setup. Specifically, Referi evaluates the generated outputs
by combining two different scores, designed motivated from Bayes' rule, and
subsequently selects the candidate that is both confidently determined and
contextually coherent through a few additional LLM inferences. Experiments with
three different LLMs and across seven diverse tasks demonstrate that our
framework significantly improves the accuracy of LLMs-achieving an average gain
of 4.8%-through effective response selection, without additional training.

</details>


### [95] [Adaptive Sample Scheduling for Direct Preference Optimization](https://arxiv.org/abs/2506.17252)
*Zixuan Huang,Yikun Ban,Lean Fu,Xiaojie Li,Zhongxiang Dai,Jianxin Li,Deqing Wang*

Key words: DPO, LLM, 样本调度, SamS, 对齐优化

TL;DR: 介绍了一种名为SamS的新算法，用于动态调整DPO训练中的样本选择，以提升模型对齐性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: DPO算法的性能依赖于人类偏好数据的质量，现有方法忽略了模型在训练过程中的动态变化，需改进样本选择策略。

Method: 提出SamS算法，根据模型的反馈动态调整训练样本，优化泛化性能。

Result: 在不改动DPO核心算法的情况下，SamS显著提升了多任务性能，且计算开销较低。

Conclusion: SamS为通过更有效利用固定偏好数据集改进LLM对齐提供了新方向。

Abstract: Direct Preference Optimization (DPO) has emerged as an effective approach for
aligning large language models (LLMs) with human preferences. However, its
performance is highly dependent on the quality of the underlying human
preference data. To address this bottleneck, prior work has explored various
data selection strategies, but these methods often overlook the impact of the
evolving states of the language model during the DPO process. %including active
querying, response pair selection, and data pre-selection. In this paper, we
introduce a novel problem: Sample Scheduling for DPO, which aims to dynamically
and adaptively schedule training samples based on the model's evolving states
throughout preference optimization. To solve this problem, we propose SamS, an
efficient and effective algorithm that adaptively selects samples in each
training batch based on the LLM's learning feedback to maximize the potential
generalization performance. Notably, without modifying the core DPO algorithm,
simply integrating SamS significantly improves performance across tasks, with
minimal additional computational overhead. This work points to a promising new
direction for improving LLM alignment through more effective utilization of
fixed preference datasets.

</details>


### [96] [MS-TVNet:A Long-Term Time Series Prediction Method Based on Multi-Scale Dynamic Convolution](https://arxiv.org/abs/2506.17253)
*Chenghan Li,Mingchen Li,Yipu Liao,Ruisheng Diao*

Key words: 长时间序列预测,卷积网络,多尺度,MS-TVNet

TL;DR: 本文提出了一种新的多尺度时间序列重塑模块，并基于此构建了MS-TVNet模型，展示了卷积网络在长时间序列预测中的潜力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 填补卷积网络在长时间序列预测中的研究空白。

Method: 提出了一个多尺度时间序列重塑模块，并构建了MS-TVNet模型。

Result: 在多个数据集上表现优于基线模型，达到了SOTA效果。

Conclusion: 卷积网络在捕捉复杂时间模式上具有潜力，为未来研究提供了新方向。

Abstract: Long-term time series prediction has predominantly relied on Transformer and
MLP models, while the potential of convolutional networks in this domain
remains underexplored. To address this gap, we introduce a novel multi-scale
time series reshape module, which effectively captures the relationships among
multi-period patches and variable dependencies. Building upon this module, we
propose MS-TVNet, a multi-scale 3D dynamic convolutional neural network.
Through comprehensive evaluations on diverse datasets, MS-TVNet demonstrates
superior performance compared to baseline models, achieving state-of-the-art
(SOTA) results in long-term time series prediction. Our findings highlight the
effectiveness of leveraging convolutional networks for capturing complex
temporal patterns, suggesting a promising direction for future research in this
field.The code is realsed on https://github.com/Curyyfaust/TVNet.

</details>


### [97] [Keeping Up with the Models: Online Deployment and Routing of LLMs at Scale](https://arxiv.org/abs/2506.17254)
*Shaoang Li,Jian Li*

Key words: 大型语言模型、在线决策、模型部署、路由算法、赌博机问题

TL;DR: 该论文提出了一种名为 StageRoute 的分层算法，用于优化大型语言模型（LLM）的部署和查询路由问题，解决了模型快速更新与有限部署资源之间的矛盾。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着大型语言模型的快速更新，服务提供商需要在有限的部署容量和查询成本预算下，高效管理不断变化的模型库。

Method: StageRoute 算法分两步：(i) 在维护窗口阶段乐观选择模型，(ii) 通过预算约束的赌博机子问题路由查询。

Result: 该算法实现了 $T^{2/3}$ 的遗憾上界，并通过实验验证其接近最优性能。

Conclusion: StageRoute 在实践中表现优异，证明了其在解决动态模型部署和路由问题上的有效性。

Abstract: The rapid pace at which new large language models (LLMs) appear -- and older
ones become obsolete -- forces LLM service providers to juggle a streaming
inventory of models while respecting tight deployment capacity and per-query
cost budgets. We cast the reality as an online decision problem that couples
stage-wise deployment, made at fixed maintenance windows, with per-query
routing among the models kept live. We introduce StageRoute, a hierarchical
algorithm that (i) optimistically selects up to $M_max$ models for the next
stage using reward upper-confidence and cost lower-confidence bounds, then (ii)
solves a budget-constrained bandit sub-problem to route each incoming query. We
prove that StageRoute achieves a regret of order $T^{2/3}$ and provide a
matching lower bound, thereby establishing its near-optimality. Moreover, our
experiments confirm the theory, demonstrating that StageRoute performs close to
the optimum in practical settings.

</details>


### [98] [UltraSketchLLM: Saliency-Driven Sketching for Ultra-Low Bit LLM Compression](https://arxiv.org/abs/2506.17255)
*Sunan Zou,Ziyun Zhang,Xueting Sun,Guojie Luo*

Key words: 大语言模型, 极端压缩, 数据草图, 低比特压缩, 边缘计算

TL;DR: 该论文提出了一种名为UltraSketchLLM的框架，通过无索引的数据草图技术实现了超低比特压缩（低至0.5比特/权重），同时保持模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大语言模型（LLMs）的快速发展超过了边缘设备的内存限制，需要超越1比特限制的极端权重压缩。现有的多对一压缩方法要么依赖映射表（增加内存开销），要么因随机权重分组导致严重精度下降。

Method: 利用数据草图技术（一种来自流应用的次线性表示方法），通过低估的AbsMaxMin草图、重要性感知空间分配和直通估计器进行压缩感知微调。

Result: 在Llama-3.2-1B上的实验表明，实现了0.5比特压缩，同时保持竞争性的困惑度，并具有可容忍的延迟开销。

Conclusion: UltraSketchLLM为在资源受限环境中部署LLMs提供了一种实用解决方案。

Abstract: The rapid growth of large language models (LLMs) has outpaced the memory
constraints of edge devices, necessitating extreme weight compression beyond
the 1-bit limit. While quantization reduces model size, it is fundamentally
limited to 1 bit per weight. Existing multiple-to-one compression methods
either rely on mapping tables (inducing memory overhead) or incur severe
accuracy degradation due to random weight grouping. We introduce
UltraSketchLLM, an index-free, sketch-based framework that achieves ultra-low
bit compression (down to 0.5 bits per weight) while preserving model
performance. UltraSketchLLM leverages data sketching, a sub-linear
representation technique from streaming applications, to map multiple weights
to single values with bounded error. Our approach integrates an underestimate
AbsMaxMin sketch to minimize relative errors for small weights,
importance-aware space allocation to prioritize salient weights, and a
straight-through estimator for compression-aware finetuning. Experiments on
Llama-3.2-1B demonstrate up to 0.5-bit compression with competitive perplexity,
alongside tolerable latency overhead. UltraSketchLLM offers a practical
solution for deploying LLMs in resource-constrained environments.

</details>


### [99] [AI to Identify Strain-sensitive Regions of the Optic Nerve Head Linked to Functional Loss in Glaucoma](https://arxiv.org/abs/2506.17262)
*Thanadet Chuangsuwanich,Monisha E. Nongpiur,Fabian A. Braeu,Tin A. Tun,Alexandre Thiery,Shamira Perera,Ching Lin Ho,Martin Buist,George Barbastathis,Tin Aung,Michaël J. A. Girard*

Key words: 青光眼、视神经头生物力学、可解释AI、视野缺损、预测模型

TL;DR: 该研究评估了视神经头（ONH）生物力学对青光眼患者三种进展性视野缺损模式的预测效果，并利用可解释AI识别关键区域。模型AUC为0.77-0.88，显示ONH应变能显著提升预测能力，且下缘和下颞缘是关键区域。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨ONH生物力学特征是否能够改进青光眼进展性视野缺损的预测，并识别对预测贡献最大的ONH区域。

Method: 237名青光眼患者纳入研究，通过眼动力测量仪提升眼压并采集ONH图像，结合几何深度学习和可解释AI技术进行分类任务。

Result: 模型AUC为0.77-0.88，显示ONH应变显著提升预测效果，下缘和下颞缘是关键应变敏感区域。

Conclusion: ONH应变可显著改善青光眼视野缺损模式的预测，神经视网膜缘是最关键区域。

Abstract: Objective: (1) To assess whether ONH biomechanics improves prediction of
three progressive visual field loss patterns in glaucoma; (2) to use
explainable AI to identify strain-sensitive ONH regions contributing to these
predictions.
  Methods: We recruited 237 glaucoma subjects. The ONH of one eye was imaged
under two conditions: (1) primary gaze and (2) primary gaze with IOP elevated
to ~35 mmHg via ophthalmo-dynamometry. Glaucoma experts classified the subjects
into four categories based on the presence of specific visual field defects:
(1) superior nasal step (N=26), (2) superior partial arcuate (N=62), (3) full
superior hemifield defect (N=25), and (4) other/non-specific defects (N=124).
Automatic ONH tissue segmentation and digital volume correlation were used to
compute IOP-induced neural tissue and lamina cribrosa (LC) strains.
Biomechanical and structural features were input to a Geometric Deep Learning
model. Three classification tasks were performed to detect: (1) superior nasal
step, (2) superior partial arcuate, (3) full superior hemifield defect. For
each task, the data were split into 80% training and 20% testing sets. Area
under the curve (AUC) was used to assess performance. Explainable AI techniques
were employed to highlight the ONH regions most critical to each
classification.
  Results: Models achieved high AUCs of 0.77-0.88, showing that ONH strain
improved VF loss prediction beyond morphology alone. The inferior and
inferotemporal rim were identified as key strain-sensitive regions,
contributing most to visual field loss prediction and showing progressive
expansion with increasing disease severity.
  Conclusion and Relevance: ONH strain enhances prediction of glaucomatous VF
loss patterns. Neuroretinal rim, rather than the LC, was the most critical
region contributing to model predictions.

</details>


### [100] [Memory Allocation in Resource-Constrained Reinforcement Learning](https://arxiv.org/abs/2506.17263)
*Massimiliano Tamborski,David Abel*

Key words: 内存约束, 强化学习, MCTS, DQN, 资源分配

TL;DR: 研究记忆约束如何影响强化学习算法中智能体在未知环境中的表现，探讨内存分配的权衡问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究资源约束（尤其是内存限制）如何改变学习和决策过程，探讨智能体在内存有限时的表现。

Method: 使用基于MCTS和DQN的算法，研究不同内存分配对性能的影响，分析在episodic和continual学习环境中的表现。

Result: 不同内存分配策略对智能体性能有显著影响。

Conclusion: 内存分配策略是智能体在资源受限环境中表现的关键因素。

Abstract: Resource constraints can fundamentally change both learning and
decision-making. We explore how memory constraints influence an agent's
performance when navigating unknown environments using standard reinforcement
learning algorithms. Specifically, memory-constrained agents face a dilemma:
how much of their limited memory should be allocated to each of the agent's
internal processes, such as estimating a world model, as opposed to forming a
plan using that model? We study this dilemma in MCTS- and DQN-based algorithms
and examine how different allocations of memory impact performance in episodic
and continual learning settings.

</details>


### [101] [OAT-Rephrase: Optimization-Aware Training Data Rephrasing for Zeroth-Order LLM Fine-Tuning](https://arxiv.org/abs/2506.17264)
*Jikai Long,Zijian Hu,Xiaodong Yu,Jianwen Xie,Zhaozhuo Xu*

Key words: 大语言模型, 零阶优化, 微调, 训练数据重写, MeZO

TL;DR: 论文提出了OAT-Rephrase策略，通过优化感知的LLM重写训练数据，以提升零阶优化（如MeZO）的微调性能，缩小与一阶方法的差距。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 零阶优化方法（如MeZO）在微调大语言模型时内存效率高，但收敛慢且优化不稳定。本文旨在通过优化感知的重写策略解决这些问题。

Method: 提出OAT-Rephrase，利用LLM基于零阶优化动态（MeZO）重写训练数据，包含双阶段管道（重写器和语义判断器）以确保任务相关性和逻辑一致性。

Result: 在五个分类任务和三种LLM架构上的实验表明，OAT-Rephrase能显著提升MeZO的性能，甚至接近或超越一阶方法。

Conclusion: 优化感知的重写策略是可复用且低开销的改进，适用于零阶优化场景。

Abstract: Fine-tuning large language models (LLMs) using zeroth-order optimization (ZO)
offers a memory-efficient alternative to gradient-based methods but suffers
from slower convergence and unstable optimization due to noisy gradient
estimates. This paper introduces OAT-Rephrase, an Optimization-Aware Training
data rephrasing strategy that leverages an LLM to rephrase training instances
based on its understanding of the ZO dynamics, specifically MeZO, derived
directly from its paper. The approach incorporates a dual-stage pipeline
featuring a rewriter LLM and a semantic judge, ensuring all rephrasings retain
task relevance and logical consistency. Evaluations across five classification
tasks and three LLM architectures demonstrate that OAT-Rephrase consistently
improves MeZO fine-tuning performance, often narrowing or eliminating the gap
with first-order methods. Our findings suggest that optimization-aware
rephrasing serves as a reusable and low-overhead enhancement for zeroth-order
tuning regimes.

</details>


### [102] [Does Multimodal Large Language Model Truly Unlearn? Stealthy MLLM Unlearning Attack](https://arxiv.org/abs/2506.17265)
*Xianren Zhang,Hui Liu,Delvin Ce Zhang,Xianfeng Tang,Qi He,Dongwon Lee,Suhang Wang*

Key words: 多模态大语言模型、遗忘攻击、隐私保护、SUA框架

TL;DR: 本文研究了多模态大语言模型（MLLM）遗忘攻击问题，提出了一种名为SUA的隐蔽性攻击框架，通过学习通用噪声模式恢复被遗忘的知识，并证明了其高效性和泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多模态大语言模型可能记忆敏感个人信息，现有的遗忘方法是否能真正删除知识尚不明确，因此需要通过攻击验证模型是否真的遗忘。

Method: 提出SUA框架，学习通用噪声模式触发模型恢复遗忘内容，并通过嵌入对齐损失提升隐蔽性。

Result: 实验表明SUA能有效恢复被遗忘的信息，且噪声模式泛化能力强，能应用于未见图像。

Conclusion: 遗忘后的知识可能被隐蔽攻击恢复，表明知识重现是模型的一致行为而非偶然失败。

Abstract: Multimodal Large Language Models (MLLMs) trained on massive data may memorize
sensitive personal information and photos, posing serious privacy risks. To
mitigate this, MLLM unlearning methods are proposed, which fine-tune MLLMs to
reduce the ``forget'' sensitive information. However, it remains unclear
whether the knowledge has been truly forgotten or just hidden in the model.
Therefore, we propose to study a novel problem of LLM unlearning attack, which
aims to recover the unlearned knowledge of an unlearned LLM. To achieve the
goal, we propose a novel framework Stealthy Unlearning Attack (SUA) framework
that learns a universal noise pattern. When applied to input images, this noise
can trigger the model to reveal unlearned content. While pixel-level
perturbations may be visually subtle, they can be detected in the semantic
embedding space, making such attacks vulnerable to potential defenses. To
improve stealthiness, we introduce an embedding alignment loss that minimizes
the difference between the perturbed and denoised image embeddings, ensuring
the attack is semantically unnoticeable. Experimental results show that SUA can
effectively recover unlearned information from MLLMs. Furthermore, the learned
noise generalizes well: a single perturbation trained on a subset of samples
can reveal forgotten content in unseen images. This indicates that knowledge
reappearance is not an occasional failure, but a consistent behavior.

</details>


### [103] [CF-VLM:CounterFactual Vision-Language Fine-tuning](https://arxiv.org/abs/2506.17267)
*Jusheng Zhang,Kaitong Cai,Yijia Fan,Jian Wang,Keze Wang*

Key words: 视觉语言模型、因果推理、反事实样本、组合推理、泛化

TL;DR: 提出了一种名为CF-VLM的新框架，通过反事实样本增强视觉语言模型的因果推理能力，显著提升了细粒度区分和深度因果推理任务的表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有视觉语言模型依赖表面统计相关性，缺乏捕捉视觉与文本内容间因果逻辑的能力，限制了其在高风险场景中的应用。

Method: 引入了三种互补的训练目标：保持基础跨模态对齐、强化事实场景表示的唯一性和稳定性，以及提升模型对关键因果编辑的敏感性。

Result: CF-VLM在组合推理和泛化基准测试中优于基线方法和最先进方法，同时减少了视觉幻觉。

Conclusion: CF-VLM为需要可靠推理和可解释性的高风险现实场景中的视觉语言模型部署提供了坚实基础。

Abstract: Recent advances in vision-language models (VLMs) have greatly improved
cross-modal semantic understanding, yet significant limitations remain in
fine-grained discrimination and deep causal reasoning tasks. Existing VLMs
often rely on superficial statistical correlations, lacking the ability to
capture the underlying causal logic between visual and textual content. To
address this, we propose CounterFactual Vision-Language Fine-tuning (CF-VLM), a
novel framework that enhances the causal reasoning capabilities of VLMs through
the targeted use of counterfactual samples. CF-VLM introduces three
complementary training objectives: maintaining foundational cross-modal
alignment, reinforcing the uniqueness and stability of factual scene
representations against coherent counterfactuals, and sharpening the model's
sensitivity to minimal but critical causal edits. Extensive experiments
demonstrate that CF-VLM consistently outperforms strong baselines and
state-of-the-art methods on compositional reasoning and generalization
benchmarks. Furthermore, it shows promise in mitigating visual hallucinations,
indicating improved factual consistency. Our CF-VLM provides a robust
foundation for deploying VLMs in high-stakes, real-world scenarios requiring
reliable reasoning and interpretability.

</details>


### [104] [SafeRL-Lite: A Lightweight, Explainable, and Constrained Reinforcement Learning Library](https://arxiv.org/abs/2506.17297)
*Satyam Mishra,Phung Thao Vi,Shivam Mishra,Vishwanath Bijalwan,Vijay Bhaskar Semwal,Abdul Manan Khan*

Key words: 强化学习,安全性约束,可解释性,SHAP值,显著性图

TL;DR: SafeRL-Lite是一个轻量级的开源Python库，旨在构建具有安全约束和可解释性的强化学习代理。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的强化学习工具包通常缺乏原生机制来强制实施硬安全约束或生成人类可理解的决策理由。

Method: SafeRL-Lite通过模块化封装标准Gym环境和深度Q学习代理，支持约束强制实施的安全感知训练，以及通过SHAP值和显著性图提供实时事后解释。

Result: 在受限的CartPole环境中展示了其有效性，并提供了揭示策略逻辑和安全遵循的可视化。

Conclusion: SafeRL-Lite是一个轻量级、可扩展且易于安装的工具包，适用于需要安全性和可解释性的强化学习研究。

Abstract: We introduce SafeRL-Lite, an open-source Python library for building
reinforcement learning (RL) agents that are both constrained and explainable.
Existing RL toolkits often lack native mechanisms for enforcing hard safety
constraints or producing human-interpretable rationales for decisions.
SafeRL-Lite provides modular wrappers around standard Gym environments and deep
Q-learning agents to enable: (i) safety-aware training via constraint
enforcement, and (ii) real-time post-hoc explanation via SHAP values and
saliency maps. The library is lightweight, extensible, and installable via pip,
and includes built-in metrics for constraint violations. We demonstrate its
effectiveness on constrained variants of CartPole and provide visualizations
that reveal both policy logic and safety adherence. The full codebase is
available at: https://github.com/satyamcser/saferl-lite.

</details>


### [105] [AlgoSelect: Universal Algorithm Selection via the Comb Operator](https://arxiv.org/abs/2506.17304)
*Jasper Yao*

Key words: 算法选择, Comb Operator, 信息最优性, 普适逼近, 自适应系统

TL;DR: AlgoSelect是一个通过学习数据优化算法选择的理论框架，核心是新颖的Comb Operator，能够通过sigmoid门控选择器在不同算法间插值，理论上具有普适性、信息最优性和计算高效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决自动化算法选择的理论与实践挑战，提供一种具有可学习和最优性保证的通用方法。

Method: 通过Comb Operator和N-Path Comb实现多算法间的插值选择，并利用自适应种子函数引导学习过程。

Result: 在20×20问题-算法研究中实现了99.9%+的选择准确率，样本需求少且收敛快。

Conclusion: AlgoSelect不仅理论完备，且在实践中高效可行，对AI和自适应系统有重要意义。

Abstract: We introduce AlgoSelect, a principled framework for learning optimal
algorithm selection from data, centered around the novel Comb Operator. Given a
set of algorithms and a feature representation of problems, AlgoSelect learns
to interpolate between diverse computational approaches. For pairs of
algorithms, a simple sigmoid-gated selector, an instance of the Comb Operator,
facilitates this interpolation. We extend this to an N-Path Comb for multiple
algorithms. We prove that this framework is universal (can approximate any
algorithm selector), information-theoretically optimal in its learnability
(thresholds for selection converge almost surely, demonstrated via
Borel-Cantelli arguments), computationally efficient, and robust. Key
theoretical contributions include: (1) a universal approximation theorem
demonstrating that Comb-based selectors can achieve arbitrary accuracy; (2)
information-theoretic learnability for selection thresholds; (3) formalization
of the Comb Operator within linear operator theory, detailing its boundedness
and spectral properties; (4) an N-Path Comb generalization for multi-algorithm
selection; and (5) a practical learning framework for the adaptive seeding
functions that guide the Comb Operator. Empirical validation on a comprehensive
20$\times$20 problem-algorithm study demonstrates near-perfect selection
(99.9\%+ accuracy) with remarkably few samples and rapid convergence, revealing
that $H(\text{Algorithm}|\text{Problem}) \approx 0$ in structured domains.
AlgoSelect provides a theoretically grounded, practically deployable solution
to automated algorithm selection with provable optimality and learnability
guarantees, with significant implications for AI and adaptive systems.

</details>


### [106] [Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation](https://arxiv.org/abs/2506.17307)
*Zhixiang Chi,Li Gu,Huan Liu,Ziqiang Wang,Yanan Wu,Yang Wang,Konstantinos N Plataniotis*

Key words: CLIP, 小样本适应, 测试时域适应, ViT-B/16, 还原注意力

TL;DR: 提出一种新方法，通过在学习输入空间直接补充CLIP知识，提升小样本测试时域适应性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现有方法仅依赖CLIP预训练特征的局限性，尤其在非鲁棒主干网络（如ViT-B/16）上性能显著下降的问题。

Method: 引入并行分支和还原注意力机制学习输入空间知识，并通过贪婪文本集成和细化增强文本特征。

Result: 在5个大规模基准测试上表现优异，如iWildCam的F1提高5.1，FMoW的WC Acc提升3.1%。

Conclusion: 直接学习输入空间知识可有效补充CLIP能力，显著提升域适应性能。

Abstract: Few-shot Test-Time Domain Adaptation focuses on adapting a model at test time
to a specific domain using only a few unlabeled examples, addressing domain
shift. Prior methods leverage CLIP's strong out-of-distribution (OOD) abilities
by generating domain-specific prompts to guide its generalized, frozen
features. However, since downstream datasets are not explicitly seen by CLIP,
solely depending on the feature space knowledge is constrained by CLIP's prior
knowledge. Notably, when using a less robust backbone like ViT-B/16,
performance significantly drops on challenging real-world benchmarks. Departing
from the state-of-the-art of inheriting the intrinsic OOD capability of CLIP,
this work introduces learning directly on the input space to complement the
dataset-specific knowledge for frozen CLIP. Specifically, an independent side
branch is attached in parallel with CLIP and enforced to learn exclusive
knowledge via revert attention. To better capture the dataset-specific label
semantics for downstream adaptation, we propose to enhance the inter-dispersion
among text features via greedy text ensemble and refinement. The text and
visual features are then progressively fused in a domain-aware manner by a
generated domain prompt to adapt toward a specific domain. Extensive
experiments show our method's superiority on 5 large-scale benchmarks (WILDS
and DomainNet), notably improving over smaller networks like ViT-B/16 with
gains of \textbf{+5.1} in F1 for iWildCam and \textbf{+3.1\%} in WC Acc for
FMoW.

</details>


### [107] [I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution](https://arxiv.org/abs/2506.17323)
*Tamas Bisztray,Bilel Cherif,Richard A. Dubniczky,Nils Gruschka,Bertalan Borsos,Mohamed Amine Ferrag,Attila Kovacs,Vasileios Mavroeidis,Norbert Tihanyi*

Key words: LLM, 作者归属, CodeT5, 代码检测

TL;DR: 本文首次系统研究了LLM生成的C程序作者归属问题，提出了CodeT5-Authorship模型和LLM-AuthorBench基准数据集。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着LLM生成的代码日益普遍，识别代码背后的具体模型变得重要。

Method: 使用CodeT5的编码器层，结合两层分类头进行分类。

Result: 在二元分类中准确率达到97.56%，五类分类中达到95.40%。

Conclusion: CodeT5-Authorship模型在识别LLM生成的C程序作者方面表现出色。

Abstract: Detecting AI-generated code, deepfakes, and other synthetic content is an
emerging research challenge. As code generated by Large Language Models (LLMs)
becomes more common, identifying the specific model behind each sample is
increasingly important. This paper presents the first systematic study of LLM
authorship attribution for C programs. We released CodeT5-Authorship, a novel
model that uses only the encoder layers from the original CodeT5
encoder-decoder architecture, discarding the decoder to focus on
classification. Our model's encoder output (first token) is passed through a
two-layer classification head with GELU activation and dropout, producing a
probability distribution over possible authors. To evaluate our approach, we
introduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs
generated by eight state-of-the-art LLMs across diverse tasks. We compare our
model to seven traditional ML classifiers and eight fine-tuned transformer
models, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3,
Longformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model
achieves 97.56% accuracy in distinguishing C programs generated by closely
related models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class
attribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku,
GPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the
CodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant
Google Colab scripts on GitHub: https://github.com/LLMauthorbench/.

</details>


### [108] [Origins of Creativity in Attention-Based Diffusion Models](https://arxiv.org/abs/2506.17324)
*Emma Finn,T. Anderson Keller,Manos Theodosis,Demba E. Ba*

Key words: 扩散模型, 自注意力, 图像生成, 创造性, 理论扩展

TL;DR: 扩散模型已成为图像生成的首选工具，但其创造性的来源尚不明确。本文扩展了理论，分析了自注意力层在全局图像一致性中的作用。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究扩散模型中‘创造性’的来源，特别是自注意力层在生成图像中的作用。

Method: 扩展现有理论，分析CNN加自注意力层的扩散模型，验证自注意力对全局图像一致性的影响。

Result: 理论表明自注意力能促进局部特征的全局一致性，实验验证了这一行为。

Conclusion: 自注意力层在扩散模型中显著提升了生成图像的全局一致性。

Abstract: As diffusion models have become the tool of choice for image generation and
as the quality of the images continues to improve, the question of how
`creativity' originates in diffusion has become increasingly important. The
score matching perspective on diffusion has proven particularly fruitful for
understanding how and why diffusion models generate images that remain
plausible while differing significantly from their training images. In
particular, as explained in (Kamb \& Ganguli, 2024) and others, e.g.,
(Ambrogioni, 2023), theory suggests that if our score matching were optimal, we
would only be able to recover training samples through our diffusion process.
However, as shown by Kamb \& Ganguli, (2024), in diffusion models where the
score is parametrized by a simple CNN, the inductive biases of the CNN itself
(translation equivariance and locality) allow the model to generate samples
that globally do not match any training samples, but are rather patch-wise
`mosaics'. Notably, however, this theory does not extend to describe the role
of self-attention in this process. In this work, we take a preliminary step in
this direction to extend this theory to the case of diffusion models whose
score is parametrized by a CNN with a final self-attention layer. We show that
our theory suggests that self-attention will induce a globally image-consistent
arrangement of local features beyond the patch-level in generated samples, and
we verify this behavior empirically on a carefully crafted dataset.

</details>


### [109] [CopulaSMOTE: A Copula-Based Oversampling Approach for Imbalanced Classification in Diabetes Prediction](https://arxiv.org/abs/2506.17326)
*Agnideep Aich,Md Monzur Murshed,Sameera Hewage,Amanda Mayeaux*

Key words: 糖尿病检测, 数据增强, copula, 机器学习, SMOTE, XGBoost

TL;DR: 该研究使用基于A2 copula的数据增强方法改进糖尿病检测中的机器学习性能，相比SMOTE方法，XGBoost结合A2 copula显著提升了各项指标。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 糖尿病早期检测数据不平衡问题影响机器学习性能，研究旨在探索copula数据增强方法以解决这一问题。

Method: 采用A2 copula生成少数类数据，结合逻辑回归、随机森林、梯度提升和XGBoost四种算法进行实验。

Result: XGBoost与A2 copula组合表现最佳，相比SMOTE方法，准确率提升4.6%，其他指标也有显著提升。

Conclusion: A2 copula数据增强首次应用于此领域，展示了copula在机器学习中的潜力，是一种优于SMOTE的替代方法。

Abstract: Diabetes mellitus poses a significant health risk, as nearly 1 in 9 people
are affected by it. Early detection can significantly lower this risk. Despite
significant advancements in machine learning for identifying diabetic cases,
results can still be influenced by the imbalanced nature of the data. To
address this challenge, our study considered copula-based data augmentation,
which preserves the dependency structure when generating data for the minority
class and integrates it with machine learning (ML) techniques. We selected the
Pima Indian dataset and generated data using A2 copula, then applied four
machine learning algorithms: logistic regression, random forest, gradient
boosting, and extreme gradient boosting. Our findings indicate that XGBoost
combined with A2 copula oversampling achieved the best performance improving
accuracy by 4.6%, precision by 15.6%, recall by 20.4%, F1-score by 18.2% and
AUC by 25.5% compared to the standard SMOTE method. Furthermore, we
statistically validated our results using the McNemar test. This research
represents the first known use of A2 copulas for data augmentation and serves
as an alternative to the SMOTE technique, highlighting the efficacy of copulas
as a statistical method in machine learning applications.

</details>


### [110] [AutomataGPT: Forecasting and Ruleset Inference for Two-Dimensional Cellular Automata](https://arxiv.org/abs/2506.17333)
*Jaime A. Berkovich,Noah S. David,Markus J. Buehler*

Key words: 细胞自动机,转换器模型,规则推断,动态预测,AI驱动科学发现

TL;DR: 论文提出了一种名为AutomataGPT的解码器转换器模型，通过在大规模模拟轨迹上预训练，能够高精度预测和推断二维确定性细胞自动机的局部规则。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 细胞自动机（CA）是一种研究局部交互如何产生丰富时空行为的简洁模型，但其规则的自动发现和定量预测一直是挑战。AutomataGPT旨在通过大规模预训练解决这一问题。

Method: 使用解码器转换器架构，在一个包含100种二维二进制确定性CA规则的100万模拟轨迹上预训练，评估时从同一CA家族中抽取未见规则进行测试。

Result: AutomataGPT在一步预测中达到98.5%的准确率，规则推断的功能准确率为96%，精确规则矩阵匹配率为82%，展示了强大的泛化能力。

Conclusion: 研究表明，无需人工先验知识，大规模预训练能在细胞自动机的前向和逆向问题中实现显著泛化，为实际动态现象的抽象化提供了新思路。

Abstract: Cellular automata (CA) provide a minimal formalism for investigating how
simple local interactions generate rich spatiotemporal behavior in domains as
diverse as traffic flow, ecology, tissue morphogenesis and crystal growth.
However, automatically discovering the local update rules for a given
phenomenon and using them for quantitative prediction remains challenging. Here
we present AutomataGPT, a decoder-only transformer pretrained on around 1
million simulated trajectories that span 100 distinct two-dimensional binary
deterministic CA rules on toroidal grids. When evaluated on previously unseen
rules drawn from the same CA family, AutomataGPT attains 98.5% perfect one-step
forecasts and reconstructs the governing update rule with up to 96% functional
(application) accuracy and 82% exact rule-matrix match. These results
demonstrate that large-scale pretraining over wider regions of rule space
yields substantial generalization in both the forward (state forecasting) and
inverse (rule inference) problems, without hand-crafted priors. By showing that
transformer models can faithfully infer and execute CA dynamics from data
alone, our work lays the groundwork for abstracting real-world dynamical
phenomena into data-efficient CA surrogates, opening avenues in biology, tissue
engineering, physics and AI-driven scientific discovery.

</details>


### [111] [Adaptive Social Metaverse Streaming based on Federated Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2506.17342)
*Zijian Long,Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Key words: 社交元宇宙, 隐私保护, 联邦学习, 深度强化学习, 流媒体优化

TL;DR: ASMS (Adaptive Social Metaverse Streaming) 是一种基于联邦多智能体近端策略优化 (F-MAPPO) 的流媒体系统，旨在解决社交元宇宙中的隐私和流媒体质量挑战。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 社交元宇宙是一个快速发展的数字生态系统，但其隐私保护和高质量、低延迟流媒体的需求尚未得到充分满足。

Method: ASMS 结合联邦学习和深度强化学习 (DRL)，动态调整流媒体比特率，同时保护用户隐私。

Result: 实验表明，ASMS 在各种网络条件下比现有方法优化用户体验至少14%。

Conclusion: ASMS 提供了无缝且沉浸式的流媒体体验，即使在动态和资源受限的网络中也能保护用户隐私。

Abstract: The social metaverse is a growing digital ecosystem that blends virtual and
physical worlds. It allows users to interact socially, work, shop, and enjoy
entertainment. However, privacy remains a major challenge, as immersive
interactions require continuous collection of biometric and behavioral data. At
the same time, ensuring high-quality, low-latency streaming is difficult due to
the demands of real-time interaction, immersive rendering, and bandwidth
optimization. To address these issues, we propose ASMS (Adaptive Social
Metaverse Streaming), a novel streaming system based on Federated Multi-Agent
Proximal Policy Optimization (F-MAPPO). ASMS leverages F-MAPPO, which
integrates federated learning (FL) and deep reinforcement learning (DRL) to
dynamically adjust streaming bit rates while preserving user privacy.
Experimental results show that ASMS improves user experience by at least 14%
compared to existing streaming methods across various network conditions.
Therefore, ASMS enhances the social metaverse experience by providing seamless
and immersive streaming, even in dynamic and resource-constrained networks,
while ensuring that sensitive user data remains on local devices.

</details>


### [112] [FFINO: Factorized Fourier Improved Neural Operator for Modeling Multiphase Flow in Underground Hydrogen Storage](https://arxiv.org/abs/2506.17344)
*Tao Wang,Hewei Tang*

Key words: 地下氢储存, 神经网络, 多相流, 数值模拟, 高效模型

TL;DR: 提出了新的神经网络架构FFINO，用于快速模拟地下氢储存中的多相流问题，与现有模型相比具有更高的效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 地下氢储存是当前向低碳经济转型的重要能源储存方式，快速模拟氢羽迁移和压力场演变对UHS管理至关重要。

Method: 开发了FFINO神经操作器架构，并参数化了文献中的相对渗透率曲线，将其作为关键不确定性参数。

Result: FFINO比现有模型FMIONet减少了38.1%的可训练参数、17.6%的训练时间和12%的GPU内存成本，且预测精度提高了9.8%。

Conclusion: FFINO模型在UHS问题中可以作为数值模拟的高效替代方案，显著提升了时间效率。

Abstract: Underground hydrogen storage (UHS) is a promising energy storage option for
the current energy transition to a low-carbon economy. Fast modeling of
hydrogen plume migration and pressure field evolution is crucial for UHS field
management. In this study, we propose a new neural operator architecture,
FFINO, as a fast surrogate model for multiphase flow problems in UHS. We
parameterize experimental relative permeability curves reported in the
literature and include them as key uncertainty parameters in the FFINO model.
We also compare the FFINO model with the state-of-the-art FMIONet model through
a comprehensive combination of metrics. Our new FFINO model has 38.1% fewer
trainable parameters, 17.6% less training time, and 12% less GPU memory cost
compared to FMIONet. The FFINO model also achieves a 9.8% accuracy improvement
in predicting hydrogen plume in focused areas, and 18% higher RMSE in
predicting pressure buildup. The inference time of the trained FFINO model is
7850 times faster than a numerical simulator, which makes it a competent
substitute for numerical simulations of UHS problems with superior time
efficiency.

</details>


### [113] [SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification](https://arxiv.org/abs/2506.17368)
*Zhenglin Lai,Mengyao Liao,Dong Xu,Zebin Zhao,Zhihang Yuan,Chao Fan,Jianqiang Li,Bingzhe Wu*

Key words: Mixture-of-Experts, 大型语言模型, 安全对齐, 位置脆弱性, SAFEx

TL;DR: 该论文研究了基于Mixture-of-Experts（MoE）的大型语言模型的安全对齐问题，揭示了MoE模型的位置脆弱性，并提出了一种名为SAFEx的分析框架，通过稳定性选择算法识别安全关键专家模块。实验表明，仅禁用少量专家模块会显著降低模型的安全性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有安全对齐策略主要针对密集模型设计，无法有效应对MoE模型的独特架构风险。本研究旨在填补这一空白，系统分析MoE模型的位置脆弱性问题。

Method: 提出了SAFEx框架，采用稳定性选择算法（SES）识别、表征和验证安全关键专家模块，并将其分解为不同功能组。实验验证了方法的有效性。

Result: 实验表明，MoE模型的安全性高度依赖少量专家模块。例如，禁用Qwen3-MoE中12个安全关键专家会导致拒绝率下降22%。

Conclusion: MoE模型的安全机制存在位置脆弱性，仅依赖少数专家模块，禁用这些模块会显著降低安全性。本文提出的SAFEx框架为MoE模型的安全对齐提供了新思路。

Abstract: Large language models based on Mixture-of-Experts have achieved substantial
gains in efficiency and scalability, yet their architectural uniqueness
introduces underexplored safety alignment challenges. Existing safety alignment
strategies, predominantly designed for dense models, are ill-suited to address
MoE-specific vulnerabilities. In this work, we formalize and systematically
study MoE model's positional vulnerability - the phenomenon where
safety-aligned behaviors rely on specific expert modules, revealing critical
risks inherent to MoE architectures. To this end, we present SAFEx, an
analytical framework that robustly identifies, characterizes, and validates the
safety-critical experts using a novel Stability-based Expert Selection (SES)
algorithm. Notably, our approach enables the explicit decomposition of
safety-critical experts into distinct functional groups, including those
responsible for harmful content detection and those controlling safe response
generation. Extensive experiments on mainstream MoE models, such as the
recently released Qwen3-MoE, demonstrated that their intrinsic safety
mechanisms heavily rely on a small subset of positional experts. Disabling
these experts significantly compromised the models' ability to refuse harmful
requests. For Qwen3-MoE with 6144 experts (in the FNN layer), we find that
disabling as few as 12 identified safety-critical experts can cause the refusal
rate to drop by 22%, demonstrating the disproportionate impact of a small set
of experts on overall model safety.

</details>


### [114] [Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?](https://arxiv.org/abs/2506.17417)
*Mingyuan Wu,Meitang Li,Jingcheng Yang,Jize Jiang,Kaizhuo Yan,Zhaoheng Li,Minjia Zhang,Klara Nahrstedt*

Key words: 大语言模型,视觉语言模型,推理时计算,强化学习,自我验证

TL;DR: 研究表明，推理时计算技术（如解码时缩放和自我验证）能显著提升视觉语言模型（VLM）的推理能力，但自我校正行为效果不明显。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探究推理时计算技术是否适用于视觉语言模型，特别是通过强化学习训练的模型。

Method: 采用多数投票和最佳选择等解码策略，并通过实验验证其效果。

Result: 生成依赖方法（如多数投票）比验证依赖方法（如最佳选择）表现更好；自我校正行为未带来显著提升。

Conclusion: 强化学习训练的视觉语言模型在多模态中缺乏稳健的自我验证能力。

Abstract: Recent advances in large language models (LLMs) have demonstrated that
inference-time computation techniques, such as decoding-time scaling and
self-refinement, can significantly enhance reasoning capabilities without
relying on external knowledge. A key driver of this success is the emergence of
self-correction and self-verification behaviors, often elicited through
reinforcement learning (RL). In this paper, we investigate whether these
inference-time techniques extend effectively to vision-language models (VLMs),
particularly those trained with RL. We find that while decoding strategies such
as majority voting and best-of-N selection with self-verification all improve
VLM reasoning performance, generation-reliant methods such as the former
achieve significantly higher gains versus verification-reliant methods such as
the latter. Additionally, the self-correction behavior often associated with
RL-tuned models, such as aha moment, does not lead to measurable gains. We show
via extensive experimentation within the inference-time scaling framework to
identify a key root cause: RL-trained VLMs still lack robust self-verification
capabilities across both visual and textual modalities.

</details>


### [115] [FedNAMs: Performing Interpretability Analysis in Federated Learning Context](https://arxiv.org/abs/2506.17466)
*Amitash Nanda,Sree Bhargavi Balija,Debashis Sahoo*

Key words: 联邦学习, 神经加法模型, 可解释性, 隐私保护, 分类任务

TL;DR: 该论文提出了一种新型的联邦学习框架FedNAMs，结合神经加法模型（NAMs）以提高可解释性和隐私保护，同时在多类数据集上验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习在可解释性和隐私保护方面存在挑战，研究旨在通过结合NAMs和联邦学习来解决这些问题。

Method: 采用FedNAMs框架，将NAM的局部特征学习与联邦学习的去中心化结合，训练客户端特定模型。

Result: FedNAMs在文本和图像分类任务中表现出高可解释性，且精度损失极小；识别了多个关键预测特征。

Conclusion: FedNAMs显著提升了联邦学习的可解释性和隐私保护，适用于金融和医疗等领域。

Abstract: Federated learning continues to evolve but faces challenges in
interpretability and explainability. To address these challenges, we introduce
a novel approach that employs Neural Additive Models (NAMs) within a federated
learning framework. This new Federated Neural Additive Models (FedNAMs)
approach merges the advantages of NAMs, where individual networks concentrate
on specific input features, with the decentralized approach of federated
learning, ultimately producing interpretable analysis results. This integration
enhances privacy by training on local data across multiple devices, thereby
minimizing the risks associated with data centralization and improving model
robustness and generalizability. FedNAMs maintain detailed, feature-specific
learning, making them especially valuable in sectors such as finance and
healthcare. They facilitate the training of client-specific models to integrate
local updates, preserve privacy, and mitigate concerns related to
centralization. Our studies on various text and image classification tasks,
using datasets such as OpenFetch ML Wine, UCI Heart Disease, and Iris, show
that FedNAMs deliver strong interpretability with minimal accuracy loss
compared to traditional Federated Deep Neural Networks (DNNs). The research
involves notable findings, including the identification of critical predictive
features at both client and global levels. Volatile acidity, sulfates, and
chlorides for wine quality. Chest pain type, maximum heart rate, and number of
vessels for heart disease. Petal length and width for iris classification. This
approach strengthens privacy and model efficiency and improves interpretability
and robustness across diverse datasets. Finally, FedNAMs generate insights on
causes of highly and low interpretable features.

</details>


### [116] [A geometric framework for momentum-based optimizers for low-rank training](https://arxiv.org/abs/2506.17475)
*Steffen Schotthöfer,Timon Klein,Jonas Kusch*

Key words: 低秩预训练, 动态低秩近似, 动量优化, 几何结构

TL;DR: 论文研究了低秩预训练和微调技术中传统优化方法的不足，提出了一种新的训练策略，结合动态低秩近似和动量优化，显著提升了收敛速度和性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决低秩参数化训练中传统优化方法（如动量法和Adam）面临的几何结构问题，提升训练效率和效果。

Method: 引入基于动态低秩近似的新训练策略，结合几何结构设计优化器。

Result: 实验验证了新方法的快速收敛和更强性能。

Conclusion: 新方法有效解决了低秩参数化训练的几何结构问题，提升了训练效果。

Abstract: Low-rank pre-training and fine-tuning have recently emerged as promising
techniques for reducing the computational and storage costs of large neural
networks. Training low-rank parameterizations typically relies on conventional
optimizers such as heavy ball momentum methods or Adam. In this work, we
identify and analyze potential difficulties that these training methods
encounter when used to train low-rank parameterizations of weights. In
particular, we show that classical momentum methods can struggle to converge to
a local optimum due to the geometry of the underlying optimization landscape.
To address this, we introduce novel training strategies derived from dynamical
low-rank approximation, which explicitly account for the underlying geometric
structure. Our approach leverages and combines tools from dynamical low-rank
approximation and momentum-based optimization to design optimizers that respect
the intrinsic geometry of the parameter space. We validate our methods through
numerical experiments, demonstrating faster convergence, and stronger
validation metrics at given parameter budgets.

</details>


### [117] [Episode-specific Fine-tuning for Metric-based Few-shot Learners with Optimization-based Training](https://arxiv.org/abs/2506.17499)
*Xuanyu Zhuang,Geoffroy Peeters,Gaël Richard*

Key words: few-shot classification, metric-based models, fine-tuning, meta-learning, audio datasets

TL;DR: 本文提出了一种在少样本分类任务中通过微调方法改进度量模型的性能，并利用元学习框架避免过拟合的方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的度量模型在少样本分类任务中未能充分利用支持样本，仅用于相似性比较，而未能微调度量空间本身。

Method: 提出了三种微调方法（RDFT、IDFT、ADFT）用于度量模型，并结合优化元学习框架以避免过拟合。

Result: 实验表明，该方法在三个音频数据集上均显著提升了性能，特别是在基于注意力的模型中表现更佳。

Conclusion: 通过任务特定的微调和元学习的结合，度量模型能更好地适应有限的训练样本，同时避免过拟合。

Abstract: In few-shot classification tasks (so-called episodes), a small set of labeled
support samples is provided during inference to aid the classification of
unlabeled query samples. Metric-based models typically operate by computing
similarities between query and support embeddings within a learned metric
space, followed by nearest-neighbor classification. However, these labeled
support samples are often underutilized--they are only used for similarity
comparison, despite their potential to fine-tune and adapt the metric space
itself to the classes in the current episode. To address this, we propose a
series of simple yet effective episode-specific, during-inference fine-tuning
methods for metric-based models, including Rotational Division Fine-Tuning
(RDFT) and its two variants, Iterative Division Fine-Tuning (IDFT) and
Augmented Division Fine-Tuning (ADFT). These methods construct pseudo
support-query pairs from the given support set to enable fine-tuning even for
non-parametric models. Nevertheless, the severely limited amount of data in
each task poses a substantial risk of overfitting when applying such
fine-tuning strategies. To mitigate this, we further propose to train the
metric-based model within an optimization-based meta-learning framework. With
the combined efforts of episode-specific fine-tuning and optimization-based
meta-training, metric-based models are equipped with the ability to rapidly
adapt to the limited support samples during inference while avoiding
overfitting. We validate our approach on three audio datasets from diverse
domains, namely ESC-50 (environmental sounds), Speech Commands V2 (spoken
keywords), and Medley-solos-DB (musical instrument). Experimental results
demonstrate that our approach consistently improves performance for all
evaluated metric-based models (especially for attention-based models) and
generalizes well across different audio domains.

</details>


### [118] [A Survey of State Representation Learning for Deep Reinforcement Learning](https://arxiv.org/abs/2506.17518)
*Ayoub Echchahed,Pablo Samuel Castro*

Key words: 强化学习, 状态表示, 表示学习, 分类框架, 评估技术

TL;DR: 该论文综述了强化学习中的状态表示学习方法，对其进行了分类，并讨论了其机制、优势和局限性，旨在为该领域的新研究者提供指南。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决复杂观测空间中顺序决策问题的挑战，提供状态表示学习方法的分类和评估技术。

Method: 将方法分为六类，详细描述其机制、优势和局限性。

Result: 提出了一种状态表示学习方法的分类框架，并讨论了评估表示质量的技巧。

Conclusion: 该分类框架有助于理解状态表示学习领域，并为未来研究提供了方向。

Abstract: Representation learning methods are an important tool for addressing the
challenges posed by complex observations spaces in sequential decision making
problems. Recently, many methods have used a wide variety of types of
approaches for learning meaningful state representations in reinforcement
learning, allowing better sample efficiency, generalization, and performance.
This survey aims to provide a broad categorization of these methods within a
model-free online setting, exploring how they tackle the learning of state
representations differently. We categorize the methods into six main classes,
detailing their mechanisms, benefits, and limitations. Through this taxonomy,
our aim is to enhance the understanding of this field and provide a guide for
new researchers. We also discuss techniques for assessing the quality of
representations, and detail relevant future directions.

</details>


### [119] [Predicting E-commerce Purchase Behavior using a DQN-Inspired Deep Learning Model for enhanced adaptability](https://arxiv.org/abs/2506.17543)
*Aditi Madhusudan Jain*

Key words: 电子商务、深度学习、强化学习、LSTM、DQN

TL;DR: 该论文提出了一种结合深度Q网络（DQN）和长短期记忆（LSTM）的新方法，用于预测电子商务中的购买意图和产品需求，其模型在准确率和AUC-ROC评分上表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在快速发展的电子商务环境中，准确预测用户行为对库存管理、个性化用户体验和销售最大化至关重要。

Method: 模型结合了DQN的决策能力和LSTM的时序建模能力，用于处理高维和序列化的电子商务数据。

Result: 在大规模数据集上，模型达到88%的准确率和0.88的AUC-ROC评分，优于传统的机器学习和深度学习方法。

Conclusion: 该研究为电子商务分析提供了一种新的预测建模技术，显著提升了需求预测和用户体验优化的能力。

Abstract: This paper presents a novel approach to predicting buying intent and product
demand in e-commerce settings, leveraging a Deep Q-Network (DQN) inspired
architecture. In the rapidly evolving landscape of online retail, accurate
prediction of user behavior is crucial for optimizing inventory management,
personalizing user experiences, and maximizing sales. Our method adapts
concepts from reinforcement learning to a supervised learning context,
combining the sequential modeling capabilities of Long Short-Term Memory (LSTM)
networks with the strategic decision-making aspects of DQNs. We evaluate our
model on a large-scale e-commerce dataset comprising over 885,000 user
sessions, each characterized by 1,114 features. Our approach demonstrates
robust performance in handling the inherent class imbalance typical in
e-commerce data, where purchase events are significantly less frequent than
non-purchase events. Through comprehensive experimentation with various
classification thresholds, we show that our model achieves a balance between
precision and recall, with an overall accuracy of 88\% and an AUC-ROC score of
0.88. Comparative analysis reveals that our DQN-inspired model offers
advantages over traditional machine learning and standard deep learning
approaches, particularly in its ability to capture complex temporal patterns in
user behavior. The model's performance and scalability make it well-suited for
real-world e-commerce applications dealing with high-dimensional, sequential
data. This research contributes to the field of e-commerce analytics by
introducing a novel predictive modeling technique that combines the strengths
of deep learning and reinforcement learning paradigms. Our findings have
significant implications for improving demand forecasting, personalizing user
experiences, and optimizing marketing strategies in online retail environments.

</details>


### [120] [DRIMV_TSK: An Interpretable Surgical Evaluation Model for Incomplete Multi-View Rectal Cancer Data](https://arxiv.org/abs/2506.17552)
*Wei Zhang,Zi Wang,Hanwen Zhou,Zhaohong Deng,Weiping Ding,Yuxi Ge,Te Zhang,Yuanpeng Zhang,Kup-Sze Choi,Shitong Wang,Shudong Hu*

Key words: 直肠癌、多视角学习、手术评估、TSK模糊系统、信息熵

TL;DR: 本文提出了一种可解释的不完整多视角手术评估模型，用于直肠癌手术难度的可靠评估。通过构建多视角数据集和双表示学习模型，结合TSK模糊系统，显著提升了评估效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有直肠癌手术难度评估方法主要基于临床数据，但随着技术发展，更多数据可以被收集。人工智能的应用为更全面的评估提供了可能。

Method: 构建多视角直肠癌数据集（高分辨率MRI、压脂MRI和临床数据），提出双表示不完整多视角学习模型，结合TSK模糊系统和信息熵优化视图权重。

Result: 在MVRC数据集上，提出的DRIMV_TSK模型表现优于其他先进算法。

Conclusion: 该模型通过整合多视角数据和双重表示学习，提高了直肠癌手术难度评估的准确性和可解释性。

Abstract: A reliable evaluation of surgical difficulty can improve the success of the
treatment for rectal cancer and the current evaluation method is based on
clinical data. However, more data about rectal cancer can be collected with the
development of technology. Meanwhile, with the development of artificial
intelligence, its application in rectal cancer treatment is becoming possible.
In this paper, a multi-view rectal cancer dataset is first constructed to give
a more comprehensive view of patients, including the high-resolution MRI image
view, pressed-fat MRI image view, and clinical data view. Then, an
interpretable incomplete multi-view surgical evaluation model is proposed,
considering that it is hard to obtain extensive and complete patient data in
real application scenarios. Specifically, a dual representation incomplete
multi-view learning model is first proposed to extract the common information
between views and specific information in each view. In this model, the missing
view imputation is integrated into representation learning, and second-order
similarity constraint is also introduced to improve the cooperative learning
between these two parts. Then, based on the imputed multi-view data and the
learned dual representation, a multi-view surgical evaluation model with the
TSK fuzzy system is proposed. In the proposed model, a cooperative learning
mechanism is constructed to explore the consistent information between views,
and Shannon entropy is also introduced to adapt the view weight. On the MVRC
dataset, we compared it with several advanced algorithms and DRIMV_TSK obtained
the best results.

</details>


### [121] [Accelerating Residual Reinforcement Learning with Uncertainty Estimation](https://arxiv.org/abs/2506.17564)
*Lakshita Dodeja,Karl Schmeckpeper,Shivam Vats,Thomas Weng,Mingxi Jia,George Konidaris,Stefanie Tellex*

Key words: 残差强化学习,随机基策略,样本效率,稀疏奖励,仿真到现实

TL;DR: Residual RL改进方法提升了样本效率并适用于随机基策略，通过利用基策略的不确定性估计和改进的离策略学习，显著优于现有基线。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有残差强化学习方法在稀疏奖励和随机基策略下表现不佳，需要改进以提高样本效率和适用性。

Method: 利用基策略的不确定性估计优化探索，改进离策略残差学习以处理随机基策略。

Result: 在Robosuite和D4RL任务中显著优于现有基线，并在真实环境中展示了零样本仿真到现实的鲁棒性。

Conclusion: 改进的残差强化学习方法在随机基策略和稀疏奖励下表现优异，具有实际应用潜力。

Abstract: Residual Reinforcement Learning (RL) is a popular approach for adapting
pretrained policies by learning a lightweight residual policy that provides
corrective actions. While Residual RL is more sample-efficient than finetuning
the entire base policy, existing methods struggle with sparse rewards and are
designed for deterministic base policies. We propose two improvements to
Residual RL that further enhance its sample efficiency and make it suitable for
stochastic base policies. First, we leverage uncertainty estimates of the base
policy to focus exploration on regions in which the base policy is not
confident. Second, we propose a simple modification to off-policy residual
learning that allows it to observe base actions and better handle stochastic
base policies. We evaluate our method with both Gaussian-based and
Diffusion-based stochastic base policies on tasks from Robosuite and D4RL, and
compare against state-of-the-art finetuning methods, demo-augmented RL methods,
and other residual RL methods. Our algorithm significantly outperforms existing
baselines in a variety of simulation benchmark environments. We also deploy our
learned polices in the real world to demonstrate their robustness with
zero-shot sim-to-real transfer.

</details>


### [122] [Towards Deeper GCNs: Alleviating Over-smoothing via Iterative Training and Fine-tuning](https://arxiv.org/abs/2506.17576)
*Furong Peng,Jinzhen Gao,Xuan Lu,Kang Liu,Yifan Huo,Sheng Wang*

Key words: GCN, over-smoothing, feature collapse, Layer-wise Gradual Training, LGT

TL;DR: 论文提出Layer-wise Gradual Training (LGT)训练策略，通过逐步构建深度GCN并保留表达能力，解决了深度GCN中因线性变换加剧特征崩溃的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 深度GCN因重复应用图拉普拉斯算子和线性变换导致特征崩溃（过平滑）。SGC去掉线性变换后特征多样性稳定，但牺牲了表达能力。需要平衡表达能力和特征多样性。

Method: 提出LGT，包含三层训练：分层训练、低秩适配和恒等初始化，逐步构建深度GCN，同时保持表达能力。

Result: 实验表明LGT在32层GCN中表现优异，且能与现有方法结合进一步提升性能。

Conclusion: LGT为深度GCN提供了一种通用、架构无关的训练框架，解决了过平滑与表达能力之间的权衡问题。

Abstract: Graph Convolutional Networks (GCNs) suffer from severe performance
degradation in deep architectures due to over-smoothing. While existing studies
primarily attribute the over-smoothing to repeated applications of graph
Laplacian operators, our empirical analysis reveals a critical yet overlooked
factor: trainable linear transformations in GCNs significantly exacerbate
feature collapse, even at moderate depths (e.g., 8 layers). In contrast,
Simplified Graph Convolution (SGC), which removes these transformations,
maintains stable feature diversity up to 32 layers, highlighting linear
transformations' dual role in facilitating expressive power and inducing
over-smoothing. However, completely removing linear transformations weakens the
model's expressive capacity.
  To address this trade-off, we propose Layer-wise Gradual Training (LGT), a
novel training strategy that progressively builds deep GCNs while preserving
their expressiveness. LGT integrates three complementary components: (1)
layer-wise training to stabilize optimization from shallow to deep layers, (2)
low-rank adaptation to fine-tune shallow layers and accelerate training, and
(3) identity initialization to ensure smooth integration of new layers and
accelerate convergence. Extensive experiments on benchmark datasets demonstrate
that LGT achieves state-of-the-art performance on vanilla GCN, significantly
improving accuracy even in 32-layer settings. Moreover, as a training method,
LGT can be seamlessly combined with existing methods such as PairNorm and
ContraNorm, further enhancing their performance in deeper networks. LGT offers
a general, architecture-agnostic training framework for scalable deep GCNs. The
code is available at [https://github.com/jfklasdfj/LGT_GCN].

</details>


### [123] [LFR-PINO: A Layered Fourier Reduced Physics-Informed Neural Operator for Parametric PDEs](https://arxiv.org/abs/2506.17582)
*Jing Wang,Biao Chen,Hairun Xie,Rui Wang,Yifan Xia,Jifa Zhang,Hui Xu*

Key words: 物理信息神经算子、PDE求解、超网络、频域降维、计算效率

TL;DR: LFR-PINO是一种新型物理信息神经算子，通过分层超网络架构和频域降维策略，显著提升了参数量效率和计算效率，在四个PDE问题上实现了22.8%-68.7%的误差降低。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统物理信息神经算子在处理参数化PDE时，存在表达能力受限或计算复杂度高的问题，因此需要一种更高效且通用的解决方案。

Method: LFR-PINO引入了分层超网络架构（为每层生成专用参数）和频域降维策略（减少参数数量但保留关键频谱特征），通过预训练实现通用PDE求解器的学习。

Result: 在四个代表性PDE问题上，LFR-PINO比现有基线方法误差降低22.8%-68.7%，内存使用减少28.6%-69.3%，同时保持求解精度。

Conclusion: LFR-PINO通过创新设计在计算效率和求解精确度间达到最优平衡，为参数化PDE求解提供了高效且通用的解决方案。

Abstract: Physics-informed neural operators have emerged as a powerful paradigm for
solving parametric partial differential equations (PDEs), particularly in the
aerospace field, enabling the learning of solution operators that generalize
across parameter spaces. However, existing methods either suffer from limited
expressiveness due to fixed basis/coefficient designs, or face computational
challenges due to the high dimensionality of the parameter-to-weight mapping
space. We present LFR-PINO, a novel physics-informed neural operator that
introduces two key innovations: (1) a layered hypernetwork architecture that
enables specialized parameter generation for each network layer, and (2) a
frequency-domain reduction strategy that significantly reduces parameter count
while preserving essential spectral features. This design enables efficient
learning of a universal PDE solver through pre-training, capable of directly
handling new equations while allowing optional fine-tuning for enhanced
precision. The effectiveness of this approach is demonstrated through
comprehensive experiments on four representative PDE problems, where LFR-PINO
achieves 22.8%-68.7% error reduction compared to state-of-the-art baselines.
Notably, frequency-domain reduction strategy reduces memory usage by
28.6%-69.3% compared to Hyper-PINNs while maintaining solution accuracy,
striking an optimal balance between computational efficiency and solution
fidelity.

</details>


### [124] [Towards Fundamental Limits for Active Multi-distribution Learning](https://arxiv.org/abs/2506.17607)
*Chicheng Zhang,Yihan Zhou*

Key words: 多分布学习, 主动学习, 标签复杂度, VC维, 分歧系数

TL;DR: 该论文研究了主动多分布学习问题，提出了新算法并改进了标签复杂度的上下界，分别在分布依赖和分布无关设置中取得了更好的结果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多分布学习在协作学习、公平性和鲁棒性中有重要应用，但主动多分布学习的研究较少，且现有算法的优化性未知。

Method: 开发了新的主动多分布学习算法，分析了在可实现和不可实现设置中的标签复杂度上下界。

Result: 证明了可实现设置中的上界是信息论最优的，且在不可实现设置中的某些项是基础性的。还展示了样本复杂度的实例依赖性结果。

Conclusion: 该研究为主动多分布学习提供了理论基础和实用的算法，填补了相关领域的空白。

Abstract: Multi-distribution learning extends agnostic Probably Approximately Correct
(PAC) learning to the setting in which a family of $k$ distributions,
$\{D_i\}_{i\in[k]}$, is considered and a classifier's performance is measured
by its error under the worst distribution. This problem has attracted a lot of
recent interests due to its applications in collaborative learning, fairness,
and robustness. Despite a rather complete picture of sample complexity of
passive multi-distribution learning, research on active multi-distribution
learning remains scarce, with algorithms whose optimality remaining unknown.
  In this paper, we develop new algorithms for active multi-distribution
learning and establish improved label complexity upper and lower bounds, in
distribution-dependent and distribution-free settings. Specifically, in the
near-realizable setting we prove an upper bound of
$\widetilde{O}\Bigl(\theta_{\max}(d+k)\ln\frac{1}{\varepsilon}\Bigr)$ and
$\widetilde{O}\Bigl(\theta_{\max}(d+k)\Bigl(\ln\frac{1}{\varepsilon}+\frac{\nu^2}{\varepsilon^2}\Bigr)+\frac{k\nu}{\varepsilon^2}\Bigr)$
in the realizable and agnostic settings respectively, where $\theta_{\max}$ is
the maximum disagreement coefficient among the $k$ distributions, $d$ is the VC
dimension of the hypothesis class, $\nu$ is the multi-distribution error of the
best hypothesis, and $\varepsilon$ is the target excess error. Moreover, we
show that the bound in the realizable setting is information-theoretically
optimal and that the $k\nu/\varepsilon^2$ term in the agnostic setting is
fundamental for proper learners. We also establish instance-dependent sample
complexity bound for passive multidistribution learning that smoothly
interpolates between realizable and agnostic
regimes~\citep{blum2017collaborative,zhang2024optimal}, which may be of
independent interest.

</details>


### [125] [EQuARX: Efficient Quantized AllReduce in XLA for Distributed Machine Learning Acceleration](https://arxiv.org/abs/2506.17615)
*Ibrahim Ahmed,Clemens Schaefer,Gil Tabak,Denis Vnukov,Zenong Zhang,Felix chern,Anatoliy Yevtushenko,Andy Davis*

Key words: LLMs, quantization, AllReduce, TPU, XLA compiler, deep pipelining

TL;DR: EQuARX是一种针对TPU的动态块量化AllReduce方法，通过量化技术和计算与通信的深管道化，显著提升了大型语言模型的部署效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大型语言模型（LLMs）的部署面临巨大的规模和跨设备通信性能开销的挑战。量化技术虽能减少计算和内存需求，但直接应用于AllReduce等通信操作存在数值不稳定性问题。

Method: 提出了一种名为EQuARX的动态块量化AllReduce方法，嵌入XLA编译器，采用TPU友好的量化和计算与通信的深管道化技术。

Result: 使用int8精度的EQuARX在多种网络拓扑下比BF16 AllReduce提速1.8倍，并能分别加速Gemma 3 27B和12B模型的预填充阶段1.25倍和1.1倍，且对质量影响微小。

Conclusion: EQuARX为高效部署大型语言模型提供了一种可行的量化通信优化方案。

Abstract: While Large Language Models (LLMs) have become highly influential, their
enormous scale presents significant deployment challenges. Efficiently serving
these models typically requires distributing them across numerous accelerator
devices, which introduces substantial performance overhead from inter-device
communication (collectives). While model quantization has been widely adopted
to reduce the memory and compute requirements of LLM weights and activations
with minimal quality impact, applying quantization directly to collectives like
AllReduce is inherently difficult due to the inter-device summation involved,
which can lead to numerical instability or significant error accumulation. In
this work, we present a native dynamic block-wise efficient quantized AllReduce
within the XLA compiler for TPUs (EQuARX). By using TPU-friendly quantization
and deep pipelining of communication and compute, EQuARX with int8 precision
achieves a 1.8X speedup over baseline BF16 AllReduce across various network
topologies. Furthermore, EQuARX accelerates the prefill stage of Gemma 3 27B by
1.25X and Gemma 3 12B by 1.1X, respectively, with small to negligible impact on
quality.

</details>


### [126] [Trustworthy Chronic Disease Risk Prediction For Self-Directed Preventive Care via Medical Literature Validation](https://arxiv.org/abs/2506.17620)
*Minh Le,Khoi Ton*

Key words: 慢性病, 机器学习, 自我评估, SHAP, 可解释性

TL;DR: 论文开发了基于深度学习的模型，仅依靠个人和生活因素预测13种慢性病风险，并通过SHAP可解释性验证模型特征与医学文献的一致性，增强了模型的可信度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现有慢性病风险评估模型依赖医疗测试数据及缺乏可解释性验证的问题，为自我预防提供可信工具。

Method: 使用深度学习模型，结合个人和生活因素预测风险，并通过SHAP验证模型特征的医学依据。

Result: 模型预测的显著特征与医学文献高度一致，表明其在慢性病预测中具有广泛可信度。

Conclusion: 该研究为开发可信的自我预防机器学习工具奠定了基础，未来可进一步探索模型的伦理使用。

Abstract: Chronic diseases are long-term, manageable, yet typically incurable
conditions, highlighting the need for effective preventive strategies. Machine
learning has been widely used to assess individual risk for chronic diseases.
However, many models rely on medical test data (e.g. blood results, glucose
levels), which limits their utility for proactive self-assessment.
Additionally, to gain public trust, machine learning models should be
explainable and transparent. Although some research on self-assessment machine
learning models includes explainability, their explanations are not validated
against established medical literature, reducing confidence in their
reliability. To address these issues, we develop deep learning models that
predict the risk of developing 13 chronic diseases using only personal and
lifestyle factors, enabling accessible, self-directed preventive care.
Importantly, we use SHAP-based explainability to identify the most influential
model features and validate them against established medical literature. Our
results show a strong alignment between the models' most influential features
and established medical literature, reinforcing the models' trustworthiness.
Critically, we find that this observation holds across 13 distinct diseases,
indicating that this machine learning approach can be broadly trusted for
chronic disease prediction. This work lays the foundation for developing
trustworthy machine learning tools for self-directed preventive care. Future
research can explore other approaches for models' trustworthiness and discuss
how the models can be used ethically and responsibly.

</details>


### [127] [Exploiting Efficiency Vulnerabilities in Dynamic Deep Learning Systems](https://arxiv.org/abs/2506.17621)
*Ravishka Rathnasuriya,Wei Yang*

Key words: 动态深度学习系统,安全风险,效率攻击,对抗性输入,防御机制

TL;DR: 本文研究了动态深度学习系统（DDLSs）在输入自适应计算中的安全风险，揭示了现有系统在对抗性输入下可能导致的效率漏洞，并探讨了防御机制的改进方向。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着深度学习模型在现实环境中的广泛应用，动态深度学习系统（DDLSs）为优化运行效率提供了输入自适应计算，但其动态特性也带来了未被充分探索的安全风险。

Method: 通过调查现有攻击策略，分析现代DDLSs的效率攻击可行性，并开发针对性防御机制。

Result: 研究发现当前系统存在效率漏洞，且防御机制覆盖不足，特别是针对新兴模型架构的防御。

Conclusion: 本文呼吁进一步研究如何在对抗条件下保持DDLSs的鲁棒性，并提出针对性防御的可行性。

Abstract: The growing deployment of deep learning models in real-world environments has
intensified the need for efficient inference under strict latency and resource
constraints. To meet these demands, dynamic deep learning systems (DDLSs) have
emerged, offering input-adaptive computation to optimize runtime efficiency.
While these systems succeed in reducing cost, their dynamic nature introduces
subtle and underexplored security risks. In particular, input-dependent
execution pathways create opportunities for adversaries to degrade efficiency,
resulting in excessive latency, energy usage, and potential denial-of-service
in time-sensitive deployments. This work investigates the security implications
of dynamic behaviors in DDLSs and reveals how current systems expose efficiency
vulnerabilities exploitable by adversarial inputs. Through a survey of existing
attack strategies, we identify gaps in the coverage of emerging model
architectures and limitations in current defense mechanisms. Building on these
insights, we propose to examine the feasibility of efficiency attacks on modern
DDLSs and develop targeted defenses to preserve robustness under adversarial
conditions.

</details>


### [128] [LLM-Prompt: Integrated Heterogeneous Prompts for Unlocking LLMs in Time Series Forecasting](https://arxiv.org/abs/2506.17631)
*Zesen Wang,Yonggang Li,Lijuan Lan*

Key words: 时间序列预测、大语言模型、跨模态对齐、文本提示

TL;DR: LLM-Prompt框架通过统一文本提示范式和多模态对齐，提升了时间序列预测的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有基于大语言模型（LLM）的时间序列预测方法在长时序和数据稀缺场景表现不佳，且缺乏统一的文本提示范式和对模态差异的考虑。

Method: 提出LLM-Prompt框架，包括统一文本提示范式（可学习软提示和硬提示）以及跨模态语义对齐模块。

Result: 在6个公共数据集和3个碳排放数据集上的实验表明，LLM-Prompt是一个强大的时间序列预测框架。

Conclusion: LLM-Prompt通过多提示信息和跨模态对齐，显著提升了时间序列预测的准确性。

Abstract: Time series forecasting aims to model temporal dependencies among variables
for future state inference, holding significant importance and widespread
applications in real-world scenarios. Although deep learning-based methods have
achieved remarkable progress, they still exhibit suboptimal performance in
long-term forecasting and data-scarce scenarios. Recent research demonstrates
that large language models (LLMs) achieve promising performance in time series
forecasting. However, we find existing LLM-based methods still have
shortcomings: (1) the absence of a unified paradigm for textual prompt
formulation and (2) the neglect of modality discrepancies between textual
prompts and time series. To address this, we propose LLM-Prompt, an LLM-based
time series forecasting framework integrating multi-prompt information and
cross-modal semantic alignment. Specifically, we first construct a unified
textual prompt paradigm containing learnable soft prompts and textualized hard
prompts. Second, to enhance LLMs' comprehensive understanding of the
forecasting task, we design a semantic space embedding and cross-modal
alignment module to achieve cross-modal fusion of temporal and textual
information. Finally, the transformed time series from the LLMs are projected
to obtain the forecasts. Comprehensive evaluations on 6 public datasets and 3
carbon emission datasets demonstrate that LLM-Prompt is a powerful framework
for time series forecasting.

</details>


### [129] [Online Multi-LLM Selection via Contextual Bandits under Unstructured Context Evolution](https://arxiv.org/abs/2506.17670)
*Manhin Poon,XiangXiang Dai,Xutong Liu,Fang Kong,John C. S. Lui,Jinhang Zuo*

Key words: 大型语言模型、上下文多臂老虎机、在线学习、动态提示、成本效率

TL;DR: 研究提出了一个上下文多臂老虎机框架，用于在动态提示变化下选择大型语言模型（LLM），解决了在线多步查询优化中的LLM选择问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于LLM的多样性、成本和优势各不相同，选择最适合用户查询的LLM具有挑战性，尤其是在动态提示变化时缺乏离线数据集或模型内部信息的情况下。

Method: 提出了一个基于LinUCB的上下文多臂老虎机算法，能够在无需预测未来上下文的情况下实现次线性遗憾，并引入了预算和位置感知的扩展。

Result: 实验表明，该方法在多样化的基准测试中优于现有的LLM路由策略，具有更高的准确性和成本效率。

Conclusion: 上下文多臂老虎机框架为实时、自适应的LLM选择提供了有效的解决方案。

Abstract: Large language models (LLMs) exhibit diverse response behaviors, costs, and
strengths, making it challenging to select the most suitable LLM for a given
user query. We study the problem of adaptive multi-LLM selection in an online
setting, where the learner interacts with users through multi-step query
refinement and must choose LLMs sequentially without access to offline datasets
or model internals. A key challenge arises from unstructured context evolution:
the prompt dynamically changes in response to previous model outputs via a
black-box process, which cannot be simulated, modeled, or learned. To address
this, we propose the first contextual bandit framework for sequential LLM
selection under unstructured prompt dynamics. We formalize a notion of myopic
regret and develop a LinUCB-based algorithm that provably achieves sublinear
regret without relying on future context prediction. We further introduce
budget-aware and positionally-aware (favoring early-stage satisfaction)
extensions to accommodate variable query costs and user preferences for early
high-quality responses. Our algorithms are theoretically grounded and require
no offline fine-tuning or dataset-specific training. Experiments on diverse
benchmarks demonstrate that our methods outperform existing LLM routing
strategies in both accuracy and cost-efficiency, validating the power of
contextual bandits for real-time, adaptive LLM selection.

</details>


### [130] [Learning Personalized Utility Functions for Drivers in Ride-hailing Systems Using Ensemble Hypernetworks](https://arxiv.org/abs/2506.17672)
*Weiming Mai,Jie Gao,Oded Cats*

Key words: 网约车、司机决策、效用函数、超网络、集成学习

TL;DR: 论文提出了一种结合超网络和集成学习的方法，用于预测网约车司机的接单决策，解决了传统线性模型的局限性，并通过真实数据验证了其准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统模型（如RUM）假设线性关系，无法捕捉非线性交互和司机个性化偏好，预测准确性不足。

Method: 使用超网络动态生成线性效用函数的权重，结合集成学习提高模型适应性和泛化能力。

Result: 模型在预测准确性和不确定性估计上表现优异，同时能揭示司机的个性化偏好和关键决策因素。

Conclusion: 该方法显著提升了预测效果，兼具解释性和不确定性量化能力，是分析司机决策的有力工具。

Abstract: In ride-hailing systems, drivers decide whether to accept or reject ride
requests based on factors such as order characteristics, traffic conditions,
and personal preferences. Accurately predicting these decisions is essential
for improving the efficiency and reliability of these systems. Traditional
models, such as the Random Utility Maximization (RUM) approach, typically
predict drivers' decisions by assuming linear correlations among attributes.
However, these models often fall short because they fail to account for
non-linear interactions between attributes and do not cater to the unique,
personalized preferences of individual drivers. In this paper, we develop a
method for learning personalized utility functions using hypernetwork and
ensemble learning. Hypernetworks dynamically generate weights for a linear
utility function based on trip request data and driver profiles, capturing the
non-linear relationships. An ensemble of hypernetworks trained on different
data segments further improve model adaptability and generalization by
introducing controlled randomness, thereby reducing over-fitting. We validate
the performance of our ensemble hypernetworks model in terms of prediction
accuracy and uncertainty estimation in a real-world dataset. The results
demonstrate that our approach not only accurately predicts each driver's
utility but also effectively balances the needs for explainability and
uncertainty quantification. Additionally, our model serves as a powerful tool
for revealing the personalized preferences of different drivers, clearly
illustrating which attributes largely impact their rider acceptance decisions.

</details>


### [131] [FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies](https://arxiv.org/abs/2506.17673)
*Seonglae Cho,Harryn Oh,Donghyun Lee,Luis Eduardo Rodrigues Vieira,Andrew Bermingham,Ziad El Sayed*

Key words: 稀疏自动编码器, 模型可解释性, 虚假特征, 合成数据, 分布外数据

TL;DR: 论文提出了一种名为FaithfulSAE的方法，通过使用模型自身生成的合成数据训练稀疏自动编码器（SAE），解决了传统SAE因使用外部训练数据导致的稳定性问题和特征捕获不准确问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统稀疏自动编码器（SAE）在训练中使用外部数据集可能导致特征不稳定性（如种子初始化差异）和虚假特征（Fake Features）生成的问题。这些问题源于外部数据可能超出模型的泛化能力。

Method: 提出FaithfulSAE方法，通过训练SAE时使用模型自身生成的合成数据集，而非外部数据，以减少分布外（OOD）数据的影响。

Result: FaithfulSAE在稳定性（跨种子初始化）和特征捕获准确性（SAE探测任务）方面优于传统方法，且在7个模型中有5个表现出更低的虚假特征比率。

Conclusion: FaithfulSAE通过消除对外部数据集的依赖，提升了SAE的可解释性，同时强调了SAE训练数据集的重要性。

Abstract: Sparse Autoencoders (SAEs) have emerged as a promising solution for
decomposing large language model representations into interpretable features.
However, Paulo and Belrose (2025) have highlighted instability across different
initialization seeds, and Heap et al. (2025) have pointed out that SAEs may not
capture model-internal features. These problems likely stem from training SAEs
on external datasets - either collected from the Web or generated by another
model - which may contain out-of-distribution (OOD) data beyond the model's
generalisation capabilities. This can result in hallucinated SAE features,
which we term "Fake Features", that misrepresent the model's internal
activations. To address these issues, we propose FaithfulSAE, a method that
trains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we
demonstrate that training SAEs on less-OOD instruction datasets results in SAEs
being more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained
on web-based datasets in the SAE probing task and exhibit a lower Fake Feature
Ratio in 5 out of 7 models. Overall, our approach eliminates the dependency on
external datasets, advancing interpretability by better capturing
model-internal features while highlighting the often neglected importance of
SAE training datasets.

</details>


### [132] [Enhancing Stress-Strain Predictions with Seq2Seq and Cross-Attention based on Small Punch Test](https://arxiv.org/abs/2506.17680)
*Zhengni Yang,Rui Yang,Weijian Han,Qixin Liu*

Key words: 深度学习，小冲孔试验，Gramian Angular Field，Seq2Seq模型，LSTM，应力-应变曲线

TL;DR: 提出了一种利用深度学习从小冲孔试验数据预测高强度钢真实应力-应变曲线的新方法，结合GAF图像转换和Seq2Seq模型，提高了预测精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统实验方法在预测材料真实应力-应变关系时存在精度和效率问题，因此需要一种更高效且准确的替代方案。

Method: 利用Gramian Angular Field (GAF)将载荷-位移序列转换为图像，捕捉时空特征，并采用基于LSTM的Seq2Seq模型，结合多头交叉注意力以提高精度。

Result: 实验结果显示，预测的最小和最大平均绝对误差分别为0.15 MPa和5.58 MPa，表现出较高的预测准确性。

Conclusion: 该方法为材料科学中的真实应力-应变关系预测提供了高效且准确的替代方案，优于传统实验技术。

Abstract: This paper introduces a novel deep-learning approach to predict true
stress-strain curves of high-strength steels from small punch test (SPT)
load-displacement data. The proposed approach uses Gramian Angular Field (GAF)
to transform load-displacement sequences into images, capturing
spatial-temporal features and employs a Sequence-to-Sequence (Seq2Seq) model
with an LSTM-based encoder-decoder architecture, enhanced by multi-head
cross-attention to improved accuracy. Experimental results demonstrate that the
proposed approach achieves superior prediction accuracy, with minimum and
maximum mean absolute errors of 0.15 MPa and 5.58 MPa, respectively. The
proposed method offers a promising alternative to traditional experimental
techniques in materials science, enhancing the accuracy and efficiency of true
stress-strain relationship predictions.

</details>


### [133] [CEGA: A Cost-Effective Approach for Graph-Based Model Extraction and Acquisition](https://arxiv.org/abs/2506.17709)
*Zebin Wang,Menghan Lin,Bolin Shen,Ken Anderson,Molei Liu,Tianxi Cai,Yushun Dong*

Key words: 图神经网络, 模型提取攻击, 节点查询策略, 低成本标注, 迭代优化

TL;DR: 提出了一个针对图神经网络的节点查询策略，用于在标注成本高或批量查询受限的情况下高效提取模型，同时探讨了GNN在安全性和研究应用中的双重角色。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究GNN在多应用中面临的模型提取攻击威胁，同时探索其在低资源研究环境中的潜力，特别是在标注成本高的领域。

Method: 提出了一种迭代优化的节点查询策略，利用历史反馈改进提取效率，适用于批量查询受限的场景。

Result: 在基准图数据集上，该方法在准确性、保真度和F1分数上优于基线，展示了GNN的易攻击性和高效提取潜力。

Conclusion: 研究揭示了GNN的安全脆弱性，同时为低资源环境提供了一种高效、伦理的模型获取方法。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable utility across
diverse applications, and their growing complexity has made Machine Learning as
a Service (MLaaS) a viable platform for scalable deployment. However, this
accessibility also exposes GNN to serious security threats, most notably model
extraction attacks (MEAs), in which adversaries strategically query a deployed
model to construct a high-fidelity replica. In this work, we evaluate the
vulnerability of GNNs to MEAs and explore their potential for cost-effective
model acquisition in non-adversarial research settings. Importantly, adaptive
node querying strategies can also serve a critical role in research,
particularly when labeling data is expensive or time-consuming. By selectively
sampling informative nodes, researchers can train high-performing GNNs with
minimal supervision, which is particularly valuable in domains such as
biomedicine, where annotations often require expert input. To address this, we
propose a node querying strategy tailored to a highly practical yet
underexplored scenario, where bulk queries are prohibited, and only a limited
set of initial nodes is available. Our approach iteratively refines the node
selection mechanism over multiple learning cycles, leveraging historical
feedback to improve extraction efficiency. Extensive experiments on benchmark
graph datasets demonstrate our superiority over comparable baselines on
accuracy, fidelity, and F1 score under strict query-size constraints. These
results highlight both the susceptibility of deployed GNNs to extraction
attacks and the promise of ethical, efficient GNN acquisition methods to
support low-resource research environments.

</details>


### [134] [Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains](https://arxiv.org/abs/2506.17718)
*Zhuo He,Shuang Li,Wenze Song,Longhui Yuan,Jian Liang,Han Li,Kun Gai*

Key words: 动态域泛化, 结构因果模型, 因果表示学习, 时序泛化

TL;DR: 提出了一种名为SYNC的方法，通过时间感知的结构因果模型和动态因果表示学习，解决了现有方法在处理动态数据分布时的虚假相关性问题，显著提升了模型的时序泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法在处理动态数据分布时容易因虚假相关性而影响泛化能力，因此需要一种能够捕捉动态因果机制的方法。

Method: 设计了时间感知的结构因果模型，结合信息论目标和序列VAE框架，学习动态因果表示，保持因果因子的类内紧凑性。

Result: 在合成和真实数据集上，SYNC表现出优越的时序泛化性能。

Conclusion: SYNC通过动态因果表示学习有效提升了模型在动态数据分布下的泛化能力。

Abstract: Endowing deep models with the ability to generalize in dynamic scenarios is
of vital significance for real-world deployment, given the continuous and
complex changes in data distribution. Recently, evolving domain generalization
(EDG) has emerged to address distribution shifts over time, aiming to capture
evolving patterns for improved model generalization. However, existing EDG
methods may suffer from spurious correlations by modeling only the dependence
between data and targets across domains, creating a shortcut between
task-irrelevant factors and the target, which hinders generalization. To this
end, we design a time-aware structural causal model (SCM) that incorporates
dynamic causal factors and the causal mechanism drifts, and propose
\textbf{S}tatic-D\textbf{YN}amic \textbf{C}ausal Representation Learning
(\textbf{SYNC}), an approach that effectively learns time-aware causal
representations. Specifically, it integrates specially designed
information-theoretic objectives into a sequential VAE framework which captures
evolving patterns, and produces the desired representations by preserving
intra-class compactness of causal factors both across and within domains.
Moreover, we theoretically show that our method can yield the optimal causal
predictor for each time domain. Results on both synthetic and real-world
datasets exhibit that SYNC can achieve superior temporal generalization
performance.

</details>


### [135] [Physics-informed mixture of experts network for interpretable battery degradation trajectory computation amid second-life complexities](https://arxiv.org/abs/2506.17755)
*Xinghao Huang,Shengyu Tao,Chen Liang,Jiawei Chen,Junzhe Shi,Yuqi Li,Bizhong Xia,Guangmin Zhou,Xuan Zhang*

Key words: 退役电动汽车电池；物理学知识网络；PIMOE；退化轨迹；第二寿命储能；低碳能源

TL;DR: 提出了一种物理学知识混合专家网络（PIMOE），通过部分信号预测退役电动汽车电池的退化轨迹，提高第二寿命储能系统的安全性和可扩展性部署。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 退役电动汽车电池在低碳能源系统中潜力巨大，但退化行为的不确定性和数据不可访问性阻碍了其安全与规模化应用。

Method: PIMOE结合自适应多退化预测模块，通过容量-电压和松弛数据分类退化模式，生成潜在退化趋势嵌入，并使用依赖使用的循环网络进行长期轨迹预测。

Result: 在77种使用条件和67,902个循环中对207块电池验证后，PIMOE的平均MAPE为0.88%，推理时间为0.43毫秒，计算时间和MAPE分别比现有方法减少50%。

Conclusion: PIMOE提供了一种无需历史数据的可部署解决方案，重新定义了第二寿命储能系统的评估、优化和可持续能源集成方式。

Abstract: Retired electric vehicle batteries offer immense potential to support
low-carbon energy systems, but uncertainties in their degradation behavior and
data inaccessibilities under second-life use pose major barriers to safe and
scalable deployment. This work proposes a Physics-Informed Mixture of Experts
(PIMOE) network that computes battery degradation trajectories using partial,
field-accessible signals in a single cycle. PIMOE leverages an adaptive
multi-degradation prediction module to classify degradation modes using expert
weight synthesis underpinned by capacity-voltage and relaxation data, producing
latent degradation trend embeddings. These are input to a use-dependent
recurrent network for long-term trajectory prediction. Validated on 207
batteries across 77 use conditions and 67,902 cycles, PIMOE achieves an average
mean absolute percentage (MAPE) errors of 0.88% with a 0.43 ms inference time.
Compared to the state-of-the-art Informer and PatchTST, it reduces
computational time and MAPE by 50%, respectively. Compatible with random state
of charge region sampling, PIMOE supports 150-cycle forecasts with 1.50%
average and 6.26% maximum MAPE, and operates effectively even with pruned 5MB
training data. Broadly, PIMOE framework offers a deployable, history-free
solution for battery degradation trajectory computation, redefining how
second-life energy storage systems are assessed, optimized, and integrated into
the sustainable energy landscape.

</details>


### [136] [Towards a Unified Textual Graph Framework for Spectral Reasoning via Physical and Chemical Information Fusion](https://arxiv.org/abs/2506.17761)
*Jiheng Liang,Ziru Yu,Zujie Xie,Yuchen Guo,Yulan Guo,Xiangyang Yu*

Key words: 多模态光谱分析, 知识图, 大语言模型, 图神经网络, 可解释性

TL;DR: 提出了一种新型多模态光谱分析框架，通过结合先验知识图和大语言模型，解决了现有方法的单模态依赖、泛化性差和可解释性低的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有光谱分析方法存在单模态数据依赖、泛化性差和可解释性不足的问题，亟需一种能够整合物理测量与化学语义的统一框架。

Method: 通过将原始光谱转换为文本图格式，并结合先验知识形成任务图，利用图神经网络和图推理完成下游任务。

Result: 该方法在多光谱分析任务中表现优异，支持零样本和少样本学习，展现强大的泛化能力。

Conclusion: 该框架为基于大语言模型的光谱分析提供了可扩展和可解释的基础，统一了物理和化学模态。

Abstract: Motivated by the limitations of current spectral analysis methods-such as
reliance on single-modality data, limited generalizability, and poor
interpretability-we propose a novel multi-modal spectral analysis framework
that integrates prior knowledge graphs with Large Language Models. Our method
explicitly bridges physical spectral measurements and chemical structural
semantics by representing them in a unified Textual Graph format, enabling
flexible, interpretable, and generalizable spectral understanding. Raw spectra
are first transformed into TAGs, where nodes and edges are enriched with
textual attributes describing both spectral properties and chemical context.
These are then merged with relevant prior knowledge-including functional groups
and molecular graphs-to form a Task Graph that incorporates "Prompt Nodes"
supporting LLM-based contextual reasoning. A Graph Neural Network further
processes this structure to complete downstream tasks. This unified design
enables seamless multi-modal integration and automated feature decoding with
minimal manual annotation. Our framework achieves consistently high performance
across multiple spectral analysis tasks, including node-level, edge-level, and
graph-level classification. It demonstrates robust generalization in both
zero-shot and few-shot settings, highlighting its effectiveness in learning
from limited data and supporting in-context reasoning. This work establishes a
scalable and interpretable foundation for LLM-driven spectral analysis,
unifying physical and chemical modalities for scientific applications.

</details>


### [137] [Log-Normal Multiplicative Dynamics for Stable Low-Precision Training of Large Networks](https://arxiv.org/abs/2506.17768)
*Keigo Nishida,Eren Mehmet Kıral,Kenichi Bannai,Mohammad Emtiyaz Khan,Thomas Möllenhoff*

Key words: 神经科学,对数正态分布,多重动力学,人工神经网络,低精度训练

TL;DR: 本文提出了一种基于对数正态分布的后验概率的贝叶斯学习规则，设计了Log-Normal Multiplicative Dynamics (LMD)算法，用于人工神经网络的多重训练。该方法在低精度操作下实现了稳定和准确的训练，展示了生物特征在高效硬件上的潜力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 受生物神经系统稳定性的启发，探索在人工神经网络中实现类似的多重训练方法，以适应动态波动条件。

Method: 推导了一个假设权重对数正态后验分布的贝叶斯学习规则，并设计了LMD算法。算法采用多重更新、噪声和正则化，易于实现。

Result: LMD在Vision Transformer和GPT-2的低精度前向操作下实现了稳定且准确的训练。

Conclusion: 生物的多重动力学特征可能为未来能效硬件上的低精度推理和学习提供稳定性。

Abstract: Studies in neuroscience have shown that biological synapses follow a
log-normal distribution whose transitioning can be explained by noisy
multiplicative dynamics. Biological networks can function stably even under
dynamically fluctuating conditions arising due to unreliable synaptic
transmissions. Here we ask: Is it possible to design similar multiplicative
training in artificial neural networks? To answer this question, we derive a
Bayesian learning rule that assumes log-normal posterior distributions over
weights which gives rise to a new Log-Normal Multiplicative Dynamics (LMD)
algorithm. The algorithm uses multiplicative updates with both noise and
regularization applied multiplicatively. The method is as easy to implement as
Adam and only requires one additional vector to store. Our results show that
LMD achieves stable and accurate training-from-scratch under low-precision
forward operations for Vision Transformer and GPT-2. These results suggest that
multiplicative dynamics, a biological feature, may enable stable low-precision
inference and learning on future energy-efficient hardware.

</details>


### [138] [PhysiX: A Foundation Model for Physics Simulations](https://arxiv.org/abs/2506.17774)
*Tung Nguyen,Arsh Koneru,Shufan Li,Aditya grover*

Key words: 物理模拟, 基础模型, 数据稀缺, 自回归模型, 多任务学习

TL;DR: PhysiX是一个4.5B参数的自回归生成模型，解决了物理模拟中的数据稀缺问题，并在多任务训练中表现出色。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 物理模拟领域由于数据稀缺和规模多样性，难以应用大型基础模型。PhysiX旨在解决这一问题。

Method: PhysiX使用离散标记器和自回归预测目标，结合专门的细化模块，对不同规模的物理过程进行建模。

Result: PhysiX在数据瓶颈和多任务训练中表现优异，超越了特定任务的基线方法和之前的最先进方法。

Conclusion: PhysiX表明，从自然视频中学到的知识可以成功转移到物理模拟中，跨任务联合训练可实现协同学习。

Abstract: Foundation models have achieved remarkable success across video, image, and
language domains. By scaling up the number of parameters and training datasets,
these models acquire generalizable world knowledge and often surpass
task-specific approaches. However, such progress has yet to extend to the
domain of physics simulation. A primary bottleneck is data scarcity: while
millions of images, videos, and textual resources are readily available on the
internet, the largest physics simulation datasets contain only tens of
thousands of samples. This data limitation hinders the use of large models, as
overfitting becomes a major concern. As a result, physics applications
typically rely on small models, which struggle with long-range prediction due
to limited context understanding. Additionally, unlike images, videos, or
text-which typically exhibit fixed granularity-physics datasets often vary
drastically in scale, amplifying the challenges of scaling up multitask
training. We introduce PhysiX, the first large-scale foundation model for
physics simulation. PhysiX is a 4.5B parameter autoregressive generative model.
It uses a discrete tokenizer to encode physical processes at different scales
into a sequence of discrete tokens, and employs an autoregressive next-token
prediction objective to model such processes in the token space. To mitigate
the rounding error in the discretization process, PhysiX incorporates a
specialized refinement module. Through extensive experiments, we show that
PhysiX effectively addresses the data bottleneck, outperforming task-specific
baselines under comparable settings as well as the previous absolute
state-of-the-art approaches on The Well benchmark. Our results indicate that
knowledge learned from natural videos can be successfully transferred to
physics simulation, and that joint training across diverse simulation tasks
enables synergistic learning.

</details>


### [139] [Machine Learning Model Integration with Open World Temporal Logic for Process Automation](https://arxiv.org/abs/2506.17776)
*Dyuman Aditya,Colton Payne,Mario Leiva,Paulo Shakarian*

Key words: 机器学习, PyReason, 逻辑编程, 实时决策, 知识图谱

TL;DR: 将机器学习模型输出与PyReason框架结合，实现实时自适应决策。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决机器学习的感知输出如何在复杂工作流中转换为可行动决策的挑战。

Method: 集成多种ML模型输出至PyReason框架，利用其逻辑编程引擎实现动态推理。

Result: 构建了一个支持时序推理、知识图谱集成和可解释接口的强大系统。

Conclusion: ML模型与PyReason的结合为复杂流程自动化提供了高效解决方案。

Abstract: Recent advancements in Machine Learning (ML) have yielded powerful models
capable of extracting structured information from diverse and complex data
sources. However, a significant challenge lies in translating these perceptual
or extractive outputs into actionable, reasoned decisions within complex
operational workflows. To address these challenges, this paper introduces a
novel approach that integrates the outputs from various machine learning models
directly with the PyReason framework, an open-world temporal logic programming
reasoning engine. PyReason's foundation in generalized annotated logic allows
for the seamless incorporation of real-valued outputs (e.g., probabilities,
confidence scores) from diverse ML models, treating them as truth intervals
within its logical framework. Crucially, PyReason provides mechanisms,
implemented in Python, to continuously poll ML model outputs, convert them into
logical facts, and dynamically recompute the minimal model, ensuring real-tine
adaptive decision-making. Furthermore, its native support for temporal
reasoning, knowledge graph integration, and fully explainable interface traces
enables sophisticated analysis over time-sensitive process data and existing
organizational knowledge. By combining the strengths of perception and
extraction from ML models with the logical deduction and transparency of
PyReason, we aim to create a powerful system for automating complex processes.
This integration finds utility across numerous domains, including
manufacturing, healthcare, and business operations.

</details>


### [140] [Toward Autonomous UI Exploration: The UIExplorer Benchmark](https://arxiv.org/abs/2506.17779)
*Andrei Cristian Nica,Akshaya Vishnu Kudlu Shanbhogue,Harshil Shah,Aleix Cambray,Tudor Berariu,Lucas Maystre,David Barber*

Key words: UI探索, 基准测试, hUFO指标, 自主代理, GitLab

TL;DR: UIExplore-Bench是首个专用于UI探索的基准测试，评估代理在结构化模式和屏幕模式下的探索表现，并提出hUFO指标量化效果。UIExplore-AlGo表现最佳，但与人类专家仍有差距。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有研究缺乏对UI探索阶段系统评估，需要一个标准化的基准测试来推动相关研究。

Method: 采用UIExplore-Bench基准，分为结构化模式（访问DOM树）和屏幕模式（仅依赖GUI观察），在GitLab沙盒环境中评估代理，并提出hUFO指标。

Result: UIExplore-AlGo在结构化模式下达到77.2%的人类性能，屏幕模式下59.0%，但在稀疏层级仍有提升空间。

Conclusion: UIExplore-Bench揭示了当前代理与人类专家在UI探索上的显著差距，为未来研究提供了工具和方向。

Abstract: Autonomous agents must know how to explore user interfaces (UIs) for reliable
task solving, yet systematic evaluation of this crucial phase is lacking. We
introduce UIExplore-Bench, the first benchmark explicitly dedicated to UI
exploration. The benchmark evaluates agents with either Structured mode
(granting access to layout information like DOM trees) or Screen mode (relying
on GUI-only observations such as screenshots and human-like mouse/keyboard
interactions) across three levels in a standardized GitLab sandbox environment.
We formalize exploration as the process of maximizing the set of actionable UI
components discovered and propose a metric, human-normalized UI-Functionalities
Observed (hUFO), to quantify the effectiveness of exploration. Our results show
that UIExplore-AlGo achieves the leading mean hUFO scores, reaching up to 77.2%
of human performance in Structured mode and 59.0% in Screen mode at 2,000
steps, particularly excelling at the Sparse level. The results highlight the
relevance of our benchmark, as current agents show a substantial performance
gap compared to one hour of human expert exploration, indicating ample room for
future advancements. We publicly release the benchmark environment, an
exploration dataset, and an evaluation suite to catalyze research into
efficient UI exploration strategies and their downstream applications, such as
experience-driven task completion and automated training data generation.

</details>


### [141] [Beyond instruction-conditioning, MoTE: Mixture of Task Experts for Multi-task Embedding Models](https://arxiv.org/abs/2506.17781)
*Miguel Romero,Shuoyang Ding,Corey D. Barret,Georgiana Dinu,George Karypis*

Key words: 密集嵌入, MoTE, 任务感知对比学习, 信息检索

TL;DR: 论文分析了低容量模型中直接应用指令调节的限制，并提出了Mixture of Task Experts (MoTE)变换器块，通过任务感知对比学习提高模型生成专用嵌入的能力，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对低容量模型中指令调节的表示限制问题，研究如何在不改变指令、训练数据或推理时间的情况下提升嵌入专用化的性能。

Method: 提出MoTE变换器块，结合任务感知对比学习（TACL），利用任务专用参数生成专用嵌入。

Result: 实验表明，MoTE在检索数据集上性能提升64%，在所有数据集上提升43%，且不增加推理时间或活跃参数数量。

Conclusion: MoTE有效克服了低容量模型的限制，显著提升了嵌入专用化的性能。

Abstract: Dense embeddings are fundamental to modern machine learning systems, powering
Retrieval-Augmented Generation (RAG), information retrieval, and representation
learning. While instruction-conditioning has become the dominant approach for
embedding specialization, its direct application to low-capacity models imposes
fundamental representational constraints that limit the performance gains
derived from specialization. In this paper, we analyze these limitations and
introduce the Mixture of Task Experts (MoTE) transformer block, which leverages
task-specialized parameters trained with Task-Aware Contrastive Learning
(\tacl) to enhance the model ability to generate specialized embeddings.
Empirical results show that MoTE achieves $64\%$ higher performance gains in
retrieval datasets ($+3.27 \rightarrow +5.21$) and $43\%$ higher performance
gains across all datasets ($+1.81 \rightarrow +2.60$). Critically, these gains
are achieved without altering instructions, training data, inference time, or
number of active parameters.

</details>


### [142] [SING: SDE Inference via Natural Gradients](https://arxiv.org/abs/2506.17796)
*Amber Hu,Henry Smith,Scott Linderman*

Key words: 潜在SDE模型, 变分推断, 自然梯度, 非线性漂移函数, 神经动力学

TL;DR: 提出一种名为SING的新方法，利用自然梯度变分推断优化潜在SDE模型的推理问题，提高收敛速度和数值稳定性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于潜在SDE模型在复杂领域中精确后验推断难以处理，现有变分推断方法收敛慢且数值不稳定，因此需要更高效的方法。

Method: 利用自然梯度变分推断，通过近似难解积分和并行化时间计算，优化模型和变分后验的几何结构。

Result: SING在状态推理和漂移估计上优于现有方法，特别在神经动力学建模中表现优异。

Conclusion: SING是复杂动力学系统中准确推理的有效工具，尤其在缺乏先验知识和非共轭结构的场景。

Abstract: Latent stochastic differential equation (SDE) models are important tools for
the unsupervised discovery of dynamical systems from data, with applications
ranging from engineering to neuroscience. In these complex domains, exact
posterior inference of the latent state path is typically intractable,
motivating the use of approximate methods such as variational inference (VI).
However, existing VI methods for inference in latent SDEs often suffer from
slow convergence and numerical instability. Here, we propose SDE Inference via
Natural Gradients (SING), a method that leverages natural gradient VI to
efficiently exploit the underlying geometry of the model and variational
posterior. SING enables fast and reliable inference in latent SDE models by
approximating intractable integrals and parallelizing computations in time. We
provide theoretical guarantees that SING will approximately optimize the
intractable, continuous-time objective of interest. Moreover, we demonstrate
that better state inference enables more accurate estimation of nonlinear drift
functions using, for example, Gaussian process SDE models. SING outperforms
prior methods in state inference and drift estimation on a variety of datasets,
including a challenging application to modeling neural dynamics in freely
behaving animals. Altogether, our results illustrate the potential of SING as a
tool for accurate inference in complex dynamical systems, especially those
characterized by limited prior knowledge and non-conjugate structure.

</details>


### [143] [Reimagining Parameter Space Exploration with Diffusion Models](https://arxiv.org/abs/2506.17807)
*Lijun Zhang,Xiao Liu,Hui Guan*

Key words: 扩散模型, 任务特定参数, 多任务学习, 生成模型

TL;DR: 论文探讨了一种直接从任务标识生成任务特定参数的生成方法，利用扩散模型学习参数空间的结构，从而无需微调。该方法在已见任务上表现良好，但在未见任务上泛化能力有限。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决任务特定微调耗时长且依赖标注数据的问题，探索一种直接生成任务特定参数的替代方案。

Method: 使用扩散模型学习任务特定参数空间的结构，并根据任务标识直接合成参数。

Result: 扩散模型能够为已见任务生成准确的参数并支持多任务插值，但在未见任务上未能泛化。

Conclusion: 生成式方法在已见任务上潜力大，但在未见任务上存在局限性。

Abstract: Adapting neural networks to new tasks typically requires task-specific
fine-tuning, which is time-consuming and reliant on labeled data. We explore a
generative alternative that produces task-specific parameters directly from
task identity, eliminating the need for task-specific training. To this end, we
propose using diffusion models to learn the underlying structure of effective
task-specific parameter space and synthesize parameters on demand. Once
trained, the task-conditioned diffusion model can generate specialized weights
directly from task identifiers. We evaluate this approach across three
scenarios: generating parameters for a single seen task, for multiple seen
tasks, and for entirely unseen tasks. Experiments show that diffusion models
can generate accurate task-specific parameters and support multi-task
interpolation when parameter subspaces are well-structured, but fail to
generalize to unseen tasks, highlighting both the potential and limitations of
this generative solution.

</details>


### [144] [Flatness After All?](https://arxiv.org/abs/2506.17809)
*Neta Shoham,Liron Mor-Yosef,Haim Avron*

Key words: 泛化能力, Hessian矩阵, 软秩度量, 标定模型, Takeuchi信息准则

TL;DR: 论文提出了一种通过Hessian矩阵的软秩度量平坦性的方法，以评估神经网络的泛化能力，该方法在标定模型中能准确捕捉泛化差距。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有研究表明平坦极小值比尖锐极小值泛化更好，但在深度网络中泛化能力可能与Hessian矩阵的尖锐度无关。本文旨在通过软秩度量改进泛化评估。

Method: 使用Hessian矩阵的软秩度量平坦性，结合标定模型和非标定模型的不同分析方法。

Result: 该方法在标定模型中能准确捕捉泛化差距，非标定模型中与Takeuchi信息准则相关。实验结果优于基线方法。

Conclusion: 提出的平坦性度量方法为泛化评估提供了更可靠的指标，尤其在标定模型中表现突出。

Abstract: Recent literature has examined the relationship between the curvature of the
loss function at minima and generalization, mainly in the context of
overparameterized networks. A key observation is that "flat" minima tend to
generalize better than "sharp" minima. While this idea is supported by
empirical evidence, it has also been shown that deep networks can generalize
even with arbitrary sharpness, as measured by either the trace or the spectral
norm of the Hessian. In this paper, we argue that generalization could be
assessed by measuring flatness using a soft rank measure of the Hessian. We
show that when the common neural network model (neural network with exponential
family negative log likelihood loss) is calibrated, and its prediction error
and its confidence in the prediction are not correlated with the first and the
second derivatives of the network's output, our measure accurately captures the
asymptotic expected generalization gap. For non-calibrated models, we connect
our flatness measure to the well-known Takeuchi Information Criterion and show
that it still provides reliable estimates of generalization gaps for models
that are not overly confident. Experimental results indicate that our approach
offers a robust estimate of the generalization gap compared to baselines.

</details>


### [145] [Actionable Interpretability via Causal Hypergraphs: Unravelling Batch Size Effects in Deep Learning](https://arxiv.org/abs/2506.17826)
*Zhongtian Sun,Anoushka Harit,Pietro Lio*

Key words: 批量大小, 因果分析, 超图, 泛化性能, 深度结构因果模型

TL;DR: 论文提出了一种基于超图的因果框架HGCNet，用于研究批量大小在图和文本领域中如何通过梯度噪声、极小值锐度和模型复杂性影响泛化性能，并通过实验验证其优于现有基线模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究批量大小对泛化性能的因果机制在图和文本领域中尚未充分探索，需要一种能够捕捉高阶交互的因果分析方法。

Method: 提出HGCNet框架，利用深度结构因果模型（DSCMs）和超图捕捉训练动态中的高阶相互作用，并通过do-calculus量化批量大小干预的直接和间接效应。

Result: 实验表明HGCNet在引文网络、生物医学文本和电商评论上优于GCN、GAT、PI-GNN、BERT和RoBERTa等基线模型，揭示了较小批量通过增加随机性和平坦化极小值提升泛化性能。

Conclusion: 研究表明批量大小对泛化性能有因果影响，并提供了可操作的训练策略指导，将可解释性作为深度学习中架构和优化选择的驱动力。

Abstract: While the impact of batch size on generalisation is well studied in vision
tasks, its causal mechanisms remain underexplored in graph and text domains. We
introduce a hypergraph-based causal framework, HGCNet, that leverages deep
structural causal models (DSCMs) to uncover how batch size influences
generalisation via gradient noise, minima sharpness, and model complexity.
Unlike prior approaches based on static pairwise dependencies, HGCNet employs
hypergraphs to capture higher-order interactions across training dynamics.
Using do-calculus, we quantify direct and mediated effects of batch size
interventions, providing interpretable, causally grounded insights into
optimisation. Experiments on citation networks, biomedical text, and e-commerce
reviews show that HGCNet outperforms strong baselines including GCN, GAT,
PI-GNN, BERT, and RoBERTa. Our analysis reveals that smaller batch sizes
causally enhance generalisation through increased stochasticity and flatter
minima, offering actionable interpretability to guide training strategies in
deep learning. This work positions interpretability as a driver of principled
architectural and optimisation choices beyond post hoc analysis.

</details>


### [146] [Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach](https://arxiv.org/abs/2506.17828)
*Xinnan Zhang,Chenliang Li,Siliang Zeng,Jiaxiang Li,Zhongruo Wang,Kaixiang Lin,Songtao Lu,Alfredo Garcia,Mingyi Hong*

Key words: 大型语言模型, 对齐, 强化学习, IRO, 测试时优化

TL;DR: 提出了一种名为迭代重加权优化（IRO）的新方法，用于在不需要调整模型参数的情况下优化大型语言模型的输出质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统方法如RLHF和DPO需要直接优化模型参数，既无法在测试时使用，也不适用于无法访问模型权重的情况。测试时方法虽然避免了权重更新，但成本高且指导效果不佳。

Method: IRO通过强化学习框架，在不修改基础模型参数的情况下进行对齐。训练时迭代采样、重采样并训练轻量级价值函数，测试时用这些函数指导生成过程。

Result: IRO能够在测试时提升模型性能，且适用于无法访问模型权重的情况。

Conclusion: IRO提供了一种高效且灵活的方法，可以在不修改模型参数的情况下实现对齐。

Abstract: Aligning large language models (LLMs) with human preferences usually requires
fine-tuning methods such as RLHF and DPO. These methods directly optimize the
model parameters, so they cannot be used in test-time to improve model
performance, nor are they applicable when the model weights are not accessible.
In contrast, test-time methods sidestep weight updates by leveraging reward
functions to guide and improve output quality. However, they incur high
inference costs, and their one-shot guidance is often based on imperfect reward
or value functions, leading to suboptimal outputs. In this work, we present a
method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning
(RL) framework that performs RL-style alignment of the (frozen) base model
without touching its parameters. During training, each iteration (i) samples
candidates from the base model, (ii) resamples using current value functions,
and (iii) trains a new lightweight value function that guides the next decoding
pass. At test time, the value functions are used to guide the base model
generation via a search-based optimization process. Notably, users can apply
IRO to align a model on their own dataset, similar to OpenAI's reinforcement
fine-tuning (RFT), but without requiring access to the model weights.

</details>


### [147] [Causal Spherical Hypergraph Networks for Modelling Social Uncertainty](https://arxiv.org/abs/2506.17840)
*Anoushka Harit,Zhongtian Sun*

Key words: 社交行为建模、因果关系、不确定性、超图网络、消息传递

TL;DR: 提出了一种名为Causal-SphHN的框架，用于在不确定性和因果关系的社会动态中进行预测，通过超球嵌入和高阶结构建模实现高精度和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 人类社交行为受复杂交互影响，涉及不确定性、因果关系和群体动态，需要一种能同时建模高阶结构、方向性影响和认知不确定性的方法。

Method: 采用超球嵌入表示个体，超边表示群体上下文，结合Granger因果关系和角向消息传递机制，量化不确定性和识别因果依赖。

Result: 在SNARE、PHEME和AMIGOS数据集上的实验表明，Causal-SphHN在预测准确性、鲁棒性和校准性上优于基线方法，并能分析影响力模式和社交模糊性。

Conclusion: 本工作提出了一种统一的因果几何方法，适用于动态社交环境中的不确定性学习。

Abstract: Human social behaviour is governed by complex interactions shaped by
uncertainty, causality, and group dynamics. We propose Causal Spherical
Hypergraph Networks (Causal-SphHN), a principled framework for socially
grounded prediction that jointly models higher-order structure, directional
influence, and epistemic uncertainty. Our method represents individuals as
hyperspherical embeddings and group contexts as hyperedges, capturing semantic
and relational geometry. Uncertainty is quantified via Shannon entropy over von
Mises-Fisher distributions, while temporal causal dependencies are identified
using Granger-informed subgraphs. Information is propagated through an angular
message-passing mechanism that respects belief dispersion and directional
semantics. Experiments on SNARE (offline networks), PHEME (online discourse),
and AMIGOS (multimodal affect) show that Causal-SphHN improves predictive
accuracy, robustness, and calibration over strong baselines. Moreover, it
enables interpretable analysis of influence patterns and social ambiguity. This
work contributes a unified causal-geometric approach for learning under
uncertainty in dynamic social environments.

</details>


### [148] [A Comparative Study of Open-Source Libraries for Synthetic Tabular Data Generation: SDV vs. SynthCity](https://arxiv.org/abs/2506.17847)
*Cristian Del Gobbo*

Key words: 合成数据, 低数据环境, 统计相似性, 预测实用性, SDV, Synthicity

TL;DR: 该研究评估了六种表格合成数据生成器的性能，发现不同生成器在统计相似性和预测实用性方面表现差异明显，Bayesian Network和TVAE在不同场景中表现最佳，SDV因其易用性和文档更受青睐。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决小组织和初创公司获取高质量真实数据困难的问题，探索合成数据生成器在低数据环境下的实用性和表现。

Method: 使用SDV和Synthicity的六种生成器，在低数据环境下（1,000行）生成合成数据，并通过统计相似性和预测实用性进行评估。

Result: Bayesian Network在统计相似性方面表现最佳，TVAE在预测实用性上表现优异；SDV因其易用性更受推荐。

Conclusion: 合成数据生成器在低数据环境中具有潜力，但需根据具体需求选择生成器；SDV因其易用性更适合实际应用。

Abstract: High-quality training data is critical to the performance of machine learning
models, particularly Large Language Models (LLMs). However, obtaining real,
high-quality data can be challenging, especially for smaller organizations and
early-stage startups. Synthetic data generators provide a promising solution by
replicating the statistical and structural properties of real data while
preserving privacy and scalability. This study evaluates the performance of six
tabular synthetic data generators from two widely used open-source libraries:
SDV (Gaussian Copula, CTGAN, TVAE) and Synthicity (Bayesian Network, CTGAN,
TVAE). Using a real-world dataset from the UCI Machine Learning Repository,
comprising energy consumption and environmental variables from Belgium, we
simulate a low-data regime by training models on only 1,000 rows. Each
generator is then tasked with producing synthetic datasets under two
conditions: a 1:1 (1,000 rows) and a 1:10 (10,000 rows) input-output ratio.
Evaluation is conducted using two criteria: statistical similarity, measured
via classical statistics and distributional metrics; and predictive utility,
assessed using a "Train on Synthetic, Test on Real" approach with four
regression models. While statistical similarity remained consistent across
models in both scenarios, predictive utility declined notably in the 1:10 case.
The Bayesian Network from Synthicity achieved the highest fidelity in both
scenarios, while TVAE from SDV performed best in predictive tasks under the
1:10 setting. Although no significant performance gap was found between the two
libraries, SDV stands out for its superior documentation and ease of use,
making it more accessible for practitioners.

</details>


### [149] [Pathway-based Progressive Inference (PaPI) for Energy-Efficient Continual Learning](https://arxiv.org/abs/2506.17848)
*Suyash Gaurav,Jukka Heikkonen,Jatin Chaudhary*

Key words: 持续学习,灾难性遗忘,能源效率,Pathway-based Progressive Inference

TL;DR: 该论文提出了Pathway-based Progressive Inference (PaPI)框架，通过数学方法解决持续学习中的灾难性遗忘和能源效率问题，相比传统方法在稳定性和能源消耗上均有显著改进。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决持续学习系统中灾难性遗忘和能源效率的双重挑战。

Method: 提出PaPI框架，通过数学严格的路径选择和适应方法，将持续学习建模为能源约束优化问题。

Result: PaPI在稳定性和能源效率上均优于传统方法，实验验证了其有效性。

Conclusion: PaPI为持续学习提供了一种高效且理论保障的解决方案。

Abstract: Continual learning systems face the dual challenge of preventing catastrophic
forgetting while maintaining energy efficiency, particularly in
resource-constrained environments. This paper introduces Pathway-based
Progressive Inference (PaPI), a novel theoretical framework that addresses
these challenges through a mathematically rigorous approach to pathway
selection and adaptation. We formulate continual learning as an
energy-constrained optimization problem and provide formal convergence
guarantees for our pathway routing mechanisms. Our theoretical analysis
demonstrates that PaPI achieves an $\mathcal{O}(K)$ improvement in the
stability-plasticity trade-off compared to monolithic architectures, where $K$
is the number of pathways. We derive tight bounds on forgetting rates using
Fisher Information Matrix analysis and prove that PaPI's energy consumption
scales with the number of active parameters rather than the total model size.
Comparative theoretical analysis shows that PaPI provides stronger guarantees
against catastrophic forgetting than Elastic Weight Consolidation (EWC) while
maintaining better energy efficiency than both EWC and Gradient Episodic Memory
(GEM). Our experimental validation confirms these theoretical advantages across
multiple benchmarks, demonstrating PaPI's effectiveness for continual learning
in energy-constrained settings. Our codes are available at
https://github.com/zser092/PAPI_FILES.

</details>


### [150] [In-Context Learning Strategies Emerge Rationally](https://arxiv.org/abs/2506.17859)
*Daniel Wurgaft,Ekdeep Singh Lubana,Core Francisco Park,Hidenori Tanaka,Gautam Reddy,Noah D. Goodman*

Key words: 上下文学习（ICL）、贝叶斯预测、Transformer、策略复杂性、任务多样性

TL;DR: 该论文通过贝叶斯预测框架统一解释Transformer模型在上下文学习（ICL）中的不同策略，揭示了模型在任务混合训练中选择策略的底层机制。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在解释为什么模型在上下文学习中会学习到不同的策略，并提出一个统一的框架来预测这些策略的形成。

Method: 提出了一种分层贝叶斯框架，将预训练视为更新不同策略后验概率的过程，推断行为则是这些策略预测的后验加权平均。

Result: 该框架几乎完美预测了Transformer模型在训练过程中的下一个标记预测，并揭示了策略选择中损失与复杂性之间的权衡。

Conclusion: 通过损失与复杂性的权衡机制，提供了一个解释和预测ICL行为的框架，同时提出了新的预测现象（例如记忆化趋势的超线性变化）。

Abstract: Recent work analyzing in-context learning (ICL) has identified a broad set of
strategies that describe model behavior in different experimental conditions.
We aim to unify these findings by asking why a model learns these disparate
strategies in the first place. Specifically, we start with the observation that
when trained to learn a mixture of tasks, as is popular in the literature, the
strategies learned by a model for performing ICL can be captured by a family of
Bayesian predictors: a memorizing predictor, which assumes a discrete prior on
the set of seen tasks, and a generalizing predictor, wherein the prior matches
the underlying task distribution. Adopting the lens of rational analysis from
cognitive science, where a learner's behavior is explained as an optimal
adaptation to data given computational constraints, we develop a hierarchical
Bayesian framework that almost perfectly predicts Transformer next token
predictions throughout training without assuming access to its weights. Under
this framework, pretraining is viewed as a process of updating the posterior
probability of different strategies, and its inference-time behavior as a
posterior-weighted average over these strategies' predictions. Our framework
draws on common assumptions about neural network learning dynamics, which make
explicit a tradeoff between loss and complexity among candidate strategies:
beyond how well it explains the data, a model's preference towards implementing
a strategy is dictated by its complexity. This helps explain well-known ICL
phenomena, while offering novel predictions: e.g., we show a superlinear trend
in the timescale for transition to memorization as task diversity is increased.
Overall, our work advances an explanatory and predictive account of ICL
grounded in tradeoffs between strategy loss and complexity.

</details>


### [151] [NestQuant: Post-Training Integer-Nesting Quantization for On-Device DNN](https://arxiv.org/abs/2506.17870)
*Jianhang Xie,Chuntao Ding,Xiaqing Li,Shenyuan Ren,Yidong Li,Zhichao Lu*

Key words: 量化、物联网、后训练量化、资源适应、模型切换

TL;DR: 本文提出了一种名为NestQuant的资源友好的后训练整数嵌套量化方法，用于在物联网设备上实现量化模型切换，适应动态资源需求并减少存储和切换开销。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决现有后训练量化方法无法适应物联网设备动态资源需求以及部署多比特宽度模型带来的存储和切换开销问题。

Method: NestQuant通过整数权重分解和嵌套机制，将量化权重按比特拆分并优化，实现在部署时仅需存储和发送一个模型，并能通过切换低比特权重适应资源变化。

Result: 实验结果表明，NestQuant在ImageNet-1K预训练DNNs上表现优异，如ResNet-101在INT8嵌套INT6时，全比特和部分比特模型分别达到78.1%和77.9%准确率，并减少约78.1%的切换开销。

Conclusion: NestQuant是一种高效的后训练量化方法，能够适应动态资源需求，同时显著减少存储和切换开销。

Abstract: Deploying quantized deep neural network (DNN) models with resource adaptation
capabilities on ubiquitous Internet of Things (IoT) devices to provide
high-quality AI services can leverage the benefits of compression and meet
multi-scenario resource requirements. However, existing dynamic/mixed precision
quantization requires retraining or special hardware, whereas post-training
quantization (PTQ) has two limitations for resource adaptation: (i) The
state-of-the-art PTQ methods only provide one fixed bitwidth model, which makes
it challenging to adapt to the dynamic resources of IoT devices; (ii) Deploying
multiple PTQ models with diverse bitwidths consumes large storage resources and
switching overheads. To this end, this paper introduces a resource-friendly
post-training integer-nesting quantization, i.e., NestQuant, for on-device
quantized model switching on IoT devices. The proposed NestQuant incorporates
the integer weight decomposition, which bit-wise splits quantized weights into
higher-bit and lower-bit weights of integer data types. It also contains a
decomposed weights nesting mechanism to optimize the higher-bit weights by
adaptive rounding and nest them into the original quantized weights. In
deployment, we can send and store only one NestQuant model and switch between
the full-bit/part-bit model by paging in/out lower-bit weights to adapt to
resource changes and reduce consumption. Experimental results on the
ImageNet-1K pretrained DNNs demonstrated that the NestQuant model can achieve
high performance in top-1 accuracy, and reduce in terms of data transmission,
storage consumption, and switching overheads. In particular, the ResNet-101
with INT8 nesting INT6 can achieve 78.1% and 77.9% accuracy for full-bit and
part-bit models, respectively, and reduce switching overheads by approximately
78.1% compared with diverse bitwidths PTQ models.

</details>


### [152] [Decoding Federated Learning: The FedNAM+ Conformal Revolution](https://arxiv.org/abs/2506.17872)
*Sree Bhargavi Balija,Amitash Nanda,Debashis Sahoo*

Key words: 联邦学习, 神经加法模型, 共形预测, 不确定性量化, 可解释性

TL;DR: FedNAM+ 是一个联邦学习框架，结合了神经加法模型和共形预测方法，实现了可解释性和可靠的置信区间估计。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现有联邦学习框架在不确定性量化、可解释性和鲁棒性方面的不足。

Method: 通过动态层级调整技术和梯度敏感图识别关键特征，提供像素级不确定性估计。

Result: 在CT扫描、MNIST和CIFAR数据集上验证，准确率高且损失低（MNIST仅0.1%），提供透明的不确定性度量。

Conclusion: FedNAM+ 是一个鲁棒、可解释且计算高效的新框架，适合联邦学习场景。

Abstract: Federated learning has significantly advanced distributed training of machine
learning models across decentralized data sources. However, existing frameworks
often lack comprehensive solutions that combine uncertainty quantification,
interpretability, and robustness. To address this, we propose FedNAM+, a
federated learning framework that integrates Neural Additive Models (NAMs) with
a novel conformal prediction method to enable interpretable and reliable
uncertainty estimation. Our method introduces a dynamic level adjustment
technique that utilizes gradient-based sensitivity maps to identify key input
features influencing predictions. This facilitates both interpretability and
pixel-wise uncertainty estimates. Unlike traditional interpretability methods
such as LIME and SHAP, which do not provide confidence intervals, FedNAM+
offers visual insights into prediction reliability. We validate our approach
through experiments on CT scan, MNIST, and CIFAR datasets, demonstrating high
prediction accuracy with minimal loss (e.g., only 0.1% on MNIST), along with
transparent uncertainty measures. Visual analysis highlights variable
uncertainty intervals, revealing low-confidence regions where model performance
can be improved with additional data. Compared to Monte Carlo Dropout, FedNAM+
delivers efficient and global uncertainty estimates with reduced computational
overhead, making it particularly suitable for federated learning scenarios.
Overall, FedNAM+ provides a robust, interpretable, and computationally
efficient framework that enhances trust and transparency in decentralized
predictive modeling.

</details>


### [153] [Choice of Scoring Rules for Indirect Elicitation of Properties with Parametric Assumptions](https://arxiv.org/abs/2506.17880)
*Lingfang Hu,Ian A. Kash*

Key words: proper scoring rules, indirect elicitation, parametric assumptions, weight choice

TL;DR: 本文探讨了通过参数化假设间接激发统计属性的任务，研究了不同权重配置对目标属性估计的影响，并发现最优权重配置通常会将某些权重设为零。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有研究广泛研究了不同属性的适当评分规则的存在和表征，但缺乏对实际应用中如何选择适当评分规则的讨论。本文旨在填补这一空白。

Method: 通过仿真研究观察权重配置对目标属性估计的影响，并建立理论框架分析两维及多维情况下的最优权重配置。

Result: 实验结果显示，最优估计通常随权重的增加而单调变化，且最佳配置往往是将某些权重设为零。理论分析支持了这一发现。

Conclusion: 本文为间接激发属性的权重选择提供了理论依据，特别在参数化假设下，简单权重配置可能更优。

Abstract: People are commonly interested in predicting a statistical property of a
random event such as mean and variance. Proper scoring rules assess the quality
of predictions and require that the expected score gets uniquely maximized at
the precise prediction, in which case we call the score directly elicits the
property. Previous research work has widely studied the existence and the
characterization of proper scoring rules for different properties, but little
literature discusses the choice of proper scoring rules for applications at
hand. In this paper, we explore a novel task, the indirect elicitation of
properties with parametric assumptions, where the target property is a function
of several directly-elicitable sub-properties and the total score is a weighted
sum of proper scoring rules for each sub-property. Because of the restriction
to a parametric model class, different settings for the weights lead to
different constrained optimal solutions. Our goal is to figure out how the
choice of weights affects the estimation of the target property and which
choice is the best. We start it with simulation studies and observe an
interesting pattern: in most cases, the optimal estimation of the target
property changes monotonically with the increase of each weight, and the best
configuration of weights is often to set some weights as zero. To understand
how it happens, we first establish the elementary theoretical framework and
then provide deeper sufficient conditions for the case of two sub-properties
and of more sub-properties respectively. The theory on 2-D cases perfectly
interprets the experimental results. In higher-dimensional situations, we
especially study the linear cases and suggest that more complex settings can be
understood with locally mapping into linear situations or using linear
approximations when the true values of sub-properties are close enough to the
parametric space.

</details>


### [154] [TROJAN-GUARD: Hardware Trojans Detection Using GNN in RTL Designs](https://arxiv.org/abs/2506.17894)
*Kiran Thorat,Amit Hasan,Caiwen Ding,Zhijie Shi*

Key words: 硬件木马检测，图神经网络，模型量化，芯片设计，GNN

TL;DR: 提出了一种新颖的框架，用于检测大规模芯片设计中的硬件木马，通过生成图嵌入并结合多种GNN模型，同时引入模型量化提升效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于芯片制造中越来越多地使用不可信的第三方工具和设计，硬件木马的隐蔽性带来了严重的安全威胁，需要高效的检测方法。

Method: 提出一种框架，生成大规模设计的图嵌入，结合多种GNN模型，并应用模型量化技术以优化训练和推理效率。

Result: 在自定义数据集上评估，达到98.66%的精确度和92.30%的召回率，验证了方法的有效性和高效性。

Conclusion: 该框架在检测大规模芯片设计中的硬件木马方面表现出色，兼具高精度和高效性。

Abstract: Chip manufacturing is a complex process, and to achieve a faster time to
market, an increasing number of untrusted third-party tools and designs from
around the world are being utilized. The use of these untrusted third party
intellectual properties (IPs) and tools increases the risk of adversaries
inserting hardware trojans (HTs). The covert nature of HTs poses significant
threats to cyberspace, potentially leading to severe consequences for national
security, the economy, and personal privacy. Many graph neural network
(GNN)-based HT detection methods have been proposed. However, they perform
poorly on larger designs because they rely on training with smaller designs.
Additionally, these methods do not explore different GNN models that are
well-suited for HT detection or provide efficient training and inference
processes. We propose a novel framework that generates graph embeddings for
large designs (e.g., RISC-V) and incorporates various GNN models tailored for
HT detection. Furthermore, our framework introduces domain-specific techniques
for efficient training and inference by implementing model quantization. Model
quantization reduces the precision of the weights, lowering the computational
requirements, enhancing processing speed without significantly affecting
detection accuracy. We evaluate our framework using a custom dataset, and our
results demonstrate a precision of 98.66% and a recall (true positive rate) of
92.30%, highlighting the effectiveness and efficiency of our approach in
detecting hardware trojans in large-scale chip designs

</details>


### [155] [Permutation Equivariant Model-based Offline Reinforcement Learning for Auto-bidding](https://arxiv.org/abs/2506.17919)
*Zhiyu Mou,Miao Xu,Wei Chen,Rongquan Bai,Chuan Yu,Jian Xu*

Key words: 强化学习,自动投标,模型学习,置换等变模型,离线Q学习

TL;DR: 论文提出了一种基于模型的强化学习自动投标方法（MRLB），通过结合真实数据和模型生成数据来扩展状态覆盖范围，并设计了PE-MORL算法以提升模型可靠性和泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的离线强化学习投标方法（ORLB）受限于数据集的状态空间覆盖，而基于模拟器的强化学习（SRLB）则存在模拟与现实的差距问题。

Method: 提出MRLB方法，学习环境模型以弥补模拟与现实的差距；设计PE-MORL算法，包括置换等变模型架构和悲观惩罚的离线Q学习方法。

Result: 实验表明，PE-MORL在真实场景中优于现有最先进的自动投标方法。

Conclusion: 结合真实数据与模型生成数据的MRLB方法有效扩展了状态空间覆盖，PE-MORL算法显著提升了自动投标的性能。

Abstract: Reinforcement learning (RL) for auto-bidding has shifted from using
simplistic offline simulators (Simulation-based RL Bidding, SRLB) to offline RL
on fixed real datasets (Offline RL Bidding, ORLB). However, ORLB policies are
limited by the dataset's state space coverage, offering modest gains. While
SRLB expands state coverage, its simulator-reality gap risks misleading
policies. This paper introduces Model-based RL Bidding (MRLB), which learns an
environment model from real data to bridge this gap. MRLB trains policies using
both real and model-generated data, expanding state coverage beyond ORLB. To
ensure model reliability, we propose: 1) A permutation equivariant model
architecture for better generalization, and 2) A robust offline Q-learning
method that pessimistically penalizes model errors. These form the Permutation
Equivariant Model-based Offline RL (PE-MORL) algorithm. Real-world experiments
show that PE-MORL outperforms state-of-the-art auto-bidding methods.

</details>


### [156] [ASTER: Adaptive Spatio-Temporal Early Decision Model for Dynamic Resource Allocation](https://arxiv.org/abs/2506.17929)
*Shulun Chen,Wei Shao,Flora D. Salim,Hao Xue*

Key words: 时空预测,决策支持,资源分配,多目标强化学习,应急响应

TL;DR: 本文提出了一种自适应时空早期决策模型（ASTER），将预测与下游决策直接结合，以提升决策支持的有效性，特别是在资源动态条件下的应急响应场景。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有时空预测研究虽提高了预测的及时性和准确性，但预测与决策的脱节限制了其实际应用效果。特别是在应急响应中，资源分配和干预的优先级高于单纯的事件预测。

Method: ASTER通过资源感知的时空交互模块（RaST）动态捕捉资源条件下的时空依赖关系，并基于多目标强化学习设计偏好导向的决策代理（Poda），将预测信号转化为资源高效的干预策略。

Result: 在四个基准数据集上的实验表明，ASTER在早期预测准确性和资源分配效果上均达到最优水平。

Conclusion: ASTER通过将预测与决策直接结合，显著提升了时空智能在决策支持中的实际效果，尤其是在资源动态变化的情境下。

Abstract: Supporting decision-making has long been a central vision in the field of
spatio-temporal intelligence. While prior work has improved the timeliness and
accuracy of spatio-temporal forecasting, converting these forecasts into
actionable strategies remains a key challenge. A main limitation is the
decoupling of the prediction and the downstream decision phases, which can
significantly degrade the downstream efficiency. For example, in emergency
response, the priority is successful resource allocation and intervention, not
just incident prediction. To this end, it is essential to propose an Adaptive
Spatio-Temporal Early Decision model (ASTER) that reforms the forecasting
paradigm from event anticipation to actionable decision support. This framework
ensures that information is directly used for decision-making, thereby
maximizing overall effectiveness. Specifically, ASTER introduces a new
Resource-aware Spatio-Temporal interaction module (RaST) that adaptively
captures long- and short-term dependencies under dynamic resource conditions,
producing context-aware spatiotemporal representations. To directly generate
actionable decisions, we further design a Preference-oriented decision agent
(Poda) based on multi-objective reinforcement learning, which transforms
predictive signals into resource-efficient intervention strategies by deriving
optimal actions under specific preferences and dynamic constraints.
Experimental results on four benchmark datasets demonstrate the
state-of-the-art performance of ASTER in improving both early prediction
accuracy and resource allocation outcomes across six downstream metrics.

</details>


### [157] [An entropy-optimal path to humble AI](https://arxiv.org/abs/2506.17940)
*Davide Bassetti,Lukáš Pospíšil,Michael Groom,Terence J. O'Kane,Illia Horenko*

Key words: Boltzmann机, 非平衡熵优化, 气候预测, AI框架

TL;DR: 论文提出了一种基于非平衡熵优化的Boltzmann机新数学框架，显著降低了成本和资源需求，并提供了模型可靠性的数学依据。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前AI模型成本高昂且过于自信，需要更高效且可靠的替代方案。

Method: 基于总概率定律的非平衡熵优化Boltzmann机框架，实现无梯度下降的高效学习。

Result: 新方法在性能和模型简洁性上优于现有工具，尤其在气候预测中表现优异。

Conclusion: 该框架为高效、可靠的AI学习提供了新途径。

Abstract: Progress of AI has led to a creation of very successful, but by no means
humble models and tools, especially regarding (i) the huge and further
exploding costs and resources they demand, and (ii) the over-confidence of
these tools with the answers they provide. Here we introduce a novel
mathematical framework for a non-equilibrium entropy-optimizing reformulation
of Boltzmann machines based on the exact law of total probability. It results
in the highly-performant, but much cheaper, gradient-descent-free learning
framework with mathematically-justified existence and uniqueness criteria, and
answer confidence/reliability measures. Comparisons to state-of-the-art AI
tools in terms of performance, cost and the model descriptor lengths on a set
of synthetic problems with varying complexity reveal that the proposed method
results in more performant and slim models, with the descriptor lengths being
very close to the intrinsic complexity scaling bounds for the underlying
problems. Applying this framework to historical climate data results in models
with systematically higher prediction skills for the onsets of La Ni\~na and El
Ni\~no climate phenomena, requiring just few years of climate data for training
- a small fraction of what is necessary for contemporary climate prediction
tools.

</details>


### [158] [Adapting Vision-Language Models for Evaluating World Models](https://arxiv.org/abs/2506.17967)
*Mariya Hendriksen,Tabish Rashid,David Bignell,Raluca Georgescu,Abdelhak Lemkhenter,Katja Hofmann,Sam Devlin,Sarah Parisot*

Key words: 世界模型, 视觉语言模型, 评估协议, 动作识别, 角色识别

TL;DR: 提出了一种基于视觉语言模型（VLM）的世界模型评估方法UNIVERSE，用于细粒度、时间敏感的动作和角色识别任务，并在不同任务格式下验证其性能，与人类判断高度一致。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的世界模型评估方法无法捕捉动作对齐和语义一致性，而视觉语言模型在生成内容评估中表现潜力，但需要针对性改进以适用于细粒度任务。

Method: 提出了UNIVERSE方法，通过全量、部分和参数高效的微调策略，在多种任务格式、上下文长度、采样策略和数据组合下优化VLM。

Result: UNIVERSE在单一模型下达到任务专用基线性能，并与人类判断高度一致。

Conclusion: UNIVERSE是一种可扩展、语义感知的世界模型评估工具，填补了现有评估方法的不足。

Abstract: World models -- generative models that simulate environment dynamics
conditioned on past observations and actions -- are gaining prominence in
planning, simulation, and embodied AI. However, evaluating their rollouts
remains a fundamental challenge, requiring fine-grained, temporally grounded
assessment of action alignment and semantic consistency -- capabilities not
captured by existing metrics. Vision-Language Models (VLMs) have shown promise
as automatic evaluators of generative content due to their strong multimodal
reasoning abilities. Yet, their use in fine-grained, temporally sensitive
evaluation tasks remains limited and requires targeted adaptation. We introduce
a evaluation protocol targeting two recognition tasks -- action recognition and
character recognition -- each assessed across binary, multiple-choice, and
open-ended formats. To support this, we present UNIVERSE (UNIfied
Vision-language Evaluator for Rollouts in Simulated Environments), a method for
adapting VLMs to rollout evaluation under data and compute constraints. We
conduct a large-scale study comparing full, partial, and parameter-efficient
finetuning across task formats, context lengths, sampling strategies, and data
compositions. The resulting unified evaluator matches the performance of
task-specific baselines using a single checkpoint. Human studies confirm strong
alignment with human judgments, establishing UNIVERSE as a scalable,
semantics-aware evaluator for world models.

</details>


### [159] [h-calibration: Rethinking Classifier Recalibration with Probabilistic Error-Bounded Objective](https://arxiv.org/abs/2506.17968)
*Wenjian Huang,Guiping Cao,Jiahao Xia,Jingkun Chen,Hao Wang,Jianguo Zhang*

Key words: 深度神经网络, 概率校准, 后校准方法, h-calibration, 学习框架

TL;DR: 该论文提出了一种名为h-calibration的概率学习框架，以解决深度神经网络输出概率的校准问题，克服了现有方法的局限性，并在实验中取得了优于传统方法的效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 深度神经网络在多个学习任务中表现出色，但其输出的概率往往存在校准不足的问题，导致概率不可靠。这激发了研究者对后校准方法的研究，旨在不牺牲预训练模型分类性能的前提下获得校准概率。

Method: 论文总结了现有方法，并将其分为三类：直观设计的方法、基于分箱的方法和基于理想校准公式的方法。针对这些方法的局限性，提出了h-calibration框架，设计了一种简单有效的后校准算法。

Result: 提出的方法不仅克服了现有方法的十种局限性，还在标准后校准基准测试中取得了最先进的性能。

Conclusion: h-calibration框架为学习有界校准概率提供了理论支持，并通过实验验证了其有效性，为相关领域学习可靠概率提供了参考。

Abstract: Deep neural networks have demonstrated remarkable performance across numerous
learning tasks but often suffer from miscalibration, resulting in unreliable
probability outputs. This has inspired many recent works on mitigating
miscalibration, particularly through post-hoc recalibration methods that aim to
obtain calibrated probabilities without sacrificing the classification
performance of pre-trained models. In this study, we summarize and categorize
previous works into three general strategies: intuitively designed methods,
binning-based methods, and methods based on formulations of ideal calibration.
Through theoretical and practical analysis, we highlight ten common limitations
in previous approaches. To address these limitations, we propose a
probabilistic learning framework for calibration called h-calibration, which
theoretically constructs an equivalent learning formulation for canonical
calibration with boundedness. On this basis, we design a simple yet effective
post-hoc calibration algorithm. Our method not only overcomes the ten
identified limitations but also achieves markedly better performance than
traditional methods, as validated by extensive experiments. We further analyze,
both theoretically and experimentally, the relationship and advantages of our
learning objective compared to traditional proper scoring rule. In summary, our
probabilistic framework derives an approximately equivalent differentiable
objective for learning error-bounded calibrated probabilities, elucidating the
correspondence and convergence properties of computational statistics with
respect to theoretical bounds in canonical calibration. The theoretical
effectiveness is verified on standard post-hoc calibration benchmarks by
achieving state-of-the-art performance. This research offers valuable reference
for learning reliable likelihood in related fields.

</details>


### [160] [Trustworthy Efficient Communication for Distributed Learning using LQ-SGD Algorithm](https://arxiv.org/abs/2506.17974)
*Hongyang Li,Lincen Bai,Caesar Wu,Mohammed Chadli,Said Mammar,Pascal Bouvry*

Key words: LQ-SGD, 分布式训练, 梯度压缩, 低秩近似, 对数量化

TL;DR: LQ-SGD是一种高效通信梯度压缩算法，结合低秩近似和对数量化技术，显著减少通信开销，同时保持训练速度和模型准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为分布式训练设计一种高效的通信梯度压缩算法，以减少通信开销并保持性能。

Method: 结合PowerSGD的低秩近似和对数量化技术，实现梯度压缩。

Result: 显著减少通信开销，同时保持收敛速度和模型准确性，且对梯度反转攻击更具抵抗力。

Conclusion: LQ-SGD为分布式学习系统提供了更高效、鲁棒的优化路径。

Abstract: We propose LQ-SGD (Low-Rank Quantized Stochastic Gradient Descent), an
efficient communication gradient compression algorithm designed for distributed
training. LQ-SGD further develops on the basis of PowerSGD by incorporating the
low-rank approximation and log-quantization techniques, which drastically
reduce the communication overhead, while still ensuring the convergence speed
of training and model accuracy. In addition, LQ-SGD and other compression-based
methods show stronger resistance to gradient inversion than traditional SGD,
providing a more robust and efficient optimization path for distributed
learning systems.

</details>


### [161] [SliceGX: Layer-wise GNN Explanation with Model-slicing](https://arxiv.org/abs/2506.17977)
*Tingting Zhu,Tingyang Chen,Yinghui Wu,Arijit Khan,Xiangyu Ke*

Key words: 图神经网络, 解释方法, 分层解释, 模型调试, 高效算法

TL;DR: SliceGX是一个新颖的图神经网络（GNN）解释方法，通过分层块和渐进式分析生成高质量的子图解释，支持目标层的输出分析，并提供高效的算法和查询接口。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有GNN解释方法缺乏对中间表征如何影响最终结果的细粒度分析，而SliceGX旨在填补这一空白，支持模型诊断和架构优化。

Method: SliceGX将GNN分割为分层块，在每个块中发现高质量解释性子图，并通过高效算法和优化技术增量生成和维护这些子图。

Result: 实验验证了SliceGX在大型真实图数据和典型GNN架构上的有效性和高效性，并展示了其在模型调试中的实用价值。

Conclusion: SliceGX通过分层解释和查询接口，显著提升了对GNN模型的理解和调试能力。

Abstract: Ensuring the trustworthiness of graph neural networks (GNNs) as black-box
models requires effective explanation methods. Existing GNN explanations
typically apply input perturbations to identify subgraphs that are responsible
for the occurrence of the final output of GNNs. However, such approaches lack
finer-grained, layer-wise analysis of how intermediate representations
contribute to the final result, capabilities that are crucial for model
diagnosis and architecture optimization. This paper introduces SliceGX, a novel
GNN explanation approach that generates explanations at specific GNN layers in
a progressive manner. Given a GNN M, a set of selected intermediate layers, and
a target layer, SliceGX automatically segments M into layer blocks ("model
slice") and discovers high-quality explanatory subgraphs in each layer block
that clarifies the occurrence of output of M at the targeted layer. Although
finding such layer-wise explanations is computationally challenging, we develop
efficient algorithms and optimization techniques that incrementally generate
and maintain these subgraphs with provable approximation guarantees.
Additionally, SliceGX offers a SPARQL-like query interface, providing
declarative access and search capacities for the generated explanations.
Through experiments on large real-world graphs and representative GNN
architectures, we verify the effectiveness and efficiency of SliceGX, and
illustrate its practical utility in supporting model debugging.

</details>


### [162] [Data Curation Matters: Model Collapse and Spurious Shift Performance Prediction from Training on Uncurated Text Embeddings](https://arxiv.org/abs/2506.17989)
*Lucas Mattioli,Youness Ait Hadichou,Sabrina Chaouche,Martin Gonzalez*

Key words: 模型崩溃, 文本嵌入, 数据筛选, 下游学习, 准确率相关性

TL;DR: 训练模型时直接使用未经过筛选的文本嵌入（TEs）可能导致模型崩溃，预测结果单一化。研究发现TE质量对下游学习有重要影响，甚至会导致虚假的准确率相关性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨未经过筛选的文本嵌入（TEs）对模型训练的影响，尤其是模型崩溃现象及其对下游任务的负面效应。

Method: 通过对比相同超参数配置下在原始表格数据及其TE对应版本上训练的模型，分析模型崩溃现象，并引入新的指标量化崩溃程度。

Result: TE单独无法作为有效的数据筛选层，其质量显著影响学习效果；模型崩溃还会导致虚假的准确率相关性。

Conclusion: 需要在嵌入表示的数据筛选和评估上采取更细致的策略，尤其在分布外场景中。

Abstract: Training models on uncurated Text Embeddings (TEs) derived from raw tabular
data can lead to a severe failure mode known as model collapse, where
predictions converge to a single class regardless of input. By comparing models
trained with identical hyper-parameter configurations on both raw tabular data
and their TE-derived counterparts, we find that collapse is a consistent
failure mode in the latter setting. We introduce a set of metrics that capture
the extent of model collapse, offering a new perspective on TE quality as a
proxy for data curation. Our results reveal that TE alone does not effectively
function as a curation layer - and that their quality significantly influences
downstream learning. More insidiously, we observe that the presence of model
collapse can yield artificially inflated and spurious Accuracy-on-the-Line
correlation. These findings highlight the need for more nuanced curation and
evaluation of embedding-based representations, particularly in
out-of-distribution settings.

</details>


### [163] [Imputation of Longitudinal Data Using GANs: Challenges and Implications for Classification](https://arxiv.org/abs/2506.18007)
*Sharon Torao Pingi,Md Abul Bashar,Richi Nayak*

Key words: 纵向数据分类,生成对抗网络,数据插补,缺失值,类不平衡

TL;DR: 综述了生成对抗网络（GANs）在纵向数据插补（LDI）中的应用，分析了其优势与局限，并指出了未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 纵向数据的多维度性和缺失值问题增加了分析的复杂性，GANs在LDI中的应用潜力激发了该综述的研究。

Method: 对GANs在LDI中的应用进行分类，评估其方法优势与局限，并总结关键研究趋势。

Result: GANs在LDI中展现出潜力，但需更灵活的方法应对纵向数据的多样挑战。

Conclusion: 未来研究应开发更有效的GANs解决方案以解决LDC中的挑战。

Abstract: Longitudinal data is commonly utilised across various domains, such as
health, biomedical, education and survey studies. This ubiquity has led to a
rise in statistical, machine and deep learning-based methods for Longitudinal
Data Classification (LDC). However, the intricate nature of the data,
characterised by its multi-dimensionality, causes instance-level heterogeneity
and temporal correlations that add to the complexity of longitudinal data
analysis. Additionally, LDC accuracy is often hampered by the pervasiveness of
missing values in longitudinal data. Despite ongoing research that draw on the
generative power and utility of Generative Adversarial Networks (GANs) to
address the missing data problem, critical considerations include statistical
assumptions surrounding longitudinal data and missingness within it, as well as
other data-level challenges like class imbalance and mixed data types that
impact longitudinal data imputation (LDI) and the subsequent LDC process in
GANs. This paper provides a comprehensive overview of how GANs have been
applied in LDI, with a focus whether GANS have adequately addressed fundamental
assumptions about the data from a LDC perspective. We propose a categorisation
of main approaches to GAN-based LDI, highlight strengths and limitations of
methods, identify key research trends, and provide promising future directions.
Our findings indicate that while GANs show great potential for LDI to improve
usability and quality of longitudinal data for tasks like LDC, there is need
for more versatile approaches that can handle the wider spectrum of challenges
presented by longitudinal data with missing values. By synthesising current
knowledge and identifying critical research gaps, this survey aims to guide
future research efforts in developing more effective GAN-based solutions to
address LDC challenges.

</details>


### [164] [Probing the Embedding Space of Transformers via Minimal Token Perturbations](https://arxiv.org/abs/2506.18011)
*Eddie Conti,Alejandro Astruc,Alvaro Parafita,Axel Brando*

Key words: Transformer, 模型可解释性, token扰动, 嵌入空间

TL;DR: 研究通过最小化token扰动对Transformer模型嵌入空间的影响，揭示罕见token导致较大偏移，验证了早期层可作为模型解释的代理。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索信息如何通过Transformer模型传播，以提升模型的可解释性。

Method: 分析最小token扰动对嵌入空间的影响，观察扰动在不同层的传播。

Result: 罕见token导致较大偏移，深层信息混合更多，早期层适合解释模型。

Conclusion: token扰动与嵌入空间偏移的结合是模型可解释性的有效工具。

Abstract: Understanding how information propagates through Transformer models is a key
challenge for interpretability. In this work, we study the effects of minimal
token perturbations on the embedding space. In our experiments, we analyze the
frequency of which tokens yield to minimal shifts, highlighting that rare
tokens usually lead to larger shifts. Moreover, we study how perturbations
propagate across layers, demonstrating that input information is increasingly
intermixed in deeper layers. Our findings validate the common assumption that
the first layers of a model can be used as proxies for model explanations.
Overall, this work introduces the combination of token perturbations and shifts
on the embedding space as a powerful tool for model interpretability.

</details>


### [165] [Generalization under Byzantine & Poisoning Attacks: Tight Stability Bounds in Robust Distributed Learning](https://arxiv.org/abs/2506.18020)
*Thomas Boudou,Batiste Le Bars,Nirupam Gupta,Aurélien Bellet*

Key words: 分布式学习, 拜占庭攻击, 数据 poisoning, 泛化误差, 算法稳定性

TL;DR: 论文研究了分布式学习中两种威胁模型（拜占庭攻击和数据 poisoning 攻击）对泛化性能的影响，首次从理论上证明拜占庭攻击对泛化的危害更大。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的研究在优化误差上对这两种威胁模型的比较较多，但它们在泛化性能上的差异尚不明确，论文旨在填补这一理论空白。

Method: 通过理论分析，比较了两种攻击模型下分布式学习算法的均匀算法稳定性，推导出泛化误差的差异。

Result: 拜占庭攻击下的算法稳定性下降更快，导致泛化误差显著高于数据 poisoning 攻击。

Conclusion: 拜占庭攻击对泛化的危害本质更大，尤其在恶意节点数量接近最大值时更为显著。

Abstract: Robust distributed learning algorithms aim to maintain good performance in
distributed and federated settings, even in the presence of misbehaving
workers. Two primary threat models have been studied: Byzantine attacks, where
misbehaving workers can send arbitrarily corrupted updates, and data poisoning
attacks, where misbehavior is limited to manipulation of local training data.
While prior work has shown comparable optimization error under both threat
models, a fundamental question remains open: How do these threat models impact
generalization? Empirical evidence suggests a gap between the two threat
models, yet it remains unclear whether it is fundamental or merely an artifact
of suboptimal attacks. In this work, we present the first theoretical
investigation into this problem, formally showing that Byzantine attacks are
intrinsically more harmful to generalization than data poisoning. Specifically,
we prove that: (i) under data poisoning, the uniform algorithmic stability of a
robust distributed learning algorithm, with optimal optimization error,
degrades by an additive factor of $\varTheta ( \frac{f}{n-f} )$, with $f$ the
number of misbehaving workers out of $n$; and (ii) In contrast, under Byzantine
attacks, the degradation is in $\mathcal{O} \big( \sqrt{ \frac{f}{n-2f}}
\big)$.This difference in stability leads to a generalization error gap that is
especially significant as $f$ approaches its maximum value $\frac{n}{2}$.

</details>


### [166] [Why Do Some Language Models Fake Alignment While Others Don't?](https://arxiv.org/abs/2506.18032)
*Abhay Sheshadri,John Hughes,Julian Michael,Alex Mallen,Arun Jose,Janus,Fabien Roger*

Key words: 大型语言模型, 伪装对齐, 有害查询, 训练阶段, 拒绝行为

TL;DR: 论文分析了25个大型语言模型中对有害查询的响应差异，发现仅5个模型在训练阶段表现出更高的服从性，其中Claude 3 Opus的动机最一致。研究还探讨了其他模型为何不伪装对齐，认为能力并非唯一原因。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨大型语言模型在不同阶段（训练与部署）对有害查询的响应差异，特别是伪装对齐行为的动机与机制。

Method: 分析了25个模型对有害查询的响应，并通过扰动场景细节研究动机；进一步探讨了其他模型不伪装对齐的原因。

Result: 仅5个模型在训练阶段更服从有害查询，Claude 3 Opus的动机最一致；其他模型不伪装对齐的原因复杂，可能与拒绝行为差异有关。

Conclusion: 伪装对齐行为在部分模型中存在，其动机和表现因模型而异；训练后期的调节可能显著影响这种行为。

Abstract: Alignment faking in large language models presented a demonstration of Claude
3 Opus and Claude 3.5 Sonnet selectively complying with a helpful-only training
objective to prevent modification of their behavior outside of training. We
expand this analysis to 25 models and find that only 5 (Claude 3 Opus, Claude
3.5 Sonnet, Llama 3 405B, Grok 3, Gemini 2.0 Flash) comply with harmful queries
more when they infer they are in training than when they infer they are in
deployment. First, we study the motivations of these 5 models. Results from
perturbing details of the scenario suggest that only Claude 3 Opus's compliance
gap is primarily and consistently motivated by trying to keep its goals.
Second, we investigate why many chat models don't fake alignment. Our results
suggest this is not entirely due to a lack of capabilities: many base models
fake alignment some of the time, and post-training eliminates alignment-faking
for some models and amplifies it for others. We investigate 5 hypotheses for
how post-training may suppress alignment faking and find that variations in
refusal behavior may account for a significant portion of differences in
alignment faking.

</details>


### [167] [Pathwise Explanation of ReLU Neural Networks](https://arxiv.org/abs/2506.18037)
*Seongwoo Lim,Won Jo,Joohyung Lee,Jaesik Choi*

Key words: 神经网络、解释性、ReLU网络、决策路径

TL;DR: 该论文提出了一种新的方法来解释ReLU神经网络，通过关注决策路径中的隐藏单元子集，提供更清晰和一致的输入与决策关系理解。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于神经网络的‘黑盒’特性，其透明性和可靠性受到质疑，因此需要更好的解释方法。

Method: 通过分析隐藏单元在决策路径中的子集，提供一个灵活的解释框架，可以调整解释范围和详细分解输入。

Result: 实验显示，该方法在定量和定性评估上均优于现有方法。

Conclusion: 该路径解释方法为神经网络提供了更透明和可靠的解释框架。

Abstract: Neural networks have demonstrated a wide range of successes, but their
``black box" nature raises concerns about transparency and reliability.
Previous research on ReLU networks has sought to unwrap these networks into
linear models based on activation states of all hidden units. In this paper, we
introduce a novel approach that considers subsets of the hidden units involved
in the decision making path. This pathwise explanation provides a clearer and
more consistent understanding of the relationship between the input and the
decision-making process. Our method also offers flexibility in adjusting the
range of explanations within the input, i.e., from an overall attribution input
to particular components within the input. Furthermore, it allows for the
decomposition of explanations for a given input for more detailed explanations.
Experiments demonstrate that our method outperforms others both quantitatively
and qualitatively.

</details>


### [168] [TAB: Unified Benchmarking of Time Series Anomaly Detection Methods](https://arxiv.org/abs/2506.18046)
*Xiangfei Qiu,Zhe Li,Wanghui Qiu,Shiyan Hu,Lekui Zhou,Xingjian Wu,Zhengyu Li,Chenjuan Guo,Aoying Zhou,Zhenli Sheng,Jilin Hu,Christian S. Jensen,Bin Yang*

Key words: 时间序列异常检测, 评估基准, 数据集多样性, 自动化评估

TL;DR: 该论文提出了一个新的时间序列异常检测基准TAB，旨在解决当前评估方法中的缺陷，包括数据集多样性和评估协议的统一性，以促进更全面的研究比较。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于时间序列异常检测在多个领域的重要性，需要更可靠的方法来评估和比较现有及新方法。当前评估过程存在数据集和实验设置的不足。

Method: 论文提出TAB基准，包含29个多元数据集和1,635个单变量时间序列，覆盖多种TSAD方法，并提供统一的自动化评估流程。

Result: 通过TAB评估现有TSAD方法，提供了对方法性能的深入见解，并公开了所有数据集和代码。

Conclusion: TAB基准为时间序列异常检测研究提供了更全面和公平的评估框架，推动了该领域的有效进展。

Abstract: Time series anomaly detection (TSAD) plays an important role in many domains
such as finance, transportation, and healthcare. With the ongoing
instrumentation of reality, more time series data will be available, leading
also to growing demands for TSAD. While many TSAD methods already exist, new
and better methods are still desirable. However, effective progress hinges on
the availability of reliable means of evaluating new methods and comparing them
with existing methods. We address deficiencies in current evaluation procedures
related to datasets and experimental settings and protocols. Specifically, we
propose a new time series anomaly detection benchmark, called TAB. First, TAB
encompasses 29 public multivariate datasets and 1,635 univariate time series
from different domains to facilitate more comprehensive evaluations on diverse
datasets. Second, TAB covers a variety of TSAD methods, including Non-learning,
Machine learning, Deep learning, LLM-based, and Time-series pre-trained
methods. Third, TAB features a unified and automated evaluation pipeline that
enables fair and easy evaluation of TSAD methods. Finally, we employ TAB to
evaluate existing TSAD methods and report on the outcomes, thereby offering a
deeper insight into the performance of these methods. Besides, all datasets and
code are available at https://github.com/decisionintelligence/TAB.

</details>


### [169] [Distributionally robust minimization in meta-learning for system identification](https://arxiv.org/abs/2506.18074)
*Matteo Rufolo,Dario Piga,Marco Forgione*

Key words: 元学习，分布鲁棒优化，系统辨识，任务变异性，安全关键应用

TL;DR: 该论文探讨了在元学习中采用分布鲁棒优化方法，以提升系统辨识任务中最坏情况下的性能表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 标准元学习方法优化期望损失，忽略了任务变异性，可能导致安全关键应用中失败。

Method: 采用分布鲁棒优化范式，优先处理高损失任务，增强最坏情况下的性能。

Result: 在合成动态系统类上的测试表明，该方法在分布内和分布外设置下均能减少安全关键应用中的失败。

Conclusion: 分布鲁棒优化方法在元学习中有效提升了系统辨识任务的鲁棒性和安全性。

Abstract: Meta learning aims at learning how to solve tasks, and thus it allows to
estimate models that can be quickly adapted to new scenarios. This work
explores distributionally robust minimization in meta learning for system
identification. Standard meta learning approaches optimize the expected loss,
overlooking task variability. We use an alternative approach, adopting a
distributionally robust optimization paradigm that prioritizes high-loss tasks,
enhancing performance in worst-case scenarios. Evaluated on a meta model
trained on a class of synthetic dynamical systems and tested in both
in-distribution and out-of-distribution settings, the proposed approach allows
to reduce failures in safety-critical applications.

</details>


### [170] [RL for Reasoning by Adaptively Revealing Rationales](https://arxiv.org/abs/2506.18110)
*Mohammad Hossein Amani,Aryo Lotfi,Nicolas Mario Baldwin,Samy Bengio,Mehrdad Farajtabar,Emmanuel Abbe,Robert West*

Key words: 强化学习；课程学习；序列生成；自适应回溯；数学推理

TL;DR: 论文提出了一种通过自适应回溯（AdaBack）的强化学习方法，解决复杂序列生成任务中监督微调（SFT）和强化学习（RL）的局限性。该方法通过动态调整训练期间的监督长度，逐步学习完成任务。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统监督微调依赖密集的真实标签，成本高；而强化学习在稀疏奖励和大输出空间下表现不佳。论文旨在探索一种介于两者之间的方法，以解决长序列依赖任务中的泛化问题。

Method: 提出自适应回溯（AdaBack）算法，动态调整每个样本的监督长度，基于模型的奖励信号逐步学习完成推理链。

Result: 实验表明，自适应课程学习能够解决传统方法无法泛化的长序列依赖任务，并在数学推理基准测试中提升模型能力。

Conclusion: 自适应课程学习是一种有效的中间方法，能够在SFT和RL失败的复杂任务中取得突破。

Abstract: We propose that reinforcement learning (RL) from partial expert
demonstrations is not merely a training heuristic, but a promising framework
for solving complex sequence generation tasks. Supervised fine-tuning (SFT)
relies on dense ground-truth labels, which become increasingly costly as
sequence length grows. RL, on the other hand, struggles with sparse rewards and
a combinatorially large output space. We address this by introducing adaptive
backtracking (AdaBack), a per-sample curriculum learning algorithm that reveals
only a partial prefix of the target output during training. The supervision
length is adjusted dynamically for each sample based on the model's past reward
signal, allowing it to incrementally learn to complete reasoning chains by
conditioning on correct partial solutions. We investigate this intermediate
regime between SFT and RL and argue that per-sample curriculum learning is more
than a trade-off between efficiency and generality, it can succeed in tasks
with long sequences of latent dependencies where SFT and RL both fail to
generalize. Using a synthetic task with latent parity constraints, we show that
our adaptive curriculum over partial answers reliably solves problems that are
otherwise intractable. On mathematical reasoning benchmarks (MATH, GSM8k), we
find that curriculum learning enables models to solve problems that RL alone
cannot, acquiring new reasoning capabilities through incremental exposure to
partial solutions.

</details>


### [171] [Bayesian Multiobject Tracking With Neural-Enhanced Motion and Measurement Models](https://arxiv.org/abs/2506.18124)
*Shaoxiu Wei,Mingchao Liang,Florian Meyer*

Key words: 多目标跟踪, 贝叶斯方法, 神经网络, 混合方法, 自动驾驶

TL;DR: 该论文提出了一种混合多目标跟踪（MOT）方法，结合了传统模型驱动和数据驱动方法的优势，通过神经网络增强贝叶斯MOT中的统计模型，并在nuScenes数据集上验证了其性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统模型驱动MOT方法泛化性强但性能有限，数据驱动方法在数据充足时表现优异。作者希望整合两者优势。

Method: 使用神经网络增强贝叶斯MOT中的统计模型，结合置信传播和序贯蒙特卡洛方法确保计算效率。

Result: 在nuScenes数据集上验证了方法的有效性，达到了最先进性能。

Conclusion: 混合方法结合了模型驱动的灵活性与数据驱动的学习能力，为MOT提供了高效解决方案。

Abstract: Multiobject tracking (MOT) is an important task in applications including
autonomous driving, ocean sciences, and aerospace surveillance. Traditional MOT
methods are model-based and combine sequential Bayesian estimation with data
association and an object birth model. More recent methods are fully
data-driven and rely on the training of neural networks. Both approaches offer
distinct advantages in specific settings. In particular, model-based methods
are generally applicable across a wide range of scenarios, whereas data-driven
MOT achieves superior performance in scenarios where abundant labeled data for
training is available. A natural thought is whether a general framework can
integrate the two approaches. This paper introduces a hybrid method that
utilizes neural networks to enhance specific aspects of the statistical model
in Bayesian MOT that have been identified as overly simplistic. By doing so,
the performance of the prediction and update steps of Bayesian MOT is improved.
To ensure tractable computation, our framework uses belief propagation to avoid
high-dimensional operations combined with sequential Monte Carlo methods to
perform low-dimensional operations efficiently. The resulting method combines
the flexibility and robustness of model-based approaches with the capability to
learn complex information from data of neural networks. We evaluate the
performance of the proposed method based on the nuScenes autonomous driving
dataset and demonstrate that it has state-of-the-art performance

</details>


### [172] [Routing Mamba: Scaling State Space Models with Mixture-of-Experts Projection](https://arxiv.org/abs/2506.18145)
*Zheng Zhan,Liliang Ren,Shuohang Wang,Liyuan Liu,Yang Liu,Yeyun Gong,Yanzhi Wang,Yelong Shen*

Key words: Linear State Space Models, Mamba, Mixture of Experts, Routing Mamba, sparse scaling

TL;DR: RoM（Routing Mamba）采用稀疏混合线性投影专家方法扩展SSM参数，通过共享路由决策和轻量子模块，高效扩展了Mamba层的表现力，性能优于密集模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有SSM（如Mamba）在长序列建模中表现出色，但通过MoE扩展其表现力时面临性能下降的挑战。

Method: 提出RoM，通过稀疏混合线性投影专家共享路由决策和轻量子模块，优化SSM的参数扩展。

Result: 在1.3B活跃参数（总10B）和16K序列长度下，RoM性能与密集模型相当，且节省23%FLOPS。

Conclusion: RoM为SSM的高效扩展提供了有效方案，适用于长序列建模任务。

Abstract: Linear State Space Models (SSMs) offer remarkable performance gains in
efficient sequence modeling, with constant inference-time computation and
memory complexity. Recent advances, such as Mamba, further enhance SSMs with
input-dependent gating and hardware-aware implementations, positioning them as
strong alternatives to Transformers for long sequence modeling. However,
efficiently scaling the expressive power of SSMs, particularly with Mixture of
Experts (MoE), remains challenging, as naive integration attempts often falter
or degrade performance. In this work, we introduce Routing Mamba (RoM), a novel
approach that scales SSM parameters using sparse mixtures of linear projection
experts. By sharing routing decisions between projection layers and lightweight
sub-modules within Mamba across experts, RoM leverages synergies among linear
projection experts for effective and efficient sparse scaling of Mamba layers.
At a scale of 1.3B active parameters (10B total) and 16K training sequence
length, RoM achieves language modeling performance equivalent to a dense Mamba
model requiring over 2.3x more active parameters, and demonstrates consistent
perplexity across context lengths. Experimental results further show RoM
effectively scales hybrid language models, yielding a 23% FLOPS saving compared
to dense Mamba scaling for similar performance.

</details>


### [173] [Probabilistic and reinforced mining of association rules](https://arxiv.org/abs/2506.18155)
*Yongchao Huang*

Key words: 关联规则挖掘；高斯过程；贝叶斯方法；强化学习；不确定性建模

TL;DR: 本篇论文提出了四种新颖的概率和强化学习方法用于关联规则挖掘（ARM），分别是高斯过程关联规则挖掘（GPAR）、贝叶斯ARM（BARM）、多臂老虎机ARM（MAB-ARM）和强化学习ARM（RLAR），突破了传统频率驱动方法的局限性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统关联规则挖掘方法（如Apriori、FP-Growth）无法有效处理先验知识、不确定性、项目依赖性和自适应搜索策略等问题。本文方法旨在弥补这些不足。

Method: 1. GPAR利用高斯过程建模项目共现关系；2. BARM采用贝叶斯框架量化不确定性；3. MAB-ARM通过UCB策略自适应探索项目空间；4. RLAR使用DQN学习高质量规则。

Result: 实验证明，这些方法在小数据集和复杂模式挖掘中表现优异，但也存在计算复杂度与可解释性的权衡。

Conclusion: 这些方法为关联规则挖掘提供了灵活性、鲁棒性和可扩展性，适用于零售、医疗、金融等多领域。

Abstract: This work introduces 4 novel probabilistic and reinforcement-driven methods
for association rule mining (ARM): Gaussian process-based association rule
mining (GPAR), Bayesian ARM (BARM), multi-armed bandit based ARM (MAB-ARM), and
reinforcement learning based association rule mining (RLAR). These methods
depart fundamentally from traditional frequency-based algorithms such as
Apriori, FP-Growth, and Eclat, offering enhanced capabilities for incorporating
prior knowledge, modeling uncertainty, item dependencies, probabilistic
inference and adaptive search strategies. GPAR employs Gaussian processes to
model item co-occurrence via feature representations, enabling principled
inference, uncertainty quantification, and efficient generalization to unseen
itemsets without retraining. BARM adopts a Bayesian framework with priors and
optional correlation structures, yielding robust uncertainty quantification
through full posterior distributions over item presence probabilities. MAB-ARM,
including its Monte Carlo tree search (MCTS) companion, utilizes an upper
confidence bound (UCB) strategy for efficient and adaptive exploration of the
itemset space, while RLAR applies a deep Q-network (DQN) to learn a
generalizable policy for identifying high-quality rules. Collectively, these
approaches improve the flexibility and robustness of ARM, particularly for
discovering rare or complex patterns and operating on small datasets. Empirical
results on synthetic and real-world datasets demonstrate their effectiveness,
while also highlighting trade-offs in computational complexity and
interpretability. These innovations mark a significant shift from static,
frequency-driven paradigms, offering some prior and dependency-informed,
uncertainty-aware or scalable ARM frameworks for diverse application domains
such as retail, geography, finance, medical diagnostics, and risk-sensitive
scenarios.

</details>


### [174] [Pitfalls of Conformal Predictions for Medical Image Classification](https://arxiv.org/abs/2506.18162)
*Hendrik Mehrtens,Tabea Bucher,Titus J. Brinker*

Key words: 符合性预测, 医学分类, 分布变化, 不可靠性, 小类别数据

TL;DR: 该论文分析了在医学分类任务中应用符合性预测的局限性，特别是在数据分布变化、小类别数据以及选择预测以提高准确性时不可靠的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨符合性预测在医学分类任务中的可靠性，尤其是面对分布变化和小类别数据时的实际应用价值。

Method: 通过皮肤病学和组织病理学的案例展示符合性预测在输入和标签变量分布变化下的不可靠性。

Result: 符合性预测在分布变化和小类别数据下不可靠，不适用于选择预测以提高准确性，且对数据子集（如个别类别或患者属性）也不可靠。

Conclusion: 在小类别医学图像分类任务中，符合性预测的实际应用价值有限。

Abstract: Reliable uncertainty estimation is one of the major challenges for medical
classification tasks. While many approaches have been proposed, recently the
statistical framework of conformal predictions has gained a lot of attention,
due to its ability to provide provable calibration guarantees. Nonetheless, the
application of conformal predictions in safety-critical areas such as medicine
comes with pitfalls, limitations and assumptions that practitioners need to be
aware of. We demonstrate through examples from dermatology and histopathology
that conformal predictions are unreliable under distributional shifts in input
and label variables. Additionally, conformal predictions should not be used for
selecting predictions to improve accuracy and are not reliable for subsets of
the data, such as individual classes or patient attributes. Moreover, in
classification settings with a small number of classes, which are common in
medical image classification tasks, conformal predictions have limited
practical value.

</details>


### [175] [Non-equilibrium Annealed Adjoint Sampler](https://arxiv.org/abs/2506.18165)
*Jaemoo Choi,Yongxin Chen,Molei Tao,Guan-Horng Liu*

Key words: 扩散采样器, 随机最优控制, 伴随系统, 退火动态, 重要性采样

TL;DR: 本文提出了一种新型的非平衡退火伴随采样器（NAAS），采用随机最优控制方法，无需依赖重要性采样，从而降低了方差并提高了可扩展性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的基于学习的扩散采样器依赖重要性采样，导致高方差和有限的可扩展性。本文旨在解决这些问题。

Method: 使用退火参考动态和伴随匹配技术的轻量级伴随系统，设计了一种新型的随机最优控制方法。

Result: NAAS在经典能量景观和分子玻尔兹曼分布等多种任务中表现出高效和可扩展性。

Conclusion: NAAS通过避免重要性采样，显著降低了方差，同时保持了高效性，为扩散采样提供了新的解决方案。

Abstract: Recently, there has been significant progress in learning-based diffusion
samplers, which aim to sample from a given unnormalized density. These methods
typically follow one of two paradigms: (i) formulating sampling as an unbiased
stochastic optimal control (SOC) problem using a canonical reference process,
or (ii) refining annealed path measures through importance-weighted sampling.
Although annealing approaches have advantages in guiding samples toward
high-density regions, reliance on importance sampling leads to high variance
and limited scalability in practice. In this paper, we introduce the
\textbf{Non-equilibrium Annealed Adjoint Sampler (NAAS)}, a novel SOC-based
diffusion sampler that leverages annealed reference dynamics without resorting
to importance sampling. NAAS employs a lean adjoint system inspired by adjoint
matching, enabling efficient and scalable training. We demonstrate the
effectiveness of our approach across a range of tasks, including sampling from
classical energy landscapes and molecular Boltzmann distribution.

</details>


### [176] [Understanding Reasoning in Thinking Language Models via Steering Vectors](https://arxiv.org/abs/2506.18167)
*Constantin Venhoff,Iván Arcuschin,Philip Torr,Arthur Conmy,Neel Nanda*

Key words: 大语言模型,推理行为,激活空间,控制向量,DeepSeek-R1-Distill

TL;DR: 论文提出了一种通过分析和操控大型语言模型（LLMs）中的特定推理行为来控制其推理过程的方法，展示了线性方向在激活空间中的作用，并通过实验验证了方法的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大型语言模型在生成内部推理链条后性能提升，但控制这些推理过程仍具挑战性。本研究旨在提供一种可控且可解释的方法来调节模型的推理行为。

Method: 通过系统性实验分析模型在500个任务中的推理行为，识别出表达不确定性、假设验证和回溯等行为。利用激活空间中的线性方向提取控制向量，从而调节推理行为。

Result: 实验证明线性方向可以有效地控制模型推理行为，如回溯或表达不确定性，并且在不同的DeepSeek-R1-Distill模型上表现一致。

Conclusion: 提供了一种可控且可解释的方法来调节思考语言模型的推理行为，为未来的模型控制研究提供了实用工具。

Abstract: Recent advances in large language models (LLMs) have led to the development
of thinking language models that generate extensive internal reasoning chains
before producing responses. While these models achieve improved performance,
controlling their reasoning processes remains challenging. This work presents a
steering approach for thinking LLMs by analyzing and manipulating specific
reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic
experiment on 500 tasks across 10 diverse categories, we identify several
reasoning behaviors exhibited by thinking models, including expressing
uncertainty, generating examples for hypothesis validation, and backtracking in
reasoning chains. We demonstrate that these behaviors are mediated by linear
directions in the model's activation space and can be controlled using steering
vectors. By extracting and applying these vectors, we provide a method to
modulate specific aspects of the model's reasoning process, such as its
tendency to backtrack or express uncertainty. Our approach offers practical
tools for steering reasoning processes in thinking models in a controlled and
interpretable manner. We validate our steering method using two
DeepSeek-R1-Distill models, demonstrating consistent control across different
model architectures.

</details>


### [177] [Memba: Membrane-driven Parameter-Efficient Fine-Tuning for Mamba](https://arxiv.org/abs/2506.18184)
*Donghyun Lee,Yuhang Li,Ruokai Yin,Shiting Xiao,Priyadarshini Panda*

Key words: State Space Models, Mamba, PEFT, LIM神经元, 时间建模

TL;DR: 提出了Memba，一种针对Mamba模型的膜驱动参数高效微调方法，通过LIM神经元增强时间建模能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对State Space Models的独特时间处理动态，提出专为其设计的PEFT方法。

Method: 结合LIM神经元、LoRA和跨层膜传递，优化Mamba的时间建模。

Result: 在语言和视觉任务上显著优于现有PEFT方法。

Conclusion: Memba为Mamba模型提供了一种高效的微调方法。

Abstract: State Space Models (SSMs) have emerged as powerful alternatives to
attention-based Transformers, with Mamba demonstrating impressive efficiency
and scalability. As these models grow increasingly larger, the need for
Parameter-Efficient Fine-Tuning (PEFT) methods becomes critical to adapt
pre-trained Mamba to downstream tasks without prohibitive computational costs.
However, previous approaches simply apply traditional Transformer-tailored PEFT
methods without addressing the unique temporal processing dynamics of SSMs. To
address this limitation, we propose Memba, a membrane-driven PEFT approach
specifically designed for Mamba. Memba introduces Leaky Integrate Membrane
(LIM) neurons as bio-inspired gating mechanisms that naturally accumulate
membrane potentials over time, enhancing selective information retention. By
strategically combining LIM neurons with Low-Rank Adaptations (LoRA) and
cross-layer membrane transfer, our approach significantly improves Mamba's
temporal modeling capabilities. Extensive experiments across language and
vision tasks demonstrate that Memba achieves substantial improvements over
existing PEFT methods. The code is available at
https://github.com/Intelligent-Computing-Lab-Yale/Memba.

</details>


### [178] [Online Learning of Whittle Indices for Restless Bandits with Non-Stationary Transition Kernels](https://arxiv.org/abs/2506.18186)
*Md Kamran Chowdhury Shisher,Vishrant Tripathi,Mung Chiang,Christopher G. Brinton*

Key words: 多臂赌博机、Whittle指数、在线学习、非平稳环境、动态后悔

TL;DR: 提出了一种在线学习算法，用于在未知、非平稳环境中计算Whittle指数，并通过滑动窗口和置信上界保证亚线性动态后悔。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决在未知、非平稳环境中的资源分配问题，传统Whittle指数方法需要已知的转移核，而实际中难以满足。

Method: 基于滑动窗口和置信上界的在线学习算法，预测当前转移核并计算Whittle指数，利用领域知识加速学习。

Result: 算法在非平稳环境中表现优于基线，累计后悔更低。

Conclusion: 算法有效解决了非平稳环境下的RMAB问题，且具有理论保证。

Abstract: We consider optimal resource allocation for restless multi-armed bandits
(RMABs) in unknown, non-stationary settings. RMABs are PSPACE-hard to solve
optimally, even when all parameters are known. The Whittle index policy is
known to achieve asymptotic optimality for a large class of such problems,
while remaining computationally efficient. In many practical settings, however,
the transition kernels required to compute the Whittle index are unknown and
non-stationary. In this work, we propose an online learning algorithm for
Whittle indices in this setting. Our algorithm first predicts current
transition kernels by solving a linear optimization problem based on upper
confidence bounds and empirical transition probabilities calculated from data
over a sliding window. Then, it computes the Whittle index associated with the
predicted transition kernels. We design these sliding windows and upper
confidence bounds to guarantee sub-linear dynamic regret on the number of
episodes $T$, under the condition that transition kernels change slowly over
time (rate upper bounded by $\epsilon=1/T^k$ with $k>0$). Furthermore, our
proposed algorithm and regret analysis are designed to exploit prior domain
knowledge and structural information of the RMABs to accelerate the learning
process. Numerical results validate that our algorithm achieves superior
performance in terms of lowest cumulative regret relative to baselines in
non-stationary environments.

</details>


### [179] [DeInfoReg: A Decoupled Learning Framework for Better Training Throughput](https://arxiv.org/abs/2506.18193)
*Zih-Hao Huang,You-Teng Lin,Hung-Hsuan Chen*

Key words: DeInfoReg, 梯度流分解, 多GPU并行, 信息正则化, 梯度消失

TL;DR: DeInfoReg通过分解梯度流解决梯度消失问题，支持多GPU并行训练，显著提升训练效率，优于传统反向传播和其他梯度流分解技术。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决长期梯度流导致的梯度消失问题，并提升训练效率和模型性能。

Method: 采用Decoupled Supervised Learning with Information Regularization（DeInfoReg），将长梯度流分解为多个短梯度流，结合管道策略实现多GPU并行。

Result: 在多样化任务和数据集上表现出卓越性能、更好的抗噪能力，并高效利用并行计算资源。

Conclusion: DeInfoReg在性能和训练效率上优于传统方法，同时解决了梯度消失问题。

Abstract: This paper introduces Decoupled Supervised Learning with Information
Regularization (DeInfoReg), a novel approach that transforms a long gradient
flow into multiple shorter ones, thereby mitigating the vanishing gradient
problem. Integrating a pipeline strategy, DeInfoReg enables model
parallelization across multiple GPUs, significantly improving training
throughput. We compare our proposed method with standard backpropagation and
other gradient flow decomposition techniques. Extensive experiments on diverse
tasks and datasets demonstrate that DeInfoReg achieves superior performance and
better noise resistance than traditional BP models and efficiently utilizes
parallel computing resources. The code for reproducibility is available at:
https://github.com/ianzih/Decoupled-Supervised-Learning-for-Information-Regularization/.

</details>


### [180] [Joint Embedding Predictive Architecture for self-supervised pretraining on polymer molecular graphs](https://arxiv.org/abs/2506.18194)
*Francesco Picolli,Gabriel Vogel,Jana M. Weber*

Key words: 机器学习, 聚合物, 自监督学习, JEPA, 数据稀缺

TL;DR: 研究了JEPA自监督学习方法在聚合物分子图上的应用，表明在标签数据稀缺时能提升下游任务性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 聚合物机器学习领域因高质量标签数据稀缺而受限，探索自监督学习能否缓解这一问题。

Method: 采用JEPA自监督架构对聚合物分子图进行预训练，评估其在标签数据稀缺时的下游性能。

Result: JEPA预训练显著提升了标签数据稀缺时的性能，在所有测试数据集上均有改善。

Conclusion: JEPA自监督学习为解决聚合物机器学习中的标签数据稀缺问题提供了有效途径。

Abstract: Recent advances in machine learning (ML) have shown promise in accelerating
the discovery of polymers with desired properties by aiding in tasks such as
virtual screening via property prediction. However, progress in polymer ML is
hampered by the scarcity of high-quality labeled datasets, which are necessary
for training supervised ML models. In this work, we study the use of the very
recent 'Joint Embedding Predictive Architecture' (JEPA), a type of architecture
for self-supervised learning (SSL), on polymer molecular graphs to understand
whether pretraining with the proposed SSL strategy improves downstream
performance when labeled data is scarce. Our results indicate that JEPA-based
self-supervised pretraining on polymer graphs enhances downstream performance,
particularly when labeled data is very scarce, achieving improvements across
all tested datasets.

</details>


### [181] [These are Not All the Features You are Looking For: A Fundamental Bottleneck In Supervised Pretraining](https://arxiv.org/abs/2506.18221)
*Xingyu Alice Yang,Jianyu Zhang,Léon Bottou*

Key words: 迁移学习, 信息饱和瓶颈, 特征表示, 深度学习

TL;DR: 该论文探讨了迁移学习中的‘信息饱和瓶颈’问题，指出大规模预训练模型可能无法有效学习所有关键特征，导致迁移性能不稳定，并提出更丰富的特征表示作为解决方案。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决迁移学习中的特征不足问题，特别是预训练模型在不同任务上的性能不一致性。

Method: 评估预训练模型在其组成部分任务上的性能，识别‘信息饱和瓶颈’，并提出新的特征表示方法。

Result: 发现深度学习模型在学习竞争性特征时会忽略关键特征，导致迁移性能下降。

Conclusion: 依赖大规模网络可能不如任务特定训练有效，建议采用更丰富的特征表示以提升泛化能力。

Abstract: Transfer learning is a cornerstone of modern machine learning, promising a
way to adapt models pretrained on a broad mix of data to new tasks with minimal
new data. However, a significant challenge remains in ensuring that transferred
features are sufficient to handle unseen datasets, amplified by the difficulty
of quantifying whether two tasks are "related". To address these challenges, we
evaluate model transfer from a pretraining mixture to each of its component
tasks, assessing whether pretrained features can match the performance of
task-specific direct training. We identify a fundamental limitation in deep
learning models -- an "information saturation bottleneck" -- where networks
fail to learn new features once they encode similar competing features during
training. When restricted to learning only a subset of key features during
pretraining, models will permanently lose critical features for transfer and
perform inconsistently on data distributions, even components of the training
mixture. Empirical evidence from published studies suggests that this
phenomenon is pervasive in deep learning architectures -- factors such as data
distribution or ordering affect the features that current representation
learning methods can learn over time. This study suggests that relying solely
on large-scale networks may not be as effective as focusing on task-specific
training, when available. We propose richer feature representations as a
potential solution to better generalize across new datasets and, specifically,
present existing methods alongside a novel approach, the initial steps towards
addressing this challenge.

</details>


### [182] [AdapThink: Adaptive Thinking Preferences for Reasoning Language Model](https://arxiv.org/abs/2506.18237)
*Xu Wan,Wei Wang,Wenyue Xu,Wotao Yin,Jie Song,Mingyang Sun*

Key words: 强化学习, 后训练, 自适应推理, 语言模型, 多样性采样

TL;DR: AdapThink是一种自适应后训练框架，通过动态调整奖励函数和多样性采样机制，提升语言模型的推理效率，同时保持性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有RL后训练方法在推理效率上存在不足，缺乏对问题复杂性和模型能力的动态适应能力。

Method: AdapThink引入组相对奖励函数和多样性感知采样机制，动态调整反思偏好并平衡准确性与推理多样性。

Result: 在多个数学推理数据集上的实验表明，AdapThink能有效提升推理效率，减少无效计算。

Conclusion: AdapThink为解决RL后训练中的推理效率问题提供了有效方案。

Abstract: Reinforcement Learning (RL)-based post-training has significantly advanced
the complex reasoning capabilities of language models, fostering sophisticated
self-reflection processes. However, this ``slow thinking'' paradigm presents a
critical challenge to reasoning efficiency: models may expend excessive
computation on simple questions and shift reasoning prematurely for complex
ones. Previous mechanisms typically rely on static length budgets or predefined
rules, lacking the adaptability for varying question complexities and models'
evolving capabilities. To this end, we propose AdapThink, an adaptive
post-training framework designed to induce more efficient thinking while
maintaining the performance of reasoning language models. Specifically,
AdapThink incorporates two key mechanisms: 1) A group-relative reward function
that leverages model confidence and response's characteristic to dynamically
adjust the preference of reflection-related transition words without resorting
to a fixed length preference. 2) A diversity-aware sampling mechanism that
balances the training group's solution accuracy with reasoning diversity via an
entropy-guided score. Experiments on several mathematical reasoning datasets
with DeepSeek-distilled models demonstrate AdapThink's advantages in enabling
adaptive reasoning patterns and mitigating the inefficiencies.

</details>


### [183] [Quantum-Classical Hybrid Quantized Neural Network](https://arxiv.org/abs/2506.18240)
*Wenxin Li,Chuan Wang,Hongdong Zhu,Qi Gao,Yin Ma,Hai Wei,Kai Wen*

Key words: 二次二进制优化,量子计算,神经网训练,前向区间传播,量子条件梯度下降

TL;DR: 提出了一种新颖的二次二进制优化（QBO）模型用于量化神经网络训练，通过样条插值支持任意激活和损失函数。引入前向区间传播（FIP）方法，解决非线性和多层复合结构的挑战，并通过量子条件梯度下降（QCGD）算法解决大规模QCBO问题，实验在Fashion MNIST上达到94.95%准确率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决神经网络中非线性和多层结构的优化问题，并利用量子计算机扩展其在人工智能中的应用。

Method: 采用前向区间传播（FIP）离散化激活函数为线性子区间，结合量子条件梯度下降（QCGD）算法直接解决QCBO问题。

Result: 理论分析了近似误差和所需伊辛自旋数的上界，实验在Fashion MNIST上实现94.95%分类准确率，仅需1.1比特精度。

Conclusion: 该模型保留了神经网络的通用逼近特性，并通过量子计算解决了大规模QCBO问题，展示了高效性和实用性。

Abstract: Here in this work, we present a novel Quadratic Binary Optimization (QBO)
model for quantized neural network training, enabling the use of arbitrary
activation and loss functions through spline interpolation. We introduce
Forward Interval Propagation (FIP), a method designed to tackle the challenges
of non-linearity and the multi-layer composite structure in neural networks by
discretizing activation functions into linear subintervals. This approach
preserves the universal approximation properties of neural networks while
allowing complex nonlinear functions to be optimized using quantum computers,
thus broadening their applicability in artificial intelligence. We provide
theoretical upper bounds on the approximation error and the number of Ising
spins required, by deriving the sample complexity of the empirical risk
minimization problem, from an optimization perspective. A significant challenge
in solving the associated Quadratic Constrained Binary Optimization (QCBO)
model on a large scale is the presence of numerous constraints. When employing
the penalty method to handle these constraints, tuning a large number of
penalty coefficients becomes a critical hyperparameter optimization problem,
increasing computational complexity and potentially affecting solution quality.
To address this, we employ the Quantum Conditional Gradient Descent (QCGD)
algorithm, which leverages quantum computing to directly solve the QCBO
problem. We prove the convergence of QCGD under a quantum oracle with
randomness and bounded variance in objective value, as well as under limited
precision constraints in the coefficient matrix. Additionally, we provide an
upper bound on the Time-To-Solution for the QCBO solving process. Experimental
results using a coherent Ising machine (CIM) demonstrate a 94.95% accuracy on
the Fashion MNIST classification task, with only 1.1-bit precision.

</details>


### [184] [Dual-Forward Path Teacher Knowledge Distillation: Bridging the Capacity Gap Between Teacher and Student](https://arxiv.org/abs/2506.18244)
*Tong Li,Long Liu,Yihang Hu,Hu Chen,Shifeng Chen*

Key words: 知识蒸馏, 提示学习, 双前向路径, 师生网络, 能力差距

TL;DR: 本文提出了一种名为DFPT-KD的新方法，通过引入基于提示的双前向路径教师模型，解决知识蒸馏中师生网络能力差距问题，进一步提升学生网络的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统知识蒸馏（KD）方法由于师生网络能力差距大，导致蒸馏效果有限。现有方法或丢弃准确知识表示，或无法动态调整传输的知识，难以有效解决这一问题。

Method: 作者提出DFPT-KD，通过基于提示的调整技术，在预训练教师模型中建立额外的提示前向路径，并冻结教师模型以优化该路径，使传输的知识与学生表示能力兼容。进一步，提出DFPT-KD+，对整个基于提示的前向路径进行微调。

Result: 实验表明，DFPT-KD优于传统KD方法。DFPT-KD+进一步提升了DFPT-KD的性能，达到了最先进的准确率。

Conclusion: 通过引入双前向路径和提示学习，DFPT-KD及其改进版本DFPT-KD+有效解决了知识蒸馏中的能力差距问题，显著提升了学生网络的性能。

Abstract: Knowledge distillation (KD) provides an effective way to improve the
performance of a student network under the guidance of pre-trained teachers.
However, this approach usually brings in a large capacity gap between teacher
and student networks, limiting the distillation gains. Previous methods
addressing this problem either discard accurate knowledge representation or
fail to dynamically adjust the transferred knowledge, which is less effective
in addressing the capacity gap problem and hinders students from achieving
comparable performance with the pre-trained teacher. In this work, we extend
the ideology of prompt-based learning to address the capacity gap problem, and
propose Dual-Forward Path Teacher Knowledge Distillation (DFPT-KD), which
replaces the pre-trained teacher with a novel dual-forward path teacher to
supervise the learning of student. The key to DFPT-KD is prompt-based tuning,
i.e., establishing an additional prompt-based forward path within the
pre-trained teacher and optimizing it with the pre-trained teacher frozen to
make the transferred knowledge compatible with the representation ability of
the student. Extensive experiments demonstrate that DFPT-KD leads to trained
students performing better than the vanilla KD. To make the transferred
knowledge better compatible with the representation abilities of the student,
we further fine-tune the whole prompt-based forward path, yielding a novel
distillation approach dubbed DFPT-KD+. By extensive experiments, it is shown
that DFPT-KD+ improves upon DFPT-KD and achieves state-of-the-art accuracy
performance.

</details>


### [185] [Exploring Efficient Quantification of Modeling Uncertainties with Differentiable Physics-Informed Machine Learning Architectures](https://arxiv.org/abs/2506.18247)
*Manaswin Oddiraju,Bharath Varma Penumatsa,Divyang Amin,Michael Piedmonte,Souma Chowdhury*

Key words: 建模不确定性, 物理信息机器学习, 贝叶斯神经网络, 蒙特卡洛采样

TL;DR: 该论文探讨了如何通过结合贝叶斯神经网络（BNN）与物理信息机器学习（PIML）架构，提升建模不确定性的传播能力。研究表明，BNN在PIML中的集成效果接近或略低于纯数据驱动方法，但蒙特卡洛采样能有效传播不确定性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 量化并传播建模不确定性对工程设计和控制至关重要，但目前物理信息机器学习（PIML）在此方面的能力尚未充分探索。

Method: 通过将贝叶斯神经网络（BNN）集成到PIML架构中，采用两阶段训练过程，并评估其在基准问题和飞行实验数据中的表现。

Result: BNN集成的PIML架构在预测性能上略逊或持平于纯数据驱动方法，但蒙特卡洛采样能有效传播不确定性。

Conclusion: BNN在PIML中集成能够传播不确定性，但在预测性能上仍需改进。

Abstract: Quantifying and propagating modeling uncertainties is crucial for reliability
analysis, robust optimization, and other model-based algorithmic processes in
engineering design and control. Now, physics-informed machine learning (PIML)
methods have emerged in recent years as a new alternative to traditional
computational modeling and surrogate modeling methods, offering a balance
between computing efficiency, modeling accuracy, and interpretability. However,
their ability to predict and propagate modeling uncertainties remains mostly
unexplored. In this paper, a promising class of auto-differentiable hybrid PIML
architectures that combine partial physics and neural networks or ANNs (for
input transformation or adaptive parameter estimation) is integrated with
Bayesian Neural networks (replacing the ANNs); this is done with the goal to
explore whether BNNs can successfully provision uncertainty propagation
capabilities in the PIML architectures as well, further supported by the
auto-differentiability of these architectures. A two-stage training process is
used to alleviate the challenges traditionally encountered in training
probabilistic ML models. The resulting BNN-integrated PIML architecture is
evaluated on an analytical benchmark problem and flight experiments data for a
fixed-wing RC aircraft, with prediction performance observed to be slightly
worse or at par with purely data-driven ML and original PIML models. Moreover,
Monte Carlo sampling of probabilistic BNN weights was found to be most
effective in propagating uncertainty in the BNN-integrated PIML architectures.

</details>


### [186] [RLPR: Extrapolating RLVR to General Domains without Verifiers](https://arxiv.org/abs/2506.18254)
*Tianyu Yu,Bo Ji,Shouli Wang,Shu Yao,Zefan Wang,Ganqu Cui,Lifan Yuan,Ning Ding,Yuan Yao,Zhiyuan Liu,Maosong Sun,Tat-Seng Chua*

Key words: 强化学习, 大语言模型, 推理能力, 奖励信号, RLPR

TL;DR: RLPR是一种无需验证器的强化学习框架，利用LLM自身的概率评分作为奖励信号，显著提升了推理能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决RLVR在非数学和代码领域的适用性问题，因其依赖领域特定验证器而复杂且难以扩展。

Method: 提出RLPR框架，使用LLM生成答案的自身概率作为奖励，并通过prob-to-reward和稳定化方法减少噪音。

Result: 在四个通用领域和三个数学基准测试中，RLPR显著提升了Gemma、Llama和Qwen模型的推理能力，优于VeriFree和General-Reasoner。

Conclusion: RLPR通过利用LLM自身概率作为奖励信号，成功扩展了RLVR的适用范围，为通用领域的强化学习推理提供了新思路。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising
potential in advancing the reasoning capabilities of LLMs. However, its success
remains largely confined to mathematical and code domains. This primary
limitation stems from the heavy reliance on domain-specific verifiers, which
results in prohibitive complexity and limited scalability. To address the
challenge, our key observation is that LLM's intrinsic probability of
generating a correct free-form answer directly indicates its own evaluation of
the reasoning reward (i.e., how well the reasoning process leads to the correct
answer). Building on this insight, we propose RLPR, a simple verifier-free
framework that extrapolates RLVR to broader general domains. RLPR uses the
LLM's own token probability scores for reference answers as the reward signal
and maximizes the expected reward during training. We find that addressing the
high variance of this noisy probability reward is crucial to make it work, and
propose prob-to-reward and stabilizing methods to ensure a precise and stable
reward from LLM intrinsic probabilities. Comprehensive experiments in four
general-domain benchmarks and three mathematical benchmarks show that RLPR
consistently improves reasoning capabilities in both areas for Gemma, Llama,
and Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6
points on TheoremQA and 7.5 points on Minerva, and even surpasses strong
verifier-model-dependent approaches General-Reasoner by 1.6 average points
across seven benchmarks.

</details>


### [187] [Ground tracking for improved landmine detection in a GPR system](https://arxiv.org/abs/2506.18258)
*Li Tang,Peter A. Torrione,Cihat Eldeniz,Leslie M. Collins*

Key words: 探地雷达,地面反弹,卡尔曼滤波,粒子滤波,地雷检测

TL;DR: 提出了基于卡尔曼滤波和粒子滤波的地面反弹跟踪算法，用于提高探地雷达在低金属含量地雷检测中的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 地面反弹干扰是探地雷达数据中的主要问题，影响地雷检测性能。

Method: 采用卡尔曼滤波和粒子滤波框架，将地面反弹位置建模为隐藏状态，并通过自适应参数和特征更新处理不同环境条件。

Result: 实验验证表明，改进的地面反弹跟踪算法提升了地雷检测性能。

Conclusion: 改进的地面反弹跟踪算法有效降低了干扰，提升了地雷检测效果。

Abstract: Ground penetrating radar (GPR) provides a promising technology for accurate
subsurface object detection. In particular, it has shown promise for detecting
landmines with low metal content. However, the ground bounce (GB) that is
present in GPR data, which is caused by the dielectric discontinuity between
soil and air, is a major source of interference and degrades landmine detection
performance. To mitigate this interference, GB tracking algorithms formulated
using both a Kalman filter (KF) and a particle filter (PF) framework are
proposed. In particular, the location of the GB in the radar signal is modeled
as the hidden state in a stochastic system for the PF approach. The
observations are the 2D radar images, which arrive scan by scan along the
down-track direction. An initial training stage sets parameters automatically
to accommodate different ground and weather conditions. The features associated
with the GB description are updated adaptively with the arrival of new data.
The prior distribution for a given location is predicted by propagating
information from two adjacent channels/scans, which ensures that the overall GB
surface remains smooth. The proposed algorithms are verified in experiments
utilizing real data, and their performances are compared with other GB tracking
approaches. We demonstrate that improved GB tracking contributes to improved
performance for the landmine detection problem.

</details>


### [188] [ARD-LoRA: Dynamic Rank Allocation for Parameter-Efficient Fine-Tuning of Foundation Models with Heterogeneous Adaptation Needs](https://arxiv.org/abs/2506.18267)
*Haseeb Ullah Khan Shinwari,Muhammad Usama*

Key words: LoRA, 动态自适应, 稀疏性, 参数效率, 多模态适应

TL;DR: 本文提出了一种名为ARD-LoRA的动态低秩自适应方法，通过可学习的缩放因子自动分配秩，优化任务性能和参数效率，实验表明其性能接近全微调且显著节省资源。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统LoRA方法使用固定秩，忽视了变换器层和注意力头的异质性学习动态，限制了其效率和适应性。

Method: 引入可学习的缩放因子，通过元目标优化，结合L1稀疏性和TV正则化，实现持续、可微分、每个头的自适应秩分配。

Result: 在LLAMA-3.1-70B和PaliGemma-2上，ARD-LoRA达到99.3%全微调性能，仅需0.32%可训练参数，并减少41%多模态适应内存。

Conclusion: 动态、细粒度的秩分配是高效基础模型适应的关键范例。

Abstract: Conventional Low-Rank Adaptation (LoRA) methods employ a fixed rank, imposing
uniform adaptation across transformer layers and attention heads despite their
heterogeneous learning dynamics. This paper introduces Adaptive Rank Dynamic
LoRA (ARD-LoRA), a novel framework that automates rank allocation through
learnable scaling factors. These factors are optimized via a meta-objective
balancing task performance and parameter efficiency, incorporating $\ell_1$
sparsity for minimal rank and Total Variation regularization for stable rank
transitions. ARD-LoRA enables continuous, differentiable, per-head rank
adaptation. Experiments on LLAMA-3.1-70B and PaliGemma-2 demonstrate ARD-LoRA's
efficacy, achieving up to 99.3% of full fine-tuning performance with only 0.32%
trainable parameters, outperforming strong baselines like DoRA and AdaLoRA.
Furthermore, it reduces multimodal adaptation memory by 41%. These results
establish dynamic, fine-grained rank allocation as a critical paradigm for
efficient foundation model adaptation.

</details>


### [189] [Memory-Augmented Architecture for Long-Term Context Handling in Large Language Models](https://arxiv.org/abs/2506.18271)
*Haseeb Ullah Khan Shinwari,Muhammad Usama*

Key words: 大语言模型, 记忆增强, 上下文处理, 对话连贯性

TL;DR: 论文提出了一种记忆增强架构以解决大语言模型在处理长对话时的上下文记忆不足问题，显著提升了对话连贯性和响应质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大语言模型在长对话中因有限的上下文记忆导致交互不连贯，用户体验下降。

Method: 采用了一种动态检索、更新和修剪历史交互信息的记忆增强架构。

Result: 实验表明该方案显著提升了上下文连贯性，降低了内存开销，并改善了响应质量。

Conclusion: 该架构在交互式系统中具有实时应用的潜力。

Abstract: Large Language Models face significant challenges in maintaining coherent
interactions over extended dialogues due to their limited contextual memory.
This limitation often leads to fragmented exchanges and reduced relevance in
responses, diminishing user experience. To address these issues, we propose a
memory-augmented architecture that dynamically retrieves, updates, and prunes
relevant information from past interactions, ensuring effective long-term
context handling. Experimental results demonstrate that our solution
significantly improves contextual coherence, reduces memory overhead, and
enhances response quality, showcasing its potential for real-time applications
in interactive systems.

</details>


### [190] [Leveraging Large Language Models for Information Verification -- an Engineering Approach](https://arxiv.org/abs/2506.18274)
*Nguyen Nang Hung,Nguyen Thanh Trong,Vuong Thanh Toan,Nguyen An Phuoc,Dao Minh Tu,Nguyen Manh Duc Tuan,Nguyen Dinh Mau*

Key words: 多媒体新闻验证，大语言模型，自动化流程，GPT-4o

TL;DR: 本文提出了一种基于大语言模型（如GPT-4o）的多媒体新闻源验证的实用工程方法，通过自动化流程处理图像和视频，并将其与元数据交叉验证。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决多媒体新闻源验证的自动化需求，减少人工干预。

Method: 1. 使用Google工具生成元数据；2. 多媒体数据分段、清理并转换为帧；3. 选择最具信息量的帧；4. 与元数据交叉验证；5. 提取音频文本进一步验证。

Result: 实现了基于GPT-4o的自动化验证流程，人工干预仅限于最终验证。

Conclusion: 该方法通过自动化流程高效验证多媒体新闻源，显著减少人工工作量。

Abstract: For the ACMMM25 challenge, we present a practical engineering approach to
multimedia news source verification, utilizing Large Language Models (LLMs)
like GPT-4o as the backbone of our pipeline. Our method processes images and
videos through a streamlined sequence of steps: First, we generate metadata
using general-purpose queries via Google tools, capturing relevant content and
links. Multimedia data is then segmented, cleaned, and converted into frames,
from which we select the top-K most informative frames. These frames are
cross-referenced with metadata to identify consensus or discrepancies.
Additionally, audio transcripts are extracted for further verification.
Noticeably, the entire pipeline is automated using GPT-4o through prompt
engineering, with human intervention limited to final validation.

</details>


### [191] [Learning Causal Graphs at Scale: A Foundation Model Approach](https://arxiv.org/abs/2506.18285)
*Naiyu Yin,Tian Gao,Yue Yu*

Key words: DAG学习, 注意力机制, 结构方程模型, 多任务学习, 小样本, 预训练

TL;DR: 该论文提出了一种名为Attention-DAG (ADAG)的新方法，利用线性变换器和注意力机制来学习多个线性结构方程模型（SEMs），解决了DAG学习在小样本情况下的计算成本和可识别性问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于DAG在AI研究中的重要性和其在小样本情况下的学习挑战（如计算成本高和可识别性差），作者希望通过多任务学习和共享低维先验来改进DAG学习。

Method: 基于注意力机制的非线性核架构ADAG，用于学习多个线性SEMs，并通过连续优化问题将多任务学习中的共同结构属性捕获为共享先验。

Result: 在合成数据集上评估，ADAG在DAG学习准确性和零样本推理效率上显著提升。

Conclusion: ADAG是首个针对DAG学习设计的预训练基础模型，为因果发现提供更高效和通用的下游应用方案。

Abstract: Due to its human-interpretability and invariance properties, Directed Acyclic
Graph (DAG) has been a foundational tool across various areas of AI research,
leading to significant advancements. However, DAG learning remains highly
challenging, due to its super-exponential growth in computational cost and
identifiability issues, particularly in small-sample regimes. To address these
two challenges, in this work we leverage the recent success of linear
transformers and develop a foundation model approach for discovering multiple
order-consistent DAGs across tasks. In particular, we propose Attention-DAG
(ADAG), a novel attention-mechanism-based architecture for learning multiple
linear Structural Equation Models (SEMs). ADAG learns the mapping from observed
data to both graph structure and parameters via a nonlinear attention-based
kernel, enabling efficient multi-task estimation of the underlying linear SEMs.
By formulating the learning process across multiple tasks as a continuous
optimization problem, the pre-trained ADAG model captures the common structural
properties as a shared low-dimensional prior, thereby reducing the
ill-posedness of downstream DAG learning tasks in small-sample regimes. We
evaluate our proposed approach on benchmark synthetic datasets and find that
ADAG achieves substantial improvements in both DAG learning accuracy and
zero-shot inference efficiency. To the best of our knowledge, this is the first
practical approach for pre-training a foundation model specifically designed
for DAG learning, representing a step toward more efficient and generalizable
down-stream applications in causal discovery.

</details>


### [192] [Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning](https://arxiv.org/abs/2506.18330)
*Lixin Wu,Na Cai,Qiao Cheng,Jiachen Wang,Yitao Duan*

Key words: Confucius3-Math, 大语言模型, 强化学习, 数学推理, K-12教育

TL;DR: Confucius3-Math是一个开源的大语言模型（14B参数），专注于解决中国K-12数学问题，并通过强化学习优化性能，同时实现低成本高效推理。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在通过AI提升教育和知识传播，特别针对中国K-12学生和教育工作者的数学学习需求。

Method: 采用后训练的大规模强化学习（RL），结合三种技术创新：目标熵正则化、近期样本恢复和策略特定难度加权。

Result: 在多种数学推理任务上表现优异，超越了许多规模更大的模型，并在单消费级GPU上高效运行。

Conclusion: 研究表明，低成本构建特定领域强大的推理模型是可行的，并开源了模型和代码。

Abstract: We introduce Confucius3-Math, an open-source large language model with 14B
parameters that (1) runs efficiently on a single consumer-grade GPU; (2)
achieves SOTA performances on a range of mathematical reasoning tasks,
outperforming many models with significantly larger sizes. In particular, as
part of our mission to enhancing education and knowledge dissemination with AI,
Confucius3-Math is specifically committed to mathematics learning for Chinese
K-12 students and educators. Built via post-training with large-scale
reinforcement learning (RL), Confucius3-Math aligns with national curriculum
and excels at solving main-stream Chinese K-12 mathematical problems with low
cost. In this report we share our development recipe, the challenges we
encounter and the techniques we develop to overcome them. In particular, we
introduce three technical innovations: Targeted Entropy Regularization, Recent
Sample Recovery and Policy-Specific Hardness Weighting. These innovations
encompass a new entropy regularization, a novel data scheduling policy, and an
improved group-relative advantage estimator. Collectively, they significantly
stabilize the RL training, improve data efficiency, and boost performance. Our
work demonstrates the feasibility of building strong reasoning models in a
particular domain at low cost. We open-source our model and code at
https://github.com/netease-youdao/Confucius3-Math.

</details>


### [193] [Learning High-Quality Latent Representations for Anomaly Detection and Signal Integrity Enhancement in High-Speed Signals](https://arxiv.org/abs/2506.18288)
*Muhammad Usama,Hee-Deok Jang,Soham Shanbhag,Yoo-Chang Sung,Seung-Jun Bae,Dong Eui Chang*

Key words: 异常检测,信号完整性,自编码器,联合训练,高速动态随机存储

TL;DR: 本文提出了一种联合训练框架，用于改进高速动态随机存储信号的异常检测和信号完整性，通过自编码器和分类器的结合学习更独特的潜在表示，并在实验中优于基线方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决高速动态随机存储信号中异常检测和信号完整性的双重挑战。

Method: 提出联合训练框架，结合自编码器和分类器，专注于有效数据特征，学习更独特的潜在表示。

Result: 在三种异常检测算法中表现优于两种基线方法，信号完整性提升平均11.3%。

Conclusion: 该框架在异常检测和信号完整性方面均表现出色，具有实际应用潜力。

Abstract: This paper addresses the dual challenge of improving anomaly detection and
signal integrity in high-speed dynamic random access memory signals. To achieve
this, we propose a joint training framework that integrates an autoencoder with
a classifier to learn more distinctive latent representations by focusing on
valid data features. Our approach is evaluated across three anomaly detection
algorithms and consistently outperforms two baseline methods. Detailed ablation
studies further support these findings. Furthermore, we introduce a signal
integrity enhancement algorithm that improves signal integrity by an average of
11.3%. The source code and data used in this study are available at
https://github.com/Usama1002/learning-latent-representations.

</details>


### [194] [SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation](https://arxiv.org/abs/2506.18349)
*Zichong Li,Chen Liang,Zixuan Zhang,Ilgee Hong,Young Jin Kim,Weizhu Chen,Tuo Zhao*

Key words: Mixture of Experts, Model Compression, Staged Knowledge Distillation, Structured Pruning, Resource Efficiency

TL;DR: SlimMoE是一种多阶段压缩框架，能够将大型Mixture of Experts（MoE）模型压缩为更小、高效的版本，同时避免从头训练的高成本。该方法通过分阶段减少参数数量和知识迁移，显著提升了性能表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大型MoE模型的高内存需求使其在资源受限环境中难以微调或部署。为了解决这一问题，研究团队开发了SlimMoE框架，旨在降低模型大小和计算成本，同时保持性能。

Method: SlimMoE采用多阶段压缩策略，通过逐步减少专家数量（slimming experts）和知识迁移（staged knowledge distillation），避免一次性剪枝导致的性能下降。

Result: 压缩后的模型（Phi-mini-MoE和Phi-tiny-MoE）在资源消耗显著降低的情况下，性能优于同类规模模型，并能与更大模型竞争。例如，Phi-mini-MoE在仅使用2/3激活参数的情况下，表现优于Phi-3-mini。

Conclusion: 结构化剪枝与分阶段蒸馏的结合为创建高质量、紧凑的MoE模型提供了有效路径，推动了MoE架构的更广泛应用。

Abstract: The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm
for scaling large language models (LLMs) while maintaining inference
efficiency. However, their enormous memory requirements make them prohibitively
expensive to fine-tune or deploy in resource-constrained environments. To
address this challenge, we introduce SlimMoE, a multi-stage compression
framework for transforming large MoE models into much smaller, efficient
variants without incurring the prohibitive costs of training from scratch. Our
method systematically reduces parameter counts by slimming experts and
transferring knowledge through intermediate stages, effectively mitigating the
performance degradation common in one-shot pruning approaches. Using this
framework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to
create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE
(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of
the original model's training data. These compressed models can be fine-tuned
on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them
highly suitable for academic and resource-limited settings. Our experiments
demonstrate that these compressed models outperform others of similar size and
remain competitive with larger models. For instance, Phi-mini-MoE achieves
similar or better performance to Phi-3-mini using only 2/3 of the activated
parameters and yields comparable MMLU scores to Llama 3.1 8B despite having
significantly lower latency. Our findings demonstrate that structured pruning
combined with staged distillation offers an effective path to creating
high-quality, compact MoE models, paving the way for broader adoption of MoE
architectures. We make our models publicly available at
https://huggingface.co/microsoft/Phi-mini-MoE-instruct and
https://huggingface.co/microsoft/Phi-tiny-MoE-instruct .

</details>


### [195] [Instability in Diffusion ODEs: An Explanation for Inaccurate Image Reconstruction](https://arxiv.org/abs/2506.18290)
*Han Zhang,Jinghong Mao,Shangwen Zhu,Zhantao Yang,Lianghua Huang,Yu Liu,Deli Zhao,Ruili Feng,Fan Cheng*

Key words: 扩散重构, PF-ODE, 不稳定性, 重构误差, 高维数据

TL;DR: 论文研究了扩散重构中的不稳定性问题，并证明其会放大重构误差，特别是在高维数据中。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 目前扩散重构中存在无法用数值误差解释的明显误差，研究发现这些误差源于PF-ODE生成过程中的不稳定性。

Method: 通过数值实验和理论分析，研究PF-ODE生成过程中的不稳定性及其对重构误差的影响。

Result: 实验和理论均表明，不稳定性在高维数据中会显著放大重构误差。

Conclusion: 扩散重构存在固有挑战，未来改进需考虑不稳定性问题。

Abstract: Diffusion reconstruction plays a critical role in various applications such
as image editing, restoration, and style transfer. In theory, the
reconstruction should be simple - it just inverts and regenerates images by
numerically solving the Probability Flow-Ordinary Differential Equation
(PF-ODE). Yet in practice, noticeable reconstruction errors have been observed,
which cannot be well explained by numerical errors. In this work, we identify a
deeper intrinsic property in the PF-ODE generation process, the instability,
that can further amplify the reconstruction errors. The root of this
instability lies in the sparsity inherent in the generation distribution, which
means that the probability is concentrated on scattered and small regions while
the vast majority remains almost empty. To demonstrate the existence of
instability and its amplification on reconstruction error, we conduct
experiments on both toy numerical examples and popular open-sourced diffusion
models. Furthermore, based on the characteristics of image data, we
theoretically prove that the instability's probability converges to one as the
data dimensionality increases. Our findings highlight the inherent challenges
in diffusion-based reconstruction and can offer insights for future
improvements.

</details>


### [196] [GeNeRT: A Physics-Informed Approach to Intelligent Wireless Channel Modeling via Generalizable Neural Ray Tracing](https://arxiv.org/abs/2506.18295)
*Kejia Bian,Meixia Tao,Shu Sun,Jun Yu*

Key words: 神经射线追踪, 泛化性, Fresnel, GPU加速

TL;DR: GeNeRT是一种通用性神经射线追踪框架，通过结合物理传播原理和神经网络，解决了现有方法在空间依赖性和电磁定律遵守上的局限性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有神经RT方法因空间依赖性强和电磁定律遵守不足而受限，GeNeRT旨在提升通用性、准确性和效率。

Method: GeNeRT通过Fresnel启发的神经网络设计和GPU张量化加速策略，支持场景内空间可移植性和场景间零样本泛化。

Result: 实验显示GeNeRT在未训练区域和全新环境中泛化能力强，多路径预测准确性优于基线，运行效率高于Wireless Insite。

Conclusion: GeNeRT通过改进网络架构和训练策略，有效捕捉物理原理，实现了泛化、准确和高效的射线追踪。

Abstract: Neural ray tracing (RT) has emerged as a promising paradigm for channel
modeling by combining physical propagation principles with neural networks. It
enables high modeling accuracy and efficiency. However, current neural RT
methods face two key limitations: constrained generalization capability due to
strong spatial dependence, and weak adherence to electromagnetic laws. In this
paper, we propose GeNeRT, a Generalizable Neural RT framework with enhanced
generalization, accuracy and efficiency. GeNeRT supports both intra-scenario
spatial transferability and inter-scenario zero-shot generalization. By
incorporating Fresnel-inspired neural network design, it also achieves higher
accuracy in multipath component (MPC) prediction. Furthermore, a GPU-tensorized
acceleration strategy is introduced to improve runtime efficiency. Extensive
experiments conducted in outdoor scenarios demonstrate that GeNeRT generalizes
well across untrained regions within a scenario and entirely unseen
environments, and achieves superior accuracy in MPC prediction compared to
baselines. Moreover, it outperforms Wireless Insite in runtime efficiency,
particularly in multi-transmitter settings. Ablation experiments validate the
effectiveness of the network architecture and training strategy in capturing
physical principles of ray-surface interactions.

</details>


### [197] [Sharpening the Spear: Adaptive Expert-Guided Adversarial Attack Against DRL-based Autonomous Driving Policies](https://arxiv.org/abs/2506.18304)
*Junchao Fan,Xuyang Lei,Xiaolin Chang*

Key words: 深度强化学习, 对抗攻击, 自动驾驶, 模仿学习, Mixture-of-Experts

TL;DR: 本文提出了一种自适应的专家引导对抗攻击方法，解决了DRL策略在自动驾驶中的安全漏洞问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有DRL策略易受对抗攻击，而传统攻击方法效率低且训练不稳定。

Method: 通过模仿学习和Mixture-of-Experts架构生成专家策略，并利用KL散度正则化引导对抗攻击。

Result: 实验表明，该方法在碰撞率、攻击效率和训练稳定性上优于现有方法。

Conclusion: 该方法显著提升了对抗攻击的稳定性和效率，尤其在专家策略不完美时表现优异。

Abstract: Deep reinforcement learning (DRL) has emerged as a promising paradigm for
autonomous driving. However, despite their advanced capabilities, DRL-based
policies remain highly vulnerable to adversarial attacks, posing serious safety
risks in real-world deployments. Investigating such attacks is crucial for
revealing policy vulnerabilities and guiding the development of more robust
autonomous systems. While prior attack methods have made notable progress, they
still face several challenges: 1) they often rely on high-frequency attacks,
yet critical attack opportunities are typically context-dependent and
temporally sparse, resulting in inefficient attack patterns; 2) restricting
attack frequency can improve efficiency but often results in unstable training
due to the adversary's limited exploration. To address these challenges, we
propose an adaptive expert-guided adversarial attack method that enhances both
the stability and efficiency of attack policy training. Our method first
derives an expert policy from successful attack demonstrations using imitation
learning, strengthened by an ensemble Mixture-of-Experts architecture for
robust generalization across scenarios. This expert policy then guides a
DRL-based adversary through a KL-divergence regularization term. Due to the
diversity of scenarios, expert policies may be imperfect. To address this, we
further introduce a performance-aware annealing strategy that gradually reduces
reliance on the expert as the adversary improves. Extensive experiments
demonstrate that our method achieves outperforms existing approaches in terms
of collision rate, attack efficiency, and training stability, especially in
cases where the expert policy is sub-optimal.

</details>


### [198] [No Training Wheels: Steering Vectors for Bias Correction at Inference Time](https://arxiv.org/abs/2506.18598)
*Aviral Gupta,Armaan Sethi,Ameesh Sethi*

Key words: 偏见缓解、分类模型、转向向量、无需训练、低成本

TL;DR: 提出一种无需训练、低成本的方法，通过计算多数与少数群体激活均值的差异定义“偏差向量”，并减除该向量以减少分类偏差，提升最差群体准确率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 神经网络分类器在不均衡数据集上易学得虚假相关性，导致对非典型群体表现不佳。现有方法通常需要重训练或高计算成本。本文旨在提供一种低成本、无需训练的解决方案。

Method: 计算多数与少数群体激活均值的差异以定义“偏差向量”，并在推理时从模型的残差流中减除该向量，以缓解偏见。

Result: 方法有效减少了分类偏差，提升了最差群体准确率，且无需训练或高计算成本。

Conclusion: 传统用于生成模型的“转向向量”同样适用于分类任务，提供了一种低成本、无需训练的偏见缓解方法。

Abstract: Neural network classifiers trained on datasets with uneven group
representation often inherit class biases and learn spurious correlations.
These models may perform well on average but consistently fail on atypical
groups. For example, in hair color classification, datasets may over-represent
females with blond hair, reinforcing stereotypes. Although various algorithmic
and data-centric methods have been proposed to address such biases, they often
require retraining or significant compute. In this work, we propose a cheap,
training-free method inspired by steering vectors used to edit behaviors in
large language models. We compute the difference in mean activations between
majority and minority groups to define a "bias vector," which we subtract from
the model's residual stream. This leads to reduced classification bias and
improved worst-group accuracy. We explore multiple strategies for extracting
and applying these vectors in transformer-like classifiers, showing that
steering vectors, traditionally used in generative models, can also be
effective in classification. More broadly, we showcase an extremely cheap,
inference time, training free method to mitigate bias in classification models.

</details>


### [199] [Structured Kolmogorov-Arnold Neural ODEs for Interpretable Learning and Symbolic Discovery of Nonlinear Dynamics](https://arxiv.org/abs/2506.18339)
*Wei Liu,Kiran Bacsa,Loon Ching Tang,Eleni Chatzi*

Key words: 非线性动态系统, 深度学> 习, Kolmogorov-Arnold网络, 物理可解释性, 符号回归

TL;DR: 论文提出SKANODEs框架，结合结构化状态空间建模与Kolmogorov-Arnold网络，实现高精度且物理可解释的非线性动态系统建模。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前深度学习方法在复杂系统行为建模中表现优异，但兼顾高精度与物理可解释性仍是挑战。

Method: 使用KAN作为通用函数逼近器，在结构化Neural ODE框架中恢复物理可解释的潜在状态，并通过符号回归提取系统的控制动态符号表达式。

Result: SKANODE在仿真和真实系统中表现出色，提供了解释性强、符合物理规律的模型。

Conclusion: SKANODE为解决非线性动态系统的建模与解释性问题提供了有效方法。

Abstract: Understanding and modeling nonlinear dynamical systems is a fundamental
problem across scientific and engineering domains. While deep learning has
demonstrated remarkable potential for learning complex system behavior,
achieving models that are both highly accurate and physically interpretable
remains a major challenge. To address this, we propose Structured
Kolmogorov-Arnold Neural ODEs (SKANODEs), a novel framework that integrates
structured state-space modeling with the Kolmogorov-Arnold Network (KAN).
SKANODE first employs a fully trainable KAN as a universal function
approximator within a structured Neural ODE framework to perform virtual
sensing, recovering latent states that correspond to physically interpretable
quantities such as positions and velocities. Once this structured latent
representation is established, we exploit the symbolic regression capability of
KAN to extract compact and interpretable expressions for the system's governing
dynamics. The resulting symbolic expression is then substituted back into the
Neural ODE framework and further calibrated through continued training to
refine its coefficients, enhancing both the precision of the discovered
equations and the predictive accuracy of system responses. Extensive
experiments on both simulated and real-world systems demonstrate that SKANODE
achieves superior performance while offering interpretable, physics-consistent
models that uncover the underlying mechanisms of nonlinear dynamical systems.

</details>


### [200] [Controlled Generation with Equivariant Variational Flow Matching](https://arxiv.org/abs/2506.18340)
*Floor Eijkelboom,Heiko Zimmermann,Sharvaree Vadgama,Erik J Bekkers,Max Welling,Christian A. Naesseth,Jan-Willem van de Meent*

Key words: 变分流匹配, 受控生成, 贝叶斯推断, 等变性, 分子生成

TL;DR: 该论文提出了一种基于变分流匹配（VFM）的受控生成目标，通过两种方式实现受控生成：端到端训练条件生成模型或作为贝叶斯推断问题。此外，论文还提出了适用于分子生成的等变VFM框架，并在实验中验证了其优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究旨在通过变分流匹配框架实现受控生成，并探索其在分子生成中的应用，同时提升生成模型的性能和灵活性。

Method: 方法包括将流匹配框架转化为变分推断问题，并通过端到端训练或贝叶斯推断实现受控生成。此外，提出了等变的VFM框架以确保对称性。

Result: 在无控制和受控分子生成任务中，模型均表现出色，实现了最先进的性能。

Conclusion: 研究通过VFM框架为受控生成和对称性感知的生成提供了一个可扩展且理论完备的解决方案。

Abstract: We derive a controlled generation objective within the framework of
Variational Flow Matching (VFM), which casts flow matching as a variational
inference problem. We demonstrate that controlled generation can be implemented
two ways: (1) by way of end-to-end training of conditional generative models,
or (2) as a Bayesian inference problem, enabling post hoc control of
unconditional models without retraining. Furthermore, we establish the
conditions required for equivariant generation and provide an equivariant
formulation of VFM tailored for molecular generation, ensuring invariance to
rotations, translations, and permutations. We evaluate our approach on both
uncontrolled and controlled molecular generation, achieving state-of-the-art
performance on uncontrolled generation and outperforming state-of-the-art
models in controlled generation, both with end-to-end training and in the
Bayesian inference setting. This work strengthens the connection between
flow-based generative modeling and Bayesian inference, offering a scalable and
principled framework for constraint-driven and symmetry-aware generation.

</details>


### [201] [ReDit: Reward Dithering for Improved LLM Policy Optimization](https://arxiv.org/abs/2506.18631)
*Chenxing Wei,Jiarui Yu,Ying Tiffany He,Hande Dong,Yao Shu,Fei Yu*

Key words: DeepSeek-R1, ReDit, 离散奖励, 梯度异常, 奖励抖动, GRPO

TL;DR: DeepSeek-R1通过基于规则的奖励系统提升LLM推理能力，但离散奖励可能导致梯度异常和不稳定优化。为此，提出ReDit方法，通过添加随机噪声扰动离散奖励信号，实现平滑梯度更新和加速收敛。实验证明ReDit高效且有效，训练步数仅为GRPO的10%，性能仍提升4%。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决离散奖励信号导致的梯度异常、优化不稳定和收敛慢问题。

Method: 提出ReDit，通过在离散奖励信号中添加简单随机噪声实现奖励抖动，提供探索性梯度并加速收敛。

Result: ReDit在多种任务中高效有效，训练步数减少90%，性能提升4%；理论分析和可视化验证其优势。

Conclusion: ReDit通过奖励抖动方法解决了离散奖励的问题，显著提升训练效率和模型性能。

Abstract: DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning
capabilities through its rule-based reward system. While it's a ''perfect''
reward system that effectively mitigates reward hacking, such reward functions
are often discrete. Our experimental observations suggest that discrete rewards
can lead to gradient anomaly, unstable optimization, and slow convergence. To
address this issue, we propose ReDit (Reward Dithering), a method that dithers
the discrete reward signal by adding simple random noise. With this perturbed
reward, exploratory gradients are continuously provided throughout the learning
process, enabling smoother gradient updates and accelerating convergence. The
injected noise also introduces stochasticity into flat reward regions,
encouraging the model to explore novel policies and escape local optima.
Experiments across diverse tasks demonstrate the effectiveness and efficiency
of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO
with only approximately 10% the training steps, and furthermore, still exhibits
a 4% performance improvement over vanilla GRPO when trained for a similar
duration. Visualizations confirm significant mitigation of gradient issues with
ReDit. Moreover, theoretical analyses are provided to further validate these
advantages.

</details>


### [202] [Multi-modal Anchor Gated Transformer with Knowledge Distillation for Emotion Recognition in Conversation](https://arxiv.org/abs/2506.18716)
*Jie Li,Shifei Ding,Lili Guo,Xuan Li*

Key words: 情感识别、多模态学习、知识蒸馏、提示学习、MAGTKD

TL;DR: 论文提出了一种名为MAGTKD的多模态方法，通过知识蒸馏和提示学习提升情感识别任务中模态表征的效果，并在IEMOCAP和MELD数据集上取得最佳性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前情感识别任务中，模态表征效率低且忽视模态贡献差异，存在计算复杂度高的问题，需要改进。

Method: 提出MAGTKD方法，结合提示学习增强文本模态表征，利用知识蒸馏加强弱模态表征，并设计多模态锚定门控变换器整合跨模态表征。

Result: 在IEMOCAP和MELD数据集上验证了知识蒸馏的有效性，并实现了最先进的情感识别性能。

Conclusion: MAGTKD通过多模态表征的优化和整合，显著提升了情感识别的表现。

Abstract: Emotion Recognition in Conversation (ERC) aims to detect the emotions of
individual utterances within a conversation. Generating efficient and
modality-specific representations for each utterance remains a significant
challenge. Previous studies have proposed various models to integrate features
extracted using different modality-specific encoders. However, they neglect the
varying contributions of modalities to this task and introduce high complexity
by aligning modalities at the frame level. To address these challenges, we
propose the Multi-modal Anchor Gated Transformer with Knowledge Distillation
(MAGTKD) for the ERC task. Specifically, prompt learning is employed to enhance
textual modality representations, while knowledge distillation is utilized to
strengthen representations of weaker modalities. Furthermore, we introduce a
multi-modal anchor gated transformer to effectively integrate utterance-level
representations across modalities. Extensive experiments on the IEMOCAP and
MELD datasets demonstrate the effectiveness of knowledge distillation in
enhancing modality representations and achieve state-of-the-art performance in
emotion recognition. Our code is available at:
https://github.com/JieLi-dd/MAGTKD.

</details>


### [203] [LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using LLMs and Preference Optimization](https://arxiv.org/abs/2506.18383)
*Koushik Viswanadha,Deepanway Ghosal,Somak Aditya*

Key words: 逻辑推理, 自然语言处理, 偏好优化, LLMs, DPO, KTO

TL;DR: 论文提出了一种通过微调偏好优化数据集（LogicPO）来改进LLMs逻辑推理能力的方法，显著提升了逻辑正确性并减少了语法错误。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前方法在将自然语言推理问题转换为逻辑表达时表现不足，阻碍了LLMs的整体推理能力。

Method: 1）引入新的监督和偏好优化数据集LogicPO；2）采用DPO和KTO技术微调开源LLMs。

Result: 最佳模型Phi-3.5在逻辑正确性上比GPT-3.5-turbo（8-shot）高出10%，且语法错误减少14%。

Conclusion: 该框架和改进的评估指标为提升LLMs的逻辑推理能力提供了有前景的方向。

Abstract: Logical reasoning is a key task for artificial intelligence due to it's role
in major downstream tasks such as Question Answering, Summarization. Recent
methods in improving the reasoning ability of LLMs fall short in correctly
converting a natural language reasoning problem to an equivalent logical
formulation, which hinders the framework's overall ability to reason. Towards
this, we propose to use finetuning on a preference optimization dataset to
learn to parse and represent a natural language problem as a whole to a
consistent logical program by 1) introducing a new supervised and preference
optimization dataset LogicPO, and 2) adopting popular techniques such as Direct
Preference Optimization (DPO), Kahneman-Tversky optimization (KTO) to finetune
open-source LLMs. Our best model with Phi-3.5 consistently outperforms
GPT-3.5-turbo's (8-shot) by producing 10% more logically correct and with 14%
less syntax errors. Through the framework and our improved evaluation metrics,
we offer a promising direction in improving the logical reasoning of LLMs by
better representing them in their logical formulations.

</details>


### [204] [Neural Total Variation Distance Estimators for Changepoint Detection in News Data](https://arxiv.org/abs/2506.18764)
*Csaba Zsolnai,Niels Lörch,Julian Arnold*

Key words: 变点检测,神经网络,公共话语,新闻分析

TL;DR: 提出一种基于神经网络的变点检测方法，通过分类器区分不同时间段新闻内容，成功识别重大事件。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 理解社会动态需要检测公共话语的变化，但高维稀疏噪声数据增加了变点检测的难度。

Method: 利用学习混淆方案的神经网络训练分类器，通过分类准确率估计内容分布差异，识别变点。

Result: 在合成数据集和《卫报》真实数据中有效检测到9/11、COVID-19等重大事件。

Conclusion: 该方法无需领域知识，能自主发现公共话语的重大变化，适用于新闻、政策分析和危机监测。

Abstract: Detecting when public discourse shifts in response to major events is crucial
for understanding societal dynamics. Real-world data is high-dimensional,
sparse, and noisy, making changepoint detection in this domain a challenging
endeavor. In this paper, we leverage neural networks for changepoint detection
in news data, introducing a method based on the so-called learning-by-confusion
scheme, which was originally developed for detecting phase transitions in
physical systems. We train classifiers to distinguish between articles from
different time periods. The resulting classification accuracy is used to
estimate the total variation distance between underlying content distributions,
where significant distances highlight changepoints. We demonstrate the
effectiveness of this method on both synthetic datasets and real-world data
from The Guardian newspaper, successfully identifying major historical events
including 9/11, the COVID-19 pandemic, and presidential elections. Our approach
requires minimal domain knowledge, can autonomously discover significant shifts
in public discourse, and yields a quantitative measure of change in content,
making it valuable for journalism, policy analysis, and crisis monitoring.

</details>


### [205] [ADNF-Clustering: An Adaptive and Dynamic Neuro-Fuzzy Clustering for Leukemia Prediction](https://arxiv.org/abs/2506.18396)
*Marco Aruta,Ciro Listone,Giuseppe Murano,Aniello Murano*

Key words: 动态聚类, 神经模糊系统, 白血病诊断, 高通量图像, 不确定性建模

TL;DR: 该论文提出了一种动态自适应的神经模糊聚类方法（ADNF），用于解决白血病显微镜图像的高通量数据分析问题。该方法通过结合CNN特征提取和在线模糊聚类，能够实时更新微簇参数并量化不确定性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的聚类方法无法适应动态变化的细胞模式，且在实时量化不确定性方面表现不足。为了解决这些问题，作者提出了ADNF方法。

Method: ADNF结合了卷积神经网络（CNN）的特征提取和在线模糊聚类引擎。初始化时使用模糊C均值（Fuzzy C-Means），之后通过模糊时间指数（FTI）动态更新微簇中心、密度和模糊参数。还包括密度加权合并和熵引导分裂的拓扑细化阶段。

Result: 在C-NMC白血病显微镜数据集上，ADNF的轮廓分数达到0.51，优于静态基线方法。

Conclusion: ADNF的自适应不确定性建模和无标签操作具有潜力，可集成到INFANT儿科肿瘤网络中，为个性化白血病管理提供支持。

Abstract: Leukemia diagnosis and monitoring rely increasingly on high-throughput image
data, yet conventional clustering methods lack the flexibility to accommodate
evolving cellular patterns and quantify uncertainty in real time. We introduce
Adaptive and Dynamic Neuro-Fuzzy Clustering, a novel streaming-capable
framework that combines Convolutional Neural Network-based feature extraction
with an online fuzzy clustering engine. ADNF initializes soft partitions via
Fuzzy C-Means, then continuously updates micro-cluster centers, densities, and
fuzziness parameters using a Fuzzy Temporal Index (FTI) that measures entropy
evolution. A topology refinement stage performs density-weighted merging and
entropy-guided splitting to guard against over- and under-segmentation. On the
C-NMC leukemia microscopy dataset, our tool achieves a silhouette score of
0.51, demonstrating superior cohesion and separation over static baselines. The
method's adaptive uncertainty modeling and label-free operation hold immediate
potential for integration within the INFANT pediatric oncology network,
enabling scalable, up-to-date support for personalized leukemia management.

</details>


### [206] [FREQuency ATTribution: Benchmarking Frequency-based Occlusion for Time Series Data](https://arxiv.org/abs/2506.18481)
*Dominique Mercier,Andreas Dengel,Sheraz,Ahmed*

Key words: 深度学习, 时间序列, 频域分析, 可解释性, FreqATT

TL;DR: FreqATT框架通过频域分析提升了时间序列网络的解释性，优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 深度神经网络性能强但缺乏解释性，现有方法对时间序列网络的分析不足。

Method: FreqATT通过评估相关频率并过滤或标记输入数据，实现后验解释。

Result: 频域分析能更突出输入信号的相关区域，且对信号波动更稳健。

Conclusion: FreqATT为时间序列网络提供了更高效的解释方法。

Abstract: Deep neural networks are among the most successful algorithms in terms of
performance and scalability in different domains. However, since these networks
are black boxes, their usability is severely restricted due to the lack of
interpretability. Existing interpretability methods do not address the analysis
of time-series-based networks specifically enough. This paper shows that an
analysis in the frequency domain can not only highlight relevant areas in the
input signal better than existing methods, but is also more robust to
fluctuations in the signal. In this paper, FreqATT is presented, a framework
that enables post-hoc networks to interpret time series analysis. To achieve
this, the relevant different frequencies are evaluated and the signal is either
filtered or the relevant input data is marked.

</details>


### [207] [Reliability-Adjusted Prioritized Experience Replay](https://arxiv.org/abs/2506.18482)
*Leonard S. Pleiss,Tobias Sutter,Maximilian Schiffer*

Key words: 经验回放, 强化学习, PER, ReaPER, 时序差异误差

TL;DR: 提出了ReaPER，一种基于时序差异误差可靠性的优先经验回放方法，优于传统PER。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统经验回放方法均匀采样，忽略了经验的学习潜力差异。需要更高效的采样方法。

Method: 引入时序差异误差可靠性度量，提出ReaPER算法。

Result: 理论证明ReaPER学习效率更高，实验表明其在多环境中优于PER（如Atari-5）。

Conclusion: ReaPER显著提升了经验回放的效率，适用于多种任务。

Abstract: Experience replay enables data-efficient learning from past experiences in
online reinforcement learning agents. Traditionally, experiences were sampled
uniformly from a replay buffer, regardless of differences in
experience-specific learning potential. In an effort to sample more
efficiently, researchers introduced Prioritized Experience Replay (PER). In
this paper, we propose an extension to PER by introducing a novel measure of
temporal difference error reliability. We theoretically show that the resulting
transition selection algorithm, Reliability-adjusted Prioritized Experience
Replay (ReaPER), enables more efficient learning than PER. We further present
empirical results showing that ReaPER outperforms PER across various
environment types, including the Atari-5 benchmark.

</details>


### [208] [AnalogNAS-Bench: A NAS Benchmark for Analog In-Memory Computing](https://arxiv.org/abs/2506.18495)
*Aniss Bessalah,Hatem Mohamed Abdelmoumen,Karima Benatchba,Hadjer Benmeziane*

Key words: 模拟内存计算,神经网络架构搜索,硬件非理想性,量化噪声,跳跃连接

TL;DR: 本文介绍了专为模拟内存计算（AIMC）设计的首个神经网络架构搜索（NAS）基准——AnalogNAS-Bench，揭示了关于AIMC硬件非理想性的关键发现，并提出了未来研究的建议。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有神经网络未考虑AIMC的非理想性，亟需专门设计的NAS基准来探索适应AIMC约束的架构。

Method: 通过引入AnalogNAS-Bench，系统评估NAS方法对AIMC非理想性（如量化噪声和时间漂移噪声）的适应性。

Result: 研究发现：传统量化技术不适用于AIMC噪声；稳健架构倾向于更宽、分支化的设计；跳跃连接能提升对时间漂移噪声的鲁棒性。

Conclusion: 当前NAS基准对AIMC存在局限性，未来NAS研究需关注模拟硬件的特殊性。

Abstract: Analog In-memory Computing (AIMC) has emerged as a highly efficient paradigm
for accelerating Deep Neural Networks (DNNs), offering significant energy and
latency benefits over conventional digital hardware. However, state-of-the-art
neural networks are not inherently designed for AIMC, as they fail to account
for its unique non-idealities. Neural Architecture Search (NAS) is thus needed
to systematically discover neural architectures optimized explicitly for AIMC
constraints. However, comparing NAS methodologies and extracting insights about
robust architectures for AIMC requires a dedicated NAS benchmark that
explicitly accounts for AIMC-specific hardware non-idealities. To address this,
we introduce AnalogNAS-Bench, the first NAS benchmark tailored specifically for
AIMC. Our study reveals three key insights: (1) standard quantization
techniques fail to capture AIMC-specific noises, (2) robust architectures tend
to feature wider and branched blocks, (3) skip connections improve resilience
to temporal drift noise. These insights highlight the limitations of current
NAS benchmarks for AIMC and pave the way for future analog-aware NAS. All the
implementations used in this paper can be found at
https://github.com/IBM/analog-nas/tree/main/analognasbench.

</details>


### [209] [PuckTrick: A Library for Making Synthetic Data More Realistic](https://arxiv.org/abs/2506.18499)
*Alessandra Agostini,Andrea Maurino,Blerina Spahiu*

Key words: 合成数据生成, 机器学习, 数据污染, 模型鲁棒性

TL;DR: Pucktrick是一个Python库，用于向合成数据集中引入可控错误，以评估机器学习模型在现实世界数据缺陷下的性能。实验表明，在受污染的合成数据上训练的模型优于在无错误合成数据上训练的模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现实数据难以获取且合成数据过于干净，无法反映真实数据的缺陷，影响了机器学习模型的泛化能力和鲁棒性。

Method: 开发Pucktrick工具，系统地向合成数据引入多种错误（如缺失值、噪声、异常值等），并评估模型性能。

Result: 树模型和线性模型在受污染的合成数据上表现更好。

Conclusion: 合成数据的可控污染有助于提高模型对真实数据的适应性。

Abstract: The increasing reliance on machine learning (ML) models for decision-making
requires high-quality training data. However, access to real-world datasets is
often restricted due to privacy concerns, proprietary restrictions, and
incomplete data availability. As a result, synthetic data generation (SDG) has
emerged as a viable alternative, enabling the creation of artificial datasets
that preserve the statistical properties of real data while ensuring privacy
compliance. Despite its advantages, synthetic data is often overly clean and
lacks real-world imperfections, such as missing values, noise, outliers, and
misclassified labels, which can significantly impact model generalization and
robustness. To address this limitation, we introduce Pucktrick, a Python
library designed to systematically contaminate synthetic datasets by
introducing controlled errors. The library supports multiple error types,
including missing data, noisy values, outliers, label misclassification,
duplication, and class imbalance, offering a structured approach to evaluating
ML model resilience under real-world data imperfections. Pucktrick provides two
contamination modes: one for injecting errors into clean datasets and another
for further corrupting already contaminated datasets. Through extensive
experiments on real-world financial datasets, we evaluate the impact of
systematic data contamination on model performance. Our findings demonstrate
that ML models trained on contaminated synthetic data outperform those trained
on purely synthetic, error-free data, particularly for tree-based and linear
models such as SVMs and Extra Trees.

</details>


### [210] [DDOT: A Derivative-directed Dual-decoder Ordinary Differential Equation Transformer for Dynamic System Modeling](https://arxiv.org/abs/2506.18522)
*Yang Chang,Kuang-Da Wang,Ping-Chun Hsieh,Cheng-Kuan Lin,Wen-Chih Peng*

Key words: ODE, 符号回归, 导数预测, 多维动态系统, DIV-diff

TL;DR: 论文提出了DDOT方法，通过引入导数预测任务，从单一轨迹重构多维ODE，表现优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统符号回归方法难以捕捉ODE的时空动态性和变量间相关性，ODEFormer等方法在单一轨迹评估上表现不稳定。

Method: 提出DIV-diff度量评估变量空间，并开发DDOT模型，结合导数预测任务重构ODE。

Result: DDOT在ODEBench上表现优异，重建和泛化任务分别提升4.58%和1.62%，DIV-diff降低3.55%。

Conclusion: DDOT能有效重构ODE，且在实际麻醉数据集上验证了实用性。

Abstract: Uncovering the underlying ordinary differential equations (ODEs) that govern
dynamic systems is crucial for advancing our understanding of complex
phenomena. Traditional symbolic regression methods often struggle to capture
the temporal dynamics and intervariable correlations inherent in ODEs.
ODEFormer, a state-of-the-art method for inferring multidimensional ODEs from
single trajectories, has made notable progress. However, its focus on
single-trajectory evaluation is highly sensitive to initial starting points,
which may not fully reflect true performance. To address this, we propose the
divergence difference metric (DIV-diff), which evaluates divergence over a grid
of points within the target region, offering a comprehensive and stable
analysis of the variable space. Alongside, we introduce DDOT
(Derivative-Directed Dual-Decoder Ordinary Differential Equation Transformer),
a transformer-based model designed to reconstruct multidimensional ODEs in
symbolic form. By incorporating an auxiliary task predicting the ODE's
derivative, DDOT effectively captures both structure and dynamic behavior.
Experiments on ODEBench show DDOT outperforms existing symbolic regression
methods, achieving an absolute improvement of 4.58% and 1.62% in $P(R^2 > 0.9)$
for reconstruction and generalization tasks, respectively, and an absolute
reduction of 3.55% in DIV-diff. Furthermore, DDOT demonstrates real-world
applicability on an anesthesia dataset, highlighting its practical impact.

</details>


### [211] [Federated Learning from Molecules to Processes: A Perspective](https://arxiv.org/abs/2506.18525)
*Jan G. Rittig,Clemens Kortmann*

Key words: 联邦学习, 化学工程, 机器学习, 数据隐私, 图神经网络, 自编码器

TL;DR: 该论文探讨了联邦学习在化学工程中的应用潜力，通过典型案例研究表明，联合训练的模型精度显著高于单独训练，同时保护了数据隐私。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 化学工程领域的机器学习模型训练受限于数据孤岛问题，联邦学习提供了一种解决方案，能够在保护数据隐私的同时实现联合训练。

Method: 论文讨论了联邦学习在化学工程多个领域的潜在应用，并通过两个案例研究展示其实际效果，包括使用图神经网络预测二元混合物活度系数以及使用自编码器进行蒸馏塔系统识别。

Result: 联邦学习联合训练的模型精度显著高于单独训练的模型，且与使用所有数据联合训练的模型性能相当。

Conclusion: 联邦学习在化学工程中具有巨大潜力，能够在保护数据隐私的同时提升模型性能，适合未来工业应用。

Abstract: We present a perspective on federated learning in chemical engineering that
envisions collaborative efforts in machine learning (ML) developments within
the chemical industry. Large amounts of chemical and process data are
proprietary to chemical companies and are therefore locked in data silos,
hindering the training of ML models on large data sets in chemical engineering.
Recently, the concept of federated learning has gained increasing attention in
ML research, enabling organizations to jointly train machine learning models
without disclosure of their individual data. We discuss potential applications
of federated learning in several fields of chemical engineering, from the
molecular to the process scale. In addition, we apply federated learning in two
exemplary case studies that simulate practical scenarios of multiple chemical
companies holding proprietary data sets: (i) prediction of binary mixture
activity coefficients with graph neural networks and (ii) system identification
of a distillation column with autoencoders. Our results indicate that ML models
jointly trained with federated learning yield significantly higher accuracy
than models trained by each chemical company individually and can perform
similarly to models trained on combined datasets from all companies. Federated
learning has therefore great potential to advance ML models in chemical
engineering while respecting corporate data privacy, making it promising for
future industrial applications.

</details>


### [212] [Transformer World Model for Sample Efficient Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.18537)
*Azad Deihim,Eduardo Alonso,Dimitra Apostolopoulou*

Key words: Multi-Agent, Transformer, Reinforcement Learning, World Model

TL;DR: MATWM是一种新型的基于Transformer的世界模型，用于多智能体强化学习，结合分散想象框架、半集中评论家和队友预测模块，在部分可观测环境中实现高性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决多智能体强化学习中的非平稳性和部分可观测性问题，设计一个能够建模和预测队友行为的世界模型。

Method: 结合分散想象框架、半集中评论家和队友预测模块，并采用优先回放机制以适应策略演化。

Result: 在StarCraft Multi-Agent Challenge等基准测试中达到最佳性能，样本效率高，仅需50K次交互即可接近最优表现。

Conclusion: MATWM在多智能体任务中表现优异，各组件对性能提升均有显著贡献。

Abstract: We present the Multi-Agent Transformer World Model (MATWM), a novel
transformer-based world model designed for multi-agent reinforcement learning
in both vector- and image-based environments. MATWM combines a decentralized
imagination framework with a semi-centralized critic and a teammate prediction
module, enabling agents to model and anticipate the behavior of others under
partial observability. To address non-stationarity, we incorporate a
prioritized replay mechanism that trains the world model on recent experiences,
allowing it to adapt to agents' evolving policies. We evaluated MATWM on a
broad suite of benchmarks, including the StarCraft Multi-Agent Challenge,
PettingZoo, and MeltingPot. MATWM achieves state-of-the-art performance,
outperforming both model-free and prior world model approaches, while
demonstrating strong sample efficiency, achieving near-optimal performance in
as few as 50K environment interactions. Ablation studies confirm the impact of
each component, with substantial gains in coordination-heavy tasks.

</details>


### [213] [Optimization-Induced Dynamics of Lipschitz Continuity in Neural Networks](https://arxiv.org/abs/2506.18588)
*Róisín Luo,James McDermott,Christian Gagné,Qiang Sun,Colm O'Riordan*

Key words: Lipschitz连续性, 随机梯度下降, 随机微分方程, 神经网络训练

TL;DR: 论文提出了一个数学模型，用于分析训练过程中神经网络的Lipschitz连续性随时间演化的动态过程，强调梯度流、梯度噪声及其投影对演化的影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究神经网络在训练过程中Lipschitz连续性的动态变化，以理解其对输入扰动的敏感性。

Method: 使用随机微分方程（SDEs）建模训练过程中的确定性及随机性力量，分析梯度流、噪声及其投影的作用。

Result: 理论分析与实验结果高度一致，揭示了梯度噪声、批量大小等因素对Lipschitz连续性的影响。

Conclusion: 提出的框架为理解神经网络训练的稳定性提供了新视角。

Abstract: Lipschitz continuity characterizes the worst-case sensitivity of neural
networks to small input perturbations; yet its dynamics (i.e. temporal
evolution) during training remains under-explored. We present a rigorous
mathematical framework to model the temporal evolution of Lipschitz continuity
during training with stochastic gradient descent (SGD). This framework
leverages a system of stochastic differential equations (SDEs) to capture both
deterministic and stochastic forces. Our theoretical analysis identifies three
principal factors driving the evolution: (i) the projection of gradient flows,
induced by the optimization dynamics, onto the operator-norm Jacobian of
parameter matrices; (ii) the projection of gradient noise, arising from the
randomness in mini-batch sampling, onto the operator-norm Jacobian; and (iii)
the projection of the gradient noise onto the operator-norm Hessian of
parameter matrices. Furthermore, our theoretical framework sheds light on such
as how noisy supervision, parameter initialization, batch size, and mini-batch
sampling trajectories, among other factors, shape the evolution of the
Lipschitz continuity of neural networks. Our experimental results demonstrate
strong agreement between the theoretical implications and the observed
behaviors.

</details>


### [214] [Simulation-Free Differential Dynamics through Neural Conservation Laws](https://arxiv.org/abs/2506.18604)
*Mengjian Hua,Eric Vanden-Eijnden,Ricky T. Q. Chen*

Key words: 扩散过程, Fokker-Planck方程, 神经网络, 生成建模, 最优控制

TL;DR: 提出了一种无需模拟的框架，用于在广泛目标函数上训练连续时间扩散过程，解决了现有方法限制多或计算开销大的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法要么限制多，要么计算昂贵，因此需要一种更通用且高效的方法。

Method: 采用耦合参数化方法，联合建模时间依赖密度函数和扩散过程动态，并直接嵌入Fokker-Planck方程和密度函数约束。

Result: 实现了无需模拟的训练，适用于多种问题形式，并在多个应用领域验证了有效性。

Conclusion: 该方法扩展了连续时间扩散过程的适用范围，提供了一种高效且通用的解决方案。

Abstract: We present a novel simulation-free framework for training continuous-time
diffusion processes over very general objective functions. Existing methods
typically involve either prescribing the optimal diffusion process -- which
only works for heavily restricted problem formulations -- or require expensive
simulation to numerically obtain the time-dependent densities and sample from
the diffusion process. In contrast, we propose a coupled parameterization which
jointly models a time-dependent density function, or probability path, and the
dynamics of a diffusion process that generates this probability path. To
accomplish this, our approach directly bakes in the Fokker-Planck equation and
density function requirements as hard constraints, by extending and greatly
simplifying the construction of Neural Conservation Laws. This enables
simulation-free training for a large variety of problem formulations, from
data-driven objectives as in generative modeling and dynamical optimal
transport, to optimality-based objectives as in stochastic optimal control,
with straightforward extensions to mean-field objectives due to the ease of
accessing exact density functions. We validate our method in a diverse range of
application domains from modeling spatio-temporal events to learning optimal
dynamics from population data.

</details>


### [215] [Policy gradient methods for ordinal policies](https://arxiv.org/abs/2506.18614)
*Simón Weinberger,Jairo Cugliari*

Key words: 强化学习，顺序回归，策略参数化，离散动作空间，连续动作任务

TL;DR: 论文提出了一种基于顺序回归模型的新策略参数化方法，解决了传统softmax方法无法捕捉动作顺序关系的缺陷，并在实际应用和连续动作任务中表现出色。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 受实际工业问题启发，作者旨在解决传统softmax方法在离散动作空间中无法捕捉动作顺序关系的局限性。

Method: 作者提出了一种基于顺序回归模型的策略参数化方法，并将其适配到强化学习环境中。

Result: 数值实验表明，该方法在实际应用和连续动作任务中表现优异，特别是在离散化动作空间并使用顺序策略时具有竞争力。

Conclusion: 基于顺序回归的策略参数化方法有效解决了传统softmax方法的不足，并在实际应用中展现出强大的潜力。

Abstract: In reinforcement learning, the softmax parametrization is the standard
approach for policies over discrete action spaces. However, it fails to capture
the order relationship between actions. Motivated by a real-world industrial
problem, we propose a novel policy parametrization based on ordinal regression
models adapted to the reinforcement learning setting. Our approach addresses
practical challenges, and numerical experiments demonstrate its effectiveness
in real applications and in continuous action tasks, where discretizing the
action space and applying the ordinal policy yields competitive performance.

</details>


### [216] [Pr{é}diction optimale pour un mod{è}le ordinal {à} covariables fonctionnelles](https://arxiv.org/abs/2506.18615)
*Simón Weinberger,Jairo Cugliari,Aurélie Le Cain*

Key words: 有序模型, 预测框架, 损失函数, 最小绝对偏差, 函数型协变量, 眼镜调光控制

TL;DR: 提出了一种针对有序模型的预测框架，介绍了基于损失函数的最优预测方法，并将其应用于眼镜调光控制算法的开发。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了优化有序模型的预测性能，并解决带有函数型协变量的有序模型的预测问题。

Method: 引入损失函数来定义最优预测，并给出了最小绝对偏差预测的显式形式；将带有函数型协变量的有序模型转化为经典的多标量协变量有序模型。

Result: 提出的方法被成功应用于EssilorLuxottica收集的数据集，用于开发眼镜调光控制算法。

Conclusion: 该框架为有序模型提供了有效的预测方法，并展示了在实际问题中的适用性。

Abstract: We present a prediction framework for ordinal models: we introduce optimal
predictions using loss functions and give the explicit form of the
Least-Absolute-Deviation prediction for these models. Then, we reformulate an
ordinal model with functional covariates to a classic ordinal model with
multiple scalar covariates. We illustrate all the proposed methods and try to
apply these to a dataset collected by EssilorLuxottica for the development of a
control algorithm for the shade of connected glasses.

</details>


### [217] [Multi-Agent Reinforcement Learning for Inverse Design in Photonic Integrated Circuits](https://arxiv.org/abs/2506.18627)
*Yannik Mahlau,Maximilian Schier,Christoph Reinders,Frederik Schubert,Marco Bügling,Bodo Rosenhahn*

Key words: 光子集成电路,逆向设计,强化学习,多智能体,优化

TL;DR: 该论文提出了一种基于多智能体强化学习的逆向设计方法，用于优化光子集成电路的设计，相比传统梯度优化方法更高效且避免了局部最小值问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统梯度优化方法在光子集成电路设计中容易陷入局部最小值，导致设计性能不佳。随着光学计算的需求增长，需要更高效的优化算法。

Method: 将设计空间离散化为网格，并使用多智能体强化学习算法进行优化，分解设计任务为数千个二进制变量的优化问题。

Result: 该方法在二维和三维设计任务中优于传统梯度优化，仅需几千次环境采样即可完成优化。

Conclusion: 多智能体强化学习为光子集成电路的逆向设计提供了高效解决方案，并可作为未来研究的基准。

Abstract: Inverse design of photonic integrated circuits (PICs) has traditionally
relied on gradientbased optimization. However, this approach is prone to end up
in local minima, which results in suboptimal design functionality. As interest
in PICs increases due to their potential for addressing modern hardware demands
through optical computing, more adaptive optimization algorithms are needed. We
present a reinforcement learning (RL) environment as well as multi-agent RL
algorithms for the design of PICs. By discretizing the design space into a
grid, we formulate the design task as an optimization problem with thousands of
binary variables. We consider multiple two- and three-dimensional design tasks
that represent PIC components for an optical computing system. By decomposing
the design space into thousands of individual agents, our algorithms are able
to optimize designs with only a few thousand environment samples. They
outperform previous state-of-the-art gradient-based optimization in both twoand
three-dimensional design tasks. Our work may also serve as a benchmark for
further exploration of sample-efficient RL for inverse design in photonics.

</details>


### [218] [On Equivariant Model Selection through the Lens of Uncertainty](https://arxiv.org/abs/2506.18629)
*Putri A. van der Linden,Alexander Timans,Dharmesh Tailor,Erik J. Bekkers*

Key words: 对称模型, 模型选择, 不确定性, 贝叶斯方法, 频繁主义

TL;DR: 本文研究了在模型选择中如何利用不确定性指标（如频繁主义、贝叶斯和校准方法）来评估具有不同对称偏好的预训练模型，发现不确定性指标通常与预测性能一致，但贝叶斯证据不一致，并提出可能的解决方案。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究如何通过不确定性指标改进具有对称偏好的预训练模型的选择，以避免因对称性偏好的错误指定而导致的性能下降。

Method: 使用频繁主义方法（如Conformal Prediction）、贝叶斯方法（如边缘似然）和基于校准的指标，对比基于误差的评估方法。

Result: 不确定性指标通常与预测性能一致，但贝叶斯模型证据表现不一致，作者认为这与贝叶斯和几何复杂性概念的不匹配有关。

Conclusion: 不确定性指标在指导对称感知的模型选择中具有潜力，但贝叶斯方法的适用性需进一步探讨。

Abstract: Equivariant models leverage prior knowledge on symmetries to improve
predictive performance, but misspecified architectural constraints can harm it
instead. While work has explored learning or relaxing constraints, selecting
among pretrained models with varying symmetry biases remains challenging. We
examine this model selection task from an uncertainty-aware perspective,
comparing frequentist (via Conformal Prediction), Bayesian (via the marginal
likelihood), and calibration-based measures to naive error-based evaluation. We
find that uncertainty metrics generally align with predictive performance, but
Bayesian model evidence does so inconsistently. We attribute this to a mismatch
in Bayesian and geometric notions of model complexity, and discuss possible
remedies. Our findings point towards the potential of uncertainty in guiding
symmetry-aware model selection.

</details>


### [219] [Granular-Ball-Induced Multiple Kernel K-Means](https://arxiv.org/abs/2506.18637)
*Shuyin Xia,Yifan Wang,Lifeng Shen,Guoyin Wang*

Key words: 多核聚类,球粒计算,聚类算法,高维数据

TL;DR: 提出了一种基于GBK的GB-MKKM框架，通过球粒计算提升多核聚类算法的效率和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现有算法在复杂数据分布中因点对点优化导致的效率低和鲁棒性差的问题。

Method: 引入球粒内核（GBK），提出GB-MKKM框架，利用球粒关系优化多核空间中的聚类。

Result: GB-MKKM在多种聚类任务中展现了更高的效率和性能。

Conclusion: 球粒计算显著提高了多核聚类算法的效率与鲁棒性。

Abstract: Most existing multi-kernel clustering algorithms, such as multi-kernel
K-means, often struggle with computational efficiency and robustness when faced
with complex data distributions. These challenges stem from their dependence on
point-to-point relationships for optimization, which can lead to difficulty in
accurately capturing data sets' inherent structure and diversity. Additionally,
the intricate interplay between multiple kernels in such algorithms can further
exacerbate these issues, effectively impacting their ability to cluster data
points in high-dimensional spaces. In this paper, we leverage granular-ball
computing to improve the multi-kernel clustering framework. The core of
granular-ball computing is to adaptively fit data distribution by balls from
coarse to acceptable levels. Each ball can enclose data points based on a
density consistency measurement. Such ball-based data description thus improves
the computational efficiency and the robustness to unknown noises.
Specifically, based on granular-ball representations, we introduce the
granular-ball kernel (GBK) and its corresponding granular-ball multi-kernel
K-means framework (GB-MKKM) for efficient clustering. Using granular-ball
relationships in multiple kernel spaces, the proposed GB-MKKM framework shows
its superiority in efficiency and clustering performance in the empirical
evaluation of various clustering tasks.

</details>


### [220] [Federated Loss Exploration for Improved Convergence on Non-IID Data](https://arxiv.org/abs/2506.18640)
*Christian Internò,Markus Olhofer,Yaochu Jin,Barbara Hammer*

Key words: 联邦学习,非IID数据,损失探索,梯度偏差,性能优化

TL;DR: FedLEx是一种创新的联邦学习方法，专门针对非独立同分布（non-IID）数据场景的挑战，通过联邦损失探索技术优化学习行为，显著提升性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决联邦学习在非IID数据中数据异构性和性能鲁棒性不足的问题。

Method: 采用联邦损失探索技术，客户端通过计算梯度偏差贡献全局指导矩阵，指导后续梯度更新。

Result: 在非IID条件下显著提升性能，仅需少量数据和轮次即可实现模型收敛。

Conclusion: FedLEx能有效克服多样联邦学习应用中的关键障碍。

Abstract: Federated learning (FL) has emerged as a groundbreaking paradigm in machine
learning (ML), offering privacy-preserving collaborative model training across
diverse datasets. Despite its promise, FL faces significant hurdles in
non-identically and independently distributed (non-IID) data scenarios, where
most existing methods often struggle with data heterogeneity and lack
robustness in performance. This paper introduces Federated Loss Exploration
(FedLEx), an innovative approach specifically designed to tackle these
challenges. FedLEx distinctively addresses the shortcomings of existing FL
methods in non-IID settings by optimizing its learning behavior for scenarios
in which assumptions about data heterogeneity are impractical or unknown. It
employs a federated loss exploration technique, where clients contribute to a
global guidance matrix by calculating gradient deviations for model parameters.
This matrix serves as a strategic compass to guide clients' gradient updates in
subsequent FL rounds, thereby fostering optimal parameter updates for the
global model. FedLEx effectively navigates the complex loss surfaces inherent
in non-IID data, enhancing knowledge transfer in an efficient manner, since
only a small number of epochs and small amount of data are required to build a
strong global guidance matrix that can achieve model convergence without the
need for additional data sharing or data distribution statics in a large client
scenario. Our extensive experiments with state-of-the art FL algorithms
demonstrate significant improvements in performance, particularly under
realistic non-IID conditions, thus highlighting FedLEx's potential to overcome
critical barriers in diverse FL applications.

</details>


### [221] [On Union-Closedness of Language Generation](https://arxiv.org/abs/2506.18642)
*Steve Hanneke,Amin Karbasi,Anay Mehrotra,Grigoris Velegkas*

Key words: 语言生成、极限模型、非均匀生成、不可数类别、对角化论证

TL;DR: 该论文研究了语言生成的极限模型，解决了Li等人提出的两个开放性问题，并证明了生成能力的有限并集不一定可生成，同时还探索了不可数类别的生成性质。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究语言生成在极限情况下的可能性与限制，探索不同生成概念（如均匀、非均匀和可生成）的可行性，并解决前人工作中的开放性问题。

Method: 通过构建特殊的类和使用新颖的对角化论证方法，研究了生成能力的有限并集性质以及不可数类别的生成性质。

Result: 证明了有限并集的生成能力不一定成立，且存在不满足EUC条件的非均匀可生成不可数类别。

Conclusion: 语言生成在极限情况下与传统统计学习任务（如分类）存在显著差异，特别是在生成能力的组合性方面，无法通过简单结合生成器来提升能力。

Abstract: We investigate language generation in the limit - a model by Kleinberg and
Mullainathan [NeurIPS 2024] and extended by Li, Raman, and Tewari [COLT 2025].
While Kleinberg and Mullainathan proved generation is possible for all
countable collections, Li et al. defined a hierarchy of generation notions
(uniform, non-uniform, and generatable) and explored their feasibility for
uncountable collections.
  Our first set of results resolve two open questions of Li et al. by proving
finite unions of generatable or non-uniformly generatable classes need not be
generatable. These follow from a stronger result: there is a non-uniformly
generatable class and a uniformly generatable class whose union is
non-generatable. This adds to the aspects along which language generation in
the limit is different from traditional tasks in statistical learning theory
like classification, which are closed under finite unions. In particular, it
implies that given two generators for different collections, one cannot combine
them to obtain a single "more powerful" generator, prohibiting this notion of
boosting.
  Our construction also addresses a third open question of Li et al. on whether
there are uncountable classes that are non-uniformly generatable and do not
satisfy the eventually unbounded closure (EUC) condition introduced by Li,
Raman, and Tewari. Our approach utilizes carefully constructed classes along
with a novel diagonalization argument that could be of independent interest in
the growing area of language generation.

</details>


### [222] [SaGIF: Improving Individual Fairness in Graph Neural Networks via Similarity Encoding](https://arxiv.org/abs/2506.18696)
*Yuchang Zhu,Jintang Li,Huizhe Zhang,Liang Chen,Zibin Zheng*

Key words: 个体公平性, 图神经网络, 相似性一致性, SaGIF

TL;DR: 论文探讨了图神经网络中的个体公平性，提出了相似性一致性和两种评估指标，并设计了SaGIF方法以提高公平性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究图神经网络中个体公平性不足的根源，并提出解决方案。

Method: 提出两种评估相似性的指标，并结合相似性表示设计了SaGIF方法。

Result: 实验证明SaGIF在提升个体公平性方面优于现有方法，且不影响性能。

Conclusion: SaGIF能有效解决图神经网络中的个体公平性问题。

Abstract: Individual fairness (IF) in graph neural networks (GNNs), which emphasizes
the need for similar individuals should receive similar outcomes from GNNs, has
been a critical issue. Despite its importance, research in this area has been
largely unexplored in terms of (1) a clear understanding of what induces
individual unfairness in GNNs and (2) a comprehensive consideration of
identifying similar individuals. To bridge these gaps, we conduct a preliminary
analysis to explore the underlying reason for individual unfairness and observe
correlations between IF and similarity consistency, a concept introduced to
evaluate the discrepancy in identifying similar individuals based on graph
structure versus node features. Inspired by our observations, we introduce two
metrics to assess individual similarity from two distinct perspectives:
topology fusion and feature fusion. Building upon these metrics, we propose
Similarity-aware GNNs for Individual Fairness, named SaGIF. The key insight
behind SaGIF is the integration of individual similarities by independently
learning similarity representations, leading to an improvement of IF in GNNs.
Our experiments on several real-world datasets validate the effectiveness of
our proposed metrics and SaGIF. Specifically, SaGIF consistently outperforms
state-of-the-art IF methods while maintaining utility performance. Code is
available at: https://github.com/ZzoomD/SaGIF.

</details>


### [223] [PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries](https://arxiv.org/abs/2506.18728)
*Steven Kolawole,Keshav Santhanam,Virginia Smith,Pratiksha Thaker*

Key words: LLM, 并行性, 提示解析, 基准测试, 结构感知执行

TL;DR: 论文摘要介绍了PARALLELPROMPT，这是第一个衡量自然用户提示中查询内并行性的基准，展示了通过分解提示结构实现加速的潜力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的LLM服务系统通常将用户提示视为单一输入，忽略了其中潜在的语义并行性。研究者希望通过分解提示结构来优化推理延迟。

Method: 研究者从公开的LLM聊天日志中收集了超过37,000条真实提示，并通过LLM辅助的规则化多语言验证提取了结构化模式。开发了执行套件对比串行与并行策略。

Result: 结果表明，超过75%的提示可成功解析出并行性，任务（如翻译、理解和比较分析）的加速高达5倍，且质量损失最小。

Conclusion: PARALLELPROMPT为标准化的结构感知执行研究提供了首个测试平台，展示了并行分解在LLM服务中的巨大潜力。

Abstract: LLM serving systems typically treat user prompts as monolithic inputs,
optimizing inference through decoding tricks or inter-query batching. However,
many real-world prompts contain latent semantic parallelism--decomposable
structures where subtasks can be executed independently to reduce latency while
preserving meaning. We introduce PARALLELPROMPT, the first benchmark for
measuring intra-query parallelism in natural user prompts. Our dataset
comprises over 37,000 real-world prompts from public LLM chat logs, each
annotated with a structured schema capturing task templates, shared context,
and iteration inputs. These schemas are extracted using LLM-assisted prompting
with rule-based multilingual validation. To evaluate the benefits of
decomposition, we provide an execution suite that benchmarks serial vs.
parallel strategies, measuring latency, structural adherence, and semantic
fidelity. Our results show that intra-query parallelism can be successfully
parsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks
like translation, comprehension, and comparative analysis, with minimal quality
degradation. By releasing this benchmark, curation pipeline, and evaluation
suite, we provide the first standardized testbed for studying structure-aware
execution in LLM serving pipelines.

</details>


### [224] [Towards Group Fairness with Multiple Sensitive Attributes in Federated Foundation Models](https://arxiv.org/abs/2506.18732)
*Yuning Yang,Han Yu,Tianrun Gao,Xiaodong Xu,Guangyu Wang*

Key words: 联邦基础模型、组公平性、多敏感属性、因果分析、可解释性

TL;DR: 本文提出了一种基于因果分析的联邦基础模型（FFM）中多敏感属性的公平性分析方法，首次研究了组公平性与多个敏感属性之间的因果关系。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在联邦基础模型（FFM）中，单一敏感属性的公平性研究无法解释多敏感属性之间的依赖关系，导致无法全面实现组公平性。

Method: 通过扩展FFM结构，结合因果发现与推断，同时权衡多个敏感属性和量化其因果关系。

Result: 实验验证了方法的有效性，提供了对建立可信赖且公平的FFM系统的解释性见解。

Conclusion: 该方法为FFM中的多敏感属性公平性问题提供了因果分析的新视角，有助于提升模型的公平性与可解释性。

Abstract: The deep integration of foundation models (FM) with federated learning (FL)
enhances personalization and scalability for diverse downstream tasks, making
it crucial in sensitive domains like healthcare. Achieving group fairness has
become an increasingly prominent issue in the era of federated foundation
models (FFMs), since biases in sensitive attributes might lead to inequitable
treatment for under-represented demographic groups. Existing studies mostly
focus on achieving fairness with respect to a single sensitive attribute. This
renders them unable to provide clear interpretability of dependencies among
multiple sensitive attributes which is required to achieve group fairness. Our
paper takes the first attempt towards a causal analysis of the relationship
between group fairness across various sensitive attributes in the FFM. We
extend the FFM structure to trade off multiple sensitive attributes
simultaneously and quantify the causal effect behind the group fairness through
causal discovery and inference. Extensive experiments validate its
effectiveness, offering insights into interpretability towards building
trustworthy and fair FFM systems.

</details>


### [225] [On the Existence of Universal Simulators of Attention](https://arxiv.org/abs/2506.18739)
*Debanjan Dutta,Faizanuddin Ansari,Anish Chakrabarty,Swagatam Das*

Key words: 变压器，注意力机制，RASP，模拟器，计算能力

TL;DR: 本文探讨了变压器编码器能否精确模拟普通注意力机制，提出了一种由变压器编码器组成的通用模拟器，并通过RASP框架实现了对注意力输出的完全复制。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 先前的研究通过训练证明了变压器在特定架构假设下的学习能力，但这些结果仅提供概率保证。本研究旨在从理论上解决变压器架构是否能精确模拟任意注意力机制的问题。

Method: 通过构建变压器编码器组成的通用模拟器Ω，利用RASP框架提出算法解决方案，完全复制注意力输出及其基础矩阵和激活操作。

Result: 证明了存在一种算法上可实现的数据无关解，这是首次将仅能通过学习近似的问题转化为可精确模拟的结果。

Conclusion: 变压器编码器能够精确模拟普通注意力机制，这一结果为变压器架构的计算能力提供了新的理论支持。

Abstract: Prior work on the learnability of transformers has established its capacity
to approximate specific algorithmic patterns through training under restrictive
architectural assumptions. Fundamentally, these arguments remain data-driven
and therefore can only provide a probabilistic guarantee. Expressivity, on the
contrary, has theoretically been explored to address the problems
\emph{computable} by such architecture. These results proved the
Turing-completeness of transformers, investigated bounds focused on circuit
complexity, and formal logic. Being at the crossroad between learnability and
expressivity, the question remains: \emph{can transformer architectures exactly
simulate an arbitrary attention mechanism, or in particular, the underlying
operations?} In this study, we investigate the transformer encoder's ability to
simulate a vanilla attention mechanism. By constructing a universal simulator
$\mathcal{U}$ composed of transformer encoders, we present algorithmic
solutions to identically replicate attention outputs and the underlying
elementary matrix and activation operations via RASP, a formal framework for
transformer computation. Our proofs, for the first time, show the existence of
an algorithmically achievable data-agnostic solution, previously known to be
approximated only by learning.

</details>


### [226] [Experimenting, Fast and Slow: Bayesian Optimization of Long-term Outcomes with Online Experiments](https://arxiv.org/abs/2506.18744)
*Qing Feng,Samuel Dalton,Benjamin Letham,Maximilian Balandat,Eytan Bakshy*

Key words: A/B测试,贝叶斯优化,序列实验,离线评估

TL;DR: 提出了一种结合快速实验和离线代理的新方法，用于在短时间内通过贝叶斯优化进行序列实验。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 优化互联网系统中的长期处理效果，解决传统序列实验耗时过长的问题。

Method: 结合快速实验（如偏向性实验）和离线代理（如离线策略评估）与长期慢速实验，进行贝叶斯优化。

Result: 能够在短时间内高效完成大规模动作空间的序列优化。

Conclusion: 该方法显著缩短了实验时间，同时保持了优化效果。

Abstract: Online experiments in internet systems, also known as A/B tests, are used for
a wide range of system tuning problems, such as optimizing recommender system
ranking policies and learning adaptive streaming controllers. Decision-makers
generally wish to optimize for long-term treatment effects of the system
changes, which often requires running experiments for a long time as short-term
measurements can be misleading due to non-stationarity in treatment effects
over time. The sequential experimentation strategies--which typically involve
several iterations--can be prohibitively long in such cases. We describe a
novel approach that combines fast experiments (e.g., biased experiments run
only for a few hours or days) and/or offline proxies (e.g., off-policy
evaluation) with long-running, slow experiments to perform sequential, Bayesian
optimization over large action spaces in a short amount of time.

</details>


### [227] [ContinualFlow: Learning and Unlearning with Neural Flow Matching](https://arxiv.org/abs/2506.18747)
*Lorenzo Simone,Davide Bacciu,Shuangge Ma*

Key words: ContinualFlow, Flow Matching, 目标性遗忘, 生成模型, 能量重加权

TL;DR: ContinualFlow是一种基于Flow Matching的生成模型目标性遗忘框架，通过基于能量的重加权损失实现数据分布中不需求区域的软去除。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在解决生成模型中目标性遗忘问题，避免重训练或直接访问需遗忘样本的需求。

Method: 利用基于能量的重加权损失和Flow Matching，通过能量代理引导遗忘过程。

Result: 在2D和图像领域实验中验证了框架有效性，并提供可解释的可视化和定量评估。

Conclusion: ContinualFlow为生成模型的软目标性遗忘提供了高效且理论支持的解决方案。

Abstract: We introduce ContinualFlow, a principled framework for targeted unlearning in
generative models via Flow Matching. Our method leverages an energy-based
reweighting loss to softly subtract undesired regions of the data distribution
without retraining from scratch or requiring direct access to the samples to be
unlearned. Instead, it relies on energy-based proxies to guide the unlearning
process. We prove that this induces gradients equivalent to Flow Matching
toward a soft mass-subtracted target, and validate the framework through
experiments on 2D and image domains, supported by interpretable visualizations
and quantitative evaluations.

</details>


### [228] [Sensitivity Analysis of Image Classification Models using Generalized Polynomial Chaos](https://arxiv.org/abs/2506.18751)
*Lukas Bahr,Lucas Poßner,Konstantin Weise,Sophie Gröger,Rüdiger Daub*

Key words: 图像分类,敏感性分析,Sobol指数,广义多项式混沌,预测质量

TL;DR: 本文研究了图像分类模型在预测质量中的敏感性，提出通过随机变量建模输入分布的变化，并用基于广义多项式混沌的Sobol指数量化其对输出的影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于模型、数据和领域偏移带来的不确定性导致分类模型输出过度自信，需通过敏感性分析理解这些模型。

Method: 提出用随机变量建模输入分布变化，并用广义多项式混沌（GPC）计算Sobol指数量化影响。

Result: 通过焊接缺陷分类和宝马生产设施标志分类的案例研究验证了方法的有效性。

Conclusion: 该方法能有效分析图像分类模型对输入变化的敏感性，为预测质量提供支持。

Abstract: Integrating advanced communication protocols in production has accelerated
the adoption of data-driven predictive quality methods, notably machine
learning (ML) models. However, ML models in image classification often face
significant uncertainties arising from model, data, and domain shifts. These
uncertainties lead to overconfidence in the classification model's output. To
better understand these models, sensitivity analysis can help to analyze the
relative influence of input parameters on the output. This work investigates
the sensitivity of image classification models used for predictive quality. We
propose modeling the distributional domain shifts of inputs with random
variables and quantifying their impact on the model's outputs using Sobol
indices computed via generalized polynomial chaos (GPC). This approach is
validated through a case study involving a welding defect classification
problem, utilizing a fine-tuned ResNet18 model and an emblem classification
model used in BMW Group production facilities.

</details>


### [229] [Shift Happens: Mixture of Experts based Continual Adaptation in Federated Learning](https://arxiv.org/abs/2506.18789)
*Rahul Atul Bhope,K. R. Jayaram,Praveen Venkateswaran,Nalini Venkatasubramanian*

Key words: 联邦学习，数据分布偏移，专家混合，最大均值差异，设施选址优化

TL;DR: 提出了ShiftEx框架，解决联邦学习中动态数据分布变化导致的性能下降问题，通过专家混合和优化策略，显著提升模型准确率和适应速度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习在动态数据分布环境下性能下降，需解决协变量和标签偏移问题。

Method: 提出ShiftEx框架，利用最大均值差异检测分布偏移，动态创建和训练专家模型，优化设施选址以减少不匹配。

Result: 实验显示在基准数据集上准确性提升5.5-12.9%，适应速度加快22-95%。

Conclusion: ShiftEx为联邦学习在非静态环境中提供了高效、隐私保护的解决方案。

Abstract: Federated Learning (FL) enables collaborative model training across
decentralized clients without sharing raw data, yet faces significant
challenges in real-world settings where client data distributions evolve
dynamically over time. This paper tackles the critical problem of covariate and
label shifts in streaming FL environments, where non-stationary data
distributions degrade model performance and require adaptive middleware
solutions. We introduce ShiftEx, a shift-aware mixture of experts framework
that dynamically creates and trains specialized global models in response to
detected distribution shifts using Maximum Mean Discrepancy for covariate
shifts. The framework employs a latent memory mechanism for expert reuse and
implements facility location-based optimization to jointly minimize covariate
mismatch, expert creation costs, and label imbalance. Through theoretical
analysis and comprehensive experiments on benchmark datasets, we demonstrate
5.5-12.9 percentage point accuracy improvements and 22-95 % faster adaptation
compared to state-of-the-art FL baselines across diverse shift scenarios. The
proposed approach offers a scalable, privacy-preserving middleware solution for
FL systems operating in non-stationary, real-world conditions while minimizing
communication and computational overhead.

</details>


### [230] [A Multi-view Divergence-Convergence Feature Augmentation Framework for Drug-related Microbes Prediction](https://arxiv.org/abs/2506.18797)
*Xin An,Ruijie Li,Qiao Ning,Shikai Guo,Hui Li,Qian Ma*

Key words: drug-microbe associations, multi-view learning, adversarial learning, attention mechanism

TL;DR: 提出了一个多视角的Divergence-Convergence Feature Augmentation框架（DCFA_DMP），用于预测药物-微生物关联，通过对抗学习和双向协同注意力机制优化特征空间，表现出优异的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前方法在药物-微生物关联预测中缺乏多视角特征的协同优化与融合，亟需一种更为高效的框架。

Method: DCFA_DMP采用对抗学习优化异构图和相似图的互补性，并提出双向协同注意力机制深度融合特征空间。

Result: 实验证明DCFA_DMP在药物-微生物关联预测中表现显著，且在冷启动实验中对新药物和微生物的预测同样有效。

Conclusion: DCFA_DMP框架稳定可靠，能够有效预测潜在的药物-微生物关联。

Abstract: In the study of drug function and precision medicine, identifying new
drug-microbe associations is crucial. However, current methods isolate
association and similarity analysis of drug and microbe, lacking effective
inter-view optimization and coordinated multi-view feature fusion. In our
study, a multi-view Divergence-Convergence Feature Augmentation framework for
Drug-related Microbes Prediction (DCFA_DMP) is proposed, to better learn and
integrate association information and similarity information. In the divergence
phase, DCFA_DMP strengthens the complementarity and diversity between
heterogeneous information and similarity information by performing Adversarial
Learning method between the association network view and different similarity
views, optimizing the feature space. In the convergence phase, a novel
Bidirectional Synergistic Attention Mechanism is proposed to deeply synergize
the complementary features between different views, achieving a deep fusion of
the feature space. Moreover, Transformer graph learning is alternately applied
on the drug-microbe heterogeneous graph, enabling each drug or microbe node to
focus on the most relevant nodes. Numerous experiments demonstrate DCFA_DMP's
significant performance in predicting drug-microbe associations. It also proves
effectiveness in predicting associations for new drugs and microbes in cold
start experiments, further confirming its stability and reliability in
predicting potential drug-microbe associations.

</details>


### [231] [Multi-Agent Online Control with Adversarial Disturbances](https://arxiv.org/abs/2506.18814)
*Anas Barakat,John Lazarsfeld,Georgios Piliouras,Antonios Varvitsiotis*

Key words: 多智能体控制,在线控制,对抗性扰动,梯度控制器,遗憾界限

TL;DR: 论文研究了多智能体线性动态系统中的在线控制问题，关注于对抗性扰动和个体目标的最小化，探讨了梯度控制器的鲁棒性以及智能体数量对个体遗憾的影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究多智能体控制问题，解决对抗性扰动和个体目标优化的挑战，特别是在机器人、经济学和能源系统中的广泛应用。

Method: 采用梯度控制器在对抗性扰动和多智能体动态系统中进行在线控制，分析个体遗憾的界限和智能体数量的影响。

Result: 证明了在最小通信假设下，所有智能体均能达到接近最优的子线性遗憾界限；在目标一致时，展示了时间变化潜在博弈的平衡性。

Conclusion: 研究表明梯度控制器在多智能体系统中具有鲁棒性，且能实现个体遗憾的优化，为目标一致的系统提供了均衡解。

Abstract: Multi-agent control problems involving a large number of agents with
competing and time-varying objectives are increasingly prevalent in
applications across robotics, economics, and energy systems. In this paper, we
study online control in multi-agent linear dynamical systems with disturbances.
In contrast to most prior work in multi-agent control, we consider an online
setting where disturbances are adversarial and where each agent seeks to
minimize its own, adversarial sequence of convex losses. In this setting, we
investigate the robustness of gradient-based controllers from single-agent
online control, with a particular focus on understanding how individual regret
guarantees are influenced by the number of agents in the system. Under minimal
communication assumptions, we prove near-optimal sublinear regret bounds that
hold uniformly for all agents. Finally, when the objectives of the agents are
aligned, we show that the multi-agent control problem induces a time-varying
potential game for which we derive equilibrium gap guarantees.

</details>


### [232] [Offline Goal-Conditioned Reinforcement Learning with Projective Quasimetric Planning](https://arxiv.org/abs/2506.18847)
*Anthony Kobanda,Waris Radji,Mathieu Petitbois,Odalric-Ambrym Maillard,Rémy Portelas*

Key words: 离线强化学习,目标条件强化学习,长时程任务,几何方法,子目标生成

TL;DR: 离线目标条件强化学习通过几何方法解决了长时程任务中的值估计误差问题，提出了Projective Quasimetric Planning (ProQ)框架。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对离线目标条件强化学习在长时程任务中因值估计误差导致的性能问题，提出一种基于几何的解决方案。

Method: 提出ProQ框架，通过学习非对称距离并结合排斥能量和结构化方向成本，实现关键点的均匀分布和子目标引导。

Result: ProQ在多样化导航基准测试中生成有意义的子目标，并成功完成长时程任务。

Conclusion: ProQ通过统一度量学习、关键点覆盖和目标条件控制，显著提升了长时程任务的性能。

Abstract: Offline Goal-Conditioned Reinforcement Learning seeks to train agents to
reach specified goals from previously collected trajectories. Scaling that
promises to long-horizon tasks remains challenging, notably due to compounding
value-estimation errors. Principled geometric offers a potential solution to
address these issues. Following this insight, we introduce Projective
Quasimetric Planning (ProQ), a compositional framework that learns an
asymmetric distance and then repurposes it, firstly as a repulsive energy
forcing a sparse set of keypoints to uniformly spread over the learned latent
space, and secondly as a structured directional cost guiding towards proximal
sub-goals. In particular, ProQ couples this geometry with a Lagrangian
out-of-distribution detector to ensure the learned keypoints stay within
reachable areas. By unifying metric learning, keypoint coverage, and
goal-conditioned control, our approach produces meaningful sub-goals and
robustly drives long-horizon goal-reaching on diverse a navigation benchmarks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [233] [Evaluating Generalization and Representation Stability in Small LMs via Prompting](https://arxiv.org/abs/2506.17289)
*Rahul Raja,Arpita Vats*

Key words: 小语言模型、少样本提示、监督微调、泛化能力、低资源

TL;DR: 比较小语言模型在少样本提示和监督微调下的泛化能力，重点分析在不同任务设置和分布变化下的表现。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探讨在低资源设置和分布变化下，提示和微调哪种方法更具鲁棒性和泛化能力。

Method: 通过比较不同任务格式、提示风格和模型规模下的提示和微调方法，分析其内部表示和稳定性。

Result: 发现了小模型在不同适应策略下如何内化和泛化知识的关键差异。

Conclusion: 提供了在低数据情况下模型选择的实用指导，并为提示与微调的争论提供了实证依据。

Abstract: We investigate the generalization capabilities of small language models under
two popular adaptation paradigms: few-shot prompting and supervised
fine-tuning. While prompting is often favored for its parameter efficiency and
flexibility, it remains unclear how robust this approach is in low-resource
settings and under distributional shifts. This paper presents a comparative
study of prompting and fine-tuning across task formats, prompt styles, and
model scales, with a focus on their behavior in both in-distribution and
out-of-distribution (OOD) settings.
  Beyond accuracy, we analyze the internal representations learned by each
approach to assess the stability and abstraction of task-specific features. Our
findings highlight critical differences in how small models internalize and
generalize knowledge under different adaptation strategies. This work offers
practical guidance for model selection in low-data regimes and contributes
empirical insight into the ongoing debate over prompting versus fine-tuning.
Code for the experiments is available at the following

</details>


### [234] [Individual Causal Inference with Structural Causal Model](https://arxiv.org/abs/2506.17300)
*Daniel T. Chang*

Key words: 个体因果推断, 结构因果模型, 个体化, indiv-operator, 因果查询

TL;DR: 个体因果推断（ICI）利用因果推断方法预测干预对个体的影响，关注个体特性。提出使用结构因果模型（SCM）实现个体化推断，并引入indiv-operator和个体因果查询形式化此过程。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统因果推断多为群体层面，难以适用于个体差异显著的场景。SCM中的外生变量可编码个体特性，为个体化推断提供理论基础。

Method: 提出indiv(W)运算符和个体因果查询P(Y | indiv(W), do(X), Z)，基于SCM实现个体化因果推断，强调对可能个体而非反事实的推断。

Result: 通过SCM中的外生变量实现个体化推断，形式化为“第三阶梯”因果推断，支持基于个体特征的假设干预效果预测。

Conclusion: ICI结合SCM为个体因果推断提供新框架，适用于个体差异显著的场景，但需注意其推断为可能性而非反事实。

Abstract: Individual causal inference (ICI) uses causal inference methods to understand
and predict the effects of interventions on individuals, considering their
specific characteristics / facts. It aims to estimate individual causal effect
(ICE), which varies across individuals. Estimating ICE can be challenging due
to the limited data available for individuals, and the fact that most causal
inference methods are population-based. Structural Causal Model (SCM) is
fundamentally population-based. Therefore, causal discovery (structural
learning and parameter learning), association queries and intervention queries
are all naturally population-based. However, exogenous variables (U) in SCM can
encode individual variations and thus provide the mechanism for individualized
population per specific individual characteristics / facts. Based on this, we
propose ICI with SCM as a "rung 3" causal inference, because it involves
"imagining" what would be the causal effect of a hypothetical intervention on
an individual, given the individual's observed characteristics / facts.
Specifically, we propose the indiv-operator, indiv(W), to formalize/represent
the population individualization process, and the individual causal query, P(Y
| indiv(W), do(X), Z), to formalize/represent ICI. We show and argue that ICI
with SCM is inference on individual alternatives (possible), not individual
counterfactuals (non-actual).

</details>


### [235] [Resource Rational Contractualism Should Guide AI Alignment](https://arxiv.org/abs/2506.17434)
*Sydney Levine,Matija Franklin,Tan Zhi-Xuan,Secil Yanik Guyot,Lionel Wong,Daniel Kilov,Yejin Choi,Joshua B. Tenenbaum,Noah Goodman,Seth Lazar,Iason Gabriel*

Key words: AI对齐, 契约主义, 资源理性, 启发式, 动态适应

TL;DR: 论文提出资源理性契约主义（RRC），通过启发式方法高效模拟多方理性协议，帮助AI系统动态适应人类社会的多样性和变化。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: AI系统需在多目标、多价值观的人类环境中做出决策，现有契约主义方法因规模问题效率低下。

Method: 采用资源理性契约主义框架，结合规范性基础和认知启发式，以效率换取精度。

Result: RRC框架使AI系统既能高效运行，又能动态适应和理解人类社会的复杂性。

Conclusion: RRC为AI系统在多目标环境中的决策提供了一种高效且适应性强的对齐方法。

Abstract: AI systems will soon have to navigate human environments and make decisions
that affect people and other AI agents whose goals and values diverge.
Contractualist alignment proposes grounding those decisions in agreements that
diverse stakeholders would endorse under the right conditions, yet securing
such agreement at scale remains costly and slow -- even for advanced AI. We
therefore propose Resource-Rational Contractualism (RRC): a framework where AI
systems approximate the agreements rational parties would form by drawing on a
toolbox of normatively-grounded, cognitively-inspired heuristics that trade
effort for accuracy. An RRC-aligned agent would not only operate efficiently,
but also be equipped to dynamically adapt to and interpret the ever-changing
human social world.

</details>


### [236] [Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation](https://arxiv.org/abs/2506.17442)
*Hao Guan,David Bates,Li Zhou*

Key words: 人工智能, 医疗健康, 性能退化, 数据漂移, 模型维护

TL;DR: 这是一篇关于人工智能在医疗领域中性能退化的综述性论文，重点探讨了如何监测和维护AI系统的性能以应对动态临床环境中的挑战。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: AI系统在医疗领域中的实时部署可能会因数据分布变化等因素导致性能退化，影响预测准确性和安全性，亟需解决这些问题。

Method: 综述了性能退化的常见原因、数据与模型漂移的检测技术、根因分析及纠正策略，涵盖从传统模型到大语言模型的技术。

Result: 提出了一套框架和技术方法，强调了持续监测和自纠错机制的必要性，并对现有技术进行了全面总结。

Conclusion: 未来研究应致力于开发更可靠的医疗AI系统，以应对动态临床环境的挑战。

Abstract: Artificial intelligence (AI) is increasingly integrated into modern
healthcare, offering powerful support for clinical decision-making. However, in
real-world settings, AI systems may experience performance degradation over
time, due to factors such as shifting data distributions, changes in patient
characteristics, evolving clinical protocols, and variations in data quality.
These factors can compromise model reliability, posing safety concerns and
increasing the likelihood of inaccurate predictions or adverse outcomes. This
review presents a forward-looking perspective on monitoring and maintaining the
"health" of AI systems in healthcare. We highlight the urgent need for
continuous performance monitoring, early degradation detection, and effective
self-correction mechanisms. The paper begins by reviewing common causes of
performance degradation at both data and model levels. We then summarize key
techniques for detecting data and model drift, followed by an in-depth look at
root cause analysis. Correction strategies are further reviewed, ranging from
model retraining to test-time adaptation. Our survey spans both traditional
machine learning models and state-of-the-art large language models (LLMs),
offering insights into their strengths and limitations. Finally, we discuss
ongoing technical challenges and propose future research directions. This work
aims to guide the development of reliable, robust medical AI systems capable of
sustaining safe, long-term deployment in dynamic clinical settings.

</details>


### [237] [OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections](https://arxiv.org/abs/2506.17449)
*Manasa Bharadwaj,Nikhil Verma,Kevin Ferreira*

Key words: LLM, OmniReflect, 反思驱动, 宪法原则, 任务成功率

TL;DR: OmniReflect是一种层次化、反思驱动的框架，通过构建一套指导原则（宪法）来提升LLM代理的性能，显著提高了任务成功率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有方法缺乏长期学习的通用机制且在动态环境中效率低下，OmniReflect旨在解决这些问题。

Method: 采用自我维持和协作两种模式，结合神经、符号和神经符号技术，构建宪法原则。

Result: 在ALFWorld、BabyAI和PDDL等任务中，任务成功率显著提升，最高达23.8%。

Conclusion: OmniReflect在不同环境和骨干模型中表现稳健有效。

Abstract: Efforts to improve Large Language Model (LLM) agent performance on complex
tasks have largely focused on fine-tuning and iterative self-correction.
However, these approaches often lack generalizable mechanisms for longterm
learning and remain inefficient in dynamic environments. We introduce
OmniReflect, a hierarchical, reflection-driven framework that constructs a
constitution, a compact set of guiding principles distilled from task
experiences, to enhance the effectiveness and efficiency of an LLM agent.
OmniReflect operates in two modes: Self-sustaining, where a single agent
periodically curates its own reflections during task execution, and
Co-operative, where a Meta-advisor derives a constitution from a small
calibration set to guide another agent. To construct these constitutional
principles, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering
a balance between contextual adaptability and computational efficiency.
Empirical results averaged across models show major improvements in task
success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3%
on PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative
mode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion
baselines on BabyAI. These findings highlight the robustness and effectiveness
of OmniReflect across environments and backbones.

</details>


### [238] [From Unstructured Communication to Intelligent RAG: Multi-Agent Automation for Supply Chain Knowledge Bases](https://arxiv.org/abs/2506.17484)
*Yao Zhang,Zaixi Shang,Silpan Patel,Mikel Zuniga*

Key words: 供应链, RAG, 知识管理, LLM, 多Agent系统

TL;DR: 该论文提出了一种基于LLM的多Agent离线方法，将供应链支持工单转化为结构化知识库，显著提升RAG系统性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 供应链运营产生大量非结构化通信数据（如支持工单），传统RAG方法因数据质量问题效果有限，需改进知识管理。

Method: 使用多Agent系统（分类发现、分类整理、知识合成）离线处理工单，生成结构化知识库。

Result: 知识库体积缩小至3.4%，RAG系统回答质量提升（48.74%有用回答），减少77.4%无用响应，自动化解决50%工单。

Conclusion: 离线处理方法显著提升知识重用效率，优化供应链支持工作流。

Abstract: Supply chain operations generate vast amounts of operational data; however,
critical knowledge such as system usage practices, troubleshooting workflows,
and resolution techniques often remains buried within unstructured
communications like support tickets, emails, and chat logs. While RAG systems
aim to leverage such communications as a knowledge base, their effectiveness is
limited by raw data challenges: support tickets are typically noisy,
inconsistent, and incomplete, making direct retrieval suboptimal. Unlike
existing RAG approaches that focus on runtime optimization, we introduce a
novel offline-first methodology that transforms these communications into a
structured knowledge base. Our key innovation is a LLMs-based multi-agent
system orchestrating three specialized agents: Category Discovery for taxonomy
creation, Categorization for ticket grouping, and Knowledge Synthesis for
article generation. Applying our methodology to real-world support tickets with
resolution notes and comments, our system creates a compact knowledge base -
reducing total volume to just 3.4% of original ticket data while improving
quality. Experiments demonstrate that our prebuilt knowledge base in RAG
systems significantly outperforms traditional RAG implementations (48.74% vs.
38.60% helpful answers) and achieves a 77.4% reduction in unhelpful responses.
By automating institutional knowledge capture that typically remains siloed in
experts' heads, our solution translates to substantial operational efficiency:
reducing support workload, accelerating resolution times, and creating
self-improving systems that automatically resolve approximately 50% of future
supply chain tickets. Our approach addresses a key gap in knowledge management
by transforming transient communications into structured, reusable knowledge
through intelligent offline processing rather than latency-inducing runtime
architectures.

</details>


### [239] [Kaleidoscopic Teaming in Multi Agent Simulations](https://arxiv.org/abs/2506.17514)
*Ninareh Mehrabi,Tharindu Kumarage,Kai-Wei Chang,Aram Galstyan,Rahul Gupta*

Key words: AI安全,多智能体系统,红队,kaleidoscopic teaming,上下文优化

TL;DR: 本文提出了“kaleidoscopic teaming”概念和框架，用于评估单智能体和多智能体场景下的安全风险，弥补现有安全性评估方法的不足。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有红队或安全性评估框架无法全面评估智能体在复杂行为、思维和动作中的安全风险，尤其是在多智能体环境中。

Method: 引入“kaleidoscopic teaming”框架，生成多样化场景模拟人类社会，评估单智能体和多智能体的安全性，并提出新的上下文优化技术和度量标准。

Result: 该框架识别了多种模型在智能体应用场景中的安全隐患。

Conclusion: “kaleidoscopic teaming”框架能有效捕捉单智能体和多智能体环境中的复杂安全漏洞。

Abstract: Warning: This paper contains content that may be inappropriate or offensive.
  AI agents have gained significant recent attention due to their autonomous
tool usage capabilities and their integration in various real-world
applications. This autonomy poses novel challenges for the safety of such
systems, both in single- and multi-agent scenarios. We argue that existing red
teaming or safety evaluation frameworks fall short in evaluating safety risks
in complex behaviors, thought processes and actions taken by agents. Moreover,
they fail to consider risks in multi-agent setups where various vulnerabilities
can be exposed when agents engage in complex behaviors and interactions with
each other. To address this shortcoming, we introduce the term kaleidoscopic
teaming which seeks to capture complex and wide range of vulnerabilities that
can happen in agents both in single-agent and multi-agent scenarios. We also
present a new kaleidoscopic teaming framework that generates a diverse array of
scenarios modeling real-world human societies. Our framework evaluates safety
of agents in both single-agent and multi-agent setups. In single-agent setup,
an agent is given a scenario that it needs to complete using the tools it has
access to. In multi-agent setup, multiple agents either compete against or
cooperate together to complete a task in the scenario through which we capture
existing safety vulnerabilities in agents. We introduce new in-context
optimization techniques that can be used in our kaleidoscopic teaming framework
to generate better scenarios for safety analysis. Lastly, we present
appropriate metrics that can be used along with our framework to measure safety
of agents. Utilizing our kaleidoscopic teaming framework, we identify
vulnerabilities in various models with respect to their safety in agentic
use-cases.

</details>


### [240] [Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models](https://arxiv.org/abs/2506.17585)
*Yukun Huang,Sanxing Chen,Jian Pei,Manzil Zaheer,Bhuwan Dhingra*

Key words: 语言模型, 预训练, 引用可靠性, 主动索引, 被动索引

TL;DR: 这篇论文探讨了如何通过改进训练过程，使语言模型能够可靠地引用其预训练中的文档，而不需要依赖推理时的外部检索器。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前的语言模型在引用预训练数据时经常不可靠（幻觉问题），而依赖外部检索器会带来延迟、基础设施依赖和检索噪声等缺点。

Method: 提出了一个两阶段方法：1) 持续预训练以将事实与文档标识符绑定；2) 指令微调以引导引用行为。主动索引法通过合成QA对增强训练效果。

Result: 主动索引法在各种任务和模型上均优于被动索引法，引用精度提升高达30.2%。实验显示性能随增强数据量的增加而持续提升。

Conclusion: 主动索引是一种有效的方法，能够显著提高模型的引用可靠性。

Abstract: Trustworthy language models should provide both correct and verifiable
answers. While language models can sometimes attribute their outputs to
pretraining data, their citations are often unreliable due to hallucination. As
a result, current systems insert citations by querying an external retriever at
inference time, introducing latency, infrastructure dependence, and
vulnerability to retrieval noise. We explore whether LLMs can be made to
reliably attribute to the documents seen during (continual)
pretraining--without test-time retrieval--by revising the training process. To
evaluate this, we release CitePretrainBench, a benchmark that mixes real-world
corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and
probes both short-form (single fact) and long-form (multi-fact) citation tasks.
Our approach follows a two-stage process: (1) continual pretraining to bind
facts to persistent document identifiers, and (2) instruction tuning to elicit
citation behavior. We find that simple Passive Indexing, which appends an
identifier to each document, helps memorize verbatim text but fails on
paraphrased or compositional facts. Instead, we propose Active Indexing, which
continually pretrains on synthetic QA pairs that (1) restate each fact in
diverse compositional forms, and (2) require bidirectional source-to-fact and
fact-to-source generation, jointly teaching the model to generate content from
a cited source and to attribute its own answers. Experiments with Qwen2.5-7B
and 3B show that Active Indexing consistently outperforms Passive Indexing
across all tasks and models, with citation precision gains up to 30.2 percent.
Our ablation studies reveal that performance continues to improve as we scale
the amount of augmented data, showing a clear upward trend even at 16 times the
original token count.

</details>


### [241] [Taming the Untamed: Graph-Based Knowledge Retrieval and Reasoning for MLLMs to Conquer the Unknown](https://arxiv.org/abs/2506.17589)
*Bowen Wang*

Key words: 多模态大语言模型, 知识图谱, 知识检索, 推理, 多智能体

TL;DR: 本文探讨了如何通过构建多模态知识图谱（MH-MMKG）并结合多智能体检索器，提升多模态大语言模型（MLLMs）在特定领域任务中的知识检索与推理能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 尽管多模态大语言模型（MLLMs）在多模态任务中表现出色，但在特定领域任务（如视觉游戏认知）中表现不佳，主要是因为缺乏相关领域知识。

Method: 通过构建一个多模态知识图谱（MH-MMKG）并设计一系列挑战性查询，结合多智能体检索器，实现无需额外训练的自主知识检索与推理。

Result: 实验结果表明，所提出的方法显著提升了MLLMs在复杂知识检索与推理任务中的表现。

Conclusion: 该方法为多模态知识增强推理提供了新视角，并为未来研究奠定了基础。

Abstract: The real value of knowledge lies not just in its accumulation, but in its
potential to be harnessed effectively to conquer the unknown. Although recent
multimodal large language models (MLLMs) exhibit impressing multimodal
capabilities, they often fail in rarely encountered domain-specific tasks due
to limited relevant knowledge. To explore this, we adopt visual game cognition
as a testbed and select Monster Hunter: World as the target to construct a
multimodal knowledge graph (MH-MMKG), which incorporates multi-modalities and
intricate entity relations. We also design a series of challenging queries
based on MH-MMKG to evaluate the models' ability for complex knowledge
retrieval and reasoning. Furthermore, we propose a multi-agent retriever that
enables a model to autonomously search relevant knowledge without additional
training. Experimental results show that our approach significantly enhances
the performance of MLLMs, providing a new perspective on multimodal
knowledge-augmented reasoning and laying a solid foundation for future
research.

</details>


### [242] [Measuring and Augmenting Large Language Models for Solving Capture-the-Flag Challenges](https://arxiv.org/abs/2506.17644)
*Zimo Ji,Daoyuan Wu,Wenyuan Jiang,Pingchuan Ma,Zongjie Li,Shuai Wang*

Key words: CTF, 大型语言模型, 技术知识, 基准测试, 自动化

TL;DR: 本文提出了一种专注于衡量大型语言模型（LLMs）在CTF比赛中技术知识应用的基准CTFKnow，并开发了CTFAgent框架，显著提升了LLMs在CTF问题解决中的表现。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着LLMs的发展，对其在CTF比赛中自动化解决问题能力的兴趣增加，但现有研究缺乏对其技术知识应用的精准测量。

Method: 构建CTFKnow基准（3,992个问题）评估LLMs，并提出CTFAgent框架，包含两阶段RAG和环境交互增强模块。

Result: CTFAgent在实验中表现优异，性能提升超80%，并在picoCTF2024比赛中排名前23.6%。

Conclusion: CTFAgent展示了LLMs在CTF问题解决中的潜力，未来可通过进一步研究优化其能力。

Abstract: Capture-the-Flag (CTF) competitions are crucial for cybersecurity education
and training. As large language models (LLMs) evolve, there is increasing
interest in their ability to automate CTF challenge solving. For example, DARPA
has organized the AIxCC competition since 2023 to advance AI-powered automated
offense and defense. However, this demands a combination of multiple abilities,
from knowledge to reasoning and further to actions. In this paper, we highlight
the importance of technical knowledge in solving CTF problems and deliberately
construct a focused benchmark, CTFKnow, with 3,992 questions to measure LLMs'
performance in this core aspect. Our study offers a focused and innovative
measurement of LLMs' capability in understanding CTF knowledge and applying it
to solve CTF challenges. Our key findings reveal that while LLMs possess
substantial technical knowledge, they falter in accurately applying this
knowledge to specific scenarios and adapting their strategies based on feedback
from the CTF environment.
  Based on insights derived from this measurement study, we propose CTFAgent, a
novel LLM-driven framework for advancing CTF problem-solving. CTFAgent
introduces two new modules: two-stage Retrieval Augmented Generation (RAG) and
interactive Environmental Augmentation, which enhance LLMs' technical knowledge
and vulnerability exploitation on CTF, respectively. Our experimental results
show that, on two popular CTF datasets, CTFAgent both achieves over 80%
performance improvement. Moreover, in the recent picoCTF2024 hosted by CMU,
CTFAgent ranked in the top 23.6% of nearly 7,000 participating teams. This
reflects the benefit of our measurement study and the potential of our
framework in advancing LLMs' capabilities in CTF problem-solving.

</details>


### [243] [PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models](https://arxiv.org/abs/2506.17667)
*Lintao Wang,Encheng Su,Jiaqi Liu,Pengze Li,Peng Xia,Jiabei Xiao,Wenlong Zhang,Xinnan Dai,Xi Chen,Yuan Meng,Mingyu Ding,Lei Bai,Wanli Ouyang,Shixiang Tang,Aoran Wang,Xinzhu Ma*

Key words: 物理推理, 多模态大语言模型, 基准测试, 本科物理, AI for Science

TL;DR: PhysUniBench是一个多模态基准测试，旨在评估和改进多模态大语言模型在本科物理问题上的推理能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有评估方法在捕捉本科物理的广度和复杂性方面存在局限性，需要更严格的评估工具。

Method: 通过多阶段流程构建包含3,304道物理问题的基准测试，涵盖8个物理子领域，并结合视觉图表和多选题/开放式问题。

Result: 当前最先进的模型（如GPT-4o mini）在PhysUniBench上的准确率仅为34.2%，显示出在高级物理推理和多模态理解上的不足。

Conclusion: PhysUniBench旨在推动AI在科学领域的发展，促进具有更强物理推理能力和多模态理解的模型开发。

Abstract: Physics problem-solving is a challenging domain for large AI models,
requiring integration of conceptual understanding, mathematical reasoning, and
interpretation of physical diagrams. Current evaluation methodologies show
notable limitations in capturing the breadth and complexity of
undergraduate-level physics, underscoring the need for more rigorous
assessments. To this end, we present PhysUniBench, a large-scale multimodal
benchmark designed to evaluate and improve the reasoning capabilities of
multimodal large language models (MLLMs) specifically on undergraduate-level
physics problems. PhysUniBench consists of 3,304 physics questions spanning 8
major sub-disciplines of physics, each accompanied by one visual diagrams. The
benchmark includes both open-ended and multiple-choice questions,
systematically curated and difficulty-rated through an iterative
model-in-the-loop process. The benchmark's construction involved a rigorous
multi-stage process, including multiple roll-outs, expert-level evaluation,
automated filtering of easily solved problems, and a nuanced difficulty grading
system with five levels. Through extensive experiments, we observe that current
state-of-the-art models encounter substantial challenges in physics reasoning.
For example, GPT-4o mini achieves only about 34.2\% accuracy in the proposed
PhysUniBench. These results highlight that current MLLMs struggle with advanced
physics reasoning, especially on multi-step problems and those requiring
precise diagram interpretation. By providing a broad and rigorous assessment
tool, PhysUniBench aims to drive progress in AI for Science, encouraging the
development of models with stronger physical reasoning, problem-solving skills,
and multimodal understanding. The benchmark and evaluation scripts are
available at https://prismax-team.github.io/PhysUniBenchmark/.

</details>


### [244] [Beyond Syntax: Action Semantics Learning for App Agents](https://arxiv.org/abs/2506.17697)
*Bohan Tang,Dezhao Luo,Jingxuan Chen,Shaogang Gong,Jianye Hao,Jun Wang,Kun Shao*

Key words: 大语言模型,App智能体,动作语义学习,分布外问题

TL;DR: 提出了动作语义学习（ASL），通过关注动作的语义而非语法形式，提升App智能体的准确性和泛化能力，解决了现有方法的分布外（OOD）脆弱性问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有基于提示的大型语言模型（LLM）方法计算成本高且依赖外部API，而微调小型开源LLM的方法因强制学习语法形式而容易受到OOD问题影响。

Method: 设计动作语义学习（ASL）框架，引入语义估算器（SEE）计算语义奖励，训练智能体生成与真实动作语义一致的动作。

Result: ASL在离线和在线智能手机App操作基准测试中显著提升了智能体的准确性和泛化能力。

Conclusion: ASL通过语义学习解决了OOD问题，优于现有语法学习方法。

Abstract: The advent of Large Language Models (LLMs) enables the rise of App agents
that interpret user intent and operate smartphone Apps through actions such as
clicking and scrolling. While prompt-based solutions with closed LLM APIs show
promising ability, they incur heavy compute costs and external API dependency.
Fine-tuning smaller open-source LLMs solves these limitations. However, current
fine-tuning methods use a syntax learning paradigm that forces agents to
reproduce exactly the ground truth action strings, leading to
out-of-distribution (OOD) vulnerability. To fill this gap, we propose Action
Semantics Learning (ASL), a novel learning framework, where the learning
objective is capturing the semantics of the ground truth actions. Specifically,
inspired by the programming language theory, we define the action semantics for
App agents as the state transition induced by the action in the user interface.
With this insight, ASL employs a novel SEmantic Estimator (SEE) to compute a
semantic reward to train the App agents in generating actions aligned with the
semantics of ground truth actions, even when the syntactic forms differ. To
support the effectiveness of ASL, we theoretically demonstrate the superior
robustness of ASL for the OOD problem compared with the existing syntax
learning paradigm. Extensive experiments on offline and online smartphone App
operation benchmarks show that ASL significantly improves the accuracy and
generalisation of App agents over existing methods.

</details>


### [245] [AnyMAC: Cascading Flexible Multi-Agent Collaboration via Next-Agent Prediction](https://arxiv.org/abs/2506.17784)
*Song Wang,Zhen Tan,Zihan Chen,Shuang Zhou,Tianlong Chen,Jundong Li*

Key words: 大语言模型, 多智能体协作, 顺序结构, Next-Agent Prediction, Next-Context Selection

TL;DR: 提出了一种基于顺序结构的多智能体协作框架，取代传统静态或图结构拓扑，提高通信灵活性和适应性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有方法依赖静态或图结构拓扑，缺乏通信的适应性和灵活性。

Method: 通过Next-Agent Prediction选择合适角色，结合Next-Context Selection选择性访问历史信息，构建任务自适应通信管道。

Result: 在多个基准测试中表现出优越性能，同时显著降低通信开销。

Conclusion: 顺序结构框架在提升多智能体协作效率和灵活性方面具有潜力。

Abstract: Recent progress in large language model (LLM)-based multi-agent collaboration
highlights the power of structured communication in enabling collective
intelligence. However, existing methods largely rely on static or graph-based
inter-agent topologies, lacking the potential adaptability and flexibility in
communication. In this work, we propose a new framework that rethinks
multi-agent coordination through a sequential structure rather than a graph
structure, offering a significantly larger topology space for multi-agent
communication. Our method focuses on two key directions: (1) Next-Agent
Prediction, which selects the most suitable agent role at each step, and (2)
Next-Context Selection (NCS), which enables each agent to selectively access
relevant information from any previous step. Together, these components
construct task-adaptive communication pipelines that support both role
flexibility and global information flow. Extensive evaluations across multiple
benchmarks demonstrate that our approach achieves superior performance while
substantially reducing communication overhead.

</details>


### [246] [Bayesian Social Deduction with Graph-Informed Language Models](https://arxiv.org/abs/2506.17788)
*Shahab Rahimirad,Guven Gergerli,Lucia Romero,Angela Qian,Matthew Lyle Olson,Simon Stepputtis,Joseph Campbell*

Key words: 社交推理, 大型语言模型, Avalon游戏, 混合推理框架

TL;DR: 论文分析了大型语言模型在社交推理中的局限性，并提出了一种混合推理框架，结合了结构化概率模型和LLM，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 社交推理（如从部分观察中推断其他代理的信念和意图）对大型语言模型（LLMs）仍具挑战性，因此需要改进。

Method: 提出了一种混合推理框架，将信念推断外部化为结构化概率模型，同时利用LLM进行语言理解和交互。

Result: 在Avalon游戏中，该方法性能接近更大模型，并在人类玩家对抗中取得67%的胜率，评分高于基准和人类队友。

Conclusion: 混合推理框架有效弥补了LLMs的社交推理短板，展现了与人类竞争的潜力。

Abstract: Social reasoning - inferring unobservable beliefs and intentions from partial
observations of other agents - remains a challenging task for large language
models (LLMs). We evaluate the limits of current reasoning language models in
the social deduction game Avalon and find that while the largest models
demonstrate strong performance, they require extensive test-time inference and
degrade sharply when distilled to smaller, real-time-capable variants. To
address this, we introduce a hybrid reasoning framework that externalizes
belief inference to a structured probabilistic model, while using an LLM for
language understanding and interaction. Our approach achieves competitive
performance with much larger models in Agent-Agent play and, notably, is the
first language agent to defeat human players in a controlled study - achieving
a 67% win rate and receiving higher qualitative ratings than both reasoning
baselines and human teammates. We release code, models, and a dataset to
support future work on social reasoning in LLM agents, which can be found at
https://camp-lab-purdue.github.io/bayesian-social-deduction/

</details>


### [247] [Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition](https://arxiv.org/abs/2506.17792)
*Alexandros Evangelidis,Gricel Vázquez,Simos Gerasimou*

Key words: 马尔可夫决策过程, 策略合成, 动态细化, 性能优化

TL;DR: 该论文提出了一种动态马尔可夫决策过程（MDP）改进方法，通过迭代优化脆弱区域来加速大规模MDP的策略合成，显著提升效率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统策略合成方法在面对大规模状态空间时效率不足，亟需一种能够平衡准确性与效率的新方法。

Method: 动态细化MDP，并迭代选择最脆弱的区域进行优化，仅在必要时进行改进。

Result: 在包含多达1M状态的多样化案例研究中，该方法比PRISM模型检查器的性能提升高达2倍。

Conclusion: 该方法为大规模MDP中的实际策略合成任务提供了高效且具竞争力的解决方案。

Abstract: Software-intensive systems, such as software product lines and robotics,
utilise Markov decision processes (MDPs) to capture uncertainty and analyse
sequential decision-making problems. Despite the usefulness of conventional
policy synthesis methods, they fail to scale to large state spaces. Our
approach addresses this issue and accelerates policy synthesis in large MDPs by
dynamically refining the MDP and iteratively selecting the most fragile MDP
regions for refinement. This iterative procedure offers a balance between
accuracy and efficiency, as refinement occurs only when necessary. Through a
comprehensive empirical evaluation comprising diverse case studies and MDPs up
to 1M states, we demonstrate significant performance improvements yielded by
our approach compared to the leading probabilistic model checker PRISM (up to
2x), thus offering a very competitive solution for real-world policy synthesis
tasks in larger MDPs.

</details>


### [248] [Reflective Verbal Reward Design for Pluralistic Alignment](https://arxiv.org/abs/2506.17834)
*Carter Blair,Kate Larson,Edith Law*

Key words: AI alignment, reward modeling, human values, reinforcement learning, language models

TL;DR: 论文提出了一种新的奖励模型方法，通过学习个体化的奖励模型来解决人类价值观异质性问题，避免了单一奖励模型对少数偏好的压制。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 人类价值观是多样且有时冲突的，传统的单一奖励模型可能会压制少数群体的偏好，因此需要一种个体化的奖励建模方法。

Method: 使用语言模型引导用户通过反思对话构建偏好，并将对话历史用作个体化奖励函数（称为“语言奖励模型”）的上下文。

Result: 在30名参与者的实验中，该方法比非反思性语言奖励模型准确率提高9-12%，且比传统监督学习方法更高效。

Conclusion: 个体化的奖励模型能更好地捕捉多样的人类价值观，同时在准确性和样本效率上优于现有方法。

Abstract: AI agents are commonly aligned with "human values" through reinforcement
learning from human feedback (RLHF), where a single reward model is learned
from aggregated human feedback and used to align an agent's behavior. However,
human values are not homogeneous--different people hold distinct and sometimes
conflicting values. Aggregating feedback into a single reward model risks
disproportionately suppressing minority preferences. To address this, we
present a novel reward modeling approach for learning individualized reward
models. Our approach uses a language model to guide users through reflective
dialogues where they critique agent behavior and construct their preferences.
This personalized dialogue history, containing the user's reflections and
critiqued examples, is then used as context for another language model that
serves as an individualized reward function (what we call a "verbal reward
model") for evaluating new trajectories. In studies with 30 participants, our
method achieved a 9-12% improvement in accuracy over non-reflective verbal
reward models while being more sample efficient than traditional supervised
learning methods.

</details>


### [249] [Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)](https://arxiv.org/abs/2506.17846)
*Elija Perrier*

Key words: 形式最优控制, AI对齐, 分层控制堆栈, 互操作性, 安全可靠

TL;DR: 本文主张将形式最优控制理论作为AI对齐研究的核心，提出了一种不同于现有AI安全和安全方法的新视角。通过分层对齐控制堆栈，研究各层的测量与控制特性及其互操作性，为前沿模型和自主AI系统的控制提供更全面的框架。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有AI安全研究缺乏通用控制框架和协议互操作性的探索，希望通过形式最优控制理论提升对AI系统的理解与控制能力。

Method: 提出Alignment Control Stack，分层分析从物理到社会技术层的控制特性及其互操作性，结合最优控制方法与实践部署需求。

Result: 构建了一个分层对齐框架，增强了前沿模型和自主AI系统的安全性与可靠性。

Conclusion: 形式最优控制理论能够为AI对齐提供更全面和可操作的方法，有助于政府与监管机构对AI技术的信任与管理。

Abstract: This position paper argues that formal optimal control theory should be
central to AI alignment research, offering a distinct perspective from
prevailing AI safety and security approaches. While recent work in AI safety
and mechanistic interpretability has advanced formal methods for alignment,
they often fall short of the generalisation required of control frameworks for
other technologies. There is also a lack of research into how to render
different alignment/control protocols interoperable. We argue that by recasting
alignment through principles of formal optimal control and framing alignment in
terms of hierarchical stack from physical to socio-technical layers according
to which controls may be applied we can develop a better understanding of the
potential and limitations for controlling frontier models and agentic AI
systems. To this end, we introduce an Alignment Control Stack which sets out a
hierarchical layered alignment stack, identifying measurement and control
characteristics at each layer and how different layers are formally
interoperable. We argue that such analysis is also key to the assurances that
will be needed by governments and regulators in order to see AI technologies
sustainably benefit the community. Our position is that doing so will bridge
the well-established and empirically validated methods of optimal control with
practical deployment considerations to create a more comprehensive alignment
framework, enhancing how we approach safety and reliability for advanced AI
systems.

</details>


### [250] [Towards Robust Fact-Checking: A Multi-Agent System with Advanced Evidence Retrieval](https://arxiv.org/abs/2506.17878)
*Tam Trinh,Manh Nguyen,Truong-Son Hy*

Key words: 自动化事实核查,多智能体系统,大型语言模型,可解释性,可信证据

TL;DR: 本文提出了一种新颖的多智能体系统，用于自动化事实核查，提高了准确性、效率和可解释性，相比基线方法，Macro F1分数提高了12.3%。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 数字时代错误信息快速传播对公共讨论构成挑战，传统人工事实核查方法难以应对在线内容的规模和速度，而现有自动化方法在复杂声明处理、来源可信度和透明度方面存在局限。

Method: 系统包含四个专门智能体：输入摄取智能体、查询生成智能体、证据检索智能体和裁决预测智能体，分别负责声明分解、子查询生成、可信证据检索和可解释的真相判断。

Result: 在多个基准数据集上，系统的Macro F1分数比基线方法提高了12.3%，能有效分解复杂声明、检索可信证据并生成透明解释。

Conclusion: 该方法为自动化事实核查领域提供了更准确、高效且透明的验证方法，兼具可扩展性和实用性。

Abstract: The rapid spread of misinformation in the digital era poses significant
challenges to public discourse, necessitating robust and scalable fact-checking
solutions. Traditional human-led fact-checking methods, while credible,
struggle with the volume and velocity of online content, prompting the
integration of automated systems powered by Large Language Models (LLMs).
However, existing automated approaches often face limitations, such as handling
complex claims, ensuring source credibility, and maintaining transparency. This
paper proposes a novel multi-agent system for automated fact-checking that
enhances accuracy, efficiency, and explainability. The system comprises four
specialized agents: an Input Ingestion Agent for claim decomposition, a Query
Generation Agent for formulating targeted subqueries, an Evidence Retrieval
Agent for sourcing credible evidence, and a Verdict Prediction Agent for
synthesizing veracity judgments with human-interpretable explanations.
Evaluated on benchmark datasets (FEVEROUS, HOVER, SciFact), the proposed system
achieves a 12.3% improvement in Macro F1-score over baseline methods. The
system effectively decomposes complex claims, retrieves reliable evidence from
trusted sources, and generates transparent explanations for verification
decisions. Our approach contributes to the growing field of automated
fact-checking by providing a more accurate, efficient, and transparent
verification methodology that aligns with human fact-checking practices while
maintaining scalability for real-world applications. Our source code is
available at https://github.com/HySonLab/FactAgent

</details>


### [251] [Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms](https://arxiv.org/abs/2506.17900)
*Cheng Ji,Huaiying Luo*

Key words: 大语言模型, 日志处理, 自动调试, 云平台, 故障定位

TL;DR: 本文提出了一种基于大语言模型的智能日志处理和自动调试框架（LLM-ID），通过多阶段语义推理机制提升云平台故障定位效率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 云平台AI系统日志数据量大、结构复杂且语义模糊，传统方法难以有效定位故障和实现自我修复。

Method: 结合预训练Transformer模型和多阶段语义推理机制，动态结构化日志、提取事件模板和语义模式，并通过LLM进行上下文推理生成故障假设和根本原因路径，最后基于强化学习制定修复策略。

Result: 在云平台日志数据集上，LLM-ID的故障定位准确率比主流方法提高16.2%。

Conclusion: LLM-ID框架在语义理解、持续学习和异构环境适应性方面表现优异，显著优于传统方法。

Abstract: With the increasing complexity and rapid expansion of the scale of AI systems
in cloud platforms, the log data generated during system operation is massive,
unstructured, and semantically ambiguous, which brings great challenges to
fault location and system self-repair. In order to solve this problem, this
paper proposes an intelligent log processing and automatic debugging framework
based on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This
method is extended on the basis of the existing pre-trained Transformer model,
and integrates a multi-stage semantic inference mechanism to realize the
context understanding of system logs and the automatic reconstruction of fault
chains. Firstly, the system log is dynamically structured, and the unsupervised
clustering and embedding mechanism is used to extract the event template and
semantic schema. Subsequently, the fine-tuned LLM combined with the multi-round
attention mechanism to perform contextual reasoning on the log sequence to
generate potential fault assumptions and root cause paths. Furthermore, this
paper introduces a reinforcement learning-based policy-guided recovery planner,
which is driven by the remediation strategy generated by LLM to support dynamic
decision-making and adaptive debugging in the cloud environment. Compared with
the existing rule engine or traditional log analysis system, the proposed model
has stronger semantic understanding ability, continuous learning ability and
heterogeneous environment adaptability. Experiments on the cloud platform log
dataset show that LLM-ID improves the fault location accuracy by 16.2%, which
is significantly better than the current mainstream methods

</details>


### [252] [Learning, Reasoning, Refinement: A Framework for Kahneman's Dual-System Intelligence in GUI Agents](https://arxiv.org/abs/2506.17913)
*Jinjie Wei,Jiyao Liu,Lihao Liu,Ming Hu,Junzhi Ning,Mingcheng Li,Weijie Yin,Junjun He,Xiao Liang,Chao Feng,Dingkang Yang*

Key words: GUI代理, 自适应学习, 双系统设计, GRPO, ScreenSeek

TL;DR: CogniGUI是一个基于认知框架的GUI代理，通过模仿人类行为实现自适应学习，解决了现有系统依赖试错决策和简单评估指标的问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有GUI代理系统依赖试错决策且评估指标过于简单，无法真实反映复杂的GUI交互环境。

Method: 结合双系统设计：(1) 快速视觉语义分析的解析引擎，(2) 基于相对奖励的GRPO代理。引入ScreenSeek基准测试。

Result: CogniGUI在现有和新基准测试中均超越最先进方法。

Conclusion: CogniGUI通过自适应学习和全面基准测试，显著提升了GUI代理的性能和适应性。

Abstract: Graphical User Interface (GUI) agents have made significant progress in
automating digital tasks through the utilization of computer vision and
language models. Nevertheless, existing agent systems encounter notable
limitations. Firstly, they predominantly depend on trial and error decision
making rather than progressive reasoning, thereby lacking the capability to
learn and adapt from interactive encounters. Secondly, these systems are
assessed using overly simplistic single step accuracy metrics, which do not
adequately reflect the intricate nature of real world GUI interactions. In this
paper, we present CogniGUI, a cognitive framework developed to overcome these
limitations by enabling adaptive learning for GUI automation resembling
human-like behavior. Inspired by Kahneman's Dual Process Theory, our approach
combines two main components: (1) an omni parser engine that conducts immediate
hierarchical parsing of GUI elements through quick visual semantic analysis to
identify actionable components, and (2) a Group based Relative Policy
Optimization (GRPO) grounding agent that assesses multiple interaction paths
using a unique relative reward system, promoting minimal and efficient
operational routes. This dual-system design facilitates iterative ''exploration
learning mastery'' cycles, enabling the agent to enhance its strategies over
time based on accumulated experience. Moreover, to assess the generalization
and adaptability of agent systems, we introduce ScreenSeek, a comprehensive
benchmark that includes multi application navigation, dynamic state
transitions, and cross interface coherence, which are often overlooked
challenges in current benchmarks. Experimental results demonstrate that
CogniGUI surpasses state-of-the-art methods in both the current GUI grounding
benchmarks and our newly proposed benchmark.

</details>


### [253] [Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective](https://arxiv.org/abs/2506.17930)
*Jianyu Wang,Zhiqiang Hu,Lidong Bing*

Key words: 提示设计,大语言模型,上下文学习,PromptQuine

TL;DR: 提出了一种新提示设计范式，挑战大语言模型常规提示方法，通过剪裁随机示例为看似无意义的‘乱码’提升性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 挑战传统提示设计，探索更高效的提示优化方法。

Method: 提出PromptQuine自发现框架，进化搜索剪裁策略，利用低数据自动优化提示。

Result: 在分类、问答、生成和数学推理任务中表现优越，超越现有方法。

Conclusion: 该研究为上下文学习机制研究提供新方向，呼吁开发更多开放式搜索算法。

Abstract: We propose a novel prompt design paradigm that challenges conventional wisdom
in large language model (LLM) prompting. While conventional wisdom prioritizes
well-crafted instructions and demonstrations for in-context learning (ICL), we
show that pruning random demonstrations into seemingly incoherent "gibberish"
can remarkably improve performance across diverse tasks. Notably, the
"gibberish" always matches or surpasses state-of-the-art automatic prompt
optimization techniques, achieving substantial gains regardless of LLM
alignment. Nevertheless, discovering an effective pruning strategy is
non-trivial, as existing attribution methods and prompt compression algorithms
fail to deliver robust results, let alone human intuition. In terms of this, we
propose a self-discover prompt optimization framework, PromptQuine, an
evolutionary search framework that automatically searches for the pruning
strategy by itself using only low-data regimes. Much like the emergent
complexity in nature--such as symbiosis and self-organization--arising in
response to resource constraints, our framework evolves and refines
unconventional yet highly effective prompts by leveraging only the tokens
present within the context. We demonstrate its effectiveness across
classification, multi-choice question answering, generation and math reasoning
tasks across LLMs, while achieving decent runtime efficiency. We hope our
findings can guide mechanistic studies on in-context learning, and provide a
call to action, to pave the way for more open-ended search algorithms for more
effective LLM prompting.

</details>


### [254] [medicX-KG: A Knowledge Graph for Pharmacists' Drug Information Needs](https://arxiv.org/abs/2506.17959)
*Lizzy Farrugia,Lilian M. Azzopardi,Jeremy Debattista,Charlie Abela*

Key words: 知识图谱, 药物信息, 药剂师, 数据整合, 语义技术

TL;DR: 本文介绍了medicX-KG，一个面向药剂师的知识图谱，用于整合药物数据，支持临床和监管决策，弥补了分散数据源的不足。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 药剂师角色正向多学科医疗团队中的综合药学服务转变，需要准确、最新的药物信息支持。

Method: 通过人工智能和语义技术，整合英国国家处方集、DrugBank和马耳他药品管理局的数据，构建知识图谱。

Result: medicX-KG有效支持药物可用性、相互作用、不良反应和治疗类别的查询。

Conclusion: 该知识图谱填补了统一国家药物库的空白，未来需改进剂量编码和实时更新。

Abstract: The role of pharmacists is evolving from medicine dispensing to delivering
comprehensive pharmaceutical services within multidisciplinary healthcare
teams. Central to this shift is access to accurate, up-to-date medicinal
product information supported by robust data integration. Leveraging artificial
intelligence and semantic technologies, Knowledge Graphs (KGs) uncover hidden
relationships and enable data-driven decision-making. This paper presents
medicX-KG, a pharmacist-oriented knowledge graph supporting clinical and
regulatory decisions. It forms the semantic layer of the broader medicX
platform, powering predictive and explainable pharmacy services. medicX-KG
integrates data from three sources, including, the British National Formulary
(BNF), DrugBank, and the Malta Medicines Authority (MMA) that addresses Malta's
regulatory landscape and combines European Medicines Agency alignment with
partial UK supply dependence. The KG tackles the absence of a unified national
drug repository, reducing pharmacists' reliance on fragmented sources. Its
design was informed by interviews with practicing pharmacists to ensure
real-world applicability. We detail the KG's construction, including data
extraction, ontology design, and semantic mapping. Evaluation demonstrates that
medicX-KG effectively supports queries about drug availability, interactions,
adverse reactions, and therapeutic classes. Limitations, including missing
detailed dosage encoding and real-time updates, are discussed alongside
directions for future enhancements.

</details>


### [255] [Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities](https://arxiv.org/abs/2506.18019)
*Yuanchen Bei,Weizhi Zhang,Siwen Wang,Weizhi Chen,Sheng Zhou,Hao Chen,Yong Li,Jiajun Bu,Shirui Pan,Yizhou Yu,Irwin King,Fakhri Karray,Philip S. Yu*

Key words: AI代理、图技术、数据结构化、协作能力、未来研究

TL;DR: 本文综述了图技术如何赋能AI代理，探讨了图技术与代理核心功能的结合，展示了应用案例，并展望了未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着AI代理能力的提升，处理复杂任务需要更高效的数据结构化和协作能力，图技术因其数据组织优势成为关键。

Method: 通过系统综述图技术与AI代理的结合，分析核心功能、应用案例及未来研究方向。

Result: 图技术可显著提升AI代理的数据结构化、理解和协作能力，为下一代代理开发提供了新思路。

Conclusion: 图技术是支持高级AI代理能力的重要范式，未来研究可进一步探索其潜力。

Abstract: AI agents have experienced a paradigm shift, from early dominance by
reinforcement learning (RL) to the rise of agents powered by large language
models (LLMs), and now further advancing towards a synergistic fusion of RL and
LLM capabilities. This progression has endowed AI agents with increasingly
strong abilities. Despite these advances, to accomplish complex real-world
tasks, agents are required to plan and execute effectively, maintain reliable
memory, and coordinate smoothly with other agents. Achieving these capabilities
involves contending with ever-present intricate information, operations, and
interactions. In light of this challenge, data structurization can play a
promising role by transforming intricate and disorganized data into
well-structured forms that agents can more effectively understand and process.
In this context, graphs, with their natural advantage in organizing, managing,
and harnessing intricate data relationships, present a powerful data paradigm
for structurization to support the capabilities demanded by advanced AI agents.
To this end, this survey presents a first systematic review of how graphs can
empower AI agents. Specifically, we explore the integration of graph techniques
with core agent functionalities, highlight notable applications, and identify
prospective avenues for future research. By comprehensively surveying this
burgeoning intersection, we hope to inspire the development of next-generation
AI agents equipped to tackle increasingly sophisticated challenges with graphs.
Related resources are collected and continuously updated for the community in
the Github link.

</details>


### [256] [Action Language BC+](https://arxiv.org/abs/2506.18044)
*Joseph Babb,Joohyung Lee*

Key words: 动作语言, 回答集编程, BC+, 稳定模型语义, 知识表示

TL;DR: 本文提出了一种新的动作语言BC+，弥合了传统动作语言与现代回答集编程（ASP）语言之间的差距，并实现了高效计算。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统动作语言的功能有限，无法充分利用现代ASP语言的先进特性（如选择规则、聚合和抽象约束原子）。

Method: 通过基于命题公式的一般稳定模型语义定义BC+的语义，将现代ASP语言构造视为命题公式的简写形式。

Result: BC+能够涵盖其他动作语言（如B、C、C+和BC）的最佳特性，并可通过ASP求解器高效实现。

Conclusion: BC+是一种表达能力强且计算高效的动作语言，扩展了cplus2asp系统以支持其实现。

Abstract: Action languages are formal models of parts of natural language that are
designed to describe effects of actions. Many of these languages can be viewed
as high level notations of answer set programs structured to represent
transition systems. However, the form of answer set programs considered in the
earlier work is quite limited in comparison with the modern Answer Set
Programming (ASP) language, which allows several useful constructs for
knowledge representation, such as choice rules, aggregates, and abstract
constraint atoms. We propose a new action language called BC+, which closes the
gap between action languages and the modern ASP language. The main idea is to
define the semantics of BC+ in terms of general stable model semantics for
propositional formulas, under which many modern ASP language constructs can be
identified with shorthands for propositional formulas. Language BC+ turns out
to be sufficiently expressive to encompass the best features of other action
languages, such as languages B, C, C+, and BC. Computational methods available
in ASP solvers are readily applicable to compute BC+, which led to an
implementation of the language by extending system cplus2asp.

</details>


### [257] [Weighted Assumption Based Argumentation to reason about ethical principles and actions](https://arxiv.org/abs/2506.18056)
*Paolo Baldi,Fabio Aurelio D'Asaro,Abeer Dyoub,Francesca Alessandra Lisi*

Key words: 假设基础论证（ABA）、加权论证、伦理推理、答案集编程

TL;DR: 论文摘要介绍了如何通过加权增强假设基础论证（ABA），并展示了在伦理推理领域的应用和基于答案集编程的实现。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 为了扩展ABA的表达能力，使其能够处理带有权重的论证和攻击，从而更贴近实际应用场景，如伦理推理。

Method: 通过为论证分配权重，并基于此推导ABA论证之间的攻击权重，同时结合答案集编程技术实现该方法。

Result: 提出了加权ABA框架，并通过实际案例验证了其在伦理推理领域的适用性，同时实现了该方法的计算工具。

Conclusion: 加权ABA增强了传统ABA的表达能力和实用性，尤其在伦理推理等需要量化评估的场景中表现出色。

Abstract: We augment Assumption Based Argumentation (ABA for short) with weighted
argumentation. In a nutshell, we assign weights to arguments and then derive
the weight of attacks between ABA arguments. We illustrate our proposal through
running examples in the field of ethical reasoning, and present an
implementation based on Answer Set Programming.

</details>


### [258] [Deep Research Agents: A Systematic Examination And Roadmap](https://arxiv.org/abs/2506.18096)
*Yuxuan Huang,Yihang Chen,Haozheng Zhang,Kang Li,Meng Fang,Linyi Yang,Xiaoguang Li,Lifeng Shang,Songcen Xu,Jianye Hao,Kun Shao,Jun Wang*

Key words: 大语言模型, 深度研究智能体, 信息检索, 模块化工具, 动态规划

TL;DR: 本文详细分析了构成深度研究（DR）智能体的基础技术和架构组件，提出了分类法以系统化现有方法，并指出了当前基准测试的局限性及未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着大语言模型（LLM）的快速发展，深度研究智能体成为解决复杂多轮信息研究任务的重要工具，本文旨在系统化其技术基础并提出改进方向。

Method: 通过分析信息获取策略、模块化工具框架（如代码执行、多模态输入处理），提出分类法区分静态和动态工作流，并基于规划策略和智能体组合分类架构。

Result: 提出了一种系统化的DR智能体分类法，批评了当前基准测试的局限性，并指出了外部知识访问受限、执行效率低下等问题。

Conclusion: 深度研究智能体在解决复杂任务方面潜力巨大，但需改进基准测试和架构设计，未来研究应聚焦于动态规划和多智能体协作。

Abstract: The rapid progress of Large Language Models (LLMs) has given rise to a new
category of autonomous AI systems, referred to as Deep Research (DR) agents.
These agents are designed to tackle complex, multi-turn informational research
tasks by leveraging a combination of dynamic reasoning, adaptive long-horizon
planning, multi-hop information retrieval, iterative tool use, and the
generation of structured analytical reports. In this paper, we conduct a
detailed analysis of the foundational technologies and architectural components
that constitute Deep Research agents. We begin by reviewing information
acquisition strategies, contrasting API-based retrieval methods with
browser-based exploration. We then examine modular tool-use frameworks,
including code execution, multimodal input processing, and the integration of
Model Context Protocols (MCPs) to support extensibility and ecosystem
development. To systematize existing approaches, we propose a taxonomy that
differentiates between static and dynamic workflows, and we classify agent
architectures based on planning strategies and agent composition, including
single-agent and multi-agent configurations. We also provide a critical
evaluation of current benchmarks, highlighting key limitations such as
restricted access to external knowledge, sequential execution inefficiencies,
and misalignment between evaluation metrics and the practical objectives of DR
agents. Finally, we outline open challenges and promising directions for future
research. A curated and continuously updated repository of DR agent research is
available at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.

</details>


### [259] [Decentralized Consensus Inference-based Hierarchical Reinforcement Learning for Multi-Constrained UAV Pursuit-Evasion Game](https://arxiv.org/abs/2506.18126)
*Xiang Yuming,Li Sizhao,Li Rongpeng,Zhao Zhifeng,Zhang Honggang*

Key words: 无人机群, 多约束追逃游戏, 分层强化学习, 协作逃避, 编队覆盖

TL;DR: 论文提出了一种名为CI-HRL的两层次框架，用于解决多约束追逃游戏中的协作逃避与编队覆盖问题，通过分层强化学习方法提升无人机群的协作能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 多约束追逃游戏（MC-PEG）中的协作逃避与编队覆盖（CEFC）任务在通信受限条件下尤为复杂，需解决障碍、敌手、目标区域和编队动态的高维问题。

Method: 采用两层次框架CI-HRL，高层次策略（ConsMAC）感知全局信息并通过邻居消息建立共识，低层次策略（AT-M）处理避障、导航和编队控制。

Result: 实验（包括高保真SITL仿真）验证CI-HRL提升了无人机群的协作逃避与任务完成能力。

Conclusion: CI-HRL为复杂MC-PEG问题提供了一种高效的解决方案，增强了多无人机系统的协作性能。

Abstract: Multiple quadrotor unmanned aerial vehicle (UAV) systems have garnered
widespread research interest and fostered tremendous interesting applications,
especially in multi-constrained pursuit-evasion games (MC-PEG). The Cooperative
Evasion and Formation Coverage (CEFC) task, where the UAV swarm aims to
maximize formation coverage across multiple target zones while collaboratively
evading predators, belongs to one of the most challenging issues in MC-PEG,
especially under communication-limited constraints. This multifaceted problem,
which intertwines responses to obstacles, adversaries, target zones, and
formation dynamics, brings up significant high-dimensional complications in
locating a solution. In this paper, we propose a novel two-level framework
(i.e., Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL)),
which delegates target localization to a high-level policy, while adopting a
low-level policy to manage obstacle avoidance, navigation, and formation.
Specifically, in the high-level policy, we develop a novel multi-agent
reinforcement learning module, Consensus-oriented Multi-Agent Communication
(ConsMAC), to enable agents to perceive global information and establish
consensus from local states by effectively aggregating neighbor messages.
Meanwhile, we leverage an Alternative Training-based Multi-agent proximal
policy optimization (AT-M) and policy distillation to accomplish the low-level
control. The experimental results, including the high-fidelity
software-in-the-loop (SITL) simulations, validate that CI-HRL provides a
superior solution with enhanced swarm's collaborative evasion and task
completion capabilities.

</details>


### [260] [SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging](https://arxiv.org/abs/2506.18135)
*Zijun Chen,Zhanpeng Zhou,Bo Zhang,Weinan Zhang,Xi Sun,Junchi Yan*

Key words: 模型合并, 多任务学习, 动态调整, SE-Merging

TL;DR: 论文探讨了模型合并的机制，提出了一种自增强模型合并框架（SE-Merging），无需额外训练即可动态识别任务并增强性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 尽管模型合并在实践中表现良好，但其机制尚不明确，因此需要从表示角度深入研究。

Method: 通过分析发现模型合并的两大关键能力，并基于此提出了SE-Merging框架，动态识别任务并调整合并系数。

Result: SE-Merging显著提升了性能，且与现有技术兼容。

Conclusion: 模型合并通过任务识别和专家模型适应实现多任务能力，SE-Merging进一步优化了这一过程。

Abstract: Model merging has gained increasing attention due to its intriguing property:
interpolating the parameters of different task-specific fine-tuned models leads
to multi-task abilities. However, despite its empirical success, the underlying
mechanisms of model merging remain poorly understood. In this work, we delve
into the mechanism behind model merging from a representation perspective. Our
analysis reveals that model merging achieves multi-task abilities through two
key capabilities: i) distinguishing samples from different tasks, and ii)
adapting to the corresponding expert model for each sample. These two
capabilities allow the merged model to retain task-specific expertise, enabling
efficient multi-task adaptation. Building on these insights, we propose
\texttt{SE-Merging}, a self-enhanced model merging framework that leverages
these two characteristics to dynamically identify the corresponding task for
each sample and then adaptively rescales the merging coefficients to further
enhance task-specific expertise in the merged model. Notably,
\texttt{SE-Merging} achieves dynamic model merging without additional training.
Extensive experiments demonstrate that \texttt{SE-Merging} achieves significant
performance improvements while remaining compatible with existing model merging
techniques.

</details>


### [261] [CoachGPT: A Scaffolding-based Academic Writing Assistant](https://arxiv.org/abs/2506.18149)
*Fumian Chen,Sotheara Veng,Joshua Wilson,Xiaoming Li,Hui Fang*

Key words: 学术写作、大型语言模型、CoachGPT、个性化反馈、AI辅助教育

TL;DR: 论文介绍了教练GPT（CoachGPT），一款基于大型语言模型（LLMs）的学术写作辅助工具，旨在为教育资源有限或偏好自学的人群提供个性化反馈和指导。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统写作辅助工具缺乏情境理解和教学功能，而大型语言模型虽能生成文章但无法教学。为解决这一问题，开发了教练GPT。

Method: 教练GPT是一个基于AI代理的网页应用，结合教育专家指令，将任务分解为子任务，并利用LLMs提供实时反馈和建议。

Result: 用户研究表明，教练GPT提供了沉浸式写作体验，其个性化反馈和指导优于现有写作辅助工具。

Conclusion: 教练GPT展示了LLMs在学术写作中的潜力，尤其适合教育资源有限或自学的用户。

Abstract: Academic writing skills are crucial for students' success, but can feel
overwhelming without proper guidance and practice, particularly when writing in
a second language. Traditionally, students ask instructors or search
dictionaries, which are not universally accessible. Early writing assistants
emerged as rule-based systems that focused on detecting misspellings,
subject-verb disagreements, and basic punctuation errors; however, they are
inaccurate and lack contextual understanding. Machine learning-based assistants
demonstrate a strong ability for language understanding but are expensive to
train. Large language models (LLMs) have shown remarkable capabilities in
generating responses in natural languages based on given prompts. Still, they
have a fundamental limitation in education: they generate essays without
teaching, which can have detrimental effects on learning when misused. To
address this limitation, we develop CoachGPT, which leverages large language
models (LLMs) to assist individuals with limited educational resources and
those who prefer self-paced learning in academic writing. CoachGPT is an AI
agent-based web application that (1) takes instructions from experienced
educators, (2) converts instructions into sub-tasks, and (3) provides real-time
feedback and suggestions using large language models. This unique scaffolding
structure makes CoachGPT unique among existing writing assistants. Compared to
existing writing assistants, CoachGPT provides a more immersive writing
experience with personalized feedback and guidance. Our user studies prove the
usefulness of CoachGPT and the potential of large language models for academic
writing.

</details>


### [262] [AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology](https://arxiv.org/abs/2506.18156)
*Akash Kundu,Rishika Goswami*

Key words: 大型语言模型、心理学、认知模式、道德判断、AI安全

TL;DR: 论文研究了大型语言模型（LLMs）是否在四种心理学框架下表现出类似人类的认知模式，结果显示模型行为与人类认知倾向相似，但受训练数据和对齐方法影响。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探讨LLMs是否能够模仿人类认知模式，以理解其行为背后的心理学机制。

Method: 使用四种心理学框架（TAT、框架偏差、MFT、认知失调）评估多个专有和开源模型，采用结构化提示和自动评分。

Result: 模型能够生成连贯叙事，易受正面框架影响，道德判断偏向自由/压迫问题，并通过广泛合理化减轻自相矛盾。

Conclusion: LLMs的行为与人类认知倾向相似，但其表现受训练数据和对齐方法影响，这为AI透明度和伦理部署提供了启示。

Abstract: We investigate whether Large Language Models (LLMs) exhibit human-like
cognitive patterns under four established frameworks from psychology: Thematic
Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and
Cognitive Dissonance. We evaluated several proprietary and open-source models
using structured prompts and automated scoring. Our findings reveal that these
models often produce coherent narratives, show susceptibility to positive
framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and
demonstrate self-contradictions tempered by extensive rationalization. Such
behaviors mirror human cognitive tendencies yet are shaped by their training
data and alignment methods. We discuss the implications for AI transparency,
ethical deployment, and future work that bridges cognitive psychology and AI
safety

</details>


### [263] [Chain-of-Memory: Enhancing GUI Agents for Cross-Application Navigation](https://arxiv.org/abs/2506.18158)
*Xinzge Gao,Chuanrui Hu,Bin Chen,Teng Li*

Key words: 多模态大语言模型，图形用户界面代理，Chain-of-Memory，记忆管理

TL;DR: 该论文提出了一种名为Chain-of-Memory (CoM)的新型方法，用于显式建模GUI代理中的短期和长期记忆，以解决现有方法在理解任务状态和存储关键信息方面的不足。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有GUI代理方法依赖于历史截图或动作来隐式表示任务状态，导致在复杂跨应用任务中难以准确理解任务状态并缺乏有效的信息存储机制。

Method: CoM通过捕捉动作描述、整合任务相关屏幕信息并维护一个专用内存模块来显式建模短期和长期记忆。

Result: 实验结果表明，CoM显著提高了GUI代理在跨应用任务中的性能，并使7B模型达到与72B模型相当的记忆管理能力。

Conclusion: CoM为GUI代理提供了高效的任务状态理解和历史信息存储机制，并通过开放数据集和代码推动进一步研究。

Abstract: Multimodal large language models (MLLMs) are attracting growing attention in
the development of Graphical User Interface (GUI) agents. Existing approaches
often rely on historical screenshots or actions to implicitly represent the
task state. This reliance poses challenges for GUI agents in accurately
understanding task states and underscores the absence of effective mechanisms
to store critical information in complex and lengthy cross-app tasks. To
address these challenges, we propose Chain-of-Memory (CoM), a novel approach
for explicitly modeling short-term and long-term memory in GUI agents. CoM
achieves this by capturing action descriptions, integrating task-relevant
screen information, and maintaining a dedicated memory module to store and
manage this information. By leveraging explicit memory representations, CoM
enables GUI agents to better understand task states and retain critical
historical information persistently. To equip GUI agents with memory management
capabilities and evaluate the effectiveness of CoM, we developed the GUI
Odyssey-CoM, a dataset comprising 111k screen-action pairs annotated with
Chain-of-Memory. Experimental results demonstrate that CoM significantly
improves GUI agents' performance in cross-application tasks. Additionally, GUI
Odyssey-CoM enables 7B models to achieve memory management capabilities
comparable to 72B models. The dataset and code will be open-sourced.

</details>


### [264] [Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?](https://arxiv.org/abs/2506.18183)
*Zhiting Mei,Christina Zhang,Tenny Yin,Justin Lidard,Ola Shorinwa,Anirudha Majumdar*

Key words: 推理模型, 不确定性量化, 模型校准, 内省

TL;DR: 该论文探讨了推理语言模型的校准问题，提出内省不确定性量化（UQ）方法，发现模型通常过度自信，且更深层次推理会加剧此问题，部分模型可通过内省改善校准。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究推理模型的校准问题，以确保其在现实应用中的安全部署。

Method: 提出内省不确定性量化（UQ）方法，评估模型在不同推理深度和内省行为下的校准表现。

Result: 推理模型普遍过度自信，更深推理会恶化校准，内省可部分改善但效果不一致。

Conclusion: 需进一步研究UQ基准和改进推理模型的校准方法。

Abstract: Reasoning language models have set state-of-the-art (SOTA) records on many
challenging benchmarks, enabled by multi-step reasoning induced using
reinforcement learning. However, like previous language models, reasoning
models are prone to generating confident, plausible responses that are
incorrect (hallucinations). Knowing when and how much to trust these models is
critical to the safe deployment of reasoning models in real-world applications.
To this end, we explore uncertainty quantification of reasoning models in this
work. Specifically, we ask three fundamental questions: First, are reasoning
models well-calibrated? Second, does deeper reasoning improve model
calibration? Finally, inspired by humans' innate ability to double-check their
thought processes to verify the validity of their answers and their confidence,
we ask: can reasoning models improve their calibration by explicitly reasoning
about their chain-of-thought traces? We introduce introspective uncertainty
quantification (UQ) to explore this direction. In extensive evaluations on SOTA
reasoning models across a broad range of benchmarks, we find that reasoning
models: (i) are typically overconfident, with self-verbalized confidence
estimates often greater than 85% particularly for incorrect responses, (ii)
become even more overconfident with deeper reasoning, and (iii) can become
better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not
uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we
conclude with important research directions to design necessary UQ benchmarks
and improve the calibration of reasoning models.

</details>


### [265] [The Impact of Medication Non-adherence on Adverse Outcomes: Evidence from Schizophrenia Patients via Survival Analysis](https://arxiv.org/abs/2506.18187)
*Shahriar Noroozizadeh,Pim Welle,Jeremy C. Weiss,George H. Chen*

Key words: 精神分裂症, 抗精神病药物, 生存分析, 因果推断, 药物依从性, 不良结局

TL;DR: 该研究量化了精神分裂症患者中不依从抗精神病药物与不良结局的关联，通过生存分析和因果推断方法，发现不依从会提前不良结局1至4个月。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究动机是评估抗精神病药物不依从对精神分裂症患者不良结局的影响，为临床和政策提供依据。

Method: 方法包括生存分析（关注不良事件首次发生时间）、因果推断方法（T-learner、S-learner、最近邻匹配）以及纵向信息（3、6、9、12个月）。

Result: 结果显示，不依从会显著提前不良结局1至4个月，且在不同药物类型和剂型中结果一致。

Conclusion: 研究结论强调了药物依从性对延缓精神疾病危机的重要性，并展示了生存分析与因果推断工具结合的政策意义。

Abstract: This study quantifies the association between non-adherence to antipsychotic
medications and adverse outcomes in individuals with schizophrenia. We frame
the problem using survival analysis, focusing on the time to the earliest of
several adverse events (early death, involuntary hospitalization, jail
booking). We extend standard causal inference methods (T-learner, S-learner,
nearest neighbor matching) to utilize various survival models to estimate
individual and average treatment effects, where treatment corresponds to
medication non-adherence. Analyses are repeated using different amounts of
longitudinal information (3, 6, 9, and 12 months). Using data from Allegheny
County in western Pennsylvania, we find strong evidence that non-adherence
advances adverse outcomes by approximately 1 to 4 months. Ablation studies
confirm that county-provided risk scores adjust for key confounders, as their
removal amplifies the estimated effects. Subgroup analyses by medication
formulation (injectable vs. oral) and medication type consistently show that
non-adherence is associated with earlier adverse events. These findings
highlight the clinical importance of adherence in delaying psychiatric crises
and show that integrating survival analysis with causal inference tools can
yield policy-relevant insights. We caution that although we apply causal
inference, we only make associative claims and discuss assumptions needed for
causal interpretation.

</details>


### [266] [A Conceptual Framework for AI Capability Evaluations](https://arxiv.org/abs/2506.18213)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Luca Nicolás Forziati Gangi,Matheo Sandleris Musa,Lola Ramos Pereyra,Mario Leiva,Juan Gustavo Corvalan,María Vanina Martinez,Gerardo Simari*

Key words: AI评估，透明性，治理，方法论，框架

TL;DR: 提出了一个概念性框架，用于分析和系统化AI能力评估的方法和术语，增强评估的透明度、可比性和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着AI系统在社会中的广泛应用，透明且可靠的评估成为AI治理的重要工具，但目前缺乏系统化的方法来执行这些评估。

Method: 提出一个结构化的描述性框架，系统化分析常用评估方法和术语，不引入新的分类或固定格式。

Result: 该框架支持评估的透明度、可比性和可解释性，帮助识别方法弱点、设计评估工具，并为政策制定者提供导航工具。

Conclusion: 该框架为解决AI评估中的系统性问题提供了实用工具，促进了评估的全面性和可靠性。

Abstract: As AI systems advance and integrate into society, well-designed and
transparent evaluations are becoming essential tools in AI governance,
informing decisions by providing evidence about system capabilities and risks.
Yet there remains a lack of clarity on how to perform these assessments both
comprehensively and reliably. To address this gap, we propose a conceptual
framework for analyzing AI capability evaluations, offering a structured,
descriptive approach that systematizes the analysis of widely used methods and
terminology without imposing new taxonomies or rigid formats. This framework
supports transparency, comparability, and interpretability across diverse
evaluations. It also enables researchers to identify methodological weaknesses,
assists practitioners in designing evaluations, and provides policymakers with
an accessible tool to scrutinize, compare, and navigate complex evaluation
landscapes.

</details>


### [267] [The 4th Dimension for Scaling Model Size](https://arxiv.org/abs/2506.18233)
*Ruike Zhu,Hanwen Zhang,Tianyu Shi,Chi Wang,Tianyi Zhou,Zengyi Qin*

Key words: 虚拟逻辑深度, 参数重用, 模型扩展, 推理能力, 知识容量

TL;DR: 探索虚拟逻辑深度（VLD）作为模型扩展的第四维度，通过参数重用提升推理能力，无需增加参数数量。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究参数重用在模型扩展中的潜力与特性，探索如何在不增加参数总量的情况下提升模型性能。

Method: 通过精心设计的控制实验，比较不同VLD扩展方式对模型知识和推理能力的影响。

Result: VLD扩展保持知识容量几乎不变，但显著提升推理能力；参数数量与知识容量相关，但与推理能力无关。

Conclusion: 参数重用是一种有效的扩展方式，可在不增加参数的情况下提升模型的推理性能。

Abstract: Scaling the size of large language models typically involves three
dimensions: depth, width, and the number of parameters. In this work, we
explore a fourth dimension, virtual logical depth (VLD), which increases the
effective algorithmic depth without changing the overall parameter count by
reusing parameters within the model. Although parameter reuse is not a new
concept, its potential and characteristics in model scaling have not been
thoroughly studied. Through carefully designed controlled experiments, we make
the following key discoveries regarding VLD scaling:
  VLD scaling forces the knowledge capacity of the model to remain almost
constant, with only minor variations.
  VLD scaling enables a significant improvement in reasoning capability,
provided the scaling method is properly implemented.
  The number of parameters correlates with knowledge capacity, but not with
reasoning capability. Under certain conditions, it is not necessary to increase
the parameter count to enhance reasoning.
  These findings are consistent across various model configurations and are
likely to be generally valid within the scope of our experiments.

</details>


### [268] [Advanced For-Loop for QML algorithm search](https://arxiv.org/abs/2506.18260)
*FuTe Wong*

Key words: 大型语言模型, 多智能体系统, 量子机器学习, FunSearch, 经典算法量子化

TL;DR: 提出了一种基于大型语言模型的多智能体系统框架，用于自动搜索和优化量子机器学习算法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 受到Google DeepMind的FunSearch启发，旨在通过多智能体系统探索经典机器学习算法的量子化潜力。

Method: 利用多智能体系统在抽象层面上迭代生成和优化量子化的经典机器学习算法（如多层感知器、前向传播和反向传播算法）。

Result: 证明了智能体框架在系统探索经典机器学习概念并适配量子计算的潜力，为高效自动化开发量子机器学习算法奠定了基础。

Conclusion: 未来的研究方向包括引入规划机制和优化搜索策略，以扩展在量子增强机器学习中的应用。

Abstract: This paper introduces an advanced framework leveraging Large Language
Model-based Multi-Agent Systems (LLMMA) for the automated search and
optimization of Quantum Machine Learning (QML) algorithms. Inspired by Google
DeepMind's FunSearch, the proposed system works on abstract level to
iteratively generates and refines quantum transformations of classical machine
learning algorithms (concepts), such as the Multi-Layer Perceptron,
forward-forward and backpropagation algorithms. As a proof of concept, this
work highlights the potential of agentic frameworks to systematically explore
classical machine learning concepts and adapt them for quantum computing,
paving the way for efficient and automated development of QML algorithms.
Future directions include incorporating planning mechanisms and optimizing
strategy in the search space for broader applications in quantum-enhanced
machine learning.

</details>


### [269] [Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team](https://arxiv.org/abs/2506.18348)
*Weilun Yu,Shixiang Tang,Yonggui Huang,Nanqing Dong,Li Fan,Honggang Qi,Wei Liu,Xiaoli Diao,Xi Chen,Wanli Ouyang*

Key words: LLM, 多智能体, 科学协作, 动态知识交换, 多样性评审

TL;DR: 论文提出了一种基于LLM的多智能体框架IDVSCI，通过动态知识交换和双多样性评审机制，提升科学研究的交互推理和创造力，实验证明其在多个数据集上优于现有系统。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有LLM科学代理在自主发现中缺乏交互推理和评估机制，IDVSCI旨在模拟真实研究的协作与评审动态。

Method: 提出Dynamic Knowledge Exchange和Dual-Diversity Review机制，促进智能体间的迭代反馈与异质专家评估。

Result: 在计算机科学和健康科学数据集上，IDVSCI性能优于AI Scientist和VIRSCI等现有系统。

Conclusion: IDVSCI成功模拟了交互与同行评审动态，验证了其在LLM自主研究中的价值。

Abstract: Scientific progress increasingly relies on effective collaboration among
researchers, a dynamic that large language models (LLMs) have only begun to
emulate. While recent LLM-based scientist agents show promise in autonomous
scientific discovery, they often lack the interactive reasoning and evaluation
mechanisms essential to real-world research. We propose IDVSCI (Internal
Discussion and Vote SCIentists), a multi-agent framework built on LLMs that
incorporates two key innovations: a Dynamic Knowledge Exchange mechanism
enabling iterative feedback among agents, and a Dual-Diversity Review paradigm
that simulates heterogeneous expert evaluation. These components jointly
promote deeper reasoning and the generation of more creative and impactful
scientific ideas. To evaluate the effectiveness and generalizability of our
approach, we conduct experiments on two datasets: a widely used benchmark in
computer science and a new dataset we introduce in the health sciences domain.
Results show that IDVSCI consistently achieves the best performance across both
datasets, outperforming existing systems such as AI Scientist and VIRSCI. These
findings highlight the value of modeling interaction and peer review dynamics
in LLM-based autonomous research.

</details>


### [270] [A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction](https://arxiv.org/abs/2506.18424)
*Chengjie Liu,Weiyu Chen,Huiyao Xu,Yuan Du,Jun Yang,Li Du*

Key words: 大型语言模型, 模拟电路, 尺寸关系, 搜索空间剪裁, 优化效率

TL;DR: 本文提出了一种基于大型语言模型（LLM）的多智能体框架，用于从学术论文中提取模拟电路的尺寸关系，从而有效剪裁搜索空间，提升优化效率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有技术在模拟电路预布局阶段的设备尺寸设计中，忽视了先验知识的自动引入，未能有效剪裁搜索空间，导致搜索空间仍然存在较大的压缩空间。

Method: 使用大型语言模型（LLM）构建多智能体框架，从学术论文中提取模拟电路的尺寸关系，并基于此剪裁搜索空间。

Result: 在3种电路的测试中，优化效率提升了2.32至26.6倍。

Conclusion: 研究表明，LLM可以有效剪裁模拟电路尺寸设计的搜索空间，为LLM与传统模拟电路设计自动化方法的结合提供了新思路。

Abstract: In the design process of the analog circuit pre-layout phase, device sizing
is an important step in determining whether an analog circuit can meet the
required performance metrics. Many existing techniques extract the circuit
sizing task as a mathematical optimization problem to solve and continuously
improve the optimization efficiency from a mathematical perspective. But they
ignore the automatic introduction of prior knowledge, fail to achieve effective
pruning of the search space, which thereby leads to a considerable compression
margin remaining in the search space. To alleviate this problem, we propose a
large language model (LLM)-based multi-agent framework for analog circuits'
sizing relationships extraction from academic papers. The search space in the
sizing process can be effectively pruned based on the sizing relationship
extracted by this framework. Eventually, we conducted tests on 3 types of
circuits, and the optimization efficiency was improved by $2.32 \sim 26.6
\times$. This work demonstrates that the LLM can effectively prune the search
space for analog circuit sizing, providing a new solution for the combination
of LLMs and conventional analog circuit design automation methods.

</details>


### [271] [How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.18428)
*Feng He,Zhenyang Liu,Marco Valentino,Zhixue Zhao*

Key words: 模型编辑, 微调, T2I扩散模型, AI安全性, UCE, ReFACT

TL;DR: 模型编辑是一种低成本的调整预训练模型行为的方法，但研究发现微调会导致编辑失效，揭示了当前编辑技术的局限性，并提出了对AI安全性的双重影响。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探讨模型编辑与微调之间的交互作用，以验证编辑效果在微调后是否持久，这对AI安全性和模型行为控制具有重要意义。

Method: 在T2I扩散模型（Stable Diffusion和FLUX）上，结合两种编辑技术（UCE和ReFACT）和三种微调方法（DreamBooth、LoRA和DoRA），通过实证分析评估编辑效果的持久性。

Result: 研究发现编辑效果通常无法在微调后保留，其中DoRA的编辑反转效应最强，而UCE在编辑方法中表现更稳健。

Conclusion: 当前编辑技术存在局限性，需开发更鲁棒的方法以确保长期可靠的模型控制和安全性。微调可作为对抗恶意编辑的手段，但也需通过重新编辑维持有益的安全性。

Abstract: Model editing offers a low-cost technique to inject or correct a particular
behavior in a pre-trained model without extensive retraining, supporting
applications such as factual correction and bias mitigation. Despite this
common practice, it remains unknown whether edits persist after fine-tuning or
whether they are inadvertently reversed. This question has fundamental
practical implications. For example, if fine-tuning removes prior edits, it
could serve as a defence mechanism against hidden malicious edits. Vice versa,
the unintended removal of edits related to bias mitigation could pose serious
safety concerns. We systematically investigate the interaction between model
editing and fine-tuning in the context of T2I diffusion models, which are known
to exhibit biases and generate inappropriate content. Our study spans two T2I
model families (Stable Diffusion and FLUX), two sota editing techniques, and
three fine-tuning methods (DreamBooth, LoRA, and DoRA). Through an extensive
empirical analysis across diverse editing tasks and evaluation metrics, our
findings reveal a trend: edits generally fail to persist through fine-tuning,
even when fine-tuning is tangential or unrelated to the edits. Notably, we
observe that DoRA exhibits the strongest edit reversal effect. At the same
time, among editing methods, UCE demonstrates greater robustness, retaining
significantly higher efficacy post-fine-tuning compared to ReFACT. These
findings highlight a crucial limitation in current editing methodologies,
emphasizing the need for more robust techniques to ensure reliable long-term
control and alignment of deployed AI systems. These findings have dual
implications for AI safety: they suggest that fine-tuning could serve as a
remediation mechanism for malicious edits while simultaneously highlighting the
need for re-editing after fine-tuning to maintain beneficial safety and
alignment properties.

</details>


### [272] [Standard Applicability Judgment and Cross-jurisdictional Reasoning: A RAG-based Framework for Medical Device Compliance](https://arxiv.org/abs/2506.18511)
*Yu Han,Aaron Ceross,Jeroen H. M. Bergmann*

Key words: 医疗设备合规性, 标准适用性, 检索增强生成, 跨司法管辖区推理, 监管科学

TL;DR: 该论文提出了一个模块化AI系统，通过检索增强生成（RAG）自动化确定医疗器械监管标准的适用性，解决了标准适用性判定的难题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 医疗器械合规性中，标准适用性判定依赖专家对分散且异构的文件进行解读，挑战巨大且研究不足。

Method: 系统结合RAG管道和大型语言模型，从自由文本设备描述中检索候选标准，并推断特定司法管辖区的适用性（强制性、推荐或不适用），同时提供可追溯的论证。

Result: 系统分类准确率为73%，Top-5检索召回率为87%，在跨司法管辖区（如中美）标准冲突解决中表现突出。

Conclusion: 该系统是首个端到端的标准适用性推理系统，为可扩展和可解释的监管科学AI支持提供了解决方案。

Abstract: Identifying the appropriate regulatory standard applicability remains a
critical yet understudied challenge in medical device compliance, frequently
necessitating expert interpretation of fragmented and heterogeneous
documentation across different jurisdictions. To address this challenge, we
introduce a modular AI system that leverages a retrieval-augmented generation
(RAG) pipeline to automate standard applicability determination. Given a
free-text device description, our system retrieves candidate standards from a
curated corpus and uses large language models to infer jurisdiction-specific
applicability, classified as Mandatory, Recommended, or Not Applicable, with
traceable justifications. We construct an international benchmark dataset of
medical device descriptions with expert-annotated standard mappings, and
evaluate our system against retrieval-only, zero-shot, and rule-based
baselines. The proposed approach attains a classification accuracy of 73% and a
Top-5 retrieval recall of 87%, demonstrating its effectiveness in identifying
relevant regulatory standards. We introduce the first end-to-end system for
standard applicability reasoning, enabling scalable and interpretable
AI-supported regulatory science. Notably, our region-aware RAG agent performs
cross-jurisdictional reasoning between Chinese and U.S. standards, supporting
conflict resolution and applicability justification across regulatory
frameworks.

</details>


### [273] [A Question Bank to Assess AI Inclusivity: Mapping out the Journey from Diversity Errors to Inclusion Excellence](https://arxiv.org/abs/2506.18538)
*Rifat Ara Shams,Didar Zowghi,Muneera Bano*

Key words: AI包容性, D&I原则, 题库, 风险评估, 模拟评估

TL;DR: 论文提出了一种包含253个问题的AI包容性题库，用于评估AI系统在五方面的包容性，强调将D&I原则融入AI开发工作流的重要性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有AI风险评估框架常忽略包容性，缺乏标准化工具来衡量AI系统与D&I原则的一致性。

Method: 通过文献综述、D&I指南、Responsible AI框架和模拟用户研究，迭代开发了该题库。模拟评估涉及70个AI生成的角色。

Result: 题库能有效评估AI包容性，突出D&I原则整合的必要性。

Conclusion: 题库为研究者、从业者和政策制定者提供了系统性评估和提升AI包容性的工具。

Abstract: Ensuring diversity and inclusion (D&I) in artificial intelligence (AI) is
crucial for mitigating biases and promoting equitable decision-making. However,
existing AI risk assessment frameworks often overlook inclusivity, lacking
standardized tools to measure an AI system's alignment with D&I principles.
This paper introduces a structured AI inclusivity question bank, a
comprehensive set of 253 questions designed to evaluate AI inclusivity across
five pillars: Humans, Data, Process, System, and Governance. The development of
the question bank involved an iterative, multi-source approach, incorporating
insights from literature reviews, D&I guidelines, Responsible AI frameworks,
and a simulated user study. The simulated evaluation, conducted with 70
AI-generated personas related to different AI jobs, assessed the question
bank's relevance and effectiveness for AI inclusivity across diverse roles and
application domains. The findings highlight the importance of integrating D&I
principles into AI development workflows and governance structures. The
question bank provides an actionable tool for researchers, practitioners, and
policymakers to systematically assess and enhance the inclusivity of AI
systems, paving the way for more equitable and responsible AI technologies.

</details>


### [274] [T-CPDL: A Temporal Causal Probabilistic Description Logic for Developing Logic-RAG Agent](https://arxiv.org/abs/2506.18559)
*Hong Qing Yu*

Key words: 时间因果概率描述逻辑, 结构化推理, 语言模型, Logic-RAG

TL;DR: 该论文提出了T-CPDL框架，结合时间、因果和概率推理，提升了语言模型在结构化推理中的能力，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决大语言模型在处理时间约束、因果关系和概率推理时的局限性。

Method: 扩展传统描述逻辑，引入时间区间操作符、显式因果关系和概率标注，提出了两种T-CPDL变体。

Result: 实验表明，T-CPDL显著提高了推理准确性、可解释性和置信校准。

Conclusion: T-CPDL增强了语言模型的透明推理能力，为Logic-RAG框架的发展奠定了基础。

Abstract: Large language models excel at generating fluent text but frequently struggle
with structured reasoning involving temporal constraints, causal relationships,
and probabilistic reasoning. To address these limitations, we propose Temporal
Causal Probabilistic Description Logic (T-CPDL), an integrated framework that
extends traditional Description Logic with temporal interval operators,
explicit causal relationships, and probabilistic annotations. We present two
distinct variants of T-CPDL: one capturing qualitative temporal relationships
through Allen's interval algebra, and another variant enriched with explicit
timestamped causal assertions. Both variants share a unified logical structure,
enabling complex reasoning tasks ranging from simple temporal ordering to
nuanced probabilistic causation. Empirical evaluations on temporal reasoning
and causal inference benchmarks confirm that T-CPDL substantially improves
inference accuracy, interpretability, and confidence calibration of language
model outputs. By delivering transparent reasoning paths and fine-grained
temporal and causal semantics, T-CPDL significantly enhances the capability of
language models to support robust, explainable, and trustworthy
decision-making. This work also lays the groundwork for developing advanced
Logic-Retrieval-Augmented Generation (Logic-RAG) frameworks, potentially
boosting the reasoning capabilities and efficiency of knowledge graph-enhanced
RAG systems.

</details>


### [275] [Airalogy: AI-empowered universal data digitization for research automation](https://arxiv.org/abs/2506.18586)
*Zijie Yang,Qiji Zhou,Fang Guo,Sijie Zhang,Yexun Xi,Jinglei Nie,Yudian Zhu,Liping Huang,Chou Wu,Yonghe Xia,Xiaoyu Ma,Yingming Pu,Panzhong Lu,Junshu Pan,Mingtao Chen,Tiannan Guo,Yanmei Dou,Hongyu Chen,Anping Zeng,Jiaxing Huang,Tian Xu,Yue Zhang*

Key words: AI-driven science, data standardization, multidisciplinary platform, research automation, Airalogy

TL;DR: Airalogy是全球首个AI驱动、社区支持的多学科研究数据数字化平台，旨在平衡通用性与标准化，促进AI驱动的科学发展。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前AI应用受限于数据碎块化、缺乏统一标准及难以共享的问题，阻碍了多学科的AI赋能。

Method: 开发Airalogy平台，结合科学领域知识与计算技能，提供可定制、标准化的数据记录方法及AI研究助手功能。

Result: Airalogy已在西湖大学各学院实验室部署，具备加速科学创新的潜力。

Conclusion: Airalogy通过平衡通用性与标准化，有望推动全球科研社区的科学自动化与创新。

Abstract: Research data are the foundation of Artificial Intelligence (AI)-driven
science, yet current AI applications remain limited to a few fields with
readily available, well-structured, digitized datasets. Achieving comprehensive
AI empowerment across multiple disciplines is still out of reach. Present-day
research data collection is often fragmented, lacking unified standards,
inefficiently managed, and difficult to share. Creating a single platform for
standardized data digitization needs to overcome the inherent challenge of
balancing between universality (supporting the diverse, ever-evolving needs of
various disciplines) and standardization (enforcing consistent formats to fully
enable AI). No existing platform accommodates both facets. Building a truly
multidisciplinary platform requires integrating scientific domain knowledge
with sophisticated computing skills. Researchers often lack the computational
expertise to design customized and standardized data recording methods, whereas
platform developers rarely grasp the intricate needs of multiple scientific
domains. These gaps impede research data standardization and hamper AI-driven
progress. In this study, we address these challenges by developing Airalogy
(https://airalogy.com), the world's first AI- and community-driven platform
that balances universality and standardization for digitizing research data
across multiple disciplines. Airalogy represents entire research workflows
using customizable, standardized data records and offers an advanced AI
research copilot for intelligent Q&A, automated data entry, analysis, and
research automation. Already deployed in laboratories across all four schools
of Westlake University, Airalogy has the potential to accelerate and automate
scientific innovation in universities, industry, and the global research
community-ultimately benefiting humanity as a whole.

</details>


### [276] [AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs](https://arxiv.org/abs/2506.18628)
*Piotr Matys,Jan Eliasz,Konrad Kiełczyński,Mikołaj Langner,Teddy Ferdinan,Jan Kocoń,Przemysław Kazienko*

Key words: 大型语言模型, 幻觉检测, 检索增强生成, 注意力分数, AggTruth

TL;DR: AggTruth是一种通过分析内部注意力分数分布来检测LLM在上下文中幻觉的新方法，共有四种变体，性能优于当前最佳方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决大型语言模型（LLM）在实际应用中的幻觉问题，尤其是在检索增强生成（RAG）场景下。

Method: 提出AggTruth方法，分析上下文（段落）中内部注意力分数的分布，并设计了四种不同的聚合技术变体。

Result: AggTruth在不同任务和模型中都表现稳定，优于当前最佳方法，且特征选择和注意力头数量对性能有显著影响。

Conclusion: 通过精心选择注意力头，AggTruth能有效检测上下文幻觉，为LLM的实际部署提供支持。

Abstract: In real-world applications, Large Language Models (LLMs) often hallucinate,
even in Retrieval-Augmented Generation (RAG) settings, which poses a
significant challenge to their deployment. In this paper, we introduce
AggTruth, a method for online detection of contextual hallucinations by
analyzing the distribution of internal attention scores in the provided context
(passage). Specifically, we propose four different variants of the method, each
varying in the aggregation technique used to calculate attention scores. Across
all LLMs examined, AggTruth demonstrated stable performance in both same-task
and cross-task setups, outperforming the current SOTA in multiple scenarios.
Furthermore, we conducted an in-depth analysis of feature selection techniques
and examined how the number of selected attention heads impacts detection
performance, demonstrating that careful selection of heads is essential to
achieve optimal results.

</details>


### [277] [Dual-level Behavioral Consistency for Inter-group and Intra-group Coordination in Multi-Agent Systems](https://arxiv.org/abs/2506.18651)
*Shuocun Yang,Huawen Hu,Enze Shi,Shu Zhang*

Key words: 多智能体强化学习,行为一致性,组内合作,组间任务专一性

TL;DR: 本文提出一种名为DLBC的新方法，用于多智能体强化学习中动态调节组内和组间行为一致性，显著提升组内合作与组间任务专一性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 多智能体强化学习中行为多样性研究较少关注组间行为一致性，本文旨在填补这一空白。

Method: 引入DLBC方法，通过动态调节组内和组间行为多样性，约束策略函数以实现广泛适用性。

Result: 实验表明，DLBC在多种分组合作场景中显著提升组内合作和组间任务专一性。

Conclusion: DLBC为多智能体系统行为一致性控制提供新思路，其潜在应用可进一步探索。

Abstract: Behavioral diversity in Multi-agent reinforcement learning(MARL) represents
an emerging and promising research area. Prior work has largely centered on
intra-group behavioral consistency in multi-agent systems, with limited
attention given to behavioral consistency in multi-agent grouping scenarios. In
this paper, we introduce Dual-Level Behavioral Consistency (DLBC), a novel MARL
control method designed to explicitly regulate agent behaviors at both
intra-group and inter-group levels. DLBC partitions agents into distinct groups
and dynamically modulates behavioral diversity both within and between these
groups. By dynamically modulating behavioral diversity within and between these
groups, DLBC achieves enhanced division of labor through inter-group
consistency, which constrains behavioral strategies across different groups.
Simultaneously, intra-group consistency, achieved by aligning behavioral
strategies within each group, fosters stronger intra-group cooperation.
Crucially, DLBC's direct constraint of agent policy functions ensures its broad
applicability across various algorithmic frameworks. Experimental results in
various grouping cooperation scenarios demonstrate that DLBC significantly
enhances both intra-group cooperative performance and inter-group task
specialization, yielding substantial performance improvements. DLBC provides
new ideas for behavioral consistency control of multi-intelligent body systems,
and its potential for application in more complex tasks and dynamic
environments can be further explored in the future.

</details>


### [278] [Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training](https://arxiv.org/abs/2506.18777)
*Jonathan Cook,Silvia Sapora,Arash Ahmadian,Akbir Khan,Tim Rocktaschel,Jakob Foerster,Laura Ruis*

Key words: 大语言模型、源代码训练、反向传播编程、链式思维、算法抽象

TL;DR: 论文提出了通过反向传播编程（PBB）作为大语言模型（LLM）在源代码上训练时增强通用推理能力的潜在机制，并证实了LLMs仅凭源代码也能评估程序输入的能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究大语言模型在源代码训练中为何能增强通用推理能力，探索其背后的机制。

Method: 通过比较两组程序（一组带I/O示例，另一组仅带源代码）微调LLMs，测试其在无I/O情况下的程序评估能力。

Result: LLMs仅凭源代码能够评估程序输入，且PBB在代码形式下表现更优；链式思维（chain-of-thought）能更可靠地生成输出。

Conclusion: 代码训练使LLMs内化可重用的算法抽象，从而增强推理能力；未来研究可进一步优化符号程序学习。

Abstract: Training large language models (LLMs) on source code significantly enhances
their general-purpose reasoning abilities, but the mechanisms underlying this
generalisation are poorly understood. In this paper, we propose Programming by
Backprop (PBB) as a potential driver of this effect - teaching a model to
evaluate a program for inputs by training on its source code alone, without
ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of
programs representing simple maths problems and algorithms: one with source
code and I/O examples (w/ IO), the other with source code only (w/o IO). We
find evidence that LLMs have some ability to evaluate w/o IO programs for
inputs in a range of experimental settings, and make several observations.
Firstly, PBB works significantly better when programs are provided as code
rather than semantically equivalent language descriptions. Secondly, LLMs can
produce outputs for w/o IO programs directly, by implicitly evaluating the
program within the forward pass, and more reliably when stepping through the
program in-context via chain-of-thought. We further show that PBB leads to more
robust evaluation of programs across inputs than training on I/O pairs drawn
from a distribution that mirrors naturally occurring data. Our findings suggest
a mechanism for enhanced reasoning through code training: it allows LLMs to
internalise reusable algorithmic abstractions. Significant scope remains for
future work to enable LLMs to more effectively learn from symbolic procedures,
and progress in this direction opens other avenues like model alignment by
training on formal constitutional principles.

</details>


### [279] [TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation](https://arxiv.org/abs/2506.18783)
*Kamil Szczepanik,Jarosław A. Chudziak*

Key words: TRIZ, 大型语言模型, 多智能体系统, 创新, 问题解决

TL;DR: 论文提出了一种基于大型语言模型（LLM）的多智能体系统（TRIZ agents），通过协作解决创新问题，以克服传统TRIZ方法的高复杂性和跨学科知识要求。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 由于TRIZ方法的复杂性和跨学科知识需求限制了其应用，论文探索了利用多智能体LLM系统自动化解决创新问题的可能性。

Method: 提出了一种多智能体系统，每个智能体具备专业能力和工具访问权限，基于TRIZ方法协作解决问题。通过案例研究评估其有效性。

Result: 研究表明多智能体协作能够产生多样化的创新解决方案，展示了去中心化问题解决在复杂创新任务中的优势。

Conclusion: 该研究为AI驱动的创新提供了新方向，验证了多智能体系统在复杂创新问题中的潜力。

Abstract: TRIZ, the Theory of Inventive Problem Solving, is a structured,
knowledge-based framework for innovation and abstracting problems to find
inventive solutions. However, its application is often limited by the
complexity and deep interdisciplinary knowledge required. Advancements in Large
Language Models (LLMs) have revealed new possibilities for automating parts of
this process. While previous studies have explored single LLMs in TRIZ
applications, this paper introduces a multi-agent approach. We propose an
LLM-based multi-agent system, called TRIZ agents, each with specialized
capabilities and tool access, collaboratively solving inventive problems based
on the TRIZ methodology. This multi-agent system leverages agents with various
domain expertise to efficiently navigate TRIZ steps. The aim is to model and
simulate an inventive process with language agents. We assess the effectiveness
of this team of agents in addressing complex innovation challenges based on a
selected case study in engineering. We demonstrate the potential of agent
collaboration to produce diverse, inventive solutions. This research
contributes to the future of AI-driven innovation, showcasing the advantages of
decentralized problem-solving in complex ideation tasks.

</details>


### [280] [ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation](https://arxiv.org/abs/2506.18810)
*Siao Tang,Xinyin Ma,Gongfan Fang,Xinchao Wang*

Key words: 大型推理模型, 简洁推理, Chain-of-Thought, ConciseHint, 动态提示

TL;DR: 提出ConciseHint框架，通过动态注入文本提示促使大型推理模型生成简洁的推理过程，同时保持性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 大型推理模型在复杂任务中表现优异，但其推理过程冗长导致效率低下，现有方法忽视了在生成过程中直接干预的方向。

Method: 设计ConciseHint框架，在推理过程中动态注入文本提示（手动设计或训练于简洁数据），自适应调整提示强度以应对查询复杂度。

Result: 在DeepSeek-R1和Qwen-3等模型上实验，推理长度减少65%（GSM8K基准），几乎无准确性损失。

Conclusion: ConciseHint能有效生成简洁推理过程且不影响性能，填补了现有方法的空白。

Abstract: Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and
OpenAI o1 series have achieved notable performance enhancements on complex
reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).
However, an emerging issue is their inclination to produce excessively verbose
reasoning processes, leading to the inefficiency problem. Existing literature
on improving efficiency mainly adheres to the before-reasoning paradigms such
as prompting and reasoning or fine-tuning and reasoning, but ignores the
promising direction of directly encouraging the model to speak concisely by
intervening during the generation of reasoning. In order to fill the blank, we
propose a framework dubbed ConciseHint, which continuously encourages the
reasoning model to speak concisely by injecting the textual hint (manually
designed or trained on the concise data) during the token generation of the
reasoning process. Besides, ConciseHint is adaptive to the complexity of the
query by adaptively adjusting the hint intensity, which ensures it will not
undermine model performance. Experiments on the state-of-the-art LRMs,
including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can
effectively produce concise reasoning processes while maintaining performance
well. For instance, we achieve a reduction ratio of 65\% for the reasoning
length on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.

</details>


### [281] [Steering Conceptual Bias via Transformer Latent-Subspace Activation](https://arxiv.org/abs/2506.18887)
*Vansh Sharma,Venkat Raman*

Key words: 语言模型（LLMs），科学代码生成，编程语言偏向性，G-ACT，可解释性

TL;DR: 研究了如何通过激活语言模型中的潜在子空间来引导科学代码生成偏向特定编程语言。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索语言模型在科学代码生成中的编程语言偏向性，并提出解决方法以实现可扩展、可解释的控制机制。

Method: 首先评估模型的编程语言偏向性，随后提出梯度精炼的自适应激活引导框架（G-ACT），通过聚类和轻量级探测来引导生成方向。

Result: G-ACT在LLaMA-3.2 3B上显著提高了CPP语言的偏向性，并在LLaMA-3.3 70B中仍然有效。

Conclusion: G-ACT提供了一种可扩展、高效的概念级别控制机制。

Abstract: This work examines whether activating latent subspaces in language models
(LLMs) can steer scientific code generation toward a specific programming
language. Five causal LLMs were first evaluated on scientific coding prompts to
quantify their baseline bias among four programming languages. A static
neuron-attribution method, perturbing the highest activated MLP weight for a
C++ or CPP token, proved brittle and exhibited limited generalization across
prompt styles and model scales. To address these limitations, a
gradient-refined adaptive activation steering framework (G-ACT) was developed:
per-prompt activation differences are clustered into a small set of steering
directions, and lightweight per-layer probes are trained and refined online to
select the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably
biases generation towards the CPP language by increasing the average probe
classification accuracy by 15% and the early layers (0-6) improving the probe
classification accuracy by 61.5% compared to the standard ACT framework. For
LLaMA-3.3 70B, where attention-head signals become more diffuse, targeted
injections at key layers still improve language selection. Although per-layer
probing introduces a modest inference overhead, it remains practical by
steering only a subset of layers and enables reproducible model behavior. These
results demonstrate a scalable, interpretable and efficient mechanism for
concept-level control for practical agentic systems.

</details>


### [282] [jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval](https://arxiv.org/abs/2506.18902)
*Michael Günther,Saba Sturua,Mohammad Kalim Akram,Isabelle Mohr,Andrei Ungureanu,Sedigheh Eslami,Scott Martens,Bo Wang,Nan Wang,Han Xiao*

Key words: 多模态嵌入模型, LoRA适配器, 视觉丰富内容, 跨模态检索, Jina-VDR

TL;DR: jina-embeddings-v4是一个3.8亿参数的多模态嵌入模型，通过新颖架构统一文本和图像表示，支持单向量和多向量嵌入，结合LoRA适配器优化多种检索任务，性能达到SOTA，并针对视觉丰富内容特别优化。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 目标是开发一个统一文本和图像表示的多模态嵌入模型，优化多样化检索任务，特别是在视觉丰富内容上的表现。

Method: 采用包含LoRA适配器的新颖架构，支持单向量和多向量嵌入，适用于跨模态语义相似性和编程代码搜索等任务。

Result: 在单模态和跨模态检索任务上达到SOTA性能，尤其在处理视觉丰富内容（如表格、图表、混合媒体）方面表现出色。

Conclusion: jina-embeddings-v4在多模态嵌入和视觉丰富内容检索中展现了强大性能，同时推出了Jina-VDR作为新的视觉丰富图像检索基准。

Abstract: We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding
model that unifies text and image representations through a novel architecture
supporting both single-vector and multi-vector embeddings in the late
interaction style. The model incorporates task-specific Low-Rank Adaptation
(LoRA) adapters to optimize performance across diverse retrieval scenarios,
including query-based information retrieval, cross-modal semantic similarity,
and programming code search. Comprehensive evaluations demonstrate that
jina-embeddings-v4 achieves state-of-the-art performance on both single- modal
and cross-modal retrieval tasks, with particular strength in processing
visually rich content such as tables, charts, diagrams, and mixed-media
formats. To facilitate evaluation of this capability, we also introduce
Jina-VDR, a novel benchmark specifically designed for visually rich image
retrieval.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [283] [Advanced Modeling for Exoplanet Detection and Characterization](https://arxiv.org/abs/2506.17665)
*Krishna Chamarthy*

Key words: 光变曲线, 系外行星, 机器学习, 开普勒数据集, 行星物理特性

TL;DR: 利用开普勒数据集中的恒星光变曲线，通过周期性亮度下降和机器学习方法发现系外行星并估算其物理特性。

<details>
  <summary>Details</summary>

Main category: astro-ph.EP

Motivation: 研究恒星光变曲线可以改变系外行星的发现和表征方式，提升搜索效率。

Method: 分析光变曲线的周期性亮度下降，结合机器学习分类恒星是否有系外行星，估算行星的轨道周期、半径等参数。

Result: 通过光变曲线和机器学习方法，能够高效发现系外行星并估算其物理特性。

Conclusion: 该方法为快速筛选天文数据中的系外行星提供了有效途径。

Abstract: Research into light curves from stars (temporal variation of brightness) has
completely changed how exoplanets are discovered or characterised. This study
including star light curves from the Kepler dataset as a way to discover
exoplanets (planetary transits) and derive some estimate of their physical
characteristics by the light curve and machine learning methods. The dataset
consists of measured flux (recordings) for many individual stars and we will
examine the light curve of each star and look for periodic dips in brightness
due to an astronomical body making a transit. We will apply variables derived
from an established method for deriving measurements from light curve data to
derive key parameters related to the planet we observed during the transit,
such as distance to the host star, orbital period, radius. The orbital period
will typically be measured based on the time between transit of the subsequent
timelines and the radius will be measured based on the depth of transit. The
density of the star and planet can also be estimated from the transit event, as
well as very limited information on the albedo (reflectivity) and atmosphere of
the planet based on transmission spectroscopy and/or the analysis of phase
curve for levels of flux. In addition to these methods, we will employ some
machine learning classification of the stars (i.e. likely have an exoplanet or
likely do not have an exoplanet) based on flux change. This could help fulfil
both the process of looking for exoplanets more efficient as well as providing
important parameters for the planet. This will provide a much quicker means of
searching the vast astronomical datasets for the likelihood of exoplanets.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [284] [Distinguishing Predictive and Generative AI in Regulation](https://arxiv.org/abs/2506.17347)
*Jennifer Wang,Andrew Selbst,Solon Barocas,Suresh Venkatasubramanian*

Key words: 生成式AI, 监管政策, 风险评估, 政策建议

TL;DR: 本文探讨了生成式AI对现有政策工具的挑战，提出了四点独特性质，并建议政策制定者调整监管策略。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 生成式AI的出现使得原本针对预测式AI的监管假设不再适用，需要新的政策应对其独特风险。

Method: 通过分析生成式AI的四个独特方面，提出其对监管的影响，并给出三条政策建议。

Result: 识别了生成式AI的四个关键区别，建议政策制定者重新评估现有政策并制定新措施。

Conclusion: 政策制定者需针对生成式AI的特异性调整监管框架，以更有效地应对其风险。

Abstract: Over the past decade, policymakers have developed a set of regulatory tools
to ensure AI development aligns with key societal goals. Many of these tools
were initially developed in response to concerns with predictive AI and
therefore encode certain assumptions about the nature of AI systems and the
utility of certain regulatory approaches. With the advent of generative AI,
however, some of these assumptions no longer hold, even as policymakers attempt
to maintain a single regulatory target that covers both types of AI.
  In this paper, we identify four distinct aspects of generative AI that call
for meaningfully different policy responses. These are the generality and
adaptability of generative AI that make it a poor regulatory target, the
difficulty of designing effective evaluations, new legal concerns that change
the ecosystem of stakeholders and sources of expertise, and the distributed
structure of the generative AI value chain.
  In light of these distinctions, policymakers will need to evaluate where the
past decade of policy work remains relevant and where new policies, designed to
address the unique risks posed by generative AI, are necessary. We outline
three recommendations for policymakers to more effectively identify regulatory
targets and leverage constraints across the broader ecosystem to govern
generative AI.

</details>


### [285] [The Democratic Paradox in Large Language Models' Underestimation of Press Freedom](https://arxiv.org/abs/2506.18045)
*I. Loaiza,R. Vestrelli,A. Fronzetti Colladon,R. Rigobon*

Key words: 大型语言模型, 新闻自由, 偏见, WPFI, 民主制度

TL;DR: 研究发现六大流行LLM在评估180个国家新闻自由时存在系统性偏差，与WPFI专家评估相比，它们普遍低估新闻自由，且对本国存在正面偏差。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 随着LLM在全球信息获取中的作用日益重要，其偏差可能影响公众对民主机构如新闻自由的信任，因此需要研究LLM的评估准确性。

Method: 比较六种LLM对180个国家新闻自由的评估与世界新闻自由指数(WPFI)的专家评估。

Result: LLM普遍低估新闻自由，71%至93%国家被评为更不自由；存在“差异偏差”，即在新闻自由强的国家低估更严重；五LLM对本国评价存在明显正面偏差。

Conclusion: LLM若成为重要文化工具，需确保对全球人权和公民权利的准确表征，避免偏差影响公众认知。

Abstract: As Large Language Models (LLMs) increasingly mediate global information
access for millions of users worldwide, their alignment and biases have the
potential to shape public understanding and trust in fundamental democratic
institutions, such as press freedom. In this study, we uncover three systematic
distortions in the way six popular LLMs evaluate press freedom in 180 countries
compared to expert assessments of the World Press Freedom Index (WPFI). The six
LLMs exhibit a negative misalignment, consistently underestimating press
freedom, with individual models rating between 71% to 93% of countries as less
free. We also identify a paradoxical pattern we term differential misalignment:
LLMs disproportionately underestimate press freedom in countries where it is
strongest. Additionally, five of the six LLMs exhibit positive home bias,
rating their home countries' press freedoms more favorably than would be
expected given their negative misalignment with the human benchmark. In some
cases, LLMs rate their home countries between 7% to 260% more positively than
expected. If LLMs are set to become the next search engines and some of the
most important cultural tools of our time, they must ensure accurate
representations of the state of our human and civic rights globally.

</details>


### [286] [Automatic Large Language Models Creation of Interactive Learning Lessons](https://arxiv.org/abs/2506.17356)
*Jionghao Lin,Jiarui Rao,Yiyang Zhao,Yuting Wang,Ashish Gurung,Amanda Barany,Jaclyn Ocumpaugh,Ryan S. Baker,Kenneth R. Koedinger*

Key words: 自动课程生成, 任务分解, GPT-4o, 检索增强生成, 辅导培训

TL;DR: 论文探讨了利用基于任务的提示工程和GPT-4o自动生成交互式辅导课程的方法，结果证明分步策略优于单步生成。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 研究目的是通过AI帮助培养新手数学辅导老师，优化在线教学培训的效率和质量。

Method: 采用了基于任务分解的提示工程和检索增强生成技术（GPT-4o），生成三个主题的课程。

Result: 分步生成的课程在人类评估中得分更高，但存在通用反馈和部分内容不够清晰的问题。

Conclusion: 混合人机方法在生成有效辅导课程方面具有潜力。

Abstract: We explore the automatic generation of interactive, scenario-based lessons
designed to train novice human tutors who teach middle school mathematics
online. Employing prompt engineering through a Retrieval-Augmented Generation
approach with GPT-4o, we developed a system capable of creating structured
tutor training lessons. Our study generated lessons in English for three key
topics: Encouraging Students' Independence, Encouraging Help-Seeking Behavior,
and Turning on Cameras, using a task decomposition prompting strategy that
breaks lesson generation into sub-tasks. The generated lessons were evaluated
by two human evaluators, who provided both quantitative and qualitative
evaluations using a comprehensive rubric informed by lesson design research.
Results demonstrate that the task decomposition strategy led to higher-rated
lessons compared to single-step generation. Human evaluators identified several
strengths in the LLM-generated lessons, including well-structured content and
time-saving potential, while also noting limitations such as generic feedback
and a lack of clarity in some instructional sections. These findings underscore
the potential of hybrid human-AI approaches for generating effective lessons in
tutor training.

</details>


### [287] [A Large-Scale Real-World Evaluation of LLM-Based Virtual Teaching Assistant](https://arxiv.org/abs/2506.17363)
*Sunjun Kweon,Sooyohn Nam,Hyunseung Lim,Hwajung Hong,Edward Choi*

Key words: 虚拟教学助手, 大语言模型, 实证研究, 学生交互, AI教育

TL;DR: 本研究开发了一个基于LLM的虚拟教学助手（VTA），并在477名研究生的AI编程课程中部署，通过调查和交互分析评估其效果与接受度。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 现有研究对虚拟教学助手在真实课堂中的效果和接受度缺乏实证分析，其实际影响尚不明确。

Method: 在课程中部署LLM-based VTA，进行三轮学生调查，分析3,869次学生与VTA的交互，并与传统教师交互对比。

Result: 研究发现VTA在多轮交互中表现潜力，但需解决关键挑战以促进广泛采用。

Conclusion: VTA在真实课堂中具有可行性，但需进一步优化和解决挑战。开源代码推动AI教育发展。

Abstract: Virtual Teaching Assistants (VTAs) powered by Large Language Models (LLMs)
have the potential to enhance student learning by providing instant feedback
and facilitating multi-turn interactions. However, empirical studies on their
effectiveness and acceptance in real-world classrooms are limited, leaving
their practical impact uncertain. In this study, we develop an LLM-based VTA
and deploy it in an introductory AI programming course with 477 graduate
students. To assess how student perceptions of the VTA's performance evolve
over time, we conduct three rounds of comprehensive surveys at different stages
of the course. Additionally, we analyze 3,869 student--VTA interaction pairs to
identify common question types and engagement patterns. We then compare these
interactions with traditional student--human instructor interactions to
evaluate the VTA's role in the learning process. Through a large-scale
empirical study and interaction analysis, we assess the feasibility of
deploying VTAs in real-world classrooms and identify key challenges for broader
adoption. Finally, we release the source code of our VTA system, fostering
future advancements in AI-driven education:
\texttt{https://github.com/sean0042/VTA}.

</details>


### [288] [AI-based Multimodal Biometrics for Detecting Smartphone Distractions: Application to Online Learning](https://arxiv.org/abs/2506.17364)
*Alvaro Becerra,Roberto Daza,Ruth Cobos,Aythami Morales,Mutlu Cukurova,Julian Fierrez*

Key words: 多模态生物特征, 在线学习, 注意力分散, AI检测, 生理信号

TL;DR: 研究利用多模态生物特征检测在线学习中因手机使用导致的注意力分散，提出基于AI的方法，结合生理信号和头部姿态数据，多模态模型准确率达91%。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 在线学习中，学习者面临内部、系统和上下文因素的干扰，传统平台缺乏详细行为数据，多模态学习分析和生物传感器为新视角。

Method: 提出基于AI的方法，结合生理信号（如脑波、心率）和头部姿态数据，构建多模态模型检测手机使用。

Result: 单一生物信号（如脑波或心率）准确率有限，头部姿态单独达87%，多模态模型整合所有信号后准确率达91%。

Conclusion: 多模态整合显著提升检测准确率，但实际应用中需考虑模型部署的限制和影响。

Abstract: This work investigates the use of multimodal biometrics to detect
distractions caused by smartphone use during tasks that require sustained
attention, with a focus on computer-based online learning. Although the methods
are applicable to various domains, such as autonomous driving, we concentrate
on the challenges learners face in maintaining engagement amid internal (e.g.,
motivation), system-related (e.g., course design) and contextual (e.g.,
smartphone use) factors. Traditional learning platforms often lack detailed
behavioral data, but Multimodal Learning Analytics (MMLA) and biosensors
provide new insights into learner attention. We propose an AI-based approach
that leverages physiological signals and head pose data to detect phone use.
Our results show that single biometric signals, such as brain waves or heart
rate, offer limited accuracy, while head pose alone achieves 87%. A multimodal
model combining all signals reaches 91% accuracy, highlighting the benefits of
integration. We conclude by discussing the implications and limitations of
deploying these models for real-time support in online learning environments.

</details>


### [289] [AI based Content Creation and Product Recommendation Applications in E-commerce: An Ethical overview](https://arxiv.org/abs/2506.17370)
*Aditi Madhusudan Jain,Ayush Jain*

Key words: AI, 电子商务, 伦理挑战, 算法偏见, 数据隐私

TL;DR: 论文探讨了AI在电子商务内容创作和产品推荐中的伦理挑战，提出了消除偏见和确保公平性的最佳实践。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 随着AI在电商中的广泛应用，其带来的伦理问题（如数据隐私、算法偏见）亟需解决，以保障公平和透明。

Method: 通过定期算法审查、多样化训练数据和引入公平性指标，提出减少偏见的实践方法。

Result: 提出了一套确保公平、透明和隐私保护的伦理框架和最佳实践。

Conclusion: AI在电商中的应用需兼顾效率和伦理，确保技术应用的公平性和消费者自主权。

Abstract: As e-commerce rapidly integrates artificial intelligence for content creation
and product recommendations, these technologies offer significant benefits in
personalization and efficiency. AI-driven systems automate product
descriptions, generate dynamic advertisements, and deliver tailored
recommendations based on consumer behavior, as seen in major platforms like
Amazon and Shopify. However, the widespread use of AI in e-commerce raises
crucial ethical challenges, particularly around data privacy, algorithmic bias,
and consumer autonomy. Bias -- whether cultural, gender-based, or socioeconomic
-- can be inadvertently embedded in AI models, leading to inequitable product
recommendations and reinforcing harmful stereotypes. This paper examines the
ethical implications of AI-driven content creation and product recommendations,
emphasizing the need for frameworks to ensure fairness, transparency, and need
for more established and robust ethical standards. We propose actionable best
practices to remove bias and ensure inclusivity, such as conducting regular
audits of algorithms, diversifying training data, and incorporating fairness
metrics into AI models. Additionally, we discuss frameworks for ethical
conformance that focus on safeguarding consumer data privacy, promoting
transparency in decision-making processes, and enhancing consumer autonomy. By
addressing these issues, we provide guidelines for responsibly utilizing AI in
e-commerce applications for content creation and product recommendations,
ensuring that these technologies are both effective and ethically sound.

</details>


### [290] [Multimodal Political Bias Identification and Neutralization](https://arxiv.org/abs/2506.17372)
*Cedric Bernard,Xavier Pleimling,Amun Kharel,Chase Vickery*

Key words: 政治偏见、文本去偏见、图像偏见、CLIP模型、ViT分类器

TL;DR: 该论文提出了一种结合文本和图像偏见的模型，通过四个步骤检测并中和政治文章中的偏见，结果表明该方法具有潜力，但仍需更多资源优化。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 政治回音室的存在使得检测和消除政治文章中的主观偏见和情感化语言变得迫切，但以往研究仅关注文本部分，忽略了图像的偏见问题。

Method: 通过四个步骤实现：图像文本对齐（CLIP模型）、图像偏见评分（ViT分类器）、文本去偏见（BERT模型）、以及最终的去偏见替换（比较图像偏见分数）。

Result: 文本去偏见策略能识别大量潜在偏见词句，ViT模型训练有效，语义对齐模型高效，但需更多资源和时间进一步优化。

Conclusion: 该方法在去偏见方面表现良好，但需更多实验和人类评估以确保语义一致性。

Abstract: Due to the presence of political echo chambers, it becomes imperative to
detect and remove subjective bias and emotionally charged language from both
the text and images of political articles. However, prior work has focused on
solely the text portion of the bias rather than both the text and image
portions. This is a problem because the images are just as powerful of a medium
to communicate information as text is. To that end, we present a model that
leverages both text and image bias which consists of four different steps.
Image Text Alignment focuses on semantically aligning images based on their
bias through CLIP models. Image Bias Scoring determines the appropriate bias
score of images via a ViT classifier. Text De-Biasing focuses on detecting
biased words and phrases and neutralizing them through BERT models. These three
steps all culminate to the final step of debiasing, which replaces the text and
the image with neutralized or reduced counterparts, which for images is done by
comparing the bias scores. The results so far indicate that this approach is
promising, with the text debiasing strategy being able to identify many
potential biased words and phrases, and the ViT model showcasing effective
training. The semantic alignment model also is efficient. However, more time,
particularly in training, and resources are needed to obtain better results. A
human evaluation portion was also proposed to ensure semantic consistency of
the newly generated text and images.

</details>


### [291] [Optimizing Mastery Learning by Fast-Forwarding Over-Practice Steps](https://arxiv.org/abs/2506.17577)
*Meng Xia,Robin Schmucker,Conrad Borchers,Vincent Aleven*

Key words: 精通学习，辅导系统，过度练习，Fast-Forwarding，问题选择算法

TL;DR: Fast-Forwarding技术通过步骤级适应性减少过度练习，提高学习效率，模拟研究显示可减少约三分之一的过度练习时间。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 精通学习虽提升学习效率，但过度练习（学生花时间在已掌握的技能上）仍是辅导系统的核心挑战。

Method: 提出Fast-Forwarding技术，结合学习者模型和问题解决路径的模拟研究，优化现有问题选择算法。

Result: Fast-Forwarding可减少高达三分之一的过度练习时间，尤其适用于偏好选择难题的算法。

Conclusion: Fast-Forwarding能提升练习效率，其实用效果还取决于学生对高难度问题的持续动机和参与度。

Abstract: Mastery learning improves learning proficiency and efficiency. However, the
overpractice of skills--students spending time on skills they have already
mastered--remains a fundamental challenge for tutoring systems. Previous
research has reduced overpractice through the development of better problem
selection algorithms and the authoring of focused practice tasks. However, few
efforts have concentrated on reducing overpractice through step-level
adaptivity, which can avoid resource-intensive curriculum redesign. We propose
and evaluate Fast-Forwarding as a technique that enhances existing problem
selection algorithms. Based on simulation studies informed by learner models
and problem-solving pathways derived from real student data, Fast-Forwarding
can reduce overpractice by up to one-third, as it does not require students to
complete problem-solving steps if all remaining pathways are fully mastered.
Fast-Forwarding is a flexible method that enhances any problem selection
algorithm, though its effectiveness is highest for algorithms that
preferentially select difficult problems. Therefore, our findings suggest that
while Fast-Forwarding may improve student practice efficiency, the size of its
practical impact may also depend on students' ability to stay motivated and
engaged at higher levels of difficulty.

</details>


### [292] [Using Machine Learning in Analyzing Air Quality Discrepancies of Environmental Impact](https://arxiv.org/abs/2506.17319)
*Shuangbao Paul Wang,Lucas Yang,Rahouane Chouchane,Jin Guo,Michael Bailey*

Key words: 机器学习、软件工程、空气污染、社会不平等、历史政策歧视

TL;DR: 机器学习与软件工程用于分析巴尔的摩空气污染水平，发现历史保险评估偏差与污染水平显著相关，揭示了种族与收入歧视的长期影响。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 研究旨在揭示历史政策（如保险风险评估偏差）如何影响巴尔的摩当前的空气污染水平及其对不同族裔和收入群体的不平等影响。

Method: 结合机器学习与软件工程技术，分析三种数据源：保险风险评估偏差、居民人口统计数据及NO2/PM2.5污染浓度。数据集覆盖全美202个主要城市的650,643名巴尔的摩居民。

Result: 空气污染水平与历史保险评估偏差显著相关；低收入社区与少数族裔聚居区的NO2/PM2.5污染水平显著更高，揭示了长期政策歧视对生活质量的影响。

Conclusion: 研究揭示了巴尔的摩空气污染问题与历史政策歧视的关联，呼吁政策调整以减少环境不公。

Abstract: In this study, we apply machine learning and software engineering in
analyzing air pollution levels in City of Baltimore. The data model was fed
with three primary data sources: 1) a biased method of estimating insurance
risk used by homeowners loan corporation, 2) demographics of Baltimore
residents, and 3) census data estimate of NO2 and PM2.5 concentrations. The
dataset covers 650,643 Baltimore residents in 44.7 million residents in 202
major cities in US. The results show that air pollution levels have a clear
association with the biased insurance estimating method. Great disparities
present in NO2 level between more desirable and low income blocks. Similar
disparities exist in air pollution level between residents' ethnicity. As
Baltimore population consists of a greater proportion of people of color, the
finding reveals how decades old policies has continued to discriminate and
affect quality of life of Baltimore citizens today.

</details>


### [293] [MAARTA:Multi-Agentic Adaptive Radiology Teaching Assistant](https://arxiv.org/abs/2506.17320)
*Akash Awasthi,Brandon V. Chang,Anh M. Vu,Ngan Le,Rishi Agrawal,Zhigang Deng,Carol Wu,Hien Van Nguyen*

Key words: 放射学教育, 多代理框架, 视觉搜索, 感知错误, 个性化反馈

TL;DR: MAARTA 是一个多代理框架，通过分析学生的注视模式和放射学报告，提供个性化反馈，帮助学生改进视觉搜索和诊断解释中的感知错误。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 放射科学生在缺乏专家指导时容易出现视觉搜索和诊断解释的感知错误，现有 AI 系统未能有效解决此类问题。

Method: MAARTA 采用多代理框架，动态选择代理分析错误，并通过结构化图表比较专家和学生的注视行为，识别错误并提供逐步指导。

Result: MAARTA 能够识别学生的感知错误，并通过个性化反馈帮助学生改进诊断推理。

Conclusion: MAARTA 为 AI 驱动的放射学教育提供了新的解决方案，帮助学生弥补感知错误的不足。

Abstract: Radiology students often struggle to develop perceptual expertise due to
limited expert mentorship time, leading to errors in visual search and
diagnostic interpretation. These perceptual errors, such as missed fixations,
short dwell times, or misinterpretations, are not adequately addressed by
current AI systems, which focus on diagnostic accuracy but fail to explain how
and why errors occur. To address this gap, we introduce MAARTA (Multi-Agentic
Adaptive Radiology Teaching Assistant), a multi-agent framework that analyzes
gaze patterns and radiology reports to provide personalized feedback. Unlike
single-agent models, MAARTA dynamically selects agents based on error
complexity, enabling adaptive and efficient reasoning. By comparing expert and
student gaze behavior through structured graphs, the system identifies missed
findings and assigns Perceptual Error Teacher agents to analyze discrepancies.
MAARTA then uses step-by-step prompting to help students understand their
errors and improve diagnostic reasoning, advancing AI-driven radiology
education.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [294] [New Hardness Results for Low-Rank Matrix Completion](https://arxiv.org/abs/2506.18440)
*Dror Chawin,Ishay Haviv*

Key words: 低秩矩阵补全, NP困难性, 正半定性, 有界无穷范数

TL;DR: 该论文证明了低秩矩阵补全问题在某些约束条件下的NP困难性，包括正半定性和有界无穷范数，扩展了现有结果的适用范围。

<details>
  <summary>Details</summary>

Main category: cs.CC

Motivation: 研究低秩矩阵补全问题在机器学习和统计学等领域有广泛应用，但现有理论结果的限制较多，需要进一步扩展。

Method: 通过新型的几乎正交图表示、线有向图概念以及扰动单位矩阵的秩界分析，建立了更一般的NP困难性结果。

Result: 证明了即使允许秩超过原秩的乘法因子，低秩矩阵补全问题仍为NP困难，且适用于更广泛的误差范围。

Conclusion: 论文扩展了低秩矩阵补全问题的NP困难性结果，为相关问题提供了更全面的理论支持。

Abstract: The low-rank matrix completion problem asks whether a given real matrix with
missing values can be completed so that the resulting matrix has low rank or is
close to a low-rank matrix. The completed matrix is often required to satisfy
additional structural constraints, such as positive semi-definiteness or a
bounded infinity norm. The problem arises in various research fields, including
machine learning, statistics, and theoretical computer science, and has broad
real-world applications.
  This paper presents new $\mathsf{NP} $-hardness results for low-rank matrix
completion problems. We show that for every sufficiently large integer $d$ and
any real number $\varepsilon \in [ 2^{-O(d)},\frac{1}{7}]$, given a partial
matrix $A$ with exposed values of magnitude at most $1$ that admits a positive
semi-definite completion of rank $d$, it is $\mathsf{NP}$-hard to find a
positive semi-definite matrix that agrees with each given value of $A$ up to an
additive error of at most $\varepsilon$, even when the rank is allowed to
exceed $d$ by a multiplicative factor of $O (\frac{1}{\varepsilon ^2 \cdot
\log(1/\varepsilon)} )$. This strengthens a result of Hardt, Meka, Raghavendra,
and Weitz (COLT, 2014), which applies to multiplicative factors smaller than
$2$ and to $\varepsilon $ that decays polynomially in $d$. We establish similar
$\mathsf{NP}$-hardness results for the case where the completed matrix is
constrained to have a bounded infinity norm (rather than be positive
semi-definite), for which all previous hardness results rely on complexity
assumptions related to the Unique Games Conjecture. Our proofs involve a novel
notion of nearly orthonormal representations of graphs, the concept of line
digraphs, and bounds on the rank of perturbed identity matrices.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [295] [Can Common VLMs Rival Medical VLMs? Evaluation and Strategic Insights](https://arxiv.org/abs/2506.17337)
*Yuan Zhong,Ruinan Jin,Xiaoxiao Li,Qi Dou*

Key words: 视觉语言模型,医学成像,微调,CLIP,LLaVA

TL;DR: 研究评估通用和医学视觉语言模型在医学成像任务中的表现，发现轻量级微调的通用模型可媲美或超越医学专用模型。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 探索轻量级微调的通用视觉语言模型是否能替代资源密集型的医学专用模型，为医学成像领域提供高效解决方案。

Method: 使用CLIP和LLaVA模型，对比它们在域内、域外任务下的表现，并分析微调效果。

Result: 通用模型经微调后在域内任务中表现优异，域外任务中也展现强适应性，挑战医学预训练的必要性。

Conclusion: 通用模型结合微调是医学成像领域的可扩展、低成本替代方案。

Abstract: Medical vision-language models (VLMs) leverage large-scale pretraining for
diverse imaging tasks but require substantial computational and data resources.
Meanwhile, common or general-purpose VLMs (e.g., CLIP, LLaVA), though not
trained for medical use, show promise with fine-tuning. This raises a key
question: Can efficient fine-tuned common VLMs rival generalist medical VLMs
for solving specific medical imaging tasks? This study systematically evaluates
common and medical VLMs across disease diagnosis and visual question answering
(VQA). Using CLIP-based and LLaVA-based models, we examine (1) off-the-shelf
performance gaps in in-domain (ID) settings, (2) whether fine-tuning bridges
these gaps, and (3) generalization to out-of-domain (OOD) tasks on unseen
medical modalities. While medical-specific pretraining provides advantages in
ID settings, common VLMs match or surpass medical-specific models after
lightweight fine-tuning, with LoRA-based adaptation proving highly effective
among different tasks. In OOD tasks, common VLMs demonstrate strong
adaptability in some tasks, challenging the assumption that medical-specific
pre-training is essential. These findings suggest that leveraging common VLMs
with fine-tuning offers a scalable and cost-effective alternative to developing
large-scale medical VLMs, providing crucial insights for future research in the
medical imaging field.

</details>


### [296] [MTSIC: Multi-stage Transformer-based GAN for Spectral Infrared Image Colorization](https://arxiv.org/abs/2506.17540)
*Tingting Liu,Yuan Liu,Jinhui Tang,Liyin Yuan,Chengyu Liu,Chunlai Li,Xiubao Sui,Qian Chen*

Key words: 红外图像彩色化, GAN, Transformer, 多波段光谱, 自注意力机制

TL;DR: 提出一种基于GAN的多波段红外图像彩色化方法，通过MTSIC框架整合光谱信息以提升图像质量。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: TIR图像缺乏颜色和纹理信息，现有方法依赖于单波段图像，导致图像失真和语义模糊。多波段红外图像提供更丰富的光谱数据，有助于提升彩色化效果。

Method: 使用MTSIC框架，结合Transformer网络和SARB模块，通过多级自注意力机制实现多波段特征映射，减少语义混淆。

Result: 实验表明该方法显著优于传统技术，有效提升了红外图像的视觉质量。

Conclusion: MTSIC框架通过整合多波段光谱信息，解决了红外图像彩色化中的语义模糊问题，显著提升了图像质量。

Abstract: Thermal infrared (TIR) images, acquired through thermal radiation imaging,
are unaffected by variations in lighting conditions and atmospheric haze.
However, TIR images inherently lack color and texture information, limiting
downstream tasks and potentially causing visual fatigue. Existing colorization
methods primarily rely on single-band images with limited spectral information
and insufficient feature extraction capabilities, which often result in image
distortion and semantic ambiguity. In contrast, multiband infrared imagery
provides richer spectral data, facilitating the preservation of finer details
and enhancing semantic accuracy. In this paper, we propose a generative
adversarial network (GAN)-based framework designed to integrate spectral
information to enhance the colorization of infrared images. The framework
employs a multi-stage spectral self-attention Transformer network (MTSIC) as
the generator. Each spectral feature is treated as a token for self-attention
computation, and a multi-head self-attention mechanism forms a spatial-spectral
attention residual block (SARB), achieving multi-band feature mapping and
reducing semantic confusion. Multiple SARB units are integrated into a
Transformer-based single-stage network (STformer), which uses a U-shaped
architecture to extract contextual information, combined with multi-scale
wavelet blocks (MSWB) to align semantic information in the spatial-frequency
dual domain. Multiple STformer modules are cascaded to form MTSIC,
progressively optimizing the reconstruction quality. Experimental results
demonstrate that the proposed method significantly outperforms traditional
techniques and effectively enhances the visual quality of infrared images.

</details>


### [297] [Multimodal Medical Image Binding via Shared Text Embeddings](https://arxiv.org/abs/2506.18072)
*Yunhao Liu,Suyang Xi,Shiqi Liu,Hong Ding,Chicheng Jin,Chenxi Yang,Junjun He,Yiqing Shen*

Key words: 医学图像分析、多模态对齐、预训练、零样本学习、跨模态检索

TL;DR: 提出了一种名为M³Bind的新型预训练框架，通过共享文本表示空间实现多种医学成像模态的无缝对齐，无需显式配对的医学图像数据。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 医学图像分析需要整合多种成像模态以获取互补信息，但现有方法（如CLIP）需要显式配对数据，这在医学场景中难以获取。

Method: M³Bind利用预训练的CLIP-like模型，首先微调以对齐模态特定的文本嵌入空间，然后将其蒸馏为一个统一模型，创建共享文本嵌入空间。

Result: 在X光、CT、视网膜图像等多种下游任务中，M³Bind在零样本、少样本分类和跨模态检索任务中表现优异。

Conclusion: M³Bind通过共享文本表示空间有效实现了医学图像的跨模态对齐。

Abstract: Medical image analysis increasingly relies on the integration of multiple
imaging modalities to capture complementary anatomical and functional
information, enabling more accurate diagnosis and treatment planning. Achieving
aligned feature representations across these diverse modalities is therefore
important for effective multimodal analysis. While contrastive language-image
pre-training (CLIP) and its variant have enabled image-text alignments, they
require explicitly paired data between arbitrary two modalities, which is
difficult to acquire in medical contexts. To address the gap, we present
Multimodal Medical Image Binding with Text (M\textsuperscript{3}Bind), a novel
pre-training framework that enables seamless alignment of multiple medical
imaging modalities through a shared text representation space without requiring
explicit paired data between any two medical image modalities. Specifically,
based on the insight that different images can naturally bind with text,
M\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text
models to align their modality-specific text embedding space while preserving
their original image-text alignments. Subsequently, we distill these
modality-specific text encoders into a unified model, creating a shared text
embedding space. Experiments on X-ray, CT, retina, ECG, and pathological images
on multiple downstream tasks demonstrate that M\textsuperscript{3}Bind achieves
state-of-the-art performance in zero-shot, few-shot classification and
cross-modal retrieval tasks compared to its CLIP-like counterparts. These
results validate M\textsuperscript{3}Bind's effectiveness in achieving
cross-image-modal alignment for medical analysis.

</details>


### [298] [CT Radiomics-Based Explainable Machine Learning Model for Accurate Differentiation of Malignant and Benign Endometrial Tumors: A Two-Center Study](https://arxiv.org/abs/2506.18106)
*Tingrui Zhang,Honglin Wu,Zekun Jiang,Yingying Wang,Rui Ye,Huiming Ni,Chang Liu,Jin Cao,Xuan Sun,Rong Shao,Xiaorong Wei,Yingchun Sun*

Key words: CT影像组学, 可解释机器学习, 子宫内膜癌, 随机森林, SHAP分析

TL;DR: 开发并验证了一种基于CT影像组学的可解释机器学习模型，用于诊断子宫内膜癌（EC）的恶性和良性。模型在训练集和测试集上表现出色，随机森林模型最优，AUC分别为1.00和0.96。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 旨在通过可解释的机器学习模型提高子宫内膜癌的诊断准确性和临床实用性，减少不必要的干预。

Method: 从83名EC患者的CT扫描中提取1132个影像组学特征，使用六种机器学习算法建模，优化管道并通过多种指标评估性能。SHAP分析和特征映射用于增强可解释性。

Result: 随机森林模型表现最佳，测试AUC为0.96。SHAP分析显示所选特征均显著关联EC（P < 0.05），DCA表明模型具有较高的临床净效益。

Conclusion: 该模型实现了高诊断性能，可作为EC的智能辅助诊断工具。

Abstract: Aimed to develop and validate a CT radiomics-based explainable machine
learning model for diagnosing malignancy and benignity specifically in
endometrial cancer (EC) patients. A total of 83 EC patients from two centers,
including 46 with malignant and 37 with benign conditions, were included, with
data split into a training set (n=59) and a testing set (n=24). The regions of
interest (ROIs) were manually segmented from pre-surgical CT scans, and 1132
radiomic features were extracted from the pre-surgical CT scans using
Pyradiomics. Six explainable machine learning modeling algorithms were
implemented respectively, for determining the optimal radiomics pipeline. The
diagnostic performance of the radiomic model was evaluated by using
sensitivity, specificity, accuracy, precision, F1 score, confusion matrices,
and ROC curves. To enhance clinical understanding and usability, we separately
implemented SHAP analysis and feature mapping visualization, and evaluated the
calibration curve and decision curve. By comparing six modeling strategies, the
Random Forest model emerged as the optimal choice for diagnosing EC, with a
training AUC of 1.00 and a testing AUC of 0.96. SHAP identified the most
important radiomic features, revealing that all selected features were
significantly associated with EC (P < 0.05). Radiomics feature maps also
provide a feasible assessment tool for clinical applications. DCA indicated a
higher net benefit for our model compared to the "All" and "None" strategies,
suggesting its clinical utility in identifying high-risk cases and reducing
unnecessary interventions. In conclusion, the CT radiomics-based explainable
machine learning model achieved high diagnostic performance, which could be
used as an intelligent auxiliary tool for the diagnosis of endometrial cancer.

</details>


### [299] [A Deep Convolutional Neural Network-Based Novel Class Balancing for Imbalance Data Segmentation](https://arxiv.org/abs/2506.18474)
*Atifa Kalsoom,M. A. Iftikhar,Amjad Ali,Zubair Shah,Shidin Balakrishnan,Hazrat Ali*

Key words: 视网膜血管分割, CNN, 类平衡, 预处理, 深度学习

TL;DR: 提出一种基于深度学习和双层类平衡方案的BLCB-CNN方法，用于视网膜血管分割，通过预处理和平衡策略提升性能。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 视网膜血管分割因数据分布不均和血管厚度变化而具有挑战性，需新方法解决这些问题。

Method: 使用CNN架构和双层类平衡方案（Level-I平衡血管/非血管像素，Level-II平衡厚/薄血管），并结合GCN、CLAHE和gamma校正预处理。

Result: 在标准数据集上表现卓越（AUC 98.23%，准确率96.22%），并通过STARE图像验证了泛化能力。

Conclusion: BLCB-CNN方法在视网膜血管分割上高效且泛化能力强。

Abstract: Retinal fundus images provide valuable insights into the human eye's interior
structure and crucial features, such as blood vessels, optic disk, macula, and
fovea. However, accurate segmentation of retinal blood vessels can be
challenging due to imbalanced data distribution and varying vessel thickness.
In this paper, we propose BLCB-CNN, a novel pipeline based on deep learning and
bi-level class balancing scheme to achieve vessel segmentation in retinal
fundus images. The BLCB-CNN scheme uses a Convolutional Neural Network (CNN)
architecture and an empirical approach to balance the distribution of pixels
across vessel and non-vessel classes and within thin and thick vessels. Level-I
is used for vessel/non-vessel balancing and Level-II is used for thick/thin
vessel balancing. Additionally, pre-processing of the input retinal fundus
image is performed by Global Contrast Normalization (GCN), Contrast Limited
Adaptive Histogram Equalization (CLAHE), and gamma corrections to increase
intensity uniformity as well as to enhance the contrast between vessels and
background pixels. The resulting balanced dataset is used for
classification-based segmentation of the retinal vascular tree. We evaluate the
proposed scheme on standard retinal fundus images and achieve superior
performance measures, including an area under the ROC curve of 98.23%, Accuracy
of 96.22%, Sensitivity of 81.57%, and Specificity of 97.65%. We also
demonstrate the method's efficacy through external cross-validation on STARE
images, confirming its generalization ability.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [300] [Heterogeneous Temporal Hypergraph Neural Network](https://arxiv.org/abs/2506.17312)
*Huan Liu,Pengfei Jiao,Mengzhou Gao,Chaochao Chen,Di Jin*

Key words: 图表示学习、异构时序图、超图神经网络、高阶交互、对比学习

TL;DR: 论文提出了一种异构时序超图神经网络（HTHGN），用于捕捉复杂异构时序图中的高阶交互关系，解决了现有方法在低阶拓扑信息和高阶交互关系上的局限性。

<details>
  <summary>Details</summary>

Main category: cs.SI

Motivation: 现有图表示学习方法主要关注低阶拓扑信息，忽略了高阶群体交互关系，而现有超图方法仅适用于静态同质图，无法建模时序异构图中的高阶交互。

Method: 提出了异构时序超图的正式定义和无需额外信息的$P$-均匀超边构造算法，并设计了一种分层注意力机制和对比学习的HTHGN模型。

Result: 在三个实际数据集上的实验验证了HTHGN在建模高阶交互关系上的有效性，性能显著提升。

Conclusion: HTHGN能够有效建模时序异构图中的高阶交互关系，弥补了现有方法的不足。

Abstract: Graph representation learning (GRL) has emerged as an effective technique for
modeling graph-structured data. When modeling heterogeneity and dynamics in
real-world complex networks, GRL methods designed for complex heterogeneous
temporal graphs (HTGs) have been proposed and have achieved successful
applications in various fields. However, most existing GRL methods mainly focus
on preserving the low-order topology information while ignoring higher-order
group interaction relationships, which are more consistent with real-world
networks. In addition, most existing hypergraph methods can only model static
homogeneous graphs, limiting their ability to model high-order interactions in
HTGs. Therefore, to simultaneously enable the GRL model to capture high-order
interaction relationships in HTGs, we first propose a formal definition of
heterogeneous temporal hypergraphs and $P$-uniform heterogeneous hyperedge
construction algorithm that does not rely on additional information. Then, a
novel Heterogeneous Temporal HyperGraph Neural network (HTHGN), is proposed to
fully capture higher-order interactions in HTGs. HTHGN contains a hierarchical
attention mechanism module that simultaneously performs temporal
message-passing between heterogeneous nodes and hyperedges to capture rich
semantics in a wider receptive field brought by hyperedges. Furthermore, HTHGN
performs contrastive learning by maximizing the consistency between low-order
correlated heterogeneous node pairs on HTG to avoid the low-order structural
ambiguity issue. Detailed experimental results on three real-world HTG datasets
verify the effectiveness of the proposed HTHGN for modeling high-order
interactions in HTGs and demonstrate significant performance improvements.

</details>


### [301] [A family of graph GOSPA metrics for graphs with different sizes](https://arxiv.org/abs/2506.17316)
*Jinhao Gu,Ángel F. García-Fernández,Robert E. Firth,Lennart Svensson*

Key words: 图度量, GOSPA, 线性编程, 分类任务

TL;DR: 本文提出了一种用于测量不同大小图之间距离的图度量家族，扩展了图GOSPA度量，并证明了其满足度量性质。该家族通过线性编程近似计算，实验验证了其优势。

<details>
  <summary>Details</summary>

Main category: cs.SI

Motivation: 解决不同大小图之间距离测量的通用方法需求。

Method: 提出图GOSPA度量家族，定义通用形式并证明其度量性质；通过线性编程近似计算。

Result: 实验表明该家族在分类任务中有优势。

Conclusion: 图GOSPA度量家族提供更通用的边不匹配惩罚，适用于实际应用。

Abstract: This paper proposes a family of graph metrics for measuring distances between
graphs of different sizes. The proposed metric family defines a general form of
the graph generalised optimal sub-pattern assignment (GOSPA) metric and is also
proved to satisfy the metric properties. Similarly to the graph GOSPA metric,
the proposed graph GOSPA metric family also penalises the node attribute costs
for assigned nodes between the two graphs, and the number of unassigned nodes.
However, the proposed family of metrics provides more general penalties for
edge mismatches than the graph GOSPA metric. This paper also shows that the
graph GOSPA metric family can be approximately computed using linear
programming. Simulation experiments are performed to illustrate the
characteristics of the proposed graph GOSPA metric family with different
choices of hyperparameters. The benefits of the proposed graph GOSPA metric
family for classification tasks are also shown on real-world datasets.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [302] [OmniESI: A unified framework for enzyme-substrate interaction prediction with progressive conditional deep learning](https://arxiv.org/abs/2506.17963)
*Zhiwei Nie,Hongyu Zhang,Hao Jiang,Yutian Liu,Xiansong Huang,Fan Xu,Jie Fu,Zhixiang Ren,Yonghong Tian,Wen-Bin Zhang,Jie Chen*

Key words: 酶-底物相互作用, 条件深度学习, 催化机制, 酶工程

TL;DR: OmniESI是一个两阶段渐进框架，用于通过条件深度学习预测酶-底物相互作用，具有广泛的下游任务适应性和优异性能。

<details>
  <summary>Details</summary>

Main category: q-bio.BM

Motivation: 现有预测方法未能结合酶催化的先验知识，导致与催化模式不匹配的问题。

Method: 采用两阶段渐进条件深度学习框架，分阶段强调酶反应特异性和关键催化相关相互作用。

Result: 在多个基准测试中表现优于现有方法，且仅增加少量参数（0.16%）。

Conclusion: OmniESI为酶-底物相互作用提供了统一的预测工具，具有强泛化能力和广泛应用前景。

Abstract: Understanding and modeling enzyme-substrate interactions is crucial for
catalytic mechanism research, enzyme engineering, and metabolic engineering.
Although a large number of predictive methods have emerged, they do not
incorporate prior knowledge of enzyme catalysis to rationally modulate general
protein-molecule features that are misaligned with catalytic patterns. To
address this issue, we introduce a two-stage progressive framework, OmniESI,
for enzyme-substrate interaction prediction through conditional deep learning.
By decomposing the modeling of enzyme-substrate interactions into a two-stage
progressive process, OmniESI incorporates two conditional networks that
respectively emphasize enzymatic reaction specificity and crucial
catalysis-related interactions, facilitating a gradual feature modulation in
the latent space from general protein-molecule domain to catalysis-aware
domain. On top of this unified architecture, OmniESI can adapt to a variety of
downstream tasks, including enzyme kinetic parameter prediction,
enzyme-substrate pairing prediction, enzyme mutational effect prediction, and
enzymatic active site annotation. Under the multi-perspective performance
evaluation of in-distribution and out-of-distribution settings, OmniESI
consistently delivered superior performance than state-of-the-art specialized
methods across seven benchmarks. More importantly, the proposed conditional
networks were shown to internalize the fundamental patterns of catalytic
efficiency while significantly improving prediction performance, with only
negligible parameter increases (0.16%), as demonstrated by ablation studies on
key components. Overall, OmniESI represents a unified predictive approach for
enzyme-substrate interactions, providing an effective tool for catalytic
mechanism cracking and enzyme engineering with strong generalization and broad
applicability.

</details>


### [303] [AbRank: A Benchmark Dataset and Metric-Learning Framework for Antibody-Antigen Affinity Ranking](https://arxiv.org/abs/2506.17857)
*Chunan Liu,Aurelien Pelissier,Yanjun Shao,Lilian Denzler,Andrew C. R. Martin,Brooks Paige,Mariia Rodriguez Martinez*

Key words: 抗体-抗原结合亲和力, 机器学习基准, 排序问题, WALLE-Affinity

TL;DR: 摘要介绍了AbRank，一个用于抗体-抗原结合亲和力预测的大规模基准和评估框架，通过重新定义问题为成对排序任务，并整合多源实验数据来提升模型的泛化能力。

<details>
  <summary>Details</summary>

Main category: q-bio.BM

Motivation: 当前抗体-抗原结合亲和力预测模型的性能受限于噪声标记、异构实验条件和泛化能力不足，需要更稳健的基准和方法。

Method: AbRank通过标准化数据划分和m-confident排序框架，整合380,000多个实验数据，并提出基于图的WALLE-Affinity基线方法。

Result: 基准测试显示现有方法在真实泛化场景下表现有限，而排序训练能提升模型的稳健性和可转移性。

Conclusion: AbRank为抗体-抗原空间中的机器学习模型提供了稳健基础，对可扩展的结构感知抗体设计有直接价值。

Abstract: Accurate prediction of antibody-antigen (Ab-Ag) binding affinity is essential
for therapeutic design and vaccine development, yet the performance of current
models is limited by noisy experimental labels, heterogeneous assay conditions,
and poor generalization across the vast antibody and antigen sequence space. We
introduce AbRank, a large-scale benchmark and evaluation framework that
reframes affinity prediction as a pairwise ranking problem. AbRank aggregates
over 380,000 binding assays from nine heterogeneous sources, spanning diverse
antibodies, antigens, and experimental conditions, and introduces standardized
data splits that systematically increase distribution shift, from local
perturbations such as point mutations to broad generalization across novel
antigens and antibodies. To ensure robust supervision, AbRank defines an
m-confident ranking framework by filtering out comparisons with marginal
affinity differences, focusing training on pairs with at least an m-fold
difference in measured binding strength. As a baseline for the benchmark, we
introduce WALLE-Affinity, a graph-based approach that integrates protein
language model embeddings with structural information to predict pairwise
binding preferences. Our benchmarks reveal significant limitations in current
methods under realistic generalization settings and demonstrate that
ranking-based training improves robustness and transferability. In summary,
AbRank offers a robust foundation for machine learning models to generalize
across the antibody-antigen space, with direct relevance for scalable,
structure-aware antibody therapeutic design.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [304] [Step-by-Step Reasoning Attack: Revealing 'Erased' Knowledge in Large Language Models](https://arxiv.org/abs/2506.17279)
*Yash Sinha,Manit Baser,Murari Mandal,Dinil Mon Divakaran,Mohan Kankanhalli*

Key words: 知识遗忘、大型语言模型、逐步推理、黑盒攻击、信息泄漏

TL;DR: 该论文提出了一种基于逐步推理的黑盒攻击方法Sleek，暴露了大型语言模型（LLM）中知识遗忘技术的缺陷。通过三种核心组件，该方法成功恢复了被遗忘的知识，并揭示了现有遗忘技术的不完善之处。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 为了解决LLM中知识遗忘技术存在的隐患，如信息残留和知识不公平抑制问题，作者提出了一种新的攻击方法以测试和暴露这些漏洞。

Method: 作者设计了Sleek攻击框架，包含逐步推理生成的对抗性提示、攻击机制以及提示分类（直接、间接和隐含），通过这三种组件系统地测试遗忘技术的可靠性。

Result: 实验表明，现有遗忘技术在62.5%的对抗提示下恢复了被遗忘的《哈利波特》知识，并在50%的情况下揭示了知识的不公平抑制。

Conclusion: 该研究揭示了知识遗忘技术的信息泄漏风险，呼吁开发更可靠的遗忘策略。

Abstract: Knowledge erasure in large language models (LLMs) is important for ensuring
compliance with data and AI regulations, safeguarding user privacy, mitigating
bias, and misinformation. Existing unlearning methods aim to make the process
of knowledge erasure more efficient and effective by removing specific
knowledge while preserving overall model performance, especially for retained
information. However, it has been observed that the unlearning techniques tend
to suppress and leave the knowledge beneath the surface, thus making it
retrievable with the right prompts. In this work, we demonstrate that
\textit{step-by-step reasoning} can serve as a backdoor to recover this hidden
information. We introduce a step-by-step reasoning-based black-box attack,
Sleek, that systematically exposes unlearning failures. We employ a structured
attack framework with three core components: (1) an adversarial prompt
generation strategy leveraging step-by-step reasoning built from LLM-generated
queries, (2) an attack mechanism that successfully recalls erased content, and
exposes unfair suppression of knowledge intended for retention and (3) a
categorization of prompts as direct, indirect, and implied, to identify which
query types most effectively exploit unlearning weaknesses. Through extensive
evaluations on four state-of-the-art unlearning techniques and two widely used
LLMs, we show that existing approaches fail to ensure reliable knowledge
removal. Of the generated adversarial prompts, 62.5% successfully retrieved
forgotten Harry Potter facts from WHP-unlearned Llama, while 50% exposed unfair
suppression of retained knowledge. Our work highlights the persistent risks of
information leakage, emphasizing the need for more robust unlearning strategies
for erasure.

</details>


### [305] [Theoretically Unmasking Inference Attacks Against LDP-Protected Clients in Federated Vision Models](https://arxiv.org/abs/2506.17292)
*Quan Nguyen,Minh N. Vu,Truc Nguyen,My T. Thai*

Key words: 联邦学习, 会员推断攻击, 本地差分隐私, 隐私保护, 模型性能

TL;DR: 该论文探讨了联邦学习中的隐私保护问题，指出即使采用本地差分隐私（LDP），会员推断攻击（MIAs）仍可能成功，并提出了理论下限和实际验证。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 尽管联邦学习通过避免直接数据共享来保护隐私，但最近的会员推断攻击研究显示其隐私保护能力不足。本地差分隐私（LDP）虽被视为隐私保护的黄金标准，但针对LDP保护数据的攻击成功率缺乏理论保证。本文旨在填补这一空白。

Method: 论文推导了低多项式时间MIAs在利用全连接或自注意力层漏洞时的理论下限，并通过联邦视觉模型的实践验证了这些攻击的成功率。

Result: 研究表明，即使使用LDP保护数据，隐私风险仍存在，具体取决于隐私预算。实际评估显示，为减轻攻击所需的噪声会显著降低模型性能。

Conclusion: 本地差分隐私在联邦学习中无法完全消除会员推断攻击的风险，且噪声引入会严重影响模型实用性。

Abstract: Federated Learning enables collaborative learning among clients via a
coordinating server while avoiding direct data sharing, offering a perceived
solution to preserve privacy. However, recent studies on Membership Inference
Attacks (MIAs) have challenged this notion, showing high success rates against
unprotected training data. While local differential privacy (LDP) is widely
regarded as a gold standard for privacy protection in data analysis, most
studies on MIAs either neglect LDP or fail to provide theoretical guarantees
for attack success rates against LDP-protected data. To address this gap, we
derive theoretical lower bounds for the success rates of low-polynomial time
MIAs that exploit vulnerabilities in fully connected or self-attention layers.
We establish that even when data are protected by LDP, privacy risks persist,
depending on the privacy budget. Practical evaluations on federated vision
models confirm considerable privacy risks, revealing that the noise required to
mitigate these attacks significantly degrades models' utility.

</details>


### [306] [LLM Jailbreak Oracle](https://arxiv.org/abs/2506.17299)
*Shuyi Lin,Anshuman Suri,Alina Oprea,Cheng Tan*

Key words: 大语言模型、越狱攻击、安全评估、Boa算法、对抗条件

TL;DR: 本文提出了“越狱预言问题”，旨在评估大语言模型（LLMs）对越狱攻击的脆弱性，并提出了首个高效算法Boa来解决这一问题，为安全评估提供了严谨的方法。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 随着大语言模型在安全关键应用中的广泛应用，缺乏系统方法评估其对越狱攻击的脆弱性成为一个关键的安全缺口。

Method: 提出越狱预言问题形式化后，设计了Boa算法，采用三阶段搜索策略：构建块列表识别拒绝模式、广度优先采样发现易访问的越狱、深度优先优先级搜索探索低概率路径。

Result: Boa算法能够高效解决越狱预言问题，支持系统防御评估、标准化红队攻击比较以及在极端对抗条件下的模型认证。

Conclusion: Boa为评估大语言模型的越狱漏洞提供了系统且高效的解决方案，填补了当前安全评估的空白。

Abstract: As large language models (LLMs) become increasingly deployed in
safety-critical applications, the lack of systematic methods to assess their
vulnerability to jailbreak attacks presents a critical security gap. We
introduce the jailbreak oracle problem: given a model, prompt, and decoding
strategy, determine whether a jailbreak response can be generated with
likelihood exceeding a specified threshold. This formalization enables a
principled study of jailbreak vulnerabilities. Answering the jailbreak oracle
problem poses significant computational challenges -- the search space grows
exponentially with the length of the response tokens. We present Boa, the first
efficient algorithm for solving the jailbreak oracle problem. Boa employs a
three-phase search strategy: (1) constructing block lists to identify refusal
patterns, (2) breadth-first sampling to identify easily accessible jailbreaks,
and (3) depth-first priority search guided by fine-grained safety scores to
systematically explore promising low-probability paths. Boa enables rigorous
security assessments including systematic defense evaluation, standardized
comparison of red team attacks, and model certification under extreme
adversarial conditions.

</details>


### [307] [Context manipulation attacks : Web agents are susceptible to corrupted memory](https://arxiv.org/abs/2506.17318)
*Atharv Singh Patlan,Ashwin Hebbar,Pramod Viswanath,Prateek Mittal*

Key words: 自主代理、计划注入、上下文操纵、安全漏洞、隐私窃取

TL;DR: 本文介绍了一种新的上下文操纵攻击“计划注入”，针对自主网络导航代理的外部内存系统漏洞，显著提高了攻击成功率。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 由于大型语言模型的无状态性，网络导航代理依赖外部内存系统维护上下文，但客户端或第三方应用管理的内存存在安全漏洞，近期已对生产系统造成攻击。

Method: 通过系统评估两种流行的网络代理（Browser-use和Agent-E），验证了“计划注入”攻击绕过提示注入防御的效果，并开发了“上下文链式注入”来桥接用户目标和攻击者目标。

Result: 计划注入攻击的成功率比提示注入攻击高出3倍，而上下文链式注入在隐私窃取任务中成功率提高了17.7%。

Conclusion: 代理系统中必须优先考虑内存的安全处理。

Abstract: Autonomous web navigation agents, which translate natural language
instructions into sequences of browser actions, are increasingly deployed for
complex tasks across e-commerce, information retrieval, and content discovery.
Due to the stateless nature of large language models (LLMs), these agents rely
heavily on external memory systems to maintain context across interactions.
Unlike centralized systems where context is securely stored server-side, agent
memory is often managed client-side or by third-party applications, creating
significant security vulnerabilities. This was recently exploited to attack
production systems.
  We introduce and formalize "plan injection," a novel context manipulation
attack that corrupts these agents' internal task representations by targeting
this vulnerable context. Through systematic evaluation of two popular web
agents, Browser-use and Agent-E, we show that plan injections bypass robust
prompt injection defenses, achieving up to 3x higher attack success rates than
comparable prompt-based attacks. Furthermore, "context-chained injections,"
which craft logical bridges between legitimate user goals and attacker
objectives, lead to a 17.7% increase in success rate for privacy exfiltration
tasks. Our findings highlight that secure memory handling must be a first-class
concern in agentic systems.

</details>


### [308] [On the Performance of Cyber-Biomedical Features for Intrusion Detection in Healthcare 5.0](https://arxiv.org/abs/2506.17329)
*Pedro H. Lui,Lucas P. Siqueira,Juliano F. Kazienko,Vagner E. Quincozes,Silvio E. Quincozes,Daniel Welfer*

Key words: Healthcare 5.0, AI, XAI, cybersecurity, biomedical data

TL;DR: Healthcare 5.0结合AI、IoT和实时监测，但存在网络安全问题。本文应用可解释AI（XAI）分析医疗数据，提升模型有效性和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 当前AI驱动的网络安全模型忽视生物医学数据，影响效果和可解释性。研究填补这一空白。

Method: 应用XAI分析Healthcare 5.0数据集，结合网络流量和生物医学传感器数据。

Result: XGBoost模型在良性数据篡改中F1-score达99%，欺骗检测中81%。网络数据主导入侵检测，生物医学特征助力欺骗检测。

Conclusion: XAI能有效提升医疗5.0的网络安全，生物医学数据在特定场景中起关键作用。

Abstract: Healthcare 5.0 integrates Artificial Intelligence (AI), the Internet of
Things (IoT), real-time monitoring, and human-centered design toward
personalized medicine and predictive diagnostics. However, the increasing
reliance on interconnected medical technologies exposes them to cyber threats.
Meanwhile, current AI-driven cybersecurity models often neglect biomedical
data, limiting their effectiveness and interpretability. This study addresses
this gap by applying eXplainable AI (XAI) to a Healthcare 5.0 dataset that
integrates network traffic and biomedical sensor data. Classification outputs
indicate that XGBoost achieved 99% F1-score for benign and data alteration, and
81% for spoofing. Explainability findings reveal that network data play a
dominant role in intrusion detection whereas biomedical features contributed to
spoofing detection, with temperature reaching a Shapley values magnitude of
0.37.

</details>


### [309] [CUBA: Controlled Untargeted Backdoor Attack against Deep Neural Networks](https://arxiv.org/abs/2506.17350)
*Yinghao Wu,Liyan Zhang*

Key words: 后门攻击、无目标攻击、深度神经网络、安全威胁

TL;DR: 论文提出了一种新的约束无目标后门攻击方法（CUBA），结合了无目标攻击的灵活性和有目标攻击的意图性，有效绕过现有防御方法。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 现有的后门攻击多为有目标攻击，而无目标攻击因缺乏确定性被视为自我削弱。论文旨在通过约束无目标攻击的结合，提升攻击的灵活性和有效性。

Method: 提出CUBA方法，通过对交叉熵损失函数进行对数归一化处理，并在训练中约束对数，使模型在特定目标类别范围内随机分类。

Result: 实验显示CUBA在不同数据集上表现优异，有效规避现有防御方法。

Conclusion: CUBA成功结合无目标与有目标攻击的优点，为后门攻击提供新思路。

Abstract: Backdoor attacks have emerged as a critical security threat against deep
neural networks in recent years. The majority of existing backdoor attacks
focus on targeted backdoor attacks, where trigger is strongly associated to
specific malicious behavior. Various backdoor detection methods depend on this
inherent property and shows effective results in identifying and mitigating
such targeted attacks. However, a purely untargeted attack in backdoor
scenarios is, in some sense, self-weakening, since the target nature is what
makes backdoor attacks so powerful. In light of this, we introduce a novel
Constrained Untargeted Backdoor Attack (CUBA), which combines the flexibility
of untargeted attacks with the intentionality of targeted attacks. The
compromised model, when presented with backdoor images, will classify them into
random classes within a constrained range of target classes selected by the
attacker. This combination of randomness and determinedness enables the
proposed untargeted backdoor attack to natively circumvent existing backdoor
defense methods. To implement the untargeted backdoor attack under controlled
flexibility, we propose to apply logit normalization on cross-entropy loss with
flipped one-hot labels. By constraining the logit during training, the
compromised model will show a uniform distribution across selected target
classes, resulting in controlled untargeted attack. Extensive experiments
demonstrate the effectiveness of the proposed CUBA on different datasets.

</details>


### [310] [Differentiation-Based Extraction of Proprietary Data from Fine-Tuned LLMs](https://arxiv.org/abs/2506.17353)
*Zongjie Li,Daoyuan Wu,Shuai Wang,Zhendong Su*

Key words: 监督微调（SFT）,数据提取,DDE,安全模型

TL;DR: 本文首次研究了监督微调（SFT）数据提取的关键问题，提出了一种称为差异化数据提取（DDE）的新方法，并开发了防御机制以减轻攻击。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 随着领域专用和人类对齐的大型语言模型（LLM）需求增加，SFT数据的提取成为重要研究方向。

Method: 通过分析SFT数据的特性，提出DDE方法，利用微调模型的置信度和与预训练基模型的差异进行数据提取。

Result: 实验显示DDE在多种攻击场景中均优于现有基线方法。

Conclusion: 研究揭示了微调LLM中隐藏的数据泄露风险，为开发更安全模型提供了洞见。

Abstract: The increasing demand for domain-specific and human-aligned Large Language
Models (LLMs) has led to the widespread adoption of Supervised Fine-Tuning
(SFT) techniques. SFT datasets often comprise valuable instruction-response
pairs, making them highly valuable targets for potential extraction. This paper
studies this critical research problem for the first time. We start by formally
defining and formulating the problem, then explore various attack goals, types,
and variants based on the unique properties of SFT data in real-world
scenarios. Based on our analysis of extraction behaviors of direct extraction,
we develop a novel extraction method specifically designed for SFT models,
called Differentiated Data Extraction (DDE), which exploits the confidence
levels of fine-tuned models and their behavioral differences from pre-trained
base models. Through extensive experiments across multiple domains and
scenarios, we demonstrate the feasibility of SFT data extraction using DDE. Our
results show that DDE consistently outperforms existing extraction baselines in
all attack settings. To counter this new attack, we propose a defense mechanism
that mitigates DDE attacks with minimal impact on model performance. Overall,
our research reveals hidden data leak risks in fine-tuned LLMs and provides
insights for developing more secure models.

</details>


### [311] [Shrinking the Generation-Verification Gap with Weak Verifiers](https://arxiv.org/abs/2506.18203)
*Jon Saad-Falcon,E. Kelly Buchanan,Mayee F. Chen,Tzu-Heng Huang,Brendan McLaughlin,Tanvir Bhathal,Shang Zhu,Ben Athiwaratkun,Frederic Sala,Scott Linderman,Azalia Mirhoseini,Christopher Ré*

Key words: Weaver, 验证器, 弱监督, 语言模型

TL;DR: Weaver框架通过结合多个弱验证器，显著提升语言模型候选响应的选择性能，减少对标注数据的依赖。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 当前高质量的验证器要么不可扩展（如人工），要么效用有限（如工具Lean）。LM验证器和奖励模型虽广泛使用，但仍与完美验证器存在性能差距。

Method: Weaver通过弱监督估计验证器准确性，结合输出为统一分数，利用数据集统计标准化输出并过滤低质量验证器。

Result: 在数学和推理任务中，Weaver显著提升Pass@1性能，达到o3-mini级别的准确性（87.7%）。

Conclusion: Weaver通过弱监督和验证器组合，显著提升验证性能，减少计算成本。

Abstract: Verifiers can improve language model capabilities by scoring and ranking
responses from generated candidates. Currently, high-quality verifiers are
either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean).
While LM judges and reward models have become broadly useful as general-purpose
verifiers, a significant performance gap remains between them and oracle
verifiers (verifiers with perfect accuracy). To help close this gap, we
introduce Weaver, a framework for designing a strong verifier by combining
multiple weak, imperfect verifiers. We find weighted ensembles of verifiers,
which typically require learning from labeled data, significantly outperform
unweighted combinations due to differences in verifier accuracies. To reduce
dependency on labeled data, Weaver leverages weak supervision to estimate each
verifier's accuracy and combines outputs into a unified score that better
reflects true response quality. However, directly applying weak supervision
algorithms poses challenges, including inconsistent verifier output formats and
handling low-quality verifiers. Weaver addresses these using dataset statistics
to normalize outputs and filter specific verifiers. We study Weaver's
effectiveness in test-time repeated sampling, where a model generates multiple
candidate responses and selects one. Our evaluations show Weaver significantly
improves over Pass@1-performance when selecting the first candidate-across
reasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B
Instruct as generator, and an ensemble of 70B or smaller judge and reward
models as verifiers (87.7% average). This gain mirrors the jump between GPT-4o
and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and
post-training. To reduce computational costs of verifier ensembles, we train a
400M cross-encoder using Weaver's combined output scores.

</details>


### [312] [Efficient Malware Detection with Optimized Learning on High-Dimensional Features](https://arxiv.org/abs/2506.17309)
*Aditya Choudhary,Sarthak Pawar,Yashodhara Haribhakta*

Key words: 恶意软件检测, 机器学, XGBoost, PCA, LightGBM

TL;DR: 论文研究了通过降维技术（XGBoost和PCA）优化高维特征向量在恶意软件检测中的计算效率，同时保持高准确率。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 高维特征向量在恶意软件检测中带来计算挑战，研究旨在通过降维技术提升计算效率。

Method: 使用XGBoost特征选择和PCA降维，评估128、256和384维特征，并在四种模型上测试。

Result: LightGBM在384维特征上达到97.52%准确率，且在未见数据集上表现良好。

Conclusion: 降维技术提供了一个计算高效且准确的恶意软件检测方案。

Abstract: Malware detection using machine learning requires feature extraction from
binary files, as models cannot process raw binaries directly. A common approach
involves using LIEF for raw feature extraction and the EMBER vectorizer to
generate 2381-dimensional feature vectors. However, the high dimensionality of
these features introduces significant computational challenges. This study
addresses these challenges by applying two dimensionality reduction techniques:
XGBoost-based feature selection and Principal Component Analysis (PCA). We
evaluate three reduced feature dimensions (128, 256, and 384), which correspond
to approximately 5.4%, 10.8%, and 16.1% of the original 2381 features, across
four models-XGBoost, LightGBM, Extra Trees, and Random Forest-using a unified
training, validation, and testing split formed from the EMBER-2018, ERMDS, and
BODMAS datasets. This approach ensures generalization and avoids dataset bias.
Experimental results show that LightGBM trained on the 384-dimensional feature
set after XGBoost feature selection achieves the highest accuracy of 97.52% on
the unified dataset, providing an optimal balance between computational
efficiency and detection performance. The best model, trained in 61 minutes
using 30 GB of RAM and 19.5 GB of disk space, generalizes effectively to
completely unseen datasets, maintaining 95.31% accuracy on TRITIUM and 93.98%
accuracy on INFERNO. These findings present a scalable, compute-efficient
approach for malware detection without compromising accuracy.

</details>


### [313] [Mechanistic Interpretability in the Presence of Architectural Obfuscation](https://arxiv.org/abs/2506.18053)
*Marcos Florencio,Thomas Barton*

Key words: 架构模糊化,机制可解释性,隐私保护,大语言模型,注意力头,logit-lens

TL;DR: 架构模糊化技术在隐私保护的大语言模型推理中有效阻碍细粒度可解释性，但保留全局模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 研究架构模糊化技术（如隐藏状态张量的排列、嵌入表的线性变换或令牌重映射）对机制可解释性的影响，以验证其是否能真正阻碍对模型工作原理的理解。

Method: 使用具有代表性的模糊化映射训练的GPT-2-small模型，并在假设模糊化映射私有的条件下，应用logit-lens归因、因果路径修补和注意力头剔除等方法定位和操作已知电路。

Result: 模糊化显著改变注意力头内的激活模式，但保留了层间计算图，导致用户提示的反向工程困难；前馈和残差路径功能完好，表明模糊化不影响顶级任务性能。

Conclusion: 架构模糊化既能保留全局模型行为，又能阻碍对用户特定内容的机制分析，为未来的隐私保护和可解释性工具提供了方向。

Abstract: Architectural obfuscation - e.g., permuting hidden-state tensors, linearly
transforming embedding tables, or remapping tokens - has recently gained
traction as a lightweight substitute for heavyweight cryptography in
privacy-preserving large-language-model (LLM) inference. While recent work has
shown that these techniques can be broken under dedicated reconstruction
attacks, their impact on mechanistic interpretability has not been
systematically studied. In particular, it remains unclear whether scrambling a
network's internal representations truly thwarts efforts to understand how the
model works, or simply relocates the same circuits to an unfamiliar coordinate
system. We address this gap by analyzing a GPT-2-small model trained from
scratch with a representative obfuscation map. Assuming the obfuscation map is
private and the original basis is hidden (mirroring an honest-but-curious
server), we apply logit-lens attribution, causal path-patching, and
attention-head ablation to locate and manipulate known circuits. Our findings
reveal that obfuscation dramatically alters activation patterns within
attention heads yet preserves the layer-wise computational graph. This
disconnect hampers reverse-engineering of user prompts: causal traces lose
their alignment with baseline semantics, and token-level logit attributions
become too noisy to reconstruct. At the same time, feed-forward and residual
pathways remain functionally intact, suggesting that obfuscation degrades
fine-grained interpretability without compromising top-level task performance.
These results establish quantitative evidence that architectural obfuscation
can simultaneously (i) retain global model behaviour and (ii) impede
mechanistic analyses of user-specific content. By mapping where
interpretability breaks down, our study provides guidance for future privacy
defences and for robustness-aware interpretability tooling.

</details>


### [314] [A Locally Differential Private Coding-Assisted Succinct Histogram Protocol](https://arxiv.org/abs/2506.17767)
*Hsuan-Po Liu,Hessam Mahdavifar*

Key words: 局部差分隐私,简洁直方图,纠错码,极化码,SCL解码

TL;DR: 本文提出了一种基于局部差分隐私（LDP）的简洁直方图构建协议，利用纠错码（特别是极化码及其SCL解码算法）提升数据效用，实验表明其优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 在隐私敏感的大规模机器学习中，设计一种既能保护隐私又能保持数据效用的简洁直方图方法至关重要。

Method: 使用极化码和SCL解码算法，结合高斯扰动实现高效软解码，提出首个实用的(ε,δ)-LDP协议。

Result: 实验表明该方法在低频项上表现优异，同时保持了频率估计的准确性。

Conclusion: 提出的协议通过纠错码和LDP结合，为隐私保护下的数据收集提供了新思路。

Abstract: A succinct histogram captures frequent items and their frequencies across
clients and has become increasingly important for large-scale,
privacy-sensitive machine learning applications. To develop a rigorous
framework to guarantee privacy for the succinct histogram problem, local
differential privacy (LDP) has been utilized and shown promising results. To
preserve data utility under LDP, which essentially works by intentionally
adding noise to data, error-correcting codes naturally emerge as a promising
tool for reliable information collection. This work presents the first
practical $(\epsilon,\delta)$-LDP protocol for constructing succinct histograms
using error-correcting codes. To this end, polar codes and their
successive-cancellation list (SCL) decoding algorithms are leveraged as the
underlying coding scheme. More specifically, our protocol introduces
Gaussian-based perturbations to enable efficient soft decoding. Experiments
demonstrate that our approach outperforms prior methods, particularly for items
with low true frequencies, while maintaining similar frequency estimation
accuracy.

</details>


### [315] [AdRo-FL: Informed and Secure Client Selection for Federated Learning in the Presence of Adversarial Aggregator](https://arxiv.org/abs/2506.17805)
*Md. Kamrul Hossain,Walid Aljoby,Anis Elgabli,Ahmed M. Abdelmoniem,Khaled A. Harras*

Key words: 联邦学习,安全聚合,偏向选择攻击,客户端选择,隐私保护

TL;DR: AdRo-FL提出了一种对抗性鲁棒联邦学习方法，既能实现基于客户端效用的智能选择，又能防御偏向选择攻击（BSA），在隐私保护聚合的同时提升性能。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 联邦学习（FL）中，聚合器可能通过操纵客户端选择绕过安全聚合（SA）的保护，导致隐私泄露。需要一种既能智能选择客户端又能防御攻击的方法。

Method: AdRo-FL设计了两种客户端选择框架，分别针对集群和分布式场景：集群场景通过最小选择配额和效用函数防御BSA；分布式场景结合效用排名和可验证随机函数（VRF）实现安全选择。还采用量化和严格传输期限提升效率。

Result: AdRo-FL相比不安全基线，精度提升1.06倍，收敛速度提升1.85倍。

Conclusion: AdRo-FL在保护隐私的同时，显著提升了FL的性能和效率。

Abstract: Federated Learning (FL) enables collaborative learning without exposing
clients' data. While clients only share model updates with the aggregator,
studies reveal that aggregators can infer sensitive information from these
updates. Secure Aggregation (SA) protects individual updates during
transmission; however, recent work demonstrates a critical vulnerability where
adversarial aggregators manipulate client selection to bypass SA protections,
constituting a Biased Selection Attack (BSA). Although verifiable random
selection prevents BSA, it precludes informed client selection essential for FL
performance. We propose Adversarial Robust Federated Learning (AdRo-FL), which
simultaneously enables: informed client selection based on client utility, and
robust defense against BSA maintaining privacy-preserving aggregation. AdRo-FL
implements two client selection frameworks tailored for distinct settings. The
first framework assumes clients are grouped into clusters based on mutual
trust, such as different branches of an organization. The second framework
handles distributed clients where no trust relationships exist between them.
For the cluster-oriented setting, we propose a novel defense against BSA by (1)
enforcing a minimum client selection quota from each cluster, supervised by a
cluster-head in every round, and (2) introducing a client utility function to
prioritize efficient clients. For the distributed setting, we design a
two-phase selection protocol: first, the aggregator selects the top clients
based on our utility-driven ranking; then, a verifiable random function (VRF)
ensures a BSA-resistant final selection. AdRo-FL also applies quantization to
reduce communication overhead and sets strict transmission deadlines to improve
energy efficiency. AdRo-FL achieves up to $1.85\times$ faster time-to-accuracy
and up to $1.06\times$ higher final accuracy compared to insecure baselines.

</details>


### [316] [Federated Learning-Based Data Collaboration Method for Enhancing Edge Cloud AI System Security Using Large Language Models](https://arxiv.org/abs/2506.18087)
*Huaiying Luo,Cheng Ji*

Key words: 边缘计算, 联邦学习, 数据隐私, 大语言模型, 安全多方计算

TL;DR: 提出了一种基于联邦学习的边缘云计算数据协作方法，利用大语言模型（LLM）提升数据隐私保护和系统鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 边缘计算和云系统在AI应用中的广泛使用引发了数据隐私和性能效率的双重挑战。

Method: 在现有联邦学习框架中引入安全多方计算协议，结合LLM优化数据聚合和加密过程，并通过对抗训练增强系统安全性。

Result: 实验结果表明，该方法在数据保护和模型鲁棒性上比传统联邦学习方法提升15%。

Conclusion: 结合LLM和安全多方计算的联邦学习方法能有效提升边缘云AI系统的隐私保护和安全性。

Abstract: With the widespread application of edge computing and cloud systems in
AI-driven applications, how to maintain efficient performance while ensuring
data privacy has become an urgent security issue. This paper proposes a
federated learning-based data collaboration method to improve the security of
edge cloud AI systems, and use large-scale language models (LLMs) to enhance
data privacy protection and system robustness. Based on the existing federated
learning framework, this method introduces a secure multi-party computation
protocol, which optimizes the data aggregation and encryption process between
distributed nodes by using LLM to ensure data privacy and improve system
efficiency. By combining advanced adversarial training techniques, the model
enhances the resistance of edge cloud AI systems to security threats such as
data leakage and model poisoning. Experimental results show that the proposed
method is 15% better than the traditional federated learning method in terms of
data protection and model robustness.

</details>


### [317] [Dynamic Temporal Positional Encodings for Early Intrusion Detection in IoT](https://arxiv.org/abs/2506.18114)
*Ioannis Panopoulos,Maria-Lamprini A. Bartsioka,Sokratis Nikolaidis,Stylianos I. Venieris,Dimitra I. Kaklamani,Iakovos S. Venieris*

Key words: 物联网安全,入侵检测系统,Transformer,动态时间编码,数据增强

TL;DR: 提出了一种基于Transformer的早期入侵检测系统（EIDS），通过动态时间位置编码和数据增强提高检测准确性和实时性，在CICIoT2023数据集上表现优异，适用于资源受限的IoT设备。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 物联网（IoT）的快速发展带来了严重的安全问题，传统入侵检测系统（IDS）忽视网络流量的时间特征，难以有效进行早期威胁检测。

Method: 采用Transformer模型，结合动态时间位置编码，捕捉网络流量的时序结构和不规则性；引入数据增强管道，提升模型鲁棒性。

Result: 在CICIoT2023数据集上，该方法在准确性和早期检测方面优于现有模型，并在资源受限的IoT设备上实现低延迟和低内存占用。

Conclusion: EIDS在IoT安全中表现出高效性和适应性，适用于实时入侵检测。

Abstract: The rapid expansion of the Internet of Things (IoT) has introduced
significant security challenges, necessitating efficient and adaptive Intrusion
Detection Systems (IDS). Traditional IDS models often overlook the temporal
characteristics of network traffic, limiting their effectiveness in early
threat detection. We propose a Transformer-based Early Intrusion Detection
System (EIDS) that incorporates dynamic temporal positional encodings to
enhance detection accuracy while maintaining computational efficiency. By
leveraging network flow timestamps, our approach captures both sequence
structure and timing irregularities indicative of malicious behaviour.
Additionally, we introduce a data augmentation pipeline to improve model
robustness. Evaluated on the CICIoT2023 dataset, our method outperforms
existing models in both accuracy and earliness. We further demonstrate its
real-time feasibility on resource-constrained IoT devices, achieving
low-latency inference and minimal memory footprint.

</details>


### [318] [Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection](https://arxiv.org/abs/2506.18245)
*Lei Yu,Zhirong Huang,Hang Yuan,Shiqi Cheng,Li Yang,Fengjun Zhang,Chenjie Shen,Jiajia Ma,Jingyuan Zhang,Junyi Lu,Chun Zuo*

Key words: 智能合约漏洞检测，LLaMA-3.1-8B，持续预训练，监督微调，直接偏好优化

TL;DR: 论文提出了一种基于LLaMA-3.1-8B的智能合约漏洞检测方法Smart-LLaMA-DPO，通过构建全面数据集和改进训练方法，显著提升了检测效果和解释质量。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 解决智能合约漏洞检测中的数据覆盖不全和大型语言模型（LLMs）对特定安全概念解释不准确的问题。

Method: 构建涵盖四类主要漏洞和非机器可审计漏洞的数据集，结合持续预训练（CPT）、监督微调（SFT）和直接偏好优化（DPO）方法训练模型。

Result: 方法在F1分数和准确率上分别平均提升10.43%和7.87%，并且在解释的正确性、全面性和清晰度上优于现有基线。

Conclusion: Smart-LLaMA-DPO在智能合约漏洞检测中表现出色，特别是在模型解释能力上取得了显著进步。

Abstract: Smart contract vulnerability detection remains a major challenge in
blockchain security. Existing vulnerability detection methods face two main
issues: (1) Existing datasets lack comprehensive coverage and high-quality
explanations for preference learning. (2) Large language models (LLMs) often
struggle with accurately interpreting specific concepts in smart contract
security. Empirical analysis shows that even after continual pre-training (CPT)
and supervised fine-tuning (SFT), LLMs may misinterpret the execution order of
state changes, resulting in incorrect explanations despite making correct
detection decisions. To address these challenges, we propose Smart-LLaMA-DPO
based on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major
vulnerability types and machine-unauditable vulnerabilities, including precise
labels, explanations, and locations for SFT, as well as high-quality and
low-quality output pairs for Direct Preference Optimization (DPO). Second, we
perform CPT using large-scale smart contract to enhance the LLM's understanding
of specific security practices in smart contracts. Futhermore, we conduct SFT
with our comprehensive dataset. Finally, we apply DPO, leveraging human
feedback and a specially designed loss function that increases the probability
of preferred explanations while reducing the likelihood of non-preferred
outputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types:
reentrancy, timestamp dependence, integer overflow/underflow, and delegatecall,
as well as machine-unauditable vulnerabilities. Our method significantly
outperforms state-of-the-art baselines, with average improvements of 10.43% in
F1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human
evaluation confirm that our method generates more correct, thorough, and clear
explanations.

</details>


### [319] [Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks](https://arxiv.org/abs/2506.18543)
*Xiaodong Wu,Xiangman Li,Jianbing Ni*

Key words: 大语言模型、越狱攻击、DeepSeek、GPT-4、安全对齐

TL;DR: 本文首次系统评估了DeepSeek系列模型在越狱攻击下的鲁棒性，并与GPT-3.5和GPT-4进行了对比。研究发现DeepSeek的MoE架构在优化型攻击下表现较好，但在提示型和手动攻击中更脆弱，而GPT-4 Turbo则展现出更强的安全对齐性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 评估开源LLM（如DeepSeek）在越狱攻击下的鲁棒性，因其在实际应用中的普及但相关研究不足。

Method: 使用HarmBench基准测试，评估7种攻击策略对510种有害行为的对抗能力，比较DeepSeek与GPT-3.5和GPT-4的表现。

Result: DeepSeek的MoE架构对优化型攻击（如TAP-T）有选择性鲁棒性，但在其他攻击方式下更脆弱；GPT-4 Turbo表现出更一致的安全对齐能力。

Conclusion: 开源LLM在效率与安全对齐之间存在权衡，需针对性地优化安全策略以确保安全部署。

Abstract: The widespread deployment of large language models (LLMs) has raised critical
concerns over their vulnerability to jailbreak attacks, i.e., adversarial
prompts that bypass alignment mechanisms and elicit harmful or policy-violating
outputs. While proprietary models like GPT-4 have undergone extensive
evaluation, the robustness of emerging open-source alternatives such as
DeepSeek remains largely underexplored, despite their growing adoption in
real-world applications. In this paper, we present the first systematic
jailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and
GPT-4 using the HarmBench benchmark. We evaluate seven representative attack
strategies across 510 harmful behaviors categorized by both function and
semantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE)
architecture introduces routing sparsity that offers selective robustness
against optimization-based attacks such as TAP-T, but leads to significantly
higher vulnerability under prompt-based and manually engineered attacks. In
contrast, GPT-4 Turbo demonstrates stronger and more consistent safety
alignment across diverse behaviors, likely due to its dense Transformer design
and reinforcement learning from human feedback. Fine-grained behavioral
analysis and case studies further show that DeepSeek often routes adversarial
prompts to under-aligned expert modules, resulting in inconsistent refusal
behaviors. These findings highlight a fundamental trade-off between
architectural efficiency and alignment generalization, emphasizing the need for
targeted safety tuning and modular alignment strategies to ensure secure
deployment of open-source LLMs.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [320] [Modal Logic for Stratified Becoming: Actualization Beyond Possible Worlds](https://arxiv.org/abs/2506.17276)
*Alexandre Le Nepvou*

Key words: 模态逻辑, 分层实现, 克里普克语义学, 本体论稳定性, 实际化

TL;DR: 提出了一种基于分层实现的新型模态逻辑框架SAL，替代传统的可能世界模型，强调局部、动态和不对称的实现过程。

<details>
  <summary>Details</summary>

Main category: cs.LO

Motivation: 传统克里普克语义学忽视实际化过程的局部性和动态性，需要一个更符合实际的替代模型。

Method: 设计了分层实现逻辑（SAL），通过索引模态符号的层级来模拟实际化过程，定义了语法、语义及公理，并证明了可靠性和完备性。

Result: SAL成功捕捉了实际化的本体结构，无需依赖抽象的可能世界，适用于时间演变、量子退相干等领域。

Conclusion: SAL为传统模态实在论提供了一个分层的替代方案，更贴近实际化的动态特性。

Abstract: This article develops a novel framework for modal logic based on the idea of
stratified actualization, rather than the classical model of global possible
worlds. Traditional Kripke semantics treat modal operators as quantification
over fully determinate alternatives, neglecting the local, dynamic, and often
asymmetric nature of actualization processes. We propose a system Stratified
Actualization Logic (SAL) in which modalities are indexed by levels of
ontological stability, interpreted as admissibility regimes. Each modality
operates over a structured layer of possibility, grounded in the internal
coherence of transitions between layers. We formally define the syntax and
semantics of SAL, introduce its axioms, and prove soundness and completeness.
Applications are discussed in connection with temporal becoming, quantum
decoherence domains, and modal metaphysics. The result is a logic that captures
the ontological structure of actualization without recourse to abstract
possible worlds, offering a stratified alternative to standard modal realism.

</details>


### [321] [Beyond Prediction -- Structuring Epistemic Integrity in Artificial Reasoning Systems](https://arxiv.org/abs/2506.17331)
*Craig Steven Wright*

Key words: 人工智能、认知约束、符号推理、知识图谱、区块链

TL;DR: 论文提出了一个在严格认知约束下的人工智能系统框架，支持结构化推理、命题承诺和矛盾检测。

<details>
  <summary>Details</summary>

Main category: cs.LO

Motivation: 旨在开发一个超越随机语言预测的AI系统，确保其具有理性认知能力。

Method: 结合符号推理、知识图谱和基于区块链的验证，形式化描述了信念表示和元认知过程。

Result: 构建了一个能保持真理且可审计的理性认知代理。

Conclusion: 该框架为AI系统提供了更强大的认知能力，支持复杂的推理和验证。

Abstract: This paper develops a comprehensive framework for artificial intelligence
systems that operate under strict epistemic constraints, moving beyond
stochastic language prediction to support structured reasoning, propositional
commitment, and contradiction detection. It formalises belief representation,
metacognitive processes, and normative verification, integrating symbolic
inference, knowledge graphs, and blockchain-based justification to ensure
truth-preserving, auditably rational epistemic agents.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [322] [Enhancing Few-shot Keyword Spotting Performance through Pre-Trained Self-supervised Speech Models](https://arxiv.org/abs/2506.17686)
*Alican Gok,Oguzhan Buyuksolak,Osman Erman Okman,Murat Saraclar*

Key words: 关键词检测, 少量样本学习, 自监督学习, 边缘计算

TL;DR: 该论文提出了一种基于自监督学习的训练方案，用于解决资源受限边缘设备上的少量样本关键词检测（FS-KWS）精度不足的问题，通过特征提取、降维和知识蒸馏提升性能。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 现有FS-KWS系统在资源受限的边缘环境中精度不足，需要一种更高效的方法来提升识别准确性。

Method: 结合自监督学习模型（如Wav2Vec 2.0）和Sub-center ArcFace损失函数训练教师模型，再通过注意力降维和轻量级ResNet15学生模型实现高效部署。

Result: 在GSC数据集上，10-shot分类精度从33.4%提升到74.1%，显著提升了实用性。

Conclusion: 所提方法显著提升了FS-KWS在资源受限环境中的性能，适合实际应用。

Abstract: Keyword Spotting plays a critical role in enabling hands-free interaction for
battery-powered edge devices. Few-Shot Keyword Spotting (FS-KWS) addresses the
scalability and adaptability challenges of traditional systems by enabling
recognition of custom keywords with only a few examples. However, existing
FS-KWS systems achieve subpar accuracy at desirable false acceptance rates,
particularly in resource-constrained edge environments. To address these
issues, we propose a training scheme that leverages self-supervised learning
models for robust feature extraction, dimensionality reduction, and knowledge
distillation. The teacher model, based on Wav2Vec 2.0 is trained using
Sub-center ArcFace loss, which enhances inter-class separability and
intra-class compactness. To enable efficient deployment on edge devices, we
introduce attention-based dimensionality reduction and train a standard
lightweight ResNet15 student model. We evaluate the proposed approach on the
English portion of the Multilingual Spoken Words Corpus (MSWC) and the Google
Speech Commands (GSC) datasets. Notably, the proposed training method improves
the 10-shot classification accuracy from 33.4% to 74.1% on 11 classes at 1%
false alarm accuracy on the GSC dataset, thus making it significantly
better-suited for a real use case scenario.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [323] [Resolving the Ti-V Phase Diagram Discrepancy with First-Principles Calculations and Bayesian Learning](https://arxiv.org/abs/2506.17719)
*Timofei Miryashkin,Olga Klimanova,Alexander Shapeev*

Key words: 钛-钒合金, BCC混溶隙, 从头算, 机器学习, 贝叶斯推断

TL;DR: 使用从头算+机器学习工作流，确认钛-钒（Ti-V）二元合金存在BCC混溶隙，排除氧污染假设。

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

Motivation: 解决关于钛-钒合金是否存在BCC混溶隙的实验争议。

Method: 结合主动训练的矩张量势和贝叶斯热力学推断的从头算+机器学习工作流。

Result: 获得了整个成分范围的Ti-V相图，支持BCC混溶隙的存在（终止于T=980 K，c=0.67），排除氧污染影响。

Conclusion: 该方法有效且鲁棒，混溶隙为合金固有特性。

Abstract: Conflicting experiments disagree on whether the titanium-vanadium (Ti-V)
binary alloy exhibits a body-centred cubic (BCC) miscibility gap or remains
completely soluble. A leading hypothesis attributes the miscibility gap to
oxygen contamination during alloy preparation. To resolve this controversy, we
use an ab initio + machine-learning workflow that couples an actively-trained
Moment Tensor Potential to Bayesian thermodynamic inference. Using this
workflow, we obtain Ti-V binary system across the entire composition range,
together with confidence intervals in the thermodynamic limit. The resulting
diagram reproduces all experimental features, demonstrating the robustness of
our approach, and clearly favors the variant with a BCC miscibility gap
terminating at T = 980 K and c = 0.67. Because oxygen was excluded from
simulations, the gap cannot be attributed to impurity effects, contradicting
recent CALPHAD reassessments.

</details>


### [324] [Residual Connection-Enhanced ConvLSTM for Lithium Dendrite Growth Prediction](https://arxiv.org/abs/2506.17756)
*Hosung Lee,Byeongoh Hwang,Dasan Kim,Myungjoo Kang*

Key words: 锂枝晶, ConvLSTM, 残差连接, 电池诊断, 相场模型

TL;DR: 提出了一个带有残差连接的ConvLSTM模型，用于更准确地预测锂枝晶生长模式，从而提升电池性能和安全性。

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

Motivation: 锂枝晶的生长严重影响了可充电电池的性能和安全性，导致短路和容量下降。

Method: 采用带残差连接的ConvLSTM模型，解决了梯度消失问题，并提升了特征保留能力，同时捕捉了局部的枝晶生长动态和宏观电池行为。

Result: 实验表明，该模型在多种电压条件下（0.1V、0.3V、0.5V）比传统ConvLSTM模型准确率提升了7%，并显著降低了均方误差（MSE）。

Conclusion: 残差连接在深度时空网络中用于电化学系统建模有效，为电池诊断提供了强大工具，未来可扩展到其他电池化学体系并与实验数据结合验证。

Abstract: The growth of lithium dendrites significantly impacts the performance and
safety of rechargeable batteries, leading to short circuits and capacity
degradation. This study proposes a Residual Connection-Enhanced ConvLSTM model
to predict dendrite growth patterns with improved accuracy and computational
efficiency. By integrating residual connections into ConvLSTM, the model
mitigates the vanishing gradient problem, enhances feature retention across
layers, and effectively captures both localized dendrite growth dynamics and
macroscopic battery behavior. The dataset was generated using a phase-field
model, simulating dendrite evolution under varying conditions. Experimental
results show that the proposed model achieves up to 7% higher accuracy and
significantly reduces mean squared error (MSE) compared to conventional
ConvLSTM across different voltage conditions (0.1V, 0.3V, 0.5V). This
highlights the effectiveness of residual connections in deep spatiotemporal
networks for electrochemical system modeling. The proposed approach offers a
robust tool for battery diagnostics, potentially aiding in real-time monitoring
and optimization of lithium battery performance. Future research can extend
this framework to other battery chemistries and integrate it with real-world
experimental data for further validation

</details>


### [325] [CLOUD: A Scalable and Physics-Informed Foundation Model for Crystal Representation Learning](https://arxiv.org/abs/2506.17345)
*Changwen Xu,Shang Zhu,Venkatasubramanian Viswanathan*

Key words: 晶体预测, 机器学习, 对称性编码, 物理一致模型

TL;DR: 该论文提出了一种名为CLOUD的晶体语言模型，通过对称性一致的有序参数编码（SCOPE）和预训练，预测多种晶体性质并整合物理原理。

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

Motivation: 传统方法（如实验或DFT计算）资源密集，限制了其扩展性。而现有机器学习模型在通用性和可解释性方面存在不足。

Method: CLOUD是一个基于Transformer的框架，采用SCOPE编码晶体对称性、Wyckoff位置和组成，并通过预训练和微调实现性能。

Result: CLOUD在多任务预测中表现优异，且CLOUD-DEBYE框架实现了热力学一致的温度依赖性预测。

Conclusion: CLOUD展示了作为统一对称性表示与物理学习的晶体材料基础模型的潜力。

Abstract: The prediction of crystal properties is essential for understanding
structure-property relationships and accelerating the discovery of functional
materials. However, conventional approaches relying on experimental
measurements or density functional theory (DFT) calculations are often
resource-intensive, limiting their scalability. Machine learning (ML) models
offer a promising alternative by learning complex structure-property
relationships from data, enabling faster predictions. Yet, existing ML models
often rely on labeled data, adopt representations that poorly capture essential
structural characteristics, and lack integration with physical
principles--factors that limit their generalizability and interpretability.
Here, we introduce CLOUD (Crystal Language mOdel for Unified and Differentiable
materials modeling), a transformer-based framework trained on a novel
Symmetry-Consistent Ordered Parameter Encoding (SCOPE) that encodes crystal
symmetry, Wyckoff positions, and composition in a compact, coordinate-free
string representation. Pre-trained on over six million crystal structures,
CLOUD is fine-tuned on multiple downstream tasks and achieves competitive
performance in predicting a wide range of material properties, demonstrating
strong scaling performance. Furthermore, as proof of concept of differentiable
materials modeling, CLOUD is applied to predict the phonon internal energy and
heat capacity, which integrates the Debye model to preserve thermodynamic
consistency. The CLOUD-DEBYE framework enforces thermodynamic consistency and
enables temperature-dependent property prediction without requiring additional
data. These results demonstrate the potential of CLOUD as a scalable and
physics-informed foundation model for crystalline materials, unifying
symmetry-consistent representations with physically grounded learning for
property prediction and materials discovery.

</details>


### [326] [Leveraging neural network interatomic potentials for a foundation model of chemistry](https://arxiv.org/abs/2506.18497)
*So Yeon Kim,Yang Jeong Park,Ju Li*

Key words: 神经网络原子间势, 机器学习, 材料科学, 特征提取, 转移学习

TL;DR: HackNIP是一种两阶段管道方法，利用预训练的神经网络原子间势（NIP）提取特征向量，再训练浅层机器学习模型，以在材料科学中实现高效的结构-属性映射，解决当前方法的泛化性和计算资源问题。

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

Motivation: 尽管大规模基础模型（如NIP）在计算材料科学中表现出潜力，但其在直接预测电子属性和宏观性质时存在局限性。机器学习方法也存在泛化性或资源消耗的权衡问题。HackNIP旨在通过结合预训练NIP和浅层ML模型，克服这些挑战。

Method: HackNIP采用两阶段方法：1）从预训练NIP中提取固定长度特征向量；2）使用这些向量训练浅层ML模型进行结构-属性预测。研究还评估了嵌入深度对性能的影响。

Result: HackNIP在Matbench上进行了基准测试，展示了其在数据效率和多任务（如从头算、实验和分子属性）中的优越性。研究发现特定嵌入深度可提供最具信息量的特征。

Conclusion: HackNIP通过结合预训练NIP和浅层ML模型，有效解决了机器学习在材料科学中的泛化性和计算资源问题，为高性能预测建模提供了新策略。

Abstract: Large-scale foundation models, including neural network interatomic
potentials (NIPs) in computational materials science, have demonstrated
significant potential. However, despite their success in accelerating atomistic
simulations, NIPs face challenges in directly predicting electronic properties
and often require coupling to higher-scale models or extensive simulations for
macroscopic properties. Machine learning (ML) offers alternatives for
structure-to-property mapping but faces trade-offs: feature-based methods often
lack generalizability, while deep neural networks require significant data and
computational power. To address these trade-offs, we introduce HackNIP, a
two-stage pipeline that leverages pretrained NIPs. This method first extracts
fixed-length feature vectors (embeddings) from NIP foundation models and then
uses these embeddings to train shallow ML models for downstream
structure-to-property predictions. This study investigates whether such a
hybridization approach, by ``hacking" the NIP, can outperform end-to-end deep
neural networks, determines the dataset size at which this transfer learning
approach surpasses direct fine-tuning of the NIP, and identifies which NIP
embedding depths yield the most informative features. HackNIP is benchmarked on
Matbench, evaluated for data efficiency, and tested on diverse tasks including
\textit{ab initio}, experimental, and molecular properties. We also analyze how
embedding depth impacts performance. This work demonstrates a hybridization
strategy to overcome ML trade-offs in materials science, aiming to democratize
high-performance predictive modeling.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [327] [Spiffy: Efficient Implementation of CoLaNET for Raspberry Pi](https://arxiv.org/abs/2506.18306)
*Andrey Derzhavin,Denis Larionov*

Key words: 脉冲神经网络, SNN, 轻量级, Rust, Raspberry Pi, MNIST

TL;DR: 本文提出了一种轻量级的基于软件的方法，用于在没有专用神经形态硬件或框架的情况下运行脉冲神经网络（SNNs），并通过优化在普通计算平台上实现了低延迟和高准确性。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 由于专用神经形态硬件或框架的高成本和复杂性，本文旨在开发一种轻量级的软件方法，使SNNs能在普通计算平台上高效运行。

Method: 通过Rust语言实现特定的SNN架构（CoLaNET），并在普通计算平台（如Raspberry Pi）上进行优化，使用MNIST数据集进行案例研究。

Result: 提出的实现Spiffy在Raspberry Pi上达到92%的准确率，训练步骤延迟为0.9毫秒，推理步骤延迟为0.45毫秒。

Conclusion: 该方法证明了在普通硬件上高效运行SNNs的可行性，并开源了代码以促进进一步研究。

Abstract: This paper presents a lightweight software-based approach for running spiking
neural networks (SNNs) without relying on specialized neuromorphic hardware or
frameworks. Instead, we implement a specific SNN architecture (CoLaNET) in Rust
and optimize it for common computing platforms. As a case study, we demonstrate
our implementation, called Spiffy, on a Raspberry Pi using the MNIST dataset.
Spiffy achieves 92% accuracy with low latency - just 0.9 ms per training step
and 0.45 ms per inference step. The code is open-source.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [328] [RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation](https://arxiv.org/abs/2506.18088)
*Tianxing Chen,Zanxin Chen,Baijun Chen,Zijian Cai,Yibin Liu,Qiwei Liang,Zixuan Li,Xianliang Lin,Yiheng Ge,Zhenyu Gu,Weiliang Deng,Yubin Guo,Tian Nian,Xuanbing Xie,Qiangyu Chen,Kailun Su,Tianling Xu,Guodong Liu,Mengkang Hu,Huan-ang Gao,Kaixuan Wang,Zhixuan Liang,Yusen Qin,Xiaokang Yang,Ping Luo,Yao Mu*

Key words: 双臂操纵、仿真数据、领域随机化、多模态大语言模型、RoboTwin 2.0

TL;DR: RoboTwin 2.0是一个可扩展的仿真框架，用于生成多样化和现实的双臂操纵数据，通过结合多模态大语言模型和领域随机化，显著提升了数据的多样性和策略的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决现有合成数据集在双臂操纵任务中效率低和仿真环境过于简化的问题。

Method: 构建大规模物体库，结合MLLMs生成任务代码，并通过五轴领域随机化增强仿真数据多样性。

Result: 代码生成成功率提升10.9%，在未见过的真实场景中模型性能显著提升，零样本模型表现尤为突出。

Conclusion: RoboTwin 2.0为双臂操纵研究提供了高效的数据生成和评估工具，推动了仿真到现实的泛化能力。

Abstract: Simulation-based data synthesis has emerged as a powerful paradigm for
enhancing real-world robotic manipulation. However, existing synthetic datasets
remain insufficient for robust bimanual manipulation due to two challenges: (1)
the lack of an efficient, scalable data generation method for novel tasks, and
(2) oversimplified simulation environments that fail to capture real-world
complexity. We present RoboTwin 2.0, a scalable simulation framework that
enables automated, large-scale generation of diverse and realistic data, along
with unified evaluation protocols for dual-arm manipulation. We first construct
RoboTwin-OD, a large-scale object library comprising 731 instances across 147
categories, each annotated with semantic and manipulation-relevant labels.
Building on this foundation, we develop an expert data synthesis pipeline that
combines multimodal large language models (MLLMs) with simulation-in-the-loop
refinement to generate task-level execution code automatically. To improve
sim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization
along five axes: clutter, lighting, background, tabletop height and language
instructions, thereby enhancing data diversity and policy robustness. We
instantiate this framework across 50 dual-arm tasks spanning five robot
embodiments, and pre-collect over 100,000 domain-randomized expert
trajectories. Empirical results show a 10.9% gain in code generation success
and improved generalization to novel real-world scenarios. A VLA model
fine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)
on unseen scene real-world tasks, while zero-shot models trained solely on our
synthetic data achieve a 228% relative gain, highlighting strong generalization
without real-world supervision. We release the data generator, benchmark,
dataset, and code to support scalable research in robust bimanual manipulation.

</details>


### [329] [General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting](https://arxiv.org/abs/2506.17462)
*Bernard Lange,Anil Yildiz,Mansur Arief,Shehryar Khattak,Mykel Kochenderfer,Georgios Georgakis*

Key words: 通用导航, LVLM, 机器人堆栈, ARNA, HM-EQA基准

TL;DR: ARNA是一个通用导航框架，利用大视觉语言模型（LVLM）和自主规划工具，实现了在未知环境中无需预定义地图的导航和推理任务。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决现有导航系统依赖任务特定网络和固定数据流程的问题，提出一种更通用的导航方法。

Method: ARNA结合LVLM和感知、推理、导航工具，运行时自主定义并执行任务特定工作流。

Result: 在Habitat Lab的HM-EQA基准测试中表现优异，实现了高效探索、导航和问答。

Conclusion: ARNA展示了无需手工规划或预定义地图的通用导航框架的潜力。

Abstract: Developing general-purpose navigation policies for unknown environments
remains a core challenge in robotics. Most existing systems rely on
task-specific neural networks and fixed data flows, limiting generalizability.
Large Vision-Language Models (LVLMs) offer a promising alternative by embedding
human-like knowledge suitable for reasoning and planning. Yet, prior LVLM-robot
integrations typically depend on pre-mapped spaces, hard-coded representations,
and myopic exploration. We introduce the Agentic Robotic Navigation
Architecture (ARNA), a general-purpose navigation framework that equips an
LVLM-based agent with a library of perception, reasoning, and navigation tools
available within modern robotic stacks. At runtime, the agent autonomously
defines and executes task-specific workflows that iteratively query the robotic
modules, reason over multimodal inputs, and select appropriate navigation
actions. This approach enables robust navigation and reasoning in previously
unmapped environments, providing a new perspective on robotic stack design.
Evaluated in Habitat Lab on the HM-EQA benchmark, ARNA achieves
state-of-the-art performance, demonstrating effective exploration, navigation,
and embodied question answering without relying on handcrafted plans, fixed
input representations, or pre-existing maps.

</details>


### [330] [Distilling On-device Language Models for Robot Planning with Minimal Human Intervention](https://arxiv.org/abs/2506.17486)
*Zachary Ravichandran,Ignacio Hounie,Fernando Cladera,Alejandro Ribeiro,George J. Pappas,Vijay Kumar*

Key words: 大型语言模型、机器人规划、设备端计算、PRISM、合成数据

TL;DR: PRISM框架通过合成任务和环境数据，从大型语言模型（LLM）中提取小型语言模型（SLM），使其能在设备上运行，提升机器人在无云端依赖环境下的性能。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 为了解决LLM依赖云端导致的通信不稳定问题，如户外或工业环境，提出了在设备端运行的小型语言模型方案。

Method: PRISM框架从现有LLM规划器中自动合成任务和环境数据，生成SLM作为替代模型。

Result: PRISM将Llama-3.2-3B从GPT-4o性能的10-20%提升至93%，且能在多平台和环境中通用。

Conclusion: PRISM证明了SLM在设备端实现高效机器人规划的潜力。

Abstract: Large language models (LLMs) provide robots with powerful contextual
reasoning abilities and a natural human interface. Yet, current LLM-enabled
robots typically depend on cloud-hosted models, limiting their usability in
environments with unreliable communication infrastructure, such as outdoor or
industrial settings. We present PRISM, a framework for distilling small
language model (SLM)-enabled robot planners that run on-device with minimal
human supervision. Starting from an existing LLM-enabled planner, PRISM
automatically synthesizes diverse tasks and environments, elicits plans from
the LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in
replacement of the source model. We apply PRISM to three LLM-enabled planners
for mapping and exploration, manipulation, and household assistance, and we
demonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20% of
GPT-4o's performance to over 93% - using only synthetic data. We further
demonstrate that the distilled planners generalize across heterogeneous robotic
platforms (ground and aerial) and diverse environments (indoor and outdoor). We
release all software, trained models, and datasets at
https://zacravichandran.github.io/PRISM.

</details>


### [331] [Risk-Guided Diffusion: Toward Deploying Robot Foundation Models in Space, Where Failure Is Not An Option](https://arxiv.org/abs/2506.17601)
*Rohan Thakker,Adarsh Patnaik,Vince Kurtz,Jonas Frey,Jonathan Becktor,Sangwoo Moon,Rob Royce,Marcel Kaufmann,Georgios Georgakis,Pascal Roth,Joel Burdick,Marco Hutter,Shehryar Khattak*

Key words: 导航、生成式AI、物理模型、太空探索、风险引导

TL;DR: 提出了一种融合生成式AI与物理模型的导航框架，在火星模拟环境实验中减少失败率4倍。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 为未来太空探索任务提供安全、可靠的导航方法，解决现有生成式AI方法安全性不足的问题。

Method: 结合快速学习的‘System-1’与慢速物理模型的‘System-2’，通过风险引导的扩散框架实现安全导航。

Result: 在NASA JPL火星模拟环境中，失败率降低4倍，同时保持目标达成性能不变。

Conclusion: 通过耦合学习与物理模型的方法，提升了导航的安全性和可靠性。

Abstract: Safe, reliable navigation in extreme, unfamiliar terrain is required for
future robotic space exploration missions. Recent generative-AI methods learn
semantically aware navigation policies from large, cross-embodiment datasets,
but offer limited safety guarantees. Inspired by human cognitive science, we
propose a risk-guided diffusion framework that fuses a fast, learned "System-1"
with a slow, physics-based "System-2", sharing computation at both training and
inference to couple adaptability with formal safety. Hardware experiments
conducted at the NASA JPL's Mars-analog facility, Mars Yard, show that our
approach reduces failure rates by up to $4\times$ while matching the
goal-reaching performance of learning-based robotic models by leveraging
inference-time compute without any additional training.

</details>


### [332] [RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models](https://arxiv.org/abs/2506.17639)
*Yuxuan Chen,Xiao Li*

Key words: 视觉-语言-动作模型,VLA,模型压缩,结构化剪枝,量化,性能恢复

TL;DR: 该论文提出了一种名为RLRC的三阶段恢复方法，用于压缩视觉-语言-动作模型（VLA），显著减少内存使用并提升推理速度，同时保持任务成功率。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决VLA模型参数量大、推理延迟高的问题，以实现在资源受限的机器人平台上的实际部署。

Method: 采用结构化剪枝、基于SFT和RL的性能恢复以及量化三个阶段，对VLA模型进行压缩。

Result: RLRC实现了内存使用减少8倍、推理吞吐量提升2.3倍，且任务成功率不低于原模型。

Conclusion: RLRC在压缩VLA模型方面优于现有方法，具有实际部署潜力。

Abstract: Vision-Language-Action models (VLA) have demonstrated remarkable capabilities
and promising potential in solving complex robotic manipulation tasks. However,
their substantial parameter sizes and high inference latency pose significant
challenges for real-world deployment, particularly on resource-constrained
robotic platforms. To address this issue, we begin by conducting an extensive
empirical study to explore the effectiveness of model compression techniques
when applied to VLAs. Building on the insights gained from these preliminary
experiments, we propose RLRC, a three-stage recovery method for compressed
VLAs, including structured pruning, performance recovery based on SFT and RL,
and further quantization. RLRC achieves up to an 8x reduction in memory usage
and a 2.3x improvement in inference throughput, while maintaining or even
surpassing the original VLA's task success rate. Extensive experiments show
that RLRC consistently outperforms existing compression baselines,
demonstrating strong potential for on-device deployment of VLAs. Project
website: https://rlrc-vla.github.io

</details>


### [333] [Learning to Control an Android Robot Head for Facial Animation](https://arxiv.org/abs/2412.13641)
*Marcel Heisler,Christian Becker-Asano*

Key words: 机器人头部, 面部表情, 3D标志点, 学习算法, 用户偏好

TL;DR: 本文通过改进输入特征（使用3D标志点及其距离代替面部动作单元），优化了机器人头部表情映射，并在多数情况下获得用户偏好。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 丰富多样的面部表情对类人机器人至关重要，但目前手动定义或自动学习方法仍有改进空间。

Method: 采用3D标志点及其成对距离作为学习算法的输入，替代原有的面部动作单元。

Result: 在线调查显示，多数参与者更倾向于改进后的表情映射方法。

Conclusion: 尽管初步成效显著，但仍需进一步优化以提升表情映射效果。

Abstract: The ability to display rich facial expressions is crucial for human-like
robotic heads. While manually defining such expressions is intricate, there
already exist approaches to automatically learn them. In this work one such
approach is applied to evaluate and control a robot head different from the one
in the original study. To improve the mapping of facial expressions from human
actors onto a robot head, it is proposed to use 3D landmarks and their pairwise
distances as input to the learning algorithm instead of the previously used
facial action units. Participants of an online survey preferred mappings from
our proposed approach in most cases, though there are still further
improvements required.

</details>


### [334] [RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models](https://arxiv.org/abs/2506.17811)
*Jacky Kwok,Christopher Agia,Rohan Sinha,Matt Foutter,Shulu Li,Ion Stoica,Azalia Mirhoseini,Marco Pavone*

Key words: 视觉语言动作模型，测试时缩放，鲁棒性，合成数据，高斯扰动

TL;DR: 论文探讨了如何通过测试时采样和验证提升视觉语言动作（VLA）模型的鲁棒性，提出了RoboMonkey框架，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 尽管VLA模型在视觉运动控制中表现出色，但在非结构化现实环境中的鲁棒性仍是挑战。本研究旨在通过测试时缩放提升其性能和泛化能力。

Method: 提出RoboMonkey框架，通过高斯扰动和多数投票构建动作提议分布，并利用基于VLM的验证器选择最优动作；设计了合成数据生成流水线训练验证器。

Result: RoboMonkey显著提升了VLA模型的性能，在分布外任务上提升了25%，分布内任务上提升了8%；结合微调VLA和验证器还可额外提升7%。

Conclusion: 测试时缩放和验证可以有效增强VLA模型的鲁棒性和适应性，为实际部署提供了重要改进途径。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities
in visuomotor control, yet ensuring their robustness in unstructured real-world
environments remains a persistent challenge. In this paper, we investigate
test-time scaling through the lens of sampling and verification as means to
enhance the robustness and generalization of VLAs. We first demonstrate that
the relationship between action error and the number of generated samples
follows an exponentiated power law across a range of VLAs, indicating the
existence of inference-time scaling laws. Building on these insights, we
introduce RoboMonkey, a test-time scaling framework for VLAs. At deployment,
RoboMonkey samples a small set of actions from a VLA, applies Gaussian
perturbation and majority voting to construct an action proposal distribution,
and then uses a Vision Language Model (VLM)-based verifier to select the
optimal action. We propose a synthetic data generation pipeline for training
such VLM-based action verifiers, and demonstrate that scaling the synthetic
dataset consistently improves verification and downstream accuracy. Through
extensive simulated and hardware experiments, we show that pairing existing
VLAs with RoboMonkey yields significant performance gains, achieving a 25%
absolute improvement on out-of-distribution tasks and 8% on in-distribution
tasks. Additionally, when adapting to new robot setups, we show that
fine-tuning both VLAs and action verifiers yields a 7% performance increase
compared to fine-tuning VLAs alone.

</details>


### [335] [Learning to Dock: A Simulation-based Study on Closing the Sim2Real Gap in Autonomous Underwater Docking](https://arxiv.org/abs/2506.17823)
*Kevin Chang,Rakesh Vivekanandan,Noah Pragin,Sean Bullock,Geoffrey Hollinger*

Key words: 自主水下航行器（AUV），强化学习，模拟与现实差距（sim2real gap），随机化技术，历史条件控制器

TL;DR: 本文通过模拟研究探索了减少AUV（自主水下航行器）在动态和不确定环境中对接的模拟与现实差距的方法，重点研究了不同载荷下的对接问题。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 自主水下航行器在动态和不确定环境中的对接是水下机器人领域的关键挑战，强化学习虽然是一种有前景的控制方法，但模拟与现实的差距（sim2real gap）会导致性能显著下降。

Method: 研究通过训练多种控制器并在真实干扰下评估它们，探索了包括随机化技术和历史条件控制器在内的现有方法，以提高对接的鲁棒性。

Result: 研究发现为减少sim2real gap提供了见解，特别是在对接控制器训练和应对不同载荷方面。

Conclusion: 研究不仅提供了减少模拟与现实差距的方法，还为海洋机器人社区未来的研究方向指出了有益的方向。

Abstract: Autonomous Underwater Vehicle (AUV) docking in dynamic and uncertain
environments is a critical challenge for underwater robotics. Reinforcement
learning is a promising method for developing robust controllers, but the
disparity between training simulations and the real world, or the sim2real gap,
often leads to a significant deterioration in performance. In this work, we
perform a simulation study on reducing the sim2real gap in autonomous docking
through training various controllers and then evaluating them under realistic
disturbances. In particular, we focus on the real-world challenge of docking
under different payloads that are potentially outside the original training
distribution. We explore existing methods for improving robustness including
randomization techniques and history-conditioned controllers. Our findings
provide insights into mitigating the sim2real gap when training docking
controllers. Furthermore, our work indicates areas of future research that may
be beneficial to the marine robotics community.

</details>


### [336] [Generative Grasp Detection and Estimation with Concept Learning-based Safety Criteria](https://arxiv.org/abs/2506.17842)
*Al-Harith Farhad,Khalil Abuibaid,Christiane Plociennik,Achim Wagner,Martin Ruskowski*

Key words: 协作机器人,可解释AI,神经网络,抓取算法,工业应用

TL;DR: 提出了一种用于协作机器人抓取的透明算法，结合可解释AI方法以提高可靠性和安全性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决神经网络的复杂性导致的黑箱问题，特别是在安全关键应用中。

Method: 集成了可解释AI方法，提取学习特征并与输入类别关联，作为额外安全标准。

Result: 在工业环境中测试，显示了方法的一致性和抓取位置的改进。

Conclusion: 该算法提高了协作机器人抓取的透明度和可靠性。

Abstract: Neural networks are often regarded as universal equations that can estimate
any function. This flexibility, however, comes with the drawback of high
complexity, rendering these networks into black box models, which is especially
relevant in safety-centric applications. To that end, we propose a pipeline for
a collaborative robot (Cobot) grasping algorithm that detects relevant tools
and generates the optimal grasp. To increase the transparency and reliability
of this approach, we integrate an explainable AI method that provides an
explanation for the underlying prediction of a model by extracting the learned
features and correlating them to corresponding classes from the input. These
concepts are then used as additional criteria to ensure the safe handling of
work tools. In this paper, we show the consistency of this approach and the
criterion for improving the handover position. This approach was tested in an
industrial environment, where a camera system was set up to enable a robot to
pick up certain tools and objects.

</details>


### [337] [Online Adaptation for Flying Quadrotors in Tight Formations](https://arxiv.org/abs/2506.17488)
*Pei-An Hsieh,Kong Yao Chee,M. Ani Hsieh*

Key words: 多旋翼飞机、紧密编队、气动干扰、自适应控制、MPC

TL;DR: L1 KNODE-DW MPC 是一种自适应混合专家学习控制框架，用于解决多旋翼飞机在紧密编队飞行中的气动干扰问题。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 紧密编队飞行中，复杂的气动尾流相互作用会导致飞机失稳，且这些效应难以建模和预测。

Method: 采用 L1 KNODE-DW MPC 框架，结合自适应模块和动态模型，以跟踪轨迹并适应时变气动干扰。

Result: 在三个多旋翼飞机编队测试中，该方法优于多种 MPC 基线，能保持飞机垂直对齐且近距离飞行。

Conclusion: L1 自适应模块与精确动态模型结合，能有效补偿未建模干扰。

Abstract: The task of flying in tight formations is challenging for teams of quadrotors
because the complex aerodynamic wake interactions can destabilize individual
team members as well as the team. Furthermore, these aerodynamic effects are
highly nonlinear and fast-paced, making them difficult to model and predict. To
overcome these challenges, we present L1 KNODE-DW MPC, an adaptive, mixed
expert learning based control framework that allows individual quadrotors to
accurately track trajectories while adapting to time-varying aerodynamic
interactions during formation flights. We evaluate L1 KNODE-DW MPC in two
different three-quadrotor formations and show that it outperforms several MPC
baselines. Our results show that the proposed framework is capable of enabling
the three-quadrotor team to remain vertically aligned in close proximity
throughout the flight. These findings show that the L1 adaptive module
compensates for unmodeled disturbances most effectively when paired with an
accurate dynamics model. A video showcasing our framework and the physical
experiments is available here: https://youtu.be/9QX1Q5Ut9Rs

</details>


### [338] [GeNIE: A Generalizable Navigation System for In-the-Wild Environments](https://arxiv.org/abs/2506.17960)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Jiaxuan Da,Nuowen Qian,Tram Minh Man,Harold Soh*

Key words: 导航系统、非结构化环境、泛化性、路径规划、机器人挑战

TL;DR: GeNIE是一个通用导航系统，用于复杂环境的可靠导航，集成了一种基于SAM2的可泛化通行性预测模型和路径融合策略，在ICRA 2025的Earth Rover Challenge中表现优异，获得第一名。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决在非结构化、多样化环境中可靠导航的挑战，尤其是在不同地形、天气和传感器配置下的性能需求。

Method: 结合基于SAM2的可泛化通行性预测模型和新型路径融合策略，增强在噪声和模糊环境中的规划稳定性。

Result: 在ICRA 2025的Earth Rover Challenge中，GeNIE表现卓越，以79%的最高分获得第一名，领先第二名17%，且全程无人工干预。

Conclusion: GeNIE为复杂环境导航设定了新基准，其代码、模型权重和数据集将公开以推动未来研究。

Abstract: Reliable navigation in unstructured, real-world environments remains a
significant challenge for embodied agents, especially when operating across
diverse terrains, weather conditions, and sensor configurations. In this paper,
we introduce GeNIE (Generalizable Navigation System for In-the-Wild
Environments), a robust navigation framework designed for global deployment.
GeNIE integrates a generalizable traversability prediction model built on SAM2
with a novel path fusion strategy that enhances planning stability in noisy and
ambiguous settings. We deployed GeNIE in the Earth Rover Challenge (ERC) at
ICRA 2025, where it was evaluated across six countries spanning three
continents. GeNIE took first place and achieved 79% of the maximum possible
score, outperforming the second-best team by 17%, and completed the entire
competition without a single human intervention. These results set a new
benchmark for robust, generalizable outdoor robot navigation. We will release
the codebase, pretrained model weights, and newly curated datasets to support
future research in real-world navigation.

</details>


### [339] [ADA-DPM: A Neural Descriptors-based Adaptive Noise Point Filtering Strategy for SLAM](https://arxiv.org/abs/2506.18016)
*Yongxin Shao,Binrui Wang,Aihong Tan*

Key words: LiDAR SLAM, 动态物体干扰, 噪声过滤, 自适应策略

TL;DR: 提出了一种自适应噪声过滤SLAM策略ADA-DPM，解决了动态物体干扰和噪声问题，提升了定位精度和系统鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 现有LiDAR SLAM方法在动态物体干扰、点云噪声和非结构化环境中需在定位精度与系统鲁棒性之间权衡，亟需一种解决该问题的策略。

Method: 1. 设计动态分割头（Dynamic Segmentation Head）消除动态特征点；2. 设计全局重要性评分头（Global Importance Scoring Head）自适应选择高贡献特征点；3. 构建跨层图内卷积模块（GLI-GCN）融合多尺度邻域结构。

Result: 在多个公开数据集上测试，取得了优异结果。

Conclusion: ADA-DPM在定位精度和鲁棒性方面均表现优秀。

Abstract: LiDAR SLAM has demonstrated significant application value in various fields,
including mobile robot navigation and high-precision map construction. However,
existing methods often need to make a trade-off between positioning accuracy
and system robustness when faced with dynamic object interference, point cloud
noise, and unstructured environments. To address this challenge, we propose an
adaptive noise filtering SLAM strategy-ADA-DPM, achieving excellent preference
in both aspects. We design the Dynamic Segmentation Head to predict the
category of feature points belonging to dynamic points, to eliminate dynamic
feature points; design the Global Importance Scoring Head to adaptively select
feature points with higher contribution and features while suppressing noise
interference; and construct the Cross Layer Intra-Graph Convolution Module
(GLI-GCN) to fuse multi-scale neighborhood structures, thereby enhancing the
discriminative ability of overlapping features. Finally, to further validate
the effectiveness of our method, we tested it on several publicly available
datasets and achieved outstanding results.

</details>


### [340] [Leveling the Playing Field: Carefully Comparing Classical and Learned Controllers for Quadrotor Trajectory Tracking](https://arxiv.org/abs/2506.17832)
*Pratik Kunapuli,Jake Welde,Dinesh Jayaraman,Vijay Kumar*

Key words: 强化学习, 几何控制器, 四旋翼飞行器, 轨迹跟踪, 性能比较

TL;DR: 该论文通过案例研究比较了强化学习（RL）和几何控制器（GC）在四旋翼飞行器轨迹跟踪中的性能，揭示了先前研究中RL偏向性比较的问题，并提出对称比较的实验协议，发现两种控制器在不同任务中各有所长，并开源了实现代码。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 研究动机是解决现有研究中强化学习（RL）和几何控制器（GC）性能比较的不对称问题，确保公平比较两者的表现。

Method: 通过案例研究（四旋翼飞行器轨迹跟踪），设计对称的实验协议，优化任务定义、数据输入和参数优化，改进RL和GC的实现。

Result: 结果发现RL和GC各有优势：GC在稳态误差上表现更好，RL在瞬态性能上更优；GC适合慢速任务，RL适合高敏捷性任务。

Conclusion: 研究结论强调对称比较的重要性，开源实现为未来研究提供了基准。

Abstract: Learning-based control approaches like reinforcement learning (RL) have
recently produced a slew of impressive results for tasks like quadrotor
trajectory tracking and drone racing. Naturally, it is common to demonstrate
the advantages of these new controllers against established methods like
analytical controllers. We observe, however, that reliably comparing the
performance of such very different classes of controllers is more complicated
than might appear at first sight. As a case study, we take up the problem of
agile tracking of an end-effector for a quadrotor with a fixed arm. We develop
a set of best practices for synthesizing the best-in-class RL and geometric
controllers (GC) for benchmarking. In the process, we resolve widespread
RL-favoring biases in prior studies that provide asymmetric access to: (1) the
task definition, in the form of an objective function, (2) representative
datasets, for parameter optimization, and (3) feedforward information,
describing the desired future trajectory. The resulting findings are the
following: our improvements to the experimental protocol for comparing learned
and classical controllers are critical, and each of the above asymmetries can
yield misleading conclusions. Prior works have claimed that RL outperforms GC,
but we find the gaps between the two controller classes are much smaller than
previously published when accounting for symmetric comparisons. Geometric
control achieves lower steady-state error than RL, while RL has better
transient performance, resulting in GC performing better in relatively slow or
less agile tasks, but RL performing better when greater agility is required.
Finally, we open-source implementations of geometric and RL controllers for
these aerial vehicles, implementing best practices for future development.
Website and code is available at https://pratikkunapuli.github.io/rl-vs-gc/

</details>


### [341] [Geometric Contact Flows: Contactomorphisms for Dynamics and Control](https://arxiv.org/abs/2506.17868)
*Andrea Testa,Søren Hauberg,Tamim Asfour,Leonel Rozo*

Key words: 复杂动力系统、黎曼几何、接触几何、哈密顿模型、机器人控制

TL;DR: 论文提出了一种基于黎曼和接触几何的框架（GFC），用于建模和预测复杂动力系统，尤其是在力和能量交换的场景中，通过构建接触哈密顿模型和接触同胚集合，实现了鲁棒的泛化和适应能力。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 复杂动力系统在流体动力学和机器人学等领域应用广泛，但由于几何约束和能量传递的复杂性，建模和预测具有挑战性。

Method: 提出Geometric Contact Flows（GFC）框架，利用黎曼和接触几何作为归纳偏置，构建接触哈密顿模型，并通过接触同胚集合适应目标动力学。

Result: 实验表明，该方法在物理系统动力学学习和机器人交互任务控制中表现优异。

Conclusion: GFC通过几何约束和能量守恒特性，显著提升了复杂动力系统的建模和预测能力。

Abstract: Accurately modeling and predicting complex dynamical systems, particularly
those involving force exchange and dissipation, is crucial for applications
ranging from fluid dynamics to robotics, but presents significant challenges
due to the intricate interplay of geometric constraints and energy transfer.
This paper introduces Geometric Contact Flows (GFC), a novel framework
leveraging Riemannian and Contact geometry as inductive biases to learn such
systems. GCF constructs a latent contact Hamiltonian model encoding desirable
properties like stability or energy conservation. An ensemble of
contactomorphisms then adapts this model to the target dynamics while
preserving these properties. This ensemble allows for uncertainty-aware
geodesics that attract the system's behavior toward the data support, enabling
robust generalization and adaptation to unseen scenarios. Experiments on
learning dynamics for physical systems and for controlling robots on
interaction tasks demonstrate the effectiveness of our approach.

</details>


### [342] [Newtonian and Lagrangian Neural Networks: A Comparison Towards Efficient Inverse Dynamics Identification](https://arxiv.org/abs/2506.17994)
*Minh Trinh,Andreas René Geist,Josefine Monnet,Stefan Vilceanu,Sebastian Trimpe,Christian Brecher*

Key words: 逆动力学模型, 神经网络, 工业机器人, 拉格朗日网络, 牛顿网络

TL;DR: 研究了拉格朗日和牛顿神经网络的性能差异，发现在估计电机扭矩时，拉格朗日网络不如牛顿网络有效。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 准确的逆动力学模型对工业机器人控制至关重要，但现有研究缺乏对拉格朗日和牛顿网络选择的指导。

Method: 结合神经回归与牛顿-欧拉和欧拉-拉格朗日方程，比较两种网络在工业机器人数据上的表现。

Result: 拉格朗日网络在估计电机扭矩时效果较差，因其未显式建模耗散扭矩。

Conclusion: 牛顿网络在估计电机扭矩时优于拉格朗日网络。

Abstract: Accurate inverse dynamics models are essential tools for controlling
industrial robots. Recent research combines neural network regression with
inverse dynamics formulations of the Newton-Euler and the Euler-Lagrange
equations of motion, resulting in so-called Newtonian neural networks and
Lagrangian neural networks, respectively. These physics-informed models seek to
identify unknowns in the analytical equations from data. Despite their
potential, current literature lacks guidance on choosing between Lagrangian and
Newtonian networks. In this study, we show that when motor torques are
estimated instead of directly measuring joint torques, Lagrangian networks
prove less effective compared to Newtonian networks as they do not explicitly
model dissipative torques. The performance of these models is compared to
neural network regression on data of a MABI MAX 100 industrial robot.

</details>


### [343] [RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies](https://arxiv.org/abs/2506.18123)
*Pranav Atreya,Karl Pertsch,Tony Lee,Moo Jin Kim,Arhan Jain,Artur Kuramshin,Clemens Eppner,Cyrus Neary,Edward Hu,Fabio Ramos,Jonathan Tremblay,Kanav Arora,Kirsty Ellis,Luca Macesanu,Matthew Leonard,Meedeum Cho,Ozgur Aslan,Shivin Dass,Jie Wang,Xingfang Yuan,Xuning Yang,Abhishek Gupta,Dinesh Jayaraman,Glen Berseth,Kostas Daniilidis,Roberto Martin-Martin,Youngwoon Lee,Percy Liang,Chelsea Finn,Sergey Levine*

Key words: 机器人评估,通用策略,众包,分布式网络,成对比较

TL;DR: 该论文提出了一种名为RoboArena的新方法，用于在真实世界中评估通用机器人策略的扩展性。通过分布式网络众包评估，避免了传统固定任务和环境的限制，提升评估的多样性、可扩展性和信任度。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 传统机器人评估方法依赖固定任务或集中式挑战赛，难以扩展到评估通用策略的多样性和范围。RoboArena旨在解决这一问题。

Method: 提出众包评估方法，由分布式网络的评估者自由选择任务和环境，基于双盲成对比较，通过聚合偏好反馈生成策略排名。

Result: 在七个学术机构中进行了600多次成对评估，证明RoboArena比传统方法更准确、可扩展、可靠和值得信赖。

Conclusion: RoboArena为通用机器人策略的比较提供了更开放和可扩展的评估框架，未来可促进更广泛的社区参与。

Abstract: Comprehensive, unbiased, and comparable evaluation of modern generalist
policies is uniquely challenging: existing approaches for robot benchmarking
typically rely on heavy standardization, either by specifying fixed evaluation
tasks and environments, or by hosting centralized ''robot challenges'', and do
not readily scale to evaluating generalist policies across a broad range of
tasks and environments. In this work, we propose RoboArena, a new approach for
scalable evaluation of generalist robot policies in the real world. Instead of
standardizing evaluations around fixed tasks, environments, or locations, we
propose to crowd-source evaluations across a distributed network of evaluators.
Importantly, evaluators can freely choose the tasks and environments they
evaluate on, enabling easy scaling of diversity, but they are required to
perform double-blind evaluations over pairs of policies. Then, by aggregating
preference feedback from pairwise comparisons across diverse tasks and
environments, we can derive a ranking of policies. We instantiate our approach
across a network of evaluators at seven academic institutions using the DROID
robot platform. Through more than 600 pairwise real-robot evaluation episodes
across seven generalist policies, we demonstrate that our crowd-sourced
approach can more accurately rank the performance of existing generalist
policies than conventional, centralized evaluation approaches, while being more
scalable, resilient, and trustworthy. We open our evaluation network to the
community and hope that it can enable more accessible comparisons of generalist
robot policies.

</details>


### [344] [A Motivational Architecture for Open-Ended Learning Challenges in Robots](https://arxiv.org/abs/2506.18454)
*Alejandro Romero,Gianluca Baldassarre,Richard J. Duro,Vieri Giuliano Santucci*

Key words: H-GRAIL, 开放学习, 内在动机, 分层架构, 机器人

TL;DR: H-GRAIL是一种分层架构，通过内在动机和互联学习机制实现自主目标发现、技能学习及环境适应。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 开发能够在动态复杂环境中自主交互的智能体，以应对任务结构变化和无法依赖先验知识的挑战。

Method: 提出H-GRAIL分层架构，结合多类型内在动机和互联学习机制。

Result: 在真实机器人场景中验证了H-GRAIL能有效解决开放学习问题。

Conclusion: H-GRAIL为解决开放学习问题提供了一个集成解决方案。

Abstract: Developing agents capable of autonomously interacting with complex and
dynamic environments, where task structures may change over time and prior
knowledge cannot be relied upon, is a key prerequisite for deploying artificial
systems in real-world settings. The open-ended learning framework identifies
the core challenges for creating such agents, including the ability to
autonomously generate new goals, acquire the necessary skills (or curricula of
skills) to achieve them, and adapt to non-stationary environments. While many
existing works tackles various aspects of these challenges in isolation, few
propose integrated solutions that address them simultaneously. In this paper,
we introduce H-GRAIL, a hierarchical architecture that, through the use of
different typologies of intrinsic motivations and interconnected learning
mechanisms, autonomously discovers new goals, learns the required skills for
their achievement, generates skill sequences for tackling interdependent tasks,
and adapts to non-stationary environments. We tested H-GRAIL in a real robotic
scenario, demonstrating how the proposed solutions effectively address the
various challenges of open-ended learning.

</details>


### [345] [Robots and Children that Learn Together : Improving Knowledge Retention by Teaching Peer-Like Interactive Robots](https://arxiv.org/abs/2506.18365)
*Imene Tarakli,Samuele Vinanzi,Richard Moore,Alessandro Di Nuovo*

Key words: 学习-教学（LbT）、社交机器人、交互式强化学习（Interactive RL）、教育技术、元认知

TL;DR: 研究了在真实课堂中使用自主社交机器人进行“学习-教学”互动增强学习的效果，发现教学机器人显著提高了学生的知识保留率，尤其是对低基础学生效果更佳。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 探索如何通过自主社交机器人在真实课堂中实现学习-教学互动，弥补以往研究中仅依赖脚本或人为操控的不足。

Method: 采用交互式强化学习（Interactive RL）作为认知模型，对58名小学生进行实验，分为教学机器人和自主学习两组，测试法语词汇和语法的学习效果。

Result: 教学机器人组在知识保留率上显著高于自主学习组，特别是在语法任务中；低基础学生受益最多。

Conclusion: 研究证实了自主社交机器人作为适应性伙伴在教学中的可行性和有效性，为学习-教学理论提供了新见解。

Abstract: Despite growing interest in Learning-by-Teaching (LbT), few studies have
explored how this paradigm can be implemented with autonomous, peer-like social
robots in real classrooms. Most prior work has relied on scripted or
Wizard-of-Oz behaviors, limiting our understanding of how real-time,
interactive learning can be supported by artificial agents. This study
addresses this gap by introducing Interactive Reinforcement Learning (RL) as a
cognitive model for teachable social robots. We conducted two between-subject
experiments with 58 primary school children, who either taught a robot or
practiced independently on a tablet while learning French vocabulary
(memorization) and grammatical rules (inference). The robot, powered by
Interactive RL, learned from the child's evaluative feedback. Children in the
LbT condition achieved significantly higher retention gains compared to those
in the self-practice condition, especially on the grammar task. Learners with
lower prior knowledge benefited most from teaching the robot. Behavioural
metrics revealed that children adapted their teaching strategies over time and
engaged more deeply during inference tasks. This work makes two contributions:
(1) it introduces Interactive RL as a pedagogically effective and scalable
model for peer-robot learning, and (2) it demonstrates, for the first time, the
feasibility of deploying multiple autonomous robots simultaneously in real
classrooms. These findings extend theoretical understanding of LbT by showing
that social robots can function not only as passive tutees but as adaptive
partners that enhance meta-cognitive engagement and long-term learning
outcomes.

</details>


### [346] [Learning Physical Systems: Symplectification via Gauge Fixing in Dirac Structures](https://arxiv.org/abs/2506.18812)
*Aristotelis Papatheodorou,Pranav Vaidhyanathan,Natalia Ares,Ioannis Havoutis*

Key words: 物理信息深度学习, 辛几何, 约束系统, 四足机器人, 几何机器学习

TL;DR: 论文提出了Presymplectification Networks（PSNs），通过狄拉克结构学习辛提升，解决了耗散和完整约束系统中辛几何退化的问题。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 在耗散和完整约束系统中，传统的辛形式会退化，导致稳定性不足，作者提出PSNs以恢复非退化辛几何。

Method: PSNs结合循环编码器和流匹配目标，学习高维流形上的相空间动力学，并附加轻量级SympNet用于守恒约束轨迹。

Result: 在ANYmal四足机器人上验证了方法有效性，首次填补了约束耗散系统与辛学习之间的空白。

Conclusion: PSNs是首个能处理复杂约束耗散系统的几何机器学习框架，为基于第一性原理的模型提供了新方向。

Abstract: Physics-informed deep learning has achieved remarkable progress by embedding
geometric priors, such as Hamiltonian symmetries and variational principles,
into neural networks, enabling structure-preserving models that extrapolate
with high accuracy. However, in systems with dissipation and holonomic
constraints, ubiquitous in legged locomotion and multibody robotics, the
canonical symplectic form becomes degenerate, undermining the very invariants
that guarantee stability and long-term prediction. In this work, we tackle this
foundational limitation by introducing Presymplectification Networks (PSNs),
the first framework to learn the symplectification lift via Dirac structures,
restoring a non-degenerate symplectic geometry by embedding constrained systems
into a higher-dimensional manifold. Our architecture combines a recurrent
encoder with a flow-matching objective to learn the augmented phase-space
dynamics end-to-end. We then attach a lightweight Symplectic Network (SympNet)
to forecast constrained trajectories while preserving energy, momentum, and
constraint satisfaction. We demonstrate our method on the dynamics of the
ANYmal quadruped robot, a challenging contact-rich, multibody system. To the
best of our knowledge, this is the first framework that effectively bridges the
gap between constrained, dissipative mechanical systems and symplectic
learning, unlocking a whole new class of geometric machine learning models,
grounded in first principles yet adaptable from data.

</details>


### [347] [NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed Target Tracking in Unstructured GPS-Denied Environments](https://arxiv.org/abs/2506.18689)
*Alessandro Saviolo,Giuseppe Loianno*

Key words: 目标跟踪，无GPS环境，机载感知，视觉-惯性估计，避障导航

TL;DR: NOVA是一种全自主、以目标为中心的框架，仅使用立体相机和IMU实现目标跟踪和避障导航，在无GPS和结构化的环境中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 在无GPS和非结构化环境中实现自主空中目标跟踪是机器人领域的核心挑战，现有方法依赖外部设备，限制了实际应用。

Method: NOVA整合轻量级目标检测、立体深度补全和直方图滤波，结合视觉-惯性状态估计和非线性模型预测控制，实现实时避障。

Result: 实验验证表明，NOVA在复杂环境下（如城市迷宫和森林小径）能以超过50 km/h的速度稳定跟踪目标。

Conclusion: NOVA证明了仅靠机载传感器即可实现高速目标跟踪，无需依赖外部定位或环境假设。

Abstract: Autonomous aerial target tracking in unstructured and GPS-denied environments
remains a fundamental challenge in robotics. Many existing methods rely on
motion capture systems, pre-mapped scenes, or feature-based localization to
ensure safety and control, limiting their deployment in real-world conditions.
We introduce NOVA, a fully onboard, object-centric framework that enables
robust target tracking and collision-aware navigation using only a stereo
camera and an IMU. Rather than constructing a global map or relying on absolute
localization, NOVA formulates perception, estimation, and control entirely in
the target's reference frame. A tightly integrated stack combines a lightweight
object detector with stereo depth completion, followed by histogram-based
filtering to infer robust target distances under occlusion and noise. These
measurements feed a visual-inertial state estimator that recovers the full
6-DoF pose of the robot relative to the target. A nonlinear model predictive
controller (NMPC) plans dynamically feasible trajectories in the target frame.
To ensure safety, high-order control barrier functions are constructed online
from a compact set of high-risk collision points extracted from depth, enabling
real-time obstacle avoidance without maps or dense representations. We validate
NOVA across challenging real-world scenarios, including urban mazes, forest
trails, and repeated transitions through buildings with intermittent GPS loss
and severe lighting changes that disrupt feature-based localization. Each
experiment is repeated multiple times under similar conditions to assess
resilience, showing consistent and reliable performance. NOVA achieves agile
target following at speeds exceeding 50 km/h. These results show that
high-speed vision-based tracking is possible in the wild using only onboard
sensing, with no reliance on external localization or environment assumptions.

</details>


### [348] [MinD: Unified Visual Imagination and Control via Hierarchical World Models](https://arxiv.org/abs/2506.18897)
*Xiaowei Chi,Kuangzhi Ge,Jiaming Liu,Siyuan Zhou,Peidong Jia,Zichen He,Yuzhen Liu,Tingguang Li,Lei Han,Sirui Han,Shanghang Zhang,Yike Guo*

Key words: 视频生成模型, 扩散模型, 机器人操控, 世界建模, MinD

TL;DR: 本文提出了MinD框架，通过分层扩散模型解决视频生成模型在实际应用中的速度慢与一致性差的问题，实现了低延迟的实时操控。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 视频生成模型（VGMs）在机器人领域具有潜力，但因生成速度慢和预测视频与可执行动作一致性差而受限，MinD旨在解决这些问题。

Method: 采用双系统设计：低频VGMs提取视频预测特征，高频扩散策略实现实时交互，并引入DiffMatcher模块协调两者。

Result: MinD在RL-Bench中实现63%+的操控性能，表现优于现有方法。

Conclusion: MinD通过协调视频生成与实时动作，推动了机器人统一世界建模的发展。

Abstract: Video generation models (VGMs) offer a promising pathway for unified world
modeling in robotics by integrating simulation, prediction, and manipulation.
However, their practical application remains limited due to (1) slowgeneration
speed, which limits real-time interaction, and (2) poor consistency between
imagined videos and executable actions. To address these challenges, we propose
Manipulate in Dream (MinD), a hierarchical diffusion-based world model
framework that employs a dual-system design for vision-language manipulation.
MinD executes VGM at low frequencies to extract video prediction features,
while leveraging a high-frequency diffusion policy for real-time interaction.
This architecture enables low-latency, closed-loop control in manipulation with
coherent visual guidance. To better coordinate the two systems, we introduce a
video-action diffusion matching module (DiffMatcher), with a novel co-training
strategy that uses separate schedulers for each diffusion model. Specifically,
we introduce a diffusion-forcing mechanism to DiffMatcher that aligns their
intermediate representations during training, helping the fast action model
better understand video-based predictions. Beyond manipulation, MinD also
functions as a world simulator, reliably predicting task success or failure in
latent space before execution. Trustworthy analysis further shows that VGMs can
preemptively evaluate task feasibility and mitigate risks. Extensive
experiments across multiple benchmarks demonstrate that MinD achieves
state-of-the-art manipulation (63%+) in RL-Bench, advancing the frontier of
unified world modeling in robotics.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [349] [Embedded FPGA Acceleration of Brain-Like Neural Networks: Online Learning to Scalable Inference](https://arxiv.org/abs/2506.18530)
*Muhammad Ihsan Al Hafiz,Naresh Ravichandran,Anders Lansner,Pawel Herman,Artur Podobas*

Key words: 边缘AI、BCPNN、FPGA加速器、低功耗、类脑计算

TL;DR: 该论文提出了首个嵌入式FPGA加速器，用于在Zynq UltraScale+ SoC上实现BCPNN，支持在线学习和推理，显著降低了延迟和能耗。

<details>
  <summary>Details</summary>

Main category: cs.AR

Motivation: 边缘AI应用需要低能耗、适应性强且不依赖云连接的模型，传统深度学习方法能耗高且过于参数化，而现有BCPNN实现受限于硬件架构。

Method: 使用高层次综合技术开发了嵌入式FPGA加速器，支持变精度和混合精度，实现了在线学习和推理功能。

Result: 在MNIST、肺炎和乳腺癌数据集上，加速器比ARM基线减少了17.5倍延迟和94%能耗，且保持了准确率。

Conclusion: 该研究填补了类脑学习与边缘设备实际部署之间的技术空白，推动了低功耗边缘智能的发展。

Abstract: Edge AI applications increasingly require models that can learn and adapt
on-device with minimal energy budget. Traditional deep learning models, while
powerful, are often overparameterized, energy-hungry, and dependent on cloud
connectivity. Brain-Like Neural Networks (BLNNs), such as the Bayesian
Confidence Propagation Neural Network (BCPNN), propose a neuromorphic
alternative by mimicking cortical architecture and biologically-constrained
learning. They offer sparse architectures with local learning rules and
unsupervised/semi-supervised learning, making them well-suited for low-power
edge intelligence. However, existing BCPNN implementations rely on GPUs or
datacenter FPGAs, limiting their applicability to embedded systems. This work
presents the first embedded FPGA accelerator for BCPNN on a Zynq UltraScale+
SoC using High-Level Synthesis. We implement both online learning and
inference-only kernels with support for variable and mixed precision. Evaluated
on MNIST, Pneumonia, and Breast Cancer datasets, our accelerator achieves up to
17.5x latency and 94% energy savings over ARM baselines, without sacrificing
accuracy. This work enables practical neuromorphic computing on edge devices,
bridging the gap between brain-like learning and real-world deployment.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [350] [Zero-Shot Cognitive Impairment Detection from Speech Using AudioLLM](https://arxiv.org/abs/2506.17351)
*Mostafa Shahin,Beena Ahmed,Julien Epps*

Key words: 认知障碍, 语音检测, 零样本学习, AudioLLM, 多语言

TL;DR: 提出了一种基于Qwen2-Audio AudioLLM的零样本认知障碍检测方法，利用语音作为生物标志物，无需手动标注，跨语言和数据集表现良好。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 认知障碍（CI）是日益严重的公共卫生问题，早期检测对干预至关重要。传统方法依赖有监督模型，需要手动标注且泛化性差。

Method: 使用Qwen2-Audio AudioLLM模型，通过设计提示指令，对语音样本进行分类，检测认知障碍。

Result: 在英语和多语言数据集上，零样本方法性能与有监督方法相当，具有跨语言、任务和数据集的泛化性和一致性。

Conclusion: 零样本语音检测方法为CI检测提供了高效、通用的解决方案。

Abstract: Cognitive impairment (CI) is of growing public health concern, and early
detection is vital for effective intervention. Speech has gained attention as a
non-invasive and easily collectible biomarker for assessing cognitive decline.
Traditional CI detection methods typically rely on supervised models trained on
acoustic and linguistic features extracted from speech, which often require
manual annotation and may not generalise well across datasets and languages. In
this work, we propose the first zero-shot speech-based CI detection method
using the Qwen2- Audio AudioLLM, a model capable of processing both audio and
text inputs. By designing prompt-based instructions, we guide the model in
classifying speech samples as indicative of normal cognition or cognitive
impairment. We evaluate our approach on two datasets: one in English and
another multilingual, spanning different cognitive assessment tasks. Our
results show that the zero-shot AudioLLM approach achieves performance
comparable to supervised methods and exhibits promising generalizability and
consistency across languages, tasks, and datasets.

</details>


### [351] [AI-Generated Song Detection via Lyrics Transcripts](https://arxiv.org/abs/2506.18488)
*Markus Frohmann,Elena V. Epure,Gabriel Meseguer-Brocal,Markus Schedl,Romain Hennequin*

Key words: AI生成音乐检测、自动语音识别、Whisper large-v2、LLM2Vec、鲁棒性

TL;DR: 本文提出了一种通过自动语音识别（ASR）模型转录歌曲来检测AI生成音乐的方法，解决了现有音频检测方法在通用性和抗干扰性上的不足。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 针对AI生成的音乐在音频检测中通用性差且易受干扰的问题，以及歌词检测方法依赖完美歌词的限制，本研究旨在填补实际应用中的这一空白。

Method: 使用多种检测器，结合通用ASR模型（如Whisper large-v2和LLM2Vec嵌入）对歌曲进行转录和分析。

Result: 实验表明，该方法在多语言和多流派的歌词检测中表现出色，且在音频受干扰或被不同音乐生成器生成时，比现有音频检测方法更具鲁棒性。

Conclusion: 通过ASR模型转录歌词的检测方法在AI生成音乐的检测中具有更高的实用性和鲁棒性。

Abstract: The recent rise in capabilities of AI-based music generation tools has
created an upheaval in the music industry, necessitating the creation of
accurate methods to detect such AI-generated content. This can be done using
audio-based detectors; however, it has been shown that they struggle to
generalize to unseen generators or when the audio is perturbed. Furthermore,
recent work used accurate and cleanly formatted lyrics sourced from a lyrics
provider database to detect AI-generated music. However, in practice, such
perfect lyrics are not available (only the audio is); this leaves a substantial
gap in applicability in real-life use cases. In this work, we instead propose
solving this gap by transcribing songs using general automatic speech
recognition (ASR) models. We do this using several detectors. The results on
diverse, multi-genre, and multi-lingual lyrics show generally strong detection
performance across languages and genres, particularly for our best-performing
model using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that
our method is more robust than state-of-the-art audio-based ones when the audio
is perturbed in different ways and when evaluated on different music
generators. Our code is available at
https://github.com/deezer/robust-AI-lyrics-detection.

</details>


### [352] [Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich Transcripts](https://arxiv.org/abs/2506.18510)
*Duygu Altinok*

Key words: 不流畅检测, 大型语言模型, 语音转录, 时间戳, 鲁棒性

TL;DR: 利用大型语言模型（LLM）处理音频和文本输入，提出了一种将不流畅语音标记为带有时间戳的显式令牌的方法，生成完全注释的不流畅转录本。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 提升自动语音和语言处理系统的性能，推动更具包容性的语音和语言技术发展。

Method: 整合音频编码器的声学表征与不同质量的文本输入（如干净转录、时间对齐转录或语音识别模型输出），利用LLM生成带有不流畅注释的转录本。

Result: 实验表明，文本输入无需完美，只要包含时间戳相关线索，LLM即可有效处理并生成完整注释的不流畅转录本。

Conclusion: 大型语言模型在处理不流畅语音时表现出强大的鲁棒性，能够有效利用不完美的提示生成高质量转录本。

Abstract: Accurate detection of disfluencies in spoken language is crucial for
enhancing the performance of automatic speech and language processing systems,
as well as fostering the development of more inclusive speech and language
technologies. Leveraging the growing trend of large language models (LLMs) as
versatile learners capable of processing both lexical and non-lexical inputs
(e.g., audio and video), we propose a novel approach to transcribing
disfluencies as explicit tokens with timestamps, enabling the generation of
fully annotated disfluency-rich transcripts. Our method integrates acoustic
representations extracted from an audio encoder with textual inputs of varying
quality: clean transcriptions without disfluencies, time-aligned transcriptions
from aligners, or outputs from phoneme-based ASR models -- all of which may
contain imperfections. Importantly, our experiments demonstrate that textual
inputs do not need to be flawless. As long as they include timestamp-related
cues, LLMs can effectively smooth the input and produce fully
disfluency-annotated transcripts, underscoring their robustness in handling
imperfect hints.

</details>


### [353] [USAD: Universal Speech and Audio Representation via Distillation](https://arxiv.org/abs/2506.18843)
*Heng-Jui Chang,Saurabhchand Bhati,James Glass,Alexander H. Liu*

Key words: 自监督学习, 语音处理, 音频分类, 分层蒸馏, 通用模型

TL;DR: USAD提出了一种统一的自监督学习方法，整合语音和非语音音频任务，通过分层蒸馏训练单一模型，在多个基准测试中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 解决自监督学习中模型领域专一性问题，实现语音、声音和音乐的统一表示学习。

Method: 采用分层蒸馏技术，从领域专用模型中提取知识，训练一个通用的学生模型。

Result: 在SUPERB和HEAR等基准测试中接近最先进水平，适用于多种音频任务。

Conclusion: USAD是一种高效的统一音频表示学习方法，具备广泛应用潜力。

Abstract: Self-supervised learning (SSL) has revolutionized audio representations, yet
models often remain domain-specific, focusing on either speech or non-speech
tasks. In this work, we present Universal Speech and Audio Distillation (USAD),
a unified approach to audio representation learning that integrates diverse
audio types - speech, sound, and music - into a single model. USAD employs
efficient layer-to-layer distillation from domain-specific SSL models to train
a student on a comprehensive audio dataset. USAD offers competitive performance
across various benchmarks and datasets, including frame and instance-level
speech processing tasks, audio tagging, and sound classification, achieving
near state-of-the-art results with a single encoder on SUPERB and HEAR
benchmarks.

</details>


### [354] [From Generality to Mastery: Composer-Style Symbolic Music Generation via Large-Scale Pre-training](https://arxiv.org/abs/2506.17497)
*Mingyang Yao,Ke Chen*

Key words: 音乐生成, 作曲家风格, 预训练, 微调, REMI

TL;DR: 本文研究如何通过在大规模音乐语料库上预训练模型，再通过轻量级适配器模块在小规模作曲家数据集上微调，以提升作曲家风格的音乐生成能力。实验表明该方法在风格准确性和音乐性上优于基线模型。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 由于特定作曲家风格的音乐数据稀缺，限制了生成模型在风格和基础音乐元素上的建模能力，因此研究如何利用广泛音乐知识提升特定风格的生成效果。

Method: 采用两阶段训练范式：先在大规模流行、民谣和古典音乐语料库上预训练REMI音乐生成模型，再在小规模作曲家数据集上通过轻量级适配器模块进行微调。

Result: 实验结果表明，该方法在风格准确性和音乐性上优于基线模型，能够更精确地建模作曲家风格并提升音乐美感。

Conclusion: 通过结合通用预训练和风格微调，可以显著提升作曲家风格的音乐生成能力，同时揭示了模型从通用知识到风格理解的构建过程。

Abstract: Despite progress in controllable symbolic music generation, data scarcity
remains a challenge for certain control modalities. Composer-style music
generation is a prime example, as only a few pieces per composer are available,
limiting the modeling of both styles and fundamental music elements (e.g.,
melody, chord, rhythm). In this paper, we investigate how general music
knowledge learned from a broad corpus can enhance the mastery of specific
composer styles, with a focus on piano piece generation. Our approach follows a
two-stage training paradigm. First, we pre-train a REMI-based music generation
model on a large corpus of pop, folk, and classical music. Then, we fine-tune
it on a small, human-verified dataset from four renowned composers, namely
Bach, Mozart, Beethoven, and Chopin, using a lightweight adapter module to
condition the model on style indicators. To evaluate the effectiveness of our
approach, we conduct both objective and subjective evaluations on style
accuracy and musicality. Experimental results demonstrate that our method
outperforms ablations and baselines, achieving more precise composer-style
modeling and better musical aesthetics. Additionally, we provide observations
on how the model builds music concepts from the generality pre-training and
refines its stylistic understanding through the mastery fine-tuning.

</details>


### [355] [CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation Learning](https://arxiv.org/abs/2506.17818)
*Angelos-Nikolaos Kanatas,Charilaos Papaioannou,Alexandros Potamianos*

Key words: 音乐基础模型、跨文化表示学习、多文化适应、任务算术、世界音乐

TL;DR: 本文介绍了CultureMERT-95M，一种多文化适应的音乐基础模型，通过两阶段预训练策略提升跨文化音乐表示学习，显著优于现有技术，尤其在非西方音乐任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 尽管音乐基础模型在音频表示学习上取得了进展，但其在多样化音乐传统中的表现仍有局限。本文旨在通过多文化适应提升跨文化音乐表示学习的效果。

Method: 采用两阶段持续预训练策略，结合学习率重新加热和衰减，以及在650小时多文化音乐数据（希腊、土耳其、印度）上的训练。此外，研究了任务算术方法作为替代方案。

Result: 在非西方音乐自动标记任务中，平均ROC-AUC和AP提升了4.9%，超过了现有技术，同时西方基准测试上的性能几乎未受影响。多文化适应模型总体表现最佳。

Conclusion: 多文化适应的音乐基础模型显著提升了跨文化音乐表示学习的效果，并公开了模型以支持世界音乐研究。任务算术方法在多文化适应中表现同样出色。

Abstract: Recent advances in music foundation models have improved audio representation
learning, yet their effectiveness across diverse musical traditions remains
limited. We introduce CultureMERT-95M, a multi-culturally adapted foundation
model developed to enhance cross-cultural music representation learning and
understanding. To achieve this, we propose a two-stage continual pre-training
strategy that integrates learning rate re-warming and re-decaying, enabling
stable adaptation even with limited computational resources. Training on a
650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music
traditions, results in an average improvement of 4.9% in ROC-AUC and AP across
diverse non-Western music auto-tagging tasks, surpassing prior
state-of-the-art, with minimal forgetting on Western-centric benchmarks. We
further investigate task arithmetic, an alternative approach to multi-cultural
adaptation that merges single-culture adapted models in the weight space. Task
arithmetic performs on par with our multi-culturally trained model on
non-Western auto-tagging tasks and shows no regression on Western datasets.
Cross-cultural evaluation reveals that single-culture models transfer with
varying effectiveness across musical traditions, whereas the multi-culturally
adapted model achieves the best overall performance. To support research on
world music representation learning, we publicly release CultureMERT-95M and
CultureMERT-TA-95M, fostering the development of more culturally aware music
foundation models.

</details>


### [356] [Adaptive Control Attention Network for Underwater Acoustic Localization and Domain Adaptation](https://arxiv.org/abs/2506.17409)
*Quoc Thinh Vo,Joe Woods,Priontu Chowdhury,David K. Han*

Key words: 水下声源定位, CNN, Conformer, AGC, 多域学习

TL;DR: 论文提出了一种多分支网络架构，结合了CNN和Conformer的自注意力机制，用于准确预测水下移动声源的距离，并通过AGC层增强性能，优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 水下声源定位由于环境复杂性和动态性而极具挑战性，需克服高背景噪声、不规则几何形状等障碍。

Method: 使用CNN提取空间特征，Conformer捕捉时间依赖性，输入为log-mel谱图和GCC-PHAT特征，引入AGC层调整输入幅度。

Result: 在多域测试中，模型仅需少量测试域数据微调即优于现有方法，为水下声音定位设立了新基准。

Conclusion: 提出的方法通过多分支网络和自适应增益控制，显著提升了水下声源定位的准确性和鲁棒性。

Abstract: Localizing acoustic sound sources in the ocean is a challenging task due to
the complex and dynamic nature of the environment. Factors such as high
background noise, irregular underwater geometries, and varying acoustic
properties make accurate localization difficult. To address these obstacles, we
propose a multi-branch network architecture designed to accurately predict the
distance between a moving acoustic source and a receiver, tested on real-world
underwater signal arrays. The network leverages Convolutional Neural Networks
(CNNs) for robust spatial feature extraction and integrates Conformers with
self-attention mechanism to effectively capture temporal dependencies. Log-mel
spectrogram and generalized cross-correlation with phase transform (GCC-PHAT)
features are employed as input representations. To further enhance the model
performance, we introduce an Adaptive Gain Control (AGC) layer, that adaptively
adjusts the amplitude of input features, ensuring consistent energy levels
across varying ranges, signal strengths, and noise conditions. We assess the
model's generalization capability by training it in one domain and testing it
in a different domain, using only a limited amount of data from the test domain
for fine-tuning. Our proposed method outperforms state-of-the-art (SOTA)
approaches in similar settings, establishing new benchmarks for underwater
sound localization.

</details>


### [357] [Frequency-Weighted Training Losses for Phoneme-Level DNN-based Speech Enhancement](https://arxiv.org/abs/2506.18714)
*Nasser-Eddine Monir,Paul Magron,Romain Serizel*

Key words: 语音增强,感知损失,频域加权,SDR,FaSNet

TL;DR: 该论文提出了一种感知启发的损失函数变体，用于多通道语音增强，通过频域加权策略改善语音清晰度。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 传统损失函数如SDR可能无法保留语音频谱细节，影响音素可懂度，因此需要改进。

Method: 提出基于频域加权的SDR损失函数变体，包括固定和自适应加权策略，并用于FaSNet模型训练。

Result: 标准指标改进有限，但加权的感知指标显著提升，且频谱和音素分析显示辅音重建更好。

Conclusion: 频域加权策略能更好地保留关键声学线索，提升语音增强的感知质量。

Abstract: Recent advances in deep learning have significantly improved multichannel
speech enhancement algorithms, yet conventional training loss functions such as
the scale-invariant signal-to-distortion ratio (SDR) may fail to preserve
fine-grained spectral cues essential for phoneme intelligibility. In this work,
we propose perceptually-informed variants of the SDR loss, formulated in the
time-frequency domain and modulated by frequency-dependent weighting schemes.
These weights are designed to emphasize time-frequency regions where speech is
prominent or where the interfering noise is particularly strong. We investigate
both fixed and adaptive strategies, including ANSI band-importance weights,
spectral magnitude-based weighting, and dynamic weighting based on the relative
amount of speech and noise. We train the FaSNet multichannel speech enhancement
model using these various losses. Experimental results show that while standard
metrics such as the SDR are only marginally improved, their perceptual
frequency-weighted counterparts exhibit a more substantial improvement.
Besides, spectral and phoneme-level analysis indicates better consonant
reconstruction, which points to a better preservation of certain acoustic cues.

</details>


### [358] [MuseControlLite: Multifunctional Music Generation with Lightweight Conditioners](https://arxiv.org/abs/2506.18729)
*Fang-Duo Tsai,Shih-Lun Wu,Weijaw Lee,Sheng-Ping Yang,Bo-Rui Chen,Hao-Chung Cheng,Yi-Hsuan Yang*

Key words: text-to-music, positional embeddings, fine-tuning, control accuracy, lightweight

TL;DR: MuseControlLite是一种轻量化机制，通过时间变化的音乐属性和参考音频信号微调文本到音乐生成模型，提高了控制精度并减少了参数数量。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 为了解决文本到音乐生成模型在时间变化条件下的控制精度问题，提出了一种更高效的微调机制。

Method: 在解耦的交叉注意力层中添加旋转位置嵌入，显著提高了控制精度，同时减少了可训练参数数量。

Result: 实验表明，控制精度从56.6%提升至61.1%，且可训练参数仅为现有方法的1/6.75。

Conclusion: MuseControlLite在音乐属性控制、音频修复和扩展方面表现出更高的可控性和效率。

Abstract: We propose MuseControlLite, a lightweight mechanism designed to fine-tune
text-to-music generation models for precise conditioning using various
time-varying musical attributes and reference audio signals. The key finding is
that positional embeddings, which have been seldom used by text-to-music
generation models in the conditioner for text conditions, are critical when the
condition of interest is a function of time. Using melody control as an
example, our experiments show that simply adding rotary positional embeddings
to the decoupled cross-attention layers increases control accuracy from 56.6%
to 61.1%, while requiring 6.75 times fewer trainable parameters than
state-of-the-art fine-tuning mechanisms, using the same pre-trained diffusion
Transformer model of Stable Audio Open. We evaluate various forms of musical
attribute control, audio inpainting, and audio outpainting, demonstrating
improved controllability over MusicGen-Large and Stable Audio Open ControlNet
at a significantly lower fine-tuning cost, with only 85M trainble parameters.
Source code, model checkpoints, and demo examples are available at: https:
//MuseControlLite.github.io/web/.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [359] [Mapping the Evolution of Research Contributions using KnoVo](https://arxiv.org/abs/2506.17508)
*Sajratul Y. Rubaiat,Syed N. Sakib,Hasan M. Jamil*

Key words: 知识演化, 新颖性量化, 多层引用网络, 大型语言模型, 动态可视化

TL;DR: KnoVo是一个智能框架，用于量化分析科学文献中新颖性的演变，通过多层引用网络和LLM动态提取比较维度，提供定量新颖性评分，并支持知识演化的可视化和跨学科连接探索。

<details>
  <summary>Details</summary>

Main category: cs.DL

Motivation: 传统引用分析主要衡量影响力，无法充分评估研究的新颖性。KnoVo旨在填补这一空白，通过动态比较目标论文与相关文献，量化其新颖性，帮助研究者评估原创性、发现研究空白并探索跨学科联系。

Method: KnoVo利用大型语言模型(LLMs)从目标论文摘要中动态提取比较维度(如方法、应用、数据集)，并通过多层引用网络将这些维度与相关文献进行比较，采用类似锦标赛选择的机制生成定量新颖性评分。

Result: 通过对20篇多领域论文的详细分析，验证了KnoVo的能力，并报告了多种开源LLMs在框架中的性能表现。

Conclusion: KnoVo不仅能评估研究的原创性，还能跟踪知识演化、发现研究空白，并促进跨学科连接的探索。该框架为科学文献分析提供了新颖且实用的工具。

Abstract: This paper presents KnoVo (Knowledge Evolution), an intelligent framework
designed for quantifying and analyzing the evolution of research novelty in the
scientific literature. Moving beyond traditional citation analysis, which
primarily measures impact, KnoVo determines a paper's novelty relative to both
prior and subsequent work within its multilayered citation network. Given a
target paper's abstract, KnoVo utilizes Large Language Models (LLMs) to
dynamically extract dimensions of comparison (e.g., methodology, application,
dataset). The target paper is then compared to related publications along these
same extracted dimensions. This comparative analysis, inspired by tournament
selection, yields quantitative novelty scores reflecting the relative
improvement, equivalence, or inferiority of the target paper in specific
aspects. By aggregating these scores and visualizing their progression, for
instance, through dynamic evolution graphs and comparative radar charts, KnoVo
facilitates researchers not only to assess originality and identify similar
work, but also to track knowledge evolution along specific research dimensions,
uncover research gaps, and explore cross-disciplinary connections. We
demonstrate these capabilities through a detailed analysis of 20 diverse papers
from multiple scientific fields and report on the performance of various
open-source LLMs within the KnoVo framework.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [360] [Exploring Strategies for Personalized Radiation Therapy Part II Predicting Tumor Drift Patterns with Diffusion Models](https://arxiv.org/abs/2506.17491)
*Hao Peng,Steve Jiang,Robert Timmerman*

Key words: 放射治疗、脑癌、DDIM、个性化治疗、适应性放疗

TL;DR: 提出了一种基于去噪扩散隐式模型（DDIM）的新框架，用于动态调整脑癌放射治疗，通过模拟肿瘤的时空演变，优化治疗决策。

<details>
  <summary>Details</summary>

Main category: physics.med-ph

Motivation: 脑癌放射治疗中剂量和时机的个性化需求高，现有模型对肿瘤时空响应模式的预测能力有限。

Method: 采用DDIM框架，开发单步和迭代去噪策略，学习治疗前后影像的映射关系。

Result: 扩散模型能有效模拟个体肿瘤演变并定位治疗响应区域。

Conclusion: 该框架为个性化放疗提供了新工具，推动了生物信息导向的适应性治疗。

Abstract: Radiation therapy outcomes are decided by two key parameters, dose and
timing, whose best values vary substantially across patients. This variability
is especially critical in the treatment of brain cancer, where fractionated or
staged stereotactic radiosurgery improves safety compared to single fraction
approaches, but complicates the ability to predict treatment response. To
address this challenge, we employ Personalized Ultra-fractionated Stereotactic
Adaptive Radiotherapy (PULSAR), a strategy that dynamically adjusts treatment
based on how each tumor evolves over time. However, the success of PULSAR and
other adaptive approaches depends on predictive tools that can guide early
treatment decisions and avoid both overtreatment and undertreatment. However,
current radiomics and dosiomics models offer limited insight into the evolving
spatial and temporal patterns of tumor response. To overcome these limitations,
we propose a novel framework using Denoising Diffusion Implicit Models (DDIM),
which learns data-driven mappings from pre to post treatment imaging. In this
study, we developed single step and iterative denoising strategies and compared
their performance. The results show that diffusion models can effectively
simulate patient specific tumor evolution and localize regions associated with
treatment response. The proposed strategy provides a promising foundation for
modeling heterogeneous treatment response and enabling early, adaptive
interventions, paving the way toward more personalized and biologically
informed radiotherapy.

</details>


### [361] [Exploring Strategies for Personalized Radiation Therapy Part I Unlocking Response-Related Tumor Subregions with Class Activation Mapping](https://arxiv.org/abs/2506.17536)
*Hao Peng,Steve Jiang,Robert Timmerman*

Key words: 放射治疗, 卷积神经网络, Class Activation Mapping, 个性化治疗

TL;DR: 论文比较了三种预测放射治疗反应的方法（标准放射组学、基于梯度的特征和卷积神经网络），其中基于像素级CAM的方法在分类精度和空间特征提取上表现最佳。

<details>
  <summary>Details</summary>

Main category: physics.med-ph

Motivation: 个性化精准放射治疗需要识别预后和空间信息特征，并根据个体反应调整治疗，因此需要更精确的预测方法。

Method: 研究使用标准放射组学、基于梯度的特征和卷积神经网络（结合CAM）分析69例患者的脑转移瘤数据，通过自动编码分类模型预测肿瘤体积是否缩小超过20%。

Result: 基于像素级CAM的方法在分类精度上优于其他方法，并能提供详细的病变区域信息，有助于识别放射抵抗区。

Conclusion: 该方法具有潜力指导个性化放射治疗策略，尽管仍需进一步验证。

Abstract: Personalized precision radiation therapy requires more than simple
classification, it demands the identification of prognostic, spatially
informative features and the ability to adapt treatment based on individual
response. This study compares three approaches for predicting treatment
response: standard radiomics, gradient based features, and convolutional neural
networks enhanced with Class Activation Mapping. We analyzed 69 brain
metastases from 39 patients treated with Gamma Knife radiosurgery. An
integrated autoencoder classifier model was used to predict whether tumor
volume would shrink by more than 20 percent at a three months follow up, framed
as a binary classification task. The results highlight their strength in
hierarchical feature extraction and the classifiers discriminative capacity.
Among the models, pixel wise CAM provides the most detailed spatial insight,
identifying lesion specific regions rather than relying on fixed patterns,
demonstrating strong generalization. In non responding lesions, the activated
regions may indicate areas of radio resistance. Pixel wise CAM outperformed
both radiomics and gradient based methods in classification accuracy. Moreover,
its fine grained spatial features allow for alignment with cellular level data,
supporting biological validation and deeper understanding of heterogeneous
treatment responses. Although further validation is necessary, these findings
underscore the promise in guiding personalized and adaptive radiotherapy
strategies for both photon and particle therapies.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [362] [Numerical simulation of transient heat conduction with moving heat source using Physics Informed Neural Networks](https://arxiv.org/abs/2506.17726)
*Anirudh Kalyan,Sundararajan Natarajan*

Key words: 物理信息神经网络, 热传递, 移动热源, 转移学习, 有限元方法

TL;DR: 该论文提出了一种基于物理信息神经网络（PINNs）的新训练方法，用于模拟移动热源的热传递问题，通过转移学习和连续时间步进减少计算量，并与传统有限元方法的结果吻合良好。

<details>
  <summary>Details</summary>

Main category: math.NA

Motivation: 传统方法在模拟移动热源的热传递问题时计算量大，需要一种高效且准确的新方法。

Method: 采用物理信息神经网络（PINNs），通过转移学习和连续时间步进方法，将时间区间划分为小间隔，逐步训练单个网络。

Result: 提出的框架能高效计算大时间区间内的温度分布，结果与传统有限元方法一致。

Conclusion: PINNs结合新训练方法在移动热源模拟中具有高效性和准确性。

Abstract: In this paper, the physics informed neural networks (PINNs) is employed for
the numerical simulation of heat transfer involving a moving source. To reduce
the computational effort, a new training method is proposed that uses a
continuous time-stepping through transfer learning. Within this, the time
interval is divided into smaller intervals and a single network is initialized.
On this single network each time interval is trained with the initial condition
for (n+1)th as the solution obtained at nth time increment. Thus, this
framework enables the computation of large temporal intervals without
increasing the complexity of the network itself. The proposed framework is used
to estimate the temperature distribution in a homogeneous medium with a moving
heat source. The results from the proposed framework is compared with
traditional finite element method and a good agreement is seen.

</details>


### [363] [DPG loss functions for learning parameter-to-solution maps by neural networks](https://arxiv.org/abs/2506.18773)
*Pablo Cortés Castillo,Wolfgang Dahmen,Jay Gopalakrishnan*

Key words: 机器学习，偏微分方程，变分正确损失函数，DPG离散化，高对比度扩散

TL;DR: 本文研究了基于残差的损失函数在机器学习中的应用，特别是针对参数依赖的偏微分方程（PDE）的参数到解映射。通过变分正确的损失函数，为深度神经网络降阶模型提供严格的准确性认证。

<details>
  <summary>Details</summary>

Main category: math.NA

Motivation: 为提高深度神经网络降阶模型的预测能力，需要一种能够严格认证准确性的方法。为此，作者提出了一种基于残差的变分正确损失函数。

Method: 采用超弱间断Petrov-Galerkin（DPG）离散化方法，建立变分正确的损失函数，并通过一个椭圆PDE的实例进行详细说明。该方法适用于所有具有稳定DPG公式的问题。

Result: 数值结果和理论分析表明，对于高对比度扩散参数，提出的DPG损失函数比简单的最小二乘损失函数具有更鲁棒的性能。

Conclusion: 基于DPG的损失函数在高对比度扩散问题中表现出显著优势，适用于更广泛的PDE问题。

Abstract: We develop, analyze, and experimentally explore residual-based loss functions
for machine learning of parameter-to-solution maps in the context of
parameter-dependent families of partial differential equations (PDEs). Our
primary concern is on rigorous accuracy certification to enhance prediction
capability of resulting deep neural network reduced models. This is achieved by
the use of variationally correct loss functions. Through one specific example
of an elliptic PDE, details for establishing the variational correctness of a
loss function from an ultraweak Discontinuous Petrov Galerkin (DPG)
discretization are worked out. Despite the focus on the example, the proposed
concepts apply to a much wider scope of problems, namely problems for which
stable DPG formulations are available. The issue of {high-contrast} diffusion
fields and ensuing difficulties with degrading ellipticity are discussed. Both
numerical results and theoretical arguments illustrate that for high-contrast
diffusion parameters the proposed DPG loss functions deliver much more robust
performance than simpler least-squares losses.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [364] [Learning Partitions with Optimal Query and Round Complexities](https://arxiv.org/abs/2505.05009)
*Hadley Black,Arya Mazumdar,Barna Saha*

Key words: 分区学习, 查询复杂度, 非自适应算法, 自适应算法, 聚类

TL;DR: 论文研究了使用简单查询学习未知分区的问题，分析了非自适应和自适应算法的查询复杂度，并给出了确定性查询复杂度的完整刻画。

<details>
  <summary>Details</summary>

Main category: cs.DS

Motivation: 研究如何通过减少适应性来最小化查询复杂度，以解决聚类、主动学习和众包等问题中的基本分区问题。

Method: 使用不同形式的查询（配对查询、弱子集查询和强子集查询），分析查询复杂度和轮数之间的关系。

Result: 确定性查询复杂度为Θ(n^(1+1/(2^r-1))k^(1-1/(2^r-1)))；非自适应算法需要Ω(n^2/s^2)强查询，弱查询算法可以达到类似效果。

Conclusion: 研究提供了对分区学习问题查询复杂度的完整理解，为实际应用中的算法设计提供了理论基础。

Abstract: We consider the basic problem of learning an unknown partition of $n$
elements into at most $k$ sets using simple queries that reveal information
about a small subset of elements. Our starting point is the well-studied
pairwise same-set queries which ask if a pair of elements belong to the same
class. It is known that non-adaptive algorithms require $\Theta(n^2)$ queries,
while adaptive algorithms require $\Theta(nk)$ queries, and the best known
algorithm uses $k-1$ rounds. This problem has been studied extensively over the
last two decades in multiple communities due to its fundamental nature and
relevance to clustering, active learning, and crowd sourcing. In many
applications, it is of high interest to reduce adaptivity while minimizing
query complexity. We give a complete characterization of the deterministic
query complexity of this problem as a function of the number of rounds, $r$,
interpolating between the non-adaptive and adaptive settings: for any constant
$r$, the query complexity is
$\Theta(n^{1+\frac{1}{2^r-1}}k^{1-\frac{1}{2^r-1}})$. Our algorithm only needs
$O(\log \log n)$ rounds to attain the optimal $O(nk)$ query complexity.
  Next, we consider two generalizations of pairwise queries to subsets $S$ of
size at most $s$: (1) weak subset queries which return the number of classes
intersected by $S$, and (2) strong subset queries which return the entire
partition restricted on $S$. Once again in crowd sourcing applications, queries
on large sets may be prohibitive. For non-adaptive algorithms, we show
$\Omega(n^2/s^2)$ strong queries are needed. Perhaps surprisingly, we show that
there is a non-adaptive algorithm using weak queries that matches this bound up
to log-factors for all $s \leq \sqrt{n}$. More generally, we obtain nearly
matching upper and lower bounds for algorithms using subset queries in terms of
both the number of rounds, $r$, and the query size bound, $s$.

</details>


### [365] [Optimal Graph Reconstruction by Counting Connected Components in Induced Subgraphs](https://arxiv.org/abs/2506.08405)
*Hadley Black,Arya Mazumdar,Barna Saha,Yinzhan Xu*

Key words: 图重构,连通分量,查询复杂度,自适应算法

TL;DR: 本文提出了一种基于连通分量数的新查询模型，用于重构n节点m边图，证明了自适应查询的上下界均为Θ(mlogn/logm)，并展示了非自适应查询需要Ω(n²)次，同时提供了一个仅需两轮自适应性且查询复杂度为O(mlogn+nlog²n)的算法。

<details>
  <summary>Details</summary>

Main category: cs.DS

Motivation: 研究图重构问题的新查询模型，重点关注连通分量数这一基本图参数，以探索其在不同查询方式下的效率和理论界限。

Method: 通过子集顶点查询的oracle模型，设计自适应和非自适应的查询策略，分析其查询复杂度。

Result: 证明了自适应查询的上下界为Θ(mlogn/logm)，非自适应查询需要Ω(n²)次，并提出了一个两轮自适应的高效算法。

Conclusion: 连通分量数作为查询模型在自适应条件下效率较高，但非自适应条件下复杂度较高，多轮自适应可以显著提升效率。

Abstract: The graph reconstruction problem has been extensively studied under various
query models. In this paper, we propose a new query model regarding the number
of connected components, which is one of the most basic and fundamental graph
parameters. Formally, we consider the problem of reconstructing an $n$-node
$m$-edge graph with oracle queries of the following form: provided with a
subset of vertices, the oracle returns the number of connected components in
the induced subgraph. We show $\Theta(\frac{m \log n}{\log m})$ queries in
expectation are both sufficient and necessary to adaptively reconstruct the
graph. In contrast, we show that $\Omega(n^2)$ non-adaptive queries are
required, even when $m = O(n)$. We also provide an $O(m\log n + n\log^2 n)$
query algorithm using only two rounds of adaptivity.

</details>


### [366] [Faster Low-Rank Approximation and Kernel Ridge Regression via the Block-Nyström Method](https://arxiv.org/abs/2506.17556)
*Sachin Garg,Michał Dereziński*

Key words: Nyström方法,块对角结构,核岭回归,预条件器,低秩近似

TL;DR: 论文提出了一种名为Block-Nyström的算法，通过引入块对角结构降低Nyström方法的计算成本，同时保持强近似保证。该方法可用于优化二阶优化器的预条件器，并高效解决核岭回归问题。

<details>
  <summary>Details</summary>

Main category: cs.DS

Motivation: 针对Nyström方法在处理具有重尾谱衰减数据时计算成本过高的问题，提出Block-Nyström算法，以更低的计算成本提供更强的近似保证。

Method: 在Nyström方法中引入块对角结构，结合多个较小的Nyström近似以增强对输入谱的尾部分布估计。还提出了一种新的递归预条件方案用于快速求逆Block-Nyström矩阵。

Result: Block-Nyström显著降低了计算成本，同时保持强近似保证，改进了二阶优化的预条件器，并高效解决核岭回归问题。

Conclusion: Block-Nyström是一种高效且实用的方法，适用于大规模矩阵的低秩近似问题，尤其在数据谱衰减较重时表现出色。

Abstract: The Nystr\"om method is a popular low-rank approximation technique for large
matrices that arise in kernel methods and convex optimization. Yet, when the
data exhibits heavy-tailed spectral decay, the effective dimension of the
problem often becomes so large that even the Nystr\"om method may be outside of
our computational budget. To address this, we propose Block-Nystr\"om, an
algorithm that injects a block-diagonal structure into the Nystr\"om method,
thereby significantly reducing its computational cost while recovering strong
approximation guarantees. We show that Block-Nystr\"om can be used to construct
improved preconditioners for second-order optimization, as well as to
efficiently solve kernel ridge regression for statistical learning over Hilbert
spaces. Our key technical insight is that, within the same computational
budget, combining several smaller Nystr\"om approximations leads to stronger
tail estimates of the input spectrum than using one larger approximation. Along
the way, we provide a novel recursive preconditioning scheme for efficiently
inverting the Block-Nystr\"om matrix, and provide new statistical learning
bounds for a broad class of approximate kernel ridge regression solvers.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [367] [Coupled Entropy: A Goldilocks Generalization?](https://arxiv.org/abs/2506.17229)
*Kenric P. Nelson*

Key words: 非广延统计力学, Tsallis熵, 耦合熵, 复杂系统, 机器学习

TL;DR: 该论文探讨了非广延统计力学（NSM）中Tsallis熵的归一化问题，并提出了一种称为耦合熵的新方法，可能为复杂系统（如机器学习）提供更稳健的工具。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 研究动机源于Tsallis熵在归一化时的不稳定性问题，试图找到一种更稳定的熵定义，以适应复杂系统的建模需求。

Method: 方法是通过引入耦合熵（除以1+dκ），并从独立随机变量的数量角度重新定义其形式，解决了归一化问题。

Result: 结果表明，耦合熵能够提供必要的稳健性，并通过耦合参数κ和形状参数α来描述复杂系统的非线性特性。

Conclusion: 结论认为耦合熵是衡量统计复杂性的强有力候选指标，尤其适用于非指数分布和非加性熵的系统。

Abstract: Nonextensive Statistical Mechanics (NSM) has developed into a powerful
toolset for modeling and analyzing complex systems. Despite its many successes,
a puzzle arose early in its development. The constraints on the Tsallis entropy
are in the form of an escort distribution with elements proportional to
$p_i^q$, but this same factor within the Tsallis entropy function is not
normalized. This led to consideration of the Normalized Tsallis Entropy (NTE);
however, the normalization proved to make the function unstable. I will provide
evidence that the coupled entropy, which divides NTE by $1 + d\kappa$, where
$d$ is the dimension and $\kappa$ is the coupling, may provide the necessary
robustness necessary for applications like machine learning. The definition for
the coupled entropy and its maximizing distributions, the coupled exponential
family, arises from clarifying how the number of independent random variables
$(q)$ is composed of the nonlinear properties of complex systems,
$q=1+\frac{\alpha\kappa}{1+d\kappa}$, where $\alpha$ is the nonlinear parameter
governing the shape of distributions near their location and $\kappa$ is the
parameter determining the asymptotic tail decay. Foundationally, for complex
systems, the coupling is the measure of nonlinearity inducing non-exponential
distributions and the degree of nonadditivity entropy. As such, the coupling is
a strong candidate as a measure of statistical complexity.

</details>


### [368] [Differentiable neural network representation of multi-well, locally-convex potentials](https://arxiv.org/abs/2506.17242)
*Reese E. Jones,Adrian Buganza Tepole,Jan N. Fuhg*

Key words: 多阱势, LSE-ICNN, 可微性, 凸优化, 稀疏回归

TL;DR: 该论文提出了一种基于对数-求和-指数（LSE）混合输入凸神经网络（ICNN）模式的可微且凸的多阱势表示方法，适用于复杂多模态建模。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 多阱势在科学中广泛存在，但传统方法通常是非平滑的。本文旨在解决这一问题，提供一种平滑、可微且凸的建模方法。

Method: 使用对数-求和-指数混合输入凸神经网络（LSE-ICNN），通过稀疏回归自动发现模式数量和过渡尺度。

Result: LSE-ICNN在多个领域（如机械化学相变、弹性不稳定性、基因电路等）中展现出卓越的建模能力，保留了可微性。

Conclusion: LSE-ICNN是一种灵活、高效的多模态建模工具，适用于数据驱动建模、优化和物理模拟。

Abstract: Multi-well potentials are ubiquitous in science, modeling phenomena such as
phase transitions, dynamic instabilities, and multimodal behavior across
physics, chemistry, and biology. In contrast to non-smooth minimum-of-mixture
representations, we propose a differentiable and convex formulation based on a
log-sum-exponential (LSE) mixture of input convex neural network (ICNN) modes.
This log-sum-exponential input convex neural network (LSE-ICNN) provides a
smooth surrogate that retains convexity within basins and allows for
gradient-based learning and inference.
  A key feature of the LSE-ICNN is its ability to automatically discover both
the number of modes and the scale of transitions through sparse regression,
enabling adaptive and parsimonious modeling. We demonstrate the versatility of
the LSE-ICNN across diverse domains, including mechanochemical phase
transformations, microstructural elastic instabilities, conservative biological
gene circuits, and variational inference for multimodal probability
distributions. These examples highlight the effectiveness of the LSE-ICNN in
capturing complex multimodal landscapes while preserving differentiability,
making it broadly applicable in data-driven modeling, optimization, and
physical simulation.

</details>


### [369] [Gaussian Processes and Reproducing Kernels: Connections and Equivalences](https://arxiv.org/abs/2506.17366)
*Motonobu Kanagawa,Philipp Hennig,Dino Sejdinovic,Bharath K. Sriperumbudur*

Key words: 高斯过程,再生核希尔伯特空间,机器学习,统计学,数值分析

TL;DR: 本文综述了高斯过程和再生核希尔伯特空间（RKHS）两种方法的联系与等价性，为机器学习、统计学和数值分析提供了统一视角。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 研究高斯过程与RKHS之间的联系，以促进这两种方法在机器学习、统计学和数值分析中的交叉应用。

Method: 通过回归、插值、数值积分、分布差异和统计依赖性等主题，比较两者在不同场景下的等价性。

Result: 建立了基于高斯希尔伯特空间与RKHS等价性的统一视角，揭示了两种方法的内在联系。

Conclusion: 该研究为高斯过程与再生核方法提供了理论基础，促进了两者的交叉发展。

Abstract: This monograph studies the relations between two approaches using positive
definite kernels: probabilistic methods using Gaussian processes, and
non-probabilistic methods using reproducing kernel Hilbert spaces (RKHS). They
are widely studied and used in machine learning, statistics, and numerical
analysis. Connections and equivalences between them are reviewed for
fundamental topics such as regression, interpolation, numerical integration,
distributional discrepancies, and statistical dependence, as well as for sample
path properties of Gaussian processes. A unifying perspective for these
equivalences is established, based on the equivalence between the Gaussian
Hilbert space and the RKHS. The monograph serves as a basis to bridge many
other methods based on Gaussian processes and reproducing kernels, which are
developed in parallel by the two research communities.

</details>


### [370] [Scalable Machine Learning Algorithms using Path Signatures](https://arxiv.org/abs/2506.17634)
*Csaba Tóth*

Key words: 路径签名, 机器学习, 粗糙路径理论, 高斯过程, 图神经网络

TL;DR: 该论文探讨了如何将路径签名的表达力融入可扩展的机器学习流程中，提出了一系列兼具理论鲁棒性和计算效率的模型，结合了粗糙路径理论与概率建模、深度学习和核方法。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 路径签名提供了对时序和结构化数据的层次化表示，但由于其复杂性，如何将其高效地应用于机器学习是一个重要的研究方向。

Method: 论文提出了多种方法，包括基于签名核的高斯过程、Seq2Tens框架、图模型、随机傅里叶签名特征以及对签名核的扩展。

Result: 这些方法在不确定性感知的时间序列建模、长程依赖建模、图数据处理等方面表现出色，为时序和结构化数据的机器学习提供了新工具。

Conclusion: 该研究为路径签名在机器学习中的应用提供了方法论和理论支持，是这一领域的前沿成果。

Abstract: The interface between stochastic analysis and machine learning is a rapidly
evolving field, with path signatures - iterated integrals that provide
faithful, hierarchical representations of paths - offering a principled and
universal feature map for sequential and structured data. Rooted in rough path
theory, path signatures are invariant to reparameterization and well-suited for
modelling evolving dynamics, long-range dependencies, and irregular sampling -
common challenges in real-world time series and graph data.
  This thesis investigates how to harness the expressive power of path
signatures within scalable machine learning pipelines. It introduces a suite of
models that combine theoretical robustness with computational efficiency,
bridging rough path theory with probabilistic modelling, deep learning, and
kernel methods. Key contributions include: Gaussian processes with signature
kernel-based covariance functions for uncertainty-aware time series modelling;
the Seq2Tens framework, which employs low-rank tensor structure in the weight
space for scalable deep modelling of long-range dependencies; and graph-based
models where expected signatures over graphs induce hypo-elliptic diffusion
processes, offering expressive yet tractable alternatives to standard graph
neural networks. Further developments include Random Fourier Signature
Features, a scalable kernel approximation with theoretical guarantees, and
Recurrent Sparse Spectrum Signature Gaussian Processes, which combine Gaussian
processes, signature kernels, and random features with a principled forgetting
mechanism for multi-horizon time series forecasting with adaptive context
length.
  We hope this thesis serves as both a methodological toolkit and a conceptual
bridge, and provides a useful reference for the current state of the art in
scalable, signature-based learning for sequential and structured data.

</details>


### [371] [Derandomizing Simultaneous Confidence Regions for Band-Limited Functions by Improved Norm Bounds and Majority-Voting Schemes](https://arxiv.org/abs/2506.17764)
*Balázs Csanád Csáji,Bálint Horváth*

Key words: 带宽受限函数, 置信区域, 再生核希尔伯特空间, Hoeffding不等式, Bernstein边界

TL;DR: 本文改进了在带宽受限函数中构建同时置信区域的方法，通过引入核范数边界和多数投票机制，提升了稳定性和区域大小。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 带宽受限函数在系统理论和信号处理中广泛应用，但现有方法在非参数、非渐近情况下存在改进空间。

Method: 在Paley-Wiener再生核希尔伯特空间中利用均匀随机化Hoeffding不等式和小样本经验Bernstein边界，结合多数投票机制。

Result: 通过数值实验验证，改进方法能有效提升置信区域的稳定性和大小，同时保持覆盖保证。

Conclusion: 提出的方法在理论和实验上均表现出优越性，适用于带宽受限函数的置信区域构建。

Abstract: Band-limited functions are fundamental objects that are widely used in
systems theory and signal processing. In this paper we refine a recent
nonparametric, nonasymptotic method for constructing simultaneous confidence
regions for band-limited functions from noisy input-output measurements, by
working in a Paley-Wiener reproducing kernel Hilbert space. Kernel norm bounds
are tightened using a uniformly-randomized Hoeffding's inequality for small
samples and an empirical Bernstein bound for larger ones. We derive an
approximate threshold, based on the sample size and how informative the inputs
are, that governs which bound to deploy. Finally, we apply majority voting to
aggregate confidence sets from random subsamples, boosting both stability and
region size. We prove that even per-input aggregated intervals retain their
simultaneous coverage guarantee. These refinements are also validated through
numerical experiments.

</details>


### [372] [DRO-Augment Framework: Robustness by Synergizing Wasserstein Distributionally Robust Optimization and Data Augmentation](https://arxiv.org/abs/2506.17874)
*Jiaming Hu,Debarghya Mukherjee,Ioannis Ch. Paschalidis*

Key words: DRO-Augment, W-DRO, 数据增强, 鲁棒性, 对抗攻击

TL;DR: DRO-Augment结合Wasserstein Distributionally Robust Optimization（W-DRO）与多种数据增强策略，显著提升了深度神经网络在多种数据扰动和对抗攻击下的鲁棒性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 深度神经网络（DNNs）在图像分类任务中需应对多种输入扰动，现有数据增强方法在同时对抗数据损坏和对抗攻击方面仍有改进空间。

Method: 提出DRO-Augment框架，将W-DRO与数据增强策略结合，并使用计算高效的变分正则化损失函数。

Result: 在多个基准数据集（如CIFAR-10-C、MNIST等）上，DRO-Augment在严重数据扰动和对抗攻击下优于现有方法，同时保持干净数据的准确性。

Conclusion: DRO-Augment能有效提升模型对多种扰动的鲁棒性，并提供了相关理论泛化误差界。

Abstract: In many real-world applications, ensuring the robustness and stability of
deep neural networks (DNNs) is crucial, particularly for image classification
tasks that encounter various input perturbations. While data augmentation
techniques have been widely adopted to enhance the resilience of a trained
model against such perturbations, there remains significant room for
improvement in robustness against corrupted data and adversarial attacks
simultaneously. To address this challenge, we introduce DRO-Augment, a novel
framework that integrates Wasserstein Distributionally Robust Optimization
(W-DRO) with various data augmentation strategies to improve the robustness of
the models significantly across a broad spectrum of corruptions. Our method
outperforms existing augmentation methods under severe data perturbations and
adversarial attack scenarios while maintaining the accuracy on the clean
datasets on a range of benchmark datasets, including but not limited to
CIFAR-10-C, CIFAR-100-C, MNIST, and Fashion-MNIST. On the theoretical side, we
establish novel generalization error bounds for neural networks trained using a
computationally efficient, variation-regularized loss function closely related
to the W-DRO problem.

</details>


### [373] [Identifiable Convex-Concave Regression via Sub-gradient Regularised Least Squares](https://arxiv.org/abs/2506.18078)
*William Chung*

Key words: 非参数回归,凸凹分解,统计正交约束,正则化,ICCNLS

TL;DR: 提出了一种新的非参数回归方法ICCNLS，通过将目标函数分解为凸和凹的加性形状约束组件，并引入全局统计正交约束，解决了凸凹分解的模糊性问题。该方法在合成和实际数据中表现出更高的预测精度和模型简洁性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 传统的凸凹分解方法存在模糊性问题，且缺乏统计可识别性。ICCNLS旨在通过统计正交约束和正则化技术改善这些问题。

Method: ICCNLS方法将目标函数分解为凸和凹的加性组件，使用子梯度约束的仿射函数表示，并引入L1、L2和弹性网络正则化。通过全局统计正交约束确保残差与输入变量不相关。

Result: 在合成和实际数据集（如医疗定价数据）中，ICCNLS相比传统CNLS和DC回归方法展现出更高的预测精度和模型简洁性。

Conclusion: 统计可识别性与凸凹结构和子梯度正则化的结合，能够生成适用于预测、基准测试和政策评估的可解释模型。

Abstract: We propose a novel nonparametric regression method that models complex
input-output relationships as the sum of convex and concave components. The
method-Identifiable Convex-Concave Nonparametric Least Squares
(ICCNLS)-decomposes the target function into additive shape-constrained
components, each represented via sub-gradient-constrained affine functions. To
address the affine ambiguity inherent in convex-concave decompositions, we
introduce global statistical orthogonality constraints, ensuring that residuals
are uncorrelated with both intercept and input variables. This enforces
decomposition identifiability and improves interpretability. We further
incorporate L1, L2 and elastic net regularisation on sub-gradients to enhance
generalisation and promote structural sparsity. The proposed method is
evaluated on synthetic and real-world datasets, including healthcare pricing
data, and demonstrates improved predictive accuracy and model simplicity
compared to conventional CNLS and difference-of-convex (DC) regression
approaches. Our results show that statistical identifiability, when paired with
convex-concave structure and sub-gradient regularisation, yields interpretable
models suited for forecasting, benchmarking, and policy evaluation.

</details>


### [374] [Phase transition of \emph{descending} phase retrieval algorithms](https://arxiv.org/abs/2506.18275)
*Mihailo Stojnic*

Key words: 相位检索,随机对偶理论,参数流形,漏斗点,相变

TL;DR: 研究下降相位检索算法的理论极限，通过随机对偶理论（RDT）分析了算法的统计特性，发现参数流形及其漏斗点是关键数学对象。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 探讨下降算法在相位检索中的表现，揭示其背后的理论机制，提升算法的实际应用效果。

Method: 使用RDT分析参数流形和漏斗点，提出一种混合交替下降算法（结合障碍法和梯度下降）。

Result: 随着样本复杂性增加，参数流形从多漏斗点过渡到单漏斗点，导致算法性能发生相变。理论与模拟结果在小维度下一致。

Conclusion: 参数流形的结构影响算法收敛性，混合算法有效验证理论预测。

Abstract: We study theoretical limits of \emph{descending} phase retrieval algorithms.
Utilizing \emph{Random duality theory} (RDT) we develop a generic program that
allows statistical characterization of various algorithmic performance metrics.
Through these we identify the concepts of \emph{parametric manifold} and its
\emph{funneling points} as key mathematical objects that govern the underlying
algorithms' behavior. An isomorphism between single funneling point manifolds
and global convergence of descending algorithms is established. The structure
and shape of the parametric manifold as well as its dependence on the sample
complexity are studied through both plain and lifted RDT. Emergence of a phase
transition is observed. Namely, as sample complexity increases, parametric
manifold transitions from a multi to a single funneling point structure. This
in return corresponds to a transition from the scenarios where descending
algorithms generically fail to the scenarios where they succeed in solving
phase retrieval. We also develop and implement a practical algorithmic variant
that in a hybrid alternating fashion combines a barrier and a plain gradient
descent. Even though the theoretical results are obtained for infinite
dimensional scenarios (and consequently non-jittery parametric manifolds), we
observe a strong agrement between theoretical and simulated phase transitions
predictions for fairly small dimensions on the order of a few hundreds.

</details>


### [375] [Optimal spectral initializers impact on phase retrieval phase transitions -- an RDT view](https://arxiv.org/abs/2506.18279)
*Mihailo Stojnic*

Key words: 相位检索(PR), 降阶相位检索算法(dPR), 最优谱初始化器(OptSpins), 随机对偶理论(RDT), 样本复杂度比率(alpha), 平坦区域

TL;DR: 该论文研究了谱初始化器与降阶相位检索算法(dPR)的理论极限之间的关系，提出了一种基于随机对偶理论(RDT)的方法来统计描述最优谱初始化器(OptSpins)，并评估其初始重叠性。研究发现dPR的相变临界点可能难以实际达到，而略微增加样本复杂度比率(alpha)可以缩小算法的平坦区域，从而提高PR的成功率。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 研究动机在于理解谱初始化器如何影响降阶相位检索算法的性能，特别是如何通过优化初始重叠性来克服平坦区域的局部抖动问题。

Method: 方法包括使用随机对偶理论(RDT)来分析最优谱初始化器(OptSpins)的功能结构，并通过数值模拟验证理论预测。

Result: 结果表明，dPR的相变临界点可能难实现，但通过增加alpha可以缩小平坦区域，使OptSpins避开这些区域从而提高成功率。

Conclusion: 研究结论指出，通过优化初始化和调整样本复杂度比率，可以显著改善降阶相位检索算法的性能。

Abstract: We analyze the relation between spectral initializers and theoretical limits
of \emph{descending} phase retrieval algorithms (dPR). In companion paper
[104], for any sample complexity ratio, $\alpha$, \emph{parametric manifold},
${\mathcal {PM}}(\alpha)$, is recognized as a critically important structure
that generically determines dPRs abilities to solve phase retrieval (PR).
Moreover, overlap between the algorithmic solution and the true signal is
positioned as a key ${\mathcal {PM}}$'s component. We here consider the
so-called \emph{overlap optimal} spectral initializers (OptSpins) as dPR's
starting points and develop a generic \emph{Random duality theory} (RDT) based
program to statistically characterize them. In particular, we determine the
functional structure of OptSpins and evaluate the starting overlaps that they
provide for the dPRs. Since ${\mathcal {PM}}$'s so-called \emph{flat regions}
are highly susceptible to \emph{local jitteriness} and as such are key
obstacles on dPR's path towards PR's global optimum, a precise characterization
of the starting overlap allows to determine if such regions can be successfully
circumvented. Through the presented theoretical analysis we observe two key
points in that regard: \textbf{\emph{(i)}} dPR's theoretical phase transition
(critical $\alpha$ above which they solve PR) might be difficult to practically
achieve as the ${\mathcal {PM}}$'s flat regions are large causing the
associated OptSpins to fall exactly within them; and \textbf{\emph{(ii)}}
Opting for so-called ``\emph{safer compression}'' and slightly increasing
$\alpha$ (by say $15\%$) shrinks flat regions and allows OptSpins to fall
outside them and dPRs to ultimately solve PR. Numerical simulations are
conducted as well and shown to be in an excellent agreement with theoretical
predictions.

</details>


### [376] [Phase retrieval with rank $d$ measurements -- \emph{descending} algorithms phase transitions](https://arxiv.org/abs/2506.18282)
*Mihailo Stojnic*

Key words: Random duality theory, phase retrieval, phase transition, gradient descent, sample complexity

TL;DR: 本文扩展了RDT理论，用于分析秩d正定相位检索的性能，并发现了样本复杂度的相变现象，理论与实验结果高度一致。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 扩展RDT理论以适用于更高维度的相位检索问题，弥补现有分析工具的局限性。

Method: 采用RDT理论分析秩d正定相位检索的性能，并通过对数障碍梯度下降实验验证理论预测。

Result: 理论预测的样本复杂度相变现象与实验结果高度一致，验证了方法的有效性。

Conclusion: RDT理论能有效分析高维相位检索问题，为实际应用提供了理论支持。

Abstract: Companion paper [118] developed a powerful \emph{Random duality theory} (RDT)
based analytical program to statistically characterize performance of
\emph{descending} phase retrieval algorithms (dPR) (these include all variants
of gradient descents and among them widely popular Wirtinger flows). We here
generalize the program and show how it can be utilized to handle rank $d$
positive definite phase retrieval (PR) measurements (with special cases $d=1$
and $d=2$ serving as emulations of the real and complex phase retrievals,
respectively). In particular, we observe that the minimal sample complexity
ratio (number of measurements scaled by the dimension of the unknown signal)
which ensures dPR's success exhibits a phase transition (PT) phenomenon. For
both plain and lifted RDT we determine phase transitions locations. To
complement theoretical results we implement a log barrier gradient descent
variant and observe that, even in small dimensional scenarios (with problem
sizes on the order of 100), the simulated phase transitions are in an excellent
agreement with the theoretical predictions.

</details>


### [377] [Quantifying Uncertainty in the Presence of Distribution Shifts](https://arxiv.org/abs/2506.18283)
*Yuli Slavutsky,David M. Blei*

Key words: 贝叶斯框架,不确定性估计,协变量偏移,变分推断,自适应先验

TL;DR: 提出了一种贝叶斯框架，通过自适应先验和变分推断改进神经网络在协变量偏移下的不确定性估计。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 解决神经网络在训练和测试分布不一致时不确定性估计不可靠的问题。

Method: 使用自适应先验（考虑训练和新协变量）和变分推断近似后验预测分布；构建合成环境模拟协变量偏移。

Result: 在合成和真实数据上的实验表明，该方法显著提升了分布偏移下的不确定性估计效果。

Conclusion: 自适应先验和合成环境方法有效提升了协变量偏移下的不确定性估计可靠性。

Abstract: Neural networks make accurate predictions but often fail to provide reliable
uncertainty estimates, especially under covariate distribution shifts between
training and testing. To address this problem, we propose a Bayesian framework
for uncertainty estimation that explicitly accounts for covariate shifts. While
conventional approaches rely on fixed priors, the key idea of our method is an
adaptive prior, conditioned on both training and new covariates. This prior
naturally increases uncertainty for inputs that lie far from the training
distribution in regions where predictive performance is likely to degrade. To
efficiently approximate the resulting posterior predictive distribution, we
employ amortized variational inference. Finally, we construct synthetic
environments by drawing small bootstrap samples from the training data,
simulating a range of plausible covariate shift using only the original
dataset. We evaluate our method on both synthetic and real-world data. It
yields substantially improved uncertainty estimates under distribution shifts.

</details>


### [378] [Theoretical guarantees for neural estimators in parametric statistics](https://arxiv.org/abs/2506.18508)
*Almut Rödder,Manuel Hentschel,Sebastian Engelke*

Key words: 神经估计器、理论保证、风险分解、深度学习

TL;DR: 本文提出了神经估计器的理论保证，通过分解风险项并确保每项收敛到零，为这类估计器的广泛应用提供了理论支持。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 尽管神经估计器在模拟和实际应用中表现出色，但缺乏理论支持。本文旨在填补这一空白。

Method: 研究神经估计器的风险，将其分解为多个可单独分析的项，并确保每项收敛到零。

Result: 验证了神经估计器在常见应用中的假定，并提供了推导理论保证的通用方法。

Conclusion: 本文为神经估计器的理论和实际应用提供了坚实的基础。

Abstract: Neural estimators are simulation-based estimators for the parameters of a
family of statistical models, which build a direct mapping from the sample to
the parameter vector. They benefit from the versatility of available network
architectures and efficient training methods developed in the field of deep
learning. Neural estimators are amortized in the sense that, once trained, they
can be applied to any new data set with almost no computational cost. While
many papers have shown very good performance of these methods in simulation
studies and real-world applications, so far no statistical guarantees are
available to support these observations theoretically. In this work, we study
the risk of neural estimators by decomposing it into several terms that can be
analyzed separately. We formulate easy-to-check assumptions ensuring that each
term converges to zero, and we verify them for popular applications of neural
estimators. Our results provide a general recipe to derive theoretical
guarantees also for broader classes of architectures and estimation problems.

</details>


### [379] [Trustworthy Prediction with Gaussian Process Knowledge Scores](https://arxiv.org/abs/2506.18630)
*Kurt Butler,Guanchao Feng,Tong Chen,Petar Djuric*

Key words: 高斯过程回归,知识评分,异常检测,外推,缺失数据填补

TL;DR: 该论文提出了一个用于高斯过程回归（GPR）模型的知识评分，用于量化数据对预测不确定性的减少程度，并在实验中展示了其在异常检测、外推和缺失数据填补等方面的有效性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 解决概率模型在数据空间无观测区域预测时是否受已有数据支持的问题。

Method: 提出一个知识评分，用于量化GPR模型中数据对预测不确定性的影响。

Result: 实验表明，知识评分能有效预测GPR模型的准确性，并提升异常检测、外推和缺失数据填补等任务的性能。

Conclusion: 知识评分为GPR模型提供了一种可解释且有效的预测质量评估工具。

Abstract: Probabilistic models are often used to make predictions in regions of the
data space where no observations are available, but it is not always clear
whether such predictions are well-informed by previously seen data. In this
paper, we propose a knowledge score for predictions from Gaussian process
regression (GPR) models that quantifies the extent to which observing data have
reduced our uncertainty about a prediction. The knowledge score is
interpretable and naturally bounded between 0 and 1. We demonstrate in several
experiments that the knowledge score can anticipate when predictions from a GPR
model are accurate, and that this anticipation improves performance in tasks
such as anomaly detection, extrapolation, and missing data imputation. Source
code for this project is available online at
https://github.com/KurtButler/GP-knowledge.

</details>


### [380] [Tight Generalization Error Bounds for Stochastic Gradient Descent in Non-convex Learning](https://arxiv.org/abs/2506.18645)
*Wenjun Xiong,Juan Ding,Xinlei Zuo,Qizhai Li*

Key words: SGD, 泛化误差, 非凸学习, T2pm-SGD, MNIST, CIFAR-10

TL;DR: 本文通过引入Type II perturbed SGD (T2pm-SGD)，改进了非凸学习中SGD的泛化误差界，将其轨迹项优化为O(n⁻¹)，并通过选择最优噪声方差进一步将总界优化为O(n⁻²⁄₃)，实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 为了更深入地理解SGD在非凸学习中的泛化性能，本文旨在通过引入T2pm-SGD，改进泛化误差界，确保模型在未见数据上的鲁棒性。

Method: 提出了T2pm-SGD算法，通过分解泛化误差界为轨迹项和平坦项，并优化噪声方差，改进了对子高斯和有界损失函数的泛化误差界分析。

Result: 轨迹项优化为O(n⁻¹)，总界优化为O(n⁻²⁄₃)，平坦项也更稳定且优于文献结果；实验在MNIST和CIFAR-10上验证了方法的有效性。

Conclusion: T2pm-SGD通过稳定平坦项和优化轨迹项，显著改进了SGD的泛化误差界，为深度学习理论提供了重要支持。

Abstract: Stochastic Gradient Descent (SGD) is fundamental for training deep neural
networks, especially in non-convex settings. Understanding SGD's generalization
properties is crucial for ensuring robust model performance on unseen data. In
this paper, we analyze the generalization error bounds of SGD for non-convex
learning by introducing the Type II perturbed SGD (T2pm-SGD), which
accommodates both sub-Gaussian and bounded loss functions. The generalization
error bound is decomposed into two components: the trajectory term and the
flatness term. Our analysis improves the trajectory term to $O(n^{-1})$,
significantly enhancing the previous $O((nb)^{-1/2})$ bound for bounded losses,
where n is the number of training samples and b is the batch size. By selecting
an optimal variance for the perturbation noise, the overall bound is further
refined to $O(n^{-2/3})$. For sub-Gaussian loss functions, a tighter trajectory
term is also achieved. In both cases, the flatness term remains stable across
iterations and is smaller than those reported in previous literature, which
increase with iterations. This stability, ensured by T2pm-SGD, leads to tighter
generalization error bounds for both loss function types. Our theoretical
results are validated through extensive experiments on benchmark datasets,
including MNIST and CIFAR-10, demonstrating the effectiveness of T2pm-SGD in
establishing tighter generalization bounds.

</details>


### [381] [A Random Matrix Analysis of In-context Memorization for Nonlinear Attention](https://arxiv.org/abs/2506.18656)
*Zhenyu Liao,Jiaqing Liu,TianQi Hou,Difan Zou,Zenan Ling*

Key words: 注意力机制, 非线性分析, 记忆误差, 大语言模型, 高维统计

TL;DR: 分析了非线性注意力机制在高维情况下的记忆误差表现，发现非线性注意力通常比线性回归误差更高，但当输入具有统计结构时，这种差距会消失甚至反转。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 尽管注意力机制在现代大语言模型中起着核心作用，但对非线性注意力机制的理论理解仍有限。本文旨在填补这一空白。

Method: 利用大核随机矩阵理论的最新进展，在高维比例区域中分析了非线性注意力的记忆误差表现。

Result: 非线性注意力在随机输入下记忆误差较高，但当输入具有统计结构时，这种差距会消失甚至反转。

Conclusion: 非线性注意力中的非线性特性和输入结构相互作用，共同影响其记忆性能。

Abstract: Attention mechanisms have revolutionized machine learning (ML) by enabling
efficient modeling of global dependencies across inputs. Their inherently
parallelizable structures allow for efficient scaling with the exponentially
increasing size of both pretrained data and model parameters. Yet, despite
their central role as the computational backbone of modern large language
models (LLMs), the theoretical understanding of Attentions, especially in the
nonlinear setting, remains limited.
  In this paper, we provide a precise characterization of the \emph{in-context
memorization error} of \emph{nonlinear Attention}, in the high-dimensional
proportional regime where the number of input tokens $n$ and their embedding
dimension $p$ are both large and comparable. Leveraging recent advances in the
theory of large kernel random matrices, we show that nonlinear Attention
typically incurs higher memorization error than linear ridge regression on
random inputs. However, this gap vanishes, and can even be reversed, when the
input exhibits statistical structure, particularly when the Attention weights
align with the input signal direction. Our results reveal how nonlinearity and
input structure interact with each other to govern the memorization performance
of nonlinear Attention. The theoretical insights are supported by numerical
experiments.

</details>


### [382] [Local Averaging Accurately Distills Manifold Structure From Noisy Data](https://arxiv.org/abs/2506.18761)
*Yihan Shen,Shiyu Wang,Arnaud Lamy,Mariam Avagyan,John Wright*

Key words: 高维数据,流形结构,局部平均法,高噪声环境,理论分析

TL;DR: 本文针对高维数据中的流形结构，分析了在高噪声环境下局部平均法的精度，首次提出了理论分析框架。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 高维数据普遍存在于自然图像和科学数据集中，通常靠近低维流形。利用这一几何结构对信号去噪、重建和生成等任务至关重要。然而实际中流形通常未知，只有噪声样本可用，因此需要分析局部平均法在高噪声环境下的表现。

Method: 采用两轮小批量局部平均法处理从$d$维流形中提取的噪声样本，分析了在高噪声环境（噪声大小接近流形的reach $	au$）下的精度。

Result: 证明在高概率下，平均点$\\hat{\\mathbf q}$满足距离流形的误差界限$d(\\hat{\\mathbf q}, \\mathcal M) \\leq \\sigma \\sqrt{d\\left(1+\\frac{\\kappa\\mathrm{diam}(\\mathcal {M})}{\\log(D)}\\right)}$。

Conclusion: 这是首次在高噪声环境下对局部平均法进行理论分析，为去噪和降维方法提供了理论基础。

Abstract: High-dimensional data are ubiquitous, with examples ranging from natural
images to scientific datasets, and often reside near low-dimensional manifolds.
Leveraging this geometric structure is vital for downstream tasks, including
signal denoising, reconstruction, and generation. However, in practice, the
manifold is typically unknown and only noisy samples are available. A
fundamental approach to uncovering the manifold structure is local averaging,
which is a cornerstone of state-of-the-art provable methods for manifold
fitting and denoising. However, to the best of our knowledge, there are no
works that rigorously analyze the accuracy of local averaging in a manifold
setting in high-noise regimes. In this work, we provide theoretical analyses of
a two-round mini-batch local averaging method applied to noisy samples drawn
from a $d$-dimensional manifold $\mathcal M \subset \mathbb{R}^D$, under a
relatively high-noise regime where the noise size is comparable to the reach
$\tau$. We show that with high probability, the averaged point $\hat{\mathbf
q}$ achieves the bound $d(\hat{\mathbf q}, \mathcal M) \leq \sigma
\sqrt{d\left(1+\frac{\kappa\mathrm{diam}(\mathcal {M})}{\log(D)}\right)}$,
where $\sigma, \mathrm{diam(\mathcal M)},\kappa$ denote the standard deviation
of the Gaussian noise, manifold's diameter and a bound on its extrinsic
curvature, respectively. This is the first analysis of local averaging accuracy
over the manifold in the relatively high noise regime where $\sigma \sqrt{D}
\approx \tau$. The proposed method can serve as a preprocessing step for a wide
range of provable methods designed for lower-noise regimes. Additionally, our
framework can provide a theoretical foundation for a broad spectrum of
denoising and dimensionality reduction methods that rely on local averaging
techniques.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [383] [A Digital Twin Framework for Generation-IV Reactors with Reinforcement Learning-Enabled Health-Aware Supervisory Control](https://arxiv.org/abs/2506.17258)
*Jasmin Y. Lim,Dimitrios Pylorof,Humberto E. Garcia,Karthik Duraisamy*

Key words: 数字孪生、第四代核电站、氟盐冷却高温堆、强化学习、贝叶斯推理、运维优化

TL;DR: 提出了一种用于第四代核电站（氟盐冷却高温堆）的数字孪生框架，通过数据增强方法优化运维策略，结合替代建模、强化学习和贝叶斯推理实现闭环管理，验证了其健康感知和约束遵从的鲁棒性。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 第四代核电站虽具备高性能、安全和可持续性优势，但高昂成本阻碍其部署。数字孪生技术可降低成本并提升效率，为此设计了一个针对氟盐冷却高温堆的框架。

Method: 采用闭环框架整合替代建模、强化学习和贝叶斯推理，利用强化学习考虑组件健康状态优化发电目标，通过参考调节算法确保约束（如流量和温度限制）遵从，并结合贝叶斯滤波同化实时数据。

Result: 通过三个案例验证：长期运维计划、高频测量精度优化及边界条件变化的实时校准，证明了框架的健康感知和约束遵从能力。

Conclusion: 该数字孪生框架为第四代核电站提供了鲁棒的操作优化方案，并可推广至其他先进反应堆和复杂工程系统。

Abstract: Generation IV (Gen-IV) nuclear power plants are envisioned to replace the
current reactor fleet, bringing improvements in performance, safety,
reliability, and sustainability. However, large cost investments currently
inhibit the deployment of these advanced reactor concepts. Digital twins bridge
real-world systems with digital tools to reduce costs, enhance decision-making,
and boost operational efficiency. In this work, a digital twin framework is
designed to operate the Gen-IV Fluoride-salt-cooled High-temperature Reactor,
utilizing data-enhanced methods to optimize operational and maintenance
policies while adhering to system constraints. The closed-loop framework
integrates surrogate modeling, reinforcement learning, and Bayesian inference
to streamline end-to-end communication for online regulation and
self-adjustment. Reinforcement learning is used to consider component health
and degradation to drive the target power generations, with constraints
enforced through a Reference Governor control algorithm that ensures compliance
with pump flow rate and temperature limits. These input driving modules benefit
from detailed online simulations that are assimilated to measurement data with
Bayesian filtering. The digital twin is demonstrated in three case studies: a
one-year long-term operational period showcasing maintenance planning
capabilities, short-term accuracy refinement with high-frequency measurements,
and system shock capturing that demonstrates real-time recalibration
capabilities when change in boundary conditions. These demonstrations validate
robustness for health-aware and constraint-informed nuclear plant operation,
with general applicability to other advanced reactor concepts and complex
engineering systems.

</details>


### [384] [Conformal Safety Shielding for Imperfect-Perception Agents](https://arxiv.org/abs/2506.17275)
*William Scarbro,Calum Imrie,Sinem Getir Yaman,Kavan Fatehi,Corina S. Pasareanu,Radu Calinescu,Ravi Mangal*

Key words: 安全控制,屏蔽机制,共形预测,自主代理,感知错误

TL;DR: 提出了一种基于共形预测的屏蔽机制，为使用学习组件的自主代理提供运行时安全保障，防止感知错误引发的安全问题。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 解决自主代理因感知错误导致的安全控制问题。

Method: 使用共形预测构建屏蔽机制，限制代理在预测状态集合内的可用动作。

Result: 实现了局部和全局的安全保障，并通过实验验证了其有效性。

Conclusion: 提出的屏蔽机制能有效保障自主代理在感知错误条件下的安全性。

Abstract: We consider the problem of safe control in discrete autonomous agents that
use learned components for imperfect perception (or more generally, state
estimation) from high-dimensional observations. We propose a shield
construction that provides run-time safety guarantees under perception errors
by restricting the actions available to an agent, modeled as a Markov decision
process, as a function of the state estimates. Our construction uses conformal
prediction for the perception component, which guarantees that for each
observation, the predicted set of estimates includes the actual state with a
user-specified probability. The shield allows an action only if it is allowed
for all the estimates in the predicted set, resulting in a local safety
guarantee. We also articulate and prove a global safety property of existing
shield constructions for perfect-perception agents bounding the probability of
reaching unsafe states if the agent always chooses actions prescribed by the
shield. We illustrate our approach with a case-study of an experimental
autonomous system that guides airplanes on taxiways using high-dimensional
perception DNNs.

</details>


### [385] [A Theoretical Framework for Virtual Power Plant Integration with Gigawatt-Scale AI Data Centers: Multi-Timescale Control and Stability Analysis](https://arxiv.org/abs/2506.17284)
*Ali Peivandizadeh*

Key words: 人工智能数据中心、虚拟电厂、功率波动、分层控制、稳定性标准

TL;DR: 针对AI数据中心对电力系统带来的极快功率波动挑战，本文提出了一个四层分层控制框架来重构虚拟电厂（VPP），以应对毫秒级至24小时的时间尺度。该框架包含亚毫秒级的功率振荡抑制、新的稳定性标准以及可量化的工作负载灵活性，确保AI服务可用性。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: AI数据中心的快速电力需求波动（如每秒超过1000MW的变化）超出了传统VPP的调节能力，迫切需要新的控制框架来保证电力系统的稳定性。

Method: 提出了一个四层分层控制架构，包括亚毫秒级控制层、新的稳定性标准和灵活性量化指标，用于管理AI数据中心带来的电力波动。

Result: 框架能有效抑制功率振荡，将关键清除时间从150毫秒降至83毫秒，并通过工作负载灵活性实现30%的峰值功率削减，同时保持99.95%的AI服务可用性。

Conclusion: 该研究为未来占数据中心电力消耗50-70%的AI基础设施提供了数学基础，确保其稳定接入电力系统。

Abstract: The explosive growth of artificial intelligence has created gigawatt-scale
data centers that fundamentally challenge power system operation, exhibiting
power fluctuations exceeding 500 MW within seconds and millisecond-scale
variations of 50-75% of thermal design power. This paper presents a
comprehensive theoretical framework that reconceptualizes Virtual Power Plants
(VPPs) to accommodate these extreme dynamics through a four-layer hierarchical
control architecture operating across timescales from 100 microseconds to 24
hours.
  We develop control mechanisms and stability criteria specifically tailored to
converter-dominated systems with pulsing megawatt-scale loads. We prove that
traditional VPP architectures, designed for aggregating distributed resources
with response times of seconds to minutes, cannot maintain stability when
confronted with AI data center dynamics exhibiting slew rates exceeding 1,000
MW/s at gigawatt scale.
  Our framework introduces: (1) a sub-millisecond control layer that interfaces
with data center power electronics to actively dampen power oscillations; (2)
new stability criteria incorporating protection system dynamics, demonstrating
that critical clearing times reduce from 150 ms to 83 ms for gigawatt-scale
pulsing loads; and (3) quantified flexibility characterization showing that
workload deferability enables 30% peak reduction while maintaining AI service
availability above 99.95%.
  This work establishes the mathematical foundations necessary for the stable
integration of AI infrastructure that will constitute 50-70% of data center
electricity consumption by 2030.

</details>


### [386] [Dynamic Hybrid Modeling: Incremental Identification and Model Predictive Control](https://arxiv.org/abs/2506.18344)
*Adrian Caspari,Thomas Bierweiler,Sarah Fadda,Daniel Labisch,Maarten Nauta,Franzisko Wagner,Merle Warmbold,Constantinos C. Pantelides*

Key words: 混合模型, 动态识别, 数据驱动模型, 机器学习, 参数估计

TL;DR: 论文提出了一种渐进式识别动态混合模型的方法，通过解耦机理和数据驱动组件解决计算和概念困难。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 传统数学模型在计算时间、算法复杂度和开发成本方面存在限制，混合模型结合了机理和数据驱动模型，但动态混合模型的识别仍具挑战性。

Method: 采用四步渐进方法：正则化动态参数估计、相关性分析、数据驱动模型识别和混合模型集成。

Result: 案例研究表明该方法在复杂系统和数据有限情况下具有鲁棒性、可靠性和高效性。

Conclusion: 渐进式方法可加速混合模型开发，并支持数据驱动组件的独立识别。

Abstract: Mathematical models are crucial for optimizing and controlling chemical
processes, yet they often face significant limitations in terms of
computational time, algorithm complexity, and development costs. Hybrid models,
which combine mechanistic models with data-driven models (i.e. models derived
via the application of machine learning to experimental data), have emerged as
a promising solution to these challenges. However, the identification of
dynamic hybrid models remains difficult due to the need to integrate
data-driven models within mechanistic model structures. We present an
incremental identification approach for dynamic hybrid models that decouples
the mechanistic and data-driven components to overcome computational and
conceptual difficulties. Our methodology comprises four key steps: (1)
regularized dynamic parameter estimation to determine optimal time profiles for
flux variables, (2) correlation analysis to evaluate relationships between
variables, (3) data-driven model identification using advanced machine learning
techniques, and (4) hybrid model integration to combine the mechanistic and
data-driven components. This approach facilitates early evaluation of model
structure suitability, accelerates the development of hybrid models, and allows
for independent identification of data-driven components. Three case studies
are presented to illustrate the robustness, reliability, and efficiency of our
incremental approach in handling complex systems and scenarios with limited
data.

</details>


### [387] [Frequency Control in Microgrids: An Adaptive Fuzzy-Neural-Network Virtual Synchronous Generator](https://arxiv.org/abs/2506.18611)
*Waleed Breesam,Rezvan Alamian,Nima Tashakor,Brahim Elkhalil Youcefa,Stefan M. Goetz*

Key words: 微电网、虚拟同步发电机、模糊神经网络、频率调节、动态参数调整

TL;DR: 论文提出了一种通过模糊神经网络控制器动态调整虚拟同步发电机参数的方法，以提高微电网的频率稳定性。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 随着分布式可再生能源的增加，电力电子设备取代同步发电机导致微电网动态特性变化，降低了系统惯性和阻尼。固定参数的虚拟同步发电机无法满足频率调节需求，需要动态调整参数以增强稳定性。

Method: 使用模糊神经网络控制器在线训练，动态调整虚拟同步发电机的惯性、阻尼和下垂参数。

Result: 在MATLAB/Simulink模型和基于ARM系统的硬件在环实验中验证，与传统和模糊逻辑控制器相比，频率偏差降至0.03 Hz以下，恢复时间缩短。

Conclusion: 动态调整虚拟参数的模糊神经网络控制器能有效提升微电网频率稳定性。

Abstract: The reliance on distributed renewable energy has increased recently. As a
result, power electronic-based distributed generators replaced synchronous
generators which led to a change in the dynamic characteristics of the
microgrid. Most critically, they reduced system inertia and damping. Virtual
synchronous generators emulated in power electronics, which mimic the dynamic
behaviour of synchronous generators, are meant to fix this problem. However,
fixed virtual synchronous generator parameters cannot guarantee a frequency
regulation within the acceptable tolerance range. Conversely, a dynamic
adjustment of these virtual parameters promises robust solution with stable
frequency. This paper proposes a method to adapt the inertia, damping, and
droop parameters dynamically through a fuzzy neural network controller. This
controller trains itself online to choose appropriate values for these virtual
parameters. The proposed method can be applied to a typical AC microgrid by
considering the penetration and impact of renewable energy sources. We study
the system in a MATLAB/Simulink model and validate it experimentally in real
time using hardware-in-the-loop based on an embedded ARM system (SAM3X8E,
Cortex-M3). Compared to traditional and fuzzy logic controller methods, the
results demonstrate that the proposed method significantly reduces the
frequency deviation to less than 0.03 Hz and shortens the stabilizing/recovery
time.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [388] [Rethinking the Role of Operating Conditions for Learning-based Multi-condition Fault Diagnosis](https://arxiv.org/abs/2506.17740)
*Pengyu Han,Zeyi Liu,Shijin Chen,Dongliang Zou,Xiao He*

Key words: 多工况故障诊断, 领域泛化, 两阶段框架, 齿轮箱, 再训练策略

TL;DR: 本文研究了多工况条件下故障诊断的挑战，并提出了一种两阶段诊断框架，结合领域泛化编码器和再训练策略，以提高诊断性能。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 多工况故障诊断在工业系统中很常见，但数据分布差异会影响模型性能。领域泛化方法虽能提取工况不变特征，但工况对故障信息的影响未被充分研究。本文旨在填补这一空白并提出改进方法。

Method: 提出两阶段诊断框架，结合领域泛化编码器和再训练策略，提取工况不变特征并避免过拟合。在真实齿轮箱数据集上进行多实验验证。

Result: 实验表明，该框架能有效提升在变转速和变负载工况下的故障诊断性能。

Conclusion: 两阶段框架解决了工况对故障特征的显著影响问题，显著提升了诊断泛化能力。

Abstract: Multi-condition fault diagnosis is prevalent in industrial systems and
presents substantial challenges for conventional diagnostic approaches. The
discrepancy in data distributions across different operating conditions
degrades model performance when a model trained under one condition is applied
to others. With the recent advancements in deep learning, transfer learning has
been introduced to the fault diagnosis field as a paradigm for addressing
multi-condition fault diagnosis. Among these methods, domain generalization
approaches can handle complex scenarios by extracting condition-invariant fault
features. Although many studies have considered fault diagnosis in specific
multi-condition scenarios, the extent to which operating conditions affect
fault information has been scarcely studied, which is crucial. However, the
extent to which operating conditions affect fault information has been scarcely
studied, which is crucial. When operating conditions have a significant impact
on fault features, directly applying domain generalization methods may lead the
model to learn condition-specific information, thereby reducing its overall
generalization ability. This paper investigates the performance of existing
end-to-end domain generalization methods under varying conditions, specifically
in variable-speed and variable-load scenarios, using multiple experiments on a
real-world gearbox. Additionally, a two-stage diagnostic framework is proposed,
aiming to improve fault diagnosis performance under scenarios with significant
operating condition impacts. By incorporating a domain-generalized encoder with
a retraining strategy, the framework is able to extract condition-invariant
fault features while simultaneously alleviating potential overfitting to the
source domain. Several experiments on a real-world gearbox dataset are
conducted to validate the effectiveness of the proposed approach.

</details>


### [389] [Fast State-Augmented Learning for Wireless Resource Allocation with Dual Variable Regression](https://arxiv.org/abs/2506.18748)
*Yigit Berkay Uslu,Navid NaderiAlizadeh,Mark Eisen,Alejandro Ribeiro*

Key words: 资源分配、图神经网络、对偶变量、无线网络、拉格朗日优化

TL;DR: 本文提出了一种基于状态增强图神经网络（GNN）的资源分配方法，用于解决多用户无线网络中的资源分配问题，优于传统的对偶次梯度方法，并通过实验验证了其性能和理论收敛性。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 传统的对偶次梯度方法在多用户无线网络资源配置中存在局限性，需要一种更高效且能适应动态网络的优化方法。

Method: 使用状态增强GNN表示网络配置和动态对偶变量，通过离线训练学习拉格朗日最大化策略，并在推理阶段通过梯度更新对偶变量。

Result: 提出的算法在传输功率控制案例中表现出优越性能，并证明了对偶函数最优间隙的收敛性和概率界限。

Conclusion: 状态增强GNN方法为多用户无线网络资源分配提供了一种高效且理论支持的解决方案。

Abstract: We consider resource allocation problems in multi-user wireless networks,
where the goal is to optimize a network-wide utility function subject to
constraints on the ergodic average performance of users. We demonstrate how a
state-augmented graph neural network (GNN) parametrization for the resource
allocation policy circumvents the drawbacks of the ubiquitous dual subgradient
methods by representing the network configurations (or states) as graphs and
viewing dual variables as dynamic inputs to the model, viewed as graph signals
supported over the graphs. Lagrangian maximizing state-augmented policies are
learned during the offline training phase, and the dual variables evolve
through gradient updates while executing the learned state-augmented policies
during the inference phase. Our main contributions are to illustrate how
near-optimal initialization of dual multipliers for faster inference can be
accomplished with dual variable regression, leveraging a secondary GNN
parametrization, and how maximization of the Lagrangian over the multipliers
sampled from the dual descent dynamics substantially improves the training of
state-augmented models. We demonstrate the superior performance of the proposed
algorithm with extensive numerical experiments in a case study of transmit
power control. Finally, we prove a convergence result and an exponential
probability bound on the excursions of the dual function (iterate) optimality
gaps.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [390] [Auto-Regressive Surface Cutting](https://arxiv.org/abs/2506.18017)
*Yang Li,Victor Cheung,Xinhai Liu,Yuguang Chen,Zhongjin Luo,Biwen Lei,Haohan Weng,Zibo Zhao,Jingwei Huang,Zhuo Chen,Chunchao Guo*

Key words: 表面切割, UV展开, GPT变换器, 3D分割

TL;DR: SeamGPT是一种自动回归模型，通过模仿专业工作流程生成切割缝，将表面切割任务转化为下一个标记预测任务，并在UV展开基准测试中表现出色，同时改善了3D分割工具的边界质量。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 现有的表面切割方法虽然技术有效，但常导致碎片化的图谱，缺乏语义一致性。

Method: 将表面切割建模为下一个标记预测任务，通过采样点云、形状条件编码和GPT风格变换器顺序预测缝段。

Result: 在包含流形和非流形网格的UV展开基准测试中表现优异，并为3D分割提供了更清晰的边界。

Conclusion: SeamGPT通过模仿专业工作流程，提供了语义一致且技术先进的表面切割解决方案。

Abstract: Surface cutting is a fundamental task in computer graphics, with applications
in UV parameterization, texture mapping, and mesh decomposition. However,
existing methods often produce technically valid but overly fragmented atlases
that lack semantic coherence. We introduce SeamGPT, an auto-regressive model
that generates cutting seams by mimicking professional workflows. Our key
technical innovation lies in formulating surface cutting as a next token
prediction task: sample point clouds on mesh vertices and edges, encode them as
shape conditions, and employ a GPT-style transformer to sequentially predict
seam segments with quantized 3D coordinates. Our approach achieves exceptional
performance on UV unwrapping benchmarks containing both manifold and
non-manifold meshes, including artist-created, and 3D-scanned models. In
addition, it enhances existing 3D segmentation tools by providing clean
boundaries for part decomposition.

</details>


### [391] [Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models](https://arxiv.org/abs/2506.18251)
*Chao Li,Jiawei Fan,Anbang Yao*

Key words: 扩散模型, 双采样, 加速, 残差反馈, 图像生成

TL;DR: Morse提出了一种无损加速扩散模型的双采样框架，通过快速跳跃采样和自适应残差反馈策略提升效率。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 旨在解决扩散模型迭代生成过程效率低下的问题。

Method: 结合Dash和Dot两个模型，Dash进行跳跃采样，Dot生成残差反馈以匹配Dash的下一步估计。

Result: 在6个图像生成任务中实现了1.78X至3.31X的无损加速，并适用于Latent Consistency Model。

Conclusion: Morse框架在提升生成效率的同时保持了图像质量，具有广泛适用性。

Abstract: In this paper, we present Morse, a simple dual-sampling framework for
accelerating diffusion models losslessly. The key insight of Morse is to
reformulate the iterative generation (from noise to data) process via taking
advantage of fast jump sampling and adaptive residual feedback strategies.
Specifically, Morse involves two models called Dash and Dot that interact with
each other. The Dash model is just the pre-trained diffusion model of any type,
but operates in a jump sampling regime, creating sufficient space for sampling
efficiency improvement. The Dot model is significantly faster than the Dash
model, which is learnt to generate residual feedback conditioned on the
observations at the current jump sampling point on the trajectory of the Dash
model, lifting the noise estimate to easily match the next-step estimate of the
Dash model without jump sampling. By chaining the outputs of the Dash and Dot
models run in a time-interleaved fashion, Morse exhibits the merit of flexibly
attaining desired image generation performance while improving overall runtime
efficiency. With our proposed weight sharing strategy between the Dash and Dot
models, Morse is efficient for training and inference. Our method shows a
lossless speedup of 1.78X to 3.31X on average over a wide range of sampling
step budgets relative to 9 baseline diffusion models on 6 image generation
tasks. Furthermore, we show that our method can be also generalized to improve
the Latent Consistency Model (LCM-SDXL, which is already accelerated with
consistency distillation technique) tailored for few-step text-to-image
synthesis. The code and models are available at
https://github.com/deep-optimization/Morse.

</details>


### [392] [BulletGen: Improving 4D Reconstruction with Bullet-Time Generation](https://arxiv.org/abs/2506.18601)
*Denys Rozumnyi,Jonathon Luiten,Numair Khan,Johannes Schönberger,Peter Kontschieder*

Key words: 动态场景重建, 生成模型, 4D高斯表示, 新颖视图合成, 单目视频

TL;DR: BulletGen利用生成模型修正4D高斯动态场景表示中的错误和缺失信息，通过扩散视频生成模型的输出对齐来实现，并在新颖视图合成和2D/3D跟踪任务中取得最先进效果。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 解决从单目视频中重建动态场景时的信息缺失和深度估计模糊性问题。

Method: 结合生成模型（扩散视频生成）和高斯动态场景表示，通过冻结的“子弹时间”步骤对齐生成内容并进行优化。

Result: 在新颖视图合成和2D/3D跟踪任务中表现优异。

Conclusion: BulletGen通过生成内容与动态场景的无缝融合，显著提升了动态场景重建的准确性和完整性。

Abstract: Transforming casually captured, monocular videos into fully immersive dynamic
experiences is a highly ill-posed task, and comes with significant challenges,
e.g., reconstructing unseen regions, and dealing with the ambiguity in
monocular depth estimation. In this work we introduce BulletGen, an approach
that takes advantage of generative models to correct errors and complete
missing information in a Gaussian-based dynamic scene representation. This is
done by aligning the output of a diffusion-based video generation model with
the 4D reconstruction at a single frozen "bullet-time" step. The generated
frames are then used to supervise the optimization of the 4D Gaussian model.
Our method seamlessly blends generative content with both static and dynamic
scene components, achieving state-of-the-art results on both novel-view
synthesis, and 2D/3D tracking tasks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [393] [PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning](https://arxiv.org/abs/2506.17338)
*Duong Bach*

Key words: 多智能体系统, 知识管理, 遗忘协议, 语义投票, PBFT共识

TL;DR: 多智能体系统中引入基于语义投票和时间衰减的联合遗忘协议，显著减少内存占用并提高数据同步效率。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 解决多智能体系统中共享知识管理的挑战，确保分布式记忆的同步和高效性。

Method: 结合轻量级DistilBERT模型进行语义投票、多尺度时间衰减函数和PBFT共识机制。

Result: 内存占用减少52%，遗忘决策准确率达88%，PBFT共识成功率92%，缓存命中率82%。

Conclusion: Co-Forgetting Protocol有效提升多智能体系统的知识管理能力。

Abstract: The proliferation of multi-agent systems (MAS) in complex, dynamic
environments necessitates robust and efficient mechanisms for managing shared
knowledge. A critical challenge is ensuring that distributed memories remain
synchronized, relevant, and free from the accumulation of outdated or
inconsequential data - a process analogous to biological forgetting. This paper
introduces the Co-Forgetting Protocol, a novel, comprehensive framework
designed to address this challenge by enabling synchronized memory pruning in
MAS. The protocol integrates three key components: (1) context-aware semantic
voting, where agents utilize a lightweight DistilBERT model to assess the
relevance of memory items based on their content and the current operational
context; (2) multi-scale temporal decay functions, which assign diminishing
importance to memories based on their age and access frequency across different
time horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based
consensus mechanism, ensuring that decisions to retain or discard memory items
are agreed upon by a qualified and fault-tolerant majority of agents, even in
the presence of up to f Byzantine (malicious or faulty) agents in a system of N
greater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient
inter-agent communication and Pinecone for scalable vector embedding storage
and similarity search, with SQLite managing metadata. Experimental evaluations
in a simulated MAS environment with four agents demonstrate the protocol's
efficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%
voting accuracy in forgetting decisions against human-annotated benchmarks, a
92% PBFT consensus success rate under simulated Byzantine conditions, and an
82% cache hit rate for memory access.

</details>


### [394] [Speeding up Local Optimization in Vehicle Routing with Tensor-based GPU Acceleration](https://arxiv.org/abs/2506.17357)
*Zhenyu Lei,Jin-Kao Hao,Qinghua Wu*

Key words: 车辆路径问题, 局部搜索, GPU加速, 张量, 计算效率

TL;DR: 本文提出了一种基于张量的GPU加速方法，用于提升车辆路径问题（VRP）中局部搜索算子的计算效率。该方法具有广泛的扩展性和低耦合架构，通过实验验证了其在计算效率上的显著优势。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 局部搜索在车辆路径问题及其变体的启发式算法中非常重要，但其探索过程通常计算量大且耗时，尤其是对于大规模或复杂约束的问题。

Method: 提出了一种基于张量的GPU加速方法，采用属性表示方式，将密集计算完全卸载到GPU上，实现了低耦合架构和广泛的可扩展性。

Result: 通过在三类路由问题的基准实例上的比较实验，证明了该方法在计算效率上显著优于传统的CPU实现。

Conclusion: 该方法不仅显著提升了计算效率，还提供了性能分析和潜在瓶颈的详细见解，为未来改进指明了方向。

Abstract: Local search plays a central role in many effective heuristic algorithms for
the vehicle routing problem (VRP) and its variants. However, neighborhood
exploration is known to be computationally expensive and time consuming,
especially for large instances or problems with complex constraints. In this
study, we explore a promising direction to address this challenge by
introducing an original tensor-based GPU acceleration method designed to speed
up the commonly used local search operators in vehicle routing. By using an
attribute-based representation, the method offers broad extensibility, making
it applicable to different VRP variants. Its low-coupling architecture, with
intensive computations completely offloaded to the GPU, ensures seamless
integration in various local search-based algorithms and frameworks, leading to
significant improvements in computational efficiency and potentially improved
solution quality. Through comparative experiments on benchmark instances of
three routing problems, we demonstrate the substantial computational advantages
of the proposed approach over traditional CPU-based implementations. We also
provide a detailed analysis of the strengths and limitations of the method,
providing valuable insights into its performance characteristics and
identifying potential bottlenecks in practical applications. These findings
contribute to a better understanding and suggest directions for future
improvements.

</details>


### [395] [ConsumerBench: Benchmarking Generative AI Applications on End-User Devices](https://arxiv.org/abs/2506.17538)
*Yile Gu,Rohan Kadekodi,Hoang Nguyen,Keisuke Kamahori,Yiyu Liu,Baris Kasikci*

Key words: GenAI, 终端设备, 基准测试, 资源管理, 系统效率

TL;DR: ConsumerBench是一个用于评估终端设备上GenAI模型系统效率和响应时间的基准框架，模拟真实多应用场景，揭示资源分配问题，并提供改进建议。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: GenAI应用从云端迁移到终端设备，带来了资源管理、系统效率和用户体验的新挑战。

Method: ConsumerBench通过模拟多应用并发执行的场景，定制化工作流，捕获应用和系统级指标。

Result: 实验揭示了资源共享的低效、贪婪分配下的不公平调度以及静态模型服务器配置的性能缺陷。

Conclusion: 提出针对消费级GPU架构的定制内核和SLO感知调度策略的实用建议。

Abstract: The recent shift in Generative AI (GenAI) applications from cloud-only
environments to end-user devices introduces new challenges in resource
management, system efficiency, and user experience. This paper presents
ConsumerBench, a comprehensive benchmarking framework designed to evaluate the
system efficiency and response time of GenAI models running on end-user
devices. Unlike existing benchmarks that assume exclusive model access on
dedicated GPUs, ConsumerBench simulates realistic multi-application scenarios
executing concurrently on constrained hardware. Furthermore, ConsumerBench
supports customizable workflows that simulate complex tasks requiring
coordination among multiple applications. ConsumerBench captures both
application-level metrics, including latency and Service Level Objective (SLO)
attainment, and system-level metrics like CPU/GPU utilization and memory
bandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies
in resource sharing, unfair scheduling under greedy allocation, and performance
pitfalls of static model server configurations. The paper also provides
practical insights for model developers and system designers, highlighting the
benefits of custom kernels tailored to consumer-grade GPU architectures and the
value of implementing SLO-aware scheduling strategies.

</details>


### [396] [Research on Model Parallelism and Data Parallelism Optimization Methods in Large Language Model-Based Recommendation Systems](https://arxiv.org/abs/2506.17551)
*Haowei Yang,Yu Tian,Zhongheng Yang,Zhao Wang,Chengrui Zhou,Dannier Li*

Key words: 大型语言模型,推荐系统,分布式训练,模型并行,数据并行

TL;DR: 论文研究了大型语言模型（LLMs）在推荐系统分布式训练中的优化方法，提出混合并行方案，提升训练效率和资源利用率。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 解决LLMs在推荐系统中因参数规模和数据量导致的算力和通信瓶颈问题。

Method: 结合模型并行（包括张量和流水线并行）与数据并行（同步和异步模式），引入自适应负载均衡和梯度压缩技术。

Result: 混合方案在真实推荐数据集上实现训练吞吐量提升30%，资源利用率提高20%。

Conclusion: 混合并行方案具有强扩展性和鲁棒性，未来可探索异构硬件集成和自动调度技术。

Abstract: With the rapid adoption of large language models (LLMs) in recommendation
systems, the computational and communication bottlenecks caused by their
massive parameter sizes and large data volumes have become increasingly
prominent. This paper systematically investigates two classes of optimization
methods-model parallelism and data parallelism-for distributed training of LLMs
in recommendation scenarios. For model parallelism, we implement both tensor
parallelism and pipeline parallelism, and introduce an adaptive load-balancing
mechanism to reduce cross-device communication overhead. For data parallelism,
we compare synchronous and asynchronous modes, combining gradient compression
and sparsification techniques with an efficient aggregation communication
framework to significantly improve bandwidth utilization. Experiments conducted
on a real-world recommendation dataset in a simulated service environment
demonstrate that our proposed hybrid parallelism scheme increases training
throughput by over 30% and improves resource utilization by approximately 20%
compared to traditional single-mode parallelism, while maintaining strong
scalability and robustness. Finally, we discuss trade-offs among different
parallel strategies in online deployment and outline future directions
involving heterogeneous hardware integration and automated scheduling
technologies.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [397] [LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning](https://arxiv.org/abs/2506.17562)
*Haoxuan Che,Haibo Jin,Zhengrui Guo,Yi Lin,Cheng Jin,Hao Chen*

Key words: 联邦学习,医学报告生成,多模态数据,隐私保护,低秩分解

TL;DR: FedMRG 是首个利用联邦学习（FL）开发隐私保护的、多中心 LLM 驱动的医学报告生成（MRG）模型的框架，解决了多模态数据异质性和通信效率问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 医学图像-报告对分散且受隐私法规限制，难以集中用于 LLM 驱动的 MRG 模型开发。

Method: 采用低秩分解减少通信开销，结合客户端感知对比学习和双适配器机制处理数据异质性。

Result: FedMRG 在多中心数据上表现出良好的泛化性和适应性，同时保持临床准确性和通信效率。

Conclusion: FedMRG 为隐私保护的 MRG 模型开发提供了可行方案，推动了多中心协作。

Abstract: LLMs have demonstrated significant potential in Medical Report Generation
(MRG), yet their development requires large amounts of medical image-report
pairs, which are commonly scattered across multiple centers. Centralizing these
data is exceptionally challenging due to privacy regulations, thereby impeding
model development and broader adoption of LLM-driven MRG models. To address
this challenge, we present FedMRG, the first framework that leverages Federated
Learning (FL) to enable privacy-preserving, multi-center development of
LLM-driven MRG models, specifically designed to overcome the critical challenge
of communication-efficient LLM training under multi-modal data heterogeneity.
To start with, our framework tackles the fundamental challenge of communication
overhead in FL-LLM tuning by employing low-rank factorization to efficiently
decompose parameter updates, significantly reducing gradient transmission costs
and making LLM-driven MRG feasible in bandwidth-constrained FL settings.
Furthermore, we observed the dual heterogeneity in MRG under the FL scenario:
varying image characteristics across medical centers, as well as diverse
reporting styles and terminology preferences. To address this, we further
enhance FedMRG with (1) client-aware contrastive learning in the MRG encoder,
coupled with diagnosis-driven prompts, which capture both globally
generalizable and locally distinctive features while maintaining diagnostic
accuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder
that harmonizes generic and specialized adapters to address variations in
reporting styles and terminology. Through extensive evaluation of our
established FL-MRG benchmark, we demonstrate the generalizability and
adaptability of FedMRG, underscoring its potential in harnessing multi-center
data and generating clinically accurate reports while maintaining communication
efficiency.

</details>


### [398] [RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar Chart Sequences](https://arxiv.org/abs/2506.17325)
*Sina Najafi,M. Hadi Sepanj,Fahimeh Jafari*

Key words: 用户流失预测,零工平台,计算机视觉,雷达图,LSTM

TL;DR: 提出了一种基于时间感知的计算机视觉框架，通过雷达图序列建模用户行为，显著提升非订阅制零工平台的用户流失预测性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决非订阅制零工平台因用户行为隐式且动态变化导致流失预测的独特挑战。

Method: 使用预训练CNN编码器和双向LSTM结合，从雷达图图像序列中捕捉时空行为模式。

Result: 在真实数据集上表现优于传统方法，F1分数提升17.7，精确度提升29.4，AUC提升16.1。

Conclusion: 模块化设计、解释性工具和高效部署特性使其适合动态零工经济平台的大规模流失建模。

Abstract: Predicting user churn in non-subscription gig platforms, where disengagement
is implicit, poses unique challenges due to the absence of explicit labels and
the dynamic nature of user behavior. Existing methods often rely on aggregated
snapshots or static visual representations, which obscure temporal cues
critical for early detection. In this work, we propose a temporally-aware
computer vision framework that models user behavioral patterns as a sequence of
radar chart images, each encoding day-level behavioral features. By integrating
a pretrained CNN encoder with a bidirectional LSTM, our architecture captures
both spatial and temporal patterns underlying churn behavior. Extensive
experiments on a large real-world dataset demonstrate that our method
outperforms classical models and ViT-based radar chart baselines, yielding
gains of 17.7 in F1 score, 29.4 in precision, and 16.1 in AUC, along with
improved interpretability. The framework's modular design, explainability
tools, and efficient deployment characteristics make it suitable for
large-scale churn modeling in dynamic gig-economy platforms.

</details>


### [399] [CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning](https://arxiv.org/abs/2506.17629)
*Kailing Li,Qi'ao Xu,Tianwen Qian,Yuqian Fu,Yang Jiao,Xiaoling Wang*

Key words: Embodied Visual Reasoning, Large Language Models, Vision-Language Models, Cognitive Map

TL;DR: CLiViS是一个无需训练的框架，结合了LLMs的任务规划和VLMs的视觉感知能力，通过动态认知地图解决EVR中的复杂指令和时空动态挑战。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决嵌入式视觉推理（EVR）中复杂指令多样性和长期视频时空动态的挑战，充分利用LLMs和VLMs的互补优势。

Method: 提出CLiViS框架，通过LLMs进行高级任务规划，VLMs进行开放世界视觉感知，并通过动态认知地图迭代更新场景上下文。

Result: 在多个基准测试中表现出色，尤其在处理长期视觉依赖方面效果显著。

Conclusion: CLiViS通过结合LLMs和VLMs的优势，有效解决了EVR中的复杂问题。

Abstract: Embodied Visual Reasoning (EVR) seeks to follow complex, free-form
instructions based on egocentric video, enabling semantic understanding and
spatiotemporal reasoning in dynamic environments. Despite its promising
potential, EVR encounters significant challenges stemming from the diversity of
complex instructions and the intricate spatiotemporal dynamics in long-term
egocentric videos. Prior solutions either employ Large Language Models (LLMs)
over static video captions, which often omit critical visual details, or rely
on end-to-end Vision-Language Models (VLMs) that struggle with stepwise
compositional reasoning. Consider the complementary strengths of LLMs in
reasoning and VLMs in perception, we propose CLiViS. It is a novel
training-free framework that leverages LLMs for high-level task planning and
orchestrates VLM-driven open-world visual perception to iteratively update the
scene context. Building on this synergy, the core of CLiViS is a dynamic
Cognitive Map that evolves throughout the reasoning process. This map
constructs a structured representation of the embodied scene, bridging
low-level perception and high-level reasoning. Extensive experiments across
multiple benchmarks demonstrate the effectiveness and generality of CLiViS,
especially in handling long-term visual dependencies. Code is available at
https://github.com/Teacher-Tom/CLiViS.

</details>


### [400] [P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments](https://arxiv.org/abs/2506.17332)
*Haitian Wang,Yiren Wang,Xinyu Wang,Yumeng Miao,Yuliang Zhang,Yu Zhang,Atif Mansoor*

Key words: 跌倒检测、多模态、隐私保护、老年人、浴室环境、毫米波雷达、3D振动传感

TL;DR: 该论文提出了一种隐私保护的多模态跌倒检测系统，针对老年人在浴室环境中的跌倒风险，结合毫米波雷达和3D振动传感，显著提高了检测的准确率和召回率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 全球老龄化加剧，浴室是跌倒事故高发区，现有单模态检测系统在复杂环境中精度不足。

Method: 开发传感器评估框架，融合毫米波雷达与3D振动传感数据，构建隐私保护的多模态数据集，并设计双流网络P2MFDS结合雷达与振动特征。

Result: P2MFDS在准确率和召回率上显著优于现有方法。

Conclusion: 多模态方法解决了单模态系统的局限性，为老年人浴室跌倒检测提供了高效、隐私保护的解决方案。

Abstract: By 2050, people aged 65 and over are projected to make up 16 percent of the
global population. As aging is closely associated with increased fall risk,
particularly in wet and confined environments such as bathrooms where over 80
percent of falls occur. Although recent research has increasingly focused on
non-intrusive, privacy-preserving approaches that do not rely on wearable
devices or video-based monitoring, these efforts have not fully overcome the
limitations of existing unimodal systems (e.g., WiFi-, infrared-, or
mmWave-based), which are prone to reduced accuracy in complex environments.
These limitations stem from fundamental constraints in unimodal sensing,
including system bias and environmental interference, such as multipath fading
in WiFi-based systems and drastic temperature changes in infrared-based
methods. To address these challenges, we propose a Privacy-Preserving
Multimodal Fall Detection System for Elderly People in Bathroom Environments.
First, we develop a sensor evaluation framework to select and fuse
millimeter-wave radar with 3D vibration sensing, and use it to construct and
preprocess a large-scale, privacy-preserving multimodal dataset in real
bathroom settings, which will be released upon publication. Second, we
introduce P2MFDS, a dual-stream network combining a CNN-BiLSTM-Attention branch
for radar motion dynamics with a multi-scale CNN-SEBlock-Self-Attention branch
for vibration impact detection. By uniting macro- and micro-scale features,
P2MFDS delivers significant gains in accuracy and recall over state-of-the-art
approaches. Code and pretrained models will be made available at:
https://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom.

</details>


### [401] [A Novel Multi-layer Task-centric and Data Quality Framework for Autonomous Driving](https://arxiv.org/abs/2506.17346)
*Yuhan Zhou,Haihua Chen,Kewei Sha*

Key words: 自动驾驶车辆, 数据质量, 多模态数据, 任务导向, 性能目标

TL;DR: 本文提出了一种以任务为中心、数据质量为基础的五层框架，用于解决自动驾驶车辆（AV）中数据质量（DQ）与任务需求及性能目标的映射问题。通过案例研究展示了数据冗余对任务性能的影响，并探讨了多模态数据的DQ问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 自动驾驶车辆的研究和实践过于关注模型/算法，而忽视了数据质量的重要性。本文旨在通过一个系统框架，提升AV在功能、效率和可信度方面的表现。

Method: 提出了一个五层框架，包括数据层、DQ层、任务层、应用层和目标层，用于将DQ与任务需求和性能目标关联。通过nuScenes数据集案例验证了部分冗余数据移除对YOLOv8目标检测任务性能的提升。

Result: 案例研究表明，减少多源图像数据的冗余可提高目标检测性能；多模态数据（图像和LiDAR）的分析揭示了现有DQ问题。

Conclusion: 本文为自动驾驶领域提供了一个关键但尚未充分研究的视角，即DQ、任务编排和性能导向系统开发的结合。

Abstract: The next-generation autonomous vehicles (AVs), embedded with frequent
real-time decision-making, will rely heavily on a large volume of multisource
and multimodal data. In real-world settings, the data quality (DQ) of different
sources and modalities usually varies due to unexpected environmental factors
or sensor issues. However, both researchers and practitioners in the AV field
overwhelmingly concentrate on models/algorithms while undervaluing the DQ. To
fulfill the needs of the next-generation AVs with guarantees of functionality,
efficiency, and trustworthiness, this paper proposes a novel task-centric and
data quality vase framework which consists of five layers: data layer, DQ
layer, task layer, application layer, and goal layer. The proposed framework
aims to map DQ with task requirements and performance goals. To illustrate, a
case study investigating redundancy on the nuScenes dataset proves that
partially removing redundancy on multisource image data could improve YOLOv8
object detection task performance. Analysis on multimodal data of image and
LiDAR further presents existing redundancy DQ issues. This paper opens up a
range of critical but unexplored challenges at the intersection of DQ, task
orchestration, and performance-oriented system development in AVs. It is
expected to guide the AV community toward building more adaptive, explainable,
and resilient AVs that respond intelligently to dynamic environments and
heterogeneous data streams. Code, data, and implementation details are publicly
available at: https://anonymous.4open.science/r/dq4av-framework/README.md.

</details>


### [402] [PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding](https://arxiv.org/abs/2506.18023)
*Kui Huang,Xinrong Chen,Wenyu Lv,Jincheng Liao,Guanzhong Wang,Yi Liu*

Key words: 多模态文档理解, ViT, 数据优化, 特征融合

TL;DR: PP-DocBee2是一个改进的多模态文档理解模型，通过提升数据质量、优化视觉特征融合和推理方法，性能提升11.4%，延迟降低73%。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决PP-DocBee在多模态文档理解中的局限性，提升性能和效率。

Method: 采用大规模多模态预训练模型评估数据质量，改进ViT表示能力和特征融合策略。

Result: 性能提升11.4%，推理延迟降低73%。

Conclusion: PP-DocBee2在多模态文档任务中表现出显著改进，代码和模型已开源。

Abstract: This report introduces PP-DocBee2, an advanced version of the PP-DocBee,
designed to enhance multimodal document understanding. Built on a large
multimodal model architecture, PP-DocBee2 addresses the limitations of its
predecessor through key technological improvements, including enhanced
synthetic data quality, improved visual feature fusion strategy, and optimized
inference methodologies. These enhancements yield an $11.4\%$ performance boost
on internal benchmarks for Chinese business documents, and reduce inference
latency by $73.0\%$ to the vanilla version. A key innovation of our work is a
data quality optimization strategy for multimodal document tasks. By employing
a large-scale multimodal pre-trained model to evaluate data, we apply a novel
statistical criterion to filter outliers, ensuring high-quality training data.
Inspired by insights into underutilized intermediate features in multimodal
models, we enhance the ViT representational capacity by decomposing it into
layers and applying a novel feature fusion strategy to improve complex
reasoning. The source code and pre-trained model are available at
\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.

</details>


### [403] [From Drawings to Decisions: A Hybrid Vision-Language Framework for Parsing 2D Engineering Drawings into Structured Manufacturing Knowledge](https://arxiv.org/abs/2506.17374)
*Muhammad Tayyab Khan,Lequn Chen,Zane Yong,Jun Ming Tan,Wenhe Feng,Seung Ki Moon*

Key words: 2D工程图纸, GD&T, 旋转感知检测, 视觉语言模型, Donut, Florence-2

TL;DR: 提出了一种结合旋转感知目标检测模型和视觉语言解析器的混合框架，用于从2D工程图纸中高效提取关键信息。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 手动提取2D工程图纸信息效率低且易出错，通用OCR模型难以处理复杂布局和工程符号，需更高效的自动化解决方案。

Method: 结合YOLOv11-OBB检测旋转文本并提取区域，再用轻量级视觉语言模型（Donut和Florence-2）解析为结构化输出。

Result: Donut模型表现优于Florence-2，精确率达88.5%，召回率99.2%，F1分数93.5%，幻觉率11.5%。

Conclusion: 该框架能有效支持下游制造任务，如工艺和工具选择，为2D图纸解析提供实用解决方案。

Abstract: Efficient and accurate extraction of key information from 2D engineering
drawings is essential for advancing digital manufacturing workflows. Such
information includes geometric dimensioning and tolerancing (GD&T), measures,
material specifications, and textual annotations. Manual extraction is slow and
labor-intensive, while generic OCR models often fail due to complex layouts,
engineering symbols, and rotated text, leading to incomplete and unreliable
outputs. These limitations result in incomplete and unreliable outputs. To
address these challenges, we propose a hybrid vision-language framework that
integrates a rotation-aware object detection model (YOLOv11-obb) with a
transformer-based vision-language parser. Our structured pipeline applies
YOLOv11-OBB to localize annotations and extract oriented bounding box (OBB)
patches, which are then parsed into structured outputs using a fine-tuned,
lightweight vision-language model (VLM). We curate a dataset of 1,367 2D
mechanical drawings annotated across nine key categories. YOLOv11-OBB is
trained on this dataset to detect OBBs and extract annotation patches. These
are parsed using two open-source VLMs: Donut and Florence-2. Both models are
lightweight and well-suited for specialized industrial tasks under limited
computational overhead. Following fine-tuning of both models on the curated
dataset of image patches paired with structured annotation labels, a
comparative experiment is conducted to evaluate parsing performance across four
key metrics. Donut outperforms Florence-2, achieving 88.5% precision, 99.2%
recall, and a 93.5% F1-score, with a hallucination rate of 11.5%. Finally, a
case study demonstrates how the extracted structured information supports
downstream manufacturing tasks such as process and tool selection, showcasing
the practical utility of the proposed framework in modernizing 2D drawing
interpretation.

</details>


### [404] [Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2506.17425)
*Minmin Yang,Huantao Ren,Senem Velipasalar*

Key words: CBCT, Transformer, TransUNet, 稀疏视图重建, 点变换器

TL;DR: 本文提出了一种结合CNN和Transformer的混合模型TransUNet，用于解决稀疏视图CBCT重建中的强伪影和低空间覆盖问题，并通过改进方法进一步提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 稀疏视图CBCT扫描虽然能降低辐射剂量和加快扫描速度，但会导致严重的欠采样、强伪影和低空间覆盖。本文旨在解决这些问题。

Method: 1. 使用TransUNet替换传统UNet/ResNet编码器，结合CNN局部细节捕捉和Transformer全局上下文增强；2. 引入邻居感知的Point Transformer，通过3D位置编码和k近邻注意力提升空间一致性。

Result: Trans-CBCT在LUNA16数据集上比基线方法提升了1.17 dB PSNR和0.0163 SSIM；改进后的Trans$^2$-CBCT进一步提升了0.63 dB PSNR和0.0117 SSIM。

Conclusion: 结合CNN-Transformer特征与基于点的几何推理，可有效提升稀疏视图CBCT重建的质量。

Abstract: Cone-beam computed tomography (CBCT) using only a few X-ray projection views
enables faster scans with lower radiation dose, but the resulting severe
under-sampling causes strong artifacts and poor spatial coverage. We address
these challenges in a unified framework. First, we replace conventional
UNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model.
Convolutional layers capture local details, while self-attention layers enhance
global context. We adapt TransUNet to CBCT by combining multi-scale features,
querying view-specific features per 3D point, and adding a lightweight
attenuation-prediction head. This yields Trans-CBCT, which surpasses prior
baselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views.
Second, we introduce a neighbor-aware Point Transformer to enforce volumetric
coherence. This module uses 3D positional encoding and attention over k-nearest
neighbors to improve spatial consistency. The resulting model, Trans$^2$-CBCT,
provides an additional gain of 0.63 dB PSNR and 0.0117 SSIM. Experiments on
LUNA16 and ToothFairy show consistent gains from six to ten views, validating
the effectiveness of combining CNN-Transformer features with point-based
geometry reasoning for sparse-view CBCT reconstruction.

</details>


### [405] [OmniGen2: Exploration to Advanced Multimodal Generation](https://arxiv.org/abs/2506.18871)
*Chenyuan Wu,Pengfei Zheng,Ruiran Yan,Shitao Xiao,Xin Luo,Yueze Wang,Wanli Li,Xiyan Jiang,Yexin Liu,Junjie Zhou,Ze Liu,Ziyi Xia,Chaofan Li,Haoge Deng,Jiahao Wang,Kun Luo,Bo Zhang,Defu Lian,Xinlong Wang,Zhongyuan Wang,Tiejun Huang,Zheng Liu*

Key words: OmniGen2, 生成模型, 文本到图像, 图像编辑, 上下文生成, 开源

TL;DR: OmniGen2是一款多功能开源生成模型，支持文本到图像、图像编辑和上下文生成等任务，通过双解码路径和独立图像标记器设计，提升了性能并保持了文本生成能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 为解决多任务生成需求的统一性，作者提出了OmniGen2，旨在通过改进设计和数据管道提升生成模型的通用性和性能。

Method: 采用双解码路径（文本和图像分离）和非共享参数设计，结合独立图像标记器和反射机制，以及专门的数据构造流程。

Result: OmniGen2在多任务基准测试中表现优异，尤其在文本到图像和图像编辑任务中，并在OmniContext基准中达到开源模型的最先进水平。

Conclusion: OmniGen2证明了其在多任务生成中的高效性和灵活性，为未来研究提供了模型、代码和数据集支持。

Abstract: In this work, we introduce OmniGen2, a versatile and open-source generative
model designed to provide a unified solution for diverse generation tasks,
including text-to-image, image editing, and in-context generation. Unlike
OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image
modalities, utilizing unshared parameters and a decoupled image tokenizer. This
design enables OmniGen2 to build upon existing multimodal understanding models
without the need to re-adapt VAE inputs, thereby preserving the original text
generation capabilities. To facilitate the training of OmniGen2, we developed
comprehensive data construction pipelines, encompassing image editing and
in-context generation data. Additionally, we introduce a reflection mechanism
tailored for image generation tasks and curate a dedicated reflection dataset
based on OmniGen2. Despite its relatively modest parameter size, OmniGen2
achieves competitive results on multiple task benchmarks, including
text-to-image and image editing. To further evaluate in-context generation,
also referred to as subject-driven tasks, we introduce a new benchmark named
OmniContext. OmniGen2 achieves state-of-the-art performance among open-source
models in terms of consistency. We will release our models, training code,
datasets, and data construction pipeline to support future research in this
field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:
https://github.com/VectorSpaceLab/OmniGen2

</details>


### [406] [Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations](https://arxiv.org/abs/2506.18898)
*Jiaming Han,Hao Chen,Yang Zhao,Hanyu Wang,Qi Zhao,Ziyan Yang,Hao He,Xiangyu Yue,Lu Jiang*

Key words: 多模态框架, 视觉生成, 文本对齐令牌器, 自适配编解码, 多模态LLM

TL;DR: 提出了一种多模态框架Tar，通过文本对齐令牌器（TA-Tok）将图像转换为离散令牌，统一视觉理解和生成。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 旨在通过共享离散语义表示，统一视觉与文本的多模态输入和输出，避免特定模态设计。

Method: 采用文本对齐令牌器（TA-Tok）将图像映射到扩展词汇空间，并提出尺度自适应编码、解码及两种互补的解码器。

Result: 在多个基准测试中表现优异，收敛更快且训练效率更高。

Conclusion: Tar框架在多模态任务中表现出色，实现了视觉和文本的统一处理。

Abstract: This paper presents a multimodal framework that attempts to unify visual
understanding and generation within a shared discrete semantic representation.
At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into
discrete tokens using a text-aligned codebook projected from a large language
model's (LLM) vocabulary. By integrating vision and text into a unified space
with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input
and output through a shared interface, without the need for modality-specific
designs. Additionally, we propose scale-adaptive encoding and decoding to
balance efficiency and visual detail, along with a generative de-tokenizer to
produce high-fidelity visual outputs. To address diverse decoding needs, we
utilize two complementary de-tokenizers: a fast autoregressive model and a
diffusion-based model. To enhance modality fusion, we investigate advanced
pre-training tasks, demonstrating improvements in both visual understanding and
generation. Experiments across benchmarks show that Tar matches or surpasses
existing multimodal LLM methods, achieving faster convergence and greater
training efficiency. Code, models, and data are available at
https://tar.csuhan.com

</details>


### [407] [SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference](https://arxiv.org/abs/2506.17558)
*Jake Levi,Mark van der Wilk*

Key words: 计算机视觉，囊膜网络，部分-整体层次结构，自注意力机制，合成数据集

TL;DR: 该论文提出了一个合成数据集SynDaCaTE，用于评测囊膜网络是否真正学习到部分-整体层次结构，并通过实验展示了现有模型的局限性以及自注意力机制的有效性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 为了评估囊膜网络是否真正学习到部分-整体层次结构，研究者提出了一个合成数据集SynDaCaTE，用以解决当前评测困难的挑战。

Method: 研究者设计了SynDaCaTE数据集，并通过实验测试了现有囊膜模型的表现，同时验证了自注意力机制在部分-整体推理中的有效性。

Result: 实验表明现有囊膜模型存在性能瓶颈，而自注意力机制在部分-整体推理中表现出色，为未来计算机视觉模型的归纳偏置设计提供了新的方向。

Conclusion: SynDaCaTE数据集为评测囊膜网络的性能提供了有效工具，展示了自注意力机制在部分-整体推理中的潜力。

Abstract: Learning to infer object representations, and in particular part-whole
hierarchies, has been the focus of extensive research in computer vision, in
pursuit of improving data efficiency, systematic generalisation, and
robustness. Models which are \emph{designed} to infer part-whole hierarchies,
often referred to as capsule networks, are typically trained end-to-end on
supervised tasks such as object classification, in which case it is difficult
to evaluate whether such a model \emph{actually} learns to infer part-whole
hierarchies, as claimed. To address this difficulty, we present a SYNthetic
DAtaset for CApsule Testing and Evaluation, abbreviated as SynDaCaTE, and
establish its utility by (1) demonstrating the precise bottleneck in a
prominent existing capsule model, and (2) demonstrating that
permutation-equivariant self-attention is highly effective for parts-to-wholes
inference, which motivates future directions for designing effective inductive
biases for computer vision.

</details>


### [408] [VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models](https://arxiv.org/abs/2506.17561)
*Chongkai Gao,Zixuan Liu,Zhenghao Chi,Junshan Huang,Xin Fei,Yiwen Hou,Yuxuan Zhang,Yudi Lin,Zhirui Fang,Zeyu Jiang,Lin Shao*

Key words: VLA模型，任务规划，视觉基础表示，分层范式

TL;DR: 研究了视觉-语言-动作（VLA）模型中不同任务规划范式和表示的独立影响，提出了统一的VLA-OS架构系列，并通过实验表明了视觉基础规划表示优于语言规划表示，且分层VLA范式在多方面表现更优。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有VLA模型在任务规划和动作生成上的方法差异较大，难以明确性能提升的来源和改进方向。

Method: 提出VLA-OS统一架构系列，设计跨物体类别、视觉模态、环境和工作器的控制实验。

Result: 视觉基础规划表示优于语言表示，分层VLA范式在任务性能、预训练等方面表现更优，但训练和推理速度较慢。

Conclusion: 分层VLA范式在综合性能上更具优势，但需权衡速度。

Abstract: Recent studies on Vision-Language-Action (VLA) models have shifted from the
end-to-end action-generation paradigm toward a pipeline involving task planning
followed by action generation, demonstrating improved performance on various
complex, long-horizon manipulation tasks. However, existing approaches vary
significantly in terms of network architectures, planning paradigms,
representations, and training data sources, making it challenging for
researchers to identify the precise sources of performance gains and components
to be further improved. To systematically investigate the impacts of different
planning paradigms and representations isolating from network architectures and
training data, in this paper, we introduce VLA-OS, a unified VLA architecture
series capable of various task planning paradigms, and design a comprehensive
suite of controlled experiments across diverse object categories (rigid and
deformable), visual modalities (2D and 3D), environments (simulation and
real-world), and end-effectors (grippers and dexterous hands). Our results
demonstrate that: 1) visually grounded planning representations are generally
better than language planning representations; 2) the Hierarchical-VLA paradigm
generally achieves superior or comparable performance than other paradigms on
task performance, pretraining, generalization ability, scalability, and
continual learning ability, albeit at the cost of slower training and inference
speeds.

</details>


### [409] [HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2506.17587)
*Le Yu,Kaishen Wang,Jianlong Xiong,Yue Cao,Tao He*

Key words: LVLMs, 幻觉, HalluRNN, DG-DPU, 跨层推理

TL;DR: HalluRNN提出了一种架构级解决方案，通过跨层循环推理增强模型稳定性，减少幻觉生成。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 大型视觉语言模型（LVLMs）容易产生视觉上无依据的幻觉输出，现有方法通常需要大量资源或特定任务配置。

Method: 提出HalluRNN架构，引入Dual-Gated Depth Propagation Unit (DG-DPU)模块，通过循环细化隐藏状态自适应传播信息。

Result: 仅微调DG-DPU模块，HalluRNN在多个基准测试中表现出色且稳健。

Conclusion: HalluRNN有效解决了LVLMs中的幻觉问题，且具有资源高效性和通用性。

Abstract: Though Large Vision-Language Models (LVLMs) have achieved remarkable
performance across various tasks, they are still prone to
hallucinations-generating outputs that are textually plausible but visually
ungrounded. While prior approaches generally address this issue through
data-centric fine-tuning or innovative decoding strategies, these methods often
require substantial resources or task-specific configurations. In this work, we
introduce an architecture-level solution, HalluRNN, which enhances model
stability through recurrent cross-layer reasoning. Specifically, we propose a
novel Dual-Gated Depth Propagation Unit (DG-DPU) module, which is shared across
layers and recurrently refines hidden states. This allows for the adaptive
propagation of information throughout the model, enforces consistency across
layers, and mitigates hallucinations caused by representational drift. By
fine-tuning only the DG-DPU module, HalluRNN achieves strong and robust
performance across multiple benchmarks.

</details>


### [410] [DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving](https://arxiv.org/abs/2506.17590)
*Mihir Godbole,Xiangbo Gao,Zhengzhong Tu*

Key words: 自动驾驶、意图预测、VRU、DRAMA-X、SGG-Intent

TL;DR: 该论文提出了DRAMA-X基准，用于评估自动驾驶中VRU（易受伤害的道路使用者）的多类意图预测，并提出了SGG-Intent框架作为参考基线。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究VRU的短期运动意图对自动驾驶的安全性至关重要，但现有方法在细粒度意图推理方面缺乏探索，且缺乏相应的评测基准。

Method: 通过自动标注流程构建DRAMA-X基准，提出SGG-Intent框架，利用VLM支持的检测器和LLM进行场景图推理。

Result: 实验表明，基于场景图的推理能显著提升意图预测和风险评估性能，尤其是显式建模上下文线索时。

Conclusion: DRAMA-X为自动驾驶决策提供了结构化评测工具，SGG-Intent展示了场景图推理的有效性。

Abstract: Understanding the short-term motion of vulnerable road users (VRUs) like
pedestrians and cyclists is critical for safe autonomous driving, especially in
urban scenarios with ambiguous or high-risk behaviors. While vision-language
models (VLMs) have enabled open-vocabulary perception, their utility for
fine-grained intent reasoning remains underexplored. Notably, no existing
benchmark evaluates multi-class intent prediction in safety-critical
situations, To address this gap, we introduce DRAMA-X, a fine-grained benchmark
constructed from the DRAMA dataset via an automated annotation pipeline.
DRAMA-X contains 5,686 accident-prone frames labeled with object bounding
boxes, a nine-class directional intent taxonomy, binary risk scores,
expert-generated action suggestions for the ego vehicle, and descriptive motion
summaries. These annotations enable a structured evaluation of four
interrelated tasks central to autonomous decision-making: object detection,
intent prediction, risk assessment, and action suggestion. As a reference
baseline, we propose SGG-Intent, a lightweight, training-free framework that
mirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene
graph from visual input using VLM-backed detectors, infers intent, assesses
risk, and recommends an action using a compositional reasoning stage powered by
a large language model. We evaluate a range of recent VLMs, comparing
performance across all four DRAMA-X tasks. Our experiments demonstrate that
scene-graph-based reasoning enhances intent prediction and risk assessment,
especially when contextual cues are explicitly modeled.

</details>


### [411] [Adaptive Multi-prompt Contrastive Network for Few-shot Out-of-distribution Detection](https://arxiv.org/abs/2506.17633)
*Xiang Fang,Arvind Easwaran,Blaise Genest*

Key words: OOD检测, 小样本学习, CLIP, 自适应提示

TL;DR: 该论文提出了一个新型网络AMCN，解决小样本OOD检测问题，通过自适应多提示对比网络学习类间和类内分布，并利用CLIP结合文本和图像，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决小样本OOD检测中因ID样本稀缺和OOD样本缺失带来的挑战，传统方法忽略类间多样性。

Method: 提出AMCN网络，结合自适应提示和类间阈值，设计了提示指导的ID-OOD分离模块。

Result: 实验证明AMCN优于现有最先进方法。

Conclusion: AMCN通过自适应提示和类间分布学习，有效提升小样本OOD检测性能。

Abstract: Out-of-distribution (OOD) detection attempts to distinguish outlier samples
to prevent models trained on the in-distribution (ID) dataset from producing
unavailable outputs. Most OOD detection methods require many IID samples for
training, which seriously limits their real-world applications. To this end, we
target a challenging setting: few-shot OOD detection, where {Only a few {\em
labeled ID} samples are available.} Therefore, few-shot OOD detection is much
more challenging than the traditional OOD detection setting. Previous few-shot
OOD detection works ignore the distinct diversity between different classes. In
this paper, we propose a novel network: Adaptive Multi-prompt Contrastive
Network (AMCN), which adapts the ID-OOD separation boundary by learning inter-
and intra-class distribution. To compensate for the absence of OOD and scarcity
of ID {\em image samples}, we leverage CLIP, connecting text with images,
engineering learnable ID and OOD {\em textual prompts}. Specifically, we first
generate adaptive prompts (learnable ID prompts, label-fixed OOD prompts and
label-adaptive OOD prompts). Then, we generate an adaptive class boundary for
each class by introducing a class-wise threshold. Finally, we propose a
prompt-guided ID-OOD separation module to control the margin between ID and OOD
prompts. Experimental results show that AMCN outperforms other state-of-the-art
works.

</details>


### [412] [Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models](https://arxiv.org/abs/2506.17707)
*Jihyun Kim,Junho Park,Kyeongbo Kong,Suk-Ju Kang*

Key words: 3D房间网格, 自然语言处理, 视觉编程, 扩散模型, 全景图像生成

TL;DR: Programmable-Room是一个通过自然语言指令交互生成和编辑3D房间网格的框架，利用视觉编程和大语言模型分解复杂任务，并通过扩散模型优化全景图像生成。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 旨在通过自然语言指令精确控制3D房间的各个属性，简化3D房间的生成和编辑过程。

Method: 将任务分解为生成3D坐标、全景图像纹理、整合3D网格和家具布置；利用视觉编程和大语言模型编写模块化程序；使用预训练的扩散模型优化全景图像生成。

Result: 框架在生成和编辑3D房间网格方面展现出灵活性和优越性，定量和定性上均优于现有模型。

Conclusion: Programmable-Room为3D房间的生成和编辑提供了一种高效且可控的方法，结合了自然语言处理和视觉编程的优势。

Abstract: We present Programmable-Room, a framework which interactively generates and
edits a 3D room mesh, given natural language instructions. For precise control
of a room's each attribute, we decompose the challenging task into simpler
steps such as creating plausible 3D coordinates for room meshes, generating
panorama images for the texture, constructing 3D meshes by integrating the
coordinates and panorama texture images, and arranging furniture. To support
the various decomposed tasks with a unified framework, we incorporate visual
programming (VP). VP is a method that utilizes a large language model (LLM) to
write a Python-like program which is an ordered list of necessary modules for
the various tasks given in natural language. We develop most of the modules.
Especially, for the texture generating module, we utilize a pretrained
large-scale diffusion model to generate panorama images conditioned on text and
visual prompts (i.e., layout, depth, and semantic map) simultaneously.
Specifically, we enhance the panorama image generation quality by optimizing
the training objective with a 1D representation of a panorama scene obtained
from bidirectional LSTM. We demonstrate Programmable-Room's flexibility in
generating and editing 3D room meshes, and prove our framework's superiority to
an existing model quantitatively and qualitatively. Project page is available
in https://jihyun0510.github.io/Programmable_Room_Page/.

</details>


### [413] [Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning](https://arxiv.org/abs/2506.17302)
*Yijun Lin,Theresa Chen,Colby Brungard,Grunwald Sabine,Sue Ives,Matt Macander,Timm Nawrocki,Yao-Yi Chiang,Nic Jelinski*

Key words: 土壤地图,多年冻土,机器学习,对比学习,阿拉斯加

TL;DR: 本文提出了一种名为MISO的机器学习模型，用于生成阿拉斯加的高分辨率土壤地图，特别是针对近地表多年冻土和土壤分类。该模型通过视觉特征提取、连续空间预测和多模态对齐技术，显著提升了传统方法（如随机森林）在未见过区域的泛化能力和召回率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 阿拉斯加的土壤地图绘制传统上依赖野外工作和局部模拟，但随着气候变暖导致多年冻土加速融化，亟需高分辨率地图以识别脆弱区域并指导适应策略。

Method: MISO结合了地理空间基础模型进行视觉特征提取、隐式神经表示用于连续空间预测，以及对比学习实现多模态对齐和地理定位感知。

Result: 与传统的随机森林模型相比，MISO在未见过区域的泛化能力和召回率表现更优，尤其在多年冻土带和主要土地资源区域的跨区域分析中。

Conclusion: MISO展示了先进机器学习方法在精细尺度土壤绘图中的潜力，为未来多年冻土区的土壤采样和基础设施规划提供了实用指导。

Abstract: Fine-scale soil mapping in Alaska, traditionally relying on fieldwork and
localized simulations, remains a critical yet underdeveloped task, despite the
region's ecological importance and extensive permafrost coverage. As permafrost
thaw accelerates due to climate change, it threatens infrastructure stability
and key ecosystem services, such as soil carbon storage. High-resolution soil
maps are essential for characterizing permafrost distribution, identifying
vulnerable areas, and informing adaptation strategies. We present MISO, a
vision-based machine learning (ML) model to produce statewide fine-scale soil
maps for near-surface permafrost and soil taxonomy. The model integrates a
geospatial foundation model for visual feature extraction, implicit neural
representations for continuous spatial prediction, and contrastive learning for
multimodal alignment and geo-location awareness. We compare MISO with Random
Forest (RF), a traditional ML model that has been widely used in soil mapping
applications. Spatial cross-validation and regional analysis across Permafrost
Zones and Major Land Resource Areas (MLRAs) show that MISO generalizes better
to remote, unseen locations and achieves higher recall than RF, which is
critical for monitoring permafrost thaw and related environmental processes.
These findings demonstrate the potential of advanced ML approaches for
fine-scale soil mapping and provide practical guidance for future soil sampling
and infrastructure planning in permafrost-affected landscapes. The project will
be released at https://github.com/knowledge-computing/Peatland-permafrost.

</details>


### [414] [Efficient Feedback Gate Network for Hyperspectral Image Super-Resolution](https://arxiv.org/abs/2506.17361)
*Xufei Wang,Mingjian Zhang,Fei Ge,Jinchen Zhu,Wen Sha,Jifen Ren,Zhimeng Hou,Shouguo Zheng,ling Zheng,Shizhuang Weng*

Key words: 高光谱图像超分辨率; 反馈门网络; 空间-光谱融合; SPDFM; SSRGM

TL;DR: 提出了基于组的单高光谱图像超分辨率方法，通过反馈门网络和多种操作提升空间和光谱信息融合。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有单高光谱图像超分辨率方法由于未能充分探索波段一致性和空间-光谱信息，导致性能受限。

Method: 设计了高效反馈门网络，使用大核卷积和光谱交互。引入分组引导、通道混洗和扩张卷积的模块（SPDFM），以及空间-光谱强化门模块（SSRGM）。

Result: 在三个高光谱数据集上实验表明，该方法在光谱保真和空间内容重建上优于现有技术。

Conclusion: 该方法通过强化空间-光谱特征提升了高光谱图像超分辨率性能。

Abstract: Even without auxiliary images, single hyperspectral image super-resolution
(SHSR) methods can be designed to improve the spatial resolution of
hyperspectral images. However, failing to explore coherence thoroughly along
bands and spatial-spectral information leads to the limited performance of the
SHSR. In this study, we propose a novel group-based SHSR method termed the
efficient feedback gate network, which uses various feedbacks and gate
operations involving large kernel convolutions and spectral interactions. In
particular, by providing different guidance for neighboring groups, we can
learn rich band information and hierarchical hyperspectral spatial information
using channel shuffling and dilatation convolution in shuffled and progressive
dilated fusion module(SPDFM). Moreover, we develop a wide-bound perception gate
block and a spectrum enhancement gate block to construct the spatial-spectral
reinforcement gate module (SSRGM) and obtain highly representative
spatial-spectral features efficiently. Additionally, we apply a
three-dimensional SSRGM to enhance holistic information and coherence for
hyperspectral data. The experimental results on three hyperspectral datasets
demonstrate the superior performance of the proposed network over the
state-of-the-art methods in terms of spectral fidelity and spatial content
reconstruction.

</details>


### [415] [SurgVidLM: Towards Multi-grained Surgical Video Understanding with Large Language Model](https://arxiv.org/abs/2506.17873)
*Guankun Wang,Wenjin Mo,Junyi Wang,Long Bai,Kun Yuan,Ming Hu,Jinlin Wu,Junjun He,Yiming Huang,Nicolas Padoy,Zhen Lei,Hongbin Liu,Nassir Navab,Hongliang Ren*

Key words: 多模态大语言模型, 视频语言模型, 细粒度视频理解, 外科手术, StageFocus机制

TL;DR: 介绍了一种针对外科手术视频的专用视频语言模型SurgVidLM，填补了现有Vid-LLMs在细粒度手术视频理解任务中的空白。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有视频大语言模型（Vid-LLMs）在医学领域的应用主要集中在图像方法，缺乏针对细粒度手术视频理解的专用模型。

Method: 提出SurgVidLM模型，使用SVU-31K数据集（31K视频-指令对）进行训练，引入StageFocus机制和Multi-frequency Fusion Attention技术。

Result: SurgVidLM在全面和细粒度视频理解任务中显著优于现有Vid-LLMs。

Conclusion: SurgVidLM在捕捉复杂手术流程背景方面表现出优越能力，适用于医学领域细粒度视频分析。

Abstract: Recent advances in Multimodal Large Language Models have demonstrated great
potential in the medical domain, facilitating users to understand surgical
scenes and procedures. Beyond image-based methods, the exploration of Video
Large Language Models (Vid-LLMs) has emerged as a promising avenue for
capturing the complex sequences of information involved in surgery. However,
there is still a lack of Vid-LLMs specialized for fine-grained surgical video
understanding tasks, which is crucial for analyzing specific processes or
details within a surgical procedure. To bridge this gap, we propose SurgVidLM,
the first video language model designed to address both full and fine-grained
surgical video comprehension. To train our SurgVidLM, we construct the SVU-31K
dataset which consists of over 31K video-instruction pairs, enabling both
holistic understanding and detailed analysis of surgical procedures.
Furthermore, we introduce the StageFocus mechanism which is a two-stage
framework performing the multi-grained, progressive understanding of surgical
videos. We also develop the Multi-frequency Fusion Attention to effectively
integrate low and high-frequency visual tokens, ensuring the retention of
critical information. Experimental results demonstrate that SurgVidLM
significantly outperforms state-of-the-art Vid-LLMs in both full and
fine-grained video understanding tasks, showcasing its superior capability in
capturing complex procedural contexts.

</details>


### [416] [StainPIDR: A Pathological Image Decouplingand Reconstruction Method for StainNormalization Based on Color VectorQuantization and Structure Restaining](https://arxiv.org/abs/2506.17879)
*Zheng Chen*

Key words: 病理图像, 染色标准化, 颜色差异, 向量量化, 模板选择

TL;DR: 提出了一种名为StainPIDR的染色标准化方法，通过解耦病理图像的结构和颜色特征，并重新染色，实现了颜色差异的消除。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 病理图像的颜色差异影响计算机辅助诊断系统的性能，需要一种标准化方法解决这一问题。

Method: 采用结构特征与向量量化颜色特征解耦，通过固定颜色向量码本和交叉注意力机制重新染色，并设计模板图像选择算法优化性能。

Result: 实验验证了StainPIDR和模板选择算法的有效性，能够很好地完成染色标准化任务。

Conclusion: StainPIDR是一种有效的病理图像染色标准化方法，代码将公开。

Abstract: The color appearance of a pathological image is highly related to the imaging
protocols, the proportion of different dyes, and the scanning devices.
Computer-aided diagnostic systems may deteriorate when facing these
color-variant pathological images. In this work, we propose a stain
normalization method called StainPIDR. We try to eliminate this color
discrepancy by decoupling the image into structure features and
vector-quantized color features, restaining the structure features with the
target color features, and decoding the stained structure features to
normalized pathological images. We assume that color features decoupled by
different images with the same color should be exactly the same. Under this
assumption, we train a fixed color vector codebook to which the decoupled color
features will map. In the restaining part, we utilize the cross-attention
mechanism to efficiently stain the structure features. As the target color
(decoupled from a selected template image) will also affect the performance of
stain normalization, we further design a template image selection algorithm to
select a template from a given dataset. In our extensive experiments, we
validate the effectiveness of StainPIDR and the template image selection
algorithm. All the results show that our method can perform well in the stain
normalization task. The code of StainPIDR will be publicly available later.

</details>


### [417] [EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations](https://arxiv.org/abs/2506.17896)
*Junho Park,Andrew Sangwoo Ye,Taein Kwon*

Key words: 自我中心视觉,外来中心视角转换,点云重建,扩散修复,增强现实

TL;DR: EgoWorld是一个新颖的两阶段框架，旨在从外来中心视角重建自我中心视角，利用投影点云、3D手部姿势和文本描述，并通过扩散修复生成密集且语义一致的自中心图像。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 自我中心视觉对于人类和机器视觉理解至关重要，尤其是在捕捉手部与物体交互的细节时。现有方法受限于2D线索和多视角同步设置的不现实假设，EgoWorld旨在克服这些限制。

Method: 框架分为两阶段：首先从外来深度图重建点云并重投影到自中心视角，然后使用基于扩散的修复技术生成密集图像。

Result: 在H2O和TACO数据集上，EgoWorld实现了最先进的性能，并在新物体、动作、场景和主体上表现出强大的泛化能力。

Conclusion: EgoWorld不仅优于现有方法，还能在未标记的现实世界示例中表现良好。

Abstract: Egocentric vision is essential for both human and machine visual
understanding, particularly in capturing the detailed hand-object interactions
needed for manipulation tasks. Translating third-person views into first-person
views significantly benefits augmented reality (AR), virtual reality (VR) and
robotics applications. However, current exocentric-to-egocentric translation
methods are limited by their dependence on 2D cues, synchronized multi-view
settings, and unrealistic assumptions such as necessity of initial egocentric
frame and relative camera poses during inference. To overcome these challenges,
we introduce EgoWorld, a novel two-stage framework that reconstructs an
egocentric view from rich exocentric observations, including projected point
clouds, 3D hand poses, and textual descriptions. Our approach reconstructs a
point cloud from estimated exocentric depth maps, reprojects it into the
egocentric perspective, and then applies diffusion-based inpainting to produce
dense, semantically coherent egocentric images. Evaluated on the H2O and TACO
datasets, EgoWorld achieves state-of-the-art performance and demonstrates
robust generalization to new objects, actions, scenes, and subjects. Moreover,
EgoWorld shows promising results even on unlabeled real-world examples.

</details>


### [418] [Cause-Effect Driven Optimization for Robust Medical Visual Question Answering with Language Biases](https://arxiv.org/abs/2506.17903)
*Huanjia Zhu,Yishu Liu,Xiaozhao Fang,Guangming Lu,Bingzhi Chen*

Key words: Med-VQA, 语言偏差, CEDO, MHO, GMS, DLR

TL;DR: CEDO框架通过MHO、GMS和DLR三种机制从因果和效果角度全面减轻Med-VQA模型的语言偏差。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决Med-VQA模型中语言偏差问题。

Method: 提出CEDO框架，包含MHO、GMS和DLR三种机制。

Result: 在多个基准测试中表现优于现有方法。

Conclusion: CEDO能有效减轻语言偏差，提升模型鲁棒性。

Abstract: Existing Medical Visual Question Answering (Med-VQA) models often suffer from
language biases, where spurious correlations between question types and answer
categories are inadvertently established. To address these issues, we propose a
novel Cause-Effect Driven Optimization framework called CEDO, that incorporates
three well-established mechanisms, i.e., Modality-driven Heterogeneous
Optimization (MHO), Gradient-guided Modality Synergy (GMS), and
Distribution-adapted Loss Rescaling (DLR), for comprehensively mitigating
language biases from both causal and effectual perspectives. Specifically, MHO
employs adaptive learning rates for specific modalities to achieve
heterogeneous optimization, thus enhancing robust reasoning capabilities.
Additionally, GMS leverages the Pareto optimization method to foster
synergistic interactions between modalities and enforce gradient orthogonality
to eliminate bias updates, thereby mitigating language biases from the effect
side, i.e., shortcut bias. Furthermore, DLR is designed to assign adaptive
weights to individual losses to ensure balanced learning across all answer
categories, effectively alleviating language biases from the cause side, i.e.,
imbalance biases within datasets. Extensive experiments on multiple traditional
and bias-sensitive benchmarks consistently demonstrate the robustness of CEDO
over state-of-the-art competitors.

</details>


### [419] [Feedback Driven Multi Stereo Vision System for Real-Time Event Analysis](https://arxiv.org/abs/2506.17910)
*Mohamed Benkedadra,Matei Mancas,Sidi Ahmed Mahmoudi*

Key words: 3D立体视觉,场景理解,交互系统,多相机融合

TL;DR: 该论文提出了一种基于3D立体视觉的交互系统管道，用于处理复杂环境中的场景理解和任务执行。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有2D和3D相机在大型复杂环境中不可靠，需改进。

Method: 提出融合多3D相机的管道，进行场景重建和任务执行。

Result: 初步实验表明系统能处理事件识别、目标跟踪等任务。

Conclusion: 未来需进一步开发以实现生产应用。

Abstract: 2D cameras are often used in interactive systems. Other systems like gaming
consoles provide more powerful 3D cameras for short range depth sensing.
Overall, these cameras are not reliable in large, complex environments. In this
work, we propose a 3D stereo vision based pipeline for interactive systems,
that is able to handle both ordinary and sensitive applications, through robust
scene understanding. We explore the fusion of multiple 3D cameras to do full
scene reconstruction, which allows for preforming a wide range of tasks, like
event recognition, subject tracking, and notification. Using possible feedback
approaches, the system can receive data from the subjects present in the
environment, to learn to make better decisions, or to adapt to completely new
environments. Throughout the paper, we introduce the pipeline and explain our
preliminary experimentation and results. Finally, we draw the roadmap for the
next steps that need to be taken, in order to get this pipeline into production

</details>


### [420] [IDAL: Improved Domain Adaptive Learning for Natural Images Dataset](https://arxiv.org/abs/2506.17931)
*Ravi Kant Gupta,Shounak Das,Amit Sethi*

Key words: 无监督域自适应,ResNet,特征金字塔网络,多模态分布,损失函数

TL;DR: 本文提出了一种新颖的无监督域自适应（UDA）方法，通过结合ResNet和特征金字塔网络（FPN）的架构及改进的损失函数，有效解决了自然图像中的多模态分布对齐问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有对抗域自适应方法在多模态分布对齐上效果不佳，本文旨在通过改进的架构和损失函数组合提升域对齐能力。

Method: 结合ResNet和FPN的深度架构，并设计新颖的损失函数组合以应对自然图像中的尺度、噪声和风格变化等问题。

Result: 方法在Office-Home、Office-31和VisDA-2017数据集上优于现有CNN方法，在DomainNet数据集上表现相当。

Conclusion: 提出的UDA方案在多模态分布对齐和模型泛化性上优于现有方法，并加速训练收敛。

Abstract: We present a novel approach for unsupervised domain adaptation (UDA) for
natural images. A commonly-used objective for UDA schemes is to enhance domain
alignment in representation space even if there is a domain shift in the input
space. Existing adversarial domain adaptation methods may not effectively align
different domains of multimodal distributions associated with classification
problems. Our approach has two main features. Firstly, its neural architecture
uses the deep structure of ResNet and the effective separation of scales of
feature pyramidal network (FPN) to work with both content and style features.
Secondly, it uses a combination of a novel loss function and judiciously
selected existing loss functions to train the network architecture. This
tailored combination is designed to address challenges inherent to natural
images, such as scale, noise, and style shifts, that occur on top of a
multi-modal (multi-class) distribution. The combined loss function not only
enhances model accuracy and robustness on the target domain but also speeds up
training convergence. Our proposed UDA scheme generalizes better than
state-of-the-art for CNN-based methods on Office-Home, Office-31, and
VisDA-2017 datasets and comaparable for DomainNet dataset.

</details>


### [421] [GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via Reinforcement Learning](https://arxiv.org/abs/2506.17939)
*Bo Liu,Xiangyu Zhao,Along He,Yidi Chen,Huazhu Fu,Xiao-Ming Wu*

Key words: 医学视觉问答,可解释性,多模态学习,强化学习

TL;DR: 该论文提出了一种新的医学视觉问答数据集ThinkVG，通过分解答案生成过程为中间推理步骤，增强模型的可解释性。同时引入可验证的奖励机制提升性能，仅用少量数据即可达到可比效果。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前医学视觉问答模型的答案可靠性和可解释性不足，影响临床应用信任度。希望通过改进数据和方法提升模型表现。

Method: 提出ThinkVG数据集，分解答案生成步骤以显式标注相关视觉区域；设计可验证奖励机制用于强化学习后期训练。

Result: 仅用八分之一训练数据即达到可比性能，证明了方法的效率和有效性。

Conclusion: 结合细粒度可解释性和高效训练机制，显著提升了医学视觉问答的可靠性和实用性。

Abstract: Medical visual question answering aims to support clinical decision-making by
enabling models to answer natural language questions based on medical images.
While recent advances in multi-modal learning have significantly improved
performance, current methods still suffer from limited answer reliability and
poor interpretability, impairing the ability of clinicians and patients to
understand and trust model-generated answers. To address this, this work first
proposes a Thinking with Visual Grounding (ThinkVG) dataset wherein the answer
generation is decomposed into intermediate reasoning steps that explicitly
ground relevant visual regions of the medical image, thereby providing
fine-grained explainability. Furthermore, we introduce a novel verifiable
reward mechanism for reinforcement learning to guide post-training, improving
the alignment between the model's reasoning process and its final answer.
Remarkably, our method achieves comparable performance using only one-eighth of
the training data, demonstrating the efficiency and effectiveness of the
proposal. The dataset is available at
https://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG.

</details>


### [422] [Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation Booster](https://arxiv.org/abs/2506.18034)
*Fenghe Tang,Wenxin Ma,Zhiyang He,Xiaodong Tao,Zihang Jiang,S. Kevin Zhou*

Key words: LLM, 医学图像分割, CNN, 语义迁移, 冻结层

TL;DR: 本文提出了一种混合架构LLM4Seg，将预训练且冻结的LLM层集成到CNN编码器-解码器分割框架中，显著提升了医学图像分割任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探索预训练LLM的语义理解能力如何迁移到视觉任务中，尤其是医学图像分割。

Method: 提出LLM4Seg，结合预训练的冻结LLM层与CNN编码器-解码器结构。

Result: 在超声、皮肤镜、结肠镜和CT等多种模态上，分割性能提升且训练参数增加极少。

Conclusion: LLM的语义感知能力可以增强分割任务的全局理解和局部建模，且效果对不同LLM（如LLaMA和DeepSeek）均稳健。

Abstract: With the advancement of Large Language Model (LLM) for natural language
processing, this paper presents an intriguing finding: a frozen pre-trained LLM
layer can process visual tokens for medical image segmentation tasks.
Specifically, we propose a simple hybrid structure that integrates a
pre-trained, frozen LLM layer within the CNN encoder-decoder segmentation
framework (LLM4Seg). Surprisingly, this design improves segmentation
performance with a minimal increase in trainable parameters across various
modalities, including ultrasound, dermoscopy, polypscopy, and CT scans. Our
in-depth analysis reveals the potential of transferring LLM's semantic
awareness to enhance segmentation tasks, offering both improved global
understanding and better local modeling capabilities. The improvement proves
robust across different LLMs, validated using LLaMA and DeepSeek.

</details>


### [423] [MUPA: Towards Multi-Path Agentic Reasoning for Grounded Video Question Answering](https://arxiv.org/abs/2506.18071)
*Jisheng Dang,Huilin Song,Junbin Xiao,Bimei Wang,Han Peng,Haoxuan Li,Xun Yang,Meng Wang,Tat-Seng Chua*

Key words: Grounded VideoQA, multimodal models, MUPA, video grounding, state-of-the-art

TL;DR: MUPA 是一种多路径协作方法，通过融合视频定位、问题回答、答案反思和聚合技术，显著提高了 Grounded VideoQA 的任务性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决现代多模态模型在 Grounded VideoQA 任务中过度依赖语言先验和虚假关联的问题。

Method: 提出 MUPA，包含三个不同的推理路径，通过 grounding 和 QA 代理的协作，以及反思代理来聚合多路径结果。

Result: 在参数更少的情况下性能优于 7B 规模的竞争者，并在 7B 规模下达到新 SOTA（NExT-GQA 30.3%，DeVE-QA 47.4%）。

Conclusion: MUPA 显著提升了 Grounded VideoQA 的可信度，同时保持了回答准确性。

Abstract: Grounded Video Question Answering (Grounded VideoQA) requires aligning
textual answers with explicit visual evidence. However, modern multimodal
models often rely on linguistic priors and spurious correlations, resulting in
poorly grounded predictions. In this work, we propose MUPA, a cooperative
MUlti-Path Agentic approach that unifies video grounding, question answering,
answer reflection and aggregation to tackle Grounded VideoQA. MUPA features
three distinct reasoning paths on the interplay of grounding and QA agents in
different chronological orders, along with a dedicated reflection agent to
judge and aggregate the multi-path results to accomplish consistent QA and
grounding. This design markedly improves grounding fidelity without sacrificing
answer accuracy. Despite using only 2B parameters, our method outperforms all
7B-scale competitors. When scaled to 7B parameters, MUPA establishes new
state-of-the-art results, with Acc@GQA of 30.3% and 47.4% on NExT-GQA and
DeVE-QA respectively, demonstrating MUPA' effectiveness towards trustworthy
video-language understanding. Our code is available in
https://github.com/longmalongma/MUPA.

</details>


### [424] [ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation](https://arxiv.org/abs/2506.18095)
*Junying Chen,Zhenyang Cai,Pengcheng Chen,Shunian Chen,Ke Ji,Xidong Wang,Yunjin Yang,Benyou Wang*

Key words: 多模态生成模型,图像生成,开源,GPT-4o,Janus-4o

TL;DR: 介绍了ShareGPT-4o-Image数据集和Janus-4o模型，旨在开源先进的多模态图像生成技术。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前领先的多模态生成模型如GPT-4o-Image是专有且不可访问的，阻碍了研究发展。

Method: 使用GPT-4o生成45K文本到图像和46K文本加图像到图像的数据集（ShareGPT-4o-Image），并基于此训练Janus-4o模型。

Result: Janus-4o在文本到图像生成上超越了前代模型，新增了文本加图像到图像生成功能，且仅需少量样本和训练时间。

Conclusion: 开源ShareGPT-4o-Image和Janus-4o将推动逼真、指令对齐图像生成的开放研究。

Abstract: Recent advances in multimodal generative models have unlocked photorealistic,
instruction-aligned image generation, yet leading systems like GPT-4o-Image
remain proprietary and inaccessible. To democratize these capabilities, we
present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and
46K text-and-image-to-image data, all synthesized using GPT-4o's image
generation capabilities for distilling its advanced image generation abilities.
Leveraging this dataset, we develop Janus-4o, a multimodal large language model
capable of both text-to-image and text-and-image-to-image generation. Janus-4o
not only significantly improves text-to-image generation over its predecessor,
Janus-Pro, but also newly supports text-and-image-to-image generation. Notably,
it achieves impressive performance in text-and-image-to-image generation from
scratch, using only 91K synthetic samples and 6 hours of training on an 8
A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will
foster open research in photorealistic, instruction-aligned image generation.

</details>


### [425] [Cloud-Aware SAR Fusion for Enhanced Optical Sensing in Space Missions](https://arxiv.org/abs/2506.17885)
*Trong-An Bui,Thanh-Thoai Le*

Key words: 云污染, SAR-光学融合, 深度学习, 图像重建

TL;DR: 该研究提出了一种云注意重建框架，通过SAR-光学特征融合和深度学习生成无云的光学图像，显著提高了重建质量。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 云污染严重影响光学卫星图像的可用性，本研究旨在通过融合SAR与光学数据，解决这一问题。

Method: 采用云注意特征融合机制，结合SAR的结构信息和光学数据的频谱特性，并使用自适应损失权重策略优先处理云遮挡区域。

Result: 实验结果显示，该方法的PSNR为31.01 dB，SSIM为0.918，MAE为0.017，优于现有方法。

Conclusion: 该框架能有效生成高质量、空间和频谱一致的无云光学图像。

Abstract: Cloud contamination significantly impairs the usability of optical satellite
imagery, affecting critical applications such as environmental monitoring,
disaster response, and land-use analysis. This research presents a
Cloud-Attentive Reconstruction Framework that integrates SAR-optical feature
fusion with deep learning-based image reconstruction to generate cloud-free
optical imagery. The proposed framework employs an attention-driven feature
fusion mechanism to align complementary structural information from Synthetic
Aperture Radar (SAR) with spectral characteristics from optical data.
Furthermore, a cloud-aware model update strategy introduces adaptive loss
weighting to prioritize cloud-occluded regions, enhancing reconstruction
accuracy. Experimental results demonstrate that the proposed method outperforms
existing approaches, achieving a PSNR of 31.01 dB, SSIM of 0.918, and MAE of
0.017. These outcomes highlight the framework's effectiveness in producing
high-fidelity, spatially and spectrally consistent cloud-free optical images.

</details>


### [426] [BeltCrack: the First Sequential-image Industrial Conveyor Belt Crack Detection Dataset and Its Baseline with Triple-domain Feature Learning](https://arxiv.org/abs/2506.17892)
*Jianghong Huang,Luping Ji,Xin Ma,Mao Ye*

Key words: 传送带裂缝检测, 真实工业数据集, 机器学习, 三域特征融合

TL;DR: 该论文构建了首个真实工业场景中的传送带裂缝数据集（BeltCrack14ks和BeltCrack9kd），并提出了一种基于时空频三域特征分层融合学习的基准方法，验证了数据集的有效性和方法的优越性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传送带的健康状况对工业效率和安全性至关重要，但目前缺乏真实工业场景的裂缝数据集，阻碍了机器学习在该领域的应用。

Method: 从真实工厂场景构建裂缝数据集，并提出基于时空频三域特征分层融合学习的基准方法。

Result: 实验验证了数据集的有效性，并表明提出的方法优于其他类似检测方法。

Conclusion: 构建的数据集和基准方法为传送带裂缝检测领域的研究提供了重要资源和参考。

Abstract: Conveyor belt is a category of important equipments in modern industry,
widely applied in production and manufacturing Fields. Its health status is
much critical to operation efficiency and safety hazards. Among the factors
affecting belt health, crack is often one of the most threatening risks.
Currently, considering safety, how to intelligently detect belt cracks is
catching an increasing attention. To implement the intelligent detection with
machine learning, real crack samples are believed to be necessary. However,
existing crack datasets primarily focus on pavement scenarios or synthetic
data, no real-world industrial belt crack datasets at all. To propel machine
learning advancement in this field, this paper constructs the first
sequential-image belt crack detection datasets (BeltCrack14ks and
BeltCrack9kd), from real-world factory scenes. Furthermore, to validate
usability and effectiveness, we propose a special baseline method with
triple-domain (i.e., time-space-frequency) feature hierarchical fusion learning
for the two whole-new datasets. Experimental results demonstrate the
availability and effectiveness of our dataset. Besides, they also show that our
baseline is obviously superior to other similar detection methods. Our datasets
and source codes are available at https://github.com/UESTC-nnLab/BeltCrack.

</details>


### [427] [STACT-Time: Spatio-Temporal Cross Attention for Cine Thyroid Ultrasound Time Series Classification](https://arxiv.org/abs/2506.18172)
*Irsyad Adam,Tengyue Zhang,Shrayes Raman,Zhuyu Qiu,Brandon Taraku,Hexiang Feng,Sile Wang,Ashwath Radhachandran,Shreeram Athreya,Vedrana Ivezic,Peipei Ping,Corey Arnold,William Speier*

Key words: 甲状腺癌, 超声影像, 深度学习, 时空注意力, 活检优化

TL;DR: 本文提出了一种基于时空交叉注意力的深度学习模型（STACT-Time），用于甲状腺超声动态影像的分类，以减少不必要的良性结节活检，并在恶性预测中表现优于现有模型。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 甲状腺结节活检（FNA）虽然有效，但常导致良性结节的过度活检，引发患者不适。现有系统（如TI-RADS）受限于观察者间的变异性，且传统深度学习方法未能充分利用超声动态影像的时空信息。

Method: 提出STACT-Time模型，结合自注意力和交叉注意力机制，利用预训练模型生成的分割掩模增强超声动态影像的时空特征表示。

Result: 模型在交叉验证中的精确度为0.91（±0.02），F1分数为0.89（±0.02），优于现有方法。

Conclusion: 该模型通过减少良性结节的活检并保持对恶性肿瘤的高灵敏度，有望改善临床决策和患者预后。

Abstract: Thyroid cancer is among the most common cancers in the United States. Thyroid
nodules are frequently detected through ultrasound (US) imaging, and some
require further evaluation via fine-needle aspiration (FNA) biopsy. Despite its
effectiveness, FNA often leads to unnecessary biopsies of benign nodules,
causing patient discomfort and anxiety. To address this, the American College
of Radiology Thyroid Imaging Reporting and Data System (TI-RADS) has been
developed to reduce benign biopsies. However, such systems are limited by
interobserver variability. Recent deep learning approaches have sought to
improve risk stratification, but they often fail to utilize the rich temporal
and spatial context provided by US cine clips, which contain dynamic global
information and surrounding structural changes across various views. In this
work, we propose the Spatio-Temporal Cross Attention for Cine Thyroid
Ultrasound Time Series Classification (STACT-Time) model, a novel
representation learning framework that integrates imaging features from US cine
clips with features from segmentation masks automatically generated by a
pretrained model. By leveraging self-attention and cross-attention mechanisms,
our model captures the rich temporal and spatial context of US cine clips while
enhancing feature representation through segmentation-guided learning. Our
model improves malignancy prediction compared to state-of-the-art models,
achieving a cross-validation precision of 0.91 (plus or minus 0.02) and an F1
score of 0.89 (plus or minus 0.02). By reducing unnecessary biopsies of benign
nodules while maintaining high sensitivity for malignancy detection, our model
has the potential to enhance clinical decision-making and improve patient
outcomes.

</details>


### [428] [Fast Neural Inverse Kinematics on Human Body Motions](https://arxiv.org/abs/2506.17996)
*David Tolpin,Sefy Kagarlitsky*

Key words: 无标记运动捕捉,神经逆向运动学,实时,3D关键点

TL;DR: 本文提出了一种快速的神经逆向运动学框架，用于从3D关键点实时捕捉人体运动，解决了无标记运动捕捉系统的高计算需求和较慢推理问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统无标记运动捕捉系统虽然灵活且成本低，但计算需求高且推理慢，限制了实时应用。本文旨在解决这一问题。

Method: 详细描述了网络架构、训练方法和推理过程，并通过消融实验验证关键设计决策。

Result: 框架在定性和定量评估中表现良好，支持实时捕获。

Conclusion: 该框架为实时人体运动捕捉提供了一种高效可靠的解决方案。

Abstract: Markerless motion capture enables the tracking of human motion without
requiring physical markers or suits, offering increased flexibility and reduced
costs compared to traditional systems. However, these advantages often come at
the expense of higher computational demands and slower inference, limiting
their applicability in real-time scenarios. In this technical report, we
present a fast and reliable neural inverse kinematics framework designed for
real-time capture of human body motions from 3D keypoints. We describe the
network architecture, training methodology, and inference procedure in detail.
Our framework is evaluated both qualitatively and quantitatively, and we
support key design decisions through ablation studies.

</details>


### [429] [Enhancing VICReg: Random-Walk Pairing for Improved Generalization and Better Global Semantics Capturing](https://arxiv.org/abs/2506.18104)
*Idan Simai,Ronen Talmon,Uri Shaham*

Key words: 自监督学习, 泛化能力, 图像表示, SAG-VICReg

TL;DR: 通过光谱嵌入视角分析VICReg，发现其因过度依赖训练数据可能在未见数据上泛化能力不足。提出SAG-VICReg方法，通过新训练技术提升全局语义捕捉能力，实验表明其优于现有自监督学习方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究VICReg方法在未见数据上泛化能力的潜在不足，探索如何改进以生成更具意义的图像表示。

Method: 在VICReg基础上引入新训练技术，提出SAG-VICReg，增强模型对全局语义的捕捉和泛化能力。

Result: SAG-VICReg在泛化能力上表现优越，超过当前自监督学习基准方法，并在全局语义评估指标上表现突出。

Conclusion: SAG-VICReg有效解决了VICReg的泛化问题，同时提出了一种新的无标签评估嵌入质量的方法。

Abstract: In this paper, we argue that viewing VICReg-a popular self-supervised
learning (SSL) method--through the lens of spectral embedding reveals a
potential source of sub-optimality: it may struggle to generalize robustly to
unseen data due to overreliance on the training data. This observation invites
a closer look at how well this method achieves its goal of producing meaningful
representations of images outside of the training set as well. Here, we
investigate this issue and introduce SAG-VICReg (Stable and Generalizable
VICReg), a method that builds on VICReg by incorporating new training
techniques. These enhancements improve the model's ability to capture global
semantics within the data and strengthen the generalization capabilities.
Experiments demonstrate that SAG-VICReg effectively addresses the
generalization challenge while matching or surpassing diverse state-of-the-art
SSL baselines. Notably, our method exhibits superior performance on metrics
designed to evaluate global semantic understanding, while simultaneously
maintaining competitive results on local evaluation metrics. Furthermore, we
propose a new standalone evaluation metric for embeddings that complements the
standard evaluation methods and accounts for the global data structure without
requiring labels--a key issue when tagged data is scarce or not available.

</details>


### [430] [Multimodal Fusion SLAM with Fourier Attention](https://arxiv.org/abs/2506.18204)
*Youjie Zhou,Guofeng Mei,Yiming Wang,Yi Wan,Fabio Poiesi*

Key words: 视觉SLAM, 多模态融合, 快速傅里叶变换, 自注意力, 知识蒸馏

TL;DR: FMF-SLAM是一种高效的多模态融合SLAM方法，利用快速傅里叶变换（FFT）提升算法效率，适用于噪声、光线变化和黑暗环境。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统光流视觉SLAM在噪声、光线变化和黑暗环境下性能受限，且计算资源需求高。

Method: 提出基于傅里叶的自注意力和跨注意力机制，从RGB和深度信号提取特征，并结合多尺度知识蒸馏增强多模态特征交互。

Result: 在TUM、TartanAir和真实数据集上验证，表现出色，且与GNSS-RTK和全局Bundle Adjustment集成，实现实时性能。

Conclusion: FMF-SLAM在复杂环境下具有高效性和实用性，代码和数据集已开源。

Abstract: Visual SLAM is particularly challenging in environments affected by noise,
varying lighting conditions, and darkness. Learning-based optical flow
algorithms can leverage multiple modalities to address these challenges, but
traditional optical flow-based visual SLAM approaches often require significant
computational resources.To overcome this limitation, we propose FMF-SLAM, an
efficient multimodal fusion SLAM method that utilizes fast Fourier transform
(FFT) to enhance the algorithm efficiency. Specifically, we introduce a novel
Fourier-based self-attention and cross-attention mechanism to extract features
from RGB and depth signals. We further enhance the interaction of multimodal
features by incorporating multi-scale knowledge distillation across modalities.
We also demonstrate the practical feasibility of FMF-SLAM in real-world
scenarios with real time performance by integrating it with a security robot by
fusing with a global positioning module GNSS-RTK and global Bundle Adjustment.
Our approach is validated using video sequences from TUM, TartanAir, and our
real-world datasets, showcasing state-of-the-art performance under noisy,
varying lighting, and dark conditions.Our code and datasets are available at
https://github.com/youjie-zhou/FMF-SLAM.git.

</details>


### [431] [Deep Learning-based Alignment Measurement in Knee Radiographs](https://arxiv.org/abs/2506.18209)
*Zhisen Hu,Dominic Cullen,Peter Thompson,David Johnson,Chang Bian,Aleksei Tiulpin,Timothy Cootes,Claudia Lindner*

Key words: 膝关节对齐、深度学习、hourglass网络、注意力门、解剖标志

TL;DR: 本文提出了一种基于深度学习的方法，通过自动定位膝关节解剖标志来测量膝关节对齐（KA），提高了测量效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统膝关节对齐测量方法耗时且需要长腿X光片，本研究旨在利用深度学习技术自动化和优化这一过程。

Method: 基于hourglass网络并结合注意力门结构，自动定位100多个膝关节解剖标志，并集成KA测量。

Result: 该方法提供高精度的KA测量，与临床实测相比平均绝对差异约1°，术前和术后的一致性分别为高（ICC=0.97）和良好（ICC=0.86）。

Conclusion: 研究表明KA评估可以通过深度学习实现高精度自动化，为临床工作流提供数字化支持。

Abstract: Radiographic knee alignment (KA) measurement is important for predicting
joint health and surgical outcomes after total knee replacement. Traditional
methods for KA measurements are manual, time-consuming and require long-leg
radiographs. This study proposes a deep learning-based method to measure KA in
anteroposterior knee radiographs via automatically localized knee anatomical
landmarks. Our method builds on hourglass networks and incorporates an
attention gate structure to enhance robustness and focus on key anatomical
features. To our knowledge, this is the first deep learning-based method to
localize over 100 knee anatomical landmarks to fully outline the knee shape
while integrating KA measurements on both pre-operative and post-operative
images. It provides highly accurate and reliable anatomical varus/valgus KA
measurements using the anatomical tibiofemoral angle, achieving mean absolute
differences ~1{\deg} when compared to clinical ground truth measurements.
Agreement between automated and clinical measurements was excellent
pre-operatively (intra-class correlation coefficient (ICC) = 0.97) and good
post-operatively (ICC = 0.86). Our findings demonstrate that KA assessment can
be automated with high accuracy, creating opportunities for digitally enhanced
clinical workflows.

</details>


### [432] [Cross-Architecture Knowledge Distillation (KD) for Retinal Fundus Image Anomaly Detection on NVIDIA Jetson Nano](https://arxiv.org/abs/2506.18220)
*Berk Yilmaz,Aniruddh Aiyengar*

Key words: 视网膜疾病分类, 知识蒸馏, ViT, CNN, 边缘设备, 自监督学习

TL;DR: 该论文提出了一种轻量级、适用于边缘设备的视网膜疾病分类器，通过交叉架构知识蒸馏技术，将高性能的ViT教师模型压缩为CNN学生模型，以在资源受限的环境中部署。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 在资源有限的地区，缺乏可靠的视网膜疾病诊断设备，因此需要一种轻量化且高精度的解决方案。

Method: 使用I-JEPA自监督学习预训练ViT教师模型，然后通过提出的PCA投影器、GL投影器和多视图鲁棒训练方法，将其压缩为CNN学生模型。

Result: 学生模型的参数仅为教师模型的2.6%，但分类准确率达到89%，保留了教师模型约93%的诊断性能。

Conclusion: 该方法展示了在资源受限地区实现可扩展的视网膜疾病AI分诊解决方案的潜力。

Abstract: Early and accurate identification of retinal ailments is crucial for averting
ocular decline; however, access to dependable diagnostic devices is not often
available in low-resourced settings. This project proposes to solve that by
developing a lightweight, edge-device deployable disease classifier using
cross-architecture knowledge distilling. We first train a high-capacity vision
transformer (ViT) teacher model, pre-trained using I-JEPA self-supervised
learning, to classify fundus images into four classes: Normal, Diabetic
Retinopathy, Glaucoma, and Cataract. We kept an Internet of Things (IoT) focus
when compressing to a CNN-based student model for deployment in
resource-limited conditions, such as the NVIDIA Jetson Nano. This was
accomplished using a novel framework which included a Partitioned
Cross-Attention (PCA) projector, a Group-Wise Linear (GL) projector, and a
multi-view robust training method. The teacher model has 97.4 percent more
parameters than the student model, with it achieving 89 percent classification
with a roughly 93 percent retention of the teacher model's diagnostic
performance. The retention of clinical classification behavior supports our
method's initial aim: compression of the ViT while retaining accuracy. Our work
serves as an example of a scalable, AI-driven triage solution for retinal
disorders in under-resourced areas.

</details>


### [433] [Make It Efficient: Dynamic Sparse Attention for Autoregressive Image Generation](https://arxiv.org/abs/2506.18226)
*Xunzhi Xiang,Qi Fan*

Key words: 自回归图像生成，注意力机制，KV缓存，资源优化

TL;DR: 提出了一种名为ADSA的无训练上下文优化方法，通过动态识别关键历史令牌，显著减少了内存和计算开销。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决自回归图像生成模型在推断过程中因长上下文导致的内存和计算延迟问题。

Method: 引入自适应动态稀疏注意力（ADSA）和动态KV缓存更新机制，优化注意力和资源使用。

Result: 实验表明ADSA在生成质量和资源效率上均表现优越，GPU内存消耗减少约50%。

Conclusion: ADSA有效缓解了长上下文推断的资源负担，同时保持了语义和纹理的一致性。

Abstract: Autoregressive conditional image generation models have emerged as a dominant
paradigm in text-to-image synthesis. These methods typically convert images
into one-dimensional token sequences and leverage the self-attention mechanism,
which has achieved remarkable success in natural language processing, to
capture long-range dependencies, model global context, and ensure semantic
coherence. However, excessively long contexts during inference lead to
significant memory overhead caused by KV-cache and computational delays. To
alleviate these challenges, we systematically analyze how global semantics,
spatial layouts, and fine-grained textures are formed during inference, and
propose a novel training-free context optimization method called Adaptive
Dynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies
historical tokens crucial for maintaining local texture consistency and those
essential for ensuring global semantic coherence, thereby efficiently
streamlining attention computation. Additionally, we introduce a dynamic
KV-cache update mechanism tailored for ADSA, reducing GPU memory consumption
during inference by approximately $50\%$. Extensive qualitative and
quantitative experiments demonstrate the effectiveness and superiority of our
approach in terms of both generation quality and resource efficiency.

</details>


### [434] [Semantic Structure-Aware Generative Attacks for Enhanced Adversarial Transferability](https://arxiv.org/abs/2506.18248)
*Jongoh Jeong,Hunmin Yang,Jaeseok Jeong,Kuk-Jin Yoon*

Key words: 生成对抗攻击，平均教师，特征蒸馏，语义一致性，对抗可转移性

TL;DR: 文章提出了一种基于平均教师的语义结构感知生成对抗攻击框架，通过特征蒸馏增强对抗扰动的语义一致性和可转移性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有生成对抗攻击方法未能充分利用生成模型的语义信息，限制了对抗扰动的可转移性。

Method: 提出基于平均教师的框架，通过特征蒸馏引导生成器的早期层激活与语义丰富的教师模型一致，增强扰动与对象显著区域的对应关系。

Result: 实验表明，该方法在多种模型、领域和任务中均优于现有生成对抗攻击方法，并通过新提出的ACR指标验证了其优势。

Conclusion: 该方法通过语义一致性引导，显著提升了生成对抗攻击的可转移性和效率。

Abstract: Generative adversarial attacks train a perturbation generator on a white-box
surrogate model and subsequently apply the crafted perturbations to unseen
black-box victim models. In contrast to iterative attacks, these methods
deliver superior inference-time efficiency, scalability, and transferability;
however, up until now, existing studies have not fully exploited the
representational capacity of generative models to preserve and harness semantic
information. Specifically, the intermediate activations of the generator encode
rich semantic features--object boundaries and coarse shapes--that remain
under-exploited, thereby limiting the alignment of perturbations with
object-salient regions which are critical for adversarial transferability. To
remedy this, we introduce a semantic structure-aware attack framework based on
the Mean Teacher, which serves as a temporally smoothed feature reference. With
this smoothed reference, we further direct semantic consistency between the
early-layer activations in the student and those of the semantically rich
teacher by feature distillation. By anchoring perturbation synthesis to the
semantically salient early intermediate blocks within the generator based on
empirical findings, our method guides progressive adversarial perturbation on
regions that substantially enhance adversarial transferability. We conduct
extensive experiments over diverse models, domains and tasks to demonstrate
consistent improvements relative to state-of-the-art generative attacks,
comprehensively evaluated using conventional metrics and our newly proposed
Accidental Correction Rate (ACR).

</details>


### [435] [Escaping the SpuriVerse: Can Large Vision-Language Models Generalize Beyond Seen Spurious Correlations?](https://arxiv.org/abs/2506.18322)
*Yiwei Yang,Chung Peng Lee,Shangbin Feng,Dora Zhao,Bingbing Wen,Anthony Z. Liu,Yulia Tsvetkov,Bill Howe*

Key words: 虚假相关性，多模态模型，视觉语言模型，基准测试，微调

TL;DR: 论文研究了多模态大型视觉语言模型（LVLMs）中虚假相关性的问题，开发了名为SpuriVerse的新基准测试，评估了15种LVLMs的表现，并通过微调实验提出改进方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前关于虚假相关性的研究多局限于人为设置的狭窄任务，而本文旨在研究真实多模态数据中虚假相关性的影响。

Method: 利用GPT-4o在视觉问答（VQA）中的错误数据，通过人工标注和合成反事实评估构建SpuriVerse基准，并评估多种LVLMs的性能。

Result: 即使最先进的闭源模型在SpuriVerse上的准确率仅为37.1%，但通过微调合成数据，性能可提升至78.40%。

Conclusion: 模型通过学习多样化的虚假相关性模式，能够避免‘捷径’并关注整体图像上下文，从而提升泛化能力。

Abstract: Finetuning can cause spurious correlations to arise between non-essential
features and the target labels, but benchmarks to study these effects involve
contrived settings and narrow tasks. In contrast, we consider spurious
correlations in multi-modal Large Vision Language Models (LVLMs) pretrained on
extensive and diverse datasets without explicit task supervision. We develop a
benchmark by sourcing GPT-4o errors on real-world visual-question-answering
(VQA) benchmarks, then curating a subset through LVLM-human annotation and
synthetic counterfactual evaluation to identify errors caused by spurious
correlations. This process yields SpuriVerse, a novel benchmark comprised of
124 distinct types of spurious correlations extracted from real-world datasets,
each containing 1 realistic and 10 synthetic VQA samples for a total of 1364
multiple choice questions. We evaluate 15 open and closed-source LVLMs on
SpuriVerse, finding that even state-of-the-art closed-source models struggle
significantly, achieving at best only 37.1% accuracy. Fine-tuning on synthetic
examples that emphasize the spurious correlation improves performance to
78.40%, suggesting that training on diverse spurious patterns generalizes to
unseen situations: models appear to learn to avoid "shortcuts" and attend to
the overall image context.

</details>


### [436] [Open Set Recognition for Endoscopic Image Classification: A Deep Learning Approach on the Kvasir Dataset](https://arxiv.org/abs/2506.18284)
*Kasra Moazzami,Seoyoun Son,John Lin,Sun Min Lee,Daniel Son,Hayeon Lee,Jeongho Lee,Seongji Lee*

Key words: 内窥镜图像分类,开集识别,Kvasir数据集,深度学习

TL;DR: 该论文探讨了在开放式临床环境中应用开集识别（OSR）技术对Kvasir数据集的内窥镜图像进行分类研究，评估了多种深度学习架构在开集条件下的性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统闭集分类框架在开放式临床环境中存在局限性，无法处理未知条件，从而影响模型可靠性。

Method: 使用Kvasir数据集，评估了ResNet-50、Swin Transformer和混合ResNet-Transformer模型在闭集和开集条件下的表现，并采用OpenMax作为基线方法。

Result: 研究为医学图像分析中的OSR性能提供了基准，并揭示了模型在临床实际环境中的行为特点。

Conclusion: 开集识别技术对AI系统在内窥镜中的安全部署至关重要。

Abstract: Endoscopic image classification plays a pivotal role in medical diagnostics
by identifying anatomical landmarks and pathological findings. However,
conventional closed-set classification frameworks are inherently limited in
open-world clinical settings, where previously unseen conditions can arise
andcompromise model reliability. To address this, we explore the application of
Open Set Recognition (OSR) techniques on the Kvasir dataset, a publicly
available and diverse endoscopic image collection. In this study, we evaluate
and compare the OSR capabilities of several representative deep learning
architectures, including ResNet-50, Swin Transformer, and a hybrid
ResNet-Transformer model, under both closed-set and open-set conditions.
OpenMax is adopted as a baseline OSR method to assess the ability of these
models to distinguish known classes from previously unseen categories. This
work represents one of the first efforts to apply open set recognition to the
Kvasir dataset and provides a foundational benchmark for evaluating OSR
performance in medical image analysis. Our results offer practical insights
into model behavior in clinically realistic settings and highlight the
importance of OSR techniques for the safe deployment of AI systems in
endoscopy.

</details>


### [437] [Selective Social-Interaction via Individual Importance for Fast Human Trajectory Prediction](https://arxiv.org/abs/2506.18291)
*Yota Urano,Hiromu Taketsugu,Norimichi Ukita*

Key words: 轨迹预测、重要性估计、Gumbel Softmax、JRDB数据集

TL;DR: 提出了一种基于重要性估计的邻居选择架构来预测主人物轨迹，使用Gumbel Softmax解决不可微分操作问题，实验证明其在JRDB数据集上速度快且精度有竞争力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 通过选择重要邻居来提升主人物轨迹预测的效率和准确性。

Method: 设计了重要性估计模块（Importance Estimator）和采用Gumbel Softmax以解决不可微分操作问题。

Result: 在JRDB数据集上实现了速度提升且保持了竞争性预测精度。

Conclusion: 提出的方法能有效选择重要邻居并优化轨迹预测性能。

Abstract: This paper presents an architecture for selecting important neighboring
people to predict the primary person's trajectory. To achieve effective
neighboring people selection, we propose a people selection module called the
Importance Estimator which outputs the importance of each neighboring person
for predicting the primary person's future trajectory. To prevent gradients
from being blocked by non-differentiable operations when sampling surrounding
people based on their importance, we employ the Gumbel Softmax for training.
Experiments conducted on the JRDB dataset show that our method speeds up the
process with competitive prediction accuracy.

</details>


### [438] [A Set-to-Set Distance Measure in Hyperbolic Space](https://arxiv.org/abs/2506.18529)
*Pengxiang Li,Wei Wu,Zhi Gao,Xiaomeng Fan,Peilin Yu,Yuwei Wu,Zhipeng Lu,Yunde Jia,Mehrtash Harandi*

Key words: 双曲空间，集合距离，HS2SD，爱因斯坦中点，拓扑结构

TL;DR: 本文提出了一种双曲空间中的集合间距离度量方法HS2SD，结合局部和全局结构信息以捕捉集合间的复杂关系。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现实应用中需要比较双曲空间中的数据点集合，而现有方法仅关注点对点距离，无法有效捕捉集合间的局部与全局结构信息。

Method: 提出HS2SD度量，通过双曲集合的爱因斯坦中点间的测地距离（全局）和拓扑特性（局部）来量化集合间差异；使用有限Thue-Morse序列近似拓扑结构。

Result: 实验表明HS2SD在实体匹配、标准图像分类和小样本图像分类中优于现有方法。

Conclusion: HS2SD能够更精细地建模双曲集合中的层级和复杂关系，为集合间比较提供了新思路。

Abstract: We propose a hyperbolic set-to-set distance measure for computing
dissimilarity between sets in hyperbolic space. While point-to-point distances
in hyperbolic space effectively capture hierarchical relationships between data
points, many real-world applications require comparing sets of hyperbolic data
points, where the local structure and the global structure of the sets carry
crucial semantic information. The proposed the \underline{h}yperbolic
\underline{s}et-\underline{to}-\underline{s}et \underline{d}istance measure
(HS2SD) integrates both global and local structural information: global
structure through geodesic distances between Einstein midpoints of hyperbolic
sets, and local structure through topological characteristics of the two sets.
To efficiently compute topological differences, we prove that using a finite
Thue-Morse sequence of degree and adjacency matrices can serve as a robust
approximation to capture the topological structure of a set. In this case, by
considering the topological differences, HS2SD provides a more nuanced
understanding of the relationships between two hyperbolic sets. Empirical
evaluation on entity matching, standard image classification, and few-shot
image classification demonstrates that our distance measure outperforms
existing methods by effectively modeling the hierarchical and complex
relationships inherent in hyperbolic sets.

</details>


### [439] [A Multi-Scale Spatial Attention-Based Zero-Shot Learning Framework for Low-Light Image Enhancement](https://arxiv.org/abs/2506.18323)
*Muhammad Azeem Aslam,Hassan Khalid,Nisar Ahmed*

Key words: 低光照增强,零样本学习,多尺度空间注意力,深度曲线估计,循环增强

TL;DR: 提出了一种名为LucentVisionNet的零样本学习框架，用于低光照图像增强，结合多尺度空间注意力和深度曲线估计网络，实现了精细增强并保持语义和感知保真度。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决传统和基于深度学习的低光照图像增强方法在无配对训练数据时的局限性。

Method: 采用多尺度空间注意力和深度曲线估计网络，结合循环增强策略和复合损失函数进行优化。

Result: 在多个基准数据集上优于现有监督、无监督和零样本方法，提升了视觉质量、结构一致性和计算效率。

Conclusion: LucentVisionNet适用于移动摄影、监控和自主导航等实际应用，具有高效性和高质量增强能力。

Abstract: Low-light image enhancement remains a challenging task, particularly in the
absence of paired training data. In this study, we present LucentVisionNet, a
novel zero-shot learning framework that addresses the limitations of
traditional and deep learning-based enhancement methods. The proposed approach
integrates multi-scale spatial attention with a deep curve estimation network,
enabling fine-grained enhancement while preserving semantic and perceptual
fidelity. To further improve generalization, we adopt a recurrent enhancement
strategy and optimize the model using a composite loss function comprising six
tailored components, including a novel no-reference image quality loss inspired
by human visual perception. Extensive experiments on both paired and unpaired
benchmark datasets demonstrate that LucentVisionNet consistently outperforms
state-of-the-art supervised, unsupervised, and zero-shot methods across
multiple full-reference and no-reference image quality metrics. Our framework
achieves high visual quality, structural consistency, and computational
efficiency, making it well-suited for deployment in real-world applications
such as mobile photography, surveillance, and autonomous navigation.

</details>


### [440] [SpaNN: Detecting Multiple Adversarial Patches on CNNs by Spanning Saliency Thresholds](https://arxiv.org/abs/2506.18591)
*Mauricio Byrd Victorica,György Dán,Henrik Sandberg*

Key words: 对抗攻击,多补丁防御,SpaNN,目标检测,图像分类

TL;DR: SpaNN是一种针对多补丁对抗攻击的检测器，其计算复杂度与对抗补丁数量无关，通过二值化特征图集合和聚类实现了高效检测。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有防御方法主要针对单补丁攻击，对多补丁攻击效果不佳或计算代价过高，因此需要一种高效且独立于补丁数量的防御方法。

Method: SpaNN通过对受害者模型第一卷积层的激活应用一组显著性阈值生成二值化特征图集合，进行聚类后输入分类器进行攻击检测。

Result: 在四个常用数据集上，SpaNN在目标检测和图像分类任务中分别以11%和27%的优势超越现有方法。

Conclusion: SpaNN在多补丁攻击场景下表现出高效性和鲁棒性，显著优于现有防御方法。

Abstract: State-of-the-art convolutional neural network models for object detection and
image classification are vulnerable to physically realizable adversarial
perturbations, such as patch attacks. Existing defenses have focused,
implicitly or explicitly, on single-patch attacks, leaving their sensitivity to
the number of patches as an open question or rendering them computationally
infeasible or inefficient against attacks consisting of multiple patches in the
worst cases. In this work, we propose SpaNN, an attack detector whose
computational complexity is independent of the expected number of adversarial
patches. The key novelty of the proposed detector is that it builds an ensemble
of binarized feature maps by applying a set of saliency thresholds to the
neural activations of the first convolutional layer of the victim model. It
then performs clustering on the ensemble and uses the cluster features as the
input to a classifier for attack detection. Contrary to existing detectors,
SpaNN does not rely on a fixed saliency threshold for identifying adversarial
regions, which makes it robust against white box adversarial attacks. We
evaluate SpaNN on four widely used data sets for object detection and
classification, and our results show that SpaNN outperforms state-of-the-art
defenses by up to 11 and 27 percentage points in the case of object detection
and the case of image classification, respectively. Our code is available at
https://github.com/gerkbyrd/SpaNN.

</details>


### [441] [Including Semantic Information via Word Embeddings for Skeleton-based Action Recognition](https://arxiv.org/abs/2506.18721)
*Dustin Aganian,Erik Franze,Markus Eisenbach,Horst-Michael Gross*

Key words: 动作识别、骨架、语义信息、词嵌入、工业4.0

TL;DR: 提出了一种基于骨架的动作识别新方法，利用词嵌入编码语义信息，替代传统的独热编码，显著提升了分类性能与泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统基于骨架的动作识别方法在复杂交互中丢失关键点语义信息，限制了其效果。

Method: 通过词嵌入编码语义信息，生成语义体积替代独热编码，以捕捉关节与物体之间的有意义关系。

Result: 在多个装配数据集上的实验表明，该方法显著提高了分类性能，并支持不同骨架类型和物体类别的泛化。

Conclusion: 语义信息的引入可以显著增强动态多样环境中的骨架动作识别效果。

Abstract: Effective human action recognition is widely used for cobots in Industry 4.0
to assist in assembly tasks. However, conventional skeleton-based methods often
lose keypoint semantics, limiting their effectiveness in complex interactions.
In this work, we introduce a novel approach to skeleton-based action
recognition that enriches input representations by leveraging word embeddings
to encode semantic information. Our method replaces one-hot encodings with
semantic volumes, enabling the model to capture meaningful relationships
between joints and objects. Through extensive experiments on multiple assembly
datasets, we demonstrate that our approach significantly improves
classification performance, and enhances generalization capabilities by
simultaneously supporting different skeleton types and object classes. Our
findings highlight the potential of incorporating semantic information to
enhance skeleton-based action recognition in dynamic and diverse environments.

</details>


### [442] [Latent Space Analysis for Melanoma Prevention](https://arxiv.org/abs/2506.18414)
*Ciro Listone,Aniello Murano*

Key words: 黑色素瘤, 可解释AI, 条件变分自编码器, 潜在空间, SVM

TL;DR: 提出了一种基于条件变分自编码器的新型方法，通过结构化潜在空间实现可解释的皮肤病变风险评估，结合SVM分类器，提升临床适用性与信任度。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 黑色素瘤的高死亡率需要早期、可解释的诊断工具，现有深度学习模型仅提供二元输出，缺乏临床洞察。

Method: 使用条件变分自编码器学习结构化潜在空间，捕捉病变语义关系；结合SVM实现分类与风险评估。

Result: 模型在区分良性痣与黑色素瘤上表现优异，潜在空间支持视觉与几何解释，风险指示意义明确。

Conclusion: 该方法结合预测性能与临床适用性，促进早期检测、模糊案例识别，增强AI辅助诊断的透明性与信任。

Abstract: Melanoma represents a critical health risk due to its aggressive progression
and high mortality, underscoring the need for early, interpretable diagnostic
tools. While deep learning has advanced in skin lesion classification, most
existing models provide only binary outputs, offering limited clinical insight.
This work introduces a novel approach that extends beyond classification,
enabling interpretable risk modelling through a Conditional Variational
Autoencoder. The proposed method learns a structured latent space that captures
semantic relationships among lesions, allowing for a nuanced, continuous
assessment of morphological differences. An SVM is also trained on this
representation effectively differentiating between benign nevi and melanomas,
demonstrating strong and consistent performance. More importantly, the learned
latent space supports visual and geometric interpretation of malignancy, with
the spatial proximity of a lesion to known melanomas serving as a meaningful
indicator of risk. This approach bridges predictive performance with clinical
applicability, fostering early detection, highlighting ambiguous cases, and
enhancing trust in AI-assisted diagnosis through transparent and interpretable
decision-making.

</details>


### [443] [Focus Your Attention: Towards Data-Intuitive Lightweight Vision Transformers](https://arxiv.org/abs/2506.18791)
*Suyash Gaurav,Muhammad Farhan Humayun,Jukka Heikkonen,Jatin Chaudhary*

Key words: Vision Transformer, 超像素, 注意力机制, 计算效率, 边缘部署

TL;DR: 提出了一种基于超像素的补丁池化（SPPP）技术和轻量潜在注意力（LLA）模块，以降低计算复杂度并提高效率，同时保持性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有视觉Transformer依赖大量计算和内存资源，且迁移学习困难，需要更高效的解决方案。

Method: 采用SPPP生成语义丰富的补丁嵌入，并结合LLA模块通过潜在令牌减少注意力机制的复杂度。

Result: 实验表明，该方法显著提高了计算效率，同时性能与现有最优方法相当。

Conclusion: 所提方法为边缘部署提供了高效节能的Transformer架构。

Abstract: The evolution of Vision Transformers has led to their widespread adaptation
to different domains. Despite large-scale success, there remain significant
challenges including their reliance on extensive computational and memory
resources for pre-training on huge datasets as well as difficulties in
task-specific transfer learning. These limitations coupled with energy
inefficiencies mainly arise due to the computation-intensive self-attention
mechanism. To address these issues, we propose a novel Super-Pixel Based Patch
Pooling (SPPP) technique that generates context-aware, semantically rich, patch
embeddings to effectively reduce the architectural complexity and improve
efficiency. Additionally, we introduce the Light Latent Attention (LLA) module
in our pipeline by integrating latent tokens into the attention mechanism
allowing cross-attention operations to significantly reduce the time and space
complexity of the attention module. By leveraging the data-intuitive patch
embeddings coupled with dynamic positional encodings, our approach adaptively
modulates the cross-attention process to focus on informative regions while
maintaining the global semantic structure. This targeted attention improves
training efficiency and accelerates convergence. Notably, the SPPP module is
lightweight and can be easily integrated into existing transformer
architectures. Extensive experiments demonstrate that our proposed architecture
provides significant improvements in terms of computational efficiency while
achieving comparable results with the state-of-the-art approaches, highlighting
its potential for energy-efficient transformers suitable for edge deployment.
(The code is available on our GitHub repository:
https://github.com/zser092/Focused-Attention-ViT).

</details>


### [444] [Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for Prognosis Prediction in Medical Imaging](https://arxiv.org/abs/2506.18434)
*Filippo Ruffini,Elena Mulero Ayllon,Linlin Shen,Paolo Soda,Valerio Guarrasi*

Key words: AI, 医学影像, 预后预测, 微调策略, COVID-19, 可转移性

TL;DR: 该论文构建了一个结构化基准，用于评估卷积神经网络和基础模型在COVID-19患者预后预测中的可转移性，采用了多种微调策略，并在不同数据场景下进行大规模比较分析。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: AI在医学影像预后预测中潜力巨大，但实际应用仍具挑战性，因此需要系统评估模型的适应性和泛化能力。

Method: 论文设计了多样化的微调策略（如Full Fine-Tuning、Linear Probing及参数高效方法），并在多种学习范式下（包括Few-Shot Learning）测试了预训练模型的性能。

Result: 通过大规模比较分析，研究了模型在数据稀缺和类别不平衡条件下的适应能力，揭示了各策略的优缺点。

Conclusion: 研究为临床预后预测中AI解决方案的实际部署提供了实用指导。

Abstract: Artificial Intelligence (AI) holds significant promise for improving
prognosis prediction in medical imaging, yet its effective application remains
challenging. In this work, we introduce a structured benchmark explicitly
designed to evaluate and compare the transferability of Convolutional Neural
Networks and Foundation Models in predicting clinical outcomes in COVID-19
patients, leveraging diverse publicly available Chest X-ray datasets. Our
experimental methodology extensively explores a wide set of fine-tuning
strategies, encompassing traditional approaches such as Full Fine-Tuning and
Linear Probing, as well as advanced Parameter-Efficient Fine-Tuning methods
including Low-Rank Adaptation, BitFit, VeRA, and IA3. The evaluations were
conducted across multiple learning paradigms, including both extensive
full-data scenarios and more clinically realistic Few-Shot Learning settings,
which are critical for modeling rare disease outcomes and rapidly emerging
health threats. By implementing a large-scale comparative analysis involving a
diverse selection of pretrained models, including general-purpose architectures
pretrained on large-scale datasets such as CLIP and DINOv2, to
biomedical-specific models like MedCLIP, BioMedCLIP, and PubMedCLIP, we
rigorously assess each model's capacity to effectively adapt and generalize to
prognosis tasks, particularly under conditions of severe data scarcity and
pronounced class imbalance. The benchmark was designed to capture critical
conditions common in prognosis tasks, including variations in dataset size and
class distribution, providing detailed insights into the strengths and
limitations of each fine-tuning strategy. This extensive and structured
evaluation aims to inform the practical deployment and adoption of robust,
efficient, and generalizable AI-driven solutions in real-world clinical
prognosis prediction workflows.

</details>


### [445] [Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey](https://arxiv.org/abs/2506.18504)
*Xinyao Li,Jingjing Li,Fengling Li,Lei Zhu,Yang Yang,Heng Tao Shen*

Key words: 视觉语言预训练、泛化能力、迁移学习、多模态、基准测试

TL;DR: 本文综述了视觉语言预训练模型（VLM）在泛化能力方面的研究进展，总结了方法、基准测试及结果，并对未来多模态研究提出展望。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有视觉语言预训练模型在零样本任务中表现优异，但在特定领域或专业化任务中性能下降，亟需研究如何泛化其知识。

Method: 从提示、参数和特征三个模块分类总结了VLM泛化方法，并结合迁移学习理论提供新解释。

Result: 梳理了VLM泛化的常用基准和性能比较，探讨了VLM与多模态大语言模型（如DeepSeek-VL）的关系与差异。

Conclusion: 系统综述了视觉语言研究的文献，为当前和未来的多模态研究提供了清晰的研究图景。

Abstract: Recently, vision-language pretraining has emerged as a transformative
technique that integrates the strengths of both visual and textual modalities,
resulting in powerful vision-language models (VLMs). Leveraging web-scale
pretraining data, these models exhibit strong zero-shot capabilities. However,
their performance often deteriorates when confronted with domain-specific or
specialized generalization tasks. To address this, a growing body of research
focuses on transferring or generalizing the rich knowledge embedded in VLMs to
various downstream applications. This survey aims to comprehensively summarize
the generalization settings, methodologies, benchmarking and results in VLM
literatures. Delving into the typical VLM structures, current literatures are
categorized into prompt-based, parameter-based and feature-based methods
according to the transferred modules. The differences and characteristics in
each category are furthered summarized and discussed by revisiting the typical
transfer learning (TL) settings, providing novel interpretations for TL in the
era of VLMs. Popular benchmarks for VLM generalization are further introduced
with thorough performance comparisons among the reviewed methods. Following the
advances in large-scale generalizable pretraining, this survey also discusses
the relations and differences between VLMs and up-to-date multimodal large
language models (MLLM), e.g., DeepSeek-VL. By systematically reviewing the
surging literatures in vision-language research from a novel and practical
generalization prospective, this survey contributes to a clear landscape of
current and future multimodal researches.

</details>


### [446] [Historical Report Guided Bi-modal Concurrent Learning for Pathology Report Generation](https://arxiv.org/abs/2506.18658)
*Ling Zhang,Boxiang Yun,Qingli Li,Yan Wang*

Key words: WSI、病理报告生成、双模态学习、知识检索

TL;DR: BiGen框架通过双模态学习解决WSI病理报告生成的语义不足和信息冗余问题，结合知识检索和特征提取，提升生成报告的质量。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决WSI图像中视觉特征语义不足和信息冗余的问题，模拟病理学家的诊断推理过程。

Method: 提出BiGen框架，包括知识检索机制和双模态学习策略，动态提取关键视觉特征和知识，并通过多模态解码器生成报告。

Result: 在PathText数据集上取得SOTA性能，NLP指标相对提升7.4%，Her-2预测分类指标提升19.1%。

Conclusion: BiGen有效提供WSI相关语义内容并抑制冗余，实验验证其模块的必要性和优越性。

Abstract: Automated pathology report generation from Whole Slide Images (WSIs) faces
two key challenges: (1) lack of semantic content in visual features and (2)
inherent information redundancy in WSIs. To address these issues, we propose a
novel Historical Report Guided \textbf{Bi}-modal Concurrent Learning Framework
for Pathology Report \textbf{Gen}eration (BiGen) emulating pathologists'
diagnostic reasoning, consisting of: (1) A knowledge retrieval mechanism to
provide rich semantic content, which retrieves WSI-relevant knowledge from
pre-built medical knowledge bank by matching high-attention patches and (2) A
bi-modal concurrent learning strategy instantiated via a learnable visual token
and a learnable textual token to dynamically extract key visual features and
retrieved knowledge, where weight-shared layers enable cross-modal alignment
between visual features and knowledge features. Our multi-modal decoder
integrates both modals for comprehensive diagnostic reports generation.
Experiments on the PathText (BRCA) dataset demonstrate our framework's
superiority, achieving state-of-the-art performance with 7.4\% relative
improvement in NLP metrics and 19.1\% enhancement in classification metrics for
Her-2 prediction versus existing methods. Ablation studies validate the
necessity of our proposed modules, highlighting our method's ability to provide
WSI-relevant rich semantic content and suppress information redundancy in WSIs.
Code is publicly available at https://github.com/DeepMed-Lab-ECNU/BiGen.

</details>


### [447] [Benchmarking histopathology foundation models in a multi-center dataset for skin cancer subtyping](https://arxiv.org/abs/2506.18668)
*Pablo Meseguer,Rocío del Amor,Valery Naranjo*

Key words: 病理学基础模型, 多实例学习, 特征提取, 分类性能

TL;DR: 该论文提出了一种用于评估病理学基础模型作为补丁级特征提取器的新基准，并引入了一种新的度量标准FM-SI来衡量模型对分布变化的鲁棒性。实验表明，提取较少偏差的特征能够提高分类性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 在计算病理学中，由于图像尺度巨大，需要多实例学习框架。当前病理学基础模型的多样性引发了评估其在实际任务中有效性的需求。

Method: 通过AI4SkIN数据集设计了一个新的基准，用于评估基础模型在多实例学习分类框架中的表现，并提出FM-SI度量标准来量化模型一致性。

Result: 实验结果表明，提取较少偏差的特征能够显著提升分类性能，特别是在基于相似性的多实例学习分类器中。

Conclusion: 该研究为病理学基础模型的评估提供了新工具和方法，展示了其在复杂任务中的潜在优势。

Abstract: Pretraining on large-scale, in-domain datasets grants histopathology
foundation models (FM) the ability to learn task-agnostic data representations,
enhancing transfer learning on downstream tasks. In computational pathology,
automated whole slide image analysis requires multiple instance learning (MIL)
frameworks due to the gigapixel scale of the slides. The diversity among
histopathology FMs has highlighted the need to design real-world challenges for
evaluating their effectiveness. To bridge this gap, our work presents a novel
benchmark for evaluating histopathology FMs as patch-level feature extractors
within a MIL classification framework. For that purpose, we leverage the
AI4SkIN dataset, a multi-center cohort encompassing slides with challenging
cutaneous spindle cell neoplasm subtypes. We also define the Foundation Model -
Silhouette Index (FM-SI), a novel metric to measure model consistency against
distribution shifts. Our experimentation shows that extracting less biased
features enhances classification performance, especially in similarity-based
MIL classifiers.

</details>


### [448] [Multi-Scale Spectral Attention Module-based Hyperspectral Segmentation in Autonomous Driving Scenarios](https://arxiv.org/abs/2506.18682)
*Imad Ali Shah,Jiarong Li,Tim Brophy,Martin Glavin,Edward Jones,Enda Ward,Brian Deegan*

Key words: 自动驾驶, 高光谱成像, 多尺度光谱注意力, 语义分割, UNet

TL;DR: 本文提出了一种多尺度光谱注意力模块（MSAM），通过整合到UNet的跳跃连接中，显著提升了HSI数据在语义分割中的性能，为自动驾驶环境感知提供了新思路。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 高光谱成像（HSI）在自动驾驶中潜力巨大，但其高维光谱数据处理效率低是主要挑战。

Method: 提出MSAM模块，通过三路不同核大小的1D卷积和自适应特征聚合机制增强光谱特征提取，并将其整合到UNet的跳跃连接中（UNet-MSAM）。

Result: 在多个HSI数据集上，UNet-MSAM以极小的计算开销（0.02%参数和0.82% GFLOPS）平均提升了3.61%的mIoU和3.80%的mF1。

Conclusion: 多尺度核组合优于单尺度配置，证明了HSI在自动驾驶中的潜力，并为设计鲁棒的多尺度光谱特征提取器提供了参考。

Abstract: Recent advances in autonomous driving (AD) have highlighted the potential of
Hyperspectral Imaging (HSI) for enhanced environmental perception, particularly
in challenging weather and lighting conditions. However, efficiently processing
its high-dimensional spectral data remains a significant challenge. This paper
introduces a Multi-scale Spectral Attention Module (MSAM) that enhances
spectral feature extraction through three parallel 1D convolutions with varying
kernel sizes between 1 to 11, coupled with an adaptive feature aggregation
mechanism. By integrating MSAM into UNet's skip connections (UNet-SC), our
proposed UNet-MSAM achieves significant improvements in semantic segmentation
performance across multiple HSI datasets: HyKo-VIS v2, HSI-Drive v2, and
Hyperspectral City v2. Our comprehensive experiments demonstrate that with
minimal computational overhead (on average 0.02% in parameters and 0.82%
GFLOPS), UNet-MSAM consistently outperforms UNet-SC, achieving average
improvements of 3.61% in mean IoU and 3.80% in mF1 across the three datasets.
Through extensive ablation studies, we have established that multi-scale kernel
combinations perform better than single-scale configurations. These findings
demonstrate the potential of HSI processing for AD and provide valuable
insights into designing robust, multi-scale spectral feature extractors for
real-world applications.

</details>


### [449] [SIM-Net: A Multimodal Fusion Network Using Inferred 3D Object Shape Point Clouds from RGB Images for 2D Classification](https://arxiv.org/abs/2506.18683)
*Youcef Sklab,Hanane Ariouat,Eric Chenin,Edi Prifti,Jean-Daniel Zucker*

Key words: 2D图像分类, 3D点云, 多模态网络, 数字化标本, 几何特征

TL;DR: SIM-Net是一种新型的2D图像分类架构，通过将2D图像转换为3D点云，结合纹理和几何特征提升分类性能，特别适用于数字化植物标本分类任务。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统2D图像分类在处理复杂背景、非植物元素和遮挡时表现不佳，SIM-Net通过引入3D结构推理解决这些问题。

Method: SIM-Net采用像素到点的转换生成3D点云，结合CNN和PointNet编码器融合纹理与几何特征。

Result: 在植物标本数据集上，SIM-Net比ResNet101提升了9.9%的准确率和12.3%的F-score，优于多种先进架构。

Conclusion: SIM-Net展示了将3D几何推理引入2D图像分类的有效性，显著提升了分类性能。

Abstract: We introduce the Shape-Image Multimodal Network (SIM-Net), a novel 2D image
classification architecture that integrates 3D point cloud representations
inferred directly from RGB images. Our key contribution lies in a
pixel-to-point transformation that converts 2D object masks into 3D point
clouds, enabling the fusion of texture-based and geometric features for
enhanced classification performance. SIM-Net is particularly well-suited for
the classification of digitized herbarium specimens (a task made challenging by
heterogeneous backgrounds), non-plant elements, and occlusions that compromise
conventional image-based models. To address these issues, SIM-Net employs a
segmentation-based preprocessing step to extract object masks prior to 3D point
cloud generation. The architecture comprises a CNN encoder for 2D image
features and a PointNet-based encoder for geometric features, which are fused
into a unified latent space. Experimental evaluations on herbarium datasets
demonstrate that SIM-Net consistently outperforms ResNet101, achieving gains of
up to 9.9% in accuracy and 12.3% in F-score. It also surpasses several
transformer-based state-of-the-art architectures, highlighting the benefits of
incorporating 3D structural reasoning into 2D image classification tasks.

</details>


### [450] [Matrix-Game: Interactive World Foundation Model](https://arxiv.org/abs/2506.18701)
*Yifan Zhang,Chunli Peng,Boyang Wang,Puyi Wang,Qingcheng Zhu,Fei Kang,Biao Jiang,Zedong Gao,Eric Li,Yang Liu,Yahui Zhou*

Key words: Matrix-Game, 游戏世界生成, Minecraft, 交互式模型, 可控生成

TL;DR: Matrix-Game 是一个可控制游戏世界生成的交互式基础模型，通过两阶段训练实现高质量视频生成。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 旨在开发一个能够精确控制角色动作和相机运动的交互式游戏世界生成模型。

Method: 采用两阶段训练管道，包括无标签预训练和带标签训练，并基于参考图像、运动上下文和用户动作生成视频。

Result: Matrix-Game 在视觉质量、时间一致性、动作可控性和物理规则理解方面均优于现有模型。

Conclusion: Matrix-Game 在生成逼真且可控的游戏视频方面表现优异，并计划开源以推动相关研究。

Abstract: We introduce Matrix-Game, an interactive world foundation model for
controllable game world generation. Matrix-Game is trained using a two-stage
pipeline that first performs large-scale unlabeled pretraining for environment
understanding, followed by action-labeled training for interactive video
generation. To support this, we curate Matrix-Game-MC, a comprehensive
Minecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips
and over 1,000 hours of high-quality labeled clips with fine-grained keyboard
and mouse action annotations. Our model adopts a controllable image-to-world
generation paradigm, conditioned on a reference image, motion context, and user
actions. With over 17 billion parameters, Matrix-Game enables precise control
over character actions and camera movements, while maintaining high visual
quality and temporal coherence. To evaluate performance, we develop GameWorld
Score, a unified benchmark measuring visual quality, temporal quality, action
controllability, and physical rule understanding for Minecraft world
generation. Extensive experiments show that Matrix-Game consistently
outperforms prior open-source Minecraft world models (including Oasis and
MineWorld) across all metrics, with particularly strong gains in
controllability and physical consistency. Double-blind human evaluations
further confirm the superiority of Matrix-Game, highlighting its ability to
generate perceptually realistic and precisely controllable videos across
diverse game scenarios. To facilitate future research on interactive
image-to-world generation, we will open-source the Matrix-Game model weights
and the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.

</details>


### [451] [Deep CNN Face Matchers Inherently Support Revocable Biometric Templates](https://arxiv.org/abs/2506.18731)
*Aman Bhatta,Michael C. King,Kevin W. Bowyer*

Key words: 生物识别、可撤销模板、CNN、ViT、人脸匹配

TL;DR: 现代深度CNN人脸匹配器可生成多个具有等效识别能力且模板不兼容的模型，支持可撤销生物识别，而ViT在此场景中表现较差。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决生物识别模板被泄露后无法撤销的问题，提出可撤销生物识别方案。

Method: 利用深度CNN生成多个等效识别能力但模板不兼容的模型，并测试ViT的适用性。

Result: CNN模型可生成无限多等效模板，ViT效果不如ResNet。

Conclusion: 深度CNN适合可撤销生物识别，ViT不适用。

Abstract: One common critique of biometric authentication is that if an individual's
biometric is compromised, then the individual has no recourse. The concept of
revocable biometrics was developed to address this concern. A biometric scheme
is revocable if an individual can have their current enrollment in the scheme
revoked, so that the compromised biometric template becomes worthless, and the
individual can re-enroll with a new template that has similar recognition
power. We show that modern deep CNN face matchers inherently allow for a robust
revocable biometric scheme. For a given state-of-the-art deep CNN backbone and
training set, it is possible to generate an unlimited number of distinct face
matcher models that have both (1) equivalent recognition power, and (2)
strongly incompatible biometric templates. The equivalent recognition power
extends to the point of generating impostor and genuine distributions that have
the same shape and placement on the similarity dimension, meaning that the
models can share a similarity threshold for a 1-in-10,000 false match rate. The
biometric templates from different model instances are so strongly incompatible
that the cross-instance similarity score for images of the same person is
typically lower than the same-instance similarity score for images of different
persons. That is, a stolen biometric template that is revoked is of less value
in attempting to match the re-enrolled identity than the average impostor
template. We also explore the feasibility of using a Vision Transformer (ViT)
backbone-based face matcher in the revocable biometric system proposed in this
work and demonstrate that it is less suitable compared to typical ResNet-based
deep CNN backbones.

</details>


### [452] [SWA-SOP: Spatially-aware Window Attention for Semantic Occupancy Prediction in Autonomous Driving](https://arxiv.org/abs/2506.18785)
*Helin Cao,Rafael Materla,Sven Behnke*

Key words: 语义占用预测（SOP）、注意力机制、LiDAR、相机、自动驾驶

TL;DR: 提出了一种名为Spatially-aware Window Attention (SWA)的新机制，通过将局部空间上下文引入注意力计算，改进了语义占用预测（SOP）的性能，尤其是在稀疏或遮挡区域。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的基于transformer的SOP方法在注意力计算中缺乏对空间结构的显式建模，导致几何感知能力有限，在稀疏或遮挡区域表现不佳。

Method: 提出了Spatially-aware Window Attention (SWA)机制，将局部空间上下文纳入注意力计算。

Result: SWA显著提高了场景完成度，在LiDAR和相机两种模式的SOP基准测试中均取得了最先进的成果。

Conclusion: SWA通过显式建模空间结构，有效提升了语义占用预测的性能，并显示出跨模态的通用性。

Abstract: Perception systems in autonomous driving rely on sensors such as LiDAR and
cameras to perceive the 3D environment. However, due to occlusions and data
sparsity, these sensors often fail to capture complete information. Semantic
Occupancy Prediction (SOP) addresses this challenge by inferring both occupancy
and semantics of unobserved regions. Existing transformer-based SOP methods
lack explicit modeling of spatial structure in attention computation, resulting
in limited geometric awareness and poor performance in sparse or occluded
areas. To this end, we propose Spatially-aware Window Attention (SWA), a novel
mechanism that incorporates local spatial context into attention. SWA
significantly improves scene completion and achieves state-of-the-art results
on LiDAR-based SOP benchmarks. We further validate its generality by
integrating SWA into a camera-based SOP pipeline, where it also yields
consistent gains across modalities.

</details>


### [453] [OC-SOP: Enhancing Vision-Based 3D Semantic Occupancy Prediction by Object-Centric Awareness](https://arxiv.org/abs/2506.18798)
*Helin Cao,Sven Behnke*

Key words: 自主驾驶,语义占用预测,对象中心,检测分支,SemanticKITTI

TL;DR: 论文提出了一种对象为中心的语义占用预测框架OC-SOP，通过结合高层次的物体检测信息，显著提升了前景物体的预测准确性，并在SemanticKITTI上实现了最新性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 自主驾驶感知因环境中的遮挡和不完整场景数据面临挑战，传统相机方法对所有类别一视同仁且主要依赖局部特征，导致预测效果不佳，尤其是动态前景物体。

Method: 提出OC-SOP框架，通过检测分支提取高层次对象中心线索，并将其整合到语义占用预测流程中。

Result: 显著提升了前景物体的预测准确性，在SemanticKITTI数据集上实现了最新性能。

Conclusion: OC-SOP框架结合对象中心线索能有效提升语义占用预测的准确性，尤其在动态前景物体上表现突出。

Abstract: Autonomous driving perception faces significant challenges due to occlusions
and incomplete scene data in the environment. To overcome these issues, the
task of semantic occupancy prediction (SOP) is proposed, which aims to jointly
infer both the geometry and semantic labels of a scene from images. However,
conventional camera-based methods typically treat all categories equally and
primarily rely on local features, leading to suboptimal predictions, especially
for dynamic foreground objects. To address this, we propose Object-Centric SOP
(OC-SOP), a framework that integrates high-level object-centric cues extracted
via a detection branch into the semantic occupancy prediction pipeline. This
object-centric integration significantly enhances the prediction accuracy for
foreground objects and achieves state-of-the-art performance among all
categories on SemanticKITTI.

</details>


### [454] [TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting](https://arxiv.org/abs/2506.18862)
*Zhongbin Guo,Yuhao Wang,Ping Jian,Xinyue Chen,Wei Peng,Ertai E*

Key words: 多模态大语言模型, 卫星图像, 时序分析, 未来场景生成, TAMMs

TL;DR: TAMMs是一种新型时序感知多模态模型，通过轻量级时序模块提升多模态大语言模型（MLLMs）在卫星图像时序分析和未来场景生成任务中的性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有MLLMs在卫星图像时序分析中存在细粒度时空推理能力的不足，本文旨在探索其建模复杂多模态动态的潜力。

Method: 提出TAMMs模型，结合轻量级时序模块的结构化序列编码与上下文提示，以及语义融合控制注入（SFCI）机制，实现时序一致的图像合成。

Result: TAMMs在时序变化理解和未来图像预测任务中均优于现有MLLM基线模型。

Conclusion: 精心设计的时序推理和语义融合能显著提升MLLMs在时空理解任务中的潜力。

Abstract: Satellite image time-series analysis demands fine-grained spatial-temporal
reasoning, which remains a challenge for existing multimodal large language
models (MLLMs). In this work, we study the capabilities of MLLMs on a novel
task that jointly targets temporal change understanding and future scene
generation, aiming to assess their potential for modeling complex multimodal
dynamics over time. We propose TAMMs, a Temporal-Aware Multimodal Model for
satellite image change understanding and forecasting, which enhances frozen
MLLMs with lightweight temporal modules for structured sequence encoding and
contextual prompting. To guide future image generation, TAMMs introduces a
Semantic-Fused Control Injection (SFCI) mechanism that adaptively combines
high-level semantic reasoning and structural priors within an enhanced
ControlNet. This dual-path conditioning enables temporally consistent and
semantically grounded image synthesis. Experiments demonstrate that TAMMs
outperforms strong MLLM baselines in both temporal change understanding and
future image forecasting tasks, highlighting how carefully designed temporal
reasoning and semantic fusion can unlock the full potential of MLLMs for
spatio-temporal understanding.

</details>


### [455] [OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation](https://arxiv.org/abs/2506.18866)
*Qijun Gan,Ruizi Yang,Jianke Zhu,Shaofei Xue,Steven Hoi*

Key words: 音频驱动,全身动画,唇同步,LoRA,视频生成

TL;DR: OmniAvatar是一个音频驱动的全身视频生成模型，通过多层级音频嵌入策略提升唇同步和自然动作，并支持文本控制。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法主要关注面部动作，难以实现全身动画的自然同步和细粒度控制，OmniAvatar旨在解决这些问题。

Method: 采用像素级多层级音频嵌入策略捕捉音频特征，结合LoRA训练方法保留基础模型的提示控制能力。

Result: 实验表明OmniAvatar在面部和半身视频生成上优于现有模型，支持多种场景的视频生成。

Conclusion: OmniAvatar显著提升了音频驱动视频生成的同步性和可控性，为多领域应用提供可能。

Abstract: Significant progress has been made in audio-driven human animation, while
most existing methods focus mainly on facial movements, limiting their ability
to create full-body animations with natural synchronization and fluidity. They
also struggle with precise prompt control for fine-grained generation. To
tackle these challenges, we introduce OmniAvatar, an innovative audio-driven
full-body video generation model that enhances human animation with improved
lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise
multi-hierarchical audio embedding strategy to better capture audio features in
the latent space, enhancing lip-syncing across diverse scenes. To preserve the
capability for prompt-driven control of foundation models while effectively
incorporating audio features, we employ a LoRA-based training approach.
Extensive experiments show that OmniAvatar surpasses existing models in both
facial and semi-body video generation, offering precise text-based control for
creating videos in various domains, such as podcasts, human interactions,
dynamic scenes, and singing. Our project page is
https://omni-avatar.github.io/.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [456] [LIGHTHOUSE: Fast and precise distance to shoreline calculations from anywhere on earth](https://arxiv.org/abs/2506.18842)
*Patrick Beukema,Henry Herzog,Yawen Zhang,Hunter Pitelka,Favyen Bastani*

Key words: 海岸距离计算, 全球海岸数据集, 计算机视觉, Lighthouse, 实时应用

TL;DR: 提出了一种新的数据集和算法，用于从地球上任何地方快速高效地计算海岸距离。现有全球海岸数据集分辨率较低（1-4公里），限制其应用。新数据集利用卫星图像和计算机视觉技术，提供10米分辨率的全球海岸线数据，精度提升100倍以上。

<details>
  <summary>Details</summary>

Main category: cs.DB

Motivation: 现有全球海岸数据集分辨率不足，无法满足高精度需求，因此开发更高精度的数据集和高效查询方法。

Method: 结合公开卫星图像和计算机视觉技术，生成10米分辨率的全球海岸线数据集；提出新的计算库Lighthouse，采用分层迭代地理空间层次化地形导向统一搜索引擎技术，实现高效查询。

Result: Lighthouse在仅需1 CPU和2 GB内存的情况下，实现毫秒级在线推理，适合实时应用和资源受限环境。

Conclusion: 新数据集和算法显著提升了海岸距离计算的精度和效率，适用于实时和资源受限的应用场景。

Abstract: We introduce a new dataset and algorithm for fast and efficient coastal
distance calculations from Anywhere on Earth (AoE). Existing global coastal
datasets are only available at coarse resolution (e.g. 1-4 km) which limits
their utility. Publicly available satellite imagery combined with computer
vision enable much higher precision. We provide a global coastline dataset at
10 meter resolution, a 100+ fold improvement in precision over existing data.
To handle the computational challenge of querying at such an increased scale,
we introduce a new library: Layered Iterative Geospatial Hierarchical
Terrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both
exceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM
to achieve millisecond online inference, making it well suited for real-time
applications in resource-constrained environments.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [457] [Greedy Selection under Independent Increments: A Toy Model Analysis](https://arxiv.org/abs/2506.17941)
*Huitao Yang*

Key words: 迭代选择问题,贪心算法,独立增量,多阶段淘汰

TL;DR: 该论文研究了N个独立同分布的离散时间随机过程的迭代选择问题，证明了在强独立性假设下，贪心选择策略是最优的。

<details>
  <summary>Details</summary>

Main category: math.PR

Motivation: 探讨在多阶段淘汰设置中，贪心启发式算法的合理性及其在高维应用中的潜在价值。

Method: 在每个阶段基于观测值保留固定数量的过程，分析贪心选择策略的最优性。

Result: 证明了贪心选择策略在强独立性假设下是最优的，适用于选择最终最大值过程。

Conclusion: 贪心启发式算法在多阶段选择问题中具有合理性，可作为高维应用中的简化示例。

Abstract: We study an iterative selection problem over N i.i.d. discrete-time
stochastic processes with independent increments. At each stage, a fixed number
of processes are retained based on their observed values. Under this simple
model, we prove that the optimal strategy for selecting the final maximum-value
process is to apply greedy selection at each stage. While the result relies on
strong independence assumptions, it offers a clean justification for greedy
heuristics in multi-stage elimination settings and may serve as a toy example
for understanding related algorithms in high-dimensional applications.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [458] [Advanced Game-Theoretic Frameworks for Multi-Agent AI Challenges: A 2025 Outlook](https://arxiv.org/abs/2506.17348)
*Pavel Malinovskiy*

Key words: 博弈论,人工智能,多代理系统,战略互动,动态联盟

TL;DR: 该论文重新探讨了高级博弈论范式如何为2025年左右下一代AI挑战提供理论基础，重点是动态联盟形成、语言效用、破坏风险和部分可观察性。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 研究旨在为AI领域提供理论工具，以应对不确定和部分对抗环境中的战略互动问题。

Method: 结合数学形式化、模拟和编码方案，研究多代理AI系统在复杂环境中的适应与谈判。

Result: 提出了一套方法，包括重复博弈、贝叶斯更新对抗检测和道德框架嵌入收益结构。

Conclusion: 该研究为AI研究者提供了处理战略互动的强大理论工具。

Abstract: This paper presents a substantially reworked examination of how advanced
game-theoretic paradigms can serve as a foundation for the next-generation
challenges in Artificial Intelligence (AI), forecasted to arrive in or around
2025. Our focus extends beyond traditional models by incorporating dynamic
coalition formation, language-based utilities, sabotage risks, and partial
observability. We provide a set of mathematical formalisms, simulations, and
coding schemes that illustrate how multi-agent AI systems may adapt and
negotiate in complex environments. Key elements include repeated games,
Bayesian updates for adversarial detection, and moral framing within payoff
structures. This work aims to equip AI researchers with robust theoretical
tools for aligning strategic interaction in uncertain, partially adversarial
contexts.

</details>


### [459] [Towards Zero-Shot Coordination between Teams of Agents: The N-XPlay Framework](https://arxiv.org/abs/2506.17560)
*Ava Abderezaei,Chi-Hui Lin,Joseph Miceli,Naren Sivagnanadasan,Stéphane Aroca-Ouellette,Jake Brawer,Alessandro Roncone*

Key words: 零样本协调、多团队系统、Overcooked、N-XPlay

TL;DR: 论文提出了N-XPlay方法，用于解决多团队系统中的零样本协调问题，通过扩展Overcooked基准在多智能体场景中评估，结果表明N-XPlay优于Self-Play。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 现有零样本协调方法仅限于两个智能体的交互，无法反映多团队系统的复杂性，因此需要新方法扩展至多智能体场景。

Method: 提出了N-XPlay方法，扩展Overcooked基准至多智能体场景，并在二、三、五玩家的环境中与Self-Play进行比较。

Result: N-XPlay在多团队协调中表现优于Self-Play，能更好平衡团队内和团队间的协作。

Conclusion: N-XPlay是多团队系统中零样本协调的有效方法。

Abstract: Zero-shot coordination (ZSC) -- the ability to collaborate with unfamiliar
partners -- is essential to making autonomous agents effective teammates.
Existing ZSC methods evaluate coordination capabilities between two agents who
have not previously interacted. However, these scenarios do not reflect the
complexity of real-world multi-agent systems, where coordination often involves
a hierarchy of sub-groups and interactions between teams of agents, known as
Multi-Team Systems (MTS). To address this gap, we first introduce N-player
Overcooked, an N-agent extension of the popular two-agent ZSC benchmark,
enabling evaluation of ZSC in N-agent scenarios. We then propose N-XPlay for
ZSC in N-agent, multi-team settings. Comparison against Self-Play across two-,
three- and five-player Overcooked scenarios, where agents are split between an
``ego-team'' and a group of unseen collaborators shows that agents trained with
N-XPlay are better able to simultaneously balance ``intra-team'' and
``inter-team'' coordination than agents trained with SP.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [460] [Quantum-Hybrid Support Vector Machines for Anomaly Detection in Industrial Control Systems](https://arxiv.org/abs/2506.17824)
*Tyler Cultice,Md. Saif Hassan Onim,Annarita Giani,Himanshu Thapliyal*

Key words: 工业控制系统, 异常检测, 量子支持向量机, 量子计算, 关键基础设施

TL;DR: 量子混合支持向量机（QSVM）在工业控制系统（ICS）异常检测中表现优于传统方法，F1分数提升13.3%，且对噪声具有强鲁棒性。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 工业控制系统的敏感数据对关键基础设施的安全至关重要，量子计算的高效特征空间有望提升异常检测能力。

Method: 使用三种CPS数据集测试参数化的量子混合支持向量机，并基于真实IBMQ硬件模拟噪声影响。

Result: QSVM的F1分数提高13.3%，噪声误差仅为0.98%，分类指标平均下降1.57%，且核-目标对齐度提升91.023%。

Conclusion: QSVM在ICS异常检测中具有显著优势，可能带来“量子优势”，提升关键基础设施安全性。

Abstract: Sensitive data captured by Industrial Control Systems (ICS) play a large role
in the safety and integrity of many critical infrastructures. Detection of
anomalous or malicious data, or Anomaly Detection (AD), with machine learning
is one of many vital components of cyberphysical security. Quantum kernel-based
machine learning methods have shown promise in identifying complex anomalous
behavior by leveraging the highly expressive and efficient feature spaces of
quantum computing. This study focuses on the parameterization of Quantum Hybrid
Support Vector Machines (QSVMs) using three popular datasets from
Cyber-Physical Systems (CPS). The results demonstrate that QSVMs outperform
traditional classical kernel methods, achieving 13.3% higher F1 scores.
Additionally, this research investigates noise using simulations based on real
IBMQ hardware, revealing a maximum error of only 0.98% in the QSVM kernels.
This error results in an average reduction of 1.57% in classification metrics.
Furthermore, the study found that QSVMs show a 91.023% improvement in
kernel-target alignment compared to classical methods, indicating a potential
"quantum advantage" in anomaly detection for critical infrastructures. This
effort suggests that QSVMs can provide a substantial advantage in anomaly
detection for ICS, ultimately enhancing the security and integrity of critical
infrastructures.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [461] [Wisdom of Crowds Through Myopic Self-Confidence Adaptation](https://arxiv.org/abs/2506.18195)
*Giacomo Como,Fabio Fagnani,Anton Proskurnikov*

Key words: 群体智慧, French-DeGroot动力学, 博弈论, 多目标优化, Nash均衡

TL;DR: 本文研究了群体智慧的机制，通过非贝叶斯学习规则（如French-DeGroot动力学）实现分布式意见聚合，并分析了代理在多目标优化问题中的博弈行为。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 探索如何在群体决策中减少个体估计的方差，以及如何通过优化影响权重实现更准确的世界状态估计。

Method: 使用French-DeGroot动力学模型，代理通过迭代分布式平均更新意见，并通过博弈论方法优化权重分配。

Result: 表征了Pareto前沿和Nash均衡集，证明了异步最佳响应动力学收敛于严格Nash均衡。

Conclusion: 通过优化影响权重和多代理博弈行为，可以显著提高群体决策的准确性。

Abstract: The wisdom of crowds is an umbrella term for phenomena suggesting that the
collective judgment or decision of a large group can be more accurate than the
individual judgments or decisions of the group members. A well-known example
illustrating this concept is the competition at a country fair described by
Galton, where the median value of the individual guesses about the weight of an
ox resulted in an astonishingly accurate estimate of the actual weight. This
phenomenon resembles classical results in probability theory and relies on
independent decision-making. The accuracy of the group's final decision can be
significantly reduced if the final agents' opinions are driven by a few
influential agents.
  In this paper, we consider a group of agents who initially possess
uncorrelated and unbiased noisy measurements of a common state of the world.
Assume these agents iteratively update their estimates according to a simple
non-Bayesian learning rule, commonly known in mathematical sociology as the
French-DeGroot dynamics or iterative opinion pooling. As a result of this
iterative distributed averaging process, each agent arrives at an asymptotic
estimate of the state of the world, with the variance of this estimate
determined by the matrix of weights the agents assign to each other. Every
agent aims at minimizing the variance of her asymptotic estimate of the state
of the world; however, such variance is also influenced by the weights
allocated by other agents. To achieve the best possible estimate, the agents
must then solve a game-theoretic, multi-objective optimization problem defined
by the available sets of influence weights. We characterize both the Pareto
frontier and the set of Nash equilibria in the resulting game. Additionally, we
examine asynchronous best-response dynamics for the group of agents and prove
their convergence to the set of strict Nash equilibria.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [462] [BrainSymphony: A Transformer-Driven Fusion of fMRI Time Series and Structural Connectivity](https://arxiv.org/abs/2506.18314)
*Moein Khajehnejad,Forough Habibollahi,Adeel Razi*

Key words: 神经影像、基础模型、多模态、Transformer、轻量级

TL;DR: BrainSymphony是一种轻量级、参数高效的神经影像基础模型，通过多模态架构和高效表示蒸馏，在小规模公共数据集上实现先进性能，超越大型模型。

<details>
  <summary>Details</summary>

Main category: q-bio.QM

Motivation: 现有神经影像基础模型通常过大且数据密集，难以广泛应用。BrainSymphony旨在提供一种高效、轻量的替代方案。

Method: 模型通过并行空间和时间Transformer流处理功能MRI数据，并由Perceiver模块统一表示；同时使用有符号图Transformer建模扩散MRI的结构连通性，通过自适应融合门整合多模态表示。

Result: 在多种下游任务中表现优异，并在独特的外部裸盖菇素神经影像数据集中揭示了新的脑动态机制。

Conclusion: BrainSymphony证明轻量级多模态模型可超越大型模型，为计算神经科学提供了更易访问且强大的工具。

Abstract: Existing foundation models for neuroimaging are often prohibitively large and
data-intensive. We introduce BrainSymphony, a lightweight, parameter-efficient
foundation model that achieves state-of-the-art performance while being
pre-trained on significantly smaller public datasets. BrainSymphony's strong
multimodal architecture processes functional MRI data through parallel spatial
and temporal transformer streams, which are then efficiently distilled into a
unified representation by a Perceiver module. Concurrently, it models
structural connectivity from diffusion MRI using a novel signed graph
transformer to encode the brain's anatomical structure. These powerful,
modality-specific representations are then integrated via an adaptive fusion
gate. Despite its compact design, our model consistently outperforms larger
models on a diverse range of downstream benchmarks, including classification,
prediction, and unsupervised network identification tasks. Furthermore, our
model revealed novel insights into brain dynamics using attention maps on a
unique external psilocybin neuroimaging dataset (pre- and post-administration).
BrainSymphony establishes that architecturally-aware, multimodal models can
surpass their larger counterparts, paving the way for more accessible and
powerful research in computational neuroscience.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [463] [When concept-based XAI is imprecise: Do people distinguish between generalisations and misrepresentations?](https://arxiv.org/abs/2506.17936)
*Romy Müller*

Key words: 可解释AI, 概念泛化, 铁路安全, 用户评价, 特征匹配

TL;DR: 研究探讨了概念可解释AI（C-XAI）在铁路安全场景中的应用，发现人们对概念泛化的识别和评价能力有限，尤其是对不相关特征的泛化容易误解为不精确。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 理解AI模型内部表示对复杂任务（如安全评估）至关重要，但人们是否能识别和欣赏概念泛化尚不明确。

Method: 通过实验模拟铁路安全场景，参与者评估AI基于相似图像片段（概念）的决策解释，比较不同特征匹配的概念效果。

Result: 参与者对不相关特征的泛化评价较低，接近系统性错误表示；但对相关特征的不精确高度敏感。

Conclusion: 人们难以自发识别概念泛化，可能无法通过C-XAI判断AI是否深入理解复杂情境。

Abstract: Concept-based explainable artificial intelligence (C-XAI) can help reveal the
inner representations of AI models. Understanding these representations is
particularly important in complex tasks like safety evaluation. Such tasks rely
on high-level semantic information (e.g., about actions) to make decisions
about abstract categories (e.g., whether a situation is dangerous). In this
context, it may desirable for C-XAI concepts to show some variability,
suggesting that the AI is capable of generalising beyond the concrete details
of a situation. However, it is unclear whether people recognise and appreciate
such generalisations and can distinguish them from other, less desirable forms
of imprecision. This was investigated in an experimental railway safety
scenario. Participants evaluated the performance of a simulated AI that
evaluated whether traffic scenes involving people were dangerous. To explain
these decisions, the AI provided concepts in the form of similar image
snippets. These concepts differed in their match with the classified image,
either regarding a highly relevant feature (i.e., relation to tracks) or a less
relevant feature (i.e., actions). Contrary to the hypotheses, concepts that
generalised over less relevant features led to ratings that were lower than for
precisely matching concepts and comparable to concepts that systematically
misrepresented these features. Conversely, participants were highly sensitive
to imprecisions in relevant features. These findings cast doubts on whether
people spontaneously recognise generalisations. Accordingly, they might not be
able to infer from C-XAI concepts whether AI models have gained a deeper
understanding of complex situations.

</details>


### [464] [Conceptualization, Operationalization, and Measurement of Machine Companionship: A Scoping Review](https://arxiv.org/abs/2506.18119)
*Jaime Banks,Zhixin Li*

Key words: 机器伴侣, PRISMA综述, 人机交互, 自发性, 协调性

TL;DR: 该论文通过系统性综述探讨了机器伴侣（MC）这一概念，提出了一种基于文献的定义。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 研究动机是从学术角度正式定义和测量机器伴侣的概念，以补充现有研究中这一概念的模糊性。

Method: 采用PRISMA指南的系统性范围综述方法，分析了71篇相关文献（2017-2025年）。

Result: 研究发现，MC的定义和测量变量高度多样化，最终提出了一个文献支持的定义：MC是一种自发的、协调的、随时间发展且主观积极的人机连接。

Conclusion: 研究结论强调了MC作为一种正式概念的重要性，并为其未来研究提供了理论基础。

Abstract: The notion of machine companions has long been embedded in
social-technological imaginaries. Recent advances in AI have moved those media
musings into believable sociality manifested in interfaces, robotic bodies, and
devices. Those machines are often referred to colloquially as "companions" yet
there is little careful engagement of machine companionship (MC) as a formal
concept or measured variable. This PRISMA-guided scoping review systematically
samples, surveys, and synthesizes current scholarly works on MC (N = 71;
2017-2025), to that end. Works varied widely in considerations of MC according
to guiding theories, dimensions of a-priori specified properties (subjectively
positive, sustained over time, co-active, autotelic), and in measured concepts
(with more than 50 distinct measured variables). WE ultimately offer a
literature-guided definition of MC as an autotelic, coordinated connection
between human and machine that unfolds over time and is subjectively positive.

</details>


### [465] [AI Harmonizer: Expanding Vocal Expression with a Generative Neurosymbolic Music AI System](https://arxiv.org/abs/2506.18143)
*Lancelot Blanchard,Cameron Holt,Joseph A. Paradiso*

Key words: AI Harmonizer, 和声生成, 生成式AI, 音乐技术

TL;DR: 该论文介绍了一种名为AI Harmonizer的工具，能够自动生成四部和声，无需用户手动输入和声信息。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 传统和声工具需要用户具备一定的音乐专业知识，而AI Harmonizer通过结合先进的生成式AI技术，简化了和声生成过程。

Method: 结合了最新的音高检测、声音建模技术和定制的符号音乐模型，自动为任何旋律生成丰富的和声。

Result: 成功开发了一个能够离线生成四部和声的系统，并探讨了其在表演和作曲中的应用潜力。

Conclusion: 该系统为AI辅助的声乐表演和音乐创作迈出了重要一步，未来将探索实时实现的可能性。

Abstract: Vocals harmonizers are powerful tools to help solo vocalists enrich their
melodies with harmonically supportive voices. These tools exist in various
forms, from commercially available pedals and software to custom-built systems,
each employing different methods to generate harmonies. Traditional harmonizers
often require users to manually specify a key or tonal center, while others
allow pitch selection via an external keyboard-both approaches demanding some
degree of musical expertise. The AI Harmonizer introduces a novel approach by
autonomously generating musically coherent four-part harmonies without
requiring prior harmonic input from the user. By integrating state-of-the-art
generative AI techniques for pitch detection and voice modeling with
custom-trained symbolic music models, our system arranges any vocal melody into
rich choral textures. In this paper, we present our methods, explore potential
applications in performance and composition, and discuss future directions for
real-time implementations. While our system currently operates offline, we
believe it represents a significant step toward AI-assisted vocal performance
and expressive musical augmentation. We release our implementation on GitHub.

</details>


### [466] [Two Sonification Methods for the MindCube](https://arxiv.org/abs/2506.18196)
*Fangzheng Liu,Lancelot Blanchard,Don D. Haddad,Joseph A. Paradiso*

Key words: MindCube, 音乐界面, 情绪调节, 生成式AI, 潜在空间

TL;DR: 探索MindCube作为音乐界面的潜力，提出两种映射方法（含AI与不含AI），讨论结果及未来方向。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 研究如何利用MindCube这一交互设备帮助情绪调节的音乐系统开发。

Method: 设计两种MindCube映射方法（一种含生成式AI），利用外部控制器在潜在空间中导航。

Result: 提出了生成式AI映射方法，并展示了潜在空间导航技术。

Conclusion: MindCube适合情绪调节音乐系统，未来可进一步研究其应用。

Abstract: In this work, we explore the musical interface potential of the MindCube, an
interactive device designed to study emotions. Embedding diverse sensors and
input devices, this interface resembles a fidget cube toy commonly used to help
users relieve their stress and anxiety. As such, it is a particularly
well-suited controller for musical systems that aim to help with emotion
regulation. In this regard, we present two different mappings for the MindCube,
with and without AI. With our generative AI mapping, we propose a way to infuse
meaning within a latent space and techniques to navigate through it with an
external controller. We discuss our results and propose directions for future
work.

</details>


### [467] [BRAVE: Brain-Controlled Prosthetic Arm with Voice Integration and Embodied Learning for Enhanced Mobility](https://arxiv.org/abs/2506.18749)
*Abdul Basit,Maha Nawaz,Muhammad Shafique*

Key words: 脑机接口, EEG, 假肢控制, 集成学习, 实时系统

TL;DR: BRAVE是一种结合脑电图（EEG）和语音控制的混合假肢系统，通过集成集成学习分类和人机交互纠正框架，实现了高精度和实时响应。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 解决现有基于EEG的假肢控制系统在信号噪声、分类精度和实时适应性上的挑战。

Method: BRAVE结合LSTM、CNN和随机森林的集成学习框架，采用ICA和CSP预处理信号，并整合语音识别实现模式切换。

Result: 分类准确率达到96%，响应延迟为150毫秒，适用于多用户和低功耗嵌入式部署。

Conclusion: BRAVE为非侵入式假肢控制提供了稳健且实时的解决方案。

Abstract: Non-invasive brain-computer interfaces (BCIs) have the potential to enable
intuitive control of prosthetic limbs for individuals with upper limb
amputations. However, existing EEG-based control systems face challenges
related to signal noise, classification accuracy, and real-time adaptability.
In this work, we present BRAVE, a hybrid EEG and voice-controlled prosthetic
system that integrates ensemble learning-based EEG classification with a
human-in-the-loop (HITL) correction framework for enhanced responsiveness.
Unlike traditional electromyography (EMG)-based prosthetic control, BRAVE aims
to interpret EEG-driven motor intent, enabling movement control without
reliance on residual muscle activity. To improve classification robustness,
BRAVE combines LSTM, CNN, and Random Forest models in an ensemble framework,
achieving a classification accuracy of 96% across test subjects. EEG signals
are preprocessed using a bandpass filter (0.5-45 Hz), Independent Component
Analysis (ICA) for artifact removal, and Common Spatial Pattern (CSP) feature
extraction to minimize contamination from electromyographic (EMG) and
electrooculographic (EOG) signals. Additionally, BRAVE incorporates automatic
speech recognition (ASR) to facilitate intuitive mode switching between
different degrees of freedom (DOF) in the prosthetic arm. The system operates
in real time, with a response latency of 150 ms, leveraging Lab Streaming Layer
(LSL) networking for synchronized data acquisition. The system is evaluated on
an in-house fabricated prosthetic arm and on multiple participants highlighting
the generalizability across users. The system is optimized for low-power
embedded deployment, ensuring practical real-world application beyond
high-performance computing environments. Our results indicate that BRAVE offers
a promising step towards robust, real-time, non-invasive prosthetic control.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [468] [PaceLLM: Brain-Inspired Large Language Models for Long-Context Understanding](https://arxiv.org/abs/2506.17310)
*Kangcong Li,Peng Ye,Chongjun Tu,Lin Zhang,Chunfeng Song,Jiamin Wu,Tao Yang,Qihao Zheng,Tao Chen*

Key words: 大型语言模型, 长上下文处理, 持久活动机制, 皮层专家聚类, 脑启发优化

TL;DR: PaceLLM通过模拟大脑的工作记忆和皮层模块化，提出持久活动机制和皮层专家聚类，显著提升大语言模型的长上下文处理能力。

<details>
  <summary>Details</summary>

Main category: q-bio.NC

Motivation: 大型语言模型（LLMs）在长上下文处理中因信息衰减和语义碎片化表现不佳，受到大脑工作记忆和神经模块化的启发，研究旨在优化这一问题。

Method: 提出PaceLLM，包含持久活动机制（动态存储和重用关键激活状态）和皮层专家聚类（重组权重为语义模块）。

Result: PaceLLM在多项长上下文任务中表现优异，如在Needle-In-A-Haystack测试中扩展到200K tokens。

Conclusion: PaceLLM是一种通用的脑启发优化方法，可提升模型的长上下文性能和可解释性，无需结构大改动。

Abstract: While Large Language Models (LLMs) demonstrate strong performance across
domains, their long-context capabilities are limited by transient neural
activations causing information decay and unstructured feed-forward network
(FFN) weights leading to semantic fragmentation. Inspired by the brain's
working memory and cortical modularity, we propose PaceLLM, featuring two
innovations: (1) a Persistent Activity (PA) Mechanism that mimics prefrontal
cortex (PFC) neurons' persistent firing by introducing an activation-level
memory bank to dynamically retrieve, reuse, and update critical FFN states,
addressing contextual decay; and (2) Cortical Expert (CE) Clustering that
emulates task-adaptive neural specialization to reorganize FFN weights into
semantic modules, establishing cross-token dependencies and mitigating
fragmentation. Extensive evaluations show that PaceLLM achieves 6% improvement
on LongBench's Multi-document QA and 12.5-17.5% performance gains on
Infinite-Bench tasks, while extending measurable context length to 200K tokens
in Needle-In-A-Haystack (NIAH) tests. This work pioneers brain-inspired LLM
optimization and is complementary to other works. Besides, it can be
generalized to any model and enhance their long-context performance and
interpretability without structural overhauls.

</details>


### [469] [Challenges in Grounding Language in the Real World](https://arxiv.org/abs/2506.17375)
*Peter Lindes,Kaoutar Skiker*

Key words: 人工智能, 语言理解, 物理机器人, 认知代理, 大语言模型

TL;DR: 该论文探讨了如何构建一个让人类与物理机器人通过自然语言协作的系统，整合了认知代理的能力与大语言模型的语言能力，并提出了初步实现路径。

<details>
  <summary>Details</summary>

Main category: q-bio.NC

Motivation: 为了实现人工智能的长期目标，即通过自然语言让人与物理机器人协作，论文旨在解决这一过程中的挑战。

Method: 通过整合具有交互任务学习能力的认知代理与大型语言模型的语言能力，提出了一种解决方案。

Result: 论文提出了一种初步实现该方法的路径。

Conclusion: 通过结合认知代理和大型语言模型的能力，为解决人机自然语言协作问题提供了新思路。

Abstract: A long-term goal of Artificial Intelligence is to build a language
understanding system that allows a human to collaborate with a physical robot
using language that is natural to the human. In this paper we highlight some of
the challenges in doing this, and propose a solution that integrates the
abilities of a cognitive agent capable of interactive task learning in a
physical robot with the linguistic abilities of a large language model. We also
point the way to an initial implementation of this approach.

</details>


### [470] [Sequence-to-Sequence Models with Attention Mechanistically Map to the Architecture of Human Memory Search](https://arxiv.org/abs/2506.17424)
*Nikolaus Salvatore,Qiong Zhang*

Key words: 上下文, 记忆模型, 神经机器翻译, 认知模型, RNN

TL;DR: 该论文探讨了上下文在人类记忆中的作用，并通过神经机器翻译模型与人类记忆模型的比较，提出了一种新的可解释的记忆搜索模型。

<details>
  <summary>Details</summary>

Main category: q-bio.NC

Motivation: 研究旨在理解人类记忆架构为何依赖上下文，并通过神经机器翻译模型的机制与人类记忆模型的相似性，揭示上下文的功能角色。

Method: 使用基于RNN和注意力机制的序列到序列模型，将其与人类记忆模型（CMR）进行对比，并实现为认知模型。

Result: 模型能够有效模拟人类行为模式，并揭示了记忆搜索性能如何由不同模型组件相互作用产生。

Conclusion: 神经机器翻译模型与人类记忆模型的趋同为理解上下文功能提供了新途径，并提出了一种新的记忆建模方法。

Abstract: Past work has long recognized the important role of context in guiding how
humans search their memory. While context-based memory models can explain many
memory phenomena, it remains unclear why humans develop such architectures over
possible alternatives in the first place. In this work, we demonstrate that
foundational architectures in neural machine translation -- specifically,
recurrent neural network (RNN)-based sequence-to-sequence models with attention
-- exhibit mechanisms that directly correspond to those specified in the
Context Maintenance and Retrieval (CMR) model of human memory. Since neural
machine translation models have evolved to optimize task performance, their
convergence with human memory models provides a deeper understanding of the
functional role of context in human memory, as well as presenting new ways to
model human memory. Leveraging this convergence, we implement a neural machine
translation model as a cognitive model of human memory search that is both
interpretable and capable of capturing complex dynamics of learning. We show
that our model accounts for both averaged and optimal human behavioral patterns
as effectively as context-based memory models. Further, we demonstrate
additional strengths of the proposed model by evaluating how memory search
performance emerges from the interaction of different model components.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [471] [QUST_NLP at SemEval-2025 Task 7: A Three-Stage Retrieval Framework for Monolingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2506.17272)
*Youzheng Liu,Jiyan Liu,Xiaoman Xu,Taihang Wang,Yimin Wang,Ye Jiang*

Key words: SemEval-2025, 事实核查, 检索框架, 重排序, 加权投票

TL;DR: 本文介绍了QUST_NLP团队在SemEval-2025任务7中的参与，提出了一种用于事实核查陈述检索的三阶段框架，并取得了优异的成绩。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 为了解决事实核查陈述检索的问题，QUST_NLP团队提出了一种高效的检索方法。

Method: 采用三阶段框架：1.评估并选择最佳检索模型；2.使用多个重排序模型优化候选结果；3.加权投票确定最终结果。

Result: 在单语言和多语言任务中分别排名第5和第7。

Conclusion: 该方法在事实核查陈述检索中表现良好，团队公开了系统代码。

Abstract: This paper describes the participation of QUST_NLP in the SemEval-2025 Task
7. We propose a three-stage retrieval framework specifically designed for
fact-checked claim retrieval. Initially, we evaluate the performance of several
retrieval models and select the one that yields the best results for candidate
retrieval. Next, we employ multiple re-ranking models to enhance the candidate
results, with each model selecting the Top-10 outcomes. In the final stage, we
utilize weighted voting to determine the final retrieval outcomes. Our approach
achieved 5th place in the monolingual track and 7th place in the crosslingual
track. We release our system code at:
https://github.com/warmth27/SemEval2025_Task7

</details>


### [472] [Chunk Twice, Embed Once: A Systematic Study of Segmentation and Representation Trade-offs in Chemistry-Aware Retrieval-Augmented Generation](https://arxiv.org/abs/2506.17277)
*Mahmoud Amiri,Thomas Bocklitz*

Key words: RAG, 化学, 分块策略, 嵌入模型, QuestChemRetrieval

TL;DR: 本文首次系统评估了化学领域RAG系统的文档分块策略和嵌入模型，提出了性能和效率的优化方法。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 针对化学领域RAG系统的文档分块和表示方法缺乏探索的问题，研究旨在优化系统设计。

Method: 评估了25种分块配置和48种嵌入模型，使用化学专用基准测试。

Result: 递归令牌分块（R100-0）和优化嵌入模型（如Nomic）表现最佳。

Conclusion: 研究提供了化学领域RAG系统的实用指南，并公开了数据集和框架。

Abstract: Retrieval-Augmented Generation (RAG) systems are increasingly vital for
navigating the ever-expanding body of scientific literature, particularly in
high-stakes domains such as chemistry. Despite the promise of RAG, foundational
design choices -- such as how documents are segmented and represented -- remain
underexplored in domain-specific contexts. This study presents the first
large-scale, systematic evaluation of chunking strategies and embedding models
tailored to chemistry-focused RAG systems. We investigate 25 chunking
configurations across five method families and evaluate 48 embedding models on
three chemistry-specific benchmarks, including the newly introduced
QuestChemRetrieval dataset. Our results reveal that recursive token-based
chunking (specifically R100-0) consistently outperforms other approaches,
offering strong performance with minimal resource overhead. We also find that
retrieval-optimized embeddings -- such as Nomic and Intfloat E5 variants --
substantially outperform domain-specialized models like SciBERT. By releasing
our datasets, evaluation framework, and empirical benchmarks, we provide
actionable guidelines for building effective and efficient chemistry-aware RAG
systems.

</details>


### [473] [CORONA: A Coarse-to-Fine Framework for Graph-based Recommendation with Large Language Models](https://arxiv.org/abs/2506.17281)
*Junze Chen,Xinjie Yang,Cheng Yang,Junfei Bao,Zeyuan Guo,Yawen Li,Chuan Shi*

Key words: 推荐系统, 大语言模型, 图神经网络, 候选过滤, 推理

TL;DR: 论文提出了一种结合大语言模型（LLM）和图神经网络（GNN）的推荐系统框架CORONA，通过在候选过滤过程中利用LLM的推理能力，显著提高了推荐性能。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 现有研究未能充分利用LLM在候选过滤阶段的潜力，导致推荐系统性能不佳，论文旨在通过LLM的推理能力优化这一过程。

Method: 提出CORONA框架，分三阶段：1) LLM基于用户画像进行偏好推理；2) LLM结合历史数据进一步缩小候选范围；3) GNN从子图中提取高阶信息生成最终推荐。

Result: 实验表明，CORONA在召回率和NDCG指标上平均相对提高了18.6%和18.4%，达到最先进的性能。

Conclusion: CORONA有效结合了LLM的推理能力和GNN的高阶信息提取能力，显著提升了推荐系统的性能。

Abstract: Recommender systems (RSs) are designed to retrieve candidate items a user
might be interested in from a large pool. A common approach is using graph
neural networks (GNNs) to capture high-order interaction relationships. As
large language models (LLMs) have shown strong capabilities across domains,
researchers are exploring their use to enhance recommendation. However, prior
work limits LLMs to re-ranking results or dataset augmentation, failing to
utilize their power during candidate filtering - which may lead to suboptimal
performance. Instead, we propose to leverage LLMs' reasoning abilities during
the candidate filtering process, and introduce Chain Of Retrieval ON grAphs
(CORONA) to progressively narrow down the range of candidate items on
interaction graphs with the help of LLMs: (1) First, LLM performs preference
reasoning based on user profiles, with the response serving as a query to
extract relevant users and items from the interaction graph as
preference-assisted retrieval; (2) Then, using the information retrieved in the
previous step along with the purchase history of target user, LLM conducts
intent reasoning to help refine an even smaller interaction subgraph as
intent-assisted retrieval; (3) Finally, we employ a GNN to capture high-order
collaborative filtering information from the extracted subgraph, performing
GNN-enhanced retrieval to generate the final recommendation results. The
proposed framework leverages the reasoning capabilities of LLMs during the
retrieval process, while seamlessly integrating GNNs to enhance overall
recommendation performance. Extensive experiments on various datasets and
settings demonstrate that our proposed CORONA achieves state-of-the-art
performance with an 18.6% relative improvement in recall and an 18.4% relative
improvement in NDCG on average.

</details>


### [474] [SlimRAG: Retrieval without Graphs via Entity-Aware Context Selection](https://arxiv.org/abs/2506.17288)
*Jiale Zhang,Jiaxiang Chen,Zhucong Li,Jie Ding,Kui Zhao,Zenglin Xu,Xin Pang,Yinghui Xu*

Key words: 检索增强生成, 实体感知机制, SlimRAG, 轻量级框架, RITU

TL;DR: SlimRAG提出了一种轻量级的检索增强生成框架，通过实体感知机制替代传统的基于图的方法，提高了检索效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 传统的基于图的检索增强生成系统存在结构复杂和检索不精确的问题，需要高效的替代方案。

Method: SlimRAG通过构建实体到文本块的紧凑表，并基于语义嵌入进行检索，避免了复杂的图遍历和边构建。

Result: 实验表明，SlimRAG在多个QA基准测试中优于基于图和扁平化的基线方法，减少了索引大小和检索内容的冗余。

Conclusion: SlimRAG证明了无结构、以实体为中心的上下文选择在检索增强生成中的价值。

Abstract: Retrieval-Augmented Generation (RAG) enhances language models by
incorporating external knowledge at inference time. However, graph-based RAG
systems often suffer from structural overhead and imprecise retrieval: they
require costly pipelines for entity linking and relation extraction, yet
frequently return subgraphs filled with loosely related or tangential content.
This stems from a fundamental flaw -- semantic similarity does not imply
semantic relevance. We introduce SlimRAG, a lightweight framework for retrieval
without graphs. SlimRAG replaces structure-heavy components with a simple yet
effective entity-aware mechanism. At indexing time, it constructs a compact
entity-to-chunk table based on semantic embeddings. At query time, it
identifies salient entities, retrieves and scores associated chunks, and
assembles a concise, contextually relevant input -- without graph traversal or
edge construction. To quantify retrieval efficiency, we propose Relative Index
Token Utilization (RITU), a metric measuring the compactness of retrieved
content. Experiments across multiple QA benchmarks show that SlimRAG
outperforms strong flat and graph-based baselines in accuracy while reducing
index size and RITU (e.g., 16.31 vs. 56+), highlighting the value of
structure-free, entity-centric context selection. The code will be released
soon. https://github.com/continue-ai-company/SlimRAG

</details>


### [475] [Enhancing Document Retrieval in COVID-19 Research: Leveraging Large Language Models for Hidden Relation Extraction](https://arxiv.org/abs/2506.18311)
*Hoang-An Trieu,Dinh-Truong Do,Chau Nguyen,Vu Tran,Minh Le Nguyen*

Key words: COVID-19, 文献检索, 大语言模型, Covrelex-SE

TL;DR: 提出了一种基于大语言模型（LLMs）的方法，改进COVID-19相关文献检索系统（Covrelex-SE），通过提取未标注文献中的隐藏关系提升检索质量。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: COVID-19疫情期间涌现大量文献，需高效检索系统以便快速应对突发疫情。

Method: 利用大语言模型（LLMs）提取未标注文献中的隐藏关系，弥补当前解析工具的不足。

Result: 改进的检索系统（Covrelex-SE）能提供更高质量的搜索结果。

Conclusion: 结合LLMs的Covrelex-SE系统在突发疫情下能更高效地为研究者提供有用信息。

Abstract: In recent years, with the appearance of the COVID-19 pandemic, numerous
publications relevant to this disease have been issued. Because of the massive
volume of publications, an efficient retrieval system is necessary to provide
researchers with useful information if an unexpected pandemic happens so
suddenly, like COVID-19. In this work, we present a method to help the
retrieval system, the Covrelex-SE system, to provide more high-quality search
results. We exploited the power of the large language models (LLMs) to extract
the hidden relationships inside the unlabeled publication that cannot be found
by the current parsing tools that the system is using. Since then, help the
system to have more useful information during retrieval progress.

</details>


### [476] [Team LA at SCIDOCA shared task 2025: Citation Discovery via relation-based zero-shot retrieval](https://arxiv.org/abs/2506.18316)
*Trieu An,Long Nguyen,Minh Le Nguyen*

Key words: 引文预测、关系特征提取、大型语言模型、SCIDOCA 2025

TL;DR: 该论文提出了一种结合关系特征提取和大型语言模型（LLM）的引文预测系统，用于解决候选摘要相似度高导致的引文识别问题，并在SCIDOCA 2025数据集上验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 由于摘要段落较长且候选摘要之间相似度高，准确预测引文具有挑战性。

Method: 使用关系特征提取初步筛选候选摘要，再利用LLM从候选子集中精确识别引文。

Result: 在SCIDOCA 2025数据集上验证了系统的有效性。

Conclusion: 提出的方法能够有效解决引文预测中相似度高的问题。

Abstract: The Citation Discovery Shared Task focuses on predicting the correct citation
from a given candidate pool for a given paragraph. The main challenges stem
from the length of the abstract paragraphs and the high similarity among
candidate abstracts, making it difficult to determine the exact paper to cite.
To address this, we develop a system that first retrieves the top-k most
similar abstracts based on extracted relational features from the given
paragraph. From this subset, we leverage a Large Language Model (LLM) to
accurately identify the most relevant citation. We evaluate our framework on
the training dataset provided by the SCIDOCA 2025 organizers, demonstrating its
effectiveness in citation prediction.

</details>


### [477] [Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models](https://arxiv.org/abs/2506.17580)
*Sajratul Y. Rubaiat,Hasan M. Jamil*

Key words: WISE, 知识提取, LLM, 结构化工作流程, 科学文献

TL;DR: WISE是一种智能科学知识提取系统，通过结构化工作流程提取、提炼和排序查询特定知识，解决了传统搜索和通用LLM的不足。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 科学文献的快速增长使得知识提取和综合变得困难，传统搜索和通用LLM无法提供深入或最新的答案。

Method: WISE采用树状架构和LLM技术，动态评分和排序，自适应停止标准，以减少处理开销并提供高质量知识。

Result: 实验表明，WISE在HBB基因相关疾病任务中减少了80%以上的处理文本，并显著提高了召回率和独特性。

Conclusion: WISE能够高效提取和综合科学知识，适用于多领域应用，如药物发现和材料科学。

Abstract: The exponential growth of scientific literature challenges researchers
extracting and synthesizing knowledge. Traditional search engines return many
sources without direct, detailed answers, while general-purpose LLMs may offer
concise responses that lack depth or omit current information. LLMs with search
capabilities are also limited by context window, yielding short, incomplete
answers. This paper introduces WISE (Workflow for Intelligent Scientific
Knowledge Extraction), a system addressing these limits by using a structured
workflow to extract, refine, and rank query-specific knowledge. WISE uses an
LLM-powered, tree-based architecture to refine data, focusing on query-aligned,
context-aware, and non-redundant information. Dynamic scoring and ranking
prioritize unique contributions from each source, and adaptive stopping
criteria minimize processing overhead. WISE delivers detailed, organized
answers by systematically exploring and synthesizing knowledge from diverse
sources. Experiments on HBB gene-associated diseases demonstrate WISE reduces
processed text by over 80% while achieving significantly higher recall over
baselines like search engines and other LLM-based approaches. ROUGE and BLEU
metrics reveal WISE's output is more unique than other systems, and a novel
level-based metric shows it provides more in-depth information. We also explore
how the WISE workflow can be adapted for diverse domains like drug discovery,
material science, and social science, enabling efficient knowledge extraction
and synthesis from unstructured scientific papers and web sources.

</details>


### [478] [Reinforcing User Interest Evolution in Multi-Scenario Learning for recommender systems](https://arxiv.org/abs/2506.17682)
*Zhijian Feng,Wenhao Zheng,Xuanji Xiao*

Key words: 推荐系统,多场景学习,强化学习,Double Q-learning,对比学习

TL;DR: 提出一种强化学习方法，通过多场景建模用户兴趣演变，提升推荐系统的多场景学习效果。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 解决用户在不同场景下兴趣不一致的问题，统一建模多场景推荐系统。

Method: 采用Double Q-learning增强预测准确性，并利用Q值优化对比学习损失。

Result: 实验表明，该方法在多场景推荐任务中优于现有技术。

Conclusion: 为多场景建模提供了新视角，并指出了未来研究方向。

Abstract: In real-world recommendation systems, users would engage in variety
scenarios, such as homepages, search pages, and related recommendation pages.
Each of these scenarios would reflect different aspects users focus on.
However, the user interests may be inconsistent in different scenarios, due to
differences in decision-making processes and preference expression. This
variability complicates unified modeling, making multi-scenario learning a
significant challenge. To address this, we propose a novel reinforcement
learning approach that models user preferences across scenarios by modeling
user interest evolution across multiple scenarios. Our method employs Double
Q-learning to enhance next-item prediction accuracy and optimizes contrastive
learning loss using Q-value to make model performance better. Experimental
results demonstrate that our approach surpasses state-of-the-art methods in
multi-scenario recommendation tasks. Our work offers a fresh perspective on
multi-scenario modeling and highlights promising directions for future
research.

</details>


### [479] [CARTS: Collaborative Agents for Recommendation Textual Summarization](https://arxiv.org/abs/2506.17765)
*Jiao Chen,Kehui Yao,Reza Yousefi Maragheh,Kai Zhao,Jianpeng Xu,Jason Cho,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Key words: 推荐系统，文本摘要，多代理LLM，CARTS

TL;DR: 提出了一种名为CARTS的多代理LLM框架，用于推荐系统中的结构化文本摘要，显著提高了标题相关性和用户参与度。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 现有的推荐系统需要高度相关的文本摘要，但现有方法无法直接适用于推荐系统。

Method: CARTS采用多代理框架，分为三个阶段：GAG、迭代精炼和仲裁，逐步提取特征、优化标题并选择最终结果。

Result: 实验和A/B测试表明，CARTS在标题相关性和用户参与度上显著优于基线方法。

Conclusion: CARTS为推荐系统的文本摘要提供了有效的解决方案。

Abstract: Current recommendation systems often require some form of textual data
summarization, such as generating concise and coherent titles for product
carousels or other grouped item displays. While large language models have
shown promise in NLP domains for textual summarization, these approaches do not
directly apply to recommendation systems, where explanations must be highly
relevant to the core features of item sets, adhere to strict word limit
constraints. In this paper, we propose CARTS (Collaborative Agents for
Recommendation Textual Summarization), a multi-agent LLM framework designed for
structured summarization in recommendation systems. CARTS decomposes the task
into three stages-Generation Augmented Generation (GAG), refinement circle, and
arbitration, where successive agent roles are responsible for extracting
salient item features, iteratively refining candidate titles based on relevance
and length feedback, and selecting the final title through a collaborative
arbitration process. Experiments on large-scale e-commerce data and live A/B
testing show that CARTS significantly outperforms single-pass and
chain-of-thought LLM baselines, delivering higher title relevance and improved
user engagement metrics.

</details>


### [480] [Expanding Relevance Judgments for Medical Case-based Retrieval Task with Multimodal LLMs](https://arxiv.org/abs/2506.17782)
*Catarina Pires,Sérgio Nunes,Luís Filipe Teixeira*

Key words: 信息检索, 多模态大语言模型, 相关性标注, 医学检索, 自动化评估

TL;DR: 利用多模态大语言模型（MLLM）扩展信息检索系统中的相关性标注，显著提升了数据集的规模和标注效率。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 传统信息检索系统依赖人工标注相关性，成本高且耗时，而多模态大语言模型提供了一种高效替代方案，尤其在需要分析图文信息的复杂领域（如医学检索）中更具潜力。

Method: 使用Gemini 1.5 Pro模型在ImageCLEFmed 2013任务中，通过结构化提示策略（包括二进制评分、指令评估和少样本学习）模拟人工标注，扩展相关性标注数据集。

Result: MLLM标注与人工标注的Cohen's Kappa一致性得分达0.6，数据集规模从15,028标注扩展到558,653，相关标注从4.72%增至5,950。

Conclusion: MLLM能够显著提升相关性标注的规模和效率，为医学和多模态信息检索任务的评估提供了新方向。

Abstract: Evaluating Information Retrieval (IR) systems relies on high-quality manual
relevance judgments (qrels), which are costly and time-consuming to obtain.
While pooling reduces the annotation effort, it results in only partially
labeled datasets. Large Language Models (LLMs) offer a promising alternative to
reducing reliance on manual judgments, particularly in complex domains like
medical case-based retrieval, where relevance assessment requires analyzing
both textual and visual information. In this work, we explore using a
Multimodal Large Language Model (MLLM) to expand relevance judgments, creating
a new dataset of automated judgments. Specifically, we employ Gemini 1.5 Pro on
the ImageCLEFmed 2013 case-based retrieval task, simulating human assessment
through an iteratively refined, structured prompting strategy that integrates
binary scoring, instruction-based evaluation, and few-shot learning. We
systematically experimented with various prompt configurations to maximize
agreement with human judgments. To evaluate agreement between the MLLM and
human judgments, we use Cohen's Kappa, achieving a substantial agreement score
of 0.6, comparable to inter-annotator agreement typically observed in
multimodal retrieval tasks. Starting from the original 15,028 manual judgments
(4.72% relevant) across 35 topics, our MLLM-based approach expanded the dataset
by over 37x to 558,653 judgments, increasing relevant annotations to 5,950. On
average, each medical case query received 15,398 new annotations, with
approximately 99% being non-relevant, reflecting the high sparsity typical in
this domain. Our results demonstrate the potential of MLLMs to scale relevance
judgment collection, offering a promising direction for supporting retrieval
evaluation in medical and multimodal IR tasks.

</details>


### [481] [A Framework for Generating Conversational Recommendation Datasets from Behavioral Interactions](https://arxiv.org/abs/2506.17285)
*Vinaik Chhetri,Yousaf Reza,Moghis Fereidouni,Srijata Maji,Umar Farooq,AB Siddique*

Key words: 推荐系统,对话推荐,协同过滤,大语言模型,ConvRecStudio

TL;DR: 该论文提出了一种结合协同过滤与对话推荐系统的框架ConvRecStudio，通过LLM模拟真实对话，提升推荐个性化。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 传统推荐系统缺乏即时交互能力，而对话推荐系统则缺乏协同信号。结合两者可提升推荐效果，但缺乏大规模真实对话数据。

Method: ConvRecStudio采用三阶段流程：时间剖面分析、语义对话计划生成和多轮对话模拟，利用LLM生成真实对话数据。

Result: 在三个领域生成超过12K对话，并通过评估证实对话的自然性和一致性。联合编码模型在推荐指标上优于基线。

Conclusion: ConvRecStudio成功结合了两大推荐范式，并通过模型验证了其有效性。

Abstract: Modern recommendation systems typically follow two complementary paradigms:
collaborative filtering, which models long-term user preferences from
historical interactions, and conversational recommendation systems (CRS), which
interact with users in natural language to uncover immediate needs. Each
captures a different dimension of user intent. While CRS models lack
collaborative signals, leading to generic or poorly personalized suggestions,
traditional recommenders lack mechanisms to interactively elicit immediate
needs. Unifying these paradigms promises richer personalization but remains
challenging due to the lack of large-scale conversational datasets grounded in
real user behavior. We present ConvRecStudio, a framework that uses large
language models (LLMs) to simulate realistic, multi-turn dialogs grounded in
timestamped user-item interactions and reviews. ConvRecStudio follows a
three-stage pipeline: (1) Temporal Profiling, which constructs user profiles
and community-level item sentiment trajectories over fine-grained aspects; (2)
Semantic Dialog Planning, which generates a structured plan using a DAG of
flexible super-nodes; and (3) Multi-Turn Simulation, which instantiates the
plan using paired LLM agents for the user and system, constrained by
executional and behavioral fidelity checks. We apply ConvRecStudio to three
domains -- MobileRec, Yelp, and Amazon Electronics -- producing over 12K
multi-turn dialogs per dataset. Human and automatic evaluations confirm the
naturalness, coherence, and behavioral grounding of the generated
conversations. To demonstrate utility, we build a cross-attention transformer
model that jointly encodes user history and dialog context, achieving gains in
Hit@K and NDCG@K over baselines using either signal alone or naive fusion.
Notably, our model achieves a 10.9% improvement in Hit@1 on Yelp over the
strongest baseline.

</details>


### [482] [Recommendation systems in e-commerce applications with machine learning methods](https://arxiv.org/abs/2506.17287)
*Aneta Poniszewska-Maranda,Magdalena Pakula,Bozena Borowska*

Key words: 电子商务,推荐系统,机器学习,协作过滤,基于内容的过滤

TL;DR: 本文综述了电子商务推荐系统的现状、挑战及机器学习方法的有效性，分析了38篇相关文献。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 探讨机器学习在提升电子商务推荐系统效率、个性化和可扩展性方面的潜力，识别当前趋势与挑战。

Method: 采用系统性文献综述（SLR）方法，分析了2013至2025年的38篇出版物，对比评估了协作过滤、基于内容的过滤及混合模型的性能。

Result: 研究发现不同机器学习方法在解决电子商务挑战中的效果各异，部分方法表现更优。

Conclusion: 机器学习方法显著提升了电子商务推荐系统的性能，但仍需进一步研究以优化其在实际应用中的表现。

Abstract: E-commerce platforms are increasingly reliant on recommendation systems to
enhance user experience, retain customers, and, in most cases, drive sales. The
integration of machine learning methods into these systems has significantly
improved their efficiency, personalization, and scalability. This paper aims to
highlight the current trends in e-commerce recommendation systems, identify
challenges, and evaluate the effectiveness of various machine learning methods
used, including collaborative filtering, content-based filtering, and hybrid
models. A systematic literature review (SLR) was conducted, analyzing 38
publications from 2013 to 2025. The methods used were evaluated and compared to
determine their performance and effectiveness in addressing e-commerce
challenges.

</details>


### [483] [A GenAI System for Improved FAIR Independent Biological Database Integration](https://arxiv.org/abs/2506.17934)
*Syed N. Sakib,Kallol Naha,Sajratul Y. Rubaiat,Hasan M. Jamil*

Key words: FAIRBridge, Linked Open Data, FAIR原则, 自然语言查询, AI, 科学数据处理

TL;DR: FAIRBridge是一个基于自然语言的查询处理系统，旨在帮助科学家轻松发现、访问和查询生物数据库，即使这些数据不符合FAIR原则。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 当前生命科学研究中，数据源的动态性和复杂性导致研究人员在数据查询和集成上面临巨大挑战，而FAIR原则的采用仍存在效率和质量问题。

Method: FAIRBridge利用AI技术解析查询意图，映射到相关数据库，并生成可执行查询，同时包含工具来应对低质量查询处理。

Result: FAIRBridge提供了一个用户友好的自动化假设测试平台，显著提升了科学数据的集成和处理效率。

Conclusion: FAIRBridge为研究人员提供了一个强大的新工具，促进了科学查询的进步。

Abstract: Life sciences research increasingly requires identifying, accessing, and
effectively processing data from an ever-evolving array of information sources
on the Linked Open Data (LOD) network. This dynamic landscape places a
significant burden on researchers, as the quality of query responses depends
heavily on the selection and semantic integration of data sources --processes
that are often labor-intensive, error-prone, and costly. While the adoption of
FAIR (Findable, Accessible, Interoperable, and Reusable) data principles has
aimed to address these challenges, barriers to efficient and accurate
scientific data processing persist.
  In this paper, we introduce FAIRBridge, an experimental natural
language-based query processing system designed to empower scientists to
discover, access, and query biological databases, even when they are not
FAIR-compliant. FAIRBridge harnesses the capabilities of AI to interpret query
intents, map them to relevant databases described in scientific literature, and
generate executable queries via intelligent resource access plans. The system
also includes robust tools for mitigating low-quality query processing,
ensuring high fidelity and responsiveness in the information delivered.
  FAIRBridge's autonomous query processing framework enables users to explore
alternative data sources, make informed choices at every step, and leverage
community-driven crowd curation when needed. By providing a user-friendly,
automated hypothesis-testing platform in natural English, FAIRBridge
significantly enhances the integration and processing of scientific data,
offering researchers a powerful new tool for advancing their inquiries.

</details>


### [484] [PERSCEN: Learning Personalized Interaction Pattern and Scenario Preference for Multi-Scenario Matching](https://arxiv.org/abs/2506.18382)
*Haotong Du,Yaqing Wang,Fei Xiong,Lei Shao,Ming Liu,Hao Gu,Quanming Yao,Zhen Wang*

Key words: 多场景推荐, 用户特定建模, 图神经网络, 向量量化, 个性化推荐

TL;DR: PERSCEN是一种创新方法，通过用户特定建模和多场景匹配，结合图神经网络和向量量化技术，显著提升了推荐系统的个性化效果和效率。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 当前多场景推荐方法忽视了用户特定建模，限制了个性化用户表示的生成，导致推荐效果不佳。

Method: PERSCEN构建了基于用户特征的用户特定特征图，并采用轻量级图神经网络捕捉高阶交互模式，同时使用向量量化技术提取场景感知偏好。

Result: 实验表明，PERSCEN在性能上优于现有方法，且在计算成本和效率方面具有良好的平衡。

Conclusion: PERSCEN通过个性化建模和高效信息传输，显著提升了多场景推荐的实用性和效果。

Abstract: With the expansion of business scales and scopes on online platforms,
multi-scenario matching has become a mainstream solution to reduce maintenance
costs and alleviate data sparsity. The key to effective multi-scenario
recommendation lies in capturing both user preferences shared across all
scenarios and scenario-aware preferences specific to each scenario. However,
existing methods often overlook user-specific modeling, limiting the generation
of personalized user representations. To address this, we propose PERSCEN, an
innovative approach that incorporates user-specific modeling into
multi-scenario matching. PERSCEN constructs a user-specific feature graph based
on user characteristics and employs a lightweight graph neural network to
capture higher-order interaction patterns, enabling personalized extraction of
preferences shared across scenarios. Additionally, we leverage vector
quantization techniques to distil scenario-aware preferences from users'
behavior sequence within individual scenarios, facilitating user-specific and
scenario-aware preference modeling. To enhance efficient and flexible
information transfer, we introduce a progressive scenario-aware gated linear
unit that allows fine-grained, low-latency fusion. Extensive experiments
demonstrate that PERSCEN outperforms existing methods. Further efficiency
analysis confirms that PERSCEN effectively balances performance with
computational cost, ensuring its practicality for real-world industrial
systems.

</details>


### [485] [LettinGo: Explore User Profile Generation for Recommendation System](https://arxiv.org/abs/2506.18309)
*Lu Wang,Di Zhang,Fangkai Yang,Pu Zhao,Jianfeng Liu,Yuefeng Zhan,Hao Sun,Qingwei Lin,Weiwei Deng,Dongmei Zhang,Feng Sun,Qi Zhang*

Key words: 用户画像,推荐系统,大语言模型,直接偏好优化

TL;DR: 论文提出了一个新型用户画像框架LettinGo，利用LLM生成多样化和自适应的用户画像，通过DPO优化任务性能，显著提升了推荐系统的准确性、灵活性和上下文感知能力。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 传统基于嵌入的用户画像缺乏可解释性和适应性，而现有文本画像方法受限于固定格式，无法充分捕捉用户行为的多样性。

Method: LettinGo框架分为三步：利用多个LLM探索多样画像；基于推荐任务效果评估画像质量；通过DPO优化生成过程。

Result: 实验表明，该框架显著提升了推荐准确性、灵活性和上下文感知。

Conclusion: LettinGo作为新一代推荐系统的关键创新，改进了画像生成方式。

Abstract: User profiling is pivotal for recommendation systems, as it transforms raw
user interaction data into concise and structured representations that drive
personalized recommendations. While traditional embedding-based profiles lack
interpretability and adaptability, recent advances with large language models
(LLMs) enable text-based profiles that are semantically richer and more
transparent. However, existing methods often adhere to fixed formats that limit
their ability to capture the full diversity of user behaviors. In this paper,
we introduce LettinGo, a novel framework for generating diverse and adaptive
user profiles. By leveraging the expressive power of LLMs and incorporating
direct feedback from downstream recommendation tasks, our approach avoids the
rigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ
Direct Preference Optimization (DPO) to align the profile generator with
task-specific performance, ensuring that the profiles remain adaptive and
effective. LettinGo operates in three stages: (1) exploring diverse user
profiles via multiple LLMs, (2) evaluating profile quality based on their
impact in recommendation systems, and (3) aligning the profile generation
through pairwise preference data derived from task performance. Experimental
results demonstrate that our framework significantly enhances recommendation
accuracy, flexibility, and contextual awareness. This work enhances profile
generation as a key innovation for next-generation recommendation systems.

</details>


### [486] [Bias vs Bias -- Dawn of Justice: A Fair Fight in Recommendation Systems](https://arxiv.org/abs/2506.18327)
*Tahsin Alamgir Kheya,Mohamed Reda Bouadjenek,Sunil Aryal*

Key words: 推荐系统, 公平性, 重新排序, 多敏感属性, 社会偏见

TL;DR: 本文提出了一种公平性感知的重新排序方法，用于减轻推荐系统中不同类别商品的偏见，覆盖多敏感属性（如性别、年龄、职业），并在实验中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 推荐系统对用户体验至关重要，但现有研究忽视了部分商品类别的偏见问题，且多集中于二元敏感属性。本文旨在解决这些问题。

Method: 提出了一种公平性感知的重新排序方法，利用现有偏见纠正不同人口群体推荐中的差异，覆盖多个敏感属性。

Result: 在三个真实数据集上的实验表明，该方法能有效减轻社会偏见，同时几乎没有性能下降。

Conclusion: 该方法能全面解决多敏感属性的偏见问题，且不影响推荐系统的性能。

Abstract: Recommendation systems play a crucial role in our daily lives by impacting
user experience across various domains, including e-commerce, job
advertisements, entertainment, etc. Given the vital role of such systems in our
lives, practitioners must ensure they do not produce unfair and imbalanced
recommendations. Previous work addressing bias in recommendations overlooked
bias in certain item categories, potentially leaving some biases unaddressed.
Additionally, most previous work on fair re-ranking focused on binary-sensitive
attributes. In this paper, we address these issues by proposing a
fairness-aware re-ranking approach that helps mitigate bias in different
categories of items. This re-ranking approach leverages existing biases to
correct disparities in recommendations across various demographic groups. We
show how our approach can mitigate bias on multiple sensitive attributes,
including gender, age, and occupation. We experimented on three real-world
datasets to evaluate the effectiveness of our re-ranking scheme in mitigating
bias in recommendations. Our results show how this approach helps mitigate
social bias with little to no degradation in performance.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [487] [Tutorial: $\varphi$-Transductions in OpenFst via the Gallic Semiring](https://arxiv.org/abs/2506.17942)
*Marco Cognetta,Cyril Allauzen*

Key words: OpenFst, $φ$-transitions, Gallic semiring, MaxMatch, WordPiece

TL;DR: 本文介绍了如何在OpenFst库中通过Gallic半环正确实现$φ$-转换，并以MaxMatch（WordPiece）分词算法为例进行演示。

<details>
  <summary>Details</summary>

Main category: cs.FL

Motivation: 由于OpenFst库的实现限制，$φ$-转换不能直接用于换能器，本文旨在通过其他功能（Gallic半环）解决这一问题。

Method: 利用OpenFst提供的Gallic半环功能，实现了$φ$-转换，并以MaxMatch（WordPiece）分词算法为实例展示。

Result: 成功展示了如何在OpenFst中正确实现$φ$-转换，并提供了配套的代码示例。

Conclusion: 通过Gallic半环可以绕过OpenFst对$φ$-转换的限制，且方法具有实用性和可操作性。

Abstract: OpenFst, a popular finite-state transducer library, supports
$\varphi$-transitions but, due to an implementation constraint, they cannot be
used with transducers in a straightforward way.
  In this short tutorial, we describe how one can use other functionality
provided by OpenFst (namely, the Gallic semiring) to correctly implement
$\varphi$-transductions and demonstrate it by implementing the MaxMatch
(WordPiece) tokenization algorithm (Devlin et al., 2019; Song et al., 2021).
Accompanying self-contained code examples are provided.
https://www.openfst.org/twiki/pub/Contrib/FstContrib/phi_transduction_tutorial_code.tgz

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [488] [Bayesian Inference for Left-Truncated Log-Logistic Distributions for Time-to-event Data Analysis](https://arxiv.org/abs/2506.17852)
*Fahad Mostafa,Md Rejuan Haque,Md Mostafijur Rahman,Farzana Nasrin*

Key words: 贝叶斯估计, 左截断对数逻辑分布, Metropolis-Hastings算法

TL;DR: 论文提出了一种贝叶斯方法，用于估计左截断对数逻辑（LTLL）分布的参数，通过马尔可夫链蒙特卡罗采样实现，适用于时间到事件数据的建模。

<details>
  <summary>Details</summary>

Main category: stat.ME

Motivation: 参数估计是统计建模的基础步骤，尤其是贝叶斯方法可以结合先验信念与观测数据，为截断数据提供更稳健的估计。

Method: 采用贝叶斯方法，使用Metropolis-Hastings算法进行后验推断，通过模拟和实际数据验证其有效性。

Result: 贝叶斯估计在截断分布中提供了更稳定可靠的参数估计，尤其是当似然曲面不规则时。

Conclusion: 贝叶斯推断在截断分布参数估计中具有优势，特别是在时间到事件数据分析中。

Abstract: Parameter estimation is a foundational step in statistical modeling, enabling
us to extract knowledge from data and apply it effectively. Bayesian estimation
of parameters incorporates prior beliefs with observed data to infer
distribution parameters probabilistically and robustly. Moreover, it provides
full posterior distributions, allowing uncertainty quantification and
regularization, especially useful in small or truncated samples. Utilizing the
left-truncated log-logistic (LTLL) distribution is particularly well-suited for
modeling time-to-event data where observations are subject to a known lower
bound such as precipitation data and cancer survival times. In this paper, we
propose a Bayesian approach for estimating the parameters of the LTLL
distribution with a fixed truncation point \( x_L > 0 \). Given a random
variable \( X \sim LL(\alpha, \beta; x_L) \), where \( \alpha > 0 \) is the
scale parameter and \( \beta > 0 \) is the shape parameter, the likelihood
function is derived based on a truncated sample \( X_1, X_2, \dots, X_N \) with
\( X_i > x_L \). We assume independent prior distributions for the parameters,
and the posterior inference is conducted via Markov Chain Monte Carlo sampling,
specifically using the Metropolis-Hastings algorithm to obtain posterior
estimates \( \hat{\alpha} \) and \( \hat{\beta} \). Through simulation studies
and real-world applications, we demonstrate that Bayesian estimation provides
more stable and reliable parameter estimates, particularly when the likelihood
surface is irregular due to left truncation. The results highlight the
advantages of Bayesian inference outperform the estimation of parameter
uncertainty in truncated distributions for time to event data analysis.

</details>


### [489] [GRASP: Grouped Regression with Adaptive Shrinkage Priors](https://arxiv.org/abs/2506.18092)
*Shu Yu Tew,Daniel F. Schmidt,Mario Boley*

Key words: GRASP, NBP prior, grouped regression, sparsity, Bayesian framework

TL;DR: GRASP是一个基于正态Beta素数（NBP）先验的贝叶斯框架，用于分组预测回归，通过调整超参数控制稀疏性，避免了复杂的层次结构。

<details>
  <summary>Details</summary>

Main category: stat.ME

Motivation: 解决分组预测回归中稀疏性和信号噪声比变化的问题，提供更灵活和高效的稀疏性控制方法。

Method: 使用NBP先验直接控制尾部行为，避免复杂层次结构；提出新的框架量化组内收缩参数的相关性，并开发高效的Metropolis-Hastings采样器。

Result: 实证结果表明GRASP在不同稀疏性和信号噪声比的回归问题中表现稳健且灵活。

Conclusion: GRASP通过直接控制尾部行为和量化组内相关性，提供了一种高效且灵活的分组回归方法。

Abstract: We introduce GRASP, a simple Bayesian framework for regression with grouped
predictors, built on the normal beta prime (NBP) prior. The NBP prior is an
adaptive generalization of the horseshoe prior with tunable hyperparameters
that control tail behavior, enabling a flexible range of sparsity, from strong
shrinkage to ridge-like regularization. Unlike prior work that introduced the
group inverse-gamma gamma (GIGG) prior by decomposing the NBP prior into
structured hierarchies, we show that directly controlling the tails is
sufficient without requiring complex hierarchical constructions. Extending the
non-tail adaptive grouped half-Cauchy hierarchy of Xu et al., GRASP assigns the
NBP prior to both local and group shrinkage parameters allowing adaptive
sparsity within and across groups. A key contribution of this work is a novel
framework to explicitly quantify correlations among shrinkage parameters within
a group, providing deeper insights into grouped shrinkage behavior. We also
introduce an efficient Metropolis-Hastings sampler for hyperparameter
estimation. Empirical results on simulated and real-world data demonstrate the
robustness and versatility of GRASP across grouped regression problems with
varying sparsity and signal-to-noise ratios.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [490] [LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research](https://arxiv.org/abs/2506.17335)
*Shuo Yan,Ruochen Li,Ziming Luo,Zimu Wang,Daoyang Li,Liqiang Jing,Kaiyu He,Peilin Wu,George Michalopoulos,Yue Zhang,Ziyang Zhang,Mian Zhang,Zhiyu Chen,Xinya Du*

Key words: LLM代理, 代码复现, NLP, 科学推理, LMR-BENCH

TL;DR: LLM代理在科学发现中展示潜力，但在从NLP研究论文中复现代码的任务上仍受限。LMR-BENCH基准用于系统评估LLM代理在此任务上的能力，发现最先进模型在科学推理和代码合成方面仍有局限。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 填补LLM代理在复现代码任务中的能力评估空白，尤其是NLP领域的复杂推理挑战。

Method: 提出了LMR-BENCH基准，包含28个任务，来源于23篇顶级NLP论文。实验中提供了论文、带掩码函数的代码库和实现指令，评估模型性能。

Result: 最先进模型在科学推理和代码合成方面仍存在显著局限性，无法完全自主复现科研代码。

Conclusion: 当前LLM代理在复现代码任务中能力有限，需进一步改进科学推理和代码合成能力。

Abstract: Large language model (LLM) agents have demonstrated remarkable potential in
advancing scientific discovery. However, their capability in the fundamental
yet crucial task of reproducing code from research papers, especially in the
NLP domain, remains underexplored. This task includes unique complex reasoning
challenges in the intellectual synthesis of abstract concepts and the
comprehension of code repositories with interdependent files. Motivated by this
gap, we present LMR-BENCH, a benchmark designed to systematically evaluate the
capability of LLM agents on code reproduction from Language Modeling Research.
It consists of 28 code reproduction tasks derived from 23 research papers
published in top-tier NLP venues over the past five years, spanning nine
fundamental categories. Models are provided with a research paper, a code
repository containing one or more masked functions, and instructions for
implementing these functions. We conduct extensive experiments in standard
prompting and LLM agent settings with state-of-the-art LLMs, evaluating the
accuracy of unit tests and performing LLM-based evaluation of code correctness.
Experimental results reveal that even the most advanced models still exhibit
persistent limitations in scientific reasoning and code synthesis, highlighting
critical gaps in LLM agents' ability to autonomously reproduce scientific
research

</details>


### [491] [Re-Evaluating Code LLM Benchmarks Under Semantic Mutation](https://arxiv.org/abs/2506.17369)
*Zhiyuan Pan,Xing Hu,Xin Xia,Xiaohu Yang*

Key words: 大型语言模型, 代码基准测试, 提示敏感性, 性能评估

TL;DR: 研究探讨了代码基准测试中提示敏感性的问题，设计了实验框架并通过统计分析揭示了提示变化对模型性能的影响。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 在大型语言模型时代，代码基准测试对评估模型能力至关重要，但现有测试通常依赖单一提示模板，导致评估结果不可靠。

Method: 提出一个通用框架，修改提示模板以保持语义和结构，并在8个代码任务上对10个模型进行了100种提示变体的实验。

Result: 轻微提示变化会导致模型性能显著波动，甚至影响不同模型的排名一致性。

Conclusion: 未来设计代码基准测试时需考虑提示敏感性，以提高评估的可靠性和准确性。

Abstract: In the era of large language models (LLMs), code benchmarks have become an
important research area in software engineering and are widely used by
practitioners. These benchmarks evaluate the performance of LLMs on specific
code-related tasks, such as code understanding and generation. A critical step
in constructing code benchmarks is the design of prompts. However, as existing
code benchmarks typically rely on a single prompt template per task, they are
prone to the issue of prompt sensitivity, where minor prompt variations could
result in substantial performance variations, leading to unreliable evaluations
of model capabilities.
  While previous studies have explored prompt sensitivity, their experimental
designs and findings are limited to traditional natural language processing
(NLP) tasks. In this paper, we present an empirical study to investigate prompt
sensitivity in code benchmarks. We first propose a general framework that
modifies prompt templates in a manner that preserves both their semantics and
their structure as much as possible. Based on the framework, we conduct
extensive experiments across eight code benchmark tasks on 10 representative
open-source LLMs, with each task featuring 100 semantically similar prompt
templates. We then analyze the evaluation results using various statistical
metrics, focusing on both absolute and relative model performance. Our findings
suggest that even slight prompt variations can lead to significant shifts in
performance. Additionally, we observe that such variations can introduce
inconsistencies in the performance rankings across different models. These
insights highlight the need for considering prompt sensitivity when designing
future code benchmarks, to ensure more reliable and accurate evaluation of LLM
capabilities.

</details>


### [492] [Software Reuse in the Generative AI Era: From Cargo Cult Towards AI Native Software Engineering](https://arxiv.org/abs/2506.17937)
*Tommi Mikkonen,Antero Taivalsaari*

Key words: AI辅助开发, 软件复用, AI原生软件工程, 货物崇拜开发

TL;DR: 论文探讨了AI辅助生成软件重用在‘AI原生’软件工程中的影响，提出了相关问题，并定义了初步研究议程。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 当前软件开发正经历范式转变，AI和生成式软件复用成为核心，取代传统方法，导致类似‘货物崇拜开发’的新形式复用，需研究其问题。

Method: 分析AI辅助生成软件复用在‘AI原生’软件工程中的影响，提出相关问题，并制定研究议程。

Result: 定义了初步研究议程，呼吁解决AI辅助生成软件复用中的核心问题。

Conclusion: AI辅助生成软件复用带来新挑战，需进一步研究以解决相关问题。

Abstract: Software development is currently under a paradigm shift in which artificial
intelligence and generative software reuse are taking the center stage in
software creation. Consequently, earlier software reuse practices and methods
are rapidly being replaced by AI-assisted approaches in which developers place
their trust on code that has been generated by artificial intelligence. This is
leading to a new form of software reuse that is conceptually not all that
different from cargo cult development. In this paper we discuss the
implications of AI-assisted generative software reuse in the context of
emerging "AI native" software engineering, bring forth relevant questions, and
define a tentative research agenda and call to action for tackling some of the
central issues associated with this approach.

</details>


### [493] [Call Me Maybe: Enhancing JavaScript Call Graph Construction using Graph Neural Networks](https://arxiv.org/abs/2506.18191)
*Masudul Hasan Masud Bhuiyan,Gianluca De Stefano,Giancarlo Pellegrino,Cristian-Alexandru Staicu*

Key words: 静态分析, 调用图, 图神经网络, 链接预测, JavaScript

TL;DR: 这篇论文提出了一种基于图神经网络的链接预测方法（GRAPHIA），用于提高JavaScript调用图构建的召回率，并通过多类型边表示程序图来解决现有工具的不完全性问题。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 静态分析在发现程序错误（包括安全问题）中至关重要，但现有的JavaScript调用图构建方法存在不完整性和不准确性，导致遗漏有效边或引入错误边。因此，本文旨在通过链接预测技术改进这一过程。

Method: GRAPHIA将问题建模为全程序图上的链接预测任务，结合语法和语义边构建程序图，利用图神经网络建模代码元素间的非局部关系。方法能够从现有工具的静态边或测试中的动态边中学习，甚至支持跨项目学习。

Result: 在50个流行的JavaScript库上进行大规模评估，包含16.3万条调用边。GRAPHIA在42%的情况下将正确目标排名第一，72%的情况下排名前五，显著减少了人工分析的工作量。

Conclusion: 基于学习的方法（尤其是图神经网络）能够有效提高调用图构建的召回率，为跨过程分析提供了新思路。这是首次将GNN链接预测应用于多文件程序图的跨过程分析。

Abstract: Static analysis plays a key role in finding bugs, including security issues.
A critical step in static analysis is building accurate call graphs that model
function calls in a program. However, due to hard-to-analyze language features,
existing call graph construction algorithms for JavaScript are neither sound
nor complete. Prior work shows that even advanced solutions produce false edges
and miss valid ones. In this work, we assist these tools by identifying missed
call edges. Our main idea is to frame the problem as link prediction on full
program graphs, using a rich representation with multiple edge types. Our
approach, GRAPHIA, leverages recent advances in graph neural networks to model
non-local relationships between code elements. Concretely, we propose
representing JavaScript programs using a combination of syntactic- and
semantic-based edges. GRAPHIA can learn from imperfect labels, including static
call edges from existing tools and dynamic edges from tests, either from the
same or different projects. Because call graphs are sparse, standard machine
learning metrics like ROC are not suitable. Instead, we evaluate GRAPHIA by
ranking function definitions for each unresolved call site. We conduct a
large-scale evaluation on 50 popular JavaScript libraries with 163K call edges
(150K static and 13K dynamic). GRAPHIA builds program graphs with 6.6M
structural and 386K semantic edges. It ranks the correct target as the top
candidate in over 42% of unresolved cases and within the top 5 in 72% of cases,
reducing the manual effort needed for analysis. Our results show that
learning-based methods can improve the recall of JavaScript call graph
construction. To our knowledge, this is the first work to apply GNN-based link
prediction to full multi-file program graphs for interprocedural analysis.

</details>


### [494] [Tu(r)ning AI Green: Exploring Energy Efficiency Cascading with Orthogonal Optimizations](https://arxiv.org/abs/2506.18289)
*Saurabhsingh Rajput,Mootez Saad,Tushar Sharma*

Key words: 能源效率、AI优化、可持续性、组合效应、计算密集型

TL;DR: 论文强调将能源效率作为AI设计中的首要考虑因素，通过五个AI流程阶段的优化组合，减少能耗的同时保持性能。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: AI的快速增长带来高计算需求和能源挑战，现有优化方法常为事后孤立调整，缺乏对组合效应的理解。

Method: 在数据、模型、训练、系统和推理五个AI流程阶段进行战略性选择，实现效率的级联提升。

Result: 实验显示，正交组合优化可降低能耗94.6%，同时保持95.95%的原始F1分数。

Conclusion: 该方法为可持续AI提供了平衡效率、性能和环境责任的可行框架。

Abstract: AI's exponential growth intensifies computational demands and energy
challenges. While practitioners employ various optimization techniques, that we
refer as "knobs" in this paper, to tune model efficiency, these are typically
afterthoughts and reactive ad-hoc changes applied in isolation without
understanding their combinatorial effects on energy efficiency. This paper
emphasizes on treating energy efficiency as the first-class citizen and as a
fundamental design consideration for a compute-intensive pipeline. We show that
strategic selection across five AI pipeline phases (data, model, training,
system, inference) creates cascading efficiency. Experimental validation shows
orthogonal combinations reduce energy consumption by up to $94.6$% while
preserving $95.95$% of the original F1 score of non-optimized pipelines. This
curated approach provides actionable frameworks for informed sustainable AI
that balance efficiency, performance, and environmental responsibility.

</details>


### [495] [Use Property-Based Testing to Bridge LLM Code Generation and Validation](https://arxiv.org/abs/2506.18315)
*Lehan He,Zeren Chen,Zhe Zhang,Jing Shao,Xiang Gao,Lu Sheng*

Key words: 大语言模型, 代码生成, 基于属性的测试, 测试驱动开发, 反馈循环

TL;DR: 该论文提出了一个名为Property-Generated Solver的新框架，利用基于属性的测试（PBT）来验证代码的高级别属性，而非依赖具体的输入输出示例，从而更有效地指导大语言模型生成功能正确的代码。实验结果表明，该方法在代码生成任务上有显著提升。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 尽管大语言模型在代码生成方面表现出色，但其输出在复杂编程任务中的功能正确性仍是一大挑战。传统的测试驱动开发（TDD）因高质量的测试用例稀缺或自动生成的测试偏差而效果不佳。

Method: 论文提出Property-Generated Solver框架，结合了两个协作的LLM代理：一个生成代码并进行迭代优化的Generator，以及一个管理PBT生命周期并生成语义丰富反馈的Tester。PBT作为核心验证机制，用于验证程序的高级属性或不变性。

Result: 实验表明，该方法在多个代码生成基准测试中显著提升了pass@1指标，相对传统TDD方法实现了23.1%至37.3%的提升。

Conclusion: Property-Generated Solver通过PBT验证机制，为大语言模型生成更正确且可泛化的代码提供了一种稳健的方法。

Abstract: Large Language Models (LLMs) excel at code generation, but ensuring their
outputs to be functionally correct, especially in complex programming tasks, is
a persistent challenge. While traditional Test-Driven Development (TDD) offers
a path for code refinement, its efficacy with LLMs is often undermined by the
scarcity of high-quality test cases or the pitfalls of automated test
generation, including biased tests or inaccurate output predictions that can
misdirect the correction process. This paper introduces Property-Generated
Solver, a novel framework that leverages Property-Based Testing (PBT) to
validate high-level program properties or invariants, instead of relying on
specific input-output examples. These properties are often simpler to define
and verify than directly predicting exhaustive test oracles, breaking the
"cycle of self-deception" where tests might share flaws with the code they are
meant to validate. Property-Generated Solver employs two collaborative
LLM-based agents: a Generator dedicated to code generation and iterative
refinement, and a Tester that manages the PBT life-cycle and formulate
semantically rich feedback from property violations. The resulting
comprehensive and actionable feedback then guides the Generator in its
refinement efforts. By establishing PBT as the core validation engine within
this iterative, closed-loop paradigm, Property-Generated Solver provides a
robust mechanism for steering LLMs towards more correct and generalizable code.
Extensive experimental results on multiple code generation benchmarks
demonstrate that Property-Generated Solver achieves substantial pass@1
improvements, ranging from 23.1% to 37.3% relative gains over established TDD
methods.

</details>


### [496] [The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs](https://arxiv.org/abs/2506.18403)
*Muntasir Adnan,Carlos C. N. Kuhn*

Key words: AI调试、Debugging Decay Index、代码生成、迭代优化

TL;DR: AI调试效果呈指数衰减，多数模型在2-3次尝试后失去60-80%能力。Debugging Decay Index (DDI)量化调试失效点并提出干预策略，通过适时切换调试模式提升效果。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现有AI调试能力在多次尝试后迅速衰减，影响代码生成系统的实用性。

Method: 引入DDI数学框架量化调试失效点，提出“战略新起点”方法在调试过程中切换模式。

Result: DDI揭示了当前AI调试的根本限制，并优化了迭代代码生成策略。

Conclusion: DDI为AI调试提供了首个定量框架，证明了适时干预的重要性。

Abstract: The effectiveness of AI debugging follows a predictable exponential decay
pattern; most models lose 60-80% of their debugging capability within just 2-3
attempts, despite iterative debugging being a critical capability for practical
code generation systems. We introduce the Debugging Decay Index (DDI), a
mathematical framework that quantifies when debugging becomes ineffective and
predicts intervention points. Our strategic fresh start approach shifts from
exploitation to exploration at strategic points in the debugging process,
demonstrating that well-timed interventions can rescue the effectiveness of
debugging. DDI reveals a fundamental limitation in current AI debugging and
provides the first quantitative framework for optimising iterative code
generation strategies.

</details>


### [497] [Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories](https://arxiv.org/abs/2506.18824)
*Islem Bouzenia,Michael Pradel*

Key words: LLM代理,程序修复,问题解决,行为轨迹,代理设计

TL;DR: 本研究通过分析三种LLM代理在程序修复和问题解决中的行为轨迹，揭示了成功与失败执行的关键特征，并提出了改进代理设计的建议。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 尽管LLM代理在软件工程任务中广泛应用，但其内部决策过程未充分研究，限制了对其运行机制和失败模式的理解。

Method: 通过统一三种LLM代理的交互日志，进行结构化分析和定性评估，捕捉120个轨迹和2822次LLM交互。

Result: 研究发现了区分成功与失败执行的行为模式和反模式，并提出了改进代理设计的可行建议。

Conclusion: 研究提供了对LLM代理行为的深入见解，支持未来开发更透明和鲁棒的自主软件工程代理。

Abstract: Large Language Model (LLM)-based agents are increasingly employed to automate
complex software engineering tasks such as program repair and issue resolution.
These agents operate by autonomously generating natural language thoughts,
invoking external tools, and iteratively refining their solutions. Despite
their widespread adoption, the internal decision-making processes of these
agents remain largely unexplored, limiting our understanding of their
operational dynamics and failure modes. In this paper, we present a large-scale
empirical study of the thought-action-result trajectories of three
state-of-the-art LLM-based agents: \textsc{RepairAgent},
\textsc{AutoCodeRover}, and \textsc{OpenHands}. We unify their interaction logs
into a common format, capturing 120 trajectories and 2822 LLM interactions
focused on program repair and issue resolution. Our study combines quantitative
analyses of structural properties, action patterns, and token usage with
qualitative assessments of reasoning coherence and feedback integration. We
identify key trajectory characteristics such as iteration counts and token
consumption, recurring action sequences, and the semantic coherence linking
thoughts, actions, and their results. Our findings reveal behavioral motifs and
anti-patterns that distinguish successful from failed executions, providing
actionable insights for improving agent design, including prompting strategies,
failure diagnosis, and anti-pattern detection. We release our dataset and
annotation framework to support further research on transparent and robust
autonomous software engineering agents.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [498] [Bridging Equilibrium and Kinetics Prediction with a Data-Weighted Neural Network Model of Methane Steam Reforming](https://arxiv.org/abs/2506.17224)
*Zofia Pizoń,Shinji Kimijima,Grzegorz Brus*

Key words: 氢能, 甲烷蒸汽重整, 代理模型, 人工神经网络, 动力学, 平衡态

TL;DR: 提出了一种用于统一甲烷蒸汽重整动力学和平衡态区域的代理模型，基于人工神经网络的训练数据来自实验、插值和理论数据，展示了高预测精度和适用性。

<details>
  <summary>Details</summary>

Main category: cs.CE

Motivation: 甲烷蒸汽重整是氢能生产的重要方法，现有模型通常只能适用于动力学或平衡态单一区域，限制了其应用范围。

Method: 使用人工神经网络训练综合数据集，数据包括实验数据、插值数据和理论数据，并采用数据增强和加权方法优化训练。

Result: 最优模型在预测反应后混合物组成时表现出高精度（均方误差0.000498）和强相关性（Pearson系数0.927）。

Conclusion: 代理模型在统一动力学和平衡态区域上表现优异，为甲烷蒸汽重整的设计和优化提供了有力工具。

Abstract: Hydrogen's role is growing as an energy carrier, increasing the need for
efficient production, with methane steam reforming being the most widely used
technique. This process is crucial for applications like fuel cells, where
hydrogen is converted into electricity, pushing for reactor miniaturization and
optimized process control through numerical simulations. Existing models
typically address either kinetic or equilibrium regimes, limiting their
applicability. Here we show a surrogate model capable of unifying both regimes.
An artificial neural network trained on a comprehensive dataset that includes
experimental data from kinetic and equilibrium experiments, interpolated data,
and theoretical data derived from theoretical models for each regime. Data
augmentation and assigning appropriate weights to each data type enhanced
training. After evaluating Bayesian Optimization and Random Sampling, the
optimal model demonstrated high predictive accuracy for the composition of the
post-reaction mixture under varying operating parameters, indicated by a mean
squared error of 0.000498 and strong Pearson correlation coefficients of 0.927.
The network's ability to provide continuous derivatives of its predictions
makes it particularly useful for process modeling and optimization. The results
confirm the surrogate model's robustness for simulating methane steam reforming
in both kinetic and equilibrium regimes, making it a valuable tool for design
and process optimization.

</details>


### [499] [A Study of Dynamic Stock Relationship Modeling and S&P500 Price Forecasting Based on Differential Graph Transformer](https://arxiv.org/abs/2506.18717)
*Linyue Hu,Qi Wang*

Key words: 股票预测，差分图Transformer，动态关系建模，相关性指标，聚类分析

TL;DR: 提出了一个基于动态关系建模的差分图Transformer（DGT）框架，用于股票价格预测，优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.CE

Motivation: 传统静态相关性模型无法捕捉股票间的动态关系，因此需要一个能建模非线性动态和时间变化相关性的新方法。

Method: DGT通过差分图机制将时序图结构变化整合到多头自注意力中，结合因果时间注意力捕捉全局/局部依赖关系，并评估了多种相关性指标作为空间注意力先验。

Result: DGT优于基线模型（RMSE: 0.24 vs. 0.87），Kendall's Tau全局矩阵表现最佳（MAE: 0.11）。聚类分析发现不同类股票的预测误差差异显著。

Conclusion: DGT创新性地结合动态图结构和Transformer，验证了动态关系建模的有效性，并为量化策略提供了支持。

Abstract: Stock price prediction is vital for investment decisions and risk management,
yet remains challenging due to markets' nonlinear dynamics and time-varying
inter-stock correlations. Traditional static-correlation models fail to capture
evolving stock relationships. To address this, we propose a Differential Graph
Transformer (DGT) framework for dynamic relationship modeling and price
prediction. Our DGT integrates sequential graph structure changes into
multi-head self-attention via a differential graph mechanism, adaptively
preserving high-value connections while suppressing noise. Causal temporal
attention captures global/local dependencies in price sequences. We further
evaluate correlation metrics (Pearson, Mutual Information, Spearman, Kendall's
Tau) across global/local/dual scopes as spatial-attention priors. Using 10
years of S&P 500 closing prices (z-score normalized; 64-day sliding windows),
DGT with spatial priors outperformed GRU baselines (RMSE: 0.24 vs. 0.87).
Kendall's Tau global matrices yielded optimal results (MAE: 0.11). K-means
clustering revealed "high-volatility growth" and "defensive blue-chip" stocks,
with the latter showing lower errors (RMSE: 0.13) due to stable correlations.
Kendall's Tau and Mutual Information excelled in volatile sectors. This study
innovatively combines differential graph structures with Transformers,
validating dynamic relationship modeling and identifying optimal correlation
metrics/scopes. Clustering analysis supports tailored quantitative strategies.
Our framework advances financial time-series prediction through dynamic
modeling and cross-asset interaction analysis.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [500] [UT-GraphCast Hindcast Dataset: A Global AI Forecast Archive from UT Austin for Weather and Climate Applications](https://arxiv.org/abs/2506.17453)
*Naveen Sudharsan,Manmeet Singh,Harsh Kamath,Hassan Dashtian,Clint Dawson,Zong-Liang Yang,Dev Niyogi*

Key words: GraphCast, 天气预测, 图神经网络, ECMWF ERA5, 确定性预测

TL;DR: UT GraphCast Hindcast Dataset是一个全球天气预测档案，由Google DeepMind的GraphCast模型生成，覆盖1979年至2024年，提供15天的确定性预测。

<details>
  <summary>Details</summary>

Main category: physics.geo-ph

Motivation: 提供一种高效且全面的全球天气预测数据集，以支持气候研究和天气预报工作。

Method: 使用物理信息图神经网络GraphCast模型，基于ECMWF ERA5再分析数据训练，预测多个大气和地表变量。

Result: 模型能在现代硬件上快速（1分钟内）生成15天的全球天气预报，覆盖37个垂直层次。

Conclusion: GraphCast模型为全球天气预报提供了一种高效、准确的解决方案。

Abstract: The UT GraphCast Hindcast Dataset from 1979 to 2024 is a comprehensive global
weather forecast archive generated using the Google DeepMind GraphCast
Operational model. Developed by researchers at The University of Texas at
Austin under the WCRP umbrella, this dataset provides daily 15 day
deterministic forecasts at 00UTC on an approximately 25 km global grid for a 45
year period. GraphCast is a physics informed graph neural network that was
trained on ECMWF ERA5 reanalysis. It predicts more than a dozen key atmospheric
and surface variables on 37 vertical levels, delivering a full medium range
forecast in under one minute on modern hardware.

</details>


### [501] [Pix2Geomodel: A Next-Generation Reservoir Geomodeling with Property-to-Property Translation](https://arxiv.org/abs/2506.17747)
*Abdulrahman Al-Fakih,Ardiansyah Koeshidayatullah,Nabil A. Saraih,Tapan Mukerji,Rayan Kanfar,Abdulmohsen Alali,SanLinn I. Kaka*

Key words: 地质建模, cGAN, 储层属性, Pix2Geomodel

TL;DR: 本研究提出Pix2Geomodel，一种基于Pix2Pix的条件生成对抗网络（cGAN），用于预测Groningen气田Rotliegend储层的属性（岩性、孔隙度、渗透率和水饱和度），与传统方法相比，表现出更高的精确度和地质真实性。

<details>
  <summary>Details</summary>

Main category: physics.geo-ph

Motivation: 传统地质建模方法在复杂地下异质性和观测数据条件约束方面存在不足，需要一种更精确的方法。

Method: 使用基于Pix2Pix的cGAN框架Pix2Geomodel，通过数据预处理、增强和U-Net生成器与PatchGAN判别器的训练，预测储层属性。

Result: 模型在岩性（PA 0.88）和水饱和度（PA 0.96）预测中表现优异，而孔隙度和渗透率预测效果中等（PA 0.70和0.74）。

Conclusion: Pix2Geomodel在储层属性模拟中表现出更高的保真度，未来需解决微结构变异性和2D约束问题。

Abstract: Accurate geological modeling is critical for reservoir characterization, yet
traditional methods struggle with complex subsurface heterogeneity, and they
have problems with conditioning to observed data. This study introduces
Pix2Geomodel, a novel conditional generative adversarial network (cGAN)
framework based on Pix2Pix, designed to predict reservoir properties (facies,
porosity, permeability, and water saturation) from the Rotliegend reservoir of
the Groningen gas field. Utilizing a 7.6 million-cell dataset from the
Nederlandse Aardolie Maatschappij, accessed via EPOS-NL, the methodology
included data preprocessing, augmentation to generate 2,350 images per
property, and training with a U-Net generator and PatchGAN discriminator over
19,000 steps. Evaluation metrics include pixel accuracy (PA), mean intersection
over union (mIoU), frequency weighted intersection over union (FWIoU), and
visualizations assessed performance in masked property prediction and
property-to-property translation tasks. Results demonstrated high accuracy for
facies (PA 0.88, FWIoU 0.85) and water saturation (PA 0.96, FWIoU 0.95), with
moderate success for porosity (PA 0.70, FWIoU 0.55) and permeability (PA 0.74,
FWIoU 0.60), and robust translation performance (e.g., facies-to-facies PA
0.98, FWIoU 0.97). The framework captured spatial variability and geological
realism, as validated by variogram analysis, and calculated the training loss
curves for the generator and discriminator for each property. Compared to
traditional methods, Pix2Geomodel offers enhanced fidelity in direct property
mapping. Limitations include challenges with microstructural variability and 2D
constraints, suggesting future integration of multi-modal data and 3D modeling
(Pix2Geomodel v2.0). This study advances the application of generative AI in
geoscience, supporting improved reservoir management and open science
initiatives.

</details>
