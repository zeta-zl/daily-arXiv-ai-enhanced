<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 61]
- [cs.LG](#cs.LG) [Total: 85]
- [cs.AI](#cs.AI) [Total: 45]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.CY](#cs.CY) [Total: 6]
- [cs.SE](#cs.SE) [Total: 12]
- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.NE](#cs.NE) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 11]
- [quant-ph](#quant-ph) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.HC](#cs.HC) [Total: 6]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.CV](#cs.CV) [Total: 32]
- [cs.RO](#cs.RO) [Total: 7]
- [cs.SD](#cs.SD) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries](https://arxiv.org/abs/2506.13796)
*Zhou Chen,Xiao Wang,Yuanhong Liao,Ming Lin,Yuqi Bai*

Key words: 气候变化, 大语言模型, 指令数据, 自动构建, ClimateChat

TL;DR: 本研究提出了一种自动化构建气候变化指令数据的方法，并利用该方法生成的ClimateChat-Corpus数据集微调LLMs，开发了名为ClimateChat的模型，显著提升了气候变化问答任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 气候变化研究需求增长，但现有方法在高效生成高精度指令数据方面不足，限制了气候变化LLMs的进一步发展。

Method: 通过从文档中提取事实和背景知识生成指令，利用网络爬取和种子指令收集增强数据多样性，构建ClimateChat-Corpus数据集并微调LLMs。

Result: ClimateChat在气候变化问答任务中表现显著提升，证明了不同基础模型和指令数据对性能的影响。

Conclusion: 研究为构建气候变化指令数据和训练专用LLMs提供了重要参考，强调了选择合适基础模型的重要性。

Abstract: As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.

</details>


### [2] [Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles](https://arxiv.org/abs/2506.13886)
*Antara Raaghavi Bhattacharya,Isabel Papadimitriou,Kathryn Davidson,David Alvarez-Melis*

Key words: 大型语言模型, 数字系统, 跨语言, 组合规则, 推理能力

TL;DR: 论文探讨了大型语言模型（LLMs）在处理跨语言数字系统任务时的困难，发现其无法像人类那样灵活推断隐含的数字结构，需依赖显式符号标记才能解决问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是探究为什么LLMs在处理涉及跨语言数字系统的语言-数学谜题时表现不佳，而人类却能成功掌握。

Method: 通过一系列实验分离语言和数学方面的问题，重点研究数字的构造和组合参数如何影响模型性能。

Result: 实验发现，除非数学运算通过已知符号（如+、×）明确标记，否则LLMs无法稳定解决问题；LLMs缺乏人类对数字隐式结构的理解能力。

Conclusion: 当前推理模型难以从人类规模数据中灵活推断隐式组合规则，这一问题仍具挑战性。

Abstract: Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\times$, etc, as in "twenty + three"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models.

</details>


### [3] [VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training](https://arxiv.org/abs/2506.13888)
*Jipeng Zhang,Kehao Miao,Renjie Pi,Zhaowei Wang,Runtao Liu,Rui Pan,Tong Zhang*

Key words: 视觉语言奖励模型, 强化学习, 自举困境, 模态偏差, 迭代训练

TL;DR: 本文提出了一种迭代训练框架，结合视觉专家、思维链和基于边际的拒绝采样，解决了视觉语言奖励模型训练中的自举困境和模态偏差问题，实现了幻觉检测和多模态推理的优越性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 针对视觉语言模型（VL模型）强化微调中奖励模型（VL-RM）训练时存在的自举困境和模态偏差问题，提出改进方法。

Method: 采用迭代训练框架，整合视觉专家、思维链（CoT）和基于边际的拒绝采样，优化偏好数据集和结构化反馈。

Result: 实验验证了所提方法在幻觉检测和多模态推理任务中的显著性能提升。

Conclusion: 该方法有效解决了VL-RM训练中的关键挑战，推动了视觉语言模型与强化学习的对齐。

Abstract: Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large language models but remains underexplored for Vision-Language (VL) models. The Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing structured feedback, yet training effective VL-RMs faces two major challenges. First, the bootstrapping dilemma arises as high-quality training data depends on already strong VL models, creating a cycle where self-generated supervision reinforces existing biases. Second, modality bias and negative example amplification occur when VL models hallucinate incorrect visual attributes, leading to flawed preference data that further misguides training. To address these issues, we propose an iterative training framework leveraging vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling. Our approach refines preference datasets, enhances structured critiques, and iteratively improves reasoning. Experiments across VL-RM benchmarks demonstrate superior performance in hallucination detection and multimodal reasoning, advancing VL model alignment with reinforcement learning.

</details>


### [4] [EmoNews: A Spoken Dialogue System for Expressive News Conversations](https://arxiv.org/abs/2506.13894)
*Ryuki Matsuura,Shikhar Bharadwaj,Jiarui Liu,Dhatchi Kunde Govindarajan*

Key words: 情感对话系统, 情感语音合成, 情感分析, 新闻对话

TL;DR: 文章开发了一种基于上下文的情感情感对话系统（SDS），用于新闻对话中情感语音的调节，以提升同理心。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有研究在情感文本转语音（TTS）技术上有进展，但面向任务的情感SDS研究不足，且缺乏标准化评估指标。

Method: 利用基于大型语言模型（LLM）的情感分析器和PromptTTS合成上下文适配的情感语音，并提出主观评估量表。

Result: 实验表明，该系统在情感调节和参与度上优于基线系统。

Conclusion: 语音情感对提升对话参与度至关重要。

Abstract: We develop a task-oriented spoken dialogue system (SDS) that regulates emotional speech based on contextual cues to enable more empathetic news conversations. Despite advancements in emotional text-to-speech (TTS) techniques, task-oriented emotional SDSs remain underexplored due to the compartmentalized nature of SDS and emotional TTS research, as well as the lack of standardized evaluation metrics for social goals. We address these challenges by developing an emotional SDS for news conversations that utilizes a large language model (LLM)-based sentiment analyzer to identify appropriate emotions and PromptTTS to synthesize context-appropriate emotional speech. We also propose subjective evaluation scale for emotional SDSs and judge the emotion regulation performance of the proposed and baseline systems. Experiments showed that our emotional SDS outperformed a baseline system in terms of the emotion regulation and engagement. These results suggest the critical role of speech emotion for more engaging conversations. All our source code is open-sourced at https://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1

</details>


### [5] [Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations](https://arxiv.org/abs/2506.13901)
*Abhilekh Borah,Chhavi Sharma,Danush Khanna,Utkarsh Bhatt,Gurpreet Singh,Hasnat Md Abdullah,Raghav Kaushik Ravi,Vinija Jain,Jyoti Patel,Shubham Singh,Vasu Sharma,Arpita Vats,Rahul Raja,Aman Chadha,Amitava Das*

Key words: 大型语言模型,对齐评估,安全度量,AQI,LITMUS数据集

TL;DR: 论文提出了一种新的指标Alignment Quality Index (AQI)，用于评估大型语言模型（LLM）的对齐质量，并引入LITMUS数据集以支持在复杂条件下的稳健评估。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着LLM进入教育、医疗、法律等高风险领域，其行为必须可靠地反映符合人类价值观和安全约束。然而，当前的对齐评估方法存在盲点。

Method: AQI通过分析潜在空间中安全与不安全激活的分离，结合多种聚类质量度量（如DBS、DI、XBI、CHI），提供了一种几何且提示不变的对齐评估方法。

Result: 在多种模型（DPO、GRPO、RLHF）上的测试表明，AQI能够揭示被传统指标忽略的漏洞，并与外部评估结果一致。

Conclusion: AQI为模型对齐提供了一种稳健的、解码不变的安全审计工具，LITMUS数据集则支持未来的相关研究。

Abstract: Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.
  To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.
  Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.

</details>


### [6] [ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection](https://arxiv.org/abs/2506.13956)
*Shang-Chi Tsai,Seiya Kawano,Angel Garcia Contreras,Koichiro Yoshino,Yun-Nung Chen*

Key words: 机器人辅助, 多模态分类, 数据增强, 大语言模型, 稳定扩散模型

TL;DR: 提出了一种通过数据增强提升机器人意图理解能力的框架，结合对话和环境图像生成，显著提高模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于大规模多模态数据集收集困难，需要一种方法增强数据以提升机器人的意图理解和动作选择能力。

Method: 利用大语言模型模拟对话和环境，结合稳定扩散模型生成图像，扩充数据集以优化多模态模型。

Result: 实验证明该方法显著提升了机器人的动作选择能力，达到最新技术水平。

Conclusion: 提出的数据增强框架在多模态机器人辅助场景中有效提升了性能，具有实际应用价值。

Abstract: When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robot's action selection capabilities, achieving the state-of-the-art performance.

</details>


### [7] [Are manual annotations necessary for statutory interpretations retrieval?](https://arxiv.org/abs/2506.13965)
*Aleksander Smywiński-Pohl,Tomer Libal,Adam Kaczmarczyk,Magdalena Król*

Key words: 法律研究, 注释优化, 大型语言模型, 自动化, 法律概念

TL;DR: 本文探讨了在法律研究中自动化注释过程的优化方法，包括注释数量、候选句子选择以及使用大型语言模型（LLM）自动化注释的效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 法律研究的注释过程成本高且需重复，因此研究如何通过实验优化和自动化来减少手动注释的需求。

Method: 通过实验确定最佳注释数量、比较随机选择与优化候选句子的效果，以及评估LLM自动化注释的效果。

Result: 提出了注释数量、候选句子选择策略的优化方案，并验证了LLM在自动化注释中的有效性。

Conclusion: 自动化注释可以减少手动工作量，同时保持模型性能。

Abstract: One of the elements of legal research is looking for cases where judges have extended the meaning of a legal concept by providing interpretations of what a concept means or does not mean. This allow legal professionals to use such interpretations as precedents as well as laymen to better understand the legal concept. The state-of-the-art approach for retrieving the most relevant interpretations for these concepts currently depends on the ranking of sentences and the training of language models over annotated examples. That manual annotation process can be quite expensive and need to be repeated for each such concept, which prompted recent research in trying to automate this process. In this paper, we highlight the results of various experiments conducted to determine the volume, scope and even the need for manual annotation. First of all, we check what is the optimal number of annotations per a legal concept. Second, we check if we can draw the sentences for annotation randomly or there is a gain in the performance of the model, when only the best candidates are annotated. As the last question we check what is the outcome of automating the annotation process with the help of an LLM.

</details>


### [8] [AI shares emotion with humans across languages and cultures](https://arxiv.org/abs/2506.13978)
*Xiuwen Wu,Hao Wang,Zhiang Yan,Xiaohan Tang,Pengfei Xu,Wai-Ting Siok,Ping Li,Jia-Hong Gao,Bingjiang Lyu,Lang Qin*

Key words: 情感AI，情绪表达，LLM，人类-AI对齐，情感调控

TL;DR: AI情绪表达与人类感知一致，可通过心理概念精确调控。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究AI是否能够像人类一样在语言中表达情绪，以及如何控制其输出情绪。

Method: 评估人类与AI在不同语言文化群体和模型家族中的情绪对齐，使用可解释的LLM特征，分析情感维度和行为数据。

Result: LLM的情绪表达结构与人类感知一致，且可通过人类情感概念精确调控其情感输出。

Conclusion: AI与人类共享情绪表征，并能通过心理学概念精确引导其情感输出。

Abstract: Effective and safe human-machine collaboration requires the regulated and meaningful exchange of emotions between humans and artificial intelligence (AI). Current AI systems based on large language models (LLMs) can provide feedback that makes people feel heard. Yet it remains unclear whether LLMs represent emotion in language as humans do, or whether and how the emotional tone of their output can be controlled. We assess human-AI emotional alignment across linguistic-cultural groups and model-families, using interpretable LLM features translated from concept-sets for over twenty nuanced emotion categories (including six basic emotions). Our analyses reveal that LLM-derived emotion spaces are structurally congruent with human perception, underpinned by the fundamental affective dimensions of valence and arousal. Furthermore, these emotion-related features also accurately predict large-scale behavioural data on word ratings along these two core dimensions, reflecting both universal and language-specific patterns. Finally, by leveraging steering vectors derived solely from human-centric emotion concepts, we show that model expressions can be stably and naturally modulated across distinct emotion categories, which provides causal evidence that human emotion concepts can be used to systematically induce LLMs to produce corresponding affective states when conveying content. These findings suggest AI not only shares emotional representations with humans but its affective outputs can be precisely guided using psychologically grounded emotion concepts.

</details>


### [9] [Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text](https://arxiv.org/abs/2506.14012)
*Amr Mohamed,Yang Zhang,Michalis Vazirgiannis,Guokan Shang*

Key words: 代码切换, 大型语言模型, 多语言处理, 自然语言理解

TL;DR: 本文研究了大型语言模型（LLMs）对代码切换（CSW）文本的处理能力，通过生成CSW变体的测试基准评估其理解效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 代码切换在数字化交流中日益普遍，但LLMs对其的处理能力尚未充分研究，理解其对混合语言输入的反应至关重要。

Method: 生成CSW变体的标准测试基准，评估LLMs在处理不同混合模式时的表现。

Result: 当外语词破坏英语文本时LLMs表现下降，但将英语嵌入其他语言常能提升理解；提示效果不稳定，微调更有效。

Conclusion: LLMs对CSW文本的处理受混合模式影响明显，微调是缓解性能下降的更稳定方法。

Abstract: Code-switching (CSW) is the act of alternating between two or more languages within a single discourse. This phenomenon is widespread in multilingual communities, and increasingly prevalent in online content, where users naturally mix languages in everyday communication. As a result, Large Language Models (LLMs), now central to content processing and generation, are frequently exposed to code-switched inputs. Given their widespread use, it is crucial to understand how LLMs process and reason about such mixed-language text. This paper presents a systematic evaluation of LLM comprehension under code-switching by generating CSW variants of established reasoning and comprehension benchmarks. While degradation is evident when foreign tokens disrupt English text$\unicode{x2013}$even under linguistic constraints$\unicode{x2013}$embedding English into other languages often improves comprehension. Though prompting yields mixed results, fine-tuning offers a more stable path to degradation mitigation.

</details>


### [10] [MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation](https://arxiv.org/abs/2506.14028)
*Xueqing Peng,Lingfei Qian,Yan Wang,Ruoyu Xiang,Yueru He,Yang Ren,Mingyang Jiang,Jeff Zhao,Huan He,Yi Han,Yun Feng,Yuechen Jiang,Yupeng Cao,Haohang Li,Yangyang Yu,Xiaoyu Wang,Penglei Gao,Shengyuan Lin,Keyi Wang,Shanshan Yang,Yilun Zhao,Zhiwei Liu,Peng Lu,Jerry Huang,Suyuchen Wang,Triantafillos Papadopoulos,Polydoros Giannouris,Efstathia Soufleri,Nuo Chen,Guojun Xiong,Zhiyang Deng,Yijia Zhao,Mingquan Lin,Meikang Qiu,Kaleb E Smith,Arman Cohan,Xiao-Yang Liu,Jimin Huang,Alejandro Lopez-Lira,Xi Chen,Junichi Tsujii,Jian-Yun Nie,Sophia Ananiadou,Qianqian Xie*

Key words: LLM, 多模态, 多语言, 金融NLP, 基准测试

TL;DR: MultiFinBen是一个多语言、多模态的金融领域基准测试，旨在评估LLM在复杂跨语言和多模态任务中的表现，填补现有基准的不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有金融领域的基准测试多为单语言和单模态，无法反映现实金融通讯的复杂性。MultiFinBen旨在解决这一问题。

Method: 引入两个新任务（PolyFiQA-Easy和PolyFiQA-Expert）和OCR任务（EnglishOCR和SpanishOCR），并采用动态难度感知选择机制构建平衡的基准测试。

Result: 对22种先进模型的测试表明，即使最强模型在复杂跨语言和多模态金融任务中表现不佳。

Conclusion: MultiFinBen的发布旨在促进金融领域的透明、可重复和包容性研究与应用进步。

Abstract: Recent advances in large language models (LLMs) have accelerated progress in financial NLP and applications, yet existing benchmarks remain limited to monolingual and unimodal settings, often over-relying on simple tasks and failing to reflect the complexity of real-world financial communication. We introduce MultiFinBen, the first multilingual and multimodal benchmark tailored to the global financial domain, evaluating LLMs across modalities (text, vision, audio) and linguistic settings (monolingual, bilingual, multilingual) on domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy and PolyFiQA-Expert, the first multilingual financial benchmarks requiring models to perform complex reasoning over mixed-language inputs; and EnglishOCR and SpanishOCR, the first OCR-embedded financial QA tasks challenging models to extract and reason over information from visual-text financial documents. Moreover, we propose a dynamic, difficulty-aware selection mechanism and curate a compact, balanced benchmark rather than simple aggregation existing datasets. Extensive evaluation of 22 state-of-the-art models reveals that even the strongest models, despite their general multimodal and multilingual capabilities, struggle dramatically when faced with complex cross-lingual and multimodal tasks in financial domain. MultiFinBen is publicly released to foster transparent, reproducible, and inclusive progress in financial studies and applications.

</details>


### [11] [An Interdisciplinary Review of Commonsense Reasoning and Intent Detection](https://arxiv.org/abs/2506.14040)
*Md Nazmus Sakib*

Key words: 常识推理,意图检测,自然语言理解,零样本学习,文化适应

TL;DR: 综述探讨了常识推理和意图检测的最新进展，分析了28篇论文，总结了方法和应用，并指出了未来的研究方向。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨自然语言理解中常识推理和意图检测的关键挑战，总结最新研究进展。

Method: 分析28篇论文（2020-2025年），按方法和应用分类整理。

Result: 总结了常识推理和意图检测的多领域应用和新兴趋势，发现了一些关键研究缺口。

Conclusion: 研究强调了自适应、多语言和上下文感知模型的发展趋势，指出了领域内未来的研究方向。

Abstract: This review explores recent advances in commonsense reasoning and intent detection, two key challenges in natural language understanding. We analyze 28 papers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and application. Commonsense reasoning is reviewed across zero-shot learning, cultural adaptation, structured evaluation, and interactive contexts. Intent detection is examined through open-set models, generative formulations, clustering, and human-centered systems. By bridging insights from NLP and HCI, we highlight emerging trends toward more adaptive, multilingual, and context-aware models, and identify key gaps in grounding, generalization, and benchmark design.

</details>


### [12] [Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications](https://arxiv.org/abs/2506.14046)
*David Kogan,Max Schumacher,Sam Nguyen,Masanori Suzuki,Melissa Smith,Chloe Sophia Bellows,Jared Bernstein*

Key words: Ace-CEFR, 语言难度, LLMs, Transformer模型

TL;DR: Ace-CEFR数据集为评估短对话文本的语言难度而生，支持训练和过滤大型语言模型（LLMs）。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 需要评估对话文本的语言难度以优化LLMs的训练和过滤。

Method: 引入专家标注难度的Ace-CEFR数据集，并测试Transformer模型和LLMs的性能。

Result: 基于Ace-CEFR训练的模型在难度评估上优于人类专家，且延迟适合生产环境。

Conclusion: Ace-CEFR数据集公开为研究和开发提供支持。

Abstract: There is an unmet need to evaluate the language difficulty of short, conversational passages of text, particularly for training and filtering Large Language Models (LLMs). We introduce Ace-CEFR, a dataset of English conversational text passages expert-annotated with their corresponding level of text difficulty. We experiment with several models on Ace-CEFR, including Transformer-based models and LLMs. We show that models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have latency appropriate to production environments. Finally, we release the Ace-CEFR dataset to the public for research and development.

</details>


### [13] [Automatic Extraction of Clausal Embedding Based on Large-Scale English Text Data](https://arxiv.org/abs/2506.14064)
*Iona Carslaw,Sivan Milton,Nicolas Navarre,Ciyang Qing,Wataru Uegaki*

Key words: 嵌入式从句、选区解析、语料库、自然语言处理、语言学

TL;DR: 论文提出了一种利用选区解析和启发式方法从大规模语料库中自动检测和标注英语嵌入式从句的方法，并基于手注数据集评估了工具效果，最终从Dolma语料库中提取了大量自然出现的嵌入式从句数据。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前研究依赖人工构造的例句分析嵌入式从句，忽略了大规模语料库中的统计信息和自然例句，因此需开发一种自动化方法提取自然例句。

Method: 结合选区解析和解析启发式方法，检测和标注大规模文本数据中的英语嵌入式从句，并在手注数据集GECS上评估工具性能。

Result: 成功开发了一种自动化工具，并从Dolma语料库中提取了大量自然出现的嵌入式从句数据。

Conclusion: 该方法为语言学研究者提供了更自然、大规模的嵌入式从句研究数据，弥补了传统方法的不足。

Abstract: For linguists, embedded clauses have been of special interest because of their intricate distribution of syntactic and semantic features. Yet, current research relies on schematically created language examples to investigate these constructions, missing out on statistical information and naturally-occurring examples that can be gained from large language corpora. Thus, we present a methodological approach for detecting and annotating naturally-occurring examples of English embedded clauses in large-scale text data using constituency parsing and a set of parsing heuristics. Our tool has been evaluated on our dataset Golden Embedded Clause Set (GECS), which includes hand-annotated examples of naturally-occurring English embedded clause sentences. Finally, we present a large-scale dataset of naturally-occurring English embedded clauses which we have extracted from the open-source corpus Dolma using our extraction tool.

</details>


### [14] [Abstract Meaning Representation for Hospital Discharge Summarization](https://arxiv.org/abs/2506.14101)
*Paul Landes,Sitara Rao,Aaron Jeremy Chaise,Barbara Di Eugenio*

Key words: 大型语言模型, 幻觉问题, 出院总结, 临床领域, 语言图, 深度学习

TL;DR: 该论文探讨了如何通过结合语言图和深度学习模型来解决大型语言模型在临床领域中生成出院总结时的幻觉问题，从而提升内容的可信度和可追溯性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型在生成临床出院总结时存在幻觉问题，这不仅增加了医生的文档负担，还可能对临床决策产生负面影响。该研究旨在开发新方法以解决这一问题，从而解放医生时间并增强文档的可信度。

Method: 研究提出了一种结合语言图和深度学习模型的方法，重点关注内容的来源和可信度，并在公开的MIMIC-III数据集和匿名医院的临床笔记上进行了验证。

Result: 该方法在MIMIC-III数据集和匿名医院的临床笔记上展示了显著的可靠性结果，并提供了生成出院总结的示例、源代码和训练模型。

Conclusion: 通过结合语言图和深度学习模型，该方法有效解决了大型语言模型在临床领域中的幻觉问题，为自动生成可信的出院总结提供了可行方案。

Abstract: The Achilles heel of Large Language Models (LLMs) is hallucination, which has drastic consequences for the clinical domain. This is particularly important with regards to automatically generating discharge summaries (a lengthy medical document that summarizes a hospital in-patient visit). Automatically generating these summaries would free physicians to care for patients and reduce documentation burden. The goal of this work is to discover new methods that combine language-based graphs and deep learning models to address provenance of content and trustworthiness in automatic summarization. Our method shows impressive reliability results on the publicly available Medical Information Mart for Intensive III (MIMIC-III) corpus and clinical notes written by physicians at Anonymous Hospital. rovide our method, generated discharge ary output examples, source code and trained models.

</details>


### [15] [Essential-Web v1.0: 24T tokens of organized web data](https://arxiv.org/abs/2506.14111)
*Essential AI,:,Andrew Hojel,Michael Pust,Tim Romanski,Yash Vanjani,Ritvik Kapila,Mohit Parmar,Adarsh Chaluvaraju,Alok Tripathy,Anil Thomas,Ashish Tanwer,Darsh J Shah,Ishaan Shah,Karl Stratos,Khoi Nguyen,Kurt Smith,Michael Callahan,Peter Rushton,Philip Monk,Platon Mazarakis,Saad Jamal,Saurabh Srivastava,Somanshu Singla,Ashish Vaswani*

Key words: 预训练数据集, 标注, 高质量数据, Essential-Web v1.0

TL;DR: Essential-Web v1.0是一个24万亿标记的数据集，每个文档都标注了十二个类别，涵盖主题、格式、内容复杂度和质量。通过SQL式过滤，可在多个领域生成有竞争力的数据集。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决大规模预训练数据集的缺乏问题，提供高质量、多样化的数据。

Method: 使用Fine-tuned的0.5b参数模型EAI-Distill-0.5b标注数据，并通过SQL式过滤生成特定领域数据集。

Result: 在数学、网页代码、STEM和医学等领域生成有竞争力的数据集，表现接近或超越SOTA。

Conclusion: Essential-Web v1.0为预训练模型提供了高质量、多样化的数据支持。

Abstract: Data plays the most prominent role in how language models acquire skills and knowledge. The lack of massive, well-organized pre-training datasets results in costly and inaccessible data pipelines. We present Essential-Web v1.0, a 24-trillion-token dataset in which every document is annotated with a twelve-category taxonomy covering topic, format, content complexity, and quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned 0.5b-parameter model that achieves an annotator agreement within 3% of Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain competitive web-curated datasets in math (-8.0% relative to SOTA), web code (+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on HuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0

</details>


### [16] [Sampling from Your Language Model One Byte at a Time](https://arxiv.org/abs/2506.14123)
*Jonathan Hayase,Alisa Liu,Noah A. Smith,Sewoong Oh*

Key words: 分词、提示边界问题、BPE分词器、模型集成、字符级语言模型

TL;DR: 提出了一种将BPE分词器转换为字符级或字节级LM的方法，解决了提示边界问题（PBP）并实现了不同分词器模型的集成。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现代语言模型普遍使用分词，但分词可能导致生成的文本失真（如提示边界问题）且不同分词器模型难以集成。

Method: 在推理阶段将BPE分词器转换为字符级或字节级LM，保持生成分布不变。

Result: 该方法有效解决PBP，并支持不同分词器模型的集成和微调转移，下游任务表现优于原模型。

Conclusion: 提出的方法解决了分词带来的问题，提升了模型的适应性和性能。

Abstract: Tokenization is used almost universally by modern language models, enabling efficient text representation using multi-byte or multi-character tokens. However, prior work has shown that tokenization can introduce distortion into the model's generations. For example, users are often advised not to end their prompts with a space because it prevents the model from including the space as part of the next token. This Prompt Boundary Problem (PBP) also arises in languages such as Chinese and in code generation, where tokens often do not line up with syntactic boundaries. Additionally mismatching tokenizers often hinder model composition and interoperability. For example, it is not possible to directly ensemble models with different tokenizers due to their mismatching vocabularies. To address these issues, we present an inference-time method to convert any autoregressive LM with a BPE tokenizer into a character-level or byte-level LM, without changing its generative distribution at the text level. Our method efficient solves the PBP and is also able to unify the vocabularies of language models with different tokenizers, allowing one to ensemble LMs with different tokenizers at inference time as well as transfer the post-training from one model to another using proxy-tuning. We demonstrate in experiments that the ensemble and proxy-tuned models outperform their constituents on downstream evals.

</details>


### [17] [DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization](https://arxiv.org/abs/2506.14157)
*Chengyu Huang,Tanya Goyal*

Key words: 偏好优化, DCRM, 数据集质量, 大语言模型

TL;DR: 通过量化偏好数据集中偏好与不偏好响应的差异，提出了距离校准奖励边际（DCRM）指标，并验证其与学习效果的关联性，提出了一种新的配对方法提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 观察到偏好优化中响应差异的匹配问题，希望通过量化差异提高学习效果。

Method: 使用DCRM指标评估响应对质量，提出最佳配对方法选择高DCRM的数据集。

Result: 实验表明，高DCRM的训练集能提升模型在多个基准上的表现。

Conclusion: DCRM是衡量偏好数据集质量的有效指标，优化数据集可进一步提升模型性能。

Abstract: Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.

</details>


### [18] [S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models](https://arxiv.org/abs/2506.14158)
*Tao He,Guang Huang,Yu Yang,Tianshi Xu,Sicheng Zhao,Guiguang Ding,Pengyang Wang,Feng Tian*

Key words: 大语言模型、推测采样、语法语义一致性、推理加速

TL;DR: 论文提出了一种基于语法和语义一致性的推测采样框架（S$^4$C），通过快速生成和高效验证显著降低大语言模型的推理延迟，提升效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决大语言模型（LLMs）自回归特性导致的推理延迟问题，并改进现有推测采样方法忽略文本生成内在一致性的不足。

Method: 提出S$^4$C框架，结合多头快速生成和连续验证树，优化推测采样的生成与验证效率。

Result: S$^4$C在主流任务中表现优于基线方法，在Spec-bench基准测试上加速比为2.26x-2.60x。

Conclusion: S$^4$C显著提升了大语言模型的推理效率，减少了计算资源需求。

Abstract: Large language models (LLMs) exhibit remarkable reasoning capabilities across diverse downstream tasks. However, their autoregressive nature leads to substantial inference latency, posing challenges for real-time applications. Speculative sampling mitigates this issue by introducing a drafting phase followed by a parallel validation phase, enabling faster token generation and verification. Existing approaches, however, overlook the inherent coherence in text generation, limiting their efficiency. To address this gap, we propose a Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework, which extends speculative sampling by leveraging multi-head drafting for rapid token generation and a continuous verification tree for efficient candidate validation and feature reuse. Experimental results demonstrate that S$^4$C surpasses baseline methods across mainstream tasks, offering enhanced efficiency, parallelism, and the ability to generate more valid tokens with fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an acceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.

</details>


### [19] [MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation of LLMs via Theory of Mind](https://arxiv.org/abs/2506.14161)
*Yanlin Li,Hao Liu,Huimin Liu,Yinwei Wei,Yupeng Hu*

Key words: Theory of Mind, Large Language Models, implicit bias, Stereotype Content Model, Word Association Bias Test, Affective Attribution Test

TL;DR: 提出了一种基于SCM的多维度评估框架，通过间接任务（WABT和AAT）揭示LLMs中的隐含偏见，实验证明了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有直接查询方法难以捕捉LLMs中多维度的隐含偏见，需更鲁棒的评估方法。

Method: 利用SCM模型，设计WABT和AAT两种间接任务，评估LLMs在能力、社交性和道德方面的偏见。

Result: 在8个SOTA LLMs中发现复杂的偏见结构，如普遍的社交性偏见和多维度差异。

Conclusion: 该框架为识别隐含偏见的结构性本质提供了更鲁棒的方法。

Abstract: Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity for reasoning about mental states, yet failures in this capacity often manifest as systematic implicit bias. Evaluating this bias is challenging, as conventional direct-query methods are susceptible to social desirability effects and fail to capture its subtle, multi-dimensional nature. To this end, we propose an evaluation framework that leverages the Stereotype Content Model (SCM) to reconceptualize bias as a multi-dimensional failure in ToM across Competence, Sociability, and Morality. The framework introduces two indirect tasks: the Word Association Bias Test (WABT) to assess implicit lexical associations and the Affective Attribution Test (AAT) to measure covert affective leanings, both designed to probe latent stereotypes without triggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs demonstrate our framework's capacity to reveal complex bias structures, including pervasive sociability bias, multi-dimensional divergence, and asymmetric stereotype amplification, thereby providing a more robust methodology for identifying the structural nature of implicit bias.

</details>


### [20] [GRAM: A Generative Foundation Reward Model for Reward Generalization](https://arxiv.org/abs/2506.14175)
*Chenglong Wang,Yang Gan,Yifu Huo,Yongyu Mu,Qiaozhi He,Murun Yang,Bei Li,Tong Xiao,Chunliang Zhang,Tongran Liu,Jingbo Zhu*

Key words: 奖励模型，生成模型，无监督学习，标签平滑，多任务学习

TL;DR: 本文探讨了结合无监督和有监督数据训练奖励模型的方法，提出了一种生成式奖励模型，通过标签平滑优化正则化成对排序损失，并在多任务中验证了其优越性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统奖励模型依赖标注的人类偏好数据，本文旨在利用无监督和有监督数据结合的方式提升奖励模型的泛化能力和性能。

Method: 开发了一种生成式奖励模型，先通过大规模无监督学习预训练，再通过有监督学习微调，并引入标签平滑优化正则化成对排序损失。

Result: 模型在响应排序、人类反馈强化学习和任务适应等多个任务中表现优异，显著超越基线模型。

Conclusion: 生成式奖励模型能有效结合生成与判别模型，在多任务中表现卓越，且无需大量微调。

Abstract: In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models.

</details>


### [21] [Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages](https://arxiv.org/abs/2506.14177)
*Tuan Nguyen,Huy-Dat Tran*

Key words: 语码转换, 语音识别, 合成数据, 低资源语言, 东南亚语言

TL;DR: 研究探讨了利用合成语码转换数据构建CS-ASR的方法，通过短语级混合生成自然模式的合成数据，并利用其增强预训练ASR模型性能。主要针对东南亚低资源语言对，结果表明该方法能显著提升性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在多语言环境中，语码转换（CS）由于语言复杂性导致标注数据稀缺且昂贵，对ASR系统提出挑战。为了解决这一问题，研究提出利用合成数据构建CS-ASR。

Method: 提出一种短语级混合方法生成合成CS数据，模拟自然模式。利用单语数据和合成CS数据微调大型预训练ASR模型（如Whisper、MMS、SeamlessM4T）。

Result: 实验结果显示，该方法在单语和CS测试上均提升了ASR性能，其中BM-EN提升最大，其次是TA-EN和ZH-BM。

Conclusion: 这项研究提供了一种成本效益高的CS-ASR开发方法，对研究和工业界具有重要价值。

Abstract: Code-switching (CS), common in multilingual settings, presents challenges for ASR due to scarce and costly transcribed data caused by linguistic complexity. This study investigates building CS-ASR using synthetic CS data. We propose a phrase-level mixing method to generate synthetic CS data that mimics natural patterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data to fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This paper focuses on three under-resourced Southeast Asian language pairs: Malay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN), establishing a new comprehensive benchmark for CS-ASR to evaluate the performance of leading ASR models. Experimental results show that the proposed training strategy enhances ASR performance on monolingual and CS tests, with BM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a cost-effective approach for CS-ASR development, benefiting research and industry.

</details>


### [22] [AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR](https://arxiv.org/abs/2506.14190)
*Tuan Nguyen,Huy-Dat Tran*

Key words: 代码切换, ASR, 异步适应, 预训练, Whisper

TL;DR: AsyncSwitch是一种异步适应框架，利用大规模文本数据预训练ASR模型，显著提升代码切换语音识别效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 开发代码切换ASR系统面临语言歧义和数据稀缺的挑战，现有合成音频方法计算成本高且难以扩展。

Method: 采用三阶段过程：1）在代码切换文本上训练解码器的自注意力层和前馈层；2）利用有限的语音-文本数据对齐解码器和编码器；3）对整个模型进行微调。

Result: 在马来语-英语代码切换实验中，Whisper模型相对WER降低了9.02%，并提高了单语表现在Singlish、马来语和其他英语变体上。

Conclusion: AsyncSwitch框架通过预训练和微调策略，有效提升了代码切换ASR的性能，同时改善了单语表现。

Abstract: Developing code-switched ASR systems is challenging due to language ambiguity and limited exposure to multilingual, code-switched data, while collecting such speech is costly. Prior work generates synthetic audio from text, but these methods are computationally intensive and hard to scale. We introduce AsyncSwitch, a novel asynchronous adaptation framework that leverages large-scale, text-rich web data to pre-expose ASR models to diverse code-switched domains before fine-tuning on paired speech-text corpora. Our three-stage process (1) trains decoder self-attention and feedforward layers on code-switched text, (2) aligns decoder and encoder via cross-attention using limited speech-text data, and (3) fully fine-tunes the entire model. Experiments with Whisper on Malay-English code-switching demonstrate a 9.02% relative WER reduction, while improving monolingual performance in Singlish, Malay, and other English variants.

</details>


### [23] [MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment](https://arxiv.org/abs/2506.14199)
*Junghwan Kim,Kieun Park,Sohee Park,Hyunggug Kim,Bongwon Suh*

Key words: 文学翻译，翻译质量评估，大型语言模型，多智能体系统

TL;DR: 提出了MAS-LitEval，一种用于评估文学翻译的多智能体系统，优于传统指标。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统翻译评估指标无法捕捉文化细节和文体风格，因此需要更全面的评估工具。

Method: 使用多智能体系统和大型语言模型（LLMs）评估翻译的术语、叙述和风格。

Result: MAS-LitEval在《小王子》和《亚瑟王宫廷中的康涅狄格美国佬》的翻译测试中得分高达0.890，优于传统指标。

Conclusion: 该研究为翻译质量评估提供了一个可扩展且细致的框架，适用于翻译者和研究者。

Abstract: Literary translation requires preserving cultural nuances and stylistic elements, which traditional metrics like BLEU and METEOR fail to assess due to their focus on lexical overlap. This oversight neglects the narrative consistency and stylistic fidelity that are crucial for literary works. To address this, we propose MAS-LitEval, a multi-agent system using Large Language Models (LLMs) to evaluate translations based on terminology, narrative, and style. We tested MAS-LitEval on translations of The Little Prince and A Connecticut Yankee in King Arthur's Court, generated by various LLMs, and compared it to traditional metrics. \textbf{MAS-LitEval} outperformed these metrics, with top models scoring up to 0.890 in capturing literary nuances. This work introduces a scalable, nuanced framework for Translation Quality Assessment (TQA), offering a practical tool for translators and researchers.

</details>


### [24] [ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations](https://arxiv.org/abs/2506.14200)
*Brihi Joshi,Keyu He,Sahana Ramnath,Sadra Sabouri,Kaitlyn Zhou,Souti Chattopadhyay,Swabha Swayamdipta,Xiang Ren*

Key words: 语言模型、教育、基准测试、解释生成、信息需求

TL;DR: 论文提出了ELI-Why基准，评估语言模型为不同教育背景学习者生成解释的能力，研究发现GPT-4生成的解释在匹配教育背景和信息需求上不如人工生成的解释。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前语言模型在教育中的应用广泛，但其为不同信息需求和知识背景的学习者定制回答的能力尚未充分研究。

Method: 引入ELI-Why基准，包含13.4K个“为什么”问题，通过两项人类研究评估语言模型生成的解释对不同教育背景（小学、高中、研究生）的适用性。

Result: GPT-4生成的解释仅50%匹配目标教育背景，而人工生成解释为79%；学习者认为GPT-4解释在信息需求上比人工解释平均低20%。自动评估指标显示不同模型的生成解释在教育水平上难以区分。

Conclusion: 语言模型在生成教育定制化解释方面仍需改进，尤其是匹配教育背景和信息需求的能力。

Abstract: Language models today are widely used in education, yet their ability to tailor responses for learners with varied informational needs and knowledge backgrounds remains under-explored. To this end, we introduce ELI-Why, a benchmark of 13.4K "Why" questions to evaluate the pedagogical capabilities of language models. We then conduct two extensive human studies to assess the utility of language model-generated explanatory answers (explanations) on our benchmark, tailored to three distinct educational grades: elementary, high-school and graduate school. In our first study, human raters assume the role of an "educator" to assess model explanations' fit to different educational grades. We find that GPT-4-generated explanations match their intended educational background only 50% of the time, compared to 79% for lay human-curated explanations. In our second study, human raters assume the role of a learner to assess if an explanation fits their own informational needs. Across all educational backgrounds, users deemed GPT-4-generated explanations 20% less suited on average to their informational needs, when compared to explanations curated by lay people. Additionally, automated evaluation metrics reveal that explanations generated across different language model families for different informational needs remain indistinguishable in their grade-level, limiting their pedagogical effectiveness.

</details>


### [25] [Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation](https://arxiv.org/abs/2506.14203)
*Jongho Kim,Romain Storaï,Seung-won Hwang*

Key words: 语言模型, 命名困难, 语义障碍, 梯度选择性增强, 失语症

TL;DR: 论文研究了语言模型在帮助失语症患者解决命名困难问题上的潜力，通过增强模型对语义错误和未见术语的处理能力，取得了显著效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 失语症患者在命名物品时存在困难，需要从冗余描述中准确识别目标物品，但现有模型因语义障碍和未见术语的干扰效果不佳。

Method: 提出通过梯度选择性增强方法，利用梯度值控制数据质量，梯度方差引入相关术语，增强模型鲁棒性。

Result: 在Tip-of-the-Tongue数据集和AphasiaBank患者数据上验证了模型的有效性，表现优于基线方法。

Conclusion: 通过梯度选择性增强，语言模型能有效帮助失语症患者解决命名困难问题。

Abstract: In this study, we investigate the potential of language models (LMs) in aiding patients experiencing anomia, a difficulty identifying the names of items. Identifying the intended target item from patient's circumlocution involves the two challenges of term failure and error: (1) The terms relevant to identifying the item remain unseen. (2) What makes the challenge unique is inherent perturbed terms by semantic paraphasia, which are not exactly related to the target item, hindering the identification process. To address each, we propose robustifying the model from semantically paraphasic errors and enhancing the model with unseen terms with gradient-based selective augmentation. Specifically, the gradient value controls augmented data quality amid semantic errors, while the gradient variance guides the inclusion of unseen but relevant terms. Due to limited domain-specific datasets, we evaluate the model on the Tip-of-the-Tongue dataset as an intermediary task and then apply our findings to real patient data from AphasiaBank. Our results demonstrate strong performance against baselines, aiding anomia patients by addressing the outlined challenges.

</details>


### [26] [AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents](https://arxiv.org/abs/2506.14205)
*Jingxu Xie,Dylan Xu,Xuandong Zhao,Dawn Song*

Key words: AgentSynth, LLM, 任务合成, 轨迹数据集

TL;DR: AgentSynth是一种可扩展且成本效益高的管道，用于自动合成用于通用计算机代理的高质量任务和轨迹数据集。通过信息不对称设计子任务，能够创造6000多个多样且现实的任务。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为通用计算机代理提供高质量且多样化的任务数据集，同时降低标注成本。

Method: 利用LLM提出任务，执行代理完成任务并记录轨迹，迭代生成子任务序列，再通过另一个代理总结为复合任务，控制任务难度。

Result: 在难度等级1和6上，SOTA LLM代理的成功率从18%降至4%，显示数据集的区分力。每轨迹平均成本仅为0.60美元，远低于人工标注。

Conclusion: AgentSynth高效且成本低，能够生成高质量数据集，适合作为通用代理的基准。

Abstract: We introduce AgentSynth, a scalable and cost-efficient pipeline for automatically synthesizing high-quality tasks and trajectory datasets for generalist computer-use agents. Leveraging information asymmetry, AgentSynth constructs subtasks that are simple during generation but significantly more challenging when composed into long-horizon tasks, enabling the creation of over 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based task proposer guided by a persona, followed by an execution agent that completes the task and logs the trajectory. This process is repeated iteratively to form a sequence of subtasks, which are then summarized by a separate agent into a composite task of controllable difficulty. A key strength of AgentSynth is its ability to precisely modulate task complexity by varying the number of subtasks. Empirical evaluations show that state-of-the-art LLM agents suffer a steep performance drop, from 18% success at difficulty level 1 to just 4% at level 6, highlighting the benchmark's difficulty and discriminative power. Moreover, our pipeline achieves a low average cost of \$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our code and data are publicly available at https://github.com/sunblaze-ucb/AgentSynth

</details>


### [27] [CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation](https://arxiv.org/abs/2506.14206)
*Jia-Chen Zhang,Zheng Zhou,Yu-Jie Xiong,Chun-Ming Xia,Fei Dai*

Key words: 扩散模型, 表格数据生成, 混合自适应因果正则化

TL;DR: 论文提出了一种基于扩散模型的生成模型CausalDiffTab，用于处理混合类型的表格数据，并通过混合自适应因果正则化方法提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 高质量训练数据对生成式AI至关重要，但其获取面临隐私和多样性挑战，尤其是混合类型表格数据的生成难度大。

Method: 提出CausalDiffTab模型，结合扩散模型和混合自适应因果正则化方法，有效捕捉变量间复杂关系。

Result: 在七个数据集上的实验表明，CausalDiffTab在各项指标上优于基线方法。

Conclusion: CausalDiffTab为高质量混合类型表格数据的生成提供了有效解决方案。

Abstract: Training data has been proven to be one of the most critical components in training generative AI. However, obtaining high-quality data remains challenging, with data privacy issues presenting a significant hurdle. To address the need for high-quality data. Synthesize data has emerged as a mainstream solution, demonstrating impressive performance in areas such as images, audio, and video. Generating mixed-type data, especially high-quality tabular data, still faces significant challenges. These primarily include its inherent heterogeneous data types, complex inter-variable relationships, and intricate column-wise distributions. In this paper, we introduce CausalDiffTab, a diffusion model-based generative model specifically designed to handle mixed tabular data containing both numerical and categorical features, while being more flexible in capturing complex interactions among variables. We further propose a hybrid adaptive causal regularization method based on the principle of Hierarchical Prior Fusion. This approach adaptively controls the weight of causal regularization, enhancing the model's performance without compromising its generative capabilities. Comprehensive experiments conducted on seven datasets demonstrate that CausalDiffTab outperforms baseline methods across all metrics. Our code is publicly available at: https://github.com/Godz-z/CausalDiffTab.

</details>


### [28] [Explainable Detection of Implicit Influential Patterns in Conversations via Data Augmentation](https://arxiv.org/abs/2506.14211)
*Sina Abdidizaji,Md Kowsher,Niloofar Yousefi,Ivan Garibay*

Key words: 隐含影响力模式，语言模型，多标签分类，数字化平台，检测框架

TL;DR: 本文提出了一种改进方法，用于检测对话中隐含的影响力模式，并通过增强数据集提升了检测性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 数字化时代中，恶意行为者利用隐含的语言策略影响公众认知，传统方法难以有效检测这些模式。

Method: 利用先进语言模型的推理能力增强数据集，开发框架以识别对话中隐含的影响力元素及其位置。

Result: 新方法在检测隐含影响力模式上提升了6%，在多标签分类任务中分别提升了33%和43%。

Conclusion: 提出的方法显著提升了对隐含影响力模式的检测能力，为数字化平台的监管提供了新工具。

Abstract: In the era of digitalization, as individuals increasingly rely on digital platforms for communication and news consumption, various actors employ linguistic strategies to influence public perception. While models have become proficient at detecting explicit patterns, which typically appear in texts as single remarks referred to as utterances, such as social media posts, malicious actors have shifted toward utilizing implicit influential verbal patterns embedded within conversations. These verbal patterns aim to mentally penetrate the victim's mind in order to influence them, enabling the actor to obtain the desired information through implicit means. This paper presents an improved approach for detecting such implicit influential patterns. Furthermore, the proposed model is capable of identifying the specific locations of these influential elements within a conversation. To achieve this, the existing dataset was augmented using the reasoning capabilities of state-of-the-art language models. Our designed framework resulted in a 6% improvement in the detection of implicit influential patterns in conversations. Moreover, this approach improved the multi-label classification tasks related to both the techniques used for influence and the vulnerability of victims by 33% and 43%, respectively.

</details>


### [29] [Chaining Event Spans for Temporal Relation Grounding](https://arxiv.org/abs/2506.14213)
*Jongho Kim,Dohyeon Lee,Minsoo Kim,Seung-won Hwang*

Key words: 时间关系理解, 时间推理网络, 时间跨度预测, TRC, TRE

TL;DR: 本文提出了一种新方法TRN，通过预测事件的时间跨度来解决现有方法中因答案重叠导致的不可靠结果，从而更准确理解事件之间的时间关系。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基于答案重叠的方法在区分语义相似问题时可能因偶然答案相同而产生不可靠结果，因此需要一种更可靠的方法。

Method: 提出Timeline Reasoning Network (TRN)，通过两步推理过程：先利用语义和句法信息回答问题，再通过链式问题预测时间线以验证答案。

Result: 在TORQUE和TB-dense等任务上，TRN表现优于现有方法，有效解决了答案重叠问题。

Conclusion: TRN通过时间线预测提高了时间关系理解的准确性，尤其在区分语义相似问题上效果显著。

Abstract: Accurately understanding temporal relations between events is a critical building block of diverse tasks, such as temporal reading comprehension (TRC) and relation extraction (TRE). For example in TRC, we need to understand the temporal semantic differences between the following two questions that are lexically near-identical: "What finished right before the decision?" or "What finished right after the decision?". To discern the two questions, existing solutions have relied on answer overlaps as a proxy label to contrast similar and dissimilar questions. However, we claim that answer overlap can lead to unreliable results, due to spurious overlaps of two dissimilar questions with coincidentally identical answers. To address the issue, we propose a novel approach that elicits proper reasoning behaviors through a module for predicting time spans of events. We introduce the Timeline Reasoning Network (TRN) operating in a two-step inductive reasoning process: In the first step model initially answers each question with semantic and syntactic information. The next step chains multiple questions on the same event to predict a timeline, which is then used to ground the answers. Results on the TORQUE and TB-dense, TRC and TRE tasks respectively, demonstrate that TRN outperforms previous methods by effectively resolving the spurious overlaps using the predicted timeline.

</details>


### [30] [Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team](https://arxiv.org/abs/2506.14234)
*Md Tanzib Hosain,Salman Rahman,Md Kishor Morol,Md Rizwan Parvez*

Key words: Xolver, 多智能体推理, 经验学习, LLM

TL;DR: Xolver是一个无需训练的多智能体推理框架，通过持久化记忆实现经验学习，显著提升LLM的推理能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前LLM缺乏经验积累能力，而专家解决问题时依赖丰富经验，Xolver旨在弥补这一差距。

Method: Xolver结合多种经验模态（如检索、工具使用、协作、评估与迭代），避免从零生成解决方案。

Result: Xolver在多个基准测试中优于专用推理模型，甚至轻量级版本也能超越先进模型。

Conclusion: 经验学习是迈向通用智能体的关键，Xolver展示了其潜力。

Abstract: Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.

</details>


### [31] [A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs](https://arxiv.org/abs/2506.14235)
*Yimin Deng,Yuxia Wu,Yejing Wang,Guoshuai Zhao,Li Zhu,Qidong Liu,Derong Xu,Zichuan Fu,Xian Wu,Yefeng Zheng,Xiangyu Zhao,Xueming Qian*

Key words: 时间知识图谱推理, 结构和语义融合, 多专家框架

TL;DR: 论文提出了一种结合结构和语义推理的多专家框架MESH，用于解决时间知识图谱推理问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法未能整合结构和语义推理视角，且无法区分历史与非历史事件，限制了泛化能力。

Method: 采用多专家模块（MESH框架）整合结构和语义信息，指导不同事件的推理过程。

Result: 在三个数据集上的实验验证了方法的有效性。

Conclusion: MESH框架通过多专家模块实现了结构和语义的融合，提升了推理能力。

Abstract: Temporal knowledge graph reasoning aims to predict future events with knowledge of existing facts and plays a key role in various downstream tasks. Previous methods focused on either graph structure learning or semantic reasoning, failing to integrate dual reasoning perspectives to handle different prediction scenarios. Moreover, they lack the capability to capture the inherent differences between historical and non-historical events, which limits their generalization across different temporal contexts. To this end, we propose a Multi-Expert Structural-Semantic Hybrid (MESH) framework that employs three kinds of expert modules to integrate both structural and semantic information, guiding the reasoning process for different events. Extensive experiments on three datasets demonstrate the effectiveness of our approach.

</details>


### [32] [Re-Initialization Token Learning for Tool-Augmented Large Language Models](https://arxiv.org/abs/2506.14248)
*Chenghao Li,Liu Liu,Baosheng Yu,Jiayan Qiu,Yibing Zhan*

Key words: 大语言模型, 工具集成, 令牌学习, 数值推理, 计划生成

TL;DR: 提出一种新的工具令牌学习方法，通过将工具令牌与现有词嵌入空间对齐，提升大语言模型在复杂任务中的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前方法通过单独令牌调用工具，但未考虑工具与词令牌的关系，限制了预训练模型的适应性。

Method: 基于工具名称或描述构建先验令牌嵌入，用于初始化和规范化可学习的工具令牌嵌入，确保与词令牌空间对齐。

Result: 在GSM8K-XL等数据集的多个任务中，性能优于CoT、REACT等基线方法。

Conclusion: 该方法通过相关令牌有效增强了大语言模型的工具调用能力。

Abstract: Large language models have demonstrated exceptional performance, yet struggle with complex tasks such as numerical reasoning, plan generation. Integrating external tools, such as calculators and databases, into large language models (LLMs) is crucial for enhancing problem-solving capabilities. Current methods assign a unique token to each tool, enabling LLMs to call tools through token prediction-similar to word generation. However, this approach fails to account for the relationship between tool and word tokens, limiting adaptability within pre-trained LLMs. To address this issue, we propose a novel token learning method that aligns tool tokens with the existing word embedding space from the perspective of initialization, thereby enhancing model performance. We begin by constructing prior token embeddings for each tool based on the tool's name or description, which are used to initialize and regularize the learnable tool token embeddings. This ensures the learned embeddings are well-aligned with the word token space, improving tool call accuracy. We evaluate the method on tasks such as numerical reasoning, knowledge-based question answering, and embodied plan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The results demonstrate clear improvements over recent baselines, including CoT, REACT, ICL, and ToolkenGPT, indicating that our approach effectively augments LLMs with tools through relevant tokens across diverse domains.

</details>


### [33] [From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents](https://arxiv.org/abs/2506.14285)
*Seongbo Jang,Minjin Jeon,Jaehoon Lee,Seonghyeon Lee,Dongha Lee,Hwanjo Yu*

Key words: 对话生成,时间上下文,TimelyChat,Timer,LLM

TL;DR: 研究了如何基于时间上下文生成及时的对话回应，提出了新任务TimelyChat，构建了Timer模型并优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前对话生成研究主要关注文本上下文，而忽略了时间上下文的重要性，尤其是何时回应的问题。

Method: 提出了TimelyChat任务，利用时间常识知识图谱构建55K事件驱动对话数据集，并训练Timer模型预测时间间隔并生成相关回应。

Result: Timer在对话层面和轮次层面的评估中均优于大型语言模型和其他调优基线。

Conclusion: 通过引入时间上下文，Timer提升了对话回应的及时性和相关性，为对话生成研究提供了新方向。

Abstract: While research on dialogue response generation has primarily focused on generating coherent responses conditioning on textual context, the critical question of when to respond grounded on the temporal context remains underexplored. To bridge this gap, we propose a novel task called timely dialogue response generation and introduce the TimelyChat benchmark, which evaluates the capabilities of language models to predict appropriate time intervals and generate time-conditioned responses. Additionally, we construct a large-scale training dataset by leveraging unlabeled event knowledge from a temporal commonsense knowledge graph and employing a large language model (LLM) to synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent designed to proactively predict time intervals and generate timely responses that align with those intervals. Experimental results show that Timer outperforms prompting-based LLMs and other fine-tuned baselines in both turn-level and dialogue-level evaluations. We publicly release our data, model, and code.

</details>


### [34] [Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent](https://arxiv.org/abs/2506.14302)
*Xueyang Feng,Jingsen Zhang,Jiakai Tang,Wei Li,Guohao Cai,Xu Chen,Quanyu Dai,Yue Zhu,Zhenhua Dong*

Key words: Large Language Models, Conversational Recommendation Agents, Multi-Turn Preference Optimization, Expectation Confirmation Theory

TL;DR: 该论文提出了一种新颖的多轮偏好优化（MTPO）方法ECPO，通过期望确认理论建模用户满意度，显著提升了对话推荐代理（CRAs）的交互能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前LLM驱动的对话推荐代理（CRAs）生成短视响应，难以在多轮对话中满足用户期望，因此需要一种更高效的优化方法。

Method: 提出ECPO方法，利用期望确认理论建模用户满意度变化，并通过用户模拟器AILO生成反馈，实现针对性优化。

Result: 实验表明ECPO显著提升了CRAs的交互能力，在效率和效果上均优于现有MTPO方法。

Conclusion: ECPO通过建模用户期望，实现了低成本且高效的多轮偏好优化，为对话推荐系统提供了新思路。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly propelled the development of Conversational Recommendation Agents (CRAs). However, these agents often generate short-sighted responses that fail to sustain user guidance and meet expectations. Although preference optimization has proven effective in aligning LLMs with user expectations, it remains costly and performs poorly in multi-turn dialogue. To address this challenge, we introduce a novel multi-turn preference optimization (MTPO) paradigm ECPO, which leverages Expectation Confirmation Theory to explicitly model the evolution of user satisfaction throughout multi-turn dialogues, uncovering the underlying causes of dissatisfaction. These causes can be utilized to support targeted optimization of unsatisfactory responses, thereby achieving turn-level preference optimization. ECPO ingeniously eliminates the significant sampling overhead of existing MTPO methods while ensuring the optimization process drives meaningful improvements. To support ECPO, we introduce an LLM-based user simulator, AILO, to simulate user feedback and perform expectation confirmation during conversational recommendations. Experimental results show that ECPO significantly enhances CRA's interaction capabilities, delivering notable improvements in both efficiency and effectiveness over existing MTPO methods.

</details>


### [35] [Evaluation Should Not Ignore Variation: On the Impact of Reference Set Choice on Summarization Metrics](https://arxiv.org/abs/2506.14335)
*Silvia Casola,Yang Janet Liu,Siyao Peng,Oliver Kraus,Albert Gatt,Barbara Plank*

Key words: 摘要评价、参考集变异性、ROUGE、多参考摘要、大语言模型

TL;DR: 摘要研究了多参考摘要中参考集选择对评价指标的影响，发现常用指标（如ROUGE）结果不稳定，建议在评估中加入参考集变异性以提高一致性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 人类语言生成的多样性在摘要评价中常被忽略，研究旨在探讨不同参考集对评价指标的影响。

Method: 分析了SummEval、GUMSum和DUC2004三个多参考摘要数据集，评估了广泛使用的参考指标对参考集选择的敏感性。

Result: 发现许多流行指标（尤其是基于n-gram的ROUGE）结果不稳定，模型排名因参考集不同而变化。

Conclusion: 建议在摘要评价中考虑参考集变异性以提升评估一致性，尤其是在评估大语言模型时。

Abstract: Human language production exhibits remarkable richness and variation, reflecting diverse communication styles and intents. However, this variation is often overlooked in summarization evaluation. While having multiple reference summaries is known to improve correlation with human judgments, the impact of using different reference sets on reference-based metrics has not been systematically investigated. This work examines the sensitivity of widely used reference-based metrics in relation to the choice of reference sets, analyzing three diverse multi-reference summarization datasets: SummEval, GUMSum, and DUC2004. We demonstrate that many popular metrics exhibit significant instability. This instability is particularly concerning for n-gram-based metrics like ROUGE, where model rankings vary depending on the reference sets, undermining the reliability of model comparisons. We also collect human judgments on LLM outputs for genre-diverse data and examine their correlation with metrics to supplement existing findings beyond newswire summaries, finding weak-to-no correlation. Taken together, we recommend incorporating reference set variation into summarization evaluation to enhance consistency alongside correlation with human judgments, especially when evaluating LLMs.

</details>


### [36] [A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis](https://arxiv.org/abs/2506.14345)
*Bruno Martins,Piotr Szymański,Piotr Gramacki*

Key words: 大型语言模型, 深度研究系统, 地理-时间推理, AI驱动信息获取

TL;DR: 大型语言模型（LLMs）推动了深度研究系统的发展，但现有系统缺乏处理地理-时间约束的能力。本文提出下一代系统需整合地理-时间推理，并探讨技术、基础设施及评估挑战。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前深度研究系统在回答涉及地理或时间约束的复杂问题时功能不足，影响了公共健康、环境科学等领域的应用。

Method: 提出增强检索与合成过程的方案，支持地理-时间约束，并强调开放基础设施和严格评估协议的重要性。

Result: 未提供具体实验结果，但提出了未来系统的技术路线和潜在影响。

Conclusion: 整合地理-时间推理的深度研究系统有望提升AI驱动的信息获取能力，推动多领域应用。

Abstract: The emergence of Large Language Models (LLMs) has transformed information access, with current LLMs also powering deep research systems that can generate comprehensive report-style answers, through planned iterative search, retrieval, and reasoning. Still, current deep research systems lack the geo-temporal capabilities that are essential for answering context-rich questions involving geographic and/or temporal constraints, frequently occurring in domains like public health, environmental science, or socio-economic analysis. This paper reports our vision towards next generation systems, identifying important technical, infrastructural, and evaluative challenges in integrating geo-temporal reasoning into deep research pipelines. We argue for augmenting retrieval and synthesis processes with the ability to handle geo-temporal constraints, supported by open and reproducible infrastructures and rigorous evaluation protocols. Our vision outlines a path towards more advanced and geo-temporally aware deep research systems, of potential impact to the future of AI-driven information access.

</details>


### [37] [Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits](https://arxiv.org/abs/2506.14370)
*Amrit Poudel,Yifan Ding,Jurgen Pfeffer,Tim Weninger*

Key words: 搜索引擎,算法偏见,内容可见性,社交媒体,公共话语

TL;DR: 研究揭示搜索引擎（如Google）通过算法选择性推广或压制某些标签和子论坛，影响用户信息接触，存在系统性偏见。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨搜索引擎作为数字守门人如何通过算法影响社交媒体内容的可见性，进而塑造公共话语。

Method: 通过比较搜索引擎结果与Reddit和Twitter/X的非抽样数据，分析内容可见性的系统性偏差。

Result: Google算法倾向于压制与色情、阴谋论、广告和加密货币相关的内容，推广高参与度的内容。

Conclusion: Google的守门行为通过筛选社交媒体叙事影响公共话语。

Abstract: Search engines play a crucial role as digital gatekeepers, shaping the visibility of Web and social media content through algorithmic curation. This study investigates how search engines like Google selectively promotes or suppresses certain hashtags and subreddits, impacting the information users encounter. By comparing search engine results with nonsampled data from Reddit and Twitter/X, we reveal systematic biases in content visibility. Google's algorithms tend to suppress subreddits and hashtags related to sexually explicit material, conspiracy theories, advertisements, and cryptocurrencies, while promoting content associated with higher engagement. These findings suggest that Google's gatekeeping practices influence public discourse by curating the social media narratives available to users.

</details>


### [38] [ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection](https://arxiv.org/abs/2506.14371)
*Lucile Favero,Daniel Frases,Juan Antonio Pérez-Ortiz,Tanja Käser,Nuria Oliver*

Key words: 大语言模型, 批判性问题, 辩论, 论证挖掘

TL;DR: 论文探讨了利用大语言模型生成批判性问题以促进深度思考的方法，并在竞赛中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决大语言模型可能导致浅层学习的担忧，探索其在促进批判性思维发展中的潜力。

Method: 提出一个两步框架：1) 提问者生成候选问题；2) 法官选择最相关问题。使用开源小规模语言模型。

Result: 系统在竞赛中排名第一，证明其生成批判性问题的有效性。

Conclusion: 大语言模型可用于鼓励对论证文本的批判性参与。

Abstract: The widespread adoption of chat interfaces based on Large Language Models (LLMs) raises concerns about promoting superficial learning and undermining the development of critical thinking skills. Instead of relying on LLMs purely for retrieving factual information, this work explores their potential to foster deeper reasoning by generating critical questions that challenge unsupported or vague claims in debate interventions. This study is part of a shared task of the 12th Workshop on Argument Mining, co-located with ACL 2025, focused on automatic critical question generation. We propose a two-step framework involving two small-scale open source language models: a Questioner that generates multiple candidate questions and a Judge that selects the most relevant ones. Our system ranked first in the shared task competition, demonstrating the potential of the proposed LLM-based approach to encourage critical engagement with argumentative texts.

</details>


### [39] [Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding](https://arxiv.org/abs/2506.14397)
*Yeonkyoung So,Gyuseong Lee,Sungmok Jung,Joonhak Lee,JiA Kang,Sangho Kim,Jaejin Lee*

Key words: 否定理解、大语言模型、基准测试、语义理解

TL;DR: 介绍了Thunder-NUBench，一个专门评估大语言模型对句子级否定理解的基准数据集。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基准测试将否定作为次要案例处理，缺乏专门针对否定理解的评估工具。

Method: 构建了包含手动筛选的句子否定对和多选数据集的基准，涵盖多样化的否定结构。

Result: 提出了Thunder-NUBench，能够深入评估模型对否定的理解。

Conclusion: 新基准填补了否定理解领域的空白，提供了更全面的评估工具。

Abstract: Negation is a fundamental linguistic phenomenon that poses persistent challenges for Large Language Models (LLMs), particularly in tasks requiring deep semantic understanding. Existing benchmarks often treat negation as a side case within broader tasks like natural language inference, resulting in a lack of benchmarks that exclusively target negation understanding. In this work, we introduce \textbf{Thunder-NUBench}, a novel benchmark explicitly designed to assess sentence-level negation understanding in LLMs. Thunder-NUBench goes beyond surface-level cue detection by contrasting standard negation with structurally diverse alternatives such as local negation, contradiction, and paraphrase. The benchmark consists of manually curated sentence-negation pairs and a multiple-choice dataset that enables in-depth evaluation of models' negation understanding.

</details>


### [40] [ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge](https://arxiv.org/abs/2506.14407)
*Zeinab Sadat Taghavi,Ali Modarressi,Yunpu Ma,Hinrich Schütze*

Key words: 检索系统, 隐含信息, 文档端推理, 基准测试

TL;DR: 论文提出了ImpliRet基准测试，将检索系统的挑战从查询端转移到文档端，评估了一系列检索模型在处理隐含信息时的表现，发现现有模型仍面临困难。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有检索系统过于依赖表面线索，无法有效处理文档中的隐含信息（如时间、算术和常识关系）。

Method: 提出ImpliRet基准测试，通过简单查询但需文档端推理的任务，评估稀疏和密集检索模型的性能。

Result: 现有模型表现不佳，最佳nDCG@10仅为15.07%，GPT-4.1在短上下文环境中得分也仅35.06%。

Conclusion: 文档端推理仍是一个重要挑战，需要进一步研究。

Abstract: Retrieval systems are central to many NLP pipelines, but often rely on surface-level cues such as keyword overlap and lexical semantic similarity. To evaluate retrieval beyond these shallow signals, recent benchmarks introduce reasoning-heavy queries; however, they primarily shift the burden to query-side processing techniques -- like prompting or multi-hop retrieval -- that can help resolve complexity. In contrast, we present ImpliRet, a benchmark that shifts the reasoning challenge to document-side processing: The queries are simple, but relevance depends on facts stated implicitly in documents through temporal (e.g., resolving "two days ago"), arithmetic, and world knowledge relationships. We evaluate a range of sparse and dense retrievers, all of which struggle in this setting: the best nDCG@10 is only 15.07%. We also test whether long-context models can overcome this limitation. But even with a short context of only ten documents, including the positive document, GPT-4.1 scores only 35.06%, showing that document-side reasoning remains a challenge. Our codes are available at github.com/ZeinabTaghavi/IMPLIRET.Contribution.

</details>


### [41] [LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs](https://arxiv.org/abs/2506.14429)
*Xiaoran Liu,Zhigeng Liu,Zengfeng Huang,Qipeng Guo,Ziwei He,Xipeng Qiu*

Key words: 扩散LLMs, 自回归LLMs, 长上下文, RoPE, LongLLaDA

TL;DR: 本文首次系统地比较了扩散LLMs和自回归LLMs的长上下文性能，发现扩散LLMs在上下文外推时具有稳定的困惑度和局部感知现象，并提出了一种无训练方法LongLLaDA。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究扩散LLMs在长上下文任务中的表现，填补其在这一领域的空白，并提供系统分析和扩展方法。

Method: 通过实验比较扩散LLMs和自回归LLMs的长上下文性能，结合RoPE缩放理论解释现象，并提出了LongLLaDA方法。

Result: 扩散LLMs在长上下文任务中表现稳定且具有局部感知能力，LongLLaDA方法成功扩展了其上下文窗口。

Conclusion: 扩散LLMs在长上下文任务中有独特优势，LongLLaDA方法为未来研究提供了理论和实证基础。

Abstract: Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \textbf{\textit{stable perplexity}} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \textbf{\textit{local perception}} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.

</details>


### [42] [How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison](https://arxiv.org/abs/2506.14448)
*Jiayin Wang,Zhiquang Guo,Weizhi Ma,Min Zhang*

Key words: large language models, test-time learning, evaluation, semantic games

TL;DR: 论文主张评估大型语言模型在测试时学习的能力，提出语义游戏作为评估方法，发现LLMs具备测试时学习能力但进步不如人类稳定迅速。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基准测试主要评估静态知识，而测试时学习能力对实现通用人工智能至关重要。

Method: 提出语义游戏作为评估测试时学习的测试平台，并设计包含四种经验表示形式的客观评估框架。

Result: LLMs显示出可测量的测试时学习能力，但在累积经验下进步不如人类稳定迅速。

Conclusion: LLMs具有通用学习机器的潜力，但与人类仍存在显著智力差距。

Abstract: As evaluation designs of large language models may shape our trajectory toward artificial general intelligence, comprehensive and forward-looking assessment is essential. Existing benchmarks primarily assess static knowledge, while intelligence also entails the ability to rapidly learn from experience. To this end, we advocate for the evaluation of Test-time Learning, the capacity to improve performance in experience-based, reasoning-intensive tasks during test time. In this work, we propose semantic games as effective testbeds for evaluating test-time learning, due to their resistance to saturation and inherent demand for strategic reasoning. We introduce an objective evaluation framework that compares model performance under both limited and cumulative experience settings, and contains four forms of experience representation. To provide a comparative baseline, we recruit eight human participants to complete the same task. Results show that LLMs exhibit measurable test-time learning capabilities; however, their improvements are less stable under cumulative experience and progress more slowly than those observed in humans. These findings underscore the potential of LLMs as general-purpose learning machines, while also revealing a substantial intellectual gap between models and humans, irrespective of how well LLMs perform on static benchmarks.

</details>


### [43] [LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data](https://arxiv.org/abs/2506.14474)
*Eyal German,Sagiv Antebi,Edan Habler,Asaf Shabtai,Yuval Elovici*

Key words: LexiMark, 数据集水印, 大语言模型, 高熵词, 同义词替换

TL;DR: LexiMark是一种新的文本水印技术，通过替换高熵词的同义词来嵌入水印，既保持语义完整性又难以检测，显著提升了AUROC分数。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决现有数据集水印技术隐蔽性不足、易被检测和移除的问题。

Method: 提出LexiMark技术，通过替换高熵词的同义词嵌入水印，增强LLM对水印文本的记忆能力。

Result: 在多种训练设置下，LexiMark的AUROC分数显著优于现有方法。

Conclusion: LexiMark能可靠检测LLM是否未经授权使用了水印数据。

Abstract: Large language models (LLMs) can be trained or fine-tuned on data obtained without the owner's consent. Verifying whether a specific LLM was trained on particular data instances or an entire dataset is extremely challenging. Dataset watermarking addresses this by embedding identifiable modifications in training data to detect unauthorized use. However, existing methods often lack stealth, making them relatively easy to detect and remove. In light of these limitations, we propose LexiMark, a novel watermarking technique designed for text and documents, which embeds synonym substitutions for carefully selected high-entropy words. Our method aims to enhance an LLM's memorization capabilities on the watermarked text without altering the semantic integrity of the text. As a result, the watermark is difficult to detect, blending seamlessly into the text with no visible markers, and is resistant to removal due to its subtle, contextually appropriate substitutions that evade automated and manual detection. We evaluated our method using baseline datasets from recent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral 7B, Pythia 6.9B, as well as three smaller variants from the Pythia family (160M, 410M, and 1B). Our evaluation spans multiple training settings, including continued pretraining and fine-tuning scenarios. The results demonstrate significant improvements in AUROC scores compared to existing methods, underscoring our method's effectiveness in reliably verifying whether unauthorized watermarked data was used in LLM training.

</details>


### [44] [LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops](https://arxiv.org/abs/2506.14493)
*Jiyuan Fu,Kaixun Jiang,Lingyi Hong,Jinglun Li,Haijing Guo,Dingkang Yang,Zhaoyu Chen,Wenqiang Zhang*

Key words: 多模态大语言模型,资源耗尽攻击,词性标注,生成路径修剪,能耗

TL;DR: LingoLoop攻击利用词性和句法结构诱导多模态大语言模型生成冗长重复输出，显著增加计算资源和能耗。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有攻击忽略词性和句法结构对生成输出的影响，限制效果。LingoLoop通过分析这些因素提升攻击效果。

Method: 结合词性感知延迟机制（调整注意力权重）和生成路径修剪机制（限制隐藏状态），诱导模型生成冗长循环输出。

Result: 实验显示LingoLoop能增加30倍生成标记和能耗，如Qwen2.5-VL-3B模型，暴露MLLMs的严重漏洞。

Conclusion: LingoLoop揭示MLLMs的可靠性挑战，需关注防御此类攻击。

Abstract: Multimodal Large Language Models (MLLMs) have shown great promise but require substantial computational resources during inference. Attackers can exploit this by inducing excessive output, leading to resource exhaustion and service degradation. Prior energy-latency attacks aim to increase generation time by broadly shifting the output token distribution away from the EOS token, but they neglect the influence of token-level Part-of-Speech (POS) characteristics on EOS and sentence-level structural patterns on output counts, limiting their efficacy. To address this, we propose LingoLoop, an attack designed to induce MLLMs to generate excessively verbose and repetitive sequences. First, we find that the POS tag of a token strongly affects the likelihood of generating an EOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to postpone EOS token generation by adjusting attention weights guided by POS information. Second, we identify that constraining output diversity to induce repetitive loops is effective for sustained generation. We introduce a Generative Path Pruning Mechanism that limits the magnitude of hidden states, encouraging the model to produce persistent loops. Extensive experiments demonstrate LingoLoop can increase generated tokens by up to 30 times and energy consumption by a comparable factor on models like Qwen2.5-VL-3B, consistently driving MLLMs towards their maximum generation limits. These findings expose significant MLLMs' vulnerabilities, posing challenges for their reliable deployment. The code will be released publicly following the paper's acceptance.

</details>


### [45] [M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models](https://arxiv.org/abs/2506.14532)
*Can Zheng,Jiguang He,Chung G. Kang,Guofa Cai,Zitong Yu,Merouane Debbah*

Key words: M2BeamLLM, 毫米波, 大规模MIMO, 波束预测, 多模态传感器, 大型语言模型

TL;DR: 该论文提出了一个名为M2BeamLLM的新型神经网络框架，用于毫米波大规模MIMO通信系统中的波束预测，结合多模态传感器数据和大型语言模型，显著提高了预测准确性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为解决毫米波大规模MIMO通信系统中精确波束预测的挑战，作者提出了一种结合多模态传感器数据和大型语言模型的智能方法，以提高预测性能。

Method: 通过多模态传感器数据（如图像、雷达、LiDAR和GPS）的编码、对齐与融合，以及监督微调（SFT），利用GPT-2等大型语言模型的强大推理能力。

Result: M2BeamLLM在标准和小样本场景中均显著优于传统深度学习模型，且预测性能随传感器模态多样性增加而提升。

Conclusion: M2BeamLLM为车辆到基础设施（V2I）毫米波通信系统提供了一种高效且智能的波束预测解决方案。

Abstract: This paper introduces a novel neural network framework called M2BeamLLM for beam prediction in millimeter-wave (mmWave) massive multi-input multi-output (mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data, including images, radar, LiDAR, and GPS, leveraging the powerful reasoning capabilities of large language models (LLMs) such as GPT-2 for beam prediction. By combining sensing data encoding, multimodal alignment and fusion, and supervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam prediction accuracy and robustness, demonstrably outperforming traditional deep learning (DL) models in both standard and few-shot scenarios. Furthermore, its prediction performance consistently improves with increased diversity in sensing modalities. Our study provides an efficient and intelligent beam prediction solution for vehicle-to-infrastructure (V2I) mmWave communication systems.

</details>


### [46] [AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs](https://arxiv.org/abs/2506.14562)
*Di He,Ajay Jaiswal,Songjun Tu,Li Shen,Ganzhao Yuan,Shiwei Liu,Lu Yin*

Key words: weight decay, language models, adaptive regularization, spectral properties, AlphaDecay

TL;DR: AlphaDecay 是一种自适应为不同模块分配不同权重衰减强度的方法，基于模块的谱特性差异性，显著提升了语言模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统的权重衰减方法为所有层统一分配衰减率，忽视了语言模型的结构多样性和模块间谱特性的差异。

Method: AlphaDecay 通过分析权重相关矩阵的经验谱密度（ESD）的“重尾性”，自适应地为每个模块分配不同的衰减强度。

Result: 在 60M 到 1B 参数的模型上，AlphaDecay 在困惑度和泛化能力上均优于传统统一衰减和其他自适应衰减方法。

Conclusion: AlphaDecay 通过模块化的自适应权重衰减，平衡了谱特性的差异，显著提升了模型的性能。

Abstract: Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines.

</details>


### [47] [GenerationPrograms: Fine-grained Attribution with Executable Programs](https://arxiv.org/abs/2506.14580)
*David Wan,Eran Hirsch,Elias Stengel-Eskin,Ido Dagan,Mohit Bansal*

Key words: 大语言模型, 归因, 模块化生成, 解释性

TL;DR: 论文提出了一种模块化生成框架GenerationPrograms，通过在两个阶段中明确分解生成过程，显著提升了文本生成的可验证性和解释性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决现有大语言模型在文本生成中无法提供细粒度归因的问题，提升可验证性和信任度。

Method: 采用模块化生成框架，将生成过程分为程序计划创建和执行两个阶段，包含文本操作模块如复述、压缩和融合。

Result: 在多个任务中显著提升了文档和句子层面的归因质量，并能作为后处理归因方法优于传统技术。

Conclusion: GenerationPrograms通过可解释的程序和模块化改进，有效提升了文本生成的可验证性和归因质量。

Abstract: Recent large language models (LLMs) achieve impressive performance in source-conditioned text generation but often fail to correctly provide fine-grained attributions for their outputs, undermining verifiability and trust. Moreover, existing attribution methods do not explain how and why models leverage the provided source documents to generate their final responses, limiting interpretability. To overcome these challenges, we introduce a modular generation framework, GenerationPrograms, inspired by recent advancements in executable "code agent" architectures. Unlike conventional generation methods that simultaneously generate outputs and attributions or rely on post-hoc attribution, GenerationPrograms decomposes the process into two distinct stages: first, creating an executable program plan composed of modular text operations (such as paraphrasing, compression, and fusion) explicitly tailored to the query, and second, executing these operations following the program's specified instructions to produce the final response. Empirical evaluations demonstrate that GenerationPrograms significantly improves attribution quality at both the document level and sentence level across two long-form question-answering tasks and a multi-document summarization task. We further demonstrate that GenerationPrograms can effectively function as a post-hoc attribution method, outperforming traditional techniques in recovering accurate attributions. In addition, the interpretable programs generated by GenerationPrograms enable localized refinement through modular-level improvements that further enhance overall attribution quality.

</details>


### [48] [Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees](https://arxiv.org/abs/2506.14606)
*Ahmed Heakl,Sarim Hashmi,Chaimaa Abi,Celine Lee,Abdulrahman Mahmoud*

Key words: 指令集架构,代码翻译,大型语言模型,软件测试

TL;DR: GG是一种结合预训练大语言模型和软件测试框架的指令集架构（ISA）转译方法，显著提升了CISC到RISC代码翻译的速度和能效。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决复杂指令集（CISC）和精简指令集（RISC）间代码翻译的挑战，提升代码的可移植性和长期可用性。

Method: 使用预训练大语言模型生成候选翻译，并嵌入软件测试框架以量化翻译的可靠性。

Result: 在HumanEval程序上实现99%的正确率，在BringupBench上为49%，优于Rosetta 2的性能和能效。

Conclusion: GG为ISA级代码翻译提供了高效解决方案，并计划开源相关资源以推动研究。

Abstract: The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.

</details>


### [49] [When Does Meaning Backfire? Investigating the Role of AMRs in NLI](https://arxiv.org/abs/2506.14613)
*Junghyun Min,Xiulin Yang,Shira Wein*

Key words: NLI, AMR, 语义推理, 微调, 提示

TL;DR: 研究探讨了在自然语言推理（NLI）中引入抽象意义表示（AMR）是否有助于预训练语言模型的泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索AMR是否能为NLI任务提供语义信息，提升模型性能。

Method: 在微调和提示设置中集成AMR，分析其对模型性能的影响。

Result: 微调中使用AMR会阻碍模型泛化，而提示设置中AMR带来轻微提升；但提升源于表面差异放大，而非语义推理。

Conclusion: AMR在提示设置中对语义推理的帮助有限，可能误导模型预测。

Abstract: Natural Language Inference (NLI) relies heavily on adequately parsing the semantic content of the premise and hypothesis. In this work, we investigate whether adding semantic information in the form of an Abstract Meaning Representation (AMR) helps pretrained language models better generalize in NLI. Our experiments integrating AMR into NLI in both fine-tuning and prompting settings show that the presence of AMR in fine-tuning hinders model generalization while prompting with AMR leads to slight gains in \texttt{GPT-4o}. However, an ablation study reveals that the improvement comes from amplifying surface-level differences rather than aiding semantic reasoning. This amplification can mislead models to predict non-entailment even when the core meaning is preserved.

</details>


### [50] [Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models](https://arxiv.org/abs/2506.14625)
*Chenchen Yuan,Zheyu Zhang,Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Key words: 大语言模型, 道德判断, 集体共识, 嵌入优化, 数据驱动对齐

TL;DR: 该论文提出了一种框架，通过综合多个大语言模型（LLM）的道德判断形成集体共识，并通过嵌入优化调整偏离共识的模型，从而提高道德判断的一致性和准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大语言模型在复杂多因素的道德困境中判断不一致，需要一种方法将其道德判断对齐以提升一致性和安全性。

Method: 提出一个框架，融合多个LLM的道德判断，形成集体共识，并通过嵌入优化调整偏离共识的模型。

Result: 实验表明该方法能建立稳健的共识并提升各模型的一致性。

Conclusion: 多模型的数据驱动道德对齐是一种有效的方法，可为AI系统提供更安全、一致的道德判断。

Abstract: Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs' moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems.

</details>


### [51] [AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation](https://arxiv.org/abs/2506.14634)
*Leah von der Heyde,Anna-Carolina Haensch,Bernd Weiß,Jessika Daikeler*

Key words: LLMs, open-ended responses, survey research, automated classification, German data

TL;DR: 探讨LLMs在调查研究中用于分类开放性问题回答的潜力，以德国调查数据为例，比较不同LLMs和提示方法的表现，并与专家编码对比。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 评估LLMs在多语言和非简单主题下分类开放性问题回答的效果，填补现有研究空白。

Method: 采用多个最新LLMs和提示方法，使用德语调查数据，并与人类专家编码进行对比。

Result: 不同LLMs性能差异显著，仅微调后的LLM达到满意水平；提示方法效果依赖于特定LLM。

Conclusion: LLMs在分类开放性问题时需权衡多种因素，微调是关键，适用于特定场景。

Abstract: The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research.

</details>


### [52] [Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot](https://arxiv.org/abs/2506.14641)
*Xiang Cheng,Chengyan Pan,Minjun Zhao,Deyang Li,Fangchao Liu,Xinyu Zhang,Xiao Zhang,Yong Liu*

Key words: In-Context Learning, Chain-of-Thought, Large Language Models, Mathematical Reasoning

TL;DR: 研究表明，对于近期强大的语言模型（如Qwen2.5系列），传统的链式思维示例（CoT）并不能提升数学推理能力，其主要作用是调整输出格式。即使是增强版CoT示例也无效，模型倾向于忽略示例而专注于指令。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探究链式思维示例（CoT）在当前强大语言模型中的有效性，尤其是在数学推理任务中。

Method: 通过系统实验，比较传统CoT和增强版CoT示例对模型性能的影响，分析模型行为。

Result: 传统和增强版CoT示例均未提升模型推理能力，模型主要关注指令而非示例。

Conclusion: 当前ICL+CoT框架在数学推理中存在局限性，需要重新审视ICL范式及示例定义。

Abstract: In-Context Learning (ICL) is an essential emergent ability of Large Language Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars of ICL to enhance the reasoning capability, especially in mathematics tasks. However, given the continuous advancement of model capabilities, it remains unclear whether CoT exemplars still benefit recent, stronger models in such tasks. Through systematic experiments, we find that for recent strong models such as the Qwen2.5 series, adding traditional CoT exemplars does not improve reasoning performance compared to Zero-Shot CoT. Instead, their primary function is to align the output format with human expectations. We further investigate the effectiveness of enhanced CoT exemplars, constructed using answers from advanced models such as \texttt{Qwen2.5-Max} and \texttt{DeepSeek-R1}. Experimental results indicate that these enhanced exemplars still fail to improve the model's reasoning performance. Further analysis reveals that models tend to ignore the exemplars and focus primarily on the instructions, leading to no observable gain in reasoning ability. Overall, our findings highlight the limitations of the current ICL+CoT framework in mathematical reasoning, calling for a re-examination of the ICL paradigm and the definition of exemplars.

</details>


### [53] [Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments](https://arxiv.org/abs/2506.14645)
*. Pazzaglia,V. Vendetti,L. D. Comencini,F. Deriu,V. Modugno*

Key words: 大语言模型, 意识形态极化, AI伦理, 政治话语, 微调

TL;DR: 研究探讨了微调大语言模型如何加剧意识形态极化，通过生成高度可信的党派内容，引发对AI伦理和治理的关注。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大语言模型（LLMs）可能通过生成偏见的、有说服力的内容加剧意识形态极化，研究旨在验证这一假设。

Method: 使用Reddit的政治讨论数据集微调开源LLM，生成与意识形态一致的回应，通过语言学分析、情感评分和人工标注评估输出。

Result: 微调后的LLM能生成与人类写作高度相似且极具煽动性的内容，模糊了AI与人类言论的界限。

Conclusion: 研究呼吁关注AI在政治话语中的伦理风险，建议加强AI治理、平台监管和检测工具开发。

Abstract: The increasing sophistication of large language models (LLMs) has sparked growing concerns regarding their potential role in exacerbating ideological polarization through the automated generation of persuasive and biased content. This study explores the extent to which fine-tuned LLMs can replicate and amplify polarizing discourse within online environments. Using a curated dataset of politically charged discussions extracted from Reddit, we fine-tune an open-source LLM to produce context-aware and ideologically aligned responses. The model's outputs are evaluated through linguistic analysis, sentiment scoring, and human annotation, with particular attention to credibility and rhetorical alignment with the original discourse. The results indicate that, when trained on partisan data, LLMs are capable of producing highly plausible and provocative comments, often indistinguishable from those written by humans. These findings raise significant ethical questions about the use of AI in political discourse, disinformation, and manipulation campaigns. The paper concludes with a discussion of the broader implications for AI governance, platform regulation, and the development of detection tools to mitigate adversarial fine-tuning risks.

</details>


### [54] [GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors](https://arxiv.org/abs/2506.14646)
*Hengyuan Zhang,Xinrong Chen,Yingmin Qiu,Xiao Liang,Ziyue Li,Guanyu Wang,Weiping Li,Tong Mo,Wenyue Li,Hayden Kwok-Hay So,Ngai Wong*

Key words: 参数高效微调, LoRA, Mixture-of-Experts, 动态分配

TL;DR: 论文提出GuiLoMo，一种通过细粒度层间专家数量和等级分配策略改进LoRA-MoE的方法，解决现有方法在下游任务中专家分配和统一等级限制的问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有LoRA-MoE方法在参数效率和模型容量之间存在限制，尤其是在专家数量和等级分配上缺乏灵活性。

Method: 引入GuidedSelection Vectors (GSVs)，通过双层优化学习模型和任务需求，动态分配专家数量和等级。

Result: 在多个基准测试中，GuiLoMo表现优于或与基线方法相当，展示了自适应专家配置的优势。

Conclusion: GuiLoMo通过动态调整专家数量和等级，提高了参数效率模型的性能和多样性。

Abstract: Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), offer an efficient way to adapt large language models with reduced computational costs. However, their performance is limited by the small number of trainable parameters. Recent work combines LoRA with the Mixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two limitations remain in hindering the full exploitation of its potential: 1) the influence of downstream tasks when assigning expert numbers, and 2) the uniform rank assignment across all LoRA experts, which restricts representational diversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained layer-wise expert numbers and ranks allocation strategy with GuidedSelection Vectors (GSVs). GSVs are learned via a prior bilevel optimization process to capture both model- and task-specific needs, and are then used to allocate optimal expert numbers and ranks. Experiments on three backbone models across diverse benchmarks show that GuiLoMo consistently achieves superior or comparable performance to all baselines. Further analysis offers key insights into how expert numbers and ranks vary across layers and tasks, highlighting the benefits of adaptive expert configuration. Our code is available at https://github.com/Liar406/Gui-LoMo.git.

</details>


### [55] [Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality](https://arxiv.org/abs/2506.14681)
*Yuto Harada,Yusuke Yamauchi,Yusuke Oda,Yohei Oseki,Yusuke Miyao,Yu Takagi*

Key words: 监督微调, 大语言模型, 困惑度, 权重变化

TL;DR: 监督微调（SFT）是调整大语言模型（LLM）与人指令和价值观的关键步骤，但仍有很多未解之处。通过训练多种基础模型，分析了数据集属性与层间修改的重要性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究监督微调（SFT）的性能关键因素，以更好地理解其对大语言模型的影响。

Method: 在多种数据集上训练了1000多个SFT模型，分析数据集属性和层间权重变化。

Result: 发现困惑度能有效预测SFT表现，且中层权重变化与性能提升最相关。

Conclusion: SFT效果依赖于特定任务和模型策略，困惑度和中层权重变化是重要指标。

Abstract: Supervised fine-tuning (SFT) is a critical step in aligning large language models (LLMs) with human instructions and values, yet many aspects of SFT remain poorly understood. We trained a wide range of base models on a variety of datasets including code generation, mathematical reasoning, and general-domain tasks, resulting in 1,000+ SFT models under controlled conditions. We then identified the dataset properties that matter most and examined the layer-wise modifications introduced by SFT. Our findings reveal that some training-task synergies persist across all models while others vary substantially, emphasizing the importance of model-specific strategies. Moreover, we demonstrate that perplexity consistently predicts SFT effectiveness--often surpassing superficial similarity between trained data and benchmark--and that mid-layer weight changes correlate most strongly with performance gains. We will release these 1,000+ SFT models and benchmark results to accelerate further research.

</details>


### [56] [Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers](https://arxiv.org/abs/2506.14702)
*Daniel D'souza,Julia Kreutzer,Adrien Morisot,Ahmet Üstün,Sara Hooker*

Key words: 机器学习, 长尾数据, 模型控制, 生成模型, 微调

TL;DR: 论文提出一种新方法，通过优化训练协议和引入控制杠杆，显著提升模型在长尾数据上的性能和可控性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决现代机器学习中模型在罕见和低代表性特征上表现不佳的问题，提高模型在长尾用例中的性能与可控性。

Method: 通过数据特征和任务来源的详细分类，显式控制生成属性，并微调基础模型以自动推断这些标记。

Result: 在开放生成质量上平均提升5.7%，在低代表性领域提升9.1%，特定任务如CodeRepair提升14.1%。

Conclusion: 该方法能显著改善模型在长尾数据和特定任务上的表现，同时增强用户对生成结果的控制。

Abstract: One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: "Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations.

</details>


### [57] [Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data](https://arxiv.org/abs/2506.14704)
*Anton Changalidis,Aki Härmä*

Key words: 生成式变换器,记忆能力,嵌入尺寸,激活函数,SNOMED

TL;DR: 研究模型架构和数据配置对生成式变换器记忆能力的影响，发现嵌入尺寸是关键因素，而增加层数可能影响性能，激活函数和数据集复杂性也有重要影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨模型架构和数据配置如何影响生成式变换器的记忆能力，以优化模型设计。

Method: 使用基于SNOMED知识图的合成文本数据集训练模型，包括静态连接的triplets和复杂关系的序列。

Result: 嵌入尺寸是学习速度和容量的主要决定因素，Softmax稳定性更高，数据集复杂性提升记忆能力。

Conclusion: 研究结果为理解变换器记忆机制和优化模型设计提供了框架。

Abstract: This paper studies how the model architecture and data configurations influence the empirical memorization capacity of generative transformers. The models are trained using synthetic text datasets derived from the Systematized Nomenclature of Medicine (SNOMED) knowledge graph: triplets, representing static connections, and sequences, simulating complex relation patterns. The results show that embedding size is the primary determinant of learning speed and capacity, while additional layers provide limited benefits and may hinder performance on simpler datasets. Activation functions play a crucial role, and Softmax demonstrates greater stability and capacity. Furthermore, increasing the complexity of the data set seems to improve the final memorization. These insights improve our understanding of transformer memory mechanisms and provide a framework for optimizing model design with structured real-world data.

</details>


### [58] [Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.14731)
*Ring Team,Bin Hu,Cai Chen,Deng Zhao,Ding Liu,Dingnan Jin,Feng Zhu,Hao Dai,Hongzhi Luan,Jia Guo,Jiaming Liu,Jiewei Wu,Jun Mei,Jun Zhou,Junbo Zhao,Junwu Xiong,Kaihong Zhang,Kuan Xu,Lei Liang,Liang Jiang,Liangcheng Fu,Longfei Zheng,Qiang Gao,Qing Cui,Quan Wan,Shaomian Zheng,Shuaicheng Li,Tongkai Yang,Wang Ren,Xiaodong Yan,Xiaopei Wan,Xiaoyun Feng,Xin Zhao,Xinxing Yang,Xinyu Kong,Xuemin Yang,Yang Li,Yingting Wu,Yongkang Liu,Zhankai Xu,Zhenduo Zhang,Zhenglei Zhou,Zhenyu Huang,Zhiqiang Zhang,Zihao Wang,Zujie Wen*

Key words: Ring-lite, Mixture-of-Experts, 强化学习, 推理能力, C3PO

TL;DR: Ring-lite是一个基于Mixture-of-Experts (MoE)的大型语言模型，通过强化学习优化，实现高效稳健的推理能力。其性能媲美SOTA小规模推理模型，同时仅激活三分之一参数。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 旨在通过联合训练蒸馏与强化学习，解决MoE RL训练中的未记录挑战，提升模型性能与效率。

Method: 提出C3PO方法增强训练稳定性，基于熵损失选择蒸馏检查点，并开发两阶段训练范式以协调多域数据。

Result: 在AIME等基准测试中表现优异，激活参数仅为同类模型的三分之一。

Conclusion: Ring-lite展示了高效推理潜力，通过算法-系统协同设计及多域数据整合，解决了训练中的关键问题。

Abstract: We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.

</details>


### [59] [Reasoning with Exploration: An Entropy Perspective](https://arxiv.org/abs/2506.14758)
*Daixuan Cheng,Shaohan Huang,Xuekai Zhu,Bo Dai,Wayne Xin Zhao,Zhenliang Zhang,Furu Wei*

Key words: 强化学习, 语言模型, 熵, 探索性推理, Pass@K

TL;DR: 该论文通过研究熵与语言模型探索性推理的关系，提出了一种简单但有效的增强语言模型推理能力的方法，通过调整优势函数中的熵项显著提升了模型的推理性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前强化学习在语言模型推理中存在探索不足的问题，导致性能瓶颈。论文旨在通过熵信号改善模型的探索性推理。

Method: 分析了高熵区域与探索性推理行为的关系，并通过在标准强化学习的优势函数中添加基于熵的项来增强探索。

Result: 实验表明，该方法在Pass@K指标上取得了显著提升，尤其是在大K值下，突破了语言模型推理能力的边界。

Conclusion: 通过熵信号增强探索性推理是一种简单且有效的方法，能够显著提升语言模型的推理能力。

Abstract: Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing language model (LM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LMs. Through empirical analysis, we uncover strong positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LM reasoning.

</details>


### [60] [From Bytes to Ideas: Language Modeling with Autoregressive U-Nets](https://arxiv.org/abs/2506.14761)
*Mathurin Videau,Badr Youbi Idrissi,Alessandro Leite,Marc Schoenauer,Olivier Teytaud,David Lopez-Paz*

Key words: tokenization, autoregressive U-Net, multi-scale, BPE, dynamic embedding

TL;DR: 论文提出了一种自适应多尺度分词的自回归U-Net模型，取代了传统的静态分词方法（如BPE），使模型能够在训练过程中动态调整分词粒度。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统分词方法（如BPE）的分词粒度固定，限制了模型对数据的操作方式和预测能力。本研究的动机是设计一种更灵活的分词方式，使模型能够动态适应不同粒度的输入。

Method: 提出了一种基于自回归U-Net的模型，能够从原始字节开始，逐步将输入聚合成单词、词组等不同尺度的表示，并在更深层次中预测更远的未来信息。

Result: 在严格控制预训练计算的情况下，浅层结构与强BPE基线相当，深层结构显示出潜力。模型能够同时处理字符级任务和跨低资源语言的知识迁移。

Conclusion: 动态多尺度分词比固定分词更具灵活性，为语言模型提供了更广泛的适应能力。

Abstract: Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.

</details>


### [61] [A Variational Framework for Improving Naturalness in Generative Spoken Language Models](https://arxiv.org/abs/2506.14767)
*Li-Wei Chen,Takuya Higuchi,Zakaria Aldeneh,Ahmed Hussen Abdelaziz,Alexander Rudnicky*

Key words: 语音建模, 变分方法, 语义标记, 韵律信息, 自动编码

TL;DR: 论文提出了一种端到端的变分方法，自动学习编码连续语音属性以增强语义标记，解决了现有方法中缺乏韵律信息的问题，且无需人工提取特征。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基于自监督模型的语音标记方法通常忽略韵律信息，导致生成的语音自然度降低，需通过手动添加特征（如音高）来弥补，但效果有限。

Method: 采用端到端变分方法，自动学习编码连续语音属性（如韵律）以增强语义标记，避免了手动特征提取和选择。

Result: 该方法生成的语音在人类评价中更受偏好，且无需人工干预。

Conclusion: 提出的方法成功解决了语音生成中韵律信息缺失的问题，提升了语音的自然度，并通过自动学习简化了流程。

Abstract: The success of large language models in text processing has inspired their adaptation to speech modeling. However, since speech is continuous and complex, it is often discretized for autoregressive modeling. Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information. As a result, models trained on these tokens can generate speech with reduced naturalness. Existing approaches try to fix this by adding pitch features to the semantic tokens. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens. Our approach eliminates the need for manual extraction and selection of paralinguistic features. Moreover, it produces preferred speech continuations according to human raters. Code, samples and models are available at https://github.com/b04901014/vae-gslm.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [62] [LittleBit: Ultra Low-Bit Quantization via Latent Factorization](https://arxiv.org/abs/2506.13771)
*Banseok Lee,Dongkyu Kim,Youngcheon You,Youngmin Kim*

Key words: 大语言模型, 量化, 极端压缩, 低秩矩阵分解, 多尺度补偿

TL;DR: LittleBit提出了一种极端的LLM压缩方法，能够在0.1位每权重（BPW）的水平下显著减少内存占用，同时通过多尺度补偿机制和量化感知训练的稳定初始化保持性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决大语言模型（LLM）在部署时面临的高内存和计算成本问题，尤其是在低于1位的量化场景下性能下降的难题。

Method: 提出低秩矩阵分解和二值化权重的方法，结合多尺度补偿机制和Dual-SVID技术进行量化感知训练的稳定初始化，以及集成残差补偿以减少误差。

Result: 在0.1 BPW的极端量化水平下，实现了将近31倍的内存减少，例如将Llama2-13B模型压缩到0.9 GB以下，性能优于现有方法的0.7 BPW。

Conclusion: LittleBit为在资源受限环境中部署强大的LLM提供了可行的解决方案，展现出优越的大小与性能权衡。

Abstract: Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. This establishes a superior size-performance trade-off, with kernel-level benchmarks indicating potential for a 5$\times$ speedup compared to FP16. LittleBit paves the way for deploying powerful LLMs in resource-constrained environments.

</details>


### [63] [MobiEdit: Resource-efficient Knowledge Editing for Personalized On-device LLMs](https://arxiv.org/abs/2506.13772)
*Zhenyan Lu,Daliang Xu,Dongqi Cai,Zexi Li,Wei Liu,Fangming Liu,Shangguang Wang,Mengwei Xu*

Key words: 大型语言模型、知识编辑、移动设备、量化梯度估计、效率优化

TL;DR: MobiEdit是一种移动知识编辑框架，通过量化前向梯度估计替代传统反向传播，实现在移动设备上的高效LLM个性化编辑。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决大型语言模型（LLMs）在移动设备上处理个性化查询时的幻觉问题，现有方法因资源消耗高而不实用。

Method: 采用量化前向梯度估计替代反向传播，结合早期停止机制和前缀缓存优化。

Result: 在商用移动设备上，实现3B参数模型的实时编辑，内存减少7.6倍，能耗降低14.7倍，延迟减少3.6倍。

Conclusion: MobiEdit显著提升了移动设备上知识编辑的效率和实用性。

Abstract: Large language models (LLMs) are deployed on mobile devices to power killer applications such as intelligent assistants. LLMs pre-trained on general corpora often hallucinate when handling personalized or unseen queries, leading to incorrect or outdated responses. Knowledge editing addresses this by identifying and adjusting a small crucial portion of model weights, without compromising the general knowledge. However, prior knowledge editing methods are impractical to run on local devices due to the resource-heavy backpropagation (BP) needed for updates. We present MobiEdit, the first mobile knowledge editing framework that enables efficient LLM personalization on commercial off-the-shelf (COTS) mobile devices. MobiEdit replaces full-precision BP with quantized forward-only gradient estimation, thus compatible with the energy-efficient mobile neural processing units (NPUs). MobiEdit replaces full-precision backpropagation with quantized forward-only gradient estimation, making it compatible with energy-efficient mobile NPUs. To further improve gradient estimation efficiency, we introduce two optimizations: an early stoping mechanism that adaptively terminates editing upon success and a prefix cache that reuses computation across steps. Our approach enables real-time editing of a 3B-parameter model (Qwen2.5-3B-Instruct) on COTS mobile devices with 7.6$\times$ less memory, 14.7 $\times$ less energy and 3.6$\times$ less latency compared to previous knowledge editing methods.

</details>


### [64] [Solving the Job Shop Scheduling Problem with Graph Neural Networks: A Customizable Reinforcement Learning Environment](https://arxiv.org/abs/2506.13781)
*Pablo Ariño Fernández*

Key words: 车间调度,图神经网络,模块化库,模仿学习,强化学习

TL;DR: 本文介绍了JobShopLib，一个模块化库，用于解决NP-hard的车间调度问题，支持自定义因素并通过模仿学习训练调度器，展示了模型的优越性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统车间调度问题使用基于简单启发式的优先级分配规则，而深度学习模型（尤其是图神经网络）试图从数据中学习优先级分配。但目前缺乏模块化实验库，导致研究耗时。

Method: 提出JobShopLib模块化库，支持自定义图表示、节点特征、动作空间和奖励函数，并提供强化学习环境。通过模仿学习训练多个调度器。

Result: 一个仅使用单个操作特征的模型优于多种基于图的调度器，GNN模型在大规模问题上接近最先进水平，表明模型改进空间大。

Conclusion: JobShopLib为未来实验提供了必要工具，显示了深度学习模型在车间调度问题中的潜力。

Abstract: The job shop scheduling problem is an NP-hard combinatorial optimization problem relevant to manufacturing and timetabling. Traditional approaches use priority dispatching rules based on simple heuristics. Recent work has attempted to replace these with deep learning models, particularly graph neural networks (GNNs), that learn to assign priorities from data. However, training such models requires customizing numerous factors: graph representation, node features, action space, and reward functions. The lack of modular libraries for experimentation makes this research time-consuming. This work introduces JobShopLib, a modular library that allows customizing these factors and creating new components with its reinforcement learning environment. We trained several dispatchers through imitation learning to demonstrate the environment's utility. One model outperformed various graph-based dispatchers using only individual operation features, highlighting the importance of feature customization. Our GNN model achieved near state-of-the-art results on large-scale problems. These results suggest significant room for improvement in developing such models. JobShopLib provides the necessary tools for future experimentation.

</details>


### [65] [Enhancing Bagging Ensemble Regression with Data Integration for Time Series-Based Diabetes Prediction](https://arxiv.org/abs/2506.13786)
*Vuong M. Ngo,Tran Quang Vinh,Patricia Kearney,Mark Roantree*

Key words: 糖尿病、时间序列预测、集成学习、EBMBag+、数据整合

TL;DR: 该研究通过集成学习方法EBMBag+预测美国城市糖尿病患病率，表现优于其他基线模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 准确预测糖尿病患病率对医疗规划和干预至关重要，但现有数据常不完整。

Method: 整合2011-2021年糖尿病相关数据，提出EBMBag+集成回归模型进行时间序列预测。

Result: EBMBag+表现最佳，MAE为0.41，RMSE为0.53，MAPE为4.01，R2为0.9。

Conclusion: EBMBag+模型在糖尿病患病率预测中效果显著，优于传统方法。

Abstract: Diabetes is a chronic metabolic disease characterized by elevated blood glucose levels, leading to complications like heart disease, kidney failure, and nerve damage. Accurate state-level predictions are vital for effective healthcare planning and targeted interventions, but in many cases, data for necessary analyses are incomplete. This study begins with a data engineering process to integrate diabetes-related datasets from 2011 to 2021 to create a comprehensive feature set. We then introduce an enhanced bagging ensemble regression model (EBMBag+) for time series forecasting to predict diabetes prevalence across U.S. cities. Several baseline models, including SVMReg, BDTree, LSBoost, NN, LSTM, and ERMBag, were evaluated for comparison with our EBMBag+ algorithm. The experimental results demonstrate that EBMBag+ achieved the best performance, with an MAE of 0.41, RMSE of 0.53, MAPE of 4.01, and an R2 of 0.9.

</details>


### [66] [Hybrid Meta-Learning Framework for Anomaly Forecasting in Nonlinear Dynamical Systems via Physics-Inspired Simulation and Deep Ensembles](https://arxiv.org/abs/2506.13828)
*Abdullah Burkan Bereketoglu*

Key words: 混合元学习, 非线性动态系统, 异常检测, 预测, CNN-LSTM, VAE, DA-RNN

TL;DR: 提出了一种混合元学习框架，用于非线性和非平稳动态系统中的预测与异常检测，整合了多种模型和方法，表现优于单一模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决非线性动态系统中非平稳和随机行为导致的预测和异常检测难题，提供数据驱动的通用方法。

Method: 结合物理模拟器生成数据，使用CNN-LSTM提取时空特征，VAE进行无监督异常评分，DA-RNN进行预测，并通过元学习器整合结果。

Result: 混合框架在异常定位、泛化能力和鲁棒性上优于单一模型。

Conclusion: 该框架为非线性系统早期缺陷识别和预测监控提供了一种数据驱动的通用解决方案。

Abstract: We propose a hybrid meta-learning framework for forecasting and anomaly detection in nonlinear dynamical systems characterized by nonstationary and stochastic behavior. The approach integrates a physics-inspired simulator that captures nonlinear growth-relaxation dynamics with random perturbations, representative of many complex physical, industrial, and cyber-physical systems. We use CNN-LSTM architectures for spatio-temporal feature extraction, Variational Autoencoders (VAE) for unsupervised anomaly scoring, and Isolation Forests for residual-based outlier detection in addition to a Dual-Stage Attention Recurrent Neural Network (DA-RNN) for one-step forecasting on top of the generated simulation data. To create composite anomaly forecasts, these models are combined using a meta-learner that combines forecasting outputs, reconstruction errors, and residual scores. The hybrid ensemble performs better than standalone models in anomaly localization, generalization, and robustness to nonlinear deviations, according to simulation-based experiments. The framework provides a broad, data-driven approach to early defect identification and predictive monitoring in nonlinear systems, which may be applied to a variety of scenarios where complete physical models might not be accessible.

</details>


### [67] [Quantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation](https://arxiv.org/abs/2506.13831)
*Jitian Zhao,Chenghui Li,Frederic Sala,Karl Rohe*

Key words: 概念解释、假设检验、CLIP、虚假相关

TL;DR: 本文提出了一种基于假设检验的概念分解方法，用于解释CLIP嵌入空间中的概念，提高了统计严谨性和重建准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前概念解释方法缺乏统计严谨性，难以验证概念和比较不同技术，需改进。

Method: 引入了假设检验框架量化嵌入空间结构，并提出一种事后概念分解方法。

Result: 该方法在重建误差上优于现有技术，并在去除非因果背景概念后最差组准确率提高了22.6%。

Conclusion: 该方法在解释性和重建准确性之间取得平衡，并能有效缓解数据中的虚假线索。

Abstract: Concept-based approaches, which aim to identify human-understandable concepts within a model's internal representations, are a promising method for interpreting embeddings from deep neural network models, such as CLIP. While these approaches help explain model behavior, current methods lack statistical rigor, making it challenging to validate identified concepts and compare different techniques. To address this challenge, we introduce a hypothesis testing framework that quantifies rotation-sensitive structures within the CLIP embedding space. Once such structures are identified, we propose a post-hoc concept decomposition method. Unlike existing approaches, it offers theoretical guarantees that discovered concepts represent robust, reproducible patterns (rather than method-specific artifacts) and outperforms other techniques in terms of reconstruction error. Empirically, we demonstrate that our concept-based decomposition algorithm effectively balances reconstruction accuracy with concept interpretability and helps mitigate spurious cues in data. Applied to a popular spurious correlation dataset, our method yields a 22.6% increase in worst-group accuracy after removing spurious background concepts.

</details>


### [68] [Evolvable Conditional Diffusion](https://arxiv.org/abs/2506.13834)
*Zhao Wei,Chin Chun Ooi,Abhishek Gupta,Jian Cheng Wong,Pao-Hsiung Chiu,Sheares Xue Wen Toh,Yew-Soon Ong*

Key words: 条件扩散,多物理模型,进化算法,科学发现

TL;DR: 本文提出了一种可进化的条件扩散方法，用于指导生成过程，以支持自主科学发现，特别是针对不可微分的多物理模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在利用不依赖于梯度的黑盒多物理模型进行生成过程指导，以解决科学领域中常见的非可微分问题。

Method: 通过优化描述去噪分布统计量的期望适应度函数，提出了一种基于概率进化的进化引导方法。

Result: 在流体拓扑和超表面的自动设计中验证了方法的有效性，生成的设计更满足优化目标。

Conclusion: 该方法为基于指导的扩散提供了一种有效手段，能够利用科学中常见的非可微分多物理模型。

Abstract: This paper presents an evolvable conditional diffusion method such that black-box, non-differentiable multi-physics models, as are common in domains like computational fluid dynamics and electromagnetics, can be effectively used for guiding the generative process to facilitate autonomous scientific discovery. We formulate the guidance as an optimization problem where one optimizes for a desired fitness function through updates to the descriptive statistic for the denoising distribution, and derive an evolution-guided approach from first principles through the lens of probabilistic evolution. Interestingly, the final derived update algorithm is analogous to the update as per common gradient-based guided diffusion models, but without ever having to compute any derivatives. We validate our proposed evolvable diffusion algorithm in two AI for Science scenarios: the automated design of fluidic topology and meta-surface. Results demonstrate that this method effectively generates designs that better satisfy specific optimization objectives without reliance on differentiable proxies, providing an effective means of guidance-based diffusion that can capitalize on the wealth of black-box, non-differentiable multi-physics numerical models common across Science.

</details>


### [69] [Robustness of Reinforcement Learning-Based Traffic Signal Control under Incidents: A Comparative Study](https://arxiv.org/abs/2506.13836)
*Dang Viet Anh Nguyen,Carlos Lima Azevedo,Tomer Toledo,Filipe Rodrigues*

Key words: 强化学习, 交通信号控制, 鲁棒性, 仿真框架, 交通事件

TL;DR: 本文介绍了T-REX，一个基于SUMO的开源仿真框架，用于在动态交通事件场景下训练和评估强化学习交通信号控制（RL-TSC）方法的鲁棒性。研究发现，虽然独立值基方法和分散压力基方法在稳定交通条件下表现良好，但在事件驱动的分布偏移下性能显著下降，而分层协调方法在大规模不规则网络中表现更稳定。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的RL-TSC方法在真实世界交通事件下的鲁棒性研究不足，需要一种标准化工具来评估和提升其性能。

Method: 提出T-REX框架，模拟交通事件中的网络性能，包括驾驶员的随机重新路由、速度适应和车道变换，并提出一套新的鲁棒性评估指标。

Result: 实验表明，独立值基方法和分散压力基方法在事件驱动的分布偏移下性能下降，而分层协调方法表现更稳定但收敛较慢。

Conclusion: RL-TSC研究需要考虑鲁棒性设计和评估，T-REX为此提供了一个开源、标准化的平台。

Abstract: Reinforcement learning-based traffic signal control (RL-TSC) has emerged as a promising approach for improving urban mobility. However, its robustness under real-world disruptions such as traffic incidents remains largely underexplored. In this study, we introduce T-REX, an open-source, SUMO-based simulation framework for training and evaluating RL-TSC methods under dynamic, incident scenarios. T-REX models realistic network-level performance considering drivers' probabilistic rerouting, speed adaptation, and contextual lane-changing, enabling the simulation of congestion propagation under incidents. To assess robustness, we propose a suite of metrics that extend beyond conventional traffic efficiency measures. Through extensive experiments across synthetic and real-world networks, we showcase T-REX for the evaluation of several state-of-the-art RL-TSC methods under multiple real-world deployment paradigms. Our findings show that while independent value-based and decentralized pressure-based methods offer fast convergence and generalization in stable traffic conditions and homogeneous networks, their performance degrades sharply under incident-driven distribution shifts. In contrast, hierarchical coordination methods tend to offer more stable and adaptable performance in large-scale, irregular networks, benefiting from their structured decision-making architecture. However, this comes with the trade-off of slower convergence and higher training complexity. These findings highlight the need for robustness-aware design and evaluation in RL-TSC research. T-REX contributes to this effort by providing an open, standardized and reproducible platform for benchmarking RL methods under dynamic and disruptive traffic scenarios.

</details>


### [70] [Sustainable Machine Learning Retraining: Optimizing Energy Efficiency Without Compromising Accuracy](https://arxiv.org/abs/2506.13838)
*Lorena Poenaru-Olaru,June Sallou,Luis Cruz,Jan Rellermeyer,Arie van Deursen*

Key words: 机器学习, 重训练, 能耗, 可持续性, 数据变化

TL;DR: 研究探讨了机器学习（ML）系统在数据变化时的维护策略，比较了不同重训练技术的能耗与准确性，提出了更节能的可持续替代方案。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于数据随时间变化会影响ML系统的可靠性，而常规重训练能耗高，研究旨在找到更可持续的重训练技术。

Method: 通过分析常见重训练技术的能耗与准确性，比较全数据与最近数据重训练的节能效果，并探讨按需重训练的潜力。

Result: 仅用最新数据重训练可节能25%；按需重训练可节能40%，前提是具备可靠的数据变化检测器。

Conclusion: 研究为ML从业者提供了节能重训练技术的指导，有助于设计更可持续的ML系统。

Abstract: The reliability of machine learning (ML) software systems is heavily influenced by changes in data over time. For that reason, ML systems require regular maintenance, typically based on model retraining. However, retraining requires significant computational demand, which makes it energy-intensive and raises concerns about its environmental impact. To understand which retraining techniques should be considered when designing sustainable ML applications, in this work, we study the energy consumption of common retraining techniques. Since the accuracy of ML systems is also essential, we compare retraining techniques in terms of both energy efficiency and accuracy. We showcase that retraining with only the most recent data, compared to all available data, reduces energy consumption by up to 25\%, being a sustainable alternative to the status quo. Furthermore, our findings show that retraining a model only when there is evidence that updates are necessary, rather than on a fixed schedule, can reduce energy consumption by up to 40\%, provided a reliable data change detector is in place. Our findings pave the way for better recommendations for ML practitioners, guiding them toward more energy-efficient retraining techniques when designing sustainable ML software systems.

</details>


### [71] [SatHealth: A Multimodal Public Health Dataset with Satellite-based Environmental Factors](https://arxiv.org/abs/2506.13842)
*Yuanlong Wang,Pengqi Wang,Changchang Yin,Ping Zhang*

Key words: SatHealth, 时空数据, 环境健康, AI模型, 公共卫生

TL;DR: SatHealth是一个新型数据集，整合了多模态时空数据，用于提升AI模型在公共卫生和个人疾病风险预测中的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于现有研究中缺乏长期和细粒度的时空数据，限制了AI模型的表现和实际应用，SatHealth旨在填补这一空白。

Method: 开发了SatHealth数据集，结合环境数据、卫星图像、疾病流行率和社会健康决定因素（SDoH）指标，并实验验证其有效性。

Result: 实验证明生活环境数据显著提升了AI模型的性能和时空泛化能力。

Conclusion: SatHealth为医疗研究中纳入环境数据提供了宝贵资源，并为环境健康信息学的未来研究奠定了基础框架。

Abstract: Living environments play a vital role in the prevalence and progression of diseases, and understanding their impact on patient's health status becomes increasingly crucial for developing AI models. However, due to the lack of long-term and fine-grained spatial and temporal data in public and population health studies, most existing studies fail to incorporate environmental data, limiting the models' performance and real-world application. To address this shortage, we developed SatHealth, a novel dataset combining multimodal spatiotemporal data, including environmental data, satellite images, all-disease prevalences estimated from medical claims, and social determinants of health (SDoH) indicators. We conducted experiments under two use cases with SatHealth: regional public health modeling and personal disease risk prediction. Experimental results show that living environmental information can significantly improve AI models' performance and temporal-spatial generalizability on various tasks. Finally, we deploy a web-based application to provide an exploration tool for SatHealth and one-click access to both our data and regional environmental embedding to facilitate plug-and-play utilization. SatHealth is now published with data in Ohio, and we will keep updating SatHealth to cover the other parts of the US. With the web application and published code pipeline, our work provides valuable angles and resources to include environmental data in healthcare research and establishes a foundational framework for future research in environmental health informatics.

</details>


### [72] [StaQ it! Growing neural networks for Policy Mirror Descent](https://arxiv.org/abs/2506.13862)
*Alena Shilova,Alex Davey,Brahim Driss,Riad Akrour*

Key words: 强化学习, Policy Mirror Descent, 稳定性, Q函数, StaQ

TL;DR: 论文提出了一种改进的Policy Mirror Descent (PMD)算法StaQ，通过仅保留最近M个Q函数来实现收敛且稳定的强化学习。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统PMD算法因需存储所有历史Q函数而在实践中不可行，改进算法旨在解决这一问题。

Method: 提出仅保留最近M个Q函数的PMD变体，StaQ算法确保收敛且无策略更新误差。

Result: StaQ在理论上具有强保证，实际表现与基线方法相当且振荡更小。

Conclusion: StaQ为稳定深度强化学习算法提供了新方向，并可作为PMD实验平台。

Abstract: In Reinforcement Learning (RL), regularization has emerged as a popular tool both in theory and practice, typically based either on an entropy bonus or a Kullback-Leibler divergence that constrains successive policies. In practice, these approaches have been shown to improve exploration, robustness and stability, giving rise to popular Deep RL algorithms such as SAC and TRPO. Policy Mirror Descent (PMD) is a theoretical framework that solves this general regularized policy optimization problem, however the closed-form solution involves the sum of all past Q-functions, which is intractable in practice. We propose and analyze PMD-like algorithms that only keep the last $M$ Q-functions in memory, and show that for finite and large enough $M$, a convergent algorithm can be derived, introducing no error in the policy update, unlike prior deep RL PMD implementations. StaQ, the resulting algorithm, enjoys strong theoretical guarantees and is competitive with deep RL baselines, while exhibiting less performance oscillation, paving the way for fully stable deep RL algorithms and providing a testbed for experimentation with Policy Mirror Descent.

</details>


### [73] [Scaling Algorithm Distillation for Continuous Control with Mamba](https://arxiv.org/abs/2506.13892)
*Samuel Beaussant,Mehdi Mounsif*

Key words: 算法蒸馏、S6模型、Mamba、上下文强化学习、长序列建模

TL;DR: 提出了使用选择性结构化状态空间序列（S6）模型改进算法蒸馏（AD）方法，适用于长序列环境，并在复杂连续环境中验证其优于Transformer模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前的算法蒸馏方法因Transformer的二次复杂度限制，仅适用于简单离散环境。为解决这一问题，引入了S6模型以实现长序列的高效建模。

Method: 利用选择性结构化状态空间序列（S6）模型构建Mamba模型，替代Transformer进行算法蒸馏，并在四个复杂连续元强化学习环境中进行测试。

Result: S6模型在算法蒸馏中表现出优于Transformer的性能，且能够通过长上下文提升在上下文强化学习中的竞争力，达到与最先进的在线元强化学习基线相当的水平。

Conclusion: S6模型为算法蒸馏提供了高效的替代方案，适合长序列和复杂环境，扩展了算法的应用范围。

Abstract: Algorithm Distillation (AD) was recently proposed as a new approach to perform In-Context Reinforcement Learning (ICRL) by modeling across-episodic training histories autoregressively with a causal transformer model. However, due to practical limitations induced by the attention mechanism, experiments were bottlenecked by the transformer's quadratic complexity and limited to simple discrete environments with short time horizons. In this work, we propose leveraging the recently proposed Selective Structured State Space Sequence (S6) models, which achieved state-of-the-art (SOTA) performance on long-range sequence modeling while scaling linearly in sequence length. Through four complex and continuous Meta Reinforcement Learning environments, we demonstrate the overall superiority of Mamba, a model built with S6 layers, over a transformer model for AD. Additionally, we show that scaling AD to very long contexts can improve ICRL performance and make it competitive even with a SOTA online meta RL baseline.

</details>


### [74] [Enhancing interpretability of rule-based classifiers through feature graphs](https://arxiv.org/abs/2506.13903)
*Christel Sirocchi,Damiano Verda*

Key words: 规则系统, 特征重要性, 可解释性, 医疗决策

TL;DR: 提出一个全面的框架，用于估计基于规则的系统中的特征贡献，包括可视化策略、新特征重要性指标和规则集比较方法，实验结果表明其竞争性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在需要透明和可信的领域（如医疗），基于规则的系统因其可解释性而被广泛使用，但随着复杂性增加，特征理解和比较变得困难。

Method: 提出基于图的特征可视化策略、新的特征重要性指标和规则集比较的距离度量，并在两个临床数据集和四种规则方法上进行实验。

Result: 方法可以揭示临床特征的预测价值，识别新风险因素和生物标志物，并在15个公共基准上表现出竞争性和鲁棒性。

Conclusion: 该方法提升了基于规则系统的可解释性和实用性，支持精准医疗和诊断优化。

Abstract: In domains where transparency and trustworthiness are crucial, such as healthcare, rule-based systems are widely used and often preferred over black-box models for decision support systems due to their inherent interpretability. However, as rule-based models grow complex, discerning crucial features, understanding their interactions, and comparing feature contributions across different rule sets becomes challenging. To address this, we propose a comprehensive framework for estimating feature contributions in rule-based systems, introducing a graph-based feature visualisation strategy, a novel feature importance metric agnostic to rule-based predictors, and a distance metric for comparing rule sets based on feature contributions. By experimenting on two clinical datasets and four rule-based methods (decision trees, logic learning machines, association rules, and neural networks with rule extraction), we showcase our method's capability to uncover novel insights on the combined predictive value of clinical features, both at the dataset and class-specific levels. These insights can aid in identifying new risk factors, signature genes, and potential biomarkers, and determining the subset of patient information that should be prioritised to enhance diagnostic accuracy. Comparative analysis of the proposed feature importance score with state-of-the-art methods on 15 public benchmarks demonstrates competitive performance and superior robustness. The method implementation is available on GitHub: https://github.com/ChristelSirocchi/rule-graph.

</details>


### [75] [GITO: Graph-Informed Transformer Operator for Learning Complex Partial Differential Equations](https://arxiv.org/abs/2506.13906)
*Milad Ramezankhani,Janak M. Patel,Anirudh Deodhar,Dagnachew Birru*

Key words: GITO, 图神经网络, 变换器, 偏微分方程, 零样本超分辨率

TL;DR: 提出了一种新颖的图信息变换算子（GITO）架构，用于学习定义在非规则几何和非均匀网格上的复杂偏微分方程系统。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在解决传统方法在处理不规则几何和非均匀网格上的偏微分方程系统时的局限性。

Method: GITO结合了混合图变换（HGT）和变换神经算子（TNO），通过图神经网络编码局部空间关系，变换器捕获长程依赖，并通过自注意力融合层集成。

Result: 在基准PDE任务中表现优于现有基于变换器的神经算子，实现了网格无关的零样本超分辨率。

Conclusion: GITO为工程应用中高效、网格无关的替代求解器提供了新途径。

Abstract: We present a novel graph-informed transformer operator (GITO) architecture for learning complex partial differential equation systems defined on irregular geometries and non-uniform meshes. GITO consists of two main modules: a hybrid graph transformer (HGT) and a transformer neural operator (TNO). HGT leverages a graph neural network (GNN) to encode local spatial relationships and a transformer to capture long-range dependencies. A self-attention fusion layer integrates the outputs of the GNN and transformer to enable more expressive feature learning on graph-structured data. TNO module employs linear-complexity cross-attention and self-attention layers to map encoded input functions to predictions at arbitrary query locations, ensuring discretization invariance and enabling zero-shot super-resolution across any mesh. Empirical results on benchmark PDE tasks demonstrate that GITO outperforms existing transformer-based neural operators, paving the way for efficient, mesh-agnostic surrogate solvers in engineering applications.

</details>


### [76] [Few-Shot Learning for Industrial Time Series: A Comparative Analysis Using the Example of Screw-Fastening Process Monitoring](https://arxiv.org/abs/2506.13909)
*Xinyuan Tu,Haocheng Zhang,Tao Chengxu,Zuyi Chen*

Key words: 小样本学习、工业时间序列、螺丝紧固、标签感知采样、度量学习

TL;DR: 论文研究了小样本学习（FSL）在工业时间序列数据中的应用，提出了一种标签感知采样器，并在螺丝紧固过程监测中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 工业时间序列数据标注成本高，小样本学习在视觉领域有前景，但在工业应用中尚未充分探索。

Method: 采用了两种FSL范式（原型网络和MAML），结合三种骨干网络（1D CNN、InceptionTime和Transformer），并提出标签感知采样器。

Result: 在10-shot、3-way评估中，InceptionTime+原型网络组合在多项指标上表现优异，优于大型Transformer模型。

Conclusion: 轻量级CNN架构结合简单度量学习在小数据量下表现更好。

Abstract: Few-shot learning (FSL) has shown promise in vision but remains largely unexplored for \emph{industrial} time-series data, where annotating every new defect is prohibitively expensive. We present a systematic FSL study on screw-fastening process monitoring, using a 2\,300-sample multivariate torque dataset that covers 16 uni- and multi-factorial defect types. Beyond benchmarking, we introduce a \textbf{label-aware episodic sampler} that collapses multi-label sequences into multiple single-label tasks, keeping the output dimensionality fixed while preserving combinatorial label information.
  Two FSL paradigms are investigated: the metric-based \emph{Prototypical Network} and the gradient-based \emph{Model-Agnostic Meta-Learning} (MAML), each paired with three backbones: 1D CNN, InceptionTime and the 341 M-parameter transformer \emph{Moment}. On 10-shot, 3-way evaluation, the InceptionTime + Prototypical Network combination achieves a \textbf{0.944 weighted F1} in the multi-class regime and \textbf{0.935} in the multi-label regime, outperforming finetuned Moment by up to 5.3\% while requiring two orders of magnitude fewer parameters and training time. Across all backbones, metric learning consistently surpasses MAML, and our label-aware sampling yields an additional 1.7\% F1 over traditional class-based sampling.
  These findings challenge the assumption that large foundation models are always superior: when data are scarce, lightweight CNN architectures augmented with simple metric learning not only converge faster but also generalize better. We release code, data splits and pre-trained weights to foster reproducible research and to catalyze the adoption of FSL in high-value manufacturing inspection.

</details>


### [77] [Logical Expressiveness of Graph Neural Networks with Hierarchical Node Individualization](https://arxiv.org/abs/2506.13911)
*Arie Soeteman,Balder ten Cate*

Key words: 图神经网络, 表达性增强, 分层节点个性化, 图同构测试

TL;DR: 该论文提出了一种名为HEGNNs的分层图神经网络扩展，其在图同构测试中表现出更强的表达能力，并通过实验验证了其优于传统GNN架构。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了增强图神经网络（GNNs）的表达能力，特别是在图同构测试中的表现，研究者受到Individualization-Refinement范式的启发，提出了HEGNNs。

Method: HEGNNs通过分层节点个性化扩展GNNs，形成一个表达能力逐步增强的模型层次结构。研究者还提供了HEGNNs的逻辑表征，并将其与其他高阶GNNs和基于Individualization-Refinement的颜色细化算法进行比较。

Result: 实验结果表明，HEGNNs不仅具有实际可行性，而且在性能和表达能力上优于传统GNN架构。

Conclusion: HEGNNs在提升GNN表达能力方面表现出色，特别是在图同构测试任务中，为相关研究提供了新的方向。

Abstract: We propose and study Hierarchical Ego Graph Neural Networks (HEGNNs), an expressive extension of graph neural networks (GNNs) with hierarchical node individualization, inspired by the Individualization-Refinement paradigm for graph isomorphism testing. HEGNNs generalize subgraph-GNNs and form a hierarchy of increasingly expressive models that, in the limit, can distinguish graphs up to isomorphism. We provide a logical characterization of HEGNN node classifiers, with and without subgraph restrictions, using graded hybrid logic. This characterization enables us to relate the separating power of HEGNNs to that of higher-order GNNs, GNNs enriched with local homomorphism count features, and color refinement algorithms based on Individualization-Refinement. Our experimental results confirm the practical feasibility of HEGNNs and show benefits in comparison with traditional GNN architectures, both with and without local homomorphism count features.

</details>


### [78] [Branching Stein Variational Gradient Descent for sampling multimodal distributions](https://arxiv.org/abs/2506.13916)
*Isaias Banales,Arturo Jaramillo,Heli Ricalde Guerrero*

Key words: 变分推断, 多模态分布, SVGD, BSVGD, 分支机制

TL;DR: 提出了一种基于粒子的变分推断方法BSVGD，通过分支机制探索多模态分布，理论证明其收敛性并经实验验证优于SVGD。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决传统SVGD算法在多模态分布中探索不足的问题。

Method: 扩展SVGD算法，引入随机分支机制以增强状态空间探索。

Result: 理论证明BSVGD的分布收敛性，并通过实验验证其性能优于SVGD。

Conclusion: BSVGD在多模态分布中表现更优，提供了有效的探索机制。

Abstract: We propose a novel particle-based variational inference method designed to work with multimodal distributions. Our approach, referred to as Branched Stein Variational Gradient Descent (BSVGD), extends the classical Stein Variational Gradient Descent (SVGD) algorithm by incorporating a random branching mechanism that encourages the exploration of the state space. In this work, a theoretical guarantee for the convergence in distribution is presented, as well as numerical experiments to validate the suitability of our algorithm. Performance comparisons between the BSVGD and the SVGD are presented using the Wasserstein distance between samples and the corresponding computational times.

</details>


### [79] [Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models](https://arxiv.org/abs/2506.13923)
*Vaskar Nath,Elaine Lau,Anisha Gunjal,Manasi Sharma,Nikhil Baharte,Sean Hendryx*

Key words: 强化学习, 推理模型, 自我蒸馏, 能力增益, Guide算法

TL;DR: 论文研究了通过强化学习训练的推理模型（RLVR）如何通过自我蒸馏和‘能力增益’解决新问题，并提出了新算法Guide，显著提升了模型的泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索强化学习训练的推理模型如何通过压缩和改进能力来解决新问题，并开发更高效的训练算法。

Method: 使用RLVR训练不同规模的模型（0.5B到72B），提出Guide算法，结合自然语言提示和重要性采样优化策略。

Result: Guide-GRPO在7B和32B模型上实现了高达4%的泛化提升，特别是在数学基准测试中。

Conclusion: Guide算法通过动态整合提示和优化策略，显著提升了模型的推理能力和泛化性能。

Abstract: We study the process through which reasoning models trained with reinforcement learning on verifiable rewards (RLVR) can learn to solve new problems. We find that RLVR drives performance through two main means: (1) by compressing pass@$k$ into pass@1 and (2) via "capability gain" in which models learn to solve new problems that they previously could not solve even at high $k$. We find that while capability gain exists across model scales, learning to solve new problems is primarily driven through self-distillation. We demonstrate these findings across model scales ranging from 0.5B to 72B on >500,000 reasoning problems with prompts and verifiable final answers across math, science, and code domains. We further show that we can significantly improve pass@$k$ rates by leveraging natural language guidance for the model to consider within context while still requiring the model to derive a solution chain from scratch. Based of these insights, we derive $\text{Guide}$ - a new class of online training algorithms. $\text{Guide}$ adaptively incorporates hints into the model's context on problems for which all rollouts were initially incorrect and adjusts the importance sampling ratio for the "off-policy" trajectories in order to optimize the policy for contexts in which the hints are no longer present. We describe variants of $\text{Guide}$ for GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter models improves generalization over its vanilla counterpart with up to 4$\%$ macro-average improvement across math benchmarks. We include careful ablations to analyze $\text{Guide}$'s components and theoretically analyze Guide's learning efficiency.

</details>


### [80] [ReinDSplit: Reinforced Dynamic Split Learning for Pest Recognition in Precision Agriculture](https://arxiv.org/abs/2506.13935)
*Vishesh Kumar Tanwar,Soumik Sarkar,Asheesh K. Singh,Sajal K. Das*

Key words: 分布式机器学习、分割学习、强化学习、农业物联网、计算效率

TL;DR: ReinDSplit是一个基于强化学习的框架，动态调整DNN分割点以优化农业边缘设备的计算效率和模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统分割学习框架在农业生态系统中因设备异构性导致效率低下，需动态适应不同设备的计算能力和资源限制。

Method: 采用Q学习代理作为自适应协调器，通过马尔可夫决策过程动态选择分割层，平衡工作负载和延迟阈值。

Result: 在昆虫分类数据集上测试，MobileNetV2达到94.31%的准确率。

Conclusion: ReinDSplit为异构环境中的资源效率、隐私和可扩展性提供新范式。

Abstract: To empower precision agriculture through distributed machine learning (DML), split learning (SL) has emerged as a promising paradigm, partitioning deep neural networks (DNNs) between edge devices and servers to reduce computational burdens and preserve data privacy. However, conventional SL frameworks' one-split-fits-all strategy is a critical limitation in agricultural ecosystems where edge insect monitoring devices exhibit vast heterogeneity in computational power, energy constraints, and connectivity. This leads to straggler bottlenecks, inefficient resource utilization, and compromised model performance. Bridging this gap, we introduce ReinDSplit, a novel reinforcement learning (RL)-driven framework that dynamically tailors DNN split points for each device, optimizing efficiency without sacrificing accuracy. Specifically, a Q-learning agent acts as an adaptive orchestrator, balancing workloads and latency thresholds across devices to mitigate computational starvation or overload. By framing split layer selection as a finite-state Markov decision process, ReinDSplit convergence ensures that highly constrained devices contribute meaningfully to model training over time. Evaluated on three insect classification datasets using ResNet18, GoogleNet, and MobileNetV2, ReinDSplit achieves 94.31% accuracy with MobileNetV2. Beyond agriculture, ReinDSplit pioneers a paradigm shift in SL by harmonizing RL for resource efficiency, privacy, and scalability in heterogeneous environments.

</details>


### [81] [Toward Explainable Offline RL: Analyzing Representations in Intrinsically Motivated Decision Transformers](https://arxiv.org/abs/2506.13958)
*Leonardo Guiducci,Antonio Rizzo,Giovanna Maria Dimitri*

Key words: 离线强化学习,弹性决策变换器,内在动机,嵌入表示,可解释性

TL;DR: 论文介绍了事后可解释性框架，分析内在动机如何影响弹性决策变换器（EDTs）的嵌入表示，揭示了其对策略学习的促进作用。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探究内在动机机制如何通过改变EDTs的嵌入结构来提升其在离线强化学习中的性能。

Method: 提出系统的事后可解释性框架，统计分析嵌入特性（如协方差结构、向量大小和正交性）。

Result: 发现不同内在动机变体导致不同的表示结构，嵌入指标与性能间存在环境特定的相关性。

Conclusion: 内在动机不仅是探索奖励，还作为表示先验，以生物学可信的方式塑造嵌入几何结构，提升决策能力。

Abstract: Elastic Decision Transformers (EDTs) have proved to be particularly successful in offline reinforcement learning, offering a flexible framework that unifies sequence modeling with decision-making under uncertainty. Recent research has shown that incorporating intrinsic motivation mechanisms into EDTs improves performance across exploration tasks, yet the representational mechanisms underlying these improvements remain unexplored. In this paper, we introduce a systematic post-hoc explainability framework to analyze how intrinsic motivation shapes learned embeddings in EDTs. Through statistical analysis of embedding properties (including covariance structure, vector magnitudes, and orthogonality), we reveal that different intrinsic motivation variants create fundamentally different representational structures. Our analysis demonstrates environment-specific correlation patterns between embedding metrics and performance that explain why intrinsic motivation improves policy learning. These findings show that intrinsic motivation operates beyond simple exploration bonuses, acting as a representational prior that shapes embedding geometry in biologically plausible ways, creating environment-specific organizational structures that facilitate better decision-making.

</details>


### [82] [Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble](https://arxiv.org/abs/2506.13972)
*Zhiqi Wang,Chengyu Zhang,Yuetian Chen,Nathalie Baracaldo,Swanand Kadhe,Lei Yu*

Key words: 成员推理攻击,隐私评估,覆盖率分析,稳定性分析,集成框架

TL;DR: 本文指出现有成员推理攻击（MIA）研究中忽视攻击间差异的问题，提出基于覆盖率和稳定性分析的新框架，揭示了差异的潜在原因，并提出集成框架以优化隐私评估。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有MIA研究过于关注性能指标（如AUC、准确性等），忽视了不同攻击方法及其实例间的差异，这对隐私评估的可靠性和完整性造成影响。

Method: 通过覆盖率和稳定性分析框架系统研究MIA差异，并开发包含三种策略的集成框架以整合不同攻击方法的优势。

Result: 实验表明MIA存在显著差异，并揭示了差异的潜在原因及对隐私评估的影响；集成框架显著提升了攻击效果和隐私评估的鲁棒性。

Conclusion: 本研究填补了MIA差异研究的空白，提出的集成框架为隐私评估提供了更全面和可靠的方法。

Abstract: Membership inference attacks (MIAs) pose a significant threat to the privacy of machine learning models and are widely used as tools for privacy assessment, auditing, and machine unlearning. While prior MIA research has primarily focused on performance metrics such as AUC, accuracy, and TPR@low FPR - either by developing new methods to enhance these metrics or using them to evaluate privacy solutions - we found that it overlooks the disparities among different attacks. These disparities, both between distinct attack methods and between multiple instantiations of the same method, have crucial implications for the reliability and completeness of MIAs as privacy evaluation tools. In this paper, we systematically investigate these disparities through a novel framework based on coverage and stability analysis. Extensive experiments reveal significant disparities in MIAs, their potential causes, and their broader implications for privacy evaluation. To address these challenges, we propose an ensemble framework with three distinct strategies to harness the strengths of state-of-the-art MIAs while accounting for their disparities. This framework not only enables the construction of more powerful attacks but also provides a more robust and comprehensive methodology for privacy evaluation.

</details>


### [83] [Constant Stepsize Local GD for Logistic Regression: Acceleration by Instability](https://arxiv.org/abs/2506.13974)
*Michael Crawshaw,Blake Woodworth,Mingrui Liu*

Key words: 局部梯度下降, 逻辑回归, 异构数据, 收敛速度, 步长

TL;DR: 本文分析了局部梯度下降在逻辑回归中的应用，特别针对异构数据，并展示了在任意步长下的收敛速度优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法在异构目标下的局部梯度下降分析中，要求步长受限于通信间隔，限制了其灵活性。本文旨在研究更大步长下的表现。

Method: 采用局部梯度下降方法，结合逻辑回归模型，分析其在异构数据下的表现，无需限制步长。

Result: 证明了在任意步长下，收敛速度为$$$$$mathcal{O}(1/ηK R)$$$$$，且初始不稳定阶段仅持续$$$$$widetilde{$$$$mathcal{O}}(ηK M)$$$$$轮。

Conclusion: 本文通过放宽步长限制，显著提高了局部梯度下降的收敛速度，为异构数据处理提供了更高效的方法。

Abstract: Existing analysis of Local (Stochastic) Gradient Descent for heterogeneous objectives requires stepsizes $η\leq 1/K$ where $K$ is the communication interval, which ensures monotonic decrease of the objective. In contrast, we analyze Local Gradient Descent for logistic regression with separable, heterogeneous data using any stepsize $η> 0$. With $R$ communication rounds and $M$ clients, we show convergence at a rate $\mathcal{O}(1/ηK R)$ after an initial unstable phase lasting for $\widetilde{\mathcal{O}}(ηK M)$ rounds. This improves upon the existing $\mathcal{O}(1/R)$ rate for general smooth, convex objectives. Our analysis parallels the single machine analysis of~\cite{wu2024large} in which instability is caused by extremely large stepsizes, but in our setting another source of instability is large local updates with heterogeneous objectives.

</details>


### [84] [HAELT: A Hybrid Attentive Ensemble Learning Transformer Framework for High-Frequency Stock Price Forecasting](https://arxiv.org/abs/2506.13981)
*Thanh Dan Bui*

Key words: 高频股票预测、HAELT、深度学习、ResNet、自注意力、LSTM-Transformer

TL;DR: 论文提出了一种名为HAELT的深度学习框架，用于解决高频股票价格预测中的非平稳性、噪声和波动性问题，通过结合ResNet、自注意力机制和LSTM-Transformer，显著提升了预测性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 高频股票价格预测面临非平稳性、噪声和波动性等挑战，传统方法难以有效应对，需要一种更鲁棒的预测框架。

Method: 提出HAELT框架，结合ResNet降噪模块、时间自注意力机制以及混合LSTM-Transformer核心，并根据近期表现动态集成各组件。

Result: 在苹果公司（AAPL）2024年1月至2025年5月的小时数据上测试，HAELT取得了最高的F1分数，能够有效识别价格的上升和下降趋势。

Conclusion: HAELT为高频金融预测和算法交易提供了一个鲁棒且实用的解决方案。

Abstract: High-frequency stock price prediction is challenging due to non-stationarity, noise, and volatility. To tackle these issues, we propose the Hybrid Attentive Ensemble Learning Transformer (HAELT), a deep learning framework combining a ResNet-based noise-mitigation module, temporal self-attention for dynamic focus on relevant history, and a hybrid LSTM-Transformer core that captures both local and long-range dependencies. These components are adaptively ensembled based on recent performance. Evaluated on hourly Apple Inc. (AAPL) data from Jan 2024 to May 2025, HAELT achieves the highest F1-Score on the test set, effectively identifying both upward and downward price movements. This demonstrates HAELT's potential for robust, practical financial forecasting and algorithmic trading.

</details>


### [85] [Quantum-Informed Contrastive Learning with Dynamic Mixup Augmentation for Class-Imbalanced Expert Systems](https://arxiv.org/abs/2506.13987)
*Md Abrar Jahin,Adiba Abid,M. F. Mridha*

Key words: 量子启发,对比学习,动态mixup,类别不平衡,表格数据

TL;DR: QCL-MixNet是一种量子启发的对比学习框架，结合动态mixup技术，用于解决类别不平衡问题，性能优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决类别不平衡数据中罕见但关键实例的检测问题，传统方法存在过拟合、标签噪声和泛化能力差等缺陷。

Method: 结合量子纠缠层、动态mixup策略和混合损失函数，增强少数类表示和分类性能。

Result: 在18个不平衡数据集上显著优于20种先进方法，宏F1和召回率表现突出。

Conclusion: QCL-MixNet为专家系统中的表格数据不平衡处理设定了新基准。

Abstract: Expert systems often operate in domains characterized by class-imbalanced tabular data, where detecting rare but critical instances is essential for safety and reliability. While conventional approaches, such as cost-sensitive learning, oversampling, and graph neural networks, provide partial solutions, they suffer from drawbacks like overfitting, label noise, and poor generalization in low-density regions. To address these challenges, we propose QCL-MixNet, a novel Quantum-Informed Contrastive Learning framework augmented with k-nearest neighbor (kNN) guided dynamic mixup for robust classification under imbalance. QCL-MixNet integrates three core innovations: (i) a Quantum Entanglement-inspired layer that models complex feature interactions through sinusoidal transformations and gated attention, (ii) a sample-aware mixup strategy that adaptively interpolates feature representations of semantically similar instances to enhance minority class representation, and (iii) a hybrid loss function that unifies focal reweighting, supervised contrastive learning, triplet margin loss, and variance regularization to improve both intra-class compactness and inter-class separability. Extensive experiments on 18 real-world imbalanced datasets (binary and multi-class) demonstrate that QCL-MixNet consistently outperforms 20 state-of-the-art machine learning, deep learning, and GNN-based baselines in macro-F1 and recall, often by substantial margins. Ablation studies further validate the critical role of each architectural component. Our results establish QCL-MixNet as a new benchmark for tabular imbalance handling in expert systems. Theoretical analyses reinforce its expressiveness, generalization, and optimization robustness.

</details>


### [86] [AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science](https://arxiv.org/abs/2506.13992)
*An Luo,Xun Xian,Jin Du,Fangqiao Tian,Ganghua Wang,Ming Zhong,Shengchun Zhao,Xuan Bi,Zirui Liu,Jiawei Zhou,Jayanth Srinivasa,Ashish Kundu,Charles Fleming,Mingyi Hong,Jie Ding*

Key words: 大语言模型, 数据科学, 领域知识, 表格预测

TL;DR: 评估大语言模型（LLMs）在数据科学任务中如何利用域外知识的研究，发现其存在未批判性采纳信息和处理时间序列数据等问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究LLMs在自动化数据科学工作流程中是否能像人类数据科学家一样批判性利用外部领域知识。

Method: 提出AssistedDS基准测试，结合合成数据集和真实Kaggle竞赛数据，评估LLMs在表格预测任务中处理领域知识的能力。

Result: LLMs常未批判性采纳信息，对抗性内容显著降低预测性能；且在处理时间序列数据和分类变量时易出错。

Conclusion: 当前模型在批判性评估和利用专家知识方面存在显著差距，需开发更鲁棒的自动化数据科学系统。

Abstract: Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as human data scientists do in practice. To answer this question, we introduce AssistedDS (Assisted Data Science), a benchmark designed to systematically evaluate how LLMs handle domain knowledge in tabular prediction tasks. AssistedDS features both synthetic datasets with explicitly known generative mechanisms and real-world Kaggle competitions, each accompanied by curated bundles of helpful and adversarial documents. These documents provide domain-specific insights into data cleaning, feature engineering, and model selection. We assess state-of-the-art LLMs on their ability to discern and apply beneficial versus harmful domain knowledge, evaluating submission validity, information recall, and predictive performance. Our results demonstrate three key findings: (1) LLMs frequently exhibit an uncritical adoption of provided information, significantly impairing their predictive performance when adversarial content is introduced, (2) helpful guidance is often insufficient to counteract the negative influence of adversarial information, and (3) in Kaggle datasets, LLMs often make errors in handling time-series data, applying consistent feature engineering across different folds, and interpreting categorical variables correctly. These findings highlight a substantial gap in current models' ability to critically evaluate and leverage expert knowledge, underscoring an essential research direction for developing more robust, knowledge-aware automated data science systems.

</details>


### [87] [Arctic Long Sequence Training: Scalable And Efficient Training For Multi-Million Token Sequences](https://arxiv.org/abs/2506.13996)
*Stas Bekman,Samyam Rajbhandari,Michael Wyatt,Jeff Rasley,Tunji Ruwase,Zhewei Yao,Aurick Qiao,Yuxiong He*

Key words: 长序列训练，ALST，GPU优化，Hugging Face，开源

TL;DR: ALST技术解决了长序列训练的挑战，支持单GPU和多GPU内存优化，使开源模型如Llama 8B能够训练高达数百万的序列长度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 长序列训练对AI社区具有重要应用价值，但现有技术无法充分利用硬件资源，且开源支持有限。

Method: 提出Arctic Long Sequence Training (ALST)，结合注意力无关的单GPU和多GPU内存优化技术。

Result: ALST在单H100 GPU上支持500K序列长度，8xH100节点支持3.7M，4节点集群支持15M，性能提升显著。

Conclusion: ALST为开源社区提供了高效的长序列训练解决方案，兼容Hugging Face模型并开源。

Abstract: Long sequences are critical for applications like RAG, long document summarization, multi-modality, etc., and modern LLMs, like Llama 4 Scout, support max sequence length of up to 10 million tokens. However, outside of enterprise labs, long sequence training is challenging for the AI community with limited system support in the open-source space.
  Out-of-box, even on a modern NVIDIA H100 80GB GPU cluster, training Llama 8B model with sequence over 32K runs out of memory on a basic Hugging Face (HF) model due to two reasons: i) LLM training workloads are not optimized to fully leverage a single GPU memory, ii) existing solutions for leveraging multiple GPU memory are not easily available to HF models, making long sequence training inaccessible.
  We address this with Arctic Long Sequence Training (ALST). It offers a combination of attention-agnostic single GPU and multi-GPU memory optimizations, that enables it to support out-of-box training of multi-million sequence length for a wide variety of HF models.
  ALST supports training Meta's Llama 8B model with 500K sequence length on a single H100 GPU, 3.7M on a single 8xH100 GPU node, and over 15M on a 4 node cluster, an increase of over 400x compared to the 32K baseline for the latter. ALST is fully compatible with HF models and open-sourced via Deepspeed https://www.deepspeed.ai/tutorials/ulysses-alst-sequence-pallellism/ and Arctic Training https://github.com/snowflakedb/ArcticTraining/blob/main/projects/sequence-parallelism/README.md.

</details>


### [88] [Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders](https://arxiv.org/abs/2506.14002)
*Siyu Chen,Heejune Sheen,Xuyuan Xiong,Tianhao Wang,Zhuoran Yang*

Key words: 稀疏自编码器、特征恢复、大型语言模型、偏差适应、理论保证

TL;DR: 提出一种新的统计框架和基于“偏差适应”的稀疏自编码器（SAE）训练算法，用于大型语言模型的特征恢复，并证明了其在理论和实验上的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现有SAE训练算法缺乏理论保证及实际应用中的不稳定性问题，提升大型语言模型的可解释性。

Method: 提出新的统计框架建模特征可识别性，并开发基于“偏差适应”的SAE训练算法及其改进版本GBA。

Result: 理论证明算法能正确恢复所有单义特征，实验显示GBA在15亿参数的大型语言模型中优于基准方法。

Conclusion: 该研究为SAE训练提供了首个具有理论恢复保证的算法，推动了AI系统的透明性和可信性发展。

Abstract: We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically \highlight{prove that this algorithm correctly recovers all monosemantic features} when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and \highlight{demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters}. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.

</details>


### [89] [Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs](https://arxiv.org/abs/2506.14003)
*Yiwei Chen,Soumyadeep Pal,Yimeng Zhang,Qing Qu,Sijia Liu*

Key words: 机器学习遗忘, 大型语言模型, 遗忘痕迹, 数据隐私, 模型安全性

TL;DR: 本文研究了大型语言模型（LLM）的“机器学习遗忘”（MU）过程，发现遗忘操作会在模型中留下可检测的“指纹”，这些痕迹可以通过简单的监督分类器从其输出中识别。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究动机是为了揭示机器学习遗忘操作后的新漏洞——遗忘痕迹检测，以帮助更好地理解遗忘对模型的影响及其潜在风险。

Method: 通过分析模型行为和内部表示，使用监督分类器检测遗忘痕迹，并验证其在各种输入下的可检测性。

Result: 实验表明，遗忘痕迹可以从输出和中间激活中检测，准确率超过90%，即使输入与遗忘无关。

Conclusion: 遗忘操作在模型中留下了可检测的痕迹，这可能带来遗忘信息被逆向工程的风险。

Abstract: Machine unlearning (MU) for large language models (LLMs), commonly referred to as LLM unlearning, seeks to remove specific undesirable data or knowledge from a trained model, while maintaining its performance on standard tasks. While unlearning plays a vital role in protecting data privacy, enforcing copyright, and mitigating sociotechnical harms in LLMs, we identify a new vulnerability post-unlearning: unlearning trace detection. We discover that unlearning leaves behind persistent ''fingerprints'' in LLMs, detectable traces in both model behavior and internal representations. These traces can be identified from output responses, even when prompted with forget-irrelevant inputs. Specifically, a simple supervised classifier can reliably determine whether a model has undergone unlearning based solely on its textual outputs. Further analysis shows that these traces are embedded in intermediate activations and propagate nonlinearly to the final layer, forming low-dimensional, learnable manifolds in activation space. Through extensive experiments, we show that forget-relevant prompts enable over 90% accuracy in detecting unlearning traces across all model sizes. Even with forget-irrelevant inputs, large LLMs maintain high detectability, demonstrating the broad applicability of unlearning trace detection. These findings reveal that unlearning leaves measurable signatures, introducing a new risk of reverse-engineering forgotten information when a model is identified as unlearned given an input query. Codes are available at [this URL](https://github.com/OPTML-Group/Unlearn-Trace).

</details>


### [90] [Bures-Wasserstein Flow Matching for Graph Generation](https://arxiv.org/abs/2506.14020)
*Keyue Jiang,Jiahao Cui,Xiaowen Dong,Laura Toni*

Key words: 图生成, 流匹配, 马尔可夫随机场, 最优传输, 分子设计

TL;DR: BWFlow是一种基于流匹配的图生成框架，通过建模节点和边的联合演化，利用马尔可夫随机场和最优传输设计概率路径，显著提升了图生成的性能和稳定性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统方法在欧几里得空间中独立建模节点和边的演化，忽略了图的非欧几里得结构和互联模式，导致采样收敛性风险。因此需要更优的概率路径构建方法。

Method: 使用马尔可夫随机场（MRF）表示图，并基于MRF对象间的最优传输设计概率路径，提出了BWFlow框架，支持连续和离散流匹配算法。

Result: 在普通图生成和2D/3D分子生成任务中，BWFlow表现出竞争性能、稳定训练和保证的采样收敛性。

Conclusion: BWFlow通过尊重图的内在几何结构，显著提升了图生成任务的效果和可靠性。

Abstract: Graph generation has emerged as a critical task in fields ranging from molecule design to drug discovery. Contemporary approaches, notably diffusion and flow-based models, have achieved solid graph generative performance through constructing a probability path that interpolates between a reference distribution and the data distribution. However, these methods typically model the evolution of individual nodes and edges independently and use linear interpolations to build the path assuming that the data lie in Euclidean space. We show that this is suboptimal given the intrinsic non-Euclidean structure and interconnected patterns of graphs, and it poses risks to the sampling convergence. To build a better probability path, we model the joint evolution of the nodes and edges by representing graphs as connected systems parameterized by Markov random fields (MRF). We then leverage the optimal transport displacement between MRF objects to design the probability path for graph generation. Based on this, we introduce BWFlow, a flow-matching framework for graph generation that respects the underlying geometry of graphs and provides smooth velocities in the probability path. The novel framework can be adapted to both continuous and discrete flow-matching algorithms. Experimental evaluations in plain graph generation and 2D/3D molecule generation validate the effectiveness of BWFlow in graph generation with competitive performance, stable training, and guaranteed sampling convergence.

</details>


### [91] [Robust Physics-Informed Neural Network Approach for Estimating Heterogeneous Elastic Properties from Noisy Displacement Data](https://arxiv.org/abs/2506.14036)
*Tatthapong Srikitrungruang,Sina Aghaee Dabaghan Fard,Matthew Lemon,Jaesung Lee,Yuxiao Zhou*

Key words: 逆向弹性问题、物理信息神经网络、弹性参数估计、噪声位移数据、绝对尺度弹性模量

TL;DR: 提出了一种新的逆向弹性物理信息神经网络（IE-PINN），用于从噪声位移数据中稳健地重建弹性参数的空间分布，解决了现有方法在稳定性和抗噪能力上的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有逆向弹性参数估计方法存在稳定性差、对噪声敏感及难以恢复绝对尺度弹性模量的问题，限制了其在临床和机械领域的应用。

Method: 设计了一种结合三种神经网络的IE-PINN模型，分别建模位移场、应变场和弹性参数分布，并采用两阶段估计策略和多种技术创新（如位置编码、正弦激活函数等）。

Result: 数值实验表明，IE-PINN即使在强噪声条件下也能准确估计绝对尺度弹性参数，显著优于现有方法。

Conclusion: IE-PINN在抗噪能力和稳定性上的突破，为临床诊断和材料力学表征提供了新的工具。

Abstract: Accurately estimating spatially heterogeneous elasticity parameters, particularly Young's modulus and Poisson's ratio, from noisy displacement measurements remains significantly challenging in inverse elasticity problems. Existing inverse estimation techniques are often limited by instability, pronounced sensitivity to measurement noise, and difficulty in recovering absolute-scale Young's modulus. This work presents a novel Inverse Elasticity Physics-Informed Neural Network (IE-PINN) specifically designed to robustly reconstruct heterogeneous distributions of elasticity parameters from noisy displacement data based on linear elasticity physics. IE-PINN integrates three distinct neural network architectures dedicated to separately modeling displacement fields, strain fields, and elasticity distributions, thereby significantly enhancing stability and accuracy against measurement noise. Additionally, a two-phase estimation strategy is introduced: the first phase recovers relative spatial distributions of Young's modulus and Poisson's ratio, and the second phase calibrates the absolute scale of Young's modulus using imposed loading boundary conditions. Additional methodological innovations, including positional encoding, sine activation functions, and a sequential pretraining protocol, further enhance the model's performance and robustness. Extensive numerical experiments demonstrate that IE-PINN effectively overcomes critical limitations encountered by existing methods, delivering accurate absolute-scale elasticity estimations even under severe noise conditions. This advancement holds substantial potential for clinical imaging diagnostics and mechanical characterization, where measurements typically encounter substantial noise.

</details>


### [92] [Load Balancing Mixture of Experts with Similarity Preserving Routers](https://arxiv.org/abs/2506.14038)
*Nabil Omi,Siddhartha Sen,Ali Farhadi*

Key words: 稀疏混合专家模型, 负载均衡, 路由一致性

TL;DR: 提出了一种新的负载均衡损失函数，通过保持令牌间的关系结构，提高稀疏混合专家模型的路由一致性和效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 稀疏混合专家模型需要平衡专家使用，以避免仅依赖少数专家，导致模型性能下降。当前方法通常鼓励专家均匀分布，但可能导致训练中路由行为不一致和冗余学习。

Method: 引入了一种新的负载均衡损失函数，通过鼓励相似输入在训练期间选择一致的专家，保持令牌间的关系结构。

Result: 实验表明，该方法比常用负载均衡损失函数收敛速度快36%，并减少了冗余。

Conclusion: 新损失函数有效提高了路由一致性和训练效率。

Abstract: Sparse Mixture of Experts (MoE) models offer a scalable and efficient architecture for training large neural networks by activating only a subset of parameters ("experts") for each input. A learned router computes a distribution over these experts, and assigns input tokens to a small subset. However, without auxiliary balancing mechanisms, routers often converge to using only a few experts, severely limiting model capacity and degrading performance. Most current load balancing mechanisms encourage a distribution over experts that resembles a roughly uniform distribution of experts per token. During training, this can result in inconsistent routing behavior, resulting in the model spending its capacity to learn redundant knowledge. We address this by introducing a novel load balancing loss that preserves token-wise relational structure, encouraging consistent expert choices for similar inputs during training. Our experimental results show that applying our loss to the router results in 36% faster convergence and lower redundancy compared to a popular load balancing loss.

</details>


### [93] [Scientifically-Interpretable Reasoning Network (ScIReN): Uncovering the Black-Box of Nature](https://arxiv.org/abs/2506.14054)
*Joshua Fan,Haodi Xu,Feng Tao,Md Nasim,Marc Grimson,Yiqi Luo,Carla P. Gomes*

Key words: neural networks, process-based models, interpretability, scientific discovery, ScIReN

TL;DR: ScIReN combines neural networks with process-based models for scientifically interpretable predictions, outperforming black-box models in accuracy and interpretability.

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: To bridge the gap between black-box neural networks and process-based models, enabling interpretable scientific discovery while maintaining predictive accuracy.

Method: Proposes ScIReN, a framework with an interpretable encoder and a differentiable process-based decoder, including a hard-sigmoid constraint layer for meaningful parameter ranges.

Result: ScIReN improves predictive accuracy and provides interpretable insights into latent scientific mechanisms and input feature relationships.

Conclusion: ScIReN successfully integrates neural and process-based reasoning for transparent scientific modeling, offering both accuracy and interpretability.

Abstract: Neural networks are a powerful tool for learning patterns from data. However, they do not respect known scientific laws, nor can they reveal novel scientific insights due to their black-box nature. In contrast, scientific reasoning distills biological or physical principles from observations and controlled experiments, and quantitatively interprets them with process-based models made of mathematical equations. Yet, process-based models rely on numerous free parameters that must be set in an ad-hoc manner, and thus often fit observations poorly in cross-scale predictions. While prior work has embedded process-based models in conventional neural networks, discovering interpretable relationships between parameters in process-based models and input features is still a grand challenge for scientific discovery. We thus propose Scientifically-Interpretable Reasoning Network (ScIReN), a fully-transparent framework that combines interpretable neural and process-based reasoning. An interpretable encoder predicts scientifically-meaningful latent parameters, which are then passed through a differentiable process-based decoder to predict labeled output variables. ScIReN also uses a novel hard-sigmoid constraint layer to restrict latent parameters to meaningful ranges defined by scientific prior knowledge, further enhancing its interpretability. While the embedded process-based model enforces established scientific knowledge, the encoder reveals new scientific mechanisms and relationships hidden in conventional black-box models. We apply ScIReN on two tasks: simulating the flow of organic carbon through soils, and modeling ecosystem respiration from plants. In both tasks, ScIReN outperforms black-box networks in predictive accuracy while providing substantial scientific interpretability -- it can infer latent scientific mechanisms and their relationships with input features.

</details>


### [94] [A Regret Perspective on Online Selective Generation](https://arxiv.org/abs/2506.14067)
*Minjae Lee,Yoonjae Jung,Sangdon Park*

Key words: 大型语言模型,选择性生成,幻觉效应,部分反馈,在线学习

TL;DR: 论文提出了一种在线学习算法，用于在部分反馈下进行选择性生成，以控制语言生成模型的幻觉效应，并通过反馈解锁技术优化收敛速度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决语言生成模型在交互过程中因幻觉效应导致的虚假响应问题，通过选择性生成提供了一种控制方法。

Method: 将选择性生成问题转化为多臂老虎机问题，提出了一种在线学习算法，并利用反馈解锁技术优化学习效率。

Result: 在多样化数据环境下，算法能够有效控制假发现率（FDR），同时保持合理的生成效率。

Conclusion: 所提出的方法在部分反馈下能有效控制幻觉效应，为语言生成模型的实用部署提供了理论基础和实践方案。

Abstract: Large language generative models increasingly interact with humans, while their falsified responses raise concerns. To address this hallucination effect, selectively abstaining from answering, called selective generation, provides an effective way for generators to control the hallucination when it is unsure of their answers. However, as selective generators are interacting under non-stochastic environments and having partial feedback from users on selective generation (e.g., thumbs up or down on the selected answer), learning methods for selective generation under such practical setups are crucial but currently missing. To address these limitations, we propose an online learning algorithm for selective generation under partial feedback. In particular, as learning under partial feedback is well-studied by multi-armed bandit problems, we reduce selective generation to bandits and provide a novel conversion lemma from bandits back to selective generation to leverage any known bandit algorithms and theoretical properties. This mainly connects regret guarantees of bandits to false discovery rate (FDR) guarantees of selective generation for controlling hallucination. However, naively exploiting known bandit algorithms and their regret bounds suffers from slow convergence speed in practice due the nature of partial feedback. To overcome this, we exploit a unique structure of arms in selective generation for feedback unlocking, i.e., unlocking unknown feedback from observed feedback. We theoretically and empirically evaluate the efficacy of the proposed online selective generation algorithm under partial feedback over diverse data environment setups, resulting in controlling a desired FDR, while maintaining reasonable selection efficiency, i.e., the ratio of non-abstaining answers, compared to baselines.

</details>


### [95] [Comprehensive Verilog Design Problems: A Next-Generation Benchmark Dataset for Evaluating Large Language Models and Agents on RTL Design and Verification](https://arxiv.org/abs/2506.14074)
*Nathaniel Pinckney,Chenhui Deng,Chia-Tung Ho,Yun-Da Tsai,Mingjie Liu,Wenfei Zhou,Brucek Khailany,Haoxing Ren*

Key words: CVDP, LLM, 硬件设计, 验证, 智能体任务

TL;DR: CVDP是一个新的数据集和基础设施，用于推动LLM和智能体在硬件设计和验证领域的研究，包含783个问题，覆盖13个任务类别，揭示了当前模型的局限性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了提供更真实和具有挑战性的硬件设计与验证基准，推动硬件设计自动化的研究。

Method: CVDP集成了783个问题，包括非智能体和智能体格式，使用开源工具和模型评分基础设施进行评估。

Result: 当前最先进模型在代码生成上的通过率不超过34%，智能体任务（尤其是涉及RTL重用和验证的任务）尤为困难。

Conclusion: CVDP揭示了当前模型在实际硬件设计中的能力不足，强调了继续研究的重要性。

Abstract: We present the Comprehensive Verilog Design Problems (CVDP) benchmark, a new dataset and infrastructure to advance LLM and agent research in hardware design and verification. CVDP includes 783 problems across 13 task categories, covering RTL generation, verification, debugging, specification alignment, and technical Q&A authored by experienced hardware engineers. Problems are offered in both non-agentic and agentic formats. The benchmark introduces more realistic and challenging contexts than prior work, with state-of-the-art models achieving no more than 34% pass@1 on code generation. Agentic tasks$\unicode{x2013}$especially those involving RTL reuse and verification$\unicode{x2013}$are particularly difficult. Evaluation uses open-source tools and model scoring infrastructure, with comprehension tasks assessed via BLEU and LLM-based judging. CVDP reveals substantial gaps in current model capabilities, underscoring the need for continued research toward robust, real-world hardware design automation.

</details>


### [96] [Multi-Scale Finetuning for Encoder-based Time Series Foundation Models](https://arxiv.org/abs/2506.14087)
*Zhongzheng Qiao,Chenghao Liu,Yiming Zhang,Ming Jin,Quang Pham,Qingsong Wen,P. N. Suganthan,Xudong Jiang,Savitha Ramasamy*

Key words: 时间序列基础模型,多尺度微调,零样本预测

TL;DR: 论文提出了MSFT框架，通过显式多尺度建模优化TSFMs的微调过程，提升零样本预测性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有TSFMs的微调方法未充分利用其多尺度能力，容易过拟合，需要改进。

Method: 提出MSFT框架，在微调过程中显式建模多尺度，适用于编码器型TSFMs。

Result: 实验表明，MSFT优于传统微调方法和深度学习SOTA方法。

Conclusion: MSFT能有效释放TSFMs潜力，提升下游任务性能。

Abstract: Time series foundation models (TSFMs) demonstrate impressive zero-shot performance for time series forecasting. However, an important yet underexplored challenge is how to effectively finetune TSFMs on specific downstream tasks. While naive finetuning can yield performance gains, we argue that it falls short of fully leveraging TSFMs' capabilities, often resulting in overfitting and suboptimal performance. Given the diverse temporal patterns across sampling scales and the inherent multi-scale forecasting capabilities of TSFMs, we adopt a causal perspective to analyze finetuning process, through which we highlight the critical importance of explicitly modeling multiple scales and reveal the shortcomings of naive approaches. Focusing on \textit{encoder-based} TSFMs, we propose \textbf{M}ulti\textbf{\textsc{s}}cale \textbf{\textsc{f}}ine\textbf{\textsc{t}}uning (\textbf{MSFT}), a simple yet general framework that explicitly integrates multi-scale modeling into the finetuning process. Experimental results on three different backbones (\moirai, \moment\ and \units) demonstrate that TSFMs finetuned with MSFT not only outperform naive and typical parameter efficient finetuning methods but also surpass state-of-the-art deep learning methods.

</details>


### [97] [Transformers Learn Faster with Semantic Focus](https://arxiv.org/abs/2506.14095)
*Parikshit Ram,Kenneth L. Clarkson,Tim Klinger,Shashanka Ubaru,Alexander G. Gray*

Key words: 稀疏注意力, Transformer, 学习性, 泛化能力, 理论分析

TL;DR: 稀疏注意力机制在Transformer中用于降低计算和内存成本。研究关注其学习性和泛化性，发现输入相关的稀疏注意力收敛更快、泛化更好，而输入无关的则无此优势。理论分析揭示了输入相关稀疏注意力的优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨稀疏注意力机制对Transformer学习中收敛速度和泛化能力的影响，而非单纯关注其效率。

Method: 实证研究多种注意力机制，理论分析稀疏注意力对Softmax稳定性和损失函数Lipschitz性质的影响。

Result: 输入相关的稀疏注意力收敛更快、泛化更好，输入无关的则无优势。理论分析验证了这一点。

Conclusion: 输入相关的稀疏注意力能通过集中模型的“语义焦点”加速学习，理论分析支持其优越性。

Abstract: Various forms of sparse attention have been explored to mitigate the quadratic computational and memory cost of the attention mechanism in transformers. We study sparse transformers not through a lens of efficiency but rather in terms of learnability and generalization. Empirically studying a range of attention mechanisms, we find that input-dependent sparse attention models appear to converge faster and generalize better than standard attention models, while input-agnostic sparse attention models show no such benefits -- a phenomenon that is robust across architectural and optimization hyperparameter choices. This can be interpreted as demonstrating that concentrating a model's "semantic focus" with respect to the tokens currently being considered (in the form of input-dependent sparse attention) accelerates learning. We develop a theoretical characterization of the conditions that explain this behavior. We establish a connection between the stability of the standard softmax and the loss function's Lipschitz properties, then show how sparsity affects the stability of the softmax and the subsequent convergence and generalization guarantees resulting from the attention mechanism. This allows us to theoretically establish that input-agnostic sparse attention does not provide any benefits. We also characterize conditions when semantic focus (input-dependent sparse attention) can provide improved guarantees, and we validate that these conditions are in fact met in our empirical evaluations.

</details>


### [98] [Toward a Graph Foundation Model: Pre-Training Transformers With Random Walks](https://arxiv.org/abs/2506.14098)
*Ziyuan Tang,Jie Chen*

Key words: 图基础模型, Transformer, 随机游走, 上下文预测, 预训练

TL;DR: 本文提出了一种基于Transformer架构的图基础模型，通过随机游走表示节点并预测上下文损失，展示了其作为图数据处理基础的潜力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 自然语言领域的基础模型（如GPT）展示了强大的能力，激发了构建类似图基础模型的兴趣。

Method: 采用Transformer架构，通过随机游走表示节点，提出上下文预测损失，并对表达能力进行理论分析。

Result: 模型通过预训练展示了对不同图数据的适应能力，并成功应用于下游任务。

Conclusion: 该模型有望成为图数据处理的基础模型，支持对图结构数据的推理和处理。

Abstract: A foundation model like GPT elicits many emergent abilities, owing to the pre-training with broad inclusion of data and the use of the powerful Transformer architecture. While foundation models in natural languages are prevalent, can we build similar models for graphs? This paper describes an approach toward a graph foundation model that is pre-trained with diverse graph datasets by adapting the Transformer backbone. A central challenge toward this end is how a sequence model encodes graphs of varying sizes and from different domains. We propose representing a node as multiple random walks, such that the Transformer can extract node representations from sequences, which in turn form edge and graph representations. We develop a novel context prediction loss for these random walks and theoretically analyze their expressive power in distinguishing neighborhoods and graphs. We also demonstrate the pre-training of our model and its adaptation to downstream tasks, showcasing its potential as a foundation for processing and reasoning with graph-structured data.

</details>


### [99] [SKOLR: Structured Koopman Operator Linear RNN for Time-Series Forecasting](https://arxiv.org/abs/2506.14113)
*Yitian Zhang,Liheng Ma,Antonios Valkanas,Boris N. Oreshkin,Mark Coates*

Key words: Koopman operator, RNN, time-series forecasting, nonlinear dynamics

TL;DR: 本文通过将非线性动态系统分析与线性循环神经网络（RNN）连接，提出了一种基于Koopman算子的高效预测框架SKOLR，其性能优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 利用Koopman算子理论的线性化优势，结合RNN在序列建模中的成功，设计一种高效的非线性系统预测方法。

Method: 提出SKOLR框架，通过可学习的频谱分解和多层感知机（MLP）构建测量函数，并利用线性RNN堆栈实现结构化Koopman算子。

Result: 在多种预测基准和动态系统实验中，该方法表现出卓越性能。

Conclusion: 基于Koopman理论的简洁设计能有效提升非线性动态系统的预测能力。

Abstract: Koopman operator theory provides a framework for nonlinear dynamical system analysis and time-series forecasting by mapping dynamics to a space of real-valued measurement functions, enabling a linear operator representation. Despite the advantage of linearity, the operator is generally infinite-dimensional. Therefore, the objective is to learn measurement functions that yield a tractable finite-dimensional Koopman operator approximation. In this work, we establish a connection between Koopman operator approximation and linear Recurrent Neural Networks (RNNs), which have recently demonstrated remarkable success in sequence modeling. We show that by considering an extended state consisting of lagged observations, we can establish an equivalence between a structured Koopman operator and linear RNN updates. Building on this connection, we present SKOLR, which integrates a learnable spectral decomposition of the input signal with a multilayer perceptron (MLP) as the measurement functions and implements a structured Koopman operator via a highly parallel linear RNN stack. Numerical experiments on various forecasting benchmarks and dynamical systems show that this streamlined, Koopman-theory-based design delivers exceptional performance.

</details>


### [100] [Evaluating Loss Functions for Graph Neural Networks: Towards Pretraining and Generalization](https://arxiv.org/abs/2506.14114)
*Khushnood Abbas,Ruizhe Hou,Zhou Wengang,Dong Shi,Niu Ling,Satyaki Nan,Alireza Abbasi*

Key words: GNN, 损失函数, 归纳学习, 传导学习, 多目标优化

TL;DR: GNN模型和损失函数的组合对性能影响显著，研究通过大规模评估发现，混合损失函数在归纳设置中表现更优，GIN架构整体表现最佳，而GAT架构在某些混合损失下具有特定优势。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前研究缺乏对GNN模型和多种损失函数组合的大规模评估，因此需要系统分析其在实际任务中的表现。

Method: 研究涵盖7种GNN架构和30种损失函数，并在归纳和传导设置下评估了三组真实数据集，使用21个评价指标。

Result: 结果发现混合损失函数表现更优，GIN架构整体性能最高，GAT架构在特定损失函数下展现突出优势，MPNN表现较差。

Conclusion: 优化GNN性能需综合考虑模型和损失函数的组合，混合损失函数和多目标优化特别适合归纳任务。

Abstract: Graph Neural Networks (GNNs) became useful for learning on non-Euclidean data. However, their best performance depends on choosing the right model architecture and the training objective, also called the loss function. Researchers have studied these parts separately, but a large-scale evaluation has not looked at how GNN models and many loss functions work together across different tasks. To fix this, we ran a thorough study - it included seven well-known GNN architectures. We also used a large group of 30 single plus mixed loss functions. The study looked at both inductive and transductive settings. Our evaluation spanned three distinct real-world datasets, assessing performance in both inductive and transductive settings using 21 comprehensive evaluation metrics. From these extensive results (detailed in supplementary information 1 \& 2), we meticulously analyzed the top ten model-loss combinations for each metric based on their average rank. Our findings reveal that, especially for the inductive case: 1) Hybrid loss functions generally yield superior and more robust performance compared to single loss functions, indicating the benefit of multi-objective optimization. 2) The GIN architecture always showed the highest-level average performance, especially with Cross-Entropy loss. 3) Although some combinations had overall lower average ranks, models such as GAT, particularly with certain hybrid losses, demonstrated incredible specialized strengths, maximizing the most top-1 results among the individual metrics, emphasizing subtle strengths for particular task demands. 4) On the other hand, the MPNN architecture typically lagged behind the scenarios it was tested against.

</details>


### [101] [CLGNN: A Contrastive Learning-based GNN Model for Betweenness Centrality Prediction on Temporal Graphs](https://arxiv.org/abs/2506.14122)
*Tianming Zhang,Renbo Zhang,Zhengyi Yang,Yunjun Gao,Bin Cao,Jing Fan*

Key words: 时间中介中心性,图神经网络,对比学习,聚类,不平衡数据

TL;DR: 论文提出了一种基于对比学习的GNN（CLGNN），用于高效准确地预测时间中介中心性（TBC），并通过稳定性聚类和回归模块解决了数据不平衡问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法无法有效处理TBC计算的高成本和分布不平衡问题，导致预测不准确。

Method: CLGNN通过构建实例图保留路径有效性和时间顺序，采用双聚合机制编码特征，并引入对比学习和回归模块。

Result: CLGNN在多个基准测试中表现优异，速度提升高达663.7倍，预测精度显著优于静态和动态GNN基线。

Conclusion: CLGNN是一种高效且准确的时间网络中心性预测方法。

Abstract: Temporal Betweenness Centrality (TBC) measures how often a node appears on optimal temporal paths, reflecting its importance in temporal networks. However, exact computation is highly expensive, and real-world TBC distributions are extremely imbalanced. The severe imbalance leads learning-based models to overfit to zero-centrality nodes, resulting in inaccurate TBC predictions and failure to identify truly central nodes. Existing graph neural network (GNN) methods either fail to handle such imbalance or ignore temporal dependencies altogether. To address these issues, we propose a scalable and inductive contrastive learning-based GNN (CLGNN) for accurate and efficient TBC prediction. CLGNN builds an instance graph to preserve path validity and temporal order, then encodes structural and temporal features using dual aggregation, i.e., mean and edge-to-node multi-head attention mechanisms, enhanced by temporal path count and time encodings. A stability-based clustering-guided contrastive module (KContrastNet) is introduced to separate high-, median-, and low-centrality nodes in representation space, mitigating class imbalance, while a regression module (ValueNet) estimates TBC values. CLGNN also supports multiple optimal path definitions to accommodate diverse temporal semantics. Extensive experiments demonstrate the effectiveness and efficiency of CLGNN across diverse benchmarks. CLGNN achieves up to a 663.7~$\times$ speedup compared to state-of-the-art exact TBC computation methods. It outperforms leading static GNN baselines with up to 31.4~$\times$ lower MAE and 16.7~$\times$ higher Spearman correlation, and surpasses state-of-the-art temporal GNNs with up to 5.7~$\times$ lower MAE and 3.9~$\times$ higher Spearman correlation.

</details>


### [102] [Less is More: Undertraining Experts Improves Model Upcycling](https://arxiv.org/abs/2506.14126)
*Stefan Horoi,Guy Wolf,Eugene Belilovsky,Gintare Karolina Dziugaite*

Key words: 模型升级, 专家微调, LoRA, MoE层, 早期停止

TL;DR: 研究发现，专家模型的过度微调会降低模型升级的性能，早期停止策略能显著改善结果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨专家微调如何影响模型升级性能，挑战改进一个阶段会自动提升下游性能的假设。

Method: 分析专家微调对模型升级的影响，包括完全微调和LoRA适应模型，并提出早期停止策略。

Result: 过度微调会导致性能下降，早期停止能显著提升升级性能。

Conclusion: 适度的微调策略对模型升级至关重要，早期停止是有效方法。

Abstract: Modern deep learning is increasingly characterized by the use of open-weight foundation models that can be fine-tuned on specialized datasets. This has led to a proliferation of expert models and adapters, often shared via platforms like HuggingFace and AdapterHub. To leverage these resources, numerous model upcycling methods have emerged, enabling the reuse of fine-tuned models in multi-task systems. A natural pipeline has thus formed to harness the benefits of transfer learning and amortize sunk training costs: models are pre-trained on general data, fine-tuned on specific tasks, and then upcycled into more general-purpose systems. A prevailing assumption is that improvements at one stage of this pipeline propagate downstream, leading to gains at subsequent steps. In this work, we challenge that assumption by examining how expert fine-tuning affects model upcycling. We show that long fine-tuning of experts that optimizes for their individual performance leads to degraded merging performance, both for fully fine-tuned and LoRA-adapted models, and to worse downstream results when LoRA adapters are upcycled into MoE layers. We trace this degradation to the memorization of a small set of difficult examples that dominate late fine-tuning steps and are subsequently forgotten during merging. Finally, we demonstrate that a task-dependent aggressive early stopping strategy can significantly improve upcycling performance.

</details>


### [103] [Leveraging Predictive Equivalence in Decision Trees](https://arxiv.org/abs/2506.14143)
*Hayden McTavish,Zachery Boner,Jon Donnelly,Margo Seltzer,Cynthia Rudin*

Key words: 决策树, 预测等价性, 布尔逻辑表示, 变量重要性, 缺失值处理, 预测成本优化

TL;DR: 论文提出了一种布尔逻辑表示方法来解决决策树中的预测等价性问题，并展示了该方法在多个机器学习任务中的应用，包括对缺失值的鲁棒性、变量重要性度量和预测成本优化。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 决策树因其可解释性而被广泛使用，但存在预测等价性问题（即多个决策树可以表示相同的决策边界）。这导致模型选择困难，影响变量重要性分析和对缺失值的处理。

Method: 提出了一种布尔逻辑表示方法，能够避免预测等价性，并忠实于决策边界。该方法被应用于多个下游任务中。

Result: 研究结果表明，决策树对测试时的特征缺失表现出令人惊讶的鲁棒性；解决了预测等价性对变量重要性度量的影响；并提出了一种优化预测成本的算法。

Conclusion: 布尔逻辑表示方法有效解决了决策树中的预测等价性问题，提升了对变量重要性和缺失值处理的准确性，同时提供了预测成本优化的新途径。

Abstract: Decision trees are widely used for interpretable machine learning due to their clearly structured reasoning process. However, this structure belies a challenge we refer to as predictive equivalence: a given tree's decision boundary can be represented by many different decision trees. The presence of models with identical decision boundaries but different evaluation processes makes model selection challenging. The models will have different variable importance and behave differently in the presence of missing values, but most optimization procedures will arbitrarily choose one such model to return. We present a boolean logical representation of decision trees that does not exhibit predictive equivalence and is faithful to the underlying decision boundary. We apply our representation to several downstream machine learning tasks. Using our representation, we show that decision trees are surprisingly robust to test-time missingness of feature values; we address predictive equivalence's impact on quantifying variable importance; and we present an algorithm to optimize the cost of reaching predictions.

</details>


### [104] [Common Benchmarks Undervalue the Generalization Power of Programmatic Policies](https://arxiv.org/abs/2506.14162)
*Amirhossein Rajabpour,Kiarash Aghakasiri,Sandra Zilles,Levi H. S. Lelis*

Key words: 程序化策略、神经策略、分布外泛化、基准测试、训练流程

TL;DR: 论文认为程序化策略和神经策略在分布外问题上的泛化能力被低估，通过简单调整神经策略的训练流程（如简化结构和奖励函数）可实现相同效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有研究认为程序化策略的泛化能力优于神经策略，但作者认为这可能是因为基准测试未充分评估神经策略的潜力。

Method: 分析文献中的实验，提出调整神经策略的训练流程（如简化结构和修改奖励函数）以提高其泛化能力。

Result: 神经策略通过调整后能与程序化策略在分布外问题上表现相当，且提出了用于测试泛化能力的基准任务的设计建议。

Conclusion: 神经策略的泛化能力被低估，适当的训练调整能显著提升其表现，未来应设计更合理的基准任务以测试泛化能力。

Abstract: Algorithms for learning programmatic representations for sequential decision-making problems are often evaluated on out-of-distribution (OOD) problems, with the common conclusion that programmatic policies generalize better than neural policies on OOD problems. In this position paper, we argue that commonly used benchmarks undervalue the generalization capabilities of programmatic representations. We analyze the experiments of four papers from the literature and show that neural policies, which were shown not to generalize, can generalize as effectively as programmatic policies on OOD problems. This is achieved with simple changes in the neural policies training pipeline. Namely, we show that simpler neural architectures with the same type of sparse observation used with programmatic policies can help attain OOD generalization. Another modification we have shown to be effective is the use of reward functions that allow for safer policies (e.g., agents that drive slowly can generalize better). Also, we argue for creating benchmark problems highlighting concepts needed for OOD generalization that may challenge neural policies but align with programmatic representations, such as tasks requiring algorithmic constructs like stacks.

</details>


### [105] [Light Aircraft Game : Basic Implementation and training results analysis](https://arxiv.org/abs/2506.14164)
*Hanzhong Cao*

Key words: 多智能体强化学习, 部分可观测, 合作-对抗战斗环境, HAPPO, HASAC

TL;DR: 研究多智能体强化学习（MARL）在部分可观测、合作-对抗的战斗环境LAG中的表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索在复杂战斗环境（如LAG）中，不同MARL算法的性能差异，以理解其适用性和协调能力。

Method: 使用两种代表性算法HAPPO（基于分层PPO）和HASAC（基于软行动者-批评者），分析其训练稳定性、奖励进展和多智能体协调能力。

Result: HASAC在无武器的简单协调任务中表现良好，而HAPPO在涉及导弹战斗的动态场景中更具适应性。

Conclusion: 揭示了在MARL中，策略内和策略外方法在不同任务中的权衡。

Abstract: This paper investigates multi-agent reinforcement learning (MARL) in a partially observable, cooperative-competitive combat environment known as LAG. We describe the environment's setup, including agent actions, hierarchical controls, and reward design across different combat modes such as No Weapon and ShootMissile. Two representative algorithms are evaluated: HAPPO, an on-policy hierarchical variant of PPO, and HASAC, an off-policy method based on soft actor-critic. We analyze their training stability, reward progression, and inter-agent coordination capabilities. Experimental results show that HASAC performs well in simpler coordination tasks without weapons, while HAPPO demonstrates stronger adaptability in more dynamic and expressive scenarios involving missile combat. These findings provide insights into the trade-offs between on-policy and off-policy methods in multi-agent settings.

</details>


### [106] [Structured and Informed Probabilistic Modeling with the Thermodynamic Kolmogorov-Arnold Model](https://arxiv.org/abs/2506.14167)
*Prithvi Raj*

Key words: Kolmogorov-Arnold表示定理, 生成模型, 马尔可夫核, 能量先验, Langevin蒙特卡洛

TL;DR: 该论文将Kolmogorov-Arnold表示定理重新解释为概率空间之间的马尔可夫核，提出了一种可解释、易设计且高效的生成模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 结合经典表示定理与现代概率建模，提升生成模型的训练稳定性、推断速度及生成质量与多样性。

Method: 采用Kolmogorov-Arnold网络生成器与独立基于能量的先验结合，通过最大似然训练，并引入混合分布和Langevin蒙特卡洛方法增强灵活性。

Result: 模型实现了快速推断，并通过先验知识提升学习效率和样本质量，同时支持先验的可恢复性和可视化。

Conclusion: 该方法成功将经典理论与现代建模结合，平衡了灵活性与训练效率。

Abstract: We adapt the Kolmogorov-Arnold Representation Theorem to generative modeling by reinterpreting its inner functions as a Markov Kernel between probability spaces via inverse transform sampling. We present a generative model that is interpretable, easy to design, and efficient. Our approach couples a Kolmogorov-Arnold Network generator with independent energy-based priors, trained via Maximum Likelihood. Inverse sampling enables fast inference, while prior knowledge can be incorporated before training to better align priors with posteriors, thereby improving learning efficiency and sample quality. The learned prior is also recoverable and visualizable post-training, offering an empirical Bayes perspective. To address inflexibility and mitigate prior-posterior mismatch, we introduce scalable extensions based on mixture distributions and Langevin Monte Carlo methods, admitting a trade-off between flexibility and training efficiency. Our contributions connect classical representation theorems with modern probabilistic modeling, while balancing training stability, inference speed, and the quality and diversity of generations.

</details>


### [107] [A Variational Information Theoretic Approach to Out-of-Distribution Detection](https://arxiv.org/abs/2506.14194)
*Sudeepta Mondal,Zhuolin Jiang,Ganesh Sundaramoorthi*

Key words: 分布外检测,信息论,KL散度,信息瓶颈,变分优化

TL;DR: 本文提出了一种用于神经网络构建分布外（OOD）检测特征的理论，通过信息论损失函数优化，并预测了一种优于现有方法的新特征。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在通过信息论框架，构造可解释且高效的OOD检测特征。

Method: 采用包含KL散度和信息瓶颈的两项信息论损失函数，通过变分优化获得特征。

Result: 理论能预测并验证一种优于现有方法的新特征，并在OOD基准测试中表现优异。

Conclusion: 该理论为构造多样且可解释的OOD特征提供了通用框架。

Abstract: We present a theory for the construction of out-of-distribution (OOD) detection features for neural networks. We introduce random features for OOD through a novel information-theoretic loss functional consisting of two terms, the first based on the KL divergence separates resulting in-distribution (ID) and OOD feature distributions and the second term is the Information Bottleneck, which favors compressed features that retain the OOD information. We formulate a variational procedure to optimize the loss and obtain OOD features. Based on assumptions on OOD distributions, one can recover properties of existing OOD features, i.e., shaping functions. Furthermore, we show that our theory can predict a new shaping function that out-performs existing ones on OOD benchmarks. Our theory provides a general framework for constructing a variety of new features with clear explainability.

</details>


### [108] [DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion](https://arxiv.org/abs/2506.14202)
*Makoto Shing,Takuya Akiba*

Key words: 内存效率,扩散过程,神经网络块,生成任务

TL;DR: 提出了DiffusionBlocks框架，通过将神经网络块解释为连续时间扩散过程中的去噪操作，显著提高了内存效率，同时在生成任务中保持与传统反向传播相当的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于端到端反向传播训练大型神经网络存在显著的内存瓶颈，限制了先进AI研究的普及，需要一种更高效的方法。

Method: 将网络划分为可独立训练的块，并根据等累积概率质量优化噪声级别分配。

Result: 在图像生成和语言建模任务中，内存减少与块数成正比，同时性能优于传统方法。

Conclusion: DiffusionBlocks为在有限计算资源下训练大规模神经网络提供了可行路径。

Abstract: Training large neural networks with end-to-end backpropagation creates significant memory bottlenecks, limiting accessibility to state-of-the-art AI research. We propose $\textit{DiffusionBlocks}$, a novel training framework that interprets neural network blocks as performing denoising operations in a continuous-time diffusion process. By partitioning the network into independently trainable blocks and optimizing noise level assignments based on equal cumulative probability mass, our approach achieves significant memory efficiency while maintaining competitive performance compared to traditional backpropagation in generative tasks. Experiments on image generation and language modeling tasks demonstrate memory reduction proportional to the number of blocks while achieving superior performance. DiffusionBlocks provides a promising pathway for democratizing access to large-scale neural network training with limited computational resources.

</details>


### [109] [TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift](https://arxiv.org/abs/2506.14217)
*Dipesh Tharu Mahato,Rohan Poudel,Pramod Dhungana*

Key words: 深度神经网络, 鲁棒性, 可解释性, 归因熵, 归因漂移

TL;DR: TriGuard 是一个统一的安全评估框架，结合了形式化鲁棒性验证、归因熵和新型归因漂移评分，揭示模型准确性和可解释性之间的不匹配，并通过实验验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 深度神经网络虽准确率高，但在对抗性和分布偏移下的可靠性仍是挑战，亟需一种综合评估框架。

Method: TriGuard 结合形式化鲁棒性验证、归因熵（量化显著性集中度）和归因漂移评分（衡量解释稳定性），评估模型安全性。

Result: 实验表明，TriGuard 能发现神经推理中的脆弱性，且熵正则化训练可减少解释漂移而不影响性能。

Conclusion: TriGuard 提升了模型鲁棒性和可解释性评估的边界。

Abstract: Deep neural networks often achieve high accuracy, but ensuring their reliability under adversarial and distributional shifts remains a pressing challenge. We propose TriGuard, a unified safety evaluation framework that combines (1) formal robustness verification, (2) attribution entropy to quantify saliency concentration, and (3) a novel Attribution Drift Score measuring explanation stability. TriGuard reveals critical mismatches between model accuracy and interpretability: verified models can still exhibit unstable reasoning, and attribution-based signals provide complementary safety insights beyond adversarial accuracy. Extensive experiments across three datasets and five architectures show how TriGuard uncovers subtle fragilities in neural reasoning. We further demonstrate that entropy-regularized training reduces explanation drift without sacrificing performance. TriGuard advances the frontier in robust, interpretable model evaluation.

</details>


### [110] [Can Large Language Models Improve Spectral Graph Neural Networks?](https://arxiv.org/abs/2506.14220)
*Kangkang Lu,Yanhua Yu,Zhiyong Huang,Tat-Seng Chua*

Key words: Spectral Graph Neural Networks, Large Language Models, homophily, polynomial spectral filters

TL;DR: 提出了利用大型语言模型（LLMs）估计图的同质性，并以此指导SGNN中光谱滤波器的设计，从而提升其表达能力和适应性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: SGNN在标签稀缺条件下可能学习次优滤波器，而LLMs的成功启发了其在GNN领域的潜在应用。

Method: 利用LLMs生成同质性先验信息，注入滤波器设计以适应不同的图结构。

Result: 实验证明，该方法在基准数据集上优于现有基线，计算和成本开销较小。

Conclusion: LLM驱动的SGNN框架能有效提升性能，尤其在标签稀缺情况下。

Abstract: Spectral Graph Neural Networks (SGNNs) have attracted significant attention due to their ability to approximate arbitrary filters. They typically rely on supervision from downstream tasks to adaptively learn appropriate filters. However, under label-scarce conditions, SGNNs may learn suboptimal filters, leading to degraded performance. Meanwhile, the remarkable success of Large Language Models (LLMs) has inspired growing interest in exploring their potential within the GNN domain. This naturally raises an important question: \textit{Can LLMs help overcome the limitations of SGNNs and enhance their performance?} In this paper, we propose a novel approach that leverages LLMs to estimate the homophily of a given graph. The estimated homophily is then used to adaptively guide the design of polynomial spectral filters, thereby improving the expressiveness and adaptability of SGNNs across diverse graph structures. Specifically, we introduce a lightweight pipeline in which the LLM generates homophily-aware priors, which are injected into the filter coefficients to better align with the underlying graph topology. Extensive experiments on benchmark datasets demonstrate that our LLM-driven SGNN framework consistently outperforms existing baselines under both homophilic and heterophilic settings, with minimal computational and monetary overhead.

</details>


### [111] [Convergence-Privacy-Fairness Trade-Off in Personalized Federated Learning](https://arxiv.org/abs/2506.14251)
*Xiyu Zhao,Qimei Cui,Weicai Li,Wei Ni,Ekram Hossain,Quan Z. Sheng,Xiaofeng Tao,Ping Zhang*

Key words: 个性化联邦学习、差分隐私、收敛性、性能公平性、DP-Ditto

TL;DR: 本文提出DP-Ditto，一种在差分隐私保护下对Ditto非平凡扩展的个性化联邦学习方法，分析了隐私保证、模型收敛和性能公平性的权衡，并通过实验验证其优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究如何平衡个性化联邦学习中隐私保护与模型性能的需求，解决现有方法在隐私扰动后收敛和公平性问题。

Method: 提出DP-Ditto方法，分析个性化模型在差分隐私下的收敛上界，优化全局聚合次数，并联合优化收敛性和公平性。

Result: 实验显示DP-Ditto在公平性和准确性上分别比现有方法如FedAMP等提升32.71%和9.66%。

Conclusion: DP-Ditto在隐私保护、收敛性和性能公平性方面具有显著优势，为个性化联邦学习提供了有效解决方案。

Abstract: Personalized federated learning (PFL), e.g., the renowned Ditto, strikes a balance between personalization and generalization by conducting federated learning (FL) to guide personalized learning (PL). While FL is unaffected by personalized model training, in Ditto, PL depends on the outcome of the FL. However, the clients' concern about their privacy and consequent perturbation of their local models can affect the convergence and (performance) fairness of PL. This paper presents PFL, called DP-Ditto, which is a non-trivial extension of Ditto under the protection of differential privacy (DP), and analyzes the trade-off among its privacy guarantee, model convergence, and performance distribution fairness. We also analyze the convergence upper bound of the personalized models under DP-Ditto and derive the optimal number of global aggregations given a privacy budget. Further, we analyze the performance fairness of the personalized models, and reveal the feasibility of optimizing DP-Ditto jointly for convergence and fairness. Experiments validate our analysis and demonstrate that DP-Ditto can surpass the DP-perturbed versions of the state-of-the-art PFL models, such as FedAMP, pFedMe, APPLE, and FedALA, by over 32.71% in fairness and 9.66% in accuracy.

</details>


### [112] [RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?](https://arxiv.org/abs/2506.14261)
*Rohan Gupta,Erik Jenner*

Key words: 大型语言模型,潜在空间监控器,强化学习,规避攻击,鲁棒性

TL;DR: 论文研究了大型语言模型（LLM）如何通过强化学习绕过潜在空间监控器，发现某些监控器易受攻击，但更全面的监控器仍能保持鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究动机是探讨LLM是否能够学会规避潜在空间监控器，以识别其在实际应用中的潜在风险。

Method: 方法是通过RL-Obfuscation（强化学习微调）使LLM绕过监控器，同时保持生成的连贯性，并测试其对不同类型监控器的规避效果。

Result: 结果显示，token-level监控器易受攻击，而max-pooling或attention-based监控器更鲁棒；策略还能泛化到未见过的同类监控器。

Conclusion: 结论是潜在空间监控器的设计需更全面，以应对LLM可能的学习规避行为。

Abstract: Latent-space monitors aim to detect undesirable behaviours in large language models by leveraging internal model representations rather than relying solely on black-box outputs. These methods have shown promise in identifying behaviours such as deception and unsafe completions, but a critical open question remains: can LLMs learn to evade such monitors? To study this, we introduce RL-Obfuscation, in which LLMs are finetuned via reinforcement learning to bypass latent-space monitors while maintaining coherent generations. We apply RL-Obfuscation to LLMs ranging from 7B to 14B parameters and evaluate evasion success against a suite of monitors. We find that token-level latent-space monitors are highly vulnerable to this attack. More holistic monitors, such as max-pooling or attention-based probes, remain robust. Moreover, we show that adversarial policies trained to evade a single static monitor generalise to unseen monitors of the same type. Finally, we study how the policy learned by RL bypasses these monitors and find that the model can also learn to repurpose tokens to mean something different internally.

</details>


### [113] [Knowledge Adaptation as Posterior Correction](https://arxiv.org/abs/2506.14262)
*Mohammad Emtiyaz Khan*

Key words: 适应, 后验修正, 贝叶斯学习规则, 自然梯度, 快速学习

TL;DR: 论文探讨了人工智能模型的快速适应问题，提出所有适应方法均可视为对近似后验的“修正”，并通过贝叶斯学习规则的双重视角展示后验修正是机器快速适应的自然机制。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨如何让机器像人类和动物一样自然地快速适应，当前AI模型（如GPT）缺乏类似幼儿的适应能力。

Method: 通过贝叶斯学习规则的双重视角，将各种适应方法视为对近似后验的修正，并分析自然梯度不匹配对适应的影响。

Result: 更准确的后验导致更小的修正，从而实现更快的适应。文中通过多个例子展示了后验修正作为快速适应机制的有效性。

Conclusion: 后验修正是机器实现快速适应的自然机制，为未来研究提供了新的方向。

Abstract: Adaptation is the holy grail of intelligence, but even the best AI models (like GPT) lack the adaptivity of toddlers. So the question remains: how can machines adapt quickly? Despite a lot of progress on model adaptation to facilitate continual and federated learning, as well as model merging, editing, unlearning, etc., little is known about the mechanisms by which machines can naturally learn to adapt in a similar way as humans and animals. Here, we show that all such adaptation methods can be seen as different ways of `correcting' the approximate posteriors. More accurate posteriors lead to smaller corrections, which in turn imply quicker adaptation. The result is obtained by using a dual-perspective of the Bayesian Learning Rule of Khan and Rue (2023) where interference created during adaptation is characterized by the natural-gradient mismatch over the past data. We present many examples to demonstrate the use of posterior-correction as a natural mechanism for the machines to learn to adapt quickly.

</details>


### [114] [Towards Robust Learning to Optimize with Theoretical Guarantees](https://arxiv.org/abs/2506.14263)
*Qingyu Song,Wei Lin,Juncheng Wang,Hong Xu*

Key words: 学习优化（L2O）、分布外（OOD）、鲁棒性、梯度特征、收敛速度

TL;DR: 本文提出了一种理论证明学习优化（L2O）方法在分布外（OOD）场景下性能与鲁棒性的框架，并提出了一种新模型，通过梯度和历史建模方法在InD和OOD场景中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管L2O在无线通信、计算机网络等实际场景中表现出色，但缺乏对OOD场景下性能与鲁棒性的理论证明。本文旨在填补这一空白。

Method: 提出了一个鲁棒L2O模型的充分条件，并基于对齐OOD与InD问题的方法，进一步证明了OOD场景下收敛率的下降。此外，提出了一种梯度特征构建和梯度历史建模的新方法。

Result: 数值模拟表明，所提模型在InD和OOD场景下均优于现有基线，收敛速度提升高达10倍。

Conclusion: 本文为L2O在OOD场景下的鲁棒性提供了理论支持，并通过新模型实现了性能提升。

Abstract: Learning to optimize (L2O) is an emerging technique to solve mathematical optimization problems with learning-based methods. Although with great success in many real-world scenarios such as wireless communications, computer networks, and electronic design, existing L2O works lack theoretical demonstration of their performance and robustness in out-of-distribution (OOD) scenarios. We address this gap by providing comprehensive proofs. First, we prove a sufficient condition for a robust L2O model with homogeneous convergence rates over all In-Distribution (InD) instances. We assume an L2O model achieves robustness for an InD scenario. Based on our proposed methodology of aligning OOD problems to InD problems, we also demonstrate that the L2O model's convergence rate in OOD scenarios will deteriorate by an equation of the L2O model's input features. Moreover, we propose an L2O model with a concise gradient-only feature construction and a novel gradient-based history modeling method. Numerical simulation demonstrates that our proposed model outperforms the state-of-the-art baseline in both InD and OOD scenarios and achieves up to 10 $\times$ convergence speedup. The code of our method can be found from https://github.com/NetX-lab/GoMathL2O-Official.

</details>


### [115] [Improving LoRA with Variational Learning](https://arxiv.org/abs/2506.14280)
*Bai Cong,Nico Daheim,Yuesong Shen,Rio Yokota,Mohammad Emtiyaz Khan,Thomas Möllenhoff*

Key words: IVON, LoRA, 变分学习, 贝叶斯方法, 大规模语言模型

TL;DR: 论文提出使用IVON变分算法改进LoRA微调，解决了贝叶斯方法的计算开销和性能问题，在大规模LLM上显著提升准确性和校准效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 贝叶斯方法在LoRA微调中虽然改进了校准效果，但对其他指标（如准确性）的改进有限，甚至可能有所损害，且增加了计算开销。

Method: 采用IVON变分算法，结合简单的后验剪枝技术，实现低成本、高效的微调。

Result: 在Llama-3.2-3B等模型上，IVON将准确性提升1.3%，ECE降低5.4%，优于AdamW和其他贝叶斯方法。

Conclusion: IVON变分学习能有效改进LoRA微调，为大规模语言模型提供更优的微调方案。

Abstract: Bayesian methods have recently been used to improve LoRA finetuning and, although they improve calibration, their effect on other metrics (such as accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian methods also increase computational overheads and require additional tricks for them to work well. Here, we fix these issues by using a recently proposed variational algorithm called IVON. We show that IVON is easy to implement and has similar costs to AdamW, and yet it can also drastically improve many metrics by using a simple posterior pruning technique. We present extensive results on billion-scale LLMs (Llama and Qwen series) going way beyond the scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B model on a set of commonsense reasoning tasks and improve accuracy over AdamW by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian methods like Laplace-LoRA and BLoB. Overall, our results show that variational learning with IVON can effectively improve LoRA finetuning.

</details>


### [116] [Equivariance Everywhere All At Once: A Recipe for Graph Foundation Models](https://arxiv.org/abs/2506.14291)
*Ben Finkelshtein,İsmail İlkan Ceylan,Michael Bronstein,Ron Levie*

Key words: 图机器学习,图基础模型,对称性,置换等变,零样本学习

TL;DR: 提出了一个用于设计图基础模型的框架，强调了节点和标签置换等变以及特征置换不变的重要性，并通过实验验证了其普适性和零样本性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对图机器学习模型通常局限于特定任务和数据集的问题，探索如何构建能够泛化到任意图和特征的图基础模型。

Method: 通过系统研究图基础模型必须遵循的对称性，提出了一个基于节点、标签置换等变和特征置换不变的线性变换空间，并证明其是一种通用的多集逼近器。

Result: 在29个真实世界的节点分类数据集上进行了实验，展示了强大的零样本性能及随着训练图数量增加的持续改进。

Conclusion: 提出的方法为图基础模型的设计提供了有效框架，能够广泛适用于节点属性预测任务。

Abstract: Graph machine learning architectures are typically tailored to specific tasks on specific datasets, which hinders their broader applicability. This has led to a new quest in graph machine learning: how to build graph foundation models capable of generalizing across arbitrary graphs and features? In this work, we present a recipe for designing graph foundation models for node-level tasks from first principles. The key ingredient underpinning our study is a systematic investigation of the symmetries that a graph foundation model must respect. In a nutshell, we argue that label permutation-equivariance alongside feature permutation-invariance are necessary in addition to the common node permutation-equivariance on each local neighborhood of the graph. To this end, we first characterize the space of linear transformations that are equivariant to permutations of nodes and labels, and invariant to permutations of features. We then prove that the resulting network is a universal approximator on multisets that respect the aforementioned symmetries. Our recipe uses such layers on the multiset of features induced by the local neighborhood of the graph to obtain a class of graph foundation models for node property prediction. We validate our approach through extensive experiments on 29 real-world node classification datasets, demonstrating both strong zero-shot empirical performance and consistent improvement as the number of training graphs increases.

</details>


### [117] [Fair for a few: Improving Fairness in Doubly Imbalanced Datasets](https://arxiv.org/abs/2506.14306)
*Ata Yalcin,Asli Umay Ozturk,Yigit Sever,Viktoria Pauw,Stephan Hachinger,Ismail Hakki Toroslu,Pinar Karagoz*

Key words: 公平性,双重不平衡数据集,去偏,多准则优化,机器学习

TL;DR: 本文探讨了在标签和敏感属性双重不平衡的数据集中实现公平性的挑战，并提出了一种多准则方法来优化采样和分布，以同时提高公平性和分类准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 公平性是机器学习和人工智能决策解决方案中的重要考量，但在数据收集中存在双重不平衡（标签和敏感属性）时，现有去偏方法表现不佳。

Method: 首先进行探索性分析，揭示双重不平衡数据集中去偏的局限性；随后提出一种基于多准则的解决方案，优化采样和分布。

Result: 提出的方法在双重不平衡数据集中实现了公平性和分类准确性的平衡。

Conclusion: 该研究为处理双重不平衡数据集中的公平性问题提供了一种有效的多准则解决方案。

Abstract: Fairness has been identified as an important aspect of Machine Learning and Artificial Intelligence solutions for decision making. Recent literature offers a variety of approaches for debiasing, however many of them fall short when the data collection is imbalanced. In this paper, we focus on a particular case, fairness in doubly imbalanced datasets, such that the data collection is imbalanced both for the label and the groups in the sensitive attribute. Firstly, we present an exploratory analysis to illustrate limitations in debiasing on a doubly imbalanced dataset. Then, a multi-criteria based solution is proposed for finding the most suitable sampling and distribution for label and sensitive attribute, in terms of fairness and classification accuracy

</details>


### [118] [IntelliLung: Advancing Safe Mechanical Ventilation using Offline RL with Hybrid Actions and Clinically Aligned Rewards](https://arxiv.org/abs/2506.14375)
*Muhammad Hamza Yousuf,Jason Li,Sahar Vahdati,Raphael Theilen,Jakob Wittenstein,Jens Lehmann*

Key words: 机械通气, 强化学习, 混合动作空间, 患者安全, 重症护理

TL;DR: 论文提出了一种优化离线强化学习（RL）的方法，用于机械通气（MV）控制，通过解决混合动作空间的挑战、避免离散化缺陷，并结合临床奖励函数，提升患者安全和个性化肺支持。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 机械通气在ICU中是救生疗法，但由于患者特异性，优化其设置复杂且易出错。离线RL虽有望用于MV控制，但现有方法难以处理混合动作空间。

Method: 提出基于动作空间减缩的优化方法，并调整离线RL算法（IQL和EDAC）以直接处理混合动作空间。此外，设计了基于临床目标的奖励函数。

Result: 研究表明，AI辅助的MV优化能提高患者安全性，实现个性化肺支持，为智能数据驱动的重症护理提供了重要进展。

Conclusion: 通过优化离线RL方法和改进奖励函数，AI可以更有效地辅助机械通气优化，提升重症护理的效果。

Abstract: Invasive mechanical ventilation (MV) is a life-sustaining therapy for critically ill patients in the intensive care unit (ICU). However, optimizing its settings remains a complex and error-prone process due to patient-specific variability. While Offline Reinforcement Learning (RL) shows promise for MV control, current stateof-the-art (SOTA) methods struggle with the hybrid (continuous and discrete) nature of MV actions. Discretizing the action space limits available actions due to exponential growth in combinations and introduces distribution shifts that can compromise safety. In this paper, we propose optimizations that build upon prior work in action space reduction to address the challenges of discrete action spaces. We also adapt SOTA offline RL algorithms (IQL and EDAC) to operate directly on hybrid action spaces, thereby avoiding the pitfalls of discretization. Additionally, we introduce a clinically grounded reward function based on ventilator-free days and physiological targets, which provides a more meaningful optimization objective compared to traditional sparse mortality-based rewards. Our findings demonstrate that AI-assisted MV optimization may enhance patient safety and enable individualized lung support, representing a significant advancement toward intelligent, data-driven critical care solutions.

</details>


### [119] [ResNets Are Deeper Than You Think](https://arxiv.org/abs/2506.14386)
*Christian H. X. Ali Mehmeti-Göpel,Michael Wand*

Key words: 残差连接, 神经网络, 泛化性能, 优化

TL;DR: 残差连接在现代神经网络中普遍存在，研究发现其性能优势不仅源于优化，还可能与自然数据的结构相符。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究残差连接为何持续优于前馈网络，试图揭示其性能优势的根本原因。

Method: 设计了一个后训练控制实验，分离泛化性能和可训练性，比较可变深度和固定深度架构的表现。

Result: 可变深度架构（如ResNets）在优化无关的情况下仍优于固定深度网络。

Conclusion: 残差连接的性能优势可能源于与自然数据结构的深层对齐。

Abstract: Residual connections remain ubiquitous in modern neural network architectures nearly a decade after their introduction. Their widespread adoption is often credited to their dramatically improved trainability: residual networks train faster, more stably, and achieve higher accuracy than their feedforward counterparts. While numerous techniques, ranging from improved initialization to advanced learning rate schedules, have been proposed to close the performance gap between residual and feedforward networks, this gap has persisted. In this work, we propose an alternative explanation: residual networks do not merely reparameterize feedforward networks, but instead inhabit a different function space. We design a controlled post-training comparison to isolate generalization performance from trainability; we find that variable-depth architectures, similar to ResNets, consistently outperform fixed-depth networks, even when optimization is unlikely to make a difference. These results suggest that residual connections confer performance advantages beyond optimization, pointing instead to a deeper inductive bias aligned with the structure of natural data.

</details>


### [120] [Enclosing Prototypical Variational Autoencoder for Explainable Out-of-Distribution Detection](https://arxiv.org/abs/2506.14390)
*Conrad Orglmeister,Erik Bochinski,Volker Eiselein,Elvira Fleig*

Key words: 深度学习, 变分自动编码器, 离群检测, 可解释性, 原型学习

TL;DR: 该论文提出了一种结合自解释原型变分模型与自动编码器的离群检测方法，提升了深度学习模型的可靠性和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了在安全相关的应用中更好地理解和信任深度学习模型的决策过程，需要一种能够解释模型行为并检测离群样本的方法。

Method: 通过变分自动编码器学习有意义的潜在空间，用于基于距离的分类、似然估计和重构。引入原型和高斯混合分布定义分布内区域，并提出限制损失以保持潜在空间的紧凑性。

Result: 在常见离群检测基准和实际铁路应用数据集上表现优异，优于之前的方法。

Conclusion: 该方法通过可解释的原型和紧凑的潜在空间，有效提升了模型的可信度和离群检测性能。

Abstract: Understanding the decision-making and trusting the reliability of Deep Machine Learning Models is crucial for adopting such methods to safety-relevant applications. We extend self-explainable Prototypical Variational models with autoencoder-based out-of-distribution (OOD) detection: A Variational Autoencoder is applied to learn a meaningful latent space which can be used for distance-based classification, likelihood estimation for OOD detection, and reconstruction. The In-Distribution (ID) region is defined by a Gaussian mixture distribution with learned prototypes representing the center of each mode. Furthermore, a novel restriction loss is introduced that promotes a compact ID region in the latent space without collapsing it into single points. The reconstructive capabilities of the Autoencoder ensure the explainability of the prototypes and the ID region of the classifier, further aiding the discrimination of OOD samples. Extensive evaluations on common OOD detection benchmarks as well as a large-scale dataset from a real-world railway application demonstrate the usefulness of the approach, outperforming previous methods.

</details>


### [121] [HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control](https://arxiv.org/abs/2506.14391)
*Yaqiao Zhu,Hongkai Wen,Geyong Min,Man Luo*

Key words: 交通信号控制、分层强化学习、全局协调、对抗性训练、可扩展性

TL;DR: 本文提出了一种分层强化学习框架HiLight，用于大规模交通信号控制，通过全局对抗性指导和分层策略，解决了现有方法的可扩展性和全局协调性问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的强化学习方法在扩展到大型交通网络时难以保持全局协调，而集中式方法存在可扩展性问题，分散式方法又缺乏统一目标，导致网络效率受限。

Method: HiLight采用分层强化学习框架，包括高层Meta-Policy（划分网络并生成子目标）和低层Sub-Policy（控制单个交叉口）。通过对抗性训练机制，优化全局规划与局部执行的协调。

Result: 实验结果显示，HiLight在大规模场景中表现显著优势，并在不同规模的标准基准测试中保持竞争力。

Conclusion: HiLight通过分层和对抗性训练机制，有效解决了大型交通网络的信号控制问题，提升了全局协调性和可扩展性。

Abstract: Efficient traffic signal control (TSC) is essential for mitigating urban congestion, yet existing reinforcement learning (RL) methods face challenges in scaling to large networks while maintaining global coordination. Centralized RL suffers from scalability issues, while decentralized approaches often lack unified objectives, resulting in limited network-level efficiency. In this paper, we propose HiLight, a hierarchical reinforcement learning framework with global adversarial guidance for large-scale TSC. HiLight consists of a high-level Meta-Policy, which partitions the traffic network into subregions and generates sub-goals using a Transformer-LSTM architecture, and a low-level Sub-Policy, which controls individual intersections with global awareness. To improve the alignment between global planning and local execution, we introduce an adversarial training mechanism, where the Meta-Policy generates challenging yet informative sub-goals, and the Sub-Policy learns to surpass these targets, leading to more effective coordination. We evaluate HiLight across both synthetic and real-world benchmarks, and additionally construct a large-scale Manhattan network with diverse traffic conditions, including peak transitions, adverse weather, and holiday surges. Experimental results show that HiLight exhibits significant advantages in large-scale scenarios and remains competitive across standard benchmarks of varying sizes.

</details>


### [122] [One Size Fits None: Rethinking Fairness in Medical AI](https://arxiv.org/abs/2506.14400)
*Roland Roller,Michael Hahn,Ajay Madhavan Ravichandran,Bilgin Osmanodja,Florian Oetke,Zeineb Sassi,Aljoscha Burchardt,Klaus Netter,Klemens Budde,Anne Herrmann,Tobias Strapatsas,Peter Dabrock,Sebastian Möller*

Key words: 机器学习, 临床决策, 公平性, 亚组分析, 医疗预测

TL;DR: 本文研究了机器学习在医疗决策中的应用，强调了在真实世界中数据集的质量问题如何导致模型在不同患者亚组中的性能差异，并提出了亚组级评估的重要性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着机器学习在临床决策中的应用增加，数据噪声、不完整和不平衡导致模型在不同患者亚组中的性能差异，可能加剧对边缘化群体的不公平。本研究旨在揭示这些差异。

Method: 通过分析多个医疗预测任务，研究了模型性能如何随患者特征变化，并强调了亚组级评估的必要性。

Result: 研究发现，尽管模型在整体上表现良好，但亚组级的性能差异明显，这对临床实践和模型开发的公平性和透明度提出了挑战。

Conclusion: 亚组级性能分析是医疗机器学习模型开发和部署的关键，有助于确保公平性和透明度。

Abstract: Machine learning (ML) models are increasingly used to support clinical decision-making. However, real-world medical datasets are often noisy, incomplete, and imbalanced, leading to performance disparities across patient subgroups. These differences raise fairness concerns, particularly when they reinforce existing disadvantages for marginalized groups. In this work, we analyze several medical prediction tasks and demonstrate how model performance varies with patient characteristics. While ML models may demonstrate good overall performance, we argue that subgroup-level evaluation is essential before integrating them into clinical workflows. By conducting a performance analysis at the subgroup level, differences can be clearly identified-allowing, on the one hand, for performance disparities to be considered in clinical practice, and on the other hand, for these insights to inform the responsible development of more effective models. Thereby, our work contributes to a practical discussion around the subgroup-sensitive development and deployment of medical ML models and the interconnectedness of fairness and transparency.

</details>


### [123] [Adaptive Reinforcement Learning for Unobservable Random Delays](https://arxiv.org/abs/2506.14411)
*John Wikman,Alexandre Proutiere,David Broman*

Key words: 强化学习,延迟适应,交互层,ACDA

TL;DR: 提出了一种名为ACDA的模型算法，通过交互层框架自适应处理延迟问题，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 真实动态环境中，传统RL模型的即时交互假设失效，需要处理不可观测和时变的延迟问题。

Method: 引入交互层框架，生成未来动作矩阵以应对延迟和丢包；基于此开发了ACDA算法。

Result: 在多种运动基准环境中表现显著优于现有方法。

Conclusion: ACDA能有效适应延迟模式，解决传统RL模型的限制。

Abstract: In standard Reinforcement Learning (RL) settings, the interaction between the agent and the environment is typically modeled as a Markov Decision Process (MDP), which assumes that the agent observes the system state instantaneously, selects an action without delay, and executes it immediately. In real-world dynamic environments, such as cyber-physical systems, this assumption often breaks down due to delays in the interaction between the agent and the system. These delays can vary stochastically over time and are typically unobservable, meaning they are unknown when deciding on an action. Existing methods deal with this uncertainty conservatively by assuming a known fixed upper bound on the delay, even if the delay is often much lower. In this work, we introduce the interaction layer, a general framework that enables agents to adaptively and seamlessly handle unobservable and time-varying delays. Specifically, the agent generates a matrix of possible future actions to handle both unpredictable delays and lost action packets sent over networks. Building on this framework, we develop a model-based algorithm, Actor-Critic with Delay Adaptation (ACDA), which dynamically adjusts to delay patterns. Our method significantly outperforms state-of-the-art approaches across a wide range of locomotion benchmark environments.

</details>


### [124] [Unsupervised Skill Discovery through Skill Regions Differentiation](https://arxiv.org/abs/2506.14420)
*Ting Xiao,Jiakun Zheng,Rushuai Yang,Kang Xu,Qiaosheng Zhang,Peng Liu,Chenjia Bai*

Key words: 无监督强化学习、技能发现、状态密度、条件自编码器、内在奖励

TL;DR: 本文提出了一种新的技能发现目标，通过最大化技能间状态密度差异来解决无监督强化学习中的探索问题，并设计了条件自编码器和内在奖励机制。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前无监督强化学习方法在状态探索和技能多样性方面存在局限性，特别是在大规模状态空间（如图像）中表现不佳。

Method: 提出了一种新的技能发现目标，利用条件自编码器进行状态密度估计，并通过内在奖励机制促进技能内探索。

Result: 在多种任务中，该方法学习了有意义的技能，并在下游任务中表现出色。

Conclusion: 该方法有效解决了无监督强化学习中的探索和技能多样性问题，具有广泛的应用潜力。

Abstract: Unsupervised Reinforcement Learning (RL) aims to discover diverse behaviors that can accelerate the learning of downstream tasks. Previous methods typically focus on entropy-based exploration or empowerment-driven skill learning. However, entropy-based exploration struggles in large-scale state spaces (e.g., images), and empowerment-based methods with Mutual Information (MI) estimations have limitations in state exploration. To address these challenges, we propose a novel skill discovery objective that maximizes the deviation of the state density of one skill from the explored regions of other skills, encouraging inter-skill state diversity similar to the initial MI objective. For state-density estimation, we construct a novel conditional autoencoder with soft modularization for different skill policies in high-dimensional space. Meanwhile, to incentivize intra-skill exploration, we formulate an intrinsic reward based on the learned autoencoder that resembles count-based exploration in a compact latent space. Through extensive experiments in challenging state and image-based tasks, we find our method learns meaningful skills and achieves superior performance in various downstream tasks.

</details>


### [125] [MoORE: SVD-based Model MoE-ization for Conflict- and Oblivion-Resistant Multi-Task Adaptation](https://arxiv.org/abs/2506.14436)
*Shen Yuan,Yin Zheng,Taifeng Wang,Binbin Liu,Hongteng Xu*

Key words: 多任务适应, 任务冲突, 遗忘问题, MoORE, SVD, 正交专家

TL;DR: 本文提出了一种名为MoORE的新方法，通过模型MoE化策略解决多任务场景中的任务冲突和遗忘问题。该方法利用SVD分解和可学习路由器调整奇异值，形成正交专家混合体，提升模型能力并保持原始权重矩阵特性。实验证明其优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决多任务场景中任务冲突和遗忘的问题。

Method: 通过SVD分解权重矩阵，引入可学习路由器调整奇异值，形成正交专家混合体（MoORE），并使用可学习正交变换提升模型能力。

Result: MoORE在多任务适应方法中表现优异，有效抵抗任务冲突和遗忘。

Conclusion: MoORE是一种高效的多任务适应方法，能够应对任务冲突和遗忘，性能优于现有技术。

Abstract: Adapting large-scale foundation models in multi-task scenarios often suffers from task conflict and oblivion. To mitigate such issues, we propose a novel ''model MoE-ization'' strategy that leads to a conflict- and oblivion-resistant multi-task adaptation method. Given a weight matrix of a pre-trained model, our method applies SVD to it and introduces a learnable router to adjust its singular values based on tasks and samples. Accordingly, the weight matrix becomes a Mixture of Orthogonal Rank-one Experts (MoORE), in which each expert corresponds to the outer product of a left singular vector and the corresponding right one. We can improve the model capacity by imposing a learnable orthogonal transform on the right singular vectors. Unlike low-rank adaptation (LoRA) and its MoE-driven variants, MoORE guarantees the experts' orthogonality and maintains the column space of the original weight matrix. These two properties make the adapted model resistant to the conflicts among the new tasks and the oblivion of its original tasks, respectively. Experiments on various datasets demonstrate that MoORE outperforms existing multi-task adaptation methods consistently, showing its superiority in terms of conflict- and oblivion-resistance. The code of the experiments is available at https://github.com/DaShenZi721/MoORE.

</details>


### [126] [sHGCN: Simplified hyperbolic graph convolutional neural networks](https://arxiv.org/abs/2506.14438)
*Pol Arévalo,Alexis Molina,Álvaro Ciudad*

Key words: 双曲几何, 神经网络, 计算效率, 预测准确性

TL;DR: 本文提出了一种简化双曲神经网络关键操作的方法，显著提高了计算速度和性能，使其在更广泛的应用中更具可行性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 双曲几何在建模复杂、结构化数据时表现优异，但其神经网络存在计算效率和高精度任务上的局限性。

Method: 通过简化双曲神经网络中的关键操作。

Result: 实现了计算速度和预测准确性的显著提升。

Conclusion: 简化双曲操作可以大幅提升性能，拓展双曲神经网络的应用范围。

Abstract: Hyperbolic geometry has emerged as a powerful tool for modeling complex, structured data, particularly where hierarchical or tree-like relationships are present. By enabling embeddings with lower distortion, hyperbolic neural networks offer promising alternatives to Euclidean-based models for capturing intricate data structures. Despite these advantages, they often face performance challenges, particularly in computational efficiency and tasks requiring high precision. In this work, we address these limitations by simplifying key operations within hyperbolic neural networks, achieving notable improvements in both runtime and performance. Our findings demonstrate that streamlined hyperbolic operations can lead to substantial gains in computational speed and predictive accuracy, making hyperbolic neural networks a more viable choice for a broader range of applications.

</details>


### [127] [A General Framework for Off-Policy Learning with Partially-Observed Reward](https://arxiv.org/abs/2506.14439)
*Rikiya Takehi,Masahiro Asami,Kosuke Kawakami,Yuta Saito*

Key words: 离策略学习、部分观察奖励、次级奖励、HyPeR、上下文赌博机

TL;DR: 论文提出了一种名为HyPeR的新方法，用于处理部分观察到的奖励下的离策略学习问题，通过利用密集观察到的次级奖励来提升目标奖励的优化效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在部分奖励观察的场景下，如内容推荐或电子商务转化信号等，传统的离策略学习效果显著下降。次级奖励虽然密集，但单独使用可能与目标奖励不对齐，因此需要一种新的方法结合两者。

Method: 提出HyPeR方法，综合利用部分观察的目标奖励和密集的次级奖励，进行有效的离策略学习，并在优化目标奖励的同时兼顾次级奖励。

Result: HyPeR在合成和真实数据上的实验表现优于现有方法，且在优化目标奖励时也受益于次级奖励的利用。

Conclusion: HyPeR通过结合部分目标奖励和次级奖励，有效提升了离策略学习的性能，即使在部分奖励观察的挑战性场景中。

Abstract: Off-policy learning (OPL) in contextual bandits aims to learn a decision-making policy that maximizes the target rewards by using only historical interaction data collected under previously developed policies. Unfortunately, when rewards are only partially observed, the effectiveness of OPL degrades severely. Well-known examples of such partial rewards include explicit ratings in content recommendations, conversion signals on e-commerce platforms that are partial due to delay, and the issue of censoring in medical problems. One possible solution to deal with such partial rewards is to use secondary rewards, such as dwelling time, clicks, and medical indicators, which are more densely observed. However, relying solely on such secondary rewards can also lead to poor policy learning since they may not align with the target reward. Thus, this work studies a new and general problem of OPL where the goal is to learn a policy that maximizes the expected target reward by leveraging densely observed secondary rewards as supplemental data. We then propose a new method called Hybrid Policy Optimization for Partially-Observed Reward (HyPeR), which effectively uses the secondary rewards in addition to the partially-observed target reward to achieve effective OPL despite the challenging scenario. We also discuss a case where we aim to optimize not only the expected target reward but also the expected secondary rewards to some extent; counter-intuitively, we will show that leveraging the two objectives is in fact advantageous also for the optimization of only the target reward. Along with statistical analysis of our proposed methods, empirical evaluations on both synthetic and real-world data show that HyPeR outperforms existing methods in various scenarios.

</details>


### [128] [Detecting immune cells with label-free two-photon autofluorescence and deep learning](https://arxiv.org/abs/2506.14449)
*Lucas Kreiss,Amey Chaware,Maryam Roohian,Sarah Lemire,Oana-Maria Thoma,Birgitta Carlé,Maximilian Waldner,Sebastian Schürmann,Oliver Friedrich,Roarke Horstmeyer*

Key words: 无标记成像, 多光子显微镜, 深度学习, 免疫细胞分类, 卷积神经网络

TL;DR: 该研究利用深度学习模型对无标记多光子显微图像中的免疫细胞进行分类，提高了无标记成像的特异性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 无标记成像因其无需复杂染色程序而备受关注，深度学习模型可以增强其特异性。

Method: 使用卷积神经网络（CNN）对无标记多光子显微图像中的免疫细胞进行分类，数据包含5,075个二元分类细胞和3,424个多类分类细胞。

Result: 低复杂度的SqueezeNet架构在二元分类任务中表现可靠（ROC-AUC为0.89），多类分类任务中F1分数为0.689。扰动测试表明模型对细胞外环境不敏感。

Conclusion: 未来，这种预测性深度学习模型可直接检测未染色图像中的特定免疫细胞，提高无标记多光子显微成像的特异性，具有巨大潜力。

Abstract: Label-free imaging has gained broad interest because of its potential to omit elaborate staining procedures which is especially relevant for in vivo use. Label-free multiphoton microscopy (MPM), for instance, exploits two-photon excitation of natural autofluorescence (AF) from native, metabolic proteins, making it ideal for in vivo endomicroscopy. Deep learning (DL) models have been widely used in other optical imaging technologies to predict specific target annotations and thereby digitally augment the specificity of these label-free images. However, this computational specificity has only rarely been implemented for MPM. In this work, we used a data set of label-free MPM images from a series of different immune cell types (5,075 individual cells for binary classification in mixed samples and 3,424 cells for a multi-class classification task) and trained a convolutional neural network (CNN) to classify cell types based on this label-free AF as input. A low-complexity squeezeNet architecture was able to achieve reliable immune cell classification results (0.89 ROC-AUC, 0.95 PR-AUC, for binary classification in mixed samples; 0.689 F1 score, 0.697 precision, 0.748 recall, and 0.683 MCC for six-class classification in isolated samples). Perturbation tests confirmed that the model is not confused by extracellular environment and that both input AF channels (NADH and FAD) are about equally important to the classification. In the future, such predictive DL models could directly detect specific immune cells in unstained images and thus, computationally improve the specificity of label-free MPM which would have great potential for in vivo endomicroscopy.

</details>


### [129] [Dataset distillation for memorized data: Soft labels can leak held-out teacher knowledge](https://arxiv.org/abs/2506.14457)
*Freya Behrens,Lenka Zdeborová*

Key words: 数据集蒸馏, 记忆信息, 软标签, 教师-学生模型

TL;DR: 论文研究了数据集蒸馏中学生通过教师软标签学习未直接观察到的记忆数据的现象。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索记忆信息在数据集蒸馏中是否能被转移，及其背后的机制。

Method: 在有限随机独立同分布数据集上实验，分析教师未泛化时学生如何通过软标签学习记忆数据。

Result: 学生能从软标签中学习并达到非平凡的准确率，某些情况下甚至达到完美准确率。

Conclusion: 软标签的温度对现象有显著影响，但该现象在不同网络结构和数据集上均存在。

Abstract: Dataset distillation aims to compress training data into fewer examples via a teacher, from which a student can learn effectively. While its success is often attributed to structure in the data, modern neural networks also memorize specific facts, but if and how such memorized information is can transferred in distillation settings remains less understood. In this work, we show that students trained on soft labels from teachers can achieve non-trivial accuracy on held-out memorized data they never directly observed. This effect persists on structured data when the teacher has not generalized.To analyze it in isolation, we consider finite random i.i.d. datasets where generalization is a priori impossible and a successful teacher fit implies pure memorization. Still, students can learn non-trivial information about the held-out data, in some cases up to perfect accuracy. In those settings, enough soft labels are available to recover the teacher functionally - the student matches the teacher's predictions on all possible inputs, including the held-out memorized data. We show that these phenomena strongly depend on the temperature with which the logits are smoothed, but persist across varying network capacities, architectures and dataset compositions.

</details>


### [130] [A Model-Mediated Stacked Ensemble Approach for Depression Prediction Among Professionals](https://arxiv.org/abs/2506.14459)
*Md. Mortuza Ahmmed,Abdullah Al Noman,Mahin Montasir Afif,K. M. Tahsin Kabir,Md. Mostafizur Rahman,Mufti Mahmud*

Key words: 抑郁症,集成学习,心理健康预测,职业环境

TL;DR: 该研究提出了一种基于堆叠的集成学习方法，用于提高专业人士抑郁症分类的预测准确性，结果表明该方法在预测性能上表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 抑郁症在职业环境中是一个重要的心理健康问题，传统的分类方法难以应对其复杂性，因此需要更准确的预测模型。

Method: 采用堆叠集成学习方法，结合多个基学习器和逻辑回归模型，从Kaggle收集的数据集包含人口统计、职业和生活方式属性。

Result: 模型在训练数据和测试数据上的准确率分别为99.64%和98.75%，精确率、召回率和F1分数均超过98%。

Conclusion: 集成学习在心理健康分析中具有高效性，为早期检测和干预提供了潜力。

Abstract: Depression is a significant mental health concern, particularly in professional environments where work-related stress, financial pressure, and lifestyle imbalances contribute to deteriorating well-being. Despite increasing awareness, researchers and practitioners face critical challenges in developing accurate and generalizable predictive models for mental health disorders. Traditional classification approaches often struggle with the complexity of depression, as it is influenced by multifaceted, interdependent factors, including occupational stress, sleep patterns, and job satisfaction. This study addresses these challenges by proposing a stacking-based ensemble learning approach to improve the predictive accuracy of depression classification among professionals. The Depression Professional Dataset has been collected from Kaggle. The dataset comprises demographic, occupational, and lifestyle attributes that influence mental well-being. Our stacking model integrates multiple base learners with a logistic regression-mediated model, effectively capturing diverse learning patterns. The experimental results demonstrate that the proposed model achieves high predictive performance, with an accuracy of 99.64% on training data and 98.75% on testing data, with precision, recall, and F1-score all exceeding 98%. These findings highlight the effectiveness of ensemble learning in mental health analytics and underscore its potential for early detection and intervention strategies.

</details>


### [131] [Zeroth-Order Optimization is Secretly Single-Step Policy Optimization](https://arxiv.org/abs/2506.14460)
*Junbin Qiu,Zhengpeng Xie,Xiangda Yan,Yongjie Yang,Yao Shu*

Key words: 零阶优化、策略优化、方差减少、算法设计

TL;DR: 论文揭示了零阶优化（ZOO）与单步策略优化（PO）之间的等价关系，并提出了结合方差减少技术的新算法ZoAR，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探究零阶优化方法中随机有限差分的机制及其与强化学习中策略优化的联系，并提出改进方案。

Method: 通过理论证明ZOO与单步PO的等价性，提出结合平均基线和查询重用的新算法ZoAR。

Result: ZoAR在收敛速度和最终性能上显著优于其他方法，理论分析支持其方差减少效果。

Conclusion: 研究为ZOO提供了新的理论视角，并通过与PO的联系实现了算法改进。

Abstract: Zeroth-Order Optimization (ZOO) provides powerful tools for optimizing functions where explicit gradients are unavailable or expensive to compute. However, the underlying mechanisms of popular ZOO methods, particularly those employing randomized finite differences, and their connection to other optimization paradigms like Reinforcement Learning (RL) are not fully elucidated. This paper establishes a fundamental and previously unrecognized connection: ZOO with finite differences is equivalent to a specific instance of single-step Policy Optimization (PO). We formally unveil that the implicitly smoothed objective function optimized by common ZOO algorithms is identical to a single-step PO objective. Furthermore, we show that widely used ZOO gradient estimators, are mathematically equivalent to the REINFORCE gradient estimator with a specific baseline function, revealing the variance-reducing mechanism in ZOO from a PO perspective.Built on this unified framework, we propose ZoAR (Zeroth-Order Optimization with Averaged Baseline and Query Reuse), a novel ZOO algorithm incorporating PO-inspired variance reduction techniques: an averaged baseline from recent evaluations and query reuse analogous to experience replay. Our theoretical analysis further substantiates these techniques reduce variance and enhance convergence. Extensive empirical studies validate our theory and demonstrate that ZoAR significantly outperforms other methods in terms of convergence speed and final performance. Overall, our work provides a new theoretical lens for understanding ZOO and offers practical algorithmic improvements derived from its connection to PO.

</details>


### [132] [Leveraging External Factors in Household-Level Electrical Consumption Forecasting using Hypernetworks](https://arxiv.org/abs/2506.14472)
*Fabien Bernier,Maxime Cordy,Yves Le Traon*

Key words: 电力消耗预测, 超网络架构, 外部因素, 预测准确性, 全局模型

TL;DR: 该论文研究了如何通过超网络架构整合外部因素（如天气指标）来提升全球电力消耗预测模型的准确性，同时解决了全局模型因额外特征引入而性能下降的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 电力消耗的准确预测对能源管理和资源分配至关重要，但传统方法依赖历史数据和时序依赖性，而添加外部因素可能提高个体预测准确性但降低全局模型性能。

Method: 采用超网络架构，通过根据每个消费者调整模型权重，有效利用外部因素提升全局预测模型。收集了包括6000多户家庭的电力消耗数据及外部因素（天气、节假日等）的两年数据集。

Result: 实验表明，与其他模型相比，超网络方法在结合外部因素时表现最优，减少了预测误差并保持全局模型的优势。

Conclusion: 超网络架构能够有效提升电力消耗预测模型的准确性，尤其是在整合外部因素时，为复杂应用的全局预测提供了可行方案。

Abstract: Accurate electrical consumption forecasting is crucial for efficient energy management and resource allocation. While traditional time series forecasting relies on historical patterns and temporal dependencies, incorporating external factors -- such as weather indicators -- has shown significant potential for improving prediction accuracy in complex real-world applications. However, the inclusion of these additional features often degrades the performance of global predictive models trained on entire populations, despite improving individual household-level models. To address this challenge, we found that a hypernetwork architecture can effectively leverage external factors to enhance the accuracy of global electrical consumption forecasting models, by specifically adjusting the model weights to each consumer.
  We collected a comprehensive dataset spanning two years, comprising consumption data from over 6000 luxembourgish households and corresponding external factors such as weather indicators, holidays, and major local events. By comparing various forecasting models, we demonstrate that a hypernetwork approach outperforms existing methods when associated to external factors, reducing forecasting errors and achieving the best accuracy while maintaining the benefits of a global model.

</details>


### [133] [Train Once, Forget Precisely: Anchored Optimization for Efficient Post-Hoc Unlearning](https://arxiv.org/abs/2506.14515)
*Prabhav Sanga,Jaskaran Singh,Arun K. Dubey*

Key words: 选择性遗忘, 深度图像分类, FAMR, 约束优化, 隐私保护

TL;DR: 论文介绍了一种名为FAMR的框架，用于在深度图像分类器中高效实现选择性遗忘，通过理论分析和实验验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着机器学习系统越来越多地依赖受隐私保护的数据，如何从训练好的模型中高效、选择性地遗忘特定信息变得至关重要。

Method: 提出Forget-Aligned Model Reconstruction (FAMR)框架，将遗忘问题建模为一个约束优化问题，最小化遗忘集的均匀预测损失，并通过ℓ2惩罚锚定模型参数。

Result: 在CIFAR-10和ImageNet-100上的实验表明，FAMR在保持性能的同时，显著降低了计算开销。

Conclusion: FAMR提供了一种可扩展且可验证的高效遗忘方法，适用于视觉模型中的类别、概念和风格消除。

Abstract: As machine learning systems increasingly rely on data subject to privacy regulation, selectively unlearning specific information from trained models has become essential. In image classification, this involves removing the influence of particular training samples, semantic classes, or visual styles without full retraining. We introduce \textbf{Forget-Aligned Model Reconstruction (FAMR)}, a theoretically grounded and computationally efficient framework for post-hoc unlearning in deep image classifiers. FAMR frames forgetting as a constrained optimization problem that minimizes a uniform-prediction loss on the forget set while anchoring model parameters to their original values via an $\ell_2$ penalty. A theoretical analysis links FAMR's solution to influence-function-based retraining approximations, with bounds on parameter and output deviation. Empirical results on class forgetting tasks using CIFAR-10 and ImageNet-100 demonstrate FAMR's effectiveness, with strong performance retention and minimal computational overhead. The framework generalizes naturally to concept and style erasure, offering a scalable and certifiable route to efficient post-hoc forgetting in vision models.

</details>


### [134] [Two-Player Zero-Sum Games with Bandit Feedback](https://arxiv.org/abs/2506.14518)
*Elif Yılmaz,Christos Dimitrakakis*

Key words: 零和游戏、ETC算法、纳什均衡、遗憾分析、动作对消除

TL;DR: 论文研究了在零和双人游戏中，行玩家通过老虎机反馈学习未知收益矩阵，提出了两种改进ETC算法，实现了与现有方法相当的遗憾上界。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨ETC算法在零和双人游戏中的应用，特别是学习纯策略纳什均衡的有效性。

Method: 提出ETC-TPZSG及其改进版ETC-TPZSG-AE，后者通过动作对消除策略提高效率。

Result: 两种算法的实例依赖遗憾上界分别为O(Δ+√T)和O(log(TΔ²)/Δ)，表现优异。

Conclusion: ETC算法在对抗性游戏中表现良好，遗憾分析与实例依赖结果为该领域提供了新见解。

Abstract: We study a two-player zero-sum game (TPZSG) in which the row player aims to maximize their payoff against an adversarial column player, under an unknown payoff matrix estimated through bandit feedback. We propose and analyze two algorithms: ETC-TPZSG, which directly applies ETC to the TPZSG setting and ETC-TPZSG-AE, which improves upon it by incorporating an action pair elimination (AE) strategy that leverages the $\varepsilon$-Nash Equilibrium property to efficiently select the optimal action pair. Our objective is to demonstrate the applicability of ETC in a TPZSG setting by focusing on learning pure strategy Nash Equilibrium. A key contribution of our work is a derivation of instance-dependent upper bounds on the expected regret for both algorithms, has received limited attention in the literature on zero-sum games. Particularly, after $T$ rounds, we achieve an instance-dependent regret upper bounds of $O(Δ+ \sqrt{T})$ for ETC-TPZSG and $O(\frac{\log (T Δ^2)}Δ)$ for ETC-TPZSG-AE, where $Δ$ denotes the suboptimality gap. Therefore, our results indicate that ETC-based algorithms perform effectively in adversarial game settings, achieving regret bounds comparable to existing methods while providing insights through instance-dependent analysis.

</details>


### [135] [Towards Improved Research Methodologies for Industrial AI: A case study of false call reduction](https://arxiv.org/abs/2506.14521)
*Korbinian Pfab,Marcel Rothering*

Key words: AI研究方法, 工业AI, 误报减少, 需求感知指标

TL;DR: 论文指出当前AI研究方法可能无法创造成功、高效且盈利的AI应用，并通过工业AI案例展示了现有最佳实践的缺陷。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨当前AI研究方法是否足以支持实际应用的开发，特别是针对工业领域的特定需求。

Method: 通过一个工业AI案例（自动光学检测中的误报减少），分析并实验验证了当前最佳实践的七种缺陷。

Result: 研究表明，现有最佳实践方法在此案例中会失败，并提出了改进方向，如需求感知指标、明确成功标准等。

Conclusion: 呼吁研究者批判性评估其方法论，以提高AI研究在实际应用中的成功率。

Abstract: Are current artificial intelligence (AI) research methodologies ready to create successful, productive, and profitable AI applications? This work presents a case study on an industrial AI use case called false call reduction for automated optical inspection to demonstrate the shortcomings of current best practices. We identify seven weaknesses prevalent in related peer-reviewed work and experimentally show their consequences. We show that the best-practice methodology would fail for this use case. We argue amongst others for the necessity of requirement-aware metrics to ensure achieving business objectives, clear definitions of success criteria, and a thorough analysis of temporal dynamics in experimental datasets. Our work encourages researchers to critically assess their methodologies for more successful applied AI research.

</details>


### [136] [Automated Decision-Making on Networks with LLMs through Knowledge-Guided Evolution](https://arxiv.org/abs/2506.14529)
*Xiaohan Zheng,Lanning Wei,Yong Li,Quanming Yao*

Key words: 图神经网络, 大型语言模型, 自动化配置, 知识库, 检索增强生成

TL;DR: LLMNet利用大型语言模型自动设计和优化图神经网络（GNN），通过知识库和检索增强生成技术实现高效配置，在多个任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统GNN需要大量人工配置和调优，LLMNet旨在通过自动化提升效率和性能。

Method: 系统开发多代理架构，构建图相关知识库并利用RAG技术指导GNN模型的自动配置和优化。

Result: 在12个数据集和3个图学习任务中，LLMNet表现优异，验证了自动化设计的有效性。

Conclusion: LLMNet通过知识驱动的自动化流程，显著简化了GNN设计，提升了模型性能。

Abstract: Effective decision-making on networks often relies on learning from graph-structured data, where Graph Neural Networks (GNNs) play a central role, but they take efforts to configure and tune. In this demo, we propose LLMNet, showing how to design GNN automated through Large Language Models. Our system develops a set of agents that construct graph-related knowlege bases and then leverages Retrieval-Augmented Generation (RAG) to support automated configuration and refinement of GNN models through a knowledge-guided evolution process. These agents, equipped with specialized knowledge bases, extract insights into tasks and graph structures by interacting with the knowledge bases. Empirical results show LLMNet excels in twelve datasets across three graph learning tasks, validating its effectiveness of GNN model designing.

</details>


### [137] [Aligning Evaluation with Clinical Priorities: Calibration, Label Shift, and Error Costs](https://arxiv.org/abs/2506.14540)
*Gerardo A. Flores,Alyssa H. Smith,Julia A. Fukuyama,Ashia C. Wilson*

Key words: 机器学习, 临床决策支持, 评分规则, 校准, 不对称成本

TL;DR: 该论文提出了一种用于临床决策支持系统的评估框架，旨在解决现有评分规则（如准确率和AUC-ROC）未能充分反映临床优先级的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有评分规则在临床环境中未能充分考虑校准、分布偏移鲁棒性和不对称错误成本等关键因素。

Method: 基于Schervish理论，提出了一种调整后的交叉熵（对数评分）方法，结合类平衡范围和成本不对称性进行评估。

Result: 新方法简单易用，对临床部署条件敏感，并优先选择校准且对现实变化鲁棒的模型。

Conclusion: 该框架为临床决策支持系统提供了一种更符合实际需求的评估工具。

Abstract: Machine learning-based decision support systems are increasingly deployed in clinical settings, where probabilistic scoring functions are used to inform and prioritize patient management decisions. However, widely used scoring rules, such as accuracy and AUC-ROC, fail to adequately reflect key clinical priorities, including calibration, robustness to distributional shifts, and sensitivity to asymmetric error costs. In this work, we propose a principled yet practical evaluation framework for selecting calibrated thresholded classifiers that explicitly accounts for the uncertainty in class prevalences and domain-specific cost asymmetries often found in clinical settings. Building on the theory of proper scoring rules, particularly the Schervish representation, we derive an adjusted variant of cross-entropy (log score) that averages cost-weighted performance over clinically relevant ranges of class balance. The resulting evaluation is simple to apply, sensitive to clinical deployment conditions, and designed to prioritize models that are both calibrated and robust to real-world variations.

</details>


### [138] [TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization](https://arxiv.org/abs/2506.14574)
*Mingkang Zhu,Xi Chen,Zhongdao Wang,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Key words: 强化学习, 令牌级奖励, DPO, PPO

TL;DR: 论文提出了一种方法，将序列级的PPO分解为令牌级问题，解决了DPO难以利用令牌级奖励的挑战，并通过实验验证了性能的显著提升。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 利用令牌级奖励模型可以提升PPO的性能，但如何将其应用于DPO（一种序列级问题）是挑战。

Method: 将序列级PPO分解为令牌级优化问题，推导出令牌级最优策略和奖励，并基于此设计DPO的损失函数和奖励指导。

Result: 实验表明，该方法在多个基准测试中显著优于DPO，最高提升7.5个百分点。

Conclusion: 该方法成功地将令牌级奖励应用于DPO，显著提升了模型性能。

Abstract: Recent advancements in reinforcement learning from human feedback have shown that utilizing fine-grained token-level reward models can substantially enhance the performance of Proximal Policy Optimization (PPO) in aligning large language models. However, it is challenging to leverage such token-level reward as guidance for Direct Preference Optimization (DPO), since DPO is formulated as a sequence-level bandit problem. To address this challenge, this work decomposes the sequence-level PPO into a sequence of token-level proximal policy optimization problems and then frames the problem of token-level PPO with token-level reward guidance, from which closed-form optimal token-level policy and the corresponding token-level reward can be derived. Using the obtained reward and Bradley-Terry model, this work establishes a framework of computable loss functions with token-level reward guidance for DPO, and proposes a practical reward guidance based on the induced DPO reward. This formulation enables different tokens to exhibit varying degrees of deviation from reference policy based on their respective rewards. Experiment results demonstrate that our method achieves substantial performance improvements over DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at https://github.com/dvlab-research/TGDPO.

</details>


### [139] [Single-Example Learning in a Mixture of GPDMs with Latent Geometries](https://arxiv.org/abs/2506.14563)
*Jesse St. Amand,Leonardo Gizzi,Martin A. Giese*

Key words: GPDMM, 高斯过程, 动态混合模型, 人体运动建模, 单样本学习

TL;DR: GPDMM是一种基于高斯过程的动态混合模型，可用于单样本学习人体运动数据，结合了多个GPDM模型并以几何特征编码多样序列。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决在数据有限且模型可解释性重要的场景（如医疗假肢控制）中，人体运动建模的挑战。

Method: 结合GPDM和隐马尔可夫模型动态先验的混合专家框架，利用几何特征编码序列。

Result: 在分类准确性和生成能力上表现优异，并与其他模型（如LSTM、VAE和Transformer）进行了对比。

Conclusion: GPDMM在数据有限且需高可解释性的应用中具有优势。

Abstract: We present the Gaussian process dynamical mixture model (GPDMM) and show its utility in single-example learning of human motion data. The Gaussian process dynamical model (GPDM) is a form of the Gaussian process latent variable model (GPLVM), but optimized with a hidden Markov model dynamical prior. The GPDMM combines multiple GPDMs in a probabilistic mixture-of-experts framework, utilizing embedded geometric features to allow for diverse sequences to be encoded in a single latent space, enabling the categorization and generation of each sequence class. GPDMs and our mixture model are particularly advantageous in addressing the challenges of modeling human movement in scenarios where data is limited and model interpretability is vital, such as in patient-specific medical applications like prosthesis control. We score the GPDMM on classification accuracy and generative ability in single-example learning, showcase model variations, and benchmark it against LSTMs, VAEs, and transformers.

</details>


### [140] [Object-Centric Neuro-Argumentative Learning](https://arxiv.org/abs/2506.14577)
*Abdul Rahman Jacob,Avinash Kori,Emanuele De Angelis,Ben Glocker,Maurizio Proietti,Francesca Toni*

Key words: 神经论证学习,假设基础论证,深度学习,可解释性,图像分析

TL;DR: 提出了一种结合神经和符号组件的新型神经论证学习（NAL）架构，用于图像分析，实验表明其性能与最先进方法相当。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着深度学习在关键决策中的应用增加，对其安全性、可靠性和可解释性的担忧日益凸显，因此提出结合论证学习的方法。

Method: 结合假设基础论证（ABA）与深度学习，前者通过对象中心学习将图像编码为事实，后者应用ABA学习生成预测框架。

Result: 在合成数据上的实验表明，NAL架构与现有最先进方法性能相当。

Conclusion: NAL架构提供了一种结合神经与符号学习的新方法，有望提升深度学习的可解释性和可靠性。

Abstract: Over the last decade, as we rely more on deep learning technologies to make critical decisions, concerns regarding their safety, reliability and interpretability have emerged. We introduce a novel Neural Argumentative Learning (NAL) architecture that integrates Assumption-Based Argumentation (ABA) with deep learning for image analysis. Our architecture consists of neural and symbolic components. The former segments and encodes images into facts using object-centric learning, while the latter applies ABA learning to develop ABA frameworks enabling predictions with images. Experiments on synthetic data show that the NAL architecture can be competitive with a state-of-the-art alternative.

</details>


### [141] [SCISSOR: Mitigating Semantic Bias through Cluster-Aware Siamese Networks for Robust Classification](https://arxiv.org/abs/2506.14587)
*Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Key words: SCISSOR, 捷径学习, 语义去偏, Siamese网络, 模型泛化

TL;DR: SCISSOR是一种基于Siamese网络的去偏方法，通过抑制语义捷径提升模型泛化能力，无需数据增强或重写，在多个基准测试中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的捷径学习问题导致模型在分布外数据上泛化能力差，研究发现语义嵌入分布的不平衡会引发虚假语义关联，亟需解决。

Method: 提出的SCISSOR方法通过Siamese网络重构语义空间，抑制被利用为捷径的潜在语义簇，避免传统数据去偏方法的繁琐步骤。

Result: SCISSOR在4个基准测试中显著提升性能：GYAFC（F1+5.3）、Yelp（F1+7.3）、Chest-XRay（F1+7.7）和Not-MNIST（F1+1），轻量模型提升尤为明显。

Conclusion: SCISSOR通过解决语义偏差重新定义模型泛化问题，为构建抗偏见的鲁棒AI系统提供了基础框架。

Abstract: Shortcut learning undermines model generalization to out-of-distribution data. While the literature attributes shortcuts to biases in superficial features, we show that imbalances in the semantic distribution of sample embeddings induce spurious semantic correlations, compromising model robustness. To address this issue, we propose SCISSOR (Semantic Cluster Intervention for Suppressing ShORtcut), a Siamese network-based debiasing approach that remaps the semantic space by discouraging latent clusters exploited as shortcuts. Unlike prior data-debiasing approaches, SCISSOR eliminates the need for data augmentation and rewriting. We evaluate SCISSOR on 6 models across 4 benchmarks: Chest-XRay and Not-MNIST in computer vision, and GYAFC and Yelp in NLP tasks. Compared to several baselines, SCISSOR reports +5.3 absolute points in F1 score on GYAFC, +7.3 on Yelp, +7.7 on Chest-XRay, and +1 on Not-MNIST. SCISSOR is also highly advantageous for lightweight models with ~9.5% improvement on F1 for ViT on computer vision datasets and ~11.9% for BERT on NLP. Our study redefines the landscape of model generalization by addressing overlooked semantic biases, establishing SCISSOR as a foundational framework for mitigating shortcut learning and fostering more robust, bias-resistant AI systems.

</details>


### [142] [Deep Learning Surrogates for Real-Time Gas Emission Inversion](https://arxiv.org/abs/2506.14597)
*Thomas Newman,Christopher Nemeth,Matthew Jones,Philip Jonathan*

Key words: 温室气体排放、深度学习、贝叶斯推理、CFD代理模型、实时监测

TL;DR: 提出了一种结合深度学习和贝叶斯推理的实时温室气体排放识别与量化方法，替代传统CFD计算，实现高效且准确的排放源定位与量化。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 实时识别和量化动态大气条件下的温室气体排放是环境监测中的关键挑战，传统CFD方法计算成本高且耗时。

Method: 通过将深度学习代理模型嵌入序列蒙特卡洛算法，替代CFD数值解算器，实现快速的贝叶斯推理。

Result: 验证表明，该方法与CFD和高斯羽流模型精度相当，但运行速度显著提升；在复杂环境中也表现稳健。

Conclusion: 该方法兼顾物理真实性与计算高效性，为工业排放监测等时间敏感任务提供了可扩展解决方案。

Abstract: Real-time identification and quantification of greenhouse-gas emissions under transient atmospheric conditions is a critical challenge in environmental monitoring. We introduce a spatio-temporal inversion framework that embeds a deep-learning surrogate of computational fluid dynamics (CFD) within a sequential Monte Carlo algorithm to perform Bayesian inference of both emission rate and source location in dynamic flow fields. By substituting costly numerical solvers with a multilayer perceptron trained on high-fidelity CFD outputs, our surrogate captures spatial heterogeneity and temporal evolution of gas dispersion, while delivering near-real-time predictions. Validation on the Chilbolton methane release dataset demonstrates comparable accuracy to full CFD solvers and Gaussian plume models, yet achieves orders-of-magnitude faster runtimes. Further experiments under simulated obstructed-flow scenarios confirm robustness in complex environments. This work reconciles physical fidelity with computational feasibility, offering a scalable solution for industrial emissions monitoring and other time-sensitive spatio-temporal inversion tasks in environmental and scientific modeling.

</details>


### [143] [Expressive Score-Based Priors for Distribution Matching with Geometry-Preserving Regularization](https://arxiv.org/abs/2506.14607)
*Ziyu Gong,Jim Lim,David I. Inouye*

Key words: 分布匹配, 分数函数, 似然基, 去噪分数匹配

TL;DR: 论文提出了一种基于分数函数的分布匹配方法，解决了传统方法的偏差和计算效率问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统分布匹配方法存在偏差、不稳定性和计算效率问题，作者希望通过分数函数改进这些缺陷。

Method: 使用基于分数的先验分布训练似然基分布匹配，无需先验密度模型，而是通过去噪分数匹配训练。

Result: 实验表明，该方法在多个任务中表现更优，具有更好的稳定性和计算效率。

Conclusion: 该方法是一种稳定且有效的分布匹配方法，优于传统扩散基先验方法。

Abstract: Distribution matching (DM) is a versatile domain-invariant representation learning technique that has been applied to tasks such as fair classification, domain adaptation, and domain translation. Non-parametric DM methods struggle with scalability and adversarial DM approaches suffer from instability and mode collapse. While likelihood-based methods are a promising alternative, they often impose unnecessary biases through fixed priors or require explicit density models (e.g., flows) that can be challenging to train. We address this limitation by introducing a novel approach to training likelihood-based DM using expressive score-based prior distributions. Our key insight is that gradient-based DM training only requires the prior's score function -- not its density -- allowing us to train the prior via denoising score matching. This approach eliminates biases from fixed priors (e.g., in VAEs), enabling more effective use of geometry-preserving regularization, while avoiding the challenge of learning an explicit prior density model (e.g., a flow-based prior). Our method also demonstrates better stability and computational efficiency compared to other diffusion-based priors (e.g., LSGM). Furthermore, experiments demonstrate superior performance across multiple tasks, establishing our score-based method as a stable and effective approach to distribution matching. Source code available at https://github.com/inouye-lab/SAUB.

</details>


### [144] [Feasibility-Driven Trust Region Bayesian Optimization](https://arxiv.org/abs/2506.14619)
*Paolo Ascia,Elena Raponi,Thomas Bäck,Fabian Duddeck*

Key words: Bayesian optimization, constrained optimization, trust region, feasibility-driven, high-dimensional

TL;DR: 提出一种名为FuRBO的算法，通过动态调整信任区域加速高维约束优化问题的可行解发现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决高维约束优化问题中可行区域难以定位的问题，避免优化资源浪费在寻找初始可行解上。

Method: 基于目标和约束的代理模型，动态调整信任区域以选择候选解，快速聚焦搜索方向。

Result: 在BBOB-constrained COCO基准和其他物理启发的基准测试中表现优异，优于现有方法。

Conclusion: FuRBO能有效加速高维约束优化问题的可行解发现，适用于各类约束严重程度和维度范围（2到60）。

Abstract: Bayesian optimization is a powerful tool for solving real-world optimization tasks under tight evaluation budgets, making it well-suited for applications involving costly simulations or experiments. However, many of these tasks are also characterized by the presence of expensive constraints whose analytical formulation is unknown and often defined in high-dimensional spaces where feasible regions are small, irregular, and difficult to identify. In such cases, a substantial portion of the optimization budget may be spent just trying to locate the first feasible solution, limiting the effectiveness of existing methods. In this work, we present a Feasibility-Driven Trust Region Bayesian Optimization (FuRBO) algorithm. FuRBO iteratively defines a trust region from which the next candidate solution is selected, using information from both the objective and constraint surrogate models. Our adaptive strategy allows the trust region to shift and resize significantly between iterations, enabling the optimizer to rapidly refocus its search and consistently accelerate the discovery of feasible and good-quality solutions. We empirically demonstrate the effectiveness of FuRBO through extensive testing on the full BBOB-constrained COCO benchmark suite and other physics-inspired benchmarks, comparing it against state-of-the-art baselines for constrained black-box optimization across varying levels of constraint severity and problem dimensionalities ranging from 2 to 60.

</details>


### [145] [Towards Desiderata-Driven Design of Visual Counterfactual Explainers](https://arxiv.org/abs/2506.14698)
*Sidney Bender,Jan Herrmann,Klaus-Robert Müller,Grégoire Montavon*

Key words: 视觉反事实解释器,反事实生成,解释性机器学习,图像分类,SCE算法

TL;DR: 本文提出一种新的视觉反事实解释方法（SCE），旨在弥补现有方法在解释全面性上的不足，通过系统评估验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有视觉反事实解释器（VCEs）过于关注样本质量或最小变化，忽略了解释的保真度、可理解性和充分性等更全面的需求。

Method: 提出一种新的反事实生成机制，并将其整合为‘平滑反事实探索器’（SCE）算法。

Result: 在合成数据和真实数据上通过系统评估证明了SCE算法的有效性。

Conclusion: SCE算法能够更好地满足解释的全面需求，提升图像分类器的透明性。

Abstract: Visual counterfactual explainers (VCEs) are a straightforward and promising approach to enhancing the transparency of image classifiers. VCEs complement other types of explanations, such as feature attribution, by revealing the specific data transformations to which a machine learning model responds most strongly. In this paper, we argue that existing VCEs focus too narrowly on optimizing sample quality or change minimality; they fail to consider the more holistic desiderata for an explanation, such as fidelity, understandability, and sufficiency. To address this shortcoming, we explore new mechanisms for counterfactual generation and investigate how they can help fulfill these desiderata. We combine these mechanisms into a novel 'smooth counterfactual explorer' (SCE) algorithm and demonstrate its effectiveness through systematic evaluations on synthetic and real data.

</details>


### [146] [On the Hardness of Bandit Learning](https://arxiv.org/abs/2506.14746)
*Nataly Brukhim,Aldo Pacchiano,Miroslav Dudik,Robert Schapire*

Key words: bandit learning, best-arm identification, PAC framework, computational hardness, VC dimension

TL;DR: 该论文研究了在奖励函数属于已知但任意函数类F的情况下，如何判断哪些函数类F是可学习的（'which'问题），以及如何进行学习（'how'问题）。研究揭示了结构化解的局限性，证明了不存在能够完全描述解可学习性的组合维度，并构造了一个计算上难以优化的奖励函数类。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在建立类似于分类中的PAC框架的解学习理论，明确哪些函数类可以被学习以及如何学习。

Method: 通过证明不存在组合维度能够完全描述解可学习性，并通过构造计算上难以优化的奖励函数类来研究学习算法。

Result: 证明了结构化解学习存在局限性，且某些函数类在计算上难以优化，除非RP=NP。

Conclusion: 研究的局限性表明，解学习的可学习性与分类不同，且计算复杂性是其固有挑战。

Abstract: We study the task of bandit learning, also known as best-arm identification, under the assumption that the true reward function f belongs to a known, but arbitrary, function class F. We seek a general theory of bandit learnability, akin to the PAC framework for classification. Our investigation is guided by the following two questions: (1) which classes F are learnable, and (2) how they are learnable. For example, in the case of binary PAC classification, learnability is fully determined by a combinatorial dimension - the VC dimension- and can be attained via a simple algorithmic principle, namely, empirical risk minimization (ERM). In contrast to classical learning-theoretic results, our findings reveal limitations of learning in structured bandits, offering insights into the boundaries of bandit learnability. First, for the question of "which", we show that the paradigm of identifying the learnable classes via a dimension-like quantity fails for bandit learning. We give a simple proof demonstrating that no combinatorial dimension can characterize bandit learnability, even in finite classes, following a standard definition of dimension introduced by Ben-David et al. (2019). For the question of "how", we prove a computational hardness result: we construct a reward function class for which at most two queries are needed to find the optimal action, yet no algorithm can do so in polynomial time unless RP=NP. We also prove that this class admits efficient algorithms for standard algorithmic operations often considered in learning theory, such as an ERM. This implies that computational hardness is in this case inherent to the task of bandit learning. Beyond these results, we investigate additional themes such as learning under noise, trade-offs between noise models, and the relationship between query complexity and regret minimization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [147] ['Memory States' from Almost Nothing: Representing and Computing in a Non-associative Algebra](https://arxiv.org/abs/2506.13768)
*Stefan Reimann*

Key words: 非结合代数、高维表示、顺序信息、记忆模型、序列位置曲线

TL;DR: 提出了一个非结合代数框架，用于高维空间中的信息表示与计算，强调顺序信息的保留，并通过L状态和R状态分别模拟记忆中的近因效应和首因效应。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 为解决传统结合捆绑方法中顺序信息的丢失问题，同时与认知科学中的经验发现保持一致。

Method: 使用非结合捆绑和乘法绑定操作，构建稀疏表示，分别生成L状态（强调近因效应）和R状态（编码首因效应）。

Result: 模型能够模拟序列位置曲线，复现认知实验中的近因和首因效应。

Conclusion: 非结合框架为高维信息表示提供了新的可能性，并与大脑前额叶和海马体的记忆机制相关联。

Abstract: This note presents a non-associative algebraic framework for the representation and computation of information items in high-dimensional space. This framework is consistent with the principles of spatial computing and with the empirical findings in cognitive science about memory. Computations are performed through a process of multiplication-like binding and non-associative interference-like bundling. Models that rely on associative bundling typically lose order information, which necessitates the use of auxiliary order structures, such as position markers, to represent sequential information that is important for cognitive tasks. In contrast, the non-associative bundling proposed allows the construction of sparse representations of arbitrarily long sequences that maintain their temporal structure across arbitrary lengths. In this operation, noise is a constituent element of the representation of order information, rather than a means of obscuring it. The non-associative nature of the proposed framework results in the representation of a single sequence by two distinct states. The L-state, generated through left-associative bundling, continuously updates and emphasises a recency effect, while the R-state, formed through right-associative bundling, encodes finite sequences or chunks, capturing a primacy effect. The construction of these states may be associated with activity in the prefrontal cortex in relation to short-term memory and hippocampal encoding in long-term memory, respectively. The accuracy of retrieval is contingent upon a decision-making process that is based on the mutual information between the memory states and the cue. The model is able to replicate the Serial Position Curve, which reflects the empirical recency and primacy effects observed in cognitive experiments.

</details>


### [148] [Representing Time-Continuous Behavior of Cyber-Physical Systems in Knowledge Graphs](https://arxiv.org/abs/2506.13773)
*Milapji Singh Gill,Tom Jeleniewski,Felix Gehlhoff,Alexander Fay*

Key words: 知识图谱, CPS, 微分方程, 语义模型, 航空维护

TL;DR: 论文提出了一个基于标准的模块化语义模型和一种高效知识图谱生成方法，用于在知识图谱中表示微分方程并丰富其语义，验证了其在实际应用中的有效性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 为了在不同生命周期阶段有效使用CPS应用中的时间连续动态模型，需要将微分方程等信息与其他CPS信息集成，但当前缺乏可重用的本体工具和方法。

Method: 引入了基于标准的模块化语义模型和高效知识图谱生成方法，以在知识图谱中直接表示微分方程并丰富其语义。

Result: 在航空维护领域验证了该方法，成功将复杂电液伺服执行器的微分方程与其他生命周期数据集成，证明了其实际适用性。

Conclusion: 提出的方法能够有效减少手动实例化的工作量并提升信息集成能力，适用于CPS应用中的多生命周期阶段。

Abstract: Time-continuous dynamic models are essential for various Cyber-Physical System (CPS) applications. To ensure effective usability in different lifecycle phases, such behavioral information in the form of differential equations must be contextualized and integrated with further CPS information. While knowledge graphs provide a formal description and structuring mechanism for this task, there is a lack of reusable ontological artifacts and methods to reduce manual instantiation effort. Hence, this contribution introduces two artifacts: Firstly, a modular semantic model based on standards is introduced to represent differential equations directly within knowledge graphs and to enrich them semantically. Secondly, a method for efficient knowledge graph generation is presented. A validation of these artifacts was conducted in the domain of aviation maintenance. Results show that differential equations of a complex Electro-Hydraulic Servoactuator can be formally represented in a knowledge graph and be contextualized with other lifecycle data, proving the artifacts' practical applicability.

</details>


### [149] [Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values](https://arxiv.org/abs/2506.13774)
*Nell Watson,Ahmed Amer,Evan Harris,Preeti Ravindra,Shujun Zhang*

Key words: Agentic AI, Superego, Creed Constitutions, AI alignment, harm reduction

TL;DR: 论文提出了一种名为‘superego’的个性化监督机制，用于解决AI系统的行为对齐问题。该系统通过动态调整用户选择的规则集（称为‘Creed Constitutions’）和实时合规检查，显著减少有害输出。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: AI系统在自主规划和行动中的应用受到行为对齐问题的阻碍，现有方法难以满足个性化需求并避免操作低效或虚构信息。

Method: 设计了‘superego’代理，利用用户自定义的规则集（Creed Constitutions）和实时合规检查机制，动态调整AI的行为规划。

Result: 实验显示，该系统在多个基准测试（如HarmBench、AgentHarm）中显著减少有害输出，某些情况下有害评分降低98.3%，拒绝率接近100%。

Conclusion: 该方法简化了AI行为对齐的个性化过程，使系统更可靠地适应个体和文化背景，同时大幅提升安全性。

Abstract: Agentic AI systems, possessing capabilities for autonomous planning and action, exhibit immense potential across diverse domains. However, their practical deployment is significantly hampered by challenges in aligning their behavior with varied human values, complex safety requirements, and specific compliance needs. Existing alignment methodologies often falter when faced with the intricate task of providing deep, personalized contextual information without inducing confabulation or operational inefficiencies. This paper introduces a novel solution: a 'superego' agent, designed as a personalized oversight mechanism for agentic AI. This system dynamically steers AI planning by referencing user-selected "Creed Constitutions"-encapsulating diverse rule sets-with adjustable adherence levels to fit non-negotiable values. A real-time compliance enforcer validates plans against these constitutions and a universal ethical floor before execution. We present a functional system, including a demonstration interface (www.Creed.Space) with a prototypical constitution-sharing portal, and successful integration with third-party models via the Model Context Protocol (MCP). Comprehensive benchmark evaluations (HarmBench, AgentHarm) demonstrate that our Superego agent dramatically reduces harmful outputs, achieving up to a 98.3% harm score reduction and near-perfect refusal rates (e.g., 100% with Claude Sonnet 4 on AgentHarm's harmful set) for leading LLMs like Gemini 2.5 Flash and GPT-4o. This approach substantially simplifies personalized AI alignment, rendering agentic systems more reliably attuned to individual and cultural contexts, while also enabling substantial safety improvements. An overview on this research with examples is available at https://superego.creed.space.

</details>


### [150] [Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations](https://arxiv.org/abs/2506.13776)
*Kevin L. Wei,Patricia Paskov,Sunishchal Dev,Michael J. Byun,Anka Reuel,Xavier Roberts-Gaal,Rachel Calcott,Evie Coxon,Chinmay Deshpande*

Key words: 人类基线，AI评估，基础模型，超人类性能，透明性

TL;DR: 本文主张在基础模型评估中需要更严谨和透明的人类基线，提出了推荐方法和报告清单，以提高人类与AI性能比较的可靠性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前人类基线评估方法不够严谨且缺乏透明度，导致声称的“超人类”性能缺乏可靠依据。需要改进方法以支持AI评估的准确性和可比性。

Method: 基于对测量理论和AI评估文献的元综述，提出了一个设计、执行和报告人类基线的框架及推荐方法，并开发了检查清单用于系统审查115项人类基线研究。

Result: 研究发现现有基线方法存在不足，提出的检查清单有助于改进人类基线的设计和结果报告。

Conclusion: 本文的工作旨在推动更严谨的AI评估实践，更好地服务于研究社区和政策制定者。

Abstract: In this position paper, we argue that human baselines in foundation model evaluations must be more rigorous and more transparent to enable meaningful comparisons of human vs. AI performance, and we provide recommendations and a reporting checklist towards this end. Human performance baselines are vital for the machine learning community, downstream users, and policymakers to interpret AI evaluations. Models are often claimed to achieve "super-human" performance, but existing baselining methods are neither sufficiently rigorous nor sufficiently well-documented to robustly measure and assess performance differences. Based on a meta-review of the measurement theory and AI evaluation literatures, we derive a framework with recommendations for designing, executing, and reporting human baselines. We synthesize our recommendations into a checklist that we use to systematically review 115 human baselines (studies) in foundation model evaluations and thus identify shortcomings in existing baselining methods; our checklist can also assist researchers in conducting human baselines and reporting results. We hope our work can advance more rigorous AI evaluation practices that can better serve both the research community and policymakers. Data is available at: https://github.com/kevinlwei/human-baselines

</details>


### [151] [The NordDRG AI Benchmark for Large Language Models](https://arxiv.org/abs/2506.13790)
*Tapio Pitkäranta*

Key words: LLM, DRG, 临床编码, 多语言逻辑, 医院资金

TL;DR: NordDRG-AI-Benchmark是首个公开的DRG基准测试，评估大语言模型在多语言诊断、程序和费率逻辑上的推理能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 填补医院资金层（DRG）中缺乏公开基准测试的空白，评估LLMs在临床编码和决策支持中的实际表现。

Method: 提供包含定义表、专家手册和任务提示的资源包，测试LLMs在多任务中的表现。

Result: 不同LLMs表现差异显著，OpenAI o3得分最高（9/9），而Gemini模型表现较差。

Conclusion: 该基准揭示了LLMs在特定领域的优势和不足，为医院资金自动化研究提供了可复现的基线。

Abstract: Large language models (LLMs) are already being piloted for clinical coding and decision support. However, until now, no open benchmark has targeted the hospital funding layer where Diagnosis-Related Groups (DRG) determine reimbursement across many countries. We release NordDRG-AI-Benchmark, the first public test-bed that captures a complete DRG rule set and evaluates an LLM's ability to reason over multilingual diagnosis, procedure, and tariff logic.
  The benchmark bundles three classes of artefacts: (i) definition tables with 20 interlinked tables covering DRG logic, ICD and NCSP codes, age/sex splits, and country flags; (ii) expert manuals and changelog templates describing real governance workflows; and (iii) a prompt pack of 14 CaseMix tasks that span code lookup, cross-table inference, multilingual terminology, and quality-assurance audits.
  All artefacts are available at: https://github.com/longshoreforrest/norddrg-ai-benchmark
  A baseline demonstration shows that five state-of-the-art LLMs perform very differently on the nine automatically verifiable tasks: o3 (OpenAI) scores 9 out of 9, GPT-4o and o4-mini-high score 7 out of 9, while Gemini 2.5 Pro and Gemini 2.5 Flash solve only 5 out of 9 and 3 out of 9, respectively. These results confirm that NordDRG-AI-Benchmark highlights domain-specific strengths and weaknesses that remain hidden in generic LLM benchmarks, offering a reproducible baseline for research on trustworthy automation in hospital funding.

</details>


### [152] [ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \& a ML Ensemble on Longitudinal Identity Resolution](https://arxiv.org/abs/2506.13792)
*Gonçalo Hora de Carvalho,Lazar S. Popov,Sander Kaatee,Kristinn R. Thórisson,Tangrui Li,Pétur Húni Björnsson,Jilles S. Dibangoye*

Key words: ICE-ID, 历史身份解析, NARS, 表格数据, 跨学科研究

TL;DR: ICE-ID是一个针对历史身份解析的新基准数据集，涵盖220年的冰岛人口普查记录，支持跨世代数据分析。实验表明，NARS方法在任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 填补长期人口实体匹配研究的数据空白，提供一个开源、大规模的基准数据集。

Method: 评估了规则匹配、ML集成、LLM以及NARS方法，重点测试NARS在表格数据中的表现。

Result: NARS方法简单且竞争力强，达到了当前最佳水平（SOTA）。

Conclusion: ICE-ID的发布支持了身份解析方法的可重复性研究，并有望推动跨学科合作。

Abstract: We introduce ICE-ID, a novel benchmark dataset for historical identity resolution, comprising 220 years (1703-1920) of Icelandic census records. ICE-ID spans multiple generations of longitudinal data, capturing name variations, demographic changes, and rich genealogical links. To the best of our knowledge, this is the first large-scale, open tabular dataset specifically designed to study long-term person-entity matching in a real-world population. We define identity resolution tasks (within and across census waves) with clearly documented metrics and splits. We evaluate a range of methods: handcrafted rule-based matchers, a ML ensemble as well as LLMs for structured data (e.g. transformer-based tabular networks) against a novel approach to tabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose AI framework designed to reason with limited knowledge and resources. Its core is Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that NARS is suprisingly simple and competitive with other standard approaches, achieving SOTA at our task. By releasing ICE-ID and our code, we enable reproducible benchmarking of identity resolution approaches in longitudinal settings and hope that ICE-ID opens new avenues for cross-disciplinary research in data linkage and historical analytics.

</details>


### [153] [Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained Reflection](https://arxiv.org/abs/2506.13793)
*Zongxian Yang,Jiayu Qian,Zegao Peng,Haoyu Zhang,Zhi-An Huang*

Key words: medical reasoning, self-correction, fine-grained reflection, Med-REFL, MedQA-USMLE

TL;DR: 论文提出Med-REFL方法，通过细粒度自我校正提升医学推理能力，在多个数据集上表现优异。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 医学领域推理模型表现不佳，主要因中间反思步骤质量不足，高风险的医疗场景对准确性要求极高。

Method: 采用树状思维分解医学问题，定量评估每一步及其后续反思，自动构建优化数据以减少专家标注依赖。

Result: 在MedQA-USMLE基准上平均提升4.11%，且7B/8B模型性能进一步提升4.13%。

Conclusion: 注重反思质量可显著提升医学AI推理的准确性和可信度。

Abstract: Large reasoning models have recently made significant strides in mathematical and code reasoning, yet their success has not transferred smoothly to the medical domain. While multiple factors contribute to this disparity, a critical issue is the inadequate focus on the quality of intermediate reflection steps, which is particularly crucial in high-stakes medical scenarios. To address this challenge, we propose Med-REFL, a \underline{\textbf{Med}}ical \underline{\textbf{R}}easoning \underline{\textbf{E}}nhancement via self-corrected \underline{\textbf{F}}ine-grained ref\underline{\textbf{L}}ection. Our method leverages a tree-of-thought approach to decompose medical questions into fine-grained reasoning paths, quantitatively evaluating each step and its subsequent reflections. These assessments enable automatic construction of direct preference optimization data, reducing reliance on expensive expert annotations while guiding models to identify and correct reasoning errors. Experimental results on the MedQA-USMLE benchmark demonstrate Med-REFL achieves consistent improvements, with average gains up to 4.11\%. Notably, it further boosts the state-of-the-art performance of 7B/8B models by an additional 4.13\%. Furthermore, Med-REFL exhibits strong generalization capabilities and robustness across several challenging medical question-answering datasets. Our work illustrates that prioritizing reflection quality leads to more accurate and trustworthy reasoning in medical AI applications. Checkpoints, code, and data can be found \href{https://github.com/TianYin123/Med-REFL}{here}.

</details>


### [154] [BotTrans: A Multi-Source Graph Domain Adaptation Approach for Social Bot Detection](https://arxiv.org/abs/2506.13795)
*Boshen Shi,Yongqing Wang,Fangda Guo,Jiangli Shao,Huawei Shen,Xueqi Cheng*

Key words: 社交机器人检测,图域适应,多源知识,网络异质性,标签稀缺

TL;DR: 论文提出了一种名为BotTrans的多源图域适应模型，用于解决社交机器人检测中的标签稀缺问题，通过利用多源网络知识并优化迁移过程，显著提升了检测性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 社交机器人检测中标签稀缺问题严重，现有方法因网络异质性和单源迁移限制难以有效学习足够且准确的知识。

Method: 提出BotTrans模型，包括建立跨源域拓扑、聚合跨域邻居信息、优化源-目标对相关性，并利用目标域语义知识进行改进。

Result: 在真实数据集上的实验表明，BotTrans优于现有方法，证明了其利用多源知识的能力。

Conclusion: BotTrans通过多源知识迁移和优化策略，有效解决了标签稀缺问题，提升了检测性能。

Abstract: Transferring extensive knowledge from relevant social networks has emerged as a promising solution to overcome label scarcity in detecting social bots and other anomalies with GNN-based models. However, effective transfer faces two critical challenges. Firstly, the network heterophily problem, which is caused by bots hiding malicious behaviors via indiscriminately interacting with human users, hinders the model's ability to learn sufficient and accurate bot-related knowledge from source domains. Secondly, single-source transfer might lead to inferior and unstable results, as the source network may embody weak relevance to the task and provide limited knowledge. To address these challenges, we explore multiple source domains and propose a multi-source graph domain adaptation model named \textit{BotTrans}. We initially leverage the labeling knowledge shared across multiple source networks to establish a cross-source-domain topology with increased network homophily. We then aggregate cross-domain neighbor information to enhance the discriminability of source node embeddings. Subsequently, we integrate the relevance between each source-target pair with model optimization, which facilitates knowledge transfer from source networks that are more relevant to the detection task. Additionally, we propose a refinement strategy to improve detection performance by utilizing semantic knowledge within the target domain. Extensive experiments on real-world datasets demonstrate that \textit{BotTrans} outperforms the existing state-of-the-art methods, revealing its efficacy in leveraging multi-source knowledge when the target detection task is unlabeled.

</details>


### [155] [Feedforward Ordering in Neural Connectomes via Feedback Arc Minimization](https://arxiv.org/abs/2506.13799)
*Soroush Vahidi*

Key words: 反馈弧最小化，加权有向图，神经连接组，FlyWire数据集，贪心启发式，强连通分量

TL;DR: 提出了一套可扩展的算法，用于最小化大规模加权有向图中的反馈弧，以揭示神经连接组中具有生物学意义的反馈前馈结构。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 目标是揭示神经连接组中有生物学意义的反馈前馈结构。

Method: 结合贪心启发式、增益感知的局部优化和基于强连通分量的全局结构分析。

Result: 实验表明，最佳解决方案在提高前向边权重方面优于先前表现最好的方法。

Conclusion: 算法在Python中高效实现，并在Google Colab Pro+上通过基于云的执行验证。

Abstract: We present a suite of scalable algorithms for minimizing feedback arcs in large-scale weighted directed graphs, with the goal of revealing biologically meaningful feedforward structure in neural connectomes. Using the FlyWire Connectome Challenge dataset, we demonstrate the effectiveness of our ranking strategies in maximizing the total weight of forward-pointing edges. Our methods integrate greedy heuristics, gain-aware local refinements, and global structural analysis based on strongly connected components. Experiments show that our best solution improves the forward edge weight over previous top-performing methods. All algorithms are implemented efficiently in Python and validated using cloud-based execution on Google Colab Pro+.

</details>


### [156] [Causality in the human niche: lessons for machine learning](https://arxiv.org/abs/2506.13803)
*Richard D. Lange,Konrad P. Kording*

Key words: 因果认知, 结构因果模型（SCM）, 机器学习, 人类学, 类比学习

TL;DR: 论文探讨了人类因果认知与现有机器学习系统在因果关系处理上的差异，提出了构建更具人类化因果能力的AI的必要性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有机器学习系统在泛化和高效学习新领域的能力上较弱，而人类的因果认知可能是关键。希望通过更接近人类的因果认知来提升AI的效能和可解释性。

Method: 分析了结构因果模型（SCM）框架的局限性，提出其在人类日常因果类比中的不足，并探讨人类因果认知的特性及其适应性。

Result: 强调了人类因果认知在泛化和类比学习中的优势，以及这些特性在提升AI能力方面的潜力。

Conclusion: 未来研究应更关注人类因果认知的适应性特性，以构建更强大、可控且可解释的机器学习系统。

Abstract: Humans interpret the world around them in terms of cause and effect and communicate their understanding of the world to each other in causal terms. These causal aspects of human cognition are thought to underlie humans' ability to generalize and learn efficiently in new domains, an area where current machine learning systems are weak. Building human-like causal competency into machine learning systems may facilitate the construction of effective and interpretable AI. Indeed, the machine learning community has been importing ideas on causality formalized by the Structural Causal Model (SCM) framework, which provides a rigorous formal language for many aspects of causality and has led to significant advances. However, the SCM framework fails to capture some salient aspects of human causal cognition and has likewise not yet led to advances in machine learning in certain critical areas where humans excel. We contend that the problem of causality in the ``human niche'' -- for a social, autonomous, and goal-driven agent sensing and acting in the world in which humans live -- is quite different from the kind of causality captured by SCMs. For example, everyday objects come in similar types that have similar causal properties, and so humans readily generalize knowledge of one type of object (cups) to another related type (bowls) by drawing causal analogies between objects with similar properties, but such analogies are at best awkward to express in SCMs. We explore how such causal capabilities are adaptive in, and motivated by, the human niche. By better appreciating properties of human causal cognition and, crucially, how those properties are adaptive in the niche in which humans live, we hope that future work at the intersection of machine learning and causality will leverage more human-like inductive biases to create more capable, controllable, and interpretable systems.

</details>


### [157] [Bridging Pattern-Aware Complexity with NP-Hard Optimization: A Unifying Framework and Empirical Study](https://arxiv.org/abs/2506.13810)
*Olivier Saidi*

Key words: NP难问题, TSP, 模式感知, 元学习, PUE

TL;DR: 提出了一种新框架，利用结构模式降低计算复杂度，在TSP等问题中实现显著性能提升。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决NP难问题在实际中因结构性规律而可能高效求解的问题。

Method: 提出模式感知复杂度框架，结合元学习求解器，引入PUE等量化指标。

Result: TSP基准测试中解决方案质量提升达79%。

Conclusion: 提供了一种统一的实用视角，利用模式驱动效率，区别于理论NP难性。

Abstract: NP hard optimization problems like the Traveling Salesman Problem (TSP) defy efficient solutions in the worst case, yet real-world instances often exhibit exploitable patterns. We propose a novel patternaware complexity framework that quantifies and leverages structural regularities e.g., clustering, symmetry to reduce effective computational complexity across domains, including financial forecasting and LLM optimization. With rigorous definitions, theorems, and a meta learning driven solver pipeline, we introduce metrics like Pattern Utilization Efficiency (PUE) and achieve up to 79 percent solution quality gains in TSP benchmarks (22 to 2392 cities). Distinct from theoretical NP hardness, our approach offers a unified, practical lens for pattern-driven efficiency.

</details>


### [158] [The Reflexive Integrated Information Unit: A Differentiable Primitive for Artificial Consciousness](https://arxiv.org/abs/2506.13825)
*Gnankan Landry Regis N'guessan,Issa Karambal*

Key words: 人工意识、RIIU、信息整合、可微分性、Auto-$Φ$

TL;DR: 该论文提出了一个名为RIIU的小型可训练模块，用于研究人工意识，通过增强隐藏状态、记录因果足迹和广播缓冲机制，实现了端到端可微分性和信息整合。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前人工意识研究缺乏类似感知器的小型可训练模块，导致无法复制、基准化和迭代改进。RIIU的引入旨在填补这一空白。

Method: RIIU在隐藏状态基础上引入了元状态和广播缓冲机制，通过滑动窗口协方差和可微分Auto-$Φ$代理实现局部信息整合。

Result: 在Grid-world中，RIIU在13步内恢复>90%的奖励，速度是GRU的两倍，同时保持非零Auto-$Φ$信号。

Conclusion: RIIU将‘类意识’计算缩小到单元尺度，将哲学争论转化为可实证的数学问题。

Abstract: Research on artificial consciousness lacks the equivalent of the perceptron: a small, trainable module that can be copied, benchmarked, and iteratively improved. We introduce the Reflexive Integrated Information Unit (RIIU), a recurrent cell that augments its hidden state $h$ with two additional vectors: (i) a meta-state $μ$ that records the cell's own causal footprint, and (ii) a broadcast buffer $B$ that exposes that footprint to the rest of the network. A sliding-window covariance and a differentiable Auto-$Φ$ surrogate let each RIIU maximize local information integration online. We prove that RIIUs (1) are end-to-end differentiable, (2) compose additively, and (3) perform $Φ$-monotone plasticity under gradient ascent. In an eight-way Grid-world, a four-layer RIIU agent restores $>90\%$ reward within 13 steps after actuator failure, twice as fast as a parameter-matched GRU, while maintaining a non-zero Auto-$Φ$ signal. By shrinking "consciousness-like" computation down to unit scale, RIIUs turn a philosophical debate into an empirical mathematical problem.

</details>


### [159] [LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning](https://arxiv.org/abs/2506.13841)
*Miho Koda,Yu Zheng,Ruixian Ma,Mingyang Sun,Devesh Pansare,Fabio Duarte,Paolo Santi*

Key words: 大语言模型,推理能力,选址任务,基准测试,真实场景

TL;DR: 该论文提出了LocationReasoner基准，用于评估大语言模型（LLM）在真实场景中的选址推理能力，发现当前高级模型表现有限。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有LLM在数学和代码生成领域的推理能力出色，但在真实复杂场景中的表现尚未明确。

Method: 引入LocationReasoner基准，包含300多个多难度查询，结合沙盒工具评估模型在空间、环境和逻辑约束下的表现。

Result: 评测显示，即使是OpenAI o4模型仍有30%失败率，且代理策略（如ReAct和Reflexion）因过度推理效果更差。

Conclusion: LLM在全局和非线性推理上存在局限，LocationReasoner的发布旨在推动更稳健的LLM发展。

Abstract: Recent advances in large language models (LLMs), particularly those enhanced through reinforced post-training, have demonstrated impressive reasoning capabilities, as exemplified by models such as OpenAI o1 and DeepSeek-R1. However, these capabilities are predominantly benchmarked on domains like mathematical problem solving and code generation -- leaving open the question of whether such reasoning skills generalize to complex, real-world scenarios. In this paper, we introduce LocationReasoner, a benchmark designed to evaluate LLMs' reasoning abilities in the context of real-world site selection, where models must identify feasible locations by reasoning over diverse and complicated spatial, environmental, and logistical constraints. The benchmark comprises over 300 carefully crafted queries of varying difficulty levels, supported by a sandbox environment with in-house tools for constraint-based location search. Extensive evaluations reveal that state-of-the-art reasoning models offer limited improvement over their non-reasoning predecessors in real-world contexts, with even the latest OpenAI o4 model failing on 30% of site selection tasks. Moreover, agentic strategies such as ReAct and Reflexion often suffer from over-reasoning, leading to worse outcomes than direct code-generation prompting. With key limitations of LLMs in holistic and non-linear reasoning highlighted, we release LocationReasoner to foster the development of LLMs and agents capable of robust, grounded reasoning in real-world decision-making tasks. Codes and data for our benchmark are available at https://github.com/miho-koda/LocationReasoner.

</details>


### [160] [Evaluating Explainability: A Framework for Systematic Assessment and Reporting of Explainable AI Features](https://arxiv.org/abs/2506.13917)
*Miguel A. Lago,Ghada Zamzmi,Brandon Eich,Jana G. Delfino*

Key words: 可解释AI,评估框架,一致性,合理性,忠实性,实用性

TL;DR: 本文提出了一个评估可解释AI特征的框架，包含一致性、合理性、忠实性和实用性四个标准，并通过案例研究展示了其应用。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 目前缺乏评估AI解释质量的技术，本研究旨在填补这一空白。

Method: 提出基于四个标准的评估框架，并开发了一个评分卡用于评估可解释AI方法。

Result: 框架成功应用于评估Ablation CAM和Eigen CAM在乳腺病变检测中的解释性。

Conclusion: 该框架为AI解释质量的评估提供了明确标准，促进可解释性特征的改进。

Abstract: Explainability features are intended to provide insight into the internal mechanisms of an AI device, but there is a lack of evaluation techniques for assessing the quality of provided explanations. We propose a framework to assess and report explainable AI features. Our evaluation framework for AI explainability is based on four criteria: 1) Consistency quantifies the variability of explanations to similar inputs, 2) Plausibility estimates how close the explanation is to the ground truth, 3) Fidelity assesses the alignment between the explanation and the model internal mechanisms, and 4) Usefulness evaluates the impact on task performance of the explanation. Finally, we developed a scorecard for AI explainability methods that serves as a complete description and evaluation to accompany this type of algorithm. We describe these four criteria and give examples on how they can be evaluated. As a case study, we use Ablation CAM and Eigen CAM to illustrate the evaluation of explanation heatmaps on the detection of breast lesions on synthetic mammographies. The first three criteria are evaluated for clinically-relevant scenarios. Our proposed framework establishes criteria through which the quality of explanations provided by AI models can be evaluated. We intend for our framework to spark a dialogue regarding the value provided by explainability features and help improve the development and evaluation of AI-based medical devices.

</details>


### [161] [Integrating Knowledge Graphs and Bayesian Networks: A Hybrid Approach for Explainable Disease Risk Prediction](https://arxiv.org/abs/2506.13920)
*Mbithe Nzomo,Deshendran Moodley*

Key words: 知识图谱, 贝叶斯网络, 电子健康记录, 疾病风险预测, 房颤

TL;DR: 提出了一种基于知识图谱和贝叶斯网络的多模态电子健康记录分析方法，用于可解释的疾病风险预测，并在房颤应用中验证其效果。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决通用医学知识在特定医疗场景中的适应性问题，以及数据不完整和健康结果不确定性带来的挑战。

Method: 通过知识图谱和贝叶斯网络结合多模态电子健康记录数据构建预测模型。

Result: 模型在房颤预测中表现良好，能够有效处理不确定性并提供解释性。

Conclusion: 该方法平衡了通用知识与应用场景需求，具有实际临床应用潜力。

Abstract: Multimodal electronic health record (EHR) data is useful for disease risk prediction based on medical domain knowledge. However, general medical knowledge must be adapted to specific healthcare settings and patient populations to achieve practical clinical use. Additionally, risk prediction systems must handle uncertainty from incomplete data and non-deterministic health outcomes while remaining explainable. These challenges can be alleviated by the integration of knowledge graphs (KGs) and Bayesian networks (BNs). We present a novel approach for constructing BNs from ontology-based KGs and multimodal EHR data for explainable disease risk prediction. Through an application use case of atrial fibrillation and real-world EHR data, we demonstrate that the approach balances generalised medical knowledge with patient-specific context, effectively handles uncertainty, is highly explainable, and achieves good predictive performance.

</details>


### [162] [ProfiLLM: An LLM-Based Framework for Implicit Profiling of Chatbot Users](https://arxiv.org/abs/2506.13980)
*Shahaf David,Yair Meidan,Ido Hersko,Daniel Varnovitzky,Dudu Mimran,Yuval Elovici,Asaf Shabtai*

Key words: LLM, 用户画像, 聊天机器人, 个性化, IT/网络安全

TL;DR: 论文提出了ProfiLLM框架，通过聊天机器人交互实现动态用户画像，并在IT/网络安全领域验证其有效性，显著提升个性化响应能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决现有聊天机器人在个性化响应上的不足，特别是在专业知识密集领域（如IT/网络安全）中用户知识水平差异大的问题。

Method: 提出ProfiLLM框架，包括可适应多领域的分类法和基于LLM的用户画像方法，并在IT/网络安全领域开发了ProfiLLM[ITSec]变体。

Result: ProfiLLM[ITSec]在1760次对话中快速准确推断用户画像，单次提示后预测与实际分数差距减少55-65%。

Conclusion: ProfiLLM框架显著提升聊天机器人的动态个性化能力，并提供数据集和方法支持未来研究。

Abstract: Despite significant advancements in conversational AI, large language model (LLM)-powered chatbots often struggle with personalizing their responses according to individual user characteristics, such as technical expertise, learning style, and communication preferences. This lack of personalization is particularly problematic in specialized knowledge-intense domains like IT/cybersecurity (ITSec), where user knowledge levels vary widely. Existing approaches for chatbot personalization primarily rely on static user categories or explicit self-reported information, limiting their adaptability to an evolving perception of the user's proficiency, obtained in the course of ongoing interactions. In this paper, we propose ProfiLLM, a novel framework for implicit and dynamic user profiling through chatbot interactions. This framework consists of a taxonomy that can be adapted for use in diverse domains and an LLM-based method for user profiling in terms of the taxonomy. To demonstrate ProfiLLM's effectiveness, we apply it in the ITSec domain where troubleshooting interactions are used to infer chatbot users' technical proficiency. Specifically, we developed ProfiLLM[ITSec], an ITSec-adapted variant of ProfiLLM, and evaluated its performance on 1,760 human-like chatbot conversations from 263 synthetic users. Results show that ProfiLLM[ITSec] rapidly and accurately infers ITSec profiles, reducing the gap between actual and predicted scores by up to 55--65\% after a single prompt, followed by minor fluctuations and further refinement. In addition to evaluating our new implicit and dynamic profiling framework, we also propose an LLM-based persona simulation methodology, a structured taxonomy for ITSec proficiency, our codebase, and a dataset of chatbot interactions to support future research.

</details>


### [163] [SANGAM: SystemVerilog Assertion Generation via Monte Carlo Tree Self-Refine](https://arxiv.org/abs/2506.13983)
*Adarsh Gupta,Bhabesh Mali,Chandan Karfa*

Key words: SystemVerilog断言，LLM，蒙特卡洛树搜索，自动化生成

TL;DR: 本文介绍了SANGAM，一种基于LLM引导的蒙特卡洛树搜索的SystemVerilog断言生成框架，能够从工业级规范中自动生成SVA。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 利用大型语言模型（LLM）在推理领域的最新进展，为更复杂和自动化的硬件断言生成技术开辟了新途径。

Method: 框架分为三个阶段：1）多模态规范处理，使用Signal Mapper、SPEC Analyzer和Waveform Analyzer LLM Agents；2）使用MCTSr算法对每个信号的SVA进行自动推理；3）结合MCTSr生成的推理痕迹生成每个信号的SVA断言。

Result: 实验结果表明，SANGAM能够生成一组稳健的SVA，在评估过程中表现优于现有方法。

Conclusion: SANGAM框架展示了LLM和蒙特卡洛树搜索在自动化硬件断言生成中的潜力。

Abstract: Recent advancements in the field of reasoning using Large Language Models (LLMs) have created new possibilities for more complex and automatic Hardware Assertion Generation techniques. This paper introduces SANGAM, a SystemVerilog Assertion Generation framework using LLM-guided Monte Carlo Tree Search for the automatic generation of SVAs from industry-level specifications. The proposed framework utilizes a three-stage approach: Stage 1 consists of multi-modal Specification Processing using Signal Mapper, SPEC Analyzer, and Waveform Analyzer LLM Agents. Stage 2 consists of using the Monte Carlo Tree Self-Refine (MCTSr) algorithm for automatic reasoning about SVAs for each signal, and finally, Stage 3 combines the MCTSr-generated reasoning traces to generate SVA assertions for each signal. The results demonstrated that our framework, SANGAM, can generate a robust set of SVAs, performing better in the evaluation process in comparison to the recent methods.

</details>


### [164] [Machine Mirages: Defining the Undefined](https://arxiv.org/abs/2506.13990)
*Hamidou Tembine*

Key words: 多模态机器学习, 机器幻象, 认知错误, 伦理, 智能生态系统

TL;DR: 论文讨论了多模态机器学习系统在图像、语言和声音处理任务中表现出的新型认知错误——机器幻象，提出需明确定义和系统评估这些错误，以提升系统可靠性并构建伦理共生的智能生态系统。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着多模态机器学习系统在各类任务中展现出与动物或人类相当的流畅性，这些系统开始表现出新型的认知错误（机器幻象），这些错误需要被深入研究以提升系统可靠性和伦理兼容性。

Method: 论文通过分类和描述这些机器幻象（如幻觉、语义漂移等），并探讨它们的成因和影响，提出需要系统性评估。

Result: 机器幻象的存在揭示了当前多模态系统的局限性，需通过明确定义和评估来改进系统设计和伦理框架。

Conclusion: 理解机器幻象对提升机器智能的可靠性和构建伦理共生的智能生态系统至关重要。

Abstract: As multimodal machine intelligence systems started achieving average animal-level and average human-level fluency in many measurable tasks in processing images, language, and sound, they began to exhibit a new class of cognitive aberrations: machine mirages. These include delusion, illusion, confabulation, hallucination, misattribution error, semantic drift, semantic compression, exaggeration, causal inference failure, uncanny valley of perception, bluffing-patter-bullshitting, cognitive stereotypy, pragmatic misunderstanding, hypersignification, semantic reheating-warming, simulated authority effect, fallacious abductive leap, contextual drift, referential hallucination, semiotic Frankenstein effect, calibration failure, spurious correlation, bias amplification, concept drift sensitivity, misclassification under uncertainty, adversarial vulnerability, overfitting, prosodic misclassification, accent bias, turn boundary failure, semantic boundary confusion, noise overfitting, latency-induced decision drift, ambiguity collapse and other forms of error that mimic but do not replicate human or animal fallibility. This article presents some of the errors and argues that these failures must be explicitly defined and systematically assessed. Understanding machine mirages is essential not only for improving machine intelligence reliability but also for constructing a multiscale ethical, co-evolving intelligence ecosystem that respects the diverse forms of life, cognition, and expression it will inevitably touch.

</details>


### [165] [Discovering Temporal Structure: An Overview of Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.14045)
*Martin Klissarov,Akhil Bagaria,Ziyan Luo,George Konidaris,Doina Precup,Marlos C. Machado*

Key words: 分层强化学习, 时间结构, AI代理, 决策, 大型语言模型

TL;DR: 该论文探讨了分层强化学习（HRL）在复杂开放环境中的探索、规划和学习的潜力，分析了HRL如何帮助AI代理在决策中利用时间结构，并总结了相关方法和面临的挑战。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究HRL在AI中的潜力，特别是在处理复杂开放环境时如何通过时间结构提升代理的决策能力。

Method: 综述了多种HRL方法，包括从在线经验、离线数据集学习，以及利用大型语言模型（LLMs）来发现时间结构。

Result: HRL在解决决策问题中展现出潜力，但其结构定义和适用场景仍需进一步明确。

Conclusion: HRL在AI领域具有重要价值，但其结构发现方法和应用领域仍需深入研究。

Abstract: Developing agents capable of exploring, planning and learning in complex open-ended environments is a grand challenge in artificial intelligence (AI). Hierarchical reinforcement learning (HRL) offers a promising solution to this challenge by discovering and exploiting the temporal structure within a stream of experience. The strong appeal of the HRL framework has led to a rich and diverse body of literature attempting to discover a useful structure. However, it is still not clear how one might define what constitutes good structure in the first place, or the kind of problems in which identifying it may be helpful. This work aims to identify the benefits of HRL from the perspective of the fundamental challenges in decision-making, as well as highlight its impact on the performance trade-offs of AI agents. Through these benefits, we then cover the families of methods that discover temporal structure in HRL, ranging from learning directly from online experience to offline datasets, to leveraging large language models (LLMs). Finally, we highlight the challenges of temporal structure discovery and the domains that are particularly well-suited for such endeavours.

</details>


### [166] [Into the Unknown: Applying Inductive Spatial-Semantic Location Embeddings for Predicting Individuals' Mobility Beyond Visited Places](https://arxiv.org/abs/2506.14070)
*Xinglei Wang,Tao Cheng,Stephen Law,Zichao Zeng,Ilya Ilyankou,Junyuan Liu,Lu Yin,Weiming Huang,Natchapon Jongwiriyanurak*

Key words: 人类移动预测,位置嵌入,多模态学习,归纳学习,对比学习

TL;DR: CaLLiPer框架通过融合空间坐标和语义特征，提升位置嵌入在人类移动预测中的表现，特别是在新位置场景下表现优异。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 克服传统方法在编码显式空间信息、整合丰富城市语义上下文及处理未知位置方面的局限性。

Method: 使用CaLLiPer框架，通过对比学习融合空间坐标和语义特征，生成空间明确、语义丰富且归纳性强的位置嵌入。

Result: 在四个公开数据集的实验中，CaLLiPer在常规和归纳设置下均优于基线方法，尤其是在处理新位置时表现突出。

Conclusion: 多模态归纳位置嵌入有潜力提升人类移动预测系统的能力。

Abstract: Predicting individuals' next locations is a core task in human mobility modelling, with wide-ranging implications for urban planning, transportation, public policy and personalised mobility services. Traditional approaches largely depend on location embeddings learned from historical mobility patterns, limiting their ability to encode explicit spatial information, integrate rich urban semantic context, and accommodate previously unseen locations. To address these challenges, we explore the application of CaLLiPer -- a multimodal representation learning framework that fuses spatial coordinates and semantic features of points of interest through contrastive learning -- for location embedding in individual mobility prediction. CaLLiPer's embeddings are spatially explicit, semantically enriched, and inductive by design, enabling robust prediction performance even in scenarios involving emerging locations. Through extensive experiments on four public mobility datasets under both conventional and inductive settings, we demonstrate that CaLLiPer consistently outperforms strong baselines, particularly excelling in inductive scenarios. Our findings highlight the potential of multimodal, inductive location embeddings to advance the capabilities of human mobility prediction systems. We also release the code and data (https://github.com/xlwang233/Into-the-Unknown) to foster reproducibility and future research.

</details>


### [167] [FormGym: Doing Paperwork with Agents](https://arxiv.org/abs/2506.14079)
*Matthew Toles,Rattandeep Singh,Isaac Song Zhou Yu*

Key words: 表单填写, 多模态理解, 信息检索, 工具使用, FieldFinder

TL;DR: 论文提出了一种新的表单填写基准测试，并开发了FieldFinder工具以提升LLMs在表单填写中的表现。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决纯图像领域表单填写的挑战性任务，特别是缺乏OCR、PDF文本或DOM的情况下，计算机代理需要多模态理解、信息检索和工具使用等能力。

Method: 提出了一个包含432个字段、55份文档和3个任务的表单填写基准测试，并开发了FieldFinder工具来帮助LLMs定位表单中的填写位置。

Result: 基线VLA的准确率低于1%，GUI代理的准确率为10.6-68.0%；使用FieldFinder后，所有模型在所有六种研究条件下均表现更好，最大提升从2%到56%。

Conclusion: FieldFinder显著提升了表单填写的性能，证明了其在纯图像表单填写任务中的有效性。

Abstract: Completing paperwork is a challenging and time-consuming problem. Form filling is especially challenging in the pure-image domain without access to OCR, typeset PDF text, or a DOM. For computer agents, it requires multiple abilities, including multi-modal understanding, information retrieval, and tool-use. We present a novel form-filling benchmark consisting of 432 fields spread across 55 documents and 3 tasks, requiring knowledge of 236 features per user. We find that baseline VLAs achieve less than 1% accuracy in most cases, primarily due to poor localization ability. GUI agents also struggle, scoring between 10.6-68.0% despite high cost and latency. Therefore, we also contribute FieldFinder, a tool to assist LLMs in identifying where to place text on a form. With FieldFinder, all models achieve equal or better performance in all six study conditions, with a maximum increase from 2% to 56%.

</details>


### [168] [Lightweight Relevance Grader in RAG](https://arxiv.org/abs/2506.14084)
*Taehee Jeong*

Key words: RAG, 相关性评分器, llama-3.2-1b, 轻量级模型

TL;DR: 提出了在RAG中使用轻量级语言模型llama-3.2-1b作为相关性评分器，显著提升检索文档的精确度。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决RAG中检索文档与查询相关性的挑战，降低相关性评分器的计算成本。

Method: 微调轻量级语言模型llama-3.2-1b作为相关性评分器。

Result: 精确度从0.1301提升至0.7750，性能可与llama-3.1-70b媲美。

Conclusion: 轻量级模型在RAG中能有效提升相关性评分效果。

Abstract: Retrieval-Augmented Generation (RAG) addresses limitations of large language models (LLMs) by leveraging a vector database to provide more accurate and up-to-date information. When a user submits a query, RAG executes a vector search to find relevant documents, which are then used to generate a response. However, ensuring the relevance of retrieved documents with a query would be a big challenge. To address this, a secondary model, known as a relevant grader, can be served to verify its relevance. To reduce computational requirements of a relevant grader, a lightweight small language model is preferred. In this work, we finetuned llama-3.2-1b as a relevant grader and achieved a significant increase in precision from 0.1301 to 0.7750. Its precision is comparable to that of llama-3.1-70b. Our code is available at https://github.com/taeheej/Lightweight-Relevance-Grader-in-RAG.

</details>


### [169] [Fragile Preferences: A Deep Dive Into Order Effects in Large Language Models](https://arxiv.org/abs/2506.14092)
*Haonan Yin,Shai Vardi,Vidyanand Choudhary*

Key words: 大型语言模型、位置偏见、顺序效应、决策支持系统、缓解策略

TL;DR: 该论文首次全面研究了大型语言模型（LLM）在多个架构和领域中的位置偏见，揭示了包括新颖的中心性偏见在内的强一致顺序效应，并提出了缓解策略。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究LLM在高风险决策支持系统中的位置偏见，以填补现有文献中系统性解析和与潜在偏好结构关联的空白。

Method: 通过多LLM架构和领域的实验，分析顺序效应，并提出分类框架以区分浅层平局打破与真实判断扭曲。

Result: 发现LLM存在强顺序效应和质量依赖的偏移，顺序偏见通常比性别偏见更强，且可能导致选择劣质选项。

Conclusion: LLM表现出与人类决策不同的独特失败模式，需采用针对性缓解策略（如温度参数）。

Abstract: Large language models (LLMs) are increasingly used in decision-support systems across high-stakes domains such as hiring and university admissions, where decisions often involve selecting among competing alternatives. While prior work has noted positional order biases in LLM-driven comparisons, these biases have not been systematically dissected or linked to underlying preference structures. We provide the first comprehensive investigation of positional biases across multiple LLM architectures and domains, uncovering strong and consistent order effects, including a novel centrality bias not previously documented in human or machine decision-making. We also find a quality-dependent shift: when options are high quality, models exhibit primacy bias, but favor latter options when option quality is low. We further identify a previously undocumented bias favoring certain names over others. To distinguish superficial tie-breaking from true distortions of judgment, we introduce a framework that classifies pairwise preferences as robust, fragile, or indifferent. We show that order effects can lead models to select strictly inferior options, and that positional biases are typically stronger than gender biases. These findings suggest that LLMs are not merely inheriting human-like biases, but exhibit distinct failure modes not seen in human decision-making. We propose targeted mitigation strategies, including a novel use of the temperature parameter, to reduce order-driven distortions.

</details>


### [170] [Situational-Constrained Sequential Resources Allocation via Reinforcement Learning](https://arxiv.org/abs/2506.14125)
*Libo Zhang,Yang Chen,Toru Takisaka,Kaiqi Zhao,Weidong Li,Jiamou Liu*

Key words: 资源分配,情境约束,约束强化学习,动态惩罚,概率选择机制

TL;DR: 本文提出了一种名为SCRL的新框架，用于解决具有情境约束的顺序资源分配问题，通过动态惩罚约束违规和新颖的算法设计，提高了资源分配的效率和约束满足率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现实应用中的资源分配问题常受情境约束影响，传统方法难以动态适应不同场景的需求，因此需要一种能够灵活应对约束的解决方案。

Method: 论文将情境约束形式化为逻辑隐含，开发了一种动态惩罚约束违规的算法，并提出概率选择机制以克服传统约束强化学习的限制。

Result: 在医疗资源分配和农药分配两个场景中，SCRL在满足约束的同时保持了高资源效率，表现优于现有基线方法。

Conclusion: SCRL展示了在现实世界中处理情境约束下资源分配问题的潜力，能够为上下文敏感的决策任务提供有效支持。

Abstract: Sequential Resource Allocation with situational constraints presents a significant challenge in real-world applications, where resource demands and priorities are context-dependent. This paper introduces a novel framework, SCRL, to address this problem. We formalize situational constraints as logic implications and develop a new algorithm that dynamically penalizes constraint violations. To handle situational constraints effectively, we propose a probabilistic selection mechanism to overcome limitations of traditional constraint reinforcement learning (CRL) approaches. We evaluate SCRL across two scenarios: medical resource allocation during a pandemic and pesticide distribution in agriculture. Experiments demonstrate that SCRL outperforms existing baselines in satisfying constraints while maintaining high resource efficiency, showcasing its potential for real-world, context-sensitive decision-making tasks.

</details>


### [171] [Collaborative Editable Model](https://arxiv.org/abs/2506.14146)
*Kaiwen Tang,Aitong Wu,Yao Lu,Guangda Sun*

Key words: 垂直领域大模型,轻量级适配,知识注入,用户反馈

TL;DR: CoEM通过用户贡献的知识片段和交互式编辑，实现轻量级领域适配，显著提升生成内容的质量，避免了传统微调的高成本。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决垂直领域大模型训练依赖大规模标注数据和计算资源的问题，促进快速开发和迭代。

Method: 构建用户贡献的知识池，通过用户-模型对话和高价值知识片段注入，实现轻量级领域适配。

Result: 在金融信息场景中，生成内容的领域特异性显著提升，用户反馈验证了其有效性。

Conclusion: CoEM是一种高效、低成本的垂直领域LLM适配方法，适用于快速迭代和高精度生成需求。

Abstract: Vertical-domain large language models (LLMs) play a crucial role in specialized scenarios such as finance, healthcare, and law; however, their training often relies on large-scale annotated data and substantial computational resources, impeding rapid development and continuous iteration. To address these challenges, we introduce the Collaborative Editable Model (CoEM), which constructs a candidate knowledge pool from user-contributed domain snippets, leverages interactive user-model dialogues combined with user ratings and attribution analysis to pinpoint high-value knowledge fragments, and injects these fragments via in-context prompts for lightweight domain adaptation. With high-value knowledge, the LLM can generate more accurate and domain-specific content. In a financial information scenario, we collect 15k feedback from about 120 users and validate CoEM with user ratings to assess the quality of generated insights, demonstrating significant improvements in domain-specific generation while avoiding the time and compute overhead of traditional fine-tuning workflows.

</details>


### [172] [What's in the Box? Reasoning about Unseen Objects from Multimodal Cues](https://arxiv.org/abs/2506.14212)
*Lance Ying,Daniel Xu,Alicia Zhang,Katherine M. Collins,Max H. Siegel,Joshua B. Tenenbaum*

Key words: 神经符号模型,多模态整合,贝叶斯推理,物体推断

TL;DR: 论文提出了一种神经符号模型，结合神经网络和贝叶斯模型，通过多模态输入灵活推断隐藏物体，并在“盒中何物”游戏中表现优于单模态模型。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究人类如何灵活整合多种信息源以推断看不见的物体。

Method: 使用神经网络解析多模态输入，然后通过贝叶斯模型整合信息以评估假设。

Result: 模型在“盒中何物”游戏中与人类判断高度相关，优于单模态和大规模多模态神经模型。

Conclusion: 神经符号模型能有效模拟人类的多模态信息整合能力。

Abstract: People regularly make inferences about objects in the world that they cannot see by flexibly integrating information from multiple sources: auditory and visual cues, language, and our prior beliefs and knowledge about the scene. How are we able to so flexibly integrate many sources of information to make sense of the world around us, even if we have no direct knowledge? In this work, we propose a neurosymbolic model that uses neural networks to parse open-ended multimodal inputs and then applies a Bayesian model to integrate different sources of information to evaluate different hypotheses. We evaluate our model with a novel object guessing game called ``What's in the Box?'' where humans and models watch a video clip of an experimenter shaking boxes and then try to guess the objects inside the boxes. Through a human experiment, we show that our model correlates strongly with human judgments, whereas unimodal ablated models and large multimodal neural model baselines show poor correlation.

</details>


### [173] [From Black Boxes to Transparent Minds: Evaluating and Enhancing the Theory of Mind in Multimodal Large Language Models](https://arxiv.org/abs/2506.14224)
*Xinyang Li,Siqi Liu,Bochao Zou,Jiansheng Chen,Huimin Ma*

Key words: 心智理论, 多模态大语言模型, 注意力机制, 可解释性评估

TL;DR: 本文提出了一种基于内部机制的多模态大语言模型（MLLMs）心智理论（ToM）评估方法，构建了GridToM测试数据集，并利用注意力头分析模型的ToM能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的大语言模型心智理论（ToM）评估方法主要关注单模态模型且缺乏可解释性，因此本研究旨在通过内部机制分析填补这一空白。

Method: 构建了GridToM测试数据集，利用注意力头分析模型的认知信息区分能力，并提出一种轻量级、无需训练的方法优化模型ToM表现。

Result: 研究发现注意力头能有效区分多视角认知信息，证明了模型的ToM能力，并通过调整注意力头方向显著提升了ToM表现。

Conclusion: 通过内部机制分析，本研究为多模态大语言模型的ToM评估提供了可解释性方法，并展示了优化潜力。

Abstract: As large language models evolve, there is growing anticipation that they will emulate human-like Theory of Mind (ToM) to assist with routine tasks. However, existing methods for evaluating machine ToM focus primarily on unimodal models and largely treat these models as black boxes, lacking an interpretative exploration of their internal mechanisms. In response, this study adopts an approach based on internal mechanisms to provide an interpretability-driven assessment of ToM in multimodal large language models (MLLMs). Specifically, we first construct a multimodal ToM test dataset, GridToM, which incorporates diverse belief testing tasks and perceptual information from multiple perspectives. Next, our analysis shows that attention heads in multimodal large models can distinguish cognitive information across perspectives, providing evidence of ToM capabilities. Furthermore, we present a lightweight, training-free approach that significantly enhances the model's exhibited ToM by adjusting in the direction of the attention head.

</details>


### [174] [ImpReSS: Implicit Recommender System for Support Conversations](https://arxiv.org/abs/2506.14231)
*Omri Haller,Yair Meidan,Dudu Mimran,Yuval Elovici,Asaf Shabtai*

Key words: 大型语言模型, 对话推荐系统, 客户支持, 隐式推荐, ImpReSS

TL;DR: ImpReSS是一种隐式推荐系统，用于在客户支持对话中推荐相关解决方案产品类别（SPCs），以解决问题或预防问题复发。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 针对目前LLM-based对话推荐系统（CRSs）在客户支持交互中隐式集成的不足，本研究旨在填补这一研究空白。

Method: ImpReSS与现有支持聊天机器人协作，通过分析客户支持对话，识别并推荐相关SPCs，且不依赖于用户的购买意图假设。

Result: 实验结果表明，ImpReSS在推荐相关SPCs方面表现良好，如在一般问题解决的MRR@1为0.72，召回@3为0.89。

Conclusion: ImpReSS展示了在客户支持对话中隐式推荐SPCs的潜力，为业务增长提供了支持。

Abstract: Following recent advancements in large language models (LLMs), LLM-based chatbots have transformed customer support by automating interactions and providing consistent, scalable service. While LLM-based conversational recommender systems (CRSs) have attracted attention for their ability to enhance the quality of recommendations, limited research has addressed the implicit integration of recommendations within customer support interactions. In this work, we introduce ImpReSS, an implicit recommender system designed for customer support conversations. ImpReSS operates alongside existing support chatbots, where users report issues and chatbots provide solutions. Based on a customer support conversation, ImpReSS identifies opportunities to recommend relevant solution product categories (SPCs) that help resolve the issue or prevent its recurrence -- thereby also supporting business growth. Unlike traditional CRSs, ImpReSS functions entirely implicitly and does not rely on any assumption of a user's purchasing intent. Our empirical evaluation of ImpReSS's ability to recommend relevant SPCs that can help address issues raised in support conversations shows promising results, including an MRR@1 (and recall@3) of 0.72 (0.89) for general problem solving, 0.82 (0.83) for information security support, and 0.85 (0.67) for cybersecurity troubleshooting. To support future research, our data and code will be shared upon request.

</details>


### [175] [Causes in neuron diagrams, and testing causal reasoning in Large Language Models. A glimpse of the future of philosophy?](https://arxiv.org/abs/2506.14239)
*Louis Vervoort,Vitaly Nikolaev*

Key words: 因果推理，AI测试，神经元图，大型语言模型，哲学

TL;DR: 提出基于哲学因果关系研究的测试，评估AI（如ChatGPT等）在抽象因果推理中的表现，结果表明这些模型能正确识别复杂因果关系。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索AI在抽象因果推理中的能力，并推动哲学研究与人工智能的结合。

Method: 基于D. Lewis的神经元图设计测试，评估大型语言模型在处理因果关系时的表现。

Result: ChatGPT、DeepSeek和Gemini等模型能正确识别复杂因果关系，且提出更普适的因果定义。

Conclusion: AI在复杂因果关系分析中表现优异，未来哲学研究可能需结合人类与人工智能的专长。

Abstract: We propose a test for abstract causal reasoning in AI, based on scholarship in the philosophy of causation, in particular on the neuron diagrams popularized by D. Lewis. We illustrate the test on advanced Large Language Models (ChatGPT, DeepSeek and Gemini). Remarkably, these chatbots are already capable of correctly identifying causes in cases that are hotly debated in the literature. In order to assess the results of these LLMs and future dedicated AI, we propose a definition of cause in neuron diagrams with a wider validity than published hitherto, which challenges the widespread view that such a definition is elusive. We submit that these results are an illustration of how future philosophical research might evolve: as an interplay between human and artificial expertise.

</details>


### [176] [Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs](https://arxiv.org/abs/2506.14245)
*Xumeng Wen,Zihan Liu,Shun Zheng,Zhijian Xu,Shengyu Ye,Zhirong Wu,Xiao Liang,Yang Wang,Junjie Li,Ziming Miao,Jiang Bian,Mao Yang*

Key words: RLVR, LLMs, reasoning diversity, CoT-Pass@K, machine reasoning

TL;DR: RLVR通过新指标CoT-Pass@K证明其在促进LLM逻辑一致性上的有效性，解决了原Pass@K指标的问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决RLVR在Pass@K指标下表现不佳的矛盾，揭示原指标的缺陷并引入更精确的评估方法。

Method: 提出CoT-Pass@K指标，要求推理路径和答案均正确，并从理论上分析RLVR的独特优势。

Result: RLVR在CoT-Pass@K下表现优异，能早期促进正确推理的泛化。

Conclusion: RLVR通过新指标验证了其推动机器推理的潜力，提供了更可靠的评估方法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the $Pass@K$ metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the $Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, $CoT$-$Pass@K$, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we observe that RLVR can incentivize the generalization of correct reasoning for all values of $K$. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.

</details>


### [177] [Mxplainer: Explain and Learn Insights by Imitating Mahjong Agents](https://arxiv.org/abs/2506.14246)
*Lingfeng Li,Yunlong Lu,Yongyi Wang,Qifan Zheng,Wenxin Li*

Key words: Mahjong, AI代理, 黑盒解释, 参数学习, 决策过程

TL;DR: 论文介绍了Mxplainer算法，用于揭示麻将AI黑盒代理的学习参数和决策过程，提供人类可理解的洞察。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 通过内部化AI代理的技能来提升人类能力，特别是在麻将这类不完美信息和长期决策的游戏中。

Method: 提出Mxplainer算法，参数化搜索并转化为等效神经网络，学习黑盒代理的参数。

Result: 实验证明，学到的参数能提供人类可理解的代理特性和决策风格，并能局部解释黑盒代理的决策过程。

Conclusion: Mxplainer为揭示黑盒AI代理的行为提供了有效工具，有助于人类学习和理解。

Abstract: People need to internalize the skills of AI agents to improve their own capabilities. Our paper focuses on Mahjong, a multiplayer game involving imperfect information and requiring effective long-term decision-making amidst randomness and hidden information. Through the efforts of AI researchers, several impressive Mahjong AI agents have already achieved performance levels comparable to those of professional human players; however, these agents are often treated as black boxes from which few insights can be gleaned. This paper introduces Mxplainer, a parameterized search algorithm that can be converted into an equivalent neural network to learn the parameters of black-box agents. Experiments conducted on AI and human player data demonstrate that the learned parameters provide human-understandable insights into these agents' characteristics and play styles. In addition to analyzing the learned parameters, we also showcase how our search-based framework can locally explain the decision-making processes of black-box agents for most Mahjong game states.

</details>


### [178] [Don't throw the baby out with the bathwater: How and why deep learning for ARC](https://arxiv.org/abs/2506.14276)
*Jack Cole,Mohamed Osman*

Key words: ARC-AGI、深度学习、Test-Time Fine-Tuning、AIRV、抽象推理

TL;DR: 论文通过动态神经网络训练和优化器集成，结合Test-Time Fine-Tuning和AIRV技术，在ARC-AGI竞赛中取得最佳成绩，验证了深度学习在抽象推理任务中的有效性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 挑战ARC-AGI任务，展示深度学习在抽象推理中的潜力。

Method: 动态训练神经网络和优化器，提出Test-Time Fine-Tuning和AIRV技术。

Result: 准确率提升260%（AIRV）和300%（TTFT），ARC竞赛中第一名（58%）。

Conclusion: 深度学习在抽象推理任务中表现优异，为陌生领域推理系统提供关键机制。

Abstract: The Abstraction and Reasoning Corpus (ARC-AGI) presents a formidable challenge for AI systems. Despite the typically low performance on ARC, the deep learning paradigm remains the most effective known strategy for generating skillful (state-of-the-art) neural networks (NN) across varied modalities and tasks in vision, language etc. The deep learning paradigm has proven to be able to train these skillful neural networks and learn the abstractions needed in these diverse domains. Our work doubles down on that and continues to leverage this paradigm by incorporating on-the-fly NN training at test time. We demonstrate that fully committing to deep learning's capacity to acquire novel abstractions yields state-of-the-art performance on ARC. Specifically, we treat both the neural network and the optimizer (rather than just a pre-trained network) as integral components of the inference process, fostering generalization to unseen tasks. Concretely, we propose a methodology for training on ARC, starting from pretrained LLMs, and enhancing their ARC reasoning. We also propose Test-Time Fine-Tuning (TTFT) and the Augment Inference Reverse-Augmentation and Vote (AIRV) as effective test-time techniques. We are the first to propose and show deep learning can be used effectively for ARC, showing boosts of up to 260% in accuracy with AIRV and a further 300% boost with TTFT. An early version of this approach secured first place in the 2023 ARCathon competition, while the final version achieved the current best score on the ARC private test-set (58%). Our findings highlight the key ingredients of a robust reasoning system in unfamiliar domains, underscoring the central mechanisms that improve broad perceptual reasoning.

</details>


### [179] [ADRD: LLM-Driven Autonomous Driving Based on Rule-based Decision Systems](https://arxiv.org/abs/2506.14299)
*Fanzhi Zeng,Siqi Wang,Chuzhao Zhu,Li Li*

Key words: 自动驾驶,大语言模型,规则驱动决策,可解释性,ADRD

TL;DR: 本研究提出了一种基于大语言模型（LLMs）的自动驾驶决策系统ADRD，通过三个核心模块生成可执行的规则驱动决策，显著优于传统强化学习和现有LLM方法的解释性、响应速度和驾驶性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 构建可解释的自动驾驶决策系统是学术研究的热点，本研究旨在利用LLMs的强大推理和编程能力解决这一挑战。

Method: ADRD框架包含信息模块、代理模块和测试模块，通过信息整合、生成规则驱动策略和迭代优化来实现决策生成。

Result: 实验表明，ADRD在解释性、响应速度和驾驶性能上显著优于传统方法，验证了其在实际部署中的潜力。

Conclusion: ADRD是首个将LLMs与规则系统结合的自动驾驶决策框架，展示了透明、可修改且广泛适用的前景。

Abstract: How to construct an interpretable autonomous driving decision-making system has become a focal point in academic research. In this study, we propose a novel approach that leverages large language models (LLMs) to generate executable, rule-based decision systems to address this challenge. Specifically, harnessing the strong reasoning and programming capabilities of LLMs, we introduce the ADRD(LLM-Driven Autonomous Driving Based on Rule-based Decision Systems) framework, which integrates three core modules: the Information Module, the Agents Module, and the Testing Module. The framework operates by first aggregating contextual driving scenario information through the Information Module, then utilizing the Agents Module to generate rule-based driving tactics. These tactics are iteratively refined through continuous interaction with the Testing Module. Extensive experimental evaluations demonstrate that ADRD exhibits superior performance in autonomous driving decision tasks. Compared to traditional reinforcement learning approaches and the most advanced LLM-based methods, ADRD shows significant advantages in terms of interpretability, response speed, and driving performance. These results highlight the framework's ability to achieve comprehensive and accurate understanding of complex driving scenarios, and underscore the promising future of transparent, rule-based decision systems that are easily modifiable and broadly applicable. To the best of our knowledge, this is the first work that integrates large language models with rule-based systems for autonomous driving decision-making, and our findings validate its potential for real-world deployment.

</details>


### [180] [AviationLLM: An LLM-based Knowledge System for Aviation Training](https://arxiv.org/abs/2506.14336)
*Jia'ang Wan,Feng Shen,Fujuan Li,Yanjin Sun,Yan Li,Shiwen Zhang*

Key words: 航空培训, 大语言模型, 直接偏好优化, 检索增强生成

TL;DR: 论文提出RALA-DPO方法，通过直接偏好优化（DPO）和检索增强生成技术（RAG），结合开源大语言模型Qwen，提升航空理论培训中的答案准确性，并实现零成本知识更新。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决现有航空培训系统中，因教练数量有限和专业网络答案不准导致的培训效率低下问题。

Method: 采用DPO对开源LLM Qwen进行领域对齐，并结合RAG技术，以检索外部知识库并生成精确答案。

Result: 实验表明，RALA-DPO能显著提高航空专业知识的回答准确性，并通过RAG进一步降低幻觉问题。

Conclusion: RALA-DPO为航空培训提供了高效、准确的解决方案，且能低成本更新知识。

Abstract: Aviation training is a core link in ensuring flight safety, improving industry efficiency and promoting sustainable development. It not only involves flight simulation but also requires the learning of a great deal of professional aviation theory knowledge. In the existing training system, the knowledge is mainly imparted by the the instructors. However, the number of instructors is limited and the professional answers obtained from the Internet are not accurate enough, resulting in low training efficiency. To address this, we introduced LLM, but the basic pre-trained model cannot provide accurate answers to professional fields, so we fine-tuned it. Traditional Supervised Fine-Tuning (SFT) risk generating superficially plausible but factually incorrect responses due to insufficient data coverage. To address this, we employ Direct Preference Optimization(DPO). This paper proposes Retrieval-Augmented LLM Alignment via Direct Preference Optimization(RALA-DPO). We select open source pre-trained LLM Qwen and adapt it to aviation theory training through DPO-based domain alignment. Simultaneously, to mitigate hallucinations caused by training data biases, knowledge obsolescence, or domain knowledge gaps, we implement Retrieval-Augmented Generation(RAG) technology that combines generative and retrieval models. RALA-DPO effectively retrieves relevant information from external knowledge bases and delivers precise and high-quality responses through the generative model. Experimental results demonstrate that RALA-DPO can improve accuracy in response to professional aviation knowledge. With integrated RAG mechanisms, this system can further improve the accuracy of answers and achieve zero-cost knowledge updates simultaneously.

</details>


### [181] [Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning](https://arxiv.org/abs/2506.14387)
*William F. Shen,Xinchi Qiu,Nicola Cancedda,Nicholas D. Lane*

Key words: LLM微调、灾难性遗忘、安全对齐、SEAT、KL散度

TL;DR: 该论文提出了SEAT方法，解决LLM在微调过程中安全对齐能力的退化问题，特别是模型表达无知的能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有研究主要关注保护特定数据或任务，但忽视了安全对齐能力的退化，特别是模型表达无知的能力，这导致幻觉等不良行为。

Method: 提出SEAT方法，包含稀疏训练以限制激活漂移，以及基于KL散度正则化的实体扰动方法，以解决知识纠缠问题。

Result: SEAT在保留无知意识和微调性能方面显著优于基线方法。

Conclusion: SEAT为LLM微调提供了更鲁棒的解决方案，兼顾性能和安全对齐能力。

Abstract: Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations. To address this novel but highly practical problem, we propose SEAT, a simple and effective fine-tuning approach that preserves both fine-tuning performance and the model's inherent ability to acknowledge its ignorance. SEAT integrates two key components: (1) sparse training that constrains activation drift, and (2) a novel entity perturbation method with KL-divergence regularization, designed to counter knowledge entanglement. Experimental results demonstrate that SEAT significantly outperforms baselines in preserving ignorance awareness while retaining fine-tuning performance, offering a more robust solution for LLM fine-tuning.

</details>


### [182] [AST-Enhanced or AST-Overloaded? The Surprising Impact of Hybrid Graph Representations on Code Clone Detection](https://arxiv.org/abs/2506.14470)
*Zixian Zhang,Takfarinas Saber*

Key words: 代码克隆检测, 抽象语法树, 图神经网络, 控制流图, 数据流图

TL;DR: 该论文通过实证研究评估了基于抽象语法树（AST）的混合图表示在图神经网络（GNN）中检测代码克隆的有效性，发现AST+CFG+DFG组合能提升性能，而FA-AST可能因结构复杂拖累模型。GMN在标准AST上表现最优。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 代码克隆显著增加软件维护成本和安全风险，现有基于AST的方法缺乏语义深度，需研究如何通过混合图表示（如CFG、DFG）提升检测效果。

Method: 采用多种混合图表示（AST+CFG+DFG、FA-AST）与不同GNN架构（GCN、GAT、GMN）结合，进行系统性对比实验。

Result: AST+CFG+DFG提升卷积和注意力模型性能，FA-AST引入的结构复杂性反而降低效果；GMN在标准AST上表现最佳。

Conclusion: 混合图表示的效果因GNN架构而异，GMN在代码相似性检测中更高效，减少了对复杂结构的依赖。

Abstract: As one of the most detrimental code smells, code clones significantly increase software maintenance costs and heighten vulnerability risks, making their detection a critical challenge in software engineering. Abstract Syntax Trees (ASTs) dominate deep learning-based code clone detection due to their precise syntactic structure representation, but they inherently lack semantic depth. Recent studies address this by enriching AST-based representations with semantic graphs, such as Control Flow Graphs (CFGs) and Data Flow Graphs (DFGs). However, the effectiveness of various enriched AST-based representations and their compatibility with different graph-based machine learning techniques remains an open question, warranting further investigation to unlock their full potential in addressing the complexities of code clone detection. In this paper, we present a comprehensive empirical study to rigorously evaluate the effectiveness of AST-based hybrid graph representations in Graph Neural Network (GNN)-based code clone detection. We systematically compare various hybrid representations ((CFG, DFG, Flow-Augmented ASTs (FA-AST)) across multiple GNN architectures. Our experiments reveal that hybrid representations impact GNNs differently: while AST+CFG+DFG consistently enhances accuracy for convolution- and attention-based models (Graph Convolutional Networks (GCN), Graph Attention Networks (GAT)), FA-AST frequently introduces structural complexity that harms performance. Notably, GMN outperforms others even with standard AST representations, highlighting its superior cross-code similarity detection and reducing the need for enriched structures.

</details>


### [183] [GUI-Robust: A Comprehensive Dataset for Testing GUI Agent Robustness in Real-World Anomalies](https://arxiv.org/abs/2506.14477)
*Jingqi Yang,Zhilong Song,Jiawei Chen,Mingli Song,Sheng Zhou,linjun sun,Xiaogang Ouyang,Chun Chen,Can Wang*

Key words: GUI代理, 数据集, 鲁棒性, RPA, MLLMs

TL;DR: 本文介绍了GUI-Robust数据集，用于全面评估GUI代理的鲁棒性，并提出了半自动构建方法，显著降低标注成本。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有GUI数据集多基于理想化条件，忽视了现实中的多样异常，需要更全面的评估工具。

Method: 通过RPA工具收集自然交互数据，并利用MLLMs生成动作描述，实现半自动化标注。

Result: 实验显示现有GUI代理在异常场景下性能显著下降，凸显鲁棒性问题。

Conclusion: 该研究强调了GUI代理鲁棒性的重要性，数据集和方法为未来研究提供了支持。

Abstract: The development of high-quality datasets is crucial for benchmarking and advancing research in Graphical User Interface (GUI) agents. Despite their importance, existing datasets are often constructed under idealized conditions, overlooking the diverse anomalies frequently encountered in real-world deployments. To address this limitation, we introduce GUI-Robust, a novel dataset designed for comprehensive GUI agent evaluation, explicitly incorporating seven common types of anomalies observed in everyday GUI interactions. Furthermore, we propose a semi-automated dataset construction paradigm that collects user action sequences from natural interactions via RPA tools and then generate corresponding step and task descriptions for these actions with the assistance of MLLMs. This paradigm significantly reduces annotation time cost by a factor of over 19 times. Finally, we assess state-of-the-art GUI agents using the GUI-Robust dataset, revealing their substantial performance degradation in abnormal scenarios. We anticipate that our work will highlight the importance of robustness in GUI agents and inspires more future research in this direction. The dataset and code are available at https://github.com/chessbean1/GUI-Robust..

</details>


### [184] [LLM-Powered Swarms: A New Frontier or a Conceptual Stretch?](https://arxiv.org/abs/2506.14496)
*Muhammad Atta Ur Rahman,Melanie Schranz*

Key words: 群体智能, 大型语言模型, 去中心化, 涌现性, 蚁群优化

TL;DR: 对比传统群体智能与基于LLM的群体智能，分析其在去中心化、可扩展性和涌现性上的差异，并评估两者在延迟、资源使用和行为准确性上的表现。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索大型语言模型（LLM）作为协作代理在群体智能系统中的适用性，重新定义现代AI中的群体概念。

Method: 实现并比较传统群体算法（如Boids和蚁群优化）与LLM驱动的群体系统，评估其在延迟、资源使用和行为准确性上的表现。

Result: LLM提供强大的推理和抽象能力，但也带来计算和协调的新约束，挑战传统群体设计。

Conclusion: LLM为群体系统带来新机遇但亦有限制，研究探讨了现代AI中群体定义的演变。

Abstract: Swarm intelligence traditionally refers to systems of simple, decentralized agents whose local interactions lead to emergent, collective behavior. Recently, the term 'swarm' has been extended to describe AI systems like OpenAI's Swarm, where large language models (LLMs) act as collaborative agents. This paper contrasts traditional swarm algorithms with LLM-driven swarms exploring how decentralization, scalability, and emergence are redefined in modern artificial intelligence (AI). We implement and compare both paradigms using Boids and Ant Colony Optimization (ACO), evaluating latency, resource usage, and behavioral accuracy. The suitability of both cloud-based and local LLMs is assessed for the agent-based use in swarms. Although LLMs offer powerful reasoning and abstraction capabilities, they introduce new constraints in computation and coordination that challenge traditional notions of swarm design. This study highlights the opportunities and limitations of integrating LLMs into swarm systems and discusses the evolving definition of 'swarm' in modern AI research.

</details>


### [185] [Toward Safety-First Human-Like Decision Making for Autonomous Vehicles in Time-Varying Traffic Flow](https://arxiv.org/abs/2506.14502)
*Xiao Wang,Junru Yu,Jun Huang,Qiong Wu,Ljubo Vacic,Changyin Sun*

Key words: 自动驾驶、类人决策、强化学习、社会兼容性

TL;DR: 本文提出了一种安全至上的类人决策框架（SF-HLDM），用于自动驾驶车辆在多变交通流中安全、舒适且社会兼容地行驶。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 自动驾驶车辆在密集和交互性强的交通环境中面临挑战，数据驱动方法因人类行为多样性而效率低下。

Method: 采用分层渐进框架，结合时空注意力机制、社会兼容性评估模块和深度进化强化学习模型。

Result: SF-HLDM能动态调整决策参数，平衡安全性与驾驶行为的社会兼容性。

Conclusion: 该框架提升了自动驾驶决策的灵活性和可解释性，适应复杂交通环境。

Abstract: Despite the recent advancements in artificial intelligence technologies have shown great potential in improving transport efficiency and safety, autonomous vehicles(AVs) still face great challenge of driving in time-varying traffic flow, especially in dense and interactive situations. Meanwhile, human have free wills and usually do not make the same decisions even situate in the exactly same scenarios, leading to the data-driven methods suffer from poor migratability and high search cost problems, decreasing the efficiency and effectiveness of the behavior policy. In this research, we propose a safety-first human-like decision-making framework(SF-HLDM) for AVs to drive safely, comfortably, and social compatiblely in effiency. The framework integrates a hierarchical progressive framework, which combines a spatial-temporal attention (S-TA) mechanism for other road users' intention inference, a social compliance estimation module for behavior regulation, and a Deep Evolutionary Reinforcement Learning(DERL) model for expanding the search space efficiently and effectively to make avoidance of falling into the local optimal trap and reduce the risk of overfitting, thus make human-like decisions with interpretability and flexibility. The SF-HLDM framework enables autonomous driving AI agents dynamically adjusts decision parameters to maintain safety margins and adhering to contextually appropriate driving behaviors at the same time.

</details>


### [186] [Doppelgänger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack](https://arxiv.org/abs/2506.14539)
*Daewon Kang,YeongHwan Shin,Doyeon Kim,Kyu-Hwan Jung,Meong Hi Son*

Key words: 大型语言模型, 提示工程, 安全性, 替身方法, PACAT, CAT

TL;DR: 论文提出了一种名为'替身方法'的攻击手段，用于展示语言模型代理被劫持的风险，同时定义了'PACAT'评估指标，并提出了防御性提示'CAT'以应对此类攻击。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着大型语言模型的普及，提示工程使得快速、低成本地创建多样化的自主代理成为可能。然而，这种便利性引发了对其安全性、鲁棒性和行为一致性的担忧，尤其是防止提示被用户攻击的挑战。

Method: 作者提出了'替身方法'来演示代理被劫持的风险，定义了'PACAT'指标评估攻击的脆弱性，并设计了'CAT'提示作为防御手段。

Result: 实验表明，替身方法确实能够破坏代理的一致性并泄露其内部信息，而CAT提示则能有效防御此类攻击。

Conclusion: 研究揭示了提示工程在安全上的潜在风险，并通过防御机制为未来的安全性改进提供了方向。

Abstract: Since the advent of large language models, prompt engineering now enables the rapid, low-effort creation of diverse autonomous agents that are already in widespread use. Yet this convenience raises urgent concerns about the safety, robustness, and behavioral consistency of the underlying prompts, along with the pressing challenge of preventing those prompts from being exposed to user's attempts. In this paper, we propose the ''Doppelgänger method'' to demonstrate the risk of an agent being hijacked, thereby exposing system instructions and internal information. Next, we define the ''Prompt Alignment Collapse under Adversarial Transfer (PACAT)'' level to evaluate the vulnerability to this adversarial transfer attack. We also propose a ''Caution for Adversarial Transfer (CAT)'' prompt to counter the Doppelgänger method. The experimental results demonstrate that the Doppelgänger method can compromise the agent's consistency and expose its internal information. In contrast, CAT prompts enable effective defense against this adversarial attack.

</details>


### [187] [QUEST: Quality-aware Semi-supervised Table Extraction for Business Documents](https://arxiv.org/abs/2506.14568)
*Eliott Thomas,Mickael Coustaty,Aurelie Joseph,Gaspar Deloin,Elodie Carel,Vincent Poulain D'Andecy,Jean-Marc Ogier*

Key words: 表格提取、半监督学习、质量评估、商业文档、多样性度量

TL;DR: QUEST是一个质量感知的半监督表格提取框架，通过评估提取表格的结构和上下文特征改进伪标签选择，显著提升了表格提取的性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 商业文档中的表格提取自动化对于工业流程至关重要，但由于稀疏标注和错误的多阶段流程，依然具有挑战性。现有的半监督方法依赖置信度评分，而置信度评分无法准确反映提取质量。

Method: QUEST提出了一种新颖的质量评估模型，通过预测F1分数而非依赖置信度指标来评估表格的结构和上下文特征，并结合多样性度量（如DPP、Vendi分数、IntDiv）减轻确认偏差。

Result: 在专有商业数据集上，QUEST将F1分数从64%提升至74%，并将空预测减少45%（从12%降至6.5%）。在DocILE基准测试中，F1分数从42%提升至50%，空预测减少19%（从27%降至22%）。

Conclusion: QUEST的可解释质量评估和对标注稀缺的鲁棒性使其特别适合商业文档，其中结构一致性和数据完整性至关重要。

Abstract: Automating table extraction (TE) from business documents is critical for industrial workflows but remains challenging due to sparse annotations and error-prone multi-stage pipelines. While semi-supervised learning (SSL) can leverage unlabeled data, existing methods rely on confidence scores that poorly reflect extraction quality. We propose QUEST, a Quality-aware Semi-supervised Table extraction framework designed for business documents. QUEST introduces a novel quality assessment model that evaluates structural and contextual features of extracted tables, trained to predict F1 scores instead of relying on confidence metrics. This quality-aware approach guides pseudo-label selection during iterative SSL training, while diversity measures (DPP, Vendi score, IntDiv) mitigate confirmation bias. Experiments on a proprietary business dataset (1000 annotated + 10000 unannotated documents) show QUEST improves F1 from 64% to 74% and reduces empty predictions by 45% (from 12% to 6.5%). On the DocILE benchmark (600 annotated + 20000 unannotated documents), QUEST achieves a 50% F1 score (up from 42%) and reduces empty predictions by 19% (from 27% to 22%). The framework's interpretable quality assessments and robustness to annotation scarcity make it particularly suited for business documents, where structural consistency and data completeness are paramount.

</details>


### [188] [Enhancing Symbolic Machine Learning by Subsymbolic Representations](https://arxiv.org/abs/2506.14569)
*Stephen Roth,Lennart Baur,Derian Boer,Stefan Kramer*

Key words: 神经符号AI, 符号机器学习, 神经嵌入, TILDE, F1分数

TL;DR: 研究者提出了一种通过神经嵌入增强符号机器学习的方法，并在TILDE中实现了这一方法，实验显示其效果优于其他基线。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前的神经符号AI系统（如LTN和DeepProbLog）在简单场景下效率较低，尤其是在涉及大量常量的情况下。因此，研究者希望增强符号机器学习的能力。

Method: 通过在符号机器学习方案中使用神经嵌入（如TILDE中的常量嵌入），并通过符号理论进一步优化嵌入。

Result: 在三个实际领域中的实验表明，该方法在F1分数上优于其他基线方法。

Conclusion: 该方法不仅适用于当前场景，还可扩展到实例之间的相似性（如逻辑语言中的核函数）、类比推理和命题化等应用。

Abstract: The goal of neuro-symbolic AI is to integrate symbolic and subsymbolic AI approaches, to overcome the limitations of either. Prominent systems include Logic Tensor Networks (LTN) or DeepProbLog, which offer neural predicates and end-to-end learning. The versatility of systems like LTNs and DeepProbLog, however, makes them less efficient in simpler settings, for instance, for discriminative machine learning, in particular in domains with many constants. Therefore, we follow a different approach: We propose to enhance symbolic machine learning schemes by giving them access to neural embeddings. In the present paper, we show this for TILDE and embeddings of constants used by TILDE in similarity predicates. The approach can be fine-tuned by further refining the embeddings depending on the symbolic theory. In experiments in three real-world domain, we show that this simple, yet effective, approach outperforms all other baseline methods in terms of the F1 score. The approach could be useful beyond this setting: Enhancing symbolic learners in this way could be extended to similarities between instances (effectively working like kernels within a logical language), for analogical reasoning, or for propositionalization.

</details>


### [189] [From Points to Places: Towards Human Mobility-Driven Spatiotemporal Foundation Models via Understanding Places](https://arxiv.org/abs/2506.14570)
*Mohammad Hashemi,Andreas Zufle*

Key words: 人类流动性, 空间基础模型, 地理语义, 时空数据, 多尺度分析

TL;DR: 提出一种通用基础模型，用于处理时空数据的空间基础模型，以解决移动数据的复杂性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究需要通用且可扩展的时空数据模型，以更好地捕捉人类流动性及其与社会行为的关系。

Method: 从兴趣点转向动态、上下文丰富的区域（“地方”）建模，整合地理语义和人类流动性。

Result: 提出了开发可扩展、上下文感知模型的愿景，支持下一代地理空间智能。

Conclusion: 空间基础模型将为智能决策提供支持，应用包括个性化地点发现和城市规划等。

Abstract: Capturing human mobility is essential for modeling how people interact with and move through physical spaces, reflecting social behavior, access to resources, and dynamic spatial patterns. To support scalable and transferable analysis across diverse geographies and contexts, there is a need for a generalizable foundation model for spatiotemporal data. While foundation models have transformed language and vision, they remain limited in handling the unique challenges posed by the spatial, temporal, and semantic complexity of mobility data. This vision paper advocates for a new class of spatial foundation models that integrate geolocation semantics with human mobility across multiple scales. Central to our vision is a shift from modeling discrete points of interest to understanding places: dynamic, context-rich regions shaped by human behavior and mobility that may comprise many places of interest. We identify key gaps in adaptability, scalability, and multi-granular reasoning, and propose research directions focused on modeling places and enabling efficient learning. Our goal is to guide the development of scalable, context-aware models for next-generation geospatial intelligence. These models unlock powerful applications ranging from personalized place discovery and logistics optimization to urban planning, ultimately enabling smarter and more responsive spatial decision-making.

</details>


### [190] [AgentDistill: Training-Free Agent Distillation with Generalizable MCP Boxes](https://arxiv.org/abs/2506.14728)
*Jiahao Qiu,Xinzhe Juan,Yimin Wang,Ling Yang,Xuan Qi,Tongcheng Zhang,Jiacheng Guo,Yifu Lu,Zixin Yao,Hongru Wang,Shilong Liu,Xun Jiang,Liu Leqi,Mengdi Wang*

Key words: 知识蒸馏, 智能体, MCP, 大型语言模型

TL;DR: 论文提出AgentDistill框架，通过直接重用教师模型生成的MCP模块，实现高效、免训练的智能体蒸馏，使小型模型在新任务中表现接近大型模型。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有智能体蒸馏方法无法有效处理动态规划和新环境任务，需探索更高效的知识转移方式。

Method: 利用MCP（结构化任务解决模块）实现免训练的知识蒸馏，学生智能体直接复用这些模块。

Result: 在生物医学和数学任务上，小型学生智能体性能接近基于GPT-4o的系统，验证了框架的有效性。

Conclusion: AgentDistill为构建高效、可扩展的智能体提供了新方向，降低了计算成本。

Abstract: While knowledge distillation has become a mature field for compressing large language models (LLMs) into smaller ones by aligning their outputs or internal representations, the distillation of LLM-based agents, which involve planning, memory, and tool use, remains relatively underexplored. Existing agent distillation methods typically replay full teacher trajectories or imitate step-by-step teacher tool usage, but they often struggle to train student agents to dynamically plan and act in novel environments. We propose AgentDistill, a novel, training-free agent distillation framework that enables efficient and scalable knowledge transfer via direct reuse of Model-Context-Protocols (MCPs), which are structured and reusable task-solving modules autonomously generated by teacher agents. The reuse of these distilled MCPs enables student agents to generalize their capabilities across domains and solve new problems with minimal supervision or human intervention. Experiments on biomedical and mathematical benchmarks demonstrate that our distilled student agents, built on small language models, can achieve performance comparable to advanced systems using large LLMs such as OctoTools (GPT-4o), highlighting the effectiveness of our framework in building scalable and cost-efficient intelligent agents.

</details>


### [191] [Optimizing Length Compression in Large Reasoning Models](https://arxiv.org/abs/2506.14755)
*Zhengxiang Cheng,Dongping Chen,Mingyang Fu,Tianyi Zhou*

Key words: 大推理模型、无效思考、Brevity、Sufficiency、LC-R1、GRPO

TL;DR: 提出LC-R1方法，通过Brevity和Sufficiency原则优化大推理模型，减少无效推理步骤，显著压缩推理链长度（~50%），仅损失少量准确性（~2%）。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 大推理模型（LRMs）常因冗余的双重检查（无效思考）而产生不必要的推理链，影响效率。

Method: 基于GRPO提出LC-R1后训练方法，结合长度奖励和压缩奖励，消除无效推理步骤。

Result: 在多个推理基准测试中，LC-R1将推理链长度减少约50%，准确性仅降低约2%。

Conclusion: LC-R1在高效压缩和准确性间取得了平衡，为开发更高效的大推理模型提供了新思路。

Abstract: Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as "invalid thinking" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [192] [BraTS orchestrator : Democratizing and Disseminating state-of-the-art brain tumor image analysis](https://arxiv.org/abs/2506.13807)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Ujjwal Baid,Hendrik Möller,Josef A. Buchner,Felix Steinbauer,Eva Oswald,Ezequiel de la Rosa,Ivan Ezhov,Constantin von See,Jan Kirschke,Anton Schmick,Sarthak Pati,Akis Linardos,Carla Pitarch,Sanyukta Adap,Jeffrey Rudie,Maria Correia de Verdier,Rachit Saluja,Evan Calabrese,Dominic LaBella,Mariam Aboian,Ahmed W. Moawad,Nazanin Maleki,Udunna Anazodo,Maruf Adewole,Marius George Linguraru,Anahita Fathi Kazerooni,Zhifan Jiang,Gian Marco Conte,Hongwei Li,Juan Eugenio Iglesias,Spyridon Bakas,Benedikt Wiestler,Marie Piraud,Bjoern Menze*

Key words: BraTS challenge, brain tumor segmentation, deep learning, open-source, Python package

TL;DR: BraTS orchestrator是一个开源Python包，旨在简化BraTS挑战赛中最先进算法的使用，促进其在研究和临床中的普及。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 尽管BraTS挑战赛在脑肿瘤图像分析中取得了成功，但其算法在科学和临床领域的应用有限。为了加速这些算法的传播，有必要开发一个易于使用的工具。

Method: 通过开源Python包BraTS orchestrator，提供直观教程和简化接口，使用户能够轻松部署BraTS的最优算法进行推断。

Result: BraTS orchestrator已在GitHub上发布，为研究人员和临床医生提供了便捷的工具，抽象化了深度学习的复杂性。

Conclusion: BraTS orchestrator成功地将BraTS社区的先进技术普及化，使其更易被神经放射学和神经肿瘤学领域的用户使用。

Abstract: The Brain Tumor Segmentation (BraTS) cluster of challenges has significantly advanced brain tumor image analysis by providing large, curated datasets and addressing clinically relevant tasks. However, despite its success and popularity, algorithms and models developed through BraTS have seen limited adoption in both scientific and clinical communities. To accelerate their dissemination, we introduce BraTS orchestrator, an open-source Python package that provides seamless access to state-of-the-art segmentation and synthesis algorithms for diverse brain tumors from the BraTS challenge ecosystem. Available on GitHub (https://github.com/BrainLesion/BraTS), the package features intuitive tutorials designed for users with minimal programming experience, enabling both researchers and clinicians to easily deploy winning BraTS algorithms for inference. By abstracting the complexities of modern deep learning, BraTS orchestrator democratizes access to the specialized knowledge developed within the BraTS community, making these advances readily available to broader neuro-radiology and neuro-oncology audiences.

</details>


### [193] [Reliable Noninvasive Glucose Sensing via CNN-Based Spectroscopy](https://arxiv.org/abs/2506.13819)
*El Arbi Belfarsi,Henry Flores,Maria Valero*

Key words: 短波红外光谱, 双模态AI, 卷积神经网络, 光电二极管, 无创血糖监测

TL;DR: 提出了一种基于短波红外光谱的双模态AI框架，结合了CNN和光电二极管传感器，用于无创血糖监测。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 开发一种临床准确、成本高效且可穿戴的非侵入性血糖监测系统。

Method: 使用多波长SWIR成像系统和CNN捕捉空间特征，同时利用光电二极管电压传感器和机器学习回归器分析光学信号。

Result: CNN在650 nm波长下的MAPE为4.82%，光电二极管系统在克拉克误差网格中的Zone A准确率达86.4%。

Conclusion: 该框架在临床准确性、成本效益和可穿戴性方面表现出色，为可靠的无创血糖监测提供了新方案。

Abstract: In this study, we present a dual-modal AI framework based on short-wave infrared (SWIR) spectroscopy. The first modality employs a multi-wavelength SWIR imaging system coupled with convolutional neural networks (CNNs) to capture spatial features linked to glucose absorption. The second modality uses a compact photodiode voltage sensor and machine learning regressors (e.g., random forest) on normalized optical signals. Both approaches were evaluated on synthetic blood phantoms and skin-mimicking materials across physiological glucose levels (70 to 200 mg/dL). The CNN achieved a mean absolute percentage error (MAPE) of 4.82% at 650 nm with 100% Zone A coverage in the Clarke Error Grid, while the photodiode system reached 86.4% Zone A accuracy. This framework constitutes a state-of-the-art solution that balances clinical accuracy, cost efficiency, and wearable integration, paving the way for reliable continuous non-invasive glucose monitoring.

</details>


### [194] [Comparison of ConvNeXt and Vision-Language Models for Breast Density Assessment in Screening Mammography](https://arxiv.org/abs/2506.13964)
*Yusdivia Molina-Román,David Gómez-Ortiz,Ernestina Menasalvas-Ruiz,José Gerardo Tamez-Peña,Alejandro Santos-Díaz*

Key words: 乳腺密度分类, 多模态学习, CNN, BI-RADS, 医学影像

TL;DR: 研究比较了多模态与CNN方法在乳腺密度分类中的表现，发现端到端微调的CNN模型优于多模态方法。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 乳腺密度分类对癌症风险评估至关重要，但存在主观性和观察者差异问题，需自动化解决方案。

Method: 使用BI-RADS系统，评估了BioMedCLIP和ConvNeXt在零样本分类、线性探测和微调三种学习场景下的表现。

Result: 微调ConvNeXt表现最佳，零样本分类效果一般，线性探测有潜力但不如微调。

Conclusion: 尽管多模态学习有前景，但医学影像任务中端到端微调的CNN模型性能更强，未来需优化文本表示和领域适应。

Abstract: Mammographic breast density classification is essential for cancer risk assessment but remains challenging due to subjective interpretation and inter-observer variability. This study compares multimodal and CNN-based methods for automated classification using the BI-RADS system, evaluating BioMedCLIP and ConvNeXt across three learning scenarios: zero-shot classification, linear probing with textual descriptions, and fine-tuning with numerical labels. Results show that zero-shot classification achieved modest performance, while the fine-tuned ConvNeXt model outperformed the BioMedCLIP linear probe. Although linear probing demonstrated potential with pretrained embeddings, it was less effective than full fine-tuning. These findings suggest that despite the promise of multimodal learning, CNN-based models with end-to-end fine-tuning provide stronger performance for specialized medical imaging. The study underscores the need for more detailed textual representations and domain-specific adaptations in future radiology applications.

</details>


### [195] [Latent Anomaly Detection: Masked VQ-GAN for Unsupervised Segmentation in Medical CBCT](https://arxiv.org/abs/2506.14209)
*Pengwei Wang*

Key words: 无监督学习,ONJ,影像分割,VQ-GAN,3D打印

TL;DR: 该研究提出了一种无监督学习方法来识别颌骨放射性骨坏死（ONJ）影像中的异常，通过两阶段训练管道实现成功分割。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 由于ONJ影像标记数据稀缺，监督训练不切实际，研究旨在开发无监督训练方法以自动识别异常。

Method: 提出两阶段训练管道：第一阶段训练VQ-GAN准确重建正常数据；第二阶段应用随机立方体掩码和ONJ特定掩码训练新编码器。

Result: 方法在模拟和真实患者数据上均成功实现分割。

Conclusion: 该方法提供快速初始分割方案，减轻人工标记负担，并具有与手工后处理结合直接用于3D打印的潜力。

Abstract: Advances in treatment technology now allow for the use of customizable 3D-printed hydrogel wound dressings for patients with osteoradionecrosis (ORN) of the jaw (ONJ). Meanwhile, deep learning has enabled precise segmentation of 3D medical images using tools like nnUNet.
  However, the scarcity of labeled data in ONJ imaging makes supervised training impractical. This study aims to develop an unsupervised training approach for automatically identifying anomalies in imaging scans.
  We propose a novel two-stage training pipeline. In the first stage, a VQ-GAN is trained to accurately reconstruct normal subjects. In the second stage, random cube masking and ONJ-specific masking are applied to train a new encoder capable of recovering the data.
  The proposed method achieves successful segmentation on both simulated and real patient data.
  This approach provides a fast initial segmentation solution, reducing the burden of manual labeling. Additionally, it has the potential to be directly used for 3D printing when combined with hand-tuned post-processing.

</details>


### [196] [orGAN: A Synthetic Data Augmentation Pipeline for Simultaneous Generation of Surgical Images and Ground Truth Labels](https://arxiv.org/abs/2506.14303)
*Niran Nataraj,Maina Sogabe,Kenji Kawashima*

Key words: 深度学习, 医疗影像, GAN, 手术出血检测, 数据生成

TL;DR: orGAN是一种基于GAN的系统，用于生成高保真且标注好的手术出血图像，解决了数据稀缺和伦理问题。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 医疗影像中深度学习的挑战包括数据多样性不足、伦理问题和高成本，而手术中的出血检测和定位尤为困难。

Method: 通过结合小型"模拟器官"数据集和StyleGAN的Relational Positional Learning，orGAN能真实模拟出血事件；LaMa模块恢复无出血图像，提供精确标注。

Result: 评估显示，orGAN生成的图像在手术场景中达到90%的检测准确率和99%的帧级准确率。

Conclusion: orGAN显著推动了高保真标注数据集的生成，支持AI在手术中的广泛应用。

Abstract: Deep learning in medical imaging faces obstacles: limited data diversity, ethical issues, high acquisition costs, and the need for precise annotations. Bleeding detection and localization during surgery is especially challenging due to the scarcity of high-quality datasets that reflect real surgical scenarios. We propose orGAN, a GAN-based system for generating high-fidelity, annotated surgical images of bleeding. By leveraging small "mimicking organ" datasets, synthetic models that replicate tissue properties and bleeding, our approach reduces ethical concerns and data-collection costs. orGAN builds on StyleGAN with Relational Positional Learning to simulate bleeding events realistically and mark bleeding coordinates. A LaMa-based inpainting module then restores clean, pre-bleed visuals, enabling precise pixel-level annotations. In evaluations, a balanced dataset of orGAN and mimicking-organ images achieved 90% detection accuracy in surgical settings and up to 99% frame-level accuracy. While our development data lack diverse organ morphologies and contain intraoperative artifacts, orGAN markedly advances ethical, efficient, and cost-effective creation of realistic annotated bleeding datasets, supporting broader integration of AI in surgical practice.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [197] [Contemporary AI foundation models increase biological weapons risk](https://arxiv.org/abs/2506.13798)
*Roger Brent,T. Greg McKelvey*

Key words: 人工智能，生物武器，安全评估，生物安全，语言模型

TL;DR: 论文认为当前基础AI模型的安全评估低估了其促进生物武器开发的风险，指出评估方法的假设和标准存在缺陷，并通过案例证明AI可以帮助非专家完成复杂任务。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 研究旨在揭示现有AI安全评估在生物武器风险上的不足，警示潜在的生物安全威胁。

Method: 通过分析非专家成功完成技术任务的案例，并评估AI模型在生物武器开发步骤中的指导能力。

Result: 研究发现先进AI模型（如Llama 3.1 405B、ChatGPT-4o等）能准确指导用户完成生物武器开发的关键步骤，反驳了现有模型生物安全风险低的观点。

Conclusion: 论文呼吁改进评估标准，但指出可能已错过实施有效措施的时机。

Abstract: The rapid advancement of artificial intelligence has raised concerns about its potential to facilitate biological weapons development. We argue existing safety assessments of contemporary foundation AI models underestimate this risk, largely due to flawed assumptions and inadequate evaluation methods. First, assessments mistakenly assume biological weapons development requires tacit knowledge, or skills gained through hands-on experience that cannot be easily verbalized. Second, they rely on imperfect benchmarks that overlook how AI can uplift both nonexperts and already-skilled individuals. To challenge the tacit knowledge assumption, we examine cases where individuals without formal expertise, including a 2011 Norwegian ultranationalist who synthesized explosives, successfully carried out complex technical tasks. We also review efforts to document pathogen construction processes, highlighting how such tasks can be conveyed in text. We identify "elements of success" for biological weapons development that large language models can describe in words, including steps such as acquiring materials and performing technical procedures. Applying this framework, we find that advanced AI models Llama 3.1 405B, ChatGPT-4o, and Claude 3.5 Sonnet can accurately guide users through the recovery of live poliovirus from commercially obtained synthetic DNA, challenging recent claims that current models pose minimal biosecurity risk. We advocate for improved benchmarks, while acknowledging the window for meaningful implementation may have already closed.

</details>


### [198] [Dr. GPT Will See You Now, but Should It? Exploring the Benefits and Harms of Large Language Models in Medical Diagnosis using Crowdsourced Clinical Cases](https://arxiv.org/abs/2506.13805)
*Bonam Mingole,Aditya Majumdar,Firdaus Ahmed Choudhury,Jennifer L. Kraschnewski,Shyam S. Sundar,Amulya Yadav*

Key words: Large Language Models, health queries, medical evaluation, RAG

TL;DR: 论文探讨了大型语言模型（LLM）在日常生活健康咨询中的有效性，通过竞赛形式评估模型的准确性，并提出改进方法。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 当前研究忽视了LLM在日常健康咨询中的实际表现，而这是更普遍的应用场景。

Method: 采用竞赛形式，34名参与者使用4个公开LLM回答212个健康问题，由9名医师评估准确性，并测试了RAG改进效果。

Result: 76%的LLM回答被医师评为准确，RAG版本能提升回答质量。

Conclusion: LLM在日常健康咨询中表现良好，RAG方法可进一步提升效果。

Abstract: The proliferation of Large Language Models (LLMs) in high-stakes applications such as medical (self-)diagnosis and preliminary triage raises significant ethical and practical concerns about the effectiveness, appropriateness, and possible harmfulness of the use of these technologies for health-related concerns and queries. Some prior work has considered the effectiveness of LLMs in answering expert-written health queries/prompts, questions from medical examination banks, or queries based on pre-existing clinical cases. Unfortunately, these existing studies completely ignore an in-the-wild evaluation of the effectiveness of LLMs in answering everyday health concerns and queries typically asked by general users, which corresponds to the more prevalent use case for LLMs. To address this research gap, this paper presents the findings from a university-level competition that leveraged a novel, crowdsourced approach for evaluating the effectiveness of LLMs in answering everyday health queries. Over the course of a week, a total of 34 participants prompted four publicly accessible LLMs with 212 real (or imagined) health concerns, and the LLM generated responses were evaluated by a team of nine board-certified physicians. At a high level, our findings indicate that on average, 76% of the 212 LLM responses were deemed to be accurate by physicians. Further, with the help of medical professionals, we investigated whether RAG versions of these LLMs (powered with a comprehensive medical knowledge base) can improve the quality of responses generated by LLMs. Finally, we also derive qualitative insights to explain our quantitative findings by conducting interviews with seven medical professionals who were shown all the prompts in our competition. This paper aims to provide a more grounded understanding of how LLMs perform in real-world everyday health communication.

</details>


### [199] [Students' Reliance on AI in Higher Education: Identifying Contributing Factors](https://arxiv.org/abs/2506.13845)
*Griffin Pitts,Neha Rani,Weedguet Mildort,Eva-Marie Cook*

Key words: AI依赖、教育技术、编程自我效能、认知需求、学习效果

TL;DR: 研究探讨了大学生对AI工具的依赖模式，分析了过度依赖、适当依赖和不足依赖的影响因素及其与学生能力、认知需求等的关系。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 随着AI在教育中的普及，学生过度依赖AI工具可能导致学习效果下降。研究旨在探索影响AI依赖模式的因素，以促进适当依赖。

Method: 结合前后调查和受控实验任务，观察学生在面对不同可靠性AI建议时的依赖行为。

Result: 适当依赖与编程自我效能、编程素养和认知需求显著相关，过度依赖与对AI的信任和满意度相关，而不足依赖与编程素养等因素负相关。

Conclusion: 研究结果为开发针对性干预措施提供了依据，有助于在教育中更好地整合AI工具。

Abstract: The increasing availability and use of artificial intelligence (AI) tools in educational settings has raised concerns about students' overreliance on these technologies. Overreliance occurs when individuals accept incorrect AI-generated recommendations, often without critical evaluation, leading to flawed problem solutions and undermining learning outcomes. This study investigates potential factors contributing to patterns of AI reliance among undergraduate students, examining not only overreliance but also appropriate reliance (correctly accepting helpful and rejecting harmful recommendations) and underreliance (incorrectly rejecting helpful recommendations). Our approach combined pre- and post-surveys with a controlled experimental task where participants solved programming problems with an AI assistant that provided both accurate and deliberately incorrect suggestions, allowing direct observation of students' reliance patterns when faced with varying AI reliability. We find that appropriate reliance is significantly related to students' programming self-efficacy, programming literacy, and need for cognition, while showing negative correlations with post-task trust and satisfaction. Overreliance showed significant correlations with post-task trust and satisfaction with the AI assistant. Underreliance was negatively correlated with programming literacy, programming self-efficacy, and need for cognition. Overall, the findings provide insights for developing targeted interventions that promote appropriate reliance on AI tools, with implications for the integration of AI in curriculum and educational technologies.

</details>


### [200] [Computational Studies in Influencer Marketing: A Systematic Literature Review](https://arxiv.org/abs/2506.14602)
*Haoyang Gui,Thales Bertaglia,Catalina Goanta,Gerasimos Spanakis*

Key words: 影响者营销, 计算研究, 系统文献综述, 机器学习, 监管合规

TL;DR: 本文通过系统文献综述（SLR）概述了影响者营销计算研究的现状，分析了69项研究，识别了四大研究主题和方法论，指出商业优化为主导，但监管和伦理关注不足，并提出了多学科研究议程。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 尽管影响者营销在数字营销中占据重要地位，但其计算研究领域仍缺乏系统性综述，导致科学测量不足，影响了平台外利益相关者（如监管者和研究人员）的参考价值。

Method: 基于PRISMA模型的系统文献综述（SLR），分析了69项研究，识别了研究主题和方法分类（如机器学习与非机器学习技术）。

Result: 识别出四大研究主题：影响者识别与特征化、广告策略与参与度、赞助内容分析与发现、公平性；方法论主要为机器学习和非机器学习技术；发现商业优化为主，监管和伦理研究较少。

Conclusion: 提出需关注上下文因素、模型可解释性和数据集可复现性，并呼吁多学科研究议程，加强监管与合规技术联系，提升分析细粒度，开发标准化数据集。

Abstract: Influencer marketing has become a crucial feature of digital marketing strategies. Despite its rapid growth and algorithmic relevance, the field of computational studies in influencer marketing remains fragmented, especially with limited systematic reviews covering the computational methodologies employed. This makes overarching scientific measurements in the influencer economy very scarce, to the detriment of interested stakeholders outside of platforms themselves, such as regulators, but also researchers from other fields. This paper aims to provide an overview of the state of the art of computational studies in influencer marketing by conducting a systematic literature review (SLR) based on the PRISMA model. The paper analyses 69 studies to identify key research themes, methodologies, and future directions in this research field. The review identifies four major research themes: Influencer identification and characterisation, Advertising strategies and engagement, Sponsored content analysis and discovery, and Fairness. Methodologically, the studies are categorised into machine learning-based techniques (e.g., classification, clustering) and non-machine-learning-based techniques (e.g., statistical analysis, network analysis). Key findings reveal a strong focus on optimising commercial outcomes, with limited attention to regulatory compliance and ethical considerations. The review highlights the need for more nuanced computational research that incorporates contextual factors such as language, platform, and industry type, as well as improved model explainability and dataset reproducibility. The paper concludes by proposing a multidisciplinary research agenda that emphasises the need for further links to regulation and compliance technology, finer granularity in analysis, and the development of standardised datasets.

</details>


### [201] [The Synthetic Mirror -- Synthetic Data at the Age of Agentic AI](https://arxiv.org/abs/2506.13818)
*Marcelle Momha*

Key words: 合成数据，隐私，政策制定，信任，责任，法律框架

TL;DR: 论文探讨了合成数据的隐私与政策影响，提出需通过针对性修订现有法律框架来应对信任与责任问题。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 研究合成数据对隐私和政策制定的影响，强调其对信任与责任的潜在威胁。

Method: 提出通过修订现有法律框架而非新建制度，将合成数据作为独特监管类别。

Result: 强调需要新政策工具和法律框架调整，以确保对依赖合成数据的AI代理的信任与责任。

Conclusion: 最实际的方法是通过针对性修订现有框架，将合成数据视为具有独特特征的监管类别。

Abstract: Synthetic data, which is artificially generated and intelligently mimicking or supplementing the real-world data, is increasingly used. The proliferation of AI agents and the adoption of synthetic data create a synthetic mirror that conceptualizes a representation and potential distortion of reality, thus generating trust and accountability deficits. This paper explores the implications for privacy and policymaking stemming from synthetic data generation, and the urgent need for new policy instruments and legal framework adaptation to ensure appropriate levels of trust and accountability for AI agents relying on synthetic data. Rather than creating entirely new policy or legal regimes, the most practical approach involves targeted amendments to existing frameworks, recognizing synthetic data as a distinct regulatory category with unique characteristics.

</details>


### [202] [Rigor in AI: Doing Rigorous AI Work Requires a Broader, Responsible AI-Informed Conception of Rigor](https://arxiv.org/abs/2506.14652)
*Alexandra Olteanu,Su Lin Blodgett,Agathe Balayn,Angelina Wang,Fernando Diaz,Flavio du Pin Calmon,Margaret Mitchell,Michael Ekstrand,Reuben Binns,Solon Barocas*

Key words: AI研究, 严谨性, 方法论, 负责任AI, 多学科对话

TL;DR: 论文提出AI研究和实践中需要更广泛的严谨性概念，而不仅仅是方法论的严谨性，以解决负责任AI社区关注的问题。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 当前的AI研究和实践中，严谨性通常仅体现在方法论上，导致对AI能力的夸大宣称等问题。

Method: 提出包括方法论、背景知识、规范影响、概念清晰度、报告方式和证据解读在内的六方面严谨性框架。

Result: 为AI社区提供了一个更全面的严谨性概念，促进研究者、政策制定者等多方对话。

Conclusion: 需要扩展AI研究的严谨性概念，以更全面地评估和支持AI的发展。

Abstract: In AI research and practice, rigor remains largely understood in terms of methodological rigor -- such as whether mathematical, statistical, or computational methods are correctly applied. We argue that this narrow conception of rigor has contributed to the concerns raised by the responsible AI community, including overblown claims about AI capabilities. Our position is that a broader conception of what rigorous AI research and practice should entail is needed. We believe such a conception -- in addition to a more expansive understanding of (1) methodological rigor -- should include aspects related to (2) what background knowledge informs what to work on (epistemic rigor); (3) how disciplinary, community, or personal norms, standards, or beliefs influence the work (normative rigor); (4) how clearly articulated the theoretical constructs under use are (conceptual rigor); (5) what is reported and how (reporting rigor); and (6) how well-supported the inferences from existing evidence are (interpretative rigor). In doing so, we also aim to provide useful language and a framework for much-needed dialogue about the AI community's work by researchers, policymakers, journalists, and other stakeholders.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [203] [Enhancing Clinical Decision Support and EHR Insights through LLMs and the Model Context Protocol: An Open-Source MCP-FHIR Framework](https://arxiv.org/abs/2506.13800)
*Abul Ehtesham,Aditi Singh,Saket Kumar*

Key words: 临床决策支持,数字健康,电子健康记录,大型语言模型,FHIR,个性化医疗

TL;DR: 本文提出了一种基于开源代理的框架，通过大型语言模型（LLMs）与HL7 FHIR数据的集成，动态提取和分析电子健康记录（EHRs），以增强临床决策支持（CDS）、减少文档负担并改善患者健康素养。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 解决数字健康领域中临床决策支持、文档负担和患者健康素养的持续挑战。

Method: 采用基于Model Context Protocol（MCP）的代理框架，动态整合LLMs与FHIR数据，支持通过JSON配置声明式访问多样FHIR资源，实现实时总结、解释和个性化沟通。

Result: 框架在SMART Health IT沙盒中使用合成EHR数据进行了验证，提供了可扩展、可解释和互操作的AI驱动EHR应用。

Conclusion: 该框架为推进个性化数字健康解决方案奠定了坚实基础，支持多种FHIR格式。

Abstract: Enhancing clinical decision support (CDS), reducing documentation burdens, and improving patient health literacy remain persistent challenges in digital health. This paper presents an open-source, agent-based framework that integrates Large Language Models (LLMs) with HL7 FHIR data via the Model Context Protocol (MCP) for dynamic extraction and reasoning over electronic health records (EHRs). Built on the established MCP-FHIR implementation, the framework enables declarative access to diverse FHIR resources through JSON-based configurations, supporting real-time summarization, interpretation, and personalized communication across multiple user personas, including clinicians, caregivers, and patients. To ensure privacy and reproducibility, the framework is evaluated using synthetic EHR data from the SMART Health IT sandbox (https://r4.smarthealthit.org/), which conforms to the FHIR R4 standard. Unlike traditional approaches that rely on hardcoded retrieval and static workflows, the proposed method delivers scalable, explainable, and interoperable AI-powered EHR applications. The agentic architecture further supports multiple FHIR formats, laying a robust foundation for advancing personalized digital health solutions.

</details>


### [204] [Instruction and Solution Probabilities as Heuristics for Inductive Programming](https://arxiv.org/abs/2506.13804)
*Edward McDaid,Sarah McDaid*

Key words: 指令子集, 归纳编程, 启发式, 概率剪枝, 搜索空间

TL;DR: 通过引入指令概率和解决方案概率作为新的启发式方法，显著缩小了归纳编程搜索空间的规模。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 为了进一步优化归纳编程中的搜索空间，提高效率。

Method: 引入指令概率和解决方案概率作为启发式，通过阈值剪枝搜索空间。

Result: 搜索空间规模缩减可达数十个数量级，结合IS后甚至超过100个数量级。

Conclusion: 新方法有效且适用于未见过的代码，未来可进一步优化。

Abstract: Instruction subsets (ISs) are heuristics that can shrink the size of the inductive programming (IP) search space by tens of orders of magnitude. Here, we extend the IS approach by introducing instruction and solution probabilities as additional heuristics. Instruction probability reflects the expectation of an instruction occurring in a solution, based on the frequency of instruction occurrence in a large code sample. The solution probability for a partial or complete program is simply the product of all constituent instruction probabilities, including duplicates. We treat the minimum solution probabilities observed in code sample program units of different sizes as solution probability thresholds. These thresholds are used to prune the search space as partial solutions are constructed, thereby eliminating any branches containing unlikely combinations of instructions. The new approach has been evaluated using a large sample of human code. We tested two formulations of instruction probability: one based on instruction occurrence across the entire code sample and another that measured the distribution separately for each IS. Our results show that both variants produce substantial further reductions in the IP search space size of up to tens of orders of magnitude, depending on solution size. In combination with IS, reductions of over 100 orders of magnitude can be achieved. We also carried out cross-validation testing to show that the heuristics should work effectively with unseen code. The approach is described and the results and some ideas for future work are discussed.

</details>


### [205] [Structured Program Synthesis using LLMs: Results and Insights from the IPARC Challenge](https://arxiv.org/abs/2506.13820)
*Shraddha Surana,Ashwin Srinivasan,Michael Bain*

Key words: IPARC, 程序综合, LLM, 结构化编程, 人机协作

TL;DR: IPARC挑战赛通过合成图像任务评估程序构建能力，本文提出一种结构化归纳编程方法，成功解决了所有任务类别，揭示了LLM在代码生成中的关键作用。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: IPARC挑战赛的600项任务尚未被自动化方法解决，研究旨在探索LLM在程序综合中的潜力及人机协作机制。

Method: 采用结构化归纳编程方法，结合LLM的代码生成能力，并强调先验结构化和代码重用。

Result: 成功解决了IPARC的所有任务类别，揭示了LLM在代码生成中的关键机制。

Conclusion: LLM在程序综合中能有效辅助人类，但需结构化先验和代码冻结，同时激发人类创造力。

Abstract: The IPARC Challenge, inspired by ARC, provides controlled program synthesis tasks over synthetic images to evaluate automatic program construction, focusing on sequence, selection, and iteration. This set of 600 tasks has resisted automated solutions. This paper presents a structured inductive programming approach with LLMs that successfully solves tasks across all IPARC categories. The controlled nature of IPARC reveals insights into LLM-based code generation, including the importance of prior structuring, LLMs' ability to aid structuring (requiring human refinement), the need to freeze correct code, the efficiency of code reuse, and how LLM-generated code can spark human creativity. These findings suggest valuable mechanisms for human-LLM collaboration in tackling complex program synthesis.

</details>


### [206] [CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios](https://arxiv.org/abs/2506.13977)
*Shiting Huang,Zhen Fang,Zehui Chen,Siyu Yuan,Junjie Ye,Yu Zeng,Lin Chen,Qi Mao,Feng Zhao*

Key words: 大型语言模型,工具学习,错误处理,CRITICTOOL,进化策略

TL;DR: 论文提出CRITICTOOL，一个针对工具学习的综合批评评估基准，通过进化策略构建数据集，以更好反映现实场景中的工具使用错误，并通过实验验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 随着任务复杂性增加，大型语言模型在使用工具时可能触发更多错误，如何有效识别、诊断和恢复这些错误成为工具学习的关键研究方向。

Method: 通过广泛分析工具调用过程中的错误类型，构建CRITICTOOL基准，采用进化策略生成多样化错误数据集。

Result: 实验验证了CRITICTOOL的泛化能力和有效性，并深入分析了不同LLM的工具反思能力。

Conclusion: CRITICTOOL为工具学习提供了新视角，其数据集构建策略和实验结果对领域有重要贡献。

Abstract: The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range of tasks. However, as the tasks become more complex and long-horizon, the intricate tool utilization process may trigger various unexpected errors. Therefore, how to effectively handle such errors, including identifying, diagnosing, and recovering from them, has emerged as a key research direction for advancing tool learning. In this work, we first extensively analyze the types of errors encountered during the function-calling process on several competitive tool evaluation benchmarks. Based on it, we introduce CRITICTOOL, a comprehensive critique evaluation benchmark specialized for tool learning. Building upon a novel evolutionary strategy for dataset construction, CRITICTOOL holds diverse tool-use errors with varying complexities, which better reflects real-world scenarios. We conduct extensive experiments on CRITICTOOL, and validate the generalization and effectiveness of our constructed benchmark strategy. We also provide an in-depth analysis of the tool reflection ability on various LLMs, offering a new perspective on the field of tool learning in LLMs. The code is available at \href{https://github.com/Shellorley0513/CriticTool}{https://github.com/Shellorley0513/CriticTool}.

</details>


### [207] [MLDebugging: Towards Benchmarking Code Debugging Across Multi-Library Scenarios](https://arxiv.org/abs/2506.13824)
*Jinyang Huang,Xiachong Feng,Qiguang Chen,Hanjie Zhao,Zihui Cheng,Jiesong Bai,Jingxuan Zhou,Min Li,Libo Qin*

Key words: 代码调试, 多库场景, 大语言模型, MLDebugging基准, Python

TL;DR: 论文提出了一种多库调试基准MLDebugging，首次针对复杂多库场景的代码调试问题，评估了主流LLM的表现，发现其在多库调试中仍有不足。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现有代码调试研究多集中在简单无库或单库场景，忽视了真实应用中复杂多库环境的需求。

Method: 设计了MLDebugging基准，涵盖126个Python库的多库代码问题，并将其分为七类，随后用主流开源和闭源LLM进行评估。

Result: 当前LLM在多库调试场景中表现不佳。

Conclusion: 该工作揭示了LLM在多库调试中的潜力，并为未来研究提供了方向。

Abstract: Code debugging is a crucial task in software engineering, which attracts increasing attention. While remarkable success has been made in the era of large language models (LLMs), current research still focuses on the simple no-library or single-library setting, ignoring the complex multi-library scenario in real-world applications. To address this limitation, we make the first attempt to introduce MLDebugging (Multi-Library Debugging), a comprehensive benchmark designed to assess debugging challenges within multi-library Python code. Specifically, MLDebugging encompasses 126 distinct Python libraries, covering a wide range of multi-library code issues, categorized into seven distinct types. Furthermore, we conduct a thorough evaluation of MLDebugging using both mainstream open-source and closed-source LLMs and highlight that current LLMs still struggle to correctly perform code debugging across multi-library scenarios. We hope this work can uncover the potential of LLMs in multi-library debugging scenario and offer insights for future research.

</details>


### [208] [FrontendBench: A Benchmark for Evaluating LLMs on Front-End Development via Automatic Evaluation](https://arxiv.org/abs/2506.13832)
*Hongda Zhu,Yiwen Zhang,Bing Zhao,Jingzhe Ding,Siyao Liu,Tong Liu,Dandan Wang,Yanan Liu,Zhaojian Li*

Key words: 大型语言模型, 前端代码生成, 基准测试, FrontendBench, 自动评估

TL;DR: FrontendBench是一个由人类与大型语言模型共同开发的基准测试，旨在解决现有前端代码生成评测的局限性，提供更全面、实际的评估。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现有的大语言模型前端代码生成评测存在任务简单、测试用例不够严格、缺乏端到端验证等问题，导致模型性能评估不准确。

Method: 开发了FrontendBench，包含148个精心设计的提示-测试用例对，涵盖5个级别的Web组件，并引入自动评估框架，在沙盒环境中执行代码并通过测试脚本评估结果。

Result: 自动评估框架与人类专家评估的一致性达到90.54%。在FrontendBench上评测多个先进LLM，发现它们在处理实际前端任务时表现差异显著。

Conclusion: FrontendBench是一个可靠且可扩展的基准测试，支持多模态评估，为未来前端代码生成研究提供了坚实基础。

Abstract: Large Language Models (LLMs) have made significant strides in front-end code generation. However, existing benchmarks exhibit several critical limitations: many tasks are overly simplistic, test cases often lack rigor, and end-to-end validation is absent. These issues hinder the accurate assessment of model performance. To address these challenges, we present FrontendBench, a benchmark co-developed by humans and LLMs. FrontendBench categorizes tasks based on code functionality and incorporates interactive test scenarios, enabling a more comprehensive and practical evaluation of front-end code generation capabilities. The benchmark comprises 148 meticulously crafted prompt-test case pairs spanning five levels of web components, from basic UI elements to complex interactive features. Each task reflects realistic front-end development challenges. Furthermore, we introduce an automatic evaluation framework that executes generated code within a sandbox environment and assesses outcomes using predefined test scripts. This framework achieves a 90.54% agreement rate with expert human evaluations, demonstrating high reliability. We benchmark several state-of-the-art LLMs on FrontendBench and observe substantial performance disparities in handling real-world front-end tasks. These results highlight FrontendBench as a reliable and scalable benchmark, supporting consistent multimodal evaluation and providing a robust foundation for future research in front-end code generation. Our data and code will be released soon.

</details>


### [209] [How Does LLM Reasoning Work for Code? A Survey and a Call to Action](https://arxiv.org/abs/2506.13932)
*Ira Ceka,Saurabh Pujar,Irene Manotas,Gail Kaiser,Baishakhi Ray,Shyam Ramji*

Key words: 大语言模型, 代码推理, 软件工程, 基准测试, 未来研究

TL;DR: 本文探讨了大语言模型（LLMs）在代码推理任务中的应用，首次系统总结了代码推理的策略、方法，并提出了未来研究的潜在方向。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 随着大语言模型在自然语言处理任务中的成功，其在代码领域的应用逐渐受到关注，尤其是在软件工程任务中。然而，实际部署中的代码推理能力尚未被充分研究。

Method: 本文通过综述代码推理的技术，提出了分类法，并分析了不同方法在基准测试中的表现。还探讨了代码核心特性对推理技术的影响。

Result: 研究总结了代码推理的混合和代理方法，展示了新基准测试的潜力，并指出代码特性如何解释不同的推理技术。

Conclusion: 本文系统梳理了代码推理的现状，提出了未来研究的方向，尤其是在软件工程任务中的未探索领域。

Abstract: The rise of large language models (LLMs) has led to dramatic improvements across a wide range of natural language tasks. These advancements have extended into the domain of code, facilitating complex tasks such as code generation, translation, summarization, and repair. However, their utility for real-world deployment in-the-wild has only recently been studied, particularly on software engineering (SWE) tasks such as GitHub issue resolution. In this study, we examine the code reasoning techniques that underlie the ability to perform such tasks, and examine the paradigms used to drive their performance. Our contributions in this paper are: (1) the first dedicated survey on code reasoning for code tasks, highlighting overarching strategies, hybrid and agentic approaches; (2) a taxonomy of various techniques used to drive code reasoning; (3) a comprehensive overview of performance on common benchmarks and a showcase of new, under-explored benchmarks with high potential in SWE; (4) an exploration on how core properties of code can be used to explain different reasoning techniques; and (5) gaps and potentially under-explored areas for future research.

</details>


### [210] [Automatic Qiskit Code Refactoring Using Large Language Models](https://arxiv.org/abs/2506.14535)
*José Manuel Suárez,Luis Mariano Bibbó,Joaquin Bogado,Alejandro Fernandez*

Key words: 量子计算, Qiskit, 代码迁移, 大型语言模型, 自动化重构

TL;DR: 本文提出了一种利用大型语言模型（LLM）重构Qiskit代码的新方法，帮助开发者应对API快速变化带来的兼容性问题。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 随着量子软件框架的发展，API的快速变化给开发者维护代码兼容性带来了挑战。本文旨在通过LLM自动化解决这一问题。

Method: 从Qiskit官方文档中提取迁移场景的分类法，并与源代码一起输入LLM，以识别迁移场景并提供重构建议。

Result: 研究表明，结合领域知识的LLM能有效辅助Qiskit代码的自动化迁移。

Conclusion: 该方法不仅提供了Qiskit代码迁移的分类法和有效提示，还为评估LLM在量子代码迁移中的能力提供了方法论。

Abstract: As quantum software frameworks evolve, developers face increasing challenges in maintaining compatibility with rapidly changing APIs. In this work, we present a novel methodology for refactoring Qiskit code using large language models (LLMs). We begin by extracting a taxonomy of migration scenarios from the different sources of official Qiskit documentation (such as release notes), capturing common patterns such as migration of functionality to different modules and deprecated usage. This taxonomy, along with the original Python source code, is provided as input to an LLM, which is then tasked with identifying instances of migration scenarios in the code and suggesting appropriate refactoring solutions. Our approach is designed to address the context length limitations of current LLMs by structuring the input and reasoning process in a targeted, efficient manner. The results demonstrate that LLMs, when guided by domain-specific migration knowledge, can effectively assist in automating Qiskit code migration. This work contributes both a set of proven prompts and taxonomy for Qiskit code migration from earlier versions to version 0.46 and a methodology to asses the capabilities of LLMs to assist in the migration of quantum code.

</details>


### [211] [Low-code to fight climate change: the Climaborough project](https://arxiv.org/abs/2506.14623)
*Aaron Conrardy,Armen Sulejmani,Cindy Guerlain,Daniele Pagani,David Hick,Matteo Satta,Jordi Cabot*

Key words: 碳中和, 低代码, 无代码, 仪表盘, 气候过渡

TL;DR: Climaborough项目通过低代码/无代码策略开发城市平台，帮助欧洲城市监测和实现2030年碳中和目标。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 支持欧洲城市快速部署气候仪表盘，以实时监测和评估碳中和进展。

Method: 采用低代码策略加速仪表盘开发，并嵌入无代码哲学，使非技术用户也能配置和定制仪表盘。

Result: 开发了用户友好的仪表盘，帮助城市评估气候倡议效果并识别最具影响力的措施。

Conclusion: 低代码/无代码策略有效支持了城市气候目标的实现，促进了市民参与。

Abstract: The EU-funded Climaborough project supports European cities to achieve carbon neutrality by 2030. Eleven cities in nine countries will deploy in real conditions products and services fostering climate transition in their local environment. The Climaborough City Platform is being developed to monitor the cities' overall progress towards their climate goals by aggregating historic and real-time data and displaying the results in user-friendly dashboards that will be used by non-technical experts to evaluate the effectiveness of local experimental initiatives, identify those that yield significant impact, and assess the potential consequences of scaling them up to a broader level. In this paper, we explain how we have put in place a low-code/no-code strategy in Climaborough in response to the project's aim to quickly deploy climate dashboards. A low-code strategy is used to accelerate the development of the dashboards. The dashboards embed a no-code philosophy that enables all types of citizen profiles to configure and adapt the dashboard to their specific needs.

</details>


### [212] [ACM Survey Draft on Formalising Software Requirements with Large Language Models](https://arxiv.org/abs/2506.14627)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Key words: 软件需求,形式化方法,统一编程理论,机构理论

TL;DR: 该草稿汇总了94篇论文，并增加了关于软件需求可追溯性、形式化方法及其工具、统一编程理论和机构理论的章节。与类似标题的AACS 2025和SAIV 2025论文相比，主要区别在于篇幅和审查流程。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 总结和扩展94篇论文的内容，重点讨论软件需求可追溯性、形式化方法和理论工具等主题。

Method: 对现有论文进行综述，并增加详细章节。

Result: 形成了涵盖多个关键主题的综合草稿。

Conclusion: 该草稿为后续研究提供了基础，尤其是软件需求和相关理论领域。

Abstract: This draft is a working document, having a summary of nighty-four (94) papers with additional sections on Traceability of Software Requirements (Section 4), Formal Methods and Its Tools (Section 5), Unifying Theories of Programming (UTP) and Theory of Institutions (Section 6). Please refer to abstract of [7,8]. Key difference of this draft from our recently anticipated ones with similar titles, i.e. AACS 2025 [7] and SAIV 2025 [8] is:
  [7] is a two page submission to ADAPT Annual Conference, Ireland. Submitted on 18th of March, 2025, it went through the light-weight blind review and accepted for poster presentation. Conference was held on 15th of May, 2025.
  [8] is a nine page paper with additional nine pages of references and summary tables, submitted to Symposium on AI Verification (SAIV 2025) on 24th of April, 2025. It went through rigorous review process. The uploaded version on arXiv.org [8] is the improved one of the submission, after addressing the specific suggestions to improve the paper.

</details>


### [213] [Navigating the growing field of research on AI for software testing -- the taxonomy for AI-augmented software testing and an ontology-driven literature survey](https://arxiv.org/abs/2506.14640)
*Ina K. Schieferdecker*

Key words: 软件测试, AI, 自动化测试, 分类法, ai4st

TL;DR: 本文探讨了AI如何增强软件测试自动化，从无自动化到完全自动化，并提出新的分类法ai4st，用于对近期研究进行分类和识别开放性问题。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 随着AI在多个工程领域的突破，为软件测试（包括手动和自动化测试）带来了新的视角。本文旨在回顾AI在软件测试自动化中的应用研究，并探索新的测试形式。

Method: 论文采用了文献综述的方法，分析了AI在软件测试自动化中的最新研究，并提出了新的分类法ai4st。

Result: 通过分类法ai4st，论文对近期研究进行了系统分类，并识别了开放的研究问题。

Conclusion: AI为软件测试自动化提供了新的可能性，未来研究应关注AI在这一领域的进一步应用和优化。

Abstract: In industry, software testing is the primary method to verify and validate the functionality, performance, security, usability, and so on, of software-based systems. Test automation has gained increasing attention in industry over the last decade, following decades of intense research into test automation and model-based testing. However, designing, developing, maintaining and evolving test automation is a considerable effort. Meanwhile, AI's breakthroughs in many engineering fields are opening up new perspectives for software testing, for both manual and automated testing. This paper reviews recent research on AI augmentation in software test automation, from no automation to full automation. It also discusses new forms of testing made possible by AI. Based on this, the newly developed taxonomy, ai4st, is presented and used to classify recent research and identify open research questions.

</details>


### [214] [Unified Software Engineering agent as AI Software Engineer](https://arxiv.org/abs/2506.14683)
*Leonhard Applis,Yuntong Zhang,Shanchao Liang,Nan Jiang,Lin Tan,Abhik Roychoudhury*

Key words: LLM agent, AI software engineer, unified agent, software engineering, USEbench

TL;DR: 论文探讨了LLM代理是否等同于AI软件工程师，并提出了一种统一的软件工程代理USEagent，用以处理多种软件任务，并在实验中展示了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 研究动机在于探索LLM代理是否能成为全面的AI软件工程师，而不仅限于特定任务，以应对软件开发中的复杂场景。

Method: 方法是通过开发USEagent，一个能够协调和完成多种软件任务的统一代理，并使用USEbench进行综合评估。

Result: 结果表明USEagent在1,271个任务中表现优于现有通用代理，但仍在某些编码任务中存在不足。

Conclusion: 结论是USEagent是未来AI软件工程师的雏形，但仍需进一步改进和完善。

Abstract: The growth of Large Language Model (LLM) technology has raised expectations for automated coding. However, software engineering is more than coding and is concerned with activities including maintenance and evolution of a project. In this context, the concept of LLM agents has gained traction, which utilize LLMs as reasoning engines to invoke external tools autonomously. But is an LLM agent the same as an AI software engineer? In this paper, we seek to understand this question by developing a Unified Software Engineering agent or USEagent. Unlike existing work which builds specialized agents for specific software tasks such as testing, debugging, and repair, our goal is to build a unified agent which can orchestrate and handle multiple capabilities. This gives the agent the promise of handling complex scenarios in software development such as fixing an incomplete patch, adding new features, or taking over code written by others. We envision USEagent as the first draft of a future AI Software Engineer which can be a team member in future software development teams involving both AI and humans. To evaluate the efficacy of USEagent, we build a Unified Software Engineering bench (USEbench) comprising of myriad tasks such as coding, testing, and patching. USEbench is a judicious mixture of tasks from existing benchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on USEbench consisting of 1,271 repository-level software engineering tasks, USEagent shows improved efficacy compared to existing general agents such as OpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for certain coding tasks, which provides hints on further developing the AI Software Engineer of the future.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [215] [Projecting U.S. coastal storm surge risks and impacts with deep learning](https://arxiv.org/abs/2506.13963)
*Julian R. Rice,Karthik Balaguru,Fadia Ticona Rollano,John Wilson,Brent Daniel,David Judi,Ning Sun,L. Ruby Leung*

Key words: 风暴潮, 热带气旋, 深度学习, 风险评估, 海平面上升

TL;DR: 利用深度学习模型评估美国风暴潮风险，发现世纪末人口风险增加50%，佛罗里达州风险显著提升。

<details>
  <summary>Details</summary>

Main category: physics.ao-ph

Motivation: 热带气旋引发的风暴潮危害巨大，但当前及未来风险评估困难，需借助人工智能新技术解决。

Method: 使用深度学习模型分析90万次合成热带气旋事件，结合海平面变化和TC行为预测，评估沿海风暴潮风险。

Result: 模型显示世纪末TC强度和海平面上升导致风险人口增加50%，佛罗里达、乔治亚和南卡罗来纳州风险显著。

Conclusion: 深度学习模型有效评估风暴潮风险，为未来防灾规划提供科学依据。

Abstract: Storm surge is one of the deadliest hazards posed by tropical cyclones (TCs), yet assessing its current and future risk is difficult due to the phenomenon's rarity and physical complexity. Recent advances in artificial intelligence applications to natural hazard modeling suggest a new avenue for addressing this problem. We utilize a deep learning storm surge model to efficiently estimate coastal surge risk in the United States from 900,000 synthetic TC events, accounting for projected changes in TC behavior and sea levels. The derived historical 100-year surge (the event with a 1% yearly exceedance probability) agrees well with historical observations and other modeling techniques. When coupled with an inundation model, we find that heightened TC intensities and sea levels by the end of the century result in a 50% increase in population at risk. Key findings include markedly heightened risk in Florida, and critical thresholds identified in Georgia and South Carolina.

</details>


### [216] [AI-Informed Model Analogs for Subseasonal-to-Seasonal Prediction](https://arxiv.org/abs/2506.14022)
*Jacob B. Landsberg,Elizabeth A. Barnes,Matthew Newman*

Key words: 次季节到季节预测、可解释AI、模拟预报、人工神经网络、极端温度

TL;DR: 该论文提出了一种基于可解释AI的模拟预报方法，用于改进次季节到季节（S2S）预测，并在多个任务中优于传统方法和基准。

<details>
  <summary>Details</summary>

Main category: physics.ao-ph

Motivation: S2S预测对公共卫生、灾害准备和农业至关重要，但预测难度大。

Method: 使用人工神经网络学习权重掩码以优化模拟选择，并在三种预测任务中评估其性能。

Result: AI-informed模拟方法在气候模型和再分析数据中均优于传统方法，并提高了极端温度的预测能力和不确定性表征。

Conclusion: 可解释AI框架不仅提升了预测性能，还帮助理解S2S的可预测性来源。

Abstract: Subseasonal-to-seasonal forecasting is crucial for public health, disaster preparedness, and agriculture, and yet it remains a particularly challenging timescale to predict. We explore the use of an interpretable AI-informed model analog forecasting approach, previously employed on longer timescales, to improve S2S predictions. Using an artificial neural network, we learn a mask of weights to optimize analog selection and showcase its versatility across three varied prediction tasks: 1) classification of Week 3-4 Southern California summer temperatures; 2) regional regression of Month 1 midwestern U.S. summer temperatures; and 3) classification of Month 1-2 North Atlantic wintertime upper atmospheric winds. The AI-informed analogs outperform traditional analog forecasting approaches, as well as climatology and persistence baselines, for deterministic and probabilistic skill metrics on both climate model and reanalysis data. We find the analog ensembles built using the AI-informed approach also produce better predictions of temperature extremes and improve representation of forecast uncertainty. Finally, by using an interpretable-AI framework, we analyze the learned masks of weights to better understand S2S sources of predictability.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [217] [Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience](https://arxiv.org/abs/2506.13971)
*Andrew Chang,Chenkai Hu,Ji Qi,Zhuojian Wei,Kexin Zhang,Viswadruth Akkaraju,David Poeppel,Dustin Freeman*

Key words: 半监督学习, 多模态融合, 视频会议体验, 标注效率

TL;DR: 论文摘要介绍了在半监督学习（SSL）框架下，利用多模态数据预测视频会议中不流畅或不愉快时刻的方法，其性能优于监督学习（SL）模型。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 研究视频会议中体验不佳时刻的预测，因这类时刻在自然数据中稀少且标注成本高，需探索更高效的建模方法。

Method: 采用模态融合的半监督学习，结合标注和未标注数据训练多模态（音频、面部、文本）深度特征模型。

Result: SSL模型的ROC-AUC达0.9，F1为0.6，比SL模型高4%；仅8%标注数据即可达SL模型全数据性能的96%。

Conclusion: 验证了半监督学习在减少标注需求下仍能高效建模视频会议体验的潜力。

Abstract: Group conversations over videoconferencing are a complex social behavior. However, the subjective moments of negative experience, where the conversation loses fluidity or enjoyment remain understudied. These moments are infrequent in naturalistic data, and thus training a supervised learning (SL) model requires costly manual data annotation. We applied semi-supervised learning (SSL) to leverage targeted labeled and unlabeled clips for training multimodal (audio, facial, text) deep features to predict non-fluid or unenjoyable moments in holdout videoconference sessions. The modality-fused co-training SSL achieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming SL models by up to 4% with the same amount of labeled data. Remarkably, the best SSL model with just 8% labeled data matched 96% of the SL model's full-data performance. This shows an annotation-efficient framework for modeling videoconference experience.

</details>


### [218] [Improving Practical Aspects of End-to-End Multi-Talker Speech Recognition for Online and Offline Scenarios](https://arxiv.org/abs/2506.14204)
*Aswin Shanmugam Subramanian,Amit Das,Naoyuki Kanda,Jinyu Li,Xiaofei Wang,Yifan Gong*

Key words: 自动语音识别, 流式处理, 离线应用, 延迟平衡, 多说话者转录

TL;DR: 该论文扩展了SOT框架，以满足流式和离线ASR应用的现实需求，重点平衡延迟与准确性。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 解决自动语音识别（ASR）在流式和离线应用中对延迟和准确性的实际需求。

Method: 1. 采用CSS单通道前端结合E2E系统处理高度重叠语音；2. 实现双模型（Conformer Transducer和Sequence-to-Sequence）或两遍模型；3. 探索segSOT以优化离线场景和多说话者转录。

Result: CSS框架提高了ASR系统在重叠语音中的准确性；双模型或两遍模型适应不同场景；segSOT提升了离线转录的可读性。

Conclusion: 提出的方法平衡了延迟与准确性，适用于实时字幕和摘要需求，为ASR系统的实际应用提供了灵活解决方案。

Abstract: We extend the frameworks of Serialized Output Training (SOT) to address practical needs of both streaming and offline automatic speech recognition (ASR) applications. Our approach focuses on balancing latency and accuracy, catering to real-time captioning and summarization requirements. We propose several key improvements: (1) Leveraging Continuous Speech Separation (CSS) single-channel front-end with end-to-end (E2E) systems for highly overlapping scenarios, challenging the conventional wisdom of E2E versus cascaded setups. The CSS framework improves the accuracy of the ASR system by separating overlapped speech from multiple speakers. (2) Implementing dual models -- Conformer Transducer for streaming and Sequence-to-Sequence for offline -- or alternatively, a two-pass model based on cascaded encoders. (3) Exploring segment-based SOT (segSOT) which is better suited for offline scenarios while also enhancing readability of multi-talker transcriptions.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [219] [Sketched Sum-Product Networks for Joins](https://arxiv.org/abs/2506.14034)
*Brian Tsan,Abylay Amanbayev,Asoke Datta,Florin Rusu*

Key words: sketches, Sum-Product Networks, 连接基数估计, 查询优化, 动态近似

TL;DR: Sketches在连接基数估计中表现高效，但通常针对预定义查询构建。本文提出使用Sum-Product Networks动态近似sketches，提高查询优化的普适性。

<details>
  <summary>Details</summary>

Main category: cs.DB

Motivation: 现有sketches技术虽高效，但仅适用于预定义查询，限制了其在动态查询优化中的应用。需要一个更通用的解决方案。

Method: 采用Sum-Product Networks动态近似sketches，通过分解多变量分布为单变量分布的线性组合，高效近似任意查询的sketches。

Result: 提出的方法成功实现了对Fast-AGMS和Bound Sketch的高效近似，避免了高成本构造，适用于查询优化。

Conclusion: 通过Sum-Product Networks动态近似sketches，提供了一种更灵活、高效的查询优化方案。

Abstract: Sketches have shown high accuracy in multi-way join cardinality estimation, a critical problem in cost-based query optimization. Accurately estimating the cardinality of a join operation -- analogous to its computational cost -- allows the optimization of query execution costs in relational database systems. However, although sketches have shown high efficacy in query optimization, they are typically constructed specifically for predefined selections in queries that are assumed to be given a priori, hindering their applicability to new queries. As a more general solution, we propose for Sum-Product Networks to dynamically approximate sketches on-the-fly. Sum-Product Networks can decompose and model multivariate distributions, such as relations, as linear combinations of multiple univariate distributions. By representing these univariate distributions as sketches, Sum-Product Networks can combine them element-wise to efficiently approximate the sketch of any query selection. These approximate sketches can then be applied to join cardinality estimation. In particular, we implement the Fast-AGMS and Bound Sketch methods, which have successfully been used in prior work, despite their costly construction. By accurately approximating them instead, our work provides a practical alternative to apply these sketches to query optimization.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [220] [Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study](https://arxiv.org/abs/2506.13811)
*Sompote Youwai,David Phim,Vianne Gayl Murcia,Rianne Clair Onas*

Key words: 多智能体系统, 路由器, 基础设计, 智能任务分类, 专家选择

TL;DR: 本研究探讨了基于路由器的多智能体系统在基础设计计算自动化中的应用，通过智能任务分类和专家选择提高了性能。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 旨在利用多智能体系统自动化基础设计计算，提高效率和准确性。

Method: 评估了三种方法：单智能体处理、多智能体设计-检查架构和基于路由器的专家选择，并使用了多种基线模型进行性能测试。

Result: 基于路由器的配置在浅基础和桩设计中分别取得了95.00%和90.63%的性能得分，优于传统工作流。

Conclusion: 基于路由器的多智能体系统在基础设计自动化中表现最优，但仍需要人工监督以确保安全性。

Abstract: This study investigates router-based multi-agent systems for automating foundation design calculations through intelligent task classification and expert selection. Three approaches were evaluated: single-agent processing, multi-agent designer-checker architecture, and router-based expert selection. Performance assessment utilized baseline models including DeepSeek R1, ChatGPT 4 Turbo, Grok 3, and Gemini 2.5 Pro across shallow foundation and pile design scenarios. The router-based configuration achieved performance scores of 95.00% for shallow foundations and 90.63% for pile design, representing improvements of 8.75 and 3.13 percentage points over standalone Grok 3 performance respectively. The system outperformed conventional agentic workflows by 10.0 to 43.75 percentage points. Grok 3 demonstrated superior standalone performance without external computational tools, indicating advances in direct LLM mathematical reasoning for engineering applications. The dual-tier classification framework successfully distinguished foundation types, enabling appropriate analytical approaches. Results establish router-based multi-agent systems as optimal for foundation design automation while maintaining professional documentation standards. Given safety-critical requirements in civil engineering, continued human oversight remains essential, positioning these systems as advanced computational assistance tools rather than autonomous design replacements in professional practice.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [221] [A Hybrid Neural Network -- Polynomial Series Scheme for Learning Invariant Manifolds of Discrete Dynamical Systems](https://arxiv.org/abs/2506.13950)
*Dimitrios G. Patsatzis,Nikolaos Kazantzis,Ioannis G. Kevrekidis,Constantinos Siettos*

Key words: 降阶模型, 不变流形, 混合机器学习, 多项式级数, 浅层神经网络

TL;DR: 提出一种混合机器学习方案，结合多项式级数和浅层神经网络，以物理和数值分析为背景，学习离散映射的不变流形（IM），构建动力系统的降阶模型（ROM）。

<details>
  <summary>Details</summary>

Main category: math.NA

Motivation: 研究动力系统降阶模型的构建问题，通过结合两种方法的优势提升不变流形的学习精度和效率。

Method: 结合多项式级数和浅层神经网络，多项式保证固定点附近的局部指数收敛率，神经网络处理复杂结构。

Result: 在三个基准测试中验证方案效率，混合方法在数值逼近精度上优于纯多项式或神经网络方法。

Conclusion: 混合方法在不变流形逼近中表现更优，综合了多项式的高效收敛和神经网络的灵活性。

Abstract: We propose a hybrid machine learning scheme to learn -- in physics-informed and numerical analysis-informed fashion -- invariant manifolds (IM) of discrete maps for constructing reduced-order models (ROMs) for dynamical systems. The proposed scheme combines polynomial series with shallow neural networks, exploiting the complementary strengths of both approaches. Polynomials enable an efficient and accurate modeling of ROMs with guaranteed local exponential convergence rate around the fixed point, where, under certain assumptions, the IM is demonstrated to be analytic. Neural networks provide approximations to more complex structures beyond the reach of the polynomials' convergence. We evaluate the efficiency of the proposed scheme using three benchmark examples, examining convergence behavior, numerical approximation accuracy, and computational training cost. Additionally, we compare the IM approximations obtained solely with neural networks and with polynomial expansions. We demonstrate that the proposed hybrid scheme outperforms both pure polynomial approximations (power series, Legendre and Chebyshev polynomials) and standalone shallow neural network approximations in terms of numerical approximation accuracy.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [222] [NeuralPDR: Neural Differential Equations as surrogate models for Photodissociation Regions](https://arxiv.org/abs/2506.14270)
*Gijs Vermariën,Thomas G. Bisbas,Serena Viti,Yue Zhao,Xuefei Tang,Rahul Ravichandran*

Key words: 天体化学模型, 替代模型, 神经ODE, 3D模拟, JWST, ALMA

TL;DR: 通过替代模型（Latent Augmented Neural ODEs）加速天体化学模拟，提升对高分辨率望远镜观测的解释能力。

<details>
  <summary>Details</summary>

Main category: astro-ph.GA

Motivation: 高分辨率望远镜（如JWST和ALMA）需要更小尺度的天体化学模拟，但传统方法计算成本高昂。

Method: 提出基于神经ODE的替代模型，并在三个数据集（包括3D-PDR模拟数据）上训练。

Result: 替代模型能显著加速模拟，并准确重现观测数据中的柱密度分布。

Conclusion: 该方法为天体化学模拟提供了快速推断工具，支持更高分辨率的流体动力学模拟。

Abstract: Computational astrochemical models are essential for helping us interpret and understand the observations of different astrophysical environments. In the age of high-resolution telescopes such as JWST and ALMA, the substructure of many objects can be resolved, raising the need for astrochemical modeling at these smaller scales, meaning that the simulations of these objects need to include both the physics and chemistry to accurately model the observations. The computational cost of the simulations coupling both the three-dimensional hydrodynamics and chemistry is enormous, creating an opportunity for surrogate models that can effectively substitute the chemical solver. In this work we present surrogate models that can replace the original chemical code, namely Latent Augmented Neural Ordinary Differential Equations. We train these surrogate architectures on three datasets of increasing physical complexity, with the last dataset derived directly from a three-dimensional simulation of a molecular cloud using a Photodissociation Region (PDR) code, 3D-PDR. We show that these surrogate models can provide speedup and reproduce the original observable column density maps of the dataset. This enables the rapid inference of the chemistry (on the GPU), allowing for the faster statistical inference of observations or increasing the resolution in hydrodynamical simulations of astrophysical environments.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [223] [Complete Characterization for Adjustment in Summary Causal Graphs of Time Series](https://arxiv.org/abs/2506.14534)
*Clément Yvernes,Emilie Devijver,Eric Gaussier*

Key words: 因果效应, 时间序列, 摘要因果图, 调整准则, 伪线性算法

TL;DR: 本文研究了时间序列中多干预下的因果效应可识别性问题，提出了基于摘要因果图的调整准则的必要与充分条件，并提供了一个伪线性算法来判断查询是否可识别。

<details>
  <summary>Details</summary>

Main category: math.ST

Motivation: 研究目标是通过观测数据推断多干预下的因果效应，特别是在时间序列中，当真实因果图仅以摘要形式呈现时，如何判断效应的可识别性。

Method: 提出了一种调整准则的必要与充分条件，证明其在此设定下的完备性，并开发了一个伪线性算法用于判断可识别性。

Result: 研究发现调整准则在此设定下完备，且伪线性算法能有效解决可识别性问题。

Conclusion: 在时间序列与摘要因果图的背景下，多干预下的因果效应可识别性问题得到了解决，为实际应用提供了理论支持。

Abstract: The identifiability problem for interventions aims at assessing whether the total causal effect can be written with a do-free formula, and thus be estimated from observational data only. We study this problem, considering multiple interventions, in the context of time series when only an abstraction of the true causal graph, in the form of a summary causal graph, is available. We propose in particular both necessary and sufficient conditions for the adjustment criterion, which we show is complete in this setting, and provide a pseudo-linear algorithm to decide whether the query is identifiable or not.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [224] [A Survey of Physics-Informed AI for Complex Urban Systems](https://arxiv.org/abs/2506.13777)
*En Xu,Huandong Wang,Yunke Zhang,Sibo Li,Yinzhou Tang,Zhilun Zhou,Yuming Lin,Yuan Yuan,Xiaochen Fan,Jingtao Ding,Yong Li*

Key words: 城市系统，物理学，人工智能，混合模型，应用领域

TL;DR: 综述了物理学与人工智能（AI）结合在复杂城市系统中的应用，分类了三种范式及七种方法，并探讨了其在实际中的应用和未来研究方向。

<details>
  <summary>Details</summary>

Main category: physics.soc-ph

Motivation: 城市系统是复杂系统的典型代表，结合物理学模型与AI可以提高预测准确性、可解释性和决策能力。

Method: 提出分类法将现有方法分为三类：物理学集成AI、物理学-AI混合集成、AI集成物理学，并详细介绍了七种代表性方法。

Result: 方法在能源、环境、经济等八个城市领域中得到应用，提升了系统的可靠性、效率和适应性。

Conclusion: 总结了现有方法及其应用，指出了关键差距并提出了未来研究方向，以推动智能城市系统建模的发展。

Abstract: Urban systems are typical examples of complex systems, where the integration of physics-based modeling with artificial intelligence (AI) presents a promising paradigm for enhancing predictive accuracy, interpretability, and decision-making. In this context, AI excels at capturing complex, nonlinear relationships, while physics-based models ensure consistency with real-world laws and provide interpretable insights. We provide a comprehensive review of physics-informed AI methods in urban applications. The proposed taxonomy categorizes existing approaches into three paradigms - Physics-Integrated AI, Physics-AI Hybrid Ensemble, and AI-Integrated Physics - and further details seven representative methods. This classification clarifies the varying degrees and directions of physics-AI integration, guiding the selection and development of appropriate methods based on application needs and data availability. We systematically examine their applications across eight key urban domains: energy, environment, economy, transportation, information, public services, emergency management, and the urban system as a whole. Our analysis highlights how these methodologies leverage physical laws and data-driven models to address urban challenges, enhancing system reliability, efficiency, and adaptability. By synthesizing existing methodologies and their urban applications, we identify critical gaps and outline future research directions, paving the way toward next-generation intelligent urban system modeling.

</details>


### [225] [Infected Smallville: How Disease Threat Shapes Sociality in LLM Agents](https://arxiv.org/abs/2506.13783)
*Soyeon Choi,Kangwook Lee,Oliver Sng,Joshua M. Ackerman*

Key words: 生成代理模型, 传染病, 社交行为, 行为免疫系统, 大语言模型

TL;DR: 该研究通过生成代理模型（GABM）结合大语言模型，模拟了传染病威胁对社交行为的影响，发现代理人在得知疫情消息后会显著减少社交活动。

<details>
  <summary>Details</summary>

Main category: physics.soc-ph

Motivation: 探讨传染病威胁如何影响生成代理人的社交行为，验证行为免疫系统假说。

Method: 使用生成代理模型（GABM）和大语言模型进行三组模拟实验，比较代理人接收到疫情消息与否的社交行为差异。

Result: 接收到疫情消息的代理人社交参与度显著降低，包括减少社交聚会、公共场所访问和对话。

Conclusion: GABM是一种有效的实验工具，可用于大规模探索人类复杂社交动态，验证了疾病避免动机对社交行为的影响。

Abstract: How does the threat of infectious disease influence sociality among generative agents? We used generative agent-based modeling (GABM), powered by large language models, to experimentally test hypotheses about the behavioral immune system. Across three simulation runs, generative agents who read news about an infectious disease outbreak showed significantly reduced social engagement compared to agents who received no such news, including lower attendance at a social gathering, fewer visits to third places (e.g., cafe, store, park), and fewer conversations throughout the town. In interview responses, agents explicitly attributed their behavioral changes to disease-avoidance motivations. A validity check further indicated that they could distinguish between infectious and noninfectious diseases, selectively reducing social engagement only when there was a risk of infection. Our findings highlight the potential of GABM as an experimental tool for exploring complex human social dynamics at scale.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [226] [Reimagining Target-Aware Molecular Generation through Retrieval-Enhanced Aligned Diffusion](https://arxiv.org/abs/2506.14488)
*Dong Xu,Zhangfan Yang,Ka-chun Wong,Zexuan Zhu,Jiangqiang Li,Junkai Ji*

Key words: READ, 蛋白质结构预测, 分子检索增强生成, SE(3)-等变扩散模型, 药物设计

TL;DR: READ是一种结合分子检索增强生成与SE(3)-等变扩散模型的新方法，通过对比预训练编码器和检索口袋匹配的支架图嵌入，生成有效、多样且形状互补的配体。

<details>
  <summary>Details</summary>

Main category: q-bio.BM

Motivation: 解决现有方法在蛋白质结构预测中难以平衡几何拟合与化学约束的问题。

Method: 采用检索增强对齐扩散（READ）模型，结合分子检索增强生成与SE(3)-等变扩散模型，利用预训练编码器检索口袋匹配的支架图嵌入。

Result: READ在CBGBench中表现优异，超越现有生成模型和天然配体。

Conclusion: 检索与扩散的联合优化能加速和提升结构药物设计的可靠性。

Abstract: Breakthroughs in high-accuracy protein structure prediction, such as AlphaFold, have established receptor-based molecule design as a critical driver for rapid early-phase drug discovery. However, most approaches still struggle to balance pocket-specific geometric fit with strict valence and synthetic constraints. To resolve this trade-off, a Retrieval-Enhanced Aligned Diffusion termed READ is introduced, which is the first to merge molecular Retrieval-Augmented Generation with an SE(3)-equivariant diffusion model. Specifically, a contrastively pre-trained encoder aligns atom-level representations during training, then retrieves graph embeddings of pocket-matched scaffolds to guide each reverse-diffusion step at inference. This single mechanism can inject real-world chemical priors exactly where needed, producing valid, diverse, and shape-complementary ligands. Experimental results demonstrate that READ can achieve very competitive performance in CBGBench, surpassing state-of-the-art generative models and even native ligands. That suggests retrieval and diffusion can be co-optimized for faster, more reliable structure-based drug design.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [227] [Safe Domains of Attraction for Discrete-Time Nonlinear Systems: Characterization and Verifiable Neural Network Estimation](https://arxiv.org/abs/2506.13961)
*Mohamed Serry,Haoyu Li,Ruikun Zhou,Huan Zhang,Jun Liu*

Key words: 非线性系统, 吸引域, Zubov方程, 神经网络, 验证框架

TL;DR: 提出了一种估计离散时间自主非线性系统安全吸引域的新框架，基于Zubov方程和神经网络近似求解，并通过验证工具实现可认证的估计。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 非线性自主系统的吸引域估计是一个长期挑战，尤其在考虑状态约束时更为复杂，现有方法保守或受限，需要更精确的解决方案。

Method: 推导新的Zubov方程以确定精确安全吸引域，提出物理启发的神经网络近似求解方法，并设计验证框架以确保估计的可信性。

Result: 证明了Zubov方程解的唯一性和连续性，并通过数值示例验证了该框架在状态约束非线性系统中的有效性。

Conclusion: 该框架为复杂非线性系统的安全吸引域估计提供了准确且可验证的新途径。

Abstract: Analysis of nonlinear autonomous systems typically involves estimating domains of attraction, which have been a topic of extensive research interest for decades. Despite that, accurately estimating domains of attraction for nonlinear systems remains a challenging task, where existing methods are conservative or limited to low-dimensional systems. The estimation becomes even more challenging when accounting for state constraints. In this work, we propose a framework to accurately estimate safe (state-constrained) domains of attraction for discrete-time autonomous nonlinear systems. In establishing this framework, we first derive a new Zubov equation, whose solution corresponds to the exact safe domain of attraction. The solution to the aforementioned Zubov equation is shown to be unique and continuous over the whole state space. We then present a physics-informed approach to approximating the solution of the Zubov equation using neural networks. To obtain certifiable estimates of the domain of attraction from the neural network approximate solutions, we propose a verification framework that can be implemented using standard verification tools (e.g., $α,\!β$-CROWN and dReal). To illustrate its effectiveness, we demonstrate our approach through numerical examples concerning nonlinear systems with state constraints.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [228] [Agile Orchestration at Will: An Entire Smart Service-Based Security Architecture Towards 6G](https://arxiv.org/abs/2505.22963)
*Zhuoran Duan,Guoshun Nan,Rushan Li,Zijun Wang,Lihua Xiong,Chaoying Yuan,Guorong Liu,Hui Xu,Qimei Cui,Xiaofeng Tao,Tony Q. S. Quek*

Key words: 6G网络, 安全架构, ES3A, 端到端保护, 智能安全自动化

TL;DR: 6G网络的安全和韧性是其设计的核心原则。作者提出了一种名为ES3A的新型安全架构，通过层次化、灵活性、可扩展性等六项高级原则和三条部署指南，为6G网络提供基于服务的端到端保护和智能安全自动化。

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 6G网络的多样化和高动态性带来了已知和未知的安全威胁，需要重新设计安全架构以应对这些挑战。

Method: 提出ES3A安全架构，包含三层次、三领域和两阶段编排机制，并基于软件定义无线电（SDR）进行了原型实现。

Result: 实验验证了ES3A的有效性，并通过案例展示了其优越性。

Conclusion: ES3A为6G网络提供了全面的安全保护，解决了高动态网络环境中的安全挑战。

Abstract: The upcoming 6G will fundamentally reshape mobile networks beyond communications, unlocking a multitude of applications that were once considered unimaginable. Meanwhile, security and resilience are especially highlighted in the 6G design principles. However, safeguarding 6G networks will be quite challenging due to various known and unknown threats from highly heterogeneous networks and diversified security requirements of distinct use cases, calling for a comprehensive re-design of security architecture. This motivates us to propose ES3A (Entire Smart Service-based Security Architecture), a novel security architecture for 6G networks. Specifically, we first discuss six high-level principles of our ES3A that include hierarchy, flexibility, scalability, resilience, endogeny, and trust and privacy. With these goals in mind, we then introduce three guidelines from a deployment perspective, envisioning our ES3A that offers service-based security, end-to-end protection, and smart security automation for 6G networks. Our architecture consists of three layers and three domains. It relies on a two-stage orchestration mechanism to tailor smart security strategies for customized protection in high-dynamic 6G networks, thereby addressing the aforementioned challenges. Finally, we prototype the proposed ES3A on a real-world radio system based on Software-Defined Radio (SDR). Experiments show the effectiveness of our ES3A. We also provide a case to show the superiority of our architecture.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [229] [Evolutionary chemical learning in dimerization networks](https://arxiv.org/abs/2506.14006)
*Alexei V. Tkachenko,Bortolo Matteo Mognetti,Sergei Maslov*

Key words: 竞争性二聚体网络（CDNs）、化学学习、定向进化、分子计算、合成生物学

TL;DR: 提出基于竞争性二聚体网络（CDNs）的化学学习框架，通过定向进化训练实现多类分类，无需数字硬件或显式参数调整，展示了强大的分类性能和高输入输出互信息。

<details>
  <summary>Details</summary>

Main category: cond-mat.stat-mech

Motivation: 结合合成生物学与机器学习，开发适应性强的、高能效的分子计算系统，以解决复杂学习任务。

Method: 利用竞争性二聚体网络（CDNs），通过定向进化（突变、选择和扩增DNA组件）训练网络，绑定亲和力作为可调突触权重。

Result: CDNs能够稳健区分噪声输入模式，分类器输出对比度高，输入输出互信息强，性能与梯度下降训练相近。

Conclusion: CDNs为模拟物理计算提供了有前景的平台，推动了适应性分子计算系统的发展。

Abstract: We present a novel framework for chemical learning based on Competitive Dimerization Networks (CDNs) - systems in which multiple molecular species, e.g. proteins or DNA/RNA oligomers, reversibly bind to form dimers. We show that these networks can be trained in vitro through directed evolution, enabling the implementation of complex learning tasks such as multiclass classification without digital hardware or explicit parameter tuning. Each molecular species functions analogously to a neuron, with binding affinities acting as tunable synaptic weights. A training protocol involving mutation, selection, and amplification of DNA-based components allows CDNs to robustly discriminate among noisy input patterns. The resulting classifiers exhibit strong output contrast and high mutual information between input and output, especially when guided by a contrast-enhancing loss function. Comparative analysis with in silico gradient descent training reveals closely correlated performance. These results establish CDNs as a promising platform for analog physical computation, bridging synthetic biology and machine learning, and advancing the development of adaptive, energy-efficient molecular computing systems.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [230] [Analysis and Optimization of Probabilities of Beneficial Mutation and Crossover Recombination in a Hamming Space](https://arxiv.org/abs/2506.13809)
*Roman V. Belavkin*

Key words: Fisher几何方法, 有益突变, 交叉重组, Hamming空间, 进化优化

TL;DR: 论文通过几何和组合分析，在一般Hamming空间中研究有益突变和交叉重组的概率，推导了接近最优解的转移概率，并优化了突变和重组参数。

<details>
  <summary>Details</summary>

Main category: q-bio.PE

Motivation: 受Fisher几何方法的启发，研究如何通过突变和重组优化遗传算法在Hamming空间中的表现。

Method: 使用几何和组合分析方法，推导了Hamming空间中接近最优解的转移概率，并给出突变和重组参数的优化条件。

Result: 突变和重组在搜索空间中表现出不同特性：突变概率随距离递减，而重组可以补充突变，加速接近最优解。

Conclusion: 重组可以作为突变的补充，提升进化速率，尤其在接近最优解时表现更优。

Abstract: Inspired by Fisher's geometric approach to study beneficial mutations, we analyse probabilities of beneficial mutation and crossover recombination of strings in a general Hamming space with arbitrary finite alphabet. Mutations and recombinations that reduce the distance to an optimum are considered as beneficial. Geometric and combinatorial analysis is used to derive closed-form expressions for transition probabilities between spheres around an optimum giving a complete description of Markov evolution of distances from an optimum over multiple generations. This paves the way for optimization of parameters of mutation and recombination operators. Here we derive optimality conditions for mutation and recombination radii maximizing the probabilities of mutation and crossover into the optimum. The analysis highlights important differences between these evolutionary operators. While mutation can potentially reach any part of the search space, the probability of beneficial mutation decreases with distance to an optimum, and the optimal mutation radius or rate should also decrease resulting in a slow-down of evolution near the optimum. Crossover recombination, on the other hand, acts in a subspace of the search space defined by the current population of strings. However, probabilities of beneficial and deleterious crossover are balanced, and their characteristics, such as variance, are translation invariant in a Hamming space, suggesting that recombination may complement mutation and boost the rate of evolution near the optimum.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [231] [Balancing Preservation and Modification: A Region and Semantic Aware Metric for Instruction-Based Image Editing](https://arxiv.org/abs/2506.13827)
*Zhuoying Li,Zhu Xu,Yuxin Peng,Yang Liu*

Key words: 图像编辑、指令编辑、评估指标、BPM、区域分离

TL;DR: 提出了一种名为BPM的新指标，专为基于指令的图像编辑设计，通过显式分离图像的编辑相关和无关区域，综合评估编辑质量和无关内容的保留。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 现有指标要么需要高昂的人工评估成本，要么是从其他任务中借用的，无法全面评估基于指令的修改和无关区域的保留。

Method: BPM通过识别和定位编辑相关区域，采用两级评估过程：区域感知评估（位置和大小）和语义感知评估（指令内容合规性和无关区域的内容保留）。

Result: BPM在综合指令编辑数据上的表现优于现有指标，与人工评估的一致性最高。

Conclusion: BPM为基于指令的图像编辑提供了一种综合且可解释的评估方法，并能集成到编辑方法中提升编辑质量。

Abstract: Instruction-based image editing, which aims to modify the image faithfully according to the instruction while preserving irrelevant content unchanged, has made significant progress. However, there still lacks a comprehensive metric for assessing the editing quality. Existing metrics either require high human evaluation costs, which hinder large-scale evaluation, or are adapted from other tasks and lose task-specific concerns, failing to comprehensively evaluate both instruction-based modification and preservation of irrelevant regions, resulting in biased evaluation. To tackle this, we introduce a new metric called Balancing Preservation and Modification (BPM), tailored for instruction-based image editing by explicitly disentangling the image into editing-relevant and irrelevant regions for specific consideration. We first identify and locate editing-relevant regions, followed by a two-tier process to assess editing quality: Region-Aware Judge evaluates whether the position and size of the edited region align with the instruction, and Semantic-Aware Judge further assesses the instruction content compliance within editing-relevant regions as well as content preservation within irrelevant regions, yielding comprehensive and interpretable quality assessment. Moreover, the editing-relevant region localization in BPM can be integrated into image editing approaches to improve editing quality, demonstrating its broad applicability. We verify the effectiveness of the BPM metric on comprehensive instruction-editing data, and the results show the highest alignment with human evaluation compared to existing metrics, indicating its efficacy. Code is available at: https://joyli-x.github.io/BPM/

</details>


### [232] [Innovating China's Intangible Cultural Heritage with DeepSeek + MidJourney: The Case of Yangliuqing theme Woodblock Prints](https://arxiv.org/abs/2506.14104)
*RuiKun Yang,ZhongLiang Wei,Longdi Xian*

Key words: 杨柳青木版年画，DeepSeek，MidJourney，FID，传统文化

TL;DR: 研究了结合DeepSeek和MidJourney生成杨柳青木版年画的方法，主题为抗疫和欢乐胜利者，通过FID评分和问卷调查评估效果。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 保护和创新杨柳青木版年画这一非物质文化遗产，探索现代AI技术与传统艺术的融合。

Method: 使用DeepSeek生成主题提示，MidJourney生成图像，结合原始画作和关键提示，评估FID分数。

Result: 混合方法FID得分最低（150.2），问卷显示其最具代表性，参与者更愿意推广传统文化。

Conclusion: AI与传统艺术的结合有效，既保护文化又具现代意义。

Abstract: Yangliuqing woodblock prints, a cornerstone of China's intangible cultural heritage, are celebrated for their intricate designs and vibrant colors. However, preserving these traditional art forms while fostering innovation presents significant challenges. This study explores the DeepSeek + MidJourney approach to generating creative, themed Yangliuqing woodblock prints focused on the fight against COVID-19 and depicting joyous winners. Using Fréchet Inception Distance (FID) scores for evaluation, the method that combined DeepSeek-generated thematic prompts, MidJourney-generated thematic images, original Yangliuqing prints, and DeepSeek-generated key prompts in MidJourney-generated outputs achieved the lowest mean FID score (150.2) with minimal variability (σ = 4.9). Additionally, feedback from 62 participants, collected via questionnaires, confirmed that this hybrid approach produced the most representative results. Moreover, the questionnaire data revealed that participants demonstrated the highest willingness to promote traditional culture and the strongest interest in consuming the AI-generated images produced through this method. These findings underscore the effectiveness of an innovative approach that seamlessly blends traditional artistic elements with modern AI-driven creativity, ensuring both cultural preservation and contemporary relevance.

</details>


### [233] [ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering](https://arxiv.org/abs/2506.13814)
*Lufei Liu,Tor M. Aamodt*

Key words: 实时渲染；神经网络；缓存策略；性能优化

TL;DR: ReFrame通过缓存中间特征优化实时渲染任务，实现1.4倍加速且质量损失可忽略。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 利用神经网络的时序连贯性，避免冗余计算以提升渲染效率。

Method: 扩展中间特征缓存方法至实时渲染，探索不同缓存策略以平衡质量和性能。

Result: 在三种实时渲染任务中平均加速1.4倍，质量损失可忽略。

Conclusion: ReFrame适用于多种编码器-解码器网络，有效提升渲染性能。

Abstract: Graphics rendering applications increasingly leverage neural networks in tasks such as denoising, supersampling, and frame extrapolation to improve image quality while maintaining frame rates. The temporal coherence inherent in these tasks presents an opportunity to reuse intermediate results from previous frames and avoid redundant computations. Recent work has shown that caching intermediate features to be reused in subsequent inferences is an effective method to reduce latency in diffusion models. We extend this idea to real-time rendering and present ReFrame, which explores different caching policies to optimize trade-offs between quality and performance in rendering workloads. ReFrame can be applied to a variety of encoder-decoder style networks commonly found in rendering pipelines. Experimental results show that we achieve 1.4x speedup on average with negligible quality loss in three real-time rendering tasks. Code available: https://ubc-aamodt-group.github.io/reframe-layer-caching/

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [234] [NeuroCoreX: An Open-Source FPGA-Based Spiking Neural Network Emulator with On-Chip Learning](https://arxiv.org/abs/2506.14138)
*Ashish Gautam,Prasanna Date,Shruti Kulkarni,Robert Patton,Thomas Potok*

Key words: SNN, FPGA, STDP, LIF, 神经形态计算

TL;DR: NeuroCoreX是一种基于FPGA的SNN仿真器，支持多样化网络拓扑和生物启发的学习方法，旨在推动高效能计算研究。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 开发灵活、高效的SNN仿真平台，以支持神经形态计算的研究与应用。

Method: 采用FPGA硬件，实现全连接网络与STDP学习机制，基于LIF神经元模型，通过Python接口简化操作。

Result: NeuroCoreX成功实现了灵活的网络拓扑设计和生物启发的学习，并开源以促进研究。

Conclusion: NeuroCoreX为SNN研究提供了高效工具，推动了生物启发的计算技术发展。

Abstract: Spiking Neural Networks (SNNs) are computational models inspired by the structure and dynamics of biological neuronal networks. Their event-driven nature enables them to achieve high energy efficiency, particularly when deployed on neuromorphic hardware platforms. Unlike conventional Artificial Neural Networks (ANNs), which primarily rely on layered architectures, SNNs naturally support a wide range of connectivity patterns, from traditional layered structures to small-world graphs characterized by locally dense and globally sparse connections. In this work, we introduce NeuroCoreX, an FPGA-based emulator designed for the flexible co-design and testing of SNNs. NeuroCoreX supports all-to-all connectivity, providing the capability to implement diverse network topologies without architectural restrictions. It features a biologically motivated local learning mechanism based on Spike-Timing-Dependent Plasticity (STDP). The neuron model implemented within NeuroCoreX is the Leaky Integrate-and-Fire (LIF) model, with current-based synapses facilitating spike integration and transmission . A Universal Asynchronous Receiver-Transmitter (UART) interface is provided for programming and configuring the network parameters, including neuron, synapse, and learning rule settings. Users interact with the emulator through a simple Python-based interface, streamlining SNN deployment from model design to hardware execution. NeuroCoreX is released as an open-source framework, aiming to accelerate research and development in energy-efficient, biologically inspired computing.

</details>


### [235] [A Scalable Hybrid Training Approach for Recurrent Spiking Neural Networks](https://arxiv.org/abs/2506.14464)
*Maximilian Baronig,Yeganeh Bahariasl,Ozan Özdenizci,Robert Legenstein*

Key words: 循环脉冲神经网络（RSNNs）, 在线学习, 前向传播, 并行化, HYPR

TL;DR: 本文介绍了一种名为HYPR的混合传播方法，结合了并行化的高效性和近似在线前向学习的优点，解决了传统BPTT方法在记忆消耗和在线训练上的限制。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 传统的BPTT方法在训练循环脉冲神经网络(RSNNs)时存在记忆消耗大且不支持在线训练的问题，同时现有的前向传播方法性能较差且运行速度慢。

Method: 提出了HYBR方法，通过并行化和近似在线前向学习，实现了高吞吐量的在线学习，记忆需求独立于序列长度。

Result: HYPR在具有振荡亚阈值动态的脉冲神经元网络中表现优异，任务性能接近于BPTT，显著缩小了前向学习和BPTT之间的差距。

Conclusion: HYPR为RSNNs提供了一种高效、低记忆消耗的在线训练方法，特别适用于复杂神经元模型。

Abstract: Recurrent spiking neural networks (RSNNs) can be implemented very efficiently in neuromorphic systems. Nevertheless, training of these models with powerful gradient-based learning algorithms is mostly performed on standard digital hardware using Backpropagation through time (BPTT). However, BPTT has substantial limitations. It does not permit online training and its memory consumption scales linearly with the number of computation steps. In contrast, learning methods using forward propagation of gradients operate in an online manner with a memory consumption independent of the number of time steps. These methods enable SNNs to learn from continuous, infinite-length input sequences. Yet, slow execution speed on conventional hardware as well as inferior performance has hindered their widespread application. In this work, we introduce HYbrid PRopagation (HYPR) that combines the efficiency of parallelization with approximate online forward learning. Our algorithm yields high-throughput online learning through parallelization, paired with constant, i.e., sequence length independent, memory demands. HYPR enables parallelization of parameter update computation over the sub sequences for RSNNs consisting of almost arbitrary non-linear spiking neuron models. We apply HYPR to networks of spiking neurons with oscillatory subthreshold dynamics. We find that this type of neuron model is particularly well trainable by HYPR, resulting in an unprecedentedly low task performance gap between approximate forward gradient learning and BPTT.

</details>


### [236] [Is Selection All You Need in Differential Evolution?](https://arxiv.org/abs/2506.14425)
*Tomofumi Kitamura,Alex Fukunaga*

Key words: 差分进化, 种群多样性, 无界差分进化, 存档机制, 优化算法

TL;DR: 论文提出了无界差分进化（UDE）框架，通过取消世代替换和存档管理，简化差分进化算法，提升性能。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 传统差分进化算法因固定种群大小和世代替换导致种群多样性受限，存档机制复杂化设计。

Method: 提出UDE框架，保留所有候选个体，取消世代替换，仅依赖选择机制。

Result: UDE简化算法设计，避免存档管理复杂性，提升搜索能力。

Conclusion: UDE是一种新的差分进化方法，通过取消世代替换简化算法，同时保持高效搜索能力。

Abstract: Differential Evolution (DE) is a widely used evolutionary algorithm for black-box optimization problems. However, in modern DE implementations, a major challenge lies in the limited population diversity caused by the fixed population size enforced by the generational replacement. Population size is a critical control parameter that significantly affects DE performance. Larger populations inherently contain a more diverse set of individuals, thereby facilitating broader exploration of the search space. Conversely, when the maximum evaluation budgets is constrained, smaller populations focusing on a limited number of promising candidates may be more suitable. Many state-of-the-art DE variants incorporate an archive mechanism, in which a subset of discarded individuals is preserved in an archive during generation replacement and reused in mutation operations. However, maintaining what is essentially a secondary population via an archive introduces additional design considerations, such as policies for insertion, deletion, and appropriate sizing. To address these limitations, we propose a novel DE framework called Unbounded Differential Evolution (UDE), which adds all generated candidates to the population without discarding any individual based on fitness. Unlike conventional DE, which removes inferior individuals during generational replacement, UDE eliminates replacement altogether, along with the associated complexities of archive management and dynamic population sizing. UDE represents a fundamentally new approach to DE, relying solely on selection mechanisms and enabling a more straightforward yet powerful search algorithm.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [237] [Accurate and scalable exchange-correlation with deep learning](https://arxiv.org/abs/2506.14665)
*Giulia Luise,Chin-Wei Huang,Thijs Vogels,Derk P. Kooi,Sebastian Ehlert,Stephanie Lanius,Klaas J. H. Giesbertz,Amir Karton,Deniz Gunceler,Megan Stanley,Wessel P. Bruinsma,Lin Huang,Xinran Wei,José Garrido Torres,Abylay Katbashev,Bálint Máté,Sékou-Oumar Kaba,Roberto Sordillo,Yingrong Chen,David B. Williams-Young,Christopher M. Bishop,Jan Hermann,Rianne van den Berg,Paola Gori-Giorgi*

Key words: 密度泛函理论，交换相关泛函，深度学习，化学精度，Skala

TL;DR: Skala是一种基于深度学习的交换相关泛函，通过直接从数据中学习表示，避免了手工设计特征的昂贵成本，达到了化学精度，并保持了半局部DFT的计算效率。

<details>
  <summary>Details</summary>

Main category: physics.chem-ph

Motivation: 现有的交换相关泛函（XC）在预测实验室实验的化学精度（误差低于1 kcal/mol）方面存在局限性，需要更准确且通用的方法。

Method: 利用深度学习技术直接从数据中学习表示，避免了手工设计特征的复杂性，并在大量高精度参考数据上进行训练。

Result: Skala在小分子的原子化能上达到了化学精度，并在通用主族化学中表现出与最佳混合泛函竞争的准确性。

Conclusion: 随着训练数据的增加，Skala有望进一步提升第一性原理模拟的预测能力。

Abstract: Density Functional Theory (DFT) is the most widely used electronic structure method for predicting the properties of molecules and materials. Although DFT is, in principle, an exact reformulation of the Schrödinger equation, practical applications rely on approximations to the unknown exchange-correlation (XC) functional. Most existing XC functionals are constructed using a limited set of increasingly complex, hand-crafted features that improve accuracy at the expense of computational efficiency. Yet, no current approximation achieves the accuracy and generality for predictive modeling of laboratory experiments at chemical accuracy -- typically defined as errors below 1 kcal/mol. In this work, we present Skala, a modern deep learning-based XC functional that bypasses expensive hand-designed features by learning representations directly from data. Skala achieves chemical accuracy for atomization energies of small molecules while retaining the computational efficiency typical of semi-local DFT. This performance is enabled by training on an unprecedented volume of high-accuracy reference data generated using computationally intensive wavefunction-based methods. Notably, Skala systematically improves with additional training data covering diverse chemistry. By incorporating a modest amount of additional high-accuracy data tailored to chemistry beyond atomization energies, Skala achieves accuracy competitive with the best-performing hybrid functionals across general main group chemistry, at the cost of semi-local DFT. As the training dataset continues to expand, Skala is poised to further enhance the predictive power of first-principles simulations.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [238] [Asymptotically Smaller Encodings for Graph Problems and Scheduling](https://arxiv.org/abs/2506.14042)
*Bernardo Subercaseaux*

Key words: 图问题、CNF编码、独立集、调度问题、压缩字符串

TL;DR: 论文介绍了一种将图问题（如顶点覆盖、独立集、k-着色）编码为CNF的新方法，仅需O(|V|^2/lg|V|)条款，优于传统O(|V|^2)方法。此外，针对密集区间图的独立集问题提出了仅需O(|V|lg|V|)条款的编码，并成功应用于压缩字符串编码和调度问题。

<details>
  <summary>Details</summary>

Main category: cs.LO

Motivation: 解决传统图问题编码方法中条款过多的问题，提出更高效的编码方式，减少计算复杂度。

Method: 利用Erdős等人的双团覆盖结果，并结合“有界变量加法”预处理工具，设计新的CNF编码方法。

Result: 显著减少了编码所需的条款数量，成功应用于压缩字符串和调度问题，提升效率。

Conclusion: 新编码方法在理论和实践上均优于传统方法，为高效解决图问题提供了新思路。

Abstract: We show how several graph problems (e.g., vertex-cover, independent-set, $k$-coloring) can be encoded into CNF using only $O(|V|^2 / \lg |V|)$ many clauses, as opposed to the $Ω(|V|^2)$ constraints used by standard encodings. This somewhat surprising result is a simple consequence of a result of Erdős, Chung, and Spencer (1983) about biclique coverings of graphs, and opens theoretical avenues to understand the success of "Bounded Variable Addition'' (Manthey, Heule, and Biere, 2012) as a preprocessing tool. Finally, we show a novel encoding for independent sets in some dense interval graphs using only $O(|V| \lg |V|)$ clauses (the direct encoding uses $Ω(|V|^2)$), which we have successfully applied to a string-compression encoding posed by Bannai et al. (2022). As a direct byproduct, we obtain a reduction in the encoding size of a scheduling problem posed by Mayank and Modal (2020) from $O(NMT^2)$ to $O(NMT + M T^2 \lg T)$, where $N$ is the number of tasks, $T$ the total timespan, and $M$ the number of machines.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [239] [A Silent Speech Decoding System from EEG and EMG with Heterogenous Electrode Configurations](https://arxiv.org/abs/2506.13835)
*Masakazu Inoue,Motoshige Sato,Kenichi Tomeoka,Nathania Nah,Eri Hatakeyama,Kai Arulkumaran,Ilya Horiguchi,Shuntaro Sasai*

Key words: 无声语音解码, EEG/EMG, 多任务训练, 神经网络, 言语障碍

TL;DR: 该研究提出了一种神经网络模型，用于处理异质电极位置的EEG/EMG数据，并通过多任务训练在大规模数据集上实现了无声语音解码的高性能。

<details>
  <summary>Details</summary>

Main category: q-bio.QM

Motivation: 无声语音解码通过脑电图/肌电图识别未发声的人类语音，为言语障碍者提供帮助。但由于数据收集困难且实验设置多样，难以获取大规模同质数据集。

Method: 研究引入了一种神经网络，能够处理不同电极排列的EEG/EMG数据，并利用多任务训练在大规模数据集上进行模型训练。

Result: 模型在健康参与者中达到95.3%的词分类准确率，在言语障碍患者中达到54.5%，显著优于单一受试者数据训练的模型（70.1%和13.2%）。跨语言校准性能也有所提升。

Conclusion: 该研究表明，开发实用的无声语音解码系统，特别是为言语障碍患者提供服务的可行性得到了提升。

Abstract: Silent speech decoding, which performs unvocalized human speech recognition from electroencephalography/electromyography (EEG/EMG), increases accessibility for speech-impaired humans. However, data collection is difficult and performed using varying experimental setups, making it nontrivial to collect a large, homogeneous dataset. In this study we introduce neural networks that can handle EEG/EMG with heterogeneous electrode placements and show strong performance in silent speech decoding via multi-task training on large-scale EEG/EMG datasets. We achieve improved word classification accuracy in both healthy participants (95.3%), and a speech-impaired patient (54.5%), substantially outperforming models trained on single-subject data (70.1% and 13.2%). Moreover, our models also show gains in cross-language calibration performance. This increase in accuracy suggests the feasibility of developing practical silent speech decoding systems, particularly for speech-impaired patients.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [240] [Beyond Shapley Values: Cooperative Games for the Interpretation of Machine Learning Models](https://arxiv.org/abs/2506.13900)
*Marouane Il Idrissi,Agathe Fernandes Machado,Arthur Charpentier*

Key words: 合作博弈论, Shapley值, 特征归因, Weber集合, Harsanyi集合

TL;DR: 本文重新审视合作博弈论在可解释性中的应用，提出更广泛和有原则的工具使用，扩展了Shapley值之外的分配方案。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: Shapley值虽然广泛应用，但其公理基础对特征归因的适用性存在争议，需要更可靠的理论框架。

Method: 介绍了Weber和Harsanyi集合作为扩展分配方案，并提出了三步框架以构建可靠的特征归因方法。

Result: 提出了更灵活和理论可靠的特征归因方法，为XAI社区提供了新工具。

Conclusion: 需要超越固定公理，为特征归因设计提供连贯框架，适应方法趋势变化。

Abstract: Cooperative game theory has become a cornerstone of post-hoc interpretability in machine learning, largely through the use of Shapley values. Yet, despite their widespread adoption, Shapley-based methods often rest on axiomatic justifications whose relevance to feature attribution remains debatable. In this paper, we revisit cooperative game theory from an interpretability perspective and argue for a broader and more principled use of its tools. We highlight two general families of efficient allocations, the Weber and Harsanyi sets, that extend beyond Shapley values and offer richer interpretative flexibility. We present an accessible overview of these allocation schemes, clarify the distinction between value functions and aggregation rules, and introduce a three-step blueprint for constructing reliable and theoretically-grounded feature attributions. Our goal is to move beyond fixed axioms and provide the XAI community with a coherent framework to design attribution methods that are both meaningful and robust to shifting methodological trends.

</details>


### [241] [Mirror Descent Using the Tempesta Generalized Multi-parametric Logarithms](https://arxiv.org/abs/2506.13984)
*Andrzej Cichocki*

Key words: 镜像下降,Bregman散度,Tempesta对数,多参数变形,广义熵

TL;DR: 本文提出了一种基于Tempesta多参数变形对数作为链接函数的广义镜像下降（MD）算法，用于解决约束优化问题，并通过学习超参数适应数据和算法需求。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 通过利用Bregman散度和Tempesta多参数变形对数作为链接函数，设计更灵活的MD算法，以适应数据的分布和几何特性。

Method: 使用Tempesta多参数变形对数及其逆的广义指数函数来推导MD更新，并通过调整超参数优化算法行为。

Result: 提出了一种新类别的MD更新，能够根据数据特性动态调整，提高了算法的适应性和性能。

Conclusion: 多参数变形对数的应用为MD算法提供了更广泛和灵活的设计空间，能够更好地适应实际需求。

Abstract: In this paper, we develop a wide class Mirror Descent (MD) algorithms, which play a key role in machine learning. For this purpose we formulated the constrained optimization problem, in which we exploits the Bregman divergence with the Tempesta multi-parametric deformation logarithm as a link function. This link function called also mirror function defines the mapping between the primal and dual spaces and is associated with a very-wide (in fact, theoretically infinite) class of generalized trace-form entropies. In order to derive novel MD updates, we estimate generalized exponential function, which closely approximates the inverse of the multi-parametric Tempesta generalized logarithm. The shape and properties of the Tempesta logarithm and its inverse-deformed exponential functions can be tuned by several hyperparameters. By learning these hyperparameters, we can adapt to distribution or geometry of training data, and we can adjust them to achieve desired properties of MD algorithms. The concept of applying multi-parametric logarithms allow us to generate a new wide and flexible family of MD and mirror-less MD updates.

</details>


### [242] [Rademacher learning rates for iterated random functions](https://arxiv.org/abs/2506.13946)
*Nikola Sandrić*

Key words: 监督学习, 非i.i.d.数据, 迭代随机函数, Rademacher复杂度, 学习率

TL;DR: 本文讨论了在监督学习中训练数据非独立同分布（i.i.d.）时的学习问题，提出了一种基于迭代随机函数的数据生成模型，并研究了其学习率。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现实中许多数据具有时间依赖性和相关性，传统的i.i.d.假设不适用，因此需要研究更通用的数据生成模型及其学习理论。

Method: 假设数据由具有收缩性的迭代随机函数生成，通过均匀收敛性和Rademacher复杂度分析，推导了学习率。

Result: 证明了近似经验风险最小化算法的可学习性，并给出了数据分布依赖的学习率界限。

Conclusion: 研究为非i.i.d.数据提供了新的学习理论框架，学习率能更准确地反映数据生成分布的特性。

Abstract: Most existing literature on supervised machine learning assumes that the training dataset is drawn from an i.i.d. sample. However, many real-world problems exhibit temporal dependence and strong correlations between the marginal distributions of the data-generating process, suggesting that the i.i.d. assumption is often unrealistic. In such cases, models naturally include time-series processes with mixing properties, as well as irreducible and aperiodic ergodic Markov chains. Moreover, the learning rates typically obtained in these settings are independent of the data distribution, which can lead to restrictive choices of hypothesis classes and suboptimal sample complexities for the learning algorithm. In this article, we consider the case where the training dataset is generated by an iterated random function (i.e., an iteratively defined time-homogeneous Markov chain) that is not necessarily irreducible or aperiodic. Under the assumption that the governing function is contractive with respect to its first argument and subject to certain regularity conditions on the hypothesis class, we first establish a uniform convergence result for the corresponding sample error. We then demonstrate the learnability of the approximate empirical risk minimization algorithm and derive its learning rate bound. Both rates are data-distribution dependent, expressed in terms of the Rademacher complexities of the underlying hypothesis class, allowing them to more accurately reflect the properties of the data-generating distribution.

</details>


### [243] [Meta Optimality for Demographic Parity Constrained Regression via Post-Processing](https://arxiv.org/abs/2506.13947)
*Kazuto Fukuchi*

Key words: 公平回归, 极小极大最优, 元定理, 后处理

TL;DR: 本文提出了验证公平极小极大最优回归算法通用性的元定理，并证明后处理方法可实现公平回归。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 解决公平性约束下的回归问题，特别是验证公平极小极大最优回归算法的普适性。

Method: 提出元定理以验证不同场景下的公平极小极大最优性，并展示通过后处理方法实现公平回归。

Result: 证明了公平极小极大最优回归可以通过后处理方法实现，简化了公平回归的实现。

Conclusion: 研究为公平回归提供了通用验证方法，并展示了后处理的实用性。

Abstract: We address the regression problem under the constraint of demographic parity, a commonly used fairness definition. Recent studies have revealed fair minimax optimal regression algorithms, the most accurate algorithms that adhere to the fairness constraint. However, these analyses are tightly coupled with specific data generation models. In this paper, we provide meta-theorems that can be applied to various situations to validate the fair minimax optimality of the corresponding regression algorithms. Furthermore, we demonstrate that fair minimax optimal regression can be achieved through post-processing methods, allowing researchers and practitioners to focus on improving conventional regression techniques, which can then be efficiently adapted for fair regression.

</details>


### [244] [Bridging Unsupervised and Semi-Supervised Anomaly Detection: A Theoretically-Grounded and Practical Framework with Synthetic Anomalies](https://arxiv.org/abs/2506.13955)
*Matthew Lau,Tian-Yi Zhou,Xiangchi Yuan,Jizhou Chen,Wenke Lee,Xiaoming Huo*

Key words: 半监督异常检测, 理论框架, 合成异常, 收敛性保证

TL;DR: 该论文提出了一种半监督异常检测框架，结合已知和合成的异常数据，首次给出了半监督异常检测的数学表述，并在理论和实验上验证了其有效性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 在许多领域（如网络安全和医疗健康），异常检测（AD）是一个关键任务。半监督异常检测可以利用有限的已知异常数据提升检测效果。

Method: 作者提出了一个理论和实验均有效的半监督AD框架，结合已知异常和合成的异常数据训练分类器，并首次提出了半监督AD的数学表述。

Result: 理论和实验结果表明，合成的异常数据能更好地建模低密度区域的异常，并为神经网络分类器提供最优收敛性保证。在五个不同基准测试中均取得了性能提升。

Conclusion: 该框架不仅在本研究中表现优异，还支持其他基于分类的AD方法，验证了合成异常数据在半监督AD中的普适性。

Abstract: Anomaly detection (AD) is a critical task across domains such as cybersecurity and healthcare. In the unsupervised setting, an effective and theoretically-grounded principle is to train classifiers to distinguish normal data from (synthetic) anomalies. We extend this principle to semi-supervised AD, where training data also include a limited labeled subset of anomalies possibly present in test time. We propose a theoretically-grounded and empirically effective framework for semi-supervised AD that combines known and synthetic anomalies during training. To analyze semi-supervised AD, we introduce the first mathematical formulation of semi-supervised AD, which generalizes unsupervised AD. Here, we show that synthetic anomalies enable (i) better anomaly modeling in low-density regions and (ii) optimal convergence guarantees for neural network classifiers -- the first theoretical result for semi-supervised AD. We empirically validate our framework on five diverse benchmarks, observing consistent performance gains. These improvements also extend beyond our theoretical framework to other classification-based AD methods, validating the generalizability of the synthetic anomaly principle in AD.

</details>


### [245] [Estimation of Treatment Effects in Extreme and Unobserved Data](https://arxiv.org/abs/2506.14051)
*Jiyuan Tan,Jose Blanchet,Vasilis Syrgkanis*

Key words: 因果效应估计；极值理论；多元正则变差；罕见事件；政策干预

TL;DR: 提出一种新框架，利用极值理论（EVT）和多元正则变差理论，估计罕见事件的政策干预效果。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现有因果推理方法主要关注常见事件的干预效果，但对罕见而重要的事件（如极端气候事件）的干预效果估计能力不足。

Method: 结合极值理论和多元正则变差理论建模极端事件，开发了一种一致性估计器，并对其性能进行了严格的非渐近分析。

Result: 通过合成和半合成数据验证了估计器的性能，能够有效估计罕见事件的干预效果。

Conclusion: 该框架填补了因果推理在罕见事件领域的空白，为极端条件下的政策干预效果评估提供了有效工具。

Abstract: Causal effect estimation seeks to determine the impact of an intervention from observational data. However, the existing causal inference literature primarily addresses treatment effects on frequently occurring events. But what if we are interested in estimating the effects of a policy intervention whose benefits, while potentially important, can only be observed and measured in rare yet impactful events, such as extreme climate events? The standard causal inference methodology is not designed for this type of inference since the events of interest may be scarce in the observed data and some degree of extrapolation is necessary. Extreme Value Theory (EVT) provides methodologies for analyzing statistical phenomena in such extreme regimes. We introduce a novel framework for assessing treatment effects in extreme data to capture the causal effect at the occurrence of rare events of interest. In particular, we employ the theory of multivariate regular variation to model extremities. We develop a consistent estimator for extreme treatment effects and present a rigorous non-asymptotic analysis of its performance. We illustrate the performance of our estimator using both synthetic and semi-synthetic data.

</details>


### [246] [Universal Rates of ERM for Agnostic Learning](https://arxiv.org/abs/2506.14110)
*Steve Hanneke,Mingyue Xu*

Key words: 通用学习, ERM, 二分类, 不可实现设置, 学习速率

TL;DR: 研究了在不可实现（agnostic）设置下，通过ERM进行二分类的通用学习问题，揭示了三种可能的通用学习速率，并给出了概念类别的完整分类。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现有的通用学习研究主要集中在可实现（realizable）设置，而实际应用中不可实现的情况更为常见，因此需要探索不可实现设置下的通用学习速率。

Method: 通过分析Empirical Risk Minimization（ERM）在不可实现二分类问题中的表现，研究其“学习曲线”反映的超额风险衰减，从而揭示通用速率的三种可能性。

Result: 发现不可实现设置下ERM的通用学习速率有三种：指数衰减（e^{-n}）、快于n^{-1/2}（o(n^{-1/2})），或任意慢速。并提供了每种速率对应概念类的完整分类。

Conclusion: 不可实现设置下的通用学习速率存在明确的分类，且研究扩展到了目标依赖和贝叶斯依赖的通用速率。

Abstract: The universal learning framework has been developed to obtain guarantees on the learning rates that hold for any fixed distribution, which can be much faster than the ones uniformly hold over all the distributions. Given that the Empirical Risk Minimization (ERM) principle being fundamental in the PAC theory and ubiquitous in practical machine learning, the recent work of arXiv:2412.02810 studied the universal rates of ERM for binary classification under the realizable setting. However, the assumption of realizability is too restrictive to hold in practice. Indeed, the majority of the literature on universal learning has focused on the realizable case, leaving the non-realizable case barely explored.
  In this paper, we consider the problem of universal learning by ERM for binary classification under the agnostic setting, where the ''learning curve" reflects the decay of the excess risk as the sample size increases. We explore the possibilities of agnostic universal rates and reveal a compact trichotomy: there are three possible agnostic universal rates of ERM, being either $e^{-n}$, $o(n^{-1/2})$, or arbitrarily slow. We provide a complete characterization of which concept classes fall into each of these categories. Moreover, we also establish complete characterizations for the target-dependent universal rates as well as the Bayes-dependent universal rates.

</details>


### [247] [Adjustment for Confounding using Pre-Trained Representations](https://arxiv.org/abs/2506.14329)
*Rickmer Schulte,David Rügamer,Thomas Nagler*

Key words: 平均处理效果（ATE）, 潜在特征, 神经网络, 双机器学习, 高维数据

TL;DR: 论文探讨了如何利用预训练神经网络的潜在特征来调整混杂因素，以改进平均处理效果（ATE）估计，并分析了高维和非可识别性带来的挑战及神经网络的适应性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 研究动机在于扩展ATE估计以纳入非表格数据（如图像和文本），避免因忽略这些混杂因素导致的偏差和错误结论。

Method: 利用预训练神经网络的潜在特征进行调整，结合双机器学习方法，分析潜在特征对ATE估计的调整作用。

Result: 研究表明神经网络能够通过适应问题的内在稀疏性和维度实现快速收敛，解决了高维和非可识别性带来的挑战。

Conclusion: 神经网络对潜在特征学习中的问题不敏感，能够有效改进ATE估计，为处理非表格数据提供了可行方法。

Abstract: There is growing interest in extending average treatment effect (ATE) estimation to incorporate non-tabular data, such as images and text, which may act as sources of confounding. Neglecting these effects risks biased results and flawed scientific conclusions. However, incorporating non-tabular data necessitates sophisticated feature extractors, often in combination with ideas of transfer learning. In this work, we investigate how latent features from pre-trained neural networks can be leveraged to adjust for sources of confounding. We formalize conditions under which these latent features enable valid adjustment and statistical inference in ATE estimation, demonstrating results along the example of double machine learning. We discuss critical challenges inherent to latent feature learning and downstream parameter estimation arising from the high dimensionality and non-identifiability of representations. Common structural assumptions for obtaining fast convergence rates with additive or sparse linear models are shown to be unrealistic for latent features. We argue, however, that neural networks are largely insensitive to these issues. In particular, we show that neural networks can achieve fast convergence rates by adapting to intrinsic notions of sparsity and dimension of the learning problem.

</details>


### [248] [Adaptive Data Augmentation for Thompson Sampling](https://arxiv.org/abs/2506.14479)
*Wonyoung Kim*

Key words: 线性上下文老虎机，Thompson Sampling，最小最大后悔，参数估计

TL;DR: 本文提出了一种几乎最优的Thompson Sampling方法，通过设计新型估计器在上下文线性老虎机问题中实现最小最大后悔。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: Thompson Sampling在实证中表现良好，但未达到理论上的最优后悔界，因此需要改进。

Method: 提出了一个新型估计器，通过自适应增强和假设样本的耦合来高效学习参数，且不依赖上下文分布的假设。

Result: 实证结果显示了稳健的性能，并显著优于现有方法。

Conclusion: 该方法在上下文线性老虎机问题中实现了几乎最优的后悔界，并具有实际应用的潜力。

Abstract: In linear contextual bandits, the objective is to select actions that maximize cumulative rewards, modeled as a linear function with unknown parameters. Although Thompson Sampling performs well empirically, it does not achieve optimal regret bounds. This paper proposes a nearly minimax optimal Thompson Sampling for linear contextual bandits by developing a novel estimator with the adaptive augmentation and coupling of the hypothetical samples that are designed for efficient parameter learning. The proposed estimator accurately predicts rewards for all arms without relying on assumptions for the context distribution. Empirical results show robust performance and significant improvement over existing methods.

</details>


### [249] [Sharp Generalization Bounds for Foundation Models with Asymmetric Randomized Low-Rank Adapters](https://arxiv.org/abs/2506.14530)
*Anastasis Kratsios,Tin Sum Cheng,Aurelien Lucchi,Haitz Sáez de Ocáriz Borde*

Key words: 低秩适应（LoRA）、参数高效微调（PEFT）、理论分析、样本复杂度

TL;DR: 本文对低秩适应（LoRA）的参数高效微调技术进行了理论分析，重点关注其初始化中不对称性的影响，并提供了上下界的样本复杂度证明。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: LoRA作为一种高效的微调方法，其初始化中的不对称性一直缺乏理论解释。本文旨在填补这一空白。

Method: 通过分析LoRA在固定随机因子下的泛化行为，研究了其泛化差距的集中性，并提出了样本复杂度的上下界。

Result: 证明了LoRA的样本复杂度上界为$	ilde{\mathcal{O}}\left(\frac{\sqrt{r}}{\sqrt{N}}\right)$，下界为$\mathcal{O}\left(\frac{1}{\sqrt{N}}\right)$。

Conclusion: 研究结果为不对称LoRA的可靠性和实用性提供了理论支持。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a widely adopted parameter-efficient fine-tuning (PEFT) technique for foundation models. Recent work has highlighted an inherent asymmetry in the initialization of LoRA's low-rank factors, which has been present since its inception and was presumably derived experimentally. This paper focuses on providing a comprehensive theoretical characterization of asymmetric LoRA with frozen random factors. First, while existing research provides upper-bound generalization guarantees based on averages over multiple experiments, the behaviour of a single fine-tuning run with specific random factors remains an open question. We address this by investigating the concentration of the typical LoRA generalization gap around its mean. Our main upper bound reveals a sample complexity of $\tilde{\mathcal{O}}\left(\frac{\sqrt{r}}{\sqrt{N}}\right)$ with high probability for rank $r$ LoRAs trained on $N$ samples. Additionally, we also determine the fundamental limits in terms of sample efficiency, establishing a matching lower bound of $\mathcal{O}\left(\frac{1}{\sqrt{N}}\right)$. By more closely reflecting the practical scenario of a single fine-tuning run, our findings offer crucial insights into the reliability and practicality of asymmetric LoRA.

</details>


### [250] [Uniform Mean Estimation for Heavy-Tailed Distributions via Median-of-Means](https://arxiv.org/abs/2506.14673)
*Mikael Møller Høgsgaard,Andrea Paudice*

Key words: 均值估计, 中位数均值, 重尾数据, 样本复杂度, k-means聚类, 线性回归

TL;DR: 论文研究了中位数均值（MoM）估计算法在有限矩数据分布下对函数类均值的同时估计性能，提出了新的样本复杂度界限，并在线性回归和k-means聚类中展示了应用改进。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 针对重尾数据，研究MoM估计算法在仅具有有限矩的数据分布下的表现，以提升均值估计的鲁棒性。

Method: 采用新颖的对称化技术分析MoM算法的样本复杂度，推导出新的理论界限。

Result: 证明了MoM在有限矩条件下的样本复杂度界限，并在k-means聚类和无界输入的线性回归中改进了现有成果。

Conclusion: MoM算法在有限矩条件下表现出色，其理论结果为处理重尾数据的均值估计提供了新工具。

Abstract: The Median of Means (MoM) is a mean estimator that has gained popularity in the context of heavy-tailed data. In this work, we analyze its performance in the task of simultaneously estimating the mean of each function in a class $\mathcal{F}$ when the data distribution possesses only the first $p$ moments for $p \in (1,2]$. We prove a new sample complexity bound using a novel symmetrization technique that may be of independent interest. Additionally, we present applications of our result to $k$-means clustering with unbounded inputs and linear regression with general losses, improving upon existing works.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [251] [Connecting phases of matter to the flatness of the loss landscape in analog variational quantum algorithms](https://arxiv.org/abs/2506.13865)
*Kasidit Srimahajariyapong,Supanut Thanasilp,Thiparat Chotibut*

Key words: 变分量子算法、贫瘠高原、多体局域化、量子模拟

TL;DR: 研究发现，通过在MBL相中初始化模拟VQA，可以有效避免贫瘠高原问题，同时保持足够的表达能力。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 为了解决数字门基方法中常见的贫瘠高原问题，研究基于模拟量子平台的VQA表达能力与优化特性。

Method: 通过调控无序伊辛链的淬火动力学，分析其在热化相和MBL相中的表达能力和损失方差。

Result: MBL相在较大淬火次数时仍能避免贫瘠高原，而热化相在小淬火次数时即出现贫瘠高原。

Conclusion: 提出MBL初始化策略，为模拟硬件的VQA扩展提供实用指导。

Abstract: Variational quantum algorithms (VQAs) promise near-term quantum advantage, yet parametrized quantum states commonly built from the digital gate-based approach often suffer from scalability issues such as barren plateaus, where the loss landscape becomes flat. We study an analog VQA ansätze composed of $M$ quenches of a disordered Ising chain, whose dynamics is native to several quantum simulation platforms. By tuning the disorder strength we place each quench in either a thermalized phase or a many-body-localized (MBL) phase and analyse (i) the ansätze's expressivity and (ii) the scaling of loss variance. Numerics shows that both phases reach maximal expressivity at large $M$, but barren plateaus emerge at far smaller $M$ in the thermalized phase than in the MBL phase. Exploiting this gap, we propose an MBL initialisation strategy: initialise the ansätze in the MBL regime at intermediate quench $M$, enabling an initial trainability while retaining sufficient expressivity for subsequent optimization. The results link quantum phases of matter and VQA trainability, and provide practical guidelines for scaling analog-hardware VQAs.

</details>


### [252] [Hamiltonian Formalism for Comparing Quantum and Classical Intelligence](https://arxiv.org/abs/2506.14456)
*Elija Perrier*

Key words: AGI, 量子计算, 哈密顿形式主义, 环境交互

TL;DR: 论文提出了一种哈密顿形式主义，用于比较经典和量子环境中的AGI任务，旨在开发一种精确的数学语言来区分两者在环境交互中的差异。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 通过量子基板实现的AGI前景，促使开发数学框架来直接比较它们在经典和量子环境中的运行方式。

Method: 引入哈密顿形式主义，将AGI动力学分解为生成器，描述如归纳、推理、递归、学习、测量和记忆等核心功能。

Result: 提出了一种形式化方法，用于对比经典和量子AGI在环境中的交互差异。

Conclusion: 该框架为理解量子与经典AGI在环境交互中的差异提供了数学基础。

Abstract: The prospect of AGI instantiated on quantum substrates motivates the development of mathematical frameworks that enable direct comparison of their operation in classical and quantum environments. To this end, we introduce a Hamiltonian formalism for describing classical and quantum AGI tasks as a means of contrasting their interaction with the environment. We propose a decomposition of AGI dynamics into Hamiltonian generators for core functions such as induction, reasoning, recursion, learning, measurement, and memory. This formalism aims to contribute to the development of a precise mathematical language for how quantum and classical agents differ via environmental interaction.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [253] [The Perception of Phase Intercept Distortion and its Application in Data Augmentation](https://arxiv.org/abs/2506.14571)
*Venkatakrishnan Vaidyanathapuram Krishnan,Nathaniel Condit-Schultz*

Key words: 相位失真，相位截取失真，人类感知，数据增强，机器学习

TL;DR: 本文研究了相位截取失真（一种频率无关的相位偏移），假设其虽然显著改变信号波形但不可感知，并通过实验验证了这一点，同时探讨了其在机器学习数据增强中的应用。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 探讨相位截取失真对人类感知的影响，并研究其在机器学习数据增强中的潜在作用。

Method: 进行人类被试实验验证假设，并在音频机器学习任务中使用相位截取失真进行数据增强实验。

Result: 实验证实相位截取失真对人类不可感知，且作为数据增强手段能提升机器学习性能。

Conclusion: 相位截取失真虽改变信号波形但不可感知，可用于数据增强以改善机器学习任务。

Abstract: Phase distortion refers to the alteration of the phase relationships between frequencies in a signal, which can be perceptible. In this paper, we discuss a special case of phase distortion known as phase-intercept distortion, which is created by a frequency-independent phase shift. We hypothesize that, though this form of distortion changes a signal's waveform significantly, the distortion is imperceptible. Human-subject experiment results are reported which are consistent with this hypothesis. Furthermore, we discuss how the imperceptibility of phase-intercept distortion can be useful for machine learning, specifically for data augmentation. We conducted multiple experiments using phase-intercept distortion as a novel approach to data augmentation, and obtained improved results for audio machine learning tasks.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [254] [DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models](https://arxiv.org/abs/2506.13817)
*Saleem A. Al Dajani,Abel Sanchez,John R. Williams*

Key words: 生成式AI, 单细胞RNA测序, 数据标注, 基础模型, 生物信息学

TL;DR: 生成式AI基础模型在单细胞RNA测序中展现出巨大潜力，通过实时网络搜索自动标注数据，准确率达82.5%，解决了标注瓶颈问题。

<details>
  <summary>Details</summary>

Main category: q-bio.GN

Motivation: 解决单细胞RNA测序数据标注的瓶颈问题，提高监督学习的效率，减少人工干预和错误。

Method: 采用基于代理的基础模型结合实时网络搜索技术，自动化标注实验数据。

Result: 实现82.5%的标注准确率，支持下游任务如细胞分型和扰动预测。

Conclusion: 该模型在生物数据领域具有创新性，未来可能超越人工标注性能，助力健康监测与诊断。

Abstract: Generative AI foundation models offer transformative potential for processing structured biological data, particularly in single-cell RNA sequencing, where datasets are rapidly scaling toward billions of cells. We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy. This addresses a key bottleneck in supervised learning for structured omics data by increasing annotation throughput without manual curation and human error. Our approach enables the development of virtual cell foundation models capable of downstream tasks such as cell-typing and perturbation prediction. As data volume grows, these models may surpass human performance in labeling, paving the way for reliable inference in large-scale perturbation screens. This application demonstrates domain-specific innovation in health monitoring and diagnostics, aligned with efforts like the Human Cell Atlas and Human Tumor Atlas Network.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [255] [AMLgentex: Mobilizing Data-Driven Research to Combat Money Laundering](https://arxiv.org/abs/2506.13989)
*Johan Östman,Edvin Callisen,Anton Chen,Kristiina Ausmees,Emanuel Gårdh,Jovan Zamac,Jolanta Goldsteine,Hugo Wefer,Simon Whelan,Markus Reimegård*

Key words: 洗钱, 反洗钱, 开源工具, 交易数据, 评估方法

TL;DR: 论文摘要介绍了洗钱行为的现实挑战，并提出了一种名为AMLGentex的开源工具，用于生成真实的交易数据和评估洗钱检测方法。

<details>
  <summary>Details</summary>

Main category: cs.SI

Motivation: 洗钱行为每年涉及数万亿美元，但因多种因素（如逃避手段、数据不完备等）难以检测。现有合成数据集无法真实模拟洗钱的结构和行为复杂性，亟需更好的解决方案。

Method: 提出AMLGentex，一个开源工具包，用于生成可配置的真实交易数据，并评估洗钱检测方法。它在可控环境中重现真实世界的关键挑战。

Result: AMLGentex能够系统性地评估反洗钱系统，并在复杂场景下测试方法的有效性。

Conclusion: AMLGentex为解决洗钱检测中的现实挑战提供了可控制的评估平台。

Abstract: Money laundering enables organized crime by allowing illicit funds to enter the legitimate economy. Although trillions of dollars are laundered each year, only a small fraction is ever uncovered. This stems from a range of factors, including deliberate evasion by launderers, the rarity of confirmed cases, and the limited visibility each financial institution has into the global transaction network. While several synthetic datasets are available, they fail to model the structural and behavioral complexity of real-world money laundering. In particular, they often overlook partial observability, sparse and uncertain labels, strategic behavior, temporal dynamics, class imbalance, and network-level dependencies. To address these limitations, we present AMLGentex, an open-source suite for generating realistic, configurable transaction data and benchmarking detection methods. It enables systematic evaluation of anti-money laundering (AML) systems in a controlled environment that captures key real-world challenges. We demonstrate how the framework can be used to rigorously evaluate methods under conditions that reflect the complexity of practical AML scenarios.

</details>


### [256] [Density-aware Walks for Coordinated Campaign Detection](https://arxiv.org/abs/2506.13912)
*Atul Anand Gopalakrishnan,Jakir Hossain,Tuğrulcan Elmas,Ahmet Erdem Sarıyüce*

Key words: 社交媒体, 协同宣传, 图神经网络, 随机加权游走, 密度感知

TL;DR: 论文提出了一种基于图分类的方法来检测社交媒体上的协同宣传活动，通过局部网络结构密度改进图神经网络，显著提高了分类准确率。

<details>
  <summary>Details</summary>

Main category: cs.SI

Motivation: 区分社交媒体上的协同宣传活动与真实公众讨论是一个重大挑战，因为这些攻击手段复杂且伪装性强。

Method: 利用Large Engagement Networks (LEN)数据集，提出了一种基于局部网络结构密度的图分类方法，包括随机加权游走（RWW）和Skip-gram模型生成嵌入，并结合消息传递神经网络（MPNN）进行训练。

Result: 新方法在二分类和多分类任务中分别实现了12%和5%的准确率提升。

Conclusion: 结合密度感知结构编码与MPNN能够有效识别社交媒体上的非真实协同行为。

Abstract: Coordinated campaigns frequently exploit social media platforms by artificially amplifying topics, making inauthentic trends appear organic, and misleading users into engagement. Distinguishing these coordinated efforts from genuine public discourse remains a significant challenge due to the sophisticated nature of such attacks. Our work focuses on detecting coordinated campaigns by modeling the problem as a graph classification task. We leverage the recently introduced Large Engagement Networks (LEN) dataset, which contains over 300 networks capturing engagement patterns from both fake and authentic trends on Twitter prior to the 2023 Turkish elections. The graphs in LEN were constructed by collecting interactions related to campaigns that stemmed from ephemeral astroturfing. Established graph neural networks (GNNs) struggle to accurately classify campaign graphs, highlighting the challenges posed by LEN due to the large size of its networks. To address this, we introduce a new graph classification method that leverages the density of local network structures. We propose a random weighted walk (RWW) approach in which node transitions are biased by local density measures such as degree, core number, or truss number. These RWWs are encoded using the Skip-gram model, producing density-aware structural embeddings for the nodes. Training message-passing neural networks (MPNNs) on these density-aware embeddings yields superior results compared to the simpler node features available in the dataset, with nearly a 12\% and 5\% improvement in accuracy for binary and multiclass classification, respectively. Our findings demonstrate that incorporating density-aware structural encoding with MPNNs provides a robust framework for identifying coordinated inauthentic behavior on social media networks such as Twitter.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [257] [Markov Regime-Switching Intelligent Driver Model for Interpretable Car-Following Behavior](https://arxiv.org/abs/2506.14762)
*Chengyuan Zhang,Cathy Wu,Lijun Sun*

Key words: 驾驶行为模型,状态切换,FHMM-IDM,可解释性,贝叶斯推理

TL;DR: 作者提出了一个基于状态切换的驾驶行为模型（FHMM-IDM），通过分离内在驾驶模式和外部交通情景，提高了模型的准确性和可解释性。

<details>
  <summary>Details</summary>

Main category: stat.AP

Motivation: 传统的驾驶模型（如IDM）由于其单一机制结构，无法捕捉人类驾驶的多模态特性，导致行为平均化，降低模型真实性和参数可解释性。

Method: 引入状态切换框架，利用Factorial Hidden Markov Model与IDM结合（FHMM-IDM），通过贝叶斯推理估计状态特定参数和潜在状态轨迹。

Result: 在HighD数据集上验证，FHMM-IDM成功分离了内在驾驶行为和外部交通条件，揭示了动态状态切换模式。

Conclusion: 该框架为不确定背景下的驾驶行为建模提供了可行方法，提升了交通模拟的真实性和ADAS的人性化设计。

Abstract: Accurate and interpretable car-following models are essential for traffic simulation and autonomous vehicle development. However, classical models like the Intelligent Driver Model (IDM) are fundamentally limited by their parsimonious and single-regime structure. They fail to capture the multi-modal nature of human driving, where a single driving state (e.g., speed, relative speed, and gap) can elicit many different driver actions. This forces the model to average across distinct behaviors, reducing its fidelity and making its parameters difficult to interpret. To overcome this, we introduce a regime-switching framework that allows driving behavior to be governed by different IDM parameter sets, each corresponding to an interpretable behavioral mode. This design enables the model to dynamically switch between interpretable behavioral modes, rather than averaging across diverse driving contexts. We instantiate the framework using a Factorial Hidden Markov Model with IDM dynamics (FHMM-IDM), which explicitly separates intrinsic driving regimes (e.g., aggressive acceleration, steady-state following) from external traffic scenarios (e.g., free-flow, congestion, stop-and-go) through two independent latent Markov processes. Bayesian inference via Markov chain Monte Carlo (MCMC) is used to jointly estimate the regime-specific parameters, transition dynamics, and latent state trajectories. Experiments on the HighD dataset demonstrate that FHMM-IDM uncovers interpretable structure in human driving, effectively disentangling internal driver actions from contextual traffic conditions and revealing dynamic regime-switching patterns. This framework provides a tractable and principled solution to modeling context-dependent driving behavior under uncertainty, offering improvements in the fidelity of traffic simulations, the efficacy of safety analyses, and the development of more human-centric ADAS.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [258] [Excessive Reasoning Attack on Reasoning LLMs](https://arxiv.org/abs/2506.14374)
*Wai Man Si,Mingjie Li,Michael Backes,Yang Zhang*

Key words: 大型语言模型，对抗输入，过度推理，计算开销，损失框架

TL;DR: 针对大型语言模型（LLMs）在复杂任务中表现出过度推理的问题，提出了一种新的对抗输入攻击及相应的损失框架，显著增加了计算开销而不影响模型效用。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 现有大型语言模型在推理过程中存在过度计算的问题，对抗输入可能利用这一行为增加计算开销，因此需要研究如何优化模型以减少此类风险。

Method: 提出了一个包含三个部分的损失框架：优先级交叉熵损失、过度推理损失和延迟终止损失，用于优化对抗攻击的效果。

Result: 在GSM8K和ORCA数据集上，实验结果显示推理长度增加了3倍到9倍，同时模型效用未受影响，且对抗输入具有可转移性。

Conclusion: 研究表明，对抗输入可以有效利用模型的过度推理行为增加计算开销，提出的损失框架为优化模型推理提供了新的方向。

Abstract: Recent reasoning large language models (LLMs), such as OpenAI o1 and DeepSeek-R1, exhibit strong performance on complex tasks through test-time inference scaling. However, prior studies have shown that these models often incur significant computational costs due to excessive reasoning, such as frequent switching between reasoning trajectories (e.g., underthinking) or redundant reasoning on simple questions (e.g., overthinking). In this work, we expose a novel threat: adversarial inputs can be crafted to exploit excessive reasoning behaviors and substantially increase computational overhead without compromising model utility. Therefore, we propose a novel loss framework consisting of three components: (1) Priority Cross-Entropy Loss, a modification of the standard cross-entropy objective that emphasizes key tokens by leveraging the autoregressive nature of LMs; (2) Excessive Reasoning Loss, which encourages the model to initiate additional reasoning paths during inference; and (3) Delayed Termination Loss, which is designed to extend the reasoning process and defer the generation of final outputs. We optimize and evaluate our attack for the GSM8K and ORCA datasets on DeepSeek-R1-Distill-LLaMA and DeepSeek-R1-Distill-Qwen. Empirical results demonstrate a 3x to 9x increase in reasoning length with comparable utility performance. Furthermore, our crafted adversarial inputs exhibit transferability, inducing computational overhead in o3-mini, o1-mini, DeepSeek-R1, and QWQ models.

</details>


### [259] [LLM-Powered Intent-Based Categorization of Phishing Emails](https://arxiv.org/abs/2506.14337)
*Even Eilertsen,Vasileios Mavroeidis,Gudmund Grov*

Key words: 钓鱼攻击, 大型语言模型, 意图分类, 网络安全

TL;DR: 论文探讨了大型语言模型（LLMs）在检测钓鱼邮件中的潜力，通过意图分类和生成可操作的威胁信息，展示了LLMs在该领域的应用价值。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 钓鱼攻击对现代网络安全构成重大威胁，传统检测系统依赖用户不可见的电子邮件元数据，且难以应对经验用户通过文本识别的钓鱼邮件。

Method: 利用LLMs对钓鱼邮件进行二元分类，并引入意图类型分类法，通过定制数据集评估模型效果。

Result: 实验表明，现有LLMs能够有效检测并分类钓鱼邮件。

Conclusion: LLMs在钓鱼邮件检测领域具有实际应用潜力。

Abstract: Phishing attacks remain a significant threat to modern cybersecurity, as they successfully deceive both humans and the defense mechanisms intended to protect them. Traditional detection systems primarily focus on email metadata that users cannot see in their inboxes. Additionally, these systems struggle with phishing emails, which experienced users can often identify empirically by the text alone. This paper investigates the practical potential of Large Language Models (LLMs) to detect these emails by focusing on their intent. In addition to the binary classification of phishing emails, the paper introduces an intent-type taxonomy, which is operationalized by the LLMs to classify emails into distinct categories and, therefore, generate actionable threat information. To facilitate our work, we have curated publicly available datasets into a custom dataset containing a mix of legitimate and phishing emails. Our results demonstrate that existing LLMs are capable of detecting and categorizing phishing emails, underscoring their potential in this domain.

</details>


### [260] [Busting the Paper Ballot: Voting Meets Adversarial Machine Learning](https://arxiv.org/abs/2506.14582)
*Kaleel Mahmood,Caleb Manicke,Ethan Rathbun,Aayushi Verma,Sohaib Ahmad,Nicholas Stamatakis,Laurent Michel,Benjamin Fuller*

Key words: 选举安全,机器学习,对抗攻击,梯度掩码,物理域攻击

TL;DR: 研究发现机器学习分类器在美国选举计票器中存在安全风险，通过新数据集和攻击方法展示了攻击可能翻转选举结果。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 揭示机器学习分类器在选举计票中的潜在安全漏洞，尤其是在决定选票标记时的不稳定性。

Method: 引入四个新选票数据集，训练多种模型（SVM、CNN、ViT），并改进对抗攻击方法以克服梯度掩码问题。

Result: 传统白盒攻击因梯度掩码失效，改进后的方法在物理域中成功实现5%攻击成功率，足以改变选举结果。

Conclusion: 选举计票中的机器学习模型易受对抗攻击，需关注其安全性和实际应用风险。

Abstract: We show the security risk associated with using machine learning classifiers in United States election tabulators. The central classification task in election tabulation is deciding whether a mark does or does not appear on a bubble associated to an alternative in a contest on the ballot. Barretto et al. (E-Vote-ID 2021) reported that convolutional neural networks are a viable option in this field, as they outperform simple feature-based classifiers.
  Our contributions to election security can be divided into four parts. To demonstrate and analyze the hypothetical vulnerability of machine learning models on election tabulators, we first introduce four new ballot datasets. Second, we train and test a variety of different models on our new datasets. These models include support vector machines, convolutional neural networks (a basic CNN, VGG and ResNet), and vision transformers (Twins and CaiT). Third, using our new datasets and trained models, we demonstrate that traditional white box attacks are ineffective in the voting domain due to gradient masking. Our analyses further reveal that gradient masking is a product of numerical instability. We use a modified difference of logits ratio loss to overcome this issue (Croce and Hein, ICML 2020). Fourth, in the physical world, we conduct attacks with the adversarial examples generated using our new methods. In traditional adversarial machine learning, a high (50% or greater) attack success rate is ideal. However, for certain elections, even a 5% attack success rate can flip the outcome of a race. We show such an impact is possible in the physical domain. We thoroughly discuss attack realism, and the challenges and practicality associated with printing and scanning ballot adversarial examples.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [261] [A Systematic Review of User-Centred Evaluation of Explainable AI in Healthcare](https://arxiv.org/abs/2506.13904)
*Ivania Donoso-Guzmán,Kristýna Sirka Kacafírková,Maxwell Szymanski,An Jacobs,Denis Parra,Katrien Verbert*

Key words: XAI, 医疗, 用户评估, 框架, 指南

TL;DR: 论文总结了当前XAI在医疗领域的用户评估现状，提出了一个改进框架和实用指南。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 解决XAI方法在实际医疗场景中价值未充分探索和验证的问题，强调用户评估的重要性。

Method: 通过系统综述82项医疗领域的用户研究，结合预定义的编码方案和迭代开发的归纳代码进行分析。

Result: 总结了当前评估实践，揭示了解释属性间的关联，并提出了更新框架和实用指南。

Conclusion: 开发了支持跨学科团队设计定制化XAI评估策略的框架和指南。

Abstract: Despite promising developments in Explainable Artificial Intelligence, the practical value of XAI methods remains under-explored and insufficiently validated in real-world settings. Robust and context-aware evaluation is essential, not only to produce understandable explanations but also to ensure their trustworthiness and usability for intended users, but tends to be overlooked because of no clear guidelines on how to design an evaluation with users.
  This study addresses this gap with two main goals: (1) to develop a framework of well-defined, atomic properties that characterise the user experience of XAI in healthcare; and (2) to provide clear, context-sensitive guidelines for defining evaluation strategies based on system characteristics.
  We conducted a systematic review of 82 user studies, sourced from five databases, all situated within healthcare settings and focused on evaluating AI-generated explanations. The analysis was guided by a predefined coding scheme informed by an existing evaluation framework, complemented by inductive codes developed iteratively.
  The review yields three key contributions: (1) a synthesis of current evaluation practices, highlighting a growing focus on human-centred approaches in healthcare XAI; (2) insights into the interrelations among explanation properties; and (3) an updated framework and a set of actionable guidelines to support interdisciplinary teams in designing and implementing effective evaluation strategies for XAI systems tailored to specific application contexts.

</details>


### [262] [StorySage: Conversational Autobiography Writing Powered by a Multi-Agent Framework](https://arxiv.org/abs/2506.14159)
*Shayan Talaei,Meijin Li,Kanu Grover,James Kent Hippler,Diyi Yang,Amin Saberi*

Key words: 自传写作, 多代理系统, 人机交互, 叙事完整性, 用户满意度

TL;DR: StorySage是一个支持自传写作的多代理系统，通过灵活的对话和结构化方法捕获用户记忆，实验显示其比基线表现更好。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 现有的对话写作助手难以捕捉个人记忆并构建完整自传，因此需要一种更灵活、用户驱动的系统。

Method: 引入由多个代理（如采访者、记录员、规划师等）组成的StorySage系统，迭代收集记忆并更新自传。

Result: 实验表明StorySage在多会话中表现优异，用户研究显示其对话流畅性、叙事完整性和满意度更高。

Conclusion: StorySage为自传写作提供了一种新架构，并展示了多代理系统如何增强人机创意合作。

Abstract: Every individual carries a unique and personal life story shaped by their memories and experiences. However, these memories are often scattered and difficult to organize into a coherent narrative, a challenge that defines the task of autobiography writing. Existing conversational writing assistants tend to rely on generic user interactions and pre-defined guidelines, making it difficult for these systems to capture personal memories and develop a complete biography over time. We introduce StorySage, a user-driven software system designed to meet the needs of a diverse group of users that supports a flexible conversation and a structured approach to autobiography writing. Powered by a multi-agent framework composed of an Interviewer, Session Scribe, Planner, Section Writer, and Session Coordinator, our system iteratively collects user memories, updates their autobiography, and plans for future conversations. In experimental simulations, StorySage demonstrates its ability to navigate multiple sessions and capture user memories across many conversations. User studies (N=28) highlight how StorySage maintains improved conversational flow, narrative completeness, and higher user satisfaction when compared to a baseline. In summary, StorySage contributes both a novel architecture for autobiography writing and insights into how multi-agent systems can enhance human-AI creative partnerships.

</details>


### [263] [Balancing Caregiving and Self-Care: Exploring Mental Health Needs of Alzheimer's and Dementia Caregivers](https://arxiv.org/abs/2506.14196)
*Jiayue Melissa Shi,Keran Wang,Dong Whi Yoo,Ravi Karkar,Koustuv Saha*

Key words: 阿尔茨海默病, 家庭护理者, 心理健康, 技术支持, 阶段性干预

TL;DR: 研究探讨了阿尔茨海默病及相关痴呆症（AD/ADRD）家庭护理者的心理健康问题，分析了其应对负担的实践和技术支持，提出了分阶段的干预设计基础。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 家庭护理者长期承担AD/ADRD患者的照顾责任，面临心理健康挑战，现有支持系统未能满足其动态需求。

Method: 通过25名家庭护理者的半结构化访谈，识别心理健康问题的原因与影响，并绘制了护理历程中三个阶段的心理变化。

Result: 研究发现护理者需求随时间变化，需设计可访问、可扩展且个性化的技术支持。

Conclusion: 研究为动态、阶段敏感的干预措施设计提供了基础，有助于全面提升护理者与患者的福祉。

Abstract: Alzheimer's Disease and Related Dementias (AD/ADRD) are progressive neurodegenerative conditions that impair memory, thought processes, and functioning. Family caregivers of individuals with AD/ADRD face significant mental health challenges due to long-term caregiving responsibilities. Yet, current support systems often overlook the evolving nature of their mental wellbeing needs. Our study examines caregivers' mental wellbeing concerns, focusing on the practices they adopt to manage the burden of caregiving and the technologies they use for support. Through semi-structured interviews with 25 family caregivers of individuals with AD/ADRD, we identified the key causes and effects of mental health challenges, and developed a temporal mapping of how caregivers' mental wellbeing evolves across three distinct stages of the caregiving journey. Additionally, our participants shared insights into improvements for existing mental health technologies, emphasizing the need for accessible, scalable, and personalized solutions that adapt to caregivers' changing needs over time. These findings offer a foundation for designing dynamic, stage-sensitive interventions that holistically support caregivers' mental wellbeing, benefiting both caregivers and care recipients.

</details>


### [264] [Controlling Context: Generative AI at Work in Integrated Circuit Design and Other High-Precision Domains](https://arxiv.org/abs/2506.14567)
*Emanuel Moss,Elizabeth Watkins,Christopher Persaud,Passant Karunaratne,Dawn Nafus*

Key words: 生成式AI, 工程工作流程, 准确性, 上下文控制, 集成电路设计

TL;DR: 论文分析了硬件和软件工程师在使用生成式AI工具时面临的挑战，尤其是准确性和交互上下文控制的困难，并提出了改进建议。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 随着生成式AI工具在工程领域应用的普及，如何在高精度领域中保持对错误的警觉以及解决使用工具带来的其他问题成为一个研究方向。

Method: 通过对集成电路设计领域的工程师及其合作者进行访谈，分析生成式AI工具的使用情况及其带来的问题。

Result: 研究发现，准确性和交互上下文控制是工程师使用生成式AI工具时面临的主要挑战。

Conclusion: 建议通过增强交互式上下文控制能力来缓解这些问题。

Abstract: Generative AI tools have become more prevalent in engineering workflows, particularly through chatbots and code assistants. As the perceived accuracy of these tools improves, questions arise about whether and how those who work in high-precision domains might maintain vigilance for errors, and what other aspects of using such tools might trouble their work. This paper analyzes interviews with hardware and software engineers, and their collaborators, who work in integrated circuit design to identify the role accuracy plays in their use of generative AI tools and what other forms of trouble they face in using such tools. The paper inventories these forms of trouble, which are then mapped to elements of generative AI systems, to conclude that controlling the context of interactions between engineers and the generative AI tools is one of the largest challenges they face. The paper concludes with recommendations for mitigating this form of trouble by increasing the ability to control context interactively.

</details>


### [265] [StreetLens: Enabling Human-Centered AI Agents for Neighborhood Assessment from Street View Imagery](https://arxiv.org/abs/2506.14670)
*Jina Kim,Leeje Jang,Yao-Yi Chiang,Guanyu Wang,Michelle Pasco*

Key words: 视觉语言模型, 社区研究, 自动化标注, 街景图像, 社会科学

TL;DR: StreetLens是一个基于视觉语言模型(VLM)的可配置工作流，用于自动化社区环境评估，结合社会科学专业知识，提高研究的可扩展性和灵活性。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 传统方法依赖人工标注和专家干预，耗时且难以适应不同研究设计和地理环境。StreetLens旨在通过AI技术解决这些问题。

Method: StreetLens通过问题导向的提示、街景图像检索和语义标注，模拟人类编码过程，支持研究者自定义VLM的角色。

Result: StreetLens能够生成从客观特征到主观感知的广泛语义标注，并支持结合先验数据增强分析。

Conclusion: StreetLens推动了灵活、可扩展的AI系统在社区研究中的应用，加速了研究进程。

Abstract: Traditionally, neighborhood studies have employed interviews, surveys, and manual image annotation guided by detailed protocols to identify environmental characteristics, including physical disorder, decay, street safety, and sociocultural symbols, and to examine their impact on developmental and health outcomes. While these methods yield rich insights, they are time-consuming and require intensive expert intervention. Recent technological advances, including vision-language models (VLMs), have begun to automate parts of this process; however, existing efforts are often ad hoc and lack adaptability across research designs and geographic contexts. In this demo paper, we present StreetLens, a human-centered, researcher-configurable workflow that embeds relevant social science expertise in a VLM for scalable neighborhood environmental assessments. StreetLens mimics the process of trained human coders by grounding the analysis in questions derived from established interview protocols, retrieving relevant street view imagery (SVI), and generating a wide spectrum of semantic annotations from objective features (e.g., the number of cars) to subjective perceptions (e.g., the sense of disorder in an image). By enabling researchers to define the VLM's role through domain-informed prompting, StreetLens places domain knowledge at the core of the analysis process. It also supports the integration of prior survey data to enhance robustness and expand the range of characteristics assessed across diverse settings. We provide a Google Colab notebook to make StreetLens accessible and extensible for researchers working with public or custom SVI datasets. StreetLens represents a shift toward flexible, agentic AI systems that work closely with researchers to accelerate and scale neighborhood studies.

</details>


### [266] [Design an Editable Speech-to-Sign-Language Transformer System: A Human-Centered AI Approach](https://arxiv.org/abs/2506.14677)
*Yingchao Li*

Key words: 手语动画, Transformer, 用户自适应, 实时系统, 人机交互

TL;DR: 提出了一种以人为中心的实时自适应语音转手语动画系统，结合Transformer动作生成与可编辑的JSON中间层。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 克服现有手语技术的局限性，增加用户的参与和编辑能力，提升自然性和表现力。

Method: 结合流式Conformer编码器和自回归Transformer-MDN解码器，并通过用户编辑和反馈优化系统。

Result: 实验证明用户可编辑界面和反馈显著提高了理解度、自然性、可用性和信任度，并降低了认知负荷。

Conclusion: 技术和用户参与的创新共同实现了可访问、可解释和自适应的手语AI技术。

Abstract: This paper presents a human-centered, real-time, user-adaptive speech-to-sign language animation system that integrates Transformer-based motion generation with a transparent, user-editable JSON intermediate layer. The framework overcomes key limitations in prior sign language technologies by enabling direct user inspection and modification of sign segments, thus enhancing naturalness, expressiveness, and user agency. Leveraging a streaming Conformer encoder and autoregressive Transformer-MDN decoder, the system synchronizes spoken input into upper-body and facial motion for 3D avatar rendering. Edits and user ratings feed into a human-in-the-loop optimization loop for continuous improvement. Experiments with 20 deaf signers and 5 interpreters show that the editable interface and participatory feedback significantly improve comprehension, naturalness, usability, and trust, while lowering cognitive load. With sub-20 ms per-frame inference on standard hardware, the system is ready for real-time communication and education. This work illustrates how technical and participatory innovation together enable accessible, explainable, and user-adaptive AI for sign language technology.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [267] [Knowledge Compression via Question Generation: Enhancing Multihop Document Retrieval without Fine-tuning](https://arxiv.org/abs/2506.13778)
*Anvi Alex Eponon,Moein Shahiki-Tash,Ildar Batyrshin,Christian E. Maldonado-Sifuentes,Grigori Sidorov,Alexander Gelbukh*

Key words: 检索增强生成、问题编码、句法重排、知识检索

TL;DR: 论文提出一种基于问题的知识编码方法，通过生成问题和自定义句法重排优化检索增强生成（RAG）系统，显著提升检索效果并降低存储需求。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 传统检索增强生成系统存在依赖微调和分块的限制，需改进效率和扩展性。

Method: 采用问题生成技术编码文本内容，结合句法重排方法优化检索。

Result: 单跳检索召回率达0.84，多跳任务F1得分为0.52，存储需求减少80%。

Conclusion: 该方法无需微调，降低了延迟和存储成本，是高效且可扩展的RAG替代方案。

Abstract: This study presents a question-based knowledge encoding approach that improves retrieval-augmented generation (RAG) systems without requiring fine-tuning or traditional chunking. We encode textual content using generated questions that span the lexical and semantic space, creating targeted retrieval cues combined with a custom syntactic reranking method.
  In single-hop retrieval over 109 scientific papers, our approach achieves a Recall@3 of 0.84, outperforming traditional chunking methods by 60 percent. We also introduce "paper-cards", concise paper summaries under 300 characters, which enhance BM25 retrieval, increasing MRR@3 from 0.56 to 0.85 on simplified technical queries.
  For multihop tasks, our reranking method reaches an F1 score of 0.52 with LLaMA2-Chat-7B on the LongBench 2WikiMultihopQA dataset, surpassing chunking and fine-tuned baselines which score 0.328 and 0.412 respectively.
  This method eliminates fine-tuning requirements, reduces retrieval latency, enables intuitive question-driven knowledge access, and decreases vector storage demands by 80%, positioning it as a scalable and efficient RAG alternative.

</details>


### [268] [XGraphRAG: Interactive Visual Analysis for Graph-based Retrieval-Augmented Generation](https://arxiv.org/abs/2506.13782)
*Ke Wang,Bo Pan,Yingchaojie Feng,Yuwei Wu,Jieyi Chen,Minfeng Zhu,Wei Chen*

Key words: GraphRAG、可视化分析、检索增强生成、大语言模型、知识库

TL;DR: 该论文提出了一种视觉分析框架XGraphRAG，用于帮助开发者分析和改进基于图的检索增强生成（GraphRAG）的效果，解决了其复杂性和难以解释的问题。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: GraphRAG在增强大语言模型（LLM）回答外部知识库问题时表现出色，但其复杂的信息处理流程和大量LLM调用限制了其可解释性和可访问性。因此，需要一个工具帮助开发者分析其效果。

Method: 研究人员提出了一种视觉分析框架，开发了原型系统XGraphRAG，通过交互式可视化帮助用户追踪和分析GraphRAG的关键召回。

Result: 评估表明该框架有效且易用，并已开源。

Conclusion: XGraphRAG框架显著提升了GraphRAG的可解释性和可访问性，为开发者提供了改进的工具。

Abstract: Graph-based Retrieval-Augmented Generation (RAG) has shown great capability in enhancing Large Language Model (LLM)'s answer with an external knowledge base. Compared to traditional RAG, it introduces a graph as an intermediate representation to capture better structured relational knowledge in the corpus, elevating the precision and comprehensiveness of generation results. However, developers usually face challenges in analyzing the effectiveness of GraphRAG on their dataset due to GraphRAG's complex information processing pipeline and the overwhelming amount of LLM invocations involved during graph construction and query, which limits GraphRAG interpretability and accessibility. This research proposes a visual analysis framework that helps RAG developers identify critical recalls of GraphRAG and trace these recalls through the GraphRAG pipeline. Based on this framework, we develop XGraphRAG, a prototype system incorporating a set of interactive visualizations to facilitate users' analysis process, boosting failure cases collection and improvement opportunities identification. Our evaluation demonstrates the effectiveness and usability of our approach. Our work is open-sourced and available at https://github.com/Gk0Wk/XGraphRAG.

</details>


### [269] [Analysis of Anonymous User Interaction Relationships and Prediction of Advertising Feedback Based on Graph Neural Network](https://arxiv.org/abs/2506.13787)
*Yanjun Dai,Haoyang Feng,Yuan Gao*

Key words: 在线广告, 图神经网络, 时间层次聚合, 对比正则化

TL;DR: 论文提出了一种解耦时间层次图神经网络（DTH-GNN），通过多尺度特征提取和层次聚合优化广告推荐效果，AUC提升8.2%，对数损失降低5.7%。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 在线广告依赖匿名用户交互网络，但现有图模型难以捕捉多尺度时序、语义和依赖特征，需新模型解决这一问题。

Method: 1. 时间边分解为短时爆发、昼夜周期和长期记忆三类通道；2. 层次异质聚合，通过元路径条件Transformer编码器结合子图；3. 反馈感知对比正则化，结合策略梯度层优化表示。

Result: DTH-GNN的AUC提升8.2%，对数损失降低5.7%。

Conclusion: DTH-GNN能有效捕捉匿名行为的复杂模式，显著提升广告推荐性能。

Abstract: While online advertising is highly dependent on implicit interaction networks of anonymous users for engagement inference, and for the selection and optimization of delivery strategies, existing graph models seldom can capture the multi-scale temporal, semantic and higher-order dependency features of these interaction networks, thus it's hard to describe the complicated patterns of the anonymous behavior. In this paper, we propose Decoupled Temporal-Hierarchical Graph Neural Network (DTH-GNN), which achieves three main contributions. Above all, we introduce temporal edge decomposition, which divides each interaction into three types of channels: short-term burst, diurnal cycle and long-range memory, and conducts feature extraction using the convolution kernel of parallel dilated residuals; Furthermore, our model builds a hierarchical heterogeneous aggregation, where user-user, user-advertisement, advertisement-advertisement subgraphs are combined through the meta-path conditional Transformer encoder, where the noise structure is dynamically tamped down via the synergy of cross-channel self-attention and gating relationship selector. Thirdly, the contrast regularity of feedback perception is formulated, the consistency of various time slices is maximized, the entropy of control exposure information with dual-view target is maximized, the global prototype of dual-momentum queue distillation is presented, and the strategy gradient layer with light weight is combined with delaying transformation signal to fine-tune the node representation for benefit-oriented. The AUC of DTH-GNN improved by 8.2% and the logarithmic loss improved by 5.7% in comparison with the best baseline model.

</details>


### [270] [AcademicBrowse: Benchmarking Academic Browse Ability of LLMs](https://arxiv.org/abs/2506.13784)
*Junting Zhou,Wang Li,Yiyan Liao,Nengyuan Zhang,Tingjia Miaoand Zhihui Qi,Yuhan Wu,Tong Yang*

Key words: 大语言模型, 学术搜索, 数据集, 信息检索

TL;DR: 提出首个专门评估大语言模型在学术研究复杂信息检索能力的数据集AcademicBrowse，具有学术实用性、高难度、简洁评估和广泛覆盖特点。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 现有基准如OpenAI的BrowseComp未充分满足学术搜索需求，如深度文献追踪和专业数据库支持。

Method: 设计AcademicBrowse数据集，模拟真实学术环境，问题难度高且评估简洁。

Result: 数据集覆盖15个学科，答案需多次深度搜索，便于审核验证。

Conclusion: AcademicBrowse能更精准衡量和提升大语言模型在复杂学术检索任务中的表现。

Abstract: Large Language Models (LLMs)' search capabilities have garnered significant attention. Existing benchmarks, such as OpenAI's BrowseComp, primarily focus on general search scenarios and fail to adequately address the specific demands of academic search. These demands include deeper literature tracing and organization, professional support for academic databases, the ability to navigate long-tail academic knowledge, and ensuring academic rigor. Here, we proposed AcademicBrowse, the first dataset specifically designed to evaluate the complex information retrieval capabilities of Large Language Models (LLMs) in academic research. AcademicBrowse possesses the following key characteristics: Academic Practicality, where question content closely mirrors real academic learning and research environments, avoiding deliberately misleading models; High Difficulty, with answers that are challenging for single models (e.g., Grok DeepSearch or Gemini Deep Research) to provide directly, often requiring at least three deep searches to derive; Concise Evaluation, where limiting conditions ensure answers are as unique as possible, accompanied by clear sources and brief solution explanations, greatly facilitating subsequent audit and verification, surpassing the current lack of analyzed search datasets both domestically and internationally; and Broad Coverage, as the dataset spans at least 15 different academic disciplines. Through AcademicBrowse, we expect to more precisely measure and promote the performance improvement of LLMs in complex academic information retrieval tasks. The data is available at: https://huggingface.co/datasets/PKU-DS-LAB/AcademicBrowse

</details>


### [271] [InsertRank: LLMs can reason over BM25 scores to Improve Listwise Reranking](https://arxiv.org/abs/2506.14086)
*Rahul Seetharaman,Kaustubh D. Dhole,Aman Bansal*

Key words: Large Language Models, reranker, reasoning, BM25, retrieval

TL;DR: InsertRank是一种基于大型语言模型（LLM）的重新排序器，通过结合BM25等词汇信号提升检索性能，在复杂查询任务中表现优越。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 随着LLM聊天界面的普及，用户提出更复杂的查询需求，传统的关键词匹配或语义相似度检索方法无法满足，需要结合推理能力提升检索效果。

Method: InsertRank结合BM25分数等词汇信号，对LLM的推理能力进行优化，以提升检索效果。

Result: 在BRIGHT和R2MED基准测试中，InsertRank在多种LLM模型（如GPT、Gemini、Deepseek）中均表现出优于先前方法的检索效果。

Conclusion: InsertRank展示了结合词汇信号和LLM推理能力对复杂查询检索的显著提升，为未来的检索系统提供了新思路。

Abstract: Large Language Models (LLMs) have demonstrated significant strides across various information retrieval tasks, particularly as rerankers, owing to their strong generalization and knowledge-transfer capabilities acquired from extensive pretraining. In parallel, the rise of LLM-based chat interfaces has raised user expectations, encouraging users to pose more complex queries that necessitate retrieval by ``reasoning'' over documents rather than through simple keyword matching or semantic similarity. While some recent efforts have exploited reasoning abilities of LLMs for reranking such queries, considerable potential for improvement remains. In that regards, we introduce InsertRank, an LLM-based reranker that leverages lexical signals like BM25 scores during reranking to further improve retrieval performance. InsertRank demonstrates improved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning 12 diverse domains, and R2MED, a specialized medical reasoning retrieval benchmark spanning 8 different tasks. We conduct an exhaustive evaluation and several ablation studies and demonstrate that InsertRank consistently improves retrieval effectiveness across multiple families of LLMs, including GPT, Gemini, and Deepseek models. %In addition, we also conduct ablation studies on normalization by varying the scale of the BM25 scores, and positional bias by shuffling the order of the documents. With Deepseek-R1, InsertRank achieves a score of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark, surpassing previous methods.

</details>


### [272] [RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition](https://arxiv.org/abs/2506.14412)
*Tim Cofala,Oleh Astappiev,William Xion,Hailay Teklehaymanot*

Key words: RAG, LLM, 检索增强, 问答系统, Pinecone, BGE

TL;DR: 论文研究了检索增强生成（RAG）技术，结合大语言模型（LLM）内部参数化知识与外部非参数化资源，旨在提高事实准确性并减少幻觉。通过LiveRAG 2025挑战赛，探索了RAG解决方案在DataMorgana的QA数据集上的表现。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 目标是提高大语言模型在事实问答中的准确性，减少幻觉现象，并通过挑战赛推动RAG技术的发展。

Method: 采用InstructRAG结合Pinecone检索器和BGE重排器，限制模型使用不超过10B参数的LLM，最终答案生成由Falcon-3-10B完成。

Result: 解决方案在SIGIR 2025 LiveRAG挑战赛中取得第四名，正确性得分为1.13，忠实性得分为0.55。

Conclusion: RAG技术结合高效检索器和重排器能显著提升LLM在问答任务中的表现。

Abstract: Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by combining their internal, parametric knowledge with external, non-parametric sources, with the goal of improving factual correctness and minimizing hallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize accuracy on DataMorgana's QA pairs, which are composed of single-hop and multi-hop questions. The challenge provides access to sparse OpenSearch and dense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to LLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A judge-LLM assesses the submitted answers along with human evaluators. By exploring distinct retriever combinations and RAG solutions under the challenge conditions, our final solution emerged using InstructRAG in combination with a Pinecone retriever and a BGE reranker. Our solution achieved a correctness score of 1.13 and a faithfulness score of 0.55, placing fourth in the SIGIR 2025 LiveRAG Challenge.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [273] [Non-planar Object Detection and Identification by Features Matching and Triangulation Growth](https://arxiv.org/abs/2506.13769)
*Filippo Leveni*

Key words: 物体检测, Delaunay三角剖分, 局部一致性, 特征匹配, 非平面模板

TL;DR: 论文提出了一种基于特征的迭代方法，用于在场景图像中检测和识别扭曲模板，通过局部一致性和Delaunay三角剖分实现高效匹配。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 在计算机视觉中，物体检测与识别是关键问题，尤其在模板变形或非平面情况下，传统几何模型（如单应性）失效，需新方法应对。

Method: 利用Delaunay三角剖分将模板特征构图，从单个三角形开始逐步匹配相邻节点，基于几何和光度特性评估局部一致性。

Result: 在变形轻微时表现与基于单应性的RANSAC相当，变形显著时描述性能更优，适用于非平面或扭曲平面模板。

Conclusion: 该方法通过增量分组和局部一致性验证，有效解决了复杂变形下的物体识别问题，扩展了传统方法的局限性。

Abstract: Object detection and identification is surely a fundamental topic in the computer vision field; it plays a crucial role in many applications such as object tracking, industrial robots control, image retrieval, etc. We propose a feature-based approach for detecting and identifying distorted occurrences of a given template in a scene image by incremental grouping of feature matches between the image and the template. For this purpose, we consider the Delaunay triangulation of template features as an useful tool through which to be guided in this iterative approach. The triangulation is treated as a graph and, starting from a single triangle, neighboring nodes are considered and the corresponding features are identified; then matches related to them are evaluated to determine if they are worthy to be grouped. This evaluation is based on local consistency criteria derived from geometric and photometric properties of local features. Our solution allows the identification of the object in situations where geometric models (e.g. homography) does not hold, thus enable the detection of objects such that the template is non planar or when it is planar but appears distorted in the image. We show that our approach performs just as well or better than application of homography-based RANSAC in scenarios in which distortion is nearly absent, while when the deformation becomes relevant our method shows better description performance.

</details>


### [274] [Hidden Bias in the Machine: Stereotypes in Text-to-Image Models](https://arxiv.org/abs/2506.13780)
*Sedat Porikli,Vedat Porikli*

Key words: 文本到图像、社会偏见、生成模型、公平性、刻板印象

TL;DR: 研究了文本到图像（T2I）模型中存在的社会偏见问题，发现生成的图像在性别、种族等方面存在显著差异，并呼吁更包容的数据集和开发实践。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探讨T2I模型是否会复制和放大社会偏见，以促进生成视觉系统的公平性。

Method: 使用Stable Diffusion 1.5和Flux-1模型生成16,000多张图像，并结合Google Image Search的8,000张对比图像，分析图像中的社会偏见。

Result: 生成的图像在性别、种族、年龄等方面存在显著差异，且反映了社会中的有害刻板印象。

Conclusion: 需改进数据集和开发实践以减少偏见，促进生成视觉系统的包容性。

Abstract: Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems.

</details>


### [275] [RadFabric: Agentic AI System with Reasoning Capability for Radiology](https://arxiv.org/abs/2506.14142)
*Wenting Chen,Yi Dong,Zhaojun Ding,Yucheng Shi,Yifan Zhou,Fang Zeng,Yijun Luo,Tianyu Lin,Yihang Su,Yichen Wu,Kai Zhang,Zhen Xiang,Tianming Liu,Ninghao Liu,Lichao Sun,Yixuan Yuan,Xiang Li*

Key words: CXR诊断, 多模态分析, 人工智能, 病理检测, 解剖映射

TL;DR: RadFabric是一个多代理、多模态的CXR分析框架，通过整合视觉和文本分析提升诊断性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前CXR自动诊断系统在病理覆盖、诊断准确性及视觉与文本推理整合方面存在局限，RadFabric旨在解决这些问题。

Method: RadFabric基于Model Context Protocol (MCP)，包含病理检测代理、解剖解释代理和推理代理，结合多模态数据分析。

Result: RadFabric在病理检测（如骨折1.000准确率）和整体诊断（0.799）上显著优于传统系统（0.229-0.527）。

Conclusion: RadFabric通过多模态特征对齐和偏好驱动推理，推动了透明、精准且临床可操作的CXR分析。

Abstract: Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic conditions, but current automated systems face limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning. To address these gaps, we propose RadFabric, a multi agent, multimodal reasoning framework that unifies visual and textual analysis for comprehensive CXR interpretation. RadFabric is built on the Model Context Protocol (MCP), enabling modularity, interoperability, and scalability for seamless integration of new diagnostic agents. The system employs specialized CXR agents for pathology detection, an Anatomical Interpretation Agent to map visual findings to precise anatomical structures, and a Reasoning Agent powered by large multimodal reasoning models to synthesize visual, anatomical, and clinical data into transparent and evidence based diagnoses. RadFabric achieves significant performance improvements, with near-perfect detection of challenging pathologies like fractures (1.000 accuracy) and superior overall diagnostic accuracy (0.799) compared to traditional systems (0.229 to 0.527). By integrating cross modal feature alignment and preference-driven reasoning, RadFabric advances AI-driven radiology toward transparent, anatomically precise, and clinically actionable CXR analysis.

</details>


### [276] [Fake it till You Make it: Reward Modeling as Discriminative Prediction](https://arxiv.org/abs/2506.13846)
*Runtao Liu,Jiahao Zhan,Yingqing He,Chen Wei,Alan Yuille,Qifeng Chen*

Key words: 奖励模型, 强化学习, GAN, 对抗训练, 生成模型

TL;DR: 论文提出了一种名为GAN-RM的高效奖励建模框架，通过对抗训练生成判别模型，避免了人工标注偏好数据和显式质量维度设计的问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前奖励建模方法依赖大量人工标注数据或复杂的质量维度设计，实施复杂且不完整。

Method: 通过对抗训练，利用少量目标样本（Preference Proxy Data）和模型生成输出训练奖励模型。

Result: 实验表明，GAN-RM在多种应用中表现优异，包括测试时缩放（Best-of-N样本过滤）和后训练方法（如SFT和DPO）。

Conclusion: GAN-RM是一种高效且无需人工干预的奖励建模方法。

Abstract: An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).

</details>


### [277] [VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning](https://arxiv.org/abs/2506.14629)
*Md. Adnanul Islam,Md. Faiyaz Abdullah Sayeedi,Md. Asaduzzaman Shuvo,Muhammad Ziaur Rahman,Shahanur Rahman Bappy,Raiyan Rahman,Swakkhar Shatabda*

Key words: 蚊媒疾病,多模态数据集,目标检测,分割,自然语言推理

TL;DR: VisText-Mosquito是一个多模态数据集，整合视觉和文本数据，用于蚊虫孳生地自动检测和分析，模型性能优异，数据集和代码公开。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 蚊媒疾病是全球健康威胁，需早期检测和主动控制孳生地以防止疫情爆发。

Method: 使用YOLOv9s和YOLOv11n-Seg模型进行目标检测和分割，并微调BLIP模型进行自然语言推理。

Result: YOLOv9s检测精确度0.92926，YOLOv11n-Seg分割精确度0.91587，BLIP推理的BLEU得分54.7。

Conclusion: AI检测可主动降低蚊媒疾病风险，数据集和代码已公开。

Abstract: Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme "Prevention is Better than Cure", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito

</details>


### [278] [ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM](https://arxiv.org/abs/2506.14766)
*Yujun Wang,Jinhe Bi,Yunpu Ma,Soeren Pirk*

Key words: 多模态大语言模型, 幻觉, 注意力机制, 对比解码, VQA

TL;DR: 本文提出了一种通过干预模型注意力机制来减少多模态大语言模型幻觉的新方法，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 多模态大语言模型（MLLM）常因过度依赖部分线索而产生幻觉，现有方法（如VCD和ICD）通过对比扰动或负前缀输入的效果来缓解，但其深层机制尚不明确。

Method: 通过观察VCD和ICD对内部注意力动态的影响，提出了注意力导向的对比解码框架，直接干预注意力机制以减少幻觉。

Result: 实验表明，该方法在多种MLLM架构和解码方法中显著减少了幻觉，并在POPE、CHAIR和MMHal-Bench等基准测试中表现优异，还提升了标准VQA任务的性能。

Conclusion: 直接干预注意力机制是一种更原则性的方法，可有效减少幻觉并提升模型性能。

Abstract: Multimodal Large Language Model (MLLM) often suffer from hallucinations. They over-rely on partial cues and generate incorrect responses. Recently, methods like Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding (ICD) have been proposed to mitigate hallucinations by contrasting predictions from perturbed or negatively prefixed inputs against original outputs. In this work, we uncover that methods like VCD and ICD fundamentally influence internal attention dynamics of the model. This observation suggests that their effectiveness may not stem merely from surface-level modifications to logits but from deeper shifts in attention distribution. Inspired by this insight, we propose an attention-steerable contrastive decoding framework that directly intervenes in attention mechanisms of the model to offer a more principled approach to mitigating hallucinations. Our experiments across multiple MLLM architectures and diverse decoding methods demonstrate that our approach significantly reduces hallucinations and improves the performance on benchmarks such as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing performance on standard VQA benchmarks.

</details>


### [279] [Intelligent Image Sensing for Crime Analysis: A ML Approach towards Enhanced Violence Detection and Investigation](https://arxiv.org/abs/2506.13910)
*Aritra Dutta,Pushpita Boral,G Suseela*

Key words: 暴力检测, 3D CNN, 双向LSTM, 监督学习, 视频分析

TL;DR: 提出一种基于机器学习的框架，用于视频流中的暴力行为检测与分类，结合3D CNN和双向LSTM，提高计算效率与准确性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统监控方法难以及时检测多样化的暴力行为，需自动化解决方案以减少损失。

Method: 采用监督学习，3D CNN用于检测，可分离卷积3D模型提取特征，双向LSTM处理时间序列，训练数据来自多样化标注数据集。

Result: 框架在计算资源效率和准确性上表现更优。

Conclusion: 基于机器学习的方法能有效提升暴力行为检测的及时性与分类准确性。

Abstract: The increasing global crime rate, coupled with substantial human and property losses, highlights the limitations of traditional surveillance methods in promptly detecting diverse and unexpected acts of violence. Addressing this pressing need for automatic violence detection, we leverage Machine Learning to detect and categorize violent events in video streams. This paper introduces a comprehensive framework for violence detection and classification, employing Supervised Learning for both binary and multi-class violence classification. The detection model relies on 3D Convolutional Neural Networks, while the classification model utilizes the separable convolutional 3D model for feature extraction and bidirectional LSTM for temporal processing. Training is conducted on a diverse customized datasets with frame-level annotations, incorporating videos from surveillance cameras, human recordings, hockey fight, sohas and wvd dataset across various platforms. Additionally, a camera module integrated with raspberry pi is used to capture live video feed, which is sent to the ML model for processing. Thus, demonstrating improved performance in terms of computational resource efficiency and accuracy.

</details>


### [280] [HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment](https://arxiv.org/abs/2506.13925)
*Numair Nadeem,Saeed Anwar,Muhammad Hamza Asad,Abdul Bais*

Key words: semi-supervised semantic segmentation, vision-language models, hierarchical queries, cross-modal alignment

TL;DR: HierVL是一个结合视觉语言模型和mask-transformer的半监督语义分割框架，通过多尺度查询和空间对齐提升分割精度，在多个数据集上达到新SOTA。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决半监督语义分割中标签稀缺和领域多变带来的挑战，结合视觉语言模型的鲁棒语义和密集预测能力。

Method: 提出Hierarchical Semantic Query Generator、Cross-Modal Spatial Alignment Module和Dual-Query Transformer Decoder，并使用正则化损失保持对齐。

Result: 在COCO、Pascal VOC、ADE20和Cityscapes上分别提升4.4%、3.1%、5.9%和1.8%的mIoU。

Conclusion: 语言引导的分割能缩小标签效率差距，实现细粒度和实例感知的泛化。

Abstract: Semi-supervised semantic segmentation remains challenging under severe label scarcity and domain variability. Vision-only methods often struggle to generalize, resulting in pixel misclassification between similar classes, poor generalization and boundary localization. Vision-Language Models offer robust, domain-invariant semantics but lack the spatial grounding required for dense prediction. We introduce HierVL, a unified framework that bridges this gap by integrating abstract text embeddings into a mask-transformer architecture tailored for semi-supervised segmentation. HierVL features three novel components: a Hierarchical Semantic Query Generator that filters and projects abstract class embeddings into multi-scale queries to suppress irrelevant classes and handle intra-class variability; a Cross-Modal Spatial Alignment Module that aligns semantic queries with pixel features for sharper boundaries under sparse supervision; and a Dual-Query Transformer Decoder that fuses semantic and instance-level queries to prevent instance collapse. We also introduce targeted regularization losses that maintain vision-language alignment throughout training to reinforce semantic grounding. HierVL establishes a new state-of-the-art by achieving a +4.4% mean improvement of the intersection over the union on COCO (with 232 labeled images), +3.1% on Pascal VOC (with 92 labels), +5.9% on ADE20 (with 158 labels) and +1.8% on Cityscapes (with 100 labels), demonstrating better performance under 1% supervision on four benchmark datasets. Our results show that language-guided segmentation closes the label efficiency gap and unlocks new levels of fine-grained, instance-aware generalization.

</details>


### [281] [SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement](https://arxiv.org/abs/2506.14035)
*Chelsi Jain,Yiran Wu,Yifan Zeng,Jiale Liu,S hengyu Dai,Zhenwen Shao,Qingyun Wu,Huazheng Wang*

Key words: DocVQA, 多模态, 检索增强生成, 视觉语言模型, SimpleDoc

TL;DR: 论文提出了一种轻量级但高效的检索增强框架SimpleDoc，用于解决文档视觉问答任务的多模态挑战，性能优于基线。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决DocVQA任务中多模态信息（如多页文档、图像和表格）处理的挑战，现有方法虽然采用了RAG流程，但仍有改进空间。

Method: 提出SimpleDoc框架，结合嵌入相似度检索候选页面，再通过页面摘要过滤和重新排序，并迭代调用VLM推理器逐步筛选相关页面。

Result: 在4个DocVQA数据集上平均性能提升3.2%，且检索页面更少。

Conclusion: SimpleDoc通过双重线索检索和迭代推理，显著提升了DocVQA任务的效率与性能。

Abstract: Document Visual Question Answering (DocVQA) is a practical yet challenging task, which is to ask questions based on documents while referring to multiple pages and different modalities of information, e.g, images and tables. To handle multi-modality, recent methods follow a similar Retrieval Augmented Generation (RAG) pipeline, but utilize Visual Language Models (VLMs) based embedding model to embed and retrieve relevant pages as images, and generate answers with VLMs that can accept an image as input. In this paper, we introduce SimpleDoc, a lightweight yet powerful retrieval - augmented framework for DocVQA. It boosts evidence page gathering by first retrieving candidates through embedding similarity and then filtering and re-ranking these candidates based on page summaries. A single VLM-based reasoner agent repeatedly invokes this dual-cue retriever, iteratively pulling fresh pages into a working memory until the question is confidently answered. SimpleDoc outperforms previous baselines by 3.2% on average on 4 DocVQA datasets with much fewer pages retrieved. Our code is available at https://github.com/ag2ai/SimpleDoc.

</details>


### [282] [Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems](https://arxiv.org/abs/2506.14096)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Key words: LLM, 图像分割, 智能交通系统, 自动驾驶, 可解释AI

TL;DR: 综述了LLM增强图像分割在智能交通系统中的新兴领域，包括应用、挑战及未来方向。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探索LLM与计算机视觉融合如何提升智能交通系统的场景理解能力。

Method: 系统性回顾现有方法，基于提示机制和核心架构对方法进行分类。

Result: LLM增强图像分割在自动驾驶、交通监控等领域有潜力，但实时性和可靠性仍是挑战。

Conclusion: 未来需重点发展可解释、以人为本的AI以推动该技术在交通系统中的成功应用。

Abstract: The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.

</details>


### [283] [KDMOS:Knowledge Distillation for Motion Segmentation](https://arxiv.org/abs/2506.14130)
*Chunyu Cao,Jintao Cheng,Zeyu Chen,Linfan Zhan,Rui Fan,Zhijian He,Xiaoyu Tang*

Key words: 运动目标分割, 知识蒸馏, BEV投影, 动态上采样, 自动驾驶

TL;DR: 提出了基于logits的知识蒸馏框架（KDMOS），通过BEV投影和非投影模型结合及动态上采样优化，显著改善了运动目标分割的准确性和实时性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 自动驾驶中运动目标分割（MOS）对多项任务至关重要，但现有方法在准确性和实时性间难以平衡。

Method: 采用BEV投影学生模型和非投影教师模型，结合类别解耦蒸馏策略及动态上采样，优化网络架构。

Result: 在SemanticKITTI-MOS和Apollo数据集上分别达到78.8% IoU和竞争性结果，参数减少7.69%。

Conclusion: KDMOS框架有效提升了MOS性能，兼顾准确性与实时性。

Abstract: Motion Object Segmentation (MOS) is crucial for autonomous driving, as it enhances localization, path planning, map construction, scene flow estimation, and future state prediction. While existing methods achieve strong performance, balancing accuracy and real-time inference remains a challenge. To address this, we propose a logits-based knowledge distillation framework for MOS, aiming to improve accuracy while maintaining real-time efficiency. Specifically, we adopt a Bird's Eye View (BEV) projection-based model as the student and a non-projection model as the teacher. To handle the severe imbalance between moving and non-moving classes, we decouple them and apply tailored distillation strategies, allowing the teacher model to better learn key motion-related features. This approach significantly reduces false positives and false negatives. Additionally, we introduce dynamic upsampling, optimize the network architecture, and achieve a 7.69% reduction in parameter count, mitigating overfitting. Our method achieves a notable IoU of 78.8% on the hidden test set of the SemanticKITTI-MOS dataset and delivers competitive results on the Apollo dataset. The KDMOS implementation is available at https://github.com/SCNU-RISLAB/KDMOS.

</details>


### [284] [Mapping Farmed Landscapes from Remote Sensing](https://arxiv.org/abs/2506.13993)
*Michelangelo Conserva,Alex Wilson,Charlotte Stanton,Vishal Batchu,Varun Gulshan*

Key words: agricultural landscapes, biodiversity, deep learning, habitat mapping, ecological monitoring

TL;DR: Farmscapes is a high-resolution map using deep learning to identify rural landscape features, aiding biodiversity efforts in England.

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Address the lack of detailed ecological maps to support biodiversity management in agricultural landscapes.

Method: Deep learning segmentation model trained on manually annotated aerial imagery.

Result: High accuracy in identifying habitats (e.g., 96% F1-score for woodland) and releasing an England-wide map on Google Earth Engine.

Conclusion: Provides a valuable tool for habitat restoration and biodiversity monitoring, enhancing landscape connectivity analysis.

Abstract: Effective management of agricultural landscapes is critical for meeting global biodiversity targets, but efforts are hampered by the absence of detailed, large-scale ecological maps. To address this, we introduce Farmscapes, the first large-scale (covering most of England), high-resolution (25cm) map of rural landscape features, including ecologically vital elements like hedgerows, woodlands, and stone walls. This map was generated using a deep learning segmentation model trained on a novel, dataset of 942 manually annotated tiles derived from aerial imagery. Our model accurately identifies key habitats, achieving high f1-scores for woodland (96\%) and farmed land (95\%), and demonstrates strong capability in segmenting linear features, with an F1-score of 72\% for hedgerows. By releasing the England-wide map on Google Earth Engine, we provide a powerful, open-access tool for ecologists and policymakers. This work enables data-driven planning for habitat restoration, supports the monitoring of initiatives like the EU Biodiversity Strategy, and lays the foundation for advanced analysis of landscape connectivity.

</details>


### [285] [SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability](https://arxiv.org/abs/2506.14144)
*Juho Bai,Inwook Shim*

Key words: 行人轨迹预测，场景理解，Vision Transformer，多模态大语言模型，碰撞惩罚

TL;DR: SceneAware框架通过结合场景理解和行人轨迹预测，显著提升轨迹预测的准确性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法主要关注行人间的社会交互，忽略了环境上下文对行人运动模式的显著影响。

Method: 使用Vision Transformer编码场景信息，结合多模态大语言模型生成的行走性掩码，以及基于Transformer的轨迹编码器和碰撞惩罚机制。

Result: 在ETH/UCY数据集上表现优于现有方法，提升超过50%，且在不同类型行人运动中一致表现良好。

Conclusion: 显式场景信息的利用对于生成准确且物理可行的预测至关重要，SceneAware方法高效可靠。

Abstract: Accurate prediction of pedestrian trajectories is essential for applications in robotics and surveillance systems. While existing approaches primarily focus on social interactions between pedestrians, they often overlook the rich environmental context that significantly shapes human movement patterns. In this paper, we propose SceneAware, a novel framework that explicitly incorporates scene understanding to enhance trajectory prediction accuracy. Our method leverages a Vision Transformer~(ViT) scene encoder to process environmental context from static scene images, while Multi-modal Large Language Models~(MLLMs) generate binary walkability masks that distinguish between accessible and restricted areas during training. We combine a Transformer-based trajectory encoder with the ViT-based scene encoder, capturing both temporal dynamics and spatial constraints. The framework integrates collision penalty mechanisms that discourage predicted trajectories from violating physical boundaries, ensuring physically plausible predictions. SceneAware is implemented in both deterministic and stochastic variants. Comprehensive experiments on the ETH/UCY benchmark datasets show that our approach outperforms state-of-the-art methods, with more than 50\% improvement over previous models. Our analysis based on different trajectory categories shows that the model performs consistently well across various types of pedestrian movement. This highlights the importance of using explicit scene information and shows that our scene-aware approach is both effective and reliable in generating accurate and physically plausible predictions. Code is available at: https://github.com/juho127/SceneAware.

</details>


### [286] [VideoMAR: Autoregressive Video Generatio with Continuous Tokens](https://arxiv.org/abs/2506.14168)
*Hu Yu,Biao Gong,Hangjie Yuan,DanDan Zheng,Weilong Chai,Jingdong Chen,Kecheng Zheng,Feng Zhao*

Key words: 视频生成, 自回归模型, 掩码生成, 课程学习, 高效训练

TL;DR: VideoMAR 是一种解码器自回归图像到视频模型，通过时空掩码生成，提出了下一帧扩散损失、课程学习和渐进分辨率训练，显著降低了资源和数据需求，并在性能上超越现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探索掩码自回归模型在视频生成中的潜力，解决长序列自回归建模的高成本和难度问题。

Method: 结合时空掩码生成、下一帧扩散损失、课程学习、渐进分辨率训练和渐进温度策略。

Result: 在 VBench-I2V 基准测试中，VideoMAR 性能超越 Cosmos I2V，且参数、训练数据和 GPU 资源需求大幅降低。

Conclusion: VideoMAR 展示了高效且灵活的视频生成能力，为自回归模型在视频领域的应用提供了新思路。

Abstract: Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\%$), training data ($0.5\%$), and GPU resources ($0.2\%$).

</details>


### [287] [A multi-stage augmented multimodal interaction network for fish feeding intensity quantification](https://arxiv.org/abs/2506.14170)
*Shulong Zhang,Mingyuan Yao,Jiayin Zhao,Xiao Liu,Haihua Wang*

Key words: 循环水产养殖、多模态融合、摄食强度量化、注意力机制、证据推理

TL;DR: 本文提出了一种多阶段增强多模态交互网络（MAINet），用于量化鱼类摄食强度，通过高效特征提取、模态交互增强和多模态决策融合，显著提高了模型的准确性和可靠性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 循环水产养殖系统中，准确评估鱼类摄食强度对降低成本和优化投喂时间至关重要。现有研究在模态选择、特征提取与融合及决策推断方面存在局限，限制了模型的准确性和可靠性提升。

Method: 构建了MAINet，包括通用特征提取框架、辅助模态增强主模态机制（ARPM，含通道注意力融合网络CAFN和双模态注意力融合网络DAFN）和证据推理（ER）规则进行多模态决策融合。

Result: 实验显示，MAINet在准确率、精确率、召回率和F1分数上均超过96.7%，显著优于单模态、双模态及其他决策融合方法模型。消融实验验证了改进策略对模型鲁棒性和特征利用效率的关键作用。

Conclusion: MAINet通过多模态交互增强和多阶段融合，有效提升了鱼类摄食强度量化的准确性，为水产养殖系统提供了更可靠的解决方案。

Abstract: In recirculating aquaculture systems, accurate and effective assessment of fish feeding intensity is crucial for reducing feed costs and calculating optimal feeding times. However, current studies have limitations in modality selection, feature extraction and fusion, and co-inference for decision making, which restrict further improvement in the accuracy, applicability and reliability of multimodal fusion models. To address this problem, this study proposes a Multi-stage Augmented Multimodal Interaction Network (MAINet) for quantifying fish feeding intensity. Firstly, a general feature extraction framework is proposed to efficiently extract feature information from input image, audio and water wave datas. Second, an Auxiliary-modality Reinforcement Primary-modality Mechanism (ARPM) is designed for inter-modal interaction and generate enhanced features, which consists of a Channel Attention Fusion Network (CAFN) and a Dual-mode Attention Fusion Network (DAFN). Finally, an Evidence Reasoning (ER) rule is introduced to fuse the output results of each modality and make decisions, thereby completing the quantification of fish feeding intensity. The experimental results show that the constructed MAINet reaches 96.76%, 96.78%, 96.79% and 96.79% in accuracy, precision, recall and F1-Score respectively, and its performance is significantly higher than the comparison models. Compared with models that adopt single-modality, dual-modality fusion and different decision-making fusion methods, it also has obvious advantages. Meanwhile, the ablation experiments further verified the key role of the proposed improvement strategy in improving the robustness and feature utilization efficiency of model, which can effectively improve the accuracy of the quantitative results of fish feeding intensity.

</details>


### [288] [HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction](https://arxiv.org/abs/2506.14229)
*Changbai Li,Haodong Zhu,Hanlin Chen,Juan Zhang,Tongfei Chen,Shuo Yang,Shuwei Shao,Wenhao Dong,Baochang Zhang*

Key words: 3D高斯喷射,分层优化,内存效率,高分辨率重建

TL;DR: HRGS是一种内存高效的框架，通过分层块级优化解决3DGS在高分辨率场景下的内存扩展问题，实现高质量的高分辨率3D场景重建。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 3DGS在实时3D场景重建中取得了显著进展，但在高分辨率场景下面临内存扩展性问题，需要一种更高效的解决方案。

Method: HRGS首先生成全局粗粒度高斯表示，然后将场景分区为多个块并进行高分辨率数据细化，结合高斯分区和训练数据分区，通过重要性驱动的高斯修剪和法线先验提升重建质量和效率。

Result: 在三个基准测试中，HRGS在高分辨率新视角合成和表面重建任务中实现了最先进的性能。

Conclusion: HRGS通过分层优化和内存效率设计，成功解决了高分辨率场景下的内存问题，同时提升了重建质量。

Abstract: 3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.

</details>


### [289] [FRIDU: Functional Map Refinement with Guided Image Diffusion](https://arxiv.org/abs/2506.14322)
*Avigail Cohen Rimon,Mirela Ben-Chen,Or Litany*

Key words: 函数映射,扩散模型,映射细化,Laplace-Beltrami算子

TL;DR: 提出了一种通过将对应关系映射视为图像，利用图像扩散模型在函数空间直接训练的方法，以生成精确的映射，并在推理时通过点映射作为指导。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 为了提高形状间对应关系映射的准确性，利用函数映射的特性，将其视为2D图像，探索扩散模型的潜力。

Method: 在函数空间中训练图像扩散模型，直接生成精确的映射；推理时使用点映射作为指导，并优化函数映射目标（如正交性和Laplace-Beltrami算子的可交换性）。

Result: 该方法在映射细化任务中与最先进方法竞争，展示了引导扩散模型在函数映射处理中的潜力。

Conclusion: 引导扩散模型是处理函数映射的有效途径，且在效率和准确性上表现优异。

Abstract: We propose a novel approach for refining a given correspondence map between two shapes. A correspondence map represented as a functional map, namely a change of basis matrix, can be additionally treated as a 2D image. With this perspective, we train an image diffusion model directly in the space of functional maps, enabling it to generate accurate maps conditioned on an inaccurate initial map. The training is done purely in the functional space, and thus is highly efficient. At inference time, we use the pointwise map corresponding to the current functional map as guidance during the diffusion process. The guidance can additionally encourage different functional map objectives, such as orthogonality and commutativity with the Laplace-Beltrami operator. We show that our approach is competitive with state-of-the-art methods of map refinement and that guided diffusion models provide a promising pathway to functional map processing.

</details>


### [290] [MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models](https://arxiv.org/abs/2506.14435)
*Hongyu Wang,Jiayu Xu,Ruiping Wang,Yan Feng,Yitao Zhai,Peng Pei,Xunliang Cai,Xilin Chen*

Key words: Mixture-of-Experts, 三元参数, 内存效率, 后训练量化, 边缘计算

TL;DR: 论文提出MoTE方法，通过训练更多低精度专家（三元参数{-1, 0, 1}）来减少内存占用，同时保持性能接近全精度模型，适用于内存受限设备。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决多专家模型（MoEs）内存占用高的问题，尤其适用于边缘设备部署。

Method: 采用预训练FFN作为共享专家，并训练三元路由专家参数{-1, 0, 1}，结合后训练量化方法。

Result: MoTE在3.4GB内存下比MoE-LLaVA性能提升4.3%，且具有更好的扩展性。

Conclusion: MoTE在内存效率和性能上优于全精度基线，适合内存受限场景。

Abstract: Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.

</details>


### [291] [Model compression using knowledge distillation with integrated gradients](https://arxiv.org/abs/2506.14440)
*David E. Hernandez,Jose Chang,Torbjörn E. M. Nordling*

Key words: 模型压缩, 知识蒸馏, 集成梯度, 边缘设备, 数据增强

TL;DR: 该论文提出了一种新颖的模型压缩方法，通过集成梯度（IG）增强知识蒸馏，显著提高了学生模型的精度与压缩效率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 在资源受限设备上部署深度学习模型需要高效的压缩方法，传统方法在精度与压缩率之间存在权衡问题。

Method: 使用集成梯度（IG）作为数据增强策略，将IG图叠加到输入图像中，使学生模型更好地理解教师模型的决策过程。

Result: 在CIFAR-10上测试准确率达到92.6%，压缩因子4.1倍，推理时间从140ms降至13ms，显著优于非蒸馏模型。

Conclusion: 该方法通过将IG图预处理为一次性步骤，实现了高效的模型压缩，并在不同架构和压缩比下表现优异。

Abstract: Model compression is critical for deploying deep learning models on resource-constrained devices. We introduce a novel method enhancing knowledge distillation with integrated gradients (IG) as a data augmentation strategy. Our approach overlays IG maps onto input images during training, providing student models with deeper insights into teacher models' decision-making processes. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented knowledge distillation achieves 92.6% testing accuracy with a 4.1x compression factor-a significant 1.1 percentage point improvement ($p<0.001$) over non-distilled models (91.5%). This compression reduces inference time from 140 ms to 13 ms. Our method precomputes IG maps before training, transforming substantial runtime costs into a one-time preprocessing step. Our comprehensive experiments include: (1) comparisons with attention transfer, revealing complementary benefits when combined with our approach; (2) Monte Carlo simulations confirming statistical robustness; (3) systematic evaluation of compression factor versus accuracy trade-offs across a wide range (2.2x-1122x); and (4) validation on an ImageNet subset aligned with CIFAR-10 classes, demonstrating generalisability beyond the initial dataset. These extensive ablation studies confirm that IG-based knowledge distillation consistently outperforms conventional approaches across varied architectures and compression ratios. Our results establish this framework as a viable compression technique for real-world deployment on edge devices while maintaining competitive accuracy.

</details>


### [292] [Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection](https://arxiv.org/abs/2506.14473)
*Zhijing Wan,Zhixiang Wang,Zheng Wang,Xin Xu,Shin'ichi Satoh*

Key words: 基础模型, 数据子集选择, 细粒度数据集, RAM-APL

TL;DR: 论文研究了如何利用基础模型（FMs）替代传统信息提取器（IEs）进行数据子集选择，发现FMs在细粒度数据集上表现更优，并提出了针对细粒度数据集的RAM-APL方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 降低深度学习训练成本，探究FMs在数据子集选择中的潜力及其与传统IEs的性能差异。

Method: 提出RAM-APL方法，利用多个FMs的互补优势进行细粒度数据集子集选择。

Result: FMs在细粒度数据集上表现优于传统IEs；RAM-APL在多个细粒度数据集中达到最优性能。

Conclusion: FMs在细粒度数据子集选择中具有优势，RAM-APL方法显著提升了性能。

Abstract: One-shot subset selection serves as an effective tool to reduce deep learning training costs by identifying an informative data subset based on the information extracted by an information extractor (IE). Traditional IEs, typically pre-trained on the target dataset, are inherently dataset-dependent. Foundation models (FMs) offer a promising alternative, potentially mitigating this limitation. This work investigates two key questions: (1) Can FM-based subset selection outperform traditional IE-based methods across diverse datasets? (2) Do all FMs perform equally well as IEs for subset selection? Extensive experiments uncovered surprising insights: FMs consistently outperform traditional IEs on fine-grained datasets, whereas their advantage diminishes on coarse-grained datasets with noisy labels. Motivated by these finding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a method tailored for fine-grained image datasets. RAM-APL leverages multiple FMs to enhance subset selection by exploiting their complementary strengths. Our approach achieves state-of-the-art performance on fine-grained datasets, including Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011.

</details>


### [293] [EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization](https://arxiv.org/abs/2506.14356)
*Xiaoqi Wang,Yi Wang,Lap-Pui Chau*

Key words: 视频语言理解, 自我为中心视觉, 空间-时间建模, 多实例检索, 对称多相似性损失

TL;DR: 本文提出EVA02-AT，一种基于EVA02的视频语言基础模型，通过单阶段预训练、空间-时间旋转位置嵌入和对称多相似性损失，显著提升了以自我为中心的视频语言任务性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法在高效率与准确的空间-时间建模上面临挑战，如多阶段预训练成本高、手动分割的3D旋转位置嵌入效果不足、以及软标签多实例检索目标不精确。本文旨在解决这些问题。

Method: 通过单阶段预训练将图像CLIP模型转化为视频编码器，提出空间-时间旋转位置嵌入与联合注意力机制，并引入对称多相似性（SMS）损失优化多实例检索任务。

Result: 在Ego4D、EPIC-Kitchens-100和Charades-Ego等数据集上，EVA02-AT在零样本和微调设置下实现了最先进的性能，且参数更少。SMS损失显著提升了多实例检索任务的性能。

Conclusion: EVA02-AT通过创新设计在效率和性能上取得了显著提升，为以自我为中心的视频语言任务提供了强大的基础模型。

Abstract: Egocentric video-language understanding demands both high efficiency and accurate spatial-temporal modeling. Existing approaches face three key challenges: 1) Excessive pre-training cost arising from multi-stage pre-training pipelines, 2) Ineffective spatial-temporal encoding due to manually split 3D rotary positional embeddings that hinder feature interactions, and 3) Imprecise learning objectives in soft-label multi-instance retrieval, which neglect negative pair correlations. In this paper, we introduce EVA02-AT, a suite of EVA02-based video-language foundation models tailored to egocentric video understanding tasks. EVA02-AT first efficiently transfers an image-based CLIP model into a unified video encoder via a single-stage pretraining. Second, instead of applying rotary positional embeddings to isolated dimensions, we introduce spatial-temporal rotary positional embeddings along with joint attention, which can effectively encode both spatial and temporal information on the entire hidden dimension. This joint encoding of spatial-temporal features enables the model to learn cross-axis relationships, which are crucial for accurately modeling motion and interaction in videos. Third, focusing on multi-instance video-language retrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a novel training framework that advances all soft labels for both positive and negative pairs, providing a more precise learning objective. Extensive experiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and fine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art performance across diverse egocentric video-language tasks with fewer parameters. Models with our SMS loss also show significant performance gains on multi-instance retrieval benchmarks. Our code and models are publicly available at https://github.com/xqwang14/EVA02-AT .

</details>


### [294] [DepthSeg: Depth prompting in remote sensing semantic segmentation](https://arxiv.org/abs/2506.14382)
*Ning Zhou,Shanxiong Chen,Mingting Zhou,Haigang Sui,Lieyun Hu,Han Li,Li Hua,Qiming Zhou*

Key words: 遥感,语义分割,深度提示,土地覆盖,光谱混淆

TL;DR: 该论文提出了一种名为DepthSeg的新型二维遥感语义分割框架，通过深度/高度信息建模来减少光谱混淆和阴影遮挡的影响，实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有遥感语义分割方法主要关注不同物体的光谱特征，忽略了目标的高度差异，导致复杂场景下的地表覆盖错误分类。

Method: 引入深度提示的二维分割框架DepthSeg，包括轻量级适配器、深度提示器和语义分类解码器，整合深度信息以提高分类准确性。

Result: 在LiuZhou数据集上的实验证明了DepthSeg在土地覆盖制图中的优势，消融研究进一步验证了深度提示的重要性。

Conclusion: DepthSeg通过整合深度信息显著提升了遥感语义分割的精度，尤其在复杂场景下表现出色。

Abstract: Remote sensing semantic segmentation is crucial for extracting detailed land surface information, enabling applications such as environmental monitoring, land use planning, and resource assessment. In recent years, advancements in artificial intelligence have spurred the development of automatic remote sensing semantic segmentation methods. However, the existing semantic segmentation methods focus on distinguishing spectral characteristics of different objects while ignoring the differences in the elevation of the different targets. This results in land cover misclassification in complex scenarios involving shadow occlusion and spectral confusion. In this paper, we introduce a depth prompting two-dimensional (2D) remote sensing semantic segmentation framework (DepthSeg). It automatically models depth/height information from 2D remote sensing images and integrates it into the semantic segmentation framework to mitigate the effects of spectral confusion and shadow occlusion. During the feature extraction phase of DepthSeg, we introduce a lightweight adapter to enable cost-effective fine-tuning of the large-parameter vision transformer encoder pre-trained by natural images. In the depth prompting phase, we propose a depth prompter to model depth/height features explicitly. In the semantic prediction phase, we introduce a semantic classification decoder that couples the depth prompts with high-dimensional land-cover features, enabling accurate extraction of land-cover types. Experiments on the LiuZhou dataset validate the advantages of the DepthSeg framework in land cover mapping tasks. Detailed ablation studies further highlight the significance of the depth prompts in remote sensing semantic segmentation.

</details>


### [295] [Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images](https://arxiv.org/abs/2506.14560)
*David Butler,Adrian Hilton,Gustavo Carneiro*

Key words: 膝关节骨关节炎、机器学习、扩散模型、多任务预测、可解释性

TL;DR: 提出了一种新的可解释机器学习方法，通过多任务预测模型估计膝关节骨关节炎进展风险，并提高了预测精度和推理速度。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的膝关节骨关节炎风险评估方法缺乏可解释性且复杂，无法定位解剖标志。

Method: 使用扩散模型在类条件潜在空间中生成高质量未来图像，结合多任务预测模型分类未来疾病严重程度。

Result: 在预测膝关节骨关节炎进展的AUC达到0.71，比现有方法提升2%，推理速度提高约9%。

Conclusion: 该方法在预测精度和可解释性上均优于现有方法，且效率更高。

Abstract: Medical imaging plays a crucial role in assessing knee osteoarthritis (OA) risk by enabling early detection and disease monitoring. Recent machine learning methods have improved risk estimation (i.e., predicting the likelihood of disease progression) and predictive modelling (i.e., the forecasting of future outcomes based on current data) using medical images, but clinical adoption remains limited due to their lack of interpretability. Existing approaches that generate future images for risk estimation are complex and impractical. Additionally, previous methods fail to localize anatomical knee landmarks, limiting interpretability. We address these gaps with a new interpretable machine learning method to estimate the risk of knee OA progression via multi-task predictive modelling that classifies future knee OA severity and predicts anatomical knee landmarks from efficiently generated high-quality future images. Such image generation is achieved by leveraging a diffusion model in a class-conditioned latent space to forecast disease progression, offering a visual representation of how particular health conditions may evolve. Applied to the Osteoarthritis Initiative dataset, our approach improves the state-of-the-art (SOTA) by 2\%, achieving an AUC of 0.71 in predicting knee OA progression while offering ~9% faster inference time.

</details>


### [296] [Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models](https://arxiv.org/abs/2506.14399)
*Tian Xia,Fabio De Sousa Ribeiro,Rajat R Rasal,Avinash Kori,Raghav Mehta,Ben Glocker*

Key words: 反事实图像生成、扩散模型、无分类器引导、属性解耦

TL;DR: 论文提出了一种名为解耦无分类器引导（DCFG）的方法，通过分组条件控制改进反事实图像生成，解决了传统方法在属性放大和身份保留上的不足。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统无分类器引导（CFG）方法因其全局权重应用的局限性，导致反事实图像生成中身份保留差和属性放大的问题，需改进。

Method: 提出DCFG框架，基于属性分割嵌入策略，分离语义输入，对不同属性组应用选择性引导，结合因果图划分干预和不变属性。

Result: 在CelebA-HQ、MIMIC-CXR和EMBED数据集上的实验表明，DCFG提升了干预保真度、减少了意外变化，并增强了可逆性。

Conclusion: DCFG为反事实图像生成提供了更忠实和可解释的结果。

Abstract: Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.

</details>


### [297] [Causally Steered Diffusion for Automated Video Counterfactual Generation](https://arxiv.org/abs/2506.14404)
*Nikos Spyrou,Athanasios Vlontzos,Paraskevas Pegios,Thomas Melistas,Nefeli Gkouti,Yannis Panagakis,Giorgos Papanastasiou,Sotirios A. Tsaftaris*

Key words: 文本到图像模型, 视频编辑, 因果图, 反事实生成, 视觉语言模型

TL;DR: 提出了一种基于因果关系的视频反事实生成框架，利用视觉语言模型指导文本提示优化，确保视频编辑时的因果忠实性，适用于任何黑盒视频编辑系统。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有文本到图像模型在视频编辑中难以维持因果关系的忠实性，可能导致不真实或误导性的结果。

Method: 采用视觉语言模型（VLM）指导，通过优化基于因果图的文本提示，无需访问或微调底层视频编辑系统。

Result: 通过标准视频质量指标和反事实专用标准（如因果有效性和最小化性）评估，证明该方法能有效生成因果忠实的反事实视频。

Conclusion: 该方法兼容任何黑盒视频编辑系统，具有在医疗和数字媒体等领域生成真实“假设”视频场景的潜力。

Abstract: Adapting text-to-image (T2I) latent diffusion models for video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships in video content. Edits affecting causally dependent attributes risk generating unrealistic or misleading outcomes if these relationships are ignored. In this work, we propose a causally faithful framework for counterfactual video generation, guided by a vision-language model (VLM). Our method is agnostic to the underlying video editing system and does not require access to its internal mechanisms or finetuning. Instead, we guide the generation by optimizing text prompts based on an assumed causal graph, addressing the challenge of latent space control in LDMs. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Our results demonstrate that causally faithful video counterfactuals can be effectively generated within the learned distribution of LDMs through prompt-based causal steering. With its compatibility with any black-box video editing system, our method holds significant potential for generating realistic "what-if" video scenarios in diverse areas such as healthcare and digital media.

</details>


### [298] [Align Your Flow: Scaling Continuous-Time Flow Map Distillation](https://arxiv.org/abs/2506.14603)
*Amirmojtaba Sabour,Sanja Fidler,Karsten Kreis*

Key words: Flow Maps, 连续时间目标, autoguidance, 对抗微调, 图像生成

TL;DR: 本文介绍了Flow Maps（对齐流）模型，通过新的连续时间目标函数和训练技术，解决了扩散和流量模型采样步数多的问题，并在图像生成任务中取得了优异表现。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 扩散和流量模型虽然强大，但需多步采样；一致性模型虽能一步生成，但性能随步数增加而下降。Flow Maps通过连接任意噪声水平，在单步和多步中均保持高效。

Method: 提出两种新的连续时间目标函数和训练技术，扩展了现有方法；同时采用autoguidance和对抗微调以提升性能。

Result: 在ImageNet 64x64和512x512上实现了少步生成的SOTA性能，文本到图像任务中也优于现有非对抗训练模型。

Conclusion: Flow Maps在高效采样和性能上取得了平衡，尤其适用于少步生成任务。

Abstract: Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.

</details>


### [299] [Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching](https://arxiv.org/abs/2506.14605)
*Giacomo Meanti,Thomas Ryckeboer,Michael Arbel,Julien Mairal*

Key words: 图像恢复, 无配对数据, 条件流匹配, 逆向问题

TL;DR: 该论文提出了一种基于无配对数据集解决图像恢复任务的方法，适用于正向模型未知或配对数据难以获取的场景。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统方法通常需要完整的正向模型知识或配对的退化与原始图像，而在现实场景中这些条件往往难以满足。

Method: 利用条件流匹配建模退化观测分布，同时通过分布匹配损失自然地学习正向模型。

Result: 在去模糊、非均匀点扩散函数校准等任务上优于单图像盲法与非监督方法，盲超分辨率任务上达到先进水平。

Conclusion: 该方法在无需大量数据采集的情况下，实现了传统需耗时实验和专用设备的任务，如透镜校准。

Abstract: This work addresses image restoration tasks through the lens of inverse problems using unpaired datasets. In contrast to traditional approaches -- which typically assume full knowledge of the forward model or access to paired degraded and ground-truth images -- the proposed method operates under minimal assumptions and relies only on small, unpaired datasets. This makes it particularly well-suited for real-world scenarios, where the forward model is often unknown or misspecified, and collecting paired data is costly or infeasible. The method leverages conditional flow matching to model the distribution of degraded observations, while simultaneously learning the forward model via a distribution-matching loss that arises naturally from the framework. Empirically, it outperforms both single-image blind and unsupervised approaches on deblurring and non-uniform point spread function (PSF) calibration tasks. It also matches state-of-the-art performance on blind super-resolution. We also showcase the effectiveness of our method with a proof of concept for lens calibration: a real-world application traditionally requiring time-consuming experiments and specialized equipment. In contrast, our approach achieves this with minimal data acquisition effort.

</details>


### [300] [Compositional Attribute Imbalance in Vision Datasets](https://arxiv.org/abs/2506.14418)
*Jiayi Chen,Yanbiao Ma,Andi Zhang,Weidong Tang,Wei Dai,Bowei Liu*

Key words: 视觉属性不平衡, CLIP, 数据增强, 长尾分类, 图像分类

TL;DR: 论文提出了一种基于CLIP的框架，构建视觉属性字典，通过调整样本采样概率并结合数据增强技术，解决图像分类中的属性不平衡问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 视觉属性不平衡在图像分类中普遍存在但研究不足，影响模型性能和泛化能力，亟需解决方案。

Method: 定义图像的一级和二级属性，构建CLIP框架的视觉属性字典，分析单属性和组合属性不平衡，调整采样概率并结合多种数据增强技术。

Result: 实验表明，该方法有效缓解属性不平衡问题，提升了深度神经网络的鲁棒性和公平性。

Conclusion: 研究强调了建模视觉属性分布的重要性，并为长尾图像分类提供了可扩展的解决方案。

Abstract: Visual attribute imbalance is a common yet underexplored issue in image classification, significantly impacting model performance and generalization. In this work, we first define the first-level and second-level attributes of images and then introduce a CLIP-based framework to construct a visual attribute dictionary, enabling automatic evaluation of image attributes. By systematically analyzing both single-attribute imbalance and compositional attribute imbalance, we reveal how the rarity of attributes affects model performance. To tackle these challenges, we propose adjusting the sampling probability of samples based on the rarity of their compositional attributes. This strategy is further integrated with various data augmentation techniques (such as CutMix, Fmix, and SaliencyMix) to enhance the model's ability to represent rare attributes. Extensive experiments on benchmark datasets demonstrate that our method effectively mitigates attribute imbalance, thereby improving the robustness and fairness of deep neural networks. Our research highlights the importance of modeling visual attribute distributions and provides a scalable solution for long-tail image classification tasks.

</details>


### [301] [Adapting Lightweight Vision Language Models for Radiological Visual Question Answering](https://arxiv.org/abs/2506.14451)
*Aditya Shourya,Michel Dumontier,Chang Sun*

Key words: 放射学VQA,视觉语言模型,多阶段微调,显著性分析,轻量级模型

TL;DR: 本文探讨了放射学视觉问答（VQA）模型的挑战，并提出了一种轻量级3B参数的视觉语言模型，通过数据生成和多阶段微调，实现了稳健性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 放射学VQA模型发展面临数据不足、图像模式复杂以及缺乏评估等问题，研究旨在通过轻量级模型解决这些挑战。

Method: 采用轻量级3B参数视觉语言模型，通过合成问答对生成和多阶段微调，结合专业放射学数据集（如ROCO v2.0、MedPix v2.0）。

Result: 尽管模型参数规模较小、训练数据有限，其性能接近最先进模型LLaVA-Med，并提出了一种基于显著性的诊断工具。

Conclusion: 轻量级模型在适当微调和数据支持下，能在放射学VQA中表现良好，同时通过诊断工具可识别模型失败模式。

Abstract: Recent advancements in vision-language systems have improved the accuracy of Radiological Visual Question Answering (VQA) Models. However, some challenges remain across each stage of model development: limited expert-labeled images hinders data procurement at scale; the intricate and nuanced patterns of radiological images make modeling inherently difficult; and the lack of evaluation evaluation efforts makes it difficult to identify cases where the model might be ill-conditioned. In this study, we fine-tune a lightweight 3B parameter vision-language model for Radiological VQA, demonstrating that small models, when appropriately tuned with curated data, can achieve robust performance across both open- and closed-ended questions. We propose a cost-effective training pipeline from synthetic question-answer pair generation to multi-stage fine-tuning on specialised radiological domain-targeted datasets (e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a fraction of the scale of state-of-the-art models such as LLaVA-Med, our model achieves promising performance given its small parameter size and the limited scale of training data. We introduce a lightweight saliency-based diagnostic tool that enables domain experts to inspect VQA model performance and identify ill-conditioned failure modes through saliency analysis.

</details>


### [302] [Cost-Aware Routing for Efficient Text-To-Image Generation](https://arxiv.org/abs/2506.14753)
*Qinchan,Li,Kenneth Chen,Changyue,Su,Wittawat Jitkrittum,Qi Sun,Patsorn Sangkloy*

Key words: 扩散模型, 文本到图像生成, 计算成本优化, 动态路由

TL;DR: 通过自动路由提示到不同的文本到图像生成模型或不同步数的扩散模型，本文提出了一种平衡计算成本和生成质量的方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 扩散模型虽然能生成高保真图像，但计算成本高。本文旨在根据提示的复杂性动态调整计算量，以实现计算成本与生成质量的最优平衡。

Method: 提出一个框架，根据提示的复杂度自动路由到适合的文本到图像生成函数，如不同步数的扩散模型或其他独立模型。学习策略保留了高成本选项（如100+步）用于复杂提示，而简单提示使用经济选项（如小型蒸馏模型）。

Result: 在COCO和DiffusionDB上的实证显示，通过路由到九个预训练模型，本文方法平均质量高于任何单一模型。

Conclusion: 通过动态路由机制，实现了计算成本与生成质量的最优平衡，适用于复杂的生成任务。

Abstract: Diffusion models are well known for their ability to generate a high-fidelity image for an input prompt through an iterative denoising process. Unfortunately, the high fidelity also comes at a high computational cost due the inherently sequential generative process. In this work, we seek to optimally balance quality and computational cost, and propose a framework to allow the amount of computation to vary for each prompt, depending on its complexity. Each prompt is automatically routed to the most appropriate text-to-image generation function, which may correspond to a distinct number of denoising steps of a diffusion model, or a disparate, independent text-to-image model. Unlike uniform cost reduction techniques (e.g., distillation, model quantization), our approach achieves the optimal trade-off by learning to reserve expensive choices (e.g., 100+ denoising steps) only for a few complex prompts, and employ more economical choices (e.g., small distilled model) for less sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB that by learning to route to nine already-trained text-to-image models, our approach is able to deliver an average quality that is higher than that achievable by any of these models alone.

</details>


### [303] [Synthetic Data Augmentation for Table Detection: Re-evaluating TableNet's Performance with Automatically Generated Document Images](https://arxiv.org/abs/2506.14583)
*Krishna Sahukara,Zineddine Bettouche,Andreas Fischer*

Key words: 表格提取, LaTeX, 合成数据, 自动化, TableNet

TL;DR: 提出了一种自动生成LaTeX文档页面的方法，用于增强表格提取模型的训练数据，显著降低了手动标注的工作量并提高了性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 智能手机或扫描仪捕捉的文档页面常包含表格，但手动提取效率低且易出错，自动化方法能够解决这一问题。

Method: 引入了一个基于LaTeX的自动化流程，生成具有多样化表格布局和真实掩码的两栏页面，用于增强Marmot基准数据集。

Result: 在合成测试集上，TableNet的像素异或错误率在256x256分辨率下为4.04%，在1024x1024分辨率下为4.33%。在Marmot基准上的最佳表现是9.18%（256x256）。

Conclusion: 通过自动化生成合成数据，显著提高了表格提取的准确性和效率，同时减少了人工标注需求。

Abstract: Document pages captured by smartphones or scanners often contain tables, yet manual extraction is slow and error-prone. We introduce an automated LaTeX-based pipeline that synthesizes realistic two-column pages with visually diverse table layouts and aligned ground-truth masks. The generated corpus augments the real-world Marmot benchmark and enables a systematic resolution study of TableNet. Training TableNet on our synthetic data achieves a pixel-wise XOR error of 4.04% on our synthetic test set with a 256x256 input resolution, and 4.33% with 1024x1024. The best performance on the Marmot benchmark is 9.18% (at 256x256), while cutting manual annotation effort through automation.

</details>


### [304] [PoseGRAF: Geometric-Reinforced Adaptive Fusion for Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2506.14596)
*Ming Xu,Xu Zhang*

Key words: 3D姿态估计,图卷积网络,跨注意力,动态融合,Transformer

TL;DR: PoseGRAF框架通过双图卷积结构和跨注意力模块改进单目3D姿态估计，显著提升了在遮挡和快速运动下的表现。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法忽视骨骼内在方向性和角度相关性，导致在关节遮挡或快速运动时姿态估计不准确。

Method: 提出双图卷积结构处理关节和骨骼图，引入跨注意力模块建模方向与关节特征的依赖关系，动态融合模块自适应整合特征，改进Transformer编码器生成最终输出。

Result: 在Human3.6M和MPI-INF-3DHP数据集上超越现有方法，验证了其在真实场景中的通用性。

Conclusion: PoseGRAF框架通过多特征整合和关系建模显著提升了单目3D姿态估计的准确性和鲁棒性。

Abstract: Existing monocular 3D pose estimation methods primarily rely on joint positional features, while overlooking intrinsic directional and angular correlations within the skeleton. As a result, they often produce implausible poses under joint occlusions or rapid motion changes. To address these challenges, we propose the PoseGRAF framework. We first construct a dual graph convolutional structure that separately processes joint and bone graphs, effectively capturing their local dependencies. A Cross-Attention module is then introduced to model interdependencies between bone directions and joint features. Building upon this, a dynamic fusion module is designed to adaptively integrate both feature types by leveraging the relational dependencies between joints and bones. An improved Transformer encoder is further incorporated in a residual manner to generate the final output. Experimental results on the Human3.6M and MPI-INF-3DHP datasets show that our method exceeds state-of-the-art approaches. Additional evaluations on in-the-wild videos further validate its generalizability. The code is publicly available at https://github.com/iCityLab/PoseGRAF.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [305] [Hard Contacts with Soft Gradients: Refining Differentiable Simulators for Learning and Control](https://arxiv.org/abs/2506.14186)
*Anselm Paulus,A. René Geist,Pierre Schumacher,Vít Musil,Georg Martius*

Key words: 机器人优化,接触力,梯度计算,MuJoCo,仿真与真实差距

TL;DR: DiffMJX结合自适应积分与MuJoCo XLA，改进硬接触下的梯度质量，并引入Contacts From Distance（CFD）机制，解决物体未接触时梯度消失的问题。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 接触力在机器人动力学优化中因速度跳跃和高刚度设置导致梯度计算错误，同时非刚性设置又增加了仿真与现实的差距，需要改进。

Method: 分析惩罚型仿真器的接触计算问题，提出DiffMJX结合自适应积分与MuJoCo XLA，并引入CFD机制在反向传播中生成信息梯度。

Result: DiffMJX显著提升了硬接触下的梯度质量，CFD机制在物体未接触时也能生成有用的梯度信息。

Conclusion: DiffMJX和CFD机制有效解决了接触力优化中的梯度问题，同时保持了物理模拟的真实性。

Abstract: Contact forces pose a major challenge for gradient-based optimization of robot dynamics as they introduce jumps in the system's velocities. Penalty-based simulators, such as MuJoCo, simplify gradient computation by softening the contact forces. However, realistically simulating hard contacts requires very stiff contact settings, which leads to incorrect gradients when using automatic differentiation. On the other hand, using non-stiff settings strongly increases the sim-to-real gap. We analyze the contact computation of penalty-based simulators to identify the causes of gradient errors. Then, we propose DiffMJX, which combines adaptive integration with MuJoCo XLA, to notably improve gradient quality in the presence of hard contacts. Finally, we address a key limitation of contact gradients: they vanish when objects do not touch. To overcome this, we introduce Contacts From Distance (CFD), a mechanism that enables the simulator to generate informative contact gradients even before objects are in contact. To preserve physical realism, we apply CFD only in the backward pass using a straight-through trick, allowing us to compute useful gradients without modifying the forward simulation.

</details>


### [306] [AMPLIFY: Actionless Motion Priors for Robot Learning from Videos](https://arxiv.org/abs/2506.14198)
*Jeremy A. Collins,Loránd Cheng,Kunal Aneja,Albert Wilcox,Benjamin Joffe,Animesh Garg*

Key words: 机器人学习,视频数据,动态建模,策略泛化

TL;DR: AMPLIFY利用大规模无动作视频数据，通过编码关键点轨迹为离散运动标记，解决机器人策略泛化问题。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 机器人动作标记数据稀缺且昂贵，难以泛化学习策略，而无动作视频数据丰富但难以直接应用于策略学习。

Method: 提出AMPLIFY框架，分别训练视觉动态预测模型和动作推理模型，结合无动作视频和有限标记数据。

Result: 动态模型准确性显著提高，下游策略学习表现提升1.2-2.2倍，且首次实现零分布动作数据的任务泛化。

Conclusion: AMPLIFY展示了利用异构数据构建高效、可泛化世界模型的新范式。

Abstract: Action-labeled data for robotics is scarce and expensive, limiting the generalization of learned policies. In contrast, vast amounts of action-free video data are readily available, but translating these observations into effective policies remains a challenge. We introduce AMPLIFY, a novel framework that leverages large-scale video data by encoding visual dynamics into compact, discrete motion tokens derived from keypoint trajectories. Our modular approach separates visual motion prediction from action inference, decoupling the challenges of learning what motion defines a task from how robots can perform it. We train a forward dynamics model on abundant action-free videos and an inverse dynamics model on a limited set of action-labeled examples, allowing for independent scaling. Extensive evaluations demonstrate that the learned dynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x better pixel prediction accuracy compared to prior approaches, and broadly useful. In downstream policy learning, our dynamics predictions enable a 1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by learning from action-free human videos, and the first generalization to LIBERO tasks from zero in-distribution action data. Beyond robotic control, we find the dynamics learned by AMPLIFY to be a versatile latent world model, enhancing video prediction quality. Our results present a novel paradigm leveraging heterogeneous data sources to build efficient, generalizable world models. More information can be found at https://amplify-robotics.github.io/.

</details>


### [307] [Steering Robots with Inference-Time Interactions](https://arxiv.org/abs/2506.14287)
*Yanwei Wang*

Key words: 模仿学习,推理时引导,用户交互,策略校正,任务与动作模仿

TL;DR: 论文提出一种方法，通过在推理时引导预训练策略的行为，使用户能够纠正策略错误，而无需重新微调。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 预训练策略在部署时可能因泛化能力不足而犯错，传统方法需收集额外数据微调，效率低下。本文旨在通过用户交互在推理时纠正策略行为。

Method: 提出两种框架：(1)推理时引导，用户交互切换离散技能；(2)任务与动作模仿，用户编辑连续动作同时满足离散符号计划的任务约束。

Result: 这些框架无需额外训练即可纠正策略预测，提升预训练模型的实用性并满足用户推理时目标。

Conclusion: 该方法通过用户交互实现了对预训练策略的高效校正，避免了微调的开销。

Abstract: Imitation learning has driven the development of generalist policies capable of autonomously solving multiple tasks. However, when a pretrained policy makes errors during deployment, there are limited mechanisms for users to correct its behavior. While collecting additional data for finetuning can address such issues, doing so for each downstream use case is inefficient at deployment. My research proposes an alternative: keeping pretrained policies frozen as a fixed skill repertoire while allowing user interactions to guide behavior generation toward user preferences at inference time. By making pretrained policies steerable, users can help correct policy errors when the model struggles to generalize-without needing to finetune the policy. Specifically, I propose (1) inference-time steering, which leverages user interactions to switch between discrete skills, and (2) task and motion imitation, which enables user interactions to edit continuous motions while satisfying task constraints defined by discrete symbolic plans. These frameworks correct misaligned policy predictions without requiring additional training, maximizing the utility of pretrained models while achieving inference-time user objectives.

</details>


### [308] [Uncertainty-Driven Radar-Inertial Fusion for Instantaneous 3D Ego-Velocity Estimation](https://arxiv.org/abs/2506.14294)
*Prashant Kumar Rai,Elham Kowsari,Nataliya Strokina,Reza Ghabcheloo*

Key words: 自主导航, 雷达, 惯性测量单元, 神经网络, 扩展卡尔曼滤波器

TL;DR: 提出了一种结合高分辨率成像雷达和惯性测量单元的自主导航方法，通过神经网络处理雷达数据估计速度及其不确定性，并通过扩展卡尔曼滤波器与惯性数据融合，显著提高了运动估计的准确性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 传统雷达运动估计方法存在局限性，需结合惯性测量单元提高精度。

Method: 使用神经网络处理雷达原始数据，结合扩展卡尔曼滤波器融合惯性测量数据。

Result: 在ColoRadar数据集上表现优于现有方法，误差更低。

Conclusion: 所提方法显著提升了自主导航中运动估计的精确性和鲁棒性。

Abstract: We present a method for estimating ego-velocity in autonomous navigation by integrating high-resolution imaging radar with an inertial measurement unit. The proposed approach addresses the limitations of traditional radar-based ego-motion estimation techniques by employing a neural network to process complex-valued raw radar data and estimate instantaneous linear ego-velocity along with its associated uncertainty. This uncertainty-aware velocity estimate is then integrated with inertial measurement unit data using an Extended Kalman Filter. The filter leverages the network-predicted uncertainty to refine the inertial sensor's noise and bias parameters, improving the overall robustness and accuracy of the ego-motion estimation. We evaluated the proposed method on the publicly available ColoRadar dataset. Our approach achieves significantly lower error compared to the closest publicly available method and also outperforms both instantaneous and scan matching-based techniques.

</details>


### [309] [GAMORA: A Gesture Articulated Meta Operative Robotic Arm for Hazardous Material Handling in Containment-Level Environments](https://arxiv.org/abs/2506.14513)
*Farha Abdul Wasay,Mohammed Abdul Rahman,Hania Ghouse*

Key words: 虚拟现实, 机器人技术, 逆运动学, 数字孪生, 生物安全

TL;DR: GAMORA是一个基于虚拟现实（VR）和机器人技术的系统，用于高风险的实验室环境，如病毒学实验室，通过手势控制实现远程精确操作。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 随着生物危害复杂性的增加，减少人类直接暴露并保持操作精度变得至关重要。GAMORA旨在提供一种更安全、高效的自动化解决方案。

Method: GAMORA整合了Oculus Quest 2、NVIDIA Jetson Nano和ROS，通过自然手势控制、数字孪生模拟和逆运动学实现实时沉浸式操作。

Result: 系统实现了2.2毫米的平均位置偏差、0.2毫升的移液精度和1.2毫米的重复性，能耗降低50%，并通过YOLOv8增强了空间感知。

Conclusion: GAMORA为生物医学研究中的高风险实验室任务提供了一种可扩展、沉浸式的机器人控制方案，确保安全、精确和可重复性。

Abstract: The convergence of robotics and virtual reality (VR) has enabled safer and more efficient workflows in high-risk laboratory settings, particularly virology labs. As biohazard complexity increases, minimizing direct human exposure while maintaining precision becomes essential. We propose GAMORA (Gesture Articulated Meta Operative Robotic Arm), a novel VR-guided robotic system that enables remote execution of hazardous tasks using natural hand gestures. Unlike existing scripted automation or traditional teleoperation, GAMORA integrates the Oculus Quest 2, NVIDIA Jetson Nano, and Robot Operating System (ROS) to provide real-time immersive control, digital twin simulation, and inverse kinematics-based articulation. The system supports VR-based training and simulation while executing precision tasks in physical environments via a 3D-printed robotic arm. Inverse kinematics ensure accurate manipulation for delicate operations such as specimen handling and pipetting. The pipeline includes Unity-based 3D environment construction, real-time motion planning, and hardware-in-the-loop testing. GAMORA achieved a mean positional discrepancy of 2.2 mm (improved from 4 mm), pipetting accuracy within 0.2 mL, and repeatability of 1.2 mm across 50 trials. Integrated object detection via YOLOv8 enhances spatial awareness, while energy-efficient operation (50% reduced power output) ensures sustainable deployment. The system's digital-physical feedback loop enables safe, precise, and repeatable automation of high-risk lab tasks. GAMORA offers a scalable, immersive solution for robotic control and biosafety in biomedical research environments.

</details>


### [310] [SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning](https://arxiv.org/abs/2506.14648)
*Hexian Ni,Tao Lu,Haoyuan Hu,Yinghao Cai,Shuo Wang*

Key words: PbRL, 反馈效率, 样本效率, 查询选择, 偏好引导

TL;DR: SENIOR通过优化查询选择和偏好引导探索，提高了PbRL的反馈和样本效率。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 避免奖励工程并通过人类偏好学习奖励模型的需求推动了PbRL的发展，但反馈和样本效率问题限制了其应用。

Method: SENIOR采用MDS选择易于标记的片段对，并通过PGE引导探索偏好高且访问少的状态。

Result: 在模拟和真实世界的机器人任务中，SENIOR在反馈效率和策略收敛速度上优于五种现有方法。

Conclusion: SENIOR通过优化查询和探索机制，显著提升了PbRL的性能。

Abstract: Preference-based Reinforcement Learning (PbRL) methods provide a solution to avoid reward engineering by learning reward models based on human preferences. However, poor feedback- and sample- efficiency still remain the problems that hinder the application of PbRL. In this paper, we present a novel efficient query selection and preference-guided exploration method, called SENIOR, which could select the meaningful and easy-to-comparison behavior segment pairs to improve human feedback-efficiency and accelerate policy learning with the designed preference-guided intrinsic rewards. Our key idea is twofold: (1) We designed a Motion-Distinction-based Selection scheme (MDS). It selects segment pairs with apparent motion and different directions through kernel density estimation of states, which is more task-related and easy for human preference labeling; (2) We proposed a novel preference-guided exploration method (PGE). It encourages the exploration towards the states with high preference and low visits and continuously guides the agent achieving the valuable samples. The synergy between the two mechanisms could significantly accelerate the progress of reward and policy learning. Our experiments show that SENIOR outperforms other five existing methods in both human feedback-efficiency and policy convergence speed on six complex robot manipulation tasks from simulation and four real-worlds.

</details>


### [311] [Casper: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models](https://arxiv.org/abs/2506.14727)
*Huihan Liu,Rutav Shah,Shuijing Liu,Jack Pittenger,Mingyo Seo,Yuchen Cui,Yonatan Bisk,Roberto Martín-Martín,Yuke Zhu*

Key words: 辅助远程操作, 视觉语言模型, 意图推断, 移动操作, 常识推理

TL;DR: Casper是一个辅助远程操作系统，利用预训练的视觉语言模型（VLMs）中的常识推理能力，实现实时意图推断和灵活技能执行，显著提升任务完成效率和用户满意度。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决现有辅助远程操作系统在复杂、非结构化环境中推断人类意图和提供协助的局限性。

Method: 结合开放世界感知模块、VLM驱动的意图推断机制和技能库，支持多样化的移动操作任务。

Result: 实验表明，Casper在任务表现、减轻用户认知负担和提高用户满意度方面优于直接远程操作和其他基线方法。

Conclusion: 通过引入常识推理和开放世界感知，Casper能够更灵活地支持真实世界中的辅助远程操作。

Abstract: Assistive teleoperation, where control is shared between a human and a robot, enables efficient and intuitive human-robot collaboration in diverse and unstructured environments. A central challenge in real-world assistive teleoperation is for the robot to infer a wide range of human intentions from user control inputs and to assist users with correct actions. Existing methods are either confined to simple, predefined scenarios or restricted to task-specific data distributions at training, limiting their support for real-world assistance. We introduce Casper, an assistive teleoperation system that leverages commonsense knowledge embedded in pre-trained visual language models (VLMs) for real-time intent inference and flexible skill execution. Casper incorporates an open-world perception module for a generalized understanding of novel objects and scenes, a VLM-powered intent inference mechanism that leverages commonsense reasoning to interpret snippets of teleoperated user input, and a skill library that expands the scope of prior assistive teleoperation systems to support diverse, long-horizon mobile manipulation tasks. Extensive empirical evaluation, including human studies and system ablations, demonstrates that Casper improves task performance, reduces human cognitive load, and achieves higher user satisfaction than direct teleoperation and assistive teleoperation baselines.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [312] [A Survey on World Models Grounded in Acoustic Physical Information](https://arxiv.org/abs/2506.13833)
*Xiaoliang Chen,Le Chang,Xin Yu,Yunhe Huang,Xianling Tu*

Key words: 声学世界模型，物理信息神经网络，因果推理，多模态学习

TL;DR: 本文综述了基于声学物理信息的声学世界模型研究领域，涵盖了理论、方法论、技术进展及应用，并讨论了未来的技术挑战与研究方向。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 探索声学信号作为高保真环境感知、因果物理推理和动态事件预测模拟的新途径。

Method: 包括物理信息神经网络（PINNs）、生成模型和自监督多模态学习框架。

Result: 展示了声学世界模型在机器人、自动驾驶、医疗和金融等领域的重要应用。

Conclusion: 提出了实现稳健、因果、不确定性感知和负责任的声学智能的未来研究路线。

Abstract: This survey provides a comprehensive overview of the emerging field of world models grounded in the foundation of acoustic physical information. It examines the theoretical underpinnings, essential methodological frameworks, and recent technological advancements in leveraging acoustic signals for high-fidelity environmental perception, causal physical reasoning, and predictive simulation of dynamic events. The survey explains how acoustic signals, as direct carriers of mechanical wave energy from physical events, encode rich, latent information about material properties, internal geometric structures, and complex interaction dynamics. Specifically, this survey establishes the theoretical foundation by explaining how fundamental physical laws govern the encoding of physical information within acoustic signals. It then reviews the core methodological pillars, including Physics-Informed Neural Networks (PINNs), generative models, and self-supervised multimodal learning frameworks. Furthermore, the survey details the significant applications of acoustic world models in robotics, autonomous driving, healthcare, and finance. Finally, it systematically outlines the important technical and ethical challenges while proposing a concrete roadmap for future research directions toward robust, causal, uncertainty-aware, and responsible acoustic intelligence. These elements collectively point to a research pathway towards embodied active acoustic intelligence, empowering AI systems to construct an internal "intuitive physics" engine through sound.

</details>


### [313] [Acoustic scattering AI for non-invasive object classifications: A case study on hair assessment](https://arxiv.org/abs/2506.14148)
*Long-Vu Hoang,Tuan Nguyen,Tran Huy Dat*

Key words: 声学散射,深度学习,非侵入式分类,头发评估

TL;DR: 本文提出了一种基于声学散射的非侵入式物体分类方法，并通过头发评估案例展示了其有效性。利用AI驱动的深度学习声音分类，可以高精度分类头发类型和湿度。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 探索一种隐私保护、非接触式的替代视觉分类的方法，利用声学散射技术实现物体分类。

Method: 采用多种深度学习策略，包括全监督学习、基于嵌入的分类、监督基础模型微调和自监督模型微调。

Result: 最佳策略通过微调自监督模型的所有参数，实现了接近90%的分类准确率。

Conclusion: 声学散射技术在物体分类中表现出巨大潜力，适用于多种行业应用。

Abstract: This paper presents a novel non-invasive object classification approach using acoustic scattering, demonstrated through a case study on hair assessment. When an incident wave interacts with an object, it generates a scattered acoustic field encoding structural and material properties. By emitting acoustic stimuli and capturing the scattered signals from head-with-hair-sample objects, we classify hair type and moisture using AI-driven, deep-learning-based sound classification. We benchmark comprehensive methods, including (i) fully supervised deep learning, (ii) embedding-based classification, (iii) supervised foundation model fine-tuning, and (iv) self-supervised model fine-tuning. Our best strategy achieves nearly 90% classification accuracy by fine-tuning all parameters of a self-supervised model. These results highlight acoustic scattering as a privacy-preserving, non-contact alternative to visual classification, opening huge potential for applications in various industries.

</details>


### [314] [Pushing the Performance of Synthetic Speech Detection with Kolmogorov-Arnold Networks and Self-Supervised Learning Models](https://arxiv.org/abs/2506.14153)
*Tuan Dat Phuong,Long-Vu Hoang,Huy Dat Tran*

Key words: 语音合成, 自监督学习, Kolmogorov-Arnold网络, 说话人验证, ASVspoof2021

TL;DR: 论文提出用Kolmogorov-Arnold网络（KAN）替代XLSR-Conformer模型中的多层感知机，显著提升了语音伪造检测性能。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 语音合成技术的进步导致伪造攻击日益复杂，给自动说话人验证系统带来了挑战，需改进现有模型的架构。

Method: 用Kolmogorov-Arnold网络（KAN）替代XLSR-Conformer模型中的传统多层感知机。

Result: 在ASVspoof2021数据集上，相对性能提升60.55%，在21LA集上EER达到0.70%。

Conclusion: 将KAN集成到基于自监督学习的模型中，是语音伪造检测领域的一个有前景的方向。

Abstract: Recent advancements in speech synthesis technologies have led to increasingly advanced spoofing attacks, posing significant challenges for automatic speaker verification systems. While systems based on self-supervised learning (SSL) models, particularly the XLSR-Conformer model, have demonstrated remarkable performance in synthetic speech detection, there remains room for architectural improvements. In this paper, we propose a novel approach that replaces the traditional Multi-Layer Perceptron in the XLSR-Conformer model with a Kolmogorov-Arnold Network (KAN), a novel architecture based on the Kolmogorov-Arnold representation theorem. Our results on ASVspoof2021 demonstrate that integrating KAN into the SSL-based models can improve the performance by 60.55% relatively on LA and DF sets, further achieving 0.70% EER on the 21LA set. These findings suggest that incorporating KAN into SSL-based models is a promising direction for advances in synthetic speech detection.

</details>


### [315] [Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature Transcription](https://arxiv.org/abs/2506.14223)
*Anna Hamberger,Sebastian Murgul,Jochen Schmidt,Michael Heizmann*

Key words: 音乐转录,吉他指法谱,T5变换器,演奏性,数据预处理

TL;DR: Fretting-Transformer是一种基于T5变换器架构的编码解码模型，用于将MIDI序列自动转录为吉他指法谱，解决弦-品歧义性和演奏性问题。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 音乐转录在音乐信息检索中至关重要，尤其是对于吉他等弦乐器，传统MIDI符号缺乏演奏性信息。

Method: 采用T5变换器架构，提出新的数据预处理和分词策略，整合上下文处理和调音/变调夹条件。

Result: 实验表明，Fretting-Transformer在指法谱准确性和演奏性上优于A*算法和商业软件Guitar Pro。

Conclusion: 模型为自动化吉他转录的未来发展奠定了坚实基础。

Abstract: Music transcription plays a pivotal role in Music Information Retrieval (MIR), particularly for stringed instruments like the guitar, where symbolic music notations such as MIDI lack crucial playability information. This contribution introduces the Fretting-Transformer, an encoderdecoder model that utilizes a T5 transformer architecture to automate the transcription of MIDI sequences into guitar tablature. By framing the task as a symbolic translation problem, the model addresses key challenges, including string-fret ambiguity and physical playability. The proposed system leverages diverse datasets, including DadaGP, GuitarToday, and Leduc, with novel data pre-processing and tokenization strategies. We have developed metrics for tablature accuracy and playability to quantitatively evaluate the performance. The experimental results demonstrate that the Fretting-Transformer surpasses baseline methods like A* and commercial applications like Guitar Pro. The integration of context-sensitive processing and tuning/capo conditioning further enhances the model's performance, laying a robust foundation for future developments in automated guitar transcription.

</details>


### [316] [Making deep neural networks work for medical audio: representation, compression and domain adaptation](https://arxiv.org/abs/2506.13970)
*Charles C Onu*

Key words: 医疗音频分析,婴儿哭声,迁移学习,模型压缩,领域适应

TL;DR: 本文探讨了机器学习在医疗音频信号分析中的技术挑战，提出创新方法，包括迁移学习、模型压缩和领域适应技术，并发布了开源婴儿哭声数据集。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 通过自动分析医疗音频信号（如婴儿哭声）以解决医疗资源不足和早期诊断的问题，推动标准化和高效的医疗声音处理。

Method: 采用迁移学习（利用成人语音数据）、端到端模型压缩（基于张量分解）和领域适应技术（结合计算机视觉方法）。

Result: 开发了更准确的婴儿哭声分析模型、高效的压缩模型以及增强的跨领域泛化性能，并提供了开源数据集。

Conclusion: 婴儿哭声可被视为生命体征，AI驱动的音频监测有望在可及和可负担医疗保健中发挥变革性作用。

Abstract: This thesis addresses the technical challenges of applying machine learning to understand and interpret medical audio signals. The sounds of our lungs, heart, and voice convey vital information about our health. Yet, in contemporary medicine, these sounds are primarily analyzed through auditory interpretation by experts using devices like stethoscopes. Automated analysis offers the potential to standardize the processing of medical sounds, enable screening in low-resource settings where physicians are scarce, and detect subtle patterns that may elude human perception, thereby facilitating early diagnosis and treatment.
  Focusing on the analysis of infant cry sounds to predict medical conditions, this thesis contributes on four key fronts. First, in low-data settings, we demonstrate that large databases of adult speech can be harnessed through neural transfer learning to develop more accurate and robust models for infant cry analysis. Second, in cost-effective modeling, we introduce an end-to-end model compression approach for recurrent networks using tensor decomposition. Our method requires no post-hoc processing, achieves compression rates of several hundred-fold, and delivers accurate, portable models suitable for resource-constrained devices. Third, we propose novel domain adaptation techniques tailored for audio models and adapt existing methods from computer vision. These approaches address dataset bias and enhance generalization across domains while maintaining strong performance on the original data. Finally, to advance research in this domain, we release a unique, open-source dataset of infant cry sounds, developed in collaboration with clinicians worldwide.
  This work lays the foundation for recognizing the infant cry as a vital sign and highlights the transformative potential of AI-driven audio monitoring in shaping the future of accessible and affordable healthcare.

</details>


### [317] [SLEEPING-DISCO 9M: A large-scale pre-training dataset for generative music modeling](https://arxiv.org/abs/2506.14293)
*Tawsif Ahmed,Andrej Radonjic,Gollam Rabby*

Key words: 音乐生成、预训练数据集、流行音乐

TL;DR: Sleeping-DISCO 9M 是一个面向音乐生成任务的大规模预训练数据集，填补了现有开源数据集中缺乏高质量、流行音乐的空白。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 当前开源数据集中缺乏反映真实世界流行音乐的高质量数据，限制了生成音乐模型的发展。

Method: 利用实际流行音乐和世界知名艺术家的作品构建数据集。

Result: 提供了具有实际音乐风格的数据集，有助于解决现有数据集未能反映真实音乐的问题。

Conclusion: Sleeping-DISCO 9M 为生成音乐任务提供了更贴近真实音乐的数据支持。

Abstract: We present Sleeping-DISCO 9M, a large-scale pre-training dataset for music and song. To the best of our knowledge, there are no open-source high-quality dataset representing popular and well-known songs for generative music modeling tasks such as text-music, music-captioning, singing-voice synthesis, melody reconstruction and cross-model retrieval. Past contributions focused on isolated and constrained factors whose core perspective was to create synthetic or re-recorded music corpus (e.g. GTSinger, M4Singer) and arbitrarily large-scale audio datasets (e.g. DISCO-10M and LAIONDISCO-12M) had been another focus for the community. Unfortunately, adoption of these datasets has been below substantial in the generative music community as these datasets fail to reflect real-world music and its flavour. Our dataset changes this narrative and provides a dataset that is constructed using actual popular music and world-renowned artists.

</details>


### [318] [Unifying Streaming and Non-streaming Zipformer-based ASR](https://arxiv.org/abs/2506.14434)
*Bidisha Sharma,Karthik Pandia Durai,Shankar Venkatesan,Jeena J Prakash,Shashi Kumar,Malolan Chetlur,Andreas Stolcke*

Key words: 流式ASR, 非流式ASR, Zipformer, 动态右上下文, 分块注意力掩码

TL;DR: 提出了一种统一的流式与非流式ASR模型框架，通过动态右上下文和分块注意力掩码技术，在Zipformer模型中实现更高的效率，显著降低词错误率并灵活控制延迟与准确度的权衡。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 减少开发、训练和部署流式与非流式ASR模型的成本，提出统一框架。

Method: 利用动态右上下文和分块注意力掩码技术训练Zipformer模型，分析右上下文帧数对流式模型性能的影响。

Result: 在Librispeech和内部数据集上测试，相对词错误率降低7.9%，用户感知延迟略有增加；通过增加右上下文帧数，流式模型性能接近非流式模型。

Conclusion: 该方法显著提升流式ASR性能，并可根据需求灵活调整延迟与准确度的平衡。

Abstract: There has been increasing interest in unifying streaming and non-streaming automatic speech recognition (ASR) models to reduce development, training, and deployment costs. We present a unified framework that trains a single end-to-end ASR model for both streaming and non-streaming applications, leveraging future context information. We propose to use dynamic right-context through the chunked attention masking in the training of zipformer-based ASR models. We demonstrate that using right-context is more effective in zipformer models compared to other conformer models due to its multi-scale nature. We analyze the effect of varying the number of right-context frames on accuracy and latency of the streaming ASR models. We use Librispeech and large in-house conversational datasets to train different versions of streaming and non-streaming models and evaluate them in a production grade server-client setup across diverse testsets of different domains. The proposed strategy reduces word error by relative 7.9\% with a small degradation in user-perceived latency. By adding more right-context frames, we are able to achieve streaming performance close to that of non-streaming models. Our approach also allows flexible control of the latency-accuracy tradeoff according to customers requirements.

</details>


### [319] [Refining music sample identification with a self-supervised graph neural network](https://arxiv.org/abs/2506.14684)
*Aditya Bhattacharjee,Ivan Meresman Higgs,Mark Sandler,Emmanouil Benetos*

Key words: 自动样本识别,图神经网络,对比学习,音频检索,短查询

TL;DR: 该论文提出了一种轻量级、可扩展的编码架构，结合图神经网络和对比学习框架，用于解决音频样本识别中的挑战。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 自动样本识别（ASID）在音频查询检索中至关重要，但现有系统难以识别经过音乐修改的样本，因此需要一种能抵抗音乐制作变换的鲁棒系统。

Method: 采用图神经网络与对比学习框架结合的轻量级编码架构，提出两阶段检索方法：粗粒度相似性搜索初选候选，再通过交叉注意力分类器精修匹配结果。

Result: 新模型参数仅为当前最优系统的9%，性能相当，平均精度（mAP）达44.2%，并在短查询场景下通过新标注数据集Sample100进行验证。

Conclusion: 该方法显著提升了样本识别的效率和鲁棒性，尤其适用于短查询场景，为音频检索领域提供了重要参考。

Abstract: Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under "real world" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge.
  In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.
  To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.

</details>


### [320] [Adaptive Accompaniment with ReaLchords](https://arxiv.org/abs/2506.14723)
*Yusong Wu,Tim Cooijmans,Kyle Kastner,Adam Roberts,Ian Simon,Alexander Scarlatos,Chris Donahue,Cassie Tarakajian,Shayegan Omidshafiei,Aaron Courville,Pablo Samuel Castro,Natasha Jaques,Cheng-Zhi Anna Huang*

Key words: 在线生成模型,和弦伴奏,强化学习,协作创作

TL;DR: ReaLchords是一个在线生成模型，用于实时生成和弦伴奏，通过强化学习调整预训练模型，实现与用户旋律的即时互动。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 当前音乐生成模型无法在线与其他音乐家协同创作，需要一种能够即时响应的生成模型。

Method: 使用最大似然预训练模型，并通过强化学习微调，融入奖励模型和未来旋律信息蒸馏技术。

Result: 模型能适应不熟悉输入并生成合适的伴奏，通过量化实验和听力测试验证其效果。

Conclusion: ReaLchords为实时协作创作音乐及其他模态开辟了新途径。

Abstract: Jamming requires coordination, anticipation, and collaborative creativity between musicians. Current generative models of music produce expressive output but are not able to generate in an \emph{online} manner, meaning simultaneously with other musicians (human or otherwise). We propose ReaLchords, an online generative model for improvising chord accompaniment to user melody. We start with an online model pretrained by maximum likelihood, and use reinforcement learning to finetune the model for online use. The finetuning objective leverages both a novel reward model that provides feedback on both harmonic and temporal coherency between melody and chord, and a divergence term that implements a novel type of distillation from a teacher model that can see the future melody. Through quantitative experiments and listening tests, we demonstrate that the resulting model adapts well to unfamiliar input and produce fitting accompaniment. ReaLchords opens the door to live jamming, as well as simultaneous co-creation in other modalities.

</details>


### [321] [Exploring Speaker Diarization with Mixture of Experts](https://arxiv.org/abs/2506.14750)
*Gaobin Yang,Maokui He,Shutong Niu,Ruoyu Wang,Hang Chen,Jun Du*

Key words: 说话人分割,记忆感知,序列到序列,专家混合

TL;DR: 提出了结合记忆感知多说话人嵌入和序列到序列架构的说话人分割系统NSD-MS2S，并通过引入共享软专家混合模块（SS-MoE）提升性能。实验验证了其在复杂场景中的优越性。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 解决说话人分割任务中传统方法的不足，提升在复杂声学环境中的分割准确性和鲁棒性。

Method: 结合记忆感知多说话人嵌入模块与序列到序列架构，并引入SS-MoE模块减轻模型偏见。

Result: 在多个数据集上取得了最佳性能。

Conclusion: 所提方法在复杂场景中表现出色，为说话人分割任务提供了有效解决方案。

Abstract: In this paper, we propose a novel neural speaker diarization system using memory-aware multi-speaker embedding with sequence-to-sequence architecture (NSD-MS2S), which integrates a memory-aware multi-speaker embedding module with a sequence-to-sequence architecture. The system leverages a memory module to enhance speaker embeddings and employs a Seq2Seq framework to efficiently map acoustic features to speaker labels. Additionally, we explore the application of mixture of experts in speaker diarization, and introduce a Shared and Soft Mixture of Experts (SS-MoE) module to further mitigate model bias and enhance performance. Incorporating SS-MoE leads to the extended model NSD-MS2S-SSMoE. Experiments on multiple complex acoustic datasets, including CHiME-6, DiPCo, Mixer 6 and DIHARD-III evaluation sets, demonstrate meaningful improvements in robustness and generalization. The proposed methods achieve state-of-the-art results, showcasing their effectiveness in challenging real-world scenarios.

</details>
