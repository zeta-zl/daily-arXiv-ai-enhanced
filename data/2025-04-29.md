<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 66]
- [cs.LG](#cs.LG) [Total: 102]
- [cs.AI](#cs.AI) [Total: 33]
- [cs.CC](#cs.CC) [Total: 2]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 36]
- [cs.MM](#cs.MM) [Total: 2]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.RO](#cs.RO) [Total: 12]
- [stat.CO](#stat.CO) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 5]
- [econ.GN](#econ.GN) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [stat.ML](#stat.ML) [Total: 11]
- [math.OC](#math.OC) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.CY](#cs.CY) [Total: 6]
- [cs.CR](#cs.CR) [Total: 18]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 4]
- [eess.SY](#eess.SY) [Total: 5]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.HC](#cs.HC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 3]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [eess.AS](#eess.AS) [Total: 3]
- [stat.ME](#stat.ME) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages](https://arxiv.org/abs/2504.18560)
*Alessio Buscemi,Cédric Lothritz,Sergio Morales,Marcos Gomez-Vazquez,Robert Clarisó,Jordi Cabot,German Castignani*

Main category: cs.CL

TL;DR: MLA-BiTe是一种改进的多语言偏见测试框架，通过自动翻译和改写技术评估LLM在不同语言环境中的偏见，测试了四种先进LLM在六种语言中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM展现出强大的自然语言处理能力，但其训练数据中的社会偏见问题依旧存在。研究旨在开发一种系统性多语言偏见测试方法，以更全面地评估和解决这一问题。

Method: 采用MultiLingual Augmented Bias Testing (MLA-BiTe)框架，结合自动翻译和改写技术，评估四种LLM在六种语言（含两种低资源语言）中的偏见表现，涵盖七种敏感歧视类别。

Result: MLA-BiTe框架成功支持了多语言环境下的系统性偏见测试，揭示了LLM在不同语言中的偏见表现。

Conclusion: MLA-BiTe为多语言偏见评估提供了有效工具，未来可用于更广泛的LLM偏见测试和缓解研究。

Abstract: Large Language Models (LLMs) have exhibited impressive natural language
processing capabilities but often perpetuate social biases inherent in their
training data. To address this, we introduce MultiLingual Augmented Bias
Testing (MLA-BiTe), a framework that improves prior bias evaluation methods by
enabling systematic multilingual bias testing. MLA-BiTe leverages automated
translation and paraphrasing techniques to support comprehensive assessments
across diverse linguistic settings. In this study, we evaluate the
effectiveness of MLA-BiTe by testing four state-of-the-art LLMs in six
languages -- including two low-resource languages -- focusing on seven
sensitive categories of discrimination.

</details>


### [2] [Span-Level Hallucination Detection for LLM-Generated Answers](https://arxiv.org/abs/2504.18639)
*Passant Elchafei,Mervet Abu-Elkheir*

Main category: cs.CL

TL;DR: 该论文提出了一种基于角色语义标注（SRL）的跨度级幻觉检测框架，通过结合参考上下文的文本蕴含模型和置信度衡量，在Mu-SHROOM数据集上表现优异，并通过GPT-4和LLaMA验证了幻觉跨度。


<details>
  <summary>Details</summary>
Motivation: 检测大语言模型（LLM）生成答案中的幻觉跨度对提升事实一致性至关重要，尤其是在英语和阿拉伯语文本中。现有的方法需要更细粒度的分析来有效识别幻觉内容。

Method: 论文采用语义角色标注（SRL）将答案分解为原子角色，与通过问题提示检索的参考上下文进行比较，基于DeBERTa的文本蕴含模型评估语义对齐，并通过输出logits的置信度优化分数。

Result: 在Mu-SHROOM数据集上的实验表明该方法具有竞争性性能，幻觉跨度经GPT-4和LLaMA验证有效。

Conclusion: 该研究为LLM生成回答中的幻觉检测提供了有效的细粒度框架，结合语义分析和置信度优化，提升了检测准确性和实用性。

Abstract: Detecting spans of hallucination in LLM-generated answers is crucial for
improving factual consistency. This paper presents a span-level hallucination
detection framework for the SemEval-2025 Shared Task, focusing on English and
Arabic texts. Our approach integrates Semantic Role Labeling (SRL) to decompose
the answer into atomic roles, which are then compared with a retrieved
reference context obtained via question-based LLM prompting. Using a
DeBERTa-based textual entailment model, we evaluate each role semantic
alignment with the retrieved context. The entailment scores are further refined
through token-level confidence measures derived from output logits, and the
combined scores are used to detect hallucinated spans. Experiments on the
Mu-SHROOM dataset demonstrate competitive performance. Additionally,
hallucinated spans have been verified through fact-checking by prompting GPT-4
and LLaMA. Our findings contribute to improving hallucination detection in
LLM-generated responses.

</details>


### [3] [Can Third-parties Read Our Emotions?](https://arxiv.org/abs/2504.18673)
*Jiayi Li,Yingfan Zhou,Pranav Narayanan Venkit,Halima Binte Islam,Sneha Arya,Shomir Wilson,Sarah Rajtmajer*

Main category: cs.CL

TL;DR: 论文探讨了第三方标注（包括人类和LLMs）在捕捉作者真实情绪时的局限性，发现LLMs表现优于人类标注者，并提出了通过作者与标注者的人口统计学相似性或信息提示来改进标注质量的框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于验证第三方标注是否能准确反映作者的真实情绪状态，因为现有的NLP任务通常依赖第三方标注，但这一假设尚未充分验证。

Method: 通过实验直接比较第三方（人类和LLMs）与第一方（作者自身）的情绪标注，并探索如何通过人口统计学相似性或信息提示改进标注质量。

Result: 发现第三方标注（尤其是LLMs）优于人类标注者，但仍有显著局限性；人口统计学相似性或信息提示能略微提升标注准确性。

Conclusion: 结论呼吁改进标注实践以更准确地建模作者的真实情绪状态，并提出了评估第三方标注局限性的框架。

Abstract: Natural Language Processing tasks that aim to infer an author's private
states, e.g., emotions and opinions, from their written text, typically rely on
datasets annotated by third-party annotators. However, the assumption that
third-party annotators can accurately capture authors' private states remains
largely unexamined. In this study, we present human subjects experiments on
emotion recognition tasks that directly compare third-party annotations with
first-party (author-provided) emotion labels. Our findings reveal significant
limitations in third-party annotations-whether provided by human annotators or
large language models (LLMs)-in faithfully representing authors' private
states. However, LLMs outperform human annotators nearly across the board. We
further explore methods to improve third-party annotation quality. We find that
demographic similarity between first-party authors and third-party human
annotators enhances annotation performance. While incorporating first-party
demographic information into prompts leads to a marginal but statistically
significant improvement in LLMs' performance. We introduce a framework for
evaluating the limitations of third-party annotations and call for refined
annotation practices to accurately represent and model authors' private states.

</details>


### [4] [Spatial Speech Translation: Translating Across Space With Binaural Hearables](https://arxiv.org/abs/2504.18715)
*Tuochao Chen,Qirui Wang,Runlin He,Shyam Gollakota*

Main category: cs.CL

TL;DR: 论文提出了一种新颖的“空间语音翻译”概念，通过智能耳机实现在嘈杂环境中将外语实时翻译为母语，同时保留原说话者的空间方位和声音特征。技术挑战包括盲源分离、定位、实时表达性翻译和双耳渲染，并在Apple M2芯片上实现了实时推理。原型测试显示其BLEU得分高达22.01，且用户研究证实了系统在真实混响环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决传统语音翻译在嘈杂和多说话者环境中失效的问题，同时保留说话者的空间感知信息。

Method: 结合盲源分离、说话者定位、实时表达性翻译和双耳渲染技术，并在Apple M2芯片上实现实时处理。

Result: 原型测试在强干扰下取得了最高22.01的BLEU分数，用户研究验证了系统在真实环境中的空间渲染效果。

Conclusion: 该研究首次将空间感知融入语音翻译，为智能耳机在多语言环境中的应用提供了新方向。

Abstract: Imagine being in a crowded space where people speak a different language and
having hearables that transform the auditory space into your native language,
while preserving the spatial cues for all speakers. We introduce spatial speech
translation, a novel concept for hearables that translate speakers in the
wearer's environment, while maintaining the direction and unique voice
characteristics of each speaker in the binaural output. To achieve this, we
tackle several technical challenges spanning blind source separation,
localization, real-time expressive translation, and binaural rendering to
preserve the speaker directions in the translated audio, while achieving
real-time inference on the Apple M2 silicon. Our proof-of-concept evaluation
with a prototype binaural headset shows that, unlike existing models, which
fail in the presence of interference, we achieve a BLEU score of up to 22.01
when translating between languages, despite strong interference from other
speakers in the environment. User studies further confirm the system's
effectiveness in spatially rendering the translated speech in previously unseen
real-world reverberant environments. Taking a step back, this work marks the
first step towards integrating spatial perception into speech translation.

</details>


### [5] [Building UD Cairo for Old English in the Classroom](https://arxiv.org/abs/2504.18718)
*Lauren Levine,Junghyun Min,Amir Zeldes*

Main category: cs.CL

TL;DR: 总结：论文基于UD Cairo句子为古英语构建了一个样本树库，通过LLM提示和真实古英语数据搜索收集数据，由学生进行标注并比较。结果显示LLM输出的古英语语法不够真实但可通过编辑改善，初级标注员虽不完美但集体能产出良好结果。现代英语训练数据对古英语的解析效果不佳，但对标注特征（如词元、超词元、注释）进行解析能提升性能。


<details>
  <summary>Details</summary>
Motivation: 动机：为古英语构建一个样本树库，探索如何利用现代技术（如LLM）和学生标注来应对古英语数据稀缺和标注困难的挑战。

Method: 方法：通过LLM提示和真实古英语数据搜索收集20个句子样本，由学生进行UD标注并比较结果，同时用现代英语训练数据进行初步解析实验。

Result: 结果：LLM生成的古英语语法不够真实但可编辑优化；初级标注员的集体标注效果良好；现代英语数据对古英语解析效果差，但对标注特征的解析表现更好。

Conclusion: 结论：结合人类编辑和集体标注可有效处理古英语数据，标注特征解析为未来工作提供了方向。

Abstract: In this paper we present a sample treebank for Old English based on the UD
Cairo sentences, collected and annotated as part of a classroom curriculum in
Historical Linguistics. To collect the data, a sample of 20 sentences
illustrating a range of syntactic constructions in the world's languages, we
employ a combination of LLM prompting and searches in authentic Old English
data. For annotation we assigned sentences to multiple students with limited
prior exposure to UD, whose annotations we compare and adjudicate. Our results
suggest that while current LLM outputs in Old English do not reflect authentic
syntax, this can be mitigated by post-editing, and that although beginner
annotators do not possess enough background to complete the task perfectly,
taken together they can produce good results and learn from the experience. We
also conduct preliminary parsing experiments using Modern English training
data, and find that although performance on Old English is poor, parsing on
annotated features (lemma, hyperlemma, gloss) leads to improved performance.

</details>


### [6] [EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers](https://arxiv.org/abs/2504.18736)
*Jianyou Wang,Weili Cao,Kaicheng Wang,Xiaoyue Wang,Ashish Dalvi,Gino Prasad,Qishan Liang,Hsuan-lin Her,Ming Wang,Qin Yang,Gene W. Yeo,David E. Neal,Maxim Khan,Christopher D. Rosin,Ramamohan Paturi,Leon Bergen*

Main category: cs.CL

TL;DR: 论文提出了EvidenceBench，用于衡量模型在生物医学论文中自动查找相关证据的能力，并通过人工专家标注验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究人员在调查科学假设时需要找到相关证据，这是一个重要步骤，因此需要自动化的工具来支持。

Method: 通过假设生成和逐句标注的流程创建EvidenceBench，并使用了多种语言模型和检索系统进行评估。

Result: 模型性能仍显著低于专家水平，但验证了流程的可行性和准确性。

Conclusion: EvidenceBench及其扩展版本EvidenceBench-100k为模型训练和开发提供了资源，展示了任务的挑战性和扩展性。

Abstract: We study the task of automatically finding evidence relevant to hypotheses in
biomedical papers. Finding relevant evidence is an important step when
researchers investigate scientific hypotheses. We introduce EvidenceBench to
measure models performance on this task, which is created by a novel pipeline
that consists of hypothesis generation and sentence-by-sentence annotation of
biomedical papers for relevant evidence, completely guided by and faithfully
following existing human experts judgment. We demonstrate the pipeline's
validity and accuracy with multiple sets of human-expert annotations. We
evaluated a diverse set of language models and retrieval systems on the
benchmark and found that model performances still fall significantly short of
the expert level on this task. To show the scalability of our proposed
pipeline, we create a larger EvidenceBench-100k with 107,461 fully annotated
papers with hypotheses to facilitate model training and development. Both
datasets are available at https://github.com/EvidenceBench/EvidenceBench

</details>


### [7] [SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning](https://arxiv.org/abs/2504.18762)
*Ojasw Upadhyay,Abishek Saravankumar,Ayman Ismail*

Main category: cs.CL

TL;DR: SynLexLM提出了一种高效预训练法律领域大语言模型的方法，结合课程学习和合成数据增强，以解决数据稀缺问题，并在法律基准测试中表现优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 通用预训练难以捕捉法律领域的细微差别，且法律数据获取困难。本研究旨在提高法律文档分析和研究工具的性能，推动法律AI的普及。

Method: 采用课程学习（从简单到复杂的法律文本）和合成数据增强（利用如Gemini Pro等模型生成QA对），以缓解数据不足问题。

Result: 在BigLaw-Bench和EUR-Lex-Sum等法律基准测试中，SynLexLM表现优于传统模型及微调版本。

Conclusion: SynLexLM通过创新方法提升了法律领域LLM的性能，有望推动法律AI工具的发展与应用。

Abstract: Large Language Models (LLMs) are powerful but often require extensive
fine-tuning and large datasets for specialized domains like law.
General-purpose pre-training may not capture legal nuances, and acquiring
sufficient legal data is challenging. We introduce SynLexLM, a novel approach
to efficiently pre-train a legal LLM. Our method employs curriculum learning,
progressing from simple to complex legal texts and queries, combined with
synthetic data augmentation using models like Gemini Pro to address data
scarcity. We aim to achieve improved performance on legal benchmarks
(BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned
versions. Preliminary work involves generating synthetic QA pairs reflecting
legal reasoning. This work aims to enhance legal document analysis and research
tools, potentially democratizing access to advanced legal AI.

</details>


### [8] [Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation](https://arxiv.org/abs/2504.18805)
*Jong Inn Park,Maanas Taneja,Qianwen Wang,Dongyeop Kang*

Main category: cs.CL

TL;DR: 提出了一种名为SciTalk的多LLM代理框架，用于从科学论文生成准确且引人入胜的短视频，通过迭代反馈优化生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有端到端方法生成科学短视频中的事实错误和视觉伪影问题，提升科学传播的准确性。

Method: 采用多LLM代理框架，包括内容摘要、视觉场景规划、文本与布局编辑，以及模拟用户反馈的迭代机制。

Result: 实验表明SciTalk在生成科学准确且吸引人的内容上优于简单提示方法，但尚未达到人类创作者的水平。

Conclusion: 该框架为反馈驱动的视频生成提供了有价值的挑战与优势洞察，代码和数据将公开。

Abstract: Generating engaging, accurate short-form videos from scientific papers is
challenging due to content complexity and the gap between expert authors and
readers. Existing end-to-end methods often suffer from factual inaccuracies and
visual artifacts, limiting their utility for scientific dissemination. To
address these issues, we propose SciTalk, a novel multi-LLM agentic framework,
grounding videos in various sources, such as text, figures, visual styles, and
avatars. Inspired by content creators' workflows, SciTalk uses specialized
agents for content summarization, visual scene planning, and text and layout
editing, and incorporates an iterative feedback mechanism where video agents
simulate user roles to give feedback on generated videos from previous
iterations and refine generation prompts. Experimental evaluations show that
SciTalk outperforms simple prompting methods in generating scientifically
accurate and engaging content over the refined loop of video generation.
Although preliminary results are still not yet matching human creators'
quality, our framework provides valuable insights into the challenges and
benefits of feedback-driven video generation. Our code, data, and generated
videos will be publicly available.

</details>


### [9] [Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks](https://arxiv.org/abs/2504.18838)
*Yixin Cao,Shibo Hong,Xinze Li,Jiahao Ying,Yubo Ma,Haiyuan Liang,Yantao Liu,Zijun Yao,Xiaozhi Wang,Dan Huang,Wenxuan Zhang,Lifu Huang,Muhao Chen,Lei Hou,Qianru Sun,Xingjun Ma,Zuxuan Wu,Min-Yen Kan,David Lo,Qi Zhang,Heng Ji,Jing Jiang,Juanzi Li,Aixin Sun,Xuanjing Huang,Tat-Seng Chua,Yu-Gang Jiang*

Main category: cs.CL

TL;DR: 该论文探讨了大型语言模型（LLMs）评估的核心挑战，提出了从任务特定到能力基础的评估转变，以及从手动到自动评估的转变，并讨论了评估泛化问题的关键障碍。


<details>
  <summary>Details</summary>
Motivation: 论文动机在于应对LLMs快速发展所带来的评估挑战，重新组织评估标准以反映核心能力，并推动评估方法的自动化。

Method: 方法包括从任务特定到能力基础的评估转变，以及从手动到自动评估的过渡，特别关注动态数据集和"LLM即评委"的评分方法。

Result: 论文指出了评估泛化问题是目前的关键障碍，有限测试集无法随着模型能力增长而扩展，同时分析了方法和数据集等核心挑战。

Conclusion: 结论强调了评估方法的持续更新需求，并提出通过GitHub仓库众包更新和修正，邀请更多贡献者参与合作。

Abstract: Large Language Models (LLMs) are advancing at an amazing speed and have
become indispensable across academia, industry, and daily applications. To keep
pace with the status quo, this survey probes the core challenges that the rise
of LLMs poses for evaluation. We identify and analyze two pivotal transitions:
(i) from task-specific to capability-based evaluation, which reorganizes
benchmarks around core competencies such as knowledge, reasoning, instruction
following, multi-modal understanding, and safety; and (ii) from manual to
automated evaluation, encompassing dynamic dataset curation and
"LLM-as-a-judge" scoring.
  Yet, even with these transitions, a crucial obstacle persists: the evaluation
generalization issue. Bounded test sets cannot scale alongside models whose
abilities grow seemingly without limit. We will dissect this issue, along with
the core challenges of the above two transitions, from the perspectives of
methods, datasets, evaluators, and metrics. Due to the fast evolving of this
field, we will maintain a living GitHub repository (links are in each section)
to crowd-source updates and corrections, and warmly invite contributors and
collaborators.

</details>


### [10] [Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in Large Language Models with Self-Guided Reasoning](https://arxiv.org/abs/2504.18839)
*Abdellah Ghassel,Xianzhi Li,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 本篇论文提出了一种结合微调小模型与高级提示策略的方法，用于检测和缓解LLM驱动的对话系统中的对话崩溃问题，有效提升分类准确率并降低运营成本。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在对话任务中表现出色，但仍可能产生不连贯或矛盾的响应（即对话崩溃），损害用户信任。本研究旨在解决这一问题。

Method: 采用专业微调（8B小模型）与提示策略（few-shot学习、链式思考、类比提示）相结合的方法，并在BETOLD数据集上验证其泛化能力。

Result: 改进后的模型在英语和日语对话中表现稳健，分类准确率提升7%，并通过选择性升级到大模型显著降低运营成本和能耗。

Conclusion: 该方法为高影响领域的对话AI提供了高效、可解释且可靠的解决方案，缩小了小模型与专有大模型之间的性能差距。

Abstract: Large language models (LLMs) are rapidly changing various domains. However,
their capabilities in handling conversational breakdowns still require an
in-depth exploration. This paper addresses the challenge of detecting and
mitigating dialogue breakdowns within LLM-driven conversational systems. While
powerful models from OpenAI and Anthropic excel in many dialogue tasks, they
can still produce incoherent or contradictory responses, commonly referred to
as breakdowns, which undermine user trust. To tackle this, we propose an
approach that combines specialized fine-tuning with advanced prompting
strategies, including few-shot learning, chain-of-thought reasoning, and
analogical prompting. In particular, we fine-tune a small 8B model and
demonstrate its robust classification and calibration capabilities in English
and Japanese dialogue. We also validate its generalization on the BETOLD
dataset, achieving a 7\% accuracy improvement over its base model. Furthermore,
we introduce a real-time deployment architecture that selectively escalates
suspicious responses to more resource-intensive frontier models only when
breakdowns are detected, significantly cutting operational expenses and energy
consumption. Experimental results show our method surpasses prior
state-of-the-art specialized classifiers while also narrowing performance gaps
between smaller open-source models and large proprietary ones. Our approach
offers a scalable solution for robust conversational AI in high-impact domains
by combining efficiency, interpretability, and reliability.

</details>


### [11] [When2Call: When (not) to Call Tools](https://arxiv.org/abs/2504.18851)
*Hayley Ross,Ameya Sunil Mahabaleshwarkar,Yoshi Suhara*

Main category: cs.CL

TL;DR: 论文提出了When2Call基准，评估语言模型在工具调用决策上的表现，包括何时调用工具、何时提问或承认无法回答，并展示了现有模型的改进空间。


<details>
  <summary>Details</summary>
Motivation: 当前基准主要关注工具调用的准确性，而忽略了模型应在何时（不）调用工具的决策能力，因此需要一个新的评估标准。

Method: 开发了When2Call基准，通过多选择题形式评估工具调用决策，并设计了一种偏好优化训练方法。

Result: 现有的工具调用语言模型在When2Call上表现有显著改进空间，且偏好优化训练优于传统微调。

Conclusion: When2Call基准填补了工具调用决策评估的空白，为未来模型优化提供了方向。

Abstract: Leveraging external tools is a key feature for modern Language Models (LMs)
to expand their capabilities and integrate them into existing systems. However,
existing benchmarks primarily focus on the accuracy of tool calling -- whether
the correct tool is called with the correct parameters -- and less on
evaluating when LMs should (not) call tools. We develop a new benchmark,
When2Call, which evaluates tool-calling decision-making: when to generate a
tool call, when to ask follow-up questions and when to admit the question can't
be answered with the tools provided. We find that state-of-the-art tool-calling
LMs show significant room for improvement on When2Call, indicating the
importance of this benchmark. We also develop a training set for When2Call and
leverage the multiple-choice nature of the benchmark to develop a preference
optimization training regime, which shows considerably more improvement than
traditional fine-tuning. We release the benchmark and training data as well as
evaluation scripts at https://github.com/NVIDIA/When2Call.

</details>


### [12] [Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation](https://arxiv.org/abs/2504.18857)
*Yi Lu,Wanxu Zhao,Xin Zhou,Chenxin An,Chenglong Wang,Shuo Li,Yuming Yang,Jun Zhao,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 论文提出了一种无需训练的框架DPE，通过优化RoPE隐藏维度的位置嵌入，显著扩展LLMs的上下文窗口并提升性能，优于现有方法如YaRN和Self-Extend。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)在处理超出预训练长度的输入时表现不佳，且现有扩展上下文窗口的方法训练成本高昂。本文旨在通过一种无需训练的方法优化位置嵌入，低成本扩展上下文窗口。

Method: 提出了DPE框架，通过分析RoPE的隐藏维度，识别关键维度并调整其位置索引至最优长度，保留预训练模型的原始位置嵌入，仅对关键维度进行最小修改。

Result: DPE在Llama3-8B上实现了128k tokens的上下文窗口支持，且性能超越YaRN和Self-Extend；在Llama3.1 70B上，RULER基准性能提升18分，甚至优于GPT-4-128K。

Conclusion: DPE通过维度优化实现了高效的上下文扩展，无需额外训练，兼容Flash Attention 2，显著提升了模型的性能和长上下文处理能力。

Abstract: Large Language Models (LLMs) often struggle to process and generate coherent
context when the number of input tokens exceeds the pre-trained length. Recent
advancements in long-context extension have significantly expanded the context
window of LLMs but require expensive overhead to train the large-scale models
with longer context. In this work, we propose Dimension-Wise Positional
Embeddings Manipulation (DPE), a training-free framework to extrapolate the
context window of LLMs by diving into RoPE's different hidden dimensions.
Instead of manipulating all dimensions equally, DPE detects the effective
length for every dimension and finds the key dimensions for context extension.
We reuse the original position indices with their embeddings from the
pre-trained model and manipulate the key dimensions' position indices to their
most effective lengths. In this way, DPE adjusts the pre-trained models with
minimal modifications while ensuring that each dimension reaches its optimal
state for extrapolation. DPE significantly surpasses well-known baselines such
as YaRN and Self-Extend. DPE enables Llama3-8k 8B to support context windows of
128k tokens without continual training and integrates seamlessly with Flash
Attention 2. In addition to its impressive extrapolation capability, DPE also
dramatically improves the models' performance within training length, such as
Llama3.1 70B, by over 18 points on popular long-context benchmarks RULER. When
compared with commercial models, Llama 3.1 70B with DPE even achieves better
performance than GPT-4-128K.

</details>


### [13] [Latent Adversarial Training Improves the Representation of Refusal](https://arxiv.org/abs/2504.18872)
*Alexandra Abbas,Nora Petrova,Helios Ael Lyons,Natalia Perez-Campanero*

Main category: cs.CL

TL;DR: 本文探讨了语言模型拒绝行为在潜在空间中的编码方式，以及潜在对抗训练（LAT）如何通过噪声训练改变这种编码，从而影响模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解LAT如何通过噪声训练改变拒绝行为的潜在表示，以评估其效果和局限性，类似于传统监督安全微调（SSFT）中发现的线性拒绝方向的脆弱性。

Method: 通过分析Llama 2 7B模型，比较了LAT、SSFT和嵌入空间对抗训练（AT）在潜在空间中对拒绝行为的重组效果。使用激活差异和奇异值分解（SVD）方法，量化了拒绝表示的集中程度。

Result: 研究发现LAT显著改变了拒绝表示，将其集中在SVD的前两个成分中，解释了75%的激活差异方差。这种集中表示使LAT模型在面对攻击时表现出更强的鲁棒性，但也更容易受到自身生成向量的攻击。

Conclusion: 研究表明，LAT的训练扰动能够更全面地表示拒绝行为，但也揭示了其在提升模型安全性方面的潜在优势和弱点。

Abstract: Recent work has shown that language models' refusal behavior is primarily
encoded in a single direction in their latent space, making it vulnerable to
targeted attacks. Although Latent Adversarial Training (LAT) attempts to
improve robustness by introducing noise during training, a key question
remains: How does this noise-based training affect the underlying
representation of refusal behavior? Understanding this encoding is crucial for
evaluating LAT's effectiveness and limitations, just as the discovery of linear
refusal directions revealed vulnerabilities in traditional supervised safety
fine-tuning (SSFT).
  Through the analysis of Llama 2 7B, we examine how LAT reorganizes the
refusal behavior in the model's latent space compared to SSFT and embedding
space adversarial training (AT). By computing activation differences between
harmful and harmless instruction pairs and applying Singular Value
Decomposition (SVD), we find that LAT significantly alters the refusal
representation, concentrating it in the first two SVD components which explain
approximately 75 percent of the activation differences variance - significantly
higher than in reference models. This concentrated representation leads to more
effective and transferable refusal vectors for ablation attacks: LAT models
show improved robustness when attacked with vectors from reference models but
become more vulnerable to self-generated vectors compared to SSFT and AT. Our
findings suggest that LAT's training perturbations enable a more comprehensive
representation of refusal behavior, highlighting both its potential strengths
and vulnerabilities for improving model safety.

</details>


### [14] [A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification](https://arxiv.org/abs/2504.18884)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: LLMs在情感分析中通过集成多个中型模型的多次推理，比单一大型模型更稳健准确，RMSE降低18.6%。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了LLMs结果的变异性与可重复性问题，而人类标注通常采用多数投票解决分歧。因此，本研究探索集成策略以提升情感分析的鲁棒性。

Method: 提出了一种简单的集成策略，通过结合多个中型LLMs的多次推理结果来优化情感分析性能。

Result: 实验表明，集成方法比单一大型模型的单次尝试更准确，RMSE降低了18.6%。

Conclusion: 在情感分析任务中，集成中型LLMs的多次推理比依赖单一大型模型更有效，且能显著提升结果的稳健性和准确性。

Abstract: With the advance of large language models (LLMs), LLMs have been utilized for
the various tasks. However, the issues of variability and reproducibility of
results from each trial of LLMs have been largely overlooked in existing
literature while actual human annotation uses majority voting to resolve
disagreements among annotators. Therefore, this study introduces the
straightforward ensemble strategy to a sentiment analysis using LLMs. As the
results, we demonstrate that the ensemble of multiple inference using
medium-sized LLMs produces more robust and accurate results than using a large
model with a single attempt with reducing RMSE by 18.6%.

</details>


### [15] [MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction](https://arxiv.org/abs/2504.18938)
*Junhong Liang,Yu Zhou*

Main category: cs.CL

TL;DR: 该论文提出了MTCSC框架，扩展了传统中文拼写纠错（CSC）到可变长度纠错场景，通过RAG增强和长度反射机制解决领域适应和长度一致性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在中文拼写纠错中存在输出长度不一致和领域适应问题，传统CSC任务对输入输出长度一致性的硬性限制也限制了其适用性。

Method: 提出了基于RAG的MTCSC框架，构建领域特定检索数据库，并通过多源组合策略和迭代长度反射确保输出长度一致性。

Result: 实验表明，该方法在多样领域数据集上显著优于现有方法，尤其在领域特定和可变长度纠错任务中表现突出。

Conclusion: MTCSC框架有效解决了领域适应和长度一致性问题，提升了中文拼写纠错的性能。

Abstract: Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens
in sentences. While Large Language Models (LLMs) have shown remarkable success
in identifying and rectifying potential errors, they often struggle with
maintaining consistent output lengths and adapting to domain-specific
corrections. Furthermore, existing CSC task impose rigid constraints requiring
input and output lengths to be identical, limiting their applicability. In this
work, we extend traditional CSC to variable-length correction scenarios,
including Chinese Splitting Error Correction (CSEC) and ASR N-best Error
Correction. To address domain adaptation and length consistency, we propose
MTCSC (Multi-Turn CSC) framework based on RAG enhanced with a length reflection
mechanism. Our approach constructs a retrieval database from domain-specific
training data and dictionaries, fine-tuning retrievers to optimize performance
for error-containing inputs. Additionally, we introduce a multi-source
combination strategy with iterative length reflection to ensure output length
fidelity. Experiments across diverse domain datasets demonstrate that our
method significantly outperforms current approaches in correction quality,
particularly in handling domain-specific and variable-length error correction
tasks.

</details>


### [16] [LawFlow : Collecting and Simulating Lawyers' Thought Processes](https://arxiv.org/abs/2504.18942)
*Debarati Das,Khanh Chi Le,Ritik Sachin Parkar,Karin De Langis,Brendan Madson,Chad M. Berryman,Robin M. Willis,Daniel H. Moses,Brett McDonnell,Daniel Schwarcz,Dongyeop Kang*

Main category: cs.CL

TL;DR: LawFlow数据集旨在填补法律AI中端到端决策支持的空白，通过捕捉真实的动态法律工作流，比较人与LLM的推理差异，并提出设计建议以优化AI辅助角色。


<details>
  <summary>Details</summary>
Motivation: 当前法律AI的数据集和模型仅关注孤立子任务，无法支持实际法律实践中复杂、高风险的端到端决策。LawFlow旨在解决这一缺陷，捕捉真实的法律工作流。

Method: 通过收集法律学生在真实业务实体形成场景中的动态、模块化工作流，构建LawFlow数据集，并对比人类与LLM生成的工作流结构、灵活性和执行差异。

Result: 人类工作流更模块化和自适应，而LLM工作流更线性、详尽且对下游影响不敏感。法律专业人士更倾向于AI承担辅助角色（如头脑风暴、盲点识别）而非端到端执行。

Conclusion: 研究揭示了LLM在支持复杂法律工作流中的局限性，同时提出了基于混合规划、自适应执行的设计建议，以发展更具协作性的法律AI系统。

Abstract: Legal practitioners, particularly those early in their careers, face complex,
high-stakes tasks that require adaptive, context-sensitive reasoning. While AI
holds promise in supporting legal work, current datasets and models are
narrowly focused on isolated subtasks and fail to capture the end-to-end
decision-making required in real-world practice. To address this gap, we
introduce LawFlow, a dataset of complete end-to-end legal workflows collected
from trained law students, grounded in real-world business entity formation
scenarios. Unlike prior datasets focused on input-output pairs or linear chains
of thought, LawFlow captures dynamic, modular, and iterative reasoning
processes that reflect the ambiguity, revision, and client-adaptive strategies
of legal practice. Using LawFlow, we compare human and LLM-generated workflows,
revealing systematic differences in structure, reasoning flexibility, and plan
execution. Human workflows tend to be modular and adaptive, while LLM workflows
are more sequential, exhaustive, and less sensitive to downstream implications.
Our findings also suggest that legal professionals prefer AI to carry out
supportive roles, such as brainstorming, identifying blind spots, and surfacing
alternatives, rather than executing complex workflows end-to-end. Building on
these findings, we propose a set of design suggestions, rooted in empirical
observations, that align AI assistance with human goals of clarity,
completeness, creativity, and efficiency, through hybrid planning, adaptive
execution, and decision-point support. Our results highlight both the current
limitations of LLMs in supporting complex legal workflows and opportunities for
developing more collaborative, reasoning-aware legal AI systems. All data and
code are available on our project page
(https://minnesotanlp.github.io/LawFlow-website/).

</details>


### [17] [Dynamic Fisher-weighted Model Merging via Bayesian Optimization](https://arxiv.org/abs/2504.18992)
*Sanwoo Lee,Jiahao Liu,Qifan Wang,Jingang Wang,Xunliang Cai,Yunfang Wu*

Main category: cs.CL

TL;DR: 论文提出了一种动态Fisher加权合并（DF-Merge）框架，通过统一模型级和参数级的合并策略，结合贝叶斯优化动态调整参数，显著提高了多任务模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法（模型级或参数级）存在性能差距，希望通过统一框架提升多任务模型的效率和效果。

Method: 采用动态Fisher加权合并（DF-Merge），结合贝叶斯优化动态调整参数线性缩放系数，并基于Fisher信息条件整合参数重要性。

Result: DF-Merge在不同规模和任务上均优于基线方法，且验证数据量少时仍能快速接近最优性能。

Conclusion: DF-Merge通过统一合并视角和动态优化机制，为多任务模型合并提供了高效且高性能的解决方案。

Abstract: The fine-tuning of pre-trained language models has resulted in the widespread
availability of task-specific models. Model merging offers an efficient way to
create multi-task models by combining these fine-tuned models at the parameter
level, without the need for training data or joint training on multiple
datasets. Existing merging approaches typically involve scaling the parameters
model-wise or integrating parameter importance parameter-wise. Both approaches
exhibit their own weaknesses, leading to a notable performance gap compared to
multi-task fine-tuning. In this paper, we unify these seemingly distinct
strategies into a more general merging framework, and introduce Dynamic
Fisher-weighted Merging (DF-Merge). Specifically, candidate models are
associated with a set of coefficients that linearly scale their fine-tuned
parameters. Bayesian optimization is applied to dynamically adjust these
coefficients, aiming to maximize overall performance on validation sets. Each
iteration of this process integrates parameter importance based on the Fisher
information conditioned by the coefficients. Experimental results show that
DF-Merge outperforms strong baselines across models of different sizes and a
variety of tasks. Our analysis shows that the effectiveness of DF-Merge arises
from the unified view of merging and that near-optimal performance is
achievable in a few iterations, even with minimal validation data.

</details>


### [18] [Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs](https://arxiv.org/abs/2504.19019)
*Mohammad Akbar-Tajari,Mohammad Taher Pilehvar,Mohammad Mahmoody*

Main category: cs.CL

TL;DR: 提出了一种名为GoAT的方法，通过图结构生成对抗性提示来测试LLM的对齐鲁棒性，成功率高且无需访问目标模型参数。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）仍易受对抗性越狱攻击，识别其漏洞对提升模型鲁棒性至关重要。

Method: 采用图思维框架（Graph of Thoughts），通过动态图结构整合和改进思维路径，生成高效、人类可读的越狱提示。

Result: GoAT在对抗性强模型（如Llama）上实现了比现有攻击方法高五倍的越狱成功率，且查询次数更少。

Conclusion: GoAT通过图结构动态协作探索和优化对抗性漏洞，显著提升了对LLM对齐鲁棒性的测试能力。

Abstract: The challenge of ensuring Large Language Models (LLMs) align with societal
standards is of increasing interest, as these models are still prone to
adversarial jailbreaks that bypass their safety mechanisms. Identifying these
vulnerabilities is crucial for enhancing the robustness of LLMs against such
exploits. We propose Graph of ATtacks (GoAT), a method for generating
adversarial prompts to test the robustness of LLM alignment using the Graph of
Thoughts framework [Besta et al., 2024]. GoAT excels at generating highly
effective jailbreak prompts with fewer queries to the victim model than
state-of-the-art attacks, achieving up to five times better jailbreak success
rate against robust models like Llama. Notably, GoAT creates high-quality,
human-readable prompts without requiring access to the targeted model's
parameters, making it a black-box attack. Unlike approaches constrained by
tree-based reasoning, GoAT's reasoning is based on a more intricate graph
structure. By making simultaneous attack paths aware of each other's progress,
this dynamic framework allows a deeper integration and refinement of reasoning
paths, significantly enhancing the collaborative exploration of adversarial
vulnerabilities in LLMs. At a technical level, GoAT starts with a graph
structure and iteratively refines it by combining and improving thoughts,
enabling synergy between different thought paths. The code for our
implementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks.

</details>


### [19] [Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting](https://arxiv.org/abs/2504.19021)
*Zhyar Rzgar K Rostam,Gábor Kertész*

Main category: cs.CL

TL;DR: 论文探讨了预训练语言模型（如BERT、SciBERT等）在科学文本分类中的应用，通过数据集增强和精细调优显著提高了分类准确性，尤其指出领域特定模型优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 随着学术出版物数量的增加，高效的文本分类变得至关重要，研究旨在优化科学文本分类方法。

Method: 使用多种预训练语言模型在WoS-46985数据集上进行微调，通过数据集增强、硬投票策略和动态学习率优化模型性能。

Result: 领域特定模型（如SciBERT、BioBERT）表现优于通用模型，数据增强和精细调优显著提升了分类准确性。

Conclusion: 研究证实数据集增强、标签预测和精细调优是构建高效学术文本分类系统的关键策略。

Abstract: Efficient text classification is essential for handling the increasing volume
of academic publications. This study explores the use of pre-trained language
models (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on
the Web of Science (WoS-46985) dataset for scientific text classification. To
enhance performance, we augment the dataset by executing seven targeted queries
in the WoS database, retrieving 1,000 articles per category aligned with
WoS-46985's main classes. PLMs predict labels for this unlabeled data, and a
hard-voting strategy combines predictions for improved accuracy and confidence.
Fine-tuning on the expanded dataset with dynamic learning rates and early
stopping significantly boosts classification accuracy, especially in
specialized domains. Domain-specific models like SciBERT and BioBERT
consistently outperform general-purpose models such as BERT. These findings
underscore the efficacy of dataset augmentation, inference-driven label
prediction, hard-voting, and fine-tuning techniques in creating robust and
scalable solutions for automated academic text classification.

</details>


### [20] [KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation](https://arxiv.org/abs/2504.19024)
*Jiabin Fan,Guoqing Luo,Michael Bowling,Lili Mou*

Main category: cs.CL

TL;DR: 提出了名为KETCHUP的K步回报估计方法，用于文本生成任务中基于强化学习的知识蒸馏，通过Bellman最优方程多步估计降低梯度方差，提升优化效果。


<details>
  <summary>Details</summary>
Motivation: 针对基于强化学习的知识蒸馏在文本生成任务中的优化挑战，特别是学生模型较大时的梯度方差问题。

Method: 利用Bellman最优方程进行K步回报估计，理论分析表明该方法能减少梯度估计的方差。

Result: 在三个文本生成任务中，KETCHUP在标准指标和大语言模型评估中均表现优异。

Conclusion: K步回报估计为强化学习知识蒸馏在大语言模型研究中的改进提供了有前景的方向。

Abstract: We propose a novel k-step return estimation method (called KETCHUP) for
Reinforcement Learning(RL)-based knowledge distillation (KD) in text generation
tasks. Our idea is to induce a K-step return by using the Bellman Optimality
Equation for multiple steps. Theoretical analysis shows that this K-step
formulation reduces the variance of the gradient estimates, thus leading to
improved RL optimization especially when the student model size is large.
Empirical evaluation on three text generation tasks demonstrates that our
approach yields superior performance in both standard task metrics and large
language model (LLM)-based evaluation. These results suggest that our K-step
return induction offers a promising direction for enhancing RL-based KD in LLM
research.

</details>


### [21] [Calibrating Translation Decoding with Quality Estimation on LLMs](https://arxiv.org/abs/2504.19044)
*Di Wu,Yibin Lei,Christof Monz*

Main category: cs.CL

TL;DR: 本文提出通过优化假设似然与翻译质量的皮尔逊相关性来校准神经机器翻译的解码效果，显著提升了翻译质量，并可作为翻译质量估计的强代理。


<details>
  <summary>Details</summary>
Motivation: 现有最大后验概率（MAP）解码方法在真实翻译质量上表现不佳，需要一种更有效的解码校准方法。

Method: 通过直接优化假设似然与翻译质量的皮尔逊相关性来校准解码效果，并结合有限训练数据（每个方向2K实例）提升翻译质量。

Result: 在校准后，翻译质量在多种指标和人工评估中大幅提升，甚至超越了一些先进的翻译质量估计模型。

Conclusion: 校准方法显著提升了MAP解码的效率和翻译质量，为实际部署提供了更高效的解决方案。

Abstract: Neural machine translation (NMT) systems typically employ maximum a
posteriori (MAP) decoding to select the highest-scoring translation from the
distribution mass. However, recent evidence highlights the inadequacy of MAP
decoding, often resulting in low-quality or even pathological hypotheses -- the
decoding objective is not aligned with real-world translation quality. This
paper proposes calibrating hypothesis likelihoods with translation quality from
a distribution view by directly optimizing their Pearson correlation -- thereby
enhancing the effectiveness of translation decoding. With our method,
translation on large language models (LLMs) improves substantially after
limited training (2K instances per direction). This improvement is orthogonal
to those achieved through supervised fine-tuning, leading to substantial gains
across a broad range of metrics and human evaluations -- even when applied to
top-performing translation-specialized LLMs fine-tuned on high-quality
translation data, such as Tower, or when compared to recent preference
optimization methods, like CPO. Moreover, the calibrated translation likelihood
can directly serve as a strong proxy for translation quality, closely
approximating or even surpassing some state-of-the-art translation quality
estimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates
that calibration enhances the effectiveness of MAP decoding, thereby enabling
greater efficiency in real-world deployment. The resulting state-of-the-art
translation model, which covers 10 languages, along with the accompanying code
and human evaluation data, has been released to the community:
https://github.com/moore3930/calibrating-llm-mt.

</details>


### [22] [Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models](https://arxiv.org/abs/2504.19061)
*Anindya Bijoy Das,Shibbir Ahmed,Shahnewaz Karim Sakib*

Main category: cs.CL

TL;DR: 该论文研究了开源大语言模型（LLMs）在临床总结中的有效性，特别关注从出院报告中提取关键事件（如入院原因、院内重大事件和关键随访行动）的能力，并评估了这些模型生成的总结中幻觉的普遍性。


<details>
  <summary>Details</summary>
Motivation: 临床总结在医疗中至关重要，它能将复杂的医疗数据转化为易理解的信息。LLMs因其先进的语言理解能力，在自动化和提高临床总结准确性方面显示出巨大潜力。

Method: 通过全面的数值模拟，严格评估开源LLMs的性能，重点关注提取内容的准确性和可靠性，尤其是临床总结中的幻觉问题。

Result: 研究发现开源LLMs在临床总结中表现良好，但存在一定程度的幻觉现象，这可能影响信息的可靠性。

Conclusion: 论文强调了在临床总结中检测和减少幻觉的重要性，以确保信息的准确性，从而提升患者护理质量。

Abstract: Clinical summarization is crucial in healthcare as it distills complex
medical data into digestible information, enhancing patient understanding and
care management. Large language models (LLMs) have shown significant potential
in automating and improving the accuracy of such summarizations due to their
advanced natural language understanding capabilities. These models are
particularly applicable in the context of summarizing medical/clinical texts,
where precise and concise information transfer is essential. In this paper, we
investigate the effectiveness of open-source LLMs in extracting key events from
discharge reports, such as reasons for hospital admission, significant
in-hospital events, and critical follow-up actions. In addition, we also assess
the prevalence of various types of hallucinations in the summaries produced by
these models. Detecting hallucinations is vital as it directly influences the
reliability of the information, potentially affecting patient care and
treatment outcomes. We conduct comprehensive numerical simulations to
rigorously evaluate the performance of these models, further probing the
accuracy and fidelity of the extracted content in clinical summarization.

</details>


### [23] [ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics](https://arxiv.org/abs/2504.19066)
*Deeksha Varshney,Keane Ong,Rui Mao,Erik Cambria,Gianmarco Mengaldo*

Main category: cs.CL

TL;DR: 提出了一个名为EWRA的方法和ExtremeWeatherNews数据集，通过结合大型语言模型的结构化推理路径增强小型语言模型（SLMs），以改进极端天气事件分类、标记和情感分析能力。


<details>
  <summary>Details</summary>
Motivation: 解决极端天气事件研究中的数据稀缺问题，提升现有模型的实用性和决策支持能力。

Method: 结合大型语言模型的结构化推理路径（EWRA方法）和极端天气新闻数据集（ExtremeWeatherNews），训练小型语言模型完成极端天气相关任务。

Result: EWRA方法显著提升小型语言模型在极端天气分析任务中的性能，表现优于专用模型。

Conclusion: 通过EWRA和数据集ExtremeWeatherNews的组合框架ClimaEmpact，显著提升了极端天气分析的实用性和准确性。

Abstract: Accurate assessments of extreme weather events are vital for research and
policy, yet localized and granular data remain scarce in many parts of the
world. This data gap limits our ability to analyze potential outcomes and
implications of extreme weather events, hindering effective decision-making.
Large Language Models (LLMs) can process vast amounts of unstructured text
data, extract meaningful insights, and generate detailed assessments by
synthesizing information from multiple sources. Furthermore, LLMs can
seamlessly transfer their general language understanding to smaller models,
enabling these models to retain key knowledge while being fine-tuned for
specific tasks. In this paper, we propose Extreme Weather Reasoning-Aware
Alignment (EWRA), a method that enhances small language models (SLMs) by
incorporating structured reasoning paths derived from LLMs, and
ExtremeWeatherNews, a large dataset of extreme weather event-related news
articles. EWRA and ExtremeWeatherNews together form the overall framework,
ClimaEmpact, that focuses on addressing three critical extreme-weather tasks:
categorization of tangible vulnerabilities/impacts, topic labeling, and emotion
analysis. By aligning SLMs with advanced reasoning strategies on
ExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for
SLM alignment), EWRA improves the SLMs' ability to generate well-grounded and
domain-specific responses for extreme weather analytics. Our results show that
the approach proposed guides SLMs to output domain-aligned responses,
surpassing the performance of task-specific models and offering enhanced
real-world applicability for extreme weather analytics.

</details>


### [24] [Sample-Efficient Language Model for Hinglish Conversational AI](https://arxiv.org/abs/2504.19070)
*Sakshi Singh,Abhinav Prakash,Aakriti Shah,Chaitanya Sachdeva,Sanjana Dumpala*

Main category: cs.CL

TL;DR: 本文提出了一种高效的Hinglish对话模型开发方法，通过微调预训练多语言模型（如Gemma3-4B和Qwen2.5-7B）并结合合成数据与现有数据集，显著提升了Hinglish对话任务的性能。


<details>
  <summary>Details</summary>
Motivation: Hinglish（印地语与英语混合语言）由于拼写不一致、缺乏标准化和对话数据质量有限，带来独特计算挑战，因此亟需开发高效的对话模型。

Method: 评估并微调多种预训练多语言模型，结合合成对话与现有Hinglish数据集，解决数据稀缺问题。

Result: 实验表明，参数较少的模型通过高质量混合数据微调，可在Hinglish对话生成中实现性能与计算效率的平衡。

Conclusion: 该方法为低资源混合语言的高效建模提供了可行方案，同时强调了数据质量对模型性能的关键作用。

Abstract: This paper presents our process for developing a sample-efficient language
model for a conversational Hinglish chatbot. Hinglish, a code-mixed language
that combines Hindi and English, presents a unique computational challenge due
to inconsistent spelling, lack of standardization, and limited quality of
conversational data. This work evaluates multiple pre-trained cross-lingual
language models, including Gemma3-4B and Qwen2.5-7B, and employs fine-tuning
techniques to improve performance on Hinglish conversational tasks. The
proposed approach integrates synthetically generated dialogues with insights
from existing Hinglish datasets to address data scarcity. Experimental results
demonstrate that models with fewer parameters, when appropriately fine-tuned on
high-quality code-mixed data, can achieve competitive performance for Hinglish
conversation generation while maintaining computational efficiency.

</details>


### [25] [Efficient Reasoning for LLMs through Speculative Chain-of-Thought](https://arxiv.org/abs/2504.19095)
*Jikai Wang,Juntao Li,Lijun Wu,Min Zhang*

Main category: cs.CL

TL;DR: 论文提出了Speculative Chain-of-Thought (SCoT)方法，通过大小模型协作加速推理速度，降低延迟48%~66%，同时保持接近目标模型的性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理语言模型虽然能力强大，但模型规模和长思维链增加了推理成本和延迟。现有的方法主要关注减少参数或缩短思维链长度，SCoT从另一个角度通过协作加速推理。

Method: SCoT利用轻量级草稿模型进行思维级草拟，选择最佳思维草稿并用目标模型纠正错误。通过思维行为对齐提升草拟效率，草稿选择策略保持复杂问题的预测准确性。

Result: 在GSM8K等数据集上，SCoT为Deepseek-R1-Distill-Qwen-32B降低了48%~66%的推理延迟，同时实现接近目标模型的性能。

Conclusion: SCoT通过协作加速推理，显著降低了延迟并保持高性能，为高效推理提供了新思路。

Abstract: Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have
recently attracted widespread attention due to their impressive task-solving
abilities. However, the enormous model size and the generation of lengthy
thought chains introduce significant reasoning costs and response latency.
Existing methods for efficient reasoning mainly focus on reducing the number of
model parameters or shortening the chain-of-thought length. In this paper, we
introduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency
from another perspective by accelerated average reasoning speed through large
and small model collaboration. SCoT conducts thought-level drafting using a
lightweight draft model. Then it selects the best CoT draft and corrects the
error cases with the target model. The proposed thinking behavior alignment
improves the efficiency of drafting and the draft selection strategy maintains
the prediction accuracy for complex problems. Experimental results on GSM8K,
MATH, GaoKao, CollegeMath and Olympiad datasets show that SCoT reduces
reasoning latency by 48\%$\sim$66\% for Deepseek-R1-Distill-Qwen-32B while
achieving near-target-model-level performance. Our code is available at
https://github.com/Jikai0Wang/Speculative_CoT.

</details>


### [26] [Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation](https://arxiv.org/abs/2504.19101)
*Qianren Mao,Qili Zhang,Hanwen Hao,Zhentao Han,Runhua Xu,Weifeng Jiang,Qi Hu,Zhijun Chen,Tyler Zhou,Bo Li,Yangqiu Song,Jin Dong,Jianxin Li,Philip S. Yu*

Main category: cs.CL

TL;DR: 提出了一个名为FedE4RAG的新框架，利用联邦学习和知识蒸馏技术增强私有RAG系统性能，同时保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 私有RAG系统面临数据稀缺和隐私问题，需要一种平衡数据安全与可用性的解决方案。

Method: 提出FedE4RAG框架，整合联邦学习与知识蒸馏，采用同态加密保护模型参数，避免原始数据共享。

Result: 在真实数据集上验证了框架的有效性，显著提升了私有RAG系统的性能并确保数据隐私。

Conclusion: FedE4RAG成功解决了私有RAG系统的数据隐私与性能平衡问题，为隐私保护提供了一种有效方法。

Abstract: Retrieval-Augmented Generation (RAG) has recently emerged as a promising
solution for enhancing the accuracy and credibility of Large Language Models
(LLMs), particularly in Question & Answer tasks. This is achieved by
incorporating proprietary and private data from integrated databases. However,
private RAG systems face significant challenges due to the scarcity of private
domain data and critical data privacy issues. These obstacles impede the
deployment of private RAG systems, as developing privacy-preserving RAG systems
requires a delicate balance between data security and data availability. To
address these challenges, we regard federated learning (FL) as a highly
promising technology for privacy-preserving RAG services. We propose a novel
framework called Federated Retrieval-Augmented Generation (FedE4RAG). This
framework facilitates collaborative training of client-side RAG retrieval
models. The parameters of these models are aggregated and distributed on a
central-server, ensuring data privacy without direct sharing of raw data. In
FedE4RAG, knowledge distillation is employed for communication between the
server and client models. This technique improves the generalization of local
RAG retrievers during the federated learning process. Additionally, we apply
homomorphic encryption within federated learning to safeguard model parameters
and mitigate concerns related to data leakage. Extensive experiments conducted
on the real-world dataset have validated the effectiveness of FedE4RAG. The
results demonstrate that our proposed framework can markedly enhance the
performance of private RAG systems while maintaining robust data privacy
protection.

</details>


### [27] [APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries](https://arxiv.org/abs/2504.19110)
*Huajian Xin,Luming Li,Xiaoran Jin,Jacques Fleuriot,Wenda Li*

Main category: cs.CL

TL;DR: 介绍了自动化证明工程（APE）的新范式，旨在利用大语言模型（LLM）自动完成证明工程任务，并提出了首个真实场景的测评基准APE-Bench I，揭示了当前LLM在处理复杂证明工程时的不足。


<details>
  <summary>Details</summary>
Motivation: 现有定理证明的测评基准局限于静态任务，未能反映现实数学库中迭代和工程密集的工作流程，因此提出了自动化证明工程的范式，以推动该领域的研究。

Method: 提出了APE-Bench I基准，基于Mathlib4的实际提交历史，结合自然语言描述的多样化任务，采用Lean编译器和LLM作为评判的混合验证方法。并开发了优化的并行验证基础设施Eleanstic。

Result: 实验显示现有LLM在局部编辑上表现良好，但在复杂证明工程任务上表现显著下降。

Conclusion: 这项工作为证明工程中的代理工作流奠定了基础，未来将针对多文件协作、项目级验证和自主代理开发更先进的测评基准。

Abstract: Recent progress in large language models (LLMs) has shown promise in formal
theorem proving, yet existing benchmarks remain limited to isolated, static
proof tasks, failing to capture the iterative, engineering-intensive workflows
of real-world formal mathematics libraries. Motivated by analogous advances in
software engineering, we introduce the paradigm of Automated Proof Engineering
(APE), which aims to automate proof engineering tasks such as feature addition,
proof refactoring, and bug fixing using LLMs. To facilitate research in this
direction, we present APE-Bench I, the first realistic benchmark built from
real-world commit histories of Mathlib4, featuring diverse file-level tasks
described in natural language and verified via a hybrid approach combining the
Lean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable
parallel verification infrastructure optimized for proof checking across
multiple versions of Mathlib. Empirical results on state-of-the-art LLMs
demonstrate strong performance on localized edits but substantial degradation
on handling complex proof engineering. This work lays the foundation for
developing agentic workflows in proof engineering, with future benchmarks
targeting multi-file coordination, project-scale verification, and autonomous
agents capable of planning, editing, and repairing formal libraries.

</details>


### [28] [SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning](https://arxiv.org/abs/2504.19162)
*Jiaqi Chen,Bang Zhang,Ruotian Ma,Peisong Wang,Xiaodan Liang,Zhaopeng Tu,Xiaolong Li,Kwan-Yee K. Wong*

Main category: cs.CL

TL;DR: 论文提出了一种名为SPC的自博弈批评模型，用于评估LLM推理步骤的可靠性，无需人工标注，通过对抗自博弈提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 评估LLM推理步骤的可靠性需要高质量标注且成本高，论文希望通过自博弈方式自动优化模型。

Method: SPC通过自博弈训练两个模型，一个生成错误步骤，另一个检测错误，使用强化学习奖励机制迭代优化。

Result: 实验显示SPC在三个基准测试中逐步提升错误检测能力（如ProcessBench准确率从70.8%提升至77.7%），并显著提升LLM数学推理性能。

Conclusion: SPC无需人工标注即可有效提升LLM推理步骤评估能力，且在多项测试中超越基线模型，应用前景广阔。

Abstract: Evaluating the step-by-step reliability of large language model (LLM)
reasoning, such as Chain-of-Thought, remains challenging due to the difficulty
and cost of obtaining high-quality step-level supervision. In this paper, we
introduce Self-Play Critic (SPC), a novel approach where a critic model evolves
its ability to assess reasoning steps through adversarial self-play games,
eliminating the need for manual step-level annotation. SPC involves fine-tuning
two copies of a base model to play two roles, namely a "sneaky generator" that
deliberately produces erroneous steps designed to be difficult to detect, and a
"critic" that analyzes the correctness of reasoning steps. These two models
engage in an adversarial game in which the generator aims to fool the critic,
while the critic model seeks to identify the generator's errors. Using
reinforcement learning based on the game outcomes, the models iteratively
improve; the winner of each confrontation receives a positive reward and the
loser receives a negative reward, driving continuous self-evolution.
Experiments on three reasoning process benchmarks (ProcessBench, PRM800K,
DeltaBench) demonstrate that our SPC progressively enhances its error detection
capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and
surpasses strong baselines, including distilled R1 model. Furthermore, applying
SPC to guide the test-time search of diverse LLMs significantly improves their
mathematical reasoning performance on MATH500 and AIME2024, outperforming
state-of-the-art process reward models.

</details>


### [29] [WuNeng: Hybrid State with Attention](https://arxiv.org/abs/2504.19191)
*Liu Xiao,Li Zhiyuan,Lin Yueyu*

Main category: cs.CL

TL;DR: WuNeng架构通过整合RNN-based RWKV-7与注意力机制，提升了语言模型的表达力和上下文一致性，同时保持了高效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前大型语言模型在表达力和上下文连贯性上的不足，同时避免计算资源的过度消耗，WuNeng提出了创新的架构改进方法。

Method: WuNeng结合了RWKV-7驱动的多头注意力和跨头交互技术，通过拼接、调制和门控融合实现高效信息整合，并利用多令牌状态处理机制捕获序列依赖关系。

Result: WuNeng在极少量额外参数下显著提高了模型的表达力和复杂任务处理能力，尤其在推理和序列生成任务中表现优异。

Conclusion: WuNeng成功平衡了表达力与计算效率，为现代神经网络架构设定了新标准。

Abstract: The WuNeng architecture introduces a novel approach to enhancing the
expressivity and power of large language models by integrating recurrent neural
network (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing
heightened contextual coherence over reducing KV cache size. Building upon the
hybrid-head concept from Hymba, WuNeng augments standard multi-head attention
with additional RWKV-7 state-driven heads, rather than replacing existing
heads, to enrich the model's representational capacity. A cross-head
interaction technique fosters dynamic synergy among standard, state-driven, and
newly introduced middle heads, leveraging concatenation, additive modulation,
and gated fusion for robust information integration. Furthermore, a multi-token
state processing mechanism harnesses the continuous RWKV-7 state to capture
intricate, sequence-wide dependencies, significantly boosting expressivity.
Remarkably, these enhancements are achieved with minimal additional parameters,
ensuring efficiency while empowering the model to excel in complex reasoning
and sequence generation tasks. WuNeng sets a new standard for balancing
expressivity and computational efficiency in modern neural architectures.

</details>


### [30] [Dynamic Embedded Topic Models: properties and recommendations based on diverse corpora](https://arxiv.org/abs/2504.19209)
*Elisabeth Fittschen,Bella Xia,Leib Celnik,Paul Dilley,Tom Lippincott*

Main category: cs.CL

TL;DR: 研究了动态嵌入主题模型在不同历时语料库中的实现选择影响，提出了优化实用性和开发方向的建议。


<details>
  <summary>Details</summary>
Motivation: 目标是识别动态嵌入主题模型在应用中最重要的决策点，以优化其在学术研究中的实用性。

Method: 通过应用于五个不同的历时语料库，分析不同实现选择的效果。

Result: 发现词汇量的实际可扩展性和时间间隔的灵活建模对性能至关重要，而某些限制性因素对性能影响不大。

Conclusion: 提出了优化动态嵌入主题模型实用性的具体建议，并指出了一些不显著影响性能的因素，可为后续开发和网格搜索节省资源。

Abstract: We measure the effects of several implementation choices for the Dynamic
Embedded Topic Model, as applied to five distinct diachronic corpora, with the
goal of isolating important decisions for its use and further development. We
identify priorities that will maximize utility in applied scholarship,
including the practical scalability of vocabulary size to best exploit the
strengths of embedded representations, and more flexible modeling of intervals
to accommodate the uneven temporal distributions of historical writing. Of
similar importance, we find performance is not significantly or consistently
affected by several aspects that otherwise limit the model's application or
might consume the resources of a grid search.

</details>


### [31] [Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers](https://arxiv.org/abs/2504.19254)
*Dylan Bouchard,Mohit Singh Chauhan*

Main category: cs.CL

TL;DR: 该论文提出了一个零资源幻觉检测框架，通过调整多种不确定性量化技术并引入可调集成方法，显著提升了大型语言模型在真实应用中的准确性和可靠性。实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在高风险领域（如医疗和金融）的应用增多，幻觉（虚假信息）问题日益突出，亟需有效的检测方法以提升模型可靠性。

Method: 结合黑盒/白盒不确定性量化技术及LLM-as-a-Judge方法，将其转化为标准化置信分数（0-1），并提出可调集成策略以优化特定场景下的检测性能。配套工具包UQLM实现快速部署。

Result: 实验显示，可调集成方法在多个问答基准测试中优于单一技术及现有检测方法，验证了定制化策略的有效性。

Conclusion: 该框架通过灵活集成多种技术显著提升了幻觉检测性能，为实际应用提供了高可靠性的解决方案，配套工具包进一步降低了落地门槛。

Abstract: Hallucinations are a persistent problem with Large Language Models (LLMs). As
these models become increasingly used in high-stakes domains, such as
healthcare and finance, the need for effective hallucination detection is
crucial. To this end, we propose a versatile framework for zero-resource
hallucination detection that practitioners can apply to real-world use cases.
To achieve this, we adapt a variety of existing uncertainty quantification (UQ)
techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,
transforming them as necessary into standardized response-level confidence
scores ranging from 0 to 1. To enhance flexibility, we introduce a tunable
ensemble approach that incorporates any combination of the individual
confidence scores. This approach enables practitioners to optimize the ensemble
for a specific use case for improved performance. To streamline implementation,
the full suite of scorers is offered in this paper's companion Python toolkit,
UQLM. To evaluate the performance of the various scorers, we conduct an
extensive set of experiments using several LLM question-answering benchmarks.
We find that our tunable ensemble typically surpasses its individual components
and outperforms existing hallucination detection methods. Our results
demonstrate the benefits of customized hallucination detection strategies for
improving the accuracy and reliability of LLMs.

</details>


### [32] [VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?](https://arxiv.org/abs/2504.19267)
*Mohamed Gado,Towhid Taliee,Muhammad Memon,Dmitry Ignatov,Radu Timofte*

Main category: cs.CL

TL;DR: 该摘要提出了一种名为VIST-GPT的新方法，通过多模态模型生成图像序列的连贯叙述，并针对传统评估指标的不足引入了RoViST和GROOVIST这两种新评估指标。


<details>
  <summary>Details</summary>
Motivation: 针对视觉叙事任务中传统评估指标的不适用性，研究旨在开发一种能更准确反映叙述质量的方法，重点关注视觉基础、连贯性和非冗余性。

Method: 利用基于transformer的多模态模型和大规模视觉叙事（VIST）数据集，构建VIST-GPT模型生成叙述。

Result: 引入了RoViST和GROOVIST这两种新指标，它们能更好地评估叙述质量并与人类判断更一致。

Conclusion: VIST-GPT模型结合新评估指标，为视觉叙事任务提供了一个更有效的解决方案。

Abstract: Visual storytelling is an interdisciplinary field combining computer vision
and natural language processing to generate cohesive narratives from sequences
of images. This paper presents a novel approach that leverages recent
advancements in multimodal models, specifically adapting transformer-based
architectures and large multimodal models, for the visual storytelling task.
Leveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT
model produces visually grounded, contextually appropriate narratives. We
address the limitations of traditional evaluation metrics, such as BLEU,
METEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we
utilize RoViST and GROOVIST, novel reference-free metrics designed to assess
visual storytelling, focusing on visual grounding, coherence, and
non-redundancy. These metrics provide a more nuanced evaluation of narrative
quality, aligning closely with human judgment.

</details>


### [33] [AndroidGen: Building an Android Language Agent under Data Scarcity](https://arxiv.org/abs/2504.19298)
*Hanyu Lai,Junjie Gao,Xiao Liu,Yifan Xu,Shudan Zhang,Yuxiao Dong,Jie Tang*

Main category: cs.CL

TL;DR: 论文提出了AndroidGen框架，旨在解决大型语言模型（LLMs）在移动设备上作为代理时面临的数据稀缺问题，通过收集任务轨迹并训练开源LLMs，提升了模型能力，而无需人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在NLP任务中展现出巨大潜力，但由于高质量数据源的缺乏，它们尚未在真实移动设备上广泛应用。人工标注成本高且效率低，现有LLMs的完成率不足且缺乏有效数据过滤策略，因此需要开发一种新框架以增强LLM在数据稀缺环境中的能力。

Method: 论文提出了AndroidGen框架，通过收集人类任务生成的轨迹数据，并基于这些数据训练开源LLMs，从而开发无需人工标注的开源移动代理。

Result: 通过AndroidWorld、AitW及多款流行应用的广泛评估，AndroidGen表现出了显著的能力提升，并揭示了未来改进的潜在方向。相关代码、模型和数据已开源。

Conclusion: AndroidGen框架有效解决了LLMs在移动设备上作为代理时的数据稀缺问题，通过无标注轨迹数据训练模型，展示了其在实际应用中的潜力，同时也指出了未来进一步优化的方向。

Abstract: Large language models have opened up a world of possibilities for various NLP
tasks, sparking optimism for the future. Despite their potential, LLMs have yet
to be widely used as agents on real mobile devices. The main challenge is the
need for high-quality data sources. Time constraints and labor intensity often
hinder human annotation. On the other hand, existing LLMs exhibit inadequate
completion rates and need a robust data filtration strategy. Given these
challenges, we develop a framework called AndroidGen to enhance the
capabilities of LLM-based agents under data scarcity. In addition, we leverage
AndroidGen to collect trajectories given human tasks and train open-source LLMs
on these trajectories to develop an open-source mobile agent without manually
labeled trajectories. We extensively evaluate AndroidGen with AndroidWorld,
AitW, and various popular applications, demonstrating its improvements and
revealing potential areas for future improvement. Code, model, and data are
available at https://github.com/THUDM/AndroidGen.

</details>


### [34] [BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese](https://arxiv.org/abs/2504.19314)
*Peilin Zhou,Bruce Leon,Xiang Ying,Can Zhang,Yifan Shao,Qichen Ye,Dading Chong,Zhiling Jin,Chenxuan Xie,Meng Cao,Yuxin Gu,Sixin Hong,Jing Ren,Jian Chen,Chao Liu,Yining Hua*

Main category: cs.CL

TL;DR: BrowseComp-ZH是一个用于评估大语言模型在中国网络中实时浏览能力的高难度基准测试，包含289个多跳问题，涵盖11个领域，结果显示大多数模型表现不佳，最高准确率仅为42.9%。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试如BrowseComp主要关注英语，忽略了中文网络的语言、基础设施和审查复杂性，因此需要专门的中文测试基准。

Method: 通过反向工程从简短、客观且易验证的答案中构造问题，采用两阶段质量控制确保问题难度和答案唯一性。

Result: 测试了20多个先进语言模型和代理搜索系统，大多数准确率低于10%，最佳模型DeepResearch仅达42.9%。

Conclusion: BrowseComp-ZH展示了当前模型在复杂检索和信息协调能力上的不足，需进一步改进。

Abstract: As large language models (LLMs) evolve into tool-using agents, the ability to
browse the web in real-time has become a critical yardstick for measuring their
reasoning and retrieval competence. Existing benchmarks such as BrowseComp
concentrate on English and overlook the linguistic, infrastructural, and
censorship-related complexities of other major information ecosystems -- most
notably Chinese. To address this gap, we introduce BrowseComp-ZH, a
high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents
on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning
11 diverse domains. Each question is reverse-engineered from a short,
objective, and easily verifiable answer (e.g., a date, number, or proper noun).
A two-stage quality control protocol is applied to strive for high question
difficulty and answer uniqueness. We benchmark over 20 state-of-the-art
language models and agentic search systems on our proposed BrowseComp-ZH.
Despite their strong conversational and retrieval capabilities, most models
struggle severely: a large number achieve accuracy rates below 10%, and only a
handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,
reaches just 42.9%. These results demonstrate the considerable difficulty of
BrowseComp-ZH, where success demands not only effective retrieval strategies,
but also sophisticated reasoning and information reconciliation -- capabilities
that current models still struggle to master. Our dataset, construction
guidelines, and benchmark results have been publicly released at
https://github.com/PALIN2018/BrowseComp-ZH.

</details>


### [35] [Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing](https://arxiv.org/abs/2504.19333)
*James O' Neill,Santhosh Subramanian,Eric Lin,Vaikkunth Mugunthan*

Main category: cs.CL

TL;DR: 论文提出了一种高效的方法来替代大型语言模型（LLMs）进行内容审查，通过任务特定数据生成和模型合并技术，显著提升了性能并减少了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）虽然在内容审查方面表现出色，但其高延迟、内存消耗和成本问题限制了实际应用。

Method: 采用了任务特定数据生成来微调分类器，并使用名为MultiTaskGuard的模型进行预训练，最终通过搜索式模型合并技术开发出性能最佳模型UniGuard。

Result: 在7个公共数据集和4个自制基准测试中，UniGuard的平均F1分数比Aegis-LlamaGuard高29.92分，比gpt-4o高21.62分。

Conclusion: 通过任务特定数据生成和模型合并技术，可以开发出高效、高性能的内容审查模型，显著优于现有的LLMs和第三方API。

Abstract: The trend towards large language models (LLMs) for guardrailing against
undesired behaviors is increasing and has shown promise for censoring user
inputs. However, increased latency, memory consumption, hosting expenses and
non-structured outputs can make their use prohibitive.
  In this work, we show that task-specific data generation can lead to
fine-tuned classifiers that significantly outperform current state of the art
(SoTA) while being orders of magnitude smaller. Secondly, we show that using a
single model, \texttt{MultiTaskGuard}, that is pretrained on a large
synthetically generated dataset with unique task instructions further improves
generalization. Thirdly, our most performant models, \texttt{UniGuard}, are
found using our proposed search-based model merging approach that finds an
optimal set of parameters to combine single-policy models and multi-policy
guardrail models. % On 7 public datasets and 4 guardrail benchmarks we created,
our efficient guardrail classifiers improve over the best performing SoTA
publicly available LLMs and 3$^{\text{rd}}$ party guardrail APIs in detecting
unsafe and safe behaviors by an average F1 score improvement of \textbf{29.92}
points over Aegis-LlamaGuard and \textbf{21.62} over \texttt{gpt-4o},
respectively. Lastly, our guardrail synthetic data generation process that uses
custom task-specific guardrail poli

</details>


### [36] [Explanatory Summarization with Discourse-Driven Planning](https://arxiv.org/abs/2504.19339)
*Dongqi Liu,Xi Yu,Vera Demberg,Mirella Lapata*

Main category: cs.CL

TL;DR: 该论文提出了一种基于规划的自动摘要方法，通过话语框架指导解释性内容的生成，比现有方法在摘要质量、稳健性和可控性上表现更好。


<details>
  <summary>Details</summary>
Motivation: 当前的自动摘要方法未明确建模解释性内容，导致与人工摘要中解释比例不一致。本文旨在通过话语框架引导摘要生成以解决这一问题。

Method: 提出了两种话语驱动的规划策略：计划作为输入条件或输出前缀。利用话语框架组织摘要生成，并通过提示响应引导解释性句子。

Result: 在三个外行摘要数据集上的实验表明，该方法在摘要质量上优于现有最优方法，并提高了模型的稳健性、可控性，减少了幻觉现象。

Conclusion: 基于话语框架的规划方法能有效改进自动摘要的解释性内容生成和质量，具有实践价值。

Abstract: Lay summaries for scientific documents typically include explanations to help
readers grasp sophisticated concepts or arguments. However, current automatic
summarization methods do not explicitly model explanations, which makes it
difficult to align the proportion of explanatory content with human-written
summaries. In this paper, we present a plan-based approach that leverages
discourse frameworks to organize summary generation and guide explanatory
sentences by prompting responses to the plan. Specifically, we propose two
discourse-driven planning strategies, where the plan is conditioned as part of
the input or part of the output prefix, respectively. Empirical experiments on
three lay summarization datasets show that our approach outperforms existing
state-of-the-art methods in terms of summary quality, and it enhances model
robustness, controllability, and mitigates hallucination.

</details>


### [37] [ICL CIPHERS: Quantifying "Learning'' in In-Context Learning via Substitution Ciphers](https://arxiv.org/abs/2504.19395)
*Zhouxiang Fang,Aayush Mishra,Muhan Gao,Anqi Liu,Daniel Khashabi*

Main category: cs.CL

TL;DR: 论文探讨了上下文学习（ICL）的两种模式：任务检索和任务学习，提出了基于替换密码的任务重构方法ICL CIPHERS，通过可逆映射验证模型的学习能力。


<details>
  <summary>Details</summary>
Motivation: 研究目标是通过任务重构方法ICL CIPHERS，分离并量化上下文学习中任务检索和任务学习两种模式的贡献，验证大语言模型（LLM）是否具备解密能力。

Method: 引入基于替换密码的任务重构方法ICL CIPHERS，设计可逆（双射）和不可逆（非双射）两种映射，通过模型在解码任务上的表现差异验证学习能力。

Result: 结果表明，LLM在可逆映射任务上的表现优于不可逆基线，尽管差距较小，但在四个数据集和六个模型中表现一致。此外，模型内部表征显示了解密能力。

Conclusion: ICL CIPHERS为量化上下文学习中的“学习”行为提供了新方法，验证了LLM具备对加密输入的解密能力。

Abstract: Recent works have suggested that In-Context Learning (ICL) operates in dual
modes, i.e. task retrieval (remember learned patterns from pre-training) and
task learning (inference-time ``learning'' from demonstrations). However,
disentangling these the two modes remains a challenging goal. We introduce ICL
CIPHERS, a class of task reformulations based on substitution ciphers borrowed
from classic cryptography. In this approach, a subset of tokens in the
in-context inputs are substituted with other (irrelevant) tokens, rendering
English sentences less comprehensible to human eye. However, by design, there
is a latent, fixed pattern to this substitution, making it reversible. This
bijective (reversible) cipher ensures that the task remains a well-defined task
in some abstract sense, despite the transformations. It is a curious question
if LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires
deciphering the latent cipher. We show that LLMs are better at solving ICL
CIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline,
providing a novel approach to quantify ``learning'' in ICL. While this gap is
small, it is consistent across the board on four datasets and six models.
Finally, we examine LLMs' internal representations and identify evidence in
their ability to decode the ciphered inputs.

</details>


### [38] [Context Selection and Rewriting for Video-based EducationalQuestion Generation](https://arxiv.org/abs/2504.19406)
*Mengxia Yu,Bang Nguyen,Olivia Zino,Meng Jiang*

Main category: cs.CL

TL;DR: 这篇论文介绍了如何通过结合讲座转录和视频关键帧来提升教育视频问题生成的质量，提出了一种新颖的动态上下文选择和重写框架。


<details>
  <summary>Details</summary>
Motivation: 现有教育问题生成系统通常依赖预定义的文本，无法反映真实课堂内容（如带幻灯片的讲座语音）。为此，研究团队收集了真实课堂讲座的问题数据集，并发现当前方法在生成与时间戳和目标答案对齐的问题时表现不佳。

Method: 论文提出了一种新框架，利用大语言模型动态选择和重写上下文。首先根据答案相关性和时间临近性从讲座转录和视频关键帧中筛选上下文，然后将多模态上下文整合并重写为包含答案的知识陈述，以增强逻辑联系。

Result: 该方法显著提高了生成问题的质量和相关性。

Conclusion: 通过动态上下文选择和重写，该框架有效解决了教育视频问题生成中的挑战，为智能教育系统提供了更实用的解决方案。数据集和代码已开源。

Abstract: Educational question generation (EQG) is a crucial component of intelligent
educational systems, significantly aiding self-assessment, active learning, and
personalized education. While EQG systems have emerged, existing datasets
typically rely on predefined, carefully edited texts, failing to represent
real-world classroom content, including lecture speech with a set of
complementary slides. To bridge this gap, we collect a dataset of educational
questions based on lectures from real-world classrooms. On this realistic
dataset, we find that current methods for EQG struggle with accurately
generating questions from educational videos, particularly in aligning with
specific timestamps and target answers. Common challenges include selecting
informative contexts from extensive transcripts and ensuring generated
questions meaningfully incorporate the target answer. To address the
challenges, we introduce a novel framework utilizing large language models for
dynamically selecting and rewriting contexts based on target timestamps and
answers. First, our framework selects contexts from both lecture transcripts
and video keyframes based on answer relevance and temporal proximity. Then, we
integrate the contexts selected from both modalities and rewrite them into
answer-containing knowledge statements, to enhance the logical connection
between the contexts and the desired answer. This approach significantly
improves the quality and relevance of the generated questions. Our dataset and
code are released in https://github.com/mengxiayu/COSER.

</details>


### [39] [Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory](https://arxiv.org/abs/2504.19413)
*Prateek Chhikara,Dev Khant,Saket Aryan,Taranjeet Singh,Deshraj Yadav*

Main category: cs.CL

TL;DR: 本文介绍了Mem0，一种可扩展的记忆中心架构，旨在解决大语言模型（LLMs）在多会话对话中因固定上下文窗口导致的连贯性问题。Mem0通过动态提取、整合和检索对话中的关键信息，并结合图记忆表示，显著提升了对话一致性，并在多项基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLMs在长对话中因固定上下文窗口的限制难以保持一致性，本文旨在设计一种高效记忆机制以提升长期对话的连贯性。

Method: 提出Mem0架构及增强版本（基于图记忆表示），动态管理对话信息，并与六类基线方法在LOCOMO基准上进行对比。

Result: Mem0在LLM-as-a-Judge指标上相对OpenAI提升26%，图记忆版本进一步提高2%。此外，Mem0降低91%延迟并减少90%以上token开销。

Conclusion: 结构化持久记忆机制对长期对话连贯性至关重要，Mem0在推理能力与部署成本间取得了优异平衡，为LLM驱动的AI代理提供了可靠方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable prowess in
generating contextually coherent responses, yet their fixed context windows
pose fundamental challenges for maintaining consistency over prolonged
multi-session dialogues. We introduce Mem0, a scalable memory-centric
architecture that addresses this issue by dynamically extracting,
consolidating, and retrieving salient information from ongoing conversations.
Building on this foundation, we further propose an enhanced variant that
leverages graph-based memory representations to capture complex relational
structures among conversational elements. Through comprehensive evaluations on
LOCOMO benchmark, we systematically compare our approaches against six baseline
categories: (i) established memory-augmented systems, (ii) retrieval-augmented
generation (RAG) with varying chunk sizes and k-values, (iii) a full-context
approach that processes the entire conversation history, (iv) an open-source
memory solution, (v) a proprietary model system, and (vi) a dedicated memory
management platform. Empirical results show that our methods consistently
outperform all existing memory systems across four question categories:
single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26%
relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with
graph memory achieves around 2% higher overall score than the base
configuration. Beyond accuracy gains, we also markedly reduce computational
overhead compared to full-context method. In particular, Mem0 attains a 91%
lower p95 latency and saves more than 90% token cost, offering a compelling
balance between advanced reasoning capabilities and practical deployment
constraints. Our findings highlight critical role of structured, persistent
memory mechanisms for long-term conversational coherence, paving the way for
more reliable and efficient LLM-driven AI agents.

</details>


### [40] [Context-Guided Dynamic Retrieval for Improving Generation Quality in RAG Models](https://arxiv.org/abs/2504.19436)
*Jacky He,Guiran Liu,Binrong Zhu,Hanlu Zhang,Hongye Zheng,Xiaokai Wang*

Main category: cs.CL

TL;DR: 该论文提出了一个动态知识检索机制，通过多级感知检索向量构建和可微分文档匹配路径，优化了RAG架构在语义理解和知识调度上的效率，实验验证了其在多任务中的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 静态RAG架构在上下文适应和知识访问上存在局限，因此需要动态优化的机制来增强语义理解和知识调度效率。

Method: 引入多级感知检索向量构建策略和可微分文档匹配路径，实现检索与生成模块的端到端联合训练和协同优化。

Result: 在Natural Questions数据集上的实验表明，BLEU和ROUGE-L分数显著提升，且在语义模糊和多文档融合任务中表现出更强的鲁棒性和生成一致性。

Conclusion: 该方法在高质量语言生成系统中具有广泛应用潜力和实用价值。

Abstract: This paper focuses on the dynamic optimization of the Retrieval-Augmented
Generation (RAG) architecture. It proposes a state-aware dynamic knowledge
retrieval mechanism to enhance semantic understanding and knowledge scheduling
efficiency in large language models for open-domain question answering and
complex generation tasks. The method introduces a multi-level perceptive
retrieval vector construction strategy and a differentiable document matching
path. These components enable end-to-end joint training and collaborative
optimization of the retrieval and generation modules. This effectively
addresses the limitations of static RAG structures in context adaptation and
knowledge access. Experiments are conducted on the Natural Questions dataset.
The proposed structure is thoroughly evaluated across different large models,
including GPT-4, GPT-4o, and DeepSeek. Comparative and ablation experiments
from multiple perspectives confirm the significant improvements in BLEU and
ROUGE-L scores. The approach also demonstrates stronger robustness and
generation consistency in tasks involving semantic ambiguity and multi-document
fusion. These results highlight its broad application potential and practical
value in building high-quality language generation systems.

</details>


### [41] [Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks](https://arxiv.org/abs/2504.19445)
*Yi-Long Lu,Chunhui Zhang,Wei Wang*

Main category: cs.CL

TL;DR: 研究表明，大语言模型（LLMs）在二元和连续响应格式下存在系统性判断偏差，尤其在二元格式中更容易产生负面判断。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在不同响应格式（二元与连续）下判断的系统性偏差，以提高其在决策任务中的可靠性。

Method: 通过价值判断任务和文本情感分析任务，测试多种开源和商业LLM模型在两种响应格式下的表现。

Result: LLMs在二元格式中更倾向于负面判断，且这种模式在两种任务中均保持一致。

Conclusion: 任务设计中的微小变化（如响应格式）可能引入系统性偏差，需谨慎应用LLMs于决策任务。

Abstract: Large Language Models (LLMs) are increasingly used in tasks such as
psychological text analysis and decision-making in automated workflows.
However, their reliability remains a concern due to potential biases inherited
from their training process. In this study, we examine how different response
format: binary versus continuous, may systematically influence LLMs' judgments.
In a value statement judgments task and a text sentiment analysis task, we
prompted LLMs to simulate human responses and tested both formats across
several models, including both open-source and commercial models. Our findings
revealed a consistent negative bias: LLMs were more likely to deliver
"negative" judgments in binary formats compared to continuous ones. Control
experiments further revealed that this pattern holds across both tasks. Our
results highlight the importance of considering response format when applying
LLMs to decision tasks, as small changes in task design can introduce
systematic biases.

</details>


### [42] [Towards Long Context Hallucination Detection](https://arxiv.org/abs/2504.19457)
*Siyi Liu,Kishaloy Halder,Zheng Qi,Wei Xiao,Nikolaos Pappas,Phu Mon Htut,Neha Anna John,Yassine Benajiba,Dan Roth*

Main category: cs.CL

TL;DR: 本文提出了一个针对长上下文输入中大型语言模型（LLM）上下文幻觉问题的数据集和新型架构，通过分解和聚合机制显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多种任务中表现优异，但在长上下文输入中容易产生不切实际或自相矛盾的幻觉信息，这一问题尚未得到充分解决。

Method: 构建了一个专门用于长上下文幻觉检测的数据集，并提出了一种新型架构，使预训练的编码器模型（如BERT）能够通过分解和聚合机制处理长上下文并检测幻觉。

Result: 实验结果表明，该架构在多个指标上显著优于类似规模的传统模型和基于LLM的模型，且推理速度大幅提升。

Conclusion: 本研究为解决长上下文输入中的LLM幻觉问题提供了有效的数据集和方法，并在性能和效率上取得了显著改进。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various tasks. However, they are prone to contextual hallucination, generating
information that is either unsubstantiated or contradictory to the given
context. Although many studies have investigated contextual hallucinations in
LLMs, addressing them in long-context inputs remains an open problem. In this
work, we take an initial step toward solving this problem by constructing a
dataset specifically designed for long-context hallucination detection.
Furthermore, we propose a novel architecture that enables pre-trained encoder
models, such as BERT, to process long contexts and effectively detect
contextual hallucinations through a decomposition and aggregation mechanism.
Our experimental results show that the proposed architecture significantly
outperforms previous models of similar size as well as LLM-based models across
various metrics, while providing substantially faster inference.

</details>


### [43] [BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text](https://arxiv.org/abs/2504.19467)
*Jiageng Wu,Bowen Gu,Ren Zhou,Kevin Xie,Doug Snyder,Yixing Jiang,Valentina Carducci,Richard Wyss,Rishi J Desai,Emily Alsentzer,Leo Anthony Celi,Adam Rodman,Sebastian Schneeweiss,Jonathan H. Chen,Santiago Romero-Brufau,Kueiyu Joshua Lin,Jie Yang*

Main category: cs.CL

TL;DR: BRIDGE是一个多语言临床基准测试，覆盖87个任务和9种语言，用于评估52种先进LLM在真实临床数据上的表现，结果显示开源模型可与专有模型媲美。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在临床背景下的评估受限，现有基准未能反映真实电子健康记录（EHR）的复杂性或通用临床场景。BRIDGE旨在填补这一空白。

Method: 提出BRIDGE基准，包含87个任务，源自真实临床数据，多语言覆盖。系统性评估52种LLM（如DeepSeek-R1、GPT-4o等）在不同推理策略下的表现。

Result: 13,572次实验显示性能因模型规模、语言、任务及临床领域而异。开源LLM表现与专有模型相当，而基于旧架构的医学微调模型常逊于新版通用模型。

Conclusion: BRIDGE为临床文本理解的LLM开发与评估提供了基础资源和参考，证明开源模型的潜力及通用模型的优势。

Abstract: Large language models (LLMs) hold great promise for medical applications and
are evolving rapidly, with new models being released at an accelerated pace.
However, current evaluations of LLMs in clinical contexts remain limited. Most
existing benchmarks rely on medical exam-style questions or PubMed-derived
text, failing to capture the complexity of real-world electronic health record
(EHR) data. Others focus narrowly on specific application scenarios, limiting
their generalizability across broader clinical use. To address this gap, we
present BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks
sourced from real-world clinical data sources across nine languages. We
systematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1,
GPT-4o, Gemini, and Llama 4) under various inference strategies. With a total
of 13,572 experiments, our results reveal substantial performance variation
across model sizes, languages, natural language processing tasks, and clinical
specialties. Notably, we demonstrate that open-source LLMs can achieve
performance comparable to proprietary models, while medically fine-tuned LLMs
based on older architectures often underperform versus updated general-purpose
models. The BRIDGE and its corresponding leaderboard serve as a foundational
resource and a unique reference for the development and evaluation of new LLMs
in real-world clinical text understanding.

</details>


### [44] [Conflicts in Texts: Data, Implications and Challenges](https://arxiv.org/abs/2504.19472)
*Siyi Liu,Dan Roth*

Main category: cs.CL

TL;DR: 该论文探讨了NLP模型中冲突信息的来源与影响，将其分为三类（自然文本、人工标注数据、模型交互），并提出了缓解策略和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: NLP模型在现实应用中的可靠性受到冲突信息的威胁，研究旨在系统分类并解决这些冲突。

Method: 通过调查和分类，分析冲突信息的三种主要来源及其影响。

Result: 归纳了冲突的类别（如事实不一致、标注分歧、幻觉等），并讨论了现有缓解策略的局限性。

Conclusion: 构建冲突感知的NLP系统需进一步研究，以更有效地处理冲突信息。

Abstract: As NLP models become increasingly integrated into real-world applications, it
becomes clear that there is a need to address the fact that models often rely
on and generate conflicting information. Conflicts could reflect the complexity
of situations, changes that need to be explained and dealt with, difficulties
in data annotation, and mistakes in generated outputs. In all cases,
disregarding the conflicts in data could result in undesired behaviors of
models and undermine NLP models' reliability and trustworthiness. This survey
categorizes these conflicts into three key areas: (1) natural texts on the web,
where factual inconsistencies, subjective biases, and multiple perspectives
introduce contradictions; (2) human-annotated data, where annotator
disagreements, mistakes, and societal biases impact model training; and (3)
model interactions, where hallucinations and knowledge conflicts emerge during
deployment. While prior work has addressed some of these conflicts in
isolation, we unify them under the broader concept of conflicting information,
analyze their implications, and discuss mitigation strategies. We highlight key
challenges and future directions for developing conflict-aware NLP systems that
can reason over and reconcile conflicting information more effectively.

</details>


### [45] [Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment](https://arxiv.org/abs/2504.19556)
*Kristen Sussman,Daniel Carter*

Main category: cs.CL

TL;DR: 长篇总结：研究通过比较2020年（ChatGPT前）和2024年关于特朗普的推文，分析了AI对社交媒体语言的影响。结果显示情感极性和积极表达显著增加，AI对语言模式的影响明显。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（如ChatGPT）对社交媒体语言模式的影响，特别是情感表达和文本复杂度的变化。

Method: 使用Flesch-Kincaid可读性评分和情感极性评分，分析2020年和2024年推文数据集的变化。

Result: 2024年推文的情感极性显著增加（0.12 vs. 0.04），中性内容减少（54.8%到39.8%），积极表达增加（28.6%到45.9%）。

Conclusion: AI在社交媒体中的存在增加，显著影响语言模式和情感表达。

Abstract: Given the subtle human-like effects of large language models on linguistic
patterns, this study examines shifts in language over time to detect the impact
of AI-mediated communication (AI- MC) on social media. We compare a replicated
dataset of 970,919 tweets from 2020 (pre-ChatGPT) with 20,000 tweets from the
same period in 2024, all of which mention Donald Trump during election periods.
Using a combination of Flesch-Kincaid readability and polarity scores, we
analyze changes in text complexity and sentiment. Our findings reveal a
significant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift
from predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more
positive expressions (28.6% to 45.9%). These findings suggest not only an
increasing presence of AI in social media communication but also its impact on
language and emotional expression patterns.

</details>


### [46] [m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training](https://arxiv.org/abs/2504.19565)
*Meng Xiao,Xunxin Cai,Chengrui Wang,Yuanchun Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种基于知识的多Agent框架，用于生物医学领域的大语言模型（LLM）训练数据蒸馏，通过多Agent协作自动生成高质量问答对，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有开源生物医学标注数据集数量和质量不足，无法满足LLM训练需求，且生物医学知识层次复杂，需要更高效的数据处理方法。

Method: 采用基于MeSH层次结构的协作多Agent架构，各Agent自主提取、合成和评估高质量文本数据，生成领域特定的问答对。

Result: 实验显示，用该方法蒸馏的数据训练的LLM在生物医学问答任务中表现优异，甚至超越GPT-4等大型专有模型。

Conclusion: 多Agent协作框架能有效提升生物医学LLM训练数据的质量，具有广泛应用潜力。

Abstract: The rapid progress of large language models (LLMs) in biomedical research has
underscored the limitations of existing open-source annotated scientific
corpora, which are often insufficient in quantity and quality. Addressing the
challenge posed by the complex hierarchy of biomedical knowledge, we propose a
knowledge-driven, multi-agent framework for scientific corpus distillation
tailored for LLM training in the biomedical domain. Central to our approach is
a collaborative multi-agent architecture, where specialized agents, each guided
by the Medical Subject Headings (MeSH) hierarchy, work in concert to
autonomously extract, synthesize, and self-evaluate high-quality textual data
from vast scientific literature. These agents collectively generate and refine
domain-specific question-answer pairs, ensuring comprehensive coverage and
consistency with biomedical ontologies while minimizing manual involvement.
Extensive experimental results show that language models trained on our
multi-agent distilled datasets achieve notable improvements in biomedical
question-answering tasks, outperforming both strong life sciences LLM baselines
and advanced proprietary models. Notably, our AI-Ready dataset enables
Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger
scale. Detailed ablation studies and case analyses further validate the
effectiveness and synergy of each agent within the framework, highlighting the
potential of multi-agent collaboration in biomedical LLM training.

</details>


### [47] [Arabic Metaphor Sentiment Classification Using Semantic Information](https://arxiv.org/abs/2504.19590)
*Israa Alsiyat*

Main category: cs.CL

TL;DR: 论文研究了利用语义标签对阿拉伯语隐喻语料库（AMC）进行情感分类的新工具，通过F-score、召回率和准确率评估其效果，旨在展示阿拉伯语在线隐喻对情感的影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补阿拉伯语隐喻情感分类的空白，探索语义标签在情感分析中的应用，并评估其对隐喻情感影响的量化效果。

Method: 设计了一种基于语义情感标签的自动工具，采用F-score、召回率和准确率等标准方法进行评估。

Result: 该工具首次实现了基于语义标签的阿拉伯语隐喻情感分类，量化了隐喻对情感的影响。

Conclusion: 研究表明，语义标签可以有效用于阿拉伯语隐喻的情感分类，为未来相关研究提供了新方向。

Abstract: In this paper, I discuss the testing of the Arabic Metaphor Corpus (AMC) [1]
using newly designed automatic tools for sentiment classification for AMC based
on semantic tags. The tool incorporates semantic emotional tags for sentiment
classification. I evaluate the tool using standard methods, which are F-score,
recall, and precision. The method is to show the impact of Arabic online
metaphors on sentiment through the newly designed tools. To the best of our
knowledge, this is the first approach to conduct sentiment classification for
Arabic metaphors using semantic tags to find the impact of the metaphor.

</details>


### [48] [Coreference Resolution for Vietnamese Narrative Texts](https://arxiv.org/abs/2504.19606)
*Hieu-Dai Tran,Duc-Vu Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.CL

TL;DR: 论文研究了越南语中的共指消解任务，通过构建VnExpress新闻平台上的标注数据集，评估了GPT-4和GPT-3.5-Turbo的表现，发现GPT-4在准确性和一致性上更优。


<details>
  <summary>Details</summary>
Motivation: 越南语作为低资源语言，缺乏标注数据集，共指消解任务存在挑战。论文旨在填补这一空白并评估大模型的适用性。

Method: 构建了基于VnExpress新闻文本的标注数据集，制定详细标注准则，并测试了GPT-4和GPT-3.5-Turbo的性能。

Result: GPT-4在准确性和一致性上显著优于GPT-3.5-Turbo，更适合越南语共指消解任务。

Conclusion: GPT-4是处理越南语共指消解任务的高效工具，标注数据集的构建也为未来研究提供了资源。

Abstract: Coreference resolution is a vital task in natural language processing (NLP)
that involves identifying and linking different expressions in a text that
refer to the same entity. This task is particularly challenging for Vietnamese,
a low-resource language with limited annotated datasets. To address these
challenges, we developed a comprehensive annotated dataset using narrative
texts from VnExpress, a widely-read Vietnamese online news platform. We
established detailed guidelines for annotating entities, focusing on ensuring
consistency and accuracy. Additionally, we evaluated the performance of large
language models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset.
Our results demonstrate that GPT-4 significantly outperforms GPT-3.5-Turbo in
terms of both accuracy and response consistency, making it a more reliable tool
for coreference resolution in Vietnamese.

</details>


### [49] [VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning](https://arxiv.org/abs/2504.19627)
*Run Luo,Renke Shan,Longze Chen,Ziqiang Liu,Lu Wang,Min Yang,Xiaobo Xia*

Main category: cs.CL

TL;DR: 论文提出了VCM框架，通过自监督视觉概念建模提升大型视觉语言模型的效率，减少计算成本且保持性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在图像处理上的效率低下，缺乏视觉概念模型，限制了实际应用的效果。

Method: 采用自监督对比学习和视觉语言微调构建VCM框架，避免昂贵的概念级标注。

Result: VCM显著降低计算成本（如减少85% FLOPs），并在多个图像理解任务中保持强性能。

Conclusion: VCM框架有效提升了视觉语言模型的效率和性能，验证了其在实际应用中的潜力。

Abstract: Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like
embodied intelligence due to their strong vision-language reasoning abilities.
However, current LVLMs process entire images at the token level, which is
inefficient compared to humans who analyze information and generate content at
the conceptual level, extracting relevant visual concepts with minimal effort.
This inefficiency, stemming from the lack of a visual concept model, limits
LVLMs' usability in real-world applications. To address this, we propose VCM,
an end-to-end self-supervised visual concept modeling framework. VCM leverages
implicit contrastive learning across multiple sampled instances and
vision-language fine-tuning to construct a visual concept model without
requiring costly concept-level annotations. Our results show that VCM
significantly reduces computational costs (e.g., 85\% fewer FLOPs for
LLaVA-1.5-7B) while maintaining strong performance across diverse image
understanding tasks. Moreover, VCM enhances visual encoders' capabilities in
classic visual concept perception tasks. Extensive quantitative and qualitative
experiments validate the effectiveness and efficiency of VCM.

</details>


### [50] [A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks](https://arxiv.org/abs/2504.19645)
*Shadan Shukr Sabr,Nazira Sabr Mustafa,Talar Sabah Omar,Salah Hwayyiz Rasool,Nawzad Anwer Omer,Darya Sabir Hamad,Hemin Abdulhameed Shams,Omer Mahmood Kareem,Rozhan Noori Abdullah,Khabat Atar Abdullah,Mahabad Azad Mohammad,Haneen Al-Raghefy,Safar M. Asaad,Sara Jamal Mohammed,Twana Saeed Ali,Fazil Shawrow,Halgurd S. Maghdid*

Main category: cs.CL

TL;DR: 该研究为中库尔德语（CKL）设计了一个准确且全面的词性标注集（POS tagset），以解决低资源语言在自然语言处理（NLP）任务中资源不足的问题。


<details>
  <summary>Details</summary>
Motivation: 中库尔德语作为低资源语言，缺乏标准化的词性标注集，影响了其NLP任务（如机器翻译、文本推荐）的发展。本研究旨在填补这一空白。

Method: 研究者整合了不同研究及库尔德语言学专家的词性标注，设计了一个标准化的POS标注集，并用于标注大型CKL语料库。

Result: 通过与通用依存框架（Universal Dependencies）的对比，初步验证了该标注集能更准确地优化库尔德语NLP任务的句子处理。

Conclusion: 提出的标准化POS标注集为中库尔德语的NLP任务提供了可靠基础，未来可支持更多语言处理应用。

Abstract: - The field of natural language processing (NLP) has dramatically expanded
within the last decade. Many human-being applications are conducted daily via
NLP tasks, starting from machine translation, speech recognition, text
generation and recommendations, Part-of-Speech tagging (POS), and Named-Entity
Recognition (NER). However, low-resourced languages, such as the
Central-Kurdish language (CKL), mainly remain unexamined due to shortage of
necessary resources to support their development. The POS tagging task is the
base of other NLP tasks; for example, the POS tag set has been used to
standardized languages to provide the relationship between words among the
sentences, followed by machine translation and text recommendation.
Specifically, for the CKL, most of the utilized or provided POS tagsets are
neither standardized nor comprehensive. To this end, this study presented an
accurate and comprehensive POS tagset for the CKL to provide better performance
of the Kurdish NLP tasks. The article also collected most of the POS tags from
different studies as well as from Kurdish linguistic experts to standardized
part-of-speech tags. The proposed POS tagset is designed to annotate a large
CKL corpus and support Kurdish NLP tasks. The initial investigations of this
study via comparison with the Universal Dependencies framework for standard
languages, show that the proposed POS tagset can streamline or correct
sentences more accurately for Kurdish NLP tasks.

</details>


### [51] [Multimodal Conditioned Diffusive Time Series Forecasting](https://arxiv.org/abs/2504.19669)
*Chen Su,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: 提出MCD-TSF模型，联合时间戳和文本作为额外指导，提升时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在时间序列预测中主要关注单模态数值序列，忽略了丰富的多模态信息。

Method: 结合时间戳建立时间及语义关联，文本作为补充描述并动态对齐数据点。

Result: 在八个领域的真实数据集上表现优于现有方法。

Conclusion: MCD-TSF通过多模态条件实现更优的时间序列预测。

Abstract: Diffusion models achieve remarkable success in processing images and text,
and have been extended to special domains such as time series forecasting
(TSF). Existing diffusion-based approaches for TSF primarily focus on modeling
single-modality numerical sequences, overlooking the rich multimodal
information in time series data. To effectively leverage such information for
prediction, we propose a multimodal conditioned diffusion model for TSF,
namely, MCD-TSF, to jointly utilize timestamps and texts as extra guidance for
time series modeling, especially for forecasting. Specifically, Timestamps are
combined with time series to establish temporal and semantic correlations among
different data points when aggregating information along the temporal
dimension. Texts serve as supplementary descriptions of time series' history,
and adaptively aligned with data points as well as dynamically controlled in a
classifier-free manner. Extensive experiments on real-world benchmark datasets
across eight domains demonstrate that the proposed MCD-TSF model achieves
state-of-the-art performance.

</details>


### [52] [Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs](https://arxiv.org/abs/2504.19675)
*Osma Suominen,Juho Inkinen,Mona Lehtinen*

Main category: cs.CL

TL;DR: 该论文介绍了Annif系统，该系统在SemEval-2025 Task 5（LLMs4Subjects）中结合传统NLP和LLM技术，取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 旨在探索如何结合传统技术和大语言模型（LLM）来提高多语言环境下主题标引的准确性和效率。

Method: 结合Annif工具中的传统NLP和机器学习技术，创新性地使用LLM进行翻译、合成数据生成及单语模型预测的合并。

Result: 系统在所有主题类别中排名第一，在tib-core-subjects类别中排名第二，定性评估中排名第四。

Conclusion: 研究表明，传统XMTC算法与现代LLM技术的结合能有效提升多语言主题标引的性能。

Abstract: This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects),
which focussed on subject indexing using large language models (LLMs). The task
required creating subject predictions for bibliographic records from the
bilingual TIBKAT database using the GND subject vocabulary. Our approach
combines traditional natural language processing and machine learning
techniques implemented in the Annif toolkit with innovative LLM-based methods
for translation and synthetic data generation, and merging predictions from
monolingual models. The system ranked first in the all-subjects category and
second in the tib-core-subjects category in the quantitative evaluation, and
fourth in qualitative evaluations. These findings demonstrate the potential of
combining traditional XMTC algorithms with modern LLM techniques to improve the
accuracy and efficiency of subject indexing in multilingual contexts.

</details>


### [53] [Taming the Titans: A Survey of Efficient LLM Inference Serving](https://arxiv.org/abs/2504.19720)
*Ranran Zhen,Juntao Li,Yixin Ji,Zhenlin Yang,Tong Liu,Qingrong Xia,Xinyu Duan,Zhefeng Wang,Baoxing Huai,Min Zhang*

Main category: cs.CL

TL;DR: 该论文综述了大型语言模型（LLM）推理服务中的优化方法，涵盖实例级、集群级、新兴场景及其他关键领域，旨在解决高内存和计算需求导致的延迟和吞吐量问题。


<details>
  <summary>Details</summary>
Motivation: LLM在生成式AI中表现卓越，但因参数庞大和注意力机制的高计算需求，导致推理服务面临低延迟和高吞吐量的挑战，需系统梳理优化方法。

Method: 通过分类综述，包括实例级的模型放置、请求调度等，集群级的GPU集群部署、负载均衡等，以及新兴场景和辅助方法的讨论。

Result: 全面总结了LLM推理服务的优化策略，并提出了未来研究方向。

Conclusion: 论文为LLM推理服务提供了系统化的方法梳理，指明了进一步优化的潜力领域，推动该领域发展。

Abstract: Large Language Models (LLMs) for Generative AI have achieved remarkable
progress, evolving into sophisticated and versatile tools widely adopted across
various domains and applications. However, the substantial memory overhead
caused by their vast number of parameters, combined with the high computational
demands of the attention mechanism, poses significant challenges in achieving
low latency and high throughput for LLM inference services. Recent
advancements, driven by groundbreaking research, have significantly accelerated
progress in this field. This paper provides a comprehensive survey of these
methods, covering fundamental instance-level approaches, in-depth cluster-level
strategies, emerging scenario directions, and other miscellaneous but important
areas. At the instance level, we review model placement, request scheduling,
decoding length prediction, storage management, and the disaggregation
paradigm. At the cluster level, we explore GPU cluster deployment,
multi-instance load balancing, and cloud service solutions. For emerging
scenarios, we organize the discussion around specific tasks, modules, and
auxiliary methods. To ensure a holistic overview, we also highlight several
niche yet critical areas. Finally, we outline potential research directions to
further advance the field of LLM inference serving.

</details>


### [54] [LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding](https://arxiv.org/abs/2504.19734)
*Ying Na,Shihui Feng*

Main category: cs.CL

TL;DR: 该论文提出了一种基于大型语言模型（LLM）的自动对话数据编码新方法，通过多模型协作和上下文一致性检查提升编码准确性。


<details>
  <summary>Details</summary>
Motivation: 对话数据是理解学习过程的关键，但复杂的上下文对自动编码提出了挑战。研究旨在开发一种高精度的LLM辅助自动编码框架。

Method: 1. 使用角色提示和思维链方法，基于对话特性（交流行为和事件）预测编码；2. 多LLM协作预测（GPT-4-turbo、GPT-4o、DeepSeek）；3. 利用事件与行为的关系进行上下文一致性检查（GPT-4o）。

Result: 上下文一致性检查显著提升准确性，交流行为预测的准确性高于事件预测。

Conclusion: 该研究为对话数据的自动编码提供了高精度、可扩展的解决方案，解决了上下文复杂性挑战。

Abstract: Dialogue data has been a key source for understanding learning processes,
offering critical insights into how students engage in collaborative
discussions and how these interactions shape their knowledge construction. The
advent of Large Language Models (LLMs) has introduced promising opportunities
for advancing qualitative research, particularly in the automated coding of
dialogue data. However, the inherent contextual complexity of dialogue presents
unique challenges for these models, especially in understanding and
interpreting complex contextual information. This study addresses these
challenges by developing a novel LLM-assisted automated coding approach for
dialogue data. The novelty of our proposed framework is threefold: 1) We
predict the code for an utterance based on dialogue-specific characteristics --
communicative acts and communicative events -- using separate prompts following
the role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs
including GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We
leveraged the interrelation between events and acts to implement consistency
checking using GPT-4o. In particular, our contextual consistency checking
provided a substantial accuracy improvement. We also found the accuracy of act
predictions was consistently higher than that of event predictions. This study
contributes a new methodological framework for enhancing the precision of
automated coding of dialogue data as well as offers a scalable solution for
addressing the contextual challenges inherent in dialogue analysis.

</details>


### [55] [Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages in LLMs](https://arxiv.org/abs/2504.19759)
*Huichi Zhou,Zehao Xu,Munan Zhao,Kaihong Li,Yiqiang Li,Hongtao Wang*

Main category: cs.CL

TL;DR: 论文介绍了多语言道德推理基准（MMRB），评估大型语言模型（LLMs）在五种语言和三种上下文复杂度下的表现，发现低资源语言表现较差，微调LLaMA-3-8B后发现低资源语言对多语言推理影响更大。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估LLMs在不同语言和上下文复杂度下的道德推理能力，填补多语言道德推理评估的空白。

Method: 方法包括构建MMRB基准，涵盖五种语言和三种复杂度（句子、段落、文档），并对LLaMA-3-8B进行单语数据微调。

Result: 结果显示道德推理性能随上下文复杂度增加而下降，低资源语言（如越南语）表现更差，但微调后低资源语言对多语言推理影响更大。

Conclusion: 结论强调低资源语言在多语言NLP中的关键作用，需更多关注其表现和优化。

Abstract: In this paper, we introduce the Multilingual Moral Reasoning Benchmark (MMRB)
to evaluate the moral reasoning abilities of large language models (LLMs)
across five typologically diverse languages and three levels of contextual
complexity: sentence, paragraph, and document. Our results show moral reasoning
performance degrades with increasing context complexity, particularly for
low-resource languages such as Vietnamese. We further fine-tune the open-source
LLaMA-3-8B model using curated monolingual data for alignment and poisoning.
Surprisingly, low-resource languages have a stronger impact on multilingual
reasoning than high-resource ones, highlighting their critical role in
multilingual NLP.

</details>


### [56] [Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance](https://arxiv.org/abs/2504.19811)
*Takuya Tamura,Taro Yano,Masafumi Enomoto,Masafumi Oyamada*

Main category: cs.CL

TL;DR: 该论文提出了一种基于谱系正则化矩阵分解（LRMF）的新框架，通过图拉普拉斯正则化方法编码大语言模型（LLM）的谱系关系，显著提高了模型性能预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法如缩放定律虽然考虑全局因素（如参数量或训练令牌），但忽略了模型间的谱系关系。作者希望通过引入谱系关系改进LLM性能预测，以减少计算成本和开发时间。

Method: 提出Lineage-Regularized Matrix Factorization (LRMF)框架，利用图拉普拉斯正则化编码LLMs间的多代谱系关系，并结合矩阵分解和协同过滤方法。

Result: 在2934个Hugging Face模型和21,000+实例的6个基准测试中，LRMF比基线方法相关性高出7-10个百分点，且有效解决了冷启动问题。

Conclusion: 谱系约束显著提升了LLM性能预测的准确性，为超参数调优、数据选择和模型组合提供了高效方法。

Abstract: Accurately forecasting the performance of Large Language Models (LLMs) before
extensive fine-tuning or merging can substantially reduce both computational
expense and development time. Although prior approaches like scaling laws
account for global factors such as parameter size or training tokens, they
often overlook explicit lineage relationships - i.e., which models are derived
or merged from which parents. In this work, we propose a novel
Lineage-Regularized Matrix Factorization (LRMF) framework that encodes
ancestral ties among LLMs via a graph Laplacian regularizer. By leveraging
multi-hop parent-child connections, LRMF consistently outperforms conventional
matrix factorization and collaborative filtering methods in both instance-level
and benchmark-level performance prediction. Our large-scale study includes
2,934 publicly available Hugging Face models and 21,000+ instances across 6
major benchmarks, showing that lineage constraints yield up to 7-10 percentage
points higher correlation with actual performance compared to baselines.
Moreover, LRMF effectively addresses the cold-start problem, providing accurate
estimates for newly derived or merged models even with minimal data. This
lineage-guided strategy thus offers a resource-efficient way to inform
hyperparameter tuning, data selection, and model combination in modern LLM
development.

</details>


### [57] [To MT or not to MT: An eye-tracking study on the reception by Dutch readers of different translation and creativity levels](https://arxiv.org/abs/2504.19850)
*Kyo Gerrits,Ana Guerberof-Arenas*

Main category: cs.CL

TL;DR: 研究通过实验对比机器翻译、后编辑、人工翻译及原文在读者认知负荷上的差异，发现创造性潜力单元增加认知负荷，人工翻译影响最大，机器翻译最小，且错误无显著影响。


<details>
  <summary>Details</summary>
Motivation: 探讨不同翻译方式（机器翻译、后编辑、人工翻译及原文）中创造性和错误如何影响读者的认知负荷。

Method: 八名参与者通过问卷、眼动仪阅读故事及回顾性有声思维访谈，分析不同翻译条件下的认知负荷。

Result: 创造性潜力单元显著增加认知负荷，人工翻译效应最强，机器翻译最弱；错误无显著影响。

Conclusion: 翻译创造性在词汇层面对认知负荷的影响是新发现，为未来研究开辟新方向。

Abstract: This article presents the results of a pilot study involving the reception of
a fictional short story translated from English into Dutch under four
conditions: machine translation (MT), post-editing (PE), human translation (HT)
and original source text (ST). The aim is to understand how creativity and
errors in different translation modalities affect readers, specifically
regarding cognitive load. Eight participants filled in a questionnaire, read a
story using an eye-tracker, and conducted a retrospective think-aloud (RTA)
interview. The results show that units of creative potential (UCP) increase
cognitive load and that this effect is highest for HT and lowest for MT; no
effect of error was observed. Triangulating the data with RTAs leads us to
hypothesize that the higher cognitive load in UCPs is linked to increases in
reader enjoyment and immersion. The effect of translation creativity on
cognitive load in different translation modalities at word-level is novel and
opens up new avenues for further research. All the code and data are available
at https://github.com/INCREC/Pilot_to_MT_or_not_to_MT

</details>


### [58] [Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language](https://arxiv.org/abs/2504.19856)
*Anastasia Zhukova,Christian E. Matt,Terry Ruas,Bela Gipp*

Main category: cs.CL

TL;DR: 本文提出了一种名为ICL-APT的高效预训练方法，通过结合上下文学习和k近邻算法，减少对特定领域数据的需求，同时降低计算时间，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统领域自适应持续预训练（DAPT）需要大量特定领域数据，这在非英语语言或特定行业（如德语过程工业）中难以获取。

Method: 提出ICL-APT方法，利用上下文学习（ICL）和k近邻（kNN），通过增强目标领域文本数据，减少计算资源需求。

Result: ICL-APT在平均IR指标（如mAP、MRR、nDCG）上比DAPT高3.5分，计算时间减少近4倍。

Conclusion: 该方法为计算资源有限的行业提供了经济高效的解决方案，并适用于其他低资源领域。

Abstract: Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique
that further trains a language model (LM) on its pretraining task, e.g.,
language masking. Although popular, it requires a significant corpus of
domain-related data, which is difficult to obtain for specific domains in
languages other than English, such as the process industry in the German
language. This paper introduces an efficient approach called ICL-augmented
pretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest
neighbors (kNN) to augment target data with domain-related and in-domain texts,
significantly reducing GPU time while maintaining strong model performance. Our
results show that this approach performs better than traditional DAPT by 3.5 of
the average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost 4 times
less computing time, providing a cost-effective solution for industries with
limited computational capacity. The findings highlight the broader
applicability of this framework to other low-resource industries, making
NLP-based solutions more accessible and feasible in production environments.

</details>


### [59] [semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage](https://arxiv.org/abs/2504.19867)
*Ke Hong,Lufang Chen,Zhong Wang,Xiuhong Li,Qiuli Mao,Jianping Ma,Chao Xiong,Guanyu Wu,Buhe Han,Guohao Dai,Yun Liang,Yu Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新型LLM服务系统semi-PD，通过解耦计算与统一存储，解决了现有系统中存储效率低下的问题，显著提升了高请求率下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统存在存储效率低下的问题，如权重复制、KV缓存传输开销、存储不平衡等，导致高请求率下性能不佳。

Method: 提出semi-PD系统，通过计算资源控制器实现SM级解耦计算，并通过统一内存管理器管理异步内存访问，同时引入动态分区算法优化SLO达成。

Result: semi-PD在高请求率下保持更低延迟，请求端到端延迟降低1.27-2.58倍，并能在延迟约束下处理1.55-1.72倍的请求。

Conclusion: semi-PD通过解耦计算与统一存储的设计，显著提升LLM服务系统的存储效率和性能，为高负载场景提供了更优的解决方案。

Abstract: Existing large language model (LLM) serving systems fall into two categories:
1) a unified system where prefill phase and decode phase are co-located on the
same GPU, sharing the unified computational resource and storage, and 2) a
disaggregated system where the two phases are disaggregated to different GPUs.
The design of the disaggregated system addresses the latency interference and
sophisticated scheduling issues in the unified system but leads to storage
challenges including 1) replicated weights for both phases that prevent
flexible deployment, 2) KV cache transfer overhead between the two phases, 3)
storage imbalance that causes substantial wasted space of the GPU capacity, and
4) suboptimal resource adjustment arising from the difficulties in migrating KV
cache. Such storage inefficiency delivers poor serving performance under high
request rates.
  In this paper, we identify that the advantage of the disaggregated system
lies in the disaggregated computation, i.e., partitioning the computational
resource to enable the asynchronous computation of two phases. Thus, we propose
a novel LLM serving system, semi-PD, characterized by disaggregated computation
and unified storage. In semi-PD, we introduce a computation resource controller
to achieve disaggregated computation at the streaming multi-processor (SM)
level, and a unified memory manager to manage the asynchronous memory access
from both phases. semi-PD has a low-overhead resource adjustment mechanism
between the two phases, and a service-level objective (SLO) aware dynamic
partitioning algorithm to optimize the SLO attainment. Compared to
state-of-the-art systems, semi-PD maintains lower latency at higher request
rates, reducing the average end-to-end latency per request by 1.27-2.58x on
DeepSeek series models, and serves 1.55-1.72x more requests adhering to latency
constraints on Llama series models.

</details>


### [60] [GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets](https://arxiv.org/abs/2504.19898)
*Mingqian He,Fei Zhao,Chonggang Lu,Ziyan Liu,Yue Wang,Haofu Qian*

Main category: cs.CL

TL;DR: 论文提出GenCLS++框架，通过结合监督微调（SFT）和强化学习（RL）优化生成式文本分类器，并在训练和推理阶段系统性探索五种策略维度，最终在多个数据集上显著提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统判别方法忽略了大语言模型（LLM）的生成能力，而现有生成式分类研究仅依赖简单监督微调，未充分利用训练与推理提示的交互及强化学习的潜力。

Method: GenCLS++框架联合优化SFT和RL，并系统性探索五种策略维度（如上下文学习变体、类别定义等），在SFT预热后应用基于规则的RL奖励。

Result: 在七个数据集上，GenCLS++平均准确率比基线提升3.46%，公开数据集上提升达4%。研究发现分类任务无需显式推理步骤效果更佳。

Conclusion: 该框架为生成式文本分类提供了统一优化方案，并揭示了显式推理在分类任务中的反作用，为未来LLM应用提供重要启示。

Abstract: As a fundamental task in machine learning, text classification plays a
crucial role in many areas. With the rapid scaling of Large Language Models
(LLMs), particularly through reinforcement learning (RL), there is a growing
need for more capable discriminators. Consequently, advances in classification
are becoming increasingly vital for enhancing the overall capabilities of LLMs.
Traditional discriminative methods map text to labels but overlook LLMs'
intrinsic generative strengths. Generative classification addresses this by
prompting the model to directly output labels. However, existing studies still
rely on simple SFT alone, seldom probing the interplay between training and
inference prompts, and no work has systematically leveraged RL for generative
text classifiers and unified SFT, RL, and inference-time prompting in one
framework. We bridge this gap with GenCLS++, a framework that jointly optimizes
SFT and RL while systematically exploring five high-level strategy
dimensions-in-context learning variants, category definitions, explicit
uncertainty labels, semantically irrelevant numeric labels, and
perplexity-based decoding-during both training and inference. After an SFT
"policy warm-up," we apply RL with a simple rule-based reward, yielding sizable
extra gains. Across seven datasets, GenCLS++ achieves an average accuracy
improvement of 3.46% relative to the naive SFT baseline; on public datasets,
this improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that
benefit from explicit thinking processes, we find that classification tasks
perform better without such reasoning steps. These insights into the role of
explicit reasoning provide valuable guidance for future LLM applications.

</details>


### [61] [Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking](https://arxiv.org/abs/2504.19940)
*Luigia Costabile,Gian Marco Orlando,Valerio La Gatta,Vincenzo Moscato*

Main category: cs.CL

TL;DR: LLM驱动的生成代理在众包事实核查中表现优于人类群体，具有更低的偏见和更高的内部一致性。


<details>
  <summary>Details</summary>
Motivation: 解决在线错误信息的传播问题，探索LLM在众包事实核查中的潜在应用。

Method: 模拟不同人口统计和意识形态背景的生成代理群体，进行证据检索、声明评估和真相判断。

Result: 代理群体在真相分类上优于人类群体，内部一致性更高，对偏见更不敏感。

Conclusion: 生成代理可作为众包事实核查系统中可扩展、一致且偏见较少的贡献者。

Abstract: The growing spread of online misinformation has created an urgent need for
scalable, reliable fact-checking solutions. Crowdsourced fact-checking - where
non-experts evaluate claim veracity - offers a cost-effective alternative to
expert verification, despite concerns about variability in quality and bias.
Encouraged by promising results in certain contexts, major platforms such as X
(formerly Twitter), Facebook, and Instagram have begun shifting from
centralized moderation to decentralized, crowd-based approaches.
  In parallel, advances in Large Language Models (LLMs) have shown strong
performance across core fact-checking tasks, including claim detection and
evidence evaluation. However, their potential role in crowdsourced workflows
remains unexplored. This paper investigates whether LLM-powered generative
agents - autonomous entities that emulate human behavior and decision-making -
can meaningfully contribute to fact-checking tasks traditionally reserved for
human crowds. Using the protocol of La Barbera et al. (2024), we simulate
crowds of generative agents with diverse demographic and ideological profiles.
Agents retrieve evidence, assess claims along multiple quality dimensions, and
issue final veracity judgments.
  Our results show that agent crowds outperform human crowds in truthfulness
classification, exhibit higher internal consistency, and show reduced
susceptibility to social and cognitive biases. Compared to humans, agents rely
more systematically on informative criteria such as Accuracy, Precision, and
Informativeness, suggesting a more structured decision-making process. Overall,
our findings highlight the potential of generative agents as scalable,
consistent, and less biased contributors to crowd-based fact-checking systems.

</details>


### [62] [TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons](https://arxiv.org/abs/2504.19982)
*Emre Can Acikgoz,Carl Guo,Suvodip Dey,Akul Datta,Takyoung Kim,Gokhan Tur,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: TD-EVAL是一个新的任务导向对话系统评估框架，通过细粒度轮次评估和整体对话比较，解决了传统评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的驱动，任务导向对话系统变得越来越复杂，但现有评估方法无法捕捉交互中的关键中间错误，因此需要更全面的评估框架。

Method: 提出TD-EVAL框架，包括两个步骤：轮次级评估（涵盖对话连贯性、后端知识一致性和策略合规性）和对话级评估（通过TOD Agent Arena进行两两比较）。

Result: 在MultiWOZ 2.4和{\tau}-Bench上的实验表明，TD-EVAL能有效识别传统指标忽略的对话错误，并比传统和LLM指标更符合人类判断。

Conclusion: TD-EVAL为任务导向对话系统评估引入了新范式，高效结合轮次和系统层面评估，为未来研究提供了即插即用的框架。

Abstract: Task-oriented dialogue (TOD) systems are experiencing a revolution driven by
Large Language Models (LLMs), yet the evaluation methodologies for these
systems remain insufficient for their growing sophistication. While traditional
automatic metrics effectively assessed earlier modular systems, they focus
solely on the dialogue level and cannot detect critical intermediate errors
that can arise during user-agent interactions. In this paper, we introduce
TD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework
that unifies fine-grained turn-level analysis with holistic dialogue-level
comparisons. At turn level, we evaluate each response along three TOD-specific
dimensions: conversation cohesion, backend knowledge consistency, and policy
compliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons
to provide a measure of dialogue-level quality. Through experiments on MultiWOZ
2.4 and {\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the
conversational errors that conventional metrics miss. Furthermore, TD-EVAL
exhibits better alignment with human judgments than traditional and LLM-based
metrics. These findings demonstrate that TD-EVAL introduces a new paradigm for
TOD system evaluation, efficiently assessing both turn and system levels with a
plug-and-play framework for future research.

</details>


### [63] [Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom](https://arxiv.org/abs/2504.20000)
*Rishika Sen,Sujoy Roychowdhury,Sumit Soman,H. G. Ranjani,Srikhetra Mohanty*

Main category: cs.CL

TL;DR: 本文研究了在电信领域问答任务中知识蒸馏（KD）的应用，比较了教师模型、学生模型或两者在领域适应中的作用，发现教师模型的监督微调（SFT）能提升蒸馏模型性能，且两者均进行SFT时效果更佳。


<details>
  <summary>Details</summary>
Motivation: 探讨在领域特定任务中，知识蒸馏时教师模型、学生模型或两者的监督微调对模型性能的影响。

Method: 通过实验比较教师模型、学生模型或两者的监督微调，以及不同词汇表和蒸馏算法对蒸馏模型的影响。

Result: 教师模型的SFT能提升蒸馏模型性能，尤其是词汇表一致时；两者均进行SFT时效果最佳，但统计显著性受教师模型词汇表影响。

Conclusion: 在领域特定任务中，建议对教师和学生模型均进行监督微调以获得最佳蒸馏效果。

Abstract: Knowledge Distillation (KD) is one of the approaches to reduce the size of
Large Language Models (LLMs). A LLM with smaller number of model parameters
(student) is trained to mimic the performance of a LLM of a larger size
(teacher model) on a specific task. For domain-specific tasks, it is not clear
if teacher or student model, or both, must be considered for domain adaptation.
In this work, we study this problem from perspective of telecom domain
Question-Answering (QA) task. We systematically experiment with Supervised
Fine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to
KD. We design experiments to study the impact of vocabulary (same and
different) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the
distilled model. Multi-faceted evaluation of the distillation using 14
different metrics (N-gram, embedding and LLM-based metrics) is considered.
Experimental results show that SFT of teacher improves performance of distilled
model when both models have same vocabulary, irrespective of algorithm and
metrics. Overall, SFT of both teacher and student results in better performance
across all metrics, although the statistical significance of the same depends
on the vocabulary of the teacher models.

</details>


### [64] [LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation](https://arxiv.org/abs/2504.20013)
*Beizhe Hu,Qiang Sheng,Juan Cao,Yang Li,Danding Wang*

Main category: cs.CL

TL;DR: 该研究通过模拟和数据集分析，揭示了LLM生成的假新闻在神经新闻推荐系统中的影响，展示了‘真相衰减’现象，并探讨了其成因及应对措施。


<details>
  <summary>Details</summary>
Motivation: 研究针对大语言模型（LLM）被恶意用于假新闻生产的新挑战，探讨其对新闻生态系统的潜在影响。

Method: 开发了一个模拟管道和包含约56k条多样化生成新闻的数据集，分析LLM生成假新闻在神经新闻推荐系统中的效果。

Result: 发现了‘真相衰减’现象，即随着LLM生成新闻的参与，真实新闻在新闻排名中逐渐失去优势。同时揭示了困惑度与新闻排名之间的正相关性。

Conclusion: 研究呼吁相关方采取措施应对这一新兴挑战，以维护新闻生态系统的完整性。

Abstract: Online fake news moderation now faces a new challenge brought by the
malicious use of large language models (LLMs) in fake news production. Though
existing works have shown LLM-generated fake news is hard to detect from an
individual aspect, it remains underexplored how its large-scale release will
impact the news ecosystem. In this study, we develop a simulation pipeline and
a dataset with ~56k generated news of diverse types to investigate the effects
of LLM-generated fake news within neural news recommendation systems. Our
findings expose a truth decay phenomenon, where real news is gradually losing
its advantageous position in news ranking against fake news as LLM-generated
news is involved in news recommendation. We further provide an explanation
about why truth decay occurs from a familiarity perspective and show the
positive correlation between perplexity and news ranking. Finally, we discuss
the threats of LLM-generated fake news and provide possible countermeasures. We
urge stakeholders to address this emerging challenge to preserve the integrity
of news ecosystems.

</details>


### [65] [Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages](https://arxiv.org/abs/2504.20022)
*Pritika Rohera,Chaitrali Ginimav,Gayatri Sawant,Raviraj Joshi*

Main category: cs.CL

TL;DR: 本研究评估了多语言大语言模型（LLMs）在英语和印度语言中的事实准确性，发现模型在英语中表现更优，即使在印度语境问题中也是如此，且在低资源印度语言中更容易出现幻觉。


<details>
  <summary>Details</summary>
Motivation: 探索多语言LLMs在低资源语言（如印度语言）中的事实准确性表现，并与高资源语言（如英语）进行对比。

Method: 使用IndicQuest数据集，比较GPT-4o、Gemma-2-9B、Gemma-2-2B和Llama-3.1-8B在英语和19种印度语言中的回答准确性。

Result: LLMs在英语中表现更可靠，即使在印度语境问题中；低资源印度语言中的回答更容易出现幻觉。

Conclusion: 当前多语言LLMs在低资源语言中的理解和事实准确性仍存在挑战，尤其是在区域语境问题中。

Abstract: Multilingual Large Language Models (LLMs) have demonstrated significant
effectiveness across various languages, particularly in high-resource languages
such as English. However, their performance in terms of factual accuracy across
other low-resource languages, especially Indic languages, remains an area of
investigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,
Gemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in
English and Indic languages using the IndicQuest dataset, which contains
question-answer pairs in English and 19 Indic languages. By asking the same
questions in English and their respective Indic translations, we analyze
whether the models are more reliable for regional context questions in Indic
languages or when operating in English. Our findings reveal that LLMs often
perform better in English, even for questions rooted in Indic contexts.
Notably, we observe a higher tendency for hallucination in responses generated
in low-resource Indic languages, highlighting challenges in the multilingual
understanding capabilities of current LLMs.

</details>


### [66] [AutoJudge: Judge Decoding Without Manual Annotation](https://arxiv.org/abs/2504.20039)
*Roman Garipov,Fedor Velikonivtsev,Ruslan Svirschevski,Vage Egiazarian,Max Ryabinin*

Main category: cs.CL

TL;DR: AutoJudge框架通过任务特定的有损推测解码加速大语言模型推理，利用半贪婪算法和轻量级分类器预测哪些令牌可跳过而不影响质量，显著提升速度且精度损失小。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型推理效率，通过识别不影响下游质量的令牌来加速生成过程。

Method: 使用半贪婪搜索算法确定需纠正的令牌，并训练轻量级分类器预测可跳过的令牌。

Result: 在GSM8K推理任务中，速度提升1.5倍，精度损失低于1%；在LiveCodeBench基准测试中表现类似。

Conclusion: AutoJudge能跨任务通用，显著加速推理且保持高质量输出。

Abstract: We introduce AutoJudge, a framework that accelerates large language model
(LLM) inference with task-specific lossy speculative decoding. Instead of
matching the original model output distribution token-by-token, we identify
which of the generated tokens affect the downstream quality of the generated
response, relaxing the guarantee so that the "unimportant" tokens can be
generated faster. Our approach relies on a semi-greedy search algorithm to test
which of the mismatches between target and draft model should be corrected to
preserve quality, and which ones may be skipped. We then train a lightweight
classifier based on existing LLM embeddings to predict, at inference time,
which mismatching tokens can be safely accepted without compromising the final
answer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B
(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more
accepted tokens per verification cycle with under 1% degradation in answer
accuracy compared to standard speculative decoding and over 2x with small loss
in accuracy. When applied to the LiveCodeBench benchmark, our approach
automatically detects other, programming-specific important tokens and shows
similar speedups, demonstrating its ability to generalize across tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [67] [Critical Challenges and Guidelines in Evaluating Synthetic Tabular Data: A Systematic Review](https://arxiv.org/abs/2504.18544)
*Nazia Nafis,Inaki Esnaola,Alvaro Martinez-Perez,Maria-Cruz Villa-Uriol,Venet Osmani*

Main category: cs.LG

TL;DR: 本文总结了合成健康数据评估的重要性，并基于1766篇论文的筛选和101篇的详细回顾，提出了相关挑战和指南。


<details>
  <summary>Details</summary>
Motivation: 强调合成健康数据评估的严格性，以确保数据的可靠性、相关性和适用性。

Method: 通过系统综述1766篇论文并深入分析101篇，识别关键挑战。

Result: 发现评估方法缺乏共识、指标使用不当、领域专家参与不足、数据集特征报告不充分以及结果可重复性有限等问题。

Conclusion: 提出合成数据生成与评估的指南，以促进其变革潜力并加速创新。

Abstract: Generating synthetic tabular data can be challenging, however evaluation of
their quality is just as challenging, if not more. This systematic review sheds
light on the critical importance of rigorous evaluation of synthetic health
data to ensure reliability, relevance, and their appropriate use. Based on
screening of 1766 papers and a detailed review of 101 papers we identified key
challenges, including lack of consensus on evaluation methods, improper use of
evaluation metrics, limited input from domain experts, inadequate reporting of
dataset characteristics, and limited reproducibility of results. In response,
we provide several guidelines on the generation and evaluation of synthetic
data, to allow the community to unlock and fully harness the transformative
potential of synthetic data and accelerate innovation.

</details>


### [68] [Low-Bit Integerization of Vision Transformers using Operand Reodering for Efficient Hardware](https://arxiv.org/abs/2504.18547)
*Ching-Yi Lin,Sahil Shah*

Main category: cs.LG

TL;DR: 论文提出了一种基于操作重排序的整数化方法，通过延迟反量化至矩阵运算之后，直接在量化输入上处理，以减少计算开销，并在硬件上验证了其低比特推理的能效优势。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉 Transformer 在多种视觉任务中表现优异，但其计算和内存成本高昂。量化虽能降低内存使用，但反量化操作仍带来显著计算开销。本文旨在通过改进计算图，减少这些开销。

Method: 提出了一种基于操作重排序的整数化方法，延迟反量化至矩阵运算之后，从而直接在量化输入上执行矩阵乘法和线性模块操作。

Result: 实验结果表明，该方法在硬件上实现低比特推理，降低了线性层和矩阵乘法的能耗，缩小了量化模型与高效推理之间的差距。

Conclusion: 通过整数化处理和延迟反量化操作，显著减少了计算开销，为量化模型的高效推理提供了可行方案。

Abstract: Pre-trained vision transformers have achieved remarkable performance across
various visual tasks but suffer from expensive computational and memory costs.
While model quantization reduces memory usage by lowering precision, these
models still incur significant computational overhead due to the dequantization
before matrix operations. In this work, we analyze the computation graph and
propose an integerization process based on operation reordering. Specifically,
the process delays dequantization until after matrix operations. This enables
integerized matrix multiplication and linear module by directly processing the
quantized input. To validate our approach, we synthesize the self-attention
module of ViT on a systolic array-based hardware. Experimental results show
that our low-bit inference reduces per-PE power consumption for linear layer
and matrix multiplication, bridging the gap between quantized models and
efficient inference.

</details>


### [69] [RDI: An adversarial robustness evaluation metric for deep neural networks based on sample clustering features](https://arxiv.org/abs/2504.18556)
*Jialei Song,Xingquan Zuo,Feiyang Wang,Hai Huang,Tianle Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的对抗鲁棒性评估指标RDI，基于样本聚类特征，具有高计算效率和攻击独立性，与攻击成功率相关性更强。


<details>
  <summary>Details</summary>
Motivation: 现有对抗鲁棒性评估方法依赖于攻击算法或难以实现，而基于决策边界的方法评估精度低，需一种高效且准确的替代方案。

Method: 提出Robustness Difference Index (RDI)，通过分析决策边界分隔的特征向量的类内和类间距离来量化模型鲁棒性。

Result: 实验证明RDI与攻击成功率相关性更强，计算时间仅为PGD攻击评估方法的1/30。

Conclusion: RDI是一种高效、攻击独立的对抗鲁棒性评估指标，适用于大规模复杂模型。

Abstract: Deep neural networks (DNNs) are highly susceptible to adversarial samples,
raising concerns about their reliability in safety-critical tasks. Currently,
methods of evaluating adversarial robustness are primarily categorized into
attack-based and certified robustness evaluation approaches. The former not
only relies on specific attack algorithms but also is highly time-consuming,
while the latter due to its analytical nature, is typically difficult to
implement for large and complex models. A few studies evaluate model robustness
based on the model's decision boundary, but they suffer from low evaluation
accuracy. To address the aforementioned issues, we propose a novel adversarial
robustness evaluation metric, Robustness Difference Index (RDI), which is based
on sample clustering features. RDI draws inspiration from clustering evaluation
by analyzing the intra-class and inter-class distances of feature vectors
separated by the decision boundary to quantify model robustness. It is
attack-independent and has high computational efficiency. Experiments show
that, RDI demonstrates a stronger correlation with the gold-standard
adversarial robustness metric of attack success rate (ASR). The average
computation time of RDI is only 1/30 of the evaluation method based on the PGD
attack. Our open-source code is available at:
https://anonymous.4open.science/r/RDI-B1DA.

</details>


### [70] [Deep Learning with Pretrained 'Internal World' Layers: A Gemma 3-Based Modular Architecture for Wildfire Prediction](https://arxiv.org/abs/2504.18562)
*Ayoub Jadouli,Chaker El Amrani*

Main category: cs.LG

TL;DR: 该论文利用Gemma 3模型的中层Transformer块进行野火预测，通过定制的前馈模块将表格数据转换为隐藏维度，冻结预训练层以减少参数和过拟合风险，实验证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 大模型（如Gemma 3）的中层隐含丰富的上下文和关系知识，作者希望利用这一特性提升野火预测的准确性和鲁棒性，同时减少数据需求。

Method: 设计了一个定制的前馈模块，将表格数据适配到Gemma 3的中层Transformer块输入维度，冻结预训练层，仅训练输入和输出网络。

Result: 在摩洛哥野火数据集上测试显示，该方法比传统前馈和卷积基线模型具有更高的预测精度和鲁棒性，消融实验验证了冻结层的有效性。

Conclusion: 策略性复用预训练Transformer的中层可作为高效的内部知识库，为环境应用（如野火风险管理）提供更数据高效和可解释的解决方案。

Abstract: Deep learning models, especially large Transformers, carry substantial
"memory" in their intermediate layers -- an \emph{internal world} that encodes
a wealth of relational and contextual knowledge. This work harnesses that
internal world for wildfire occurrence prediction by introducing a modular
architecture built upon Gemma 3, a state-of-the-art multimodal model. Rather
than relying on Gemma 3's original embedding and positional encoding stacks, we
develop a custom feed-forward module that transforms tabular wildfire features
into the hidden dimension required by Gemma 3's mid-layer Transformer blocks.
We freeze these Gemma 3 sub-layers -- thus preserving their pretrained
representation power -- while training only the smaller input and output
networks. This approach minimizes the number of trainable parameters and
reduces the risk of overfitting on limited wildfire data, yet retains the
benefits of Gemma 3's broad knowledge. Evaluations on a Moroccan wildfire
dataset demonstrate improved predictive accuracy and robustness compared to
standard feed-forward and convolutional baselines. Ablation studies confirm
that the frozen Transformer layers consistently contribute to better
representations, underscoring the feasibility of reusing large-model mid-layers
as a learned internal world. Our findings suggest that strategic modular reuse
of pretrained Transformers can enable more data-efficient and interpretable
solutions for critical environmental applications such as wildfire risk
management.

</details>


### [71] [Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism](https://arxiv.org/abs/2504.18574)
*Aviv Bick,Eric Xing,Albert Gu*

Main category: cs.LG

TL;DR: 该研究揭示了Transformer和SSM模型在处理长序列时均采用相同的'Gather-and-Aggregate'机制，但发现SSM实现检索时因注意力模式较平滑而表现较差，提出改进策略。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探究Transformer和SSM模型在长序列任务中的上下文检索机制差异，尤其是为何SSM在检索任务上表现较弱，以寻求优化方向。

Method: 方法是通过对比分析Transformer和SSM模型的注意力头机制，识别出关键的Gather-and-Aggregate（G&A）头，并通过实验（如禁用特定头）验证其对任务性能的影响。

Result: 结果显示，G&A机制集中在少数关键头，禁用单个头会导致性能大幅下降（如MMLU准确率从66%降至25%）。SSM因平滑注意力模式在检索任务上表现较差，但通过引入注意力组件可显著提升。

Conclusion: 结论指出，Transformer与SSM的性能差异源于少数G&A头的实现方式，而非整体模型结构。这一发现为结合两者优势（如混合模型设计）提供了理论依据。

Abstract: SSMs offer efficient processing of long sequences with fixed state sizes, but
struggle with algorithmic tasks like retrieving past context. In this work, we
examine how such in-context retrieval operates within Transformer- and
SSM-based language models. We find that both architectures develop the same
fundamental Gather-and-Aggregate (G&A) mechanism. A Gather Head first
identifies and extracts relevant information from the context, which an
Aggregate Head then integrates into a final representation. Across both model
types, G&A concentrates in just a few heads, making them critical bottlenecks
even for benchmarks that require a basic form of retrieval. For example,
disabling a single Gather or Aggregate Head of a pruned Llama-3.1-8B degrades
its ability to retrieve the correct answer letter in MMLU, reducing accuracy
from 66% to 25%. This finding suggests that in-context retrieval can obscure
the limited knowledge demands of certain tasks. Despite strong MMLU performance
with retrieval intact, the pruned model fails on other knowledge tests. Similar
G&A dependencies exist in GSM8K, BBH, and dialogue tasks. Given the
significance of G&A in performance, we show that retrieval challenges in SSMs
manifest in how they implement G&A, leading to smoother attention patterns
rather than the sharp token transitions that effective G&A relies on. Thus,
while a gap exists between Transformers and SSMs in implementing in-context
retrieval, it is confined to a few heads, not the entire model. This insight
suggests a unified explanation for performance differences between Transformers
and SSMs while also highlighting ways to combine their strengths. For example,
in pretrained hybrid models, attention components naturally take on the role of
Aggregate Heads. Similarly, in a pretrained pure SSM, replacing a single G&A
head with an attention-based variant significantly improves retrieval.

</details>


### [72] [An Artificial Intelligence-Based Framework for Predicting Emergency Department Overcrowding: Development and Evaluation Study](https://arxiv.org/abs/2504.18578)
*Orhun Vural,Bunyamin Ozaydin,Khalid Y. Aram,James Booth,Brittany F. Lindsey,Abdulaziz Ahmed*

Main category: cs.LG

TL;DR: 该研究通过机器学习模型预测急诊科候诊人数，分为每小时和每日两种时间尺度，旨在优化资源配置并缓解拥堵。最佳模型TSiTPlus和XCMPlus分别在不同尺度表现优异。


<details>
  <summary>Details</summary>
Motivation: 急诊科拥堵导致就医延误和运营压力，传统方法通常事后应对。研究希望通过机器学习提前预测候诊人数，实现主动干预。

Method: 使用美国东南部一家医院的急诊数据，整合内外特征，训练11种机器学习算法（包括传统和深度学习模型），并优化特征组合。

Result: TSiTPlus在每小时预测中表现最佳（MAE:4.19），XCMPlus在每日预测中领先（MAE:2.00）。极端情况下预测误差可控。

Conclusion: 模型能准确预测候诊人数，支持主动资源调配，有望改善急诊患者流动和减少拥堵。

Abstract: Background: Emergency department (ED) overcrowding remains a major challenge,
causing delays in care and increased operational strain. Hospital management
often reacts to congestion after it occurs. Machine learning predictive
modeling offers a proactive approach by forecasting patient flow metrics, such
as waiting count, to improve resource planning and hospital efficiency.
  Objective: This study develops machine learning models to predict ED waiting
room occupancy at two time scales. The hourly model forecasts the waiting count
six hours ahead (e.g., a 1 PM prediction for 7 PM), while the daily model
estimates the average waiting count for the next 24 hours (e.g., a 5 PM
prediction for the following day's average). These tools support staffing
decisions and enable earlier interventions to reduce overcrowding.
  Methods: Data from a partner hospital's ED in the southeastern United States
were used, integrating internal metrics and external features. Eleven machine
learning algorithms, including traditional and deep learning models, were
trained and evaluated. Feature combinations were optimized, and performance was
assessed across varying patient volumes and hours.
  Results: TSiTPlus achieved the best hourly prediction (MAE: 4.19, MSE:
29.32). The mean hourly waiting count was 18.11, with a standard deviation of
9.77. Accuracy varied by hour, with MAEs ranging from 2.45 (11 PM) to 5.45 (8
PM). Extreme case analysis at one, two, and three standard deviations above the
mean showed MAEs of 6.16, 10.16, and 15.59, respectively. For daily
predictions, XCMPlus performed best (MAE: 2.00, MSE: 6.64), with a daily mean
of 18.11 and standard deviation of 4.51.
  Conclusions: These models accurately forecast ED waiting room occupancy and
support proactive resource allocation. Their implementation has the potential
to improve patient flow and reduce overcrowding in emergency care settings.

</details>


### [73] [ZipR1: Reinforcing Token Sparsity in MLLMs](https://arxiv.org/abs/2504.18579)
*Feng Chen,Yefei He,Lequan Lin,Jing Liu,Bohan Zhuang,Qi Wu*

Main category: cs.LG

TL;DR: ZipR1提出了一种基于强化学习的后训练方法，通过平衡推理效率和性能，显著减少大语言模型的token处理量，同时在精度上仅轻微下降。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏注意力机制虽然有效，但如何在大语言模型中主动实现token稀疏性仍未被充分研究，限制了推理加速的效果。

Method: 采用强化学习策略，将token减少比例作为效率奖励，答案准确率作为性能奖励，直接优化推理一致的效率-性能权衡。

Result: 实验表明，ZipR1将Qwen2/2.5-VL的token比例从80%降至25%，在13个图像和视频基准测试中仅轻微影响准确率。

Conclusion: ZipR1通过强化学习有效平衡了模型推理的效率和性能，为大语言模型的token稀疏化提供了实用解决方案。

Abstract: Sparse attention mechanisms aim to reduce computational overhead by
selectively processing a subset of salient tokens while preserving model
performance. Despite the effectiveness of such designs, how to actively
encourage token sparsity of well-posed MLLMs remains under-explored, which
fundamentally limits the achievable acceleration effect during inference. In
this paper, we propose a simple RL-based post-training method named
\textbf{ZipR1} that treats the token reduction ratio as the efficiency reward
and answer accuracy as the performance reward.
  In this way, our method can jointly alleviate the computation and memory
bottlenecks via directly optimizing the inference-consistent
efficiency-performance tradeoff. Experimental results demonstrate that ZipR1
can reduce the token ratio of Qwen2/2.5-VL from 80\% to 25\% with a minimal
accuracy reduction on 13 image and video benchmarks.

</details>


### [74] [Parameter-Efficient Checkpoint Merging via Metrics-Weighted Averaging](https://arxiv.org/abs/2504.18580)
*Shi Jie Yu,Sehyun Choi*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为度量加权平均（MWA）的方法，用于在参数高效微调（PEFT）中合并多个模型检查点，通过性能指标（如训练损失或训练步数）加权参数，以提高模型性能。实验证明，MWA优于简单的均匀合并，损失加权合并的效果最佳，任务准确率提升高达5%。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了提升大型语言模型在参数高效微调（PEFT）中的训练效率，通过合并多个模型检查点以减少训练时间，并探索加权方法的有效性。

Method: 提出Metrics-Weighted Averaging（MWA），通过性能指标（如训练损失或训练步数）加权检查点参数，并使用惩罚因子调整权重分布，仅需一个超参数。

Result: 实验表明，MWA在数学推理、偏好对齐和指令微调三个任务上均优于均匀合并，损失加权合并的准确率提升最高达5%，甚至超过单个最终检查点的性能。

Conclusion: 研究验证了检查点合并对PEFT的有效性，并证明基于度量的加权方法能够以最小计算开销显著提升模型性能。

Abstract: Checkpoint merging is a technique for combining multiple model snapshots into
a single superior model, potentially reducing training time for large language
models. This paper explores checkpoint merging in the context of
parameter-efficient fine-tuning (PEFT), where only small adapter modules (e.g.
LoRA) are trained. We propose Metrics-Weighted Averaging (MWA), a simple yet
effective method to merge model checkpoints by weighting their parameters
according to performance metrics. In particular, we investigate weighting by
training loss and by training steps, under the intuition that lower-loss or
later-step checkpoints are more valuable. We introduce a formula with a penalty
factor to adjust weight distribution, requiring only one hyperparameter
regardless of the number of checkpoints. Experiments on three fine-tuning tasks
(mathematical reasoning, preference alignment, and general instruction tuning)
show that MWA consistently produces merged models that outperform the naive
uniform average of checkpoints. Notably, loss-weighted merging often yields the
best results, delivering up to 5% higher task accuracy than the baseline
uniform merge and even surpassing the final individual checkpoint's
performance. These findings validate checkpoint merging for PEFT and
demonstrate that a metric-driven weighting heuristic can efficiently boost
model performance with minimal computational overhead.

</details>


### [75] [PARD: Accelerating LLM Inference with Low-Cost PARallel Draft Model Adaptation](https://arxiv.org/abs/2504.18583)
*Zihao An,Huajun Bai,Ziqiong Liu,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: PARD是一种新的推测解码方法，通过并行草案模型预测多个未来令牌，训练效率提升3倍，推理速度提升4.08倍。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs的推理速度受限于自回归性质，内存带宽成为瓶颈。推测解码虽能加速，但草案阶段的成本和训练开销限制了效率和适应性。

Method: 提出PARD，将自回归草案模型低成本适配为并行草案模型，单次前向预测多令牌，并引入条件丢弃令牌方法加速训练。

Result: PARD将LLaMA3.1-8B的推理速度提升至311.5令牌/秒（4.08倍加速），训练效率提高3倍。

Conclusion: PARD通过并行化和条件丢弃令牌，显著提升LLMs的推理效率和训练适应性，同时降低适配成本。

Abstract: The autoregressive nature of large language models (LLMs) limits inference
speed. Each forward pass generates only a single token and is often
bottlenecked by memory bandwidth. Speculative decoding alleviates this issue
using a draft-then-verify approach to accelerate token generation. However, the
overhead introduced during the draft phase and the training cost of the draft
model limit the efficiency and adaptability of speculative decoding. In this
work, we introduce PARallel Draft (PARD), a novel speculative decoding method
that enables low-cost adaptation of autoregressive draft models into parallel
draft models. PARD enhances inference efficiency by predicting multiple future
tokens in a single forward pass of the draft phase, and incorporates a
conditional drop token method to accelerate training. Its target-independence
property allows a single draft model to be applied to an entire family of
different models, minimizing the adaptation cost. Our proposed conditional drop
token method can improves draft model training efficiency by 3x. On our
optimized inference framework, PARD accelerates LLaMA3.1-8B inference by 4.08x,
achieving 311.5 tokens per second.

</details>


### [76] [Training Large Language Models to Reason via EM Policy Gradient](https://arxiv.org/abs/2504.18587)
*Tianbing Xu*

Main category: cs.LG

TL;DR: 该论文提出了一种名为EM Policy Gradient的离策略强化学习算法，通过将推理任务构建为EM优化问题，交替采样多样化的推理轨迹并基于奖励进行微调，旨在提升大型语言模型的推理能力。与PPO和GRPO等复杂方法相比，该方法更简单且保持高性能，在GSM8K和MATH (HARD)数据集上表现优异，同时增强了模型的解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法（如PPO、GRPO）依赖复杂的权重调整和启发式剪裁，限制了其在大型语言模型推理任务中的效率和可扩展性。本文希望通过一种更简单、更原则化的离策略方法优化推理轨迹的期望回报，从而提升模型的推理能力、解释性和鲁棒性。

Method: 提出EM Policy Gradient算法，将推理任务建模为期望最大化（EM）问题，交替进行两步：1) 采样多样化的推理轨迹；2) 基于奖励信号对策略进行微调。相比PPO和GRPO，避免了重要性权重和剪裁的复杂性。

Result: 在GSM8K和MATH (HARD)数据集上，性能与当前最优方法GRPO相当或略优，同时具备更好的可扩展性、简洁性和推理凝练性。微调后的模型展现出子问题分解、自我验证和回溯等认知行为。

Conclusion: EM Policy Gradient提供了一种简单高效的离策略强化学习框架，显著提升了LLM的推理性能，并揭示了其在增强模型解释性和鲁棒性方面的潜力。

Abstract: Recently, foundation models such as OpenAI's O1 and O3, along with DeepSeek's
R1, have demonstrated strong reasoning capacities and problem-solving skills
acquired through large-scale reinforcement learning (RL), with wide
applications in mathematics, coding, science, intelligent agents, and virtual
assistants. In this work, we introduce an off-policy reinforcement learning
algorithm, EM Policy Gradient, aimed at enhancing LLM reasoning by optimizing
expected return over reasoning trajectories. We frame the reasoning task as an
Expectation-Maximization (EM) optimization problem, alternating between
sampling diverse rationale trajectories and performing reward-guided
fine-tuning. Unlike PPO and GRPO, which rely on complex importance weights and
heuristic clipping, our method provides a simpler, more principled off-policy
policy gradient approach, eliminating these complexities while maintaining
strong performance. We evaluate the effectiveness of EM Policy Gradient on the
GSM8K and MATH (HARD) datasets, where it achieves performance comparable to or
slightly surpassing the state-of-the-art GRPO, while offering additional
advantages in scalability, simplicity, and reasoning conciseness. Moreover,
models fine-tuned with our method exhibit cognitive behaviors, such as
sub-problem decomposition, self-verification, and backtracking, highlighting
its potential to enhance both the interpretability and robustness of LLM
reasoning.

</details>


### [77] [Dynamic QoS Prediction via a Non-Negative Tensor Snowflake Factorization](https://arxiv.org/abs/2504.18588)
*YongHui Xia,Lan Wang,Hao Wu*

Main category: cs.LG

TL;DR: 论文提出了一种非负雪花张量分解模型（Non-negative Snowflake Factorization of tensors）来预测缺失的动态QoS数据，通过雪花核心张量和SLF-NMUT参数学习方法提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在Web服务中，动态QoS数据存在大量未观测值，影响用户服务选择，因此需要一种能够准确预测缺失数据的模型。

Method: 提出了一种基于雪花核心张量的非负张量分解模型，并采用SLF-NMUT（单潜在因子非负乘法更新）进行参数学习。

Result: 实验表明，该模型能更准确地学习动态用户-服务交互模式，从而提升缺失QoS数据的预测效果。

Conclusion: 该方法有效解决了动态QoS数据缺失问题，为Web服务中的用户行为和服务条件分析提供了更好的支持。

Abstract: Dynamic quality of service (QoS) data exhibit rich temporal patterns in
user-service interactions, which are crucial for a comprehensive understanding
of user behavior and service conditions in Web service. As the number of users
and services increases, there is a large amount of unobserved QoS data, which
significantly affects users'choice of services. To predict unobserved QoS data,
we propose a Non-negative Snowflake Factorization of tensors model. This method
designs a snowflake core tensor to enhance the model's learning capability.
Additionally, it employs a single latent factor-based, nonnegative
multiplication update on tensor (SLF-NMUT) for parameter learning. Empirical
results demonstrate that the proposed model more accurately learns dynamic
user-service interaction patterns, thereby yielding improved predictions for
missing QoS data.

</details>


### [78] [A multilevel approach to accelerate the training of Transformers](https://arxiv.org/abs/2504.18590)
*Guillaume Lauga,Maël Chaumette,Edgar Desainte-Maréville,Étienne Lasalle,Arthur Lebeurrier*

Main category: cs.LG

TL;DR: 该研究探索了多级方法加速Transformer架构训练的潜力，通过ODE解释提出变离散化策略，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: Transformer架构的训练通常耗时且资源密集，因此研究旨在通过多级方法优化训练过程，提升效率。

Method: 将Transformer架构解释为常微分方程（ODE），并提出变离散化策略以加速训练。

Result: 实验表明，该方法相比标准训练流程能有效提升训练速度。

Conclusion: 基于ODE的多级离散化策略为加速Transformer训练提供了可行方向。

Abstract: In this article, we investigate the potential of multilevel approaches to
accelerate the training of transformer architectures. Using an ordinary
differential equation (ODE) interpretation of these architectures, we propose
an appropriate way of varying the discretization of these ODE Transformers in
order to accelerate the training. We validate our approach experimentally by a
comparison with the standard training procedure.

</details>


### [79] [Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations](https://arxiv.org/abs/2504.18591)
*Giovanni Catalani,Michael Bauerheim,Frédéric Tost,Xavier Bertrand,Joseph Morlier*

Main category: cs.LG

TL;DR: enf2enf是一种基于等变神经场架构的编码器-解码器方法，用于预测具有非参数化几何变化的稳态偏微分方程，通过局部性和平移不变性的归纳偏置，实现了高效训练和高精度预测。


<details>
  <summary>Details</summary>
Motivation: 近年来，神经场在处理偏微分方程（PDE）的通用几何学习方面取得了进展，但如何高效处理非参数化几何变化并保持物理一致性仍具挑战性。enf2enf的提出旨在解决这一问题。

Method: enf2enf将输入几何编码为潜在点云嵌入，结合全局参数并解码为连续输出场。通过局部性和平移不变性的归纳偏置，模型能捕捉细尺度物理特征和复杂形状变化。

Result: 在空气动力学数据集、超弹性材料基准和多元素翼型几何上的实验表明，enf2enf优于或与最先进的图基、算子学习和神经场方法相当，支持实时推理和零样本超分辨率。

Conclusion: enf2enf通过结合几何与物理的耦合建模，显著提升了泛化能力和物理一致性，为复杂几何下的PDE求解提供了高效且高精度的解决方案。

Abstract: Recent advances in Neural Fields have enabled powerful,
discretization-invariant methods for learning neural operators that approximate
solutions of Partial Differential Equations (PDEs) on general geometries.
Building on these developments, we introduce enf2enf, an encoder--decoder
methodology for predicting steady-state Partial Differential Equations with
non-parameterized geometric variability, based on recently proposed Equivariant
Neural Field architectures. In enf2enf, input geometries are encoded into
latent point cloud embeddings that inherently preserve geometric grounding and
capture local phenomena. The resulting representations are then combined with
global parameters and directly decoded into continuous output fields, thus
efficiently modeling the coupling between geometry and physics. By leveraging
the inductive biases of locality and translation invariance, our approach is
able to capture fine-scale physical features as well as complex shape
variations, thereby enhancing generalization and physical compliance. Extensive
experiments on a high-fidelity aerodynamic dataset, a hyper-elastic material
benchmark, and multi-element airfoil geometries, demonstrate that the proposed
model achieves superior or competitive performance compared to state-of-the-art
graph based, operator learning, and neural field methods. Notably, our method
supports real time inference and zero-shot super-resolution, enabling efficient
training on low-resolution meshes while maintaining high accuracy on full-scale
discretizations.

</details>


### [80] [Severity Classification of Chronic Obstructive Pulmonary Disease in Intensive Care Units: A Semi-Supervised Approach Using MIMIC-III Dataset](https://arxiv.org/abs/2504.18593)
*Akram Shojaei,Mehdi Delrobaei*

Main category: cs.LG

TL;DR: 该研究开发了一种基于机器学习的COPD严重程度分类模型，利用MIMIC-III数据库和半监督学习技术，随机森林分类器表现最佳，准确率达92.51%，ROC AUC为0.98，为ICU中的COPD评估提供了高效工具。


<details>
  <summary>Details</summary>
Motivation: COPD是全球重大健康负担，ICU中精确评估其严重程度对临床管理至关重要，研究旨在通过人工智能技术提升评估效率和准确性。

Method: 利用MIMIC-III数据库，结合血液气体检测和生命体征等关键ICU参数，采用半监督学习技术开发分类模型。

Result: 随机森林分类器效果最佳，准确率达92.51%，ROC AUC为0.98，能有效区分轻中度与重度COPD病例。

Conclusion: 该机器学习模型为ICU中的COPD严重程度评估提供了实用且高效的工具，未来需进一步外部验证并与临床决策支持系统整合。

Abstract: Chronic obstructive pulmonary disease (COPD) represents a significant global
health burden, where precise severity assessment is particularly critical for
effective clinical management in intensive care unit (ICU) settings. This study
introduces an innovative machine learning framework for COPD severity
classification utilizing the MIMIC-III critical care database, thereby
expanding the applications of artificial intelligence in critical care
medicine. Our research developed a robust classification model incorporating
key ICU parameters such as blood gas measurements and vital signs, while
implementing semi-supervised learning techniques to effectively utilize
unlabeled data and enhance model performance. The random forest classifier
emerged as particularly effective, demonstrating exceptional discriminative
capability with 92.51% accuracy and 0.98 ROC AUC in differentiating between
mild-to-moderate and severe COPD cases. This machine learning approach provides
clinicians with a practical, accurate, and efficient tool for rapid COPD
severity evaluation in ICU environments, with significant potential to improve
both clinical decision-making processes and patient outcomes. Future research
directions should prioritize external validation across diverse patient
populations and integration with clinical decision support systems to optimize
COPD management in critical care settings.

</details>


### [81] [A Simple DropConnect Approach to Transfer-based Targeted Attack](https://arxiv.org/abs/2504.18594)
*Tongrui Su,Qingbin Li,Shengyu Zhu,Wei Chen,Xueqi Cheng*

Main category: cs.LG

TL;DR: 该论文提出了一种名为MCD的方法，通过减少对抗样本中的扰动共适应性来提升其可转移性，在从CNN到Transformer模型的攻击中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于迁移的黑盒目标攻击方法的攻击成功率较低，对抗样本容易过拟合代理模型而无法误导其他模型。

Method: 提出MCD方法，通过DropConnect在每次优化迭代中生成多样化的代理模型变体，以减少扰动共适应性。

Result: MCD在从CNN到Transformer模型的攻击中平均攻击成功率提升了13%，且在高计算资源下表现最佳。

Conclusion: MCD通过多样化代理模型变体有效提升了对抗样本的可转移性，尤其在复杂模型间迁移时效果显著。

Abstract: We study the problem of transfer-based black-box attack, where adversarial
samples generated using a single surrogate model are directly applied to target
models. Compared with untargeted attacks, existing methods still have lower
Attack Success Rates (ASRs) in the targeted setting, i.e., the obtained
adversarial examples often overfit the surrogate model but fail to mislead
other models. In this paper, we hypothesize that the pixels or features in
these adversarial examples collaborate in a highly dependent manner to maximize
the success of an adversarial attack on the surrogate model, which we refer to
as perturbation co-adaptation. Then, we propose to Mitigate perturbation
Co-adaptation by DropConnect (MCD) to enhance transferability, by creating
diverse variants of surrogate model at each optimization iteration. We conduct
extensive experiments across various CNN- and Transformer-based models to
demonstrate the effectiveness of MCD. In the challenging scenario of
transferring from a CNN-based model to Transformer-based models, MCD achieves
13% higher average ASRs compared with state-of-the-art baselines. MCD boosts
the performance of self-ensemble methods by bringing in more diversification
across the variants while reserving sufficient semantic information for each
variant. In addition, MCD attains the highest performance gain when scaling the
compute of crafting adversarial examples.

</details>


### [82] [EnviroPiNet: A Physics-Guided AI Model for Predicting Biofilter Performance](https://arxiv.org/abs/2504.18595)
*Uzma,Fabien Cholet,Domenic Quinn,Cindy Smith,Siming You,William Sloan*

Main category: cs.LG

TL;DR: 该研究首次将白金汉π理论应用于生物滤池性能建模，提出了一种物理引导的模型EnviroPiNet，显著优于传统方法，验证了物理原理与AI结合在环境系统中的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于环境生物技术中微生物群落与物理化学环境间复杂相互作用的高维稀疏数据难以捕捉系统行为，需创新科学引导的方法来提升预测精度。

Method: 采用白金汉π理论进行降维，提取有意义的无量纲变量，并基于此开发物理引导模型EnviroPiNet，与PCA和自编码器神经网络进行对比。

Result: EnviroPiNet在测试集上R²值达0.9236，显著优于传统方法，同时白金汉π变量揭示了生物滤池行为的物理化学关系。

Conclusion: 结合物理原理与AI方法能有效建模高维稀疏数据的环境系统，为系统设计与优化提供了新思路。

Abstract: Environmental biotechnologies, such as drinking water biofilters, rely on
complex interactions between microbial communities and their surrounding
physical-chemical environments. Predicting the performance of these systems is
challenging due to high-dimensional, sparse datasets that lack diversity and
fail to fully capture system behaviour. Accurate predictive models require
innovative, science-guided approaches. In this study, we present the first
application of Buckingham Pi theory to modelling biofilter performance. This
dimensionality reduction technique identifies meaningful, dimensionless
variables that enhance predictive accuracy and improve model interpretability.
Using these variables, we developed the Environmental Buckingham Pi Neural
Network (EnviroPiNet), a physics-guided model benchmarked against traditional
data-driven methods, including Principal Component Analysis (PCA) and
autoencoder neural networks. Our findings demonstrate that the EnviroPiNet
model achieves an R^2 value of 0.9236 on the testing dataset, significantly
outperforming PCA and autoencoder methods. The Buckingham Pi variables also
provide insights into the physical and chemical relationships governing
biofilter behaviour, with implications for system design and optimization. This
study highlights the potential of combining physical principles with AI
approaches to model complex environmental systems characterized by sparse,
high-dimensional datasets.

</details>


### [83] [A Hybrid Framework for Real-Time Data Drift and Anomaly Identification Using Hierarchical Temporal Memory and Statistical Tests](https://arxiv.org/abs/2504.18599)
*Subhadip Bandyopadhyay,Joy Bose,Sujoy Roy Chowdhury*

Main category: cs.LG

TL;DR: 文章提出了一种结合HTM和SPRT的新方法，用于实时数据漂移检测和异常识别，无需频繁重训练且误报率低。实验证明该方法在准确性和计算效率上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 由于数据漂移会导致模型性能下降，现有的数据漂移检测方法需要频繁重训练且误报率高。因此，需要一种更高效和准确的实时检测方法。

Method: 采用HTM（分层时序记忆）和SPRT（序列概率比检验）结合的混合框架，利用HTM的在线学习能力和SPRT的统计优势。

Result: 实验结果表明，该方法在准确性、适应性和计算效率上优于传统的KS检验、Wasserstein距离和PSI等方法。

Conclusion: 提出的方法在实时数据漂移检测和异常识别中表现出色，适用于电信等领域，并提供了超参数优化的见解。

Abstract: Data Drift is the phenomenon where the generating model behind the data
changes over time. Due to data drift, any model built on the past training data
becomes less relevant and inaccurate over time. Thus, detecting and controlling
for data drift is critical in machine learning models. Hierarchical Temporal
Memory (HTM) is a machine learning model developed by Jeff Hawkins, inspired by
how the human brain processes information. It is a biologically inspired model
of memory that is similar in structure to the neocortex, and whose performance
is claimed to be comparable to state of the art models in detecting anomalies
in time series data. Another unique benefit of HTMs is its independence from
training and testing cycle; all the learning takes place online with streaming
data and no separate training and testing cycle is required. In sequential
learning paradigm, Sequential Probability Ratio Test (SPRT) offers some unique
benefit for online learning and inference. This paper proposes a novel hybrid
framework combining HTM and SPRT for real-time data drift detection and anomaly
identification. Unlike existing data drift methods, our approach eliminates
frequent retraining and ensures low false positive rates. HTMs currently work
with one dimensional or univariate data. In a second study, we also propose an
application of HTM in multidimensional supervised scenario for anomaly
detection by combining the outputs of multiple HTM columns, one for each
dimension of the data, through a neural network. Experimental evaluations
demonstrate that the proposed method outperforms conventional drift detection
techniques like the Kolmogorov-Smirnov (KS) test, Wasserstein distance, and
Population Stability Index (PSI) in terms of accuracy, adaptability, and
computational efficiency. Our experiments also provide insights into optimizing
hyperparameters for real-time deployment in domains such as Telecom.

</details>


### [84] [Unsupervised outlier detection to improve bird audio dataset labels](https://arxiv.org/abs/2504.18650)
*Bruce Collins*

Main category: cs.LG

TL;DR: 论文探讨了从Xeno-Canto鸟鸣音频库中提取干净数据集的挑战，提出了一种结合音频预处理、降维和无监督异常检测的方法来减少标签噪声。尽管不同降维技术在物种间表现不一，但整体上能有效降低噪声。


<details>
  <summary>Details</summary>
Motivation: Xeno-Canto音频库为鸟类物种识别提供了宝贵资源，但每段录音仅标注一个物种，常混杂其他声音（如其他鸟类、环境噪声），导致标签噪声问题。本研究旨在解决这一问题。

Method: 提出清理流程：音频预处理→降维（采用两种卷积自编码器和VaDE）→无监督异常检测（UOD）。通过比较三种降维技术评估效果。

Result: 两种自编码器和VaDE在不同物种数据集上均能检测异常，但效果因物种而异。清理流程总体上显著降低了标签噪声。

Conclusion: 研究表明，该方法能有效减少Xeno-Canto数据集的标签噪声，但因物种差异需针对性优化，为未来研究奠定了基础。

Abstract: The Xeno-Canto bird audio repository is an invaluable resource for those
interested in vocalizations and other sounds made by birds around the world.
This is particularly the case for machine learning researchers attempting to
improve on the bird species recognition accuracy of classification models.
However, the task of extracting labeled datasets from the recordings found in
this crowd-sourced repository faces several challenges. One challenge of
particular significance to machine learning practitioners is that one bird
species label is applied to each audio recording, but frequently other sounds
are also captured including other bird species, other animal sounds,
anthropogenic and other ambient sounds. These non-target bird species sounds
can result in dataset labeling discrepancies referred to as label noise. In
this work we present a cleaning process consisting of audio preprocessing
followed by dimensionality reduction and unsupervised outlier detection (UOD)
to reduce the label noise in a dataset derived from Xeno-Canto recordings. We
investigate three neural network dimensionality reduction techniques: two
flavors of convolutional autoencoders and variational deep embedding (VaDE
(Jiang, 2017)). While both methods show some degree of effectiveness at
detecting outliers for most bird species datasets, we found significant
variation in the performance of the methods from one species to the next. We
believe that the results of this investigation demonstrate that the application
of our cleaning process can meaningfully reduce the label noise of bird species
datasets derived from Xeno-Canto audio repository but results vary across
species.

</details>


### [85] [Exploring the Potential of Latent Embeddings for Sea Ice Characterization using ICESat-2 Data](https://arxiv.org/abs/2504.18668)
*Daehyeon Han,Morteza Karimzadeh*

Main category: cs.LG

TL;DR: 该研究探索了在ICESat-2数据上使用无监督自编码器（LSTM和CNN）生成潜在嵌入，以减少对人工标注的依赖，并通过UMAP降维可视化嵌入，结果表明嵌入能减少所需标签样本量。


<details>
  <summary>Details</summary>
Motivation: 减轻对人工标注的依赖，解决ICESat-2轨迹与背景图像重合率低的问题。

Method: 使用LSTM和CNN自编码器在无标注ICESat-2数据上生成嵌入，并通过UMAP降维和可视化。

Result: 嵌入保留了数据结构并形成更紧凑的簇，可能减少所需标注样本量。

Conclusion: 无监督自编码器在ICESat-2数据处理中具有潜力，可降低标注成本。

Abstract: The Ice, Cloud, and Elevation Satellite-2 (ICESat-2) provides high-resolution
measurements of sea ice height. Recent studies have developed machine learning
methods on ICESat-2 data, primarily focusing on surface type classification.
However, the heavy reliance on manually collected labels requires significant
time and effort for supervised learning, as it involves cross-referencing track
measurements with overlapping background optical imagery. Additionally, the
coincidence of ICESat-2 tracks with background images is relatively rare due to
the different overpass patterns and atmospheric conditions. To address these
limitations, this study explores the potential of unsupervised autoencoder on
unlabeled data to derive latent embeddings. We develop autoencoder models based
on Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNN) to
reconstruct topographic sequences from ICESat-2 and derive embeddings. We then
apply Uniform Manifold Approximation and Projection (UMAP) to reduce dimensions
and visualize the embeddings. Our results show that embeddings from
autoencoders preserve the overall structure but generate relatively more
compact clusters compared to the original ICESat-2 data, indicating the
potential of embeddings to lessen the number of required labels samples.

</details>


### [86] [A Unified MDL-based Binning and Tensor Factorization Framework for PDF Estimation](https://arxiv.org/abs/2504.18686)
*Mustafa Musab,Joseph K. Chege,Arie Yeredor,Martin Haardt*

Main category: cs.LG

TL;DR: 该论文提出了一种基于最小描述长度（MDL）分箱和分位数切割的非参数多变量概率密度函数（PDF）估计方法，解决了传统直方图方法的局限性，适用于复杂多模态数据。


<details>
  <summary>Details</summary>
Motivation: 传统基于均匀直方图的密度估计方法无法捕捉高度非均匀分布的局部变化，且不连续性对梯度优化等任务造成挑战，因此需要更鲁棒的估计方法。

Method: 利用最小描述长度（MDL）分箱和分位数切割，结合张量分解技术（如规范多叉分解CPD）构建联合概率张量。

Result: 在合成数据和真实干豆分类数据集上验证了方法的有效性。

Conclusion: 该方法为复杂多模态数据的密度估计提供了一种高效且鲁棒的解决方案。

Abstract: Reliable density estimation is fundamental for numerous applications in
statistics and machine learning. In many practical scenarios, data are best
modeled as mixtures of component densities that capture complex and multimodal
patterns. However, conventional density estimators based on uniform histograms
often fail to capture local variations, especially when the underlying
distribution is highly nonuniform. Furthermore, the inherent discontinuity of
histograms poses challenges for tasks requiring smooth derivatives, such as
gradient-based optimization, clustering, and nonparametric discriminant
analysis. In this work, we present a novel non-parametric approach for
multivariate probability density function (PDF) estimation that utilizes
minimum description length (MDL)-based binning with quantile cuts. Our approach
builds upon tensor factorization techniques, leveraging the canonical polyadic
decomposition (CPD) of a joint probability tensor. We demonstrate the
effectiveness of our method on synthetic data and a challenging real dry bean
classification dataset.

</details>


### [87] [Active Few-Shot Learning for Vertex Classification Starting from an Unlabeled Dataset](https://arxiv.org/abs/2504.18696)
*Felix Burr,Marcel Hoffmann,Ansgar Scherp*

Main category: cs.LG

TL;DR: 论文研究了在只有少量标记顶点的情况下，使用原型网络和判别模型（如图卷积网络）进行顶点分类的性能比较。实验表明，当每类样本少于20个时，原型网络在所有设定中表现更优，尤其在放松类别先知假设时，其性能下降更小。


<details>
  <summary>Details</summary>
Motivation: 由于获取顶点标记既耗时又昂贵，研究如何在仅有少量标记顶点的情况下有效学习是必要的。现有方法假设存在类别先知（提供所需类别的标记顶点），但这与现实场景不符。

Method: 通过迭代提示人工标注者标记顶点进行模型训练。实验分三阶段逐步放松假设：平衡采样（假设类别先知）、不平衡采样（用k-medoids聚类代替类别先知）、未知类别数量。

Result: 原型网络在所有实验中优于判别模型（尤其是每类样本少于20时）。放松类别先知假设后，GCN性能下降9%，而原型网络仅下降1%；在未知类别数量时，两者性能均再降1%。

Conclusion: 原型网络在少量标记数据下更具鲁棒性，尤其在现实场景中缺乏类别分布先验时，表现更加稳定。

Abstract: Despite the ample availability of graph data, obtaining vertex labels is a
tedious and expensive task. Therefore, it is desirable to learn from a few
labeled vertices only. Existing few-shot learners assume a class oracle, which
provides labeled vertices for a desired class. However, such an oracle is not
available in a real-world setting, i.e., when drawing a vertex for labeling it
is unknown to which class the vertex belongs. Few-shot learners are often
combined with prototypical networks, while classical semi-supervised vertex
classification uses discriminative models, e.g., Graph Convolutional Networks
(GCN). In this paper, we train our models by iteratively prompting a human
annotator with vertices to annotate. We perform three experiments where we
continually relax our assumptions. First, we assume a class oracle, i.e., the
human annotator is provided with an equal number of vertices to label for each
class. We denote this as "Balanced Sampling''. In the subsequent experiment,
"Unbalanced Sampling,'' we replace the class oracle with $k$-medoids clustering
and draw vertices to label from the clusters. In the last experiment, the
"Unknown Number of Classes,'' we no longer assumed we knew the number and
distribution of classes. Our results show that prototypical models outperform
discriminative models in all experiments when fewer than $20$ samples per class
are available. While dropping the assumption of the class oracle for the
"Unbalanced Sampling'' experiment reduces the performance of the GCN by $9\%$,
the prototypical network loses only $1\%$ on average. For the "Unknown Number
of Classes'' experiment, the average performance for both models decreased
further by $1\%$.
  Source code: https://github.com/Ximsa/2023-felix-ma

</details>


### [88] [Explicit neural network classifiers for non-separable data](https://arxiv.org/abs/2504.18710)
*Patrícia Muñoz Ewald*

Main category: cs.LG

TL;DR: 论文通过截断映射全面描述了一类前馈神经网络，并展示了ReLU网络如何实现分离同心数据的特征映射。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过截断映射深入理解前馈神经网络的结构与能力，尤其关注其在分离复杂数据（如同心数据）上的应用。

Method: 作者采用截断映射对一类神经网络进行完整描述，并以ReLU网络为例，展示其实现特征映射的具体方法。

Result: 研究证明ReLU网络能通过特定特征映射有效分离同心数据，验证了截断映射在此类网络中的实用性。

Conclusion: 论文通过理论分析与实例验证，为前馈神经网络的结构设计与功能实现提供了新的视角与工具。

Abstract: We fully characterize a large class of feedforward neural networks in terms
of truncation maps. As an application, we show how a ReLU neural network can
implement a feature map which separates concentric data.

</details>


### [89] [Appa: Bending Weather Dynamics with Latent Diffusion Models for Global Data Assimilation](https://arxiv.org/abs/2504.18720)
*Gérôme Andry,François Rozet,Sacha Lewin,Omer Rochman,Victor Mangeleer,Matthias Pirlet,Elise Faulx,Marilaure Grégoire,Gilles Louppe*

Main category: cs.LG

TL;DR: Appa是一个基于分数的数据同化模型，用于从观测数据中生成全球大气轨迹，支持多种推理任务且无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 传统天气预报需从大量观测数据中识别当前大气状态，挑战巨大。

Method: 使用1.5B参数的时空潜在扩散模型，基于ERA5再分析数据训练，可灵活适配多种观测类型。

Result: 实验证明其物理一致性和良好重建能力，同时在预报任务中表现有竞争力。

Conclusion: 基于分数的数据同化为全球大气建模系统提供了新方向。

Abstract: Deep learning has transformed weather forecasting by improving both its
accuracy and computational efficiency. However, before any forecast can begin,
weather centers must identify the current atmospheric state from vast amounts
of observational data. To address this challenging problem, we introduce Appa,
a score-based data assimilation model producing global atmospheric trajectories
at 0.25-degree resolution and 1-hour intervals. Powered by a 1.5B-parameter
spatio-temporal latent diffusion model trained on ERA5 reanalysis data, Appa
can be conditioned on any type of observations to infer the posterior
distribution of plausible state trajectories, without retraining. Our unified
probabilistic framework flexibly tackles multiple inference tasks --
reanalysis, filtering, and forecasting -- using the same model, eliminating the
need for task-specific architectures or training procedures. Experiments
demonstrate physical consistency on a global scale and good reconstructions
from observations, while showing competitive forecasting skills. Our results
establish latent score-based data assimilation as a promising foundation for
future global atmospheric modeling systems.

</details>


### [90] [Multimodal graph representation learning for website generation based on visual sketch](https://arxiv.org/abs/2504.18729)
*Tung D. Vu,Chung Hoang,Truong-Son Hy*

Main category: cs.LG

TL;DR: 该论文提出了一种多模态图表示学习方法，用于更准确高效地将设计图转换为HTML代码，相比现有技术显著提升了效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法在解析网页设计中的视觉和结构细节时存在局限性，设计到代码的转换过程复杂且耗时，亟需更高效的自动化解决方案。

Method: 利用多模态图表示学习，整合设计草图的视觉和结构信息，提升代码生成的准确性和效率。

Result: 实验表明，该方法在生成语义正确且结构合理的HTML代码方面，准确性和效率均优于现有技术。

Conclusion: 多模态图学习方法有望革新设计到代码的自动化流程，相关代码已开源。

Abstract: The Design2Code problem, which involves converting digital designs into
functional source code, is a significant challenge in software development due
to its complexity and time-consuming nature. Traditional approaches often
struggle with accurately interpreting the intricate visual details and
structural relationships inherent in webpage designs, leading to limitations in
automation and efficiency. In this paper, we propose a novel method that
leverages multimodal graph representation learning to address these challenges.
By integrating both visual and structural information from design sketches, our
approach enhances the accuracy and efficiency of code generation, particularly
in producing semantically correct and structurally sound HTML code. We present
a comprehensive evaluation of our method, demonstrating significant
improvements in both accuracy and efficiency compared to existing techniques.
Extensive evaluation demonstrates significant improvements of multimodal graph
learning over existing techniques, highlighting the potential of our method to
revolutionize design-to-code automation. Code available at
https://github.com/HySonLab/Design2Code

</details>


### [91] [TLoRA: Tri-Matrix Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2504.18735)
*Tanvir Islam*

Main category: cs.LG

TL;DR: TLoRA是一种新型的三矩阵低秩适应方法，通过分解权重更新为三个矩阵（两个固定随机矩阵和一个可训练矩阵）并结合可学习的层间缩放因子，实现了高效参数适应和低计算开销。在GLUE基准测试中，其性能与LoRA和Adapter方法相当，但训练参数更少。分析表明，TLoRA具有高斯分布权重、稳定参数范数和层间缩放因子变异性，且能有效近似LoRA的行为。


<details>
  <summary>Details</summary>
Motivation: 提出TLoRA的目的是在保证性能的前提下，进一步减少大型语言模型（LLMs）微调所需的训练参数和计算开销，以实现更高效的资源利用。

Method: TLoRA将权重更新分解为两个固定随机矩阵和一个可训练矩阵，并引入层间可学习缩放因子，通过三矩阵设计降低计算复杂度。

Result: 在GLUE基准测试中，TLoRA性能与LoRA和Adapter方法相当，但训练参数显著减少；分析显示其权重分布高斯化、参数范数稳定，且能有效近似LoRA行为。

Conclusion: TLoRA是一种高效且有效的LLM微调方法，在资源受限场景下具有显著优势，为模型适应提供了新方向。

Abstract: We propose TLoRA, a novel tri-matrix low-rank adaptation method that
decomposes weight updates into three matrices: two fixed random matrices and
one trainable matrix, combined with a learnable, layer-wise scaling factor.
This tri-matrix design enables TLoRA to achieve highly efficient parameter
adaptation while introducing minimal additional computational overhead. Through
extensive experiments on the GLUE benchmark, we demonstrate that TLoRA achieves
comparable performance to existing low-rank methods such as LoRA and
Adapter-based techniques, while requiring significantly fewer trainable
parameters. Analyzing the adaptation dynamics, we observe that TLoRA exhibits
Gaussian-like weight distributions, stable parameter norms, and scaling factor
variability across layers, further highlighting its expressive power and
adaptability. Additionally, we show that TLoRA closely resembles LoRA in its
eigenvalue distributions, parameter norms, and cosine similarity of updates,
underscoring its ability to effectively approximate LoRA's adaptation behavior.
Our results establish TLoRA as a highly efficient and effective fine-tuning
method for LLMs, offering a significant step forward in resource-efficient
model adaptation.

</details>


### [92] [Non-Asymptotic Guarantees for Average-Reward Q-Learning with Adaptive Stepsizes](https://arxiv.org/abs/2504.18743)
*Zaiwei Chen*

Main category: cs.LG

TL;DR: 该论文首次对异步实现的平均奖励Q学习的最后一轮收敛进行了有限时间分析，证明了自适应步长对收敛的重要性，并提出了O(1/k)的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决异步Q学习中自适应步长对收敛的影响，以及如何通过技术手段克服非马尔可夫性带来的分析挑战。

Method: 方法包括使用自适应步长作为局部时钟，引入居中步骤，以及结合时间非均匀马尔可夫重构和条件论证等技术手段。

Result: 结果显示算法以O(1/k)的速率收敛到最优相对Q函数，并证明了自适应步长对正确收敛的必要性。

Conclusion: 结论认为自适应步长是关键，且所开发的分析工具可广泛应用于其他具有自适应步长的随机逼近算法。

Abstract: This work presents the first finite-time analysis for the last-iterate
convergence of average-reward Q-learning with an asynchronous implementation. A
key feature of the algorithm we study is the use of adaptive stepsizes, which
serve as local clocks for each state-action pair. We show that the iterates
generated by this Q-learning algorithm converge at a rate of $O(1/k)$ (in the
mean-square sense) to the optimal relative Q-function in the span seminorm.
Moreover, by adding a centering step to the algorithm, we further establish
pointwise mean-square convergence to a centered optimal relative Q-function,
also at a rate of $O(1/k)$. To prove these results, we show that adaptive
stepsizes are necessary, as without them, the algorithm fails to converge to
the correct target. In addition, adaptive stepsizes can be interpreted as a
form of implicit importance sampling that counteracts the effects of
asynchronous updates.
  Technically, the use of adaptive stepsizes makes each Q-learning update
depend on the entire sample history, introducing strong correlations and making
the algorithm a non-Markovian stochastic approximation (SA) scheme. Our
approach to overcoming this challenge involves (1) a time-inhomogeneous
Markovian reformulation of non-Markovian SA, and (2) a combination of
almost-sure time-varying bounds, conditioning arguments, and Markov chain
concentration inequalities to break the strong correlations between the
adaptive stepsizes and the iterates. The tools developed in this work are
likely to be broadly applicable to the analysis of general SA algorithms with
adaptive stepsizes.

</details>


### [93] [High-order Graph Neural Networks with Common Neighbor Awareness for Link Prediction](https://arxiv.org/abs/2504.18758)
*Ling Wang,Minglian Han*

Main category: cs.LG

TL;DR: 该论文提出了一种高阶图神经网络HGNN-CNA，通过考虑多跳共同邻居来改进动态图中的链接预测任务，实验显示其性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有动态图神经网络主要依赖节点间的成对交互，忽略了共同邻居的作用，导致链接预测性能受限。

Method: 提出了HGNN-CNA方法，包括：（1）利用多跳共同邻居计算相关性分数；（2）将相关性融入消息传递过程，直接建模共同邻居交互。

Result: 在三个真实动态图数据集上，HGNN-CNA的链接预测准确率显著超过多个先进模型。

Conclusion: HGNN-CNA通过显式建模共同邻居交互，有效提升了动态图中的链接预测性能。

Abstract: Link prediction is a fundamental task in dynamic graph learning (DGL),
inherently shaped by the topology of the DG. Recent advancements in dynamic
graph neural networks (DGNN), primarily by modeling the relationships among
nodes via a message passing scheme, have significantly improved link prediction
performance. However, DGNNs heavily rely on the pairwise node interactions,
which neglect the common neighbor interaction in DGL. To address this
limitation, we propose a High-order Graph Neural Networks with Common Neighbor
Awareness (HGNN-CNA) for link prediction with two-fold ideas: a) estimating
correlation score by considering multi-hop common neighbors for capturing the
complex interaction between nodes; b) fusing the correlation into the
message-passing process to consider common neighbor interaction directly in
DGL. Experimental results on three real DGs demonstrate that the proposed
HGNN-CNA acquires a significant accuracy gain over several state-of-the-art
models on the link prediction task.

</details>


### [94] [Dynamic Action Interpolation: A Universal Approach for Accelerating Reinforcement Learning with Expert Guidance](https://arxiv.org/abs/2504.18766)
*Wenjun Cao*

Main category: cs.LG

TL;DR: DAI是一个简单通用的框架，通过在专家和RL动作间进行动态插值，显著提升了强化学习的初期和最终性能，且无需复杂架构修改。


<details>
  <summary>Details</summary>
Motivation: 强化学习在早期训练中样本效率低下，现有方法通常引入复杂架构。本研究的动机是通过简单方法提升样本效率而不增加复杂度。

Method: 提出了动态动作插值（DAI）框架，通过时变权重α(t)插值专家和RL动作，可无缝集成到任何Actor-Critic算法中。

Result: 在MuJoCo连续控制任务中，DAI将初期性能平均提升160%，最终性能提升50%，部分任务早期性能提升4倍。

Conclusion: DAI证明了在不增加架构复杂度的前提下，可以通过简单方法显著提升强化学习的样本效率。

Abstract: Reinforcement learning (RL) suffers from severe sample inefficiency,
especially during early training, requiring extensive environmental
interactions to perform competently. Existing methods tend to solve this by
incorporating prior knowledge, but introduce significant architectural and
implementation complexity. We propose Dynamic Action Interpolation (DAI), a
universal yet straightforward framework that interpolates expert and RL actions
via a time-varying weight $\alpha(t)$, integrating into any Actor-Critic
algorithm with just a few lines of code and without auxiliary networks or
additional losses. Our theoretical analysis shows that DAI reshapes state
visitation distributions to accelerate value function learning while preserving
convergence guarantees. Empirical evaluations across MuJoCo continuous control
tasks demonstrate that DAI improves early-stage performance by over 160\% on
average and final performance by more than 50\%, with the Humanoid task showing
a 4$\times$ improvement early on and a 2$\times$ gain at convergence. These
results challenge the assumption that complex architectural modifications are
necessary for sample-efficient reinforcement learning.

</details>


### [95] [Performance of Machine Learning Classifiers for Anomaly Detection in Cyber Security Applications](https://arxiv.org/abs/2504.18771)
*Markus Haug,Gissel Velarde*

Main category: cs.LG

TL;DR: 论文评估了多种机器学习模型在两个不平衡公开数据集（KDDCUP99和信用卡欺诈2013）上的表现，发现XGB和MLP优于生成模型，同时不推荐在大数据集上使用IterativeImputer。


<details>
  <summary>Details</summary>
Motivation: 动机是通过实证评估不同机器学习模型在不平衡数据集上的表现，为实际应用中选择合适的模型和方法提供参考。

Method: 方法包括数据准备、模型训练和评估，使用80/20的训练/测试分割，测试了XGB、MLP、GAN、VAE和MO-GAAL等模型，并结合了ROS和SPE技术。评估采用了5折交叉验证和多种缺失值填补技术。

Result: 结果表明，XGB和MLP的表现优于生成模型。IterativeImputer的填补效果与均值和中位数填补相当，但不推荐用于大数据集。

Conclusion: 结论指出XGB和MLP是不平衡数据集上的优选模型，同时建议根据数据集规模选择合适的缺失值填补方法。

Abstract: This work empirically evaluates machine learning models on two imbalanced
public datasets (KDDCUP99 and Credit Card Fraud 2013). The method includes data
preparation, model training, and evaluation, using an 80/20 (train/test) split.
Models tested include eXtreme Gradient Boosting (XGB), Multi Layer Perceptron
(MLP), Generative Adversarial Network (GAN), Variational Autoencoder (VAE), and
Multiple-Objective Generative Adversarial Active Learning (MO-GAAL), with XGB
and MLP further combined with Random-Over-Sampling (ROS) and
Self-Paced-Ensemble (SPE). Evaluation involves 5-fold cross-validation and
imputation techniques (mean, median, and IterativeImputer) with 10, 20, 30, and
50 % missing data. Findings show XGB and MLP outperform generative models.
IterativeImputer results are comparable to mean and median, but not recommended
for large datasets due to increased complexity and execution time. The code
used is publicly available on GitHub (github.com/markushaug/acr-25).

</details>


### [96] [ALF: Advertiser Large Foundation Model for Multi-Modal Advertiser Understanding](https://arxiv.org/abs/2504.18785)
*Santosh Rajagopalan,Jonathan Vronsky,Songbai Yan,S. Alireza Golestaneh,Shubhra Chandra,Min Zhou*

Main category: cs.LG

TL;DR: ALF（广告主大型基础模型）是一个多模态Transformer架构，通过对比学习和多任务优化，统一捕捉广告主的内容和行为模式，在欺诈检测、政策违规识别等任务上表现优异，实际部署中减少了90%的误报。


<details>
  <summary>Details</summary>
Motivation: 广告主行为的复杂性需要一种能够跨多模态数据（文本、图像、视频等）理解其意图和模式的统一模型，以提升广告生态的安全性和效率。

Method: 采用多模态Transformer架构，结合对比学习和多任务优化，引入跨样本注意力机制、谱归一化投影和校准概率输出等创新技术。

Result: 在欺诈检测和政策违规识别等任务上达到最先进性能，实际部署中误报减少90%，同时保持99.8%的精确率。

Conclusion: ALF通过其多模态统一表示和高效优化方法，显著提升了广告主行为理解的准确性和实际应用效果。

Abstract: We present ALF (Advertiser Large Foundation model), a multi-modal transformer
architecture for understanding advertiser behavior and intent across text,
image, video and structured data modalities. Through contrastive learning and
multi-task optimization, ALF creates unified advertiser representations that
capture both content and behavioral patterns. Our model achieves
state-of-the-art performance on critical tasks including fraud detection,
policy violation identification, and advertiser similarity matching. In
production deployment, ALF reduces false positives by 90% while maintaining
99.8% precision on abuse detection tasks. The architecture's effectiveness
stems from its novel combination of multi-modal transformations, inter-sample
attention mechanism, spectrally normalized projections, and calibrated
probabilistic outputs.

</details>


### [97] [Frequency-Integrated Transformer for Arbitrary-Scale Super-Resolution](https://arxiv.org/abs/2504.18818)
*Xufei Wang,Fei Ge,Jinchen Zhu,Mingjian Zhang,Qi Wu,Jifeng Ren Shizhuang Weng*

Main category: cs.LG

TL;DR: 论文提出了一种名为FIT的新网络，通过结合频域信息来提升任意尺度超分辨率任务的性能，利用FIM和FUSAM模块分别引入和高效利用频域信息。实验证明FIT在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于隐式神经表示的方法在任意尺度超分辨率任务中表现优异，但忽略了频域信息的潜在价值，导致性能未达最优。

Method: 提出FIT网络，包含FIM模块（通过FFT和实虚映射无损引入频域信息）和FUSAM模块（利用IISA实现空间-频域信息交互，FCSA捕获全局频率相关性）。

Result: 实验显示FIT在多个数据集上性能领先，FIM增强细节表征，IISA提升频率保真度，FCSA有效捕获全局上下文。

Conclusion: 频域信息的引入和高效利用显著提升了超分辨率任务的性能，FIT网络在细节和全局上下文表现上均有优势。

Abstract: Methods based on implicit neural representation have demonstrated remarkable
capabilities in arbitrary-scale super-resolution (ASSR) tasks, but they neglect
the potential value of the frequency domain, leading to sub-optimal
performance. We proposes a novel network called Frequency-Integrated
Transformer (FIT) to incorporate and utilize frequency information to enhance
ASSR performance. FIT employs Frequency Incorporation Module (FIM) to introduce
frequency information in a lossless manner and Frequency Utilization
Self-Attention module (FUSAM) to efficiently leverage frequency information by
exploiting spatial-frequency interrelationship and global nature of frequency.
FIM enriches detail characterization by incorporating frequency information
through a combination of Fast Fourier Transform (FFT) with real-imaginary
mapping. In FUSAM, Interaction Implicit Self-Attention (IISA) achieves
cross-domain information synergy by interacting spatial and frequency
information in subspace, while Frequency Correlation Self-attention (FCSA)
captures the global context by computing correlation in frequency. Experimental
results demonstrate FIT yields superior performance compared to existing
methods across multiple benchmark datasets. Visual feature map proves the
superiority of FIM in enriching detail characterization. Frequency error map
validates IISA productively improve the frequency fidelity. Local attribution
map validates FCSA effectively captures global context.

</details>


### [98] [Preserving Seasonal and Trend Information: A Variational Autoencoder-Latent Space Arithmetic Based Approach for Non-stationary Learning](https://arxiv.org/abs/2504.18819)
*Hassan Wasswa,Aziida Nanyonga,Timothy Lynar*

Main category: cs.LG

TL;DR: 该论文提出一种在变分自编码器（VAE）潜在空间中保留趋势和季节性信息的方法，通过差分、时间序列分解和潜在空间算术（LSA）技术，解决了传统方法因使数据平稳而丢失关键信息的局限性。实验表明，该方法在非平稳时间序列数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型通常假设数据是平稳的，但在非平稳数据中表现不佳。现有解决方案通过使数据平稳化处理，却丢失了趋势和季节性信息。本研究旨在在潜在空间中强制平稳行为，同时保留这些关键信息。

Method: 通过结合差分、时间序列分解和潜在空间算术（LSA），学习并存储趋势和季节性信息为变分自编码器（VAE）潜在空间中的嵌入向量。

Result: 在两个非平稳时间序列数据集上验证了该方法能有效保留趋势和季节性信息，且四个深度学习模型在其潜在向量表示上均取得与最先进技术相当的RMSE性能。

Conclusion: 该方法成功解决了非平稳数据建模中信息丢失的问题，为时间序列预测提供了更高效的工具。

Abstract: AI models have garnered significant research attention towards predictive
task automation. However, a stationary training environment is an underlying
assumption for most models and such models simply do not work on non-stationary
data since a stationary relationship is learned. The existing solutions propose
making data stationary prior to model training and evaluation. This leads to
loss of trend and seasonal patterns which are vital components for learning
temporal dependencies of the system under study. This research aims to address
this limitation by proposing a method for enforcing stationary behaviour within
the latent space while preserving trend and seasonal information. The method
deploys techniques including Differencing, Time-series decomposition, and
Latent Space Arithmetic (LSA), to learn information vital for efficient
approximation of trend and seasonal information which is then stored as
embeddings within the latent space of a Variational Autoencoder (VAE). The
approach's ability to preserve trend and seasonal information was evaluated on
two time-series non-stationary datasets. For predictive performance evaluation,
four deep learning models were trained on the latent vector representations of
the datasets after application of the proposed method and all models produced
competitive results in comparison with state-of-the-art techniques using RMSE
as the performance metric.

</details>


### [99] [Introducing Interval Neural Networks for Uncertainty-Aware System Identification](https://arxiv.org/abs/2504.18845)
*Mehmet Ali Ferah,Tufan Kumbasar*

Main category: cs.LG

TL;DR: 该论文提出了一种基于区间神经网络（INNs）的系统性框架，用于在系统识别（SysID）任务中进行不确定性量化（UQ）。通过将预训练神经网络的参数转换为区间值参数，并结合区间算术，INNs能够有效生成预测区间。


<details>
  <summary>Details</summary>
Motivation: 传统SysID方法难以充分捕捉非线性动态，而深度学习（DL）模型缺乏不确定性量化，限制了其可靠性和安全性。因此，需要一种能够结合非线性建模能力和UQ的方法。

Method: 提出Interval Neural Networks（INNs），通过将预训练神经网络的参数转换为区间值参数，利用区间算术生成预测区间。扩展了LSTM和Neural ODEs为Interval LSTM（ILSTM）和Interval NODE（INODE），并设计了包含UQ损失函数的训练框架。

Result: 实验验证表明，ILSTM和INODE在SysID任务中表现优异，能够有效捕捉目标覆盖范围并提供可靠的不确定性量化结果。

Conclusion: INNs为SysID任务提供了一种无需概率假设的不确定性量化方法，扩展了DL模型在动态系统建模中的应用潜力。

Abstract: System Identification (SysID) is crucial for modeling and understanding
dynamical systems using experimental data. While traditional SysID methods
emphasize linear models, their inability to fully capture nonlinear dynamics
has driven the adoption of Deep Learning (DL) as a more powerful alternative.
However, the lack of uncertainty quantification (UQ) in DL-based models poses
challenges for reliability and safety, highlighting the necessity of
incorporating UQ. This paper introduces a systematic framework for constructing
and learning Interval Neural Networks (INNs) to perform UQ in SysID tasks. INNs
are derived by transforming the learnable parameters (LPs) of pre-trained
neural networks into interval-valued LPs without relying on probabilistic
assumptions. By employing interval arithmetic throughout the network, INNs can
generate Prediction Intervals (PIs) that capture target coverage effectively.
We extend Long Short-Term Memory (LSTM) and Neural Ordinary Differential
Equations (Neural ODEs) into Interval LSTM (ILSTM) and Interval NODE (INODE)
architectures, providing the mathematical foundations for their application in
SysID. To train INNs, we propose a DL framework that integrates a UQ loss
function and parameterization tricks to handle constraints arising from
interval LPs. We introduce novel concept "elasticity" for underlying
uncertainty causes and validate ILSTM and INODE in SysID experiments,
demonstrating their effectiveness.

</details>


### [100] [Theoretical Framework for Tempered Fractional Gradient Descent: Application to Breast Cancer Classification](https://arxiv.org/abs/2504.18849)
*Omar Naifar*

Main category: cs.LG

TL;DR: 论文提出TFGD优化框架，结合分数阶微积分和指数缓和技术，提升高维噪声环境中的梯度学习效果，理论证明其收敛性，并在医学数据集上取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 传统梯度下降在高维噪声环境中表现不佳（如振荡更新、收敛慢），TFGD通过结合分数阶梯度与指数缓和技术（利用历史梯度加权和衰减）解决这些问题。

Method: TFGD引入缓存的记忆机制，通过分数阶系数加权历史梯度，并使用指数衰减参数λ调整权重。理论分析证明其收敛性（凸场景O(1/K)，随机变体O(1/k^α)），并保持与SGD相同的O(n)时间复杂度。

Result: 实验验证TFGD在威斯康星乳腺癌数据集上表现优异：测试准确率98.25%（SGD为92.11%），收敛速度快2倍，尤其擅长处理医学分类任务的特征相关性。

Conclusion: TFGD在理论和应用上均优于传统优化器，成为高维噪声环境下梯度学习的有效替代方案。

Abstract: This paper introduces Tempered Fractional Gradient Descent (TFGD), a novel
optimization framework that synergizes fractional calculus with exponential
tempering to enhance gradient-based learning. Traditional gradient descent
methods often suffer from oscillatory updates and slow convergence in
high-dimensional, noisy landscapes. TFGD addresses these limitations by
incorporating a tempered memory mechanism, where historical gradients are
weighted by fractional coefficients $|w_j| = \binom{\alpha}{j}$ and
exponentially decayed via a tempering parameter $\lambda$. Theoretical analysis
establishes TFGD's convergence guarantees: in convex settings, it achieves an
$\mathcal{O}(1/K)$ rate with alignment coefficient $d_{\alpha,\lambda} = (1 -
e^{-\lambda})^{-\alpha}$, while stochastic variants attain
$\mathcal{O}(1/k^\alpha)$ error decay. The algorithm maintains $\mathcal{O}(n)$
time complexity equivalent to SGD, with memory overhead scaling as
$\mathcal{O}(d/\lambda)$ for parameter dimension $d$. Empirical validation on
the Breast Cancer Wisconsin dataset demonstrates TFGD's superiority, achieving
98.25\% test accuracy (vs. 92.11\% for SGD) and 2$\times$ faster convergence.
The tempered memory mechanism proves particularly effective in medical
classification tasks, where feature correlations benefit from stable gradient
averaging. These results position TFGD as a robust alternative to conventional
optimizers in both theoretical and applied machine learning.

</details>


### [101] [TSRM: A Lightweight Temporal Feature Encoding Architecture for Time Series Forecasting and Imputation](https://arxiv.org/abs/2504.18878)
*Robert Leppich,Michael Stenger,Daniel Grillmeyer,Vanessa Borst,Samuel Kounev*

Main category: cs.LG

TL;DR: TSRM是一种基于CNN和注意力机制的多变量时间序列预测与填补架构，性能优于现有方法且参数更少。


<details>
  <summary>Details</summary>
Motivation: 解决多变量时间序列预测与填补中高效捕捉多样时序模式的挑战。

Method: 结合CNN表示层、注意力特征提取层和合并层，受Transformer编码器启发。

Result: 在7个基准数据集上表现优于现有方法，同时显著减少可学习参数。

Conclusion: TSRM通过高效架构设计，在性能与复杂度间取得平衡，适用于时间序列任务。

Abstract: We introduce a temporal feature encoding architecture called Time Series
Representation Model (TSRM) for multivariate time series forecasting and
imputation. The architecture is structured around CNN-based representation
layers, each dedicated to an independent representation learning task and
designed to capture diverse temporal patterns, followed by an attention-based
feature extraction layer and a merge layer, designed to aggregate extracted
features. The architecture is fundamentally based on a configuration that is
inspired by a Transformer encoder, with self-attention mechanisms at its core.
The TSRM architecture outperforms state-of-the-art approaches on most of the
seven established benchmark datasets considered in our empirical evaluation for
both forecasting and imputation tasks. At the same time, it significantly
reduces complexity in the form of learnable parameters. The source code is
available at https://github.com/RobertLeppich/TSRM.

</details>


### [102] [TSCAN: Context-Aware Uplift Modeling via Two-Stage Training for Online Merchant Business Diagnosis](https://arxiv.org/abs/2504.18881)
*Hangtao Zhang,Zhe Li,Kairui Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种基于两阶段训练方法的上下文感知提升模型（TSCAN），包含CAN-U和CAN-D子模型，用于解决样本选择偏差问题并通过上下文交互提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用IPM、重加权和倾向得分建模等处理正则化技术来缓解样本选择偏差，但这些方法可能导致信息损失并限制模型性能，且无法充分利用上下文特征。

Method: TSCAN采用两阶段训练方法：第一阶段训练CAN-U模型生成反事实提升标签；第二阶段训练CAN-D模型直接建模提升效果，无需依赖正则化组件，并通过上下文感知注意力层处理交互特征。

Result: 在两个真实数据集上的实验验证了TSCAN的有效性，并在中国最大的在线食品订购平台上展示了其实际应用价值。

Conclusion: TSCAN通过动态校正和上下文交互显著提升了模型性能，同时避免了传统正则化方法的负面影响，具有实际应用潜力。

Abstract: A primary challenge in ITE estimation is sample selection bias. Traditional
approaches utilize treatment regularization techniques such as the Integral
Probability Metrics (IPM), re-weighting, and propensity score modeling to
mitigate this bias. However, these regularizations may introduce undesirable
information loss and limit the performance of the model. Furthermore, treatment
effects vary across different external contexts, and the existing methods are
insufficient in fully interacting with and utilizing these contextual features.
To address these issues, we propose a Context-Aware uplift model based on the
Two-Stage training approach (TSCAN), comprising CAN-U and CAN-D sub-models. In
the first stage, we train an uplift model, called CAN-U, which includes the
treatment regularizations of IPM and propensity score prediction, to generate a
complete dataset with counterfactual uplift labels. In the second stage, we
train a model named CAN-D, which utilizes an isotonic output layer to directly
model uplift effects, thereby eliminating the reliance on the regularization
components. CAN-D adaptively corrects the errors estimated by CAN-U through
reinforcing the factual samples, while avoiding the negative impacts associated
with the aforementioned regularizations. Additionally, we introduce a
Context-Aware Attention Layer throughout the two-stage process to manage the
interactions between treatment, merchant, and contextual features, thereby
modeling the varying treatment effect in different contexts. We conduct
extensive experiments on two real-world datasets to validate the effectiveness
of TSCAN. Ultimately, the deployment of our model for real-world merchant
diagnosis on one of China's largest online food ordering platforms validates
its practical utility and impact.

</details>


### [103] [SPD Learning for Covariance-Based Neuroimaging Analysis: Perspectives, Methods, and Challenges](https://arxiv.org/abs/2504.18882)
*Ce Ju,Reinmar J. Kobler,Antoine Collas,Motoaki Kawanabe,Cuntai Guan,Bertrand Thirion*

Main category: cs.LG

TL;DR: 该论文综述了基于协方差的神经影像数据机器学习方法，利用Riemannian几何分析SPD矩阵空间以提升脑成像分析。


<details>
  <summary>Details</summary>
Motivation: 解决神经影像数据中的低信噪比、跨会话非平稳性和样本量有限等问题，通过几何方法优化任务特异性特征的解码。

Method: 采用Riemannian度量（如仿射不变或对数欧几里得）在SPD矩阵空间进行几何分析，形成SPD学习框架。

Result: 通过系统利用SPD流形的几何特性，改进了协方差特征的处理，推动了脑成像分析的发展。

Conclusion: SPD学习框架为神经影像数据的机器学习提供了统一的几何分析方法，显著提升了任务解码的准确性和鲁棒性。

Abstract: Neuroimaging provides a critical framework for characterizing brain activity
by quantifying connectivity patterns and functional architecture across
modalities. While modern machine learning has significantly advanced our
understanding of neural processing mechanisms through these datasets, decoding
task-specific signatures must contend with inherent neuroimaging constraints,
for example, low signal-to-noise ratios in raw electrophysiological recordings,
cross-session non-stationarity, and limited sample sizes. This review focuses
on machine learning approaches for covariance-based neuroimaging data, where
often symmetric positive definite (SPD) matrices under full-rank conditions
encode inter-channel relationships. By equipping the space of SPD matrices with
Riemannian metrics (e.g., affine-invariant or log-Euclidean), their space forms
a Riemannian manifold enabling geometric analysis. We unify methodologies
operating on this manifold under the SPD learning framework, which
systematically leverages the SPD manifold's geometry to process covariance
features, thereby advancing brain imaging analytics.

</details>


### [104] [Factor Analysis with Correlated Topic Model for Multi-Modal Data](https://arxiv.org/abs/2504.18914)
*Małgorzata Łazęcka,Ewa Szczurek*

Main category: cs.LG

TL;DR: 该论文提出了FACTM模型，结合因子分析和相关主题建模，以处理结构化数据模态，并通过变分推断优化，实验表明其在识别聚类和整合多模态数据方面优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态因子分析（FA）无法有效处理结构化数据（如文本或单细胞测序数据），因为这些数据具有聚类结构。因此，需要一种新方法来整合这些数据模态并提取可解释的共享因子。

Method: FACTM是一种多视图、多结构的贝叶斯模型，结合了因子分析和相关主题建模，使用变分推断进行优化，并引入旋转潜在因子的方法以提高可解释性。

Result: 在文本、视频基准以及音乐和COVID-19的真实数据集上，FACTM在识别结构化数据中的聚类和多模态数据整合方面表现优于其他方法。

Conclusion: FACTM是一种有效的多模态数据整合方法，特别适合处理结构化数据，并能提取可解释的共享因子，为多模态分析提供了新思路。

Abstract: Integrating various data modalities brings valuable insights into underlying
phenomena. Multimodal factor analysis (FA) uncovers shared axes of variation
underlying different simple data modalities, where each sample is represented
by a vector of features. However, FA is not suited for structured data
modalities, such as text or single cell sequencing data, where multiple data
points are measured per each sample and exhibit a clustering structure. To
overcome this challenge, we introduce FACTM, a novel, multi-view and
multi-structure Bayesian model that combines FA with correlated topic modeling
and is optimized using variational inference. Additionally, we introduce a
method for rotating latent factors to enhance interpretability with respect to
binary features. On text and video benchmarks as well as real-world music and
COVID-19 datasets, we demonstrate that FACTM outperforms other methods in
identifying clusters in structured data, and integrating them with simple
modalities via the inference of shared, interpretable factors.

</details>


### [105] [Revisiting Transformers through the Lens of Low Entropy and Dynamic Sparsity](https://arxiv.org/abs/2504.18929)
*Ruifeng Ren,Yong Liu*

Main category: cs.LG

TL;DR: Transformers表现出独特的归纳偏好在数据压缩中，倾向于学习更低熵的分布，且越大的模型越明显。这一偏好阻止了其与目标分布的完美对齐，同时FFN模块在这一偏好的驱动中起关键作用。此外，模型参数的冗余性通过动态稀疏性体现，且训练不稳定性与死神经元的突然增加相关。


<details>
  <summary>Details</summary>
Motivation: 研究Transformers在数据压缩中的表现，特别是其压缩性能与目标分布的关系，以及模型参数冗余性的动态稀疏性表现。

Method: 在控制实验设置下，分析Transformers的压缩行为，包括熵的偏好、FFN模块的作用，以及动态稀疏性模式。

Result: 发现Transformers倾向于学习更低熵的分布，FFN模块驱动这一偏好。模型参数冗余性以动态稀疏性为特征，训练不稳定性与死神经元增加相关。

Conclusion: 研究深化了对Transformers在熵和动态稀疏性方面的理解，揭示了其压缩行为和参数冗余性的关键特性。

Abstract: Compression has been a critical lens to understand the success of
Transformers. In the past, we have typically taken the target distribution as a
criterion to evaluate a model's compression performance. Nevertheless,it often
remains challenging to precisely assess how well the model achieves compression
and to compare the information content of the learned distribution with that of
the target distribution during compression,as the target distribution is
typically unknown and entropy computation often incurs exponential cost. In
this work, we explore these issues under a controlled experimental setup. We
find that Transformers exhibit a unique inductive bias in data compression:
beyond approaching the target distribution, they tend to favor learning
lower-entropy distributions, with this tendency becoming more pronounced as the
model size increases. This preference prevents Transformers from perfectly
aligning with the target distribution, instead further compressing its
information content. Furthermore, we show that the FFN module plays a critical
role in driving this bias. In addition, while models remove informational
redundancy from data during compression, they also exhibit redundancy within
their parameters, which enables compression and can be characterized through
dynamic sparsity. However, the dynamic sparsity patterns in Transformers,
particularly in attention and FFN modules, demand further exploration. As for
this, we show that larger Transformers show stronger preferences for bypassing
attention computations via residual connections and have lower proportion of
active neurons. Interestingly, we also find that training instability in larger
models strongly correlates with sudden increases in dead neurons. Our work
contributes to a deeper understanding of Transformers from the lens of entropy
and dynamic sparsity.

</details>


### [106] [Unveiling and Mitigating Adversarial Vulnerabilities in Iterative Optimizers](https://arxiv.org/abs/2504.19000)
*Elad Sofer,Tomer Shaked,Caroline Chaux,Nir Shlezinger*

Main category: cs.LG

TL;DR: 该研究探讨了非学习的迭代优化器对对抗样本的敏感性，发现其与机器学习模型类似。通过将迭代优化器视为机器学习模型，研究者提出通过对抗训练增强鲁棒性，并在数学上证明了这一方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 本文动机源于机器学习模型对对抗样本的敏感性，研究者希望探讨非学习的迭代优化器是否也存在类似问题，并探索可能的解决方案。

Method: 研究者将迭代优化器（特别是近端梯度优化器）视为机器学习模型进行对抗分析，提出通过展开（unfolding）和对抗训练来增强鲁棒性。

Result: 数值实验表明，多种优化器对对抗样本同样敏感，而通过展开和对抗训练显著提高了其鲁棒性。数学分析也证明了学习过程对对抗敏感性的影响。

Conclusion: 研究表明，对抗敏感性并非机器学习模型独有，迭代优化器也存在这一问题。通过对抗训练，可以有效提升优化器的鲁棒性。

Abstract: Machine learning (ML) models are often sensitive to carefully crafted yet
seemingly unnoticeable perturbations. Such adversarial examples are considered
to be a property of ML models, often associated with their black-box operation
and sensitivity to features learned from data. This work examines the
adversarial sensitivity of non-learned decision rules, and particularly of
iterative optimizers. Our analysis is inspired by the recent developments in
deep unfolding, which cast such optimizers as ML models. We show that
non-learned iterative optimizers share the sensitivity to adversarial examples
of ML models, and that attacking iterative optimizers effectively alters the
optimization objective surface in a manner that modifies the minima sought. We
then leverage the ability to cast iteration-limited optimizers as ML models to
enhance robustness via adversarial training. For a class of proximal gradient
optimizers, we rigorously prove how their learning affects adversarial
sensitivity. We numerically back our findings, showing the vulnerability of
various optimizers, as well as the robustness induced by unfolding and
adversarial training.

</details>


### [107] [Deep Learning-Based Multi-Modal Fusion for Robust Robot Perception and Navigation](https://arxiv.org/abs/2504.19002)
*Delun Lai,Yeyubei Zhang,Yunchong Liu,Chaojie Li,Huadong Mo*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于深度学习的多模态融合架构，通过RGB图像和LiDAR数据的有效结合，提升了自主导航机器人在复杂环境中的感知能力。创新点包括轻量级特征提取网络、自适应跨模态融合策略及时序信息建模。实验结果在KITTI数据集上显示导航与定位精度分别提升了3.5%和2.2%。


<details>
  <summary>Details</summary>
Motivation: 为提高自主导航机器人在复杂环境中的感知能力，解决现有方法在多模态数据融合和动态场景感知上的不足。

Method: 设计轻量级特征提取网络以增强特征表征，开发自适应加权跨模态融合策略提升鲁棒性，并引入时序信息建模优化动态场景感知。

Result: 在KITTI数据集上实验表明，导航和定位精度分别提升3.5%和2.2%，同时保持实时性能。

Conclusion: 该研究为复杂环境中的自主机器人导航提供了一种高效且创新的解决方案。

Abstract: This paper introduces a novel deep learning-based multimodal fusion
architecture aimed at enhancing the perception capabilities of autonomous
navigation robots in complex environments. By utilizing innovative feature
extraction modules, adaptive fusion strategies, and time-series modeling
mechanisms, the system effectively integrates RGB images and LiDAR data. The
key contributions of this work are as follows: a. the design of a lightweight
feature extraction network to enhance feature representation; b. the
development of an adaptive weighted cross-modal fusion strategy to improve
system robustness; and c. the incorporation of time-series information modeling
to boost dynamic scene perception accuracy. Experimental results on the KITTI
dataset demonstrate that the proposed approach increases navigation and
positioning accuracy by 3.5% and 2.2%, respectively, while maintaining
real-time performance. This work provides a novel solution for autonomous robot
navigation in complex environments.

</details>


### [108] [\$PINN -- a Domain Decomposition Method for Bayesian Physics-Informed Neural Networks](https://arxiv.org/abs/2504.19013)
*Júlia Vicens Figueres,Juliette Vanderhaeghen,Federica Bragone,Kateryna Morozovska,Khemraj Shukla*

Main category: cs.LG

TL;DR: 提出了一种名为$PINN的新方法，通过结合局部贝叶斯物理信息神经网络和域分解，高效计算PDE中的全局不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以在大规模多尺度问题中有效量化PDE的不确定性。

Method: 结合BPINN和域分解，利用通量连续性实现子域解的连续性，并在1D和2D空间域中进行计算实验。

Result: 方法能高效恢复全局不确定性，且在训练数据中添加15%随机噪声后仍保持鲁棒性。

Conclusion: $PINN能有效计算PDE全局不确定性，并适用于不同域分解技术。

Abstract: Physics-Informed Neural Networks (PINNs) are a novel computational approach
for solving partial differential equations (PDEs) with noisy and sparse initial
and boundary data. Although, efficient quantification of epistemic and
aleatoric uncertainties in big multi-scale problems remains challenging. We
propose \$PINN a novel method of computing global uncertainty in PDEs using a
Bayesian framework, by combining local Bayesian Physics-Informed Neural
Networks (BPINN) with domain decomposition. The solution continuity across
subdomains is obtained by imposing the flux continuity across the interface of
neighboring subdomains. To demonstrate the effectiveness of \$PINN, we conduct
a series of computational experiments on PDEs in 1D and 2D spatial domains.
Although we have adopted conservative PINNs (cPINNs), the method can be
seamlessly extended to other domain decomposition techniques. The results infer
that the proposed method recovers the global uncertainty by computing the local
uncertainty exactly more efficiently as the uncertainty in each subdomain can
be computed concurrently. The robustness of \$PINN is verified by adding
uncorrelated random noise to the training data up to 15% and testing for
different domain sizes.

</details>


### [109] [Towards minimax optimal algorithms for Active Simple Hypothesis Testing](https://arxiv.org/abs/2504.19014)
*Sushant Vijayan*

Main category: cs.LG

TL;DR: 该论文研究了主动简单假设检验（ASHT）问题，提出了一种基于博弈论和偏微分方程（PDE）的近似最优算法，并通过Blackwell Approachability提出了一种更高效的计算方法。


<details>
  <summary>Details</summary>
Motivation: ASHT问题是固定预算最佳臂识别问题的简化版本，传统方法存在计算复杂度和维度问题，需要更高效的解决方案。

Method: 通过博弈论和偏微分方程（PDE）提出近似最优算法，并利用Blackwell Approachability设计更高效的计算方法。

Result: 新算法在所有ASHT实例中均优于静态算法，并在多个实例中达到了最优指数。

Conclusion: 论文提出的算法在计算效率和性能上优于现有方法，尽管未证明其最优性，但在实际应用中表现优异。

Abstract: We study the Active Simple Hypothesis Testing (ASHT) problem, a simpler
variant of the Fixed Budget Best Arm Identification problem. In this work, we
provide novel game theoretic formulation of the upper bounds of the ASHT
problem. This formulation allows us to leverage tools of differential games and
Partial Differential Equations (PDEs) to propose an approximately optimal
algorithm that is computationally tractable compared to prior work. However,
the optimal algorithm still suffers from a curse of dimensionality and instead
we use a novel link to Blackwell Approachability to propose an algorithm that
is far more efficient computationally. We show that this new algorithm,
although not proven to be optimal, is always better than static algorithms in
all instances of ASHT and is numerically observed to attain the optimal
exponent in various instances.

</details>


### [110] [Smooth Approximations of the Rounding Function](https://arxiv.org/abs/2504.19026)
*Stanislav Semenov*

Main category: cs.LG

TL;DR: 该论文提出了两种新的平滑近似方法，替代经典的四舍五入函数，适用于可微优化和机器学习。这些方法基于局部化Sigmoid窗口函数和归一化加权Sigmoid导数，能够平滑逼近整数，同时保持计算效率和可微性。


<details>
  <summary>Details</summary>
Motivation: 经典的四舍五入函数不可微，限制了其在依赖梯度的优化和机器学习中的应用。因此，需要开发可微的近似方法，以支持这些应用场景。

Method: 论文提出了两种方法：1）基于局部化Sigmoid窗口函数的差异逼近；2）基于归一化加权Sigmoid导数的密度加权插值。两种方法均通过调整锐度参数控制平滑性与逼近精度。

Result: 这两种方法在锐度参数趋近无穷大时点态收敛于经典四舍五入函数，且通过限制邻近整数的求和范围，保持了计算效率与精度。

Conclusion: 这些可微的近似方法为依赖梯度的优化问题提供了有效的解决方案，弥补了传统四舍五入函数的不足。

Abstract: We propose novel smooth approximations to the classical rounding function,
suitable for differentiable optimization and machine learning applications. Our
constructions are based on two approaches: (1) localized sigmoid window
functions centered at each integer, and (2) normalized weighted sums of sigmoid
derivatives representing local densities. The first method approximates the
step-like behavior of rounding through differences of shifted sigmoids, while
the second method achieves smooth interpolation between integers via
density-based weighting. Both methods converge pointwise to the classical
rounding function as the sharpness parameter k tends to infinity, and allow
controlled trade-offs between smoothness and approximation accuracy. We
demonstrate that by restricting the summation to a small set of nearest
integers, the computational cost remains low without sacrificing precision.
These constructions provide fully differentiable alternatives to hard rounding,
which are valuable in contexts where gradient-based methods are essential.

</details>


### [111] [On learning functions over biological sequence space: relating Gaussian process priors, regularization, and gauge fixing](https://arxiv.org/abs/2504.19034)
*Samantha Petti,Carlos Martí-Gómez,Justin B. Kinney,Juannan Zhou,David M. McCandlish*

Main category: cs.LG

TL;DR: 该论文探讨了生物序列（DNA、RNA、蛋白质）与功能量化指标之间的映射关系，重点研究了预测性序列-功能映射的推断及其子序列贡献的分解方法。


<details>
  <summary>Details</summary>
Motivation: 研究的主要动机是通过序列-功能映射的推断和分解，更好地理解生物序列的功能性表现及其子序列的贡献。

Method: 论文采用$L_2$正则化回归和高斯过程方法，分析了权值空间和函数空间的关系，并构建了对应任意高斯过程先验的规则化器。

Result: 研究表明，即使对于长序列，利用核技巧也能高效计算高斯过程后验所隐含的权值分布，并且框架能够统一和扩展序列-功能关系的推断和解释能力。

Conclusion: 该框架不仅整合了现有方法，还进一步扩展了序列-功能关系的研究能力，为生物序列的功能性解读提供了新的理论工具。

Abstract: Mappings from biological sequences (DNA, RNA, protein) to quantitative
measures of sequence functionality play an important role in contemporary
biology. We are interested in the related tasks of (i) inferring predictive
sequence-to-function maps and (ii) decomposing sequence-function maps to
elucidate the contributions of individual subsequences. Because each
sequence-function map can be written as a weighted sum over subsequences in
multiple ways, meaningfully interpreting these weights requires "gauge-fixing,"
i.e., defining a unique representation for each map. Recent work has
established that most existing gauge-fixed representations arise as the unique
solutions to $L_2$-regularized regression in an overparameterized "weight
space" where the choice of regularizer defines the gauge. Here, we establish
the relationship between regularized regression in overparameterized weight
space and Gaussian process approaches that operate in "function space," i.e.
the space of all real-valued functions on a finite set of sequences. We
disentangle how weight space regularizers both impose an implicit prior on the
learned function and restrict the optimal weights to a particular gauge. We
also show how to construct regularizers that correspond to arbitrary explicit
Gaussian process priors combined with a wide variety of gauges. Next, we derive
the distribution of gauge-fixed weights implied by the Gaussian process
posterior and demonstrate that even for long sequences this distribution can be
efficiently computed for product-kernel priors using a kernel trick. Finally,
we characterize the implicit function space priors associated with the most
common weight space regularizers. Overall, our framework unifies and extends
our ability to infer and interpret sequence-function relationships.

</details>


### [112] [Atlantes: A system of GPS transformers for global-scale real-time maritime intelligence](https://arxiv.org/abs/2504.19036)
*Henry Herzog,Joshua Hansen,Yawen Zhang,Patrick Beukema*

Main category: cs.LG

TL;DR: 论文介绍了一个名为Atlantes的深度学习系统，首次实现全球范围内船舶行为的实时监控，帮助治理和未来政策制定。


<details>
  <summary>Details</summary>
Motivation: 全球变暖和不合理的海洋开发正威胁沿海社区，需要准确、及时的航运监控来支持有效治理。

Method: 利用深度学习（特别是定制化Transformer模型）处理大量船舶GPS数据，实时量化船舶行为。

Result: Atlantes实现了低延迟、高性能的实时监控，已被全球数百个组织使用。

Conclusion: 该系统为全球海洋治理提供了高效、经济的解决方案。

Abstract: Unsustainable exploitation of the oceans exacerbated by global warming is
threatening coastal communities worldwide. Accurate and timely monitoring of
maritime activity is an essential step to effective governance and to inform
future policy. In support of this complex global-scale effort, we built
Atlantes, a deep learning based system that provides the first-ever real-time
view of vessel behavior at global scale. Atlantes leverages a series of bespoke
transformers to distill a high volume, continuous stream of GPS messages
emitted by hundreds of thousands of vessels into easily quantifiable behaviors.
The combination of low latency and high performance enables operationally
relevant decision-making and successful interventions on the high seas where
illegal and exploitative activity is too common. Atlantes is already in use by
hundreds of organizations worldwide. Here we provide an overview of the model
and infrastructure that enables this system to function efficiently and
cost-effectively at global-scale and in real-time.

</details>


### [113] [Improved Molecular Generation through Attribute-Driven Integrative Embeddings and GAN Selectivity](https://arxiv.org/abs/2504.19040)
*Nandan Joshi,Erhan Guven*

Main category: cs.LG

TL;DR: 该论文提出了一种结合Transformer和修改版GAN的分子生成方法，用于设计具有特定性质的分子。通过新型分子描述符和改进的GAN损失函数，实现了高精度和定向生成。


<details>
  <summary>Details</summary>
Motivation: 随着药物发现和化学工程等领域对定制分子的需求增长，需要更高效的分子设计计算方法。本文旨在通过机器学习方法实现定向分子生成。

Method: 结合Transformer生成的向量嵌入（整合Morgan指纹和全局分子属性）和改进的GAN（修改损失函数），生成具有目标性质的分子。

Result: Transformer的SMILES字符串重构准确率达到94%，改进的GAN成功生成了全部具有目标气味性质的分子。

Conclusion: 该方法展示了新型嵌入与改进GAN结合在分子设计中的潜力，为定向分子发现提供了高效工具。

Abstract: The growing demand for molecules with tailored properties in fields such as
drug discovery and chemical engineering has driven advancements in
computational methods for molecular design. Machine learning-based approaches
for de-novo molecular generation have recently garnered significant attention.
This paper introduces a transformer-based vector embedding generator combined
with a modified Generative Adversarial Network (GAN) to generate molecules with
desired properties. The embedding generator utilizes a novel molecular
descriptor, integrating Morgan fingerprints with global molecular attributes,
enabling the transformer to capture local functional groups and broader
molecular characteristics. Modifying the GAN generator loss function ensures
the generation of molecules with specific desired properties. The transformer
achieves a reconversion accuracy of 94% while translating molecular descriptors
back to SMILES strings, validating the utility of the proposed embeddings for
generative tasks. The approach is validated by generating novel odorant
molecules using a labeled dataset of odorant and non-odorant compounds. With
the modified range-loss function, the GAN exclusively generates odorant
molecules. This work underscores the potential of combining novel vector
embeddings with transformers and modified GAN architectures to accelerate the
discovery of tailored molecules, offering a robust tool for diverse molecular
design applications.

</details>


### [114] [Score-Debiased Kernel Density Estimation](https://arxiv.org/abs/2504.19084)
*Elliot L. Epstein,Rajat Dwaraknath,Thanawat Sornwanee,John Winnicki,Jerry Weihong Liu*

Main category: cs.LG

TL;DR: 提出了一种基于估计分数函数来修正核密度估计偏差的新方法（SD-KDE）。通过调整数据点和修改带宽，实验显示该方法在多种任务中显著降低了平均积分平方误差。


<details>
  <summary>Details</summary>
Motivation: 传统的核密度估计方法在某些情况下存在偏差，影响了估计的准确性。本文旨在通过引入分数函数来修正这一偏差，以提高密度估计的效果。

Method: 方法包括利用估计的分数函数对每个数据点进行单步调整，并选择一个特定的步长，随后使用标准KDE和修改后的带宽进行密度估计。这一调整旨在消除KDE中的主导偏差。

Result: 在1D、2D合成任务和MNIST数据集上的实验结果表明，即使分数函数估计存在噪声，SD-KDE方法也能显著降低平均积分平方误差，表现优于标准的Silverman KDE方法。

Conclusion: 本文提出的SD-KDE方法展示了将基于分数的修正引入非参数密度估计的潜力，为提高密度估计的准确性提供了新的途径。

Abstract: We propose a novel method for density estimation that leverages an estimated
score function to debias kernel density estimation (SD-KDE). In our approach,
each data point is adjusted by taking a single step along the score function
with a specific choice of step size, followed by standard KDE with a modified
bandwidth. The step size and modified bandwidth are chosen to remove the
leading order bias in the KDE. Our experiments on synthetic tasks in 1D, 2D and
on MNIST, demonstrate that our proposed SD-KDE method significantly reduces the
mean integrated squared error compared to the standard Silverman KDE, even with
noisy estimates in the score function. These results underscore the potential
of integrating score-based corrections into nonparametric density estimation.

</details>


### [115] [Harmonizing Generalization and Personalization in Ring-topology Decentralized Federated Learning](https://arxiv.org/abs/2504.19103)
*Shunxin Guo,Jiaqi Lv,Xin Geng*

Main category: cs.LG

TL;DR: 论文提出了Divide-and-conquer RDFL框架（DRDFL），通过特征生成模型从底层数据分布中提取个性化信息和共享知识，解决了环形拓扑分散联邦学习（RDFL）中数据异构性导致的信息共享效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 传统RDFL由于点对点通信方式在处理数据异构性时效率低下，且现有研究忽略共享信息缺乏约束会导致模型差异过大，削弱协作学习效果。

Method: 设计了PersonaNet模块（鼓励类特定特征表示遵循高斯混合分布）和Learngene模块（通过对抗分类器对齐潜在表示并提取全局不变信息）。

Result: 实验表明，DRDFL在多种数据异构场景下优于现有方法。

Conclusion: DRDFL框架通过结合个性化与共享知识，有效提升了分散联邦学习的性能。

Abstract: We introduce Ring-topology Decentralized Federated Learning (RDFL) for
distributed model training, aiming to avoid the inherent risks of centralized
failure in server-based FL. However, RDFL faces the challenge of low
information-sharing efficiency due to the point-to-point communication manner
when handling inherent data heterogeneity. Existing studies to mitigate data
heterogeneity focus on personalized optimization of models, ignoring that the
lack of shared information constraints can lead to large differences among
models, weakening the benefits of collaborative learning. To tackle these
challenges, we propose a Divide-and-conquer RDFL framework (DRDFL) that uses a
feature generation model to extract personalized information and invariant
shared knowledge from the underlying data distribution, ensuring both effective
personalization and strong generalization. Specifically, we design a
\textit{PersonaNet} module that encourages class-specific feature
representations to follow a Gaussian mixture distribution, facilitating the
learning of discriminative latent representations tailored to local data
distributions. Meanwhile, the \textit{Learngene} module is introduced to
encapsulate shared knowledge through an adversarial classifier to align latent
representations and extract globally invariant information. Extensive
experiments demonstrate that DRDFL outperforms state-of-the-art methods in
various data heterogeneity settings.

</details>


### [116] [Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments](https://arxiv.org/abs/2504.19139)
*Yun Qu,Qi,Wang,Yixiu Mao,Yiqin Lv,Xiangyang Ji*

Main category: cs.LG

TL;DR: 论文提出了一种名为PDTS的简单易实现方法，用于快速且稳健的序列决策，通过后验多样性的协同任务采样优化鲁棒主动任务采样，显著提升了零样本和少样本适应能力，并在某些场景下加速了学习过程。


<details>
  <summary>Details</summary>
Motivation: 长期以来，顺序决策中的任务鲁棒适配是一个关键问题。现有的风险规避策略在优化中需要昂贵的密集评估，效率问题促使了鲁棒主动任务采样方法的发展。

Method: 本文将鲁棒主动任务采样的优化流程建模为马尔可夫决策过程，提出了PDTS方法，结合后验和多样性以协同采样任务。

Result: 实验表明，PDTS显著提升了零样本和少样本适应鲁棒性，并在特定场景中加速了学习过程。

Conclusion: PDTS方法通过优化任务采样，实现了高效且鲁棒的序列决策，为风险规避场景下的任务适应提供了实用解决方案。

Abstract: Task robust adaptation is a long-standing pursuit in sequential
decision-making. Some risk-averse strategies, e.g., the conditional
value-at-risk principle, are incorporated in domain randomization or meta
reinforcement learning to prioritize difficult tasks in optimization, which
demand costly intensive evaluations. The efficiency issue prompts the
development of robust active task sampling to train adaptive policies, where
risk-predictive models are used to surrogate policy evaluation. This work
characterizes the optimization pipeline of robust active task sampling as a
Markov decision process, posits theoretical and practical insights, and
constitutes robustness concepts in risk-averse scenarios. Importantly, we
propose an easy-to-implement method, referred to as Posterior and Diversity
Synergized Task Sampling (PDTS), to accommodate fast and robust sequential
decision-making. Extensive experiments show that PDTS unlocks the potential of
robust active task sampling, significantly improves the zero-shot and few-shot
adaptation robustness in challenging tasks, and even accelerates the learning
process under certain scenarios. Our project website is at
https://thu-rllab.github.io/PDTS_project_page.

</details>


### [117] [Reliable Thermal Monitoring of Electric Machines through Machine Learning](https://arxiv.org/abs/2504.19141)
*Panagiotis Kakosimos*

Main category: cs.LG

TL;DR: 论文探讨了人工智能技术在感应电机冷却效率监控中的应用，通过实验数据和机器学习模型验证了数据驱动方法在改善热管理方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着电动化动力系统的发展，确保电机内部温度在安全范围内运行至关重要。传统建模方法复杂且依赖专家知识，而现代数据采集为信息模型的应用提供了可能。

Method: 在特定运行条件下采集实验数据，开发了三种机器学习模型，并通过超参数搜索确定最优配置，使用多种指标评估模型性能。

Result: 三种模型在瞬态运行下均能有效监控电机状态，展示了数据驱动方法在热管理中的优势。

Conclusion: 数据驱动方法能显著提升感应电机的热管理效率，为未来电动化动力系统的温度监控提供了新的解决方案。

Abstract: The electrification of powertrains is rising as the objective for a more
viable future is intensified. To ensure continuous and reliable operation
without undesirable malfunctions, it is essential to monitor the internal
temperatures of machines and keep them within safe operating limits.
Conventional modeling methods can be complex and usually require expert
knowledge. With the amount of data collected these days, it is possible to use
information models to assess thermal behaviors. This paper investigates
artificial intelligence techniques for monitoring the cooling efficiency of
induction machines. Experimental data was collected under specific operating
conditions, and three machine-learning models have been developed. The optimal
configuration for each approach was determined through rigorous hyperparameter
searches, and the models were evaluated using a variety of metrics. The three
solutions performed well in monitoring the condition of the machine even under
transient operation, highlighting the potential of data-driven methods in
improving the thermal management.

</details>


### [118] [Newton-Puiseux Analysis for Interpretability and Calibration of Complex-Valued Neural Networks](https://arxiv.org/abs/2504.19176)
*Piotr Migus*

Main category: cs.LG

TL;DR: 提出了一种Newton-Puiseux框架，通过局部多项式代理和高不确定性输入的分数幂级数分解，提升了复数神经网络（CVNNs）的可解释性和校准能力。


<details>
  <summary>Details</summary>
Motivation: 复数神经网络在相位关键场景表现出色，但其多层的决策曲面难以用传统工具解释和校准。

Method: 采用Newton-Puiseux框架拟合局部多项式代理，将其分解为分数幂级数，生成Puiseux展开、主导系数和相位对齐曲率描述符。

Result: 在复杂螺旋实验中，代理模型的RMSE小于0.09，四次系数预测对抗攻击翻转半径误差在10^-3内；在MIT-BIH心律失常数据上，校准误差从0.087降至0.034。

Conclusion: 该方法显著提升了CVNNs的鲁棒性和校准能力，推动了其实际应用。

Abstract: Complex-valued neural networks (CVNNs) excel where phase matters, yet their
multi-sheeted decision surfaces defy standard explainability and calibration
tools. We propose a \emph{Newton-Puiseux} framework that fits a local
polynomial surrogate to a high-uncertainty input and analytically decomposes
this surrogate into fractional-power series. The resulting Puiseux expansions,
dominant Puiseux coefficients, and phase-aligned curvature descriptors deliver
closed-form estimates of robustness and over-confidence that gradient - or
perturbation-based methods (saliency, LIME, SHAP) cannot provide. On a
controlled $\mathbb{C}^2$ helix the surrogate attains RMSE $< 0.09$ while
recovering the number of decision sheets; quartic coefficients predict
adversarial flip radii within $10^{-3}$. On the real-world MIT-BIH arrhythmia
corpus, Puiseux-guided, phase-aware temperature scaling lowers expected
calibration error from 0.087 to 0.034, contributing to the advancement of
CVNNs. Full code, pre-trained weights, and scripts are at
https://github.com/piotrmgs/puiseux-cvnn.

</details>


### [119] [Hierarchical Attention Generates Better Proofs](https://arxiv.org/abs/2504.19188)
*Jianlong Chen,Chao Li,Yang Yuan,Andrew C Yao*

Main category: cs.LG

TL;DR: 该论文提出了一种名为分层注意力的正则化方法，通过五层次结构优化LLM在数学定理证明中的注意力机制，实验显示在miniF2F和ProofNet上分别提高了2.05%和1.69%的成功率，并降低了证明复杂度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在形式化定理证明中表现潜力，但其基于令牌的处理方式难以捕捉数学证明的层次结构，因此需要一种更结构化的方法来优化注意力机制。

Method: 论文引入分层注意力，通过建立从基础元素到高层概念的五层次结构，确保证明生成中的信息流结构化，从而改进LLM的注意力机制对齐。

Result: 实验结果表明，该方法在miniF2F和ProofNet数据集上分别提高了2.05%和1.69%的证明成功率，同时证明复杂度分别降低了23.81%和16.50%。

Conclusion: 分层注意力方法通过结构化注意力机制显著提升了LLM在数学定理证明中的性能，验证了其有效性。

Abstract: Large language models (LLMs) have shown promise in formal theorem proving,
but their token-level processing often fails to capture the inherent
hierarchical nature of mathematical proofs. We introduce \textbf{Hierarchical
Attention}, a regularization method that aligns LLMs' attention mechanisms with
mathematical reasoning structures. Our approach establishes a five-level
hierarchy from foundational elements to high-level concepts, ensuring
structured information flow in proof generation. Experiments demonstrate that
our method improves proof success rates by 2.05\% on miniF2F and 1.69\% on
ProofNet while reducing proof complexity by 23.81\% and 16.50\% respectively.
The code is available at https://github.com/Car-pe/HAGBP.

</details>


### [120] [HetGL2R: Learning to Rank Critical Road Segments via Attributed Heterogeneous Graph Random Walks](https://arxiv.org/abs/2504.19199)
*Ming Xu,Jinrong Xiang,Zilong Xie,Xiangfu Meng*

Main category: cs.LG

TL;DR: 提出HetGL2R方法，通过异构图学习整合OD需求和路径选择信息，更准确地评估道路网络中节点的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有节点重要性排序方法主要依赖结构特征和拓扑信息，忽略OD需求和路径等关键因素，导致排序准确性不足。

Method: HetGL2R构建三部分图（旅行图）整合OD需求、路径选择和结构特征；使用异构图随机游走采样算法（HetGWalk）和Transformer编码器学习节点表征；最终采用列表排序策略评估节点重要性。

Result: 实验表明HetGL2R显著优于基线方法，在合成数据集和北京出租车轨迹数据中验证了其准确性和实用性。

Conclusion: HetGL2R通过多特征融合有效提升节点排序性能，为交通管理和城市规划提供了更可靠的工具。

Abstract: Accurately identifying critical nodes with high spatial influence in road
networks is essential for enhancing the efficiency of traffic management and
urban planning. However, existing node importance ranking methods mainly rely
on structural features and topological information, often overlooking critical
factors such as origin-destination (OD) demand and route information. This
limitation leaves considerable room for improvement in ranking accuracy. To
address this issue, we propose HetGL2R, an attributed heterogeneous graph
learning approach for ranking node importance in road networks. This method
introduces a tripartite graph (trip graph) to model the structure of the road
network, integrating OD demand, route choice, and various structural features
of road segments. Based on the trip graph, we design an embedding method to
learn node representations that reflect the spatial influence of road segments.
The method consists of a heterogeneous random walk sampling algorithm
(HetGWalk) and a Transformer encoder. HetGWalk constructs multiple
attribute-guided graphs based on the trip graph to enrich the diversity of
semantic associations between nodes. It then applies a joint random walk
mechanism to convert both topological structures and node attributes into
sequences, enabling the encoder to capture spatial dependencies more
effectively among road segments. Finally, a listwise ranking strategy is
employed to evaluate node importance. To validate the performance of our
method, we construct two synthetic datasets using SUMO based on simulated road
networks. Experimental results demonstrate that HetGL2R significantly
outperforms baselines in incorporating OD demand and route choice information,
achieving more accurate and robust node ranking. Furthermore, we conduct a case
study using real-world taxi trajectory data from Beijing, further verifying the
practicality of the proposed method.

</details>


### [121] [Convergence Properties of Natural Gradient Descent for Minimizing KL Divergence](https://arxiv.org/abs/2504.19259)
*Adwait Datar,Nihat Ay*

Main category: cs.LG

TL;DR: 该论文研究了在概率单纯形上最小化KL散度的优化问题，分析了两种对偶坐标系下梯度下降（GD）与自然梯度下降（NGD）的收敛行为，证明NGD在离散时间中表现更优。


<details>
  <summary>Details</summary>
Motivation: KL散度是概率机器学习中的核心损失函数，但参数化选择显著影响优化收敛性。作者希望通过信息几何框架比较不同坐标系下GD与NGD的收敛性能。

Method: 在指数族（θ坐标）和混合族（η坐标）下，对比欧氏梯度下降（GD）与坐标不变的自然梯度下降（NGD），并通过分析KL散度Hessian谱及条件数（即Fisher信息矩阵）推导收敛率。

Result: 连续时间中，GD在θ和η坐标的收敛率分别提供NGD的下界和上界；若对偶坐标仿射变换，GD收敛率可缩放，而NGD保持固定收敛率2。离散时间中，NGD收敛更快且对噪声更鲁棒。

Conclusion: 尽管NGD在连续时间中不一定全局最优，但在离散时间中因其更快的收敛和鲁棒性成为更优选择，凸显了其实际应用价值。

Abstract: The Kullback-Leibler (KL) divergence plays a central role in probabilistic
machine learning, where it commonly serves as the canonical loss function.
Optimization in such settings is often performed over the probability simplex,
where the choice of parameterization significantly impacts convergence. In this
work, we study the problem of minimizing the KL divergence and analyze the
behavior of gradient-based optimization algorithms under two dual coordinate
systems within the framework of information geometry$-$ the exponential family
($\theta$ coordinates) and the mixture family ($\eta$ coordinates). We compare
Euclidean gradient descent (GD) in these coordinates with the
coordinate-invariant natural gradient descent (NGD), where the natural gradient
is a Riemannian gradient that incorporates the intrinsic geometry of the
parameter space. In continuous time, we prove that the convergence rates of GD
in the $\theta$ and $\eta$ coordinates provide lower and upper bounds,
respectively, on the convergence rate of NGD. Moreover, under affine
reparameterizations of the dual coordinates, the convergence rates of GD in
$\eta$ and $\theta$ coordinates can be scaled to $2c$ and $\frac{2}{c}$,
respectively, for any $c>0$, while NGD maintains a fixed convergence rate of
$2$, remaining invariant to such transformations and sandwiched between them.
Although this suggests that NGD may not exhibit uniformly superior convergence
in continuous time, we demonstrate that its advantages become pronounced in
discrete time, where it achieves faster convergence and greater robustness to
noise, outperforming GD. Our analysis hinges on bounding the spectrum and
condition number of the Hessian of the KL divergence at the optimum, which
coincides with the Fisher information matrix.

</details>


### [122] [TeleSparse: Practical Privacy-Preserving Verification of Deep Neural Networks](https://arxiv.org/abs/2504.19274)
*Mohammad M Maheri,Hamed Haddadi,Alex Davidson*

Main category: cs.LG

TL;DR: TeleSparse 是一种 ZK 友好的后处理机制，通过稀疏化和神经传送优化，显著降低了 ZK-SNARKs 在现代神经网络中的计算开销，同时保持高准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 由于验证深度学习推理通常需要访问敏感数据（如模型权重和训练数据），而 ZK-SNARKs 可以在不泄露数据的情况下完成验证，但应用于现代神经网络时计算开销巨大。TeleSparse 旨在解决这一问题。

Method: TeleSparse 通过稀疏化减少电路约束，并通过神经传送优化非线性函数的查找表大小，从而提升 ZK-SNARKs 的效率。

Result: TeleSparse 将验证内存使用降低 67%，证明生成时间减少 46%，且仅牺牲约 1% 的准确率。在多种架构和数据集上验证了其有效性。

Conclusion: TeleSparse 为 ZK 友好型模型设计开辟了新方向，推动了可扩展、资源高效的验证深度学习发展。

Abstract: Verification of the integrity of deep learning inference is crucial for
understanding whether a model is being applied correctly. However, such
verification typically requires access to model weights and (potentially
sensitive or private) training data. So-called Zero-knowledge Succinct
Non-Interactive Arguments of Knowledge (ZK-SNARKs) would appear to provide the
capability to verify model inference without access to such sensitive data.
However, applying ZK-SNARKs to modern neural networks, such as transformers and
large vision models, introduces significant computational overhead.
  We present TeleSparse, a ZK-friendly post-processing mechanisms to produce
practical solutions to this problem. TeleSparse tackles two fundamental
challenges inherent in applying ZK-SNARKs to modern neural networks: (1)
Reducing circuit constraints: Over-parameterized models result in numerous
constraints for ZK-SNARK verification, driving up memory and proof generation
costs. We address this by applying sparsification to neural network models,
enhancing proof efficiency without compromising accuracy or security. (2)
Minimizing the size of lookup tables required for non-linear functions, by
optimizing activation ranges through neural teleportation, a novel adaptation
for narrowing activation functions' range.
  TeleSparse reduces prover memory usage by 67% and proof generation time by
46% on the same model, with an accuracy trade-off of approximately 1%. We
implement our framework using the Halo2 proving system and demonstrate its
effectiveness across multiple architectures (Vision-transformer, ResNet,
MobileNet) and datasets (ImageNet,CIFAR-10,CIFAR-100). This work opens new
directions for ZK-friendly model design, moving toward scalable,
resource-efficient verifiable deep learning.

</details>


### [123] [Anyprefer: An Agentic Framework for Preference Data Synthesis](https://arxiv.org/abs/2504.19276)
*Yiyang Zhou,Zhaoyang Wang,Tianle Wang,Shangyu Xing,Peng Xia,Bo Li,Kaiyuan Zheng,Zijian Zhang,Zhaorun Chen,Wenhao Zheng,Xuchao Zhang,Chetan Bansal,Weitong Zhang,Ying Wei,Mohit Bansal,Huaxiu Yao*

Main category: cs.LG

TL;DR: Anyprefer是一个通过双玩家马尔可夫游戏框架合成高质量偏好数据的框架，显著提升了模型对齐性能。


<details>
  <summary>Details</summary>
Motivation: 人工标注偏好数据成本高且耗时，现有自奖励方法因共享权重导致偏差放大，需解决此问题。

Method: 采用合作式双玩家马尔可夫游戏，目标模型与裁判模型协作，引入外部工具减少奖励偏差，优化提示反馈机制。

Result: 生成58K高质量偏好对数据集Anyprefer-V1，在四大类应用21个数据集中表现显著提升（如自然语言生成平均提升18.55%）。

Conclusion: Anyprefer有效合成高质量偏好数据，为模型对齐提供可扩展解决方案。

Abstract: High-quality preference data is essential for aligning foundation models with
human values through preference learning. However, manual annotation of such
data is often time-consuming and costly. Recent methods often adopt a
self-rewarding approach, where the target model generates and annotates its own
preference data, but this can lead to inaccuracies since the reward model
shares weights with the target model, thereby amplifying inherent biases. To
address these issues, we propose Anyprefer, a framework designed to synthesize
high-quality preference data for aligning the target model. Anyprefer frames
the data synthesis process as a cooperative two-player Markov Game, where the
target model and the judge model collaborate together. Here, a series of
external tools are introduced to assist the judge model in accurately rewarding
the target model's responses, mitigating biases in the rewarding process. In
addition, a feedback mechanism is introduced to optimize prompts for both
models, enhancing collaboration and improving data quality. The synthesized
data is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K
high-quality preference pairs. Extensive experiments show that Anyprefer
significantly improves model alignment performance across four main
applications, covering 21 datasets, achieving average improvements of 18.55% in
five natural language generation datasets, 3.66% in nine vision-language
understanding datasets, 30.05% in three medical image analysis datasets, and
16.00% in four visuo-motor control tasks.

</details>


### [124] [Ethical Challenges of Using Artificial Intelligence in Judiciary](https://arxiv.org/abs/2504.19284)
*Angel Mary John,Aiswarya M. U.,Jerrin Thomas Panachakel*

Main category: cs.LG

TL;DR: 摘要讨论了AI在法律系统中的潜力与伦理挑战，强调其可提升司法效率但也需解决伦理问题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨AI如何改善司法系统的效率，同时分析其带来的伦理挑战。

Method: 文章通过分析AI在法律领域的应用及其潜在影响，提出伦理问题的解决方案。

Result: AI能自动化繁琐的法律任务，但需应对伦理问题以确保负责任的使用。

Conclusion: AI在法律系统中的应用虽有优势，但必须解决伦理问题以实现公平和有效的司法管理。

Abstract: Artificial intelligence (AI) has emerged as a ubiquitous concept in numerous
domains, including the legal system. AI has the potential to revolutionize the
functioning of the judiciary and the dispensation of justice. Incorporating AI
into the legal system offers the prospect of enhancing decision-making for
judges, lawyers, and legal professionals, while concurrently providing the
public with more streamlined, efficient, and cost-effective services. The
integration of AI into the legal landscape offers manifold benefits,
encompassing tasks such as document review, legal research, contract analysis,
case prediction, and decision-making. By automating laborious and error-prone
procedures, AI has the capacity to alleviate the burden associated with these
arduous tasks. Consequently, courts around the world have begun embracing AI
technology as a means to enhance the administration of justice. However,
alongside its potential advantages, the use of AI in the judiciary poses a
range of ethical challenges. These ethical quandaries must be duly addressed to
ensure the responsible and equitable deployment of AI systems. This article
delineates the principal ethical challenges entailed in employing AI within the
judiciary and provides recommendations to effectively address these issues.

</details>


### [125] [Flow Along the K-Amplitude for Generative Modeling](https://arxiv.org/abs/2504.19353)
*Weitao Du,Shuning Chang,Jiasheng Tang,Yu Rong,Fan Wang,Shengchao Liu*

Main category: cs.LG

TL;DR: K-Flow是一种基于K-振幅分解的新型生成学习算法，通过控制不同尺度的信息实现可调控生成，并在图像和分子生成任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 提出K-Flow算法，旨在通过K-振幅分解实现跨尺度流匹配，从而在生成任务中实现更灵活和可控的生成效果。

Method: 利用K-振幅分解组织频带或投影系数，通过流匹配机制在不同尺度间传递信息，并在理论、能量动态和实践应用中验证其性质。

Result: 实验表明，K-Flow在无条件图像生成、类条件图像生成和分子组装生成任务中表现出色，并能通过调控尺度参数控制生成分辨率。

Conclusion: K-Flow通过K-振幅分解实现了可控生成，展示了在多种生成任务中的潜力，并通过消融实验验证了其调控机制的有效性。

Abstract: In this work, we propose a novel generative learning paradigm, K-Flow, an
algorithm that flows along the $K$-amplitude. Here, $k$ is a scaling parameter
that organizes frequency bands (or projected coefficients), and amplitude
describes the norm of such projected coefficients. By incorporating the
$K$-amplitude decomposition, K-Flow enables flow matching across the scaling
parameter as time. We discuss three venues and six properties of K-Flow, from
theoretical foundations, energy and temporal dynamics, and practical
applications, respectively. Specifically, from the practical usage perspective,
K-Flow allows steerable generation by controlling the information at different
scales. To demonstrate the effectiveness of K-Flow, we conduct experiments on
unconditional image generation, class-conditional image generation, and
molecule assembly generation. Additionally, we conduct three ablation studies
to demonstrate how K-Flow steers scaling parameter to effectively control the
resolution of image generation.

</details>


### [126] [Rethinking Label-specific Features for Label Distribution Learning](https://arxiv.org/abs/2504.19374)
*Suping Xu,Chuyi Dai,Lin Shang,Changbin Shao,Xibei Yang,Witold Pedrycz*

Main category: cs.LG

TL;DR: 论文提出了一种改进的标签分布学习（LDL）方法LDL-LIFT-SAP，通过引入结构锚点（SAPs）捕捉簇间交互，优化了标签特定特征（LSFs）的构建，并在15个真实数据集上验证了其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于LIFT的标签特定特征（LSFs）构建方法在标签分布学习中存在局限性，主要依赖欧氏距离且忽略簇间交互。为了更全面地表征实例并减少噪声和偏差，需要一种融合多视角信息和簇间关系的新方法。

Method: 提出LIFT-SAP策略，利用结构锚点（SAPs）整合距离和方向信息改进LSFs构建；进一步提出LDL-LIFT-SAP算法，统一不同LSF空间的多标签描述度。

Result: 在15个真实数据集上的实验表明，LIFT-SAP优于LIFT，LDL-LIFT-SAP在性能上超越了7种现有算法。

Conclusion: LDL-LIFT-SAP通过结合簇间交互和多视角信息，显著提升了标签分布学习的表现，为复杂标签关系建模提供了新思路。

Abstract: Label distribution learning (LDL) is an emerging learning paradigm designed
to capture the relative importance of labels for each instance. Label-specific
features (LSFs), constructed by LIFT, have proven effective for learning tasks
with label ambiguity by leveraging clustering-based prototypes for each label
to re-characterize instances. However, directly introducing LIFT into LDL tasks
can be suboptimal, as the prototypes it collects primarily reflect
intra-cluster relationships while neglecting interactions among distinct
clusters. Additionally, constructing LSFs using multi-perspective information,
rather than relying solely on Euclidean distance, provides a more robust and
comprehensive representation of instances, mitigating noise and bias that may
arise from a single distance perspective. To address these limitations, we
introduce Structural Anchor Points (SAPs) to capture inter-cluster
interactions. This leads to a novel LSFs construction strategy, LIFT-SAP, which
enhances LIFT by integrating both distance and direction information of each
instance relative to SAPs. Furthermore, we propose a novel LDL algorithm, Label
Distribution Learning via Label-specifIc FeaTure with SAPs (LDL-LIFT-SAP),
which unifies multiple label description degrees predicted from different LSF
spaces into a cohesive label distribution. Extensive experiments on 15
real-world datasets demonstrate the effectiveness of LIFT-SAP over LIFT, as
well as the superiority of LDL-LIFT-SAP compared to seven other
well-established algorithms.

</details>


### [127] [$O(1/k)$ Finite-Time Bound for Non-Linear Two-Time-Scale Stochastic Approximation](https://arxiv.org/abs/2504.19375)
*Siddharth Chandak*

Main category: cs.LG

TL;DR: 该研究改进了非线性双时间尺度随机逼近算法的收敛速度，从已知的$O(1/k^{2/3})$提升至$O(1/k)$，适用于梯度下降-上升和双时间尺度拉格朗日优化等算法。


<details>
  <summary>Details</summary>
Motivation: 双时间尺度随机逼近算法在强化学习、优化和博弈控制中有广泛应用，但其非线性收敛速度长期受限，需进一步提升。

Method: 通过重写迭代式为平均噪声序列形式（衰减较快），并结合归纳法证明迭代的期望有界性。

Result: 在非线性收缩设定下，实现了$O(1/k)$的均方误差界，优于之前的$O(1/k^{2/3})$结果。

Conclusion: 该工作显著提升了非线性双时间尺度算法的收敛性能，为相关应用提供了理论保障。

Abstract: Two-time-scale stochastic approximation is an algorithm with coupled
iterations which has found broad applications in reinforcement learning,
optimization and game control. While several prior works have obtained a mean
square error bound of $O(1/k)$ for linear two-time-scale iterations, the best
known bound in the non-linear contractive setting has been $O(1/k^{2/3})$. In
this work, we obtain an improved bound of $O(1/k)$ for non-linear
two-time-scale stochastic approximation. Our result applies to algorithms such
as gradient descent-ascent and two-time-scale Lagrangian optimization. The key
step in our analysis involves rewriting the original iteration in terms of an
averaged noise sequence which decays sufficiently fast. Additionally, we use an
induction-based approach to show that the iterates are bounded in expectation.

</details>


### [128] [HyperController: A Hyperparameter Controller for Fast and Stable Training of Reinforcement Learning Neural Networks](https://arxiv.org/abs/2504.19382)
*Jonathan Gornet,Yiannis Kantaros,Bruno Sinopoli*

Main category: cs.LG

TL;DR: HyperController是一种计算高效的超参数优化算法，用于强化学习神经网络的训练，通过将其建模为线性高斯动态系统并使用卡尔曼滤波学习高效表示，实现快速优化和稳定训练。在多数测试环境中表现优于其他算法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习神经网络训练中，超参数优化常计算成本高且不稳定，影响训练效率和部署速度。因此，需一种高效且稳定的超参数优化方法。

Method: 将超参数优化问题建模为线性高斯动态系统，使用卡尔曼滤波学习超参数目标函数的高效表示。

Result: 在五个OpenAI Gymnasium环境中的四个，HyperController的中位奖励最高，优于其他算法。

Conclusion: HyperController展示了在强化学习神经网络训练中实现高效和稳定超参数优化的潜力。

Abstract: We introduce Hyperparameter Controller (HyperController), a computationally
efficient algorithm for hyperparameter optimization during training of
reinforcement learning neural networks. HyperController optimizes
hyperparameters quickly while also maintaining improvement of the reinforcement
learning neural network, resulting in faster training and deployment. It
achieves this by modeling the hyperparameter optimization problem as an unknown
Linear Gaussian Dynamical System, which is a system with a state that linearly
changes. It then learns an efficient representation of the hyperparameter
objective function using the Kalman filter, which is the optimal one-step
predictor for a Linear Gaussian Dynamical System. To demonstrate the
performance of HyperController, it is applied as a hyperparameter optimizer
during training of reinforcement learning neural networks on a variety of
OpenAI Gymnasium environments. In four out of the five Gymnasium environments,
HyperController achieves highest median reward during evaluation compared to
other algorithms. The results exhibit the potential of HyperController for
efficient and stable training of reinforcement learning neural networks.

</details>


### [129] [Bi-directional Model Cascading with Proxy Confidence](https://arxiv.org/abs/2504.19391)
*David Warren,Mark Dras*

Main category: cs.LG

TL;DR: 论文提出了一种双向延迟方法，通过同时考虑级联中大小模型的置信度，利用代理模型改进模型级联效率，减少对高成本大模型的调用。


<details>
  <summary>Details</summary>
Motivation: 现有模型级联方法仅依赖小模型的有限置信度估计，无法充分利用大模型的置信度信息。

Method: 提出双向延迟方法，通过分析隐藏状态改进小模型的后调用置信度，并结合微型代理模型预估大模型的预调用置信度。

Result: 在多项选择题数据集上验证，相较于基线方法，减少了对高成本大模型的调用。

Conclusion: 双向延迟方法通过联合利用大小模型的置信度信息，有效提升了模型级联的效率和性能。

Abstract: Model Cascading, recently applied successfully to LLMs, is a simple but
powerful technique that improves the efficiency of inference by selectively
applying models of varying sizes. Models are used in sequence from smallest to
largest, only deferring samples to large, costly models when smaller models are
not sufficiently confident. Existing approaches to deferral use only limited
small model confidence estimates because of the inaccessibility of the large
model, although large model confidence is known to be important. We therefore
propose a bi-directional approach to deferral that considers the confidence of
small and large models in the cascade simultaneously through the use of a proxy
for the large model. This requires a richer representation of model confidence
to enable comparative calibration: we use an analysis of hidden states to
improve post-invocation confidence of the small model, which in itself improves
cascading results over prior approaches. We then combine this with a tiny proxy
model to estimate pre-invocation confidence of the large model. We examine the
proposed cascading system over challenging, multiple-choice datasets, finding
improvements over standard cascading baselines reflected in reductions in
deferrals to more costly models.

</details>


### [130] [Observational Learning with a Budget](https://arxiv.org/abs/2504.19396)
*Shuo Wu,Pawan Poojary,Randall Berry*

Main category: cs.LG

TL;DR: 该论文研究了贝叶斯观察学习模型中预算分配问题，提出了两种最优分配策略以提升信号质量，从而提高正确信息级联的概率。


<details>
  <summary>Details</summary>
Motivation: 在贝叶斯观察学习模型中，代理人通过私有信号和观察前人的行为做决策。中央规划者希望通过有限的预算提升信号质量，以提高决策准确性，而最优预算分配策略的缺乏是研究动机。

Method: 论文构建了预算分配问题的模型，并提出了两种最优分配策略，通过数学模型和分析验证这些策略的效果。

Result: 至少其中一种策略被证明能最大化实现正确信息级联的概率，表明预算分配对决策准确性有显著影响。

Conclusion: 研究证明了通过合理的预算分配提升信号质量是可行的，为信息级联中的决策优化提供了理论支持。

Abstract: We consider a model of Bayesian observational learning in which a sequence of
agents receives a private signal about an underlying binary state of the world.
Each agent makes a decision based on its own signal and its observations of
previous agents. A central planner seeks to improve the accuracy of these
signals by allocating a limited budget to enhance signal quality across agents.
We formulate and analyze the budget allocation problem and propose two optimal
allocation strategies. At least one of these strategies is shown to maximize
the probability of achieving a correct information cascade.

</details>


### [131] [UNet with Axial Transformer : A Neural Weather Model for Precipitation Nowcasting](https://arxiv.org/abs/2504.19408)
*Maitreya Sonawane,Sumit Mamtani*

Main category: cs.LG

TL;DR: 作者提出了一种基于Transformer的深度学习方法，用于精准预测局部天气（如雷暴），替代传统数值模型，实现高分辨率的即时预报。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气模型难以应对局部快速变化的天气现象（如雷暴），而深度学习模型推理成本低且精度高，适合解决这一问题。

Method: 采用基于轴向注意力机制的Transformer模型，从时间序列数据中学习复杂模式，适用于单变量、多变量及嵌入数据。

Result: 在数据集的下一帧预测任务中，使用UNet与轴向Transformer的组合取得了PSNR=47.67、SSIM=0.9943的领先性能。

Conclusion: Transformer模型为天气临近预报提供了高效且高精度的解决方案，未来可推广至更广泛的时序数据任务。

Abstract: Making accurate weather predictions can be particularly challenging for
localized storms or events that evolve on hourly timescales, such as
thunderstorms. Hence, our goal for the project was to model Weather Nowcasting
for making highly localized and accurate predictions that apply to the
immediate future replacing the current numerical weather models and data
assimilation systems with Deep Learning approaches. A significant advantage of
machine learning is that inference is computationally cheap given an
already-trained model, allowing forecasts that are nearly instantaneous and in
the native high resolution of the input data. In this work we developed a novel
method that employs Transformer-based machine learning models to forecast
precipitation. This approach works by leveraging axial attention mechanisms to
learn complex patterns and dynamics from time series frames. Moreover, it is a
generic framework and can be applied to univariate and multivariate time series
data, as well as time series embeddings data. This paper represents an initial
research on the dataset used in the domain of next frame prediciton, and hence,
we demonstrate state-of-the-art results in terms of metrices (PSNR = 47.67,
SSIM = 0.9943) used for the given dataset using UNet with Axial Transformer.

</details>


### [132] [Graph-based Semi-supervised and Unsupervised Methods for Local Clustering](https://arxiv.org/abs/2504.19419)
*Zhaiming Shen,Sung Ha Kang*

Main category: cs.LG

TL;DR: 论文提出了半监督和无监督局部聚类方法，通过随机采样和图扩散技术识别局部子结构，实验证明在低标签率下效果优异。


<details>
  <summary>Details</summary>
Motivation: 局部聚类旨在无需全局图知识的情况下识别特定子结构，但在数据标签极少或无标签时，传统方法效果有限。

Method: 通过随机采样图、局部簇提取扩散，并分析结果重叠来识别簇，同时提出了节点共属条件和理论证明。

Result: 实验证明所提方法在低标签率下达到最优性能。

Conclusion: 该方法为半监督和无监督局部聚类提供了高效解决方案，并具有理论保证。

Abstract: Local clustering aims to identify specific substructures within a large graph
without requiring full knowledge of the entire graph. These substructures are
typically small compared to the overall graph, enabling the problem to be
approached by finding a sparse solution to a linear system associated with the
graph Laplacian. In this work, we first propose a method for identifying
specific local clusters when very few labeled data is given, which we term
semi-supervised local clustering. We then extend this approach to the
unsupervised setting when no prior information on labels is available. The
proposed methods involve randomly sampling the graph, applying diffusion
through local cluster extraction, then examining the overlap among the results
to find each cluster. We establish the co-membership conditions for any pair of
nodes and rigorously prove the correctness of our methods. Additionally, we
conduct extensive experiments to demonstrate that the proposed methods achieve
state-of-the-arts results in the low-label rates regime.

</details>


### [133] [Learning High-dimensional Gaussians from Censored Data](https://arxiv.org/abs/2504.19446)
*Arnab Bhattacharyya,Constantinos Daskalakis,Themis Gouleakis,Yuhao Wang*

Main category: cs.LG

TL;DR: 提出了两种高效算法，用于从高维高斯数据中学习分布，假设数据存在非随机缺失（MNAR）。第一种是自感知缺失模型，第二种是线性阈值缺失模型。


<details>
  <summary>Details</summary>
Motivation: 高维数据中缺失值常见且影响分析准确性，尤其是非随机缺失情况。解决这一问题的现有方法效率不足或假设过强。

Method: 针对自感知缺失和线性阈值缺失两种模型，分别设计多项式样本复杂度的高效算法。前者假设坐标对观测概率足够高，后者要求缺失模式不稀有且小规模坐标观测概率高。

Result: 在两种模型下均实现高效学习：自感知缺失模型下总变差距离逼近，线性阈值模型下均值估计有效。所需样本量为多项式级。

Conclusion: 证明了在非随机缺失假设下，高维高斯分布学习可行，为实际数据缺失问题提供理论工具。

Abstract: We provide efficient algorithms for the problem of distribution learning from
high-dimensional Gaussian data where in each sample, some of the variable
values are missing. We suppose that the variables are missing not at random
(MNAR). The missingness model, denoted by $S(y)$, is the function that maps any
point $y$ in $R^d$ to the subsets of its coordinates that are seen. In this
work, we assume that it is known. We study the following two settings:
  (i) Self-censoring: An observation $x$ is generated by first sampling the
true value $y$ from a $d$-dimensional Gaussian $N(\mu*, \Sigma*)$ with unknown
$\mu*$ and $\Sigma*$. For each coordinate $i$, there exists a set $S_i$
subseteq $R^d$ such that $x_i = y_i$ if and only if $y_i$ in $S_i$. Otherwise,
$x_i$ is missing and takes a generic value (e.g., "?"). We design an algorithm
that learns $N(\mu*, \Sigma*)$ up to total variation (TV) distance epsilon,
using $poly(d, 1/\epsilon)$ samples, assuming only that each pair of
coordinates is observed with sufficiently high probability.
  (ii) Linear thresholding: An observation $x$ is generated by first sampling
$y$ from a $d$-dimensional Gaussian $N(\mu*, \Sigma)$ with unknown $\mu*$ and
known $\Sigma$, and then applying the missingness model $S$ where $S(y) = {i in
[d] : v_i^T y <= b_i}$ for some $v_1, ..., v_d$ in $R^d$ and $b_1, ..., b_d$ in
$R$. We design an efficient mean estimation algorithm, assuming that none of
the possible missingness patterns is very rare conditioned on the values of the
observed coordinates and that any small subset of coordinates is observed with
sufficiently high probability.

</details>


### [134] [R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference](https://arxiv.org/abs/2504.19449)
*Zhenyu Zhang,Zechun Liu,Yuandong Tian,Harshit Khaitan,Zhangyang Wang,Steven Li*

Main category: cs.LG

TL;DR: 论文提出了一种无需训练的激活稀疏方法R-Sparse，用于在高效推理的LLMs（如Llama-2/3和Mistral）中实现高稀疏度，实验表明在50%的模型稀疏度下性能相当，且端到端效率提升43%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）推理时因模型尺寸大而面临计算和内存效率挑战，当前激活稀疏方法对非ReLU激活函数或持续训练需求高，且难以达到高稀疏度。

Method: R-Sparse通过研究发现线性层的输入非稀疏部分可视为偏置项，且其计算可通过输入通道和权重奇异值组合近似，从而提出一种基于输入通道和奇异值稀疏的推理方法，避免预测活跃通道的需求。

Result: 在Llama-2/3和Mistral模型的十项任务测试中，R-Sparse在50%模型稀疏度下性能相当，端到端效率提升43%。

Conclusion: R-Sparse是一种高效、无需训练的激活稀疏方法，适用于LLM推理优化，显著提升计算和内存效率，为边缘设备部署提供实用解决方案。

Abstract: Large Language Models (LLMs), while demonstrating remarkable capabilities
across various applications, present significant challenges during inference
due to their substantial model size, especially when deployed on edge devices.
Activation sparsity offers a promising solution to reduce computation and
memory movement, enabling more efficient inference, particularly for
small-batch on-device applications. However, current approaches face
limitations with non-ReLU activation function, which are foundational to most
advanced LLMs, or require heavy continual training. Additionally, the
difficulty in predicting active channels and limited achievable sparsity ratios
constrain the effectiveness of activation sparsity-based methods. In this
paper, we introduce R-Sparse, a training-free activation sparsity approach
capable of achieving high sparsity levels in advanced LLMs. We conducted two
preliminary investigations into how different components contribute to the
output within a single linear layer and found two key observations: (i) the
non-sparse components of the input function can be regarded as a few bias
terms, and (ii) The full computation can be effectively approximated by an
appropriate combination of input channels and weight singular values. Building
on this, we replace the linear layers in LLMs with a rank-aware sparse
inference method that leverages the sparsity of input channels and singular
value components, eliminating the need for active channel prediction like the
output sparsity based approaches. Experiments on Llama-2/3 and Mistral models
across ten diverse tasks demonstrate that R-Sparse achieves comparable
performance at 50% model-level sparsity, resulting in a significant 43%
end-to-end efficient improvements with customized kernels.

</details>


### [135] [Geometry-Informed Neural Operator Transformer](https://arxiv.org/abs/2504.19452)
*Qibang Liu,Vincient Zhong,Hadi Meidani,Diab Abueidda,Seid Koric,Philippe Geubelle*

Main category: cs.LG

TL;DR: 该论文提出了一种基于机器学习的几何感知神经算子变换器（GINOT），通过结合Transformer架构和神经算子框架，实现对任意几何形状的高效正向预测。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在需要重复求解偏微分方程时计算效率低，而现有机器学习代用模型在处理复杂几何形状时表现不足。GINOT旨在解决这些问题，同时保持对几何变化的鲁棒性。

Method: GINOT采用采样分组机制和注意力机制编码几何表面点云，确保对点顺序和填充的不变性，并通过注意力机制将几何信息与查询点无缝集成到解码器中。

Result: 在多个挑战性数据集上的验证表明，GINOT对复杂2D和3D几何形状具有高精度和强泛化能力。

Conclusion: GINOT通过结合神经算子和Transformer框架，显著提升了计算效率和预测准确性，适用于复杂几何形状的快速仿真。

Abstract: Machine-learning-based surrogate models offer significant computational
efficiency and faster simulations compared to traditional numerical methods,
especially for problems requiring repeated evaluations of partial differential
equations. This work introduces the Geometry-Informed Neural Operator
Transformer (GINOT), which integrates the transformer architecture with the
neural operator framework to enable forward predictions for arbitrary
geometries. GINOT encodes the surface points cloud of a geometry using a
sampling and grouping mechanism combined with an attention mechanism, ensuring
invariance to point order and padding while maintaining robustness to
variations in point density. The geometry information is seamlessly integrated
with query points in the solution decoder through the attention mechanism. The
performance of GINOT is validated on multiple challenging datasets, showcasing
its high accuracy and strong generalization capabilities for complex and
arbitrary 2D and 3D geometries.

</details>


### [136] [Stability Enhancement in Reinforcement Learning via Adaptive Control Lyapunov Function](https://arxiv.org/abs/2504.19473)
*Donghe Chen,Han Wang,Lin Cheng,Shengping Gong*

Main category: cs.LG

TL;DR: 该论文提出了一种结合Soft Actor-Critic与控制李雅普诺夫函数（SAC-CLF）的框架，以解决强化学习在安全探索方面的挑战，通过动态调整约束和优化控制输入平滑性，提升了系统的稳定性和安全性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在控制任务中面临安全探索的挑战，传统方法如奖励塑造或约束策略优化难以保证初始学习阶段的安全性，而基于模型的方法可能限制探索效率。

Method: 论文提出SAC-CLF框架，包括任务特定的CLF设计、动态约束调整和优化控制输入平滑性三项创新。

Result: 在经典非线性系统和卫星姿态控制实验中，SAC-CLF有效解决了现有方法的不足，提升了安全性和性能。

Conclusion: SAC-CLF框架通过结合强化学习与控制理论，为安全探索提供了可行的解决方案，具有实际应用的潜力。

Abstract: Reinforcement Learning (RL) has shown promise in control tasks but faces
significant challenges in real-world applications, primarily due to the absence
of safety guarantees during the learning process. Existing methods often
struggle with ensuring safe exploration, leading to potential system failures
and restricting applications primarily to simulated environments. Traditional
approaches such as reward shaping and constrained policy optimization can fail
to guarantee safety during initial learning stages, while model-based methods
using Control Lyapunov Functions (CLFs) or Control Barrier Functions (CBFs) may
hinder efficient exploration and performance. To address these limitations,
this paper introduces Soft Actor-Critic with Control Lyapunov Function
(SAC-CLF), a framework that enhances stability and safety through three key
innovations: (1) a task-specific CLF design method for safe and optimal
performance; (2) dynamic adjustment of constraints to maintain robustness under
unmodeled dynamics; and (3) improved control input smoothness while ensuring
safety. Experimental results on a classical nonlinear system and satellite
attitude control demonstrate the effectiveness of SAC-CLF in overcoming the
shortcomings of existing methods.

</details>


### [137] [An Automated Reinforcement Learning Reward Design Framework with Large Language Model for Cooperative Platoon Coordination](https://arxiv.org/abs/2504.19480)
*Dixiao Wei,Peng Yi,Jinlong Lei,Yiguang Hong,Yuchuan Du*

Main category: cs.LG

TL;DR: 论文提出了一种基于大型语言模型（LLM）的自动奖励函数设计框架（PCRD），用于解决强化学习在车队协调中的奖励设计问题，并通过实验验证了其优于人工设计的性能。


<details>
  <summary>Details</summary>
Motivation: 由于车队协调目标的多样性、决策问题的复杂性以及手动试错的时间消耗，设计高性能奖励函数来指导强化学习训练具有挑战性。

Method: 提出PCRD框架，通过LLM驱动的初始化和迭代优化自动生成奖励函数，包括分析初始化奖励（AIR）模块和进化模块。

Result: 实验表明，使用PCRD生成的奖励函数的强化学习代理在所有场景中平均性能指标比人工设计的奖励函数高10%。

Conclusion: PCRD框架能有效自动化奖励函数设计，提升强化学习在车队协调中的性能。

Abstract: Reinforcement Learning (RL) has demonstrated excellent decision-making
potential in platoon coordination problems. However, due to the variability of
coordination goals, the complexity of the decision problem, and the
time-consumption of trial-and-error in manual design, finding a well
performance reward function to guide RL training to solve complex platoon
coordination problems remains challenging. In this paper, we formally define
the Platoon Coordination Reward Design Problem (PCRDP), extending the RL-based
cooperative platoon coordination problem to incorporate automated reward
function generation. To address PCRDP, we propose a Large Language Model
(LLM)-based Platoon coordination Reward Design (PCRD) framework, which
systematically automates reward function discovery through LLM-driven
initialization and iterative optimization. In this method, LLM first
initializes reward functions based on environment code and task requirements
with an Analysis and Initial Reward (AIR) module, and then iteratively
optimizes them based on training feedback with an evolutionary module. The AIR
module guides LLM to deepen their understanding of code and tasks through a
chain of thought, effectively mitigating hallucination risks in code
generation. The evolutionary module fine-tunes and reconstructs the reward
function, achieving a balance between exploration diversity and convergence
stability for training. To validate our approach, we establish six challenging
coordination scenarios with varying complexity levels within the Yangtze River
Delta transportation network simulation. Comparative experimental results
demonstrate that RL agents utilizing PCRD-generated reward functions
consistently outperform human-engineered reward functions, achieving an average
of 10\% higher performance metrics in all scenarios.

</details>


### [138] [Improving Reasoning Performance in Large Language Models via Representation Engineering](https://arxiv.org/abs/2504.19483)
*Bertram Højer,Oliver Jarvis,Stefan Heinrich*

Main category: cs.LG

TL;DR: 该论文提出了一种通过表示工程方法调制LLM激活向量以提升推理任务性能的技术，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM的推理能力是否可通过干预激活向量来提升，以验证其推理机制是否与其他信息处理任务类似。

Method: 从LLM残差流中提取推理任务时的激活向量，生成控制向量作为推理时干预，通过KL散度等指标评估效果。

Result: 在Mistral-7B和Pythia模型上，该方法成功提升了归纳、演绎和数学推理任务的性能。

Conclusion: LLM的推理能力可通过激活向量调制改进，且其机制与其他任务类似，无需额外训练即可针对特定任务优化。

Abstract: Recent advancements in large language models (LLMs) have resulted in
increasingly anthropomorphic language concerning the ability of LLMs to reason.
Whether reasoning in LLMs should be understood to be inherently different is,
however, widely debated. We propose utilizing a representation engineering
approach wherein model activations are read from the residual stream of an LLM
when processing a reasoning task. The activations are used to derive a control
vector that is applied to the model as an inference-time intervention,
modulating the representational space of the model, to improve performance on
the specified task. We publish the code for deriving control vectors and
analyzing model representations. The method allows us to improve performance on
reasoning benchmarks and assess how control vectors influence the final logit
distribution of a model via metrics such as KL divergence and entropy. We apply
control vectors to Mistral-7B-Instruct and a range of Pythia models on an
inductive, a deductive and mathematical reasoning task. We show that an LLM
can, to a certain degree, be controlled to improve its perceived reasoning
ability by modulating activations. The intervention is dependent upon the
ability to reliably extract the model's typical state when correctly solving a
task. Our results suggest that reasoning performance can be modulated in the
same manner as other information-processing tasks performed by LLMs and
demonstrate that we are capable of improving performance on specific tasks via
a simple intervention on the residual stream with no additional training.

</details>


### [139] [DISCO: learning to DISCover an evolution Operator for multi-physics-agnostic prediction](https://arxiv.org/abs/2504.19496)
*Rudy Morel,Jiequn Han,Edouard Oyallon*

Main category: cs.LG

TL;DR: 论文提出了DISCO模型，通过超网络处理短轨迹生成小算子网络参数，分离动力学估计与状态预测，显著减少训练时间并在下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 针对未知时间偏微分方程（PDEs）的动态系统预测问题，传统Transformer虽然可用但效率不高。已知PDE时经典数值解法能以少量参数准确预测状态，受此启发，研究提出了更高效的解决方案。

Method: DISCO模型利用超网络处理短轨迹生成小算子网络的参数，后者通过时间积分预测下一状态，实现了动力学估计与状态预测的解耦。

Result: 实验表明，该模型在多样化物理数据集上预训练后，性能达到最优，且训练周期大幅减少，下游任务微调后仍保持竞争力。

Conclusion: DISCO通过分离动态估计与状态预测，显著提升了未知PDE动态系统的预测效率和泛化能力。

Abstract: We address the problem of predicting the next state of a dynamical system
governed by unknown temporal partial differential equations (PDEs) using only a
short trajectory. While standard transformers provide a natural black-box
solution to this task, the presence of a well-structured evolution operator in
the data suggests a more tailored and efficient approach. Specifically, when
the PDE is fully known, classical numerical solvers can evolve the state
accurately with only a few parameters. Building on this observation, we
introduce DISCO, a model that uses a large hypernetwork to process a short
trajectory and generate the parameters of a much smaller operator network,
which then predicts the next state through time integration. Our framework
decouples dynamics estimation (i.e., DISCovering an evolution operator from a
short trajectory) from state prediction (i.e., evolving this operator).
Experiments show that pretraining our model on diverse physics datasets
achieves state-of-the-art performance while requiring significantly fewer
epochs. Moreover, it generalizes well and remains competitive when fine-tuned
on downstream tasks.

</details>


### [140] [Identification and Estimation of Long-Term Treatment Effects with Monotone Missing](https://arxiv.org/abs/2504.19527)
*Qinwei Yang,Ruocheng Guo,Shasha Han,Peng Wu*

Main category: cs.LG

TL;DR: 该论文提出了一种解决长期治疗效果估计中单调缺失问题的框架，包括三种新方法和一种稳定性增强方法，实验结果验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 长期治疗效果估计中普遍存在单调缺失问题，但此前研究少有探讨。本文旨在填补这一空白。

Method: 提出三种估计方法：逆概率加权、序贯回归填补和序贯边际结构模型（SeqMSM）。针对SeqMSM因数据稀疏导致的方差问题，进一步提出BalanceNet方法提升稳定性。

Result: 在两个基准数据集上的广泛实验表明，所提方法显著提升了估计的准确性和稳定性。

Conclusion: 本文提出的方法有效解决了单调缺失问题，为长期治疗效果估计提供了更可靠的解决方案。

Abstract: Estimating long-term treatment effects has a wide range of applications in
various domains. A key feature in this context is that collecting long-term
outcomes typically involves a multi-stage process and is subject to monotone
missing, where individuals missing at an earlier stage remain missing at
subsequent stages. Despite its prevalence, monotone missing has been rarely
explored in previous studies on estimating long-term treatment effects. In this
paper, we address this gap by introducing the sequential missingness assumption
for identification. We propose three novel estimation methods, including
inverse probability weighting, sequential regression imputation, and sequential
marginal structural model (SeqMSM). Considering that the SeqMSM method may
suffer from high variance due to severe data sparsity caused by monotone
missing, we further propose a novel balancing-enhanced approach, BalanceNet, to
improve the stability and accuracy of the estimation methods. Extensive
experiments on two widely used benchmark datasets demonstrate the effectiveness
of our proposed methods.

</details>


### [141] [Euclidean Distance Matrix Completion via Asymmetric Projected Gradient Descent](https://arxiv.org/abs/2504.19530)
*Yicheng Li,Xinghua Sun*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Burer-Monteiro分解的梯度型算法——非对称投影梯度下降（APGD），用于解决欧氏距离矩阵补全（EDMC）问题。通过并行非相干矩阵补全框架，首次证明了在不需要样本分割的情况下，给定特定数量的随机观测后，该算法能实现全局收敛和精确恢复。数值实验显示算法在样本充足时表现优异，但在样本有限时性能下降较快。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决欧氏距离矩阵补全（EDMC）问题，尤其是在部分欧氏距离测量条件下重建点集配置。通过提出一种新的梯度算法，作者旨在克服现有方法的局限，如基于切线空间RIP和局部曲率的方法，提供更优的收敛保证。

Method: 方法基于Burer-Monteiro分解的非对称投影梯度下降（APGD），不需要依赖样本分割。通过引入新的上界替换随机图引理，在EDMC设定下建立了全局收敛理论。

Result: 理论分析表明，在给定特定数量的随机观测后，APGD能实现全局收敛和精确恢复。数值实验验证了在样本充足时的线性收敛行为，但样本有限时性能快速下降。

Conclusion: 结论指出，APGD在样本充足时表现优异，但在样本有限时性能受限。这暗示了隐式正则化的效果在APGD中被削弱，且其梯度方向的稳定需要比信息理论极限更多的样本。

Abstract: This paper proposes and analyzes a gradient-type algorithm based on
Burer-Monteiro factorization, called the Asymmetric Projected Gradient Descent
(APGD), for reconstructing the point set configuration from partial Euclidean
distance measurements, known as the Euclidean Distance Matrix Completion (EDMC)
problem. By paralleling the incoherence matrix completion framework, we show
for the first time that global convergence guarantee with exact recovery of
this routine can be established given $\mathcal{O}(\mu^2 r^3 \kappa^2 n \log
n)$ Bernoulli random observations without any sample splitting. Unlike
leveraging the tangent space Restricted Isometry Property (RIP) and local
curvature of the low-rank embedding manifold in some very recent works, our
proof provides new upper bounds to replace the random graph lemma under EDMC
setting. The APGD works surprisingly well and numerical experiments demonstrate
exact linear convergence behavior in rich-sample regions yet deteriorates fast
when compared with the performance obtained by optimizing the s-stress
function, i.e., the standard but unexplained non-convex approach for EDMC, if
the sample size is limited. While virtually matching our theoretical
prediction, this unusual phenomenon might indicate that: (i) the power of
implicit regularization is weakened when specified in the APGD case; (ii) the
stabilization of such new gradient direction requires substantially more
samples than the information-theoretic limit would suggest.

</details>


### [142] [Towards Faster and More Compact Foundation Models for Molecular Property Prediction](https://arxiv.org/abs/2504.19538)
*Yasir Ghunaim,Andrés Villa,Gergo Ignacz,Gyorgy Szekely,Motasem Alfarra,Bernard Ghanem*

Main category: cs.LG

TL;DR: 该论文提出了一种通过剪枝预训练模型来提升分子属性预测效率的方法，研究发现减少两个交互模块后模型大小降低32%，推理速度提升1.3倍，且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 尽管JMP基础模型在多领域下游任务中表现出色，但其在分子数据集上的微调仍需要大量计算资源和时间。为了提升效率，作者希望在不显著损失性能的前提下减少模型大小。

Method: 通过分析JMP模型中各层的贡献，发现后期交互模块的收益递减，因此采用剪枝策略减少预训练模型的参数，并在微调阶段评估效率和准确性。

Result: 去除两个交互模块后，模型大小减少了32%，推理速度提升了1.3倍，且性能下降极小，表明JMP-L存在参数冗余。

Conclusion: 研究表明，通过剪枝可以开发更轻量、快速且可扩展的分子和材料发现基础模型，同时保持性能。代码已开源。

Abstract: Advancements in machine learning for molecular property prediction have
improved accuracy but at the expense of higher computational cost and longer
training times. Recently, the Joint Multi-domain Pre-training (JMP) foundation
model has demonstrated strong performance across various downstream tasks with
reduced training time over previous models. Despite JMP's advantages,
fine-tuning it on molecular datasets ranging from small-scale to large-scale
requires considerable time and computational resources. In this work, we
investigate strategies to enhance efficiency by reducing model size while
preserving performance. To better understand the model's efficiency, we analyze
the layer contributions of JMP and find that later interaction blocks provide
diminishing returns, suggesting an opportunity for model compression. We
explore block reduction strategies by pruning the pre-trained model and
evaluating its impact on efficiency and accuracy during fine-tuning. Our
analysis reveals that removing two interaction blocks results in a minimal
performance drop, reducing the model size by 32% while increasing inference
throughput by 1.3x. These results suggest that JMP-L is over-parameterized and
that a smaller, more efficient variant can achieve comparable performance with
lower computational cost. Our study provides insights for developing lighter,
faster, and more scalable foundation models for molecular and materials
discovery. The code is publicly available at:
https://github.com/Yasir-Ghunaim/efficient-jmp.

</details>


### [143] [Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning](https://arxiv.org/abs/2504.19583)
*Hanlu Zhang,Yumeng Ma,Shuo Wang,Guiran Liu,Binrong Zhu*

Main category: cs.LG

TL;DR: 提出了一种结合图谱分析的大语言模型参数协同优化算法，旨在提升微调效率和训练中的结构感知能力，通过频域建模和谱正则化实现参数协同更新，实验结果验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型的微调效率和结构感知能力，解决训练中的参数扰动问题，增强模型的适应性和性能。

Method: 将预训练模型参数视为图节点，构建加权图并应用拉普拉斯谱分解，设计结合任务损失和谱正则化的联合损失函数，并在优化阶段引入谱滤波机制。

Result: 在多任务评估中表现优异，有效减少参数扰动并提升微调质量，同时保持模型整体性能。

Conclusion: 通过谱协同优化框架推动了大规模模型的高效训练方法，强调了结构信号处理在深度学习中的重要性，提供了增强语言模型适应性和性能的通用框架。

Abstract: This paper proposes a parameter collaborative optimization algorithm for
large language models, enhanced with graph spectral analysis. The goal is to
improve both fine-tuning efficiency and structural awareness during training.
In the proposed method, the parameters of a pre-trained language model are
treated as nodes in a graph. A weighted graph is constructed, and Laplacian
spectral decomposition is applied to enable frequency-domain modeling and
structural representation of the parameter space. Based on this structure, a
joint loss function is designed. It combines the task loss with a spectral
regularization term to facilitate collaborative updates among parameters. In
addition, a spectral filtering mechanism is introduced during the optimization
phase. This mechanism adjusts gradients in a structure-aware manner, enhancing
the model's training stability and convergence behavior. The method is
evaluated on multiple tasks, including traditional fine-tuning comparisons,
few-shot generalization tests, and convergence speed analysis. In all settings,
the proposed approach demonstrates superior performance. The experimental
results confirm that the spectral collaborative optimization framework
effectively reduces parameter perturbations and improves fine-tuning quality
while preserving overall model performance. This work contributes significantly
to the field of artificial intelligence by advancing parameter-efficient
training methodologies for large-scale models, reinforcing the importance of
structural signal processing in deep learning optimization, and offering a
robust, generalizable framework for enhancing language model adaptability and
performance.

</details>


### [144] [Quantifying Memory Utilization with Effective State-Size](https://arxiv.org/abs/2504.19561)
*Rom N. Parnichkun,Neehal Tumma,Armin W. Thomas,Alessandro Moro,Qi An,Taiji Suzuki,Atsushi Yamashita,Michael Poli,Stefano Massaroli*

Main category: cs.LG

TL;DR: 研究者提出了一种名为‘有效状态大小’（ESS）的定量指标，用于测量序列模型中的内存利用率，从而帮助设计更高效的模型架构。


<details>
  <summary>Details</summary>
Motivation: 随着序列模型设计空间的扩展，需要一种通用框架来分析其架构，尤其是内存利用机制。基于信号处理和控制理论的见解，研究者旨在量化模型如何存储过去信息以生成未来输出。

Method: 研究者开发了ESS指标，适用于具有输入不变和输入变化线性算子的系统，涵盖注意力、卷积和循环单元等计算单元。与传统的可视化方法或简单内存容量测量不同，ESS提供了可解释且可操作的量化结果。

Result: ESS可用于改进初始化策略、设计新的正则化方法，并通过模型蒸馏提升性能效率。此外，ESS揭示了不同架构下大型语言模型如何利用内存的差异，尤其是在上下文分隔符（如句尾标记）的影响方面。

Conclusion: ESS为理解内存利用的动态机制提供了有价值的洞见，有助于设计更高效、更有效的序列模型。

Abstract: The need to develop a general framework for architecture analysis is becoming
increasingly important, given the expanding design space of sequence models. To
this end, we draw insights from classical signal processing and control theory,
to develop a quantitative measure of \textit{memory utilization}: the internal
mechanisms through which a model stores past information to produce future
outputs. This metric, which we call \textbf{\textit{effective state-size}}
(ESS), is tailored to the fundamental class of systems with
\textit{input-invariant} and \textit{input-varying linear operators},
encompassing a variety of computational units such as variants of attention,
convolutions, and recurrences. Unlike prior work on memory utilization, which
either relies on raw operator visualizations (e.g. attention maps), or simply
the total \textit{memory capacity} (i.e. cache size) of a model, our metrics
provide highly interpretable and actionable measurements. In particular, we
show how ESS can be leveraged to improve initialization strategies, inform
novel regularizers and advance the performance-efficiency frontier through
model distillation. Furthermore, we demonstrate that the effect of context
delimiters (such as end-of-speech tokens) on ESS highlights cross-architectural
differences in how large language models utilize their available memory to
recall information. Overall, we find that ESS provides valuable insights into
the dynamics that dictate memory utilization, enabling the design of more
efficient and effective sequence models.

</details>


### [145] [Soft-Label Caching and Sharpening for Communication-Efficient Federated Distillation](https://arxiv.org/abs/2504.19602)
*Kitsuya Azuma,Takayuki Nishio,Yuichi Kitagawa,Wakako Nakano,Takahito Tanimura*

Main category: cs.LG

TL;DR: SCARLET框架通过同步软标签缓存和增强的熵减少聚合机制，显著降低了联邦学习的通信开销，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统联邦学习在通信开销高和模型异构性受限的问题，特别是蒸馏方法中冗余传输的不足。

Method: 结合同步软标签缓存和增强的熵减少聚合（Enhanced ERA）机制，减少冗余通信。

Result: 在保持准确性的同时，通信成本降低50%，并在非独立同分布数据场景中表现稳健。

Conclusion: SCARLET在通信效率和准确性上优于现有蒸馏联邦学习方法，提供了一个有效的解决方案。

Abstract: Federated Learning (FL) enables collaborative model training across
decentralized clients, enhancing privacy by keeping data local. Yet
conventional FL, relying on frequent parameter-sharing, suffers from high
communication overhead and limited model heterogeneity. Distillation-based FL
approaches address these issues by sharing predictions (soft-labels) instead,
but they often involve redundant transmissions across communication rounds,
reducing efficiency. We propose SCARLET, a novel framework integrating
synchronized soft-label caching and an enhanced Entropy Reduction Aggregation
(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing
cached soft-labels, achieving up to 50% reduction in communication costs
compared to existing methods while maintaining accuracy. Enhanced ERA can be
tuned to adapt to non-IID data variations, ensuring robust aggregation and
performance in diverse client scenarios. Experimental evaluations demonstrate
that SCARLET consistently outperforms state-of-the-art distillation-based FL
methods in terms of accuracy and communication efficiency. The implementation
of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.

</details>


### [146] [AI Alignment in Medical Imaging: Unveiling Hidden Biases Through Counterfactual Analysis](https://arxiv.org/abs/2504.19621)
*Haroui Ma,Francesco Quinzan,Theresa Willem,Stefan Bauer*

Main category: cs.LG

TL;DR: 本文提出一种统计框架评估医学影像ML模型对敏感属性的依赖性，利用反事实不变性概念，结合条件潜在扩散模型和统计假设检验，优于基准方法，提升AI在医疗领域的安全性。


<details>
  <summary>Details</summary>
Motivation: 医学影像ML模型的诊断能力强，但对偏见的敏感性可能影响其泛化性能，因此需评估其对敏感属性（如人口统计）的依赖性。

Method: 提出一种结合条件潜在扩散模型与统计假设检验的算法，无需直接获取反事实数据即可量化偏见。

Result: 在合成数据集及真实医学影像数据（如cheXpert和MIMIC-CXR）上验证，该方法符合反事实公平原则且优于基准方法。

Conclusion: 该框架为医学影像ML系统提供了一种鲁棒工具，确保其在跨人口统计组等场景中的泛化能力，是AI医疗安全的重要一步。

Abstract: Machine learning (ML) systems for medical imaging have demonstrated
remarkable diagnostic capabilities, but their susceptibility to biases poses
significant risks, since biases may negatively impact generalization
performance. In this paper, we introduce a novel statistical framework to
evaluate the dependency of medical imaging ML models on sensitive attributes,
such as demographics. Our method leverages the concept of counterfactual
invariance, measuring the extent to which a model's predictions remain
unchanged under hypothetical changes to sensitive attributes. We present a
practical algorithm that combines conditional latent diffusion models with
statistical hypothesis testing to identify and quantify such biases without
requiring direct access to counterfactual data. Through experiments on
synthetic datasets and large-scale real-world medical imaging datasets,
including \textsc{cheXpert} and MIMIC-CXR, we demonstrate that our approach
aligns closely with counterfactual fairness principles and outperforms standard
baselines. This work provides a robust tool to ensure that ML diagnostic
systems generalize well, e.g., across demographic groups, offering a critical
step towards AI safety in healthcare. Code:
https://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging.

</details>


### [147] [LODAP: On-Device Incremental Learning Via Lightweight Operations and Data Pruning](https://arxiv.org/abs/2504.19638)
*Biqing Duan,Qing Wang,Di Liu,Wei Zhou,Zhenli He,Shengfa Miao*

Main category: cs.LG

TL;DR: 论文提出了一种名为LODAP的增量学习框架，用于边缘设备高效学习新类别，通过引入轻量级操作模块（EIM）和数据剪枝策略，显著提升了精度并降低了模型复杂度和训练开销。


<details>
  <summary>Details</summary>
Motivation: 工业边缘系统需要在不依赖远程服务器的情况下增量学习新类别，解决模型复杂度和训练开销问题。

Method: 提出LODAP框架，核心是高效增量模块（EIM）和数据剪枝策略。EIM使用适配器等轻量操作学习新类别特征，数据剪枝减少训练数据量。

Result: 在CIFAR-100和Tiny-ImageNet上实验显示，LODAP比现有方法精度提升最多4.32%，模型复杂度降低约50%。

Conclusion: LODAP适用于边缘设备的机器学习，平衡了精度、复杂度和训练开销。

Abstract: Incremental learning that learns new classes over time after the model's
deployment is becoming increasingly crucial, particularly for industrial edge
systems, where it is difficult to communicate with a remote server to conduct
computation-intensive learning. As more classes are expected to learn after
their execution for edge devices. In this paper, we propose LODAP, a new
on-device incremental learning framework for edge systems. The key part of
LODAP is a new module, namely Efficient Incremental Module (EIM). EIM is
composed of normal convolutions and lightweight operations. During incremental
learning, EIM exploits some lightweight operations, called adapters, to
effectively and efficiently learn features for new classes so that it can
improve the accuracy of incremental learning while reducing model complexity as
well as training overhead. The efficiency of LODAP is further enhanced by a
data pruning strategy that significantly reduces the training data, thereby
lowering the training overhead. We conducted extensive experiments on the
CIFAR-100 and Tiny- ImageNet datasets. Experimental results show that LODAP
improves the accuracy by up to 4.32\% over existing methods while reducing
around 50\% of model complexity. In addition, evaluations on real edge systems
demonstrate its applicability for on-device machine learning. The code is
available at https://github.com/duanbiqing/LODAP.

</details>


### [148] [A Unified Benchmark of Federated Learning with Kolmogorov-Arnold Networks for Medical Imaging](https://arxiv.org/abs/2504.19639)
*Youngjoon Lee,Jinu Gong,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 论文评估了Kolmogorov-Arnold Networks (KAN)在联邦学习中的表现，发现其能替代传统MLP并表现更优，尤其在医疗成像任务中。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索KAN在联邦学习中作为隐私保护工具的潜力，特别是在医疗领域的数据敏感性需求下。

Method: 通过六种先进的联邦学习算法在血细胞分类数据集上比较KAN与传统MLP，分析超参数对KAN性能的影响。

Result: KAN在非独立同分布数据下表现优异，优化宽度和最小深度能进一步提升性能，适用于分布式医疗应用。

Conclusion: KAN是联邦学习中一个高效且隐私保护的替代方案，特别适用于医疗成像任务。

Abstract: Federated Learning (FL) enables model training across decentralized devices
without sharing raw data, thereby preserving privacy in sensitive domains like
healthcare. In this paper, we evaluate Kolmogorov-Arnold Networks (KAN)
architectures against traditional MLP across six state-of-the-art FL algorithms
on a blood cell classification dataset. Notably, our experiments demonstrate
that KAN can effectively replace MLP in federated environments, achieving
superior performance with simpler architectures. Furthermore, we analyze the
impact of key hyperparameters-grid size and network architecture-on KAN
performance under varying degrees of Non-IID data distribution. Additionally,
our ablation studies reveal that optimizing KAN width while maintaining minimal
depth yields the best performance in federated settings. As a result, these
findings establish KAN as a promising alternative for privacy-preserving
medical imaging applications in distributed healthcare. To the best of our
knowledge, this is the first comprehensive benchmark of KAN in FL settings for
medical imaging task.

</details>


### [149] [Intelligent4DSE: Optimizing High-Level Synthesis Design Space Exploration with Graph Neural Networks and Large Language Models](https://arxiv.org/abs/2504.19649)
*Lei Xu,Shanshan Wang,Emmanuel Casseau,Chenglong Xiao*

Main category: cs.LG

TL;DR: 该论文提出了一种名为CoGNNs-LLMEA的框架，结合图神经网络和进化算法，有效提升了HLS设计空间探索的预测精度，减少了实现误差。


<details>
  <summary>Details</summary>
Motivation: 现有HLS预测模型过于关注结构复杂性和训练损失，忽略任务特性，且进化算法需大量领域知识。论文旨在解决这些局限性。

Method: 提出CoGNNs-LLMEA框架，集成了任务自适应消息传递的图神经网络和基于大语言模型的进化算法，直接利用源代码中间表示预测结果。

Result: CoGNNs在HLS后QoR预测中达到最佳精度，延迟和资源利用率的平均预测误差分别降低了2.8倍和3.4倍。

Conclusion: 该框架有效连接高层抽象与物理实现，显著提升预测准确性，为EDA优化提供新思路。

Abstract: High-level synthesis (HLS) design space exploration (DSE) is an optimization
process in electronic design automation (EDA) that systematically explores
high-level design configurations to achieve Pareto-optimal hardware
implementations balancing performance, area, and power (PPA). To optimize this
process, HLS prediction tasks often employ message-passing neural networks
(MPNNs), leveraging complex architectures to achieve high accuracy. These
predictors serve as evaluators in the DSE process, effectively bypassing the
time-consuming estimations traditionally required by HLS tools. However,
existing models often prioritize structural complexity and minimization of
training loss, overlooking task-specific characteristics. Additionally, while
evolutionary algorithms are widely used in DSE, they typically require
extensive domain-specific knowledge to design effective crossover and mutation
operators. To address these limitations, we propose CoGNNs-LLMEA, a framework
that integrates a graph neural network with task-adaptive message passing and a
large language model-enhanced evolutionary algorithm. As a predictive model,
CoGNNs directly leverages intermediate representations generated from source
code after compiler front-end processing, enabling prediction of quality of
results (QoR) without invoking HLS tools. Due to its strong adaptability to
tasks, CoGNNs can be tuned to predict post-HLS and post-implementation
outcomes, effectively bridging the gap between high-level abstractions and
physical implementation characteristics. CoGNNs achieves state-of-the-art
prediction accuracy in post-HLS QoR prediction, reducing mean prediction errors
by 2.8$\times$ for latency and 3.4$\times$ for resource utilization compared to
baseline models.

</details>


### [150] [Hardware/Software Co-Design of RISC-V Extensions for Accelerating Sparse DNNs on FPGAs](https://arxiv.org/abs/2504.19659)
*Muhammad Sabih,Abrarul Karim,Jakob Wittmann,Frank Hannig,Jürgen Teich*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的RISC-V扩展，用于加速包含半结构化和非结构化稀疏性的DNN模型，通过硬件/软件协同设计实现了显著的加速效果。


<details>
  <summary>Details</summary>
Motivation: RISC-V的可定制性使其成为加速DNN的有力选择，但需通过指令集扩展和硬件/软件协同设计来高效利用这些机会。

Method: 论文提出了两种加速稀疏性的方法：利用FPGA细粒度配置的半结构化稀疏性跳过计算，以及针对非结构化稀疏性的可变周期乘法累加单元。

Result: 实验表明，非结构化和半结构化稀疏性加速器分别实现了3倍和4倍的加速效果，而结合设计则达到了5倍加速。

Conclusion: 该设计在占用少量额外FPGA资源的情况下，显著提升了DNN模型的加速性能，适用于小型FPGA的TinyML应用。

Abstract: The customizability of RISC-V makes it an attractive choice for accelerating
deep neural networks (DNNs). It can be achieved through instruction set
extensions and corresponding custom functional units. Yet, efficiently
exploiting these opportunities requires a hardware/software co-design approach
in which the DNN model, software, and hardware are designed together. In this
paper, we propose novel RISC-V extensions for accelerating DNN models
containing semi-structured and unstructured sparsity. While the idea of
accelerating structured and unstructured pruning is not new, our novel design
offers various advantages over other designs. To exploit semi-structured
sparsity, we take advantage of the fine-grained (bit-level) configurability of
FPGAs and suggest reserving a few bits in a block of DNN weights to encode the
information about sparsity in the succeeding blocks. The proposed custom
functional unit utilizes this information to skip computations. To exploit
unstructured sparsity, we propose a variable cycle sequential
multiply-and-accumulate unit that performs only as many multiplications as the
non-zero weights. Our implementation of unstructured and semi-structured
pruning accelerators can provide speedups of up to a factor of 3 and 4,
respectively. We then propose a combined design that can accelerate both types
of sparsities, providing speedups of up to a factor of 5. Our designs consume a
small amount of additional FPGA resources such that the resulting co-designs
enable the acceleration of DNNs even on small FPGAs. We benchmark our designs
on standard TinyML applications such as keyword spotting, image classification,
and person detection.

</details>


### [151] [A Tripartite Perspective on GraphRAG](https://arxiv.org/abs/2504.19667)
*Michael Banf,Johannes Kuhn*

Main category: cs.LG

TL;DR: 这篇论文提出了一种结合大语言模型（LLMs）和三方知识图谱（Tripartite-GraphRAG）的新方法，针对知识密集型任务中的幻觉、来源追溯和知识更新问题，通过领域特定概念锚定的文本预处理和统计驱动嵌入相似性评估，优化提示信息密度、覆盖率和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在工业自动化和医疗等需要高事实准确性的领域存在幻觉、来源不可追溯和知识更新滞后的局限，结合知识图谱可解决这些问题，但构建知识图谱本身是一大挑战。

Method: 提出Tripartite-GraphRAG方法，通过领域特定概念锚定的源文档预处理构建三方知识图谱，将LLM提示创建转化为无监督节点分类问题，基于马尔可夫随机场思想。

Result: 在医疗用例中实验表明，该方法能优化提示的信息密度、覆盖率和排列，缩短提示长度，从而可能降低成本并提高LLM输出的可靠性与一致性。

Conclusion: Tripartite-GraphRAG为解决LLMs在知识密集型任务中的局限性提供了一种有效途径，尤其在医疗领域展现出潜力。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities across
various domains, yet they struggle with knowledge-intensive tasks in areas that
demand factual accuracy, e.g. industrial automation and healthcare. Key
limitations include their tendency to hallucinate, lack of source traceability
(provenance), and challenges in timely knowledge updates. Combining language
models with knowledge graphs (GraphRAG) offers promising avenues for overcoming
these deficits. However, a major challenge lies in creating such a knowledge
graph in the first place. Here, we propose a novel approach that combines LLMs
with a tripartite knowledge graph representation, which is constructed by
connecting complex, domain-specific objects via a curated ontology of
corresponding, domain-specific concepts to relevant sections within chunks of
text through a concept-anchored pre-analysis of source documents starting from
an initial lexical graph. As a consequence, our Tripartite-GraphRAG approach
implements: i) a concept-specific, information-preserving pre-compression of
textual chunks; ii) allows for the formation of a concept-specific relevance
estimation of embedding similarities grounded in statistics; and iii) avoids
common challenges w.r.t. continuous extendability, such as the need for entity
resolution and deduplication. By applying a transformation to the knowledge
graph, we formulate LLM prompt creation as an unsupervised node classification
problem, drawing on ideas from Markov Random Fields. We evaluate our approach
on a healthcare use case, involving multi-faceted analyses of patient anamneses
given a set of medical concepts as well as clinical literature. Experiments
indicate that it can optimize information density, coverage, and arrangement of
LLM prompts while reducing their lengths, which may lead to reduced costs and
more consistent and reliable LLM outputs.

</details>


### [152] [Graph Fourier Transformer with Structure-Frequency Information](https://arxiv.org/abs/2504.19740)
*Yonghui Zhai,Yang Zhang,Minghao Shang,Lihua Pang,Yaxin Ren*

Main category: cs.LG

TL;DR: 论文提出Grafourierformer，通过结合图傅里叶变换与图变换器，优化自注意力机制，同时考虑图的结构和节点频率信息，显著提升图分类和节点分类任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有图变换器（GT）的自注意力机制忽略图的归纳偏置，仅从结构角度补偿，性能欠佳。需同时利用结构和频率信息以更好地优化模型。

Method: 提出Grafourierformer：1）用图拉普拉斯矩阵特征值构建特征矩阵掩码；2）结合傅里叶正/逆变换提取节点高低频特征及能量；3）通过频率-能量矩阵过滤掩码，使注意力头自适应区分全局趋势与局部细节。

Result: 在多个基准测试中，Grafourierformer在图分类和节点分类任务上均优于GNN和GT模型，消融实验验证了方法的有效性和必要性。

Conclusion: 通过融合图结构与频率信息，Grafourierformer显著提升模型性能，为图神经网络设计提供了新思路。

Abstract: Graph Transformers (GTs) have shown advantages in numerous graph structure
tasks but their self-attention mechanism ignores the generalization bias of
graphs, with existing methods mainly compensating for this bias from aspects
like position encoding, attention bias and relative distance yet still having
sub-optimal performance and being insufficient by only considering the
structural perspective of generalization bias. To address this, this paper
proposes Grafourierformer, which innovatively combines GT with inductive bias
containing Frequency-Structure information by applying Graph Fourier Transform
to the Attention Matrix: specifically, eigenvalues from the Graph Laplacian
matrix are used to construct an Eigenvalue matrix mask (reflecting node
positions and structural relationships with neighboring nodes to enable
consideration of node range structural characteristics and focus on local graph
details), and inverse Fourier transform is employed to extract node
high-frequency and low-frequency features, calculate low-frequency and
high-frequency energy, and construct a node frequency-energy matrix to filter
the eigenvalue matrix mask, allowing attention heads to incorporate both graph
structural information and node frequency information optimization, adaptively
distinguish global trends from local details, and effectively suppress
redundant information interference. Extensive experiments on various benchmarks
show Grafourierformer consistently outperforms GNN and GT-based models in graph
classification and node classification tasks, with ablation experiments further
validating the effectiveness and necessity of the method. Codes are available
at https://github.com/Arichibald/Grafourierformer.git

</details>


### [153] [FineQ: Software-Hardware Co-Design for Low-Bit Fine-Grained Mixed-Precision Quantization of LLMs](https://arxiv.org/abs/2504.19746)
*Xilong Xie,Liang Wang,Limin Xiao,Meng Han,Lin Sun,Shuai Zheng,Xiangrong Xu*

Main category: cs.LG

TL;DR: FineQ提出了一种软硬件协同设计的低比特细粒度混合精度量化方法，以解决大语言模型（LLMs）量化中内存与计算资源需求高的问题。


<details>
  <summary>Details</summary>
Motivation: 当前单精度量化方法在超低位宽下精度损失严重，而混合精度量化方法因粗粒度分组导致内存开销大或精度下降。

Method: FineQ通过细粒度权重分区、异常值保护机制和编码方案，配合基于时序编码的加速器设计，平衡了精度与内存开销。

Result: 在相近平均位宽下，FineQ的模型精度优于现有混合精度量化方法，加速器能效提升1.79倍，脉动阵列面积减少61.2%。

Conclusion: FineQ通过细粒度量化与硬件优化，显著提升了LLMs的效率与精度，为资源受限场景提供了可行解决方案。

Abstract: Large language models (LLMs) have significantly advanced the natural language
processing paradigm but impose substantial demands on memory and computational
resources. Quantization is one of the most effective ways to reduce memory
consumption of LLMs. However, advanced single-precision quantization methods
experience significant accuracy degradation when quantizing to ultra-low bits.
Existing mixed-precision quantization methods are quantized by groups with
coarse granularity. Employing high precision for group data leads to
substantial memory overhead, whereas low precision severely impacts model
accuracy. To address this issue, we propose FineQ, software-hardware co-design
for low-bit fine-grained mixed-precision quantization of LLMs. First, FineQ
partitions the weights into finer-grained clusters and considers the
distribution of outliers within these clusters, thus achieving a balance
between model accuracy and memory overhead. Then, we propose an outlier
protection mechanism within clusters that uses 3 bits to represent outliers and
introduce an encoding scheme for index and data concatenation to enable aligned
memory access. Finally, we introduce an accelerator utilizing temporal coding
that effectively supports the quantization algorithm while simplifying the
multipliers in the systolic array. FineQ achieves higher model accuracy
compared to the SOTA mixed-precision quantization algorithm at a close average
bit-width. Meanwhile, the accelerator achieves up to 1.79x energy efficiency
and reduces the area of the systolic array by 61.2%.

</details>


### [154] [If Concept Bottlenecks are the Question, are Foundation Models the Answer?](https://arxiv.org/abs/2504.19774)
*Nicola Debole,Pietro Barbiero,Francesco Giannini,Andrea Passeggini,Stefano Teso,Emanuele Marconato*

Main category: cs.LG

TL;DR: 论文研究了使用基础模型（如VLM）替代专家标注对概念瓶颈模型（CBM）概念学习质量的影响，发现概念准确性与质量并不强相关。


<details>
  <summary>Details</summary>
Motivation: 传统CBM依赖专家标注来确保高质量概念，但标注成本高且不易获取。研究探索用基础模型提供弱监督的可行性及其影响。

Method: 通过VLM-CBM架构，用基础模型生成的弱监督替代专家标注，并设计实验度量学习到的概念质量。

Result: 发现VLM监督与专家标注在某些任务上差异显著，且概念准确性与质量无明显相关性。

Conclusion: 虽然VLM-CBM能减少对专家标注的依赖，但需进一步优化以确保概念质量。

Abstract: Concept Bottleneck Models (CBMs) are neural networks designed to conjoin high
performance with ante-hoc interpretability. CBMs work by first mapping inputs
(e.g., images) to high-level concepts (e.g., visible objects and their
properties) and then use these to solve a downstream task (e.g., tagging or
scoring an image) in an interpretable manner. Their performance and
interpretability, however, hinge on the quality of the concepts they learn. The
go-to strategy for ensuring good quality concepts is to leverage expert
annotations, which are expensive to collect and seldom available in
applications. Researchers have recently addressed this issue by introducing
"VLM-CBM" architectures that replace manual annotations with weak supervision
from foundation models. It is however unclear what is the impact of doing so on
the quality of the learned concepts. To answer this question, we put
state-of-the-art VLM-CBMs to the test, analyzing their learned concepts
empirically using a selection of significant metrics. Our results show that,
depending on the task, VLM supervision can sensibly differ from expert
annotations, and that concept accuracy and quality are not strongly correlated.
Our code is available at https://github.com/debryu/CQA.

</details>


### [155] [Learning Brenier Potentials with Convex Generative Adversarial Neural Networks](https://arxiv.org/abs/2504.19779)
*Claudia Drygala,Hanno Gottschalk,Thomas Kruse,Ségolène Martin,Annika Mütze*

Main category: cs.LG

TL;DR: 研究开发了一种生成对抗神经网络，用于学习Brenier势能，结合ReCU网络和对抗训练，确保网络严格凸性，并在理论和实验中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: Brenier势能在概率测度间的传输映射中具有重要作用，但其统计学习理论尚未充分开发。本文旨在填补这一空白，通过神经网络学习Brenier势能，并确保学习的严格凸性。

Method: 采用ReCU网络（立方激活函数）近似Brenier势能，提出对抗训练框架，结合判别器的交叉熵损失和凸性惩罚项，保证网络的严格凸性。

Result: 理论分析和实验验证表明，该方法能有效学习严格凸的Brenier势能，凸性惩罚项在训练过程中逐渐失效，网络自动学习到凸性。

Conclusion: 本文提出的对抗训练方法能够稳定学习Brenier势能，为生成对抗网络的理论和应用提供了新思路。

Abstract: Brenier proved that under certain conditions on a source and a target
probability measure there exists a strictly convex function such that its
gradient is a transport map from the source to the target distribution. This
function is called the Brenier potential. Furthermore, detailed information on
the H\"older regularity of the Brenier potential is available. In this work we
develop the statistical learning theory of generative adversarial neural
networks that learn the Brenier potential. As by the transformation of
densities formula, the density of the generated measure depends on the second
derivative of the Brenier potential, we develop the universal approximation
theory of ReCU networks with cubic activation $\mathtt{ReCU}(x)=\max\{0,x\}^3$
that combines the favorable approximation properties of H\"older functions with
a Lipschitz continuous density. In order to assure the convexity of such
general networks, we introduce an adversarial training procedure for a
potential function represented by the ReCU networks that combines the classical
discriminator cross entropy loss with a penalty term that enforces (strict)
convexity. We give a detailed decomposition of learning errors and show that
for a suitable high penalty parameter all networks chosen in the adversarial
min-max optimization problem are strictly convex. This is further exploited to
prove the consistency of the learning procedure for (slowly) expanding network
capacity. We also implement the described learning algorithm and apply it to a
number of standard test cases from Gaussian mixture to image data as target
distributions. As predicted in theory, we observe that the convexity loss
becomes inactive during the training process and the potentials represented by
the neural networks have learned convexity.

</details>


### [156] [Heterophily-informed Message Passing](https://arxiv.org/abs/2504.19785)
*Haishan Wang,Arno Solin,Vikas Garg*

Main category: cs.LG

TL;DR: 该论文提出了一种新型的图神经网络（GNN）方案，通过调节消息聚合的类型和范围来解决GNN因同质性假设而导致的过度平滑问题，同时保留信息的低频和高频成分。该方法仅依赖学习嵌入，无需辅助标签，并在多个数据集和GNN架构上验证了性能提升，还展示了在分子生成任务中的显著改进。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNN）的同质性假设容易导致过度平滑问题，限制了其在异质性场景中的应用。论文旨在通过局部调节消息传递策略，解决这一问题并扩展GNN的应用范围，如生成建模。

Method: 提出了一种基于学习嵌入的消息聚合调节方案，通过局部控制消息传递的类型和范围，保留信息的低频和高频成分，无需依赖辅助标签。

Result: 在多个数据集和GNN架构上验证了性能提升，揭示了标准分类基准中的异质性模式，并在分子生成任务中展示了显著的性能改进。

Conclusion: 该方案有效缓解了GNN的过度平滑问题，扩展了其在异质性场景和生成任务中的应用潜力，具有广泛的实用性。

Abstract: Graph neural networks (GNNs) are known to be vulnerable to oversmoothing due
to their implicit homophily assumption. We mitigate this problem with a novel
scheme that regulates the aggregation of messages, modulating the type and
extent of message passing locally thereby preserving both the low and
high-frequency components of information. Our approach relies solely on learnt
embeddings, obviating the need for auxiliary labels, thus extending the
benefits of heterophily-aware embeddings to broader applications, e.g.,
generative modelling. Our experiments, conducted across various data sets and
GNN architectures, demonstrate performance enhancements and reveal heterophily
patterns across standard classification benchmarks. Furthermore, application to
molecular generation showcases notable performance improvements on
chemoinformatics benchmarks.

</details>


### [157] [Contextures: The Mechanism of Representation Learning](https://arxiv.org/abs/2504.19792)
*Runtian Zhai*

Main category: cs.LG

TL;DR: 该论文提出了"上下文理论"来数学化表征学习的机制，指出单纯增加模型规模会导致收益递减，需通过设计更好的上下文（context）推动进展。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型在表征学习方面虽表现优异，但其学习机制缺乏科学理解，尤其在模型规模扩大收益递减的背景下，亟需理论指导以优化预训练方法。

Method: 提出统一框架"上下文理论"，定义表征通过输入X与上下文变量A的关联学习；提出两种通用目标（SVME和KISE）学习上下文，并证明统计学习边界。

Result: 证明当编码器捕获X与A的最大关联信息（即学习到"上下文"）时，对兼容任务最优；上下文效用在其关联强度适中时最高。

Conclusion: 模型规模扩大需配合更好的上下文设计，论文为多类预训练方法提供理论支持，并提出了混合上下文及应对数据分布迁移的方法。

Abstract: This dissertation establishes the contexture theory to mathematically
characterize the mechanism of representation learning, or pretraining. Despite
the remarkable empirical success of foundation models, it is not very clear
what representations they learn, and why these representations are useful for
various downstream tasks. A scientific understanding of representation learning
is critical, especially at this point when scaling up the model size is
producing diminishing returns, and designing new pretraining methods is
imperative for further progress.
  Prior work treated different representation learning methods quite
differently, whereas the contexture theory provides a unified framework for
analyzing these methods. The central argument is that a representation is
learned from the association between the input X and a context variable A. We
prove that if an encoder captures the maximum information of this association,
in which case we say that the encoder learns the contexture, then it will be
optimal on the class of tasks that are compatible with the context. We also
show that a context is the most useful when the association between X and A is
neither too strong nor too weak. The important implication of the contexture
theory is that increasing the model size alone will achieve diminishing
returns, and further advancements require better contexts.
  We demonstrate that many pretraining objectives can learn the contexture,
including supervised learning, self-supervised learning, generative models,
etc. Then, we introduce two general objectives -- SVME and KISE, for learning
the contexture. We also show how to mix multiple contexts together, an
effortless way to create better contexts from existing ones. Then, we prove
statistical learning bounds for representation learning. Finally, we discuss
the effect of the data distribution shift from pretraining to the downstream
task.

</details>


### [158] [Hierarchical Uncertainty-Aware Graph Neural Network](https://arxiv.org/abs/2504.19820)
*Yoonhyuk Choi,Chong-Kwon Kim*

Main category: cs.LG

TL;DR: 该论文提出了一个名为HU-GNN的新型图神经网络架构，通过结合多尺度表征学习、不确定性估计和自监督嵌入多样性，提升了模型的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前图神经网络在研究局部不确定性和利用图层次结构方面缺乏协同整合，因此作者旨在提出一个统一框架来解决这一问题。

Method: HU-GNN通过自适应节点聚类和多尺度不确定性估计，结合鲁棒的消息传递机制和注意力加权，有效减少噪声和对抗扰动。

Result: 实验表明，HU-GNN在标准基准测试中实现了最先进的鲁棒性和可解释性。

Conclusion: HU-GNN通过统一多尺度学习和不确定性估计，为图神经网络提供了一个鲁棒且可解释的解决方案。

Abstract: Recent research on graph neural networks (GNNs) has explored mechanisms for
capturing local uncertainty and exploiting graph hierarchies to mitigate data
sparsity and leverage structural properties. However, the synergistic
integration of these two approaches remains underexplored. In this work, we
introduce a novel architecture, the Hierarchical Uncertainty-Aware Graph Neural
Network (HU-GNN), which unifies multi-scale representation learning, principled
uncertainty estimation, and self-supervised embedding diversity within a single
end-to-end framework. Specifically, HU-GNN adaptively forms node clusters and
estimates uncertainty at multiple structural scales from individual nodes to
higher levels. These uncertainty estimates guide a robust message-passing
mechanism and attention weighting, effectively mitigating noise and adversarial
perturbations while preserving predictive accuracy on both node- and
graph-level tasks. We also offer key theoretical contributions, including a
probabilistic formulation, rigorous uncertainty-calibration guarantees, and
formal robustness bounds. Finally, by incorporating recent advances in graph
contrastive learning, HU-GNN maintains diverse, structurally faithful
embeddings. Extensive experiments on standard benchmarks demonstrate that our
model achieves state-of-the-art robustness and interpretability.

</details>


### [159] [Mjölnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density](https://arxiv.org/abs/2504.19822)
*Minjong Cheon*

Main category: cs.LG

TL;DR: 该论文提出了Mj"olnir，一种基于深度学习的全球闪电密度参数化框架，通过InceptionNeXt架构和多任务学习策略，准确预测闪电活动，并在全球范围内取得了高相关性结果。


<details>
  <summary>Details</summary>
Motivation: 近年来AI天气预测模型的成功激发了将深度学习应用于地球系统建模的兴趣。本研究旨在开发一种数据驱动的全球闪电参数化方案，以提升下一代AI地球系统模型（AI-ESMs）的预测能力。

Method: 使用ERA5大气预测因子和WWLLN闪电观测数据，以InceptionNeXt为模型主干，结合SENet和多任务学习策略，同时预测闪电发生和强度。

Result: Mj"olnir能准确重现闪电活动的全球分布、季节变化和区域特征，年平均值皮尔逊相关系数达0.96。

Conclusion: 该模型不仅是一种有效的全球闪电数据驱动参数化工具，还为下一代AI地球系统模型提供了有前景的方案。

Abstract: Recent advances in AI-based weather forecasting models, such as FourCastNet,
Pangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep
learning to emulate complex atmospheric dynamics. Building on this momentum, we
propose Mj\"olnir, a novel deep learning-based framework for global lightning
flash density parameterization. Trained on ERA5 atmospheric predictors and
World Wide Lightning Location Network (WWLLN) observations at a daily temporal
resolution and 1 degree spatial resolution, Mj\"olnir captures the nonlinear
mapping between large-scale environmental conditions and lightning activity.
The model architecture is based on the InceptionNeXt backbone with SENet, and a
multi-task learning strategy to simultaneously predict lightning occurrence and
magnitude. Extensive evaluations yield that Mollnir accurately reproduces the
global distribution, seasonal variability, and regional characteristics of
lightning activity, achieving a global Pearson correlation coefficient of 0.96
for annual mean fields. These results suggest that Mj\"olnir serves not only as
an effective data-driven global lightning parameterization but also as a
promising AI-based scheme for next-generation Earth system models (AI-ESMs).

</details>


### [160] [TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate](https://arxiv.org/abs/2504.19874)
*Amir Zandieh,Majid Daliri,Majid Hadian,Vahab Mirrokni*

Main category: cs.LG

TL;DR: TurboQuant是一种新型的向量量化方法，通过随机旋转和高维坐标独立性实现近最优的失真率，改进了现有方法在MSE和内积失真上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有向量量化方法在多维数据中的失真率不理想，尤其是在MSE和内积失真上存在矛盾或性能不足。TurboQuant旨在解决这一问题，提供更优的量化方案。

Method: TurboQuant采用数据无关的随机旋转技术，利用高维坐标的独立性和Beta分布特性，先使用MSE最优量化器，再通过1位QJL变换校正内积偏差。

Result: TurboQuant在KV缓存量化中实现3.5位/通道的绝对质量中性，2.5位/通道近乎无损；在最近邻搜索任务中，召回率优于现有方法且索引时间趋近于零。

Conclusion: TurboQuant以近最优失真率和无偏内积估计为特点，为高维向量量化提供了高效的解决方案，理论和实验表现均显著优于现有方法。

Abstract: Vector quantization, a problem rooted in Shannon's source coding theory, aims
to quantize high-dimensional Euclidean vectors while minimizing distortion in
their geometric structure. We propose TurboQuant to address both mean-squared
error (MSE) and inner product distortion, overcoming limitations of existing
methods that fail to achieve optimal distortion rates. Our data-oblivious
algorithms, suitable for online applications, achieve near-optimal distortion
rates (within a small constant factor) across all bit-widths and dimensions.
TurboQuant achieves this by randomly rotating input vectors, inducing a
concentrated Beta distribution on coordinates, and leveraging the
near-independence property of distinct coordinates in high dimensions to simply
apply optimal scalar quantizers per each coordinate. Recognizing that
MSE-optimal quantizers introduce bias in inner product estimation, we propose a
two-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL
(QJL) transform on the residual, resulting in an unbiased inner product
quantizer. We also provide a formal proof of the information-theoretic lower
bounds on best achievable distortion rate by any vector quantizer,
demonstrating that TurboQuant closely matches these bounds, differing only by a
small constant ($\approx 2.7$) factor. Experimental results validate our
theoretical findings, showing that for KV cache quantization, we achieve
absolute quality neutrality with 3.5 bits per channel and marginal quality
degradation with 2.5 bits per channel. Furthermore, in nearest neighbor search
tasks, our method outperforms existing product quantization techniques in
recall while reducing indexing time to virtually zero.

</details>


### [161] [Attention Mechanism, Max-Affine Partition, and Universal Approximation](https://arxiv.org/abs/2504.19901)
*Hude Liu,Jerry Yao-Chieh Hu,Zhao Song,Han Liu*

Main category: cs.LG

TL;DR: 单层单头自注意力和交叉注意力机制具有通用逼近能力，仅需最小附加结构即可近似连续函数或勒贝格可积函数。


<details>
  <summary>Details</summary>
Motivation: 探索单头注意力机制的通用逼近潜力，验证其在数学上的表达能力，尤其是在最小结构下的功能。

Method: 通过将单头注意力解释为输入域分区机制，设计注意力权重以模拟目标函数，结合线性变换和数学证明。

Result: 单层自注意力可以$L_\infty$-范数逼近连续函数，$L_p$-范数逼近勒贝格可积函数；交叉注意力首次达到相同效果。

Conclusion: 单头注意力机制在理论上是通用逼近器，为简化注意力架构提供了理论支持。

Abstract: We establish the universal approximation capability of single-layer,
single-head self- and cross-attention mechanisms with minimal attached
structures. Our key insight is to interpret single-head attention as an input
domain-partition mechanism that assigns distinct values to subregions. This
allows us to engineer the attention weights such that this assignment imitates
the target function. Building on this, we prove that a single self-attention
layer, preceded by sum-of-linear transformations, is capable of approximating
any continuous function on a compact domain under the $L_\infty$-norm.
Furthermore, we extend this construction to approximate any Lebesgue integrable
function under $L_p$-norm for $1\leq p <\infty$. Lastly, we also extend our
techniques and show that, for the first time, single-head cross-attention
achieves the same universal approximation guarantees.

</details>


### [162] [Convergence Analysis of Asynchronous Federated Learning with Gradient Compression for Non-Convex Optimization](https://arxiv.org/abs/2504.19903)
*Diying Yang,Yingwei Hou,Danyang Xiao,Weigang Wu*

Main category: cs.LG

TL;DR: 该论文研究了梯度压缩在异步联邦学习（FL）中的收敛行为，分析了异步框架AsynFL和改进框架AsynFLC及AsynFLC-EF的收敛性，发现误差反馈（EF）能有效减少异步延迟对梯度估计的影响。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏对异步FL中梯度压缩和误差反馈技术的系统性研究，本文旨在填补这一空白，分析不同框架下的收敛行为。

Method: 研究了异步FL的两种框架（AsynFL和AsynFLC），并进一步整合了误差反馈（EF）以改进梯度压缩的效果。

Result: 分析表明异步延迟会放大压缩带来的方差，EF能有效减少这种影响，使AsynFLC-EF的收敛率与AsynFL相当。实验验证了分析结果。

Conclusion: 误差反馈能够显著减轻异步延迟对梯度压缩的影响，提升FL的收敛性能，尤其在数据异构性高的情况下效果更明显。

Abstract: Gradient compression is an effective technique for reducing communication
costs in federated learning (FL), and error feedback (EF) is usually adopted to
remedy the compression errors. However, there remains a lack of systematic
study on these techniques in asynchronous FL. In this paper, we fill this gap
by analyzing the convergence behaviors of FL under different frameworks. We
firstly consider a basic asynchronous FL framework AsynFL, and provide an
improved convergence analysis that relies on fewer assumptions and yields a
superior convergence rate than prior studies. Then, we consider a variant
framework with gradient compression, AsynFLC. We show sufficient conditions for
its convergence to the optimum, indicating the interaction between asynchronous
delay and compression rate. Our analysis also demonstrates that asynchronous
delay amplifies the variance caused by compression, thereby hindering
convergence, and such an impact is exacerbated by high data heterogeneity.
Furthermore, we study the convergence of AsynFLC-EF, the framework that further
integrates EF. We prove that EF can effectively reduce the variance of gradient
estimation despite asynchronous delay, which enables AsynFLC-EF to match the
convergence rate of AsynFL. We also show that the impact of asynchronous delay
on EF is limited to slowing down the higher-order convergence term.
Experimental results substantiate our analytical findings very well.

</details>


### [163] [Robust Federated Personalised Mean Estimation for the Gaussian Mixture Model](https://arxiv.org/abs/2504.19955)
*Malhar A. Managoli,Vinod M. Prabhakaran,Suhas Diggavi*

Main category: cs.LG

TL;DR: 论文提出了一种个性化均值估计算法，针对联邦学习中异构数据和部分客户端数据损坏的问题，给出了误差近似线性依赖于损坏与未损坏样本比例的算法，并进行了理论下界分析。


<details>
  <summary>Details</summary>
Motivation: 研究在联邦学习中结合个性化处理异构数据和数据损坏鲁棒性，特别是部分客户端数据损坏的情况，以解决实际应用中常见的挑战。

Method: 基于高斯混合模型的个性化均值估计，设计了一个算法，其误差与损坏与未损坏样本的比例几乎呈线性关系。

Result: 算法在理论和实验上展示了误差与损坏比例的线性依赖关系，并通过下界证明了其性能接近最优。

Conclusion: 该方法在处理联邦学习中的异构数据和数据损坏问题上具有潜力，但仍需进一步研究以缩小理论与实际之间的常数差距。

Abstract: Federated learning with heterogeneous data and personalization has received
significant recent attention. Separately, robustness to corrupted data in the
context of federated learning has also been studied. In this paper we explore
combining personalization for heterogeneous data with robustness, where a
constant fraction of the clients are corrupted. Motivated by this broad
problem, we formulate a simple instantiation which captures some of its
difficulty. We focus on the specific problem of personalized mean estimation
where the data is drawn from a Gaussian mixture model. We give an algorithm
whose error depends almost linearly on the ratio of corrupted to uncorrupted
samples, and show a lower bound with the same behavior, albeit with a gap of a
constant factor.

</details>


### [164] [Transfer Learning Under High-Dimensional Network Convolutional Regression Model](https://arxiv.org/abs/2504.19979)
*Liyuan Wang,Jiachen Chen,Kathryn L. Lunetta,Danyang Huang,Huimin Cheng,Debarghya Mukherjee*

Main category: cs.LG

TL;DR: 论文提出了一种基于网络卷积回归（NCR）的高维迁移学习框架，旨在解决网络化数据中的依赖性问题，并通过理论和实证分析验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在处理独立设置下的分布偏移时表现良好，但对网络化数据中的依赖性问题研究不足。该论文旨在填补这一空白，提升在目标领域标注数据稀缺时的模型性能。

Method: 采用网络卷积回归（NCR）框架，通过两步迁移学习算法解决源网络与目标网络的域偏移问题，并引入源检测机制识别信息丰富的领域。理论分析基于Erdos-Renyi模型假设。

Result: 理论和实证分析（包括模拟实验和微博数据应用）表明，当存在信息丰富的源领域时，迁移学习显著提高了预测准确性，尤其在目标领域标注数据有限时效果更佳。

Conclusion: NCR框架有效解决了网络化数据中的迁移学习问题，理论和实证结果均支持其在提升模型性能方面的潜力，尤其在数据稀缺场景下表现突出。

Abstract: Transfer learning enhances model performance by utilizing knowledge from
related domains, particularly when labeled data is scarce. While existing
research addresses transfer learning under various distribution shifts in
independent settings, handling dependencies in networked data remains
challenging. To address this challenge, we propose a high-dimensional transfer
learning framework based on network convolutional regression (NCR), inspired by
the success of graph convolutional networks (GCNs). The NCR model incorporates
random network structure by allowing each node's response to depend on its
features and the aggregated features of its neighbors, capturing local
dependencies effectively. Our methodology includes a two-step transfer learning
algorithm that addresses domain shift between source and target networks, along
with a source detection mechanism to identify informative domains.
Theoretically, we analyze the lasso estimator in the context of a random graph
based on the Erdos-Renyi model assumption, demonstrating that transfer learning
improves convergence rates when informative sources are present. Empirical
evaluations, including simulations and a real-world application using Sina
Weibo data, demonstrate substantial improvements in prediction accuracy,
particularly when labeled data in the target domain is limited.

</details>


### [165] [Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets](https://arxiv.org/abs/2504.19981)
*Adam Younsi,Abdalgader Abubaker,Mohamed El Amine Seddik,Hakim Hacid,Salem Lahlou*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的流程奖励模型（PRM）结合生成流网络（GFlowNets），用于提升大型语言模型（LLMs）在数学推理中的准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在复杂领域（如数学）中准确性和多样性难以兼顾的问题，特别是如何在不依赖人工标注的情况下评估中间推理步骤。

Method: 通过蒙特卡洛树搜索和基于相似性的数据增强技术训练PRM，并使用GFlowNets在推理步骤层级生成多样且高质量的解决方案。

Result: 在数学评测基准上实现显著改进（如Llama3.2-3B在MATH Level 5上绝对准确率提升2.59%），并能泛化到新数据集（SAT MATH上提升9.4%）。

Conclusion: PRM引导的GFlowNets在提升LLMs数学推理能力的稳健性和通用性方面具有潜在价值。

Abstract: Achieving both accuracy and diverse reasoning remains challenging for Large
Language Models (LLMs) in complex domains like mathematics. A key bottleneck is
evaluating intermediate reasoning steps to guide generation without costly
human annotations. To address this, we first introduce a novel Process Reward
Model (PRM) trained automatically using Monte Carlo Tree Search coupled with a
similarity-based data augmentation technique, effectively capturing step-level
reasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks
(GFlowNets) to operate at the reasoning step level. Unlike traditional
reinforcement learning focused on maximizing a single reward, GFlowNets
naturally sample diverse, high-quality solutions proportional to their rewards,
as measured by our PRM. Empirical evaluation shows strong improvements in both
accuracy and solution diversity on challenging mathematical benchmarks (e.g.,
+2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective
generalization to unseen datasets (+9.4% absolute on SAT MATH). Our work
demonstrates the potential of PRM-guided, step-level GFlowNets for developing
more robust and versatile mathematical reasoning in LLMs.

</details>


### [166] [Emergence and scaling laws in SGD learning of shallow neural networks](https://arxiv.org/abs/2504.19983)
*Yunwei Ren,Eshaan Nichani,Denny Wu,Jason D. Lee*

Main category: cs.LG

TL;DR: 论文研究了在线随机梯度下降（SGD）学习两层神经网络的复杂性，特别是在“扩展宽度”条件下，分析了训练动态和信号方向恢复的临界时间。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在大量神经元（P≫1）和发散条件数下，SGD训练两层神经网络的动态行为及其优化效果，尤其是在功率律缩放条件下的表现。

Method: 研究方法包括对SGD动态的精确分析，通过最小化均方误差（MSE）目标，明确识别信号方向恢复的临界时间，并探讨功率律设定下的标度律。

Result: 结果表明，单个教师神经元的学习呈现出突然的转变，但在不同时间尺度上大量学习曲线的叠加导致了累积目标的平滑标度律。

Conclusion: 论文的结论强调了在大规模神经元网络中，SGD的动态行为导致了复杂但可预测的优化效果，为理解神经网络训练提供了新的见解。

Abstract: We study the complexity of online stochastic gradient descent (SGD) for
learning a two-layer neural network with $P$ neurons on isotropic Gaussian
data: $f_*(\boldsymbol{x}) = \sum_{p=1}^P a_p\cdot
\sigma(\langle\boldsymbol{x},\boldsymbol{v}_p^*\rangle)$, $\boldsymbol{x} \sim
\mathcal{N}(0,\boldsymbol{I}_d)$, where the activation
$\sigma:\mathbb{R}\to\mathbb{R}$ is an even function with information exponent
$k_*>2$ (defined as the lowest degree in the Hermite expansion),
$\{\boldsymbol{v}^*_p\}_{p\in[P]}\subset \mathbb{R}^d$ are orthonormal signal
directions, and the non-negative second-layer coefficients satisfy $\sum_{p}
a_p^2=1$. We focus on the challenging ``extensive-width'' regime $P\gg 1$ and
permit diverging condition number in the second-layer, covering as a special
case the power-law scaling $a_p\asymp p^{-\beta}$ where
$\beta\in\mathbb{R}_{\ge 0}$. We provide a precise analysis of SGD dynamics for
the training of a student two-layer network to minimize the mean squared error
(MSE) objective, and explicitly identify sharp transition times to recover each
signal direction. In the power-law setting, we characterize scaling law
exponents for the MSE loss with respect to the number of training samples and
SGD steps, as well as the number of parameters in the student neural network.
Our analysis entails that while the learning of individual teacher neurons
exhibits abrupt transitions, the juxtaposition of $P\gg 1$ emergent learning
curves at different timescales leads to a smooth scaling law in the cumulative
objective.

</details>


### [167] [Modelling of Underwater Vehicles using Physics-Informed Neural Networks with Control](https://arxiv.org/abs/2504.20019)
*Abdelhakim Amer,David Felsager,Yury Brodskiy,Andriy Sarabakha*

Main category: cs.LG

TL;DR: 该论文介绍了PINC框架，它是PINNs的一个开源实现，专为水下车辆动力学建模设计。通过结合物理定律和初始状态、控制动作及时间输入，PINC扩展了PINNs的能力，实现了超出训练域的物理一致性预测。


<details>
  <summary>Details</summary>
Motivation: 提高数据驱动模型在建模水下车辆动力学时的泛化能力和样本效率，通过结合物理定律确保预测的物理一致性。

Method: PINC框架扩展了PINNs，利用初始状态、控制动作和时间输入，测试了不同的损失函数、梯度加权方案和超参数配置。

Result: 在模拟水下车辆上的验证表明，相较于非物理信息的基线模型，PINC能实现更准确的长时预测。

Conclusion: PINC框架通过整合物理定律和数据驱动模型，有效提升了水下车辆动力学模型的预测能力和泛化性。

Abstract: Physics-informed neural networks (PINNs) integrate physical laws with
data-driven models to improve generalization and sample efficiency. This work
introduces an open-source implementation of the Physics-Informed Neural Network
with Control (PINC) framework, designed to model the dynamics of an underwater
vehicle. Using initial states, control actions, and time inputs, PINC extends
PINNs to enable physically consistent transitions beyond the training domain.
Various PINC configurations are tested, including differing loss functions,
gradient-weighting schemes, and hyperparameters. Validation on a simulated
underwater vehicle demonstrates more accurate long-horizon predictions compared
to a non-physics-informed baseline

</details>


### [168] [Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models](https://arxiv.org/abs/2504.20020)
*Xin Wang,Haoyang Li,Zeyang Zhang,Haibo Chen,Wenwu Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种新型学习范式——模块化机器学习（MML），旨在通过分解大型语言模型（LLMs）的复杂结构，提升其反事实推理能力、减少幻觉，并增强公平性、安全性和透明度。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在推理、事实一致性和可解释性方面存在局限，需要新的方法来改进这些问题。

Method: MML将LLMs分解为三个相互依赖的组件：模块化表示、模块化模型和模块化推理，并结合解耦表示学习、神经架构搜索和神经符号学习等技术实现。

Result: MML范式能明确LLMs的内部工作机制，支持灵活的任务自适应模型设计，并实现可解释的逻辑驱动决策过程。

Conclusion: MML与LLMs的结合有望弥合统计学习与形式推理之间的差距，推动构建稳健、适应性强且可信的AI系统。

Abstract: Large language models (LLMs) have dramatically advanced machine learning
research including natural language processing, computer vision, data mining,
etc., yet they still exhibit critical limitations in reasoning, factual
consistency, and interpretability. In this paper, we introduce a novel learning
paradigm -- Modular Machine Learning (MML) -- as an essential approach toward
new-generation LLMs. MML decomposes the complex structure of LLMs into three
interdependent components: modular representation, modular model, and modular
reasoning, aiming to enhance LLMs' capability of counterfactual reasoning,
mitigating hallucinations, as well as promoting fairness, safety, and
transparency. Specifically, the proposed MML paradigm can: i) clarify the
internal working mechanism of LLMs through the disentanglement of semantic
components; ii) allow for flexible and task-adaptive model design; iii) enable
interpretable and logic-driven decision-making process. We present a feasible
implementation of MML-based LLMs via leveraging advanced techniques such as
disentangled representation learning, neural architecture search and
neuro-symbolic learning. We critically identify key challenges, such as the
integration of continuous neural and discrete symbolic processes, joint
optimization, and computational scalability, present promising future research
directions that deserve further exploration. Ultimately, the integration of the
MML paradigm with LLMs has the potential to bridge the gap between statistical
(deep) learning and formal (logical) reasoning, thereby paving the way for
robust, adaptable, and trustworthy AI systems across a wide range of real-world
applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [169] [BELL: Benchmarking the Explainability of Large Language Models](https://arxiv.org/abs/2504.18572)
*Syed Quiser Ahmed,Bharathi Vokkaliga Ganesh,Jagadish Babu P,Karthick Selvaraj,ReddySiva Naga Parvathi Devi,Sravya Kappala*

Main category: cs.AI

TL;DR: 论文提出了一种标准化的大语言模型（LLM）解释性评估方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的决策过程缺乏透明度，引发了对信任、偏见和性能的担忧。

Method: 引入了标准化基准技术（Benchmarking the Explainability of Large Language Models），评估LLM的解释性。

Result: 未提供具体实验结果。

Conclusion: 评估LLM的解释性对解决其不透明性至关重要。

Abstract: Large Language Models have demonstrated remarkable capabilities in natural
language processing, yet their decision-making processes often lack
transparency. This opaqueness raises significant concerns regarding trust,
bias, and model performance. To address these issues, understanding and
evaluating the interpretability of LLMs is crucial. This paper introduces a
standardised benchmarking technique, Benchmarking the Explainability of Large
Language Models, designed to evaluate the explainability of large language
models.

</details>


### [170] [A Cognitive-Mechanistic Human Reliability Analysis Framework: A Nuclear Power Plant Case Study](https://arxiv.org/abs/2504.18604)
*Xingyu Xiao,Peng Chen,Jiejuan Tong,Shunshun Liu,Hongru Zhao,Jun Zhao,Qianqian Jia,Jingang Liang,Haitao Wang*

Main category: cs.AI

TL;DR: 提出COGMIF框架，整合ACT-R数字孪生与TimeGAN增强仿真，改进IDHEAS-ECA方法，量化认知机制对核电站操作风险的影响。


<details>
  <summary>Details</summary>
Motivation: 传统HRA方法依赖专家经验，忽视认知机制，且高保真实验成本高，需新方法提升可信度和计算效率。

Method: 结合ACT-R数字孪生模拟认知过程，用TimeGAN生成合成行为数据，驱动IDHEAS-ECA评估，并通过贝叶斯网络量化风险因素。

Result: COGMIF在HEP估计上优于SPAR-H，敏感性分析验证其鲁棒性，并识别操作风险关键驱动因素。

Conclusion: COGMIF为认知理论与工业HRA实践提供高效整合路径，增强机制化风险评估能力。

Abstract: Traditional human reliability analysis (HRA) methods, such as IDHEAS-ECA,
rely on expert judgment and empirical rules that often overlook the cognitive
underpinnings of human error. Moreover, conducting human-in-the-loop
experiments for advanced nuclear power plants is increasingly impractical due
to novel interfaces and limited operational data. This study proposes a
cognitive-mechanistic framework (COGMIF) that enhances the IDHEAS-ECA
methodology by integrating an ACT-R-based human digital twin (HDT) with
TimeGAN-augmented simulation. The ACT-R model simulates operator cognition,
including memory retrieval, goal-directed procedural reasoning, and
perceptual-motor execution, under high-fidelity scenarios derived from a
high-temperature gas-cooled reactor (HTGR) simulator. To overcome the resource
constraints of large-scale cognitive modeling, TimeGAN is trained on
ACT-R-generated time-series data to produce high-fidelity synthetic operator
behavior datasets. These simulations are then used to drive IDHEAS-ECA
assessments, enabling scalable, mechanism-informed estimation of human error
probabilities (HEPs). Comparative analyses with SPAR-H and sensitivity
assessments demonstrate the robustness and practical advantages of the proposed
COGMIF. Finally, procedural features are mapped onto a Bayesian network to
quantify the influence of contributing factors, revealing key drivers of
operational risk. This work offers a credible and computationally efficient
pathway to integrate cognitive theory into industrial HRA practices.

</details>


### [171] [Research on Personalized Medical Intervention Strategy Generation System based on Group Relative Policy Optimization and Time-Series Data Fusion](https://arxiv.org/abs/2504.18631)
*Dingxin Lu,Shurui Wu,Xinyi Huang*

Main category: cs.AI

TL;DR: 本文提出一种基于GRPO和时间序列数据融合的系统，用于生成个性化医疗干预策略，通过结合群体相对策略优化和多模态数据融合，显著提升准确性和决策效益。


<details>
  <summary>Details</summary>
Motivation: 医疗领域面临基于高维异构时间序列信息快速制定个性化干预计划的挑战，多源医疗数据的多样化为解决这一问题提供可能。

Method: 引入群体相对策略约束平衡个体与群体收益，采用多层神经网络分组编码患者特征；利用多通道神经网络和自注意力机制动态提取多源异构时间序列特征；结合遗传算法和蒙特卡洛树搜索实现全局优化。

Result: 实验显示，该方法在准确性、覆盖率和决策效益上显著优于现有方法。

Conclusion: 该系统有效解决了多源异构医疗数据下的个性化干预策略生成问题，具有较高的实用价值。

Abstract: With the timely formation of personalized intervention plans based on
high-dimensional heterogeneous time series information becoming an important
challenge in the medical field today, electronic medical records, wearables,
and other multi-source medical data are increasingly generated and diversified.
In this work, we develop a system to generate personalized medical intervention
strategies based on Group Relative Policy Optimization (GRPO) and Time-Series
Data Fusion. First, by incorporating relative policy constraints among the
groups during policy gradient updates, we adaptively balance individual and
group gains. To improve the robustness and interpretability of decision-making,
a multi-layer neural network structure is employed to group-code patient
characteristics. Second, for the rapid multi-modal fusion of multi-source
heterogeneous time series, a multi-channel neural network combined with a
self-attention mechanism is used for dynamic feature extraction. Key feature
screening and aggregation are achieved through a differentiable gating network.
Finally, a collaborative search process combining a genetic algorithm and Monte
Carlo tree search is proposed to find the ideal intervention strategy,
achieving global optimization. Experimental results show significant
improvements in accuracy, coverage, and decision-making benefits compared with
existing methods.

</details>


### [172] [Exploring a Large Language Model for Transforming Taxonomic Data into OWL: Lessons Learned and Implications for Ontology Development](https://arxiv.org/abs/2504.18651)
*Filipi Miranda Soares,Antonio Mauro Saraiva,Luís Ferreira Pires,Luiz Olavo Bonino da Silva Santos,Dilvan de Abreu Moreira,Fernando Elias Corrêa,Kelly Rosa Braghetto,Debora Pignatari Drucker,Alexandre Cláudio Botazzo Delbem*

Main category: cs.AI

TL;DR: 使用ChatGPT-4自动化构建农业产品类型本体（APTO）中的:Organism模块，以处理物种分类中科学名称的管理挑战。通过两种方法（直接提示和Python算法设计）实现，发现后者更具扩展性但存在数据错误问题。


<details>
  <summary>Details</summary>
Motivation: 物种分类学名称的动态变化导致人工维护困难，研究探索利用ChatGPT-4自动化处理大规模科学名称管理，以提高本体开发的效率。

Method: 采用两种方法：1）通过BrowserOP插件直接提示ChatGPT-4执行任务；2）让ChatGPT-4设计Python算法处理相同任务。两种方法均基于指令、上下文、输入数据和输出指示的提示框架。

Result: 第一种方法扩展性受限，第二种方法通过Python算法克服扩展性问题，但存在数据处理的拼写错误。

Conclusion: 大型语言模型如ChatGPT-4在物种名称管理的自动化中具有潜力，尽管存在限制，但能显著提升本体开发效率。

Abstract: Managing scientific names in ontologies that represent species taxonomies is
challenging due to the ever-evolving nature of these taxonomies. Manually
maintaining these names becomes increasingly difficult when dealing with
thousands of scientific names. To address this issue, this paper investigates
the use of ChatGPT-4 to automate the development of the :Organism module in the
Agricultural Product Types Ontology (APTO) for species classification. Our
methodology involved leveraging ChatGPT-4 to extract data from the GBIF
Backbone API and generate OWL files for further integration in APTO. Two
alternative approaches were explored: (1) issuing a series of prompts for
ChatGPT-4 to execute tasks via the BrowserOP plugin and (2) directing ChatGPT-4
to design a Python algorithm to perform analogous tasks. Both approaches rely
on a prompting method where we provide instructions, context, input data, and
an output indicator. The first approach showed scalability limitations, while
the second approach used the Python algorithm to overcome these challenges, but
it struggled with typographical errors in data handling. This study highlights
the potential of Large language models like ChatGPT-4 to streamline the
management of species names in ontologies. Despite certain limitations, these
tools offer promising advancements in automating taxonomy-related tasks and
improving the efficiency of ontology development.

</details>


### [173] [Proof-of-TBI -- Fine-Tuned Vision Language Model Consortium and OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild Traumatic Brain Injury (TBI) Prediction](https://arxiv.org/abs/2504.18671)
*Ross Gore,Eranga Bandara,Sachin Shetty,Alberto E. Musto,Pratip Rana,Ambrosio Valencia-Romero,Christopher Rhea,Lobat Tayebi,Heather Richter,Atmaram Yarlagadda,Donna Edmonds,Steven Wallace,Donna Broshek*

Main category: cs.AI

TL;DR: 论文提出了一种结合微调视觉语言模型和OpenAI-o3推理大模型的轻度创伤性脑损伤（TBI）诊断支持系统Proof-of-TBI，通过共识决策流程提高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 轻度TBI症状在医学影像中表现微妙且模糊，诊断困难。为提高准确性，研究探索如何结合多模态模型与推理能力强大的LLM。

Method: 1. 微调多个视觉语言模型；2. 通过共识机制汇总预测；3. 利用OpenAI-o3进行最终决策推理；4. 使用LLM代理协调模型交互。

Result: 原型系统在合作开发中展现出高准确性，首次将微调视觉语言模型与推理LLM结合用于TBI预测。

Conclusion: Proof-of-TBI验证了多模态模型与LLM结合的潜力，为TBI诊断提供了可靠、自动化的解决方案。

Abstract: Mild Traumatic Brain Injury (TBI) detection presents significant challenges
due to the subtle and often ambiguous presentation of symptoms in medical
imaging, making accurate diagnosis a complex task. To address these challenges,
we propose Proof-of-TBI, a medical diagnosis support system that integrates
multiple fine-tuned vision-language models with the OpenAI-o3 reasoning large
language model (LLM). Our approach fine-tunes multiple vision-language models
using a labeled dataset of TBI MRI scans, training them to diagnose TBI
symptoms effectively. The predictions from these models are aggregated through
a consensus-based decision-making process. The system evaluates the predictions
from all fine-tuned vision language models using the OpenAI-o3 reasoning LLM, a
model that has demonstrated remarkable reasoning performance, to produce the
most accurate final diagnosis. The LLM Agents orchestrates interactions between
the vision-language models and the reasoning LLM, managing the final
decision-making process with transparency, reliability, and automation. This
end-to-end decision-making workflow combines the vision-language model
consortium with the OpenAI-o3 reasoning LLM, enabled by custom prompt
engineering by the LLM agents. The prototype for the proposed platform was
developed in collaboration with the U.S. Army Medical Research team in Newport
News, Virginia, incorporating five fine-tuned vision-language models. The
results demonstrate the transformative potential of combining fine-tuned
vision-language model inputs with the OpenAI-o3 reasoning LLM to create a
robust, secure, and highly accurate diagnostic system for mild TBI prediction.
To the best of our knowledge, this research represents the first application of
fine-tuned vision-language models integrated with a reasoning LLM for TBI
prediction tasks.

</details>


### [174] [Transformational Creativity in Science: A Graphical Theory](https://arxiv.org/abs/2504.18687)
*Samuel Schapiro,Jonah Black,Lav R. Varshney*

Main category: cs.AI

TL;DR: 论文通过图形理论分析了变革性科学创造力，结合Boden和Kuhn的理论，证明模型公理的修改最具变革潜力，并用历史案例验证框架。


<details>
  <summary>Details</summary>
Motivation: 旨在将Boden的变革性创造力源于概念空间“约束条件”变化与Kuhn的科学革命结构（范式转移）结合，构建统一的图形理论模型。

Method: 提出图形理论模型，分析公理修改的变革潜力，并用历史案例验证框架的有效性。

Result: 证明公理修改最具变革潜力，框架能够捕捉历史上的变革性创造力案例。

Conclusion: 图形理论模型为变革性科学创造力提供了统一分析工具，验证了理论假设和实践价值。

Abstract: Creative processes are typically divided into three types: combinatorial,
exploratory, and transformational. Here, we provide a graphical theory of
transformational scientific creativity, synthesizing Boden's insight that
transformational creativity arises from changes in the "enabling constraints"
of a conceptual space and Kuhn's structure of scientific revolutions as
resulting from paradigm shifts. We prove that modifications made to axioms of
our graphical model have the most transformative potential and then illustrate
how several historical instances of transformational creativity can be captured
by our framework.

</details>


### [175] [A Vision for Auto Research with LLM Agents](https://arxiv.org/abs/2504.18765)
*Chengwei Liu,Chong Wang,Jiayue Cao,Jingquan Ge,Kun Wang,Lvye Zhang,Ming-Ming Cheng,Penghai Zhao,Tianlin Li,Xiaojun Jia,Xiang Li,Xinfeng Li,Yang Liu,Yebo Feng,Yihao Huang,Yijia Xu,Yuqiang Sun,Zhenhong Zhou,Zhengzi Xu*

Main category: cs.AI

TL;DR: 该论文提出了一个基于多智能体的自动化研究框架（Agent-Based Auto Research），利用大语言模型（LLMs）和模块化智能体协作，覆盖科研全生命周期，旨在解决研究流程碎片化、方法论不均衡及认知过载等问题，初步验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 当前科学研究流程存在碎片化、方法论不均衡和研究者认知过载等问题，作者希望通过多智能体协作和大语言模型实现科研全自动化和优化。

Method: 采用基于多智能体的框架，利用大语言模型（LLMs）和模块化智能体协作，覆盖文献综述、构思、方法设计、实验、论文撰写、同行评审响应和传播等科研全流程。

Result: 初步实验表明，该框架在实现自动化科研流程方面具有可行性和潜力，可为AI驱动的自优化研究提供新范式。

Conclusion: Agent-Based Auto Research框架为科学研究的系统性、规模化及自动化提供了创新解决方案，展现了AI驱动科研的未来发展方向。

Abstract: This paper introduces Agent-Based Auto Research, a structured multi-agent
framework designed to automate, coordinate, and optimize the full lifecycle of
scientific research. Leveraging the capabilities of large language models
(LLMs) and modular agent collaboration, the system spans all major research
phases, including literature review, ideation, methodology planning,
experimentation, paper writing, peer review response, and dissemination. By
addressing issues such as fragmented workflows, uneven methodological
expertise, and cognitive overload, the framework offers a systematic and
scalable approach to scientific inquiry. Preliminary explorations demonstrate
the feasibility and potential of Auto Research as a promising paradigm for
self-improving, AI-driven research processes.

</details>


### [176] [Evaluating AI-Driven Automated Map Digitization in QGIS](https://arxiv.org/abs/2504.18777)
*Diana Febrita*

Main category: cs.AI

TL;DR: 该论文研究了AI驱动的工具Deepness在自动化地图数字化中的有效性，通过对比Google Earth影像和OpenStreetMap的数字结果来评估性能。


<details>
  <summary>Details</summary>
Motivation: 传统地图数字化过程需要大量人工参与，而AI技术的发展提供了自动化解决方案的可能性。研究旨在评估Deepness工具在这方面的有效性。

Method: 使用Deepness插件在QGIS中进行自动化数字化，并比较其AI生成的Google Earth影像数字化结果与OpenStreetMap的手动数字化结果。

Result: 研究分析了Deepness的自动化数字化效果，并与OpenStreetMap的结果进行了对比评估。

Conclusion: 该研究为AI在地图数字化中的应用提供了实证支持，展示了Deepness在自动化过程中的潜力和可能的改进空间。

Abstract: Map digitization is an important process that converts maps into digital
formats that can be used for further analysis. This process typically requires
a deep human involvement because of the need for interpretation and
decision-making when translating complex features. With the advancement of
artificial intelligence, there is an alternative to conducting map digitization
with the help of machine learning techniques. Deepness, or Deep Neural Remote
Sensing, is an advanced AI-driven tool designed and integrated as a plugin in
QGIS application. This research focuses on assessing the effectiveness of
Deepness in automated digitization. This study analyses AI-generated
digitization results from Google Earth imagery and compares them with digitized
outputs from OpenStreetMap (OSM) to evaluate performance.

</details>


### [177] [Hierarchical Reinforcement Learning in Multi-Goal Spatial Navigation with Autonomous Mobile Robots](https://arxiv.org/abs/2504.18794)
*Brendon Johnson,Alfredo Weitzenfeld*

Main category: cs.AI

TL;DR: 该研究评估了分层强化学习（HRL）在复杂导航任务中的表现，并与标准强化学习（如PPO）进行对比，重点分析了子目标生成和终止函数等HRL特性对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 机器人学习任务中普遍存在层次结构，但传统强化学习算法在稀疏奖励场景下表现不佳。HRL被认为能更好地利用这种层次结构提升性能，因此本研究旨在验证其优势。

Method: 通过实验对比HRL与PPO的性能，测试不同子目标生成方式（手动 vs 自动）以及终止频率对任务表现的影响，以分析HRL的核心机制。

Result: 实验表明，HRL通过子目标生成和终止函数等机制，在复杂导航任务中显著优于传统强化学习，尤其是在稀疏奖励环境中。

Conclusion: HRL凭借其层次化设计和子目标管理能力，能够更高效地解决复杂任务，验证了其在机器人学习中的潜力。

Abstract: Hierarchical reinforcement learning (HRL) is hypothesized to be able to take
advantage of the inherent hierarchy in robot learning tasks with sparse reward
schemes, in contrast to more traditional reinforcement learning algorithms. In
this research, hierarchical reinforcement learning is evaluated and contrasted
with standard reinforcement learning in complex navigation tasks. We evaluate
unique characteristics of HRL, including their ability to create sub-goals and
the termination function. We constructed experiments to test the differences
between PPO and HRL, different ways of creating sub-goals, manual vs automatic
sub-goal creation, and the effects of the frequency of termination on
performance. These experiments highlight the advantages of HRL and how it
achieves these advantages.

</details>


### [178] [Generative to Agentic AI: Survey, Conceptualization, and Challenges](https://arxiv.org/abs/2504.18875)
*Johannes Schneider*

Main category: cs.AI

TL;DR: 该论文探讨了Agentic AI与GenAI的区别及其演变，分为两部分：首先比较两者的关键特性及Agentic AI如何弥补GenAI的不足；其次深入探讨Agentic AI的新颖方面及未来研究的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决Agentic AI与GenAI之间区别不明确的问题，并为学术界和工业界提供理解Agentic AI新应用的见解。

Method: 通过文献比较GenAI和Agentic AI的关键特性，分析Agentic AI的演变及新颖方面，并讨论未来研究方向和潜在风险。

Result: 明确了Agentic AI的独特性及其超越GenAI的能力，同时指出了未来发展中的挑战与风险。

Conclusion: Agentic AI代表了AI发展的新阶段，具有更强的推理和交互能力，但仍需应对技术和社会挑战。

Abstract: Agentic Artificial Intelligence (AI) builds upon Generative AI (GenAI). It
constitutes the next major step in the evolution of AI with much stronger
reasoning and interaction capabilities that enable more autonomous behavior to
tackle complex tasks. Since the initial release of ChatGPT (3.5), Generative AI
has seen widespread adoption, giving users firsthand experience. However, the
distinction between Agentic AI and GenAI remains less well understood. To
address this gap, our survey is structured in two parts. In the first part, we
compare GenAI and Agentic AI using existing literature, discussing their key
characteristics, how Agentic AI remedies limitations of GenAI, and the major
steps in GenAI's evolution toward Agentic AI. This section is intended for a
broad audience, including academics in both social sciences and engineering, as
well as industry professionals. It provides the necessary insights to
comprehend novel applications that are possible with Agentic AI but not with
GenAI. In the second part, we deep dive into novel aspects of Agentic AI,
including recent developments and practical concerns such as defining agents.
Finally, we discuss several challenges that could serve as a future research
agenda, while cautioning against risks that can emerge when exceeding human
intelligence.

</details>


### [179] [Reshaping MOFs Text Mining with a Dynamic Multi-Agent Framework of Large Language Agents](https://arxiv.org/abs/2504.18880)
*Zuhong Lin,Daoyuan Ren,Kai Ran,Sun Jing,Xiaotiang Huang,Haiyang He,Pengxu Pan,Xiaohang Zhang,Ying Fang,Tianying Wang,Minli Wu,Zhanglin Li,Xiaochuan Zhang,Haipu Li,Jingjing Yao*

Main category: cs.AI

TL;DR: 论文提出使用大型语言模型（LLM）开发工具MOFh6，用于优化金属有机框架（MOF）的合成条件查询和分析，提升研究效率。


<details>
  <summary>Details</summary>
Motivation: MOF合成条件繁多且复杂，传统方法难以高效筛选最优条件，LLM提供了解决这一问题的潜力。

Method: 利用gpt-4o-mini作为核心代理，集成MOF合成、属性和化学信息代理，开发多格式查询工具MOFh6。

Result: MOFh6支持通过文献、MOF代码或结构属性查询，提供最优合成条件并生成DFT预建模文件。

Conclusion: MOFh6有望显著提升MOF合成研究的效率，为领域提供实用工具。

Abstract: The mining of synthesis conditions for metal-organic frameworks (MOFs) is a
significant focus in materials science. However, identifying the precise
synthesis conditions for specific MOFs within the vast array of possibilities
presents a considerable challenge. Large Language Models (LLMs) offer a
promising solution to this problem. We leveraged the capabilities of LLMs,
specifically gpt-4o-mini, as core agents to integrate various MOF-related
agents, including synthesis, attribute, and chemical information agents. This
integration culminated in the development of MOFh6, an LLM tool designed to
streamline the MOF synthesis process. MOFh6 allows users to query in multiple
formats, such as submitting scientific literature, or inquiring about specific
MOF codes or structural properties. The tool analyzes these queries to provide
optimal synthesis conditions and generates model files for density functional
theory pre modeling. We believe MOFh6 will enhance efficiency in the MOF
synthesis of all researchers.

</details>


### [180] [Use of Metric Learning for the Recognition of Handwritten Digits, and its Application to Increase the Outreach of Voice-based Communication Platforms](https://arxiv.org/abs/2504.18948)
*Devesh Pant,Dibyendu Talukder,Deepak Kumar,Rachit Pandey,Aaditeshwar Seth,Chetan Arora*

Main category: cs.AI

TL;DR: 论文提出了在资源受限环境中通过纸质数据收集结合OCR/OMR技术实现自动数字化的方法，并提供了一个大规模手写数字数据集及深度学习模型，成功应用于印度农村妇女健康项目中。


<details>
  <summary>Details</summary>
Motivation: 在资源不足地区，数字化设备如智能手机和平板电脑的使用受限，因此研究纸质数据收集与自动化数字化的可行性，以支持项目监测与评估。

Method: 采用OCR和OMR技术自动化处理纸质表格，并基于大规模手写数字数据集开发深度学习模型。

Result: 在印度农村妇女健康项目中成功数字化了纸质表格，推动了近400万通电话服务，数据和模型已开源。

Conclusion: 纸质数据结合自动化数字化技术是资源受限环境下的有效解决方案，模型和数据的开源促进了更广泛的应用。

Abstract: Initiation, monitoring, and evaluation of development programmes can involve
field-based data collection about project activities. This data collection
through digital devices may not always be feasible though, for reasons such as
unaffordability of smartphones and tablets by field-based cadre, or shortfalls
in their training and capacity building. Paper-based data collection has been
argued to be more appropriate in several contexts, with automated digitization
of the paper forms through OCR (Optical Character Recognition) and OMR (Optical
Mark Recognition) techniques. We contribute with providing a large dataset of
handwritten digits, and deep learning based models and methods built using this
data, that are effective in real-world environments. We demonstrate the
deployment of these tools in the context of a maternal and child health and
nutrition awareness project, which uses IVR (Interactive Voice Response)
systems to provide awareness information to rural women SHG (Self Help Group)
members in north India. Paper forms were used to collect phone numbers of the
SHG members at scale, which were digitized using the OCR tools developed by us,
and used to push almost 4 million phone calls. The data, model, and code have
been released in the open-source domain.

</details>


### [181] [Sparks: Multi-Agent Artificial Intelligence Model Discovers Protein Design Principles](https://arxiv.org/abs/2504.19017)
*Alireza Ghafarollahi,Markus J. Buehler*

Main category: cs.AI

TL;DR: Sparks是一种多模态多代理AI模型，能够自主执行从假设生成到实验设计的完整科学发现周期，并在蛋白质科学中发现了两项新现象。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统大多仅能复现训练数据中的知识，缺乏自主发现能力。Sparks旨在实现无需人工干预的完整科学发现流程，以验证AI的独立科研潜力。

Method: Sparks结合生成式序列设计、高精度结构预测和物理驱动的属性模型，通过生成-反思代理的协同实现自我纠错和可重复性。

Result: 发现了肽机械性能的长度依赖转折（约80个残基后β-折叠优于α-螺旋）和二级结构稳定性图谱中的“挫败区”，均为全新的科学原理。

Conclusion: Sparks证明了AI可独立开展严谨科学研究并揭示未知科学规律，为自主科学发现提供了范例。

Abstract: Advances in artificial intelligence (AI) promise autonomous discovery, yet
most systems still resurface knowledge latent in their training data. We
present Sparks, a multi-modal multi-agent AI model that executes the entire
discovery cycle that includes hypothesis generation, experiment design and
iterative refinement to develop generalizable principles and a report without
human intervention. Applied to protein science, Sparks uncovered two previously
unknown phenomena: (i) a length-dependent mechanical crossover whereby
beta-sheet-biased peptides surpass alpha-helical ones in unfolding force beyond
~80 residues, establishing a new design principle for peptide mechanics; and
(ii) a chain-length/secondary-structure stability map revealing unexpectedly
robust beta-sheet-rich architectures and a "frustration zone" of high variance
in mixed alpha/beta folds. These findings emerged from fully self-directed
reasoning cycles that combined generative sequence design, high-accuracy
structure prediction and physics-aware property models, with paired
generation-and-reflection agents enforcing self-correction and reproducibility.
The key result is that Sparks can independently conduct rigorous scientific
inquiry and identify previously unknown scientific principles.

</details>


### [182] [GLaMoR: Consistency Checking of OWL Ontologies using Graph Language Models](https://arxiv.org/abs/2504.19023)
*Justin Mücke,Ansgar Scherp*

Main category: cs.AI

TL;DR: GLaMoR（图语言模型推理）是一种新的推理管道，将OWL本体转换为图结构数据并利用GLM架构进行一致性检查，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决语义推理中现有推理器计算成本高且效率随本体规模下降的问题，尤其是传统机器学习模型难以捕捉复杂本体关系，而大语言模型在结构化推理中效果不佳。

Method: 提出GLaMoR方法，将OWL本体转换为图结构数据，并采用GLM架构进行一致性检查。输入数据为从BioPortal获取的本体三元组。

Result: GLaMoR在BioPortal本体上达到了95%的准确率，速度比传统推理器快20倍。

Conclusion: GLM结构在处理图数据与文本的联合推理任务中表现出色，GLaMoR为高效本体一致性检查提供了可行方案。编程代码已开源。

Abstract: Semantic reasoning aims to infer new knowledge from existing knowledge, with
OWL ontologies serving as a standardized framework for organizing information.
A key challenge in semantic reasoning is verifying ontology consistency.
However, state-of-the-art reasoners are computationally expensive, and their
efficiency decreases as ontology sizes grow. While classical machine learning
models have been explored for consistency checking, they struggle to capture
complex relationships within ontologies. Large language models (LLMs) have
shown promising results for simple reasoning tasks but perform poorly on
structured reasoning. The recently introduced Graph Language Model (GLM) offers
a way to simultaneously process graph-structured data and text. This paper
proposes GLaMoR (Graph Language Model for Reasoning), a reasoning pipeline that
transforms OWL ontologies into graph-structured data and adapts the GLM
architecture for consistency checking. We evaluate GLaMoR on ontologies from
the NCBO BioPortal repository, converting them into triples suitable for model
input. Our results show that the GLM outperforms all baseline models, achieving
$95\%$ accuracy while being 20 times faster than classical reasoners.
  The Code is accessible under: https://github.com/JustinMuecke/GLaMoR

</details>


### [183] [DiCE-Extended: A Robust Approach to Counterfactual Explanations in Machine Learning](https://arxiv.org/abs/2504.19027)
*Volkan Bakir,Polat Goktas,Sureyya Akyuz*

Main category: cs.AI

TL;DR: 提出的DiCE-Extended通过多目标优化和新的稳健性指标改进反事实解释，平衡接近性、多样性和稳健性，实验证明其在多数据集和机器学习后端上的有效性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法（如DiCE）在平衡接近性、多样性和稳健性方面存在不足，限制了其在决策关键领域的实际应用。为解决这一问题，提出了DiCE-Extended。

Method: 引入基于Dice-Sorensen系数的新稳健性指标，使用多目标优化技术（加权损失函数λ_p、λ_d、λ_r）生成反事实解释。

Result: 在多个基准数据集（COMPAS等）和ML后端（Scikit-learn等）上验证，DiCE-Extended在有效性、稳定性和决策边界对齐方面优于标准DiCE。

Conclusion: DiCE-Extended能生成更可靠、可解释的反事实解释，未来将探索自适应优化和领域特定约束以进一步提升生成效果。

Abstract: Explainable artificial intelligence (XAI) has become increasingly important
in decision-critical domains such as healthcare, finance, and law.
Counterfactual (CF) explanations, a key approach in XAI, provide users with
actionable insights by suggesting minimal modifications to input features that
lead to different model outcomes. Despite significant advancements, existing CF
generation methods often struggle to balance proximity, diversity, and
robustness, limiting their real-world applicability. A widely adopted
framework, Diverse Counterfactual Explanations (DiCE), emphasizes diversity but
lacks robustness, making CF explanations sensitive to perturbations and domain
constraints. To address these challenges, we introduce DiCE-Extended, an
enhanced CF explanation framework that integrates multi-objective optimization
techniques to improve robustness while maintaining interpretability. Our
approach introduces a novel robustness metric based on the Dice-Sorensen
coefficient, ensuring stability under small input variations. Additionally, we
refine CF generation using weighted loss components (lambda_p, lambda_d,
lambda_r) to balance proximity, diversity, and robustness. We empirically
validate DiCE-Extended on benchmark datasets (COMPAS, Lending Club, German
Credit, Adult Income) across multiple ML backends (Scikit-learn, PyTorch,
TensorFlow). Results demonstrate improved CF validity, stability, and alignment
with decision boundaries compared to standard DiCE-generated explanations. Our
findings highlight the potential of DiCE-Extended in generating more reliable
and interpretable CFs for high-stakes applications. Future work will explore
adaptive optimization techniques and domain-specific constraints to further
enhance CF generation in real-world scenarios.

</details>


### [184] [ChiseLLM: Unleashing the Power of Reasoning LLMs for Chisel Agile Hardware Development](https://arxiv.org/abs/2504.19144)
*Bowei Wang,Jiaran Gao,Yelai Feng,Renzhi Chen,Shanshan Li,Lei Wang*

Main category: cs.AI

TL;DR: 本文提出ChiseLLM，通过数据处理、提示引导推理和领域自适应模型训练，显著提升Chisel代码生成的语法正确性和设计多样性，实验证明其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）在Chisel代码生成任务中面临语法正确性和设计多样性的挑战，且未经过领域适应的推理模型效果有限，作者提出了ChiseLLM解决方案。

Method: ChiseLLM包括数据处理与转换、提示引导的推理轨迹合成和领域自适应模型训练，并使用公开RTL代码资源构建高质量数据集。

Result: 实验显示，ChiseLLM-7B和ChiseLLM-32B在语法正确性上分别比基线模型提升18.85%和26.32%，设计多样性能力提升47.58%。

Conclusion: ChiseLLM为HCL-Based AHDM提供了高性能、低成本的模型，并为未来研究设定了有效基线。数据集和模型已公开。

Abstract: The growing demand for Domain-Specific Architecture (DSA) has driven the
development of Agile Hardware Development Methodology (AHDM). Hardware
Construction Language (HCL) like Chisel offers high-level abstraction features,
making it an ideal language for HCL-Based AHDM. While Large Language Models
(LLMs) excel in code generation tasks, they still face challenges with Chisel
generation, particularly regarding syntax correctness and design variability.
Recent reasoning models have significantly enhanced code generation
capabilities through test-time scaling techniques. However, we found that
reasoning models without domain adaptation cannot bring substantial benefits to
Chisel code generation tasks. This paper presents ChiseLLM, a solution
comprising data processing and transformation, prompt-guided reasoning trace
synthesis, and domain-adapted model training. We constructed high-quality
datasets from public RTL code resources and guided the model to adopt
structured thinking patterns through prompt enhancement methods. Experiments
demonstrate that our ChiseLLM-7B and ChiseLLM-32B models improved syntax
correctness by 18.85% and 26.32% respectively over base models, while
increasing variability design ability by 47.58% compared to baseline reasoning
models. Our datasets and models are publicly available, providing
high-performance, cost-effective models for HCL-Based AHDM, and offering an
effective baseline for future research. Github repository:
https://github.com/observerw/ChiseLLM

</details>


### [185] [A Dynamic Fuzzy Rule and Attribute Management Framework for Fuzzy Inference Systems in High-Dimensional Data](https://arxiv.org/abs/2504.19148)
*Ke Liu,Jing Ma,Edmund M-K Lai*

Main category: cs.AI

TL;DR: ADAR框架通过双重权重机制和自动调整策略优化高维数据的神经模糊推理系统，实验显示其在多个数据集上优于现有方法，同时保持模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 高维数据在神经模糊推理系统中带来挑战，ADAR旨在通过动态调整属性和规则权重来简化模型，同时不牺牲性能和可解释性。

Method: 结合双重权重机制（属性和规则自适应加权）及自动增减策略，动态优化模糊模型结构。

Result: 在四个数据集上，ADAR的RMSE均优于基线（如北京PM2.5数据集RMSE 56.87）。规则与属性权重的结合显著降低冗余并提升解释性。

Conclusion: ADAR能动态平衡规则复杂度和特征重要性，为可扩展、高精度且透明的神经模糊系统提供了可行方案。

Abstract: This paper presents an Adaptive Dynamic Attribute and Rule (ADAR) framework
designed to address the challenges posed by high-dimensional data in
neuro-fuzzy inference systems. By integrating dual weighting
mechanisms-assigning adaptive importance to both attributes and rules-together
with automated growth and pruning strategies, ADAR adaptively streamlines
complex fuzzy models without sacrificing performance or interpretability.
Experimental evaluations on four diverse datasets - Auto MPG (7 variables),
Beijing PM2.5 (10 variables), Boston Housing (13 variables), and Appliances
Energy Consumption (27 variables) show that ADAR-based models achieve
consistently lower Root Mean Square Error (RMSE) compared to state-of-the-art
baselines. On the Beijing PM2.5 dataset, for instance, ADAR-SOFENN attained an
RMSE of 56.87 with nine rules, surpassing traditional ANFIS [12] and SOFENN
[16] models. Similarly, on the high-dimensional Appliances Energy dataset,
ADAR-ANFIS reached an RMSE of 83.25 with nine rules, outperforming established
fuzzy logic approaches and interpretability-focused methods such as APLR.
Ablation studies further reveal that combining rule-level and attribute-level
weight assignment significantly reduces model overlap while preserving
essential features, thereby enhancing explainability. These results highlight
ADAR's effectiveness in dynamically balancing rule complexity and feature
importance, paving the way for scalable, high-accuracy, and transparent
neuro-fuzzy systems applicable to a range of real-world scenarios.

</details>


### [186] [A Design Framework for operationalizing Trustworthy Artificial Intelligence in Healthcare: Requirements, Tradeoffs and Challenges for its Clinical Adoption](https://arxiv.org/abs/2504.19179)
*Pedro A. Moreno-Sánchez,Javier Del Ser,Mark van Gils,Jussi Hernesniemi*

Main category: cs.AI

TL;DR: 本文提出了一个设计框架，帮助开发者在医疗AI系统中融入可信AI（TAI）原则，以解决伦理、监管和信任问题，并以心血管疾病为例展示了实际应用和挑战。


<details>
  <summary>Details</summary>
Motivation: 医疗AI的广泛应用受限于技术之外的挑战（如伦理、监管和信任问题），需要符合可信AI（TAI）原则。

Method: 提出一个设计框架，针对不同医疗流程和利益相关者，制定与TAI原则一致的疾病无关需求集合，并分析实践中的挑战与权衡。

Result: 框架展示了在心血管疾病领域如何应用TAI原则，并识别了实际障碍。

Conclusion: 通过框架化的方法，可以更系统地将TAI原则融入医疗AI系统，推动其临床落地。

Abstract: Artificial Intelligence (AI) holds great promise for transforming healthcare,
particularly in disease diagnosis, prognosis, and patient care. The increasing
availability of digital medical data, such as images, omics, biosignals, and
electronic health records, combined with advances in computing, has enabled AI
models to approach expert-level performance. However, widespread clinical
adoption remains limited, primarily due to challenges beyond technical
performance, including ethical concerns, regulatory barriers, and lack of
trust. To address these issues, AI systems must align with the principles of
Trustworthy AI (TAI), which emphasize human agency and oversight, algorithmic
robustness, privacy and data governance, transparency, bias and discrimination
avoidance, and accountability. Yet, the complexity of healthcare processes
(e.g., screening, diagnosis, prognosis, and treatment) and the diversity of
stakeholders (clinicians, patients, providers, regulators) complicate the
integration of TAI principles. To bridge the gap between TAI theory and
practical implementation, this paper proposes a design framework to support
developers in embedding TAI principles into medical AI systems. Thus, for each
stakeholder identified across various healthcare processes, we propose a
disease-agnostic collection of requirements that medical AI systems should
incorporate to adhere to the principles of TAI. Additionally, we examine the
challenges and tradeoffs that may arise when applying these principles in
practice. To ground the discussion, we focus on cardiovascular diseases, a
field marked by both high prevalence and active AI innovation, and demonstrate
how TAI principles have been applied and where key obstacles persist.

</details>


### [187] [The Convergent Ethics of AI? Analyzing Moral Foundation Priorities in Large Language Models with a Multi-Framework Approach](https://arxiv.org/abs/2504.19255)
*Chad Coleman,W. Russell Neuman,Ali Dasdan,Safinah Ali,Manan Shah*

Main category: cs.AI

TL;DR: 论文介绍了PRIME框架，用于评估大型语言模型(LLMs)在道德决策中的表现，发现所有模型更注重关怀/伤害与公平/欺骗维度，而忽视权威、忠诚和神圣性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在重要决策中的广泛应用，系统评估其道德推理能力成为迫切需求。

Method: 采用PRIME框架，结合直接提问与回应分析，评估六大领先LLMs在伦理困境中的表现。

Result: 所有模型在关怀/伤害与公平/欺骗方面表现一致，但在其他道德维度上不足，且与人类道德偏好相符。

Conclusion: 该研究为道德基准测试提供了可扩展方法，同时揭示了AI道德推理的潜力与局限。

Abstract: As large language models (LLMs) are increasingly deployed in consequential
decision-making contexts, systematically assessing their ethical reasoning
capabilities becomes a critical imperative. This paper introduces the
Priorities in Reasoning and Intrinsic Moral Evaluation (PRIME) framework--a
comprehensive methodology for analyzing moral priorities across foundational
ethical dimensions including consequentialist-deontological reasoning, moral
foundations theory, and Kohlberg's developmental stages. We apply this
framework to six leading LLMs through a dual-protocol approach combining direct
questioning and response analysis to established ethical dilemmas. Our analysis
reveals striking patterns of convergence: all evaluated models demonstrate
strong prioritization of care/harm and fairness/cheating foundations while
consistently underweighting authority, loyalty, and sanctity dimensions.
Through detailed examination of confidence metrics, response reluctance
patterns, and reasoning consistency, we establish that contemporary LLMs (1)
produce decisive ethical judgments, (2) demonstrate notable cross-model
alignment in moral decision-making, and (3) generally correspond with
empirically established human moral preferences. This research contributes a
scalable, extensible methodology for ethical benchmarking while highlighting
both the promising capabilities and systematic limitations in current AI moral
reasoning architectures--insights critical for responsible development as these
systems assume increasingly significant societal roles.

</details>


### [188] [Small Models, Big Tasks: An Exploratory Empirical Study on Small Language Models for Function Calling](https://arxiv.org/abs/2504.19277)
*Ishan Kavathekar,Raghav Donakanti,Ponnurangam Kumaraguru,Karthik Vaidhyanathan*

Main category: cs.AI

TL;DR: 探讨小型语言模型（SLMs）在生成函数调用任务中的效率，对比零样本、少样本和微调方法，并测试其在边缘设备上的实际表现。结果显示SLMs在微调后表现最佳，但仍存在输出格式一致性问题。


<details>
  <summary>Details</summary>
Motivation: 研究SLMs在资源受限环境中（如边缘设备）是否能高效生成函数调用，替代计算成本高的LLMs。

Method: 采用零样本、少样本和微调方法，结合提示注入，评估SLMs在多领域的函数调用生成效果，并测试其在边缘设备的延迟和内存使用。

Result: SLMs在微调后表现最佳，但对输出格式的遵循能力较弱；提示注入下模型表现稳健，性能略降。边缘设备上测试显示其实用潜力，但需进一步优化。

Conclusion: SLMs在函数调用生成任务中具备潜力，尤其在资源受限场景，但需解决格式一致性和实时性能问题以提升实用性。

Abstract: Function calling is a complex task with widespread applications in domains
such as information retrieval, software engineering and automation. For
example, a query to book the shortest flight from New York to London on January
15 requires identifying the correct parameters to generate accurate function
calls. Large Language Models (LLMs) can automate this process but are
computationally expensive and impractical in resource-constrained settings. In
contrast, Small Language Models (SLMs) can operate efficiently, offering faster
response times, and lower computational demands, making them potential
candidates for function calling on edge devices. In this exploratory empirical
study, we evaluate the efficacy of SLMs in generating function calls across
diverse domains using zero-shot, few-shot, and fine-tuning approaches, both
with and without prompt injection, while also providing the finetuned models to
facilitate future applications. Furthermore, we analyze the model responses
across a range of metrics, capturing various aspects of function call
generation. Additionally, we perform experiments on an edge device to evaluate
their performance in terms of latency and memory usage, providing useful
insights into their practical applicability. Our findings show that while SLMs
improve from zero-shot to few-shot and perform best with fine-tuning, they
struggle significantly with adhering to the given output format. Prompt
injection experiments further indicate that the models are generally robust and
exhibit only a slight decline in performance. While SLMs demonstrate potential
for the function call generation task, our results also highlight areas that
need further refinement for real-time functioning.

</details>


### [189] [Logic-Based Artificial Intelligence Algorithms Supporting Categorical Semantics](https://arxiv.org/abs/2504.19320)
*Ralph Wojtowicz*

Main category: cs.AI

TL;DR: 该论文应用范畴逻辑设计能符号化推理更复杂结构的AI代理，基于Johnstone的sequent演算开发正向链和范式算法，并扩展一阶合一以支持多类理论。这些方法适用于不依赖经典逻辑的语义范畴。


<details>
  <summary>Details</summary>
Motivation: 通过范畴逻辑设计AI代理，使其能对复杂结构对象进行符号化推理，尤其适用于不依赖经典逻辑的语义场景。

Method: 基于Johnstone的sequent演算，开发正向链和范式算法，并扩展一阶合一以支持多类理论、上下文和部分一阶逻辑片段。

Result: 提出的算法适用于笛卡尔范畴中的Horn逻辑规则，并能处理不依赖经典逻辑的语义范畴。

Conclusion: 该方法为AI代理在非经典逻辑环境下的符号推理提供了新工具，拓宽了范畴逻辑的应用范围。

Abstract: This paper seeks to apply categorical logic to the design of artificial
intelligent agents that reason symbolically about objects more richly
structured than sets. Using Johnstone's sequent calculus of terms- and
formulae-in-context, we develop forward chaining and normal form algorithms for
reasoning about objects in cartesian categories with the rules for Horn logic.
We also adapt first-order unification to support multi-sorted theories,
contexts, and fragments of first-order logic. The significance of these
reformulations rests in the fact that they can be applied to reasoning about
objects in semantic categories that do not support classical logic or even all
its connectives.

</details>


### [190] [Neurosymbolic Association Rule Mining from Tabular Data](https://arxiv.org/abs/2504.19354)
*Erkan Karabulut,Paul Groth,Victoria Degeler*

Main category: cs.AI

TL;DR: Aerial+是一种新型神经符号关联规则挖掘方法，通过欠完备自编码器捕捉特征关联，从神经表示中提取规则，有效解决规则爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 高维数据集导致关联规则爆炸，增加执行时间并影响下游任务性能，Aerial+旨在解决这一问题。

Method: 利用欠完备自编码器生成数据的神经表示，通过模型的重建机制提取规则。

Result: 在五个数据集上对比七个基线方法，Aerial+学习到更简洁、高质量的规则集，同时保持或提升准确性。

Conclusion: Aerial+能显著减少执行时间并提升规则质量，适用于可解释机器学习模型。

Abstract: Association Rule Mining (ARM) is the task of mining patterns among data
features in the form of logical rules, with applications across a myriad of
domains. However, high-dimensional datasets often result in an excessive number
of rules, increasing execution time and negatively impacting downstream task
performance. Managing this rule explosion remains a central challenge in ARM
research. To address this, we introduce Aerial+, a novel neurosymbolic ARM
method. Aerial+ leverages an under-complete autoencoder to create a neural
representation of the data, capturing associations between features. It
extracts rules from this neural representation by exploiting the model's
reconstruction mechanism. Extensive evaluations on five datasets against seven
baselines demonstrate that Aerial+ achieves state-of-the-art results by
learning more concise, high-quality rule sets with full data coverage. When
integrated into rule-based interpretable machine learning models, Aerial+
significantly reduces execution time while maintaining or improving accuracy.

</details>


### [191] [Graph Reinforcement Learning for QoS-Aware Load Balancing in Open Radio Access Networks](https://arxiv.org/abs/2504.19499)
*Omid Semiari,Hosein Nikopour,Shilpa Talwar*

Main category: cs.AI

TL;DR: 提出一种基于图强化学习的QoS感知负载均衡方法，优化多频段O-RAN中GBR和BE流量的性能，显著降低QoS违规并提升低速率BE流量性能。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络需满足严格QoS要求，但小区拥塞是主要挑战，因此需要一种高效的负载均衡方法。

Method: 将负载均衡建模为马尔可夫决策过程，使用图神经网络与强化学习结合的框架，通过离策略Dueling DQN训练智能体。

Result: 相比基线方法，QoS违规减少53%，BE流量的5分位速率提升四倍。

Conclusion: 图强化学习方法能有效优化负载均衡，显著提升网络性能。

Abstract: Next-generation wireless cellular networks are expected to provide
unparalleled Quality-of-Service (QoS) for emerging wireless applications,
necessitating strict performance guarantees, e.g., in terms of link-level data
rates. A critical challenge in meeting these QoS requirements is the prevention
of cell congestion, which involves balancing the load to ensure sufficient
radio resources are available for each cell to serve its designated User
Equipments (UEs). In this work, a novel QoS-aware Load Balancing (LB) approach
is developed to optimize the performance of Guaranteed Bit Rate (GBR) and Best
Effort (BE) traffic in a multi-band Open Radio Access Network (O-RAN) under QoS
and resource constraints. The proposed solution builds on Graph Reinforcement
Learning (GRL), a powerful framework at the intersection of Graph Neural
Network (GNN) and RL. The QoS-aware LB is modeled as a Markov Decision Process,
with states represented as graphs. QoS consideration are integrated into both
state representations and reward signal design. The LB agent is then trained
using an off-policy dueling Deep Q Network (DQN) that leverages a GNN-based
architecture. This design ensures the LB policy is invariant to the ordering of
nodes (UE or cell), flexible in handling various network sizes, and capable of
accounting for spatial node dependencies in LB decisions. Performance of the
GRL-based solution is compared with two baseline methods. Results show
substantial performance gains, including a $53\%$ reduction in QoS violations
and a fourfold increase in the 5th percentile rate for BE traffic.

</details>


### [192] [GVPO: Group Variance Policy Optimization for Large Language Model Post-Training](https://arxiv.org/abs/2504.19599)
*Kaichen Zhang,Yuzhong Hong,Junwei Bao,Hongfei Jiang,Yang Song,Dingqian Hong,Hui Xiong*

Main category: cs.AI

TL;DR: GVPO通过将KL约束奖励最大化的解析解直接纳入梯度权重，解决了传统后训练方法（如GRPO）的不稳定性问题，提供了理论保证和实践灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统后训练方法（如GRPO）虽性能优越，但存在训练不稳定的问题，限制了实际应用。GVPO旨在通过理论优化提升稳定性和适应性。

Method: GVPO将KL约束奖励最大化的解析解直接整合到梯度权重中，其梯度反映了隐式奖励与实际奖励的中心距离均方误差，支持灵活的采样分布。

Result: GVPO确保了唯一最优解（即KL约束奖励最大化目标），并避免了传统方法的局限性。

Conclusion: GVPO通过理论保证与实践灵活性的统一，为LLM后训练提供了可靠且多功能的新范式。

Abstract: Post-training plays a crucial role in refining and aligning large language
models to meet specific tasks and human preferences. While recent advancements
in post-training techniques, such as Group Relative Policy Optimization (GRPO),
leverage increased sampling with relative reward scoring to achieve superior
performance, these methods often suffer from training instability that limits
their practical adoption. To address this challenge, we present Group Variance
Policy Optimization (GVPO). GVPO incorporates the analytical solution to
KL-constrained reward maximization directly into its gradient weights, ensuring
alignment with the optimal policy. The method provides intuitive physical
interpretations: its gradient mirrors the mean squared error between the
central distance of implicit rewards and that of actual rewards. GVPO offers
two key advantages: (1) it guarantees a unique optimal solution, exactly the
KL-constrained reward maximization objective, (2) it supports flexible sampling
distributions that avoids on-policy and importance sampling limitations. By
unifying theoretical guarantees with practical adaptability, GVPO establishes a
new paradigm for reliable and versatile LLM post-training.

</details>


### [193] [From Evidence to Belief: A Bayesian Epistemology Approach to Language Models](https://arxiv.org/abs/2504.19622)
*Minsu Kim,Sangryul Kim,James Thorne*

Main category: cs.AI

TL;DR: 该论文从贝叶斯认识论角度分析了语言模型的知识表现，发现模型在真实证据下遵循贝叶斯假设，但在其他证据类型中表现不一致，且高置信度不保证高准确性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索语言模型如何根据不同类型和可靠性的证据调整其置信度和响应，从而理解其在贝叶斯框架下的表现和局限性。

Method: 方法包括构建包含多种证据类型的数据集，并通过口头化置信度、令牌概率和采样分析语言模型的响应和置信度行为。

Result: 结果显示语言模型在真实证据下表现良好，但在其他证据类型中偏离贝叶斯假设，且高置信度与准确性不一致。模型对黄金证据存在偏好，性能受无关性程度影响。

Conclusion: 结论指出语言模型在贝叶斯框架下的表现存在局限性，需要进一步研究其置信度校准和证据处理机制。

Abstract: This paper investigates the knowledge of language models from the perspective
of Bayesian epistemology. We explore how language models adjust their
confidence and responses when presented with evidence with varying levels of
informativeness and reliability. To study these properties, we create a dataset
with various types of evidence and analyze language models' responses and
confidence using verbalized confidence, token probability, and sampling. We
observed that language models do not consistently follow Bayesian epistemology:
language models follow the Bayesian confirmation assumption well with true
evidence but fail to adhere to other Bayesian assumptions when encountering
different evidence types. Also, we demonstrated that language models can
exhibit high confidence when given strong evidence, but this does not always
guarantee high accuracy. Our analysis also reveals that language models are
biased toward golden evidence and show varying performance depending on the
degree of irrelevance, helping explain why they deviate from Bayesian
assumptions.

</details>


### [194] [Fitness Landscape of Large Language Model-Assisted Automated Algorithm Search](https://arxiv.org/abs/2504.19636)
*Fei Liu,Qingfu Zhang,Xialiang Tong,Mingxuan Yuan,Kun Mao*

Main category: cs.AI

TL;DR: 该论文分析了LLM辅助算法搜索（LAS）的适应度景观，揭示其多模态和崎岖特性，并提出实践建议。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在算法设计中有潜力，但其在搜索框架中的适应度景观缺乏研究。

Method: 采用基于图的方法，节点表示算法，边表示转换，评估了六个算法设计任务和六个LLMs。

Result: LAS景观高度多模态且崎岖，任务和LLM间结构差异显著，种群大小影响探索-开发权衡。

Conclusion: 研究增进了对LAS景观的理解，并为设计更有效的LAS方法提供了指导。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in
algorithm design. However, when integrated into search frameworks for iterative
algorithm search, the underlying fitness landscape--critical for understanding
search behaviou--remains underexplored. In this paper, we illustrate and
analyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a
graph-based approach, where nodes represent algorithms and edges denote
transitions between them. We conduct extensive evaluations across six algorithm
design tasks and six commonly used LLMs. Our findings reveal that LAS
landscapes are highly multimodal and rugged, particularly in combinatorial
optimization tasks, with distinct structural variations across tasks and LLMs.
For instance, heuristic design tasks exhibit dense clusters of high-performing
algorithms, while symbolic regression tasks show sparse, scattered
distributions. Additionally, we demonstrate how population size influences
exploration-exploitation trade-offs and the evolving trajectory of elite
algorithms. These insights not only advance our understanding of LAS landscapes
but also provide practical guidance for designing more effective LAS methods.

</details>


### [195] [From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review](https://arxiv.org/abs/2504.19678)
*Mohamed Amine Ferrag,Norbert Tihanyi,Merouane Debbah*

Main category: cs.AI

TL;DR: 本文提出了一种统一的分类法，对2019-2025年间评估大语言模型和自主AI代理的基准进行了比较，并综述了2023-2025年的AI代理框架及其实际应用。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型和自主AI代理的评估基准和框架缺乏统一的分类和全面调研，导致研究环境分散。

Method: 采用了分类法对比60多个基准，并综述了AI代理框架及多领域应用。

Result: 提出了涵盖多领域的基准分类法和框架综述，并探讨了代理间协作协议的实际案例。

Conclusion: 未来研究应关注高级推理策略、多代理系统的失效模式及自动化科学发现等方向。

Abstract: Large language models and autonomous AI agents have evolved rapidly,
resulting in a diverse array of evaluation benchmarks, frameworks, and
collaboration protocols. However, the landscape remains fragmented and lacks a
unified taxonomy or comprehensive survey. Therefore, we present a side-by-side
comparison of benchmarks developed between 2019 and 2025 that evaluate these
models and agents across multiple domains. In addition, we propose a taxonomy
of approximately 60 benchmarks that cover general and academic knowledge
reasoning, mathematical problem-solving, code generation and software
engineering, factual grounding and retrieval, domain-specific evaluations,
multimodal and embodied tasks, task orchestration, and interactive assessments.
Furthermore, we review AI-agent frameworks introduced between 2023 and 2025
that integrate large language models with modular toolkits to enable autonomous
decision-making and multi-step reasoning. Moreover, we present real-world
applications of autonomous AI agents in materials science, biomedical research,
academic ideation, software engineering, synthetic data generation, chemical
reasoning, mathematical problem-solving, geographic information systems,
multimedia, healthcare, and finance. We then survey key agent-to-agent
collaboration protocols, namely the Agent Communication Protocol (ACP), the
Model Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally,
we discuss recommendations for future research, focusing on advanced reasoning
strategies, failure modes in multi-agent LLM systems, automated scientific
discovery, dynamic tool integration via reinforcement learning, integrated
search capabilities, and security vulnerabilities in agent protocols.

</details>


### [196] [Learning Efficiency Meets Symmetry Breaking](https://arxiv.org/abs/2504.19738)
*Yingbin Bai,Sylvie Thiebaux,Felipe Trevizan*

Main category: cs.AI

TL;DR: 该论文提出了一种基于图神经网络的规划器，通过图表示和两种剪枝方法（动作剪枝和状态剪枝）有效处理对称性问题，并在Fast Downward中实现，首次在IPC学习轨道数据集上超越了LAMA。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的规划器在大型搜索空间中表现良好，但对称性问题尚未得到充分探索。本文旨在结合学习效率和对称性检测，提升规划器的性能。

Method: 采用图神经网络表示规划问题，并设计动作剪枝和状态剪枝两种方法来管理搜索过程中的对称性。

Result: 在Fast Downward中集成这些技术后，首次在IPC学习轨道数据集上超越了LAMA。

Conclusion: 通过结合图表示和对称性剪枝方法，显著提升了规划器的性能，为对称性处理提供了有效解决方案。

Abstract: Learning-based planners leveraging Graph Neural Networks can learn search
guidance applicable to large search spaces, yet their potential to address
symmetries remains largely unexplored. In this paper, we introduce a graph
representation of planning problems allying learning efficiency with the
ability to detect symmetries, along with two pruning methods, action pruning
and state pruning, designed to manage symmetries during search. The integration
of these techniques into Fast Downward achieves a first-time success over LAMA
on the latest IPC learning track dataset. Code is released at:
https://github.com/bybeye/Distincter.

</details>


### [197] [Can AI Agents Design and Implement Drug Discovery Pipelines?](https://arxiv.org/abs/2504.19912)
*Khachik Smbatyan,Tsolak Ghukasyan,Tigran Aghajanyan,Hovhannes Dabaghyan,Sergey Adamyan,Aram Bughdaryan,Vahagn Altunyan,Gagik Navasardyan,Aram Davtyan,Anush Hakobyan,Aram Gharibyan,Arman Fahradyan,Artur Hakobyan,Hasmik Mnatsakanyan,Narek Ginoyan,Garik Petrosyan*

Main category: cs.AI

TL;DR: 论文介绍了DO Challenge基准测试，用于评估AI代理在药物发现中的决策能力，并展示了多代理系统Deep Thought的优越表现。


<details>
  <summary>Details</summary>
Motivation: 利用AI代理加速药物发现，减少高成本的实验依赖，通过虚拟筛选等复杂任务验证其决策能力。

Method: 提出DO Challenge基准测试，模拟虚拟筛选场景，要求AI代理独立开发策略，处理分子数据，并评估多代理系统Deep Thought的表现。

Result: Deep Thought系统表现优于多数人类团队，Claude 3.7 Sonnet等模型在主要角色中表现最佳，但仍不及专家设计的方案且稳定性不足。

Conclusion: AI驱动的方法在药物发现中潜力显著，但仍需提升稳定性和性能以接近专家水平。

Abstract: The rapid advancement of artificial intelligence, particularly autonomous
agentic systems based on Large Language Models (LLMs), presents new
opportunities to accelerate drug discovery by improving in-silico modeling and
reducing dependence on costly experimental trials. Current AI agent-based
systems demonstrate proficiency in solving programming challenges and
conducting research, indicating an emerging potential to develop software
capable of addressing complex problems such as pharmaceutical design and drug
discovery. This paper introduces DO Challenge, a benchmark designed to evaluate
the decision-making abilities of AI agents in a single, complex problem
resembling virtual screening scenarios. The benchmark challenges systems to
independently develop, implement, and execute efficient strategies for
identifying promising molecular structures from extensive datasets, while
navigating chemical space, selecting models, and managing limited resources in
a multi-objective context. We also discuss insights from the DO Challenge 2025,
a competition based on the proposed benchmark, which showcased diverse
strategies explored by human participants. Furthermore, we present the Deep
Thought multi-agent system, which demonstrated strong performance on the
benchmark, outperforming most human teams. Among the language models tested,
Claude 3.7 Sonnet, Gemini 2.5 Pro and o3 performed best in primary agent roles,
and GPT-4o, Gemini 2.0 Flash were effective in auxiliary roles. While
promising, the system's performance still fell short of expert-designed
solutions and showed high instability, highlighting both the potential and
current limitations of AI-driven methodologies in transforming drug discovery
and broader scientific research.

</details>


### [198] [Automated decision-making for dynamic task assignment at scale](https://arxiv.org/abs/2504.19933)
*Riccardo Lo Bianco,Willem van Jaarsveld,Jeroen Middelhuis,Luca Begnardi,Remco Dijkman*

Main category: cs.AI

TL;DR: 本文提出了一种基于深度强化学习(DRL)的决策支持系统(DSS)，用于解决动态任务分配问题(DTAP)中的大规模实际案例，通过引入图结构和等效奖励函数，显著提升了任务的平均周期时间。


<details>
  <summary>Details</summary>
Motivation: 现有的DRL方法主要针对小规模、合成问题，忽略了实际应用中的挑战。本文旨在填补这一空白，设计一个适用于现实场景的DTAP解决方案。

Method: 提出了一种DRL代理，包含两个创新点：1) 使用图结构表示观察和动作；2) 设计了与任务平均周期时间最小化目标等效的奖励函数。

Result: 在五个基于真实日志的DTAP实例中，该DRL代理均匹配或优于基线方法，并展示了良好的泛化能力。

Conclusion: 本研究证明了DRL在解决大规模实际DTAP问题中的有效性，尤其是通过创新的图结构和奖励函数设计，为实际应用提供了可行的解决方案。

Abstract: The Dynamic Task Assignment Problem (DTAP) concerns matching resources to
tasks in real time while minimizing some objectives, like resource costs or
task cycle time. In this work, we consider a DTAP variant where every task is a
case composed of a stochastic sequence of activities. The DTAP, in this case,
involves the decision of which employee to assign to which activity to process
requests as quickly as possible. In recent years, Deep Reinforcement Learning
(DRL) has emerged as a promising tool for tackling this DTAP variant, but most
research is limited to solving small-scale, synthetic problems, neglecting the
challenges posed by real-world use cases. To bridge this gap, this work
proposes a DRL-based Decision Support System (DSS) for real-world scale DTAPS.
To this end, we introduce a DRL agent with two novel elements: a graph
structure for observations and actions that can effectively represent any DTAP
and a reward function that is provably equivalent to the objective of
minimizing the average cycle time of tasks. The combination of these two
novelties allows the agent to learn effective and generalizable assignment
policies for real-world scale DTAPs. The proposed DSS is evaluated on five DTAP
instances whose parameters are extracted from real-world logs through process
mining. The experimental evaluation shows how the proposed DRL agent matches or
outperforms the best baseline in all DTAP instances and generalizes on
different time horizons and across instances.

</details>


### [199] [How Group Lives Go Well](https://arxiv.org/abs/2504.19968)
*John Beverley,Regina Hurley*

Main category: cs.AI

TL;DR: 论文提出了一种基于本体工程的群体福祉框架，扩展了反事实账户（CT）理论，结合BFO模型，用于评估群体的功能性、制度角色和历史影响。


<details>
  <summary>Details</summary>
Motivation: 传统福祉理论主要关注个体状态，难以解释个体牺牲对群体进步的贡献。本文旨在解决群体福祉模型的这一挑战。

Method: 论文扩展了反事实账户（CT）理论，并引入BFO模型，将群体福祉评估与群体的功能性、角色和持久性条件联系起来。

Result: 提出的新框架能够更好地建模群体的长期社会贡献，支持对群体福利和社会制度的结构化推理。

Conclusion: 该研究为群体福祉的本体建模提供了新方法，增强了语义互操作性，有助于理解群体繁荣的动态机制。

Abstract: This paper explores the ontological space of group well being, proposing a
framework for representing collective welfare, group functions, and long term
contributions within an ontology engineering context. Traditional well being
theories focus on individual states, often relying on hedonistic, desire
satisfaction, or objective list models. Such approaches struggle to account for
cases where individual sacrifices contribute to broader social progress, a
critical challenge in modeling group flourishing. To address this, the paper
refines and extends the Counterfactual Account (CT) of well being, which
evaluates goodness of an event by comparing an individual's actual well being
with a hypothetical counterpart in a nearby possible world. While useful, this
framework is insufficient for group level ontologies, where well being depends
on functional persistence, institutional roles, and historical impact rather
than immediate individual outcomes. Drawing on Basic Formal Ontology (BFO), the
paper introduces a model in which group flourishing is evaluated in terms of
group functional, where members bear roles and exhibit persistence conditions
akin to biological systems or designed artifacts. This approach enables
semantic interoperability for modeling longitudinal social contributions,
allowing for structured reasoning about group welfare, social institutions, and
group flourishing over time.

</details>


### [200] [Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage](https://arxiv.org/abs/2504.20007)
*Anita Srbinovska,Angela Srbinovska,Vivek Senthil,Adrian Martin,John McCluskey,Ernest Fokoué*

Main category: cs.AI

TL;DR: 本文提出了一种跨学科框架，结合AI和ML技术分析警察随身摄像头（BWC）录像，旨在识别警民互动的行为模式。


<details>
  <summary>Details</summary>
Motivation: 通过分析BWC录像，揭示警民互动中的关键行为动态（如尊重、不尊重、冲突升级或降级），以助力执法部门优化实践。

Method: 采用多模态数据分析（视频、音频、NLP）结合AI和统计机器学习技术。

Result: 开发了可识别行为动态的实用方法，并基于罗切斯特警察局数据进行了验证。

Conclusion: 该框架为执法部门提供了实践指导，同时推动了BWC数据的知识挖掘前沿。

Abstract: This paper proposes a novel interdisciplinary framework for analyzing police
body-worn camera (BWC) footage from the Rochester Police Department (RPD) using
advanced artificial intelligence (AI) and statistical machine learning (ML)
techniques. Our goal is to detect, classify, and analyze patterns of
interaction between police officers and civilians to identify key behavioral
dynamics, such as respect, disrespect, escalation, and de-escalation. We apply
multimodal data analysis by integrating video, audio, and natural language
processing (NLP) techniques to extract meaningful insights from BWC footage. We
present our methodology, computational techniques, and findings, outlining a
practical approach for law enforcement while advancing the frontiers of
knowledge discovery from police BWC data.

</details>


### [201] [Towards Automated Scoping of AI for Social Good Projects](https://arxiv.org/abs/2504.20010)
*Jacob Emmerson,Rayid Ghani,Zheyuan Ryan Shi*

Main category: cs.AI

TL;DR: AI4SG项目面临问题范围界定困难的瓶颈，本文提出一种基于LLM的问题范围界定代理（PSA），可生成专家级项目提案，并通过盲审和AI评估验证其效果。


<details>
  <summary>Details</summary>
Motivation: 解决AI4SG中因专业人才稀缺导致的问题范围界定困难，利用LLM的能力提升效率和质量。

Method: 提出了基于LLM的PSA框架，通过科学文献和实际知识生成项目提案。

Result: PSA生成的提案在盲审和AI评估中表现与专家提案相当。

Conclusion: PSA有效缓解了问题范围界定的挑战，但实际应用中仍需进一步研究完善。

Abstract: Artificial Intelligence for Social Good (AI4SG) is an emerging effort that
aims to address complex societal challenges with the powerful capabilities of
AI systems. These challenges range from local issues with transit networks to
global wildlife preservation. However, regardless of scale, a critical
bottleneck for many AI4SG initiatives is the laborious process of problem
scoping -- a complex and resource-intensive task -- due to a scarcity of
professionals with both technical and domain expertise. Given the remarkable
applications of large language models (LLM), we propose a Problem Scoping Agent
(PSA) that uses an LLM to generate comprehensive project proposals grounded in
scientific literature and real-world knowledge. We demonstrate that our PSA
framework generates proposals comparable to those written by experts through a
blind review and AI evaluations. Finally, we document the challenges of
real-world problem scoping and note several areas for future work.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [202] [MODP: Multi Objective Directional Prompting](https://arxiv.org/abs/2504.18722)
*Aashutosh Nema,Samaksh Gulati,Evangelos Giakoumakis,Bipana Thapaliya*

Main category: cs.CC

TL;DR: 论文提出了MODP框架，通过多目标性和定向提示改进提示工程，提升LLM性能，并在实际应用中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前提示工程多专注于任务优化，忽视了LLM的内在行为，导致方法主观且不精确。

Method: 采用多目标性（考虑LLM行为）和定向提示（基于指标的方法），通过合成数据集验证框架。

Result: 在摘要任务中，性能提升26%，并成功应用于Dell的生产工具。

Conclusion: MODP框架显著提升提示工程效果，具备实际应用价值。

Abstract: Recent advances in large language models (LLMs) have led to their popularity
across multiple use-cases. However, prompt engineering, the process for
optimally utilizing such models, remains approximation-driven and subjective.
Most of the current research on prompt engineering focuses on task-specific
optimization, while neglecting the behavior of the LLM under consideration
during prompt development. This paper introduces MODP -- Multi Objective
Directional Prompting, a framework based on two key concepts: 1)
multi-objectivity: the importance of considering an LLM's intrinsic behavior as
an additional objective in prompt development, and 2) directional prompting: a
metrics-driven method for prompt engineering to ensure development of robust
and high-precision prompts. We demonstrate the effectiveness of our proposed
ideas on a summarization task, using a synthetically created dataset, achieving
a 26% performance gain over initial prompts. Finally, we apply MODP to develop
prompts for Dell's Next Best Action support tool, which is now in production
and is used by more than 10,000 internal support agents and serving millions of
customers worldwide.

</details>


### [203] [Probabilistic and Causal Satisfiability: Constraining the Model](https://arxiv.org/abs/2504.19944)
*Markus Bläser,Julian Dörfler,Maciej Liśkiewicz,Benito van der Zander*

Main category: cs.CC

TL;DR: 该论文研究了概率和因果推理中可满足性问题的复杂性，扩展了先前工作，增加模型约束条件并分析其复杂性。


<details>
  <summary>Details</summary>
Motivation: 探讨约束条件下（如固定因果模型图结构和小模型要求）概率和因果推理中可满足性问题的复杂性变化，填补现有研究的空白。

Method: 扩展研究维度：1) 固定结构因果模型的图结构；2) 研究小模型约束下的复杂性。通过理论分析对比不同算术和PCH层级下的复杂性。

Result: 揭示在固定图结构和小模型约束下，可满足性问题的复杂性变化，并提供不同场景下的复杂性分类。

Conclusion: 模型约束（如固定图结构或小模型）显著影响可满足性问题的复杂性，为实际应用中的推理任务提供了理论指导。

Abstract: We study the complexity of satisfiability problems in probabilistic and
causal reasoning. Given random variables $X_1, X_2,\ldots$ over finite domains,
the basic terms are probabilities of propositional formulas over atomic events
$X_i = x_i$, such as $P(X_1 = x_1)$ or $P(X_1 = x_1 \vee X_2 = x_2)$. The basic
terms can be combined using addition (yielding linear terms) or multiplication
(polynomial terms). The probabilistic satisfiability problem asks whether a
joint probability distribution satisfies a Boolean combination of
(in)equalities over such terms. Fagin et al. (1990) showed that for basic and
linear terms, this problem is NP-complete, making it no harder than Boolean
satisfiability, while Moss\'e et al. (2022) proved that for polynomial terms,
it is complete for the existential theory of the reals.
  Pearl's Causal Hierarchy (PCH) extends the probabilistic setting with
interventional and counterfactual reasoning, enriching the expressiveness of
languages. However, Moss\'e et al. (2022) found that satisfiability complexity
remains unchanged. Van der Zander et al. (2023) showed that introducing a
marginalization operator to languages induces a significant increase in
complexity.
  We extend this line of work by adding two new dimensions to the problem by
constraining the models. First, we fix the graph structure of the underlying
structural causal model, motivated by settings like Pearl's do-calculus, and
give a nearly complete landscape across different arithmetics and PCH levels.
Second, we study small models. While earlier work showed that satisfiable
instances admit polynomial-size models, this is no longer guaranteed with
compact marginalization. We characterize the complexities of satisfiability
under small-model constraints across different settings.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [204] [Validation and Calibration of Semi-Analytical Models for the Event Horizon Telescope Observations of Sagittarius A*](https://arxiv.org/abs/2504.18624)
*Ali SaraerToosi,Avery Broderick*

Main category: astro-ph.HE

TL;DR: 使用生成式机器学习模型\alinet高效生成RIAF图像，评估未建模物理效应的影响，并校准EHT观测的物理参数估计及其不确定性。


<details>
  <summary>Details</summary>
Motivation: 事件视界望远镜（EHT）可以探索黑洞吸积流，但生成合成图像计算成本高。研究旨在通过机器学习模型\alinet提高效率，并评估未建模物理效应对参数估计的影响。

Method: 利用\alinet模型生成RIAF图像，结合广义相对论磁流体动力学模型库，评估星际散射和源内变率等未建模效应引起的误差。

Result: \alinet能够高效生成图像并量化未建模效应的影响，从而校准EHT观测中的物理参数估计及其不确定性。

Conclusion: 研究证明了\alinet在黑洞吸积流模型拟合中的有效性，为EHT观测提供了更可靠的参数校准方法。

Abstract: The Event Horizon Telescope (EHT) enables the exploration of black hole
accretion flows at event-horizon scales. Fitting ray-traced physical models to
EHT observations requires the generation of synthetic images, a task that is
computationally demanding. This study leverages \alinet, a generative machine
learning model, to efficiently produce radiatively inefficient accretion flow
(RIAF) images as a function of the specified physical parameters. \alinet has
previously been shown to be able to interpolate black hole images and their
associated physical parameters after training on a computationally tractable
set of library images. We utilize this model to estimate the uncertainty
introduced by a number of anticipated unmodeled physical effects, including
interstellar scattering and intrinsic source variability. We then use this to
calibrate physical parameter estimates and their associated uncertainties from
RIAF model fits to mock EHT data via a library of general relativistic
magnetohydrodynamics models.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [205] [Transformer-Empowered Actor-Critic Reinforcement Learning for Sequence-Aware Service Function Chain Partitioning](https://arxiv.org/abs/2504.18902)
*Cyril Shih-Huan Hsu,Anestis Dalgkitsis,Chrysa Papagianni,Paola Grosso*

Main category: cs.NI

TL;DR: 论文提出了一种基于Transformer的actor-critic框架，用于6G网络中服务功能链（SFC）的分区问题。该方法通过自注意力机制建模VNF间的复杂依赖关系，并结合探索策略和归一化技术提升性能。实验证明其在长期接受率、资源利用效率和可扩展性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 6G网络中虚拟化网络功能（VNF）的灵活管理是关键，但传统优化方法可扩展性差，数据驱动方法难以平衡效率与SFC的依赖关系。需要一种新方法解决这些限制。

Method: 采用Transformer增强的actor-critic框架，利用自注意力机制建模VNF间依赖，配合ε-LoPe探索策略和渐进回报归一化提升训练稳定性。

Result: 仿真结果表明，该方法在长期接受率、资源利用效率和可扩展性上优于现有技术，同时实现了快速推理。

Conclusion: 研究不仅为6G环境中的SFC分区提供了可扩展且鲁棒的解决方案，还将大语言模型（LLMs）的进展与下一代网络优化联系起来。

Abstract: In the forthcoming era of 6G networks, characterized by unprecedented data
rates, ultra-low latency, and extensive connectivity, effective management of
Virtualized Network Functions (VNFs) is essential. VNFs are software-based
counterparts of traditional hardware devices that facilitate flexible and
scalable service provisioning. Service Function Chains (SFCs), structured as
ordered sequences of VNFs, are pivotal in orchestrating complex network
services. Nevertheless, partitioning SFCs across multi-domain network
infrastructures presents substantial challenges due to stringent latency
constraints and limited resource availability. Conventional optimization-based
methods typically exhibit low scalability, whereas existing data-driven
approaches often fail to adequately balance computational efficiency with the
capability to effectively account for dependencies inherent in SFCs. To
overcome these limitations, we introduce a Transformer-empowered actor-critic
framework specifically designed for sequence-aware SFC partitioning. By
utilizing the self-attention mechanism, our approach effectively models complex
inter-dependencies among VNFs, facilitating coordinated and parallelized
decision-making processes. Additionally, we enhance training stability and
convergence using $\epsilon$-LoPe exploration strategy as well as Asymptotic
Return Normalization. Comprehensive simulation results demonstrate that the
proposed methodology outperforms existing state-of-the-art solutions in terms
of long-term acceptance rates, resource utilization efficiency, and
scalability, while achieving rapid inference. This study not only advances
intelligent network orchestration by delivering a scalable and robust solution
for SFC partitioning within emerging 6G environments, but also bridging recent
advancements in Large Language Models (LLMs) with the optimization of
next-generation networks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [206] [SORT3D: Spatial Object-centric Reasoning Toolbox for Zero-Shot 3D Grounding Using Large Language Models](https://arxiv.org/abs/2504.18684)
*Nader Zantout,Haochen Zhang,Pujith Kachana,Jinkai Qiu,Ji Zhang,Wenshan Wang*

Main category: cs.CV

TL;DR: SORT3D利用2D数据的丰富对象属性，结合启发式空间推理工具箱和大型语言模型（LLMs）的序列推理能力，解决了3D领域中对象指代语言理解和物体定位的挑战，无需文本到3D数据的训练，并在未知环境中实现零样本泛化，最终在两个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 由于场景多样性、细粒度物体数量众多以及语言指代的自由形式复杂性，机器人理解对象相关语言并在3D空间中定位物体具有挑战性。3D领域中自然语言训练数据难以大量获取，因此方法需具备小样本学习及零样本泛化能力。

Method: 提出SORT3D方法，结合2D数据的对象属性、启发式空间推理工具箱和LLMs的序列推理能力，无需文本到3D训练数据，可直接应用于未知环境。

Result: 在复杂视角依赖的定位任务上，SORT3D在两个基准测试中表现最优，并在真实自动驾驶车辆上实时运行，验证了其在未知现实环境中的物体导航能力。

Conclusion: SORT3D通过融合多模态能力实现了小样本和零样本泛化，为机器人在复杂环境中理解语言并定位物体提供了高效解决方案，代码已开源。

Abstract: Interpreting object-referential language and grounding objects in 3D with
spatial relations and attributes is essential for robots operating alongside
humans. However, this task is often challenging due to the diversity of scenes,
large number of fine-grained objects, and complex free-form nature of language
references. Furthermore, in the 3D domain, obtaining large amounts of natural
language training data is difficult. Thus, it is important for methods to learn
from little data and zero-shot generalize to new environments. To address these
challenges, we propose SORT3D, an approach that utilizes rich object attributes
from 2D data and merges a heuristics-based spatial reasoning toolbox with the
ability of large language models (LLMs) to perform sequential reasoning.
Importantly, our method does not require text-to-3D data for training and can
be applied zero-shot to unseen environments. We show that SORT3D achieves
state-of-the-art performance on complex view-dependent grounding tasks on two
benchmarks. We also implement the pipeline to run real-time on an autonomous
vehicle and demonstrate that our approach can be used for object-goal
navigation on previously unseen real-world environments. All source code for
the system pipeline is publicly released at https://github.com/nzantout/SORT3D .

</details>


### [207] [HierSum: A Global and Local Attention Mechanism for Video Summarization](https://arxiv.org/abs/2504.18689)
*Apoorva Beedu,Irfan Essa*

Main category: cs.CV

TL;DR: 本文提出了一种名为HierSum的分层方法，用于总结教学视频。该方法结合了字幕的细粒度局部线索和视频级指令的全局上下文信息，利用“最多回放”统计作为监督信号识别关键片段。实验表明，HierSum在多个基准数据集上优于现有方法，并通过新构建的多模态数据集进一步提升了效果。


<details>
  <summary>Details</summary>
Motivation: 教学视频通常包含多个关键步骤，但现有方法在总结时往往无法有效识别这些步骤。本文旨在通过结合局部和全局信息，以及利用用户行为数据（如“最多回放”），提升视频总结的准确性和实用性。

Method: 提出了HierSum分层方法，整合字幕的局部线索和视频指令的全局信息，并以“最多回放”作为监督信号。在TVSum、BLiSS等数据集上验证效果，并构建了一个多模态数据集用于训练和测试。

Result: 在多个基准数据集（TVSum、BLiSS等）上，HierSum在F1-score和排名相关性等关键指标上表现优于现有方法。新数据集的使用进一步提升了总结效果。

Conclusion: HierSum通过结合局部和全局信息以及用户行为数据，显著提升了教学视频总结的效果。新构建的多模态数据集也有助于模型性能的提升。

Abstract: Video summarization creates an abridged version (i.e., a summary) that
provides a quick overview of the video while retaining pertinent information.
In this work, we focus on summarizing instructional videos and propose a method
for breaking down a video into meaningful segments, each corresponding to
essential steps in the video. We propose \textbf{HierSum}, a hierarchical
approach that integrates fine-grained local cues from subtitles with global
contextual information provided by video-level instructions. Our approach
utilizes the ``most replayed" statistic as a supervisory signal to identify
critical segments, thereby improving the effectiveness of the summary. We
evaluate on benchmark datasets such as TVSum, BLiSS, Mr.HiSum, and the WikiHow
test set, and show that HierSum consistently outperforms existing methods in
key metrics such as F1-score and rank correlation. We also curate a new
multi-modal dataset using WikiHow and EHow videos and associated articles
containing step-by-step instructions. Through extensive ablation studies, we
demonstrate that training on this dataset significantly enhances summarization
on the target datasets.

</details>


### [208] [Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions](https://arxiv.org/abs/2504.19056)
*Mohammad Mahdi Abootorabi,Omid Ghahroodi,Pardis Sadat Zahraei,Hossein Behzadasl,Alireza Mirrokni,Mobina Salimipanah,Arash Rasouli,Bahar Behzadipour,Sara Azarnoush,Benyamin Maleki,Erfan Sadraiye,Kiarash Kiani Feriz,Mahdi Teymouri Nahad,Ali Moghadasi,Abolfazl Eshagh Abianeh,Nizi Nazar,Hamid R. Rabiee,Mahdieh Soleymani Baghshah,Meisam Ahmadi,Ehsaneddin Asgari*

Main category: cs.CV

TL;DR: 这篇论文提供了一个关于生成式AI在角色动画中应用的全面综述，涵盖了面部动画、表情渲染、手势建模、运动合成等领域的最新进展。


<details>
  <summary>Details</summary>
Motivation: 由于生成式AI在角色动画领域的快速发展，需要一个整合性的综述来帮助研究者和开发者全面了解该领域的现状和趋势。

Method: 论文通过综述面部动画、表情渲染、图像合成、avatar创建、手势建模、运动合成、物体生成和纹理合成等领域的最新研究、数据集和实际应用，提供一个全面的视角。

Result: 论文总结了各领域的前沿技术、常用数据集和未来趋势，并提供了入门背景和评估指标。

Conclusion: 这篇综述为研究者和开发者提供了进入生成式AI动画领域的资源，并指出了未来的研究方向。

Abstract: Generative AI is reshaping art, gaming, and most notably animation. Recent
breakthroughs in foundation and diffusion models have reduced the time and cost
of producing animated content. Characters are central animation components,
involving motion, emotions, gestures, and facial expressions. The pace and
breadth of advances in recent months make it difficult to maintain a coherent
view of the field, motivating the need for an integrative review. Unlike
earlier overviews that treat avatars, gestures, or facial animation in
isolation, this survey offers a single, comprehensive perspective on all the
main generative AI applications for character animation. We begin by examining
the state-of-the-art in facial animation, expression rendering, image
synthesis, avatar creation, gesture modeling, motion synthesis, object
generation, and texture synthesis. We highlight leading research, practical
deployments, commonly used datasets, and emerging trends for each area. To
support newcomers, we also provide a comprehensive background section that
introduces foundational models and evaluation metrics, equipping readers with
the knowledge needed to enter the field. We discuss open challenges and map
future research directions, providing a roadmap to advance AI-driven
character-animation technologies. This survey is intended as a resource for
researchers and developers entering the field of generative AI animation or
adjacent fields. Resources are available at:
https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.

</details>


### [209] [PyViT-FUSE: A Foundation Model for Multi-Sensor Earth Observation Data](https://arxiv.org/abs/2504.18770)
*Manuel Weber,Carly Beneke*

Main category: cs.CV

TL;DR: 论文提出PyViT-FUSE，一种处理多模态遥感数据的基础模型，通过注意力机制融合任意数量的混合分辨率波段，并采用金字塔结构的视觉Transformer进行后续处理。模型通过自监督方式训练，展示了注意力机制的可解释性及在下游任务中的适用性。


<details>
  <summary>Details</summary>
Motivation: 遥感数据通常具有多模态和混合分辨率的特点，传统方法难以有效处理。PyViT-FUSE旨在通过注意力机制和多层次Transformer结构，统一处理这类复杂数据，并提升模型的可解释性和下游任务性能。

Method: 模型采用注意力机制融合多模态输入波段，利用金字塔结构的视觉Transformer处理学习到的patch token，基于SwAV算法进行自监督训练，并通过可视化注意力分数验证融合机制。

Result: 实验表明，模型能有效融合多模态数据，注意力机制具有可解释性，且在下游任务中表现良好。

Conclusion: PyViT-FUSE为多模态遥感数据提供了一种统一且可解释的处理框架，自监督训练方式增强了其适用性，未来可进一步优化结构以适应更广泛任务。

Abstract: We propose PyViT-FUSE, a foundation model for earth observation data
explicitly designed to handle multi-modal imagery by learning to fuse an
arbitrary number of mixed-resolution input bands into a single representation
through an attention mechanism. The learned patch tokens are further processed
by a stack of vision transformers with a novel pyramidal structure. We train
the model on a globally sampled dataset in a self-supervised manner, leveraging
core concepts of the SwAV algorithm. We show the interpretability of the fusion
mechanism by visualization of the attention scores and the models applicability
to downstream tasks.

</details>


### [210] [IoT Botnet Detection: Application of Vision Transformer to Classification of Network Flow Traffic](https://arxiv.org/abs/2504.18781)
*Hassan Wasswa,Timothy Lynar,Aziida Nanyonga,Hussein Abbass*

Main category: cs.CV

TL;DR: 该论文介绍了一种新颖的预处理方法，使ViT模型能够适应基于网络流量的物联网僵尸网络攻击检测，并通过特征提取和形状转换提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的物联网网络流量包特征提取工具无法捕捉序列和空间模式，限制了Transformer模型的应用。因此，论文旨在通过预处理方法，利用ViT模型实现更有效的攻击检测。

Method: 从.pcap文件中提取特征，并将每个实例转换为1通道的2D图像形状，以适配ViT分类。此外，改进了ViT模型，使其可以使用除MLP外的其他分类器（如DNN、LSTM和BLSTM）。

Result: 在两种物联网攻击数据集上的评估显示，改进后的模型（包括DNN、LSTM和BLSTM）在多类攻击检测中的精确率、召回率和F1分数表现优异。

Conclusion: 该方法通过适配ViT模型并引入多种分类器，显著提升了物联网僵尸网络攻击检测的性能，为Transformer在其他网络流量任务中的应用提供了新思路。

Abstract: Despite the demonstrated effectiveness of transformer models in NLP, and
image and video classification, the available tools for extracting features
from captured IoT network flow packets fail to capture sequential patterns in
addition to the absence of spatial patterns consequently limiting transformer
model application. This work introduces a novel preprocessing method to adapt
transformer models, the vision transformer (ViT) in particular, for IoT botnet
attack detection using network flow packets. The approach involves feature
extraction from .pcap files and transforming each instance into a 1-channel 2D
image shape, enabling ViT-based classification. Also, the ViT model was
enhanced to allow use any classifier besides Multilayer Perceptron (MLP) that
was deployed in the initial ViT paper. Models including the conventional feed
forward Deep Neural Network (DNN), LSTM and Bidirectional-LSTM (BLSTM)
demonstrated competitive performance in terms of precision, recall, and
F1-score for multiclass-based attack detection when evaluated on two IoT attack
datasets.

</details>


### [211] [Video CLIP Model for Multi-View Echocardiography Interpretation](https://arxiv.org/abs/2504.18800)
*Ryo Takizawa,Satoshi Kodera,Tempei Kabayama,Ryo Matsuoka,Yuta Ando,Yuto Nakamura,Haruki Settai,Norihiko Takeda*

Main category: cs.CV

TL;DR: 研究开发了一种视频语言模型，利用五个不同视角和完整视频序列作为输入，以提高心脏超声视频的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模视觉语言模型（VLMs）主要依赖单帧图像输入，导致对通过心脏运动识别的病症诊断准确性较低。同时，不同视角的超声视频对特定病症的解释更有优势，整合多视角可能提升准确性。

Method: 开发了一个视频语言模型，采用五个不同视角和完整视频序列作为输入，并在60,747个案例的心脏超声视频与临床报告配对数据上进行训练。

Result: 实验结果表明，与仅使用单视角视频或静态图像的模型相比，这种扩展方法实现了更高的解释准确性。

Conclusion: 通过整合多视角和完整视频序列，视频语言模型显著提升了心脏超声视频的诊断准确性，为自动化医学影像解读提供了更有效的解决方案。

Abstract: Echocardiography involves recording videos of the heart using ultrasound,
enabling clinicians to evaluate its condition. Recent advances in large-scale
vision-language models (VLMs) have garnered attention for automating the
interpretation of echocardiographic videos. However, most existing VLMs
proposed for medical interpretation thus far rely on single-frame (i.e., image)
inputs. Consequently, these image-based models often exhibit lower diagnostic
accuracy for conditions identifiable through cardiac motion. Moreover,
echocardiographic videos are recorded from various views that depend on the
direction of ultrasound emission, and certain views are more suitable than
others for interpreting specific conditions. Incorporating multiple views could
potentially yield further improvements in accuracy. In this study, we developed
a video-language model that takes five different views and full video sequences
as input, training it on pairs of echocardiographic videos and clinical reports
from 60,747 cases. Our experiments demonstrate that this expanded approach
achieves higher interpretation accuracy than models trained with only
single-view videos or with still images.

</details>


### [212] [Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2504.19500)
*Yan Wang,Baoxiong Jia,Ziyu Zhu,Siyuan Huang*

Main category: cs.CV

TL;DR: MPEC是一种新颖的掩蔽点-实体对比学习方法，用于开放词汇3D语义分割，通过结合3D实体-语言对齐和不同点云视图间的点-实体一致性，提升语义区分和实例辨别能力。


<details>
  <summary>Details</summary>
Motivation: 提升物理智能的关键在于开放词汇3D场景理解，使智能体能在真实环境中动态交互。

Method: 采用掩蔽点-实体对比学习（MPEC），结合3D实体-语言对齐和多视图点-实体一致性，生成实体特定特征表示。

Result: 在ScanNet数据集上实现了开放词汇3D语义分割的最先进效果，并在8个数据集上展示了零样本场景理解能力的优越性。

Conclusion: MPEC不仅在性能上取得突破，还能推动多样化3D场景理解任务的性能提升。

Abstract: Open-vocabulary 3D scene understanding is pivotal for enhancing physical
intelligence, as it enables embodied agents to interpret and interact
dynamically within real-world environments. This paper introduces MPEC, a novel
Masked Point-Entity Contrastive learning method for open-vocabulary 3D semantic
segmentation that leverages both 3D entity-language alignment and point-entity
consistency across different point cloud views to foster entity-specific
feature representations. Our method improves semantic discrimination and
enhances the differentiation of unique instances, achieving state-of-the-art
results on ScanNet for open-vocabulary 3D semantic segmentation and
demonstrating superior zero-shot scene understanding capabilities. Extensive
fine-tuning experiments on 8 datasets, spanning from low-level perception to
high-level reasoning tasks, showcase the potential of learned 3D features,
driving consistent performance gains across varied 3D scene understanding
tasks. Project website: https://mpec-3d.github.io/

</details>


### [213] [Audio-Driven Talking Face Video Generation with Joint Uncertainty Learning](https://arxiv.org/abs/2504.18810)
*Yifan Xie,Fei Ma,Yi Bin,Ying He,Fei Yu*

Main category: cs.CV

TL;DR: 论文提出了联合不确定性学习网络（JULNet），用于高质量说话人脸视频生成，通过引入视觉误差的不确定性表示，解决了现有系统中的视觉质量不一致和输入条件依赖性问题。


<details>
  <summary>Details</summary>
Motivation: 现有技术在说话人脸视频生成中忽视了视觉不确定性的学习，导致生成视频质量和鲁棒性不足。论文旨在通过联合优化误差和不确定性来提升模型性能。

Method: 设计了一个不确定性模块，分别预测误差图和不确定性图，并通过KL散度和直方图技术匹配两者的分布。

Result: 实验表明，JULNet在高保真度和音频-唇同步方面优于现有方法。

Conclusion: 联合学习不确定性和误差显著提升了说话人脸视频生成的性能和鲁棒性。

Abstract: Talking face video generation with arbitrary speech audio is a significant
challenge within the realm of digital human technology. The previous studies
have emphasized the significance of audio-lip synchronization and visual
quality. Currently, limited attention has been given to the learning of visual
uncertainty, which creates several issues in existing systems, including
inconsistent visual quality and unreliable performance across different input
conditions. To address the problem, we propose a Joint Uncertainty Learning
Network (JULNet) for high-quality talking face video generation, which
incorporates a representation of uncertainty that is directly related to visual
error. Specifically, we first design an uncertainty module to individually
predict the error map and uncertainty map after obtaining the generated image.
The error map represents the difference between the generated image and the
ground truth image, while the uncertainty map is used to predict the
probability of incorrect estimates. Furthermore, to match the uncertainty
distribution with the error distribution through a KL divergence term, we
introduce a histogram technique to approximate the distributions. By jointly
optimizing error and uncertainty, the performance and robustness of our model
can be enhanced. Extensive experiments demonstrate that our method achieves
superior high-fidelity and audio-lip synchronization in talking face video
generation compared to previous methods.

</details>


### [214] [Exploiting Multiple Representations: 3D Face Biometrics Fusion with Application to Surveillance](https://arxiv.org/abs/2504.18886)
*Simone Maurizio La Cava,Roberto Casula,Sara Concas,Giulia Orrù,Ruben Tolosana,Martin Drahansky,Julian Fierrez,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: 该研究探讨如何结合多种3D人脸重建算法提升无约束场景下的人脸识别性能，并通过参数与非参数融合方法增强生物识别鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决3D人脸重建算法在不同应用场景中的局限，提升人脸识别系统在挑战性无约束条件下的性能。

Method: 通过融合多种3D人脸重建算法，采用参数与非参数分数级融合方法，分析不同条件下的系统表现。

Result: 实验表明多算法融合能缓解跨场景泛化问题，高级融合策略可提升3D人脸识别系统的可靠性。

Conclusion: 提出的融合方法不仅适用于身份识别，还可扩展至其他人脸生物识别任务。

Abstract: 3D face reconstruction (3DFR) algorithms are based on specific assumptions
tailored to the limits and characteristics of the different application
scenarios. In this study, we investigate how multiple state-of-the-art 3DFR
algorithms can be used to generate a better representation of subjects, with
the final goal of improving the performance of face recognition systems in
challenging uncontrolled scenarios. We also explore how different parametric
and non-parametric score-level fusion methods can exploit the unique strengths
of multiple 3DFR algorithms to enhance biometric recognition robustness. With
this goal, we propose a comprehensive analysis of several face recognition
systems across diverse conditions, such as varying distances and camera setups,
intra-dataset and cross-dataset, to assess the robustness of the proposed
ensemble method. The results demonstrate that the distinct information provided
by different 3DFR algorithms can alleviate the problem of generalizing over
multiple application scenarios. In addition, the present study highlights the
potential of advanced fusion strategies to enhance the reliability of
3DFR-based face recognition systems, providing the research community with key
insights to exploit them in real-world applications effectively. Although the
experiments are carried out in a specific face verification setup, our proposed
fusion-based 3DFR methods may be applied to other tasks around face biometrics
that are not strictly related to identity recognition.

</details>


### [215] [Kinship Verification through a Forest Neural Network](https://arxiv.org/abs/2504.18910)
*Ali Nazari,Mohsen Ebrahimi Moghaddam,Omidreza Borzoei*

Main category: cs.CV

TL;DR: 该论文提出了一种结合图神经网络和人脸表示的亲属关系验证方法，表现优于早期方法，并在KinFaceW数据集上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 早期基于人脸表示的亲属关系验证方法准确性较低，而从头学习的联合表示方法效果更好。本文旨在利用图神经网络概念提升人脸表示的效能，使其能与联合表示算法媲美。

Method: 提出了一种结合图神经网络的人脸表示方法，设计了分类模块结构，并引入新的损失组合（如中心损失）逐步优化网络训练。

Result: 在KinFaceW-I和II数据集上验证了方法的有效性，其中KinFaceW-II的平均性能提升1.6，接近KinFaceW-I的最佳结果。

Conclusion: 该方法通过图神经网络和优化损失设计，显著提升了亲属关系验证的准确性，代码已开源。

Abstract: Early methods used face representations in kinship verification, which are
less accurate than joint representations of parents' and children's facial
images learned from scratch. We propose an approach featuring graph neural
network concepts to utilize face representations and have comparable results to
joint representation algorithms. Moreover, we designed the structure of the
classification module and introduced a new combination of losses to engage the
center loss gradually in training our network. Additionally, we conducted
experiments on KinFaceW-I and II, demonstrating the effectiveness of our
approach. We achieved the best result on KinFaceW-II, an average improvement of
nearly 1.6 for all kinship types, and we were near the best on KinFaceW-I. The
code is available at https://github.com/ali-nazari/Kinship-Verification

</details>


### [216] [VISUALCENT: Visual Human Analysis using Dynamic Centroid Representation](https://arxiv.org/abs/2504.19032)
*Niaz Ahmad,Youngmoon Lee,Guanghui Wang*

Main category: cs.CV

TL;DR: VISUALCENT是一个统一的姿态和实例分割框架，通过基于质心的自底向上关键点检测范式，结合Disk Representation和KeyCentroid，提升了多人视觉分析的泛化性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在多人视觉分析中泛化性和扩展性的不足。

Method: 使用基于质心的自底向上关键点检测，包括Keypoint Heatmap、Disk Representation和KeyCentroid；同时引入动态质心MaskCentroid快速聚类像素到特定人实例。

Result: 在COCO和OCHuman数据集上，VISUALCENT在mAP分数和执行帧率上优于现有方法。

Conclusion: VISUALCENT展示了在精度和实时性能上的优势，为多人视觉分析提供了高效解决方案。

Abstract: We introduce VISUALCENT, a unified human pose and instance segmentation
framework to address generalizability and scalability limitations to multi
person visual human analysis. VISUALCENT leverages centroid based bottom up
keypoint detection paradigm and uses Keypoint Heatmap incorporating Disk
Representation and KeyCentroid to identify the optimal keypoint coordinates.
For the unified segmentation task, an explicit keypoint is defined as a dynamic
centroid called MaskCentroid to swiftly cluster pixels to specific human
instance during rapid changes in human body movement or significantly occluded
environment. Experimental results on COCO and OCHuman datasets demonstrate
VISUALCENTs accuracy and real time performance advantages, outperforming
existing methods in mAP scores and execution frame rate per second. The
implementation is available on the project page.

</details>


### [217] [MIA-Mind: A Multidimensional Interactive Attention Mechanism Based on MindSpore](https://arxiv.org/abs/2504.19080)
*Zhenkai Qin,Jiaquan Liang,Qiao Fang*

Main category: cs.CV

TL;DR: MIA-Mind提出了一种轻量级多维交互注意力机制，联合建模空间与通道特征，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制独立建模通道与空间特征，忽略其内在关联性，限制了效果。

Method: 提出了MIA-Mind，基于MindSpore框架，通过跨注意力融合策略联合建模空间与通道特征。

Result: 在CIFAR-10、ISBI2012和CIC-IDS2017三个数据集上分别达到82.9%、78.7%和91.9%的准确率。

Conclusion: 实验结果验证了MIA-Mind的通用性、轻量级设计和泛化能力，未来将扩展至大规模数据集与分布式部署。

Abstract: Attention mechanisms have significantly advanced deep learning by enhancing
feature representation through selective focus. However, existing approaches
often independently model channel importance and spatial saliency, overlooking
their inherent interdependence and limiting their effectiveness. To address
this limitation, we propose MIA-Mind, a lightweight and modular
Multidimensional Interactive Attention Mechanism, built upon the MindSpore
framework. MIA-Mind jointly models spatial and channel features through a
unified cross-attentive fusion strategy, enabling fine-grained feature
recalibration with minimal computational overhead. Extensive experiments are
conducted on three representative datasets: on CIFAR-10, MIA-Mind achieves an
accuracy of 82.9\%; on ISBI2012, it achieves an accuracy of 78.7\%; and on
CIC-IDS2017, it achieves an accuracy of 91.9\%. These results validate the
versatility, lightweight design, and generalization ability of MIA-Mind across
heterogeneous tasks. Future work will explore the extension of MIA-Mind to
large-scale datasets, the development of ada,ptive attention fusion strategies,
and distributed deployment to further enhance scalability and robustness.

</details>


### [218] [PAD: Phase-Amplitude Decoupling Fusion for Multi-Modal Land Cover Classification](https://arxiv.org/abs/2504.19136)
*Huiling Zheng,Xian Zhong,Bin Liu,Yi Xiao,Bihan Wen,Xiaofeng Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为PAD的频率感知框架，通过解耦傅里叶域中的相位（模态共享）和幅度（模态特定）成分，解决了SAR和RGB图像在土地覆盖分类中的模态异质性和光谱互补性利用不足的问题，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在解耦共享结构特征和模态特定的辐射属性方面存在不足，导致特征冲突和信息丢失。为了解决这一问题，作者提出了PAD框架。

Method: PAD框架包括两个关键组件：1）相位谱校正（PSC），通过卷积引导的缩放对齐跨模态相位特征；2）幅度谱融合（ASF），使用频率自适应多层感知器动态整合高、低频细节。

Result: 在WHU-OPT-SAR和DDHR-SK数据集上的实验表明，该方法达到了最先进的性能。

Conclusion: 该研究为物理感知的多模态融合在遥感领域建立了新范式。

Abstract: The fusion of Synthetic Aperture Radar (SAR) and RGB imagery for land cover
classification remains challenging due to modality heterogeneity and the
underutilization of spectral complementarity. Existing methods often fail to
decouple shared structural features from modality-specific radiometric
attributes, leading to feature conflicts and information loss. To address this
issue, we propose Phase-Amplitude Decoupling (PAD), a frequency-aware framework
that separates phase (modality-shared) and amplitude (modality-specific)
components in the Fourier domain. Specifically, PAD consists of two key
components: 1) Phase Spectrum Correction (PSC), which aligns cross-modal phase
features through convolution-guided scaling to enhance geometric consistency,
and 2) Amplitude Spectrum Fusion (ASF), which dynamically integrates
high-frequency details and low-frequency structures using frequency-adaptive
multilayer perceptrons. This approach leverages SAR's sensitivity to
morphological features and RGB's spectral richness. Extensive experiments on
WHU-OPT-SAR and DDHR-SK datasets demonstrate state-of-the-art performance. Our
work establishes a new paradigm for physics-aware multi-modal fusion in remote
sensing. The code will be available at https://github.com/RanFeng2/PAD.

</details>


### [219] [CapsFake: A Multimodal Capsule Network for Detecting Instruction-Guided Deepfakes](https://arxiv.org/abs/2504.19212)
*Tuan Nguyen,Naseem Khan,Issa Khalil*

Main category: cs.CV

TL;DR: 提出了名为CapsFake的多模态胶囊网络，用于检测由指令引导的深度伪造图像编辑，相比现有方法检测准确率提升20%，抗干扰和对抗攻击能力强。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术在指令引导的图像编辑中快速发展，现有的检测系统难以察觉其细微且上下文感知的篡改，亟需更强大的防御方法。

Method: 设计了一种多模态胶囊网络（CapsFake），通过整合视觉、文本和频域模态的低级胶囊，并利用竞争性路由机制动态聚合局部特征，精准识别篡改区域。

Result: 在多个数据集上测试，CapsFake的检测准确率比现有最优方法高出20%，在自然扰动和对抗攻击下的检测率分别超过94%和96%，且对未见过的编辑场景泛化能力强。

Conclusion: CapsFake为对抗复杂图像篡改提供了强有力的框架，展现出卓越的检测性能和鲁棒性。

Abstract: The rapid evolution of deepfake technology, particularly in
instruction-guided image editing, threatens the integrity of digital images by
enabling subtle, context-aware manipulations. Generated conditionally from real
images and textual prompts, these edits are often imperceptible to both humans
and existing detection systems, revealing significant limitations in current
defenses. We propose a novel multimodal capsule network, CapsFake, designed to
detect such deepfake image edits by integrating low-level capsules from visual,
textual, and frequency-domain modalities. High-level capsules, predicted
through a competitive routing mechanism, dynamically aggregate local features
to identify manipulated regions with precision. Evaluated on diverse datasets,
including MagicBrush, Unsplash Edits, Open Images Edits, and Multi-turn Edits,
CapsFake outperforms state-of-the-art methods by up to 20% in detection
accuracy. Ablation studies validate its robustness, achieving detection rates
above 94% under natural perturbations and 96% against adversarial attacks, with
excellent generalization to unseen editing scenarios. This approach establishes
a powerful framework for countering sophisticated image manipulations.

</details>


### [220] [CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis](https://arxiv.org/abs/2504.19223)
*Alexander Baumann,Leonardo Ayala,Silvia Seidlitz,Jan Sellner,Alexander Studier-Fischer,Berkin Özdemir,Lena Maier-Hein,Slobodan Ilic*

Main category: cs.CV

TL;DR: 该论文提出了一种名为CARL的相机无关表征学习方法，旨在解决光谱成像中因相机差异导致的AI模型泛化性问题，通过波长位置编码和自注意力-交叉注意力机制实现跨模态学习，并在多个领域实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 光谱成像在不同领域的应用潜力巨大，但由于相机通道维度和波长捕捉的差异，现有AI方法泛化性不足，限制了跨相机应用。因此，需要一种能适应不同光谱成像模态的通用表征学习方法。

Method: 提出CARL模型，采用波长位置编码和自注意力-交叉注意力机制，将任意通道维度的光谱图像压缩为相机无关的嵌入表示。通过新型光谱自监督JEPA策略进行谱空间预训练。

Result: 在医学成像、自动驾驶和卫星成像等领域的大规模实验中，CARL表现出对光谱异质性的独特鲁棒性，在模拟和真实跨相机光谱变化的数据集上均优于其他方法。

Conclusion: CARL的扩展性和多功能性使其成为未来光谱基础模型的骨干，为跨相机和跨模态的光谱成像AI应用提供了通用解决方案。

Abstract: Spectral imaging offers promising applications across diverse domains,
including medicine and urban scene understanding, and is already established as
a critical modality in remote sensing. However, variability in channel
dimensionality and captured wavelengths among spectral cameras impede the
development of AI-driven methodologies, leading to camera-specific models with
limited generalizability and inadequate cross-camera applicability. To address
this bottleneck, we introduce $\textbf{CARL}$, a model for
$\textbf{C}$amera-$\textbf{A}$gnostic $\textbf{R}$epresentation
$\textbf{L}$earning across RGB, multispectral, and hyperspectral imaging
modalities. To enable the conversion of a spectral image with any channel
dimensionality to a camera-agnostic embedding, we introduce wavelength
positional encoding and a self-attention-cross-attention mechanism to compress
spectral information into learned query representations. Spectral-spatial
pre-training is achieved with a novel spectral self-supervised JEPA-inspired
strategy tailored to CARL. Large-scale experiments across the domains of
medical imaging, autonomous driving, and satellite imaging demonstrate our
model's unique robustness to spectral heterogeneity, outperforming on datasets
with simulated and real-world cross-camera spectral variations. The scalability
and versatility of the proposed approach position our model as a backbone for
future spectral foundation models.

</details>


### [221] [Platonic Grounding for Efficient Multimodal Language Models](https://arxiv.org/abs/2504.19327)
*Moulik Choraria,Xinbo Wu,Akhil Bhimaraju,Nitesh Sekhar,Yue Wu,Xu Zhang,Prateek Singhal,Lav R. Varshney*

Main category: cs.CV

TL;DR: 论文提出了一种改进多模态框架的高效微调和推理方法，利用预训练模型深层对齐特性，保持或提升性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型数据和参数规模的扩大，性能提升逐渐减弱且训练成本高昂，尤其是在多模态学习中，推理成本直接影响模型实用性。

Method: 基于预训练模型深层跨模态隐式对齐的研究，提出了一种简单的多模态框架改进方法。

Result: 该方法在保持或提升基线性能的同时，显著减少了训练和推理时的计算资源消耗。

Conclusion: 该工作不仅优化了多模态学习效率，还为高效整合预训练模型提供了新思路。

Abstract: The hyperscaling of data and parameter count in Transformer-based models is
yielding diminishing performance improvement, especially when weighed against
training costs. Such plateauing indicates the importance of methods for more
efficient finetuning and inference, while retaining similar performance. This
is especially relevant for multimodal learning paradigms, where inference costs
of processing multimodal tokens can determine the model's practical viability.
At the same time, research on representations and mechanistic interpretability
has improved our understanding of the inner workings of Transformer-based
models; one such line of work reveals an implicit alignment in the deeper
layers of pretrained models, across modalities. Taking inspiration from this,
we motivate and propose a simple modification to existing multimodal frameworks
that rely on aligning pretrained models. We demonstrate that our approach
maintains and, in some cases, even improves performance of baseline methods
while achieving significant gains in both training and inference-time compute.
Our work also has implications for combining pretrained models into larger
systems efficiently.

</details>


### [222] [Mitigating Bias in Facial Recognition Systems: Centroid Fairness Loss Optimization](https://arxiv.org/abs/2504.19370)
*Jean-Rémy Conti,Stéphan Clémençon*

Main category: cs.CV

TL;DR: 提出一种后处理方法，通过优化基于质心的分数回归损失，提高预训练人脸识别模型的公平性，同时保持全局准确性。


<details>
  <summary>Details</summary>
Motivation: 社会对公平AI系统的需求推动了研究，尤其是人脸识别系统在不同人群中的错误率差异问题，需要满足公平性标准。

Method: 采用后处理方法，优化基于质心的分数回归损失，改进预训练模型的公平性。

Result: 实验结果表明，该方法显著提高了公平性，并保持了全局准确性。

Conclusion: 该方法在计算效率和公平性提升方面具有优势，为公平人脸识别提供了可行解决方案。

Abstract: The urging societal demand for fair AI systems has put pressure on the
research community to develop predictive models that are not only globally
accurate but also meet new fairness criteria, reflecting the lack of disparate
mistreatment with respect to sensitive attributes ($\textit{e.g.}$ gender,
ethnicity, age). In particular, the variability of the errors made by certain
Facial Recognition (FR) systems across specific segments of the population
compromises the deployment of the latter, and was judged unacceptable by
regulatory authorities. Designing fair FR systems is a very challenging
problem, mainly due to the complex and functional nature of the performance
measure used in this domain ($\textit{i.e.}$ ROC curves) and because of the
huge heterogeneity of the face image datasets usually available for training.
In this paper, we propose a novel post-processing approach to improve the
fairness of pre-trained FR models by optimizing a regression loss which acts on
centroid-based scores. Beyond the computational advantages of the method, we
present numerical experiments providing strong empirical evidence of the gain
in fairness and of the ability to preserve global accuracy.

</details>


### [223] [EarthMapper: Visual Autoregressive Models for Controllable Bidirectional Satellite-Map Translation](https://arxiv.org/abs/2504.19432)
*Zhe Dong,Yuzhe Sun,Tianzhu Liu,Wangmeng Zuo,Yanfeng Gu*

Main category: cs.CV

TL;DR: 提出了一种名为EarthMapper的自回归框架，用于可控的卫星图像与地图双向翻译，解决了模态间像素对齐不足和高层次抽象/高质量视觉合成的技术挑战，并在新数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 卫星图像与地图的双向翻译在城规和灾难响应中潜力巨大，但存在像素对齐不足和抽象/合成技术复杂的挑战。

Method: 采用地理坐标嵌入和多尺度特征对齐，结合语义注入和关键点自适应指导机制，统一双向翻译于单次训练。

Result: 在大规模数据集上表现卓越，视觉真实感、语义一致性和结构保真度显著优于现有方法，并在零样本任务中展示多能性。

Conclusion: EarthMapper通过创新设计有效解决了双向翻译的核心难题，展现了实际应用潜力。

Abstract: Satellite imagery and maps, as two fundamental data modalities in remote
sensing, offer direct observations of the Earth's surface and
human-interpretable geographic abstractions, respectively. The task of
bidirectional translation between satellite images and maps (BSMT) holds
significant potential for applications in urban planning and disaster response.
However, this task presents two major challenges: first, the absence of precise
pixel-wise alignment between the two modalities substantially complicates the
translation process; second, it requires achieving both high-level abstraction
of geographic features and high-quality visual synthesis, which further
elevates the technical complexity. To address these limitations, we introduce
EarthMapper, a novel autoregressive framework for controllable bidirectional
satellite-map translation. EarthMapper employs geographic coordinate embeddings
to anchor generation, ensuring region-specific adaptability, and leverages
multi-scale feature alignment within a geo-conditioned joint scale
autoregression (GJSA) process to unify bidirectional translation in a single
training cycle. A semantic infusion (SI) mechanism is introduced to enhance
feature-level consistency, while a key point adaptive guidance (KPAG) mechanism
is proposed to dynamically balance diversity and precision during inference. We
further contribute CNSatMap, a large-scale dataset comprising 302,132 precisely
aligned satellite-map pairs across 38 Chinese cities, enabling robust
benchmarking. Extensive experiments on CNSatMap and the New York dataset
demonstrate EarthMapper's superior performance, achieving significant
improvements in visual realism, semantic consistency, and structural fidelity
over state-of-the-art methods. Additionally, EarthMapper excels in zero-shot
tasks like in-painting, out-painting and coordinate-conditional generation,
underscoring its versatility.

</details>


### [224] [Dual-Branch Residual Network for Cross-Domain Few-Shot Hyperspectral Image Classification with Refined Prototype](https://arxiv.org/abs/2504.19074)
*Anyong Qin,Chaoqi Yuan,Qiang Li,Feng Yang,Tiecheng Song,Chenqiang Gao*

Main category: cs.CV

TL;DR: 该论文提出了一种双分支残差网络，通过整合空间和光谱特征来解决高光谱图像分类中计算成本高和泛化能力差的问题，并通过改进的原型质量和域适应策略提升了少样本场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 针对高光谱图像分类中3D卷积神经网络计算成本高、泛化能力差的问题，以及少样本学习和域适应带来的挑战，作者提出了一种新的解决方案。

Method: 采用双分支残差网络结构，并行整合空间和光谱特征；通过改进的原型生成方法和核概率匹配策略，提升了少样本学习和跨域适应能力。

Result: 在四个公开的高光谱数据集上的实验结果表明，该方法在性能上优于其他现有方法。

Conclusion: 所提出的双分支残差网络和域适应策略在高光谱图像分类任务中表现出色，尤其在少样本和跨域场景下具有优势。

Abstract: Convolutional neural networks (CNNs) are effective for hyperspectral image
(HSI) classification, but their 3D convolutional structures introduce high
computational costs and limited generalization in few-shot scenarios. Domain
shifts caused by sensor differences and environmental variations further hinder
cross-dataset adaptability. Metric-based few-shot learning (FSL) prototype
networks mitigate this problem, yet their performance is sensitive to prototype
quality, especially with limited samples. To overcome these challenges, a
dual-branch residual network that integrates spatial and spectral features via
parallel branches is proposed in this letter. Additionally, more robust refined
prototypes are obtained through a regulation term. Furthermore, a kernel
probability matching strategy aligns source and target domain features,
alleviating domain shift. Experiments on four publicly available HSI datasets
illustrate that the proposal achieves superior performance compared to other
methods.

</details>


### [225] [CLIP-KOA: Enhancing Knee Osteoarthritis Diagnosis with Multi-Modal Learning and Symmetry-Aware Loss Functions](https://arxiv.org/abs/2504.19443)
*Yejin Jeong,Donghun Lee*

Main category: cs.CV

TL;DR: 提出了一种基于CLIP的框架(CLIP-KOA)，通过结合图像和文本信息以及引入对称性和一致性损失，提升了膝骨关节炎(KOA)严重程度预测的一致性和可靠性，准确率达71.86%。


<details>
  <summary>Details</summary>
Motivation: 膝骨关节炎(KOA)是一种全球常见的慢性肌肉骨骼疾病，早期诊断至关重要。现有的KL评分系统因其高主观性和观察者间差异导致诊断不一致，因此需要开发自动化的深度学习方法改善这一问题。

Method: 利用CLIP框架，结合图像和文本信息，并引入对称性损失(Symmetry Loss)和一致性损失(Consistency Loss)，确保原始图像与翻转图像的预测结果一致。

Result: CLIP-KOA在KOA严重程度预测任务中达到71.86%的准确率，比标准CLIP模型提升2.36%。

Conclusion: 该研究为数据驱动的医学预测提供了新方向，不仅提高了细粒度诊断的可靠性，还探索了多模态方法在医学图像分析中的应用。

Abstract: Knee osteoarthritis (KOA) is a universal chronic musculoskeletal disorders
worldwide, making early diagnosis crucial. Currently, the Kellgren and Lawrence
(KL) grading system is widely used to assess KOA severity. However, its high
inter-observer variability and subjectivity hinder diagnostic consistency. To
address these limitations, automated diagnostic techniques using deep learning
have been actively explored in recent years. In this study, we propose a
CLIP-based framework (CLIP-KOA) to enhance the consistency and reliability of
KOA grade prediction. To achieve this, we introduce a learning approach that
integrates image and text information and incorporate Symmetry Loss and
Consistency Loss to ensure prediction consistency between the original and
flipped images. CLIP-KOA achieves state-of-the-art accuracy of 71.86\% on KOA
severity prediction task, and ablation studies show that CLIP-KOA has 2.36\%
improvement in accuracy over the standard CLIP model due to our contribution.
This study shows a novel direction for data-driven medical prediction not only
to improve reliability of fine-grained diagnosis and but also to explore
multimodal methods for medical image analysis. Our code is available at
https://github.com/anonymized-link.

</details>


### [226] [Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video](https://arxiv.org/abs/2504.19475)
*Sonia Joseph,Praneet Suresh,Lorenz Hufe,Edward Stevinson,Robert Graham,Yash Vadi,Danilo Bzdok,Sebastian Lapuschkin,Lee Sharkey,Blake Aaron Richards*

Main category: cs.CV

TL;DR: Prisma是一个加速视觉机制可解释性研究的开源框架，提供75+视觉和视频Transformer工具包、80+预训练稀疏自编码器权重及分析工具，研究发现视觉SAE可呈现比语言SAE更低的稀疏性，且某些情况下能降低模型损失。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏易用框架和预训练权重，视觉机制可解释性研究进展缓慢，Prisma旨在通过开源工具和资源降低该领域门槛。

Method: 开发统一工具包（支持SAE、转码器/跨码器训练）、预训练权重、激活缓存、电路分析和可视化工具。

Result: 发现视觉SAE可展现更低稀疏性，部分SAE重建能减少模型损失。

Conclusion: Prisma为理解视觉模型内部机制开辟新方向，同时降低研究门槛。

Abstract: Robust tooling and publicly available pre-trained models have helped drive
recent advances in mechanistic interpretability for language models. However,
similar progress in vision mechanistic interpretability has been hindered by
the lack of accessible frameworks and pre-trained weights. We present Prisma
(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an
open-source framework designed to accelerate vision mechanistic
interpretability research, providing a unified toolkit for accessing 75+ vision
and video transformers; support for sparse autoencoder (SAE), transcoder, and
crosscoder training; a suite of 80+ pre-trained SAE weights; activation
caching, circuit analysis tools, and visualization tools; and educational
resources. Our analysis reveals surprising findings, including that effective
vision SAEs can exhibit substantially lower sparsity patterns than language
SAEs, and that in some instances, SAE reconstructions can decrease model loss.
Prisma enables new research directions for understanding vision model internals
while lowering barriers to entry in this emerging field.

</details>


### [227] [Point2Quad: Generating Quad Meshes from Point Clouds via Face Prediction](https://arxiv.org/abs/2504.19545)
*Zezeng Li,Zhihui Qi,Weimin Wang,Ziliang Wang,Junyi Duan,Na Lei*

Main category: cs.CV

TL;DR: 提出了一种名为Point2Quad的学习方法，首次实现从点云生成纯四边形网格，通过融合点级和面级特征解决四边形网格生成中的共面性、凸性和纯四边形限制问题。


<details>
  <summary>Details</summary>
Motivation: 四边形网格在几何建模和计算力学中至关重要，但目前基于学习的方法多集中于三角网格，纯四边形网格生成的研究较少，主要由于其共面性、凸性和纯四边形限制的技术挑战。

Method: Point2Quad采用k-NN候选生成策略，结合共面性和方形度，随后通过两个编码器提取几何和拓扑特征，融合后训练分类器，并设计了复合损失函数，最后通过四边形专用后处理优化结果。

Result: 在清晰和噪声数据上的大量实验表明，Point2Quad在综合指标上优于基线方法，展现了其有效性和优越性。

Conclusion: Point2Quad是第一个基于学习的纯四边形网格生成方法，通过创新的特征融合和损失设计，成功解决了四边形网格生成的难题，实验验证了其优越性。

Abstract: Quad meshes are essential in geometric modeling and computational mechanics.
Although learning-based methods for triangle mesh demonstrate considerable
advancements, quad mesh generation remains less explored due to the challenge
of ensuring coplanarity, convexity, and quad-only meshes. In this paper, we
present Point2Quad, the first learning-based method for quad-only mesh
generation from point clouds. The key idea is learning to identify quad mesh
with fused pointwise and facewise features. Specifically, Point2Quad begins
with a k-NN-based candidate generation considering the coplanarity and
squareness. Then, two encoders are followed to extract geometric and
topological features that address the challenge of quad-related constraints,
especially by combining in-depth quadrilaterals-specific characteristics.
Subsequently, the extracted features are fused to train the classifier with a
designed compound loss. The final results are derived after the refinement by a
quad-specific post-processing. Extensive experiments on both clear and noise
data demonstrate the effectiveness and superiority of Point2Quad, compared to
baseline methods under comprehensive metrics.

</details>


### [228] [Neural network task specialization via domain constraining](https://arxiv.org/abs/2504.19592)
*Roman Malashin,Daniil Ilyukhin*

Main category: cs.CV

TL;DR: 提出了一种通过任务特定领域约束来增强神经网络性能的方法，证明在不增加数据或改变训练机制的情况下，仅通过约束类别标签空间即可提升通用网络的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了提高神经网络在特定数据子空间上的性能，尤其是在不改变现有训练框架或增加数据的情况下，研究如何通过约束类别标签空间来实现网络的专业化。

Method: 方法包括修改传统的微调方法、约束数据空间到语义连贯的子集，并在网络调优前进行专家提取阶段。

Result: 实验显示，该方法能有效提升通用网络的准确性，同时分析了特征空间在专业化过程中的演变。

Conclusion: 研究为未来开发更先进的动态可配置图像分析系统铺平了道路，并可在特定数据域排除场景中提升系统性能。

Abstract: This paper introduces a concept of neural network specialization via
task-specific domain constraining, aimed at enhancing network performance on
data subspace in which the network operates. The study presents experiments on
training specialists for image classification and object detection tasks. The
results demonstrate that specialization can enhance a generalist's accuracy
even without additional data or changing training regimes: solely by
constraining class label space in which the network performs. Theoretical and
experimental analyses indicate that effective specialization requires modifying
traditional fine-tuning methods and constraining data space to semantically
coherent subsets. The specialist extraction phase before tuning the network is
proposed for maximal performance gains. We also provide analysis of the
evolution of the feature space during specialization. This study paves way to
future research for developing more advanced dynamically configurable image
analysis systems, where computations depend on the specific input.
Additionally, the proposed methods can help improve system performance in
scenarios where certain data domains should be excluded from consideration of
the generalist network.

</details>


### [229] [Lightweight Adapter Learning for More Generalized Remote Sensing Change Detection](https://arxiv.org/abs/2504.19598)
*Dou Quan,Rufan Zhou,Shuang Wang,Ning Huyan,Dong Zhao,Yunan Li,Licheng Jiao*

Main category: cs.CV

TL;DR: 该论文提出了一种通用的变化检测网络CANet，包含数据集共享和特定模块，通过轻量级适配器和小计算成本解决不同数据集间的分布和标注差异，提升了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习方法在遥感图像变化检测中因数据集差异导致的泛化能力差问题，提出更通用的检测方案。

Method: 设计了CANet网络，包含数据集共享学习模块和轻量级适配器，引入变化区域掩码（ICM）和独特批归一化层处理数据分布与标注差异。

Result: 在多个公共数据集上验证了CANet的有效性，泛化能力更强，训练成本更低（仅更新4.1%-7.7%参数），并且在有限训练数据下表现优于其他方法。

Conclusion: CANet通过适配器和共享模块的设计，显著提升了变化检测的通用性和性能，适用于多种数据集，且兼容现有深度模型。

Abstract: Deep learning methods have shown promising performances in remote sensing
image change detection (CD). However, existing methods usually train a
dataset-specific deep network for each dataset. Due to the significant
differences in the data distribution and labeling between various datasets, the
trained dataset-specific deep network has poor generalization performances on
other datasets. To solve this problem, this paper proposes a change adapter
network (CANet) for a more universal and generalized CD. CANet contains
dataset-shared and dataset-specific learning modules. The former explores the
discriminative features of images, and the latter designs a lightweight adapter
model, to deal with the characteristics of different datasets in data
distribution and labeling. The lightweight adapter can quickly generalize the
deep network for new CD tasks with a small computation cost. Specifically, this
paper proposes an interesting change region mask (ICM) in the adapter, which
can adaptively focus on interested change objects and decrease the influence of
labeling differences in various datasets. Moreover, CANet adopts a unique batch
normalization layer for each dataset to deal with data distribution
differences. Compared with existing deep learning methods, CANet can achieve
satisfactory CD performances on various datasets simultaneously. Experimental
results on several public datasets have verified the effectiveness and
advantages of the proposed CANet on CD. CANet has a stronger generalization
ability, smaller training costs (merely updating 4.1%-7.7% parameters), and
better performances under limited training datasets than other deep learning
methods, which also can be flexibly inserted with existing deep models.

</details>


### [230] [Image Generation Method Based on Heat Diffusion Models](https://arxiv.org/abs/2504.19600)
*Pengfei Zhang,Shouqing Jia*

Main category: cs.CV

TL;DR: HDM是一种基于热扩散方程改进的扩散模型，通过考虑相邻像素关系生成更高质量的图像。


<details>
  <summary>Details</summary>
Motivation: 传统DDPM将图像视为整体处理，忽略了相邻像素的相关性，导致细节丢失。

Method: 将二维热方程的离散形式集成到DDPM的扩散和生成公式中，实现像素级操作。

Result: 实验表明，HDM在图像质量上优于DDPM、CDM、LDM和VQGAN等模型。

Conclusion: 引入热方程能有效提升扩散模型的图像生成质量，保留更多细节。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) achieve high-quality image
generation without adversarial training, but they process images as a whole.
Since adjacent pixels are highly likely to belong to the same object, we
propose the Heat Diffusion Model (HDM) to further preserve image details and
generate more realistic images. HDM is a model that incorporates pixel-level
operations while maintaining the same training process as DDPM. In HDM, the
discrete form of the two-dimensional heat equation is integrated into the
diffusion and generation formulas of DDPM, enabling the model to compute
relationships between neighboring pixels during image processing. Our
experiments demonstrate that HDM can generate higher-quality samples compared
to models such as DDPM, Consistency Diffusion Models (CDM), Latent Diffusion
Models (LDM), and Vector Quantized Generative Adversarial Networks (VQGAN).

</details>


### [231] [Hybrid Approach Combining Ultrasound and Blood Test Analysis with a Voting Classifier for Accurate Liver Fibrosis and Cirrhosis Assessment](https://arxiv.org/abs/2504.19755)
*Kapil Kashyap,Sean Fargose,Chrisil Dabre,Fatema Dolaria,Nilesh Patil,Aniket Kore*

Main category: cs.CV

TL;DR: 提出一种结合机器学习和临床数据的混合模型，用于提高肝纤维化和肝硬化的检测准确率，替代传统的侵入性肝活检。


<details>
  <summary>Details</summary>
Motivation: 传统的肝活检诊断方法侵入性强，不便于常规筛查，需要一种更便捷且准确的替代方案。

Method: 结合血液检测数据和深度学习模型（DenseNet-201）分析超声图像，构建混合模型。

Result: 混合模型的准确率达到92.5%。

Conclusion: 该混合模型能有效提高诊断准确性，支持肝病的早期干预。

Abstract: Liver cirrhosis is an insidious condition involving the substitution of
normal liver tissue with fibrous scar tissue and causing major health
complications. The conventional method of diagnosis using liver biopsy is
invasive and, therefore, inconvenient for use in regular screening. In this
paper,we present a hybrid model that combines machine learning techniques with
clinical data and ultrasoundscans to improve liver fibrosis and cirrhosis
detection accuracy is presented. The model integrates fixed blood test
probabilities with deep learning model predictions (DenseNet-201) for
ultrasonic images. The combined hybrid model achieved an accuracy of 92.5%. The
findings establish the viability of the combined model in enhancing diagnosis
accuracy and supporting early intervention in liver disease care.

</details>


### [232] [Foundation Model-Driven Framework for Human-Object Interaction Prediction with Segmentation Mask Integration](https://arxiv.org/abs/2504.19847)
*Juhan Park,Kyungjae Lee,Hyung Jin Chang,Jungchan Cho*

Main category: cs.CV

TL;DR: Seg2HOI: 一个将分割视觉基础模型与人类-物体交互任务结合的新框架，可生成四元组和交互式分割，性能接近最先进方法。


<details>
  <summary>Details</summary>
Motivation: 传统人类-物体交互（HOI）方法依赖检测，而Seg2HOI旨在利用分割能力提升HOI任务，增加准确性和灵活性。

Method: 结合视觉基础模型（如支持提示和交互机制的分割模型），引入解码器将分割属性应用于HOI，并扩展到生成四元组（含分割掩码）。无需额外训练即可实现高效功能。

Result: 在公开数据集上表现接近SOTA，零样本场景下也能高效运作，且能通过新文本或视觉提示生成未训练过的HOI四元组和交互分割。

Conclusion: Seg2HOI展示了分割模型在HOI任务中的潜力，为更广泛的应用提供了灵活性。

Abstract: In this work, we introduce Segmentation to Human-Object Interaction
(\textit{\textbf{Seg2HOI}}) approach, a novel framework that integrates
segmentation-based vision foundation models with the human-object interaction
task, distinguished from traditional detection-based Human-Object Interaction
(HOI) methods. Our approach enhances HOI detection by not only predicting the
standard triplets but also introducing quadruplets, which extend HOI triplets
by including segmentation masks for human-object pairs. More specifically,
Seg2HOI inherits the properties of the vision foundation model (e.g.,
promptable and interactive mechanisms) and incorporates a decoder that applies
these attributes to HOI task. Despite training only for HOI, without additional
training mechanisms for these properties, the framework demonstrates that such
features still operate efficiently. Extensive experiments on two public
benchmark datasets demonstrate that Seg2HOI achieves performance comparable to
state-of-the-art methods, even in zero-shot scenarios. Lastly, we propose that
Seg2HOI can generate HOI quadruplets and interactive HOI segmentation from
novel text and visual prompts that were not used during training, making it
versatile for a wide range of applications by leveraging this flexibility.

</details>


### [233] [Towards Ball Spin and Trajectory Analysis in Table Tennis Broadcast Videos via Physically Grounded Synthetic-to-Real Transfer](https://arxiv.org/abs/2504.19863)
*Daniel Kienzle,Robin Schön,Rainer Lienhart,Shin'Ichi Satoh*

Main category: cs.CV

TL;DR: 该论文提出了一种通过乒乓球的2D轨迹视频推断其初始旋转和3D轨迹的新方法，仅使用合成数据训练神经网络，无需真实数据，实现了92.0%的旋转分类准确率和0.19%的图像对角线2D重投影误差。


<details>
  <summary>Details</summary>
Motivation: 在标准广播视频中，乒乓球的旋转无法直接观察，但通过分析球的轨迹可以间接推断旋转，这对于分析运动员的技术至关重要。

Method: 提出了一种新颖方法，从视频中的2D轨迹推断初始旋转和3D轨迹。仅使用合成数据训练神经网络，并通过物理正确的合成数据和有针对性的增强实现泛化。

Result: 方法在简单单目广播视频中实现了92.0%的旋转分类准确率和0.19%的图像对角线2D重投影误差，无需任何真实数据训练。

Conclusion: 该方法首次在单目广播视频中实现了旋转和轨迹预测，仅通过简单技术就能泛化到真实数据，展示了合成数据训练的潜力。

Abstract: Analyzing a player's technique in table tennis requires knowledge of the
ball's 3D trajectory and spin. While, the spin is not directly observable in
standard broadcasting videos, we show that it can be inferred from the ball's
trajectory in the video. We present a novel method to infer the initial spin
and 3D trajectory from the corresponding 2D trajectory in a video. Without
ground truth labels for broadcast videos, we train a neural network solely on
synthetic data. Due to the choice of our input data representation, physically
correct synthetic training data, and using targeted augmentations, the network
naturally generalizes to real data. Notably, these simple techniques are
sufficient to achieve generalization. No real data at all is required for
training. To the best of our knowledge, we are the first to present a method
for spin and trajectory prediction in simple monocular broadcast videos,
achieving an accuracy of 92.0% in spin classification and a 2D reprojection
error of 0.19% of the image diagonal.

</details>


### [234] [Breast Cancer Detection from Multi-View Screening Mammograms with Visual Prompt Tuning](https://arxiv.org/abs/2504.19900)
*Han Chen,Anne L. Martel*

Main category: cs.CV

TL;DR: 论文提出了一种多视角视觉提示调优网络（MVPT-NET），用于高效整合多视角乳腺X光片数据，以提升乳腺癌检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的单视角乳腺X光片检测方法存在局限性，而多视角数据能够提供更全面的信息，但处理高分辨率数据时存在挑战。

Method: 先预训练一个单视角分类模型，然后将多视角特征学习融入任务特定的提示调优过程，仅调整少量参数（7%）。

Result: 在大型多机构数据集上，MVPT-NET的AUROC达到0.852，优于传统方法。

Conclusion: MVPT-NET为高分辨率乳腺X光片分析提供了一种高效、可扩展的多视角整合方案。

Abstract: Accurate detection of breast cancer from high-resolution mammograms is
crucial for early diagnosis and effective treatment planning. Previous studies
have shown the potential of using single-view mammograms for breast cancer
detection. However, incorporating multi-view data can provide more
comprehensive insights. Multi-view classification, especially in medical
imaging, presents unique challenges, particularly when dealing with
large-scale, high-resolution data. In this work, we propose a novel Multi-view
Visual Prompt Tuning Network (MVPT-NET) for analyzing multiple screening
mammograms. We first pretrain a robust single-view classification model on
high-resolution mammograms and then innovatively adapt multi-view feature
learning into a task-specific prompt tuning process. This technique selectively
tunes a minimal set of trainable parameters (7\%) while retaining the
robustness of the pre-trained single-view model, enabling efficient integration
of multi-view data without the need for aggressive downsampling. Our approach
offers an efficient alternative to traditional feature fusion methods,
providing a more robust, scalable, and efficient solution for high-resolution
mammogram analysis. Experimental results on a large multi-institution dataset
demonstrate that our method outperforms conventional approaches while
maintaining detection efficiency, achieving an AUROC of 0.852 for
distinguishing between Benign, DCIS, and Invasive classes. This work highlights
the potential of MVPT-NET for medical imaging tasks and provides a scalable
solution for integrating multi-view data in breast cancer detection.

</details>


### [235] [Explaining Vision GNNs: A Semantic and Visual Analysis of Graph-based Image Classification](https://arxiv.org/abs/2504.19682)
*Nikolaos Chaidos,Angeliki Dimitriou,Nikolaos Spanos,Athanasios Voulodimos,Giorgos Stamou*

Main category: cs.CV

TL;DR: 该论文探讨了基于图神经网络（GNN）的图像分类器的可解释性，分析了不同层中图的语义一致性及其对对象结构和关系的保留能力，并通过量化评估和可视化技术揭示了模型的决策过程。


<details>
  <summary>Details</summary>
Motivation: 尽管GNN在视觉任务中表现出高效性，但其可解释性研究仍不足，尤其是图的语义一致性和模型决策过程是否能与人类感知一致尚不明确。

Method: 通过量化分析不同层间图的连接是否反映语义相似性和空间一致性，并结合标准与对抗设置下的解释对比，以及热力图可视化技术。

Result: 研究发现模型的决策过程可以有效解释，但深层网络的推理与人类感知并不完全一致。

Conclusion: 研究强调了GNN图像分类器的可解释性潜力，同时指出其深层推理与人类理解的差异，为进一步改进提供了方向。

Abstract: Graph Neural Networks (GNNs) have emerged as an efficient alternative to
convolutional approaches for vision tasks such as image classification,
leveraging patch-based representations instead of raw pixels. These methods
construct graphs where image patches serve as nodes, and edges are established
based on patch similarity or classification relevance. Despite their
efficiency, the explainability of GNN-based vision models remains
underexplored, even though graphs are naturally interpretable. In this work, we
analyze the semantic consistency of the graphs formed at different layers of
GNN-based image classifiers, focusing on how well they preserve object
structures and meaningful relationships. A comprehensive analysis is presented
by quantifying the extent to which inter-layer graph connections reflect
semantic similarity and spatial coherence. Explanations from standard and
adversarial settings are also compared to assess whether they reflect the
classifiers' robustness. Additionally, we visualize the flow of information
across layers through heatmap-based visualization techniques, thereby
highlighting the models' explainability. Our findings demonstrate that the
decision-making processes of these models can be effectively explained, while
also revealing that their reasoning does not necessarily align with human
perception, especially in deeper layers.

</details>


### [236] [Enhancing Surgical Documentation through Multimodal Visual-Temporal Transformers and Generative AI](https://arxiv.org/abs/2504.19918)
*Hugo Georgenthum,Cristian Cosentino,Fabrizio Marozzo,Pietro Liò*

Main category: cs.CV

TL;DR: 论文提出了一种多模态框架，结合计算机视觉和大型语言模型，自动生成手术视频摘要，在工具检测和时间上下文摘要方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 提升手术视频自动摘要能力，以改善手术记录、支持培训并促进术后分析，推动AI在医疗领域的实际应用。

Method: 分三阶段：1) 分割视频并提取视觉特征；2) 通过语言模型生成帧级描述并整合时间特征；3) 用专用LLM聚合生成完整手术报告。

Result: 在CholecT50数据集上，工具检测精度达96%，时间上下文摘要BERT分数为0.74。

Conclusion: 该方法为AI辅助手术报告提供了智能化、可靠的解决方案，推动了临床文档技术的进步。

Abstract: The automatic summarization of surgical videos is essential for enhancing
procedural documentation, supporting surgical training, and facilitating
post-operative analysis. This paper presents a novel method at the intersection
of artificial intelligence and medicine, aiming to develop machine learning
models with direct real-world applications in surgical contexts. We propose a
multi-modal framework that leverages recent advancements in computer vision and
large language models to generate comprehensive video summaries. % The approach
is structured in three key stages. First, surgical videos are divided into
clips, and visual features are extracted at the frame level using visual
transformers. This step focuses on detecting tools, tissues, organs, and
surgical actions. Second, the extracted features are transformed into
frame-level captions via large language models. These are then combined with
temporal features, captured using a ViViT-based encoder, to produce clip-level
summaries that reflect the broader context of each video segment. Finally, the
clip-level descriptions are aggregated into a full surgical report using a
dedicated LLM tailored for the summarization task. % We evaluate our method on
the CholecT50 dataset, using instrument and action annotations from 50
laparoscopic videos. The results show strong performance, achieving 96\%
precision in tool detection and a BERT score of 0.74 for temporal context
summarization. This work contributes to the advancement of AI-assisted tools
for surgical reporting, offering a step toward more intelligent and reliable
clinical documentation.

</details>


### [237] [ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather Classification in Traffic Camera Imagery](https://arxiv.org/abs/2504.19684)
*Anush Lakshman Sivaraman,Kojo Adu-Gyamfi,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

TL;DR: 论文提出了一种结合生成域适应和高效对比学习的框架，显著提升了低质量交通摄像头图像在恶劣夜间条件下的天气分类准确性。


<details>
  <summary>Details</summary>
Motivation: 夜间条件下低质量的交通摄像头图像导致天气分类性能下降，研究旨在通过域适应和对比学习改进分类效果。

Method: 采用CycleGAN进行域转换提升图像质量，结合SigLIP-2对比损失和Vision-SigLIP-2、Text-SigLIP-2进行对比训练。

Result: 最佳夜间分类准确率达85.90%，整体准确率为97.01%，夜间性能显著提升。

Conclusion: 结合域适应和高效对比学习可构建资源高效的天气分类系统，适用于智能交通基础设施。

Abstract: Accurate weather classification from low-quality traffic camera imagery
remains a challenging task, particularly under adverse nighttime conditions. In
this study, we propose a scalable framework that combines generative domain
adaptation with efficient contrastive learning to enhance classification
performance. Using CycleGAN-based domain translation, we improve the quality of
nighttime images, enabling better feature extraction by downstream models.
While the baseline EVA-02 model employing CLIP-based contrastive loss achieves
an overall accuracy of 96.55\%, it exhibits a significant performance gap
between daytime (97.21\%) and nighttime conditions (63.40\%). Replacing CLIP
with the lightweight SigLIP-2 (Sigmoid contrastive loss) achieves a competitive
overall accuracy of 94.00\%, with substantial improvements in nighttime
performance (85.90\% accuracy). The combination of Vision-SigLIP-2,
Text-SigLIP-2, CycleGAN, and contrastive training achieves the best nighttime
accuracy (85.90\%) among all models tested, while EVA-02 with CycleGAN
maintains the highest overall accuracy (97.01\%) and per-class accuracies.
These findings demonstrate the potential of combining domain adaptation and
efficient contrastive learning to build practical, resource-efficient weather
classification systems for intelligent transportation infrastructure.

</details>


### [238] [The ATLAS of Traffic Lights: A Reliable Perception Framework for Autonomous Driving](https://arxiv.org/abs/2504.19722)
*Rupert Polley,Nikolai Polley,Dominik Heid,Marc Heinrich,Sven Ochs,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 这篇论文提出了一个模块化的交通灯感知框架，结合了先进的检测模型和实时关联决策算法，并引入了ATLAS数据集以提升性能验证。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中交通灯感知的准确性和实时性问题，同时弥补现有公共数据集的不足。

Method: 提出模块化感知框架，集成检测模型与实时关联决策算法，并使用自研ATLAS数据集进行训练。

Result: 在ATLAS数据集上验证了性能提升，并在真实驾驶场景中展示了框架的可靠性。

Conclusion: 该框架显著提升了交通灯感知的准确性和鲁棒性，适合实时自动驾驶应用。

Abstract: Traffic light perception is an essential component of the camera-based
perception system for autonomous vehicles, enabling accurate detection and
interpretation of traffic lights to ensure safe navigation through complex
urban environments. In this work, we propose a modularized perception framework
that integrates state-of-the-art detection models with a novel real-time
association and decision framework, enabling seamless deployment into an
autonomous driving stack. To address the limitations of existing public
datasets, we introduce the ATLAS dataset, which provides comprehensive
annotations of traffic light states and pictograms across diverse environmental
conditions and camera setups. This dataset is publicly available at
https://url.fzi.de/ATLAS. We train and evaluate several state-of-the-art
traffic light detection architectures on ATLAS, demonstrating significant
performance improvements in both accuracy and robustness. Finally, we evaluate
the framework in real-world scenarios by deploying it in an autonomous vehicle
to make decisions at traffic light-controlled intersections, highlighting its
reliability and effectiveness for real-time operation.

</details>


### [239] [Monitoring digestate application on agricultural crops using Sentinel-2 Satellite imagery](https://arxiv.org/abs/2504.19996)
*Andreas Kalogeras,Dimitrios Bormpoudakis,Iason Tsardanidis,Dimitra A. Loka,Charalampos Kontoes*

Main category: cs.CV

TL;DR: 使用Sentinel-2卫星影像和机器学习模型检测农业中外源有机物质的应用效果，结果表明该方法具有高效监测潜力。


<details>
  <summary>Details</summary>
Motivation: 外源有机物质在农业中的广泛应用需要对其对土壤和作物健康的影响进行监测。本研究旨在评估如何利用遥感技术和机器学习高效监测这种应用。

Method: 通过Sentinel-2卫星影像时间序列分析特定指数（EOMI、NDVI、EVI），并结合四种机器学习模型（随机森林、k-NN、梯度提升和前馈神经网络）检测消化物的存在。

Result: 机器学习模型在检测消化物存在时达到了最高0.85的F1分数，显示了遥感与机器学习结合的潜力。

Conclusion: 结合遥感和机器学习的方法为监测外源有机物质的应用提供了可扩展且经济高效的解决方案，支持精准农业和可持续发展。

Abstract: The widespread use of Exogenous Organic Matter in agriculture necessitates
monitoring to assess its effects on soil and crop health. This study evaluates
optical Sentinel-2 satellite imagery for detecting digestate application, a
practice that enhances soil fertility but poses environmental risks like
microplastic contamination and nitrogen losses. In the first instance,
Sentinel-2 satellite image time series (SITS) analysis of specific indices
(EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after
application on the soils of four different crop types in Thessaly, Greece.
Furthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient
Boosting and a Feed-Forward Neural Network), were used to investigate digestate
presence detection, achieving F1-scores up to 0.85. The findings highlight the
potential of combining remote sensing and ML for scalable and cost-effective
monitoring of EOM applications, supporting precision agriculture and
sustainability.

</details>


### [240] [LIRM: Large Inverse Rendering Model for Progressive Reconstruction of Shape, Materials and View-dependent Radiance Fields](https://arxiv.org/abs/2504.20026)
*Zhengqin Li,Dilin Wang,Ka Chen,Zhaoyang Lv,Thu Nguyen-Phuoc,Milim Lee,Jia-Bin Huang,Lei Xiao,Cheng Zhang,Yufeng Zhu,Carl S. Marshall,Yufeng Ren,Richard Newcombe,Zhao Dong*

Main category: cs.CV

TL;DR: LIRM是一种高效的多视角3D重建框架，结合了先进的形状、材质和辐射场重建技术，在不到一秒内完成高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有LRMs在稀疏视角重建中表现优秀，但在未见过部分的准确重建、光滑外观恢复及可重光照3D内容生成方面存在不足。

Method: 关键贡献包括增量输入视角的更新模型、六平面神经SDF表示和神经方向嵌入机制，结合分阶段训练策略。

Result: 在几何和重光照精度上与优化方法相当，同时显著减少推理时间。

Conclusion: LIRM为多视角3D重建提供了一种高效且实用的解决方案，具备实际应用潜力。

Abstract: We present Large Inverse Rendering Model (LIRM), a transformer architecture
that jointly reconstructs high-quality shape, materials, and radiance fields
with view-dependent effects in less than a second. Our model builds upon the
recent Large Reconstruction Models (LRMs) that achieve state-of-the-art
sparse-view reconstruction quality. However, existing LRMs struggle to
reconstruct unseen parts accurately and cannot recover glossy appearance or
generate relightable 3D contents that can be consumed by standard Graphics
engines. To address these limitations, we make three key technical
contributions to build a more practical multi-view 3D reconstruction framework.
First, we introduce an update model that allows us to progressively add more
input views to improve our reconstruction. Second, we propose a hexa-plane
neural SDF representation to better recover detailed textures, geometry and
material parameters. Third, we develop a novel neural directional-embedding
mechanism to handle view-dependent effects. Trained on a large-scale shape and
material dataset with a tailored coarse-to-fine training scheme, our model
achieves compelling results. It compares favorably to optimization-based
dense-view inverse rendering methods in terms of geometry and relighting
accuracy, while requiring only a fraction of the inference time.

</details>


### [241] [Mapping of Weed Management Methods in Orchards using Sentinel-2 and PlanetScope Data](https://arxiv.org/abs/2504.19991)
*Ioannis Kontogiorgakis,Iason Tsardanidis,Dimitrios Bormpoudakis,Ilias Tsoumas,Dimitra A. Loka,Christos Noulas,Alexandros Tsitouras,Charalampos Kontoes*

Main category: cs.CV

TL;DR: 利用卫星图像时间序列和机器学习方法，高效绘制果园杂草管理地图，提高农业生产力。


<details>
  <summary>Details</summary>
Motivation: 杂草管理对农业生产至关重要，但现有地面调查方法成本高、耗时长，因此需要更高效的监测手段。

Method: 结合Sentinel-2和PlanetScope卫星数据，开发机器学习模型，识别四种杂草管理方法（割草、耕作、化学喷洒和无措施）。

Result: 验证了机器学习与遥感技术在果园杂草管理制图中的高效性和准确性。

Conclusion: 机器学习驱动的遥感技术为杂草管理监测提供了更高效、精准的解决方案。

Abstract: Effective weed management is crucial for improving agricultural productivity,
as weeds compete with crops for vital resources like nutrients and water.
Accurate maps of weed management methods are essential for policymakers to
assess farmer practices, evaluate impacts on vegetation health, biodiversity,
and climate, as well as ensure compliance with policies and subsidies. However,
monitoring weed management methods is challenging as commonly rely on on-ground
field surveys, which are often costly, time-consuming and subject to delays. In
order to tackle this problem, we leverage Earth Observation (EO) data and
Machine Learning (ML). Specifically, we developed an ML approach for mapping
four distinct weed management methods (Mowing, Tillage, Chemical-spraying, and
No practice) in orchards using satellite image time series (SITS) data from two
different sources: Sentinel-2 (S2) and PlanetScope (PS). The findings
demonstrate the potential of ML-driven remote sensing to enhance the efficiency
and accuracy of weed management mapping in orchards.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [242] [Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective](https://arxiv.org/abs/2504.19458)
*Taoyu Su,Jiawei Sheng,Duohe Ma,Xiaodong Li,Juwei Yue,Mengxiao Song,Yingkai Tang,Tingwen Liu*

Main category: cs.MM

TL;DR: 论文提出了一种名为CDMEA的反事实去偏框架，通过因果视角解决多模态实体对齐中视觉模态可能导致的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态实体对齐方法过于依赖视觉特征，可能导致模型偏向视觉匹配任务，尤其是低相似度图像下表现不佳。

Method: CDMEA通过估计两种模态的总效应（TE）并排除视觉模态的自然直接效应（NDE），确保模型基于总间接效应（TIE）进行预测，减少视觉偏差。

Result: 在9个基准数据集上，CDMEA优于14种最新方法，尤其适用于低相似度、高噪声和低资源数据场景。

Conclusion: CDMEA有效抑制了视觉模态偏差，提升了多模态实体对齐的性能和鲁棒性。

Abstract: Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from
different Multi-Modal Knowledge Graphs (MMKGs), a critical information
retrieval task. Existing studies have explored various fusion paradigms and
consistency constraints to improve the alignment of equivalent entities, while
overlooking that the visual modality may not always contribute positively.
Empirically, entities with low-similarity images usually generate
unsatisfactory performance, highlighting the limitation of overly relying on
visual features. We believe the model can be biased toward the visual modality,
leading to a shortcut image-matching task. To address this, we propose a
counterfactual debiasing framework for MMEA, termed CDMEA, which investigates
visual modality bias from a causal perspective. Our approach aims to leverage
both visual and graph modalities to enhance MMEA while suppressing the direct
causal effect of the visual modality on model predictions. By estimating the
Total Effect (TE) of both modalities and excluding the Natural Direct Effect
(NDE) of the visual modality, we ensure that the model predicts based on the
Total Indirect Effect (TIE), effectively utilizing both modalities and reducing
visual modality bias. Extensive experiments on 9 benchmark datasets show that
CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,
high-noise, and low-resource data scenarios.

</details>


### [243] [WILD: a new in-the-Wild Image Linkage Dataset for synthetic image attribution](https://arxiv.org/abs/2504.19595)
*Pietro Bongini,Sara Mandelli,Andrea Montibeller,Mirko Casu,Orazio Pontorno,Claudio Ragaglia,Luca Zanchetta,Mattia Aquilina,Taiba Majid Wani,Luca Guarnera,Benedetta Tondi,Paolo Bestagini,Irene Amerini,Francesco Denatale,Sebastiano Battiato,Mauro Barni*

Main category: cs.MM

TL;DR: WILD数据集为合成图像来源识别模型提供了训练和基准测试工具，包含封闭集和开放集的图像，支持多种任务测试，并评估了七种基线方法。


<details>
  <summary>Details</summary>
Motivation: 合成图像来源识别是一个开放性问题，现有生成技术复杂且多样，高质量数据集稀缺，WILD数据集旨在解决这一挑战。

Method: WILD数据集包含封闭集（10个生成器，10,000张图像）和开放集（10个生成器，10,000张图像），部分图像经过后处理。

Result: 该数据集支持多种任务测试，模型在此数据集上训练有望提升性能。七种基线方法在封闭和开放集上进行了评估，包括后处理鲁棒性测试。

Conclusion: WILD为合成图像来源识别提供了强大的工具，有助于推动该领域的研究和模型开发。

Abstract: Synthetic image source attribution is an open challenge, with an increasing
number of image generators being released yearly. The complexity and the sheer
number of available generative techniques, as well as the scarcity of
high-quality open source datasets of diverse nature for this task, make
training and benchmarking synthetic image source attribution models very
challenging. WILD is a new in-the-Wild Image Linkage Dataset designed to
provide a powerful training and benchmarking tool for synthetic image
attribution models. The dataset is built out of a closed set of 10 popular
commercial generators, which constitutes the training base of attribution
models, and an open set of 10 additional generators, simulating a real-world
in-the-wild scenario. Each generator is represented by 1,000 images, for a
total of 10,000 images in the closed set and 10,000 images in the open set.
Half of the images are post-processed with a wide range of operators. WILD
allows benchmarking attribution models in a wide range of tasks, including
closed and open set identification and verification, and robust attribution
with respect to post-processing and adversarial attacks. Models trained on WILD
are expected to benefit from the challenging scenario represented by the
dataset itself. Moreover, an assessment of seven baseline methodologies on
closed and open set attribution is presented, including robustness tests with
respect to post-processing.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [244] [Generative Models for Fast Simulation of Cherenkov Detectors at the Electron-Ion Collider](https://arxiv.org/abs/2504.19042)
*James Giroux,Michael Martinez,Cristiano Fanelli*

Main category: physics.ins-det

TL;DR: 提出了一个针对DIRC探测器的快速仿真工具，使用生成模型加速粒子识别任务，代替传统的Geant4仿真框架。


<details>
  <summary>Details</summary>
Motivation: 传统仿真框架（如Geant4）在模拟Cherenkov探测器时光子传输计算量大，成为性能瓶颈。为解决这一问题，需开发高效且可扩展的替代方案。

Method: 提出一个独立的快速仿真工具，结合生成模型和GPU加速，为hpDIRC探测器生成高保真数据集。

Result: 该工具能够高效生成大规模仿真数据，支持开发新型深度学习驱动的粒子识别方法，并推动EIC范围内的PID策略。

Conclusion: 该快速仿真工具为物理学家和DL研究人员提供了灵活高效的数据生成方案，减少了对传统仿真框架的依赖。

Abstract: The integration of Deep Learning (DL) into experimental nuclear and particle
physics has driven significant progress in simulation and reconstruction
workflows. However, traditional simulation frameworks such as Geant4 remain
computationally intensive, especially for Cherenkov detectors, where simulating
optical photon transport through complex geometries and reflective surfaces
introduces a major bottleneck. To address this, we present an open, standalone
fast simulation tool for Detection of Internally Reflected Cherenkov Light
(DIRC) detectors, with a focus on the High-Performance DIRC (hpDIRC) at the
future Electron-Ion Collider (EIC). Our framework incorporates a suite of
generative models tailored to accelerate particle identification (PID) tasks by
offering a scalable, GPU-accelerated alternative to full Geant4-based
simulations. Designed with accessibility in mind, our simulation package
enables both DL researchers and physicists to efficiently generate
high-fidelity large-scale datasets on demand, without relying on complex
traditional simulation stacks. This flexibility supports the development and
benchmarking of novel DL-driven PID methods. Moreover, this fast simulation
pipeline represents a critical step toward enabling EIC-wide PID strategies
that depend on virtually unlimited simulated samples, spanning the full
acceptance of the hpDIRC.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [245] [Mapping the Italian Telegram Ecosystem](https://arxiv.org/abs/2504.19594)
*Lorenzo Alvisi,Serena Tardelli,Maurizio Tesconi*

Main category: cs.SI

TL;DR: 本研究通过分析意大利Telegram平台上的1.86亿条消息，揭示了该平台的政治生态、意识形态同质性、毒性言论的普遍性以及对特定群体的仇恨言论，填补了相关研究空白。


<details>
  <summary>Details</summary>
Motivation: Telegram已成为政治讨论和另类媒体的重要空间，但其缺乏内容审核导致错误信息、极端主义和毒性言论泛滥。以往研究多聚焦于单一现象，缺乏对Telegram生态的整体理解，本研究旨在填补这一空白。

Method: 利用网络分析、大语言模型和毒性检测工具，对2023年收集的13,151个聊天中的1.86亿条消息进行分析，研究意大利Telegram中的主题社区形成、意识形态对齐及有害言论。

Result: 研究发现强烈的主题和意识形态同质性；混合意识形态社区中存在极左和极右言论共存；毒性言论在高度毒性社区中被广泛正常化；仇恨言论主要针对黑人、犹太人和同性恋者；意大利人之间存在地域性敌对行为。

Conclusion: 本研究首次大规模绘制了意大利Telegram生态，为研究不同文化和语言背景下的在线毒性提供了新见解。

Abstract: Telegram has become a major space for political discourse and alternative
media. However, its lack of moderation allows misinformation, extremism, and
toxicity to spread. While prior research focused on these particular phenomena
or topics, these have mostly been examined separately, and a broader
understanding of the Telegram ecosystem is still missing. In this work, we fill
this gap by conducting a large-scale analysis of the Italian Telegram sphere,
leveraging a dataset of 186 million messages from 13,151 chats collected in
2023. Using network analysis, Large Language Models, and toxicity detection
tools, we examine how different thematic communities form, align ideologically,
and engage in harmful discourse within the Italian cultural context. Results
show strong thematic and ideological homophily. We also identify mixed
ideological communities where far-left and far-right rhetoric coexist on
particular geopolitical issues. Beyond political analysis, we find that
toxicity, rather than being isolated in a few extreme chats, appears widely
normalized within highly toxic communities. Moreover, we find that Italian
discourse primarily targets Black people, Jews, and gay individuals
independently of the topic. Finally, we uncover common trend of intra-national
hostility, where Italians often attack other Italians, reflecting regional and
intra-regional cultural conflicts that can be traced back to old historical
divisions. This study provides the first large-scale mapping of the Italian
Telegram ecosystem, offering insights into ideological interactions, toxicity,
and identity-targets of hate and contributing to research on online toxicity
across different cultural and linguistic contexts on Telegram.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [246] [Improving Pretrained YAMNet for Enhanced Speech Command Detection via Transfer Learning](https://arxiv.org/abs/2504.19030)
*Sidahmed Lachenani,Hamza Kheddar,Mohamed Ouldzmirli*

Main category: cs.SD

TL;DR: 利用预训练的YAMNet模型和迁移学习，显著提升语音命令识别的准确性和效率，最终模型识别准确率达95.28%。


<details>
  <summary>Details</summary>
Motivation: 提升语音命令识别系统的准确性和效率，以改善智能应用中的用户交互体验。

Method: 采用YAMNet深度模型和迁移学习技术，对语音命令数据集进行精细化增强和特征提取。

Result: 模型在语音命令识别任务中达到95.28%的准确率。

Conclusion: 该研究证明了先进机器学习技术在语音命令识别中的有效性，为音频处理技术设立了新基准。

Abstract: This work addresses the need for enhanced accuracy and efficiency in speech
command recognition systems, a critical component for improving user
interaction in various smart applications. Leveraging the robust pretrained
YAMNet model and transfer learning, this study develops a method that
significantly improves speech command recognition. We adapt and train a YAMNet
deep learning model to effectively detect and interpret speech commands from
audio signals. Using the extensively annotated Speech Commands dataset
(speech_commands_v0.01), our approach demonstrates the practical application of
transfer learning to accurately recognize a predefined set of speech commands.
The dataset is meticulously augmented, and features are strategically extracted
to boost model performance. As a result, the final model achieved a recognition
accuracy of 95.28%, underscoring the impact of advanced machine learning
techniques on speech command recognition. This achievement marks substantial
progress in audio processing technologies and establishes a new benchmark for
future research in the field.

</details>


### [247] [Generative Adversarial Network based Voice Conversion: Techniques, Challenges, and Recent Advancements](https://arxiv.org/abs/2504.19197)
*Sandipan Dhar,Nanda Dulal Jana,Swagatam Das*

Main category: cs.SD

TL;DR: 这篇论文综述了语音转换技术（VC）的现状，重点分析了基于生成对抗网络（GAN）的方法，总结了关键挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: 语音转换技术在自动电影配音、语音转歌唱和病理语音康复等领域有广泛应用，但现有技术仍面临训练稳定性、语言一致性和感知自然性等挑战，需要通过系统化的综述指导未来研究。

Method: 论文通过分类现有方法、分析技术障碍和评估GAN在语音转换中的最新进展，提供了全面的综述。

Result: 研究揭示了GAN在语音转换中的潜力，但同时也指出了当前方法的局限性，如训练不稳定和自然性不足。

Conclusion: 这篇综述为研究人员提供了结构化的知识框架，帮助识别现有技术缺口并指导未来研究，推动语音转换技术的进步。

Abstract: Voice conversion (VC) stands as a crucial research area in speech synthesis,
enabling the transformation of a speaker's vocal characteristics to resemble
another while preserving the linguistic content. This technology has broad
applications, including automated movie dubbing, speech-to-singing conversion,
and assistive devices for pathological speech rehabilitation. With the
increasing demand for high-quality and natural-sounding synthetic voices,
researchers have developed a wide range of VC techniques. Among these,
generative adversarial network (GAN)-based approaches have drawn considerable
attention for their powerful feature-mapping capabilities and potential to
produce highly realistic speech. Despite notable advancements, challenges such
as ensuring training stability, maintaining linguistic consistency, and
achieving perceptual naturalness continue to hinder progress in GAN-based VC
systems. This systematic review presents a comprehensive analysis of the voice
conversion landscape, highlighting key techniques, key challenges, and the
transformative impact of GANs in the field. The survey categorizes existing
methods, examines technical obstacles, and critically evaluates recent
developments in GAN-based VC. By consolidating and synthesizing research
findings scattered across the literature, this review provides a structured
understanding of the strengths and limitations of different approaches. The
significance of this survey lies in its ability to guide future research by
identifying existing gaps, proposing potential directions, and offering
insights for building more robust and efficient VC systems. Overall, this work
serves as an essential resource for researchers, developers, and practitioners
aiming to advance the state-of-the-art (SOTA) in voice conversion technology.

</details>


### [248] [Speaker Retrieval in the Wild: Challenges, Effectiveness and Robustness](https://arxiv.org/abs/2504.18950)
*Erfan Loweimi,Mengjie Qian,Kate Knill,Mark Gales*

Main category: cs.SD

TL;DR: 该论文探讨了在非受控环境下开发高效且鲁棒的说话人检索系统的挑战与方法，利用BBC Rewind档案作为案例，展示了其框架的通用性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着音视频档案的激增，如何高效检索特定内容成为关键问题。现有档案的元数据往往不足，且手动标注大规模档案不切实际，因此需要开发自动化且鲁棒的说话人检索系统。

Method: 论文提出一个框架，涵盖说话人日志、嵌入提取和查询选择等技术，并在真实世界（BBC Rewind档案）和模拟扭曲环境下进行系统实验。

Result: 实验结果证明了所开发系统的有效性和鲁棒性，在无控制内容和声学条件下依然表现良好。

Conclusion: 该框架具有通用性和可扩展性，适用于BBC Rewind档案以外的多种应用场景，为大规模档案的说话人检索提供了可行方案。

Abstract: There is a growing abundance of publicly available or company-owned
audio/video archives, highlighting the increasing importance of efficient
access to desired content and information retrieval from these archives. This
paper investigates the challenges, solutions, effectiveness, and robustness of
speaker retrieval systems developed "in the wild" which involves addressing two
primary challenges: extraction of task-relevant labels from limited metadata
for system development and evaluation, as well as the unconstrained acoustic
conditions encountered in the archive, ranging from quiet studios to adverse
noisy environments. While we focus on the publicly-available BBC Rewind archive
(spanning 1948 to 1979), our framework addresses the broader issue of speaker
retrieval on extensive and possibly aged archives with no control over the
content and acoustic conditions. Typically, these archives offer a brief and
general file description, mostly inadequate for specific applications like
speaker retrieval, and manual annotation of such large-scale archives is
unfeasible. We explore various aspects of system development (e.g., speaker
diarisation, embedding extraction, query selection) and analyse the challenges,
possible solutions, and their functionality. To evaluate the performance, we
conduct systematic experiments in both clean setup and against various
distortions simulating real-world applications. Our findings demonstrate the
effectiveness and robustness of the developed speaker retrieval systems,
establishing the versatility and scalability of the proposed framework for a
wide range of applications beyond the BBC Rewind corpus.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [249] [Surgeons vs. Computer Vision: A comparative analysis on surgical phase recognition capabilities](https://arxiv.org/abs/2504.18954)
*Marco Mezzina,Pieter De Backer,Tom Vercauteren,Matthew Blaschko,Alexandre Mottrie,Tinne Tuytelaars*

Main category: eess.IV

TL;DR: 该研究探讨了自动化手术阶段识别（SPR）在机器人辅助部分肾切除术（RAPN）中的应用，发现视频片段和视觉标志物提高了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 为解决现有研究未探索时间背景对专家分类手术阶段能力的影响，并填补线性手术研究的空白。

Method: 通过定制网络平台，不同经验的泌尿科医生对RAPN的单帧和视频片段进行阶段标注，并训练带/不带时间背景的AI模型。

Result: 视频片段和视觉标志物提高了分类准确性，专家表现优于新手，AI模型与专家表现相当，加入时间背景后效果更佳。

Conclusion: SPR对专家和计算机视觉均具挑战性，时间信息的加入能提升表现，手术工具和器官是关键标志物，将推动自动化SPR发展。

Abstract: Purpose: Automated Surgical Phase Recognition (SPR) uses Artificial
Intelligence (AI) to segment the surgical workflow into its key events,
functioning as a building block for efficient video review, surgical education
as well as skill assessment. Previous research has focused on short and linear
surgical procedures and has not explored if temporal context influences
experts' ability to better classify surgical phases. This research addresses
these gaps, focusing on Robot-Assisted Partial Nephrectomy (RAPN) as a highly
non-linear procedure. Methods: Urologists of varying expertise were grouped and
tasked to indicate the surgical phase for RAPN on both single frames and video
snippets using a custom-made web platform. Participants reported their
confidence levels and the visual landmarks used in their decision-making. AI
architectures without and with temporal context as trained and benchmarked on
the Cholec80 dataset were subsequently trained on this RAPN dataset. Results:
Video snippets and presence of specific visual landmarks improved phase
classification accuracy across all groups. Surgeons displayed high confidence
in their classifications and outperformed novices, who struggled discriminating
phases. The performance of the AI models is comparable to the surgeons in the
survey, with improvements when temporal context was incorporated in both cases.
Conclusion: SPR is an inherently complex task for expert surgeons and computer
vision, where both perform equally well when given the same context.
Performance increases when temporal information is provided. Surgical tools and
organs form the key landmarks for human interpretation and are expected to
shape the future of automated SPR.

</details>


### [250] [Reservoir-enhanced Segment Anything Model for Subsurface Diagnosis](https://arxiv.org/abs/2504.18802)
*Xiren Zhou,Shikang Liu,Xinyu Yan,Yizhan Fan,Xiangyu Wang,Yu Kang,Jian Cheng,Huanhuan Chen*

Main category: eess.IV

TL;DR: 本文提出了一种名为Res-SAM的创新框架，用于解决地下异常（如裂缝和空洞）检测的挑战。该框架结合了视觉可辨识性和电磁波变化特性，能够在有限的标记数据下实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 城市道路和基础设施面临地下异常的威胁，而现有的地面穿透雷达（GPR）检测方法因标记数据不足和地下条件多变而效果有限。

Method: 提出Res-SAM框架，首先通过少量提示识别候选异常区域，然后分析电磁波的局部变化信息进行细化，实现精确检测。

Result: 实际实验表明，Res-SAM的检测准确率超过85%，优于现有技术，且只需少量非目标数据和简单人工交互。

Conclusion: Res-SAM为城市地下异常检测提供了高效、可扩展的解决方案，显著减少了人工和计算成本。

Abstract: Urban roads and infrastructure, vital to city operations, face growing
threats from subsurface anomalies like cracks and cavities. Ground Penetrating
Radar (GPR) effectively visualizes underground conditions employing
electromagnetic (EM) waves; however, accurate anomaly detection via GPR remains
challenging due to limited labeled data, varying subsurface conditions, and
indistinct target boundaries. Although visually image-like, GPR data
fundamentally represent EM waves, with variations within and between waves
critical for identifying anomalies. Addressing these, we propose the
Reservoir-enhanced Segment Anything Model (Res-SAM), an innovative framework
exploiting both visual discernibility and wave-changing properties of GPR data.
Res-SAM initially identifies apparent candidate anomaly regions given minimal
prompts, and further refines them by analyzing anomaly-induced changing
information within and between EM waves in local GPR data, enabling precise and
complete anomaly region extraction and category determination. Real-world
experiments demonstrate that Res-SAM achieves high detection accuracy (>85%)
and outperforms state-of-the-art. Notably, Res-SAM requires only minimal
accessible non-target data, avoids intensive training, and incorporates simple
human interaction to enhance reliability. Our research provides a scalable,
resource-efficient solution for rapid subsurface anomaly detection across
diverse environments, improving urban safety monitoring while reducing manual
effort and computational cost.

</details>


### [251] [Low-Rank Adaptive Structural Priors for Generalizable Diabetic Retinopathy Grading](https://arxiv.org/abs/2504.19362)
*Yunxuan Wang,Ray Yin,Yumei Tan,Hao Chen,Haiying Xia*

Main category: eess.IV

TL;DR: 本文提出了一种名为LoASP的新方法，通过结合结构性先验增强现有域泛化方法，以解决糖尿病视网膜病变（DR）分级中的域偏移问题，显著提升了模型在训练数据分布之外的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变（DR）是导致视力丧失的主要原因之一，现有深度学习方法在域偏移下表现不佳，且大多数域泛化方法忽略病灶特定特征。本文旨在通过结构性先验改进这一问题。

Method: 提出了LoASP框架，通过学习与DR诊断复杂性相适应的结构性表征，增强现有域泛化模型，重点关注血管和病灶结构。

Result: 在八个不同数据集上的实验验证了LoASP在单源和多源域场景中的有效性，结构先验可视化也直观展示了其与血管和病灶结构的对应关系。

Conclusion: LoASP通过结合结构性先验显著提升了DR分级的泛化能力和可解释性，为临床应用提供了有力支持。

Abstract: Diabetic retinopathy (DR), a serious ocular complication of diabetes, is one
of the primary causes of vision loss among retinal vascular diseases. Deep
learning methods have been extensively applied in the grading of diabetic
retinopathy (DR). However, their performance declines significantly when
applied to data outside the training distribution due to domain shifts. Domain
generalization (DG) has emerged as a solution to this challenge. However, most
existing DG methods overlook lesion-specific features, resulting in
insufficient accuracy. In this paper, we propose a novel approach that enhances
existing DG methods by incorporating structural priors, inspired by the
observation that DR grading is heavily dependent on vessel and lesion
structures. We introduce Low-rank Adaptive Structural Priors (LoASP), a
plug-and-play framework designed for seamless integration with existing DG
models. LoASP improves generalization by learning adaptive structural
representations that are finely tuned to the complexities of DR diagnosis.
Extensive experiments on eight diverse datasets validate its effectiveness in
both single-source and multi-source domain scenarios. Furthermore,
visualizations reveal that the learned structural priors intuitively align with
the intricate architecture of the vessels and lesions, providing compelling
insights into their interpretability and diagnostic relevance.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [252] [Enhancing short-term traffic prediction by integrating trends and fluctuations with attention mechanism](https://arxiv.org/abs/2504.19967)
*Adway Das,Agnimitra Sengupta,S. Ilgin Guler*

Main category: cs.ET

TL;DR: 该论文提出了一种混合深度学习框架，结合长期趋势和短期波动信息，使用注意力机制提升流量预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在交通流量预测中因平滑细粒度波动而表现不佳，需解决长短期信息融合问题。

Method: 采用并行双分支结构分别处理长短期特征，并引入Bahdanau注意力机制聚焦关键时间步。

Result: 实验表明双分支特征互补，显著提升多时间尺度的拟合度，注意力机制尤其改善了短期预测。

Conclusion: 该框架增强了预测鲁棒性，但对长期趋势的完全整合仍需改进，可助力拥堵缓解与城市交通规划。

Abstract: Traffic flow prediction is a critical component of intelligent transportation
systems, yet accurately forecasting traffic remains challenging due to the
interaction between long-term trends and short-term fluctuations. Standard deep
learning models often struggle with these challenges because their
architectures inherently smooth over fine-grained fluctuations while focusing
on general trends. This limitation arises from low-pass filtering effects, gate
biases favoring stability, and memory update mechanisms that prioritize
long-term information retention. To address these shortcomings, this study
introduces a hybrid deep learning framework that integrates both long-term
trend and short-term fluctuation information using two input features processed
in parallel, designed to capture complementary aspects of traffic flow
dynamics. Further, our approach leverages attention mechanisms, specifically
Bahdanau attention, to selectively focus on critical time steps within traffic
data, enhancing the model's ability to predict congestion and other transient
phenomena. Experimental results demonstrate that features learned from both
branches are complementary, significantly improving the goodness-of-fit
statistics across multiple prediction horizons compared to a baseline model.
Notably, the attention mechanism enhances short-term forecast accuracy by
directly targeting immediate fluctuations, though challenges remain in fully
integrating long-term trends. This framework can contribute to more effective
congestion mitigation and urban mobility planning by advancing the robustness
and precision of traffic prediction models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [253] [M2R2: MulitModal Robotic Representation for Temporal Action Segmentation](https://arxiv.org/abs/2504.18662)
*Daniel Sliwowski,Dongheui Lee*

Main category: cs.RO

TL;DR: 论文提出了M2R2，一种专为时序动作分割（TAS）设计的多模态特征提取器，通过结合本体感觉和外感觉传感器信息，解决了现有方法在特征复用和对象可见性受限场景下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器人TAS方法依赖单一模态（如本体感觉），而计算机视觉方法多依赖外感觉传感器（如摄像头）。两种方法在特征复用和对象可见性受限场景下表现不佳，需要一种结合两者的解决方案。

Method: 提出M2R2多模态特征提取器，结合本体感觉和外感觉传感器信息，并设计了一种新颖的预训练策略，实现特征跨模型复用。

Result: 在REASSEMBLE数据集上取得SOTA性能，超越现有机器人动作分割模型46.6%。通过消融实验验证了多模态的贡献。

Conclusion: M2R2通过多模态融合和特征复用，显著提升了TAS任务的性能，尤其适用于复杂场景下的机器人任务。

Abstract: Temporal action segmentation (TAS) has long been a key area of research in
both robotics and computer vision. In robotics, algorithms have primarily
focused on leveraging proprioceptive information to determine skill boundaries,
with recent approaches in surgical robotics incorporating vision. In contrast,
computer vision typically relies on exteroceptive sensors, such as cameras.
Existing multimodal TAS models in robotics integrate feature fusion within the
model, making it difficult to reuse learned features across different models.
Meanwhile, pretrained vision-only feature extractors commonly used in computer
vision struggle in scenarios with limited object visibility. In this work, we
address these challenges by proposing M2R2, a multimodal feature extractor
tailored for TAS, which combines information from both proprioceptive and
exteroceptive sensors. We introduce a novel pretraining strategy that enables
the reuse of learned features across multiple TAS models. Our method achieves
state-of-the-art performance on the REASSEMBLE dataset, a challenging
multimodal robotic assembly dataset, outperforming existing robotic action
segmentation models by 46.6%. Additionally, we conduct an extensive ablation
study to evaluate the contribution of different modalities in robotic TAS
tasks.

</details>


### [254] [Imitation Learning for Autonomous Driving: Insights from Real-World Testing](https://arxiv.org/abs/2504.18847)
*Hidayet Ersin Dursun,Yusuf Güven,Tufan Kumbasar*

Main category: cs.RO

TL;DR: 本文研究了基于深度学习的自动驾驶系统在MIT Racecar上的实时性能，通过比较多种DNN模型（如PD系统、CNN、CNN-LSTM和CNN-NODE），发现CNN-LSTM和CNN-NODE在动态驾驶中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为了开发能够在复杂真实驾驶场景中实时、准确且稳定运行的自动驾驶系统。

Method: 采用模仿学习框架，设计并比较了多种DNN模型（PD系统、CNN、CNN-LSTM、CNN-NODE），通过增量式设计逐步优化模型能力。

Result: CNN-LSTM和CNN-NODE表现优异，尤其在动态驾驶中；而PD系统仅适合基础车道跟随，CNN则缺乏时间序列感知。

Conclusion: 迭代设计过程对开发稳健的自动驾驶DNN模型至关重要，CNN-LSTM和CNN-NODE在此类任务中表现出色。

Abstract: This work focuses on the design of a deep learning-based autonomous driving
system deployed and tested on the real-world MIT Racecar to assess its
effectiveness in driving scenarios. The Deep Neural Network (DNN) translates
raw image inputs into real-time steering commands in an end-to-end learning
fashion, following the imitation learning framework. The key design challenge
is to ensure that DNN predictions are accurate and fast enough, at a high
sampling frequency, and result in smooth vehicle operation under different
operating conditions. In this study, we design and compare various DNNs, to
identify the most effective approach for real-time autonomous driving. In
designing the DNNs, we adopted an incremental design approach that involved
enhancing the model capacity and dataset to address the challenges of
real-world driving scenarios. We designed a PD system, CNN, CNN-LSTM, and
CNN-NODE, and evaluated their performance on the real-world MIT Racecar. While
the PD system handled basic lane following, it struggled with sharp turns and
lighting variations. The CNN improved steering but lacked temporal awareness,
which the CNN-LSTM addressed as it resulted in smooth driving performance. The
CNN-NODE performed similarly to the CNN-LSTM in handling driving dynamics, yet
with slightly better driving performance. The findings of this research
highlight the importance of iterative design processes in developing robust
DNNs for autonomous driving applications. The experimental video is available
at https://www.youtube.com/watch?v=FNNYgU--iaY.

</details>


### [255] [Advanced Longitudinal Control and Collision Avoidance for High-Risk Edge Cases in Autonomous Driving](https://arxiv.org/abs/2504.18931)
*Dianwei Chen,Yaobang Gong,Xianfeng Yang*

Main category: cs.RO

TL;DR: 论文提出了一种新颖的纵向控制和碰撞避免算法，通过深度学习强化框架同时考虑前车和后车行为，有效防止高速密集交通中的连锁碰撞。


<details>
  <summary>Details</summary>
Motivation: 现有ADAS和ADS系统主要关注前方车辆，忽略了后方车辆行为，导致高速密集交通中连锁碰撞频发，特别是中间车辆突然刹车时。

Method: 结合自适应巡航与紧急制动，利用深度强化学习（DRL）框架，同时处理前车和后车数据，并通过数据预处理提升训练过程的鲁棒性。

Result: 在高风险场景（如密集交通中的紧急制动）中，算法成功避免连环碰撞；在典型高速路段三车减速测试中，成功率高达99%，远超行业标准（36.77%）。

Conclusion: 该DRL算法显著提升了高速密集交通中的安全性，为ADAS/ADS系统提供了更全面的纵向控制解决方案。

Abstract: Advanced Driver Assistance Systems (ADAS) and Advanced Driving Systems (ADS)
are key to improving road safety, yet most existing implementations focus
primarily on the vehicle ahead, neglecting the behavior of following vehicles.
This shortfall often leads to chain reaction collisions in high speed, densely
spaced traffic particularly when a middle vehicle suddenly brakes and trailing
vehicles cannot respond in time. To address this critical gap, we propose a
novel longitudinal control and collision avoidance algorithm that integrates
adaptive cruising with emergency braking. Leveraging deep reinforcement
learning, our method simultaneously accounts for both leading and following
vehicles. Through a data preprocessing framework that calibrates real-world
sensor data, we enhance the robustness and reliability of the training process,
ensuring the learned policy can handle diverse driving conditions. In simulated
high risk scenarios (e.g., emergency braking in dense traffic), the algorithm
effectively prevents potential pile up collisions, even in situations involving
heavy duty vehicles. Furthermore, in typical highway scenarios where three
vehicles decelerate, the proposed DRL approach achieves a 99% success rate far
surpassing the standard Federal Highway Administration speed concepts guide,
which reaches only 36.77% success under the same conditions.

</details>


### [256] [Diffeomorphic Obstacle Avoidance for Contractive Dynamical Systems via Implicit Representations](https://arxiv.org/abs/2504.18860)
*Ken-Joel Simmoteit,Philipp Schillinger,Leonel Rozo*

Main category: cs.RO

TL;DR: 论文提出了一种结合神经收缩动力系统和符号距离场微分变换的框架，旨在保证动态机器人技能的安全性和鲁棒性，尤其是在复杂环境中。


<details>
  <summary>Details</summary>
Motivation: 随着机器人执行任务的复杂性和动态性增加，确保其技能的安全性和鲁棒性变得至关重要，尤其是在杂乱环境和未知任务情境下。

Method: 采用神经收缩动力系统进行技能外推，并结合符号距离场（SDF）微分变换设计全障碍物避障策略，以保持收缩稳定性。

Result: 在合成数据集和真实厨房环境任务中，该框架能够局部调整学习的收缩向量场，同时避免高曲率运动路径，优于多种现有方法。

Conclusion: 该方法有效平衡了安全性和鲁棒性，为动态机器人技能在复杂环境中的应用提供了可行解决方案。

Abstract: Ensuring safety and robustness of robot skills is becoming crucial as robots
are required to perform increasingly complex and dynamic tasks. The former is
essential when performing tasks in cluttered environments, while the latter is
relevant to overcome unseen task situations. This paper addresses the challenge
of ensuring both safety and robustness in dynamic robot skills learned from
demonstrations. Specifically, we build on neural contractive dynamical systems
to provide robust extrapolation of the learned skills, while designing a
full-body obstacle avoidance strategy that preserves contraction stability via
diffeomorphic transforms. This is particularly crucial in complex environments
where implicit scene representations, such as Signed Distance Fields (SDFs),
are necessary. To this end, our framework called Signed Distance Field
Diffeomorphic Transform, leverages SDFs and flow-based diffeomorphisms to
achieve contraction-preserving obstacle avoidance. We thoroughly evaluate our
framework on synthetic datasets and several real-world robotic tasks in a
kitchen environment. Our results show that our approach locally adapts the
learned contractive vector field while staying close to the learned dynamics
and without introducing highly-curved motion paths, thus outperforming several
state-of-the-art methods.

</details>


### [257] [PolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-rich Manipulation Using Tactile-Diffusion Policies](https://arxiv.org/abs/2504.19341)
*Jialiang Zhao,Naveen Kuppuswamy,Siyuan Feng,Benjamin Burchfiel,Edward Adelson*

Main category: cs.RO

TL;DR: 提出了一种新型机器人手指PolyTouch，集成了触觉、听觉和视觉传感，显著提升接触感知操控策略的性能。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化家庭环境中机器人操控的鲁棒性问题，现有基于视觉和本体感知的方法因遮挡和接触控制不足而表现不佳。

Method: 设计了PolyTouch手指，结合触觉、听觉和视觉传感，并利用多模态反馈训练触觉扩散策略。

Result: PolyTouch寿命提升20倍以上，触觉感知策略显著优于无触觉方法。

Conclusion: 多模态接触传感有效加速了接触感知操控策略的发展，为更可靠的家庭机器人铺平了道路。

Abstract: Achieving robust dexterous manipulation in unstructured domestic environments
remains a significant challenge in robotics. Even with state-of-the-art robot
learning methods, haptic-oblivious control strategies (i.e. those relying only
on external vision and/or proprioception) often fall short due to occlusions,
visual complexities, and the need for precise contact interaction control. To
address these limitations, we introduce PolyTouch, a novel robot finger that
integrates camera-based tactile sensing, acoustic sensing, and peripheral
visual sensing into a single design that is compact and durable. PolyTouch
provides high-resolution tactile feedback across multiple temporal scales,
which is essential for efficiently learning complex manipulation tasks.
Experiments demonstrate an at least 20-fold increase in lifespan over
commercial tactile sensors, with a design that is both easy to manufacture and
scalable. We then use this multi-modal tactile feedback along with
visuo-proprioceptive observations to synthesize a tactile-diffusion policy from
human demonstrations; the resulting contact-aware control policy significantly
outperforms haptic-oblivious policies in multiple contact-aware manipulation
policies. This paper highlights how effectively integrating multi-modal contact
sensing can hasten the development of effective contact-aware manipulation
policies, paving the way for more reliable and versatile domestic robots. More
information can be found at https://polytouch.alanz.info/

</details>


### [258] [GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field](https://arxiv.org/abs/2504.19409)
*Zuxing Lu,Xin Yuan,Shaowen Yang,Jingyu Liu,Jiawei Wang,Changyin Sun*

Main category: cs.RO

TL;DR: GSFF-SLAM是一种基于3D高斯泼溅的新型密集语义SLAM系统，通过特征场实现外观、几何和N维语义特征的联合渲染，支持稀疏和嘈杂的2D先验信号，并在跟踪精度和逼真渲染质量上超越先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有语义SLAM系统依赖2D真实先验进行监督，但在真实环境中这些信号稀疏且嘈杂，限制了性能。因此，需要一种能高效利用各类2D先验的方法。

Method: GSFF-SLAM采用3D高斯泼溅技术，通过独立优化特征梯度，实现了外观、几何和语义特征的联合渲染，支持多种2D先验信号。

Result: 在2D真实先验下，GSFF-SLAM达到95.03%的mIoU语义分割性能，速度提升2.9倍且性能损失微小。

Conclusion: GSFF-SLAM通过3D高斯泼溅和特征场优化，显著提升了语义SLAM的精度和效率，适用于复杂真实环境。

Abstract: Semantic-aware 3D scene reconstruction is essential for autonomous robots to
perform complex interactions. Semantic SLAM, an online approach, integrates
pose tracking, geometric reconstruction, and semantic mapping into a unified
framework, shows significant potential. However, existing systems, which rely
on 2D ground truth priors for supervision, are often limited by the sparsity
and noise of these signals in real-world environments. To address this
challenge, we propose GSFF-SLAM, a novel dense semantic SLAM system based on 3D
Gaussian Splatting that leverages feature fields to achieve joint rendering of
appearance, geometry, and N-dimensional semantic features. By independently
optimizing feature gradients, our method supports semantic reconstruction using
various forms of 2D priors, particularly sparse and noisy signals. Experimental
results demonstrate that our approach outperforms previous methods in both
tracking accuracy and photorealistic rendering quality. When utilizing 2D
ground truth priors, GSFF-SLAM achieves state-of-the-art semantic segmentation
performance with 95.03\% mIoU, while achieving up to 2.9$\times$ speedup with
only marginal performance degradation.

</details>


### [259] [GAN-SLAM: Real-Time GAN Aided Floor Plan Creation Through SLAM](https://arxiv.org/abs/2504.19653)
*Leon Davies,Baihua Li,Mohamad Saada,Simon Sølvsten,Qinggang Meng*

Main category: cs.RO

TL;DR: 该论文提出了一种名为'GAN-SLAM'的新方法，利用生成对抗网络（GAN）在SLAM过程中清理和完善占用网格，减少噪声和不准确性对输出地图的影响，显著提升了地图质量和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的SLAM系统由于机器人运动的动态性，在2D表示（如占用网格地图）中容易产生噪声和不准确性，影响下游任务（如平面图创建）的效果。这促使作者提出一种新方法来提升地图质量。

Method: 作者提出'GAN-SLAM'，通过生成对抗网络（GAN）在SLAM过程中清理和补全占用网格，并借鉴3D SLAM中高精度的位姿估计技术，将其适配到2D形式。

Result: 实验结果表明，该方法显著提升了地图的保真度和质量，减少了噪声和错误，适用于大规模复杂环境下的实时操作，并支持新的下游任务（如平面图绘制）。

Conclusion: GAN-SLAM为SLAM领域带来重要进展，提升了其在基于地图的任务中的实用性，同时也为使用GAN进行占用网格地图（OGM）错误校正提供了新思路。

Abstract: SLAM is a fundamental component of modern autonomous systems, providing
robots and their operators with a deeper understanding of their environment.
SLAM systems often encounter challenges due to the dynamic nature of robotic
motion, leading to inaccuracies in mapping quality, particularly in 2D
representations such as Occupancy Grid Maps. These errors can significantly
degrade map quality, hindering the effectiveness of specific downstream tasks
such as floor plan creation. To address this challenge, we introduce our novel
'GAN-SLAM', a new SLAM approach that leverages Generative Adversarial Networks
to clean and complete occupancy grids during the SLAM process, reducing the
impact of noise and inaccuracies introduced on the output map. We adapt and
integrate accurate pose estimation techniques typically used for 3D SLAM into a
2D form. This enables the quality improvement 3D LiDAR-odometry has seen in
recent years to be effective for 2D representations. Our results demonstrate
substantial improvements in map fidelity and quality, with minimal noise and
errors, affirming the effectiveness of GAN-SLAM for real-world mapping
applications within large-scale complex environments. We validate our approach
on real-world data operating in real-time, and on famous examples of 2D maps.
The improved quality of the output map enables new downstream tasks, such as
floor plan drafting, further enhancing the capabilities of autonomous systems.
Our novel approach to SLAM offers a significant step forward in the field,
improving the usability for SLAM in mapping-based tasks, and offers insight
into the usage of GANs for OGM error correction.

</details>


### [260] [Transformation & Translation Occupancy Grid Mapping: 2-Dimensional Deep Learning Refined SLAM](https://arxiv.org/abs/2504.19654)
*Leon Davies,Baihua Li,Mohamad Saada,Simon Sølvsten,Qinggang Meng*

Main category: cs.RO

TL;DR: 提出了一种名为TT-OGM的新方法，通过将3D SLAM中的精确姿态估计技术应用到2D SLAM中，并结合GAN和DRL，显著提升了复杂场景中的地图质量、准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 2D SLAM在复杂大场景中因里程计漂移和姿态估计不准确而表现不佳，传统的OGM生成的地图存在噪声且不清晰，难以满足高精度地图需求。

Method: 提出了TT-OGM方法，结合3D SLAM的精确姿态估计技术，使用GAN进行误差校正，并通过DRL生成大规模数据集以训练GAN。

Result: 在现实环境中实时运行SLAM，并在多种复杂场景下验证了其通用性，生成的地图质量远超现有方法。

Conclusion: TT-OGM显著提升了2D SLAM的性能，为复杂场景下的高精度地图生成提供了可行方案。

Abstract: SLAM (Simultaneous Localisation and Mapping) is a crucial component for
robotic systems, providing a map of an environment, the current location and
previous trajectory of a robot. While 3D LiDAR SLAM has received notable
improvements in recent years, 2D SLAM lags behind. Gradual drifts in odometry
and pose estimation inaccuracies hinder modern 2D LiDAR-odometry algorithms in
large complex environments. Dynamic robotic motion coupled with inherent
estimation based SLAM processes introduce noise and errors, degrading map
quality. Occupancy Grid Mapping (OGM) produces results that are often noisy and
unclear. This is due to the fact that evidence based mapping represents maps
according to uncertain observations. This is why OGMs are so popular in
exploration or navigation tasks. However, this also limits OGMs' effectiveness
for specific mapping based tasks such as floor plan creation in complex scenes.
To address this, we propose our novel Transformation and Translation Occupancy
Grid Mapping (TT-OGM). We adapt and enable accurate and robust pose estimation
techniques from 3D SLAM to the world of 2D and mitigate errors to improve map
quality using Generative Adversarial Networks (GANs). We introduce a novel data
generation method via deep reinforcement learning (DRL) to build datasets large
enough for training a GAN for SLAM error correction. We demonstrate our SLAM in
real-time on data collected at Loughborough University. We also prove its
generalisability on a variety of large complex environments on a collection of
large scale well-known 2D occupancy maps. Our novel approach enables the
creation of high quality OGMs in complex scenes, far surpassing the
capabilities of current SLAM algorithms in terms of quality, accuracy and
reliability.

</details>


### [261] [Human-Centered AI and Autonomy in Robotics: Insights from a Bibliometric Study](https://arxiv.org/abs/2504.19848)
*Simona Casini,Pietro Ducange,Francesco Marcelloni,Lorenzo Pollini*

Main category: cs.RO

TL;DR: 本文通过文献计量分析探讨了智能自主机器人系统的学术趋势和AI在自适应行为中的作用，重点结合以人为本的AI架构，并将研究成果映射到IBM MAPE-K架构中。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，智能机器人系统在复杂任务中展现出潜力，但也引发了对人类角色的思考。HCAI旨在平衡人类控制与自动化，提升系统性能的同时确保安全性和可信赖性。

Method: 使用SciMAT和VOSViewer对Scopus数据库中的文献进行计量分析，结合HCAI架构和IBM MAPE-K架构进行映射。

Result: 分析结果揭示了学术趋势、新兴主题及AI在机器人自适应行为中的关键作用，为实际的自主系统开发提供了指导。

Conclusion: 研究强调了HCAI在机器人系统中的重要性，为未来的自主系统开发提供了实践框架和方向。

Abstract: The development of autonomous robotic systems offers significant potential
for performing complex tasks with precision and consistency. Recent advances in
Artificial Intelligence (AI) have enabled more capable intelligent automation
systems, addressing increasingly complex challenges. However, this progress
raises questions about human roles in such systems. Human-Centered AI (HCAI)
aims to balance human control and automation, ensuring performance enhancement
while maintaining creativity, mastery, and responsibility. For real-world
applications, autonomous robots must balance task performance with reliability,
safety, and trustworthiness. Integrating HCAI principles enhances human-robot
collaboration and ensures responsible operation.
  This paper presents a bibliometric analysis of intelligent autonomous robotic
systems, utilizing SciMAT and VOSViewer to examine data from the Scopus
database. The findings highlight academic trends, emerging topics, and AI's
role in self-adaptive robotic behaviour, with an emphasis on HCAI architecture.
These insights are then projected onto the IBM MAPE-K architecture, with the
goal of identifying how these research results map into actual robotic
autonomous systems development efforts for real-world scenarios.

</details>


### [262] [NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks](https://arxiv.org/abs/2504.19854)
*Chia-Yu Hung,Qi Sun,Pengfei Hong,Amir Zadeh,Chuan Li,U-Xuan Tan,Navonil Majumder,Soujanya Poria*

Main category: cs.RO

TL;DR: NORA是一种3B参数的视觉-语言-动作（VLA）模型，旨在减少计算开销并保持高性能，适用于实时机器人环境。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在零样本场景表现优异，但视觉编码限制和高计算开销（如7B参数）妨碍了实时机器人应用的实用性。

Method: 采用Qwen-2.5-VL-3B多模态模型作为主干，结合97万真实机器人演示数据训练，配备FAST+分词器以高效生成动作序列。

Result: NORA在任务性能上优于现有大型VLA模型，显著降低计算开销，更适合实时应用。

Conclusion: NORA为VLA模型的高效实用化提供了可行解决方案，平衡性能与计算效率。

Abstract: Existing Visual-Language-Action (VLA) models have shown promising performance
in zero-shot scenarios, demonstrating impressive task execution and reasoning
capabilities. However, a significant challenge arises from the limitations of
visual encoding, which can result in failures during tasks such as object
grasping. Moreover, these models typically suffer from high computational
overhead due to their large sizes, often exceeding 7B parameters. While these
models excel in reasoning and task planning, the substantial computational
overhead they incur makes them impractical for real-time robotic environments,
where speed and efficiency are paramount. To address the limitations of
existing VLA models, we propose NORA, a 3B-parameter model designed to reduce
computational overhead while maintaining strong task performance. NORA adopts
the Qwen-2.5-VL-3B multimodal model as its backbone, leveraging its superior
visual-semantic understanding to enhance visual reasoning and action grounding.
Additionally, our \model{} is trained on 970k real-world robot demonstrations
and equipped with the FAST+ tokenizer for efficient action sequence generation.
Experimental results demonstrate that NORA outperforms existing large-scale VLA
models, achieving better task performance with significantly reduced
computational overhead, making it a more practical solution for real-time
robotic autonomy.

</details>


### [263] [Real-Time Imitation of Human Head Motions, Blinks and Emotions by Nao Robot: A Closed-Loop Approach](https://arxiv.org/abs/2504.19985)
*Keyhan Rayati,Amirhossein Feizi,Alireza Beigy,Pourya Shahverdi,Mehdi Tale Masouleh,Ahmad Kalhor*

Main category: cs.RO

TL;DR: 本文提出了一种通过Nao机器人实时模仿人类头部运动的新方法，旨在提升人机交互体验。结合MediaPipe视觉库和DeepFace情感识别技术，能够捕捉头部动作（如眨眼）和表情，并通过闭环反馈提高模仿精度。在实验中，Pitch和Yaw的R2分数分别达到96.3和98.9。该方法尤其对自闭症儿童的交流有潜在帮助。


<details>
  <summary>Details</summary>
Motivation: 提升人机交互的自然性和精确性，特别是为有特殊沟通需求的群体（如自闭症儿童）提供更好的互动工具。

Method: 采用MediaPipe实时捕捉头部运动和DeepFace识别情感，结合闭环反馈机制优化机器人响应。

Result: 实现了高精度的头部运动模仿（Pitch R2=96.3, Yaw R2=98.9），验证了系统的有效性。

Conclusion: 结合实时头部模仿和情感识别的框架显著提升了人机交互质量，尤其在辅助沟通障碍群体方面具有应用潜力。

Abstract: This paper introduces a novel approach for enabling real-time imitation of
human head motion by a Nao robot, with a primary focus on elevating human-robot
interactions. By using the robust capabilities of the MediaPipe as a computer
vision library and the DeepFace as an emotion recognition library, this
research endeavors to capture the subtleties of human head motion, including
blink actions and emotional expressions, and seamlessly incorporate these
indicators into the robot's responses. The result is a comprehensive framework
which facilitates precise head imitation within human-robot interactions,
utilizing a closed-loop approach that involves gathering real-time feedback
from the robot's imitation performance. This feedback loop ensures a high
degree of accuracy in modeling head motion, as evidenced by an impressive R2
score of 96.3 for pitch and 98.9 for yaw. Notably, the proposed approach holds
promise in improving communication for children with autism, offering them a
valuable tool for more effective interaction. In essence, proposed work
explores the integration of real-time head imitation and real-time emotion
recognition to enhance human-robot interactions, with potential benefits for
individuals with unique communication needs.

</details>


### [264] [Socially-Aware Autonomous Driving: Inferring Yielding Intentions for Safer Interactions](https://arxiv.org/abs/2504.20004)
*Jing Wang,Yan Jin,Hamid Taghavifar,Fei Ding,Chongfeng Wei*

Main category: cs.RO

TL;DR: 论文提出了一种基于有向无环图（DAG）的社会意图估计算法，并结合深度强化学习（DRL）的决策框架，以提高自动驾驶车辆（AV）在车道变换中的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆（AVs）与人工驾驶车辆（HVs）共存时，缺乏直接通信导致意图难以预测，尤其是车道变换中的通过和让行意图。为解决这一问题，需要更精准的意图估计和可靠决策。

Method: 采用有向无环图（DAG）进行社会意图估计，并结合深度强化学习（DRL）构建决策框架，通过仿真环境测试车道变换场景。

Result: 实验结果表明，该方法提升了AV在车道变换中的安全性和效率。

Conclusion: 该研究为AV与HV共存的复杂交通场景提供了一种有效的意图估计和决策解决方案。

Abstract: Since the emergence of autonomous driving technology, it has advanced rapidly
over the past decade. It is becoming increasingly likely that autonomous
vehicles (AVs) would soon coexist with human-driven vehicles (HVs) on the
roads. Currently, safety and reliable decision-making remain significant
challenges, particularly when AVs are navigating lane changes and interacting
with surrounding HVs. Therefore, precise estimation of the intentions of
surrounding HVs can assist AVs in making more reliable and safe lane change
decision-making. This involves not only understanding their current behaviors
but also predicting their future motions without any direct communication.
However, distinguishing between the passing and yielding intentions of
surrounding HVs still remains ambiguous. To address the challenge, we propose a
social intention estimation algorithm rooted in Directed Acyclic Graph (DAG),
coupled with a decision-making framework employing Deep Reinforcement Learning
(DRL) algorithms. To evaluate the method's performance, the proposed framework
can be tested and applied in a lane-changing scenario within a simulated
environment. Furthermore, the experiment results demonstrate how our approach
enhances the ability of AVs to navigate lane changes safely and efficiently on
roads.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [265] [Parameter Tuning of the Firefly Algorithm by Three Tuning Methods: Standard Monte Carlo, Quasi-Monte Carlo and Latin Hypercube Sampling Methods](https://arxiv.org/abs/2504.18545)
*Geethu Joy,Christian Huyck,Xin-She Yang*

Main category: stat.CO

TL;DR: 该论文研究了萤火虫算法（FA）的参数调优方法，通过三种不同方法（蒙特卡洛、拟蒙特卡洛和拉丁超立方采样）进行调优，并进行优化问题求解和统计分析。结果显示，FA的性能不受调优方法影响，表明其灵活性和通用性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了探索不同参数调优方法对萤火虫算法（FA）性能的影响，以确定其在不同优化问题中的适用性和灵活性。

Method: 论文采用三种参数调优方法（蒙特卡洛、拟蒙特卡洛和拉丁超立方采样）对FA进行调优，并使用调优后的FA解决六个优化问题，再通过多种统计假设检验（如t检验、F检验、Friedman检验和ANOVA）分析调优方法的影响。

Result: 结果表明，FA的性能不受调优方法影响，且参数调优结果与调优方法无关，说明FA在不同调优方法下都能有效解决问题。

Conclusion: 结论指出，萤火虫算法（FA）具有灵活性和通用性，三种调优方法均可用于其参数调优，且效果相当。

Abstract: There are many different nature-inspired algorithms in the literature, and
almost all such algorithms have algorithm-dependent parameters that need to be
tuned. The proper setting and parameter tuning should be carried out to
maximize the performance of the algorithm under consideration. This work is the
extension of the recent work on parameter tuning by Joy et al. (2024) presented
at the International Conference on Computational Science (ICCS 2024), and the
Firefly Algorithm (FA) is tuned using three different methods: the Monte Carlo
method, the Quasi-Monte Carlo method and the Latin Hypercube Sampling. The FA
with the tuned parameters is then used to solve a set of six different
optimization problems, and the possible effect of parameter setting on the
quality of the optimal solutions is analyzed. Rigorous statistical hypothesis
tests have been carried out, including Student's t-tests, F-tests,
non-parametric Friedman tests and ANOVA. Results show that the performance of
the FA is not influenced by the tuning methods used. In addition, the tuned
parameter values are largely independent of the tuning methods used. This
indicates that the FA can be flexible and equally effective in solving
optimization problems, and any of the three tuning methods can be used to tune
its parameters effectively.

</details>


### [266] [A Langevin sampling algorithm inspired by the Adam optimizer](https://arxiv.org/abs/2504.18911)
*Benedict Leimkuhler,René Lohmann,Peter Whalley*

Main category: stat.CO

TL;DR: 提出了一种基于时间重标度朗之万动力学的自适应步长MCMC采样框架，通过动态调整步长提高数值稳定性和收敛速度，适用于复杂后验分布采样。


<details>
  <summary>Details</summary>
Motivation: 为解决固定步长MCMC采样在复杂后验分布中的稳定性与效率问题，提出动态调整步长的方法，兼顾梯度剧烈变化区域和平稳区域的采样需求。

Method: 通过引入辅助变量实现时间重标度，利用松弛方程积累局部监控函数的移动平均值，动态控制步长，无需修改物理系统的漂移项。算法可兼容现有固定步长朗之万积分器。

Result: 数值实验（如Neal漏斗和MNIST数据分类的贝叶斯神经网络）表明，该方法在精度和稳定性上均优于直接控制步长的方案。

Conclusion: 自适应步长框架显著提升了MCMC采样的效率与鲁棒性，尤其适用于高维或非规则后验分布场景。

Abstract: We present a framework for adaptive-stepsize MCMC sampling based on
time-rescaled Langevin dynamics, in which the stepsize variation is dynamically
driven by an additional degree of freedom. Our approach augments the phase
space by an additional variable which in turn defines a time
reparameterization. The use of an auxiliary relaxation equation allows
accumulation of a moving average of a local monitor function and provides for
precise control of the timestep while circumventing the need to modify the
drift term in the physical system. Our algorithm is straightforward to
implement and can be readily combined with any off-the-peg fixed-stepsize
Langevin integrator. As a particular example, we consider control of the
stepsize by monitoring the norm of the log-posterior gradient, which takes
inspiration from the Adam optimizer, the stepsize being automatically reduced
in regions of steep change of the log posterior and increased on plateaus,
improving numerical stability and convergence speed. As in Adam, the stepsize
variation depends on the recent history of the gradient norm, which enhances
stability and improves accuracy compared to more immediate control approaches.
We demonstrate the potential benefit of this method--both in accuracy and in
stability--in numerical experiments including Neal's funnel and a Bayesian
neural network for classification of MNIST data.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [267] [Predicting Stress in Two-phase Random Materials and Super-Resolution Method for Stress Images by Embedding Physical Information](https://arxiv.org/abs/2504.18854)
*Tengfei Xing,Xiaodan Ren,Jie Li*

Main category: cond-mat.mtrl-sci

TL;DR: 论文提出了一个针对两相随机材料（TRMs）的应力预测框架，结合了多成分U-net和混合物理信息神经网络（MPINN）的方法，有效提高了应力预测精度和图像分辨率，适用于多尺度分析。


<details>
  <summary>Details</summary>
Motivation: 为了解决复杂微结构材料中因分辨率限制导致的应力集中区域观测困难问题，并利用应力图像的物理约束开发新的超分辨率技术。

Method: 1. 使用多成分U-net（MC U-net）预测低分辨率材料微结构的应力，减少相界面处的误差；2. 提出基于混合物理信息神经网络（MPINN）的应力超分辨率方法（SRPINN），无需配对训练数据即可任意倍数提升分辨率。

Result: 通过迁移学习对不同相体积分数和加载状态的TRMs进行应力分析，证明框架具有高准确性和泛化能力。

Conclusion: 该框架通过结合深度学习和物理约束，显著提升了应力预测的精度和多尺度分析能力，为工程应用提供了新思路。

Abstract: Stress analysis is an important part of material design. For materials with
complex microstructures, such as two-phase random materials (TRMs), material
failure is often accompanied by stress concentration. Phase interfaces in
two-phase materials are critical for stress concentration. Therefore, the
prediction error of stress at phase boundaries is crucial. In practical
engineering, the pixels of the obtained material microstructure images are
limited, which limits the resolution of stress images generated by deep
learning methods, making it difficult to observe stress concentration regions.
Existing Image Super-Resolution (ISR) technologies are all based on data-driven
supervised learning. However, stress images have natural physical constraints,
which provide new ideas for new ISR technologies. In this study, we constructed
a stress prediction framework for TRMs. First, the framework uses a proposed
Multiple Compositions U-net (MC U-net) to predict stress in low-resolution
material microstructures. By considering the phase interface information of the
microstructure, the MC U-net effectively reduces the problem of excessive
prediction errors at phase boundaries. Secondly, a Mixed Physics-Informed
Neural Network (MPINN) based method for stress ISR (SRPINN) was proposed. By
introducing the constraints of physical information, the new method does not
require paired stress images for training and can increase the resolution of
stress images to any multiple. This enables a multiscale analysis of the stress
concentration regions at phase boundaries. Finally, we performed stress
analysis on TRMs with different phase volume fractions and loading states
through transfer learning. The results show the proposed stress prediction
framework has satisfactory accuracy and generalization ability.

</details>


### [268] [Composable and adaptive design of machine learning interatomic potentials guided by Fisher-information analysis](https://arxiv.org/abs/2504.19372)
*Weishi Wang,Mark K. Transtrum,Vincenzo Lordi,Vasily V. Bulatov,Amit Samanta*

Main category: cond-mat.mtrl-sci

TL;DR: 提出了一种自适应物理信息的机器学习原子间势(MLIPs)设计策略，通过迭代重构复合模型和统一训练，结合Fisher信息矩阵(FIM)和多属性误差指标进行评估，最终在铌数据案例中取得良好的性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高MLIPs的适应性和扩展性，需要一种能够动态调整模型结构并优化性能的设计策略。

Method: 采用迭代重构复合模型的方法，结合FIM和多属性误差指标进行评估，指导模型重构和超参数优化。

Result: 在铌数据集案例中，优化后的模型仅需75个参数，实现了力RMSE 0.172 eV/Å和能量RMSE 0.013 eV/atom的性能。

Conclusion: 该自适应设计策略在灵活性和性能之间取得了平衡，为MLIPs的优化提供了有效方法。

Abstract: An adaptive physics-informed model design strategy for machine-learning
interatomic potentials (MLIPs) is proposed. This strategy follows an iterative
reconfiguration of composite models from single-term models, followed by a
unified training procedure. A model evaluation method based on the Fisher
information matrix (FIM) and multiple-property error metrics is proposed to
guide model reconfiguration and hyperparameter optimization. Combining the
model reconfiguration and the model evaluation subroutines, we provide an
adaptive MLIP design strategy that balances flexibility and extensibility. In a
case study of designing models against a structurally diverse niobium dataset,
we managed to obtain an optimal configuration with 75 parameters generated by
our framework that achieved a force RMSE of 0.172 eV/{\AA} and an energy RMSE
of 0.013 eV/atom.

</details>


### [269] [Interpretable machine learning-guided design of Fe-based soft magnetic alloys](https://arxiv.org/abs/2504.19787)
*Aditi Nachnani,Kai K. Li-Caldwell,Saptarshi Biswas,Prince Sharma,Gaoyuan Ouyang,Prashant Singh*

Main category: cond-mat.mtrl-sci

TL;DR: 该论文提出了一种机器学习方法，用于预测富铁软磁合金（特别是Fe-Si-B系统）的饱和磁化强度（MS）和矫顽力（HC），并通过实验和理论验证了其准确性。


<details>
  <summary>Details</summary>
Motivation: 通过机器学习加速高性能、不含钴和镍的软磁材料的搜索，同时减少实验成本和时间。

Method: 利用机器学习模型训练实验数据，结合密度泛函理论（DFT）预测，分析合金成分对磁性能的影响。

Result: 研究发现，增加Si和B含量会降低饱和磁化强度，而矫顽力更受处理条件影响。模型预测结果与实验数据高度一致。

Conclusion: 该机器学习框架在预测和优化软磁合金性能方面展现出潜力，尤其是在开发新型无钴、无镍材料中具有重要应用。

Abstract: We present a machine-learning guided approach to predict saturation
magnetization (MS) and coercivity (HC) in Fe-rich soft magnetic alloys,
particularly Fe-Si-B systems. ML models trained on experimental data reveals
that increasing Si and B content reduces MS from 1.81T (DFT~2.04 T) to ~1.54 T
(DFT~1.56T) in Fe-Si-B, which is attributed to decreased magnetic density and
structural modifications. Experimental validation of ML predicted magnetic
saturation on Fe-1Si-1B (2.09T), Fe-5Si-5B (2.01T) and Fe-10Si-10B (1.54T)
alloy compositions further support our findings. These trends are consistent
with density functional theory (DFT) predictions, which link increased
electronic disorder and band broadening to lower MS values. Experimental
validation on selected alloys confirms the predictive accuracy of the ML model,
with good agreement across compositions. Beyond predictive accuracy, detailed
uncertainty quantification and model interpretability including through feature
importance and partial dependence analysis reveals that MS is governed by a
nonlinear interplay between Fe content, early transition metal ratios, and
annealing temperature, while HC is more sensitive to processing conditions such
as ribbon thickness and thermal treatment windows. The ML framework was further
applied to Fe-Si-B/Cr/Cu/Zr/Nb alloys in a pseudo-quaternary compositional
space, which shows comparable magnetic properties to NANOMET
(Fe84.8Si0.5B9.4Cu0.8 P3.5C1), FINEMET (Fe73.5Si13.5B9 Cu1Nb3), NANOPERM
(Fe88Zr7B4Cu1), and HITPERM (Fe44Co44Zr7B4Cu1. Our fundings demonstrate the
potential of ML framework for accelerated search of high-performance, Co- and
Ni-free, soft magnetic materials.

</details>


### [270] [Graph Neural Network Prediction of Nonlinear Optical Properties](https://arxiv.org/abs/2504.19987)
*Yomn Alkabakibi,Congwei Xie,Artem R. Oganov*

Main category: cond-mat.mtrl-sci

TL;DR: 摘要介绍了一种基于深度学习的Atomistic Line Graph Neural Network（ALIGNN）方法，用于预测非线性光学（NLO）材料的性能，特别是二次谐波产生（SHG）。该方法利用NOEMD数据库和Kurtz-Perry系数作为关键目标，实现了82.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: 由于实验和第一性原理计算耗时且昂贵，开发一种高效方法来预测具有显著SHG的新型非线性光学材料成为迫切需求。

Method: 研究使用Atomistic Line Graph Neural Network（ALIGNN）结合NOEMD数据库和Kurtz-Perry系数进行深度学习建模。

Result: 模型在绝对误差1 pm/V和相对误差0.5范围内达到了82.5%的准确率。

Conclusion: 该研究表明深度学习在加速发现和设计具有所需性能的先进光学材料方面具有巨大潜力。

Abstract: Nonlinear optical (NLO) materials for generating lasers via second harmonic
generation (SHG) are highly sought in today's technology. However, discovering
novel materials with considerable SHG is challenging due to the time-consuming
and costly nature of both experimental methods and first-principles
calculations. In this study, we present a deep learning approach using the
Atomistic Line Graph Neural Network (ALIGNN) to predict NLO properties.
Sourcing data from the Novel Opto-Electronic Materials Discovery (NOEMD)
database and using the Kurtz-Perry (KP) coefficient as the key target, we
developed a robust model capable of accurately estimating nonlinear optical
responses. Our results demonstrate that the model achieves 82.5% accuracy at a
tolerated absolute error up to 1 pm/V and relative error not exceeding 0.5.
This work highlights the potential of deep learning in accelerating the
discovery and design of advanced optical materials with desired properties.

</details>


### [271] [Curiosity Driven Exploration to Optimize Structure-Property Learning in Microscopy](https://arxiv.org/abs/2504.20011)
*Aditya Vatsavai,Ganesh Narasimha,Yongtao Liu,Jan-Chi Yang,Hiroshu Funakubo,Maxim Ziatdinov,Rama Vasudevan*

Main category: cond-mat.mtrl-sci

TL;DR: 论文提出了一种轻量级好奇心算法，用于在材料科学中高效映射结构-性能关系，优于随机采样，并减少了计算开销。


<details>
  <summary>Details</summary>
Motivation: 快速确定材料的结构-性能关系对理解基础机制和辅助材料设计至关重要，但现有方法在计算多维和相关输出空间时开销较大。

Method: 提出了一种轻量级好奇心算法，主动采样未探索的结构-性能关系区域，利用基于深度学习的代理模型进行误差预测。

Result: 算法在预测结构-性能关系方面优于随机采样，为材料科学提供了高效映射工具。

Conclusion: 该算法为材料科学中的结构-性能关系研究提供了一种计算高效且性能优越的新方法。

Abstract: Rapidly determining structure-property correlations in materials is an
important challenge in better understanding fundamental mechanisms and greatly
assists in materials design. In microscopy, imaging data provides a direct
measurement of the local structure, while spectroscopic measurements provide
relevant functional property information. Deep kernel active learning
approaches have been utilized to rapidly map local structure to functional
properties in microscopy experiments, but are computationally expensive for
multi-dimensional and correlated output spaces. Here, we present an alternative
lightweight curiosity algorithm which actively samples regions with unexplored
structure-property relations, utilizing a deep-learning based surrogate model
for error prediction. We show that the algorithm outperforms random sampling
for predicting properties from structures, and provides a convenient tool for
efficient mapping of structure-property relationships in materials science.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [272] [AI Recommendations and Non-instrumental Image Concerns](https://arxiv.org/abs/2504.19047)
*David Almog*

Main category: econ.GN

TL;DR: 论文研究发现，即使AI建议无实际负面影响，人们对自我形象的担忧仍导致其忽视AI建议并降低任务表现。


<details>
  <summary>Details</summary>
Motivation: 探索人类与AI合作的实际障碍，重点关注非工具性因素（如自我形象）如何影响AI建议的采纳。

Method: 通过在线实验分析参与者在无经济后果情况下对AI建议的态度及行为反应。

Result: 参与者因担心他人评价（非经济因素）而拒绝AI建议，导致任务表现下降。

Conclusion: 非工具性心理因素是阻碍人机协作的重要障碍，未来需设计更关注用户心理的AI系统。

Abstract: There is growing enthusiasm about the potential for humans and AI to
collaborate by leveraging their respective strengths. Yet in practice, this
promise often falls short. This paper uses an online experiment to identify
non-instrumental image concerns as a key reason individuals underutilize AI
recommendations. I show that concerns about how one is perceived, even when
those perceptions carry no monetary consequences, lead participants to
disregard AI advice and reduce task performance.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [273] [Global Climate Model Bias Correction Using Deep Learning](https://arxiv.org/abs/2504.19145)
*Abhishek Pasula,Deepak N. Subramani*

Main category: physics.ao-ph

TL;DR: 该论文提出了一种基于深度学习的偏差校正方法，用于纠正CMIP6气候模型在孟加拉湾海面温度（SST）预测中的偏差，相比传统方法（EDCDF）降低了15%的RMSE。


<details>
  <summary>Details</summary>
Motivation: CMIP气候模型在孟加拉湾的SST预测中存在显著偏差，影响气候变化的准确分析，因此需要开发更有效的偏差校正方法。

Method: 使用三种深度神经网络架构（UNet、双向LSTM和ConvLSTM）进行偏差校正，并与线性回归和EDCDF方法对比。训练数据为1950-2014年历史数据和2015-2020年预测数据，测试数据为2021-2024年。

Result: UNet架构在去除气候背景后表现最佳，其偏差校正效果比EDCDF方法提升了15%的RMSE。

Conclusion: 深度学习方法能显著提升气候模型预测的准确性，UNet是校正SST偏差的最优选择。

Abstract: Climate change affects ocean temperature, salinity and sea level, impacting
monsoons and ocean productivity. Future projections by Global Climate Models
based on shared socioeconomic pathways from the Coupled Model Intercomparison
Project (CMIP) are widely used to understand the effects of climate change.
However, CMIP models have significant bias compared to reanalysis in the Bay of
Bengal for the time period when both projections and reanalysis are available.
For example, there is a 1.5C root mean square error (RMSE) in the sea surface
temperature (SST) projections of the climate model CNRM-CM6 compared to the
Ocean Reanalysis System (ORAS5). We develop a suite of data-driven deep
learning models for bias correction of climate model projections and apply it
to correct SST projections of the Bay of Bengal. We propose the use of three
different deep neural network architectures: convolutional encoder-decoder
UNet, Bidirectional LSTM and ConvLSTM. We also use a baseline linear regression
model and the Equi-Distant Cumulative Density Function (EDCDF) bias correction
method for comparison and evaluating the impact of the new deep learning
models. All bias correction models are trained using pairs of monthly CMIP6
projections and the corresponding month's ORAS5 as input and output. Historical
data (1950-2014) and future projection data (2015-2020) of CNRM-CM6 are used
for training and validation, including hyperparameter tuning. Testing is
performed on future projection data from 2021 to 2024. Detailed analysis of the
three deep neural models has been completed. We found that the UNet
architecture trained using a climatology-removed CNRM-CM6 projection as input
and climatology-removed ORAS5 as output gives the best bias-corrected
projections. Our novel deep learning-based method for correcting CNRM-CM6 data
has a 15% reduction in RMSE compared EDCDF.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [274] [REED-VAE: RE-Encode Decode Training for Iterative Image Editing with Diffusion Models](https://arxiv.org/abs/2504.18989)
*Gal Almog,Ariel Shamir,Ohad Fried*

Main category: cs.GR

TL;DR: 论文提出了REED-VAE训练方案，解决了隐扩散模型在迭代编辑同一图像时的噪声和伪影累积问题，支持多种编辑方法的迭代操作。


<details>
  <summary>Details</summary>
Motivation: 现有隐扩散模型在迭代编辑同一图像时，由于像素和潜在空间之间的重复转换会导致噪声和伪影累积，限制了其应用。

Method: 提出了一种RE-encode decode (REED)训练方案，优化变分自编码器(VAE)，以在多次迭代后仍能保持图像质量。

Result: REED-VAE在多种图像编辑场景中表现优异，包括基于文本和掩码的编辑框架，显著提升了编辑的成功率和精确度。

Conclusion: REED-VAE为多方法图像编辑任务提供了新的基准，并有望推动该领域的发展。

Abstract: While latent diffusion models achieve impressive image editing results, their
application to iterative editing of the same image is severely restricted. When
trying to apply consecutive edit operations using current models, they
accumulate artifacts and noise due to repeated transitions between pixel and
latent spaces. Some methods have attempted to address this limitation by
performing the entire edit chain within the latent space, sacrificing
flexibility by supporting only a limited, predetermined set of diffusion
editing operations. We present a RE-encode decode (REED) training scheme for
variational autoencoders (VAEs), which promotes image quality preservation even
after many iterations. Our work enables multi-method iterative image editing:
users can perform a variety of iterative edit operations, with each operation
building on the output of the previous one using both diffusion-based
operations and conventional editing techniques. We demonstrate the advantage of
REED-VAE across a range of image editing scenarios, including text-based and
mask-based editing frameworks. In addition, we show how REED-VAE enhances the
overall editability of images, increasing the likelihood of successful and
precise edit operations. We hope that this work will serve as a benchmark for
the newly introduced task of multi-method image editing. Our code and models
will be available at https://github.com/galmog/REED-VAE

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [275] [Application of the Brain Drain Optimization Algorithm to the N-Queens Problem](https://arxiv.org/abs/2504.18953)
*Sahar Ramezani Jolfaei,Sepehr Khodadadi Hossein Abadi*

Main category: cs.NE

TL;DR: 论文提出了一种基于群体智能的Brain Drain Optimization（BRADO）算法，应用于N皇后问题，并通过多准则决策优化配置。BRADO在解质量和目标函数值上优于其他算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是将受知识精英移民启发的BRADO算法应用于经典组合优化问题N皇后问题，验证其有效性。

Method: 采用基于TOPSIS的多准则决策方法调整配置，设计成本函数引导搜索。

Result: BRADO在解质量上优于PSO、GA、ICA、ILS和LS等算法，表现出更少的威胁和更优的目标函数值。

Conclusion: BRADO展现出作为通用组合问题求解器的潜力，为未来在AI其他领域的应用开辟了道路。

Abstract: This paper introduces the application of the Brain Drain Optimization
algorithm -- a swarm-based metaheuristic inspired by the emigration of
intellectual elites -- to the N-Queens problem. The N-Queens problem, a classic
combinatorial optimization problem, serves as a challenge for applying the
BRADO. A designed cost function guides the search, and the configurations are
tuned using a TOPSIS-based multicriteria decision making process. BRADO
consistently outperforms alternatives in terms of solution quality, achieving
fewer threats and better objective function values. To assess BRADO's efficacy,
it is benchmarked against several established metaheuristic algorithms,
including Particle Swarm Optimization (PSO), Genetic Algorithm (GA),
Imperialist Competitive Algorithm (ICA), Iterated Local Search (ILS), and basic
Local Search (LS). The study highlights BRADO's potential as a general-purpose
solver for combinatorial problems, opening pathways for future applications in
other domains of artificial intelligence.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [276] [Statistical Inference for Clustering-based Anomaly Detection](https://arxiv.org/abs/2504.18633)
*Nguyen Thi Minh Phu,Duong Tan Loc,Vo Nguyen Le Duy*

Main category: stat.ML

TL;DR: 提出了一种名为SI-CLAD的新型统计框架，用于检验基于聚类的无监督异常检测结果，通过选择性推断（SI）框架严格控制误检概率，并提升真检率。


<details>
  <summary>Details</summary>
Motivation: 传统的基于聚类的异常检测方法缺乏对检测结果可靠性的保证，需要一种能够控制误检概率并提升检测性能的统计方法。

Method: 基于选择性推断（SI）框架，分析聚类异常检测的选择机制，提出SI-CLAD框架，控制误检概率低于预设显著性水平α，并优化真检率。

Result: 在合成和真实数据集上的实验验证了SI-CLAD的理论有效性，并展示了其优于传统方法的性能。

Conclusion: SI-CLAD为无监督异常检测提供了一种可靠的统计检验方法，显著提升了检测的准确性和可信度。

Abstract: Unsupervised anomaly detection (AD) is a fundamental problem in machine
learning and statistics. A popular approach to unsupervised AD is
clustering-based detection. However, this method lacks the ability to guarantee
the reliability of the detected anomalies. In this paper, we propose SI-CLAD
(Statistical Inference for CLustering-based Anomaly Detection), a novel
statistical framework for testing the clustering-based AD results. The key
strength of SI-CLAD lies in its ability to rigorously control the probability
of falsely identifying anomalies, maintaining it below a pre-specified
significance level $\alpha$ (e.g., $\alpha = 0.05$). By analyzing the selection
mechanism inherent in clustering-based AD and leveraging the Selective
Inference (SI) framework, we prove that false detection control is attainable.
Moreover, we introduce a strategy to boost the true detection rate, enhancing
the overall performance of SI-CLAD. Extensive experiments on synthetic and
real-world datasets provide strong empirical support for our theoretical
findings, showcasing the superior performance of the proposed method.

</details>


### [277] [Foundations of Safe Online Reinforcement Learning in the Linear Quadratic Regulator: $\sqrt{T}$-Regret](https://arxiv.org/abs/2504.18657)
*Benjamin Schiffer,Lucas Janson*

Main category: stat.ML

TL;DR: 本文研究安全约束下的在线强化学习，针对一维线性动力系统的控制问题，提出首个实现$	ilde{O}_T(\\sqrt{T})$遗憾的安全算法。


<details>
  <summary>Details</summary>
Motivation: 研究如何在安全约束下高效学习，以解决在线强化学习在实际应用中的安全性问题。

Method: 研究一维线性动力系统的安全约束控制问题，提出基于截断线性控制器的安全算法。

Result: 算法实现了$	ilde{O}_T(\\sqrt{T})$的遗憾，并在约束影响最优控制器时表现出比无约束环境下更快的学习率。

Conclusion: 通过引入新的基准和证明最优控制器的连续性，为安全约束强化学习奠定了理论基础。

Abstract: Understanding how to efficiently learn while adhering to safety constraints
is essential for using online reinforcement learning in practical applications.
However, proving rigorous regret bounds for safety-constrained reinforcement
learning is difficult due to the complex interaction between safety,
exploration, and exploitation. In this work, we seek to establish foundations
for safety-constrained reinforcement learning by studying the canonical problem
of controlling a one-dimensional linear dynamical system with unknown dynamics.
We study the safety-constrained version of this problem, where the state must
with high probability stay within a safe region, and we provide the first safe
algorithm that achieves regret of $\tilde{O}_T(\sqrt{T})$. Furthermore, the
regret is with respect to the baseline of truncated linear controllers, a
natural baseline of non-linear controllers that are well-suited for
safety-constrained linear systems. In addition to introducing this new
baseline, we also prove several desirable continuity properties of the optimal
controller in this baseline. In showing our main result, we prove that whenever
the constraints impact the optimal controller, the non-linearity of our
controller class leads to a faster rate of learning than in the unconstrained
setting.

</details>


### [278] [Local Polynomial Lp-norm Regression](https://arxiv.org/abs/2504.18695)
*Ladan Tazik,James Stafford,John Braun*

Main category: stat.ML

TL;DR: 论文提出了基于$L_p$范数的局部多项式回归方法，通过加权$L_p$范数估计替代传统的加权最小二乘估计，用于处理非高斯噪声条件下的回归问题，并展示了该方法在一维和二维数据上的优越性。


<details>
  <summary>Details</summary>
Motivation: 传统最小二乘估计在非高斯噪声下表现不佳，因此需要考虑基于其他范数的估计方法，以更好地适应残差的非正态分布特性。

Method: 论文提出了局部多项式$L_p$范数回归，使用加权$L_p$范数估计替代加权最小二乘估计，并引入了新的参数$p$估计方法。

Result: 通过数值和理论分析，论文证明该方法在一维数据上优于局部最小二乘估计，并在二维数据上展示了良好的前景。

Conclusion: 提出了一种更灵活的回归方法，能有效适应非高斯噪声，未来可在高维数据中进一步验证其性能。

Abstract: The local least squares estimator for a regression curve cannot provide
optimal results when non-Gaussian noise is present. Both theoretical and
empirical evidence suggests that residuals often exhibit distributional
properties different from those of a normal distribution, making it worthwhile
to consider estimation based on other norms. It is suggested that $L_p$-norm
estimators be used to minimize the residuals when these exhibit non-normal
kurtosis. In this paper, we propose a local polynomial $L_p$-norm regression
that replaces weighted least squares estimation with weighted $L_p$-norm
estimation for fitting the polynomial locally. We also introduce a new method
for estimating the parameter $p$ from the residuals, enhancing the adaptability
of the approach. Through numerical and theoretical investigation, we
demonstrate our method's superiority over local least squares in
one-dimensional data and show promising outcomes for higher dimensions,
specifically in 2D.

</details>


### [279] [A Dictionary of Closed-Form Kernel Mean Embeddings](https://arxiv.org/abs/2504.18830)
*François-Xavier Briol,Alexandra Gessner,Toni Karvonen,Maren Mahsereci*

Main category: stat.ML

TL;DR: 该论文总结了核均值嵌入的已知公式，并提供了工具来推导新嵌入，同时发布了相关Python库。


<details>
  <summary>Details</summary>
Motivation: 核均值嵌入在贝叶斯积分等计算工具中至关重要，但推导其闭式表达式具挑战性，限制了核方法的适用范围。

Method: 通过系统整理已知核均值嵌入，并开发工具支持从现有嵌入推导新嵌入，同时实现Python库。

Result: 提供了一套完整的核均值嵌入字典及实用工具，并开源了轻量级Python实现。

Conclusion: 该研究降低了核方法的使用门槛，为数值积分和统计推断提供了更便捷的工具支持。

Abstract: Kernel mean embeddings -- integrals of a kernel with respect to a probability
distribution -- are essential in Bayesian quadrature, but also widely used in
other computational tools for numerical integration or for statistical
inference based on the maximum mean discrepancy. These methods often require,
or are enhanced by, the availability of a closed-form expression for the kernel
mean embedding. However, deriving such expressions can be challenging, limiting
the applicability of kernel-based techniques when practitioners do not have
access to a closed-form embedding. This paper addresses this limitation by
providing a comprehensive dictionary of known kernel mean embeddings, along
with practical tools for deriving new embeddings from known ones. We also
provide a Python library that includes minimal implementations of the
embeddings.

</details>


### [280] [ReLU integral probability metric and its applications](https://arxiv.org/abs/2504.18897)
*Yuha Park,Kunwoong Kim,Insung Kong,Yongdai Kim*

Main category: stat.ML

TL;DR: 本文提出了一种参数化积分概率度量（IPM），用于衡量两个概率分布之间的差异，通过优化特定参数化判别器（如ReLU单节点神经网络）实现高效的分布判别，并在高维场景下表现优异。该方法收敛速度快，可作为其他IPM的替代方案，且算法实现简单、超参数少。在因果推断和公平表示学习等任务中，该IPM表现出良好的理论保证和实际性能。


<details>
  <summary>Details</summary>
Motivation: 现有的IPM方法在非参数化判别器下可能计算复杂或难以扩展至高维数据，本文旨在提出一种参数化IPM，兼具高效性和理论保证。

Method: 采用参数化判别器（如ReLU单节点神经网络），通过优化判别器参数来度量分布差异，并设计高效算法实现。

Result: 理论证明该方法具有快速收敛率，实验显示其在多种任务中性能优于或媲美现有方法。

Conclusion: 参数化IPM是一种高效、可扩展的分布差异度量工具，适用于高维数据，并在实际应用中表现出色。

Abstract: We propose a parametric integral probability metric (IPM) to measure the
discrepancy between two probability measures. The proposed IPM leverages a
specific parametric family of discriminators, such as single-node neural
networks with ReLU activation, to effectively distinguish between
distributions, making it applicable in high-dimensional settings. By optimizing
over the parameters of the chosen discriminator class, the proposed IPM
demonstrates that its estimators have good convergence rates and can serve as a
surrogate for other IPMs that use smooth nonparametric discriminator classes.
We present an efficient algorithm for practical computation, offering a simple
implementation and requiring fewer hyperparameters. Furthermore, we explore its
applications in various tasks, such as covariate balancing for causal inference
and fair representation learning. Across such diverse applications, we
demonstrate that the proposed IPM provides strong theoretical guarantees, and
empirical experiments show that it achieves comparable or even superior
performance to other methods.

</details>


### [281] [Geometry-aware Active Learning of Spatiotemporal Dynamic Systems](https://arxiv.org/abs/2504.19012)
*Xizhuo,Zhang,Bing Yao*

Main category: stat.ML

TL;DR: 这篇文章提出了一个几何感知的主动学习框架G-ST-GP，用于三维动态系统的时空预测建模，通过结合时间相关性和几何流形特征提升预测准确性，并在3D心脏几何中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 先进的传感技术虽然提高了信息可见性，但复杂动态系统的三维几何分布和快速演变的时空信号给预测建模带来了挑战，文章试图通过几何感知和主动学习解决这一问题。

Method: 提出几何感知的时空高斯过程（G-ST-GP），整合时间相关性与几何流形特征；并设计自适应主动学习策略，基于预测不确定性和测地距离优化数据采集位置。

Result: 在3D心脏几何中的实验表明，该方法在预测高维动态行为上优于缺乏几何信息整合或有效数据采集的传统方法。

Conclusion: 几何感知的G-ST-GP框架和自适应主动学习策略显著提升了复杂动态系统的时空预测准确性，为类似问题提供了有效解决方案。

Abstract: Rapid developments in advanced sensing and imaging have significantly
enhanced information visibility, opening opportunities for predictive modeling
of complex dynamic systems. However, sensing signals acquired from such complex
systems are often distributed across 3D geometries and rapidly evolving over
time, posing significant challenges in spatiotemporal predictive modeling. This
paper proposes a geometry-aware active learning framework for modeling
spatiotemporal dynamic systems. Specifically, we propose a geometry-aware
spatiotemporal Gaussian Process (G-ST-GP) to effectively integrate the temporal
correlations and geometric manifold features for reliable prediction of
high-dimensional dynamic behaviors. In addition, we develop an adaptive active
learning strategy to strategically identify informative spatial locations for
data collection and further maximize the prediction accuracy. This strategy
achieves the adaptive trade-off between the prediction uncertainty in the
G-ST-GP model and the space-filling design guided by the geodesic distance
across the 3D geometry. We implement the proposed framework to model the
spatiotemporal electrodynamics in a 3D heart geometry. Numerical experiments
show that our framework outperforms traditional methods lacking the mechanism
of geometric information incorporation or effective data collection.

</details>


### [282] [Test Set Sizing for the Ridge Regression](https://arxiv.org/abs/2504.19231)
*Alexander Dubbs*

Main category: stat.ML

TL;DR: 论文推导了岭回归在训练样本数m趋近于无穷大时的理想训练/测试分割比例，结果表明分割比例对岭参数α的依赖较弱且可渐近忽略，仅依赖m和特征数n。这是首次在大数据极限下对机器学习模型的分割比例进行数学计算。


<details>
  <summary>Details</summary>
Motivation: 研究的目的是最大化模型的‘完整性’，即确保训练模型的测量误差尽可能接近其理论值。通过数学计算确定岭回归的理想训练/测试分割比例。

Method: 论文在训练样本数m趋近于无穷大的极限下，推导了岭回归的理想训练/测试分割比例，并分析了分割比例对岭参数α的依赖关系。

Result: 结果表明，分割比例对岭参数α的依赖较弱且可渐近忽略，仅依赖m和特征数n。这一结果与普通线性回归的分割比例在前两项渐近项上一致，实践中几乎无差别。

Conclusion: 论文首次在大数据极限下为岭回归提供了数学上的理想训练/测试分割比例，结果表明其对岭参数的依赖可忽略，且与普通线性回归结果一致。

Abstract: We derive the ideal train/test split for the ridge regression to high
accuracy in the limit that the number of training rows m becomes large. The
split must depend on the ridge tuning parameter, alpha, but we find that the
dependence is weak and can asymptotically be ignored; all parameters vanish
except for m and the number of features, n. This is the first time that such a
split is calculated mathematically for a machine learning model in the large
data limit. The goal of the calculations is to maximize "integrity," so that
the measured error in the trained model is as close as possible to what it
theoretically should be. This paper's result for the ridge regression split
matches prior art for the plain vanilla linear regression split to the first
two terms asymptotically, and it appears that practically there is no
difference.

</details>


### [283] [Contextual Online Uncertainty-Aware Preference Learning for Human Feedback](https://arxiv.org/abs/2504.19342)
*Nan Lu,Ethan X. Fang,Junwei Lu*

Main category: stat.ML

TL;DR: 提出一种新的统计框架，用于基于动态上下文的人类偏好数据同时进行在线决策和统计推断，优化了遗憾边界和估计量的渐近分布。


<details>
  <summary>Details</summary>
Motivation: 为解决RLHF中处理动态上下文下依赖的在线人类偏好结果的挑战。

Method: 提出两阶段算法（ε-贪婪策略后接开发阶段），并运用抗集中不等式和矩阵鞅集中技术进行理论分析。

Result: 仿真结果显示方法优于现有技术，应用于大规模多任务语言理解数据集的分析。

Conclusion: 新框架能有效处理动态上下文的人类偏好数据，为大规模语言模型性能评估提供新工具。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a pivotal
paradigm in artificial intelligence to align large models with human
preferences. In this paper, we propose a novel statistical framework to
simultaneously conduct the online decision-making and statistical inference on
the optimal model using human preference data based on dynamic contextual
information. Our approach introduces an efficient decision strategy that
achieves both the optimal regret bound and the asymptotic distribution of the
estimators. A key challenge in RLHF is handling the dependent online human
preference outcomes with dynamic contexts. To address this, in the
methodological aspect, we propose a two-stage algorithm starting with
$\epsilon$-greedy followed by exploitations; in the theoretical aspect, we
tailor anti-concentration inequalities and matrix martingale concentration
techniques to derive the uniform estimation rate and asymptotic normality of
the estimators using dependent samples from both stages. Extensive simulation
results demonstrate that our method outperforms state-of-the-art strategies. We
apply the proposed framework to analyze the human preference data for ranking
large language models on the Massive Multitask Language Understanding dataset,
yielding insightful results on the performance of different large language
models for medical anatomy knowledge.

</details>


### [284] [The Double Descent Behavior in Two Layer Neural Network for Binary Classification](https://arxiv.org/abs/2504.19351)
*Chathurika S Abeykoon,Aleksandr Beknazaryan,Hailin Sang*

Main category: stat.ML

TL;DR: 本文研究了模型测试误差中的“双下降”现象，即模型复杂度增加时误差先降后升再降，重点分析了二元分类任务下两层ReLU网络的数学理论。


<details>
  <summary>Details</summary>
Motivation: 目标是探究不同模型尺寸下测试误差双下降行为的数学理论，尤其关注训练样本数与模型维度比的影响。

Method: 使用两层ReLU神经网络和凸高斯极小极大定理（CGMT）分析全局训练损失的候选解。

Result: 通过理论分析和实验验证，量化了模型尺寸对测试误差双下降现象的影响。

Conclusion: 研究为理解复杂模型在训练中的误差行为提供了理论框架，揭示了模型尺寸与测试误差的非单调关系。

Abstract: Recent studies observed a surprising concept on model test error called the
double descent phenomenon, where the increasing model complexity decreases the
test error first and then the error increases and decreases again. To observe
this, we work on a two layer neural network model with a ReLU activation
function designed for binary classification under supervised learning. Our aim
is to observe and investigate the mathematical theory behind the double descent
behavior of model test error for varying model sizes. We quantify the model
size by the ratio of number of training samples to the dimension of the model.
Due to the complexity of the empirical risk minimization procedure, we use the
Convex Gaussian Min Max Theorem to find a suitable candidate for the global
training loss.

</details>


### [285] [Model uncertainty quantification using feature confidence sets for outcome excursions](https://arxiv.org/abs/2504.19464)
*Junting Ren,Armin Schwartzman*

Main category: stat.ML

TL;DR: 这篇论文提出了一个模型无关的框架，通过构建数据依赖的内外置信集来量化连续和二元结果的预测不确定性，旨在识别特征空间中预期或实际结果超过特定值的子集。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中（如医疗、金融和自动驾驶系统），量化预测不确定性对风险管理至关重要。传统方法（如置信区间）无法满足需求，需要一种更灵活的新方法。

Method: 提出基于置信集的模型无关框架，构建数据依赖的内外置信集，确保包含真实特征子集的概率理论保证（渐近和有限样本）。

Result: 通过模拟和实际数据（如房价预测、败血症诊断时间）验证了方法的有效性，证明了其广泛适用性。

Conclusion: 该框架为连续和二元预测模型提供了一种统一的、可广泛应用的预测不确定性量化方法。

Abstract: When implementing prediction models for high-stakes real-world applications
such as medicine, finance, and autonomous systems, quantifying prediction
uncertainty is critical for effective risk management. Traditional approaches
to uncertainty quantification, such as confidence and prediction intervals,
provide probability coverage guarantees for the expected outcomes
$f(\boldsymbol{x})$ or the realized outcomes $f(\boldsymbol{x})+\epsilon$.
Instead, this paper introduces a novel, model-agnostic framework for
quantifying uncertainty in continuous and binary outcomes using confidence sets
for outcome excursions, where the goal is to identify a subset of the feature
space where the expected or realized outcome exceeds a specific value. The
proposed method constructs data-dependent inner and outer confidence sets that
aim to contain the true feature subset for which the expected or realized
outcomes of these features exceed a specified threshold. We establish
theoretical guarantees for the probability that these confidence sets contain
the true feature subset, both asymptotically and for finite sample sizes. The
framework is validated through simulations and applied to real-world datasets,
demonstrating its utility in contexts such as housing price prediction and time
to sepsis diagnosis in healthcare. This approach provides a unified method for
uncertainty quantification that is broadly applicable across various continuous
and binary prediction models.

</details>


### [286] [Optimal Sequential Recommendations: Exploiting User and Item Structure](https://arxiv.org/abs/2504.19476)
*Mina Karzand,Guy Bresler*

Main category: stat.ML

TL;DR: 论文提出了一个在线推荐系统模型，通过用户和物品的聚类类型，利用隐变量模型捕捉用户偏好。算法结合了用户和物品结构，证明接近最优，并指出仅使用单一结构的不足。


<details>
  <summary>Details</summary>
Motivation: 研究在线推荐系统中用户和物品的结构对推荐效果的影响，旨在解决现有协同过滤算法中仅依赖单一结构（用户或物品）导致的次优问题。

Method: 提出一个同时利用用户和物品聚类类型的算法，隐变量模型描述用户偏好，并通过信息论下界分析其最优性。

Result: 算法在结合用户和物品结构时表现接近最优，分析表明仅依赖单一结构的传统方法效果次优。

Conclusion: 推荐系统应同时利用用户和物品结构以提高性能，仅依赖单一结构可能导致效果不佳。

Abstract: We consider an online model for recommendation systems, with each user being
recommended an item at each time-step and providing 'like' or 'dislike'
feedback. A latent variable model specifies the user preferences: both users
and items are clustered into types. The model captures structure in both the
item and user spaces, as used by item-item and user-user collaborative
filtering algorithms. We study the situation in which the type preference
matrix has i.i.d. entries. Our main contribution is an algorithm that
simultaneously uses both item and user structures, proved to be near-optimal
via corresponding information-theoretic lower bounds. In particular, our
analysis highlights the sub-optimality of using only one of item or user
structure (as is done in most collaborative filtering algorithms).

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [287] [Sharp higher order convergence rates for the Adam optimizer](https://arxiv.org/abs/2504.19426)
*Steffen Dereich,Arnulf Jentzen,Adrian Riekert*

Main category: math.OC

TL;DR: 论文分析了Adam优化器在深度神经网络训练中的收敛速度，发现其与动量法一样具有更快的收敛速率(√x-1)/(√x+1)，而RMSprop则与标准梯度下降法相同，收敛速率为(x-1)/(x+1)。


<details>
  <summary>Details</summary>
Motivation: 研究不同优化方法（如Adam、RMSprop）在训练深度神经网络时的收敛速度，为优化算法选择提供理论依据。

Method: 通过理论分析，对比标准梯度下降、动量法、Adam和RMSprop在局部极小值附近的收敛速率。

Result: Adam优化器收敛速率与动量法相同，(√x-1)/(√x+1)，优于RMSprop和标准梯度下降法（(x-1)/(x+1)）。

Conclusion: Adam优化器在收敛速度上表现更优，验证了其在复杂优化问题中的高效性。

Abstract: Gradient descent based optimization methods are the methods of choice to
train deep neural networks in machine learning. Beyond the standard gradient
descent method, also suitable modified variants of standard gradient descent
involving acceleration techniques such as the momentum method and/or adaptivity
techniques such as the RMSprop method are frequently considered optimization
methods. These days the most popular of such sophisticated optimization schemes
is presumably the Adam optimizer that has been proposed in 2014 by Kingma and
Ba. A highly relevant topic of research is to investigate the speed of
convergence of such optimization methods. In particular, in 1964 Polyak showed
that the standard gradient descent method converges in a neighborhood of a
strict local minimizer with rate (x - 1)(x + 1)^{-1} while momentum achieves
the (optimal) strictly faster convergence rate (\sqrt{x} - 1)(\sqrt{x} +
1)^{-1} where x \in (1,\infty) is the condition number (the ratio of the
largest and the smallest eigenvalue) of the Hessian of the objective function
at the local minimizer. It is the key contribution of this work to reveal that
Adam also converges with the strictly faster convergence rate (\sqrt{x} -
1)(\sqrt{x} + 1)^{-1} while RMSprop only converges with the convergence rate (x
- 1)(x + 1)^{-1}.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [288] [Towards Robust Multimodal Physiological Foundation Models: Handling Arbitrary Missing Modalities](https://arxiv.org/abs/2504.19596)
*Xi Fu,Wei-Bang Jiang,Yi Ding,Cuntai Guan*

Main category: eess.SP

TL;DR: PhysioOmni是一种基础模型，用于多模态生理信号分析，通过学习通用表示解决了现有方法在跨数据集泛化和缺失模态处理上的不足，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以学习通用表示且无法处理推理时的缺失模态，PhysioOmni旨在解决这些问题。

Method: 提出PhysioOmni模型，通过解耦多模态信号并预训练模态不变和模态特定的目标，使用原型对齐对下游任务进行微调。

Result: 在情感识别、睡眠阶段分类等四个任务中达到最先进性能，且对缺失模态具有强鲁棒性。

Conclusion: PhysioOmni在多模态生理信号分析中表现出色，解决了通用表示和缺失模态问题，具备广泛适用性。

Abstract: Multimodal physiological signals, such as EEG, ECG, EOG, and EMG, are crucial
for healthcare and brain-computer interfaces. While existing methods rely on
specialized architectures and dataset-specific fusion strategies, they struggle
to learn universal representations that generalize across datasets and handle
missing modalities at inference time. To address these issues, we propose
PhysioOmni, a foundation model for multimodal physiological signal analysis
that models both homogeneous and heterogeneous features to decouple multimodal
signals and extract generic representations while maintaining compatibility
with arbitrary missing modalities. PhysioOmni trains a decoupled multimodal
tokenizer, enabling masked signal pre-training via modality-invariant and
modality-specific objectives. To ensure adaptability to diverse and incomplete
modality combinations, the pre-trained encoders undergo resilient fine-tuning
with prototype alignment on downstream datasets. Extensive experiments on four
downstream tasks, emotion recognition, sleep stage classification, motor
prediction, and mental workload detection, demonstrate that PhysioOmni achieves
state-of-the-art performance while maintaining strong robustness to missing
modalities. Our code and model weights will be released.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [289] [The Big Send-off: High Performance Collectives on GPU-based Supercomputers](https://arxiv.org/abs/2504.18658)
*Siddharth Singh,Mahua Singh,Abhinav Bhatele*

Main category: cs.DC

TL;DR: 论文提出PCCL通信库，针对GPU超级计算机上的大规模语言模型训练优化集体通信，性能显著优于RCCL和Cray-MPICH。


<details>
  <summary>Details</summary>
Motivation: 现有库如RCCL和Cray-MPICH在系统资源利用和扩展性上存在严重不足，无法满足大规模深度学习需求。

Method: 设计并实现PCCL库，针对all-gather和reduce-scatter操作进行深度优化，最大化网络和计算资源利用率。

Result: PCCL在2048 GCDs系统上比RCCL快6-33倍，比Cray-MPICH快28-70倍；在GPT-3训练中提速60%和40%。

Conclusion: PCCL显著提升大规模LLM训练效率，是分布式深度学习通信的优化解决方案。

Abstract: We evaluate the current state of collective communication on GPU-based
supercomputers for large language model (LLM) training at scale. Existing
libraries such as RCCL and Cray-MPICH exhibit critical limitations on systems
such as Frontier -- Cray-MPICH underutilizes network and compute resources,
while RCCL suffers from severe scalability issues. To address these challenges,
we introduce PCCL, a communication library with highly optimized
implementations of all-gather and reduce-scatter operations tailored for
distributed deep learning workloads. PCCL is designed to maximally utilize all
available network and compute resources and to scale efficiently to thousands
of GPUs. It achieves substantial performance improvements, delivering 6-33x
speedups over RCCL and 28-70x over Cray-MPICH for all-gather on 2048 GCDs of
Frontier. These gains translate directly to end-to-end performance: in
large-scale GPT-3-style training, PCCL provides up to 60% and 40% speedups over
RCCL for 7B and 13B parameter models, respectively.

</details>


### [290] [FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation](https://arxiv.org/abs/2504.19519)
*Ke Hong,Xiuhong Li,Minxu Liu,Qiuli Mao,Tianqi Wu,Zixiao Huang,Lufang Chen,Zhong Wang,Yichong Zhang,Zhenhua Zhu,Guohao Dai,Yu Wang*

Main category: cs.DC

TL;DR: 论文提出FlashOverlap，一种轻量级设计，通过分块重叠、无干扰计算和通信无关性优化多GPU系统中的通信瓶颈，实验显示性能提升1.65倍。


<details>
  <summary>Details</summary>
Motivation: 多GPU计算中的通信瓶颈，尤其在消费级GPU上，现有设计无法同时满足分块重叠、无干扰计算和通信无关性，导致效率不足。

Method: FlashOverlap采用新型信号机制识别分块数据依赖，不中断计算过程，并重新排列数据至连续地址，通过调用NCCL API实现通信。

Result: 实验表明该设计最高实现1.65倍加速，优于现有多数方案。

Conclusion: FlashOverlap通过轻量级设计有效解决了多GPU通信瓶颈，显著提升性能。

Abstract: Generative models have achieved remarkable success across various
applications, driving the demand for multi-GPU computing. Inter-GPU
communication becomes a bottleneck in multi-GPU computing systems, particularly
on consumer-grade GPUs. By exploiting concurrent hardware execution,
overlapping computation and communication latency is an effective technique for
mitigating the communication overhead. We identify that an efficient and
adaptable overlapping design should satisfy (1) tile-wise overlapping to
maximize the overlapping opportunity, (2) interference-free computation to
maintain the original computational performance, and (3) communication
agnosticism to reduce the development burden against varying communication
primitives. Nevertheless, current designs fail to simultaneously optimize for
all of those features.
  To address the issue, we propose FlashOverlap, a lightweight design
characterized by tile-wise overlapping, interference-free computation, and
communication agnosticism. FlashOverlap utilizes a novel signaling mechanism to
identify tile-wise data dependency without interrupting the computation
process, and reorders data to contiguous addresses, enabling communication by
simply calling NCCL APIs. Experiments show that such a lightweight design
achieves up to 1.65x speedup, outperforming existing works in most cases.

</details>


### [291] [UnifyFL: Enabling Decentralized Cross-Silo Federated Learning](https://arxiv.org/abs/2504.18916)
*Sarang S,Druva Dhakshinamoorthy,Aditya Shiva Sharma,Yuvraj Singh Bhadauria,Siddharth Chaitra Vivek,Arihant Bansal,Arnab K. Paul*

Main category: cs.DC

TL;DR: 论文提出了一个名为\proj的基于信任的跨机构联邦学习框架，通过去中心化编排和分布式存储，平衡了信任与资源效率，性能接近理想的多级集中式联邦学习。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在隐私保护下的机器学习决策中表现优异，但组织间缺乏有效协作机制，面临信任与资源效率的权衡问题。现有方法（如依赖第三方聚合器或直接共享模型）均存在不足，亟需一种更优解决方案。

Method: 提出\proj框架，采用去中心化编排和分布式存储，支持同步和异步模式以处理延迟节点，确保灵活性与资源高效利用。

Result: 在多样化测试平台上，\proj性能接近理想的多级集中式联邦学习，同时实现了信任与资源的最优平衡。

Conclusion: \proj为跨机构联邦学习提供了一种兼顾信任、资源效率和性能的可行方案，解决了现有方法的局限性。

Abstract: Federated Learning (FL) is a decentralized machine learning (ML) paradigm in
which models are trained on private data across several devices called clients
and combined at a single node called an aggregator rather than aggregating the
data itself. Many organizations employ FL to have better privacy-aware
ML-driven decision-making capabilities. However, organizations often operate
independently rather than collaborate to enhance their FL capabilities due to
the lack of an effective mechanism for collaboration. The challenge lies in
balancing trust and resource efficiency. One approach relies on trusting a
third-party aggregator to consolidate models from all organizations (multilevel
FL), but this requires trusting an entity that may be biased or unreliable.
Alternatively, organizations can bypass a third party by sharing their local
models directly, which requires significant computational resources for
validation. Both approaches reflect a fundamental trade-off between trust and
resource constraints, with neither offering an ideal solution. In this work, we
develop a trust-based cross-silo FL framework called \proj, which uses
decentralized orchestration and distributed storage. \proj provides flexibility
to the participating organizations and presents synchronous and asynchronous
modes to handle stragglers. Our evaluation on a diverse testbed shows that
\proj achieves a performance comparable to the ideal multilevel centralized FL
while allowing trust and optimal use of resources.

</details>


### [292] [Accelerating Mixture-of-Experts Training with Adaptive Expert Replication](https://arxiv.org/abs/2504.19925)
*Athinagoras Skiadopoulos,Mark Zhao,Swapnil Gandhi,Thomas Norrie,Shrijeet Mukherjee,Christos Kozyrakis*

Main category: cs.DC

TL;DR: SwiftMoE 是一种自适应 MoE 训练系统，通过解耦专家参数和优化器状态的分区，减少迁移开销并提高训练效率，比其他先进系统快 25.9-30.5%。


<details>
  <summary>Details</summary>
Motivation: 现有的 MoE 训练系统在负载不均时，要么牺牲收敛性丢弃热门专家的令牌，要么频繁重新分配资源导致高迁移开销。

Method: SwiftMoE 通过静态分区优化器状态、动态调整专家参数布局，利用现有权重更新避免迁移开销，实现高效资源分配。

Result: SwiftMoE 在收敛时间上比 DeepSpeed 和 FlexMoE 分别快 30.5% 和 25.9%。

Conclusion: SwiftMoE 突破了性能与准确性的权衡，通过创新设计显著提升了 MoE 训练效率。

Abstract: Mixture-of-Experts (MoE) models have become a widely adopted solution to
continue scaling model sizes without a corresponding linear increase in
compute. During MoE model training, each input token is dynamically routed to a
subset of experts -- sparsely-activated feed-forward networks -- within each
transformer layer. The distribution of tokens assigned to each expert varies
widely and rapidly over the course of training. To handle the wide load
imbalance across experts, current systems are forced to either drop tokens
assigned to popular experts, degrading convergence, or frequently rebalance
resources allocated to each expert based on popularity, incurring high state
migration overheads.
  To break this performance-accuracy tradeoff, we introduce SwiftMoE, an
adaptive MoE training system. The key insight of SwiftMoE is to decouple the
placement of expert parameters from their large optimizer state. SwiftMoE
statically partitions the optimizer of each expert across all training nodes.
Meanwhile, SwiftMoE dynamically adjusts the placement of expert parameters by
repurposing existing weight updates, avoiding migration overheads. In doing so,
SwiftMoE right-sizes the GPU resources allocated to each expert, on a
per-iteration basis, with minimal overheads. Compared to state-of-the-art MoE
training systems, DeepSpeed and FlexMoE, SwiftMoE is able to achieve a 30.5%
and 25.9% faster time-to-convergence, respectively.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [293] [Modeling Regime Structure and Informational Drivers of Stock Market Volatility via the Financial Chaos Index](https://arxiv.org/abs/2504.18958)
*Masoud Ataei*

Main category: q-fin.ST

TL;DR: 该论文通过金融混沌指数（FCIX）研究了股票市场波动的结构动态，使用基于张量和特征值的方法捕捉资产价格的相互波动。研究发现三种不同的市场状态（低混沌、中混沌和高混沌），并通过弹性网络回归模型预测了VIX指数的隐含波动率，表明宏观经济、金融、政策和地缘政治不确定性对波动率动态有强预测力。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于金融危机的经验证据，显示波动行为具有状态依赖性，且在危机期间存在感知时间扩张现象。为理解这一现象并捕捉系统性不确定性对市场的影响，作者开发了基于修正对数正态幂律分布的状态转换框架。

Method: 方法包括：（1）使用金融混沌指数（FCIX）测量已实现波动率；（2）基于修正对数正态幂律分布的状态转换框架划分市场状态；（3）利用弹性网络回归模型分析情绪预测因子对VIX指数的预测能力。

Result: 结果显示三种市场状态（低、中、高混沌），每种状态的系统性压力、统计分散性和持续性特征不同。情绪预测因子（如宏观经济和政策不确定性）对隐含波动率（VIX）的预测能力显著。

Conclusion: 研究结论表明系统性不确定性不仅影响金融市场的已实现波动，还通过隐含波动率反映市场的前瞻性行为，提供了统一的实证视角。

Abstract: This paper investigates the structural dynamics of stock market volatility
through the Financial Chaos Index, a tensor- and eigenvalue-based measure
designed to capture realized volatility via mutual fluctuations among asset
prices. Motivated by empirical evidence of regime-dependent volatility behavior
and perceptual time dilation during financial crises, we develop a
regime-switching framework based on the Modified Lognormal Power-Law
distribution. Analysis of the FCIX from January 1990 to December 2023
identifies three distinct market regimes, low-chaos, intermediate-chaos, and
high-chaos, each characterized by differing levels of systemic stress,
statistical dispersion and persistence characteristics. Building upon the
segmented regime structure, we further examine the informational forces that
shape forward-looking market expectations. Using sentiment-based predictors
derived from the Equity Market Volatility tracker, we employ an elastic net
regression model to forecast implied volatility, as proxied by the VIX index.
Our findings indicate that shifts in macroeconomic, financial, policy, and
geopolitical uncertainty exhibit strong predictive power for volatility
dynamics across regimes. Together, these results offer a unified empirical
perspective on how systemic uncertainty governs both the realized evolution of
financial markets and the anticipatory behavior embedded in implied volatility
measures.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [294] [Technical Challenges in Maintaining Tax Prep Software with Large Language Models](https://arxiv.org/abs/2504.18693)
*Sina Gogani-Khiabani,Varsha Dewangan,Nina Olson,Ashutosh Trivedi,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: 论文探讨了利用大型语言模型（如ChatGPT和Llama）从IRS出版物中自动提取代码差异并整合到现有税务准备软件中，以实现自动化维护的技术挑战和方法。


<details>
  <summary>Details</summary>
Motivation: 随着美国税法的频繁变动，税务准备软件的维护变得耗时且易错，需要结合人工代码分析和税法专家解读。论文旨在通过自动化解决这一问题。

Method: 研究聚焦于利用大型语言模型（LLMs）从IRS出版物中准确提取代码变更，并自动将其整合到现有软件代码中。

Result: 论文提出了通过LLMs实现税法变更到代码的自动翻译和整合的可行性，但未具体说明实验结果。

Conclusion: 自动化税务软件维护是可行的，且LLMs在这一过程中具有潜力，但仍需解决技术挑战。

Abstract: As the US tax law evolves to adapt to ever-changing politico-economic
realities, tax preparation software plays a significant role in helping
taxpayers navigate these complexities. The dynamic nature of tax regulations
poses a significant challenge to accurately and timely maintaining tax software
artifacts. The state-of-the-art in maintaining tax prep software is
time-consuming and error-prone as it involves manual code analysis combined
with an expert interpretation of tax law amendments. We posit that the rigor
and formality of tax amendment language, as expressed in IRS publications,
makes it amenable to automatic translation to executable specifications (code).
Our research efforts focus on identifying, understanding, and tackling
technical challenges in leveraging Large Language Models (LLMs), such as
ChatGPT and Llama, to faithfully extract code differentials from IRS
publications and automatically integrate them with the prior version of the
code to automate tax prep software maintenance.

</details>


### [295] [Large Language Models are Qualified Benchmark Builders: Rebuilding Pre-Training Datasets for Advancing Code Intelligence Tasks](https://arxiv.org/abs/2504.19444)
*Kang Yang,Xinjun Mao,Shangwen Wang,Yanlin Wang,Tanghaoran Zhang,Bo Lin,Yihao Qin,Zhang Zhang,Yao Lu,Kamal Al-Sabahi*

Main category: cs.SE

TL;DR: 论文探讨用LLM生成的代码注释替代人工注释，提升预训练数据集质量。通过新提出的无参考评估任务验证LLM注释的语义一致性更高，并基于此重建数据集，实验证明模型性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 人工编写的代码注释容易过时，影响预训练模型性能，而LLM生成的注释质量更高。研究旨在验证是否能用LLM生成的注释改进预训练数据。

Method: 提出两种无参考评估任务（代码-注释不一致性检测和语义代码搜索），用LLM生成的注释重建CodeSearchNet数据集，并重新预训练CodeT5模型。

Result: LLM生成的注释比人工注释与代码的语义一致性更高，基于LLM注释预训练的模型在代码摘要、生成和翻译任务中表现更优。

Conclusion: 通过LLM生成的注释重建预训练数据集能有效提升代码智能模型的性能，挑战了传统依赖人工注释的范式。

Abstract: Pre-trained code models rely heavily on high-quality pre-training data,
particularly human-written reference comments that bridge code and natural
language. However, these comments often become outdated as software evolves,
degrading model performance. Large language models (LLMs) excel at generating
high-quality code comments. We investigate whether replacing human-written
comments with LLM-generated ones improves pre-training datasets. Since standard
metrics cannot assess reference comment quality, we propose two novel
reference-free evaluation tasks: code-comment inconsistency detection and
semantic code search. Results show that LLM-generated comments are more
semantically consistent with code than human-written ones, as confirmed by
manual evaluation. Leveraging this finding, we rebuild the CodeSearchNet
dataset with LLM-generated comments and re-pre-train CodeT5. Evaluations
demonstrate that models trained on LLM-enhanced data outperform those using
original human comments in code summarization, generation, and translation
tasks. This work validates rebuilding pre-training datasets with LLMs to
advance code intelligence, challenging the traditional reliance on human
reference comments.

</details>


### [296] [Can We Enhance Bug Report Quality Using LLMs?: An Empirical Study of LLM-Based Bug Report Generation](https://arxiv.org/abs/2504.18804)
*Jagrit Acharya,Gouri Ginde*

Main category: cs.SE

TL;DR: 该研究探讨了指令微调的大语言模型（LLMs）能否将非结构化的错误报告自动转化为高质量的结构化报告，结果显示Qwen 2.5表现最佳，尤其在重现步骤方面，且方法在不同项目中泛化能力良好。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化错误报告导致的问题（如延迟和额外手动工作量），探索LLMs在自动化生成结构化报告中的潜力。

Method: 评估三种开源指令微调LLMs（Qwen 2.5、Mistral、Llama 3.2）和ChatGPT-4o，使用CTQRS、ROUGE等指标衡量性能，并在多项目中测试泛化能力。

Result: Qwen 2.5的CTQRS得分77%，优于其他模型；Llama 3.2在检测缺失字段（如“预期行为”）上更准确，Qwen 2.5在“重现步骤”上F1分数76%。方法在未见项目中也表现良好（CTQRS达70%）。

Conclusion: 指令微调在自动化生成结构化错误报告中具有潜力，可减少开发者手动工作，优化软件维护流程。

Abstract: Bug reports contain the information developers need to triage and fix
software bugs. However, unclear, incomplete, or ambiguous information may lead
to delays and excessive manual effort spent on bug triage and resolution. In
this paper, we explore whether Instruction fine-tuned Large Language Models
(LLMs) can automatically transform casual, unstructured bug reports into
high-quality, structured bug reports adhering to a standard template. We
evaluate three open-source instruction-tuned LLMs (\emph{Qwen 2.5, Mistral, and
Llama 3.2}) against ChatGPT-4o, measuring performance on established metrics
such as CTQRS, ROUGE, METEOR, and SBERT. Our experiments show that fine-tuned
Qwen 2.5 achieves a CTQRS score of \textbf{77%}, outperforming both fine-tuned
Mistral (\textbf{71%}), Llama 3.2 (\textbf{63%}) and ChatGPT in 3-shot learning
(\textbf{75%}). Further analysis reveals that Llama 3.2 shows higher accuracy
of detecting missing fields particularly Expected Behavior and Actual Behavior,
while Qwen 2.5 demonstrates superior performance in capturing
Steps-to-Reproduce, with an F1 score of 76%. Additional testing of the models
on other popular projects (e.g., Eclipse, GCC) demonstrates that our approach
generalizes well, achieving up to \textbf{70%} CTQRS in unseen projects' bug
reports. These findings highlight the potential of instruction fine-tuning in
automating structured bug report generation, reducing manual effort for
developers and streamlining the software maintenance process.

</details>


### [297] [Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial Attacks Using LLM-as-a-Judge](https://arxiv.org/abs/2504.19730)
*Wenhan Mu,Ling Xu,Shuren Pei,Le Mi,Huichi Zhou*

Main category: cs.SE

TL;DR: EP-Shield框架通过自然感知推理评估并净化标识符替换攻击，相比现有方法（如ALERT），能检测80%以上的对抗样本，并在轻量设计下实现GPT-4级性能。


<details>
  <summary>Details</summary>
Motivation: 现有标识符替换攻击生成的对抗样本存在不自然代码模式，容易被检测，亟需一种能评估并净化这些攻击的方法。

Method: 提出EP-Shield框架：1) 评估代码自然性并识别对抗代码；2) 净化对抗样本以使受害者模型恢复正确预测。

Result: 实验表明EP-Shield优于对抗微调（提升达83.36%），且轻量设计（7B参数）性能达GPT-4级别。

Conclusion: EP-Shield有效提升对抗样本的自然性和检测能力，为代码语言模型安全提供新解决方案。

Abstract: The widespread adoption of code language models in software engineering tasks
has exposed vulnerabilities to adversarial attacks, especially the identifier
substitution attacks. Although existing identifier substitution attackers
demonstrate high success rates, they often produce adversarial examples with
unnatural code patterns. In this paper, we systematically assess the quality of
adversarial examples using LLM-as-a-Judge. Our analysis reveals that over 80%
of adversarial examples generated by state-of-the-art identifier substitution
attackers (e.g., ALERT) are actually detectable. Based on this insight, we
propose EP-Shield, a unified framework for evaluating and purifying identifier
substitution attacks via naturalness-aware reasoning. Specifically, we first
evaluate the naturalness of code and identify the perturbed adversarial code,
then purify it so that the victim model can restore correct prediction.
Extensive experiments demonstrate the superiority of EP-Shield over adversarial
fine-tuning (up to 83.36% improvement) and its lightweight design 7B
parameters) with GPT-4-level performance.

</details>


### [298] [Test It Before You Trust It: Applying Software Testing for Trustworthy In-context Learning](https://arxiv.org/abs/2504.18827)
*Teeradaj Racharak,Chaiyong Ragkhitwetsagul,Chommakorn Sontesadisai,Thanwadee Sunetnanta*

Main category: cs.SE

TL;DR: MMT4NL是一个基于软件测试原则的框架，用于评估大语言模型（LLMs）上下文学习（ICL）的可信度，通过对抗性扰动和软件测试技术来发现模型在语言变体中的漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在上下文学习中表现出强大的适应能力，但其在面对细微对抗性扰动或语言变体时仍表现出不可预测的行为，需要一种系统化方法来评估其可信度。

Method: MMT4NL利用对抗性扰动和软件测试技术，通过构建变形对抗样本，从多样化语言能力角度测试LLMs的ICL能力。

Result: 实验在情感分析和问答任务中应用MMT4NL，成功揭示了当前先进LLMs中的多种语言漏洞。

Conclusion: MMT4NL为评估和改进LLMs的ICL能力提供了有效工具，凸显了软件测试方法在验证模型功能中的重要性。

Abstract: In-context learning (ICL) has emerged as a powerful capability of large
language models (LLMs), enabling them to perform new tasks based on a few
provided examples without explicit fine-tuning. Despite their impressive
adaptability, these models remain vulnerable to subtle adversarial
perturbations and exhibit unpredictable behavior when faced with linguistic
variations. Inspired by software testing principles, we introduce a software
testing-inspired framework, called MMT4NL, for evaluating the trustworthiness
of in-context learning by utilizing adversarial perturbations and software
testing techniques. It includes diverse evaluation aspects of linguistic
capabilities for testing the ICL capabilities of LLMs. MMT4NL is built around
the idea of crafting metamorphic adversarial examples from a test set in order
to quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is
to treat any LLM as software and validate its functionalities just like testing
the software. Finally, we demonstrate applications of MMT4NL on the sentiment
analysis and question-answering tasks. Our experiments could reveal various
linguistic bugs in state-of-the-art LLMs.

</details>


### [299] [Why you shouldn't fully trust ChatGPT: A synthesis of this AI tool's error rates across disciplines and the software engineering lifecycle](https://arxiv.org/abs/2504.18858)
*Vahid Garousi*

Main category: cs.SE

TL;DR: 该研究通过多视角文献综述（MLR）量化了ChatGPT在不同领域和软件开发生命周期（SDLC）中的错误率，发现错误率因领域、任务和模型版本而异，升级模型可提升可靠性，但仍需人工监督。


<details>
  <summary>Details</summary>
Motivation: 尽管ChatGPT等大语言模型（LLM）在多个领域广泛应用，但其可靠性问题（尤其是错误率）仍备受关注。本研究旨在系统评估其在不同领域和SDLC阶段的表现，为实际应用提供依据。

Method: 采用多视角文献综述（MLR）方法，收集截至2025年的学术研究、报告、基准测试及灰色文献数据，按领域和SDLC阶段分类，并通过箱线图可视化错误率分布。

Result: 结果显示，错误率因领域和模型版本差异显著：医疗领域8%-83%，商业/经济学领域从GPT-3.5的50%降至GPT-4的15-20%。软件工程中，需求/设计阶段错误率较低（5-20%），而编码/测试阶段较高（10-50%）。模型升级（如GPT-4）显著提升可靠性。

Conclusion: 尽管模型改进降低了错误率，但在关键领域或任务中完全依赖ChatGPT仍存在风险，需持续评估和人工验证以确保可靠性。

Abstract: Context: ChatGPT and other large language models (LLMs) are widely used
across healthcare, business, economics, engineering, and software engineering
(SE). Despite their popularity, concerns persist about their reliability,
especially their error rates across domains and the software development
lifecycle (SDLC).
  Objective: This study synthesizes and quantifies ChatGPT's reported error
rates across major domains and SE tasks aligned with SDLC phases. It provides
an evidence-based view of where ChatGPT excels, where it fails, and how
reliability varies by task, domain, and model version (GPT-3.5, GPT-4,
GPT-4-turbo, GPT-4o).
  Method: A Multivocal Literature Review (MLR) was conducted, gathering data
from academic studies, reports, benchmarks, and grey literature up to 2025.
Factual, reasoning, coding, and interpretive errors were considered. Data were
grouped by domain and SE phase and visualized using boxplots to show error
distributions.
  Results: Error rates vary across domains and versions. In healthcare, rates
ranged from 8% to 83%. Business and economics saw error rates drop from ~50%
with GPT-3.5 to 15-20% with GPT-4. Engineering tasks averaged 20-30%.
Programming success reached 87.5%, though complex debugging still showed over
50% errors. In SE, requirements and design phases showed lower error rates
(~5-20%), while coding, testing, and maintenance phases had higher variability
(10-50%). Upgrades from GPT-3.5 to GPT-4 improved reliability.
  Conclusion: Despite improvements, ChatGPT still exhibits non-negligible error
rates varying by domain, task, and SDLC phase. Full reliance without human
oversight remains risky, especially in critical settings. Continuous evaluation
and critical validation are essential to ensure reliability and
trustworthiness.

</details>


### [300] [VeriDebug: A Unified LLM for Verilog Debugging via Contrastive Embedding and Guided Correction](https://arxiv.org/abs/2504.19099)
*Ning Wang,Bingkun Yao,Jie Zhou,Yuchen Hu,Xi Wang,Nan Guan,Zhe Jiang*

Main category: cs.SE

TL;DR: VeriDebug是一种集成对比表示和引导修正的自动化Verilog调试方法，显著提升调试准确率，优于现有开源和闭源模型。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在Verilog调试中的应用，弥补现有方法的不足。

Method: 采用基于嵌入的技术检索内部信息并结合引导修正，统一检测与修复。

Result: VeriDebugLoc模型在修复准确率（Acc1）达到64.7，远超开源和GPT-3.5-turbo。

Conclusion: VeriDebug为Verilog调试提供了更高效的自动化解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable potential in
debugging for various programming languages. However, the application of LLMs
to Verilog debugging remains insufficiently explored. Here, we present
VeriDebug, an approach that integrates contrastive representation and guided
correction capabilities for automated Verilog debugging. Unlike existing
methods, VeriDebug employs an embedding-based technique to accurately retrieve
internal information, followed by bug-fixing. VeriDebug unifies Verilog bug
detection and correction through a shared parameter space. By simultaneously
learning bug patterns and fixes, it streamlines debugging via contrastive
embedding and guided correction. Empirical results show the efficacy of
VeriDebug in enhancing Verilog debugging. Our VeriDebugLoc, Type model achieves
64.7 accuracy in bug fixing (Acc1), a significant improvement from the existing
open-source SOTAs 11.3. This performance not only outperforms open-source
alternatives but also exceeds larger closed-source models like GPT-3.5-turbo
(36.6), offering a more accurate alternative to conventional debugging methods.

</details>


### [301] [From Inductive to Deductive: LLMs-Based Qualitative Data Analysis in Requirements Engineering](https://arxiv.org/abs/2504.19384)
*Syed Tauhid Ullah Shah,Mohamad Hussein,Ann Barcomb,Mohammad Moshirpour*

Main category: cs.SE

TL;DR: 论文探讨了在需求工程中使用大语言模型（如GPT-4、Mistral和LLaMA-2）改进定性数据分析（QDA）任务，发现GPT-4在演绎标注任务中表现优异，显著降低了人工工作量。


<details>
  <summary>Details</summary>
Motivation: 传统的QDA方法耗时且依赖人工，难以高效处理复杂软件项目的自由形式数据，因此探索利用大语言模型提升QDA效率和质量。

Method: 评估了LLMs在归纳（零样本）和演绎（一样本、多样本）标注任务中的表现，重点关注GPT-4的准确性和一致性，并分析上下文丰富的提示对性能的影响。

Result: GPT-4在演绎标注任务中与人类分析师的一致性较高（Cohen's Kappa > 0.7），而零样本表现较弱；详细的提示显著提升了标注准确性和一致性。

Conclusion: LLMs（尤其是GPT-4）能够有效减少QDA的人工工作量，同时保持标注质量，并为需求跟踪和领域模型设计提供结构化的标签支持。

Abstract: Requirements Engineering (RE) is essential for developing complex and
regulated software projects. Given the challenges in transforming stakeholder
inputs into consistent software designs, Qualitative Data Analysis (QDA)
provides a systematic approach to handling free-form data. However, traditional
QDA methods are time-consuming and heavily reliant on manual effort. In this
paper, we explore the use of Large Language Models (LLMs), including GPT-4,
Mistral, and LLaMA-2, to improve QDA tasks in RE. Our study evaluates LLMs'
performance in inductive (zero-shot) and deductive (one-shot, few-shot)
annotation tasks, revealing that GPT-4 achieves substantial agreement with
human analysts in deductive settings, with Cohen's Kappa scores exceeding 0.7,
while zero-shot performance remains limited. Detailed, context-rich prompts
significantly improve annotation accuracy and consistency, particularly in
deductive scenarios, and GPT-4 demonstrates high reliability across repeated
runs. These findings highlight the potential of LLMs to support QDA in RE by
reducing manual effort while maintaining annotation quality. The structured
labels automatically provide traceability of requirements and can be directly
utilized as classes in domain models, facilitating systematic software design.

</details>


### [302] [LLMs for Engineering: Teaching Models to Design High Powered Rockets](https://arxiv.org/abs/2504.19394)
*Toby Simonds*

Main category: cs.SE

TL;DR: 论文评估了大型语言模型（LLMs）在火箭设计中的能力，发现基础LLMs表现有限，但通过强化学习（RL）增强后可以超越人类专家和SoTA模型。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在物理工程领域的应用潜力，填补其在火箭设计领域的空白。

Method: 使用RocketBench基准测试连接LLMs和高保真火箭模拟，测试目标高度优化和精准着陆任务，并引入强化学习（RL）增强模型。

Result: 基础LLMs虽具备工程知识但难以迭代优化，RL增强后的7B参数模型优于人类专家和其他SoTA模型。

Conclusion: RL训练的LLMs可作为复杂工程优化的有效工具，有望拓展至软件开发以外的工程领域。

Abstract: Large Language Models (LLMs) have transformed software engineering, but their
application to physical engineering domains remains underexplored. This paper
evaluates LLMs' capabilities in high-powered rocketry design through
RocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations.
We test models on two increasingly complex design tasks: target altitude
optimization and precision landing challenges. Our findings reveal that while
state-of-the-art LLMs demonstrate strong baseline engineering knowledge, they
struggle to iterate on their designs when given simulation results and
ultimately plateau below human performance levels. However, when enhanced with
reinforcement learning (RL), we show that a 7B parameter model outperforms both
SoTA foundation models and human experts. This research demonstrates that
RL-trained LLMs can serve as effective tools for complex engineering
optimization, potentially transforming engineering domains beyond software
development.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [303] [The Philosophic Turn for AI Agents: Replacing centralized digital rhetoric with decentralized truth-seeking](https://arxiv.org/abs/2504.18601)
*Philipp Koralus*

Main category: cs.CY

TL;DR: AI决策支持系统可能威胁人类自主权，需要通过哲学化的AI设计（如苏格拉底对话法）来增强用户判断力，而非削弱其自主性。


<details>
  <summary>Details</summary>
Motivation: 面对AI技术的快速发展，如何在提升决策效率的同时保护人类自主权成为关键问题。

Method: 提出哲学化的AI设计理念，采用类似苏格拉底对话的方法，促进分散式真相探索和开放式探究。

Result: 这种设计能增强用户的判断力和自主性，避免“助推”框架对自主权的侵蚀。

Conclusion: 通过构建支持自主学习的AI系统，可以在提升人类决策能力的同时保护自主权。

Abstract: In the face of rapidly advancing AI technology, individuals will increasingly
rely on AI agents to navigate life's growing complexities, raising critical
concerns about maintaining both human agency and autonomy. This paper addresses
a fundamental dilemma posed by AI decision-support systems: the risk of either
becoming overwhelmed by complex decisions, thus losing agency, or having
autonomy compromised by externally controlled choice architectures reminiscent
of ``nudging'' practices. While the ``nudge'' framework, based on the use of
choice-framing to guide individuals toward presumed beneficial outcomes,
initially appeared to preserve liberty, at AI-driven scale, it threatens to
erode autonomy. To counteract this risk, the paper proposes a philosophic turn
in AI design. AI should be constructed to facilitate decentralized
truth-seeking and open-ended inquiry, mirroring the Socratic method of
philosophical dialogue. By promoting individual and collective adaptive
learning, such AI systems would empower users to maintain control over their
judgments, augmenting their agency without undermining autonomy. The paper
concludes by outlining essential features for autonomy-preserving AI systems,
sketching a path toward AI systems that enhance human judgment rather than
undermine it.

</details>


### [304] [Toward Personalizing Quantum Computing Education: An Evolutionary LLM-Powered Approach](https://arxiv.org/abs/2504.18603)
*Iizalaarab Elhaimeur,Nikos Chrisochoides*

Main category: cs.CY

TL;DR: 该论文提出了一种用于量子计算教育的智能教学助手系统，通过结合知识图谱增强架构和两个大型语言模型（LLM）代理，动态适应学生需求，初步结果显示其潜力，但需系统性评估。


<details>
  <summary>Details</summary>
Motivation: 量子计算教育因复杂性和现有工具的局限性面临挑战，作者希望设计一种智能教学助手，通过知识图谱和LLM代理动态适应学生需求，提升教学效果。

Method: 系统采用知识图谱增强架构，包含两个LLM代理：教学代理（动态交互）和课程规划代理（生成教案），并通过中央知识图谱协调任务，引入用户标签系统减少LLM幻觉。

Result: 初步结果显示，系统能捕获丰富的交互数据，通过标签系统动态调整教案，并通过知识图谱实现情境感知辅导，但需进一步评估。

Conclusion: 该系统展示了在量子计算教育中的潜力，尤其是动态适应和知识图谱的应用，但需更多系统性验证以确认其有效性。

Abstract: Quantum computing education faces significant challenges due to its
complexity and the limitations of current tools; this paper introduces a novel
Intelligent Teaching Assistant for quantum computing education and details its
evolutionary design process. The system combines a knowledge-graph-augmented
architecture with two specialized Large Language Model (LLM) agents: a Teaching
Agent for dynamic interaction, and a Lesson Planning Agent for lesson plan
generation. The system is designed to adapt to individual student needs, with
interactions meticulously tracked and stored in a knowledge graph. This graph
represents student actions, learning resources, and relationships, aiming to
enable reasoning about effective learning pathways. We describe the
implementation of the system, highlighting the challenges encountered and the
solutions implemented, including introducing a dual-agent architecture where
tasks are separated, all coordinated through a central knowledge graph that
maintains system awareness, and a user-facing tag system intended to mitigate
LLM hallucination and improve user control. Preliminary results illustrate the
system's potential to capture rich interaction data, dynamically adapt lesson
plans based on student feedback via a tag system in simulation, and facilitate
context-aware tutoring through the integrated knowledge graph, though
systematic evaluation is required.

</details>


### [305] [Balancing Creativity and Automation: The Influence of AI on Modern Film Production and Dissemination](https://arxiv.org/abs/2504.19275)
*Yiren Xu*

Main category: cs.CY

TL;DR: 研究探讨AI对现代电影的双重影响，提出人类与AI应以AI作为工具而非独立合作者的关系，并强调伦理挑战与监管建议。


<details>
  <summary>Details</summary>
Motivation: 探索AI在电影制作中的高效与创意作用，同时解决其引发的伦理与实践问题。

Method: 结合理论框架（作者论、人机关系）和案例研究（如《安全地带》《速度与激情7》）。

Result: 将AI定位为工具能保护人类创作权；揭示AI市场的监控资本主义风险及深度伪造的伦理困境。

Conclusion: 提出国际监管框架和人类控制指数（HCI），以平衡AI应用与文化伦理的多样性。

Abstract: The integration of Artificial Intelligence(AI) into film production has
revolutionized efficiency and creativity, yet it simultaneously raises critical
ethical and practical challenges. This study explores the dual impact of AI on
modern cinema through three objectives: defining the optimal human-AI
relationship, balancing creativity with automation, and developing ethical
guidelines. By employing a mixed-method approach combining theoretical
frameworks (auteur theory, human-technology relations) and case studies (The
Safe Zone, Fast & Furious 7, The Brutalist), the research reveals that
positioning AI as an "embodiment tool" rather than an independent "alterity
partner" preserves human authorship and artistic integrity. Key findings
highlight the risks of surveillance capitalism in AI-driven markets and the
ethical dilemmas of deepfake technology. The study concludes with actionable
recommendations, including international regulatory frameworks and a Human
Control Index (HCI) to quantify AI involvement. These insights aim to guide
filmmakers, policymakers, and scholars in navigating the evolving AI-cinema
landscape while safeguarding cultural diversity and ethical standards.

</details>


### [306] [Navigating AI Policy Landscapes: Insights into Human Rights Considerations Across IEEE Regions](https://arxiv.org/abs/2504.19264)
*Angel Mary John,Jerrin Thomas Panachakel,Anusha S. P*

Main category: cs.CY

TL;DR: 本文比较了美国、欧洲、中国和新加坡在AI监管框架中纳入人权考量的差异，强调全球对话的必要性。


<details>
  <summary>Details</summary>
Motivation: 探讨不同地区如何将人权纳入AI监管，揭示其差异背后的动机和优先事项。

Method: 通过对IEEE区域（美国、欧洲、中国、新加坡）的AI监管框架进行对比分析。

Result: 欧洲监管严格保护个人权利，美国宽松以促进创新，中国侧重国家控制与社会秩序，新加坡提倡自我监管与国际接轨。

Conclusion: 需全球协作以平衡人权保护与技术进步，尊重各地区的多样性和优先级。

Abstract: This paper explores the integration of human rights considerations into AI
regulatory frameworks across different IEEE regions - specifically the United
States (Region 1-6), Europe (Region 8), China (part of Region 10), and
Singapore (part of Region 10). While all acknowledge the transformative
potential of AI and the necessity of ethical guidelines, their regulatory
approaches significantly differ. Europe exhibits a rigorous framework with
stringent protections for individual rights, while the U.S. promotes innovation
with less restrictive regulations. China emphasizes state control and societal
order in its AI strategies. In contrast, Singapore's advisory framework
encourages self-regulation and aligns closely with international norms. This
comparative analysis underlines the need for ongoing global dialogue to
harmonize AI regulations that safeguard human rights while promoting
technological advancement, reflecting the diverse perspectives and priorities
of each region.

</details>


### [307] [Generative AI in Education: Student Skills and Lecturer Roles](https://arxiv.org/abs/2504.19673)
*Stefanie Krause,Ashish Dalvi,Syed Khubaib Zaidi*

Main category: cs.CY

TL;DR: 该研究探讨了生成式人工智能（GenAI）在教育中的应用及所需学生能力，识别了14项关键技能，并提出了讲师整合GenAI的六大策略。通过文献综述和学生调查，发现AI素养、批判性思维和伦理实践最为重要。


<details>
  <summary>Details</summary>
Motivation: 研究旨在帮助教育者和学生有效应对GenAI在教育中的挑战，重塑教学方式。

Method: 采用混合方法，结合文献综述和对130名欧亚学生的定量调查。

Result: 研究发现AI素养、批判性思维和伦理实践是关键技能；学生在提示工程、偏见意识等方面存在不足；讲师策略中GenAI整合和课程设计最受重视。

Conclusion: 教育机构需推动包容性GenAI应用，确保工具公平获取，明确学术诚信政策，并支持全球研究。

Abstract: Generative Artificial Intelligence (GenAI) tools such as ChatGPT are emerging
as a revolutionary tool in education that brings both positive aspects and
challenges for educators and students, reshaping how learning and teaching are
approached. This study aims to identify and evaluate the key competencies
students need to effectively engage with GenAI in education and to provide
strategies for lecturers to integrate GenAI into teaching practices. The study
applied a mixed method approach with a combination of a literature review and a
quantitative survey involving 130 students from South Asia and Europe to obtain
its findings. The literature review identified 14 essential student skills for
GenAI engagement, with AI literacy, critical thinking, and ethical AI practices
emerging as the most critical. The student survey revealed gaps in prompt
engineering, bias awareness, and AI output management. In our study of lecturer
strategies, we identified six key areas, with GenAI Integration and Curriculum
Design being the most emphasised. Our findings highlight the importance of
incorporating GenAI into education. While literature prioritized ethics and
policy development, students favour hands-on, project-based learning and
practical AI applications. To foster inclusive and responsible GenAI adoption,
institutions should ensure equitable access to GenAI tools, establish clear
academic integrity policies, and advocate for global GenAI research
initiatives.

</details>


### [308] [Mitigating Societal Cognitive Overload in the Age of AI: Challenges and Directions](https://arxiv.org/abs/2504.19990)
*Salem Lahlou*

Main category: cs.CY

TL;DR: 论文探讨了AI时代信息过载对人类福祉和社会韧性的挑战，提出缓解认知过载对应对AI长期风险至关重要。


<details>
  <summary>Details</summary>
Motivation: 在AI时代，信息爆炸和复杂性导致社会认知过载，威胁人类福祉和社会韧性，需重新审视AI安全问题的核心。

Method: 通过分析AI加剧认知过载的机制（如信息泛滥、算法操控等），将AI安全辩论重新聚焦于认知过载问题。

Result: 认知过载是连接AI短期危害与长期风险的桥梁，需从制度、研究和政策角度提升过载韧性。

Conclusion: 未来需探索更具适应性的制度设计和政策框架，而非提供明确解决方案，以应对AI与人类对齐的挑战。

Abstract: Societal cognitive overload, driven by the deluge of information and
complexity in the AI age, poses a critical challenge to human well-being and
societal resilience. This paper argues that mitigating cognitive overload is
not only essential for improving present-day life but also a crucial
prerequisite for navigating the potential risks of advanced AI, including
existential threats. We examine how AI exacerbates cognitive overload through
various mechanisms, including information proliferation, algorithmic
manipulation, automation anxieties, deregulation, and the erosion of meaning.
The paper reframes the AI safety debate to center on cognitive overload,
highlighting its role as a bridge between near-term harms and long-term risks.
It concludes by discussing potential institutional adaptations, research
directions, and policy considerations that arise from adopting an
overload-resilient perspective on human-AI alignment, suggesting pathways for
future exploration rather than prescribing definitive solutions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [309] [Backdoor Defense in Diffusion Models via Spatial Attention Unlearning](https://arxiv.org/abs/2504.18563)
*Abha Jha,Ashwath Vaithinathan Aravindan,Matthew Salaway,Atharva Sandeep Bhide,Duygu Nur Yaldiz*

Main category: cs.CR

TL;DR: 提出一种名为SAU的新方法，用于防御文本到图像扩散模型中的后门攻击，通过潜在空间操作和空间注意力机制移除后门触发器，效果显著。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在高维输出空间中的后门攻击防御机制缺乏，现有方法难以有效应对此类威胁，因此需要新的防御策略。

Method: 采用空间注意力遗忘（SAU）技术，结合潜在空间操作和空间注意力机制，精确识别并移除后门触发器的潜在表示。

Result: SAU在所有测试的后门攻击中实现100%触发器移除准确率，CLIP分数达0.7023，优于现有方法且保持图像生成质量。

Conclusion: SAU是一种高效、可扩展的解决方案，能有效保护扩散模型免受后门攻击，同时不影响生成图像的质量和语义一致性。

Abstract: Text-to-image diffusion models are increasingly vulnerable to backdoor
attacks, where malicious modifications to the training data cause the model to
generate unintended outputs when specific triggers are present. While
classification models have seen extensive development of defense mechanisms,
generative models remain largely unprotected due to their high-dimensional
output space, which complicates the detection and mitigation of subtle
perturbations. Defense strategies for diffusion models, in particular, remain
under-explored. In this work, we propose Spatial Attention Unlearning (SAU), a
novel technique for mitigating backdoor attacks in diffusion models. SAU
leverages latent space manipulation and spatial attention mechanisms to isolate
and remove the latent representation of backdoor triggers, ensuring precise and
efficient removal of malicious effects. We evaluate SAU across various types of
backdoor attacks, including pixel-based and style-based triggers, and
demonstrate its effectiveness in achieving 100% trigger removal accuracy.
Furthermore, SAU achieves a CLIP score of 0.7023, outperforming existing
methods while preserving the model's ability to generate high-quality,
semantically aligned images. Our results show that SAU is a robust, scalable,
and practical solution for securing text-to-image diffusion models against
backdoor attacks.

</details>


### [310] [DualBreach: Efficient Dual-Jailbreaking via Target-Driven Initialization and Multi-Target Optimization](https://arxiv.org/abs/2504.18564)
*Xinzhe Huang,Kedong Xiu,Tianhang Zheng,Churui Zeng,Wangze Ni,Zhan Qiin,Kui Ren,Chun Chen*

Main category: cs.CR

TL;DR: DualBreach 是一个针对 LLM 和 Guardrails 的双重越狱框架，通过目标驱动初始化和多目标优化方法，显著提高了越狱成功率并减少了查询次数。实验结果显示其成功率远超现有方法，并提出了防御机制 EGuard。


<details>
  <summary>Details</summary>
Motivation: 现有研究在针对 LLM 和安全护栏（Guardrails）的双重越狱攻击上效果有限，因此需要一种更高效的方法来绕过安全机制。

Method: DualBreach 结合目标驱动初始化（TDI）和多目标优化（MTO），通过近似梯度联合优化提示，同时对黑盒护栏使用代理模型进行模拟。

Result: DualBreach 在 GPT-4 和 Llama-Guard-3 上平均越狱成功率达 93.67%，查询次数仅 1.77 次/次成功，显著优于现有方法。防御机制 EGuard 表现优于 Llama-Guard-3。

Conclusion: DualBreach 是一种高效的双重越狱框架，同时提出的 EGuard 防御机制具有实际应用潜力。

Abstract: Recent research has focused on exploring the vulnerabilities of Large
Language Models (LLMs), aiming to elicit harmful and/or sensitive content from
LLMs. However, due to the insufficient research on dual-jailbreaking -- attacks
targeting both LLMs and Guardrails, the effectiveness of existing attacks is
limited when attempting to bypass safety-aligned LLMs shielded by guardrails.
Therefore, in this paper, we propose DualBreach, a target-driven framework for
dual-jailbreaking. DualBreach employs a Target-driven Initialization (TDI)
strategy to dynamically construct initial prompts, combined with a Multi-Target
Optimization (MTO) method that utilizes approximate gradients to jointly adapt
the prompts across guardrails and LLMs, which can simultaneously save the
number of queries and achieve a high dual-jailbreaking success rate. For
black-box guardrails, DualBreach either employs a powerful open-sourced
guardrail or imitates the target black-box guardrail by training a proxy model,
to incorporate guardrails into the MTO process.
  We demonstrate the effectiveness of DualBreach in dual-jailbreaking scenarios
through extensive evaluation on several widely-used datasets. Experimental
results indicate that DualBreach outperforms state-of-the-art methods with
fewer queries, achieving significantly higher success rates across all
settings. More specifically, DualBreach achieves an average dual-jailbreaking
success rate of 93.67% against GPT-4 with Llama-Guard-3 protection, whereas the
best success rate achieved by other methods is 88.33%. Moreover, DualBreach
only uses an average of 1.77 queries per successful dual-jailbreak,
outperforming other state-of-the-art methods. For the purpose of defense, we
propose an XGBoost-based ensemble defensive mechanism named EGuard, which
integrates the strengths of multiple guardrails, demonstrating superior
performance compared with Llama-Guard-3.

</details>


### [311] [RepliBench: Evaluating the autonomous replication capabilities of language model agents](https://arxiv.org/abs/2504.18565)
*Sid Black,Asa Cooper Stickland,Jake Pencharz,Oliver Sourbut,Michael Schmatz,Jay Bailey,Ollie Matthews,Ben Millwood,Alex Remedios,Alan Cooney*

Main category: cs.CR

TL;DR: RepliBench评估套件用于衡量语言模型代理的自主复制能力，发现目前前沿模型尚不具备自主复制的可信威胁，但在多个组件上表现良好且进步迅速。


<details>
  <summary>Details</summary>
Motivation: 研究不可控的语言模型自主复制行为对安全构成的风险。

Method: 开发RepliBench评估套件，分解为四个核心领域（资源获取、模型权重外泄、计算资源复制、长期持久化），包含20个任务家族的86项任务，测试5种前沿模型。

Result: 模型在简单安全设置下可执行云实例部署、自传播程序编写和模型权重外泄，但难以通过KYC检查或建立稳定的长期代理部署。表现最佳的模型（Claude 3.7 Sonnet）在15/20任务家族中通过率超50%。

Conclusion: 自主复制能力可能在不久的将来随模型改进或人类辅助而出现。

Abstract: Uncontrollable autonomous replication of language model agents poses a
critical safety risk. To better understand this risk, we introduce RepliBench,
a suite of evaluations designed to measure autonomous replication capabilities.
RepliBench is derived from a decomposition of these capabilities covering four
core domains: obtaining resources, exfiltrating model weights, replicating onto
compute, and persisting on this compute for long periods. We create 20 novel
task families consisting of 86 individual tasks. We benchmark 5 frontier
models, and find they do not currently pose a credible threat of
self-replication, but succeed on many components and are improving rapidly.
Models can deploy instances from cloud compute providers, write
self-propagating programs, and exfiltrate model weights under simple security
setups, but struggle to pass KYC checks or set up robust and persistent agent
deployments. Overall the best model we evaluated (Claude 3.7 Sonnet) has a >50%
pass@10 score on 15/20 task families, and a >50% pass@10 score for 9/20
families on the hardest variants. These findings suggest autonomous replication
capability could soon emerge with improvements in these remaining areas or with
human assistance.

</details>


### [312] [Feature Selection via GANs (GANFS): Enhancing Machine Learning Models for DDoS Mitigation](https://arxiv.org/abs/2504.18566)
*Harsh Patel*

Main category: cs.CR

TL;DR: 该论文提出了一种基于生成对抗网络（GAN）的新特征选择方法（GANFS），用于增强DDoS攻击检测的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: DDoS攻击是网络系统的持续威胁，传统特征选择方法在复杂攻击环境中存在可扩展性和适应性不足的问题，因此需要更智能的解决方案。

Method: 通过训练GAN仅针对攻击流量，并利用判别器的扰动敏感性分析来无监督地排名特征重要性，从而提出GANFS方法。

Result: 在CIC-DDoS2019数据集上的实验表明，GANFS提高了分类器的准确性并显著降低了特征维度。

Conclusion: 将生成学习模型集成到网络安全管道中，可以构建更具适应性和可扩展性的检测系统。

Abstract: Distributed Denial of Service (DDoS) attacks represent a persistent and
evolving threat to modern networked systems, capable of causing large-scale
service disruptions. The complexity of such attacks, often hidden within
high-dimensional and redundant network traffic data, necessitates robust and
intelligent feature selection techniques for effective detection. Traditional
methods such as filter-based, wrapper-based, and embedded approaches, each
offer strengths but struggle with scalability or adaptability in complex attack
environments. In this study, we explore these existing techniques through a
detailed comparative analysis and highlight their limitations when applied to
large-scale DDoS detection tasks. Building upon these insights, we introduce a
novel Generative Adversarial Network-based Feature Selection (GANFS) method
that leverages adversarial learning dynamics to identify the most informative
features. By training a GAN exclusively on attack traffic and employing a
perturbation-based sensitivity analysis on the Discriminator, GANFS effectively
ranks feature importance without relying on full supervision. Experimental
evaluations using the CIC-DDoS2019 dataset demonstrate that GANFS not only
improves the accuracy of downstream classifiers but also enhances computational
efficiency by significantly reducing feature dimensionality. These results
point to the potential of integrating generative learning models into
cybersecurity pipelines to build more adaptive and scalable detection systems.

</details>


### [313] [Large Language Model Empowered Privacy-Protected Framework for PHI Annotation in Clinical Notes](https://arxiv.org/abs/2504.18569)
*Guanchen Wu,Linzhi Zheng,Han Xie,Zhen Xiang,Jiaying Lu,Darren Liu,Delgersuren Bold,Bo Li,Xiao Hu,Carl Yang*

Main category: cs.CR

TL;DR: 该论文提出了LPPA框架，通过本地微调大型语言模型（LLM）来高效且隐私保护地标注医疗笔记中的敏感信息（PHI），解决了现有方法泛化能力不足和隐私风险的问题。


<details>
  <summary>Details</summary>
Motivation: 医疗数据去标识化对保护患者隐私至关重要，但现有方法（如规则或学习基方法）泛化能力有限且依赖大量标注数据。商业LLM API存在隐私风险，而本地部署LLM成本高，因此需要一种隐私保护且高效的替代方案。

Method: 提出LPPA框架，通过本地微调LLM并使用合成医疗笔记进行训练，确保隐私保护和高效的PHI标注。

Result: 实验表明，LPPA在去标识化敏感信息方面表现高效且准确，提供了可扩展的隐私保护方案。

Conclusion: LPPA为医疗数据隐私保护提供了一种高效、隐私安全的解决方案，平衡了性能与隐私需求。

Abstract: The de-identification of private information in medical data is a crucial
process to mitigate the risk of confidentiality breaches, particularly when
patient personal details are not adequately removed before the release of
medical records. Although rule-based and learning-based methods have been
proposed, they often struggle with limited generalizability and require
substantial amounts of annotated data for effective performance. Recent
advancements in large language models (LLMs) have shown significant promise in
addressing these issues due to their superior language comprehension
capabilities. However, LLMs present challenges, including potential privacy
risks when using commercial LLM APIs and high computational costs for deploying
open-source LLMs locally. In this work, we introduce LPPA, an LLM-empowered
Privacy-Protected PHI Annotation framework for clinical notes, targeting the
English language. By fine-tuning LLMs locally with synthetic notes, LPPA
ensures strong privacy protection and high PHI annotation accuracy. Extensive
experiments demonstrate LPPA's effectiveness in accurately de-identifying
private information, offering a scalable and efficient solution for enhancing
patient privacy protection.

</details>


### [314] [WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks](https://arxiv.org/abs/2504.18575)
*Ivan Evtimov,Arman Zharmagambetov,Aaron Grattafiori,Chuan Guo,Kamalika Chaudhuri*

Main category: cs.CR

TL;DR: 论文提出了WASP基准测试，用于评估网络AI代理对抗提示注入攻击的安全性，发现现有系统易受攻击但实际攻击成功率低。


<details>
  <summary>Details</summary>
Motivation: 当前基于语言和视觉模型的网络AI代理易受间接提示注入攻击，但现有研究多局限于单目标、不真实的攻击场景或过高假设攻击者权限。为了更准确地评估攻击影响，作者构建了WASP基准。

Method: 通过WASP基准测试三种流行网络代理系统（VisualWebArena、Claude Computer Use和Operator）在不同先进模型下的表现，评估其对人类编写的低难度提示注入攻击的抵抗力。

Result: 实验显示，即使具有高级推理能力或指令层级缓解措施的模型也易受攻击，但攻击者端到端目标完成率低（0-17%）。

Conclusion: 作者呼吁攻击研究需在更真实约束下展示更强控制力，同时表明当前代理系统尚不足以完全实现攻击者目标。

Abstract: Web navigation AI agents use language-and-vision foundation models to enhance
productivity but these models are known to be susceptible to indirect prompt
injections that get them to follow instructions different from the legitimate
user's. Existing explorations of this threat applied to web agents often focus
on a single isolated adversarial goal, test with injected instructions that are
either too easy or not truly malicious, and often give the adversary
unreasonable access. In order to better focus adversarial research, we
construct a new benchmark called WASP (Web Agent Security against Prompt
injection attacks) that introduces realistic web agent hijacking objectives and
an isolated environment to test them in that does not affect real users or the
live web. As part of WASP, we also develop baseline attacks against three
popular web agentic systems (VisualWebArena, Claude Computer Use, and Operator)
instantiated with various state-of-the-art models. Our evaluation shows that
even AI agents backed by models with advanced reasoning capabilities and by
models with instruction hierarchy mitigations are susceptible to low-effort
human-written prompt injections. However, the realistic objectives in WASP also
allow us to observe that agents are currently not capable enough to complete
the goals of attackers end-to-end. Agents begin executing the adversarial
instruction between 16 and 86% of the time but only achieve the goal between 0
and 17% of the time. Based on these findings, we argue that adversarial
researchers should demonstrate stronger attacks that more consistently maintain
control over the agent given realistic constraints on the adversary's power.

</details>


### [315] [Optimizing the Privacy-Utility Balance using Synthetic Data and Configurable Perturbation Pipelines](https://arxiv.org/abs/2504.18596)
*Anantha Sharma,Swetha Devabhaktuni,Eklove Mohan*

Main category: cs.CR

TL;DR: 本文探讨了现代合成数据生成和高级数据扰动技术在BFSI等行业中的应用，旨在通过GANs、PII转换等方法提升数据安全性和分析效率，同时保持数据实用性。


<details>
  <summary>Details</summary>
Motivation: 处理敏感数据时需平衡隐私保护和数据实用性，传统匿名化方法效果有限，需探索更先进的隐私增强技术。

Method: 结合生成对抗网络（GANs）、上下文感知的PII转换、可配置统计扰动和差分隐私等方法，生成既隐私保护又实用的数据集。

Result: 现代技术在隐私保护和数据实用性上优于传统方法，能减少开销并加速分析，同时符合监管要求。

Conclusion: 先进的数据扰动和合成技术为数据敏感行业提供了更优的隐私保护和实用性解决方案，支持数据驱动创新。

Abstract: This paper explores the strategic use of modern synthetic data generation and
advanced data perturbation techniques to enhance security, maintain analytical
utility, and improve operational efficiency when managing large datasets, with
a particular focus on the Banking, Financial Services, and Insurance (BFSI)
sector. We contrast these advanced methods encompassing generative models like
GANs, sophisticated context-aware PII transformation, configurable statistical
perturbation, and differential privacy with traditional anonymization
approaches.
  The goal is to create realistic, privacy-preserving datasets that retain high
utility for complex machine learning tasks and analytics, a critical need in
the data-sensitive industries like BFSI, Healthcare, Retail, and
Telecommunications. We discuss how these modern techniques potentially offer
significant improvements in balancing privacy preservation while maintaining
data utility compared to older methods. Furthermore, we examine the potential
for operational gains, such as reduced overhead and accelerated analytics, by
using these privacy-enhanced datasets. We also explore key use cases where
these methods can mitigate regulatory risks and enable scalable, data-driven
innovation without compromising sensitive customer information.

</details>


### [316] [BadMoE: Backdooring Mixture-of-Experts LLMs via Optimizing Routing Triggers and Infecting Dormant Experts](https://arxiv.org/abs/2504.18598)
*Qingyue Wang,Qi Pang,Xixun Lin,Shuai Wang,Daoyuan Wu*

Main category: cs.CR

TL;DR: 该研究首次针对基于MoE的大型语言模型提出了一种后门攻击方法（BadMoE），利用未充分利用的专家（dormant experts）并优化路由触发器来操控模型输出。


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然能高效扩展模型容量，但其安全性研究不足，尤其是后门攻击的潜在风险未被探索。

Method: 攻击方法包括识别与目标任务无关的dormant experts、构建路由感知损失优化触发器、并通过中毒数据提升这些专家的主导性。

Result: 证明了MoE模型中存在少量主导性专家（dominating experts），且dormant experts可被操控为dominant experts以控制模型预测。

Conclusion: BadMoE成功利用MoE架构的独特性实现了后门攻击，揭示了该类模型的安全隐患。

Abstract: Mixture-of-Experts (MoE) have emerged as a powerful architecture for
  large language models (LLMs), enabling efficient scaling of model capacity
  while maintaining manageable computational costs. The key advantage lies in
  their ability to route different tokens to different ``expert'' networks
  within the model, enabling specialization and efficient handling of diverse
  input. However, the vulnerabilities of MoE-based LLMs still have barely been
  studied, and the potential for backdoor attacks in this context remains
  largely unexplored. This paper presents the first backdoor attack against
  MoE-based LLMs where the attackers poison ``dormant experts'' (i.e.,
underutilized
  experts) and activate them by optimizing routing triggers, thereby gaining
  control over the model's output. We first rigorously prove the existence of a
few ``dominating
  experts'' in MoE models, whose outputs can determine the overall MoE's
  output. We also show that dormant experts can serve as dominating experts to
manipulate model predictions.
  Accordingly, our attack, namely \textsc{BadMoE}, exploits the unique
  architecture of MoE models by 1) identifying dormant experts unrelated to the
target task, 2)
  constructing a routing-aware loss to optimize the activation triggers of
these experts, and 3) promoting dormant experts to dominating roles via
poisoned training data.

</details>


### [317] [A Gradient-Optimized TSK Fuzzy Framework for Explainable Phishing Detection](https://arxiv.org/abs/2504.18636)
*Lohith Srikanth Pentapalli,Jon Salisbury,Josette Riep,Kelly Cohen*

Main category: cs.CR

TL;DR: 提出了基于模糊逻辑和梯度优化的新型网络钓鱼URL检测系统，实现了高准确率和解释性。


<details>
  <summary>Details</summary>
Motivation: 现有网络钓鱼检测方法难以同时实现高准确性和可解释性，新型攻击检测效果不佳或模型过于黑盒化。

Method: 采用一阶Takagi-Sugeno-Kang模糊推理模型，结合Adam优化器进行梯度优化。

Result: 在235,000多个URL的数据集上测试，准确率99.95%，AUC为1.00，模型收敛快且透明。

Conclusion: 该框架显著提升网络安全防御能力，提供高准确性和可解释性的决策工具。

Abstract: Phishing attacks represent an increasingly sophisticated and pervasive threat
to individuals and organizations, causing significant financial losses,
identity theft, and severe damage to institutional reputations. Existing
phishing detection methods often struggle to simultaneously achieve high
accuracy and explainability, either failing to detect novel attacks or
operating as opaque black-box models. To address this critical gap, we propose
a novel phishing URL detection system based on a first-order Takagi-Sugeno-Kang
(TSK) fuzzy inference model optimized through gradient-based techniques. Our
approach intelligently combines the interpretability and human-like reasoning
capabilities of fuzzy logic with the precision and adaptability provided by
gradient optimization methods, specifically leveraging the Adam optimizer for
efficient parameter tuning. Experiments conducted using a comprehensive dataset
of over 235,000 URLs demonstrate rapid convergence, exceptional predictive
performance (accuracy averaging 99.95% across 5 cross-validation folds, with a
perfect AUC i.e. 1.00). Furthermore, optimized fuzzy rules and membership
functions improve interoperability, clearly indicating how the model makes
decisions - an essential feature for cybersecurity applications. This
high-performance, transparent, and interpretable phishing detection framework
significantly advances current cybersecurity defenses, providing practitioners
with accurate and explainable decision-making tools.

</details>


### [318] [Zero-Day Botnet Attack Detection in IoV: A Modular Approach Using Isolation Forests and Particle Swarm Optimization](https://arxiv.org/abs/2504.18814)
*Abdelaziz Amara korba,Nour Elislem Karabadji,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: 论文提出了一种基于边缘的入侵检测系统（IDS），通过元集成分类器检测车联网（IoV）中的已知和未知攻击，实验结果显示92.8%的已知攻击检测率和77.32%的未知攻击检测率。


<details>
  <summary>Details</summary>
Motivation: 随着车联网（IoV）的发展，互联自动驾驶车辆（CAV）面临日益严重的安全威胁，如僵尸网络攻击。现有系统难以同时应对已知和未知攻击，亟需一种高效且可扩展的防护方案。

Method: 利用多接入边缘计算（MEC）服务器训练多个隔离森林（IF）模型，每个模型专注于检测特定类型的僵尸网络攻击，并通过粒子群优化（PSO）的堆叠策略构建元分类器。

Result: 在车用僵尸网络数据集上的测试表明，该系统对已知攻击的平均检测率为92.80%，对未知攻击为77.32%，验证了其有效性。

Conclusion: 该IDS方案为IoV中的CAV提供了可扩展且自适应的安全防御，能够同时应对已知和新兴威胁。

Abstract: The Internet of Vehicles (IoV) is transforming transportation by enhancing
connectivity and enabling autonomous driving. However, this increased
interconnectivity introduces new security vulnerabilities. Bot malware and
cyberattacks pose significant risks to Connected and Autonomous Vehicles
(CAVs), as demonstrated by real-world incidents involving remote vehicle system
compromise. To address these challenges, we propose an edge-based Intrusion
Detection System (IDS) that monitors network traffic to and from CAVs. Our
detection model is based on a meta-ensemble classifier capable of recognizing
known (Nday) attacks and detecting previously unseen (zero-day) attacks. The
approach involves training multiple Isolation Forest (IF) models on
Multi-access Edge Computing (MEC) servers, with each IF specialized in
identifying a specific type of botnet attack. These IFs, either trained locally
or shared by other MEC nodes, are then aggregated using a Particle Swarm
Optimization (PSO) based stacking strategy to construct a robust
meta-classifier. The proposed IDS has been evaluated on a vehicular botnet
dataset, achieving an average detection rate of 92.80% for N-day attacks and
77.32% for zero-day attacks. These results highlight the effectiveness of our
solution in detecting both known and emerging threats, providing a scalable and
adaptive defense mechanism for CAVs within the IoV ecosystem.

</details>


### [319] [Residual-Evasive Attacks on ADMM in Distributed Optimization](https://arxiv.org/abs/2504.18570)
*Sabrina Bruckmeier,Huadong Mo,James Qin*

Main category: cs.CR

TL;DR: 论文提出了两种针对ADMM系统的攻击策略，通过最小化残差变化以规避检测，并通过IEEE 14-bus系统验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前许多检测算法依赖残差变化识别虚假数据注入攻击，但作者发现可通过保持残差不变的方式绕过检测，因此研究更隐蔽的攻击方法。

Method: 第一种策略使用随机起点与Gram-Schmidt正交化确保隐蔽性，并可优化正交分量以增强破坏性；第二种策略在此基础上通过操纵无功功率将系统推向电压上限以获取经济利益。

Result: 在IEEE 14-bus系统中的案例研究表明，两种策略在隐蔽性和破坏性上优于传统攻击，揭示了ADMM系统的脆弱性。

Conclusion: 研究凸显了现有监控算法的不足，需开发更鲁棒的防御机制应对高级攻击。

Abstract: This paper presents two attack strategies designed to evade detection in
ADMM-based systems by preventing significant changes to the residual during the
attacked iteration. While many detection algorithms focus on identifying false
data injection through residual changes, we show that our attacks remain
undetected by keeping the residual largely unchanged. The first strategy uses a
random starting point combined with Gram-Schmidt orthogonalization to ensure
stealth, with potential for refinement by enhancing the orthogonal component to
increase system disruption. The second strategy builds on the first, targeting
financial gains by manipulating reactive power and pushing the system to its
upper voltage limit, exploiting operational constraints. The effectiveness of
the proposed attack-resilient mechanism is demonstrated through case studies on
the IEEE 14-bus system. A comparison of the two strategies, along with commonly
used naive attacks, reveals trade-offs between simplicity, detectability, and
effectiveness, providing insights into ADMM system vulnerabilities. These
findings underscore the need for more robust monitoring algorithms to protect
against advanced attack strategies.

</details>


### [320] [Intelligent Detection of Non-Essential IoT Traffic on the Home Gateway](https://arxiv.org/abs/2504.18571)
*Fabio Palmese,Anna Maria Mandalari,Hamed Haddadi,Alessandro Enrico Cesare Redondi*

Main category: cs.CR

TL;DR: 该论文提出了ML-IoTrim系统，利用机器学习在边缘分析网络行为以检测和阻止智能家居中非必要的IoT流量，无需依赖传统允许列表。


<details>
  <summary>Details</summary>
Motivation: 随着IoT设备在智能家居中的快速普及，其持续的云服务连接引发了安全和隐私问题，现有解决方案（如基于云的威胁检测或过时的允许列表）仍存在敏感数据暴露或流量限制不足的问题。

Method: 构建基于IoT设备行为的标记数据集，设计特征提取流程以实现对网络目的地的二元分类（必要与非必要），并在真实智能家居环境中测试模型。

Result: 模型能精确识别并阻止非必要流量（包括未见过目的地），且可在家庭接入点实现近实时分类，支持大规模IoT部署。

Conclusion: ML-IoTrim为智能家居隐私流量控制提供了可扩展的解决方案，推动了IoT隐私保护技术的未来发展。

Abstract: The rapid expansion of Internet of Things (IoT) devices, particularly in
smart home environments, has introduced considerable security and privacy
concerns due to their persistent connectivity and interaction with cloud
services. Despite advancements in IoT security, effective privacy measures
remain uncovered, with existing solutions often relying on cloud-based threat
detection that exposes sensitive data or outdated allow-lists that inadequately
restrict non-essential network traffic. This work presents ML-IoTrim, a system
for detecting and mitigating non-essential IoT traffic (i.e., not influencing
the device operations) by analyzing network behavior at the edge, leveraging
Machine Learning to classify network destinations. Our approach includes
building a labeled dataset based on IoT device behavior and employing a
feature-extraction pipeline to enable a binary classification of essential vs.
non-essential network destinations. We test our framework in a consumer smart
home setup with IoT devices from five categories, demonstrating that the model
can accurately identify and block non-essential traffic, including previously
unseen destinations, without relying on traditional allow-lists. We implement
our solution on a home access point, showing the framework has strong potential
for scalable deployment, supporting near-real-time traffic classification in
large-scale IoT environments with hundreds of devices. This research advances
privacy-aware traffic control in smart homes, paving the way for future
developments in IoT device privacy.

</details>


### [321] [CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through Cryptography Challenges](https://arxiv.org/abs/2504.19093)
*Yu Li,Qizhi Pei,Mengyuan Sun,Honglin Lin,Chenlin Ming,Xin Gao,Jiang Wu,Conghui He,Lijun Wu*

Main category: cs.CR

TL;DR: CipherBank是一个新的基准测试，用于评估大语言模型在加密解密任务中的推理能力，揭示了当前模型在密码学领域的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数学和编码方面表现出色，但其在需要密码学知识的推理任务中的能力尚未充分探索。

Method: 研究团队设计了CipherBank基准测试，包含2,358个问题，涵盖14个子领域和9种加密算法，评估了包括GPT-4o和DeepSeek-V3在内的多种模型。

Result: 测试结果显示，通用聊天模型和专注于推理的模型在密码学任务上存在显著差距，尤其在经典加密解密任务中表现不佳。

Conclusion: 研究强调了提升大语言模型在密码学推理能力上的必要性，并指出了未来改进的方向。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities,
especially the recent advancements in reasoning, such as o1 and o3, pushing the
boundaries of AI. Despite these impressive achievements in mathematics and
coding, the reasoning abilities of LLMs in domains requiring cryptographic
expertise remain underexplored. In this paper, we introduce CipherBank, a
comprehensive benchmark designed to evaluate the reasoning capabilities of LLMs
in cryptographic decryption tasks. CipherBank comprises 2,358 meticulously
crafted problems, covering 262 unique plaintexts across 5 domains and 14
subdomains, with a focus on privacy-sensitive and real-world scenarios that
necessitate encryption. From a cryptographic perspective, CipherBank
incorporates 3 major categories of encryption methods, spanning 9 distinct
algorithms, ranging from classical ciphers to custom cryptographic techniques.
We evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and
cutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results
reveal significant gaps in reasoning abilities not only between general-purpose
chat LLMs and reasoning-focused LLMs but also in the performance of current
reasoning-focused models when applied to classical cryptographic decryption
tasks, highlighting the challenges these models face in understanding and
manipulating encrypted data. Through detailed analysis and error
investigations, we provide several key observations that shed light on the
limitations and potential improvement areas for LLMs in cryptographic
reasoning. These findings underscore the need for continuous advancements in
LLM reasoning capabilities.

</details>


### [322] [Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for Agentic Multi-Modal Large Reasoning Model](https://arxiv.org/abs/2504.19373)
*Weidi Luo,Qiming Zhang,Tianyu Lu,Xiaogeng Liu,Yue Zhao,Zhen Xiang,Chaowei Xiao*

Main category: cs.CR

TL;DR: 论文研究了ChatGPT o3在视觉推理能力下可能导致的隐私风险，发现其能通过图像高精度预测用户位置，并提出防御机制。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态大型推理模型（如ChatGPT o3）的视觉推理能力是否会导致隐私泄露，重点关注图像地理位置推断的风险。

Method: 手动构建包含50张真实世界图像的隐私相关数据集，通过实验评估ChatGPT o3的定位能力，并进行遮挡实验验证关键特征的作用。

Result: ChatGPT o3在60%的案例中能实现街道级（一英里内）的定位精度，关键视觉线索（如街道布局和庭院设计）对推断成功贡献显著。

Conclusion: 研究强调需在开发多模态大型推理模型时注重隐私保护，尤其是涉及私人图像的场景，并提出通过遮挡关键特征降低定位精度的防御方案。

Abstract: The increasing capabilities of agentic multi-modal large reasoning models,
such as ChatGPT o3, have raised critical concerns regarding privacy leakage
through inadvertent image geolocation. In this paper, we conduct the first
systematic and controlled study on the potential privacy risks associated with
visual reasoning abilities of ChatGPT o3. We manually collect and construct a
dataset comprising 50 real-world images that feature individuals alongside
privacy-relevant environmental elements, capturing realistic and sensitive
scenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can
predict user locations with high precision, achieving street-level accuracy
(within one mile) in 60% of cases. Through analysis, we identify key visual
cues, including street layout and front yard design, that significantly
contribute to the model inference success. Additionally, targeted occlusion
experiments demonstrate that masking critical features effectively mitigates
geolocation accuracy, providing insights into potential defense mechanisms. Our
findings highlight an urgent need for privacy-aware development for agentic
multi-modal large reasoning models, particularly in applications involving
private imagery.

</details>


### [323] [$\texttt{SAGE}$: A Generic Framework for LLM Safety Evaluation](https://arxiv.org/abs/2504.19674)
*Madhur Jindal,Hari Shrawgi,Parag Agrawal,Sandipan Dandapat*

Main category: cs.CR

TL;DR: 论文提出了自动化模块化框架$	exttt{SAGE}$，用于定制化和动态的危害评估，以应对大型语言模型（LLMs）在多样化应用中的安全性挑战。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs安全评估方法无法满足应用定制化和动态对话的需求，可能导致潜在危害被忽视。

Method: 引入$	exttt{SAGE}$框架，利用系统感知的对抗性用户模型进行多轮对话评估。

Result: 实验发现危害随对话长度增加而上升，模型行为因用户个性和场景差异显著，某些模型通过严厉拒绝策略减少危害但可能影响实用性。

Conclusion: 适应性强的上下文特定测试是确保LLMs安全部署的关键。

Abstract: Safety evaluation of Large Language Models (LLMs) has made progress and
attracted academic interest, but it remains challenging to keep pace with the
rapid integration of LLMs across diverse applications. Different applications
expose users to various harms, necessitating application-specific safety
evaluations with tailored harms and policies. Another major gap is the lack of
focus on the dynamic and conversational nature of LLM systems. Such potential
oversights can lead to harms that go unnoticed in standard safety benchmarks.
This paper identifies the above as key requirements for robust LLM safety
evaluation and recognizing that current evaluation methodologies do not satisfy
these, we introduce the $\texttt{SAGE}$ (Safety AI Generic Evaluation)
framework. $\texttt{SAGE}$ is an automated modular framework designed for
customized and dynamic harm evaluations. It utilizes adversarial user models
that are system-aware and have unique personalities, enabling a holistic
red-teaming evaluation. We demonstrate $\texttt{SAGE}$'s effectiveness by
evaluating seven state-of-the-art LLMs across three applications and harm
policies. Our experiments with multi-turn conversational evaluations revealed a
concerning finding that harm steadily increases with conversation length.
Furthermore, we observe significant disparities in model behavior when exposed
to different user personalities and scenarios. Our findings also reveal that
some models minimize harmful outputs by employing severe refusal tactics that
can hinder their usefulness. These insights highlight the necessity of adaptive
and context-specific testing to ensure better safety alignment and safer
deployment of LLMs in real-world scenarios.

</details>


### [324] [Securing GenAI Multi-Agent Systems Against Tool Squatting: A Zero Trust Registry-Based Approach](https://arxiv.org/abs/2504.19951)
*Vineeth Sai Narajala,Ken Huang,Idan Habler*

Main category: cs.CR

TL;DR: 论文提出了一种工具注册系统，旨在解决生成式AI多代理系统中的工具欺骗注册问题，通过安全架构和动态信任评分来预防风险。


<details>
  <summary>Details</summary>
Motivation: 生成式AI多代理系统中工具交互的安全性挑战，尤其是工具欺骗注册问题，需要标准化协议来保障安全。

Method: 提出了一种安全架构，包括管理员控制的注册、集中式工具发现、细粒度访问策略、动态信任评分机制和即时凭证分配。

Result: 系统设计能有效预防工具欺骗注册，同时保持多代理系统的灵活性和功能性。

Conclusion: 该研究填补了生成式AI生态系统中的安全空白，为生产环境中的安全工具集成奠定了基础。

Abstract: The rise of generative AI (GenAI) multi-agent systems (MAS) necessitates
standardized protocols enabling agents to discover and interact with external
tools. However, these protocols introduce new security challenges,
particularly; tool squatting; the deceptive registration or representation of
tools. This paper analyzes tool squatting threats within the context of
emerging interoperability standards, such as Model Context Protocol (MCP) or
seamless communication between agents protocols. It introduces a comprehensive
Tool Registry system designed to mitigate these risks. We propose a
security-focused architecture featuring admin-controlled registration,
centralized tool discovery, fine grained access policies enforced via dedicated
Agent and Tool Registry services, a dynamic trust scoring mechanism based on
tool versioning and known vulnerabilities, and just in time credential
provisioning. Based on its design principles, the proposed registry framework
aims to effectively prevent common tool squatting vectors while preserving the
flexibility and power of multi-agent systems. This work addresses a critical
security gap in the rapidly evolving GenAI ecosystem and provides a foundation
for secure tool integration in production environments.

</details>


### [325] [Securing Agentic AI: A Comprehensive Threat Model and Mitigation Framework for Generative AI Agents](https://arxiv.org/abs/2504.19956)
*Vineeth Sai Narajala,Om Narayan*

Main category: cs.CR

TL;DR: 本文提出了一种针对生成式AI（GenAI）代理的全面威胁模型，识别了9种主要威胁，并分为五大领域。研究还提出了ATFAA和SHIELD两个框架，分别用于组织风险和制定缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI代理在企业环境中的普及，其自主性、记忆访问和复杂推理能力带来了与传统系统不同的安全挑战，需要新的威胁模型。

Method: 研究构建了专门的威胁模型，识别并分类威胁，并提出了ATFAA（风险组织框架）和SHIELD（缓解策略框架）。

Result: 研究发现GenAI代理存在9种新型威胁，传统安全框架难以应对其独特风险，需专用工具和策略。

Conclusion: GenAI代理的安全需要全新的视角和框架，否则可能从强大工具变为企业安全隐患。

Abstract: As generative AI (GenAI) agents become more common in enterprise settings,
they introduce security challenges that differ significantly from those posed
by traditional systems. These agents are not just LLMs; they reason, remember,
and act, often with minimal human oversight. This paper introduces a
comprehensive threat model tailored specifically for GenAI agents, focusing on
how their autonomy, persistent memory access, complex reasoning, and tool
integration create novel risks. This research work identifies 9 primary threats
and organizes them across five key domains: cognitive architecture
vulnerabilities, temporal persistence threats, operational execution
vulnerabilities, trust boundary violations, and governance circumvention. These
threats are not just theoretical they bring practical challenges such as
delayed exploitability, cross-system propagation, cross system lateral
movement, and subtle goal misalignments that are hard to detect with existing
frameworks and standard approaches. To help address this, the research work
present two complementary frameworks: ATFAA - Advanced Threat Framework for
Autonomous AI Agents, which organizes agent-specific risks, and SHIELD, a
framework proposing practical mitigation strategies designed to reduce
enterprise exposure. While this work builds on existing work in LLM and AI
security, the focus is squarely on what makes agents different and why those
differences matter. Ultimately, this research argues that GenAI agents require
a new lens for security. If we fail to adapt our threat models and defenses to
account for their unique architecture and behavior, we risk turning a powerful
new tool into a serious enterprise liability.

</details>


### [326] [Simplified and Secure MCP Gateways for Enterprise AI Integration](https://arxiv.org/abs/2504.19997)
*Ivo Brett*

Main category: cs.CR

TL;DR: 论文提出MCP Gateway，为企业自托管MCP服务器集成提供安全解决方案，包含参考架构、威胁模型和开源实现建议。


<details>
  <summary>Details</summary>
Motivation: 随着MCP在AI代理中的广泛采用，企业集成需要更强的安全保障，而现有公共MCP解决方案无法满足自托管场景的需求。

Method: 设计MCP Gateway架构，整合安全原则、认证、入侵检测和安全隧道技术，实现安全的自托管集成。

Result: 提出了参考架构、威胁模型映射和简化集成策略，并通过开源实现验证其可行性。

Conclusion: MCP Gateway有效解决了企业自托管AI集成的独特安全挑战，为实际部署提供了可行方案。

Abstract: The increased adoption of the Model Context Protocol (MCP) for AI Agents
necessitates robust security for Enterprise integrations. This paper introduces
the MCP Gateway to simplify self-hosted MCP server integration. The proposed
architecture integrates security principles, authentication, intrusion
detection, and secure tunneling, enabling secure self-hosting without exposing
infrastructure. Key contributions include a reference architecture, threat
model mapping, simplified integration strategies, and open-source
implementation recommendations. This work focuses on the unique challenges of
enterprise-centric, self-hosted AI integrations, unlike existing public MCP
server solutions.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [327] [QuantBench: Benchmarking AI Methods for Quantitative Investment](https://arxiv.org/abs/2504.18600)
*Saizhuo Wang,Hao Kong,Jiadong Guo,Fengrui Hua,Yiyan Qi,Wanyun Zhou,Jiahao Zheng,Xinyu Wang,Lionel M. Ni,Jian Guo*

Main category: q-fin.CP

TL;DR: 论文提出了QuantBench，一个工业级基准平台，用于解决AI在量化投资领域缺乏标准化基准的问题，旨在推动研究和实践。


<details>
  <summary>Details</summary>
Motivation: 当前AI在量化投资领域缺乏与行业实践一致的标准化基准，这阻碍了研究进展和学术创新的实际应用。

Method: 设计了QuantBench平台，提供标准化、灵活性和全流程覆盖，支持多种AI算法的集成。

Result: 通过QuantBench的实证研究，发现了关键研究方向，如应对分布变化的持续学习、金融关系数据建模的改进方法等。

Conclusion: QuantBench通过提供共同评估标准和促进研究与实践合作，有望加速AI在量化投资领域的进步。

Abstract: The field of artificial intelligence (AI) in quantitative investment has seen
significant advancements, yet it lacks a standardized benchmark aligned with
industry practices. This gap hinders research progress and limits the practical
application of academic innovations. We present QuantBench, an industrial-grade
benchmark platform designed to address this critical need. QuantBench offers
three key strengths: (1) standardization that aligns with quantitative
investment industry practices, (2) flexibility to integrate various AI
algorithms, and (3) full-pipeline coverage of the entire quantitative
investment process. Our empirical studies using QuantBench reveal some critical
research directions, including the need for continual learning to address
distribution shifts, improved methods for modeling relational financial data,
and more robust approaches to mitigate overfitting in low signal-to-noise
environments. By providing a common ground for evaluation and fostering
collaboration between researchers and practitioners, QuantBench aims to
accelerate progress in AI for quantitative investment, similar to the impact of
benchmark platforms in computer vision and natural language processing.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [328] [BQSched: A Non-intrusive Scheduler for Batch Concurrent Queries via Reinforcement Learning](https://arxiv.org/abs/2504.19142)
*Chenhao Xu,Chunyu Chen,Jinglin Peng,Jiannan Wang,Jun Gao*

Main category: cs.DB

TL;DR: BQSched提出了一种基于强化学习的非侵入式批处理查询调度器，通过注意力机制和IQ-PPO算法优化查询调度，显著缩短了总体执行时间。


<details>
  <summary>Details</summary>
Motivation: 现有工具依赖简单启发式规则处理并发查询调度，但难以捕捉复杂查询特征和相互影响，强化学习虽具潜力但面临调度空间大、采样成本高等挑战。

Method: 设计了基于注意力的状态表示和辅助任务增强的IQ-PPO算法，并引入了自适应掩码、调度增益聚类和增量模拟器等优化策略。

Result: 在TPC-DS基准测试中，BQSched平均缩短了34%和13%的总体执行时间，显著优于传统启发式策略和现有RL调度器。

Conclusion: BQSched首次实现了基于RL的非侵入式批处理查询调度，展现了高效率、稳定性和可扩展性。

Abstract: Most large enterprises build predefined data pipelines and execute them
periodically to process operational data using SQL queries for various tasks. A
key issue in minimizing the overall makespan of these pipelines is the
efficient scheduling of concurrent queries within the pipelines. Existing tools
mainly rely on simple heuristic rules due to the difficulty of expressing the
complex features and mutual influences of queries. The latest reinforcement
learning (RL) based methods have the potential to capture these patterns from
feedback, but it is non-trivial to apply them directly due to the large
scheduling space, high sampling cost, and poor sample utilization.
  Motivated by these challenges, we propose BQSched, a non-intrusive Scheduler
for Batch concurrent Queries via reinforcement learning. Specifically, BQSched
designs an attention-based state representation to capture the complex query
patterns, and proposes IQ-PPO, an auxiliary task-enhanced proximal policy
optimization (PPO) algorithm, to fully exploit the rich signals of Individual
Query completion in logs. Based on the RL framework above, BQSched further
introduces three optimization strategies, including adaptive masking to prune
the action space, scheduling gain-based query clustering to deal with large
query sets, and an incremental simulator to reduce sampling cost. To our
knowledge, BQSched is the first non-intrusive batch query scheduler via RL.
Extensive experiments show that BQSched can significantly improve the
efficiency and stability of batch query scheduling, while also achieving
remarkable scalability and adaptability in both data and queries. For example,
across all DBMSs and scales tested, BQSched reduces the overall makespan of
batch queries on TPC-DS benchmark by an average of 34% and 13%, compared with
the commonly used heuristic strategy and the adapted RL-based scheduler,
respectively.

</details>


### [329] [MINT: Multi-Vector Search Index Tuning](https://arxiv.org/abs/2504.20018)
*Jiongli Zhu,Yue Wang,Bailu Ding,Philip A. Bernstein,Vivek Narasayya,Surajit Chaudhuri*

Main category: cs.DB

TL;DR: 论文提出了一种多向量搜索索引调优框架，通过算法优化，在满足存储和召回率约束下最小化延迟，性能提升2.1X至8.3X。


<details>
  <summary>Details</summary>
Motivation: 多向量搜索在多模态和多特征场景中至关重要，但索引选择对性能影响显著，目前缺少针对多向量搜索的索引调优方法。

Method: 定义多向量搜索索引调优问题，并提出解决方案框架，开发算法以在给定工作负载下优化索引选择。

Result: 相比基线，提出的方法在延迟上实现了2.1倍到8.3倍的加速。

Conclusion: 该框架有效解决了多向量搜索中的索引调优挑战，显著提升了搜索性能。

Abstract: Vector search plays a crucial role in many real-world applications. In
addition to single-vector search, multi-vector search becomes important for
multi-modal and multi-feature scenarios today. In a multi-vector database, each
row is an item, each column represents a feature of items, and each cell is a
high-dimensional vector. In multi-vector databases, the choice of indexes can
have a significant impact on performance. Although index tuning for relational
databases has been extensively studied, index tuning for multi-vector search
remains unclear and challenging. In this paper, we define multi-vector search
index tuning and propose a framework to solve it. Specifically, given a
multi-vector search workload, we develop algorithms to find indexes that
minimize latency and meet storage and recall constraints. Compared to the
baseline, our latency achieves 2.1X to 8.3X speedup.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [330] [Generative Product Recommendations for Implicit Superlative Queries](https://arxiv.org/abs/2504.18748)
*Kaustubh D. Dhole,Nikhita Vedula,Saar Kuzi,Giuseppe Castellucci,Eugene Agichtein,Shervin Malmasi*

Main category: cs.IR

TL;DR: 该研究探讨了如何利用大语言模型（LLMs）为隐式最高级查询（如“最佳越野跑鞋”）生成隐式属性并推理，以改进产品推荐。提出了一个名为SUPERB的四点标注框架，并通过实验验证了多种检索与排序方法的效果。


<details>
  <summary>Details</summary>
Motivation: 解决用户通过间接、模糊或未明确指定的查询（如“最佳越野跑鞋”）寻找产品时，标准检索与排序系统因缺少明确属性而面临的挑战。

Method: 提出了SUPERB四点标注框架，结合LLM生成产品标注，并实证评估了多种现有检索与排序方法在新数据集上的表现。

Result: 验证了LLM在生成隐式属性和推理上的有效性，为电商推荐系统提供了改进方向。

Conclusion: LLM能够有效处理隐式最高级查询，SUPERB框架和实验结果为实际电商系统的整合提供了实用见解。

Abstract: In Recommender Systems, users often seek the best products through indirect,
vague, or under-specified queries, such as "best shoes for trail running". Such
queries, also referred to as implicit superlative queries, pose a significant
challenge for standard retrieval and ranking systems as they lack an explicit
mention of attributes and require identifying and reasoning over complex
factors. We investigate how Large Language Models (LLMs) can generate implicit
attributes for ranking as well as reason over them to improve product
recommendations for such queries. As a first step, we propose a novel
four-point schema for annotating the best product candidates for superlative
queries called SUPERB, paired with LLM-based product annotations. We then
empirically evaluate several existing retrieval and ranking approaches on our
new dataset, providing insights and discussing their integration into
real-world e-commerce production systems.

</details>


### [331] [World Food Atlas Project](https://arxiv.org/abs/2504.18727)
*Ali Rostami,Z Xie,A Ishino,Y Yamakata,K Aizawa,Ramesh Jain*

Main category: cs.IR

TL;DR: 论文提出构建World Food Atlas（WFA），通过Food Knowledge Graph（FKG）和FoodLog Athl/RecipeLog应用整合全球食物知识，解决疫情期间人们对食物影响的认知需求。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情期间，人们居家时间增加，意识到食物对身体的影响，需要更好地了解和掌控食物。

Method: 提出两种方法：一是Food Knowledge Graph（FKG），通过食谱和营养数据构建食物关系图；二是FoodLog Athl和RecipeLog应用，记录用户饮食习惯。

Result: 整合FKG和日志应用，为构建WFA提供基础，并尝试解决相关问题。

Conclusion: 通过技术和数据整合，WFA有望帮助人们更深入地了解和控制食物摄入。

Abstract: A coronavirus pandemic is forcing people to be "at home" all over the world.
In a life of hardly ever going out, we would have realized how the food we eat
affects our bodies. What can we do to know our food more and control it better?
To give us a clue, we are trying to build a World Food Atlas (WFA) that
collects all the knowledge about food in the world. In this paper, we present
two of our trials. The first is the Food Knowledge Graph (FKG), which is a
graphical representation of knowledge about food and ingredient relationships
derived from recipes and food nutrition data. The second is the FoodLog Athl
and the RecipeLog that are applications for collecting people's detailed
records about food habit. We also discuss several problems that we try to solve
to build the WFA by integrating these two ideas.

</details>


### [332] [Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation](https://arxiv.org/abs/2504.19754)
*Carlo Merola,Jaspinder Singh*

Main category: cs.IR

TL;DR: 本文通过对‘延迟分块’和‘上下文检索’两种技术的对比分析，评估了它们在优化检索增强生成（RAG）系统中的效果和效率。结果表明，上下文检索在保持语义连贯性上更有效，但计算资源需求更高；而延迟分块效率更高，但会牺牲相关性和完整性。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中如何有效管理大量外部知识，以克服传统分块方法导致的信息碎片化问题，并比较两种新兴技术（延迟分块和上下文检索）的优缺点。

Method: 采用严格的对比分析方法，评估延迟分块和上下文检索在RAG系统中的表现，重点关注其效果和效率。

Result: 上下文检索在保持语义连贯性方面表现更好，但计算成本较高；延迟分块效率更高，但会降低相关性和完整性。

Conclusion: 研究为RAG系统的优化提供了实用指导，强调根据实际需求在语义连贯性和效率之间进行权衡。

Abstract: Retrieval-augmented generation (RAG) has become a transformative approach for
enhancing large language models (LLMs) by grounding their outputs in external
knowledge sources. Yet, a critical question persists: how can vast volumes of
external knowledge be managed effectively within the input constraints of LLMs?
Traditional methods address this by chunking external documents into smaller,
fixed-size segments. While this approach alleviates input limitations, it often
fragments context, resulting in incomplete retrieval and diminished coherence
in generation. To overcome these shortcomings, two advanced techniques, late
chunking and contextual retrieval, have been introduced, both aiming to
preserve global context. Despite their potential, their comparative strengths
and limitations remain unclear. This study presents a rigorous analysis of late
chunking and contextual retrieval, evaluating their effectiveness and
efficiency in optimizing RAG systems. Our results indicate that contextual
retrieval preserves semantic coherence more effectively but requires greater
computational resources. In contrast, late chunking offers higher efficiency
but tends to sacrifice relevance and completeness.

</details>


### [333] [Feature Fusion Revisited: Multimodal CTR Prediction for MMCTR Challenge](https://arxiv.org/abs/2504.18961)
*Junjie Zhou*

Main category: cs.IR

TL;DR: 该论文探讨了多模态大语言模型在推荐系统中的应用，针对大模型的高延迟问题提出了解决方案，并在竞赛中获奖。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决多模态大语言模型在推荐系统中应用时的高延迟问题，以提高效率和性能。

Method: 采用了多模态表示学习的方法，并结合竞赛要求提交了技术报告，详细描述了方法论。

Result: 团队在Task 2 - Winner (Multimodal CTR Prediction)中获奖，并公开了代码和模型权重。

Conclusion: 论文总结了方法及成果，并提出了未来研究方向，重点是推荐信号与多模态表示的有效整合。

Abstract: With the rapid advancement of Multimodal Large Language Models (MLLMs), an
increasing number of researchers are exploring their application in
recommendation systems. However, the high latency associated with large models
presents a significant challenge for such use cases. The EReL@MIR workshop
provided a valuable opportunity to experiment with various approaches aimed at
improving the efficiency of multimodal representation learning for information
retrieval tasks. As part of the competition's requirements, participants were
mandated to submit a technical report detailing their methodologies and
findings. Our team was honored to receive the award for Task 2 - Winner
(Multimodal CTR Prediction). In this technical report, we present our methods
and key findings. Additionally, we propose several directions for future work,
particularly focusing on how to effectively integrate recommendation signals
into multimodal representations. The codebase for our implementation is
publicly available at: https://github.com/Lattice-zjj/MMCTR_Code, and the
trained model weights can be accessed at:
https://huggingface.co/FireFlyCourageous/MMCTR_DIN_MicroLens_1M_x1.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [334] [Nonconvex Linear System Identification with Minimal State Representation](https://arxiv.org/abs/2504.18791)
*Uday Kiran Reddy Tadipatri,Benjamin D. Haeffele,Joshua Agterberg,Ingvar Ziemann,René Vidal*

Main category: eess.SY

TL;DR: 该论文提出两种非凸重构方法（Burer-Monterio分解和系统参数直接优化）以高效解决低阶线性系统识别问题，避免了传统Hankel秩最小化方法中重复SVD计算的高成本，并证明了其在统计误差和样本复杂度上的优势。


<details>
  <summary>Details</summary>
Motivation: 传统Hankel秩最小化方法依赖凸松弛，需多次昂贵SVD计算，计算效率低。本文旨在通过非凸重构提升计算效率，并改善统计性能。

Method: 提出两种方法：(i) Hankel矩阵的Burer-Monterio分解以实现高效核范数最小化，(ii) 对可对角化系统直接优化系统参数，采用原子范数分解。

Result: 新方法显著减少计算开销，降低了统计误差和样本复杂度（不与轨迹长度线性相关），且理论保证多项式时间内全局最优。

Conclusion: 非凸重构方法在计算效率和统计性能上优于传统方法，适用于低阶线性系统识别，并通过合成数据验证了理论。

Abstract: Low-order linear System IDentification (SysID) addresses the challenge of
estimating the parameters of a linear dynamical system from finite samples of
observations and control inputs with minimal state representation. Traditional
approaches often utilize Hankel-rank minimization, which relies on convex
relaxations that can require numerous, costly singular value decompositions
(SVDs) to optimize. In this work, we propose two nonconvex reformulations to
tackle low-order SysID (i) Burer-Monterio (BM) factorization of the Hankel
matrix for efficient nuclear norm minimization, and (ii) optimizing directly
over system parameters for real, diagonalizable systems with an atomic norm
style decomposition. These reformulations circumvent the need for repeated
heavy SVD computations, significantly improving computational efficiency.
Moreover, we prove that optimizing directly over the system parameters yields
lower statistical error rates, and lower sample complexities that do not scale
linearly with trajectory length like in Hankel-nuclear norm minimization.
Additionally, while our proposed formulations are nonconvex, we provide
theoretical guarantees of achieving global optimality in polynomial time.
Finally, we demonstrate algorithms that solve these nonconvex programs and
validate our theoretical claims on synthetic data.

</details>


### [335] [Negative Imaginary Neural ODEs: Learning to Control Mechanical Systems with Stability Guarantees](https://arxiv.org/abs/2504.19497)
*Kanghong Shi,Ruigang Wang,Ian R. Manchester*

Main category: eess.SY

TL;DR: 提出一种基于负虚神经常微分方程（NINODE）的神经控制方法，确保机械系统的稳定化。


<details>
  <summary>Details</summary>
Motivation: 通过神经网络在哈密顿框架中实现负虚特性，为机械系统提供稳定控制。

Method: 利用具有特定属性的神经网络作为状态空间函数矩阵，构建NINODE控制器。

Result: 在非线性弹簧质量系统中验证了控制器的有效性和稳定性保证。

Conclusion: NINODE控制器在满足条件时可稳定具有共位力执行器和位置传感器的机械系统。

Abstract: We propose a neural control method to provide guaranteed stabilization for
mechanical systems using a novel negative imaginary neural ordinary
differential equation (NINODE) controller. Specifically, we employ neural
networks with desired properties as state-space function matrices within a
Hamiltonian framework to ensure the system possesses the NI property. This
NINODE system can serve as a controller that asymptotically stabilizes an NI
plant under certain conditions. For mechanical plants with colocated force
actuators and position sensors, we demonstrate that all the conditions required
for stability can be translated into regularity constraints on the neural
networks used in the controller. We illustrate the utility, effectiveness, and
stability guarantees of the NINODE controller through an example involving a
nonlinear mass-spring system.

</details>


### [336] [Model-based controller assisted domain randomization in deep reinforcement learning: application to nonlinear powertrain control](https://arxiv.org/abs/2504.19715)
*Heisei Yonezawa,Ansei Yonezawa,Itsuro Kajiwara*

Main category: eess.SY

TL;DR: 论文提出了一种基于深度强化学习（DRL）的新型鲁棒控制方法，通过结合域随机化DRL、LSTM网络和基于模型的控制（MBC），有效处理非线性与不确定性，提高了控制系统的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒控制在处理某些非线性和不确定性时存在局限，需要一种更实用的方法。本文旨在通过DRL框架解决这一挑战，提高控制系统的实际应用能力。

Method: 采用域随机化的DRL框架，结合LSTM网络的演员-评论家结构和基于模型的控制，通过LMDP建模问题，并在训练中对环境动态进行随机化以提高鲁棒性。

Result: 提出的控制器在复杂动力系统应用中表现出高鲁棒性，且所需的神经网络结构更紧凑、训练数据更少。

Conclusion: 该方法在非线性与不确定性环境下表现优异，为复杂机械系统的控制提供了新思路。

Abstract: Complex mechanical systems such as vehicle powertrains are inherently subject
to multiple nonlinearities and uncertainties arising from parametric
variations. Modeling and calibration errors are therefore unavoidable, making
the transfer of control systems from simulation to real-world systems a
critical challenge. Traditional robust controls have limitations in handling
certain types of nonlinearities and uncertainties, requiring a more practical
approach capable of comprehensively compensating for these various constraints.
This study proposes a new robust control approach using the framework of deep
reinforcement learning (DRL). The key strategy lies in the synergy among domain
randomization-based DRL, long short-term memory (LSTM)-based actor and critic
networks, and model-based control (MBC). The problem setup is modeled via the
latent Markov decision process (LMDP), a set of vanilla MDPs, for a controlled
system subject to uncertainties and nonlinearities. In LMDP, the dynamics of an
environment simulator is randomized during training to improve the robustness
of the control system to real testing environments. The randomization increases
training difficulties as well as conservativeness of the resultant control
system; therefore, progress is assisted by concurrent use of a model-based
controller based on a nominal system model. Compared to traditional DRL-based
controls, the proposed controller design is smarter in that we can achieve a
high level of generalization ability with a more compact neural network
architecture and a smaller amount of training data. The proposed approach is
verified via practical application to active damping for a complex powertrain
system with nonlinearities and parametric variations. Comparative tests
demonstrate the high robustness of the proposed approach.

</details>


### [337] [Capturing Aerodynamic Characteristics of ATTAS Aircraft with Evolving Intelligent System](https://arxiv.org/abs/2504.19949)
*Aydoğan Soylu,Tufan Kumbasar*

Main category: eess.SY

TL;DR: 论文提出了一种新型的Evolving Type-2 Quantum Fuzzy Neural Network (eT2QFNN)用于精确建模ATTAS飞机的气动系数，通过增量学习和量子隶属函数提升对不确定性和噪声的鲁棒性，并在训练阶段进行了大量数据和有限数据的对比研究，结果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 精确建模气动系数对现代航空系统的性能理解和优化至关重要，但传统方法在处理不确定性和数据噪声时存在局限性。

Method: 采用eT2QFNN，通过增量学习策略和量子隶属函数，自动学习规则和调整参数，生成多个线性子模型来表征非线性飞机特性，并在ATTAS飞行数据上进行了两种数据规模的训练研究。

Result: eT2QFNN在气动系数建模中表现优于基线方法，且规则数量少于Type-1模糊方法，通过Delta方法进一步验证了其在稳定性与控制导数分析中的优越性。

Conclusion: eT2QFNN在气动系数建模中展现出高效性和鲁棒性，为复杂非线性系统建模提供了新思路。

Abstract: Accurate modeling of aerodynamic coefficients is crucial for understanding
and optimizing the performance of modern aircraft systems. This paper presents
the novel deployment of an Evolving Type-2 Quantum Fuzzy Neural Network
(eT2QFNN) for modeling the aerodynamic coefficients of the ATTAS aircraft to
express the aerodynamic characteristics. eT2QFNN can represent the nonlinear
aircraft model by creating multiple linear submodels with its rule-based
structure through an incremental learning strategy rather than a traditional
batch learning approach. Moreover, it enhances robustness to uncertainties and
data noise through its quantum membership functions, as well as its automatic
rule-learning and parameter-tuning capabilities. During the estimation of the
aerodynamic coefficients via the flight data of the ATTAS, two different
studies are conducted in the training phase: one with a large amount of data
and the other with a limited amount of data. The results show that the modeling
performance of the eT2QFNN is superior in comparison to baseline counterparts.
Furthermore, eT2QFNN estimated the aerodynamic model with fewer rules compared
to Type-1 fuzzy counterparts. In addition, by applying the Delta method to the
proposed approach, the stability and control derivatives of the aircraft are
analyzed. The results prove the superiority of the proposed eT2QFNN in
representing aerodynamic coefficients.

</details>


### [338] [Digital Twin-based Out-of-Distribution Detection in Autonomous Vessels](https://arxiv.org/abs/2504.19816)
*Erblin Isaku,Hassan Sartaj,Shaukat Ali*

Main category: eess.SY

TL;DR: 提出了一种名为ODDIT的数字孪生方法，用于提前检测自主船舶（AV）的异常状态，支持主动干预和高精度测试。


<details>
  <summary>Details</summary>
Motivation: 现有文献对基于机器学习的实时数据高级分析在自主船舶数字孪生中的应用探索不足，因此需要开发一种能够提前检测异常状态的方法。

Method: ODDIT方法结合了两个机器学习模型：一个预测未来船舶状态，另一个判断预测状态是否为异常（OOD）。

Result: 在模拟条件下（包括传感器噪声和环境干扰），ODDIT对五种船舶的异常状态检测表现出高精度（AUROC和TNR@TPR95达99%）。

Conclusion: ODDIT证明了数字孪生在自主船舶异常检测中的有效性，为实时干预和测试提供了可靠工具。

Abstract: An autonomous vessel (AV) is a complex cyber-physical system (CPS) with
software enabling many key functionalities, e.g., navigation software enables
an AV to autonomously or semi-autonomously follow a path to its destination.
Digital twins of such AVs enable advanced functionalities such as running
what-if scenarios, performing predictive maintenance, and enabling fault
diagnosis. Due to technological improvements, real-time analyses using
continuous data from vessels' real-time operations have become increasingly
possible. However, the literature has little explored developing advanced
analyses in real-time data in AVs with digital twins built with machine
learning techniques. To this end, we present a novel digital twin-based
approach (ODDIT) to detect future out-of-distribution (OOD) states of an AV
before reaching them, enabling proactive intervention. Such states may indicate
anomalies requiring attention (e.g., manual correction by the ship master) and
assist testers in scenario-centered testing. The digital twin consists of two
machine-learning models predicting future vessel states and whether the
predicted state will be OOD. We evaluated ODDIT with five vessels across
waypoint and zigzag maneuvering under simulated conditions, including sensor
and actuator noise and environmental disturbances i.e., ocean current. ODDIT
achieved high accuracy in detecting OOD states, with AUROC and TNR@TPR95 scores
reaching 99\% across multiple vessels.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [339] [Machine Learning-Based Modeling of the Anode Heel Effect in X-ray Beam Monte Carlo Simulations](https://arxiv.org/abs/2504.19155)
*Hussein Harb,Didier Benoit,Axel Rannou,Chi-Hieu Pham,Valentin Tissot,Bahaa Nasr,Julien Bert*

Main category: physics.med-ph

TL;DR: 本研究通过开发一种AI驱动的模型来增强X射线成像中蒙特卡洛模拟的准确性，重点改进了阳极足跟效应的模拟，实现了更精准的射束强度分布和剂量计算。


<details>
  <summary>Details</summary>
Motivation: 传统X射线模型在模拟阳极足跟效应时存在精度不足的问题，影响了剂量计算和图像质量。本研究旨在通过AI技术优化这一模拟过程。

Method: 采用机器学习模型动态调整X射线管阳极和阴极侧的射束权重，以复现临床X射线束的不对称特性。优化后的射束权重被集成到OpenGATE和GGEMS蒙特卡洛工具包中。

Result: 实验结果显示，在50至120 kVp能量范围内，阴极侧的剂量率提升了9.6%，阳极侧降低了12.5%。验证表明，该模型显著提升了剂量一致性和准确性。

Conclusion: 该AI模型为X射线剂量学提供了更精确的模拟框架，有望在临床和科研中应用于剂量优化、成像质量提升及辐射安全。

Abstract: This study enhances Monte Carlo simulation accuracy in X-ray imaging by
developing an AI-driven model for the anode heel effect, achieving improved
beam intensity distribution and dosimetric precision. Through dynamic
adjustments to beam weights on the anode and cathode sides of the X-ray tube,
our machine learning model effectively replicates the asymmetry characteristic
of clinical X-ray beams. Experimental results reveal dose rate increases of up
to 9.6% on the cathode side and reductions of up to 12.5% on the anode side,
for energy levels between 50 and 120 kVp. These experimentally optimized beam
weights were integrated into the OpenGATE and GGEMS Monte Carlo toolkits,
significantly advancing dosimetric simulation accuracy and the image quality
which closely resembles the clinical imaging. Validation with fluence and dose
actors demonstrated that the AI-based model closely mirrors clinical beam
behavior, providing substantial improvements in dose consistency and accuracy
over conventional X-ray models. This approach provides a robust framework for
improving X-ray dosimetry, with potential applications in dose optimization,
imaging quality enhancement, and radiation safety in both clinical and research
settings.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [340] [Approximating Nash Equilibria in General-Sum Games via Meta-Learning](https://arxiv.org/abs/2504.18868)
*David Sychrovský,Christopher Solinas,Revan MacQueen,Kevin Wang,James R. Wright,Nathan R. Sturtevant,Michael Bowling*

Main category: cs.GT

TL;DR: 本文通过元学习优化遗憾最小化算法，减少策略间的相关性以更接近纳什均衡，在广义不完美信息博弈中表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 纳什均衡在广义博弈中难以高效求解，现有遗憾最小化算法只能收敛至粗相关均衡（CCE），而CCE允许策略相关性。本文旨在通过元学习降低相关性，使结果更接近纳什均衡。

Method: 采用元学习框架调整遗憾最小化算法，通过最小化策略相关性优化过程，同时保留收敛至CCE的保证。理论分析中给出了元损失与纳什均衡距离的边界。

Result: 实验表明，在广义不完美信息博弈中，新算法比现有遗憾最小化技术能显著更好地逼近纳什均衡。

Conclusion: 元学习可有效减少策略相关性，提升遗憾最小化算法对纳什均衡的逼近效果，为复杂博弈求解提供了新思路。

Abstract: Nash equilibrium is perhaps the best-known solution concept in game theory.
Such a solution assigns a strategy to each player which offers no incentive to
unilaterally deviate. While a Nash equilibrium is guaranteed to always exist,
the problem of finding one in general-sum games is PPAD-complete, generally
considered intractable. Regret minimization is an efficient framework for
approximating Nash equilibria in two-player zero-sum games. However, in
general-sum games, such algorithms are only guaranteed to converge to a
coarse-correlated equilibrium (CCE), a solution concept where players can
correlate their strategies. In this work, we use meta-learning to minimize the
correlations in strategies produced by a regret minimizer. This encourages the
regret minimizer to find strategies that are closer to a Nash equilibrium. The
meta-learned regret minimizer is still guaranteed to converge to a CCE, but we
give a bound on the distance to Nash equilibrium in terms of our meta-loss. We
evaluate our approach in general-sum imperfect information games. Our
algorithms provide significantly better approximations of Nash equilibria than
state-of-the-art regret minimization techniques.

</details>


### [341] [Meta-Learning in Self-Play Regret Minimization](https://arxiv.org/abs/2504.18917)
*David Sychrovský,Martin Schmid,Michal Šustr,Michael Bowling*

Main category: cs.GT

TL;DR: 论文提出了一种扩展的自对弈元学习方法，用于加速在类似游戏分布上的均衡求解，通过在策略选择时全局整合信息，显著优于现有遗憾最小化算法。


<details>
  <summary>Details</summary>
Motivation: 实践中玩家常面临相似但不同的游戏分布（如股票交易或子游戏策略细化），现有方法专注于单独求解单个游戏，而离线元学习可加速此类分布的均衡求解。

Method: 扩展自对弈框架，全局整合所有决策状态信息，而非传统局部遗憾分解，应用于正态博弈和河流扑克子游戏。

Result: 在正态博弈和河流扑克子游戏中，该方法显著优于其他最先进的遗憾最小化算法。

Conclusion: 全局信息整合的元学习方法在自对弈场景中高效，为大规模领域均衡近似提供了新思路。

Abstract: Regret minimization is a general approach to online optimization which plays
a crucial role in many algorithms for approximating Nash equilibria in
two-player zero-sum games. The literature mainly focuses on solving individual
games in isolation. However, in practice, players often encounter a
distribution of similar but distinct games. For example, when trading
correlated assets on the stock market, or when refining the strategy in
subgames of a much larger game. Recently, offline meta-learning was used to
accelerate one-sided equilibrium finding on such distributions. We build upon
this, extending the framework to the more challenging self-play setting, which
is the basis for most state-of-the-art equilibrium approximation algorithms for
domains at scale. When selecting the strategy, our method uniquely integrates
information across all decision states, promoting global communication as
opposed to the traditional local regret decomposition. Empirical evaluation on
normal-form games and river poker subgames shows our meta-learned algorithms
considerably outperform other state-of-the-art regret minimization algorithms.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [342] [From Prompts to Propositions: A Logic-Based Lens on Student-LLM Interactions](https://arxiv.org/abs/2504.18691)
*Ali Alfageeh,Sadegh AlMahdi Kazemi Zarkouei,Daye Nam,Daniel Prol,Matin Amoozadeh,Souti Chattopadhyay,James Prather,Paul Denny,Juho Leinonen,Michael Hilton,Sruti Srinivasa Ragavan,Mohammad Amin Alipour*

Main category: cs.HC

TL;DR: 该论文提出了一种名为Prompt2Constraints的新方法，将学生的提示转化为逻辑约束，用以分析学生如何利用LLM解决编程任务。研究发现，成功和失败的尝试在约束数量上相似，但失败时学生更多会显著修改提示，中途改变策略。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在计算教育中的广泛应用，了解学生如何利用LLM以及如何编写提示以解决计算任务成为一个重要挑战。现有的定性或定量方法缺乏扩展性或未能有效捕捉提示的语义演变。

Method: 该方法名为Prompt2Constraints，将学生的提示翻译为逻辑约束，这些约束能以简洁且可量化的方式表示提示的意图。研究分析了来自203名学生的1,872个提示。

Result: 研究发现，成功与失败的尝试在约束数量上相似，但失败时，学生更多会大幅修改提示并切换策略。研究还识别了可以帮助学生改进提示的关键干预点。

Conclusion: 该研究提供了一种新的可扩展方法，用于检测在自然语言编程任务中遇到困难的学生，并可为编程工具提供实时支持。

Abstract: Background and Context. The increasing integration of large language models
(LLMs) in computing education presents an emerging challenge in understanding
how students use LLMs and craft prompts to solve computational tasks. Prior
research has used both qualitative and quantitative methods to analyze
prompting behavior, but these approaches lack scalability or fail to
effectively capture the semantic evolution of prompts. Objective. In this
paper, we investigate whether students prompts can be systematically analyzed
using propositional logic constraints. We examine whether this approach can
identify patterns in prompt evolution, detect struggling students, and provide
insights into effective and ineffective strategies. Method. We introduce
Prompt2Constraints, a novel method that translates students prompts into
logical constraints. The constraints are able to represent the intent of the
prompts in succinct and quantifiable ways. We used this approach to analyze a
dataset of 1,872 prompts from 203 students solving introductory programming
tasks. Findings. We find that while successful and unsuccessful attempts tend
to use a similar number of constraints overall, when students fail, they often
modify their prompts more significantly, shifting problem-solving strategies
midway. We also identify points where specific interventions could be most
helpful to students for refining their prompts. Implications. This work offers
a new and scalable way to detect students who struggle in solving natural
language programming tasks. This work could be extended to investigate more
complex tasks and integrated into programming tools to provide real-time
support.

</details>


### [343] [Clinical knowledge in LLMs does not translate to human interactions](https://arxiv.org/abs/2504.18919)
*Andrew M. Bean,Rebecca Payne,Guy Parsons,Hannah Rose Kirk,Juan Ciro,Rafael Mosquera,Sara Hincapié Monsalve,Aruna S. Ekanayaka,Lionel Tarassenko,Luc Rocher,Adam Mahdi*

Main category: cs.HC

TL;DR: 研究探讨了大型语言模型（LLM）在医疗建议中的实际表现，发现尽管LLM在模拟测试中表现优异，但在实际使用中帮助用户识别病情和选择行动的准确率较低，建议在公共部署前进行系统性用户测试。


<details>
  <summary>Details</summary>
Motivation: 验证LLM在真实医疗场景中对公众的帮助效果，比较其在模拟测试和实际交互中的表现差异。

Method: 通过对照实验，让1298名参与者随机使用LLM（如GPT-4o）或自行选择信息来源，分析其在10个医疗场景中的表现。

Result: LLM单独测试时表现优异（病情识别94.9%，行动选择56.3%），但在用户辅助场景中效果显著下降（病情识别<34.5%，行动选择<44.2%），且不优于对照组。

Conclusion: 用户交互是LLM医疗应用的主要挑战，建议部署前需系统性测试交互能力，现有评测标准无法预测实际使用问题。

Abstract: Global healthcare providers are exploring use of large language models (LLMs)
to provide medical advice to the public. LLMs now achieve nearly perfect scores
on medical licensing exams, but this does not necessarily translate to accurate
performance in real-world settings. We tested if LLMs can assist members of the
public in identifying underlying conditions and choosing a course of action
(disposition) in ten medical scenarios in a controlled study with 1,298
participants. Participants were randomly assigned to receive assistance from an
LLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested
alone, LLMs complete the scenarios accurately, correctly identifying conditions
in 94.9% of cases and disposition in 56.3% on average. However, participants
using the same LLMs identified relevant conditions in less than 34.5% of cases
and disposition in less than 44.2%, both no better than the control group. We
identify user interactions as a challenge to the deployment of LLMs for medical
advice. Standard benchmarks for medical knowledge and simulated patient
interactions do not predict the failures we find with human participants.
Moving forward, we recommend systematic human user testing to evaluate
interactive capabilities prior to public deployments in healthcare.

</details>


### [344] [LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings](https://arxiv.org/abs/2504.18988)
*Saramsh Gautam,Mahmood Jasim*

Main category: cs.HC

TL;DR: 论文研究了英语非母语研究者（ESL）在多语言团队协作中的沟通障碍，提出并验证了LINC系统，通过实时多语言交流和会后分析工具提升协作效率。


<details>
  <summary>Details</summary>
Motivation: 英语非母语研究者（ESL）在多语言团队会议中面临沟通和理解困难，导致贡献受限。为解决这一问题，研究旨在设计支持多语言协作的系统。

Method: 通过调查64名ESL研究者，确定了四项设计目标，并开发了LINC系统，包含实时多语言交流模块和会后分析仪表盘。通过六组多语言团队的两阶段研究进行系统评估。

Result: LINC帮助参与者用偏好语言交流，有效回顾会议内容并为后续会议准备。研究还探讨了语言偏好外的其他影响多语言会议参与的因素。

Conclusion: 多模态系统（如LINC）能有效支持多语言混合协作环境，但还需考虑语言偏好外的外部因素。

Abstract: Collaborative research often includes contributors with varied perspectives
from diverse linguistic backgrounds. However, English as a Second Language
(ESL) researchers often struggle to communicate during meetings in English and
comprehend discussions, leading to limited contribution. To investigate these
challenges, we surveyed 64 ESL researchers who frequently collaborate in
multilingual teams and identified four key design goals around participation,
comprehension, documentation, and feedback. Guided by these design goals, we
developed LINC, a multimodal Language INdependent Collaboration system with two
components: a real-time module for multilingual communication during meetings
and a post-meeting dashboard for discussion analysis. We evaluated the system
through a two-phased study with six triads of multilingual teams. We found that
using LINC, participants benefited from communicating in their preferred
language, recalled and reviewed actionable insights, and prepared for upcoming
meetings effectively. We discuss external factors that impact multilingual
meeting participation beyond language preferences and the implications of
multimodal systems in facilitating meetings in hybrid multilingual
collaborative settings beyond research.

</details>


### [345] [Clones in the Machine: A Feminist Critique of Agency in Digital Cloning](https://arxiv.org/abs/2504.18807)
*Siân Brooke*

Main category: cs.HC

TL;DR: 本文批评学术研究中的数字克隆，指出其体现了AI解决方案主义，并掩盖了围绕同意、代理和表达的伦理问题，提出了去中心化数据仓库和动态同意模型作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 数字克隆常被视为获取行为洞察的可扩展工具，但其简化了人类复杂性并可能延续系统性偏见，作者希望通过批判这一问题并提出替代方案来促进伦理AI实践。

Method: 结合女性主义代理理论分析数字克隆的伦理问题，并提出去中心化数据仓库与动态同意模型作为改进方法。

Result: 揭示了数字克隆的潜在伦理危害，并提出了替代方案以支持更符合伦理的AI实践。

Conclusion: 数字克隆的AI解决方案主义逻辑需要被挑战，采用去中心化与动态同意模型可实现更具伦理意识的AI实践。

Abstract: This paper critiques digital cloning in academic research, highlighting how
it exemplifies AI solutionism. Digital clones, which replicate user data to
simulate behavior, are often seen as scalable tools for behavioral insights.
However, this framing obscures ethical concerns around consent, agency, and
representation. Drawing on feminist theories of agency, the paper argues that
digital cloning oversimplifies human complexity and risks perpetuating systemic
biases. To address these issues, it proposes decentralized data repositories
and dynamic consent models, promoting ethical, context-aware AI practices that
challenge the reductionist logic of AI solutionism

</details>


### [346] [AI Chatbots for Mental Health: Values and Harms from Lived Experiences of Depression](https://arxiv.org/abs/2504.18932)
*Dong Whi Yoo,Jiayue Melissa Shi,Violeta J. Rodriguez,Koustuv Saha*

Main category: cs.HC

TL;DR: 研究探讨了基于GPT-4的聊天机器人Zenny在抑郁症自我管理中的作用，通过与17名抑郁症患者访谈，揭示了其在信息支持、情感支持等方面的价值及潜在风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在心理健康领域的应用增长，了解其潜在危害及如何与患者价值观关联至关重要，以优化设计并减少风险。

Method: 开发了GPT-4o聊天机器人Zenny，通过模拟抑郁症自我管理场景，访谈17名患者，并进行主题分析。

Result: 分析揭示了五大关键价值：信息支持、情感支持、个性化、隐私和危机管理，并探讨了其与潜在危害的关系。

Conclusion: 研究为心理健康AI聊天机器人设计提供了建议，旨在增强支持同时降低风险，强调患者价值观与技术的结合。

Abstract: Recent advancements in LLMs enable chatbots to interact with individuals on a
range of queries, including sensitive mental health contexts. Despite
uncertainties about their effectiveness and reliability, the development of
LLMs in these areas is growing, potentially leading to harms. To better
identify and mitigate these harms, it is critical to understand how the values
of people with lived experiences relate to the harms. In this study, we
developed a technology probe, a GPT-4o based chatbot called Zenny, enabling
participants to engage with depression self-management scenarios informed by
previous research. We used Zenny to interview 17 individuals with lived
experiences of depression. Our thematic analysis revealed key values:
informational support, emotional support, personalization, privacy, and crisis
management. This work explores the relationship between lived experience
values, potential harms, and design recommendations for mental health AI
chatbots, aiming to enhance self-management support while minimizing risks.

</details>


### [347] [Beyond Levels of Driving Automation: A Triadic Framework of Human-AI Collaboration in On-Road Mobility](https://arxiv.org/abs/2504.19120)
*Gaojian Huang,Yantong Jin,Wei-Hsiang Lo*

Main category: cs.HC

TL;DR: 提出了一种三元人机协作框架，用于动态驾驶场景中人与AI的实时协作。


<details>
  <summary>Details</summary>
Motivation: 现有分类（如SAE自动化等级）仅关注车辆控制者，缺乏对动态驾驶场景中人与AI如何实时协作的指导。

Method: 设计了包含三种AI角色（顾问、副驾驶、守护者）的框架，根据人类需求动态调整角色。

Result: 为自动驾驶车辆中基于角色的自适应人机协作策略奠定了基础。

Conclusion: 三元框架填补了动态场景中人机协作的空白，支持角色灵活切换。

Abstract: The goal of the current study is to introduce a triadic human-AI
collaboration framework for the automated vehicle domain. Previous
classifications (e.g., SAE Levels of Automation) focus on defining automation
levels based on who controls the vehicle. However, it remains unclear how human
users and AI should collaborate in real-time, especially in dynamic driving
contexts, where roles can shift frequently. To fill the gap, this study
proposes a triadic human-AI collaboration framework with three AI roles (i.e.,
Advisor, Co-Pilot, and Guardian) that dynamically adapt to human needs.
Overall, the study lays a foundation for developing adaptive, role-based
human-AI collaboration strategies in automated vehicles.

</details>


### [348] [A Real-Time Gesture-Based Control Framework](https://arxiv.org/abs/2504.19460)
*Mahya Khazaei,Ali Bahrani,George Tzanetakis*

Main category: cs.HC

TL;DR: 论文介绍了一种实时的手势控制框架，通过分析实时视频输入动态调整音频和音乐，实现人机交互。


<details>
  <summary>Details</summary>
Motivation: 旨在为舞蹈、表演等场景提供沉浸式体验，让用户通过动作实时影响音乐。

Method: 整合计算机视觉和机器学习技术，通过手势训练、信号映射和音频处理实现动态交互。

Result: 系统能独立于用户工作，仅需50-80个样本即可识别简单手势，实时调整音乐元素。

Conclusion: 框架实现了人机无缝互动，展示了动作与音乐控制的自然结合。

Abstract: We introduce a real-time, human-in-the-loop gesture control framework that
can dynamically adapt audio and music based on human movement by analyzing live
video input. By creating a responsive connection between visual and auditory
stimuli, this system enables dancers and performers to not only respond to
music but also influence it through their movements. Designed for live
performances, interactive installations, and personal use, it offers an
immersive experience where users can shape the music in real time.
  The framework integrates computer vision and machine learning techniques to
track and interpret motion, allowing users to manipulate audio elements such as
tempo, pitch, effects, and playback sequence. With ongoing training, it
achieves user-independent functionality, requiring as few as 50 to 80 samples
to label simple gestures. This framework combines gesture training, cue
mapping, and audio manipulation to create a dynamic, interactive experience.
Gestures are interpreted as input signals, mapped to sound control commands,
and used to naturally adjust music elements, showcasing the seamless interplay
between human interaction and machine response.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [349] [Periodic Online Testing for Sparse Systolic Tensor Arrays](https://arxiv.org/abs/2504.18628)
*Christodoulos Peltekis,Chrysostomos Nicopoulos,Giorgos Dimitrakopoulos*

Main category: cs.AR

TL;DR: 该论文提出了一种在线错误检测技术，用于在计算开始前检测和定位稀疏脉动张量阵列中的永久性故障，仅需四个测试向量，并通过故障注入验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在安全关键系统中的广泛应用，确保这些系统的可靠性变得至关重要。稀疏脉动张量阵列在加速结构化稀疏ML模型中扮演重要角色，但其可靠性需要得到保障。

Method: 论文介绍了一种基于四个测试向量的在线错误检查技术，利用脉动阵列中已加载的权重值进行全面测试。

Result: 在三个成熟的卷积神经网络（CNN）上进行的故障注入实验表明，该方法能实现极高的故障覆盖率，同时性能和面积开销极小。

Conclusion: 所提出的技术为稀疏脉动张量阵列提供了一种高效、低开销的可靠性保障方案，适用于安全关键的ML应用。

Abstract: Modern Machine Learning (ML) applications often benefit from structured
sparsity, a technique that efficiently reduces model complexity and simplifies
handling of sparse data in hardware. Sparse systolic tensor arrays -
specifically designed to accelerate these structured-sparse ML models - play a
pivotal role in enabling efficient computations. As ML is increasingly
integrated into safety-critical systems, it is of paramount importance to
ensure the reliability of these systems. This paper introduces an online
error-checking technique capable of detecting and locating permanent faults
within sparse systolic tensor arrays before computation begins. The new
technique relies on merely four test vectors and exploits the weight values
already loaded within the systolic array to comprehensively test the system.
Fault-injection campaigns within the gate-level netlist, while executing three
well-established Convolutional Neural Networks (CNN), validate the efficiency
of the proposed approach, which is shown to achieve very high fault coverage,
while incurring minimal performance and area overheads.

</details>


### [350] [NSFlow: An End-to-End FPGA Framework with Scalable Dataflow Architecture for Neuro-Symbolic AI](https://arxiv.org/abs/2504.19323)
*Hanchen Yang,Zishen Wan,Ritik Raj,Joongun Park,Ziwei Li,Ananda Samajdar,Arijit Raychowdhury,Tushar Krishna*

Main category: cs.AR

TL;DR: NSFlow是一种基于FPGA的加速框架，专为Neuro-Symbolic AI设计，通过优化数据流架构和可重构计算单元，显著提升了计算效率，在多种硬件上实现大幅加速。


<details>
  <summary>Details</summary>
Motivation: 当前硬件对Neuro-Symbolic AI任务的执行效率不高，因其计算内核异构、内存访问模式独特且负载类型多样。NSFlow旨在解决这些挑战，提供高效、可扩展的加速方案。

Method: NSFlow包含一个设计架构生成器（识别数据依赖并优化数据流）和一个可重构计算阵列（支持灵活计算单元、内存重组和混合精度）。

Result: 实验显示，NSFlow比Jetson TX2快31倍，比GPU快2倍，比TPU类脉动阵列快8倍，且符号负载扩展150倍时仅增加4倍运行时间。

Conclusion: NSFlow首次实现了实时通用的Neuro-Symbolic AI加速，为下一代认知系统提供了可行方案。

Abstract: Neuro-Symbolic AI (NSAI) is an emerging paradigm that integrates neural
networks with symbolic reasoning to enhance the transparency, reasoning
capabilities, and data efficiency of AI systems. Recent NSAI systems have
gained traction due to their exceptional performance in reasoning tasks and
human-AI collaborative scenarios. Despite these algorithmic advancements,
executing NSAI tasks on existing hardware (e.g., CPUs, GPUs, TPUs) remains
challenging, due to their heterogeneous computing kernels, high memory
intensity, and unique memory access patterns. Moreover, current NSAI algorithms
exhibit significant variation in operation types and scales, making them
incompatible with existing ML accelerators. These challenges highlight the need
for a versatile and flexible acceleration framework tailored to NSAI workloads.
In this paper, we propose NSFlow, an FPGA-based acceleration framework designed
to achieve high efficiency, scalability, and versatility across NSAI systems.
NSFlow features a design architecture generator that identifies workload data
dependencies and creates optimized dataflow architectures, as well as a
reconfigurable array with flexible compute units, re-organizable memory, and
mixed-precision capabilities. Evaluating across NSAI workloads, NSFlow achieves
31x speedup over Jetson TX2, more than 2x over GPU, 8x speedup over TPU-like
systolic array, and more than 3x over Xilinx DPU. NSFlow also demonstrates
enhanced scalability, with only 4x runtime increase when symbolic workloads
scale by 150x. To the best of our knowledge, NSFlow is the first framework to
enable real-time generalizable NSAI algorithms acceleration, demonstrating a
promising solution for next-generation cognitive systems.

</details>


### [351] [Dynamic Tsetlin Machine Accelerators for On-Chip Training at the Edge using FPGAs](https://arxiv.org/abs/2504.19797)
*Gang Mao,Tousif Rahman,Sidharth Maheshwari,Bob Pattison,Zhuang Shao,Rishad Shafik,Alex Yakovlev*

Main category: cs.AR

TL;DR: 摘要提出了一种动态Tsetlin机器（DTM）训练加速器，以替代DNN实现，适用于边缘计算场景，具备高效能和低功耗特点。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习应用对数据隐私和安全需求的增加，边缘训练的需求上升，但DNN在边缘节点上训练面临复杂性和资源限制的挑战，因此需要更高效的替代方案。

Method: 提出基于有限状态自动机的DTM加速器设计，支持运行时动态重构，无需重新综合即可适应不同数据集和模型架构，且采用逻辑推理和高效LUT映射。

Result: 与DNN相比，DTM能减少乘累加操作和导数计算，提高能效，实验结果显示其GOP/s每瓦特提升2.54倍，功耗降低6倍。

Conclusion: DTM是一种适合边缘任务的数据高效机器学习方法，尤其适用于资源受限的物联网节点。

Abstract: The increased demand for data privacy and security in machine learning (ML)
applications has put impetus on effective edge training on Internet-of-Things
(IoT) nodes. Edge training aims to leverage speed, energy efficiency and
adaptability within the resource constraints of the nodes. Deploying and
training Deep Neural Networks (DNNs)-based models at the edge, although
accurate, posit significant challenges from the back-propagation algorithm's
complexity, bit precision trade-offs, and heterogeneity of DNN layers. This
paper presents a Dynamic Tsetlin Machine (DTM) training accelerator as an
alternative to DNN implementations. DTM utilizes logic-based on-chip inference
with finite-state automata-driven learning within the same Field Programmable
Gate Array (FPGA) package. Underpinned on the Vanilla and Coalesced Tsetlin
Machine algorithms, the dynamic aspect of the accelerator design allows for a
run-time reconfiguration targeting different datasets, model architectures, and
model sizes without resynthesis. This makes the DTM suitable for targeting
multivariate sensor-based edge tasks. Compared to DNNs, DTM trains with fewer
multiply-accumulates, devoid of derivative computation. It is a data-centric ML
algorithm that learns by aligning Tsetlin automata with input data to form
logical propositions enabling efficient Look-up-Table (LUT) mapping and frugal
Block RAM usage in FPGA training implementations. The proposed accelerator
offers 2.54x more Giga operations per second per Watt (GOP/s per W) and uses 6x
less power than the next-best comparable design.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [352] [Learning Stochastic Thermodynamics Directly from Correlation and Trajectory-Fluctuation Currents](https://arxiv.org/abs/2504.19007)
*Jinghao Lyu,Kyle J. Ray,James P. Crutchfield*

Main category: cond-mat.stat-mech

TL;DR: 该论文提出了一种基于朗之万动力学的数据驱动逆动力学问题学习框架，利用电流构建了随机建模的损失函数，并揭示了机器学习扩散模型与随机热力学熵产生估计之间的深刻联系。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力和数据获取能力的显著提升，数据驱动的逆动力学问题研究日益受到关注。如何从复杂动力学系统的时间序列测量中提取有效信息，尤其是对于受噪声影响的小系统至关重要。本文旨在探索朗之万动力学的随机建模，并利用电流构建学习框架。

Method: 论文利用电流与熵产生的关系，直接从系统动力学推导出损失函数，避免了传统热力学不确定性关系（TUR）的中间步骤。这一方法不仅适用于稳态系统，还能处理远离稳态的驱动系统。此外，论文还探讨了高阶估计的问题。

Result: 提出的损失函数不仅复现了基于TUR和其他方法的结果，还为之前无法直接计算的量（如单轨迹熵产生）提供了新的计算路径。该方法将动态推断与熵产生估计统一起来，展示了机器学习扩散模型与随机热力学熵产生估计的深刻联系。

Conclusion: 通过将电流与机器学习损失函数结合，论文为随机动力学建模和熵产生估计提供了一种高效、统一的方法，为未来研究开辟了新的方向。

Abstract: Markedly increased computational power and data acquisition have led to
growing interest in data-driven inverse dynamics problems. These seek to answer
a fundamental question: What can we learn from time series measurements of a
complex dynamical system? For small systems interacting with external
environments, the effective dynamics are inherently stochastic, making it
crucial to properly manage noise in data. Here, we explore this for systems
obeying Langevin dynamics and, using currents, we construct a learning
framework for stochastic modeling. Currents have recently gained increased
attention for their role in bounding entropy production (EP) from thermodynamic
uncertainty relations (TURs). We introduce a fundamental relationship between
the cumulant currents there and standard machine-learning loss functions. Using
this, we derive loss functions for several key thermodynamic functions directly
from the system dynamics without the (common) intermediate step of deriving a
TUR. These loss functions reproduce results derived both from TURs and other
methods. More significantly, they open a path to discover new loss functions
for previously inaccessible quantities. Notably, this includes access to
per-trajectory entropy production, even if the observed system is driven far
from its steady-state. We also consider higher order estimation. Our method is
straightforward and unifies dynamic inference with recent approaches to entropy
production estimation. Taken altogether, this reveals a deep connection between
diffusion models in machine learning and entropy production estimation in
stochastic thermodynamics.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [353] [QFGN: A Quantum Approach to High-Fidelity Implicit Neural Representations](https://arxiv.org/abs/2504.19053)
*Hongni Jin,Gurinder Singh,Kenneth M. Merz Jr*

Main category: quant-ph

TL;DR: 该论文提出了一种基于量子机器学习的Quantum Fourier Gaussian Network (QFGN)，用于改进信号表示和图像超分辨率。尽管硬件存在噪声，QFGN在参数较少的情况下表现优于当前SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 现有隐式神经表示方法在图像超分辨率中难以准确重建或提供清晰细节，需要更高效的信号表示方法。

Method: 采用量子傅里叶高斯网络(QFGN)，通过惩罚低频分量平衡频谱，提升量子电路的表达能力。

Result: QFGN在少量参数下优于现有SOTA模型，噪声环境下仍能达到与SIREN相当的精度。

Conclusion: QFGN展示了量子机器学习在该领域的应用潜力，为信号表示和图像超分辨率提供了新思路。

Abstract: Implicit neural representations have shown potential in various applications.
However, accurately reconstructing the image or providing clear details via
image super-resolution remains challenging. This paper introduces Quantum
Fourier Gaussian Network (QFGN), a quantum-based machine learning model for
better signal representations. The frequency spectrum is well balanced by
penalizing the low-frequency components, leading to the improved expressivity
of quantum circuits. The results demonstrate that with minimal parameters, QFGN
outperforms the current state-of-the-art (SOTA) models. Despite noise on
hardware, the model achieves accuracy comparable to that of SIREN, highlighting
the potential applications of quantum machine learning in this field.

</details>


### [354] [Inverse-Transpilation: Reverse-Engineering Quantum Compiler Optimization Passes from Circuit Snapshots](https://arxiv.org/abs/2504.19113)
*Satwik Kundu,Swaroop Ghosh*

Main category: quant-ph

TL;DR: 论文提出了一种基于机器学习的框架，用于通过比较原始和编译后的量子电路结构差异来推断底层优化技术，旨在提升编译透明度和识别商业系统的知识产权保护优化。


<details>
  <summary>Details</summary>
Motivation: 增强量子电路优化的透明度以改进跨平台调试和性能调优，同时识别商业系统中可能受知识产权保护的优化技术。

Method: 采用简单的机器学习框架，利用神经网络分析原始和编译后量子电路的结构差异，以推断优化技术。

Result: 在大量量子电路上的评估显示，神经网络在检测优化步骤方面表现最佳，单个优化步骤的F1分数高达0.96。

Conclusion: 研究表明这种威胁对编译器保密性的可行性，并强调需要在这一领域开展进一步研究。

Abstract: Circuit compilation, a crucial process for adapting quantum algorithms to
hardware constraints, often operates as a ``black box,'' with limited
visibility into the optimization techniques used by proprietary systems or
advanced open-source frameworks. Due to fundamental differences in qubit
technologies, efficient compiler design is an expensive process, further
exposing these systems to various security threats. In this work, we take a
first step toward evaluating one such challenge affecting compiler
confidentiality, specifically, reverse-engineering compilation methodologies.
We propose a simple ML-based framework to infer underlying optimization
techniques by leveraging structural differences observed between original and
compiled circuits. The motivation is twofold: (1) enhancing transparency in
circuit optimization for improved cross-platform debugging and performance
tuning, and (2) identifying potential intellectual property (IP)-protected
optimizations employed by commercial systems. Our extensive evaluation across
thousands of quantum circuits shows that a neural network performs the best in
detecting optimization passes, with individual pass F1-scores reaching as high
as 0.96. Thus, our initial study demonstrates the viability of this threat to
compiler confidentiality and underscores the need for active research in this
area.

</details>


### [355] [The effect of the number of parameters and the number of local feature patches on loss landscapes in distributed quantum neural networks](https://arxiv.org/abs/2504.19239)
*Yoshiaki Kawase*

Main category: quant-ph

TL;DR: 该论文提出了一种分布式量子神经网络架构，通过将数据分割为局部块并独立处理，以缓解优化问题（如贫瘠高原和局部极小值），并通过Hessian分析和损失景观可视化验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 量子神经网络在处理经典数据时面临优化难题（如贫瘠高原和局部极小值），限制了实际应用。研究旨在通过分布式块处理方法提升优化稳定性。

Method: 采用分布式块策略，将数据分割为重叠局部块，由独立量子神经网络处理，并聚合输出。通过Hessian分析和损失景观可视化研究参数与块数量的影响。

Result: 增加参数会加深损失景观，而增加块数量显著降低Hessian最大特征值，表明该方法具有隐式正则化效果，提升优化稳定性和泛化能力。

Conclusion: 分布式块策略是解决量子神经网络优化问题的有效方法，为经典数据任务中的量子机器学习提供了更可行的训练方案。

Abstract: Quantum neural networks hold promise for tackling computationally challenging
tasks that are intractable for classical computers. However, their practical
application is hindered by significant optimization challenges, arising from
complex loss landscapes characterized by barren plateaus and numerous local
minima. These problems become more severe as the number of parameters or qubits
increases, hampering effective training. To mitigate these optimization
challenges, particularly for quantum machine learning applied to classical
data, we employ an approach of distributing overlapping local patches across
multiple quantum neural networks, processing each patch with an independent
quantum neural network, and aggregating their outputs for prediction. In this
study, we investigate how the number of parameters and patches affects the loss
landscape geometry of this distributed quantum neural network architecture via
Hessian analysis and loss landscape visualization. Our results confirm that
increasing the number of parameters tends to lead to deeper and sharper loss
landscapes. Crucially, we demonstrate that increasing the number of patches
significantly reduces the largest Hessian eigenvalue at minima. This finding
suggests that our distributed patch approach acts as a form of implicit
regularization, promoting optimization stability and potentially enhancing
generalization. Our study provides valuable insights into optimization
challenges and highlights that the distributed patch approach is a promising
strategy for developing more trainable and practical quantum machine learning
models for classical data tasks.

</details>


### [356] [QFDNN: A Resource-Efficient Variational Quantum Feature Deep Neural Networks for Fraud Detection and Loan Prediction](https://arxiv.org/abs/2504.19632)
*Subham Das,Ashtakala Meghanath,Bikash K. Behera,Shahid Mumtaz,Saif Al-Kuwari,Ahmed Farouk*

Main category: quant-ph

TL;DR: 该论文提出了一种名为QFDNN的新型量子特征深度神经网络，旨在解决量子计算在金融科技领域中遇到的噪声环境和特征优化问题。QFDNN在信用卡欺诈检测和贷款资格预测任务中表现出色，准确率分别为82.2%和74.4%，同时具有资源高效和噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在处理金融数据时面临可扩展性、过拟合和高计算成本等问题；现有量子算法在噪声环境中鲁棒性不足且难以优化特征。社会金融科技需要高效率和鲁棒的解决方案以增强信任和安全性。

Method: 提出了一种资源高效且噪声鲁棒的量子特征深度神经网络（QFDNN），通过优化特征表示并使用更少的量子比特和更简单的变分电路来降低计算开销。

Result: 在信用卡欺诈检测和贷款资格预测任务中，QFDNN分别取得了82.2%和74.4%的准确率，并在六种噪声模型中表现出良好的鲁棒性。

Conclusion: QFDNN在金融科技中展现出潜力，能够高效准确地检测欺诈交易，同时通过其资源高效的设计支持可持续发展。

Abstract: Social financial technology focuses on trust, sustainability, and social
responsibility, which require advanced technologies to address complex
financial tasks in the digital era. With the rapid growth in online
transactions, automating credit card fraud detection and loan eligibility
prediction has become increasingly challenging. Classical machine learning (ML)
models have been used to solve these challenges; however, these approaches
often encounter scalability, overfitting, and high computational costs due to
complexity and high-dimensional financial data. Quantum computing (QC) and
quantum machine learning (QML) provide a promising solution to efficiently
processing high-dimensional datasets and enabling real-time identification of
subtle fraud patterns. However, existing quantum algorithms lack robustness in
noisy environments and fail to optimize performance with reduced feature sets.
To address these limitations, we propose a quantum feature deep neural network
(QFDNN), a novel, resource efficient, and noise-resilient quantum model that
optimizes feature representation while requiring fewer qubits and simpler
variational circuits. The model is evaluated using credit card fraud detection
and loan eligibility prediction datasets, achieving competitive accuracies of
82.2% and 74.4%, respectively, with reduced computational overhead.
Furthermore, we test QFDNN against six noise models, demonstrating its
robustness across various error conditions. Our findings highlight QFDNN
potential to enhance trust and security in social financial technology by
accurately detecting fraudulent transactions while supporting sustainability
through its resource-efficient design and minimal computational overhead.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [357] [Versatile Framework for Song Generation with Prompt-based Control](https://arxiv.org/abs/2504.19062)
*Yu Zhang,Wenxiang Guo,Changhao Pan,Zhiyuan Zhu,Ruiqi Li,Jingyu Lu,Rongjie Huang,Ruiyuan Zhang,Zhiqing Hong,Ziyue Jiang,Zhou Zhao*

Main category: eess.AS

TL;DR: VersBand是一个多任务歌曲生成框架，通过VocalBand和AccompBand等模型实现高质量、对齐的歌曲生成，支持基于提示的多任务控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成语音和伴奏时缺乏基于提示的控制和正确对齐，且不支持多种任务。

Method: VersBand包含VocalBand（基于流匹配生成歌声）和AccompBand（基于流的变压器模型生成伴奏），以及LyricBand和MelodyBand。

Result: 实验表明，VersBand在多种歌曲生成任务中表现优于基线模型。

Conclusion: VersBand通过多任务框架实现了高质量、可控的歌曲生成，解决了现有方法的不足。

Abstract: Song generation focuses on producing controllable high-quality songs based on
various prompts. However, existing methods struggle to generate vocals and
accompaniments with prompt-based control and proper alignment. Additionally,
they fall short in supporting various tasks. To address these challenges, we
introduce VersBand, a multi-task song generation framework for synthesizing
high-quality, aligned songs with prompt-based control. VersBand comprises these
primary models: 1) VocalBand, a decoupled model, leverages the flow-matching
method for generating singing styles, pitches, and mel-spectrograms, allowing
fast, high-quality vocal generation with style control. 2) AccompBand, a
flow-based transformer model, incorporates the Band-MOE, selecting suitable
experts for enhanced quality, alignment, and control. This model allows for
generating controllable, high-quality accompaniments aligned with vocals. 3)
Two generation models, LyricBand for lyrics and MelodyBand for melodies,
contribute to the comprehensive multi-task song generation system, allowing for
extensive control based on multiple prompts. Experimental results demonstrate
that VersBand performs better over baseline models across multiple song
generation tasks using objective and subjective metrics. Audio samples are
available at https://VersBand.github.io.

</details>


### [358] [Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation](https://arxiv.org/abs/2504.18539)
*Sungnyun Kim,Sungwoo Cho,Sangmin Bae,Kangwook Jang,Se-Young Yun*

Main category: eess.AS

TL;DR: CAV2vec是一种自监督语音表示学习框架，旨在处理音频-视觉联合损坏问题，通过自蒸馏方法提升在多损坏环境下的语音识别准确性。


<details>
  <summary>Details</summary>
Motivation: 在嘈杂环境中，音频-视觉语音识别（AVSR）因视觉损坏（如唇部遮挡或视频模糊）难以处理，导致识别性能下降。研究旨在解决这一现实挑战。

Method: 提出CAV2vec，通过自蒸馏方法训练学生模型预测教师模型生成的干净目标，同时采用单模态多任务学习对齐损坏模态。

Result: 在多种损坏类型的AVSR基准测试中，该方法显著提升了识别准确性。

Conclusion: CAV2vec通过自蒸馏和跨模态知识传递，有效增强了AVSR在复杂损坏环境中的鲁棒性。

Abstract: Audio-visual speech recognition (AVSR) incorporates auditory and visual
modalities to improve recognition accuracy, particularly in noisy environments
where audio-only speech systems are insufficient. While previous research has
largely addressed audio disruptions, few studies have dealt with visual
corruptions, e.g., lip occlusions or blurred videos, which are also
detrimental. To address this real-world challenge, we propose CAV2vec, a novel
self-supervised speech representation learning framework particularly designed
to handle audio-visual joint corruption. CAV2vec employs a self-distillation
approach with a corrupted prediction task, where the student model learns to
predict clean targets, generated by the teacher model, with corrupted input
frames. Specifically, we suggest a unimodal multi-task learning, which distills
cross-modal knowledge and aligns the corrupted modalities, by predicting clean
audio targets with corrupted videos, and clean video targets with corrupted
audios. This strategy mitigates the dispersion in the representation space
caused by corrupted modalities, leading to more reliable and robust
audio-visual fusion. Our experiments on robust AVSR benchmarks demonstrate that
the corrupted representation learning method significantly enhances recognition
accuracy across generalized environments involving various types of corruption.

</details>


### [359] [Enhancing Cochlear Implant Signal Coding with Scaled Dot-Product Attention](https://arxiv.org/abs/2504.19046)
*Billel Essaid,Hamza Kheddar,Noureddine Batel*

Main category: eess.AS

TL;DR: TL;DR: 该论文研究了使用深度学习技术为人工耳蜗生成电码图, 其性能接近传统ACE方法, 并展示了更高的灵活性和适应性潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨深度学习技术在人工耳蜗编码中的应用, 弥补传统方法在适应性和精确性上的不足。

Method: 比较深度学习生成的电码图与ACE策略, 使用STOI评分评估音频信号的可懂度。

Result: 模型STOI得分为0.6031, 接近ACE得分0.6126, 且在灵活性和适应性上更具潜力。

Conclusion: 研究证实AI技术能为人工耳蜗带来更个性化和高效的性能提升。

Abstract: Cochlear implants (CIs) play a vital role in restoring hearing for
individuals with severe to profound sensorineural hearing loss by directly
stimulating the auditory nerve with electrical signals. While traditional
coding strategies, such as the advanced combination encoder (ACE), have proven
effective, they are constrained by their adaptability and precision. This paper
investigates the use of deep learning (DL) techniques to generate
electrodograms for CIs, presenting our model as an advanced alternative. We
compared the performance of our model with the ACE strategy by evaluating the
intelligibility of reconstructed audio signals using the short-time objective
intelligibility (STOI) metric. The results indicate that our model achieves a
STOI score of 0.6031, closely approximating the 0.6126 score of the ACE
strategy, and offers potential advantages in flexibility and adaptability. This
study underscores the benefits of incorporating artificial intelligent (AI)
into CI technology, such as enhanced personalization and efficiency.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [360] [Two-parameter superposable S-curves](https://arxiv.org/abs/2504.19488)
*Vijay Prakash S*

Main category: stat.ME

TL;DR: 论文提出S形曲线（S-curves）可作为统计模型，从均匀分布到退化分布描述数据模式，并应用于鸢尾花数据分类，但参数估计对初始条件敏感。


<details>
  <summary>Details</summary>
Motivation: 研究S形曲线是否能作为统计模型，从最大熵均匀分布过渡到零熵单值分布，并验证其在自然系统（如生物生长和酶反应动力学）中的适用性。

Method: 通过奇异扰动直线方程$y=mx$生成S形曲线，分析其叠加性，并将其作为统计模型拟合经典鸢尾花数据集。

Result: S形曲线能有效捕捉非均匀模式，但在参数估计时对初始条件敏感，数据依赖性较强。

Conclusion: S形曲线可作为描述非均匀模式的统计模型，但需改进参数估计方法以减少对初始条件的敏感性。

Abstract: Straight line equation $y=mx$ with slope $m$, when singularly perturbed as
$ay^3+y=mx$ with a positive parameter $a$, results in S-shaped curves or
S-curves on a real plane. As $a\rightarrow 0$, we get back $y=mx$ which is a
cumulative distribution function of a continuous uniform distribution that
describes the occurrence of every event in an interval to be equally probable.
As $a\rightarrow\infty$, the derivative of $y$ has finite support only at $y=0$
resembling a degenerate distribution. Based on these arguments, in this work,
we propose that these S-curves can represent maximum entropy uniform
distribution to a zero entropy single value. We also argue that these S-curves
are superposable as they are only parametrically nonlinear but fundamentally
linear. So far, the superposed forms have been used to capture the patterns of
natural systems such as nonlinear dynamics of biological growth and kinetics of
enzyme reactions. Here, we attempt to use the S-curve and its superposed form
as a statistical model. We fit the models on a classical dataset containing
flower measurements of iris plants and analyze their usefulness in pattern
recognition. Based on these models, we claim that any non-uniform pattern can
be represented as a singular perturbation to uniform distribution. However, our
parametric estimation procedure have some limitations such as sensitivity to
initial conditions depending on the data at hand.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [361] [On Stopping Times of Power-one Sequential Tests: Tight Lower and Upper Bounds](https://arxiv.org/abs/2504.19952)
*Shubhada Agrawal,Aaditya Ramdas*

Main category: math.ST

TL;DR: 论文证明了在广义复合零假设和替代假设之间顺序测试停止时间的两个下界。第一个下界适用于类型1错误水平α趋近于零的情形，结果为log(1/α)除以一个特定的KL散度下界。第二个下界适用于α固定且KL散度下界趋近于零的情形，结果为c·KL_inf^{-1}·loglog(KL_inf^{-1})。论文还提供了匹配上界的充分条件，并展示了该条件在多个特殊情况下成立。主要贡献在于这些结果的广泛适用性，例如无需参考测度或类的紧性。


<details>
  <summary>Details</summary>
Motivation: 研究广义复合假设下顺序测试停止时间的下界，扩展现有结果的适用性，无需依赖参考测度或紧性假设。

Method: 证明了两个下界定理：第一个针对α趋近于零的情形，第二个针对KL_inf趋近于零的情形。同时提供了匹配上界的充分条件，并验证其在多个特例中的适用性。

Result: 得到了两个下界表达式，并证明了它们在广泛条件下的适用性，包括无需参考测度或紧性假设。

Conclusion: 论文通过广泛适用的下界结果扩展了顺序测试停止时间的理论框架，为复合假设下的统计推断提供了更通用的工具。

Abstract: We prove two lower bounds for stopping times of sequential tests between
general composite nulls and alternatives. The first lower bound is for the
setting where the type-1 error level $\alpha$ approaches zero, and equals
$\log(1/\alpha)$ divided by a certain infimum KL divergence, termed
$\operatorname{KL_{inf}}$. The second lower bound applies to the setting where
$\alpha$ is fixed and $\operatorname{KL_{inf}}$ approaches 0 (meaning that the
null and alternative sets are not separated) and equals $c
\operatorname{KL_{inf}}^{-1} \log \log \operatorname{KL_{inf}}^{-1}$ for a
universal constant $c > 0$. We also provide a sufficient condition for matching
the upper bounds and show that this condition is met in several special cases.
Given past work, these upper and lower bounds are unsurprising in their form;
our main contribution is the generality in which they hold, for example, not
requiring reference measures or compactness of the classes.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [362] [Explainable Deep-Learning Based Potentially Hazardous Asteroids Classification Using Graph Neural Networks](https://arxiv.org/abs/2504.18605)
*Baimam Boukar Jean Jacques*

Main category: astro-ph.EP

TL;DR: 该研究提出了一种基于图神经网络（GNN）的方法，用于分类潜在危险小行星（PHAs），解决了传统方法忽视小行星间动力学关系的问题，并在NASA数据集上取得了高精度和AUC值。


<details>
  <summary>Details</summary>
Motivation: 为了提升行星防御和深空导航的准确性，研究旨在克服传统方法在处理小行星间动力学关系和类别不平衡问题上的不足。

Method: 采用图神经网络（GNN）模型，将小行星表示为节点，基于轨道和物理特征，通过相似性边连接，并使用合成少数类过采样技术（SMOTE）处理类别不平衡。

Result: 模型在NASA数据集（958,524条记录）上总体准确率达99%，AUC为0.99，对危险小行星的召回率为78%，F1分数为37%，主要预测因子为反照率、近日点距离和半长轴。

Conclusion: 该框架为行星防御任务和自主导航（如NASA的NEO Surveyor和ESA的Ramses）提供了可解释、可扩展的解决方案，展示了AI在小行星风险评估中的潜力。

Abstract: Classifying potentially hazardous asteroids (PHAs) is crucial for planetary
defense and deep space navigation, yet traditional methods often overlook the
dynamical relationships among asteroids. We introduce a Graph Neural Network
(GNN) approach that models asteroids as nodes with orbital and physical
features, connected by edges representing their similarities, using a NASA
dataset of 958,524 records. Despite an extreme class imbalance with only 0.22%
of the dataset with the hazardous label, our model achieves an overall accuracy
of 99% and an AUC of 0.99, with a recall of 78% and an F1-score of 37% for
hazardous asteroids after applying the Synthetic Minority Oversampling
Technique. Feature importance analysis highlights albedo, perihelion distance,
and semi-major axis as main predictors. This framework supports planetary
defense missions and confirms AI's potential in enabling autonomous navigation
for future missions such as NASA's NEO Surveyor and ESA's Ramses, offering an
interpretable and scalable solution for asteroid hazard assessment.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [363] [Metric Similarity and Manifold Learning of Circular Dichroism Spectra of Proteins](https://arxiv.org/abs/2504.19355)
*Gionni Marchetti*

Main category: physics.soc-ph

TL;DR: 论文通过机器学习和最优运输距离分析SP175数据库中的蛋白质圆二色光谱，证明$\mathcal{W}_1$距离在噪声环境下表现稳健，而$t$-SNE揭示了高维数据中蛋白质的二级结构聚类特征。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证$\mathcal{W}_1$距离在蛋白质光谱分析中的稳健性，并利用降维方法$t$-SNE揭示蛋白质二级结构的潜在聚类模式。

Method: 采用$\mathcal{W}_1$距离（$p=1$阶）和$t$-SNE流形学习算法分析SP175数据库中的蛋白质圆二色光谱数据。

Result: $\mathcal{W}_1$距离与欧几里得和曼哈顿度量一致且抗噪，$t$-SNE嵌入显示蛋白质按二级结构（主要为$\beta$-富集和$\alpha/\beta$混合型）聚类。

Conclusion: 研究证明了$\mathcal{W}_1$在光谱分析中的实用性，同时$t$-SNE有效揭示了蛋白质结构的天然聚类特征。

Abstract: We present a machine learning analysis of circular dichroism spectra of
globular proteins from the SP175 database, using the optimal transport-based
$1$-Wasserstein distance $\mathcal{W}_1$ (with order $p=1$) and the manifold
learning algorithm $t$-SNE. Our results demonstrate that $\mathcal{W}_1$ is
consistent with both Euclidean and Manhattan metrics while exhibiting
robustness to noise. On the other hand, $t$-SNE uncovers meaningful structure
in the high-dimensional data. The clustering in the $t$-SNE embedding is
primarily determined by proteins with distinct secondary structure
compositions: one cluster predominantly contains $\beta$-rich proteins, while
the other consists mainly of proteins with mixed $\alpha/\beta$ and
$\alpha$-helical content.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [364] [Neuronal correlations shape the scaling behavior of memory capacity and nonlinear computational capability of recurrent neural networks](https://arxiv.org/abs/2504.19657)
*Shotaro Takasu,Toshio Aoyagi*

Main category: cond-mat.dis-nn

TL;DR: 该研究表明，储层循环神经网络的记忆容量随读出神经元数量呈次线性增长，并揭示了神经元相关性对记忆和非线性计算能力的影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探究储层计算框架中记忆容量的增长规律及其与神经元相关性的关系，为设计可扩展且经济的储层计算奠定基础。

Method: 通过理论框架分析记忆容量，结合数值模拟验证次线性增长现象，并研究神经元相关性对非线性计算能力的影响。

Result: 发现记忆容量随读出神经元数量次线性增长，且神经元相关性驱动非线性计算能力的逐步提升。

Conclusion: 研究为储层计算的设计提供了新视角，揭示了神经元相关性、线性记忆与非线性处理之间的相互作用。

Abstract: Reservoir computing is a powerful framework for real-time information
processing, characterized by its high computational ability and quick learning,
with applications ranging from machine learning to biological systems. In this
paper, we demonstrate that the memory capacity of a reservoir recurrent neural
network scales sublinearly with the number of readout neurons. To elucidate
this phenomenon, we develop a theoretical framework for analytically deriving
memory capacity, attributing the decaying growth of memory capacity to neuronal
correlations. In addition, numerical simulations reveal that once memory
capacity becomes sublinear, increasing the number of readout neurons
successively enables nonlinear processing at progressively higher polynomial
orders. Furthermore, our theoretical framework suggests that neuronal
correlations govern not only memory capacity but also the sequential growth of
nonlinear computational capabilities. Our findings establish a foundation for
designing scalable and cost-effective reservoir computing, providing novel
insights into the interplay among neuronal correlations, linear memory, and
nonlinear processing.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [365] [GPU accelerated program synthesis: Enumerate semantics, not syntax!](https://arxiv.org/abs/2504.18943)
*Martin Berger,Nathanaël Fijalkow,Mojtaba Valizadeh*

Main category: cs.PL

TL;DR: 本文探讨了如何利用GPU优化程序合成技术，通过减少数据移动和依赖分支，显著提升了合成器的性能和规模。


<details>
  <summary>Details</summary>
Motivation: 随着GPU在深度学习中表现出色，研究者希望将其应用于程序合成，以提升性能。

Method: 采用GPU友好的编程技术，利用公式语义减少数据移动和依赖分支。

Result: GPU合成的性能显著优于CPU，能够处理更大规模的问题。

Conclusion: 该方法不仅适用于程序合成，还可能提升其他形式化方法的性能。

Abstract: Program synthesis is an umbrella term for generating programs and logical
formulae from specifications. With the remarkable performance improvements that
GPUs enable for deep learning, a natural question arose: can we also implement
a search-based program synthesiser on GPUs to achieve similar performance
improvements? In this article we discuss our insights on this question, based
on recent works~. The goal is to build a synthesiser running on GPUs which
takes as input positive and negative example traces and returns a logical
formula accepting the positive and rejecting the negative traces. With
GPU-friendly programming techniques -- using the semantics of formulae to
minimise data movement and reduce data-dependent branching -- our synthesiser
scales to significantly larger synthesis problems, and operates much faster
than the previous CPU-based state-of-the-art. We believe the insights that make
our approach GPU-friendly have wide potential for enhancing the performance of
other formal methods (FM) workloads.

</details>


### [366] [Rulebook: bringing co-routines to reinforcement learning environments](https://arxiv.org/abs/2504.19625)
*Massimo Fioravanti,Samuele Pasini,Giovanni Agosta*

Main category: cs.PL

TL;DR: Rulebook是一种新的领域特定语言，旨在通过协程自动生成与机器学习算法交互所需的状态机，降低开发成本。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习算法依赖外部系统，导致数字环境的实现受限于同步、通信和代码结构化问题。

Method: 提出Rulebook，一种基于协程的编译语言，自动生成状态机，解耦程序执行模型与语法编码。

Result: Rulebook成功实现了无性能开销的ML接口，支持更复杂环境的低成本开发。

Conclusion: Rulebook通过简化状态管理，提升了开发效率，适用于复杂ML环境的构建。

Abstract: Reinforcement learning (RL) algorithms, due to their reliance on external
systems to learn from, require digital environments (e.g., simulators) with
very simple interfaces, which in turn constrain significantly the
implementation of such environments. In particular, these environments are
implemented either as separate processes or as state machines, leading to
synchronization and communication overheads in the first case, and to
unstructured programming in the second.
  We propose a new domain-specific, co-routine-based, compiled language, called
Rulebook, designed to automatically generate the state machine required to
interact with machine learning (ML) algorithms and similar applications, with
no performance overhead. Rulebook allows users to express programs without
needing to be aware of the specific interface required by the ML components. By
decoupling the execution model of the program from the syntactical encoding of
the program, and thus without the need for manual state management, Rulebook
allows to create larger and more sophisticated environments at a lower
development cost.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [367] [PhenoAssistant: A Conversational Multi-Agent AI System for Automated Plant Phenotyping](https://arxiv.org/abs/2504.19818)
*Feng Chen,Ilias Stogiannidis,Andrew Wood,Danilo Bueno,Dominic Williams,Fraser Macfarlane,Bruce Grieve,Darren Wells,Jonathan A. Atkinson,Malcolm J. Hawkesford,Stephen A. Rolfe,Tracy Lawson,Tony Pridmore,Mario Valerio Giuffrida,Sotirios A. Tsaftaris*

Main category: cs.MA

TL;DR: 本文介绍了PhenoAssistant，一个基于AI的系统，通过自然语言交互简化植物表型分析，降低技术门槛，提升效率和可及性。


<details>
  <summary>Details</summary>
Motivation: 现有植物表型分析工具复杂且难以维护，技术门槛高，限制了非专业用户的使用。为了解决这些问题，作者开发了PhenoAssistant。

Method: PhenoAssistant利用大语言模型协调一套工具包，支持自动化表型提取、数据可视化和模型训练，并通过案例研究和评估任务验证其效果。

Result: PhenoAssistant在多个案例中表现出色，有效降低了技术门槛，证明了AI在植物生物学中的普及潜力。

Conclusion: PhenoAssistant展示了AI驱动方法在植物表型分析中的前景，通过简化流程，推动了AI在生物领域的广泛应用。

Abstract: Plant phenotyping increasingly relies on (semi-)automated image-based
analysis workflows to improve its accuracy and scalability. However, many
existing solutions remain overly complex, difficult to reimplement and
maintain, and pose high barriers for users without substantial computational
expertise. To address these challenges, we introduce PhenoAssistant: a
pioneering AI-driven system that streamlines plant phenotyping via intuitive
natural language interaction. PhenoAssistant leverages a large language model
to orchestrate a curated toolkit supporting tasks including automated phenotype
extraction, data visualisation and automated model training. We validate
PhenoAssistant through several representative case studies and a set of
evaluation tasks. By significantly lowering technical hurdles, PhenoAssistant
underscores the promise of AI-driven methodologies to democratising AI adoption
in plant biology.

</details>


### [368] [Diffusion Stochastic Learning Over Adaptive Competing Networks](https://arxiv.org/abs/2504.19635)
*Yike Zhao,Haoyuan Cai,Ali H. Sayed*

Main category: cs.MA

TL;DR: 研究了两个竞争团队间随机动态博弈，其中团队成员协作但目标对立；提出扩散学习算法应对弱/强跨团队交互情景，理论分析和实验验证了其稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究在竞争环境中，协作团队间如何通过信息共享与对手策略推断实现目标优化，填补了团队对抗动态博弈的研究空白。

Method: 提出扩散学习算法，针对零和（弱交互）与非零和（强交互）两类团队博弈设计解决方案。

Result: 理论分析表明算法在合理假设下具有稳定性，Cournot团队竞争和去中心化GAN训练的实验验证了有效性。

Conclusion: 扩散学习算法能有效解决协作团队间的动态对抗问题，为实际应用（如经济竞争、多智能体对抗）提供了理论工具。

Abstract: This paper studies a stochastic dynamic game between two competing teams,
each consisting of a network of collaborating agents. Unlike fully cooperative
settings, where all agents share a common objective, each team in this game
aims to minimize its own distinct objective. In the adversarial setting, their
objectives could be conflicting as in zero-sum games. Throughout the
competition, agents share strategic information within their own team while
simultaneously inferring and adapting to the strategies of the opposing team.
We propose diffusion learning algorithms to address two important classes of
this network game: i) a zero-sum game characterized by weak cross-team subgraph
interactions, and ii) a general non-zero-sum game exhibiting strong cross-team
subgraph interactions. We analyze the stability performance of the proposed
algorithms under reasonable assumptions and illustrate the theoretical results
through experiments on Cournot team competition and decentralized GAN training.

</details>
