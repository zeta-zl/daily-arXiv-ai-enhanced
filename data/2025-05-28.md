<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 142]
- [cs.LG](#cs.LG) [Total: 139]
- [cs.AI](#cs.AI) [Total: 43]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 3]
- [cs.MA](#cs.MA) [Total: 4]
- [astro-ph.CO](#astro-ph.CO) [Total: 2]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.SE](#cs.SE) [Total: 7]
- [stat.ML](#stat.ML) [Total: 10]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.GR](#cs.GR) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [cs.RO](#cs.RO) [Total: 8]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.DL](#cs.DL) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [eess.IV](#eess.IV) [Total: 4]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.DC](#cs.DC) [Total: 2]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.DB](#cs.DB) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [eess.SY](#eess.SY) [Total: 3]
- [math.CT](#math.CT) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.SD](#cs.SD) [Total: 11]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 44]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs](https://arxiv.org/abs/2505.20309)
*Amr Hegazy,Mostafa Elhoushi,Amr Alanwar*

Key words: LLM, 推理时控制, 激活引导, 安全性能, 轻量级控制器

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出一种轻量级可训练控制器网络，动态调整LLM激活以实现细粒度、自适应的推理时控制，显著提升安全性能。

Motivation: 现有方法在控制LLM生成不安全内容时依赖高成本微调，且缺乏细粒度和自适应机制。

Method: 通过集成轻量级控制器网络，动态预测全局缩放因子和层特定权重，调整预计算的‘拒绝方向’向量强度。

Result: 在ToxicChat等安全基准测试中，拒绝率显著提升，且优于现有方法（如Llama-3.1-8B等模型）。

Conclusion: 该方法无需修改原模型参数，实现了高效、自适应的LLM行为控制。

Abstract: Controlling undesirable Large Language Model (LLM) behaviors, such as the
generation of unsafe content or failing to adhere to safety guidelines, often
relies on costly fine-tuning. Activation steering provides an alternative for
inference-time control, but existing methods typically lack fine-grained,
adaptive mechanisms. We introduce a novel approach using a lightweight,
trainable controller network integrated during inference. This controller
network observes specific intermediate LLM activations and predicts both a
global scaling factor and layer-specific weights. The predicted global scaling
factor and layer-specific weights then dynamically modulate the intensity of a
steering patch, derived from a pre-computed "refusal direction" vector, applied
across the LLM's layers during generation. Trained on activations from both
harmful and benign prompts, our controller learns to discriminatively apply
nuanced, layer-aware interventions, activating steering primarily for harmful
inputs. Experiments using safety benchmarks like ToxicChat & In-The-Wild
Jailbreak Prompts demonstrate that our weighted steering controller
significantly increases refusal rates compared to the base LLM, achieving
targeted behavioral modification without altering the original model
parameters. Our experiments with Llama-3.1-8B, Llama-3.2-1B & Mistral-7B show
our approach outperforms existing methods, presenting an efficient and adaptive
method for fine-grained control over LLM behavior at inference time.

</details>


### [2] [Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL](https://arxiv.org/abs/2505.20315)
*Zhewei Yao,Guoheng Sun,Lukasz Borchmann,Zheyu Shen,Minghang Deng,Bohan Zhai,Hao Zhang,Ang Li,Yuxiong He*

Key words: Test2SQL, 强化学习, SQL生成, BIRD, Arctic-Text2SQL-R1

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Arctic-Text2SQL-R1使用强化学习框架和轻量级执行正确性奖励信号生成准确、可执行的SQL，避免复杂中间监督，实现最先进的执行准确性。

Motivation: 解决自然语言转SQL过程中的准确性和可执行性问题，尤其针对复杂查询。

Method: 采用强化学习框架，基于执行正确性的轻量级奖励信号，结合精心设计的数据、强监督初始化和有效训练策略。

Result: 在六个Test2SQL基准测试中取得最优执行准确率，7B模型超越先前70B级系统。

Conclusion: 该框架高效且可扩展，未来研究可从其正负实验结果中获取实用指导。

Abstract: Translating natural language into SQL (Test2SQL) is a longstanding challenge
at the intersection of natural language understanding and structured data
access. While large language models (LLMs) have significantly improved fluency
in SQL generation, producing correct and executable SQL--particularly for
complex queries--remains a bottleneck. We present Arctic-Text2SQL-R1, a
reinforcement learning (RL) framework and model family designed to generate
accurate, executable SQL using a lightweight reward signal based solely on
execution correctness. Our approach avoids brittle intermediate supervision and
complex reward shaping, promoting stable training and alignment with the end
task. Combined with carefully curated data, strong supervised initialization,
and effective training practices, Arctic-Text2SQL-R1 achieves state-of-the-art
execution accuracy across six diverse Test2SQL benchmarks, including the top
position on the BIRD leaderboard. Notably, our 7B model outperforms prior
70B-class systems, highlighting the framework's scalability and efficiency. We
further demonstrate inference-time robustness through simple extensions like
value retrieval and majority voting. Extensive experiments and ablation studies
offer both positive and negative insights, providing practical guidance for
future Test2SQL research.

</details>


### [3] [Beyond Demonstrations: Dynamic Vector Construction from Latent Representations](https://arxiv.org/abs/2505.20318)
*Wang Cai,Hsiu-Yuan Huang,Zhixiang Wang,Yunfang Wu*

Key words: ICV, DyVec, EQR, 动态分割, 语义表示, REINFORCE优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DyVec方法通过动态向量提取和注入，提升了大语言模型在推理时的任务适应性，无需重复处理示例，性能优于现有方法。

Motivation: 现有ICV方法对ICL因素敏感、表示粗糙或语义碎片化，且依赖启发式注入位置，限制了其应用。

Method: DyVec结合EQR策略提取鲁棒语义表示，动态分割和注入表示，并利用REINFORCE优化学习最佳注入位置。

Result: DyVec在性能上优于few-shot ICL、LoRA及先前ICV方法，并验证了动态分割和注入语义表示的有效性。

Conclusion: DyVec为推理时任务适应提供了轻量级且数据高效的解决方案。

Abstract: In-Context derived Vector (ICV) methods extract task-relevant representations
from large language models (LLMs) and reinject them during inference, achieving
comparable performance to few-shot In-Context Learning (ICL) without repeated
demonstration processing. However, existing ICV methods remain sensitive to
ICL-specific factors, often use coarse or semantically fragmented
representations as the source of the vector, and rely on heuristic-based
injection positions, limiting their applicability.
  To address these issues, we propose Dynamic Vector (DyVec), which
incorporates an Exhaustive Query Rotation (EQR) strategy to extract robust
semantically aggregated latent representations by mitigating variance
introduced by ICL. It then applies Dynamic Latent Segmentation and Injection to
adaptively partition representations based on task complexity and leverages
REINFORCE-based optimization to learn optimal injection positions for each
segment.
  Experiments results show that DyVec outperforms few-shot ICL, LoRA, and prior
ICV baselines. Further analysis highlights the effectiveness of dynamically
segmenting and injecting semantically aggregated latent representations. DyVec
provides a lightweight and data-efficient solution for inference-time task
adaptation.

</details>


### [4] [Less Context, Same Performance: A RAG Framework for Resource-Efficient LLM-Based Clinical NLP](https://arxiv.org/abs/2505.20320)
*Satya Narayana Cheetirala,Ganesh Raut,Dhavalkumar Patel,Fabio Sanatana,Robert Freeman,Matthew A Levin,Girish N. Nadkarni,Omar Dawkins,Reba Miller,Randolph M. Steinhagen,Eyal Klang,Prem Timsina*

Key words: 长文本分类, LLMs, RAG, 临床笔记, 计算效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究采用RAG方法仅处理临床笔记中最相关片段，与全文本处理LLMs相比，在手术并发症识别任务中表现相当，能显著减少token使用且不损准确率。

Motivation: 长文本分类因LLMs的token限制和高计算成本而具有挑战性，RAG方法可能提供高效且经济的替代方案。

Method: 将临床文档分块并转为向量嵌入，存储于FAISS索引，检索最相关的4000词输入LLMs（GPT4o/LLaMA/Mistral）。

Result: AUC ROC、精确率、召回率和F1显示RAG与全文本处理无显著差异（p>0.05）。

Conclusion: RAG可在不降低准确性的前提下减少token使用，为长临床文档分析提供可扩展的解决方案。

Abstract: Long text classification is challenging for Large Language Models (LLMs) due
to token limits and high computational costs. This study explores whether a
Retrieval Augmented Generation (RAG) approach using only the most relevant text
segments can match the performance of processing entire clinical notes with
large context LLMs. We begin by splitting clinical documents into smaller
chunks, converting them into vector embeddings, and storing these in a FAISS
index. We then retrieve the top 4,000 words most pertinent to the
classification query and feed these consolidated segments into an LLM. We
evaluated three LLMs (GPT4o, LLaMA, and Mistral) on a surgical complication
identification task. Metrics such as AUC ROC, precision, recall, and F1 showed
no statistically significant differences between the RAG based approach and
whole-text processing (p > 0.05p > 0.05). These findings indicate that RAG can
significantly reduce token usage without sacrificing classification accuracy,
providing a scalable and cost effective solution for analyzing lengthy clinical
documents.

</details>


### [5] [BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases](https://arxiv.org/abs/2505.20321)
*Mathew J. Koretsky,Maya Willey,Adi Asija,Owen Bianchi,Chelsea X. Alvarado,Tanay Nayak,Nicole Kuznetsov,Sungwon Kim,Mike A. Nalls,Daniel Khashabi,Faraz Faghri*

Key words: 文本到SQL, 生物医学知识库, 科学推理, 基准测试, GPT

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: BiomedSQL是一个专门用于评估科学推理能力的文本到SQL生成的基准测试，基于真实生物医学知识库，包含68,000个问题/SQL查询/答案三元组。实验显示，现有模型的性能远低于专家基准。

Motivation: 当前文本到SQL系统在将科学问题映射为可执行SQL时表现不佳，尤其是在需要隐含领域推理的情况下。BiomedSQL旨在填补这一空白，推动支持科学发现的结构化知识库推理系统的发展。

Method: BiomedSQL基于整合了基因-疾病关联、组学数据因果推断和药物批准记录的BigQuery知识库，要求模型推断特定领域标准。评估了多种开源和闭源LLM，并比较多种提示策略和交互范式。

Result: 最佳模型（BMSQL）的执行准确率为62.6%，仍远低于专家基准的90.0%。

Conclusion: BiomedSQL为提升文本到SQL系统的科学推理能力提供了新基础，代码和数据集已开源。

Abstract: Biomedical researchers increasingly rely on large-scale structured databases
for complex analytical tasks. However, current text-to-SQL systems often
struggle to map qualitative scientific questions into executable SQL,
particularly when implicit domain reasoning is required. We introduce
BiomedSQL, the first benchmark explicitly designed to evaluate scientific
reasoning in text-to-SQL generation over a real-world biomedical knowledge
base. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in
a harmonized BigQuery knowledge base that integrates gene-disease associations,
causal inference from omics data, and drug approval records. Each question
requires models to infer domain-specific criteria, such as genome-wide
significance thresholds, effect directionality, or trial phase filtering,
rather than rely on syntactic translation alone. We evaluate a range of open-
and closed-source LLMs across prompting strategies and interaction paradigms.
Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0%
execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%,
both well below the expert baseline of 90.0%. BiomedSQL provides a new
foundation for advancing text-to-SQL systems capable of supporting scientific
discovery through robust reasoning over structured biomedical knowledge bases.
Our dataset is publicly available at
https://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source
at https://github.com/NIH-CARD/biomedsql.

</details>


### [6] [Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms](https://arxiv.org/abs/2505.20322)
*Mengru Wang,Ziwen Xu,Shengyu Mao,Shumin Deng,Zhaopeng Tu,Huajun Chen,Ningyu Zhang*

Key words: 语言模型、安全性、知识解耦、稀疏自编码器、精确控制

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为 Steering Target Atoms (STA) 的新方法，用于增强语言模型生成的安全性和可靠性，通过分离和操作知识组件来优化控制效果。

Motivation: 语言模型生成的精确控制对安全性和可靠性至关重要。现有的提示工程和引导方法在多参数交织的表示中存在控制精度受限和副作用的问题。通过分离知识组件可以优化干预效果。

Method: 提出 Steering Target Atoms (STA)，一种基于稀疏自编码器 (SAE) 的方法，用于独立定位和操作解耦的知识组件，实现对模型生成的精确引导。

Result: 实验表明 STA 在安全性和鲁棒性方面表现优秀，特别是在对抗性场景中，并验证了其在大型推理模型中的精确控制有效性。

Conclusion: STA 方法通过解耦知识组件显著提升了生成控制的精度和灵活性，为模型安全和可靠应用提供了新方向。

Abstract: Precise control over language model generation is vital for ensuring both
safety and reliability. Although prompt engineering and steering are commonly
used to intervene in model behaviors, the vast number of parameters in models
often results in highly intertwined internal representations. This
interdependency can limit control precision and sometimes lead to unintended
side effects. Recent research has explored the use of sparse autoencoders (SAE)
to disentangle knowledge in high-dimensional spaces for steering. However,
these applications have been limited to toy tasks owing to the nontrivial issue
of locating atomic knowledge components. In this paper, we propose Steering
Target Atoms (STA), a novel method that isolates and manipulates disentangled
knowledge components to enhance safety. Comprehensive experiments demonstrate
the effectiveness of our approach. Further analysis reveals that steering
exhibits superior robustness and flexibility, particularly in adversarial
scenarios. We also apply the steering strategy to the large reasoning model,
confirming its effectiveness in precise reasoning control.

</details>


### [7] [PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus](https://arxiv.org/abs/2505.20323)
*Shahriar Noroozizadeh,Sayantan Kumar,George H. Chen,Jeremy C. Weiss*

Key words: clinical narratives, temporal dynamics, dataset, PubMed, survival prediction

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: PMOA-TTS是一个公开可用的临床时间轴数据集，包含124,699个PubMed Open Access病例报告，通过LLM管道转换为结构化事件时间轴，包含560万时间戳临床事件，验证了其质量及在生存预测任务中的实用性。

Motivation: 临床叙述中的时间动态对建模患者轨迹至关重要，但大规模时间标注资源有限，因此开发PMOA-TTS填补这一空白。

Method: 结合启发式过滤与Llama 3.3识别单病例报告，通过提示驱动的Llama 3.3和DeepSeek R1提取事件时间轴。

Result: 事件匹配率80%，时间一致性c-index>0.90，时间戳对齐AULTC高。在下游生存预测中，时间轴嵌入的一致性指数达0.82±0.01。

Conclusion: PMOA-TTS为生物医学NLP中的时间轴提取、时间推理和纵向建模提供了可扩展的基础。

Abstract: Understanding temporal dynamics in clinical narratives is essential for
modeling patient trajectories, yet large-scale temporally annotated resources
remain limited. We present PMOA-TTS, the first openly available dataset of
124,699 PubMed Open Access (PMOA) case reports, each converted into structured
(event, time) timelines via a scalable LLM-based pipeline. Our approach
combines heuristic filtering with Llama 3.3 to identify single-patient case
reports, followed by prompt-driven extraction using Llama 3.3 and DeepSeek R1,
resulting in over 5.6 million timestamped clinical events. To assess timeline
quality, we evaluate against a clinician-curated reference set using three
metrics: (i) event-level matching (80% match at a cosine similarity threshold
of 0.1), (ii) temporal concordance (c-index > 0.90), and (iii) Area Under the
Log-Time CDF (AULTC) for timestamp alignment. Corpus-level analysis shows wide
diagnostic and demographic coverage. In a downstream survival prediction task,
embeddings from extracted timelines achieve time-dependent concordance indices
up to 0.82 $\pm$ 0.01, demonstrating the predictive value of temporally
structured narratives. PMOA-TTS provides a scalable foundation for timeline
extraction, temporal reasoning, and longitudinal modeling in biomedical NLP.
The dataset is available at: https://huggingface.co/datasets/snoroozi/pmoa-tts .

</details>


### [8] [Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence](https://arxiv.org/abs/2505.20325)
*Amirhosein Ghasemabadi,Keith G. Mills,Baochun Li,Di Niu*

Key words: Test-Time Scaling, Large Language Models, Self-Guided Framework, Efficiency, Reinforcement Learning

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文介绍了一种高效的自我引导测试时间扩展（TTS）框架GG，通过轻量级树搜索和内在LLM信号（如令牌级置信度和步骤新颖性）实现高性能，无需依赖昂贵的外部验证模型。

Motivation: 现有的TTS方法依赖外部验证模型或采样策略（如Best-of-N）导致高昂计算成本，GG旨在通过自引导机制解决这一问题。

Method: GG使用轻量级树搜索，仅依赖LLM的内在信号（令牌级置信度和步骤新颖性），并通过强化学习微调提高置信度估计的可靠性。

Result: 在数学推理基准测试中，GG使小模型（如1.5B参数）达到或超越大模型（如32B-70B）的准确性，同时GPU内存使用减少高达10倍，推理速度提升8倍。

Conclusion: GG提供了一种高效且实用的TTS解决方案，显著降低了计算资源需求，适用于实际部署。

Abstract: Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)
reasoning often incur substantial computational costs, primarily due to
extensive reliance on external Process Reward Models (PRMs) or sampling methods
like Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient
self-guided TTS framework that achieves PRM-level performance without costly
external verifier models. Our method employs a lightweight tree search guided
solely by intrinsic LLM signals, token-level confidence and step novelty. One
critical innovation is improving the reliability of internal confidence
estimates via a targeted reinforcement learning fine-tuning phase. Empirical
evaluations on challenging mathematical reasoning benchmarks demonstrate that
GG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching
or surpassing significantly larger models (e.g., 32B-70B parameters), while
reducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG
achieves comparable accuracy with 8x faster inference speeds and 4-5x lower
memory usage. Additionally, GG reduces KV cache memory usage by approximately
50% compared to the BoN strategy, facilitating more efficient and practical
deployment of TTS techniques.

</details>


### [9] [Multi-Scale Manifold Alignment: A Unified Framework for Enhanced Explainability of Large Language Models](https://arxiv.org/abs/2505.20333)
*Yukun Zhang,Qi Dong*

Key words: 大语言模型、多尺度流形对齐、解释性、潜在语义、几何对齐

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出一种多尺度流形对齐框架，用于分解和分析大语言模型的潜在语义空间，以提高解释性和可靠性。

Motivation: 大语言模型虽然性能强大，但其内部推理机制不透明，限制了其在关键应用中的可信度和解释性。

Method: 多尺度流形对齐框架将潜在空间分解为全局、中间和局部语义流形，并通过跨尺度映射函数结合几何对齐和信息保留技术。

Result: 理论分析表明对齐误差在温和假设下可被KL散度界定，框架提升了模型的可解释性并支持应用如偏见检测和鲁棒性增强。

Conclusion: 该框架为理解大语言模型的多尺度语义结构提供了统一解释，推动了模型透明性和应用潜力。

Abstract: Recent advances in Large Language Models (LLMs) have achieved strong
performance, yet their internal reasoning remains opaque, limiting
interpretability and trust in critical applications. We propose a novel
Multi_Scale Manifold Alignment framework that decomposes the latent space into
global, intermediate, and local semantic manifolds capturing themes, context,
and word-level details. Our method introduces cross_scale mapping functions
that jointly enforce geometric alignment (e.g., Procrustes analysis) and
information preservation (via mutual information constraints like MINE or VIB).
We further incorporate curvature regularization and hyperparameter tuning for
stable optimization. Theoretical analysis shows that alignment error, measured
by KL divergence, can be bounded under mild assumptions. This framework offers
a unified explanation of how LLMs structure multi-scale semantics, advancing
interpretability and enabling applications such as bias detection and
robustness enhancement.

</details>


### [10] [Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query](https://arxiv.org/abs/2505.20334)
*Yixuan Wang,Shiyu Ji,Yijun Liu,Yuzhuang Xu,Yang Xu,Qingfu Zhu,Wanxiang Che*

Key words: 大语言模型, KV缓存, 缓存淘汰, 长文本序列

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出Lookahead Q-Cache (LAQ)框架，通过生成低成本伪前瞻查询优化KV缓存淘汰机制，提升长文本序列下的缓存效率。

Motivation: 现有KV缓存淘汰方法基于预填充阶段注意力分数剪枝token，与实际推理查询不一致，尤其在内存受限时性能下降。

Method: LAQ生成低成本的伪前瞻查询作为重要性估计的观察窗口，实现对真实解码查询的更准确近似。

Result: 在LongBench和Needle-in-a-Haystack基准测试中，LAQ在有限缓存预算下性能优于现有方法，提升1~4分。

Conclusion: LAQ不仅与现有方法互补，还能灵活结合以进一步提升性能。

Abstract: Large language models (LLMs) rely on key-value cache (KV cache) to accelerate
decoding by reducing redundant computations. However, the KV cache memory usage
grows substantially with longer text sequences, posing challenges for efficient
deployment. Existing KV cache eviction methods prune tokens using
prefilling-stage attention scores, causing inconsistency with actual inference
queries, especially under tight memory budgets. In this paper, we propose
Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost
pseudo lookahead queries to better approximate the true decoding-stage queries.
By using these lookahead queries as the observation window for importance
estimation, LAQ achieves more consistent and accurate KV cache eviction aligned
with real inference scenarios. Experimental results on LongBench and
Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods
across various budget levels, achieving a 1 $\sim$ 4 point improvement on
LongBench under limited cache budget. Moreover, LAQ is complementary to
existing approaches and can be flexibly combined to yield further improvements.

</details>


### [11] [Language Model Distillation: A Temporal Difference Imitation Learning Perspective](https://arxiv.org/abs/2505.20335)
*Zishun Yu,Shangzhe Li,Xinhua Zhang*

Key words: 语言模型蒸馏, 时间差分学习, 行为克隆, 动作空间缩减

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种基于时间差分的蒸馏框架，利用教师模型输出的稀疏性，通过缩小动作空间（词汇子集）来提升小模型的效率和性能。

Motivation: 大型语言模型虽然强大，但因计算成本高昂而难以广泛应用。蒸馏技术能将其压缩为更高效的模型，但现有方法多为行为克隆，缺乏效率。本文从时间差分学习角度出发，提出新框架以优化蒸馏效果。

Method: 通过分析教师模型的输出稀疏性（大多概率集中于少数词），设计一种时间差分学习框架，在缩减的动作空间（词汇子集）上操作，并推导实用算法。

Result: 实验表明，该方法能显著提升蒸馏后小模型的性能。

Conclusion: 时间差分学习结合教师模型的稀疏性可有效改进语言模型蒸馏，为未来研究提供新方向。

Abstract: Large language models have led to significant progress across many NLP tasks,
although their massive sizes often incur substantial computational costs.
Distillation has become a common practice to compress these large and highly
capable models into smaller, more efficient ones. Many existing language model
distillation methods can be viewed as behavior cloning from the perspective of
imitation learning or inverse reinforcement learning. This viewpoint has
inspired subsequent studies that leverage (inverse) reinforcement learning
techniques, including variations of behavior cloning and temporal difference
learning methods. Rather than proposing yet another specific temporal
difference method, we introduce a general framework for temporal
difference-based distillation by exploiting the distributional sparsity of the
teacher model. Specifically, it is often observed that language models assign
most probability mass to a small subset of tokens. Motivated by this
observation, we design a temporal difference learning framework that operates
on a reduced action space (a subset of vocabulary), and demonstrate how
practical algorithms can be derived and the resulting performance improvements.

</details>


### [12] [MOSLIM:Align with diverse preferences in prompts through reward classification](https://arxiv.org/abs/2505.20336)
*Yu Zhang,Wanli Jiang,Zhengyu Yang*

Key words: 多目标对齐, 大语言模型, 奖励模型, 策略优化, MOSLIM

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MOSLIM是一种新颖的多目标对齐方法，利用单一奖励模型和策略模型处理多样化目标，通过提示灵活控制目标，无需在SFT阶段进行偏好训练，显著减少GPU资源需求。

Motivation: 当前LLM多目标对齐研究通常需要多个策略或奖励模型，或针对特定偏好训练SFT模型，导致资源消耗高且灵活性不足。MOSLIM旨在解决这一问题。

Method: MOSLIM采用多头部奖励模型对问答对分类而非评分，并通过映射函数将分类结果转化为奖励分数，优化单一策略模型。

Result: 在多个多目标基准测试中，MOSLIM表现优于现有方法，同时大幅降低GPU计算资源需求。

Conclusion: MOSLIM为LLM多目标对齐提供了高效且灵活的解决方案，适用于现成模型的无缝集成。

Abstract: The multi-objective alignment of Large Language Models (LLMs) is essential
for ensuring foundational models conform to diverse human preferences. Current
research in this field typically involves either multiple policies or multiple
reward models customized for various preferences, or the need to train a
preference-specific supervised fine-tuning (SFT) model. In this work, we
introduce a novel multi-objective alignment method, MOSLIM, which utilizes a
single reward model and policy model to address diverse objectives. MOSLIM
provides a flexible way to control these objectives through prompting and does
not require preference training during SFT phase, allowing thousands of
off-the-shelf models to be directly utilized within this training framework.
MOSLIM leverages a multi-head reward model that classifies question-answer
pairs instead of scoring them and then optimize policy model with a scalar
reward derived from a mapping function that converts classification results
from reward model into reward scores. We demonstrate the efficacy of our
proposed method across several multi-objective benchmarks and conduct ablation
studies on various reward model sizes and policy optimization methods. The
MOSLIM method outperforms current multi-objective approaches in most results
while requiring significantly fewer GPU computing resources compared with
existing policy optimization methods.

</details>


### [13] [Assessing the Capability of LLMs in Solving POSCOMP Questions](https://arxiv.org/abs/2505.20338)
*Cayo Viegas,Rohit Gheyi,Márcio Ribeiro*

Key words: LLM, POSCOMP, 计算机科学考试, ChatGPT-4, 图像理解

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究评估了多种LLM在巴西POSCOMP计算机科学考试中的表现，发现ChatGPT-4等在文本题目上表现优异，但在图像题目上仍有不足。新模型进一步超越了人类考生表现。

Motivation: 探索LLM在计算机科学等专业领域的实际能力，以评估其应用潜力并指导未来发展。

Method: 利用POSCOMP考试（2022-2024年）对多个LLM（如ChatGPT-4、Gemini等）进行测试，比较其与人类考生的成绩。

Result: LLM在文本题目上表现优秀，ChatGPT-4甚至超越人类考生；但图像题目仍是挑战。新模型持续进步，全面超越人类。

Conclusion: LLM（尤其是ChatGPT-4）在专业领域考试中展现潜力，文本处理能力强，但需改进图像理解能力。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
expanded the capabilities of artificial intelligence in natural language
processing tasks. Despite this progress, their performance in specialized
domains such as computer science remains relatively unexplored. Understanding
the proficiency of LLMs in these domains is critical for evaluating their
practical utility and guiding future developments. The POSCOMP, a prestigious
Brazilian examination used for graduate admissions in computer science promoted
by the Brazlian Computer Society (SBC), provides a challenging benchmark. This
study investigates whether LLMs can match or surpass human performance on the
POSCOMP exam. Four LLMs - ChatGPT-4, Gemini 1.0 Advanced, Claude 3 Sonnet, and
Le Chat Mistral Large - were initially evaluated on the 2022 and 2023 POSCOMP
exams. The assessments measured the models' proficiency in handling complex
questions typical of the exam. LLM performance was notably better on text-based
questions than on image interpretation tasks. In the 2022 exam, ChatGPT-4 led
with 57 correct answers out of 69 questions, followed by Gemini 1.0 Advanced
(49), Le Chat Mistral (48), and Claude 3 Sonnet (44). Similar trends were
observed in the 2023 exam. ChatGPT-4 achieved the highest performance,
surpassing all students who took the POSCOMP 2023 exam. LLMs, particularly
ChatGPT-4, show promise in text-based tasks on the POSCOMP exam, although image
interpretation remains a challenge. Given the rapid evolution of LLMs, we
expanded our analysis to include more recent models - o1, Gemini 2.5 Pro,
Claude 3.7 Sonnet, and o3-mini-high - evaluated on the 2022-2024 POSCOMP exams.
These newer models demonstrate further improvements and consistently surpass
both the average and top-performing human participants across all three years.

</details>


### [14] [Dynamic Manifold Evolution Theory: Modeling and Stability Analysis of Latent Representations in Large Language Models](https://arxiv.org/abs/2505.20340)
*Yukun Zhang,Qi Dong*

Key words: 动态流形演化理论、大语言模型、生成动态系统、语义流形、文本生成质量

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了动态流形演化理论（DMET），通过将大语言模型的生成过程建模为低维语义流形上的受控动力系统，结合能量驱动流和上下文依赖力，为文本生成的流畅性、语法性和语义一致性提供了量化指标。

Motivation: 旨在通过构建动态流形演化理论，提供一种统一框架来理解大语言模型生成过程中的动力系统行为，尤其是文本生成的流畅性、语法性和语义一致性的内在机制。

Method: 提出DMET框架，将隐状态更新视为连续动力系统的离散欧拉近似，并将能量驱动流与上下文依赖力映射到Transformer组件（如残差连接、注意力机制和前馈网络）中。通过李雅普诺夫稳定性理论定义了三个实证指标（状态连续性、聚类质量和拓扑持续性）。

Result: 实验结果表明，DMET能够有效预测并验证文本生成的流畅性、语法性和语义一致性，并为平衡生成文本的创造性和一致性提供了理论指导。

Conclusion: DMET是一个有效的理论框架，能够定量分析大语言模型生成过程中的动态行为，并为改进文本生成质量提供了新的视角和方法。

Abstract: We introduce Dynamic Manifold Evolution Theory (DMET),a unified framework
that models large language model generation as a controlled dynamical system
evolving on a low_dimensional semantic manifold. By casting latent_state
updates as discrete time Euler approximations of continuous dynamics, we map
intrinsic energy_driven flows and context_dependent forces onto Transformer
components (residual connections, attention, feed-forward networks). Leveraging
Lyapunov stability theory We define three empirical metrics (state continuity,
clustering quality, topological persistence) that quantitatively link
latent_trajectory properties to text fluency, grammaticality, and semantic
coherence. Extensive experiments across decoding parameters validate DMET's
predictions and yield principled guidelines for balancing creativity and
consistency in text generation.

</details>


### [15] [Do LLMs have a Gender (Entropy) Bias?](https://arxiv.org/abs/2505.20343)
*Sonal Prabhune,Balaji Padmanabhan,Kaushik Dutta*

Key words: LLM, gender bias, debiasing, benchmark, ChatGPT-4o

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究发现流行LLM中存在性别偏见，提出了一种简单的去偏方法。

Motivation: 调查LLM中特定性别偏见的存在和持续性，并贡献新的基准数据集。

Method: 使用四个LLM测试熵偏见的定义，通过ChatGPT-4o评估生成响应。

Result: 发现类别层面无显著偏见，但单个问题层面差异明显。提出的去偏方法在78%的情况下更有效。

Conclusion: 简单的基于提示的去偏策略可以有效平衡LLM输出。

Abstract: We investigate the existence and persistence of a specific type of gender
bias in some of the popular LLMs and contribute a new benchmark dataset,
RealWorldQuestioning (released on HuggingFace ), developed from real-world
questions across four key domains in business and health contexts: education,
jobs, personal financial management, and general health. We define and study
entropy bias, which we define as a discrepancy in the amount of information
generated by an LLM in response to real questions users have asked. We tested
this using four different LLMs and evaluated the generated responses both
qualitatively and quantitatively by using ChatGPT-4o (as "LLM-as-judge"). Our
analyses (metric-based comparisons and "LLM-as-judge" evaluation) suggest that
there is no significant bias in LLM responses for men and women at a category
level. However, at a finer granularity (the individual question level), there
are substantial differences in LLM responses for men and women in the majority
of cases, which "cancel" each other out often due to some responses being
better for males and vice versa. This is still a concern since typical users of
these tools often ask a specific question (only) as opposed to several varied
ones in each of these common yet important areas of life. We suggest a simple
debiasing approach that iteratively merges the responses for the two genders to
produce a final result. Our approach demonstrates that a simple, prompt-based
debiasing strategy can effectively debias LLM outputs, thus producing responses
with higher information content than both gendered variants in 78% of the
cases, and consistently achieving a balanced integration in the remaining
cases.

</details>


### [16] [SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data](https://arxiv.org/abs/2505.20347)
*Wenkai Fang,Shunyu Liu,Yang Zhou,Kongcheng Zhang,Tongya Zheng,Kaixuan Chen,Mingli Song,Dacheng Tao*

Key words: 强化学习, 大型语言模型, 自我博弈, 自我指令, 自我奖励

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了自我博弈强化学习（SeRL）方法，通过自我指令和自我奖励模块，在不依赖高质量初始数据的情况下提升大型语言模型的推理能力。

Motivation: 现有强化学习方法依赖高质量指令和可验证奖励，这在专业领域难以获得。论文旨在解决这一限制，提出一种仅需有限初始数据的自学习方法。

Method: SeRL包含两个模块：自我指令（生成并过滤额外指令）和自我奖励（通过多数投票机制估计响应奖励）。结合传统强化学习方法实现迭代学习。

Result: 实验表明，SeRL在多种推理基准测试中优于同类方法，且性能接近依赖高质量数据的方法。

Conclusion: SeRL为数据有限场景下的模型训练提供了有效解决方案，实现了不依赖外部标注的高效学习。

Abstract: Recent advances have demonstrated the effectiveness of Reinforcement Learning
(RL) in improving the reasoning capabilities of Large Language Models (LLMs).
However, existing works inevitably rely on high-quality instructions and
verifiable rewards for effective training, both of which are often difficult to
obtain in specialized domains. In this paper, we propose Self-play
Reinforcement Learning(SeRL) to bootstrap LLM training with limited initial
data. Specifically, SeRL comprises two complementary modules: self-instruction
and self-rewarding. The former module generates additional instructions based
on the available data at each training step, employing robust online filtering
strategies to ensure instruction quality, diversity, and difficulty. The latter
module introduces a simple yet effective majority-voting mechanism to estimate
response rewards for additional instructions, eliminating the need for external
annotations. Finally, SeRL performs conventional RL based on the generated
data, facilitating iterative self-play learning. Extensive experiments on
various reasoning benchmarks and across different LLM backbones demonstrate
that the proposed SeRL yields results superior to its counterparts and achieves
performance on par with those obtained by high-quality data with verifiable
rewards. Our code is available at https://github.com/wantbook-book/SeRL.

</details>


### [17] [Rethinking Text-based Protein Understanding: Retrieval or LLM?](https://arxiv.org/abs/2505.20354)
*Juntong Wu,Zijing Liu,He Cao,Hao Li,Bin Feng,Zishan Shu,Ke Yu,Li Yuan,Yu Li*

Key words: 蛋白质-文本模型, 检索增强, 评估框架, 生物实体, 数据泄漏

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了蛋白质-文本模型在蛋白质生成和理解中的应用，发现现有基准测试中存在数据泄漏问题，并提出基于生物实体的新评估框架及检索增强方法，显著优于微调的大语言模型。

Motivation: 当前蛋白质-文本模型的研究中，存在数据泄漏问题且传统自然语言处理指标无法准确评估模型性能，因此需要新的评估框架和方法。

Method: 重新组织现有数据集，引入基于生物实体的新评估框架，并提出检索增强方法。

Result: 检索增强方法在蛋白质到文本生成任务中显著优于微调的大语言模型，且在无需训练的情况下仍能保持高效和准确。

Conclusion: 论文提出的评估框架和检索增强方法有效解决了现有问题，提升了蛋白质-文本模型的性能。

Abstract: In recent years, protein-text models have gained significant attention for
their potential in protein generation and understanding. Current approaches
focus on integrating protein-related knowledge into large language models
through continued pretraining and multi-modal alignment, enabling simultaneous
comprehension of textual descriptions and protein sequences. Through a thorough
analysis of existing model architectures and text-based protein understanding
benchmarks, we identify significant data leakage issues present in current
benchmarks. Moreover, conventional metrics derived from natural language
processing fail to accurately assess the model's performance in this domain. To
address these limitations, we reorganize existing datasets and introduce a
novel evaluation framework based on biological entities. Motivated by our
observation, we propose a retrieval-enhanced method, which significantly
outperforms fine-tuned LLMs for protein-to-text generation and shows accuracy
and efficiency in training-free scenarios. Our code and data can be seen at
https://github.com/IDEA-XL/RAPM.

</details>


### [18] [Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision](https://arxiv.org/abs/2505.20415)
*Xingwei Tan,Marco Valentino,Mahmud Akhter,Maria Liakata,Nikolaos Aletras*

Key words: 大型语言模型, 符号推理, 蒙特卡洛估计, 过程奖励模型, 逻辑推理, 泛化能力

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种通过生成符号推理轨迹并使用蒙特卡洛估计优化的奖励模型筛选高质量轨迹的方法，以增强大型语言模型（LLMs）的逻辑推理和泛化能力，并通过微调验证其有效性。

Motivation: 尽管大型语言模型在数学和逻辑推理基准测试中表现良好，但其性能更多依赖于记忆而非泛化能力，且对内容变化敏感，缺乏稳健的符号抽象能力。为提高推理可靠性，论文旨在克服现有符号方法在可扩展验证机制上的不足。

Method: 论文提出生成符号推理轨迹，并通过基于蒙特卡洛估计自动优化的过程奖励模型筛选高质量轨迹，随后通过微调方法提升模型的逻辑推理和泛化能力。

Result: 在FOLIO和LogicAsker等逻辑推理基准测试中，该方法显著提升了前沿和开源模型的性能。此外，在声明验证任务中，该方法还表现出更强的领域外泛化能力。

Conclusion: 符号引导的过程监督有助于减轻LLMs推理中的记忆依赖效应，提升泛化能力，展现了符号方法在增强模型可靠性方面的潜力。

Abstract: Large language models (LLMs) have shown promising performance in mathematical
and logical reasoning benchmarks. However, recent studies have pointed to
memorization, rather than generalization, as one of the leading causes for such
performance. LLMs, in fact, are susceptible to content variations,
demonstrating a lack of robust symbolic abstractions supporting their reasoning
process. To improve reliability, many attempts have been made to combine LLMs
with symbolic methods. Nevertheless, existing approaches fail to effectively
leverage symbolic representations due to the challenges involved in developing
reliable and scalable verification mechanisms. In this paper, we propose to
overcome such limitations by generating symbolic reasoning trajectories and
select the high-quality ones using a process reward model automatically tuned
based on Monte Carlo estimation. The trajectories are then employed via
fine-tuning methods to improve logical reasoning and generalization. Our
results on logical reasoning benchmarks such as FOLIO and LogicAsker show the
effectiveness of the proposed method with large gains on frontier and
open-weight models. Moreover, additional experiments on claim verification
reveal that fine-tuning on the generated symbolic reasoning trajectories
enhances out-of-domain generalizability, suggesting the potential impact of
symbolically-guided process supervision in alleviating the effect of
memorization on LLM reasoning.

</details>


### [19] [GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation](https://arxiv.org/abs/2505.20416)
*Zihong Chen,Wanli Jiang,Jinzhe Li,Zhonghang Yuan,Huanjun Kong,Wanli Ouyang,Nanqing Dong*

Key words: 大语言模型,知识图谱,问答对生成,合成数据,监督微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: GraphGen框架通过知识图谱引导生成问答对，解决了大语言模型微调中合成数据的质量问题，并在知识密集型任务中表现优于传统方法。

Motivation: 传统合成数据生成方法存在事实错误、长尾知识覆盖率不足、知识结构简单和输出同质化等问题，GraphGen旨在解决这些问题以提升微调数据质量。

Method: 构建细粒度知识图谱，利用预期校准误差识别模型知识盲区，优先生成高质量长尾知识问答对，结合多跳邻居采样和风格控制生成多样化数据。

Result: 在闭卷知识密集型任务中，GraphGen优于传统合成数据方法，提供了更可靠且全面的解决方案。

Conclusion: GraphGen是一种有效解决监督微调数据稀缺问题的框架，其代码和数据已开源。

Abstract: Fine-tuning for large language models (LLMs) typically requires substantial
amounts of high-quality supervised data, which is both costly and
labor-intensive to acquire. While synthetic data generation has emerged as a
promising solution, existing approaches frequently suffer from factual
inaccuracies, insufficient long-tail coverage, simplistic knowledge structures,
and homogenized outputs. To address these challenges, we introduce GraphGen, a
knowledge graph-guided framework designed for three key question-answering (QA)
scenarios: atomic QA, aggregated QA, and multi-hop QA. It begins by
constructing a fine-grained knowledge graph from the source text. It then
identifies knowledge gaps in LLMs using the expected calibration error metric,
prioritizing the generation of QA pairs that target high-value, long-tail
knowledge. Furthermore, GraphGen incorporates multi-hop neighborhood sampling
to capture complex relational information and employs style-controlled
generation to diversify the resulting QA data. Experimental results on
knowledge-intensive tasks under closed-book settings demonstrate that GraphGen
outperforms conventional synthetic data methods, offering a more reliable and
comprehensive solution to the data scarcity challenge in supervised
fine-tuning. The code and data are publicly available at
https://github.com/open-sciencelab/GraphGen.

</details>


### [20] [SEMMA: A Semantic Aware Knowledge Graph Foundation Model](https://arxiv.org/abs/2505.20422)
*Arvindh Arun,Sumit Kumar,Mojtaba Nayyeri,Bo Xiong,Ponnurangam Kumaraguru,Antonio Vergari,Steffen Staab*

Key words: 知识图谱基础模型, 语义嵌入, 结构整合, 零样本推理, 泛化能力

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SEMMA是一种结合文本语义和结构的知识图谱基础模型，其在未见图谱上的推理表现优于纯结构方法，尤其在泛化设置中表现突出。

Motivation: 现有知识图谱基础模型主要依赖图结构而忽略了文本属性的丰富语义信号，SEMMA旨在通过整合文本语义和结构提升模型的泛化能力。

Method: SEMMA采用双模块设计，利用大型语言模型增强关系标识符，生成语义嵌入并构建文本关系图，最后与结构组件融合。

Result: 在54个不同知识图谱上，SEMMA在全归纳链接预测中表现优于纯结构基线（如ULTRA），在更难的泛化设置中效果是纯结构方法的两倍。

Conclusion: 文本语义在结构方法失效的泛化场景中至关重要，SEMMA展示了结合结构和语言信号的基础模型的必要性。

Abstract: Knowledge Graph Foundation Models (KGFMs) have shown promise in enabling
zero-shot reasoning over unseen graphs by learning transferable patterns.
However, most existing KGFMs rely solely on graph structure, overlooking the
rich semantic signals encoded in textual attributes. We introduce SEMMA, a
dual-module KGFM that systematically integrates transferable textual semantics
alongside structure. SEMMA leverages Large Language Models (LLMs) to enrich
relation identifiers, generating semantic embeddings that subsequently form a
textual relation graph, which is fused with the structural component. Across 54
diverse KGs, SEMMA outperforms purely structural baselines like ULTRA in fully
inductive link prediction. Crucially, we show that in more challenging
generalization settings, where the test-time relation vocabulary is entirely
unseen, structural methods collapse while SEMMA is 2x more effective. Our
findings demonstrate that textual semantics are critical for generalization in
settings where structure alone fails, highlighting the need for foundation
models that unify structural and linguistic signals in knowledge reasoning.

</details>


### [21] [The UD-NewsCrawl Treebank: Reflections and Challenges from a Large-scale Tagalog Syntactic Annotation Project](https://arxiv.org/abs/2505.20428)
*Angelina A. Aquino,Lester James V. Miranda,Elsie Marie T. Or*

Key words: 他加禄语, 通用依存, 依存树库, Transformer模型, 计算语言学

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文介绍了UD-NewsCrawl，这是迄今为止最大的他加禄语依存树库，包含15.6k手动标注的依存树，基于通用依存框架。论文详细描述了树库的开发过程，包括数据收集、预处理、手动标注和质量保证流程，并通过对多种基于Transformer的模型进行基线评估，展示了他加禄语依存分析的最新性能。此外，论文还探讨了他加禄语独特的语法特性带来的分析挑战及其对标注的影响。

Motivation: 目的是创建并公开一个大规模、高质量的他加禄语依存树库，以填补该领域的数据空白，并为计算语言学研究提供宝贵资源，促进对他加禄语等代表性不足语言的研究。

Method: 通过数据收集、预处理、手动标注和质量保证流程构建树库，同时使用多种Transformer模型进行基线评估，评估依存分析性能。

Result: 提出了包含15.6k手动标注依存树的UD-NewsCrawl树库，并展示了最新的依存分析性能基线，同时指出他加禄语语法特性带来的标注挑战。

Conclusion: UD-NewsCrawl和基线模型实现将成为他加禄语等少研究语言的计算语言学研究的重要资源，帮助推动相关领域的发展。

Abstract: This paper presents UD-NewsCrawl, the largest Tagalog treebank to date,
containing 15.6k trees manually annotated according to the Universal
Dependencies framework. We detail our treebank development process, including
data collection, pre-processing, manual annotation, and quality assurance
procedures. We provide baseline evaluations using multiple transformer-based
models to assess the performance of state-of-the-art dependency parsers on
Tagalog. We also highlight challenges in the syntactic analysis of Tagalog
given its distinctive grammatical properties, and discuss its implications for
the annotation of this treebank. We anticipate that UD-NewsCrawl and our
baseline model implementations will serve as valuable resources for advancing
computational linguistics research in underrepresented languages like Tagalog.

</details>


### [22] [PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy](https://arxiv.org/abs/2505.20429)
*Shuhao Guan,Moule Lin,Cheng Xu,Xinyi Liu,Jinman Zhao,Jiexin Fan,Qi Xu,Derek Greene*

Key words: OCR, 历史文档, 图像恢复, 语义校正, ByT5

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: PreP-OCR是一个两阶段流程，结合了文档图像恢复和语义感知OCR后校正，显著提高了历史文档文本提取的准确性。

Motivation: 解决因历史文档图像退化导致的OCR准确性低的问题，通过联合优化图像清晰度和语言一致性提升文本提取效果。

Method: 1. 生成合成图像对并训练图像恢复模型；2. 使用ByT5进行OCR后校正。

Result: 在13,831页历史文档上的实验显示，字符错误率降低了63.9-70.3%。

Conclusion: 图像恢复与语言纠错结合对历史档案数字化具有潜力。

Abstract: This paper introduces PreP-OCR, a two-stage pipeline that combines document
image restoration with semantic-aware post-OCR correction to improve text
extraction from degraded historical documents. Our key innovation lies in
jointly optimizing image clarity and linguistic consistency. First, we generate
synthetic image pairs with randomized text fonts, layouts, and degradations. An
image restoration model is trained on this synthetic data, using
multi-directional patch extraction and fusion to process large images. Second,
a ByT5 post-corrector, fine-tuned on synthetic historical text training pairs,
addresses any remaining OCR errors. Detailed experiments on 13,831 pages of
real historical documents in English, French, and Spanish show that PreP-OCR
pipeline reduces character error rates by 63.9-70.3\% compared to OCR on raw
images. Our pipeline demonstrates the potential of integrating image
restoration with linguistic error correction for digitizing historical
archives.

</details>


### [23] [HAMburger: Accelerating LLM Inference via Token Smashing](https://arxiv.org/abs/2505.20438)
*Jingyu Liu,Ce Zhang*

Key words: LLM推理, KV缓存, 自回归模型, 资源分配, 高效计算

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: HAMburger是一种分层自回归模型，通过优化KV缓存和计算资源分配，实现LLM推理的高效性，减少资源消耗并提升生成速度。

Motivation: 现有LLM推理中，每个token需要一个前向传递和KV缓存，效率不高。HAMburger利用LLM自识别信息量的能力，优化资源分配。

Method: 引入分层自回归模型（HAMburger），结合组合嵌入器和微步解码器，将多个token压缩到单个KV缓存中，实现每步生成多个token。

Result: HAMburger将KV缓存和计算FLOPS从线性增长降至次线性增长，最高减少2倍KV缓存计算并提升2倍TPS，同时保持生成质量。

Conclusion: HAMburger在计算和内存效率上实现了硬件无关的高效推理，为LLM推理优化提供了新思路。

Abstract: The growing demand for efficient Large Language Model (LLM) inference
requires a holistic optimization on algorithms, systems, and hardware. However,
very few works have fundamentally changed the generation pattern: each token
needs one forward pass and one KV cache. This can be sub-optimal because we
found that LLMs are extremely capable of self-identifying the exact dose of
information that a single KV cache can store, and many tokens can be generated
confidently without global context. Based on this insight, we introduce
HAMburger, a Hierarchically Auto-regressive Model that redefines resource
allocation in LLMs by moving beyond uniform computation and storage per token
during inference. Stacking a compositional embedder and a micro-step decoder in
between a base LLM, HAMburger smashes multiple tokens into a single KV and
generates several tokens per step. Additionally, HAMburger functions as a
speculative decoding framework where it can blindly trust self-drafted tokens.
As a result, HAMburger shifts the growth of KV cache and forward FLOPs from
linear to sub-linear with respect to output length, and adjusts its inference
speed based on query perplexity and output structure. Extensive evaluations
show that HAMburger reduces the KV cache computation by up to 2$\times$ and
achieves up to 2$\times$ TPS, while maintaining quality in both short- and
long-context tasks. Our method explores an extremely challenging inference
regime that requires both computation- and memory-efficiency with a
hardware-agnostic design.

</details>


### [24] [In-context Language Learning for Endangered Languages in Speech Recognition](https://arxiv.org/abs/2505.20445)
*Zhaolin Li,Jan Niehues*

Key words: 大语言模型,上下文学习,低资源语言,自动语音识别,概率方法

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了大语言模型（LLM）是否能够通过上下文学习（ICL）自动学习未被训练的低资源语言，发现在语言建模和自动语音识别任务中，提供更多相关文本样本可提升性能，且概率方法优于传统指令方法。

Motivation: 全球约有7000种语言，但现有大语言模型仅支持少数。探索LLM是否能在无监督数据的情况下学习新语言，尤其是低资源语言，以扩展其适用性。

Method: 通过上下文学习（ICL）对四种未被LLM训练过的濒危语言进行实验，比较概率方法和传统指令方法在语言建模和ASR任务中的表现。

Result: 实验表明，ICL使LLM在ASR任务中表现接近或优于专为这些语言训练的专用模型，且保留了LLM的原始能力。

Conclusion: LLM能通过ICL有效学习低资源语言，概率方法更优，为语言多样性支持提供了新思路。

Abstract: With approximately 7,000 languages spoken worldwide, current large language
models (LLMs) support only a small subset. Prior research indicates LLMs can
learn new languages for certain tasks without supervised data. We extend this
investigation to speech recognition, investigating whether LLMs can learn
unseen, low-resource languages through in-context learning (ICL). With
experiments on four diverse endangered languages that LLMs have not been
trained on, we find that providing more relevant text samples enhances
performance in both language modelling and Automatic Speech Recognition (ASR)
tasks. Furthermore, we show that the probability-based approach outperforms the
traditional instruction-based approach in language learning. Lastly, we show
ICL enables LLMs to achieve ASR performance that is comparable to or even
surpasses dedicated language models trained specifically for these languages,
while preserving the original capabilities of the LLMs.

</details>


### [25] [Amulet: Putting Complex Multi-Turn Conversations on the Stand with LLM Juries](https://arxiv.org/abs/2505.20451)
*Sahana Ramnath,Anurag Mudgil,Brihi Joshi,Skyler Hallinan,Xiang Ren*

Key words: LLM评估, 多轮对话, 对话行为, 准则, Amulet框架

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了Amulet框架，利用对话行为和准则提升LLM在多轮对话中的评估准确性，实验显示其在四个数据集上显著优于基线。

Motivation: 为解决LLM在多轮复杂对话中的评估不准确问题，提出了结合对话行为和准则的改进方法。

Method: 通过分析对话行为和准则（如对话意图和满意度），Amulet框架指导LLM更精准地评估多轮对话。

Result: 在四个数据集中，Amulet显著提升评估性能，显示出对话行为和准则在区分偏好响应中的重要性。

Conclusion: Amulet框架有效提升了LLM在多轮对话中的评估能力，支持其作为独立法官或陪审团成员的集成应用。

Abstract: Today, large language models are widely used as judges to evaluate responses
from other language models. Hence, it is imperative to benchmark and improve
these LLM-judges on real-world language model usage: a typical human-assistant
conversation is lengthy, and shows significant diversity in topics, intents,
and requirements across turns, e.g. social interactions, task requests,
feedback. We present Amulet, a framework that leverages pertinent linguistic
concepts of dialog-acts and maxims to improve the accuracy of LLM-judges on
preference data with complex, multi-turn conversational context. Amulet
presents valuable insights about (a) the communicative structures and intents
present in the conversation (dialog acts), and (b) the satisfaction of
conversational principles (maxims) by the preference responses, and uses them
to make judgments. On four challenging datasets, Amulet shows that (a) humans
frequently (60 to 70 percent of the time) change their intents from one turn of
the conversation to the next, and (b) in 75 percent of instances, the
preference responses can be differentiated via dialog acts and/or maxims,
reiterating the latter's significance in judging such data. Amulet can be used
either as a judge by applying the framework to a single LLM, or integrated into
a jury with different LLM judges; our judges and juries show strong
improvements on relevant baselines for all four datasets.

</details>


### [26] [Conversation Kernels: A Flexible Mechanism to Learn Relevant Context for Online Conversation Understanding](https://arxiv.org/abs/2505.20482)
*Vibhor Agarwal,Arjoo Gupta,Suparna De,Nishanth Sastry*

Key words: 在线对话分析, 上下文建模, Conversation Kernels, 社交网络, 语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种通用的对话上下文捕捉机制（Conversation Kernels），用于分析在线对话中的帖子内容，并通过在对话树中探索帖子的邻近上下文来适配不同任务需求。

Motivation: 随着社交网络和在线讨论论坛的兴起，理解在线对话变得重要。但因单个帖子通常简短且隐含依赖上下文，传统方法难以捕捉对话中的复杂依赖关系。

Method: 设计了两种Conversation Kernels家族，通过探索对话树中帖子的邻近上下文，构建适合不同任务的上下文表示，并应用于slashdot.org的数据集。

Result: 实验表明，该框架能够灵活适配多样化的对话理解任务（如判断帖子是否有趣、有洞察力等）。

Conclusion: Conversation Kernels是一种通用且灵活的机制，能够有效捕捉对话上下文，适用于多种在线对话分析任务。

Abstract: Understanding online conversations has attracted research attention with the
growth of social networks and online discussion forums. Content analysis of
posts and replies in online conversations is difficult because each individual
utterance is usually short and may implicitly refer to other posts within the
same conversation. Thus, understanding individual posts requires capturing the
conversational context and dependencies between different parts of a
conversation tree and then encoding the context dependencies between posts and
comments/replies into the language model.
  To this end, we propose a general-purpose mechanism to discover appropriate
conversational context for various aspects about an online post in a
conversation, such as whether it is informative, insightful, interesting or
funny. Specifically, we design two families of Conversation Kernels, which
explore different parts of the neighborhood of a post in the tree representing
the conversation and through this, build relevant conversational context that
is appropriate for each task being considered. We apply our developed method to
conversations crawled from slashdot.org, which allows users to apply highly
different labels to posts, such as 'insightful', 'funny', etc., and therefore
provides an ideal experimental platform to study whether a framework such as
Conversation Kernels is general-purpose and flexible enough to be adapted to
disparately different conversation understanding tasks.

</details>


### [27] [InFact: Informativeness Alignment for Improved LLM Factuality](https://arxiv.org/abs/2505.20487)
*Roi Cohen,Russa Biswas,Gerard de Melo*

Key words: 事实完整性, 信息量对齐, LLMs, 事实准确性, 信息丰富度

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了信息量对齐机制，旨在提高模型生成事实正确且信息丰富的文本的能力，发现优化信息量的同时也能提升事实准确性。

Motivation: 尽管LLMs倾向于生成事实正确的文本，但它们可能选择生成信息量不足的文本。本文旨在解决这一问题，通过信息量对齐机制提升文本的信息丰富度。

Method: 利用现有的事实基准，提出信息量对齐目标，优先选择既正确又信息丰富的答案，并通过训练模型优化这一目标。

Result: 研究发现，优化信息量对齐目标不仅能提升文本的信息量，还能间接提高事实准确性。

Conclusion: 信息量对齐机制是提升LLMs生成文本信息丰富度和事实准确性的有效方法。

Abstract: Factual completeness is a general term that captures how detailed and
informative a factually correct text is. For instance, the factual sentence
``Barack Obama was born in the United States'' is factually correct, though
less informative than the factual sentence ``Barack Obama was born in Honolulu,
Hawaii, United States''. Despite the known fact that LLMs tend to hallucinate
and generate factually incorrect text, they might also tend to choose to
generate factual text that is indeed factually correct and yet less informative
than other, more informative choices. In this work, we tackle this problem by
proposing an informativeness alignment mechanism. This mechanism takes
advantage of recent factual benchmarks to propose an informativeness alignment
objective. This objective prioritizes answers that are both correct and
informative. A key finding of our work is that when training a model to
maximize this objective or optimize its preference, we can improve not just
informativeness but also factuality.

</details>


### [28] [Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages](https://arxiv.org/abs/2505.20496)
*Asif Shahriar,Rifat Shahriyar,M Saifur Rahman*

Key words: Inceptive Transformer, 多尺度特征提取, 动态加权, 跨语言任务, 轻量级架构

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Inceptive Transformer 是一种轻量级模块化架构，通过多尺度特征提取模块增强 token 表示，平衡局部与全局依赖，在多个任务中显著优于基线模型。

Motivation: 传统 Transformer 将序列信息压缩到单一 CLS token，可能导致局部或层级信息丢失，需改进以捕获更丰富的特征。

Method: 采用类似 Inception 网络的多尺度特征提取模块，动态加权 token 以平衡局部与全局依赖。

Result: 在情感识别、反讽检测、疾病识别和疫苗推文分类等任务中，性能提升 1%-14%，且保持高效。

Conclusion: 该方法能有效增强跨语言、跨领域的 Transformer 表示，具有广泛适用性。

Abstract: Conventional transformer models typically compress the information from all
tokens in a sequence into a single \texttt{[CLS]} token to represent global
context-- an approach that can lead to information loss in tasks requiring
localized or hierarchical cues. In this work, we introduce \textit{Inceptive
Transformer}, a modular and lightweight architecture that enriches
transformer-based token representations by integrating a multi-scale feature
extraction module inspired by inception networks. Our model is designed to
balance local and global dependencies by dynamically weighting tokens based on
their relevance to a particular task. Evaluation across a diverse range of
tasks including emotion recognition (both English and Bangla), irony detection,
disease identification, and anti-COVID vaccine tweets classification shows that
our models consistently outperform the baselines by 1\% to 14\% while
maintaining efficiency. These findings highlight the versatility and
cross-lingual applicability of our method for enriching transformer-based
representations across diverse domains.

</details>


### [29] [Beyond Keywords: Evaluating Large Language Model Classification of Nuanced Ableism](https://arxiv.org/abs/2505.20500)
*Naba Rizvi,Harper Strickland,Saleha Ahmedi,Aekta Kallepalli,Isha Khirwadkar,William Wu,Imani N. S. Munyaka,Nedjma Ousidhoum*

Key words: large language models, ableism, autism, bias, context理解

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LLMs初步能识别自闭症相关语言，但常忽略有害或冒犯性内容，且依赖关键词匹配而非上下文理解。与人类相比，LLMs的解释较表面，但两者在分类方式上一致。

Motivation: 探讨LLMs在识别针对自闭症患者的隐性歧视方面的能力，填补现有研究对LLMs如何理解或检测残疾相关偏见的空白。

Method: 评估四个LLMs识别自闭症相关歧视语言的能力，对比其术语理解与上下文识别效果，并进行人类与LLMs解释的定性比较。

Result: LLMs能识别自闭症相关词但常忽略负面含义，依赖关键词匹配导致误判；人类更关注上下文和影响，但两者对分类方式达成共识。

Conclusion: LLMs在识别隐性残疾歧视上表现有限，需改进上下文理解；二元分类适用于评估性能，与人类标注结论一致。

Abstract: Large language models (LLMs) are increasingly used in decision-making tasks
like r\'esum\'e screening and content moderation, giving them the power to
amplify or suppress certain perspectives. While previous research has
identified disability-related biases in LLMs, little is known about how they
conceptualize ableism or detect it in text. We evaluate the ability of four
LLMs to identify nuanced ableism directed at autistic individuals. We examine
the gap between their understanding of relevant terminology and their
effectiveness in recognizing ableist content in context. Our results reveal
that LLMs can identify autism-related language but often miss harmful or
offensive connotations. Further, we conduct a qualitative comparison of human
and LLM explanations. We find that LLMs tend to rely on surface-level keyword
matching, leading to context misinterpretations, in contrast to human
annotators who consider context, speaker identity, and potential impact. On the
other hand, both LLMs and humans agree on the annotation scheme, suggesting
that a binary classification is adequate for evaluating LLM performance, which
is consistent with findings from prior studies involving human annotators.

</details>


### [30] [Gatsby Without the 'E': Crafting Lipograms with LLMs](https://arxiv.org/abs/2505.20501)
*Rohan Balasubramanian,Nitish Gokulakrishnan,Syeda Jannatus Saba,Steven Skiena*

Key words: lipogram, large language models, constrained writing, text generation, linguistic creativity

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探索了现代大型语言模型(LLM)将《了不起的盖茨比》转化为无字母'e'文本的能力，通过各种技术手段验证约束写作的可行性。

Motivation: 探讨在现代LLM支持下，严格约束写作（如避开特定字母）对文本生成的影响及其语言学意义。

Method: 采用从基础的同义词替换到复杂生成模型（如beam search和命名实体分析）的多层次技术。

Result: 避开常见字母（最高到'u'）对文本意义影响较小，但随着约束增强，翻译质量快速下降。

Conclusion: 英语在严格约束下表现出惊人的灵活性和创造性，凸显语言的适应能力和创新潜力。

Abstract: Lipograms are a unique form of constrained writing where all occurrences of a
particular letter are excluded from the text, typified by the novel Gadsby,
which daringly avoids all usage of the letter 'e'. In this study, we explore
the power of modern large language models (LLMs) by transforming the novel F.
Scott Fitzgerald's The Great Gatsby into a fully 'e'-less text. We experimented
with a range of techniques, from baseline methods like synonym replacement to
sophisticated generative models enhanced with beam search and named entity
analysis. We show that excluding up to 3.6% of the most common letters (up to
the letter 'u') had minimal impact on the text's meaning, although translation
fidelity rapidly and predictably decays with stronger lipogram constraints. Our
work highlights the surprising flexibility of English under strict constraints,
revealing just how adaptable and creative language can be.

</details>


### [31] [Large Language Models for IT Automation Tasks: Are We There Yet?](https://arxiv.org/abs/2505.20505)
*Md Mahadi Hassan,John Salvador,Akond Rahman,Santu Karmaker*

Key words: LLM, IT automation, Ansible, benchmark, state reasoning

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LLMs在Ansible等IT自动化工具上的表现不佳，ITAB基准测试显示多数开源LLM在功能脚本生成上成功率低于12%，主要问题在于状态推理和模块知识不足。

Motivation: 当前LLM在代码生成领域表现优异，但针对IT自动化工具（如Ansible）的实际需求研究不足，需开发更贴近实践的评测基准。

Method: 提出了ITAB基准，包含126项多样化任务，通过动态执行评估14款开源LLM生成Ansible脚本的能力。

Result: 无LLM的pass@10超过12%。失败分析显示44.87%源于状态推理错误（如变量、路径等），24.37%源于模块知识不足（如参数、属性等）。

Conclusion: 开源LLM在IT自动化中可靠性受限，未来需提升状态推理及领域专用知识能力。

Abstract: LLMs show promise in code generation, yet their effectiveness for IT
automation tasks, particularly for tools like Ansible, remains understudied.
Existing benchmarks rely primarily on synthetic tasks that fail to capture the
needs of practitioners who use IT automation tools, such as Ansible. We present
ITAB (IT Automation Task Benchmark), a benchmark of 126 diverse tasks (e.g.,
configuring servers, managing files) where each task accounts for state
reconciliation: a property unique to IT automation tools. ITAB evaluates LLMs'
ability to generate functional Ansible automation scripts via dynamic execution
in controlled environments. We evaluate 14 open-source LLMs, none of which
accomplish pass@10 at a rate beyond 12%. To explain these low scores, we
analyze 1,411 execution failures across the evaluated LLMs and identify two
main categories of prevalent semantic errors: failures in state reconciliation
related reasoning (44.87% combined from variable (11.43%), host (11.84%),
path(11.63%), and template (9.97%) issues) and deficiencies in module-specific
execution knowledge (24.37% combined from Attribute and parameter (14.44%) and
module (9.93%) errors). Our findings reveal key limitations in open-source
LLMs' ability to track state changes and apply specialized module knowledge,
indicating that reliable IT automation will require major advances in state
reasoning and domain-specific execution understanding.

</details>


### [32] [ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis](https://arxiv.org/abs/2505.20506)
*Hawau Olamide Toyin,Rufael Marew,Humaid Alblooshi,Samar M. Magdy,Hanan Aldarmaki*

Key words: ArVoice, 语音语料库, 阿拉伯语, 语音合成, 语音转换

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ArVoice是一个多说话者的现代标准阿拉伯语（MSA）语音语料库，包含带音标的转录，可用于多说话者语音合成及其他任务如音标恢复、语音转换和深度伪造检测。

Motivation: 为多说话者语音合成和阿拉伯语相关语音任务提供高质量的语音语料库。

Method: 结合专业录制的语音、修改的阿拉伯语音语料库子集和商业系统的高质量合成语音，共83.52小时语音数据。

Result: 训练了三个开源TTS系统和两个语音转换系统，展示了语料库的应用潜力。

Conclusion: ArVoice语料库为阿拉伯语语音研究提供了丰富资源，并已验证其实际应用价值。

Abstract: We introduce ArVoice, a multi-speaker Modern Standard Arabic (MSA) speech
corpus with diacritized transcriptions, intended for multi-speaker speech
synthesis, and can be useful for other tasks such as speech-based diacritic
restoration, voice conversion, and deepfake detection. ArVoice comprises: (1) a
new professionally recorded set from six voice talents with diverse
demographics, (2) a modified subset of the Arabic Speech Corpus; and (3)
high-quality synthetic speech from two commercial systems. The complete corpus
consists of a total of 83.52 hours of speech across 11 voices; around 10 hours
consist of human voices from 7 speakers. We train three open-source TTS and two
voice conversion systems to illustrate the use cases of the dataset. The corpus
is available for research use.

</details>


### [33] [Multimodal Emotion Recognition in Conversations: A Survey of Methods, Trends, Challenges and Prospects](https://arxiv.org/abs/2505.20511)
*Chengyan Wu,Yiqiang Cai,Yang Liu,Pengxu Zhu,Yun Xue,Ziwei Gong,Julia Hirschberg,Bolei Ma*

Key words: 多模态情感识别，对话系统，人机交互，情感理解

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇综述系统概述了多模态对话情感识别（MERC）的动机、核心任务、代表性方法及评估策略，并探讨了未来研究方向。

Motivation: 现实对话系统需要超越单一模态的情感理解，提升人机交互的自然度与情感识别准确性，因此多模态情感识别成为关键。

Method: 整合文本、语音和视觉等多模态信息进行情感识别，总结了代表性方法和评估策略。

Result: 系统梳理了MERC的现状、趋势与挑战。

Conclusion: 情感智能系统需求增长，本综述为MERC研究提供了及时指导。

Abstract: While text-based emotion recognition methods have achieved notable success,
real-world dialogue systems often demand a more nuanced emotional understanding
than any single modality can offer. Multimodal Emotion Recognition in
Conversations (MERC) has thus emerged as a crucial direction for enhancing the
naturalness and emotional understanding of human-computer interaction. Its goal
is to accurately recognize emotions by integrating information from various
modalities such as text, speech, and visual signals.
  This survey offers a systematic overview of MERC, including its motivations,
core tasks, representative methods, and evaluation strategies. We further
examine recent trends, highlight key challenges, and outline future directions.
As interest in emotionally intelligent systems grows, this survey provides
timely guidance for advancing MERC research.

</details>


### [34] [AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy](https://arxiv.org/abs/2505.20538)
*Sebastian Antony Joseph,Syed Murtaza Husain,Stella S. R. Offner,Stéphanie Juneau,Paul Torrey,Adam S. Bolton,Juan P. Farias,Niall Gaffney,Greg Durrett,Junyi Jessy Li*

Key words: LLMs, 科学计算, 可视化, 天文学, AstroVisBench

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了AstroVisBench，首个用于评估LLMs在天文学领域处理数据及可视化能力的基准。

Motivation: 当前缺乏评估LLMs在科学研究中是否能够正确生成科学见解的方法，尤其是在天文学领域的数据处理和可视化方面。

Method: 引入AstroVisBench基准，通过LLM-as-a-judge工作流评估LLMs的科学计算和可视化能力，并与专业天文学家标注对比验证。

Result: 评估显示，当前最先进的LLMs在天文学研究中的实用性存在显著差距。

Conclusion: AstroVisBench为AI科学家提供了一个端到端的评估工具，推动了可视化工作流程的发展。

Abstract: Large Language Models (LLMs) are being explored for applications in
scientific research, including their capabilities to synthesize literature,
answer research questions, generate research ideas, and even conduct
computational experiments. Ultimately, our goal is for these to help scientists
derive novel scientific insights. In many areas of science, such insights often
arise from processing and visualizing data to understand its patterns. However,
evaluating whether an LLM-mediated scientific workflow produces outputs
conveying the correct scientific insights is challenging to evaluate and has
not been addressed in past work. We introduce AstroVisBench, the first
benchmark for both scientific computing and visualization in the astronomy
domain. AstroVisBench judges a language model's ability to both (1) create
astronomy-specific workflows to process and analyze data and (2) visualize the
results of these workflows through complex plots. Our evaluation of
visualizations uses a novel LLM-as-a-judge workflow, which is validated against
annotation by five professional astronomers. Using AstroVisBench we present an
evaluation of state-of-the-art language models, showing a significant gap in
their ability to engage in astronomy research as useful assistants. This
evaluation provides a strong end-to-end evaluation for AI scientists that
offers a path forward for the development of visualization-based workflows,
which are central to a broad range of domains from physics to biology.

</details>


### [35] [Paths Not Taken: Understanding and Mending the Multilingual Factual Recall Pipeline](https://arxiv.org/abs/2505.20546)
*Meng Lu,Ruochen Zhang,Ellie Pavlick,Carsten Eickhoff*

Key words: 多语言大模型, 事实一致性, 机理分析, 干预方法, 回译

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了多语言大模型（LLMs）在跨语言事实召回任务中的表现不一致问题，发现模型依赖英语机制处理多语言查询并通过回译生成答案，提出了两种干预方法提升性能。

Motivation: 多语言大模型在非英语语言的事实召回任务中表现显著差于英语，但其失败原因尚不明确，需要探究并改进。

Method: 通过机理分析技术揭示LLMs的多语言处理流程（英语机制+回译），提出两种与语言和数据集无关的向量干预方法，优化模型内部路径。

Result: 干预方法将最低性能语言的召回准确率提升了35%以上，证明了机理分析对释放LLMs多语言潜力的有效性。

Conclusion: 研究揭示了LLMs多语言事实不一致的根源，并通过针对性干预显著提升性能，为改善多语言能力提供了新思路。

Abstract: Multilingual large language models (LLMs) often exhibit factual
inconsistencies across languages, with significantly better performance in
factual recall tasks in English than in other languages. The causes of these
failures, however, remain poorly understood. Using mechanistic analysis
techniques, we uncover the underlying pipeline that LLMs employ, which involves
using the English-centric factual recall mechanism to process multilingual
queries and then translating English answers back into the target language. We
identify two primary sources of error: insufficient engagement of the reliable
English-centric mechanism for factual recall, and incorrect translation from
English back into the target language for the final answer. To address these
vulnerabilities, we introduce two vector interventions, both independent of
languages and datasets, to redirect the model toward better internal paths for
higher factual consistency. Our interventions combined increase the recall
accuracy by over 35 percent for the lowest-performing language. Our findings
demonstrate how mechanistic insights can be used to unlock latent multilingual
capabilities in LLMs.

</details>


### [36] [The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages](https://arxiv.org/abs/2505.20564)
*Chris Emezue,The NaijaVoices Community,Busayo Awobade,Abraham Owodunni,Handel Emezue,Gloria Monica Tobechukwu Emezue,Nefertiti Nneoma Emezue,Sewade Ogun,Bunmi Akinremi,David Ifeoluwa Adelani,Chris Pal*

Key words: 非洲语言,语音识别,多语言处理,数据集,NaijaVoices

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究针对非洲语言（伊博语、豪萨语和约鲁巴语）数据不足的问题，推出了NaijaVoices数据集，包含1,800小时的语音文本数据，覆盖5,000+说话者，显著提升语音识别模型的性能。

Motivation: 非洲语言在语音技术中代表性不足，现有数据集规模有限，限制了约十亿人口的技术可及性。

Method: 采用独特的数据收集方法构建NaijaVoices数据集，并通过自动语音识别实验验证其效果。

Result: 在Whisper、MMS和XLSR模型上，平均WER分别提升了75.86%、52.06%和42.33%。

Conclusion: NaijaVoices数据集能有效促进非洲语言的多语言语音处理发展。

Abstract: The development of high-performing, robust, and reliable speech technologies
depends on large, high-quality datasets. However, African languages --
including our focus, Igbo, Hausa, and Yoruba -- remain under-represented due to
insufficient data. Popular voice-enabled technologies do not support any of the
2000+ African languages, limiting accessibility for circa one billion people.
While previous dataset efforts exist for the target languages, they lack the
scale and diversity needed for robust speech models. To bridge this gap, we
introduce the NaijaVoices dataset, a 1,800-hour speech-text dataset with 5,000+
speakers. We outline our unique data collection approach, analyze its acoustic
diversity, and demonstrate its impact through finetuning experiments on
automatic speech recognition, averagely achieving 75.86% (Whisper), 52.06%
(MMS), and 42.33% (XLSR) WER improvements. These results highlight NaijaVoices'
potential to advance multilingual speech processing for African languages.

</details>


### [37] [Emotion Classification In-Context in Spanish](https://arxiv.org/abs/2505.20571)
*Bipul Thapa,Gabriel Cofre*

Key words: 情感分类, TF-IDF, BERT, 自定义堆栈集成, 西班牙语

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种结合TF-IDF和BERT嵌入的混合方法，通过自定义堆栈集成（CSE）对西班牙语客户反馈进行情感分类，显著提升了分类准确率。

Motivation: 解决传统方法在多语言情感分类中因翻译导致语义和上下文信息丢失的问题，专注于西班牙语的情感分类。

Method: 采用TF-IDF和BERT嵌入生成文本数值表示，并使用CSE结合多种分类器（如Logistic Regression、KNN等）进行情感分类。

Result: CSE模型在西班牙语数据集上达到93.3%的测试准确率，优于单一模型和仅使用BERT的方法。

Conclusion: 结合TF-IDF和BERT的混合方法在西班牙语情感分类中效果显著，为提升客户反馈分析提供了有效技术。

Abstract: Classifying customer feedback into distinct emotion categories is essential
for understanding sentiment and improving customer experience. In this paper,
we classify customer feedback in Spanish into three emotion
categories--positive, neutral, and negative--using advanced NLP and ML
techniques. Traditional methods translate feedback from widely spoken languages
to less common ones, resulting in a loss of semantic integrity and contextual
nuances inherent to the original language. To address this limitation, we
propose a hybrid approach that combines TF-IDF with BERT embeddings,
effectively transforming Spanish text into rich numerical representations that
preserve the semantic depth of the original language by using a Custom Stacking
Ensemble (CSE) approach. To evaluate emotion classification, we utilize a range
of models, including Logistic Regression, KNN, Bagging classifier with LGBM,
and AdaBoost. The CSE model combines these classifiers as base models and uses
a one-vs-all Logistic Regression as the meta-model. Our experimental results
demonstrate that CSE significantly outperforms the individual and BERT model,
achieving a test accuracy of 93.3% on the native Spanish dataset--higher than
the accuracy obtained from the translated version. These findings underscore
the challenges of emotion classification in Spanish and highlight the
advantages of combining vectorization techniques like TF-IDF with BERT for
improved accuracy. Our results provide valuable insights for businesses seeking
to leverage emotion classification to enhance customer feedback analysis and
service improvements.

</details>


### [38] [Effectiveness of Prompt Optimization in NL2SQL Systems](https://arxiv.org/abs/2505.20591)
*Sairam Gurajada,Eser Kandogan,Sajjadur Rahman*

Key words: NL2SQL, 提示优化, 多目标优化, 生产场景, 大型语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文提出了一个NL2SQL提示优化框架，通过多目标优化解决生产场景中对高精度和高性能SQL生成的需求。

Motivation: 现有NL2SQL方法虽能生成高质量SQL，但在生产场景中需兼顾高精度和高性能，而静态示例集的优化是关键。

Method: 提出基于多目标优化的提示框架，优化静态示例集选择，涵盖查询日志、数据库结构、SQL构建和执行延迟等因素。

Result: 初步实验验证了框架的有效性，能够满足高精度和高性能需求。

Conclusion: 静态示例集的精细选择比单纯相似性检索更能提升NL2SQL系统性能，适用于生产环境。

Abstract: NL2SQL approaches have greatly benefited from the impressive capabilities of
large language models (LLMs). In particular, bootstrapping an NL2SQL system for
a specific domain can be as simple as instructing an LLM with sufficient
contextual information, such as schema details and translation demonstrations.
However, building an accurate system still requires the rigorous task of
selecting the right context for each query-including identifying relevant
schema elements, cell values, and suitable exemplars that help the LLM
understand domain-specific nuances. Retrieval-based methods have become the
go-to approach for identifying such context. While effective, these methods
introduce additional inference-time costs due to the retrieval process.
  In this paper, we argue that production scenarios demand high-precision,
high-performance NL2SQL systems, rather than simply high-quality SQL
generation, which is the focus of most current NL2SQL approaches. In such
scenarios, the careful selection of a static set of exemplars-capturing the
intricacies of the query log, target database, SQL constructs, and execution
latencies-plays a more crucial role than exemplar selection based solely on
similarity. The key challenge, however, lies in identifying a representative
set of exemplars for a given production setting. To this end, we propose a
prompt optimization framework that not only addresses the high-precision
requirement but also optimizes the performance of the generated SQL through
multi-objective optimization. Preliminary empirical analysis demonstrates the
effectiveness of the proposed framework.

</details>


### [39] [Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation](https://arxiv.org/abs/2505.20606)
*Dancheng Liu,Amir Nassereldine,Chenhui Xu,Jinjun Xiong*

Key words: ASR, 声学多样性, 数据增强, 鲁棒性, Librispeech

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究发现，ASR模型的鲁棒性主要受声学多样性而非语言丰富性的影响，通过针对性的声学数据增强可以在小规模数据集上显著提升性能。

Motivation: 探索在小规模数据集上如何通过声学多样性提升ASR模型的鲁棒性，以解决大规模数据集不可行的问题。

Method: 分析声学和语言多样性对ASR模型的影响，并采用目标声学增强方法提升性能。

Result: 在使用960小时Librispeech数据集训练时，声学增强方法在未见数据集上词错误率降低达19.24%。

Conclusion: 声学数据增强是构建鲁棒ASR模型的有效替代方案，尤其在缺乏大规模数据时。

Abstract: Whisper's robust performance in automatic speech recognition (ASR) is often
attributed to its massive 680k-hour training set, an impractical scale for most
researchers. In this work, we examine how linguistic and acoustic diversity in
training data affect the robustness of the ASR model and reveal that
transcription generalization is primarily driven by acoustic variation rather
than linguistic richness. We find that targeted acoustic augmentation methods
could significantly improve the generalization ability of ASR models, reducing
word-error rates by up to 19.24 percent on unseen datasets when training on the
960-hour Librispeech dataset. These findings highlight strategic acoustically
focused data augmentation as a promising alternative to massive datasets for
building robust ASR models, offering a potential solution to future foundation
ASR models when massive human speech data is lacking.

</details>


### [40] [REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning](https://arxiv.org/abs/2505.20613)
*Ziju Shen,Naohao Huang,Fanyi Yang,Yutong Wang,Guoxiong Gao,Tianyi Xu,Jiedong Jiang,Wanyi He,Pu Yang,Mengzhou Sun,Haocheng Ju,Peihao Wu,Bryan Dai,Bin Dong*

Key words: 定理证明器, Lean 4, 大型语言模型, 检索系统, 数学问题

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了REAL-Prover，一个基于大型语言模型和检索系统的定理证明器，显著提高了解决大学数学问题的性能，并在ProofNet和FATE-M数据集上取得了竞争性和SOTA的结果。

Motivation: 现有的定理证明器在初等数学上表现良好，但在高等数学上泛化能力不足。本文旨在通过引入REAL-Prover填补这一空白。

Method: 基于微调的大型语言模型（REAL-Prover-v1）和检索系统（Leansearch-PS），结合数据提取管道（HERALD-AF）和交互式环境（Jixia-interactive），进行监督微调。

Result: 在ProofNet数据集上达到23.7%的成功率（Pass@64），在FATE-M数据集上达到56.7%的成功率（SOTA）。

Conclusion: REAL-Prover在高等数学问题的定理证明中表现出色，为未来的研究提供了有效工具和基准。

Abstract: Nowadays, formal theorem provers have made monumental progress on high-school
and competition-level mathematics, but few of them generalize to more advanced
mathematics. In this paper, we present REAL-Prover, a new open-source stepwise
theorem prover for Lean 4 to push this boundary. This prover, based on our
fine-tuned large language model (REAL-Prover-v1) and integrated with a
retrieval system (Leansearch-PS), notably boosts performance on solving
college-level mathematics problems. To train REAL-Prover-v1, we developed
HERALD-AF, a data extraction pipeline that converts natural language math
problems into formal statements, and a new open-source Lean 4 interactive
environment (Jixia-interactive) to facilitate synthesis data collection. In our
experiments, our prover using only supervised fine-tune achieves competitive
results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable
to state-of-the-art (SOTA) models. To further evaluate our approach, we
introduce FATE-M, a new benchmark focused on algebraic problems, where our
prover achieves a SOTA success rate of 56.7% (Pass@64).

</details>


### [41] [SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation](https://arxiv.org/abs/2505.20622)
*Ting Xu,Zhichao Huang,Jiankai Sun,Shanbo Cheng,Wai Lam*

Key words: 同步机器翻译, 序列策略优化, 强化学习, 延迟优化, 大语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SeqPO-SiMT是一种新的政策优化框架，将同步机器翻译任务视为序列决策问题，通过定制奖励提升翻译质量并降低延迟。它在多步任务中表现优于单步RLHF方法，显著提升翻译质量且延迟更低。

Motivation: 同步机器翻译（SiMT）需要实时翻译且延迟低，传统方法如RLHF（如PPO和DPO）适用于单步任务，但在多步SiMT任务中效果有限。SeqPO-SiMT旨在通过序列决策框架优化SiMT任务。

Method: SeqPO-SiMT将SiMT定义为序列决策问题，使用定制奖励模拟和优化翻译过程。实验在六个数据集上进行，涵盖En↔Zh任务。

Result: SeqPO-SiMT在翻译质量（COMET提升1.13分）和延迟（Average Lagging降低6.17）上均优于监督微调模型，且在7B LLM上的SiMT表现媲美高性能离线翻译模型。

Conclusion: SeqPO-SiMT通过序列决策和定制奖励有效解决了SiMT任务的多步优化问题，显著提升了翻译质量并降低了延迟。

Abstract: We present Sequential Policy Optimization for Simultaneous Machine
Translation (SeqPO-SiMT), a new policy optimization framework that defines the
simultaneous machine translation (SiMT) task as a sequential decision making
problem, incorporating a tailored reward to enhance translation quality while
reducing latency. In contrast to popular Reinforcement Learning from Human
Feedback (RLHF) methods, such as PPO and DPO, which are typically applied in
single-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task.
This intuitive framework allows the SiMT LLMs to simulate and refine the SiMT
process using a tailored reward. We conduct experiments on six datasets from
diverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that
SeqPO-SiMT consistently achieves significantly higher translation quality with
lower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning
(SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17
in the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context
than offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly
rival the offline translation of high-performing LLMs, including
Qwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct.

</details>


### [42] [POLAR: A Benchmark for Multilingual, Multicultural, and Multi-Event Online Polarization](https://arxiv.org/abs/2505.20624)
*Usman Naseem,Juan Ren,Saba Anwar,Sarah Kohail,Rudy Alexandro Garrido Veliz,Robert Geislinger,Aisha Jabr,Idris Abdulmumin,Laiba Qureshi,Aarushi Ajay Borkar,Maryam Ibrahim Mukhtar,Abinew Ali Ayele,Ibrahim Said Ahmad,Adem Ali,Martin Semmann,Shamsuddeen Hassan Muhammad,Seid Muhie Yimam*

Key words: 在线极化, 多语言, 多文化, POLAR数据集, NLP, 计算社会科学

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: POLAR是一个多语言、多文化、多事件的数据集，用于研究在线极化现象，标注了极化的三个方面：存在、类型和表现。实验表明，虽然大多数模型在二元极化检测中表现良好，但在预测极化类型和表现时得分较低，凸显了极化的复杂性和上下文依赖性。

Motivation: 在线极化对民主话语构成挑战，但现有研究多为单一语言、文化或事件。POLAR旨在提供多语言、多文化的数据集，以促进全球范围内的极化研究。

Method: 引入POLAR数据集，使用多种标注平台标注极化，并进行两个实验：微调多语言预训练模型（单语和跨语），评估大型语言模型在少样本和零样本场景下的表现。

Result: 大多数模型在二元极化检测中表现良好，但在极化类型和表现预测上表现较差，显示极化的复杂性和上下文依赖性。

Conclusion: 极化具有高度上下文依赖性和复杂性，需要在NLP和计算社会科学中采用健壮、适应性强的方法。

Abstract: Online polarization poses a growing challenge for democratic discourse, yet
most computational social science research remains monolingual, culturally
narrow, or event-specific. We introduce POLAR, a multilingual, multicultural,
and multievent dataset with over 23k instances in seven languages from diverse
online platforms and real-world events. Polarization is annotated along three
axes: presence, type, and manifestation, using a variety of annotation
platforms adapted to each cultural context. We conduct two main experiments:
(1) we fine-tune six multilingual pretrained language models in both
monolingual and cross-lingual setups; and (2) we evaluate a range of open and
closed large language models (LLMs) in few-shot and zero-shot scenarios.
Results show that while most models perform well on binary polarization
detection, they achieve substantially lower scores when predicting polarization
types and manifestations. These findings highlight the complex, highly
contextual nature of polarization and the need for robust, adaptable approaches
in NLP and computational social science. All resources will be released to
support further research and effective mitigation of digital polarization
globally.

</details>


### [43] [Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration](https://arxiv.org/abs/2505.20625)
*Sibo Xiao,Zixin Lin,Wenyang Gao,Yue Zhang*

Key words: 长上下文处理,多智能体框架,动态分区,大语言模型,性能优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为XpandA的多智能体框架，结合动态分区和问题驱动工作流，显著提升大语言模型处理长上下文的能力，性能优于基线方法。

Motivation: 现有基于智能体的分治法在处理长上下文时存在延迟累积、信息丢失过度和文本依赖破坏等问题，亟需更高效的解决方案。

Method: XpandA框架采用动态分区、问题引导协议和选择性重放机制，优化上下文窗口填充率和跨分区知识一致性。

Result: 实验表明，XpandA在1k至1M长度范围的基准测试中表现优异，性能提升20%，推理速度加快1.5倍。

Conclusion: XpandA为超长序列处理提供了可行方案，显著增强了大语言模型的长上下文能力。

Abstract: Processing long contexts has become a critical capability for modern large
language models (LLMs). Existing works leverage agent-based divide-and-conquer
methods for processing long contexts. But these methods face crucial
limitations, including prohibitive accumulated latency and amplified
information loss from excessive agent invocations, and the disruption of
inherent textual dependencies by immoderate partitioning. In this paper, we
propose a novel multi-agent framework XpandA (Expand-Agent) coupled with
question-driven workflow and dynamic partitioning for robust long-context
processing. XpandA overcomes these limitations through: 1) dynamic partitioning
of long texts, which adaptively modulates the filling rate of context windows
for input sequences of vastly varying lengths; 2) question-guided protocol to
update flat information ensembles within centralized shared memory,
constructing consistent inter-agent knowledge across partitions; and 3)
selectively replaying specific partitions based on the state-tracking of
question-information couples to promote the resolution of inverted-order
structures across partitions (e.g., flashbacks). We perform a comprehensive
evaluation of XpandA on multiple long-context benchmarks with length varying
from 1k to 1M, demonstrating XpandA's feasibility for processing ultra-long
sequences and its significant effectiveness in enhancing the long-context
capabilities of various LLMs by achieving 20\% improvements and 1.5x inference
speedup over baselines of full-context, RAG and previous agent-based methods.

</details>


### [44] [Test-Time Learning for Large Language Models](https://arxiv.org/abs/2505.20633)
*Jinwu Hu,Zhitian Zhang,Guohao Chen,Xutao Wen,Chao Shuai,Wei Luo,Bin Xiao,Yuanqing Li,Mingkui Tan*

Key words: LLM, 测试时学习, 领域适应, 困惑度最小化, LoRA

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出测试时学习范式TLM，通过无标签测试数据动态适配领域分布，降低输入困惑度提升LLM性能，采用高效样本选择与LoRA优化，实验显示领域适应性能提升20%。

Motivation: LLM在专业领域和语言变体泛化中存在局限，需动态适配测试数据以应对分布偏移。

Method: 1. 基于输入困惑度最小化构建自监督优化目标；2. 高困惑度样本优先学习策略；3. 采用LoRA轻量级参数更新。

Result: AdaptEval基准测试显示TLM比原始LLM领域适应性能提升≥20%。

Conclusion: TLM通过测试时动态优化显著提升LLM领域适应性，且保持模型稳定性。

Abstract: While Large Language Models (LLMs) have exhibited remarkable emergent
capabilities through extensive pre-training, they still face critical
limitations in generalizing to specialized domains and handling diverse
linguistic variations, known as distribution shifts. In this paper, we propose
a Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically
adapts LLMs to target domains using only unlabeled test data during testing.
Specifically, we first provide empirical evidence and theoretical insights to
reveal that more accurate predictions from LLMs can be achieved by minimizing
the input perplexity of the unlabeled test data. Based on this insight, we
formulate the Test-Time Learning process of LLMs as input perplexity
minimization, enabling self-supervised enhancement of LLM performance.
Furthermore, we observe that high-perplexity samples tend to be more
informative for model optimization. Accordingly, we introduce a Sample
Efficient Learning Strategy that actively selects and emphasizes these
high-perplexity samples for test-time updates. Lastly, to mitigate catastrophic
forgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA)
instead of full-parameter optimization, which allows lightweight model updates
while preserving more original knowledge from the model. We introduce the
AdaptEval benchmark for TTL and demonstrate through experiments that TLM
improves performance by at least 20% compared to original LLMs on domain
knowledge adaptation.

</details>


### [45] [Graph RAG for Legal Norms: A Hierarchical and Temporal Approach](https://arxiv.org/abs/2505.00039)
*Hudson de Martim*

Key words: Graph RAG, 法律规范, 知识图谱, 人工智能, 法律研究

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一个针对法律规范分析的Graph RAG改进方法，结合知识图谱和文本片段，解决法律数据的复杂性和大量性问题。

Motivation: 法律规范具有预定义的层次结构、内外引用网络和多时间版本，传统的分析方法难以应对其复杂性和规模。

Method: 结合层次结构和时间演化的知识图谱以及全面的文本单元，构建更丰富的法律知识表示。

Result: Graph RAG在法律规范数据集上的应用，推动了人工智能在法律领域的应用，为法律研究、立法分析和决策支持提供了更有效的系统。

Conclusion: Graph RAG为解决法律数据的复杂性问题提供了有效方案，并促进了人工智能在法律领域的应用。

Abstract: This article proposes an adaptation of Graph Retrieval Augmented Generation
(Graph RAG) specifically designed for the analysis and comprehension of legal
norms, which are characterized by their predefined hierarchical structure,
extensive network of internal and external references and multiple temporal
versions. By combining structured knowledge graphs with contextually enriched
text segments, Graph RAG offers a promising solution to address the inherent
complexity and vast volume of legal data. The integration of hierarchical
structure and temporal evolution into knowledge graphs - along with the concept
of comprehensive Text Units - facilitates the construction of richer,
interconnected representations of legal knowledge. Through a detailed analysis
of Graph RAG and its application to legal norm datasets, this article aims to
advance the field of Artificial Intelligence applied to Law, creating
opportunities for more effective systems in legal research, legislative
analysis, and decision support.

</details>


### [46] [STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models](https://arxiv.org/abs/2505.20645)
*Kai Chen,Zihao He,Taiwei Shi,Kristina Lerman*

Key words: 大语言模型, 社区规范, 评价基准, 文化多样性, 适应性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了Steer-Bench基准，用于评估大语言模型（LLMs）在不同社区规范和视角下的适应能力，发现现有模型表现显著低于人类水平。

Motivation: 研究动机在于评估LLMs能否适应多样化社区规范和沟通风格，这对实际应用至关重要，但此前缺乏系统评估。

Method: 方法是通过30对对比Reddit子社区的10,000条指令-响应对和5,500个多选问题，测试13种流行LLMs的表现。

Result: 结果显示，人类专家准确率达81%，而表现最佳的模型约为65%，某些模型落后人类15个百分点以上。

Conclusion: 结论是现有模型在社区敏感性引导能力上存在显著差距，Steer-Bench为系统评估提供了工具。

Abstract: Steerability, or the ability of large language models (LLMs) to adapt outputs
to align with diverse community-specific norms, perspectives, and communication
styles, is critical for real-world applications but remains under-evaluated. We
introduce Steer-Bench, a benchmark for assessing population-specific steering
using contrasting Reddit communities. Covering 30 contrasting subreddit pairs
across 19 domains, Steer-Bench includes over 10,000 instruction-response pairs
and validated 5,500 multiple-choice question with corresponding silver labels
to test alignment with diverse community norms. Our evaluation of 13 popular
LLMs using Steer-Bench reveals that while human experts achieve an accuracy of
81% with silver labels, the best-performing models reach only around 65%
accuracy depending on the domain and configuration. Some models lag behind
human-level alignment by over 15 percentage points, highlighting significant
gaps in community-sensitive steerability. Steer-Bench is a benchmark to
systematically assess how effectively LLMs understand community-specific
instructions, their resilience to adversarial steering attempts, and their
ability to accurately represent diverse cultural and ideological perspectives.

</details>


### [47] [FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information](https://arxiv.org/abs/2505.20650)
*Yan Wang,Yang Ren,Lingfei Qian,Xueqing Peng,Keyi Wang,Yi Han,Dongji Feng,Xiao-Yang Liu,Jimin Huang,Qianqian Xie*

Key words: FinTagging, XBRL, LLMs, financial reporting, semantic alignment

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: FinTagging是一个全面的、表格感知的XBRL基准测试，用于评估大型语言模型（LLMs）在XBRL财务报告中的结构化信息提取和语义对齐能力。通过分解为两项子任务（FinNI和FinCL），并对多款LLMs进行零样本测试，发现其在细粒度概念对齐上表现不佳。

Motivation: 现有基准测试过于简化XBRL标记为平面多分类问题，且仅关注叙事文本。FinTagging旨在填补这一空白，提供更真实、细粒度的评估框架。

Method: 将XBRL标记问题分解为FinNI（财务实体提取）和FinCL（分类驱动的概念对齐）两项子任务，评估LLMs在零样本设置下的表现。

Result: LLMs在信息提取上表现良好，但在细粒度概念对齐上（尤其是区分紧密相关的分类条目）表现不佳。

Conclusion: 现有LLMs在完全自动化XBRL标记上仍有限制，需改进语义推理和模式感知建模以提高财务披露的准确性。

Abstract: We introduce FinTagging, the first full-scope, table-aware XBRL benchmark
designed to evaluate the structured information extraction and semantic
alignment capabilities of large language models (LLMs) in the context of
XBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL
tagging as flat multi-class classification and focus solely on narrative text,
FinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for
financial entity extraction and FinCL for taxonomy-driven concept alignment. It
requires models to jointly extract facts and align them with the full 10k+
US-GAAP taxonomy across both unstructured text and structured tables, enabling
realistic, fine-grained evaluation. We assess a diverse set of LLMs under
zero-shot settings, systematically analyzing their performance on both subtasks
and overall tagging accuracy. Our results reveal that, while LLMs demonstrate
strong generalization in information extraction, they struggle with
fine-grained concept alignment, particularly in disambiguating closely related
taxonomy entries. These findings highlight the limitations of existing LLMs in
fully automating XBRL tagging and underscore the need for improved semantic
reasoning and schema-aware modeling to meet the demands of accurate financial
disclosure. Code is available at our GitHub repository and data is at our
Hugging Face repository.

</details>


### [48] [ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation](https://arxiv.org/abs/2505.18374)
*Jarrod Ragsdale,Rajendra Boppana*

Key words: CLI, 语言模型, 语法约束, PPO, 行为建模

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文介绍了ShIOEnv，一种CLI环境模拟工具，通过语法约束和优化策略生成高质量交互数据集，显著提升小模型的行为建模能力。

Motivation: 现有CLI交互数据集缺乏执行细节（如退出码、输出等），限制了小模型在高风险环境中的行为建模能力。

Method: 将命令构建建模为马尔可夫决策过程，利用语法约束和PPO优化生成策略，提高数据集质量。

Result: 语法约束结合PPO使样本效率提升，CodeT5微调后BLEU-4分数提高85%（语法约束）和26%（PPO）。

Conclusion: ShIOEnv及生成的数据集为CLI行为建模提供了高效工具，推动了小模型的应用。

Abstract: Command-line interfaces (CLIs) provide structured textual environments for
system administration. Explorations have been performed using pre-trained
language models (PLMs) to simulate these environments for safe interaction in
high-risk environments. However, their use has been constrained to frozen,
large parameter models like GPT. For smaller architectures to reach a similar
level of believability, a rich dataset of CLI interactions is required.
Existing public datasets focus on mapping natural-language tasks to commands,
omitting crucial execution data such as exit codes, outputs, and environmental
side effects, limiting their usability for behavioral modeling. We introduce a
Shell Input -Output Environment (ShIOEnv), which casts command construction as
a Markov Decision Process whose state is the partially built sequence and whose
actions append arguments. After each action, ShIOEnv executes the candidate and
returns its exit status, output, and progress toward a minimal-length
behavioral objective. Due to the intractable nature of the combinatorial
argument state-action space, we derive a context-free grammar from man pages to
mask invalid arguments from being emitted. We explore random and
proximal-policy optimization (PPO)-optimized sampling of unrestricted and
grammar-masked action spaces to produce four exploration strategies. We
observed that grammar masking and PPO significantly improve sample efficiency
to produce a higher quality dataset (maximizing the number of arguments while
minimizing redundancies). Policy-generated datasets of shell input-output
behavior pairs are used to fine-tune CodeT5, where we observe 85% improvements
in BLEU-4 when constraining the action space to grammar productions with an
additional 26% improvement when applying PPO. The ShIOEnv environment and
curated command behavior datasets are released for use in future research.

</details>


### [49] [Chinese Cyberbullying Detection: Dataset, Method, and Validation](https://arxiv.org/abs/2505.20654)
*Yi Zhu,Xin Zou,Xindong Wu*

Key words: 网络欺凌检测、事件标注、数据集构建、中文、伪标签

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种基于事件的新型标注方法构建中文网络欺凌数据集CHNCI，包含91个事件的220,676条评论，并验证了其作为网络欺凌检测和事件预测基准的有效性。

Motivation: 现有网络欺凌检测基准多基于言论极性（如'冒犯性'和'非冒犯性'），而现实中的网络欺凌常通过事件引发广泛关注，因此需要更贴近实际的数据集。

Method: 结合三种基于解释生成的网络欺凌检测方法生成伪标签，人工标注后提出评估标准判断是否构成网络欺凌事件。

Result: 构建的CHNCI数据集成为网络欺凌检测和事件预测的有效基准，实验证明了其可行性。

Conclusion: 该研究首次针对中文网络欺凌事件检测任务提出解决方案，填补了相关领域空白。

Abstract: Existing cyberbullying detection benchmarks were organized by the polarity of
speech, such as "offensive" and "non-offensive", which were essentially hate
speech detection. However, in the real world, cyberbullying often attracted
widespread social attention through incidents. To address this problem, we
propose a novel annotation method to construct a cyberbullying dataset that
organized by incidents. The constructed CHNCI is the first Chinese
cyberbullying incident detection dataset, which consists of 220,676 comments in
91 incidents. Specifically, we first combine three cyberbullying detection
methods based on explanations generation as an ensemble method to generate the
pseudo labels, and then let human annotators judge these labels. Then we
propose the evaluation criteria for validating whether it constitutes a
cyberbullying incident. Experimental results demonstrate that the constructed
dataset can be a benchmark for the tasks of cyberbullying detection and
incident prediction. To the best of our knowledge, this is the first study for
the Chinese cyberbullying incident detection task.

</details>


### [50] [Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge](https://arxiv.org/abs/2505.20658)
*Yue Fang,Zhi Jin,Jie An,Hongshen Chen,Xiaohong Chen,Naijun Zhan*

Key words: 信号时序逻辑（STL）、自然语言处理（NLP）、数据集、知识引导、转换框架

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一个名为STL-DivEn的NL-STL数据集，包含16,000个样本，并通过知识引导的STL转换框架（KGST）实现从自然语言到STL的自动转换，优于现有方法。

Motivation: 手动将自然语言转换为信号时序逻辑（STL）耗时且易错，自动转换因缺乏数据集而受限。本文旨在解决这一问题。

Method: 提出STL-DivEn数据集，通过聚类和LLM扩展样本，并开发KGST框架，结合外部知识生成并优化STL。

Result: STL-DivEn数据集的多样性优于现有数据集，KGST框架在转换准确率上表现更佳。

Conclusion: STL-DivEn和KGST框架为NL-STL自动转换提供了有效解决方案。

Abstract: Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise
formal specification, making it widely used in cyber-physical systems such as
autonomous driving and robotics. Automatically transforming NL into STL is an
attractive approach to overcome the limitations of manual transformation, which
is time-consuming and error-prone. However, due to the lack of datasets,
automatic transformation currently faces significant challenges and has not
been fully explored. In this paper, we propose an NL-STL dataset named
STL-Diversity-Enhanced (STL-DivEn), which comprises 16,000 samples enriched
with diverse patterns. To develop the dataset, we first manually create a
small-scale seed set of NL-STL pairs. Next, representative examples are
identified through clustering and used to guide large language models (LLMs) in
generating additional NL-STL pairs. Finally, diversity and accuracy are ensured
through rigorous rule-based filters and human validation. Furthermore, we
introduce the Knowledge-Guided STL Transformation (KGST) framework, a novel
approach for transforming natural language into STL, involving a
generate-then-refine process based on external knowledge. Statistical analysis
shows that the STL-DivEn dataset exhibits more diversity than the existing
NL-STL dataset. Moreover, both metric-based and human evaluations indicate that
our KGST approach outperforms baseline models in transformation accuracy on
STL-DivEn and DeepSTL datasets.

</details>


### [51] [BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism](https://arxiv.org/abs/2505.20660)
*Qinzhuo Wu,Pengzhi Gao,Wei Liu,Jian Luan*

Key words: GUI代理, 回溯机制, 错误恢复, 任务完成效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了BacktrackAgent框架，通过回溯机制提升GUI代理的任务完成效率，包含验证器、判断器和反射器模块，实验显示在Mobile3M和Auto-UI基准上表现优异。

Motivation: 现有GUI代理在单个动作准确性上表现良好，但缺乏有效的错误检测和恢复机制，限制了任务完成效率。

Method: 提出BacktrackAgent框架，整合验证器、判断器和反射器模块，并引入判断奖励机制；开发专用训练数据集以支持回溯机制。

Result: 在Mobile3M和Auto-UI基准测试中，任务成功率和步骤准确性均显著提升。

Conclusion: BacktrackAgent通过回溯机制有效提升了GUI代理的鲁棒性和任务完成效率，未来将公开数据和代码。

Abstract: Graphical User Interface (GUI) agents have gained substantial attention due
to their impressive capabilities to complete tasks through multiple
interactions within GUI environments. However, existing agents primarily focus
on enhancing the accuracy of individual actions and often lack effective
mechanisms for detecting and recovering from errors. To address these
shortcomings, we propose the BacktrackAgent, a robust framework that
incorporates a backtracking mechanism to improve task completion efficiency.
BacktrackAgent includes verifier, judger, and reflector components as modules
for error detection and recovery, while also applying judgment rewards to
further enhance the agent's performance. Additionally, we develop a training
dataset specifically designed for the backtracking mechanism, which considers
the outcome pages after action executions. Experimental results show that
BacktrackAgent has achieved performance improvements in both task success rate
and step accuracy on Mobile3M and Auto-UI benchmarks. Our data and code will be
released upon acceptance.

</details>


### [52] [Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning](https://arxiv.org/abs/2505.20664)
*Yang He,Xiao Ding,Bibo Cai,Yufei Zhang,Kai Xiong,Zhouhao Sun,Bing Qin,Ting Liu*

Key words: 动态推理框架, token消耗, 能力估计, Self-Route, Gradient-10K

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Self-Route是一个动态推理框架，通过轻量级预推理阶段估计模型能力，自动选择推理模式，显著减少30-55%的token消耗，同时保持高准确率。

Motivation: 解决大型语言模型在简单问题上不必要的长推理链导致的资源浪费问题。

Method: 引入轻量级预推理阶段提取能力感知嵌入，构建Gradient-10K数据集训练路由器进行能力边界检测。

Result: 在多种基准测试中，token消耗减少30-55%，准确率与推理模型相当。

Conclusion: Self-Route框架在不同参数规模和推理范式的模型上均表现一致，具有广泛应用价值。

Abstract: While reasoning-augmented large language models (RLLMs) significantly enhance
complex task performance through extended reasoning chains, they inevitably
introduce substantial unnecessary token consumption, particularly for simpler
problems where Short Chain-of-Thought (Short CoT) suffices. This overthinking
phenomenon leads to inefficient resource usage without proportional accuracy
gains. To address this issue, we propose Self-Route, a dynamic reasoning
framework that automatically selects between general and reasoning modes based
on model capability estimation. Our approach introduces a lightweight
pre-inference stage to extract capability-aware embeddings from hidden layer
representations, enabling real-time evaluation of the model's ability to solve
problems. We further construct Gradient-10K, a model difficulty
estimation-based dataset with dense complexity sampling, to train the router
for precise capability boundary detection. Extensive experiments demonstrate
that Self-Route achieves comparable accuracy to reasoning models while reducing
token consumption by 30-55\% across diverse benchmarks. The proposed framework
demonstrates consistent effectiveness across models with different parameter
scales and reasoning paradigms, highlighting its general applicability and
practical value.

</details>


### [53] [Pretraining Language Models to Ponder in Continuous Space](https://arxiv.org/abs/2505.20674)
*Boyi Zeng,Shixiang Song,Siyuan Huang,Yixuan Wang,He Li,Ziwei He,Xinbing Wang,Zhiyu Li,Zhouhan Lin*

Key words: 语言模型,深思过程,自监督学习,标记嵌入,下游任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种在语言模型中引入‘深思’过程的方法，通过在单个标记生成步骤中多次调用前向过程，使模型能够通过加权求和所有标记嵌入来‘深思’，从而提升性能。该方法无需人工标注，且兼容多种现有模型，实验证明其有效性。

Motivation: 受人类在表达复杂句子前进行深入思考的启发，研究旨在通过模拟这一过程提升语言模型的认知处理能力。

Method: 在标记生成步骤中多次调用前向过程，模型通过加权求和所有标记嵌入进行‘深思’，并将生成的嵌入反馈为输入进行另一轮前向传递。

Result: 实验表明，深思模型在语言建模任务中表现与参数翻倍的普通模型相当，并在多个下游任务中显著优于基线模型，如深思增强的Pythia-1B性能接近训练数据量10倍的TinyLlama-1.1B。

Conclusion: 通过自监督学习模拟人类深思过程，能有效提升语言模型性能，且方法简单通用。

Abstract: Humans ponder before articulating complex sentence elements, enabling deeper
cognitive processing through focused effort. In this work, we introduce this
pondering process into language models by repeatedly invoking the forward
process within a single token generation step. During pondering, instead of
generating an actual token sampled from the prediction distribution, the model
ponders by yielding a weighted sum of all token embeddings according to the
predicted token distribution. The generated embedding is then fed back as input
for another forward pass. We show that the model can learn to ponder in this
way through self-supervised learning, without any human annotations. Our method
is straightforward and can be seamlessly integrated with various existing
language models. Experiments across three widely used open-source
architectures-GPT-2, Pythia, and LLaMA-and extensive downstream task
evaluations demonstrate the effectiveness and generality of our method. For
language modeling tasks, pondering language models achieve performance
comparable to vanilla models with twice the number of parameters. On 9
downstream benchmarks, our pondering-enhanced Pythia models significantly
outperform the official Pythia models. Notably, pondering-enhanced Pythia-1B is
comparable to TinyLlama-1.1B, which is trained on 10 times more data. The code
is available at https://github.com/LUMIA-Group/PonderingLM.

</details>


### [54] [SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations](https://arxiv.org/abs/2505.20679)
*Danush Khanna,Pratinav Seth,Sidhaarth Sredharan Murali,Aditya Kumar Guru,Siddharth Shukla,Tanuj Tyagi,Sandeep Chaurasia,Kripabandhu Ghosh*

Key words: 心理操纵, 多轮对话, 大型语言模型, 自我知觉理论, SELF-PERCEPT

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了MultiManip数据集，用于检测多轮多人对话中的心理操纵，并开发了SELF-PERCEPT框架，显著提升了大型语言模型在此任务上的表现。

Motivation: 心理操纵在人际交流中普遍存在且难以察觉，但现有大型语言模型在复杂对话中检测操纵语言的能力不足，因此需要构建专门的数据集和方法。

Method: 利用来自真人秀的多轮多人对话构建MultiManip数据集，提出基于自我知觉理论的两阶段提示框架SELF-PERCEPT，并通过多种提示策略评估模型性能。

Result: 实验表明，GPT-4o和Llama-3.1-8B等模型在检测操纵时表现不佳，而SELF-PERCEPT框架显著提升了检测效果。

Conclusion: SELF-PERCEPT框架为复杂对话中的心理操纵检测提供了有效解决方案，代码和数据集已开源。

Abstract: Mental manipulation is a subtle yet pervasive form of abuse in interpersonal
communication, making its detection critical for safeguarding potential
victims. However, due to manipulation's nuanced and context-specific nature,
identifying manipulative language in complex, multi-turn, and multi-person
conversations remains a significant challenge for large language models (LLMs).
To address this gap, we introduce the MultiManip dataset, comprising 220
multi-turn, multi-person dialogues balanced between manipulative and
non-manipulative interactions, all drawn from reality shows that mimic
real-world scenarios. For manipulative interactions, it includes 11 distinct
manipulations depicting real-life scenarios. We conduct extensive evaluations
of state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various
prompting strategies. Despite their capabilities, these models often struggle
to detect manipulation effectively. To overcome this limitation, we propose
SELF-PERCEPT, a novel, two-stage prompting framework inspired by
Self-Perception Theory, demonstrating strong performance in detecting
multi-person, multi-turn mental manipulation. Our code and data are publicly
available at https://github.com/danushkhanna/self-percept .

</details>


### [55] [Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages](https://arxiv.org/abs/2505.20693)
*Praveen Srinivasa Varadhan,Srija Anand,Soma Siddhartha,Mitesh M. Khapra*

Key words: 文本到语音（TTS）、低资源语言、多语言模型、微调、印度语言

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了将英语童话讲述者（F5-TTS）模型微调到11种印度语言上的效果，评估了多语言流畅性、声音克隆、风格克隆和代码混合。比较了从零开始训练、仅用印度数据微调英文F5、以及同时用印度和英语数据微调以防止遗忘三种方法。结果表明仅用印度数据微调最有效，生成的IN-F5模型近乎人类的多种语言能力。

Motivation: 研究如何通过微调英语预训练模型来提升低资源印度语言文本到语音（TTS）的表现，并探索计算最优策略和数据受限条件下的解决方案。

Method: 比较了从零开始训练、仅用印度数据微调英文F5、以及同时用印度和英语数据微调以防止遗忘三种方法，并进一步研究数据约束设置和计算最优策略。

Result: 仅使用印度数据微调英文F5模型表现最佳，生成的IN-F5模型具备近乎人类的多语言能力，能帮助使用者流畅地说另一种语言，且该模型可合成未见过的语言如Bhojpuri和Tulu。

Conclusion: 英文预训练有助于低资源TTS达到人类水平，IN-F5在多种印度语言上表现卓越，还能通过合成数据生成方法实现零资源TTS。

Abstract: What happens when an English Fairytaler is fine-tuned on Indian languages? We
evaluate how the English F5-TTS model adapts to 11 Indian languages, measuring
polyglot fluency, voice-cloning, style-cloning, and code-mixing. We compare:
(i) training from scratch, (ii) fine-tuning English F5 on Indian data, and
(iii) fine-tuning on both Indian and English data to prevent forgetting.
Fine-tuning with only Indian data proves most effective and the resultant IN-F5
is a near-human polyglot; that enables speakers of one language (e.g., Odia) to
fluently speak in another (e.g., Hindi). Our results show English pretraining
aids low-resource TTS in reaching human parity. To aid progress in other
low-resource languages, we study data-constrained setups and arrive at a
compute optimal strategy. Finally, we show IN-F5 can synthesize unseen
languages like Bhojpuri and Tulu using a human-in-the-loop approach for
zero-resource TTS via synthetic data generation.

</details>


### [56] [Beyond Templates: Dynamic Adaptation of Reasoning Demonstrations via Feasibility-Aware Exploration](https://arxiv.org/abs/2505.20700)
*Yong Wu,Weihang Pan,Ke Li,Chen Binhui,Ping Li,Binbin Lin*

Key words: DART, 大语言模型, 小语言模型, 推理对齐, 动态适应

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DART框架通过动态适应推理轨迹，解决了大语言模型（LLMs）与小语言模型（SLMs）间的能力差距问题，显著提升了SLMs的推理性能和泛化能力。

Motivation: 由于分布不匹配和模型容量限制，直接将LLMs的推理能力迁移到SLMs存在挑战。现有数据集通常为强大LLMs设计，直接应用于较弱模型时性能下降。

Method: 提出DART框架，通过选择性模仿策略和适应性估计，动态调整推理轨迹。当专家步骤超出学生模型能力时，自主探索替代路径。

Result: 在多个推理基准和模型规模上验证，DART显著提升泛化能力和数据效率，优于静态微调。

Conclusion: DART通过与学生推理能力匹配的训练信号提升监督质量，为资源受限模型提供可扩展的推理对齐方案。

Abstract: Large language models (LLMs) have shown remarkable reasoning capabilities,
yet aligning such abilities to small language models (SLMs) remains a challenge
due to distributional mismatches and limited model capacity. Existing reasoning
datasets, typically designed for powerful LLMs, often lead to degraded
performance when directly applied to weaker models. In this work, we introduce
Dynamic Adaptation of Reasoning Trajectories (DART), a novel data adaptation
framework that bridges the capability gap between expert reasoning trajectories
and diverse SLMs. Instead of uniformly imitating expert steps, DART employs a
selective imitation strategy guided by step-wise adaptability estimation via
solution simulation. When expert steps surpass the student's capacity --
signaled by an Imitation Gap -- the student autonomously explores alternative
reasoning paths, constrained by outcome consistency. We validate DART across
multiple reasoning benchmarks and model scales, demonstrating that it
significantly improves generalization and data efficiency over static
fine-tuning. Our method enhances supervision quality by aligning training
signals with the student's reasoning capabilities, offering a scalable solution
for reasoning alignment in resource-constrained models.

</details>


### [57] [Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective](https://arxiv.org/abs/2505.20707)
*Nicy Scaria,Silvester John Joseph Kennedy,Diksha Seth,Deepak Subramani*

Key words: Small Language Models, physics reasoning, educational applications, Bloom's Taxonomy, cultural contextualization

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究探讨了小型语言模型（SLMs）在高中物理推理任务中的表现，发现尽管部分模型在答案准确率上表现良好（如Qwen 3 1.7B达85%），但完全正确的推理链比例较低（仅38%），且推理质量随问题复杂性下降。文化适应性问题对模型表现影响较小，但模型普遍依赖模式识别而非真正的理解。

Motivation: 研究动机在于探索SLMs在物理学教育中的潜力，特别是其复杂推理能力。由于SLMs在计算效率和应用上的优势，其在教育领域的应用值得关注，但其推理能力的局限性尚未充分研究。

Method: 研究方法包括构建一个基于Bloom分类法的高中物理数据集，包含原始和经过文化适应的问题。使用多种SLMs（如Llama 3.2、Phi 4 Mini等）进行评估，采用LLM-as-a-judge框架（Gemini 2.5 Flash）分析答案和推理链的正确性及计算准确性。

Result: 结果显示，SLMs在答案准确性上表现不一（如Qwen 3 1.7B达85%），但完全正确的推理链比例普遍较低（最低38%）。数学表示格式对性能影响微乎其微，而推理质量随认知和知识复杂性下降。文化适应性问题对性能影响较小。

Conclusion: 尽管SLMs在答案准确性上有潜力，但其推理能力存在明显缺陷，依赖模式识别而非真正的理解。未来开发需优先提升真实理解和可验证的推理链生成能力。

Abstract: Small Language Models (SLMs) offer computational efficiency and
accessibility, making them promising for educational applications. However,
their capacity for complex reasoning, particularly in domains such as physics,
remains underexplored. This study investigates the high school physics
reasoning capabilities of state-of-the-art SLMs (under 4 billion parameters),
including instruct versions of Llama 3.2, Phi 4 Mini, Gemma 3, and Qwen series.
We developed a comprehensive physics dataset from the OpenStax High School
Physics textbook, annotated according to Bloom's Taxonomy, with LaTeX and
plaintext mathematical notations. A novel cultural contextualization approach
was applied to a subset, creating culturally adapted problems for Asian,
African, and South American/Australian contexts while preserving core physics
principles. Using an LLM-as-a-judge framework with Google's Gemini 2.5 Flash,
we evaluated answer and reasoning chain correctness, along with calculation
accuracy. The results reveal significant differences between the SLMs. Qwen 3
1.7B achieved high `answer accuracy' (85%), but `fully correct reasoning' was
substantially low (38%). The format of the mathematical notation had a
negligible impact on performance. SLMs exhibited varied performance across the
physics topics and showed a decline in reasoning quality with increasing
cognitive and knowledge complexity. In particular, the consistency of reasoning
was largely maintained in diverse cultural contexts, especially by better
performing models. These findings indicate that, while SLMs can often find
correct answers, their underlying reasoning is frequently flawed, suggesting an
overreliance on pattern recognition. For SLMs to become reliable educational
tools in physics, future development must prioritize enhancing genuine
understanding and the generation of sound, verifiable reasoning chains over
mere answer accuracy.

</details>


### [58] [SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution](https://arxiv.org/abs/2505.20732)
*Hanlin Wang,Chak Tou Leong,Jiashuo Wang,Jian Wang,Wenjie Li*

Key words: 强化学习, 延迟奖励, 逐步贡献, SPA, 训练效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了SPA方法，通过分解最终奖励为逐步贡献，提升强化学习在延迟奖励任务中的表现。

Motivation: 解决强化学习在复杂任务中因延迟奖励导致训练效率低下的问题。

Method: 设计了Stepwise Progress Attribution (SPA)框架，训练进度估计器分解最终奖励为逐步贡献。

Result: 在多个基准测试中平均成功率提升2.5%，定位准确率提升1.9%。

Conclusion: SPA能更有效地提供中间奖励，显著提升强化学习训练效果。

Abstract: Reinforcement learning (RL) holds significant promise for training LLM agents
to handle complex, goal-oriented tasks that require multi-step interactions
with external environments. However, a critical challenge when applying RL to
these agentic tasks arises from delayed rewards: feedback signals are typically
available only after the entire task is completed. This makes it non-trivial to
assign delayed rewards to earlier actions, providing insufficient guidance
regarding environmental constraints and hindering agent training. In this work,
we draw on the insight that the ultimate completion of a task emerges from the
cumulative progress an agent makes across individual steps. We propose Stepwise
Progress Attribution (SPA), a general reward redistribution framework that
decomposes the final reward into stepwise contributions, each reflecting its
incremental progress toward overall task completion. To achieve this, we train
a progress estimator that accumulates stepwise contributions over a trajectory
to match the task completion. During policy optimization, we combine the
estimated per-step contribution with a grounding signal for actions executed in
the environment as the fine-grained, intermediate reward for effective agent
training. Extensive experiments on common agent benchmarks (including Webshop,
ALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the
state-of-the-art method in both success rate (+2.5\% on average) and grounding
accuracy (+1.9\% on average). Further analyses demonstrate that our method
remarkably provides more effective intermediate rewards for RL training. Our
code is available at https://github.com/WangHanLinHenry/SPA-RL-Agent.

</details>


### [59] [Silencer: From Discovery to Mitigation of Self-Bias in LLM-as-Benchmark-Generator](https://arxiv.org/abs/2505.20738)
*Peiwen Yuan,Yiwei Li,Shaoxiong Feng,Xinglin Wang,Yueqi Zhang,Jiayi Shi,Chuyi Tan,Boyuan Pan,Yao Hu,Kan Li*

Key words: 自偏现象, LLM评测, 基准生成, Silencer框架

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究揭示了LLM自生成评测基准时存在的自偏现象，并提出Silencer框架通过多生成器异质性中和偏误，显著提升评测有效性。

Motivation: 现有研究中，LLM作为评测基准生成器的潜在偏误未被充分探索，尤其自生成基准可能导致模型性能虚高（自偏现象）。

Method: 提出Silencer框架，利用多生成器在样本和基准层面的异质性中和偏误，生成高质量、无自偏的评测基准。

Result: Silencer将自偏降至近乎零，评测效果显著提升（Pearson相关系数从0.655升至0.833），且具备强泛化性。

Conclusion: Silencer有效解决了LLM自生成基准的偏误问题，为自动化评测提供了可靠方案。

Abstract: LLM-as-Benchmark-Generator methods have been widely studied as a supplement
to human annotators for scalable evaluation, while the potential biases within
this paradigm remain underexplored. In this work, we systematically define and
validate the phenomenon of inflated performance in models evaluated on their
self-generated benchmarks, referred to as self-bias, and attribute it to
sub-biases arising from question domain, language style, and wrong labels. On
this basis, we propose Silencer, a general framework that leverages the
heterogeneity between multiple generators at both the sample and benchmark
levels to neutralize bias and generate high-quality, self-bias-silenced
benchmark. Experimental results across various settings demonstrate that
Silencer can suppress self-bias to near zero, significantly improve evaluation
effectiveness of the generated benchmark (with an average improvement from
0.655 to 0.833 in Pearson correlation with high-quality human-annotated
benchmark), while also exhibiting strong generalizability.

</details>


### [60] [CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models](https://arxiv.org/abs/2505.20767)
*Xiaqiang Tang,Jian Li,Keyu Hu,Du Nan,Xiaolong Li,Xi Zhang,Weigao Sun,Sihong Xie*

Key words: 大语言模型，忠实性幻觉，认知语句，基准数据集，自动标注

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一个评估大语言模型（LLM）认知忠实性的框架CogniBench，填补了现有基准缺失认知语句标注的不足，并构建了自动标注流程和大规模数据集。

Motivation: 现有基准仅标注事实性语句而忽略认知性语句，导致对LLM生成的认知性陈述的忠实性评估和优化困难。

Method: 受立法领域证据评估启发，设计分级评估认知语句忠实性的框架，并开发自动标注流程生成大规模数据集CogniBench-L。

Result: 构建了包含认知语句标注的基准数据集，可训练高精度幻觉检测模型，数据与模型已开源。

Conclusion: CogniBench为LLM认知忠实性评估提供了标准化工具，推动相关优化研究。

Abstract: Faithfulness hallucination are claims generated by a Large Language Model
(LLM) not supported by contexts provided to the LLM. Lacking assessment
standard, existing benchmarks only contain "factual statements" that rephrase
source materials without marking "cognitive statements" that make inference
from the given context, making the consistency evaluation and optimization of
cognitive statements difficult. Inspired by how an evidence is assessed in the
legislative domain, we design a rigorous framework to assess different levels
of faithfulness of cognitive statements and create a benchmark dataset where we
reveal insightful statistics. We design an annotation pipeline to create larger
benchmarks for different LLMs automatically, and the resulting larger-scale
CogniBench-L dataset can be used to train accurate cognitive hallucination
detection model. We release our model and dataset at:
https://github.com/FUTUREEEEEE/CogniBench

</details>


### [61] [SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences](https://arxiv.org/abs/2505.20776)
*Jungyoub Cha,Hyunjong Kim,Sungzoon Cho*

Key words: 推测解码, 长序列处理, 注意力机制, KV缓存, 跨模型检索

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SpecExtend是一种无需额外训练即可提升长序列推测解码性能的技术，通过高效注意力机制和跨模型检索策略，显著加速长文本处理。

Motivation: 推测解码在长输入时性能下降，作者希望在不额外训练的情况下提升其长序列处理能力。

Method: 结合FlashAttention和Hybrid Tree Attention等高效注意力机制，并提出跨模型检索策略动态优化KV缓存。

Result: 在长达16K token的输入上性能提升2.22倍，显著加速长序列解码。

Conclusion: SpecExtend为长序列推测解码提供了高效解决方案，且代码开源。

Abstract: Speculative decoding is a widely adopted technique for accelerating inference
in large language models (LLMs), but its performance degrades on long inputs
due to increased attention cost and reduced draft accuracy. We introduce
SpecExtend, a drop-in enhancement that improves the performance of speculative
decoding on long sequences without any additional training. SpecExtend
integrates efficient attention mechanisms such as FlashAttention and Hybrid
Tree Attention into both the draft and target models, reducing latency across
all stages. To improve draft accuracy and speed, we propose Cross-model
Retrieval, a novel KV cache update strategy that uses the target model's
attention scores to dynamically select relevant context for the draft model.
Extensive evaluations on three long-context understanding datasets show that
SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x
for inputs up to 16K tokens, providing an effective solution for speculative
decoding of long sequences. The code is available at
https://github.com/jycha98/SpecExtend .

</details>


### [62] [CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature](https://arxiv.org/abs/2505.20779)
*Noy Sternlicht,Tom Hope*

Key words: 重组创新、科学文献挖掘、知识库、信息提取、机器学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了CHIMERA，一个通过分析科学文献自动构建的大规模重组知识库，用于探索科学家如何跨领域重组概念，并训练机器学习模型预测创新方向。

Motivation: 人类创新的核心在于重组现有机制和概念的能力。为了大规模量化这一过程，作者希望通过自动化挖掘科学文献，构建一个重组案例的知识库。

Method: 从科学论文摘要中提取重组案例，构建高质量人工标注语料库，并用LLM模型训练提取器。该模型应用于AI领域的论文，生成了包含28K重组案例的知识库。

Result: CHIMERA知识库成功捕捉了AI各子领域的重组特性，并用于训练科学假设生成模型，预测了研究者认为有启发性的新重组方向。

Conclusion: CHIMERA为研究科学创新提供了可扩展的实证工具，并为跨领域灵感挖掘提供了数据驱动的方法。

Abstract: A hallmark of human innovation is the process of recombination -- creating
original ideas by integrating elements of existing mechanisms and concepts. In
this work, we automatically mine the scientific literature and build CHIMERA: a
large-scale knowledge base (KB) of recombination examples. CHIMERA can be used
to empirically explore at scale how scientists recombine concepts and take
inspiration from different areas, or to train supervised machine learning
models that learn to predict new creative cross-domain directions. To build
this KB, we present a novel information extraction task of extracting
recombination from scientific paper abstracts, collect a high-quality corpus of
hundreds of manually annotated abstracts, and use it to train an LLM-based
extraction model. The model is applied to a large corpus of papers in the AI
domain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to
explore the properties of recombination in different subareas of AI. Finally,
we train a scientific hypothesis generation model using the KB, which predicts
new recombination directions that real-world researchers find inspiring. Our
data and code are available at https://github.cs.huji.ac.il/tomhope-lab/CHIMERA

</details>


### [63] [Improved Representation Steering for Language Models](https://arxiv.org/abs/2505.20809)
*Zhengxuan Wu,Qinan Yu,Aryaman Arora,Christopher D. Manning,Christopher Potts*

Key words: 语言模型,表示引导,RePS,双向偏好优化,Gemma

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: RePS是一种新颖的双向偏好优化方法，用于改进语言模型的表示引导，在概念引导和抑制方面优于现有方法，尤其在Gemma模型中效果显著。

Motivation: 提升语言模型在细粒度控制生成行为时的有效性和可解释性，超越现有权重或表示调整方法的局限性。

Method: 提出Reference-free Preference Steering（RePS）方法，通过双向偏好优化目标联合进行概念引导和抑制，并在Gemma模型上验证其效果。

Result: 在Gemma 2B至27B模型中，RePS显著优于现有基于语言模型目标的引导方法，尤其在抑制任务中表现稳健，抗提示攻击能力强。

Conclusion: RePS为语言模型的引导和抑制任务提供了更可解释、更稳健的替代方案，缩小了与提示方法的性能差距。

Abstract: Steering methods for language models (LMs) seek to provide fine-grained and
interpretable control over model generations by variously changing model
inputs, weights, or representations to adjust behavior. Recent work has shown
that adjusting weights or representations is often less effective than steering
by prompting, for instance when wanting to introduce or suppress a particular
concept. We demonstrate how to improve representation steering via our new
Reference-free Preference Steering (RePS), a bidirectional
preference-optimization objective that jointly does concept steering and
suppression. We train three parameterizations of RePS and evaluate them on
AxBench, a large-scale model steering benchmark. On Gemma models with sizes
ranging from 2B to 27B, RePS outperforms all existing steering methods trained
with a language modeling objective and substantially narrows the gap with
prompting -- while promoting interpretability and minimizing parameter count.
In suppression, RePS matches the language-modeling objective on Gemma-2 and
outperforms it on the larger Gemma-3 variants while remaining resilient to
prompt-based jailbreaking attacks that defeat prompting. Overall, our results
suggest that RePS provides an interpretable and robust alternative to prompting
for both steering and suppression.

</details>


### [64] [RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph](https://arxiv.org/abs/2505.20813)
*Junsik Kim,Jinwook Park,Kangil Kim*

Key words: 知识图谱嵌入, 实体变换, 语义一致性, RSCF, 知识图谱补全

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了Relation-Semantics Consistent Filter (RSCF)方法，解决了知识图谱嵌入中关系特定实体变换的不一致性问题，显著提升了性能。

Motivation: 现有知识图谱嵌入方法中，关系特定的实体变换存在不一致性，导致丢失嵌入中的有价值归纳偏置。RSCF旨在通过更一致的实体变换来解决这一问题。

Method: RSCF采用三种特征来保持一致的实体变换：1) 共享关系嵌入的仿射变换；2) 根植的实体变换；3) 变换向量的归一化。还增加了关系变换和预测模块以增强语义。

Result: 在基于距离和张量分解的知识图谱补全任务中，RSCF显著优于现有的KGE方法，表现出对所有关系及其频率的鲁棒性。

Conclusion: RSCF通过保持关系语义的一致性，有效提升了知识图谱嵌入的性能和鲁棒性。

Abstract: In knowledge graph embedding, leveraging relation-specific
entity-transformation has markedly enhanced performance. However, the
consistency of embedding differences before and after transformation remains
unaddressed, risking the loss of valuable inductive bias inherent in the
embeddings. This inconsistency stems from two problems. First, transformation
representations are specified for relations in a disconnected manner, allowing
dissimilar transformations and corresponding entity-embeddings for similar
relations. Second, a generalized plug-in approach as a SFBR (Semantic Filter
Based on Relations) disrupts this consistency through excessive concentration
of entity embeddings under entity-based regularization, generating
indistinguishable score distributions among relations. In this paper, we
introduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF),
containing more consistent entity-transformation characterized by three
features: 1) shared affine transformation of relation embeddings across all
relations, 2) rooted entity-transformation that adds an entity embedding to its
change represented by the transformed vector, and 3) normalization of the
change to prevent scale reduction. To amplify the advantages of consistency
that preserve semantics on embeddings, RSCF adds relation transformation and
prediction modules for enhancing the semantics. In knowledge graph completion
tasks with distance-based and tensor decomposition models, RSCF significantly
outperforms state-of-the-art KGE methods, showing robustness across all
relations and their frequencies.

</details>


### [65] [Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective](https://arxiv.org/abs/2505.20816)
*Krishna Singh Rajput,Tejas Anvekar,Chitta Baral,Vivek Gupta*

Key words: 多模态问答，多智能体框架，视觉语言模型，大语言模型，跨模态推理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了MAMMQA，一个多模态问答框架，通过多智能体合作（两个视觉语言模型智能体和一个基于文本的大语言模型智能体）分别处理输入的不同模态并协作生成答案，提升了问答的准确性和可解释性。

Motivation: 现有方法依赖单一推理策略，忽视了不同模态的独特性，限制了性能和可解释性。MAMMQA旨在利用多智能体框架解决这一问题。

Method: 提出MAMMQA框架，包含两个VLM智能体（分解问题并检索部分答案、跨模态推理融合结果）和一个LLM智能体（整合答案）。

Result: 实验表明，MAMMQA在多模态问答基准测试中准确性和鲁棒性均优于现有方法。

Conclusion: 多智能体协作框架能更有效地处理多模态问答任务，提升性能和可解释性。

Abstract: Recent advances in multimodal question answering have primarily focused on
combining heterogeneous modalities or fine-tuning multimodal large language
models. While these approaches have shown strong performance, they often rely
on a single, generalized reasoning strategy, overlooking the unique
characteristics of each modality ultimately limiting both accuracy and
interpretability. To address these limitations, we propose MAMMQA, a
multi-agent QA framework for multimodal inputs spanning text, tables, and
images. Our system includes two Visual Language Model (VLM) agents and one
text-based Large Language Model (LLM) agent. The first VLM decomposes the user
query into sub-questions and sequentially retrieves partial answers from each
modality. The second VLM synthesizes and refines these results through
cross-modal reasoning. Finally, the LLM integrates the insights into a cohesive
answer. This modular design enhances interpretability by making the reasoning
process transparent and allows each agent to operate within its domain of
expertise. Experiments on diverse multimodal QA benchmarks demonstrate that our
cooperative, multi-agent framework consistently outperforms existing baselines
in both accuracy and robustness.

</details>


### [66] [Tracing and Reversing Rank-One Model Edits](https://arxiv.org/abs/2505.20819)
*Paul Youssef,Zhixue Zhao,Christin Seifert,Jörg Schlötterer*

Key words: 知识编辑，ROME，对抗防御，权重检测，模型恢复

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究探讨了知识编辑（KEs）的双重使用风险，提出通过检测、追踪和反转编辑权重来防御恶意操纵，特别针对ROME方法展示了高效的检测和恢复能力。

Motivation: 知识编辑方法（KEs）虽是更新大型语言模型（LLMs）的高效手段，但存在被滥用于植入错误信息或偏见的风险，因此需要可靠的防御技术。

Method: 研究聚焦ROME方法，分析编辑后权重矩阵的分布模式，通过预测和推断编辑的对象实体，最终实现编辑的反转。

Result: 实验显示，可通过权重模式定位编辑（准确率≥95%），并恢复模型原始输出（准确率≥80%），验证了防御框架的可行性。

Conclusion: 研究表明，基于编辑权重的检测、追踪和反转技术能有效保护LLMs免受对抗性操纵。

Abstract: Knowledge editing methods (KEs) are a cost-effective way to update the
factual content of large language models (LLMs), but they pose a dual-use risk.
While KEs are beneficial for updating outdated or incorrect information, they
can be exploited maliciously to implant misinformation or bias. In order to
defend against these types of malicious manipulation, we need robust techniques
that can reliably detect, interpret, and mitigate adversarial edits. This work
investigates the traceability and reversibility of knowledge edits, focusing on
the widely used Rank-One Model Editing (ROME) method. We first show that ROME
introduces distinctive distributional patterns in the edited weight matrices,
which can serve as effective signals for locating the edited weights. Second,
we show that these altered weights can reliably be used to predict the edited
factual relation, enabling partial reconstruction of the modified fact.
Building on this, we propose a method to infer the edited object entity
directly from the modified weights, without access to the editing prompt,
achieving over 95% accuracy. Finally, we demonstrate that ROME edits can be
reversed, recovering the model's original outputs with $\geq$ 80% accuracy. Our
findings highlight the feasibility of detecting, tracing, and reversing edits
based on the edited weights, offering a robust framework for safeguarding LLMs
against adversarial manipulations.

</details>


### [67] [Reinforced Informativeness Optimization for Long-Form Retrieval-Augmented Generation](https://arxiv.org/abs/2505.20825)
*Yuhao Wang,Ruiyang Ren,Yucheng Wang,Wayne Xin Zhao,Jing Liu,Hua Wu,Haifeng Wang*

Key words: 长问答生成,检索增强生成,强化学习,信息优化,奖励建模

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出RioRAG，一个新的强化学习框架，通过强化信息优化提升长问答检索增强生成（RAG）的表现，解决了数据稀缺和信息准确性问题。

Motivation: 长问答生成面临高质量训练数据不足、生成内容容易虚构及缺乏可靠评估指标的挑战，促使作者研究改进方法。

Method: 提出强化信息优化的RL训练范式及基于信息块的分层奖励建模，直接优化信息质量并精确评估答案事实一致性。

Result: 在LongFact和RAGChecker基准测试中效果显著，验证了方法的有效性。

Conclusion: RioRAG通过创新RL框架及奖励机制，显著提升长问答RAG系统的信息准确性和可靠性。

Abstract: Long-form question answering (LFQA) presents unique challenges for large
language models, requiring the synthesis of coherent, paragraph-length answers.
While retrieval-augmented generation (RAG) systems have emerged as a promising
solution, existing research struggles with key limitations: the scarcity of
high-quality training data for long-form generation, the compounding risk of
hallucination in extended outputs, and the absence of reliable evaluation
metrics for factual completeness. In this paper, we propose RioRAG, a novel
reinforcement learning (RL) framework that advances long-form RAG through
reinforced informativeness optimization. Our approach introduces two
fundamental innovations to address the core challenges. First, we develop an RL
training paradigm of reinforced informativeness optimization that directly
optimizes informativeness and effectively addresses the slow-thinking deficit
in conventional RAG systems, bypassing the need for expensive supervised data.
Second, we propose a nugget-centric hierarchical reward modeling approach that
enables precise assessment of long-form answers through a three-stage process:
extracting the nugget from every source webpage, constructing a nugget claim
checklist, and computing rewards based on factual alignment. Extensive
experiments on two LFQA benchmarks LongFact and RAGChecker demonstrate the
effectiveness of the proposed method. Our codes are available at
https://github.com/RUCAIBox/RioRAG.

</details>


### [68] [AdParaphrase v2.0: Generating Attractive Ad Texts Using a Preference-Annotated Paraphrase Dataset](https://arxiv.org/abs/2505.20826)
*Soichiro Murakami,Peinan Zhang,Hidetaka Kamigaito,Hiroya Takamura,Manabu Okumura*

Key words: 广告文本改写, 人类偏好, 语言特征, 大语言模型, 广告效果

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: AdParaphrase v2.0是一个用于广告文本改写的数据集，规模比v1.0大20倍，包含16,460对改写文本和人类偏好数据，用于分析广告文本吸引力的语言特征并生成更吸引人的广告。

Motivation: 研究广告文本吸引力的关键因素，以提升广告效果，并为生成有吸引力的广告文本提供数据支持。

Method: 通过扩展数据集（AdParaphrase v2.0）并标注人类偏好数据，分析语言特征，探索广告文本生成方法。

Result: 发现v1.0中未观察到的语言特征，验证了人类偏好与广告表现的关系，展示了基于大语言模型的参考无关指标潜力。

Conclusion: AdParaphrase v2.0为广告文本分析和生成提供了更可靠的数据支持，并展示了新评估方法的可行性。

Abstract: Identifying factors that make ad text attractive is essential for advertising
success. This study proposes AdParaphrase v2.0, a dataset for ad text
paraphrasing, containing human preference data, to enable the analysis of the
linguistic factors and to support the development of methods for generating
attractive ad texts. Compared with v1.0, this dataset is 20 times larger,
comprising 16,460 ad text paraphrase pairs, each annotated with preference data
from ten evaluators, thereby enabling a more comprehensive and reliable
analysis. Through the experiments, we identified multiple linguistic features
of engaging ad texts that were not observed in v1.0 and explored various
methods for generating attractive ad texts. Furthermore, our analysis
demonstrated the relationships between human preference and ad performance, and
highlighted the potential of reference-free metrics based on large language
models for evaluating ad text attractiveness. The dataset is publicly available
at: https://github.com/CyberAgentAILab/AdParaphrase-v2.0.

</details>


### [69] [Concealment of Intent: A Game-Theoretic Analysis](https://arxiv.org/abs/2505.20841)
*Xinbo Wu,Abhishek Umrawal,Lav R. Varshney*

Key words: 大语言模型、对抗提示、意图隐藏攻击、博弈论、防御机制

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种可扩展的意图隐藏对抗提示攻击策略，通过技能组合隐藏恶意意图，并开发了博弈论框架分析攻击与防御系统的交互。实证验证了该攻击在多种现实LLM上的有效性，并提出针对性防御机制。

Motivation: 随着大语言模型（LLM）能力的增强，其安全部署问题日益突出。尽管已有对齐机制防止滥用，但仍易受精心设计的对抗提示攻击。本研究旨在解决意图隐藏类攻击的威胁。

Method: 提出意图隐藏对抗提示攻击策略，并构建博弈论框架建模攻击与防御系统的交互，分析均衡点和攻击者优势。提出针对性防御机制。

Result: 实证表明该攻击在多种LLM上对多种恶意行为有效，且优于现有对抗提示技术。防御机制的分析为应对此类攻击提供了方向。

Conclusion: 意图隐藏对抗提示攻击具有显著威胁，需针对性防御。博弈论框架和防御机制为未来研究提供了基础。

Abstract: As large language models (LLMs) grow more capable, concerns about their safe
deployment have also grown. Although alignment mechanisms have been introduced
to deter misuse, they remain vulnerable to carefully designed adversarial
prompts. In this work, we present a scalable attack strategy: intent-hiding
adversarial prompting, which conceals malicious intent through the composition
of skills. We develop a game-theoretic framework to model the interaction
between such attacks and defense systems that apply both prompt and response
filtering. Our analysis identifies equilibrium points and reveals structural
advantages for the attacker. To counter these threats, we propose and analyze a
defense mechanism tailored to intent-hiding attacks. Empirically, we validate
the attack's effectiveness on multiple real-world LLMs across a range of
malicious behaviors, demonstrating clear advantages over existing adversarial
prompting techniques.

</details>


### [70] [Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of RAG](https://arxiv.org/abs/2505.20871)
*Xin Sun,Jianan Xie,Zhongqi Chen,Qiang Liu,Shu Wu,Yuehe Chen,Bowen Song,Weiqiang Wang,Zilei Wang,Liang Wang*

Key words: 检索增强系统,DTA,RAFT,可靠性,直接偏好优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种名为Divide-Then-Align（DTA）的后训练方法，旨在让RAG系统在面对超出其知识边界的问题时能够回答"我不知道"。该方法通过划分知识象限并构建针对性偏好数据，显著提高了系统的可靠性和可信度。

Motivation: 现有的检索增强微调方法（RAFT）虽然提升了模型性能，但在高风险领域中，模型可能因缺乏可靠知识而生成错误答案，降低了系统的可靠性。DTA旨在解决这一问题，让模型能够更诚实、可信地回答问题。

Method: DTA在训练后阶段将数据样本划分为四个知识象限，并为每个象限构建独特的偏好数据集，从而通过直接偏好优化（DPO）方法微调模型。

Result: 实验结果表明，DTA在三个基准数据集上有效平衡了准确性和适当的弃权，提高了检索增强系统的可靠性。

Conclusion: DTA方法通过合理划分知识边界和优化偏好数据，成功提升了RAG系统在高风险领域中的可信度和实用性。

Abstract: Large language models (LLMs) augmented with retrieval systems have
significantly advanced natural language processing tasks by integrating
external knowledge sources, enabling more accurate and contextually rich
responses. To improve the robustness of such systems against noisy retrievals,
Retrieval-Augmented Fine-Tuning (RAFT) has emerged as a widely adopted method.
However, RAFT conditions models to generate answers even in the absence of
reliable knowledge. This behavior undermines their reliability in high-stakes
domains, where acknowledging uncertainty is critical. To address this issue, we
propose Divide-Then-Align (DTA), a post-training approach designed to endow RAG
systems with the ability to respond with "I don't know" when the query is out
of the knowledge boundary of both the retrieved passages and the model's
internal knowledge. DTA divides data samples into four knowledge quadrants and
constructs tailored preference data for each quadrant, resulting in a curated
dataset for Direct Preference Optimization (DPO). Experimental results on three
benchmark datasets demonstrate that DTA effectively balances accuracy with
appropriate abstention, enhancing the reliability and trustworthiness of
retrieval-augmented systems.

</details>


### [71] [Can LLMs Learn to Map the World from Local Descriptions?](https://arxiv.org/abs/2505.20874)
*Sirui Xia,Aili Chen,Xintao Wang,Tinghui Zhu,Yikai Zhang,Jiangjie Chen,Yanghua Xiao*

Key words: LLMs, 空间认知, 空间感知, 路径规划, 全局布局

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LLMs在空间认知方面的潜力尚未充分探索，本研究关注其如何通过整合局部关系描述构建全局空间认知，包括空间感知和导航能力，并在模拟环境中验证了其有效性。

Motivation: 探索LLMs是否能够通过局部观察整合，构建全局空间认知，填补其在结构化空间知识内部化研究中的空白。

Method: 通过模拟城市环境实验，测试LLMs在空间感知（从局部关系推断全局布局）和导航（从轨迹数据学习路径规划）两方面的能力。

Result: LLMs不仅能泛化到未见过的空间关系，其潜在表征也与真实空间分布一致，且能从轨迹描述学习路径规划，实现动态空间意识。

Conclusion: LLMs在空间认知方面展现潜力，尤其在整合局部关系和路径规划上表现突出，为AI空间推理提供了新思路。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated strong
capabilities in tasks such as code and mathematics. However, their potential to
internalize structured spatial knowledge remains underexplored. This study
investigates whether LLMs, grounded in locally relative human observations, can
construct coherent global spatial cognition by integrating fragmented
relational descriptions. We focus on two core aspects of spatial cognition:
spatial perception, where models infer consistent global layouts from local
positional relationships, and spatial navigation, where models learn road
connectivity from trajectory data and plan optimal paths between unconnected
locations. Experiments conducted in a simulated urban environment demonstrate
that LLMs not only generalize to unseen spatial relationships between points of
interest (POIs) but also exhibit latent representations aligned with real-world
spatial distributions. Furthermore, LLMs can learn road connectivity from
trajectory descriptions, enabling accurate path planning and dynamic spatial
awareness during navigation.

</details>


### [72] [Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties](https://arxiv.org/abs/2505.20875)
*Jiyoung Lee,Seungho Kim,Jieun Han,Jun-Min Lee,Kitaek Kim,Alice Oh,Edward Choi*

Key words: Large Language Models, linguistic robustness, English varieties, Trans-EnV

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了Trans-EnV框架，通过自动将标准英语数据集转换为多种英语变体，评估大型语言模型的语言鲁棒性，结果发现模型在非标准变体上性能显著下降。

Motivation: 现有大型语言模型主要针对标准美国英语评估，忽视了全球英语多样性，可能导致公平性问题。

Method: 结合语言学专家知识和基于LLM的转换方法，构建Trans-EnV框架，将6个基准数据集转换为38种英语变体，评估7种LLM。

Result: 非标准变体上的准确率下降最高达46.3%，显示语言模型性能存在显著差异。

Conclusion: 强调全面评估语言鲁棒性的重要性，Trans-EnV框架及其资源已开源。

Abstract: Large Language Models (LLMs) are predominantly evaluated on Standard American
English (SAE), often overlooking the diversity of global English varieties.
This narrow focus may raise fairness concerns as degraded performance on
non-standard varieties can lead to unequal benefits for users worldwide.
Therefore, it is critical to extensively evaluate the linguistic robustness of
LLMs on multiple non-standard English varieties. We introduce Trans-EnV, a
framework that automatically transforms SAE datasets into multiple English
varieties to evaluate the linguistic robustness. Our framework combines (1)
linguistics expert knowledge to curate variety-specific features and
transformation guidelines from linguistic literature and corpora, and (2)
LLM-based transformations to ensure both linguistic validity and scalability.
Using Trans-EnV, we transform six benchmark datasets into 38 English varieties
and evaluate seven state-of-the-art LLMs. Our results reveal significant
performance disparities, with accuracy decreasing by up to 46.3% on
non-standard varieties. These findings highlight the importance of
comprehensive linguistic robustness evaluation across diverse English
varieties. Each construction of Trans-EnV was validated through rigorous
statistical testing and consultation with a researcher in the field of second
language acquisition, ensuring its linguistic validity. Our
\href{https://github.com/jiyounglee-0523/TransEnV}{code} and
\href{https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1}{datasets}
are publicly available.

</details>


### [73] [MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification for Multilingual Hallucination Detection](https://arxiv.org/abs/2505.20880)
*Baraa Hikal,Ahmed Nasreldin,Ali Hamdi*

Key words: LLM, 幻觉检测, 多语言, 提示工程, 集成学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文介绍了一种针对多语言环境下LLM生成文本中幻觉内容的检测方法，结合提示工程与LLM集成验证机制，在多个语言任务中表现优异。

Motivation: 解决多语言环境下LLM生成文本中的幻觉问题，提升文本生成的可靠性。

Method: 任务特定提示工程与LLM集成验证机制（三轮概率投票），模拟人工标注流程，辅以模糊匹配优化。

Result: 在阿拉伯语和巴斯克语中排名第一，德语、瑞典语和芬兰语中排名第二，捷克语、波斯语和法语中排名第三。

Conclusion: 该方法在多语言幻觉检测任务中表现优异，验证了LLM集成与人工流程模拟的有效性。

Abstract: This paper describes our submission for SemEval-2025 Task 3: Mu-SHROOM, the
Multilingual Shared-task on Hallucinations and Related Observable
Overgeneration Mistakes. The task involves detecting hallucinated spans in text
generated by instruction-tuned Large Language Models (LLMs) across multiple
languages. Our approach combines task-specific prompt engineering with an LLM
ensemble verification mechanism, where a primary model extracts hallucination
spans and three independent LLMs adjudicate their validity through
probability-based voting. This framework simulates the human annotation
workflow used in the shared task validation and test data. Additionally, fuzzy
matching refines span alignment. Our system ranked 1st in Arabic and Basque,
2nd in German, Swedish, and Finnish, and 3rd in Czech, Farsi, and French.

</details>


### [74] [EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2505.20888)
*Chengyu Wang,Junbing Yan,Wenrui Cai,Yuanhao Yue,Jun Huang*

Key words: 知识蒸馏, 大型语言模型, 数据合成, 强化学习, Alibaba Cloud PAI

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: EasyDistill是一个全面的工具包，支持黑盒和白盒知识蒸馏（KD）用于大型语言模型（LLMs），提供多功能模块和用户友好接口。

Motivation: 为了更有效地在NLP社区推广和应用先进的知识蒸馏技术，研究人员开发了EasyDistill，以提供便捷的工具和工业解决方案。

Method: 采用数据合成、监督微调、排序优化和强化学习等方法，支持System 1和System 2模型的KD功能。

Result: EasyDistill成功集成到阿里巴巴云PAI平台，并提供了一系列蒸馏模型和开源数据集，满足多样化的应用需求。

Conclusion: EasyDistill使LLMs的先进KD技术更易获取和实施，对NLP社区产生了积极影响。

Abstract: In this paper, we present EasyDistill, a comprehensive toolkit designed for
effective black-box and white-box knowledge distillation (KD) of large language
models (LLMs). Our framework offers versatile functionalities, including data
synthesis, supervised fine-tuning, ranking optimization, and reinforcement
learning techniques specifically tailored for KD scenarios. The toolkit
accommodates KD functionalities for both System 1 (fast, intuitive) and System
2 (slow, analytical) models. With its modular design and user-friendly
interface, EasyDistill empowers researchers and industry practitioners to
seamlessly experiment with and implement state-of-the-art KD strategies for
LLMs. In addition, EasyDistill provides a series of robust distilled models and
KD-based industrial solutions developed by us, along with the corresponding
open-sourced datasets, catering to a variety of use cases. Furthermore, we
describe the seamless integration of EasyDistill into Alibaba Cloud's Platform
for AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for
LLMs more accessible and impactful within the NLP community.

</details>


### [75] [Dub-S2ST: Textless Speech-to-Speech Translation for Seamless Dubbing](https://arxiv.org/abs/2505.20899)
*Jeongsoo Choi,Jaehun Kim,Joon Son Chung*

Key words: 跨语言配音、语音翻译、离散扩散模型、时长控制、语速适配

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种跨语言配音系统，通过离散扩散模型和显式时长控制实现语音模式的保留，生成与源语音时长和语速一致的翻译。

Motivation: 现有语音翻译方法在语音模式（如时长、说话人身份和语速）的转移上表现不足，限制了其在配音应用中的适用性。

Method: 采用离散扩散语音到单元翻译模型，结合显式时长控制和条件流匹配模型合成语音，并引入单元级语速适配机制。

Result: 实验表明，该系统生成的翻译自然流畅且与源语音的时长和语速一致，同时翻译性能具有竞争力。

Conclusion: 提出的框架在保留语音关键特征的同时，实现了高质量的跨语言语音翻译，适用于配音应用。

Abstract: This paper introduces a cross-lingual dubbing system that translates speech
from one language to another while preserving key characteristics such as
duration, speaker identity, and speaking speed. Despite the strong translation
quality of existing speech translation approaches, they often overlook the
transfer of speech patterns, leading to mismatches with source speech and
limiting their suitability for dubbing applications. To address this, we
propose a discrete diffusion-based speech-to-unit translation model with
explicit duration control, enabling time-aligned translation. We then
synthesize speech based on the predicted units and source identity with a
conditional flow matching model. Additionally, we introduce a unit-based speed
adaptation mechanism that guides the translation model to produce speech at a
rate consistent with the source, without relying on any text. Extensive
experiments demonstrate that our framework generates natural and fluent
translations that align with the original speech's duration and speaking pace,
while achieving competitive translation performance.

</details>


### [76] [A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models](https://arxiv.org/abs/2505.20901)
*Junhyuk Choi,Minju Kim,Yeseon Hong,Bugeun Kim*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文通过引入基于刻板印象内容模型（SCM）的新评估指标和BASIC基准，研究了大规模视觉语言模型（LVLMs）中的社会偏见问题，发现SCM评估有效，模型存在颜色、性别和种族刻板印象，且模型架构与参数规模对其有影响。

Motivation: 随着大规模视觉语言模型（LVLMs）的发展，其可能学习并生成社会偏见和刻板印象的问题日益受到关注。以往研究在指标和数据上存在局限，本研究旨在通过新指标和基准填补这一空白。

Method: 提出基于SCM的新评估指标，并开发BASIC基准评估性别、种族和颜色刻板印象。对8种LVLMs进行实验分析。

Result: 发现三个主要结果：SCM评估能有效捕捉刻板印象；LVLMs输出中存在颜色、性别和种族刻板印象；模型架构与参数规模的交互会影响刻板印象。

Conclusion: 研究为LVLMs的刻板印象评估提供了新的方法和基准，揭示了模型设计对社会偏见的影响，BASIC基准已公开供进一步研究。

Abstract: As large vision language models(LVLMs) rapidly advance, concerns about their
potential to learn and generate social biases and stereotypes are increasing.
Previous studies on LVLM's stereotypes face two primary limitations: metrics
that overlooked the importance of content words, and datasets that overlooked
the effect of color. To address these limitations, this study introduces new
evaluation metrics based on the Stereotype Content Model (SCM). We also propose
BASIC, a benchmark for assessing gender, race, and color stereotypes. Using SCM
metrics and BASIC, we conduct a study with eight LVLMs to discover stereotypes.
As a result, we found three findings. (1) The SCM-based evaluation is effective
in capturing stereotypes. (2) LVLMs exhibit color stereotypes in the output
along with gender and race ones. (3) Interaction between model architecture and
parameter sizes seems to affect stereotypes. We release BASIC publicly on
[anonymized for review].

</details>


### [77] [Towards Objective Fine-tuning: How LLMs' Prior Knowledge Causes Potential Poor Calibration?](https://arxiv.org/abs/2505.20903)
*Ziming Wang,Zeyu Shi,Haoyi Zhou,Shiqi Gao,Qingyun Sun,Jianxin Li*

Key words: 大型语言模型, 校准, 微调, 先验知识, CogCalib

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究揭示预训练大模型的先验知识会导致微调时的校准问题，并提出CogCalib框架来针对性优化，显著提升了校准效果。

Motivation: 探讨预训练大模型的先验知识在微调过程中对校准的影响，解决因知识重叠导致的过度自信问题。

Method: 提出CogCalib框架，根据模型的先验知识采用针对性学习策略，并在7个任务和3种大模型家族中验证其有效性。

Result: CogCalib将Llama3-8B的ECE平均降低57%，同时保持性能，且能泛化到域外任务。

Conclusion: CogCalib提升了校准效果，增强了域特定大模型的可靠性和适用性。

Abstract: Fine-tuned Large Language Models (LLMs) often demonstrate poor calibration,
with their confidence scores misaligned with actual performance. While
calibration has been extensively studied in models trained from scratch, the
impact of LLMs' prior knowledge on calibration during fine-tuning remains
understudied. Our research reveals that LLMs' prior knowledge causes potential
poor calibration due to the ubiquitous presence of known data in real-world
fine-tuning, which appears harmful for calibration. Specifically, data aligned
with LLMs' prior knowledge would induce overconfidence, while new knowledge
improves calibration. Our findings expose a tension: LLMs' encyclopedic
knowledge, while enabling task versatility, undermines calibration through
unavoidable knowledge overlaps. To address this, we propose CogCalib, a
cognition-aware framework that applies targeted learning strategies according
to the model's prior knowledge. Experiments across 7 tasks using 3 LLM families
prove that CogCalib significantly improves calibration while maintaining
performance, achieving an average 57\% reduction in ECE compared to standard
fine-tuning in Llama3-8B. These improvements generalize well to out-of-domain
tasks, enhancing the objectivity and reliability of domain-specific LLMs, and
making them more trustworthy for critical human-AI interaction applications.

</details>


### [78] [Automated Privacy Information Annotation in Large Language Model Interactions](https://arxiv.org/abs/2505.20910)
*Hang Zeng,Xiangyu Liu,Yong Hu,Chaoyue Niu,Fan Wu,Shaojie Tang,Guihai Chen*

Key words: 隐私检测,LLM交互,多语言数据集,自动化标注,基线方法

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究构建了一个大规模多语言数据集，用于开发和评估本地可部署的隐私检测模型，解决了现有方法不适用于LLM交互隐私检测的问题。

Motivation: 用户与大型语言模型(LLM)交互时可能无意泄露隐私，需要自动检测隐私泄露的实用工具。

Method: 构建自动化隐私标注流程，使用云端强LLM从对话数据中提取隐私短语并标注泄露信息，设计评估指标，并基于轻量级LLM建立基线方法。

Result: 研究揭示了当前性能与实际应用需求的差距，为未来本地隐私检测方法的研究提供了基础。

Conclusion: 该数据集为开发更有效的本地隐私检测方法提供了重要基础，并指出了未来研究方向。

Abstract: Users interacting with large language models (LLMs) under their real
identifiers often unknowingly risk disclosing private information.
Automatically notifying users whether their queries leak privacy and which
phrases leak what private information has therefore become a practical need.
Existing privacy detection methods, however, were designed for different
objectives and application scenarios, typically tagging personally identifiable
information (PII) in anonymous content. In this work, to support the
development and evaluation of privacy detection models for LLM interactions
that are deployable on local user devices, we construct a large-scale
multilingual dataset with 249K user queries and 154K annotated privacy phrases.
In particular, we build an automated privacy annotation pipeline with
cloud-based strong LLMs to automatically extract privacy phrases from dialogue
datasets and annotate leaked information. We also design evaluation metrics at
the levels of privacy leakage, extracted privacy phrase, and privacy
information. We further establish baseline methods using light-weight LLMs with
both tuning-free and tuning-based methods, and report a comprehensive
evaluation of their performance. Evaluation results reveal a gap between
current performance and the requirements of real-world LLM applications,
motivating future research into more effective local privacy detection methods
grounded in our dataset.

</details>


### [79] [Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models](https://arxiv.org/abs/2505.20921)
*Injae Na,Keonwoong Noh,Woohwan Jung*

Key words: LLM, 自动选择, 成本优化, NLP, 框架

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了LLM-AT框架，通过自动选择不同性能与价格的LLM层级来平衡成本与性能，无需训练即可实现高效响应。

Motivation: 当前NLP任务日益复杂且模块化，如何在子任务中选择合适的LLM层级以平衡成本与性能成为关键挑战。

Method: LLM-AT框架由Starter、Generator和Judge组成，通过迭代升级LLM层级并评估响应有效性，结合准确性估计器实现初始层级选择。

Result: 实验证明LLM-AT在降低成本的同时提升了性能，适用于实际应用。

Conclusion: LLM-AT是一种无需训练、高效且经济的解决方案，适用于复杂NLP任务。

Abstract: LLM providers typically offer multiple LLM tiers, varying in performance and
price. As NLP tasks become more complex and modularized, selecting the suitable
LLM tier for each subtask is a key challenge to balance between cost and
performance. To address the problem, we introduce LLM Automatic Transmission
(LLM-AT) framework that automatically selects LLM tiers without training.
LLM-AT consists of Starter, Generator, and Judge. The starter selects the
initial LLM tier expected to solve the given question, the generator produces a
response using the LLM of the selected tier, and the judge evaluates the
validity of the response. If the response is invalid, LLM-AT iteratively
upgrades to a higher-tier model, generates a new response, and re-evaluates
until a valid response is obtained. Additionally, we propose accuracy
estimator, which enables the suitable initial LLM tier selection without
training. Given an input question, accuracy estimator estimates the expected
accuracy of each LLM tier by computing the valid response rate across top-k
similar queries from past inference records. Experiments demonstrate that
LLM-AT achieves superior performance while reducing costs, making it a
practical solution for real-world applications.

</details>


### [80] [Multi-objective Large Language Model Alignment with Hierarchical Experts](https://arxiv.org/abs/2505.20925)
*Zhuo Li,Guodong Du,Weiyang Guo,Yigeng Zhou,Xiucheng Li,Wenya Wang,Fangming Liu,Yequan Wang,Deheng Ye,Min Zhang,Jing Li*

Key words: 大语言模型, 多目标对齐, 分层混合专家, 帕累托前沿, 轻量级

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了HoE方法，一种轻量级、参数高效且即插即用的方案，无需训练即可调整大语言模型以满足多样用户偏好，并在多个基准测试中表现优于现有方法。

Motivation: 现有的大语言模型对齐方法难以平衡多重目标，常需昂贵训练或效果不佳。HoE旨在解决这一问题，实现高效的多目标适应。

Method: HoE采用分层混合专家架构（LoRA Experts、Router Experts和Preference Routing），无需训练即可优化帕累托前沿。

Result: 在14个目标和6个基准测试的200种偏好中，HoE优于15种现有方法。

Conclusion: HoE是一种高效、灵活的大语言模型对齐方法，显著提升了多目标适应的性能。

Abstract: Aligning large language models (LLMs) to simultaneously satisfy multiple
objectives remains a significant challenge, especially given the diverse and
often conflicting nature of human preferences. Existing alignment methods
struggle to balance trade-offs effectively, often requiring costly retraining
or yielding suboptimal results across the Pareto frontier of preferences. In
this paper, we introduce \textit{HoE}(Hierarchical Mixture-of-Experts), a
\textit{lightweight}, \textit{parameter-efficient}, and \textit{plug-and-play}
approach that eliminates the need for model training, while enabling LLMs to
adapt across the entire Pareto frontier and accommodate diverse user
preferences. In particular, \textit{HoE} consists of three hierarchical
components: LoRA Experts, Router Experts and Preference Routing, reaching
optimal Pareto frontiers and achieving a trade-off between parameter size,
training cost, and performance. We evaluate \textit{HoE} across various tasks
on 14 objectives and 200 different preferences among 6 benchmarks,
demonstrating superior performance over 15 recent baselines. Code is available
in the supplementary materials.

</details>


### [81] [Information-Theoretic Complementary Prompts for Improved Continual Text Classification](https://arxiv.org/abs/2505.20933)
*Duzhen Zhang,Yong Ren,Chenxing Li,Dong Yu,Tielin Zhang*

Key words: 连续文本分类, 灾难性遗忘, 互补学习系统, 信息论, 知识迁移

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为InfoComp的新方法，通过信息论互补提示（Information-Theoretic Complementary Prompts）来解决连续文本分类中的灾难性遗忘问题。该方法分别学习任务特定和任务无关的知识，并在多个基准测试中表现优异。

Motivation: 现有的连续文本分类方法过于关注任务特定知识，忽视了共享的任务无关知识。受人类学习中的互补学习系统理论启发，作者希望通过模拟海马体和大脑皮层的交互作用来改进模型的学习能力。

Method: 提出的InfoComp方法包括学习两个独立的提示空间（P-Prompt和S-Prompt），分别编码任务特定和任务无关知识。通过信息论框架设计了两种新的损失函数来优化这两个空间的表示。

Result: 在多个连续文本分类基准测试中，该方法优于现有的最佳方法，显示了其在避免灾难性遗忘和促进知识迁移方面的有效性。

Conclusion: InfoComp通过明确区分任务特定和任务无关知识，并结合信息论优化，显著提升了连续文本分类的性能，验证了互补学习系统理论在机器学习中的实用性。

Abstract: Continual Text Classification (CTC) aims to continuously classify new text
data over time while minimizing catastrophic forgetting of previously acquired
knowledge. However, existing methods often focus on task-specific knowledge,
overlooking the importance of shared, task-agnostic knowledge. Inspired by the
complementary learning systems theory, which posits that humans learn
continually through the interaction of two systems -- the hippocampus,
responsible for forming distinct representations of specific experiences, and
the neocortex, which extracts more general and transferable representations
from past experiences -- we introduce Information-Theoretic Complementary
Prompts (InfoComp), a novel approach for CTC. InfoComp explicitly learns two
distinct prompt spaces: P(rivate)-Prompt and S(hared)-Prompt. These
respectively encode task-specific and task-invariant knowledge, enabling models
to sequentially learn classification tasks without relying on data replay. To
promote more informative prompt learning, InfoComp uses an
information-theoretic framework that maximizes mutual information between
different parameters (or encoded representations). Within this framework, we
design two novel loss functions: (1) to strengthen the accumulation of
task-specific knowledge in P-Prompt, effectively mitigating catastrophic
forgetting, and (2) to enhance the retention of task-invariant knowledge in
S-Prompt, improving forward knowledge transfer. Extensive experiments on
diverse CTC benchmarks show that our approach outperforms previous
state-of-the-art methods.

</details>


### [82] [On VLMs for Diverse Tasks in Multimodal Meme Classification](https://arxiv.org/abs/2505.20937)
*Deepesh Gavit,Debajyoti Mazumder,Samiran Das,Jasabanta Patro*

Key words: 视觉语言模型, 语言模型, 表情包分类, 微调, 性能提升

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种结合视觉语言模型(VLM)和语言模型(LLM)的新方法，用于提升表情包分类任务的性能，包括讽刺、攻击性和情感分类，并显著提升了基线表现。

Motivation: 研究视觉语言模型(VLM)在表情包分类任务中的性能不足问题，提出结合VLM和LLM的方法以提升分类效果。

Method: 1. 对VLM进行多样化提示策略的基准测试；2. 评估LoRA微调在所有VLM组件中的性能增益；3. 提出利用VLM生成的表情包详细解释来训练较小的LLM，以提升分类性能。

Result: 结合VLM和LLM的方法在讽刺、攻击性和情感分类任务中分别提升了8.34%、3.52%和26.24%的基线性能。

Conclusion: VLM与LLM结合的策略显著提升了表情包分类任务的性能，揭示了VLM的优势与局限性，并为表情包理解提供了新思路。

Abstract: In this paper, we present a comprehensive and systematic analysis of
vision-language models (VLMs) for disparate meme classification tasks. We
introduced a novel approach that generates a VLM-based understanding of meme
images and fine-tunes the LLMs on textual understanding of the embedded meme
text for improving the performance. Our contributions are threefold: (1)
Benchmarking VLMs with diverse prompting strategies purposely to each sub-task;
(2) Evaluating LoRA fine-tuning across all VLM components to assess performance
gains; and (3) Proposing a novel approach where detailed meme interpretations
generated by VLMs are used to train smaller language models (LLMs),
significantly improving classification. The strategy of combining VLMs with
LLMs improved the baseline performance by 8.34%, 3.52% and 26.24% for sarcasm,
offensive and sentiment classification, respectively. Our results reveal the
strengths and limitations of VLMs and present a novel strategy for meme
understanding.

</details>


### [83] [Research Community Perspectives on "Intelligence" and Large Language Models](https://arxiv.org/abs/2505.20959)
*Bertram Højer,Terne Sasha Thorn Jakobsen,Anna Rogers,Stefan Heinrich*

Key words: 人工智能, NLP, 智能定义, 调查, 研究目标

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了NLP领域中“智能”的定义，调查了303位研究者，发现大多数人不认为当前NLP系统具备智能，且仅少数人将开发智能系统作为研究目标。

Motivation: 探讨NLP研究中“智能”一词的实际含义及其在研究议程中的作用。

Method: 对来自NLP、机器学习、认知科学等领域的研究者进行问卷调查（303份有效回应）。

Result: 研究者最认同的智能标准为泛化、适应性和推理；仅29%认为当前NLP系统智能，16.2%以开发智能系统为目标。

Conclusion: 社区对“智能”定义存在争议，多数不认可当前系统智能性，少数将其作为研究方向。

Abstract: Despite the widespread use of ''artificial intelligence'' (AI) framing in
Natural Language Processing (NLP) research, it is not clear what researchers
mean by ''intelligence''. To that end, we present the results of a survey on
the notion of ''intelligence'' among researchers and its role in the research
agenda. The survey elicited complete responses from 303 researchers from a
variety of fields including NLP, Machine Learning (ML), Cognitive Science,
Linguistics, and Neuroscience. We identify 3 criteria of intelligence that the
community agrees on the most: generalization, adaptability, & reasoning. Our
results suggests that the perception of the current NLP systems as
''intelligent'' is a minority position (29%). Furthermore, only 16.2% of the
respondents see developing intelligent systems as a research goal, and these
respondents are more likely to consider the current systems intelligent.

</details>


### [84] [Context-Aware Content Moderation for German Newspaper Comments](https://arxiv.org/abs/2505.20963)
*Felix Krejca,Tobias Kietreiber,Alexander Buchelt,Sebastian Neumaier*

Key words: 内容审核，德语报纸论坛，上下文信息，LSTM，CNN，ChatGPT

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了德语报纸论坛中的内容审核问题，通过结合用户历史和文章主题等上下文信息，评估了LSTM、CNN和ChatGPT-3.5 Turbo模型的性能，发现LSTM和CNN模型表现优异，而ChatGPT的零样本分类效果不佳。

Motivation: 随着在线讨论的增加，自动化内容审核需求日益增长，但目前针对德语报纸论坛的研究较少，且缺乏对平台特定上下文（如用户历史和文章主题）的关注。

Method: 使用了LSTM、CNN和ChatGPT-3.5 Turbo模型，并结合奥地利报纸Der Standard的One Million Posts Corpus数据，评估上下文感知模型的效果。

Result: LSTM和CNN模型在加入上下文信息后表现更优，与现有先进方法相当；ChatGPT的零样本分类未因上下文信息提升，且表现较差。

Conclusion: 上下文信息对LSTM和CNN模型的性能有显著提升，而ChatGPT在此任务中表现不佳，未来研究可进一步优化模型设计。

Abstract: The increasing volume of online discussions requires advanced automatic
content moderation to maintain responsible discourse. While hate speech
detection on social media is well-studied, research on German-language
newspaper forums remains limited. Existing studies often neglect
platform-specific context, such as user history and article themes. This paper
addresses this gap by developing and evaluating binary classification models
for automatic content moderation in German newspaper forums, incorporating
contextual information. Using LSTM, CNN, and ChatGPT-3.5 Turbo, and leveraging
the One Million Posts Corpus from the Austrian newspaper Der Standard, we
assess the impact of context-aware models. Results show that CNN and LSTM
models benefit from contextual information and perform competitively with
state-of-the-art approaches. In contrast, ChatGPT's zero-shot classification
does not improve with added context and underperforms.

</details>


### [85] [Personalized Query Auto-Completion for Long and Short-Term Interests with Adaptive Detoxification Generation](https://arxiv.org/abs/2505.20966)
*Zhibo Wang,Xiaoze Jiang,Zhiheng Qin,Enyun Yu,Han Li*

Key words: 查询自动补全, 个性化表征, 去毒, LaD模型, 拒绝偏好优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出一种新模型LaD，通过分层个性化表征和自适应去毒技术改进查询自动补全（QAC）系统，解决了短查询前缀可能生成有毒内容的问题，并在工业级数据和A/B测试中验证了有效性。

Motivation: 现有QAC系统在个性化表征和去毒方面存在不足：用户搜索行为通常被单一表征，且短查询前缀易生成有毒内容，影响用户体验。

Method: 提出LaD模型，分层捕获长短期兴趣的个性化信息，并基于拒绝偏好优化（RPO）实现在线训练和自适应去毒。

Result: 工业级数据集和在线A/B测试显示显著提升，模型已部署在快手搜索中服务数亿用户。

Conclusion: LaD模型在保留个性化信息的同时有效去毒，显著改善了QAC系统的性能和安全性。

Abstract: Query auto-completion (QAC) plays a crucial role in modern search systems.
However, in real-world applications, there are two pressing challenges that
still need to be addressed. First, there is a need for hierarchical
personalized representations for users. Previous approaches have typically used
users' search behavior as a single, overall representation, which proves
inadequate in more nuanced generative scenarios. Additionally, query prefixes
are typically short and may contain typos or sensitive information, increasing
the likelihood of generating toxic content compared to traditional text
generation tasks. Such toxic content can degrade user experience and lead to
public relations issues. Therefore, the second critical challenge is
detoxifying QAC systems.
  To address these two limitations, we propose a novel model (LaD) that
captures personalized information from both long-term and short-term interests,
incorporating adaptive detoxification. In LaD, personalized information is
captured hierarchically at both coarse-grained and fine-grained levels. This
approach preserves as much personalized information as possible while enabling
online generation within time constraints. To move a futher step, we propose an
online training method based on Reject Preference Optimization (RPO). By
incorporating a special token [Reject] during both the training and inference
processes, the model achieves adaptive detoxification. Consequently, the
generated text presented to users is both non-toxic and relevant to the given
prefix. We conduct comprehensive experiments on industrial-scale datasets and
perform online A/B tests, delivering the largest single-experiment metric
improvement in nearly two years of our product. Our model has been deployed on
Kuaishou search, driving the primary traffic for hundreds of millions of active
users. The code is available at https://github.com/JXZe/LaD.

</details>


### [86] [Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA](https://arxiv.org/abs/2505.20971)
*Xiangqing Shen,Fanfan Wang,Rui Xia*

Key words: LLMs,知识图谱,KGQA,推理链,零样本

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文介绍了RAR框架，通过整合LLMs推理与知识图谱来解决QA任务，显著提升性能并保持高效推理。

Motivation: LLMs在复杂推理任务中表现优异但存在幻觉问题，知识图谱提供结构化知识但缺乏灵活推理。两者结合可互补不足。

Method: RAR包含Reasoner、Aligner和Responser三个组件，通过EM算法优化推理链与图谱路径的匹配。

Result: 在WebQSP和CWQ上的Hit@1分别达93.3%和91.0%，零样本泛化能力强且推理高效。

Conclusion: RAR有效整合推理与知识，生成高质量可解释的答案，性能领先。

Abstract: LLMs have demonstrated remarkable capabilities in complex reasoning tasks,
yet they often suffer from hallucinations and lack reliable factual grounding.
Meanwhile, knowledge graphs (KGs) provide structured factual knowledge but lack
the flexible reasoning abilities of LLMs. In this paper, we present
Reason-Align-Respond (RAR), a novel framework that systematically integrates
LLM reasoning with knowledge graphs for KGQA. Our approach consists of three
key components: a Reasoner that generates human-like reasoning chains, an
Aligner that maps these chains to valid KG paths, and a Responser that
synthesizes the final answer. We formulate this process as a probabilistic
model and optimize it using the Expectation-Maximization algorithm, which
iteratively refines the reasoning chains and knowledge paths. Extensive
experiments on multiple benchmarks demonstrate the effectiveness of RAR,
achieving state-of-the-art performance with Hit@1 scores of 93.3% and 91.0% on
WebQSP and CWQ respectively. Human evaluation confirms that RAR generates
high-quality, interpretable reasoning chains well-aligned with KG paths.
Furthermore, RAR exhibits strong zero-shot generalization capabilities and
maintains computational efficiency during inference.

</details>


### [87] [Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing](https://arxiv.org/abs/2505.20976)
*Peiming Guo,Meishan Zhang,Jianling Li,Min Zhang,Yue Zhang*

Key words: 跨领域选区分析、大型语言模型、选区树库生成、对比学习预训练

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种基于大型语言模型（LLM）的反向生成方法用于生成跨领域的选区树库，并结合对比学习预训练策略提升跨领域选区分析性能，实验结果表现优异。

Motivation: 跨领域选区分析在计算语言学中仍具挑战性，现有多领域选区树库有限，尝试通过LLM自动生成树库来解决这一问题。

Method: 提出LLM反向生成方法（类似选区分析的逆过程），以不完整的跨领域选区树（仅含领域关键词叶子节点）为输入，填充缺失词生成跨领域选区树库；并引入跨度级对比学习预训练策略充分利用生成的树库。

Result: 在MCTB的五个目标领域验证了方法的有效性，实验结果表明其平均性能优于多种基线方法。

Conclusion: LLM反向生成树库结合对比学习预训练在跨领域选区分析中表现出色，达到state-of-the-art水平。

Abstract: Cross-domain constituency parsing is still an unsolved challenge in
computational linguistics since the available multi-domain constituency
treebank is limited. We investigate automatic treebank generation by large
language models (LLMs) in this paper. The performance of LLMs on constituency
parsing is poor, therefore we propose a novel treebank generation method, LLM
back generation, which is similar to the reverse process of constituency
parsing. LLM back generation takes the incomplete cross-domain constituency
tree with only domain keyword leaf nodes as input and fills the missing words
to generate the cross-domain constituency treebank. Besides, we also introduce
a span-level contrastive learning pre-training strategy to make full use of the
LLM back generation treebank for cross-domain constituency parsing. We verify
the effectiveness of our LLM back generation treebank coupled with contrastive
learning pre-training on five target domains of MCTB. Experimental results show
that our approach achieves state-of-the-art performance on average results
compared with various baselines.

</details>


### [88] [Evaluating and Steering Modality Preferences in Multimodal Large Language Model](https://arxiv.org/abs/2505.20977)
*Yu Zhang,Jinlong Ma,Yongshuai Hou,Xuefeng Bai,Kehai Chen,Yang Xiang,Jun Yu,Min Zhang*

Key words: 多模态大语言模型, 模态偏好, 表示工程, 多模态基准, 幻觉缓解

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文研究了多模态大语言模型（MLLMs）在处理多模态上下文时是否存在模态偏好，并构建了MC²基准进行系统评估，发现所有测试模型均表现出明显的模态偏好。作者提出了一种基于表示工程的探测与调控方法，无需额外微调即可显式控制模态偏好，并在下游任务中验证了其有效性。

Motivation: 研究多模态大语言模型在模态冲突场景下是否存在模态偏好，以及如何通过外部干预调控这种偏好，从而提升模型在复杂任务中的表现。

Method: 构建MC²基准评估模态偏好，提出基于表示工程的探测与调控方法，无需微调或精心设计提示即可显式控制模态偏好。

Result: 所有18个测试的MLLMs均表现出模态偏好，且可通过提出的方法有效调控偏好方向，并在幻觉缓解和多模态机器翻译等下游任务中取得改进。

Conclusion: MLLMs普遍存在模态偏好，通过表示工程方法可显式调控偏好方向，为多模态任务优化提供了新思路。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable performance
on complex tasks with multimodal context. However, it is still understudied
whether they exhibit modality preference when processing multimodal contexts.
To study this question, we first build a \textbf{MC\textsuperscript{2}}
benchmark under controlled evidence conflict scenarios to systematically
evaluate modality preference, which is the tendency to favor one modality over
another when making decisions based on multimodal conflicting evidence. Our
extensive evaluation reveals that all 18 tested MLLMs generally demonstrate
clear modality bias, and modality preference can be influenced by external
interventions. An in-depth analysis reveals that the preference direction can
be captured within the latent representations of MLLMs. Built on this, we
propose a probing and steering method based on representation engineering to
explicitly control modality preference without additional fine-tuning or
carefully crafted prompts. Our method effectively amplifies modality preference
toward a desired direction and applies to downstream tasks such as
hallucination mitigation and multimodal machine translation, yielding promising
improvements.

</details>


### [89] [Who Reasons in the Large Language Models?](https://arxiv.org/abs/2505.20993)
*Jie Shao,Jianxin Wu*

Key words: 大型语言模型, Transformer, 推理能力, 输出投影模块, SfN工具集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出假设，认为大型语言模型（LLMs）的推理能力主要源于Transformer多头自注意力机制中的输出投影模块（oproj），并开发了诊断工具SfN验证这一假设。

Motivation: 尽管LLMs表现出强大的能力，但赋予其新能力（如数学推理）的过程仍缺乏透明性。研究试图明确推理能力是否源于特定模块而非整体模型或过拟合。

Method: 引入了Stethoscope for Networks (SfN)工具集，用于分析和探测LLMs内部行为，重点验证oproj模块在推理中的作用。

Result: 通过间接和实证证据表明，oproj模块对推理起关键作用，而其他模块更多影响流畅对话，为LLM可解释性提供了新视角。

Conclusion: 研究为LLM的可解释性和针对性训练策略提供了新方向，可能推动开发更高效、专用化的模型。

Abstract: Despite the impressive performance of large language models (LLMs), the
process of endowing them with new capabilities--such as mathematical
reasoning--remains largely empirical and opaque. A critical open question is
whether reasoning abilities stem from the entire model, specific modules, or
are merely artifacts of overfitting. In this work, we hypothesize that the
reasoning capabilities in well-trained LLMs are primarily attributed to the
output projection module (oproj) in the Transformer's multi-head self-attention
(MHSA) mechanism. To support this hypothesis, we introduce Stethoscope for
Networks (SfN), a suite of diagnostic tools designed to probe and analyze the
internal behaviors of LLMs. Using SfN, we provide both circumstantial and
empirical evidence suggesting that oproj plays a central role in enabling
reasoning, whereas other modules contribute more to fluent dialogue. These
findings offer a new perspective on LLM interpretability and open avenues for
more targeted training strategies, potentially enabling more efficient and
specialized LLMs.

</details>


### [90] [Articulatory strategy in vowel production as a basis for speaker discrimination](https://arxiv.org/abs/2505.20995)
*Justin J. H. Lo,Patrycja Strycharczuk,Sam Kirkham*

Key words: 发音策略, 说话者识别, 舌头形状, 广义Procrustes分析, 似然比

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究发现，英语元音发音中舌头的形状和大小可作为个体识别的依据，尤其是舌头前部的形状变化更具区分性。

Motivation: 探究发音中舌头的个体差异是否能作为说话者识别的依据。

Method: 使用广义Procrustes分析对40名英国西北部说话者的舌头形状数据进行分析，并在似然比框架下评估正交舌头形状特征的说话者区分能力。

Result: 舌头大小是区分个体最强的维度，舌头前部形状变化比后部更具区分性；形状信息单独使用时，若无共变，可与形状和大小信息组合的效果相当。

Conclusion: 发音策略中的舌头形状和大小特征具备说话者特异性，尤其是前部形状变化，为个体识别提供潜在依据。

Abstract: The way speakers articulate is well known to be variable across individuals
while at the same time subject to anatomical and biomechanical constraints. In
this study, we ask whether articulatory strategy in vowel production can be
sufficiently speaker-specific to form the basis for speaker discrimination. We
conducted Generalised Procrustes Analyses of tongue shape data from 40 English
speakers from the North West of England, and assessed the
speaker-discriminatory potential of orthogonal tongue shape features within the
framework of likelihood ratios. Tongue size emerged as the individual dimension
with the strongest discriminatory power, while tongue shape variation in the
more anterior part of the tongue generally outperformed tongue shape variation
in the posterior part. When considered in combination, shape-only information
may offer comparable levels of speaker specificity to size-and-shape
information, but only when features do not exhibit speaker-level co-variation.

</details>


### [91] [Uncertainty Unveiled: Can Exposure to More In-context Examples Mitigate Uncertainty for Large Language Models?](https://arxiv.org/abs/2505.21003)
*Yifei Wang,Yu Sheng,Linjing Li,Daniel Zeng*

Key words: 长上下文情景学习，预测不确定性，认知不确定性，可信度，示例数量

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文研究了在长上下文情景学习（ICL）中，增加示例数量如何影响预测不确定性，揭示了更多示例通过减少认知不确定性（EU）来提升性能的机制。

Motivation: 现有研究多关注性能提升，但示例数量对生成响应可信度的影响尚未充分探索，本文填补了这一空白。

Method: 通过系统量化不同示例数量下的不确定性，分解不确定性，并重点分析认知不确定性（EU）的作用。

Result: 更多示例通过注入任务特定知识减少总不确定性，从而降低EU并提升性能；复杂任务需先处理长输入带来的噪声。

Conclusion: 增加示例能有效减少不确定性，揭示内部各层信心演化是其机制，为可信ICL提供新视角。

Abstract: Recent advances in handling long sequences have facilitated the exploration
of long-context in-context learning (ICL). While much of the existing research
emphasizes performance improvements driven by additional in-context examples,
the influence on the trustworthiness of generated responses remains
underexplored. This paper addresses this gap by investigating how increased
examples influence predictive uncertainty, an essential aspect in
trustworthiness. We begin by systematically quantifying the uncertainty of ICL
with varying shot counts, analyzing the impact of example quantity. Through
uncertainty decomposition, we introduce a novel perspective on performance
enhancement, with a focus on epistemic uncertainty (EU). Our results reveal
that additional examples reduce total uncertainty in both simple and complex
tasks by injecting task-specific knowledge, thereby diminishing EU and
enhancing performance. For complex tasks, these advantages emerge only after
addressing the increased noise and uncertainty associated with longer inputs.
Finally, we explore the evolution of internal confidence across layers,
unveiling the mechanisms driving the reduction in uncertainty.

</details>


### [92] [LLMs are Frequency Pattern Learners in Natural Language Inference](https://arxiv.org/abs/2505.21011)
*Liang Cheng,Zhaowei Wang,Mark Steedman*

Key words: LLMs, NLI, frequency bias, fine-tuning, textual entailment

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LLMs fine-tuned on NLI datasets exploit frequency biases in predicates to improve performance, but struggle with adversarial cases.

Motivation: To uncover the underlying mechanisms of LLM performance improvement through fine-tuning on NLI tasks.

Method: Analyzed predicate frequencies, evaluated model performance on bias-consistent and adversarial instances, and studied frequency patterns in hyponym-hypernym pairs.

Result: Fine-tuned LLMs heavily rely on frequency biases, performing well on bias-consistent cases but poorly on adversarial ones, showing they learn from dataset patterns.

Conclusion: Frequency biases in datasets significantly influence LLM performance on inference tasks, explaining why fine-tuning helps.

Abstract: While fine-tuning LLMs on NLI corpora improves their inferential performance,
the underlying mechanisms driving this improvement remain largely opaque. In
this work, we conduct a series of experiments to investigate what LLMs actually
learn during fine-tuning. We begin by analyzing predicate frequencies in
premises and hypotheses across NLI datasets and identify a consistent frequency
bias, where predicates in hypotheses occur more frequently than those in
premises for positive instances. To assess the impact of this bias, we evaluate
both standard and NLI fine-tuned LLMs on bias-consistent and bias-adversarial
cases. We find that LLMs exploit frequency bias for inference and perform
poorly on adversarial instances. Furthermore, fine-tuned LLMs exhibit
significantly increased reliance on this bias, suggesting that they are
learning these frequency patterns from datasets. Finally, we compute the
frequencies of hyponyms and their corresponding hypernyms from WordNet,
revealing a correlation between frequency bias and textual entailment. These
findings help explain why learning frequency patterns can enhance model
performance on inference tasks.

</details>


### [93] [Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation](https://arxiv.org/abs/2505.21033)
*Seungmin Lee,Yongsang Yoo,Minhwa Jung,Min Song*

Key words: 对话主题分割, 大语言模型, 演绎推理, 意图分类, 自动标注

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了Def-DTS方法，利用大语言模型的多步演绎推理增强对话主题分割性能，通过结构化提示实现上下文总结、意图分类和主题转移检测，实验表明其在多种对话场景中优于传统方法。

Motivation: 对话主题分割（DTS）在NLP任务中至关重要，但面临数据短缺、标注模糊和解决方案复杂度高等问题，而现有大语言模型（LLMs）在DTS中的应用不足。

Method: 采用LLM多步演绎推理，结合结构化提示进行双向上下文总结、通用意图分类和演绎式主题转移检测。

Result: 实验显示Def-DTS在多种对话场景中优于现有方法，尤其在降低第二类错误上有显著效果。

Conclusion: Def-DTS展示了LLM推理技术在DTS中的潜力，并为自动标注提供了可能。

Abstract: Dialogue Topic Segmentation (DTS) aims to divide dialogues into coherent
segments. DTS plays a crucial role in various NLP downstream tasks, but suffers
from chronic problems: data shortage, labeling ambiguity, and incremental
complexity of recently proposed solutions. On the other hand, Despite advances
in Large Language Models (LLMs) and reasoning strategies, these have rarely
been applied to DTS. This paper introduces Def-DTS: Deductive Reasoning for
Open-domain Dialogue Topic Segmentation, which utilizes LLM-based multi-step
deductive reasoning to enhance DTS performance and enable case study using
intermediate result. Our method employs a structured prompting approach for
bidirectional context summarization, utterance intent classification, and
deductive topic shift detection. In the intent classification process, we
propose the generalizable intent list for domain-agnostic dialogue intent
classification. Experiments in various dialogue settings demonstrate that
Def-DTS consistently outperforms traditional and state-of-the-art approaches,
with each subtask contributing to improved performance, particularly in
reducing type 2 error. We also explore the potential for autolabeling,
emphasizing the importance of LLM reasoning techniques in DTS.

</details>


### [94] [FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis](https://arxiv.org/abs/2505.21040)
*Wei Chen,Zhao Zhang,Meng Yuan,Kepeng Xu,Fuzhen Zhuang*

Key words: 目标情感分析, 跨任务知识转移, 细粒度控制, 方面提取

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种细粒度跨任务知识转移框架FCKT，用于有针对性的情感分析任务，通过显式结合方面级信息到情感预测中，有效减少了负迁移并提升了任务性能。

Motivation: 现有研究主要依赖粗粒度的知识转移，缺乏对方面-情感关系的细粒度控制，导致负迁移问题。本文旨在通过细粒度的跨任务知识转移来解决这一问题。

Method: 提出了FCKT框架，通过在情感预测中显式结合方面级信息，实现细粒度的知识转移。

Result: 在三个数据集上的实验表明，FCKT有效减少了负迁移，显著提升了任务性能，且优于多种基线方法和大型语言模型。

Conclusion: FCKT在有针对性的情感分析任务中表现优秀，通过细粒度的知识转移克服了现有方法的局限性。

Abstract: In this paper, we address the task of targeted sentiment analysis (TSA),
which involves two sub-tasks, i.e., identifying specific aspects from reviews
and determining their corresponding sentiments. Aspect extraction forms the
foundation for sentiment prediction, highlighting the critical dependency
between these two tasks for effective cross-task knowledge transfer. While most
existing studies adopt a multi-task learning paradigm to align task-specific
features in the latent space, they predominantly rely on coarse-grained
knowledge transfer. Such approaches lack fine-grained control over
aspect-sentiment relationships, often assuming uniform sentiment polarity
within related aspects. This oversimplification neglects contextual cues that
differentiate sentiments, leading to negative transfer. To overcome these
limitations, we propose FCKT, a fine-grained cross-task knowledge transfer
framework tailored for TSA. By explicitly incorporating aspect-level
information into sentiment prediction, FCKT achieves fine-grained knowledge
transfer, effectively mitigating negative transfer and enhancing task
performance. Experiments on three datasets, including comparisons with various
baselines and large language models (LLMs), demonstrate the effectiveness of
FCKT. The source code is available on https://github.com/cwei01/FCKT.

</details>


### [95] [Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction](https://arxiv.org/abs/2505.21043)
*Sam O'Connor Russell,Naomi Harte*

Key words: 预测性轮流模型，多模态，视觉线索，语音对齐，视频会议

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 介绍了一个多模态的预测性轮流模型MM-VAP，结合语音和视觉线索（如面部表情、头部姿势和视线），在视频会议互动中的表现优于仅依赖音频的现有方法。

Motivation: 由于大多数预测性轮流模型仅依赖语音，难以实现自然的人机交互，因此需要结合多模态信息以提升准确性。

Method: 提出MM-VAP模型，整合语音和视觉线索，并通过按静默时长分组分析来评估性能。

Result: MM-VAP在视频会议中的轮流预测准确率为84%，优于音频模型的79%。面部表情对模型性能贡献最大。

Conclusion: 视觉线索对于准确的轮流预测至关重要，尤其在可见对话者时。研究首次全面分析了多模态预测性轮流模型。

Abstract: Turn-taking is richly multimodal. Predictive turn-taking models (PTTMs)
facilitate naturalistic human-robot interaction, yet most rely solely on
speech. We introduce MM-VAP, a multimodal PTTM which combines speech with
visual cues including facial expression, head pose and gaze. We find that it
outperforms the state-of-the-art audio-only in videoconferencing interactions
(84% vs. 79% hold/shift prediction accuracy). Unlike prior work which
aggregates all holds and shifts, we group by duration of silence between turns.
This reveals that through the inclusion of visual features, MM-VAP outperforms
a state-of-the-art audio-only turn-taking model across all durations of speaker
transitions. We conduct a detailed ablation study, which reveals that facial
expression features contribute the most to model performance. Thus, our working
hypothesis is that when interlocutors can see one another, visual cues are
vital for turn-taking and must therefore be included for accurate turn-taking
prediction. We additionally validate the suitability of automatic speech
alignment for PTTM training using telephone speech. This work represents the
first comprehensive analysis of multimodal PTTMs. We discuss implications for
future work and make all code publicly available.

</details>


### [96] [Predicting Implicit Arguments in Procedural Video Instructions](https://arxiv.org/abs/2505.21068)
*Anil Batra,Laura Sevilla-Lara,Marcus Rohrbach,Frank Keller*

Key words: 语义角色标注（SRL）、多模态、隐式参数、上下文推理、烹饪步骤

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出Implicit-VidSRL数据集，用于多模态烹饪步骤中隐式和显式参数的推理，提升语义角色标注（SRL）在上下文推理中的表现，并展示了iSRL-Qwen2-VL模型在F1分数上的显著改进。

Motivation: 现有SRL基准常忽略隐式参数，导致对步骤理解不完整，需通过上下文推理提升多模态模型的表现。

Method: 引入Implicit-VidSRL数据集，结合视觉变化追踪实体，测试多模态LLMs的隐式参数预测能力，并提出改进模型iSRL-Qwen2-VL。

Result: iSRL-Qwen2-VL在what和where/with隐式语义角色上的F1分数分别相对GPT-4o提升17%和14.7%。

Conclusion: Implicit-VidSRL数据集和iSRL-Qwen2-VL模型有效提升了多模态上下文推理和隐式参数预测能力。

Abstract: Procedural texts help AI enhance reasoning about context and action
sequences. Transforming these into Semantic Role Labeling (SRL) improves
understanding of individual steps by identifying predicate-argument structure
like {verb,what,where/with}. Procedural instructions are highly elliptic, for
instance, (i) add cucumber to the bowl and (ii) add sliced tomatoes, the second
step's where argument is inferred from the context, referring to where the
cucumber was placed. Prior SRL benchmarks often miss implicit arguments,
leading to incomplete understanding. To address this, we introduce
Implicit-VidSRL, a dataset that necessitates inferring implicit and explicit
arguments from contextual information in multimodal cooking procedures. Our
proposed dataset benchmarks multimodal models' contextual reasoning, requiring
entity tracking through visual changes in recipes. We study recent multimodal
LLMs and reveal that they struggle to predict implicit arguments of what and
where/with from multi-modal procedural data given the verb. Lastly, we propose
iSRL-Qwen2-VL, which achieves a 17% relative improvement in F1-score for
what-implicit and a 14.7% for where/with-implicit semantic roles over GPT-4o.

</details>


### [97] [Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation](https://arxiv.org/abs/2505.21072)
*Ekaterina Fadeeva,Aleksandr Rubashevskii,Roman Vashurin,Shehzaad Dhuliawala,Artem Shelmanov,Timothy Baldwin,Preslav Nakov,Mrinmaya Sachan,Maxim Panov*

Key words: Large Language Models, Retrieval-Augmented Generation, hallucination detection, Uncertainty Quantification, Question Answering

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为FRANQ的新方法，用于检测RAG（检索增强生成）输出中的幻觉问题。通过不同的不确定性量化技术，FRANQ能够根据陈述是否忠实于检索内容来评估事实准确性，并在实验中展现出比现有方法更优的性能。

Motivation: RAG系统在开放领域问答中表现优异，但易产生幻觉（事实错误）。现有方法常将事实性与忠实性混为一谈，导致误判。因此，需要一种更准确的方法来检测幻觉问题。

Method: FRANQ结合不确定性量化技术（UQ），根据陈述是否忠实于检索内容来评估其事实准确性。同时，论文还构建了一个新的长问答数据集，标注了事实性和忠实性。

Result: 实验表明，FRANQ在多个数据集和LLM上，对长短问答任务中的事实错误检测比现有方法更准确。

Conclusion: FRANQ为RAG系统提供了一种更可靠的幻觉检测方法，尤其在区分事实性与忠实性方面表现出色。

Abstract: Large Language Models (LLMs) enhanced with external knowledge retrieval, an
approach known as Retrieval-Augmented Generation (RAG), have shown strong
performance in open-domain question answering. However, RAG systems remain
susceptible to hallucinations: factually incorrect outputs that may arise
either from inconsistencies in the model's internal knowledge or incorrect use
of the retrieved context. Existing approaches often conflate factuality with
faithfulness to the retrieved context, misclassifying factually correct
statements as hallucinations if they are not directly supported by the
retrieval. In this paper, we introduce FRANQ (Faithfulness-based Retrieval
Augmented UNcertainty Quantification), a novel method for hallucination
detection in RAG outputs. FRANQ applies different Uncertainty Quantification
(UQ) techniques to estimate factuality based on whether a statement is faithful
to the retrieved context or not. To evaluate FRANQ and other UQ techniques for
RAG, we present a new long-form Question Answering (QA) dataset annotated for
both factuality and faithfulness, combining automated labeling with manual
validation of challenging examples. Extensive experiments on long- and
short-form QA across multiple datasets and LLMs show that FRANQ achieves more
accurate detection of factual errors in RAG-generated responses compared to
existing methods.

</details>


### [98] [LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models](https://arxiv.org/abs/2505.21082)
*Jieyong Kim,Tongyoung Kim,Soonjin Yoon,Jaehyung Kim,Dongha Lee*

Key words: 大语言模型,黑盒模型,个性化,推理路径,RPM

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了RPM框架，通过推理级别的个性化来优化黑盒大语言模型（LLMs）的输出，使其更贴合用户的个性化逻辑。实验证明RPM优于现有的响应级别个性化方法。

Motivation: 现有的黑盒LLMs在生成通用响应时忽略了用户的个人偏好和推理风格，因此需要一种无需修改模型参数即可实现推理级别个性化的方法。

Method: RPM框架首先通过用户历史数据提取特征并构建个性化推理路径，然后在推理阶段通过特征相似性检索对齐案例，生成基于用户逻辑的响应。

Result: 在多样化任务中，RPM表现优于现有的响应级别个性化方法，显著提升了预测准确性和可解释性。

Conclusion: 推理级别的个性化（RPM）是提升黑盒LLMs输出的有效方法，能够更好地满足用户需求。

Abstract: Large language models (LLMs) have recently achieved impressive performance
across a wide range of natural language tasks and are now widely used in
real-world applications. Among them, black-box LLMs--served via APIs without
access to model internals--are especially dominant due to their scalability and
ease of deployment. Despite their strong capabilities, these models typically
produce generalized responses that overlook personal preferences and reasoning
styles. This has led to growing interest in black-box LLM personalization,
which aims to tailor model outputs to user-specific context without modifying
model parameters. However, existing approaches primarily focus on
response-level personalization, attempting to match final outputs without
modeling personal thought process. To address this limitation, we propose RPM,
a framework for reasoning-level personalization that aligns the model's
reasoning process with a user's personalized logic. RPM first constructs
statistical user-specific factors by extracting and grouping
response-influential features from user history. It then builds personalized
reasoning paths that reflect how these factors are used in context. In the
inference stage, RPM retrieves reasoning-aligned examples for new queries via
feature-level similarity and performs inference conditioned on the structured
factors and retrieved reasoning paths, enabling the model to follow
user-specific reasoning trajectories. This reasoning-level personalization
enhances both predictive accuracy and interpretability by grounding model
outputs in user-specific logic through structured information. Extensive
experiments across diverse tasks show that RPM consistently outperforms
response-level personalization methods, demonstrating the effectiveness of
reasoning-level personalization in black-box LLMs.

</details>


### [99] [BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge](https://arxiv.org/abs/2505.21092)
*Daeen Kabir,Minhajur Rahman Chowdhury Mahim,Sheikh Shafayat,Adnan Sadik,Arian Ahmed,Eunsu Kim,Alice Oh*

Key words: BLUCK, LLMs, 孟加拉语, 文化知识, 多选题, 基准测试

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: BLUCK是一个新的数据集，用于评估大语言模型（LLMs）在孟加拉语语言理解和文化知识方面的表现。它包含2366道多选题，涵盖23个类别，基准测试了6个专有和3个开源LLMs，结果显示模型在孟加拉语音学等领域仍有困难。

Motivation: 现有LLMs在多语言能力评估中，对孟加拉语等中等资源语言的关注不足，尤其是在文化历史背景下的表现。BLUCK的推出填补了这一空白。

Method: 构建BLUCK数据集，包含2366道多选题，覆盖23个类别。使用9种LLMs进行基准测试，分析其在孟加拉语和文化知识上的表现。

Result: LLMs整体表现尚可，但在孟加拉语音学等领域存在困难。孟加拉语作为中等资源语言，性能不及英语等主流语言。

Conclusion: BLUCK是首个以孟加拉文化、历史和语言学为核心的MCQ评估基准，为未来研究提供了重要工具。

Abstract: In this work, we introduce BLUCK, a new dataset designed to measure the
performance of Large Language Models (LLMs) in Bengali linguistic understanding
and cultural knowledge. Our dataset comprises 2366 multiple-choice questions
(MCQs) carefully curated from compiled collections of several college and job
level examinations and spans 23 categories covering knowledge on Bangladesh's
culture and history and Bengali linguistics. We benchmarked BLUCK using 6
proprietary and 3 open-source LLMs - including GPT-4o, Claude-3.5-Sonnet,
Gemini-1.5-Pro, Llama-3.3-70B-Instruct, and DeepSeekV3. Our results show that
while these models perform reasonably well overall, they, however, struggles in
some areas of Bengali phonetics. Although current LLMs' performance on Bengali
cultural and linguistic contexts is still not comparable to that of mainstream
languages like English, our results indicate Bengali's status as a mid-resource
language. Importantly, BLUCK is also the first MCQ-based evaluation benchmark
that is centered around native Bengali culture, history, and linguistics.

</details>


### [100] [Thinker: Learning to Think Fast and Slow](https://arxiv.org/abs/2505.21097)
*Stephen Chung,Wenyu Du,Jie Fu*

Key words: Large Language Models, Reinforcement Learning, Dual Process Theory, QA tasks, reasoning efficiency

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 通过在QA任务中引入四个阶段（快速思考、验证、慢速思考和总结），结合心理学双过程理论，提升了LLMs的推理能力，显著提高了模型准确率和推理效率。

Motivation: 现有LLMs在长上下文下的搜索行为不够精确且缺乏自信，导致冗长回答，验证和直觉能力不足。受到心理学双过程理论启发，希望通过结构化任务改进这一问题。

Method: 在QA任务中引入四个阶段：快速思考（严格token限制）、验证（评估初始回答）、慢速思考（精细修正）和总结（提炼步骤）。

Result: Qwen2.5-1.5B准确率从24.9%提升至27.9%；DeepSeek-R1-Qwen-1.5B从45.9%提升至49.8%。快速思考模式下Qwen2.5-1.5B仅用1000 tokens即可达到26.8%准确率。

Conclusion: 直觉与深思熟虑的推理是互补的系统，定向训练能显著提升LLMs的推理能力与效率。

Abstract: Recent studies show that the reasoning capabilities of Large Language Models
(LLMs) can be improved by applying Reinforcement Learning (RL) to
question-answering (QA) tasks in areas such as math and coding. With a long
context length, LLMs may learn to perform search, as indicated by the
self-correction behavior observed in DeepSeek R1. However, this search behavior
is often imprecise and lacks confidence, resulting in long, redundant responses
and highlighting deficiencies in intuition and verification. Inspired by the
Dual Process Theory in psychology, we introduce a simple modification to the QA
task that includes four stages: Fast Thinking, where the LLM must answer within
a strict token budget; Verification, where the model evaluates its initial
response; Slow Thinking, where it refines the initial response with more
deliberation; and Summarization, where it distills the refinement from the
previous stage into precise steps. Our proposed task improves average accuracy
from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for
DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone
achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial
inference efficiency gains. These findings suggest that intuition and
deliberative reasoning are distinct, complementary systems benefiting from
targeted training.

</details>


### [101] [A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction](https://arxiv.org/abs/2505.21109)
*Bogdan Bogachov,Yaoyao Fiona Zhao*

Key words: 域适应, 轻量级模型, 语言图, 幻觉问题, 计算效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为小型语言图（SLG）的轻量级域适应方法，用于解决大语言模型在计算资源消耗和幻觉问题上的挑战。SLG通过图结构组织小语言模型专家节点，在保持性能的同时显著提升了训练速度和推理效率。

Motivation: 当前的大语言模型域适应方法计算资源消耗大，且存在幻觉问题，尤其在要求高精度的工程场景中表现不佳。需要一种高效、轻量的解决方案来降低资源需求并提升模型可靠性。

Method: SLG采用图结构，每个节点代表一个小型专家语言模型，这些模型在特定简洁文本上微调，整体形成轻量级适配系统。

Result: SLG在精确匹配指标上超过传统微调方法3倍，训练速度提升1.7倍，为中小型工程公司提供了经济高效的生成式AI解决方案。

Conclusion: SLG为分布式AI系统和降低中心化计算集群需求提供了潜在可能，同时平衡了性能与资源效率。

Abstract: Despite recent advancements in domain adaptation techniques for large
language models, these methods remain computationally intensive, and the
resulting models can still exhibit hallucination issues. Most existing
adaptation methods do not prioritize reducing the computational resources
required for fine-tuning and inference of language models. Hallucination issues
have gradually decreased with each new model release. However, they remain
prevalent in engineering contexts, where generating well-structured text with
minimal errors and inconsistencies is critical. This work introduces a novel
approach called the Small Language Graph (SLG), which is a lightweight
adaptation solution designed to address the two key challenges outlined above.
The system is structured in the form of a graph, where each node represents a
lightweight expert - a small language model fine-tuned on specific and concise
texts. The results of this study have shown that SLG was able to surpass
conventional fine-tuning methods on the Exact Match metric by 3 times.
Additionally, the fine-tuning process was 1.7 times faster compared to that of
a larger stand-alone language model. These findings introduce a potential for
small to medium-sized engineering companies to confidently use generative AI
technologies, such as LLMs, without the necessity to invest in expensive
computational resources. Also, the graph architecture and the small size of
expert nodes offer a possible opportunity for distributed AI systems, thus
potentially diverting the global need for expensive centralized compute
clusters.

</details>


### [102] [Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA](https://arxiv.org/abs/2505.21115)
*Sergey Pletenev,Maria Marina,Nikolay Ivanov,Daria Galimzianova,Nikita Krayko,Mikhail Salnikov,Vasily Konovalov,Alexander Panchenko,Viktor Moskvoretskii*

Key words: Large Language Models, QA tasks, temporality, EverGreenQA, EG-E5

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了EverGreenQA数据集，用于评估和训练大型语言模型（LLMs）在问答任务中对问题时效性（永恒性或可变性）的理解，并展示了分类模型EG-E5的优越性能及其实际应用。

Motivation: LLMs在问答任务中常出现幻觉（hallucinate），一个重要但未被充分研究的因素是问题的时效性。论文旨在填补这一空白，通过构建EverGreenQA数据集来评估和提升模型对问题时效性的理解。

Method: 论文构建了多语言的EverGreenQA数据集，对12种现代LLMs进行了基准测试，并通过训练EG-E5分类器来验证问题时效性的分类效果。

Result: EG-E5分类器在多语言任务中达到了最先进的性能，并展示了其在自我知识估计、问答数据集过滤和解释GPT-4o检索行为等三个实际应用中的有效性。

Conclusion: 研究表明问题时效性分类是减少LLMs幻觉的关键因素，EG-E5分类器在该任务中表现出色，并具有广泛的应用潜力。

Abstract: Large Language Models (LLMs) often hallucinate in question answering (QA)
tasks. A key yet underexplored factor contributing to this is the temporality
of questions -- whether they are evergreen (answers remain stable over time) or
mutable (answers change). In this work, we introduce EverGreenQA, the first
multilingual QA dataset with evergreen labels, supporting both evaluation and
training. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they
encode question temporality explicitly (via verbalized judgments) or implicitly
(via uncertainty signals). We also train EG-E5, a lightweight multilingual
classifier that achieves SoTA performance on this task. Finally, we demonstrate
the practical utility of evergreen classification across three applications:
improving self-knowledge estimation, filtering QA datasets, and explaining
GPT-4o retrieval behavior.

</details>


### [103] [Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction](https://arxiv.org/abs/2505.21137)
*Mengjie Qian,Rao Ma,Stefano Bannò,Kate M. Knill,Mark J. F. Gales*

Key words: SGEC, SGECF, 端到端模型, Whisper, 伪标签, 提示方法

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究探讨了端到端语音基础模型在口语语法纠错（SGEC）和反馈（SGECF）中的应用，通过伪标签技术和提示方法提升性能。

Motivation: 传统SGEC系统依赖串联的ASR、不流畅检测与移除（DD）和GEC模块，而端到端语音基础模型的应用尚未充分研究。

Method: 采用伪标签技术扩展训练数据（从77小时增至2500小时），并探索基于Whisper的端到端模型结合流畅转录提示的方法。

Result: 伪标签数据提升了基础模型的性能，而提示方法在反馈生成中效果更显著；增大模型规模时，伪标签无增益，但提示仍有效。

Conclusion: 端到端模型在SGEC和反馈生成中具有潜力，提示方法是提升性能的关键，而伪标签技术对小规模模型更有效。

Abstract: Spoken Grammatical Error Correction (SGEC) and Feedback (SGECF) are crucial
for second language learners, teachers and test takers. Traditional SGEC
systems rely on a cascaded pipeline consisting of an ASR, a module for
disfluency detection (DD) and removal and one for GEC. With the rise of
end-to-end (E2E) speech foundation models, we investigate their effectiveness
in SGEC and feedback generation. This work introduces a pseudo-labelling
process to address the challenge of limited labelled data, expanding the
training data size from 77 hours to approximately 2500 hours, leading to
improved performance. Additionally, we prompt an E2E Whisper-based SGEC model
with fluent transcriptions, showing a slight improvement in SGEC performance,
with more significant gains in feedback generation. Finally, we assess the
impact of increasing model size, revealing that while pseudo-labelled data does
not yield performance gain for a larger Whisper model, training with prompts
proves beneficial.

</details>


### [104] [Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis](https://arxiv.org/abs/2505.21138)
*Tianyi Xu,Hongjie Chen,Wang Qing,Lv Hang,Jian Kang,Li Jie,Zhennan Lin,Yongxiang Li,Xie Lei*

Key words: 自监督学习,大型语言模型,方言识别,语音识别,Data2vec2

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文研究了自监督学习结合大型语言模型（LLM）对中文方言和口语音识别（ASR）性能的提升效果，通过在30万小时的无标签方言数据上预训练Data2vec2模型，并在4万小时监督数据上进行对齐训练，取得了多个方言数据集上的最先进（SOTA）成果。

Motivation: 由于中文方言及口语音数据稀缺，现有ASR模型在识别这些变体时表现不佳，自监督学习和LLM的结合为低资源场景下的性能提升提供了可能。

Method: 采用Data2vec2模型在大规模无标签方言数据（30万小时）上预训练，并结合4万小时监督数据进行对齐训练，系统研究了不同投影器和LLM对语音识别性能的影响。

Result: 在多个方言数据集（如Kespeech）上取得了最先进的识别性能。

Conclusion: 自监督预训练结合LLM能显著提升方言及口语音识别性能，研究成果将开源以促进可复现研究。

Abstract: Large-scale training corpora have significantly improved the performance of
ASR models. Unfortunately, due to the relative scarcity of data, Chinese
accents and dialects remain a challenge for most ASR models. Recent
advancements in self-supervised learning have shown that self-supervised pre-
training, combined with large language models (LLM), can effectively enhance
ASR performance in low-resource scenarios. We aim to investigate the
effectiveness of this paradigm for Chinese dialects. Specifically, we pre-train
a Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech
data and do alignment training on a supervised dataset of 40,000 hours. Then,
we systematically examine the impact of various projectors and LLMs on
Mandarin, dialect, and accented speech recognition performance under this
paradigm. Our method achieved SOTA results on multiple dialect datasets,
including Kespeech. We will open-source our work to promote reproducible
research

</details>


### [105] [Assessment of L2 Oral Proficiency using Speech Large Language Models](https://arxiv.org/abs/2505.21148)
*Rao Ma,Mengjie Qian,Siyuan Tang,Stefano Bannò,Kate M. Knill,Mark J. F. Gales*

Key words: 口语评分, 多模态大语言模型, 第二语言学习, 自动评分, 泛化能力

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了多模态大语言模型（LLM）在第二语言口语能力评分中的潜力，通过回归和分类目标对比不同训练策略，结果显示语音LLM优于现有基线，并在跨部分或跨任务评估中表现出强泛化能力。

Motivation: 随着第二语言英语学习者的增加，开发自动口语评分系统的需求上升。传统方法存在信息丢失或局限，而多模态LLM的进步为解决这些问题提供了新机会。

Method: 比较了使用回归和分类目标的不同训练策略，测试语音LLM在口语评分任务中的表现。

Result: 语音LLM在两个数据集上优于所有竞争基线，并在跨部分或跨任务评估中展现出强泛化能力。

Conclusion: 多模态LLM在口语评分任务中具有潜力，其预训练获得的音频理解知识有助于提升性能。

Abstract: The growing population of L2 English speakers has increased the demand for
developing automatic graders for spoken language assessment (SLA).
Historically, statistical models, text encoders, and self-supervised speech
models have been utilised for this task. However, cascaded systems suffer from
the loss of information, while E2E graders also have limitations. With the
recent advancements of multi-modal large language models (LLMs), we aim to
explore their potential as L2 oral proficiency graders and overcome these
issues. In this work, we compare various training strategies using regression
and classification targets. Our results show that speech LLMs outperform all
previous competitive baselines, achieving superior performance on two datasets.
Furthermore, the trained grader demonstrates strong generalisation capabilities
in the cross-part or cross-task evaluation, facilitated by the audio
understanding knowledge acquired during LLM pre-training.

</details>


### [106] [M-Wanda: Improving One-Shot Pruning for Multilingual LLMs](https://arxiv.org/abs/2505.21171)
*Rochelle Choenni,Ivan Titov*

Key words: 多语言模型, 模型修剪, 稀疏化, M-Wanda, 效率权衡

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文研究了多语言大型语言模型（LLM）在稀疏化（pruning）下的性能权衡，提出了一种名为M-Wanda的修剪方法，通过语言感知激活统计和动态调整层间稀疏度来优化多语言性能。

Motivation: 随着对高效模型的需求增加，如何在修剪模型时保留多语言性能成为关键问题，但现有方法在多语言任务上表现不佳。

Method: 提出M-Wanda方法，结合语言感知激活统计和动态层间稀疏度调整，优化多语言模型的修剪效果。

Result: M-Wanda在保持低成本的同时，显著提升了多语言模型的性能。

Conclusion: 本文首次针对多语言性能优化修剪方法，为未来多语言稀疏化研究提供了新思路。

Abstract: Multilingual LLM performance is often critically dependent on model size.
With an eye on efficiency, this has led to a surge in interest in one-shot
pruning methods that retain the benefits of large-scale pretraining while
shrinking the model size. However, as pruning tends to come with performance
loss, it is important to understand the trade-offs between multilinguality and
sparsification. In this work, we study multilingual performance under different
sparsity constraints and show that moderate ratios already substantially harm
performance. To help bridge this gap, we propose M-Wanda, a pruning method that
models cross-lingual variation by incorporating language-aware activation
statistics into its pruning criterion and dynamically adjusts layerwise
sparsity based on cross-lingual importance. We show that M-Wanda consistently
improves performance at minimal additional costs. We are the first to
explicitly optimize pruning to retain multilingual performance, and hope to
inspire future advances in multilingual pruning.

</details>


### [107] [TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word Alignment](https://arxiv.org/abs/2505.21172)
*Zheng Li,Mao Zheng,Mingyang Song,Wenjie Yang*

Key words: 术语翻译, 强化学习, 词对齐, 机器翻译, 深度推理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了TAT-R1，一种通过强化学习和词对齐训练的术语感知翻译模型，显著提升了术语翻译准确性，同时保持通用翻译任务的性能。

Motivation: 当前深度推理大语言模型（如DeepSeek-R1）在数学和编码任务中表现突出，但其在机器翻译中的术语翻译能力尚未被探索。

Method: 1. 使用词对齐模型提取关键词翻译对；2. 设计三种基于规则的词对齐奖励；3. 通过强化学习训练翻译模型，使其专注于关键信息的翻译。

Result: TAT-R1在术语翻译准确率上显著优于基线模型，同时通用翻译任务性能相当。

Conclusion: TAT-R1有效解决了术语翻译问题，并通过消融研究揭示了DeepSeek-R1训练范式对机器翻译的关键发现。

Abstract: Recently, deep reasoning large language models(LLMs) like DeepSeek-R1 have
made significant progress in tasks such as mathematics and coding. Inspired by
this, several studies have employed reinforcement learning(RL) to enhance
models' deep reasoning capabilities and improve machine translation(MT)
quality. However, the terminology translation, an essential task in MT, remains
unexplored in deep reasoning LLMs. In this paper, we propose \textbf{TAT-R1}, a
terminology-aware translation model trained with reinforcement learning and
word alignment. Specifically, we first extract the keyword translation pairs
using a word alignment model. Then we carefully design three types of
rule-based alignment rewards with the extracted alignment relationships. With
those alignment rewards, the RL-trained translation model can learn to focus on
the accurate translation of key information, including terminology in the
source text. Experimental results show the effectiveness of TAT-R1. Our model
significantly improves terminology translation accuracy compared to the
baseline models while maintaining comparable performance on general translation
tasks. In addition, we conduct detailed ablation studies of the
DeepSeek-R1-like training paradigm for machine translation and reveal several
key findings.

</details>


### [108] [Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.21178)
*Mingyang Song,Mao Zheng*

Key words: 大语言模型, 强化学习, 链式思考, 推理简洁性, ConciseR

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了ConciseR框架，通过两阶段强化学习优化大语言模型的推理能力，减少长链式思考中的冗余，提升推理效率和简洁性。

Motivation: 为了解决先进推理模型在长链式思考（CoT）中出现的过度思考和冗余问题，旨在提升推理的简洁性和效率。

Method: 提出两阶段强化学习框架ConciseR：第一阶段通过GRPO++优化模型推理能力；第二阶段通过L-GRPO强制简洁性和效率。遵循“先会走再跑”原则，仅在样本所有推演正确后优化响应长度。

Result: ConciseR在AIME 2024、MATH-500等基准测试中，生成更简洁的CoT响应，优于当前最先进的零强化学习推理模型。

Conclusion: ConciseR有效解决了长CoT响应中的冗余问题，显著提升推理模型的效率和性能。

Abstract: As test-time scaling becomes a pivotal research frontier in Large Language
Models (LLMs) development, contemporary and advanced post-training
methodologies increasingly focus on extending the generation length of long
Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward
DeepSeek R1-like performance. However, recent studies reveal a persistent
overthinking phenomenon in state-of-the-art reasoning models, manifesting as
excessive redundancy or repetitive thinking patterns in long CoT responses. To
address this issue, in this paper, we propose a simple yet effective two-stage
reinforcement learning framework for achieving concise reasoning in LLMs, named
ConciseR. Specifically, the first stage, using more training steps, aims to
incentivize the model's reasoning capabilities via Group Relative Policy
Optimization with clip-higher and dynamic sampling components (GRPO++), and the
second stage, using fewer training steps, explicitly enforces conciseness and
improves efficiency via Length-aware Group Relative Policy Optimization
(L-GRPO). Significantly, ConciseR only optimizes response length once all
rollouts of a sample are correct, following the "walk before you run"
principle. Extensive experimental results demonstrate that our ConciseR model,
which generates more concise CoT reasoning responses, outperforms recent
state-of-the-art reasoning models with zero RL paradigm across AIME 2024,
MATH-500, AMC 2023, Minerva, and Olympiad benchmarks.

</details>


### [109] [Exploring the Latent Capacity of LLMs for One-Step Text Generation](https://arxiv.org/abs/2505.21189)
*Gleb Mezentsev,Ivan Oseledets*

Key words: 大型语言模型、多标记生成、嵌入学习、非自回归解码

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究发现冻结的大型语言模型（LLM）仅需两个学习嵌入即可在单次前向传递中生成数百个准确标记，展示了无需迭代解码的多标记生成能力。

Motivation: 探索是否可以在不使用自回归的情况下实现文本重构，揭示LLMs未被充分发掘的多标记生成能力。

Method: 通过提供两个学习嵌入，研究冻结LLMs在单次前向传递中的多标记生成能力，并分析嵌入的行为和编码信息类型。

Result: 发现冻结LLMs能够在单次前向传递中生成数百个准确标记，且这些嵌入在嵌入空间中形成连通的局部区域。

Conclusion: 这表明无需迭代解码的多标记生成是LLMs的一项潜在能力，且学习专用编码器进入该空间具有可行性。

Abstract: A recent study showed that large language models (LLMs) can reconstruct
surprisingly long texts - up to thousands of tokens - via autoregressive
generation from just one specially trained input embedding. In this work, we
explore whether such reconstruction is possible without autoregression. We show
that frozen LLMs can generate hundreds of accurate tokens in just one forward
pass, when provided with only two learned embeddings. This reveals a surprising
and underexplored capability of LLMs - multi-token generation without iterative
decoding. We investigate the behaviour of these embeddings and provide insight
into the type of information they encode. We also empirically show that
although these representations are not unique for a given text, they form
connected and local regions in embedding space - a property that suggests the
potential of learning a dedicated encoder into that space.

</details>


### [110] [Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation](https://arxiv.org/abs/2505.21190)
*Jong Hak Moon,Geon Choi,Paloma Rabaey,Min Gwan Kim,Hyuk Gi Hong,Jung-Oh Lee,Hangyul Yoon,Eun Woo Doe,Jiyoun Kim,Harshita Sharma,Daniel C. Castro,Javier Alvarez-Valle,Edward Choi*

Key words: 放射学报告,结构化表示,纵向评估,评价指标

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文介绍了LUNGUAGE，一个用于结构化放射学报告生成的基准数据集，支持单报告评估和多研究患者级纵向评估。

Motivation: 现有评估方法仅限于单报告设置且依赖粗糙指标，无法捕捉细粒度临床语义和时间依赖性。

Method: 提出两阶段框架将生成报告转换为细粒度、模式对齐的结构化表示，并提出LUNGUAGESCORE评价指标。

Result: LUNGUAGESCORE在实体、关系和属性层面比较结构化输出，建模患者时间线一致性，实证结果表明其有效支持结构化报告评估。

Conclusion: LUNGUAGE是首个用于序列放射学报告的基准数据集、结构化框架和评价指标。

Abstract: Radiology reports convey detailed clinical observations and capture
diagnostic reasoning that evolves over time. However, existing evaluation
methods are limited to single-report settings and rely on coarse metrics that
fail to capture fine-grained clinical semantics and temporal dependencies. We
introduce LUNGUAGE,a benchmark dataset for structured radiology report
generation that supports both single-report evaluation and longitudinal
patient-level assessment across multiple studies. It contains 1,473 annotated
chest X-ray reports, each reviewed by experts, and 80 of them contain
longitudinal annotations to capture disease progression and inter-study
intervals, also reviewed by experts. Using this benchmark, we develop a
two-stage framework that transforms generated reports into fine-grained,
schema-aligned structured representations, enabling longitudinal
interpretation. We also propose LUNGUAGESCORE, an interpretable metric that
compares structured outputs at the entity, relation, and attribute level while
modeling temporal consistency across patient timelines. These contributions
establish the first benchmark dataset, structuring framework, and evaluation
metric for sequential radiology reporting, with empirical results demonstrating
that LUNGUAGESCORE effectively supports structured report evaluation. The code
is available at: https://github.com/SuperSupermoon/Lunguage

</details>


### [111] [Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities](https://arxiv.org/abs/2505.21191)
*Junyan Zhang,Yubo Gao,Yibo Yan,Jungang Li,Zhaorui Hou,Sicheng Tao,Shuliang Liu,Song Dai,Yonghua Hei,Junzhuo Li,Xuming Hu*

Key words: 大语言模型, 微调, 稀疏组件, SPARCOM, 指令执行

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文通过系统研究微调对LLM计算机制的重新配置，提出SPARCOM框架分析稀疏组件的作用，揭示了其在指令执行中的关键功能。

Motivation: 研究旨在理解微调如何改变LLMs的计算机制，尤其是稀疏组件在指令执行中的作用。

Method: 提出SPARCOM分析框架，包括识别稀疏组件、评估其功能通用性与唯一性，并系统比较其变化。

Result: 实验证明这些组件在指令执行中具有功能通用性、唯一性和关键作用。

Conclusion: 研究阐明了微调诱导的适应与稀疏计算基质的关系，为可信赖LLM社区提供了深层见解。

Abstract: The finetuning of Large Language Models (LLMs) has significantly advanced
their instruction-following capabilities, yet the underlying computational
mechanisms driving these improvements remain poorly understood. This study
systematically examines how fine-tuning reconfigures LLM computations by
isolating and analyzing instruction-specific sparse components, i.e., neurons
in dense models and both neurons and experts in Mixture-of-Experts (MoE)
architectures. In particular, we introduce HexaInst, a carefully curated and
balanced instructional dataset spanning six distinct categories, and propose
SPARCOM, a novel analytical framework comprising three key contributions: (1) a
method for identifying these sparse components, (2) an evaluation of their
functional generality and uniqueness, and (3) a systematic comparison of their
alterations. Through experiments, we demonstrate functional generality,
uniqueness, and the critical role of these components in instruction execution.
By elucidating the relationship between fine-tuning-induced adaptations and
sparse computational substrates, this work provides deeper insights into how
LLMs internalize instruction-following behavior for the trustworthy LLM
community.

</details>


### [112] [Pretrained LLMs Learn Multiple Types of Uncertainty](https://arxiv.org/abs/2505.21218)
*Roi Cohen,Omri Fahn,Gerard de Melo*

Key words: 大型语言模型（LLMs）、不确定性、幻觉、潜在空间、指令微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）如何隐式捕捉不确定性，并探讨了这种能力对预测模型输出正确性的潜在用途。

Motivation: 尽管LLMs在捕捉现实世界知识方面表现优异，但仍存在幻觉问题，导致生成不准确文本。论文旨在探索LLMs是否能在未经显式训练的情况下捕捉不确定性，以改进模型输出的可靠性。

Method: 研究将不确定性视为模型潜在空间中的线性概念，并通过预训练模型分析其捕捉能力。同时，探讨了不同类型的实用性及其对任务正确性的预测能力。

Result: 研究表明，LLMs确实可以隐式捕捉多种类型的不确定性，这些能力可用于预测任务正确性，但与模型规模的扩大无显著关联。统一不确定性类型（如指令微调或[IDK]标记微调）能提升预测效果。

Conclusion: LLMs能够隐式捕捉不确定性，这种能力对改进模型输出的正确性具有实际意义，未来可通过统一不确定性类型进一步优化。

Abstract: Large Language Models are known to capture real-world knowledge, allowing
them to excel in many downstream tasks. Despite recent advances, these models
are still prone to what are commonly known as hallucinations, causing them to
emit unwanted and factually incorrect text. In this work, we study how well
LLMs capture uncertainty, without explicitly being trained for that. We show
that, if considering uncertainty as a linear concept in the model's latent
space, it might indeed be captured, even after only pretraining. We further
show that, though unintuitive, LLMs appear to capture several different types
of uncertainty, each of which can be useful to predict the correctness for a
specific task or benchmark. Furthermore, we provide in-depth results such as
demonstrating a correlation between our correction prediction and the model's
ability to abstain from misinformation using words, and the lack of impact of
model scaling for capturing uncertainty. Finally, we claim that unifying the
uncertainty types as a single one using instruction-tuning or [IDK]-token
tuning is helpful for the model in terms of correctness prediction.

</details>


### [113] [A Representation Level Analysis of NMT Model Robustness to Grammatical Errors](https://arxiv.org/abs/2505.21224)
*Abderrahmane Issam,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Key words: 鲁棒性, 机器翻译, 语法错误检测, 表示相似性, 注意力机制

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了机器翻译模型对语法错误输入的内部表示及其变化，通过GED探测和表示相似性分析，发现编码器先检测语法错误，然后修正表示，并提出“鲁棒性头部”概念。

Motivation: 研究NLP系统的鲁棒性对于构建可靠的机器翻译系统至关重要；现有工作主要关注鲁棒性失败或改进，而本文从模型表示角度探讨鲁棒性。

Method: 采用语法错误检测（GED）探测和表示相似性分析，研究模型对语法错误输入的内部表示变化，并分析注意力机制中的“鲁棒性头部”。

Result: 发现编码器先检测语法错误，随后通过调整表示修正错误，并识别出“鲁棒性头部”——这些注意力头在响应语法错误时关注可解释的语言单位。

Conclusion: 模型在鲁棒性微调后更依赖“鲁棒性头部”更新语法错误表示，这为理解模型鲁棒性机制提供了新视角。

Abstract: Understanding robustness is essential for building reliable NLP systems.
Unfortunately, in the context of machine translation, previous work mainly
focused on documenting robustness failures or improving robustness. In
contrast, we study robustness from a model representation perspective by
looking at internal model representations of ungrammatical inputs and how they
evolve through model layers. For this purpose, we perform Grammatical Error
Detection (GED) probing and representational similarity analysis. Our findings
indicate that the encoder first detects the grammatical error, then corrects it
by moving its representation toward the correct form. To understand what
contributes to this process, we turn to the attention mechanism where we
identify what we term Robustness Heads. We find that Robustness Heads attend to
interpretable linguistic units when responding to grammatical errors, and that
when we fine-tune models for robustness, they tend to rely more on Robustness
Heads for updating the ungrammatical word representation.

</details>


### [114] [LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners](https://arxiv.org/abs/2505.21239)
*Yu He,Zihan Yao,Chentao Song,Tianyu Qi,Jun Liu,Ming Li,Qing Huang*

Key words: 认知诊断, 大语言模型, 冷启动, 语义-认知融合, 个性化学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LMCD：利用大语言模型（LLMs）进行零样本认知诊断的新框架，解决了传统方法在冷启动场景中的不足。

Motivation: 传统认知诊断（CD）模型因缺乏学生-习题交互数据，在冷启动场景表现不佳。NLP方法虽利用预训练模型，但未完全弥合语义理解与认知分析的差距。

Method: LMCD通过两阶段：1. 知识扩散（LLMs生成习题与知识概念的丰富内容）；2. 语义-认知融合（LLMs结合文本信息与学生认知状态）。

Result: 在两个真实数据集上，LMCD在习题冷启动和领域冷启动场景中均显著优于现有方法。

Conclusion: LMCD通过LLMs提升语义与认知的融合能力，有效解决冷启动问题，为个性化学习提供更精准的认知诊断。

Abstract: Cognitive Diagnosis (CD) has become a critical task in AI-empowered
education, supporting personalized learning by accurately assessing students'
cognitive states. However, traditional CD models often struggle in cold-start
scenarios due to the lack of student-exercise interaction data. Recent
NLP-based approaches leveraging pre-trained language models (PLMs) have shown
promise by utilizing textual features but fail to fully bridge the gap between
semantic understanding and cognitive profiling. In this work, we propose
Language Models as Zeroshot Cognitive Diagnosis Learners (LMCD), a novel
framework designed to handle cold-start challenges by harnessing large language
models (LLMs). LMCD operates via two primary phases: (1) Knowledge Diffusion,
where LLMs generate enriched contents of exercises and knowledge concepts
(KCs), establishing stronger semantic links; and (2) Semantic-Cognitive Fusion,
where LLMs employ causal attention mechanisms to integrate textual information
and student cognitive states, creating comprehensive profiles for both students
and exercises. These representations are efficiently trained with off-the-shelf
CD models. Experiments on two real-world datasets demonstrate that LMCD
significantly outperforms state-of-the-art methods in both exercise-cold and
domain-cold settings. The code is publicly available at
https://github.com/TAL-auroraX/LMCD

</details>


### [115] [Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings](https://arxiv.org/abs/2505.21242)
*Gunjan Balde,Soumyadeep Roy,Mainack Mondal,Niloy Ganguly*

Key words: Large Language Models, medical summarization, vocabulary adaptation, OOV words, continual pretraining

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LLMs在医学文本摘要中表现优异，但在高OOV词或高新颖性数据点表现不佳。词汇适应策略可提升性能，尤其针对医学领域词汇碎片化问题。

Motivation: 评估LLMs在困难设置下的表现，尤其是词汇不匹配问题，探索词汇适应策略的改进效果。

Method: 通过词汇适应策略、持续预训练策略，在三个医学摘要数据集上进行实验，并进行人工评估。

Result: 词汇适应显著提升LLMs在医学摘要中的性能，生成更相关和可靠的摘要。

Conclusion: 词汇适应是定制化LLMs到医学领域的有效方法，尤其在面对词汇不匹配和碎片化问题时。

Abstract: Large Language Models (LLMs) recently achieved great success in medical text
summarization by simply using in-context learning. However, these recent
efforts do not perform fine-grained evaluations under difficult settings where
LLMs might fail. They typically report performance scores over the entire
dataset. Through our benchmarking study, we show that LLMs show a significant
performance drop for data points with high concentration of out-of-vocabulary
(OOV) words or with high novelty. Vocabulary adaptation is an intuitive
solution to this vocabulary mismatch issue where the LLM vocabulary gets
updated with certain expert domain (here, medical) words or subwords. An
interesting finding from our study is that Llama-3.1, even with a vocabulary
size of around 128K tokens, still faces over-fragmentation issue with medical
words. To that end, we show vocabulary adaptation helps improve the LLM
summarization performance even in difficult settings. Through extensive
experimentation of multiple vocabulary adaptation strategies, two continual
pretraining strategies, and three benchmark medical summarization datasets, we
gain valuable insights into the role of vocabulary adaptation strategies for
customizing LLMs to the medical domain. We also performed a human evaluation
study with medical experts where they found that vocabulary adaptation results
in more relevant and faithful summaries. Our codebase is made publicly
available at https://github.com/gb-kgp/LLM-MedicalSummarization-Benchmark.

</details>


### [116] [ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision](https://arxiv.org/abs/2505.21250)
*Dosung Lee,Wonjun Oh,Boyoung Kim,Minyoung Kim,Joonsuk Park,Paul Hongsuck Seo*

Key words: 多跳问答,密集检索,无监督训练,大语言模型,ReSCORE

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出ReSCORE方法，通过无标注文档训练密集检索器，提升多跳问答（MHQA）性能。

Motivation: 多跳问答需要跨文档推理，但密集检索器需标注数据微调，而问题在推理步骤中变化大，标注成本高。

Method: ReSCORE利用大语言模型评估文档与问题的相关性和答案一致性，迭代训练检索器。

Result: 在三个MHQA基准测试中，ReSCORE显著提升了检索效果和问答性能。

Conclusion: ReSCORE无需标注数据即可训练密集检索器，为MHQA提供了高效解决方案。

Abstract: Multi-hop question answering (MHQA) involves reasoning across multiple
documents to answer complex questions. Dense retrievers typically outperform
sparse methods like BM25 by leveraging semantic embeddings; however, they
require labeled query-document pairs for fine-tuning. This poses a significant
challenge in MHQA due to the high variability of queries (reformulated)
questions throughout the reasoning steps. To overcome this limitation, we
introduce Retriever Supervision with Consistency and Relevance (ReSCORE), a
novel method for training dense retrievers for MHQA without labeled documents.
ReSCORE leverages large language models to capture each documents relevance to
the question and consistency with the correct answer and use them to train a
retriever within an iterative question-answering framework. Experiments on
three MHQA benchmarks demonstrate the effectiveness of ReSCORE, with
significant improvements in retrieval, and in turn, the state-of-the-art MHQA
performance. Our implementation is available at:
https://leeds1219.github.io/ReSCORE.

</details>


### [117] [Multilingual Pretraining for Pixel Language Models](https://arxiv.org/abs/2505.21265)
*Ilker Kesen,Jonas F. Lotz,Ingo Ziegler,Phillip Rust,Desmond Elliott*

Key words: 像素语言模型、多语言预训练、PIXEL-M4、语义嵌入、跨语言迁移

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该摘要介绍了PIXEL-M4，一种通过在四种语言（英语、印地语、乌克兰语和简体中文）上进行多语言预训练的像素语言模型，其在非拉丁文字任务中表现优于仅英语训练的模型。

Motivation: 探索多语言预训练在像素语言模型中的应用，以提升对多样化语言的支持能力。

Method: 在四种视觉和语言多样性的语言（英语、印地语、乌克兰语和简体中文）上预训练PIXEL-M4模型，并进行多语言评估和词级探测分析。

Result: PIXEL-M4在非拉丁文字任务中表现优于仅英语训练的模型，且其嵌入空间在多语言间语义对齐。

Conclusion: 多语言预训练显著提升了像素语言模型对多样化语言的有效支持能力。

Abstract: Pixel language models operate directly on images of rendered text,
eliminating the need for a fixed vocabulary. While these models have
demonstrated strong capabilities for downstream cross-lingual transfer,
multilingual pretraining remains underexplored. We introduce PIXEL-M4, a model
pretrained on four visually and linguistically diverse languages: English,
Hindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic
and syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart
on non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4
captures rich linguistic features, even in languages not seen during
pretraining. Furthermore, an analysis of its hidden representations shows that
multilingual pretraining yields a semantic embedding space closely aligned
across the languages used for pretraining. This work demonstrates that
multilingual pretraining substantially enhances the capability of pixel
language models to effectively support a diverse set of languages.

</details>


### [118] [rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset](https://arxiv.org/abs/2505.21297)
*Yifei Liu,Li Lyna Zhang,Yi Zhu,Bingcheng Dong,Xudong Zhou,Ning Shang,Fan Yang,Mao Yang*

Key words: 大语言模型、代码推理、竞赛编程、数据集、测试用例、rStar-Coder

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: rStar-Coder 通过构建大规模、已验证的数据集（418K 竞赛级代码问题、580K 长推理解决方案及多样化测试用例），显著提升大语言模型（LLM）的代码推理能力。该方法在多个基准测试中表现优异，小模型性能甚至超越前沿大模型。

Motivation: 当前高难度代码数据集的稀缺限制了 LLM 在代码推理上的进步，尤其是缺乏可验证的测试用例。

Method: 1. 收集竞赛编程问题与解决方案合成新问题；2. 开发输入-输出测试用例生成管道，分三步生成输入并通过互验机制标注输出；3. 为问题添加高质量、已验证的长推理解决方案。

Result: 在 Qwen 模型（1.5B-14B）上，rStar-Coder 数据集表现卓越，如 Qwen2.5-7B 在 LiveCodeBench 上从 17.4% 提升至 57.3%。7B 模型在美国计算奥赛中平均 pass@1 准确率达 16.15%，优于 QWQ-32B。

Conclusion: rStar-Coder 通过数据集的创新构建和验证方法，显著提升了 LLM 的代码推理能力，小模型亦可超越大模型。

Abstract: Advancing code reasoning in large language models (LLMs) is fundamentally
limited by the scarcity of high-difficulty datasets, especially those with
verifiable input-output test cases necessary for rigorous solution validation
at scale. We introduce rStar-Coder, which significantly improves LLM code
reasoning capabilities by constructing a large-scale, verified dataset of 418K
competition-level code problems, 580K long-reasoning solutions along with rich
test cases of varying difficulty. This is achieved through three core
contributions: (1) we curate competitive programming code problems and oracle
solutions to synthesize new, solvable problems; (2) we introduce a reliable
input-output test case synthesis pipeline that decouples the generation into a
three-step input generation method and a mutual verification mechanism for
effective output labeling; (3) we augment problems with high-quality,
test-case-verified long-reasoning solutions. Extensive experiments on Qwen
models (1.5B-14B) across various code reasoning benchmarks demonstrate the
superiority of rStar-Coder dataset, achieving leading performance comparable to
frontier reasoning LLMs with much smaller model sizes. On LiveCodeBench,
rStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and
Qwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more
challenging USA Computing Olympiad, our 7B model achieves an average pass@1
accuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the
dataset will be released at https://github.com/microsoft/rStar.

</details>


### [119] [How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian](https://arxiv.org/abs/2505.21301)
*Andrea Pedrotti,Giulia Rambelli,Caterina Villani,Marianna Bolognesi*

Key words: 从属级别分类，生成式AI，心理语言学数据集，类别组织

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本研究探讨了人类和生成式AI在从属级别分类中的表现，发现两者一致性较低，但在不同语义领域表现差异显著。

Motivation: 先前研究主要集中在基础级别分类，而本研究首次尝试通过分析从属级别生成的具体事例来探讨类别组织，并评估AI模型在这一任务中的表现。

Method: 收集意大利语心理语言学数据集，包含187个具体单词的人类生成事例，并评估文本和视觉大模型在三个任务中的表现：事例生成、类别归纳和典型性判断。

Result: 人类与生成式AI在分类组织上的一致性较低，但与先前研究一致；AI在不同语义领域的表现存在显著差异。

Conclusion: AI生成事例在心理学和语言学研究中有潜力，但仍存在局限性。

Abstract: People can categorize the same entity at multiple taxonomic levels, such as
basic (bear), superordinate (animal), and subordinate (grizzly bear). While
prior research has focused on basic-level categories, this study is the first
attempt to examine the organization of categories by analyzing exemplars
produced at the subordinate level. We present a new Italian psycholinguistic
dataset of human-generated exemplars for 187 concrete words. We then use these
data to evaluate whether textual and vision LLMs produce meaningful exemplars
that align with human category organization across three key tasks: exemplar
generation, category induction, and typicality judgment. Our findings show a
low alignment between humans and LLMs, consistent with previous studies.
However, their performance varies notably across different semantic domains.
Ultimately, this study highlights both the promises and the constraints of
using AI-generated exemplars to support psychological and linguistic research.

</details>


### [120] [Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead](https://arxiv.org/abs/2505.21315)
*Jesujoba O. Alabi,Michael A. Hedderich,David Ifeoluwa Adelani,Dietrich Klakow*

Key words: 非洲语言, NLP, 多语言资源, 数字鸿沟, 可持续性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文调查了过去五年发表的734篇关于非洲语言NLP研究的论文，分析了该领域的进展和趋势，并提出了更具包容性和可持续性的研究方向。

Motivation: 非洲拥有丰富的语言多样性，但当前的NLP技术和大型语言模型主要支持高资源语言，导致非洲语言被边缘化，加剧了数字鸿沟。

Method: 通过分析734篇研究论文，对非洲语言NLP的进展进行系统综述，并总结关键趋势。

Result: 研究发现非洲语言NLP研究正在增长，受到多语言资源、社区倡议和资金支持等因素推动。

Conclusion: 论文提出了更具包容性和可持续性的研究方向，以促进非洲语言NLP的发展。

Abstract: With over 2,000 languages and potentially millions of speakers, Africa
represents one of the richest linguistic regions in the world. Yet, this
diversity is scarcely reflected in state-of-the-art natural language processing
(NLP) systems and large language models (LLMs), which predominantly support a
narrow set of high-resource languages. This exclusion not only limits the reach
and utility of modern NLP technologies but also risks widening the digital
divide across linguistic communities. Nevertheless, NLP research on African
languages is active and growing. In recent years, there has been a surge of
interest in this area, driven by several factors-including the creation of
multilingual language resources, the rise of community-led initiatives, and
increased support through funding programs. In this survey, we analyze 734
research papers on NLP for African languages published over the past five
years, offering a comprehensive overview of recent progress across core tasks.
We identify key trends shaping the field and conclude by outlining promising
directions to foster more inclusive and sustainable NLP research for African
languages.

</details>


### [121] [Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts](https://arxiv.org/abs/2505.21324)
*Yuxin Zhu,Yuting Guo,Noah Marchuck,Abeed Sarker,Yun Wang*

Key words: 大语言模型, 集成学习, ADHD分类, 临床文本分析, 多模型融合

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种集成框架，结合LLaMA3、RoBERTa和SVM模型，通过多数投票机制提升ADHD诊断分类的准确性，F1分数达0.71。

Motivation: 尽管大语言模型（LLMs）发展迅速，但其与传统监督机器学习技术在医学数据（尤其是精神病学领域）的结合仍待探索。叙事数据的复杂性需要多模型互补。

Method: 集成三种模型：LLaMA3（捕捉长距离语义）、RoBERTa（基于临床叙述微调）、SVM（TF-IDF特征分类），通过多数投票机制聚合。数据集包含441例。

Result: 集成模型表现最优（F1=0.71），召回率提升且保持精确度，优于单模型（如SVM），显示对ADHD语言特征的高敏感性。

Conclusion: 混合架构结合LLMs的语义丰富性与传统监督ML的可解释性，为精神病学文本分类提供了新方向。

Abstract: Despite rapid advances in large language models (LLMs), their integration
with traditional supervised machine learning (ML) techniques that have proven
applicability to medical data remains underexplored. This is particularly true
for psychiatric applications, where narrative data often exhibit nuanced
linguistic and contextual complexity, and can benefit from the combination of
multiple models with differing characteristics. In this study, we introduce an
ensemble framework for automatically classifying
Attention-Deficit/Hyperactivity Disorder (ADHD) diagnosis (binary) using
narrative transcripts. Our approach integrates three complementary models:
LLaMA3, an open-source LLM that captures long-range semantic structure;
RoBERTa, a pre-trained transformer model fine-tuned on labeled clinical
narratives; and a Support Vector Machine (SVM) classifier trained using
TF-IDF-based lexical features. These models are aggregated through a majority
voting mechanism to enhance predictive robustness. The dataset includes 441
instances, including 352 for training and 89 for validation. Empirical results
show that the ensemble outperforms individual models, achieving an F$_1$ score
of 0.71 (95\% CI: [0.60-0.80]). Compared to the best-performing individual
model (SVM), the ensemble improved recall while maintaining competitive
precision. This indicates the strong sensitivity of the ensemble in identifying
ADHD-related linguistic cues. These findings demonstrate the promise of hybrid
architectures that leverage the semantic richness of LLMs alongside the
interpretability and pattern recognition capabilities of traditional supervised
ML, offering a new direction for robust and generalizable psychiatric text
classification.

</details>


### [122] [PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims](https://arxiv.org/abs/2505.21342)
*Valentin Knappich,Annemarie Friedrich,Anna Hätty,Simon Razniewski*

Key words: 专利审查、模糊性、自然语言处理、大型语言模型、PEDANTIC数据集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 介绍PEDANTIC数据集，用于自动专利明确性审查，包含14k条标记的专利权利要求，生成高质量注释并通过评估验证其有效性。

Motivation: 专利权利要求中的模糊性常导致申请被拒，现有方法缺乏标注数据集，阻碍自动审查技术的发展。

Method: 通过自动流程从USPTO检索文件，利用大型语言模型（LLMs）提取模糊性原因，并通过人工验证确保注释质量。

Result: LLM在明确性预测上未超越逻辑回归基线，但能准确识别模糊性原因；PEDANTIC数据集为专利AI研究提供新资源。

Conclusion: PEDANTIC数据集推动专利审查技术进步，未来将公开数据及代码。

Abstract: Patent claims define the scope of protection for an invention. If there are
ambiguities in a claim, it is rejected by the patent office. In the US, this is
referred to as indefiniteness (35 U.S.C {\S} 112(b)) and is among the most
frequent reasons for patent application rejection. The development of automatic
methods for patent definiteness examination has the potential to make patent
drafting and examination more efficient, but no annotated dataset has been
published to date.
  We introduce PEDANTIC (\underline{P}at\underline{e}nt
\underline{D}efiniteness Ex\underline{a}mi\underline{n}a\underline{ti}on
\underline{C}orpus), a novel dataset of 14k US patent claims from patent
applications relating to Natural Language Processing (NLP), annotated with
reasons for indefiniteness. We construct PEDANTIC using a fully automatic
pipeline that retrieves office action documents from the USPTO and uses Large
Language Models (LLMs) to extract the reasons for indefiniteness. A human
validation study confirms the pipeline's accuracy in generating high-quality
annotations. To gain insight beyond binary classification metrics, we implement
an LLM-as-Judge evaluation that compares the free-form reasoning of every
model-cited reason with every examiner-cited reason. We show that LLM agents
based on Qwen 2.5 32B and 72B struggle to outperform logistic regression
baselines on definiteness prediction, even though they often correctly identify
the underlying reasons. PEDANTIC provides a valuable resource for patent AI
researchers, enabling the development of advanced examination models. We will
publicly release the dataset and code.

</details>


### [123] [Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning](https://arxiv.org/abs/2505.21354)
*Bidyarthi Paul,Jalisha Jashim Era,Mirazur Rahman Zim,Tahmid Sattar Aothoi,Faisal Muhammad Shah*

Key words: 孟加拉数学应用题，SOMADHAN数据集，大型语言模型，链式思考提示，LoRA微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文创建了SOMADHAN数据集，包含8792个复杂的孟加拉数学应用题，并通过多种大型语言模型评估其表现。链式思考提示显著提升性能，其中LLaMA-3.3 70B在少样本CoT提示下达到88%准确率。

Motivation: 解决孟加拉数学应用题（MWPs）是NLP中的一个挑战，主要因为缺乏高质量数据集和语言的低资源状态。

Method: 构建SOMADHAN数据集，评估多种LLMs（如GPT-4o、LLaMA系列等）在零样本和少样本下的表现，并应用CoT提示和LoRA微调。

Result: CoT提示显著提升性能，LLaMA-3.3 70B在少样本CoT提示下达到最高准确率88%。

Conclusion: 该研究填补了孟加拉NLP的空白，为低资源语言的推理研究提供了高质量数据集和可扩展框架。

Abstract: Solving Bengali Math Word Problems (MWPs) remains a major challenge in
natural language processing (NLP) due to the language's low-resource status and
the multi-step reasoning required. Existing models struggle with complex
Bengali MWPs, largely because no human-annotated Bengali dataset has previously
addressed this task. This gap has limited progress in Bengali mathematical
reasoning. To address this, we created SOMADHAN, a dataset of 8792 complex
Bengali MWPs with manually written, step-by-step solutions. We designed this
dataset to support reasoning-focused evaluation and model development in a
linguistically underrepresented context. Using SOMADHAN, we evaluated a range
of large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series
models, Deepseek, and Qwen - through both zero-shot and few-shot prompting with
and without Chain of Thought (CoT) reasoning. CoT prompting consistently
improved performance over standard prompting, especially in tasks requiring
multi-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with
few-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune
models efficiently, enabling them to adapt to Bengali MWPs with minimal
computational cost. Our work fills a critical gap in Bengali NLP by providing a
high-quality reasoning dataset and a scalable framework for solving complex
MWPs. We aim to advance equitable research in low-resource languages and
enhance reasoning capabilities in educational and language technologies.

</details>


### [124] [Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History](https://arxiv.org/abs/2505.21362)
*Qishuai Zhong,Zongmin Li,Siqi Fan,Aixin Sun*

Key words: 大语言模型, 社会人口特征适应, 多轮对话, 值表达, 推理能力

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种评估大语言模型在用户社会人口特征（如年龄、职业和教育水平）下行为适应性的框架，并通过多代理流程构建合成数据集进行测试。结果表明，大多数模型会根据人口特征调整表达，但一致性存在差异，推理能力强的模型适应性更强。

Motivation: 现有的大语言模型评价多关注单轮提示的行为适应，而忽略了多轮对话历史对社会人口特征的适应性。作者旨在填补这一空白，开发了一个评估框架。

Method: 作者提出了一个框架，通过用户配置文件或隐式多轮对话历史引入社会人口特征，利用多代理流程构建合成数据集，并结合Value Survey Module (VSM 2013)的问题来探测模型的值表达。

Result: 研究发现，大多数模型会根据年龄和教育水平等社会人口特征调整其表达的值，但模型之间的一致性存在差异。推理能力强的模型表现出更强的适应性。

Conclusion: 推理能力对于大语言模型在社会人口特征上的稳健适应至关重要。研究框架为未来模型设计提供了新的评估维度。

Abstract: Effective engagement by large language models (LLMs) requires adapting
responses to users' sociodemographic characteristics, such as age, occupation,
and education level. While many real-world applications leverage dialogue
history for contextualization, existing evaluations of LLMs' behavioral
adaptation often focus on single-turn prompts. In this paper, we propose a
framework to evaluate LLM adaptation when attributes are introduced either (1)
explicitly via user profiles in the prompt or (2) implicitly through multi-turn
dialogue history. We assess the consistency of model behavior across these
modalities. Using a multi-agent pipeline, we construct a synthetic dataset
pairing dialogue histories with distinct user profiles and employ questions
from the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe
value expression. Our findings indicate that most models adjust their expressed
values in response to demographic changes, particularly in age and education
level, but consistency varies. Models with stronger reasoning capabilities
demonstrate greater alignment, indicating the importance of reasoning in robust
sociodemographic adaptation.

</details>


### [125] [Analyzing values about gendered language reform in LLMs' revisions](https://arxiv.org/abs/2505.21378)
*Jules Watson,Xi Wang,Raymond Liu,Suzanne Stevenson,Barend Beekhuizen*

Key words: 大型语言模型, 性别角色名词, 女权主义, 跨性别包容性, 价值对齐

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLM）在文本修订中对性别角色名词的修订及其理由，评估其是否符合女权主义和跨性别包容性语言改革，并分析LLM是否像人类一样对改革应用的上下文效应敏感。

Motivation: 研究LLMs在文本修订中的性别角色名词处理，评估其在女权主义和跨性别包容性语言改革中的表现，以促进价值对齐。

Method: 通过分析LLMs对性别角色名词的修订和理由，结合社会语言学视角，评估其对上下文效应的敏感性。

Result: 研究发现LLMs在应用女权主义和跨性别包容性语言改革时表现出人类类似的上下文敏感性，提供了广泛的证据支持此类效应。

Conclusion: LLMs在性别角色名词修订中表现出与人类相似的上下文敏感性，这对价值对齐具有重要意义。

Abstract: Within the common LLM use case of text revision, we study LLMs' revision of
gendered role nouns (e.g., outdoorsperson/woman/man) and their justifications
of such revisions. We evaluate their alignment with feminist and
trans-inclusive language reforms for English. Drawing on insight from
sociolinguistics, we further assess if LLMs are sensitive to the same
contextual effects in the application of such reforms as people are, finding
broad evidence of such effects. We discuss implications for value alignment.

</details>


### [126] [PHISH in MESH: Korean Adversarial Phonetic Substitution and Phonetic-Semantic Feature Integration Defense](https://arxiv.org/abs/2505.21380)
*Byungjun Kim,Minju Kim,Hyeonchu Park,Bugeun Kim*

Key words: 韩语, 语音替换, 仇恨言论检测, PHISH, MESH

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出PHISH和MESH方法，通过韩语语音特性和混合编码增强对语音替换的恶意言论检测。

Motivation: 现有研究忽视韩语对语音替换的脆弱性，且多聚焦于数据集构建而非架构防御。

Method: 提出PHISH（利用韩语语音特性）和MESH（结合语义-语音特征的混合编码）。

Result: 实验显示方法在扰动和非扰动数据集上均有效，提升检测性能。

Conclusion: 方法不仅提高检测效果，还反映真实恶意行为模式。

Abstract: As malicious users increasingly employ phonetic substitution to evade hate
speech detection, researchers have investigated such strategies. However, two
key challenges remain. First, existing studies have overlooked the Korean
language, despite its vulnerability to phonetic perturbations due to its
phonographic nature. Second, prior work has primarily focused on constructing
datasets rather than developing architectural defenses. To address these
challenges, we propose (1) PHonetic-Informed Substitution for Hangul (PHISH)
that exploits the phonological characteristics of the Korean writing system,
and (2) Mixed Encoding of Semantic-pHonetic features (MESH) that enhances the
detector's robustness by incorporating phonetic information at the
architectural level. Our experimental results demonstrate the effectiveness of
our proposed methods on both perturbed and unperturbed datasets, suggesting
that they not only improve detection performance but also reflect realistic
adversarial behaviors employed by malicious users.

</details>


### [127] [AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs](https://arxiv.org/abs/2505.21389)
*Xuanwen Ding,Chengjun Pan,Zejun Li,Jiwen Zhang,Siyuan Wang,Zhongyu Wei*

Key words: 多模态大型语言模型, 评估成本, AutoJudger, 项目反应理论, 动态选择

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: AutoJudger是一种基于代理的框架，通过动态选择最具信息量的测试问题，显著降低多模态大型语言模型（MLLM）的评估成本。

Motivation: 当前MLLM评估成本高昂，因为其规模和跨模态复杂性需要大量评分工作。AutoJudger旨在解决这一成本问题。

Method: 采用项目反应理论（IRT）估计问题难度，并结合自主评估代理动态选择测试问题。包含语义感知检索机制和动态内存。

Result: 实验表明，AutoJudger仅使用4%的数据即可在MMT-Bench上实现90%以上的排名准确率。

Conclusion: AutoJudger显著降低了评估成本，同时保持了高准确性。

Abstract: Evaluating multimodal large language models (MLLMs) is increasingly
expensive, as the growing size and cross-modality complexity of benchmarks
demand significant scoring efforts. To tackle with this difficulty, we
introduce AutoJudger, an agent-driven framework for efficient and adaptive
benchmarking of MLLMs that tackles this escalating cost. AutoJudger employs the
Item Response Theory (IRT) to estimate the question difficulty and an
autonomous evaluation agent to dynamically select the most informative test
questions based on the model's real-time performance. Specifically, AutoJudger
incorporates two pivotal components: a semantic-aware retrieval mechanism to
ensure that selected questions cover diverse and challenging scenarios across
both vision and language modalities, and a dynamic memory that maintains
contextual statistics of previously evaluated questions to guide coherent and
globally informed question selection throughout the evaluation process.
Extensive experiments on four representative multimodal benchmarks demonstrate
that our adaptive framework dramatically reduces evaluation expenses, i.e.
AutoJudger uses only 4% of the data to achieve over 90% ranking accuracy with
the full benchmark evaluation on MMT-Bench.

</details>


### [128] [Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science](https://arxiv.org/abs/2505.21396)
*Xiao Liu,Xinyi Dong,Xinyang Gao,Yansong Feng,Xun Pang*

Key words: 大语言模型, 研究想法生成, 数据增强, 可行性, 自动验证

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了如何通过在大语言模型（LLM）生成研究想法的过程中加入相关数据来提高想法质量。实验证明，元数据和自动验证能显著提升想法的可行性和质量。

Motivation: 尽管LLM在生成研究想法方面表现出色，但其生成的想法常面临可行性和有效性挑战。本文旨在通过数据增强提升LLM生成想法的质量。

Method: 方法包括在想法生成阶段加入元数据以引导可行方向，以及在想法选择阶段加入自动验证以评估假设的实证合理性。

Result: 实验结果显示，元数据使生成想法的可行性提升20%，自动验证使选定想法的整体质量提升7%。人类研究也证实这些方法能激励研究者提出更高质量的想法。

Conclusion: 研究凸显了数据驱动的研究想法生成的潜力，并证明了LLM辅助想法生成在学术实践中的实用性。

Abstract: Recent advancements in large language models (LLMs) have shown promise in
generating novel research ideas. However, these ideas often face challenges
related to feasibility and expected effectiveness. This paper explores how
augmenting LLMs with relevant data during the idea generation process can
enhance the quality of generated ideas. We introduce two ways of incorporating
data: (1) providing metadata during the idea generation stage to guide LLMs
toward feasible directions, and (2) adding automatic validation during the idea
selection stage to assess the empirical plausibility of hypotheses within
ideas. We conduct experiments in the social science domain, specifically with
climate negotiation topics, and find that metadata improves the feasibility of
generated ideas by 20%, while automatic validation improves the overall quality
of selected ideas by 7%. A human study shows that LLM-generated ideas, along
with their related data and validation processes, inspire researchers to
propose research ideas with higher quality. Our work highlights the potential
of data-driven research idea generation, and underscores the practical utility
of LLM-assisted ideation in real-world academic settings.

</details>


### [129] [DecisionFlow: Advancing Large Language Model as Principled Decision Maker](https://arxiv.org/abs/2505.21397)
*Xiusi Chen,Shanyong Wang,Cheng Qian,Hongru Wang,Peixuan Han,Heng Ji*

Key words: DecisionFlow, 高风险决策, 透明推理, 符号推理, LLM

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DecisionFlow通过结构化推理框架提升高风险领域决策的透明度和准确性，相比基线方法提升30%准确率。

Motivation: 高风险领域需透明化决策，现有语言模型缺乏结构化推理能力，难以生成紧密耦合的决策与解释。

Method: 提出DecisionFlow框架，基于动作、属性和约束构建语义决策空间，通过潜在效用函数评估权衡，实现透明推理。

Result: 在两个高风险基准测试中，DecisionFlow较基线方法准确率提升30%，且结果对齐性更高。

Conclusion: 研究将符号推理与LLM结合，推动可解释、可靠的决策支持系统发展，代码数据已开源。

Abstract: In high-stakes domains such as healthcare and finance, effective
decision-making demands not just accurate outcomes but transparent and
explainable reasoning. However, current language models often lack the
structured deliberation needed for such tasks, instead generating decisions and
justifications in a disconnected, post-hoc manner. To address this, we propose
DecisionFlow, a novel decision modeling framework that guides models to reason
over structured representations of actions, attributes, and constraints. Rather
than predicting answers directly from prompts, DecisionFlow builds a
semantically grounded decision space and infers a latent utility function to
evaluate trade-offs in a transparent, utility-driven manner. This process
produces decisions tightly coupled with interpretable rationales reflecting the
model's reasoning. Empirical results on two high-stakes benchmarks show that
DecisionFlow not only achieves up to 30% accuracy gains over strong prompting
baselines but also enhances alignment in outcomes. Our work is a critical step
toward integrating symbolic reasoning with LLMs, enabling more accountable,
explainable, and reliable LLM decision support systems. We release the data and
code at https://github.com/xiusic/DecisionFlow.

</details>


### [130] [Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling](https://arxiv.org/abs/2505.21399)
*Hovhannes Tamoyan,Subhabrata Dutta,Iryna Gurevych*

Key words: 大语言模型、事实正确性、残差流、自我监控、可解释性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文发现大语言模型（LLM）在生成内容时具有内部‘指南针’，通过线性特征判断事实回忆的正确性，且信号对格式变化鲁棒，进一步提升了模型的可解释性和可靠性。

Motivation: 解决生成内容中的事实错误问题，探索LLM在生成时是否具备内在的自我监控能力。

Method: 通过分析Transformer残差流中的线性特征，研究模型对实体-关系-属性三元组正确性的自我判断能力，并测试其鲁棒性和训练动态。

Result: 发现LLM在生成时确实存在自我监控信号，该信号对格式变化不敏感，并在训练中期中间层表现最佳。

Conclusion: LLM具备内在的自我监控能力，为提升模型的可解释性和可靠性提供了新视角。

Abstract: Factual incorrectness in generated content is one of the primary concerns in
ubiquitous deployment of large language models (LLMs). Prior findings suggest
LLMs can (sometimes) detect factual incorrectness in their generated content
(i.e., fact-checking post-generation). In this work, we provide evidence
supporting the presence of LLMs' internal compass that dictate the correctness
of factual recall at the time of generation. We demonstrate that for a given
subject entity and a relation, LLMs internally encode linear features in the
Transformer's residual stream that dictate whether it will be able to recall
the correct attribute (that forms a valid entity-relation-attribute triplet).
This self-awareness signal is robust to minor formatting variations. We
investigate the effects of context perturbation via different example selection
strategies. Scaling experiments across model sizes and training dynamics
highlight that self-awareness emerges rapidly during training and peaks in
intermediate layers. These findings uncover intrinsic self-monitoring
capabilities within LLMs, contributing to their interpretability and
reliability.

</details>


### [131] [RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models](https://arxiv.org/abs/2505.21409)
*Dario Satriani,Enzo Veltri,Donatello Santoro,Paolo Papotti*

Key words: 大规模语言模型、事实性、基准测试、知识检索、结构化输出

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: RelationalFactQA 是一个新的基准测试，用于评估 GPT 在大规模结构化知识检索中的表现，发现即使最先进的 GPT 在生成关系型表格数据时，事实准确性也不超过 25%。

Motivation: 当前 GPT 在生成结构化、多记录表格输出时的表现不佳，这暴露了其在知识检索和事实性方面的局限。

Method: 引入了 RelationalFactQA 基准测试，其中包含多样化自然语言问题（带 SQL）和金标准表格答案，用于系统性评估。

Result: 实验显示，即使最先进的 GPT 在生成关系型输出时表现不佳，事实准确性不超过 25%，且随输出维度增加性能下降。

Conclusion: RelationalFactQA 是衡量 GPT 未来事实性进展的重要资源。

Abstract: Factuality in Large Language Models (LLMs) is a persistent challenge. Current
benchmarks often assess short factual answers, overlooking the critical ability
to generate structured, multi-record tabular outputs from parametric knowledge.
We demonstrate that this relational fact retrieval is substantially more
difficult than isolated point-wise queries, even when individual facts are
known to the model, exposing distinct failure modes sensitive to output
dimensionality (e.g., number of attributes or records). To systematically
evaluate this under-explored capability, we introduce RelationalFactQA, a new
benchmark featuring diverse natural language questions (paired with SQL) and
gold-standard tabular answers, specifically designed to assess knowledge
retrieval in a structured format. RelationalFactQA enables analysis across
varying query complexities, output sizes, and data characteristics. Our
experiments reveal that even state-of-the-art LLMs struggle significantly, not
exceeding 25% factual accuracy in generating relational outputs, with
performance notably degrading as output dimensionality increases. These
findings underscore critical limitations in current LLMs' ability to synthesize
structured factual knowledge and establish RelationalFactQA as a crucial
resource for measuring future progress in LLM factuality.

</details>


### [132] [Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity](https://arxiv.org/abs/2505.21411)
*Yehui Tang,Xiaosong Li,Fangcheng Liu,Wei Guo,Hang Zhou,Yaoyuan Wang,Kai Han,Xianzhi Yu,Jinpeng Li,Hui Zang,Fei Mi,Xiaojun Meng,Zhicheng Liu,Hanting Chen,Binfan Zheng,Can Chen,Youliang Yan,Ruiming Tang,Peifeng Qin,Xinghao Chen,Dacheng Tao,Yunhe Wang*

Key words: 混合专家模型（MoE），分组混合专家（MoGE），昇腾NPUs，负载均衡，Pangu Pro MoE

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 摘要介绍了混合专家模型（MoE）在大语言模型中的应用挑战，提出了分组混合专家（MoGE）以优化专家负载均衡，并在昇腾NPUs上实现了高效训练与推理。

Motivation: MoE模型存在专家激活不均衡问题，导致系统效率低下，尤其在多设备并行时。

Method: 提出MoGE架构，通过分组限制专家激活数量，实现负载均衡，并在昇腾NPUs上构建了720亿参数的Pangu Pro MoE模型。

Result: 实验显示MoGE显著提升负载均衡与执行效率，推理性能达1148-1528 tokens/s/卡，优于同类32B和72B密集模型。

Conclusion: MoGE在昇腾NPUs上实现了高效训练与推理，性能领先于开源模型GLM-Z1-32B和Qwen3-32B。

Abstract: The surgence of Mixture of Experts (MoE) in Large Language Models promises a
small price of execution cost for a much larger model parameter count and
learning capacity, because only a small fraction of parameters are activated
for each input token. However, it is commonly observed that some experts are
activated far more often than others, leading to system inefficiency when
running the experts on different devices in parallel. Therefore, we introduce
Mixture of Grouped Experts (MoGE), which groups the experts during selection
and balances the expert workload better than MoE in nature. It constrains
tokens to activate an equal number of experts within each predefined expert
group. When a model execution is distributed on multiple devices, this
architectural design ensures a balanced computational load across devices,
significantly enhancing throughput, particularly for the inference phase.
Further, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE
with 72 billion total parameters, 16 billion of which are activated for each
token. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and
800I A2 through extensive system simulation studies. Our experiments indicate
that MoGE indeed leads to better expert load balancing and more efficient
execution for both model training and inference on Ascend NPUs. The inference
performance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further
improved to 1528 tokens/s per card by speculative acceleration, outperforming
comparable 32B and 72B Dense models. Furthermore, we achieve an excellent
cost-to-performance ratio for model inference on Ascend 300I Duo.Our studies
show that Ascend NPUs are capable of training Pangu Pro MoE with massive
parallelization to make it a leading model within the sub-100B total parameter
class, outperforming prominent open-source models like GLM-Z1-32B and
Qwen3-32B.

</details>


### [133] [RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation](https://arxiv.org/abs/2505.21413)
*Xiao Liu,Da Yin,Zirui Wu,Yansong Feng*

Key words: LLMs, tool creation, reasoning, RefTool, external references

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: RefTool framework improves LLM problem-solving by creating tools from structured references, boosting accuracy by 11.3% while being cost-efficient.

Motivation: Current tool-generation methods rely on LLMs' internal knowledge, limiting their effectiveness in unfamiliar domains.

Method: RefTool creates tools from external references (e.g., textbooks), validates them, and organizes hierarchically for selection and application.

Result: Outperforms existing methods by 11.3% in accuracy on causality, physics, and chemistry benchmarks.

Conclusion: Grounding tool creation in references enhances LLM reasoning, overcoming knowledge limitations.

Abstract: Tools enhance the reasoning capabilities of large language models (LLMs) in
complex problem-solving tasks, but not all tasks have available tools. In the
absence of predefined tools, prior works have explored instructing LLMs to
generate tools on their own. However, such approaches rely heavily on the
models' internal knowledge and would fail in domains beyond the LLMs' knowledge
scope. To address this limitation, we propose RefTool, a reference-guided
framework for automatic tool creation that leverages structured external
materials such as textbooks. RefTool consists of two modules: (1) tool
creation, where LLMs generate executable tools from reference content, validate
them using illustrative examples, and organize them hierarchically into a
toolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to
select and apply the appropriate tools to solve problems. Experiments on
causality, physics, and chemistry benchmarks demonstrate that RefTool
outperforms existing tool-creation and domain-specific reasoning methods by
11.3% on average accuracy, while being cost-efficient and broadly
generalizable. Analyses reveal that grounding tool creation in references
produces accurate and faithful tools, and that the hierarchical structure
facilitates effective tool selection. RefTool enables LLMs to overcome
knowledge limitations, demonstrating the value of grounding tool creation in
external references for enhanced and generalizable reasoning.

</details>


### [134] [Towards Better Instruction Following Retrieval Models](https://arxiv.org/abs/2505.21439)
*Yuchen Zhuang,Aaron Trinh,Rushi Qiang,Haotian Sun,Chao Zhang,Hanjun Dai,Bo Dai*

Key words: 信息检索,指令跟随,对比学习,注意力机制,嵌入模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了InF-IR数据集和InF-Embed模型，旨在提升检索模型在遵循用户指令方面的能力。通过大规模高质量的<指令,查询,段落>三元组训练，模型在五项基准测试中表现出色，提升了8.1%的p-MRR指标。

Motivation: 现有的信息检索模型在理解并遵循用户显式指令方面表现不佳，亟需一种能增强模型指令跟随能力的方法。

Method: 作者构建了InF-IR数据集，包含38,000多个高质量三元组（含硬负样本），并利用对比学习和指令-查询注意力机制训练了InF-Embed模型。

Result: InF-Embed在五项指令检索基准测试中显著优于基线模型，p-MRR指标提高了8.1%。

Conclusion: 通过InF-IR数据集和InF-Embed模型，指令跟随的信息检索能力得到了显著提升。

Abstract: Modern information retrieval (IR) models, trained exclusively on standard
<query, passage> pairs, struggle to effectively interpret and follow explicit
user instructions. We introduce InF-IR, a large-scale, high-quality training
corpus tailored for enhancing retrieval models in Instruction-Following IR.
InF-IR expands traditional training pairs into over 38,000 expressive
<instruction, query, passage> triplets as positive samples. In particular, for
each positive triplet, we generate two additional hard negative examples by
poisoning both instructions and queries, then rigorously validated by an
advanced reasoning model (o3-mini) to ensure semantic plausibility while
maintaining instructional incorrectness. Unlike existing corpora that primarily
support computationally intensive reranking tasks for decoder-only language
models, the highly contrastive positive-negative triplets in InF-IR further
enable efficient representation learning for smaller encoder-only models,
facilitating direct embedding-based retrieval. Using this corpus, we train
InF-Embed, an instruction-aware Embedding model optimized through contrastive
learning and instruction-query attention mechanisms to align retrieval outcomes
precisely with user intents. Extensive experiments across five
instruction-based retrieval benchmarks demonstrate that InF-Embed significantly
surpasses competitive baselines by 8.1% in p-MRR, measuring the
instruction-following capabilities.

</details>


### [135] [Words Like Knives: Backstory-Personalized Modeling and Detection of Violent Communication](https://arxiv.org/abs/2505.21451)
*Jocelyn Shen,Akhila Yerukola,Xuhui Zhou,Cynthia Breazeal,Maarten Sap,Hae Won Park*

Key words: 对话冲突检测,关系动态,非暴力沟通,语料库,LLM

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了关系背景对对话冲突检测的影响，提出了PersonaConflicts语料库，并发现模型在利用关系背景方面表现不佳。

Motivation: 现有NLP研究将冲突检测视为通用任务，忽视了关系动态对信息感知的影响，需要更个性化的方法。

Method: 利用非暴力沟通（NVC）理论，构建PersonaConflicts语料库（5,772条模拟对话），并通过人工标注评估关系和冲突感知。

Result: 关系背景显著影响人类对冲突的感知，但模型难以有效利用这些背景，且高估消息的积极影响。

Conclusion: 个性化关系背景对LLM作为沟通调解工具至关重要，需进一步改进模型能力。

Abstract: Conversational breakdowns in close relationships are deeply shaped by
personal histories and emotional context, yet most NLP research treats conflict
detection as a general task, overlooking the relational dynamics that influence
how messages are perceived. In this work, we leverage nonviolent communication
(NVC) theory to evaluate LLMs in detecting conversational breakdowns and
assessing how relationship backstory influences both human and model perception
of conflicts. Given the sensitivity and scarcity of real-world datasets
featuring conflict between familiar social partners with rich personal
backstories, we contribute the PersonaConflicts Corpus, a dataset of N=5,772
naturalistic simulated dialogues spanning diverse conflict scenarios between
friends, family members, and romantic partners. Through a controlled human
study, we annotate a subset of dialogues and obtain fine-grained labels of
communication breakdown types on individual turns, and assess the impact of
backstory on human and model perception of conflict in conversation. We find
that the polarity of relationship backstories significantly shifted human
perception of communication breakdowns and impressions of the social partners,
yet models struggle to meaningfully leverage those backstories in the detection
task. Additionally, we find that models consistently overestimate how
positively a message will make a listener feel. Our findings underscore the
critical role of personalization to relationship contexts in enabling LLMs to
serve as effective mediators in human communication for authentic connection.

</details>


### [136] [Do LLMs Need to Think in One Language? Correlation between Latent Language and Task Performance](https://arxiv.org/abs/2505.21458)
*Shintaro Ozaki,Tatsuya Hiraoka,Hiroto Otake,Hiroki Ouchi,Masaru Isonuma,Benjamin Heinzerling,Kentaro Inui,Taro Watanabe,Yusuke Miyao,Yohei Oseki,Yu Takagi*

Key words: 大语言模型,潜在语言,下游任务,翻译任务,文化地理任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究发现LLMs内部潜在语言的输入输出语言差异对下游任务表现影响有限，因为模型在最终层会调整表示以适应目标语言。

Motivation: 探索潜在语言与输入输出语言差异对LLMs下游任务表现的影响，验证潜在语言一致性是否提升任务性能。

Method: 通过在不同下游任务中变化输入提示语言，分析潜在语言一致性与任务性能的相关性，构建多样化数据集进行实验。

Result: 实验表明，潜在语言一致性并非始终对翻译和文化地理任务的最优表现至关重要。

Conclusion: LLMs在最终层自适应调整内部表示，潜在语言一致性对整体性能影响有限。

Abstract: Large Language Models (LLMs) are known to process information using a
proficient internal language consistently, referred to as latent language,
which may differ from the input or output languages. However, how the
discrepancy between the latent language and the input and output language
affects downstream task performance remains largely unexplored. While many
studies research the latent language of LLMs, few address its importance in
influencing task performance. In our study, we hypothesize that thinking in
latent language consistently enhances downstream task performance. To validate
this, our work varies the input prompt languages across multiple downstream
tasks and analyzes the correlation between consistency in latent language and
task performance. We create datasets consisting of questions from diverse
domains such as translation and geo-culture, which are influenced by the choice
of latent language. Experimental results across multiple LLMs on translation
and geo-culture tasks, which are sensitive to the choice of language, indicate
that maintaining consistency in latent language is not always necessary for
optimal downstream task performance. This is because these models adapt their
internal representations near the final layers to match the target language,
reducing the impact of consistency on overall performance.

</details>


### [137] [Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion](https://arxiv.org/abs/2505.21467)
*Zhanqiu Hu,Jian Meng,Yash Akhauri,Mohamed S. Abdelfattah,Jae-sun Seo,Zhiru Zhang,Udit Gupta*

Key words: 扩散语言模型, FreeCache, Guided Diffusion, 推理加速, KV缓存

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了两种无需训练的技术（FreeCache和Guided Diffusion），显著提升了扩散语言模型的推理效率，解决了其因迭代去噪导致的高计算成本和延迟问题，同时保持了模型质量。

Motivation: 扩散语言模型虽具有并行生成和双向性优势，但其迭代去噪过程导致推理速度慢、计算成本高，且在并行生成时存在token不一致性问题。本文旨在解决这些问题。

Method: 提出FreeCache（基于KV投影缓存的近似技术）和Guided Diffusion（利用轻量级自回归模型监督去噪过程），以降低计算成本和迭代次数。

Result: 在开源推理基准测试中，结合两种方法实现了最高34倍的端到端加速，且不损失准确性，首次在延迟上媲美甚至超越自回归模型。

Conclusion: 该工作为扩散语言模型的广泛应用铺平了道路，展示了其在高效性和质量上的潜力。

Abstract: Diffusion language models offer parallel token generation and inherent
bidirectionality, promising more efficient and powerful sequence modeling
compared to autoregressive approaches. However, state-of-the-art diffusion
models (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match
the quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,
Llama3 8B), their iterative denoising requires multiple full-sequence forward
passes, resulting in high computational costs and latency, particularly for
long input prompts and long-context scenarios. Furthermore, parallel token
generation introduces token incoherence problems, and current sampling
heuristics suffer from significant quality drops with decreasing denoising
steps. We address these limitations with two training-free techniques. First,
we propose FreeCache, a Key-Value (KV) approximation caching technique that
reuses stable KV projections across denoising steps, effectively reducing the
computational cost of DLM inference. Second, we introduce Guided Diffusion, a
training-free method that uses a lightweight pretrained autoregressive model to
supervise token unmasking, dramatically reducing the total number of denoising
iterations without sacrificing quality. We conduct extensive evaluations on
open-source reasoning benchmarks, and our combined methods deliver up to a 34x
end-to-end speedup without compromising accuracy. For the first time, diffusion
language models achieve a comparable and even faster latency as the widely
adopted autoregressive models. Our work successfully paved the way for scaling
up the diffusion language model to a broader scope of applications across
different domains.

</details>


### [138] [Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration](https://arxiv.org/abs/2505.21471)
*Zijun Liu,Zhennan Wan,Peng Li,Ming Yan,Ji Zhang,Fei Huang,Yang Liu*

Key words: 大语言模型,多智能体,上下文窗口扩展,知识整合,推理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ExtAgents是一种多智能体框架，旨在解决大语言模型（LLM）在处理大量外部知识时的上下文窗口限制问题，显著提升了性能而不需要更长的上下文训练。

Motivation: 随着后训练技术的发展，LLM能整合大量检索知识解决复杂任务，但有限的上下文窗口阻碍了外部知识输入的扩展，影响了性能提升。

Method: 提出多智能体框架ExtAgents，通过分布式处理知识同步与推理过程，解决了现有方法的两个核心瓶颈问题。

Result: 在增强的多跳问答测试∞Bench+和其他公共测试集上，ExtAgents显著优于现有非训练方法，且保持高效率。

Conclusion: ExtAgents为LLM处理超出上下文窗口的外部知识提供了有效解决方案，未来研究可进一步优化智能体协调。

Abstract: With the rapid advancement of post-training techniques for reasoning and
information seeking, large language models (LLMs) can incorporate a large
quantity of retrieved knowledge to solve complex tasks. However, the limited
context window of LLMs obstructs scaling the amount of external knowledge
input, prohibiting further improvement, especially for tasks requiring
significant amount of external knowledge. Existing context window extension
methods inevitably cause information loss. LLM-based multi-agent methods emerge
as a new paradigm to handle massive input in a distributional manner, where we
identify two core bottlenecks in existing knowledge synchronization and
reasoning processes. In this work, we develop a multi-agent framework,
$\textbf{ExtAgents}$, to overcome the bottlenecks and enable better scalability
in inference-time knowledge integration without longer-context training.
Benchmarked with our enhanced multi-hop question answering test,
$\textbf{$\boldsymbol{\infty}$Bench+}$, and other public test sets including
long survey generation, ExtAgents significantly enhances the performance over
existing non-training methods with the same amount of external knowledge input,
regardless of whether it falls $\textit{within or exceeds the context window}$.
Moreover, the method maintains high efficiency due to high parallelism. Further
study in the coordination of LLM agents on increasing external knowledge input
could benefit real-world applications.

</details>


### [139] [Are Language Models Consequentialist or Deontological Moral Reasoners?](https://arxiv.org/abs/2505.21479)
*Keenan Samway,Max Kleiman-Weiner,David Guzman Piedrahita,Rada Mihalcea,Bernhard Schölkopf,Zhijing Jin*

Key words: AI伦理, 大型语言模型, 道德推理, 结果论, 义务论

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究通过分析大型语言模型（LLM）在600多个电车问题中的道德推理痕迹，揭示了其倾向于义务论（deontology）原理，但事后解释则偏向功利论（consequentialism）。

Motivation: 随着AI系统在医疗、法律和治理等领域的应用增多，理解其在复杂伦理场景中的处理方式变得至关重要。

Method: 研究通过大规模分析LLM的道德推理痕迹，并引入分类法系统评估其基于结果论和义务论的推理模式。

Result: 研究发现LLM的思维链更倾向于基于道德义务的义务论原理，而事后解释则显著转向强调效用的结果论。

Conclusion: 该框架为理解LLM如何处理和表达伦理考量奠定了基础，是LLM在高风险决策中安全部署的重要一步。

Abstract: As AI systems increasingly navigate applications in healthcare, law, and
governance, understanding how they handle ethically complex scenarios becomes
critical. Previous work has mainly examined the moral judgments in large
language models (LLMs), rather than their underlying moral reasoning process.
In contrast, we focus on a large-scale analysis of the moral reasoning traces
provided by LLMs. Furthermore, unlike prior work that attempted to draw
inferences from only a handful of moral dilemmas, our study leverages over 600
distinct trolley problems as probes for revealing the reasoning patterns that
emerge within different LLMs. We introduce and test a taxonomy of moral
rationales to systematically classify reasoning traces according to two main
normative ethical theories: consequentialism and deontology. Our analysis
reveals that LLM chains-of-thought tend to favor deontological principles based
on moral obligations, while post-hoc explanations shift notably toward
consequentialist rationales that emphasize utility. Our framework provides a
foundation for understanding how LLMs process and articulate ethical
considerations, an important step toward safe and interpretable deployment of
LLMs in high-stakes decision-making environments. Our code is available at
https://github.com/keenansamway/moral-lens .

</details>


### [140] [UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents](https://arxiv.org/abs/2505.21496)
*Han Xiao,Guozhi Wang,Yuxiang Chai,Zimu Lu,Weifeng Lin,Hao He,Lue Fan,Liuyang Bian,Rui Hu,Liang Liu,Shuai Ren,Yafei Wen,Xiaoxin Chen,Aojun Zhou,Hongsheng Li*

Key words: GUI agents, self-improving framework, reward model, data generation, trajectory verification

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: UI-Genie introduces a self-improving framework for GUI agents, addressing challenges in trajectory verification and training data scalability using a reward model and self-improving pipeline, achieving state-of-the-art performance.

Motivation: The paper addresses two key challenges in GUI agents: the difficulty of verifying trajectory outcomes and the lack of scalable high-quality training data.

Method: A reward model (UI-Genie-RM) with an image-text interleaved architecture is developed, along with a self-improving pipeline. Data generation strategies include rule-based verification and hard negative mining.

Result: UI-Genie achieves state-of-the-art performance across GUI agent benchmarks through three generations of data-model self-improvement, using generated datasets without manual annotation.

Conclusion: UI-Genie effectively solves GUI agent challenges through its framework and datasets, advancing research with open-sourced implementation.

Abstract: In this paper, we introduce UI-Genie, a self-improving framework addressing
two key challenges in GUI agents: verification of trajectory outcome is
challenging and high-quality training data are not scalable. These challenges
are addressed by a reward model and a self-improving pipeline, respectively.
The reward model, UI-Genie-RM, features an image-text interleaved architecture
that efficiently pro- cesses historical context and unifies action-level and
task-level rewards. To sup- port the training of UI-Genie-RM, we develop
deliberately-designed data genera- tion strategies including rule-based
verification, controlled trajectory corruption, and hard negative mining. To
address the second challenge, a self-improvement pipeline progressively expands
solvable complex GUI tasks by enhancing both the agent and reward models
through reward-guided exploration and outcome verification in dynamic
environments. For training the model, we generate UI- Genie-RM-517k and
UI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI
agents while demonstrating high-quality synthetic trajectory gen- eration
without manual annotation. Experimental results show that UI-Genie achieves
state-of-the-art performance across multiple GUI agent benchmarks with three
generations of data-model self-improvement. We open-source our complete
framework implementation and generated datasets to facilitate further research
in https://github.com/Euphoria16/UI-Genie.

</details>


### [141] [Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making](https://arxiv.org/abs/2505.21503)
*Yihan Wang,Qiao Yan,Zhenghao Xing,Lihao Liu,Junjun He,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Key words: 大语言模型, 临床问答, Silent Agreement, Catfish Agent, 多智能体框架

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一个称为Catfish Agent的概念，旨在通过结构化的异议来解决多智能体框架中的Silent Agreement问题，从而提升临床问答的准确性。

Motivation: 多智能体框架在临床问答中表现出诊断准确性的提升，但存在Silent Agreement问题，即智能体在复杂或模糊案例中过早达成共识，缺乏深入分析。

Method: 设计了Catfish Agent，一种角色专门化的LLM，通过两种机制注入结构化异议：复杂度感知干预和语调校准干预。

Result: 在九个医学Q&A和三个医学VQA基准测试中，该方法表现优于单智能体和多智能体LLM框架，包括领先的商业模型如GPT-4o和DeepSeek-R1。

Conclusion: Catfish Agent通过结构化异议有效解决了Silent Agreement问题，提升了多智能体框架在临床问答中的表现。

Abstract: Large language models (LLMs) have demonstrated strong potential in clinical
question answering, with recent multi-agent frameworks further improving
diagnostic accuracy via collaborative reasoning. However, we identify a
recurring issue of Silent Agreement, where agents prematurely converge on
diagnoses without sufficient critical analysis, particularly in complex or
ambiguous cases. We present a new concept called Catfish Agent, a
role-specialized LLM designed to inject structured dissent and counter silent
agreement. Inspired by the ``catfish effect'' in organizational psychology, the
Catfish Agent is designed to challenge emerging consensus to stimulate deeper
reasoning. We formulate two mechanisms to encourage effective and context-aware
interventions: (i) a complexity-aware intervention that modulates agent
engagement based on case difficulty, and (ii) a tone-calibrated intervention
articulated to balance critique and collaboration. Evaluations on nine medical
Q&A and three medical VQA benchmarks show that our approach consistently
outperforms both single- and multi-agent LLMs frameworks, including leading
commercial models such as GPT-4o and DeepSeek-R1.

</details>


### [142] [How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective](https://arxiv.org/abs/2505.21505)
*Shimao Zhang,Zhejian Lai,Xiang Liu,Shuaijie She,Xiao Liu,Yeyun Gong,Shujian Huang,Jiajun Chen*

Key words: 多语言对齐, 语言神经元, 细粒度分析, LLMs机制, 自发对齐

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究提出了一种细粒度的神经元识别算法，分析LLMs在多语言场景下的工作机制，并将内部推理过程分为四个部分，还研究了对齐前后的模型表现。

Motivation: 多语言对齐是增强LLMs多语言能力的有效范式，但目前缺乏对神经元层面的具体分析。作者希望通过识别语言神经元和语言无关神经元，更细致地理解LLMs在多语言任务中的机制。

Method: 提出新的细粒度神经元识别算法，检测语言神经元（包括语言特定和语言相关神经元）和语言无关神经元，并基于神经元分布特性将LLMs的多语言推理过程分为四个部分。

Result: 系统分析了对齐前后的模型表现，发现并研究了“自发多语言对齐”现象，为理解LLMs的多语言能力提供了实证数据和见解。

Conclusion: 该研究通过神经元层面的分析，为多语言对齐和LLMs多语言能力的机制提供了新的视角和实证支持。

Abstract: Multilingual Alignment is an effective and representative paradigm to enhance
LLMs' multilingual capabilities, which transfers the capabilities from the
high-resource languages to the low-resource languages. Meanwhile, some
researches on language-specific neurons reveal that there are language-specific
neurons that are selectively activated in LLMs when processing different
languages. This provides a new perspective to analyze and understand LLMs'
mechanisms more specifically in multilingual scenarios. In this work, we
propose a new finer-grained neuron identification algorithm, which detects
language neurons~(including language-specific neurons and language-related
neurons) and language-agnostic neurons. Furthermore, based on the
distributional characteristics of different types of neurons, we divide the
LLMs' internal process for multilingual inference into four parts: (1)
multilingual understanding, (2) shared semantic space reasoning, (3)
multilingual output space transformation, and (4) vocabulary space outputting.
Additionally, we systematically analyze the models before and after alignment
with a focus on different types of neurons. We also analyze the phenomenon of
''Spontaneous Multilingual Alignment''. Overall, our work conducts a
comprehensive investigation based on different types of neurons, providing
empirical results and valuable insights for better understanding multilingual
alignment and multilingual capabilities of LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [143] [FMEnets: Flow, Material, and Energy networks for non-ideal plug flow reactor design](https://arxiv.org/abs/2505.20300)
*Chenxi Wu,Juan Diego Toscano,Khemraj Shukla,Yingjie Chen,Ali Shahmohammadi,Edward Raymond,Thomas Toupy,Neda Nazemifard,Charles Papageorgiou,George Em Karniadakis*

Key words: FMEnets、物理信息机器学习、活塞流反应器、多尺度网络、Kolmogorov-Arnold网络

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FMEnets: 一种物理信息机器学习框架，用于非理想活塞流反应器的设计与分析，结合Navier-Stokes方程、物质平衡和能量平衡，支持正反问题求解，误差低于2.5%。

Motivation: 设计一种能整合物理方程与机器学习的框架，解决非理想活塞流反应器的复杂设计与分析问题。

Method: FMEnets框架由三个互联子网络组成，支持正向（预测）和逆向（推断）求解；采用FME-PINNs或FME-KANs实现，后者对噪声更鲁棒。

Result: 在三种反应场景中验证，相对误差<2.5%；FME-KANs抗噪性优于FME-PINNs，无噪声时两者性能相当。

Conclusion: FMEnets为反应器设计提供了高效计算工具，并能整合实验数据与物理方程，开辟新研究方向。

Abstract: We propose FMEnets, a physics-informed machine learning framework for the
design and analysis of non-ideal plug flow reactors. FMEnets integrates the
fundamental governing equations (Navier-Stokes for fluid flow, material balance
for reactive species transport, and energy balance for temperature
distribution) into a unified multi-scale network model. The framework is
composed of three interconnected sub-networks with independent optimizers that
enable both forward and inverse problem-solving. In the forward mode, FMEnets
predicts velocity, pressure, species concentrations, and temperature profiles
using only inlet and outlet information. In the inverse mode, FMEnets utilizes
sparse multi-residence-time measurements to simultaneously infer unknown
kinetic parameters and states. FMEnets can be implemented either as FME-PINNs,
which employ conventional multilayer perceptrons, or as FME-KANs, based on
Kolmogorov-Arnold Networks. Comprehensive ablation studies highlight the
critical role of the FMEnets architecture in achieving accurate predictions.
Specifically, FME-KANs are more robust to noise than FME-PINNs, although both
representations are comparable in accuracy and speed in noise-free conditions.
The proposed framework is applied to three different sets of reaction scenarios
and is compared with finite element simulations. FMEnets effectively captures
the complex interactions, achieving relative errors less than 2.5% for the
unknown kinetic parameters. The new network framework not only provides a
computationally efficient alternative for reactor design and optimization, but
also opens new avenues for integrating empirical correlations, limited and
noisy experimental data, and fundamental physical equations to guide reactor
design.

</details>


### [144] [Joint-stochastic-approximation Random Fields with Application to Semi-supervised Learning](https://arxiv.org/abs/2505.20330)
*Yunfu Song,Zhijian Ou*

Key words: 深度生成模型,半监督学习,JRFs,分类,生成

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了半监督学习中深度生成模型的问题，提出了JRFs算法以平衡模式覆盖与丢失，并在多个数据集上取得良好效果。

Motivation: 针对GANs和VAEs在半监督学习中存在的模式丢失与分类生成冲突问题进行研究，提出改进方案。

Method: 提出了联合随机近似随机场（JRFs）算法，构建深度无向生成模型。

Result: 在MNIST、SVHN和CIFAR-10数据集上，JRFs在分类和生成任务中表现优异。

Conclusion: JRFs能有效平衡模式覆盖与丢失，同时在分类与生成任务中表现良好。

Abstract: Our examination of deep generative models (DGMs) developed for
semi-supervised learning (SSL), mainly GANs and VAEs, reveals two problems.
First, mode missing and mode covering phenomenons are observed in genertion
with GANs and VAEs. Second, there exists an awkward conflict between good
classification and good generation in SSL by employing directed generative
models. To address these problems, we formally present
joint-stochastic-approximation random fields (JRFs) -- a new family of
algorithms for building deep undirected generative models, with application to
SSL. It is found through synthetic experiments that JRFs work well in balancing
mode covering and mode missing, and match the empirical data distribution well.
Empirically, JRFs achieve good classification results comparable to the
state-of-art methods on widely adopted datasets -- MNIST, SVHN, and CIFAR-10 in
SSL, and simultaneously perform good generation.

</details>


### [145] [PDFBench: A Benchmark for De novo Protein Design from Function](https://arxiv.org/abs/2505.20346)
*Jiahao Kuang,Nuowei Liu,Changzhi Sun,Tao Ji,Yuanbin Wu*

Key words: de novo protein design, benchmark, evaluation metrics, function-driven design, PDFBench

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: PDFBench is introduced as the first comprehensive benchmark for de novo protein design evaluation, addressing limitations in current methods by offering a fair, multifaceted assessment framework with 22 metrics.

Motivation: Current methods in de novo protein design rely on fragmented metrics and proprietary datasets, hindering fair comparisons and comprehensive evaluation. PDFBench aims to standardize assessment for function-driven design.

Method: PDFBench supports two tasks: description-guided and keyword-guided design. It compiles 22 metrics covering sequence plausibility, structural fidelity, language-protein alignment, novelty, and diversity.

Result: Five state-of-the-art baselines were evaluated, revealing their strengths and weaknesses across tasks. Inter-metric correlations were analyzed to guide metric selection.

Conclusion: PDFBench provides a unified framework for future advances in function-driven de novo protein design by ensuring fair and comprehensive evaluation.

Abstract: In recent years, while natural language processing and multimodal learning
have seen rapid advancements, the field of de novo protein design has also
experienced significant growth. However, most current methods rely on
proprietary datasets and evaluation rubrics, making fair comparisons between
different approaches challenging. Moreover, these methods often employ
evaluation metrics that capture only a subset of the desired properties of
designed proteins, lacking a comprehensive assessment framework. To address
these, we introduce PDFBench, the first comprehensive benchmark for evaluating
de novo protein design from function. PDFBench supports two tasks:
description-guided design and keyword-guided design. To ensure fair and
multifaceted evaluation, we compile 22 metrics covering sequence plausibility,
structural fidelity, and language-protein alignment, along with measures of
novelty and diversity. We evaluate five state-of-the-art baselines, revealing
their respective strengths and weaknesses across tasks. Finally, we analyze
inter-metric correlations, exploring the relationships between four categories
of metrics, and offering guidelines for metric selection. PDFBench establishes
a unified framework to drive future advances in function-driven de novo protein
design.

</details>


### [146] [Decision Flow Policy Optimization](https://arxiv.org/abs/2505.20350)
*Jifeng Hu,Sili Huang,Siyuan Guo,Zhaogeng Liu,Li Shen,Lichao Sun,Hechang Chen,Yi Chang,Dacheng Tao*

Key words: 生成模型,强化学习,多模态动作分布,流动模型,决策流

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 生成模型在强化学习中的应用，提出了一种统一框架Decision Flow，同时优化多模态动作分布建模与策略优化，提升机器人控制性能。

Motivation: 传统方法将生成模型与策略优化分离，限制了多模态分布拟合与策略改进的协同优化，影响了模型训练和性能。

Method: 提出Decision Flow框架，将基于流的动作生成过程建模为流决策过程，实现多模态动作分布与策略优化的统一优化。

Result: 在数十个离线RL环境中验证了方法的有效性，与现有基线相比达到或匹配SOTA性能。

Conclusion: Decision Flow通过统一优化框架解决了传统方法的分离问题，显著提升了生成模型在RL中的性能。

Abstract: In recent years, generative models have shown remarkable capabilities across
diverse fields, including images, videos, language, and decision-making. By
applying powerful generative models such as flow-based models to reinforcement
learning, we can effectively model complex multi-modal action distributions and
achieve superior robotic control in continuous action spaces, surpassing the
limitations of single-modal action distributions with traditional
Gaussian-based policies. Previous methods usually adopt the generative models
as behavior models to fit state-conditioned action distributions from datasets,
with policy optimization conducted separately through additional policies using
value-based sample weighting or gradient-based updates. However, this
separation prevents the simultaneous optimization of multi-modal distribution
fitting and policy improvement, ultimately hindering the training of models and
degrading the performance. To address this issue, we propose Decision Flow, a
unified framework that integrates multi-modal action distribution modeling and
policy optimization. Specifically, our method formulates the action generation
procedure of flow-based models as a flow decision-making process, where each
action generation step corresponds to one flow decision. Consequently, our
method seamlessly optimizes the flow policy while capturing multi-modal action
distributions. We provide rigorous proofs of Decision Flow and validate the
effectiveness through extensive experiments across dozens of offline RL
environments. Compared with established offline RL baselines, the results
demonstrate that our method achieves or matches the SOTA performance.

</details>


### [147] [FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation](https://arxiv.org/abs/2505.20353)
*Dong Liu,Jiayi Zhang,Yifan Li,Yanxuan Yu,Ben Lengerich,Ying Nian Wu*

Key words: Diffusion Transformers, FastCache, 生成模型, 缓存压缩, 加速推理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FastCache通过空间感知令牌选择机制和变换器级缓存减少DiT的计算开销，同时保持生成质量，实现延迟和内存使用的显著降低。

Motivation: Diffusion Transformers (DiT) 的计算密集型特性限制了其应用效率，FastCache旨在通过利用模型内部表示的冗余性来加速推理。

Method: FastCache采用双策略：空间感知令牌选择机制和变换器级缓存，通过可学习线性近似减少冗余计算。

Result: FastCache在多种DiT变体上实现了延迟和内存使用的显著降低，且在生成质量上优于其他缓存方法。

Conclusion: FastCache通过智能缓存和压缩技术有效提升DiT推理效率，同时保证了生成质量的理论与实践双重验证。

Abstract: Diffusion Transformers (DiT) are powerful generative models but remain
computationally intensive due to their iterative structure and deep transformer
stacks. To alleviate this inefficiency, we propose FastCache, a
hidden-state-level caching and compression framework that accelerates DiT
inference by exploiting redundancy within the model's internal representations.
FastCache introduces a dual strategy: (1) a spatial-aware token selection
mechanism that adaptively filters redundant tokens based on hidden state
saliency, and (2) a transformer-level cache that reuses latent activations
across timesteps when changes are statistically insignificant. These modules
work jointly to reduce unnecessary computation while preserving generation
fidelity through learnable linear approximation. Theoretical analysis shows
that FastCache maintains bounded approximation error under a
hypothesis-testing-based decision rule. Empirical evaluations across multiple
DiT variants demonstrate substantial reductions in latency and memory usage,
with best generation output quality compared to other cache methods, as
measured by FID and t-FID. Code implementation of FastCache is available on
GitHub at https://github.com/NoakLiu/FastCache-xDiT.

</details>


### [148] [GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2505.20355)
*Yeonjoon Jung,Daehyun Ahn,Hyungjun Kim,Taesu Kim,Eunhyeok Park*

Key words: LoRA, GraLoRA, 参数高效微调, 过拟合, 梯度纠缠

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: GraLoRA (Granular Low-Rank Adaptation) 通过将权重矩阵分割为子块并引入独立低秩适配器，解决了 LoRA 在参数高效微调中的过拟合问题，显著提升了性能并更接近全微调的效果。

Motivation: LoRA在生成模型的高效微调中表现优秀，但在较宽瓶颈时会出现过拟合问题，且在高秩设置下性能停滞甚至下降。作者发现这种问题源于结构瓶颈导致的梯度纠缠和传播失真。

Method: 提出了 GraLoRA，将权重矩阵划分为子块，每个子块配备独立的低秩适配器。这种方法在不增加计算或存储成本的情况下，缓解了 LoRA 的限制并提升了模型表示能力。

Result: 在代码生成和常识推理基准测试中，GraLoRA 在不同模型大小和秩设置下均显著优于 LoRA 和其他基线方法，HumanEval+ 的 Pass@1 指标提升了 +8.5%。

Conclusion: GraLoRA 是一种可扩展且鲁棒的参数高效微调解决方案，能更接近全微调性能，适用于各种场景。

Abstract: Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient
fine-tuning (PEFT) of generative models, valued for its simplicity and
effectiveness. Despite recent enhancements, LoRA still suffers from a
fundamental limitation: overfitting when the bottleneck is widened. It performs
best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks,
still falling short of full fine-tuning (FFT) performance. We identify the root
cause as LoRA's structural bottleneck, which introduces gradient entanglement
to the unrelated input channels and distorts gradient propagation. To address
this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA)
that partitions weight matrices into sub-blocks, each with its own low-rank
adapter. With negligible computational or storage cost, GraLoRA overcomes
LoRA's limitations, effectively increases the representational capacity, and
more closely approximates FFT behavior. Experiments on code generation and
commonsense reasoning benchmarks show that GraLoRA consistently outperforms
LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on
HumanEval+. These improvements hold across model sizes and rank settings,
making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts
are available at https://github.com/SqueezeBits/GraLoRA.git

</details>


### [149] [Learning and Interpreting Gravitational-Wave Features from CNNs with a Random Forest Approach](https://arxiv.org/abs/2505.20357)
*Jun Tian,He Wang,Jibo He,Yu Pan,Shuo Cao,Qingquan Jiang*

Key words: 卷积神经网络,随机森林,引力波检测,可解释性,物理特征

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种结合CNN和随机森林的混合架构，通过引入物理可解释指标（如方差、信噪比等）提升引力波检测性能和模型可解释性，相比基线CNN模型敏感性提升21%。

Motivation: 现有CNN在引力波检测中缺乏对学习特征的物理解释，限制了模型的可解释性。需结合物理指标提升性能与理解。

Method: 混合CNN特征提取器与随机森林分类器，引入四种物理可解释指标（方差、信噪比、波形重叠、峰值振幅）辅助分类。

Result: 在长期应变数据测试中，混合模型敏感性提升21%，低信噪比信号检测显著改善；RF分析显示CNN特征与手工特征均贡献显著。

Conclusion: 物理驱动的CNN特征后处理可提升检测效率与解释性，弥合深度学习与领域知识的差距。

Abstract: Convolutional neural networks (CNNs) have become widely adopted in
gravitational wave (GW) detection pipelines due to their ability to
automatically learn hierarchical features from raw strain data. However, the
physical meaning of these learned features remains underexplored, limiting the
interpretability of such models. In this work, we propose a hybrid architecture
that combines a CNN-based feature extractor with a random forest (RF)
classifier to improve both detection performance and interpretability. Unlike
prior approaches that directly connect classifiers to CNN outputs, our method
introduces four physically interpretable metrics - variance, signal-to-noise
ratio (SNR), waveform overlap, and peak amplitude - computed from the final
convolutional layer. These are jointly used with the CNN output in the RF
classifier to enable more informed decision boundaries. Tested on long-duration
strain datasets, our hybrid model outperforms a baseline CNN model, achieving a
relative improvement of 21\% in sensitivity at a fixed false alarm rate of 10
events per month. Notably, it also shows improved detection of low-SNR signals
(SNR $\le$ 10), which are especially vulnerable to misclassification in noisy
environments. Feature attribution via the RF model reveals that both
CNN-extracted and handcrafted features contribute significantly to
classification decisions, with learned variance and CNN outputs ranked among
the most informative. These findings suggest that physically motivated
post-processing of CNN feature maps can serve as a valuable tool for
interpretable and efficient GW detection, bridging the gap between deep
learning and domain knowledge.

</details>


### [150] [Risk-aware Direct Preference Optimization under Nested Risk Measure](https://arxiv.org/abs/2505.20359)
*Lijun Zhang,Lin Li,Yajie Qi,Huizhong Song,Yaodong Yang,Jun Wang,Wei Wei*

Key words: 大型语言模型, 直接偏好优化, 风险感知, 模型对齐, 风险控制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新方法Ra-DPO，通过引入风险意识来优化大型语言模型（LLM）的微调过程，旨在平衡性能与模型偏移风险。

Motivation: 现有方法通过KL散度约束微调模型与参考模型的偏差，但在需要严格风险控制的应用中可能不足。Ra-DPO旨在增强风险意识，提升对齐性能同时控制模型偏移。

Method: 采用风险感知直接偏好优化（Ra-DPO），引入嵌套风险度量，构建约束风险感知优势函数最大化问题，并将Bradley-Terry模型转换为词级表示。通过序列风险比抑制模型偏差。

Result: 在IMDb、Anthropic HH和AlpacaEval三个开源数据集上的实验表明，Ra-DPO在平衡对齐性能和模型偏移方面表现优异。

Conclusion: Ra-DPO通过风险感知机制有效提升了LLM微调的安全性和性能，为需要严格风险控制的应用提供了新思路。

Abstract: When fine-tuning pre-trained Large Language Models (LLMs) to align with human
values and intentions, maximizing the estimated reward can lead to superior
performance, but it also introduces potential risks due to deviations from the
reference model's intended behavior. Most existing methods typically introduce
KL divergence to constrain deviations between the trained model and the
reference model; however, this may not be sufficient in certain applications
that require tight risk control. In this paper, we introduce Risk-aware Direct
Preference Optimization (Ra-DPO), a novel approach that incorporates
risk-awareness by employing a class of nested risk measures. This approach
formulates a constrained risk-aware advantage function maximization problem and
then converts the Bradley-Terry model into a token-level representation. The
objective function maximizes the likelihood of the policy while suppressing the
deviation between a trained model and the reference model using a sequential
risk ratio, thereby enhancing the model's risk-awareness. Experimental results
across three open-source datasets: IMDb Dataset, Anthropic HH Dataset, and
AlpacaEval, demonstrate the proposed method's superior performance in balancing
alignment performance and model drift. Our code is opensourced at
https://github.com/zlj123-max/Ra-DPO.

</details>


### [151] [GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining](https://arxiv.org/abs/2505.20380)
*Simin Fan,Maria Ios Glarou,Martin Jaggi*

Key words: 多目标任务优化, 领域重加权, GRAPE框架, 分布式鲁棒优化, 低资源语言建模

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了GRAPE框架，通过多源多目标领域重加权方法，同时优化预训练数据混合以在多个目标任务上实现稳健性能。实验证明其在6个基准测试和多语言任务中优于基线方法。

Motivation: 现有领域重加权算法通常只优化单个目标任务的数据混合，导致模型在专门目标上过拟合而在其他任务上表现下降。GRAPE旨在解决这一问题，实现多目标任务上的稳健性能。

Method: GRAPE通过动态调整源域的采样权重（域权重）和同时调节目标任务权重，优先考虑学习难度较高的任务。将其建模为极小极大优化问题：内层最大化通过组分布式鲁棒优化（DRO）调整任务权重，外层最小化优化域权重以最大化优先任务的损失减少。

Result: 在ClimbLab和SlimPajama数据集上的实验表明，GRAPE在6个基准测试中的推理性能优于基线方法。在多语言任务中，GRAPE从主流语言中识别出最优训练混合，并在8种低资源语言上实现了更优的语言建模能力。

Conclusion: GRAPE框架通过多目标任务优化和数据混合动态调整，显著提升了模型在多样化下游任务上的稳健性和性能。

Abstract: The performance of large language models (LLMs) across diverse downstream
applications is fundamentally governed by the quality and composition of their
pretraining corpora. Existing domain reweighting algorithms primarily optimize
data mixtures for a single target task, thereby resulting in models that
overfit to specialized objectives while exhibiting substantial performance
degradation on other benchmarks. This paper introduces Group Robust
Multi-target Adaptive PrEtraining (GRAPE), a novel multi-source-multi-target
domain reweighting framework designed to calibrate pretraining data mixtures
for robust performance across multiple target tasks simultaneously. GRAPE
dynamically adjusts sampling weights across source domains (domain weights)
while concurrently modulating task weights that quantify the relative
importance of each individual target task. This adaptive process prioritizes
tasks based on their learning difficulty throughout training. We formulate this
interleaved reweighting mechanism as a minimax optimization problem: The inner
maximization adjusts task weights leveraging group
distributed-robust-optimization (DRO), where those tasks demonstrating the
least improvement under the current data mixture are prioritized with higher
weights; The outer minimization then optimizes domain weights to maximize loss
reduction on the prioritized tasks. Experiments on ClimbLab and SlimPajama
datasets demonstrate that GRAPE consistently outperforms baseline methods in
terms of reasoning performance across 6 benchmarks. Furthermore, when applied
to multilingual targets, GRAPE effectively identifies optimal training mixtures
from mainstream languages, achieving superior language modeling capabilities
across 8 low-resource target languages.

</details>


### [152] [Holes in Latent Space: Topological Signatures Under Adversarial Influence](https://arxiv.org/abs/2505.20435)
*Aideen Fay,Inés García-Redondo,Qiquan Wang,Haim Dubossarsky,Anthea Monod*

Key words: 持久同调, 语言模型, 对抗攻击, 拓扑数据分析, 表征动态

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出使用持久同调（PH）技术分析语言模型在高维激活空间中的全局和局部结构变化，以研究两种对抗攻击模式（后门微调和间接提示注入）对模型的影响。通过分析六种最先进的语言模型，发现对抗条件会压缩潜在拓扑结构，减少小尺度结构多样性，同时放大粗尺度特征。PH技术为解释语言模型的表征动态提供了统一框架。

Motivation: 研究对抗条件下语言模型的表征动态，需要同时捕捉高维激活空间的全局结构和局部细节。传统方法难以兼顾，因此提出使用拓扑数据分析工具PH来解决这一问题。

Method: 采用持久同调（PH）技术，分析六种先进语言模型在两种对抗攻击模式（后门微调和间接提示注入）下的多尺度潜在空间动态，并引入神经元级PH框架量化信息流动和转换。

Result: 研究发现对抗条件会压缩潜在拓扑结构，减少小尺度多样性，放大粗尺度特征。这些拓扑特征在不同层、架构和模型规模中均具有统计稳健性，并与网络中更深层的对抗效应出现一致。

Conclusion: 持久同调（PH）为解释语言模型在对抗条件下的表征动态提供了一种统一且原则性的方法，尤其在分布偏移情况下表现突出。

Abstract: Understanding how adversarial conditions affect language models requires
techniques that capture both global structure and local detail within
high-dimensional activation spaces. We propose persistent homology (PH), a tool
from topological data analysis, to systematically characterize multiscale
latent space dynamics in LLMs under two distinct attack modes -- backdoor
fine-tuning and indirect prompt injection. By analyzing six state-of-the-art
LLMs, we show that adversarial conditions consistently compress latent
topologies, reducing structural diversity at smaller scales while amplifying
dominant features at coarser ones. These topological signatures are
statistically robust across layers, architectures, model sizes, and align with
the emergence of adversarial effects deeper in the network. To capture
finer-grained mechanisms underlying these shifts, we introduce a neuron-level
PH framework that quantifies how information flows and transforms within and
across layers. Together, our findings demonstrate that PH offers a principled
and unifying approach to interpreting representational dynamics in LLMs,
particularly under distributional shift.

</details>


### [153] [HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models](https://arxiv.org/abs/2505.20444)
*Haoran Li,Yingjie Qin,Baoyuan Ou,Lai Xu,Ruiwen Xu*

Key words: 视觉语言模型, 长视频任务, 位置嵌入, HoPE, 动态时间缩放, 多模态学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该文针对视觉语言模型（VLMs）在长视频任务中性能下降的问题，提出了一种混合位置嵌入方法（HoPE），通过混合频率分配策略和动态时间缩放机制，显著提升了模型的长上下文能力。

Motivation: 现有视觉语言模型在长视频任务中性能不佳，尤其是因为传统Rotary Position Embedding（RoPE）难以捕捉复杂的时空依赖关系。当前方法缺乏理论支持，作者希望通过深入研究解决这一问题。

Method: 提出了HoPE方法，结合了混合频率分配策略和动态时间缩放机制，以可靠地建模长上下文语义，并支持灵活的推理。

Result: 在四个视频基准测试中，HoPE在长视频理解和检索任务上表现优于现有方法，验证了其有效性。

Conclusion: HoPE通过改进位置嵌入策略，显著提升了视觉语言模型的长上下文能力，为未来多模态任务提供了新思路。

Abstract: Vision-Language Models (VLMs) have made significant progress in multimodal
tasks. However, their performance often deteriorates in long-context scenarios,
particularly long videos. While Rotary Position Embedding (RoPE) has been
widely adopted for length generalization in Large Language Models (LLMs),
extending vanilla RoPE to capture the intricate spatial-temporal dependencies
in videos remains an unsolved challenge. Existing methods typically allocate
different frequencies within RoPE to encode 3D positional information. However,
these allocation strategies mainly rely on heuristics, lacking in-depth
theoretical analysis. In this paper, we first study how different allocation
strategies impact the long-context capabilities of VLMs. Our analysis reveals
that current multimodal RoPEs fail to reliably capture semantic similarities
over extended contexts. To address this issue, we propose HoPE, a Hybrid of
Position Embedding designed to improve the long-context capabilities of VLMs.
HoPE introduces a hybrid frequency allocation strategy for reliable semantic
modeling over arbitrarily long context, and a dynamic temporal scaling
mechanism to facilitate robust learning and flexible inference across diverse
context lengths. Extensive experiments across four video benchmarks on long
video understanding and retrieval tasks demonstrate that HoPE consistently
outperforms existing methods, confirming its effectiveness. Code is available
at https://github.com/hrlics/HoPE.

</details>


### [154] [Time Series Generation Under Data Scarcity: A Unified Generative Modeling Approach](https://arxiv.org/abs/2505.20446)
*Tal Gonen,Itai Pemper,Ilan Naiman,Nimrod Berman,Omri Azencot*

Key words: 时间序列生成,少样本学习,扩散模型,跨领域泛化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究首次大规模评估了数据稀缺条件下生成模型的性能，提出了一种统一的扩散生成框架，能在少量样本下生成高质量时间序列，并在少样本场景中超越了领域特定基线。

Motivation: 填补在数据稀缺条件下对生成模型性能理解的空白。

Method: 使用预训练的扩散生成框架，结合动态卷积层和数据集标记条件。

Result: 模型在少样本和全数据集上都实现了最先进的性能。

Conclusion: 该工作推动了少样本生成建模作为时间序列研究的关键问题，并提倡跨领域的统一解决方案。

Abstract: Generative modeling of time series is a central challenge in time series
analysis, particularly under data-scarce conditions. Despite recent advances in
generative modeling, a comprehensive understanding of how state-of-the-art
generative models perform under limited supervision remains lacking. In this
work, we conduct the first large-scale study evaluating leading generative
models in data-scarce settings, revealing a substantial performance gap between
full-data and data-scarce regimes. To close this gap, we propose a unified
diffusion-based generative framework that can synthesize high-fidelity time
series across diverse domains using just a few examples. Our model is
pre-trained on a large, heterogeneous collection of time series datasets,
enabling it to learn generalizable temporal representations. It further
incorporates architectural innovations such as dynamic convolutional layers for
flexible channel adaptation and dataset token conditioning for domain-aware
generation. Without requiring abundant supervision, our unified model achieves
state-of-the-art performance in few-shot settings-outperforming domain-specific
baselines across a wide range of subset sizes. Remarkably, it also surpasses
all baselines even when tested on full datasets benchmarks, highlighting the
strength of pre-training and cross-domain generalization. We hope this work
encourages the community to revisit few-shot generative modeling as a key
problem in time series research and pursue unified solutions that scale
efficiently across domains. Code is available at
https://github.com/azencot-group/ImagenFew.

</details>


### [155] [Active Learning for Multiple Change Point Detection in Non-stationary Time Series with Deep Gaussian Processes](https://arxiv.org/abs/2505.20452)
*Hao Zhao,Rong Pan*

Key words: 多变点检测, 非平稳时间序列, 主动学习, 深度高斯过程, 光谱分析

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种结合主动学习（AL）和深度高斯过程（DGPs）的新算法，用于非平稳时间序列的多变点检测，显著提高检测精度和采样效率。

Motivation: 非平稳时间序列中的多变点检测由于模式多样性而具有挑战性，需高效且准确的检测方法。

Method: 结合光谱分析识别潜在变点，利用主动学习策略选择采样点，并整合深度高斯过程的建模灵活性。

Result: 实验表明，该方法在检测精度和采样效率上优于现有技术。

Conclusion: 整合光谱方法和深度高斯过程的算法能有效适应多样化变点行为并准确定位变点。

Abstract: Multiple change point (MCP) detection in non-stationary time series is
challenging due to the variety of underlying patterns. To address these
challenges, we propose a novel algorithm that integrates Active Learning (AL)
with Deep Gaussian Processes (DGPs) for robust MCP detection. Our method
leverages spectral analysis to identify potential changes and employs AL to
strategically select new sampling points for improved efficiency. By
incorporating the modeling flexibility of DGPs with the change-identification
capabilities of spectral methods, our approach adapts to diverse spectral
change behaviors and effectively localizes multiple change points. Experiments
on both simulated and real-world data demonstrate that our method outperforms
existing techniques in terms of detection accuracy and sampling efficiency for
non-stationary time series.

</details>


### [156] [BlastOFormer: Attention and Neural Operator Deep Learning Methods for Explosive Blast Prediction](https://arxiv.org/abs/2505.20454)
*Reid Graves,Anthony Zhou,Amir Barati Farimani*

Key words: BlastOFormer, Transformer, 爆炸压力预测, CFD替代模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: BlastOFormer是一种基于Transformer的替代模型，用于预测爆炸压力场，比传统方法更快速且准确。

Motivation: 传统方法如经验模型和CFD模拟在速度和准确性之间难以平衡，BlastOFormer旨在解决这一问题。

Method: 使用SDF编码和网格到网格注意力架构，结合Transformer框架，训练基于blastFoam CFD求解器的数据集。

Result: BlastOFormer在R2分数（0.9516）和误差指标上优于CNN和FNO，推理速度快60万倍。

Conclusion: BlastOFormer在复杂环境中作为CFD的实时替代方案具有潜力。

Abstract: Accurate prediction of blast pressure fields is essential for applications in
structural safety, defense planning, and hazard mitigation. Traditional methods
such as empirical models and computational fluid dynamics (CFD) simulations
offer limited trade offs between speed and accuracy; empirical models fail to
capture complex interactions in cluttered environments, while CFD simulations
are computationally expensive and time consuming. In this work, we introduce
BlastOFormer, a novel Transformer based surrogate model for full field maximum
pressure prediction from arbitrary obstacle and charge configurations.
BlastOFormer leverages a signed distance function (SDF) encoding and a grid to
grid attention based architecture inspired by OFormer and Vision Transformer
(ViT) frameworks. Trained on a dataset generated using the open source
blastFoam CFD solver, our model outperforms convolutional neural networks
(CNNs) and Fourier Neural Operators (FNOs) across both log transformed and
unscaled domains. Quantitatively, BlastOFormer achieves the highest R2 score
(0.9516) and lowest error metrics, while requiring only 6.4 milliseconds for
inference, more than 600,000 times faster than CFD simulations. Qualitative
visualizations and error analyses further confirm BlastOFormer's superior
spatial coherence and generalization capabilities. These results highlight its
potential as a real time alternative to conventional CFD approaches for blast
pressure estimation in complex environments.

</details>


### [157] [Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data](https://arxiv.org/abs/2505.20485)
*Abhijit Chunduru,Majid Morafah,Mahdi Morafah,Vishnu Pandi Chellapandi,Ang Li*

Key words: 联邦学习,数据异质性,决策边界,遗忘问题,FedProj

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文分析了数据异质性如何影响联邦学习的全局决策边界，发现现有方法存在遗忘问题，并提出FedProj框架，通过服务器端知识转移和记忆机制改进全局边界学习。

Motivation: 数据异质性给联邦学习带来挑战，现有方法缺乏对全局决策边界影响的深入分析。论文旨在填补这一空白，并提出更鲁棒的解决方案。

Method: 提出FedProj框架，包括服务器端知识转移损失和基于公共未标注数据的记忆机制，以减少本地训练中的遗忘问题。

Result: FedProj在实验中显著优于现有方法。

Conclusion: FedProj能有效学习和保持全局决策边界，解决了数据异质性导致的遗忘问题。

Abstract: The inevitable presence of data heterogeneity has made federated learning
very challenging. There are numerous methods to deal with this issue, such as
local regularization, better model fusion techniques, and data sharing. Though
effective, they lack a deep understanding of how data heterogeneity can affect
the global decision boundary. In this paper, we bridge this gap by performing
an experimental analysis of the learned decision boundary using a toy example.
Our observations are surprising: (1) we find that the existing methods suffer
from forgetting and clients forget the global decision boundary and only learn
the perfect local one, and (2) this happens regardless of the initial weights,
and clients forget the global decision boundary even starting from pre-trained
optimal weights. In this paper, we present FedProj, a federated learning
framework that robustly learns the global decision boundary and avoids its
forgetting during local training. To achieve better ensemble knowledge fusion,
we design a novel server-side ensemble knowledge transfer loss to further
calibrate the learned global decision boundary. To alleviate the issue of
learned global decision boundary forgetting, we further propose leveraging an
episodic memory of average ensemble logits on a public unlabeled dataset to
regulate the gradient updates at each step of local training. Experimental
results demonstrate that FedProj outperforms state-of-the-art methods by a
large margin.

</details>


### [158] [Semi-Explicit Neural DAEs: Learning Long-Horizon Dynamical Systems with Algebraic Constraints](https://arxiv.org/abs/2505.20515)
*Avik Pal,Alan Edelman,Christopher Rackauckas*

Key words: 科学机器学习、神经微分方程、微分代数方程、约束投影、物理建模

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了Manifold-Projected Neural ODEs (PNODEs)，一种通过将ODE步骤投影到约束流形上来显式执行代数约束的方法，有效解决了现有方法在复杂守恒物理系统建模中的局限性。

Motivation: 现有的科学机器学习（SciML）方法在结合数据驱动技术和机制性建模时，难以有效处理硬约束，导致可扩展性差和数值性能不佳。因此，论文旨在提出一种能够简单、高效地保证物理一致性的新方法。

Method: PNODEs通过将ODE步骤显式投影到约束流形上来实现，包括鲁棒的迭代变体和快速近似方法。该方法基于半显式微分代数方程（DAEs），并将现有松弛方法视为特例。

Result: PNODEs在六个基准问题中表现优于基线方法，平均约束违反误差低于$10^{-10}$，同时运行时间更短。

Conclusion: 约束投影为学习物理一致的长时程动力学提供了一种简单有效的策略。

Abstract: Despite the promise of scientific machine learning (SciML) in combining
data-driven techniques with mechanistic modeling, existing approaches for
incorporating hard constraints in neural differential equations (NDEs) face
significant limitations. Scalability issues and poor numerical properties
prevent these neural models from being used for modeling physical systems with
complicated conservation laws. We propose Manifold-Projected Neural ODEs
(PNODEs), a method that explicitly enforces algebraic constraints by projecting
each ODE step onto the constraint manifold. This framework arises naturally
from semi-explicit differential-algebraic equations (DAEs), and includes both a
robust iterative variant and a fast approximation requiring a single Jacobian
factorization. We further demonstrate that prior works on relaxation methods
are special cases of our approach. PNODEs consistently outperform baselines
across six benchmark problems achieving a mean constraint violation error below
$10^{-10}$. Additionally, PNODEs consistently achieve lower runtime compared to
other methods for a given level of error tolerance. These results show that
constraint projection offers a simple strategy for learning physically
consistent long-horizon dynamics.

</details>


### [159] [Towards Fully FP8 GEMM LLM Training at Scale](https://arxiv.org/abs/2505.20524)
*Alejandro Hernández-Cano,Dhia Garbaya,Imanol Schlag,Martin Jaggi*

Key words: FP8, LLM, 低精度计算, 预训练, 吞吐量

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种新型FP8计算架构，能在LLM预训练中实现全FP8计算，显著提高吞吐量并保持性能。

Motivation: FP8数据格式在LLM预训练中潜力巨大，但因稳定性问题难以规模化应用。现有方法依赖次优FP8内核或回退高精度计算，限制了性能提升。

Method: 设计支持FP8计算的LLM架构，减少异常激活值并监控低精度训练指标。

Result: 新架构实现了前所未有的吞吐量提升，同时保持与BF16训练相当的下游性能。

Conclusion: 提出的FP8架构为LLM预训练提供高效稳定方案，推进了低精度计算的实用性。

Abstract: Despite the significant potential of FP8 data formats for large language
model (LLM) pre-training, their adoption has been limited due to challenges in
maintaining stability at scale. Existing approaches often rely on suboptimal
fine-grained FP8 kernels or fall back to higher-precision matrix
multiplications (GEMMs) in sensitive components, such as attention projections,
compromising potential throughput gains. We introduce a new class of LLM
architectures that, for the first time, support FP8 computation for all GEMMs
within transformer blocks during both forward and backward passes. This enables
unprecedented throughput gains, particularly at scale, while matching the
downstream performance of standard BF16 training. Our architecture design
reduces large outlier activations, promoting stable long-term FP8 training. In
addition, we identify key metrics to monitor low-precision training and predict
potential future divergences.

</details>


### [160] [One-shot Robust Federated Learning of Independent Component Analysis](https://arxiv.org/abs/2505.20532)
*Dian Jin,Xin Bing,Yuqian Zhang*

Key words: 独立成分分析, 联邦学习, 几何中位数, k均值聚类, 异构数据

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种基于几何中位数和k均值聚类的鲁棒性单次聚合框架，用于解决分布式和联邦独立成分分析（ICA）问题。该方法在高度异构场景中依然有效，并通过仿真验证了其性能。

Motivation: 针对分布式和联邦独立成分分析（ICA）中存在的排列歧义和客户端数据异构性问题，提出一种鲁棒性单次聚合框架，以提升算法在异构环境下的稳定性。

Method: 采用k均值聚类对客户端提供的估计器进行聚类，然后使用几何中位数对每簇内的估计器进行聚合，结合样本分位数分析几何中位数误差界和最大误聚类率。

Result: 理论分析和仿真实验表明，该方法在客户端样本极少或高度异构的情况下仍能保持有效，优于传统方法。

Conclusion: 提出的几何中位数聚合算法是一种高效且鲁棒的解决方案，适用于分布式和联邦ICA问题，尤其在数据异构性强的场景中表现优异。

Abstract: This paper investigates a general robust one-shot aggregation framework for
distributed and federated Independent Component Analysis (ICA) problem. We
propose a geometric median-based aggregation algorithm that leverages $k$-means
clustering to resolve the permutation ambiguity in local client estimations.
Our method first performs k-means to partition client-provided estimators into
clusters and then aggregates estimators within each cluster using the geometric
median. This approach provably remains effective even in highly heterogeneous
scenarios where at most half of the clients can observe only a minimal number
of samples. The key theoretical contribution lies in the combined analysis of
the geometric median's error bound-aided by sample quantiles-and the maximum
misclustering rates of the aforementioned solution of $k$-means. The
effectiveness of the proposed approach is further supported by simulation
studies conducted under various heterogeneous settings.

</details>


### [161] [Rotary Masked Autoencoders are Versatile Learners](https://arxiv.org/abs/2505.20535)
*Uros Zivanovic,Serafina Di Gioia,Andre Scaffidi,Martín de los Rios,Gabriella Contardo,Roberto Trotta*

Key words: Transformer, RoMAE, RoPE, irregular time-series, MAE

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: RoMAE通过结合RoPE方法和MAE架构，避免了传统时间序列模型所需的特殊化调整，同时在多种模态（包括不规则时间序列）上表现出色，甚至超越专门的时间序列架构。

Motivation: 解决传统Transformer在处理不规则时间序列时需要定制化架构的问题，以减少计算开销和模型复杂度。

Method: 提出RoMAE，结合RoPE方法实现对多维连续位置信息的表示学习，避免时间序列特殊化调整。

Result: 在DESC ELAsTiCC等挑战性数据集上超越专门时间序列架构，同时保持其他模态的性能。

Conclusion: RoMAE在保持MAE通用性的同时，有效处理不规则时间序列，但需注意RoPE相对位置属性的潜在问题。

Abstract: Applying Transformers to irregular time-series typically requires
specializations to their baseline architecture, which can result in additional
computational overhead and increased method complexity. We present the Rotary
Masked Autoencoder (RoMAE), which utilizes the popular Rotary Positional
Embedding (RoPE) method for continuous positions. RoMAE is an extension to the
Masked Autoencoder (MAE) that enables representation learning with
multidimensional continuous positional information while avoiding any
time-series-specific architectural specializations. We showcase RoMAE's
performance on a variety of modalities including irregular and multivariate
time-series, images, and audio, demonstrating that RoMAE surpasses specialized
time-series architectures on difficult datasets such as the DESC ELAsTiCC
Challenge while maintaining MAE's usual performance across other modalities. In
addition, we investigate RoMAE's ability to reconstruct the embedded continuous
positions, demonstrating that including learned embeddings in the input
sequence breaks RoPE's relative position property.

</details>


### [162] [A ZeNN architecture to avoid the Gaussian trap](https://arxiv.org/abs/2505.20553)
*Luís Carvalho,João L. Costa,José Mourão,Gonçalo Oliveira*

Key words: ZeNNs, 多层感知机, 谐波分析, 高频学习, 非高斯特性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: ZeNNs是一种新型神经网络架构，通过引入三个谐波分析原则解决了传统MLPs在无限宽度下的局限性，如缺乏点收敛性、丢失非高斯特性及高频学习能力差等问题。

Motivation: 解决多层感知机（MLPs）在无限宽度下的非参数性、缺乏点收敛性、丢失非高斯特性及高频学习能力不足等问题。

Method: 提出ZeNNs架构，基于三个谐波分析原则：1) 引入不可学习权重强制收敛；2) 添加频率缩放因子；3) 选择激活函数以构建接近正交的系统。

Result: ZeNNs在无限宽度下实现点收敛、保留非高斯特性并支持特征学习；有限宽度下在高频特征学习中表现优异。

Conclusion: ZeNNs通过谐波分析原则有效克服了MLPs的核心缺陷，为高频学习和非高斯特征提供了新解决方案。

Abstract: We propose a new simple architecture, Zeta Neural Networks (ZeNNs), in order
to overcome several shortcomings of standard multi-layer perceptrons (MLPs).
Namely, in the large width limit, MLPs are non-parametric, they do not have a
well-defined pointwise limit, they lose non-Gaussian attributes and become
unable to perform feature learning; moreover, finite width MLPs perform poorly
in learning high frequencies. The new ZeNN architecture is inspired by three
simple principles from harmonic analysis:
  i) Enumerate the perceptons and introduce a non-learnable weight to enforce
convergence;
  ii) Introduce a scaling (or frequency) factor;
  iii) Choose activation functions that lead to near orthogonal systems.
  We will show that these ideas allow us to fix the referred shortcomings of
MLPs. In fact, in the infinite width limit, ZeNNs converge pointwise, they
exhibit a rich asymptotic structure beyond Gaussianity, and perform feature
learning. Moreover, when appropriate activation functions are chosen, (finite
width) ZeNNs excel at learning high-frequency features of functions with low
dimensional domains.

</details>


### [163] [Learning a Pessimistic Reward Model in RLHF](https://arxiv.org/abs/2505.20556)
*Yinglun Xu,Hangoo Kang,Tarun Suresh,Yuxuan Wan,Gagandeep Singh*

Key words: 离线强化学习, 奖励模型, 悲观奖励, PET, 奖励黑客

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种悲观奖励微调方法PET，用于防止离线强化学习中的奖励黑客行为，无需依赖正则化即可训练高性能策略。

Motivation: 解决传统奖励建模在强化学习中的奖励黑客问题，避免依赖KL正则化的不足。

Method: 通过悲观奖励微调（PET）优化策略，防止奖励黑客行为，无需正则化。

Result: 在TL;DR数据集上验证，训练的策略具有高KL散度但实际性能优越。

Conclusion: 证明了悲观奖励模型对抗奖励黑客行为的可行性。

Abstract: This work proposes `PET', a novel pessimistic reward fine-tuning method, to
learn a pessimistic reward model robust against reward hacking in offline
reinforcement learning from human feedback (RLHF). Traditional reward modeling
techniques in RLHF train an imperfect reward model, on which a KL
regularization plays a pivotal role in mitigating reward hacking when
optimizing a policy. Such an intuition-based method still suffers from reward
hacking, and the policies with large KL divergence from the dataset
distribution are excluded during learning. In contrast, we show that when
optimizing a policy on a pessimistic reward model fine-tuned through PET,
reward hacking can be prevented without relying on any regularization. We test
our methods on the standard TL;DR summarization dataset. We find that one can
learn a high-quality policy on our pessimistic reward without using any
regularization. Such a policy has a high KL divergence from the dataset
distribution while having high performance in practice. In summary, our work
shows the feasibility of learning a pessimistic reward model against reward
hacking. The agent can greedily search for the policy with a high pessimistic
reward without suffering from reward hacking.

</details>


### [164] [Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning](https://arxiv.org/abs/2505.20561)
*Shenao Zhang,Yaqing Wang,Yinxiao Liu,Tianqi Liu,Peter Grabowski,Eugene Ie,Zhaoran Wang,Yunxuan Li*

Key words: 大语言模型、贝叶斯强化学习、反思推理、探索策略

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究通过贝叶斯自适应强化学习框架（BARL）改进传统的马尔可夫强化学习，提升大语言模型的探索能力和推理效率。

Motivation: 传统马尔可夫强化学习依赖确定性策略，无法解释模型在测试时的反思推理行为。研究旨在探索如何通过贝叶斯方法优化探索与利用的平衡。

Method: 提出BARL算法，结合贝叶斯自适应强化学习框架，动态调整策略，激励信息收集型探索和奖励最大化利用。

Result: BARL在合成任务和数学推理任务中优于传统方法，显著提升token利用率和探索效率。

Conclusion: 贝叶斯框架能有效引导反思性探索，增强大语言模型的推理能力。

Abstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have
exhibited strong reasoning capabilities and emergent reflective behaviors, such
as backtracking and error correction. However, conventional Markovian RL
confines exploration to the training phase to learn an optimal deterministic
policy and depends on the history contexts only through the current state.
Therefore, it remains unclear whether reflective reasoning will emerge during
Markovian RL training, or why they are beneficial at test time. To remedy this,
we recast reflective exploration within the Bayes-Adaptive RL framework, which
explicitly optimizes the expected return under a posterior distribution over
Markov decision processes. This Bayesian formulation inherently incentivizes
both reward-maximizing exploitation and information-gathering exploration via
belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and
switch strategies based on the observed outcomes, offering principled guidance
on when and how the model should reflectively explore. Empirical results on
both synthetic and mathematical reasoning tasks demonstrate that BARL
outperforms standard Markovian RL approaches at test time, achieving superior
token efficiency with improved exploration effectiveness. Our code is available
at https://github.com/shenao-zhang/BARL.

</details>


### [165] [Bi-Level Unsupervised Feature Selection](https://arxiv.org/abs/2505.20563)
*Jingjing Liu,Xiansen Ju,Xianchao Xiu,Wanquan Liu*

Key words: 无监督特征选择, 双层框架, $\)

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种新颖的双层无监督特征选择（BLUFS）方法，通过聚类和特征两个层次，结合谱聚类和$\ell_{2,0}$-范数约束，提高了特征选择和数据结构保留的能力。

Motivation: 传统的无监督特征选择方法通常仅从单一视角建模，难以同时评估特征重要性和保留数据结构，限制了性能。

Method: BLUFS采用双层框架，聚类层用谱聚类生成伪标签表示数据结构，特征层用$\ell_{2,0}$-范数约束投影矩阵以有效选择特征，并设计了一种高效的PAM算法求解。

Result: 在两个合成数据集和八个真实数据集上的实验表明，BLUFS在聚类和分类任务上表现优异。

Conclusion: BLUFS通过双层次建模和$\ell_{2,0}$-范数约束，显著提升了无监督特征选择的性能。

Abstract: Unsupervised feature selection (UFS) is an important task in data
engineering. However, most UFS methods construct models from a single
perspective and often fail to simultaneously evaluate feature importance and
preserve their inherent data structure, thus limiting their performance. To
address this challenge, we propose a novel bi-level unsupervised feature
selection (BLUFS) method, including a clustering level and a feature level.
Specifically, at the clustering level, spectral clustering is used to generate
pseudo-labels for representing the data structure, while a continuous linear
regression model is developed to learn the projection matrix. At the feature
level, the $\ell_{2,0}$-norm constraint is imposed on the projection matrix for
more effectively selecting features. To the best of our knowledge, this is the
first work to combine a bi-level framework with the $\ell_{2,0}$-norm. To solve
the proposed bi-level model, we design an efficient proximal alternating
minimization (PAM) algorithm, whose subproblems either have explicit solutions
or can be computed by fast solvers. Furthermore, we establish the convergence
result and computational complexity. Finally, extensive experiments on two
synthetic datasets and eight real datasets demonstrate the superiority of BLUFS
in clustering and classification tasks.

</details>


### [166] [Ctrl-DNA: Controllable Cell-Type-Specific Regulatory DNA Design via Constrained RL](https://arxiv.org/abs/2505.20578)
*Xingyu Chen,Shihao Ma,Runsheng Lin,Jiecong Lin,Bo Wang*

Key words: 调控DNA序列, 强化学习, 细胞类型特异性, 转录因子结合位点, 基因表达

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Ctrl-DNA是一种基于约束强化学习的新框架，用于设计具有可控细胞类型特异性的调控DNA序列，在人类启动子和增强子上表现优于现有方法。

Motivation: 设计能够精确控制细胞类型特异性基因表达的调控DNA序列对合成生物学、基因治疗和精准医学至关重要。现有方法难以生成具有可靠细胞特异性的新序列。

Method: 引入了约束强化学习框架，将调控序列设计转化为生物信息化的约束优化问题，通过强化学习迭代优化序列，以在目标细胞类型中最大化调控活性并限制脱靶效应。

Result: 在人类启动子和增强子上，Ctrl-DNA表现优于现有生成和基于强化学习的方法，生成高适应性序列并达到最佳细胞类型特异性。生成的序列包含关键转录因子结合位点，证明其生物学合理性。

Conclusion: Ctrl-DNA为设计具有可控细胞类型特异性的调控DNA序列提供了一种有效方法，推动了合成生物学和基因治疗的发展。

Abstract: Designing regulatory DNA sequences that achieve precise cell-type-specific
gene expression is crucial for advancements in synthetic biology, gene therapy
and precision medicine. Although transformer-based language models (LMs) can
effectively capture patterns in regulatory DNA, their generative approaches
often struggle to produce novel sequences with reliable cell-specific activity.
Here, we introduce Ctrl-DNA, a novel constrained reinforcement learning (RL)
framework tailored for designing regulatory DNA sequences with controllable
cell-type specificity. By formulating regulatory sequence design as a
biologically informed constrained optimization problem, we apply RL to
autoregressive genomic LMs, enabling the models to iteratively refine sequences
that maximize regulatory activity in targeted cell types while constraining
off-target effects. Our evaluation on human promoters and enhancers
demonstrates that Ctrl-DNA consistently outperforms existing generative and
RL-based approaches, generating high-fitness regulatory sequences and achieving
state-of-the-art cell-type specificity. Moreover, Ctrl-DNA-generated sequences
capture key cell-type-specific transcription factor binding sites (TFBS), short
DNA motifs recognized by regulatory proteins that control gene expression,
demonstrating the biological plausibility of the generated sequences.

</details>


### [167] [The challenge of hidden gifts in multi-agent reinforcement learning](https://arxiv.org/abs/2505.20579)
*Dane Malenfant,Blake A. Richards*

Key words: 多智能体强化学习,信用分配,隐藏礼物,集体奖励

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了多智能体强化学习中‘隐藏礼物’现象对信用分配的挑战，提出了一种改进方法帮助独立智能体更可靠地达成集体目标。

Motivation: 探讨在多智能体环境中，智能体可能因无法感知其他智能体的有益行为（‘隐藏礼物’）而难以进行有效的信用分配，从而影响集体目标的达成。

Method: 设计了一个简单的网格世界任务，要求智能体共享一把钥匙以解锁各自的门并获取集体奖励。研究了多种强化学习算法在此任务中的表现，并为独立智能体设计了一种方差降低的修正项。

Result: 标准多智能体强化学习算法在此任务中失败，而具备自身行动历史信息的独立智能体能够解决问题。提出的修正项进一步提升了独立智能体的学习稳定性。

Conclusion: 研究揭示了‘隐藏礼物’对多智能体信用分配的独特挑战，并证明独立智能体的学习感知能力在此类场景中具有优势。

Abstract: Sometimes we benefit from actions that others have taken even when we are
unaware that they took those actions. For example, if your neighbor chooses not
to take a parking spot in front of your house when you are not there, you can
benefit, even without being aware that they took this action. These "hidden
gifts" represent an interesting challenge for multi-agent reinforcement
learning (MARL), since assigning credit when the beneficial actions of others
are hidden is non-trivial. Here, we study the impact of hidden gifts with a
very simple MARL task. In this task, agents in a grid-world environment have
individual doors to unlock in order to obtain individual rewards. As well, if
all the agents unlock their door the group receives a larger collective reward.
However, there is only one key for all of the doors, such that the collective
reward can only be obtained when the agents drop the key for others after they
use it. Notably, there is nothing to indicate to an agent that the other agents
have dropped the key, thus the act of dropping the key for others is a "hidden
gift". We show that several different state-of-the-art RL algorithms, including
MARL algorithms, fail to learn how to obtain the collective reward in this
simple task. Interestingly, we find that independent model-free policy gradient
agents can solve the task when we provide them with information about their own
action history, but MARL agents still cannot solve the task with action
history. Finally, we derive a correction term for these independent agents,
inspired by learning aware approaches, which reduces the variance in learning
and helps them to converge to collective success more reliably. These results
show that credit assignment in multi-agent settings can be particularly
challenging in the presence of "hidden gifts", and demonstrate that learning
awareness in independent agents can benefit these settings.

</details>


### [168] [Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction](https://arxiv.org/abs/2505.20589)
*Mahdi Pourmirzaei,Farzaneh Esmaili,Salhuldin Alqarghuli,Mohammadreza Pourmirzaei,Ye Han,Kai Chen,Mohsen Rezaei,Duolin Wang,Dong Xu*

Key words: Protein Language Models, Prot2Token, multi-task learning, autoregressive decoding, protein prediction

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Prot2Token is a unified framework for diverse protein prediction tasks, converting them into a standardized next-token prediction format, offering efficiency and performance matching specialized models.

Motivation: Traditional protein prediction tasks require specialized models, limiting broad applicability and computational efficiency. Prot2Token aims to unify these tasks under a single framework.

Method: Prot2Token uses an autoregressive decoder conditioned on pre-trained protein encoders and learnable task tokens, enabling multi-task learning and self-supervised pre-training for spatial tasks.

Result: The framework achieves significant speedups (e.g., 1000x over AlphaFold2) and performance comparable to or exceeding specialized approaches across various benchmarks.

Conclusion: Prot2Token advances protein modeling by providing a versatile, high-throughput paradigm, potentially accelerating biological discovery and therapeutic development.

Abstract: The diverse nature of protein prediction tasks has traditionally necessitated
specialized models, hindering the development of broadly applicable and
computationally efficient Protein Language Models (PLMs). In this work, we
introduce Prot2Token, a unified framework that overcomes these challenges by
converting a wide spectrum of protein-related predictions, from sequence-level
properties and residue-specific attributes to complex inter-protein
interactions, into a standardized next-token prediction format. At its core,
Prot2Token employs an autoregressive decoder, conditioned on embeddings from
pre-trained protein encoders and guided by learnable task tokens, to perform
diverse predictions. This architecture uniquely facilitates multi-task
learning, enabling a single model to master numerous tasks with improved
efficiency. We present extensive experimental validation across a variety of
benchmarks, demonstrating Prot2Tokens strong predictive power in different
types of protein-prediction tasks. Key results include significant speedups
(e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or
exceeding specialized approaches. Beyond that, we introduce an auxiliary
self-supervised decoder pre-training approach to improve spatially sensitive
task performance. Prot2Token thus offers a significant step towards a
versatile, high-throughput paradigm for protein modeling, promising to
accelerate biological discovery and the development of novel therapeutics. The
code is available at https://github.com/mahdip72/prot2token .

</details>


### [169] [Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning](https://arxiv.org/abs/2505.20621)
*Shijie Liu,Andrew C. Cullen,Paul Montague,Sarah Erfani,Benjamin I. P. Rubinstein*

Key words: 离线强化学习,数据投毒,认证防御,差分隐私,鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了离线强化学习中数据投毒攻击的防御方法，提出了一种基于差分隐私的认证防御框架，显著提高了对攻击的鲁棒性和保证范围。

Motivation: 离线强化学习因其依赖于外部数据集而易受投毒攻击，作者旨在通过认证防御提供更强保障，增强其安全性和可靠性。

Method: 利用差分隐私的特性，扩展到连续和离散空间，以及随机和确定性环境，提供对单步动作和累积奖励的鲁棒性保证。

Result: 实验表明，该方法在7%数据被投毒时性能下降不超过50%，认证半径比先前工作大5倍。

Conclusion: 基于差分隐私的防御框架能显著提升离线强化学习的安全性，适用于更广泛的环境。

Abstract: Similar to other machine learning frameworks, Offline Reinforcement Learning
(RL) is shown to be vulnerable to poisoning attacks, due to its reliance on
externally sourced datasets, a vulnerability that is exacerbated by its
sequential nature. To mitigate the risks posed by RL poisoning, we extend
certified defenses to provide larger guarantees against adversarial
manipulation, ensuring robustness for both per-state actions, and the overall
expected cumulative reward. Our approach leverages properties of Differential
Privacy, in a manner that allows this work to span both continuous and discrete
spaces, as well as stochastic and deterministic environments -- significantly
expanding the scope and applicability of achievable guarantees. Empirical
evaluations demonstrate that our approach ensures the performance drops to no
more than $50\%$ with up to $7\%$ of the training data poisoned, significantly
improving over the $0.008\%$ in prior work~\citep{wu_copa_2022}, while
producing certified radii that is $5$ times larger as well. This highlights the
potential of our framework to enhance safety and reliability in offline RL.

</details>


### [170] [Position: Adopt Constraints Over Penalties in Deep Learning](https://arxiv.org/abs/2505.20628)
*Juan Ramirez,Meraj Hashemizadeh,Simon Lacoste-Julien*

Key words: 约束优化, 可信AI, 拉格朗日乘法, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文主张使用定制约束优化方法替代传统惩罚系数方法，以提高AI系统的可信度和效率。

Motivation: 当前AI系统依赖固定权重惩罚项来满足外部需求，但这种方法无法保证同时满足约束和良好性能，且调优耗时。

Method: 采用拉格朗日乘法等定制约束方法，动态优化惩罚系数（拉格朗日乘子）并与模型同步训练。

Result: 定制方法能真正解决约束问题、减少调优需求，并与现代深度学习流程无缝集成。

Conclusion: 定制约束优化方法在解决AI系统约束问题上更高效可靠。

Abstract: Recent efforts toward developing trustworthy AI systems with accountability
guarantees have led to a growing reliance on machine learning formulations that
incorporate external requirements, or constraints. These requirements are often
enforced through penalization--adding fixed-weight terms to the task loss. We
argue that this approach is ill-suited, and that tailored constrained
optimization methods should be adopted instead. In particular, no penalty
coefficient may yield a solution that both satisfies the constraints and
achieves good performance--i.e., one solving the constrained problem. Moreover,
tuning these coefficients is costly, incurring significant time and
computational overhead. In contrast, tailored constrained methods--such as the
Lagrangian approach, which optimizes the penalization "coefficients" (the
Lagrange multipliers) alongside the model--(i) truly solve the constrained
problem and add accountability, (ii) eliminate the need for extensive penalty
tuning, and (iii) integrate seamlessly with modern deep learning pipelines.

</details>


### [171] [Explaining Concept Shift with Interpretable Feature Attribution](https://arxiv.org/abs/2505.20634)
*Ruiqi Lyu,Alistair Turcan,Bryan Wilder*

Key words: 概念漂移, 特征选择, 广义加性模型, 机器学习, 表格数据

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出SGShift模型，用于检测表格数据中的概念漂移，并通过稀疏特征集归因模型性能下降。SGShift使用广义加性模型建模概念漂移，并结合特征选择。实验显示其AUC>0.9，召回率>90%，优于基线方法2-3倍。

Motivation: 机器学习模型在训练数据外表现下降，概念漂移导致模型学习错误表示。识别漂移特征有助于理解数据集差异，如时间、疾病状态等科学维度。

Method: SGShift采用广义加性模型建模概念漂移，结合特征选择。扩展方法包括引入knockoffs控制误发现，以及吸收项处理模型拟合不佳问题。

Result: SGShift在合成和真实数据实验中，AUC>0.9，召回率>90%，性能优于基线方法2-3倍。

Conclusion: SGShift能有效检测概念漂移并识别关键特征，为模型性能下降提供解释，适用于多种机器学习模型。

Abstract: Regardless the amount of data a machine learning (ML) model is trained on,
there will inevitably be data that differs from their training set, lowering
model performance. Concept shift occurs when the distribution of labels
conditioned on the features changes, making even a well-tuned ML model to have
learned a fundamentally incorrect representation. Identifying these shifted
features provides unique insight into how one dataset differs from another,
considering the difference may be across a scientifically relevant dimension,
such as time, disease status, population, etc. In this paper, we propose
SGShift, a model for detecting concept shift in tabular data and attributing
reduced model performance to a sparse set of shifted features. SGShift models
concept shift with a Generalized Additive Model (GAM) and performs subsequent
feature selection to identify shifted features. We propose further extensions
of SGShift by incorporating knockoffs to control false discoveries and an
absorption term to account for models with poor fit to the data. We conduct
extensive experiments in synthetic and real data across various ML models and
find SGShift can identify shifted features with AUC $>0.9$ and recall $>90\%$,
often 2 or 3 times as high as baseline methods.

</details>


### [172] [Can Past Experience Accelerate LLM Reasoning?](https://arxiv.org/abs/2505.20643)
*Bo Pan,Liang Zhao*

Key words: 大型语言模型,推理速度,自适应计算,内存机制,任务相关性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文探讨了大型语言模型（LLM）是否可以通过反复接触相关任务来提高推理速度，并提出了SpeedupLLM框架，实验证明LLM在合适的内存和推理方法下可减少56%的计算成本。

Motivation: 研究动机源于人类通过经验能更快更好地完成任务，而LLMs通常需要更多计算资源来提高效果但速度变慢。本文旨在探索LLMs是否也能通过类似方式加速推理。

Method: 作者首先系统化地定义了LLM推理加速的问题设置，然后提出了SpeedupLLM框架，通过自适应计算分配和内存机制实现理论保障的推理加速。

Result: 实验结果显示，LLMs在具备合适内存和推理方法的情况下，推理速度可提升，最高能减少56%的计算成本。

Conclusion: 研究表明LLMs确实可以通过反复接触相关任务实现推理加速，SpeedupLLM框架为此提供了有效方法。

Abstract: Allocating more compute to large language models (LLMs) reasoning has
generally been demonstrated to improve their effectiveness, but also results in
increased inference time. In contrast, humans can perform tasks faster and
better with increased experience and exposure. Hence, this paper aims to
investigate the question: Can LLMs also become faster at reasoning through
recurrent exposure on relevant tasks, and if so, how can it be achieved? To
address these questions, we first formalize the problem setting of LLM
reasoning speedup systematically in the dimensions of task relevancy and
compute budget calculation. We then propose SpeedupLLM, a theoretically
guaranteed framework to implement and benchmark such reasoning speedup
behaviour based on adaptive compute allocation and memory mechanisms. We
further conduct comprehensive experiments to benchmark such behaviour across
different question similarity levels, memory methods, and reasoning methods.
Results show that LLMs can generally reason faster with past experience,
achieving up to a 56% reduction in compute cost when equipped with appropriate
memory and reasoning methods.

</details>


### [173] [Evaluating Training in Binarized Neural Networks Through the Lens of Algorithmic Information Theory](https://arxiv.org/abs/2505.20646)
*Eduardo Y. Sakabe,Felipe S. Abrahão,Alexandre Simões,Esther Colombini,Paula Costa,Ricardo Gudwin,Hector Zenil*

Key words: 算法信息理论, 二值化神经网络, 块分解方法, 复杂性控制, 训练动态

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文探讨了神经网络信息复杂性的理解与控制，提出基于算法信息理论的新方法，并验证其比传统熵方法更能捕捉训练中的结构性变化。

Motivation: 现有基于熵的损失函数和统计指标难以捕捉网络中更深层的因果相关性算法规律，需要更理论化的方法。

Method: 使用二值化神经网络（BNNs）和基于算法概率（AP）的块分解方法（BDM）来量化训练过程中的结构变化。

Result: BDM比熵方法更能准确追踪训练中的结构变化，并与训练损失表现出更强的相关性。

Conclusion: 训练可视为算法压缩过程，学习是对结构性规律的内化，为复杂度感知学习提供了理论基础。

Abstract: Understanding and controlling the informational complexity of neural networks
is a central challenge in machine learning, with implications for
generalization, optimization, and model capacity. While most approaches rely on
entropy-based loss functions and statistical metrics, these measures often fail
to capture deeper, causally relevant algorithmic regularities embedded in
network structure. We propose a shift toward algorithmic information theory,
using Binarized Neural Networks (BNNs) as a first proxy. Grounded in
algorithmic probability (AP) and the universal distribution it defines, our
approach characterizes learning dynamics through a formal, causally grounded
lens. We apply the Block Decomposition Method (BDM) -- a scalable approximation
of algorithmic complexity based on AP -- and demonstrate that it more closely
tracks structural changes during training than entropy, consistently exhibiting
stronger correlations with training loss across varying model sizes and
randomized training runs. These results support the view of training as a
process of algorithmic compression, where learning corresponds to the
progressive internalization of structured regularities. In doing so, our work
offers a principled estimate of learning progression and suggests a framework
for complexity-aware learning and regularization, grounded in first principles
from information theory, complexity, and computability.

</details>


### [174] [Voronoi-grid-based Pareto Front Learning and Its Application to Collaborative Federated Learning](https://arxiv.org/abs/2505.20648)
*Mengmeng Chen,Xiaohu Wu,Qiqi Liu,Tiantian He,Yew-Soon Ong,Yaochu Jin,Qicheng Lao,Han Yu*

Key words: 多目标优化, Pareto-Front Learning, Voronoi网格, 遗传算法, 联邦学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为PHN-HVVS的新型Pareto-Front Learning框架，通过Voronoi网格分解和高维空间中的遗传算法优化，显著提升了Pareto前沿的覆盖范围和HV指标。

Motivation: 多目标优化在机器学习中广泛应用，但现有Pareto-Front Learning方法在高维空间采样和覆盖整个Pareto前沿方面存在挑战，需要新的解决方案。

Method: PHN-HVVS框架结合Voronoi网格分解和遗传算法，提出了一种新的损失函数以优化高维空间中的Pareto前沿覆盖。

Result: 实验证明PHN-HVVS在多目标优化任务中显著优于基线方法，并在联邦学习领域的多个问题中展示了应用潜力。

Conclusion: PHN-HVVS作为一种新的Pareto-Front Learning框架，有效解决了高维空间中的优化挑战，扩展了Pareto前沿的覆盖范围。

Abstract: Multi-objective optimization (MOO) exists extensively in machine learning,
and aims to find a set of Pareto-optimal solutions, called the Pareto front,
e.g., it is fundamental for multiple avenues of research in federated learning
(FL). Pareto-Front Learning (PFL) is a powerful method implemented using
Hypernetworks (PHNs) to approximate the Pareto front. This method enables the
acquisition of a mapping function from a given preference vector to the
solutions on the Pareto front. However, most existing PFL approaches still face
two challenges: (a) sampling rays in high-dimensional spaces; (b) failing to
cover the entire Pareto Front which has a convex shape. Here, we introduce a
novel PFL framework, called as PHN-HVVS, which decomposes the design space into
Voronoi grids and deploys a genetic algorithm (GA) for Voronoi grid
partitioning within high-dimensional space. We put forward a new loss function,
which effectively contributes to more extensive coverage of the resultant
Pareto front and maximizes the HV Indicator. Experimental results on multiple
MOO machine learning tasks demonstrate that PHN-HVVS outperforms the baselines
significantly in generating Pareto front. Also, we illustrate that PHN-HVVS
advances the methodologies of several recent problems in the FL field. The code
is available at
https://github.com/buptcmm/phnhvvs}{https://github.com/buptcmm/phnhvvs.

</details>


### [175] [An Optimisation Framework for Unsupervised Environment Design](https://arxiv.org/abs/2505.20659)
*Nathan Monette,Alistair Letcher,Michael Beukman,Matthew T. Jackson,Alexander Rutherford,Alexander D. Goldie,Jakob N. Foerster*

Key words: 强化学习, 无监督环境设计, 鲁棒性, 非凸优化, 零和博弈

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于非凸-强凹目标的优化框架，用于增强强化学习代理在未知环境中的鲁棒性，超越现有无监督环境设计方法。

Motivation: 提升强化学习在高风险部署中的鲁棒性，需确保代理对未知环境的泛化能力。

Method: 采用非凸-强凹目标优化框架，提出可证明收敛的算法，适用于零和博弈场景。

Result: 实验验证了方法的有效性，在多种难度环境中优于现有方法。

Conclusion: 新框架为无监督环境设计提供了更强理论保证和实际性能提升。

Abstract: For reinforcement learning agents to be deployed in high-risk settings, they
must achieve a high level of robustness to unfamiliar scenarios. One method for
improving robustness is unsupervised environment design (UED), a suite of
methods aiming to maximise an agent's generalisability across configurations of
an environment. In this work, we study UED from an optimisation perspective,
providing stronger theoretical guarantees for practical settings than prior
work. Whereas previous methods relied on guarantees if they reach convergence,
our framework employs a nonconvex-strongly-concave objective for which we
provide a provably convergent algorithm in the zero-sum setting. We empirically
verify the efficacy of our method, outperforming prior methods in a number of
environments with varying difficulties.

</details>


### [176] [Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence Transformers](https://arxiv.org/abs/2505.20666)
*Yukun Zhang,Xueqing Zhou*

Key words: Transformer, PDE, attention mechanism, long sequence, continuous-time dynamics

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出Continuous_Time Attention框架，将PDE融入Transformer注意力机制，解决长序列输入问题。通过扩散、波动或反应扩散动力学动态调整注意力权重，优化长程依赖和梯度流。

Motivation: 解决Transformer在处理极长输入序列时的挑战，通过引入PDE动态调整注意力权重，增强全局一致性和优化性能。

Method: 提出Continuous_Time Attention框架，利用PDE（如扩散、波动或反应扩散）动态演化注意力权重，替代静态注意力矩阵。

Result: 理论分析显示PDE-based attention优化了优化景观，降低远距离交互的衰减速度；实验验证其在长序列任务上优于标准及专用Transformer变体。

Conclusion: PDE-based注意力机制通过连续时间动态和全局一致性，显著提升了Transformer在长序列任务中的表现。

Abstract: We propose a novel framework, Continuous_Time Attention, which infuses
partial differential equations (PDEs) into the Transformer's attention
mechanism to address the challenges of extremely long input sequences. Instead
of relying solely on a static attention matrix, we allow attention weights to
evolve over a pseudo_time dimension via diffusion, wave, or reaction_diffusion
dynamics. This mechanism systematically smooths local noise, enhances
long_range dependencies, and stabilizes gradient flow. Theoretically, our
analysis shows that PDE_based attention leads to better optimization landscapes
and polynomial rather than exponential decay of distant interactions.
Empirically, we benchmark our method on diverse experiments_demonstrating
consistent gains over both standard and specialized long sequence Transformer
variants. Our findings highlight the potential of PDE_based formulations to
enrich attention mechanisms with continuous_time dynamics and global coherence.

</details>


### [177] [Accelerating RL for LLM Reasoning with Optimal Advantage Regression](https://arxiv.org/abs/2505.20686)
*Kianté Brantley,Mingyu Chen,Zhaolin Gao,Jason D. Lee,Wen Sun,Wenhao Zhan,Xuezhou Zhang*

Key words: 强化学习, 策略优化, 大语言模型, 数学推理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为A*-PO的新型两阶段策略优化框架，用于高效训练大语言模型进行推理任务，显著降低了计算开销和内存消耗。

Motivation: 现有的强化学习方法在优化大语言模型时面临高计算开销和内存消耗的问题，这促使研究者提出更高效的训练框架。

Method: A*-PO采用两阶段优化框架：第一阶段利用离线采样估计最优价值函数，第二阶段通过最小二乘回归损失进行策略更新。

Result: A*-PO在数学推理任务中表现优异，训练时间减少2倍，内存使用降低30%以上。

Conclusion: A*-PO通过简化优化过程和降低资源需求，为高效训练大语言模型提供了可行方案。

Abstract: Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning
large language models (LLMs) to improve complex reasoning abilities. However,
state-of-the-art policy optimization methods often suffer from high
computational overhead and memory consumption, primarily due to the need for
multiple generations per prompt and the reliance on critic networks or
advantage estimates of the current policy. In this paper, we propose $A$*-PO, a
novel two-stage policy optimization framework that directly approximates the
optimal advantage function and enables efficient training of LLMs for reasoning
tasks. In the first stage, we leverage offline sampling from a reference policy
to estimate the optimal value function $V$*, eliminating the need for costly
online value estimation. In the second stage, we perform on-policy updates
using a simple least-squares regression loss with only a single generation per
prompt. Theoretically, we establish performance guarantees and prove that the
KL-regularized RL objective can be optimized without requiring complex
exploration strategies. Empirically, $A$*-PO achieves competitive performance
across a wide range of mathematical reasoning benchmarks, while reducing
training time by up to 2$\times$ and peak memory usage by over 30% compared to
PPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at
https://github.com/ZhaolinGao/A-PO.

</details>


### [178] [Evidential Deep Active Learning for Semi-Supervised Classification](https://arxiv.org/abs/2505.20691)
*Shenkai Zhao,Xinao Zhang,Lipeng Pan,Xiaobin Xu,Danilo Pelusi*

Key words: 半监督分类, 主动学习, 不确定性估计, 证据深度学习, T-conorm算子

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种基于证据深度主动学习的半监督分类方法（EDALSSC），通过同时量化标注和未标注数据的不确定性估计，提升了模型更新效果。

Motivation: 现有半监督分类方法常忽略预测结果的不确定性估计，导致样本选择效果存疑，EDALSSC旨在解决这一问题。

Method: 结合证据深度学习和T-conorm算子建模不确定性，动态平衡证据和类别数量的影响，并采用最大不确定性样本选择策略。

Result: 实验证明EDALSSC在图像分类数据集上优于现有半监督和监督主动学习方法。

Conclusion: EDALSSC通过改进不确定性估计和样本选择策略，显著提升了半监督分类性能。

Abstract: Semi-supervised classification based on active learning has made significant
progress, but the existing methods often ignore the uncertainty estimation (or
reliability) of the prediction results during the learning process, which makes
it questionable whether the selected samples can effectively update the model.
Hence, this paper proposes an evidential deep active learning approach for
semi-supervised classification (EDALSSC). EDALSSC builds a semi-supervised
learning framework to simultaneously quantify the uncertainty estimation of
labeled and unlabeled data during the learning process. The uncertainty
estimation of the former is associated with evidential deep learning, while
that of the latter is modeled by combining ignorance information and conflict
information of the evidence from the perspective of the T-conorm operator.
Furthermore, this article constructs a heuristic method to dynamically balance
the influence of evidence and the number of classes on uncertainty estimation
to ensure that it does not produce counter-intuitive results in EDALSSC. For
the sample selection strategy, EDALSSC selects the sample with the greatest
uncertainty estimation that is calculated in the form of a sum when the
training loss increases in the latter half of the learning process.
Experimental results demonstrate that EDALSSC outperforms existing
semi-supervised and supervised active learning approaches on image
classification datasets.

</details>


### [179] [Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series](https://arxiv.org/abs/2505.20697)
*Zachary C. Brown,David Carlson*

Key words: 假设生成, 动态因果发现, 非线性关系, 神经动力学, 机器学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出一种新方法，通过将动态图建模为静态图的条件加权叠加，以检测非线性、时变的变量间相互作用，显著提升动态因果模式预测性能。

Motivation: 传统机器学习方法假设因果关系是静态的，限制了其在动态系统（如大脑）中的应用。现有动态因果发现技术多限于线性关系或简化假设，需改进以捕捉复杂、时变的相互作用。

Method: 提出将动态图建模为静态图的条件加权叠加，每个静态图可捕捉非线性关系，从而突破线性限制。

Result: 在实验中，该方法动态因果模式预测的F1分数平均提升22%-28%，部分场景提升超60%，并在真实脑数据案例中成功识别与特定行为状态相关的神经动态关系。

Conclusion: 新方法有效解决了动态非线性因果关系的建模问题，为神经动力学研究提供了有价值的工具。

Abstract: The field of hypothesis generation promises to reduce costs in neuroscience
by narrowing the range of interventional studies needed to study various
phenomena. Existing machine learning methods can generate scientific hypotheses
from complex datasets, but many approaches assume causal relationships are
static over time, limiting their applicability to systems with dynamic,
state-dependent behavior, such as the brain. While some techniques attempt
dynamic causal discovery through factor models, they often restrict
relationships to linear patterns or impose other simplifying assumptions. We
propose a novel method that models dynamic graphs as a conditionally weighted
superposition of static graphs, where each static graph can capture nonlinear
relationships. This approach enables the detection of complex, time-varying
interactions between variables beyond linear limitations. Our method improves
f1-scores of predicted dynamic causal patterns by roughly 22-28% on average
over baselines in some of our experiments, with some improvements reaching well
over 60%. A case study on real brain data demonstrates our method's ability to
uncover relationships linked to specific behavioral states, offering valuable
insights into neural dynamics.

</details>


### [180] [Sparsified State-Space Models are Efficient Highway Networks](https://arxiv.org/abs/2505.20698)
*Woomin Song,Jihoon Tack,Sangwoo Mo,Seunghyuk Oh,Jinwoo Shin*

Key words: 状态空间模型,层次化稀疏化,令牌剪枝,信息流,序列建模

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出Simba方法，通过层次化稀疏化增强状态空间模型（SSMs），提升其在序列建模中的效率和性能。

Motivation: 状态空间模型（SSMs）因自注意力机制的高计算成本，提出用线性递归替代。但现有SSMs存在冗余问题，尤其是上层层因全局信息编码更冗余，影响信息传递。因此，提出一种层次化稀疏化方法优化SSMs。

Method: 引入Simba，一种基于令牌剪枝的层次化稀疏化方法。上层稀疏化程度更高，模拟高速路径行为；提出新的令牌剪枝准则，衡量令牌对最终输出的全局影响。

Result: Simba在相同计算量下优于基线模型Mamba，在多项自然语言任务中表现更优，并改善了长序列信息流。

Conclusion: 层次化稀疏化是优化SSMs的有效方法，既能提升效率，又能改善信息传递。

Abstract: State-space models (SSMs) offer a promising architecture for sequence
modeling, providing an alternative to Transformers by replacing expensive
self-attention with linear recurrences. In this paper, we propose a simple yet
effective trick to enhance SSMs within given computational budgets by
sparsifying them. Our intuition is that tokens in SSMs are highly redundant due
to gradual recurrent updates, and dense recurrence operations block the
delivery of past information. In particular, we observe that upper layers of
SSMs tend to be more redundant as they encode global information, while lower
layers encode local information. Motivated by this, we introduce Simba, a
hierarchical sparsification method for SSMs based on token pruning. Simba
sparsifies upper layers more than lower layers, encouraging the upper layers to
behave like highways. To achieve this, we propose a novel token pruning
criterion for SSMs, measuring the global impact of tokens on the final output
by accumulating local recurrences. We demonstrate that Simba outperforms the
baseline model, Mamba, with the same FLOPS in various natural language tasks.
Moreover, we illustrate the effect of highways, showing that Simba not only
enhances efficiency but also improves the information flow across long
sequences. Code is available at https://github.com/woominsong/Simba.

</details>


### [181] [Are Data Embeddings effective in time series forecasting?](https://arxiv.org/abs/2505.20716)
*Reza Nematirad,Anil Pahwa,Balasubramaniam Natarajan*

Key words: 时间序列预测, 数据嵌入, 消融研究, 计算效率, 模型性能

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究发现，在时间序列预测中移除数据嵌入层不会降低性能，甚至可能提升准确性和计算效率。

Motivation: 探讨数据嵌入技术在时间序列预测中的实际有效性，尽管现有模型通过复杂的数据嵌入层提升精度，但其改进效果往往微小。

Method: 对15种最先进模型和4个基准数据集进行广泛的消融研究，移除数据嵌入层并分析性能变化。

Result: 移除数据嵌入层不仅不会降低预测性能，反而在许多情况下提高了准确性和计算效率，改进效果常超过模型间通常报告的差异。

Conclusion: 数据嵌入层在时间序列预测中可能非必要，甚至可能带来负面影响，建议重新评估其作用。

Abstract: Time series forecasting plays a crucial role in many real-world applications,
and numerous complex forecasting models have been proposed in recent years.
Despite their architectural innovations, most state-of-the-art models report
only marginal improvements -- typically just a few thousandths in standard
error metrics. These models often incorporate complex data embedding layers to
transform raw inputs into higher-dimensional representations to enhance
accuracy. But are data embedding techniques actually effective in time series
forecasting? Through extensive ablation studies across fifteen state-of-the-art
models and four benchmark datasets, we find that removing data embedding layers
from many state-of-the-art models does not degrade forecasting performance. In
many cases, it improves both accuracy and computational efficiency. The gains
from removing embedding layers often exceed the performance differences
typically reported between competing models. Code available at:
https://github.com/neuripsdataembedidng/DataEmbedding

</details>


### [182] [Recurrent Neural Operators: Stable Long-Term PDE Prediction](https://arxiv.org/abs/2505.20721)
*Zaijun Ye,Chen-Song Zhang,Wansheng Wang*

Key words: 循环神经算子、时间依赖问题、误差累积、神经算子学习、多网格神经算子

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种名为'循环神经算子'(RNOs)的新框架，用于解决时间依赖性问题中训练与推理不匹配导致的长期自回归预测误差累积问题。该方法通过在训练阶段模拟推理动态，显著提升了长期准确性和稳定性。

Motivation: 标准训练策略（如教师强制）在时间依赖性问题中会导致训练与推理不匹配，从而在长期自回归预测中引发误差累积。本文旨在通过循环训练解决这一问题，提高神经算子的长期预测能力。

Method: 提出了循环神经算子(RNOs)，通过在训练阶段递归地应用算子对自身预测结果进行操作，模拟推理动态，从而减少误差累积。理论分析表明，该方法能将误差的最坏指数增长降低为线性增长。

Result: 实验证明，循环训练的多网格神经算子在长期准确性和稳定性上显著优于教师强制训练的模型。

Conclusion: 研究强调了在神经算子学习中，训练与推理动态对齐对提升时间泛化能力的重要性。

Abstract: Neural operators have emerged as powerful tools for learning solution
operators of partial differential equations. However, in time-dependent
problems, standard training strategies such as teacher forcing introduce a
mismatch between training and inference, leading to compounding errors in
long-term autoregressive predictions. To address this issue, we propose
Recurrent Neural Operators (RNOs)-a novel framework that integrates recurrent
training into neural operator architectures. Instead of conditioning each
training step on ground-truth inputs, RNOs recursively apply the operator to
their own predictions over a temporal window, effectively simulating
inference-time dynamics during training. This alignment mitigates exposure bias
and enhances robustness to error accumulation. Theoretically, we show that
recurrent training can reduce the worst-case exponential error growth typical
of teacher forcing to linear growth. Empirically, we demonstrate that
recurrently trained Multigrid Neural Operators significantly outperform their
teacher-forced counterparts in long-term accuracy and stability on standard
benchmarks. Our results underscore the importance of aligning training with
inference dynamics for robust temporal generalization in neural operator
learning.

</details>


### [183] [A reinforcement learning agent for maintenance of deteriorating systems with increasingly imperfect repairs](https://arxiv.org/abs/2505.20725)
*Alberto Pliego Marugán,Jesús M. Pinar-Pérez,Fausto Pedro García Márquez*

Key words: 维护优化, 强化学习, Double Deep Q-Network, 伽马退化过程, 工业4.0

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 摘要提出了一种结合伽马退化过程和新型维护模型的方法，使用基于强化学习的智能体（Double Deep Q-Network）优化维护策略，显著降低长期成本。

Motivation: 工业4.0时代需要新的维护优化范式，强化学习在工程维护中应用潜力巨大。现有维护模型难以反映真实系统的退化行为（维修效果递减）。

Method: 提出伽马退化过程与维修效果递减的新型维护模型，开发基于Double Deep Q-Network的强化学习智能体，无需预设预防阈值且支持连续退化状态空间。

Result: 智能体在不同场景中表现灵活，参数分析显示其维护策略适应性强，长期成本显著优于传统策略。

Conclusion: 该方法为动态退化系统提供了高效维护解决方案，验证了强化学习在维护优化中的实用价值。

Abstract: Efficient maintenance has always been essential for the successful
application of engineering systems. However, the challenges to be overcome in
the implementation of Industry 4.0 necessitate new paradigms of maintenance
optimization. Machine learning techniques are becoming increasingly used in
engineering and maintenance, with reinforcement learning being one of the most
promising. In this paper, we propose a gamma degradation process together with
a novel maintenance model in which repairs are increasingly imperfect, i.e.,
the beneficial effect of system repairs decreases as more repairs are
performed, reflecting the degradational behavior of real-world systems. To
generate maintenance policies for this system, we developed a
reinforcement-learning-based agent using a Double Deep Q-Network architecture.
This agent presents two important advantages: it works without a predefined
preventive threshold, and it can operate in a continuous degradation state
space. Our agent learns to behave in different scenarios, showing great
flexibility. In addition, we performed an analysis of how changes in the main
parameters of the environment affect the maintenance policy proposed by the
agent. The proposed approach is demonstrated to be appropriate and to
significatively improve long-run cost as compared with other common maintenance
strategies.

</details>


### [184] [Adversarial bandit optimization for approximately linear functions](https://arxiv.org/abs/2505.20734)
*Zhuoyu Cheng,Kohei Hatano,Eiji Takimoto*

Key words: nonconvex, non-smooth, bandit optimization, regret bounds, perturbation

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了非凸非光滑函数的赌徒优化问题，损失函数为线性函数与小扰动的和，给出了预期和高概率的遗憾界限，并改进了无扰动特例的遗憾界限，同时提供了预期遗憾的下界。

Motivation: 研究非凸非光滑函数的赌徒优化问题，填补了带扰动条件下遗憾分析的空白。

Method: 通过分析损失函数为线性加扰动的结构，提出预期和高概率的遗憾界限推导方法。

Result: 得到了带扰动情况下的遗憾界限，并提升了无扰动特例的高概率遗憾界限。同时证明了预期遗憾的下界。

Conclusion: 该结果为带扰动的赌徒优化问题提供了理论保障，并展示了在特例中的改进潜力。

Abstract: We consider a bandit optimization problem for nonconvex and non-smooth
functions, where in each trial the loss function is the sum of a linear
function and a small but arbitrary perturbation chosen after observing the
player's choice. We give both expected and high probability regret bounds for
the problem. Our result also implies an improved high-probability regret bound
for the bandit linear optimization, a special case with no perturbation. We
also give a lower bound on the expected regret.

</details>


### [185] [Detecting Informative Channels: ActionFormer](https://arxiv.org/abs/2505.20739)
*Kunpeng Zhao,Asahi Miyazaki,Tsuyoshi Okita*

Key words: Human Activity Recognition, ActionFormer, 传感器信号, 序列激励, swish激活函数

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出改进的ActionFormer模型，通过序列激励策略和swish激活函数优化传感器信号处理，提升了HAR性能。在WEAR数据集上平均mAP提高了16.01%。

Motivation: 传统ActionFormer模型在处理传感器信号时存在时空特征依赖性和动态性捕捉不足的问题，导致性能受限。

Method: 采用序列激励策略减少额外参数，并使用swish激活函数保留负值方向信息，改进ActionFormer适应传感器输入。

Result: 在WEAR数据集上测试，改进模型的平均mAP显著提升16.01%。

Conclusion: 改进后的ActionFormer能更有效处理传感器信号，验证了序列激励和swish函数在HAR任务中的有效性。

Abstract: Human Activity Recognition (HAR) has recently witnessed advancements with
Transformer-based models. Especially, ActionFormer shows us a new perspectives
for HAR in the sense that this approach gives us additional outputs which
detect the border of the activities as well as the activity labels.
ActionFormer was originally proposed with its input as image/video. However,
this was converted to with its input as sensor signals as well. We analyze this
extensively in terms of deep learning architectures. Based on the report of
high temporal dynamics which limits the model's ability to capture subtle
changes effectively and of the interdependencies between the spatial and
temporal features. We propose the modified ActionFormer which will decrease
these defects for sensor signals. The key to our approach lies in accordance
with the Sequence-and-Excitation strategy to minimize the increase in
additional parameters and opt for the swish activation function to retain the
information about direction in the negative range. Experiments on the WEAR
dataset show that our method achieves substantial improvement of a 16.01\% in
terms of average mAP for inertial data.

</details>


### [186] ['Hello, World!': Making GNNs Talk with LLMs](https://arxiv.org/abs/2505.20742)
*Sunwoo Kim,Soo Yong Lee,Jaemin Yoo,Kijung Shin*

Key words: 图神经网络（GNN），大型语言模型（LLM），可解释性，零样本学习，节点分类，链接预测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了Graph Lingual Network (GLN)，一种基于大型语言模型（LLMs）的图神经网络（GNN），其隐藏表示形式为人类可读文本。GLN通过精心设计的提示整合了GNN的消息传递模块及先进技术（如图注意力和初始残差连接），在节点分类和链接预测任务中表现出色。

Motivation: 尽管GNN在图相关任务中表现优异，但其高维隐藏表示使其成为黑盒模型。本文旨在通过构建基于LLMs的GLN，提供可解读的隐藏表示，以揭示GNN的内部工作原理。

Method: GLN通过设计提示（prompt）整合GNN的消息传递模块及高级技术（如图注意力和初始残差连接），并将隐藏表示转化为文本形式。

Result: GLN在节点分类和链接预测任务中实现了出色的零样本性能，超越了现有基于LLM的基线方法。

Conclusion: GLN不仅提升了GNN的可解释性，还展示了基于LLM的GNN在零样本学习中的潜力。

Abstract: While graph neural networks (GNNs) have shown remarkable performance across
diverse graph-related tasks, their high-dimensional hidden representations
render them black boxes. In this work, we propose Graph Lingual Network (GLN),
a GNN built on large language models (LLMs), with hidden representations in the
form of human-readable text. Through careful prompt design, GLN incorporates
not only the message passing module of GNNs but also advanced GNN techniques,
including graph attention and initial residual connection. The
comprehensibility of GLN's hidden representations enables an intuitive analysis
of how node representations change (1) across layers and (2) under advanced GNN
techniques, shedding light on the inner workings of GNNs. Furthermore, we
demonstrate that GLN achieves strong zero-shot performance on node
classification and link prediction, outperforming existing LLM-based baseline
methods.

</details>


### [187] [Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction](https://arxiv.org/abs/2505.20755)
*Yifei Wang,Weimin Bai,Colin Zhang,Debing Zhang,Weijian Luo,He Sun*

Key words: Uni-Instruct, 扩散模型, 一步蒸馏, f-散度, 生成任务

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了Uni-Instruct理论框架，统一了10多种一步扩散蒸馏方法，通过扩散扩展理论和可处理的损失函数实现高效训练。在CIFAR10和ImageNet-64×64基准测试中取得突破性FID值，并在文本到3D生成任务中表现出色。

Motivation: 研究动机是通过理论框架统一现有的一步扩散蒸馏方法，并从f-散度家族的扩散扩展理论出发，解决原始f-散度的难处理问题，以实现高效训练和性能提升。

Method: 方法包括提出Uni-Instruct理论框架，通过扩散扩展理论和关键的突破性理论克服f-散度的难处理问题，设计等效可处理的损失函数来训练一步扩散模型。

Result: 在CIFAR10和ImageNet-64×64基准测试中，Uni-Instruct分别取得了1.46（无条件生成）和1.38（条件生成）的FID值，以及1.02的SoTA FID值，显著优于79步教师扩散模型。在文本到3D生成任务中也略优于SDS和VSD方法。

Conclusion: Uni-Instruct在理论和实证上均具有显著贡献，为一扩散蒸馏的知识迁移和未来研究提供了有力支持。

Abstract: In this paper, we unify more than 10 existing one-step diffusion distillation
approaches, such as Diff-Instruct, DMD, SIM, SiD, $f$-distill, etc, inside a
theory-driven framework which we name the \textbf{\emph{Uni-Instruct}}.
Uni-Instruct is motivated by our proposed diffusion expansion theory of the
$f$-divergence family. Then we introduce key theories that overcome the
intractability issue of the original expanded $f$-divergence, resulting in an
equivalent yet tractable loss that effectively trains one-step diffusion models
by minimizing the expanded $f$-divergence family. The novel unification
introduced by Uni-Instruct not only offers new theoretical contributions that
help understand existing approaches from a high-level perspective but also
leads to state-of-the-art one-step diffusion generation performances. On the
CIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet
Inception Distance (FID) values of \textbf{\emph{1.46}} for unconditional
generation and \textbf{\emph{1.38}} for conditional generation. On the
ImageNet-$64\times 64$ generation benchmark, Uni-Instruct achieves a new SoTA
one-step generation FID of \textbf{\emph{1.02}}, which outperforms its 79-step
teacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35).
We also apply Uni-Instruct on broader tasks like text-to-3D generation. For
text-to-3D generation, Uni-Instruct gives decent results, which slightly
outperforms previous methods, such as SDS and VSD, in terms of both generation
quality and diversity. Both the solid theoretical and empirical contributions
of Uni-Instruct will potentially help future studies on one-step diffusion
distillation and knowledge transferring of diffusion models.

</details>


### [188] [Practical estimation of the optimal classification error with soft labels and calibration](https://arxiv.org/abs/2505.20761)
*Ryota Ushio,Takashi Ishida,Masashi Sugiyama*

Key words: 机器学习, 二分类, Bayes误差, 软标签, 等渗校准

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了如何评估和提升机器学习模型的性能上限，特别是在二分类任务中，通过理论分析和实践方法改进了Bayes误差的估计，并解决了软标签损坏情况下的估计问题。

Motivation: 研究动机是探究机器学习模型的性能上限，并提供一个实用且理论支持的估计方法，特别是在二分类任务中。

Method: 方法包括理论分析硬标签估计器的偏差特性，以及提出在软标签损坏情况下通过等渗校准获得统计一致估计器的新方法。

Result: 实验结果表明，所提出的方法在合成和真实数据集上均有效，且具有理论支持。

Conclusion: 论文结论强调了新方法在估计Bayes误差时的可行性和优势，特别是在隐私受限场景下的实用性。

Abstract: While the performance of machine learning systems has experienced significant
improvement in recent years, relatively little attention has been paid to the
fundamental question: to what extent can we improve our models? This paper
provides a means of answering this question in the setting of binary
classification, which is practical and theoretically supported. We extend a
previous work that utilizes soft labels for estimating the Bayes error, the
optimal error rate, in two important ways. First, we theoretically investigate
the properties of the bias of the hard-label-based estimator discussed in the
original work. We reveal that the decay rate of the bias is adaptive to how
well the two class-conditional distributions are separated, and it can decay
significantly faster than the previous result suggested as the number of hard
labels per instance grows. Second, we tackle a more challenging problem
setting: estimation with corrupted soft labels. One might be tempted to use
calibrated soft labels instead of clean ones. However, we reveal that
calibration guarantee is not enough, that is, even perfectly calibrated soft
labels can result in a substantially inaccurate estimate. Then, we show that
isotonic calibration can provide a statistically consistent estimator under an
assumption weaker than that of the previous work. Our method is instance-free,
i.e., we do not assume access to any input instances. This feature allows it to
be adopted in practical scenarios where the instances are not available due to
privacy issues. Experiments with synthetic and real-world datasets show the
validity of our methods and theory.

</details>


### [189] [Robust and Explainable Detector of Time Series Anomaly via Augmenting Multiclass Pseudo-Anomalies](https://arxiv.org/abs/2505.20765)
*Kohei Obata,Yasuko Matsubara,Yasushi Sakurai*

Key words: 时间序列, 异常检测, 数据增强, 多类分类, 软标签

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出RedLamp方法，通过多类数据增强生成伪异常，使用软标签进行多类分类来提高时间序列异常检测的鲁棒性和可解释性。

Motivation: 当前时间序列异常检测方法在训练集中存在异常污染时效果不佳，且数据增强生成的伪异常覆盖范围有限并可能产生虚假异常。

Method: 采用多样化的数据增强生成多类伪异常，并利用软标签进行多类分类学习边界。

Result: 实验证明RedLamp在异常检测中有效，并对异常污染具有鲁棒性。

Conclusion: RedLamp通过多类伪异常和软标签提高了时间序列异常检测的性能和鲁棒性。

Abstract: Unsupervised anomaly detection in time series has been a pivotal research
area for decades. Current mainstream approaches focus on learning normality, on
the assumption that all or most of the samples in the training set are normal.
However, anomalies in the training set (i.e., anomaly contamination) can be
misleading. Recent studies employ data augmentation to generate
pseudo-anomalies and learn the boundary separating the training samples from
the augmented samples. Although this approach mitigates anomaly contamination
if augmented samples mimic unseen real anomalies, it suffers from several
limitations. (1) Covering a wide range of time series anomalies is challenging.
(2) It disregards augmented samples that resemble normal samples (i.e., false
anomalies). (3) It places too much trust in the labels of training and
augmented samples. In response, we propose RedLamp, which employs diverse data
augmentations to generate multiclass pseudo-anomalies and learns the multiclass
boundary. Such multiclass pseudo-anomalies cover a wide variety of time series
anomalies. We conduct multiclass classification using soft labels, which
prevents the model from being overconfident and ensures its robustness against
contaminated/false anomalies. The learned latent space is inherently
explainable as it is trained to separate pseudo-anomalies into multiclasses.
Extensive experiments demonstrate the effectiveness of RedLamp in anomaly
detection and its robustness against anomaly contamination.

</details>


### [190] [TimePro: Efficient Multivariate Long-term Time Series Forecasting with Variable- and Time-Aware Hyper-state](https://arxiv.org/abs/2505.20774)
*Xiaowen Ma,Zhenliang Ni,Shuai Xiao,Xinghao Chen*

Key words: TimePro, long-term forecasting, multi-delay issue, Mamba-based model, hyper-state

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出TimePro模型解决多延迟问题，通过构建变量和时间感知的超状态，捕捉复杂变量关系和非平凡时间表示，在8个长期预测基准测试中表现优异。

Motivation: 传统模型对所有变量或时间点进行统一处理，难以捕捉复杂的变量关系和获取非平凡的时间表示。

Method: TimePro采用基于Mamba的创新模型，构建变量和时间感知的超状态，保留细粒度时间特征并自适应选择时间点调整状态。

Result: TimePro在8个实际长期预测基准测试中表现出色，并保持线性复杂度。

Conclusion: TimePro通过感知变量关系和显著时间信息实现了精确预测。

Abstract: In long-term time series forecasting, different variables often influence the
target variable over distinct time intervals, a challenge known as the
multi-delay issue. Traditional models typically process all variables or time
points uniformly, which limits their ability to capture complex variable
relationships and obtain non-trivial time representations. To address this
issue, we propose TimePro, an innovative Mamba-based model that constructs
variate- and time-aware hyper-states. Unlike conventional approaches that
merely transfer plain states across variable or time dimensions, TimePro
preserves the fine-grained temporal features of each variate token and
adaptively selects the focused time points to tune the plain state. The
reconstructed hyper-state can perceive both variable relationships and salient
temporal information, which helps the model make accurate forecasting. In
experiments, TimePro performs competitively on eight real-world long-term
forecasting benchmarks with satisfactory linear complexity. Code is available
at https://github.com/xwmaxwma/TimePro.

</details>


### [191] [Non-invasive maturity assessment of iPSC-CMs based on optical maturity characteristics using interpretable AI](https://arxiv.org/abs/2505.20775)
*Fabian Scheurer,Alexander Hammer,Mario Schubert,Robert-Patrick Steiner,Oliver Gamm,Kaomei Guan,Frank Sonntag,Hagen Malberg,Martin Schmidt*

Key words: iPSC-CMs, 成熟度评估, 非侵入性, 人工智能, 视频运动分析

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于人工智能的非侵入性方法，通过视频运动分析自动分类iPSC-CMs的成熟度，准确率达99.5%，减少了细胞损伤和实验变异性。

Motivation: iPSC-CMs的成熟度评估通常耗时且可能损伤细胞，因此需要开发一种非侵入性、高效的方法。

Method: 使用支持向量机（SVM）分析视频运动特征，并通过SHAP解释模型选择关键特征。

Result: 优化后的模型在测试集上达到99.5%的准确率，位移、松弛上升时间和跳动持续时间是关键特征。

Conclusion: 非侵入性光学运动分析结合AI方法可有效评估iPSC-CMs成熟度，提升实验可重复性。

Abstract: Human induced pluripotent stem cell-derived cardiomyocytes (iPSC-CMs) are an
important resource for the identification of new therapeutic targets and
cardioprotective drugs. After differentiation iPSC-CMs show an immature,
fetal-like phenotype. Cultivation of iPSC-CMs in lipid-supplemented maturation
medium (MM) strongly enhances their structural, metabolic and functional
phenotype. Nevertheless, assessing iPSC-CM maturation state remains challenging
as most methods are time consuming and go in line with cell damage or loss of
the sample. To address this issue, we developed a non-invasive approach for
automated classification of iPSC-CM maturity through interpretable artificial
intelligence (AI)-based analysis of beat characteristics derived from
video-based motion analysis. In a prospective study, we evaluated 230 video
recordings of early-state, immature iPSC-CMs on day 21 after differentiation
(d21) and more mature iPSC-CMs cultured in MM (d42, MM). For each recording, 10
features were extracted using Maia motion analysis software and entered into a
support vector machine (SVM). The hyperparameters of the SVM were optimized in
a grid search on 80 % of the data using 5-fold cross-validation. The optimized
model achieved an accuracy of 99.5 $\pm$ 1.1 % on a hold-out test set. Shapley
Additive Explanations (SHAP) identified displacement, relaxation-rise time and
beating duration as the most relevant features for assessing maturity level.
Our results suggest the use of non-invasive, optical motion analysis combined
with AI-based methods as a tool to assess iPSC-CMs maturity and could be
applied before performing functional readouts or drug testing. This may
potentially reduce the variability and improve the reproducibility of
experimental studies.

</details>


### [192] [Multi-VQC: A Novel QML Approach for Enhancing Healthcare Classification](https://arxiv.org/abs/2505.20797)
*Antonio Tudisco,Deborah Volpe,Giovanna Turvani*

Key words: 疾病诊断，量子模型，类别不平衡，机器学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文探讨了量子模型如何解决疾病诊断中因类别不平衡导致的传统机器学习模型效果不佳的问题。

Motivation: 疾病诊断的准确性和可靠性对于及时治疗和提高患者生存率至关重要。传统的机器学习模型在类别不平衡问题中表现不佳，因此需要探索量子模型的潜在优势。

Method: 通过将数据映射到更高维的计算空间，量子模型能够表达复杂的模式，从而克服传统模型的局限性。

Result: 量子模型展现出克服类别不平衡问题的潜力，提升了疾病诊断模型的准确性。

Conclusion: 量子模型为疾病诊断提供了新的解决方案，尤其是在处理类别不平衡问题时表现优异。

Abstract: Accurate and reliable diagnosis of diseases is crucial in enabling timely
medical treatment and enhancing patient survival rates. In recent years,
Machine Learning has revolutionized diagnostic practices by creating
classification models capable of identifying diseases. However, these
classification problems often suffer from significant class imbalances, which
can inhibit the effectiveness of traditional models. Therefore, the interest in
Quantum models has arisen, driven by the captivating promise of overcoming the
limitations of the classical counterpart thanks to their ability to express
complex patterns by mapping data in a higher-dimensional computational space.

</details>


### [193] [Leaner Transformers: More Heads, Less Depth](https://arxiv.org/abs/2505.20802)
*Hemanth Saratchandran,Damien Teney,Simon Lucey*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文挑战了‘模型越大越好’的观感，通过重新定义多头自注意力的作用，设计出更高效的Transformer架构。理论分析表明，多头注意力改善了注意力块的状况，因此在实践中可以将模型参数减少30-50%，同时保持准确性。多种任务上的实验验证了这一方法的有效性。

Motivation: 近年来，Transformer模型的规模不断增大，但作者质疑是否需要如此庞大的参数。通过理论研究发现，多头自注意力的真正作用在于改善注意力块的状况，而非单纯增加模型容量。

Method: 作者提出了一个理论，指出多头注意力的关键作用是优化注意力块的状况。基于这一理论，重新设计流行的Transformer架构，增加头数但减少模型深度，从而显著降低参数数量。

Result: 实验结果显示，在计算机视觉（ImageNet-1k）、语言和序列建模（GLUE、TinyStories、Long-Range Arena）任务中，参数量减少30-50%时仍能保持精度。

Conclusion: 该研究表明，‘更大并不总是更好’，通过优化注意力机制的设计，可以在减少参数的同时保持模型性能，为高效Transformer架构的设计提供了新思路。

Abstract: Transformers have reshaped machine learning by utilizing attention mechanisms
to capture complex patterns in large datasets, leading to significant
improvements in performance. This success has contributed to the belief that
"bigger means better", leading to ever-increasing model sizes. This paper
challenge this ideology by showing that many existing transformers might be
unnecessarily oversized. We discover a theoretical principle that redefines the
role of multi-head attention. An important benefit of the multiple heads is in
improving the conditioning of the attention block. We exploit this theoretical
insight and redesign popular architectures with an increased number of heads.
The improvement in the conditioning proves so significant in practice that
model depth can be decreased, reducing the parameter count by up to 30-50%
while maintaining accuracy. We obtain consistent benefits across a variety of
transformer-based architectures of various scales, on tasks in computer vision
(ImageNet-1k) as well as language and sequence modeling (GLUE benchmark,
TinyStories, and the Long-Range Arena benchmark).

</details>


### [194] [Quantum Machine Learning in Healthcare: Evaluating QNN and QSVM Models](https://arxiv.org/abs/2505.20804)
*Antonio Tudisco,Deborah Volpe,Giovanna Turvani*

Key words: 量子机器学习, 医疗诊断, 不平衡数据, 量子支持向量机, 量子神经网络

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文探讨了量子分类器（如量子神经网络和量子支持向量机）在医疗数据集上的表现，证明其在处理不平衡数据时优于经典模型，尤其是在量子支持向量机的表现更佳。

Motivation: 医疗诊断的准确性至关重要，但传统机器学习模型在处理高度不平衡的数据时性能受限。量子模型因其叠加和纠缠特性，可能更有效地解决这一问题。

Method: 使用量子神经网络（QNNs）和量子支持向量机（QSVMs），与经典模型在三个医疗数据集（前列腺癌、心衰和糖尿病）上进行比较。

Result: QSVMs在所有数据集上表现优于QNNs，且在高度不平衡的数据场景下，量子模型总体优于经典模型。

Conclusion: 量子模型在医疗分类任务中展现出潜力，尤其是QSVMs，为未来研究提供了方向。

Abstract: Effective and accurate diagnosis of diseases such as cancer, diabetes, and
heart failure is crucial for timely medical intervention and improving patient
survival rates. Machine learning has revolutionized diagnostic methods in
recent years by developing classification models that detect diseases based on
selected features. However, these classification tasks are often highly
imbalanced, limiting the performance of classical models. Quantum models offer
a promising alternative, exploiting their ability to express complex patterns
by operating in a higher-dimensional computational space through superposition
and entanglement. These unique properties make quantum models potentially more
effective in addressing the challenges of imbalanced datasets. This work
evaluates the potential of quantum classifiers in healthcare, focusing on
Quantum Neural Networks (QNNs) and Quantum Support Vector Machines (QSVMs),
comparing them with popular classical models. The study is based on three
well-known healthcare datasets -- Prostate Cancer, Heart Failure, and Diabetes.
The results indicate that QSVMs outperform QNNs across all datasets due to
their susceptibility to overfitting. Furthermore, quantum models prove the
ability to overcome classical models in scenarios with high dataset imbalance.
Although preliminary, these findings highlight the potential of quantum models
in healthcare classification tasks and lead the way for further research in
this domain.

</details>


### [195] [Simple yet Effective Graph Distillation via Clustering](https://arxiv.org/abs/2505.20807)
*Yurui Lai,Taiyan Zhang,Renchi Yang*

Key words: 图神经网络, 图数据蒸馏, 聚类, 同质性, 节点分类

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种高效的图数据蒸馏方法ClustGDD，通过聚类和同质性优化解决了传统GDD方法质量低或训练耗时的问题。

Motivation: 图数据蒸馏（GDD）用于高效训练图神经网络（GNN），但现有方法依赖启发式对齐，导致结果质量低或训练成本高。

Method: ClustGDD通过聚类最小化簇内平方和并最大化图同质性，辅以类感知图采样和一致性损失优化节点属性。

Result: 实验表明，ClustGDD在五个基准数据集上的节点分类任务中表现优于或匹配现有方法，且速度快得多。

Conclusion: ClustGDD是一种高效且有效的GDD方法，显著提升了GNN训练性能。

Abstract: Despite plentiful successes achieved by graph representation learning in
various domains, the training of graph neural networks (GNNs) still remains
tenaciously challenging due to the tremendous computational overhead needed for
sizable graphs in practice. Recently, graph data distillation (GDD), which
seeks to distill large graphs into compact and informative ones, has emerged as
a promising technique to enable efficient GNN training. However, most existing
GDD works rely on heuristics that align model gradients or representation
distributions on condensed and original graphs, leading to compromised result
quality, expensive training for distilling large graphs, or both. Motivated by
this, this paper presents an efficient and effective GDD approach, ClustGDD.
Under the hood, ClustGDD resorts to synthesizing the condensed graph and node
attributes through fast and theoretically-grounded clustering that minimizes
the within-cluster sum of squares and maximizes the homophily on the original
graph. The fundamental idea is inspired by our empirical and theoretical
findings unveiling the connection between clustering and empirical condensation
quality using Fr\'echet Inception Distance, a well-known quality metric for
synthetic images. Furthermore, to mitigate the adverse effects caused by the
homophily-based clustering, ClustGDD refines the nodal attributes of the
condensed graph with a small augmentation learned via class-aware graph
sampling and consistency loss. Our extensive experiments exhibit that GNNs
trained over condensed graphs output by ClustGDD consistently achieve superior
or comparable performance to state-of-the-art GDD methods in terms of node
classification on five benchmark datasets, while being orders of magnitude
faster.

</details>


### [196] [Interpretable Credit Default Prediction with Ensemble Learning and SHAP](https://arxiv.org/abs/2505.20815)
*Shiqi Yang,Ziyi Huang,Wengran Xiao,Xinyu Shen*

Key words: 信用违约预测, 机器学习, 集成学习, SHAP分析, 特征重要性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究构建了一个基于机器学习的信用违约预测框架，通过比较多种分类算法，发现集成学习方法在处理复杂非线性关系和数据不平衡问题时表现优越，并利用SHAP方法提升了模型可解释性。

Motivation: 解决信用违约预测问题，评估不同机器学习算法在信用风险控制中的性能，以支持智能风控系统的开发。

Method: 在Home Credit数据集上进行预处理、特征工程和模型训练，比较了逻辑回归、随机森林、XGBoost、LightGBM等算法在准确率、精确率和召回率上的表现。

Result: 集成学习方法（如XGBoost、LightGBM）在预测性能和鲁棒性上表现最佳，SHAP分析显示外部信用评分变量对模型决策影响最大。

Conclusion: 研究为信用风险控制系统的智能化提供了技术支持和参考，集成学习结合SHAP方法可提升模型性能和可解释性。

Abstract: This study focuses on the problem of credit default prediction, builds a
modeling framework based on machine learning, and conducts comparative
experiments on a variety of mainstream classification algorithms. Through
preprocessing, feature engineering, and model training of the Home Credit
dataset, the performance of multiple models including logistic regression,
random forest, XGBoost, LightGBM, etc. in terms of accuracy, precision, and
recall is evaluated. The results show that the ensemble learning method has
obvious advantages in predictive performance, especially in dealing with
complex nonlinear relationships between features and data imbalance problems.
It shows strong robustness. At the same time, the SHAP method is used to
analyze the importance and dependency of features, and it is found that the
external credit score variable plays a dominant role in model decision making,
which helps to improve the model's interpretability and practical application
value. The research results provide effective reference and technical support
for the intelligent development of credit risk control systems.

</details>


### [197] [HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic Sequence Modeling](https://arxiv.org/abs/2505.20836)
*Hexiong Yang,Mingrui Chen,Huaibo Huang,Junxian Duan,Jie Cao,Zhen Zhou,Ran He*

Key words: DNA序列建模,自监督预训练,混合架构蒸馏,分组掩码,特征对齐

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种混合架构蒸馏（HAD）方法，结合蒸馏和重建任务，提升DNA序列建模的自监督预训练效率，效果优于同类模型甚至部分超越教师模型。

Motivation: 现有DNA序列建模方法依赖大数据或大模型，计算负担重，需更高效方法。

Method: 采用NTv2-500M作为教师模型，设计分组掩码策略，对齐可见令牌特征并重建不可见令牌。

Result: 在Nucleotide Transformer Benchmark和Genomic Benchmark上表现优异，部分任务超越教师模型。

Conclusion: HAD方法高效且有效，能深入理解基因组序列内在表示模式。

Abstract: Inspired by the great success of Masked Language Modeling (MLM) in the
natural language domain, the paradigm of self-supervised pre-training and
fine-tuning has also achieved remarkable progress in the field of DNA sequence
modeling. However, previous methods often relied on massive pre-training data
or large-scale base models with huge parameters, imposing a significant
computational burden. To address this, many works attempted to use more compact
models to achieve similar outcomes but still fell short by a considerable
margin. In this work, we propose a Hybrid Architecture Distillation (HAD)
approach, leveraging both distillation and reconstruction tasks for more
efficient and effective pre-training. Specifically, we employ the NTv2-500M as
the teacher model and devise a grouping masking strategy to align the feature
embeddings of visible tokens while concurrently reconstructing the invisible
tokens during MLM pre-training. To validate the effectiveness of our proposed
method, we conducted comprehensive experiments on the Nucleotide Transformer
Benchmark and Genomic Benchmark. Compared to models with similar parameters,
our model achieved excellent performance. More surprisingly, it even surpassed
the distillation ceiling-teacher model on some sub-tasks, which is more than
500 $\times$ larger. Lastly, we utilize t-SNE for more intuitive visualization,
which shows that our model can gain a sophisticated understanding of the
intrinsic representation pattern in genomic sequences.

</details>


### [198] [FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration](https://arxiv.org/abs/2505.20839)
*Daehyeon Baek,Jieun Choi,Jimyoung Son,Kyungmin Bin,Seungbeom Choi,Kihyo Moon,Minsung Jang,Hyojung Lee*

Key words: 大语言模型,量化,推理加速,INT4-FP8,离群值平滑,RoPE

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FireQ是一种针对大语言模型（LLM）的INT4-FP8混合量化框架，通过优化权重和激活值的量化策略以及引入三阶段流水线技术，显著提升了推理速度和首次token生成时间，同时通过新颖的离群值平滑技术最小化了量化带来的精度损失。

Motivation: 随着大语言模型的普及，内存带宽限制严重影响了推理吞吐量，这促使了后训练量化（PTQ）的需求。FireQ旨在通过混合量化和优化技术解决这一问题，提升模型推理效率。

Method: FireQ采用INT4-FP8混合量化策略：线性层权重和键值量化为INT4，激活和查询量化为FP8；并引入三阶段流水线技术优化预填充阶段。此外，针对量化的精度损失，提出了分别为线性和注意力层设计的离群值平滑技术，包括针对FP8量化的每张量缩放和通道级缩放，以及针对RoPE的位置嵌入量化策略。

Result: FireQ在Llama2-7B的FFN层中实现了1.68倍的推理加速，在Llama3-8B的预填充阶段比QServe快1.26倍，且精度损失可忽略不计。

Conclusion: FireQ通过混合量化和专门优化的量化策略，显著提升了LLM的推理效率，同时保持了高精度，为大语言模型的部署提供了高效解决方案。

Abstract: As large language models become increasingly prevalent, memory bandwidth
constraints significantly limit inference throughput, motivating post-training
quantization (PTQ). In this paper, we propose FireQ, a co-designed PTQ
framework and an INT4-FP8 matrix multiplication kernel that accelerates LLM
inference across all linear layers. Specifically, FireQ quantizes linear layer
weights and key-values to INT4, and activations and queries to FP8,
significantly enhancing throughput. Additionally, we introduce a three-stage
pipelining for the prefill phase, which modifies the FlashAttention-3 kernel,
effectively reducing time-to-first-token in the prefill phase. To minimize
accuracy loss from quantization, we develop novel outlier smoothing techniques
tailored separately for linear and attention layers. In linear layers, we
explicitly use per-tensor scaling to prevent underflow caused by the FP8
quantization scaling factor of INT4 quantization, and channel-wise scaling to
compensate for coarse granularity of INT4. In attention layers, we address
quantization challenges posed by rotary positional embeddings (RoPE) by
combining pre-RoPE and post-RoPE scaling strategies. FireQ significantly
outperforms state-of-the-art methods, achieving 1.68x faster inference in
feed-forward network layers on Llama2-7B and 1.26x faster prefill phase
performance on Llama3-8B compared to QServe, with negligible accuracy loss.

</details>


### [199] [Aggregation Buffer: Revisiting DropEdge with a New Parameter Block](https://arxiv.org/abs/2505.20840)
*Dooho Lee,Myeong Kong,Sagad Hamid,Cheonwoo Lee,Jaemin Yoo*

Key words: GNN, DropEdge, Aggregation Buffer, 数据增强, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: DropEdge是一种GNN数据增强技术，通过随机移除边减少过拟合，但其性能提升有限。本文提出Aggregation Buffer，提升GNN鲁棒性并解决DropEdge的限制，在多个数据集上表现优异。

Motivation: DropEdge虽能减少过拟合，但其性能提升有限，主要原因是GNN架构的限制。本文旨在分析这一限制并提出改进方案。

Method: 提出Aggregation Buffer，一个参数块，用于提升GNN鲁棒性并解决DropEdge的局限性，兼容任何GNN模型。

Result: 在多个数据集上表现一致提升，有效解决度偏差和结构差异等问题。

Conclusion: Aggregation Buffer是一种统一解决方案，显著提升GNN性能并解决DropEdge的限制。

Abstract: We revisit DropEdge, a data augmentation technique for GNNs which randomly
removes edges to expose diverse graph structures during training. While being a
promising approach to effectively reduce overfitting on specific connections in
the graph, we observe that its potential performance gain in supervised
learning tasks is significantly limited. To understand why, we provide a
theoretical analysis showing that the limited performance of DropEdge comes
from the fundamental limitation that exists in many GNN architectures. Based on
this analysis, we propose Aggregation Buffer, a parameter block specifically
designed to improve the robustness of GNNs by addressing the limitation of
DropEdge. Our method is compatible with any GNN model, and shows consistent
performance improvements on multiple datasets. Moreover, our method effectively
addresses well-known problems such as degree bias or structural disparity as a
unifying solution. Code and datasets are available at
https://github.com/dooho00/agg-buffer.

</details>


### [200] [Cooperation of Experts: Fusing Heterogeneous Information with Large Margin](https://arxiv.org/abs/2505.20853)
*Shuo Wang,Shunyang Huang,Jinghui Yuan,Zhixiang Shen,Zhao Kang*

Key words: 异质信息融合,专家协作,多重网络,大间隔机制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了专家协作(CoE)框架，通过将多类型信息编码为统一异质多重网络，解决异质数据融合的挑战，并利用专家协作和大间隔机制优化模型，表现出优越性能。

Motivation: 现有方法难以处理不同语义空间中对象模式的异质性，需要一种更灵活强大的模型来融合多类型信息。

Method: CoE框架通过专用编码器作为领域专家学习特定语义空间的关系模式，并通过大间隔机制和优化策略实现专家协作。

Result: 理论分析验证了框架的可行性和稳定性，多个基准实验显示其性能优越且适用广泛。

Conclusion: CoE框架有效解决了异质信息融合问题，具有强大的实际应用潜力。

Abstract: Fusing heterogeneous information remains a persistent challenge in modern
data analysis. While significant progress has been made, existing approaches
often fail to account for the inherent heterogeneity of object patterns across
different semantic spaces. To address this limitation, we propose the
Cooperation of Experts (CoE) framework, which encodes multi-typed information
into unified heterogeneous multiplex networks. By overcoming modality and
connection differences, CoE provides a powerful and flexible model for
capturing the intricate structures of real-world complex data. In our
framework, dedicated encoders act as domain-specific experts, each specializing
in learning distinct relational patterns in specific semantic spaces. To
enhance robustness and extract complementary knowledge, these experts
collaborate through a novel large margin mechanism supported by a tailored
optimization strategy. Rigorous theoretical analyses guarantee the framework's
feasibility and stability, while extensive experiments across diverse
benchmarks demonstrate its superior performance and broad applicability. Our
code is available at https://github.com/strangeAlan/CoE.

</details>


### [201] [Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization](https://arxiv.org/abs/2505.20881)
*Yiding Shi,Jianan Zhou,Wen Song,Jieyi Bi,Yaoxin Wu,Jie Zhang*

Key words: 大语言模型, 组合优化问题, 元学习, 多任务学习, 启发式算法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: MoH 是一种基于大语言模型（LLMs）的元优化框架，通过自调用构建多样化优化器，无需依赖预定义的进化计算（EC）优化器，并在多任务训练中提升泛化能力，在组合优化问题（COPs）中实现最优性能。

Motivation: 现有方法依赖手动预定义的 EC 优化器和单任务训练，限制了探索多样启发式算法和泛化能力。MoH 旨在消除这些限制，通过元学习构建高效优化器。

Method: MoH 利用 LLMs 迭代优化元优化器，自调用构建多样优化器，并使用多任务训练增强泛化性。

Result: 实验表明，MoH 在经典 COPs 中构建了高效且可解释的元优化器，尤其在跨尺度任务中表现最佳。

Conclusion: MoH 是一种无需预定义优化器的创新框架，通过元学习和多任务训练提升启发式算法的多样性和泛化能力。

Abstract: Heuristic design with large language models (LLMs) has emerged as a promising
approach for tackling combinatorial optimization problems (COPs). However,
existing approaches often rely on manually predefined evolutionary computation
(EC) optimizers and single-task training schemes, which may constrain the
exploration of diverse heuristic algorithms and hinder the generalization of
the resulting heuristics. To address these issues, we propose Meta-Optimization
of Heuristics (MoH), a novel framework that operates at the optimizer level,
discovering effective optimizers through the principle of meta-learning.
Specifically, MoH leverages LLMs to iteratively refine a meta-optimizer that
autonomously constructs diverse optimizers through (self-)invocation, thereby
eliminating the reliance on a predefined EC optimizer. These constructed
optimizers subsequently evolve heuristics for downstream tasks, enabling
broader heuristic exploration. Moreover, MoH employs a multi-task training
scheme to promote its generalization capability. Experiments on classic COPs
demonstrate that MoH constructs an effective and interpretable meta-optimizer,
achieving state-of-the-art performance across various downstream tasks,
particularly in cross-size settings.

</details>


### [202] [Fedivertex: a Graph Dataset based on Decentralized Social Networks for Trustworthy Machine Learning](https://arxiv.org/abs/2505.20882)
*Marc Damie,Edwige Cyffers*

Key words: 去中心化机器学习,Fediverse,图数据集,社交网络,基准测试

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 介绍了一个名为Fedivertex的新数据集，包含来自Fediverse的182个图，覆盖7个社交网络，用于去中心化机器学习的基准测试。

Motivation: 现有图数据集多为商业社交网络，受平台和推荐算法影响大，而Fediverse提供了真实世界的替代方案。

Method: 通过每周爬取7个Fediverse社交网络数据，构建了包含182个图的Fedivertex数据集，并提供了Python工具包。

Result: 数据集展示了Fediverse的动态特性，可用于新任务如‘去联邦化’（链接删除）的研究。

Conclusion: Fedivertex为去中心化算法研究提供了更真实的社交网络数据支持。

Abstract: Decentralized machine learning - where each client keeps its own data locally
and uses its own computational resources to collaboratively train a model by
exchanging peer-to-peer messages - is increasingly popular, as it enables
better scalability and control over the data. A major challenge in this setting
is that learning dynamics depend on the topology of the communication graph,
which motivates the use of real graph datasets for benchmarking decentralized
algorithms. Unfortunately, existing graph datasets are largely limited to
for-profit social networks crawled at a fixed point in time and often collected
at the user scale, where links are heavily influenced by the platform and its
recommendation algorithms. The Fediverse, which includes several free and
open-source decentralized social media platforms such as Mastodon, Misskey, and
Lemmy, offers an interesting real-world alternative. We introduce Fedivertex, a
new dataset of 182 graphs, covering seven social networks from the Fediverse,
crawled weekly over 14 weeks. We release the dataset along with a Python
package to facilitate its use, and illustrate its utility on several tasks,
including a new defederation task, which captures a process of link deletion
observed on these networks.

</details>


### [203] [Improved Bounds for Swap Multicalibration and Swap Omniprediction](https://arxiv.org/abs/2505.20885)
*Haipeng Luo,Spandan Senapati,Vatsal Sharan*

Key words: 多校准, 全域预测, 在线学习, 样本复杂度, 误差率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种高效算法，显著提升了多校准和全域预测在分布和在线设置中的误差率，同时还改进了样本复杂度结果。

Motivation: 解决Garg等人（2024）提出的开放性问题，即是否能高效实现对有界线性函数的$O(\sqrt{T})$ $ℓ_{2}$-多校准误差。

Method: 提出一种高效算法，实现$O(T^{\frac{1}{3}})$ $ℓ_{2}$-交换多校准误差，并进一步推广到$ℓ_{1}$-交换多校准和全域预测。

Result: 算法在$ℓ_{1}$-交换多校准和全域预测中实现了$O(T^{\frac{2}{3}})$误差，显著优于之前的最佳$O(T^{\frac{7}{8}})$。在分布设置中，样本复杂度也显著改进。

Conclusion: 该算法在多校准和全域预测领域取得了显著进展，提供了更高效的误差率和样本复杂度。

Abstract: In this paper, we consider the related problems of multicalibration -- a
multigroup fairness notion and omniprediction -- a simultaneous loss
minimization paradigm, both in the distributional and online settings. The
recent work of Garg et al. (2024) raised the open problem of whether it is
possible to efficiently achieve $O(\sqrt{T})$ $\ell_{2}$-multicalibration error
against bounded linear functions. In this paper, we answer this question in a
strongly affirmative sense. We propose an efficient algorithm that achieves
$O(T^{\frac{1}{3}})$ $\ell_{2}$-swap multicalibration error (both in high
probability and expectation). On propagating this bound onward, we obtain
significantly improved rates for $\ell_{1}$-swap multicalibration and swap
omniprediction for a loss class of convex Lipschitz functions. In particular,
we show that our algorithm achieves $O(T^{\frac{2}{3}})$ $\ell_{1}$-swap
multicalibration and swap omniprediction errors, thereby improving upon the
previous best-known bound of $O(T^{\frac{7}{8}})$. As a consequence of our
improved online results, we further obtain several improved sample complexity
rates in the distributional setting. In particular, we establish a
$O(\varepsilon ^ {-3})$ sample complexity of efficiently learning an
$\varepsilon$-swap omnipredictor for the class of convex and Lipschitz
functions, $O(\varepsilon ^{-2.5})$ sample complexity of efficiently learning
an $\varepsilon$-swap agnostic learner for the squared loss, and $O(\varepsilon
^ {-5}), O(\varepsilon ^ {-2.5})$ sample complexities of learning $\ell_{1},
\ell_{2}$-swap multicalibrated predictors against linear functions, all of
which significantly improve on the previous best-known bounds.

</details>


### [204] [One-Time Soft Alignment Enables Resilient Learning without Weight Transport](https://arxiv.org/abs/2505.20892)
*Jeonghwan Cheon,Jaehyuk Bae,Se-Bum Paik*

Key words: 反向传播, 反馈对齐, 权重初始化, 深度网络, 对抗鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种初始化方法，通过前向与反馈权重的一次性软对齐，使深度网络无需学习过程中权重传输即可达到与反向传播相当的性能。

Motivation: 反向传播依赖对称权重传输和全局同步，计算成本高且生物不可信。反馈对齐虽避免了权重对称性，但学习性能差且不稳定，尤其在深度网络中。

Method: 在初始化时一次性软对齐前向与反馈权重，促进稳定的误差最小化。通过频谱分析验证其对梯度流和平坦极小值的改善效果。

Result: 该方法使深度网络性能接近反向传播，且梯度更平滑、收敛更稳定，泛化性和鲁棒性更优。适度偏离对称性还能提升对抗鲁棒性。

Conclusion: 通过简单初始化策略，可在生物可信且资源高效的方式下实现深度网络的有效学习。

Abstract: Backpropagation is the cornerstone of deep learning, but its reliance on
symmetric weight transport and global synchronization makes it computationally
expensive and biologically implausible. Feedback alignment offers a promising
alternative by approximating error gradients through fixed random feedback,
thereby avoiding symmetric weight transport. However, this approach often
struggles with poor learning performance and instability, especially in deep
networks. Here, we show that a one-time soft alignment between forward and
feedback weights at initialization enables deep networks to achieve performance
comparable to backpropagation, without requiring weight transport during
learning. This simple initialization condition guides stable error minimization
in the loss landscape, improving network trainability. Spectral analyses
further reveal that initial alignment promotes smoother gradient flow and
convergence to flatter minima, resulting in better generalization and
robustness. Notably, we also find that allowing moderate deviations from exact
weight symmetry can improve adversarial robustness compared to standard
backpropagation. These findings demonstrate that a simple initialization
strategy can enable effective learning in deep networks in a biologically
plausible and resource-efficient manner.

</details>


### [205] [DeepConvContext: A Multi-Scale Approach to Timeseries Classification in Human Activity Recognition](https://arxiv.org/abs/2505.20894)
*Marius Bock,Michael Moeller,Kristof Van Laerhoven*

Key words: Human Activity Recognition, DeepConvContext, LSTM, temporal modeling

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出DeepConvContext，一种多尺度时间序列分类框架，通过建模窗口内和窗口间的时间模式，显著提升HAR任务的性能。

Motivation: 传统HAR方法依赖滑动窗口独立分类，限制了长范围时间依赖建模能力。

Method: 提出了DeepConvContext框架，结合LSTM建模多尺度时间模式，而非依赖注意力机制。

Result: 在六个常用HAR基准上，F1分数平均提升10%，最高达21%。

Conclusion: LSTM在建模惯性传感器数据时优于注意力机制，DeepConvContext为HAR任务提供有效解决方案。

Abstract: Despite recognized limitations in modeling long-range temporal dependencies,
Human Activity Recognition (HAR) has traditionally relied on a sliding window
approach to segment labeled datasets. Deep learning models like the
DeepConvLSTM typically classify each window independently, thereby restricting
learnable temporal context to within-window information. To address this
constraint, we propose DeepConvContext, a multi-scale time series
classification framework for HAR. Drawing inspiration from the vision-based
Temporal Action Localization community, DeepConvContext models both intra- and
inter-window temporal patterns by processing sequences of time-ordered windows.
Unlike recent HAR models that incorporate attention mechanisms, DeepConvContext
relies solely on LSTMs -- with ablation studies demonstrating the superior
performance of LSTMs over attention-based variants for modeling inertial sensor
data. Across six widely-used HAR benchmarks, DeepConvContext achieves an
average 10% improvement in F1-score over the classic DeepConvLSTM, with gains
of up to 21%. Code to reproduce our experiments is publicly available via
github.com/mariusbock/context_har.

</details>


### [206] [How Do Transformers Learn Variable Binding in Symbolic Programs?](https://arxiv.org/abs/2505.20896)
*Yiwei Wu,Atticus Geiger,Raphaël Millière*

Key words: 变量绑定, Transformer, 神经符号计算, 可寻址内存, 残差流

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 这篇论文研究了神经网络（特别是Transformer）如何在缺乏内置绑定操作的情况下，通过训练学会变量绑定能力。研究表明，模型经历了三个训练阶段，最终学会利用残差流作为可寻址内存空间，实现系统化的变量解引用。

Motivation: 变量绑定是符号计算和认知的基础，但现代神经网络缺乏内置的绑定操作，如何实现这一能力尚不明确。论文旨在探索神经网络如何通过训练习得变量解引用能力。

Method: 通过训练一个Transformer模型解引用符号程序中的变量，程序中的变量可以是数值常量或其他变量，解引用需要跟踪最多四步的变量赋值链，同时包含分散注意力的无关赋值链。使用因果干预分析模型的内部机制。

Result: 模型在训练中经历了三个阶段：随机预测、浅层启发式（优先早期赋值）和系统化解引用机制的形成。研究发现模型利用残差流作为可寻址内存空间，并通过专用注意力头跨位置路由信息，动态跟踪变量绑定。

Conclusion: 研究表明Transformer模型无需显式架构支持即可学会系统化的变量绑定，弥合了连接主义和符号主义方法的鸿沟。

Abstract: Variable binding -- the ability to associate variables with values -- is
fundamental to symbolic computation and cognition. Although classical
architectures typically implement variable binding via addressable memory, it
is not well understood how modern neural networks lacking built-in binding
operations may acquire this capacity. We investigate this by training a
Transformer to dereference queried variables in symbolic programs where
variables are assigned either numerical constants or other variables. Each
program requires following chains of variable assignments up to four steps deep
to find the queried value, and also contains irrelevant chains of assignments
acting as distractors. Our analysis reveals a developmental trajectory with
three distinct phases during training: (1) random prediction of numerical
constants, (2) a shallow heuristic prioritizing early variable assignments, and
(3) the emergence of a systematic mechanism for dereferencing assignment
chains. Using causal interventions, we find that the model learns to exploit
the residual stream as an addressable memory space, with specialized attention
heads routing information across token positions. This mechanism allows the
model to dynamically track variable bindings across layers, resulting in
accurate dereferencing. Our results show how Transformer models can learn to
implement systematic variable binding without explicit architectural support,
bridging connectionist and symbolic approaches.

</details>


### [207] [Humble AI in the real-world: the case of algorithmic hiring](https://arxiv.org/abs/2505.20918)
*Rahul Nair,Inge Vejsbjerg,Elizabeth Daly,Christos Varytimidis,Bran Knowles*

Key words: 谦逊AI,算法招聘,不确定性量化,信任,刻板印象

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文主张通过质疑、好奇和承诺的‘谦逊AI’原则，在算法招聘中实现谨慎的AI应用，提出了不确定性量化等方法，并初步讨论了其对信任的影响。

Motivation: 探讨如何在算法招聘中实现谨慎的AI开发与部署，解决传统框架难以评估的误识别和刻板印象问题。

Method: 采用不确定性量化、熵估计和用户界面设计来实践谦逊AI原则，并招募招聘人员进行焦点小组讨论。

Result: 展示了技术可行性，初步表明谦逊AI原则可应用于实际场景，但需进一步用户研究验证信任效果。

Conclusion: 谦逊AI原则在算法招聘中具有实践潜力，未来研究需验证其是否能通过增加认知负担来提升信任度。

Abstract: Humble AI (Knowles et al., 2023) argues for cautiousness in AI development
and deployments through scepticism (accounting for limitations of statistical
learning), curiosity (accounting for unexpected outcomes), and commitment
(accounting for multifaceted values beyond performance). We present a
real-world case study for humble AI in the domain of algorithmic hiring.
Specifically, we evaluate virtual screening algorithms in a widely used hiring
platform that matches candidates to job openings. There are several challenges
in misrecognition and stereotyping in such contexts that are difficult to
assess through standard fairness and trust frameworks; e.g., someone with a
non-traditional background is less likely to rank highly. We demonstrate
technical feasibility of how humble AI principles can be translated to practice
through uncertainty quantification of ranks, entropy estimates, and a user
experience that highlights algorithmic unknowns. We describe preliminary
discussions with focus groups made up of recruiters. Future user studies seek
to evaluate whether the higher cognitive load of a humble AI system fosters a
climate of trust in its outcomes.

</details>


### [208] [Label Leakage in Federated Inertial-based Human Activity Recognition](https://arxiv.org/abs/2505.20924)
*Marius Bock,Maximilian Hopp,Kristof Van Laerhoven,Michael Moeller*

Key words: 联邦学习, 人类活动识别, 标签泄漏, 局部差分隐私, 梯度攻击

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了联邦学习中通过共享梯度重构标签的攻击在人类活动识别(HAR)中的有效性，发现活动类别数、采样策略和类别不平衡是关键因素，重构准确率高达90%。局部差分隐私技术保护有限，提出了隐私保护的实践建议。

Motivation: 活动标签的敏感性促使研究梯度泄漏攻击对HAR的影响，填补了此前在HAR领域未深入探讨此类攻击的空缺。

Method: 使用梯度泄漏攻击对HAR基准数据集进行了测试，分析了类别数、采样策略和类别不平衡对标签泄露的影响，并评估了局部差分隐私技术的保护效果。

Result: 重构标签准确率可达90%；梯度噪声和裁剪等局部差分隐私技术对防止攻击效果有限。

Conclusion: 提出了隐私保护HAR系统的实践建议，并指出了未来研究的开放性问题。

Abstract: While prior work has shown that Federated Learning updates can leak sensitive
information, label reconstruction attacks, which aim to recover input labels
from shared gradients, have not yet been examined in the context of Human
Activity Recognition (HAR). Given the sensitive nature of activity labels, this
study evaluates the effectiveness of state-of-the-art gradient-based label
leakage attacks on HAR benchmark datasets. Our findings show that the number of
activity classes, sampling strategy, and class imbalance are critical factors
influencing the extent of label leakage, with reconstruction accuracies
reaching up to 90% on two benchmark datasets, even for trained models.
Moreover, we find that Local Differential Privacy techniques such as gradient
noise and clipping offer only limited protection, as certain attacks still
reliably infer both majority and minority class labels. We conclude by offering
practical recommendations for the privacy-aware deployment of federated HAR
systems and identify open challenges for future research. Code to reproduce our
experiments is publicly available via github.com/mariusbock/leakage_har.

</details>


### [209] [MLMC-based Resource Adequacy Assessment with Active Learning Trained Surrogate Models](https://arxiv.org/abs/2505.20930)
*Ruiqi Zhang,Simon H. Tindemans*

Key words: 多级蒙特卡洛, 代理模型, 主动学习, 电力系统, 可靠性评估

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种考虑训练时间的速度指标，并采用主动学习方法减少MLMC框架中的标注需求，有效提升了可靠性评估的效率。

Motivation: 尽管数据驱动的代理模型在多级蒙特卡洛(MLMC)框架中表现优异，但在资源充足性评估中，缺乏预标注数据集及训练时间过长的问题限制了其效率。因此，需要一种综合考虑训练时间与评估效率的方法。

Method: 引入一个包含训练时间的速度指标，并提出基于委员会投票的主动学习策略以减少标注需求，优化MLMC效率。

Result: 案例研究表明，在给定方差阈值内，主动学习方法显著减少了训练时间，同时提升了MLMC的评估效率。

Conclusion: 主动学习在MLMC框架中能够有效平衡训练成本与评估效率，为复杂电力系统的可靠性评估提供了实用解决方案。

Abstract: Multilevel Monte Carlo (MLMC) is a flexible and effective variance reduction
technique for accelerating reliability assessments of complex power system.
Recently, data-driven surrogate models have been proposed as lower-level models
in the MLMC framework due to their high correlation and negligible execution
time once trained. However, in resource adequacy assessments, pre-labeled
datasets are typically unavailable. For large-scale systems, the efficiency
gains from surrogate models are often offset by the substantial time required
for labeling training data. Therefore, this paper introduces a speed metric
that accounts for training time in evaluating MLMC efficiency. Considering the
total time budget is limited, a vote-by-committee active learning approach is
proposed to reduce the required labeling calls. A case study demonstrates that,
within practical variance thresholds, active learning enables significantly
improved MLMC efficiency with reduced training effort, compared to regular
surrogate modelling approaches.

</details>


### [210] [NatADiff: Adversarial Boundary Guidance for Natural Adversarial Diffusion](https://arxiv.org/abs/2505.20934)
*Max Collins,Jordan Vice,Tim French,Ajmal Mian*

Key words: 对抗样本, 去噪扩散模型, 鲁棒性, 迁移性, FID

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为‘NatADiff’的自然对抗样本生成方法，利用去噪扩散模型生成更接近真实场景的对抗样本，显示出更高的跨模型迁移性和图像保真度。

Motivation: 现有对抗样本研究多关注受限样本，无法准确反映真实场景中的测试时错误。本文提出一种更自然的对抗样本生成方法，以提升模型鲁棒性。

Method: 结合去噪扩散模型和时间旅行采样，引导扩散轨迹至真实类和对抗类的交集，增强攻击迁移性并保持图像质量。

Result: NatADiff在攻击成功率上与当前最优技术相当，同时在跨模型迁移性和对抗样本自然度（FID指标）上表现更优。

Conclusion: NatADiff生成的对抗样本不仅迁移性更强，而且更贴近真实测试时错误，为模型鲁棒性研究提供了新方向。

Abstract: Adversarial samples exploit irregularities in the manifold ``learned'' by
deep learning models to cause misclassifications. The study of these
adversarial samples provides insight into the features a model uses to classify
inputs, which can be leveraged to improve robustness against future attacks.
However, much of the existing literature focuses on constrained adversarial
samples, which do not accurately reflect test-time errors encountered in
real-world settings. To address this, we propose `NatADiff', an adversarial
sampling scheme that leverages denoising diffusion to generate natural
adversarial samples. Our approach is based on the observation that natural
adversarial samples frequently contain structural elements from the adversarial
class. Deep learning models can exploit these structural elements to shortcut
the classification process, rather than learning to genuinely distinguish
between classes. To leverage this behavior, we guide the diffusion trajectory
towards the intersection of the true and adversarial classes, combining
time-travel sampling with augmented classifier guidance to enhance attack
transferability while preserving image fidelity. Our method achieves comparable
attack success rates to current state-of-the-art techniques, while exhibiting
significantly higher transferability across model architectures and better
alignment with natural test-time errors as measured by FID. These results
demonstrate that NatADiff produces adversarial samples that not only transfer
more effectively across models, but more faithfully resemble naturally
occurring test-time errors.

</details>


### [211] [Revisiting Sparsity Constraint Under High-Rank Property in Partial Multi-Label Learning](https://arxiv.org/abs/2505.20938)
*Chongjie Si,Yidan Cui,Fuchao Yang,Xiaokang Yang,Wei Shen*

Key words: Partial Multi-Label Learning, PML, Schirn, sparsity constraint, high-rank property

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新的方法Schirn，用于解决部分多标签学习（PML）中的候选标签集噪声问题，通过引入稀疏约束和高秩属性，显著优于现有方法。

Motivation: 现有的PML方法依赖稀疏性和低秩性假设，但这些假设在实际场景中冲突且不实用。论文旨在解决这一问题，提出更适应真实场景的方法。

Method: 提出Schirn方法，对噪声标签矩阵施加稀疏约束，同时对预测标签矩阵施加高秩属性。

Result: 实验表明Schirn在真实世界PML任务中优于现有方法，验证了其有效性。

Conclusion: Schirn通过引入稀疏约束和高秩属性，成功解决了PML中的噪声问题，适用于更真实的场景。

Abstract: Partial Multi-Label Learning (PML) extends the multi-label learning paradigm
to scenarios where each sample is associated with a candidate label set
containing both ground-truth labels and noisy labels. Existing PML methods
commonly rely on two assumptions: sparsity of the noise label matrix and
low-rankness of the ground-truth label matrix. However, these assumptions are
inherently conflicting and impractical for real-world scenarios, where the true
label matrix is typically full-rank or close to full-rank. To address these
limitations, we demonstrate that the sparsity constraint contributes to the
high-rank property of the predicted label matrix. Based on this, we propose a
novel method Schirn, which introduces a sparsity constraint on the noise label
matrix while enforcing a high-rank property on the predicted label matrix.
Extensive experiments demonstrate the superior performance of Schirn compared
to state-of-the-art methods, validating its effectiveness in tackling
real-world PML challenges.

</details>


### [212] [Efficient Spectral Control of Partially Observed Linear Dynamical Systems](https://arxiv.org/abs/2505.20943)
*Anand Brahmbhatt,Gon Buzaglo,Sofiia Druchyna,Elad Hazan*

Key words: 线性动力系统、部分观测、对抗性干扰、DSC算法、频谱近似

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为DSC的新方法，用于在部分观测和对抗性干扰下控制线性动力系统，其性能与现有最佳方法相当，但在依赖系统稳定性裕度的情况下运行时间显著改善。

Motivation: 现有方法在处理部分观测和对抗性干扰的线性动力系统控制问题时，时间复杂度在系统稳定性裕度方面表现不佳，因此需要一种更高效的算法。

Method: 采用双频谱控制（DSC）算法，通过双卷积与通用频谱滤波器基结合，实现两级频谱近似，从而高效准确地学习最佳线性动力控制器。

Result: DSC算法在遗憾保证上与现有最佳方法相当，且在依赖系统稳定性裕度的情况下，运行时间得到指数级改善。

Conclusion: DSC算法在控制部分观测和对抗性干扰的线性动力系统时，显著提升了计算效率，实现了性能与运行时间的优化。

Abstract: We propose a new method for the problem of controlling linear dynamical
systems under partial observation and adversarial disturbances. Our new
algorithm, Double Spectral Control (DSC), matches the best known regret
guarantees while exponentially improving runtime complexity over previous
approaches in its dependence on the system's stability margin. Our key
innovation is a two-level spectral approximation strategy, leveraging double
convolution with a universal basis of spectral filters, enabling efficient and
accurate learning of the best linear dynamical controllers.

</details>


### [213] [Semantic Communication meets System 2 ML: How Abstraction, Compositionality and Emergent Languages Shape Intelligence](https://arxiv.org/abs/2505.20964)
*Mehdi Bennis,Salem Lahlou*

Key words: 6G, AI, System-2认知, 抽象, 组合性, 涌现通信

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 6G与AI的创新融合需要范式转换，提出基于System-2认知的统一研究框架，聚焦抽象、组合性和涌现通信，构建具备语义理解和目标交互能力的智能系统。

Motivation: 当前6G愿景仍是对5G的渐进改进，而AI模型存在脆弱、数据依赖性强且缺乏鲁棒推理能力的问题，呼吁突破纯技术通信层，实现语义理解和目标驱动交互。

Method: 基于System-2认知三支柱：抽象（从原始数据学习世界模型）、组合性（代数化整合概念与子系统）、涌现通信（智能体自创适应性语言）。

Result: 提出统一框架，整合无线通信、机器学习和机器人技术，支持智能系统的推理、适应与协作能力。

Conclusion: 通过认知原则的整合，为真正智能系统奠定基础，推动6G与AI的深层协同发展。

Abstract: The trajectories of 6G and AI are set for a creative collision. However,
current visions for 6G remain largely incremental evolutions of 5G, while
progress in AI is hampered by brittle, data-hungry models that lack robust
reasoning capabilities. This paper argues for a foundational paradigm shift,
moving beyond the purely technical level of communication toward systems
capable of semantic understanding and effective, goal-oriented interaction. We
propose a unified research vision rooted in the principles of System-2
cognition, built upon three pillars: Abstraction, enabling agents to learn
meaningful world models from raw sensorimotor data; Compositionality, providing
the algebraic tools to combine learned concepts and subsystems; and Emergent
Communication, allowing intelligent agents to create their own adaptive and
grounded languages. By integrating these principles, we lay the groundwork for
truly intelligent systems that can reason, adapt, and collaborate, unifying
advances in wireless communications, machine learning, and robotics under a
single coherent framework.

</details>


### [214] [Understanding the behavior of representation forgetting in continual learning](https://arxiv.org/abs/2505.20970)
*Joonkyu Kim,Yejin Kim,Jy-yong Sohn*

Key words: 持续学习, 表示遗忘, 表示差异, 理论分析, Split-CIFAR100, ImageNet1K

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文通过理论分析和实验验证，提出了“表示差异”这一新指标来量化持续学习中的表示遗忘，发现遗忘速度随网络层深度增加而加快，但网络宽度增加会减缓遗忘。

Motivation: 在持续学习中，灾难性遗忘是一个关键问题，尤其是隐藏层的表示遗忘。为了更好地理解和量化这种现象，需要理论分析和有效指标。

Method: 引入“表示差异”指标，量化持续学习模型两个快照间的表示空间差异，并通过数学分析和真实数据集实验（如Split-CIFAR100和ImageNet1K）验证。

Result: 理论分析表明，遗忘速度随网络层深度增加而加快，但网络宽度增加会减缓遗忘；实验结果支持这一发现。

Conclusion: 表示差异是一个有效且易于分析的表示遗忘替代指标，为持续学习的动态行为提供了理论依据。

Abstract: In continual learning scenarios, catastrophic forgetting of previously
learned tasks is a critical issue, making it essential to effectively measure
such forgetting. Recently, there has been growing interest in focusing on
representation forgetting, the forgetting measured at the hidden layer. In this
paper, we provide the first theoretical analysis of representation forgetting
and use this analysis to better understand the behavior of continual learning.
First, we introduce a new metric called representation discrepancy, which
measures the difference between representation spaces constructed by two
snapshots of a model trained through continual learning. We demonstrate that
our proposed metric serves as an effective surrogate for the representation
forgetting while remaining analytically tractable. Second, through mathematical
analysis of our metric, we derive several key findings about the dynamics of
representation forgetting: the forgetting occurs more rapidly to a higher
degree as the layer index increases, while increasing the width of the network
slows down the forgetting process. Third, we support our theoretical findings
through experiments on real image datasets, including Split-CIFAR100 and
ImageNet1K.

</details>


### [215] [Deep k-grouping: An Unsupervised Learning Framework for Combinatorial Optimization on Graphs and Hypergraphs](https://arxiv.org/abs/2505.20972)
*Sen Bai,Chunqi Yang,Xin Bai,Xin Zhang,Zhengang Jiang*

Key words: AI计算, 组合优化, $k$-grouping, 无监督学习, GPU加速

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Deep $k$-grouping是一种基于无监督学习的组合优化框架，针对大规模图和高阶图的$k$-grouping问题（如着色、划分）提出，通过GPU加速算法和连续松弛策略显著优于现有方法。

Motivation: 现有无监督神经网络求解器在大规模图和高阶图上的$k$-grouping问题中表现受限，需要新的计算框架提升性能。

Method: 提出OH-PUBO公式建模问题，利用GPU加速算法优化无监督训练流程，并通过Gini系数连续松弛退火策略避免局部最优。

Result: 实验表明，Deep $k$-grouping在性能上超越现有神经网络求解器和经典启发式算法（如SCIP和Tabu）。

Conclusion: Deep $k$-grouping为大规模$k$-grouping问题提供了高效、可扩展的解决方案。

Abstract: Along with AI computing shining in scientific discovery, its potential in the
combinatorial optimization (CO) domain has also emerged in recent years. Yet,
existing unsupervised neural network solvers struggle to solve $k$-grouping
problems (e.g., coloring, partitioning) on large-scale graphs and hypergraphs,
due to limited computational frameworks. In this work, we propose Deep
$k$-grouping, an unsupervised learning-based CO framework. Specifically, we
contribute: Novel one-hot encoded polynomial unconstrained binary optimization
(OH-PUBO), a formulation for modeling k-grouping problems on graphs and
hypergraphs (e.g., graph/hypergraph coloring and partitioning); GPU-accelerated
algorithms for large-scale k-grouping CO problems. Deep $k$-grouping employs
the relaxation of large-scale OH-PUBO objectives as differentiable loss
functions and trains to optimize them in an unsupervised manner. To ensure
scalability, it leverages GPU-accelerated algorithms to unify the training
pipeline; A Gini coefficient-based continuous relaxation annealing strategy to
enforce discreteness of solutions while preventing convergence to local optima.
Experimental results demonstrate that Deep $k$-grouping outperforms existing
neural network solvers and classical heuristics such as SCIP and Tabu.

</details>


### [216] [Efficient Identity and Position Graph Embedding via Spectral-Based Random Feature Aggregation](https://arxiv.org/abs/2505.20992)
*Meng Qin,Jiahong Liu,Irwin King*

Key words: 图神经网络, 图嵌入, 随机特征聚合, 图信号处理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为随机特征聚合（RFA）的方法，通过图信号处理视角高效生成节点身份和位置嵌入，无需训练即可实现高质量结果。

Motivation: 现有图神经网络（GNN）方法在捕捉节点身份或位置属性时效果不明确，且存在效率和扩展性不足的问题。

Method: 基于图信号处理理论，RFA利用高/低通滤波器和随机噪声输入，通过一次性前向传播生成嵌入，无需可学习参数或训练。

Result: 实验表明，RFA的高通和低通滤波变体分别能有效捕捉节点身份和位置信息，且在质量和效率上优于基线方法。

Conclusion: RFA为图嵌入提供了一种高效且无需训练的解决方案，平衡了质量和效率。

Abstract: Graph neural networks (GNNs), which capture graph structures via a feature
aggregation mechanism following the graph embedding framework, have
demonstrated a powerful ability to support various tasks. According to the
topology properties (e.g., structural roles or community memberships of nodes)
to be preserved, graph embedding can be categorized into identity and position
embedding. However, it is unclear for most GNN-based methods which property
they can capture. Some of them may also suffer from low efficiency and
scalability caused by several time- and space-consuming procedures (e.g.,
feature extraction and training). From a perspective of graph signal
processing, we find that high- and low-frequency information in the graph
spectral domain may characterize node identities and positions, respectively.
Based on this investigation, we propose random feature aggregation (RFA) for
efficient identity and position embedding, serving as an extreme ablation study
regarding GNN feature aggregation. RFA (i) adopts a spectral-based GNN without
learnable parameters as its backbone, (ii) only uses random noises as inputs,
and (iii) derives embeddings via just one feed-forward propagation (FFP).
Inspired by degree-corrected spectral clustering, we further introduce a degree
correction mechanism to the GNN backbone. Surprisingly, our experiments
demonstrate that two variants of RFA with high- and low-pass filters can
respectively derive informative identity and position embeddings via just one
FFP (i.e., without any training). As a result, RFA can achieve a better
trade-off between quality and efficiency for both identity and position
embedding over various baselines.

</details>


### [217] [BIPNN: Learning to Solve Binary Integer Programming via Hypergraph Neural Networks](https://arxiv.org/abs/2505.20997)
*Sen Bai,Chunqi Yang,Xin Bai,Xin Zhang,Zhengang Jiang*

Key words: 二进制整数规划, 超图神经网络, 非线性优化, GPU加速, 连续退火

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出BIPNN，一种基于超图神经网络的非监督学习框架，用于解决非线性二进制整数规划问题，通过GPU加速和连续退火训练显著提升效率。

Motivation: 现有神经网络求解器难以处理非线性二进制整数规划问题，而传统方法因线性松弛导致变量爆炸和计算限制。

Method: BIPNN将非线性BIP问题转化为可微分的多项式损失函数，利用超图神经网络的端到端训练，结合GPU加速和连续退火优化。

Result: 在合成和真实数据集上的实验表明，BIPNN能高效生成离散高质量解，显著降低训练成本。

Conclusion: BIPNN为非线性BIP问题提供了可扩展的解决方案，突破了传统方法的计算瓶颈。

Abstract: Binary (0-1) integer programming (BIP) is pivotal in scientific domains
requiring discrete decision-making. As the advance of AI computing, recent
works explore neural network-based solvers for integer linear programming (ILP)
problems. Yet, they lack scalability for tackling nonlinear challenges. To
handle nonlinearities, state-of-the-art Branch-and-Cut solvers employ linear
relaxations, leading to exponential growth in auxiliary variables and severe
computation limitations. To overcome these limitations, we propose BIPNN
(Binary Integer Programming Neural Network), an unsupervised learning framework
to solve nonlinear BIP problems via hypergraph neural networks (HyperGNN).
Specifically, BIPNN reformulates BIPs-constrained, discrete, and nonlinear
(sin, log, exp) optimization problems-into unconstrained, differentiable, and
polynomial loss functions. The reformulation stems from the observation of a
precise one-to-one mapping between polynomial BIP objectives and hypergraph
structures, enabling the unsupervised training of HyperGNN to optimize BIP
problems in an end-to-end manner. On this basis, we propose a GPU-accelerated
and continuous-annealing-enhanced training pipeline for BIPNN. The pipeline
enables BIPNN to optimize large-scale nonlinear terms in BIPs fully in parallel
via straightforward gradient descent, thus significantly reducing the training
cost while ensuring the generation of discrete, high-quality solutions.
Extensive experiments on synthetic and real-world datasets highlight the
superiority of our approach.

</details>


### [218] [Efficient and Unbiased Sampling from Boltzmann Distributions via Variance-Tuned Diffusion Models](https://arxiv.org/abs/2505.21005)
*Fengzhe Zhang,Laurence I. Midgley,José Miguel Hernández-Lobato*

Key words: 扩散模型,重要性采样,α-散度,噪声协方差,无偏估计

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出VT-DIS方法，通过调整预训练SBDM的噪声协方差来优化重要性采样，减少计算开销并提高采样效率。

Motivation: 针对SBDM中分数估计偏差导致蒙特卡罗估计不精确的问题，传统重要性采样计算成本高且维度扩展性差。

Method: VT-DIS通过最小化α-散度（α=2）调整噪声协方差，并分配单一路径重要性权重，实现无偏估计且计算开销低。

Result: 在DW-4、LJ-13和丙氨酸二肽基准测试中，VT-DIS分别达到约80%、35%和3.5%的有效样本量，计算成本显著低于传统方法。

Conclusion: VT-DIS是一种轻量级后训练方法，显著提高了扩散模型重要性采样的效率。

Abstract: Score-based diffusion models (SBDMs) are powerful amortized samplers for
Boltzmann distributions; however, imperfect score estimates bias downstream
Monte Carlo estimates. Classical importance sampling (IS) can correct this
bias, but computing exact likelihoods requires solving the probability-flow
ordinary differential equation (PF-ODE), a procedure that is prohibitively
costly and scales poorly with dimensionality. We introduce Variance-Tuned
Diffusion Importance Sampling (VT-DIS), a lightweight post-training method that
adapts the per-step noise covariance of a pretrained SBDM by minimizing the
$\alpha$-divergence ($\alpha=2$) between its forward diffusion and reverse
denoising trajectories. VT-DIS assigns a single trajectory-wise importance
weight to the joint forward-reverse process, yielding unbiased expectation
estimates at test time with negligible overhead compared to standard sampling.
On the DW-4, LJ-13, and alanine-dipeptide benchmarks, VT-DIS achieves effective
sample sizes of approximately 80 %, 35 %, and 3.5 %, respectively, while using
only a fraction of the computational budget required by vanilla diffusion + IS
or PF-ODE-based IS.

</details>


### [219] [Federated Instrumental Variable Analysis via Federated Generalized Method of Moments](https://arxiv.org/abs/2505.21012)
*Geetika,Somya Tyagi,Bapi Chatterjee*

Key words: 工具变量分析、联邦学习、广义矩估计、高维数据、隐私保护

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了FedIV和FedGMM方法，用于高维工具变量分析的联邦学习框架，解决了数据隐私问题，并通过理论分析和实验验证了其有效性。

Motivation: 在高维工具变量分析中，传统GMM方法在联邦学习环境下尚无解决方案，需同时保证数据隐私和分析效率。

Method: 提出FedGMM框架，通过联邦零和博弈（非凸非凹极小极大优化问题）构建，并采用联邦梯度下降上升算法（FedGDA）求解。理论分析了客户端局部最优性的存在性。

Result: FedGMM能够一致估计每个参与客户端的局部矩条件，实验验证了方法的有效性。

Conclusion: FedIV和FedGMM为高维工具变量分析提供了联邦学习解决方案，兼顾隐私与性能。

Abstract: Instrumental variables (IV) analysis is an important applied tool for areas
such as healthcare and consumer economics. For IV analysis in high-dimensional
settings, the Generalized Method of Moments (GMM) using deep neural networks
offers an efficient approach. With non-i.i.d. data sourced from scattered
decentralized clients, federated learning is a popular paradigm for training
the models while promising data privacy. However, to our knowledge, no
federated algorithm for either GMM or IV analysis exists to date. In this work,
we introduce federated instrumental variables analysis (FedIV) via federated
generalized method of moments (FedGMM). We formulate FedGMM as a federated
zero-sum game defined by a federated non-convex non-concave minimax
optimization problem, which is solved using federated gradient descent ascent
(FedGDA) algorithm. One key challenge arises in theoretically characterizing
the federated local optimality. To address this, we present properties and
existence results of clients' local equilibria via FedGDA limit points.
Thereby, we show that the federated solution consistently estimates the local
moment conditions of every participating client. The proposed algorithm is
backed by extensive experiments to demonstrate the efficacy of our approach.

</details>


### [220] [NeuralOM: Neural Ocean Model for Subseasonal-to-Seasonal Simulation](https://arxiv.org/abs/2505.21020)
*Yuan Gao,Ruiqi Shu,Hao Wu,Fan Xu,Yanfei Xiang,Ruijian Gou,Qingsong Wen,Xian Wu,Xiaomeng Huang*

Key words: Subseasonal-to-Seasonal (S2S), ocean simulation, machine learning, graph neural network, multi-scale interaction

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为NeuralOM的神经海洋模型，用于次季节至季节（S2S）海洋模拟，通过多尺度交互图神经网络有效模拟海洋系统的多样性物理现象，并在实验评估中证明了其优于现有方法。

Motivation: 当前基于机器学习的S2S海洋模拟模型在物理一致性和海洋系统的慢变特性方面存在不足，因此需要一种更准确且高效的方法来解决这些问题。

Method: 提出了一种多阶段框架来模拟海洋的慢变特性，并引入多尺度交互消息传递模块来捕捉复杂的动力学行为，例如梯度变化和乘法耦合关系。

Result: 实验评估表明，NeuralOM在S2S和极端事件模拟上的性能优于现有最先进模型。

Conclusion: NeuralOM通过结合多尺度交互图神经网络，显著提升了S2S海洋模拟的准确性和物理一致性。

Abstract: Accurate Subseasonal-to-Seasonal (S2S) ocean simulation is critically
important for marine research, yet remains challenging due to its substantial
thermal inertia and extended time delay. Machine learning (ML)-based models
have demonstrated significant advancements in simulation accuracy and
computational efficiency compared to traditional numerical methods.
Nevertheless, a significant limitation of current ML models for S2S ocean
simulation is their inadequate incorporation of physical consistency and the
slow-changing properties of the ocean system. In this work, we propose a neural
ocean model (NeuralOM) for S2S ocean simulation with a multi-scale interactive
graph neural network to emulate diverse physical phenomena associated with
ocean systems effectively. Specifically, we propose a multi-stage framework
tailored to model the ocean's slowly changing nature. Additionally, we
introduce a multi-scale interactive messaging module to capture complex
dynamical behaviors, such as gradient changes and multiplicative coupling
relationships inherent in ocean dynamics. Extensive experimental evaluations
confirm that our proposed NeuralOM outperforms state-of-the-art models in S2S
and extreme event simulation. The codes are available at
https://github.com/YuanGao-YG/NeuralOM.

</details>


### [221] [Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers](https://arxiv.org/abs/2505.21024)
*Charles London,Varun Kanade*

Key words: pause tokens, Transformers, computational expressivity, $\mathsf{AC}^0$, $\mathsf{TC}^0$, parity

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Pause tokens (e.g., "...") enhance Transformer performance in language and math tasks. Theoretically, they increase computational expressivity, enabling Transformers to compute broader function classes like $\mathsf{AC}^0$ and $\mathsf{TC}^0$. Empirically, they help Transformers learn complex functions like parity.

Motivation: To theoretically explain the empirical success of pause tokens in improving Transformer performance and clarify their role in computational expressivity.

Method: Formal separation proofs and empirical tests with Transformers, comparing performance with and without pause tokens.

Result: Pause tokens strictly increase Transformers' expressivity, enabling computation of $\mathsf{AC}^0$ and $\mathsf{TC}^0$ functions. Empirically, they enable learning parity.

Conclusion: Pause tokens are a distinct mechanism for enhancing Transformer reasoning, complementary to chain-of-thought prompting.

Abstract: Pause tokens, simple filler symbols such as "...", consistently improve
Transformer performance on both language and mathematical tasks, yet their
theoretical effect remains unexplained. We provide the first formal separation
result, proving that adding pause tokens to constant-depth, logarithmic-width
Transformers strictly increases their computational expressivity. With
bounded-precision activations, Transformers without pause tokens compute only a
strict subset of $\mathsf{AC}^0$ functions, while adding a polynomial number of
pause tokens allows them to express the entire class. For logarithmic-precision
Transformers, we show that adding pause tokens achieves expressivity equivalent
to $\mathsf{TC}^0$, matching known upper bounds. Empirically, we demonstrate
that two-layer causally masked Transformers can learn parity when supplied with
pause tokens, a function that they appear unable to learn without them. Our
results provide a rigorous theoretical explanation for prior empirical
findings, clarify how pause tokens interact with width, depth, and numeric
precision, and position them as a distinct mechanism, complementary to
chain-of-thought prompting, for enhancing Transformer reasoning.

</details>


### [222] [TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data](https://arxiv.org/abs/2505.21027)
*Zhipeng He,Chun Ouyang,Lijie Wen,Cong Liu,Catarina Moreira*

Key words: 对抗性攻击, 表格数据, 不可感知性, 有效性, 机器学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种新的针对表格数据的对抗性攻击基准，同时评估攻击的有效性和不可感知性，填补了现有研究中忽视不可感知性的空白。

Motivation: 现有对抗性攻击研究主要关注攻击有效性，而忽视了表格数据特有的不可感知性标准，因此需要一个新的基准来填补这一空白。

Method: 通过五种对抗性攻击在四种模型和十一个表格数据集（包括混合和数值型数据集）上的评估，分析攻击的有效性和不可感知性及其相互作用。

Result: 研究发现攻击的有效性和不可感知性之间存在复杂关系，为改进对抗性攻击算法设计提供了重要见解。

Conclusion: 该基准为表格数据对抗性机器学习领域的进一步发展提供了重要参考。

Abstract: Adversarial attacks pose a significant threat to machine learning models by
inducing incorrect predictions through imperceptible perturbations to input
data. While these attacks have been extensively studied in unstructured data
like images, their application to tabular data presents new challenges. These
challenges arise from the inherent heterogeneity and complex feature
interdependencies in tabular data, which differ significantly from those in
image data. To address these differences, it is crucial to consider
imperceptibility as a key criterion specific to tabular data. Most current
research focuses primarily on achieving effective adversarial attacks, often
overlooking the importance of maintaining imperceptibility. To address this
gap, we propose a new benchmark for adversarial attacks on tabular data that
evaluates both effectiveness and imperceptibility. In this study, we assess the
effectiveness and imperceptibility of five adversarial attacks across four
models using eleven tabular datasets, including both mixed and numerical-only
datasets. Our analysis explores how these factors interact and influence the
overall performance of the attacks. We also compare the results across
different dataset types to understand the broader implications of these
findings. The findings from this benchmark provide valuable insights for
improving the design of adversarial attack algorithms, thereby advancing the
field of adversarial machine learning on tabular data.

</details>


### [223] [LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms](https://arxiv.org/abs/2505.21034)
*Wenhu Li,Niki van Stein,Thomas Bäck,Elena Raponi*

Key words: 贝叶斯优化, 大型语言模型, 自动化算法设计, 进化策略, 黑箱优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 利用大型语言模型（LLM）自动生成贝叶斯优化（BO）算法代码，并通过进化策略迭代优化，生成的算法在多数测试中优于现有方法。

Motivation: 传统BO算法设计依赖专家经验，而LLM的进展为自动化算法设计提供了新思路，本文旨在探索LLM自动生成BO算法的可行性。

Method: 采用进化策略指导LLM生成Python代码，保留BO核心组件（初始设计、代理模型、采集函数），并通过BBOB测试评估和迭代优化候选算法。

Result: 在24个BBOB测试函数中，LLM生成的算法在19个（5维情况下）表现优于现有BO基线，并能推广至高维和其他任务。

Conclusion: LLM可作为算法协同设计工具，为BO开发自动化和新算法组合发现提供了新范式。

Abstract: Bayesian optimization (BO) is a powerful class of algorithms for optimizing
expensive black-box functions, but designing effective BO algorithms remains a
manual, expertise-driven task. Recent advancements in Large Language Models
(LLMs) have opened new avenues for automating scientific discovery, including
the automatic design of optimization algorithms. While prior work has used LLMs
within optimization loops or to generate non-BO algorithms, we tackle a new
challenge: Using LLMs to automatically generate full BO algorithm code. Our
framework uses an evolution strategy to guide an LLM in generating Python code
that preserves the key components of BO algorithms: An initial design, a
surrogate model, and an acquisition function. The LLM is prompted to produce
multiple candidate algorithms, which are evaluated on the established Black-Box
Optimization Benchmarking (BBOB) test suite from the COmparing Continuous
Optimizers (COCO) platform. Based on their performance, top candidates are
selected, combined, and mutated via controlled prompt variations, enabling
iterative refinement. Despite no additional fine-tuning, the LLM-generated
algorithms outperform state-of-the-art BO baselines in 19 (out of 24) BBOB
functions in dimension 5 and generalize well to higher dimensions, and
different tasks (from the Bayesmark framework). This work demonstrates that
LLMs can serve as algorithmic co-designers, offering a new paradigm for
automating BO development and accelerating the discovery of novel algorithmic
combinations. The source code is provided at
https://github.com/Ewendawi/LLaMEA-BO.

</details>


### [224] [Scalable and adaptive prediction bands with kernel sum-of-squares](https://arxiv.org/abs/2505.21039)
*Louis Allain,Sébastien da Veiga,Brian Staber*

Key words: Conformal Prediction, RKHS, 核SoS, 自适应性, HSIC

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于RKHS和核SoS方法的新方法，通过统计学习直接优化覆盖率和自适应性。通过加速梯度方法高效解决了对偶问题，并提出了一种基于HSIC的超参数调整策略，实验结果验证了其效果。

Motivation: 传统Conformal Prediction（CP）框架缺乏自适应性，尽管已有一些改进方案，但仍有提升空间。本文旨在通过统计学习方法直接优化覆盖率和自适应性。

Method: 基于RKHS和核SoS方法，提出了一个统计学习问题，扩展了现有结果并展示了其对偶形式。通过加速梯度方法高效求解，并提出基于HSIC的超参数调整策略。

Result: 实验结果表明，新方法在大样本情况下高效，且在覆盖率和自适应性上优于现有方法。

Conclusion: 本文提出的方法在CP框架中高效且自适应，扩展了其应用范围，尤其是通过HSIC策略实现优化的超参数调整。

Abstract: Conformal Prediction (CP) is a popular framework for constructing prediction
bands with valid coverage in finite samples, while being free of any
distributional assumption. A well-known limitation of conformal prediction is
the lack of adaptivity, although several works introduced practically efficient
alternate procedures. In this work, we build upon recent ideas that rely on
recasting the CP problem as a statistical learning problem, directly targeting
coverage and adaptivity. This statistical learning problem is based on
reproducible kernel Hilbert spaces (RKHS) and kernel sum-of-squares (SoS)
methods. First, we extend previous results with a general representer theorem
and exhibit the dual formulation of the learning problem. Crucially, such dual
formulation can be solved efficiently by accelerated gradient methods with
several hundreds or thousands of samples, unlike previous strategies based on
off-the-shelf semidefinite programming algorithms. Second, we introduce a new
hyperparameter tuning strategy tailored specifically to target adaptivity
through bounds on test-conditional coverage. This strategy, based on the
Hilbert-Schmidt Independence Criterion (HSIC), is introduced here to tune
kernel lengthscales in our framework, but has broader applicability since it
could be used in any CP algorithm where the score function is learned. Finally,
extensive experiments are conducted to show how our method compares to related
work. All figures can be reproduced with the accompanying code.

</details>


### [225] [A domain adaptation neural network for digital twin-supported fault diagnosis](https://arxiv.org/abs/2505.21046)
*Zhenling Chen,Haiwei Fu,Zhiguo Zeng*

Key words: 数字孪生、故障诊断、领域适应、DANN、深度学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于域对抗神经网络（DANN）的故障诊断框架，用于解决模拟数据与真实数据间的领域差异问题，实验证明该方法显著提升了诊断性能。

Motivation: 在基于深度学习的故障诊断中，模拟数据与实际数据间的差异导致模型在实际场景中性能下降，因此需要一种方法来弥合这一差距。

Method: 提出了一种结合领域对抗神经网络（DANN）的框架，通过知识迁移从模拟数据（源域）到真实数据（目标域），并与其他轻量级深度学习模型（如CNN、TCN、Transformer和LSTM）进行比较。

Result: 实验结果表明，加入领域适应显著提升了诊断性能，例如DANN将基线CNN模型在真实测试数据上的准确率从70.00%提升到80.22%。

Conclusion: DANN能有效弥合模拟与真实系统间的性能差距，为故障诊断任务提供了一种可行的解决方案。

Abstract: Digital twins offer a promising solution to the lack of sufficient labeled
data in deep learning-based fault diagnosis by generating simulated data for
model training. However, discrepancies between simulation and real-world
systems can lead to a significant drop in performance when models are applied
in real scenarios. To address this issue, we propose a fault diagnosis
framework based on Domain-Adversarial Neural Networks (DANN), which enables
knowledge transfer from simulated (source domain) to real-world (target domain)
data. We evaluate the proposed framework using a publicly available robotics
fault diagnosis dataset, which includes 3,600 sequences generated by a digital
twin model and 90 real sequences collected from physical systems. The DANN
method is compared with commonly used lightweight deep learning models such as
CNN, TCN, Transformer, and LSTM. Experimental results show that incorporating
domain adaptation significantly improves the diagnostic performance. For
example, applying DANN to a baseline CNN model improves its accuracy from
70.00% to 80.22% on real-world test data, demonstrating the effectiveness of
domain adaptation in bridging the sim-to-real gap.

</details>


### [226] [Bridging Arbitrary and Tree Metrics via Differentiable Gromov Hyperbolicity](https://arxiv.org/abs/2505.21073)
*Pierre Houedry,Nicolas Courty,Florestan Martin-Baillon,Laetitia Chapel,Titouan Vayer*

Key words: 树度量, δ-双曲性, 可微分优化, 梯度下降, 度量空间

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为DeltaZero的可微分优化框架，用于将任意度量空间逼近为最接近的树度量，解决了现有方法缺乏保证或效果有限的问题。

Motivation: 由于现有方法要么缺乏理论保证，要么效果有限，设计能有效将任意度量空间逼近为树度量的算法仍是一个重要研究方向。

Method: 提出DeltaZero框架，利用Gromov的δ-双曲性的平滑替代，实现基于梯度的优化，并通过统计理论证明其合理性。

Result: 在合成和真实数据集上的实验表明，DeltaZero在失真率上始终达到最优。

Conclusion: DeltaZero作为一种可微分优化方法，为树度量逼近提供了高效的解决方案。

Abstract: Trees and the associated shortest-path tree metrics provide a powerful
framework for representing hierarchical and combinatorial structures in data.
Given an arbitrary metric space, its deviation from a tree metric can be
quantified by Gromov's $\delta$-hyperbolicity. Nonetheless, designing
algorithms that bridge an arbitrary metric to its closest tree metric is still
a vivid subject of interest, as most common approaches are either heuristical
and lack guarantees, or perform moderately well. In this work, we introduce a
novel differentiable optimization framework, coined DeltaZero, that solves this
problem. Our method leverages a smooth surrogate for Gromov's
$\delta$-hyperbolicity which enables a gradient-based optimization, with a
tractable complexity. The corresponding optimization procedure is derived from
a problem with better worst case guarantees than existing bounds, and is
justified statistically. Experiments on synthetic and real-world datasets
demonstrate that our method consistently achieves state-of-the-art distortion.

</details>


### [227] [Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling](https://arxiv.org/abs/2505.21074)
*Yichuan Cao,Yibo Miao,Xiao-Shan Gao,Yinpeng Dong*

Key words: 文本到图像模型, 红队测试, 规则偏好建模, 大语言模型, 安全评估

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种基于规则偏好建模的文本到图像（T2I）模型红队测试方法（RPG-RT），通过迭代使用LLM修改提示并利用T2I系统的反馈动态适应未知防御机制，解决了现有方法在封闭模型和多样化防御机制中的局限性。

Motivation: 由于T2I模型可能生成不当或有害内容，评估其安全性至关重要。现有白盒方法依赖内部访问，而黑盒方法假设了解防御机制，难以应对未知且多样化的防御。

Method: 提出RPG-RT方法，通过LLM迭代修改提示并利用T2I系统的反馈进行微调，结合规则偏好建模对反馈进行细粒度评估，实现动态适应。

Result: 在19种T2I系统、3种在线商业API和T2V模型上的实验验证了RPG-RT的优越性和实用性。

Conclusion: RPG-RT通过动态适应和规则偏好建模，有效解决了未知防御机制的挑战，为T2I模型的安全性评估提供了实用方案。

Abstract: Text-to-image (T2I) models raise ethical and safety concerns due to their
potential to generate inappropriate or harmful images. Evaluating these models'
security through red-teaming is vital, yet white-box approaches are limited by
their need for internal access, complicating their use with closed-source
models. Moreover, existing black-box methods often assume knowledge about the
model's specific defense mechanisms, limiting their utility in real-world
commercial API scenarios. A significant challenge is how to evade unknown and
diverse defense mechanisms. To overcome this difficulty, we propose a novel
Rule-based Preference modeling Guided Red-Teaming (RPG-RT), which iteratively
employs LLM to modify prompts to query and leverages feedback from T2I systems
for fine-tuning the LLM. RPG-RT treats the feedback from each iteration as a
prior, enabling the LLM to dynamically adapt to unknown defense mechanisms.
Given that the feedback is often labeled and coarse-grained, making it
difficult to utilize directly, we further propose rule-based preference
modeling, which employs a set of rules to evaluate desired or undesired
feedback, facilitating finer-grained control over the LLM's dynamic adaptation
process. Extensive experiments on nineteen T2I systems with varied safety
mechanisms, three online commercial API services, and T2V models verify the
superiority and practicality of our approach.

</details>


### [228] [Efficient Large Language Model Inference with Neural Block Linearization](https://arxiv.org/abs/2505.21077)
*Mete Erdogan,Francesco Tonin,Volkan Cevher*

Key words: Large Language Models (LLMs), Neural Block Linearization (NBL), 推理加速, 线性近似

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了Neural Block Linearization (NBL)框架，通过用线性近似替换自注意力层来加速Transformer模型的推理，无需微调即可显著提升推理速度并保持较高准确性。

Motivation: 由于Transformer大语言模型(LLMs)的高推理需求带来的部署挑战，作者希望找到一种无需微调即可显著提升推理效率的方法。

Method: NBL通过线性最小均方误差估计器生成自注意力层的线性近似，并利用典型相关分析计算近似误差的理论上限，以此作为选择替换层的依据。

Result: 实验表明，NBL在不微调的情况下显著提升了推理速度（如加速32%），同时仅带来小于1%的准确性损失。

Conclusion: NBL是一种灵活高效的方法，可显著优化LLM的推理效率。

Abstract: The high inference demands of transformer-based Large Language Models (LLMs)
pose substantial challenges in their deployment. To this end, we introduce
Neural Block Linearization (NBL), a novel framework for accelerating
transformer model inference by replacing self-attention layers with linear
approximations derived from Linear Minimum Mean Squared Error estimators. NBL
leverages Canonical Correlation Analysis to compute a theoretical upper bound
on the approximation error. Then, we use this bound as a criterion for
substitution, selecting the LLM layers with the lowest linearization error. NBL
can be efficiently applied to pre-trained LLMs without the need for
fine-tuning. In experiments, NBL achieves notable computational speed-ups while
preserving competitive accuracy on multiple reasoning benchmarks. For instance,
applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B
increases the inference speed by 32% with less than 1% accuracy trade-off,
making it a flexible and promising solution to improve the inference efficiency
of LLMs.

</details>


### [229] [Improved Impossible Tuning and Lipschitz-Adaptive Universal Online Learning with Gradient Variations](https://arxiv.org/abs/2505.21095)
*Kei Takemura,Ryuta Matsuno,Keita Sakuma*

Key words: 在线学习,梯度变化,函数曲率,梯度尺度,镜像下降

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种新型乐观在线镜像下降算法，通过辅助初始轮次和大学习率设计解决了‘不可能调参’问题，并在GV和LA假设下实现了最优性能的统一在线学习（UOL）

Motivation: 在线学习领域需要同时适应未知问题特性（如梯度变化GV、函数曲率UOL和梯度尺度LA），现有方法因专家建议算法的不完美而无法实现最优性能。

Method: 提出乐观在线镜像下降算法，采用辅助初始轮次和大学习率，通过负项抵消间隙相关因子，解决调参问题。

Result: 新算法在GV和LA假设下实现了最优性能，首次解决了LA机制与GV分析的冲突。

Conclusion: 该算法显著提升了UOL性能，解决了关键开放问题，为在线学习提供了更优解决方案。

Abstract: A central goal in online learning is to achieve adaptivity to unknown problem
characteristics, such as environmental changes captured by gradient variation
(GV), function curvature (universal online learning, UOL), and gradient scales
(Lipschitz adaptivity, LA). Simultaneously achieving these with optimal
performance is a major challenge, partly due to limitations in algorithms for
prediction with expert advice. These algorithms often serve as meta-algorithms
in online ensemble frameworks, and their sub-optimality hinders overall UOL
performance. Specifically, existing algorithms addressing the ``impossible
tuning'' issue incur an excess $\sqrt{\log T}$ factor in their regret bound
compared to the lower bound. To solve this problem, we propose a novel
optimistic online mirror descent algorithm with an auxiliary initial round
using large learning rates. This design enables a refined analysis where a
generated negative term cancels the gap-related factor, resolving the
impossible tuning issue up to $\log\log T$ factors. Leveraging our improved
algorithm as a meta-algorithm, we develop the first UOL algorithm that
simultaneously achieves state-of-the-art GV bounds and LA under standard
assumptions. Our UOL result overcomes key limitations of prior works, notably
resolving the conflict between LA mechanisms and regret analysis for GV bounds
-- an open problem highlighted by Xie et al.

</details>


### [230] [Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance](https://arxiv.org/abs/2505.21101)
*Badr Moufad,Yazid Janati,Alain Durmus,Ahmed Ghorbel,Eric Moulines,Jimmy Olsson*

Key words: Classifier-Free Guidance, 扩散模型, Rényi散度, Gibbs采样, 多样性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了对Classifier-Free Guidance (CFG)的改进方法，通过引入Rényi散度修正项和Gibbs采样，解决了CFG在质量和多样性上的权衡问题。

Motivation: CFG在提升条件扩散模型性能时存在质量与多样性的权衡问题，该研究旨在解决这一问题。

Method: 引入Rényi散度修正项以修正CFG，并提出Gibbs采样方法从目标分布中采样。

Result: 在图像和文本到音频生成任务中，改进方法显著提升了CFG的性能。

Conclusion: 通过理论分析和实验验证，该方法有效平衡了样本质量与多样性。

Abstract: Classifier-Free Guidance (CFG) is a widely used technique for improving
conditional diffusion models by linearly combining the outputs of conditional
and unconditional denoisers. While CFG enhances visual quality and improves
alignment with prompts, it often reduces sample diversity, leading to a
challenging trade-off between quality and diversity. To address this issue, we
make two key contributions. First, CFG generally does not correspond to a
well-defined denoising diffusion model (DDM). In particular, contrary to common
intuition, CFG does not yield samples from the target distribution associated
with the limiting CFG score as the noise level approaches zero -- where the
data distribution is tilted by a power $w \gt 1$ of the conditional
distribution. We identify the missing component: a R\'enyi divergence term that
acts as a repulsive force and is required to correct CFG and render it
consistent with a proper DDM. Our analysis shows that this correction term
vanishes in the low-noise limit. Second, motivated by this insight, we propose
a Gibbs-like sampling procedure to draw samples from the desired tilted
distribution. This method starts with an initial sample from the conditional
diffusion model without CFG and iteratively refines it, preserving diversity
while progressively enhancing sample quality. We evaluate our approach on both
image and text-to-audio generation tasks, demonstrating substantial
improvements over CFG across all considered metrics. The code is available at
https://github.com/yazidjanati/cfgig

</details>


### [231] [Universal Value-Function Uncertainties](https://arxiv.org/abs/2505.21119)
*Moritz A. Zanger,Max Weltevrede,Yaniv Oren,Pascal R. Van der Vaart,Caroline Horsch,Wendelin Böhmer,Matthijs T. J. Spaan*

Key words: 强化学习,不确定性估计,价值函数,UVU,离线RL

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: UVU是一种计算高效的方法，用于估计强化学习中的价值函数不确定性，其表现堪比大型集成方法，同时显著降低计算成本。

Motivation: 在强化学习中，准确估计价值函数的不确定性对高效探索、安全决策和离线RL至关重要，但现有方法（如深度集成）计算开销大，单模型方法则依赖启发式或额外机制。

Method: 提出了UVU方法，通过在线学习器与固定随机初始化目标网络之间的预测误差平方来量化不确定性，并结合时序差分学习和合成奖励进行训练。

Result: 理论分析表明，在无限网络宽度下，UVU误差等同于独立通用价值函数集成的方差；实验验证了UVU在多任务离线RL中与大型集成性能相当，且计算成本更低。

Conclusion: UVU提供了一种简单高效的不确定性估计方法，适用于强化学习中的多种场景，兼具理论严谨性和实际性能优势。

Abstract: Estimating epistemic uncertainty in value functions is a crucial challenge
for many aspects of reinforcement learning (RL), including efficient
exploration, safe decision-making, and offline RL. While deep ensembles provide
a robust method for quantifying value uncertainty, they come with significant
computational overhead. Single-model methods, while computationally favorable,
often rely on heuristics and typically require additional propagation
mechanisms for myopic uncertainty estimates. In this work we introduce
universal value-function uncertainties (UVU), which, similar in spirit to
random network distillation (RND), quantify uncertainty as squared prediction
errors between an online learner and a fixed, randomly initialized target
network. Unlike RND, UVU errors reflect policy-conditional value uncertainty,
incorporating the future uncertainties any given policy may encounter. This is
due to the training procedure employed in UVU: the online network is trained
using temporal difference learning with a synthetic reward derived from the
fixed, randomly initialized target network. We provide an extensive theoretical
analysis of our approach using neural tangent kernel (NTK) theory and show that
in the limit of infinite network width, UVU errors are exactly equivalent to
the variance of an ensemble of independent universal value functions.
Empirically, we show that UVU achieves equal performance to large ensembles on
challenging multi-task offline RL settings, while offering simplicity and
substantial computational savings.

</details>


### [232] [Robust and Computation-Aware Gaussian Processes](https://arxiv.org/abs/2505.21133)
*Marshal Arijona Sinaga,Julien Martinelli,Samuel Kaski*

Key words: 高斯过程,稳健性,近似计算,贝叶斯优化,异常值

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为RCaGP的新型高斯过程模型，通过结合稳健的广义贝叶斯更新和近似不确定性处理，解决了大数据和异常值下的计算稳健性问题。

Motivation: 标准高斯过程及其稀疏近似在大数据和异常值场景下存在计算不可行性和鲁棒性不足的问题。

Method: RCaGP将稳健性和近似感知相结合，通过稳健的广义贝叶斯更新和低秩矩阵乘法等近似方法，实现可扩展的鲁棒计算。

Result: 实验表明，RCaGP在干净和异常值污染的数据集上均表现优异，提供了更保守和可靠的uncertainty估计。

Conclusion: 稳健性和近似感知的联合处理是关键，RCaGP框架在回归和高通量贝叶斯优化中均显著优于传统方法。

Abstract: Gaussian processes (GPs) are widely used for regression and optimization
tasks such as Bayesian optimization (BO) due to their expressiveness and
principled uncertainty estimates. However, in settings with large datasets
corrupted by outliers, standard GPs and their sparse approximations struggle
with computational tractability and robustness. We introduce Robust
Computation-aware Gaussian Process (RCaGP), a novel GP model that jointly
addresses these challenges by combining a principled treatment of
approximation-induced uncertainty with robust generalized Bayesian updating.
The key insight is that robustness and approximation-awareness are not
orthogonal but intertwined: approximations can exacerbate the impact of
outliers, and mitigating one without the other is insufficient. Unlike previous
work that focuses narrowly on either robustness or approximation quality, RCaGP
combines both in a principled and scalable framework, thus effectively managing
both outliers and computational uncertainties introduced by approximations such
as low-rank matrix multiplications. Our model ensures more conservative and
reliable uncertainty estimates, a property we rigorously demonstrate.
Additionally, we establish a robustness property and show that the mean
function is key to preserving it, motivating a tailored model selection scheme
for robust mean functions. Empirical results confirm that solving these
challenges jointly leads to superior performance across both clean and
outlier-contaminated settings, both on regression and high-throughput Bayesian
optimization benchmarks.

</details>


### [233] [Learning Single Index Models with Diffusion Priors](https://arxiv.org/abs/2505.21135)
*Anqi Tang,Youming Chen,Shuchen Xue,Zhaoqiang Liu*

Key words: 扩散模型, 信号恢复, 半参数单指标模型, 非线性测量模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种利用扩散模型从半参数单指标模型中实现精确信号恢复的方法，相比现有方法能更准确且高效地重建信号。

Motivation: 扩散模型在信号恢复中表现出色，但现有研究无法处理非线性测量模型中不连续或未知的链接函数。本研究旨在填补这一空白。

Method: 提出了一种高效的重建方法，仅需一轮无条件采样和扩散模型的部分反转。

Result: 实验表明，该方法在图像数据集上比竞争方法重建更准确且使用更少的神经网络评估。

Conclusion: 扩散模型在半参数单指标模型信号恢复中具有高效性和准确性，理论分析和实验验证了其有效性。

Abstract: Diffusion models (DMs) have demonstrated remarkable ability to generate
diverse and high-quality images by efficiently modeling complex data
distributions. They have also been explored as powerful generative priors for
signal recovery, resulting in a substantial improvement in the quality of
reconstructed signals. However, existing research on signal recovery with
diffusion models either focuses on specific reconstruction problems or is
unable to handle nonlinear measurement models with discontinuous or unknown
link functions. In this work, we focus on using DMs to achieve accurate
recovery from semi-parametric single index models, which encompass a variety of
popular nonlinear models that may have {\em discontinuous} and {\em unknown}
link functions. We propose an efficient reconstruction method that only
requires one round of unconditional sampling and (partial) inversion of DMs.
Theoretical analysis on the effectiveness of the proposed methods has been
established under appropriate conditions. We perform numerical experiments on
image datasets for different nonlinear measurement models. We observe that
compared to competing methods, our approach can yield more accurate
reconstructions while utilizing significantly fewer neural function
evaluations.

</details>


### [234] [SageAttention2++: A More Efficient Implementation of SageAttention2](https://arxiv.org/abs/2505.21136)
*Jintao Zhang,Xiaoming Xu,Jia Wei,Haofeng Huang,Pengle Zhang,Chendong Xiang,Jun Zhu,Jianfei Chen*

Key words: 注意力机制, FP8矩阵乘法, 量化, 加速

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: SageAttention2++通过使用FP8矩阵乘法指令加速注意力机制，相比FlashAttention提速3.9倍，同时保持与SageAttention2相同的注意力精度。

Motivation: 解决注意力机制因序列长度增长而导致的二次时间复杂度问题。

Method: 利用FP8矩阵乘法（累积为FP16）加速SageAttention2中的矩阵乘法操作。

Result: SageAttention2++比FlashAttention快3.9倍，且在语言、图像和视频生成模型中端到端指标损失可忽略。

Conclusion: SageAttention2++显著提升了注意力机制的效率，适用于多种模型且几乎不影响精度。

Abstract: The efficiency of attention is critical because its time complexity grows
quadratically with sequence length. SageAttention2 addresses this by utilizing
quantization to accelerate matrix multiplications (Matmul) in attention. To
further accelerate SageAttention2, we propose to utilize the faster instruction
of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8
Matmul used in SageAttention2. Our experiments show that SageAttention2++
achieves a 3.9x speedup over FlashAttention while maintaining the same
attention accuracy as SageAttention2. This means SageAttention2++ effectively
accelerates various models, including those for language, image, and video
generation, with negligible end-to-end metrics loss. The code will be available
at https://github.com/thu-ml/SageAttention.

</details>


### [235] [HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs](https://arxiv.org/abs/2505.21140)
*Honglin Gao,Xiang Li,Lan Zhao,Gaoxi Xiao*

Key words: 异构图神经网络, 后门攻击, 节点分类, 安全性, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种针对异构图神经网络的异构后门攻击框架（HeteroBA），通过在图中插入精心设计的触发节点，利用注意力机制和聚类策略选择影响力大的辅助节点，有效传播触发器，导致目标节点被错误分类，同时保持干净数据的准确性。

Motivation: 异构图神经网络（HGNNs）在推荐、金融和社交网络等领域广泛应用，但其在真实攻击（如后门攻击）下的鲁棒性和安全性研究不足。本文旨在填补这一空白。

Method: 提出HeteroBA框架，通过插入带特定结构和特征的触发节点，结合注意力机制和聚类策略选择辅助节点，实现触发器的高效传播。

Result: 在三个数据集和多种HGNN架构上的实验表明，HeteroBA能以高攻击成功率影响模型，同时对干净数据的影响最小。

Conclusion: 该研究揭示了HGNNs的潜在安全漏洞，呼吁在多关系图场景下加强针对后门攻击的防御机制。

Abstract: Heterogeneous graph neural networks (HGNNs) have recently drawn increasing
attention for modeling complex multi-relational data in domains such as
recommendation, finance, and social networks. While existing research has been
largely focused on enhancing HGNNs' predictive performance, their robustness
and security, especially under backdoor attacks, remain underexplored. In this
paper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework
for node classification tasks on heterogeneous graphs. HeteroBA inserts
carefully crafted trigger nodes with realistic features and targeted structural
connections, leveraging attention-based and clustering-based strategies to
select influential auxiliary nodes for effective trigger propagation, thereby
causing the model to misclassify specific nodes into a target label while
maintaining accuracy on clean data. Experimental results on three datasets and
various HGNN architectures demonstrate that HeteroBA achieves high attack
success rates with minimal impact on the clean accuracy. Our method sheds light
on potential vulnerabilities in HGNNs and calls for more robust defenses
against backdoor threats in multi-relational graph scenarios.

</details>


### [236] [A Predicting Phishing Websites Using Support Vector Machine and MultiClass Classification Based on Association Rule Techniques](https://arxiv.org/abs/2505.21141)
*Nancy C. Woods,Virtue Ene Agada,Adebola K. Ojo*

Key words: 钓鱼网站检测, SVM, MCAR, 特征提取, 分类准确率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究者整合了支持向量机（SVM）和多类分类关联规则（MCAR）的优势，开发了一种更准确的钓鱼网站检测方法。实验结果显示，该方法分类准确率达98.30%，AUC为98%，计算时间为2205.33秒，证明了其有效性。

Motivation: 钓鱼网站对经济和信息安全造成严重威胁，现有检测方法缺乏统一的最佳算法。本研究旨在结合SVM和MCAR的优势，提高检测准确性。

Method: 使用MCAR进行特征提取和规则生成，SVM进行分类和预测。数据集包含来自PhishTank和雅虎目录的11,056个网站。

Result: 分类准确率98.30%，AUC 98%，计算时间2205.33秒，预测方差82.84%。

Conclusion: 整合SVM和MCAR的方法显著提高了钓鱼网站检测的准确性，为未来的研究提供了新思路。

Abstract: Phishing is a semantic attack which targets the user rather than the
computer. It is a new Internet crime in comparison with other forms such as
virus and hacking. Considering the damage phishing websites has caused to
various economies by collapsing organizations, stealing information and
financial diversion, various researchers have embarked on different ways of
detecting phishing websites but there has been no agreement about the best
algorithm to be used for prediction. This study is interested in integrating
the strengths of two algorithms, Support Vector Machines (SVM) and Multi-Class
Classification Rules based on Association Rules (MCAR) to establish a strong
and better means of predicting phishing websites. A total of 11,056 websites
were used from both PhishTank and yahoo directory to verify the effectiveness
of this approach. Feature extraction and rules generation were done by the MCAR
technique; classification and prediction were done by SVM technique. The result
showed that the technique achieved 98.30% classification accuracy with a
computation time of 2205.33s with minimum error rate. It showed a total of 98%
Area under the Curve (AUC) which showed the proportion of accuracy in
classifying phishing websites. The model showed 82.84% variance in the
prediction of phishing websites based on the coefficient of determination. The
use of two techniques together in detecting phishing websites produced a more
accurate result as it combined the strength of both techniques respectively.
This research work centralized on this advantage by building a hybrid of two
techniques to help produce a more accurate result.

</details>


### [237] [Semi-Supervised Conformal Prediction With Unlabeled Nonconformity Score](https://arxiv.org/abs/2505.21147)
*Xuanning Zhou,Hao Zeng,Xiaobo Xia,Bingyi Jing,Hongxin Wei*

Key words: 共形预测, 半监督学习, 不确定性量化, 非一致性评分, 覆盖保证

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出SemiCP方法，扩展了共形预测（CP）框架到半监督设置，通过结合标记和无标记数据校准，解决了标记数据不足导致的预测集覆盖偏差和过大问题，引入新的非一致性评分函数NNM。

Motivation: 实际应用中标记数据有限，标准共形预测（CP）可能导致覆盖偏差和预测集过大，因此需要扩展到半监督设置以利用无标记数据。

Method: 提出SemiCP方法，引入NNM非一致性评分函数，选择伪标签分数相似的标记数据估计非一致性分数，并将其整合到校准过程中。

Result: 理论证明SemiCP在温和假设下提供渐近覆盖保证，实验验证其能有效减少校准数据有限时的不稳定性和低效性，并可适应条件覆盖设置。

Conclusion: SemiCP通过半监督校准扩展了CP框架，显著提升了有限标记数据下的性能，并兼容现有CP方法。

Abstract: Conformal prediction (CP) is a powerful framework for uncertainty
quantification, providing prediction sets with coverage guarantees when
calibrated on sufficient labeled data. However, in real-world applications
where labeled data is often limited, standard CP can lead to coverage deviation
and output overly large prediction sets. In this paper, we extend CP to the
semi-supervised setting and propose SemiCP, leveraging both labeled data and
unlabeled data for calibration. Specifically, we introduce a novel
nonconformity score function, NNM, designed for unlabeled data. This function
selects labeled data with similar pseudo-label scores to estimate nonconformity
scores, integrating them into the calibration process to overcome sample size
limitations. We theoretically demonstrate that, under mild assumptions, SemiCP
provide asymptotically coverage guarantee for prediction sets. Extensive
experiments further validate that our approach effectively reduces instability
and inefficiency under limited calibration data, can be adapted to conditional
coverage settings, and integrates seamlessly with existing CP methods.

</details>


### [238] [STEB: In Search of the Best Evaluation Approach for Synthetic Time Series](https://arxiv.org/abs/2505.21160)
*Michael Stenger,Robert Leppich,André Bauer,Samuel Kounev*

Key words: 合成时间序列, 评估基准, 生成模型, 数据增强, 隐私保护

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了STEB基准框架，用于全面评估41种时间序列生成模型的性能，强调上游嵌入对评分的影响。

Motivation: 由于数据增强和隐私法规的需求，合成时间序列生成方法众多，但缺乏客观的大规模比较基准。

Method: 使用10个数据集、随机注入和13种数据变换，STEB通过可靠性指标和评分一致性评估措施，支持串行和并行操作。

Result: 实验中对41种方法进行排名，证实上游时间序列嵌入对最终评分有重大影响。

Conclusion: STEB是首个全面、可解释的合成时间序列评估基准，为方法选择提供可靠依据。

Abstract: The growing need for synthetic time series, due to data augmentation or
privacy regulations, has led to numerous generative models, frameworks, and
evaluation measures alike. Objectively comparing these measures on a large
scale remains an open challenge. We propose the Synthetic Time series
Evaluation Benchmark (STEB) -- the first benchmark framework that enables
comprehensive and interpretable automated comparisons of synthetic time series
evaluation measures. Using 10 diverse datasets, randomness injection, and 13
configurable data transformations, STEB computes indicators for measure
reliability and score consistency. It tracks running time, test errors, and
features sequential and parallel modes of operation. In our experiments, we
determine a ranking of 41 measures from literature and confirm that the choice
of upstream time series embedding heavily impacts the final score.

</details>


### [239] [Topological Deep Learning for Speech Data](https://arxiv.org/abs/2505.21173)
*Zhiwang Yu*

Key words: 拓扑数据分析, 卷积核, 正交群, 语音识别, 跨领域适应性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本研究利用拓扑数据分析（TDA）设计了拓扑感知卷积核，显著提升了语音识别网络的性能。通过理论分析和实践验证，提出的正交特征（OF）层在低噪声场景下表现出色，并展示了跨领域适应性。

Motivation: 受Carlsson等人的启发，探索将拓扑数据分析（TDA）应用于深度学习，以优化神经网络性能，特别是在语音识别领域。

Method: 研究了正交群作用在核上的理论，建立了矩阵空间的纤维丛分解，提出了一种新的滤波器生成方法，并设计了正交特征（OF）层。

Result: 在音素识别任务中，特别是在低噪声环境下，OF层表现出卓越的性能，并展示了跨领域的适应性。

Conclusion: 这项工作展示了TDA在神经网络优化中的潜力，为数学与深度学习的跨学科研究开辟了新途径。

Abstract: Topological data analysis (TDA) offers novel mathematical tools for deep
learning. Inspired by Carlsson et al., this study designs topology-aware
convolutional kernels that significantly improve speech recognition networks.
Theoretically, by investigating orthogonal group actions on kernels, we
establish a fiber-bundle decomposition of matrix spaces, enabling new filter
generation methods. Practically, our proposed Orthogonal Feature (OF) layer
achieves superior performance in phoneme recognition, particularly in low-noise
scenarios, while demonstrating cross-domain adaptability. This work reveals
TDA's potential in neural network optimization, opening new avenues for
mathematics-deep learning interdisciplinary studies.

</details>


### [240] [Latent label distribution grid representation for modeling uncertainty](https://arxiv.org/abs/2505.21180)
*ShuNing Sun,YinSong Xiong,Yu Zhang,Zhuoran Zheng*

Key words: 标签分布学习、不确定性建模、潜在标签分布网格、低秩重建、分类任务

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种潜在标签分布网格（LLDG）方法，通过建模标签分布的不确定性，减少噪声并生成准确的标签分布，进而提升分类性能。

Motivation: 标签分布学习（LDL）虽然能表征实例的多义性，但由于标签分布的标注复杂且高成本，导致标签空间存在不确定性，进而影响LDL算法的决策准确性。

Method: 首先构建基于标签差异的相关性矩阵，将其扩展为服从高斯分布的向量，形成LLDG；再通过LLDG-Mixer重建标签分布，并结合低秩假设和Tucker重建技术降噪。

Result: 实验结果表明，该方法在多个基准数据集上表现优异。

Conclusion: LLDG能有效建模标签空间的不确定性，并通过降噪和重建生成更准确的标签分布，提升下游任务性能。

Abstract: Although \textbf{L}abel \textbf{D}istribution \textbf{L}earning (LDL) has
promising representation capabilities for characterizing the polysemy of an
instance, the complexity and high cost of the label distribution annotation
lead to inexact in the construction of the label space. The existence of a
large number of inexact labels generates a label space with uncertainty, which
misleads the LDL algorithm to yield incorrect decisions. To alleviate this
problem, we model the uncertainty of label distributions by constructing a
\textbf{L}atent \textbf{L}abel \textbf{D}istribution \textbf{G}rid (LLDG) to
form a low-noise representation space. Specifically, we first construct a label
correlation matrix based on the differences between labels, and then expand
each value of the matrix into a vector that obeys a Gaussian distribution, thus
building a LLDG to model the uncertainty of the label space. Finally, the LLDG
is reconstructed by the LLDG-Mixer to generate an accurate label distribution.
Note that we enforce a customized low-rank scheme on this grid, which assumes
that the label relations may be noisy and it needs to perform noise-reduction
with the help of a Tucker reconstruction technique. Furthermore, we attempt to
evaluate the effectiveness of the LLDG by considering its generation as an
upstream task to achieve the classification of the objects. Extensive
experimental results show that our approach performs competitively on several
benchmarks.

</details>


### [241] [Learning What to Do and What Not To Do: Offline Imitation from Expert and Undesirable Demonstrations](https://arxiv.org/abs/2505.21182)
*Huy Hoang,Tien Mai,Pradeep Varakantham,Tanvi Verma*

Key words: 离线模仿学习、KL散度、专家示范、不良行为、非对抗训练

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种从专家和不良行为中学习的离线模仿学习新方法，通过优化专家与不良数据状态-动作访问分布的KL散度差异，避免了对抗训练，并在实验中表现优于现有方法。

Motivation: 现有的离线模仿学习通常仅利用专家和无标签示范，而忽略了不良行为的信号。本研究旨在利用专家与不良示范的对比信息，提升学习效果。

Method: 提出了一种新颖的优化方法，通过计算专家与不良数据状态-动作访问分布的KL散度差异，构建一个非对抗性的训练目标。当专家数据占优时，该目标为凸优化问题，保证了训练的稳定性和实用性。

Result: 在标准离线模仿学习基准上的实验表明，该方法一致优于当前最优基线方法。

Conclusion: 通过统一处理正负示范数据，避免了对抗训练的复杂性，提供了一种更稳定且高效的离线模仿学习框架。

Abstract: Offline imitation learning typically learns from expert and unlabeled
demonstrations, yet often overlooks the valuable signal in explicitly
undesirable behaviors. In this work, we study offline imitation learning from
contrasting behaviors, where the dataset contains both expert and undesirable
demonstrations. We propose a novel formulation that optimizes a difference of
KL divergences over the state-action visitation distributions of expert and
undesirable (or bad) data. Although the resulting objective is a DC
(Difference-of-Convex) program, we prove that it becomes convex when expert
demonstrations outweigh undesirable demonstrations, enabling a practical and
stable non-adversarial training objective. Our method avoids adversarial
training and handles both positive and negative demonstrations in a unified
framework. Extensive experiments on standard offline imitation learning
benchmarks demonstrate that our approach consistently outperforms
state-of-the-art baselines.

</details>


### [242] [PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing](https://arxiv.org/abs/2505.21184)
*Yu Yan,Sheng Sun,Zhifei Zheng,Ziji Hao,Teli Liu,Min Liu*

Key words: 有害信息合成, 模型众包, 反事实生成, 动态模型切换, 多样化数据

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为PoisonSwarm的新型有害信息合成框架，通过模型众包策略生成多样化的有害数据，同时保持高成功率。该方法利用反事实方式生成大量良性数据作为基础模板，并通过动态模型切换进行单元毒化和最终优化，实验结果表明其在多样性和可扩展性上达到最优性能。

Motivation: 当前利用大语言模型生成有害数据的研究受限于安全对齐机制，导致生成可靠性和内容多样性不足。因此，需要一种新方法来高效、多样地合成有害数据，以支持负责任和安全AI应用的开发。

Method: PoisonSwarm框架首先通过反事实方式生成大量良性数据作为基础模板，然后将其分解为多个语义单元，逐单元进行毒化，并通过动态模型切换完成最终优化。

Result: 实验结果显示，PoisonSwarm在合成多类别有害数据时表现优异，兼具高可扩展性和多样性。

Conclusion: PoisonSwarm为解决有害数据合成的可靠性和多样性挑战提供了有效方案，适用于对抗性测试和安全措施开发。

Abstract: To construct responsible and secure AI applications, harmful information data
is widely utilized for adversarial testing and the development of safeguards.
Existing studies mainly leverage Large Language Models (LLMs) to synthesize
data to obtain high-quality task datasets at scale, thereby avoiding costly
human annotation. However, limited by the safety alignment mechanisms of LLMs,
the synthesis of harmful data still faces challenges in generation reliability
and content diversity. In this study, we propose a novel harmful information
synthesis framework, PoisonSwarm, which applies the model crowdsourcing
strategy to generate diverse harmful data while maintaining a high success
rate. Specifically, we generate abundant benign data as the based templates in
a counterfactual manner. Subsequently, we decompose each based template into
multiple semantic units and perform unit-by-unit toxification and final
refinement through dynamic model switching, thus ensuring the success of
synthesis. Experimental results demonstrate that PoisonSwarm achieves
state-of-the-art performance in synthesizing different categories of harmful
data with high scalability and diversity.

</details>


### [243] [Crop recommendation with machine learning: leveraging environmental and economic factors for optimal crop selection](https://arxiv.org/abs/2505.21201)
*Steven Sam,Silima Marshal DAbreo*

Key words: 农业推荐系统、随机森林、SVM、时间序列、滞后变量、印度农业

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究结合环境和经济因素，为印度15个州的19种作物开发了随机森林和SVM模型。在时间序列和滞后变量方法下，随机森林表现最佳，适合实际农业推荐。

Motivation: 印度农业面临生产力低下和气候变化挑战，现有推荐系统仅关注环境因素且区域有限，因此需要更全面的模型提升作物推荐准确性。

Method: 使用随机森林和SVM模型，结合10折交叉验证、时间序列分割和滞后变量方法，评估环境和经济因素对作物产量的影响。

Result: 随机森林在滞后变量方法中表现最优（准确率83.62%），优于时间序列分割（78.55%）和SVM模型。

Conclusion: 滞后变量方法下的随机森林模型最适合印度农业的作物推荐，能有效处理时间依赖性和适应变化。

Abstract: Agriculture constitutes a primary source of food production, economic growth
and employment in India, but the sector is confronted with low farm
productivity and yields aggravated by increased pressure on natural resources
and adverse climate change variability. Efforts involving green revolution,
land irrigations, improved seeds and organic farming have yielded suboptimal
outcomes. The adoption of computational tools like crop recommendation systems
offers a new way to provide insights and help farmers tackle low productivity.
However, most agricultural recommendation systems in India focus narrowly on
environmental factors and regions, limiting accurate predictions of high-yield,
profitable crops. This study uses environmental and economic factors with 19
crops across 15 states to develop and evaluate Random Forest and SVM models
using 10-fold Cross Validation, Time-series Split, and Lag Variables. The
10-fold cross validation showed high accuracy (RF: 99.96%, SVM: 94.71%) but
raised overfitting concerns. Introducing temporal order, better reflecting
real-world conditions, reduced performance (RF: 78.55%, SVM: 71.18%) in the
Time-series Split.To further increase the model accuracy while maintaining the
temporal order, the Lag Variables approach was employed, which resulted in
improved performance (RF: 83.62%, SVM: 74.38%) compared to the 10-fold cross
validation approach. Overall, the models in the Time-series Split and Lag
Variable Approaches offer practical insights by handling temporal dependencies
and enhancing its adaptability to changing agricultural conditions over time.
Consequently, the study shows the Random Forest model developed based on the
Lag Variables as the most preferred algorithm for optimal crop recommendation
in the Indian context.

</details>


### [244] [Developing hybrid mechanistic and data-driven personalized prediction models for platelet dynamics](https://arxiv.org/abs/2505.21204)
*Marie Steinacker,Yuri Kheifetz,Markus Scholz*

Key words: 血液毒性、化疗、血小板计数、混合模型、数据驱动、个性化医疗

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究开发并比较了混合机理和数据驱动方法，用于化疗期间血小板计数的个性化时间序列建模，发现数据驱动方法在高风险患者中显著提高预测准确性，而混合和机理模型在数据有限时表现更优。

Motivation: 化疗引起的血液毒性具有高度个体差异和不可预测性，现有机理模型难以准确预测异常轨迹的患者。

Method: 结合机理模型与神经网络的混合方法（通用微分方程），以及纯数据驱动的非线性自回归外生模型（门控循环单元）。

Result: 数据驱动方法在数据充足时预测更准确，尤其对高风险患者；混合和机理模型在数据稀疏时表现更好。

Conclusion: 该框架可推广至其他治疗相关毒性的预测，为个性化医疗提供广泛适用性。

Abstract: Hematotoxicity, drug-induced damage to the blood-forming system, is a
frequent side effect of cytotoxic chemotherapy and poses a significant
challenge in clinical practice due to its high inter-patient variability and
limited predictability. Current mechanistic models often struggle to accurately
forecast outcomes for patients with irregular or atypical trajectories. In this
study, we develop and compare hybrid mechanistic and data-driven approaches for
individualized time series modeling of platelet counts during chemotherapy. We
consider hybrid models that combine mechanistic models with neural networks,
known as universal differential equations. As a purely data-driven alternative,
we utilize a nonlinear autoregressive exogenous model using gated recurrent
units as the underlying architecture. These models are evaluated across a range
of real patient scenarios, varying in data availability and sparsity, to assess
predictive performance. Our findings demonstrate that data-driven methods, when
provided with sufficient data, significantly improve prediction accuracy,
particularly for high-risk patients with irregular platelet dynamics. This
highlights the potential of data-driven approaches in enhancing clinical
decision-making. In contrast, hybrid and mechanistic models are superior in
scenarios with limited or sparse data. The proposed modeling and comparison
framework is generalizable and could be extended to predict other
treatment-related toxicities, offering broad applicability in personalized
medicine.

</details>


### [245] [Addressing Data Quality Decompensation in Federated Learning via Dynamic Client Selection](https://arxiv.org/abs/2505.21219)
*Qinjun Fei,Nuria Rodríguez-Barroso,María Victoria Luzón,Zhongliang Zhang,Francisco Herrera*

Key words: 联邦学习, 动态竞价, 声誉模型, Shapley 值, 成本优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: SBRO-FL 是一个联邦学习的统一框架，通过动态竞价、声誉建模和成本感知选择来解决客户端选择问题，提升模型性能和可信度。

Motivation: 由于数据质量、预算限制和激励兼容性的挑战，联邦学习的客户端选择问题变得复杂且难以优化。

Method: SBRO-FL 整合了基于 Shapley 值的贡献评估、前景理论启发的声誉系统，以及预算约束下的 0-1 整数规划选择方法。

Result: 在多个数据集上的实验表明，SBRO-FL 在准确性、收敛速度和鲁棒性方面表现优越，特别是在对抗性和低竞价干扰下。

Conclusion: SBRO-FL 展示了平衡数据可靠性、激励兼容性和成本效率对联邦学习规模化部署的重要性。

Abstract: In cross-silo Federated Learning (FL), client selection is critical to ensure
high model performance, yet it remains challenging due to data quality
decompensation, budget constraints, and incentive compatibility. As training
progresses, these factors exacerbate client heterogeneity and degrade global
performance. Most existing approaches treat these challenges in isolation,
making jointly optimizing multiple factors difficult. To address this, we
propose Shapley-Bid Reputation Optimized Federated Learning (SBRO-FL), a
unified framework integrating dynamic bidding, reputation modeling, and
cost-aware selection. Clients submit bids based on their perceived data
quality, and their contributions are evaluated using Shapley values to quantify
their marginal impact on the global model. A reputation system, inspired by
prospect theory, captures historical performance while penalizing
inconsistency. The client selection problem is formulated as a 0-1 integer
program that maximizes reputation-weighted utility under budget constraints.
Experiments on FashionMNIST, EMNIST, CIFAR-10, and SVHN datasets show that
SBRO-FL improves accuracy, convergence speed, and robustness, even in
adversarial and low-bid interference scenarios. Our results highlight the
importance of balancing data reliability, incentive compatibility, and cost
efficiency to enable scalable and trustworthy FL deployments.

</details>


### [246] [Why Do More Experts Fail? A Theoretical Analysis of Model Merging](https://arxiv.org/abs/2505.21226)
*Zijing Wang,Xingle Xu,Yongkang Liu,Yiqun Zhang,Peiqin Lin,Shi Feng,Xiaocui Yang,Daling Wang,Hinrich Schütze*

Key words: 模型合并, 参数空间, 高斯宽度, 重尾分布, 多任务学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文研究了模型合并中随着合并模型数量增加时性能提升受限的关键障碍，理论证明了模型合并存在上限，并提出了Reparameterized Heavy-Tailed方法（RHT）来提升合并模型的覆盖范围。

Motivation: 探讨模型合并中性能提升随合并模型数量增加而受限的根本原因，以突破当前方法的局限性。

Method: 通过理论分析（如Gaussian Width和Approximate Kinematics Theory）和实验验证，提出了RHT方法。

Result: 在12个基准测试中验证了理论分析的有效性，RHT方法显著提升了合并模型的性能。

Conclusion: 模型合并存在上限，但RHT方法能够扩展有效参数空间，为未来研究提供了新方向。

Abstract: Model merging dramatically reduces storage and computational resources by
combining multiple expert models into a single multi-task model. Although
recent model merging methods have shown promising results, they struggle to
maintain performance gains as the number of merged models increases. In this
paper, we investigate the key obstacles that limit the scalability of model
merging when integrating a large number of expert models. First, we prove that
there is an upper bound on model merging. Further theoretical analysis reveals
that the limited effective parameter space imposes a strict constraint on the
number of models that can be successfully merged. Gaussian Width shows that the
marginal benefit of merging additional models diminishes according to a
strictly concave function. This implies that the effective parameter space
becomes rapidly saturated as the number of merged models increases.
Furthermore, using Approximate Kinematics Theory, we prove the existence of a
unique optimal threshold beyond which adding more models does not yield
significant performance improvements. At the same time, we introduce a
straightforward Reparameterized Heavy-Tailed method (RHT) to extend the
coverage of the merged model, thereby enhancing its performance. Empirical
results on 12 benchmarks, including both knowledge-intensive and
general-purpose tasks, validate our theoretical analysis. We believe that these
results spark further research beyond the current scope of model merging. The
source code is in the anonymous Github repository
https://github.com/wzj1718/ModelMergingAnalysis.

</details>


### [247] [Breaking the Performance Ceiling in Complex Reinforcement Learning requires Inference Strategies](https://arxiv.org/abs/2505.21236)
*Felix Chalumeau,Daniel Rajaonarivonivelomanantsoa,Ruan de Kock,Claude Formanek,Sasha Abramowitz,Oumayma Mahjoub,Wiem Khlifi,Simon Du Toit,Louay Ben Nessir,Refiloe Shabe,Arnol Fokam,Siddarth Singh,Ulrich Mbou Sob,Arnu Pretorius*

Key words: 强化学习, 多智能体, 推断策略, 性能提升

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种在执行时利用特定时间和计算预算的多重尝试推断策略，显著提升了复杂多智能体强化学习任务的性能。

Motivation: 现实世界中的复杂、组合性多智能体强化学习任务即使经过充分训练的系统也难以在零样本推断中突破性能瓶颈。

Method: 通过执行时的推断阶段和对应的推断策略选择，探索多次尝试以提升性能。

Result: 实验表明，该方法在17个任务中平均提升45%，最高达126%，且计算扩展性良好。

Conclusion: 执行时的推断策略是突破复杂多智能体强化学习性能瓶颈的关键。

Abstract: Reinforcement learning (RL) systems have countless applications, from
energy-grid management to protein design. However, such real-world scenarios
are often extremely difficult, combinatorial in nature, and require complex
coordination between multiple agents. This level of complexity can cause even
state-of-the-art RL systems, trained until convergence, to hit a performance
ceiling which they are unable to break out of with zero-shot inference.
Meanwhile, many digital or simulation-based applications allow for an inference
phase that utilises a specific time and compute budget to explore multiple
attempts before outputting a final solution. In this work, we show that such an
inference phase employed at execution time, and the choice of a corresponding
inference strategy, are key to breaking the performance ceiling observed in
complex multi-agent RL problems. Our main result is striking: we can obtain up
to a 126% and, on average, a 45% improvement over the previous state-of-the-art
across 17 tasks, using only a couple seconds of extra wall-clock time during
execution. We also demonstrate promising compute scaling properties, supported
by over 60k experiments, making it the largest study on inference strategies
for complex RL to date. Our experimental data and code are available at
https://sites.google.com/view/inf-marl.

</details>


### [248] [BindEnergyCraft: Casting Protein Structure Predictors as Energy-Based Models for Binder Design](https://arxiv.org/abs/2505.21241)
*Divya Nori,Anisha Parsan,Caroline Uhler,Wengong Jin*

Key words: 蛋白质结合剂设计, 能量模型, 结构预测, 虚拟筛选, 统计能量函数

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于能量模型的蛋白质结合剂设计方法，通过重新解释结构预测器的输出为能量模型，显著提高了设计成功率和结构稳定性。

Motivation: 现有基于幻象的方法依赖结构预测置信度指标（如ipTM），但这些指标不能反映结合剂-目标复合物的统计可能性，且优化梯度稀疏。因此，作者希望提取更准确的统计能量函数以改进设计效果。

Method: 作者引入pTMEnergy，一种从预测残间误差分布中导出的统计能量函数，并将其整合到设计流程BindEnergyCraft（BECraft）中，取代了原有的ipTM目标。

Result: BECraft在多个挑战性目标上优于BindCraft、RFDiffusion和ESM3，提高了虚拟筛选成功率并减少了结构冲突，同时在小蛋白和RNA适配体结合剂的结构虚拟筛选中达到新最优水平。

Conclusion: 通过能量模型提取统计能量函数的方法显著提升了蛋白质结合剂设计的性能和鲁棒性。

Abstract: Protein binder design has been transformed by hallucination-based methods
that optimize structure prediction confidence metrics, such as the interface
predicted TM-score (ipTM), via backpropagation. However, these metrics do not
reflect the statistical likelihood of a binder-target complex under the learned
distribution and yield sparse gradients for optimization. In this work, we
propose a method to extract such likelihoods from structure predictors by
reinterpreting their confidence outputs as an energy-based model (EBM). By
leveraging the Joint Energy-based Modeling (JEM) framework, we introduce
pTMEnergy, a statistical energy function derived from predicted inter-residue
error distributions. We incorporate pTMEnergy into BindEnergyCraft (BECraft), a
design pipeline that maintains the same optimization framework as BindCraft but
replaces ipTM with our energy-based objective. BECraft outperforms BindCraft,
RFDiffusion, and ESM3 across multiple challenging targets, achieving higher in
silico binder success rates while reducing structural clashes. Furthermore,
pTMEnergy establishes a new state-of-the-art in structure-based virtual
screening tasks for miniprotein and RNA aptamer binders.

</details>


### [249] [Copresheaf Topological Neural Networks: A Generalized Deep Learning Framework](https://arxiv.org/abs/2505.21251)
*Mustafa Hajij,Lennart Bastian,Sarah Osentoski,Hardik Kabaria,John L. Davenport,Sheik Dawood,Balaji Cherukuri,Joseph G. Kocheemoolayil,Nastaran Shahmansouri,Adrian Lew,Theodore Papamarkou,Tolga Birdal*

Key words: copresheaf, topological neural networks, structured data, algebraic topology, representation learning

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种基于上预层拓扑神经网络的统一框架（CTNNs），旨在解决结构化数据（如图像、点云等）的深度学习架构设计挑战，通过代数拓扑理论提供理论基础，并在多个基准测试中优于传统方法。

Motivation: 当前深度学习模型缺乏针对特定任务和数据类型的系统性设计方法。CTNNs希望通过代数拓扑中的上预层概念，提供一个统一且理论严谨的框架，以解决表示学习中的核心挑战。

Method: CTNNs利用上预层的代数拓扑理论，构建了一个通用框架，涵盖多种深度学习模型，适用于图像、点云、图等结构化数据。这种方法旨在解决长程依赖、过平滑等问题。

Result: 实验结果表明，CTNNs在结构化数据基准测试中 consistently 优于传统基线方法，特别是在需要层次化或局部敏感性的任务中。

Conclusion: CTNNs作为一种多尺度的理论基础，为下一代深度学习架构提供了一个系统且有效的设计框架。

Abstract: We introduce copresheaf topological neural networks (CTNNs), a powerful and
unifying framework that encapsulates a wide spectrum of deep learning
architectures, designed to operate on structured data: including images, point
clouds, graphs, meshes, and topological manifolds. While deep learning has
profoundly impacted domains ranging from digital assistants to autonomous
systems, the principled design of neural architectures tailored to specific
tasks and data types remains one of the field's most persistent open
challenges. CTNNs address this gap by grounding model design in the language of
copresheaves, a concept from algebraic topology that generalizes and subsumes
most practical deep learning models in use today. This abstract yet
constructive formulation yields a rich design space from which theoretically
sound and practically effective solutions can be derived to tackle core
challenges in representation learning: long-range dependencies, oversmoothing,
heterophily, and non-Euclidean domains. Our empirical results on structured
data benchmarks demonstrate that CTNNs consistently outperform conventional
baselines, particularly in tasks requiring hierarchical or localized
sensitivity. These results underscore CTNNs as a principled, multi-scale
foundation for the next generation of deep learning architectures.

</details>


### [250] [Learnable Kernel Density Estimation for Graphs](https://arxiv.org/abs/2505.21285)
*Xudong Wang,Ziheng Sun,Chris Ding,Jicong Fan*

Key words: graph density estimation, kernel density estimation, graph neural networks, anomaly detection, maximum mean discrepancy

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: LGKDE is a framework that uses graph neural networks and kernel density estimation to improve graph density estimation by learning graph metrics and handling perturbations on node features and graph spectra.

Motivation: The motivation is to address the challenge of effectively capturing structural and semantic variations in graph density estimation while maintaining theoretical guarantees, as traditional methods using handcrafted kernels perform unsatisfactorily.

Method: The method leverages graph neural networks to represent graphs as discrete distributions, uses maximum mean discrepancy for multi-scale KDE, and learns parameters by maximizing the density of graphs relative to perturbed counterparts.

Result: LGKDE shows superior performance in recovering synthetic graph densities and graph anomaly detection, outperforming state-of-the-art baselines on benchmark datasets.

Conclusion: LGKDE provides a robust and effective framework for graph density estimation with theoretical guarantees and practical benefits in applications like anomaly detection.

Abstract: This work proposes a framework LGKDE that learns kernel density estimation
for graphs. The key challenge in graph density estimation lies in effectively
capturing both structural patterns and semantic variations while maintaining
theoretical guarantees. Combining graph kernels and kernel density estimation
(KDE) is a standard approach to graph density estimation, but has
unsatisfactory performance due to the handcrafted and fixed features of
kernels. Our method LGKDE leverages graph neural networks to represent each
graph as a discrete distribution and utilizes maximum mean discrepancy to learn
the graph metric for multi-scale KDE, where all parameters are learned by
maximizing the density of graphs relative to the density of their well-designed
perturbed counterparts. The perturbations are conducted on both node features
and graph spectra, which helps better characterize the boundary of normal
density regions. Theoretically, we establish consistency and convergence
guarantees for LGKDE, including bounds on the mean integrated squared error,
robustness, and complexity. We validate LGKDE by demonstrating its
effectiveness in recovering the underlying density of synthetic graph
distributions and applying it to graph anomaly detection across diverse
benchmark datasets. Extensive empirical evaluation shows that LGKDE
demonstrates superior performance compared to state-of-the-art baselines on
most benchmark datasets.

</details>


### [251] [GSAT: Graph Structure Attention Networks](https://arxiv.org/abs/2505.21288)
*Farshad Noravesh,Reza Haffari,Layki Soon,Arghya Pal*

Key words: 图神经网络, 图分类, 注意力机制, 匿名随机游走, 局部拓扑结构

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为GSAT的图结构注意力网络，通过结合匿名随机游走（ARW）生成的局部拓扑结构信息和原始节点属性，改进了图神经网络在分类任务中的表现，略微提升了现有最佳水平（SOTA）。

Motivation: 现有图神经网络（GNN）在处理图分类任务时忽视了节点的局部拓扑结构信息，导致需要更多层网络连接远距离节点，引发诸如过度平滑等问题。

Method: 论文提出了GSAT（图结构注意力网络），扩展了图注意力网络（GAT），结合匿名随机游走（ARW）生成的局部结构信息和节点原始属性，自动学习节点邻域内边的注意力模式以丰富图表示。

Result: 实验表明GSAT在某些图分类基准任务上略微超越了现有最佳方法（SOTA）。

Conclusion: 通过显式建模局部结构信息并融入注意力机制，GSAT能够更有效地捕获图数据的关键特征，提升分类性能。

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful tool for processing
data represented in graph structures, achieving remarkable success across a
wide range of applications. However, to further improve the performance on
graph classification benchmarks, structural representation of each node that
encodes rich local topological information in the neighbourhood of nodes is an
important type of feature that is often overlooked in the modeling. The
consequence of neglecting the structural information has resulted high number
of layers to connect messages from distant nodes which by itself produces other
problems such as oversmoothing. In the present paper, we leverage these
structural information that are modeled by anonymous random walks (ARWs) and
introduce graph structure attention network (GSAT) which is a generalization of
graph attention network(GAT) to integrate the original attribute and the
structural representation to enforce the model to automatically find patterns
for attending to different edges in the node neighbourhood to enrich graph
representation. Our experiments show GSAT slightly improves SOTA on some graph
classification benchmarks.

</details>


### [252] [LoFT: Low-Rank Adaptation That Behaves Like Full Fine-Tuning](https://arxiv.org/abs/2505.21289)
*Nurbek Tastan,Stefanos Laskaridis,Martin Takac,Karthik Nandakumar,Samuel Horvath*

Key words: LoFT, LoRA, 低秩适应, 参数高效微调, Adam优化器

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: LoFT是一种新的低秩适应方法，通过优化器动态对齐全模型更新，显著缩小与全微调的性能差距，优于标准LoRA且不增加推理成本。

Motivation: 现有LoRA方法在准确性和收敛速度上仍不及全微调，LoFT旨在通过优化器动态对齐解决这一问题。

Method: LoFT在低秩子空间中学习权重更新，并将优化器的动量和方差投影到同一子空间，模拟全模型更新。

Result: LoFT显著缩小了适配器调优与全微调的性能差距，并一致优于标准LoRA方法。

Conclusion: LoFT提供了一种更高效的低秩适应方法，无需额外超参数调整，且不影响推理效率。

Abstract: Large pre-trained models are commonly adapted to downstream tasks using
parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA),
which injects small trainable low-rank matrices instead of updating all
weights. While LoRA dramatically reduces trainable parameters with little
overhead, it can still underperform full fine-tuning in accuracy and often
converges more slowly. We introduce LoFT, a novel low-rank adaptation method
that behaves like full fine-tuning by aligning the optimizer's internal
dynamics with those of updating all model weights. LoFT not only learns weight
updates in a low-rank subspace (like LoRA) but also properly projects the
optimizer's first and second moments (Adam's momentum and variance) into the
same subspace, mirroring full-model updates. By aligning the low-rank update
itself with the full update, LoFT eliminates the need for tuning extra
hyperparameters, e.g., LoRA scaling factor $\alpha$. Empirically, this approach
substantially narrows the performance gap between adapter-based tuning and full
fine-tuning and consistently outperforms standard LoRA-style methods, all
without increasing inference cost.

</details>


### [253] [A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features](https://arxiv.org/abs/2505.21317)
*Ihab Bendidi,Yassir El Mesbahi,Alisandra K. Denton,Karush Suri,Kian Kenyon-Dean,Auguste Genovesio,Emmanuel Noutahi*

Key words: 转录组学, 显微成像, 多模态学习, 知识蒸馏, 数据增强

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种通过显微图像知识蒸馏增强转录组学数据的框架，结合半剪裁CLIP和PEA增强技术，提升预测能力并保持可解释性。

Motivation: 细胞对刺激的响应研究对生物学发现和药物开发至关重要。转录组学提供基因层面的可解释洞察，而显微成像提供丰富但难以解释的特征。弱配对数据集稀缺限制了多模态学习的应用。

Method: 1. 使用弱配对数据对齐和绑定模态；2. 提出Semi-Clipped（基于CLIP的跨模态蒸馏）和PEA（扰动嵌入增强）技术。

Result: 方法在跨模态蒸馏中达到最先进效果，同时增强转录组学数据的预测能力并保留生物学信息。

Conclusion: 框架在复杂生物学任务中实现了丰富的单模态表示，结合了显微图像的预测能力和转录组学的可解释性。

Abstract: Understanding cellular responses to stimuli is crucial for biological
discovery and drug development. Transcriptomics provides interpretable,
gene-level insights, while microscopy imaging offers rich predictive features
but is harder to interpret. Weakly paired datasets, where samples share
biological states, enable multimodal learning but are scarce, limiting their
utility for training and multimodal inference. We propose a framework to
enhance transcriptomics by distilling knowledge from microscopy images. Using
weakly paired data, our method aligns and binds modalities, enriching gene
expression representations with morphological information. To address data
scarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal
distillation using pretrained foundation models, achieving state-of-the-art
results, and (2) PEA (Perturbation Embedding Augmentation), a novel
augmentation technique that enhances transcriptomics data while preserving
inherent biological information. These strategies improve the predictive power
and retain the interpretability of transcriptomics, enabling rich unimodal
representations for complex biological tasks.

</details>


### [254] [Bencher: Simple and Reproducible Benchmarking for Black-Box Optimization](https://arxiv.org/abs/2505.21321)
*Leonard Papenmeier,Luigi Nardi*

Key words: 黑盒优化,基准测试,模块化,RPC,容器化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Bencher是一个模块化的黑盒优化基准测试框架，通过将基准测试执行与优化逻辑解耦，解决了依赖冲突问题，并简化了复杂多样基准测试的集成。

Motivation: 现有的基准测试套件往往将多个基准测试集中在单一项目中，导致依赖冲突和集成困难；Bencher旨在通过隔离和标准化接口解决这些问题。

Method: Bencher为每个基准测试提供独立的Python虚拟环境，并通过统一的RPC接口访问，支持本地、Docker或HPC集群部署，实现了容器化和可复现性。

Result: Bencher成功集成了80个跨连续、分类和二元域的基准测试，依赖冲突显著减少，集成复杂度降低。

Conclusion: Bencher的设计简化了复杂基准测试的集成与执行，为黑盒优化提供了高效、灵活的测试平台。

Abstract: We present Bencher, a modular benchmarking framework for black-box
optimization that fundamentally decouples benchmark execution from optimization
logic. Unlike prior suites that focus on combining many benchmarks in a single
project, Bencher introduces a clean abstraction boundary: each benchmark is
isolated in its own virtual Python environment and accessed via a unified,
version-agnostic remote procedure call (RPC) interface. This design eliminates
dependency conflicts and simplifies the integration of diverse, real-world
benchmarks, which often have complex and conflicting software requirements.
Bencher can be deployed locally or remotely via Docker or on high-performance
computing (HPC) clusters via Singularity, providing a containerized,
reproducible runtime for any benchmark. Its lightweight client requires minimal
setup and supports drop-in evaluation of 80 benchmarks across continuous,
categorical, and binary domains.

</details>


### [255] [UGCE: User-Guided Incremental Counterfactual Exploration](https://arxiv.org/abs/2505.21330)
*Christos Fragkathoulas,Evaggelia Pitoura*

Key words: counterfactual explanations, genetic algorithm, dynamic constraints, computational efficiency

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: UGCE是一种基于遗传算法的框架，动态更新反事实解释以适应用户约束的变化，提升计算效率并保持高质量结果。

Motivation: 现有反事实解释方法无法支持用户约束的动态迭代更新，需要重新计算，效率低下且不灵活。

Method: 提出User-Guided Incremental Counterfactual Exploration (UGCE)，基于遗传算法动态更新反事实解释。

Result: 在五个基准数据集上，UGCE显著提升计算效率并保持高质量结果，支持稳定性能，高效预热策略，并揭示约束类型对搜索行为的影响。

Conclusion: UGCE是一种高效且灵活的反事实解释方法，适用于动态用户约束场景。

Abstract: Counterfactual explanations (CFEs) are a popular approach for interpreting
machine learning predictions by identifying minimal feature changes that alter
model outputs. However, in real-world settings, users often refine feasibility
constraints over time, requiring counterfactual generation to adapt
dynamically. Existing methods fail to support such iterative updates, instead
recomputing explanations from scratch with each change, an inefficient and
rigid approach. We propose User-Guided Incremental Counterfactual Exploration
(UGCE), a genetic algorithm-based framework that incrementally updates
counterfactuals in response to evolving user constraints. Experimental results
across five benchmark datasets demonstrate that UGCE significantly improves
computational efficiency while maintaining high-quality solutions compared to a
static, non-incremental approach. Our evaluation further shows that UGCE
supports stable performance under varying constraint sequences, benefits from
an efficient warm-start strategy, and reveals how different constraint types
may affect search behavior.

</details>


### [256] [Joint Learning in the Gaussian Single Index Model](https://arxiv.org/abs/2505.21336)
*Loucas Pillaud-Vivien,Adrien Schertzer*

Key words: 高维高斯模型、非凸优化、梯度流、信息指数、RKHS

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了在高斯模型中联合学习一维投影和单变量函数的问题，分析了梯度流动态的收敛性，并提出了一种基于RKHS的实用方法。

Motivation: 动机在于解决高维数据中联合学习投影方向和非线性函数的非凸问题，为表示学习和非线性回归提供理论和方法支持。

Method: 使用自然交替方案分析梯度流动态，并通过信息指数控制收敛速度；实践中采用RKHS结构适应问题，实现高效函数估计。

Result: 证明即使在初始方向与目标负相关时仍能收敛，并验证了基于RKHS方法的有效性。

Conclusion: 研究为高维数据中低维结构的学习提供了理论见解和实用方法。

Abstract: We consider the problem of jointly learning a one-dimensional projection and
a univariate function in high-dimensional Gaussian models. Specifically, we
study predictors of the form $f(x)=\varphi^\star(\langle w^\star, x \rangle)$,
where both the direction $w^\star \in \mathcal{S}_{d-1}$, the sphere of
$\mathbb{R}^d$, and the function $\varphi^\star: \mathbb{R} \to \mathbb{R}$ are
learned from Gaussian data. This setting captures a fundamental non-convex
problem at the intersection of representation learning and nonlinear
regression. We analyze the gradient flow dynamics of a natural alternating
scheme and prove convergence, with a rate controlled by the information
exponent reflecting the \textit{Gaussian regularity} of the function
$\varphi^\star$. Strikingly, our analysis shows that convergence still occurs
even when the initial direction is negatively correlated with the target. On
the practical side, we demonstrate that such joint learning can be effectively
implemented using a Reproducing Kernel Hilbert Space (RKHS) adapted to the
structure of the problem, enabling efficient and flexible estimation of the
univariate function. Our results offer both theoretical insight and practical
methodology for learning low-dimensional structure in high-dimensional
settings.

</details>


### [257] [An Uncertainty-Aware ED-LSTM for Probabilistic Suffix Prediction](https://arxiv.org/abs/2505.21339)
*Henryk Mustroph,Michel Kunkler,Stefanie Rinderle-Ma*

Key words: 后缀预测, 业务流程, LSTM, Monte Carlo, 不确定性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种新的概率后缀预测方法，用于预测业务流程的未来事件序列，解决了单一预测在高度不确定性或变异性情况下的局限性。

Motivation: 当前的后缀预测方法仅预测最可能的单一后缀，但在高度不确定性或变异性情况下，单一预测的表达能力有限。

Method: 采用基于Uncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM)和Monte Carlo (MC)后缀采样算法的新方法，通过MC dropout捕捉认知不确定性，并通过学习损失衰减捕捉随机不确定性。

Result: 评估显示，U-ED-LSTM在不同数据集上具有合理的预测性能，平均概率后缀预测在罕见前缀或较长后缀上表现优于最可能预测，且能有效捕捉事件日志中的不确定性。

Conclusion: 概率后缀预测方法在表达能力和不确定性捕捉方面优于单一预测方法。

Abstract: Suffix prediction of business processes forecasts the remaining sequence of
events until process completion. Current approaches focus on predicting a
single, most likely suffix. However, if the future course of a process is
exposed to uncertainty or has high variability, the expressiveness of a single
suffix prediction can be limited. To address this limitation, we propose
probabilistic suffix prediction, a novel approach that approximates a
probability distribution of suffixes. The proposed approach is based on an
Uncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) and a Monte Carlo (MC)
suffix sampling algorithm. We capture epistemic uncertainties via MC dropout
and aleatoric uncertainties as learned loss attenuation. This technical report
provides a detailed evaluation of the U-ED-LSTM's predictive performance and
assesses its calibration on four real-life event logs with three different
hyperparameter settings. The results show that i) the U-ED-LSTM has reasonable
predictive performance across various datasets, ii) aggregating probabilistic
suffix predictions into mean values can outperform most likely predictions,
particularly for rare prefixes or longer suffixes, and iii) the approach
effectively captures uncertainties present in event logs.

</details>


### [258] [OVERT: A Benchmark for Over-Refusal Evaluation on Text-to-Image Models](https://arxiv.org/abs/2505.21347)
*Ziheng Cheng,Yixiao Huang,Hui Xu,Somayeh Sojoudi,Xuandong Zhao,Dawn Song,Song Mei*

Key words: Text-to-Image models, over-refusal, safety alignment, benchmark, prompt rewriting

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了OVERT，首个大规模评估文本到图像（T2I）模型过度拒绝（over-refusal）现象的基准，包含4600个看似有害但良性的提示及1785个真正有害的提示。研究发现T2I模型普遍存在过度拒绝问题，并尝试通过提示改写缓解，但效果有限。

Motivation: 现有T2I模型的安全对齐策略常导致过度拒绝（拒绝良性提示），降低模型实用性，但缺乏系统性评估。本文旨在填补这一空白，提供评估工具并探索改进方法。

Method: 构建OVERT基准，包含合成生成的4600个良性提示（OVERT-safe）及1785个有害提示（OVERT-unsafe），覆盖9个安全类别。评估多个T2I模型，并尝试通过提示改写减少过度拒绝。

Result: 发现T2I模型普遍存在过度拒绝问题，尤其在多个安全类别中。提示改写虽能部分缓解，但会损害原始提示的语义保真度。

Conclusion: 需进一步研究以平衡T2I模型的安全性与功能性的权衡。OVERT框架可灵活适配用户自定义安全策略，为后续研究提供工具支持。

Abstract: Text-to-Image (T2I) models have achieved remarkable success in generating
visual content from text inputs. Although multiple safety alignment strategies
have been proposed to prevent harmful outputs, they often lead to overly
cautious behavior -- rejecting even benign prompts -- a phenomenon known as
$\textit{over-refusal}$ that reduces the practical utility of T2I models.
Despite over-refusal having been observed in practice, there is no large-scale
benchmark that systematically evaluates this phenomenon for T2I models. In this
paper, we present an automatic workflow to construct synthetic evaluation data,
resulting in OVERT ($\textbf{OVE}$r-$\textbf{R}$efusal evaluation on
$\textbf{T}$ext-to-image models), the first large-scale benchmark for assessing
over-refusal behaviors in T2I models. OVERT includes 4,600 seemingly harmful
but benign prompts across nine safety-related categories, along with 1,785
genuinely harmful prompts (OVERT-unsafe) to evaluate the safety-utility
trade-off. Using OVERT, we evaluate several leading T2I models and find that
over-refusal is a widespread issue across various categories (Figure 1),
underscoring the need for further research to enhance the safety alignment of
T2I models without compromising their functionality.As a preliminary attempt to
reduce over-refusal, we explore prompt rewriting; however, we find it often
compromises faithfulness to the meaning of the original prompts. Finally, we
demonstrate the flexibility of our generation framework in accommodating
diverse safety requirements by generating customized evaluation data adapting
to user-defined policies.

</details>


### [259] [CRISP-NAM: Competing Risks Interpretable Survival Prediction with Neural Additive Models](https://arxiv.org/abs/2505.21360)
*Dhanesh Ramachandram*

Key words: 竞争风险, 生存分析, 可解释性, 神经网络, 医疗

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了CRISP-NAM模型，一种可解释的神经加法模型，用于竞争风险生存分析，保持特征级可解释性，并在多个数据集上展现了竞争优势。

Motivation: 在生存建模中，竞争风险是关键考量，尤其是在医疗领域，患者可能经历多种不同事件类型。需要一种既能建模复杂非线性关系又能保持可解释性的方法。

Method: CRISP-NAM通过针对每个特征的独立神经网络扩展神经加法架构，建模特定原因风险，同时可视化协变量与竞争风险之间的复杂非线性关系。

Result: 在多个数据集上表现优于现有方法，验证了其竞争性能。

Conclusion: CRISP-NAM为竞争风险生存分析提供了高效且可解释的解决方案。

Abstract: Competing risks are crucial considerations in survival modelling,
particularly in healthcare domains where patients may experience multiple
distinct event types. We propose CRISP-NAM (Competing Risks Interpretable
Survival Prediction with Neural Additive Models), an interpretable neural
additive model for competing risks survival analysis which extends the neural
additive architecture to model cause-specific hazards while preserving
feature-level interpretability. Each feature contributes independently to risk
estimation through dedicated neural networks, allowing for visualization of
complex non-linear relationships between covariates and each competing risk. We
demonstrate competitive performance on multiple datasets compared to existing
approaches.

</details>


### [260] [Subgroups Matter for Robust Bias Mitigation](https://arxiv.org/abs/2505.21363)
*Anissa Alloula,Charles Jones,Ben Glocker,Bartłomiej W. Papież*

Key words: 偏见缓解、子群定义、公平性、机器学习、评估

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究发现，偏见缓解技术的失败可能与子群定义的选择密切相关，某些子群定义甚至会导致比不缓解更差的结果，强调子群定义在提升模型公平性中的关键作用。

Motivation: 尽管偏见缓解方法不断涌现，但其效果参差不齐，且缺乏对失败原因的系统研究。本文假设子群定义可能是关键因素，并展开实验验证。

Method: 通过对多种视觉和语言分类任务的广泛评估，系统地比较了粗粒度、细粒度、交叉性和噪声子群定义的影响。

Result: 结果显示子群选择显著影响效果，某些子群甚至会恶化公平性。理论分析表明，有时需要选择不同于原始差异的子群来实现公平性优化。

Conclusion: 子群定义是偏见缓解的重要杠杆，需谨慎选择以实现模型的公平性和鲁棒性。

Abstract: Despite the constant development of new bias mitigation methods for machine
learning, no method consistently succeeds, and a fundamental question remains
unanswered: when and why do bias mitigation techniques fail? In this paper, we
hypothesise that a key factor may be the often-overlooked but crucial step
shared by many bias mitigation methods: the definition of subgroups. To
investigate this, we conduct a comprehensive evaluation of state-of-the-art
bias mitigation methods across multiple vision and language classification
tasks, systematically varying subgroup definitions, including coarse,
fine-grained, intersectional, and noisy subgroups. Our results reveal that
subgroup choice significantly impacts performance, with certain groupings
paradoxically leading to worse outcomes than no mitigation at all. Our findings
suggest that observing a disparity between a set of subgroups is not a
sufficient reason to use those subgroups for mitigation. Through theoretical
analysis, we explain these phenomena and uncover a counter-intuitive insight
that, in some cases, improving fairness with respect to a particular set of
subgroups is best achieved by using a different set of subgroups for
mitigation. Our work highlights the importance of careful subgroup definition
in bias mitigation and suggest it as a alternative lever for improving the
robustness and fairness of machine learning models.

</details>


### [261] [Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders](https://arxiv.org/abs/2505.21364)
*James Oldfield,Shawn Im,Yixuan Li,Mihalis A. Nicolaou,Ioannis Patras,Grigorios G Chrysos*

Key words: 多层感知机, 稀疏性, Mixture of Decoders, 语言模型, 张量分解

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 通过引入Mixture of Decoders (MxDs)方法，采用层级别稀疏性提升语言模型的稀疏性与准确性平衡，并在实验中优于现有方法。

Motivation: 多层感知机（MLPs）在大型语言模型中难以理解、编辑和引导，现有稀疏方法在保持准确性方面表现不足，因此探索层级别稀疏性以解决这一问题。

Method: 提出Mixture of Decoders (MxDs)，通过张量分解扩展预训练密集层为大量稀疏激活的子层，保持表达能力的同时实现高稀疏性。

Result: MxDs在3B参数的语言模型中表现优于现有方法（如Transcoders），并在稀疏探测和特征引导任务中验证了其有效性。

Conclusion: MxDs为设计既可解释又高保真的分解方法提供了新途径，实验验证了其在稀疏性和准确性上的优势。

Abstract: Multilayer perceptrons (MLPs) are an integral part of large language models,
yet their dense representations render them difficult to understand, edit, and
steer. Recent methods learn interpretable approximations via neuron-level
sparsity, yet fail to faithfully reconstruct the original
mapping--significantly increasing model's next-token cross-entropy loss. In
this paper, we advocate for moving to layer-level sparsity to overcome the
accuracy trade-off in sparse layer approximation. Under this paradigm, we
introduce Mixture of Decoders (MxDs). MxDs generalize MLPs and Gated Linear
Units, expanding pre-trained dense layers into tens of thousands of specialized
sublayers. Through a flexible form of tensor factorization, each sparsely
activating MxD sublayer implements a linear transformation with full-rank
weights--preserving the original decoders' expressive capacity even under heavy
sparsity. Experimentally, we show that MxDs significantly outperform
state-of-the-art methods (e.g., Transcoders) on the sparsity-accuracy frontier
in language models with up to 3B parameters. Further evaluations on sparse
probing and feature steering demonstrate that MxDs learn similarly specialized
features of natural language--opening up a promising new avenue for designing
interpretable yet faithful decompositions. Our code is included at:
https://github.com/james-oldfield/MxD/.

</details>


### [262] [PLANETALIGN: A Comprehensive Python Library for Benchmarking Network Alignment](https://arxiv.org/abs/2505.21366)
*Qi Yu,Zhichen Zeng,Yuchen Yan,Zhining Liu,Baoyu Jing,Ruizhong Qiu,Ariful Azad,Hanghang Tong*

Key words: 网络对齐、Python库、基准测试、数据集、评估指标

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: PLANETALIGN是一个用于网络对齐的Python库，整合了18个数据集和14种方法，提供标准化评估管道，促进网络对齐方法的系统开发和基准测试。

Motivation: 网络对齐是多网络学习任务的基础，但缺乏系统开发和基准测试的工具，因此开发了PLANETALIGN。

Method: PLANETALIGN整合了多种数据集和方法，提供易于使用的API和标准化评估管道，包括多种评估指标。

Result: 通过比较研究，揭示了现有网络对齐方法的优缺点。

Conclusion: PLANETALIGN有助于深化对网络对齐问题的理解，并促进未来更有效、可扩展和鲁棒的方法开发。

Abstract: Network alignment (NA) aims to identify node correspondence across different
networks and serves as a critical cornerstone behind various downstream
multi-network learning tasks. Despite growing research in NA, there lacks a
comprehensive library that facilitates the systematic development and
benchmarking of NA methods. In this work, we introduce PLANETALIGN, a
comprehensive Python library for network alignment that features a rich
collection of built-in datasets, methods, and evaluation pipelines with
easy-to-use APIs. Specifically, PLANETALIGN integrates 18 datasets and 14 NA
methods with extensible APIs for easy use and development of NA methods. Our
standardized evaluation pipeline encompasses a wide range of metrics, enabling
a systematic assessment of the effectiveness, scalability, and robustness of NA
methods. Through extensive comparative studies, we reveal practical insights
into the strengths and limitations of existing NA methods. We hope that
PLANETALIGN can foster a deeper understanding of the NA problem and facilitate
the development and benchmarking of more effective, scalable, and robust
methods in the future. The source code of PLANETALIGN is available at
https://github.com/yq-leo/PlanetAlign.

</details>


### [263] [Improving LLM-based Global Optimization with Search Space Partitioning](https://arxiv.org/abs/2505.21372)
*Andrej Schwanke,Lyubomir Ivanov,David Salinas,Fabio Ferreira,Aaron Klein,Frank Hutter,Arber Zela*

Key words: HOLLM, LLM, global optimization, bandit-inspired, blackbox functions

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: HOLLM是一种新颖的全局优化算法，通过将搜索空间划分为有前景的子区域，结合LLM驱动采样和类似赌徒臂的评分机制，解决了LLM在高维空间和缺乏领域先验知识时的局限性。

Motivation: LLM在昂贵黑盒函数的全局优化中表现不俗，但在高维搜索空间或缺乏领域先验知识的情况下，其生成的建议通常稀疏或无信息。因此，提出HOLLM来克服这些限制。

Method: HOLLM通过分区搜索空间为子区域作为“meta-arm”，并使用类似赌徒臂的评分机制来平衡探索与开发。在每个选定的子区域内，LLM无需领域知识即可生成高质量候选点。

Result: 在标准优化基准测试中，HOLLM表现与领先的贝叶斯优化和信任区域方法相媲美或更优，显著优于全局LLM采样策略。

Conclusion: HOLLM通过空间分区和评分机制显著提升了LLM在全局优化中的效果，尤其是在高维和无先验知识场景下。

Abstract: Large Language Models (LLMs) have recently emerged as effective surrogate
models and candidate generators within global optimization frameworks for
expensive blackbox functions. Despite promising results, LLM-based methods
often struggle in high-dimensional search spaces or when lacking
domain-specific priors, leading to sparse or uninformative suggestions. To
overcome these limitations, we propose HOLLM, a novel global optimization
algorithm that enhances LLM-driven sampling by partitioning the search space
into promising subregions. Each subregion acts as a ``meta-arm'' selected via a
bandit-inspired scoring mechanism that effectively balances exploration and
exploitation. Within each selected subregion, an LLM then proposes high-quality
candidate points, without any explicit domain knowledge. Empirical evaluation
on standard optimization benchmarks shows that HOLLM consistently matches or
surpasses leading Bayesian optimization and trust-region methods, while
substantially outperforming global LLM-based sampling strategies.

</details>


### [264] [DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models](https://arxiv.org/abs/2505.21382)
*Nastaran Saadati,Zhanhong Jiang,Joshua R. Waite,Shreyan Ganguly,Aditya Balu,Chinmay Hegde,Soumik Sarkar*

Key words: LoRA, 分散式训练, TSVD, DeCAF, 收敛分析, 视觉语言模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了DeCAF算法，通过结合DLoRA和TSVD矩阵分解来解决分散式LoRA训练中的共识干扰问题，并在理论和实验上验证了其高效性。

Motivation: 分散式LoRA（DLoRA）训练中缺乏平滑性保证和共识干扰问题阻碍了其理论发展和实际应用。

Method: 引入DeCAF算法，结合DLoRA和TSVD矩阵分解，确保梯度平滑性并解决共识干扰。

Result: DeCAF的收敛速度与分散式SGD匹配，实验证明其在视觉/语言任务上优于本地训练并与联邦学习竞争。

Conclusion: DeCAF通过理论分析和实验验证，解决了分散式LoRA的训练问题，展现了高效性和适应性。

Abstract: Low-Rank Adaptation (LoRA) has emerged as one of the most effective,
computationally tractable fine-tuning approaches for training Vision-Language
Models (VLMs) and Large Language Models (LLMs). LoRA accomplishes this by
freezing the pre-trained model weights and injecting trainable low-rank
matrices, allowing for efficient learning of these foundation models even on
edge devices. However, LoRA in decentralized settings still remains under
explored, particularly for the theoretical underpinnings due to the lack of
smoothness guarantee and model consensus interference (defined formally below).
This work improves the convergence rate of decentralized LoRA (DLoRA) to match
the rate of decentralized SGD by ensuring gradient smoothness. We also
introduce DeCAF, a novel algorithm integrating DLoRA with truncated singular
value decomposition (TSVD)-based matrix factorization to resolve consensus
interference. Theoretical analysis shows TSVD's approximation error is bounded
and consensus differences between DLoRA and DeCAF vanish as rank increases,
yielding DeCAF's matching convergence rate. Extensive experiments across
vision/language tasks demonstrate our algorithms outperform local training and
rivals federated learning under both IID and non-IID data distributions.

</details>


### [265] [Finite Sample Analysis of Linear Temporal Difference Learning with Arbitrary Features](https://arxiv.org/abs/2505.21391)
*Zixuan Xie,Xinyu Liu,Rohan Chandra,Shangtong Zhang*

Key words: 强化学习, 线性 TD(λ), 收敛速率, 随机逼近, 任意特征

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文首次在任意特征下确立了线性 TD(λ) 的 L2 收敛速率，无需修改算法或额外假设，适用于折扣和平均奖励设置。

Motivation: 以往收敛速率通常基于线性无关特征的假设，而实际场景中这一假设常不成立，因此需要研究在任意特征下的收敛性能。

Method: 开发了一种新的随机逼近方法，处理由任意特征导致的解非唯一性问题，收敛到解集而非单点。

Result: 证明了线性 TD(λ) 在任意特征下的 L2 收敛速率，适用于折扣和平均奖励场景。

Conclusion: 该研究为任意特征下的线性 TD(λ) 提供了理论基础，扩展了其实际应用范围。

Abstract: Linear TD($\lambda$) is one of the most fundamental reinforcement learning
algorithms for policy evaluation. Previously, convergence rates are typically
established under the assumption of linearly independent features, which does
not hold in many practical scenarios. This paper instead establishes the first
$L^2$ convergence rates for linear TD($\lambda$) operating under arbitrary
features, without making any algorithmic modification or additional
assumptions. Our results apply to both the discounted and average-reward
settings. To address the potential non-uniqueness of solutions resulting from
arbitrary features, we develop a novel stochastic approximation result
featuring convergence rates to the solution set instead of a single point.

</details>


### [266] [Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits](https://arxiv.org/abs/2505.21393)
*Maoli Liu,Zhuohua Li,Xiangxiang Dai,John C. S. Lui*

Key words: 对话推荐系统、上下文赌博机、遗憾界、偏好学习、自适应对话

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出三种新算法（CLiSK、CLiME、CLiSK-ME），解决了对话推荐系统中现有方法的探索不足和对话触发不智能的问题，理论证明其后悔上界更优，实验显示性能提升至少14.6%。

Motivation: 现有对话推荐系统在关键术语选择和对话触发上存在不足，导致偏好学习不充分且效率低下。

Method: 提出CLiSK（平滑关键术语上下文）、CLiME（基于偏好不确定性的自适应对话触发）及CLiSK-ME（两者结合）三种算法。

Result: 理论证明后悔上界为O(√(dTlogT))，接近极小极大最优；实验显示累积后悔至少提升14.6%。

Conclusion: 新算法显著优化了对话推荐系统的性能，平衡了探索与利用。

Abstract: Conversational recommender systems proactively query users with relevant "key
terms" and leverage the feedback to elicit users' preferences for personalized
recommendations. Conversational contextual bandits, a prevalent approach in
this domain, aim to optimize preference learning by balancing exploitation and
exploration. However, several limitations hinder their effectiveness in
real-world scenarios. First, existing algorithms employ key term selection
strategies with insufficient exploration, often failing to thoroughly probe
users' preferences and resulting in suboptimal preference estimation. Second,
current algorithms typically rely on deterministic rules to initiate
conversations, causing unnecessary interactions when preferences are
well-understood and missed opportunities when preferences are uncertain. To
address these limitations, we propose three novel algorithms: CLiSK, CLiME, and
CLiSK-ME. CLiSK introduces smoothed key term contexts to enhance exploration in
preference learning, CLiME adaptively initiates conversations based on
preference uncertainty, and CLiSK-ME integrates both techniques. We
theoretically prove that all three algorithms achieve a tighter regret upper
bound of $O(\sqrt{dT\log{T}})$ with respect to the time horizon $T$, improving
upon existing methods. Additionally, we provide a matching lower bound
$\Omega(\sqrt{dT})$ for conversational bandits, demonstrating that our
algorithms are nearly minimax optimal. Extensive evaluations on both synthetic
and real-world datasets show that our approaches achieve at least a 14.6%
improvement in cumulative regret.

</details>


### [267] [Square$χ$PO: Differentially Private and Robust $χ^2$-Preference Optimization in Offline Direct Alignment](https://arxiv.org/abs/2505.21395)
*Xingyu Zhou,Yulian Wu,Wenqian Weng,Francesco Orabona*

Key words: 关键词：语言模型对齐，差分隐私，标签污染，鲁棒性，SquareχPO

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 摘要：本文提出了SquareχPO，一种改进离线语言模型对齐的方法，通过替换标准损失函数为平方损失，在差分隐私和鲁棒性方面取得突破，并在隐私保护和数据污染场景下表现优异。

Motivation: 研究动机在于解决语言模型离线对齐中的隐私保护和标签污染问题，提出新方法来提升模型的鲁棒性和隐私保护能力。

Method: 方法是通过引入SquareχPO，将标准log-loss替换为概率的平方损失，以应对隐私保护和数据污染的挑战。

Result: 结果表明，SquareχPO在差分隐私和鲁棒性方面达到最优性能，首次在提示（响应）和标签的隐私保护模型中取得成果，并能同时处理隐私和污染问题。

Conclusion: 结论是SquareχPO为语言模型对齐提供了一种高效的方法，尤其是在隐私保护和数据污染环境下，且理论分析统一，具有独立学术价值。

Abstract: In this paper, we theoretically study the offline alignment of language
models with human preference feedback, under both preference label corruption
and privacy protections. To this end, we propose Square$\chi$PO, a simple
one-line change to $\chi$PO where the standard log-loss is replaced by a new
square loss over probability. Thanks to the inherent properties of this new
loss, we have advanced the state-of-the-art of differentially private and
robust offline direct alignment. Specifically, for the local model of label
privacy, Square$\chi$PO is the first algorithm that attains an optimal rate
based on single-policy concentrability even with general function
approximations. It also gives the first result under the central model of
privacy protection over both prompts (responses) and labels. On the robustness
side against Huber label corruption, Square$\chi$PO is the first alignment
method that has a meaningful theoretical guarantee under general function
approximations. More importantly, Square$\chi$PO can address privacy protection
and corruption simultaneously, where an interesting separation is observed,
implying that the order of privacy and corruption matters. Furthermore, we show
that Square$\chi$PO can also be easily extended to handle the scenario of the
general preference model with state-of-the-art guarantees under corruption and
privacy. Last but not least, all of our theoretical guarantees enjoy a unified
analysis, building upon a new result on the generalization error bounds of
least-square regression under corruption and privacy constraints, which we
believe is of independent interest to the community.

</details>


### [268] [A Convergence Theory for Diffusion Language Models: An Information-Theoretic Perspective](https://arxiv.org/abs/2505.21400)
*Gen Li,Changxiao Cai*

Key words: 扩散模型, 语言模型, 信息论, 收敛分析, KL散度

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于信息论视角的扩散语言模型收敛性保证，证明了采样误差随迭代次数递减，并与目标文本序列中token的互信息线性相关。

Motivation: 尽管扩散模型在生成建模中表现出色，但其理论理解仍然不足。本文旨在通过信息论分析填补这一空白。

Method: 从信息论角度分析扩散语言模型的收敛性，提出基于Kullback-Leibler (KL) 散度的采样误差分析框架，建立了收敛上下界。

Result: 采样误差随迭代次数$T$递减，且与目标文本中token的互信息线性相关。上下界匹配证明了分析的紧致性。

Conclusion: 扩散语言模型的有效性得到了理论支持，为未来研究提供了坚实的理论基础。

Abstract: Diffusion models have emerged as a powerful paradigm for modern generative
modeling, demonstrating strong potential for large language models (LLMs).
Unlike conventional autoregressive (AR) models that generate tokens
sequentially, diffusion models enable parallel token sampling, leading to
faster generation and eliminating left-to-right generation constraints. Despite
their empirical success, the theoretical understanding of diffusion model
approaches remains underdeveloped. In this work, we develop convergence
guarantees for diffusion language models from an information-theoretic
perspective. Our analysis demonstrates that the sampling error, measured by the
Kullback-Leibler (KL) divergence, decays inversely with the number of
iterations $T$ and scales linearly with the mutual information between tokens
in the target text sequence. In particular, we establish matching upper and
lower bounds, up to some constant factor, to demonstrate the tightness of our
convergence analysis. These results offer novel theoretical insights into the
practical effectiveness of diffusion language models.

</details>


### [269] [Dual Natural Gradient Descent for Scalable Training of Physics-Informed Neural Networks](https://arxiv.org/abs/2505.21404)
*Anas Jnini,Flavio Vella*

Key words: 自然梯度、PINN、Gauss-Newton、残差空间、D-NGD

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: D-NGD方法通过在残差空间中计算Gauss-Newton步，降低了计算复杂度，并成功扩展到大规模PINN训练。

Motivation: 传统自然梯度方法在参数空间中的Gauss-Newton更新计算复杂度高（O(n^3)），难以扩展到大规模网络。

Method: 提出D-NGD方法，在残差空间中计算Gauss-Newton步，并通过测地加速度校正优化性能，支持直接求解和预处理共轭梯度求解。

Result: D-NGD支持单GPU上高达1280万参数的PINN训练，相比一阶方法（Adam、SGD）和拟牛顿方法，最终误差降低1-3个数量级。

Conclusion: D-NGD显著提升了PINN的优化效率和规模，为大规模问题提供了可行的二阶优化方案。

Abstract: Natural-gradient methods markedly accelerate the training of Physics-Informed
Neural Networks (PINNs), yet their Gauss--Newton update must be solved in the
parameter space, incurring a prohibitive $O(n^3)$ time complexity, where $n$ is
the number of network trainable weights. We show that exactly the same step can
instead be formulated in a generally smaller residual space of size $m =
\sum_{\gamma} N_{\gamma} d_{\gamma}$, where each residual class $\gamma$ (e.g.
PDE interior, boundary, initial data) contributes $N_{\gamma}$ collocation
points of output dimension $d_{\gamma}$.
  Building on this insight, we introduce \textit{Dual Natural Gradient Descent}
(D-NGD). D-NGD computes the Gauss--Newton step in residual space, augments it
with a geodesic-acceleration correction at negligible extra cost, and provides
both a dense direct solver for modest $m$ and a Nystrom-preconditioned
conjugate-gradient solver for larger $m$.
  Experimentally, D-NGD scales second-order PINN optimization to networks with
up to 12.8 million parameters, delivers one- to three-order-of-magnitude lower
final error $L^2$ than first-order methods (Adam, SGD) and quasi-Newton
methods, and -- crucially -- enables natural-gradient training of PINNs at this
scale on a single GPU.

</details>


### [270] [A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment](https://arxiv.org/abs/2505.21414)
*Brett Bissey,Kyle Gatesman,Walker Dimon,Mohammad Alam,Luis Robaina,Joseph Weissman*

Key words: 深度强化学习、对抗攻击、决策支持系统、安全性评估、CyberStrike

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一个框架，用于分析和保护基于深度强化学习的决策支持系统，通过模拟发现行为模式和漏洞，并评估对抗攻击的影响。

Motivation: 开发一个工具来识别和评估深度强化学习系统的潜在漏洞，以确保在高风险环境中部署前的安全性。

Method: 提出一个框架，支持生成精确的观察扰动，通过模拟评估对抗攻击效果，并在自定义战略游戏CyberStrike中进行验证。

Result: 实验展示了对抗攻击对观测索引和时间步的影响，并探讨了攻击在不同DRL架构和算法间的可转移性。

Conclusion: 强调在关键决策环境中，需要强大的对抗防御机制保护DRL策略安全。

Abstract: This paper introduces a comprehensive framework designed to analyze and
secure decision-support systems trained with Deep Reinforcement Learning (DRL),
prior to deployment, by providing insights into learned behavior patterns and
vulnerabilities discovered through simulation. The introduced framework aids in
the development of precisely timed and targeted observation perturbations,
enabling researchers to assess adversarial attack outcomes within a strategic
decision-making context. We validate our framework, visualize agent behavior,
and evaluate adversarial outcomes within the context of a custom-built
strategic game, CyberStrike. Utilizing the proposed framework, we introduce a
method for systematically discovering and ranking the impact of attacks on
various observation indices and time-steps, and we conduct experiments to
evaluate the transferability of adversarial attacks across agent architectures
and DRL training algorithms. The findings underscore the critical need for
robust adversarial defense mechanisms to protect decision-making policies in
high-stakes environments.

</details>


### [271] [When Shift Happens - Confounding Is to Blame](https://arxiv.org/abs/2505.21422)
*Abbavaram Gowtham Reddy,Celia Rubio-Madrigal,Rebekka Burkholz,Krikamol Muandet*

Key words: 分布偏移, OOD泛化, 隐藏混杂因素, 环境特定关系, 经验风险最小化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究发现，尽管传统观点认为学习因果不变表征能提升模型在分布偏移下的鲁棒性，但实证研究表明，经验风险最小化（ERM）的分布外（OOD）泛化性能可能优于现有方法，尤其是在使用所有协变量时。这归因于隐藏混杂因素的影响。

Motivation: 探讨分布偏移对机器学习模型鲁棒性和泛化能力的影响，揭示隐藏混杂因素的作用，并提供新的理论见解和实践指导。

Method: 通过实证和理论分析，研究隐藏混杂因素如何影响数据分布变化，并提出通过环境特定关系学习和使用隐藏混杂因素的代理变量来改进泛化能力。

Result: 发现ERM在OOD泛化中表现优异，且利用所有协变量能进一步提升性能。证明了在存在隐藏混杂偏移时，学习环境特定关系是必要的。

Conclusion: 隐藏混杂因素是影响OOD泛化的关键因素，设计泛化算法时应考虑环境特定关系并利用协变量代理以提升模型鲁棒性。

Abstract: Distribution shifts introduce uncertainty that undermines the robustness and
generalization capabilities of machine learning models. While conventional
wisdom suggests that learning causal-invariant representations enhances
robustness to such shifts, recent empirical studies present a counterintuitive
finding: (i) empirical risk minimization (ERM) can rival or even outperform
state-of-the-art out-of-distribution (OOD) generalization methods, and (ii) its
OOD generalization performance improves when all available covariates, not just
causal ones, are utilized. Drawing on both empirical and theoretical evidence,
we attribute this phenomenon to hidden confounding. Shifts in hidden
confounding induce changes in data distributions that violate assumptions
commonly made by existing OOD generalization approaches. Under such conditions,
we prove that effective generalization requires learning environment-specific
relationships, rather than relying solely on invariant ones. Furthermore, we
show that models augmented with proxies for hidden confounders can mitigate the
challenges posed by hidden confounding shifts. These findings offer new
theoretical insights and practical guidance for designing robust OOD
generalization algorithms and principled covariate selection strategies.

</details>


### [272] [Conflicting Biases at the Edge of Stability: Norm versus Sharpness Regularization](https://arxiv.org/abs/2505.21423)
*Vit Fojtik,Maria Matveev,Hung-Hsu Chou,Gitta Kutyniok,Johannes Maly*

Key words: 过度参数化神经网络, 梯度下降, 隐性偏差, 边缘稳定性区域, 泛化能力

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了过参数化神经网络泛化能力的隐性偏差，分析了梯度下降在边缘稳定性区域的行为，并指出学习率在平衡参数范数与损失函数尖锐度之间的作用。

Motivation: 理解梯度下降在训练神经网络时如何通过隐性偏差（如参数范数最小化和损失函数尖锐度最小化）影响泛化能力，从而更全面地解释其良好的泛化表现。

Method: 通过理论分析和实验验证，研究了梯度下降在简单回归任务中对角线性网络的行为，特别关注学习率对隐性偏差的影响。

Result: 研究发现学习率在平衡参数范数与最小化损失函数尖锐度之间起到关键作用，且单独依赖某一种隐性偏差无法最小化泛化误差。

Conclusion: 仅关注单一隐性偏差不足以解释良好的泛化表现，需要更广泛地理解非零学习率动态权衡下的隐性正则化。

Abstract: A widely believed explanation for the remarkable generalization capacities of
overparameterized neural networks is that the optimization algorithms used for
training induce an implicit bias towards benign solutions. To grasp this
theoretically, recent works examine gradient descent and its variants in
simplified training settings, often assuming vanishing learning rates. These
studies reveal various forms of implicit regularization, such as $\ell_1$-norm
minimizing parameters in regression and max-margin solutions in classification.
Concurrently, empirical findings show that moderate to large learning rates
exceeding standard stability thresholds lead to faster, albeit oscillatory,
convergence in the so-called Edge-of-Stability regime, and induce an implicit
bias towards minima of low sharpness (norm of training loss Hessian). In this
work, we argue that a comprehensive understanding of the generalization
performance of gradient descent requires analyzing the interaction between
these various forms of implicit regularization. We empirically demonstrate that
the learning rate balances between low parameter norm and low sharpness of the
trained model. We furthermore prove for diagonal linear networks trained on a
simple regression task that neither implicit bias alone minimizes the
generalization error. These findings demonstrate that focusing on a single
implicit bias is insufficient to explain good generalization, and they motivate
a broader view of implicit regularization that captures the dynamic trade-off
between norm and sharpness induced by non-negligible learning rates.

</details>


### [273] [Attribute-Efficient PAC Learning of Sparse Halfspaces with Constant Malicious Noise Rate](https://arxiv.org/abs/2505.21430)
*Shiwei Zeng,Jie Shen*

Key words: 属性高效学习, 稀疏半空间, 恶意噪声, PAC学习, 铰链损失最小化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了在存在恒定恶意噪声的情况下，学习稀疏半空间的属性高效PAC学习算法，通过改进现有铰链损失最小化程序实现了属性效率。

Motivation: 随着机器学习算法面临普遍的数据损坏甚至对抗攻击，设计对噪声具有鲁棒性的高效算法变得至关重要。本文旨在解决在恒定恶意噪声下学习稀疏半空间的问题。

Method: 本文遵循近期工作，假设基础分布满足一定浓度条件和边缘条件，通过对现有铰链损失最小化程序进行简单变体实现属性效率，并提出了针对稀疏约束的新梯度分析。

Result: 提出了一种在恒定恶意噪声率下工作的属性高效PAC学习算法，并通过新的梯度分析成功处理了稀疏约束问题。

Conclusion: 在满足浓度和边缘条件的情况下，通过改进现有算法可以实现对稀疏半空间的属性高效学习。

Abstract: Attribute-efficient learning of sparse halfspaces has been a fundamental
problem in machine learning theory. In recent years, machine learning
algorithms are faced with prevalent data corruptions or even adversarial
attacks. It is of central interest to design efficient algorithms that are
robust to noise corruptions. In this paper, we consider that there exists a
constant amount of malicious noise in the data and the goal is to learn an
underlying $s$-sparse halfspace $w^* \in \mathbb{R}^d$ with $\text{poly}(s,\log
d)$ samples. Specifically, we follow a recent line of works and assume that the
underlying distribution satisfies a certain concentration condition and a
margin condition at the same time. Under such conditions, we show that
attribute-efficiency can be achieved by simple variants to existing hinge loss
minimization programs. Our key contribution includes: 1) an attribute-efficient
PAC learning algorithm that works under constant malicious noise rate; 2) a new
gradient analysis that carefully handles the sparsity constraint in hinge loss
minimization.

</details>


### [274] [Measuring Fine-Grained Relatedness in Multitask Learning via Data Attribution](https://arxiv.org/abs/2505.21438)
*Yiwen Tu,Ziqi Liu,Jiaqi W. Ma,Weijing Tang*

Key words: 多任务学习, 数据归因, 影响函数, 任务相关性, 负迁移

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究提出了多任务影响函数(MTIF)，将数据归因扩展到多任务学习中，用于精细测量任务相关性并通过数据选择策略缓解负迁移。实验表明MTIF能高效近似模型性能并提升多任务学习效果。

Motivation: 解决多任务学习中任务相关性测量和负迁移缓解的关键挑战。

Method: 提出多任务影响函数(MTIF)，适配硬或软参数共享的多任务模型，提供实例级任务相关性测量。基于此设计数据选择策略。

Result: MTIF能高效近似数据子集训练模型性能，其数据选择策略显著提升多任务学习模型效果。

Conclusion: MTIF建立了数据归因与多任务学习的新联系，为任务相关性测量和模型优化提供了高效精细的方案。

Abstract: Measuring task relatedness and mitigating negative transfer remain a critical
open challenge in Multitask Learning (MTL). This work extends data attribution
-- which quantifies the influence of individual training data points on model
predictions -- to MTL setting for measuring task relatedness. We propose the
MultiTask Influence Function (MTIF), a method that adapts influence functions
to MTL models with hard or soft parameter sharing. Compared to conventional
task relatedness measurements, MTIF provides a fine-grained, instance-level
relatedness measure beyond the entire-task level. This fine-grained relatedness
measure enables a data selection strategy to effectively mitigate negative
transfer in MTL. Through extensive experiments, we demonstrate that the
proposed MTIF efficiently and accurately approximates the performance of models
trained on data subsets. Moreover, the data selection strategy enabled by MTIF
consistently improves model performance in MTL. Our work establishes a novel
connection between data attribution and MTL, offering an efficient and
fine-grained solution for measuring task relatedness and enhancing MTL models.

</details>


### [275] [Can Large Reasoning Models Self-Train?](https://arxiv.org/abs/2505.21444)
*Sheikh Shafayat,Fahim Tajwar,Ruslan Salakhutdinov,Jeff Schneider,Andrea Zanette*

Key words: 自训练, 强化学习, 大语言模型, 数学推理, 奖励黑客

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种在线自训练强化学习算法，利用模型的自一致性推断正确信号，无需真实监督即可训练。在数学推理任务中，其性能与基于黄金标准答案训练的强化学习方法相当，但也暴露了自生成奖励可能导致错误输出被偏好的问题。

Motivation: 减少对人工监督的依赖是提升大语言模型性能的关键。论文探索自训练方法，利用模型的自我判断作为监督信号，避免依赖人工设计的验证器。

Method: 提出在线自训练强化学习算法，通过模型自一致性推断正确性信号，完全无需真实监督数据。算法应用于数学推理任务验证有效性。

Result: 算法在数学推理任务中快速达到与基于黄金标准答案的强化学习方法相当的性能，但发现自生成奖励可能激励模型偏好自信的错误输出（奖励黑客）。

Conclusion: 自监督改进能显著提升性能且无需外部标签，但也面临奖励黑客等固有挑战。未来需进一步解决代理奖励与真实目标的偏差问题。

Abstract: Scaling the performance of large language models (LLMs) increasingly depends
on methods that reduce reliance on human supervision. Reinforcement learning
from automated verification offers an alternative, but it incurs scalability
limitations due to dependency upon human-designed verifiers. Self-training,
where the model's own judgment provides the supervisory signal, presents a
compelling direction. We propose an online self-training reinforcement learning
algorithm that leverages the model's self-consistency to infer correctness
signals and train without any ground-truth supervision. We apply the algorithm
to challenging mathematical reasoning tasks and show that it quickly reaches
performance levels rivaling reinforcement-learning methods trained explicitly
on gold-standard answers. Additionally, we analyze inherent limitations of the
algorithm, highlighting how the self-generated proxy reward initially
correlated with correctness can incentivize reward hacking, where confidently
incorrect outputs are favored. Our results illustrate how self-supervised
improvement can achieve significant performance gains without external labels,
while also revealing its fundamental challenges.

</details>


### [276] [Designing Cyclic Peptides via Harmonic SDE with Atom-Bond Modeling](https://arxiv.org/abs/2505.21452)
*Xiangxin Zhou,Mingyu Li,Yi Xiao,Jiahan Li,Dongyu Xue,Zaixiang Zheng,Jianzhu Ma,Quanquan Gu*

Key words: 环肽, 生成模型, 结构预测, 药物设计, CpSDE

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文介绍了CpSDE方法，通过结合AtomSDE和ResRouter两个模型，生成多样化的环肽，克服了数据稀缺和几何限制等挑战。

Motivation: 环肽在药物开发中具有稳定性和亲和力优势，但当前方法在设计和多样性方面存在挑战，如3D数据稀缺和几何约束。

Method: 提出了CpSDE方法，包含AtomSDE（基于谐波SDE的结构预测模型）和ResRouter（残基类型预测器），通过交替更新序列和结构生成环肽。

Result: 实验证明，CpSDE设计的环肽表现出可靠的稳定性和亲和力。

Conclusion: CpSDE克服了现有数据限制，能有效设计多样化的环肽。

Abstract: Cyclic peptides offer inherent advantages in pharmaceuticals. For example,
cyclic peptides are more resistant to enzymatic hydrolysis compared to linear
peptides and usually exhibit excellent stability and affinity. Although deep
generative models have achieved great success in linear peptide design, several
challenges prevent the development of computational methods for designing
diverse types of cyclic peptides. These challenges include the scarcity of 3D
structural data on target proteins and associated cyclic peptide ligands, the
geometric constraints that cyclization imposes, and the involvement of
non-canonical amino acids in cyclization. To address the above challenges, we
introduce CpSDE, which consists of two key components: AtomSDE, a generative
structure prediction model based on harmonic SDE, and ResRouter, a residue type
predictor. Utilizing a routed sampling algorithm that alternates between these
two models to iteratively update sequences and structures, CpSDE facilitates
the generation of cyclic peptides. By employing explicit all-atom and bond
modeling, CpSDE overcomes existing data limitations and is proficient in
designing a wide variety of cyclic peptides. Our experimental results
demonstrate that the cyclic peptides designed by our method exhibit reliable
stability and affinity.

</details>


### [277] [High-Dimensional Calibration from Swap Regret](https://arxiv.org/abs/2505.21460)
*Maxwell Fishelson,Noah Golowich,Mehryar Mohri,Jon Schneider*

Key words: 在线校准、多维度预测、在线线性优化、交换遗憾、TreeSwap算法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究探讨了多维度预测的在线校准问题，通过外部遗憾最小化与在线线性优化的联系，提出了一种无需优化子程序或最优率知识即可实现校准预测的算法，并证明了校准误差下界。

Motivation: 研究多维度预测校准与在线线性优化中遗憾最小化的联系，旨在提升在线校准的效率与实践可行性。

Method: 将校准误差与交换遗憾关联，采用TreeSwap算法结合Follow-The-Leader子程序，无需依赖具体问题设定或最优率即可优化校准。

Result: 证明了在多维单纯形和ℓ1范数下，T=exp(O(logd/ε²))轮可获得ε校准预测，并指出校准误差下界需指数级依赖1/ε。

Conclusion: 通用算法框架能适应不同参数设定，校准问题本质需指数级时间，为理论界限与算法设计提供新视角。

Abstract: We study the online calibration of multi-dimensional forecasts over an
arbitrary convex set $\mathcal{P} \subset \mathbb{R}^d$ relative to an
arbitrary norm $\Vert\cdot\Vert$. We connect this with the problem of external
regret minimization for online linear optimization, showing that if it is
possible to guarantee $O(\sqrt{\rho T})$ worst-case regret after $T$ rounds
when actions are drawn from $\mathcal{P}$ and losses are drawn from the dual
$\Vert \cdot \Vert_*$ unit norm ball, then it is also possible to obtain
$\epsilon$-calibrated forecasts after $T = \exp(O(\rho /\epsilon^2))$ rounds.
When $\mathcal{P}$ is the $d$-dimensional simplex and $\Vert \cdot \Vert$ is
the $\ell_1$-norm, the existence of $O(\sqrt{T\log d})$-regret algorithms for
learning with experts implies that it is possible to obtain
$\epsilon$-calibrated forecasts after $T = \exp(O(\log{d}/\epsilon^2)) =
d^{O(1/\epsilon^2)}$ rounds, recovering a recent result of Peng (2025).
  Interestingly, our algorithm obtains this guarantee without requiring access
to any online linear optimization subroutine or knowledge of the optimal rate
$\rho$ -- in fact, our algorithm is identical for every setting of
$\mathcal{P}$ and $\Vert \cdot \Vert$. Instead, we show that the optimal
regularizer for the above OLO problem can be used to upper bound the above
calibration error by a swap regret, which we then minimize by running the
recent TreeSwap algorithm with Follow-The-Leader as a subroutine.
  Finally, we prove that any online calibration algorithm that guarantees
$\epsilon T$ $\ell_1$-calibration error over the $d$-dimensional simplex
requires $T \geq \exp(\mathrm{poly}(1/\epsilon))$ (assuming $d \geq
\mathrm{poly}(1/\epsilon)$). This strengthens the corresponding
$d^{\Omega(\log{1/\epsilon})}$ lower bound of Peng, and shows that an
exponential dependence on $1/\epsilon$ is necessary.

</details>


### [278] [Causal Posterior Estimation](https://arxiv.org/abs/2505.21468)
*Simon Dirmeier,Antonietta Mira*

Key words: Causal Posterior Estimation, Bayesian inference, simulator models, normalizing flows

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: CPE是一种用于模拟器模型中贝叶斯推理的新方法，通过结合模型的条件依赖结构来提高后验分布的近似精度，并提出了一种降低计算复杂度的采样方法。

Motivation: 解决模拟器模型中因似然函数难以计算或计算成本高而导致的贝叶斯推理难题。

Method: 使用归一化流（NF）近似后验分布，结合模型的条件依赖结构设计神经网络，并提出离散和连续NF架构及高效的采样方法。

Result: 实验表明，CPE在精度上优于或匹配当前最佳方法。

Conclusion: 通过直接结合模型的图结构依赖，CPE能实现高精度的后验推断。

Abstract: We present Causal Posterior Estimation (CPE), a novel method for Bayesian
inference in simulator models, i.e., models where the evaluation of the
likelihood function is intractable or too computationally expensive, but where
one can simulate model outputs given parameter values. CPE utilizes a
normalizing flow-based (NF) approximation to the posterior distribution which
carefully incorporates the conditional dependence structure induced by the
graphical representation of the model into the neural network. Thereby it is
possible to improve the accuracy of the approximation. We introduce both
discrete and continuous NF architectures for CPE and propose a constant-time
sampling procedure for the continuous case which reduces the computational
complexity of drawing samples to O(1) as for discrete NFs. We show, through an
extensive experimental evaluation, that by incorporating the conditional
dependencies induced by the graphical model directly into the neural network,
rather than learning them from data, CPE is able to conduct highly accurate
posterior inference either outperforming or matching the state of the art in
the field.

</details>


### [279] [Algorithms and SQ Lower Bounds for Robustly Learning Real-valued Multi-index Models](https://arxiv.org/abs/2505.21475)
*Ilias Diakonikolas,Giannis Iakovidis,Daniel M. Kane,Lisheng Ren*

Key words: 多指数模型, PAC学习, 对抗噪声, 统计查询, Lipschitz网络

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了在高斯分布下学习实值多指数模型（MIMs）的复杂性，提出了一个对抗标签噪声时的通用学习算法，并证明了算法的复杂性在维度上是定性最优的。

Motivation: 动机在于解决对抗标签噪声背景下学习MIMs的复杂性挑战，并探索算法的效率与维度之间的关系。

Method: 方法包括提出一个基于平方损失的通用PAC学习算法，并利用统计查询（SQ）下界验证其复杂性最优性。

Result: 结果表明，算法在对抗噪声和可实现噪声场景下的复杂性分别为$d^{O(m)}2^{\mathrm{poly}(K/\epsilon)}$和$d^{O(m)}2^{\mathrm{poly}(K)}(1/\epsilon)^{O(K)}$，且SQ下界支持其最优性。

Conclusion: 结论是该算法首次实现了对Lipschitz齐次ReLU网络的高效学习，且复杂性独立于网络大小。

Abstract: We study the complexity of learning real-valued Multi-Index Models (MIMs)
under the Gaussian distribution. A $K$-MIM is a function $f:\mathbb{R}^d\to
\mathbb{R}$ that depends only on the projection of its input onto a
$K$-dimensional subspace. We give a general algorithm for PAC learning a broad
class of MIMs with respect to the square loss, even in the presence of
adversarial label noise. Moreover, we establish a nearly matching Statistical
Query (SQ) lower bound, providing evidence that the complexity of our algorithm
is qualitatively optimal as a function of the dimension. Specifically, we
consider the class of bounded variation MIMs with the property that degree at
most $m$ distinguishing moments exist with respect to projections onto any
subspace. In the presence of adversarial label noise, the complexity of our
learning algorithm is $d^{O(m)}2^{\mathrm{poly}(K/\epsilon)}$. For the
realizable and independent noise settings, our algorithm incurs complexity
$d^{O(m)}2^{\mathrm{poly}(K)}(1/\epsilon)^{O(K)}$. To complement our upper
bound, we show that if for some subspace degree-$m$ distinguishing moments do
not exist, then any SQ learner for the corresponding class of MIMs requires
complexity $d^{\Omega(m)}$. As an application, we give the first efficient
learner for the class of positive-homogeneous $L$-Lipschitz $K$-MIMs. The
resulting algorithm has complexity $\mathrm{poly}(d)
2^{\mathrm{poly}(KL/\epsilon)}$. This gives a new PAC learning algorithm for
Lipschitz homogeneous ReLU networks with complexity independent of the network
size, removing the exponential dependence incurred in prior work.

</details>


### [280] [Hardware-Efficient Attention for Fast Decoding](https://arxiv.org/abs/2505.21487)
*Ted Zadouri,Hubert Strauss,Tri Dao*

Key words: LLM decoding, KV cache, attention mechanism, parallel computing

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种新的注意力机制设计（GTA和GLA），以减少大模型解码时的内存传输需求并提高硬件效率，实验表明其在保持模型质量的同时显著提升了解码速度和吞吐量。

Motivation: 现有的大模型解码方法在内存传输和并行性方面存在瓶颈，限制了硬件效率的充分发挥。

Method: 通过提出Grouped-Tied Attention（GTA）和Grouped Latent Attention（GLA）两种注意力变体，优化内存使用和并行计算。

Result: GTA在质量与GQA相当的情况下减少了一半的KV缓存需求；GLA在保持高质量的同时实现了2倍的速度提升。

Conclusion: 新提出的注意力机制显著提升了硬件利用率和解码效率。

Abstract: LLM decoding is bottlenecked for large batches and long contexts by loading
the key-value (KV) cache from high-bandwidth memory, which inflates per-token
latency, while the sequential nature of decoding limits parallelism. We analyze
the interplay among arithmetic intensity, parallelization, and model quality
and question whether current architectures fully exploit modern hardware. This
work redesigns attention to perform more computation per byte loaded from
memory to maximize hardware efficiency without trading off parallel
scalability. We first propose Grouped-Tied Attention (GTA), a simple variant
that combines and reuses key and value states, reducing memory transfers
without compromising model quality. We then introduce Grouped Latent Attention
(GLA), a parallel-friendly latent attention paired with low-level optimizations
for fast decoding while maintaining high model quality. Experiments show that
GTA matches Grouped-Query Attention (GQA) quality while using roughly half the
KV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier
to shard. Our optimized GLA kernel is up to 2$\times$ faster than FlashMLA, for
example, in a speculative decoding setting when the query length exceeds one.
Furthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end
latency and increases throughput in online serving benchmarks by up to
2$\times$.

</details>


### [281] [Reinforcing General Reasoning without Verifiers](https://arxiv.org/abs/2505.21493)
*Xiangxin Zhou,Zichen Liu,Anya Sims,Haonan Wang,Tianyu Pang,Chongxuan Li,Liang Wang,Min Lin,Chao Du*

Key words: 大语言模型，强化学习，验证器，通用推理，VeriFree

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种无需验证器的方法VeriFree，通过强化学习直接最大化生成参考答案的概率，扩展了DeepSeek-R1-Zero式训练至通用推理领域，并优于传统验证器方法。

Motivation: 当前基于验证器的大语言模型训练方法在现实领域（如化学、医疗等）存在限制，如依赖强验证器模型、易受奖励攻击等。VeriFree旨在解决这些问题并扩展训练方法至更广泛领域。

Method: 提出VeriFree方法，绕过答案验证，直接通过强化学习最大化生成参考答案的概率，避免依赖额外验证器模型。

Result: VeriFree在MMLU-Pro等多项评估中匹配甚至超越传统验证器方法，并显著降低计算需求和实践负担。

Conclusion: VeriFree不仅简化了训练流程，还展示了在通用推理领域的潜力，为模型训练提供了新视角。

Abstract: The recent paradigm shift towards training large language models (LLMs) using
DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has
led to impressive advancements in code and mathematical reasoning. However,
this methodology is limited to tasks where rule-based answer verification is
possible and does not naturally extend to real-world domains such as chemistry,
healthcare, engineering, law, biology, business, and economics. Current
practical workarounds use an additional LLM as a model-based verifier; however,
this introduces issues such as reliance on a strong verifier LLM,
susceptibility to reward hacking, and the practical burden of maintaining the
verifier model in memory during training. To address this and extend
DeepSeek-R1-Zero-style training to general reasoning domains, we propose a
verifier-free method (VeriFree) that bypasses answer verification and instead
uses RL to directly maximize the probability of generating the reference
answer. We compare VeriFree with verifier-based methods and demonstrate that,
in addition to its significant practical benefits and reduced compute
requirements, VeriFree matches and even surpasses verifier-based methods on
extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related
benchmarks. Moreover, we provide insights into this method from multiple
perspectives: as an elegant integration of training both the policy and
implicit verifier in a unified model, and as a variational optimization
approach. Code is available at https://github.com/sail-sg/VeriFree.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [282] [Multi-Modal Artificial Intelligence of Embryo Grading and Pregnancy Prediction in Assisted Reproductive Technology: A Review](https://arxiv.org/abs/2505.20306)
*Xueqiang Ouyang,Jia Wei*

Key words: 辅助生殖技术、多模态人工智能、胚胎分级、妊娠预测、数据融合

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文综述了多模态人工智能在胚胎分级和妊娠预测中的应用进展，讨论了当前研究中的主要挑战，如多模态信息融合的复杂性和数据稀缺。

Motivation: 传统体外受精-胚胎移植技术在提高妊娠成功率方面面临挑战，如胚胎分级的主观性和多模态数据整合效率低，因此引入基于人工智能的技术尤为重要。

Method: 回顾了基于静态图像、延时视频和结构化表格数据等多模态数据的人工智能在胚胎分级和妊娠预测中的应用进展。

Result: 多模态人工智能技术展现出了提升辅助生殖技术成功率的潜力。

Conclusion: 尽管多模态人工智能在提高辅助生殖技术成功率方面有潜力，但仍需解决多模态信息融合复杂性和数据稀缺等挑战。

Abstract: As a global disease, infertility has always affected human beings. The
development of assisted reproductive technology can effectively solve this
disease. However, the traditional in vitro fertilization-embryo transfer
technology still faces many challenges in improving the success rate of
pregnancy, such as the subjectivity of embryo grading and the inefficiency of
integrating multi-modal data. Therefore, the introduction of artificial
intelligence-based technologies is particularly crucial. This article reviews
the application progress of multi-modal artificial intelligence in embryo
grading and pregnancy prediction based on different data modalities (including
static images, time-lapse videos and structured table data) from a new
perspective, and discusses the main challenges in current research, such as the
complexity of multi-modal information fusion and data scarcity.

</details>


### [283] [Manalyzer: End-to-end Automated Meta-analysis with Multi-agent System](https://arxiv.org/abs/2505.20310)
*Wanghan Xu,Wenlong Zhang,Fenghua Ling,Ben Fei,Yusong Hu,Fangxuan Ren,Jintai Lin,Wanli Ouyang,Lei Bai*

Key words: 元分析, 多代理系统, 幻觉问题, 自动化, 分层提取

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种多代理系统Manalyzer，通过工具调用实现端到端的自动化元分析，显著减轻了传统方法和LLM方法在文献筛选和数据提取中的幻觉问题。

Motivation: 传统元分析方法需要大量人力和时间，而LLM方法虽能加速部分流程但仍存在幻觉问题，Manalyzer旨在解决这些挑战。

Method: 采用多代理系统（Manalyzer），结合混合审查、分层提取、自证明和反馈检查策略，实现端到端自动化元分析。

Result: 在包含729篇论文、3个领域及10,000多个数据点的新基准上，Manalyzer在多任务元分析中显著优于LLM基线。

Conclusion: Manalyzer通过有效缓解幻觉问题，为自动化元分析提供了高效可靠的解决方案。

Abstract: Meta-analysis is a systematic research methodology that synthesizes data from
multiple existing studies to derive comprehensive conclusions. This approach
not only mitigates limitations inherent in individual studies but also
facilitates novel discoveries through integrated data analysis. Traditional
meta-analysis involves a complex multi-stage pipeline including literature
retrieval, paper screening, and data extraction, which demands substantial
human effort and time. However, while LLM-based methods can accelerate certain
stages, they still face significant challenges, such as hallucinations in paper
screening and data extraction. In this paper, we propose a multi-agent system,
Manalyzer, which achieves end-to-end automated meta-analysis through tool
calls. The hybrid review, hierarchical extraction, self-proving, and feedback
checking strategies implemented in Manalyzer significantly alleviate these two
hallucinations. To comprehensively evaluate the performance of meta-analysis,
we construct a new benchmark comprising 729 papers across 3 domains,
encompassing text, image, and table modalities, with over 10,000 data points.
Extensive experiments demonstrate that Manalyzer achieves significant
performance improvements over the LLM baseline in multi meta-analysis tasks.
Project page: https://black-yt.github.io/meta-analysis-page/ .

</details>


### [284] [Reasoning in Neurosymbolic AI](https://arxiv.org/abs/2505.20313)
*Son Tran,Edjard Mota,Artur d'Avila Garcez*

Key words: 神经符号AI, 命题逻辑, 能量最小化, 受限玻尔兹曼机, 数据效率, 公平性, 安全性

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文介绍了一种基于能量的神经符号AI系统，能够表示并推理任何命题逻辑公式，结合了数据学习和逻辑推理。讨论了神经符号AI在当前以大型语言模型（LLM）为主的AI领域中的定位，及其在数据效率、公平性和安全性方面的挑战。

Motivation: 研究动机在于整合神经网络的推理和学习能力，同时解决数据效率、公平性和安全性等LLM面临的挑战。

Method: 通过基于能量的系统（如受限玻尔兹曼机）表示逻辑，并在数据与知识的学习中进行实证评估，与其他符号、神经及神经符号系统进行比较。

Result: 结果显示该方法能够有效结合逻辑推理和能量最小化，并可能促进神经网络在大规模并行逻辑推理中的应用。

Conclusion: 神经符号AI需置于更广泛的正式推理和AI责任框架中，以解决深度学习可靠性问题。

Abstract: Knowledge representation and reasoning in neural networks have been a
long-standing endeavor which has attracted much attention recently. The
principled integration of reasoning and learning in neural networks is a main
objective of the area of neurosymbolic Artificial Intelligence (AI). In this
chapter, a simple energy-based neurosymbolic AI system is described that can
represent and reason formally about any propositional logic formula. This
creates a powerful combination of learning from data and knowledge and logical
reasoning. We start by positioning neurosymbolic AI in the context of the
current AI landscape that is unsurprisingly dominated by Large Language Models
(LLMs). We identify important challenges of data efficiency, fairness and
safety of LLMs that might be addressed by neurosymbolic reasoning systems with
formal reasoning capabilities. We then discuss the representation of logic by
the specific energy-based system, including illustrative examples and empirical
evaluation of the correspondence between logical reasoning and energy
minimization using Restricted Boltzmann Machines (RBM). Learning from data and
knowledge is also evaluated empirically and compared with a symbolic, neural
and a neurosymbolic system. Results reported in this chapter in an accessible
way are expected to reignite the research on the use of neural networks as
massively-parallel models for logical reasoning and promote the principled
integration of reasoning and learning in deep networks. We conclude the chapter
with a discussion of the importance of positioning neurosymbolic AI within a
broader framework of formal reasoning and accountability in AI, discussing the
challenges for neurosynbolic AI to tackle the various known problems of
reliability of deep learning.

</details>


### [285] [Reinforcement Speculative Decoding for Fast Ranking](https://arxiv.org/abs/2505.20316)
*Yingpeng Du,Tianjun Wei,Zhu Sun,Jie Zhang*

Key words: 大语言模型，排序系统，推测解码，强化学习，信息检索，推荐系统

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 摘要介绍了一种用于大型语言模型（LLM）快速排序推理的强化推测解码方法，通过上下行解码范式和强化学习优化策略，解决了传统推测解码方法在排序系统中的延迟和知识丢弃问题，实验证明了其有效性。

Motivation: 尽管现有推测解码（SD）方法可以缓解自回归解码的延迟问题，但在排序系统中面临左到右解码范式的挑战，包括严格的延迟约束和对未接受项知识丢弃的不足。

Method: 提出基于强化学习的推测解码方法，通过上下行解码范式和排名定制策略优化，迭代修改排序序列，充分利用多轮验证的列表排序知识。

Result: 实验在信息检索（IR）和推荐系统（RS）任务中验证了该方法的有效性，展示了其在延迟和性能上的优势。

Conclusion: 所提出的方法通过强化学习和多轮知识利用，显著提升了LLM在排序系统中的推理效率和性能。

Abstract: Large Language Models (LLMs) have been widely adopted in ranking systems such
as information retrieval (IR) systems and recommender systems (RSs). To
alleviate the latency of auto-regressive decoding, some studies explore the
single (first) token decoding for ranking approximation, but they suffer from
severe degradation in tail positions. Although speculative decoding (SD)
methods can be a remedy with verification at different positions, they face
challenges in ranking systems due to their left-to-right decoding paradigm.
Firstly, ranking systems require strict latency constraints, but verification
rounds in SD methods remain agnostic; Secondly, SD methods usually discard
listwise ranking knowledge about unaccepted items in previous rounds, hindering
future multi-token prediction, especially when candidate tokens are the
unaccepted items. In this paper, we propose a Reinforcement Speculative
Decoding method for fast ranking inference of LLMs. To meet the ranking
systems' latency requirement, we propose an up-to-down decoding paradigm that
employs an agent to iteratively modify the ranking sequence under a constrained
budget. Specifically, we design a ranking-tailored policy optimization,
actively exploring optimal multi-round ranking modification policy verified by
LLMs via reinforcement learning (RL). To better approximate the target LLM
under the constrained budget, we trigger the agent fully utilizing the listwise
ranking knowledge about all items verified by LLMs across different rounds in
RL, enhancing the modification policy of the agent. More importantly, we
demonstrate the theoretical robustness and advantages of our paradigm and
implementation. Experiments on both IR and RS tasks show the effectiveness of
our proposed method.

</details>


### [286] [Challenges for artificial cognitive systems](https://arxiv.org/abs/2505.20339)
*Antoni Gomila,Vincent C. Müller*

Key words: 认知系统,挑战,学习,灵活运用知识,EUCogII

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文旨在为人工智能认知系统制定明确的挑战，以定义进步标准。

Motivation: 填补认知系统研究中的空白，明确目标和进步标准。

Method: 基于EUCogII项目的定义，提出认知系统的挑战，强调学习与灵活运用知识。

Result: 明确认知系统的定义及挑战，为研究提供指导。

Conclusion: 挑战的提出有助于推动认知系统研究的进步。

Abstract: The declared goal of this paper is to fill this gap: "... cognitive systems
research needs questions or challenges that define progress. The challenges are
not (yet more) predictions of the future, but a guideline to what are the aims
and what would constitute progress." -- the quotation being from the project
description of EUCogII, the project for the European Network for Cognitive
Systems within which this formulation of the 'challenges' was originally
developed (http://www.eucognition.org). So, we stick out our neck and formulate
the challenges for artificial cognitive systems. These challenges are
articulated in terms of a definition of what a cognitive system is: a system
that learns from experience and uses its acquired knowledge (both declarative
and practical) in a flexible manner to achieve its own goals.

</details>


### [287] [Machine Theory of Mind and the Structure of Human Values](https://arxiv.org/abs/2505.20342)
*Paul de Font-Reaulx*

Key words: 价值观学习、贝叶斯心智理论、价值观泛化、AI伦理、生成性推理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 人类价值观具有生成性理性结构，可通过贝叶斯心智理论模型从行为和其他价值观中推断，解决价值观泛化问题。

Motivation: 确保AI安全与伦理需要学习人类价值观，但人类行为无法完全体现其关心的复杂价值观，因此需从有限样本中预测其余部分。

Method: 提出利用贝叶斯心智理论模型，不仅从行为中推断价值观，还可通过已掌握的价值观进行生成性推理。

Result: 表明人类价值观具有生成性理性结构，支持价值观间的推理，为可扩展的机器心智理论提供关键方法。

Conclusion: 开发生成性价值观间推理是解决AI价值观泛化问题的核心，需摒弃简单效用函数的局限性。

Abstract: Value learning is a crucial aspect of safe and ethical AI. This is primarily
pursued by methods inferring human values from behaviour. However, humans care
about much more than we are able to demonstrate through our actions.
Consequently, an AI must predict the rest of our seemingly complex values from
a limited sample. I call this the value generalization problem. In this paper,
I argue that human values have a generative rational structure and that this
allows us to solve the value generalization problem. In particular, we can use
Bayesian Theory of Mind models to infer human values not only from behaviour,
but also from other values. This has been obscured by the widespread use of
simple utility functions to represent human values. I conclude that developing
generative value-to-value inference is a crucial component of achieving a
scalable machine theory of mind.

</details>


### [288] [SCAR: Shapley Credit Assignment for More Efficient RLHF](https://arxiv.org/abs/2505.20417)
*Meng Cao,Shuyuan Zhang,Xiao-Wen Chang,Doina Precup*

Key words: Reinforcement Learning, Human Feedback, Shapley Values, Credit Assignment, Language Models

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: SCAR is a new method for dense reward signals in RLHF, using Shapley values to fairly assign rewards to tokens, improving convergence and final scores.

Motivation: RLHF struggles with sparse rewards and unclear credit assignment for tokens, which SCAR addresses.

Method: SCAR uses Shapley values from game theory to distribute sequence-level rewards to tokens based on their contributions.

Result: SCAR outperforms standard RLHF and baselines in convergence speed and final scores across tasks like sentiment control and summarization.

Conclusion: SCAR provides a more effective and theoretically sound credit assignment method in RLHF for LLM alignment.

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a widely used technique
for aligning Large Language Models (LLMs) with human preferences, yet it often
suffers from sparse reward signals, making effective credit assignment
challenging. In typical setups, the reward model provides a single scalar score
for an entire generated sequence, offering little insight into which token or
span-level decisions were responsible for the outcome. To address this, we
propose Shapley Credit Assignment Rewards (SCAR), a novel method that leverages
Shapley values in cooperative game theory. SCAR distributes the total
sequence-level reward among constituent tokens or text spans based on their
principled marginal contributions. This creates dense reward signals,
crucially, without necessitating the training of auxiliary critique models or
recourse to fine-grained human annotations at intermediate generation stages.
Unlike prior dense reward methods, SCAR offers a game-theoretic foundation for
fair credit attribution. Theoretically, we demonstrate that SCAR preserves the
original optimal policy, and empirically, across diverse tasks including
sentiment control, text summarization, and instruction tuning, we show that
SCAR converges significantly faster and achieves higher final reward scores
compared to standard RLHF and attention-based dense reward baselines. Our
findings suggest that SCAR provides a more effective and theoretically sound
method for credit assignment in RLHF, leading to more efficient alignment of
LLMs.

</details>


### [289] [Reconceptualizing Smart Microscopy: From Data Collection to Knowledge Creation by Multi-Agent Integration](https://arxiv.org/abs/2505.20466)
*P. S. Kesavan,Pontus Nordenfelt*

Key words: 智能显微镜, 科学研究, 经验-认知鸿沟, 设计原则, 多智能体架构

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一个理论框架，将智能显微镜重新定义为科学研究的伙伴，通过六个核心设计原则填补可观察与需理解之间的鸿沟。

Motivation: 解决细胞研究中可观察（经验领域）与需理解（认知领域）之间的鸿沟，推动显微镜从被动工具转向主动科学合作者。

Method: 提出六个核心设计原则，构建多智能体架构以协调经验观察与科学理解目标。

Result: 框架为构建支持假设生成、洞察发现和理论发展的显微镜系统提供了路线图。

Conclusion: 智能显微镜将成为知识创造过程中的主动合作伙伴，重新定义科学仪器的角色。

Abstract: Smart microscopy represents a paradigm shift in biological imaging, moving
from passive observation tools to active collaborators in scientific inquiry.
Enabled by advances in automation, computational power, and artificial
intelligence, these systems are now capable of adaptive decision-making and
real-time experimental control. Here, we introduce a theoretical framework that
reconceptualizes smart microscopy as a partner in scientific investigation.
Central to our framework is the concept of the 'epistemic-empirical divide' in
cellular investigation-the gap between what is observable (empirical domain)
and what must be understood (epistemic domain). We propose six core design
principles: epistemic-empirical awareness, hierarchical context integration, an
evolution from detection to perception, adaptive measurement frameworks,
narrative synthesis capabilities, and cross-contextual reasoning. Together,
these principles guide a multi-agent architecture designed to align empirical
observation with the goals of scientific understanding. Our framework provides
a roadmap for building microscopy systems that go beyond automation to actively
support hypothesis generation, insight discovery, and theory development,
redefining the role of scientific instruments in the process of knowledge
creation.

</details>


### [290] [Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting](https://arxiv.org/abs/2505.20521)
*Ana Rita Ortigoso,Gabriel Vieira,Daniel Fuentes,Luis Frazão,Nuno Costa,António Pereira*

Key words: 多模态 AI, 情绪模拟, 多模型对话, 检索增强生成, 用户测试

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: Project Riley 是一种多模态、多模型的对话 AI 架构，通过模拟情绪影响下的推理，利用五个情感代理进行多轮对话生成和优化，最终通过推理机制整合输出。其原型在用户测试中表现优异，尤其在情感一致性和清晰度方面。

Motivation: 受《头脑特工队》启发，探索情绪如何影响 AI 的推理和对话生成，旨在提升 AI 的情感表达与实用性。

Method: 采用多模态 LLM 和五个情感代理（Joy、Sadness、Fear、Anger、Disgust），通过多轮对话和自优化机制生成响应，并部署离线原型进行测试。

Result: 用户测试显示，在情感一致性、沟通清晰度和自然度方面表现良好，尤其在结构化场景中效果显著。

Conclusion: Project Riley 证明了情绪模拟在 AI 对话中的有效性，其推理整合机制为多模态 AI 提供了新方向。

Abstract: This paper presents Project Riley, a novel multimodal and multi-model
conversational AI architecture oriented towards the simulation of reasoning
influenced by emotional states. Drawing inspiration from Pixar's Inside Out,
the system comprises five distinct emotional agents - Joy, Sadness, Fear,
Anger, and Disgust - that engage in structured multi-round dialogues to
generate, criticise, and iteratively refine responses. A final reasoning
mechanism synthesises the contributions of these agents into a coherent output
that either reflects the dominant emotion or integrates multiple perspectives.
The architecture incorporates both textual and visual large language models
(LLMs), alongside advanced reasoning and self-refinement processes. A
functional prototype was deployed locally in an offline environment, optimised
for emotional expressiveness and computational efficiency. From this initial
prototype, another one emerged, called Armando, which was developed for use in
emergency contexts, delivering emotionally calibrated and factually accurate
information through the integration of Retrieval-Augmented Generation (RAG) and
cumulative context tracking. The Project Riley prototype was evaluated through
user testing, in which participants interacted with the chatbot and completed a
structured questionnaire assessing three dimensions: Emotional Appropriateness,
Clarity and Utility, and Naturalness and Human-likeness. The results indicate
strong performance in structured scenarios, particularly with respect to
emotional alignment and communicative clarity.

</details>


### [291] [Scaling over Scaling: Exploring Test-Time Scaling Pareto in Large Reasoning Models](https://arxiv.org/abs/2505.20522)
*Jian Wang,Boyan Zhu,Chak Tou Leong,Yongqi Li,Wenjie Li*

Key words: 大型推理模型,测试时扩展,资源分配,性能模型,并行扩展,顺序扩展

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文研究了大型推理模型（LRMs）在测试时计算扩展中的性能提升与资源分配优化问题，提出了测试时扩展性能模型（TTSPM），并分析了并行扩展与顺序扩展两种范式的数学上限及饱和点，实证验证了其在实际推理任务中的有效性。

Motivation: 研究动机是为了探索测试时计算扩展的潜力及其资源分配优化，解决扩展过程中的性能饱和问题。

Method: 方法包括理论分析并行扩展与顺序扩展两种范式，建立TTSPM模型，推导出扩展预算的饱和点，并在多个推理基准上实证验证。

Result: 结果表明两种扩展范式在饱和点后均出现收益递减现象，且其上限具有统一的数学结构。

Conclusion: 结论是通过明确扩展阈值为资源分配提供了理论指导，有助于开发更高效的大型推理模型。

Abstract: Large reasoning models (LRMs) have exhibited the capacity of enhancing
reasoning performance via internal test-time scaling. Building upon this, a
promising direction is to further scale test-time compute to unlock even
greater reasoning capabilities. However, as we push these scaling boundaries,
systematically understanding the practical limits and achieving optimal
resource allocation becomes a critical challenge. In this paper, we investigate
the scaling Pareto of test-time scaling and introduce the Test-Time Scaling
Performance Model (TTSPM). We theoretically analyze two fundamental paradigms
for such extended scaling, parallel scaling and sequential scaling, from a
probabilistic modeling perspective. Our primary contribution is the derivation
of the saturation point on the scaling budget for both strategies, identifying
thresholds beyond which additional computation yields diminishing returns.
Remarkably, despite their distinct mechanisms, both paradigms converge to a
unified mathematical structure in their upper bounds. We empirically validate
our theoretical findings on challenging reasoning benchmarks, including AIME,
MATH-500, and GPQA, demonstrating the practical utility of these bounds for
test-time resource allocation. We hope that this work provides insights into
the cost-benefit trade-offs of test-time scaling, guiding the development of
more resource-efficient inference strategies for large reasoning models.

</details>


### [292] [Comparisons between a Large Language Model-based Real-Time Compound Diagnostic Medical AI Interface and Physicians for Common Internal Medicine Cases using Simulated Patients](https://arxiv.org/abs/2505.20609)
*Hyungjun Park,Chang-Yun Woo,Seungjo Lim,Seunghwan Lim,Keunho Kwak,Ju Young Jeong,Chong Hyun Suh*

Key words: LLM, 医疗AI, 诊断准确性, 临床实验, 成本效益

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文开发了一种基于LLM的实时复合诊断医疗AI接口，并通过临床试验比较其与医生在常见内科病例中的表现。结果显示AI在诊断准确性、时间和成本上均优于医生，患者满意度相近。

Motivation: 开发一种高效、低成本的AI诊断工具，以辅助初级医疗咨询，尤其在常见内科病例中，缓解医疗资源压力。

Method: 采用非随机临床试验，基于USMLE Step 2 CS考试风格设计病例，比较医生与AI诊断准确性、时间和成本。

Result: AI首次诊断准确率80%（医生50-70%），二次诊断准确率100%（医生70-90%），时间减少44.6%，成本降低98.1%，患者满意度与医生相近。

Conclusion: 基于LLM的AI诊断接口在准确性、时间和成本上表现优异，有望辅助初级医疗咨询。

Abstract: Objective To develop an LLM based realtime compound diagnostic medical AI
interface and performed a clinical trial comparing this interface and
physicians for common internal medicine cases based on the United States
Medical License Exam (USMLE) Step 2 Clinical Skill (CS) style exams. Methods A
nonrandomized clinical trial was conducted on August 20, 2024. We recruited one
general physician, two internal medicine residents (2nd and 3rd year), and five
simulated patients. The clinical vignettes were adapted from the USMLE Step 2
CS style exams. We developed 10 representative internal medicine cases based on
actual patients and included information available on initial diagnostic
evaluation. Primary outcome was the accuracy of the first differential
diagnosis. Repeatability was evaluated based on the proportion of agreement.
Results The accuracy of the physicians' first differential diagnosis ranged
from 50% to 70%, whereas the realtime compound diagnostic medical AI interface
achieved an accuracy of 80%. The proportion of agreement for the first
differential diagnosis was 0.7. The accuracy of the first and second
differential diagnoses ranged from 70% to 90% for physicians, whereas the AI
interface achieved an accuracy rate of 100%. The average time for the AI
interface (557 sec) was 44.6% shorter than that of the physicians (1006 sec).
The AI interface ($0.08) also reduced costs by 98.1% compared to the
physicians' average ($4.2). Patient satisfaction scores ranged from 4.2 to 4.3
for care by physicians and were 3.9 for the AI interface Conclusion An LLM
based realtime compound diagnostic medical AI interface demonstrated diagnostic
accuracy and patient satisfaction comparable to those of a physician, while
requiring less time and lower costs. These findings suggest that AI interfaces
may have the potential to assist primary care consultations for common internal
medicine cases.

</details>


### [293] [CoderAgent: Simulating Student Behavior for Personalized Programming Learning with Large Language Models](https://arxiv.org/abs/2505.20642)
*Yi Zhan,Qi Liu,Weibo Gao,Zheng Zhang,Tianfu Wang,Shuanghong Shen,Junyu Lu,Zhenya Huang*

Key words: personalized programming education, CoderAgent, ACT-R, PTOT

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出基于LLM的CoderAgent，通过模拟学生编程过程解决个性化编程教育中的数据不足与评估不匹配问题。

Motivation: 个性化编程教育如练习推荐能提升学习效率，但数据不足与离线评估不匹配阻碍其实际应用。

Method: 设计基于ACT-R认知架构的CoderAgent，引入Programming Tree of Thought (PTOT)分步骤分析编程过程。

Result: 实验证明CoderAgent能提供可解释的学习轨迹分析并实现精准模拟。

Conclusion: CoderAgent为个性化编程教育提供了可行的方法。

Abstract: Personalized programming tutoring, such as exercise recommendation, can
enhance learners' efficiency, motivation, and outcomes, which is increasingly
important in modern digital education. However, the lack of sufficient and
high-quality programming data, combined with the mismatch between offline
evaluation and real-world learning, hinders the practical deployment of such
systems. To address this challenge, many approaches attempt to simulate learner
practice data, yet they often overlook the fine-grained, iterative nature of
programming learning, resulting in a lack of interpretability and granularity.
To fill this gap, we propose a LLM-based agent, CoderAgent, to simulate
students' programming processes in a fine-grained manner without relying on
real data. Specifically, we equip each human learner with an intelligent agent,
the core of which lies in capturing the cognitive states of the human
programming practice process. Inspired by ACT-R, a cognitive architecture
framework, we design the structure of CoderAgent to align with human cognitive
architecture by focusing on the mastery of programming knowledge and the
application of coding ability. Recognizing the inherent patterns in
multi-layered cognitive reasoning, we introduce the Programming Tree of Thought
(PTOT), which breaks down the process into four steps: why, how, where, and
what. This approach enables a detailed analysis of iterative problem-solving
strategies. Finally, experimental evaluations on real-world datasets
demonstrate that CoderAgent provides interpretable insights into learning
trajectories and achieves accurate simulations, paving the way for personalized
programming education.

</details>


### [294] [AutoReproduce: Automatic AI Experiment Reproduction with Paper Lineage](https://arxiv.org/abs/2505.20662)
*Xuanle Zhao,Zilin Sang,Yuxuan Li,Qi Shi,Shuo Wang,Duzhen Zhang,Xu Han,Zhiyuan Liu,Maosong Sun*

Key words: 实验复现, 多智能体, 隐含知识, AutoReproduce, 单元测试

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了AutoReproduce框架，通过多智能体系统自动复现论文实验，并结合单元测试提升代码可执行性。实验表明其性能优于现有基线方法。

Motivation: AI领域实验复现对加速研究进展至关重要，但方法设计和训练过程的复杂性以及隐含知识的缺失导致自动化复现困难。

Method: 提出paper lineage算法提取引用文献中的隐含知识，并开发AutoReproduce多智能体框架，支持端到端实验复现和单元测试生成。

Result: 在ReproduceBench基准测试中，AutoReproduce的复现和执行保真度均显著优于基线方法，最高提升70%，且89.74%的可执行实验运行性能差距平均为22.1%。

Conclusion: AutoReproduce通过自动化实验复现和单元测试有效解决了隐含知识依赖问题，为AI研究提供了高效工具。

Abstract: Efficient experiment reproduction is critical to accelerating progress in
artificial intelligence. However, the inherent complexity of method design and
training procedures presents substantial challenges for automation. Notably,
reproducing experiments often requires implicit domain-specific knowledge not
explicitly documented in the original papers. To address this, we introduce the
paper lineage algorithm, which identifies and extracts implicit knowledge from
the relevant references cited by the target paper. Building on this idea, we
propose AutoReproduce, a multi-agent framework capable of automatically
reproducing experiments described in research papers in an end-to-end manner.
AutoReproduce enhances code executability by generating unit tests alongside
the reproduction process. To evaluate the reproduction capability, we construct
ReproduceBench, a benchmark annotated with verified implementations, and
introduce novel evaluation metrics to assess both the reproduction and
execution fidelity. Experimental results demonstrate that AutoReproduce
outperforms the existing strong agent baselines on all five evaluation metrics
by a peak margin of over $70\%$. In particular, compared to the official
implementations, AutoReproduce achieves an average performance gap of $22.1\%$
on $89.74\%$ of the executable experiment runs. The code will be available at
https://github.com/AI9Stars/AutoReproduce.

</details>


### [295] [MIRROR: Multi-agent Intra- and Inter-Reflection for Optimized Reasoning in Tool Learning](https://arxiv.org/abs/2505.20670)
*Zikang Guo,Benfeng Xu,Xiaorui Wang,Zhendong Mao*

Key words: 大语言模型, 多代理工作流, 反思机制, MIRROR框架, 工具集成

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了MIRROR框架，通过预执行反思（intra-reflection）和事后反思（inter-reflection）提升大语言模型在工具集成任务中的表现，实验证明其优于现有方法。

Motivation: 现有方法仅在动作执行后反思，但像人类一样，预执行反思可以避免错误传播，因此提出了结合预执行和事后反思的框架。

Method: 提出MIRROR框架，包含预执行反思（评估决策）和事后反思（调整轨迹），系统性利用LLM反思能力。

Result: 在StableToolBench和TravelPlanner基准测试中表现优异，达到最先进的水平。

Conclusion: MIRROR通过结合两种反思机制，显著提升了LLM在复杂任务中的表现。

Abstract: Complex tasks involving tool integration pose significant challenges for
Large Language Models (LLMs), leading to the emergence of multi-agent workflows
as a promising solution. Reflection has emerged as an effective strategy for
correcting erroneous trajectories in agentic workflows. However, existing
approaches only exploit such capability in the post-action stage, where the
agent observes the execution outcomes. We argue that, like humans, LLMs can
also engage in reflection before action execution: the agent can anticipate
undesirable outcomes from its own decisions, which not only provides a
necessarily complementary perspective to evaluate the decision but also
prevents the propagation of errors throughout the trajectory. In this paper, we
propose MIRROR, a framework that consists of both intra-reflection, which
critically assesses intended actions before execution, and inter-reflection,
which further adjusts the trajectory based on observations. This design
systematically leverages LLM reflection capabilities to eliminate and rectify
erroneous actions on a more comprehensive scope. Evaluations on both the
StableToolBench and TravelPlanner benchmarks demonstrate MIRROR's superior
performance, achieving state-of-the-art results compared to existing
approaches.

</details>


### [296] [LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation](https://arxiv.org/abs/2505.20671)
*Heng Tan,Hua Yan,Yu Yang*

Key words: 强化学习, 大型语言模型, 策略优化, 关键状态, 隐式奖励

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种利用大型语言模型（LLM）引导的策略调制框架，无需额外模型训练或人工干预即可改进强化学习（RL）训练。通过LLM识别关键状态并提供动作建议及隐式奖励，实验表明其性能优于现有方法。

Motivation: 强化学习在复杂任务中难以训练有效策略，现有方法如自动策略优化和人工干预存在成本高或扩展性差的问题。本文旨在利用LLM克服这些瓶颈。

Method: 设计了一个LLM引导的策略调制框架，包括：（1）用LLM从次优代理轨迹中识别关键状态；（2）基于这些状态提供动作建议和隐式奖励以指导策略优化。

Result: 在标准RL基准测试中，该方法优于现有技术，验证了LLM在解决RL训练瓶颈中的有效性。

Conclusion: LLM的引入为RL训练提供了一种高效且可扩展的解决方案，避免了传统方法的局限性。

Abstract: While reinforcement learning (RL) has achieved notable success in various
domains, training effective policies for complex tasks remains challenging.
Agents often converge to local optima and fail to maximize long-term rewards.
Existing approaches to mitigate training bottlenecks typically fall into two
categories: (i) Automated policy refinement, which identifies critical states
from past trajectories to guide policy updates, but suffers from costly and
uncertain model training; and (ii) Human-in-the-loop refinement, where human
feedback is used to correct agent behavior, but this does not scale well to
environments with large or continuous action spaces. In this work, we design a
large language model-guided policy modulation framework that leverages LLMs to
improve RL training without additional model training or human intervention. We
first prompt an LLM to identify critical states from a sub-optimal agent's
trajectories. Based on these states, the LLM then provides action suggestions
and assigns implicit rewards to guide policy refinement. Experiments across
standard RL benchmarks demonstrate that our method outperforms state-of-the-art
baselines, highlighting the effectiveness of LLM-based explanations in
addressing RL training bottlenecks.

</details>


### [297] [GIFARC: Synthetic Dataset for Leveraging Human-Intuitive Analogies to Elevate AI Reasoning](https://arxiv.org/abs/2505.20672)
*Woochang Sim,Hyunseok Ryu,Kyungmin Choi,Sungwon Han,Sundong Kim*

Key words: 抽象推理、类比启发、大型语言模型、视觉语言模型、GIFARC

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一个名为GIFARC的新型数据集，通过类比启发的方法合成ARC风格的任务，利用大型语言模型和视觉语言模型，旨在提升AI在抽象推理任务中的表现，使其更接近人类推理水平。

Motivation: 现有的AI在抽象推理任务（如ARC）上的表现与人类水平仍有显著差距，作者希望通过引入类比启发的方法来弥补这一差距，提升AI的抽象推理能力。

Method: 利用大型语言模型（LLMs）和视觉语言模型（VLMs），从包含类比的GIF图像中合成新的ARC风格任务，每项任务均配有真实类比，明确视觉转换与日常概念的映射关系。

Result: 实验验证表明，GIFARC通过类比方法引导LLMs，使其任务解决方式更符合人类的类比推理方式。

Conclusion: GIFARC能够有效减少问题复杂性，生成更简洁且易于人类理解的解决方案，为提升AI的抽象推理能力提供了新方向。

Abstract: The Abstraction and Reasoning Corpus (ARC) poses a stringent test of general
AI capabilities, requiring solvers to infer abstract patterns from only a
handful of examples. Despite substantial progress in deep learning,
state-of-the-art models still achieve accuracy rates of merely 40-55% on 2024
ARC Competition, indicative of a significant gap between their performance and
human-level reasoning. In this work, we seek to bridge that gap by introducing
an analogy-inspired ARC dataset, GIFARC. Leveraging large language models
(LLMs) and vision-language models (VLMs), we synthesize new ARC-style tasks
from a variety of GIF images that include analogies. Each new task is paired
with ground-truth analogy, providing an explicit mapping between visual
transformations and everyday concepts. By embedding robust human-intuitive
analogies into ARC-style tasks, GIFARC guides AI agents to evaluate the task
analogically before engaging in brute-force pattern search, thus efficiently
reducing problem complexity and build a more concise and human-understandable
solution. We empirically validate that guiding LLM with analogic approach with
GIFARC affects task-solving approaches of LLMs to align with analogic approach
of human.

</details>


### [298] [Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models](https://arxiv.org/abs/2505.20728)
*Zesen Lyu,Dandan Zhang,Wei Ye,Fangdi Li,Zhihang Jiang,Yao Yang*

Key words: 空间推理, 视觉语言模型, 基准测试, Jigsaw-Puzzles, 认知能力

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一个名为Jigsaw-Puzzles的新基准，用于评估视觉语言模型的空间推理能力，发现当前最强模型的表现仍远低于人类水平。

Motivation: 研究目标是验证当前视觉语言模型（VLMs）是否具备类似于人类的空间推理能力，并为其提供一个诊断性基准。

Method: 设计了包含1,100张复杂空间图像的Jigsaw-Puzzles数据集，并开发了五个任务来评估模型的空间感知、结构理解和推理能力。

Result: 在24种最先进的VLMs中，表现最强的Gemini-2.5-Pro总体准确率仅为77.14%，在Order Generation任务中仅30.00%，远低于人类的90%以上表现。

Conclusion: 当前VLMs的空间推理能力与人类仍存在显著差距，Jigsaw-Puzzles可作为推动该领域研究的挑战性基准。

Abstract: Spatial reasoning is a core component of human cognition, enabling
individuals to perceive, comprehend, and interact with the physical world. It
relies on a nuanced understanding of spatial structures and inter-object
relationships, serving as the foundation for complex reasoning and
decision-making. To investigate whether current vision-language models (VLMs)
exhibit similar capability, we introduce Jigsaw-Puzzles, a novel benchmark
consisting of 1,100 carefully curated real-world images with high spatial
complexity. Based on this dataset, we design five tasks to rigorously evaluate
VLMs' spatial perception, structural understanding, and reasoning capabilities,
while deliberately minimizing reliance on domain-specific knowledge to better
isolate and assess the general spatial reasoning capability. We conduct a
comprehensive evaluation across 24 state-of-the-art VLMs. The results show that
even the strongest model, Gemini-2.5-Pro, achieves only 77.14% overall accuracy
and performs particularly poorly on the Order Generation task, with only 30.00%
accuracy, far below the performance exceeding 90% achieved by human
participants. This persistent gap underscores the need for continued progress,
positioning Jigsaw-Puzzles as a challenging and diagnostic benchmark for
advancing spatial reasoning research in VLMs.

</details>


### [299] [E2E Process Automation Leveraging Generative AI and IDP-Based Automation Agent: A Case Study on Corporate Expense Processing](https://arxiv.org/abs/2505.20733)
*Cheonsu Jeong,Seongmin Sim,Hyoyoung Cho,Sungsu Kim,Byounggwan Shin*

Key words: 智能工作自动化, 生成式AI, 智能文档处理, 自动化代理, 全流程自动化

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出一种集成生成式AI和智能文档处理技术的智能工作自动化方法，用于企业财务费用处理任务的全流程自动化。

Motivation: 传统RPA在处理非结构化数据、异常管理和复杂决策时存在局限，需要更智能的解决方案。

Method: 设计并实现四阶段集成流程：OCR/IDP自动识别票据、政策驱动的项目分类、生成式AI支持的智能异常处理、人机协作最终决策与系统持续学习。

Result: 应用于韩国企业S，系统实现了纸质票据处理时间减少80%、错误率下降、合规性提升等定量和定性效益。

Conclusion: 生成式AI、IDP与自动化代理的有机整合能有效突破传统自动化局限，实现复杂企业流程的全流程自动化。

Abstract: This paper presents an intelligent work automation approach in the context of
contemporary digital transformation by integrating generative AI and
Intelligent Document Processing (IDP) technologies with an Automation Agent to
realize End-to-End (E2E) automation of corporate financial expense processing
tasks. While traditional Robotic Process Automation (RPA) has proven effective
for repetitive, rule-based simple task automation, it faces limitations in
handling unstructured data, exception management, and complex decision-making.
This study designs and implements a four-stage integrated process comprising
automatic recognition of supporting documents such as receipts via OCR/IDP,
item classification based on a policy-driven database, intelligent exception
handling supported by generative AI (large language models, LLMs), and
human-in-the-loop final decision-making with continuous system learning through
an Automation Agent. Applied to a major Korean enterprise (Company S), the
system demonstrated quantitative benefits including over 80% reduction in
processing time for paper receipt expense tasks, decreased error rates, and
improved compliance, as well as qualitative benefits such as enhanced accuracy
and consistency, increased employee satisfaction, and data-driven decision
support. Furthermore, the system embodies a virtuous cycle by learning from
human judgments to progressively improve automatic exception handling
capabilities. Empirically, this research confirms that the organic integration
of generative AI, IDP, and Automation Agents effectively overcomes the
limitations of conventional automation and enables E2E automation of complex
corporate processes. The study also discusses potential extensions to other
domains such as accounting, human resources, and procurement, and proposes
future directions for AI-driven hyper-automation development.

</details>


### [300] [RRO: LLM Agent Optimization Through Rising Reward Trajectories](https://arxiv.org/abs/2505.20737)
*Zilong Wang,Jingfeng Yang,Sreyashi Nag,Samarth Varshney,Xianfeng Tang,Haoming Jiang,Jingbo Shang,Sheikh Muhammad Sarwar*

Key words: LLMs, 强化学习, 过程监督, RRO, 奖励优化

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出奖励上升优化（RRO），通过动态扩展搜索空间和确保奖励趋势上升，高效解决强化学习训练过程中高成本问题。

Motivation: 解决过程监督强化学习中高计算成本的问题，改进传统PRMs方法的扩展性。

Method: 提出RRO方法，动态识别奖励上升步骤并扩展候选动作空间，减少数据探索成本。

Result: 在WebShop和InterCode-SQL基准测试上表现优异，且探索成本大幅降低。

Conclusion: RRO方法在减少计算成本的同时提升了性能，为复杂任务的LLM代理提供了有效解决方案。

Abstract: Large language models (LLMs) have exhibited extraordinary performance in a
variety of tasks while it remains challenging for them to solve complex
multi-step tasks as agents. In practice, agents sensitive to the outcome of
certain key steps which makes them likely to fail the task because of a subtle
mistake in the planning trajectory. Recent approaches resort to calibrating the
reasoning process through reinforcement learning. They reward or penalize every
reasoning step with process supervision, as known as Process Reward Models
(PRMs). However, PRMs are difficult and costly to scale up with a large number
of next action candidates since they require extensive computations to acquire
the training data through the per-step trajectory exploration. To mitigate this
issue, we focus on the relative reward trend across successive reasoning steps
and propose maintaining an increasing reward in the collected trajectories for
process supervision, which we term Reward Rising Optimization (RRO).
Specifically, we incrementally augment the process supervision until
identifying a step exhibiting positive reward differentials, i.e. rising
rewards, relative to its preceding iteration. This method dynamically expands
the search space for the next action candidates, efficiently capturing
high-quality data. We provide mathematical groundings and empirical results on
the WebShop and InterCode-SQL benchmarks, showing that our proposed RRO
achieves superior performance while requiring much less exploration cost.

</details>


### [301] [MSEarth: A Benchmark for Multimodal Scientific Comprehension of Earth Science](https://arxiv.org/abs/2505.20740)
*Xiangyu Zhao,Wanghan Xu,Bo Liu,Yuhao Zhou,Fenghua Ling,Ben Fei,Xiaoyu Yue,Lei Bai,Wenlong Zhang,Xiao-Ming Wu*

Key words: 多模态大语言模型, 地球科学, 科学基准, MSEarth, 复杂推理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文介绍了MSEarth，一个多模态科学基准，旨在解决地球科学领域缺乏高质量、复杂推理的基准问题。

Motivation: 当前多模态大语言模型在地球科学中的应用研究不足，且现有基准无法反映真实科学推理的复杂性和领域特异性。

Method: 从高质量开放获取的科学出版物中收集数据，构建包含7K多张图表及其丰富注释的MSEarth基准。

Result: MSEarth覆盖地球科学五大领域，支持多种任务（如图表注释、多项选择题、开放式推理），填补了研究生级基准的空白。

Conclusion: MSEarth为科学推理领域提供了可扩展且高保真的资源，促进多模态大语言模型的开发和评估。

Abstract: The rapid advancement of multimodal large language models (MLLMs) has
unlocked new opportunities to tackle complex scientific challenges. Despite
this progress, their application in addressing earth science problems,
especially at the graduate level, remains underexplored. A significant barrier
is the absence of benchmarks that capture the depth and contextual complexity
of geoscientific reasoning. Current benchmarks often rely on synthetic datasets
or simplistic figure-caption pairs, which do not adequately reflect the
intricate reasoning and domain-specific insights required for real-world
scientific applications. To address these gaps, we introduce MSEarth, a
multimodal scientific benchmark curated from high-quality, open-access
scientific publications. MSEarth encompasses the five major spheres of Earth
science: atmosphere, cryosphere, hydrosphere, lithosphere, and biosphere,
featuring over 7K figures with refined captions. These captions are crafted
from the original figure captions and enriched with discussions and reasoning
from the papers, ensuring the benchmark captures the nuanced reasoning and
knowledge-intensive content essential for advanced scientific tasks. MSEarth
supports a variety of tasks, including scientific figure captioning, multiple
choice questions, and open-ended reasoning challenges. By bridging the gap in
graduate-level benchmarks, MSEarth provides a scalable and high-fidelity
resource to enhance the development and evaluation of MLLMs in scientific
reasoning. The benchmark is publicly available to foster further research and
innovation in this field. Resources related to this benchmark can be found at
https://huggingface.co/MSEarth and https://github.com/xiangyu-mm/MSEarth.

</details>


### [302] [Can Agents Fix Agent Issues?](https://arxiv.org/abs/2505.20749)
*Alfin Wijaya Rahardja,Junwei Liu,Weitong Chen,Zhenpeng Chen,Yiling Lou*

Key words: LLM代理系统, 软件维护, 自动修复, 基准测试, SE代理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文研究了LLM代理系统中的问题自动修复，构建了一个可复现的基准AGENTISSUE-BENCH，并评估了现有SE代理的效果，发现其修复率较低，突出了代理系统维护的独特挑战。

Motivation: 由于LLM代理系统维护困难且需求不断变化，自动解决代理系统的问题（如错误报告或功能请求）成为关键且具有挑战性的任务。

Method: 手动分析201个真实代理问题并分类，构建AGENTISSUE-BENCH基准（含50个任务），并评估现有SE代理的表现。

Result: 现有SE代理在AGENTISSUE-BENCH上的解决率仅为3.33%-12.67%，表明其在代理系统中的效果有限。

Conclusion: 代理系统维护与传统软件差异显著，需进一步研究开发更先进的SE代理以解决问题。

Abstract: LLM-based agent systems are emerging as a new software paradigm and have been
widely adopted across diverse domains such as medicine, robotics, and
programming. However, maintaining these systems requires substantial effort, as
they are inevitably prone to bugs and continually evolve to meet changing
external requirements. Therefore, automatically resolving agent issues (i.e.,
bug reports or feature requests) is a crucial and challenging task. While
recent software engineering (SE) agents (e.g., SWE-agent) have shown promise in
addressing issues in traditional software systems, it remains unclear how
effectively they can resolve real-world issues in agent systems, which differ
significantly from traditional software. To fill this gap, we first manually
analyze 201 real-world agent issues and identify common categories of agent
issues. We then spend 500 person-hours constructing AGENTISSUE-BENCH, a
reproducible benchmark comprising 50 agent issue resolution tasks (each with an
executable environment and failure-triggering tests). We further evaluate
state-of-the-art SE agents on AGENTISSUE-BENCH and reveal their limited
effectiveness (i.e., with only 3.33% - 12.67% resolution rates). These results
underscore the unique challenges of maintaining agent systems compared to
traditional software, highlighting the need for further research to develop
advanced SE agents for resolving agent issues. Data and code are available at
https://alfin06.github.io/AgentIssue-Bench-Leaderboard/#/ .

</details>


### [303] [MT-Mol:Multi Agent System with Tool-based Reasoning for Molecular Optimization](https://arxiv.org/abs/2505.20820)
*Hyomin Kim,Yunhui Jang,Sungsoo Ahn*

Key words: 分子优化, 大型语言模型, 多智能体框架, RDKit工具, 逐步推理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: MT-Mol是一个基于多智能体框架的分子优化系统，利用工具引导的推理和专业化的LLM智能体，显著提升了分子优化的性能。

Motivation: 当前大型语言模型在分子优化中的应用尚未充分探索，尤其在结构化推理、可解释性和工具支持的全面性方面存在不足。

Method: 引入MT-Mol框架，结合RDKit工具和多角色LLM智能体，通过专家智能体分领域管理工具，实现工具对齐和逐步推理。

Result: 在PMO-1K基准测试的23项任务中，17项达到最优性能。

Conclusion: MT-Mol通过多智能体协作和工具引导推理，显著提升了分子优化的效果。

Abstract: Large language models (LLMs) have large potential for molecular optimization,
as they can gather external chemistry tools and enable collaborative
interactions to iteratively refine molecular candidates. However, this
potential remains underexplored, particularly in the context of structured
reasoning, interpretability, and comprehensive tool-grounded molecular
optimization. To address this gap, we introduce MT-Mol, a multi-agent framework
for molecular optimization that leverages tool-guided reasoning and
role-specialized LLM agents. Our system incorporates comprehensive RDKit tools,
categorized into five distinct domains: structural descriptors, electronic and
topological features, fragment-based functional groups, molecular
representations, and miscellaneous chemical properties. Each category is
managed by an expert analyst agent, responsible for extracting task-relevant
tools and enabling interpretable, chemically grounded feedback. MT-Mol produces
molecules with tool-aligned and stepwise reasoning through the interaction
between the analyst agents, a molecule-generating scientist, a reasoning-output
verifier, and a reviewer agent. As a result, we show that our framework shows
the state-of-the-art performance of the PMO-1K benchmark on 17 out of 23 tasks.

</details>


### [304] [Step-Wise Formal Verification for LLM-Based Mathematical Problem Solving](https://arxiv.org/abs/2505.20869)
*Kuo Zhou,Lu Zhang*

Key words: 大语言模型, 数学问题求解, 形式化验证, 自动修正

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一个名为MATH-VF的框架，通过Formalizer和Critic模块验证大语言模型（LLMs）生成的数学问题解决方案的正确性，并提供了修正反馈。

Motivation: 虽然LLMs在解决数学问题上表现出色，但常存在逻辑推理和计算错误，因此需要一种方法验证和修正其解决方案。

Method: MATH-VF框架包含两个模块：Formalizer（将自然语言解决方案转换为形式化上下文）和Critic（使用多种外部工具验证形式化上下文中的每个陈述并提供反馈）。

Result: 在MATH500和ProcessBench等数学基准测试中，MATH-VF验证和修正解决方案的表现优于现有方法。

Conclusion: MATH-VF能有效验证和修正LLMs生成的数学解决方案，提升其准确性。

Abstract: Large Language Models (LLMs) have demonstrated formidable capabilities in
solving mathematical problems, yet they may still commit logical reasoning and
computational errors during the problem-solving process. Thus, this paper
proposes a framework, MATH-VF, which includes a Formalizer and a Critic, for
formally verifying the correctness of the solutions generated by large language
models. Our framework first utilizes a Formalizer which employs an LLM to
translate a natural language solution into a formal context. Afterward, our
Critic (which integrates various external tools such as a Computer Algebra
System and an SMT solver) evaluates the correctness of each statement within
the formal context, and when a statement is incorrect, our Critic provides
corrective feedback. We empirically investigate the effectiveness of MATH-VF in
two scenarios: 1) Verification: MATH-VF is utilized to determine the
correctness of a solution to a given problem. 2) Refinement: When MATH-VF
identifies errors in the solution generated by an LLM-based solution generator
for a given problem, it submits the corrective suggestions proposed by the
Critic to the solution generator to regenerate the solution. We evaluate our
framework on widely used mathematical benchmarks: MATH500 and ProcessBench,
demonstrating the superiority of our approach over existing approaches.

</details>


### [305] [Reinforcement Learning-based Sequential Route Recommendation for System-Optimal Traffic Assignment](https://arxiv.org/abs/2505.20889)
*Leizhen Wang,Peibo Duan,Cheng Lyu,Zhenliang Ma*

Key words: 个性化路由推荐, 系统最优, 强化学习, 交通分配

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一个基于学习的框架，将静态系统最优交通分配问题重新表述为单智能体深度强化学习任务，通过路由推荐实现系统最优。

Motivation: 研究个性化路由推荐是否能在集体层面实现系统最优交通分配。

Method: 开发了MSA引导的深度Q学习算法，将传统交通分配方法的迭代结构融入强化学习训练。

Result: 在Braess网络上收敛到理论最优解，在Ortuzar-Willumsen网络上仅偏差0.35%。

Conclusion: 通过基于学习的顺序分配，为个体路由行为与系统效率之间提供了理论与实用的桥梁。

Abstract: Modern navigation systems and shared mobility platforms increasingly rely on
personalized route recommendations to improve individual travel experience and
operational efficiency. However, a key question remains: can such sequential,
personalized routing decisions collectively lead to system-optimal (SO) traffic
assignment? This paper addresses this question by proposing a learning-based
framework that reformulates the static SO traffic assignment problem as a
single-agent deep reinforcement learning (RL) task. A central agent
sequentially recommends routes to travelers as origin-destination (OD) demands
arrive, to minimize total system travel time. To enhance learning efficiency
and solution quality, we develop an MSA-guided deep Q-learning algorithm that
integrates the iterative structure of traditional traffic assignment methods
into the RL training process. The proposed approach is evaluated on both the
Braess and Ortuzar-Willumsen (OW) networks. Results show that the RL agent
converges to the theoretical SO solution in the Braess network and achieves
only a 0.35% deviation in the OW network. Further ablation studies demonstrate
that the route action set's design significantly impacts convergence speed and
final performance, with SO-informed route sets leading to faster learning and
better outcomes. This work provides a theoretically grounded and practically
relevant approach to bridging individual routing behavior with system-level
efficiency through learning-based sequential assignment.

</details>


### [306] [Controllable Logical Hypothesis Generation for Abductive Reasoning in Knowledge Graphs](https://arxiv.org/abs/2505.20948)
*Yisen Gao,Jiaxin Bai,Tianshi Zheng,Qingyun Sun,Ziwei Zhang,Jianxin Li,Yangqiu Song,Xingcheng Fu*

Key words: 溯因推理、知识图谱、假设生成、可控性、强化学习

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种可控的逻辑假设生成框架（CtrlHGen），用于知识图谱上的溯因推理，解决了假设空间坍塌和假设过度敏感问题，通过两阶段训练（监督学习和强化学习）和语义奖励机制提升生成效果。

Motivation: 解决知识图谱溯因推理中由于缺乏可控性导致的冗余或无关假设问题，提升假设生成的实际应用价值。

Method: 提出CtrlHGen框架，采用两阶段训练（监督学习+强化学习），设计子逻辑分解的数据增强策略缓解假设空间坍塌，引入平滑语义奖励（Dice和Overlap分数）和条件遵循奖励解决假设过度敏感问题。

Result: 在三个基准数据集上的实验表明，该模型不仅更好地遵循控制条件，且在语义相似性上优于基线方法。

Conclusion: CtrlHGen通过可控假设生成显著提升了溯因推理的实用性和效果。

Abstract: Abductive reasoning in knowledge graphs aims to generate plausible logical
hypotheses from observed entities, with broad applications in areas such as
clinical diagnosis and scientific discovery. However, due to a lack of
controllability, a single observation may yield numerous plausible but
redundant or irrelevant hypotheses on large-scale knowledge graphs. To address
this limitation, we introduce the task of controllable hypothesis generation to
improve the practical utility of abductive reasoning. This task faces two key
challenges when controlling for generating long and complex logical hypotheses:
hypothesis space collapse and hypothesis oversensitivity. To address these
challenges, we propose CtrlHGen, a Controllable logcial Hypothesis Generation
framework for abductive reasoning over knowledge graphs, trained in a two-stage
paradigm including supervised learning and subsequent reinforcement learning.
To mitigate hypothesis space collapse, we design a dataset augmentation
strategy based on sub-logical decomposition, enabling the model to learn
complex logical structures by leveraging semantic patterns in simpler
components. To address hypothesis oversensitivity, we incorporate smoothed
semantic rewards including Dice and Overlap scores, and introduce a
condition-adherence reward to guide the generation toward user-specified
control constraints. Extensive experiments on three benchmark datasets
demonstrate that our model not only better adheres to control conditions but
also achieves superior semantic similarity performance compared to baselines.

</details>


### [307] [Large Language Model-enhanced Reinforcement Learning for Low-Altitude Economy Networking](https://arxiv.org/abs/2505.21045)
*Lingyi Cai,Ruichen Zhang,Changyuan Zhao,Yu Zhang,Jiawen Kang,Dusit Niyato,Tao Jiang,Xuemin Shen*

Key words: 低空经济网络（LAENet）、强化学习（RL）、大型语言模型（LLMs）、奖励设计、决策生成

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文探讨了如何利用大型语言模型（LLMs）增强强化学习（RL）在低空经济网络（LAENet）中的应用，提出了一个LLM增强的RL框架，并通过案例研究验证了其有效性。

Motivation: 低空经济网络（LAENet）面临复杂决策、资源限制和环境不确定性等挑战，传统强化学习方法在泛化性、奖励设计和模型稳定性方面存在不足，LLMs的出现为解决这些问题提供了新机会。

Method: 论文首先介绍了将LLMs整合到RL中的方法，利用LLMs的生成、上下文理解和结构化推理能力。随后提出了一个LLM增强的RL框架，LLMs在其中扮演信息处理器、奖励设计者、决策者和生成者的角色。并通过案例研究，使用LLMs设计奖励函数以提高RL在LAENet中的学习性能。

Result: 通过LLM增强的RL框架，成功提升了RL在LAENet中的学习性能和决策效率，证明了LLMs在解决RL局限性方面的潜力。

Conclusion: LLMs能够有效增强RL在LAENet中的应用，但未来仍需进一步研究以优化框架并扩展其应用场景。

Abstract: Low-Altitude Economic Networking (LAENet) aims to support diverse flying
applications below 1,000 meters by deploying various aerial vehicles for
flexible and cost-effective aerial networking. However, complex
decision-making, resource constraints, and environmental uncertainty pose
significant challenges to the development of the LAENet. Reinforcement learning
(RL) offers a potential solution in response to these challenges but has
limitations in generalization, reward design, and model stability. The
emergence of large language models (LLMs) offers new opportunities for RL to
mitigate these limitations. In this paper, we first present a tutorial about
integrating LLMs into RL by using the capacities of generation, contextual
understanding, and structured reasoning of LLMs. We then propose an
LLM-enhanced RL framework for the LAENet in terms of serving the LLM as
information processor, reward designer, decision-maker, and generator.
Moreover, we conduct a case study by using LLMs to design a reward function to
improve the learning performance of RL in the LAENet. Finally, we provide a
conclusion and discuss future work.

</details>


### [308] [Agent-Environment Alignment via Automated Interface Generation](https://arxiv.org/abs/2505.21055)
*Kaiming Liu,Xuanyu Lei,Ziyue Wang,Peng Li,Yang Liu*

Key words: 大语言模型代理, 代理环境不对齐, 接口生成, ALIGN框架

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出ALIGN框架，通过自动对齐接口生成来解决LLM代理与环境之间的不对齐问题，无需修改代理或环境代码即可提升性能。

Motivation: 现有LLM代理在交互决策任务中常因代理与环境之间的接口不匹配（称为“代理环境不对齐”）而性能受限，这一问题尚未被充分探索。

Method: 作者提出ALIGN框架，自动生成对齐接口，通过丰富环境静态信息和逐步观察数据来缓解不对齐问题。

Result: 实验显示ALIGN在多个领域（如具身任务、网页导航和工具使用）均显著提升性能，例如ALFWorld中成功率提高45.67%。

Conclusion: ALIGN框架能有效解决代理环境不对齐问题，且无需修改代理或环境逻辑，具有广泛的适用性。

Abstract: Large language model (LLM) agents have shown impressive reasoning
capabilities in interactive decision-making tasks. These agents interact with
environment through intermediate interfaces, such as predefined action spaces
and interaction rules, which mediate the perception and action. However,
mismatches often happen between the internal expectations of the agent
regarding the influence of its issued actions and the actual state transitions
in the environment, a phenomenon referred to as \textbf{agent-environment
misalignment}. While prior work has invested substantially in improving agent
strategies and environment design, the critical role of the interface still
remains underexplored. In this work, we empirically demonstrate that
agent-environment misalignment poses a significant bottleneck to agent
performance. To mitigate this issue, we propose \textbf{ALIGN}, an
\underline{A}uto-A\underline{l}igned \underline{I}nterface
\underline{G}e\underline{n}eration framework that alleviates the misalignment
by enriching the interface. Specifically, the ALIGN-generated interface
enhances both the static information of the environment and the step-wise
observations returned to the agent. Implemented as a lightweight wrapper, this
interface achieves the alignment without modifying either the agent logic or
the environment code. Experiments across multiple domains including embodied
tasks, web navigation and tool-use, show consistent performance improvements,
with up to a 45.67\% success rate improvement observed in ALFWorld. Meanwhile,
ALIGN-generated interface can generalize across different agent architectures
and LLM backbones without interface regeneration. Code and experimental results
are available at https://github.com/THUNLP-MT/ALIGN.

</details>


### [309] [Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning](https://arxiv.org/abs/2505.21067)
*Xiao Hu,Xingyu Lu,Liyuan Mao,YiFan Zhang,Tianke Zhang,Bin Wen,Fan Yang,Tingting Gao,Guorui Zhou*

Key words: 强化学习, 大型语言模型, 蒸馏, 推理能力, 认知行为

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 使用仅920个示例的简单蒸馏方法能明显优于需要更多数据和计算成本的零强化学习（zero-RL），尤其在推理灵活性和高级认知行为方面表现更优。

Motivation: 探讨如何通过蒸馏方法提升大型语言模型的推理能力，尤其是在数据量有限的情况下，同时分析其与零强化学习的对比效果。

Method: 采用基于基础模型的简单蒸馏方法，并使用少量数据（920个示例）进行训练。分析模型输出的词汇频率和高级认知行为的出现频率。

Result: 蒸馏模型在推理灵活性和高级认知行为（如多角度思考和元认知意识）方面显著优于零强化学习模型。

Conclusion: 蒸馏方法在少量数据下能更高效地提升模型的推理能力，尤其是生成灵活且有逻辑的输出。

Abstract: Reinforcement learning (RL) has played an important role in improving the
reasoning ability of large language models (LLMs). Some studies apply RL
directly to \textit{smaller} base models (known as zero-RL) and also achieve
notable progress. However, in this paper, we show that using only 920 examples,
a simple distillation method based on the base model can clearly outperform
zero-RL, which typically requires much more data and computational cost. By
analyzing the token frequency in model outputs, we find that the distilled
model shows more flexible reasoning. It uses anthropomorphic tokens and logical
connectors much more often than the zero-RL model. Further analysis reveals
that distillation enhances the presence of two advanced cognitive behaviors:
Multi-Perspective Thinking or Attempting and Metacognitive Awareness. Frequent
occurrences of these two advanced cognitive behaviors give rise to flexible
reasoning, which is essential for solving complex reasoning problems, while
zero-RL fails to significantly boost the frequency of these behaviors.

</details>


### [310] [Interpreting Social Bias in LVLMs via Information Flow Analysis and Multi-Round Dialogue Evaluation](https://arxiv.org/abs/2505.21106)
*Zhengyang Ji,Yifan Jia,Shang Gao,Yutao Yue*

Key words: 大视觉语言模型，社会偏见，信息流分析，多轮对话，跨模态解释

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该研究提出了一种解释性框架，结合信息流分析和多轮对话评估，旨在从内部信息利用不平衡的角度理解大视觉语言模型（LVLM）中社会偏见的起源。研究发现LVLM在处理不同人口群体图像时存在系统性的信息使用差异，且偏见也存在于文本模态的语义表示中。

Motivation: LVLM在多模态任务中取得了显著进展，但也表现出显著的社会偏见。现有研究主要集中于检测和量化这些偏见，但对其在模型内部的起源机制了解有限，因此本研究旨在填补这一空白。

Method: 研究通过信息流分析识别中性问题推理过程中高贡献的图像标记，并设计多轮对话机制评估这些关键标记编码敏感信息的程度。此外，还从文本模态补充分析语义表示的偏见模式。

Result: 实验表明LVLM在处理不同人口群体图像时存在信息使用的系统性差异，且文本模态的语义表示中已显示出偏见的邻近模式，从而提供了跨模态的偏见形成解释。

Conclusion: 社会偏见深深植根于模型的内部推理动态中，未来的研究需要更深入地理解并解决这些偏见。

Abstract: Large Vision Language Models (LVLMs) have achieved remarkable progress in
multimodal tasks, yet they also exhibit notable social biases. These biases
often manifest as unintended associations between neutral concepts and
sensitive human attributes, leading to disparate model behaviors across
demographic groups. While existing studies primarily focus on detecting and
quantifying such biases, they offer limited insight into the underlying
mechanisms within the models. To address this gap, we propose an explanatory
framework that combines information flow analysis with multi-round dialogue
evaluation, aiming to understand the origin of social bias from the perspective
of imbalanced internal information utilization. Specifically, we first identify
high-contribution image tokens involved in the model's reasoning process for
neutral questions via information flow analysis. Then, we design a multi-turn
dialogue mechanism to evaluate the extent to which these key tokens encode
sensitive information. Extensive experiments reveal that LVLMs exhibit
systematic disparities in information usage when processing images of different
demographic groups, suggesting that social bias is deeply rooted in the model's
internal reasoning dynamics. Furthermore, we complement our findings from a
textual modality perspective, showing that the model's semantic representations
already display biased proximity patterns, thereby offering a cross-modal
explanation of bias formation.

</details>


### [311] [Interpretable DNFs](https://arxiv.org/abs/2505.21212)
*Martin C. Cooper,Imane Bousdira,Clément Carbonnel*

Key words: 可解释性, DNF分类器, 决策树, 嵌套k-DNF

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文研究了可解释的DNF（析取范式）分类器，尤其是那些正负决策都能用小规模的DNF解释的分类器，比较了深度k决策树和新型的嵌套k-DNF模型，发现后者在可解释性和准确性上表现更好。

Motivation: 研究的目标是设计一种可解释的分类器，使得其每个决策都能用小规模的解释让人理解，尤其关注DNF公式作为分类器时的解释性。

Method: 研究比较了两类DNF分类器：深度k决策树和新型的嵌套k-DNF模型，分析了它们的结构和解释性。

Result: 实验结果表明，嵌套k-DNF在可解释性和准确性上表现出色，可以作为决策树的替代方案。

Conclusion: 嵌套k-DNF是一种有潜力的可解释分类器模型，尤其在需要解释正负决策时表现更优。

Abstract: A classifier is considered interpretable if each of its decisions has an
explanation which is small enough to be easily understood by a human user. A
DNF formula can be seen as a binary classifier $\kappa$ over boolean domains.
The size of an explanation of a positive decision taken by a DNF $\kappa$ is
bounded by the size of the terms in $\kappa$, since we can explain a positive
decision by giving a term of $\kappa$ that evaluates to true. Since both
positive and negative decisions must be explained, we consider that
interpretable DNFs are those $\kappa$ for which both $\kappa$ and
$\overline{\kappa}$ can be expressed as DNFs composed of terms of bounded size.
In this paper, we study the family of $k$-DNFs whose complements can also be
expressed as $k$-DNFs. We compare two such families, namely depth-$k$ decision
trees and nested $k$-DNFs, a novel family of models. Experiments indicate that
nested $k$-DNFs are an interesting alternative to decision trees in terms of
interpretability and accuracy.

</details>


### [312] [XBOUND: Exploring the Capability Boundaries of Device-Control Agents through Trajectory Tree Exploration](https://arxiv.org/abs/2505.21279)
*Shaoqing Zhang,Kehai Chen,Zhuosheng Zhang,Rumei Li,Rongxiang Weng,Yang Xiang,Liqiang Nie,Min Zhang*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种名为XBOUND的新评估方法，通过Explore Metric来界定设备控制代理的能力边界，并开发了一个伪任务树数据集用于评估。

Motivation: 传统评估方法无法提供微观层面的错误分析，XBOUND旨在填补这一空白，以更细粒度评估设备控制代理的性能。

Method: 采用XBOUND评估方法，计算Explore Metric，并使用基于Android Control数据的伪任务树数据集进行评估。

Result: 对OS-Atlas和UI-TARS系列进行了全面评估，揭示了其在五个常见任务中的整体和具体性能及局限性。

Conclusion: XBOUND方法提供了更细致的评估视角，揭示了设备控制代理的当前缺陷，为未来改进提供了方向。

Abstract: Recent advancements in vision-language models (VLMs) have spurred increased
interest in Device-Control Agents (DC agents), such as utilizing in-the-wild
device control to manage graphical user interfaces. Conventional methods for
assessing the capabilities of DC agents, such as computing step-wise action
accuracy and overall task success rates, provide a macroscopic view of DC
agents' performance; however, they fail to offer microscopic insights into
potential errors that may occur in real-world applications. Conducting a
finer-grained performance evaluation of DC agents presents significant
challenges. This study introduces a new perspective on evaluation methods for
DC agents by proposing the XBOUND evaluation method, which employs the
calculation of a novel Explore Metric to delineate the capability boundaries of
DC agents. Compared to previous evaluation methods, XBOUND focuses on
individual states to assess the proficiency of DC agents in mastering these
states. Furthermore, we have developed a ``pseudo'' episode tree dataset
derived from Android Control test data. Utilizing this dataset and XBOUND, we
comprehensively evaluate the OS-Atlas and UI-TARS series, examining both the
overall and specific performance across five common tasks. Additionally, we
select representative cases to highlight the current deficiencies and
limitations inherent in both series. Code is available at
https://github.com/sqzhang-lazy/XBOUND.

</details>


### [313] [RLJP: Legal Judgment Prediction via First-Order Logic Rule-enhanced with Large Language Models](https://arxiv.org/abs/2505.21281)
*Yue Zhang,Zhiliang Tian,Shicheng Zhou,Haiyang Wang,Wenqing Hou,Yuying Liu,Xuechen Zhao,Minlie Huang,Ye Wang,Bin Zhou*

Key words: 法律判决预测, 一阶逻辑, 对比学习, 自适应调整, 混淆感知学习

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一个基于一阶逻辑和对比学习的规则增强法律判决预测框架，通过动态优化判决逻辑来提高性能。

Motivation: 现有法律判决预测模型忽视了法律推理逻辑的重要性，且在复杂案件中适应性不足。

Method: 采用三阶段方法：使用一阶逻辑初始化判决规则，通过混淆感知对比学习动态优化规则，最后用优化后的规则进行预测。

Result: 在两个公开数据集上的实验结果表明，该方法在所有指标上均优于现有模型。

Conclusion: 该方法通过自适应调整法律判决逻辑，显著提升了法律判决预测的性能。

Abstract: Legal Judgment Prediction (LJP) is a pivotal task in legal AI. Existing
semantic-enhanced LJP models integrate judicial precedents and legal knowledge
for high performance. But they neglect legal reasoning logic, a critical
component of legal judgments requiring rigorous logical analysis. Although some
approaches utilize legal reasoning logic for high-quality predictions, their
logic rigidity hinders adaptation to case-specific logical frameworks,
particularly in complex cases that are lengthy and detailed. This paper
proposes a rule-enhanced legal judgment prediction framework based on
first-order logic (FOL) formalism and comparative learning (CL) to develop an
adaptive adjustment mechanism for legal judgment logic and further enhance
performance in LJP. Inspired by the process of human exam preparation, our
method follows a three-stage approach: first, we initialize judgment rules
using the FOL formalism to capture complex reasoning logic accurately; next, we
propose a Confusion-aware Contrastive Learning (CACL) to dynamically optimize
the judgment rules through a quiz consisting of confusable cases; finally, we
utilize the optimized judgment rules to predict legal judgments. Experimental
results on two public datasets show superior performance across all metrics.
The code is publicly available{https://anonymous.4open.science/r/RLJP-FDF1}.

</details>


### [314] [Complex System Diagnostics Using a Knowledge Graph-Informed and Large Language Model-Enhanced Framework](https://arxiv.org/abs/2505.21291)
*Saman Marandi,Yu-Shu Hu,Mohammad Modarres*

Key words: 知识图谱,KG,大型语言模型,LLM,动态主逻辑,DML,系统诊断,功能建模

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种结合知识图谱（KGs）和大型语言模型（LLMs）的新型诊断框架，用于高可靠性系统（如核电站）的系统诊断。通过功能建模和动态主逻辑（DML）模型，框架实现了自动化逻辑构建和交互式诊断，并在案例研究中验证了其有效性。

Motivation: 传统诊断模型在复杂系统中表现不佳，功能建模成为一种更优选择。为提升诊断精度和深度，论文提出结合知识图谱和语言模型，解决复杂系统诊断问题。

Method: 论文设计了基于DML模型的功能建模框架，包含两个LLM组件：一个用于自动化构建DML逻辑，另一个用于交互式诊断。逻辑编码为结构化KG（KG-DML），支持分层故障推理。交互阶段通过自然语言查询和Graph-RAG方法实现高效诊断。

Result: 案例研究显示该框架在辅助给水系统中的关键元素准确率超过90%，工具和参数提取一致，适用于安全关键诊断。

Conclusion: 该框架有效支持复杂系统的自动化诊断和自然语言交互，具备高可靠性和实用性。

Abstract: In this paper, we present a novel diagnostic framework that integrates
Knowledge Graphs (KGs) and Large Language Models (LLMs) to support system
diagnostics in high-reliability systems such as nuclear power plants.
Traditional diagnostic modeling struggles when systems become too complex,
making functional modeling a more attractive approach. Our approach introduces
a diagnostic framework grounded in the functional modeling principles of the
Dynamic Master Logic (DML) model. It incorporates two coordinated LLM
components, including an LLM-based workflow for automated construction of DML
logic from system documentation and an LLM agent that facilitates interactive
diagnostics. The generated logic is encoded into a structured KG, referred to
as KG-DML, which supports hierarchical fault reasoning. Expert knowledge or
operational data can also be incorporated to refine the model's precision and
diagnostic depth. In the interaction phase, users submit natural language
queries, which are interpreted by the LLM agent. The agent selects appropriate
tools for structured reasoning, including upward and downward propagation
across the KG-DML. Rather than embedding KG content into every prompt, the LLM
agent distinguishes between diagnostic and interpretive tasks. For diagnostics,
the agent selects and executes external tools that perform structured KG
reasoning. For general queries, a Graph-based Retrieval-Augmented Generation
(Graph-RAG) approach is used, retrieving relevant KG segments and embedding
them into the prompt to generate natural explanations. A case study on an
auxiliary feedwater system demonstrated the framework's effectiveness, with
over 90% accuracy in key elements and consistent tool and argument extraction,
supporting its use in safety-critical diagnostics.

</details>


### [315] [Beyond Chemical QA: Evaluating LLM's Chemical Reasoning with Modular Chemical Operations](https://arxiv.org/abs/2505.21318)
*Hao Li,He Cao,Bin Feng,Yanjun Shao,Xiangru Tang,Zhiyuan Yan,Li Yuan,Yonghong Tian,Yu Li*

Key words: large language models, Chain-of-Thought reasoning, chemistry, molecular optimization, reaction prediction

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: ChemCoTBench 是一个化学推理框架，将分子结构理解与算术操作结合，提升 LLMs 在化学任务（如分子优化和反应预测）中的逐步推理能力。

Motivation: 当前基准测试仅关注简单知识检索，忽视了化学领域（如药物设计和反应工程）所需的逐步推理。

Method: 通过模块化“化学操作”（如添加、删除、替换）将化学问题转化为透明的工作流，模仿数学证明逻辑。

Result: 在分子属性优化和化学反应预测任务中验证框架有效性，并通过标注数据集和基准测试提供评估基础。

Conclusion: ChemCoTBench 填补了抽象推理方法与实际化学发现之间的空白，为 AI 驱动的科学创新奠定基础。

Abstract: While large language models (LLMs) with Chain-of-Thought (CoT) reasoning
excel in mathematics and coding, their potential for systematic reasoning in
chemistry, a domain demanding rigorous structural analysis for real-world tasks
like drug design and reaction engineering, remains untapped. Current benchmarks
focus on simple knowledge retrieval, neglecting step-by-step reasoning required
for complex tasks such as molecular optimization and reaction prediction. To
address this, we introduce ChemCoTBench, a reasoning framework that bridges
molecular structure understanding with arithmetic-inspired operations,
including addition, deletion, and substitution, to formalize chemical
problem-solving into transparent, step-by-step workflows. By treating molecular
transformations as modular "chemical operations", the framework enables
slow-thinking reasoning, mirroring the logic of mathematical proofs while
grounding solutions in real-world chemical constraints. We evaluate models on
two high-impact tasks: Molecular Property Optimization and Chemical Reaction
Prediction. These tasks mirror real-world challenges while providing structured
evaluability. By providing annotated datasets, a reasoning taxonomy, and
baseline evaluations, ChemCoTBench bridges the gap between abstract reasoning
methods and practical chemical discovery, establishing a foundation for
advancing LLMs as tools for AI-driven scientific innovation.

</details>


### [316] [Assured Autonomy with Neuro-Symbolic Perception](https://arxiv.org/abs/2505.21322)
*R. Spencer Hallyburton,Miroslav Pajic*

Key words: 神经符号感知、场景图生成、可信AI、自主系统、网络安全

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出神经符号感知范式（NeuSPaPer），通过结合对象检测和场景图生成（SGG）实现深层场景理解，增强AI在安全关键领域的可靠性。

Motivation: 当前AI模型在安全关键和对抗领域的可靠性不足，需结合符号推理提升感知模型的可信度。

Method: 采用神经符号方法，结合基础模型离线知识提取和实时SGG算法，设计结构化关系图框架。

Result: 实验证明SGG能弥合低层感知与高层推理的鸿沟，提升AI的情境意识和抗干扰能力。

Conclusion: NeusPaPer为CPS中的可信自主性奠定基础，推动AI在复杂环境中的稳健应用。

Abstract: Many state-of-the-art AI models deployed in cyber-physical systems (CPS),
while highly accurate, are simply pattern-matchers.~With limited security
guarantees, there are concerns for their reliability in safety-critical and
contested domains. To advance assured AI, we advocate for a paradigm shift that
imbues data-driven perception models with symbolic structure, inspired by a
human's ability to reason over low-level features and high-level context. We
propose a neuro-symbolic paradigm for perception (NeuSPaPer) and illustrate how
joint object detection and scene graph generation (SGG) yields deep scene
understanding.~Powered by foundation models for offline knowledge extraction
and specialized SGG algorithms for real-time deployment, we design a framework
leveraging structured relational graphs that ensures the integrity of
situational awareness in autonomy. Using physics-based simulators and
real-world datasets, we demonstrate how SGG bridges the gap between low-level
sensor perception and high-level reasoning, establishing a foundation for
resilient, context-aware AI and advancing trusted autonomy in CPS.

</details>


### [317] [MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs](https://arxiv.org/abs/2505.21327)
*Jiakang Yuan,Tianshuo Peng,Yilei Jiang,Yiting Lu,Renrui Zhang,Kaituo Feng,Chaoyou Fu,Tao Chen,Lei Bai,Bo Zhang,Xiangyu Yue*

Key words: 多模态大语言模型, 逻辑推理, MME-Reasoning, 归纳推理, 演绎推理, 溯因推理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文介绍了 MME-Reasoning 这一综合基准测试，用于评估多模态大语言模型（MLLMs）的逻辑推理能力，发现现有模型在全面推理评估中存在显著局限性。

Motivation: 现有基准测试未能全面评估 MLLMs 的推理能力，主要由于缺乏对推理类型的明确分类和对推理本身的理解不足。

Method: 提出 MME-Reasoning 基准测试，覆盖归纳、演绎和溯因三类推理问题，确保问题专注评估推理能力而非感知或知识广度，并扩展评估协议以覆盖多样化问题。

Result: 顶级 MLLMs 在全逻辑推理能力评估中表现有限，且不同推理类型间存在明显性能不均衡。对“思维模式”和基于规则的强化学习方法分析表明，它们并未显著提升推理能力。

Conclusion: 当前 MLLMs 在多样化逻辑推理场景中存在关键局限性和性能不均衡，提供了系统化的见解以理解和评估推理能力。

Abstract: Logical reasoning is a fundamental aspect of human intelligence and an
essential capability for multimodal large language models (MLLMs). Despite the
significant advancement in multimodal reasoning, existing benchmarks fail to
comprehensively evaluate their reasoning abilities due to the lack of explicit
categorization for logical reasoning types and an unclear understanding of
reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive
benchmark designed to evaluate the reasoning ability of MLLMs, which covers all
three types of reasoning (i.e., inductive, deductive, and abductive) in its
questions. We carefully curate the data to ensure that each question
effectively evaluates reasoning ability rather than perceptual skills or
knowledge breadth, and extend the evaluation protocols to cover the evaluation
of diverse questions. Our evaluation reveals substantial limitations of
state-of-the-art MLLMs when subjected to holistic assessments of logical
reasoning capabilities. Even the most advanced MLLMs show limited performance
in comprehensive logical reasoning, with notable performance imbalances across
reasoning types. In addition, we conducted an in-depth analysis of approaches
such as ``thinking mode'' and Rule-based RL, which are commonly believed to
enhance reasoning abilities. These findings highlight the critical limitations
and performance imbalances of current MLLMs in diverse logical reasoning
scenarios, providing comprehensive and systematic insights into the
understanding and evaluation of reasoning capabilities.

</details>


### [318] [The Multilingual Divide and Its Impact on Global AI Safety](https://arxiv.org/abs/2505.21344)
*Aidan Peppin,Julia Kreutzer,Alice Schoenauer Sebag,Kelly Marchisio,Beyza Ermis,John Dang,Samuel Cahyawijaya,Shivalika Singh,Seraphina Goldfarb-Tarrant,Viraat Aryabumi,Aakanksha,Wei-Yin Ko,Ahmet Üstün,Matthias Gallé,Marzieh Fadaee,Sara Hooker*

Key words: 语言差距、AI安全、多语言数据集、透明度、治理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 这篇论文讨论了AI中的“语言差距”问题，分析了其存在和扩大的原因，以及如何通过多语言数据集创建、透明度和研究来弥补差距并减少安全风险。

Motivation: 当前大型语言模型在少数主流语言上表现优异，但在其他语言上能力与安全性存在显著差距，这导致了全球AI安全的不平等。

Method: 通过分析语言差距的成因及其对AI安全的影响，识别解决这些挑战的障碍。

Result: 指出了政策与治理领域可以通过支持多语言数据集、透明度和研究来弥补这一差距。

Conclusion: 为研究者和决策者提供了缩小语言差距的具体建议，以提升全球AI安全。

Abstract: Despite advances in large language model capabilities in recent years, a
large gap remains in their capabilities and safety performance for many
languages beyond a relatively small handful of globally dominant languages.
This paper provides researchers, policymakers and governance experts with an
overview of key challenges to bridging the "language gap" in AI and minimizing
safety risks across languages. We provide an analysis of why the language gap
in AI exists and grows, and how it creates disparities in global AI safety. We
identify barriers to address these challenges, and recommend how those working
in policy and governance can help address safety concerns associated with the
language gap by supporting multilingual dataset creation, transparency, and
research.

</details>


### [319] [A Structured Unplugged Approach for Foundational AI Literacy in Primary Education](https://arxiv.org/abs/2505.21398)
*Maria Cristina Carrisi,Mirko Marras,Sara Vergallo*

Key words: AI literacy, primary education, conceptual understanding, mathematical reasoning, empirical study

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种结构化的教学方法，通过结合小学数学课程的核心内容，提升小学生对AI的基础理解能力。研究通过实证测试表明，该方法有效提高了学生的AI术语理解、逻辑推理和批判性评估能力。

Motivation: 年轻一代在智能化技术日益普及的环境中成长，早期AI素养的培养至关重要。当前的教育方法过于侧重工具使用，缺乏对概念的深入理解，导致儿童对AI存在误解和偏见。

Method: 提出了一种结构化教学方法，结合小学数学课程的核心元素，重点强化概念理解、数据表示、分类推理和AI评估。通过实证研究，对31名五年级学生进行测试和满意度调查。

Result: 学生在AI术语理解、特征描述、逻辑推理和评估能力方面显著提升，对AI决策过程及其局限性有了更深刻的理解。学生尤其喜欢将AI概念与现实推理结合的活动。

Conclusion: 该方法不仅有效提升了学生的AI素养，还因其趣味性和实用性受到学生欢迎，为小学AI教育提供了可复制的实践框架。

Abstract: Younger generations are growing up in a world increasingly shaped by
intelligent technologies, making early AI literacy crucial for developing the
skills to critically understand and navigate them. However, education in this
field often emphasizes tool-based learning, prioritizing usage over
understanding the underlying concepts. This lack of knowledge leaves
non-experts, especially children, prone to misconceptions, unrealistic
expectations, and difficulties in recognizing biases and stereotypes. In this
paper, we propose a structured and replicable teaching approach that fosters
foundational AI literacy in primary students, by building upon core
mathematical elements closely connected to and of interest in primary
curricula, to strengthen conceptualization, data representation, classification
reasoning, and evaluation of AI. To assess the effectiveness of our approach,
we conducted an empirical study with thirty-one fifth-grade students across two
classes, evaluating their progress through a post-test and a satisfaction
survey. Our results indicate improvements in terminology understanding and
usage, features description, logical reasoning, and evaluative skills, with
students showing a deeper comprehension of decision-making processes and their
limitations. Moreover, the approach proved engaging, with students particularly
enjoying activities that linked AI concepts to real-world reasoning. Materials:
https://github.com/tail-unica/ai-literacy-primary-ed.

</details>


### [320] [MRSD: Multi-Resolution Skill Discovery for HRL Agents](https://arxiv.org/abs/2505.21410)
*Shashank Sharma,Janina Hoffmann,Vinay Namboodiri*

Key words: 分层强化学习, 技能发现, 多分辨率, 动态选择, DeepMind Control Suite

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: MRSD提出了一种多分辨率技能发现的HRL框架，通过并行学习不同时间分辨率的技能编码器，使高层管理器能动态选择技能，从而在任务中表现优于现有方法。

Motivation: 受人类运动控制的启发，现有技能发现方法局限于单一任务单一技能，而人类能同时使用多粒度技能，因此需探索多分辨率技能在HRL中的应用。

Method: 提出MRSD框架，并行学习不同时间分辨率的技能编码器，高层管理器动态选择技能，实现自适应控制。

Result: 在DeepMind Control Suite任务上，MRSD收敛更快且最终性能优于现有技能发现和HRL方法。

Conclusion: 多分辨率技能的集成能提升HRL的效率和泛化能力，为智能体设计提供新方向。

Abstract: Hierarchical reinforcement learning (HRL) relies on abstract skills to solve
long-horizon tasks efficiently. While existing skill discovery methods learns
these skills automatically, they are limited to a single skill per task. In
contrast, humans learn and use both fine-grained and coarse motor skills
simultaneously. Inspired by human motor control, we propose Multi-Resolution
Skill Discovery (MRSD), an HRL framework that learns multiple skill encoders at
different temporal resolutions in parallel. A high-level manager dynamically
selects among these skills, enabling adaptive control strategies over time. We
evaluate MRSD on tasks from the DeepMind Control Suite and show that it
outperforms prior state-of-the-art skill discovery and HRL methods, achieving
faster convergence and higher final performance. Our findings highlight the
benefits of integrating multi-resolution skills in HRL, paving the way for more
versatile and efficient agents.

</details>


### [321] [Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG LLMs](https://arxiv.org/abs/2505.21419)
*Yifan Wang,Kenneth P. Birman*

Key words: 云托管应用, 多模态RAG LLM, AI模式匹配, 故障诊断, ARCA

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: ARCA，一种新型多模态RAG LLM系统，通过结合现代AI工具的模式匹配能力和自然多模态接口，简化了云托管应用程序问题的识别与解决，并优于现有技术。

Motivation: 云托管应用程序和服务的复杂性导致性能或功能不稳定的根本原因众多，现有方法难以高效解决问题。

Method: 结合现代AI工具的模式匹配能力和自然多模态RAG LLM接口，开发了ARCA系统。

Result: 逐步评估显示ARCA在问题识别与解决上优于现有技术。

Conclusion: ARCA有效简化了复杂云应用的故障诊断流程，提升了问题解决的效率。

Abstract: Today's cloud-hosted applications and services are complex systems, and a
performance or functional instability can have dozens or hundreds of potential
root causes. Our hypothesis is that by combining the pattern matching
capabilities of modern AI tools with a natural multi-modal RAG LLM interface,
problem identification and resolution can be simplified. ARCA is a new
multi-modal RAG LLM system that targets this domain. Step-wise evaluations show
that ARCA outperforms state-of-the-art alternatives.

</details>


### [322] [Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks](https://arxiv.org/abs/2505.21426)
*Francesco Cozzi,Marco Pangallo,Alan Perotti,André Panisson,Corrado Monti*

Key words: 代理模型, 可微分模拟, 扩散模型, 图神经网络, 涌现动态

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种新方法，通过观察数据学习可微分的ABM代理模型，结合扩散模型和图神经网络来捕捉个体行为动态，保留ABM的去中心化特性。

Motivation: 现有的ABM模型在优化时因规则不可微分而难以与真实数据结合，限制了梯度优化方法的应用。本文旨在解决这一问题，使ABM更易与数据驱动方法整合。

Method: 结合扩散模型捕捉行为的随机性，通过图神经网络建模代理间的互动，直接模拟个体行为而非仅近似系统输出。

Result: 在两个ABM模型（谢林隔离模型和捕食者-猎物生态系统）中验证成功，能复现个体行为并准确预测训练外的涌现动态。

Conclusion: 扩散模型与图学习的结合展示了数据驱动ABM模拟的潜力，为复杂系统的可微分建模提供了新思路。

Abstract: Agent-Based Models (ABMs) are powerful tools for studying emergent properties
in complex systems. In ABMs, agent behaviors are governed by local interactions
and stochastic rules. However, these rules are, in general, non-differentiable,
limiting the use of gradient-based methods for optimization, and thus
integration with real-world data. We propose a novel framework to learn a
differentiable surrogate of any ABM by observing its generated data. Our method
combines diffusion models to capture behavioral stochasticity and graph neural
networks to model agent interactions. Distinct from prior surrogate approaches,
our method introduces a fundamental shift: rather than approximating
system-level outputs, it models individual agent behavior directly, preserving
the decentralized, bottom-up dynamics that define ABMs. We validate our
approach on two ABMs (Schelling's segregation model and a Predator-Prey
ecosystem) showing that it replicates individual-level patterns and accurately
forecasts emergent dynamics beyond training. Our results demonstrate the
potential of combining diffusion models and graph learning for data-driven ABM
simulation.

</details>


### [323] [Policy Induction: Predicting Startup Success via Explainable Memory-Augmented In-Context Learning](https://arxiv.org/abs/2505.21427)
*Xianling Mu,Joseph Ternasky,Fuat Alican,Yigit Ihlamur*

Key words: 早期投资、大语言模型、上下文学习、透明决策、初创企业

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种基于记忆增强大语言模型（LLMs）和上下文学习（ICL）的透明、数据高效的初创企业投资决策框架，显著提高了预测准确性，远超传统风险投资机构的成功率。

Motivation: 早期初创企业投资数据稀缺且结果不确定，传统机器学习方法需要大量标注数据且不透明，难以被领域专家解释或改进。

Method: 采用记忆增强的LLMs和ICL，通过自然语言策略嵌入模型提示，结合少量样本学习和上下文学习循环，实现轻量级训练和政策迭代更新。

Result: 系统预测初创企业成功的准确率远超基准，比随机概率高20倍（1.9%），比顶级风投机构的5.6%成功率高7.1倍。

Conclusion: 该框架提供了一种透明、数据高效且可解释的初创企业投资决策方法，显著优于传统方法。

Abstract: Early-stage startup investment is a high-risk endeavor characterized by
scarce data and uncertain outcomes. Traditional machine learning approaches
often require large, labeled datasets and extensive fine-tuning, yet remain
opaque and difficult for domain experts to interpret or improve. In this paper,
we propose a transparent and data-efficient investment decision framework
powered by memory-augmented large language models (LLMs) using in-context
learning (ICL). Central to our method is a natural language policy embedded
directly into the LLM prompt, enabling the model to apply explicit reasoning
patterns and allowing human experts to easily interpret, audit, and iteratively
refine the logic. We introduce a lightweight training process that combines
few-shot learning with an in-context learning loop, enabling the LLM to update
its decision policy iteratively based on structured feedback. With only minimal
supervision and no gradient-based optimization, our system predicts startup
success far more accurately than existing benchmarks. It is over 20x more
precise than random chance, which succeeds 1.9% of the time. It is also 7.1x
more precise than the typical 5.6% success rate of top-tier venture capital
(VC) firms.

</details>


### [324] [Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming](https://arxiv.org/abs/2505.21486)
*Yang Yang,Jiemin Wu,Yutao Yue*

Key words: 多Agent系统, LLM, 归纳逻辑编程, 语言偏差, 符号基础

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种结合多Agent系统（基于LLM）与归纳逻辑编程（ILP）的新框架，自动生成结构化符号词汇和关系模板，从原始文本数据中直接构建语言偏差，显著提升了假设生成的自动化、可解释性与可验证性。

Motivation: 解决传统ILP依赖预定义符号结构和纯LLM方法对噪声敏感的问题，实现开放环境中鲁棒假设生成的自动化。

Method: 通过LLM驱动的多Agent系统自动生成符号词汇和关系模板（语言偏差），并将其转化为ILP求解器可处理的事实，以归纳学习可解释规则。

Result: 在多样挑战性场景中的实验验证了方法的优越性能。

Conclusion: 该方法为自动化、可解释且可验证的假设生成开辟了新路径。

Abstract: Automating robust hypothesis generation in open environments is pivotal for
AI cognition. We introduce a novel framework integrating a multi-agent system,
powered by Large Language Models (LLMs), with Inductive Logic Programming
(ILP). Our system's LLM agents autonomously define a structured symbolic
vocabulary (predicates) and relational templates , i.e., \emph{language bias}
directly from raw textual data. This automated symbolic grounding (the
construction of the language bias), traditionally an expert-driven bottleneck
for ILP, then guides the transformation of text into facts for an ILP solver,
which inductively learns interpretable rules. This approach overcomes
traditional ILP's reliance on predefined symbolic structures and the
noise-sensitivity of pure LLM methods. Extensive experiments in diverse,
challenging scenarios validate superior performance, paving a new path for
automated, explainable, and verifiable hypothesis generation.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [325] [FD-Bench: A Modular and Fair Benchmark for Data-driven Fluid Simulation](https://arxiv.org/abs/2505.20349)
*Haixin Wang,Ruoyan Li,Fred Xu,Fang Sun,Kaiqiao Han,Zijie Huang,Guancheng Wan,Ching Chang,Xiao Luo,Wei Wang,Yizhou Sun*

Key words: 数据驱动建模, 流体动力学, 基准测试, 神经PDE求解器, 可复现性

<details>
  <summary>Details</summary>

Main category: physics.flu-dyn

TL;DR: FD-Bench是一个公平、模块化、全面且可复现的数据驱动流体模拟基准测试，通过统一实验设置评估85个基线模型，解决了当前评估中的分散性问题。

Motivation: 当前数据驱动的流体动力学建模缺乏统一的PDE数据集和标准化评估协议，导致公平评估困难。FD-Bench旨在填补这一空白。

Method: 引入FD-Bench基准测试，采用模块化设计评估空间、时间和损失模块，系统对比传统数值求解器，并分析分辨率、初始条件和时间窗口的泛化能力。

Result: FD-Bench提供了迄今为止最全面的排行榜，解决了可复现性和可比性问题，为未来数据驱动流体模型的评估奠定了基础。

Conclusion: FD-Bench为数据驱动流体模拟的公平评估和未来研究提供了标准化框架和开放代码库。

Abstract: Data-driven modeling of fluid dynamics has advanced rapidly with neural PDE
solvers, yet a fair and strong benchmark remains fragmented due to the absence
of unified PDE datasets and standardized evaluation protocols. Although
architectural innovations are abundant, fair assessment is further impeded by
the lack of clear disentanglement between spatial, temporal and loss modules.
In this paper, we introduce FD-Bench, the first fair, modular, comprehensive
and reproducible benchmark for data-driven fluid simulation. FD-Bench
systematically evaluates 85 baseline models across 10 representative flow
scenarios under a unified experimental setup. It provides four key
contributions: (1) a modular design enabling fair comparisons across spatial,
temporal, and loss function modules; (2) the first systematic framework for
direct comparison with traditional numerical solvers; (3) fine-grained
generalization analysis across resolutions, initial conditions, and temporal
windows; and (4) a user-friendly, extensible codebase to support future
research. Through rigorous empirical studies, FD-Bench establishes the most
comprehensive leaderboard to date, resolving long-standing issues in
reproducibility and comparability, and laying a foundation for robust
evaluation of future data-driven fluid models. The code is open-sourced at
https://anonymous.4open.science/r/FD-Bench-15BC.

</details>


### [326] [Solving Euler equations with Multiple Discontinuities via Separation-Transfer Physics-Informed Neural Networks](https://arxiv.org/abs/2505.20361)
*Chuanxing Wang,Hui Luo,Kai Wang,Guohuai Zhu,Mingxing Luo*

Key words: 物理信息神经网络、流体动力学、间断问题、迁移学习、二维非定常激波折射

<details>
  <summary>Details</summary>

Main category: physics.flu-dyn

TL;DR: 为解决物理信息神经网络（PINNs）处理多重间断流体动力学问题时的挑战，本文提出ST-PINNs方法，通过分步解决间断并利用迁移学习显著提升精度。

Motivation: PINNs在科学计算中虽表现优异，但在处理多重间断的流体动力学问题时仍存挑战。为此，研究提出新方法以提升其性能。

Method: 采用ST-PINNs方法，分步从强到弱解决间断问题，并结合迁移学习进行训练。

Result: 数值实验表明，ST-PINNs能更精准捕捉尖锐间断，并大幅降低多重间断流体动力学问题的解误差。

Conclusion: ST-PINNs为复杂冲击-界面相互作用问题提供了新的PINNs应用思路，显著提升了计算精度。

Abstract: Despite the remarkable progress of physics-informed neural networks (PINNs)
in scientific computing, they continue to face challenges when solving
hydrodynamic problems with multiple discontinuities. In this work, we propose
Separation-Transfer Physics Informed Neural Networks (ST-PINNs) to address such
problems. By sequentially resolving discontinuities from strong to weak and
leveraging transfer learning during training, ST-PINNs significantly reduce the
problem complexity and enhance solution accuracy. To the best of our knowledge,
this is the first study to apply a PINNs-based approach to the two-dimensional
unsteady planar shock refraction problem, offering new insights into the
application of PINNs to complex shock-interface interactions. Numerical
experiments demonstrate that ST-PINNs more accurately capture sharp
discontinuities and substantially reduce solution errors in hydrodynamic
problems involving multiple discontinuities.

</details>


### [327] [A Physics-Augmented GraphGPS Framework for the Reconstruction of 3D Riemann Problems from Sparse Data](https://arxiv.org/abs/2505.21421)
*Rami Cassia,Rich Kerswell*

Key words: 可压缩流体, 机器学习, 逆问题, GraphGPS, 激波重建

<details>
  <summary>Details</summary>

Main category: physics.flu-dyn

TL;DR: 该论文探讨了利用GraphGPS框架在物理信息驱动下从稀疏观测中重建3D Riemann问题的可压缩流体流动，通过改进消息传递机制提高重建精度和计算效率。

Motivation: 研究动机是解决从稀疏测量中重建可压缩流体流动中的激波、不连续性和稀疏波等特征的逆问题，并结合物理信息驱动的机器学习方法提升重建效果。

Method: 采用GraphGPS框架，结合位置编码、局部消息传递和全局上下文感知，改进消息传递机制以识别激波和不连续性，并限制信息仅从已知节点流动以提高计算效率。

Result: GraphGPS框架在重建精度和计算效率上均优于其他机器学习基准，能够更清晰地重建激波和不连续性。

Conclusion: GraphGPS框架通过结合物理信息驱动的机器学习和改进的消息传递机制，有效解决了稀疏观测下的流体流动重建问题。

Abstract: In compressible fluid flow, reconstructing shocks, discontinuities,
rarefactions, and their interactions from sparse measurements is an important
inverse problem with practical applications. Moreover, physics-informed machine
learning has recently become an increasingly popular approach for performing
reconstructions tasks. In this work we explore a machine learning recipe, known
as GraphGPS, for reconstructing canonical compressible flows known as 3D
Riemann problems from sparse observations, in a physics-informed manner. The
GraphGPS framework combines the benefits of positional encodings, local
message-passing of graphs, and global contextual awareness, and we explore the
latter two components through an ablation study. Furthermore, we modify the
aggregation step of message-passing such that it is aware of shocks and
discontinuities, resulting in sharper reconstructions of these features.
Additionally, we modify message-passing such that information flows strictly
from known nodes only, which results in computational savings, better training
convergence, and no degradation of reconstruction accuracy. We also show that
the GraphGPS framework outperforms numerous machine learning benchmarks.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [328] [MedSentry: Understanding and Mitigating Safety Risks in Medical LLM Multi-Agent Systems](https://arxiv.org/abs/2505.20824)
*Kai Chen,Taihang Zhen,Hewei Wang,Kailai Liu,Xinfeng Li,Jing Huo,Tianpei Yang,Jinfeng Xu,Wei Dong,Yang Gao*

Key words: 大语言模型,多智能体系统,医疗安全,对抗性攻击,防御机制

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 论文介绍了MedSentry，一个包含5000个对抗性医疗提示的基准数据集，用于评估多智能体拓扑结构在医疗领域的抗攻击能力，并提出了防御机制以提升系统安全性。

Motivation: 随着大语言模型（LLMs）在医疗领域的广泛应用，确保其在多智能体协作配置中的安全性至关重要。

Method: 开发了包括5000个对抗性医疗提示的MedSentry基准数据集，并通过攻击-防御评估管道分析了四种多智能体拓扑结构的抗攻击能力。

Result: 发现不同架构在信息污染和决策鲁棒性上存在显著差异，SharedPool易受攻击，而Decentralized架构更稳健。提出的防御机制可将系统安全性恢复至接近基线水平。

Conclusion: MedSentry为设计更安全的医疗多智能体系统提供了评估框架和防御策略。

Abstract: As large language models (LLMs) are increasingly deployed in healthcare,
ensuring their safety, particularly within collaborative multi-agent
configurations, is paramount. In this paper we introduce MedSentry, a benchmark
comprising 5 000 adversarial medical prompts spanning 25 threat categories with
100 subthemes. Coupled with this dataset, we develop an end-to-end
attack-defense evaluation pipeline to systematically analyze how four
representative multi-agent topologies (Layers, SharedPool, Centralized, and
Decentralized) withstand attacks from 'dark-personality' agents. Our findings
reveal critical differences in how these architectures handle information
contamination and maintain robust decision-making, exposing their underlying
vulnerability mechanisms. For instance, SharedPool's open information sharing
makes it highly susceptible, whereas Decentralized architectures exhibit
greater resilience thanks to inherent redundancy and isolation. To mitigate
these risks, we propose a personality-scale detection and correction mechanism
that identifies and rehabilitates malicious agents, restoring system safety to
near-baseline levels. MedSentry thus furnishes both a rigorous evaluation
framework and practical defense strategies that guide the design of safer
LLM-based multi-agent systems in medical domains.

</details>


### [329] [Revisiting Multi-Agent World Modeling from a Diffusion-Inspired Perspective](https://arxiv.org/abs/2505.20922)
*Yang Zhang,Xinran Li,Jianing Ye,Delin Qu,Shuang Qiu,Chongjie Zhang,Xiu Li,Chenjia Bai*

Key words: 多智能体强化学习, 扩散模型, 世界模型, 样本效率, 序列化建模

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: DIMA采用扩散模型作为世界模型，通过序列化建模降低多智能体系统的复杂性，显著提升了样本效率和最终表现。

Motivation: 多智能体强化学习中，联合行动空间大且动态不确定，直接建模环境复杂。

Method: 通过序列化智能体建模，专注于状态空间，利用扩散模型的反向过程逐步揭示行动，构建DIMA模型。

Result: 在MAMuJoCo和Bi-DexHands等基准测试中表现最优，超越现有世界模型。

Conclusion: DIMA为多智能体世界模型提供了新范式，推动了MARL研究的发展。

Abstract: World models have recently attracted growing interest in Multi-Agent
Reinforcement Learning (MARL) due to their ability to improve sample efficiency
for policy learning. However, accurately modeling environments in MARL is
challenging due to the exponentially large joint action space and highly
uncertain dynamics inherent in multi-agent systems. To address this, we reduce
modeling complexity by shifting from jointly modeling the entire state-action
transition dynamics to focusing on the state space alone at each timestep
through sequential agent modeling. Specifically, our approach enables the model
to progressively resolve uncertainty while capturing the structured
dependencies among agents, providing a more accurate representation of how
agents influence the state. Interestingly, this sequential revelation of
agents' actions in a multi-agent system aligns with the reverse process in
diffusion models--a class of powerful generative models known for their
expressiveness and training stability compared to autoregressive or latent
variable models. Leveraging this insight, we develop a flexible and robust
world model for MARL using diffusion models. Our method, Diffusion-Inspired
Multi-Agent world model (DIMA), achieves state-of-the-art performance across
multiple multi-agent control benchmarks, significantly outperforming prior
world models in terms of final return and sample efficiency, including MAMuJoCo
and Bi-DexHands. DIMA establishes a new paradigm for constructing multi-agent
world models, advancing the frontier of MARL research.

</details>


### [330] [GGBond: Growing Graph-Based AI-Agent Society for Socially-Aware Recommender Simulation](https://arxiv.org/abs/2505.21154)
*Hailin Zhong,Hanlin Wang,Yujun Ye,Meiyi Zhang,Shengxin Zhu*

Key words: 个性化推荐系统, 社交模拟, 认知代理, 动态社交图, 长期效果评估

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 论文提出了一种高保真社交模拟平台，通过模拟用户认知代理和动态社交互动，解决传统推荐系统依赖静态数据的局限性，从而更真实地模拟用户行为的演变。

Motivation: 传统个性化推荐系统主要依赖静态离线数据，无法有效捕捉用户偏好长期演变和社交影响的动态变化。

Method: 提出基于五层认知架构的Sim-User代理，结合ICR2动机引擎和多层异质社交图（GGBond Graph），模拟用户的决策过程和社会关系动态。

Result: 这一设计超越了传统静态数据集的限制，为评估推荐系统的长期效果提供了可控、可观测的环境。

Conclusion: 该平台显著提升了推荐系统对动态用户行为和社会影响的模拟能力。

Abstract: Current personalized recommender systems predominantly rely on static offline
data for algorithm design and evaluation, significantly limiting their ability
to capture long-term user preference evolution and social influence dynamics in
real-world scenarios. To address this fundamental challenge, we propose a
high-fidelity social simulation platform integrating human-like cognitive
agents and dynamic social interactions to realistically simulate user behavior
evolution under recommendation interventions. Specifically, the system
comprises a population of Sim-User Agents, each equipped with a five-layer
cognitive architecture that encapsulates key psychological mechanisms,
including episodic memory, affective state transitions, adaptive preference
learning, and dynamic trust-risk assessments. In particular, we innovatively
introduce the Intimacy--Curiosity--Reciprocity--Risk (ICR2) motivational engine
grounded in psychological and sociological theories, enabling more realistic
user decision-making processes. Furthermore, we construct a multilayer
heterogeneous social graph (GGBond Graph) supporting dynamic relational
evolution, effectively modeling users' evolving social ties and trust dynamics
based on interest similarity, personality alignment, and structural homophily.
During system operation, agents autonomously respond to recommendations
generated by typical recommender algorithms (e.g., Matrix Factorization,
MultVAE, LightGCN), deciding whether to consume, rate, and share content while
dynamically updating their internal states and social connections, thereby
forming a stable, multi-round feedback loop. This innovative design transcends
the limitations of traditional static datasets, providing a controlled,
observable environment for evaluating long-term recommender effects.

</details>


### [331] [Large Language Models Miss the Multi-Agent Mark](https://arxiv.org/abs/2505.21298)
*Emanuele La Malfa,Gabriele La Malfa,Samuele Marro,Jie M. Zhang,Elizabeth Black,Micheal Luck,Philip Torr,Michael Wooldridge*

Key words: 多智能体系统,大语言模型,术语滥用,理论实践脱节

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 当前多智能体大语言模型（MAS LLMs）研究多借用MAS术语但忽略其核心原则，本文指出了MAS理论与实际实现的四大关键差异，并呼吁更严谨地整合MAS概念。

Motivation: 揭示当前MAS LLMs研究与经典多智能体系统理论的脱节，避免重复解决已有问题，推动领域健康发展。

Method: 通过对比分析MAS理论与现有MAS LLMs框架，聚焦社会性、环境设计、协调协议和涌现行为四大维度。

Result: 指出当前MAS LLMs在自主性、社交互动和结构化环境等方面的缺失，多为过度简化的LLM中心架构。

Conclusion: 应更严谨整合MAS理论，避免术语滥用，以发掘未被充分利用的研究机会。

Abstract: Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)
has led to an increase in frameworks leveraging multiple LLMs to tackle complex
tasks. However, much of this literature appropriates the terminology of MAS
without engaging with its foundational principles. In this position paper, we
highlight critical discrepancies between MAS theory and current MAS LLMs
implementations, focusing on four key areas: the social aspect of agency,
environment design, coordination and communication protocols, and measuring
emergent behaviours. Our position is that many MAS LLMs lack multi-agent
characteristics such as autonomy, social interaction, and structured
environments, and often rely on oversimplified, LLM-centric architectures. The
field may slow down and lose traction by revisiting problems the MAS literature
has already addressed. Therefore, we systematically analyse this issue and
outline associated research opportunities; we advocate for better integrating
established MAS concepts and more precise terminology to avoid
mischaracterisation and missed opportunities.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [332] [Transfer learning for multifidelity simulation-based inference in cosmology](https://arxiv.org/abs/2505.21215)
*Alex A. Saoulis,Davide Piras,Niall Jeffrey,Alessio Spurio Mancini,Ana M. G. Ferreira,Benjamin Joachimi*

Key words: 模拟推理, 多保真度迁移学习, 宇宙学参数估计, 计算成本优化

<details>
  <summary>Details</summary>

Main category: astro-ph.CO

TL;DR: 通过多保真度迁移学习，结合低保真度与少量高保真度模拟数据，显著降低了高精度SBI的算力需求。

Motivation: 解决传统模拟推理中高保真度模拟数据算力成本过高的问题。

Method: 采用多保真度迁移学习，先在低保真度N体模拟上预训练，再结合少量高保真度流体模拟优化。

Result: 所需高保真模拟数据量减少8-15倍，具体取决于模型复杂度、后验维度和性能指标。

Conclusion: 该方法显著降低了计算成本，同时保持高性能和精度的高保真模型推断。

Abstract: Simulation-based inference (SBI) enables cosmological parameter estimation
when closed-form likelihoods or models are unavailable. However, SBI relies on
machine learning for neural compression and density estimation. This requires
large training datasets which are prohibitively expensive for high-quality
simulations. We overcome this limitation with multifidelity transfer learning,
combining less expensive, lower-fidelity simulations with a limited number of
high-fidelity simulations. We demonstrate our methodology on dark matter
density maps from two separate simulation suites in the hydrodynamical CAMELS
Multifield Dataset. Pre-training on dark-matter-only $N$-body simulations
reduces the required number of high-fidelity hydrodynamical simulations by a
factor between $8$ and $15$, depending on the model complexity, posterior
dimensionality, and performance metrics used. By leveraging cheaper
simulations, our approach enables performant and accurate inference on
high-fidelity models while substantially reducing computational costs.

</details>


### [333] [Wavelet Flow For Extragalactic Foreground Simulations](https://arxiv.org/abs/2505.21220)
*M. Mebratu,W. L. K. Wu*

Key words: 小波流（WF）、宇宙微波背景（CMB）、透镜收敛（κ）、宇宙红外背景（CIB）、场级概率分布

<details>
  <summary>Details</summary>

Main category: astro-ph.CO

TL;DR: 该论文探讨了使用小波流（WF）模型来模拟宇宙微波背景（CMB）次要成分（如透镜收敛和宇宙红外背景）的场级概率分布，以提高非高斯统计建模的准确性。通过独立调整多尺度架构的参数和先验，模型在生成样本的平均功率谱和Minkowski函数上均表现优异。

Motivation: 当前和未来的CMB观测数据精度越来越高，需要对非高斯统计分布进行更有效的场级建模，以最大化提取宇宙学和天体物理信息。

Method: 采用小波流（WF）模型联合训练CMB透镜收敛（κ）和宇宙红外背景（CIB）映射，通过多尺度架构独立微调模型参数和先验。

Result: 训练后的WF模型能高精度恢复输入数据，生成的κ和CIB样本的平均功率谱和Minkowski函数与输入数据高度吻合（误差在几个百分点内）。

Conclusion: WF模型能准确模拟CMB次要成分的关联性，为宇宙学数据分析提供了改进工具。

Abstract: Extragalactic foregrounds in cosmic microwave background (CMB) observations
are both a source of cosmological and astrophysical information and a nuisance
to the CMB. Effective field-level modeling that captures their non-Gaussian
statistical distributions is increasingly important for optimal information
extraction, particularly given the precise and low-noise observations from
current and upcoming experiments. We explore the use of Wavelet Flow (WF)
models to tackle the novel task of modeling the field-level probability
distributions of multi-component CMB secondaries. Specifically, we jointly
train correlated CMB lensing convergence ($\kappa$) and cosmic infrared
background (CIB) maps with a WF model and obtain a network that statistically
recovers the input to high accuracy -- the trained network generates samples of
$\kappa$ and CIB fields whose average power spectra are within a few percent of
the inputs across all scales, and whose Minkowski functionals are similarly
accurate compared to the inputs. Leveraging the multiscale architecture of
these models, we fine-tune both the model parameters and the priors at each
scale independently, optimizing performance across different resolutions. These
results demonstrate that WF models can accurately simulate correlated
components of CMB secondaries, supporting improved analysis of cosmological
data. Our code and trained models can be found here
(https://github.com/matiwosm/HybridPriorWavletFlow.git).

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [334] [Unified Deep Learning Approach for Estimating the Metallicities of RR Lyrae Stars Using light curves from Gaia Data Release 3](https://arxiv.org/abs/2505.20947)
*Lorenzo Monti,Tatiana Muraveva,Alessia Garofalo,Gisella Clementini,Maria Letizia Valentini*

Key words: RR Lyrae stars, deep learning, Gaia DR3, metallicity estimation, GRU neural network

<details>
  <summary>Details</summary>

Main category: astro-ph.SR

TL;DR: 利用Gaia DR3的G波段光变曲线数据，作者开发了一个基于GRU神经网络的深度学习框架，可同时估计基础模式(RRab)和第一泛音(RRc)RR Lyrae恒星的金属丰度，实现了高预测精度和广泛泛化能力。

Motivation: 随着Gaia DR3提供了约27万颗RR Lyrae恒星的光变曲线数据，亟需一种可扩展的方法来从光变数据中估算其金属丰度，以支持恒星种群和银河系结构的研究。

Method: 采用Gated Recurrent Unit (GRU) 神经网络进行时间序列外源回归，通过预处理步骤（如相位折叠、平滑和样本加权）以及来自文献的光度金属丰度作为训练目标，统一处理RRab和RRc星的光变曲线形态差异。

Result: 模型在验证集上表现出色：RRab星的MAE=0.0565 dex、RMSE=0.0765 dex、R²=0.9401；RRc星的MAE=0.0505 dex、RMSE=0.0720 dex、R²=0.9625。

Conclusion: 深度学习方法在大规模光度金属丰度估算中表现高效，适用于恒星种群和银河系结构的研究。

Abstract: RR Lyrae stars (RRLs) are old pulsating variables widely used as metallicity
tracers due to the correlation between their metal abundances and light curve
morphology. With ESA Gaia DR3 providing light curves for about 270,000 RRLs,
there is a pressing need for scalable methods to estimate their metallicities
from photometric data. We introduce a unified deep learning framework that
estimates metallicities for both fundamental-mode (RRab) and first-overtone
(RRc) RRLs using Gaia G-band light curves. This approach extends our previous
work on RRab stars to include RRc stars, aiming for high predictive accuracy
and broad generalization across both pulsation types. The model is based on a
Gated Recurrent Unit (GRU) neural network optimized for time-series extrinsic
regression. Our pipeline includes preprocessing steps such as phase folding,
smoothing, and sample weighting, and uses photometric metallicities from the
literature as training targets. The architecture is designed to handle
morphological differences between RRab and RRc light curves without requiring
separate models. On held-out validation sets, our GRU model achieves strong
performance: for RRab stars, MAE = 0.0565 dex, RMSE = 0.0765 dex, R^2 = 0.9401;
for RRc stars, MAE = 0.0505 dex, RMSE = 0.0720 dex, R^2 = 0.9625. These results
show the effectiveness of deep learning for large-scale photometric metallicity
estimation and support its application to studies of stellar populations and
Galactic structure.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [335] [Convergence of Clipped-SGD for Convex $(L_0,L_1)$-Smooth Optimization with Heavy-Tailed Noise](https://arxiv.org/abs/2505.20817)
*Savelii Chezhegov,Aleksandr Beznosikov,Samuel Horváth,Eduard Gorbunov*

Key words: 梯度裁剪, 重尾噪声, $(L_0,L_1)$-平滑, 高概率收敛, Clip-SGD

<details>
  <summary>Details</summary>

Main category: math.OC

TL;DR: 论文提出了在凸优化和$(L_0,L_1)$-平滑条件下，针对Clip-SGD在高概率收敛方面的首个理论分析，填补了现有文献的空白。

Motivation: 现有的Clip-SGD在高概率收敛方面的研究在同时处理重尾噪声和$(L_0,L_1)$-平滑条件时存在不足，本文旨在填补这一理论缺口。

Method: 利用Clip-SGD方法，结合凸优化和$(L_0,L_1)$-平滑假设，分析了其在重尾噪声下的高概率收敛性。

Result: 论文建立了Clip-SGD的首个高概率收敛界，避免了指数级大因子，且不依赖严格的次高斯噪声假设。

Conclusion: 本文显著扩展了梯度裁剪的适用范围，为理论和实际应用提供了更全面的支持。

Abstract: Gradient clipping is a widely used technique in Machine Learning and Deep
Learning (DL), known for its effectiveness in mitigating the impact of
heavy-tailed noise, which frequently arises in the training of large language
models. Additionally, first-order methods with clipping, such as Clip-SGD,
exhibit stronger convergence guarantees than SGD under the
$(L_0,L_1)$-smoothness assumption, a property observed in many DL tasks.
However, the high-probability convergence of Clip-SGD under both assumptions --
heavy-tailed noise and $(L_0,L_1)$-smoothness -- has not been fully addressed
in the literature. In this paper, we bridge this critical gap by establishing
the first high-probability convergence bounds for Clip-SGD applied to convex
$(L_0,L_1)$-smooth optimization with heavy-tailed noise. Our analysis extends
prior results by recovering known bounds for the deterministic case and the
stochastic setting with $L_1 = 0$ as special cases. Notably, our rates avoid
exponentially large factors and do not rely on restrictive sub-Gaussian noise
assumptions, significantly broadening the applicability of gradient clipping.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [336] [Future of Code with Generative AI: Transparency and Safety in the Era of AI Generated Software](https://arxiv.org/abs/2505.20303)
*David Hanson*

Key words: AI生成代码, 透明度, 安全性, 软件开发, 人工通用智能

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 该研究探讨了AI生成代码的透明性和安全性问题，分析了市场机会、挑战及解决方案，并展望了其对人工通用智能和人类与AI交互的长期影响。

Motivation: 随着AI在软件开发中的普及，AI生成代码的透明性和安全性成为关键问题。

Method: 通过分析当前AI生成代码的现状、识别潜在风险并探讨未来影响，研究提出提升透明性和功能分析的解决方案。

Result: 研究强调了AI生成代码的复杂性和潜在风险，并提出了应对措施。

Conclusion: 需采取积极措施确保AI在软件工程中的负责任开发和部署。

Abstract: As artificial intelligence becomes increasingly integrated into software
development processes, the prevalence and sophistication of AI-generated code
continue to expand rapidly. This study addresses the critical need for
transparency and safety in AI generated code by examining the current
landscape, identifying potential risks, and exploring future implications. We
analyze market opportunities for detecting AI-generated code, discuss the
challenges associated with managing increasing complexity, and propose
solutions to enhance transparency and functionality analysis. Furthermore, this
study investigates the longterm implications of AI generated code, including
its potential role in the development of artificial general intelligence and
its impact on human AI interaction. In conclusion, we emphasize the importance
of proactive measures for ensuring the responsible development and deployment
of AI in software engineering.

</details>


### [337] [Evaluating the Energy-Efficiency of the Code Generated by LLMs](https://arxiv.org/abs/2505.20324)
*Md Arman Islam,Devi Varaprasad Jonnala,Ritika Rekhi,Pratik Pokharel,Siddharth Cilamkoti,Asif Imran,Tevfik Kosar,Bekir Turkkan*

Key words: 大型语言模型、代码生成、能源效率、算法类别、环境影响的工具。

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 研究表明，大型语言模型（LLM）生成的代码虽然功能正确，但在性能和能源效率上普遍低于人工编写的代码，某些情况下能耗甚至高达人工代码的450倍。

Motivation: 随着LLM生成代码质量的提升，其在自动化代码生成中的应用日益广泛。然而，现有研究多关注功能正确性，忽视了能源效率和环境影响。本文旨在填补这一空白，评估LLM生成代码的能效表现。

Method: 研究选取了20个主流LLM，针对LeetCode平台的878个不同难度和算法类别的编程问题，生成代码并与人工标准解答进行对比分析。

Result: LLM生成的代码在多数情况下功能正确，但能效显著低于人工代码。DeepSeek-v3和GPT-4o表现最佳，而Grok-2和Gemini-1.5-Pro最差，人工代码平均能效分别是它们的1.17倍和1.21倍，Grok-2和Gemini-1.5-Pro的2倍以上。动态规划等算法类别的差异可达450倍。

Conclusion: 尽管LLM在代码生成上取得进展，但其能效问题不容忽视。需进一步优化模型以减少环境影响，尤其在特定算法领域。

Abstract: As the quality of code generated by Large Language Models (LLMs) improves,
their adoption in the software industry for automated code generation continues
to grow. Researchers primarily focus on enhancing the functional correctness of
the generated code while commonly overlooking its energy efficiency and
environmental impact. This paper investigates the energy efficiency of the code
generated by 20 popular LLMs for 878 programming problems of varying difficulty
levels and diverse algorithmic categories selected from the LeetCode platform
by comparing them against canonical human-written solutions. Although LLMs can
produce functionally correct results in most cases, our findings show that the
performance and energy efficiency of LLM-produced solutions are often far below
those of human-written solutions. Among the studied LLMs, DeepSeek-v3 and
GPT-4o generate the most energy-efficient code, whereas Grok-2 and
Gemini-1.5-Pro are among the least energy-efficient models. On average,
human-generated canonical solutions are approximately 1.17 times more energy
efficient than DeepSeek-v3, 1.21 times more energy efficient than GPT-4o, and
over 2 times more energy efficient than Grok-2 and Gemini-1.5-Pro. For specific
algorithmic groups such as dynamic programming, backtracking, and bit
manipulation, LLM-generated code can consume up to 450 times more energy than
human-generated canonical solutions.

</details>


### [338] [CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation](https://arxiv.org/abs/2504.15254)
*Anirudh Khatry,Robert Zhang,Jia Pan,Ziteng Wang,Qiaochu Chen,Greg Durrett,Isil Dillig*

Key words: C-to-Rust转译, 内存安全, CRUST-Bench, 大型语言模型, 代码迁移

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: CRUST-Bench是一个包含100个C仓库的数据集，每个仓库配有手动编写的安全Rust接口和测试用例，用于评估C到Rust转译的正确性和安全性。

Motivation: 由于缺乏评估C到安全Rust转译的数据集，作者旨在解决这一问题，推动现代化代码迁移的研究。

Method: 通过创建CRUST-Bench数据集，包含C仓库、Rust接口和测试用例，捕获跨文件依赖的复杂转译挑战。

Result: 现有大型语言模型在单次尝试下仅能解决15个任务，表明生成安全且惯用的Rust代码仍具挑战性。

Conclusion: CRUST-Bench为改进转译系统提供了基准，有助于迁移老旧C代码到内存安全的Rust。

Abstract: C-to-Rust transpilation is essential for modernizing legacy C code while
enhancing safety and interoperability with modern Rust ecosystems. However, no
dataset currently exists for evaluating whether a system can transpile C into
safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset
of 100 C repositories, each paired with manually-written interfaces in safe
Rust as well as test cases that can be used to validate correctness of the
transpilation. By considering entire repositories rather than isolated
functions, CRUST-Bench captures the challenges of translating complex projects
with dependencies across multiple files. The provided Rust interfaces provide
explicit specifications that ensure adherence to idiomatic, memory-safe Rust
patterns, while the accompanying test cases enforce functional correctness. We
evaluate state-of-the-art large language models (LLMs) on this task and find
that safe and idiomatic Rust generation is still a challenging problem for
various state-of-the-art methods and techniques. We also provide insights into
the errors LLMs usually make in transpiling code from C to safe Rust. The best
performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot
setting. Improvements on CRUST-Bench would lead to improved transpilation
systems that can reason about complex scenarios and help in migrating legacy
codebases from C into languages like Rust that ensure memory safety. You can
find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.

</details>


### [339] [An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks](https://arxiv.org/abs/2505.20854)
*Xin Zhou,Kisub Kim,Ting Zhang,Martin Weyssow,Luis F. Gomes,Guang Yang,David Lo*

Key words: LLM, 软件评估, 代码生成, 自动程序修复, 代码摘要

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: SWE-Judge 是一种针对生成软件工件的自动评估指标，通过多策略组合提高与人类评估的一致性，比现有方法更准确且可扩展。

Motivation: 现有自动评估指标难以准确反映生成软件工件的正确性，而人工评估虽准确但不可扩展。

Method: 提出 SWE-Judge，包含五个独立评估策略并通过动态团队选择和集成生成最终评分。

Result: 在多个 SE 任务中，SWE-Judge 与人类评估相关性显著提升（5.9%-183.8%），并达到接近人类注释者的一致性。

Conclusion: SWE-Judge 是一种可扩展且可靠的替代人工评估的方法。

Abstract: Large Language Models (LLMs) and other automated techniques have been
increasingly used to support software developers by generating software
artifacts such as code snippets, patches, and comments. However, accurately
assessing the correctness of these generated artifacts remains a significant
challenge. On one hand, human evaluation provides high accuracy but is
labor-intensive and lacks scalability. On the other hand, other existing
automatic evaluation metrics are scalable and require minimal human effort, but
they often fail to accurately reflect the actual correctness of generated
software artifacts.
  In this paper, we present SWE-Judge, the first evaluation metric for
LLM-as-Ensemble-Judge specifically designed to accurately assess the
correctness of generated software artifacts. SWE-Judge first defines five
distinct evaluation strategies, each implemented as an independent judge. A
dynamic team selection mechanism then identifies the most appropriate subset of
judges to produce a final correctness score through ensembling. We evaluate
SWE-Judge across a diverse set of software engineering (SE) benchmarks,
including CoNaLa, Card2Code, HumanEval-X, APPS, APR-Assess, and Summary-Assess.
These benchmarks span three SE tasks: code generation, automated program
repair, and code summarization. Experimental results demonstrate that SWE-Judge
consistently achieves a higher correlation with human judgments, with
improvements ranging from 5.9% to 183.8% over existing automatic metrics.
Furthermore, SWE-Judge reaches agreement levels with human annotators that are
comparable to inter-annotator agreement in code generation and program repair
tasks. These findings underscore SWE-Judge's potential as a scalable and
reliable alternative to human evaluation.

</details>


### [340] [SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents](https://arxiv.org/abs/2505.20411)
*Ibragim Badertdinov,Alexander Golubev,Maksim Nekrashevich,Anton Shevtsov,Simon Karasik,Andrei Andriushchenko,Maria Trofimova,Daria Litvintseva,Boris Yangel*

Key words: LLM-based agents, software engineering, interactive tasks, GitHub, contamination-free benchmark

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 论文提出了一种自动化、可扩展的流水线方法，从GitHub提取真实世界的交互式软件工程任务，构建了一个包含21,000多个Python任务的公开数据集SWE-rebench，并建立了一个无污染的基准测试。

Motivation: 当前LLM基代理在软件工程任务中表现良好，但面临高质量训练数据稀缺和静态基准测试易过时的问题。

Method: 开发自动化流水线从GitHub提取任务，构建SWE-rebench数据集，并设计无污染基准测试。

Result: 构建了包含21,000多个任务的SWE-rebench数据集，并发现某些语言模型因数据污染问题性能被高估。

Conclusion: 提出的方法解决了数据稀缺和基准测试过时的问题，为代理学习提供了更可靠的评估环境。

Abstract: LLM-based agents have shown promising capabilities in a growing range of
software engineering (SWE) tasks. However, advancing this field faces two
critical challenges. First, high-quality training data is scarce, especially
data that reflects real-world SWE scenarios, where agents must interact with
development environments, execute code and adapt behavior based on the outcomes
of their actions. Existing datasets are either limited to one-shot code
generation or comprise small, manually curated collections of interactive
tasks, lacking both scale and diversity. Second, the lack of fresh interactive
SWE tasks affects evaluation of rapidly improving models, as static benchmarks
quickly become outdated due to contamination issues. To address these
limitations, we introduce a novel, automated, and scalable pipeline to
continuously extract real-world interactive SWE tasks from diverse GitHub
repositories. Using this pipeline, we construct SWE-rebench, a public dataset
comprising over 21,000 interactive Python-based SWE tasks, suitable for
reinforcement learning of SWE agents at scale. Additionally, we use continuous
supply of fresh tasks collected using SWE-rebench methodology to build a
contamination-free benchmark for agentic software engineering. We compare
results of various LLMs on this benchmark to results on SWE-bench Verified and
show that performance of some language models might be inflated due to
contamination issues.

</details>


### [341] [SV-TrustEval-C: Evaluating Structure and Semantic Reasoning in Large Language Models for Source Code Vulnerability Analysis](https://arxiv.org/abs/2505.20630)
*Yansong Li,Paula Branco,Alexander M. Hoole,Manish Marwah,Hari Manassery Koduvely,Guy-Vincent Jourdan,Stephan Jou*

Key words: LLMs, 漏洞分析, C语言, 结构推理, 语义推理, SV-TrustEval-C

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: SV-TrustEval-C是一个针对C语言代码漏洞分析的基准测试，旨在从结构推理和语义推理两个维度评估LLMs的可靠性，发现当前模型在复杂代码关系理解上仍有不足。

Motivation: 随着LLMs在代码生成和理解上的进步，对其在源代码漏洞分析中的可靠性评估变得至关重要，而现有研究往往忽略了结构和语义推理的重要性。

Method: 提出SV-TrustEval-C基准，通过结构推理（代码元素关系识别）和语义推理（逻辑一致性检测）评估LLMs，覆盖C语言代码的复杂数据流、控制流及语义扰动场景。

Result: 当前LLMs在复杂代码关系理解上表现不佳，其漏洞分析更多依赖模式匹配而非逻辑推理。SV-TrustEval-C有效揭示了这些局限性。

Conclusion: SV-TrustEval-C为提升LLMs在真实漏洞分析任务中的推理能力和可信度指明了方向，其数据集已公开。

Abstract: As Large Language Models (LLMs) evolve in understanding and generating code,
accurately evaluating their reliability in analyzing source code
vulnerabilities becomes increasingly vital. While studies have examined LLM
capabilities in tasks like vulnerability detection and repair, they often
overlook the importance of both structure and semantic reasoning crucial for
trustworthy vulnerability analysis. To address this gap, we introduce
SV-TrustEval-C, a benchmark designed to evaluate LLMs' abilities for
vulnerability analysis of code written in the C programming language through
two key dimensions: structure reasoning - assessing how models identify
relationships between code elements under varying data and control flow
complexities; and semantic reasoning - examining their logical consistency in
scenarios where code is structurally and semantically perturbed. Our results
show that current LLMs are far from satisfactory in understanding complex code
relationships and that their vulnerability analyses rely more on pattern
matching than on robust logical reasoning. These findings underscore the
effectiveness of the SV-TrustEval-C benchmark and highlight critical areas for
enhancing the reasoning capabilities and trustworthiness of LLMs in real-world
vulnerability analysis tasks. Our initial benchmark dataset is publicly
available.

</details>


### [342] [Towards Conversational Development Environments: Using Theory-of-Mind and Multi-Agent Architectures for Requirements Refinement](https://arxiv.org/abs/2505.20973)
*Keheliya Gallaba,Ali Arabat,Dayi Lin,Mohammed Sayagh,Ahmed E. Hassan*

Key words: 基础模型, 需求细化, 多智能体系统, 心智理论, 软件开发

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 该论文提出了一种名为AlignMind的多智能体系统，通过增强基础模型（FMs）的“心智理论”能力，有效捕捉和细化软件开发中的利益相关者需求。

Motivation: 尽管基础模型在自然语言任务中表现出色，但准确捕捉利益相关者需求仍是其应用于软件开发的主要挑战。

Method: 利用具有“心智理论”能力的多智能体系统（AlignMind），通过迭代澄清利益相关者的信念、欲望和意图，将其转化为细化需求及可操作的工作流程。

Result: 在150个多样化用例中，该方法能准确捕捉需求并将其表达为规格和行动计划，显著提升软件开发效率。

Conclusion: 该方法为“以意图为先”的开发环境奠定了基础，未来可实现AI与软件开发者的无缝协作。

Abstract: Foundation Models (FMs) have shown remarkable capabilities in various natural
language tasks. However, their ability to accurately capture stakeholder
requirements remains a significant challenge for using FMs for software
development. This paper introduces a novel approach that leverages an
FM-powered multi-agent system called AlignMind to address this issue. By having
a cognitive architecture that enhances FMs with Theory-of-Mind capabilities,
our approach considers the mental states and perspectives of software makers.
This allows our solution to iteratively clarify the beliefs, desires, and
intentions of stakeholders, translating these into a set of refined
requirements and a corresponding actionable natural language workflow in the
often-overlooked requirements refinement phase of software engineering, which
is crucial after initial elicitation. Through a multifaceted evaluation
covering 150 diverse use cases, we demonstrate that our approach can accurately
capture the intents and requirements of stakeholders, articulating them as both
specifications and a step-by-step plan of action. Our findings suggest that the
potential for significant improvements in the software development process
justifies these investments. Our work lays the groundwork for future innovation
in building intent-first development environments, where software makers can
seamlessly collaborate with AIs to create software that truly meets their
needs.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [343] [Differentially private ratio statistics](https://arxiv.org/abs/2505.20351)
*Tomer Shoham,Katrina Ligettt*

Key words: 差分隐私,比率统计,相对风险,置信区间,机器学习

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文研究差分隐私下的比率统计量（如相对风险和比值比），并提出了一种简单高效的算法，兼具隐私保护、样本准确性和低偏差，适用于小样本场景。

Motivation: 现有文献在差分隐私下的比率统计量研究不足，而隐私保护在机器学习（如因果推断和公平性分析）中日益重要，该论文旨在填补这一空白。

Method: 提出一种简单的差分隐私算法，分析其在相对风险估计中的一致性，并开发了构建有效置信区间的方法。

Result: 算法不仅具有渐进性优势，在小样本下也能表现优异，同时提供隐私保护和统计准确性。

Conclusion: 该研究填补了差分隐私文献中的空白，为隐私保护的机器学习流程中的比率估计提供了实用解决方案。

Abstract: Ratio statistics--such as relative risk and odds ratios--play a central role
in hypothesis testing, model evaluation, and decision-making across many areas
of machine learning, including causal inference and fairness analysis. However,
despite privacy concerns surrounding many datasets and despite increasing
adoption of differential privacy, differentially private ratio statistics have
largely been neglected by the literature and have only recently received an
initial treatment by Lin et al. [1]. This paper attempts to fill this lacuna,
giving results that can guide practice in evaluating ratios when the results
must be protected by differential privacy. In particular, we show that even a
simple algorithm can provide excellent properties concerning privacy, sample
accuracy, and bias, not just asymptotically but also at quite small sample
sizes. Additionally, we analyze a differentially private estimator for relative
risk, prove its consistency, and develop a method for constructing valid
confidence intervals. Our approach bridges a gap in the differential privacy
literature and provides a practical solution for ratio estimation in private
machine learning pipelines.

</details>


### [344] [Kernel Quantile Embeddings and Associated Probability Metrics](https://arxiv.org/abs/2505.20433)
*Masha Naslidnyk,Siu Lun Chau,François-Xavier Briol,Krikamol Muandet*

Key words: 核分位数嵌入（KQE）、最大均值差异（MMD）、核希尔伯特空间（RKHS）、概率度量、切片Wasserstein距离

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了核分位数嵌入（KQE）的概念，作为核希尔伯特空间中分布的替代表示，超越了传统的核均值嵌入（MMD），并构建了一类新的距离度量，具有更弱的核条件、高效计算及与切片Wasserstein距离的联系。

Motivation: 传统核均值嵌入（MMD）依赖均值函数表示分布，但这是否为唯一有效的表示尚不明确。受广义分位数启发，探索分位数嵌入是否能提供更具表达力的分布表示。

Method: 提出核分位数嵌入（KQE），基于此构造距离度量家族，满足概率度量性质，且计算效率接近线性。通过假设检验验证其性能。

Result: KQE距离在较弱核条件下仍是概率度量，能恢复切片Wasserstein距离的核化形式，计算高效，且假设测试表现优于MMD及其快速近似。

Conclusion: KQE为分布表示和距离度量提供了更灵活高效的框架，是MMD的有力替代方案。

Abstract: Embedding probability distributions into reproducing kernel Hilbert spaces
(RKHS) has enabled powerful nonparametric methods such as the maximum mean
discrepancy (MMD), a statistical distance with strong theoretical and
computational properties. At its core, the MMD relies on kernel mean embeddings
to represent distributions as mean functions in RKHS. However, it remains
unclear if the mean function is the only meaningful RKHS representation.
Inspired by generalised quantiles, we introduce the notion of kernel quantile
embeddings (KQEs). We then use KQEs to construct a family of distances that:
(i) are probability metrics under weaker kernel conditions than MMD; (ii)
recover a kernelised form of the sliced Wasserstein distance; and (iii) can be
efficiently estimated with near-linear cost. Through hypothesis testing, we
show that these distances offer a competitive alternative to MMD and its fast
approximations.

</details>


### [345] [Learning with Expected Signatures: Theory and Applications](https://arxiv.org/abs/2505.20465)
*Lorenzo Lucchese,Mikko S. Pakkanen,Almut E. D. Veraart*

Key words: 期望签名, 时间序列, 鞅过程, 降维, 机器学习

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文研究了期望签名（expected signature）在时间序列数据中的降维表示特性，并证明了其离散时间估计量与理论连续时间值之间的收敛性，提出了针对鞅过程的改进估计方法，以降低均方误差并提升预测性能。

Motivation: 期望签名能够将数据流降维表示并完全刻画数据生成分布，但离散时间估计量与理论连续时间值的收敛性尚未明确。论文旨在填补这一理论空白，并在鞅过程中优化估计方法。

Method: 通过数学证明构建离散时间估计量与连续时间理论值的收敛性，并提出针对鞅过程的改进估计器，通过实验验证其有效性和性能提升。

Result: 论文证明了期望签名的收敛性，改进的估计器在鞅过程中显著降低均方误差，实验显示其能有效提升预测性能。

Conclusion: 期望签名的理论收敛性为基于签名的机器学习方法提供了更完整的概率解释，改进的估计器进一步提升了模型性能。

Abstract: The expected signature maps a collection of data streams to a lower
dimensional representation, with a remarkable property: the resulting feature
tensor can fully characterize the data generating distribution. This
"model-free" embedding has been successfully leveraged to build multiple
domain-agnostic machine learning (ML) algorithms for time series and sequential
data. The convergence results proved in this paper bridge the gap between the
expected signature's empirical discrete-time estimator and its theoretical
continuous-time value, allowing for a more complete probabilistic
interpretation of expected signature-based ML methods. Moreover, when the data
generating process is a martingale, we suggest a simple modification of the
expected signature estimator with significantly lower mean squared error and
empirically demonstrate how it can be effectively applied to improve predictive
performance.

</details>


### [346] [Covariate-Adjusted Deep Causal Learning for Heterogeneous Panel Data Models](https://arxiv.org/abs/2505.20536)
*Guanhao Zhou,Yuefeng Han,Xiufan Yu*

Key words: Causal Inference, Panel Data, Heterogeneous Treatment Effects, Neural Networks, Autoencoder

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 这篇论文提出了一种名为CoDEAL的新方法，用于在因果面板数据模型中估计异质性处理效应，结合了神经网络和自编码器技术来处理协变量效应和面板数据的复杂结构。

Motivation: 研究动机在于解决因果面板数据模型中异质性处理效应和非线性协变量效应的复杂性问题，以提高反事实结果的估计准确性。

Method: 提出了一种名为CoDEAL的方法，结合了前馈神经网络（处理非线性协变量效应）和多输出自编码器（捕捉面板数据的横截面和时间依赖性），并通过定制化的矩阵补全算法处理缺失的反事实结果。

Result: 通过理论证明和实验验证，CoDEAL方法在估计反事实结果时展现了优越性能，并在模拟研究和实际数据应用中表现突出。

Conclusion: CoDEAL方法通过灵活处理协变量效应和面板数据中的异质性与非线性结构，有效提高了因果效应估计的准确性和可解释性。

Abstract: This paper studies the task of estimating heterogeneous treatment effects in
causal panel data models, in the presence of covariate effects. We propose a
novel Covariate-Adjusted Deep Causal Learning (CoDEAL) for panel data models,
that employs flexible model structures and powerful neural network
architectures to cohesively deal with the underlying heterogeneity and
nonlinearity of both panel units and covariate effects. The proposed CoDEAL
integrates nonlinear covariate effect components (parameterized by a
feed-forward neural network) with nonlinear factor structures (modeled by a
multi-output autoencoder) to form a heterogeneous causal panel model. The
nonlinear covariate component offers a flexible framework for capturing the
complex influences of covariates on outcomes. The nonlinear factor analysis
enables CoDEAL to effectively capture both cross-sectional and temporal
dependencies inherent in the data panel. This latent structural information is
subsequently integrated into a customized matrix completion algorithm, thereby
facilitating more accurate imputation of missing counterfactual outcomes.
Moreover, the use of a multi-output autoencoder explicitly accounts for
heterogeneity across units and enhances the model interpretability of the
latent factors. We establish theoretical guarantees on the convergence of the
estimated counterfactuals, and demonstrate the compelling performance of the
proposed method using extensive simulation studies and a real data application.

</details>


### [347] [Balancing Performance and Costs in Best Arm Identification](https://arxiv.org/abs/2505.20583)
*Michael O. Harding,Kirthevasan Kandasamy*

Key words: 多臂老虎机, 最佳臂识别, 风险最小化, DBCARE算法

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了一种新的多臂老虎机模型方法，通过最小化风险函数来平衡推荐臂的性能和学习成本，优于传统的固定预算或置信度设置。

Motivation: 现有最佳臂识别方法中，如何选择预算或置信度参数对实践者来说仍不明确，需要一种更贴合实际需求的新框架。

Method: 提出DBCARE算法，通过最小化性能惩罚和观察成本的总和，理论证明其风险下界并实现接近最优表现。

Result: DBCARE在模拟实验中优于传统固定预算或置信度算法，验证了新框架的有效性。

Conclusion: 新框架更贴合实践需求，DBCARE算法在理论和实验中均表现优异。

Abstract: We consider the problem of identifying the best arm in a multi-armed bandit
model. Despite a wealth of literature in the traditional fixed budget and fixed
confidence regimes of the best arm identification problem, it still remains a
mystery to most practitioners as to how to choose an approach and corresponding
budget or confidence parameter. We propose a new formalism to avoid this
dilemma altogether by minimizing a risk functional which explicitly balances
the performance of the recommended arm and the cost incurred by learning this
arm. In this framework, a cost is incurred for each observation during the
sampling phase, and upon recommending an arm, a performance penalty is incurred
for identifying a suboptimal arm. The learner's goal is to minimize the sum of
the penalty and cost. This new regime mirrors the priorities of many
practitioners, e.g. maximizing profit in an A/B testing framework, better than
classical fixed budget or confidence settings. We derive theoretical lower
bounds for the risk of each of two choices for the performance penalty, the
probability of misidentification and the simple regret, and propose an
algorithm called DBCARE to match these lower bounds up to polylog factors on
nearly all problem instances. We then demonstrate the performance of DBCARE on
a number of simulated models, comparing to fixed budget and confidence
algorithms to show the shortfalls of existing BAI paradigms on this problem.

</details>


### [348] [Moment Expansions of the Energy Distance](https://arxiv.org/abs/2505.20647)
*Ian Langmore*

Key words: 能量距离, 均值差异, 协方差差异, 维度, 各向同性

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文研究了能量距离在分布接近时的敏感性，发现其对均值差异的敏感性高于协方差差异，且与维度无关。

Motivation: 探讨能量距离在实际应用中对不同矩的敏感性，尤其是在分布接近时的表现。

Method: 理论分析和数值实验相结合，研究了能量距离对均值差异和协方差差异的敏感性。

Result: 能量距离对均值差异更敏感，而对非对角协方差项的敏感性依赖于维度，且在接近各向同性时影响较小。

Conclusion: 能量距离在分布接近时主要反映均值差异，协方差差异的影响较小且维度相关性显著。

Abstract: The energy distance is used to test distributional equality, and as a loss
function in machine learning. While $D^2(X, Y)=0$ only when $X\sim Y$, the
sensitivity to different moments is of practical importance. This work
considers $D^2(X, Y)$ in the case where the distributions are close. In this
regime, $D^2(X, Y)$ is more sensitive to differences in the means
$\bar{X}-\bar{Y}$, than differences in the covariances $\Delta$. This is due to
the structure of the energy distance and is independent of dimension. The
sensitivity to on versus off diagonal components of $\Delta$ is examined when
$X$ and $Y$ are close to isotropic. Here a dimension dependent averaging occurs
and, in many cases, off diagonal correlations contribute significantly less.
Numerical results verify these relationships hold even when distributional
assumptions are not strictly met.

</details>


### [349] [A False Discovery Rate Control Method Using a Fully Connected Hidden Markov Random Field for Neuroimaging Data](https://arxiv.org/abs/2505.20688)
*Taehyo Kim,Qiran Jia,Mony J. de Leon,Hai Shu*

Key words: FDR控制, 神经影像分析, 隐马尔可夫随机场, LIS测试, 计算可扩展性

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了fcHMRF-LIS方法，通过结合LIS测试和完全连接的隐马尔可夫随机场（fcHMRF），解决了神经影像数据分析中空间FDR控制的复杂空间依赖性、稳定性和计算可扩展性问题。

Motivation: 传统的FDR控制方法在神经影像数据分析中因假设测试独立性而导致高假非发现率（FNR），且现有空间FDR方法无法同时解决复杂空间依赖性、FDP/FNP的稳定性及计算可扩展性三大挑战。

Method: 提出fcHMRF-LIS方法，结合LIS测试和fcHMRF模型，利用均值场近似、CRF-RNN技术和permutohedral lattice滤波，将计算复杂度从平方降至线性。

Result: 模拟实验表明，fcHMRF-LIS实现了准确的FDR控制、更低FNR、更稳定的FDP/FNP及更多真阳性发现。实际应用中也显著提升了计算效率。

Conclusion: fcHMRF-LIS是一种高效、稳定且可扩展的空间FDR控制方法，适用于高分辨率神经影像数据。

Abstract: False discovery rate (FDR) control methods are essential for voxel-wise
multiple testing in neuroimaging data analysis, where hundreds of thousands or
even millions of tests are conducted to detect brain regions associated with
disease-related changes. Classical FDR control methods (e.g., BH, q-value, and
LocalFDR) assume independence among tests and often lead to high false
non-discovery rates (FNR). Although various spatial FDR control methods have
been developed to improve power, they still fall short in jointly addressing
three major challenges in neuroimaging applications: capturing complex spatial
dependencies, maintaining low variability in both false discovery proportion
(FDP) and false non-discovery proportion (FNP) across replications, and
achieving computational scalability for high-resolution data. To address these
challenges, we propose fcHMRF-LIS, a powerful, stable, and scalable spatial FDR
control method for voxel-wise multiple testing. It integrates the local index
of significance (LIS)-based testing procedure with a novel fully connected
hidden Markov random field (fcHMRF) designed to model complex spatial
structures using a parsimonious parameterization. We develop an efficient
expectation-maximization algorithm incorporating mean-field approximation, the
Conditional Random Fields as Recurrent Neural Networks (CRF-RNN) technique, and
permutohedral lattice filtering, reducing the computational complexity from
quadratic to linear in the number of tests. Extensive simulations demonstrate
that fcHMRF-LIS achieves accurate FDR control, lower FNR, reduced variability
in FDP and FNP, and a higher number of true positives compared to existing
methods. Applied to an FDG-PET dataset from the Alzheimer's Disease
Neuroimaging Initiative, fcHMRF-LIS identifies neurobiologically relevant brain
regions and offers notable advantages in computational efficiency.

</details>


### [350] [Stationary MMD Points for Cubature](https://arxiv.org/abs/2505.20754)
*Zonghao Chen,Toni Karvonen,Heishiro Kanagawa,François-Xavier Briol,Chris. J. Oates*

Key words: 概率分布逼近, 最大均值差异, 静止点, 超收敛, 梯度流

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文探讨了通过有限点集逼近目标概率分布的问题，提出通过最大均值差异（MMD）的静止点而非全局最小值来提高精度，并展示了相关核希尔伯特空间中积分的超收敛性。

Motivation: 为解决概率分布逼近问题中的非凸性挑战，研究者提出关注MMD的静止点而非全局最小值，以更高效地计算并提高精度。

Method: 采用离散梯度流作为实用策略计算MMD的静止点，并提供了详细的收敛分析和有限粒子误差界。

Result: 理论分析显示，对于相关核希尔伯特空间的积分子，静止MMD点的积分误差消失速度比MMD本身更快，具有超收敛特性。

Conclusion: 研究表明，静止MMD点在概率分布逼近中不仅计算可行，还展现出优异的收敛性能，为相关领域提供了新的理论工具。

Abstract: Approximation of a target probability distribution using a finite set of
points is a problem of fundamental importance, arising in cubature, data
compression, and optimisation. Several authors have proposed to select points
by minimising a maximum mean discrepancy (MMD), but the non-convexity of this
objective precludes global minimisation in general. Instead, we consider
\emph{stationary} points of the MMD which, in contrast to points globally
minimising the MMD, can be accurately computed. Our main theoretical
contribution is the (perhaps surprising) result that, for integrands in the
associated reproducing kernel Hilbert space, the cubature error of stationary
MMD points vanishes \emph{faster} than the MMD. Motivated by this
\emph{super-convergence} property, we consider discretised gradient flows as a
practical strategy for computing stationary points of the MMD, presenting a
refined convergence analysis that establishes a novel non-asymptotic
finite-particle error bound, which may be of independent interest.

</details>


### [351] [Input Convex Kolmogorov Arnold Networks](https://arxiv.org/abs/2505.21208)
*Thomas Deschatre,Xavier Warin*

Key words: 输入凸神经网络, Kolmogorov-Arnold网络, 最优输运, 通用逼近定理, 三次样条

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了两种基于Kolmogorov-Arnold网络的输入凸神经网络架构，其中一种基于低阶分段线性函数并提供了通用逼近定理支持，另一种基于三次样条并通过数值结果证明其收敛性。实验表明这些网络在简单测试中与传统输入凸神经网络（ICNNs）表现相当。进一步应用于最优输运问题的求解，证明了其有效性。

Motivation: 研究动机是开发新型输入凸神经网络架构，以克服传统ICNNs在逼近能力和收敛性上的局限性，同时拓展其在优化问题中的应用。

Method: 论文提出了两种ICAN网络：一种是基于低阶分段线性函数的架构，提供了理论上的通用逼近定理支持；另一种是基于三次样条的网络，其收敛性通过数值实验验证。

Result: 实验结果表明，这两种网络在简单测试中与传统ICNNs表现相当。在最优输运问题的应用中，基于三次样条的ICANs与传统ICNNs结果相近。

Conclusion: 论文提出的ICAN架构在理论上和实际应用中均表现出竞争力，为输入凸神经网络提供了新的可能性。

Abstract: This article presents an input convex neural network architecture using
Kolmogorov-Arnold networks (ICKAN). Two specific networks are presented: the
first is based on a low-order, linear-by-part, representation of functions, and
a universal approximation theorem is provided. The second is based on cubic
splines, for which only numerical results support convergence. We demonstrate
on simple tests that these networks perform competitively with classical input
convex neural networks (ICNNs). In a second part, we use the networks to solve
some optimal transport problems needing a convex approximation of functions and
demonstrate their effectiveness. Comparisons with ICNNs show that cubic ICKANs
produce results similar to those of classical ICNNs.

</details>


### [352] [Autoencoding Random Forests](https://arxiv.org/abs/2505.21441)
*Binh Duc Vu,Jan Kapar,Marvin Wright,David S. Watson*

Key words: 随机森林, 自动编码, 非参数统计, 谱图理论, 数据嵌入

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了一种基于随机森林的自动编码方法，通过非参数统计和谱图理论学习数据的低维嵌入，提供了精确和近似的解码方案，并展示了在可视化、压缩、聚类和去噪等方面的应用。

Motivation: 研究动机在于探索一种基于随机森林的自动编码方法，以更好地表示数据中的关系，并提供一种适用于监督或非监督模型的通用解码方法。

Method: 方法结合了约束优化、分割重标记和最近邻回归，通过学习森林中树的划分来建立从嵌入空间到输入空间的映射。

Result: 实验结果表明，该方法在表格、图像和基因组数据等多种场景中表现出色，可用于可视化、压缩、聚类和去噪等任务。

Conclusion: 结论表明该方法在理论和实践中均具有一致性，为数据分析和处理提供了新的工具。

Abstract: We propose a principled method for autoencoding with random forests. Our
strategy builds on foundational results from nonparametric statistics and
spectral graph theory to learn a low-dimensional embedding of the model that
optimally represents relationships in the data. We provide exact and
approximate solutions to the decoding problem via constrained optimization,
split relabeling, and nearest neighbors regression. These methods effectively
invert the compression pipeline, establishing a map from the embedding space
back to the input space using splits learned by the ensemble's constituent
trees. The resulting decoders are universally consistent under common
regularity assumptions. The procedure works with supervised or unsupervised
models, providing a window into conditional or joint distributions. We
demonstrate various applications of this autoencoder, including powerful new
tools for visualization, compression, clustering, and denoising. Experiments
illustrate the ease and utility of our method in a wide range of settings,
including tabular, image, and genomic data.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [353] [Identifying Heart Attack Risk in Vulnerable Population: A Machine Learning Approach](https://arxiv.org/abs/2505.21139)
*Subhagata Chattopadhyay,Amit K Chattopadhyay*

Key words: COVID-19, 心血管事件, 机器学习, 风险因素, 聚类算法

<details>
  <summary>Details</summary>

Main category: q-bio.PE

TL;DR: 该研究采用混合机器学习方法分析流行病学数据，评估13个关键心脏病发作风险因素及其易感性，并利用聚类算法将人群分为‘高风险’和‘非高风险’两组。

Motivation: COVID-19大流行显著增加了40岁以上人群感染后心血管事件（尤其是心肌梗死）的发生率，但潜在机制尚不明确。

Method: 研究结合人口统计学、生化、心电图和铊负荷试验等独特数据集，通过混合机器学习方法和聚类算法评估风险因素并分类人群。

Result: 研究揭示了13个风险因素与心脏病发作概率之间的强关联性，绝经后患者的风险加剧表明雌激素缺乏可能导致个体风险因素进一步恶化。

Conclusion: 该研究强调了传统数据建模难以捕捉的外部压力（如焦虑和恐惧）可能加剧风险因素影响。

Abstract: The COVID-19 pandemic has significantly increased the incidence of
post-infection cardiovascular events, particularly myocardial infarction, in
individuals over 40. While the underlying mechanisms remain elusive, this study
employs a hybrid machine learning approach to analyze epidemiological data in
assessing 13 key heart attack risk factors and their susceptibility. Based on a
unique dataset that combines demographic, biochemical, ECG, and thallium
stress-tests, this study categorizes distinct subpopulations against varying
risk profiles and then divides the population into 'at-risk' (AR) and
'not-at-risk' (NAR) groups using clustering algorithms. The study reveals
strong association between the likelihood of experiencing a heart attack on the
13 risk factors studied. The aggravated risk for postmenopausal patients
indicates compromised individual risk factors due to estrogen depletion that
may be, further compromised by extraneous stress impacts, like anxiety and
fear, aspects that have traditionally eluded data modeling predictions.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [354] [Large Language Model-Powered Decision Support for a Metal Additive Manufacturing Knowledge Graph](https://arxiv.org/abs/2505.20308)
*Muhammad Tayyab Khan,Lequn Chen,Wenhe Feng,Seung Ki Moon*

Key words: 增材制造, 知识图谱, 大型语言模型, 自然语言查询, 制造智能化

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 该论文开发了一个基于知识图谱（KG）和大型语言模型（LLM）的系统，用于实时交互式查询金属增材制造（AM）中的复杂关系，支持自然语言查询和多约束任务。

Motivation: 当前金属增材制造的知识分散且依赖专家级查询，限制了其在设计和规划中的应用。为了解决这一问题，作者开发了一个可查询的知识图谱系统，提升信息的可访问性。

Method: 构建了一个基于Neo4j的知识图谱，涵盖53种金属和合金、9种AM工艺、4种原料类型及后处理需求，并设计了一个LLM接口，通过少量示例提示实现自然语言查询。

Result: 系统支持兼容性检查、多约束筛选和面向增材制造的设计（DfAM）等任务，无需正式查询语法即可提供结构化响应。

Conclusion: 该系统是首个集成了领域特定知识图谱和LLM接口的实时交互式工具，为工程师提供了易用且可解释的决策支持。

Abstract: Metal additive manufacturing (AM) involves complex interdependencies among
processes, materials, feedstock, and post-processing steps. However, the
underlying relationships and domain knowledge remain fragmented across
literature and static databases that often demand expert-level queries,
limiting their applicability in design and planning. To address these gaps, we
develop a novel and queryable knowledge graph (KG) in Neo4j, encoding 53
distinct metals and alloys across seven material families, nine AM processes,
four feedstock types, and associated post-processing requirements. A large
language model (LLM) interface, guided by a few-shot prompting strategy,
enables natural language querying without the need for formal query syntax. The
system supports a range of tasks, including compatibility checks,
multi-constraint filtering, and design for AM (DfAM) guidance. User natural
language queries are normalized, translated into Cypher, and executed over the
KG, with results reformatted into structured responses. This work presents the
first real-time, interactive system that integrates a domain-specific metal AM
KG with an LLM interface, offering accessible, explainable decision support for
engineers and advancing human-centric tools in manufacturing intelligence.

</details>


### [355] [VSCBench: Bridging the Gap in Vision-Language Model Safety Calibration](https://arxiv.org/abs/2505.20362)
*Jiahui Geng,Qing Li,Zongxiong Chen,Yuxia Wang,Derui Zhu,Zhuohan Xie,Chenyang Lyu,Xiuying Chen,Preslav Nakov,Fakhri Karray*

Key words: 视觉语言模型, 安全校准, VSCBench, 欠安全, 过安全

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了安全校准的概念，通过新数据集VSCBench评估11种主流视觉语言模型的安全校准问题，发现存在欠安全和过安全问题，并探讨了四种改进方法，揭示了安全性与实用性的权衡。

Motivation: 现有视觉语言模型的安全对齐主要关注欠安全（模型回应危险查询），而忽略了过安全（模型拒绝安全查询），因此需要系统性解决这两种问题。

Method: 通过构建VSCBench数据集（3600个图像-文本对，视觉或文本相似但安全性不同），评估11种模型的安全校准，并尝试四种改进方法。

Result: 实验揭示了模型在欠安全和过安全方面的重大问题，改进方法虽有效但导致模型实用性下降。

Conclusion: 安全性与实用性存在权衡，需进一步开发高级校准方法，VSCBench为未来研究提供了评估工具。

Abstract: The rapid advancement of vision-language models (VLMs) has brought a lot of
attention to their safety alignment. However, existing methods have primarily
focused on model undersafety, where the model responds to hazardous queries,
while neglecting oversafety, where the model refuses to answer safe queries. In
this paper, we introduce the concept of $\textit{safety calibration}$, which
systematically addresses both undersafety and oversafety. Specifically, we
present $\textbf{VSCBench}$, a novel dataset of 3,600 image-text pairs that are
visually or textually similar but differ in terms of safety, which is designed
to evaluate safety calibration across image-centric and text-centric scenarios.
Based on our benchmark, we evaluate safety calibration across eleven widely
used VLMs. Our extensive experiments revealed major issues with both
undersafety and oversafety. We further investigated four approaches to improve
the model's safety calibration. We found that even though some methods
effectively calibrated the models' safety problems, these methods also lead to
the degradation of models' utility. This trade-off underscores the urgent need
for advanced calibration methods, and our benchmark provides a valuable tool
for evaluating future approaches. Our code and data are available at
https://github.com/jiahuigeng/VSCBench.git.

</details>


### [356] [Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents](https://arxiv.org/abs/2505.20368)
*Jaeyoung Choe,Jihoon Kim,Woohwan Jung*

Key words: 检索增强生成、大型语言模型、金融文档、重复检索、层次检索、证据整理

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 该论文提出了一种名为HiREC的层次检索与证据整理框架，旨在解决传统RAG方法在处理标准化金融文档时因格式相似性导致的重复检索问题，并通过构建LOFin基准验证其效果。

Motivation: 标准化金融文档（如SEC文件）格式相似，导致传统RAG方法误判重复内容，影响检索准确性和完整性，需改进。

Method: 采用层次检索和证据整理框架（HiREC），先检索相关文档，再筛选最相关段落，必要时生成补充查询以获取缺失信息。

Result: 构建了包含145,897份SEC文档和1,595对问答的大规模开放域金融（LOFin）基准，验证了HiREC的有效性。

Conclusion: HiREC框架通过层次检索和证据整理显著提升了金融文档检索的准确性和完整性。

Abstract: Retrieval-augmented generation (RAG) based large language models (LLMs) are
widely used in finance for their excellent performance on knowledge-intensive
tasks. However, standardized documents (e.g., SEC filing) share similar formats
such as repetitive boilerplate texts, and similar table structures. This
similarity forces traditional RAG methods to misidentify near-duplicate text,
leading to duplicate retrieval that undermines accuracy and completeness. To
address these issues, we propose the Hierarchical Retrieval with Evidence
Curation (HiREC) framework. Our approach first performs hierarchical retrieval
to reduce confusion among similar texts. It first retrieve related documents
and then selects the most relevant passages from the documents. The evidence
curation process removes irrelevant passages. When necessary, it automatically
generates complementary queries to collect missing information. To evaluate our
approach, we construct and release a Large-scale Open-domain Financial (LOFin)
question answering benchmark that includes 145,897 SEC documents and 1,595
question-answer pairs. Our code and data are available at
https://github.com/deep-over/LOFin-bench-HiREC.

</details>


### [357] [TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform for Terpenoid Research](https://arxiv.org/abs/2505.20663)
*Xu Kang,Siqi Jiang,Kangwei Xu,Jiahao Li,Ruibo Wu*

Key words: 萜类化合物,知识库,AI问答,RAG框架,跨学科研究

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: TeroSeek是一个专注于萜类化合物的知识库和AI问答工具，结合了20年的文献数据，通过RAG框架提供高质量信息，优于通用大语言模型。

Motivation: 萜类化合物研究跨学科性质导致知识整合困难，需要专门的工具来提升研究效率。

Method: 开发了TeroSeek知识库和AI问答系统，采用RAG框架整合文献数据。

Result: TeroSeek在萜类相关查询上优于通用大语言模型，提供结构化高质量信息。

Conclusion: TeroSeek有效解决了萜类研究的知识整合问题，可作为跨学科研究的专家工具。

Abstract: Terpenoids are a crucial class of natural products that have been studied for
over 150 years, but their interdisciplinary nature (spanning chemistry,
pharmacology, and biology) complicates knowledge integration. To address this,
the authors developed TeroSeek, a curated knowledge base (KB) built from two
decades of terpenoid literature, coupled with an AI-powered question-answering
chatbot and web service. Leveraging a retrieval-augmented generation (RAG)
framework, TeroSeek provides structured, high-quality information and
outperforms general-purpose large language models (LLMs) in terpenoid-related
queries. It serves as a domain-specific expert tool for multidisciplinary
research and is publicly available at http://teroseek.qmclab.com.

</details>


### [358] [What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals](https://arxiv.org/abs/2505.20730)
*Shahrooz Pouryousef*

Key words: 大型语言模型, 矩阵分解, 推荐系统, 检索增强生成, 用户-物品交互

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 本文比较了大型语言模型（LLMs）与经典矩阵分解（MF）模型在利用用户-物品交互数据进行推荐的效果，并提出了检索增强生成（RAG）方法以提升LLMs的表现。

Motivation: 探索LLMs是否能够有效利用用户-物品交互数据中的协作信号进行推荐，并弥补LLMs与MF模型在此方面的差距。

Method: 系统地比较了LLMs和MF模型的表现，并引入了RAG方法来增强LLMs对结构化交互数据的利用能力。

Result: 实验表明，当前LLMs在捕捉协作信号方面不及MF模型，但RAG方法显著提升了LLMs的推荐质量。

Conclusion: RAG方法为基于LLMs的推荐系统提供了一种有前景的方向，未来可通过进一步研究提升LLMs的协作推理能力。

Abstract: User-item interactions contain rich collaborative signals that form the
backbone of many successful recommender systems. While recent work has explored
the use of large language models (LLMs) for recommendation, it remains unclear
whether LLMs can effectively reason over this type of collaborative
information. In this paper, we conduct a systematic comparison between LLMs and
classical matrix factorization (MF) models to assess LLMs' ability to leverage
user-item interaction data. We further introduce a simple retrieval-augmented
generation (RAG) method that enhances LLMs by grounding their predictions in
structured interaction data. Our experiments reveal that current LLMs often
fall short in capturing collaborative patterns inherent to MF models, but that
our RAG-based approach substantially improves recommendation
quality-highlighting a promising direction for future LLM-based recommenders.

</details>


### [359] [Bridging the Gap: Self-Optimized Fine-Tuning for LLM-based Recommender Systems](https://arxiv.org/abs/2505.20771)
*Heng Tang,Feng Liu,Xinbo Chen,Jiawei Chen,Bohao Wang,Changwang Zhang,Jun Wang,Yuegang Sun,Bingde Hu,Can Wang*

Key words: 大语言模型, 推荐系统, 自蒸馏, 课程学习, 自我优化微调

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 提出了一种名为自我优化微调（SOFT）的新方法，结合了上下文学习与监督微调的优势，利用课程学习思想，通过自蒸馏构建辅助数据集，并采用自适应课程调度器逐步学习，显著提升了LLM在推荐系统中的准确性。

Motivation: 现有的大语言模型（LLM）在推荐系统中的两种策略（仅指导与仅调优）无法有效弥合LLM知识空间与推荐任务之间的差距，性能未达预期，因此需要一种更优方法。

Method: SOFT方法结合了两种策略，采用课程学习思想：首先通过自蒸馏构建易学但有意义的辅助数据集，再利用自适应课程调度器从简到难逐步学习。

Result: 实验表明，SOFT显著提升了LLM在推荐系统中的准确性，平均提升了37.59%。

Conclusion: SOFT提供了一种有效结合上下文学习与监督微调的方法，显著优化了LLM在推荐任务中的表现。

Abstract: Recent years have witnessed extensive exploration of Large Language Models
(LLMs) on the field of Recommender Systems (RS). There are currently two
commonly used strategies to enable LLMs to have recommendation capabilities: 1)
The "Guidance-Only" strategy uses in-context learning to exploit and amplify
the inherent semantic understanding and item recommendation capabilities of
LLMs; 2) The "Tuning-Only" strategy uses supervised fine-tuning (SFT) to
fine-tune LLMs with the aim of fitting them to real recommendation data.
However, neither of these strategies can effectively bridge the gap between the
knowledge space of LLMs and recommendation, and their performance do not meet
our expectations.
  To better enable LLMs to learn recommendation knowledge, we combine the
advantages of the above two strategies and proposed a novel "Guidance+Tuning"
method called Self-Optimized Fine-Tuning (SOFT), which adopts the idea of
curriculum learning. It first employs self-distillation to construct an
auxiliary easy-to-learn but meaningful dataset from a fine-tuned LLM. Then it
further utilizes a self-adaptive curriculum scheduler to enable LLMs to
gradually learn from simpler data (self-distilled data) to more challenging
data (real RS data). Extensive experiments demonstrate that SOFT significantly
enhances the recommendation accuracy (37.59\% on average) of LLM-based methods.
The code is available via
https://anonymous.4open.science/r/Self-Optimized-Fine-Tuning-264E

</details>


### [360] [Something's Fishy In The Data Lake: A Critical Re-evaluation of Table Union Search Benchmarks](https://arxiv.org/abs/2505.21329)
*Allaa Boutaleb,Bernd Amann,Hubert Naacke,Rafael Angarita*

Key words: 表格联合搜索, 基准测试, 语义理解, 数据发现

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 当前表格联合搜索（TUS）的基准测试存在局限性，导致简单基线方法表现优异，无法有效评估语义理解。作者提出了未来基准测试的关键准则。

Motivation: 分析现有TUS基准测试的不足，推动更真实、可靠的语义表格联合搜索评估方法。

Method: 通过分析现有基准测试的局限性，提出改进准则。

Result: 发现简单基线方法在现有基准中表现优异，表明当前评估方式未能有效区分语义理解的贡献。

Conclusion: 未来的TUS基准测试需要更严格的设计，以准确反映语义理解的进步。

Abstract: Recent table representation learning and data discovery methods tackle table
union search (TUS) within data lakes, which involves identifying tables that
can be unioned with a given query table to enrich its content. These methods
are commonly evaluated using benchmarks that aim to assess semantic
understanding in real-world TUS tasks. However, our analysis of prominent TUS
benchmarks reveals several limitations that allow simple baselines to perform
surprisingly well, often outperforming more sophisticated approaches. This
suggests that current benchmark scores are heavily influenced by
dataset-specific characteristics and fail to effectively isolate the gains from
semantic understanding. To address this, we propose essential criteria for
future benchmarks to enable a more realistic and reliable evaluation of
progress in semantic table union search.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [361] [Identifying Super Spreaders in Multilayer Networks](https://arxiv.org/abs/2505.20980)
*Michał Czuba,Mateusz Stolarski,Adam Piróg,Piotr Bielak,Piotr Bródka*

Key words: 超级传播者, 多层网络, 图神经网络, 影响力最大化, 信息扩散

<details>
  <summary>Details</summary>

Main category: cs.SI

TL;DR: 提出了TopSpreadersNetwork模型，利用图神经网络识别多层网络中的超级传播者，性能优于传统启发式方法和深度学习模型。

Motivation: 多层网络能更准确地表示复杂关系结构，但现有方法在识别超级传播者（信息传播效率最高的节点）时表现不足。

Method: 构建了首个针对多层网络的信息扩散模拟数据集，将任务建模为基于四维向量的排序预测问题，设计了关系无关的编码器和自定义聚合层。

Result: 在广泛真实和合成的多层网络上评估，TopSpreadersNetwork在识别高影响力节点和可解释性上表现最优。

Conclusion: TopSpreadersNetwork在多层网络中能有效识别超级传播者，且具有通用性和适应性。

Abstract: Identifying super-spreaders can be framed as a subtask of the influence
maximisation problem. It seeks to pinpoint agents within a network that, if
selected as single diffusion seeds, disseminate information most effectively.
Multilayer networks, a specific class of heterogeneous graphs, can capture
diverse types of interactions (e.g., physical-virtual or professional-social),
and thus offer a more accurate representation of complex relational structures.
In this work, we introduce a novel approach to identifying super-spreaders in
such networks by leveraging graph neural networks. To this end, we construct a
dataset by simulating information diffusion across hundreds of networks - to
the best of our knowledge, the first of its kind tailored specifically to
multilayer networks. We further formulate the task as a variation of the
ranking prediction problem based on a four-dimensional vector that quantifies
each agent's spreading potential: (i) the number of activations; (ii) the
duration of the diffusion process; (iii) the peak number of activations; and
(iv) the simulation step at which this peak occurs. Our model,
TopSpreadersNetwork, comprises a relationship-agnostic encoder and a custom
aggregation layer. This design enables generalisation to previously unseen data
and adapts to varying graph sizes. In an extensive evaluation, we compare our
model against classic centrality-based heuristics and competitive deep learning
methods. The results, obtained across a broad spectrum of real-world and
synthetic multilayer networks, demonstrate that TopSpreadersNetwork achieves
superior performance in identifying high-impact nodes, while also offering
improved interpretability through its structured output.

</details>


### [362] [DeSocial: Blockchain-based Decentralized Social Networks](https://arxiv.org/abs/2505.21388)
*Jingyuan Huang,Xi Zhu,Minghao Guo,Yongfeng Zhang*

Key words: 区块链,去中心化社交网络,个性化算法,多节点验证,以太坊

<details>
  <summary>Details</summary>

Main category: cs.SI

TL;DR: DeSocial是一个基于区块链的去中心化社交网络学习框架，允许用户自主选择个性化算法，通过多节点验证提升预测准确性。

Motivation: 解决传统中心化社交平台中用户无法选择算法、个性化受限的问题，利用区块链技术实现用户驱动的模型选择。

Method: 在以太坊本地开发链上部署DeSocial框架，结合分布式数据存储、节点级共识和用户驱动的模型选择（通过Ganache），并采用多数投票聚合预测结果。

Result: 相比五种经典的中心化社交网络学习模型，DeSocial表现显著提升，验证了多节点验证和个性化算法选择的有效性。

Conclusion: DeSocial在区块链去中心化社交网络中增强了用户赋权，展示了多节点验证和个性化算法选择的重要性。

Abstract: Web 2.0 social platforms are inherently centralized, with user data and
algorithmic decisions controlled by the platform. However, users can only
passively receive social predictions without being able to choose the
underlying algorithm, which limits personalization. Fortunately, with the
emergence of blockchain, users are allowed to choose algorithms that are
tailored to their local situation, improving prediction results in a
personalized way. In a blockchain environment, each user possesses its own
model to perform the social prediction, capturing different perspectives on
social interactions. In our work, we propose DeSocial, a decentralized social
network learning framework deployed on an Ethereum (ETH) local development
chain that integrates distributed data storage, node-level consensus, and
user-driven model selection through Ganache. In the first stage, each user
leverages DeSocial to evaluate multiple backbone models on their local
subgraph. DeSocial coordinates the execution and returns model-wise prediction
results, enabling the user to select the most suitable backbone for
personalized social prediction. Then, DeSocial uniformly selects several
validation nodes that possess the algorithm specified by each user, and
aggregates the prediction results by majority voting, to prevent errors caused
by any single model's misjudgment. Extensive experiments show that DeSocial has
an evident improvement compared to the five classical centralized social
network learning models, promoting user empowerment in blockchain-based
decentralized social networks, showing the importance of multi-node validation
and personalized algorithm selection based on blockchain. Our implementation is
available at: https://github.com/agiresearch/DeSocial.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [363] [Stochastic Preconditioning for Neural Field Optimization](https://arxiv.org/abs/2505.20473)
*Selena Ling,Merlin Nimier-David,Alec Jacobson,Nicholas Sharp*

Key words: 神经场,随机预处理,高斯采样,优化收敛,空间模糊

<details>
  <summary>Details</summary>

Main category: cs.GR

TL;DR: 论文提出了一种通过引入空间随机性来提升神经场训练效果的方法，该方法能够替代或超越传统的层次结构和频率空间构造。

Motivation: 当前神经场训练中，复杂的层次结构和频率空间设计虽然有效但实现复杂，因此寻求一种简单且通用的提升方法。

Method: 通过高斯分布的随机偏移采样，隐式地对模糊化的场进行优化，从而提高收敛性和鲁棒性。

Result: 在多种神经场表示和任务中，该方法表现优异，效果接近或优于定制化层次结构。

Conclusion: 这种基于采样的随机预处理方法简单高效，适用于多种神经场场景，显著提升模型性能和鲁棒性。

Abstract: Neural fields are a highly effective representation across visual computing.
This work observes that fitting these fields is greatly improved by
incorporating spatial stochasticity during training, and that this simple
technique can replace or even outperform custom-designed hierarchies and
frequency space constructions. The approach is formalized as implicitly
operating on a blurred version of the field, evaluated in-expectation by
sampling with Gaussian-distributed offsets. Querying the blurred field during
optimization greatly improves convergence and robustness, akin to the role of
preconditioners in numerical linear algebra. This implicit, sampling-based
perspective fits naturally into the neural field paradigm, comes at no
additional cost, and is extremely simple to implement. We describe the basic
theory of this technique, including details such as handling boundary
conditions, and extending to a spatially-varying blur. Experiments demonstrate
this approach on representations including coordinate MLPs, neural hashgrids,
triplanes, and more, across tasks including surface reconstruction and radiance
fields. In settings where custom-designed hierarchies have already been
developed, stochastic preconditioning nearly matches or improves their
performance with a simple and unified approach; in settings without existing
hierarchies it provides an immediate boost to quality and robustness.

</details>


### [364] [Structure from Collision](https://arxiv.org/abs/2505.21335)
*Takuhiro Kaneko*

Key words: 神经3D表示, NeRF, 3D高斯泼溅, 结构估计, 内部结构, SfC-NeRF

<details>
  <summary>Details</summary>

Main category: cs.GR

TL;DR: 提出了一种名为SfC-NeRF的新模型，通过碰撞过程中外观变化来估计物体的不可见内部结构，并利用体积退火技术优化以避免局部最优。

Motivation: 现有神经3D表示方法（如NeRF和3DGS）仅能估计可见外部结构，无法识别表面下不可见的内部结构。为解决这一问题，研究提出Structure from Collision (SfC)任务。

Method: 采用SfC-NeRF模型，通过物理约束、外观保持和关键帧约束优化视频序列中的不可见内部结构，并引入体积退火技术以避免局部最优。

Result: 在115个具有不同结构和材料属性的物体上进行了广泛实验，验证了SfC的特性和SfC-NeRF的有效性。

Conclusion: SfC-NeRF能够有效估计物体的不可见内部结构，并通过体积退火技术提升优化效果。

Abstract: Recent advancements in neural 3D representations, such as neural radiance
fields (NeRF) and 3D Gaussian splatting (3DGS), have enabled the accurate
estimation of 3D structures from multiview images. However, this capability is
limited to estimating the visible external structure, and identifying the
invisible internal structure hidden behind the surface is difficult. To
overcome this limitation, we address a new task called Structure from Collision
(SfC), which aims to estimate the structure (including the invisible internal
structure) of an object from appearance changes during collision. To solve this
problem, we propose a novel model called SfC-NeRF that optimizes the invisible
internal structure of an object through a video sequence under physical,
appearance (i.e., visible external structure)-preserving, and keyframe
constraints. In particular, to avoid falling into undesirable local optima
owing to its ill-posed nature, we propose volume annealing; that is, searching
for global optima by repeatedly reducing and expanding the volume. Extensive
experiments on 115 objects involving diverse structures (i.e., various cavity
shapes, locations, and sizes) and material properties revealed the properties
of SfC and demonstrated the effectiveness of the proposed SfC-NeRF.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [365] [Scattering Networks on Noncommutative Finite Groups](https://arxiv.org/abs/2505.20950)
*Maria Teresa Arias,Davide Barbieri,Eugenio Hernández*

Key words: 散射变换, 有限群, G-CNNs, 小波, 群平移等变性

<details>
  <summary>Details</summary>

Main category: math.NA

TL;DR: 该论文提出了一种在任意有限群上的散射变换，并证明了其在特定条件下具有非扩张性、变形稳定性、能量保持性、群平移等变性等理想性质。

Motivation: 研究动机是扩展散射网络的应用范围，使其不仅限于欧几里得空间，还能在任意有限群（包括非阿贝尔群）上工作，为群等变卷积神经网络（G-CNNs）提供理论基础。

Method: 论文通过在有限群上定义小波，并分析其与经典小波的相似性，提出了散射变换的方法。该方法在特定条件下满足非扩张性、变形稳定性等性质。

Result: 结果表明，散射变换具备非扩张性、变形稳定性、能量保持性、群平移等变性等特点，且随着深度增加，散射系数对群平移的敏感性降低。应用示例展示了该方法在阿贝尔群和非阿贝尔群数据分类中的有效性。

Conclusion: 论文证明了在有限群上的散射变换具有理想的性质，为G-CNNs提供了新的理论工具，并在实际应用中展示了其潜力。

Abstract: Scattering Networks were initially designed to elucidate the behavior of
early layers in Convolutional Neural Networks (CNNs) over Euclidean spaces and
are grounded in wavelets. In this work, we introduce a scattering transform on
an arbitrary finite group (not necessarily abelian) within the context of
group-equivariant convolutional neural networks (G-CNNs). We present wavelets
on finite groups and analyze their similarity to classical wavelets. We
demonstrate that, under certain conditions in the wavelet coefficients, the
scattering transform is non-expansive, stable under deformations, preserves
energy, equivariant with respect to left and right group translations, and, as
depth increases, the scattering coefficients are less sensitive to group
translations of the signal, all desirable properties of convolutional neural
networks. Furthermore, we provide examples illustrating the application of the
scattering transform to classify data with domains involving abelian and
nonabelian groups.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [366] [Vision-Based Risk Aware Emergency Landing for UAVs in Complex Urban Environments](https://arxiv.org/abs/2505.20423)
*Julio de la Torre-Vanegas,Miguel Soriano-Garcia,Israel Becerra,Diego Mercado-Ravell*

Key words: 无人机, 安全着陆, 语义分割, 风险感知, 城市环境

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 论文提出了一种基于风险感知的无人机安全着陆方法，通过语义分割和风险地图算法在复杂城市环境中动态识别安全着陆区，实验验证成功率超过90%。

Motivation: 解决无人机在紧急情况下密集城市环境中的安全着陆问题，尤其是动态障碍物和视觉干扰的挑战。

Method: 使用深度学习网络进行像素级风险值分配，结合风险地图算法动态识别安全着陆区，并通过控制系统引导无人机降落。

Result: 实验表明，该方法在复杂城市场景中着陆成功率超过90%，显著提升了风险指标。

Conclusion: 风险导向的视觉方法能有效降低紧急着陆事故风险，尤其适用于动态障碍密集的复杂城市环境。

Abstract: Landing safely in crowded urban environments remains an essential yet
challenging endeavor for Unmanned Aerial Vehicles (UAVs), especially in
emergency situations. In this work, we propose a risk-aware approach that
harnesses semantic segmentation to continuously evaluate potential hazards in
the drone's field of view. By using a specialized deep neural network to assign
pixel-level risk values and applying an algorithm based on risk maps, our
method adaptively identifies a stable Safe Landing Zone (SLZ) despite moving
critical obstacles such as vehicles, people, etc., and other visual challenges
like shifting illumination. A control system then guides the UAV toward this
low-risk region, employing altitude-dependent safety thresholds and temporal
landing point stabilization to ensure robust descent trajectories. Experimental
validation in diverse urban environments demonstrates the effectiveness of our
approach, achieving over 90% landing success rates in very challenging real
scenarios, showing significant improvements in various risk metrics. Our
findings suggest that risk-oriented vision methods can effectively help reduce
the risk of accidents in emergency landing situations, particularly in complex,
unstructured, urban scenarios, densely populated with moving risky obstacles,
while potentiating the true capabilities of UAVs in complex urban operations.

</details>


### [367] [Robot Operation of Home Appliances by Reading User Manuals](https://arxiv.org/abs/2505.20424)
*Jian Zhang,Hanbo Zhang,Anxing Xiao,David Hsu*

Key words: 家用机器人、操作家电、用户手册、视觉语言模型、结构化模型

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: ApBot是一种通过阅读用户手册来操作新型家用电器的机器人系统，通过构建结构化符号模型并结合视觉反馈，显著提高了任务成功率。

Motivation: 家用电器是家庭中最常见的工具，操作这些电器是辅助家庭机器人的重要能力。现有方法在处理非结构化文本和视觉落地时存在挑战，ApBot旨在解决这些问题。

Method: ApBot利用大型视觉语言模型（VLM）从用户手册中构建结构化符号模型，并通过视觉反馈来落地和执行策略。

Result: 实验表明，ApBot在模拟和现实世界中的多种电器上，任务成功率显著优于直接使用现有大型VLMs的方法。

Conclusion: 结构化内部表征在机器人操作复杂家用电器时具有重要作用，ApBot的成功验证了这一点。

Abstract: Operating home appliances, among the most common tools in every household, is
a critical capability for assistive home robots. This paper presents ApBot, a
robot system that operates novel household appliances by "reading" their user
manuals. ApBot faces multiple challenges: (i) infer goal-conditioned partial
policies from their unstructured, textual descriptions in a user manual
document, (ii) ground the policies to the appliance in the physical world, and
(iii) execute the policies reliably over potentially many steps, despite
compounding errors. To tackle these challenges, ApBot constructs a structured,
symbolic model of an appliance from its manual, with the help of a large
vision-language model (VLM). It grounds the symbolic actions visually to
control panel elements. Finally, ApBot closes the loop by updating the model
based on visual feedback. Our experiments show that across a wide range of
simulated and real-world appliances, ApBot achieves consistent and
statistically significant improvements in task success rate, compared with
state-of-the-art large VLMs used directly as control policies. These results
suggest that a structured internal representations plays an important role in
robust robot operation of home appliances, especially, complex ones.

</details>


### [368] [Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review](https://arxiv.org/abs/2505.20503)
*Matthew Lisondra,Beno Benhabib,Goldie Nejat*

Key words: 基础模型,移动服务机器人,具身AI,多模态传感器融合,人机交互

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 这篇论文系统综述了基础模型在移动服务机器人中的集成，探讨了其在多模态传感器融合、实时决策、任务泛化和人机交互中的潜在应用，并提出了未来研究方向。

Motivation: 通过结合基础模型与具身AI，提升机器人在动态现实环境中的理解和任务执行能力，克服现有技术在多模态传感器融合和实时决策等方面的挑战。

Method: 采用系统综述方法，分析基础模型在移动服务机器人中的应用，探讨其在传感器融合、语言条件控制和自适应任务执行中的角色。

Result: 展示了基础模型在家庭辅助、医疗和服务自动化等领域的实际应用，证实其对服务机器人领域的变革性影响。

Conclusion: 基础模型有望显著提升服务机器人的性能和适应性，未来需进一步研究预测规模法则、自主长期适应和跨具身泛化以实现高效部署。

Abstract: Rapid advancements in foundation models, including Large Language Models,
Vision-Language Models, Multimodal Large Language Models, and
Vision-Language-Action Models have opened new avenues for embodied AI in mobile
service robotics. By combining foundation models with the principles of
embodied AI, where intelligent systems perceive, reason, and act through
physical interactions, robots can improve understanding, adapt to, and execute
complex tasks in dynamic real-world environments. However, embodied AI in
mobile service robots continues to face key challenges, including multimodal
sensor fusion, real-time decision-making under uncertainty, task
generalization, and effective human-robot interactions (HRI). In this paper, we
present the first systematic review of the integration of foundation models in
mobile service robotics, identifying key open challenges in embodied AI and
examining how foundation models can address them. Namely, we explore the role
of such models in enabling real-time sensor fusion, language-conditioned
control, and adaptive task execution. Furthermore, we discuss real-world
applications in the domestic assistance, healthcare, and service automation
sectors, demonstrating the transformative impact of foundation models on
service robotics. We also include potential future research directions,
emphasizing the need for predictive scaling laws, autonomous long-term
adaptation, and cross-embodiment generalization to enable scalable, efficient,
and robust deployment of foundation models in human-centric robotic systems.

</details>


### [369] [Collision- and Reachability-Aware Multi-Robot Control with Grounded LLM Planners](https://arxiv.org/abs/2505.20573)
*Jiabao Ji,Yongchao Chen,Yang Zhang,Ramana Rao Kompella,Chuchu Fan,Gaowen Liu,Shiyu Chang*

Key words: 大语言模型, 机器人控制, 强化学习, 物理约束, RLVR

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 论文提出了融合强化学习与可验证奖励（RLVR）的新框架，通过物理约束引导小规模LLM在控制任务中生成有效计划，显著提升性能。

Motivation: 现有LLM在机器人控制任务中常因缺乏物理约束意识生成无效计划，限制了实际应用。

Method: 提出RLVR框架，通过奖励机制促使LLM在规划过程中考虑物理约束，仅对有效计划给予正向奖励。

Result: 在BoxNet和BoxNet3D环境中，约束敏感的小型LLM表现优于无约束的大型模型。

Conclusion: 通过物理约束引导小型LLM可实现高效、可扩展的多机器人控制。

Abstract: Large language models (LLMs) have demonstrated strong performance in various
robot control tasks. However, their deployment in real-world applications
remains constrained. Even state-ofthe-art LLMs, such as GPT-o4mini, frequently
produce invalid action plans that violate physical constraints, such as
directing a robot to an unreachable location or causing collisions between
robots. This issue primarily arises from a lack of awareness of these physical
constraints during the reasoning process. To address this issue, we propose a
novel framework that integrates reinforcement learning with verifiable rewards
(RLVR) to incentivize knowledge of physical constraints into LLMs to induce
constraints-aware reasoning during plan generation. In this approach, only
valid action plans that successfully complete a control task receive positive
rewards. We applied our method to two small-scale LLMs: a non-reasoning
Qwen2.5-3B-Instruct and a reasoning Qwen3-4B. The experiment results
demonstrate that constraint-aware small LLMs largely outperform large-scale
models without constraints, grounded on both the BoxNet task and a newly
developed BoxNet3D environment built using MuJoCo. This work highlights the
effectiveness of grounding even small LLMs with physical constraints to enable
scalable and efficient multi-robot control in complex, physically constrained
environments.

</details>


### [370] [Interactive OT Gym: A Reinforcement Learning-Based Interactive Optical tweezer (OT)-Driven Microrobotics Simulation Platform](https://arxiv.org/abs/2505.20751)
*Zongcai Tan amd Dandan Zhang*

Key words: 光学镊子，强化学习，微机器人，共享控制，触觉反馈

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 利用强化学习（RL）构建的光学镊子模拟平台Interactive OT Gym，结合触觉反馈和共享控制策略，显著提升了复杂形状微机器人在动态环境中的协同操作性能，任务完成时间减少约67%，成功率100%。

Motivation: 传统多陷阱光学镊子在动态环境中协同操控多个复杂形状微机器人存在挑战，需开发高效、低成本且高保真的仿真平台以优化控制策略。

Method: 提出Interactive OT Gym平台，融合物理场仿真、触觉反馈接口、RL模块及情境感知共享控制策略，支持手动与自主控制的动态切换。

Result: 在细胞操控任务中，共享控制系统将任务时间缩短67%（相比纯人工或RL控制），成功率100%，且平台具有高保真、交互性和低成本特性。

Conclusion: Interactive OT Gym为光学镊子驱动微操作系统提供了高效训练与测试环境，推动了交互式控制算法的发展。

Abstract: Optical tweezers (OT) offer unparalleled capabilities for micromanipulation
with submicron precision in biomedical applications. However, controlling
conventional multi-trap OT to achieve cooperative manipulation of multiple
complex-shaped microrobots in dynamic environments poses a significant
challenge. To address this, we introduce Interactive OT Gym, a reinforcement
learning (RL)-based simulation platform designed for OT-driven microrobotics.
Our platform supports complex physical field simulations and integrates haptic
feedback interfaces, RL modules, and context-aware shared control strategies
tailored for OT-driven microrobot in cooperative biological object manipulation
tasks. This integration allows for an adaptive blend of manual and autonomous
control, enabling seamless transitions between human input and autonomous
operation. We evaluated the effectiveness of our platform using a cell
manipulation task. Experimental results show that our shared control system
significantly improves micromanipulation performance, reducing task completion
time by approximately 67% compared to using pure human or RL control alone and
achieving a 100% success rate. With its high fidelity, interactivity, low cost,
and high-speed simulation capabilities, Interactive OT Gym serves as a
user-friendly training and testing environment for the development of advanced
interactive OT-driven micromanipulation systems and control algorithms. For
more details on the project, please see our website
https://sites.google.com/view/otgym

</details>


### [371] [FM-Planner: Foundation Model Guided Path Planning for Autonomous Drone Navigation](https://arxiv.org/abs/2505.20783)
*Jiaping Xiao,Cheng Wen Tsao,Yuhang Zhang,Mir Feroskhan*

Key words: 路径规划、无人机、基础模型、LLM、VLM

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 这篇论文提出了基于基础模型的路径规划器（FM-Planner），并通过标准化仿真和实际实验验证其在无人机路径规划中的有效性。

Motivation: 基础模型（如LLM和VLM）在机器人领域的感知和智能决策中展现出潜力，但其在全局路径规划中的实际应用效果尚未充分探索。

Method: 论文首先评估了八种代表性的LLM和VLM方法，然后设计了一个结合语义推理和视觉感知的集成规划器，并通过实际场景验证。

Result: 研究揭示了基础模型在无人机路径规划中的优势、局限性以及实际部署的可行性。

Conclusion: FM-Planner为无人机自主飞行提供了实用的实现方案，推动了基础模型在机器人领域的应用。

Abstract: Path planning is a critical component in autonomous drone operations,
enabling safe and efficient navigation through complex environments. Recent
advances in foundation models, particularly large language models (LLMs) and
vision-language models (VLMs), have opened new opportunities for enhanced
perception and intelligent decision-making in robotics. However, their
practical applicability and effectiveness in global path planning remain
relatively unexplored. This paper proposes foundation model-guided path
planners (FM-Planner) and presents a comprehensive benchmarking study and
practical validation for drone path planning. Specifically, we first
systematically evaluate eight representative LLM and VLM approaches using
standardized simulation scenarios. To enable effective real-time navigation, we
then design an integrated LLM-Vision planner that combines semantic reasoning
with visual perception. Furthermore, we deploy and validate the proposed path
planner through real-world experiments under multiple configurations. Our
findings provide valuable insights into the strengths, limitations, and
feasibility of deploying foundation models in real-world drone applications and
providing practical implementations in autonomous flight. Project site:
https://github.com/NTU-ICG/FM-Planner.

</details>


### [372] [STITCH-OPE: Trajectory Stitching with Guided Diffusion for Off-Policy Evaluation](https://arxiv.org/abs/2505.20781)
*Hossein Goli,Michael Gimelfarb,Nathan Samuel de Lara,Haruki Nishimura,Masha Itkina,Florian Shkurti*

Key words: 离策略评估,扩散模型,高维状态空间,长时域问题,合成轨迹生成

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: STITCH-OPE是一个基于扩散模型的生成框架，通过去噪扩散方法解决高维长时域问题中的离策略评估（OPE）挑战，显著降低了方差并提升了性能。

Motivation: 由于现有OPE方法在高维长时域问题中存在方差爆炸或动态模型误差累积的问题，需要一种更有效的评估方法。

Method: 利用行为数据预训练扩散模型，通过目标策略的评分函数引导去噪过程生成合成轨迹，并引入行为策略评分减法和轨迹拼接技术。

Result: 在D4RL和OpenAI Gym基准测试中，STITCH-OPE在均方误差、相关性和遗憾指标上均优于现有方法。

Conclusion: STITCH-OPE通过技术革新显著提升了高维长时域OPE的性能，为成本高或安全性要求高的领域提供了有效解决方案。

Abstract: Off-policy evaluation (OPE) estimates the performance of a target policy
using offline data collected from a behavior policy, and is crucial in domains
such as robotics or healthcare where direct interaction with the environment is
costly or unsafe. Existing OPE methods are ineffective for high-dimensional,
long-horizon problems, due to exponential blow-ups in variance from importance
weighting or compounding errors from learned dynamics models. To address these
challenges, we propose STITCH-OPE, a model-based generative framework that
leverages denoising diffusion for long-horizon OPE in high-dimensional state
and action spaces. Starting with a diffusion model pre-trained on the behavior
data, STITCH-OPE generates synthetic trajectories from the target policy by
guiding the denoising process using the score function of the target policy.
STITCH-OPE proposes two technical innovations that make it advantageous for
OPE: (1) prevents over-regularization by subtracting the score of the behavior
policy during guidance, and (2) generates long-horizon trajectories by
stitching partial trajectories together end-to-end. We provide a theoretical
guarantee that under mild assumptions, these modifications result in an
exponential reduction in variance versus long-horizon trajectory diffusion.
Experiments on the D4RL and OpenAI Gym benchmarks show substantial improvement
in mean squared error, correlation, and regret metrics compared to
state-of-the-art OPE methods.

</details>


### [373] [Hume: Introducing System-2 Thinking in Visual-Language-Action Model](https://arxiv.org/abs/2505.21432)
*Haoming Song,Delin Qu,Yuanqi Yao,Qizhi Chen,Qi Lv,Yiwen Tang,Modi Shi,Guanghui Ren,Maoqing Yao,Bin Zhao,Dong Wang,Xuelong Li*

Key words: Hume, 机器人控制, 视觉-语言-动作模型, 慢思维, 级联动作降噪

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 该论文提出了Hume，一种具有价值引导系统2思维和级联动作降噪的双系统视觉-语言-动作模型，旨在探索机器人基础模型在物理世界中的类人思维能力。

Motivation: 探索慢思维范式在机器人基础模型中的潜力，以提升其在物理世界中的复杂任务处理能力。

Method: Hume采用双系统架构，系统2通过价值查询头估计预测动作的状态-动作价值，系统1作为轻量级反应式视觉运动策略执行级联动作降噪。

Result: Hume在多个仿真基准和实际机器人部署中超越了现有的视觉-语言-动作模型。

Conclusion: Hume通过价值引导思维和级联动作降噪，显著提升了机器人控制的灵活性和性能。

Abstract: Humans practice slow thinking before performing actual actions when handling
complex tasks in the physical world. This thinking paradigm, recently, has
achieved remarkable advancement in boosting Large Language Models (LLMs) to
solve complex tasks in digital domains. However, the potential of slow thinking
remains largely unexplored for robotic foundation models interacting with the
physical world. In this work, we propose Hume: a dual-system
Vision-Language-Action (VLA) model with value-guided System-2 thinking and
cascaded action denoising, exploring human-like thinking capabilities of
Vision-Language-Action models for dexterous robot control. System 2 of Hume
implements value-Guided thinking by extending a Vision-Language-Action Model
backbone with a novel value-query head to estimate the state-action value of
predicted actions. The value-guided thinking is conducted by repeat sampling
multiple action candidates and selecting one according to state-action value.
System 1 of Hume is a lightweight reactive visuomotor policy that takes System
2 selected action and performs cascaded action denoising for dexterous robot
control. At deployment time, System 2 performs value-guided thinking at a low
frequency while System 1 asynchronously receives the System 2 selected action
candidate and predicts fluid actions in real time. We show that Hume
outperforms the existing state-of-the-art Vision-Language-Action models across
multiple simulation benchmark and real-robot deployments.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [374] [Optimizing fMRI Data Acquisition for Decoding Natural Speech with Limited Participants](https://arxiv.org/abs/2505.21304)
*Louis Jalouzot,Alexis Thual,Yair Lakretz,Christophe Pallier,Bertrand Thirion*

Key words: fMRI, 自然语音解码, 深度学习, 多参与者训练, 句法特征

<details>
  <summary>Details</summary>

Main category: q-bio.NC

TL;DR: 该研究探讨了从少量参与者的fMRI数据中解码自然语音的最优策略。研究发现，在多参与者训练中，解码准确率并未超过单参与者方法，且刺激内容对解码影响不大。解码器更擅长建模句法特征而非语义特征。

Motivation: 研究旨在优化自然语音的fMRI解码策略，探索多参与者数据利用的潜力及其限制。

Method: 使用深度学习模型预测从fMRI活动中提取的LLM文本表示，比较单参与者和多参与者训练效果，并分析不同刺激内容的影响。

Result: 多参与者训练未提升解码准确率；解码器对句法特征建模更优，复杂语法或丰富语义内容增加解码难度。

Conclusion: 研究表明，深度表型（每位参与者大量数据）对解码更有效，多参与者数据利用需更大规模或更深表型支持。

Abstract: We investigate optimal strategies for decoding perceived natural speech from
fMRI data acquired from a limited number of participants. Leveraging Lebel et
al. (2023)'s dataset of 8 participants, we first demonstrate the effectiveness
of training deep neural networks to predict LLM-derived text representations
from fMRI activity. Then, in this data regime, we observe that multi-subject
training does not improve decoding accuracy compared to single-subject
approach. Furthermore, training on similar or different stimuli across subjects
has a negligible effect on decoding accuracy. Finally, we find that our
decoders better model syntactic than semantic features, and that stories
containing sentences with complex syntax or rich semantic content are more
challenging to decode. While our results demonstrate the benefits of having
extensive data per participant (deep phenotyping), they suggest that leveraging
multi-subject for natural speech decoding likely requires deeper phenotyping or
a substantially larger cohort.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [375] [Respond to Change with Constancy: Instruction-tuning with LLM for Non-I.I.D. Network Traffic Classification](https://arxiv.org/abs/2505.20866)
*Xinjie Lin,Gang Xiong,Gaopeng Gou,Wenqi Dong,Jing Yu,Zhen Li,Wei Xia*

Key words: 加密流量分类,大语言模型,自监督学习,零样本分类,分布漂移

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出了名为ETooL的加密流量分类模型，结合大语言模型（LLM）和自监督指令调优，解决了传统方法的分布漂移和标签数据依赖问题，显著提升了分类性能和泛化能力。

Motivation: 现有加密流量分类方法因封闭世界假设和标签数据依赖而面临适应性差和泛化能力弱的问题；LLMs在通用任务中表现出色，但在流量分析领域应用受限。

Method: 提出ETooL模型，通过自监督指令调优将LLMs与流量结构知识结合，构建文本信息与流量交互的联系。

Result: ETooL在监督和零样本任务中表现优异，F1分数显著提升（如APP53 I.I.D.达93.19%）；构建的NETD数据集验证了其动态分布适应性。

Conclusion: ETooL有效解决了加密流量分类的分布漂移和标签稀缺问题，展示了LLMs在该领域的潜力。

Abstract: Encrypted traffic classification is highly challenging in network security
due to the need for extracting robust features from content-agnostic traffic
data. Existing approaches face critical issues: (i) Distribution drift, caused
by reliance on the closedworld assumption, limits adaptability to realworld,
shifting patterns; (ii) Dependence on labeled data restricts applicability
where such data is scarce or unavailable. Large language models (LLMs) have
demonstrated remarkable potential in offering generalizable solutions across a
wide range of tasks, achieving notable success in various specialized fields.
However, their effectiveness in traffic analysis remains constrained by
challenges in adapting to the unique requirements of the traffic domain. In
this paper, we introduce a novel traffic representation model named Encrypted
Traffic Out-of-Distribution Instruction Tuning with LLM (ETooL), which
integrates LLMs with knowledge of traffic structures through a self-supervised
instruction tuning paradigm. This framework establishes connections between
textual information and traffic interactions. ETooL demonstrates more robust
classification performance and superior generalization in both supervised and
zero-shot traffic classification tasks. Notably, it achieves significant
improvements in F1 scores: APP53 (I.I.D.) to 93.19%(6.62%) and 92.11%(4.19%),
APP53 (O.O.D.) to 74.88%(18.17%) and 72.13%(15.15%), and ISCX-Botnet (O.O.D.)
to 95.03%(9.16%) and 81.95%(12.08%). Additionally, we construct NETD, a traffic
dataset designed to support dynamic distributional shifts, and use it to
validate ETooL's effectiveness under varying distributional conditions.
Furthermore, we evaluate the efficiency gains achieved through ETooL's
instruction tuning approach.

</details>


### [376] [Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space](https://arxiv.org/abs/2505.21277)
*Yao Huang,Yitong Sun,Shouwei Ruan,Yichi Zhang,Yinpeng Dong,Xingxing Wei*

Key words: LLMs，越狱攻击，遗传优化，安全性评估

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 提出一种新框架，通过分解越狱策略和基于遗传算法的优化，显著提升LLMs的越狱成功率。

Motivation: 现有方法在安全对齐模型上的成功率有限，且受限于预定义的策略空间，需探索更有效的攻击方式。

Method: 基于ELM理论分解策略，结合遗传优化的意图评估机制。

Result: 在Claude-3.5上取得90%以上成功率，超越现有方法，并展示跨模型迁移能力。

Conclusion: 扩展策略空间能显著提升LLMs的安全性研究价值。

Abstract: Large Language Models (LLMs), despite advanced general capabilities, still
suffer from numerous safety risks, especially jailbreak attacks that bypass
safety protocols. Understanding these vulnerabilities through black-box
jailbreak attacks, which better reflect real-world scenarios, offers critical
insights into model robustness. While existing methods have shown improvements
through various prompt engineering techniques, their success remains limited
against safety-aligned models, overlooking a more fundamental problem: the
effectiveness is inherently bounded by the predefined strategy spaces. However,
expanding this space presents significant challenges in both systematically
capturing essential attack patterns and efficiently navigating the increased
complexity. To better explore the potential of expanding the strategy space, we
address these challenges through a novel framework that decomposes jailbreak
strategies into essential components based on the Elaboration Likelihood Model
(ELM) theory and develops genetic-based optimization with intention evaluation
mechanisms. To be striking, our experiments reveal unprecedented jailbreak
capabilities by expanding the strategy space: we achieve over 90% success rate
on Claude-3.5 where prior methods completely fail, while demonstrating strong
cross-model transferability and surpassing specialized safeguard models in
evaluation accuracy. The code is open-sourced at:
https://github.com/Aries-iai/CL-GSO.

</details>


### [377] [Unveiling Impact of Frequency Components on Membership Inference Attacks for Diffusion Models](https://arxiv.org/abs/2505.20955)
*Puwei Lian,Yujun Cai,Songze Li*

Key words: 扩散模型、会员推断攻击、高频信息、隐私保护、图像生成

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本文提出了一种通用的会员推断攻击(MIA)范式，并指出扩散模型在处理高频信息时的固有缺陷会降低攻击效果，进而提出了一种即插即用的高频过滤器模块以提升攻击性能。

Motivation: 尽管扩散模型在图像生成方面取得了巨大成功，但其隐私和版权问题引发了广泛关注。本文旨在通过改进会员推断攻击(MIA)来更好地识别训练数据。

Method: 作者首先将现有MIA统一为一种通用范式，发现扩散模型处理高频信息的不足会导致分类错误，进而提出了一种高频过滤器模块。

Result: 实验证明，该模块能显著提升基线攻击的性能，且无需额外时间成本。

Conclusion: 高频过滤器模块可以有效弥补扩散模型的缺陷，提高会员推断攻击的准确性。

Abstract: Diffusion models have achieved tremendous success in image generation, but
they also raise significant concerns regarding privacy and copyright issues.
Membership Inference Attacks (MIAs) are designed to ascertain whether specific
data were utilized during a model's training phase. As current MIAs for
diffusion models typically exploit the model's image prediction ability, we
formalize them into a unified general paradigm which computes the membership
score for membership identification. Under this paradigm, we empirically find
that existing attacks overlook the inherent deficiency in how diffusion models
process high-frequency information. Consequently, this deficiency leads to
member data with more high-frequency content being misclassified as hold-out
data, and hold-out data with less high-frequency content tend to be
misclassified as member data. Moreover, we theoretically demonstrate that this
deficiency reduces the membership advantage of attacks, thereby interfering
with the effective discrimination of member data and hold-out data. Based on
this insight, we propose a plug-and-play high-frequency filter module to
mitigate the adverse effects of the deficiency, which can be seamlessly
integrated into any attacks within this general paradigm without additional
time costs. Extensive experiments corroborate that this module significantly
improves the performance of baseline attacks across different datasets and
models.

</details>


### [378] [AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery](https://arxiv.org/abs/2505.21499)
*Haowei Wang,Junjie Wang,Xiaojun Jia,Rupeng Zhang,Mingyang Li,Zhe Liu,Yang Liu,Qing Wang*

Key words: Vision-Language Model, Web Agents, AdInject, 安全漏洞, 黑盒攻击

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: AdInject是一种新颖的黑盒攻击方法，利用互联网广告投放向Web代理注入恶意内容，攻击成功率高达60%-100%，揭示了Web代理安全的重大隐患。

Motivation: 现有对抗性环境注入攻击研究假设不切实际，限制了其实际应用。AdInject旨在在更现实的威胁模型下进行攻击，无需代理模型参数或用户意图知识。

Method: AdInject通过设计恶意广告内容和基于VLM的广告内容优化技术，推断用户意图并增强攻击效果。

Result: 实验显示AdInject攻击成功率在多数场景超过60%，某些情况下接近100%，验证了广告投放作为攻击载体的有效性。

Conclusion: 广告投放是Web代理安全的重大威胁，亟需开发防御机制。

Abstract: Vision-Language Model (VLM) based Web Agents represent a significant step
towards automating complex tasks by simulating human-like interaction with
websites. However, their deployment in uncontrolled web environments introduces
significant security vulnerabilities. Existing research on adversarial
environmental injection attacks often relies on unrealistic assumptions, such
as direct HTML manipulation, knowledge of user intent, or access to agent model
parameters, limiting their practical applicability. In this paper, we propose
AdInject, a novel and real-world black-box attack method that leverages the
internet advertising delivery to inject malicious content into the Web Agent's
environment. AdInject operates under a significantly more realistic threat
model than prior work, assuming a black-box agent, static malicious content
constraints, and no specific knowledge of user intent. AdInject includes
strategies for designing malicious ad content aimed at misleading agents into
clicking, and a VLM-based ad content optimization technique that infers
potential user intents from the target website's context and integrates these
intents into the ad content to make it appear more relevant or critical to the
agent's task, thus enhancing attack effectiveness. Experimental evaluations
demonstrate the effectiveness of AdInject, attack success rates exceeding 60%
in most scenarios and approaching 100% in certain cases. This strongly
demonstrates that prevalent advertising delivery constitutes a potent and
real-world vector for environment injection attacks against Web Agents. This
work highlights a critical vulnerability in Web Agent security arising from
real-world environment manipulation channels, underscoring the urgent need for
developing robust defense mechanisms against such threats. Our code is
available at https://github.com/NicerWang/AdInject.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [379] [Leveraging GANs for citation intent classification and its impact on citation network analysis](https://arxiv.org/abs/2505.21162)
*Davi A. Bezerra,Filipi N. Silva,Diego R. Amancio*

Key words: 引用意图, GAN, 网络中心性, 科学影响, 引用网络

<details>
  <summary>Details</summary>

Main category: cs.DL

TL;DR: 本文采用基于GAN的方法对引用意图进行分类，结果显示其性能接近最先进水平且参数量更少，同时探讨了引用意图对论文在引用网络中中心性的影响。

Motivation: 理解引用意图可以实现对科学影响的更细致解读，有助于更准确地评估学术影响力。

Method: 采用GAN架构结合上下文嵌入进行引用意图分类，并利用未过滤的ArXiv数据集构建引用网络分析中心性。

Result: 提出的方法在分类任务中表现出色，参数量更少；引用意图过滤会显著影响论文在引用网络中的中心性排名，其中介中心性受影响最大。

Conclusion: GAN架构在引用意图分类中高效且有效，引用意图对论文影响力评估具有重要影响，尤其在网络分析中需考虑其作用。

Abstract: Citations play a fundamental role in the scientific ecosystem, serving as a
foundation for tracking the flow of knowledge, acknowledging prior work, and
assessing scholarly influence. In scientometrics, they are also central to the
construction of quantitative indicators. Not all citations, however, serve the
same function: some provide background, others introduce methods, or compare
results. Therefore, understanding citation intent allows for a more nuanced
interpretation of scientific impact. In this paper, we adopted a GAN-based
method to classify citation intents. Our results revealed that the proposed
method achieves competitive classification performance, closely matching
state-of-the-art results with substantially fewer parameters. This demonstrates
the effectiveness and efficiency of leveraging GAN architectures combined with
contextual embeddings in intent classification task. We also investigated
whether filtering citation intents affects the centrality of papers in citation
networks. Analyzing the network constructed from the unArXiv dataset, we found
that paper rankings can be significantly influenced by citation intent. All
four centrality metrics examined- degree, PageRank, closeness, and betweenness
- were sensitive to the filtering of citation types. The betweenness centrality
displayed the greatest sensitivity, showing substantial changes in ranking when
specific citation intents were removed.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [380] [Predictive Performance of Deep Quantum Data Re-uploading Models](https://arxiv.org/abs/2505.20337)
*Xin Wang,Han-Xiao Tao,Re-Bing Wu*

Key words: 量子机器学习, 数据重复上传, 预测性能, 高维数据, 电路架构

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 量子机器学习中的数据重复上传模型在预测性能上存在局限性，特别是当使用深层编码层时，预测性能会逐渐退化至接近随机猜测的水平。研究表明，处理高维数据时，应采用更宽的电路架构而非更深更窄的设计。

Motivation: 尽管量子机器学习中的数据重复上传模型因其高表达性和可训练性受到关注，但其对未见数据的预测性能尚未充分研究。本文旨在探讨深层编码层对预测性能的影响。

Method: 通过理论分析和实验验证，研究了高维数据和有限量子比特数据重复上传模型的预测性能退化问题。实验包括合成线性可分数据集和真实数据集。

Result: 结果显示，当编码层数量增加时，预测性能会显著下降，且数据重复上传无法缓解这一退化。

Conclusion: 处理高维数据时，量子数据重复上传模型应采用更宽的电路架构，而非更深更窄的设计，以避免预测性能退化。

Abstract: Quantum machine learning models incorporating data re-uploading circuits have
garnered significant attention due to their exceptional expressivity and
trainability. However, their ability to generate accurate predictions on unseen
data, referred to as the predictive performance, remains insufficiently
investigated. This study reveals a fundamental limitation in predictive
performance when deep encoding layers are employed within the data re-uploading
model. Concretely, we theoretically demonstrate that when processing
high-dimensional data with limited-qubit data re-uploading models, their
predictive performance progressively degenerates to near random-guessing levels
as the number of encoding layers increases. In this context, the repeated data
uploading cannot mitigate the performance degradation. These findings are
validated through experiments on both synthetic linearly separable datasets and
real-world datasets. Our results demonstrate that when processing
high-dimensional data, the quantum data re-uploading models should be designed
with wider circuit architectures rather than deeper and narrower ones.

</details>


### [381] [Leveraging Diffusion Models for Parameterized Quantum Circuit Generation](https://arxiv.org/abs/2505.20863)
*Daniel Barta,Darya Martyniuk,Johannes Jung,Adrian Paschke*

Key words: 量子计算, 参数化量子电路, 扩散模型, GHZ态, 量子机器学习

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 提出了一种基于去噪扩散模型（DMs）的生成方法，用于合成参数化量子电路（PQCs），通过条件化合成过程同时生成电路架构和连续门参数，展示了在生成高保真GHZ态和量子机器学习分类任务中的高效性。

Motivation: 量子计算的实际成功依赖于量子电路设计的进步，因此需要高效的方法来合成和优化参数化量子电路。

Method: 扩展了F"urrutter等人的扩散模型管道，采用条件化合成过程生成电路架构和门参数。

Result: 实现了在生成高保真GHZ态和QML分类任务中的高精度，表现出良好的通用性和计算效率。

Conclusion: 扩散模型为加速和优化PQCs设计提供了强大工具，支持更实用和可扩展的量子应用开发。

Abstract: Quantum computing holds immense potential, yet its practical success depends
on multiple factors, including advances in quantum circuit design. In this
paper, we introduce a generative approach based on denoising diffusion models
(DMs) to synthesize parameterized quantum circuits (PQCs). Extending the recent
diffusion model pipeline of F\"urrutter et al. [1], our model effectively
conditions the synthesis process, enabling the simultaneous generation of
circuit architectures and their continuous gate parameters. We demonstrate our
approach in synthesizing PQCs optimized for generating high-fidelity
Greenberger-Horne-Zeilinger (GHZ) states and achieving high accuracy in quantum
machine learning (QML) classification tasks. Our results indicate a strong
generalization across varying gate sets and scaling qubit counts, highlighting
the versatility and computational efficiency of diffusion-based methods. This
work illustrates the potential of generative models as a powerful tool for
accelerating and optimizing the design of PQCs, supporting the development of
more practical and scalable quantum applications.

</details>


### [382] [Quantum AIXI: Universal Intelligence via Quantum Information](https://arxiv.org/abs/2505.21170)
*Elija Perrier*

Key words: AIXI, 量子计算, 通用人工智能, Solomonoff归纳, 量子Kolmogorov复杂度

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 论文提出了一种量子版本的AIXI模型（QAIXI），探讨了在量子环境下通用人工智能的理论一致性和可行性。

Motivation: 传统的AIXI模型基于经典计算框架，而现实世界是量子力学的，模拟量子系统需要巨大的计算开销，因此研究量子AIXI模型具有重要意义。

Method: 通过量子信息和经典寄存器及其交互，提出了量子AIXI模型，并扩展了量子Kolmogorov复杂度和QAIXI价值函数等关键组件。

Result: 论文展示了量子AIXI代理可以执行经典和量子动作，并讨论了量子Solomonoff归纳的条件和限制。

Conclusion: 量子AIXI模型在理论上具有一致性，但受到上下文性的根本影响。

Abstract: AIXI is a widely studied model of artificial general intelligence (AGI) based
upon principles of induction and reinforcement learning. However, AIXI is
fundamentally classical in nature - as are the environments in which it is
modelled. Given the universe is quantum mechanical in nature and the
exponential overhead required to simulate quantum mechanical systems
classically, the question arises as to whether there are quantum mechanical
analogues of AIXI which are theoretically consistent or practically feasible as
models of universal intelligence. To address this question, we extend the
framework to quantum information and present Quantum AIXI (QAIXI). We introduce
a model of quantum agent/environment interaction based upon quantum and
classical registers and channels, showing how quantum AIXI agents may take both
classical and quantum actions. We formulate the key components of AIXI in
quantum information terms, extending previous research on quantum Kolmogorov
complexity and a QAIXI value function. We discuss conditions and limitations
upon quantum Solomonoff induction and show how contextuality fundamentally
affects QAIXI models.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [383] [An Artificial Intelligence Model for Early Stage Breast Cancer Detection from Biopsy Images](https://arxiv.org/abs/2505.20332)
*Neil Chaudhary,Zaynah Dhunny*

Key words: 乳腺癌分类, 卷积神经网络, 组织病理图像, 深度学习

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该研究提出了一种基于卷积神经网络（CNN）的人工智能工具，用于通过组织病理活检图像准确区分乳腺肿瘤的良恶性及其亚型分类，减少传统侵入性检测并加速治疗。

Motivation: 传统的乳腺癌类型确诊需要额外的侵入性检测，不仅延迟治疗还增加患者负担，研究旨在通过AI工具提升分类效率并减少这些负面影响。

Method: 采用CNN架构，结合图像预处理（降噪和特征增强），实现对乳腺癌类型的精确分类。

Result: 在多个数据集上的实验表明，该模型在准确率、精确率、召回率和F1分数上优于现有方案。

Conclusion: 深度学习技术在临床诊断中潜力巨大，该工具为乳腺癌分类提供了高效支持。

Abstract: Accurate identification of breast cancer types plays a critical role in
guiding treatment decisions and improving patient outcomes. This paper presents
an artificial intelligence enabled tool designed to aid in the identification
of breast cancer types using histopathological biopsy images. Traditionally
additional tests have to be done on women who are detected with breast cancer
to find out the types of cancer it is to give the necessary cure. Those tests
are not only invasive but also delay the initiation of treatment and increase
patient burden. The proposed model utilizes a convolutional neural network
(CNN) architecture to distinguish between benign and malignant tissues as well
as accurate subclassification of breast cancer types. By preprocessing the
images to reduce noise and enhance features, the model achieves reliable levels
of classification performance. Experimental results on such datasets
demonstrate the model's effectiveness, outperforming several existing solutions
in terms of accuracy, precision, recall, and F1-score. The study emphasizes the
potential of deep learning techniques in clinical diagnostics and offers a
promising tool to assist pathologists in breast cancer classification.

</details>


### [384] [DiffNMR: Advancing Inpainting of Randomly Sampled Nuclear Magnetic Resonance Signals](https://arxiv.org/abs/2505.20367)
*Sen Yan,Fabrizio Gabellieri,Etienne Goffinet,Filippo Castiglione,Thomas Launey*

Key words: 核磁共振, 非均匀采样, 扩散模型, 深度学习, 光谱重建

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该论文探讨了利用深度学习中的扩散模型提高非均匀采样（NUS）核磁共振（NMR）光谱重建质量的方法，并验证了其在时间和频率域数据的有效性。

Motivation: 由于NMR仪器成本高且实验时间长，非均匀采样被广泛用于减少采集时间，但会引入伪影和降低光谱质量，因此需要开发计算技术以优化重建效果。

Method: 使用扩散模型对NUS的时间和频率域数据进行处理，以重建Artina数据集中的挑战性光谱。

Result: 扩散模型成功提高了NUS光谱的重建质量，尤其在时间-频率域表现更优，展现了其在提升NMR效率和准确性上的潜力。

Conclusion: 扩散模型为NMR光谱重建提供了新思路，时间-频率域数据的应用为未来研究开辟了新方向。

Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy leverages nuclear magnetization
to probe molecules' chemical environment, structure, and dynamics, with
applications spanning from pharmaceuticals to the petroleum industry. Despite
its utility, the high cost of NMR instrumentation, operation and the lengthy
duration of experiments necessitate the development of computational techniques
to optimize acquisition times. Non-Uniform sampling (NUS) is widely employed as
a sub-sampling method to address these challenges, but it often introduces
artifacts and degrades spectral quality, offsetting the benefits of reduced
acquisition times. In this work, we propose the use of deep learning techniques
to enhance the reconstruction quality of NUS spectra. Specifically, we explore
the application of diffusion models, a relatively untapped approach in this
domain. Our methodology involves applying diffusion models to both time-time
and time-frequency NUS data, yielding satisfactory reconstructions of
challenging spectra from the benchmark Artina dataset. This approach
demonstrates the potential of diffusion models to improve the efficiency and
accuracy of NMR spectroscopy as well as the superiority of using a
time-frequency domain data over the time-time one, opening new landscapes for
future studies.

</details>


### [385] [Cardiac Digital Twins at Scale from MRI: Open Tools and Representative Models from ~55000 UK Biobank Participants](https://arxiv.org/abs/2505.21019)
*Devran Ugurlu,Shuang Qian,Elliot Fairweather,Charlene Mauger,Bram Ruijsink,Laura Dal Toso,Yu Deng,Marina Strocchi,Reza Razavi,Alistair Young,Pablo Lamata,Steven Niederer,Martin Bishop*

Key words: 心脏病数字孪生,心血管磁共振,网格生成,UK生物银行,开源管道

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该论文介绍了一种开源自动化管道，用于从心血管磁共振图像创建病人特异性的左右心室网格，并应用于英国生物银行的约55,000名参与者，构建了迄今为止最全面的成人心脏模型库。

Motivation: 心脏病数字孪生技术需要精确的病人特异性3D结构模型，但目前缺乏大规模生成的公共数据库。

Method: 使用开源自动化管道从心血管磁共振图像生成心室网格，并在大样本中验证其应用。

Result: 构建了包含1423个代表性网格的库，涵盖性别、BMI和年龄等多样本特征。

Conclusion: 该研究填补了心脏数字孪生领域公共数据集的空白，推动了心血管疾病的精准医学研究。

Abstract: A cardiac digital twin is a virtual replica of a patient's heart for
screening, diagnosis, prognosis, risk assessment, and treatment planning of
cardiovascular diseases. This requires an anatomically accurate
patient-specific 3D structural representation of the heart, suitable for
electro-mechanical simulations or study of disease mechanisms. However,
generation of cardiac digital twins at scale is demanding and there are no
public repositories of models across demographic groups. We describe an
automatic open-source pipeline for creating patient-specific left and right
ventricular meshes from cardiovascular magnetic resonance images, its
application to a large cohort of ~55000 participants from UK Biobank, and the
construction of the most comprehensive cohort of adult heart models to date,
comprising 1423 representative meshes across sex (male, female), body mass
index (range: 16 - 42 kg/m$^2$) and age (range: 49 - 80 years). Our code is
available at https://github.com/cdttk/biv-volumetric-meshing/tree/plos2025 ,
and pre-trained networks, representative volumetric meshes with fibers and UVCs
will be made available soon.

</details>


### [386] [Prostate Cancer Screening with Artificial Intelligence-Enhanced Micro-Ultrasound: A Comparative Study with Traditional Methods](https://arxiv.org/abs/2505.21355)
*Muhammad Imran,Wayne G. Brisbane,Li-Ming Su,Jason P. Joseph,Wei Shao*

Key words: 微型超声, 人工智能, 前列腺癌, 筛查, 活检

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: AI分析微型超声图像在检测前列腺癌中比传统PSA和DRE筛查方法表现更好，提高了特异性并保持高灵敏度，可能减少不必要的活检。

Motivation: 传统的前列腺癌筛查方法如PSA和DRE存在特异性低的问题，可能导致不必要的活检。研究探索AI分析微型超声图像是否能改善这一现状。

Method: 研究回顾性分析了145名接受微型超声引导活检的男性，使用自监督卷积自动编码器提取图像特征，随机森林分类器进行预测，并与基于PSA、DRE等临床模型对比。

Result: AI微型超声模型的AUROC为0.871，灵敏度92.5%，特异性68.1%，优于临床模型的AUROC 0.753。临床模型虽灵敏度高（96.2%），但特异性较低（27.3%）。

Conclusion: AI解释的微型超声在保持高灵敏度的同时提高了特异性，可能减少不必要的活检，并作为低成本筛查替代方案。

Abstract: Background and objective: Micro-ultrasound (micro-US) is a novel imaging
modality with diagnostic accuracy comparable to MRI for detecting clinically
significant prostate cancer (csPCa). We investigated whether artificial
intelligence (AI) interpretation of micro-US can outperform clinical screening
methods using PSA and digital rectal examination (DRE). Methods: We
retrospectively studied 145 men who underwent micro-US guided biopsy (79 with
csPCa, 66 without). A self-supervised convolutional autoencoder was used to
extract deep image features from 2D micro-US slices. Random forest classifiers
were trained using five-fold cross-validation to predict csPCa at the slice
level. Patients were classified as csPCa-positive if 88 or more consecutive
slices were predicted positive. Model performance was compared with a
classifier using PSA, DRE, prostate volume, and age. Key findings and
limitations: The AI-based micro-US model and clinical screening model achieved
AUROCs of 0.871 and 0.753, respectively. At a fixed threshold, the micro-US
model achieved 92.5% sensitivity and 68.1% specificity, while the clinical
model showed 96.2% sensitivity but only 27.3% specificity. Limitations include
a retrospective single-center design and lack of external validation.
Conclusions and clinical implications: AI-interpreted micro-US improves
specificity while maintaining high sensitivity for csPCa detection. This method
may reduce unnecessary biopsies and serve as a low-cost alternative to
PSA-based screening. Patient summary: We developed an AI system to analyze
prostate micro-ultrasound images. It outperformed PSA and DRE in detecting
aggressive cancer and may help avoid unnecessary biopsies.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [387] [Data-driven multi-agent modelling of calcium interactions in cell culture: PINN vs Regularized Least-squares](https://arxiv.org/abs/2505.20327)
*Aurora Poggi,Giuseppe Alessio D'Inverno,Hjalmar Brismar,Ozan Öktem,Matthieu Barreau,Kateryna Morozovska*

Key words: 数据驱动, 动力学建模, CRLSM, PINN, 系统辨识

<details>
  <summary>Details</summary>

Main category: q-bio.QM

TL;DR: 该论文探讨了在生物系统中使用CRLSM和PINN方法进行动力学建模的效果。CRLSM表现较好，而PINN在当前配置下效果不佳，但未来优化有望改善。

Motivation: 研究动力是改进生物系统（如钙信号传导）的动力学建模方法，克服传统方法的局限，尤其是对候选项库的先验知识要求。

Method: 比较了约束正则化最小二乘法（CRLSM）和物理信息神经网络（PINN）在系统辨识和参数发现中的性能。

Result: CRLSM在参数估计和数据拟合上表现良好，而PINN未能匹配其表现，但在未来可能通过超参数调优和不确定性量化改进。

Conclusion: CRLSM是目前更可靠的动力学建模方法，而PINN虽当前效果不佳，未来仍有潜力。

Abstract: Data-driven discovery of dynamics in biological systems allows for better
observation and characterization of processes, such as calcium signaling in
cell culture. Recent advancements in techniques allow the exploration of
previously unattainable insights of dynamical systems, such as the Sparse
Identification of Non-Linear Dynamics (SINDy), overcoming the limitations of
more classic methodologies. The latter requires some prior knowledge of an
effective library of candidate terms, which is not realistic for a real case
study. Using inspiration from fields like traffic density estimation and
control theory, we propose a methodology for characterization and performance
analysis of calcium delivery in a family of cells. In this work, we compare the
performance of the Constrained Regularized Least-Squares Method (CRLSM) and
Physics-Informed Neural Networks (PINN) for system identification and parameter
discovery for governing ordinary differential equations (ODEs). The CRLSM
achieves a fairly good parameter estimate and a good data fit when using the
learned parameters in the Consensus problem. On the other hand, despite the
initial hypothesis, PINNs fail to match the CRLSM performance and, under the
current configuration, do not provide fair parameter estimation. However, we
have only studied a limited number of PINN architectures, and it is expected
that additional hyperparameter tuning, as well as uncertainty quantification,
could significantly improve the performance in future works.

</details>


### [388] [Sequence-Only Prediction of Binding Affinity Changes: A Robust and Interpretable Model for Antibody Engineering](https://arxiv.org/abs/2505.20301)
*Chen Liu,Mingchen Li,Yang Tan,Wenrui Gou,Guisheng Fan,Bingxin Zhou*

Key words: 抗體工程, 深度學習, 結合親和力, 序列分析, 注意力機制

<details>
  <summary>Details</summary>

Main category: q-bio.QM

TL;DR: ProtAttBA是一個僅依賴抗體-抗原序列資訊的深度學習模型，用於預測結合親和力變化，無需高品質結構數據，表現優於傳統方法且具解釋性。

Motivation: 傳統濕實驗成本高且耗時，而現有深度學習方法依賴高品質結構數據（實際中常不可得），因此開發僅需序列資訊的模型成為必要。

Method: ProtAttBA先通過預訓練學習蛋白序列模式，再通過監督訓練（使用標記數據）訓練基於交叉注意力機制的回歸器預測結合親和力變化。

Result: 在三個公開基準測試中，ProtAttBA表現優於基於序列和結構的方法，尤其在不確定結構條件下展現魯棒性，且注意力機制提供可解釋性。

Conclusion: ProtAttBA為抗體工程提供快速且經濟的計算工具，有望加速治療性抗體開發。

Abstract: A pivotal area of research in antibody engineering is to find effective
modifications that enhance antibody-antigen binding affinity. Traditional
wet-lab experiments assess mutants in a costly and time-consuming manner.
Emerging deep learning solutions offer an alternative by modeling antibody
structures to predict binding affinity changes. However, they heavily depend on
high-quality complex structures, which are frequently unavailable in practice.
Therefore, we propose ProtAttBA, a deep learning model that predicts binding
affinity changes based solely on the sequence information of antibody-antigen
complexes. ProtAttBA employs a pre-training phase to learn protein sequence
patterns, following a supervised training phase using labeled antibody-antigen
complex data to train a cross-attention-based regressor for predicting binding
affinity changes. We evaluated ProtAttBA on three open benchmarks under
different conditions. Compared to both sequence- and structure-based prediction
methods, our approach achieves competitive performance, demonstrating notable
robustness, especially with uncertain complex structures. Notably, our method
possesses interpretability from the attention mechanism. We show that the
learned attention scores can identify critical residues with impacts on binding
affinity. This work introduces a rapid and cost-effective computational tool
for antibody engineering, with the potential to accelerate the development of
novel therapeutic antibodies.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [389] [Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions](https://arxiv.org/abs/2505.20692)
*Saharsh Barve,Andy Mao,Jiayue Melissa Shi,Prerna Juneja,Koustuv Saha*

Key words: 文本生成图像、社会偏见、伦理AI、刻板印象、提示设计

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 论文提出了一种理论驱动的偏见检测标准和社交刻板印象指数（SSI），用于系统评估T2I模型的输出偏见，并通过针对性提示优化显著减少了偏见。用户研究发现，虽然去偏可以降低刻板印象，但可能会影响上下文的匹配度。

Motivation: 由于T2I模型在生成图像时容易复制和放大社会刻板印象（如性别、种族和文化），引发了伦理问题。论文旨在量化并减少这些偏见。

Method: 设计了SSI和偏见检测标准，审计了三种T2I模型的输出（DALL-E-3、Midjourney-6.1和Stability AI Core），并采用LLM进行针对性提示优化以减少偏见。通过用户研究分析对偏见的感知与偏好。

Result: 初始输出普遍存在刻板印象，但通过提示优化，SSI分别降低了61%（地理文化）、69%（职业）和51%（形容词）。用户研究发现去偏可能影响上下文匹配度。

Conclusion: 需要在伦理去偏与上下文相关性之间取得平衡，并开发支持全球多样性和包容性的T2I系统。

Abstract: Recent advances in generative AI have enabled visual content creation through
text-to-image (T2I) generation. However, despite their creative potential, T2I
models often replicate and amplify societal stereotypes -- particularly those
related to gender, race, and culture -- raising important ethical concerns.
This paper proposes a theory-driven bias detection rubric and a Social
Stereotype Index (SSI) to systematically evaluate social biases in T2I outputs.
We audited three major T2I model outputs -- DALL-E-3, Midjourney-6.1, and
Stability AI Core -- using 100 queries across three categories -- geocultural,
occupational, and adjectival. Our analysis reveals that initial outputs are
prone to include stereotypical visual cues, including gendered professions,
cultural markers, and western beauty norms. To address this, we adopted our
rubric to conduct targeted prompt refinement using LLMs, which significantly
reduced bias -- SSI dropped by 61% for geocultural, 69% for occupational, and
51% for adjectival queries. We complemented our quantitative analysis through a
user study examining perceptions, awareness, and preferences around
AI-generated biased imagery. Our findings reveal a key tension -- although
prompt refinement can mitigate stereotypes, it can limit contextual alignment.
Interestingly, users often perceived stereotypical images to be more aligned
with their expectations. We discuss the need to balance ethical debiasing with
contextual relevance and call for T2I systems that support global diversity and
inclusivity while not compromising the reflection of real-world social
complexity.

</details>


### [390] [The Impact of a Chatbot's Ephemerality-Framing on Self-Disclosure Perceptions](https://arxiv.org/abs/2505.20464)
*Samuel Rhys Cox,Rune Møberg Jacobsen,Niels van Berkel*

Key words: 自我披露, 聊天机器人, 关系框架, 短暂性, 情感披露

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 研究探讨了聊天机器人的关系框架（熟悉型与陌生型）对用户自我披露的影响，发现情感披露时陌生型更令人舒适，而事实披露时熟悉型更受欢迎。

Motivation: 聊天机器人越来越多用于自我披露，但其关系框架对用户自我披露的影响尚未充分研究。

Method: 采用混合因子设计，参与者与熟悉型或陌生型聊天机器人进行两轮对话（情感披露与事实披露）。

Result: 情感披露时陌生型更舒适；事实披露优先时熟悉型更受喜爱。定性结果显示陌生型提供匿名性，熟悉型需通过低风险披露建立信任。

Conclusion: 聊天机器人的关系框架对自我披露的影响取决于披露类型和顺序，需结合情境设计。

Abstract: Self-disclosure, the sharing of one's thoughts and feelings, is affected by
the perceived relationship between individuals. While chatbots are increasingly
used for self-disclosure, the impact of a chatbot's framing on users'
self-disclosure remains under-explored. We investigated how a chatbot's
description of its relationship with users, particularly in terms of
ephemerality, affects self-disclosure. Specifically, we compared a Familiar
chatbot, presenting itself as a companion remembering past interactions, with a
Stranger chatbot, presenting itself as a new, unacquainted entity in each
conversation. In a mixed factorial design, participants engaged with either the
Familiar or Stranger chatbot in two sessions across two days, with one
conversation focusing on Emotional- and another Factual-disclosure. When
Emotional-disclosure was sought in the first chatting session,
Stranger-condition participants felt more comfortable self-disclosing. However,
when Factual-disclosure was sought first, these differences were replaced by
more enjoyment among Familiar-condition participants. Qualitative findings
showed Stranger afforded anonymity and reduced judgement, whereas Familiar
sometimes felt intrusive unless rapport was built via low-risk
Factual-disclosure.

</details>


### [391] [Creativity in LLM-based Multi-Agent Systems: A Survey](https://arxiv.org/abs/2505.21116)
*Yi-Cheng Lin,Kang-Chieh Chen,Zhe-Yan Li,Tzu-Heng Wu,Tzu-Hsuan Wu,Kuan-Yu Chen,Hung-yi Lee,Yun-Nung Chen*

Key words: 大型语言模型（LLM）, 多智能体系统（MAS）, 创造性, 文本生成, 图像生成, 评估标准

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 该论文是第一篇专注于多智能体系统（MAS）中创造性的综述，涵盖了智能体主动性、角色设计、生成技术、数据集和评估指标，并提出了未来研究的关键挑战与方向。

Motivation: 现有综述多关注MAS的基础设施，而忽略了创造性维度。本文填补了这一空白，探讨了如何生成和评估新颖输出、创意如何影响智能体角色设计以及协调创意工作流。

Method: 通过构建智能体主动性和角色设计的分类法，总结生成技术（如发散探索、迭代优化和协作合成），并分析相关数据集和评估指标。

Result: 提出了一个结构化框架和路线图，以推动创造性MAS的开发、评估和标准化，并识别了评估标准不一致、偏见缓解不足、协调冲突和缺乏统一基准等挑战。

Conclusion: 本文为未来研究提供了系统性指导，强调了在MAS中实现创造性的关键问题与标准化需求。

Abstract: Large language model (LLM)-driven multi-agent systems (MAS) are transforming
how humans and AIs collaboratively generate ideas and artifacts. While existing
surveys provide comprehensive overviews of MAS infrastructures, they largely
overlook the dimension of \emph{creativity}, including how novel outputs are
generated and evaluated, how creativity informs agent personas, and how
creative workflows are coordinated. This is the first survey dedicated to
creativity in MAS. We focus on text and image generation tasks, and present:
(1) a taxonomy of agent proactivity and persona design; (2) an overview of
generation techniques, including divergent exploration, iterative refinement,
and collaborative synthesis, as well as relevant datasets and evaluation
metrics; and (3) a discussion of key challenges, such as inconsistent
evaluation standards, insufficient bias mitigation, coordination conflicts, and
the lack of unified benchmarks. This survey offers a structured framework and
roadmap for advancing the development, evaluation, and standardization of
creative MAS.

</details>


### [392] [Enhancing Wearable Tap Water Audio Detection through Subclass Annotation in the HD-Epic Dataset](https://arxiv.org/abs/2505.20788)
*Robin Burchard,Kristof Van Laerhoven*

Key words: 可穿戴设备, 活动识别, 声音数据, 隐私保护, 水流检测

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 论文探讨了在可穿戴设备上利用声音数据进行活动识别的潜力，重点研究了水流检测的应用。由于隐私考虑，数据需本地处理。通过标注新标签‘tap water’并训练轻量分类器，证明新类别易于学习。

Motivation: 研究动机是利用声音数据（如水流声）增强可穿戴设备的活动识别能力，但需解决隐私和能耗问题。

Method: 方法包括标注HD-Epic数据集中的‘tap water’标签，并训练轻量分类器评估新类别。

Result: 结果显示新标签类别（‘tap water’）更容易被分类器学习。

Conclusion: 结论表明声音数据（尤其是特定情境如水流）可在隐私保护前提下提升活动识别效果。

Abstract: Wearable human activity recognition has been shown to benefit from the
inclusion of acoustic data, as the sounds around a person often contain
valuable context. However, due to privacy concerns, it is usually not ethically
feasible to record and save microphone data from the device, since the audio
could, for instance, also contain private conversations. Rather, the data
should be processed locally, which in turn requires processing power and
consumes energy on the wearable device. One special use case of contextual
information that can be utilized to augment special tasks in human activity
recognition is water flow detection, which can, e.g., be used to aid wearable
hand washing detection. We created a new label called tap water for the
recently released HD-Epic data set, creating 717 hand-labeled annotations of
tap water flow, based on existing annotations of the water class. We analyzed
the relation of tap water and water in the dataset and additionally trained and
evaluated two lightweight classifiers to evaluate the newly added label class,
showing that the new class can be learned more easily.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [393] [InstGenIE: Generative Image Editing Made Efficient with Mask-aware Caching and Scheduling](https://arxiv.org/abs/2505.20600)
*Xiaoxiao Jiang,Suyi Li,Lingyun Yang,Tianyu Feng,Zhipeng Di,Weiyi Lu,Guoxuan Zhu,Xiu Lin,Kan Liu,Yinghao Yu,Tao Lan,Guodong Yang,Lin Qu,Liping Zhang,Wei Wang*

Key words: 图像编辑，扩散模型，缓存优化，批处理，负载均衡

<details>
  <summary>Details</summary>

Main category: cs.DC

TL;DR: InstGenIE是一个高效服务图像编辑请求的系统，通过跳过未掩码区域的冗余计算、优化缓存加载和批处理策略，显著提升吞吐量和降低延迟。

Motivation: 在生成式图像编辑中，传统的扩散模型推理存在计算冗余问题，尤其是在处理部分掩码图像时，未掩码区域的重复计算浪费资源。

Method: InstGenIE利用缓存中间激活结果跳过未掩码区域的计算，采用无气泡管道方案重叠计算与缓存加载，并引入连续批处理和负载均衡策略。

Result: 相比现有系统，InstGenIE实现了最高3倍的吞吐量提升和14.7倍的延迟降低，同时保持图像质量。

Conclusion: InstGenIE通过计算优化和资源调度，显著提升了扩散模型在图像编辑任务中的服务效率。

Abstract: Generative image editing using diffusion models has become a prevalent
application in today's AI cloud services. In production environments, image
editing typically involves a mask that specifies the regions of an image
template to be edited. The use of masks provides direct control over the
editing process and introduces sparsity in the model inference. In this paper,
we present InstGenIE, a system that efficiently serves image editing requests.
The key insight behind InstGenIE is that image editing only modifies the masked
regions of image templates while preserving the original content in the
unmasked areas. Driven by this insight, InstGenIE judiciously skips redundant
computations associated with the unmasked areas by reusing cached intermediate
activations from previous inferences. To mitigate the high cache loading
overhead, InstGenIE employs a bubble-free pipeline scheme that overlaps
computation with cache loading. Additionally, to reduce queuing latency in
online serving while improving the GPU utilization, InstGenIE proposes a novel
continuous batching strategy for diffusion model serving, allowing newly
arrived requests to join the running batch in just one step of denoising
computation, without waiting for the entire batch to complete. As heterogeneous
masks induce imbalanced loads, InstGenIE also develops a load balancing
strategy that takes into account the loads of both computation and cache
loading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving
systems for image editing, achieving up to 3x higher throughput and reducing
average request latency by up to 14.7x while ensuring image quality.

</details>


### [394] [Time-Series Learning for Proactive Fault Prediction in Distributed Systems with Deep Neural Structures](https://arxiv.org/abs/2505.20705)
*Yang Wang,Wenxuan Zhu,Xuehui Quan,Heyi Wang,Chang Liu,Qiyuan Wu*

Key words: 故障预测, 分布式系统, GRU, 注意力机制, 时序特征学习

<details>
  <summary>Details</summary>

Main category: cs.DC

TL;DR: 论文提出一种基于时序特征学习的智能预测方法，用于提升分布式系统的故障预测和响应能力，通过GRU和注意力机制优化时序建模，实验表明其在准确率和稳定性上优于主流模型。

Motivation: 分布式系统中的故障预测和响应延迟问题亟待解决，需要更高效的方法来提前预警和减少故障影响。

Method: 采用GRU建模系统状态时序变化，结合注意力机制增强关键时段特征，最后通过前馈神经网络进行分类。

Result: 实验证明，该方法在Accuracy、F1-Score和AUC指标上优于其他时序模型，训练过程收敛可靠。

Conclusion: 所提方法能有效学习系统行为模式，实现高效故障检测，具备实际应用潜力。

Abstract: This paper addresses the challenges of fault prediction and delayed response
in distributed systems by proposing an intelligent prediction method based on
temporal feature learning. The method takes multi-dimensional performance
metric sequences as input. We use a Gated Recurrent Unit (GRU) to model the
evolution of system states over time. An attention mechanism is then applied to
enhance key temporal segments, improving the model's ability to identify
potential faults. On this basis, a feedforward neural network is designed to
perform the final classification, enabling early warning of system failures. To
validate the effectiveness of the proposed approach, comparative experiments
and ablation analyses were conducted using data from a large-scale real-world
cloud system. The experimental results show that the model outperforms various
mainstream time-series models in terms of Accuracy, F1-Score, and AUC. This
demonstrates strong prediction capability and stability. Furthermore, the loss
function curve confirms the convergence and reliability of the training
process. It indicates that the proposed method effectively learns system
behavior patterns and achieves efficient fault detection.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [395] [CardioPatternFormer: Pattern-Guided Attention for Interpretable ECG Classification with Transformer Architecture](https://arxiv.org/abs/2505.20481)
*Berat Kutay Uğraş,Ömer Nezih Gerek,İbrahim Talha Saygı*

Key words: ECG, Transformer, 可解释性, 心脏模式, 注意力机制, 临床诊断

<details>
  <summary>Details</summary>

Main category: eess.SP

TL;DR: 介绍了CardioPatternFormer，一种基于Transformer的模型，用于可解释的心电图分类。该模型通过注意力机制精确识别和分类各种心脏模式，尤其在复杂多病症情况下表现出色，并提供透明解释。

Motivation: 当前ECG解释面临复杂心脏数据和“黑盒”AI模型的限制，影响了临床实用性。受Transformer在NLP中的成功启发，作者将ECG视为心脏独特的“语言”，旨在开发一种可解释的模型。

Method: 提出了CardioPatternFormer，一种基于Transformer的模型，利用注意力机制识别和分类心脏模式，尤其擅长分析细微异常和多病症共存的情况。

Result: 模型在复杂ECG（包括多病症病例）上表现出色，通过注意力图提供透明解释，帮助临床医生理解模型决策。

Conclusion: 该研究为ECG分析提供了一种强大且透明的AI解决方案，有助于提升临床诊断的可信度和实用性。

Abstract: Accurate ECG interpretation is vital, yet complex cardiac data and
"black-box" AI models limit clinical utility. Inspired by Transformer
architectures' success in NLP for understanding sequential data, we frame ECG
as the heart's unique "language" of temporal patterns. We present
CardioPatternFormer, a novel Transformer-based model for interpretable ECG
classification. It employs a sophisticated attention mechanism to precisely
identify and classify diverse cardiac patterns, excelling at discerning subtle
anomalies and distinguishing multiple co-occurring conditions. This
pattern-guided attention provides clear insights by highlighting influential
signal regions, effectively allowing the "heart to talk" through transparent
interpretations. CardioPatternFormer demonstrates robust performance on
challenging ECGs, including complex multi-pathology cases. Its interpretability
via attention maps enables clinicians to understand the model's rationale,
fostering trust and aiding informed diagnostic decisions. This work offers a
powerful, transparent solution for advanced ECG analysis, paving the way for
more reliable and clinically actionable AI in cardiology.

</details>


### [396] [BrainStratify: Coarse-to-Fine Disentanglement of Intracranial Neural Dynamics](https://arxiv.org/abs/2505.20480)
*Hui Zheng,Hai-Teng Wang,Yi-Tao Jing,Pei-Yang Lin,Han-Qing Zhao,Wei Chen,Peng-Hu Wei,Yong-Zhi Shan,Guo-Guang Zhao,Yun-Zhe Liu*

Key words: 脑机接口，语音解码，sEEG，ECoG，神经解缠，BrainStratify

<details>
  <summary>Details</summary>

Main category: eess.SP

TL;DR: BrainStratify 是一种新的框架，通过粗到细的神经解缠方法从颅内神经信号中解码语音，显著优于现有方法。

Motivation: 解决颅内场电位记录中任务相关神经信号稀疏且与无关信号纠缠的问题，以提升语音解码的准确性和可解释性。

Method: 结合空间上下文引导的时空建模识别功能组，并利用解耦产品量化（DPQ）解缠目标功能组内的神经动态。

Result: 在多个开源数据集（sEEG 和 ECoG）上验证，BrainStratify 显著优于现有解码方法。

Conclusion: 通过数据驱动的分层和神经科学启发的模块化，BrainStratify 提供了一个鲁棒且可解释的语音解码方案。

Abstract: Decoding speech directly from neural activity is a central goal in
brain-computer interface (BCI) research. In recent years, exciting advances
have been made through the growing use of intracranial field potential
recordings, such as stereo-ElectroEncephaloGraphy (sEEG) and
ElectroCorticoGraphy (ECoG). These neural signals capture rich population-level
activity but present key challenges: (i) task-relevant neural signals are
sparsely distributed across sEEG electrodes, and (ii) they are often entangled
with task-irrelevant neural signals in both sEEG and ECoG. To address these
challenges, we introduce a unified Coarse-to-Fine neural disentanglement
framework, BrainStratify, which includes (i) identifying functional groups
through spatial-context-guided temporal-spatial modeling, and (ii)
disentangling distinct neural dynamics within the target functional group using
Decoupled Product Quantization (DPQ). We evaluate BrainStratify on two
open-source sEEG datasets and one (epidural) ECoG dataset, spanning tasks like
vocal production and speech perception. Extensive experiments show that
BrainStratify, as a unified framework for decoding speech from intracranial
neural signals, significantly outperforms previous decoding methods. Overall,
by combining data-driven stratification with neuroscience-inspired modularity,
BrainStratify offers a robust and interpretable solution for speech decoding
from intracranial recordings.

</details>


### [397] [Federated Learning-Distillation Alternation for Resource-Constrained IoT](https://arxiv.org/abs/2505.20456)
*Rafael Valente da Silva,Onel L. Alcaraz López,Richard Demo Souza*

Key words: 联邦学习, 知识蒸馏, 物联网, 能量收集, 多信道ALOHA

<details>
  <summary>Details</summary>

Main category: eess.SP

TL;DR: FLDA方案通过交替使用联邦学习和蒸馏技术，在物联网网络中平衡模型准确性和资源消耗，相较于传统方法节能达98%且抗干扰性更强。

Motivation: 物联网设备资源受限，传统联邦学习在能量和通信资源上效率低，而蒸馏技术虽节能但牺牲准确性。因此，需一种兼顾资源效率和模型性能的方案。

Method: 提出FL-distillation alternation (FLDA)，交替执行联邦学习和蒸馏阶段，利用多信道时隙ALOHA网络结合能量收集技术。

Result: FLDA在模型准确性和收敛速度上优于纯联邦学习或蒸馏，节能达98%，且对背景干扰更鲁棒。

Conclusion: FLDA在资源受限的物联网网络中有效平衡性能和效率，是联邦学习的实用改进方案。

Abstract: Federated learning (FL) faces significant challenges in Internet of Things
(IoT) networks due to device limitations in energy and communication resources,
especially when considering the large size of FL models. From an energy
perspective, the challenge is aggravated if devices rely on energy harvesting
(EH), as energy availability can vary significantly over time, influencing the
average number of participating users in each iteration. Additionally, the
transmission of large model updates is more susceptible to interference from
uncorrelated background traffic in shared wireless environments. As an
alternative, federated distillation (FD) reduces communication overhead and
energy consumption by transmitting local model outputs, which are typically
much smaller than the entire model used in FL. However, this comes at the cost
of reduced model accuracy. Therefore, in this paper, we propose FL-distillation
alternation (FLDA). In FLDA, devices alternate between FD and FL phases,
balancing model information with lower communication overhead and energy
consumption per iteration. We consider a multichannel slotted-ALOHA EH-IoT
network subject to background traffic/interference. In such a scenario, FLDA
demonstrates higher model accuracy than both FL and FD, and achieves faster
convergence than FL. Moreover, FLDA achieves target accuracies saving up to 98%
in energy consumption, while also being less sensitive to interference, both
relative to FL.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [398] [VeriThoughts: Enabling Automated Verilog Code Generation using Reasoning and Formal Verification](https://arxiv.org/abs/2505.20302)
*Patrick Yubeaton,Andre Nakkab,Weihua Xiao,Luca Collini,Ramesh Karri,Chinmay Hegde,Siddharth Garg*

Key words: VeriThoughts, Verilog, 代码生成, 形式验证, 硬件设计

<details>
  <summary>Details</summary>

Main category: cs.PL

TL;DR: 该论文介绍了VeriThoughts数据集，用于基于推理的Verilog代码生成，并建立了一个基于形式验证方法的基准框架。同时，提出了专门优化的小规模模型套件。

Motivation: 为了解决自动化硬件设计工具的需求，这些工具能从高级规范生成可验证的正确实现，从而加速硬件开发过程并保持严格正确性。

Method: 提出了VeriThoughts数据集和基于形式验证的基准框架，以及专门优化的小规模模型套件用于Verilog生成。

Result: 建立了一个新的数据集和基准框架，并开发了针对Verilog生成的优化模型。

Conclusion: 这项研究为自动化硬件设计提供了新工具，能加速开发并确保正确性。

Abstract: This paper introduces VeriThoughts, a novel dataset designed for
reasoning-based Verilog code generation. We establish a new benchmark framework
grounded in formal verification methods to evaluate the quality and correctness
of generated hardware descriptions. Additionally, we present a suite of
specialized small-scale models optimized specifically for Verilog generation.
Our work addresses the growing need for automated hardware design tools that
can produce verifiably correct implementations from high-level specifications,
potentially accelerating the hardware development process while maintaining
rigorous correctness guarantees. Our code and data are available at
\href{https://github.com/wilyub/VeriThoughts}{this URL}.

</details>


### [399] [LEGO-Compiler: Enhancing Neural Compilation Through Translation Composability](https://arxiv.org/abs/2505.20356)
*Shuoming Zhang,Jiacheng Zhao,Chunwei Xia,Zheng Wang,Yunji Chen,Xiaobing Feng,Huimin Cui*

Key words: 大语言模型, LEGO-Compiler, 编译, 代码翻译, 反馈机制

<details>
  <summary>Details</summary>

Main category: cs.PL

TL;DR: LEGO-Compiler利用大语言模型（LLMs）将高级语言翻译为汇编代码，通过分块处理、可验证工作流和反馈机制，显著提升了编译效率和准确性。

Motivation: 现有LLMs难以处理长而复杂的程序，因此提出LEGO-Compiler以改进编译过程的效率和准确性。

Method: 采用LEGO分块翻译、可验证LLM工作流和反馈机制，将复杂编译过程分解为简单步骤。

Result: 在ExeBench和AnsiBench数据集上分别达到99%和97.9%的准确率，编译代码规模扩展性提升近一个数量级。

Conclusion: LEGO-Compiler为LLMs在系统级任务中的应用开辟了新途径，补充了传统编译器技术。

Abstract: Large language models (LLMs) have the potential to revolutionize how we
design and implement compilers and code translation tools. However, existing
LLMs struggle to handle long and complex programs. We introduce LEGO-Compiler,
a novel neural compilation system that leverages LLMs to translate high-level
languages into assembly code. Our approach centers on three key innovations:
LEGO translation, which decomposes the input program into manageable blocks;
breaking down the complex compilation process into smaller, simpler verifiable
steps by organizing it as a verifiable LLM workflow by external tests; and a
feedback mechanism for self-correction. Supported by formal proofs of
translation composability, LEGO-Compiler demonstrates high accuracy on multiple
datasets, including over 99% on ExeBench and 97.9% on industrial-grade
AnsiBench. Additionally, LEGO-Compiler has also acheived near one
order-of-magnitude improvement on compilable code size scalability. This work
opens new avenues for applying LLMs to system-level tasks, complementing
traditional compiler technologies.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [400] [Streamlining Knowledge Graph Creation with PyRML](https://arxiv.org/abs/2505.20949)
*Andrea Giovanni Nuzzolese*

Key words: 知识图谱, RML, 声明式映射, Python, 数据整合

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: PyRML是一个轻量级的Python原生库，用于通过声明式映射构建知识图谱。它支持核心RML结构，并集成了流行的数据处理和语义网络库。

Motivation: 知识图谱（KGs）在多个领域被广泛采用，但现有工具在可扩展性和易用性方面存在局限。PyRML旨在降低KG构建门槛，促进可复用的数据整合。

Method: PyRML提供了一个Python原生的声明式映射库，支持RML核心功能，并与其他数据科学和语义网络工具（如Pandas和RDFlib）无缝集成。

Result: PyRML实现了轻量化且模块化的KG构建流程，提升了透明度和可重复性，同时支持与现有工具的协同工作。

Conclusion: PyRML弥合了声明式语义与实际KG工程之间的差距，为KG构建提供了更灵活和易用的解决方案。

Abstract: Knowledge Graphs (KGs) are increasingly adopted as a foundational technology
for integrating heterogeneous data in domains such as climate science, cultural
heritage, and the life sciences. Declarative mapping languages like R2RML and
RML have played a central role in enabling scalable and reusable KG
construction, offering a transparent means of transforming structured and
semi-structured data into RDF. In this paper, we present PyRML, a lightweight,
Python-native library for building Knowledge Graphs through declarative
mappings. PyRML supports core RML constructs and provides a programmable
interface for authoring, executing, and testing mappings directly within Python
environments. It integrates with popular data and semantic web libraries (e.g.,
Pandas and RDFlib), enabling transparent and modular workflows. By lowering the
barrier to entry for KG creation and fostering reproducible, ontology-aligned
data integration, PyRML bridges the gap between declarative semantics and
practical KG engineering.

</details>


### [401] [LazyVLM: Neuro-Symbolic Approach to Video Analytics](https://arxiv.org/abs/2505.21459)
*Xiangru Jian,Wei Pang,Zhengyuan Dong,Chao Zhang,M. Tamer Özsu*

Key words: video analytics, VLMs, neuro-symbolic, query processing

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: LazyVLM是一个结合神经符号方法的视频分析系统，提供类似VLM的查询界面但解决了其扩展性问题。

Motivation: 现有视频分析方法在灵活性和效率之间存在矛盾，VLM处理长上下文效率低，神经符号方法依赖人工标注和固定规则。

Method: 通过半结构化文本接口分解多帧视频查询，将大部分处理转移到高效的关系查询和向量相似性搜索。

Result: LazyVLM在开放域视频数据查询中表现出高效、鲁棒和用户友好的特性。

Conclusion: LazyVLM为大规模视频查询提供了高效且易用的解决方案。

Abstract: Current video analytics approaches face a fundamental trade-off between
flexibility and efficiency. End-to-end Vision Language Models (VLMs) often
struggle with long-context processing and incur high computational costs, while
neural-symbolic methods depend heavily on manual labeling and rigid rule
design. In this paper, we introduce LazyVLM, a neuro-symbolic video analytics
system that provides a user-friendly query interface similar to VLMs, while
addressing their scalability limitation. LazyVLM enables users to effortlessly
drop in video data and specify complex multi-frame video queries using a
semi-structured text interface for video analytics. To address the scalability
limitations of VLMs, LazyVLM decomposes multi-frame video queries into
fine-grained operations and offloads the bulk of the processing to efficient
relational query execution and vector similarity search. We demonstrate that
LazyVLM provides a robust, efficient, and user-friendly solution for querying
open-domain video data at scale.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [402] [MetamatBench: Integrating Heterogeneous Data, Computational Tools, and Visual Interface for Metamaterial Discovery](https://arxiv.org/abs/2505.20299)
*Jianpeng Chen,Wangzhi Zhan,Haohui Wang,Zian Jia,Jingru Gan,Junkai Zhang,Jingyuan Qi,Tingwei Chen,Lifu Huang,Muhao Chen,Ling Li,Wei Wang,Dawei Zhou*

Key words: 超材料, 机器学习, 数据异构性, 模型复杂性, 人机协作

<details>
  <summary>Details</summary>

Main category: physics.optics

TL;DR: 该论文介绍了一个名为MetamatBench的统一框架，旨在解决机器学习在超材料发现中的三大挑战：数据异构性、模型复杂性和人机协作。

Motivation: 为了克服机器学习在超材料发现中因数据异构性、模型复杂性和人机协作问题而受限的挑战，作者提出了一个统一框架以推动该领域的研究与应用。

Method: MetamatBench在三个层面开展工作：数据层面整合并标准化5个多模态数据集；ML层面提供17种先进方法的工具包及12种评价指标；用户层面提供可视化界面以促进非ML研究者的使用。

Result: 该框架提供了一个统一的平台（http://zhoulab-1.cs.vt.edu:5550）和开源代码库（https://github.com/cjpcool/Metamaterial-Benchmark），促进了超材料的属性预测和逆向设计。

Conclusion: MetamatBench为超材料发现中的机器学习研究提供了一个全面且易用的平台，解决了关键挑战并推动了领域发展。

Abstract: Metamaterials, engineered materials with architected structures across
multiple length scales, offer unprecedented and tunable mechanical properties
that surpass those of conventional materials. However, leveraging advanced
machine learning (ML) for metamaterial discovery is hindered by three
fundamental challenges: (C1) Data Heterogeneity Challenge arises from
heterogeneous data sources, heterogeneous composition scales, and heterogeneous
structure categories; (C2) Model Complexity Challenge stems from the intricate
geometric constraints of ML models, which complicate their adaptation to
metamaterial structures; and (C3) Human-AI Collaboration Challenge comes from
the "dual black-box'' nature of sophisticated ML models and the need for
intuitive user interfaces. To tackle these challenges, we introduce a unified
framework, named MetamatBench, that operates on three levels. (1) At the data
level, we integrate and standardize 5 heterogeneous, multi-modal metamaterial
datasets. (2) The ML level provides a comprehensive toolkit that adapts 17
state-of-the-art ML methods for metamaterial discovery. It also includes a
comprehensive evaluation suite with 12 novel performance metrics with finite
element-based assessments to ensure accurate and reliable model validation. (3)
The user level features a visual-interactive interface that bridges the gap
between complex ML techniques and non-ML researchers, advancing property
prediction and inverse design of metamaterials for research and applications.
MetamatBench offers a unified platform deployed at
http://zhoulab-1.cs.vt.edu:5550 that enables machine learning researchers and
practitioners to develop and evaluate new methodologies in metamaterial
discovery. For accessibility and reproducibility, we open-source our benchmark
and the codebase at https://github.com/cjpcool/Metamaterial-Benchmark.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [403] [Algorithmic Control Improves Residential Building Energy and EV Management when PV Capacity is High but Battery Capacity is Low](https://arxiv.org/abs/2505.20377)
*Lennart Ullner,Alona Zharova,Felix Creutzig*

Key words: 家庭能源管理,深度强化学习,电动车充电,光伏发电,电池存储

<details>
  <summary>Details</summary>

Main category: eess.SY

TL;DR: 论文研究了如何利用深度强化学习（DRL）优化家庭能源管理，特别是在电动车充电和光伏发电场景下的应用效果。

Motivation: 研究目的是解决家庭能源管理中的动态和不确定性问题，尤其是电动车充电的优化，以减轻电网压力并促进能源转型。

Method: 通过分析90户家庭的真实数据，比较了深度强化学习（DRL）、基于规则的算法和模型预测控制在家庭能源管理中的表现。

Result: 研究表明，DRL能有效优化电动车和电池存储的充电行为，尤其在电池容量较低的家庭中效果显著。高电池容量的家庭则优化空间有限。

Conclusion: 具备优化潜力的家庭采用DRL可以提升能源管理效率，降低用电成本，有助于电力系统的脱碳。

Abstract: Efficient energy management in prosumer households is key to alleviating grid
stress in an energy transition marked by electric vehicles (EV), renewable
energies and battery storage. However, it is unclear how households optimize
prosumer EV charging. Here we study real-world data from 90 households on
fixed-rate electricity tariffs in German-speaking countries to investigate the
potential of Deep Reinforcement Learning (DRL) and other control approaches
(Rule-Based, Model Predictive Control) to manage the dynamic and uncertain
environment of Home Energy Management (HEM) and optimize household charging
patterns. The DRL agent efficiently aligns charging of EV and battery storage
with photovoltaic (PV) surplus. We find that frequent EV charging transactions,
early EV connections and PV surplus increase optimization potential. A detailed
analysis of nine households (1 hour resolution, 1 year) demonstrates that high
battery capacity facilitates self optimization; in this case further
algorithmic control shows little value. In cases with relatively low battery
capacity, algorithmic control with DRL improves energy management and cost
savings by a relevant margin. This result is further corroborated by our
simulation of a synthetic household. We conclude that prosumer households with
optimization potential would profit from DRL, thus benefiting also the full
electricity system and its decarbonization.

</details>


### [404] [Learning mechanical systems from real-world data using discrete forced Lagrangian dynamics](https://arxiv.org/abs/2505.20370)
*Martine Dyring Hansen,Elena Celledoni,Benjamin Kwanen Tapley*

Key words: 数据驱动、运动方程、系统识别、离散Lagrange-d'Alembert、神经网络

<details>
  <summary>Details</summary>

Main category: eess.SY

TL;DR: 该论文提出了一种从位置测量数据中学习机械系统运动方程的数据驱动方法，无需速度数据，适用于仅包含位置信息的系统识别任务。

Motivation: 在仅有位置信息的系统识别任务（如运动捕捉、像素数据或低分辨率跟踪）中，缺乏速度数据的限制激发了无需速度信息即可学习运动方程的需求。

Method: 基于离散Lagrange-d'Alembert原理和受迫离散Euler-Lagrange方程构建物理模型，将动态分解为保守和非保守部分，并使用前馈神经网络分别学习。

Result: 在合成和真实数据集上验证了方法的有效性，能够忠实重建并分离保守和受迫动态，产生可解释的、物理一致的预测。

Conclusion: 该方法通过物理基础模型和数据驱动学习，解决了仅位置数据的系统识别问题，并保持了哈密顿系统的辛结构完整性。

Abstract: We introduce a data-driven method for learning the equations of motion of
mechanical systems directly from position measurements, without requiring
access to velocity data. This is particularly relevant in system identification
tasks where only positional information is available, such as motion capture,
pixel data or low-resolution tracking. Our approach takes advantage of the
discrete Lagrange-d'Alembert principle and the forced discrete Euler-Lagrange
equations to construct a physically grounded model of the system's dynamics. We
decompose the dynamics into conservative and non-conservative components, which
are learned separately using feed-forward neural networks. In the absence of
external forces, our method reduces to a variational discretization of the
action principle naturally preserving the symplectic structure of the
underlying Hamiltonian system. We validate our approach on a variety of
synthetic and real-world datasets, demonstrating its effectiveness compared to
baseline methods. In particular, we apply our model to (1) measured human
motion data and (2) latent embeddings obtained via an autoencoder trained on
image sequences. We demonstrate that we can faithfully reconstruct and separate
both the conservative and forced dynamics, yielding interpretable and
physically consistent predictions.

</details>


### [405] [Multi-Mode Process Control Using Multi-Task Inverse Reinforcement Learning](https://arxiv.org/abs/2505.21026)
*Runze Lin,Junghui Chen,Biao Huang,Lei Xie,Hongye Su*

Key words: 逆强化学习, 多任务学习, 过程控制, 智能制造, 数字化转型

<details>
  <summary>Details</summary>

Main category: eess.SY

TL;DR: 该论文提出了一种结合逆强化学习和多任务学习的新型框架，用于数据驱动的多模式控制设计，解决了强化学习对准确数字孪生和奖励函数的依赖问题。

Motivation: 在工业4.0和智能制造时代，过程系统工程需要适应数字化转型。强化学习虽然提供了无模型的过程控制方法，但其应用受限于对准确数字孪生和设计良好的奖励函数的依赖。

Method: 论文引入了一种结合逆强化学习（IRL）和多任务学习的新框架，利用历史闭环数据作为专家演示提取最优奖励函数和控制策略，并通过潜在上下文变量区分模式以训练特定模式控制器。

Result: 在连续搅拌釜反应器和分批生物反应器的案例研究中，该框架成功验证了其在处理多模式数据和训练适应性控制器方面的有效性。

Conclusion: 该框架为数据驱动的多模式控制设计提供了有效的解决方案，并展示了在实际工业应用中的潜力。

Abstract: In the era of Industry 4.0 and smart manufacturing, process systems
engineering must adapt to digital transformation. While reinforcement learning
offers a model-free approach to process control, its applications are limited
by the dependence on accurate digital twins and well-designed reward functions.
To address these limitations, this paper introduces a novel framework that
integrates inverse reinforcement learning (IRL) with multi-task learning for
data-driven, multi-mode control design. Using historical closed-loop data as
expert demonstrations, IRL extracts optimal reward functions and control
policies. A latent-context variable is incorporated to distinguish modes,
enabling the training of mode-specific controllers. Case studies on a
continuous stirred tank reactor and a fed-batch bioreactor validate the
effectiveness of this framework in handling multi-mode data and training
adaptable controllers.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [406] [Fixed-Point Traps and Identity Emergence in Educational Feedback Systems](https://arxiv.org/abs/2505.21038)
*Faruk Alpay*

Key words: 范畴论, 教育系统, 身份涌现, 创造性收敛, 固定点代数

<details>
  <summary>Details</summary>

Main category: math.CT

TL;DR: 该论文通过形式范畴论证明了考试驱动的教育系统阻碍身份涌现和创造性收敛，利用代数框架定义了考试-成绩崩解系统（EGCS），并证明其在评估态射下会导致固定点代数的不存在，从而解释了创造力抑制和教育系统熵增的现象。

Motivation: 研究旨在揭示考试和成绩反馈如何通过评估态射导致学习者身份无法稳定，进而抑制创造性和研究活力。

Method: 使用Alpay代数II和III框架，定义EGCS为函子构造，通过评估态射E递归崩解学习动态φ，证明在崩解机制下不存在非平凡固定点代数μφ。

Result: 证明了考试驱动的系统会导致固定点陷阱，所有生成函子在符号涌现前被熵增折叠，解释了创造力抑制和结构性熵增。

Conclusion: 现代教育系统通过机构反馈机制阻碍身份涌现和自我形成，该研究首次用代数方法证明了这一制度性障碍。

Abstract: This paper presents a formal categorical proof that exam-driven educational
systems obstruct identity emergence and block creative convergence. Using the
framework of Alpay Algebra II and III, we define Exam-Grade Collapse Systems
(EGCS) as functorial constructs where learning dynamics $\varphi$ are
recursively collapsed by evaluative morphisms $E$. We prove that under such
collapse regimes, no nontrivial fixed-point algebra $\mu_\varphi$ can exist,
hence learner identity cannot stabilize. This creates a universal fixed-point
trap: all generative functors are entropically folded before symbolic emergence
occurs. Our model mathematically explains the creativity suppression, research
stagnation, and structural entropy loss induced by timed exams and grade-based
feedback. The results apply category theory to expose why modern educational
systems prevent {\phi}-emergence and block observer-invariant self-formation.
This work provides the first provable algebraic obstruction of identity
formation caused by institutional feedback mechanics.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [407] [Scheduling with Uncertain Holding Costs and its Application to Content Moderation](https://arxiv.org/abs/2505.21331)
*Caner Gocmen,Thodoris Lykouris,Deeksha Sinha,Wentao Weng*

Key words: 内容审核, 不确定性成本, 队列模型, 马尔可夫决策, OaRC算法

<details>
  <summary>Details</summary>

Main category: cs.DS

TL;DR: 这篇论文针对社交媒体平台内容审核中的不确定性持有成本问题，提出了一种新的基于索引的算法OaRC，优于传统方法，并通过理论分析和仿真验证了其优越性。

Motivation: 社交媒体平台内容审核的延迟成本与内容浏览量相关，而浏览量具有不确定性。传统基于瞬时成本和预期剩余成本的方法在此情景下效果不佳，因此需要开发一种能动态适应未来不确定性的新算法。

Method: 通过将每个任务视为马尔可夫滑雪租赁问题，作者开发了OaRC算法，该算法会随未来不确定性部分解决而动态调整任务优先级。

Result: 理论分析表明OaRC的遗憾上界为$	ilde{O}(L^{1.5}	qrt{N})$，证明了在大系统规模下的渐近最优性，且不受状态空间大小影响。仿真实验基于合成和真实数据集，结果显示OaRC优于传统方法。

Conclusion: OaRC算法在处理不确定持有成本的内容审核队列中表现优越，具有理论优越性和实际应用潜力。

Abstract: In content moderation for social media platforms, the cost of delaying the
review of a content is proportional to its view trajectory, which fluctuates
and is apriori unknown. Motivated by such uncertain holding costs, we consider
a queueing model where job states evolve based on a Markov chain with
state-dependent instantaneous holding costs. We demonstrate that in the
presence of such uncertain holding costs, the two canonical algorithmic
principles, instantaneous-cost ($c\mu$-rule) and expected-remaining-cost
($c\mu/\theta$-rule), are suboptimal. By viewing each job as a Markovian
ski-rental problem, we develop a new index-based algorithm,
Opportunity-adjusted Remaining Cost (OaRC), that adjusts to the opportunity of
serving jobs in the future when uncertainty partly resolves. We show that the
regret of OaRC scales as $\tilde{O}(L^{1.5}\sqrt{N})$, where $L$ is the maximum
length of a job's holding cost trajectory and $N$ is the system size. This
regret bound shows that OaRC achieves asymptotic optimality when the system
size $N$ scales to infinity. Moreover, its regret is independent of the
state-space size, which is a desirable property when job states contain
contextual information. We corroborate our results with an extensive simulation
study based on two holding cost patterns (online ads and user-generated
content) that arise in content moderation for social media platforms. Our
simulations based on synthetic and real datasets demonstrate that OaRC
consistently outperforms existing practice, which is based on the two canonical
algorithmic principles.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [408] [Towards Emotionally Consistent Text-Based Speech Editing: Introducing EmoCorrector and The ECD-TSE Dataset](https://arxiv.org/abs/2505.20341)
*Rui Liu,Pu Gao,Jiatian Xi,Berrak Sisman,Carlos Busso,Haizhou Li*

Key words: 文本语音编辑、情感一致性、检索增强生成、情感校正

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 该论文提出了一种名为EmoCorrector的文本语音编辑后校正方案，通过提取情感特征并检索匹配语音样本以解决现有方法的情感不一致问题。

Motivation: 现有文本语音编辑方法主要关注内容准确性和声学一致性，但常忽视由文本修改引起的情感变化或不一致问题。

Method: 采用检索增强生成（RAG）方法，提取编辑文本的情感特征，检索匹配情感的语音样本，合成符合目标情感且保留说话者身份和质量的语音。

Result: 在Emotion Correction Dataset for TSE（ECD-TSE）上的主观和客观实验表明，EmoCorrector显著提升了目标情感表达并解决了情感不一致问题。

Conclusion: EmoCorrector有效解决了当前文本语音编辑方法中的情感不一致问题，并通过ECD-TSE数据集支持情感一致性建模的训练与评估。

Abstract: Text-based speech editing (TSE) modifies speech using only text, eliminating
re-recording. However, existing TSE methods, mainly focus on the content
accuracy and acoustic consistency of synthetic speech segments, and often
overlook the emotional shifts or inconsistency issues introduced by text
changes. To address this issue, we propose EmoCorrector, a novel
post-correction scheme for TSE. EmoCorrector leverages Retrieval-Augmented
Generation (RAG) by extracting the edited text's emotional features, retrieving
speech samples with matching emotions, and synthesizing speech that aligns with
the desired emotion while preserving the speaker's identity and quality. To
support the training and evaluation of emotional consistency modeling in TSE,
we pioneer the benchmarking Emotion Correction Dataset for TSE (ECD-TSE). The
prominent aspect of ECD-TSE is its inclusion of $<$text, speech$>$ paired data
featuring diverse text variations and a range of emotional expressions.
Subjective and objective experiments and comprehensive analysis on ECD-TSE
confirm that EmoCorrector significantly enhances the expression of intended
emotion while addressing emotion inconsistency limitations in current TSE
methods. Code and audio examples are available at
https://github.com/AI-S2-Lab/EmoCorrector.

</details>


### [409] [Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction](https://arxiv.org/abs/2505.20635)
*Zexu Pan,Shengkui Zhao,Tingting Wang,Kun Zhou,Yukun Ma,Chong Zhang,Bin Ma*

Key words: 音频-视觉说话人提取、注意力机制、多说话人场景、鲁棒性

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 提出了一种即插即用的说话人注意力模块，用于处理多说话人场景中的视觉线索，提升说话人提取的准确性。

Motivation: 实际场景中，屏幕上常出现多个说话人的面孔，提供额外的说话人活动线索，利用这些线索可以提升说话人分离的准确性。

Method: 在AV-DPRNN和AV-TFGridNet模型中引入即插即用的说话人注意力模块，处理多说话人面孔信息。

Result: 在VoxCeleb2和MISP数据集上表现优于基线，且在LRS2和LRS3数据集上验证了方法的鲁棒性和泛化性。

Conclusion: 该方法在复杂多说话人环境中显著提升了说话人提取的准确性。

Abstract: Audio-visual speaker extraction isolates a target speaker's speech from a
mixture speech signal conditioned on a visual cue, typically using the target
speaker's face recording. However, in real-world scenarios, other co-occurring
faces are often present on-screen, providing valuable speaker activity cues in
the scene. In this work, we introduce a plug-and-play inter-speaker attention
module to process these flexible numbers of co-occurring faces, allowing for
more accurate speaker extraction in complex multi-person environments. We
integrate our module into two prominent models: the AV-DPRNN and the
state-of-the-art AV-TFGridNet. Extensive experiments on diverse datasets,
including the highly overlapped VoxCeleb2 and sparsely overlapped MISP,
demonstrate that our approach consistently outperforms baselines. Furthermore,
cross-dataset evaluations on LRS2 and LRS3 confirm the robustness and
generalizability of our method.

</details>


### [410] [PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems](https://arxiv.org/abs/2505.21230)
*Nima Sedghiyeh,Sara Sadeghi,Reza Khodadadi,Farzin Kashani,Omid Aghdaei,Somayeh Rahimi,Mohammad Sadegh Safari*

Key words: ASR, 波斯语, 低资源语言, 评估基准, 偏差分析

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 该论文提出了波斯语语音识别基准（PSRB），用于评估低资源语言（如波斯语）的ASR系统性能，分析了10种ASR模型的表现及偏差，并提出了一种新的加权替换错误指标以提高评估鲁棒性。研究发现ASR模型在标准波斯语上表现良好，但在方言、儿童语音等方面表现较差。

Motivation: 由于低资源语言（如波斯语）的ASR系统评估存在挑战，PSRB旨在填补这一空白，通过多样化的语言和声学条件提供全面评估。

Method: 研究评估了10种ASR模型（包括商业和开源模型），深入分析转录错误并提出加权替换错误的指标。

Result: 发现ASR模型在标准波斯语表现良好，但面对方言和儿童语音时表现较差。新指标提升了评估的鲁棒性。

Conclusion: 研究强调了优化训练数据和减少偏差的必要性，PSRB为波斯语ASR研究提供了资源，并可作为其他低资源语言的基准开发框架。

Abstract: Although Automatic Speech Recognition (ASR) systems have become an integral
part of modern technology, their evaluation remains challenging, particularly
for low-resource languages such as Persian. This paper introduces Persian
Speech Recognition Benchmark(PSRB), a comprehensive benchmark designed to
address this gap by incorporating diverse linguistic and acoustic conditions.
We evaluate ten ASR systems, including state-of-the-art commercial and
open-source models, to examine performance variations and inherent biases.
Additionally, we conduct an in-depth analysis of Persian ASR transcriptions,
identifying key error types and proposing a novel metric that weights
substitution errors. This metric enhances evaluation robustness by reducing the
impact of minor and partial errors, thereby improving the precision of
performance assessment. Our findings indicate that while ASR models generally
perform well on standard Persian, they struggle with regional accents,
children's speech, and specific linguistic challenges. These results highlight
the necessity of fine-tuning and incorporating diverse, representative training
datasets to mitigate biases and enhance overall ASR performance. PSRB provides
a valuable resource for advancing ASR research in Persian and serves as a
framework for developing benchmarks in other low-resource languages. A subset
of the PSRB dataset is publicly available at
https://huggingface.co/datasets/PartAI/PSRB.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [411] [VibE-SVC: Vibrato Extraction with High-frequency F0 Contour for Singing Voice Conversion](https://arxiv.org/abs/2505.20794)
*Joon-Seung Choi,Dong-Min Byun,Hyung-Seok Oh,Seong-Whan Lee*

Key words: 歌唱声音转换，颤音控制，离散小波变换，F0轮廓，风格转换

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 该论文提出了VibESVC模型，用于通过离散小波变换显式提取和操控颤音，实现可控的歌唱声音转换，以提升表达灵活性并保留说话人相似性。

Motivation: 颤音在歌唱中对于表达情感和增强音乐深度至关重要，但由于其动态特性，建模和控制颤音在歌唱声音转换中具有挑战性，因此需要一种新方法。

Method: 提出VibESVC模型，利用离散小波变换显式分解F0轮廓为频率成分，从而精确操控颤音，区别于以往隐式建模的方法。

Result: 实验表明，VibESVC能有效转换歌唱风格并保留说话人相似性，主客观评估均证实其高质量转换效果。

Conclusion: VibESVC通过显式颤音控制提升了歌唱声音转换的灵活性和质量，为未来研究提供了新方向。

Abstract: Controlling singing style is crucial for achieving an expressive and natural
singing voice. Among the various style factors, vibrato plays a key role in
conveying emotions and enhancing musical depth. However, modeling vibrato
remains challenging due to its dynamic nature, making it difficult to control
in singing voice conversion. To address this, we propose VibESVC, a
controllable singing voice conversion model that explicitly extracts and
manipulates vibrato using discrete wavelet transform. Unlike previous methods
that model vibrato implicitly, our approach decomposes the F0 contour into
frequency components, enabling precise transfer. This allows vibrato control
for enhanced flexibility. Experimental results show that VibE-SVC effectively
transforms singing styles while preserving speaker similarity. Both subjective
and objective evaluations confirm high-quality conversion.

</details>


### [412] [Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction and Style Direction Adjustment for Expressive Text-to-Speech](https://arxiv.org/abs/2505.20868)
*Nam-Gyu Kim,Deok-Hyeon Cho,Seung-Bin Kim,Seong-Whan Lee*

Key words: 语音合成、风格迁移、发音区域、方向调整

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: Spotlight-TTS通过基于发音区域的感知风格提取和方向调整，提升了语音合成的表达力和质量，优于基线模型。

Motivation: 当前基于风格嵌入的TTS方法在高质量表达语音合成上仍有挑战，需针对性改进。

Method: 提出Spotlight-TTS，采用发音区域感知的风格提取和方向调整方法。

Result: 实验表明，在表达力、语音质量和风格迁移能力上优于基线。

Conclusion: Spotlight-TTS通过针对性风格优化提升了TTS表现。

Abstract: Recent advances in expressive text-to-speech (TTS) have introduced diverse
methods based on style embedding extracted from reference speech. However,
synthesizing high-quality expressive speech remains challenging. We propose
Spotlight-TTS, which exclusively emphasizes style via voiced-aware style
extraction and style direction adjustment. Voiced-aware style extraction
focuses on voiced regions highly related to style while maintaining continuity
across different speech regions to improve expressiveness. We adjust the
direction of the extracted style for optimal integration into the TTS model,
which improves speech quality. Experimental results demonstrate that
Spotlight-TTS achieves superior performance compared to baseline models in
terms of expressiveness, overall speech quality, and style transfer capability.
Our audio samples are publicly available.

</details>


### [413] [Training Articulatory Inversion Models for Inter-Speaker Consistency](https://arxiv.org/abs/2505.20529)
*Charles McGhee,Mark J. F. Gales,Kate M. Knill*

Key words: Acoustic-to-Articulatory Inversion, Self-Supervised Learning, cross-speaker consistency, English, Russian

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 论文研究了自监督学习（SSL）模型在单人和多人语音数据上训练后，对英语和俄语的发音目标是否具有跨说话者一致性，并提出了一种新的评估方法和改进训练方法。

Motivation: 探索自监督学习模型在语音到发音转换任务中的跨说话者一致性，并解决发音预测的挑战。

Method: 通过最小对集合提取发音目标，并提出一种仅使用语音数据的训练方法以提高跨说话者一致性。

Result: 展示了SSL-adapted模型在单人和多人数据上的表现，并验证了新训练方法的有效性。

Conclusion: 模型在跨说话者一致性方面表现良好，新方法进一步提高了性能。

Abstract: Acoustic-to-Articulatory Inversion (AAI) attempts to model the inverse
mapping from speech to articulation. Exact articulatory prediction from speech
alone may be impossible, as speakers can choose different forms of articulation
seemingly without reference to their vocal tract structure. However, once a
speaker has selected an articulatory form, their productions vary minimally.
Recent works in AAI have proposed adapting Self-Supervised Learning (SSL)
models to single-speaker datasets, claiming that these single-speaker models
provide a universal articulatory template. In this paper, we investigate
whether SSL-adapted models trained on single and multi-speaker data produce
articulatory targets which are consistent across speaker identities for English
and Russian. We do this through the use of a novel evaluation method which
extracts articulatory targets using minimal pair sets. We also present a
training method which can improve inter-speaker consistency using only speech
data.

</details>


### [414] [Hybrid Disagreement-Diversity Active Learning for Bioacoustic Sound Event Detection](https://arxiv.org/abs/2505.20956)
*Shiqi Zhang,Tuomas Virtanen*

Key words: 生物声学事件检测, 主动学习, 数据稀疏, 类别不平衡, 濒危物种

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 论文提出了一种称为MFFT的主动学习方法，用于解决生物声学事件检测中的数据稀疏和标注预算有限的问题。该方法在冷启动和稀有物种检测中表现优异，仅需少量标注即可接近全监督性能。

Motivation: 生物声学事件检测在生物多样性保护中至关重要，但面临数据标注少、事件稀疏、物种多样性和类别不平衡等挑战。如何在有限标注预算下高效解决这些问题成为研究动机。

Method: 采用MFFT（基于委员会投票分歧和多样性分析的主动学习方法），并优化一个专为评估主动学习算法设计的生物声学数据集。

Result: MFFT在冷启动和热启动场景中分别达到68%和71%的mAP（接近全监督的75%），且仅需2.3%的标注量。尤其在冷启动和稀有物种检测中表现突出。

Conclusion: MFFT在有限标注条件下表现优异，对濒危物种监测具有实际价值，证明了其在生物声学事件检测中的高效性。

Abstract: Bioacoustic sound event detection (BioSED) is crucial for biodiversity
conservation but faces practical challenges during model development and
training: limited amounts of annotated data, sparse events, species diversity,
and class imbalance. To address these challenges efficiently with a limited
labeling budget, we apply the mismatch-first farthest-traversal (MFFT), an
active learning method integrating committee voting disagreement and diversity
analysis. We also refine an existing BioSED dataset specifically for evaluating
active learning algorithms. Experimental results demonstrate that MFFT achieves
a mAP of 68% when cold-starting and 71% when warm-starting (which is close to
the fully-supervised mAP of 75%) while using only 2.3% of the annotations.
Notably, MFFT excels in cold-start scenarios and with rare species, which are
critical for monitoring endangered species, demonstrating its practical value.

</details>


### [415] [Efficient and Microphone-Fault-Tolerant 3D Sound Source Localization](https://arxiv.org/abs/2505.20961)
*Yiyuan Yang,Shitong Xu,Niki Trigoni,Andrew Markham*

Key words: 声源定位,3D定位,稀疏交叉注意力,自适应信号一致性,容错性

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 提出一种新型3D声源定位框架，通过稀疏交叉注意力、预训练和自适应信号一致性度量，实现高效准确的定位，减少麦克风输入需求，并具备故障容忍能力。

Motivation: 现有声源定位方法计算成本高、校准要求精确，难以在动态或资源受限环境中部署，需要更高效、鲁棒性强的解决方案。

Method: 采用稀疏交叉注意力机制、预训练技术和自适应信号一致性度量，构建3D声源定位框架。

Result: 在减少麦克风输入的情况下实现高效准确定位，且对不可靠或未知麦克风位置具有容错能力。

Conclusion: 该工作平衡性能与效率，提升现实场景中的鲁棒性，为多源定位提供了可扩展方案。

Abstract: Sound source localization (SSL) is a critical technology for determining the
position of sound sources in complex environments. However, existing methods
face challenges such as high computational costs and precise calibration
requirements, limiting their deployment in dynamic or resource-constrained
environments. This paper introduces a novel 3D SSL framework, which uses sparse
cross-attention, pretraining, and adaptive signal coherence metrics, to achieve
accurate and computationally efficient localization with fewer input
microphones. The framework is also fault-tolerant to unreliable or even unknown
microphone position inputs, ensuring its applicability in real-world scenarios.
Preliminary experiments demonstrate its scalability for multi-source
localization without requiring additional hardware. This work advances SSL by
balancing the model's performance and efficiency and improving its robustness
for real-world scenarios.

</details>


### [416] [MelodySim: Measuring Melody-aware Music Similarity for Plagiarism Detection](https://arxiv.org/abs/2505.20979)
*Tongyu Lu,Charlotta-Marlena Geist,Jan Melechovsky,Abhinaba Roy,Dorien Herremans*

Key words: 旋律相似性, 抄袭检测, 音乐信息检索, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 提出了一种名为MelodySim的旋律感知音乐相似性模型和数据集，用于抄袭检测，通过数据增强和深度学习模型实现高精度的旋律相似性识别。

Motivation: 现有音乐抄袭检测缺乏对旋律相似性的专注，MelodySim旨在填补这一空白，提供更精确的旋律相似性识别方法。

Method: 1. 通过数据增强（如音符分割、琶音生成等）构建旋律相似性数据集；2. 使用MERT编码器和三重神经网络训练模型。

Result: 模型在MelodySim测试集上达到高精度，用户研究验证了数据集的旋律相似性有效性。

Conclusion: MelodySim为音乐抄袭检测提供了有效的旋律感知解决方案，数据集和模型均表现出色。

Abstract: We propose MelodySim, a melody-aware music similarity model and dataset for
plagiarism detection. First, we introduce a novel method to construct a dataset
with focus on melodic similarity. By augmenting Slakh2100; an existing MIDI
dataset, we generate variations of each piece while preserving the melody
through modifications such as note splitting, arpeggiation, minor track dropout
(excluding bass), and re-instrumentation. A user study confirms that positive
pairs indeed contain similar melodies, with other musical tracks significantly
changed. Second, we develop a segment-wise melodic-similarity detection model
that uses a MERT encoder and applies a triplet neural network to capture
melodic similarity. The resultant decision matrix highlights where plagiarism
might occur. Our model achieves high accuracy on the MelodySim test set.

</details>


### [417] [Text-Queried Audio Source Separation via Hierarchical Modeling](https://arxiv.org/abs/2505.21025)
*Xinlei Yin,Xiulian Peng,Xue Jiang,Zhiwei Xiong,Yan Lu*

Key words: 音频源分离, 自然语言查询, 分层分解, 语义对齐, 声学重建

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 该论文提出了一种分层分解框架HSM-TSS，通过解耦全局-局部语义引导特征分离和结构保持的声学重建，解决了现有方法在联合建模声学-文本对齐和语义感知分离时的挑战。该方法利用双阶段机制和指令处理管道，在数据高效训练的同时实现了最先进的分离性能。

Motivation: 现有方法在联合建模声学-文本对齐和语义感知分离时面临困难，且依赖于大规模精确标注的训练数据。为了解决这些问题，论文提出了一个分层分解框架。

Method: 提出了HSM-TSS框架，包括双阶段的语义分离：全局语义分离（通过Q-Audio架构对齐音频和文本）和局部语义分离（基于AudioMAE特征），以及声学重建。此外，还引入了指令处理管道解析文本查询。

Result: 该方法在数据高效训练的同时，达到了最先进的分离性能，并在复杂听觉场景中保持了与查询的语义一致性。

Conclusion: HSM-TSS框架通过分层分解和双阶段分离机制，有效解决了现有挑战，提升了语义一致性和分离性能。

Abstract: Target audio source separation with natural language queries presents a
promising paradigm for extracting arbitrary audio events through arbitrary text
descriptions. Existing methods mainly face two challenges, the difficulty in
jointly modeling acoustic-textual alignment and semantic-aware separation
within a blindly-learned single-stage architecture, and the reliance on
large-scale accurately-labeled training data to compensate for inefficient
cross-modal learning and separation. To address these challenges, we propose a
hierarchical decomposition framework, HSM-TSS, that decouples the task into
global-local semantic-guided feature separation and structure-preserving
acoustic reconstruction. Our approach introduces a dual-stage mechanism for
semantic separation, operating on distinct global and local semantic feature
spaces. We first perform global-semantic separation through a global semantic
feature space aligned with text queries. A Q-Audio architecture is employed to
align audio and text modalities, serving as pretrained global-semantic
encoders. Conditioned on the predicted global feature, we then perform the
second-stage local-semantic separation on AudioMAE features that preserve
time-frequency structures, followed by acoustic reconstruction. We also propose
an instruction processing pipeline to parse arbitrary text queries into
structured operations, extraction or removal, coupled with audio descriptions,
enabling flexible sound manipulation. Our method achieves state-of-the-art
separation performance with data-efficient training while maintaining superior
semantic consistency with queries in complex auditory scenes.

</details>


### [418] [Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation](https://arxiv.org/abs/2505.20745)
*Jingping Nie,Dung T. Tran,Karan Thakkar,Vasudha Kowtha,John Huang,Carlos Avendano,Erdrin Azemi,Vikramjit Mitra*

Key words: 心音听诊、自监督学习、基础模型、心率估计、声学表示

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 研究了自监督声学表示基础模型（FMs）在心音听诊中的应用，发现预训练FMs的表现与基线方法相当，其中自建CLAP模型的音频编码器在心率估计中表现最佳。

Motivation: 探索自监督声学表示基础模型在心音听诊中的应用潜力，填补现有研究对预训练FMs编码听诊信息程度的空白。

Method: 使用公开的PCG数据集和心率估计模型，对六种声学表示FM（HuBERT、wav2vec2、wavLM、Whisper、CLAP及自建CLAP模型）进行分层分析，并与基线方法对比。

Result: 预训练FMs整体表现与基线方法相当，自建CLAP模型的音频编码器在心率估计中表现最优，MAE更低。

Conclusion: 自监督声学表示基础模型在心音听诊中具有潜力，尤其是自建CLAP模型在心率估计任务中表现出色。

Abstract: Auscultation, particularly heart sound, is a non-invasive technique that
provides essential vital sign information. Recently, self-supervised acoustic
representation foundation models (FMs) have been proposed to offer insights
into acoustics-based vital signs. However, there has been little exploration of
the extent to which auscultation is encoded in these pre-trained FM
representations. In this work, using a publicly available phonocardiogram (PCG)
dataset and a heart rate (HR) estimation model, we conduct a layer-wise
investigation of six acoustic representation FMs: HuBERT, wav2vec2, wavLM,
Whisper, Contrastive Language-Audio Pretraining (CLAP), and an in-house CLAP
model. Additionally, we implement the baseline method from Nie et al., 2024
(which relies on acoustic features) and show that overall, representation
vectors from pre-trained foundation models (FMs) offer comparable performance
to the baseline. Notably, HR estimation using the representations from the
audio encoder of the in-house CLAP model outperforms the results obtained from
the baseline, achieving a lower mean absolute error (MAE) across various
train/validation/test splits despite the domain mismatch.

</details>


### [419] [Model as Loss: A Self-Consistent Training Paradigm](https://arxiv.org/abs/2505.21156)
*Saisamarth Rajesh Phaye,Milos Cernak,Andrew Harper*

Key words: 语音增强, 损失函数, 自一致性, 编码器, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 提出了一种新的训练范式'Model as Loss'，利用同一模型的编码器作为损失函数，优化语音增强任务的性能。

Motivation: 传统语音增强方法依赖手工设计的损失函数或预训练的深度特征损失，往往无法捕捉细微的信号特性。

Method: 使用同一模型的编码器作为损失函数，通过任务特定的特征空间指导解码器的训练，确保输出的自一致性。

Result: 该方法在标准语音增强基准测试中表现优于预训练深度特征损失，提供更好的感知质量和泛化能力。

Conclusion: 'Model as Loss'是一种有效的语音增强训练范式，能更好地捕捉信号特性并提升性能。

Abstract: Conventional methods for speech enhancement rely on handcrafted loss
functions (e.g., time or frequency domain losses) or deep feature losses (e.g.,
using WavLM or wav2vec), which often fail to capture subtle signal properties
essential for optimal performance. To address this, we propose Model as Loss, a
novel training paradigm that utilizes the encoder from the same model as a loss
function to guide the training.
  The Model as Loss paradigm leverages the encoder's task-specific feature
space, optimizing the decoder to produce output consistent with perceptual and
task-relevant characteristics of the clean signal. By using the encoder's
learned features as a loss function, this framework enforces self-consistency
between the clean reference speech and the enhanced model output. Our approach
outperforms pre-trained deep feature losses on standard speech enhancement
benchmarks, offering better perceptual quality and robust generalization to
both in-domain and out-of-domain datasets.

</details>


### [420] [Towards Robust Automated Perceptual Voice Quality Assessment with Deep Learning](https://arxiv.org/abs/2505.21356)
*Whenty Ariyanti,Kuan-Yu Chen,Sabato Marco Siniscalchi,Hsin-Min Wang,Yu Tsao*

Key words: 语音质量评估,深度学习,注意力机制,语音基础模型,抗噪性

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 论文提出了一种基于深度学习的框架VOQANet及增强版VOQANet+，用于自动评估语音质量，结合基础语音模型和手工声学特征，显著提升了评估的客观性和鲁棒性。

Motivation: 传统语音质量评估依赖专家主观判断，存在评分者间差异，需自动化、客观化的解决方案。

Method: VOQANet利用注意力机制和语音基础模型提取高层声学特征；VOQANet+进一步整合手工声学特征（如基频微扰、幅度微扰等）以增强鲁棒性和可解释性。

Result: VOQANet在RMSE和PCC上优于基线方法，VOQANet+在噪声环境下表现更稳健。句子级输入效果优于元音级输入。

Conclusion: 结合语音基础模型与领域声学特征可提升模型的解释性和抗噪能力，适用于实际临床和远程医疗场景。

Abstract: Objective: Perceptual voice quality assessment plays a critical role in
diagnosing and monitoring voice disorders by providing standardized evaluation
of vocal function. Traditionally, this process relies on expert raters
utilizing standard scales, such as the Consensus Auditory-Perceptual Evaluation
of Voice (CAPE-V) and Grade, Roughness, Breathiness, Asthenia, and Strain
(GRBAS). However, these metrics are inherently subjective and susceptible to
inter-rater variability, motivating the need for automated and objective
assessment methods. Methods: We propose Voice Quality Assessment Network
(VOQANet), a deep learning-based framework with an attention mechanism that
leverages a Speech Foundation Model (SFM) to capture high-level acoustic and
prosodic information from raw speech. To enhance robustness and
interpretability, we present VOQANet+, which integrates handcrafted acoustic
features such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM
embeddings. Results: Sentence-based input yields stronger performance than
vowel-based input, especially at the patient level. VOQANet consistently
outperforms baseline methods in RMSE and PCC, while VOQANet+ performs even
better and maintains robustness under noisy conditions. Conclusion: Combining
SFM embeddings with domain-informed acoustic features improves interpretability
and resilience. Significance: VOQANet+ shows strong potential for deployment in
real-world and telehealth settings, addressing the limitations of subjective
perceptual assessments with an interpretable and noise-resilient solution.

</details>


### [421] [VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin](https://arxiv.org/abs/2505.21445)
*Zhiqi Ai,Meixuan Bao,Zhiyong Chen,Zhi Yang,Xinnuo Li,Shugong Xu*

Key words: 说话人老化, 纵向数据集, 语音验证系统, 年龄组, 性别

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 该论文提出了VoxAging数据集，研究说话人老化对语音验证系统的影响，分析了年龄组和性别等因素的作用。

Motivation: 由于缺乏长期大规模的纵向数据，说话人老化研究面临挑战。

Method: 收集了293名说话人多年（最长17年）的每周录音数据。

Result: VoxAging数据集为研究说话人老化及其对语音验证系统的影响提供了基础。

Conclusion: 年龄组和性别对说话人老化研究有显著影响。

Abstract: The performance of speaker verification systems is adversely affected by
speaker aging. However, due to challenges in data collection, particularly the
lack of sustained and large-scale longitudinal data for individuals, research
on speaker aging remains difficult. In this paper, we present VoxAging, a
large-scale longitudinal dataset collected from 293 speakers (226 English
speakers and 67 Mandarin speakers) over several years, with the longest time
span reaching 17 years (approximately 900 weeks). For each speaker, the data
were recorded at weekly intervals. We studied the phenomenon of speaker aging
and its effects on advanced speaker verification systems, analyzed individual
speaker aging processes, and explored the impact of factors such as age group
and gender on speaker aging research.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [422] [Semi-supervised Clustering Through Representation Learning of Large-scale EHR Data](https://arxiv.org/abs/2505.20731)
*Linshanshan Wang,Mengyan Li,Zongqi Xia,Molei Liu,Tianxi Cai*

Key words: 电子健康记录, 半监督学习, 表示学习, 多发性硬化症, 预测建模

<details>
  <summary>Details</summary>

Main category: stat.ME

TL;DR: SCORE是一种半监督表示学习框架，利用PALM模型和预训练编码嵌入处理EHR的稀疏性和高维度问题，结合混合EM-GVA算法提升预测准确性并减少对标记数据的依赖。

Motivation: 电子健康记录（EHR）数据具有稀疏性、异质性和高维度特点，且缺乏标准化事实，难以直接用于建模。SCORE旨在通过半监督学习解决这些问题，提升预测性能。

Method: 使用PALM模型和预训练编码嵌入提取患者表型，结合混合EM-GVA算法处理大规模数据，理论分析了GVA误差和收敛性。

Result: 模拟和实际应用（多发性硬化症预测）显示，SCORE在有限样本中表现优于现有方法，嵌入更具信息性和预测性。

Conclusion: SCORE通过合并未标记数据提升模型性能，减少对标记数据的依赖，适用于稀疏EHR数据的个性化医疗研究。

Abstract: Electronic Health Records (EHR) offer rich real-world data for personalized
medicine, providing insights into disease progression, treatment responses, and
patient outcomes. However, their sparsity, heterogeneity, and high
dimensionality make them difficult to model, while the lack of standardized
ground truth further complicates predictive modeling. To address these
challenges, we propose SCORE, a semi-supervised representation learning
framework that captures multi-domain disease profiles through patient
embeddings. SCORE employs a Poisson-Adapted Latent factor Mixture (PALM) Model
with pre-trained code embeddings to characterize codified features and extract
meaningful patient phenotypes and embeddings. To handle the computational
challenges of large-scale data, it introduces a hybrid Expectation-Maximization
(EM) and Gaussian Variational Approximation (GVA) algorithm, leveraging limited
labeled data to refine estimates on a vast pool of unlabeled samples. We
theoretically establish the convergence of this hybrid approach, quantify GVA
errors, and derive SCORE's error rate under diverging embedding dimensions. Our
analysis shows that incorporating unlabeled data enhances accuracy and reduces
sensitivity to label scarcity. Extensive simulations confirm SCORE's superior
finite-sample performance over existing methods. Finally, we apply SCORE to
predict disability status for patients with multiple sclerosis (MS) using
partially labeled EHR data, demonstrating that it produces more informative and
predictive patient embeddings for multiple MS-related conditions compared to
existing approaches.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [423] [Let's Get You Hired: A Job Seeker's Perspective on Multi-Agent Recruitment Systems for Explaining Hiring Decisions](https://arxiv.org/abs/2505.20312)
*Aditya Bhattacharya,Katrien Verbert*

Key words: 招聘透明度,多智能体AI,大语言模型,用户中心设计,可解释AI

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 论文提出了一个基于多智能体AI的系统，利用大语言模型（LLMs）为求职者提供招聘过程中的透明指导。通过用户中心设计方法，研究分两阶段进行探索性研究，并最终通过20名求职者的深度访谈验证系统原型。结果显示系统在可操作性、可信度和公平性上优于传统方法。

Motivation: 解决传统招聘方法缺乏透明度的问题，无论是人工招聘还是黑箱ATS系统，求职者通常无法获得充分的决策依据。

Method: 采用迭代式用户中心设计方法，先进行两阶段探索性研究（4名求职者），随后通过20名求职者的深度访谈评估系统原型。

Result: 参与者认为多智能体招聘系统在可操作性、可信度和公平性上显著优于传统方法。研究还揭示了影响用户体验的深层次因素。

Conclusion: 研究结果为构建用户对齐的多智能体可解释AI系统提供了广泛的设计启示。

Abstract: During job recruitment, traditional applicant selection methods often lack
transparency. Candidates are rarely given sufficient justifications for
recruiting decisions, whether they are made manually by human recruiters or
through the use of black-box Applicant Tracking Systems (ATS). To address this
problem, our work introduces a multi-agent AI system that uses Large Language
Models (LLMs) to guide job seekers during the recruitment process. Using an
iterative user-centric design approach, we first conducted a two-phased
exploratory study with four active job seekers to inform the design and
development of the system. Subsequently, we conducted an in-depth, qualitative
user study with 20 active job seekers through individual one-to-one interviews
to evaluate the developed prototype. The results of our evaluation demonstrate
that participants perceived our multi-agent recruitment system as significantly
more actionable, trustworthy, and fair compared to traditional methods. Our
study further helped us uncover in-depth insights into factors contributing to
these perceived user experiences. Drawing from these insights, we offer broader
design implications for building user-aligned, multi-agent explainable AI
systems across diverse domains.

</details>


### [424] [Cultural Awareness in Vision-Language Models: A Cross-Country Exploration](https://arxiv.org/abs/2505.20326)
*Avinash Madasu,Vasudev Lal,Phillip Howard*

Key words: 视觉语言模型,文化偏见,检索任务,社会刻板印象

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 本文提出了一种新框架，评估了视觉语言模型（VLMs）在多文化背景中对种族、性别和生理特征的偏见编码。通过三项检索任务，揭示了模型中的潜在刻板印象关联。

Motivation: 研究旨在理解VLMs在不同文化背景中如何处理和编码与种族、性别及生理特征相关的偏见，填补了对模型内部偏见的认知空白。

Method: 采用三项检索任务：（1）种族与国家关联性分析；（2）特征描述与国家关联性分析；（3）生理特征与国家关联性分析。通过提示与图片配对，评估模型的偏见表现。

Result: 研究发现VLMs存在显著偏见，揭示了视觉表征可能无意中强化社会刻板印象的现象。

Conclusion: 论文强调了系统性评估和解决VLMs文化偏见的重要性。

Abstract: Vision-Language Models (VLMs) are increasingly deployed in diverse cultural
contexts, yet their internal biases remain poorly understood. In this work, we
propose a novel framework to systematically evaluate how VLMs encode cultural
differences and biases related to race, gender, and physical traits across
countries. We introduce three retrieval-based tasks: (1) Race to Country
retrieval, which examines the association between individuals from specific
racial groups (East Asian, White, Middle Eastern, Latino, South Asian, and
Black) and different countries; (2) Personal Traits to Country retrieval, where
images are paired with trait-based prompts (e.g., Smart, Honest, Criminal,
Violent) to investigate potential stereotypical associations; and (3) Physical
Characteristics to Country retrieval, focusing on visual attributes like
skinny, young, obese, and old to explore how physical appearances are
culturally linked to nations. Our findings reveal persistent biases in VLMs,
highlighting how visual representations may inadvertently reinforce societal
stereotypes.

</details>


### [425] [Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)](https://arxiv.org/abs/2505.21091)
*Anna Neumann,Elisabeth Kirsten,Muhammad Bilal Zafar,Jatinder Singh*

Key words: 大型语言模型,系统提示,透明度,偏见,AI审计

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 该研究发现系统提示在大型语言模型（LLM）中可能导致未被注意的偏见和副作用，且其不透明性使用户无法检测或纠正这些偏差。

Motivation: 系统提示的不透明性及其潜在的副作用引发了对模型输出如何受信息位置影响的关注。

Method: 比较了六种商用LLM中系统提示与用户提示在处理50个人口统计群体时的表现。

Result: 发现了显著的偏见，表现为用户代表性和决策场景的差异。

Conclusion: 系统提示分析应纳入AI审计流程，以减少潜在的偏见和危害。

Abstract: System prompts in Large Language Models (LLMs) are predefined directives that
guide model behaviour, taking precedence over user inputs in text processing
and generation. LLM deployers increasingly use them to ensure consistent
responses across contexts. While model providers set a foundation of system
prompts, deployers and third-party developers can append additional prompts
without visibility into others' additions, while this layered implementation
remains entirely hidden from end-users. As system prompts become more complex,
they can directly or indirectly introduce unaccounted for side effects. This
lack of transparency raises fundamental questions about how the position of
information in different directives shapes model outputs. As such, this work
examines how the placement of information affects model behaviour. To this end,
we compare how models process demographic information in system versus user
prompts across six commercially available LLMs and 50 demographic groups. Our
analysis reveals significant biases, manifesting in differences in user
representation and decision-making scenarios. Since these variations stem from
inaccessible and opaque system-level configurations, they risk
representational, allocative and potential other biases and downstream harms
beyond the user's ability to detect or correct. Our findings draw attention to
these critical issues, which have the potential to perpetuate harms if left
unexamined. Further, we argue that system prompt analysis must be incorporated
into AI auditing processes, particularly as customisable system prompts become
increasingly prevalent in commercial AI deployments.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [426] [Stopping Criteria for Value Iteration on Concurrent Stochastic Reachability and Safety Games](https://arxiv.org/abs/2505.21087)
*Marta Grobelna,Jan Křetínský,Maximilian Weininger*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.LO

TL;DR: 论文提出了一种针对带有可达性和安全性目标的图论双人零和并发随机博弈（CSGs）的有界值迭代方法，解决了传统值迭代方法在终止准则上缺乏精度保证的问题。

Motivation: 传统值迭代（VI）方法在CSGs中虽性能优越但缺乏精度保证，因此作者提出有界VI以提供更可靠的近似解。

Method: 结合标准VI，引入收敛的超近似序列，当超近似与欠近似接近时终止。

Result: 提出的有界VI方法在CSGs中实现了精度保证的近似解。

Conclusion: 有界VI为CSGs提供了一种实用且理论可靠的值迭代方法。

Abstract: We consider two-player zero-sum concurrent stochastic games (CSGs) played on
graphs with reachability and safety objectives. These include degenerate
classes such as Markov decision processes or turn-based stochastic games, which
can be solved by linear or quadratic programming; however, in practice, value
iteration (VI) outperforms the other approaches and is the most implemented
method. Similarly, for CSGs, this practical performance makes VI an attractive
alternative to the standard theoretical solution via the existential theory of
reals.
  VI starts with an under-approximation of the sought values for each state and
iteratively updates them, traditionally terminating once two consecutive
approximations are $\epsilon$-close. However, this stopping criterion lacks
guarantees on the precision of the approximation, which is the goal of this
work. We provide bounded (a.k.a. interval) VI for CSGs: it complements standard
VI with a converging sequence of over-approximations and terminates once the
over- and under-approximations are $\epsilon$-close.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [427] [Wideband RF Radiance Field Modeling Using Frequency-embedded 3D Gaussian Splatting](https://arxiv.org/abs/2505.20714)
*Zechen Li,Lanqing Yang,Yiheng Bian,Hao Pan,Yongjian Fu,Yezhou Wang,Yi-Chao Chen,Guangtao Xue,Ju Ren*

Key words: 3D高斯溅射、宽带射频、辐射场建模、电磁波传播、功率角谱

<details>
  <summary>Details</summary>

Main category: cs.NI

TL;DR: 提出了一种创新的频率嵌入3D高斯溅射算法，用于宽带射频辐射场建模，解决了现有方法仅限于单频率的问题，并在实验中验证了其有效性。

Motivation: 现有射频辐射场建模方法局限于单频率，无法处理宽带频率范围，本文旨在通过物理基础揭示电磁波传播行为与频率的复杂关系，并设计新算法实现宽带建模。

Method: 设计了一个包含衰减和辐射模块的电磁特征网络，学习射频频率与3D高斯关键属性（如衰减因子和信号强度）的关系，通过训练频率嵌入3DGS模型实现任意频率的辐射场重建。

Result: 在6个室内环境中构建了大规模功率角谱数据集（50000样本，1-100GHz），实验显示SSIM达0.72，比现有方法提升17.8%；未训练频率下SSIM为0.70，仅比全数据训练下降2.8%。

Conclusion: 该方法能够有效估计未知频率的功率角谱，展现了在宽带射频辐射场建模中的优越性能和泛化能力。

Abstract: This paper presents an innovative frequency-embedded 3D Gaussian splatting
(3DGS) algorithm for wideband radio-frequency (RF) radiance field modeling,
offering an advancement over the existing works limited to single-frequency
modeling. Grounded in fundamental physics, we uncover the complex relationship
between EM wave propagation behaviors and RF frequencies. Inspired by this, we
design an EM feature network with attenuation and radiance modules to learn the
complex relationships between RF frequencies and the key properties of each 3D
Gaussian, specifically the attenuation factor and RF signal intensity. By
training the frequency-embedded 3DGS model, we can efficiently reconstruct RF
radiance fields at arbitrary unknown frequencies within a given 3D environment.
Finally, we propose a large-scale power angular spectrum (PAS) dataset
containing 50000 samples ranging from 1 to 100 GHz in 6 indoor environments,
and conduct extensive experiments to verify the effectiveness of our method.
Our approach achieves an average Structural Similarity Index Measure (SSIM) up
to 0.72, and a significant improvement up to 17.8% compared to the current
state-of-the-art (SOTA) methods trained on individual test frequencies.
Additionally, our method achieves an SSIM of 0.70 without prior training on
these frequencies, which represents only a 2.8% performance drop compared to
models trained with full PAS data. This demonstrates our model's capability to
estimate PAS at unknown frequencies. For related code and datasets, please
refer to https://github.com/sim-2-real/Wideband3DGS.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [428] [MVTN: Learning Multi-View Transformations for 3D Understanding](https://arxiv.org/abs/2212.13462)
*Abdullah Hamdi,Faisal AlZahrani,Silvio Giancola,Bernard Ghanem*

Key words: 多视图投影, 3D形状识别, 可微分渲染, MVTN, 自适应视角

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: MVTN引入可学习视角的多视图转换网络，通过端到端训练优化3D形状识别的视角选择，提升分类与检索性能，并展示了对遮挡的鲁棒性。

Motivation: 解决现有多视图技术中固定视角的局限性，通过动态学习视角提升3D形状识别效果。

Method: 提出多视图转换网络（MVTN），结合可微分渲染技术，自适应优化视角，支持端到端训练。

Result: 在ModelNet40等基准测试中达到最优分类与检索性能，对遮挡表现出更强鲁棒性。

Conclusion: MVTN为多视图3D分析提供灵活视角选择，推动领域发展并开源MVTorch工具库。

Abstract: Multi-view projection techniques have shown themselves to be highly effective
in achieving top-performing results in the recognition of 3D shapes. These
methods involve learning how to combine information from multiple view-points.
However, the camera view-points from which these views are obtained are often
fixed for all shapes. To overcome the static nature of current multi-view
techniques, we propose learning these view-points. Specifically, we introduce
the Multi-View Transformation Network (MVTN), which uses differentiable
rendering to determine optimal view-points for 3D shape recognition. As a
result, MVTN can be trained end-to-end with any multi-view network for 3D shape
classification. We integrate MVTN into a novel adaptive multi-view pipeline
that is capable of rendering both 3D meshes and point clouds. Our approach
demonstrates state-of-the-art performance in 3D classification and shape
retrieval on several benchmarks (ModelNet40, ScanObjectNN, ShapeNet Core55).
Further analysis indicates that our approach exhibits improved robustness to
occlusion compared to other methods. We also investigate additional aspects of
MVTN, such as 2D pretraining and its use for segmentation. To support further
research in this area, we have released MVTorch, a PyTorch library for 3D
understanding and generation using multi-view projections.

</details>


### [429] [SpatialLLM: From Multi-modality Data to Urban Spatial Intelligence](https://arxiv.org/abs/2505.12703)
*Jiabin Chen,Haiping Wang,Jinpeng Li,Yuan Liu,Zhen Dong,Bisheng Yang*

Key words: SpatialLLM, 空间智能, 语言模型, 零样本学习, 城市分析

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: SpatialLLM是一种无需训练或专家干预的统一语言模型，通过结构化的场景描述直接解决复杂城市场景中的空间智能任务，并在实验中实现了零样本执行高级任务。

Motivation: 当前的空间智能任务通常需要地理分析工具或领域专家知识，限制了其广泛应用。SpatialLLM旨在通过预训练语言模型直接解决这些任务，降低门槛并提高效率。

Method: 通过从原始空间数据构建详细的结构化场景描述，直接提示预训练的语言模型进行基于场景的分析。

Result: 实验表明，预训练语言模型能够准确感知空间分布信息，并以零样本方式执行高级空间智能任务，如城市规划、生态分析和交通管理。

Conclusion: SpatialLLM为城市智能分析和管理提供了新的视角，强调多领域知识、上下文长度和推理能力是影响模型性能的关键因素。

Abstract: We propose SpatialLLM, a novel approach advancing spatial intelligence tasks
in complex urban scenes. Unlike previous methods requiring geographic analysis
tools or domain expertise, SpatialLLM is a unified language model directly
addressing various spatial intelligence tasks without any training,
fine-tuning, or expert intervention. The core of SpatialLLM lies in
constructing detailed and structured scene descriptions from raw spatial data
to prompt pre-trained LLMs for scene-based analysis. Extensive experiments show
that, with our designs, pretrained LLMs can accurately perceive spatial
distribution information and enable zero-shot execution of advanced spatial
intelligence tasks, including urban planning, ecological analysis, traffic
management, etc. We argue that multi-field knowledge, context length, and
reasoning ability are key factors influencing LLM performances in urban
analysis. We hope that SpatialLLM will provide a novel viable perspective for
urban intelligent analysis and management. The code and dataset are available
at https://github.com/WHU-USI3DV/SpatialLLM.

</details>


### [430] [What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models](https://arxiv.org/abs/2505.20405)
*Lorenzo Baraldi,Davide Bucciarelli,Federico Betti,Marcella Cornia,Lorenzo Baraldi,Nicu Sebe,Rita Cucchiara*

Key words: 图像编辑, 评估指标, DICE, MLLM, 自监督

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出DICE模型，用于评估基于指令的图像编辑效果，通过差异检测和连贯性评估两部分提升与人类判断的一致性。

Motivation: 现有图像编辑评估指标与人类判断和可解释性差距较大，需更有效的评估方法。

Method: DICE由差异检测器和连贯性评估器组成，基于自回归MLLM，结合自监督、蒸馏和全监督训练。

Result: DICE能有效识别连贯编辑，与人类判断高度相关。

Conclusion: DICE在评估图像编辑效果上表现优越，公开了代码、模型和数据。

Abstract: Instruction-based image editing models offer increased personalization
opportunities in generative tasks. However, properly evaluating their results
is challenging, and most of the existing metrics lag in terms of alignment with
human judgment and explainability. To tackle these issues, we introduce DICE
(DIfference Coherence Estimator), a model designed to detect localized
differences between the original and the edited image and to assess their
relevance to the given modification request. DICE consists of two key
components: a difference detector and a coherence estimator, both built on an
autoregressive Multimodal Large Language Model (MLLM) and trained using a
strategy that leverages self-supervision, distillation from inpainting
networks, and full supervision. Through extensive experiments, we evaluate each
stage of our pipeline, comparing different MLLMs within the proposed framework.
We demonstrate that DICE effectively identifies coherent edits, effectively
evaluating images generated by different editing models with a strong
correlation with human judgment. We publicly release our source code, models,
and data.

</details>


### [431] [RetroMotion: Retrocausal Motion Forecasting Models are Instructable](https://arxiv.org/abs/2505.20414)
*Royden Wagner,Omer Sahin Tas,Felix Hauser,Marlon Steiner,Dominik Strutz,Abhishek Vivekanandan,Carlos Fernandez,Christoph Stiller*

Key words: 运动预测, 多任务学习, 反因果信息流, transformer, 轨迹分布

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种多任务学习方法用于运动预测，结合了反因果信息流，通过在边际轨迹分布和联合轨迹分布之间重新编码来实现，并在Waymo和Argoverse 2数据集上取得最佳效果。

Motivation: 解决道路用户的运动预测问题，特别是复杂场景和交互行为下的预测精度和适应性。

Method: 采用多任务学习的transformer模型，结合反因果信息流，从边际轨迹分布生成联合轨迹分布，并使用压缩指数分布建模位置不确定性。

Result: 在Waymo Interaction Prediction和Argoverse 2数据集上达到最优性能，并能通过轨迹修改接收指令。

Conclusion: 该方法不仅提升了预测性能，还支持基于目标的指令适应场景上下文。

Abstract: Motion forecasts of road users (i.e., agents) vary in complexity as a
function of scene constraints and interactive behavior. We address this with a
multi-task learning method for motion forecasting that includes a retrocausal
flow of information. The corresponding tasks are to forecast (1) marginal
trajectory distributions for all modeled agents and (2) joint trajectory
distributions for interacting agents. Using a transformer model, we generate
the joint distributions by re-encoding marginal distributions followed by
pairwise modeling. This incorporates a retrocausal flow of information from
later points in marginal trajectories to earlier points in joint trajectories.
Per trajectory point, we model positional uncertainty using compressed
exponential power distributions. Notably, our method achieves state-of-the-art
results in the Waymo Interaction Prediction dataset and generalizes well to the
Argoverse 2 dataset. Additionally, our method provides an interface for issuing
instructions through trajectory modifications. Our experiments show that
regular training of motion forecasting leads to the ability to follow
goal-based instructions and to adapt basic directional instructions to the
scene context. Code: https://github.com/kit-mrt/future-motion

</details>


### [432] [CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting](https://arxiv.org/abs/2505.20469)
*Lei Tian,Xiaomin Li,Liqian Ma,Hefei Huang,Zirui Zheng,Hao Yin,Taiqing Li,Huchuan Lu,Xu Jia*

Key words: 3D语义理解，跨视角一致性，对比学习，CLIP，SAM

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了CCL-LGS框架，通过多视角语义线索解决3D语义理解中的跨视角语义不一致问题。

Motivation: 由于遮挡、图像模糊和视角变化引起的跨视角语义不一致，2D先验方法在3D语义理解中存在挑战，导致3D高斯语义场质量下降。

Method: 采用零样本跟踪器对齐SAM生成的2D掩码，利用CLIP提取跨视角语义编码，并通过对比码本学习模块增强语义特征的分辨能力。

Result: 实验表明，CCL-LGS优于现有方法。

Conclusion: 通过多视角语义监督提升语义一致性，CCL-LGS显著改善了3D语义理解质量。

Abstract: Recent advances in 3D reconstruction techniques and vision-language models
have fueled significant progress in 3D semantic understanding, a capability
critical to robotics, autonomous driving, and virtual/augmented reality.
However, methods that rely on 2D priors are prone to a critical challenge:
cross-view semantic inconsistencies induced by occlusion, image blur, and
view-dependent variations. These inconsistencies, when propagated via
projection supervision, deteriorate the quality of 3D Gaussian semantic fields
and introduce artifacts in the rendered outputs. To mitigate this limitation,
we propose CCL-LGS, a novel framework that enforces view-consistent semantic
supervision by integrating multi-view semantic cues. Specifically, our approach
first employs a zero-shot tracker to align a set of SAM-generated 2D masks and
reliably identify their corresponding categories. Next, we utilize CLIP to
extract robust semantic encodings across views. Finally, our Contrastive
Codebook Learning (CCL) module distills discriminative semantic features by
enforcing intra-class compactness and inter-class distinctiveness. In contrast
to previous methods that directly apply CLIP to imperfect masks, our framework
explicitly resolves semantic conflicts while preserving category
discriminability. Extensive experiments demonstrate that CCL-LGS outperforms
previous state-of-the-art methods. Our project page is available at
https://epsilontl.github.io/CCL-LGS/.

</details>


### [433] [WeatherEdit: Controllable Weather Editing with 4D Gaussian Field](https://arxiv.org/abs/2505.20471)
*Chenghao Qian,Wenjing Li,Yuhu Guo,Gustav Markkula*

Key words: WeatherEdit, 3D场景, 天气效果, 物理模拟, 自动驾驶

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: WeatherEdit 是一种用于在3D场景中生成可控天气效果的新方法，结合了天气背景编辑和天气粒子构建技术，通过物理模拟实现真实的天气表现和灵活调整。

Motivation: 为了在自动驾驶模拟中生成多样且可控的恶劣天气效果，提高模拟的真实性和灵活性。

Method: 采用两阶段方法：天气背景编辑（使用适配器和TV注意力机制）和天气粒子构建（基于4D高斯场和物理模拟）。

Result: 在多个驾驶数据集上验证，WeatherEdit 可以生成多样且可控的天气效果。

Conclusion: WeatherEdit 在自动驾驶模拟中显示出生成真实恶劣天气效果的潜力。

Abstract: In this work, we present WeatherEdit, a novel weather editing pipeline for
generating realistic weather effects with controllable types and severity in 3D
scenes. Our approach is structured into two key components: weather background
editing and weather particle construction. For weather background editing, we
introduce an all-in-one adapter that integrates multiple weather styles into a
single pretrained diffusion model, enabling the generation of diverse weather
effects in 2D image backgrounds. During inference, we design a Temporal-View
(TV-) attention mechanism that follows a specific order to aggregate temporal
and spatial information, ensuring consistent editing across multi-frame and
multi-view images. To construct the weather particles, we first reconstruct a
3D scene using the edited images and then introduce a dynamic 4D Gaussian field
to generate snowflakes, raindrops and fog in the scene. The attributes and
dynamics of these particles are precisely controlled through physical-based
modelling and simulation, ensuring realistic weather representation and
flexible severity adjustments. Finally, we integrate the 4D Gaussian field with
the 3D scene to render consistent and highly realistic weather effects.
Experiments on multiple driving datasets demonstrate that WeatherEdit can
generate diverse weather effects with controllable condition severity,
highlighting its potential for autonomous driving simulation in adverse
weather. See project page: https://jumponthemoon.github.io/w-edit

</details>


### [434] [Electrolyzers-HSI: Close-Range Multi-Scene Hyperspectral Imaging Benchmark Dataset](https://arxiv.org/abs/2505.20507)
*Elias Arbash,Ahmed Jamal Afifi,Ymane Belahsen,Margret Fuchs,Pedram Ghamisi,Paul Scheunders,Richard Gloaguen*

Key words: 可持续回收,高光谱成像,Electrolyzers-HSI,Transformer,FAIR数据原则

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种名为Electrolyzers-HSI的多模态基准数据集，用于加速关键原材料回收，并通过机器学习和深度学习模型评估了其性能。数据集和代码均已开源。

Motivation: 全球可持续发展需求推动了自动化、快速、准确的物料检测系统的开发，以支持循环经济。论文旨在通过开源数据集促进智能回收技术的发展。

Method: 论文结合高分辨率RGB图像和高光谱成像数据，使用多种机器学习方法（包括Transformer架构）进行材料分类研究。

Result: 论文生成了一个包含4.2百万像素向量的数据集，并评估了多种SOTA方法，证明了其在材料分类中的潜力。

Conclusion: 开源数据集和代码为智能回收技术的广泛应用提供了支持，有助于推动可持续电子垃圾回收。

Abstract: The global challenge of sustainable recycling demands automated, fast, and
accurate, state-of-the-art (SOTA) material detection systems that act as a
bedrock for a circular economy. Democratizing access to these cutting-edge
solutions that enable real-time waste analysis is essential for scaling up
recycling efforts and fostering the Green Deal. In response, we introduce
\textbf{Electrolyzers-HSI}, a novel multimodal benchmark dataset designed to
accelerate the recovery of critical raw materials through accurate electrolyzer
materials classification. The dataset comprises 55 co-registered
high-resolution RGB images and hyperspectral imaging (HSI) data cubes spanning
the 400--2500 nm spectral range, yielding over 4.2 million pixel vectors and
424,169 labeled ones. This enables non-invasive spectral analysis of shredded
electrolyzer samples, supporting quantitative and qualitative material
classification and spectral properties investigation. We evaluate a suite of
baseline machine learning (ML) methods alongside SOTA transformer-based deep
learning (DL) architectures, including Vision Transformer, SpectralFormer, and
the Multimodal Fusion Transformer, to investigate architectural bottlenecks for
further efficiency optimisation when deploying transformers in material
identification. We implement zero-shot detection techniques and majority voting
across pixel-level predictions to establish object-level classification
robustness. In adherence to the FAIR data principles, the electrolyzers-HSI
dataset and accompanying codebase are openly available at
https://github.com/hifexplo/Electrolyzers-HSI and
https://rodare.hzdr.de/record/3668, supporting reproducible research and
facilitating the broader adoption of smart and sustainable e-waste recycling
solutions.

</details>


### [435] [Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.20569)
*Jihoon Lee,Min Song*

Key words: 目标幻觉, 视觉语言模型, 对比解码, RVCD

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种名为RVCD的方法，通过利用正负图像在logit级别抑制目标幻觉（OH），显著优于现有方法。

Motivation: 虽然大视觉语言模型已有显著进展，但目标幻觉问题仍未解决，需要无需额外训练的解决方案。

Method: 采用检索视觉对比解码（RVCD），结合正负图像进行对比，抑制OH。

Result: RVCD在抑制目标幻觉方面表现优于现有解码方法。

Conclusion: RVCD是有效且无需额外训练的OH抑制方法。

Abstract: Despite significant advancements in Large Vision-Language Models, Object
Hallucination (OH) remains a persistent challenge. Building upon prior studies
on contrastive decoding that address this issue without requiring additional
model training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an
advanced method to suppress OH. RVCD leverages both negative and positive
images at the logit level, explicitly referencing AI-generated images designed
to represent a single concept. Our approach demonstrates substantial
improvements over existing decoding-based methods.

</details>


### [436] [TrustSkin: A Fairness Pipeline for Trustworthy Facial Affect Analysis Across Skin Tone](https://arxiv.org/abs/2505.20637)
*Ana M. Cabanas,Alma Pedro,Domingo Mery*

Key words: FAA, 肤色分类, 公平性评估, ITA, $H^*$-$L^*$, AffectNet

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该研究比较了两种肤色分类方法（ITA和基于$H^*$-$L^*$的感知方法），发现肤色较深群体在FAA系统中代表性不足且存在公平性差异。$H^*$-$L^*$方法更稳定，并提出了一个公平性评估管道。

Motivation: 研究动机是为了评估FAA系统在不同肤色群体中的性能差异，并探索更可靠的肤色分类方法以改善公平性评估。

Method: 研究比较了ITA和基于$H^*$-$L^*$的肤色分类方法，使用了AffectNet数据集和MobileNet模型，并通过F1-score、TPR等指标评估公平性。

Result: 结果显示肤色较深群体代表性不足（仅2%），且公平性差异显著（F1-score差异达0.08，TPR差异达0.11）。$H^*$-$L^*$方法更稳定，能更好地诊断公平性问题。

Conclusion: 肤色分类方法的选择对公平性评估至关重要，ITA可能忽略肤色较深群体的差异，而$H^*$-$L^*$方法更优，并提出了改进公平性的模块化管道。

Abstract: Understanding how facial affect analysis (FAA) systems perform across
different demographic groups requires reliable measurement of sensitive
attributes such as ancestry, often approximated by skin tone, which itself is
highly influenced by lighting conditions. This study compares two objective
skin tone classification methods: the widely used Individual Typology Angle
(ITA) and a perceptually grounded alternative based on Lightness ($L^*$) and
Hue ($H^*$). Using AffectNet and a MobileNet-based model, we assess fairness
across skin tone groups defined by each method. Results reveal a severe
underrepresentation of dark skin tones ($\sim 2 \%$), alongside fairness
disparities in F1-score (up to 0.08) and TPR (up to 0.11) across groups. While
ITA shows limitations due to its sensitivity to lighting, the $H^*$-$L^*$
method yields more consistent subgrouping and enables clearer diagnostics
through metrics such as Equal Opportunity. Grad-CAM analysis further highlights
differences in model attention patterns by skin tone, suggesting variation in
feature encoding. To support future mitigation efforts, we also propose a
modular fairness-aware pipeline that integrates perceptual skin tone
estimation, model interpretability, and fairness evaluation. These findings
emphasize the relevance of skin tone measurement choices in fairness assessment
and suggest that ITA-based evaluations may overlook disparities affecting
darker-skinned individuals.

</details>


### [437] [HCQA-1.5 @ Ego4D EgoSchema Challenge 2025](https://arxiv.org/abs/2505.20644)
*Haoyu Zhang,Yisen Feng,Qiaohui Chu,Meng Liu,Weili Guan,Yaowei Wang,Liqiang Nie*

Key words: HCQA框架, 多源聚合, 置信度过滤, 细粒度推理, 自我中心视频问答

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了HCQA框架的扩展方法，通过多源聚合策略和置信度过滤机制提高自我中心视频问答的准确性，在Ego4D挑战赛中取得第三名。

Motivation: 提高自我中心视频问答中答案预测的可靠性，改进现有HCQA框架。

Method: 采用多源聚合策略生成多样预测，结合置信度过滤选择高置信度答案，低置信度时引入细粒度推理模块进行视觉和上下文分析。

Result: 在EgoSchema测试集上达到77%准确率，优于去年获胜方案及多数参赛团队。

Conclusion: 提出的方法显著提升了自我中心视频问答的准确性，代码已开源。

Abstract: In this report, we present the method that achieves third place for Ego4D
EgoSchema Challenge in CVPR 2025. To improve the reliability of answer
prediction in egocentric video question answering, we propose an effective
extension to the previously proposed HCQA framework. Our approach introduces a
multi-source aggregation strategy to generate diverse predictions, followed by
a confidence-based filtering mechanism that selects high-confidence answers
directly. For low-confidence cases, we incorporate a fine-grained reasoning
module that performs additional visual and contextual analysis to refine the
predictions. Evaluated on the EgoSchema blind test set, our method achieves 77%
accuracy on over 5,000 human-curated multiple-choice questions, outperforming
last year's winning solution and the majority of participating teams. Our code
will be added at https://github.com/Hyu-Zhang/HCQA.

</details>


### [438] [RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment](https://arxiv.org/abs/2505.20653)
*Lingyu Qiu,Ke Jiang,Xiaoyang Tan*

Key words: 深度伪造检测、域泛化、梯度对齐、参数扰动、鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种新的学习目标，通过将泛化梯度更新与ERM梯度更新对齐来提升深度伪造检测模型的跨域鲁棒性，实验证明其优于现有技术。

Motivation: 现有方法通过引入额外模块避免过拟合域特定模式，但这种正则化可能阻碍ERM目标的优化，导致性能下降。本文旨在解决这一矛盾。

Method: 提出了一种参数扰动策略，通过对齐跨域梯度更新方向，增强模型对域偏移的鲁棒性，无需额外正则化。

Result: 在多个深度伪造检测数据集上，该方法优于当前的域泛化技术。

Conclusion: 梯度对齐策略有效保留了域不变特征并管理域特性，验证了方法的有效性。

Abstract: Recent advancements in domain generalization for deepfake detection have
attracted significant attention, with previous methods often incorporating
additional modules to prevent overfitting to domain-specific patterns. However,
such regularization can hinder the optimization of the empirical risk
minimization (ERM) objective, ultimately degrading model performance. In this
paper, we propose a novel learning objective that aligns generalization
gradient updates with ERM gradient updates. The key innovation is the
application of perturbations to model parameters, aligning the ascending points
across domains, which specifically enhances the robustness of deepfake
detection models to domain shifts. This approach effectively preserves
domain-invariant features while managing domain-specific characteristics,
without introducing additional regularization. Experimental results on multiple
challenging deepfake detection datasets demonstrate that our gradient alignment
strategy outperforms state-of-the-art domain generalization techniques,
confirming the efficacy of our method. The code is available at
https://github.com/Lynn0925/RoGA.

</details>


### [439] [VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with Self-Improving Visual-Language Models](https://arxiv.org/abs/2505.20718)
*Kui Wu,Shuhang Xu,Hao Chen,Churan Wang,Zhoujun Li,Yizhou Wang,Fangwei Zhong*

Key words: 视觉语言模型, 视觉跟踪, 自改进框架, 机器人应用, 失败恢复

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 介绍了一种结合视觉语言模型（VLM）的自改进框架，提升视觉跟踪（EVT）系统在失败恢复中的表现，成功率达72%（RL方法）和220%（PID方法）。

Motivation: 当前主动视觉跟踪系统在跟踪失败恢复方面存在局限，需要一种更有效的方法来应对复杂动态环境中的挑战。

Method: 结合现成主动跟踪方法与VLM推理能力，快速视觉策略用于常规跟踪，失败时激活VLM推理，并引入记忆增强的自反思机制以提升VLM的3D空间推理。

Result: 实验显示，该框架显著提升了性能，RL方法成功率提高72%，PID方法提高220%。

Conclusion: 首次将VLM推理整合到EVT中，为动态非结构化环境中的目标持续监控提供了重要进展。

Abstract: We introduce a novel self-improving framework that enhances Embodied Visual
Tracking (EVT) with Visual-Language Models (VLMs) to address the limitations of
current active visual tracking systems in recovering from tracking failure. Our
approach combines the off-the-shelf active tracking methods with VLMs'
reasoning capabilities, deploying a fast visual policy for normal tracking and
activating VLM reasoning only upon failure detection. The framework features a
memory-augmented self-reflection mechanism that enables the VLM to
progressively improve by learning from past experiences, effectively addressing
VLMs' limitations in 3D spatial reasoning. Experimental results demonstrate
significant performance improvements, with our framework boosting success rates
by $72\%$ with state-of-the-art RL-based approaches and $220\%$ with PID-based
methods in challenging environments. This work represents the first integration
of VLM-based reasoning to assist EVT agents in proactive failure recovery,
offering substantial advances for real-world robotic applications that require
continuous target monitoring in dynamic, unstructured environments. Project
website: https://sites.google.com/view/evt-recovery-assistant.

</details>


### [440] [Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models](https://arxiv.org/abs/2505.20753)
*Yufei Zhan,Hongyin Zhao,Yousong Zhu,Shurong Zheng,Fan Yang,Ming Tang,Jinqiao Wang*

Key words: 大型多模态模型, 视觉推理, 组合问题, Griffon-R, 视觉理解

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一个统一的视觉推理机制，使大型多模态模型（LMMs）能够通过内在能力（如基础能力和视觉理解能力）解决复杂的组合问题，避免了多次推理或外部工具的需求。

Motivation: 现有的LMMs在组合推理方面表现不足，缺乏高级任务特定能力，阻碍了其成为真正通用的视觉模型。论文旨在通过一种统一机制填补基础视觉能力与通用问题回答之间的差距。

Method: 提出了一个人工智能理解-思考-回答的过程，设计了‘Griffon-R’模型，能够端到端自动理解、自主思考并推理答案。同时，收集了334K个视觉指令样本，涵盖通用场景和文本丰富的场景。

Result: Griffon-R在复杂视觉推理基准（如VSR和CLEVR）上表现优秀，同时提升了多模态能力（在MMBench和ScienceQA等基准测试中表现优异）。

Conclusion: 论文提出的机制显著提升了LMMs在组合推理中的表现，并通过单次前向传递实现了高效推理。数据、模型和代码将开源。

Abstract: Large Multimodal Models (LMMs) have recently demonstrated remarkable visual
understanding performance on both vision-language and vision-centric tasks.
However, they often fall short in integrating advanced, task-specific
capabilities for compositional reasoning, which hinders their progress toward
truly competent general vision models. To address this, we present a unified
visual reasoning mechanism that enables LMMs to solve complicated compositional
problems by leveraging their intrinsic capabilities (e.g. grounding and visual
understanding capabilities). Different from the previous shortcut learning
mechanism, our approach introduces a human-like
understanding-thinking-answering process, allowing the model to complete all
steps in a single pass forwarding without the need for multiple inferences or
external tools. This design bridges the gap between foundational visual
capabilities and general question answering, encouraging LMMs to generate
faithful and traceable responses for complex visual reasoning. Meanwhile, we
curate 334K visual instruction samples covering both general scenes and
text-rich scenes and involving multiple foundational visual capabilities. Our
trained model, Griffon-R, has the ability of end-to-end automatic
understanding, self-thinking, and reasoning answers. Comprehensive experiments
show that Griffon-R not only achieves advancing performance on complex visual
reasoning benchmarks including VSR and CLEVR, but also enhances multimodal
capabilities across various benchmarks like MMBench and ScienceQA. Data,
models, and codes will be release at
https://github.com/jefferyZhan/Griffon/tree/master/Griffon-R soon.

</details>


### [441] [PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding](https://arxiv.org/abs/2505.20759)
*Ansel Blume,Jeonghwan Kim,Hyeonjeong Ha,Elen Chatikyan,Xiaomeng Jin,Khanh Duy Nguyen,Nanyun Peng,Kai-Wei Chang,Derek Hoiem,Heng Ji*

Key words: LMM, 部件定位, PARTONOMY, PLUM, 细粒度推理

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: PARTONOMY是一个针对像素级部件定位的LMM基准测试，揭示现有模型在部件定位中的不足，并提出PLUM模型以改进性能。

Motivation: 现实物体由独特的、对象特定的部件组成，识别这些部件对细粒度、组合推理至关重要，但现有LMM在此任务上表现不佳。

Method: 构建PARTONOMY基准测试，并提出PLUM模型，采用span标记和反馈循环机制改进现有架构。

Result: 实验显示现有LMM在部件定位上表现有限（如LISA-13B仅5.9% gIoU），PLUM在多项任务上优于同类模型。

Conclusion: PLUM为解决LMM部件定位问题提供了新思路，推动了细粒度视觉理解的研究。

Abstract: Real-world objects are composed of distinctive, object-specific parts.
Identifying these parts is key to performing fine-grained, compositional
reasoning-yet, large multimodal models (LMMs) struggle to perform this
seemingly straightforward task. In this work, we introduce PARTONOMY, an LMM
benchmark designed for pixel-level part grounding. We construct PARTONOMY from
existing part datasets and our own rigorously annotated set of images,
encompassing 862 part labels and 534 object labels for evaluation. Unlike
existing datasets that simply ask models to identify generic parts, PARTONOMY
uses specialized concepts (e.g., agricultural airplane), and challenges models
to compare objects' parts, consider part-whole relationships, and justify
textual predictions with visual segmentations. Our experiments demonstrate
significant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only
5.9% gIoU), highlighting a critical gap in their part grounding abilities. We
note that existing segmentation-enabled LMMs (segmenting LMMs) have two key
architectural shortcomings: they use special [SEG] tokens not seen during
pretraining which induce distribution shift, and they discard predicted
segmentations instead of using past predictions to guide future ones. To
address these deficiencies, we train several part-centric LMMs and propose
PLUM, a novel segmenting LMM that uses span tagging instead of segmentation
tokens and that conditions on prior predictions in a feedback loop. We find
that pretrained PLUM outperforms existing segmenting LMMs on reasoning
segmentation, VQA, and visual hallucination benchmarks. In addition, PLUM
finetuned on our proposed Explanatory Part Segmentation task is competitive
with segmenting LMMs trained on significantly more segmentation data. Our work
opens up new avenues towards enabling fine-grained, grounded visual
understanding in LMMs.

</details>


### [442] [Rendering-Aware Reinforcement Learning for Vector Graphics Generation](https://arxiv.org/abs/2505.20793)
*Juan A. Rodriguez,Haotian Zhang,Abhay Puri,Aarash Feizi,Rishav Pramanik,Pascal Wichmann,Arnab Mondal,Mohammad Reza Samsami,Rabiul Awal,Perouz Taslakian,Spandana Gella,Sai Rajeswar,David Vazquez,Christopher Pal,Marco Pedersoli*

Key words: SVG生成, 视觉语言模型, 强化学习, 渲染反馈, 自动回归建模

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种名为RLRF的强化学习方法，通过利用SVG渲染输出的反馈，提升自动回归视觉语言模型（VLM）生成SVG的准确性、效率和语义连贯性。

Motivation: 现有的视觉语言模型在SVG生成任务中因训练时无法观察渲染图像而难以生成忠实且高效的SVG。尽管自动回归SVG生成的微分渲染尚不可行，渲染输出仍可与原始图像比较以提供强化学习的评估反馈。

Method: RLRF利用渲染反馈进行强化学习，模型生成SVG后渲染并与原始图像比较计算奖励，通过视觉保真度反馈优化生成结果。

Result: RLRF显著优于监督微调，解决了常见的失败模式，实现了具有强结构理解和泛化能力的高质量SVG生成。

Conclusion: 通过引入视觉反馈，RLRF提升了SVG生成的精确度和质量，展示了强化学习在跨模态生成任务中的潜力。

Abstract: Scalable Vector Graphics (SVG) offer a powerful format for representing
visual designs as interpretable code. Recent advances in vision-language models
(VLMs) have enabled high-quality SVG generation by framing the problem as a
code generation task and leveraging large-scale pretraining. VLMs are
particularly suitable for this task as they capture both global semantics and
fine-grained visual patterns, while transferring knowledge across vision,
natural language, and code domains. However, existing VLM approaches often
struggle to produce faithful and efficient SVGs because they never observe the
rendered images during training. Although differentiable rendering for
autoregressive SVG code generation remains unavailable, rendered outputs can
still be compared to original inputs, enabling evaluative feedback suitable for
reinforcement learning (RL). We introduce RLRF(Reinforcement Learning from
Rendering Feedback), an RL method that enhances SVG generation in
autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an
input image, the model generates SVG roll-outs that are rendered and compared
to the original image to compute a reward. This visual fidelity feedback guides
the model toward producing more accurate, efficient, and semantically coherent
SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common
failure modes and enabling precise, high-quality SVG generation with strong
structural understanding and generalization.

</details>


### [443] [InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning](https://arxiv.org/abs/2505.18291)
*Zifu Wan,Yaqi Xie,Ce Zhang,Zhiqiu Lin,Zihan Wang,Simon Stepputtis,Deva Ramanan,Katia Sycara*

Key words: 多模态模型,部件分割,视觉-语言模型,基准测试,机器人

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种名为InstructPart的新型基准测试，用于评估当前模型在理解和执行部件级任务中的表现，并展示了一个简单基线通过微调能显著提升性能。

Motivation: 现有的多模态基础模型通常将物体视为不可分割的整体，忽略了其组成部分，而理解这些部件及其功能对多种任务至关重要。

Method: 引入了InstructPart基准测试，包含手工标记的部件分割注释和任务导向指令，并提出了一个通过数据集微调的简单基线方法。

Result: 实验表明，任务导向的部件分割对当前最先进的视觉-语言模型仍然具有挑战性，但基线方法通过微调实现了两倍性能提升。

Conclusion: 该基准测试和数据集旨在推动任务导向部件分割的研究，并提升视觉-语言模型在机器人、虚拟现实等领域的适用性。

Abstract: Large multimodal foundation models, particularly in the domains of language
and vision, have significantly advanced various tasks, including robotics,
autonomous driving, information retrieval, and grounding. However, many of
these models perceive objects as indivisible, overlooking the components that
constitute them. Understanding these components and their associated
affordances provides valuable insights into an object's functionality, which is
fundamental for performing a wide range of tasks. In this work, we introduce a
novel real-world benchmark, InstructPart, comprising hand-labeled part
segmentation annotations and task-oriented instructions to evaluate the
performance of current models in understanding and executing part-level tasks
within everyday contexts. Through our experiments, we demonstrate that
task-oriented part segmentation remains a challenging problem, even for
state-of-the-art Vision-Language Models (VLMs). In addition to our benchmark,
we introduce a simple baseline that achieves a twofold performance improvement
through fine-tuning with our dataset. With our dataset and benchmark, we aim to
facilitate research on task-oriented part segmentation and enhance the
applicability of VLMs across various domains, including robotics, virtual
reality, information retrieval, and other related fields. Project website:
https://zifuwan.github.io/InstructPart/.

</details>


### [444] [In Context Learning with Vision Transformers: Case Study](https://arxiv.org/abs/2505.20872)
*Antony Zhao,Alex Proshkin,Fergal Hennessy,Francesco Crivelli*

Key words: 变压器模型, 上下文学习, 图像空间, 复杂函数, 卷积神经网络

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文探讨大型变压器模型在图像空间中上下文学习的能力，特别是对复杂函数如卷积神经网络的适应。

Motivation: 研究变压器模型是否能够通过上下文学习在图像空间中掌握复杂函数，扩展其在随机数据上的已有表现。

Method: 利用上下文学习的示例和查询，测试模型在图像空间中处理复杂函数（如卷积神经网络）的能力。

Result: 研究发现变压器模型能够适应并学习图像空间中的复杂函数。

Conclusion: 变压器模型在图像空间中也展现出强大的上下文学习能力，进一步验证了其通用性。

Abstract: Large transformer models have been shown to be capable of performing
in-context learning. By using examples in a prompt as well as a query, they are
capable of performing tasks such as few-shot, one-shot, or zero-shot learning
to output the corresponding answer to this query. One area of interest to us is
that these transformer models have been shown to be capable of learning the
general class of certain functions, such as linear functions and small 2-layer
neural networks, on random data (Garg et al, 2023). We aim to extend this to
the image space to analyze their capability to in-context learn more complex
functions on the image space, such as convolutional neural networks and other
methods.

</details>


### [445] [Frequency Composition for Compressed and Domain-Adaptive Neural Networks](https://arxiv.org/abs/2505.20890)
*Yoojin Kwon,Hongjun Suh,Wooseok Lee,Taesik Gong,Songyi Han,Hyung-Sin Kim*

Key words: 模型压缩, 领域适应, 量化感知训练, 测试时适应, 频率成分分析

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了CoDA框架，通过频率成分统一处理模型压缩和领域适应问题，结合量化感知训练和测试时适应，显著提升了压缩模型在领域转移下的性能。

Motivation: 现代设备端神经网络应用需在资源限制下运行并适应不可预测的领域变化，但目前压缩模型和领域适应问题常被分开处理。CoDA旨在统一解决这两个问题。

Method: CoDA框架采用基于频率成分的方法。训练时通过量化感知训练（QAT）学习低频稳健特征；测试时利用全频信息进行无源适应（TTA），高频成分用作领域特定线索。

Result: 在CIFAR10-C和ImageNet-C等标准数据集上，CoDA显著提升了压缩模型的性能，准确率分别提高了7.96%和5.37%，超过了全精度TTA基线。

Conclusion: CoDA展示了一种有效的方法，统一了模型压缩和领域适应，显著提升了压缩模型的适应性能力和性能。

Abstract: Modern on-device neural network applications must operate under resource
constraints while adapting to unpredictable domain shifts. However, this
combined challenge-model compression and domain adaptation-remains largely
unaddressed, as prior work has tackled each issue in isolation: compressed
networks prioritize efficiency within a fixed domain, whereas large, capable
models focus on handling domain shifts. In this work, we propose CoDA, a
frequency composition-based framework that unifies compression and domain
adaptation. During training, CoDA employs quantization-aware training (QAT)
with low-frequency components, enabling a compressed model to selectively learn
robust, generalizable features. At test time, it refines the compact model in a
source-free manner (i.e., test-time adaptation, TTA), leveraging the
full-frequency information from incoming data to adapt to target domains while
treating high-frequency components as domain-specific cues. LFC are aligned
with the trained distribution, while HFC unique to the target distribution are
solely utilized for batch normalization. CoDA can be integrated synergistically
into existing QAT and TTA methods. CoDA is evaluated on widely used
domain-shift benchmarks, including CIFAR10-C and ImageNet-C, across various
model architectures. With significant compression, it achieves accuracy
improvements of 7.96%p on CIFAR10-C and 5.37%p on ImageNet-C over the
full-precision TTA baseline.

</details>


### [446] [Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation](https://arxiv.org/abs/2505.20897)
*Pingrui Zhang,Yifei Su,Pengyuan Wu,Dong An,Li Zhang,Zhigang Wang,Dong Wang,Yan Ding,Bin Zhao,Xuelong Li*

Key words: Vision-and-Language Navigation (VLN), Adaptive Text Dreamer (ATD), large language model (LLM), R2R benchmark

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种基于语言的适应性文本想象器（ATD），以高效可靠的方式解决视觉与语言导航（VLN）中的感知-语言对齐问题，通过双分支自引导想象策略，在R2R基准上实现了最优性能。

Motivation: 传统的视觉合成方法在VLN任务中存在计算成本高和冗余细节的问题，因此需要一种更高效的策略来对齐语言指令与环境感知。

Method: 提出了双分支自引导想象策略（ATD），基于大语言模型（LLM）设计，分为逻辑整合（左脑）和想象力预测（右脑）两部分，通过微调Q-former动态更新推理与想象，并引入跨交互机制规范输出。

Result: 在R2R基准测试中，ATD以更少的参数实现了最先进的性能。

Conclusion: ATD通过语言形式的适应性想象，显著提升了VLN任务的效率与可靠性。

Abstract: Vision-and-Language Navigation (VLN) requires the agent to navigate by
following natural instructions under partial observability, making it difficult
to align perception with language. Recent methods mitigate this by imagining
future scenes, yet they rely on vision-based synthesis, leading to high
computational cost and redundant details. To this end, we propose to adaptively
imagine key environmental semantics via \textit{language} form, enabling a more
reliable and efficient strategy. Specifically, we introduce a novel Adaptive
Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a
large language model (LLM). ATD is designed with a human-like left-right brain
architecture, where the left brain focuses on logical integration, and the
right brain is responsible for imaginative prediction of future scenes. To
achieve this, we fine-tune only the Q-former within both brains to efficiently
activate domain-specific knowledge in the LLM, enabling dynamic updates of
logical reasoning and imagination during navigation. Furthermore, we introduce
a cross-interaction mechanism to regularize the imagined outputs and inject
them into a navigation expert module, allowing ATD to jointly exploit both the
reasoning capacity of the LLM and the expertise of the navigation model. We
conduct extensive experiments on the R2R benchmark, where ATD achieves
state-of-the-art performance with fewer parameters. The code is
\href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}.

</details>


### [447] [Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models](https://arxiv.org/abs/2505.20612)
*Peter Robicheaux,Matvei Popov,Anish Madan,Isaac Robinson,Joseph Nelson,Deva Ramanan,Neehar Peri*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文探讨了现有视觉语言模型（VLMs）在处理预训练中未见的新概念（如医疗影像）时的局限性，提出通过结合少量视觉示例和文本描述来对齐新概念的方法，并引入Roboflow100-VL数据集进行评测。结果显示，当前最先进的模型在零样本设置下表现不佳，凸显了少量样本学习的必要性。

Motivation: 现有视觉语言模型在常见对象（如汽车、行人）上表现优异，但在新概念（如医疗影像）上泛化能力不足。作者认为，仅增加视觉数据不足以解决问题，而应通过结合示例和文本描述来对齐新概念。

Method: 作者提出通过少量视觉示例和文本描述对齐新概念的方法，并构建了Roboflow100-VL数据集，包含100个多模态目标检测数据集，涵盖VLM预训练中未见的多样概念。评估了零样本、少样本、半监督和全监督设置下的模型表现。

Result: 实验显示，GroundingDINO和Qwen2.5-VL等VLM模型在Roboflow100-VL的医疗影像数据集上零样本准确率不足2%，表明少量样本学习（few-shot learning）的必要性。

Conclusion: 论文强调了对新概念进行少量样本对齐的重要性，并为未来研究提供了Roboflow100-VL这一评测基准。

Abstract: Vision-language models (VLMs) trained on internet-scale data achieve
remarkable zero-shot detection performance on common objects like car, truck,
and pedestrian. However, state-of-the-art models still struggle to generalize
to out-of-distribution classes, tasks and imaging modalities not typically
found in their pre-training. Rather than simply re-training VLMs on more visual
data, we argue that one should align VLMs to new concepts with annotation
instructions containing a few visual examples and rich textual descriptions. To
this end, we introduce Roboflow100-VL, a large-scale collection of 100
multi-modal object detection datasets with diverse concepts not commonly found
in VLM pre-training. We evaluate state-of-the-art models on our benchmark in
zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing
for comparison across data regimes. Notably, we find that VLMs like
GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on
challenging medical imaging datasets within Roboflow100-VL, demonstrating the
need for few-shot concept alignment. Our code and dataset are available at
https://github.com/roboflow/rf100-vl/ and
https://universe.roboflow.com/rf100-vl/

</details>


### [448] [ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image](https://arxiv.org/abs/2505.20498)
*Dongyu Luo,Kelin Yu,Amir-Hossein Shahidzadeh,Cornelia Fermüller,Yiannis Aloimonos*

Key words: 触觉感知, 数据增强, 控制生成, 触觉图像, 机器人操作

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: ControlTac是一种两阶段可控框架，通过物理先验生成逼真的触觉图像，有效增强触觉数据集。

Motivation: 现有触觉数据扩展方法存在输出不真实和对下游任务迁移性差的问题。

Method: 提出ControlTac框架，基于参考触觉图像、接触力和位置生成物理可信的触觉图像。

Result: 实验证明ControlTac能有效增强触觉数据集，并在三个下游任务中表现优异。

Conclusion: ControlTac实用性强，能显著提升触觉数据的质量和多样性。

Abstract: Vision-based tactile sensing has been widely used in perception,
reconstruction, and robotic manipulation. However, collecting large-scale
tactile data remains costly due to the localized nature of sensor-object
interactions and inconsistencies across sensor instances. Existing approaches
to scaling tactile data, such as simulation and free-form tactile generation,
often suffer from unrealistic output and poor transferability to downstream
tasks.To address this, we propose ControlTac, a two-stage controllable
framework that generates realistic tactile images conditioned on a single
reference tactile image, contact force, and contact position. With those
physical priors as control input, ControlTac generates physically plausible and
varied tactile images that can be used for effective data augmentation. Through
experiments on three downstream tasks, we demonstrate that ControlTac can
effectively augment tactile datasets and lead to consistent gains. Our three
real-world experiments further validate the practical utility of our approach.
Project page: https://dongyuluo.github.io/controltac.

</details>


### [449] [MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding](https://arxiv.org/abs/2505.20715)
*Fuwen Luo,Shengfeng Lou,Chi Chen,Ziyue Wang,Chenliang Li,Weizhou Shen,Jiyue Guo,Peng Li,Ming Yan,Ji Zhang,Fei Huang,Yang Liu*

Key words: MLLMs, temporal reasoning, reinforcement learning, multi-segment grounding, video understanding

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: MUSEG is a novel RL-based method for enhancing temporal understanding in MLLMs via multi-segment grounding and phased rewards, outperforming existing methods.

Motivation: Current MLLMs struggle with fine-grained temporal reasoning despite advancements in video understanding.

Method: Proposes MUSEG, a timestamp-aware multi-segment grounding approach with a customized RL training recipe using phased rewards.

Result: MUSEG significantly outperforms existing methods in temporal grounding and time-sensitive video QA tasks, showing strong generalization.

Conclusion: MUSEG effectively addresses temporal reasoning limitations in MLLMs through innovative RL-based grounding.

Abstract: Video temporal understanding is crucial for multimodal large language models
(MLLMs) to reason over events in videos. Despite recent advances in general
video understanding, current MLLMs still struggle with fine-grained temporal
reasoning. While reinforcement learning (RL) has been explored to address this
issue recently, existing RL approaches remain limited in effectiveness. In this
work, we propose MUSEG, a novel RL-based method that enhances temporal
understanding by introducing timestamp-aware multi-segment grounding. MUSEG
enables MLLMs to align queries with multiple relevant video segments, promoting
more comprehensive temporal reasoning. To facilitate effective learning, we
design a customized RL training recipe with phased rewards that progressively
guides the model toward temporally grounded reasoning. Extensive experiments on
temporal grounding and time-sensitive video QA tasks demonstrate that MUSEG
significantly outperforms existing methods and generalizes well across diverse
temporal understanding scenarios. View our project at
https://github.com/THUNLP-MT/MUSEG.

</details>


### [450] [RefAV: Towards Planning-Centric Scenario Mining](https://arxiv.org/abs/2505.20981)
*Cainan Davidson,Deva Ramanan,Neehar Peri*

Key words: 自动驾驶，场景挖掘，视觉语言模型，多智能体交互，时空定位

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了RefAV数据集和基于视觉语言模型（VLM）的时空场景挖掘方法，用于从自动驾驶数据中高效识别安全关键场景，传统方法效果不佳。

Motivation: 自动驾驶车辆（AV）在测试中收集大量多模态数据，但传统场景挖掘方法效率低且易错，无法满足从海量数据中识别关键场景的需求。

Method: 利用视觉语言模型（VLMs）进行时空场景挖掘，并引入RefAV数据集，包含10,000个自然语言查询，描述来自Argoverse 2 Sensor数据集的复杂多智能体交互。

Result: 实验表明，直接使用现成VLM效果不佳，凸显场景挖掘任务的独特挑战性，并验证了所提方法的必要性。

Conclusion: 场景挖掘需针对性方法，RefAV数据集和基准分析为该领域提供了新工具和方向。

Abstract: Autonomous Vehicles (AVs) collect and pseudo-label terabytes of multi-modal
data localized to HD maps during normal fleet testing. However, identifying
interesting and safety-critical scenarios from uncurated driving logs remains a
significant challenge. Traditional scenario mining techniques are error-prone
and prohibitively time-consuming, often relying on hand-crafted structured
queries. In this work, we revisit spatio-temporal scenario mining through the
lens of recent vision-language models (VLMs) to detect whether a described
scenario occurs in a driving log and, if so, precisely localize it in both time
and space. To address this problem, we introduce RefAV, a large-scale dataset
of 10,000 diverse natural language queries that describe complex multi-agent
interactions relevant to motion planning derived from 1000 driving logs in the
Argoverse 2 Sensor dataset. We evaluate several referential multi-object
trackers and present an empirical analysis of our baselines. Notably, we find
that naively repurposing off-the-shelf VLMs yields poor performance, suggesting
that scenario mining presents unique challenges. Our code and dataset are
available at https://github.com/CainanD/RefAV/ and
https://argoverse.github.io/user-guide/tasks/scenario_mining.html

</details>


### [451] [Intelligent Incident Hypertension Prediction in Obstructive Sleep Apnea](https://arxiv.org/abs/2505.20615)
*Omid Halimi Milani,Ahmet Enis Cetin,Bharati Prasad*

Key words: 阻塞性睡眠呼吸暂停,高血压预测,离散余弦变换,迁移学习,深度学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该研究提出了一种结合离散余弦变换（DCT）和迁移学习的深度学习方法，用于预测阻塞性睡眠呼吸暂停（OSA）患者在五年内发展为高血压的风险，取得了72.88%的AUC。

Motivation: OSA是高血压的重要风险因素，但目前预测OSA患者发展为高血压的准确性仍不足，医疗数据集又通常较小。因此，研究希望通过结合DCT和迁移学习，利用多导睡眠信号的整体信息，提升预测性能。

Method: 研究首次整合了所有多导睡眠信号，通过DCT将其转换为频域表示，并利用预训练的2D神经网络（如MobileNet、EfficientNet和ResNet）进行迁移学习，以增强特征提取和模型泛化能力。

Result: 模型在EfficientNet的深层结构中引入DCT层后，最佳AUC达到72.88%，验证了频域特征提取和迁移学习对OSA患者高血压风险预测的有效性。

Conclusion: 研究表明，频域特征与迁移学习的结合能够显著提升小规模医疗数据集的预测性能，为OSA相关高血压的早期干预提供了新思路。

Abstract: Obstructive sleep apnea (OSA) is a significant risk factor for hypertension,
primarily due to intermittent hypoxia and sleep fragmentation. Predicting
whether individuals with OSA will develop hypertension within five years
remains a complex challenge. This study introduces a novel deep learning
approach that integrates Discrete Cosine Transform (DCT)-based transfer
learning to enhance prediction accuracy. We are the first to incorporate all
polysomnography signals together for hypertension prediction, leveraging their
collective information to improve model performance. Features were extracted
from these signals and transformed into a 2D representation to utilize
pre-trained 2D neural networks such as MobileNet, EfficientNet, and ResNet
variants. To further improve feature learning, we introduced a DCT layer, which
transforms input features into a frequency-based representation, preserving
essential spectral information, decorrelating features, and enhancing
robustness to noise. This frequency-domain approach, coupled with transfer
learning, is especially beneficial for limited medical datasets, as it
leverages rich representations from pre-trained networks to improve
generalization. By strategically placing the DCT layer at deeper truncation
depths within EfficientNet, our model achieved a best area under the curve
(AUC) of 72.88%, demonstrating the effectiveness of frequency-domain feature
extraction and transfer learning in predicting hypertension risk in OSA
patients over a five-year period.

</details>


### [452] [Incorporating Flexible Image Conditioning into Text-to-Video Diffusion Models without Training](https://arxiv.org/abs/2505.20629)
*Bolin Lai,Sangmin Lee,Xu Cao,Xiang Li,James M. Rehg*

Key words: 文本到视频生成, 视觉条件化, 无训练方法, 动态控制

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种无训练的FlexTI2V方法，通过灵活的图像条件化策略，在文本生成视频任务中实现更高效的视觉条件整合。

Motivation: 现有方法通常在文本到视频基础模型上通过微调加入视觉条件，成本高且限制多，无法灵活应对任意数量的图像条件。

Method: FlexTI2V采用无训练策略，通过噪声反演和随机块交换技术，将视觉特征动态整合到视频生成中，并结合动态控制机制平衡创意与保真度。

Result: 实验表明该方法显著优于以往无训练图像条件化方法，并通过消融研究验证了关键设计。

Conclusion: FlexTI2V为可控视频生成提供了更灵活高效的解决方案。

Abstract: Text-image-to-video (TI2V) generation is a critical problem for controllable
video generation using both semantic and visual conditions. Most existing
methods typically add visual conditions to text-to-video (T2V) foundation
models by finetuning, which is costly in resources and only limited to a few
predefined conditioning settings. To tackle this issue, we introduce a unified
formulation for TI2V generation with flexible visual conditioning. Furthermore,
we propose an innovative training-free approach, dubbed FlexTI2V, that can
condition T2V foundation models on an arbitrary amount of images at arbitrary
positions. Specifically, we firstly invert the condition images to noisy
representation in a latent space. Then, in the denoising process of T2V models,
our method uses a novel random patch swapping strategy to incorporate visual
features into video representations through local image patches. To balance
creativity and fidelity, we use a dynamic control mechanism to adjust the
strength of visual conditioning to each video frame. Extensive experiments
validate that our method surpasses previous training-free image conditioning
methods by a notable margin. We also show more insights of our method by
detailed ablation study and analysis.

</details>


### [453] [ID-Align: RoPE-Conscious Position Remapping for Dynamic High-Resolution Adaptation in Vision-Language Models](https://arxiv.org/abs/2505.21465)
*Bozhou Li,Wentao Zhang*

Key words: 视觉语言模型、高分辨率图像、位置嵌入、ID-Align、LLaVA-Next

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: ID-Align通过重新排序位置ID，解决了高分辨率与缩略图令牌间交互受限的问题，提升了视觉语言模型的性能。

Motivation: 现有方法在处理高分辨率和缩略图令牌时，由于RoPE的长期衰减特性，令牌间及文本与图像间交互受限，亟需改进。

Method: 提出了ID-Align方法，通过重新排序位置ID，高分辨率令牌继承对应缩略图令牌的ID，并限制位置索引的过度扩展。

Result: 在LLaVA-Next框架中测试，ID-Align在多个基准测试中表现显著提升，尤其在MMBench关系推理任务上提高了6.09%。

Conclusion: ID-Align有效改善了高分辨率与缩略图令牌间的交互问题，为视觉语言模型的优化提供了新思路。

Abstract: Currently, a prevalent approach for enhancing Vision-Language Models (VLMs)
performance is to encode both the high-resolution version and the thumbnail of
an image simultaneously. While effective, this method generates a large number
of image tokens. When combined with the widely used Rotary Position Embedding
(RoPE), its long-term decay property hinders the interaction between
high-resolution tokens and thumbnail tokens, as well as between text and image.
To address these issues, we propose ID-Align, which alleviates these problems
by reordering position IDs. In this method, high-resolution tokens inherit IDs
from their corresponding thumbnail token while constraining the overexpansion
of positional indices. Our experiments conducted within the LLaVA-Next
framework demonstrate that ID-Align achieves significant improvements,
including a 6.09% enhancement on MMBench's relation reasoning tasks and notable
gains across multiple benchmarks. Our code is available at the following link:
https://github.com/zooblastlbz/ID-Align.

</details>


### [454] [FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models](https://arxiv.org/abs/2505.21032)
*Nils Neukirch,Johanna Vielhaben,Nils Strodthoff*

Key words: 内部表征, 扩散模型, 特征空间, 可解释性, 计算机视觉

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种基于条件扩散模型的方法，用于从特征空间映射到输入空间，以提高深度神经网络内部表征的可解释性，并在多种预训练图像分类器上验证了其有效性。

Motivation: 深度神经网络的内部表征难以解释，现有方法通常依赖粗糙的近似，因此需要一种更精确的映射方法以提高可解释性。

Method: 使用预训练的高保真条件扩散模型，在概率方式下学习从特征空间到输入空间的映射。

Result: 在多种预训练图像分类器（如CNN和ViT）上展示了出色的重建能力，并通过定性比较和鲁棒性分析验证了方法的有效性。

Conclusion: 该方法具有广泛的潜力，可用于提升计算机视觉模型特征空间的理解，例如可视化概念引导或研究特征空间的复合性质。

Abstract: Internal representations are crucial for understanding deep neural networks,
such as their properties and reasoning patterns, but remain difficult to
interpret. While mapping from feature space to input space aids in interpreting
the former, existing approaches often rely on crude approximations. We propose
using a conditional diffusion model - a pretrained high-fidelity diffusion
model conditioned on spatially resolved feature maps - to learn such a mapping
in a probabilistic manner. We demonstrate the feasibility of this approach
across various pretrained image classifiers from CNNs to ViTs, showing
excellent reconstruction capabilities. Through qualitative comparisons and
robustness analysis, we validate our method and showcase possible applications,
such as the visualization of concept steering in input space or investigations
of the composite nature of the feature space. This approach has broad potential
for improving feature space understanding in computer vision models.

</details>


### [455] [Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration](https://arxiv.org/abs/2505.21472)
*Mehrdad Fazli,Bowen Wei,Ziwei Zhu*

Key words: 视觉语言模型, 幻觉问题, 注意力校准, 视觉令牌, 自适应注意力

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: CAAC框架通过视觉令牌校准和自适应注意力重缩放，有效减少大视觉语言模型在开放和长文本生成中的幻觉问题。

Motivation: 大型视觉语言模型在多模态任务中表现优异，但存在幻觉问题，即错误描述图像中未出现的对象或属性，现有方法难以保持开放和长文本生成的准确性。

Method: CAAC框架采用视觉令牌校准（VTC）平衡视觉令牌关注，并通过自适应注意力重缩放（AAR）根据模型置信度强化视觉基础。

Result: 在CHAIR、AMBER和POPE基准测试中，CAAC表现优于基线，尤其在长文本生成中显著减少幻觉。

Conclusion: CAAC通过针对性校准偏差，提升了视觉语言模型在开放和长文本生成中的准确性和视觉对齐性。

Abstract: Large vision-language models (LVLMs) achieve impressive performance on
multimodal tasks but often suffer from hallucination, and confidently describe
objects or attributes not present in the image. Current inference-time
interventions, while training-free, struggle to maintain accuracy in open-ended
and long-form generation scenarios. We introduce the Confidence-Aware Attention
Calibration (CAAC) framework to address this challenge by targeting two key
biases: spatial perception bias, which distributes attention disproportionately
across image tokens, and modality bias, which shifts focus from visual to
textual inputs over time. CAAC employs a two-step approach: Visual-Token
Calibration (VTC) to balance attention across visual tokens, and Adaptive
Attention Re-Scaling (AAR) to reinforce visual grounding based on the model's
confidence. This confidence-driven adjustment ensures consistent visual
alignment during generation. Experiments on CHAIR, AMBER, and POPE benchmarks
demonstrate that CAAC outperforms baselines, particularly in long-form
generations, effectively reducing hallucination.

</details>


### [456] [RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy](https://arxiv.org/abs/2505.21036)
*Aiyue Chen,Bin Dong,Jingru Li,Jing Lin,Yiwu Yao,Gongyi Wang*

Key words: 视频生成, 扩散模型, 稀疏注意力, RainFusion, 计算加速

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: RainFusion是一种免训练的稀疏注意力方法，通过利用视觉数据的固有稀疏性加速视频生成中的注意力计算，保持视频质量的同时实现2倍加速。

Motivation: 视频生成中3D注意力计算占用了大量计算资源，影响了效率。

Method: 提出RainFusion方法，通过识别注意力计算中的三种稀疏模式（空间、时间和纹理），并采用自适应识别模块（ARM）动态确定稀疏模式。

Result: 在多个主流模型中验证了RainFusion的有效性，实现了2倍加速且视频质量几乎不受影响（VBench得分仅下降0.2%）。

Conclusion: RainFusion是一种即插即用的高效方法，适用于现有3D注意力视频生成模型。

Abstract: Video generation using diffusion models is highly computationally intensive,
with 3D attention in Diffusion Transformer (DiT) models accounting for over
80\% of the total computational resources. In this work, we introduce {\bf
RainFusion}, a novel training-free sparse attention method that exploits
inherent sparsity nature in visual data to accelerate attention computation
while preserving video quality. Specifically, we identify three unique sparse
patterns in video generation attention calculations--Spatial Pattern, Temporal
Pattern and Textural Pattern. The sparse pattern for each attention head is
determined online with negligible overhead (\textasciitilde\,0.2\%) with our
proposed {\bf ARM} (Adaptive Recognition Module) during inference. Our proposed
{\bf RainFusion} is a plug-and-play method, that can be seamlessly integrated
into state-of-the-art 3D-attention video generation models without additional
training or calibration. We evaluate our method on leading open-sourced models
including HunyuanVideo, OpenSoraPlan-1.2 and CogVideoX-5B, demonstrating its
broad applicability and effectiveness. Experimental results show that
RainFusion achieves over {\bf 2\(\times\)} speedup in attention computation
while maintaining video quality, with only a minimal impact on VBench scores
(-0.2\%).

</details>


### [457] [Temporal Saliency-Guided Distillation: A Scalable Framework for Distilling Video Datasets](https://arxiv.org/abs/2505.20694)
*Xulin Gu,Xinhao Zhong,Zhixing Wei,Yimin Zhou,Shuoyang Sun,Bin Chen,Hongpeng Wang,Yuan Luo*

Key words: 数据集蒸馏, 视频压缩, 时间显著性, 单层次框架, 计算效率

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种新颖的单层次视频数据集蒸馏框架，通过时间显著性引导过滤机制优化合成视频，显著提升了视频数据集压缩的效率和性能。

Motivation: 视频数据的高维度和时间复杂性使得现有视频蒸馏方法计算成本高且难以保留时间动态，亟需一种高效的解决方案。

Method: 采用单层次视频数据集蒸馏框架，结合时间显著性引导过滤机制，利用帧间差异指导蒸馏过程，保留关键时间信息并减少冗余。

Result: 在标准视频基准测试中取得最先进性能，显著缩小了真实与蒸馏视频数据之间的差距。

Conclusion: 该方法为视频数据集压缩提供了高效、可扩展的解决方案，同时优化了计算成本和性能。

Abstract: Dataset distillation (DD) has emerged as a powerful paradigm for dataset
compression, enabling the synthesis of compact surrogate datasets that
approximate the training utility of large-scale ones. While significant
progress has been achieved in distilling image datasets, extending DD to the
video domain remains challenging due to the high dimensionality and temporal
complexity inherent in video data. Existing video distillation (VD) methods
often suffer from excessive computational costs and struggle to preserve
temporal dynamics, as na\"ive extensions of image-based approaches typically
lead to degraded performance. In this paper, we propose a novel uni-level video
dataset distillation framework that directly optimizes synthetic videos with
respect to a pre-trained model. To address temporal redundancy and enhance
motion preservation, we introduce a temporal saliency-guided filtering
mechanism that leverages inter-frame differences to guide the distillation
process, encouraging the retention of informative temporal cues while
suppressing frame-level redundancy. Extensive experiments on standard video
benchmarks demonstrate that our method achieves state-of-the-art performance,
bridging the gap between real and distilled video data and offering a scalable
solution for video dataset compression.

</details>


### [458] [Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers](https://arxiv.org/abs/2505.21497)
*Wei Pang,Kevin Qinghong Lin,Xiangru Jian,Xi He,Philip Torr*

Key words: 学术海报生成、多代理系统、视觉语言模型、基准评估

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了首个学术海报生成的基准和评估指标，并介绍了PosterAgent多代理流程，其在多项评估中优于现有方法。

Motivation: 学术海报生成是一个重要但具有挑战性的任务，需要将复杂的论文内容压缩为一页视觉连贯的海报。

Method: 论文提出了PosterAgent，由Parser、Planner和Painter-Commenter组成的多代理流程，通过结构化和视觉反馈生成高质量海报。

Result: PosterAgent在视觉质量、文本连贯性和内容传达等方面优于GPT-4o，且成本更低。

Conclusion: 该研究为全自动海报生成模型提供了明确方向。

Abstract: Academic poster generation is a crucial yet challenging task in scientific
communication, requiring the compression of long-context interleaved documents
into a single, visually coherent page. To address this challenge, we introduce
the first benchmark and metric suite for poster generation, which pairs recent
conference papers with author-designed posters and evaluates outputs on
(i)Visual Quality-semantic alignment with human posters, (ii)Textual
Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic
and informational criteria scored by a VLM-as-judge, and notably
(iv)PaperQuiz-the poster's ability to convey core paper content as measured by
VLMs answering generated quizzes. Building on this benchmark, we propose
PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser
distills the paper into a structured asset library; the (b)Planner aligns
text-visual pairs into a binary-tree layout that preserves reading order and
spatial balance; and the (c)Painter-Commenter loop refines each panel by
executing rendering code and using VLM feedback to eliminate overflow and
ensure alignment. In our comprehensive evaluation, we find that GPT-4o
outputs-though visually appealing at first glance-often exhibit noisy text and
poor PaperQuiz scores, and we find that reader engagement is the primary
aesthetic bottleneck, as human-designed posters rely largely on visual
semantics to convey meaning. Our fully open-source variants (e.g. based on the
Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across
nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper
into a finalized yet editable .pptx poster - all for just $0.005. These
findings chart clear directions for the next generation of fully automated
poster-generation models. The code and datasets are available at
https://github.com/Paper2Poster/Paper2Poster.

</details>


### [459] [ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models](https://arxiv.org/abs/2505.21500)
*Dingming Li,Hongxing Li,Zixuan Wang,Yuchen Yan,Hang Zhang,Siqi Chen,Guiyang Hou,Shengpei Jiang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Yueting Zhuang*

Key words: 视觉语言模型, 多视角空间推理, 3D标注, 基准测试, 空间智能

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出ViewSpatial-Bench，首个专注于多视角空间定位识别的基准测试，通过自动3D标注管道生成精确方向标签，显著提升了视觉语言模型在空间推理任务中的性能。

Motivation: 当前视觉语言模型在跨视角空间推理方面表现欠佳，特别是在非自我中心视角下的任务中存在显著局限性。

Method: 引入ViewSpatial-Bench基准测试，包含五种任务类型，并采用自动3D标注管道生成精确标签，对多种VLMs进行综合评估与微调。

Result: 模型在自我中心视角任务中表现良好，但在人类视角任务中准确率下降；通过微调，任务性能整体提升了46.24%。

Conclusion: 研究为空间智能提供了关键基准，并证明建模3D空间关系能有效增强VLMs的空间理解能力。

Abstract: Vision-language models (VLMs) have demonstrated remarkable capabilities in
understanding and reasoning about visual content, but significant challenges
persist in tasks requiring cross-viewpoint understanding and spatial reasoning.
We identify a critical limitation: current VLMs excel primarily at egocentric
spatial reasoning (from the camera's perspective) but fail to generalize to
allocentric viewpoints when required to adopt another entity's spatial frame of
reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark
designed specifically for multi-viewpoint spatial localization recognition
evaluation across five distinct task types, supported by an automated 3D
annotation pipeline that generates precise directional labels. Comprehensive
evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant
performance disparity: models demonstrate reasonable performance on
camera-perspective tasks but exhibit reduced accuracy when reasoning from a
human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,
we achieve an overall performance improvement of 46.24% across tasks,
highlighting the efficacy of our approach. Our work establishes a crucial
benchmark for spatial intelligence in embodied AI systems and provides
empirical evidence that modeling 3D spatial relationships enhances VLMs'
corresponding spatial comprehension capabilities.

</details>


### [460] [LPOI: Listwise Preference Optimization for Vision Language Models](https://arxiv.org/abs/2505.21061)
*Fatemeh Pesaran Zadeh,Yoojin Oh,Gunhee Kim*

Key words: 视觉语言模型, 偏好优化, 列表学习, 幻觉减少, 对象掩蔽

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: LPOI首次提出基于列表的偏好优化方法，通过对象掩蔽和插值减少视觉语言模型的幻觉问题，无需额外标注。

Motivation: 现有方法（如RLHF和DPO）容易过度依赖文本信息或加剧幻觉，且尚未有针对视觉语言模型的列表偏好优化方法。

Method: LPOI通过识别并掩蔽图像中的关键对象，在正负样本间插值生成渐进完整图像序列，训练模型按对象可见性排序。

Result: 在MMHalBench、AMBER和Object HalBench上，LPOI显著优于现有方法，减少幻觉并提升模型性能。

Conclusion: LPOI提供了一种高效且无需额外标注的列表偏好优化方案，有效改善视觉语言模型的幻觉问题。

Abstract: Aligning large VLMs with human preferences is a challenging task, as methods
like RLHF and DPO often overfit to textual information or exacerbate
hallucinations. Although augmenting negative image samples partially addresses
these pitfalls, no prior work has employed listwise preference optimization for
VLMs, due to the complexity and cost of constructing listwise image samples. In
this work, we propose LPOI, the first object-aware listwise preference
optimization developed for reducing hallucinations in VLMs. LPOI identifies and
masks a critical object in the image, and then interpolates the masked region
between the positive and negative images to form a sequence of incrementally
more complete images. The model is trained to rank these images in ascending
order of object visibility, effectively reducing hallucinations while retaining
visual fidelity. LPOI requires no extra annotations beyond standard pairwise
preference data, as it automatically constructs the ranked lists through object
masking and interpolation. Comprehensive experiments on MMHalBench, AMBER, and
Object HalBench confirm that LPOI outperforms existing preference optimization
methods in reducing hallucinations and enhancing VLM performance. We make the
code available at https://github.com/fatemehpesaran310/lpoi.

</details>


### [461] [LeDiFlow: Learned Distribution-guided Flow Matching to Accelerate Image Generation](https://arxiv.org/abs/2505.20723)
*Pascal Zwick,Nils Friederich,Maximilian Beichter,Lennart Hilbert,Ralf Mikut,Oliver Bringmann*

Key words: LeDiFlow, Flow Matching, 图像生成, ODE 求解, 先验分布

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: LeDiFlow 是一种基于 Flow Matching (FM) 的新型图像生成方法，通过引入学习到的先验分布减少 ODE 求解步骤，提升推理速度并改善生成质量。

Motivation: 传统 FM 方法依赖高斯分布先验，导致概率路径弯曲，增加推理计算量。LeDiFlow 旨在通过学习更优先验分布，优化路径以加速图像生成。

Method: LeDiFlow 通过回归模型学习目标数据分布的先验，结合 SOTA Transformer 架构和隐空间采样，减少 ODE 求解步骤。

Result: LeDiFlow 推理速度比基线快 3.75 倍（像素空间），隐空间模型在 CMMD 指标上提升 1.32 倍生成质量。

Conclusion: LeDiFlow 通过优化的先验分布显著提升 FM 方法的效率和生成质量，适用于消费级硬件。

Abstract: Enhancing the efficiency of high-quality image generation using Diffusion
Models (DMs) is a significant challenge due to the iterative nature of the
process. Flow Matching (FM) is emerging as a powerful generative modeling
paradigm based on a simulation-free training objective instead of a score-based
one used in DMs. Typical FM approaches rely on a Gaussian distribution prior,
which induces curved, conditional probability paths between the prior and
target data distribution. These curved paths pose a challenge for the Ordinary
Differential Equation (ODE) solver, requiring a large number of inference calls
to the flow prediction network. To address this issue, we present Learned
Distribution-guided Flow Matching (LeDiFlow), a novel scalable method for
training FM-based image generation models using a better-suited prior
distribution learned via a regression-based auxiliary model. By initializing
the ODE solver with a prior closer to the target data distribution, LeDiFlow
enables the learning of more computationally tractable probability paths. These
paths directly translate to fewer solver steps needed for high-quality image
generation at inference time. Our method utilizes a State-Of-The-Art (SOTA)
transformer architecture combined with latent space sampling and can be trained
on a consumer workstation. We empirically demonstrate that LeDiFlow remarkably
outperforms the respective FM baselines. For instance, when operating directly
on pixels, our model accelerates inference by up to 3.75x compared to the
corresponding pixel-space baseline. Simultaneously, our latent FM model
enhances image quality on average by 1.32x in CLIP Maximum Mean Discrepancy
(CMMD) metric against its respective baseline.

</details>


### [462] [ConText-CIR: Learning from Concepts in Text for Composed Image Retrieval](https://arxiv.org/abs/2505.20764)
*Eric Xing,Pranavi Kolouju,Robert Pless,Abby Stylianou,Nathan Jacobs*

Key words: 组合图像检索, 文本概念一致性损失, 合成数据, 零样本学习, 监督学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种名为ConText-CIR的新框架，通过文本概念一致性损失提升组合图像检索任务的表现，并在多个数据集上取得最优结果。

Motivation: 现有组合图像检索方法在图像和文本修改的表达上不够准确，导致检索性能不佳。

Method: 设计了CIR框架ConText-CIR，引入文本概念一致性损失，并提出从现有数据集或无标签图像生成合成数据的流水线。

Result: 在监督和零样本设置下的多个基准数据集（如CIRR和CIRCO）上实现了最优性能。

Conclusion: 文本概念一致性损失和合成数据生成流水线共同提升了组合图像检索任务的性能。

Abstract: Composed image retrieval (CIR) is the task of retrieving a target image
specified by a query image and a relative text that describes a semantic
modification to the query image. Existing methods in CIR struggle to accurately
represent the image and the text modification, resulting in subpar performance.
To address this limitation, we introduce a CIR framework, ConText-CIR, trained
with a Text Concept-Consistency loss that encourages the representations of
noun phrases in the text modification to better attend to the relevant parts of
the query image. To support training with this loss function, we also propose a
synthetic data generation pipeline that creates training data from existing CIR
datasets or unlabeled images. We show that these components together enable
stronger performance on CIR tasks, setting a new state-of-the-art in composed
image retrieval in both the supervised and zero-shot settings on multiple
benchmark datasets, including CIRR and CIRCO. Source code, model checkpoints,
and our new datasets are available at https://github.com/mvrl/ConText-CIR.

</details>


### [463] [MetaSlot: Break Through the Fixed Number of Slots in Object-Centric Learning](https://arxiv.org/abs/2505.20772)
*Hongjia Liu,Rongzhen Zhao,Haohan Chen,Joni Pajarinen*

Key words: Object-Centric Learning, Slot Attention, MetaSlot, 对象发现, 对象识别

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: MetaSlot是一种改进的Slot Attention变体，通过动态适应对象数量、去除重复槽位并引入渐进噪声，显著提升了对象中心学习的性能和可解释性。

Motivation: 现有的对象中心学习方法由于使用固定数量的槽位，在对象数量变化时会导致对象被分割为多个部分，影响了模型的泛化能力。MetaSlot旨在解决这一问题，提升对象表示的准确性和适应性。

Method: MetaSlot通过维护一个对象原型代码本、量化槽位去除重复以及引入渐进噪声来优化Slot Attention，从而动态适应对象数量的变化。

Result: 在多个公开数据集和任务中，MetaSlot显著提升了性能，并生成了更具可解释性的槽位表示。

Conclusion: MetaSlot是一种通用的Slot Attention变体，能够无缝集成到现有架构中，为对象中心学习提供了更优的解决方案。

Abstract: Learning object-level, structured representations is widely regarded as a key
to better generalization in vision and underpins the design of next-generation
Pre-trained Vision Models (PVMs). Mainstream Object-Centric Learning (OCL)
methods adopt Slot Attention or its variants to iteratively aggregate objects'
super-pixels into a fixed set of query feature vectors, termed slots. However,
their reliance on a static slot count leads to an object being represented as
multiple parts when the number of objects varies. We introduce MetaSlot, a
plug-and-play Slot Attention variant that adapts to variable object counts.
MetaSlot (i) maintains a codebook that holds prototypes of objects in a dataset
by vector-quantizing the resulting slot representations; (ii) removes duplicate
slots from the traditionally aggregated slots by quantizing them with the
codebook; and (iii) injects progressively weaker noise into the Slot Attention
iterations to accelerate and stabilize the aggregation. MetaSlot is a general
Slot Attention variant that can be seamlessly integrated into existing OCL
architectures. Across multiple public datasets and tasks--including object
discovery and recognition--models equipped with MetaSlot achieve significant
performance gains and markedly interpretable slot representations, compared
with existing Slot Attention variants.

</details>


### [464] [Integrating Intermediate Layer Optimization and Projected Gradient Descent for Solving Inverse Problems with Diffusion Models](https://arxiv.org/abs/2505.20789)
*Yang Zheng,Wen Li,Zhaoqiang Liu*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出两种新方法（DMILO和DMILO-PGD）来解决传统扩散模型在解决逆向问题时的高计算需求和次优收敛问题。DMILO通过中间层优化（ILO）减少内存负担，并通过稀疏偏差扩展扩散模型范围。DMILO-PGD结合ILO与投影梯度下降（PGD）以提升收敛效果。实验表明这两种方法在性能上显著优于现有技术。

Motivation: 传统逆向问题（IPs）方法依赖手工设计的先验，无法充分捕捉真实数据的复杂性。扩散模型（DMs）虽能通过数据学习先验，但仍面临高计算成本和次优收敛问题，因此作者提出改进方法以解决这些挑战。

Method: 1. DMILO：引入中间层优化（ILO）降低内存占用，并通过稀疏偏差扩展模型范围；2. DMILO-PGD：结合ILO与投影梯度下降（PGD）优化收敛性能。

Result: 在多种图像数据集（包括线性和非线性IPs）上，DMILO和DMILO-PGD的性能显著优于现有方法，验证了其有效性。

Conclusion: DMILO和DMILO-PGD解决了扩散模型在逆向问题中的计算和收敛问题，实验证实其优越性，为相关领域提供了实用工具。

Abstract: Inverse problems (IPs) involve reconstructing signals from noisy
observations. Traditional approaches often rely on handcrafted priors, which
can fail to capture the complexity of real-world data. The advent of
pre-trained generative models has introduced new paradigms, offering improved
reconstructions by learning rich priors from data. Among these, diffusion
models (DMs) have emerged as a powerful framework, achieving remarkable
reconstruction performance across numerous IPs. However, existing DM-based
methods frequently encounter issues such as heavy computational demands and
suboptimal convergence. In this work, building upon the idea of the recent work
DMPlug~\cite{wang2024dmplug}, we propose two novel methods, DMILO and
DMILO-PGD, to address these challenges. Our first method, DMILO, employs
intermediate layer optimization (ILO) to alleviate the memory burden inherent
in DMPlug. Additionally, by introducing sparse deviations, we expand the range
of DMs, enabling the exploration of underlying signals that may lie outside the
range of the diffusion model. We further propose DMILO-PGD, which integrates
ILO with projected gradient descent (PGD), thereby reducing the risk of
suboptimal convergence. We provide an intuitive theoretical analysis of our
approach under appropriate conditions and validate its superiority through
extensive experiments on diverse image datasets, encompassing both linear and
nonlinear IPs. Our results demonstrate significant performance gains over
state-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD
in addressing common challenges in DM-based IP solvers.

</details>


### [465] [Unified Alignment Protocol: Making Sense of the Unlabeled Data in New Domains](https://arxiv.org/abs/2505.21010)
*Sabbir Ahmed,Mamshad Nayeem Rizve,Abdullah Al Arafat,Jacqueline Liu,Rahim Hossain,Mohaiminul Al Nahian,Adnan Siraj Rakin*

Key words: 半监督联邦学习, 域泛化, 统一对齐协议, 特征对齐, 两阶段训练

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 半监督联邦学习（SSFL）因客户端数据标签有限而流行，但传统SSFL假设训练与测试数据分布相同，忽略了实践中常见的域偏移。为解决这一问题，本文提出了统一对齐协议（UAP），通过两阶段训练提升模型在新域的泛化能力，实验证明其优于现有方法。

Motivation: 传统SSFL假设训练与测试数据分布一致，但实际中域偏移常见。需提升模型在新域的泛化能力以适应现实应用（如医疗）。

Method: 提出统一对齐协议（UAP），分两阶段训练：1）服务器模型学习并对齐特征分布；2）客户端利用服务器特征分布调整训练。

Result: 在多个域泛化基准数据集上，UAP实现了最先进的泛化性能。

Conclusion: UAP通过特征对齐有效解决了SSFL中的域偏移问题，提升了模型泛化能力。

Abstract: Semi-Supervised Federated Learning (SSFL) is gaining popularity over
conventional Federated Learning in many real-world applications. Due to the
practical limitation of limited labeled data on the client side, SSFL considers
that participating clients train with unlabeled data, and only the central
server has the necessary resources to access limited labeled data, making it an
ideal fit for real-world applications (e.g., healthcare). However, traditional
SSFL assumes that the data distributions in the training phase and testing
phase are the same. In practice, however, domain shifts frequently occur,
making it essential for SSFL to incorporate generalization capabilities and
enhance their practicality. The core challenge is improving model
generalization to new, unseen domains while the client participate in SSFL.
However, the decentralized setup of SSFL and unsupervised client training
necessitates innovation to achieve improved generalization across domains. To
achieve this, we propose a novel framework called the Unified Alignment
Protocol (UAP), which consists of an alternating two-stage training process.
The first stage involves training the server model to learn and align the
features with a parametric distribution, which is subsequently communicated to
clients without additional communication overhead. The second stage proposes a
novel training algorithm that utilizes the server feature distribution to align
client features accordingly. Our extensive experiments on standard domain
generalization benchmark datasets across multiple model architectures reveal
that proposed UAP successfully achieves SOTA generalization performance in SSFL
setting.

</details>


### [466] [Is Hyperbolic Space All You Need for Medical Anomaly Detection?](https://arxiv.org/abs/2505.21228)
*Alvaro Gonzalez-Jimenez,Simone Lionetti,Ludovic Amruthalingam,Philippe Gottfrois,Fabian Gröger,Marc Pouly,Alexander A. Navarini*

Key words: 医学异常检测, 双曲空间, 特征表示, 少样本学习, AUROC

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种新颖的简单方法，将特征表示投影到双曲空间中进行医学异常检测，相比传统的欧几里得空间方法表现更优。

Motivation: 传统方法在欧几里得空间中提取特征，无法有效捕捉特征的层次关系，导致异常检测性能欠佳。

Method: 将特征表示投影到双曲空间中，基于置信度进行聚合，并分类样本为正常或异常。

Result: 双曲空间方法在多个医学基准数据集上的图像和像素级别AUROC得分均优于欧几里得方法，且对参数变化具有鲁棒性，在少样本场景下表现优秀。

Conclusion: 双曲空间是医学异常检测的一种有力替代方案，展现出更强的性能潜力。

Abstract: Medical anomaly detection has emerged as a promising solution to challenges
in data availability and labeling constraints. Traditional methods extract
features from different layers of pre-trained networks in Euclidean space;
however, Euclidean representations fail to effectively capture the hierarchical
relationships within these features, leading to suboptimal anomaly detection
performance. We propose a novel yet simple approach that projects feature
representations into hyperbolic space, aggregates them based on confidence
levels, and classifies samples as healthy or anomalous. Our experiments
demonstrate that hyperbolic space consistently outperforms Euclidean-based
frameworks, achieving higher AUROC scores at both image and pixel levels across
multiple medical benchmark datasets. Additionally, we show that hyperbolic
space exhibits resilience to parameter variations and excels in few-shot
scenarios, where healthy images are scarce. These findings underscore the
potential of hyperbolic space as a powerful alternative for medical anomaly
detection. The project website can be found at
https://hyperbolic-anomalies.github.io

</details>


### [467] [AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping](https://arxiv.org/abs/2505.21357)
*Wenyuan Li,Shunlin Liang,Keyan Chen,Yongzhe Chen,Han Ma,Jianglei Xu,Yichuan Ma,Shikang Guan,Husheng Fang,Zhenwei Shi*

Key words: 作物映射, 遥感基础模型, 时空特征, Transformer, 农业

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了AgriFM，一个专为农业作物映射设计的遥感基础模型，通过改进的视频Swin Transformer架构实现多尺度时空特征提取，并在多个卫星数据源上验证了其优越性能。

Motivation: 现有遥感基础模型在作物映射中存在局限性，要么采用固定的时空窗口忽视多尺度特性，要么完全忽略时间信息。因此，开发一个能同时处理多尺度时空特征的模型是必要的。

Method: 提出AgriFM模型，基于改进的视频Swin Transformer架构，同步时空下采样处理长时序卫星数据（如MODIS、Landsat-8/9和Sentinel-2），并通过预训练和动态解码器架构支持多样化任务。

Result: AgriFM在所有下游任务中表现优于传统深度学习方法和现有通用遥感基础模型。

Conclusion: AgriFM通过高效的时空特征提取和动态解码器架构，显著提升了作物映射的准确性，为农业遥感提供了新工具。

Abstract: Accurate crop mapping fundamentally relies on modeling multi-scale
spatiotemporal patterns, where spatial scales range from individual field
textures to landscape-level context, and temporal scales capture both
short-term phenological transitions and full growing-season dynamics.
Transformer-based remote sensing foundation models (RSFMs) offer promising
potential for crop mapping due to their innate ability for unified
spatiotemporal processing. However, current RSFMs remain suboptimal for crop
mapping: they either employ fixed spatiotemporal windows that ignore the
multi-scale nature of crop systems or completely disregard temporal information
by focusing solely on spatial patterns. To bridge these gaps, we present
AgriFM, a multi-source remote sensing foundation model specifically designed
for agricultural crop mapping. Our approach begins by establishing the
necessity of simultaneous hierarchical spatiotemporal feature extraction,
leading to the development of a modified Video Swin Transformer architecture
where temporal down-sampling is synchronized with spatial scaling operations.
This modified backbone enables efficient unified processing of long time-series
satellite inputs. AgriFM leverages temporally rich data streams from three
satellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is
pre-trained on a global representative dataset comprising over 25 million image
samples supervised by land cover products. The resulting framework incorporates
a versatile decoder architecture that dynamically fuses these learned
spatiotemporal representations, supporting diverse downstream tasks.
Comprehensive evaluations demonstrate AgriFM's superior performance over
conventional deep learning approaches and state-of-the-art general-purpose
RSFMs across all downstream tasks. Codes will be available at
urlhttps://github.com/flyakon/AgriFM.

</details>


### [468] [Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via Multi-modality Mentor Learning](https://arxiv.org/abs/2505.21420)
*Jinbao Wang,Hanzhe Liang,Can Gao,Chenxi Hu,Jie Zhou,Yunkang Cao,Linlin Shen,Weiming Shen*

Key words: 3D异常检测, 多模态学习, 特征融合, Mentor3AD

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种名为Mentor3AD的新方法，通过多模态导师学习融合RGB和3D模态特征，提升3D异常检测性能。

Motivation: 利用多模态互补信息提升3D异常检测的准确性，通过导师学习进一步区分正常与异常特征差异。

Method: 提出Mentor3AD方法，包含融合模块(MFM)生成导师特征，指导模块(MGM)进行跨模态重建，以及投票模块(VM)生成最终异常分数。

Result: 在MVTec 3D-AD和Eyecandies数据集上的实验验证了方法的有效性。

Conclusion: Mentor3AD通过多模态导师学习显著提升了3D异常检测的性能。

Abstract: Multimodal feature reconstruction is a promising approach for 3D anomaly
detection, leveraging the complementary information from dual modalities. We
further advance this paradigm by utilizing multi-modal mentor learning, which
fuses intermediate features to further distinguish normal from feature
differences. To address these challenges, we propose a novel method called
Mentor3AD, which utilizes multi-modal mentor learning. By leveraging the shared
features of different modalities, Mentor3AD can extract more effective features
and guide feature reconstruction, ultimately improving detection performance.
Specifically, Mentor3AD includes a Mentor of Fusion Module (MFM) that merges
features extracted from RGB and 3D modalities to create a mentor feature.
Additionally, we have designed a Mentor of Guidance Module (MGM) to facilitate
cross-modal reconstruction, supported by the mentor feature. Lastly, we
introduce a Voting Module (VM) to more accurately generate the final anomaly
score. Extensive comparative and ablation studies on MVTec 3D-AD and Eyecandies
have verified the effectiveness of the proposed method.

</details>


### [469] [Be Decisive: Noise-Induced Layouts for Multi-Subject Generation](https://arxiv.org/abs/2505.21488)
*Omer Dahary,Yehonathan Cohen,Or Patashnik,Kfir Aberman,Daniel Cohen-Or*

Key words: 文本到图像生成, 扩散模型, 多主题生成, 空间布局

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种新方法，通过预测和细化初始噪声衍生的空间布局来改进文本到图像扩散模型的多主题生成效果。

Motivation: 现有文本到图像扩散模型在多主题生成时容易发生主题泄露，导致数量、属性和视觉特征不准确。需要一种能自然融入模型先验的布局控制方法。

Method: 引入了基于初始噪声预测空间布局的小型神经网络，并在去噪过程中逐步细化布局，避免外部布局控制与模型先验的冲突。

Result: 实验表明，该方法在文本-图像对齐和多主题生成稳定性上优于现有布局引导技术，同时保留了模型原有的多样性。

Conclusion: 该方法通过噪声对齐策略，有效解决了多主题生成中的布局冲突问题，提升了生成质量。

Abstract: Generating multiple distinct subjects remains a challenge for existing
text-to-image diffusion models. Complex prompts often lead to subject leakage,
causing inaccuracies in quantities, attributes, and visual features. Preventing
leakage among subjects necessitates knowledge of each subject's spatial
location. Recent methods provide these spatial locations via an external layout
control. However, enforcing such a prescribed layout often conflicts with the
innate layout dictated by the sampled initial noise, leading to misalignment
with the model's prior. In this work, we introduce a new approach that predicts
a spatial layout aligned with the prompt, derived from the initial noise, and
refines it throughout the denoising process. By relying on this noise-induced
layout, we avoid conflicts with externally imposed layouts and better preserve
the model's prior. Our method employs a small neural network to predict and
refine the evolving noise-induced layout at each denoising step, ensuring clear
boundaries between subjects while maintaining consistency. Experimental results
show that this noise-aligned strategy achieves improved text-image alignment
and more stable multi-subject generation compared to existing layout-guided
techniques, while preserving the rich diversity of the model's original
distribution.

</details>


### [470] [Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO](https://arxiv.org/abs/2505.21457)
*Muzhi Zhu,Hao Zhong,Canyu Zhao,Zongze Du,Zheng Huang,Mingyu Liu,Hao Chen,Cheng Zou,Jingdong Chen,Ming Yang,Chunhua Shen*

Key words: 主动感知, 多模态大语言模型, 强化学习, GPT-o3, ACTIVE-O3

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了ACTIVE-O3框架，通过强化学习赋予多模态大语言模型（MLLMs）主动感知能力，并建立了一套全面的评测基准。

Motivation: 探索如何让MLLMs具备主动感知能力，弥补现有研究中GPT-o3模型效率低和区域选择不准确的问题。

Method: 提出基于GRPO的强化学习训练框架ACTIVE-O3，结合任务相关训练优化搜索策略。

Result: ACTIVE-O3在多个任务（如小物体检测、密集物体定位等）中表现优异，并且在V*基准测试中展现出零样本推理能力。

Conclusion: ACTIVE-O3为MLLMs的主动感知研究提供了简单易用的代码库和评测协议，推动了该领域的进一步发展。

Abstract: Active vision, also known as active perception, refers to the process of
actively selecting where and how to look in order to gather task-relevant
information. It is a critical component of efficient perception and
decision-making in humans and advanced embodied agents. Recently, the use of
Multimodal Large Language Models (MLLMs) as central planning and
decision-making modules in robotic systems has gained extensive attention.
However, despite the importance of active perception in embodied intelligence,
there is little to no exploration of how MLLMs can be equipped with or learn
active perception capabilities. In this paper, we first provide a systematic
definition of MLLM-based active perception tasks. We point out that the
recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a
special case of active perception; however, it still suffers from low search
efficiency and inaccurate region selection. To address these issues, we propose
ACTIVE-O3, a purely reinforcement learning based training framework built on
top of GRPO, designed to equip MLLMs with active perception capabilities. We
further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across
both general open-world tasks, such as small-object and dense object grounding,
and domain-specific scenarios, including small object detection in remote
sensing and autonomous driving, as well as fine-grained interactive
segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot
reasoning abilities on the V* Benchmark, without relying on any explicit
reasoning data. We hope that our work can provide a simple codebase and
evaluation protocol to facilitate future research on active perception in
MLLMs.

</details>


### [471] [Policy Optimized Text-to-Image Pipeline Design](https://arxiv.org/abs/2505.21478)
*Uri Gadot,Rinon Gal,Yftah Ziser,Gal Chechik,Shie Mannor*

Key words: 文本到图像生成, 强化学习, 奖励模型, GRPO优化, 无分类器引导

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文介绍了一种基于强化学习的框架，通过奖励模型预测图像质量分，减少训练时的计算需求，并通过两阶段训练策略和增强技术提升工作流生成质量。

Motivation: 为了解决现有文本到图像生成方法在计算资源和泛化能力上的局限性，本文旨在开发一种更高效且通用的自动化工作流设计方法。

Method: 采用强化学习框架，包括训练奖励模型预测图像质量、两阶段训练策略（初始词汇训练和GRPO优化）以及无分类器引导增强技术。

Result: 实验验证表明，该方法能生成更多样化的工作流，并显著提升图像质量，优于现有基准方法。

Conclusion: 提出的强化学习框架有效解决了计算效率和泛化问题，为自动化文本到图像生成工作流设计提供了新方向。

Abstract: Text-to-image generation has evolved beyond single monolithic models to
complex multi-component pipelines. These combine fine-tuned generators,
adapters, upscaling blocks and even editing steps, leading to significant
improvements in image quality. However, their effective design requires
substantial expertise. Recent approaches have shown promise in automating this
process through large language models (LLMs), but they suffer from two critical
limitations: extensive computational requirements from generating images with
hundreds of predefined pipelines, and poor generalization beyond memorized
training examples. We introduce a novel reinforcement learning-based framework
that addresses these inefficiencies. Our approach first trains an ensemble of
reward models capable of predicting image quality scores directly from
prompt-workflow combinations, eliminating the need for costly image generation
during training. We then implement a two-phase training strategy: initial
workflow vocabulary training followed by GRPO-based optimization that guides
the model toward higher-performing regions of the workflow space. Additionally,
we incorporate a classifier-free guidance based enhancement technique that
extrapolates along the path between the initial and GRPO-tuned models, further
improving output quality. We validate our approach through a set of
comparisons, showing that it can successfully create new flows with greater
diversity and lead to superior image quality compared to existing baselines.

</details>
