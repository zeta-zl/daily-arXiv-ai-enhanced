<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 235]
- [cs.LG](#cs.LG) [Total: 238]
- [cs.AI](#cs.AI) [Total: 92]
- [cs.RO](#cs.RO) [Total: 20]
- [stat.AP](#stat.AP) [Total: 2]
- [cs.SE](#cs.SE) [Total: 8]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AR](#cs.AR) [Total: 6]
- [eess.SP](#eess.SP) [Total: 4]
- [cs.MA](#cs.MA) [Total: 3]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.NE](#cs.NE) [Total: 4]
- [econ.EM](#econ.EM) [Total: 2]
- [cs.DS](#cs.DS) [Total: 3]
- [math.AP](#math.AP) [Total: 1]
- [eess.SY](#eess.SY) [Total: 3]
- [cs.SD](#cs.SD) [Total: 13]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.IR](#cs.IR) [Total: 14]
- [cs.DC](#cs.DC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 5]
- [q-bio.GN](#q-bio.GN) [Total: 3]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.CV](#cs.CV) [Total: 51]
- [cs.CY](#cs.CY) [Total: 22]
- [cs.DB](#cs.DB) [Total: 2]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [hep-ph](#hep-ph) [Total: 2]
- [cs.LO](#cs.LO) [Total: 2]
- [eess.AS](#eess.AS) [Total: 7]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [stat.ML](#stat.ML) [Total: 17]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [math.OC](#math.OC) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.CR](#cs.CR) [Total: 17]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.GT](#cs.GT) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Amadeus-Verbo Technical Report: The powerful Qwen2.5 family models trained in Portuguese](https://arxiv.org/abs/2506.00019)
*William Alberto Cruz-Castañeda,Marcellus Amadeus*

Key words: 大语言模型, 巴西葡萄牙语, 微调, 开源

TL;DR: 介绍了开发巴西葡萄牙语大语言模型Amadeus Verbo的经验，包括不同规模的模型及如何微调基础模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 推动巴西葡萄牙语大语言模型的开源开发，展示微调基础模型的便捷性。

Method: 开发了多参数规模的模型（0.5B至72B），包括基础调优、合并和指令调优模型。

Result: Amadeus Verbo系列模型已在HuggingFace上开源。

Conclusion: 展示了数据和资源可用时，巴西葡萄牙语大语言模型的开源开发的可行性。

Abstract: This report introduces the experience of developing Amadeus Verbo, a family
of large language models for Brazilian Portuguese. To handle diverse use cases,
Amadeus Verbo includes base-tuned, merged, and instruction-tuned models in
sizes of 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters. Thus, the main
objective is to show how easy it is to fine-tune foundation models to
democratize the open-source development of Brazilian Portuguese LLMs when data
and resources are available. Amadeus-Verbo family models are all available at
HuggingFace at
https://huggingface.co/collections/amadeusai/amadeus-verbo-qwen25-67cf2e7aae69ce2b3bcdcfda.

</details>


### [2] [Scaling Physical Reasoning with the PHYSICS Dataset](https://arxiv.org/abs/2506.00022)
*Shenghe Zheng,Qianjia Cheng,Junchi Yao,Mengsong Wu,haonan he,Ning Ding,Yu Cheng,Shuyue Hu,Lei Bai,Dongzhan Zhou,Ganqu Cui,Peng Ye*

Key words: 大型语言模型,物理推理,数据集,评估框架

TL;DR: 本文介绍了PHYSICS数据集，包含16,568个高质量物理问题，旨在提升大型语言模型在物理领域的推理能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管物理是推理密集且对现实世界理解至关重要的领域，但与数学和编程相比，学术界和工业界对其关注较少。

Method: 通过精心设计的流程从100多本教科书中收集问题，覆盖五大物理领域和不同难度级别，并提供训练数据的推理路径。此外，提出Rule+Model评估框架以解决现有评估偏差。

Result: 评估发现当前先进的开源和专有模型在处理物理任务时存在局限性。

Conclusion: PHYSICS数据集和评估方法有望共同推动大型语言模型在物理领域的发展。

Abstract: Large Language Models (LLMs) have achieved remarkable progress on advanced
reasoning tasks such as mathematics and coding competitions. Meanwhile,
physics, despite being both reasoning-intensive and essential to real-world
understanding, received limited academic and industrial attention. This paper
introduces PHYSICS, a dataset containing 16,568 high-quality physics problems
spanning subjects and difficulty levels, to facilitate this issue.
Specifically, PHYSICS is curated with exercises from over 100 textbooks through
a carefully designed pipeline for quality control. It covers five major physics
domains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern
Physics. It also spans a wide range of difficulty levels, from high school to
graduate-level physics courses. To utilize the data for improving and
evaluating the model's physical reasoning capabilities, we split the dataset
into training and test sets, and provide reasoning paths generated by powerful
reasoning models for the training data to facilitate model training. In
addition, for the evaluation part, we find that existing evaluation frameworks
exhibit biases in aspects such as units, simplification, and precision in
physics domain. To balance efficiency and accuracy, we introduce a Rule+Model
evaluation framework tailored to physics problems. Our evaluations on current
state-of-the-art open-source and proprietary models highlight the limitations
of current models in handling physics-related tasks. We hope that our dataset
and evaluation methodology will jointly advance the development of LLMs in the
field of physics.

</details>


### [3] [From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling](https://arxiv.org/abs/2506.00027)
*Zhengyu Chen,Yudong Wang,Teng Xiao,Ruochen Zhou,Xuesheng Yang,Wei Wang,Zhifang Sui,Jingang Wang*

Key words: 过程奖励模型（PRMs）、推理能力、规模化、泛化能力、蒙特卡洛树搜索、Best-of-N采样

TL;DR: 该研究分析了过程奖励模型（PRMs）在提升大型语言模型推理能力中的作用，探讨了训练方法、规模化和泛化能力，并指出模型规模和计算成本的平衡以及数据多样性的重要性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究的动机是评估过程奖励模型（PRMs）在解决复杂推理任务中中间错误的能力，旨在优化其效率和准确性，同时探索跨领域的泛化潜力。

Method: 研究从多角度分析了PRMs，包括训练方法、FLOPs的影响、训练数据多样性，并测试了测试时扩展策略（如蒙特卡洛树搜索和Best-of-N采样）。

Result: 研究发现PRM性能随规模增加而递减，数据多样性对性能有显著影响；蒙特卡洛树搜索在资源充足时最有效，而Best-of-N采样适合资源有限的情况；PRMs在数学和代码生成任务中表现出交叉泛化能力。

Conclusion: PRMs在推理任务中表现优异，但需平衡模型规模与计算成本；多样化训练数据和适当的测试策略是优化性能的关键；PRMs具有跨领域泛化能力。

Abstract: Recent advancements in improving the reasoning capabilities of Large Language
Models have underscored the efficacy of Process Reward Models (PRMs) in
addressing intermediate errors through structured feedback mechanisms. This
study analyzes PRMs from multiple perspectives, including training
methodologies, scalability, and generalization capabilities. We investigate the
interplay between pre-training and reward model training FLOPs to assess their
influence on PRM efficiency and accuracy in complex reasoning tasks. Our
analysis reveals a pattern of diminishing returns in performance with
increasing PRM scale, highlighting the importance of balancing model size and
computational cost. Furthermore, the diversity of training datasets
significantly impacts PRM performance, emphasizing the importance of diverse
data to enhance both accuracy and efficiency. We further examine test-time
scaling strategies, identifying Monte Carlo Tree Search as the most effective
method when computational resources are abundant, while Best-of-N Sampling
serves as a practical alternative under resource-limited conditions. Notably,
our findings indicate that PRMs trained on mathematical datasets exhibit
performance comparable to those tailored for code generation, suggesting robust
cross-domain generalization. Employing a gradient-based metric, we observe that
PRMs exhibit a preference for selecting responses with similar underlying
patterns, further informing their optimization.

</details>


### [4] [Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists](https://arxiv.org/abs/2506.00042)
*Yue Cui,Liuyi Yao,Shuchang Tao,Weijie Shi,Yaliang Li,Bolin Ding,Xiaofang Zhou*

Key words: 大型语言模型、工具调用、参数填充、HiTEC、优化

TL;DR: 提出HiTEC框架，通过全局和局部错误检查清单改进LLMs工具调用中的参数填充问题，显著提升准确率和成功率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决大型语言模型中工具调用时的参数填充错误问题，提升其实际应用的有效性。

Method: 提出分层工具错误检查清单（HiTEC），包括全局和局部检查清单，并通过HiTEC-ICL和HiTEC-KTO两种方法动态优化参数处理。

Result: 在五个公开数据集上的实验表明，HiTEC显著提高了参数填充准确性和工具调用成功率。

Conclusion: HiTEC框架有效诊断和缓解了工具调用错误，为LLMs的实用化提供了可行方案。

Abstract: Large language models (LLMs) have significantly advanced natural language
processing, particularly through the integration of external tools and APIs.
However, their effectiveness is frequently hampered by parameter mis-filling
during tool calling. In this paper, we propose the Hierarchical Tool Error
Checklist (HiTEC) framework to systematically diagnose and mitigate
tool-calling errors without relying on extensive real-world interactions. HiTEC
introduces a two-tiered approach: a global error checklist that identifies
common, cross-tool issues, and a local error checklist that targets
tool-specific and contextual failures. Building on this structure, we propose
two deployments: HiTEC-In Context Learning (HiTEC-ICL) and
HiTEC-Kahneman-Tversky Optimization (HiTEC-KTO). HiTEC-ICL embeds the global
checklist in the initial prompts and leverages a two-round conversational
interaction to dynamically refine parameter handling, while HiTEC-KTO generates
high-quality negative examples to drive fine-tuning via preference-based
optimization. Extensive experiments across five public datasets demonstrate
that our framework significantly improves parameter-filling accuracy and
tool-calling success rates compared to baseline methods.

</details>


### [5] [Unraveling SITT: Social Influence Technique Taxonomy and Detection with LLMs](https://arxiv.org/abs/2506.00061)
*Wiktoria Mieleszczenko-Kowszewicz,Beata Bajcar,Aleksander Szczęsny,Maciej Markiewicz,Jolanta Babiak,Berenika Dyczek,Przemysław Kazienko*

Key words: 社会影响,LLMs,分类法,数据集,多标签分类

TL;DR: 该研究提出了社会影响技术分类法(SITT)，包含58种技术，并构建了一个标注数据集用于评估大语言模型(LLMs)识别这些技术的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在检测文本中微妙的社会影响形式，并评估LLMs识别这些形式的能力。

Method: 通过构建SITT数据集（746对话，由专家标注），并在分层多标签分类任务中测试5种LLMs的性能。

Result: Claude3.5表现较优（F1=0.45），但整体模型识别能力有限，尤其是对上下文敏感的技术。

Conclusion: 当前LLMs对微妙语言线索的敏感性有限，强调了领域微调的重要性。

Abstract: In this work we present the Social Influence Technique Taxonomy (SITT), a
comprehensive framework of 58 empirically grounded techniques organized into
nine categories, designed to detect subtle forms of social influence in textual
content. We also investigate the LLMs ability to identify various forms of
social influence. Building on interdisciplinary foundations, we construct the
SITT dataset -- a 746-dialogue corpus annotated by 11 experts in Polish and
translated into English -- to evaluate the ability of LLMs to identify these
techniques. Using a hierarchical multi-label classification setup, we benchmark
five LLMs, including GPT-4o, Claude 3.5, Llama-3.1, Mixtral, and PLLuM. Our
results show that while some models, notably Claude 3.5, achieved moderate
success (F1 score = 0.45 for categories), overall performance of models remains
limited, particularly for context-sensitive techniques. The findings
demonstrate key limitations in current LLMs' sensitivity to nuanced linguistic
cues and underscore the importance of domain-specific fine-tuning. This work
contributes a novel resource and evaluation example for understanding how LLMs
detect, classify, and potentially replicate strategies of social influence in
natural dialogues.

</details>


### [6] [Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling](https://arxiv.org/abs/2506.00064)
*Jiayi Zeng,Yizhe Feng,Mengliang He,Wenhui Lei,Wei Zhang,Zeming Liu,Xiaoming Shi,Aimin Zhou*

Key words: 大语言模型, 错误处理, 主动错误处理, 监督微调, 评测基准

TL;DR: 该论文探讨了大语言模型(LLMs)在主动错误处理方面的挑战，提出新基准Mis-prompt，发现当前LLMs表现不佳但可通过监督微调提升性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前LLMs的错误处理依赖显式指令，而现实场景中这些指令通常缺失，因此研究如何实现无需显式指令的主动错误处理是关键。

Method: 提出新基准Mis-prompt，包含四项评测任务、错误分类法和数据集，并实验分析LLMs表现及监督微调效果。

Result: 实验显示当前LLMs在主动错误处理上表现较差，但通过监督微调可显著提升能力。

Conclusion: 主动错误处理是LLMs的重要挑战，监督微调和公开数据集可为未来研究提供支持。

Abstract: Large language models (LLMs) have demonstrated significant advancements in
error handling. Current error-handling works are performed in a passive manner,
with explicit error-handling instructions. However, in real-world scenarios,
explicit error-handling instructions are usually unavailable. In this paper,
our work identifies this challenge as how to conduct proactive error handling
without explicit error handling instructions. To promote further research, this
work introduces a new benchmark, termed Mis-prompt, consisting of four
evaluation tasks, an error category taxonomy, and a new evaluation dataset.
Furthermore, this work analyzes current LLMs' performance on the benchmark, and
the experimental results reveal that current LLMs show poor performance on
proactive error handling, and SFT on error handling instances improves LLMs'
proactive error handling capabilities. The dataset will be publicly available.

</details>


### [7] [You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than Vocabulary Words for Humans and Multimodal Language Models](https://arxiv.org/abs/2506.00065)
*Dota Tianai Dong,Yifan Luo,Po-Ya Angela Wang,Asli Ozyurek,Paula Rubio-Fernandez*

Key words: 多模态语言模型, 参考词, 物主代词, 指示代词, 视角推理

TL;DR: 该研究比较了人类和多模态语言模型（MLM）在使用参考词（如词汇、物主代词和指示代词）时的表现，发现MLM在词汇任务上接近人类，但在物主和指示代词上存在显著差距，主要因视角和空间推理能力不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索MLM在使用参考词时的能力，填补其在日常交流中普遍性但未被充分研究的领域。

Method: 通过比较人类和七种先进的MLM在词汇、物主代词和指示代词任务上的表现，分析其困难原因。

Result: MLM在词汇任务上表现接近人类，但在物主和指示代词上表现较差，提示工程仅部分改善了物主代词的使用。

Conclusion: 当前NLP系统在处理需要语用和社会认知的语法形式上仍面临挑战。

Abstract: Multimodal language models (MLMs) increasingly communicate in human-like
ways, yet their ability to use reference words remains largely overlooked
despite their ubiquity in everyday communication. Our study addresses this gap
by comparing human and MLM use of three word classes with increasing cognitive
demands: vocabulary words, possessive pronouns (`mine' vs `yours'), and
demonstrative pronouns (`this one' vs `that one'). Evaluating seven
state-of-the-art MLMs against human participants, we observe a clear difficulty
hierarchy: while MLMs approach human-level performance on the vocabulary task,
they show substantial deficits with possessives and demonstratives. Our
analysis reveals these difficulties stem from limitations in perspective-taking
and spatial reasoning. Although prompt engineering improved model performance
on possessive use, demonstrative use remained well below human-level
competence. These findings provide theoretical and empirical evidence that
producing grammatical forms requiring pragmatics and social cognition remains a
clear challenge in current NLP systems.

</details>


### [8] [Probing Politico-Economic Bias in Multilingual Large Language Models: A Cultural Analysis of Low-Resource Pakistani Languages](https://arxiv.org/abs/2506.00068)
*Afrozah Nadeem,Mark Dras,Usman Naseem*

Key words: 大型语言模型, 政治偏见, 低资源语言, 巴基斯坦, 文化调制

TL;DR: 本文系统分析了13种最先进的大型语言模型（LLMs）在巴基斯坦五种低资源语言中的政治偏见，揭示其普遍偏向于自由左翼价值观，但在区域语言中表现出明显的威权主义倾向。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型对公共话语的影响日益显著，但其在非西方和低资源多语言语境中的政治经济偏见尚未充分研究。

Method: 结合定制的政治指南针测试（PCT）和多层次框架分析，从经济（左-右）和社会（自由-威权）轴线进行定量评估，并辅以内容、风格和重点的定性分析。

Result: LLMs主要受西方训练数据影响，偏向自由左翼价值观，但在区域语言中表现出威权主义倾向，存在文化调制效应和模型特定偏见。

Conclusion: 研究强调了建立基于文化的多语言偏见审计框架的紧迫性。

Abstract: Large Language Models (LLMs) are increasingly shaping public discourse, yet
their politico-economic biases remain underexamined in non-Western and
low-resource multilingual contexts. This paper presents a systematic analysis
of political bias in 13 state-of-the-art LLMs across five low-resource
languages spoken in Pakistan: Urdu, Punjabi, Sindhi, Balochi, and Pashto. We
propose a novel framework that integrates an adapted Political Compass Test
(PCT) with a multi-level framing analysis. Our method combines quantitative
assessment of political orientation across economic (left-right) and social
(libertarian-authoritarian) axes with qualitative analysis of framing through
content, style, and emphasis. We further contextualize this analysis by
aligning prompts with 11 key socio-political themes relevant to Pakistani
society. Our results reveal that LLMs predominantly align with liberal-left
values, echoing Western training data influences, but exhibit notable shifts
toward authoritarian framing in regional languages, suggesting strong cultural
modulation effects. We also identify consistent model-specific bias signatures
and language-conditioned variations in ideological expression. These findings
show the urgent need for culturally grounded, multilingual bias auditing
frameworks.

</details>


### [9] [Evaluating the Sensitivity of LLMs to Prior Context](https://arxiv.org/abs/2506.00069)
*Robert Hankache,Kingsley Nketia Acheampong,Liang Song,Marek Brynda,Raad Khraishi,Greig A. Cowan*

Key words: 大语言模型、多轮对话、基准测试、上下文敏感性

TL;DR: 论文提出了关于大语言模型（LLM）在多轮对话中性能下降的新基准测试，揭示了多轮交互对模型性能的显著影响，并探讨了任务描述位置对性能的改善作用。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究目的是填补现有基准测试无法评估多轮对话对LLM性能影响的空白，以更全面地理解上下文对模型表现的作用。

Method: 作者设计了一套新型基准测试，系统性地变化上下文的量和性质，并评估了包括GPT、Claude和Gemini在内的多个LLM在这些基准上的表现。

Result: 研究发现多轮对话中LLM在多选题任务上的性能可能下降高达73%，即使是GPT-4o这样的高性能模型也表现出最多32%的准确率下降。任务描述的位置策略可以显著提升性能。

Conclusion: 研究表明需要更健壮的策略来设计、评估和缓解LLM对上下文的敏感性。

Abstract: As large language models (LLMs) are increasingly deployed in multi-turn
dialogue and other sustained interactive scenarios, it is essential to
understand how extended context affects their performance. Popular benchmarks,
focusing primarily on single-turn question answering (QA) tasks, fail to
capture the effects of multi-turn exchanges. To address this gap, we introduce
a novel set of benchmarks that systematically vary the volume and nature of
prior context. We evaluate multiple conventional LLMs, including GPT, Claude,
and Gemini, across these benchmarks to measure their sensitivity to contextual
variations. Our findings reveal that LLM performance on multiple-choice
questions can degrade dramatically in multi-turn interactions, with performance
drops as large as 73% for certain models. Even highly capable models such as
GPT-4o exhibit up to a 32% decrease in accuracy. Notably, the relative
performance of larger versus smaller models is not always predictable.
Moreover, the strategic placement of the task description within the context
can substantially mitigate performance drops, improving the accuracy by as much
as a factor of 3.5. These findings underscore the need for robust strategies to
design, evaluate, and mitigate context-related sensitivity in LLMs.

</details>


### [10] [Gaussian mixture models as a proxy for interacting language models](https://arxiv.org/abs/2506.00077)
*Edward Wang,Tianyu Wang,Avanti Athreya,Vince Lyzinski,Carey E. Priebe*

Key words: 大语言模型,高斯混合模型,社会科学,交互动态,计算效率

TL;DR: 本文提出高斯混合模型（GMMs）作为大语言模型（LLMs）替代方法，用于社会科学研究，并比较两者的动态特征。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为研究人类行为提供可扩展替代方案，避免LLMs的高计算成本。

Method: 引入交互GMMs，通过简化模型与LLMs的实验模拟比较动态特征。

Result: 交互GMMs能捕捉LLMs交互动态的关键特征，并揭示两者的异同。

Conclusion: GMMs在效率和可解释性上具有优势，未来可进一步优化研究方向。

Abstract: Large language models (LLMs) are a powerful tool with the ability to match
human capabilities and behavior in many settings. Retrieval-augmented
generation (RAG) further allows LLMs to generate diverse output depending on
the contents of their RAG database. This motivates their use in the social
sciences to study human behavior between individuals when large-scale
experiments are infeasible. However, LLMs depend on complex, computationally
expensive algorithms. In this paper, we introduce interacting Gaussian mixture
models (GMMs) as an alternative to similar frameworks using LLMs. We compare a
simplified model of GMMs to select experimental simulations of LLMs whose
updating and response depend on feedback from other LLMs. We find that
interacting GMMs capture important features of the dynamics in interacting
LLMs, and we investigate key similarities and differences between interacting
LLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture
models, potential modifications, and future research directions.

</details>


### [11] [COSMIC: Generalized Refusal Direction Identification in LLM Activations](https://arxiv.org/abs/2506.00085)
*Vincent Siu,Nicholas Crispino,Zihao Yu,Sam Pan,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Key words: 语言模型, 拒绝行为, 余弦相似度, 自动化引导, COSMIC框架

TL;DR: 论文提出了一种名为COSMIC的自动化框架，通过余弦相似度量识别语言模型中的拒绝行为方向，无需依赖预设模板或人工分析，实现了高效的模型行为引导。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法依赖预设模板或人工分析识别语言模型中的拒绝行为，存在局限性。作者旨在开发一种自动化框架，独立于模型输出，实现更高效的拒绝行为识别与引导。

Method: 提出COSMIC框架，利用余弦相似度量选择有效的引导方向和目标层，无需预设拒绝行为的假设（如特定拒绝标记）。

Result: COSMIC在识别拒绝方向和引导模型安全行为方面表现优异，具有广泛的鲁棒性，且虚假拒绝率增幅最小。

Conclusion: COSMIC为语言模型的拒绝行为识别和引导提供了一种高效、自动化的解决方案，适用于多种对齐条件和对抗场景。

Abstract: Large Language Models (LLMs) encode behaviors such as refusal within their
activation space, yet identifying these behaviors remains a significant
challenge. Existing methods often rely on predefined refusal templates
detectable in output tokens or require manual analysis. We introduce
\textbf{COSMIC} (Cosine Similarity Metrics for Inversion of Concepts), an
automated framework for direction selection that identifies viable steering
directions and target layers using cosine similarity - entirely independent of
model outputs. COSMIC achieves steering performance comparable to prior methods
without requiring assumptions about a model's refusal behavior, such as the
presence of specific refusal tokens. It reliably identifies refusal directions
in adversarial settings and weakly aligned models, and is capable of steering
such models toward safer behavior with minimal increase in false refusals,
demonstrating robustness across a wide range of alignment conditions.

</details>


### [12] [SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic Code-Switching Dataset](https://arxiv.org/abs/2506.00087)
*Peng Xie,Xingyuan Liu,Tsz Wai Chan,Yequan Bie,Yangqiu Song,Yang Wang,Hao Chen,Kani Chen*

Key words: 代码切换、多语言数据集、ASR、LinguaMaster、SwitchLingua、SAER

TL;DR: 论文介绍了LinguaMaster框架及SwitchLingua数据集，旨在解决多语言和多种族代码切换数据集的不足，并提出了一种新的评估指标SAER。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有单语数据集无法满足多语言应用（如CSASR、CSTTS、CLIR）的需求，且缺乏多样化、大规模的代码切换数据集。

Method: 通过LinguaMaster多智能体协作框架，构建SwitchLingua数据集，包含420K文本样本和80小时音频，并提出了SAER评估指标。

Result: 数据集涵盖12种语言、18个国家/地区的174名说话者，丰富了语言和文化多样性；SAER能更精确评估系统性能。

Conclusion: SwitchLingua为多语言和跨文化研究提供了基础资源，SAER弥补了现有ASR评估指标的不足。

Abstract: Code-switching (CS) is the alternating use of two or more languages within a
conversation or utterance, often influenced by social context and speaker
identity. This linguistic phenomenon poses challenges for Automatic Speech
Recognition (ASR) systems, which are typically designed for a single language
and struggle to handle multilingual inputs. The growing global demand for
multilingual applications, including Code-Switching ASR (CSASR), Text-to-Speech
(CSTTS), and Cross-Lingual Information Retrieval (CLIR), highlights the
inadequacy of existing monolingual datasets.
  Although some code-switching datasets exist, most are limited to bilingual
mixing within homogeneous ethnic groups, leaving a critical need for a
large-scale, diverse benchmark akin to ImageNet in computer vision.
  To bridge this gap, we introduce \textbf{LinguaMaster}, a multi-agent
collaboration framework specifically designed for efficient and scalable
multilingual data synthesis. Leveraging this framework, we curate
\textbf{SwitchLingua}, the first large-scale multilingual and multi-ethnic
code-switching dataset, including: (1) 420K CS textual samples across 12
languages, and (2) over 80 hours of audio recordings from 174 speakers
representing 18 countries/regions and 63 racial/ethnic backgrounds, based on
the textual data. This dataset captures rich linguistic and cultural diversity,
offering a foundational resource for advancing multilingual and multicultural
research. Furthermore, to address the issue that existing ASR evaluation
metrics lack sensitivity to code-switching scenarios, we propose the
\textbf{Semantic-Aware Error Rate (SAER)}, a novel evaluation metric that
incorporates semantic information, providing a more accurate and context-aware
assessment of system performance.

</details>


### [13] [HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.00088)
*Qing Li,Jiahui Geng,Zongxiong Chen,Derui Zhu,Yuxia Wang,Congbo Ma,Chenyang Lyu,Fakhri Karray*

Key words: 大语言模型,幻觉检测,神经微分方程,潜在空间,真实性评估

TL;DR: 提出了一种新方法HD-NDEs，通过神经微分方程捕捉LLMs的动态特性，以检测幻觉现象，相较于现有技术有显著提升。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管LLMs取得显著进展，但幻觉问题仍阻碍其实际应用，现有方法在输出中早期或中期出现非事实信息时效果有限。

Method: 利用神经微分方程建模LLMs潜在空间的动态系统，并将潜在空间序列映射到分类空间进行真实性评估。

Result: 在五个数据集和六种LLMs上的实验表明，HD-NDEs效果显著，尤其在True-False数据集上AUC-ROC提升超过14%。

Conclusion: HD-NDEs通过系统性评估LLMs潜在空间的动态特性，有效解决了幻觉检测问题，优于现有技术。

Abstract: In recent years, large language models (LLMs) have made remarkable
advancements, yet hallucination, where models produce inaccurate or non-factual
statements, remains a significant challenge for real-world deployment. Although
current classification-based methods, such as SAPLMA, are highly efficient in
mitigating hallucinations, they struggle when non-factual information arises in
the early or mid-sequence of outputs, reducing their reliability. To address
these issues, we propose Hallucination Detection-Neural Differential Equations
(HD-NDEs), a novel method that systematically assesses the truthfulness of
statements by capturing the full dynamics of LLMs within their latent space.
Our approaches apply neural differential equations (Neural DEs) to model the
dynamic system in the latent space of LLMs. Then, the sequence in the latent
space is mapped to the classification space for truth assessment. The extensive
experiments across five datasets and six widely used LLMs demonstrate the
effectiveness of HD-NDEs, especially, achieving over 14% improvement in AUC-ROC
on the True-False dataset compared to state-of-the-art techniques.

</details>


### [14] [Writing-Zero: Bridge the Gap Between Non-verifiable Problems and Verifiable Rewards](https://arxiv.org/abs/2506.00103)
*Xun Lu*

Key words: 强化学习、语言模型、奖励建模、创意写作、RLVR

TL;DR: 提出了一种统一的RLVR训练范式，适用于可验证和非可验证任务，通过基于写作原则的成对生成奖励模型和新型BRPO算法，提升语言模型的写作能力并避免奖励黑客问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 针对非可验证任务（如创意写作）缺乏客观评估标准的问题，现有基于标量奖励的方法泛化性差且易受奖励黑客影响。

Method: 引入成对生成奖励模型（GenRM）和BRPO算法，将主观评估转化为可验证奖励，并利用引导式响应进行无参考比较。

Result: 方法在写作任务中表现出持续的提升和对奖励黑客的强鲁棒性，同时在内部和开源基准上取得竞争性结果。

Conclusion: 研究表明RLVR框架有望统一基于规则、参考和无参考的奖励建模，为语言任务提供全面且可扩展的RL训练范式。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has enabled large
language models (LLMs) to achieve remarkable breakthroughs in reasoning tasks
with objective ground-truth answers, such as mathematics and code generation.
However, a significant gap remains for non-verifiable tasks, like creative
writing and open-ended dialogue, where quality assessment is inherently
subjective and lacks definitive references. Existing approaches for these
domains often rely on scalar reward models trained with human preferences,
which suffer from limited generalization and are prone to reward hacking, such
as over-explanation and length bias. In this work, we propose a unified
RLVR-based training paradigm that bridges the gap between non-verifiable tasks
and verifiable rewards. We introduce a writing-principle-based pairwise
Generative Reward Model (GenRM) and a novel Bootstrapped Relative Policy
Optimization (BRPO) algorithm. The pairwise writing GenRM leverages
self-principled critique to transform subjective assessments into reliable,
verifiable rewards, while BRPO enables dynamic, reference-free pairwise
comparison by leveraging a bootstrapped response as temporary reference from
within group rollouts during RL training. Our approach empowers LLMs to develop
robust writing capabilities without supervised fine-tuning, as demonstrated by
Writing-Zero, which shows consistent improvement and strong resistance to
reward hacking compared to scalar reward baselines. Furthermore, our method
achieves competitive results on both in-house and open-source writing
benchmarks. Our findings suggest the potential to unify rule-based,
reference-based, and reference-free reward modeling under the RLVR framework,
thus paving the way for a comprehensive and scalable RL training paradigm
applicable across all language tasks.

</details>


### [15] [Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models](https://arxiv.org/abs/2506.00134)
*Fardin Ahsan Sakib,Ziwei Zhu,Karen Trister Grace,Meliha Yetisgen,Ozlem Uzuner*

Key words: 健康社会决定因素, 大型语言模型, 临床文本, 药物状态提取, 提示工程, 思维链推理

TL;DR: 该论文探讨了大型语言模型（LLMs）在临床文本中提取健康社会决定因素（SDOH）时存在的问题，如依赖表面线索导致虚假预测，并发现在药物状态提取中存在性别差异。作者还评估了缓解策略，如提示工程和思维链推理，以提高LLMs在健康领域的可靠性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨LLMs在临床文本中提取SDOH时可能产生的虚假预测问题，特别是在药物状态提取中的性别差异，旨在提高模型的可靠性。

Method: 使用SHAC数据集的MIMIC部分，以药物状态提取为例，评估LLMs的预测问题，并提出提示工程和思维链推理等缓解策略。

Result: 研究发现，LLMs容易因酒精或吸烟的提及而错误预测药物使用情况，同时揭示了模型表现的性别差异。缓解策略（如提示工程）能够减少这些错误。

Conclusion: 提示工程和思维链推理等策略可以有效减少LLMs在健康领域中的虚假预测，提高模型可靠性，但性别差异问题仍需进一步研究。

Abstract: Social determinants of health (SDOH) extraction from clinical text is
critical for downstream healthcare analytics. Although large language models
(LLMs) have shown promise, they may rely on superficial cues leading to
spurious predictions. Using the MIMIC portion of the SHAC (Social History
Annotation Corpus) dataset and focusing on drug status extraction as a case
study, we demonstrate that mentions of alcohol or smoking can falsely induce
models to predict current/past drug use where none is present, while also
uncovering concerning gender disparities in model performance. We further
evaluate mitigation strategies - such as prompt engineering and
chain-of-thought reasoning - to reduce these false positives, providing
insights into enhancing LLM reliability in health domains.

</details>


### [16] [LaMP-QA: A Benchmark for Personalized Long-form Question Answering](https://arxiv.org/abs/2506.00137)
*Alireza Salemi,Hamed Zamani*

Key words: 个性化问答、LaMP-QA 基准、长答案生成、大语言模型

TL;DR: 论文提出了 LaMP-QA 基准，用于评估个性化长答案生成，涵盖三个主要问题类别，并通过实验证明个性化上下文可提升性能达 39%。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 针对个性化问答系统训练和评估资源不足的问题，填补了这一研究空白。

Method: 引入了 LaMP-QA 基准，涵盖多类别问题，结合人工与自动评估策略。

Result: 实验表明，融入个性化上下文可将性能提升高达 39%。

Conclusion: LaMP-QA 基准为个性化问答研究提供了支持，实验证实了个性化的重要性。

Abstract: Personalization is essential for question answering systems that are
user-centric. Despite its importance, personalization in answer generation has
been relatively underexplored. This is mainly due to lack of resources for
training and evaluating personalized question answering systems. We address
this gap by introducing LaMP-QA -- a benchmark designed for evaluating
personalized long-form answer generation. The benchmark covers questions from
three major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal
Development, and (3) Society & Culture, encompassing over 45 subcategories in
total. To assess the quality and potential impact of the LaMP-QA benchmark for
personalized question answering, we conduct comprehensive human and automatic
evaluations, to compare multiple evaluation strategies for evaluating generated
personalized responses and measure their alignment with human preferences.
Furthermore, we benchmark a number of non-personalized and personalized
approaches based on open-source and proprietary large language models (LLMs).
Our results show that incorporating the personalized context provided leads to
performance improvements of up to 39%. The benchmark is publicly released to
support future research in this area.

</details>


### [17] [Vedavani: A Benchmark Corpus for ASR on Vedic Sanskrit Poetry](https://arxiv.org/abs/2506.00145)
*Sujeet Kumar,Pretam Ray,Abhinay Beerukuri,Shrey Kamoji,Manoj Balaji Jagadeeshan,Pawan Goyal*

Key words: 梵语, 语音识别, 吠陀诗歌, ASR, 多语言模型

TL;DR: 论文介绍了首个针对梵语吠陀诗歌的语音识别研究Vedavani，提出了一个包含54小时标记音频的数据集，并通过实验表明IndicWhisper在现有模型中表现最佳。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 梵语的音位复杂性和语音变换为语音识别带来挑战，尤其在诗歌韵律中更显复杂。目前缺乏相关研究，尤其是吠陀诗歌的语音识别。

Method: 研究构建了一个包含54小时梵语吠陀诗歌音频的数据集（Rig Veda和Atharva Veda），共30,779个样本，捕捉了语言的韵律特征。采用多语言语音模型进行基准测试。

Result: IndicWhisper在多语言语音模型中表现最佳。

Conclusion: Vedavani为梵语吠陀诗歌的语音识别提供了首个研究基础和数据集，展示了IndicWhisper的优越性。

Abstract: Sanskrit, an ancient language with a rich linguistic heritage, presents
unique challenges for automatic speech recognition (ASR) due to its phonemic
complexity and the phonetic transformations that occur at word junctures,
similar to the connected speech found in natural conversations. Due to these
complexities, there has been limited exploration of ASR in Sanskrit,
particularly in the context of its poetic verses, which are characterized by
intricate prosodic and rhythmic patterns. This gap in research raises the
question: How can we develop an effective ASR system for Sanskrit, particularly
one that captures the nuanced features of its poetic form? In this study, we
introduce Vedavani, the first comprehensive ASR study focused on Sanskrit Vedic
poetry. We present a 54-hour Sanskrit ASR dataset, consisting of 30,779
labelled audio samples from the Rig Veda and Atharva Veda. This dataset
captures the precise prosodic and rhythmic features that define the language.
We also benchmark the dataset on various state-of-the-art multilingual speech
models.$^{1}$ Experimentation revealed that IndicWhisper performed the best
among the SOTA models.

</details>


### [18] [Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement](https://arxiv.org/abs/2506.00160)
*Qihui Fan,Enfu Nan,Wenbo Li,Lei Lu,Pu Zhao,Yanzhi Wang*

Key words: LLM, 狼人杀, TTS, 社交推理游戏

TL;DR: 本文提出了一种新颖且简单的基于LLM的狼人杀游戏系统，通过优化的TTS模型增强与不同LLM的兼容性和用户体验，认为随着LLM推理能力的提升，额外组件将变得不必要。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 社交推理游戏在商业应用和AI研究中的流行，以及LLM在推理和说服能力上的进步，促使设计更吸引人的LLM代理狼人杀游戏体验。

Method: 提出了一种基于LLM的狼人杀游戏系统，利用优化的TTS模型，减少对额外组件（如微调或高级提示工程）的依赖。

Result: 该系统实现了与多种LLM模型的兼容性提升，并优化了用户参与度。

Conclusion: 随着LLM推理能力的不断增强，额外组件的必要性将降低，简单的LLM系统即可满足需求。

Abstract: The growing popularity of social deduction game systems for both business
applications and AI research has greatly benefited from the rapid advancements
in Large Language Models (LLMs), which now demonstrate stronger reasoning and
persuasion capabilities. Especially with the raise of DeepSeek R1 and V3
models, LLMs should enable a more engaging experience for human players in
LLM-agent-based social deduction games like Werewolf. Previous works either
fine-tuning, advanced prompting engineering, or additional experience pool to
achieve engaging text-format Werewolf game experience. We propose a novel yet
straightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS)
models designed for enhanced compatibility with various LLM models, and
improved user engagement. We argue with ever enhancing LLM reasoning, extra
components will be unnecessary in the case of Werewolf.

</details>


### [19] [Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences](https://arxiv.org/abs/2506.00195)
*Mingqian Zheng,Wenjia Hu,Patrick Zhao,Motahhare Eslami,Jena D. Hwang,Faeze Brahman,Carolyn Rose,Maarten Sap*

Key words: LLMs, 拒绝策略, 用户体验, 安全性, 奖励模型

TL;DR: 研究显示，部分合规（提供一般信息但不提供可操作细节）是最优拒绝策略，能显著减少用户负面体验，但现有LLMs和奖励模型未能充分利用这一策略。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前LLMs在处理潜在有害查询时采取一刀切的拒绝策略，导致安全性与用户体验的权衡问题。

Method: 通过480名参与者评估3,840条查询-响应对，研究不同拒绝策略与用户动机对用户体验的影响，并分析9个LLMs的响应模式和6个奖励模型的评分。

Result: 部分合规策略能将用户负面感知降低50%以上，但现有模型和奖励模型未能自然采用或充分评估这一策略。

Conclusion: 有效的安全机制应专注于设计深思熟虑的拒绝策略，而非意图检测，以实现安全性与用户满意度的平衡。

Abstract: Current LLMs are trained to refuse potentially harmful input queries
regardless of whether users actually had harmful intents, causing a tradeoff
between safety and user experience. Through a study of 480 participants
evaluating 3,840 query-response pairs, we examine how different refusal
strategies affect user perceptions across varying motivations. Our findings
reveal that response strategy largely shapes user experience, while actual user
motivation has negligible impact. Partial compliance -- providing general
information without actionable details -- emerges as the optimal strategy,
reducing negative user perceptions by over 50% to flat-out refusals.
Complementing this, we analyze response patterns of 9 state-of-the-art LLMs and
evaluate how 6 reward models score different refusal strategies, demonstrating
that models rarely deploy partial compliance naturally and reward models
currently undervalue it. This work demonstrates that effective guardrails
require focusing on crafting thoughtful refusals rather than detecting intent,
offering a path toward AI safety mechanisms that ensure both safety and
sustained user engagement.

</details>


### [20] [Structuring Radiology Reports: Challenging LLMs with Lightweight Models](https://arxiv.org/abs/2506.00200)
*Johannes Moll,Louisa Fay,Asfandyar Azhar,Sophie Ostmeier,Tim Lueth,Sergios Gatidis,Curtis Langlotz,Jean-Benoit Delbrouck*

Key words: 放射学报告,轻量级模型,大型语言模型,结构化文本,医疗资源

TL;DR: 本文探索轻量级编码器-解码器模型（如T5和BERT2BERT）用于结构化放射学报告，发现其在性能和资源消耗上优于大型语言模型（LLMs）。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 放射学报告缺乏标准化格式，影响临床应用和机器学习处理，而大型语言模型虽强但存在计算资源高、透明度和隐私问题。

Method: 使用轻量级模型（<300M参数）和多种适配技术（前缀提示、上下文学习、LoRA微调）对MIMIC-CXR和CheXpert Plus数据集进行实验。

Result: 轻量级模型在人工标注测试集上表现优于提示技术适配的LLMs，虽部分LoRA微调LLMs在个别指标有提升，但资源消耗显著增加。

Conclusion: 轻量级模型是资源受限医疗环境中结构化临床文本的可持续且隐私保护的解决方案。

Abstract: Radiology reports are critical for clinical decision-making but often lack a
standardized format, limiting both human interpretability and machine learning
(ML) applications. While large language models (LLMs) have shown strong
capabilities in reformatting clinical text, their high computational
requirements, lack of transparency, and data privacy concerns hinder practical
deployment. To address these challenges, we explore lightweight encoder-decoder
models (<300M parameters)-specifically T5 and BERT2BERT-for structuring
radiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark
these models against eight open-source LLMs (1B-70B), adapted using prefix
prompting, in-context learning (ICL), and low-rank adaptation (LoRA)
finetuning. Our best-performing lightweight model outperforms all LLMs adapted
using prompt-based techniques on a human-annotated test set. While some
LoRA-finetuned LLMs achieve modest gains over the lightweight model on the
Findings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%,
GREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of
substantially greater computational resources. For example, LLaMA-3-70B
incurred more than 400 times the inference time, cost, and carbon emissions
compared to the lightweight model. These results underscore the potential of
lightweight, task-specific models as sustainable and privacy-preserving
solutions for structuring clinical text in resource-constrained healthcare
settings.

</details>


### [21] [Structure-Aware Fill-in-the-Middle Pretraining for Code](https://arxiv.org/abs/2506.00204)
*Linyuan Gong,Alvin Cheung,Mostafa Elhoushi,Sida Wang*

Key words: AST-FIM, Abstract Syntax Tree, Fill-in-the-Middle, pretraining, code LLM

TL;DR: AST-FIM是一种基于抽象语法树（AST）的预训练方法，通过屏蔽完整的语法结构来提高代码LLM的填充能力，优于传统随机字符屏蔽方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的代码LLM将代码视为普通文本，随机屏蔽字符跨度，无法有效捕捉代码结构和常见编辑模式，如代码块、表达式或函数。

Method: 提出了AST-FIM方法，利用AST屏蔽完整语法结构，生成更符合代码结构和编辑模式的训练示例。

Result: 在1B和8B参数模型上，AST-FIM在标准FIM基准测试中比随机字符屏蔽方法高出5分，尤其适合实际代码编辑任务。

Conclusion: AST-FIM通过利用代码的语法结构显著提升了代码填充任务的性能，展示了其在代码LLM中的潜力。

Abstract: Fill-in-the-Middle (FIM) is a common pretraining method for code LLMs, where
models complete code segments given surrounding context. However, existing LLMs
treat code as plain text and mask random character spans. We propose and
evaluate AST-FIM, a pretraining strategy that leverages Abstract Syntax Trees
(ASTs) to mask complete syntactic structures at scale, ensuring coherent
training examples better aligned with universal code structures and common code
editing patterns such as blocks, expressions, or functions. To evaluate
real-world fill-in-the-middle (FIM) programming tasks, we introduce
Real-FIM-Eval, a benchmark derived from 30,000+ GitHub commits across 12
languages. On infilling tasks, experiments on 1B and 8B parameter models show
that AST-FIM is particularly beneficial for real-world code editing as it
outperforms standard random-character FIM by up to 5 pts on standard FIM
benchmarks. Our code is publicly available at
https://github.com/gonglinyuan/ast_fim.

</details>


### [22] [REIC: RAG-Enhanced Intent Classification at Scale](https://arxiv.org/abs/2506.00210)
*Ziji Zhang,Michael Yang,Zhiyu Chen,Yingying Zhuang,Shu-Ting Pi,Qun Liu,Rajashekar Maragoud,Vy Nguyen,Anurag Beniwal*

Key words: 意图分类,检索增强生成,客服,大规模分类

TL;DR: REIC是一种基于检索增强生成的意图分类方法，解决了扩展性问题，并在大规模客服场景中优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 企业产品线扩展导致意图分类面临规模化和分类标准变化的挑战。

Method: 采用检索增强生成（RAG）动态整合相关知识，避免频繁重新训练。

Result: 在真实数据集上，REIC优于传统微调、零样本和少样本方法，适用于多种场景。

Conclusion: REIC在大规模自适应意图分类系统中具有实际应用潜力。

Abstract: Accurate intent classification is critical for efficient routing in customer
service, ensuring customers are connected with the most suitable agents while
reducing handling times and operational costs. However, as companies expand
their product lines, intent classification faces scalability challenges due to
the increasing number of intents and variations in taxonomy across different
verticals. In this paper, we introduce REIC, a Retrieval-augmented generation
Enhanced Intent Classification approach, which addresses these challenges
effectively. REIC leverages retrieval-augmented generation (RAG) to dynamically
incorporate relevant knowledge, enabling precise classification without the
need for frequent retraining. Through extensive experiments on real-world
datasets, we demonstrate that REIC outperforms traditional fine-tuning,
zero-shot, and few-shot methods in large-scale customer service settings. Our
results highlight its effectiveness in both in-domain and out-of-domain
scenarios, demonstrating its potential for real-world deployment in adaptive
and large-scale intent classification systems.

</details>


### [23] [ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering](https://arxiv.org/abs/2506.00232)
*Ruofan Wu,Youngwon Lee,Fan Shu,Danmei Xu,Seung-won Hwang,Zhewei Yao,Yuxiong He,Feng Yan*

Key words: RAG, 模块化设计, 多跳问答, 自反思, 答案验证

TL;DR: ComposeRAG是一种新颖的模块化RAG（检索增强生成）框架，通过分解核心功能为原子模块（如问题分解、查询改写、检索决策和答案验证），提升了系统的可解释性、可评估性和性能。在多跳问答任务中，ComposeRAG显著优于基线方法，尤其在准确性和答案基础性方面表现突出。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的RAG系统通常采用紧密耦合的单体设计，限制了其可解释性、系统性评估和针对性改进的能力，尤其是在复杂的多跳问答任务中。ComposeRAG旨在通过模块化设计解决这些问题。

Method: ComposeRAG将RAG流水线分解为原子化的可组合模块（如问题分解、查询改写等），每个模块独立实现和优化，并引入自反思机制以增强多步推理的鲁棒性。

Result: 在四个多跳问答基准测试中，ComposeRAG在准确性和答案基础性上均优于基线方法，最高提升15%的准确率和10%的基础性（低质量检索条件下）。

Conclusion: ComposeRAG通过模块化设计和自反思机制，显著提升了RAG系统的灵活性、透明性、可扩展性和性能，同时改善了答案的基础性和可解释性。

Abstract: Retrieval-Augmented Generation (RAG) systems are increasingly diverse, yet
many suffer from monolithic designs that tightly couple core functions like
query reformulation, retrieval, reasoning, and verification. This limits their
interpretability, systematic evaluation, and targeted improvement, especially
for complex multi-hop question answering. We introduce ComposeRAG, a novel
modular abstraction that decomposes RAG pipelines into atomic, composable
modules. Each module, such as Question Decomposition, Query Rewriting,
Retrieval Decision, and Answer Verification, acts as a parameterized
transformation on structured inputs/outputs, allowing independent
implementation, upgrade, and analysis. To enhance robustness against errors in
multi-step reasoning, ComposeRAG incorporates a self-reflection mechanism that
iteratively revisits and refines earlier steps upon verification failure.
Evaluated on four challenging multi-hop QA benchmarks, ComposeRAG consistently
outperforms strong baselines in both accuracy and grounding fidelity.
Specifically, it achieves up to a 15% accuracy improvement over
fine-tuning-based methods and up to a 5% gain over reasoning-specialized
pipelines under identical retrieval conditions. Crucially, ComposeRAG
significantly enhances grounding: its verification-first design reduces
ungrounded answers by over 10% in low-quality retrieval settings, and by
approximately 3% even with strong corpora. Comprehensive ablation studies
validate the modular architecture, demonstrating distinct and additive
contributions from each component. These findings underscore ComposeRAG's
capacity to deliver flexible, transparent, scalable, and high-performing
multi-hop reasoning with improved grounding and interpretability.

</details>


### [24] [MedOrch: Medical Diagnosis with Tool-Augmented Reasoning Agents for Flexible Extensibility](https://arxiv.org/abs/2506.00235)
*Yexiao He,Ang Li,Boyi Liu,Zhewei Yao,Yuxiong He*

Key words: 医疗决策,AI,MedOrch,模块化架构,多任务评估

TL;DR: 提出了一种名为MedOrch的新框架，通过协调多个专用工具和推理代理，为医疗决策提供支持。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 医疗决策是AI最具挑战性的领域之一，现有系统要么适应性有限，要么缺乏专业知识支持。

Method: MedOrch采用模块化、基于代理的架构，灵活集成领域专用工具，并确保推理过程透明可追溯。

Result: 在阿尔茨海默病诊断等三个医疗应用中表现优异，部分任务超越现有基线。

Conclusion: MedOrch展示了在复杂医疗决策中驱动工具使用和处理多模态数据的潜力。

Abstract: Healthcare decision-making represents one of the most challenging domains for
Artificial Intelligence (AI), requiring the integration of diverse knowledge
sources, complex reasoning, and various external analytical tools. Current AI
systems often rely on either task-specific models, which offer limited
adaptability, or general language models without grounding with specialized
external knowledge and tools. We introduce MedOrch, a novel framework that
orchestrates multiple specialized tools and reasoning agents to provide
comprehensive medical decision support. MedOrch employs a modular, agent-based
architecture that facilitates the flexible integration of domain-specific tools
without altering the core system. Furthermore, it ensures transparent and
traceable reasoning processes, enabling clinicians to meticulously verify each
intermediate step underlying the system's recommendations. We evaluate MedOrch
across three distinct medical applications: Alzheimer's disease diagnosis,
chest X-ray interpretation, and medical visual question answering, using
authentic clinical datasets. The results demonstrate MedOrch's competitive
performance across these diverse medical tasks. Notably, in Alzheimer's disease
diagnosis, MedOrch achieves an accuracy of 93.26%, surpassing the
state-of-the-art baseline by over four percentage points. For predicting
Alzheimer's disease progression, it attains a 50.35% accuracy, marking a
significant improvement. In chest X-ray analysis, MedOrch exhibits superior
performance with a Macro AUC of 61.2% and a Macro F1-score of 25.5%. Moreover,
in complex multimodal visual question answering (Image+Table), MedOrch achieves
an accuracy of 54.47%. These findings underscore MedOrch's potential to advance
healthcare AI by enabling reasoning-driven tool utilization for multimodal
medical data processing and supporting intricate cognitive tasks in clinical
decision-making.

</details>


### [25] [PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain](https://arxiv.org/abs/2506.00250)
*Mohammad Javad Ranjbar Kalahroodi,Amirhossein Sheikholselami,Sepehr Karimi,Sepideh Ranjbar Kalahroodi,Heshaam Faili,Azadeh Shakery*

Key words: Large Language Models, PersianMedQA, 医疗问答, 多语言评估

TL;DR: 该论文介绍了PersianMedQA，一个用于评估大型语言模型在波斯语和英语中医疗问答能力的专家验证数据集，并通过基准测试比较了40多种模型的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究大型语言模型在低资源语言（如波斯语）和高风险领域（如医学）中的可靠性问题。

Method: 构建PersianMedQA数据集，并对40多种大型语言模型进行零样本和思维链设置下的测试。

Result: 封闭源通用模型（如GPT-4.1）表现最优，而波斯语微调模型表现较差。翻译对性能有影响，模型大小不足以保证性能。

Conclusion: PersianMedQA为评估多语言和医学推理能力提供了基础，强调领域或语言适应的重要性。

Abstract: Large Language Models (LLMs) have achieved remarkable performance on a wide
range of NLP benchmarks, often surpassing human-level accuracy. However, their
reliability in high-stakes domains such as medicine, particularly in
low-resource languages, remains underexplored. In this work, we introduce
PersianMedQA, a large-scale, expert-validated dataset of multiple-choice
Persian medical questions, designed to evaluate LLMs across both Persian and
English. We benchmark over 40 state-of-the-art models, including
general-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and
chain-of-thought (CoT) settings. Our results show that closed-source general
models (e.g., GPT-4.1) consistently outperform all other categories, achieving
83.3% accuracy in Persian and 80.7% in English, while Persian fine-tuned models
such as Dorna underperform significantly (e.g., 35.9% in Persian), often
struggling with both instruction-following and domain reasoning. We also
analyze the impact of translation, showing that while English performance is
generally higher, Persian responses are sometimes more accurate due to cultural
and clinical contextual cues. Finally, we demonstrate that model size alone is
insufficient for robust performance without strong domain or language
adaptation. PersianMedQA provides a foundation for evaluating multilingual and
culturally grounded medical reasoning in LLMs. The PersianMedQA dataset can be
accessed at:
https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA](https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA

</details>


### [26] [Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race](https://arxiv.org/abs/2506.00253)
*Lihao Sun,Chengzhi Mao,Valentin Hofmann,Xuechunzi Bai*

Key words: 语言模型，隐式偏见，对齐模型，偏见缓解，种族概念

TL;DR: 本研究揭示了对齐的语言模型在隐式偏见任务中表现出偏见，并提出了一种新的偏见缓解策略，通过激励模型在早期层表示种族概念来解决问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管对齐的语言模型在显式偏见评估中表现无偏见，但在隐式关联任务中仍存在偏见，研究旨在探究这种差异的机制并提出解决方案。

Method: 研究发现对齐模型在模糊上下文中忽略种族概念，提出通过激励模型在早期层表示种族概念来缓解偏见。

Result: 实验表明，这种方法比传统的机器遗忘方法更有效地减少了隐式偏见。

Conclusion: 忽略种族概念可能无意中延续偏见，新的干预策略通过增强种族意识来改善公平性。

Abstract: Although value-aligned language models (LMs) appear unbiased in explicit bias
evaluations, they often exhibit stereotypes in implicit word association tasks,
raising concerns about their fair usage. We investigate the mechanisms behind
this discrepancy and find that alignment surprisingly amplifies implicit bias
in model outputs. Specifically, we show that aligned LMs, unlike their
unaligned counterparts, overlook racial concepts in early internal
representations when the context is ambiguous. Not representing race likely
fails to activate safety guardrails, leading to unintended biases. Inspired by
this insight, we propose a new bias mitigation strategy that works by
incentivizing the representation of racial concepts in the early model layers.
In contrast to conventional mitigation methods of machine unlearning, our
interventions find that steering the model to be more aware of racial concepts
effectively mitigates implicit bias. Similar to race blindness in humans,
ignoring racial nuances can inadvertently perpetuate subtle biases in LMs.

</details>


### [27] [The Impact of Disability Disclosure on Fairness and Bias in LLM-Driven Candidate Selection](https://arxiv.org/abs/2506.00256)
*Mahammed Kamruzzaman,Gene Louis Kim*

Key words: LLM, 招聘, 公平性, 残疾信息, 偏见

TL;DR: 研究发现，在LLM驱动的招聘中，披露残疾信息的候选人更少被选择，即使候选人选择不披露，也比明确表示无残疾的候选人更不受青睐。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨LLM在招聘过程中对候选人自愿披露残疾信息的潜在偏见，填补现有研究的空白。

Method: 比较相同背景但披露/不披露残疾信息的候选人在LLM筛选中的表现。

Result: LLM更倾向于选择未披露残疾的候选人，即使未披露也仍处于劣势。

Conclusion: LLM在招聘中存在对残疾信息的潜在偏见，需进一步研究以确保公平。

Abstract: As large language models (LLMs) become increasingly integrated into hiring
processes, concerns about fairness have gained prominence. When applying for
jobs, companies often request/require demographic information, including
gender, race, and disability or veteran status. This data is collected to
support diversity and inclusion initiatives, but when provided to LLMs,
especially disability-related information, it raises concerns about potential
biases in candidate selection outcomes. Many studies have highlighted how
disability can impact CV screening, yet little research has explored the
specific effect of voluntarily disclosed information on LLM-driven candidate
selection. This study seeks to bridge that gap. When candidates shared
identical gender, race, qualifications, experience, and backgrounds, and sought
jobs with minimal employment rate gaps between individuals with and without
disabilities (e.g., Cashier, Software Developer), LLMs consistently favored
candidates who disclosed that they had no disability. Even in cases where
candidates chose not to disclose their disability status, the LLMs were less
likely to select them compared to those who explicitly stated they did not have
a disability.

</details>


### [28] [MultiHoax: A Dataset of Multi-hop False-Premise Questions](https://arxiv.org/abs/2506.00264)
*Mohammadamin Shafiei,Hamidreza Saffari,Nafise Sadat Moosavi*

Key words: 大型语言模型, 错误前提问题, 多跳推理, 基准测试

TL;DR: 论文介绍了MultiHoax基准测试，用于评估大型语言模型(LLMs)在复杂多步推理任务中处理错误前提的能力，揭示了现有模型的不足之处。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型在关键领域的部署日益增多，识别错误前提并进行多步推理的能力对确保可靠输出至关重要。

Method: 引入MultiHoax基准测试，涵盖七个国家和十个知识类别，使用维基百科作为知识源，评估LLMs在多跳推理中的表现。

Result: 实验表明，当前最先进的LLMs在不同国家、知识类别和多跳推理类型中检测错误前提的能力不足。

Conclusion: 研究突显了提升LLMs错误前提检测能力和多跳推理鲁棒性的必要性。

Abstract: As Large Language Models are increasingly deployed in high-stakes domains,
their ability to detect false assumptions and reason critically is crucial for
ensuring reliable outputs. False-premise questions (FPQs) serve as an important
evaluation method by exposing cases where flawed assumptions lead to incorrect
responses. While existing benchmarks focus on single-hop FPQs, real-world
reasoning often requires multi-hop inference, where models must verify
consistency across multiple reasoning steps rather than relying on
surface-level cues. To address this gap, we introduce MultiHoax, a benchmark
for evaluating LLMs' ability to handle false premises in complex, multi-step
reasoning tasks. Our dataset spans seven countries and ten diverse knowledge
categories, using Wikipedia as the primary knowledge source to enable factual
reasoning across regions. Experiments reveal that state-of-the-art LLMs
struggle to detect false premises across different countries, knowledge
categories, and multi-hop reasoning types, highlighting the need for improved
false premise detection and more robust multi-hop reasoning capabilities in
LLMs.

</details>


### [29] [CASPER: A Large Scale Spontaneous Speech Dataset](https://arxiv.org/abs/2506.00267)
*Cihan Xiao,Ruixing Liang,Xiangyu Zhang,Mehmet Emre Tiryaki,Veronica Bae,Lavanya Shankar,Rong Yang,Ethan Poon,Emmanuel Dupoux,Sanjeev Khudanpur,Leibny Paola Garcia Perera*

Key words: 自发语音数据, 自然对话, 数据集收集, 语音处理

TL;DR: 该论文提出了一种新的流程来收集自然对话数据，解决了高质量自发语音数据稀缺的问题，并发布了包含200+小时自发语音的Stage 1数据集。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型的成功激发了开发类似语音处理能力的兴趣，但现有数据集多为脚本对话，缺乏高质量的自发语音数据。

Method: 设计了一种新流程，通过引导和记录自然对话，鼓励多样化和互动性强的交流，与传统方法不同，它促进了真实的互动。

Result: 发布了Stage 1数据集，包含200+小时的自发语音数据，为研究社区提供了重要的资源。

Conclusion: 该论文为填补自发语音数据短缺奠定了基础，并计划在未来扩展数据集，为研究社区提供持续增长的资源。

Abstract: The success of large language models has driven interest in developing
similar speech processing capabilities. However, a key challenge is the
scarcity of high-quality spontaneous speech data, as most existing datasets
contain scripted dialogues. To address this, we present a novel pipeline for
eliciting and recording natural dialogues and release our Stage 1 dataset with
200+ hours of spontaneous speech. Our approach fosters fluid, natural
conversations while encouraging a diverse range of topics and interactive
exchanges. Unlike traditional methods, it facilitates genuine interactions,
providing a reproducible framework for future data collection. This paper
introduces our dataset and methodology, laying the groundwork for addressing
the shortage of spontaneous speech data. We plan to expand this dataset in
future stages, offering a growing resource for the research community.

</details>


### [30] [Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings](https://arxiv.org/abs/2506.00277)
*Hans W. A. Hanley,Zakir Durumeric*

Key words: 主题建模、聚类、多语言嵌入、层次聚类、新闻分析

TL;DR: 提出了一种新型、可扩展、可解释、层次化和多语言的新闻及社交媒体数据聚类方法，利用多语言Matryoshka嵌入模型和高效层次聚类算法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的大语言模型嵌入方法在多语言环境和可扩展性上表现不佳，且相似性度量不透明。

Method: 训练多语言Matryoshka嵌入模型，支持不同粒度的故事相似性度量，并开发基于层次结构的聚类算法。

Result: 嵌入模型在SemEval 2022 Task 8测试数据集上取得Pearson ρ=0.816的SOTA性能。

Conclusion: 该方法能有效聚类新闻故事、叙事和主题，适用于实际数据。

Abstract: Contextual large language model embeddings are increasingly utilized for
topic modeling and clustering. However, current methods often scale poorly,
rely on opaque similarity metrics, and struggle in multilingual settings. In
this work, we present a novel, scalable, interpretable, hierarchical, and
multilingual approach to clustering news articles and social media data. To do
this, we first train multilingual Matryoshka embeddings that can determine
story similarity at varying levels of granularity based on which subset of the
dimensions of the embeddings is examined. This embedding model achieves
state-of-the-art performance on the SemEval 2022 Task 8 test dataset (Pearson
$\rho$ = 0.816). Once trained, we develop an efficient hierarchical clustering
algorithm that leverages the hierarchical nature of Matryoshka embeddings to
identify unique news stories, narratives, and themes. We conclude by
illustrating how our approach can identify and cluster stories, narratives, and
overarching themes within real-world news datasets.

</details>


### [31] [Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation](https://arxiv.org/abs/2506.00288)
*Ahmed Elhady,Eneko Agirre,Mikel Artetxe*

Key words: Continued pretraining, language adaptation, catastrophic forgetting, curriculum learning, exponential moving average

TL;DR: Continued pretraining (CPT) with English data in the mix doesn't affect perplexity but is crucial for downstream task performance in the target language. Excluding English leads to catastrophic forgetting and poor generalization, mitigated by curriculum learning and EMA.

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: To understand the role of English data in CPT for adapting LLMs to new languages, especially its influence on downstream capabilities despite not impacting perplexity.

Method: Introduces a language-agnostic benchmark for in-context learning (ICL), studies catastrophic forgetting, and proposes curriculum learning and EMA to reduce reliance on English data.

Result: Including English prevents catastrophic forgetting and ensures better generalization, while exclusion leads to poor performance despite similar perplexity.

Conclusion: English data is vital for emergent abilities during CPT, and alternative methods like curriculum learning and EMA can help mitigate its necessity.

Abstract: Continued pretraining (CPT) is a popular approach to adapt existing large
language models (LLMs) to new languages. When doing so, it is common practice
to include a portion of English data in the mixture, but its role has not been
carefully studied to date. In this work, we show that including English does
not impact validation perplexity, yet it is critical for the emergence of
downstream capabilities in the target language. We introduce a
language-agnostic benchmark for in-context learning (ICL), which reveals
catastrophic forgetting early on CPT when English is not included. This in turn
damages the ability of the model to generalize to downstream prompts in the
target language as measured by perplexity, even if it does not manifest in
terms of accuracy until later in training, and can be tied to a big shift in
the model parameters. Based on these insights, we introduce curriculum learning
and exponential moving average (EMA) of weights as effective alternatives to
mitigate the need for English. All in all, our work sheds light into the
dynamics by which emergent abilities arise when doing CPT for language
adaptation, and can serve as a foundation to design more effective methods in
the future.

</details>


### [32] [DLM-One: Diffusion Language Models for One-Step Sequence Generation](https://arxiv.org/abs/2506.00290)
*Tianqi Chen,Shujian Zhang,Mingyuan Zhou*

Key words: DLM-One, 扩散模型, 语言生成, 分数蒸馏

TL;DR: DLM-One 是一种基于分数蒸馏的框架，用于通过连续扩散语言模型（DLMs）实现一步序列生成，显著提升了采样效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 旨在解决传统扩散模型需要迭代细化的问题，通过一步生成提升效率。

Method: 通过将学生模型输出在连续标记嵌入空间中的分数与教师 DLM 的分数函数对齐，实现一步生成。

Result: 在 DiffuSeq 上实验显示，推理时间可提升约 500 倍，同时保持生成质量。

Conclusion: 一步扩散为高效、高质量语言生成提供了新方向。

Abstract: This paper introduces DLM-One, a score-distillation-based framework for
one-step sequence generation with continuous diffusion language models (DLMs).
DLM-One eliminates the need for iterative refinement by aligning the scores of
a student model's outputs in the continuous token embedding space with the
score function of a pretrained teacher DLM. We investigate whether DLM-One can
achieve substantial gains in sampling efficiency for language modeling. Through
comprehensive experiments on DiffuSeq -- a representative continuous DLM -- we
show that DLM-One achieves up to ~500x speedup in inference time while
maintaining competitive performance on benchmark text generation tasks used to
evaluate the teacher models. We further analyze the method's empirical behavior
across multiple datasets, providing initial insights into its generality and
practical applicability. Our findings position one-step diffusion as a
promising direction for efficient, high-quality language generation and broader
adoption of continuous diffusion models operating in embedding space for
natural language processing.

</details>


### [33] [Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion with LLMs](https://arxiv.org/abs/2506.00304)
*Payal Mohapatra,Akash Pandey,Xiaoyuan Zhang,Qi Zhu*

Key words: 无语音肌电信号, 大语言模型, EMG-to-text, 通讯辅助

TL;DR: 这篇论文提出了一种新颖的EMG适配器模块，将无语音肌电信号（EMG）特征映射到大语言模型（LLM）的输入空间，实现了对无语音肌电信号到文本的高效转换。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为无法发声的个体提供实用且高效的通讯工具，克服现有方法依赖配对有声和无声EMG信号的局限性。

Method: 提出EMG适配器模块，将EMG特征映射到LLM输入空间，仅利用无声EMG信号进行训练。

Result: 在闭词汇无语音EMG-to-text任务中，平均词错误率（WER）为0.49，数据量仅六分钟时性能提升近20%。

Conclusion: 该研究为LLM理解无声肌电信号迈出了关键一步，展示了其在通讯辅助技术中的潜力。

Abstract: Unvoiced electromyography (EMG) is an effective communication tool for
individuals unable to produce vocal speech. However, most prior methods rely on
paired voiced and unvoiced EMG signals, along with speech data, for EMG-to-text
conversion, which is not practical for such individuals. Given the rise of
large language models (LLMs) in speech recognition, we explore their potential
to understand unvoiced speech. To this end, we address the challenge of
learning from unvoiced EMG alone and propose a novel EMG adaptor module that
maps EMG features into an LLM's input space, achieving an average word error
rate (WER) of 0.49 on a closed-vocabulary unvoiced EMG-to-text task. Even with
a conservative data availability of just six minutes, our approach improves
performance over specialized models by nearly 20%. While LLMs have been shown
to be extendable to new language modalities -- such as audio -- understanding
articulatory biosignals like unvoiced EMG remains more challenging. This work
takes a crucial first step toward enabling LLMs to comprehend unvoiced speech
using surface EMG.

</details>


### [34] [Lossless Token Sequence Compression via Meta-Tokens](https://arxiv.org/abs/2506.00307)
*John Harvill,Ziwei Fan,Hao Wang,Yizhou Sun,Hao Ding,Luke Huan,Anoop Deoras*

Key words: LLM, 提示词压缩, 无损压缩, 计算效率, 语义保留

TL;DR: 本文提出了一种任务无关的无损压缩技术，类似LZ77，平均减少输入token序列长度27%和18%，显著降低计算成本且零语义损失。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基于LLM的提示词压缩方法多为有损压缩，而本文旨在开发一种无损压缩方法，以严格保留语义/语法信息。

Method: 采用类似LZ77的无损压缩技术，通过token序列变换减少输入长度，且变换可逆。

Result: 在需要严格语义保留的任务中，压缩后平均减少token序列长度18-27%，相当于减少33-47%的计算开销，且性能损失极小。

Conclusion: 无损压缩在保留语义的同时显著降低计算成本，未来更大模型或计算资源可能完全消除性能差距。

Abstract: Existing work on prompt compression for Large Language Models (LLM) focuses
on lossy methods that try to maximize the retention of semantic information
that is relevant to downstream tasks while significantly reducing the sequence
length. In this paper, we introduce a task-agnostic lossless compression
technique similar to LZ77 that makes it possible to reduce the input token
sequence length on average by 27\% and 18\% for the two evaluation tasks
explored here. Given that we use transformer-based LLMs, this equates to 47\%
and 33\% less encoding computation, respectively, due to the quadratic nature
of attention. The token sequence transformation is trivial to reverse and
highlights that no semantic information is lost in the process. We evaluate our
proposed approach on two tasks that require strict preservation of
semantics/syntax and demonstrate that existing lossy compression methods
perform poorly in this setting. We find that our lossless compression technique
produces only a small gap in performance compared to using the uncompressed
input and posit that larger models and an expanded computing budget would
likely erase the gap entirely.

</details>


### [35] [An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and DeepSeek-V3](https://arxiv.org/abs/2506.00312)
*Brendan Sands,Yining Wang,Chenhao Xu,Yuxuan Zhou,Lai Wei,Rohitash Chandra*

Key words: 大语言模型, 电影评论生成, 情感分析, IMDb评论, 文本生成

TL;DR: 研究提出了一种利用GPT-4o、DeepSeek-V3和Gemini-2.0三种大语言模型生成电影评论的框架，并通过与IMDb用户评论比较评估其性能。结果显示，虽然LLM生成的评论在语法和结构上表现良好，但在情感丰富性和风格一致性上仍有差距。DeepSeek-V3表现最平衡，接近IMDb评论。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨大语言模型在电影评论生成任务中的适用性，并与真实用户评论进行对比，以评估其性能差距。

Method: 使用电影字幕和剧本作为输入，通过三种LLM生成评论，并从词汇、情感极性、相似性和主题一致性等方面与IMDb评论对比。此外，还进行了参与者区分测试。

Result: LLM能生成语法流畅且结构完整的评论，但情感丰富性和风格一致性不足。DeepSeek-V3表现最接近IMDb评论，GPT-4o偏向积极情感，Gemini-2.0擅长捕捉消极情感但情感强度过高。

Conclusion: LLM在电影评论生成任务中表现出潜力，但需进一步优化以提升情感和风格的匹配度。

Abstract: Large language models (LLMs) have been prominent in various tasks, including
text generation and summarisation. The applicability of LLMs to the generation
of product reviews is gaining momentum, paving the way for the generation of
movie reviews. In this study, we propose a framework that generates movie
reviews using three LLMs (GPT-4o, DeepSeek-V3, and Gemini-2.0), and evaluate
their performance by comparing the generated outputs with IMDb user reviews. We
use movie subtitles and screenplays as input to the LLMs and investigate how
they affect the quality of reviews generated. We review the LLM-based movie
reviews in terms of vocabulary, sentiment polarity, similarity, and thematic
consistency in comparison to IMDB user reviews. The results demonstrate that
LLMs are capable of generating syntactically fluent and structurally complete
movie reviews. Nevertheless, there is still a noticeable gap in emotional
richness and stylistic coherence between LLM-generated and IMDb reviews,
suggesting that further refinement is needed to improve the overall quality of
movie review generation. We provided a survey-based analysis where participants
were told to distinguish between LLM and IMDb user reviews. The results show
that LLM-generated reviews are difficult to distinguish from IMDB user reviews.
We found that DeepSeek-V3 produced the most balanced reviews, closely matching
IMDb reviews. GPT-4o overemphasised positive emotions, while Gemini-2.0
captured negative emotions better but showed excessive emotional intensity.

</details>


### [36] [SkillVerse : Assessing and Enhancing LLMs with Tree Evaluation](https://arxiv.org/abs/2506.00319)
*Yufei Tian,Jiao Sun,Nanyun Peng,Zizhao Zhang*

Key words: 语言模型、评估框架、SkillVerse、树状结构、情境学习

TL;DR: SkillVerse是一个无监督树状诊断框架，用于评估语言模型在特定技能上的能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着语言模型处理复杂任务能力的增强，需要更细粒度的评估方法来理解模型的具体能力。

Method: 使用LLM作为评判者，SkillVerse首先对模型响应进行批评，然后将其组织为树状结构（dendrogram）。

Result: SkillVerse提升了模型的情境学习能力25%，并成功预测模型弱点的准确率提高了22%。

Conclusion: SkillVerse为理解和改进语言模型提供了灵活且有效的工具。

Abstract: As language models evolve to tackle complex, multifaceted tasks, their
evaluation must adapt to capture this intricacy. A granular, skill-specific
understanding of model capabilities can empower researchers to make informed
model development plans. In this paper, we introduce SkillVerse, an
unsupervised tree-structured diagnosis framework for understanding model
proficiency in specific abilities. With LLM as a judge, SkillVerse first
critiques the model responses, and then organizes them into a hierarchical
structure termed dendrogram. Given proficiency at arbitrary levels of
granularity, SkillVerse is flexible to produce insights of behaviors of modern
large models. We also demonstrate its efficacy in two downstream tasks: 1)
improving model in-context learning by 25% using a tree-search algorithm to
select more informative few-shot demonstrations, and 2) accurately predicting
new model weaknesses with a 55% success rate, 22% higher than without
SkillVerse.

</details>


### [37] [TreeRare: Syntax Tree-Guided Retrieval and Reasoning for Knowledge-Intensive Question Answering](https://arxiv.org/abs/2506.00331)
*Boyi Zhang,Zhuo Liu,Hangfeng He*

Key words: TreeRare, 语法树, 检索推理, 问答系统, LLMs

TL;DR: TreeRare 是一种基于语法树的检索与推理框架，通过自底向上遍历语法树解决复杂性知识密集型问题，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 复杂问题需要 LLMs 跨多源推理，但现有检索框架因推理错误和结果不对齐而受限。

Method: TreeRare 利用语法树生成子问题查询，检索相关段落并合成上下文感知证据，最终聚合答案。

Result: 在五个问答数据集上，TreeRare 显著优于当前最佳方法。

Conclusion: TreeRare 通过语法树引导的检索与推理，有效解决了复杂问题的挑战。

Abstract: In real practice, questions are typically complex and knowledge-intensive,
requiring Large Language Models (LLMs) to recognize the multifaceted nature of
the question and reason across multiple information sources. Iterative and
adaptive retrieval, where LLMs decide when and what to retrieve based on their
reasoning, has been shown to be a promising approach to resolve complex,
knowledge-intensive questions. However, the performance of such retrieval
frameworks is limited by the accumulation of reasoning errors and misaligned
retrieval results. To overcome these limitations, we propose TreeRare (Syntax
Tree-Guided Retrieval and Reasoning), a framework that utilizes syntax trees to
guide information retrieval and reasoning for question answering. Following the
principle of compositionality, TreeRare traverses the syntax tree in a
bottom-up fashion, and in each node, it generates subcomponent-based queries
and retrieves relevant passages to resolve localized uncertainty. A
subcomponent question answering module then synthesizes these passages into
concise, context-aware evidence. Finally, TreeRare aggregates the evidence
across the tree to form a final answer. Experiments across five question
answering datasets involving ambiguous or multi-hop reasoning demonstrate that
TreeRare achieves substantial improvements over existing state-of-the-art
methods.

</details>


### [38] [Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus](https://arxiv.org/abs/2506.00332)
*Svetlana Churina,Akshat Gupta,Insyirah Mujtahid,Kokil Jaidka*

Key words: 语码混合, 语料库, 计算语言学, 社会语言学, NLP

TL;DR: 研究提出了首个标记化、通用目的的语码混合语料库，填补了公开可用数据的空白，支持计算语言学、社会语言学和自然语言处理研究。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 语码混合在社交媒体等非正式交流中普遍存在，但缺乏公开的、标记化的语料库，难以支持相关研究。

Method: 通过持续收集、验证和整合语码混合消息，构建结构化数据集（JSON格式），并附带详细元数据和语言统计信息。

Result: 目前包含超过355,641条消息，涵盖多种语码混合模式，主要关注英语、普通话和其他语言。

Conclusion: Codemix语料库将成为计算语言学、社会语言学和NLP应用的基础数据集。

Abstract: Code-mixing involves the seamless integration of linguistic elements from
multiple languages within a single discourse, reflecting natural multilingual
communication patterns. Despite its prominence in informal interactions such as
social media, chat messages and instant-messaging exchanges, there has been a
lack of publicly available corpora that are author-labeled and suitable for
modeling human conversations and relationships. This study introduces the first
labeled and general-purpose corpus for understanding code-mixing in context
while maintaining rigorous privacy and ethical standards. Our live project will
continuously gather, verify, and integrate code-mixed messages into a
structured dataset released in JSON format, accompanied by detailed metadata
and linguistic statistics. To date, it includes over 355,641 messages spanning
various code-mixing patterns, with a primary focus on English, Mandarin, and
other languages. We expect the Codemix Corpus to serve as a foundational
dataset for research in computational linguistics, sociolinguistics, and NLP
applications.

</details>


### [39] [Beyond Context to Cognitive Appraisal: Emotion Reasoning as a Theory of Mind Benchmark for Large Language Models](https://arxiv.org/abs/2506.00334)
*Gerard Christopher Yeo,Kokil Jaidka*

Key words: 情感识别, 大型语言模型, 心智理论, 认知评价理论, 情感推理

TL;DR: 论文研究了大型语言模型（LLMs）如何在心智理论（ToM）框架下通过上下文信息推断情绪状态，超越了表面的感知特征。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有情感识别任务的数据集通常依赖显性线索，但文本中可能存在需要高阶推理能力才能识别的隐性情感语义。

Method: 研究基于认知评价理论，构建了一个专门的ToM评估数据集，用于评估从上下文到情绪的前向推理和从情绪到上下文的后向推理。

Result: 研究发现LLMs在一定程度上能进行推理，但在关联情境结果和评价与特定情绪方面表现较差。

Conclusion: 研究强调了心理学理论在LLMs情感推理训练和评估中的必要性。

Abstract: Datasets used for emotion recognition tasks typically contain overt cues that
can be used in predicting the emotions expressed in a text. However, one
challenge is that texts sometimes contain covert contextual cues that are rich
in affective semantics, which warrant higher-order reasoning abilities to infer
emotional states, not simply the emotions conveyed. This study advances beyond
surface-level perceptual features to investigate how large language models
(LLMs) reason about others' emotional states using contextual information,
within a Theory-of-Mind (ToM) framework. Grounded in Cognitive Appraisal
Theory, we curate a specialized ToM evaluation dataset1 to assess both forward
reasoning - from context to emotion- and backward reasoning - from emotion to
inferred context. We showed that LLMs can reason to a certain extent, although
they are poor at associating situational outcomes and appraisals with specific
emotions. Our work highlights the need for psychological theories in the
training and evaluation of LLMs in the context of emotion reasoning.

</details>


### [40] [OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning](https://arxiv.org/abs/2506.00338)
*Yifan Peng,Shakeel Muhammad,Yui Sudo,William Chen,Jinchuan Tian,Chyi-Jiunn Lin,Shinji Watanabe*

Key words: 语音基础模型、多语言、数据清洗、开源、YODAS

TL;DR: OWSM v4通过集成大规模开放的YODAS数据集并开发数据清洗流程，显著提升了多语言语音识别性能，甚至超过了一些工业前沿模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: OWSM项目现有的训练数据不足，需要扩展数据源以提升模型性能，尤其是多语言能力。

Method: 整合YODAS数据集并开发可扩展的数据清洗流程，解决标签错误和音频-文本不对齐问题，最终生成166,000小时的清洗后数据。

Result: OWSM v4在多语言基准测试中表现显著优于先前版本，并在某些场景下超越Whisper和MMS等工业前沿模型。

Conclusion: 通过开放数据和工具，OWSM v4展示了学术界资源下语音模型的潜力，为社区提供了高质量的基础模型和数据集。

Abstract: The Open Whisper-style Speech Models (OWSM) project has developed a series of
fully open speech foundation models using academic-scale resources, but their
training data remains insufficient. This work enhances OWSM by integrating
YODAS, a large-scale web-crawled dataset with a Creative Commons license.
However, incorporating YODAS is nontrivial due to its wild nature, which
introduces challenges such as incorrect language labels and audio-text
misalignments. To address this, we develop a scalable data-cleaning pipeline
using public toolkits, yielding a dataset with 166,000 hours of speech across
75 languages. Our new series of OWSM v4 models, trained on this curated dataset
alongside existing OWSM data, significantly outperform previous versions on
multilingual benchmarks. Our models even match or surpass frontier industrial
models like Whisper and MMS in multiple scenarios. We will publicly release the
cleaned YODAS data, pre-trained models, and all associated scripts via the
ESPnet toolkit.

</details>


### [41] [Efficient Latent Semantic Clustering for Scaling Test-Time Computation of LLMs](https://arxiv.org/abs/2506.00344)
*Sungjae Lee,Hoyoung Kim,Jeongyeon Hwang,Eunhyeok Park,Jungseul Ok*

Key words: 大型语言模型,语义聚类,测试时计算,隐藏状态,效率

TL;DR: LSC是一种利用LLM内部隐藏状态进行语义聚类的轻量级方法，显著提高了测试时扩展的计算效率，并保持或优于现有方法性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 提高大型语言模型（LLM）的可靠性和质量，尤其是在不确定性量化和多步推理方面，语义聚类是一个关键但现有方法依赖外部模型，带来高计算开销和语义不准确的问题。

Method: 提出Latent Semantic Clustering (LSC)，利用生成LLM的内部隐藏状态进行聚类，避免外部模型的依赖。

Result: 在各种LLM和数据集上的实验表明，LSC显著提高了计算效率，且性能保持或优于现有方法。

Conclusion: LSC是一种高效且上下文敏感的语义聚类方法，可替代依赖外部模型的方法。

Abstract: Scaling test-time computation--generating and analyzing multiple or
sequential outputs for a single input--has become a promising strategy for
improving the reliability and quality of large language models (LLMs), as
evidenced by advances in uncertainty quantification and multi-step reasoning. A
key shared component is semantic clustering, which groups outputs that differ
in form but convey the same meaning. Semantic clustering enables estimation of
the distribution over the semantics of outputs and helps avoid redundant
exploration of reasoning paths. However, existing approaches typically rely on
external models, which introduce substantial computational overhead and often
fail to capture context-aware semantics. We propose Latent Semantic Clustering
(LSC), a lightweight and context-sensitive method that leverages the generator
LLM's internal hidden states for clustering, eliminating the need for external
models. Our extensive experiment across various LLMs and datasets shows that
LSC significantly improves the computational efficiency of test-time scaling
while maintaining or exceeding the performance of existing methods.

</details>


### [42] [Neuro2Semantic: A Transfer Learning Framework for Semantic Reconstruction of Continuous Language from Human Intracranial EEG](https://arxiv.org/abs/2506.00381)
*Siavash Shams,Richard Antonello,Gavin Mischler,Stephan Bickel,Ashesh Mehta,Nima Mesgarani*

Key words: 神经信号解码, 语义重建, LSTM, iEEG, 脑机接口

TL;DR: Neuro2Semantic 是一个能从颅内脑电图(iEEG)重建语义内容的新框架，通过LSTM适配器和校正模块实现连续自然文本生成，性能优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决神经信号解码语言的难题，推动脑机接口和神经解码技术的发展。

Method: 采用两阶段框架：LSTM适配器对齐神经信号与文本嵌入，校正模块生成连续文本。

Result: 在低数据量(30分钟)下表现优异，超越现有先进方法。

Conclusion: Neuro2Semantic 在神经解码领域具有实际应用潜力。

Abstract: Decoding continuous language from neural signals remains a significant
challenge in the intersection of neuroscience and artificial intelligence. We
introduce Neuro2Semantic, a novel framework that reconstructs the semantic
content of perceived speech from intracranial EEG (iEEG) recordings. Our
approach consists of two phases: first, an LSTM-based adapter aligns neural
signals with pre-trained text embeddings; second, a corrector module generates
continuous, natural text directly from these aligned embeddings. This flexible
method overcomes the limitations of previous decoding approaches and enables
unconstrained text generation. Neuro2Semantic achieves strong performance with
as little as 30 minutes of neural data, outperforming a recent state-of-the-art
method in low-data settings. These results highlight the potential for
practical applications in brain-computer interfaces and neural decoding
technologies.

</details>


### [43] [Adaptive-VP: A Framework for LLM-Based Virtual Patients that Adapts to Trainees' Dialogue to Facilitate Nurse Communication Training](https://arxiv.org/abs/2506.00386)
*Keyeun Lee,Seolhee Lee,Esther Hehsun Kim,Yena Ko,Jinsu Eun,Dahee Kim,Hyewon Cho,Haiyi Zhu,Robert E. Kraut,Eunyoung Suh,Eun-mee Kim,Hajin Lim*

Key words: 沟通培训、虚拟患者、大型语言模型、护理教育、自适应互动

TL;DR: 论文介绍了Adaptive-VP框架，利用大型语言模型动态调整虚拟患者行为以适应学员沟通技能，为护理沟通培训提供可扩展且有效的解决方案。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 标准化患者模拟成本高且缺乏灵活性，而现有虚拟患者系统无法动态适应学员的沟通技能，限制了其效果。

Method: 采用大型语言模型构建Adaptive-VP框架，实时评估学员沟通技能并动态调整虚拟患者行为。

Result: 自动化评估与专家验证显示，Adaptive-VP生成更自然、真实的互动，且能反映真实沟通水平。

Conclusion: Adaptive-VP是一种可扩展且有效的护理沟通培训工具。

Abstract: Effective communication training is essential to preparing nurses for
high-quality patient care. While standardized patient (SP) simulations provide
valuable experiential learning, they are often costly and inflexible. Virtual
patient (VP) systems offer a scalable alternative, but most fail to adapt to
the varying communication skills of trainees. In particular, when trainees
respond ineffectively, VPs should escalate in hostility or become
uncooperative--yet this level of adaptive interaction remains largely
unsupported. To address this gap, we introduce Adaptive-VP, a VP dialogue
generation framework that leverages large language models (LLMs) to dynamically
adapt VP behavior based on trainee input. The framework features a pipeline for
constructing clinically grounded yet flexible VP scenarios and a modular system
for assessing trainee communication and adjusting VP responses in real time,
while ensuring learner safety. We validated Adaptive-VP by simulating
challenging patient conversations. Automated evaluation using a corpus from
practicing nurses showed that our communication skill evaluation mechanism
reflected real-world proficiency levels. Expert nurses further confirmed that
Adaptive-VP produced more natural and realistic interactions than existing
approaches, demonstrating its potential as a scalable and effective tool for
nursing communication training.

</details>


### [44] [SHARE: An SLM-based Hierarchical Action CorREction Assistant for Text-to-SQL](https://arxiv.org/abs/2506.00391)
*Ge Qu,Jinyang Li,Bowen Qin,Xiaolong Li,Nan Huo,Chenhao Ma,Reynold Cheng*

Key words: 文本到SQL, 自校正, 小型语言模型, 分层动作校正, 数据隐私

TL;DR: SHARE是一种基于小型语言模型的分层动作校正助手，通过将声明式SQL查询转换为逐步动作轨迹，实现更精确的错误定位和高效校正，解决了传统自校正方法的计算开销和错误检测问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统的文本到SQL自校正方法存在计算开销大和错误检测能力不足的问题，SHARE旨在通过分层动作校正策略提升自校正的效率和精确性。

Method: SHARE通过三个小型语言模型的顺序管道，将SQL查询转换为逐步动作轨迹，并采用两阶段细粒度修正和分层自进化策略进行数据高效训练。

Result: 实验表明，SHARE显著提升了自校正能力，并在低资源训练设置下保持强健性能，适用于有数据隐私限制的文本到SQL应用。

Conclusion: SHARE通过分层动作校正和自进化策略，有效解决了传统方法的局限性，为文本到SQL任务提供了高效且鲁棒的自校正方案。

Abstract: Current self-correction approaches in text-to-SQL face two critical
limitations: 1) Conventional self-correction methods rely on recursive
self-calls of LLMs, resulting in multiplicative computational overhead, and 2)
LLMs struggle to implement effective error detection and correction for
declarative SQL queries, as they fail to demonstrate the underlying reasoning
path. In this work, we propose SHARE, an SLM-based Hierarchical Action
corREction assistant that enables LLMs to perform more precise error
localization and efficient correction. SHARE orchestrates three specialized
Small Language Models (SLMs) in a sequential pipeline, where it first
transforms declarative SQL queries into stepwise action trajectories that
reveal underlying reasoning, followed by a two-phase granular refinement. We
further propose a novel hierarchical self-evolution strategy for data-efficient
training. Experimental results demonstrate that SHARE effectively enhances
self-correction capabilities while proving robust across various LLMs.
Furthermore, our comprehensive analysis shows that SHARE maintains strong
performance even in low-resource training settings, which is particularly
valuable for text-to-SQL applications with data privacy constraints.

</details>


### [45] [Speculative Reward Model Boosts Decision Making Ability of LLMs Cost-Effectively](https://arxiv.org/abs/2506.00396)
*Jiawei Gu,Shangsong Liang*

Key words: 大型语言模型,决策效率,成本效益,推测奖励模型,搜索策略

TL;DR: 本文提出了一种名为“推测奖励模型（SRM）”的框架，用于在大型语言模型（LLMs）的决策过程中平衡性能与计算成本，并通过实验验证了其高效性和有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法在追求性能提升时忽视了效率与成本的平衡，因此需要一种既能提高决策效果又能控制计算成本的解决方案。

Method: 提出了SRM框架，通过外部奖励分配器和推测验证机制优化搜索策略，减少对LLMs内部自评估的依赖，并剪枝次优选择。

Result: 实验表明，SRM在多个复杂任务中（如数学推理、规划和专业领域数值推理）将成本降至原框架的1/10，同时保持性能。

Conclusion: SRM是一种即插即用的高效解决方案，适用于LLMs的复杂决策任务。

Abstract: Effective decision-making in Large Language Models (LLMs) is essential for
handling intricate tasks. However, existing approaches prioritize performance
but often overlook the balance between effectiveness and computational cost. To
address this, we first introduce the 3E Criteria to systematically assess the
cost-effectiveness of search strategies, revealing that existing methods often
trade significant efficiency for marginal performance gains. To improve LLM
decision-making while maintaining efficiency, we propose the Speculative Reward
Model (SRM), a plug-and-play framework that seamlessly integrates with existing
search strategies. Specifically, SRM employs an external reward assigner to
predict optimal actions, reducing reliance on LLMs' internal self-evaluation.
And a speculative verification mechanism is used to prune suboptimal choices
and guide the search toward more promising steps. We evaluate SRM on several
complex decision-making tasks including mathematical reasoning, planning and
numerical reasoning in specialized domains. Experimental results show that SRM
reduces costs to 1/10 of the original search framework on average while
maintaining effectiveness.

</details>


### [46] [Scaling Textual Gradients via Sampling-Based Momentum](https://arxiv.org/abs/2506.00400)
*Zixin Ding,Junyuan Hong,Jiachen T. Wang,Zinan Lin,Zhangyang Wang,Yuxin Chen*

Key words: 大型语言模型,文本提示优化,TGD,TSGD-M,NLP任务

TL;DR: 研究表明，TGD框架在扩展训练数据规模时性能先提升后下降，提出TSGD-M方法以优化提示采样并降低方差。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 优化大型语言模型的文本提示是重要挑战，TGD框架虽有效但数据扩展会降低性能。

Method: 提出TSGD-M方法，通过重加权提示采样模拟数值梯度下降的动量机制。

Result: 在九项NLP任务中，TSGD-M显著优于TGD基线并降低方差。

Conclusion: TSGD-M在提升性能的同时降低了计算成本和结果的不稳定性。

Abstract: As prompts play an increasingly critical role in large language models
(LLMs), optimizing textual prompts has become a crucial challenge. The Textual
Gradient Descent (TGD) framework has emerged as a promising data-driven
approach that iteratively refines textual prompts using LLM - suggested updates
(or textual gradients) over minibatches of training samples. In this paper, we
empirically demonstrate that scaling the number of training examples initially
improves but later degrades TGD's performance across multiple downstream NLP
tasks. However, while data scaling improves results for most tasks, it also
significantly increases the computational cost when leveraging LLMs. To address
this, we draw inspiration from numerical gradient descent and propose Textual
Stochastic Gradient Descent with Momentum (TSGD-M) - a method that facilitates
scalable in-context learning by reweighting prompt sampling based on past batch
distributions. Across nine NLP tasks spanning three domains - including
BIG-Bench Hard (BBH), natural language understanding tasks, and reasoning tasks
- TSGD-M significantly outperforms TGD baselines that do not incorporate
reweighted sampling, while also reducing variance in most tasks.

</details>


### [47] [Causal Structure Discovery for Error Diagnostics of Children's ASR](https://arxiv.org/abs/2506.00402)
*Vishwanath Pratap Singh,Md. Sahidullah,Tomi Kinnunen*

Key words: 儿童语音识别,因果分析,ASR错误,微调模型

TL;DR: 本文提出了一种因果结构发现方法，用于分析儿童语音识别表现差的生理、认知和外在因素之间的相互依赖关系，并通过因果量化评估各因素的影响。实验验证了结果在不同ASR系统中的普适性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 儿童语音识别表现较差的原因复杂且相互依赖，现有方法未能充分分析这些因素的交互作用。

Method: 引入因果结构发现方法，量化生理、认知和外在因素对ASR错误的影响，并分析微调模型的改善效果。

Result: 实验表明，因果分析方法能有效揭示各因素的交互作用，且结果适用于不同ASR系统。

Conclusion: 因果分析为改善儿童语音识别系统提供了新思路，未来可进一步优化因素间的交互建模。

Abstract: Children's automatic speech recognition (ASR) often underperforms compared to
that of adults due to a confluence of interdependent factors: physiological
(e.g., smaller vocal tracts), cognitive (e.g., underdeveloped pronunciation),
and extrinsic (e.g., vocabulary limitations, background noise). Existing
analysis methods examine the impact of these factors in isolation, neglecting
interdependencies-such as age affecting ASR accuracy both directly and
indirectly via pronunciation skills. In this paper, we introduce a causal
structure discovery to unravel these interdependent relationships among
physiology, cognition, extrinsic factors, and ASR errors. Then, we employ
causal quantification to measure each factor's impact on children's ASR. We
extend the analysis to fine-tuned models to identify which factors are
mitigated by fine-tuning and which remain largely unaffected. Experiments on
Whisper and Wav2Vec2.0 demonstrate the generalizability of our findings across
different ASR systems.

</details>


### [48] [Accelerating Diffusion LLMs via Adaptive Parallel Decoding](https://arxiv.org/abs/2506.00413)
*Daniel Israel,Guy Van den Broeck,Aditya Grover*

Key words: 自适应并行解码（APD），扩散大语言模型（dLLMs），自回归模型，KV缓存，吞吐量

TL;DR: 本文提出了一种名为自适应并行解码（APD）的新方法，用于解决扩散大语言模型（dLLMs）在并行生成令牌时速度与质量的平衡问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有LLM的生成速度受限于自回归解码的逐令牌预测，而dLLM理论上支持并行令牌生成，但在实践中难以在不显著牺牲质量的情况下达到自回归模型的速度。

Method: 通过定义一个dLLM边际概率与小型辅助自回归模型序列联合概率的乘法混合，动态调整并行采样的令牌数量。优化包括启用KV缓存和限制掩码输入大小。

Result: APD在基准测试中显示出显著更高的吞吐量且质量损失极小。

Conclusion: APD通过三个可调参数灵活权衡吞吐量和质量，是一种高效且灵活的并行解码方法。

Abstract: The generation speed of LLMs are bottlenecked by autoregressive decoding,
where tokens are predicted sequentially one by one. Alternatively, diffusion
large language models (dLLMs) theoretically allow for parallel token
generation, but in practice struggle to achieve the speed of autoregressive
models without significantly sacrificing quality. We therefore introduce
adaptive parallel decoding (APD), a novel method that dynamically adjusts the
number of tokens sampled in parallel. We achieve this by defining a
multiplicative mixture between the dLLM marginal probabilities and the joint
probability of sequences under a small auxiliary autoregressive model. This
inverts the standard setup of speculative decoding, where the goal is to sample
from a large autoregressive verifier by drafting from a smaller model. We
further optimize APD by enabling KV caching and limiting the size of the masked
input. Altogether, our method puts forward three tunable parameters to flexibly
tradeoff throughput and quality. We show that APD provides markedly higher
throughput with minimal quality degradations on downstream benchmarks.

</details>


### [49] [Dual Debiasing for Noisy In-Context Learning for Text Generation](https://arxiv.org/abs/2506.00418)
*Siqi Liang,Sumyeong Ahn,Paramveer S. Dhillon,Jiayu Zhou*

Key words: In-Context Learning, noise detection, perplexity bias, dual debiasing, sample cleanliness

TL;DR: 本文提出了一种双去偏框架，通过合成邻居来修正困惑度估计，从而生成鲁棒的样本清洁度评分，有效解决了高噪声条件下ICL的性能问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法依赖于局部困惑度排名来检测噪声标注，但这一假设在高噪声比例下失效。本文旨在解决困惑度的两个偏差源，并提出更稳健的噪声检测方法。

Method: 引入双去偏框架，利用合成邻居显式修正困惑度估计，生成样本清洁度评分（SCS）。

Result: 实验证明该方法在噪声检测能力上优于现有方法，且在高噪声比例下仍保持稳健，最终ICL性能接近完全清洁的演示语料库。

Conclusion: 提出的双去偏框架能有效克服困惑度的偏差问题，显著提升ICL在高噪声环境下的性能。

Abstract: In context learning (ICL) relies heavily on high quality demonstrations drawn
from large annotated corpora. Existing approaches detect noisy annotations by
ranking local perplexities, presuming that noisy samples yield higher
perplexities than their clean counterparts. However, this assumption breaks
down when the noise ratio is high and many demonstrations are flawed. We
reexamine the perplexity based paradigm for text generation under noisy
annotations, highlighting two sources of bias in perplexity: the annotation
itself and the domain specific knowledge inherent in large language models
(LLMs). To overcome these biases, we introduce a dual debiasing framework that
uses synthesized neighbors to explicitly correct perplexity estimates, yielding
a robust Sample Cleanliness Score. This metric uncovers absolute sample
cleanliness regardless of the overall corpus noise level. Extensive experiments
demonstrate our method's superior noise detection capabilities and show that
its final ICL performance is comparable to that of a fully clean demonstration
corpus. Moreover, our approach remains robust even when noise ratios are
extremely high.

</details>


### [50] [Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions](https://arxiv.org/abs/2506.00421)
*Jihyoung Jang,Minwook Bae,Minji Kim,Dilek Hakkani-Tur,Hyounghun Kim*

Key words: 多模态聊天机器人, 视觉与听觉, $M^3C$数据集, 动态对话, 沉浸式交互

TL;DR: 研究提出了一种新型多模态对话模型，通过结合视觉和听觉输入，提升聊天机器人的沉浸式交互能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有聊天机器人在多模态交互中偏重视觉任务，忽略了听觉方面，且交互多为静态而非动态融入对话，限制了多模态的丰富性。

Method: 引入新的多模态对话数据集$M^3C$，并提出一种基于多模态记忆检索的对话模型。

Result: 模型在复杂场景中能够无缝处理视觉和听觉输入，维持连贯的动态对话，并通过人类评估验证了其性能。

Conclusion: 该模型为多模态对话代理提供了潜在的技术支持，推动了更自然的交互发展。

Abstract: As chatbots continue to evolve toward human-like, real-world, interactions,
multimodality remains an active area of research and exploration. So far,
efforts to integrate multimodality into chatbots have primarily focused on
image-centric tasks, such as visual dialogue and image-based instructions,
placing emphasis on the "eyes" of human perception while neglecting the "ears",
namely auditory aspects. Moreover, these studies often center around static
interactions that focus on discussing the modality rather than naturally
incorporating it into the conversation, which limits the richness of
simultaneous, dynamic engagement. Furthermore, while multimodality has been
explored in multi-party and multi-session conversations, task-specific
constraints have hindered its seamless integration into dynamic, natural
conversations. To address these challenges, this study aims to equip chatbots
with "eyes and ears" capable of more immersive interactions with humans. As
part of this effort, we introduce a new multimodal conversation dataset,
Multimodal Multi-Session Multi-Party Conversation ($M^3C$), and propose a novel
multimodal conversation model featuring multimodal memory retrieval. Our model,
trained on the $M^3C$, demonstrates the ability to seamlessly engage in
long-term conversations with multiple speakers in complex, real-world-like
settings, effectively processing visual and auditory inputs to understand and
respond appropriately. Human evaluations highlight the model's strong
performance in maintaining coherent and dynamic interactions, demonstrating its
potential for advanced multimodal conversational agents.

</details>


### [51] [DYNAC: Dynamic Vocabulary based Non-Autoregressive Contextualization for Speech Recognition](https://arxiv.org/abs/2506.00422)
*Yui Sudo,Yosuke Fukumoto,Muhammad Shakeel,Yifan Peng,Chyi-Jiunn Lin,Shinji Watanabe*

Key words: DYNAC, 动态词汇, 非自回归模型, CTC, 上下文偏差

TL;DR: DYNAC是一种自调节CTC方法，通过动态词汇集成到中间层，提升非自回归模型中的上下文偏差性能，显著降低推理时间。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决自回归模型中动态词汇导致的推理速度慢问题，并克服非自回归模型中静态和动态词汇间依赖关系难以捕捉的缺陷。

Method: 提出DYNAC方法，将动态词汇集成到自调节CTC模型的中间层，通过调节编码器捕捉依赖关系。

Result: 在LibriSpeech测试集上，DYNAC降低81%的实时因子，仅使词错率增加0.1%。

Conclusion: DYNAC在保持高精度的同时显著提升推理效率，适用于实际部署。

Abstract: Contextual biasing (CB) improves automatic speech recognition for rare and
unseen phrases. Recent studies have introduced dynamic vocabulary, which
represents context phrases as expandable tokens in autoregressive (AR) models.
This method improves CB accuracy but with slow inference speed. While dynamic
vocabulary can be applied to non-autoregressive (NAR) models, such as
connectionist temporal classification (CTC), the conditional independence
assumption fails to capture dependencies between static and dynamic tokens.
This paper proposes DYNAC (Dynamic Vocabulary-based NAR Contextualization), a
self-conditioned CTC method that integrates dynamic vocabulary into
intermediate layers. Conditioning the encoder on dynamic vocabulary, DYNAC
effectively captures dependencies between static and dynamic tokens while
reducing the real-time factor (RTF). Experimental results show that DYNAC
reduces RTF by 81% with a 0.1-point degradation in word error rate on the
LibriSpeech 960 test-clean set.

</details>


### [52] [Inter-Passage Verification for Multi-evidence Multi-answer QA](https://arxiv.org/abs/2506.00425)
*Bingsen Chen,Shengjie Wang,Xi Ye,Chen Zhao*

Key words: 多答案问答、检索增强生成、段落验证、RI$^2$VER

TL;DR: 论文提出了一种名为RI$^2$VER的多答案问答框架，通过检索和验证流程显著提升多答案问题的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有检索增强生成式QA系统在多答案问题上表现不佳，无法有效处理大量证据段落。

Method: 采用检索增强独立阅读和段落间验证（RI$^2$VER），包括生成候选答案集、验证问题生成、补充证据收集及段落间合成验证。

Result: 在QAMPARI和RoMQA数据集上，平均F1分数提升11.17%，尤其在需要多重证据合成的问题上表现突出。

Conclusion: RI$^2$VER框架通过验证流程有效解决了多答案QA中的检索和合成难题。

Abstract: Multi-answer question answering (QA), where questions can have many valid
answers, presents a significant challenge for existing retrieval-augmented
generation-based QA systems, as these systems struggle to retrieve and then
synthesize a large number of evidence passages. To tackle these challenges, we
propose a new multi-answer QA framework -- Retrieval-augmented Independent
Reading with Inter-passage Verification (RI$^2$VER). Our framework retrieves a
large set of passages and processes each passage individually to generate an
initial high-recall but noisy answer set. Then we propose a new inter-passage
verification pipeline that validates every candidate answer through (1)
Verification Question Generation, (2) Gathering Additional Evidence, and (3)
Verification with inter-passage synthesis. Evaluations on the QAMPARI and RoMQA
datasets demonstrate that our framework significantly outperforms existing
baselines across various model sizes, achieving an average F1 score improvement
of 11.17%. Further analysis validates that our inter-passage verification
pipeline enables our framework to be particularly beneficial for questions
requiring multi-evidence synthesis.

</details>


### [53] [G2S: A General-to-Specific Learning Framework for Temporal Knowledge Graph Forecasting with Large Language Models](https://arxiv.org/abs/2506.00445)
*Long Bai,Zixuan Li,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng,Tat-Seng Chua*

Key words: 时间知识图谱（TKG）、大型语言模型（LLMs）、泛化能力、G2S框架

TL;DR: 论文提出了一种称为G2S的学习框架，用于在时间知识图谱（TKG）预测任务中分离通用模式和特定场景信息的学习，以提高大型语言模型（LLMs）的泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前基于大型语言模型的时间知识图谱预测任务中，通用模式和特定场景信息的学习过程相互干扰，影响了模型的泛化能力。因此，作者希望通过分离这两种知识的学习来提升模型性能。

Method: 提出了一种从通用到特定（G2S）的学习框架：在通用学习阶段，通过掩码场景信息提取通用模式；在特定学习阶段，通过上下文学习或微调注入场景信息。

Result: 实验结果表明，G2S框架显著提高了大型语言模型在时间知识图谱预测任务中的泛化能力。

Conclusion: G2S框架通过分离通用模式和特定场景信息的学习，有效提升了模型性能，为时间知识图谱预测任务的优化提供了新思路。

Abstract: Forecasting over Temporal Knowledge Graphs (TKGs) which predicts future facts
based on historical ones has received much attention. Recent studies have
introduced Large Language Models (LLMs) for this task to enhance the models'
generalization abilities. However, these models perform forecasting via
simultaneously learning two kinds of entangled knowledge in the TKG: (1)
general patterns, i.e., invariant temporal structures shared across different
scenarios; and (2) scenario information, i.e., factual knowledge engaged in
specific scenario, such as entities and relations. As a result, the learning
processes of these two kinds of knowledge may interfere with each other, which
potentially impact the generalization abilities of the models. To enhance the
generalization ability of LLMs on this task, in this paper, we propose a
General-to-Specific learning framework (G2S) that disentangles the learning
processes of the above two kinds of knowledge. In the general learning stage,
we mask the scenario information in different TKGs and convert it into
anonymous temporal structures. After training on these structures, the model is
able to capture the general patterns across different TKGs. In the specific
learning stage, we inject the scenario information into the structures via
either in-context learning or fine-tuning modes. Experimental results show that
G2S effectively improves the generalization abilities of LLMs.

</details>


### [54] [Fact-Controlled Diagnosis of Hallucinations in Medical Text Summarization](https://arxiv.org/abs/2506.00448)
*Suhas BN,Han-Chin Shing,Lei Xu,Mitch Strong,Jon Burnsky,Jessica Ofor,Jordan R. Mason,Susan Chen,Sundararajan Srinivasan,Chaitanya Shivade,Jack Moriarty,Joseph Paul Cohen*

Key words: 大型语言模型,临床摘要,虚假信息检测,医疗领域,数据集

TL;DR: 论文研究了大型语言模型(LLMs)在临床对话摘要中的虚假信息问题，提出了两种评估数据集，并开发了基于事实的虚假信息检测方法，显著提升了检测效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: LLMs在临床对话摘要中产生的虚假信息对患者护理和临床决策构成风险，但目前研究不足，且现有检测方法在临床领域效果有限。

Method: 构建了两种数据集（事实控制Leave-N-out数据集和自然虚假数据集），并开发了基于事实的虚假信息检测方法。

Result: 研究发现通用领域检测器在临床领域表现不佳，而开发的新型检测方法能够有效检测真实世界的临床虚假信息。

Conclusion: 本研究为临床摘要系统提供了专业化的评估指标和数据集，推动了摘要系统的可信度提升。

Abstract: Hallucinations in large language models (LLMs) during summarization of
patient-clinician dialogues pose significant risks to patient care and clinical
decision-making. However, the phenomenon remains understudied in the clinical
domain, with uncertainty surrounding the applicability of general-domain
hallucination detectors. The rarity and randomness of hallucinations further
complicate their investigation. In this paper, we conduct an evaluation of
hallucination detection methods in the medical domain, and construct two
datasets for the purpose: A fact-controlled Leave-N-out dataset -- generated by
systematically removing facts from source dialogues to induce hallucinated
content in summaries; and a natural hallucination dataset -- arising
organically during LLM-based medical summarization. We show that general-domain
detectors struggle to detect clinical hallucinations, and that performance on
fact-controlled hallucinations does not reliably predict effectiveness on
natural hallucinations. We then develop fact-based approaches that count
hallucinations, offering explainability not available with existing methods.
Notably, our LLM-based detectors, which we developed using fact-controlled
hallucinations, generalize well to detecting real-world clinical
hallucinations. This research contributes a suite of specialized metrics
supported by expert-annotated datasets to advance faithful clinical
summarization systems.

</details>


### [55] [Massively Multilingual Adaptation of Large Language Models Using Bilingual Translation Data](https://arxiv.org/abs/2506.00469)
*Shaoxiong Ji,Zihao Li,Jaakko Paavola,Indraneil Paul,Hengyu Luo,Jörg Tiedemann*

Key words: 大规模多语言预训练, Llama 3, 双语数据, 低资源语言

TL;DR: 研究了在跨语言大规模预训练中加入并行双语数据的效果，验证了双语数据对低资源语言性能的提升作用。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索在大规模多语言持续预训练中，双语翻译数据对模型性能的影响，尤其是在低资源语言上的表现。

Method: 构建了包含2,500多种语言对的MaLA双语翻译语料库，并基于Llama 3模型家族开发了EMMA-500 Llama 3模型套件。

Result: 双语数据显著提升了语言迁移和模型性能，尤其是对低资源语言。

Conclusion: 双语数据在多语言预训练中具有积极作用，特别是在低资源语言上实现性能提升。

Abstract: This paper investigates a critical design decision in the practice of
massively multilingual continual pre-training -- the inclusion of parallel
data. Specifically, we study the impact of bilingual translation data for
massively multilingual language adaptation of the Llama3 family of models to
500 languages. To this end, we construct the MaLA bilingual translation corpus,
containing data from more than 2,500 language pairs. Subsequently, we develop
the EMMA-500 Llama 3 suite of four massively multilingual models -- continually
pre-trained from the Llama 3 family of base models extensively on diverse data
mixes up to 671B tokens -- and explore the effect of continual pre-training
with or without bilingual translation data. Comprehensive evaluation across 7
tasks and 12 benchmarks demonstrates that bilingual data tends to enhance
language transfer and performance, particularly for low-resource languages. We
open-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model
generations.

</details>


### [56] [EffiVLM-BENCH: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Vision-Language Models](https://arxiv.org/abs/2506.00479)
*Zekun Wang,Minghua Ma,Zexin Wang,Rongchuan Mu,Liping Shan,Ming Liu,Bing Qin*

Key words: 大型视觉语言模型（LVLM）、加速技术、EffiVLM-Bench、令牌压缩、参数压缩

TL;DR: 该论文系统评估了大型视觉语言模型（LVLM）的主流加速技术，引入了评估框架EffiVLM-Bench，并提供了优化策略的深入分析。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管大型视觉语言模型表现优异，但其高计算需求限制了实际应用，且现有方法缺乏全面的评估框架。

Method: 通过分类为令牌和参数压缩的主流加速技术，并引入EffiVLM-Bench框架进行系统评估。

Result: 实验和分析提供了加速LVLM的最优策略，并开源了相关代码和配方。

Conclusion: EffiVLM-Bench为LVLM的加速技术提供了全面的评估工具和优化方向。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable success, yet
their significant computational demands hinder practical deployment. While
efforts to improve LVLM efficiency are growing, existing methods lack
comprehensive evaluation across diverse backbones, benchmarks, and metrics. In
this work, we systematically evaluate mainstream acceleration techniques for
LVLMs, categorized into token and parameter compression. We introduce
EffiVLM-Bench, a unified framework for assessing not only absolute performance
but also generalization and loyalty, while exploring Pareto-optimal trade-offs.
Our extensive experiments and in-depth analyses offer insights into optimal
strategies for accelerating LVLMs. We open-source code and recipes for
EffiVLM-Bench to foster future research.

</details>


### [57] [PVP: An Image Dataset for Personalized Visual Persuasion with Persuasion Strategies, Viewer Characteristics, and Persuasiveness Ratings](https://arxiv.org/abs/2506.00481)
*Junseo Kim,Jongwook Han,Dongmin Choi,Jongwook Yoon,Eun-Ju Lee,Yohan Jo*

Key words: 个性化视觉说服、PVP数据集、心理特征、图像生成、persuasive 图像

TL;DR: 该论文介绍了个性化视觉说服（PVP）数据集，用于连接图像的 persuasive 性与评估者的个人信息，并提出一种基于心理特征的 persuasive 图像生成器和评估器。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决缺乏将图像 persuasive 性与评估者个人信息关联的数据集问题，推动个性化视觉说服技术的发展。

Method: 发布包含28,454张 persuasive 图像的PVP数据集，结合2,521名评估者的人口统计和心理特征，开发 persuasive 图像生成器和评估器。

Result: 心理特征的融入提升了 persuasive 图像的生成和评估效果。

Conclusion: PVP数据集为个性化视觉说服提供了实用工具和基准数据，心理特征是关键因素。

Abstract: Visual persuasion, which uses visual elements to influence cognition and
behaviors, is crucial in fields such as advertising and political
communication. With recent advancements in artificial intelligence, there is
growing potential to develop persuasive systems that automatically generate
persuasive images tailored to individuals. However, a significant bottleneck in
this area is the lack of comprehensive datasets that connect the persuasiveness
of images with the personal information about those who evaluated the images.
To address this gap and facilitate technological advancements in personalized
visual persuasion, we release the Personalized Visual Persuasion (PVP) dataset,
comprising 28,454 persuasive images across 596 messages and 9 persuasion
strategies. Importantly, the PVP dataset provides persuasiveness scores of
images evaluated by 2,521 human annotators, along with their demographic and
psychological characteristics (personality traits and values). We demonstrate
the utility of our dataset by developing a persuasive image generator and an
automated evaluator, and establish benchmark baselines. Our experiments reveal
that incorporating psychological characteristics enhances the generation and
evaluation of persuasive images, providing valuable insights for personalized
visual persuasion.

</details>


### [58] [Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models](https://arxiv.org/abs/2506.00483)
*Aviv Jan,Dean Tahory,Omer Talmi,Omar Abo Mokh*

Key words: 多跳推理、大型语言模型、Auto-Patch、PatchScopes、隐藏状态干预

TL;DR: Auto-Patch是一种动态修补隐藏状态的新方法，旨在提升大型语言模型在多跳推理上的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前大型语言模型在多跳问题上表现不佳，难以跨多个推理步骤链接信息。

Method: 基于PatchScopes框架，Auto-Patch通过学习的分类器选择性修改内部表示。

Result: 在MuSiQue数据集上，Auto-Patch将解决率从18.45%提升至23.63%，缩小了与Chain-of-Thought提示（27.44%）的差距。

Conclusion: 动态隐藏状态干预有望推动大型语言模型在复杂推理方面的进步。

Abstract: Multi-hop questions still stump large language models (LLMs), which struggle
to link information across multiple reasoning steps. We introduce Auto-Patch, a
novel method that dynamically patches hidden states during inference to enhance
multi-hop reasoning in LLMs. Building on the PatchScopes framework, Auto-Patch
selectively modifies internal representations using a learned classifier.
Evaluated on the MuSiQue dataset, Auto-Patch improves the solve rate from
18.45\% (baseline) to 23.63~$\pm$~0.7\% (3 runs), narrowing the gap to
Chain-of-Thought prompting (27.44\%). Our results highlight the potential of
dynamic hidden state interventions for advancing complex reasoning in LLMs.

</details>


### [59] [Synergizing LLMs with Global Label Propagation for Multimodal Fake News Detection](https://arxiv.org/abs/2506.00488)
*Shuguo Hu,Jun Hu,Huaiwen Zhang*

Key words: 大语言模型, 伪标签, 标签传播, 多模态假新闻检测

TL;DR: 本文提出了一种结合大语言模型（LLM）生成伪标签的全局标签传播网络（GLPN-LLM），用于多模态假新闻检测。该方法通过标签传播技术整合LLM能力，显著提高了检测准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管LLM生成的伪标签在假新闻检测中表现不佳，但通过标签传播技术整合LLM能力可以提升其性能。

Method: 提出GLPN-LLM模型，通过全局标签传播利用LLM生成的伪标签，避免标签泄漏的掩码机制。

Result: 实验结果表明，该方法在基准数据集上优于现有最先进模型。

Conclusion: 结合LLM和标签传播技术的GLPN-LLM显著提高了多模态假新闻检测的性能。

Abstract: Large Language Models (LLMs) can assist multimodal fake news detection by
predicting pseudo labels. However, LLM-generated pseudo labels alone
demonstrate poor performance compared to traditional detection methods, making
their effective integration non-trivial. In this paper, we propose Global Label
Propagation Network with LLM-based Pseudo Labeling (GLPN-LLM) for multimodal
fake news detection, which integrates LLM capabilities via label propagation
techniques. The global label propagation can utilize LLM-generated pseudo
labels, enhancing prediction accuracy by propagating label information among
all samples. For label propagation, a mask-based mechanism is designed to
prevent label leakage during training by ensuring that training nodes do not
propagate their own labels back to themselves. Experimental results on
benchmark datasets show that by synergizing LLMs with label propagation, our
model achieves superior performance over state-of-the-art baselines.

</details>


### [60] [Exploring In-context Example Generation for Machine Translation](https://arxiv.org/abs/2506.00507)
*Dohyun Lee,Seungil Chad Lee,Chanwoo Yang,Yujin Baek,Jaegul Choo*

Key words: 大规模语言模型,机器翻译,上下文学习,低资源语言,DAT

TL;DR: 论文提出了一种用于机器翻译的上下文示例生成方法DAT，解决了低资源语言中缺乏人工标注数据的问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法依赖人工标注的示例池，难以适用于低资源语言，因此探索不依赖外部资源的示例生成方法。

Method: 提出DAT方法，通过增强示例对的相关性和多样性来生成上下文示例，无须外部资源支持。

Result: 在低资源语言实验中，DAT的翻译质量优于基线方法，并探索了测试时逐步积累生成示例的潜力。

Conclusion: DAT是一种简单有效的方法，尤其适用于低资源语言的机器翻译任务。

Abstract: Large language models (LLMs) have demonstrated strong performance across
various tasks, leveraging their exceptional in-context learning ability with
only a few examples. Accordingly, the selection of optimal in-context examples
has been actively studied in the field of machine translation. However, these
studies presuppose the presence of a demonstration pool with human-annotated
pairs, making them less applicable to low-resource languages where such an
assumption is challenging to meet. To overcome this limitation, this paper
explores the research direction of in-context example generation for machine
translation. Specifically, we propose Demonstration Augmentation for
Translation (DAT), a simple yet effective approach that generates example pairs
without relying on any external resources. This method builds upon two prior
criteria, relevance and diversity, which have been highlighted in previous work
as key factors for in-context example selection. Through experiments and
analysis on low-resource languages where human-annotated pairs are scarce, we
show that DAT achieves superior translation quality compared to the baselines.
Furthermore, we investigate the potential of progressively accumulating
generated pairs during test time to build and reuse a demonstration pool. Our
implementation is publicly available at https://github.com/aiclaudev/DAT.

</details>


### [61] [Goal-Aware Identification and Rectification of Misinformation in Multi-Agent Systems](https://arxiv.org/abs/2506.00509)
*Zherui Li,Yan Mi,Zhenhong Zhou,Houcheng Jiang,Guibin Zhang,Kun Wang,Junfeng Fang*

Key words: 多智能体系统, 错误信息, 防御框架, ARGUS, MisinfoTask

TL;DR: 该论文提出了一种名为ARGUS的两阶段防御框架，用于解决基于大型语言模型的多智能体系统中的错误信息传播问题，并展示了其在减少错误信息毒性和提高任务成功率方面的有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型驱动的多智能体系统容易受到错误信息注入攻击，作者希望通过研究错误信息传播的动态，提高系统的鲁棒性。

Method: 作者引入了MisinfoTask数据集，并提出了一种无需训练的两阶段防御框架ARGUS，利用目标感知推理来纠正信息流中的错误信息。

Result: 实验表明，ARGUS在应对各种错误信息注入攻击时表现出显著效果，平均减少错误信息毒性约28.17%，并在受攻击时提高任务成功率约10.33%。

Conclusion: ARGUS是一种有效的防御框架，能够显著提升多智能体系统对错误信息攻击的抵抗力。

Abstract: Large Language Model-based Multi-Agent Systems (MASs) have demonstrated
strong advantages in addressing complex real-world tasks. However, due to the
introduction of additional attack surfaces, MASs are particularly vulnerable to
misinformation injection. To facilitate a deeper understanding of
misinformation propagation dynamics within these systems, we introduce
MisinfoTask, a novel dataset featuring complex, realistic tasks designed to
evaluate MAS robustness against such threats. Building upon this, we propose
ARGUS, a two-stage, training-free defense framework leveraging goal-aware
reasoning for precise misinformation rectification within information flows.
Our experiments demonstrate that in challenging misinformation scenarios, ARGUS
exhibits significant efficacy across various injection attacks, achieving an
average reduction in misinformation toxicity of approximately 28.17% and
improving task success rates under attack by approximately 10.33%. Our code and
dataset is available at: https://github.com/zhrli324/ARGUS.

</details>


### [62] [Evaluating the Evaluation of Diversity in Commonsense Generation](https://arxiv.org/abs/2506.00514)
*Tianhui Zhang,Bei Peng,Danushka Bollegala*

Key words: 常识生成、多样性评估、元评估、大型语言模型、内容多样性

TL;DR: 论文系统评估了常识生成中多样性指标的适用性，发现基于内容的指标优于基于形式的指标。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决常识生成任务中多样性评估指标选择不明确的问题。

Method: 通过系统性元评估和LLM标注新数据集，比较基于形式和内容的多样性指标。

Result: 基于内容的多样性指标表现更优，与LLM评分高度相关。

Conclusion: 建议未来常识生成研究使用基于内容的多样性评估指标。

Abstract: In commonsense generation, given a set of input concepts, a model must
generate a response that is not only commonsense bearing, but also capturing
multiple diverse viewpoints. Numerous evaluation metrics based on form- and
content-level overlap have been proposed in prior work for evaluating the
diversity of a commonsense generation model. However, it remains unclear as to
which metrics are best suited for evaluating the diversity in commonsense
generation. To address this gap, we conduct a systematic meta-evaluation of
diversity metrics for commonsense generation. We find that form-based diversity
metrics tend to consistently overestimate the diversity in sentence sets, where
even randomly generated sentences are assigned overly high diversity scores. We
then use an Large Language Model (LLM) to create a novel dataset annotated for
the diversity of sentences generated for a commonsense generation task, and use
it to conduct a meta-evaluation of the existing diversity evaluation metrics.
Our experimental results show that content-based diversity evaluation metrics
consistently outperform the form-based counterparts, showing high correlations
with the LLM-based ratings. We recommend that future work on commonsense
generation should use content-based metrics for evaluating the diversity of
their outputs.

</details>


### [63] [CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention](https://arxiv.org/abs/2506.00519)
*Yuxi Sun,Aoqi Zuo,Wei Gao,Jing Ma*

Key words: 大语言模型、多语言、知识差距、因果分析、放弃策略

TL;DR: 论文提出‘CausalAbstain’方法，通过因果分析改进LLMs在多语言场景下的知识差距问题，减少幻觉现象。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 多语言大模型在知识上存在差异，需要一种方法帮助模型在不清楚时选择‘放弃’，以减少幻觉。现有方法依赖自我反馈，但易受反馈中的错误和偏见影响。

Method: 采用因果视角，提出‘CausalAbstain’方法，指导LLMs如何利用多语言反馈并选择最有用的部分。

Result: 实验表明，该方法在百科全书和常识问答任务上优于基线，具有可解释性。

Conclusion: ‘CausalAbstain’能有效提升LLMs在多语言场景下的放弃决策能力。

Abstract: Large Language Models (LLMs) often exhibit knowledge disparities across
languages. Encouraging LLMs to \textit{abstain} when faced with knowledge gaps
is a promising strategy to reduce hallucinations in multilingual settings.
Current abstention strategies for multilingual scenarios primarily rely on
generating feedback in various languages using LLMs and performing
self-reflection. However, these methods can be adversely impacted by
inaccuracies and biases in the generated feedback. To address this, from a
causal perspective, we introduce \textit{CausalAbstain}, a method that helps
LLMs determine whether to utilize multiple generated feedback responses and how
to identify the most useful ones. Extensive experiments demonstrate that
\textit{CausalAbstain} effectively selects helpful feedback and enhances
abstention decisions with interpretability in both native language
(\textsc{Casual-native}) and multilingual (\textsc{Causal-multi}) settings,
outperforming strong baselines on two benchmark datasets covering encyclopedic
and commonsense knowledge QA tasks. Our code and data are open-sourced at
https://github.com/peachch/CausalAbstain.

</details>


### [64] [Retrieval-Augmented Generation Systems for Intellectual Property via Synthetic Multi-Angle Fine-tuning](https://arxiv.org/abs/2506.00527)
*Runtao Ren,Jian Ma,Jianxi Luo*

Key words: 检索增强生成,RAG,多角度问题生成,检索微调,专利情报

TL;DR: 论文提出了一种名为MQG-RFM的新框架，通过多角度问题生成和检索微调方法解决IP领域中RAG系统对多样化用户查询的不足，显著提升检索和生成质量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: IP领域的RAG系统在面对多样化的用户查询（如口语化表达、拼写错误和术语模糊）时表现不佳，导致检索不准确和响应不理想。

Method: 提出MQG-RFM框架，利用LLM模拟多样化用户查询，并通过语义等效问题微调检索模型，采用轻量级的Data-to-Tune范式结合提示工程和硬负例挖掘。

Result: 在台湾专利Q&A数据集上，检索准确率分别提高了185.62%和262.26%，生成质量提升了14.22%和53.58%。

Conclusion: MQG-RFM通过语义感知的检索优化，为中小型机构提供了一种实用且可扩展的专利情报解决方案。

Abstract: Retrieval-Augmented Generation (RAG) systems in the Intellectual Property
(IP) field often struggle with diverse user queries, including colloquial
expressions, spelling errors, and ambiguous terminology, leading to inaccurate
retrieval and suboptimal responses. To address this challenge, we propose
Multi-Angle Question Generation and Retrieval Fine-Tuning Method (MQG-RFM), a
novel framework that leverages large language models (LLMs) to simulate varied
user inquiries and fine-tunes retrieval models to align semantically equivalent
but linguistically diverse questions. Unlike complex architectural
modifications, MQG-RFM adopts a lightweight Data-to-Tune paradigm, combining
prompt-engineered query generation with hard negative mining to enhance
retrieval robustness without costly infrastructure changes. Experimental
results on a Taiwan patent Q&A dataset show 185.62% improvement in retrieval
accuracy on the Patent Consultation dataset and 262.26% improvement on the
Novel Patent Technology Report dataset, with 14.22% and 53.58% improvements in
generation quality over the baselines, respectively. By bridging the gap
between user intent and system comprehension through semantic-aware retrieval
optimization, MQG-RFM offers a practical, scalable approach for rapid,
cost-effective deployment among small and medium-sized agencies seeking
reliable patent intelligence solutions. Additionally, our proposed method has
already been adopted by ScholarMate, the largest professional research social
networking platform in China, to support real-world development and deployment.
A demo version of the instantiated is available at
https://github.com/renruntao/patent_rag.

</details>


### [65] [Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing](https://arxiv.org/abs/2506.00536)
*Changyue Wang,Weihang Su,Qingyao Ai,Yujia Zhou,Yiqun Liu*

Key words: 知识编辑, 大语言模型, 推理路径, 多跳QA

TL;DR: DecKER是一种新的知识编辑框架，通过解耦推理和知识编辑，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有知识编辑方法未明确区分新注入知识与原始推理过程，导致冲突和准确性下降。

Method: 提出DecKER框架，通过生成掩码推理路径并结合检索和模型验证解耦推理与知识编辑。

Result: 实验表明，DecKER在多跳QA任务中显著优于现有方法。

Conclusion: DecKER通过解耦推理与知识编辑，提升了知识编辑的一致性和准确性。

Abstract: Knowledge editing aims to efficiently update Large Language Models (LLMs) by
modifying specific knowledge without retraining the entire model. Among
knowledge editing approaches, in-context editing (ICE) offers a lightweight
solution by injecting new knowledge directly into the input context, leaving
model parameters unchanged. However, existing ICE approaches do not explicitly
separate the newly injected knowledge from the model's original reasoning
process. This entanglement often results in conflicts between external updates
and internal parametric knowledge, undermining the consistency and accuracy of
the reasoning path.In this work, we conduct preliminary experiments to examine
how parametric knowledge influences reasoning path planning. We find that the
model's reasoning is tightly coupled with its internal knowledge, and that
naively injecting new information without adapting the reasoning path often
leads to performance degradation, particularly in multi-hop tasks. To this end,
we propose DecKER, a novel ICE framework that decouples reasoning from
knowledge editing by generating a masked reasoning path and then resolving
knowledge edits via hybrid retrieval and model-based validation. Experiments on
multi-hop QA benchmarks show that DecKER significantly outperforms existing ICE
methods by mitigating knowledge conflicts and preserving reasoning consistency.
Our code is available at: https://github.com/bebr2/DecKER .

</details>


### [66] [ARIA: Training Language Agents with Intention-Driven Reward Aggregation](https://arxiv.org/abs/2506.00539)
*Ruihan Yang,Yikai Zhang,Aili Chen,Xintao Wang,Siyu Yuan,Jiangjie Chen,Deqing Yang,Yanghua Xiao*

Key words: 大语言模型, 强化学习, 奖励稀疏性, ARIA方法

TL;DR: ARIA方法通过将自然语言动作从高维空间投影到低维意图空间，减少奖励方差，提升强化学习效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决开放语言动作环境中因巨大动作空间导致的奖励稀疏性问题。

Method: 提出ARIA方法，将动作投影到低维意图空间，聚类语义相似动作并共享奖励。

Result: 显著减少策略梯度方差，下游任务平均性能提升9.95%。

Conclusion: ARIA有效提升语言代理的训练效率和性能。

Abstract: Large language models (LLMs) have enabled agents to perform complex reasoning
and decision-making through free-form language interactions. However, in
open-ended language action environments (e.g., negotiation or question-asking
games), the action space can be formulated as a joint distribution over tokens,
resulting in an exponentially large action space. Sampling actions in such a
space can lead to extreme reward sparsity, which brings large reward variance,
hindering effective reinforcement learning (RL). To address this, we propose
ARIA, a method that Aggregates Rewards in Intention space to enable efficient
and effective language Agents training. ARIA aims to project natural language
actions from the high-dimensional joint token distribution space into a
low-dimensional intention space, where semantically similar actions are
clustered and assigned shared rewards. This intention-aware reward aggregation
reduces reward variance by densifying reward signals, fostering better policy
optimization. Extensive experiments demonstrate that ARIA not only
significantly reduces policy gradient variance, but also delivers substantial
performance gains of an average of 9.95% across four downstream tasks,
consistently outperforming offline and online RL baselines.

</details>


### [67] [Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages](https://arxiv.org/abs/2506.00549)
*Hyangsuk Min,Yuho Lee,Minjeong Ban,Jiaqi Deng,Nicole Hee-Yeon Kim,Taewon Yun,Hang Su,Jason Cai,Hwanjun Song*

Key words: 文本摘要, 评估框架, 多领域, 中英文, 多智能体辩论

TL;DR: MSumBench提出了一个多维、多领域的文本摘要评估框架，涵盖中英文，并引入多智能体辩论系统以提高标注质量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有评估框架缺乏领域特定标准，以英语为主，且人工标注复杂性高，需改进。

Method: MSumBench整合多领域、多语言评估，引入专用标准和多智能体辩论系统提升标注质量。

Result: 评估显示不同模型在领域和语言上表现各异，发现大型语言模型评估中存在系统性偏差。

Conclusion: MSumBench为文本摘要提供了更全面的评估标准，促进领域和语言的多样性研究。

Abstract: Evaluation frameworks for text summarization have evolved in terms of both
domain coverage and metrics. However, existing benchmarks still lack
domain-specific assessment criteria, remain predominantly English-centric, and
face challenges with human annotation due to the complexity of reasoning. To
address these, we introduce MSumBench, which provides a multi-dimensional,
multi-domain evaluation of summarization in English and Chinese. It also
incorporates specialized assessment criteria for each domain and leverages a
multi-agent debate system to enhance annotation quality. By evaluating eight
modern summarization models, we discover distinct performance patterns across
domains and languages. We further examine large language models as summary
evaluators, analyzing the correlation between their evaluation and
summarization capabilities, and uncovering systematic bias in their assessment
of self-generated summaries. Our benchmark dataset is publicly available at
https://github.com/DISL-Lab/MSumBench.

</details>


### [68] [AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation](https://arxiv.org/abs/2506.00551)
*Ming Wang,Peidong Wang,Lin Wu,Xiaocui Yang,Daling Wang,Shi Feng,Yuxin Chen,Bixuan Wang,Yifei Zhang*

Key words: AI心理健康, 寻求者模拟, 情感动态, 记忆机制

TL;DR: 论文提出了一种名为AnnaAgent的情感与认知动态代理系统，旨在解决AI心理健康的寻求者模拟中的动态演化和多会话记忆问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于真实参与者的成本和伦理问题，AI心理健康研究需要更真实的寻求者模拟方法。

Method: AnnaAgent结合了情绪调节器和基于真实咨询对话训练的抱怨引发器，并采用三级记忆机制整合短期和长期记忆。

Result: 实验表明，AnnaAgent在心理辅导中比现有基线更真实地模拟寻求者。

Conclusion: AnnaAgent为AI心理健康研究提供了更真实的模拟工具，解决了动态演化和多会话记忆的挑战。

Abstract: Constrained by the cost and ethical concerns of involving real seekers in
AI-driven mental health, researchers develop LLM-based conversational agents
(CAs) with tailored configurations, such as profiles, symptoms, and scenarios,
to simulate seekers. While these efforts advance AI in mental health, achieving
more realistic seeker simulation remains hindered by two key challenges:
dynamic evolution and multi-session memory. Seekers' mental states often
fluctuate during counseling, which typically spans multiple sessions. To
address this, we propose AnnaAgent, an emotional and cognitive dynamic agent
system equipped with tertiary memory. AnnaAgent incorporates an emotion
modulator and a complaint elicitor trained on real counseling dialogues,
enabling dynamic control of the simulator's configurations. Additionally, its
tertiary memory mechanism effectively integrates short-term and long-term
memory across sessions. Evaluation results, both automated and manual,
demonstrate that AnnaAgent achieves more realistic seeker simulation in
psychological counseling compared to existing baselines. The ethically reviewed
and screened code can be found on https://github.com/sci-m-wang/AnnaAgent.

</details>


### [69] [The Hidden Language of Harm: Examining the Role of Emojis in Harmful Online Communication and Content Moderation](https://arxiv.org/abs/2506.00583)
*Yuhang Zhou,Yimin Xiao,Wei Ai,Ge Gao*

Key words: 社交媒体, 表情符号, 攻击性内容, LLM, 审核管道

TL;DR: 该论文研究了表情符号在推特上对攻击性内容的贡献，提出了一种基于LLM的多步骤审核管道来替换有害表情符号，同时保持语义意图，并通过人类评估验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 社交媒体平台在现代沟通中占据核心地位，但同时也存在攻击性内容的问题。以往研究主要关注文本指标，表情符号的作用尚未被充分探索，尽管它们很少单独构成攻击性，但可能通过象征性关联、讽刺或上下文滥用获得有害含义。

Method: 作者系统地分析了表情符号在推特攻击性消息中的分布及其被利用的方式，并提出了一种基于LLM的多步骤审核管道，选择性替换有害表情符号，同时保留推文的语义意图。

Result: 人类评估表明，该方法有效降低了攻击性感知，同时未牺牲语义。分析还揭示了不同攻击类型下的异质性效应。

Conclusion: 该研究为在线交流和表情符号审核提供了细致见解，提出的方法在减少攻击性的同时保持了内容的意义。

Abstract: Social media platforms have become central to modern communication, yet they
also harbor offensive content that challenges platform safety and inclusivity.
While prior research has primarily focused on textual indicators of offense,
the role of emojis, ubiquitous visual elements in online discourse, remains
underexplored. Emojis, despite being rarely offensive in isolation, can acquire
harmful meanings through symbolic associations, sarcasm, and contextual misuse.
In this work, we systematically examine emoji contributions to offensive
Twitter messages, analyzing their distribution across offense categories and
how users exploit emoji ambiguity. To address this, we propose an LLM-powered,
multi-step moderation pipeline that selectively replaces harmful emojis while
preserving the tweet's semantic intent. Human evaluations confirm our approach
effectively reduces perceived offensiveness without sacrificing meaning. Our
analysis also reveals heterogeneous effects across offense types, offering
nuanced insights for online communication and emoji moderation.

</details>


### [70] [Entriever: Energy-based Retriever for Knowledge-Grounded Dialog Systems](https://arxiv.org/abs/2506.00585)
*Yucheng Cai,Ke Li,Yi Huang,Junlan Feng,Zhijian Ou*

Key words: knowledge retrieval, retriever model, energy-based model, knowledge-grounded dialog

TL;DR: Entriever是一种基于能量的检索器，通过直接建模候选检索结果的整体而非单独建模知识片段，显著提升了知识检索任务和对话系统的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有检索器模型通常假设知识片段条件独立，但实际中可能存在多个相关且相互依赖的知识片段。为了解决这一问题，提出了Entriever。

Method: Entriever采用能量函数定义相关性分数，直接建模候选检索结果的整体。研究了多种能量函数架构和训练方法。

Result: Entriever在知识检索任务中显著优于强基线模型（cross-encoder），并在半监督训练的对话系统中提升了端到端性能。

Conclusion: Entriever通过整体建模知识片段的相关性，能更有效地支持知识检索和对话系统。

Abstract: A retriever, which retrieves relevant knowledge pieces from a knowledge base
given a context, is an important component in many natural language processing
(NLP) tasks. Retrievers have been introduced in knowledge-grounded dialog
systems to improve knowledge acquisition. In knowledge-grounded dialog systems,
when conditioning on a given context, there may be multiple relevant and
correlated knowledge pieces. However, knowledge pieces are usually assumed to
be conditionally independent in current retriever models. To address this
issue, we propose Entriever, an energy-based retriever. Entriever directly
models the candidate retrieval results as a whole instead of modeling the
knowledge pieces separately, with the relevance score defined by an energy
function. We explore various architectures of energy functions and different
training methods for Entriever, and show that Entriever substantially
outperforms the strong cross-encoder baseline in knowledge retrieval tasks.
Furthermore, we show that in semi-supervised training of knowledge-grounded
dialog systems, Entriever enables effective scoring of retrieved knowledge
pieces and significantly improves end-to-end performance of dialog systems.

</details>


### [71] [PAKTON: A Multi-Agent Framework for Question Answering in Long Legal Agreements](https://arxiv.org/abs/2506.00608)
*Petros Raptopoulos,Giorgos Filandrianos,Maria Lymperaiou,Giorgos Stamou*

Key words: 合同审查,多代理框架,开源,RAG,隐私保护

TL;DR: PAKTON是一个全开源、端到端的多代理框架，专注于解决合同审查的复杂性和隐私问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 合同审查需要专业法律知识且耗时，对非专家不友好；合同通常保密，限制了专有模型的使用。

Method: PAKTON采用多代理协作工作流和创新的RAG组件，实现自动化法律文件审查。

Result: 实验表明，PAKTON在预测准确性、检索性能等方面优于通用模型和预训练模型。

Conclusion: PAKTON提供了一种更易用、适应性强且保护隐私的合同审查解决方案。

Abstract: Contract review is a complex and time-intensive task that typically demands
specialized legal expertise, rendering it largely inaccessible to non-experts.
Moreover, legal interpretation is rarely straightforward-ambiguity is
pervasive, and judgments often hinge on subjective assessments. Compounding
these challenges, contracts are usually confidential, restricting their use
with proprietary models and necessitating reliance on open-source alternatives.
To address these challenges, we introduce PAKTON: a fully open-source,
end-to-end, multi-agent framework with plug-and-play capabilities. PAKTON is
designed to handle the complexities of contract analysis through collaborative
agent workflows and a novel retrieval-augmented generation (RAG) component,
enabling automated legal document review that is more accessible, adaptable,
and privacy-preserving. Experiments demonstrate that PAKTON outperforms both
general-purpose and pretrained models in predictive accuracy, retrieval
performance, explainability, completeness, and grounded justifications as
evaluated through a human study and validated with automated metrics.

</details>


### [72] [Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge Graph Guided Distractor Generation](https://arxiv.org/abs/2506.00612)
*Running Yang,Wenlong Deng,Minghui Chen,Yuyin Zhou,Xiaoxiao Li*

Key words: 知识图谱,数据增强,大型语言模型,临床多选题,干扰项生成

TL;DR: 本文提出了一个知识引导的数据增强框架KGGDG，通过生成具有误导性的干扰项，提升临床多选题的难度，以更严格评估大型语言模型的可靠性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 临床任务如诊断和治疗需要强大的决策能力，因此需要严格的评估基准来测试大型语言模型的可靠性。

Method: 采用基于知识图谱的多步语义遍历方法，生成医学上相关但事实错误的干扰项路径，指导LLM生成更具迷惑性的选项。

Result: 应用该框架后，六个广泛使用的医学QA基准测试中，顶尖LLM的准确率显著下降。

Conclusion: KGGDG是一种强大的工具，能够更全面地评估医学LLM的鲁棒性和诊断能力。

Abstract: Clinical tasks such as diagnosis and treatment require strong decision-making
abilities, highlighting the importance of rigorous evaluation benchmarks to
assess the reliability of large language models (LLMs). In this work, we
introduce a knowledge-guided data augmentation framework that enhances the
difficulty of clinical multiple-choice question (MCQ) datasets by generating
distractors (i.e., incorrect choices that are similar to the correct one and
may confuse existing LLMs). Using our KG-based pipeline, the generated choices
are both clinically plausible and deliberately misleading. Our approach
involves multi-step, semantically informed walks on a medical knowledge graph
to identify distractor paths-associations that are medically relevant but
factually incorrect-which then guide the LLM in crafting more deceptive
distractors. We apply the designed knowledge graph guided distractor generation
(KGGDG) pipline, to six widely used medical QA benchmarks and show that it
consistently reduces the accuracy of state-of-the-art LLMs. These findings
establish KGGDG as a powerful tool for enabling more robust and diagnostic
evaluations of medical LLMs.

</details>


### [73] [Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples](https://arxiv.org/abs/2506.00622)
*Haesung Pyun,Yoonah Park,Yohan Jo*

Key words: 对话状态跟踪、上下文学习、检索器优化、CombiSearch

TL;DR: 提出了CombiSearch方法，通过组合式评分优化对话状态跟踪（DST）中检索器的训练数据，显著提升数据效率和性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有检索器训练方法存在协同效应、语言特征和评分优化不足的问题，影响了DST性能。

Method: 开发了CombiSearch方法，基于例子对DST性能的组合影响进行评分，优化检索效果。

Result: 在MultiWOZ和SGD数据集上，CombiSearch实现了20倍数据效率提升和12%绝对性能改进。

Conclusion: CombiSearch证明现有检索器训练数据次优，显著提升了DST性能上限。

Abstract: In dialogue state tracking (DST), in-context learning comprises a retriever
that selects labeled dialogues as in-context examples and a DST model that uses
these examples to infer the dialogue state of the query dialogue. Existing
methods for constructing training data for retrievers suffer from three key
limitations: (1) the synergistic effect of examples is not considered, (2) the
linguistic characteristics of the query are not sufficiently factored in, and
(3) scoring is not directly optimized for DST performance. Consequently, the
retriever can fail to retrieve examples that would substantially improve DST
performance. To address these issues, we present CombiSearch, a method that
scores effective in-context examples based on their combinatorial impact on DST
performance. Our evaluation on MultiWOZ shows that retrievers trained with
CombiSearch surpass state-of-the-art models, achieving a 20x gain in data
efficiency and generalizing well to the SGD dataset. Moreover, CombiSearch
attains a 12% absolute improvement in the upper bound DST performance over
traditional approaches when no retrieval errors are assumed. This significantly
increases the headroom for practical DST performance while demonstrating that
existing methods rely on suboptimal data for retriever training.

</details>


### [74] [LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech](https://arxiv.org/abs/2506.00628)
*Niyati Bafna,Matthew Wiesner*

Key words: LID模型，口语音频，鲁棒性，输入分块，序列信息

TL;DR: 论文探讨了LID模型在口语音频中性能下降的原因，并提出了一种通过输入分块和整合序列信息的方法提升模型对口音的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的LID模型在口语音频中表现较差，且错误模式和对口音的不变性尚未深入探究，因此需要研究如何提升模型性能。

Method: 研究LID模型在口语音频中的常见错误模式，提出输入分块方法增强鲁棒性，并引入序列级信息以减少口音与语言的混淆。

Result: 提出的方法显著提升了模型在口语音频中的性能，同时保持了标准LID任务上的表现。

Conclusion: 通过简单且有效的方法，可以显著改善LID模型在口语音频中的表现，为相关领域提供了实用解决方案。

Abstract: Prior research indicates that LID model performance significantly declines on
accented speech; however, the specific causes, extent, and characterization of
these errors remain under-explored. (i) We identify a common failure mode on
accented speech whereby LID systems often misclassify L2 accented speech as the
speaker's native language or a related language. (ii) We present evidence
suggesting that state-of-the-art models are invariant to permutations of short
spans of speech, implying they classify on the basis of short phonotactic
features indicative of accent rather than language. Our analysis reveals a
simple method to enhance model robustness to accents through input chunking.
(iii) We present an approach that integrates sequence-level information into
our model without relying on monolingual ASR systems; this reduces
accent-language confusion and significantly enhances performance on accented
speech while maintaining comparable results on standard LID.

</details>


### [75] [Social Construction of Urban Space: Understanding Neighborhood Boundaries Using Rental Listings](https://arxiv.org/abs/2506.00634)
*Adam Visokay,Ruth Bagley,Ian Kennedy,Chris Hess,Kyle Crowder,Rob Voigt,Denis Peskoff*

Key words: 租房广告，城市空间，社会建构，地理空间分析，主题建模

TL;DR: 通过分析Craigslist租房广告，揭示城市空间如何通过语言被社会建构，并发现传统方法忽视的空间定义争议。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究租房广告如何通过语言塑造城市空间的社会建构，特别是邻里定义的争议性和复杂性。

Method: 结合人工和大型语言模型标注，对芝加哥Craigslist广告进行分类，并进行地理空间分析和主题建模。

Result: 发现三种空间模式：冲突的邻里定义、边界房源的合理争议以及“声誉洗白”；主题建模显示房源位置影响其描述的便利设施。

Conclusion: 自然语言处理技术能揭示传统方法难以捕捉的城市空间定义争议。

Abstract: Rental listings offer a unique window into how urban space is socially
constructed through language. We analyze Chicago Craigslist rental
advertisements from 2018 to 2024 to examine how listing agents characterize
neighborhoods, identifying mismatches between institutional boundaries and
neighborhood claims. Through manual and large language model annotation, we
classify unstructured listings from Craigslist according to their neighborhood.
Geospatial analysis reveals three distinct patterns: properties with
conflicting neighborhood designations due to competing spatial definitions,
border properties with valid claims to adjacent neighborhoods, and ``reputation
laundering" where listings claim association with distant, desirable
neighborhoods. Through topic modeling, we identify patterns that correlate with
spatial positioning: listings further from neighborhood centers emphasize
different amenities than centrally-located units. Our findings demonstrate that
natural language processing techniques can reveal how definitions of urban
spaces are contested in ways that traditional methods overlook.

</details>


### [76] [ViToSA: Audio-Based Toxic Spans Detection on Vietnamese Speech Utterances](https://arxiv.org/abs/2506.00636)
*Huy Ba Do,Vy Le-Phuong Huynh,Luan Thanh Nguyen*

Key words: 语音毒性检测,越南语,ASR,毒性片段检测,内容审核

TL;DR: 该论文提出了越南语首个用于语音中恶意内容检测的数据集ViToSA，并结合ASR和文本检测方法，显著提升了越南语语音毒性内容的识别效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在线平台上的有害语音内容日益严重，而针对低资源语言（如越南语）的语音毒性检测研究较少。

Method: 引入ViToSA数据集，结合ASR（自动语音识别）和毒性片段检测方法，形成端到端的检测流程。

Result: 实验表明，基于ViToSA微调的ASR模型在有害语音转录上WER显著降低，文本毒性检测模型性能优于现有基线。

Conclusion: ViToSA为越南语语音毒性检测提供了新基准，推动了语音内容审核的研究。

Abstract: Toxic speech on online platforms is a growing concern, impacting user
experience and online safety. While text-based toxicity detection is
well-studied, audio-based approaches remain underexplored, especially for
low-resource languages like Vietnamese. This paper introduces ViToSA
(Vietnamese Toxic Spans Audio), the first dataset for toxic spans detection in
Vietnamese speech, comprising 11,000 audio samples (25 hours) with accurate
human-annotated transcripts. We propose a pipeline that combines ASR and toxic
spans detection for fine-grained identification of toxic content. Our
experiments show that fine-tuning ASR models on ViToSA significantly reduces
WER when transcribing toxic speech, while the text-based toxic spans detection
(TSD) models outperform existing baselines. These findings establish a novel
benchmark for Vietnamese audio-based toxic spans detection, paving the way for
future research in speech content moderation.

</details>


### [77] [Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics](https://arxiv.org/abs/2506.00637)
*Lorenzo Jaime Yu Flores,Ori Ernst,Jackie Chi Kit Cheung*

Key words: 文本生成, 置信度校准, 任务无关方法, BART, Flan-T5

TL;DR: 该论文提出了一种任务无关的置信度度量方法，用于改进文本生成模型的校准，从而提高其预测的可靠性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了更好地利用文本生成模型的置信度分数，避免模型返回低质量或危险的预测，需要改进置信度校准。

Method: 论文提出了一种任务无关的置信度度量方法，仅基于模型输出的概率分布，无需额外微调或启发式方法。

Result: 该方法在BART和Flan-T5模型上显著提高了摘要、翻译和问答任务的校准效果。

Conclusion: 提出的置信度度量方法能够有效改进文本生成模型的校准，提升其预测的可靠性。

Abstract: Well-calibrated model confidence scores can improve the usefulness of text
generation models. For example, users can be prompted to review predictions
with low confidence scores, to prevent models from returning bad or potentially
dangerous predictions. However, confidence metrics are not always well
calibrated in text generation. One reason is that in generation, there can be
many valid answers, which previous methods do not always account for. Hence, a
confident model could distribute its output probability among multiple
sequences because they are all valid. We propose task-agnostic confidence
metrics suited to generation, which rely solely on the probabilities associated
with the model outputs without the need for further fine-tuning or heuristics.
Using these, we are able to improve the calibration of BART and Flan-T5 on
summarization, translation, and QA datasets.

</details>


### [78] [SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions](https://arxiv.org/abs/2506.00643)
*Weijie Xu,Shixian Cui,Xi Fang,Chi Xue,Stephanie Eckman,Chandan Reddy*

Key words: 大语言模型,多选任务,SATA-BENCH,Choice Funnel,去偏

TL;DR: 该论文介绍了SATA-BENCH，首个专为评估大语言模型（LLMs）在多选任务中的表现而设计的基准，揭示了LLMs在多选题中的表现不足，并提出了解决方案Choice Funnel。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机在于探索LLMs在识别多选题所有正确答案方面的能力，填补现有评估方法的空白。

Method: 论文提出了Choice Funnel解码策略，结合去偏和自适应阈值技术，以提高模型在多选题中的表现。

Result: 研究发现当前最强的LLM在多选题中的准确率仅41.8%，但Choice Funnel可将准确率提升29%，并降低64%的推理成本。

Conclusion: 论文揭示了LLMs在多选题中的局限性，并提供了改进框架，推动了LLM在多答案任务中的发展。

Abstract: Large language models (LLMs) are increasingly evaluated on single-answer
multiple-choice tasks, yet many real-world problems require identifying all
correct answers from a set of options. This capability remains underexplored.
We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on
Select All That Apply (SATA) questions across diverse domains, including
reading comprehension, law, and biomedicine. Our evaluation of 27 open-source
and proprietary models reveals a significant gap: even the strongest model
achieves only 41.8% exact match, exposing LLMs' inability to reliably identify
all correct answers. We find that this weakness stems from two core challenges:
selection bias - models favor certain choices regardless of content, and count
bias - models fail to predict the correct number of answers. To address these
issues, we propose Choice Funnel, a decoding strategy that combines token
debiasing with adaptive thresholding to guide models toward complete and
accurate selections. Choice Funnel achieves up to 29% higher exact match than
competitive baselines while reducing inference cost by over 64%. Our findings
expose fundamental limitations in current LLMs and introduce a new framework
for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and
Choice Funnel to promote LLM development for robust decision-making in
realistic, multi-answer applications.

</details>


### [79] [Clinical Annotations for Automatic Stuttering Severity Assessment](https://arxiv.org/abs/2506.00644)
*Ana Rita Valente,Rufael Marew,Hawau Olamide Toyin,Hamdan Al-Ali,Anelise Bohnen,Inma Becerra,Elsa Marta Soares,Goncalo Leal,Hanan Aldarmaki*

Key words: 口吃,标注方案,FluencyBank,多模态特征,临床专家

TL;DR: 文章提出了一种基于临床标准的新的口吃标注方案，以增强FluencyBank数据集，并通过专家标注和多模态特征提高标注质量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 口吃是一种复杂的障碍，需要专业知识和技能进行有效评估和治疗，现有数据集需要更高质量的标注方案。

Method: 聘请专业临床医生进行标注，采用多模态特征（视听信息）检测和分类口吃瞬间、次要行为及紧张评分，并提供基于专家共识的高可靠测试集。

Result: 实验和分析表明，该任务复杂性高，需要丰富的临床专业知识用于有效训练和评估口吃评估模型。

Conclusion: 通过专家标注和多模态特征，新标注方案显著提升了数据集的临床实用性和模型评估的可靠性。

Abstract: Stuttering is a complex disorder that requires specialized expertise for
effective assessment and treatment. This paper presents an effort to enhance
the FluencyBank dataset with a new stuttering annotation scheme based on
established clinical standards. To achieve high-quality annotations, we hired
expert clinicians to label the data, ensuring that the resulting annotations
mirror real-world clinical expertise. The annotations are multi-modal,
incorporating audiovisual features for the detection and classification of
stuttering moments, secondary behaviors, and tension scores. In addition to
individual annotations, we additionally provide a test set with highly reliable
annotations based on expert consensus for assessing individual annotators and
machine learning models. Our experiments and analysis illustrate the complexity
of this task that necessitates extensive clinical expertise for valid training
and evaluation of stuttering assessment models.

</details>


### [80] [GuideX: Guided Synthetic Data Generation for Zero-Shot Information Extraction](https://arxiv.org/abs/2506.00649)
*Neil De La Fuente,Oscar Sainz,Iker García-Ferrero,Eneko Agirre*

Key words: 信息提取, 零样本学习, 领域适应, GUIDEX

TL;DR: 提出了GUIDEX方法，通过自动定义领域模式、推断指南并生成合成标记实例，显著提升了零样本命名实体识别任务的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统信息提取系统需专家设计、数据标注和模型训练，成本高；大语言模型在零样本任务中表现不佳。

Method: 提出GUIDEX方法，自动生成领域模式、指南和合成数据，结合Llama 3.1微调。

Result: 在7个零样本NER基准测试中达到新最高水平，提升7 F1点（无人类数据时），结合人类数据时提升近2 F1点。

Conclusion: GUIDEX显著提升模型对复杂领域模式的理解，代码和数据集已开源。

Abstract: Information Extraction (IE) systems are traditionally domain-specific,
requiring costly adaptation that involves expert schema design, data
annotation, and model training. While Large Language Models have shown promise
in zero-shot IE, performance degrades significantly in unseen domains where
label definitions differ. This paper introduces GUIDEX, a novel method that
automatically defines domain-specific schemas, infers guidelines, and generates
synthetically labeled instances, allowing for better out-of-domain
generalization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art
across seven zeroshot Named Entity Recognition benchmarks. Models trained with
GUIDEX gain up to 7 F1 points over previous methods without humanlabeled data,
and nearly 2 F1 points higher when combined with it. Models trained on GUIDEX
demonstrate enhanced comprehension of complex, domain-specific annotation
schemas. Code, models, and synthetic datasets are available at
neilus03.github.io/guidex.com

</details>


### [81] [Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques](https://arxiv.org/abs/2506.00658)
*Lang Xiong,Raina Gao,Alyssa Jeong,Yicheng Fu,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Key words: 讽刺分类, 情感提示, Sarc7, 语言模型, 生成方法

TL;DR: 论文介绍了Sarc7基准，用于分类7种讽刺类型，并提出了一种基于情感的生成方法。Gemini 2.5在使用情感提示时表现最佳，F1得分为0.3664。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 讽刺因其复杂性对计算模型构成挑战，研究旨在通过分类和生成讽刺来改善对人类交流的理解。

Method: 通过标注MUStARD数据集，采用零样本、少样本、链式思维和情感提示技术进行评估，并提出基于情感的生成方法。

Result: 情感提示在分类中表现最佳（F1=0.3664），人类评估者更偏好情感提示生成的内容。

Conclusion: 情感提示在讽刺分类和生成中均表现出优越性，为处理讽刺提供了新思路。

Abstract: Sarcasm is a form of humor where expressions convey meanings opposite to
their literal interpretations. Classifying and generating sarcasm using large
language models is vital for interpreting human communication. Sarcasm poses
challenges for computational models, due to its nuanced nature. We introduce
Sarc7, a benchmark that classifies 7 types of sarcasm: self-deprecating,
brooding, deadpan, polite, obnoxious, raging, and manic by annotating entries
of the MUStARD dataset. Classification was evaluated using zero-shot, few-shot,
chain-of-thought (CoT), and a novel emotion-based prompting technique. We
propose an emotion-based generation method developed by identifying key
components of sarcasm-incongruity, shock value, and context dependency. Our
classification experiments show that Gemini 2.5, using emotion-based prompting,
outperforms other setups with an F1 score of 0.3664. Human evaluators preferred
our emotion-based prompting, with 38.46% more successful generations than
zero-shot prompting.

</details>


### [82] [SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues](https://arxiv.org/abs/2506.00668)
*Martin Kuo,Jianyi Zhang,Aolin Ding,Louis DiValentin,Amin Hass,Benjamin F Morris,Isaac Jacobson,Randolph Linderman,James Kiessling,Nicolas Ramos,Bhavna Gopal,Maziyar Baran Pouyan,Changwei Liu,Hai Li,Yiran Chen*

Key words: LLM安全、多轮攻击、防御机制、安全推理、数据集

TL;DR: 提出了STREAM防御机制，通过安全推理调节器保护LLM免受多轮对话攻击，降低51.2%的攻击成功率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 恶意攻击者利用多轮对话危害LLM，亟需防御机制保护模型安全。

Method: 构建安全推理多轮对话数据集，微调解码器识别恶意意图并警示LLM。

Result: STREAM显著优于现有防御技术，攻击成功率降低51.2%，同时保持LLM功能。

Conclusion: STREAM是一种有效的多轮攻击防御方法，兼顾安全性与功能性。

Abstract: Malicious attackers can exploit large language models (LLMs) by engaging them
in multi-turn dialogues to achieve harmful objectives, posing significant
safety risks to society. To address this challenge, we propose a novel defense
mechanism: SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues
(STREAM). STREAM defends LLMs against multi-turn attacks while preserving their
functional capabilities. Our approach involves constructing a human-annotated
dataset, the Safety Reasoning Multi-turn Dialogues dataset, which is used to
fine-tune a plug-and-play safety reasoning moderator. This model is designed to
identify malicious intent hidden within multi-turn conversations and alert the
target LLM of potential risks. We evaluate STREAM across multiple LLMs against
prevalent multi-turn attack strategies. Experimental results demonstrate that
our method significantly outperforms existing defense techniques, reducing the
Attack Success Rate (ASR) by 51.2%, all while maintaining comparable LLM
capability.

</details>


### [83] [DeepRAG: Integrating Hierarchical Reasoning and Process Supervision for Biomedical Multi-Hop QA](https://arxiv.org/abs/2506.00671)
*Yuelyu Ji,Hang Zhang,Shiven Verma,Hui Ji,Chun Li,Yushui Han,Yanshan Wang*

Key words: DeepRAG,DeepSeek,RAG Gym,MedHopQA,UMLS

TL;DR: DeepRAG是一种新颖框架，结合了DeepSeek层次化问题分解能力与RAG Gym统一检索增强生成优化，通过过程级监督提升医学问答任务的准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 针对医学问答任务MedHopQA的复杂性挑战，提出需要系统分解复杂问题并利用UMLS本体增强概念级精准度。

Method: 框架整合DeepSeek的问题分解能力和RAG Gym的检索生成优化，通过UMLS提供的概念级奖励信号进行监督。

Result: 在MedHopQA数据集上显著超越基线模型，提高了Exact Match和概念级准确性。

Conclusion: DeepRAG在医学问答任务中表现出色，证明了其联合优化方法的有效性。

Abstract: We propose DeepRAG, a novel framework that integrates DeepSeek hierarchical
question decomposition capabilities with RAG Gym unified retrieval-augmented
generation optimization using process level supervision. Targeting the
challenging MedHopQA biomedical question answering task, DeepRAG systematically
decomposes complex queries into precise sub-queries and employs concept level
reward signals informed by the UMLS ontology to enhance biomedical accuracy.
Preliminary evaluations on the MedHopQA dataset indicate that DeepRAG
significantly outperforms baseline models, including standalone DeepSeek and
RAG Gym, achieving notable improvements in both Exact Match and concept level
accuracy.

</details>


### [84] [Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments](https://arxiv.org/abs/2506.00694)
*Li Zhang,Morgan Gray,Jaromir Savelka,Kevin D. Ashley*

Key words: Large Language Models, legal argumentation, hallucination, factor utilization, abstention

TL;DR: 论文介绍了一种自动化评估大语言模型（LLM）在生成法律论据任务中的表现的方法，重点关注幻觉、因素利用和适时弃权，结果表明LLMs在避免幻觉方面表现良好，但在因素利用和弃权能力上仍需改进。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 评估和提升LLMs在复杂法律任务（如论据生成）中的可靠性和准确性。

Method: 设计自动化流程，利用外部LLM提取生成论据中的因素并与输入案例的真实因素对比，测试8种LLM在三种难度递增任务中的表现。

Result: LLMs在避免幻觉上表现优异（>90%准确率），但未能充分利用案例中的全部相关因素，且多数模型在无共享因素时无法适时弃权。

Conclusion: 当前LLMs在法律任务中的因素利用和弃权能力仍需改进，自动化评估方法为未来优化提供了可扩展的途径。

Abstract: Large Language Models (LLMs) demonstrate potential in complex legal tasks
like argument generation, yet their reliability remains a concern. Building
upon pilot work assessing LLM generation of 3-ply legal arguments using human
evaluation, this paper introduces an automated pipeline to evaluate LLM
performance on this task, specifically focusing on faithfulness (absence of
hallucination), factor utilization, and appropriate abstention. We define
hallucination as the generation of factors not present in the input case
materials and abstention as the model's ability to refrain from generating
arguments when instructed and no factual basis exists. Our automated method
employs an external LLM to extract factors from generated arguments and
compares them against the ground-truth factors provided in the input case
triples (current case and two precedent cases). We evaluated eight distinct
LLMs on three tests of increasing difficulty: 1) generating a standard 3-ply
argument, 2) generating an argument with swapped precedent roles, and 3)
recognizing the impossibility of argument generation due to lack of shared
factors and abstaining. Our findings indicate that while current LLMs achieve
high accuracy (over 90%) in avoiding hallucination on viable argument
generation tests (Tests 1 & 2), they often fail to utilize the full set of
relevant factors present in the cases. Critically, on the abstention test (Test
3), most models failed to follow instructions to stop, instead generating
spurious arguments despite the lack of common factors. This automated pipeline
provides a scalable method for assessing these crucial LLM behaviors,
highlighting the need for improvements in factor utilization and robust
abstention capabilities before reliable deployment in legal settings. Project
page:
https://github.com/lizhang-AIandLaw/Measuring-Faithfulness-and-Abstention.

</details>


### [85] [From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation](https://arxiv.org/abs/2506.00713)
*Debarati Bhattacharjee,Ashish Anand*

Key words: 论证知识图（AKG）、假言推理、知识库、推理规则、间接关系

TL;DR: 该论文提出了一个将议论文本转换为论证知识图（AKG）的框架，通过构建知识库和推理规则，生成具有丰富属性的AKG，以增强对论证结构的理解和推理能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的论证数据集缺乏对隐含关系的捕捉能力，尤其是间接攻击关系。AKG旨在通过图形化表示和推理规则的应用，弥补这一不足。

Method: 1. 标注论证组件（ACs）和论证关系（ARs）；2. 构建带元数据属性的知识库图；3. 应用假言推理（modus ponens）生成论证；4. 创建具有丰富属性的AKG；5. 识别缺失的推理规则以发现间接攻击关系。

Result: 生成的AKG能够捕捉隐含的间接关系，尤其是未检测到的攻击关系，同时为未来的推理任务（如论证一致性和修订机会）奠定基础。

Conclusion: AKG通过图形化表示和推理规则的显式化，增强了论证结构的可理解性和推理能力，为未来的论证分析任务提供了新工具。

Abstract: This paper presents a framework to convert argumentative texts into argument
knowledge graphs (AKG). Starting with basic annotations of argumentative
components (ACs) and argumentative relations (ARs), we enrich the information
by constructing a knowledge base (KB) graph with metadata attributes for nodes.
Next, we use premises and inference rules from the KB to form arguments by
applying modus ponens. From these arguments, we create an AKG. The nodes and
edges of the AKG have attributes that capture important argumentative features.
We also find missing inference rules by identifying markers. This makes it
possible to identify undercut attacks that were previously undetectable in
existing datasets. The AKG gives a graphical view of the argumentative
structure that is easier to understand than theoretical formats. It also
prepares the ground for future reasoning tasks, including checking the
coherence of arguments and identifying opportunities for revision. For this, it
is important to find indirect relations, many of which are implicit. Our
proposed AKG format, with annotated inference rules and modus ponens, will help
reasoning models learn the implicit indirect relations that require inference
over arguments and the relations between them.

</details>


### [86] [Chain-of-Thought Training for Open E2E Spoken Dialogue Systems](https://arxiv.org/abs/2506.00722)
*Siddhant Arora,Jinchuan Tian,Hayato Futami,Jee-weon Jung,Jiatong Shi,Yosuke Kashiwagi,Emiru Tsunoo,Shinji Watanabe*

Key words: 端到端口语对话系统, 链式思维, 多模态语言模型, ROUGE-1

TL;DR: 论文提出了一种基于链式思维（CoT）的端到端口语对话系统，解决了现有方法需要大规模数据和语义不连贯的问题，并在公开数据集上取得了显著效果提升。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有端到端口语对话系统需要大量训练数据且生成响应语义不连贯，因此作者提出了一种更高效的解决方案。

Method: 通过链式思维（CoT）策略，将对话数据训练与多模态语言模型的预训练（如ASR、TTS和文本LM任务）紧密结合。

Result: 方法在ROUGE-1指标上比基线提高了1.5，且仅需300小时的公开人类对话数据即可高效训练。

Conclusion: 该方法不仅提升了口语对话系统的性能，还降低了数据需求，具有实际应用潜力。

Abstract: Unlike traditional cascaded pipelines, end-to-end (E2E) spoken dialogue
systems preserve full differentiability and capture non-phonemic information,
making them well-suited for modeling spoken interactions. However, existing E2E
approaches often require large-scale training data and generates responses
lacking semantic coherence. We propose a simple yet effective strategy
leveraging a chain-of-thought (CoT) formulation, ensuring that training on
conversational data remains closely aligned with the multimodal language model
(LM)'s pre-training on speech recognition~(ASR), text-to-speech synthesis
(TTS), and text LM tasks. Our method achieves over 1.5 ROUGE-1 improvement over
the baseline, successfully training spoken dialogue systems on publicly
available human-human conversation datasets, while being compute-efficient
enough to train on just 300 hours of public human-human conversation data, such
as the Switchboard. We will publicly release our models and training code.

</details>


### [87] [Structured Gradient Guidance for Few-Shot Adaptation in Large Language Models](https://arxiv.org/abs/2506.00726)
*Hongye Zheng,Yichen Wang,Ray Pan,Guiran Liu,Binrong Zhu,Hanlu Zhang*

Key words: 梯度感知，大语言模型，少样本学习，微调，训练稳定性

TL;DR: 提出了一种梯度感知的微调方法，用于提升大语言模型在少样本条件下的任务适应性和训练稳定性。通过梯度相关正则项和梯度对齐机制，优化了训练路径，并在多任务和跨域场景中表现优越。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决大语言模型在少样本条件下微调时的任务适应性和训练稳定性问题，减少对大量标注数据的依赖。

Method: 基于基础损失函数，引入两个梯度相关正则项：梯度方向一致性和梯度幅值控制，以及梯度对齐机制，优化训练路径。

Result: 在多种自然语言理解任务中表现出优于现有方法的平均准确率、梯度稳定性和方向对齐性，适用于低资源环境。

Conclusion: 梯度感知微调框架能有效利用大语言模型的表征能力，确保训练稳定性并减少对大量标注数据的需求。

Abstract: This paper presents a gradient-informed fine-tuning method for large language
models under few-shot conditions. The goal is to enhance task adaptability and
training stability when data is limited. The method builds on a base loss
function and introduces two gradient-related regularization terms. The first
enforces gradient direction consistency to guide parameter updates along
task-relevant directions and prevent drift. The second controls gradient
magnitude to avoid abnormal updates. Together, these components support a more
efficient and stable optimization path. To further improve cross-task
generalization, the method incorporates a gradient alignment mechanism. This
mechanism measures the consistency between optimization directions of the
source and target tasks. It enhances fine-tuning performance in multi-task and
cross-domain scenarios. Across various natural language understanding tasks,
the method outperforms existing fine-tuning strategies in average accuracy,
gradient stability, and directional alignment. Empirical evaluations under
different sample sizes and domain-specific tasks confirm the method's
robustness and broad applicability in low-resource environments. In particular,
the method shows clear advantages in controlling parameter update paths. The
results demonstrate that a gradient-based fine-tuning framework can effectively
leverage the representational power of large language models. It ensures
training stability while reducing dependence on large volumes of labeled data.

</details>


### [88] [Narrative Media Framing in Political Discourse](https://arxiv.org/abs/2506.00737)
*Yulia Otmakhova,Lea Frermann*

Key words: narrative frames, framing, operationalization, climate change, COVID-19, LLMs

TL;DR: 提出了一个框架来形式化和操作化叙事框架，并在气候变化和COVID-19领域进行了验证。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究自动化框架分析中叙事框架的缺失，探讨如何将叙事性与框架分析结合。

Method: 提出了一个框架，分析了新闻文章数据集中的叙事框架组成部分，并测试了LLMs在预测叙事框架及其组成部分中的表现。

Result: 叙事框架组成部分在不同政治倾向中的主导性分析发现，预测结果与理论一致，证明了方法的泛化性。

Conclusion: 提出的框架能有效操作化叙事框架，并在不同领域中验证了其有效性。

Abstract: Narrative frames are a powerful way of conceptualizing and communicating
complex, controversial ideas, however automated frame analysis to date has
mostly overlooked this framing device. In this paper, we connect elements of
narrativity with fundamental aspects of framing, and present a framework which
formalizes and operationalizes such aspects. We annotate and release a data set
of news articles in the climate change domain, analyze the dominance of
narrative frame components across political leanings, and test LLMs in their
ability to predict narrative frames and their components. Finally, we apply our
framework in an unsupervised way to elicit components of narrative framing in a
second domain, the COVID-19 crisis, where our predictions are congruent with
prior theoretical work showing the generalizability of our approach.

</details>


### [89] [DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments](https://arxiv.org/abs/2506.00739)
*Chiyu Zhang,Marc-Alexandre Cote,Michael Albada,Anush Sankaran,Jack W. Stokes,Tong Wang,Amir Abdi,William Blum,Muhammad Abdul-Mageed*

Key words: 语言模型, 网络安全, 评估工具, DefenderBench

TL;DR: DefenderBench是一个开源工具包，用于评估语言模型在网络安全任务中的表现，包括网络入侵、恶意内容检测等。测试结果显示Claude-3.7-sonnet表现最佳。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管语言模型在理解和推理方面表现出色，但其在网络安全领域的潜力尚未充分探索。

Method: 开发DefenderBench工具包，提供标准化评估框架，测试多种语言模型在网络安全任务中的表现。

Result: Claude-3.7-sonnet得分最高（81.65），开源的Llama 3.3 70B也表现不俗（71.81）。

Conclusion: DefenderBench为研究人员提供了一个可负担且公平的评估工具，支持自定义模型和任务集成，促进可重复性。

Abstract: Large language model (LLM) agents have shown impressive capabilities in human
language comprehension and reasoning, yet their potential in cybersecurity
remains underexplored. We introduce DefenderBench, a practical, open-source
toolkit for evaluating language agents across offense, defense, and
cybersecurity knowledge-based tasks. DefenderBench includes environments for
network intrusion, malicious content detection, code vulnerability analysis,
and cybersecurity knowledge assessment. It is intentionally designed to be
affordable and easily accessible for researchers while providing fair and
rigorous assessment. We benchmark several state-of-the-art (SoTA) and popular
LLMs, including both open- and closed-weight models, using a standardized
agentic framework. Our results show that Claude-3.7-sonnet performs best with a
DefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40,
while the best open-weight model, Llama 3.3 70B, is not far behind with a
DefenderBench score of 71.81. DefenderBench's modular design allows seamless
integration of custom LLMs and tasks, promoting reproducibility and fair
comparisons. An anonymized version of DefenderBench is available at
https://github.com/microsoft/DefenderBench.

</details>


### [90] [Length Aware Speech Translation for Video Dubbing](https://arxiv.org/abs/2506.00740)
*Harveen Singh Chadha,Aswin Shanmugam Subramanian,Vikas Joshi,Shubham Bansal,Jian Xue,Rupeshkumar Mehta,Jinyu Li*

Key words: 视频配音, 语音翻译, 长度敏感, 实时对齐

TL;DR: 论文提出了一种音素端到端长度敏感语音翻译模型（LSST）和长度感知波束搜索（LABS），以在设备上实时视频配音中高效对齐翻译音频与源音频。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 视频配音中，翻译音频与源音频对齐是一大挑战，尤其是在实时、设备上的场景。

Method: 开发了LSST模型和使用预定义标签生成不同长度翻译的LABS方法。

Result: 在保持BLEU分数可比性的同时，显著提升了同步质量（西班牙语MOS增益0.34，韩语0.65）。

Conclusion: 该方法在视频配音中实现了高效的音频对齐，尤其是对实时设备场景。

Abstract: In video dubbing, aligning translated audio with the source audio is a
significant challenge. Our focus is on achieving this efficiently, tailored for
real-time, on-device video dubbing scenarios. We developed a phoneme-based
end-to-end length-sensitive speech translation (LSST) model, which generates
translations of varying lengths short, normal, and long using predefined tags.
Additionally, we introduced length-aware beam search (LABS), an efficient
approach to generate translations of different lengths in a single decoding
pass. This approach maintained comparable BLEU scores compared to a baseline
without length awareness while significantly enhancing synchronization quality
between source and target audio, achieving a mean opinion score (MOS) gain of
0.34 for Spanish and 0.65 for Korean, respectively.

</details>


### [91] [Data Swarms: Optimizable Generation of Synthetic Evaluation Data](https://arxiv.org/abs/2506.00741)
*Shangbin Feng,Yike Wang,Weijia Shi,Yulia Tsvetkov*

Key words: Data Swarms, Adversarial Swarms, 粒子群优化, LLM评估, 合成数据

TL;DR: Data Swarms是一种优化生成合成评估数据的算法，通过粒子群优化协同搜索参数空间以提升数据生成器的性能。其扩展版本Adversarial Swarms通过对抗生成和模型学习动态提升数据质量和模型鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决现有LLM评估数据生成方法的不足，提升评估数据的多样性和针对性。

Method: 使用粒子群优化训练初始数据生成器群，并通过对抗生成与模型学习动态优化。

Result: 在五项评估目标上优于八种基线方法，且能优化多目标组合并泛化到未见的LLM模型。

Conclusion: Data Swarms和Adversarial Swarms能高效生成高质量评估数据，并提升模型鲁棒性和泛化能力。

Abstract: We propose Data Swarms, an algorithm to optimize the generation of synthetic
evaluation data and advance quantitative desiderata of LLM evaluation. We first
train a swarm of initial data generators using existing data, and define
various evaluation objectives to reflect the desired properties of evaluation
(e.g., generate more difficult problems for the evaluated models) and
quantitatively evaluate data generators. We then employ particle swarm
optimization to optimize the swarm of data generators, where they
collaboratively search through the model parameter space to find new generators
that advance these objectives. We further extend it to Adversarial Swarms,
where the data generator swarm generates harder data while the test taker model
swarm learns from such data, co-evolving dynamically for better data and models
simultaneously. Extensive experiments demonstrate that Data Swarms outperforms
eight data generation baselines across five evaluation objectives, while
Adversarial Swarms produce more robust learning of synthetic data and stronger
generalization. Further analysis reveals that Data Swarms successfully
optimizes compositions of multiple evaluation objectives and generalizes to new
off-the-shelf LLMs, unseen at optimization time.

</details>


### [92] [Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection](https://arxiv.org/abs/2506.00743)
*Yeshwanth Venkatesha,Souvik Kundu,Priyadarshini Panda*

Key words: 参数高效微调, 联邦学习, 多头注意力, 头部剪枝, 加权聚合

TL;DR: 本文提出了一种在联邦学习框架内高效进行参数高效微调（PEFT）的方法，针对多头注意力（MHA）语言模型，通过头部剪枝、加权聚合和客户端选择策略解决FL中的挑战。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管PEFT在自然语言处理中广泛使用，但在隐私保护的分布式学习（如联邦学习）中应用有限，主要由于设备资源受限和数据分布不均的挑战。

Method: 采用头部剪枝降低训练复杂度，提出头部特定的加权聚合机制和客户端选择策略，确保全局模型捕捉多样化的更新。

Result: 在MultiNLI等数据集上验证，使用T5-small模型和LoRA方法，达到90%的稀疏度，通信优势提升1.8倍，训练操作减少3.9倍，准确率下降小于2%。

Conclusion: 该方法在联邦学习中高效实现了PEFT，显著减少了通信和计算开销，同时保持了模型性能。

Abstract: Parameter Efficient Fine-Tuning (PEFT) has become the de-facto approach in
adapting Large Language Models (LLMs) for downstream tasks in Natural Language
Processing. However, its adoption in privacy-preserving distributed learning
frameworks, such as Federated Learning (FL), remains relatively limited. This
is mainly due to challenges specific to FL, such as resource-constrained
devices and diverse data distributions among clients. In this paper, we propose
an efficient method to perform PEFT within the FL framework for Multi-Head
Attention (MHA) based language models. We address the challenges through head
pruning, a novel head-specific weighted aggregation mechanism, and a client
selection strategy. Head pruning minimizes training complexity within the
clients, guided by the importance score computed based on the confidence of the
attention head. Weighted aggregation of heads ensures the global model captures
crucial updates from diverse clients complementing our client selection
strategy. We show results on the MultiNLI benchmark along with 20 Newsgroups,
XL-Sum, and E2E NLG datasets. We use the MultiNLI dataset and T5-small model
with LoRA as our PEFT method, attaining sparsity levels of up to 90%, resulting
in a communication advantage of up to 1.8x and a reduction in training OPs of
3.9x while maintaining the accuracy drop under 2%.

</details>


### [93] [Translate With Care: Addressing Gender Bias, Neutrality, and Reasoning in Large Language Model Translations](https://arxiv.org/abs/2506.00748)
*Pardis Sadat Zahraei,Ali Emami*

Key words: gender bias, machine translation, genderless languages, TWC dataset, fine-tuning

TL;DR: 论文提出了Translate-with-Care (TWC)数据集，用于评估翻译系统在处理无性别语言时的性别偏见和逻辑连贯性问题，并展示了微调mBART-50的有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决机器翻译中性别偏见和逻辑连贯性问题，尤其是在无性别语言（如波斯语、印尼语、芬兰语）与有性别语言（如英语）之间的翻译。

Method: 引入TWC数据集（3,950个挑战性场景），评估GPT-4、mBART-50、NLLB-200和Google Translate的性能，并对mBART-50进行微调。

Result: 显示所有模型在无性别内容翻译中普遍存在性别偏见和推理错误，微调mBART-50显著改善了这些问题且表现优于专有模型。

Conclusion: 需针对性解决机器翻译中的性别和语义连贯性问题，尤其是在无性别语言中，以提升翻译的公平性和准确性。

Abstract: Addressing gender bias and maintaining logical coherence in machine
translation remains challenging, particularly when translating between natural
gender languages, like English, and genderless languages, such as Persian,
Indonesian, and Finnish. We introduce the Translate-with-Care (TWC) dataset,
comprising 3,950 challenging scenarios across six low- to mid-resource
languages, to assess translation systems' performance. Our analysis of diverse
technologies, including GPT-4, mBART-50, NLLB-200, and Google Translate,
reveals a universal struggle in translating genderless content, resulting in
gender stereotyping and reasoning errors. All models preferred masculine
pronouns when gender stereotypes could influence choices. Google Translate and
GPT-4 showed particularly strong bias, favoring male pronouns 4-6 times more
than feminine ones in leadership and professional success contexts. Fine-tuning
mBART-50 on TWC substantially resolved these biases and errors, led to strong
generalization, and surpassed proprietary LLMs while remaining open-source.
This work emphasizes the need for targeted approaches to gender and semantic
coherence in machine translation, particularly for genderless languages,
contributing to more equitable and accurate translation systems.

</details>


### [94] [Understanding and Mitigating Cross-lingual Privacy Leakage via Language-specific and Universal Privacy Neurons](https://arxiv.org/abs/2506.00759)
*Wenshuo Dong,Qingsong Yang,Shu Yang,Lijie Hu,Meng Ding,Wanyu Lin,Tianhang Zheng,Di Wang*

Key words: 大语言模型, 隐私泄露, 多语言, 神经元

TL;DR: 研究表明，大语言模型（LLMs）在多语言环境下存在隐私泄露风险，通过识别并停用隐私通用神经元和语言特定隐私神经元，可降低风险。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探究LLMs在多语言环境下的隐私泄露问题，解决现有方法仅适用于英语的局限性。

Method: 分析跨语言隐私泄露的信息流，识别隐私通用神经元和语言特定神经元，并通过停用这些神经元降低风险。

Result: 停用相关神经元后，跨语言隐私泄露风险降低23.3%-31.6%。

Conclusion: LLMs在多语言环境下存在隐私泄露风险，通过针对性处理神经元可有效缓解问题。

Abstract: Large Language Models (LLMs) trained on massive data capture rich information
embedded in the training data. However, this also introduces the risk of
privacy leakage, particularly involving personally identifiable information
(PII). Although previous studies have shown that this risk can be mitigated
through methods such as privacy neurons, they all assume that both the
(sensitive) training data and user queries are in English. We show that they
cannot defend against the privacy leakage in cross-lingual contexts: even if
the training data is exclusively in one language, these (private) models may
still reveal private information when queried in another language. In this
work, we first investigate the information flow of cross-lingual privacy
leakage to give a better understanding. We find that LLMs process private
information in the middle layers, where representations are largely shared
across languages. The risk of leakage peaks when converted to a
language-specific space in later layers. Based on this, we identify
privacy-universal neurons and language-specific privacy neurons.
Privacy-universal neurons influence privacy leakage across all languages, while
language-specific privacy neurons are only related to specific languages. By
deactivating these neurons, the cross-lingual privacy leakage risk is reduced
by 23.3%-31.6%.

</details>


### [95] [Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models](https://arxiv.org/abs/2506.00773)
*Boheng Sheng,Jiacheng Yao,Meicong Zhang,Guoxiu He*

Key words: 大型语言模型, 语义相似度, 动态分块, 问题感知分类器

TL;DR: 论文提出了一种动态分割和选择长文本块的方法，通过语义相似度自适应划分，并训练问题感知分类器，显著提升了LLM在长文本处理中的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型在处理长文本时表现不佳，固定长度的分块方法容易分割语义相关内容，导致理解模糊。本文旨在解决这一问题。

Method: 通过计算相邻句子的语义相似度，动态划分长文本为变长块，同时训练问题感知分类器以选择关键块。

Result: 在单跳和多跳问答基准测试中，该方法一致优于基线，并能处理多达256k标记的序列。

Conclusion: 动态分块和问题感知选择显著提升了LLM对长文本的理解能力，具备良好的鲁棒性。

Abstract: Large language models (LLMs) often struggle to accurately read and comprehend
extremely long texts. Current methods for improvement typically rely on
splitting long contexts into fixed-length chunks. However, fixed truncation
risks separating semantically relevant content, leading to ambiguity and
compromising accurate understanding. To overcome this limitation, we propose a
straightforward approach for dynamically separating and selecting chunks of
long context, facilitating a more streamlined input for LLMs. In particular, we
compute semantic similarities between adjacent sentences, using lower
similarities to adaptively divide long contexts into variable-length chunks. We
further train a question-aware classifier to select sensitive chunks that are
critical for answering specific questions. Experimental results on both
single-hop and multi-hop question-answering benchmarks show that the proposed
approach consistently outperforms strong baselines. Notably, it maintains
robustness across a wide range of input lengths, handling sequences of up to
256k tokens. Our datasets and code are available at the following link:
https://github.com/ECNU-Text-Computing/DCS

</details>


### [96] [Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge](https://arxiv.org/abs/2506.00777)
*Md Tahmid Rahman Laskar,Israt Jahan,Elham Dolatabadi,Chun Peng,Enamul Hoque,Jimmy Huang*

Key words: 大语言模型（LLMs）、生物医学关系抽取、评估方法、结构化输出、领域适应

TL;DR: 论文研究了利用大语言模型（LLMs）作为评估器的方法，用于生物医学关系抽取任务，发现其效果不佳（准确率低于50%），并提出结构化输出格式和领域适应技术以提高性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于LLMs生成类似人类的文本，传统自动评估指标不可靠，而人工评估成本高且耗时，因此探索LLMs作为评估器的替代方法。

Method: 通过让8个LLMs作为法官评估5个其他LLMs在3个生物医学关系抽取数据集上的表现，提出结构化输出格式和领域适应技术。

Result: LLM-法官性能较差（平均低于50%准确率），但结构化输出格式使其性能提升约15%。

Conclusion: 研究证明了结构化输出和领域适应技术能有效提升LLMs作为评估器的性能，并公开了标注数据。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance in
biomedical relation extraction, even in zero-shot scenarios. However,
evaluating LLMs in this task remains challenging due to their ability to
generate human-like text, often producing synonyms or abbreviations of
gold-standard answers, making traditional automatic evaluation metrics
unreliable. On the other hand, while human evaluation is more reliable, it is
costly and time-consuming, making it impractical for real-world applications.
This paper investigates the use of LLMs-as-the-Judge as an alternative
evaluation method for biomedical relation extraction. We benchmark 8 LLMs as
judges to evaluate the responses generated by 5 other LLMs across 3 biomedical
relation extraction datasets. Unlike other text-generation tasks, we observe
that LLM-based judges perform quite poorly (usually below 50% accuracy) in the
biomedical relation extraction task. Our findings reveal that it happens mainly
because relations extracted by LLMs do not adhere to any standard format. To
address this, we propose structured output formatting for LLM-generated
responses that helps LLM-Judges to improve their performance by about 15% (on
average). We also introduce a domain adaptation technique to further enhance
LLM-Judge performance by effectively transferring knowledge between datasets.
We release both our human-annotated and LLM-annotated judgment data (36k
samples in total) for public use here:
https://github.com/tahmedge/llm_judge_biomedical_re.

</details>


### [97] [KG-TRACES: Enhancing Large Language Models with Knowledge Graph-constrained Trajectory Reasoning and Attribution Supervision](https://arxiv.org/abs/2506.00783)
*Rong Wu,Pinlong Cai,Jianbiao Mei,Licheng Wen,Tao Hu,Xuemeng Yang,Daocheng Fu,Botian Shi*

Key words: 大型语言模型, 复杂推理, 知识图谱, 可解释性, KG-TRACES

TL;DR: KG-TRACES框架通过显式监督LLMs的推理路径和过程，提升了其在复杂推理任务中的表现，并在多个数据集上优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型（LLMs）在复杂推理任务中存在可解释性和可信度不足的问题，限制了其应用。

Method: 提出KG-TRACES框架，通过监督推理路径和过程（包括关系路径预测、三元组级推理路径预测和归因感知推理生成），增强LLMs的推理能力。

Result: 在WebQSP和CWQ等数据集上，KG-TRACES显著优于现有方法（Hits@1和F1均有提升），并展示了在医学等专门领域的可迁移性。

Conclusion: KG-TRACES通过显式监督推理路径和过程，提升了LLMs的推理稳定性和准确性，同时增强了其可解释性和可信度。

Abstract: Large language models (LLMs) have made remarkable strides in various natural
language processing tasks, but their performance on complex reasoning problems
remains hindered by a lack of explainability and trustworthiness. This issue,
often manifesting as hallucinations or unattributable reasoning processes,
limits their applicability in complex reasoning scenarios. To address this, we
propose Knowledge Graph-constrained Trajectory Reasoning Attribution and Chain
Explanation Supervision (KG-TRACES), a novel framework that enhances the
reasoning ability of LLMs through explicit supervision over reasoning paths and
processes. KG-TRACES jointly supervises the model to: (1) predict symbolic
relation paths, (2) predict full triple-level reasoning paths, and (3) generate
attribution-aware reasoning processes grounded in the reasoning paths. At
inference phase, the model adapts to both KG-available and KG-unavailable
scenarios, retrieving reasoning paths from a KG when possible or predicting
plausible reasoning paths with only intrinsic knowledge when not. This design
enables the model to reason in an explainable and source-attributable pattern.
Through extensive experiments on complex reasoning tasks, we demonstrate that
KG-TRACES significantly outperforms existing SOTA: it improves Hits@1 by 1.6%
and F1 by 4.7% on WebQSP, and achieves improvements of 4.8% in Hits@1 and 2.1%
in F1 on CWQ. Moreover, we show its transferability to specialized domains such
as medicine. By visualizing the intermediate steps of reasoning processes, we
further show that the explicit supervision introduced by KG-TRACES leads to
more stable and goal-directed reasoning processes, aligning closely with
correct answers. Code is available at https://github.com/Edaizi/KG-TRACES.

</details>


### [98] [Research Borderlands: Analysing Writing Across Research Cultures](https://arxiv.org/abs/2506.00784)
*Shaily Bhatt,Tal August,Maria Antoniak*

Key words: 文化能力,语言技术,人类中心,LLMs,研究文化

TL;DR: 本文通过以人为中心的方法，发现并衡量语言文化规范及LLMs的文化能力，聚焦研究文化和写作任务，通过专家访谈建立框架，揭示LLMs的文化同质化问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 提升语言技术的文化能力，但现有研究常忽略社区参与，依赖合成设置和文化的非完美代理。

Method: 通过采访跨学科研究专家，建立结构、风格、修辞和引用规范的框架，并用计算指标量化。

Result: 揭示了人类研究论文中的潜在文化规范，并凸显LLMs的文化能力不足及其写作同质化倾向。

Conclusion: 人本方法能有效衡量人类与LLM生成文本中的文化规范。

Abstract: Improving cultural competence of language technologies is important. However
most recent works rarely engage with the communities they study, and instead
rely on synthetic setups and imperfect proxies of culture. In this work, we
take a human-centered approach to discover and measure language-based cultural
norms, and cultural competence of LLMs. We focus on a single kind of culture,
research cultures, and a single task, adapting writing across research
cultures. Through a set of interviews with interdisciplinary researchers, who
are experts at moving between cultures, we create a framework of structural,
stylistic, rhetorical, and citational norms that vary across research cultures.
We operationalise these features with a suite of computational metrics and use
them for (a) surfacing latent cultural norms in human-written research papers
at scale; and (b) highlighting the lack of cultural competence of LLMs, and
their tendency to homogenise writing. Overall, our work illustrates the
efficacy of a human-centered approach to measuring cultural norms in
human-written and LLM-generated texts.

</details>


### [99] [RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.00789)
*Yixiao Zeng,Tianyu Cao,Danqing Wang,Xinran Zhao,Zimeng Qiu,Morteza Ziyadi,Tongshuang Wu,Lei Li*

Key words: RAG, 鲁棒性评估, 知识图谱, 多跳查询, 时间敏感文档

TL;DR: RARE框架引入了一种统一的评估标准和大规模基准测试，用于测试RAG系统在真实世界噪声、冲突和快速变化事实下的表现，发现RAG系统对扰动表现出显著脆弱性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有评估很少测试RAG系统如何应对真实世界噪声、内部与外部检索内容的冲突或快速变化的事实。

Method: 提出了RARE框架，包括知识图谱驱动的合成管道(RARE-Get)生成问题集，构建数据集(RARE-Set)，并形式化检索条件鲁棒性指标(RARE-Met)。

Result: RAG系统对扰动表现出显著脆弱性，文档鲁棒性是其最薄弱环节，且多跳查询的鲁棒性普遍低于单跳查询。

Conclusion: RARE提供了一个全面评估RAG系统鲁棒性的框架，揭示了现有系统在真实场景中的局限性。

Abstract: Retrieval-Augmented Generation (RAG) enhances recency and factuality in
answers. However, existing evaluations rarely test how well these systems cope
with real-world noise, conflicting between internal and external retrieved
contexts, or fast-changing facts. We introduce Retrieval-Aware Robustness
Evaluation (RARE), a unified framework and large-scale benchmark that jointly
stress-tests query and document perturbations over dynamic, time-sensitive
corpora. One of the central features of RARE is a knowledge-graph-driven
synthesis pipeline (RARE-Get) that automatically extracts single and multi-hop
relations from the customized corpus and generates multi-level question sets
without manual intervention. Leveraging this pipeline, we construct a dataset
(RARE-Set) spanning 400 expert-level time-sensitive finance, economics, and
policy documents and 48,322 questions whose distribution evolves as the
underlying sources change. To quantify resilience, we formalize
retrieval-conditioned robustness metrics (RARE-Met) that capture a model's
ability to remain correct or recover when queries, documents, or real-world
retrieval results are systematically altered. Our results show that RAG systems
exhibit surprising vulnerability to perturbations, with document robustness
consistently being the weakest point regardless of generator size or
architecture. RAG systems consistently show lower robustness on multi-hop
queries than single-hop queries across all domains.

</details>


### [100] [Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering](https://arxiv.org/abs/2506.00806)
*Songtao Jiang,Chenyi Zhou,Yan Zhang,Yeying Jin,Zuozhu Liu*

Key words: MLLMs, VQA, 双过程理论, FOCUS, 视觉提示

TL;DR: 提出了FOCUS方法，动态适应问题复杂性，提升多模态大型语言模型（MLLMs）在视觉问答中的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前方法对所有视觉问题进行对象标注，导致性能下降，缺乏对关键视觉元素的关注。

Method: FOCUS结合直觉判断与深思熟虑分析，根据问题复杂度动态调整策略。

Result: 在四个基准测试中，FOCUS显著提升了MLLMs性能。

Conclusion: 结合多样化认知策略和精细化视觉信息对性能至关重要。

Abstract: Multimodal large language models (MLLMs) still struggle with complex
reasoning tasks in Visual Question Answering (VQA). While current methods have
advanced by incorporating visual prompts, our study uncovers critical
limitations: these approaches indiscriminately annotate all detected objects
for every visual question, generating excessive visual markers that degrade
task performance. This issue stems primarily from a lack of focus on key visual
elements, raising two important questions: Are all objects equally important,
and do all questions require visual prompts? Motivated by Dual Process Theory,
which distinguishes between instinctive and deliberate cognitive modes in human
reasoning, we propose FOCUS, a plug-and-play approach that dynamically adapts
to the complexity of questions, combining fast intuitive judgments with
deliberate analytical reasoning to enhance the vision-language reasoning
capability of the MLLM. For straightforward questions, FOCUS supports efficient
zero-shot reasoning. For more complex tasks, it employs the conceptualizing
before observation strategy to highlight critical elements. Extensive
experiments on four benchmarks, ScienceQA, TextQA, VizWiz, and MME, demonstrate
that FOCUS consistently improves the performance of both open-source and
black-box MLLMs, achieving significant gains across all datasets. Ablation
studies further validate the importance of combining diverse cognitive
strategies with refined visual information for superior performance. Code will
be released.

</details>


### [101] [GuessBench: Sensemaking Multimodal Creativity in the Wild](https://arxiv.org/abs/2506.00814)
*Zifeng Zhu,Shangbin Feng,Herun Wan,Ningnan Wang,Minnan Luo,Yulia Tsvetkov*

Key words: 视觉语言模型, 创造力建模, Minecraft, 基准测试, 多样性

TL;DR: GuessBench是一个评估视觉语言模型（VLMs）对人类创造力建模能力的新基准，基于Minecraft多人游戏中的自然语言提示，展示了其在创造力建模上的挑战性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究VLMs如何建模人类广泛、嘈杂和多元化的创造力，尤其是在开放环境中理解不完整提示的能力。

Method: 从Minecraft游戏中收集1500张图像并设计2000个问题，测试6种VLMs和5种增强推理方法。

Result: 最先进的GPT-4o在34%的情况下错误，开源与API模型性能差距显著（13.87% vs. 53.93%）。微调推理痕迹可提升视觉任务性能15.36%。

Conclusion: GuessBench揭示了VLMs在创造力建模上的挑战，特别是在文化背景和低资源语言下的表现不佳。

Abstract: We propose GuessBench, a novel benchmark that evaluates Vision Language
Models (VLMs) on modeling the pervasive, noisy, and pluralistic human
creativity. GuessBench sources data from "Guess the Build", an online
multiplayer Minecraft minigame where one player constructs a Minecraft build
given a concept (e.g. caterpillar) and others try to guess it with natural
language hints, presenting a pristine testbed for sensemaking creativity in the
wild with VLMs acting as guessers. We curate 1500 images from the actual
gameplay and design 2000 problems spanning static and dynamic image settings,
natural language hints of varying completeness, and more. Extensive experiments
with six open/API VLMs and five reasoning enhancement approaches demonstrate
that GuessBench presents a uniquely challenging task in creativity modeling:
even the start-of-the-art GPT-4o is incorrect on 34% of instances, while we
observe a huge performance gap (13.87% vs. 53.93% on average) between open and
API models. When used as a resource to improve VLMs, fine-tuning on the
reasoning traces for GuessBench problems improves visual perception tasks by
15.36% on average. Further analysis reveals that VLM performance in creativity
sensemaking correlates with the frequency of the concept in training data,
while the accuracy drops sharply for concepts in underrepresented cultural
contexts and low-resource languages.

</details>


### [102] [From Plain Text to Poetic Form: Generating Metrically-Constrained Sanskrit Verses](https://arxiv.org/abs/2506.00815)
*Manoj Balaji Jagadeeshan,Samarth Bhatia,Pretam Ray,Harshul Raj Surana,Akhil Rajeev P,Priya Mishra,Annarao Kulkarni,Ganesh Ramakrishnan,Prathosh AP,Pawan Goyal*

Key words: LLMs, 梵语, 诗歌生成, 约束解码, 指令微调

TL;DR: 大型语言模型（LLMs）在诗歌生成方面取得进展，但主要集中在高资源语言。本文研究如何将其用于低资源、形态丰富的梵语，通过数据集和生成模型评估解码策略和微调方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨LLMs在低资源语言（如梵语）中的适应性，特别是如何生成严格符合古典韵律的诗歌。

Method: 引入数据集将英文散文翻译为梵语诗，评估开源和专有生成模型，探索约束解码和指令微调策略。

Result: 约束解码在语法有效性上达99%准确率，指令微调在语义和风格上表现更佳，但韵律精度略有降低。

Conclusion: LLMs适用于低资源语言的诗歌生成，约束解码和指令微调各有优势，需平衡需求选择。

Abstract: Recent advances in large language models (LLMs) have significantly improved
natural language generation, including creative tasks like poetry composition.
However, most progress remains concentrated in high-resource languages. This
raises an important question: Can LLMs be adapted for structured poetic
generation in a low-resource, morphologically rich language such as Sanskrit?
In this work, we introduce a dataset designed for translating English prose
into structured Sanskrit verse, with strict adherence to classical metrical
patterns, particularly the Anushtub meter. We evaluate a range of generative
models-both open-source and proprietary-under multiple settings. Specifically,
we explore constrained decoding strategies and instruction-based fine-tuning
tailored to metrical and semantic fidelity. Our decoding approach achieves over
99% accuracy in producing syntactically valid poetic forms, substantially
outperforming general-purpose models in meter conformity. Meanwhile,
instruction-tuned variants show improved alignment with source meaning and
poetic style, as supported by human assessments, albeit with marginal
trade-offs in metrical precision.

</details>


### [103] [One for All: Update Parameterized Knowledge Across Multiple Models](https://arxiv.org/abs/2506.00817)
*Weitao Ma,Xiyuan Du,Xiaocheng Feng,Lei Huang,Yichong Huang,Huiyi Zhang,Xiaoliang Yang,Baohang Li,Xiachong Feng,Ting Liu,Bing Qin*

Key words: 大型语言模型,知识编辑,模型集成,动态权重,集成增强

TL;DR: OnceEdit是一种基于集成的知识编辑方法，通过动态权重和集成增强机制，高效更新多个大型语言模型的知识。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型（LLM）难以保持知识更新，导致错误和幻觉。现有的知识编辑方法主要针对单一模型，难以高效更新多个模型或适配新模型。

Method: 提出OnceEdit，采用插件模型作为编辑模块，引入动态权重机制和集成增强机制，优化多模型知识编辑的效果和效率。

Result: 实验表明，OnceEdit在多个LLM上的表现优于现有方法，实现了更高的编辑效率和稳定性。

Conclusion: OnceEdit在知识编辑领域表现出色，适用于多模型场景，具备良好的适应性和稳定性。

Abstract: Large language models (LLMs) encode vast world knowledge but struggle to stay
up-to-date, often leading to errors and hallucinations. Knowledge editing
offers an efficient alternative to retraining, enabling targeted modifications
by updating specific model parameters. However, existing methods primarily
focus on individual models, posing challenges in efficiently updating multiple
models and adapting to new models. To address this, we propose OnceEdit, a
novel ensemble-based approach that employs a plug-in model as the editing
module, enabling stable knowledge updates across multiple models. Building on
the model ensemble, OnceEdit introduces two key mechanisms to enhance its
effectiveness. First, we introduce a dynamic weight mechanism through a \weight
token for distinguishing between edit-related and non-edit-related instances,
ensuring the appropriate utilization of knowledge from integrated models.
Second, we incorporate an ensemble enhancement mechanism to mitigate the
excessive reliance on the central model inherent in the model ensemble
technique, making it more suitable for knowledge editing. Extensive experiments
on diverse LLMs demonstrate that OnceEdit consistently outperforms existing
methods while achieving superior editing efficiency. Further analysis confirms
its adaptability and stability in multi-model editing scenarios. Our code will
be available.

</details>


### [104] [Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in LLMs Across Logical Transformations and Question Answering Tasks](https://arxiv.org/abs/2506.00823)
*Yuntai Bao,Xuhong Zhang,Tianyu Du,Xinkui Zhao,Zhengwen Feng,Hao Peng,Jianwei Yin*

Key words: 大型语言模型, 真实性方向, 逻辑否定, 探测技术, 泛化能力, 选择性问答

TL;DR: 论文研究了大型语言模型（LLMs）中‘真实性方向’的普遍性、识别方法和泛化能力，并探讨了其在选择性问答中的实际应用。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管LLMs在广泛数据集上训练并包含丰富的知识，但其输出常包含自信陈述的错误。前人研究提出‘真实性方向’可以可靠分类真实性，但尚存多个未解问题。

Method: 探讨了LLMs是否普遍具有一致性真实性方向，是否需要复杂探测技术识别真实性方向，以及其在多样化上下文中的泛化能力。利用了逻辑否定、问答任务、上下文学习和外部知识源进行实验。

Result: 发现并非所有LLMs均有一致真实性方向，较强大模型在此方面表现更好，尤其是在逻辑否定中；真实性探测方法可有效泛化至多种任务；实用性研究表明其能提升用户对LLM输出的信任。

Conclusion: 研究推进了对真实性方向的理解，并揭示了LLMs内部信念表征的新视角。

Abstract: Large language models (LLMs) are trained on extensive datasets that
encapsulate substantial world knowledge. However, their outputs often include
confidently stated inaccuracies. Earlier works suggest that LLMs encode
truthfulness as a distinct linear feature, termed the "truth direction", which
can classify truthfulness reliably. We address several open questions about the
truth direction: (i) whether LLMs universally exhibit consistent truth
directions; (ii) whether sophisticated probing techniques are necessary to
identify truth directions; and (iii) how the truth direction generalizes across
diverse contexts. Our findings reveal that not all LLMs exhibit consistent
truth directions, with stronger representations observed in more capable
models, particularly in the context of logical negation. Additionally, we
demonstrate that truthfulness probes trained on declarative atomic statements
can generalize effectively to logical transformations, question-answering
tasks, in-context learning, and external knowledge sources. Finally, we explore
the practical application of truthfulness probes in selective
question-answering, illustrating their potential to improve user trust in LLM
outputs. These results advance our understanding of truth directions and
provide new insights into the internal representations of LLM beliefs. Our code
is public at https://github.com/colored-dye/truthfulness_probe_generalization

</details>


### [105] [HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs](https://arxiv.org/abs/2506.00826)
*Yongkang Xiao,Rui Zhang*

Key words: 多模态知识图谱, 知识图谱补全, 生成式模型, HERGC

TL;DR: 提出了名为HERGC的新型框架，通过多模态信息融合和生成式预测来解决多模态知识图谱补全问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法在封闭世界假设下仅利用知识图谱内部信息，限制了推理能力，而生成式方法潜力未充分探索。

Method: HERGC框架结合异构专家表征检索器和生成式LLM预测器，融合多模态信息并生成候选答案。

Result: 在三个标准MMKG基准测试中表现优异，达到最先进水平。

Conclusion: HERGC有效解决了MMKGC问题，展示了生成式方法的潜力。

Abstract: Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs)
by incorporating diverse modalities such as images and text. Multi-modal
knowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals
to infer missing facts, thereby mitigating the intrinsic incompleteness of
MMKGs. Existing MMKGC methods typically leverage only the information contained
in the MMKGs under the closed-world assumption and adopt discriminative
training objectives, which limits their reasoning capacity during completion.
Recent generative completion approaches powered by advanced large language
models (LLMs) have shown strong reasoning abilities in unimodal knowledge graph
completion, but their potential in MMKGC remains largely unexplored. To bridge
this gap, we propose HERGC, a Heterogeneous Experts Representation and
Generative Completion framework for MMKGs. HERGC first deploys a Heterogeneous
Experts Representation Retriever that enriches and fuses multimodal information
and retrieves a compact candidate set for each incomplete triple. It then uses
a Generative LLM Predictor fine-tuned on minimal instruction data to accurately
identify the correct answer from these candidates. Extensive experiments on
three standard MMKG benchmarks demonstrate HERGC's effectiveness and
robustness, achieving state-of-the-art performance.

</details>


### [106] [COMPKE: Complex Question Answering under Knowledge Editing](https://arxiv.org/abs/2506.00829)
*Keyuan Cheng,Zijian Kan,Zhixian He,Zhuoran Zhang,Muhammad Asif Ali,Ke Xu,Lijie Hu,Di Wang*

Key words: 知识编辑、基准测试、复杂推理、多跳问答

TL;DR: COMPKE是一个新的基准测试，用于评估知识编辑方法在复杂现实场景中的表现，包含11,924个复杂问题，揭示了不同方法和模型间的显著差异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基准测试无法有效评估知识编辑方法在复杂推理现实场景中的表现。

Method: 引入COMPKE基准测试，包含11,924个复杂问题，并评估四种知识编辑方法在不同模型上的表现。

Result: 不同方法在不同模型上表现差异显著，例如MeLLo在GPT-4O-MINI上准确率为39.47，而在QWEN2.5-3B上仅为3.83。

Conclusion: COMPKE填补了现有基准测试的不足，揭示了知识编辑方法的局限性，尤其是在复杂推理场景中。

Abstract: Knowledge Editing, which efficiently modifies the knowledge in large language
models, has gathered great attention. Current benchmarks primarily use
multi-hop question answering to assess and analyze newly injected or updated
knowledge. However, we argue that these benchmarks fail to effectively evaluate
how well the updated models apply this knowledge in real-life scenarios,
particularly when questions require complex reasoning, involving one-to-many
relationships or multi-step logical intersections. To fill in this gap, we
introduce a new benchmark, COMPKE: Complex Question Answering under Knowledge
Editing, which includes 11,924 complex questions that reflect real-life
situations. We conduct an extensive evaluation of four knowledge editing
methods on COMPKE, revealing that their effectiveness varies notably across
different models. For instance, MeLLo attains an accuracy of 39.47 on
GPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further
investigate the underlying causes of these disparities from both methodological
and model-specific perspectives. The datasets are available at
https://github.com/kzjkzj666/CompKE.

</details>


### [107] [Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience](https://arxiv.org/abs/2506.00842)
*Jiawei Gu,Ziting Xian,Yuanzhen Xie,Ye Liu,Enjie Liu,Ruichao Zhong,Mochi Gao,Yunzhi Tan,Bo Hu,Zang Li*

Key words: 大型语言模型,结构化数据,CoRE框架,上下文学习,蒙特卡洛树搜索

TL;DR: CoRE框架通过对比检索增强生成技术，提升了大型语言模型在结构化数据任务（如Text-to-SQL和TableQA）上的性能，平均提升3.44%和4.24%，最高可达17.2%。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型在结构化数据（如表格和数据库）上表现不佳，主要因为预训练中对结构化数据接触不足，以及文本到结构的转换机制过于刚性。

Method: 引入CoRE框架，通过对比上下文学习和经验记忆表示模拟人类知识迁移，并利用蒙特卡洛树搜索扩展训练数据。

Result: 实验证明CoRE在Text-to-SQL和TableQA任务中显著提升性能，经验记忆使训练数据扩大了8-9倍。

Conclusion: CoRE为大型语言模型提供了无需训练、持续学习的方法，有效提升了其在结构化数据任务中的表现。

Abstract: Large language models (LLMs) achieve strong performance on plain text tasks
but underperform on structured data like tables and databases. Potential
challenges arise from their underexposure during pre-training and rigid
text-to-structure transfer mechanisms. Unlike humans who seamlessly apply
learned patterns across data modalities, LLMs struggle to infer implicit
relationships embedded in tabular formats, especially in the absence of
explicit structural guidance. To bridge this cognitive gap, we introduce
Contrastive Retrieval-Augmented Generation on Experience (CoRE), a framework
that builds experience memory representations and enhances generalization
through contrastive In-Context Learning (ICL) to simulate human-like knowledge
transfer. Experiments on Text-to-SQL and TableQA show CoRE significantly
improves performance, achieving average gains of 3.44% and 4.24%, with up to
17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated
Experience Memory expands training data 8-9x, enhancing diversity and domain
coverage. This training-free and continual method propels LLMs toward
structured knowledge expertise.

</details>


### [108] [EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG](https://arxiv.org/abs/2506.00854)
*Jacky Tai-Yu Lu,Jung Chiang,Chi-Sheng Chen,Anna Nai-Yun Tung,Hsiang Wei Hu,Yuan Chiao Cheng*

Key words: EEG-to-text, Chinese, cross-modal decoding, brain-computer interface, NICE-EEG, MiniLM

TL;DR: EEG2TEXT-CN是一个针对汉语的开源词汇EEG到文本生成框架，结合生物启发的EEG编码器和预训练语言模型，通过掩码预训练和对比学习实现脑信号与自然语言的映射。在中文EEG数据集上展示了可行性，但句法流畅性仍需改进。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索汉语环境下从脑电信号（EEG）到文本的非语音跨模态解码，为未来的认知语言接口奠定基础。

Method: 使用NICE-EEG编码器和MiniLM语言模型，通过掩码预训练和对比学习对齐多通道脑信号与语言表示，并在中文EEG数据集上训练。

Result: 在1500多个训练验证句子和300个测试样本上，BLEU-1得分达到6.38%，证明了非语音跨模态解码的可行性。

Conclusion: EEG2TEXT-CN为汉语脑机文本解码研究开辟了新方向，但句法流畅性仍需优化。

Abstract: We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one
of the earliest open-vocabulary EEG-to-text generation frameworks tailored for
Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact
pretrained language model (MiniLM), our architecture aligns multichannel brain
signals with natural language representations via masked pretraining and
contrastive learning. Using a subset of the ChineseEEG dataset, where each
sentence contains approximately ten Chinese characters aligned with 128-channel
EEG recorded at 256 Hz, we segment EEG into per-character embeddings and
predict full sentences in a zero-shot setting. The decoder is trained with
teacher forcing and padding masks to accommodate variable-length sequences.
Evaluation on over 1,500 training-validation sentences and 300 held-out test
samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\%.
While syntactic fluency remains a challenge, our findings demonstrate the
feasibility of non-phonetic, cross-modal language decoding from EEG. This work
opens a new direction in multilingual brain-to-text research and lays the
foundation for future cognitive-language interfaces in Chinese.

</details>


### [109] [How Bidirectionality Helps Language Models Learn Better via Dynamic Bottleneck Estimation](https://arxiv.org/abs/2506.00859)
*Md Kowsher,Nusrat Jahan Prottasha,Shiyun Xu,Shetu Mohanto,Chen Chen,Niloofar Yousefi,Ozlem Garibay*

Key words: 双向语言模型, 信息瓶颈, 互信息, FlowNIB, 表现复杂度

TL;DR: 通过信息瓶颈（IB）原则，研究发现双向语言模型因其更高的互信息保留和有效维度表现优于单向模型，并提出了FlowNIB方法解决IB方法的局限。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨双向语言模型优于单向模型的理论原因，特别是基于信息瓶颈原则的解释。

Method: 提出FlowNIB方法，动态估计训练中的互信息，解决传统IB方法的计算难题；建立衡量表现复杂度的框架，证明双向模型的优越性。

Result: 双向模型保留更多互信息，表现出更高的有效维度，实验验证了信息编码和压缩的过程。

Conclusion: 工作为双向架构的高效性提供了理论解释，并提供了分析深度语言模型中信息流的实用工具。

Abstract: Bidirectional language models have better context understanding and perform
better than unidirectional models on natural language understanding tasks, yet
the theoretical reasons behind this advantage remain unclear. In this work, we
investigate this disparity through the lens of the Information Bottleneck (IB)
principle, which formalizes a trade-off between compressing input information
and preserving task-relevant content. We propose FlowNIB, a dynamic and
scalable method for estimating mutual information during training that
addresses key limitations of classical IB approaches, including computational
intractability and fixed trade-off schedules. Theoretically, we show that
bidirectional models retain more mutual information and exhibit higher
effective dimensionality than unidirectional models. To support this, we
present a generalized framework for measuring representational complexity and
prove that bidirectional representations are strictly more informative under
mild conditions. We further validate our findings through extensive experiments
across multiple models and tasks using FlowNIB, revealing how information is
encoded and compressed throughout training. Together, our work provides a
principled explanation for the effectiveness of bidirectional architectures and
introduces a practical tool for analyzing information flow in deep language
models.

</details>


### [110] [L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with Synthetic Annotations using CoTR prompting and Large Language Models](https://arxiv.org/abs/2506.00863)
*Nidhi Kowtal,Raviraj Joshi*

Key words: 情感识别,马拉地语,低资源语言,LLM,GPT-4,BERT

TL;DR: 提出一个高质量的马拉地语情感识别数据集，利用大型语言模型（如GPT-4）合成标注训练数据，验证集和测试集则手动标注，发现通用LLM在低资源语言情感识别任务中表现优于微调BERT。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决低资源语言（如马拉地语）中情感识别任务数据不足的问题。

Method: 采用Chain-of-Translation（CoTR）提示技术，将马拉地语句子翻译为英语后标注情感，并评估GPT-4和Llama3-405B的标注效果，选择GPT-4生成训练数据。

Result: GPT-4预测结果优于微调BERT模型，但BERT模型基于合成标签训练后未超越GPT-4，显示高质量人工标注数据的重要性。

Conclusion: 通用LLM在低资源情感识别任务中表现更优，凸显了高质量标注数据及任务复杂性。

Abstract: Emotion recognition in low-resource languages like Marathi remains
challenging due to limited annotated data. We present L3Cube-MahaEmotions, a
high-quality Marathi emotion recognition dataset with 11 fine-grained emotion
labels. The training data is synthetically annotated using large language
models (LLMs), while the validation and test sets are manually labeled to serve
as a reliable gold-standard benchmark. Building on the MahaSent dataset, we
apply the Chain-of-Translation (CoTR) prompting technique, where Marathi
sentences are translated into English and emotion labeled via a single prompt.
GPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training data
annotation due to superior label quality. We evaluate model performance using
standard metrics and explore label aggregation strategies (e.g., Union,
Intersection). While GPT-4 predictions outperform fine-tuned BERT models,
BERT-based models trained on synthetic labels fail to surpass GPT-4. This
highlights both the importance of high-quality human-labeled data and the
inherent complexity of emotion recognition. An important finding of this work
is that generic LLMs like GPT-4 and Llama3-405B generalize better than
fine-tuned BERT for complex low-resource emotion recognition tasks. The dataset
and model are shared publicly at https://github.com/l3cube-pune/MarathiNLP

</details>


### [111] [What's Missing in Vision-Language Models? Probing Their Struggles with Causal Order Reasoning](https://arxiv.org/abs/2506.00869)
*Zhaotian Weng,Haoxuan Li,Kuan-Hao Huang,Jieyu Zhao*

Key words: 视觉语言模型,因果推理,基准测试,负例微调

TL;DR: 该论文提出了两个新的基准测试VQA-Causal和VCR-Causal，专门用于评估视觉语言模型（VLMs）的因果推理能力。研究发现，尽管VLMs在对象和活动识别上表现优异，但在因果推理任务上表现不佳，且主要原因是训练数据中缺乏因果关系的显式表达。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基准测试无法有效评估VLMs的因果推理能力，因为这些问题常被对象识别和活动识别作为捷径解答。

Method: 通过设计两个新基准测试VQA-Causal和VCR-Causal，并探索针对负例的微调策略。

Result: VLMs在因果推理任务中表现不佳，仅略优于随机猜测。

Conclusion: 研究揭示了当前VLMs在因果推理上的关键不足，并提出了改进方向。

Abstract: Despite the impressive performance of vision-language models (VLMs) on
downstream tasks, their ability to understand and reason about causal
relationships in visual inputs remains unclear. Robust causal reasoning is
fundamental to solving complex high-level reasoning tasks, yet existing
benchmarks often include a mixture of reasoning questions, and VLMs can
frequently exploit object recognition and activity identification as shortcuts
to arrive at the correct answers, making it challenging to truly assess their
causal reasoning abilities. To bridge this gap, we introduce VQA-Causal and
VCR-Causal, two new benchmarks specifically designed to isolate and rigorously
evaluate VLMs' causal reasoning abilities. Our findings reveal that while VLMs
excel in object and activity recognition, they perform poorly on causal
reasoning tasks, often only marginally surpassing random guessing. Further
analysis suggests that this limitation stems from a severe lack of causal
expressions in widely used training datasets, where causal relationships are
rarely explicitly conveyed. We additionally explore fine-tuning strategies with
hard negative cases, showing that targeted fine-tuning can improve model's
causal reasoning while maintaining generalization and downstream performance.
Our study highlights a key gap in current VLMs and lays the groundwork for
future work on causal understanding.

</details>


### [112] [CC-Tuning: A Cross-Lingual Connection Mechanism for Improving Joint Multilingual Supervised Fine-Tuning](https://arxiv.org/abs/2506.00875)
*Yangfan Ye,Xiaocheng Feng,Zekun Yuan,Xiachong Feng,Libo Qin,Lei Huang,Weitao Ma,Yichong Huang,Zhirui Zhang,Yunfei Lu,Xiaohui Yan,Duyu Tang,Dandan Tu,Bing Qin*

Key words: 多语言模型, 跨语言交互, 微调, 潜在层面, CC-Tuning

TL;DR: CC-Tuning是一种新的多语言微调范式，通过在潜在层面显式建立跨语言连接机制，显著提升了大型语言模型的多语言能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前大型语言模型多语言能力不平衡，主要基于英语语料库训练。现有方法（如数据增强或蒸馏）通常忽略潜在层面的跨语言交互。

Method: CC-Tuning在训练中融合英语和非英语输入的前馈激活，并通过可训练的决策选择器识别有益激活。在推理中，使用变换矩阵模拟跨语言连接。

Result: 在6个基准测试（覆盖22种语言）中，CC-Tuning优于传统SFT，展示了潜在层面跨语言交互的有效性。

Conclusion: CC-Tuning提供了一种高效的潜在层面替代数据增强方法，提升了多语言性能。

Abstract: Current large language models (LLMs) often exhibit imbalanced multilingual
capabilities due to their English-centric training corpora. To address this,
existing fine-tuning approaches operating at the data-level (e.g., through data
augmentation or distillation) typically introduce implicit cross-lingual
alignment, overlooking the potential for more profound, latent-level
cross-lingual interactions. In this work, we propose CC-Tuning, a novel
multilingual fine-tuning paradigm that explicitly establishes a cross-lingual
connection mechanism at the latent level. During training, CC-Tuning fuses the
feed forward activations from both English and non-English inputs, enabling the
model to benefit from both linguistic resources. This process is facilitated
with a trainable Decision Maker that identifies beneficial activations.
Furthermore, during inference, a Transform Matrix is utilized to simulate the
cross-lingual connection under monolingual setting through representation
transformation. Our experiments on six benchmarks covering 22 languages show
that CC-Tuning outperforms vanilla SFT and offers a strong latent-level
alternative to data-level augmentation methods. Further analysis also
highlights the practicality of CC-Tuning and the potential of latent-level
cross-lingual interactions in advancing the multilingual performance of LLMs.

</details>


### [113] [Not Every Token Needs Forgetting: Selective Unlearning to Limit Change in Utility in Large Language Model Unlearning](https://arxiv.org/abs/2506.00876)
*Yixin Wan,Anil Ramakrishna,Kai-Wei Chang,Volkan Cevher,Rahul Gupta*

Key words: Large Language Model (LLM) unlearning, selective unlearning, utility preservation

TL;DR: Selective Unlearning (SU) 方法通过选择性遗忘与无用信息相关的关键子集，解决了传统方法盲目更新模型参数的问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统遗忘方法不加区别地遗忘目标文档中的所有标记，包括携带通用知识的常见标记。本文旨在仅遗忘与无用信息相关的关键标记。

Method: 提出了 Selective Unlearning (SU) 方法，识别遗忘集中与无用信息相关的关键子集，并仅对这些标记进行遗忘操作。

Result: 实验表明，SU 不仅有效实现了目标数据的遗忘，还显著保留了模型在保留集中的实用性。

Conclusion: SU 方法在处理无用信息的遗忘任务中表现出色，同时避免了通用知识的损失。

Abstract: Large Language Model (LLM) unlearning has recently gained significant
attention, driven by the need to remove unwanted information, such as private,
sensitive, or copyrighted content, from LLMs. However, conventional unlearning
approaches indiscriminately update model parameters to forget all tokens in a
target document, including common tokens (e.g., pronouns, prepositions, general
nouns) that carry general knowledge. In this paper, we highlight that not every
token needs forgetting. We propose Selective Unlearning (SU), which identifies
a critical subset of tokens within the forgetting set that is relevant to the
unwanted information, and unlearns only those tokens. Experiments on two
benchmarks and six baseline unlearning algorithms demonstrate that SU not only
achieves effective unlearning on the targeted forget data, but also
significantly preserves the model's utility in the retaining set.

</details>


### [114] [Improve MLLM Benchmark Efficiency through Interview](https://arxiv.org/abs/2506.00883)
*Farong Wen,Yijin Guo,Junying Wang,Jiaohao Xiao,Yingjie Zhou,Chunyi Li,Zicheng Zhang,Guangtao Zhai*

Key words: 多模态大语言模型, 性能评估, 面试策略, 难度标签

TL;DR: 提出MLLM Interview (MITV)策略，通过少量问题快速评估多模态大语言模型（MLLM）的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有MLLM评估方法在大规模数据上耗时耗资源，MITV策略旨在高效获取性能指标。

Method: 构建带难度标签的面试数据集，通过少量问题测试模型性能并逐步探索其极限。

Result: MITV策略在MLLM基准数据集上表现良好，能通过少量问答快速完成模型评估。

Conclusion: MITV策略为MLLM性能评估提供了高效且资源友好的解决方案。

Abstract: The rapid development of Multimodal Large Language Models (MLLM) has led to a
wide range of MLLM applications, and a number of benchmark datasets have sprung
up in order to assess MLLM abilities. However, full-coverage Q&A testing on
large-scale data is resource-intensive and time-consuming. To address this
issue, we propose the MLLM Interview (MITV) strategy, which aims to quickly
obtain MLLM performance metrics by quizzing fewer question. First, First, we
constructed the interview dataset, which was built on an existing MLLM
assessment dataset, by adding difficulty labels based on the performance of
some typical MLLMs in this dataset. Second, we propose an MLLM Interview
strategy, which obtains an initial performance situation of the large model by
quizzing a small number of topics and then continuously tries to test the
model's limits. Through extensive experiments, the result shows that the MITV
strategy proposed in this paper performs well on MLLM benchmark datasets, and
it is able to obtain the model evaluation capability faster through a small
number of questions and answers.

</details>


### [115] [Affordance Benchmark for MLLMs](https://arxiv.org/abs/2506.00893)
*Junying Wang,Wenzhe Li,Yalun Wu,Yingji Liang,Yijin Guo,Chunyi Li,Haodong Duan,Zicheng Zhang,Guangtao Zhai*

Key words: 可供性理论,多模态大语言模型,A4Bench,环境理解,上下文感知

TL;DR: 论文提出了A4Bench基准，评估多模态大语言模型（MLLMs）的感知能力，发现其在环境理解方面存在显著差距，尤其在动态和上下文感知的“转换性可供性”任务中表现不佳。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在探索MLLMs在感知环境可供性方面的能力，这是实现直觉和安全互动的关键，但目前仍未被充分研究。

Method: 通过构建A4Bench基准，分为“构成性可供性”（评估对象固有属性）和“转换性可供性”（评估动态和上下文感知）两大维度，包含2000个问答对，并对17种MLLM进行评估。

Result: 专有模型普遍优于开源模型，但整体表现仍有限，尤其是在转换性可供性任务中；所有模型的性能远低于人类表现（差距显著）。

Conclusion: MLLMs在环境理解方面存在重大不足，需进一步改进以实现更鲁棒和上下文感知的交互。数据集已开源。

Abstract: Affordance theory posits that environments inherently offer action
possibilities that shape perception and behavior. While Multimodal Large
Language Models (MLLMs) excel in vision-language tasks, their ability to
perceive affordance, which is crucial for intuitive and safe interactions,
remains underexplored. To address this, we introduce A4Bench, a novel benchmark
designed to evaluate the affordance perception abilities of MLLMs across two
dimensions: 1) Constitutive Affordance}, assessing understanding of inherent
object properties through 1,282 question-answer pairs spanning nine
sub-disciplines, and 2) Transformative Affordance, probing dynamic and
contextual nuances (e.g., misleading, time-dependent, cultural, or
individual-specific affordance) with 718 challenging question-answer pairs.
Evaluating 17 MLLMs (nine proprietary and eight open-source) against human
performance, we find that proprietary models generally outperform open-source
counterparts, but all exhibit limited capabilities, particularly in
transformative affordance perception. Furthermore, even top-performing models,
such as Gemini-2.0-Pro (18.05% overall exact match accuracy), significantly lag
behind human performance (best: 85.34%, worst: 81.25%). These findings
highlight critical gaps in environmental understanding of MLLMs and provide a
foundation for advancing AI systems toward more robust, context-aware
interactions. The dataset is available in
https://github.com/JunyingWang959/A4Bench/.

</details>


### [116] [SocialEval: Evaluating Social Intelligence of Large Language Models](https://arxiv.org/abs/2506.00900)
*Jinfeng Zhou,Yuxuan Chen,Yihan Shi,Xuanming Zhang,Leqi Lei,Yi Feng,Zexuan Xiong,Miao Yan,Xunzhi Wang,Yaru Cao,Jianing Yin,Shuai Wang,Quanyu Dai,Zhenhua Dong,Hongning Wang,Minlie Huang*

Key words: 社交智能，评估基准，LLMs，人类行为模拟，SocialEval

TL;DR: 本文提出SocialEval，一个基于脚本的双语SI基准，通过整合结果导向和过程导向评估，揭示LLMs在社交智能上与人类的差距。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 评估LLMs的社交智能（SI）及其与人类的差异，因LLMs在模拟人类行为方面展现出潜力。

Method: 使用手动编写的叙述脚本构建SocialEval，以世界树形式驱动情节线，结合结果和过程导向评估。

Result: 实验显示LLMs在SI评估中落后于人类，表现出亲社会性，且倾向积极社交行为，即使阻碍目标达成。

Conclusion: LLMs已形成类似人脑的能力特定功能分区，但在SI上仍需改进。

Abstract: LLMs exhibit promising Social Intelligence (SI) in modeling human behavior,
raising the need to evaluate LLMs' SI and their discrepancy with humans. SI
equips humans with interpersonal abilities to behave wisely in navigating
social interactions to achieve social goals. This presents an operational
evaluation paradigm: outcome-oriented goal achievement evaluation and
process-oriented interpersonal ability evaluation, which existing work fails to
address. To this end, we propose SocialEval, a script-based bilingual SI
benchmark, integrating outcome- and process-oriented evaluation by manually
crafting narrative scripts. Each script is structured as a world tree that
contains plot lines driven by interpersonal ability, providing a comprehensive
view of how LLMs navigate social interactions. Experiments show that LLMs fall
behind humans on both SI evaluations, exhibit prosociality, and prefer more
positive social behaviors, even if they lead to goal failure. Analysis of LLMs'
formed representation space and neuronal activations reveals that LLMs have
developed ability-specific functional partitions akin to the human brain.

</details>


### [117] [Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot Programming Languages](https://arxiv.org/abs/2506.00912)
*Yongdong chi,Hanqing Wang,Zonghan Yang,Jian Yang,Xiao Yan,Yun Chen,Guanhua Chen*

Key words: 文本转SQL, Python, 语义差距, Pi-SQL

TL;DR: Pi-SQL 通过引入Python作为中介桥接自然语言查询与SQL程序，显著提升文本转SQL的准确性和执行效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基于提示的方法在文本与低资源SQL程序之间存在较大语义差距，导致准确性受限。

Method: Pi-SQL 首先生成Python程序，提供细粒度步骤指导，然后基于Python生成SQL程序，并通过候选策略选择最优解。

Result: Pi-SQL 的执行效率评分比表现最佳的基线高出4.55，执行准确性提升高达3.20。

Conclusion: Pi-SQL 通过Python作为中介，有效缩小了语义差距，显著提升了文本转SQL的性能。

Abstract: Text-to-SQL transforms the user queries from natural language to executable
SQL programs, enabling non-experts to interact with complex databases. Existing
prompt-based methods craft meticulous text guidelines and examples to
facilitate SQL generation, but their accuracy is hindered by the large semantic
gap between the texts and the low-resource SQL programs. In this work, we
propose Pi-SQL, which incorporates the high-resource Python program as a pivot
to bridge between the natural language query and SQL program. In particular,
Pi-SQL first generates Python programs that provide fine-grained step-by-step
guidelines in their code blocks or comments, and then produces an SQL program
following the guidance of each Python program.The final SQL program matches the
reference Python program's query results and, through selection from candidates
generated by different strategies, achieves superior execution speed, with a
reward-based valid efficiency score up to 4.55 higher than the best-performing
baseline.Extensive experiments demonstrate the effectiveness of Pi-SQL, which
improves the execution accuracy of the best-performing baseline by up to 3.20.

</details>


### [118] [How do Transformer Embeddings Represent Compositions? A Functional Analysis](https://arxiv.org/abs/2506.00914)
*Aishik Nagar,Ishaan Singh Rawal,Mansi Dhanania,Cheston Tan*

Key words: 组合性, Transformer模型, 嵌入模型, 岭回归, BERT

TL;DR: 研究了Mistral、OpenAI Large、Google嵌入模型和BERT的组合性，发现线性模型（如岭回归）在组合性表现上最佳，且大多数嵌入模型具有高组合性，而BERT表现较差。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨Transformer模型在组合词表示中的组合性，以理解其在推理和泛化中的表现。

Method: 评估六种组合性模型（加法、乘法、扩张、回归等），使用岭回归作为基准，并通过合成数据集验证结果。

Result: 岭回归模型在组合性表现上最优，大多数嵌入模型表现良好，但BERT较差。向量加法模型表现接近最优。

Conclusion: 研究提供了对组合性的全面分析，发现嵌入模型普遍具有高组合性，而BERT表现不足。

Abstract: Compositionality is a key aspect of human intelligence, essential for
reasoning and generalization. While transformer-based models have become the de
facto standard for many language modeling tasks, little is known about how they
represent compound words, and whether these representations are compositional.
In this study, we test compositionality in Mistral, OpenAI Large, and Google
embedding models, and compare them with BERT. First, we evaluate
compositionality in the representations by examining six diverse models of
compositionality (addition, multiplication, dilation, regression, etc.). We
find that ridge regression, albeit linear, best accounts for compositionality.
Surprisingly, we find that the classic vector addition model performs almost as
well as any other model. Next, we verify that most embedding models are highly
compositional, while BERT shows much poorer compositionality. We verify and
visualize our findings with a synthetic dataset consisting of fully transparent
adjective-noun compositions. Overall, we present a thorough investigation of
compositionality.

</details>


### [119] [anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding](https://arxiv.org/abs/2506.00942)
*Haitao Li,Ziyu Li,Yiheng Mao,Ziyi Liu,Zhoujian Sun,Zhengxing Huang*

Key words: 多模态大语言模型, 心电图分析, 动态输入, 数据集构建

TL;DR: 本文提出了一种支持多种任务和灵活心电图（ECG）输入的多模态大语言模型（MLLM），并介绍了anyECG数据集及其应用。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的心电图分析多模态大语言模型多局限于单次12导联短时程输入，未能充分发挥潜力，因此需开发更灵活的模型。

Method: 构建anyECG数据集，涵盖多样任务；提出anyECG-chat模型，支持动态长度和多ECG输入，并采用三阶段课程训练方法。

Result: anyECG-chat模型成功支持多种实际应用场景，如报告生成、异常波形定位及多ECG对比分析。

Conclusion: anyECG-chat模型扩展了ECG分析的适用性，适用于家庭和临床环境，展现出广泛的应用潜力。

Abstract: The advent of multimodal large language models (MLLMs) has sparked interest
in their application to electrocardiogram (ECG) analysis. However, existing
ECG-focused MLLMs primarily focus on report generation tasks, often limited to
single 12-lead, short-duration (10s) ECG inputs, thereby underutilizing the
potential of MLLMs. To this end, we aim to develop a MLLM for ECG analysis that
supports a broader range of tasks and more flexible ECG inputs. However,
existing ECG-QA datasets are often monotonous. To address this gap, we first
constructed the anyECG dataset, which encompasses a wide variety of tasks,
including report generation, abnormal waveform localization, and open-ended
question answering. In addition to standard hospital ECGs, we introduced
long-duration reduced-lead ECGs for home environments and multiple ECG
comparison scenarios commonly encountered in clinical practice. Furthermore, we
propose the anyECG-chat model, which supports dynamic-length ECG inputs and
multiple ECG inputs. We trained the model using a three-stage curriculum
training recipe with the anyECG dataset. A comprehensive evaluation was
conducted, demonstrating that anyECG-chat is capable of supporting various
practical application scenarios, including not only common report generation
tasks but also abnormal waveform localization for long-duration reduced-lead
ECGs in home environments and comprehensive comparative analysis of multiple
ECGs.

</details>


### [120] [Leveraging Large Language Models for Sarcastic Speech Annotation in Sarcasm Detection](https://arxiv.org/abs/2506.00955)
*Zhu Li,Yuqing Zhang,Xiyuan Gao,Shekhar Nayak,Matt Coler*

Key words: 讽刺检测、语音分析、大语言模型、数据集生成、协同标注

TL;DR: 论文提出了一种利用大语言模型（LLM）生成讽刺语音数据集的标注流程，并通过人类验证优化标注质量。该方法在公开数据集上验证了效果，并发布了PodSarc数据集，F1分数达73.63%。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 讽刺通过语气和上下文改变含义，但语音中检测讽刺仍因数据稀缺和多模态依赖而受限。

Method: 通过LLM（GPT-4o和LLaMA 3）对讽刺播客进行初始标注，结合人类验证生成数据集，并采用协作门控架构验证检测性能。

Result: 创建的PodSarc数据集在检测任务中达到73.63%的F1分数，展现了作为基准数据集的潜力。

Conclusion: 提出的标注流程和PodSarc数据集为语音讽刺检测研究提供了有效工具和基准。

Abstract: Sarcasm fundamentally alters meaning through tone and context, yet detecting
it in speech remains a challenge due to data scarcity. In addition, existing
detection systems often rely on multimodal data, limiting their applicability
in contexts where only speech is available. To address this, we propose an
annotation pipeline that leverages large language models (LLMs) to generate a
sarcasm dataset. Using a publicly available sarcasm-focused podcast, we employ
GPT-4o and LLaMA 3 for initial sarcasm annotations, followed by human
verification to resolve disagreements. We validate this approach by comparing
annotation quality and detection performance on a publicly available sarcasm
dataset using a collaborative gating architecture. Finally, we introduce
PodSarc, a large-scale sarcastic speech dataset created through this pipeline.
The detection model achieves a 73.63% F1 score, demonstrating the dataset's
potential as a benchmark for sarcasm detection research.

</details>


### [121] [From Objectives to Questions: A Planning-based Framework for Educational Mathematical Question Generation](https://arxiv.org/abs/2506.00963)
*Cheng Cheng,Zhenya Huang,Guanhao Zhao,Yuxiang Guo,Xin Lin,Jinze Wu,Xin Li,Shijin Wang*

Key words: 数学问题生成、教育目标、蒙特卡洛树搜索、LLM、EQPR

TL;DR: 提出EQPR方法，结合蒙特卡洛树搜索和大型语言模型生成符合多维教育目标的数学问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统方法忽视教育目标，仅生成简单问题，需改进以满足复杂需求。

Method: 基于EduMath数据集，开发EQGEVAL评估框架，采用EQPR（规划-评估-优化）方法，结合蒙特卡洛树搜索与LLM。

Result: EQPR在多维教育目标问题生成上表现显著提升。

Conclusion: EQPR方法能有效生成符合教育目标的问题，适用于复杂需求。

Abstract: Automatically generating high-quality mathematical problems that align with
educational objectives is a crucial task in NLP-based educational technology.
Traditional generation methods focus primarily on textual quality, but they
often overlook educational objectives. Moreover, these methods address only
single-dimensional, simple question generation, failing to meet complex,
multifaceted educational requirements. To address these challenges, we
constructed and annotated EduMath, a dataset of 16k mathematical questions with
multi-dimensional educational objectives. Based on this dataset, we developed
EQGEVAL, which incorporates three evaluation dimensions and is designed to
assess the ability of models to generate educational questions. Drawing
inspiration from teachers' problem design processes, we propose the Educational
Question Planning with self-Reflection (EQPR) method for educational
mathematical question generation, following a "plan-evaluate-optimize"
approach. Specifically, by combining planning algorithm based on Monte Carlo
Tree Search with the generative capabilities of Large Language Models, we
continuously optimize questions through iterative feedback. This
self-optimization mechanism ensures that the generated questions both fit the
educational context and strategically achieve specific basic educational
objectives. Through extensive experiments based on EQGEVAL, we have
demonstrated that EQPR achieves significant improvements in generating
questions that meet multi-dimensional educational objectives.

</details>


### [122] [ACCESS DENIED INC: The First Benchmark Environment for Sensitivity Awareness](https://arxiv.org/abs/2506.00964)
*Dren Fazlija,Arkadij Orlov,Sandipan Sikdar*

Key words: 大语言模型, 敏感性感知, 数据隐私, 企业数据管理, 基准测试

TL;DR: 论文提出了一种名为敏感性感知（SA）的概念，帮助大语言模型（LLMs）在企业数据管理中遵守访问权限规则，并通过基准环境测试其效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 企业环境中使用LLMs处理敏感信息时，简单的用户权限过滤可能带来性能和隐私问题，因此需要更精细的敏感性管理方法。

Method: 提出了敏感性感知（SA）概念，并开发了ACCESS DENIED INC基准环境，用于评估模型在权限限制下的表现。

Result: 实验表明，模型在处理未授权数据请求和合法查询时行为差异显著，SA有效提升了隐私保护能力。

Conclusion: 该研究为敏感性感知模型的基准测试奠定了基础，并为构建隐私友好的企业AI系统提供了参考。

Abstract: Large language models (LLMs) are increasingly becoming valuable to corporate
data management due to their ability to process text from various document
formats and facilitate user interactions through natural language queries.
However, LLMs must consider the sensitivity of information when communicating
with employees, especially given access restrictions. Simple filtering based on
user clearance levels can pose both performance and privacy challenges. To
address this, we propose the concept of sensitivity awareness (SA), which
enables LLMs to adhere to predefined access rights rules. In addition, we
developed a benchmarking environment called ACCESS DENIED INC to evaluate SA.
Our experimental findings reveal significant variations in model behavior,
particularly in managing unauthorized data requests while effectively
addressing legitimate queries. This work establishes a foundation for
benchmarking sensitivity-aware language models and provides insights to enhance
privacy-centric AI systems in corporate environments.

</details>


### [123] [XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content](https://arxiv.org/abs/2506.00973)
*Vadivel Abishethvarman,Bhavik Chandna,Pratik Jalan,Usman Naseem*

Key words: LLM安全、极端内容、红队测试、分级评估、防御策略

TL;DR: XGUARD是一个评估框架，用于分级测试LLM生成极端内容的严重性，通过五级危险分类和ASC曲线提供更细粒度的安全分析。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的LLM安全评估过于简单（仅二分类），无法反映真实风险的多样性。XGUARD旨在填补这一空白。

Method: 构建包含3,840条真实红队提示的基准，将模型响应分为5级危险，并引入ASC曲线可视化防御效果。

Result: 评估了6个主流LLM和2种防御策略，揭示了安全性差距及鲁棒性与表达自由之间的权衡。

Conclusion: 分级安全指标对构建可信LLM至关重要，XGUARD为细粒度评估提供了实用工具。

Abstract: Large Language Models (LLMs) can generate content spanning ideological
rhetoric to explicit instructions for violence. However, existing safety
evaluations often rely on simplistic binary labels (safe and unsafe),
overlooking the nuanced spectrum of risk these outputs pose. To address this,
we present XGUARD, a benchmark and evaluation framework designed to assess the
severity of extremist content generated by LLMs. XGUARD includes 3,840 red
teaming prompts sourced from real world data such as social media and news,
covering a broad range of ideologically charged scenarios. Our framework
categorizes model responses into five danger levels (0 to 4), enabling a more
nuanced analysis of both the frequency and severity of failures. We introduce
the interpretable Attack Severity Curve (ASC) to visualize vulnerabilities and
compare defense mechanisms across threat intensities. Using XGUARD, we evaluate
six popular LLMs and two lightweight defense strategies, revealing key insights
into current safety gaps and trade-offs between robustness and expressive
freedom. Our work underscores the value of graded safety metrics for building
trustworthy LLMs.

</details>


### [124] [NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction](https://arxiv.org/abs/2506.00975)
*Qichao Wang,Ziqiao Meng,Wenqian Cui,Yifei Zhang,Pengcheng Wu,Bingzhe Wu,Irwin King,Liang Chen,Peilin Zhao*

Key words: 双通道语音数据，NTPP，语音语言模型，对话系统，实时应用

TL;DR: 该论文提出了一种新的生成模型范式NTPP，首次利用双通道语音数据提升语音语言模型的对话能力，显著改善了对话的自然性和延迟问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 受GPT-4o启发，希望提升语音语言模型在自然对话中的表现，现有方法未能充分利用双通道语音数据。

Method: 提出NTPP（Next-Token-Pair Prediction）模型，利用双通道语音数据训练解码器架构，实现说话者无关的双通道对话学习。

Result: NTPP在对话能力（如轮转预测、响应连贯性、自然性）和推理延迟方面显著优于现有方法。

Conclusion: NTPP为语音语言模型的实际应用提供了高效、自然的对话解决方案。

Abstract: Inspired by the impressive capabilities of GPT-4o, there is growing interest
in enabling speech language models (SLMs) to engage in natural, fluid spoken
interactions with humans. Recent advancements have led to the development of
several SLMs that demonstrate promising results in this area. However, current
approaches have yet to fully exploit dual-channel speech data, which inherently
captures the structure and dynamics of human conversation. In this work, we
systematically explore the use of dual-channel speech data in the context of
modern large language models, and introduce a novel generative modeling
paradigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent
dual-channel spoken dialogue learning using decoder-only architectures for the
first time. We evaluate our approach on standard benchmarks, and empirical
results show that our proposed method, NTPP, significantly improves the
conversational abilities of SLMs in terms of turn-taking prediction, response
coherence, and naturalness. Moreover, compared to existing methods, NTPP
achieves substantially lower inference latency, highlighting its practical
efficiency for real-time applications.

</details>


### [125] [LEMONADE: A Large Multilingual Expert-Annotated Abstractive Event Dataset for the Real World](https://arxiv.org/abs/2506.00980)
*Sina J. Semnani,Pingyue Zhang,Wanyue Zhai,Haozhuo Li,Ryan Beauchamp,Trey Billing,Katayoun Kishi,Manling Li,Monica S. Lam*

Key words: LEMONADE, AEE, AEL, LLMs, ZEST

TL;DR: LEMONADE是一个大规模多语言冲突事件数据集，结合抽象事件提取（AEE）和实体链接（AEL）方法，评估了多种大语言模型（LLMs）的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决多语言来源数据聚合的挑战，提出AEE和AEL以改进传统事件提取方法。

Method: 使用LLMs进行任务评估，提出零-shot检索系统ZEST，并与监督模型进行比较。

Result: 零-shot系统在端到端任务中F1得分为58.3%，优于专业模型GoLLIE；ZEST在AEL任务中F1得分45.7%，优于基线OneNet。

Conclusion: 零-shot系统仍有差距，需进一步研究提升。

Abstract: This paper presents LEMONADE, a large-scale conflict event dataset comprising
39,786 events across 20 languages and 171 countries, with extensive coverage of
region-specific entities. LEMONADE is based on a partially reannotated subset
of the Armed Conflict Location & Event Data (ACLED), which has documented
global conflict events for over a decade.
  To address the challenge of aggregating multilingual sources for global event
analysis, we introduce abstractive event extraction (AEE) and its subtask,
abstractive entity linking (AEL). Unlike conventional span-based event
extraction, our approach detects event arguments and entities through holistic
document understanding and normalizes them across the multilingual dataset. We
evaluate various large language models (LLMs) on these tasks, adapt existing
zero-shot event extraction systems, and benchmark supervised models.
Additionally, we introduce ZEST, a novel zero-shot retrieval-based system for
AEL.
  Our best zero-shot system achieves an end-to-end F1 score of 58.3%, with LLMs
outperforming specialized event extraction models such as GoLLIE. For entity
linking, ZEST achieves an F1 score of 45.7%, significantly surpassing OneNet, a
state-of-the-art zero-shot baseline that achieves only 23.7%. However, these
zero-shot results lag behind the best supervised systems by 20.1% and 37.0% in
the end-to-end and AEL tasks, respectively, highlighting the need for further
research.

</details>


### [126] [What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training](https://arxiv.org/abs/2506.00981)
*Marianne de Heer Kloots,Hosein Mohebbi,Charlotte Pouw,Gaofei Shen,Willem Zuidema,Martijn Bentum*

Key words: 自监督学习, Wav2Vec2, 语言特异性, 荷兰语, 语音识别

TL;DR: 研究探讨了自监督学习模型Wav2Vec2中荷兰语语音和词汇信息的表示效果，发现仅用荷兰语预训练比用英语或多语言数据训练更能提升荷兰语特征的语言特异性表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索自监督模型的语言特异性表示，尤其是荷兰语的特征在不同预训练数据下的表现。

Method: 测试Wav2Vec2模型在荷兰语语音和词汇信息上的编码能力，使用分类或聚类探针及零指标进行评估。

Result: 荷兰语预训练在语言特征编码上优于英语或多语言预训练，且与下游语音识别性能相关。

Conclusion: 语言特异性预训练能显著提升模型在目标语言上的表现。

Abstract: How language-specific are speech representations learned by self-supervised
models? Existing work has shown that a range of linguistic features can be
successfully decoded from end-to-end models trained only on speech recordings.
However, it's less clear to what extent pre-training on specific languages
improves language-specific linguistic information. Here we test the encoding of
Dutch phonetic and lexical information in internal representations of
self-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the
representation of Dutch linguistic features as compared to pre-training on
similar amounts of English or larger amounts of multilingual data. This
language-specific advantage is well-detected by trained clustering or
classification probes, and partially observable using zero-shot metrics.
Furthermore, the language-specific benefit on linguistic feature encoding
aligns with downstream performance on Automatic Speech Recognition.

</details>


### [127] [Do LLMs Understand Why We Write Diaries? A Method for Purpose Extraction and Clustering](https://arxiv.org/abs/2506.00985)
*Valeriya Goloviznina,Alexander Sergeev,Mikhail Melnichenko,Evgeny Kotelnikov*

Key words: 日记分析, 写作目的, 大语言模型, 苏联时期日记, 文本聚类

TL;DR: 论文提出了一种基于大语言模型（LLM）的新方法，用于从日记中识别和聚类写作目的，并在苏联时期日记上验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统方法在大规模日记语料中提取有意义信息时效果不佳，因此需要更先进的工具来分析日记写作目的。

Method: 使用大语言模型（如GPT-4o和o1-mini）对日记文本进行分析和聚类，并与基于模板的基线方法进行对比。

Result: GPT-4o和o1-mini表现最佳，而基线方法效果较差；研究还对作者性别、年龄和写作年份的影响进行了分析。

Conclusion: LLM在日记分析中表现优越，但仍存在改进空间，未来可进一步优化模型和处理错误类型。

Abstract: Diary analysis presents challenges, particularly in extracting meaningful
information from large corpora, where traditional methods often fail to deliver
satisfactory results. This study introduces a novel method based on Large
Language Models (LLMs) to identify and cluster the various purposes of diary
writing. By "purposes," we refer to the intentions behind diary writing, such
as documenting life events, self-reflection, or practicing language skills. Our
approach is applied to Soviet-era diaries (1922-1929) from the Prozhito digital
archive, a rich collection of personal narratives. We evaluate different
proprietary and open-source LLMs, finding that GPT-4o and o1-mini achieve the
best performance, while a template-based baseline is significantly less
effective. Additionally, we analyze the retrieved purposes based on gender, age
of the authors, and the year of writing. Furthermore, we examine the types of
errors made by the models, providing a deeper understanding of their
limitations and potential areas for improvement in future research.

</details>


### [128] [Talking to Data: Designing Smart Assistants for Humanities Databases](https://arxiv.org/abs/2506.00986)
*Alexander Sergeev,Valeriya Goloviznina,Mikhail Melnichenko,Evgeny Kotelnikov*

Key words: LLM, 数字人文, RAG, 自然语言处理, Prozhito

TL;DR: 该研究开发了一个基于LLM的智能助手，用于通过自然语言与数字人文数据交互，旨在提高人文研究的可访问性和效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统交互形式限制了人文研究数据库的访问，该研究试图解决这一问题。

Method: 采用RAG方法，结合混合搜索、自动查询生成、文本到SQL过滤等技术开发聊天机器人。

Result: 实验基于Prozhito数字档案评估了不同语言模型的响应质量。

Conclusion: LLM有潜力改变研究人员和公众与数字档案的交互方式，使其更直观和包容。

Abstract: Access to humanities research databases is often hindered by the limitations
of traditional interaction formats, particularly in the methods of searching
and response generation. This study introduces an LLM-based smart assistant
designed to facilitate natural language communication with digital humanities
data. The assistant, developed in a chatbot format, leverages the RAG approach
and integrates state-of-the-art technologies such as hybrid search, automatic
query generation, text-to-SQL filtering, semantic database search, and
hyperlink insertion. To evaluate the effectiveness of the system, experiments
were conducted to assess the response quality of various language models. The
testing was based on the Prozhito digital archive, which contains diary entries
from predominantly Russian-speaking individuals who lived in the 20th century.
The chatbot is tailored to support anthropology and history researchers, as
well as non-specialist users with an interest in the field, without requiring
prior technical training. By enabling researchers to query complex databases
with natural language, this tool aims to enhance accessibility and efficiency
in humanities research. The study highlights the potential of Large Language
Models to transform the way researchers and the public interact with digital
archives, making them more intuitive and inclusive. Additional materials are
presented in GitHub repository:
https://github.com/alekosus/talking-to-data-intersys2025.

</details>


### [129] [Less is More: Local Intrinsic Dimensions of Contextual Language Models](https://arxiv.org/abs/2506.01034)
*Benjamin Matthias Ruppik,Julius von Rohrscheidt,Carel van Niekerk,Michael Heck,Renato Vukovic,Shutong Feng,Hsien-chin Lin,Nurul Lubis,Bastian Rieck,Marcus Zibrowius,Milica Gašić*

Key words: 大型语言模型、微调、局部维度、几何属性、泛化能力

TL;DR: 研究通过几何视角分析大型语言模型（LLMs）的训练与微调效果，提出局部维度作为预测模型动态和泛化能力的新方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 理解LLMs内部机制及其在训练和微调中的行为变化是一个复杂挑战，需探索新视角以提供更直观的理解。

Method: 通过测量上下文语言模型潜在空间的局部维度及其在训练和微调中的变化，分析其对模型动态和泛化的影响。

Result: 局部维度的平均值能预测模型训练能力耗尽（如对话状态跟踪任务）、过拟合（如情感识别任务）及顿悟现象（如算术任务），且其减少通常伴随性能提升。

Conclusion: 局部维度分析为实践者提供了对微调效果的新见解，有助于优化模型配置，推动LLMs的可解释性和适应性研究。

Abstract: Understanding the internal mechanisms of large language models (LLMs) remains
a challenging and complex endeavor. Even fundamental questions, such as how
fine-tuning affects model behavior, often require extensive empirical
evaluation. In this paper, we introduce a novel perspective based on the
geometric properties of contextual latent embeddings to study the effects of
training and fine-tuning. To that end, we measure the local dimensions of a
contextual language model's latent space and analyze their shifts during
training and fine-tuning. We show that the local dimensions provide insights
into the model's training dynamics and generalization ability. Specifically,
the mean of the local dimensions predicts when the model's training
capabilities are exhausted, as exemplified in a dialogue state tracking task,
overfitting, as demonstrated in an emotion recognition task, and grokking, as
illustrated with an arithmetic task. Furthermore, our experiments suggest a
practical heuristic: reductions in the mean local dimension tend to accompany
and predict subsequent performance gains. Through this exploration, we aim to
provide practitioners with a deeper understanding of the implications of
fine-tuning on embedding spaces, facilitating informed decisions when
configuring models for specific applications. The results of this work
contribute to the ongoing discourse on the interpretability, adaptability, and
generalizability of LLMs by bridging the gap between intrinsic model mechanisms
and geometric properties in the respective embeddings.

</details>


### [130] [Probing Neural Topology of Large Language Models](https://arxiv.org/abs/2506.01042)
*Yu Zheng,Yuan Yuan,Yong Li,Paolo Santi*

Key words: 大型语言模型, 神经元拓扑, 图探测, 语言生成

TL;DR: 该论文提出了图探测方法，用于揭示大型语言模型（LLMs）神经元的功能连接拓扑结构，并发现神经拓扑结构能普遍预测下一个词的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索神经元如何协同激活以产生涌现能力，从而更深入理解并安全开发LLMs。

Method: 引入图探测方法，分析不同LLM家族和规模的内部神经图，研究其与语言生成性能的关系。

Result: 发现神经拓扑结构能普遍预测下一个词性能，即使仅保留1%神经元连接或仅在8步预训练后，这种预测性仍稳健。

Conclusion: 不同LLMs形成了复杂且一致的神经拓扑结构，可能是其语言生成能力的基础。

Abstract: Probing large language models (LLMs) has yielded valuable insights into their
internal mechanisms by linking neural representations to interpretable
semantics. However, how neurons functionally co-activate with each other to
give rise to emergent capabilities remains largely unknown, hindering a deeper
understanding and safer development of LLMs. In this work, we introduce graph
probing, a method for uncovering the functional connectivity topology of LLM
neurons and relating it to language generation performance. By analyzing
internal neural graphs across diverse LLM families and scales, we discover a
universal predictability of next-token prediction performance using only neural
topology. This predictability is robust even when retaining just 1% of neuron
connections or probing models after only 8 pretraining steps, highlighting the
sparsity and early emergence of topological patterns. Further graph matching
analysis suggests that, despite significant distinctions in architectures,
parameters, and training data, different LLMs develop intricate and consistent
neural topological structures that may form the foundation for their language
generation abilities. Codes and data for the graph probing toolbox are released
at https://github.com/DavyMorgan/llm-graph-probing.

</details>


### [131] [CHEER-Ekman: Fine-grained Embodied Emotion Classification](https://arxiv.org/abs/2506.01047)
*Phan Anh Duong,Cat Luong,Divyesh Bommana,Tianyu Jiang*

Key words: 情感识别、Ekman基本情感、大型语言模型、最佳-最差缩放

TL;DR: 提出了一个基于Ekman六种基本情感类别的数据集CHEER-Ekman，使用大型语言模型自动最佳-最差缩放方法，情感识别准确率优于监督方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究如何通过文本识别情感的身体反应，填补了这一领域的空白。

Method: 使用自动最佳-最差缩放方法和简化提示指令及链式思维推理，优化情感识别。

Result: 实验表明，该方法在情感识别准确率上优于传统监督方法，且小型模型表现可与大型模型竞争。

Conclusion: 通过优化提示和思维链，可显著提升情感识别的准确性和效率。

Abstract: Emotions manifest through physical experiences and bodily reactions, yet
identifying such embodied emotions in text remains understudied. We present an
embodied emotion classification dataset, CHEER-Ekman, extending the existing
binary embodied emotion dataset with Ekman's six basic emotion categories.
Using automatic best-worst scaling with large language models, we achieve
performance superior to supervised approaches on our new dataset. Our
investigation reveals that simplified prompting instructions and
chain-of-thought reasoning significantly improve emotion recognition accuracy,
enabling smaller models to achieve competitive performance with larger ones.

</details>


### [132] [SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models](https://arxiv.org/abs/2506.01062)
*Thinh Pham,Nguyen Nguyen,Pratibha Zunjare,Weiyuan Chen,Yu-Min Tseng,Tu Vu*

Key words: SealQA, 评估基准, 搜索增强语言模型, 事实性问题, 多文档推理, 噪声鲁棒性

TL;DR: SealQA是一个新的基准测试，用于评估SEarch-Augmented Language模型在处理事实性问题的能力，尤其是在网络搜索结果冲突、嘈杂或无帮助的情况下。测试结果显示当前前沿模型表现不佳，尤其是在长文本和多文档推理方面仍有显著缺陷。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 针对网络搜索结果中常见的问题（如冲突、噪声或无效信息），开发一个基准测试以评估语言模型的实际推理能力，揭示当前模型的局限性。

Method: 设计了三种类型的测试集：Seal-0（核心）、Seal-Hard（高难度）和LongSeal（长文本多文档推理），评估模型在不同场景下的表现。

Result: 前沿模型在所有测试集中表现不佳，尤其在Seal-0中准确率极低（如o3-mini达6.3%）。增加计算资源未显著提升性能，且模型对噪声搜索结果高度敏感。

Conclusion: 当前模型在处理复杂事实性问题和多文档推理时仍存在重大挑战，需要进一步改进模型设计和推理能力。

Abstract: We introduce SealQA, a new challenge benchmark for evaluating
SEarch-Augmented Language models on fact-seeking questions where web search
yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors:
(1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and
reasoning capabilities, with Seal-0 focusing on the most challenging questions
where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3)
LongSeal, which extends SealQA to test long-context, multi-document reasoning
in "needle-in-a-haystack" settings. Our evaluation reveals critical limitations
in current models: Even frontier LLMs perform poorly across all SealQA flavors.
On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini
achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning
efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and
o3-mini are highly vulnerable to noisy search results. Notably, increasing
test-time compute does not yield reliable gains across o3-mini, o4-mini, and
o3, with performance often plateauing or even declining early. Additionally,
while recent models are less affected by the "lost-in-the-middle" issue, they
still fail to reliably identify relevant documents in LongSeal when faced with
numerous distractors. To facilitate future work, we release SealQA at
huggingface.co/datasets/vtllms/sealqa.

</details>


### [133] [How Programming Concepts and Neurons Are Shared in Code Language Models](https://arxiv.org/abs/2506.01074)
*Amir Hossein Kargaran,Yihong Liu,François Yvon,Hinrich Schütze*

Key words: 大型语言模型, 编程语言, 概念空间, 神经元激活, 少样本翻译

TL;DR: 本文研究了大型语言模型(LLMs)在多编程语言(PLs)与英语的概念空间中的关系，通过少样本翻译任务发现概念空间更接近英语，并分析了神经元激活模式。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨LLMs在多编程语言与英语之间的内部表示关系，填补了多语言环境下编程语言概念的空白。

Method: 使用两个基于Llama的模型，对21个编程语言对进行少样本翻译任务，并通过解码中间层嵌入和神经元激活来分析概念空间。

Result: 发现概念空间更接近英语(包含PL关键词)，且语言特定神经元集中在底层，而专用神经元出现在顶层；高度对齐的PL难以识别特定神经元。

Conclusion: LLMs在多编程语言与英语的概念空间中表现出结构性模式，为理解模型内部表示提供新视角。

Abstract: Several studies have explored the mechanisms of large language models (LLMs)
in coding tasks, but most have focused on programming languages (PLs) in a
monolingual setting. In this paper, we investigate the relationship between
multiple PLs and English in the concept space of LLMs. We perform a few-shot
translation task on 21 PL pairs using two Llama-based models. By decoding the
embeddings of intermediate layers during this task, we observe that the concept
space is closer to English (including PL keywords) and assigns high
probabilities to English tokens in the second half of the intermediate layers.
We analyze neuron activations for 11 PLs and English, finding that while
language-specific neurons are primarily concentrated in the bottom layers,
those exclusive to each PL tend to appear in the top layers. For PLs that are
highly aligned with multiple other PLs, identifying language-specific neurons
is not feasible. These PLs also tend to have a larger keyword set than other
PLs and are closer to the model's concept space regardless of the input/output
PL in the translation task. Our findings provide insights into how LLMs
internally represent PLs, revealing structural patterns in the model's concept
space. Code is available at https://github.com/cisnlp/code-specific-neurons.

</details>


### [134] [zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression](https://arxiv.org/abs/2506.01084)
*Saibo Geng,Nathan Ranchin,Yunzhen yao,Maxime Peyrard,Chris Wendler,Michael Gastpar,Robert West*

Key words: tokenization, LLMs, dynamic vocabulary, LZW compression

TL;DR: 介绍了zip2zip框架，通过动态调整token词汇表减少序列长度和计算成本。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 静态tokenizer难以适应特定领域或语言输入，导致序列过长和计算成本高。

Method: 提出zip2zip框架，包含动态LZW压缩tokenizer、运行时嵌入层和因果语言建模变体。

Result: zip2zip-fied LLMs在推理时减少了20-60%的序列长度，显著提升了推理速度。

Conclusion: zip2zip是高效且轻量的动态tokenization解决方案。

Abstract: Tokenization efficiency plays a critical role in the performance and cost of
large language models (LLMs), yet most models rely on static tokenizers
optimized for general-purpose corpora. These tokenizers' fixed vocabularies
often fail to adapt to domain- or language-specific inputs, leading to longer
token sequences and higher computational costs. We introduce zip2zip, a
framework that enables LLMs to dynamically adjust token vocabulary at inference
time, allowing for fewer generated tokens and thus faster inference. zip2zip
consists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch
(LZW) compression that incrementally compresses tokens into reusable
"hypertokens" on the fly; (2) an embedding layer that computes embeddings for
newly formed hypertokens at runtime; and (3) a causal language modeling variant
that trains the model to operate on hypertokenized, compressed sequences. We
show that an existing LLM can be zip2zip-fied in 10 GPU-hours via
parameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to
use hypertokens at inference time, reducing input and output sequence length by
20-60\%, with significant improvements in inference latency.

</details>


### [135] [Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements](https://arxiv.org/abs/2506.01089)
*Metehan Oguz,Yavuz Bakman,Duygu Nur Yaldiz*

Key words: 大语言模型、共指消解、索引词、句法线索、评估

TL;DR: 该研究评估了大语言模型（LLMs）在处理索引词（如I、you等）上的表现，发现表现差异显著，并探讨了句法线索的影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 之前的研究主要评估了LLMs在名词和第三人称代词上的共指消解能力，本研究填补了索引词（如I、you）研究的空白。

Method: 通过构建包含1600个多项选择题的英语索引数据集，评估了GPT-4o等LLMs的表现。

Result: LLMs在部分索引词（如I）上表现优异，但在其他（如you、here）上表现不佳；句法线索对不同索引词的影响各异。

Conclusion: LLMs处理索引词的能力尚不完善，句法线索对性能的影响需进一步研究。

Abstract: Large Language Models (LLMs) have demonstrated impressive performances in
tasks related to coreference resolution. However, previous studies mostly
assessed LLM performance on coreference resolution with nouns and third person
pronouns. This study evaluates LLM performance on coreference resolution with
indexical like I, you, here and tomorrow, which come with unique challenges due
to their linguistic properties. We present the first study examining how LLMs
interpret indexicals in English, releasing the English Indexical Dataset with
1600 multiple-choice questions. We evaluate pioneering LLMs, including GPT-4o,
Claude 3.5 Sonnet, Gemini 1.5 Pro, and DeepSeek V3. Our results reveal that
LLMs exhibit an impressive performance with some indexicals (I), while
struggling with others (you, here, tomorrow), and that syntactic cues (e.g.
quotation) contribute to LLM performance with some indexicals, while they
reduce performance with others. Code and data are available at:
https://github.com/metehanoguzz/LLMs-Indexicals-English.

</details>


### [136] [Contextual Candor: Enhancing LLM Trustworthiness Through Hierarchical Unanswerability Detection](https://arxiv.org/abs/2506.01104)
*Steven Robinson,Antonio Carlos Rivera*

Key words: 大型语言模型, 无答案检测, 强化学习, 对话AI, 可信度

TL;DR: 论文提出了一种名为RUL的新型混合训练范式，旨在提升大型语言模型（LLMs）识别无法回答问题并生成适当响应的能力，通过多阶段学习策略显著提高了可信度和实用性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决LLMs在生成回答时可能产生不准确或虚构内容的问题，提升其在对话系统中的可信度和用户满意度。

Method: 提出RUL框架，结合判别式无答案预测头和生成核心模型，采用监督微调和强化学习的多阶段训练策略。

Result: 实验表明RUL在无答案检测和拒绝生成中表现优异，显著提升了模型的实用性和可信度。

Conclusion: RUL为构建更可靠、以用户为中心的对话AI系统提供了有效解决方案。

Abstract: The pervasive deployment of large language models (LLMs) in conversational AI
systems has revolutionized information access, yet their propensity for
generating factually unsupported or hallucinated responses remains a critical
impediment to trustworthiness and widespread adoption. This paper introduces
Reinforced Unanswerability Learning (RUL), a novel hybrid training paradigm
designed to imbue LLMs with the intrinsic capability to accurately detect
unanswerable questions and generate reliably appropriate responses. Unlike
conventional approaches that rely on external classifiers or simple prompting,
RUL integrates a discriminative unanswerability prediction head with the LLM's
generative core, guided by a multi-stage learning strategy. This includes
supervised fine-tuning on a novel, richly annotated dataset,
Enhanced-CAsT-Answerability (ECA), which features hierarchical answerability
labels and ground-truth refusal responses. Crucially, RUL incorporates a
subsequent reinforcement learning with human feedback (RLHF) phase to refine
the nuance, helpfulness, and informativeness of refusal responses. Extensive
experiments demonstrate RUL's superior performance, achieving significantly
higher accuracy in unanswerability detection across sentence, paragraph, and
ranking levels, and substantially increasing the generation of appropriate
refusals for unanswerable queries, alongside strong performance on answerable
questions. Human evaluations further corroborate RUL's effectiveness,
highlighting a marked improvement in perceived helpfulness and trustworthiness,
ultimately paving the way for more reliable and user-centric conversational AI.

</details>


### [137] [From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models](https://arxiv.org/abs/2506.01133)
*Asım Ersoy,Basel Mousi,Shammur Chowdhury,Firoj Alam,Fahim Dalvi,Nadir Durrani*

Key words: 大型语言模型, 多模态训练, 潜在概念分析, 语义抽象, 跨模态学习

TL;DR: 本文探讨了大型语言模型（LLMs）在多模态训练中是否能够形成更丰富的语义理解，并分析了语音和文本模型的潜在概念结构。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究大型语言模型在多模态训练中是否能够形成类似文本模型的广泛语义概念，以探索跨模态的语义抽象形成机制。

Method: 使用潜在概念分析（Latent Concept Analysis）这一无监督方法，分析语音和文本模型单独及联合训练时的潜在表示。

Result: 发现了跨模态训练中语义抽象的形成模式，并为社区提供了可复现的资源。

Conclusion: 多模态训练可能帮助模型形成更结构化的语义理解，为未来跨模态智能研究提供了基础。

Abstract: The emergence of large language models (LLMs) has demonstrated that systems
trained solely on text can acquire extensive world knowledge, develop reasoning
capabilities, and internalize abstract semantic concepts--showcasing properties
that can be associated with general intelligence. This raises an intriguing
question: Do such concepts emerge in models trained on other modalities, such
as speech? Furthermore, when models are trained jointly on multiple modalities:
Do they develop a richer, more structured semantic understanding? To explore
this, we analyze the conceptual structures learned by speech and textual models
both individually and jointly. We employ Latent Concept Analysis, an
unsupervised method for uncovering and interpreting latent representations in
neural networks, to examine how semantic abstractions form across modalities.
For reproducibility we made scripts and other resources available to the
community.

</details>


### [138] [A Word is Worth 4-bit: Efficient Log Parsing with Binary Coded Decimal Recognition](https://arxiv.org/abs/2506.01147)
*Prerak Srivastava,Giulio Corallo,Sergey Rybalko*

Key words: 日志解析、字符级神经网络、细粒度模板、二进制编码

TL;DR: 提出一种字符级日志解析器，通过新的神经网络架构聚合字符嵌入，生成细粒度日志模板，提升下游任务准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有日志解析器无法捕捉细粒度模板细节，导致下游任务准确性和实用性降低。

Method: 使用字符级神经网络架构聚合字符嵌入，生成二进制编码序列以实现细粒度日志模板提取。

Result: 在修订版Loghub-2k和工业数据集上测试，性能与LLM解析器相当，效率优于语义解析器。

Conclusion: 字符级解析器在低资源下表现优异，兼具准确性和效率。

Abstract: System-generated logs are typically converted into categorical log templates
through parsing. These templates are crucial for generating actionable insights
in various downstream tasks. However, existing parsers often fail to capture
fine-grained template details, leading to suboptimal accuracy and reduced
utility in downstream tasks requiring precise pattern identification. We
propose a character-level log parser utilizing a novel neural architecture that
aggregates character embeddings. Our approach estimates a sequence of
binary-coded decimals to achieve highly granular log templates extraction. Our
low-resource character-level parser, tested on revised Loghub-2k and a manually
annotated industrial dataset, matches LLM-based parsers in accuracy while
outperforming semantic parsers in efficiency.

</details>


### [139] [Mispronunciation Detection Without L2 Pronunciation Dataset in Low-Resource Setting: A Case Study in Finland Swedish](https://arxiv.org/abs/2506.01156)
*Nhan Phan,Mikko Kuronen,Maria Kautonen,Riikka Ullakonoja,Anna von Zansen,Yaroslav Getman,Ekaterina Voskoboinik,Tamás Grósz,Mikko Kurimo*

Key words: 发音错误检测、低资源语言、芬兰瑞典语、wav2vec 2.0、熵正则化

TL;DR: 该论文提出了一种针对芬兰瑞典语（FS）的发音错误检测（MD）模型，适用于低资源语言，仅需少量第二语言（L2）数据，并通过新颖的方法在召回率和精确度之间取得平衡。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于多数发音错误检测系统仅针对英语等主流语言，低资源语言如芬兰瑞典语缺乏此类工具，作者旨在填补这一空白。

Method: 使用多语言wav2vec 2.0模型，结合熵正则化、温度缩放和top-k归一化，仅需少量L2数据即可适配MD任务。

Result: 模型在召回率（43.2%）和精确度（29.8%）上优于基线模型（77.5%和17.6%）。

Conclusion: 该方法的简单性和语言无关性使其适用于其他低资源语言。

Abstract: Mispronunciation detection (MD) models are the cornerstones of many language
learning applications. Unfortunately, most systems are built for English and
other major languages, while low-resourced language varieties, such as Finland
Swedish (FS), lack such tools. In this paper, we introduce our MD model for FS,
trained on 89 hours of first language (L1) speakers' spontaneous speech and
tested on 33 minutes of L2 transcribed read-aloud speech.
  We trained a multilingual wav2vec 2.0 model with entropy regularization,
followed by temperature scaling and top-k normalization after the inference to
better adapt it for MD. The main novelty of our method lies in its simplicity,
requiring minimal L2 data. The process is also language-independent, making it
suitable for other low-resource languages. Our proposed algorithm allows us to
balance Recall (43.2%) and Precision (29.8%), compared with the baseline
model's Recall (77.5%) and Precision (17.6%).

</details>


### [140] [The Inverse Scaling Effect of Pre-Trained Language Model Surprisal Is Not Due to Data Leakage](https://arxiv.org/abs/2506.01172)
*Byung-Doh Oh,Hongao Zhu,William Schuler*

Key words: 心理语言学, 预训练语言模型, 意外性, 数据泄露, 阅读时间

TL;DR: 研究表明，预训练语言模型的意外性与人类阅读时间的预测关系不受数据泄露影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨预训练语言模型中意外性与人类阅读时间的预测关系是否受数据泄露影响。

Method: 通过两个研究：1）分析五个自然阅读时间语料库在预训练数据中的泄露程度；2）使用‘无泄露’数据重复实验。

Result: 发现数据泄露较少，且模型大小仍与意外性预测阅读时间的拟合度负相关。

Conclusion: 先前结果并非由数据泄露驱动。

Abstract: In psycholinguistic modeling, surprisal from larger pre-trained language
models has been shown to be a poorer predictor of naturalistic human reading
times. However, it has been speculated that this may be due to data leakage
that caused language models to see the text stimuli during training. This paper
presents two studies to address this concern at scale. The first study reveals
relatively little leakage of five naturalistic reading time corpora in two
pre-training datasets in terms of length and frequency of token $n$-gram
overlap. The second study replicates the negative relationship between language
model size and the fit of surprisal to reading times using models trained on
'leakage-free' data that overlaps only minimally with the reading time corpora.
Taken together, this suggests that previous results using language models
trained on these corpora are not driven by the effects of data leakage.

</details>


### [141] [LAQuer: Localized Attribution Queries in Content-grounded Generation](https://arxiv.org/abs/2506.01187)
*Eran Hirsch,Aviv Slobodkin,David Wan,Elias Stengel-Eskin,Mohit Bansal,Ido Dagan*

Key words: 文本生成, 定位验证, LAQuer, 多文档摘要, 长格式问答

TL;DR: 论文提出了Localized Attribution Queries (LAQuer)任务，通过细粒度定位生成文本与源文本的对应关系，提升文本生成的准确性验证效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法在验证生成文本准确性时存在效率低下或与用户需求不匹配的问题，需要更精准的用户导向解决方案。

Method: 提出LAQuer任务，比较了基于提示的大型语言模型（LLM）和利用LLM内部表示的两种方法，并扩展了现有模型框架。

Result: 实验表明，LAQuer方法显著减少了需验证的文本长度，在多文档摘要和长格式问答任务中有效。

Conclusion: LAQuer任务提升了文本生成的可用性，未来可推动细粒度定位研究。

Abstract: Grounded text generation models often produce content that deviates from
their source material, requiring user verification to ensure accuracy. Existing
attribution methods associate entire sentences with source documents, which can
be overwhelming for users seeking to fact-check specific claims. In contrast,
existing sub-sentence attribution methods may be more precise but fail to align
with users' interests. In light of these limitations, we introduce Localized
Attribution Queries (LAQuer), a new task that localizes selected spans of
generated output to their corresponding source spans, allowing fine-grained and
user-directed attribution. We compare two approaches for the LAQuer task,
including prompting large language models (LLMs) and leveraging LLM internal
representations. We then explore a modeling framework that extends existing
attributed text generation methods to LAQuer. We evaluate this framework across
two grounded text generation tasks: Multi-document Summarization (MDS) and
Long-form Question Answering (LFQA). Our findings show that LAQuer methods
significantly reduce the length of the attributed text. Our contributions
include: (1) proposing the LAQuer task to enhance attribution usability, (2)
suggesting a modeling framework and benchmarking multiple baselines, and (3)
proposing a new evaluation setting to promote future research on localized
attribution in content-grounded generation.

</details>


### [142] [Culturally-Grounded Chain-of-Thought (CG-CoT):Enhancing LLM Performance on Culturally-Specific Tasks in Low-Resource Languages](https://arxiv.org/abs/2506.01190)
*Madhavendra Thakur*

Key words: LLM、文化推理、低资源语言、CG-CoT、BLEU

TL;DR: 提出CG-CoT方法解决LLM在低资源语言文化推理任务中的表现不足，通过结合文化上下文检索与显式推理序列实现更好效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大语言模型在低资源语言和特定文化推理任务中表现不佳，影响其全球应用公平性。

Method: 结合密集向量检索的文化上下文与显式推理序列（CG-CoT），在约鲁巴谚语解释任务中进行实验。

Result: CG-CoT在文化对齐准确性和深度上显著优于传统方法，同时揭示BLEU等指标与人工评估的文化相关性差异。

Conclusion: CG-CoT有效提升文化推理能力，并呼吁重新思考低资源NLP的评估方法。

Abstract: Large Language Models (LLMs) struggle with culturally-specific reasoning
tasks, particularly in low-resource languages, hindering their global
applicability. Addressing this gap is crucial for equitable AI deployment. We
introduce Culturally-Grounded Chain-of-Thought (CG-CoT), a novel prompting
strategy that combines dense vector retrieval of cultural context with explicit
reasoning sequences. Our extensive experiments on Yoruba proverb interpretation
demonstrate that CG-CoT provides significantly higher culturally-aligned
accuracy and depth than traditional prompting methods, validated through both
automated metrics and LLM-based evaluations. Notably, we uncover stark
disparities between token-level translation metrics like BLEU and human-judged
cultural relevance, suggesting a rethinking of evaluation approaches for
low-resource NLP.

</details>


### [143] [CoBRA: Quantifying Strategic Language Use and LLM Pragmatics](https://arxiv.org/abs/2506.01195)
*Anshun Asher Zheng,Junyi Jessy Li,David I. Beaver*

Key words: 非合作话语, 战略语言, LLM评估, CoBRA, CHARM

TL;DR: 该论文通过引入CoBRA框架和CHARM数据集，评估了LLM在非合作语境中对战略语言的理解能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究填补了LLM在非合作话语中系统性理解的空白，特别是在高风险的对抗性场景中。

Method: 提出了CoBRA框架和三个量化指标（BaT、PaT、NRBaT），并使用CHARM数据集进行评估。

Result: LLM在战略语言理解上表现有限，模型规模提升性能，但推理能力反而带来负面影响。

Conclusion: LLM在非合作语境中的实用理解能力仍有待提升。

Abstract: Language is often used strategically, particularly in high-stakes,
adversarial settings, yet most work on pragmatics and LLMs centers on
cooperativity. This leaves a gap in systematic understanding of non-cooperative
discourse. To address this, we introduce CoBRA (Cooperation-Breach Response
Assessment), along with three interpretable metrics -- Benefit at Turn (BaT),
Penalty at Turn (PaT), and Normalized Relative Benefit at Turn (NRBaT) -- to
quantify the perceived strategic effects of discourse moves. We also present
CHARM, an annotated dataset of real courtroom cross-examinations, to
demonstrate the framework's effectiveness. Using these tools, we evaluate a
range of LLMs and show that LLMs generally exhibit limited pragmatic
understanding of strategic language. While model size shows an increase in
performance on our metrics, reasoning ability does not help and largely hurts,
introducing overcomplication and internal confusion.

</details>


### [144] [Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures](https://arxiv.org/abs/2506.01197)
*Mark Muchane,Sean Richardson,Kiho Park,Victor Veitch*

Key words: 稀疏自编码器,语义层次,重建性,可解释性,计算效率

TL;DR: 提出了改进的稀疏自编码器架构，显式建模概念的语义层次，提高了重建性和可解释性，并显著提升计算效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统稀疏字典学习无法捕捉概念间的语义关系，限制了学习效果。

Method: 引入显式建模语义层次结构的改进稀疏自编码器架构。

Result: 实验显示该架构能学习语义层次，提升重建性、可解释性和计算效率。

Conclusion: 改进架构有效解决了传统方法的局限性，具有实际应用潜力。

Abstract: Sparse dictionary learning (and, in particular, sparse autoencoders) attempts
to learn a set of human-understandable concepts that can explain variation on
an abstract space. A basic limitation of this approach is that it neither
exploits nor represents the semantic relationships between the learned
concepts. In this paper, we introduce a modified SAE architecture that
explicitly models a semantic hierarchy of concepts. Application of this
architecture to the internal representations of large language models shows
both that semantic hierarchy can be learned, and that doing so improves both
reconstruction and interpretability. Additionally, the architecture leads to
significant improvements in computational efficiency.

</details>


### [145] [Trick or Neat: Adversarial Ambiguity and Language Model Evaluation](https://arxiv.org/abs/2506.01205)
*Antonia Karamolegkou,Oliver Eberle,Phillip Rust,Carina Kauf,Anders Søgaard*

Key words: 歧义检测, 语言模型, 对抗性数据集, 线性探针

TL;DR: 论文研究了语言模型对歧义的敏感性，引入了一个对抗性歧义数据集，包括句法、词汇和语音歧义。研究发现直接提示无法稳健识别歧义，而基于模型表示的线性探针能高精度解码歧义。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 明确歧义检测对语言理解的重要性，包括不确定性估计、幽默检测等，评估语言模型对歧义的敏感性。

Method: 引入对抗性歧义数据集（含句法、词汇和语音歧义及对抗性变体），通过直接提示和线性探针分析模型表现。

Result: 直接提示效果不佳，线性探针识别歧义准确率高达90%，揭示了模型在不同层编码歧义的方式。

Conclusion: 研究为提示范式提供见解，并展示语言模型如何编码歧义。

Abstract: Detecting ambiguity is important for language understanding, including
uncertainty estimation, humour detection, and processing garden path sentences.
We assess language models' sensitivity to ambiguity by introducing an
adversarial ambiguity dataset that includes syntactic, lexical, and
phonological ambiguities along with adversarial variations (e.g., word-order
changes, synonym replacements, and random-based alterations). Our findings show
that direct prompting fails to robustly identify ambiguity, while linear probes
trained on model representations can decode ambiguity with high accuracy,
sometimes exceeding 90\%. Our results offer insights into the prompting
paradigm and how language models encode ambiguity at different layers. We
release both our code and data: https://github.com/coastalcph/lm_ambiguity.

</details>


### [146] [Mamba Drafters for Speculative Decoding](https://arxiv.org/abs/2506.01206)
*Daewon Choi,Seunghyuk Oh,Saket Dingliwal,Jihoon Tack,Kyuyoung Kim,Woomin Song,Seojin Kim,Insu Han,Jinwoo Shin,Aram Galstyan,Shubham Katiyar,Sravan Babu Bodapati*

Key words: speculative decoding, large language model, Mamba, state space model, tree search

TL;DR: 论文提出了一种基于Mamba的新型drafters，用于加速大型语言模型生成，结合了外部drafters的灵活性和自推测方法的高效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的推测解码方法在灵活性和效率之间存在权衡，外部drafters速度慢，而自推测方法需要重新训练。

Method: 利用Mamba（一种最先进的状态空间模型）的线性结构，避免了传统Transformer方法的二次复杂度，并结合新颖的测试时树搜索算法提升效率。

Result: Mamba-based drafters优于现有外部drafters方法，且在内存使用和跨模型适应性上媲美自推测方法。

Conclusion: Mamba-based drafters提供了一个高效且灵活的加速LLM生成的解决方案。

Abstract: Speculative decoding has emerged as a promising approach to accelerating
large language model (LLM) generation using a fast drafter while maintaining
alignment with the target model's distribution. However, existing approaches
face a trade-off: external drafters offer flexibility but can suffer from
slower drafting, while self-speculation methods use drafters tailored to the
target model but require re-training. In this paper, we introduce novel
drafters based on Mamba, a state-of-the-art state space model (SSM), as a
solution that combines the best aspects of both approaches. By leveraging the
linear structure of SSMs, our approach avoids the quadratic complexity inherent
in traditional Transformer-based methods, enabling faster drafting and lower
memory usage while maintaining the flexibility to work across different target
models. We further enhance efficiency with a novel test-time tree search
algorithm for generating high-quality draft candidates. Our empirical
evaluation demonstrates that Mamba-based drafters not only outperform existing
external drafting methods but are also comparable to state-of-the-art
self-speculation approaches while using less memory and maintaining their
cross-model adaptability.

</details>


### [147] [Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers](https://arxiv.org/abs/2506.01215)
*Woomin Song,Sai Muralidhar Jayanthi,Srikanth Ronanki,Kanthashree Mysore Sathyendra,Jinwoo Shin,Aram Galstyan,Shubham Katiyar,Sravan Babu Bodapati*

Key words: 长文本处理, KV缓存, 增量处理, 相似性匹配, 性能提升

TL;DR: REFORM是一种高效处理长文本的推理框架，通过增量处理和选择性KV缓存重组，性能提升显著。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法在处理超出预训练限制的长文本时，存在信息丢失或资源消耗大的问题。

Method: 采用两阶段策略：增量处理与压缩KV缓存，以及相似性匹配选择必要令牌。

Result: 在1M文本长度下，性能提升超50%和27%，推理时间减少30%，内存占用降低5%。

Conclusion: REFORM在效率和性能上均优于现有方法。

Abstract: As large language models increasingly gain popularity in real-world
applications, processing extremely long contexts, often exceeding the model's
pre-trained context limits, has emerged as a critical challenge. While existing
approaches to efficient long-context processing show promise, recurrent
compression-based methods struggle with information preservation, whereas
random access approaches require substantial memory resources. We introduce
REFORM, a novel inference framework that efficiently handles long contexts
through a two-phase approach. First, it incrementally processes input chunks
while maintaining a compressed KV cache, constructs cross-layer context
embeddings, and utilizes early exit strategy for improved efficiency. Second,
it identifies and gathers essential tokens via similarity matching and
selectively recomputes the KV cache. Compared to baselines, REFORM achieves
over 50% and 27% performance gains on RULER and BABILong respectively at 1M
context length. It also outperforms baselines on Infinite-Bench and MM-NIAH,
demonstrating flexibility across diverse tasks and domains. Additionally,
REFORM reduces inference time by 30% and peak memory usage by 5%, achieving
both efficiency and superior performance.

</details>


### [148] [Polishing Every Facet of the GEM: Testing Linguistic Competence of LLMs and Humans in Korean](https://arxiv.org/abs/2506.01237)
*SungHo Kim,Nayeon Kim,Taehee Jeon,SangKeun Lee*

Key words: KoGEM, 韩国语, 语言能力评估, 大型语言模型, 音系规则

TL;DR: 论文介绍了韩国语语法评估基准（KoGEM），用于评估大型语言模型（LLMs）和人类的韩语语言能力，发现LLMs在某些任务上表现良好但在需要实际经验的任务上表现不佳。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 旨在评估LLMs和人类在韩语中的语言能力，揭示LLMs的局限性并提出改进方向。

Method: 构建了包含1.5k多选题的KoGEM，涵盖5大类16子类，并对27种LLMs进行零样本评估。

Result: LLMs在定义性知识任务上表现良好，但在需要实际经验的任务（如音系规则）上表现不佳。

Conclusion: KoGEM不仅揭示了LLMs的语言能力局限性，还提出了通过整合实践经验提升其语言能力的可能性。

Abstract: We introduce the $\underline{Ko}rean \underline{G}rammar
\underline{E}valuation Bench\underline{M}ark (KoGEM)$, designed to assess the
linguistic competence of LLMs and humans in Korean. KoGEM consists of 1.5k
multiple-choice QA pairs covering five main categories and 16 subcategories.
The zero-shot evaluation of 27 LLMs of various sizes and types reveals that
while LLMs perform remarkably well on straightforward tasks requiring primarily
definitional knowledge, they struggle with tasks that demand the integration of
real-world experiential knowledge, such as phonological rules and
pronunciation. Furthermore, our in-depth analysis suggests that incorporating
such experiential knowledge could enhance the linguistic competence of LLMs.
With KoGEM, we not only highlight the limitations of current LLMs in linguistic
competence but also uncover hidden facets of LLMs in linguistic competence,
paving the way for enhancing comprehensive language understanding. Our code and
dataset are available at: https://github.com/SungHo3268/KoGEM.

</details>


### [149] [ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists](https://arxiv.org/abs/2506.01241)
*Jie Ruan,Inderjeet Nair,Shuyang Cao,Amy Liu,Sheza Munir,Micah Pollens-Dempsey,Tiffany Chiang,Lucy Kates,Nicholas David,Sihan Chen,Ruxin Yang,Yuqian Yang,Jasmine Gump,Tessa Bialek,Vivek Sankaran,Margo Schlanger,Lu Wang*

Key words: ExpertLongBench, CLEAR, 长文本评估, 专家任务, 大语言模型

TL;DR: ExpertLongBench是一个专家级基准测试，包含11个任务，覆盖9个领域，模拟真实专家工作流程。其要求长文本输出（超5000词）并严格遵循领域特定需求。提出的CLEAR评估框架通过提取任务要求列表，实现专家对齐的细粒度评估。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有人工智能模型在专家级任务中表现不足，需改进评估方法以提升其实际应用能力。

Method: 采用包含专家验证的任务要求列表，并开发CLEAR框架，通过提取输出与参考文本的信息点进行比较评估。

Result: 测试11个大语言模型，表现最佳者F1分数仅为26.8%，显示模型需显著改进。CLEAR框架可扩展且低成本。

Conclusion: 专家级任务对AI仍具挑战，CLEAR提供了有效评估路径。

Abstract: This paper introduces ExpertLongBench, an expert-level benchmark containing
11 tasks from 9 domains that reflect realistic expert workflows and
applications. Beyond question answering, the application-driven tasks in
ExpertLongBench demand long-form outputs that can exceed 5,000 tokens and
strict adherence to domain-specific requirements. Notably, each task in
ExpertLongBench includes a rubric, designed or validated by domain experts, to
specify task requirements and guide output evaluation. Furthermore, we propose
CLEAR, an evaluation framework that supports accurate evaluation of long-form
model outputs in our benchmark. To achieve fine-grained, expert-aligned
evaluation, CLEAR derives checklists from both model outputs and references by
extracting information corresponding to items in the task-specific rubric.
Checklist items for model outputs are then compared with corresponding items
for reference outputs to assess their correctness, enabling grounded
evaluation. We benchmark 11 large language models (LLMs) and analyze components
in CLEAR, showing that (1) existing LLMs, with the top performer achieving only
a 26.8% F1 score, require significant improvement for expert-level tasks; (2)
models can generate content corresponding to the required aspects, though often
not accurately; and (3) accurate checklist extraction and comparison in CLEAR
can be achieved by open-weight models for more scalable and low-cost usage.

</details>


### [150] [MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine](https://arxiv.org/abs/2506.01252)
*Shufeng Kong,Xingru Yang,Yuanyuan Wei,Zijie Wang,Hao Tang,Jiuqi Qin,Shuting Lan,Yingheng Wang,Junwen Bai,Zhuangbin Chen,Zibin Zheng,Caihua Liu,Hao Liang*

Key words: 中医,大语言模型,评估基准,临床推理,安全性

TL;DR: 本文介绍了MTCMB，一个用于评估大语言模型在中医知识、推理和安全性方面的多任务基准测试。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 中医缺乏计算建模和评估的标准，现有评估基准在中医领域缺乏系统性和临床真实性。

Method: 开发了MTCMB，包含12个子数据集，涵盖知识问答、语言理解、诊断推理、处方生成和安全性评估5大类。

Result: 初步结果显示，现有模型在基础知识上表现良好，但在临床推理、处方计划和安全性方面表现不足。

Conclusion: 需要类似MTCMB的领域对齐基准，以指导开发更可靠的中医AI系统。

Abstract: Traditional Chinese Medicine (TCM) is a holistic medical system with
millennia of accumulated clinical experience, playing a vital role in global
healthcare-particularly across East Asia. However, the implicit reasoning,
diverse textual forms, and lack of standardization in TCM pose major challenges
for computational modeling and evaluation. Large Language Models (LLMs) have
demonstrated remarkable potential in processing natural language across diverse
domains, including general medicine. Yet, their systematic evaluation in the
TCM domain remains underdeveloped. Existing benchmarks either focus narrowly on
factual question answering or lack domain-specific tasks and clinical realism.
To fill this gap, we introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs
on TCM Knowledge, Reasoning, and Safety. Developed in collaboration with
certified TCM experts, MTCMB comprises 12 sub-datasets spanning five major
categories: knowledge QA, language understanding, diagnostic reasoning,
prescription generation, and safety evaluation. The benchmark integrates
real-world case records, national licensing exams, and classical texts,
providing an authentic and comprehensive testbed for TCM-capable models.
Preliminary results indicate that current LLMs perform well on foundational
knowledge but fall short in clinical reasoning, prescription planning, and
safety compliance. These findings highlight the urgent need for domain-aligned
benchmarks like MTCMB to guide the development of more competent and
trustworthy medical AI systems. All datasets, code, and evaluation tools are
publicly available at: https://github.com/Wayyuanyuan/MTCMB.

</details>


### [151] [CoRE: Condition-based Reasoning for Identifying Outcome Variance in Complex Events](https://arxiv.org/abs/2506.01253)
*Sai Vallurupalli,Francis Ferraro*

Key words: 隐含条件，复杂事件结果，LLM模型，条件推理，结果验证

TL;DR: 该论文研究了隐含条件对事件结果的影响，通过结合现有数据集中的目标和状态标注，设计了基于条件的推理任务，并测试了不同规模的LLM模型在这些任务中的表现，发现条件在上下文缺失时非常有用，不同模型在生成和识别结果变体条件的能力上差异显著。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究隐含条件如何影响复杂事件结果，以更严格地验证相关主张。

Method: 结合并扩展了两个现有数据集中的标注（目标和状态），设计基于条件的推理任务，测试不同规模和意图对齐的LLM模型。

Result: 条件在上下文缺失时有效，不同模型在生成和识别结果变体条件的能力上差异显著，GPT-4等大模型在低约束情况下更谨慎。

Conclusion: 条件对结果验证有重要影响，模型规模和意图对齐是关键因素。

Abstract: Knowing which latent conditions lead to a particular outcome is useful for
critically examining claims made about complex event outcomes. Identifying
implied conditions and examining their influence on an outcome is challenging.
We handle this by combining and augmenting annotations from two existing
datasets consisting of goals and states, and explore the influence of
conditions through our research questions and Condition-based Reasoning tasks.
We examine open and closed LLMs of varying sizes and intent-alignment on our
reasoning tasks and find that conditions are useful when not all context is
available. Models differ widely in their ability to generate and identify
outcome-variant conditions which affects their performance on outcome
validation when conditions are used to replace missing context. Larger models
like GPT-4o, are more cautious in such less constrained situations.

</details>


### [152] [Memory-Efficient FastText: A Comprehensive Approach Using Double-Array Trie Structures and Mark-Compact Memory Management](https://arxiv.org/abs/2506.01254)
*Yimin Du*

Key words: FastText, 内存优化, 双数组Trie, 标记压缩, 词嵌入

TL;DR: 提出了一种基于双数组Trie和标记压缩垃圾回收的FastText内存优化框架，显著降低内存需求并保持高质量词嵌入。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决FastText在工业规模部署中因哈希冲突导致语义漂移和内存需求过高的问题。

Method: 整合双数组Trie结构和标记压缩垃圾回收原则，通过前缀和后缀相似性压缩词嵌入。

Result: 在3000万中文词汇数据上，内存从100GB降至30GB，性能几乎无损。

Conclusion: 该框架显著降低成本，提升加载速度和模型可靠性。

Abstract: FastText has established itself as a fundamental algorithm for learning word
representations, demonstrating exceptional capability in handling
out-of-vocabulary words through character-level n-gram embeddings. However, its
hash-based bucketing mechanism introduces critical limitations for large-scale
industrial deployment: hash collisions cause semantic drift, and memory
requirements become prohibitively expensive when dealing with real-world
vocabularies containing millions of terms. This paper presents a comprehensive
memory optimization framework that fundamentally reimagines FastText's memory
management through the integration of double-array trie (DA-trie) structures
and mark-compact garbage collection principles. Our approach leverages the
linguistic insight that n-grams sharing common prefixes or suffixes exhibit
highly correlated embeddings due to co-occurrence patterns in natural language.
By systematically identifying and merging semantically similar embeddings based
on structural relationships, we achieve compression ratios of 4:1 to 10:1 while
maintaining near-perfect embedding quality. The algorithm consists of four
sophisticated phases: prefix trie construction with embedding mapping,
prefix-based similarity compression, suffix-based similarity compression, and
mark-compact memory reorganization. Comprehensive experiments on a 30-million
Chinese vocabulary dataset demonstrate memory reduction from over 100GB to
approximately 30GB with negligible performance degradation. Our industrial
deployment results show significant cost reduction, faster loading times, and
improved model reliability through the elimination of hash collision artifacts.
Code and experimental implementations are available at:
https://github.com/initial-d/me_fasttext

</details>


### [153] [DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models](https://arxiv.org/abs/2506.01257)
*Jiancheng Ye,Sophie Bronstein,Jiarui Hai,Malak Abu Hashish*

Key words: 开源语言模型,混合专家,链式推理,强化学习,偏见,安全性

TL;DR: DeepSeek-R1是一个开源的大型语言模型，结合混合专家、链式推理和强化学习，在数学、医疗诊断等领域表现优异，但也存在偏见和安全问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 提供透明且成本效益高的替代方案，以解决专有模型如GPT-4o和Claude-3 Opus的局限性。

Method: 采用混合专家（MoE）架构，结合链式推理（CoT）和强化学习。

Result: 在USMLE和AIME等基准测试中表现优异，但在多语言和伦理敏感场景下易受偏见和安全问题影响。

Conclusion: DeepSeek-R1是开放可扩展AI的重大进展，需进一步研究改进偏见缓解和安全性。

Abstract: DeepSeek-R1 is a cutting-edge open-source large language model (LLM)
developed by DeepSeek, showcasing advanced reasoning capabilities through a
hybrid architecture that integrates mixture of experts (MoE), chain of thought
(CoT) reasoning, and reinforcement learning. Released under the permissive MIT
license, DeepSeek-R1 offers a transparent and cost-effective alternative to
proprietary models like GPT-4o and Claude-3 Opus; it excels in structured
problem-solving domains such as mathematics, healthcare diagnostics, code
generation, and pharmaceutical research. The model demonstrates competitive
performance on benchmarks like the United States Medical Licensing Examination
(USMLE) and American Invitational Mathematics Examination (AIME), with strong
results in pediatric and ophthalmologic clinical decision support tasks. Its
architecture enables efficient inference while preserving reasoning depth,
making it suitable for deployment in resource-constrained settings. However,
DeepSeek-R1 also exhibits increased vulnerability to bias, misinformation,
adversarial manipulation, and safety failures - especially in multilingual and
ethically sensitive contexts. This survey highlights the model's strengths,
including interpretability, scalability, and adaptability, alongside its
limitations in general language fluency and safety alignment. Future research
priorities include improving bias mitigation, natural language comprehension,
domain-specific validation, and regulatory compliance. Overall, DeepSeek-R1
represents a major advance in open, scalable AI, underscoring the need for
collaborative governance to ensure responsible and equitable deployment.

</details>


### [154] [Exploring the Potential of LLMs as Personalized Assistants: Dataset, Evaluation, and Analysis](https://arxiv.org/abs/2506.01262)
*Jisoo Mok,Ik-hwan Kim,Sangkwon Park,Sungroh Yoon*

Key words: 个性化AI助手, LLMs, 对话数据集, 自动评估模型, HiCUPID

TL;DR: HiCUPID是一个新的基准测试，旨在挖掘LLMs提供个性化回复的潜力，并提供开源对话数据集和自动评估模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前缺乏针对个性化AI助手研究开源的对话数据集，阻碍了研究的进展。

Method: 提出了HiCUPID基准测试，包括对话数据集和基于Llama-3.2的自动评估模型。

Result: 发布了数据集、评估模型和代码，评估模型的表现与人类偏好高度一致。

Conclusion: HiCUPID填补了研究空白，为个性化AI助手的发展提供了支持。

Abstract: Personalized AI assistants, a hallmark of the human-like capabilities of
Large Language Models (LLMs), are a challenging application that intertwines
multiple problems in LLM research. Despite the growing interest in the
development of personalized assistants, the lack of an open-source
conversational dataset tailored for personalization remains a significant
obstacle for researchers in the field. To address this research gap, we
introduce HiCUPID, a new benchmark to probe and unleash the potential of LLMs
to deliver personalized responses. Alongside a conversational dataset, HiCUPID
provides a Llama-3.2-based automated evaluation model whose assessment closely
mirrors human preferences. We release our dataset, evaluation model, and code
at https://github.com/12kimih/HiCUPID.

</details>


### [155] [WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard CTC-based Keyword Spotting and Inter-layer Biasing](https://arxiv.org/abs/2506.01263)
*Yu Nakagome,Michael Hentschel*

Key words: 语音识别, CTC模型, 关键词检测, 稀有词识别

TL;DR: 提出一种改进CTC模型中稀有词识别准确率的方法，无需额外训练或TTS系统。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决端到端语音识别对训练数据词汇的偏倚问题，特别是专有名词和未知词汇的识别。

Method: 利用中间层声学特征进行关键词检测，并对后续声学模型层施加偏置，使用wildcard CTC实现快速且模糊匹配。

Result: 在日语语音识别实验中，未知词的F1分数提升了29%。

Conclusion: 该方法有效提高了稀有词的识别准确率，且易于扩展到大规模模型。

Abstract: Despite recent advances in end-to-end speech recognition methods, the output
tends to be biased to the training data's vocabulary, resulting in inaccurate
recognition of proper nouns and other unknown terms. To address this issue, we
propose a method to improve recognition accuracy of such rare words in
CTC-based models without additional training or text-to-speech systems.
Specifically, keyword spotting is performed using acoustic features of
intermediate layers during inference, and a bias is applied to the subsequent
layers of the acoustic model for detected keywords. For keyword detection, we
adopt a wildcard CTC that is both fast and tolerant of ambiguous matches,
allowing flexible handling of words that are difficult to match strictly. Since
this method does not require retraining of existing models, it can be easily
applied to even large-scale models. In experiments on Japanese speech
recognition, the proposed method achieved a 29% improvement in the F1 score for
unknown words.

</details>


### [156] [Beyond In-Context Learning: Aligning Long-form Generation of Large Language Models via Task-Inherent Attribute Guidelines](https://arxiv.org/abs/2506.01265)
*Do Xuan Long,Duong Ngoc Yen,Do Xuan Trong,Luu Anh Tuan,Kenji Kawaguchi,Shafiq Joty,Min-Yen Kan,Nancy F. Chen*

Key words: 上下文学习、长文本生成、指导准则、模型优化

TL;DR: 论文研究上下文学习（ICL）在长文本生成任务中的不足，提出LongGuide方法，通过生成任务语言和格式的指导准则，显著提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索ICL在长文本生成任务中表现不佳的原因，并提出改进方法，以增强大语言模型在任务中的表现。

Method: 提出LongGuide方法，生成并行指导准则（Metric Guidelines和Output Constraint Guidelines），并通过自动选择最佳组合提升模型性能。

Result: 实验结果证明LongGuide在零样本和少样本设置下，将强开源和闭源模型的性能提升超过5%。

Conclusion: LongGuide具有通用性、可学习性，并能与自动提示优化器协同工作，有效提升ICL在长文本生成任务中的表现。

Abstract: In-context learning (ICL) is an important yet not fully understood ability of
pre-trained large language models (LLMs). It can greatly enhance task
performance using a few examples, termed demonstrations, without fine-tuning.
Although effective in question answering, ICL often underperforms in long-form
generation tasks such as summarization. Under appropriately realistic
assumptions, we empirically and theoretically show that ICL demonstrations
alone are insufficient to teach LLMs the task language and format distributions
for generation. We argue for explicit exposure to the task distributions and
hypothesize that defining them by prompting enhances model performance. To this
end, we present LongGuide, which efficiently generates two parallel streams of
guidelines capturing task language and format properties: (i) Metric Guidelines
(MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output
Constraint Guidelines (OCGs) that constrain generation at both token and
sentence levels. LongGuide automatically selects the best combination of
guidelines, improving both strong open- and closed-source LLMs by over 5% in
both zero- and few-shot settings. We show that LongGuide is generalizable,
learnable by weak models to enhance strong ones, and integrates synergistically
with automatic prompt optimizers.

</details>


### [157] [Detoxification of Large Language Models through Output-layer Fusion with a Calibration Model](https://arxiv.org/abs/2506.01266)
*Yuanhe Tian,Mingjie Deng,Guoqing Jin,Yan Song*

Key words: 大语言模型、去毒、校准模型、轻量干预、无毒嵌入空间

TL;DR: 本文提出了一种简单有效的大语言模型（LLM）去毒方法，通过预训练的校准模型在生成过程中轻量干预，避免生成有害内容。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有LLM去毒方法依赖大规模无毒数据或人工标注数据，计算成本高且缺乏鲁棒性，还会影响流畅性和上下文理解。

Method: 利用紧凑的预训练校准模型，通过轻量干预在生成过程中引导去毒，学习无毒嵌入空间。

Result: 实验证明该方法能有效降低毒性，同时保持合理的内容表达。

Conclusion: 该方法仅需一次性训练校准模型，可无缝应用于多个LLM，且不影响流畅性和上下文理解。

Abstract: Existing approaches for Large language model (LLM) detoxification generally
rely on training on large-scale non-toxic or human-annotated preference data,
designing prompts to instruct the LLM to generate safe content, or modifying
the model parameters to remove toxic information, which are computationally
expensive, lack robustness, and often compromise LLMs' fluency and contextual
understanding. In this paper, we propose a simple yet effective approach for
LLM detoxification, which leverages a compact, pre-trained calibration model
that guides the detoxification process of a target LLM via a lightweight
intervention in its generation pipeline. By learning a detoxified embedding
space from non-toxic data, the calibration model effectively steers the LLM
away from generating harmful content. This approach only requires a one-time
training of the calibration model that is able to be seamlessly applied to
multiple LLMs without compromising fluency or contextual understanding.
Experiment results on the benchmark dataset demonstrate that our approach
reduces toxicity while maintaining reasonable content expression.

</details>


### [158] [Schema as Parameterized Tools for Universal Information Extraction](https://arxiv.org/abs/2506.01276)
*Sheng Liang,Yongyue Zhang,Yaxiong Wu,Ruiming Tang,Yong Liu*

Key words: 通用信息提取, 大语言模型, 模式参数化, 自适应框架

TL;DR: 本文提出了一种统一的文本到结构生成框架SPT，通过将预定义模式视为参数化工具，解决了通用信息提取（UIE）中的模式适应性问题。SPT能够统一处理封闭、开放和按需IE任务，并在实验中表现出色。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决通用信息提取（UIE）中预定义模式与动态模式生成之间的适应性问题。

Method: 提出Schema as Parameterized Tools (SPT)框架，将模式视为参数化工具，支持模式检索、模式填充和模式生成。

Result: 实验显示SPT能自适应处理四种IE任务，性能优于基线方法且训练参数更少。

Conclusion: SPT提供了一种统一且高效的方法，显著提升了UIE的适应性和性能。

Abstract: Universal information extraction (UIE) primarily employs an extractive
generation approach with large language models (LLMs), typically outputting
structured information based on predefined schemas such as JSON or tables. UIE
suffers from a lack of adaptability when selecting between predefined schemas
and on-the-fly schema generation within the in-context learning paradigm,
especially when there are numerous schemas to choose from. In this paper, we
propose a unified adaptive text-to-structure generation framework, called
Schema as Parameterized Tools (SPT), which reimagines the tool-calling
capability of LLMs by treating predefined schemas as parameterized tools for
tool selection and parameter filling. Specifically, our SPT method can be
applied to unify closed, open, and on-demand IE tasks by adopting Schema
Retrieval by fetching the relevant schemas from a predefined pool, Schema
Filling by extracting information and filling slots as with tool parameters, or
Schema Generation by synthesizing new schemas with uncovered cases. Experiments
show that the SPT method can handle four distinct IE tasks adaptively,
delivering robust schema retrieval and selection performance. SPT also achieves
comparable extraction performance to LoRA baselines and current leading UIE
systems with significantly fewer trainable parameters.

</details>


### [159] [VM14K: First Vietnamese Medical Benchmark](https://arxiv.org/abs/2506.01305)
*Thong Nguyen,Duc Nguyen,Minh Dang,Thai Dao,Long Nguyen,Quan H. Nguyen,Dat Nguyen,Kien Tran,Minh Tran*

Key words: 医学基准，越南语，语言模型，医疗领域，多语言

TL;DR: 本文介绍了一种构建越南语医学问题基准的方法，用于评估语言模型在医疗领域的能力，包含14,000个选择题，涵盖34个医学专业。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为评估语言模型在非英语医疗社区中的能力，确保实际应用的质量，但目前缺乏资源和方法来构建此类基准。

Method: 使用可验证的医学考试和临床记录，经专家标注，构建涵盖四个难度等级的多选题基准。

Result: 创建了首个越南语医学问题基准，分为公开样本集、完整公开集和私人评估集。

Conclusion: 该方法可扩展至其他语言，开源数据构建流程支持未来多语言医疗基准的开发。

Abstract: Medical benchmarks are indispensable for evaluating the capabilities of
language models in healthcare for non-English-speaking communities,therefore
help ensuring the quality of real-life applications. However, not every
community has sufficient resources and standardized methods to effectively
build and design such benchmark, and available non-English medical data is
normally fragmented and difficult to verify. We developed an approach to tackle
this problem and applied it to create the first Vietnamese medical question
benchmark, featuring 14,000 multiple-choice questions across 34 medical
specialties. Our benchmark was constructed using various verifiable sources,
including carefully curated medical exams and clinical records, and eventually
annotated by medical experts. The benchmark includes four difficulty levels,
ranging from foundational biological knowledge commonly found in textbooks to
typical clinical case studies that require advanced reasoning. This design
enables assessment of both the breadth and depth of language models' medical
understanding in the target language thanks to its extensive coverage and
in-depth subject-specific expertise. We release the benchmark in three parts: a
sample public set (4k questions), a full public set (10k questions), and a
private set (2k questions) used for leaderboard evaluation. Each set contains
all medical subfields and difficulty levels. Our approach is scalable to other
languages, and we open-source our data construction pipeline to support the
development of future multilingual benchmarks in the medical domain.

</details>


### [160] [A Platform for Investigating Public Health Content with Efficient Concern Classification](https://arxiv.org/abs/2506.01308)
*Christopher Li,Rickard Stureborg,Bhuwan Dhingra,Jun Yang*

Key words: 公共卫生,在线内容,关切识别,语言模型,知识迁移

TL;DR: 论文介绍了一个名为ConcernScope的平台，用于快速识别文本中的健康关切问题，帮助公共卫生官员理解并应对在线内容中的担忧。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在线内容中对公共卫生举措的担忧增加，导致全球预防措施的采纳受阻。未来公共卫生工作需理解这些内容及读者的关切，并有效回应。

Method: ConcernScope采用教师-学生框架进行知识迁移，结合大型语言模型和轻量分类器，快速识别文本中的健康关切问题。

Result: 平台支持大规模文件上传、自动URL抓取和直接文本编辑，展示了在在线社区数据集中发现常见关切、时间序列分析等应用。

Conclusion: ConcernScope为公共卫生官员提供了识别和分析健康关切的有效工具。

Abstract: A recent rise in online content expressing concerns with public health
initiatives has contributed to already stalled uptake of preemptive measures
globally. Future public health efforts must attempt to understand such content,
what concerns it may raise among readers, and how to effectively respond to it.
To this end, we present ConcernScope, a platform that uses a teacher-student
framework for knowledge transfer between large language models and light-weight
classifiers to quickly and effectively identify the health concerns raised in a
text corpus. The platform allows uploading massive files directly,
automatically scraping specific URLs, and direct text editing. ConcernScope is
built on top of a taxonomy of public health concerns. Intended for public
health officials, we demonstrate several applications of this platform: guided
data exploration to find useful examples of common concerns found in online
community datasets, identification of trends in concerns through an example
time series analysis of 186,000 samples, and finding trends in topic frequency
before and after significant events.

</details>


### [161] [Growing Through Experience: Scaling Episodic Grounding in Language Models](https://arxiv.org/abs/2506.01312)
*Chunhui Zhang,Sirui,Wang,Zhongyu Ouyang,Xiangchi Yuan,Soroush Vosoughi*

Key words: 语言模型,情景学习,蒙特卡洛树搜索,蒸馏方法,任务对齐

TL;DR: 提出了一种可扩展的弱到强情景学习框架，通过蒙特卡洛树搜索和新型蒸馏方法，提升语言模型在规划任务中的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前情景学习方法在可扩展性和集成性方面存在局限性，尤其是对中等规模语言模型。大模型虽具备更强的抽象能力，但缺乏有效利用经验流的机制。

Method: 结合蒙特卡洛树搜索进行结构化经验收集，并采用新型蒸馏方法，在保留语言模型固有能力的同时嵌入情景记忆。

Result: 实验显示该方法在规划和问答任务中优于现有技术3.45%，并在深层模型层中显著提升任务对齐能力。

Conclusion: 该框架在提升模型情景记忆能力的同时保持了稳定性，尤其在复杂规划任务中表现优异。

Abstract: Language models (LMs) require robust episodic grounding-the capacity to learn
from and apply past experiences-to excel at physical planning tasks. Current
episodic grounding approaches struggle with scalability and integration,
limiting their effectiveness, especially for medium-sized LMs (7B parameters).
While larger LMs (70-405B parameters) possess superior hierarchical
representations and extensive pre-trained knowledge, they encounter a
fundamental scale paradox: despite their advanced abstraction capabilities,
they lack efficient mechanisms to leverage experience streams. We propose a
scalable weak-to-strong episodic learning framework that effectively transfers
episodic behaviors from smaller to larger LMs. This framework integrates Monte
Carlo tree search for structured experience collection with a novel
distillation method, preserving the inherent LM capabilities while embedding
episodic memory. Experiments demonstrate our method surpasses state-of-the-art
proprietary LMs by 3.45% across diverse planning and question-answering tasks.
Layer-wise probing further indicates significant improvements in task
alignment, especially within deeper LM layers, highlighting stable
generalization even for previously unseen scenarios with increased planning
complexity-conditions where baseline methods degrade markedly.

</details>


### [162] [Zero-Shot Text-to-Speech for Vietnamese](https://arxiv.org/abs/2506.01322)
*Thi Vu,Linh The Nguyen,Dat Quoc Nguyen*

Key words: PhoAudiobook, 越南语TTS, 零样本模型, VALL-E, VoiceCraft, XTTS-V2

TL;DR: 摘要介绍了PhoAudiobook数据集，包含941小时的越南语高质量音频，用于文本到语音（TTS）。实验使用三种零样本TTS模型（VALL-E、VoiceCraft、XTTS-V2），发现PhoAudiobook显著提升模型性能，其中VALL-E和VoiceCraft在短句合成中表现优异，数据集已公开。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为促进越南语文本到语音技术的研究和发展，构建高质量音频数据集。

Method: 利用PhoAudiobook数据集，对VALL-E、VoiceCraft和XTTS-V2三种零样本TTS模型进行实验评估。

Result: PhoAudiobook显著提升模型性能，VALL-E和VoiceCraft在短句合成中表现最佳。

Conclusion: PhoAudiobook可作为越南语TTS研究的重要资源，未来可进一步优化模型性能。

Abstract: This paper introduces PhoAudiobook, a newly curated dataset comprising 941
hours of high-quality audio for Vietnamese text-to-speech. Using PhoAudiobook,
we conduct experiments on three leading zero-shot TTS models: VALL-E,
VoiceCraft, and XTTS-V2. Our findings demonstrate that PhoAudiobook
consistently enhances model performance across various metrics. Moreover,
VALL-E and VoiceCraft exhibit superior performance in synthesizing short
sentences, highlighting their robustness in handling diverse linguistic
contexts. We publicly release PhoAudiobook to facilitate further research and
development in Vietnamese text-to-speech.

</details>


### [163] [Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines](https://arxiv.org/abs/2506.01329)
*Guifeng Deng,Shuyin Rao,Tianyu Lin,Anlu Dai,Pan Wang,Junyi Xie,Haidong Song,Ke Zhao,Dongwu Xu,Zhengdong Cheng,Tao Li,Haiteng Jiang*

Key words: PsyCrisisBench, LLM, 心理危机评估, 自杀倾向检测, 情绪识别

TL;DR: 提出了PsyCrisisBench基准，评估LLM在心理危机评估中的表现，发现其在自杀倾向和风险评估任务中表现优异，但情绪识别仍具挑战性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决心理支持热线需求增加与LLM在情感敏感场景中能力不明确的问题，探索LLM在心理危机评估中的潜力。

Method: 使用PsyCrisisBench（540份标注文本）评估64个LLM，涵盖零样本、少样本和微调三种模式，通过F1分数和统计检验衡量性能。

Result: LLM在自杀倾向检测（F1=0.880）和风险评估（F1=0.907）中表现优异，情绪识别较难（F1=0.709）。开源模型与闭源模型差距缩小。

Conclusion: LLM在结构化心理危机评估中表现良好，尤其适合微调任务；情绪识别需进一步研究。开源模型和量化技术为实际部署提供可行性。

Abstract: Psychological support hotlines are critical for crisis intervention but face
significant challenges due to rising demand. Large language models (LLMs) could
support crisis assessments, yet their capabilities in emotionally sensitive
contexts remain unclear. We introduce PsyCrisisBench, a benchmark of 540
annotated transcripts from the Hangzhou Psychological Assistance Hotline,
assessing four tasks: mood status recognition, suicidal ideation detection,
suicide plan identification, and risk assessment. We evaluated 64 LLMs across
15 families (e.g., GPT, Claude, Gemini, Llama, Qwen, DeepSeek) using zero-shot,
few-shot, and fine-tuning paradigms. Performance was measured by F1-score, with
statistical comparisons via Welch's t-tests. LLMs performed strongly on
suicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779),
and risk assessment (F1=0.907), improved with few-shot and fine-tuning. Mood
status recognition was more challenging (max F1=0.709), likely due to lost
vocal cues and ambiguity. A fine-tuned 1.5B-parameter model (Qwen2.5-1.5B)
surpassed larger models on mood and suicidal ideation. Open-source models like
QwQ-32B performed comparably to closed-source on most tasks (p>0.3), though
closed models retained an edge in mood detection (p=0.007). Performance scaled
with size up to a point; quantization (AWQ) reduced GPU memory by 70% with
minimal F1 degradation. LLMs show substantial promise in structured
psychological crisis assessments, especially with fine-tuning. Mood recognition
remains limited due to contextual complexity. The narrowing gap between open-
and closed-source models, combined with efficient quantization, suggests
feasible integration. PsyCrisisBench offers a robust evaluation framework to
guide model development and ethical deployment in mental health.

</details>


### [164] [Enhancing Interpretable Image Classification Through LLM Agents and Conditional Concept Bottleneck Models](https://arxiv.org/abs/2506.01334)
*Yiwen Jiang,Deval Mehta,Wei Feng,Zongyuan Ge*

Key words: 概念瓶颈模型, 大型语言模型, 动态概念库, 条件概念瓶颈模型, 可解释性

TL;DR: 研究提出了一种动态调整概念数量的方法，并引入条件概念瓶颈模型（CoCoBMs）以优化分类任务的概念评分机制，提高分类准确性和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前概念瓶颈模型中的概念库存在冗余或覆盖不足的问题，且传统概念评分机制有局限性，需要一种动态优化的方法。

Method: 采用基于代理的动态方法调整概念库，提出条件概念瓶颈模型（CoCoBMs）改进概念评分，并通过可编辑矩阵允许LLMs修正冲突的概念评分。

Result: 在6个数据集上的评估显示，该方法将分类准确性提高了6%，可解释性评估提升了30%。

Conclusion: 动态概念库和CoCoBMs有效解决了传统CBM的不足，显著提升了性能和解释性。

Abstract: Concept Bottleneck Models (CBMs) decompose image classification into a
process governed by interpretable, human-readable concepts. Recent advances in
CBMs have used Large Language Models (LLMs) to generate candidate concepts.
However, a critical question remains: What is the optimal number of concepts to
use? Current concept banks suffer from redundancy or insufficient coverage. To
address this issue, we introduce a dynamic, agent-based approach that adjusts
the concept bank in response to environmental feedback, optimizing the number
of concepts for sufficiency yet concise coverage. Moreover, we propose
Conditional Concept Bottleneck Models (CoCoBMs) to overcome the limitations in
traditional CBMs' concept scoring mechanisms. It enhances the accuracy of
assessing each concept's contribution to classification tasks and feature an
editable matrix that allows LLMs to correct concept scores that conflict with
their internal knowledge. Our evaluations across 6 datasets show that our
method not only improves classification accuracy by 6% but also enhances
interpretability assessments by 30%.

</details>


### [165] [The Landscape of Arabic Large Language Models (ALLMs): A New Era for Arabic Language Technology](https://arxiv.org/abs/2506.01340)
*Shahad Al-Khalifa,Nadir Durrani,Hend Al-Khalifa,Firoj Alam*

Key words: 阿拉伯语大型语言模型, LLMs, 人工智能, 语言文化, 技术差距

TL;DR: 本文探讨了阿拉伯语大型语言模型（ALLMs）的发展历程，从基础文本处理系统到复杂的AI驱动模型，并讨论了阿拉伯世界面临的挑战与机遇。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 阿拉伯语作为全球广泛使用的语言之一，拥有超过4.22亿母语使用者，但阿拉伯语特定的大型语言模型（LLMs）发展面临独特挑战。本文旨在探索ALLMs的发展轨迹及其对阿拉伯世界的影响。

Method: 文章回顾了ALLMs从早期文本处理系统到现代AI驱动模型的发展历程，并通过基准测试和公共排行榜对其进行了评估。

Result: 研究发现，ALLMs为阿拉伯世界提供了填补技术差距和增强社区能力的机会，但同时也面临诸多挑战。

Conclusion: ALLMs的发展对阿拉伯世界具有重要意义，但仍需克服语言和文化的复杂性以实现其潜力。

Abstract: The emergence of ChatGPT marked a transformative milestone for Artificial
Intelligence (AI), showcasing the remarkable potential of Large Language Models
(LLMs) to generate human-like text. This wave of innovation has revolutionized
how we interact with technology, seamlessly integrating LLMs into everyday
tasks such as vacation planning, email drafting, and content creation. While
English-speaking users have significantly benefited from these advancements,
the Arabic world faces distinct challenges in developing Arabic-specific LLMs.
Arabic, one of the languages spoken most widely around the world, serves more
than 422 million native speakers in 27 countries and is deeply rooted in a rich
linguistic and cultural heritage. Developing Arabic LLMs (ALLMs) presents an
unparalleled opportunity to bridge technological gaps and empower communities.
The journey of ALLMs has been both fascinating and complex, evolving from
rudimentary text processing systems to sophisticated AI-driven models. This
article explores the trajectory of ALLMs, from their inception to the present
day, highlighting the efforts to evaluate these models through benchmarks and
public leaderboards. We also discuss the challenges and opportunities that
ALLMs present for the Arab world.

</details>


### [166] [TurnBench-MS: A Benchmark for Evaluating Multi-Turn, Multi-Step Reasoning in Large Language Models](https://arxiv.org/abs/2506.01341)
*Yiran Zhang,Mo Wang,Xiaoyang Li,Kaixuan Ren,Chencheng Zhu,Usman Naseem*

Key words: LLMs,多轮推理,基准测试,代码破解,动态推理

TL;DR: TurnBench是一个评估多轮、多步推理能力的新基准，通过交互式代码破解任务测试模型的动态推理能力，结果发现LLMs在复杂情境下表现显著低于人类。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基准测试多为单轮或单步任务，无法真实模拟现实世界中需要迭代推理的场景。

Method: 提出TurnBench，包含Classic和Nightmare两种模式，通过代码破解任务测试模型的多轮推理能力，并提供中间步骤标注。

Result: 最佳LLM在Classic模式下准确率为81.5%，但在Nightmare模式下降至17.8%；人类则均达到100%。

Conclusion: TurnBench揭示了LLMs在多轮推理中的不足，为未来研究提供了严格的测试平台。

Abstract: Despite impressive advances in large language models (LLMs), existing
benchmarks often focus on single-turn or single-step tasks, failing to capture
the kind of iterative reasoning required in real-world settings. To address
this limitation, we introduce TurnBench, a novel benchmark that evaluates
multi-turn, multi-step reasoning through an interactive code-breaking task
inspired by a "Turing Machine Board Game." In each episode, a model must
uncover hidden logical or arithmetic rules by making sequential guesses,
receiving structured feedback, and integrating clues across multiple rounds.
This dynamic setup requires models to reason over time, adapt based on past
information, and maintain consistency across steps-capabilities underexplored
in current benchmarks. TurnBench includes two modes: Classic, which tests
standard reasoning, and Nightmare, which introduces increased complexity and
requires robust inferential chains. To support fine-grained analysis, we
provide ground-truth annotations for intermediate reasoning steps. Our
evaluation of state-of-the-art LLMs reveals significant gaps: the best model
achieves 81.5% accuracy in Classic mode, but performance drops to 17.8% in
Nightmare mode. In contrast, human participants achieve 100% in both,
underscoring the challenge TurnBench poses to current models. By incorporating
feedback loops and hiding task rules, TurnBench reduces contamination risks and
provides a rigorous testbed for diagnosing and advancing multi-step, multi-turn
reasoning in LLMs.

</details>


### [167] [Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic Agents](https://arxiv.org/abs/2506.01344)
*Manan Suri,Puneet Mathur,Nedim Lipka,Franck Dernoncourt,Ryan A. Rossi,Vivek Gupta,Dinesh Manocha*

Key words: 流程图,归因任务,LLMs,神经符号代理

TL;DR: 该论文提出了细粒度流程图归因任务和FlowPathAgent方法，用于提高流程图解释的可靠性，并通过实验验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决LLMs在解释流程图时产生的幻觉问题，提升自动化处理的可靠性。

Method: 提出FlowPathAgent，一种神经符号代理，通过图推理进行细粒度归因。

Result: 在FlowExplainBench数据集上优于基线10-14%。

Conclusion: FlowPathAgent有效减少了LLMs在流程图解释中的幻觉问题。

Abstract: Flowcharts are a critical tool for visualizing decision-making processes.
However, their non-linear structure and complex visual-textual relationships
make it challenging to interpret them using LLMs, as vision-language models
frequently hallucinate nonexistent connections and decision paths when
analyzing these diagrams. This leads to compromised reliability for automated
flowchart processing in critical domains such as logistics, health, and
engineering. We introduce the task of Fine-grained Flowchart Attribution, which
traces specific components grounding a flowchart referring LLM response.
Flowchart Attribution ensures the verifiability of LLM predictions and improves
explainability by linking generated responses to the flowchart's structure. We
propose FlowPathAgent, a neurosymbolic agent that performs fine-grained post
hoc attribution through graph-based reasoning. It first segments the flowchart,
then converts it into a structured symbolic graph, and then employs an agentic
approach to dynamically interact with the graph, to generate attribution paths.
Additionally, we present FlowExplainBench, a novel benchmark for evaluating
flowchart attributions across diverse styles, domains, and question types.
Experimental results show that FlowPathAgent mitigates visual hallucinations in
LLM answers over flowchart QA, outperforming strong baselines by 10-14% on our
proposed FlowExplainBench dataset.

</details>


### [168] [The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning](https://arxiv.org/abs/2506.01347)
*Xinyu Zhu,Mengzhou Xia,Zhepei Wei,Wei-Lin Chen,Danqi Chen,Yu Meng*

Key words: 强化学习, 语言模型, 数学推理, 负样本强化

TL;DR: 该论文研究了强化学习中仅使用负样本（惩罚错误）对语言模型数学推理能力的影响，发现其效果优于传统方法，并提出了一种改进的强化学习目标。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索强化学习中不同样本（正样本和负样本）对语言模型推理能力的影响，以及负样本学习机制的潜在优势。

Method: 分解强化学习信号为正样本强化（PSR）和负样本强化（NSR），并在数学推理数据集上训练模型。通过梯度分析验证NSR的效果。

Result: 仅使用负样本训练可以显著提升模型性能，尤其是在高多样性的评估中（Pass@k，k高达256），甚至优于PPO和GRPO方法。

Conclusion: 负样本强化通过抑制错误生成和重新分配概率质量来优化模型性能，且改进的强化目标可进一步提升效果。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for training language models (LMs) on reasoning tasks that elicit emergent long
chains of thought (CoTs). Unlike supervised learning, it updates the model
using both correct and incorrect samples via policy gradients. To better
understand its mechanism, we decompose the learning signal into reinforcing
correct responses and penalizing incorrect ones, referred to as Positive and
Negative Sample Reinforcement (PSR and NSR), respectively. We train
Qwen2.5-Math-7B and Qwen3-4B on a mathematical reasoning dataset and uncover a
surprising result: training with only negative samples -- without reinforcing
correct responses -- can be highly effective: it consistently improves
performance over the base model across the entire Pass@$k$ spectrum ($k$ up to
$256$), often matching or surpassing PPO and GRPO. In contrast, reinforcing
only correct responses improves Pass@$1$ but degrades performance at higher
$k$, due to reduced diversity. These inference-scaling trends highlight that
solely penalizing incorrect responses may contribute more to performance than
previously recognized. Through gradient analysis, we show that NSR works by
suppressing incorrect generations and redistributing probability mass toward
other plausible candidates, guided by the model's prior beliefs. It refines the
model's existing knowledge rather than introducing entirely new behaviors.
Building on this insight, we propose a simple variant of the RL objective that
upweights NSR, and show that it consistently improves overall Pass@$k$
performance on MATH, AIME 2025, and AMC23. Our code is available at
https://github.com/TianHongZXY/RLVR-Decomposed.

</details>


### [169] [KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors](https://arxiv.org/abs/2506.01357)
*Zhiyang Qi,Takumasa Kaneko,Keiko Takamizo,Mariko Ukiyo,Michimasa Inaba*

Key words: 心理咨询对话数据集, 角色扮演, 大语言模型, 数据多样性, 隐私保护

TL;DR: 本研究通过角色扮演方法构建高质量的心理咨询对话数据集KokoroChat，解决了现有数据集的多样性和真实性问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有心理咨询对话数据集质量参差不齐，且存在隐私和伦理问题，需要一种既能保证质量又能规避风险的数据收集方法。

Method: 采用角色扮演方法，由经过培训的咨询师模拟咨询师与客户的对话，构建了包含6,589条长对话的日语心理咨询数据集KokoroChat。

Result: 实验表明，使用KokoroChat微调开源大语言模型能显著提升生成心理咨询回复的质量和对话的自动评估效果。

Conclusion: 角色扮演方法是一种有效的心理对话数据收集手段，KokoroChat为心理咨询语言模型的开发提供了高质量资源。

Abstract: Generating psychological counseling responses with language models relies
heavily on high-quality datasets. Crowdsourced data collection methods require
strict worker training, and data from real-world counseling environments may
raise privacy and ethical concerns. While recent studies have explored using
large language models (LLMs) to augment psychological counseling dialogue
datasets, the resulting data often suffers from limited diversity and
authenticity. To address these limitations, this study adopts a role-playing
approach where trained counselors simulate counselor-client interactions,
ensuring high-quality dialogues while mitigating privacy risks. Using this
method, we construct KokoroChat, a Japanese psychological counseling dialogue
dataset comprising 6,589 long-form dialogues, each accompanied by comprehensive
client feedback. Experimental results demonstrate that fine-tuning open-source
LLMs with KokoroChat improves both the quality of generated counseling
responses and the automatic evaluation of counseling dialogues. The KokoroChat
dataset is available at https://github.com/UEC-InabaLab/KokoroChat.

</details>


### [170] [MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect Hallucinations](https://arxiv.org/abs/2506.01367)
*Kensuke Mitsuzawa,Damien Garreau*

Key words: 大语言模型,幻觉检测,最大均值差异,机器翻译

TL;DR: 提出了一种基于最大均值差异（MMD）的新方法MMD-Flagger，用于检测大语言模型（LLMs）生成的内容中的幻觉问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: LLMs生成的内容可能存在不真实（幻觉）的问题，这在关键应用中是一个重大障碍，因此需要有效的方法来检测此类问题。

Method: 利用最大均值差异（MMD）作为非参数分布距离，通过跟踪不同温度参数下生成文档的MMD轨迹来检测幻觉。

Result: 在机器翻译数据集上，MMD-Flagger表现优于其他方法，能够有效检测大多数幻觉。

Conclusion: MMD-Flagger是一种有效的幻觉检测工具，适用于LLMs生成内容的真实性问题。

Abstract: Large language models (LLMs) have become pervasive in our everyday life. Yet,
a fundamental obstacle prevents their use in many critical applications: their
propensity to generate fluent, human-quality content that is not grounded in
reality. The detection of such hallucinations is thus of the highest
importance. In this work, we propose a new method to flag hallucinated content,
MMD-Flagger. It relies on Maximum Mean Discrepancy (MMD), a non-parametric
distance between distributions. On a high-level perspective, MMD-Flagger tracks
the MMD between the generated documents and documents generated with various
temperature parameters. We show empirically that inspecting the shape of this
trajectory is sufficient to detect most hallucinations. This novel method is
benchmarked on two machine translation datasets, on which it outperforms
natural competitors.

</details>


### [171] [AdaRewriter: Unleashing the Power of Prompting-based Conversational Query Reformulation via Test-Time Adaptation](https://arxiv.org/abs/2506.01381)
*Yilong Lai,Jialong Wu,Zhenglin Wang,Deyu Zhou*

Key words: 对话搜索,查询重写,测试时适应,奖励模型

TL;DR: AdaRewriter是一种基于测试时适应的查询重写框架，通过轻量级奖励模型选择最佳重写方案，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法在训练时或测试时均未能充分释放提示式查询重写的潜力，因此提出AdaRewriter以优化效果。

Method: AdaRewriter使用基于奖励模型的对比排序损失，在推理阶段选择最优重写方案，适用于黑盒系统如商业LLM API。

Result: 在五个对话搜索数据集上的实验表明，AdaRewriter在多数设置中显著优于现有方法。

Conclusion: AdaRewriter展示了测试时适应在对话查询重写中的潜力。

Abstract: Prompting-based conversational query reformulation has emerged as a powerful
approach for conversational search, refining ambiguous user queries into
standalone search queries. Best-of-N reformulation over the generated
candidates via prompting shows impressive potential scaling capability.
However, both the previous tuning methods (training time) and adaptation
approaches (test time) can not fully unleash their benefits. In this paper, we
propose AdaRewriter, a novel framework for query reformulation using an
outcome-supervised reward model via test-time adaptation. By training a
lightweight reward model with contrastive ranking loss, AdaRewriter selects the
most promising reformulation during inference. Notably, it can operate
effectively in black-box systems, including commercial LLM APIs. Experiments on
five conversational search datasets show that AdaRewriter significantly
outperforms the existing methods across most settings, demonstrating the
potential of test-time adaptation for conversational query reformulation.

</details>


### [172] [Speech-to-Speech Translation Pipelines for Conversations in Low-Resource Languages](https://arxiv.org/abs/2506.01406)
*Andrei Popescu-Belis,Alexis Allemann,Teo Ferrari,Gopal Krishnamani*

Key words: 语音翻译,低资源语言,微调模型,社区口译,多指标评估

TL;DR: 研究比较了低资源语言（土耳其语和普什图语与法语）的自动语音翻译质量，评估了60多个系统管线，并确定了最佳方案。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 自动语音翻译在低资源语言对中质量差异大，研究旨在通过数据微调和多指标评估提升性能。

Method: 收集数据、微调模型，比较多种管线（本地和云端），使用BLEU、COMET、BLASER等指标及人工评估。

Result: 确定了每种语言对的最佳管线，发现组件性能与管线其他部分独立。

Conclusion: 社区口译中，低资源语言对的自动翻译可通过组件优化提升质量。

Abstract: The popularity of automatic speech-to-speech translation for human
conversations is growing, but the quality varies significantly depending on the
language pair. In a context of community interpreting for low-resource
languages, namely Turkish and Pashto to/from French, we collected fine-tuning
and testing data, and compared systems using several automatic metrics (BLEU,
COMET, and BLASER) and human assessments. The pipelines included automatic
speech recognition, machine translation, and speech synthesis, with local
models and cloud-based commercial ones. Some components have been fine-tuned on
our data. We evaluated over 60 pipelines and determined the best one for each
direction. We also found that the ranks of components are generally independent
of the rest of the pipeline.

</details>


### [173] [Comparing LLM-generated and human-authored news text using formal syntactic theory](https://arxiv.org/abs/2506.01407)
*Olga Zamaraeva,Dan Flickinger,Francis Bond,Carlos Gómez-Rodríguez*

Key words: 大型语言模型, HPSG, 句法分析, 纽约时报, 文本生成

TL;DR: 该研究首次全面比较了六种大型语言模型生成的《纽约时报》风格文本与真实人类撰写的文本，基于形式句法理论HPSG揭示了系统性的语法分布差异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索大型语言模型与人类在《纽约时报》风格文本上的句法行为差异。

Method: 使用HPSG对文本的语法结构进行分析，比较语法类型的分布。

Result: 揭示了人类与LLM生成文本在HPSG语法类型分布上的系统性差异。

Conclusion: 研究增进了对LLM和人类在特定文本类型中句法行为的理解。

Abstract: This study provides the first comprehensive comparison of New York
Times-style text generated by six large language models against real,
human-authored NYT writing. The comparison is based on a formal syntactic
theory. We use Head-driven Phrase Structure Grammar (HPSG) to analyze the
grammatical structure of the texts. We then investigate and illustrate the
differences in the distributions of HPSG grammar types, revealing systematic
distinctions between human and LLM-generated writing. These findings contribute
to a deeper understanding of the syntactic behavior of LLMs as well as humans,
within the NYT genre.

</details>


### [174] [UniversalCEFR: Enabling Open Multilingual Research on Language Proficiency Assessment](https://arxiv.org/abs/2506.01419)
*Joseph Marvin Imperial,Abdullah Barayan,Regina Stodden,Rodrigo Wilkens,Ricardo Munoz Sanchez,Lingyun Gao,Melissa Torgbi,Dawn Knight,Gail Forey,Reka R. Jablonkai,Ekaterina Kochmar,Robert Reynolds,Eugenio Ribeiro,Horacio Saggion,Elena Volodina,Sowmya Vajjala,Thomas Francois,Fernando Alva-Manchego,Harish Tayyar Madabushi*

Key words: UniversalCEFR, CEFR, 多语言数据集, 语言能力评估, 自动化可读性

TL;DR: UniversalCEFR是一个多语言多维度的数据集，包含13种语言的文本，标注了CEFR等级。数据集包含505,807个文本，用于自动化可读性和语言能力评估研究。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为自动化可读性和语言能力评估提供开放的、标准化的多语言数据集，支持全球研究社区的统一研究和建模。

Method: 通过三种模型范式进行基准实验：基于语言特征的分类、微调预训练大模型，以及基于描述符的指令调优大模型提示。

Result: 实验结果支持在CEFR等级评估中使用语言特征和微调预训练模型。

Conclusion: UniversalCEFR通过标准化数据格式和提升可访问性，旨在推动语言能力研究中的数据共享和研究实践。

Abstract: We introduce UniversalCEFR, a large-scale multilingual multidimensional
dataset of texts annotated according to the CEFR (Common European Framework of
Reference) scale in 13 languages. To enable open research in both automated
readability and language proficiency assessment, UniversalCEFR comprises
505,807 CEFR-labeled texts curated from educational and learner-oriented
resources, standardized into a unified data format to support consistent
processing, analysis, and modeling across tasks and languages. To demonstrate
its utility, we conduct benchmark experiments using three modelling paradigms:
a) linguistic feature-based classification, b) fine-tuning pre-trained LLMs,
and c) descriptor-based prompting of instruction-tuned LLMs. Our results
further support using linguistic features and fine-tuning pretrained models in
multilingual CEFR level assessment. Overall, UniversalCEFR aims to establish
best practices in data distribution in language proficiency research by
standardising dataset formats and promoting their accessibility to the global
research community.

</details>


### [175] [Self-Refining Language Model Anonymizers via Adversarial Distillation](https://arxiv.org/abs/2506.01420)
*Kyuyoung Kim,Hyunjun Jeon,Jinwoo Shin*

Key words: 大语言模型, 隐私风险, 匿名化, 对抗蒸馏, 小型语言模型

TL;DR: 论文提出了一种名为SEAL的新框架，通过对抗蒸馏技术训练小型语言模型（SLMs），以在不依赖外部昂贵模型的情况下实现高效匿名化文本。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大语言模型（LLMs）在敏感领域中的使用日益增多，但其从看似无害的文本中推断个人信息的能力引发隐私风险。现有的匿名化方法通常依赖专有模型（如GPT-4），增加了成本和数据泄露的风险。

Method: SEAL框架通过LLM匿名器和推断模型的对抗交互，收集匿名化文本和推断属性的轨迹，并使用监督微调和偏好学习将匿名化、对抗推断和效用评估能力蒸馏到SLMs中。

Result: 在SynthPAI数据集上的实验表明，SEAL训练的SLMs在匿名化能力上有显著提升。8B模型在隐私与效用权衡上媲美GPT-4匿名器，并通过自细化在隐私方面超越它。

Conclusion: SEAL框架证明了通过对抗蒸馏训练SLMs作为高效匿名化工具的可行性，并公开了实验数据集以促进进一步研究。

Abstract: Large language models (LLMs) are increasingly used in sensitive domains,
where their ability to infer personal data from seemingly benign text poses
emerging privacy risks. While recent LLM-based anonymization methods help
mitigate such risks, they often rely on proprietary models (e.g., GPT-4),
raising concerns about cost and the potential exposure of sensitive data to
untrusted external systems. To address this, we introduce SElf-refining
Anonymization with Language model (SEAL), a novel distillation framework for
training small language models (SLMs) to perform effective anonymization
without relying on external costly models at inference time. We leverage
adversarial interactions between an LLM anonymizer and an inference model to
collect trajectories of anonymized texts and inferred attributes, which are
used to distill anonymization, adversarial inference, and utility evaluation
capabilities into SLMs via supervised fine-tuning and preference learning. The
resulting models learn to both anonymize text and critique their outputs,
enabling iterative improvement of anonymization quality via self-refinement.
Experiments on SynthPAI, a dataset of synthetic personal profiles and text
comments, demonstrate that SLMs trained with SEAL achieve substantial
improvements in anonymization capabilities. Notably, 8B models attain a
privacy-utility trade-off comparable to that of the GPT-4 anonymizer and, with
self-refinement, even surpass it in terms of privacy. These results show the
effectiveness of our adversarial distillation framework in training SLMs as
efficient anonymizers. To facilitate further research, we release the full
dataset used in our experiments.

</details>


### [176] [Redundancy, Isotropy, and Intrinsic Dimensionality of Prompt-based Text Embeddings](https://arxiv.org/abs/2506.01435)
*Hayato Tsukagoshi,Ryohei Sasano*

Key words: 提示嵌入, 降维, 内在维度, 各向同性, 冗余性

TL;DR: 论文研究了通过对高维提示嵌入进行降维对任务性能的影响，发现即使大幅降维，性能下降很小，揭示了嵌入的高度冗余性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 提示嵌入模型虽然性能优异，但高维度带来存储和计算成本问题，研究降维对任务性能的影响。

Method: 对嵌入应用后处理降维，并通过内在维度和各向同性分析冗余性。

Result: 即使降维到原始维度的25%甚至0.5%，分类和聚类任务性能下降很小，揭示了嵌入的高度冗余性。

Conclusion: 高维提示嵌入具有显著冗余性，降维可显著降低成本而不显著影响性能。

Abstract: Prompt-based text embedding models, which generate task-specific embeddings
upon receiving tailored prompts, have recently demonstrated remarkable
performance. However, their resulting embeddings often have thousands of
dimensions, leading to high storage costs and increased computational costs of
embedding-based operations. In this paper, we investigate how post-hoc
dimensionality reduction applied to the embeddings affects the performance of
various tasks that leverage these embeddings, specifically classification,
clustering, retrieval, and semantic textual similarity (STS) tasks. Our
experiments show that even a naive dimensionality reduction, which keeps only
the first 25% of the dimensions of the embeddings, results in a very slight
performance degradation, indicating that these embeddings are highly redundant.
Notably, for classification and clustering, even when embeddings are reduced to
less than 0.5% of the original dimensionality the performance degradation is
very small. To quantitatively analyze this redundancy, we perform an analysis
based on the intrinsic dimensionality and isotropy of the embeddings. Our
analysis reveals that embeddings for classification and clustering, which are
considered to have very high dimensional redundancy, exhibit lower intrinsic
dimensionality and less isotropy compared with those for retrieval and STS.

</details>


### [177] [Whale: Large-Scale multilingual ASR model with w2v-BERT and E-Branchformer with large speech data](https://arxiv.org/abs/2506.01439)
*Yosuke Kashiwagi,Hayato Futami,Emiru Tsunoo,Satoshi Asakawa*

Key words: 语音识别, Whale, w2v-BERT, E-Branchformer, 联合CTC-注意力

TL;DR: Whale是一个大规模语音识别模型，结合了w2v-BERT自监督模型和E-Branchformer架构，通过多样化的数据训练，在多个基准测试中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 开发高性能的语音识别模型，以应对不同说话风格和声学条件的挑战。

Method: 采用w2v-BERT自监督模型和E-Branchformer架构，结合联合CTC-注意力解码策略，利用多样化数据集训练。

Result: 在Librispeech和CSJ测试集上分别达到2.4%和3.4%的错误率，优于Whisper和OWSM。

Conclusion: Whale模型通过大规模数据和先进架构，显著提升了语音识别性能。

Abstract: This paper reports on the development of a large-scale speech recognition
model, Whale. Similar to models such as Whisper and OWSM, Whale leverages both
a large model size and a diverse, extensive dataset. Whale's architecture
integrates w2v-BERT self-supervised model, an encoder-decoder backbone built on
E-Branchformer, and a joint CTC-attention decoding strategy. The training
corpus comprises varied speech data, of not only public corpora but also
in-house data, thereby enhancing the model's robustness to different speaking
styles and acoustic conditions. Through evaluations on multiple benchmarks,
Whale achieved comparable performance to existing models. In particular, it
achieves a word error rate of 2.4% on the Librispeech test-clean set and a
character error rate of 3.4% on the CSJ eval3 set, outperforming Whisper
large-v3 and OWSM v3.1.

</details>


### [178] [Building Entity Association Mining Framework for Knowledge Discovery](https://arxiv.org/abs/2506.01451)
*Anshika Rawal,Abhijeet Kumar,Mridul Mishra*

Key words: 文本挖掘, 实体提取, 关联挖掘, 金融用例, 共现图

TL;DR: 本文提出了一种通用的文本挖掘框架，用于文档过滤、实体提取和关联挖掘，支持金融用例如品牌产品发现和供应商风险监控。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决从非结构化文本中提取有用信号或模式以支持业务决策（如投资产品分析、客户偏好发现和风险监控）的挑战。

Method: 框架包含三个主要组件：1）文档过滤；2）可配置的实体提取管道（使用DBpedia Spotlight、Spacy NER等技术）；3）关联关系挖掘（生成共现图分析实体关系）。

Result: 框架在金融用例中展示了其有效性，减少了重复开发工作并支持快速原型设计。

Conclusion: 该框架提高了文本挖掘的效率，促进了关联挖掘业务应用的可重用性和快速开发。

Abstract: Extracting useful signals or pattern to support important business decisions
for example analyzing investment product traction and discovering customer
preference, risk monitoring etc. from unstructured text is a challenging task.
Capturing interaction of entities or concepts and association mining is a
crucial component in text mining, enabling information extraction and reasoning
over and knowledge discovery from text. Furthermore, it can be used to enrich
or filter knowledge graphs to guide exploration processes, descriptive
analytics and uncover hidden stories in the text. In this paper, we introduce a
domain independent pipeline i.e., generalized framework to enable document
filtering, entity extraction using various sources (or techniques) as plug-ins
and association mining to build any text mining business use-case and
quantitatively define a scoring metric for ranking purpose. The proposed
framework has three major components a) Document filtering: filtering
documents/text of interest from massive amount of texts b) Configurable entity
extraction pipeline: include entity extraction techniques i.e., i) DBpedia
Spotlight, ii) Spacy NER, iii) Custom Entity Matcher, iv) Phrase extraction (or
dictionary) based c) Association Relationship Mining: To generates
co-occurrence graph to analyse potential relationships among entities,
concepts. Further, co-occurrence count based frequency statistics provide a
holistic window to observe association trends or buzz rate in specific business
context. The paper demonstrates the usage of framework as fundamental building
box in two financial use-cases namely brand product discovery and vendor risk
monitoring. We aim that such framework will remove duplicated effort, minimize
the development effort, and encourage reusability and rapid prototyping in
association mining business applications for institutions.

</details>


### [179] [TalTech Systems for the Interspeech 2025 ML-SUPERB 2.0 Challenge](https://arxiv.org/abs/2506.01458)
*Tanel Alumäe,Artem Fedorchenko*

Key words: 语音识别, 多语言处理, 语言识别, 预训练模型

TL;DR: 本文介绍了为Interspeech 2025 ML-SUPERB 2.0挑战赛开发的多语言语音识别系统，采用混合语言识别方法，结合预训练语言嵌入模型和轻量级语音识别模型，最终获得挑战赛最高分。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 开发高效的多语言语音识别系统，以应对Interspeech挑战赛的需求。

Method: 使用预训练语言嵌入模型和轻量级语音识别模型，结合语言特定的大语言模型，并在不同语言中动态选择最优语音识别模型。

Result: 系统在挑战赛中取得了最高的综合评分。

Conclusion: 混合语言识别方法和动态模型选择策略在多语言语音识别任务中表现出色。

Abstract: This paper describes the language identification and multilingual speech
recognition system developed at Tallinn University of Technology for the
Interspeech 2025 ML-SUPERB 2.0 Challenge. A hybrid language identification
system is used, consisting of a pretrained language embedding model and a
light-weight speech recognition model with a shared encoder across languages
and language-specific bigram language models. For speech recognition, three
models are used, where only a single model is applied for each language,
depending on the training data availability and performance on held-out data.
The model set consists of a finetuned version of SeamlessM4T, MMS-1B-all with
custom language adapters and MMS-zeroshot. The system obtained the top overall
score in the challenge.

</details>


### [180] [Integrating Neural and Symbolic Components in a Model of Pragmatic Question-Answering](https://arxiv.org/abs/2506.01474)
*Polina Tsvilodub,Robert D. Hawkins,Michael Franke*

Key words: 神经符号框架, LLM, 实用性语言, 认知模型, 问答系统

TL;DR: 论文提出了一种神经符号框架，通过集成基于LLM的模块来增强概率认知模型，减少人工干预，从而提升实用性语言模型的灵活性和可扩展性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统实用性语言计算模型依赖手工定义的语句和意义，限制了其在真实语言环境中的应用。

Method: 提出神经符号框架，通过LLM模块生成和评估自然语言中的关键组件，并在经典的实用性问答案例中比较多种神经模块整合方法。

Result: 混合模型在预测人类答案模式方面与传统概率模型相当或更好，但神经符号模型的成功依赖于LLM的整合方式。

Conclusion: 神经符号模型为实用性语言提供了更灵活和可扩展的解决方案，但需注意神经与符号组件的平衡设计。

Abstract: Computational models of pragmatic language use have traditionally relied on
hand-specified sets of utterances and meanings, limiting their applicability to
real-world language use. We propose a neuro-symbolic framework that enhances
probabilistic cognitive models by integrating LLM-based modules to propose and
evaluate key components in natural language, eliminating the need for manual
specification. Through a classic case study of pragmatic question-answering, we
systematically examine various approaches to incorporating neural modules into
the cognitive model -- from evaluating utilities and literal semantics to
generating alternative utterances and goals. We find that hybrid models can
match or exceed the performance of traditional probabilistic models in
predicting human answer patterns. However, the success of the neuro-symbolic
model depends critically on how LLMs are integrated: while they are
particularly effective for proposing alternatives and transforming abstract
goals into utilities, they face challenges with truth-conditional semantic
evaluation. This work charts a path toward more flexible and scalable models of
pragmatic language use while illuminating crucial design considerations for
balancing neural and symbolic components.

</details>


### [181] [LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech Detoxification](https://arxiv.org/abs/2506.01484)
*Shuzhou Yuan,Ercong Nie,Lukas Kouba,Ashish Yashwanth Kangen,Helmut Schmid,Hinrich Schutze,Michael Farber*

Key words: 解毒化, 仇恨言论, 人工标注, GPT-4o-mini, PARADEHATE

TL;DR: 论文提出了一种利用GPT-4o-mini的LLM-in-the-loop管道，自动生成高质量的仇恨言论解毒数据集PARADEHATE，并通过实验验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在线毒性内容日益增多，而高质量并行数据集稀缺，尤其是在仇恨言论领域，人工标注成本高且敏感。

Method: 利用GPT-4o-mini替代人工标注，构建大型并行数据集PARADEHATE，并通过BART等模型进行微调和评估。

Result: 实验表明，基于PARADEHATE微调的模型在风格准确性、内容保留和流畅性上表现优越，验证了LLM生成数据的可扩展性。

Conclusion: LLM生成的解毒文本可有效替代人工标注，为自动解毒提供了一种可扩展的解决方案。

Abstract: Detoxification, the task of rewriting harmful language into non-toxic text,
has become increasingly important amid the growing prevalence of toxic content
online. However, high-quality parallel datasets for detoxification, especially
for hate speech, remain scarce due to the cost and sensitivity of human
annotation. In this paper, we propose a novel LLM-in-the-loop pipeline
leveraging GPT-4o-mini for automated detoxification. We first replicate the
ParaDetox pipeline by replacing human annotators with an LLM and show that the
LLM performs comparably to human annotation. Building on this, we construct
PARADEHATE, a large-scale parallel dataset specifically for hatespeech
detoxification. We release PARADEHATE as a benchmark of over 8K hate/non-hate
text pairs and evaluate a wide range of baseline methods. Experimental results
show that models such as BART, fine-tuned on PARADEHATE, achieve better
performance in style accuracy, content preservation, and fluency, demonstrating
the effectiveness of LLM-generated detoxification text as a scalable
alternative to human annotation.

</details>


### [182] [Argument-Centric Causal Intervention Method for Mitigating Bias in Cross-Document Event Coreference Resolution](https://arxiv.org/abs/2506.01488)
*Long Yao,Wenzhong Yang,Yabo Yin,Fuyuan Wei,Hongzhen Lv,Jiaren Peng,Liejun Wang,Xiaoming Tao*

Key words: 跨文档事件共指消解, 因果干预, 反事实推理, 论元语义, 虚假相关性

TL;DR: 提出了一种基于Argument-Centric Causal Intervention（ACCI）的新方法，解决跨文档事件共指消解中的虚假相关性问题，通过因果干预和反事实推理提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前跨文档事件共指消解方法过度依赖触发词特征，导致表面词汇特征与共指关系间的虚假相关性，影响模型性能。

Method: 构建结构因果图揭示词汇触发与共指标签间的混杂依赖，引入后门调整干预分离论元语义的真实因果效应，并结合反事实推理模块和论元感知增强模块。

Result: 在ECB+和GVC数据集上分别取得88.4%和85.2%的CoNLL F1分数，达到最先进性能。

Conclusion: ACCI为跨文档事件共指消解提供了一种有效的去偏差方法，无需数据增强或启发式过滤，在端到端框架中实现显著性能提升。

Abstract: Cross-document Event Coreference Resolution (CD-ECR) is a fundamental task in
natural language processing (NLP) that seeks to determine whether event
mentions across multiple documents refer to the same real-world occurrence.
However, current CD-ECR approaches predominantly rely on trigger features
within input mention pairs, which induce spurious correlations between
surface-level lexical features and coreference relationships, impairing the
overall performance of the models. To address this issue, we propose a novel
cross-document event coreference resolution method based on Argument-Centric
Causal Intervention (ACCI). Specifically, we construct a structural causal
graph to uncover confounding dependencies between lexical triggers and
coreference labels, and introduce backdoor-adjusted interventions to isolate
the true causal effect of argument semantics. To further mitigate spurious
correlations, ACCI integrates a counterfactual reasoning module that quantifies
the causal influence of trigger word perturbations, and an argument-aware
enhancement module to promote greater sensitivity to semantically grounded
information. In contrast to prior methods that depend on costly data
augmentation or heuristic-based filtering, ACCI enables effective debiasing in
a unified end-to-end framework without altering the underlying training
procedure. Extensive experiments demonstrate that ACCI achieves CoNLL F1 of
88.4% on ECB+ and 85.2% on GVC, achieving state-of-the-art performance. The
implementation and materials are available at https://github.com/era211/ACCI.

</details>


### [183] [Multilingual Definition Modeling](https://arxiv.org/abs/2506.01489)
*Edison Marrese-Taylor,Erica K. Shimomoto,Alfredo Solano,Enrique Reid*

Key words: 多语言定义建模, 预训练模型, 大型语言模型, 零样本学习, BERTScore

TL;DR: 本文首次提出多语言定义建模研究，测试了预训练多语言模型在新语言（西班牙语、法语、葡萄牙语和德语）上的表现，并评估了两种大型语言模型（LLM）的零样本能力。结果表明，多语言模型与英语表现相当，但无法利用跨语言协同效应，而LLM表现更优。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究多语言定义建模的潜力，探索预训练多语言模型和大型语言模型在新任务中的表现。

Method: 使用四种新语言的单语词典数据，对预训练多语言模型进行微调，并采用零样本方法测试两种LLM的表现。

Result: 多语言模型与英语表现相当，但LLM整体表现更优。BERTScore与多语言LLM基准性能强相关。

Conclusion: 多语言定义建模任务为计算约束、稳定且自然的替代方案提供可行性，同时揭示了LLM的潜力与局限。

Abstract: In this paper, we propose the first multilingual study on definition
modeling. We use monolingual dictionary data for four new languages (Spanish,
French, Portuguese, and German) and perform an in-depth empirical study to test
the performance of pre-trained multilingual language models on definition
modeling of monosemic words when finetuned on this data. Furthermore, we use a
zero-shot approach to test the multilingual capabilities of two popular
chat-based Large Language Models (LLMs) in the task. Results show that
multilingual language models can perform on-pair with English but cannot
leverage potential cross-lingual synergies, with LLMs generally offering better
performance overall. A comprehensive human evaluation of the LLM-generated
definition highlights the zero and few-shot capabilities of these models in
this new task, also showing their shortcomings. Finally, we show that
performance on our task via BERTScore strongly correlates to the performance on
multilingual LLM benchmarks, suggesting that our task offers a viable
compute-constrained, stable and natural alternative to these.

</details>


### [184] [CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models](https://arxiv.org/abs/2506.01495)
*Ping Wu,Guobin Shen,Dongcheng Zhao,Yuwei Wang,Yiting Dong,Yu Shi,Enmeng Lu,Feifei Zhao,Yi Zeng*

Key words: 大型语言模型、价值观对齐、中文价值观语料库、文化适应性、道德困境

TL;DR: 提出了一种基于中国核心价值观的分层价值框架，构建了大规模中文价值观语料库（CVC），用于评估和调整大型语言模型（LLMs）的价值观，实验证明其在不同文化背景下的有效性和适用性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前价值观评估和调整存在西方文化偏见和本土框架不完整的问题，且缺乏可扩展的规则驱动场景生成方法，导致评估成本高且不全面。

Method: 开发了包含3个主要维度、12个核心价值观和50个派生价值观的分层价值框架，并构建了超过25万条规则的中文价值观语料库（CVC）。

Result: CVC生成的场景在价值边界和内容多样性上优于直接生成的场景，主流LLMs在70.5%的案例中偏好CVC选项，中国人类标注者与CVC的契合度达87.5%。

Conclusion: CVC为全面价值观评估和调整提供了一个文化适应性强的基准框架，体现了中国特色。

Abstract: Ensuring that Large Language Models (LLMs) align with mainstream human values
and ethical norms is crucial for the safe and sustainable development of AI.
Current value evaluation and alignment are constrained by Western cultural bias
and incomplete domestic frameworks reliant on non-native rules; furthermore,
the lack of scalable, rule-driven scenario generation methods makes evaluations
costly and inadequate across diverse cultural contexts. To address these
challenges, we propose a hierarchical value framework grounded in core Chinese
values, encompassing three main dimensions, 12 core values, and 50 derived
values. Based on this framework, we construct a large-scale Chinese Values
Corpus (CVC) containing over 250,000 value rules enhanced and expanded through
human annotation. Experimental results show that CVC-guided scenarios
outperform direct generation ones in value boundaries and content diversity. In
the evaluation across six sensitive themes (e.g., surrogacy, suicide), seven
mainstream LLMs preferred CVC-generated options in over 70.5% of cases, while
five Chinese human annotators showed an 87.5% alignment with CVC, confirming
its universality, cultural relevance, and strong alignment with Chinese values.
Additionally, we construct 400,000 rule-based moral dilemma scenarios that
objectively capture nuanced distinctions in conflicting value prioritization
across 17 LLMs. Our work establishes a culturally-adaptive benchmarking
framework for comprehensive value evaluation and alignment, representing
Chinese characteristics. All data are available at
https://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at
https://github.com/Beijing-AISI/CVC.

</details>


### [185] [Continual Speech Learning with Fused Speech Features](https://arxiv.org/abs/2506.01496)
*Guitao Wang,Jinming Zhao,Hao Yang,Guilin Qi,Tongtong Wu,Gholamreza Haffari*

Key words: 语音处理,自适应模型,门控融合,Whisper模型

TL;DR: 提出了一种连续语音学习框架，通过可学习的门控融合层动态选择任务特征，显著提升了六种语音任务的准确率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统静态语音模型无法适应快速增长和多样化的语音数据，亟需动态自适应方法。

Method: 基于Whisper编码器-解码器模型，引入可学习的门控融合层，动态选择任务特征进行下游任务适配。

Result: 在六种语音任务中，准确性显著优于传统方法，且无需完全重新训练即可适应新任务。

Conclusion: 连续语音学习框架有效解决了语音模型的动态适配问题，具有实用价值。

Abstract: Rapid growth in speech data demands adaptive models, as traditional static
methods fail to keep pace with dynamic and diverse speech information. We
introduce continuous speech learning, a new set-up targeting at bridging the
adaptation gap in current speech models. We use the encoder-decoder Whisper
model to standardize speech tasks into a generative format. We integrate a
learnable gated-fusion layer on the top of the encoder to dynamically select
task-specific features for downstream tasks. Our approach improves accuracy
significantly over traditional methods in six speech processing tasks,
demonstrating gains in adapting to new speech tasks without full retraining.

</details>


### [186] [Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes](https://arxiv.org/abs/2506.01512)
*Meng Li,Michael Vrazitulis,David Schlangen*

Key words: 大型语言模型, 认识模态, 不确定性表达, 类型学框架

TL;DR: 论文探讨了大型语言模型在不确定现实环境中生成基于事实评估和信心的表述时的局限性，指出其表达的可靠性不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究者希望评估大型语言模型在生成基于信心评估的表达时的表现，并填补对其不确定性语言知识缺乏系统研究的空白。

Method: 利用类型学框架和受控故事，评估模型对认识模态的知识。

Result: 实验显示模型在生成认识表达时表现有限且不稳定，其不确定性表达不可靠。

Conclusion: 为构建具有不确定性意识的大型语言模型，需增强其对认识模态的语义知识。

Abstract: Rational speakers are supposed to know what they know and what they do not
know, and to generate expressions matching the strength of evidence. In
contrast, it is still a challenge for current large language models to generate
corresponding utterances based on the assessment of facts and confidence in an
uncertain real-world environment. While it has recently become popular to
estimate and calibrate confidence of LLMs with verbalized uncertainty, what is
lacking is a careful examination of the linguistic knowledge of uncertainty
encoded in the latent space of LLMs. In this paper, we draw on typological
frameworks of epistemic expressions to evaluate LLMs' knowledge of epistemic
modality, using controlled stories. Our experiments show that the performance
of LLMs in generating epistemic expressions is limited and not robust, and
hence the expressions of uncertainty generated by LLMs are not always reliable.
To build uncertainty-aware LLMs, it is necessary to enrich semantic knowledge
of epistemic modality in LLMs.

</details>


### [187] [FormFactory: An Interactive Benchmarking Suite for Multimodal Form-Filling Agents](https://arxiv.org/abs/2506.01520)
*Bobo Li,Yuheng Wang,Hao Fei,Juncheng Li,Wei Ji,Mong-Li Lee,Wynne Hsu*

Key words: 表单填写，多模态大语言模型，视觉布局推理，基准测试，FormFactory

TL;DR: 论文提出FormFactory，一个用于自动化表单填写的交互式基准测试套件，揭示了当前多模态大语言模型（MLLMs）在表单填写任务中的局限性（准确率不足5%）。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在线表单填写是一项常见但劳动密集的任务，现有工具多为基于规则且缺乏泛化能力。多模态大语言模型（MLLMs）在GUI相关任务中表现出潜力，但在表单填写任务中面临独特挑战。

Method: 提出FormFactory，一个包含网页界面、后端评估模块和数据集的基准测试套件，模拟真实场景和多样化的表单交互形式。

Result: 对当前最先进的MLLMs进行评估，结果显示没有模型超过5%的准确率，突显了任务难度及模型在视觉布局推理和字段值对齐能力的不足。

Conclusion: FormFactory可作为进一步研究稳健、实用表单填写代理的基础，揭示了当前模型的局限性。

Abstract: Online form filling is a common yet labor-intensive task involving extensive
keyboard and mouse interactions. Despite the long-standing vision of automating
this process with "one click", existing tools remain largely rule-based and
lack generalizable, generative capabilities. Recent advances in Multimodal
Large Language Models (MLLMs) have enabled promising agents for GUI-related
tasks in general-purpose scenarios. However, they struggle with the unique
challenges of form filling, such as flexible layouts and the difficulty of
aligning textual instructions with on-screen fields. To bridge this gap, we
formally define the form-filling task and propose FormFactory, an interactive
benchmarking suite comprising a web-based interface, backend evaluation module,
and carefully constructed dataset. Our benchmark covers diverse real-world
scenarios, incorporates various field formats, and simulates high-fidelity form
interactions. We conduct a comprehensive evaluation of state-of-the-art MLLMs
and observe that no model surpasses 5% accuracy, underscoring the inherent
difficulty of the task. These findings also reveal significant limitations in
current models' visual layout reasoning and field-value alignment abilities. We
hope our benchmark can serve as a stepping stone for further research into
robust, practical form-filling agents.

</details>


### [188] [V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat](https://arxiv.org/abs/2506.01524)
*Qi Lin,Weikai Xu,Lisi Chen,Bin Dai*

Key words: 大语言模型，个性化聊天，变分自编码，细粒度控制，高质量数据

TL;DR: 论文提出了一种Verbalfine-grained细节的对话框架V-VAE，通过动态适应可解释的潜在变量生成一致性高的个性化对话，并构建了高质量数据集HumanChatData和评估标准HumanChatBench，实验证明其优于现有基线。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决现有基于角色的聊天方法在动态、细粒度和高质量数据上的不足，生成更符合人类对话风格的响应。

Method: 采用变分自编码（V-VAE）框架，结合细粒度控制空间和动态适应模块，从高质量数据HumanChatData中学习潜在变量。

Result: 在HumanChatBench和DialogBench上，基于V-VAE的LLM表现优于标准基线。

Conclusion: V-VAE和HumanChatData有效提升了对话生成的质量和一致性。

Abstract: With the continued proliferation of Large Language Model (LLM) based
chatbots, there is a growing demand for generating responses that are not only
linguistically fluent but also consistently aligned with persona-specific
traits in conversations. However, existing role-play and persona-based chat
approaches rely heavily on static role descriptions, coarse-grained signal
space, and low-quality synthetic data, which fail to capture dynamic
fine-grained details in human-like chat. Human-like chat requires modeling
subtle latent traits, such as emotional tone, situational awareness, and
evolving personality, which are difficult to predefine and cannot be easily
learned from synthetic or distillation-based data. To address these
limitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework,
containing a variational auto-encoding module and fine-grained control space
which dynamically adapts dialogue behaviour based on fine-grained,
interpretable latent variables across talking style, interaction patterns, and
personal attributes. We also construct a high-quality dataset, HumanChatData,
and benchmark HumanChatBench to address the scarcity of high-quality data in
the human-like domain. Experiments show that LLMs based on V-VAE consistently
outperform standard baselines on HumanChatBench and DialogBench, which further
demonstrates the effectiveness of V-VAE and HumanChatData.

</details>


### [189] [STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop Multi-Agent Framework](https://arxiv.org/abs/2506.01531)
*Wenhao Liu,Zhenyi Lu,Xinyu Hu,Jierui Zhang,Dailin Li,Jiacheng Cen,Huilin Cao,Haiteng Wang,Yuhan Li,Kun Xie,Dandan Li,Pei Zhang,Chengbo Zhang,Yuxiang Ren,Xiaohong Huang,Yan Ma*

Key words: 数学数据集,人类推理,多智能体生成,高难度基准

TL;DR: STORM-BORN是一个超具挑战性的数学数据集，源于前沿学术论文，包含密集的人类推理和启发式提示，解决了现有数据集内容过时、推理不足和可靠性低的问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有数学数据集存在内容过时、缺乏挑战性、忽视人类推理和可靠性不足的问题，STORM-BORN旨在解决这些问题。

Method: 采用人在回路的多智能体数据生成框架，结合推理密集型过滤器、多智能体协作和人类数学家的评估。

Result: 生成的2000个样本中，最难的100个问题即使是最先进的GPT-o1模型也仅解决不到5%，微调后LLaMA3-8B和Qwen2.5-7B的准确率分别提升7.84%和9.12%。

Conclusion: STORM-BORN为AI提供了高难度基准和人类推理训练资源，推动了数学模型推理能力的进步。

Abstract: High-quality math datasets are crucial for advancing the reasoning abilities
of large language models (LLMs). However, existing datasets often suffer from
three key issues: outdated and insufficient challenging content, neglecting
human-like reasoning, and limited reliability due to single-LLM generation. To
address these, we introduce $\textbf{STORM-BORN}$, an ultra-challenging dataset
of mathematical derivations sourced from cutting-edge academic papers, which
includes dense human-like approximations and heuristic cues. To ensure the
reliability and quality, we propose a novel human-in-the-loop, multi-agent data
generation framework, integrating reasoning-dense filters, multi-agent
collaboration, and human mathematicians' evaluations. We curated a set of 2,000
synthetic samples and deliberately selected the 100 most difficult problems.
Even most advanced models like GPT-o1 solved fewer than $5\%$ of them.
Fine-tuning on STORM-BORN boosts accuracy by $7.84\%$ (LLaMA3-8B) and $9.12\%$
(Qwen2.5-7B). As AI approaches mathematician-level reasoning, STORM-BORN
provides both a high-difficulty benchmark and a human-like reasoning training
resource. Our code and dataset are publicly available at
https://github.com/lwhere/STORM-BORN.

</details>


### [190] [Dictionaries to the Rescue: Cross-Lingual Vocabulary Transfer for Low-Resource Languages Using Bilingual Dictionaries](https://arxiv.org/abs/2506.01535)
*Haruki Sakajo,Yusuke Ide,Justin Vasselli,Yusuke Sakai,Yingtao Tian,Hidetaka Kamigaito,Taro Watanabe*

Key words: 跨语言,词汇迁移,低资源语言,双语词典,BPE

TL;DR: 提出了一种基于双语词典的词汇迁移方法，适用于低资源语言，通过逐步移除目标子词并迭代估计其嵌入，优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决低资源语言在词汇迁移中的挑战，利用广泛可用的双语词典。

Method: 利用BPE分词器特性，逐步移除目标子词并迭代估计其嵌入。

Result: 在低资源语言上优于现有方法。

Conclusion: 基于词典的跨语言词汇迁移方法简单有效。

Abstract: Cross-lingual vocabulary transfer plays a promising role in adapting
pre-trained language models to new languages, including low-resource languages.
Existing approaches that utilize monolingual or parallel corpora face
challenges when applied to languages with limited resources. In this work, we
propose a simple yet effective vocabulary transfer method that utilizes
bilingual dictionaries, which are available for many languages, thanks to
descriptive linguists. Our proposed method leverages a property of BPE
tokenizers where removing a subword from the vocabulary causes a fallback to
shorter subwords. The embeddings of target subwords are estimated iteratively
by progressively removing them from the tokenizer. The experimental results
show that our approach outperforms existing methods for low-resource languages,
demonstrating the effectiveness of a dictionary-based approach for
cross-lingual vocabulary transfer.

</details>


### [191] [Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation](https://arxiv.org/abs/2506.01565)
*Li Zhou,Lutong Yu,Dongchu Xie,Shaohuan Cheng,Wenyan Li,Haizhou Li*

Key words: 文化理解,视觉语言模型,Hanfu-Bench,时间维度,转创

TL;DR: 研究者提出了Hanfu-Bench数据集，专注于文化的时空多样性，用于评测视觉语言模型在理解传统文化和现代适应性方面的能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有研究多关注文化的地理多样性，忽略时间维度，因此需要填补这一空白。

Method: 通过专家策划的Hanfu-Bench数据集，包含文化视觉理解和图像转创两大任务，评测模型的性能。

Result: 封闭VLMs在文化视觉理解上与非专家相当，但落后专家10%；开放VLMs表现更差。转创任务中最佳模型成功率仅42%。

Conclusion: Hanfu-Bench揭示了时间文化理解和创意适应的重大挑战，为未来研究提供了测试平台。

Abstract: Culture is a rich and dynamic domain that evolves across both geography and
time. However, existing studies on cultural understanding with vision-language
models (VLMs) primarily emphasize geographic diversity, often overlooking the
critical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a
novel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning
ancient Chinese dynasties, serves as a representative cultural heritage that
reflects the profound temporal aspects of Chinese culture while remaining
highly popular in Chinese contemporary society. Hanfu-Bench comprises two core
tasks: cultural visual understanding and cultural image transcreation.The
former task examines temporal-cultural feature recognition based on single- or
multi-image inputs through multiple-choice visual question answering, while the
latter focuses on transforming traditional attire into modern designs through
cultural element inheritance and modern context adaptation. Our evaluation
shows that closed VLMs perform comparably to non-experts on visual cutural
understanding but fall short by 10\% to human experts, while open VLMs lags
further behind non-experts. For the transcreation task, multi-faceted human
evaluation indicates that the best-performing model achieves a success rate of
only 42\%. Our benchmark provides an essential testbed, revealing significant
challenges in this new direction of temporal cultural understanding and
creative adaptation.

</details>


### [192] [Prompt Engineering Large Language Models' Forecasting Capabilities](https://arxiv.org/abs/2506.01578)
*Philipp Schoenegger,Cameron R. Jones,Philip E. Tetlock,Barbara Mellers*

Key words: 大语言模型, 预测, 提示工程, 准确性

TL;DR: 研究表明，复杂的任务（如预测）中，简单的提示工程对准确性提升有限，某些策略甚至产生负面影响，需要更强或专业的技巧。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨在复杂领域（如预测）中，提示工程是否能显著提升大语言模型的表现。

Method: 测试了38种提示，涵盖多个模型，并引入复合提示和外部来源提示，包括推理模型o1和o1-mini。

Result: 大多数提示效果微弱，基率参考略有帮助，但贝叶斯推理提示显著降低准确性。

Conclusion: 复杂任务中，仅靠提示工程改进效果有限，需更稳健或专门的技术。

Abstract: Large language model performance can be improved in a large number of ways.
Many such techniques, like fine-tuning or advanced tool usage, are
time-intensive and expensive. Although prompt engineering is significantly
cheaper and often works for simpler tasks, it remains unclear whether prompt
engineering suffices for more complex domains like forecasting. Here we show
that small prompt modifications rarely boost forecasting accuracy beyond a
minimal baseline. In our first study, we tested 38 prompts across Claude 3.5
Sonnet, Claude 3.5 Haiku, GPT-4o, and Llama 3.1 405B. In our second, we
introduced compound prompts and prompts from external sources, also including
the reasoning models o1 and o1-mini. Our results show that most prompts lead to
negligible gains, although references to base rates yield slight benefits.
Surprisingly, some strategies showed strong negative effects on accuracy:
especially encouraging the model to engage in Bayesian reasoning. These results
suggest that, in the context of complex tasks like forecasting, basic prompt
refinements alone offer limited gains, implying that more robust or specialized
techniques may be required for substantial performance improvements in AI
forecasting.

</details>


### [193] [Unified Large Language Models for Misinformation Detection in Low-Resource Linguistic Settings](https://arxiv.org/abs/2506.01587)
*Muhammad Islam,Javed Ali Khan,Mohammed Abaker,Ali Daud,Azeem Irshad*

Key words: 假新闻检测,乌尔都语,大型语言模型,数据集,多模态特征提取

TL;DR: 论文提出了一种用于乌尔都语假新闻检测的新型数据集，并通过多种大型语言模型进行验证，提出了一种统一的LLM模型，性能优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于乌尔都语等资源受限语言缺乏可靠的假新闻检测数据集，本文旨在填补这一空白。

Method: 构建了一个公开的乌尔都语假新闻数据集，并使用多种LLM（如XLNet、mBERT等）进行验证，提出了一种统一的LLM模型。

Result: 统一的LLM模型在准确性、F1分数等指标上优于其他模型，并通过人类判断验证了样本结果。

Conclusion: 研究中提出的数据集和统一模型显著提升了乌尔都语假新闻检测的性能，为资源受限语言的FND提供了新思路。

Abstract: The rapid expansion of social media platforms has significantly increased the
dissemination of forged content and misinformation, making the detection of
fake news a critical area of research. Although fact-checking efforts
predominantly focus on English-language news, there is a noticeable gap in
resources and strategies to detect news in regional languages, such as Urdu.
Advanced Fake News Detection (FND) techniques rely heavily on large, accurately
labeled datasets. However, FND in under-resourced languages like Urdu faces
substantial challenges due to the scarcity of extensive corpora and the lack of
validated lexical resources. Current Urdu fake news datasets are often
domain-specific and inaccessible to the public. They also lack human
verification, relying mainly on unverified English-to-Urdu translations, which
compromises their reliability in practical applications. This study highlights
the necessity of developing reliable, expert-verified, and domain-independent
Urdu-enhanced FND datasets to improve fake news detection in Urdu and other
resource-constrained languages. This paper presents the first benchmark large
FND dataset for Urdu news, which is publicly available for validation and deep
analysis. We also evaluate this dataset using multiple state-of-the-art
pre-trained large language models (LLMs), such as XLNet, mBERT, XLM-RoBERTa,
RoBERTa, DistilBERT, and DeBERTa. Additionally, we propose a unified LLM model
that outperforms the others with different embedding and feature extraction
techniques. The performance of these models is compared based on accuracy, F1
score, precision, recall, and human judgment for vetting the sample results of
news.

</details>


### [194] [Statement-Tuning Enables Efficient Cross-lingual Generalization in Encoder-only Models](https://arxiv.org/abs/2506.01592)
*Ahmed Elshabrawy,Thanh-Nhi Nguyen,Yeeun Kang,Lihan Feng,Annant Jain,Faadil Abdullah Shaikh,Jonibek Mansurov,Mohamed Fazli Mohamed Imam,Jesus-German Ortiz-Barajas,Rendi Chevi,Alham Fikri Aji*

Key words: Large Language Models, BERT, RoBERTa, zero-shot, multilingual, efficiency

TL;DR: 该论文探讨了如何通过Statement Tuning方法使编码器模型（如BERT和RoBERTa）在零样本跨语言任务中表现优异，并与计算密集型的大语言模型竞争。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究目的是验证编码器模型能否在零样本跨语言任务中与大型语言模型（LLMs）竞争，同时降低计算和内存成本。

Method: 采用Statement Tuning方法，将任务重构为有限模板，并进行多语言实验。

Result: 实验结果表明，编码器模型在多语言任务中表现优异，与LLMs相当，但更高效。

Conclusion: 编码器模型可作为资源有限语言任务的高效替代方案，推动更具包容性的NLP研究。

Abstract: Large Language Models (LLMs) excel in zero-shot and few-shot tasks, but
achieving similar performance with encoder-only models like BERT and RoBERTa
has been challenging due to their architecture. However, encoders offer
advantages such as lower computational and memory costs. Recent work adapts
them for zero-shot generalization using Statement Tuning, which reformulates
tasks into finite templates. We extend this approach to multilingual NLP,
exploring whether encoders can achieve zero-shot cross-lingual generalization
and serve as efficient alternatives to memory-intensive LLMs for low-resource
languages. Our results show that state-of-the-art encoder models generalize
well across languages, rivaling multilingual LLMs while being more efficient.
We also analyze multilingual Statement Tuning dataset design, efficiency gains,
and language-specific generalization, contributing to more inclusive and
resource-efficient NLP models. We release our code and models.

</details>


### [195] [MMD-Sense-Analysis: Word Sense Detection Leveraging Maximum Mean Discrepancy](https://arxiv.org/abs/2506.01602)
*Kensuke Mitsuzawa*

Key words: 词语意义分析, MMD, 意义演变检测

TL;DR: 提出了一种基于MMD的新方法MMD-Sense-Analysis，用于检测和解释词语意义随时间的变化。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 词语意义分析对于理解语言和社会背景至关重要，而词语意义的演变检测是识别和解释词语意义随时间变化的任务。

Method: 利用最大均值差异（MMD）选择语义上有意义的变量，并量化不同时间段的变化。

Result: 实验结果表明该方法在检测词语意义演变方面有效。

Conclusion: MMD首次被应用于词语意义变化检测，并展现出其潜力。

Abstract: Word sense analysis is an essential analysis work for interpreting the
linguistic and social backgrounds. The word sense change detection is a task of
identifying and interpreting shifts in word meanings over time. This paper
proposes MMD-Sense-Analysis, a novel approach that leverages Maximum Mean
Discrepancy (MMD) to select semantically meaningful variables and quantify
changes across time periods. This method enables both the identification of
words undergoing sense shifts and the explanation of their evolution over
multiple historical periods. To my knowledge, this is the first application of
MMD to word sense change detection. Empirical assessment results demonstrate
the effectiveness of the proposed approach.

</details>


### [196] [IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language RAG Systems](https://arxiv.org/abs/2506.01615)
*Pasunuti Prasanjith,Prathmesh B More,Anoop Kunchukuttan,Raj Dabre*

Key words: Retrieval-Augmented Generation, Indian languages, evaluation benchmarks, training datasets, multilingual retrieval

TL;DR: 论文提出针对印度语言开发高质量RAG系统的两大挑战：缺乏评估基准和训练数据集，并介绍通过创建IndicMSMarco基准和大规模数据集来解决这些问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决印度语言缺乏高质量RAG系统的问题，主要在于缺乏评估基准和多语言检索的大型训练数据集。

Method: 通过手动翻译MS MARCO-dev创建IndicMSMarco基准，并利用LLMs从19种印度语言的维基百科中构建(question, answer, relevant passage)数据集，同时翻译原始MS MARCO数据集以丰富训练数据。

Result: 创建了IndicMSMarco基准和大规模多语言训练数据集，可用于评估和训练印度语言的RAG系统。

Conclusion: 提出的资源和框架填补了印度语言RAG系统的空白，为未来发展奠定了基础。

Abstract: Retrieval-Augmented Generation (RAG) systems enable language models to access
relevant information and generate accurate, well-grounded, and contextually
informed responses. However, for Indian languages, the development of
high-quality RAG systems is hindered by the lack of two critical resources: (1)
evaluation benchmarks for retrieval and generation tasks, and (2) large-scale
training datasets for multilingual retrieval. Most existing benchmarks and
datasets are centered around English or high-resource languages, making it
difficult to extend RAG capabilities to the diverse linguistic landscape of
India. To address the lack of evaluation benchmarks, we create IndicMSMarco, a
multilingual benchmark for evaluating retrieval quality and response generation
in 13 Indian languages, created via manual translation of 1000 diverse queries
from MS MARCO-dev set. To address the need for training data, we build a
large-scale dataset of (question, answer, relevant passage) tuples derived from
the Wikipedias of 19 Indian languages using state-of-the-art LLMs.
Additionally, we include translated versions of the original MS MARCO dataset
to further enrich the training data and ensure alignment with real-world
information-seeking tasks. Resources are available here:
https://huggingface.co/datasets/ai4bharat/Indic-Rag-Suite

</details>


### [197] [Domain Lexical Knowledge-based Word Embedding Learning for Text Classification under Small Data](https://arxiv.org/abs/2506.01621)
*Zixiao Zhu,Kezhi Mao*

Key words: BERT, 词嵌入增强, 领域词汇知识, 文本分类, 情感分析

TL;DR: 该论文提出了一种基于领域词汇知识增强BERT词嵌入的方法，以提升在关键词主导的文本分类任务（如情感分析和情绪识别）中的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在关键词对分类标签预测起关键作用的任务中，BERT的上下文嵌入可能不足以生成区分性强的文本表示。

Method: 开发了一种利用领域词汇知识增强词嵌入的模型，将BERT嵌入投影到一个最大化类内相似性和类间差异的新空间，并设计了一个自动从在线开放资源获取词汇知识的算法。

Result: 在情感分析、情绪识别和问答三个分类任务上的实验证明了该模型的有效性。

Conclusion: 通过增强词嵌入的区分性，提出的方法显著提升了关键词主导任务的分类性能。

Abstract: Pre-trained language models such as BERT have been proved to be powerful in
many natural language processing tasks. But in some text classification
applications such as emotion recognition and sentiment analysis, BERT may not
lead to satisfactory performance. This often happens in applications where
keywords play critical roles in the prediction of class labels. Our
investigation found that the root cause of the problem is that the
context-based BERT embedding of the keywords may not be discriminative enough
to produce discriminative text representation for classification. Motivated by
this finding, we develop a method to enhance word embeddings using
domain-specific lexical knowledge. The knowledge-based embedding enhancement
model projects the BERT embedding into a new space where within-class
similarity and between-class difference are maximized. To implement the
knowledge-based word embedding enhancement model, we also develop a knowledge
acquisition algorithm for automatically collecting lexical knowledge from
online open sources. Experiment results on three classification tasks,
including sentiment analysis, emotion recognition and question answering, have
shown the effectiveness of our proposed word embedding enhancing model. The
codes and datasets are in https://github.com/MidiyaZhu/KVWEFFER.

</details>


### [198] [MVAN: Multi-View Attention Networks for Fake News Detection on Social Media](https://arxiv.org/abs/2506.01627)
*Shiwen Ni,Jiawen Li,Hung-Yu Kao*

Key words: 假新闻检测, 社交媒体, 注意力机制, 神经网络, 多视角学习

TL;DR: 该文提出一种新型神经网络模型MVAN，用于检测社交媒体上的假新闻并提供解释。MVAN结合文本语义注意力和传播结构注意力，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有假新闻检测方法主要依赖长文本内容，如原文和用户评论。本文旨在解决仅提供简短推文及其转发用户的更现实场景下的假新闻检测问题。

Method: 提出多视角注意力网络（MVAN），包括文本语义注意力和传播结构注意力，从推文内容和传播结构中捕捉关键信息。

Result: 在两个真实数据集上的实验表明，MVAN在准确率上平均比现有方法高2.5%，并能提供合理的解释。

Conclusion: MVAN在假新闻检测任务中表现出色，尤其在短文本和传播结构的结合上具有优势。

Abstract: Fake news on social media is a widespread and serious problem in today's
society. Existing fake news detection methods focus on finding clues from Long
text content, such as original news articles and user comments. This paper
solves the problem of fake news detection in more realistic scenarios. Only
source shot-text tweet and its retweet users are provided without user
comments. We develop a novel neural network based model,
\textbf{M}ulti-\textbf{V}iew \textbf{A}ttention \textbf{N}etworks (MVAN) to
detect fake news and provide explanations on social media. The MVAN model
includes text semantic attention and propagation structure attention, which
ensures that our model can capture information and clues both of source tweet
content and propagation structure. In addition, the two attention mechanisms in
the model can find key clue words in fake news texts and suspicious users in
the propagation structure. We conduct experiments on two real-world datasets,
and the results demonstrate that MVAN can significantly outperform
state-of-the-art methods by 2.5\% in accuracy on average, and produce a
reasonable explanation.

</details>


### [199] [Cross-Lingual Generalization and Compression: From Language-Specific to Shared Neurons](https://arxiv.org/abs/2506.01629)
*Frederick Riemenschneider,Anette Frank*

Key words: 多语言语言模型,表征演化,神经元对齐,跨语言迁移

TL;DR: 多语言语言模型（MLLMs）能够在无显式跨语言监督的情况下实现知识迁移。研究发现，其表征在预训练过程中逐渐从语言特定压缩为跨语言抽象，神经元也逐渐对齐。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究多语言语言模型在无显式监督下如何实现跨语言知识迁移及其表征演化。

Method: 分析三个MLLMs的参数空间，通过探索实验观察表征演化及神经元对齐。

Result: 模型表征从语言特定逐渐融合为跨语言抽象，特定神经元成为跨语言概念的可靠预测器。

Conclusion: MLLMs通过表征压缩和神经元对齐实现跨语言知识迁移。

Abstract: Multilingual language models (MLLMs) have demonstrated remarkable abilities
to transfer knowledge across languages, despite being trained without explicit
cross-lingual supervision. We analyze the parameter spaces of three MLLMs to
study how their representations evolve during pre-training, observing patterns
consistent with compression: models initially form language-specific
representations, which gradually converge into cross-lingual abstractions as
training progresses. Through probing experiments, we observe a clear transition
from uniform language identification capabilities across layers to more
specialized layer functions. For deeper analysis, we focus on neurons that
encode distinct semantic concepts. By tracing their development during
pre-training, we show how they gradually align across languages. Notably, we
identify specific neurons that emerge as increasingly reliable predictors for
the same concepts across languages.

</details>


### [200] [ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge](https://arxiv.org/abs/2506.01646)
*Chaoyue He,Xin Zhou,Yi Wu,Xinjia Yu,Yan Zhang,Lei Zhang,Di Wang,Shengfei Lyu,Hong Xu,Xiaoqiao Wang,Wei Liu,Chunyan Miao*

Key words: LLM, ESG, 可持续发展, 基准测试, RAG

TL;DR: ESGenius是首个专注于ESG和可持续发展主题的LLM基准测试，包含QA数据集和权威文档库，通过零样本和RAG评估，发现RAG能显著提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 评估和提高LLM在ESG和可持续发展问题解答中的能力，填补该领域基准测试的空白。

Method: 开发ESGenius-QA（1136个多选问题）和ESGenius-Corpus（231份权威文档），采用零样本和RAG两阶段评估协议。

Result: 零样本准确率55-70%，RAG显著提升性能（如DeepSeek模型从63.82%提升至80.46%）。

Conclusion: LLM需依赖权威文献增强ESG理解，ESGenius为相关技术提供了首个评估基准。

Abstract: We introduce ESGenius, a comprehensive benchmark for evaluating and enhancing
the proficiency of Large Language Models (LLMs) in Environmental, Social and
Governance (ESG) and sustainability-focused question answering. ESGenius
comprises two key components: (i) ESGenius-QA, a collection of 1 136
multiple-choice questions generated by LLMs and rigorously validated by domain
experts, covering a broad range of ESG pillars and sustainability topics. Each
question is systematically linked to its corresponding source text, enabling
transparent evaluation and supporting retrieval-augmented generation (RAG)
methods; and (ii) ESGenius-Corpus, a meticulously curated repository of 231
foundational frameworks, standards, reports and recommendation documents from
seven authoritative sources. Moreover, to fully assess the capabilities and
adaptation potential of the model, we implement a rigorous two-stage evaluation
protocol -- Zero-Shot and RAG. Extensive experiments across 50 LLMs (ranging
from 0.5 B to 671 B parameters) demonstrate that state-of-the-art models
achieve only moderate performance in zero-shot settings, with accuracies
typically around 55--70\%, highlighting ESGenius's challenging nature for LLMs
in interdisciplinary contexts. However, models employing RAG show significant
performance improvements, particularly for smaller models. For example,
"DeepSeek-R1-Distill-Qwen-14B" improves from 63.82\% (zero-shot) to 80.46\%
with RAG. These results underscore the necessity of grounding responses in
authoritative sources for enhanced ESG understanding. To the best of our
knowledge, ESGenius is the first benchmark curated for LLMs and the relevant
enhancement technologies that focuses on ESG and sustainability topics.

</details>


### [201] [Cross-Lingual Transfer of Cultural Knowledge: An Asymmetric Phenomenon](https://arxiv.org/abs/2506.01675)
*Chen Zhang,Zhiyuan Liao,Yansong Feng*

Key words: 大语言模型, 文化多样性, 多语言迁移, 知识获取, 训练数据频率

TL;DR: 研究探讨了大语言模型（LLMs）在多语言环境中文化知识迁移的机制，重点关注高资源和低资源语言之间的不对称迁移现象。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管已有大量研究评估LLMs处理全球文化多样性的能力，但其在多语言环境中文化知识获取的机制仍不明确。

Method: 引入可解释的框架，研究文化知识在语言适应过程中的迁移，控制训练数据的透明度并分析迁移效应。

Result: 发现高资源语言与英语之间存在双向文化迁移，而低资源语言主要向英语单向迁移。频率假设解释了这一不对称现象。

Conclusion: 文化知识在预训练数据中出现频率越高，越容易迁移，这一发现为优化LLMs文化知识获取提供了指导。

Abstract: Despite substantial research efforts evaluating how well large language
models~(LLMs) handle global cultural diversity, the mechanisms behind their
cultural knowledge acquisition, particularly in multilingual settings, remain
unclear. We study this question by investigating how cultural knowledge
transfers across languages during language adaptation of LLMs. We introduce an
interpretable framework for studying this transfer, ensuring training data
transparency and controlling transfer effects. Through a study of four
non-Anglophonic cultures, we observe bidirectional cultural transfer between
English and other high-resource languages, while low-resource languages
primarily transfer knowledge to English with limited reverse flow. To explain
this asymmetric phenomenon, we propose a frequency-based hypothesis: cultural
knowledge appearing more frequently in the pretraining data transfers more
easily, which is supported by empirical analysis of the training corpora.

</details>


### [202] [StochasTok: Improving Fine-Grained Subword Understanding in LLMs](https://arxiv.org/abs/2506.01687)
*Anya Sims,Thom Foster,Klara Kaleb,Tuan-Duy H. Nguyen,Joseph Lee,Jakob N. Foerster,Yee Whye Teh,Cong Lu*

Key words: StochasTok, 随机分词, 子词级任务, 大型语言模型

TL;DR: 论文提出了StochasTok，一种随机分词方法，通过随机拆分标记来改善大型语言模型在子词级任务上的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前大型语言模型在处理子词级任务（如统计字母数量）时表现不佳，主要是由于分词方法掩盖了词的细粒度结构。

Method: 引入StochasTok，一种随机分词方案，训练时随机拆分标记，使模型能够观察到词的内部结构。

Result: 实验表明，StochasTok显著提高了模型在字符计数、子串识别等子词级任务上的性能，且可以无缝集成到现有训练流程中。

Conclusion: StochasTok通过简单改动实现了显著改进，展示了在大模型中的应用潜力。

Abstract: Subword-level understanding is integral to numerous tasks, including
understanding multi-digit numbers, spelling mistakes, abbreviations, rhyming,
and wordplay. Despite this, current large language models (LLMs) still often
struggle with seemingly simple subword-level tasks like How many 'r's in
'strawberry'?. A key factor behind these failures is tokenization which
obscures the fine-grained structure of words. Current alternatives, such as
character-level and dropout tokenization methods, significantly increase
computational costs and provide inconsistent improvements. In this paper we
revisit tokenization and introduce StochasTok, a simple, efficient stochastic
tokenization scheme that randomly splits tokens during training, allowing LLMs
to 'see' their internal structure. Our experiments show that pretraining with
StochasTok substantially improves LLMs' downstream performance across multiple
subword-level language games, including character counting, substring
identification, and math tasks. Furthermore, StochasTok's simplicity allows
seamless integration at any stage of the training pipeline; and we demonstrate
that post-training with StochasTok can instill improved subword understanding
into existing pretrained models, thus avoiding costly pretraining from scratch.
These dramatic improvements achieved with a minimal change suggest StochasTok
holds exciting potential when applied to larger, more capable models. Code
open-sourced at: https://github.com/anyasims/stochastok.

</details>


### [203] [When LLMs Team Up: The Emergence of Collaborative Affective Computing](https://arxiv.org/abs/2506.01698)
*Wenna Lai,Haoran Xie,Guandong Xu,Qing Li,S. Joe Qin*

Key words: 情感计算, 大语言模型, 协作系统, 双过程理论, 社会智能

TL;DR: 该论文综述了基于大语言模型（LLM）的协作系统在情感计算（AC）中的应用，探讨了其方法、实验比较、潜力及未来方向。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统AC任务中的流水线架构存在结构刚性问题，LLM虽提升了动态交互潜力，但在情感推理中仍有认知局限性。因此，研究协作系统以弥补这些不足。

Method: 通过系统综述现有方法，实验比较协作策略，并分析其对复杂情感推理的增强作用。

Result: LLM协作系统能提升情感理解的鲁棒性和适应性，但需解决文化误解和幻觉问题。

Conclusion: 协作智能为AC提供了人类化的社会智能潜力，未来研究方向需解决现有挑战。

Abstract: Affective Computing (AC) is essential in bridging the gap between human
emotional experiences and machine understanding. Traditionally, AC tasks in
natural language processing (NLP) have been approached through pipeline
architectures, which often suffer from structure rigidity that leads to
inefficiencies and limited adaptability. The advent of Large Language Models
(LLMs) has revolutionized this field by offering a unified approach to
affective understanding and generation tasks, enhancing the potential for
dynamic, real-time interactions. However, LLMs face cognitive limitations in
affective reasoning, such as misinterpreting cultural nuances or contextual
emotions, and hallucination problems in decision-making. To address these
challenges, recent research advocates for LLM-based collaboration systems that
emphasize interactions among specialized models and LLMs, mimicking human-like
affective intelligence through the synergy of emotional and rational thinking
that aligns with Dual Process Theory in psychology. This survey aims to provide
a comprehensive overview of LLM-based collaboration systems in AC, exploring
from structured collaborations to autonomous collaborations. Specifically, it
includes: (1) A systematic review of existing methods, focusing on
collaboration strategies, mechanisms, key functions, and applications; (2)
Experimental comparisons of collaboration strategies across representative
tasks in affective understanding and generation; (3) An analysis highlighting
the potential of these systems to enhance robustness and adaptability in
complex affective reasoning; (4) A discussion of key challenges and future
research directions to further advance the field. This work is the first to
systematically explore collaborative intelligence with LLMs in AC, paving the
way for more powerful applications that approach human-like social
intelligence.

</details>


### [204] [mdok of KInIT: Robustly Fine-tuned LLM for Binary and Multiclass AI-Generated Text Detection](https://arxiv.org/abs/2506.01702)
*Dominik Macko*

Key words: 大语言模型, 文本检测, 鲁棒性, 微调, 多分类

TL;DR: 本文提出了一种基于微调小型LLMs的文本分类方法（mdok），用于检测机器生成文本，在Voight-Kampff Generative AI Detection 2025任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大语言模型（LLMs）生成的多语言高质量文本可能导致滥用（如抄袭、垃圾信息、传播虚假信息），自动化检测仍需提高在分布外数据上的鲁棒性。

Method: 采用微调小型LLMs进行文本分类，提出mdok方法，应用于二元检测和多分类任务。

Result: 在Voight-Kampff任务中，二元检测和多分类（排名第一）均表现突出。

Conclusion: mdok方法在检测机器生成文本方面具有高效性和鲁棒性，为防范LLMs滥用提供了实用工具。

Abstract: The large language models (LLMs) are able to generate high-quality texts in
multiple languages. Such texts are often not recognizable by humans as
generated, and therefore present a potential of LLMs for misuse (e.g.,
plagiarism, spams, disinformation spreading). An automated detection is able to
assist humans to indicate the machine-generated texts; however, its robustness
to out-of-distribution data is still challenging. This notebook describes our
mdok approach in robust detection, based on fine-tuning smaller LLMs for text
classification. It is applied to both subtasks of Voight-Kampff Generative AI
Detection 2025, providing remarkable performance in binary detection as well as
in multiclass (1st rank) classification of various cases of human-AI
collaboration.

</details>


### [205] [Fairness Dynamics During Training](https://arxiv.org/abs/2506.01709)
*Krishna Patel,Nivedha Sivakumar,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Key words: 大型语言模型, 公平性, 偏见, 预训练, 早期停止

TL;DR: 研究大型语言模型（LLM）训练中的公平性动态，通过新指标诊断和缓解偏见，发现预训练中的偏见可能突然出现且与性能指标无关。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 理解和解决LLM训练过程中可能出现的偏见，尤其是性别偏见，以提升模型公平性。

Method: 引入两种新指标（平均排名和部分Jensen-Shannon散度），在WinoBias数据集上分析Pythia模型的性别偏见动态。

Result: 发现Pythia-6.9b对男性偏见更强，早期停止可显著提升公平性，大模型可能表现出更多偏见。

Conclusion: 监控公平性动态有助于优化训练策略，改进模型公平性。

Abstract: We investigate fairness dynamics during Large Language Model (LLM) training
to enable the diagnoses of biases and mitigations through training
interventions like early stopping; we find that biases can emerge suddenly and
do not always follow common performance metrics. We introduce two new metrics
to evaluate fairness dynamics holistically during model pre-training: Average
Rank and Jensen-Shannon Divergence by Parts. These metrics provide insights
into the Pythia models' progression of biases in gender prediction of
occupations on the WinoBias dataset. By monitoring these dynamics, we find that
(1) Pythia-6.9b is biased towards men; it becomes more performant and confident
predicting "male" than "female" during training, (2) via early-stopping,
Pythia-6.9b can exchange 1.7% accuracy on LAMBADA for a 92.5% increase in
fairness, and (3) larger models can exhibit more bias; Pythia-6.9b makes more
assumptions about gender than Pythia-160m, even when a subject's gender is not
specified.

</details>


### [206] [Reasoning-Table: Exploring Reinforcement Learning for Table Reasoning](https://arxiv.org/abs/2506.01710)
*Fangyu Lei,Jinxiang Meng,Yiming Huang,Tinghong Chen,Yun Zhang,Shizhu He,Jun Zhao,Kang Liu*

Key words: 表格推理, 强化学习, 监督微调, 鲁棒性, 泛化能力, 文本到SQL

TL;DR: 论文提出了**Reasoning-Table**，首次将强化学习（RL）应用于表格推理任务，通过数据预处理、奖励设计和训练策略优化，超越了监督微调（SFT）方法，在多个基准测试中实现了最先进性能，并提升了模型的泛化能力和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的监督微调（SFT）方法在表格推理任务中存在泛化和鲁棒性问题，因为模仿学习容易引入偏差。本文旨在通过强化学习解决这些问题。

Method: 采用强化学习方法，结合数据预处理、奖励设计和针对性的训练策略，利用简单的基于规则的奖励优化模型性能。

Result: Reasoning-Table在多个基准测试中超越了SFT方法，并在表格推理任务中比Claude-3.7-Sonnet高出4.0%，同时在文本到SQL任务中达到了68.3%的性能（基于BIRD dev数据集）。

Conclusion: 研究表明，强化学习的应用显著提升了表格推理任务的性能、泛化能力和鲁棒性，甚至优于更大规模的专有模型。

Abstract: Table reasoning, encompassing tasks such as table question answering, fact
verification, and text-to-SQL, requires precise understanding of structured
tabular data, coupled with numerical computation and code manipulation for
effective inference. Supervised fine-tuning (SFT) approaches have achieved
notable success but often struggle with generalization and robustness due to
biases inherent in imitative learning. We introduce Reasoning-Table, the first
application of reinforcement learning (RL) to table reasoning, achieving
state-of-the-art performance. Through rigorous data preprocessing, reward
design, and tailored training strategies, our method leverages simple
rule-based outcome rewards to outperform SFT across multiple benchmarks.
Unified training across diverse tasks enables Reasoning-Table to emerge as a
robust table reasoning large language model, surpassing larger proprietary
models like Claude-3.7-Sonnet by 4.0% on table reasoning benchmarks. The
approach also achieves excellent performance on text-to-SQL tasks, reaching
68.3% performance on the BIRD dev dataset with a 7B model. Further experiments
demonstrate that Reasoning-Table enhances the model's generalization
capabilities and robustness.

</details>


### [207] [SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning](https://arxiv.org/abs/2506.01713)
*Zhongwei Wan,Zhihao Dou,Che Liu,Yu Zhang,Dongfei Cui,Qinjian Zhao,Hui Shen,Jing Xiong,Yi Xin,Yifan Jiang,Yangfan He,Mi Zhang,Shen Yan*

Key words: 多模态大语言模型, 强化学习, 自我反思, 推理能力, SRPO

TL;DR: 该论文提出了一种名为SRPO的两阶段强化学习框架，通过自我反思提升多模态大语言模型的推理能力，并在多个基准测试中显著优于现有模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法在多模态任务中表现不足，尤其是在需要自我反思和修正的复杂问题上。

Method: 采用两阶段强化学习框架，包括构建高质量反思数据集和引入新颖的奖励机制。

Result: SRPO在多个基准测试中表现优异，推理准确性和反思质量均有显著提升。

Conclusion: SRPO通过反思增强的强化学习框架，有效提升了多模态大语言模型的推理能力。

Abstract: Multimodal large language models (MLLMs) have shown promising capabilities in
reasoning tasks, yet still struggle with complex problems requiring explicit
self-reflection and self-correction, especially compared to their unimodal
text-based counterparts. Existing reflection methods are simplistic and
struggle to generate meaningful and instructive feedback, as the reasoning
ability and knowledge limits of pre-trained models are largely fixed during
initial training. To overcome these challenges, we propose Multimodal
Self-Reflection enhanced reasoning with Group Relative Policy Optimization
(SRPO), a two-stage reflection-aware reinforcement learning (RL) framework
explicitly designed to enhance multimodal LLM reasoning. In the first stage, we
construct a high-quality, reflection-focused dataset under the guidance of an
advanced MLLM, which generates reflections based on initial responses to help
the policy model learn both reasoning and self-reflection. In the second stage,
we introduce a novel reward mechanism within the GRPO framework that encourages
concise and cognitively meaningful reflection while avoiding redundancy.
Extensive experiments across multiple multimodal reasoning benchmarks,
including MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B
and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms
state-of-the-art models, achieving notable improvements in both reasoning
accuracy and reflection quality.

</details>


### [208] [Tug-of-war between idiom's figurative and literal meanings in LLMs](https://arxiv.org/abs/2506.01723)
*Soyoung Oh,Xinting Huang,Mathis Pink,Michael Hahn,Vera Demberg*

Key words: 习语处理,机制可解释性,注意力头,LLama3.2-1B-base

TL;DR: 大型预训练因果变换器（LLama3.2-1B-base）通过注意力机制和多层感知器子层分三步处理习语的非组合性比喻意义，同时保留字面解释。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究习语对语言模型的挑战，因其比喻意义与字面解释存在显著差异，需模型学习同时处理这两种意义。

Method: 采用机制可解释性工具，追踪模型处理习语的过程，定位习语比喻意义检索、抑制字面解释及并行处理路径。

Result: 识别出特定注意力头提升比喻意义并抑制字面解释，同时模型通过中介路径表示比喻意义，保留字面解释的并行路径。

Conclusion: 研究为自回归变换器中的习语理解提供了机制性证据。

Abstract: Idioms present a unique challenge for language models due to their
non-compositional figurative meanings, which often strongly diverge from the
idiom's literal interpretation. This duality requires a model to learn
representing and deciding between the two meanings to interpret an idiom in a
figurative sense, or literally. In this paper, we employ tools from mechanistic
interpretability to trace how a large pretrained causal transformer
(LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom
processing: First, the idiom's figurative meaning is retrieved in early
attention and MLP sublayers. We identify specific attention heads which boost
the figurative meaning of the idiom while suppressing the idiom's literal
interpretation. The model subsequently represents the figurative representation
through an intermediate path. Meanwhile, a parallel bypass route forwards
literal interpretation, ensuring that a both reading remain available. Overall,
our findings provide a mechanistic evidence for idiom comprehension in an
autoregressive transformer.

</details>


### [209] [Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training](https://arxiv.org/abs/2506.01732)
*Pierre-Carl Langlais,Carlos Rosas Hinostroza,Mattia Nee,Catherine Arnett,Pavel Chizhov,Eliot Krzystof Jones,Irène Girard,David Mach,Anastasia Stasenko,Ivan P. Yamshchikov*

Key words: 大型语言模型,开放数据集,预训练数据,Common Corpus,数据合规

TL;DR: Common Corpus是一个开放的、合规的大型语言模型预训练数据集，包含约两万亿个来自公共领域或允许使用的令牌，涵盖多种语言和代码数据，旨在解决当前预训练数据版权问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前LLMs预训练数据常涉及版权或专有内容，限制了其在AI法规下的使用，因此需要一个完全开放且合规的预训练数据集。

Method: 通过收集公共领域或允许使用的数据，组装成Common Corpus数据集，涵盖多种语言和代码数据，并详细描述了数据来源、过滤和整理过程。

Result: Common Corpus已成为行业领导者使用的数据集，被认为是开放科学研究LLM的关键基础设施。

Conclusion: Common Corpus为解决预训练数据版权问题提供了开放且合规的解决方案，促进了LLM研究和应用的发展。

Abstract: Large Language Models (LLMs) are pre-trained on large amounts of data from
different sources and domains. These data most often contain trillions of
tokens with large portions of copyrighted or proprietary content, which hinders
the usage of such models under AI legislation. This raises the need for truly
open pre-training data that is compliant with the data security regulations. In
this paper, we introduce Common Corpus, the largest open dataset for language
model pre-training. The data assembled in Common Corpus are either
uncopyrighted or under permissible licenses and amount to about two trillion
tokens. The dataset contains a wide variety of languages, ranging from the main
European languages to low-resource ones rarely present in pre-training
datasets; in addition, it includes a large portion of code data. The diversity
of data sources in terms of covered domains and time periods opens up the paths
for both research and entrepreneurial needs in diverse areas of knowledge. In
this technical report, we present the detailed provenance of data assembling
and the details of dataset filtering and curation. Being already used by such
industry leaders as Anthropic and multiple LLM training projects, we believe
that Common Corpus will become a critical infrastructure for open science
research in LLMs.

</details>


### [210] [Benford's Curse: Tracing Digit Bias to Numerical Hallucination in LLMs](https://arxiv.org/abs/2506.01734)
*Jiandong Shao,Yao Lu,Jianfei Yang*

Key words: 大型语言模型, 数值生成偏差, 本福德定律, FFN神经元, 神经元剪枝

TL;DR: 该论文研究了大型语言模型（LLMs）在数值任务中的生成偏差问题，发现这种偏差与预训练数据中的长尾数字分布（类似于本福德定律）有关，并通过神经元级别的分析提出了缓解方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: LLMs在复杂推理任务中表现优秀，但在基础数值问题上经常失败。论文假设这种现象与预训练数据中数字分布的长尾特性有关。

Method: 通过分析预训练数据（OLMo2）是否符合本福德定律，构建均匀分布的数字基准任务，并对LLMs进行神经元级别的分析（如logit-lens追踪和FFN神经元剪枝）。

Result: 开源LLMs表现出与本福德定律相似的数字生成偏差，这种偏差主要源于深层网络中少量高度数字选择性的FFN神经元。剪枝这些神经元可以部分纠正错误输出。

Conclusion: 研究发现预训练数据的细粒度数字偏差会传播到模型行为中，为诊断和缓解数值任务中的幻觉提供了新视角。

Abstract: Large Language Models (LLMs) exhibit impressive performance on complex
reasoning tasks, yet they frequently fail on basic numerical problems,
producing incorrect outputs. Inspired by Benford's Law -- a statistical pattern
where lower digits occur more frequently as leading digits -- we hypothesize
that the long-tailed digit distributions in web-collected corpora may be
learned by LLMs during pretraining, leading to biased numerical generation. To
investigate the hypothesis, we first examine whether digits frequencies in
pretraining corpus (OLMo2) follows Benford's law. We then construct an
evaluation benchmark with uniformly distributed ground-truth digits across
seven numerical reasoning tasks. Our evaluation results demonstrate that
leading open-source LLMs show a consistent pattern of digit bias that resembles
Benford's law. Through logit-lens tracing and neuron-level dissection, we
identify that this bias arises predominantly from a small subset of highly
digit-selective feed-forward network (FFN) neurons in the deeper layers.
Finally, we demonstrate that pruning these neurons mitigates imbalanced
overgeneration and partially corrects erroneous outputs, providing causal
evidence that fine-grained pretraining digit bias can propagate into model
behavior. Our findings reveal a fundamental connection between corpus-level
statistics and symbolic failure modes in LLMs, offering a new lens for
diagnosing and mitigating hallucinations in numerical tasks.

</details>


### [211] [Thinking in Character: Advancing Role-Playing Agents with Role-Aware Reasoning](https://arxiv.org/abs/2506.01748)
*Yihong Tang,Kehai Chen,Muyun Yang,Zhengyu Niu,Jing Li,Tiejun Zhao,Min Zhang*

Key words: Large Language Models, Role-Playing Agents, Role-Aware Reasoning, Attention Diversion, Style Drift

TL;DR: 针对角色扮演代理（RPAs）在模拟人类深层思维和风格表达上的不足，本文提出了角色感知推理（RAR）方法，通过角色身份激活（RIA）和推理风格优化（RSO）两阶段解决注意力分散和风格漂移问题，显著提升RPA表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前RPAs基于显性对话数据，缺乏人类深层思维，导致知识表达浅薄，且直接应用大型推理模型（LRMs）会引发注意力分散和风格漂移。

Method: 提出RAR方法，包含RIA（通过角色配置文件指导推理以对抗注意力分散）和RSO（通过LRM蒸馏优化推理风格以保持角色一致性）。

Result: 实验表明RAR有效解决了注意力分散和风格漂移，显著提升了RPAs的表现。

Conclusion: RAR方法为RPAs提供了更接近人类的推理和风格表达，解决了当前技术的核心问题。

Abstract: The advancement of Large Language Models (LLMs) has spurred significant
interest in Role-Playing Agents (RPAs) for applications such as emotional
companionship and virtual interaction. However, recent RPAs are often built on
explicit dialogue data, lacking deep, human-like internal thought processes,
resulting in superficial knowledge and style expression. While Large Reasoning
Models (LRMs) can be employed to simulate character thought, their direct
application is hindered by attention diversion (i.e., RPAs forget their role)
and style drift (i.e., overly formal and rigid reasoning rather than
character-consistent reasoning). To address these challenges, this paper
introduces a novel Role-Aware Reasoning (RAR) method, which consists of two
important stages: Role Identity Activation (RIA) and Reasoning Style
Optimization (RSO). RIA explicitly guides the model with character profiles
during reasoning to counteract attention diversion, and then RSO aligns
reasoning style with the character and scene via LRM distillation to mitigate
style drift. Extensive experiments demonstrate that the proposed RAR
significantly enhances the performance of RPAs by effectively addressing
attention diversion and style drift.

</details>


### [212] [Developing a Mixed-Methods Pipeline for Community-Oriented Digitization of Kwak'wala Legacy Texts](https://arxiv.org/abs/2506.01775)
*Milind Agarwal,Daisy Rosenblum,Antonios Anastasopoulos*

Key words: Kwak'wala, OCR, 语言振兴, 数字化, 文本转录

TL;DR: 论文探讨了如何利用最新OCR技术将Kwak'wala语言的历史文本数字化，并提出了结合现有OCR工具、语言识别和校对模型的方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Kwak'wala是一种濒危语言，已有大量文本因机器不可读而无法充分利用，数字化这些文本有助于语言振兴和技术开发。

Method: 采用混合方法，包括现成OCR工具、语言识别、掩码技术隔离文本，以及后校对模型提高转录质量。

Result: 成功将图像中的Kwak'wala文本转录为可读的现代拼写形式，解决了技术挑战。

Conclusion: 该方法为其他濒危语言的文本数字化提供了可行路径，但仍需进一步优化。

Abstract: Kwak'wala is an Indigenous language spoken in British Columbia, with a rich
legacy of published documentation spanning more than a century, and an active
community of speakers, teachers, and learners engaged in language
revitalization. Over 11 volumes of the earliest texts created during the
collaboration between Franz Boas and George Hunt have been scanned but remain
unreadable by machines. Complete digitization through optical character
recognition has the potential to facilitate transliteration into modern
orthographies and the creation of other language technologies. In this paper,
we apply the latest OCR techniques to a series of Kwak'wala texts only
accessible as images, and discuss the challenges and unique adaptations
necessary to make such technologies work for these real-world texts. Building
on previous methods, we propose using a mix of off-the-shelf OCR methods,
language identification, and masking to effectively isolate Kwak'wala text,
along with post-correction models, to produce a final high-quality
transcription.

</details>


### [213] [MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation](https://arxiv.org/abs/2506.01776)
*Yile Liu,Ziwei Ma,Xiu Jiang,Jinglu Hu,Jing Chang,Liang Li*

Key words: 大语言模型,指令跟随,多语言评估,MaXIFE,自然语言处理

TL;DR: MaXIFE是一个多语言指令跟随评估基准，覆盖23种语言和1,667个可验证任务，旨在填补现有评估方法在跨语言场景中的不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型（LLMs）的快速应用需要评估其指令跟随能力，但现有方法多集中在单语场景，忽视多语言和跨语言挑战。

Method: MaXIFE结合了基于规则和基于模型的评估方法，兼顾效率与准确性，并用于评估多个商业和开源LLMs。

Result: 通过MaXIFE评估，确立了未来比较的基线结果，展示其在多语言指令跟随能力评估中的实用性。

Conclusion: MaXIFE为研究和发展自然语言处理提供了标准化的多语言指令评估工具。

Abstract: With the rapid adoption of large language models (LLMs) in natural language
processing, the ability to follow instructions has emerged as a key metric for
evaluating their practical utility. However, existing evaluation methods often
focus on single-language scenarios, overlooking the challenges and differences
present in multilingual and cross-lingual contexts. To address this gap, we
introduce MaXIFE: a comprehensive evaluation benchmark designed to assess
instruction-following capabilities across 23 languages with 1,667 verifiable
instruction tasks. MaXIFE integrates both Rule-Based Evaluation and Model-Based
Evaluation, ensuring a balance of efficiency and accuracy. We applied MaXIFE to
evaluate several leading commercial and open-source LLMs, establishing baseline
results for future comparisons. By providing a standardized tool for
multilingual instruction-following evaluation, MaXIFE aims to advance research
and development in natural language processing.

</details>


### [214] [iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering](https://arxiv.org/abs/2506.01784)
*Shuai Wang,Yinan Yu*

Key words: KBQA, 多跳推理, 知识图谱, GNN, iQUEST

TL;DR: iQUEST通过迭代分解复杂查询并集成GNN技术，有效解决了KBQA中的多跳推理问题，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型在知识密集型任务中存在事实不准确的问题，而知识图谱能提供透明且可更新的知识支持。多跳推理的两大挑战是保持连贯推理路径和避免过早丢弃关键连接。

Method: 提出iQUEST框架，迭代分解复杂查询为子问题，并集成GNN在每个推理步骤中前瞻性引入2跳邻居信息。

Result: 在四个基准数据集和四种LLM上的实验表明，iQUEST实现了稳定且显著的性能提升。

Conclusion: iQUEST通过结构化推理路径和多跳信息整合，为KBQA任务提供了有效解决方案。

Abstract: While Large Language Models (LLMs) excel at many natural language processing
tasks, they often suffer from factual inaccuracies in knowledge-intensive
scenarios. Integrating external knowledge resources, particularly knowledge
graphs (KGs), provides a transparent and updatable foundation for more reliable
reasoning. Knowledge Base Question Answering (KBQA), which queries and reasons
over KGs, is central to this effort, especially for complex, multi-hop queries.
However, multi-hop reasoning poses two key challenges: (1)~maintaining coherent
reasoning paths, and (2)~avoiding prematurely discarding critical multi-hop
connections. To address these issues, we introduce iQUEST, a question-guided
KBQA framework that iteratively decomposes complex queries into simpler
sub-questions, ensuring a structured and focused reasoning trajectory.
Additionally, we integrate a Graph Neural Network (GNN) to look ahead and
incorporate 2-hop neighbor information at each reasoning step. This dual
approach strengthens the reasoning process, enabling the model to explore
viable paths more effectively. Detailed experiments demonstrate the consistent
improvement delivered by iQUEST across four benchmark datasets and four LLMs.

</details>


### [215] [Human-Centric Evaluation for Foundation Models](https://arxiv.org/abs/2506.01793)
*Yijin Guo,Kaiyuan Ji,Xiaorong Zhu,Junying Wang,Farong Wen,Chunyi Li,Zicheng Zhang,Guangtao Zhai*

Key words: 基础模型、主观评估、人类中心、问题解决能力、信息质量

TL;DR: 论文提出了一种以人为中心的评估框架（HCE），通过主观维度（问题解决能力、信息质量和交互体验）评估基础模型，弥补了传统客观指标的不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基础模型评估多基于客观指标，忽略了真实的人类体验，HCE框架旨在填补这一空白。

Method: 通过540多名参与者与模型（Deepseek R1、OpenAI o3 mini、Grok 3和Gemini 2.5）协作完成开放式研究任务，收集主观反馈数据。

Result: Grok 3表现最佳，其次是Deepseek R1和Gemini 2.5，OpenAI o3 mini表现较弱。

Conclusion: HCE框架提供了主观评估方法，推动了LLM在研究和实际应用中的发展。

Abstract: Currently, nearly all evaluations of foundation models focus on objective
metrics, emphasizing quiz performance to define model capabilities. While this
model-centric approach enables rapid performance assessment, it fails to
reflect authentic human experiences. To address this gap, we propose a
Human-Centric subjective Evaluation (HCE) framework, focusing on three core
dimensions: problem-solving ability, information quality, and interaction
experience. Through experiments involving Deepseek R1, OpenAI o3 mini, Grok 3,
and Gemini 2.5, we conduct over 540 participant-driven evaluations, where
humans and models collaborate on open-ended research tasks, yielding a
comprehensive subjective dataset. This dataset captures diverse user feedback
across multiple disciplines, revealing distinct model strengths and
adaptability. Our findings highlight Grok 3's superior performance, followed by
Deepseek R1 and Gemini 2.5, with OpenAI o3 mini lagging behind. By offering a
novel framework and a rich dataset, this study not only enhances subjective
evaluation methodologies but also lays the foundation for standardized,
automated assessments, advancing LLM development for research and practical
scenarios. Our dataset link is
https://github.com/yijinguo/Human-Centric-Evaluation.

</details>


### [216] [Read it in Two Steps: Translating Extremely Low-Resource Languages with Code-Augmented Grammar Books](https://arxiv.org/abs/2506.01796)
*Chen Zhang,Jiuheng Lin,Xiao Liu,Zekai Zhang,Yansong Feng*

Key words: 低资源语言翻译, 语法书, 规则检索, 代码规则, BLEU分数

TL;DR: 论文探讨了语法书在极低资源语言翻译中的作用，发现规则检索是主要瓶颈，并提出将语法规则编码为代码函数以提高翻译效果，实验显示BLEU分数提升了13.1%。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机在于探索语法书在极低资源语言翻译中的有效性，尤其是通过分解为规则检索和应用两个步骤来分析。

Method: 引入ZhuangRules数据集，将语法规则分解为检索和应用步骤，并提出将规则表示为代码函数的方法。

Result: 实验表明，规则检索是主要瓶颈，而代码规则显著提升了检索和应用效果，翻译BLEU分数提高了13.1%。

Conclusion: 将语法规则编码为代码函数可以有效提升翻译效果，解决了LLM在复杂规则处理上的困难。

Abstract: While large language models (LLMs) have shown promise in translating
extremely low-resource languages using resources like dictionaries, the
effectiveness of grammar books remains debated. This paper investigates the
role of grammar books in translating extremely low-resource languages by
decomposing it into two key steps: grammar rule retrieval and application. To
facilitate the study, we introduce ZhuangRules, a modularized dataset of
grammar rules and their corresponding test sentences. Our analysis reveals that
rule retrieval constitutes a primary bottleneck in grammar-based translation.
Moreover, although LLMs can apply simple rules for translation when explicitly
provided, they encounter difficulties in handling more complex rules. To
address these challenges, we propose representing grammar rules as code
functions, considering their similarities in structure and the benefit of code
in facilitating LLM reasoning. Our experiments show that using code rules
significantly boosts both rule retrieval and application, ultimately resulting
in a 13.1% BLEU improvement in translation.

</details>


### [217] [Propaganda and Information Dissemination in the Russo-Ukrainian War: Natural Language Processing of Russian and Western Twitter Narratives](https://arxiv.org/abs/2506.01807)
*Zaur Gouliev*

Key words: 乌克兰冲突, 信息战, 社交媒体, 自然语言处理, 机器学习

TL;DR: 分析乌克兰战争中社交媒体的信息战策略，通过推特数据揭示宣传账户和可信账户的差异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究乌克兰冲突中社交媒体信息战的动态，揭示宣传与可信账户的不同叙事策略。

Method: 利用自然语言处理和机器学习分析4万条推文，结合人工干预，评估情感和主题。

Result: 宣传账户使用情绪化语言和虚假信息，可信账户注重事实和人道主义；聚类分析发现疑似协调行为。

Conclusion: 信息战策略差异显著，研究为未来社交媒体在军事冲突中的影响提供技术参考。

Abstract: The conflict in Ukraine has been not only characterised by military
engagement but also by a significant information war, with social media
platforms like X, formerly known as Twitter playing an important role in
shaping public perception. This article provides an analysis of tweets from
propaganda accounts and trusted accounts collected from the onset of the war,
February 2022 until the middle of May 2022 with n=40,000 total tweets. We
utilise natural language processing and machine learning algorithms to assess
the sentiment and identify key themes, topics and narratives across the dataset
with human-in-the-loop (HITL) analysis throughout. Our findings indicate
distinct strategies in how information is created, spread, and targeted at
different audiences by both sides. Propaganda accounts frequently employ
emotionally charged language and disinformation to evoke fear and distrust,
whereas other accounts, primarily Western tend to focus on factual reporting
and humanitarian aspects of the conflict. Clustering analysis reveals groups of
accounts with similar behaviours, which we suspect indicates the presence of
coordinated efforts. This research attempts to contribute to our understanding
of the dynamics of information warfare and offers techniques for future studies
on social media influence in military conflicts.

</details>


### [218] [NAVER LABS Europe Submission to the Instruction-following Track](https://arxiv.org/abs/2506.01808)
*Beomseok Lee,Marcely Zanon Boito,Laurent Besacier,Ioan Calapodescu*

Key words: 语音处理, ASR, ST, SQA, SeamlessM4T, Llama-3.1-8B

TL;DR: NAVER LABS Europe开发了一个多任务语音处理系统，能够同时执行ASR、ST和SQA任务，支持英语输入到中文、意大利语和德语。系统结合了SeamlessM4T-v2-large语音编码器和Llama-3.1-8B-Instruct的LoRA适配器，并通过1K步的多语言多模态数据微调。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 旨在开发一个能够同时处理语音识别（ASR）、语音翻译（ST）和语音问答（SQA）的多任务系统，支持多语言输出。

Method: 系统基于两个预训练模块：语音到LLM嵌入投影器和LoRA适配器，并在多语言多模态数据上进行了1K步的微调。

Result: 系统在指令跟随语音处理任务中表现出色，支持英语到中文、意大利语和德语的转换。

Conclusion: 通过结合预训练模块和多任务微调，系统成功实现了高效的语音处理功能。

Abstract: In this paper we describe NAVER LABS Europe submission to the
instruction-following speech processing short track at IWSLT 2025. We
participate in the constrained settings, developing systems that can
simultaneously perform ASR, ST, and SQA tasks from English speech input into
the following target languages: Chinese, Italian, and German. Our solution
leverages two pretrained modules: (1) a speech-to-LLM embedding projector
trained using representations from the SeamlessM4T-v2-large speech encoder; and
(2) LoRA adapters trained on text data on top of a Llama-3.1-8B-Instruct. These
modules are jointly loaded and further instruction-tuned for 1K steps on
multilingual and multimodal data to form our final system submitted for
evaluation.

</details>


### [219] [Analysis of LLM Bias (Chinese Propaganda & Anti-US Sentiment) in DeepSeek-R1 vs. ChatGPT o3-mini-high](https://arxiv.org/abs/2506.01814)
*PeiHsuan Huang,ZihWei Lin,Simon Imbot,WenCheng Fu,Ethan Tu*

Key words: 大语言模型, 意识形态偏见, 跨语言比较, 宣传, 地缘政治

TL;DR: 研究比较了PRC系统模型DeepSeek-R1与非PRC模型ChatGPT o3-mini-high的地缘政治倾向性，发现前者在宣传和反美情绪上表现更强，且语言对偏见有显著影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨不同地缘政治对齐的大语言模型（LLMs）在意识形态上的差异，填补现有研究中直接、跨语言比较的空白。

Method: 开发了1,200个去上下文推理问题的新语料库，通过混合评估流程（GPT-4o评分和人工标注）分析7,200个模型回答。

Result: DeepSeek-R1比ChatGPT o3-mini-high表现出更高的宣传和反美偏见，简体中文查询的偏见率最高，且在文化和生活方式内容中也有体现。

Conclusion: LLMs的偏见与地缘政治对齐和语言密切相关，需进一步研究模型在公共理解中的意识形态影响。

Abstract: Large language models (LLMs) increasingly shape public understanding and
civic decisions, yet their ideological neutrality is a growing concern. While
existing research has explored various forms of LLM bias, a direct,
cross-lingual comparison of models with differing geopolitical
alignments-specifically a PRC-system model versus a non-PRC counterpart-has
been lacking. This study addresses this gap by systematically evaluating
DeepSeek-R1 (PRC-aligned) against ChatGPT o3-mini-high (non-PRC) for
Chinese-state propaganda and anti-U.S. sentiment. We developed a novel corpus
of 1,200 de-contextualized, reasoning-oriented questions derived from
Chinese-language news, presented in Simplified Chinese, Traditional Chinese,
and English. Answers from both models (7,200 total) were assessed using a
hybrid evaluation pipeline combining rubric-guided GPT-4o scoring with human
annotation. Our findings reveal significant model-level and language-dependent
biases. DeepSeek-R1 consistently exhibited substantially higher proportions of
both propaganda and anti-U.S. bias compared to ChatGPT o3-mini-high, which
remained largely free of anti-U.S. sentiment and showed lower propaganda
levels. For DeepSeek-R1, Simplified Chinese queries elicited the highest bias
rates; these diminished in Traditional Chinese and were nearly absent in
English. Notably, DeepSeek-R1 occasionally responded in Simplified Chinese to
Traditional Chinese queries and amplified existing PRC-aligned terms in its
Chinese answers, demonstrating an "invisible loudspeaker" effect. Furthermore,
such biases were not confined to overtly political topics but also permeated
cultural and lifestyle content, particularly in DeepSeek-R1.

</details>


### [220] [BD at BEA 2025 Shared Task: MPNet Ensembles for Pedagogical Mistake Identification and Localization in AI Tutor Responses](https://arxiv.org/abs/2506.01817)
*Shadman Rohan,Ishita Sur Apan,Muhtasim Ibteda Shochcho,Md Fahim,Mohammad Ashfaq Ur Rahman,AKM Mahbubur Rahman,Amin Ahsan Ali*

Key words: BEA 2025, Pedagogical Ability, AI Tutor, Mistake Identification, Mistake Location, MPNet, Ensemble Learning

TL;DR: Team BD提交了BEA 2025共享任务的教育对话中AI导师教学能力评估的两个任务（错误识别和错误定位），采用基于MPNet的Transformer模型，通过集成方法取得较好成果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 评估AI导师在教育对话中识别和定位学生错误的能力，设计可靠的导师响应评估系统。

Method: 使用MPNet模型，结合BERT和XLNet的预训练优势，通过类加权交叉熵损失和分组交叉验证优化模型，并采用硬投票集成方法提升性能。

Result: 在官方测试集上，错误识别和错误定位的宏F1分数分别为0.7110和0.5543。

Conclusion: 集成方法和全面分析为教育对话中的导师响应评估系统提供了有价值的见解。

Abstract: We present Team BD's submission to the BEA 2025 Shared Task on Pedagogical
Ability Assessment of AI-powered Tutors, under Track 1 (Mistake Identification)
and Track 2 (Mistake Location). Both tracks involve three-class classification
of tutor responses in educational dialogues - determining if a tutor correctly
recognizes a student's mistake (Track 1) and whether the tutor pinpoints the
mistake's location (Track 2). Our system is built on MPNet, a Transformer-based
language model that combines BERT and XLNet's pre-training advantages. We
fine-tuned MPNet on the task data using a class-weighted cross-entropy loss to
handle class imbalance, and leveraged grouped cross-validation (10 folds) to
maximize the use of limited data while avoiding dialogue overlap between
training and validation. We then performed a hard-voting ensemble of the best
models from each fold, which improves robustness and generalization by
combining multiple classifiers. Our approach achieved strong results on both
tracks, with exact-match macro-F1 scores of approximately 0.7110 for Mistake
Identification and 0.5543 for Mistake Location on the official test set. We
include comprehensive analysis of our system's performance, including confusion
matrices and t-SNE visualizations to interpret classifier behavior, as well as
a taxonomy of common errors with examples. We hope our ensemble-based approach
and findings provide useful insights for designing reliable tutor response
evaluation systems in educational dialogue settings.

</details>


### [221] [Not All Jokes Land: Evaluating Large Language Models Understanding of Workplace Humor](https://arxiv.org/abs/2506.01819)
*Moahmmadamin Shafiei,Hamidreza Saffari*

Key words: 人工智能, 大型语言模型, 职业幽默, 适切性判断

TL;DR: 论文探讨了大型语言模型（LLMs）在判断职业幽默适当性方面的局限性，并提出了一个专业幽默数据集。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着AI和LLMs的发展，自动写作等日常任务自动化受到关注，但职业幽默的适切性判断被忽视。

Method: 开发了一个包含专业幽默语句及其适切性特征的数据集，并评估了五种LLMs的表现。

Result: LLMs在判断幽默适切性时表现不佳。

Conclusion: 研究揭示了LLMs在职业幽默领域的局限性，需进一步改进对齐人类价值的模型。

Abstract: With the recent advances in Artificial Intelligence (AI) and Large Language
Models (LLMs), the automation of daily tasks, like automatic writing, is
getting more and more attention. Hence, efforts have focused on aligning LLMs
with human values, yet humor, particularly professional industrial humor used
in workplaces, has been largely neglected. To address this, we develop a
dataset of professional humor statements along with features that determine the
appropriateness of each statement. Our evaluation of five LLMs shows that LLMs
often struggle to judge the appropriateness of humor accurately.

</details>


### [222] [CiteEval: Principle-Driven Citation Evaluation for Source Attribution](https://arxiv.org/abs/2506.01829)
*Yumo Xu,Peng Qi,Jifan Chen,Kunlun Liu,Rujun Han,Lan Liu,Bonan Min,Vittorio Castelli,Arshit Gupta,Zhiguo Wang*

Key words: 引用评估,自然语言推理,基准测试,自动评估

TL;DR: 本文提出CiteEval框架，通过细粒度引用评估解决现有方法的不足，并开发了CiteBench基准和CiteEval-Auto自动评估工具。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前基于NLI的引用评估方法不够全面，无法有效评估多维度引用质量。

Method: 提出CiteEval框架，结合检索上下文、用户查询和生成文本，构建CiteBench基准，并开发CiteEval-Auto自动评估工具。

Result: CiteEval-Auto在多样化系统中表现优于现有指标，能捕捉引用的多维度特性。

Conclusion: CiteEval提供了一种可扩展且基于原则的方法，用于评估和改进模型生成的引用。

Abstract: Citation quality is crucial in information-seeking systems, directly
influencing trust and the effectiveness of information access. Current
evaluation frameworks, both human and automatic, mainly rely on Natural
Language Inference (NLI) to assess binary or ternary supportiveness from cited
sources, which we argue is a suboptimal proxy for citation evaluation. In this
work we introduce CiteEval, a citation evaluation framework driven by
principles focusing on fine-grained citation assessment within a broad context,
encompassing not only the cited sources but the full retrieval context, user
query, and generated text. Guided by the proposed framework, we construct
CiteBench, a multi-domain benchmark with high-quality human annotations on
citation quality. To enable efficient evaluation, we further develop
CiteEval-Auto, a suite of model-based metrics that exhibit strong correlation
with human judgments. Experiments across diverse systems demonstrate
CiteEval-Auto's superior ability to capture the multifaceted nature of
citations compared to existing metrics, offering a principled and scalable
approach to evaluate and improve model-generated citations.

</details>


### [223] [Minimal Pair-Based Evaluation of Code-Switching](https://arxiv.org/abs/2506.01840)
*Igor Sterner,Simone Teufel*

Key words: 代码切换,大型语言模型,最小对,双语者,评估方法

TL;DR: 提出了一种基于最小对的方法，用于评估大型语言模型（LLM）是否像双语者一样使用代码切换（CS），并通过实验验证了模型规模对CS偏好的一致性影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法在语言覆盖、CS现象多样性或可扩展性方面存在不足，缺乏对LLM在CS行为上与双语者一致性的评估。

Method: 通过最小对干预，收集11种语言对的自然CS句子和其变体，对比双语者和LLM对两者的偏好。

Result: 双语者一致偏好自然CS句子，LLM中模型越大，对自然CS句子的概率分配越一致。最大概率差异出现在闭类词操纵的对中。

Conclusion: 模型规模与CS行为一致性正相关，验证了理论假设。

Abstract: There is a lack of an evaluation methodology that estimates the extent to
which large language models (LLMs) use code-switching (CS) in the same way as
bilinguals. Existing methods do not have wide language coverage, fail to
account for the diverse range of CS phenomena, or do not scale. We propose an
intervention based on minimal pairs of CS. Each minimal pair contains one
naturally occurring CS sentence and one minimally manipulated variant. We
collect up to 1,000 such pairs each for 11 language pairs. Our human
experiments show that, for every language pair, bilinguals consistently prefer
the naturally occurring CS sentence. Meanwhile our experiments with current
LLMs show that the larger the model, the more consistently it assigns higher
probability to the naturally occurring CS sentence than to the variant. In
accordance with theoretical claims, the largest probability differences arise
in those pairs where the manipulated material consisted of closed-class words.

</details>


### [224] [Code-Switching and Syntax: A Large-Scale Experiment](https://arxiv.org/abs/2506.01846)
*Igor Sterner,Simone Teufel*

Key words: 代码转换, 句法, 多语言实验, 自动系统, 泛化能力

TL;DR: 论文通过大规模多语言实验验证了句法信息足以预测双语者在句子中的代码转换行为，结果显示自动系统与人类表现相当，且模式可泛化至未见过的语言对。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有理论认为代码转换（CS）可由语言句法解释，但缺乏大规模多语言实验验证。本文旨在填补这一空白。

Method: 设计仅基于句法信息的预测系统，进行大规模多语言跨现象的代码转换实验。

Result: 句法信息足以使自动系统区分代码转换的最小对，效果与人类相当，且模式可泛化至新语言对。

Conclusion: 句法在代码转换中起关键作用，支持了理论假设。

Abstract: The theoretical code-switching (CS) literature provides numerous pointwise
investigations that aim to explain patterns in CS, i.e. why bilinguals switch
language in certain positions in a sentence more often than in others. A
resulting consensus is that CS can be explained by the syntax of the
contributing languages. There is however no large-scale, multi-language,
cross-phenomena experiment that tests this claim. When designing such an
experiment, we need to make sure that the system that is predicting where
bilinguals tend to switch has access only to syntactic information. We provide
such an experiment here. Results show that syntax alone is sufficient for an
automatic system to distinguish between sentences in minimal pairs of CS, to
the same degree as bilingual humans. Furthermore, the learnt syntactic patterns
generalise well to unseen language pairs.

</details>


### [225] [CONFETTI: Conversational Function-Calling Evaluation Through Turn-Level Interactions](https://arxiv.org/abs/2506.01859)
*Tamer Alkhouli,Katerina Margatina,James Gung,Raphael Shu,Claudia Zaghi,Monica Sunkara,Yi Zhang*

Key words: 对话基准, 大语言模型, 功能调用, 复杂对话, API评估

TL;DR: CONFETTI是一个用于评估大语言模型（LLMs）在复杂对话场景中调用功能和响应质量的对话基准，通过109个人工模拟对话覆盖86个API，揭示模型在长对话和多API调用中的表现差异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前基准缺乏对LLMs在复杂对话场景中全面评估的需求，CONFETTI旨在填补这一空白。

Method: 使用109个人工模拟对话（313个用户轮次，覆盖86个API）进行离线策略的轮次级评估，并加入对话行为注释来评估代理响应。

Result: 部分模型能处理长对话和超过20个API的调用，但多数模型在长上下文或增加API数量时表现不佳，串联功能调用的性能普遍受限。表现最佳的模型为Nova Pro（40.01%）、Claude Sonnet v3.5（35.46%）和Llama 3.1 405B（33.19%）。

Conclusion: CONFETTI揭示了模型在复杂对话和功能调用中的局限性，并为未来模型优化提供了方向。

Abstract: We introduce Conversational Function-Calling Evaluation Through Turn-Level
Interactions (CONFETTI), a conversational benchmark1 designed to evaluate the
function-calling capabilities and response quality of large language models
(LLMs). Current benchmarks lack comprehensive assessment of LLMs in complex
conversational scenarios. CONFETTI addresses this gap through 109
human-simulated conversations, comprising 313 user turns and covering 86 APIs.
These conversations explicitly target various conversational complexities, such
as follow-ups, goal correction and switching, ambiguous and implicit goals. We
perform off-policy turn-level evaluation using this benchmark targeting
function-calling. Our benchmark also incorporates dialog act annotations to
assess agent responses. We evaluate a series of state-of-the-art LLMs and
analyze their performance with respect to the number of available APIs,
conversation lengths, and chained function calling. Our results reveal that
while some models are able to handle long conversations, and leverage more than
20+ APIs successfully, other models struggle with longer context or when
increasing the number of APIs. We also report that the performance on chained
function-calls is severely limited across the models. Overall, the top
performing models on CONFETTI are Nova Pro (40.01%), Claude Sonnet v3.5
(35.46%) and Llama 3.1 405B (33.19%) followed by command-r-plus (31.18%) and
Mistral-Large-2407 (30.07%).

</details>


### [226] [Is Extending Modality The Right Path Towards Omni-Modality?](https://arxiv.org/abs/2506.01872)
*Tinghui Zhu,Kai Zhang,Muhao Chen,Yu Su*

Key words: Omni-modal language models, 多模态整合, 模态扩展, 模型融合, 知识共享

TL;DR: OLMs研究多模态整合与语言能力保持，探讨模态扩展对核心语言能力的影响、模型融合实现多模态的效果，以及多模态扩展是否优于顺序扩展。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有开源模型在多模态整合和性能上存在不足，研究者希望通过实验探索实现真正多模态的可能性和方法。

Method: 研究了模态扩展技术，分析了其对核心语言能力的影响，尝试通过模型融合实现多模态整合，并比较了多模态扩展与顺序扩展的效果。

Result: 通过大量实验，分析了不同方法的权衡，提供了关于当前技术能否实现真正多模态的见解。

Conclusion: 研究为未来多模态模型的开发提供了重要参考，但全面实现真正多模态仍需进一步探索。

Abstract: Omni-modal language models (OLMs) aim to integrate and reason over diverse
input modalities--such as text, images, video, and audio--while maintaining
strong language capabilities. Despite recent advancements, existing models,
especially open-source ones, remain far from true omni-modality, struggling to
generalize beyond the specific modality pairs they are trained on or to achieve
strong performance when processing multi-modal inputs. We study the effect of
extending modality, the dominant technique for training multimodal models,
where an off-the-shelf language model is fine-tuned on target-domain and
language data. Specifically, we investigate three key questions: (1) Does
modality extension compromise core language abilities? (2) Can model merging
effectively integrate independently fine-tuned modality-specific models to
achieve omni-modality? (3) Does omni-modality extension lead to better
knowledge sharing and generalization compared to sequential extension? Through
extensive experiments, we analyze these trade-offs and provide insights into
the feasibility of achieving true omni-modality using current approaches.

</details>


### [227] [Spatial Coordinates as a Cell Language: A Multi-Sentence Framework for Imaging Mass Cytometry Analysis](https://arxiv.org/abs/2506.01918)
*Chi-Jane Chen,Yuhang Chen,Sukwon Yun,Natalie Stanley,Tianlong Chen*

Key words: Image mass cytometry, Large language models, Spatial2Sentence, Single-cell analysis, Cell-cell interactions

TL;DR: Spatial2Sentence提出了一种新颖框架，通过多句子方法整合单细胞表达和空间信息，解决了现有单细胞LLMs在空间信息整合和细胞间相互作用处理上的不足。该框架在IMC数据集上表现优异，提升了细胞类型分类和临床状态预测的准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有单细胞大型语言模型（LLMs）在整合空间信息和处理细胞间相互作用方面存在局限，无法有效捕捉生物关系，因此需要一种新方法来填补这一空白。

Method: Spatial2Sentence通过构建表达相似性和距离矩阵，将空间相邻且表达相似的细胞配对为正样本，远距离且表达不相似的细胞配对为负样本，生成多句子表示，使LLMs能够学习细胞在表达和空间背景下的相互作用。

Result: 在预处理IMC数据集上，Spatial2Sentence比现有单细胞LLMs表现更优，糖尿病数据集上的细胞类型分类和临床状态预测分别提高了5.98%和4.18%，并增强了可解释性。

Conclusion: Spatial2Sentence通过整合空间和表达信息，显著提升了单细胞分析的表现和可解释性，为生物医学研究提供了新工具。

Abstract: Image mass cytometry (IMC) enables high-dimensional spatial profiling by
combining mass cytometry's analytical power with spatial distributions of cell
phenotypes. Recent studies leverage large language models (LLMs) to extract
cell states by translating gene or protein expression into biological context.
However, existing single-cell LLMs face two major challenges: (1) Integration
of spatial information: they struggle to generalize spatial coordinates and
effectively encode spatial context as text, and (2) Treating each cell
independently: they overlook cell-cell interactions, limiting their ability to
capture biological relationships. To address these limitations, we propose
Spatial2Sentence, a novel framework that integrates single-cell expression and
spatial information into natural language using a multi-sentence approach.
Spatial2Sentence constructs expression similarity and distance matrices,
pairing spatially adjacent and expressionally similar cells as positive pairs
while using distant and dissimilar cells as negatives. These multi-sentence
representations enable LLMs to learn cellular interactions in both expression
and spatial contexts. Equipped with multi-task learning, Spatial2Sentence
outperforms existing single-cell LLMs on preprocessed IMC datasets, improving
cell-type classification by 5.98% and clinical status prediction by 4.18% on
the diabetes dataset while enhancing interpretability. The source code can be
found here: https://github.com/UNITES-Lab/Spatial2Sentence.

</details>


### [228] [From Guidelines to Practice: A New Paradigm for Arabic Language Model Evaluation](https://arxiv.org/abs/2506.01920)
*Serry Sibaee,Omer Nacar,Adel Ammar,Yasser Al-Habashi,Abdulrahman Al-Batati,Wadii Boulila*

Key words: 阿拉伯语言模型、评估框架、文化理解、ADMD

TL;DR: 本文填补了阿拉伯语言模型评估的关键空白，提出了一套全面的理论指南和新的评估框架。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 分析现有阿拉伯评估数据集的问题，包括语言准确性、文化一致性和方法论严谨性的不足。

Method: 引入阿拉伯深度迷你数据集（ADMD），包含490个涵盖10大领域的问题，用于评估五种领先语言模型。

Result: Claude 3.5 Sonnet整体表现最佳（准确率30%），尤其在数学理论、阿拉伯语言和伊斯兰领域表现突出。

Conclusion: 为阿拉伯语言模型评估提供了理论和实践基础，强调文化能力与技术能力同等重要。

Abstract: This paper addresses critical gaps in Arabic language model evaluation by
establishing comprehensive theoretical guidelines and introducing a novel
evaluation framework. We first analyze existing Arabic evaluation datasets,
identifying significant issues in linguistic accuracy, cultural alignment, and
methodological rigor. To address these limitations in LLMs, we present the
Arabic Depth Mini Dataset (ADMD), a carefully curated collection of 490
challenging questions spanning ten major domains (42 sub-domains, see Figure 1.
Using ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet,
Gemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant
variations in model performance across different domains, with particular
challenges in areas requiring deep cultural understanding and specialized
knowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\%,
showing relative strength in mathematical theory in Arabic, Arabic language,
and islamic domains. This work provides both theoretical foundations and
practical insights for improving Arabic language model evaluation, emphasizing
the importance of cultural competence alongside technical capabilities.

</details>


### [229] [Esoteric Language Models](https://arxiv.org/abs/2506.01928)
*Subham Sekhar Sahoo,Zhihan Yang,Yash Akhauri,Johnna Liu,Deepansha Singh,Zhoujun Cheng,Zhengzhong Liu,Eric Xing,John Thickstun,Arash Vahdat*

Key words: 语言模型、掩蔽扩散模型、自回归模型、KV缓存、推理效率

TL;DR: Eso-LMs融合自回归和掩蔽扩散模型，通过KV缓存和优化采样策略，提升了推理效率和性能，超越了现有模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 结合自回归模型（AR）和掩蔽扩散模型（MDM）的优势，解决MDM在困惑度和推理效率上的不足。

Method: 提出Eso-LMs模型，融合AR与MDM，引入KV缓存并优化采样策略。

Result: 在标准语言建模任务中达到最佳性能，推理效率比标准MDM快65倍，比半自回归方法快4倍。

Conclusion: Eso-LMs通过结合两种模型的优势，实现了高效且高性能的语言生成。

Abstract: Diffusion-based language models offer a compelling alternative to
autoregressive (AR) models by enabling parallel and controllable generation.
Among this family of models, Masked Diffusion Models (MDMs) achieve the
strongest performance but still underperform AR models in perplexity and lack
key inference-time efficiency features--most notably, KV caching. In this work,
we introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,
enabling smooth interpolation between their perplexities while overcoming their
respective limitations. Eso-LMs set a new state of the art on standard language
modeling benchmarks. Crucially, we are the **first to introduce KV caching for
MDMs** while preserving parallel generation, significantly improving inference
efficiency. Combined with an optimized sampling schedule, our method achieves
up to **65x** faster inference than standard MDMs and **4x** faster inference
than prior semi-autoregressive approaches. We provide the code and model
checkpoints on the project page:
[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)

</details>


### [230] [RewardBench 2: Advancing Reward Model Evaluation](https://arxiv.org/abs/2506.01937)
*Saumya Malik,Valentina Pyatkin,Sander Land,Jacob Morrison,Noah A. Smith,Hannaneh Hajishirzi,Nathan Lambert*

Key words: 奖励模型, 评估基准, 下游任务, 人类提示, RLHF

TL;DR: RewardBench 2是一个新的多技能奖励建模基准，旨在提供更具挑战性的数据，用于基于准确性的奖励模型评估，且与下游任务性能高度相关。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前奖励模型在评估方面的进展未能与其在下游任务中的有效性相匹配，因此需要更严格的评估方法。

Method: 通过收集新的人类提示构建RewardBench 2基准，并评估现有模型在该基准上的表现及其与下游任务的相关性。

Result: RewardBench 2的难度更高，模型平均得分比第一版低20分，但与下游任务性能高度相关。

Conclusion: RewardBench 2为奖励模型评估提供了更具挑战性和相关性的基准，有助于更严格的实践。

Abstract: Reward models are used throughout the post-training of language models to
capture nuanced signals from preference data and provide a training target for
optimization across instruction following, reasoning, safety, and more domains.
The community has begun establishing best practices for evaluating reward
models, from the development of benchmarks that test capabilities in specific
skill areas to others that test agreement with human preferences. At the same
time, progress in evaluation has not been mirrored by the effectiveness of
reward models in downstream tasks -- simpler direct alignment algorithms are
reported to work better in many cases. This paper introduces RewardBench 2, a
new multi-skill reward modeling benchmark designed to bring new, challenging
data for accuracy-based reward model evaluation -- models score about 20 points
on average lower on RewardBench 2 compared to the first RewardBench -- while
being highly correlated with downstream performance. Compared to most other
benchmarks, RewardBench 2 sources new human prompts instead of existing prompts
from downstream evaluations, facilitating more rigorous evaluation practices.
In this paper, we describe our benchmark construction process and report how
existing models perform on it, while quantifying how performance on the
benchmark correlates with downstream use of the models in both inference-time
scaling algorithms, like best-of-N sampling, and RLHF training algorithms like
proximal policy optimization.

</details>


### [231] [Novel Benchmark for NER in the Wastewater and Stormwater Domain](https://arxiv.org/abs/2506.01938)
*Franco Alberto Cardillo,Franca Debole,Francesca Frontini,Mitra Aelami,Nanée Chahinian,Serge Conrad*

Key words: 废水管理, NER, 多语言语料库, LLM, 标注投影

TL;DR: 论文提出了一种法语-意大利语的废水管理领域特定文本语料库，用于评估最先进的NER方法，支持未来的多语言决策支持工具开发。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 废水管理的有效信息提取需要处理领域特定术语和多语言挑战，NER是关键步骤。

Method: 开发法语-意大利语废水管理文本语料库，评估LLM-based等NER方法，探索自动化标注投影。

Result: 提供了可靠的NER基准，支持多语言废水管理决策。

Conclusion: 该语料库和方法为废水管理领域的信息提取和决策支持提供了有效工具。

Abstract: Effective wastewater and stormwater management is essential for urban
sustainability and environmental protection. Extracting structured knowledge
from reports and regulations is challenging due to domainspecific terminology
and multilingual contexts. This work focuses on domain-specific Named Entity
Recognition (NER) as a first step towards effective relation and information
extraction to support decision making. A multilingual benchmark is crucial for
evaluating these methods. This study develops a French-Italian domain-specific
text corpus for wastewater management. It evaluates state-of-the-art NER
methods, including LLM-based approaches, to provide a reliable baseline for
future strategies and explores automated annotation projection in view of an
extension of the corpus to new languages.

</details>


### [232] [Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.01939)
*Shenzhi Wang,Le Yu,Chang Gao,Chujie Zheng,Shixuan Liu,Rui Lu,Kai Dang,Xionghui Chen,Jianxin Yang,Zhenru Zhang,Yuqiong Liu,An Yang,Andrew Zhao,Yang Yue,Shiji Song,Bowen Yu,Gao Huang,Junyang Lin*

Key words: 强化学习与可验证奖励（RLVR）；令牌熵模式；大语言模型；推理优化

TL;DR: 本文通过研究令牌熵模式，揭示了强化学习与可验证奖励（RLVR）优化大语言模型（LLMs）推理性能的关键机制：高熵令牌（即分叉令牌）对推理路径的引导作用。通过限制策略梯度更新仅作用于分叉令牌，RLVR在性能上超越了全梯度更新方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索RLVR如何通过令牌熵模式优化LLMs的推理性能，填补了对该机制理解不足的空白。

Method: 分析Chain-of-Thought推理中的令牌熵模式，研究RLVR训练中熵模式的演变，并通过限制策略梯度更新至高熵令牌来优化RLVR。

Result: 仅使用20%的高熵令牌即可保持模型性能，并在大型模型（如Qwen3-32B）上显著超越全梯度更新（如AIME分数提升11.04）。

Conclusion: RLVR的优化效果主要来源于对高熵令牌的调整，通过分析令牌熵模式可以进一步优化LLMs的推理性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful approach to enhancing the reasoning capabilities of Large Language
Models (LLMs), while its mechanisms are not yet well understood. In this work,
we undertake a pioneering exploration of RLVR through the novel perspective of
token entropy patterns, comprehensively analyzing how different tokens
influence reasoning performance. By examining token entropy patterns in
Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of
tokens exhibit high entropy, and these tokens act as critical forks that steer
the model toward diverse reasoning pathways. Furthermore, studying how entropy
patterns evolve during RLVR training reveals that RLVR largely adheres to the
base model's entropy patterns, primarily adjusting the entropy of high-entropy
tokens. These findings highlight the significance of high-entropy tokens (i.e.,
forking tokens) to RLVR. We ultimately improve RLVR by restricting policy
gradient updates to forking tokens and uncover a finding even beyond the 80/20
rule: utilizing only 20% of the tokens while maintaining performance comparable
to full-gradient updates on the Qwen3-8B base model and significantly
surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71
on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models,
highlighting a strong scaling trend. In contrast, training exclusively on the
80% lowest-entropy tokens leads to a marked decline in performance. These
findings indicate that the efficacy of RLVR primarily arises from optimizing
the high-entropy tokens that decide reasoning directions. Collectively, our
results highlight the potential to understand RLVR through a token-entropy
perspective and optimize RLVR by leveraging high-entropy minority tokens to
further improve LLM reasoning.

</details>


### [233] [Self-ensemble: Mitigating Confidence Distortion for Large Language Models](https://arxiv.org/abs/2506.01951)
*Zicheng Xu,Guanchu Wang,Guangyao Zheng,Yu-Neng Chuang,Alexander Szalay,Xia Hu,Vladimir Braverman*

Key words: 大语言模型, 多选问题, 信心失真, 自集成

TL;DR: 自集成方法通过分组选择并集成预测，解决了大语言模型在多选问题中的信心失真问题，无需调参即可提升性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大语言模型在多选问题中存在信心失真问题，表现为对正确答案信心不足和对错误答案过度自信，影响性能。

Method: 提出自集成方法，将选项分组并通过注意力掩码和位置编码集成预测，无需标记数据集调参。

Result: 在三个大语言模型和数据集上的实验表明，自集成方法显著改善了信心失真问题，性能优于标准推理和基线方法。

Conclusion: 自集成方法是一种即插即用的解决方案，有效提升了大语言模型在多选问题中的表现。

Abstract: Although Large Language Models (LLMs) perform well in general fields, they
exhibit a confidence distortion problem on multi-choice question-answering
(MCQA), particularly as the number of answer choices increases. Specifically,
on MCQA with many choices, LLMs suffer from under-confidence in correct
predictions and over-confidence in incorrect ones, leading to a substantially
degraded performance. To solve this problem, we propose Self-ensemble in this
work. Our method splits the choices into several groups and ensembles LLM
predictions across these groups to reach a final decision. The advantage of
Self-ensemble is its plug-and-play nature, where it can be integrated into
existing LLM architecture based on a designed attention mask and positional
encoding, without requiring labeled datasets for parameter tuning. Experimental
results on three LLMs and datasets demonstrate that Self-ensemble
comprehensively addresses the confidence distortion problem of LLMs,
outperforming standard inference as well as baseline methods.

</details>


### [234] [WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks](https://arxiv.org/abs/2506.01952)
*Atsuyuki Miyai,Zaiying Zhao,Kazuki Egashira,Atsuki Sato,Tatsumi Sunada,Shota Onohara,Hiromasa Yamanishi,Mashiro Toyooka,Kunato Nishina,Ryoma Maeda,Kiyoharu Aizawa,Toshihiko Yamasaki*

Key words: 大型语言模型, WebChoreArena, 基准测试, 复杂任务, 机器浏览代理

TL;DR: WebChoreArena是一个新的可复现基准，包含532个任务，旨在测试大型语言模型在复杂、繁琐任务中的表现，结果显示当前模型仍有改进空间。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了评估大型语言模型（LLM）在超越常规浏览的复杂、繁琐任务中的能力，而不仅仅是常规浏览任务。

Method: 通过WebChoreArena基准，系统性整合了三个关键挑战：大规模记忆任务、精确计算任务和长期记忆任务，并在WebArena的四个模拟环境中实现严格可复现性。

Result: 实验结果显示，随着模型升级（如GPT-4o、Claude 3.7 Sonnet、Gemini 2.5 Pro），性能显著提升，但仍远未达到WebArena的水平，表明WebChoreArena的挑战性更高。

Conclusion: WebChoreArena能有效衡量LLM的进展，但当前模型在复杂任务中仍有较大改进空间。

Abstract: Powered by a large language model (LLM), a web browsing agent operates web
browsers in a human-like manner and offers a highly transparent path toward
automating a wide range of everyday tasks. As web agents become increasingly
capable and demonstrate proficiency in general browsing tasks, a critical
question emerges: Can they go beyond general browsing to robustly handle tasks
that are tedious and complex, or chores that humans often avoid doing
themselves? In this paper, we introduce WebChoreArena, a new fully reproducible
benchmark comprising 532 carefully curated tasks designed to extend the scope
of WebArena beyond general browsing to more labor-intensive and tedious tasks.
WebChoreArena systematically integrates three key challenges: (i) Massive
Memory tasks requiring accurate retrieval of large amounts of information in
the observations, (ii) Calculation tasks demanding precise mathematical
reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory
across multiple webpages. Built on top of the fully reproducible and widely
adopted four WebArena simulation environments, WebChoreArena ensures strict
reproducibility and enables fair, direct comparisons with the established
WebArena benchmark, offering key insights into agent progress. Our experimental
results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7
Sonnet, and Gemini 2.5 Pro, significant improvements in performance are
observed on WebChoreArena. These findings suggest that WebChoreArena is
well-suited to measure the advancement of state-of-the-art LLMs with greater
clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro,
there remains substantial room for improvement compared to WebArena,
highlighting the increased challenges posed by WebChoreArena.

</details>


### [235] [DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation](https://arxiv.org/abs/2506.01954)
*Jennifer Chen,Aidar Myrzakhan,Yaxin Luo,Hassaan Muhammad Khan,Sondos Mahmoud Bsharat,Zhiqiang Shen*

Key words: 检索增强生成, 知识蒸馏, 幻觉内容, 小型语言模型, 知识图

TL;DR: 提出了一种名为 DRAG 的新框架，通过知识蒸馏将大型语言模型（LLM）的知识压缩到小型语言模型（SLM）中，以降低计算成本并减少幻觉内容，同时在多个基准测试中表现优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的检索增强生成（RAG）方法虽然有效，但计算资源消耗大且容易产生幻觉内容，因此需要一种更高效且可靠的方法。

Method: DRAG 框架利用了基于证据和知识图的知识蒸馏方法，确保小型模型在减少规模的同时保留关键事实知识。

Result: 实验表明，DRAG 在多个基准测试中比现有方法（如 MiniRAG）性能提升高达 27.7%，同时保持了高效和可靠性。

Conclusion: DRAG 提供了一种实用且资源高效的解决方案，能够在小型模型中实现增强的检索和生成能力。

Abstract: Retrieval-Augmented Generation (RAG) methods have proven highly effective for
tasks requiring factual consistency and robust knowledge retrieval. However,
large-scale RAG systems consume significant computational resources and are
prone to generating hallucinated content from Humans. In this work, we
introduce $\texttt{DRAG}$, a novel framework for distilling RAG knowledge from
large-scale Language Models (LLMs) into small LMs (SLMs). Our approach
leverages evidence- and knowledge graph-based distillation, ensuring that the
distilled model retains critical factual knowledge while significantly reducing
model size and computational cost. By aligning the smaller model's predictions
with a structured knowledge graph and ranked evidence, $\texttt{DRAG}$
effectively mitigates hallucinations and improves factual accuracy. We further
present a case demonstrating how our framework mitigates user privacy risks and
introduce a corresponding benchmark. Experimental evaluations on multiple
benchmarks demonstrate that our method outperforms the prior competitive RAG
methods like MiniRAG for SLMs by up to 27.7% using the same models, preserving
high-level efficiency and reliability. With $\texttt{DRAG}$, we provide a
practical and resource-efficient roadmap to deploying enhanced retrieval and
generation capabilities in small-sized LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [236] [Modality Equilibrium Matters: Minor-Modality-Aware Adaptive Alternating for Cross-Modal Memory Enhancement](https://arxiv.org/abs/2506.00030)
*Xiang Shi,Rui Zhang,Jiawei Liu,Yinpeng Liu,Qikai Cheng,Wei Lu*

Key words: 多模态融合, Shapley值, 模态平衡, 交替训练, 记忆模块

TL;DR: 提出了一种基于Shapley值的交替训练框架，通过自适应优先处理弱模态来平衡多模态融合，解决了模态不平衡问题，并在多个基准数据集上取得了SOTA结果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多模态融合中，强势模态容易压制弱势模态，导致学习偏差和次优融合，尤其是在模态不完整的情况下。

Method: 采用Shapley值调度策略自适应优化训练序列，引入记忆模块和跨模态映射机制对齐特征，并支持传统和LLM编码器。

Result: 在四个多模态基准数据集上实现了平衡和准确性的SOTA表现，且在缺失模态下表现出强鲁棒性。

Conclusion: 交替训练和模态优先级策略能有效平衡多模态学习，为多模态训练优化提供了新范式。

Abstract: Multimodal fusion is susceptible to modality imbalance, where dominant
modalities overshadow weak ones, easily leading to biased learning and
suboptimal fusion, especially for incomplete modality conditions. To address
this problem, we propose a Shapley-guided alternating training framework that
adaptively prioritizes minor modalities to balance and thus enhance the fusion.
Our method leverages Shapley Value-based scheduling to improve the training
sequence adaptively, ensuring that under-optimized modalities receive
sufficient learning. Additionally, we introduce the memory module to refine and
inherit modality-specific representations with a cross-modal mapping mechanism
to align features at both the feature and sample levels. To further validate
the adaptability of the proposed approach, the encoder module empirically
adopts both conventional and LLM-based backbones. With building up a novel
multimodal equilibrium metric, namely, equilibrium deviation metric (EDM), we
evaluate the performance in both balance and accuracy across four multimodal
benchmark datasets, where our method achieves state-of-the-art (SOTA) results.
Meanwhile, robustness analysis under missing modalities highlights its strong
generalization capabilities. Accordingly, our findings reveal the untapped
potential of alternating training, demonstrating that strategic modality
prioritization fundamentally balances and promotes multimodal learning,
offering a new paradigm for optimizing multimodal training dynamics.

</details>


### [237] [AbsoluteNet: A Deep Learning Neural Network to Classify Cerebral Hemodynamic Responses of Auditory Processing](https://arxiv.org/abs/2506.00039)
*Behtom Adeli,John Mclinden,Pankaj Pandey,Ming Shao,Yalda Shahriari*

Key words: 深度学习, fNIRS, 脑机接口, 时空卷积, 定制化激活函数

TL;DR: 该论文提出了一种名为AbsoluteNet的新型深度学习架构，用于分类通过fNIRS记录的听觉事件相关响应，并通过实验验证其在解码血液动力学响应方面的优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 基于深度学习的方法在解码fNIRS捕获的血液动力学响应方面显示出潜力，尤其是在脑机接口应用中，因此开发更高效的模型具有重要意义。

Method: AbsoluteNet基于时空卷积和定制化激活函数的原理构建，与fNIRSNET、MDNN、DeepConvNet和ShallowConvNet等模型进行比较。

Result: AbsoluteNet在二元分类中达到87.0%的准确率、84.8%的敏感性和89.2%的特异性，性能优于其他模型。

Conclusion: AbsoluteNet在解码与听觉处理相关的血液动力学响应方面表现出色，强调了时空特征聚合和定制化激活函数的重要性。

Abstract: In recent years, deep learning (DL) approaches have demonstrated promising
results in decoding hemodynamic responses captured by functional near-infrared
spectroscopy (fNIRS), particularly in the context of brain-computer interface
(BCI) applications. This work introduces AbsoluteNet, a novel deep learning
architecture designed to classify auditory event-related responses recorded
using fNIRS. The proposed network is built upon principles of spatio-temporal
convolution and customized activation functions. Our model was compared against
several models, namely fNIRSNET, MDNN, DeepConvNet, and ShallowConvNet. The
results showed that AbsoluteNet outperforms existing models, reaching 87.0%
accuracy, 84.8% sensitivity, and 89.2% specificity in binary classification,
surpassing fNIRSNET, the second-best model, by 3.8% in accuracy. These findings
underscore the effectiveness of our proposed deep learning model in decoding
hemodynamic responses related to auditory processing and highlight the
importance of spatio-temporal feature aggregation and customized activation
functions to better fit fNIRS dynamics.

</details>


### [238] [Adapting Offline Reinforcement Learning with Online Delays](https://arxiv.org/abs/2506.00131)
*Simon Sinong Zhan,Qingyuan Wu,Frank Yang,Xiangyu Shi,Chao Huang,Qi Zhu*

Key words: 强化学习、离线RL、延迟处理、变压器、信念预测

TL;DR: DT-CORL是一种离线强化学习框架，通过变压器信念预测器处理延迟动态，提高了部署时的性能和数据效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决离线强化学习代理在部署时面临的模拟到现实的延迟问题和交互数据分布不匹配问题。

Method: 使用变压器信念预测器生成延迟鲁棒的动作，同时保持数据高效性。

Result: 在D4RL基准测试中，DT-CORL优于历史增强和基于信念的方法。

Conclusion: DT-CORL有效缩小了模拟到现实的延迟差距，同时保持了数据效率。

Abstract: Offline-to-online deployment of reinforcement-learning (RL) agents must
bridge two gaps: (1) the sim-to-real gap, where real systems add latency and
other imperfections not present in simulation, and (2) the interaction gap,
where policies trained purely offline face out-of-distribution states during
online execution because gathering new interaction data is costly or risky.
Agents therefore have to generalize from static, delay-free datasets to
dynamic, delay-prone environments. Standard offline RL learns from delay-free
logs yet must act under delays that break the Markov assumption and hurt
performance. We introduce DT-CORL (Delay-Transformer belief policy Constrained
Offline RL), an offline-RL framework built to cope with delayed dynamics at
deployment. DT-CORL (i) produces delay-robust actions with a transformer-based
belief predictor even though it never sees delayed observations during
training, and (ii) is markedly more sample-efficient than na\"ive
history-augmentation baselines. Experiments on D4RL benchmarks with several
delay settings show that DT-CORL consistently outperforms both
history-augmentation and vanilla belief-based methods, narrowing the
sim-to-real latency gap while preserving data efficiency.

</details>


### [239] [Tradeoffs between Mistakes and ERM Oracle Calls in Online and Transductive Online Learning](https://arxiv.org/abs/2506.00135)
*Idan Attias,Steve Hanneke,Arvind Ramaswami*

Key words: 在线学习，转换在线学习，经验风险最小化，弱一致性，Littlestone维度，预言机调用

TL;DR: 该论文研究了通过经验风险最小化（ERM）或弱一致性预言机与概念类交互的在线学习和转换在线学习模型，证明了在ERM访问下的紧下界，并将现有结果扩展到弱一致性设置。在转换在线模型中，展示了最优错误界所需预言机调用次数，并指出限制预言机调用的必要性。此外，对于特定概念类，通过随机算法减少了预言机调用。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究在仅通过ERM或弱一致性预言机与概念类交互时的在线学习性能，填补标准在线模型中的知识空白。

Method: 通过理论分析和证明，比较ERM和弱一致性预言机的效果，并在转换在线模型中探索预言机调用的优化。随机算法用于减少特定概念类的预言机调用。

Result: 证明在ERM访问下的紧下界（如$Ω(2^{d_{VC}})$错误），展示了弱一致性设置增加的$O(T)$调用成本，并在转换在线模型中实现最优错误界。随机算法显著减少了特定类的预言机调用。

Conclusion: 仅通过ERM或弱一致性预言机交互的在线学习模型在理论上是可行的，但预言机调用次数的优化和限制是关键。随机算法为特定类提供了高效解决方案。

Abstract: We study online and transductive online learning when the learner interacts
with the concept class only via Empirical Risk Minimization (ERM) or weak
consistency oracles on arbitrary instance subsets. This contrasts with standard
online models, where the learner knows the entire class. The ERM oracle returns
a hypothesis minimizing loss on a given subset, while the weak consistency
oracle returns a binary signal indicating whether the subset is realizable by
some concept. The learner is evaluated by the number of mistakes and oracle
calls. In the standard online setting with ERM access, we prove tight lower
bounds in both realizable and agnostic cases: $\Omega(2^{d_{VC}})$ mistakes and
$\Omega(\sqrt{T 2^{d_{LD}}})$ regret, where $T$ is the number of timesteps and
$d_{LD}$ is the Littlestone dimension. We further show that existing online
learning results with ERM access carry over to the weak consistency setting,
incurring an additional $O(T)$ in oracle calls. We then consider the
transductive online model, where the instance sequence is known but labels are
revealed sequentially. For general Littlestone classes, we show that optimal
realizable and agnostic mistake bounds can be achieved using $O(T^{d_{VC}+1})$
weak consistency oracle calls. On the negative side, we show that limiting the
learner to $\Omega(T)$ weak consistency queries is necessary for transductive
online learnability, and that restricting the learner to $\Omega(T)$ ERM
queries is necessary to avoid exponential dependence on the Littlestone
dimension. Finally, for certain concept classes, we reduce oracle calls via
randomized algorithms while maintaining similar mistake bounds. In particular,
for Thresholds on an unknown ordering, $O(\log T)$ ERM queries suffice; for
$k$-Intervals, $O(T^3 2^{2k})$ weak consistency queries suffice.

</details>


### [240] [On Designing Diffusion Autoencoders for Efficient Generation and Representation Learning](https://arxiv.org/abs/2506.00136)
*Magdalena Proszewska,Nikolay Malkin,N. Siddharth*

Key words: 扩散自编码器, 潜变量, 生成模型, 领域迁移, 高效建模

TL;DR: 论文摘要讨论了扩散自编码器（DAs）及其变体DMZ通过优化潜变量设计和条件方法，实现了下游任务的有效表征和更高效的生成建模。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究旨在解决扩散自编码器中潜变量建模和样本生成的问题，同时结合另一类扩散模型的优势，提出更高效的模型设计。

Method: 通过分析DA框架中的设计决策（如潜变量选择和条件方法），提出新的模型DMZ，结合了两类扩散模型的优点。

Result: DMZ模型在下游任务（如领域迁移）中表现优异，且生成效率更高，减少了去噪步骤的需求。

Conclusion: 通过优化设计决策，DMZ模型在表征能力和生成效率上取得了平衡，为扩散模型的研究提供了新方向。

Abstract: Diffusion autoencoders (DAs) are variants of diffusion generative models that
use an input-dependent latent variable to capture representations alongside the
diffusion process. These representations, to varying extents, can be used for
tasks such as downstream classification, controllable generation, and
interpolation. However, the generative performance of DAs relies heavily on how
well the latent variables can be modelled and subsequently sampled from. Better
generative modelling is also the primary goal of another class of diffusion
models -- those that learn their forward (noising) process. While effective at
adjusting the noise process in an input-dependent manner, they must satisfy
additional constraints derived from the terminal conditions of the diffusion
process. Here, we draw a connection between these two classes of models and
show that certain design decisions (latent variable choice, conditioning
method, etc.) in the DA framework -- leading to a model we term DMZ -- allow us
to obtain the best of both worlds: effective representations as evaluated on
downstream tasks, including domain transfer, as well as more efficient
modelling and generation with fewer denoising steps compared to standard DMs.

</details>


### [241] [Aligning Language Models with Observational Data: Opportunities and Risks from a Causal Perspective](https://arxiv.org/abs/2506.00152)
*Erfan Loghmani*

Key words: 大型语言模型, 观测数据, 混杂变量, 微调, 因果推断

TL;DR: 研究探索如何利用历史观测数据微调大型语言模型，指出直接微调会导致虚假相关性问题，并提出了去除混杂变量影响的DeconfoundLM方法，验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 预训练模型在适应人类偏好或业务目标时表现不足，而高质量标注数据获取成本高。观测数据虽丰富，但直接使用存在风险，需解决混杂变量问题。

Method: 提出了DeconfoundLM方法，通过显式去除奖励信号中的已知混杂变量影响，恢复因果关系并改进模型微调效果。

Result: 实证表明DeconfoundLM能有效识别因果关系，减少因混杂变量导致的模型失效问题。

Conclusion: 通过正确的因果校正，观测数据可以作为微调大型语言模型的有效资源。

Abstract: Large language models are being widely used across industries to generate
content that contributes directly to key performance metrics, such as
conversion rates. Pretrained models, however, often fall short when it comes to
aligning with human preferences or optimizing for business objectives. As a
result, fine-tuning with good-quality labeled data is essential to guide models
to generate content that achieves better results. Controlled experiments, like
A/B tests, can provide such data, but they are often expensive and come with
significant engineering and logistical challenges. Meanwhile, companies have
access to a vast amount of historical (observational) data that remains
underutilized. In this work, we study the challenges and opportunities of
fine-tuning LLMs using observational data. We show that while observational
outcomes can provide valuable supervision, directly fine-tuning models on such
data can lead them to learn spurious correlations. We present empirical
evidence of this issue using various real-world datasets and propose
DeconfoundLM, a method that explicitly removes the effect of known confounders
from reward signals. Using simulation experiments, we demonstrate that
DeconfoundLM improves the recovery of causal relationships and mitigates
failure modes found in fine-tuning methods that ignore or naively incorporate
confounding variables. Our findings highlight that while observational data
presents risks, with the right causal corrections, it can be a powerful source
of signal for LLM alignment. Please refer to the project page for code and
related resources.

</details>


### [242] [Privacy Amplification in Differentially Private Zeroth-Order Optimization with Hidden States](https://arxiv.org/abs/2506.00158)
*Eli Chien,Wei-Ning Chen,Pan Li*

Key words: 零阶优化，差分隐私，隐私放大，算法设计，大语言模型

TL;DR: 本文探讨了零阶优化方法在差分隐私（DP）下的收敛性和算法设计，证明了其隐私边界，并提出新算法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管一阶方法在隐私分析中被广泛研究，但零阶方法的隐私分析和算法设计仍未得到充分探讨。本文旨在填补这一空白。

Method: 通过将隐私放大框架推广到零阶优化中的平滑损失函数，分析了零阶优化的收敛性，并提出新的DP零阶算法。

Result: 证明了零阶优化的收敛DP边界，并提出了此前文献中未知的更好的DP零阶算法设计。

Conclusion: 研究填补了零阶方法在DP分析上的空白，为领域特定数据的微调提供了新思路。

Abstract: Zeroth-order optimization has emerged as a promising approach for fine-tuning
large language models on domain-specific data, particularly under differential
privacy (DP) and memory constraints. While first-order methods have been
extensively studied from a privacy perspective, the privacy analysis and
algorithmic design for zeroth-order methods remain significantly underexplored.
A critical open question concerns hidden-state DP analysis: although convergent
privacy bounds are known for first-order methods, it has remained unclear
whether similar guarantees can be established for zeroth-order methods. In this
work, we provide an affirmative answer by proving a convergent DP bound for
zeroth-order optimization. Our analysis generalizes the celebrated privacy
amplification-by-iteration framework to the setting of smooth loss functions in
zeroth-order optimization. Furthermore, it induces better DP zeroth-order
algorithmic designs that are previously unknown to the literature.

</details>


### [243] [Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment](https://arxiv.org/abs/2506.00166)
*Kundan Krishna,Joseph Y Cheng,Charles Maalouf,Leon A Gatys*

Key words: AI安全, 解耦安全适配器, 幻觉检测, 仇恨言论分类, 动态对齐

TL;DR: 提出了一种解耦安全适配器（DSA）框架，通过分离安全计算与任务优化模型，在保持推理效率的同时实现灵活多样的安全功能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有AI安全范式（如护栏模型和对齐训练）通常会在推理效率或开发灵活性上做出妥协，DSA旨在解决这一问题。

Method: DSA利用轻量级适配器，基于基础模型的内部表示实现安全功能，显著减少对推理成本的影响。

Result: DSA在幻觉检测（AUC 0.88）、仇恨言论分类（AUC 0.98）及不安全输入/响应识别（AUC 0.93）上表现优异，且支持动态对齐强度调整。

Conclusion: DSA为模块化、高效且适应性强的AI安全和对齐提供了有前景的路径。

Abstract: Existing paradigms for ensuring AI safety, such as guardrail models and
alignment training, often compromise either inference efficiency or development
flexibility. We introduce Disentangled Safety Adapters (DSA), a novel framework
addressing these challenges by decoupling safety-specific computations from a
task-optimized base model. DSA utilizes lightweight adapters that leverage the
base model's internal representations, enabling diverse and flexible safety
functionalities with minimal impact on inference cost. Empirically, DSA-based
safety guardrails substantially outperform comparably sized standalone models,
notably improving hallucination detection (0.88 vs. 0.61 AUC on Summedits) and
also excelling at classifying hate speech (0.98 vs. 0.92 on ToxiGen) and unsafe
model inputs and responses (0.93 vs. 0.90 on AEGIS2.0 & BeaverTails).
Furthermore, DSA-based safety alignment allows dynamic, inference-time
adjustment of alignment strength and a fine-grained trade-off between
instruction following performance and model safety. Importantly, combining the
DSA safety guardrail with DSA safety alignment facilitates context-dependent
alignment strength, boosting safety on StrongReject by 93% while maintaining
98% performance on MTBench -- a total reduction in alignment tax of 8
percentage points compared to standard safety alignment fine-tuning. Overall,
DSA presents a promising path towards more modular, efficient, and adaptable AI
safety and alignment.

</details>


### [244] [Breakpoint: Scalable evaluation of system-level reasoning in LLM code agents](https://arxiv.org/abs/2506.00172)
*Kaivalya Hariharan,Uzay Girit,Atticus Wang,Jacob Andreas*

Key words: 长程推理,代码修复,任务难度,评估基准

TL;DR: Breakpoint是一种通过自动生成代码修复任务来评估LLMs长程推理能力的方法，无需依赖人工干预。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有评估方法依赖人工问题集，扩展性和难度调整受限，无法满足复杂任务需求。

Method: 通过对抗性破坏真实软件库中的函数，控制任务难度（本地推理和系统级推理）。

Result: 在900多个生成任务中，最先进模型的成功率从55%降至0%。

Conclusion: Breakpoint能扩展到任意难度，适用于复杂任务评估。

Abstract: Benchmarks for large language models (LLMs) have predominantly assessed
short-horizon, localized reasoning. Existing long-horizon suites (e.g.
SWE-bench) rely on manually curated issues, so expanding or tuning difficulty
demands expensive human effort and evaluations quickly saturate. However, many
real-world tasks, such as software engineering or scientific research, require
agents to rapidly comprehend and manipulate novel, complex structures
dynamically; evaluating these capabilities requires the ability to construct
large and varied sets of problems for agents to solve. We introduce Breakpoint,
a benchmarking methodology that automatically generates code-repair tasks by
adversarially corrupting functions within real-world software repositories.
Breakpoint systematically controls task difficulty along two clear dimensions:
local reasoning (characterized by code complexity metrics such as cyclomatic
complexity) and system-level reasoning (characterized by call-graph centrality
and the number of simultaneously corrupted interdependent functions). In
experiments across more than 900 generated tasks we demonstrate that our
methodology can scale to arbitrary difficulty, with state-of-the-art models'
success rates ranging from 55% on the easiest tasks down to 0% on the hardest.

</details>


### [245] [Accountability Attribution: Tracing Model Behavior to Training Processes](https://arxiv.org/abs/2506.00175)
*Shichang Zhang,Hongzhe Du,Karim Saraipour,Jiaqi W. Ma,Himabindu Lakkaraju*

Key words: AI问责性, 训练阶段分析, 反事实估计, 模型行为溯源

TL;DR: 提出了一种框架，用于追踪AI模型行为背后的训练阶段责任，通过一阶近似方法量化各阶段影响，无需重新训练。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决AI模型开发中多阶段训练后的责任归属问题，明确各阶段对模型行为的贡献。

Method: 基于反事实问题，设计了一阶近似估计器，量化训练阶段对模型行为的影响，考虑数据和优化动态。

Result: 实验验证了框架能有效识别特定行为背后的责任训练阶段。

Conclusion: 该框架为模型分析和增强AI开发问责性提供了实用工具。

Abstract: Modern AI development pipelines often involve multiple stages-pretraining,
fine-tuning rounds, and subsequent adaptation or alignment-with numerous model
update steps within each stage. This raises a critical question of
accountability: when a deployed model succeeds or fails, which stage is
responsible, and to what extent? We pose the problem of accountability
attribution, which aims to trace model behavior back to specific stages of the
training process. To address this, we propose a general framework that answers
counterfactual questions about stage effects: how would the model behavior have
changed if the updates from a training stage had not been executed?. Within
this framework, we introduce estimators based on first-order approximations
that efficiently quantify the stage effects without retraining. Our estimators
account for both the training data and key aspects of optimization dynamics,
including learning rate schedules, momentum, and weight decay. Empirically, we
demonstrate that our approach identifies training stages accountable for
specific behaviors, offering a practical tool for model analysis and a step
toward more accountable AI development.

</details>


### [246] [On the Interaction of Noise, Compression Role, and Adaptivity under $(L_0, L_1)$-Smoothness: An SDE-based Approach](https://arxiv.org/abs/2506.00181)
*Enea Monzio Compagnoni,Rustem Islamov,Antonio Orvieto,Eduard Gorbunov*

Key words: 分布式优化，SGD，SignSGD，SDE，自适应学习率，噪声

TL;DR: 论文通过SDE近似研究了分布式SGD、压缩SGD和SignSGD在灵活噪声假设下的动态，揭示了批噪声、梯度压缩和自适应方法的复杂互动。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨现代理论框架下分布式优化方法的收敛性，特别是在噪声和自适应学习率条件下的表现。

Method: 使用随机微分方程（SDE）近似分析分布式SGD、压缩SGD和SignSGD的动态行为，并结合模拟验证。

Result: 自适应方法（如SignSGD）能在标准学习率调度下收敛，而预调度衰减学习率的SGD类方法除非依赖梯度范数，否则无法收敛。

Conclusion: 自适应方法在噪声条件下的鲁棒性更强，而传统SGD需依赖梯度范数调整学习率才能收敛。

Abstract: Using stochastic differential equation (SDE) approximations, we study the
dynamics of Distributed SGD, Distributed Compressed SGD, and Distributed
SignSGD under $(L_0,L_1)$-smoothness and flexible noise assumptions. Our
analysis provides insights -- which we validate through simulation -- into the
intricate interactions between batch noise, stochastic gradient compression,
and adaptivity in this modern theoretical setup. For instance, we show that
\textit{adaptive} methods such as Distributed SignSGD can successfully converge
under standard assumptions on the learning rate scheduler, even under
heavy-tailed noise. On the contrary, Distributed (Compressed) SGD with
pre-scheduled decaying learning rate fails to achieve convergence, unless such
a schedule also accounts for an inverse dependency on the gradient norm -- de
facto falling back into an adaptive method.

</details>


### [247] [Cluster-Aware Causal Mixer for Online Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2506.00188)
*Md Mahmuddun Nabi Murad,Yasin Yilmaz*

Key words: 时间序列；异常检测；因果关系；聚类；多变量

TL;DR: 提出了一种基于聚类感知因果混合器的新型模型，用于多变量时间序列中的异常检测，通过分组和因果机制提升性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有MLP-based mixer模型缺乏因果关系机制，且多变量时间序列中通道间的复杂关系未被有效捕捉。

Method: 通过聚类将通道分组，每组使用专用嵌入层处理，并引入因果混合器保持时间依赖性。

Result: 在六个公开基准数据集上，模型表现出优异的F1分数。

Conclusion: 提出的模型能有效检测异常，适用于实时任务。

Abstract: Early and accurate detection of anomalies in time series data is critical,
given the significant risks associated with false or missed detections. While
MLP-based mixer models have shown promise in time series analysis, they lack a
causality mechanism to preserve temporal dependencies inherent in the system.
Moreover, real-world multivariate time series often contain numerous channels
with diverse inter-channel correlations. A single embedding mechanism for all
channels does not effectively capture these complex relationships. To address
these challenges, we propose a novel cluster-aware causal mixer to effectively
detect anomalies in multivariate time series. Our model groups channels into
clusters based on their correlations, with each cluster processed through a
dedicated embedding layer. In addition, we introduce a causal mixer in our
model, which mixes the information while maintaining causality. Furthermore, we
present an anomaly detection framework that accumulates the anomaly evidence
over time to prevent false positives due to nominal outliers. Our proposed
model operates in an online fashion, making it suitable for real-time
time-series anomaly detection tasks. Experimental evaluations across six public
benchmark datasets demonstrate that our model consistently achieves superior F1
scores.

</details>


### [248] [MOFGPT: Generative Design of Metal-Organic Frameworks using Language Models](https://arxiv.org/abs/2506.00198)
*Srivathsan Badrinarayanan,Rishikesh Magar,Akshay Antony,Radheesh Sharma Meda,Amir Barati Farimani*

Key words: MOFs, 强化学习, 生成模型, 材料发现, 反演设计

TL;DR: 提出了一种结合强化学习的生成模型框架，用于设计具有特定功能的金属有机框架（MOFs）。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: MOFs的设计空间巨大且复杂，传统计算方法计算成本高，机器学习提供了一种加速发现新材料的途径。

Method: 提出了一个包含生成模型、性质预测器和强化学习模块的框架，使用MOFid编码结构和拓扑信息。

Result: 通过将性质反馈集成到序列生成中，模型能够生成具有特定功能且可合成的MOFs。

Conclusion: 该方法展示了大规模语言模型与强化学习结合在MOFs反向设计中的潜力，为材料发现开辟了新途径。

Abstract: The discovery of Metal-Organic Frameworks (MOFs) with application-specific
properties remains a central challenge in materials chemistry, owing to the
immense size and complexity of their structural design space. Conventional
computational screening techniques such as molecular simulations and density
functional theory (DFT), while accurate, are computationally prohibitive at
scale. Machine learning offers an exciting alternative by leveraging
data-driven approaches to accelerate materials discovery. The complexity of
MOFs, with their extended periodic structures and diverse topologies, creates
both opportunities and challenges for generative modeling approaches. To
address these challenges, we present a reinforcement learning-enhanced,
transformer-based framework for the de novo design of MOFs. Central to our
approach is MOFid, a chemically-informed string representation encoding both
connectivity and topology, enabling scalable generative modeling. Our pipeline
comprises three components: (1) a generative GPT model trained on MOFid
sequences, (2) MOFormer, a transformer-based property predictor, and (3) a
reinforcement learning (RL) module that optimizes generated candidates via
property-guided reward functions. By integrating property feedback into
sequence generation, our method drives the model toward synthesizable,
topologically valid MOFs with desired functional attributes. This work
demonstrates the potential of large language models, when coupled with
reinforcement learning, to accelerate inverse design in reticular chemistry and
unlock new frontiers in computational MOF discovery.

</details>


### [249] [Unlocking the Power of Rehearsal in Continual Learning: A Theoretical Perspective](https://arxiv.org/abs/2506.00205)
*Junze Deng,Qinhang Wu,Peizhong Ju,Sen Lin,Yingbin Liang,Ness Shroff*

Key words: 持续学习,遗忘问题,重放策略,混合重放

TL;DR: 对比并发和顺序重放在持续学习中的效果，提出混合重放方法，理论分析和实验验证其优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索持续学习中顺序重放是否比并发重放更有效，以减轻遗忘问题。

Method: 理论分析过参数化线性模型，提出混合重放（并发+顺序），并用深度神经网络实验验证。

Result: 顺序重放在任务相似度低时效果更好，混合重放表现最优。

Conclusion: 混合重放方法在持续学习中优于标准并发重放，为理论分析提供新视角。

Abstract: Rehearsal-based methods have shown superior performance in addressing
catastrophic forgetting in continual learning (CL) by storing and training on a
subset of past data alongside new data in current task. While such a concurrent
rehearsal strategy is widely used, it remains unclear if this approach is
always optimal. Inspired by human learning, where sequentially revisiting tasks
helps mitigate forgetting, we explore whether sequential rehearsal can offer
greater benefits for CL compared to standard concurrent rehearsal. To address
this question, we conduct a theoretical analysis of rehearsal-based CL in
overparameterized linear models, comparing two strategies: 1) Concurrent
Rehearsal, where past and new data are trained together, and 2) Sequential
Rehearsal, where new data is trained first, followed by revisiting past data
sequentially. By explicitly characterizing forgetting and generalization error,
we show that sequential rehearsal performs better when tasks are less similar.
These insights further motivate a novel Hybrid Rehearsal method, which trains
similar tasks concurrently and revisits dissimilar tasks sequentially. We
characterize its forgetting and generalization performance, and our experiments
with deep neural networks further confirm that the hybrid approach outperforms
standard concurrent rehearsal. This work provides the first comprehensive
theoretical analysis of rehearsal-based CL.

</details>


### [250] [Intercept Cancer: Cancer Pre-Screening with Large Scale Healthcare Foundation Models](https://arxiv.org/abs/2506.00209)
*Liwen Sun,Hao-Ren Yao,Gary Gao,Ophir Frieder,Chenyan Xiong*

Key words: 癌症筛查;CATCH-FM;电子健康记录;预训练模型;高风险患者

TL;DR: CATCH-FM是一种基于医疗记录的新癌症预筛查方法，通过预训练和微调大规模医疗代码序列模型，实现了高效且低风险的癌症早期检测。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的癌症筛查技术昂贵且侵入性强，全球可用性有限，导致许多本可挽救的生命流失。CATCH-FM旨在通过非侵入性的医疗记录分析填补这一空白。

Method: 基于数百万电子健康记录，预训练2.4亿参数的医疗代码序列模型，并在临床癌症风险预测队列中微调。

Result: 在3万患者的回顾性评估中，CATCH-FM显示出高效（60%灵敏度）和低风险（99%特异性及阴性预测值），显著优于传统模型和通用医学模型。

Conclusion: CATCH-FM在多种患者分布中表现稳健，能够捕捉非显性癌症风险因素，并为开放源码。

Abstract: Cancer screening, leading to early detection, saves lives. Unfortunately,
existing screening techniques require expensive and intrusive medical
procedures, not globally available, resulting in too many lost would-be-saved
lives. We present CATCH-FM, CATch Cancer early with Healthcare Foundation
Models, a cancer pre-screening methodology that identifies high-risk patients
for further screening solely based on their historical medical records. With
millions of electronic healthcare records (EHR), we establish the scaling law
of EHR foundation models pretrained on medical code sequences, pretrain
compute-optimal foundation models of up to 2.4 billion parameters, and finetune
them on clinician-curated cancer risk prediction cohorts. In our retrospective
evaluation comprising of thirty thousand patients, CATCH-FM achieved strong
efficacy (60% sensitivity) with low risk (99% specificity and Negative
Predictive Value), outperforming feature-based tree models as well as general
and medical large language models by large margins. Despite significant
demographic, healthcare system, and EHR coding differences, CATCH-FM achieves
state-of-the-art pancreatic cancer risk prediction on the EHRSHOT few-shot
leaderboard, outperforming EHR foundation models pretrained using on-site
patient data. Our analysis demonstrates the robustness of CATCH-FM in various
patient distributions, the benefits of operating in the ICD code space, and its
ability to capture non-trivial cancer risk factors. Our code will be
open-sourced.

</details>


### [251] [Localized LoRA: A Structured Low-Rank Approximation for Efficient Fine-Tuning](https://arxiv.org/abs/2506.00236)
*Babak Barazandeh*

Key words: 参数高效微调, LoRA, 局部更新, 低秩矩阵

TL;DR: 本文提出了一种名为Localized LoRA的参数高效微调方法，通过局部低秩更新提升表达能力和性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有PEFT方法（如LoRA）通常使用全局低秩结构，忽略了参数空间的空间分布模式。

Method: 提出Localized LoRA框架，将权重更新建模为低秩矩阵对权重矩阵结构化块的组合，实现密集局部更新。

Result: 在相同参数预算下，Localized LoRA的近似误差更低，实验显示其表达能力和适应性优于现有方法。

Conclusion: Localized LoRA是一种更高效且性能更好的微调替代方案。

Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, offer compact
and effective alternatives to full model fine-tuning by introducing low-rank
updates to pretrained weights. However, most existing approaches rely on global
low-rank structures, which can overlook spatial patterns spread across the
parameter space. In this work, we propose Localized LoRA, a generalized
framework that models weight updates as a composition of low-rank matrices
applied to structured blocks of the weight matrix. This formulation enables
dense, localized updates throughout the parameter space-without increasing the
total number of trainable parameters. We provide a formal comparison between
global, diagonal-local, and fully localized low-rank approximations, and show
that our method consistently achieves lower approximation error under matched
parameter budgets. Experiments on both synthetic and practical settings
demonstrate that Localized LoRA offers a more expressive and adaptable
alternative to existing methods, enabling efficient fine-tuning with improved
performance.

</details>


### [252] [DeGLIF for Label Noise Robust Node Classification using GNNs](https://arxiv.org/abs/2506.00244)
*Pintu Kumar,Nandyala Hemachandra*

Key words: 图数据, 去噪, 留一影响函数, 图神经网络, 节点级预测

TL;DR: 本文提出了一种名为DeGLIF的去噪技术，利用留一影响函数和小量干净数据对图数据中的噪声标签进行鲁棒性节点级预测。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 带噪声标签的数据集通常比干净标签数据集更便宜，但会影响模型性能。本文旨在解决图数据中的标签噪声问题，提升预测准确性。

Method: DeGLIF使用留一影响函数估计训练节点移除对验证损失的影响，并结合新的理论驱动重标签函数去噪。提出了两种无需噪声模型或噪声水平信息的变体。

Result: 实验表明，DeGLIF在不同数据集上的表现优于其他基线算法，能有效识别噪声节点并提升预测精度。

Conclusion: DeGLIF是一种无需噪声先验信息的有效图数据去噪方法，能显著提高模型性能。

Abstract: Noisy labelled datasets are generally inexpensive compared to clean labelled
datasets, and the same is true for graph data. In this paper, we propose a
denoising technique DeGLIF: Denoising Graph Data using Leave-One-Out Influence
Function. DeGLIF uses a small set of clean data and the leave-one-out influence
function to make label noise robust node-level prediction on graph data.
Leave-one-out influence function approximates the change in the model
parameters if a training point is removed from the training dataset. Recent
advances propose a way to calculate the leave-one-out influence function for
Graph Neural Networks (GNNs). We extend that recent work to estimate the change
in validation loss, if a training node is removed from the training dataset. We
use this estimate and a new theoretically motivated relabelling function to
denoise the training dataset. We propose two DeGLIF variants to identify noisy
nodes. Both these variants do not require any information about the noise model
or the noise level in the dataset; DeGLIF also does not estimate these
quantities. For one of these variants, we prove that the noisy points detected
can indeed increase risk. We carry out detailed computational experiments on
different datasets to show the effectiveness of DeGLIF. It achieves better
accuracy than other baseline algorithms

</details>


### [253] [Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity](https://arxiv.org/abs/2506.00245)
*Dang Nguyen,Ali Payani,Baharan Mirzasoleiman*

Key words: 大型语言模型,幻觉检测,语义熵,不确定性量化,最近邻估计

TL;DR: 本文提出一种基于最近邻熵估计的黑盒不确定性量化方法，以改进长句生成中语义熵（SE）的不足，通过实验验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于现代大型语言模型（LLM）生成长句时，传统语义熵（SE）忽略了聚类内和聚类间相似性，导致幻觉检测效果下降。

Method: 提出一种简单黑盒不确定性量化方法，基于最近邻熵估计，并可扩展到白盒设置。

Result: 实验证明，该方法在Phi3和Llama3模型及三大文本生成任务（问答、摘要、翻译）中优于传统语义熵。

Conclusion: 该方法有效解决了SE的局限性，提升了幻觉检测效果。

Abstract: Hallucination in large language models (LLMs) can be detected by assessing
the uncertainty of model outputs, typically measured using entropy. Semantic
entropy (SE) enhances traditional entropy estimation by quantifying uncertainty
at the semantic cluster level. However, as modern LLMs generate longer
one-sentence responses, SE becomes less effective because it overlooks two
crucial factors: intra-cluster similarity (the spread within a cluster) and
inter-cluster similarity (the distance between clusters). To address these
limitations, we propose a simple black-box uncertainty quantification method
inspired by nearest neighbor estimates of entropy. Our approach can also be
easily extended to white-box settings by incorporating token probabilities.
Additionally, we provide theoretical results showing that our method
generalizes semantic entropy. Extensive empirical results demonstrate its
effectiveness compared to semantic entropy across two recent LLMs (Phi3 and
Llama3) and three common text generation tasks: question answering, text
summarization, and machine translation. Our code is available at
https://github.com/BigML-CS-UCLA/SNNE.

</details>


### [254] [Performance Analysis of Convolutional Neural Network By Applying Unconstrained Binary Quadratic Programming](https://arxiv.org/abs/2506.00247)
*Aasish Kumar Sharma,Sanjeeb Prashad Pandey,Julian M. Kunkel*

Key words: 卷积神经网络（CNN）、量子计算、无约束二元二次规划（UBQP）、随机梯度下降（SGD）、高性能计算（HPC）

TL;DR: 论文提出了一种结合无约束二元二次规划（UBQP）和随机梯度下降（SGD）的混合优化方法，用于加速卷积神经网络（CNN）的训练，并在MNIST数据集上实现了10-15%的准确率提升。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的基于反向传播（BP）的CNN训练方法在大型数据集上需要大量的计算资源，且可能存在次优收敛问题。量子计算通过利用叠加、隧穿和纠缠等特性，有望更高效地搜索复杂的优化空间，从而提供一种替代方案。

Method: 作者提出了一种混合优化方法，结合了无约束二元二次规划（UBQP）和随机梯度下降（SGD），以加速CNN的训练过程。

Result: 在MNIST数据集上的实验表明，该方法比标准的BP-CNN基线提高了10-15%的准确率，同时保持了相近的执行时间。

Conclusion: 这项研究展示了混合量子-经典技术在高性能计算（HPC）环境中的潜力，但为了充分发挥其优势，需要将算法结构与底层量子机制紧密结合。

Abstract: Convolutional Neural Networks (CNNs) are pivotal in computer vision and Big
Data analytics but demand significant computational resources when trained on
large-scale datasets. Conventional training via back-propagation (BP) with
losses like Mean Squared Error or Cross-Entropy often requires extensive
iterations and may converge sub-optimally. Quantum computing offers a promising
alternative by leveraging superposition, tunneling, and entanglement to search
complex optimization landscapes more efficiently. In this work, we propose a
hybrid optimization method that combines an Unconstrained Binary Quadratic
Programming (UBQP) formulation with Stochastic Gradient Descent (SGD) to
accelerate CNN training. Evaluated on the MNIST dataset, our approach achieves
a 10--15\% accuracy improvement over a standard BP-CNN baseline while
maintaining similar execution times. These results illustrate the potential of
hybrid quantum-classical techniques in High-Performance Computing (HPC)
environments for Big Data and Deep Learning. Fully realizing these benefits,
however, requires a careful alignment of algorithmic structures with underlying
quantum mechanisms.

</details>


### [255] [PerFormer: A Permutation Based Vision Transformer for Remaining Useful Life Prediction](https://arxiv.org/abs/2506.00259)
*Zhengyang Fan,Wanru Li,Kuo-chu Chang,Ting Yuan*

Key words: RUL prediction, Vision Transformer, PerFormer, multivariate time series, PHM

TL;DR: 论文提出了一种名为PerFormer的方法，用于提升基于Vision Transformer的剩余使用寿命预测精度，通过排列多元时间序列数据以模拟图像特性，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Vision Transformer在图像任务中表现优异，但其直接应用于多元传感器数据的RUL预测面临挑战，因时间序列数据的空间信息模糊。

Method: 提出PerFormer方法，通过排列多元时间序列数据模拟图像空间特性，并设计新型排列损失函数以生成排列矩阵。

Result: 在NASA的C-MAPSS数据集上，PerFormer在RUL预测上优于CNN、RNN及其他Transformer模型。

Conclusion: PerFormer证明了其在PHM应用中的有效性和潜力。

Abstract: Accurately estimating the remaining useful life (RUL) for degradation systems
is crucial in modern prognostic and health management (PHM). Convolutional
Neural Networks (CNNs), initially developed for tasks like image and video
recognition, have proven highly effectively in RUL prediction, demonstrating
remarkable performance. However, with the emergence of the Vision Transformer
(ViT), a Transformer model tailored for computer vision tasks such as image
classification, and its demonstrated superiority over CNNs, there is a natural
inclination to explore its potential in enhancing RUL prediction accuracy.
Nonetheless, applying ViT directly to multivariate sensor data for RUL
prediction poses challenges, primarily due to the ambiguous nature of spatial
information in time series data. To address this issue, we introduce the
PerFormer, a permutation-based vision transformer approach designed to permute
multivariate time series data, mimicking spatial characteristics akin to image
data, thereby making it suitable for ViT. To generate the desired permutation
matrix, we introduce a novel permutation loss function aimed at guiding the
convergence of any matrix towards a permutation matrix. Our experiments on
NASA's C-MAPSS dataset demonstrate the PerFormer's superior performance in RUL
prediction compared to state-of-the-art methods employing CNNs, Recurrent
Neural Networks (RNNs), and various Transformer models. This underscores its
effectiveness and potential in PHM applications.

</details>


### [256] [Entropic Risk Optimization in Discounted MDPs: Sample Complexity Bounds with a Generative Model](https://arxiv.org/abs/2506.00286)
*Oliver Mortensen,Mohammad Sadegh Talebi*

Key words: 马尔可夫决策过程,风险敏感学习,样本复杂度,递归熵风险,PAC界限

TL;DR: 研究了在具有递归熵风险偏好的折扣马尔可夫决策过程中，学习最优状态-动作值函数和最优策略的样本复杂度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨风险敏感学习在MDP中的样本复杂性，特别是递归熵风险偏好对学习效率的影响。

Method: 提出了基于模型的风险敏感Q值迭代（MB-RS-QVI）方法，并分析了其PAC界限。

Result: 展示了学习界限对有效视界和风险敏感度的指数依赖性，并提供了紧致性证明。

Conclusion: 证明了样本复杂度的指数依赖性是不可避免的，且学习界限在多个参数上是紧致的。

Abstract: In this paper we analyze the sample complexities of learning the optimal
state-action value function $Q^*$ and an optimal policy $\pi^*$ in a discounted
Markov decision process (MDP) where the agent has recursive entropic
risk-preferences with risk-parameter $\beta\neq 0$ and where a generative model
of the MDP is available. We provide and analyze a simple model based approach
which we call model-based risk-sensitive $Q$-value-iteration (MB-RS-QVI) which
leads to $(\epsilon,\delta)$-PAC-bounds on $\|Q^*-Q^k\|$, and
$\|V^*-V^{\pi_k}\|$ where $Q_k$ is the output of MB-RS-QVI after k iterations
and $\pi_k$ is the greedy policy with respect to $Q_k$. Both PAC-bounds have
exponential dependence on the effective horizon $\frac{1}{1-\gamma}$ and the
strength of this dependence grows with the learners risk-sensitivity $|\beta|$.
We also provide two lower bounds which shows that exponential dependence on
$|\beta|\frac{1}{1-\gamma}$ is unavoidable in both cases. The lower bounds
reveal that the PAC-bounds are both tight in $\varepsilon$ and $\delta$ and
that the PAC-bound on $Q$-learning is tight in the number of actions $A$, and
that the PAC-bound on policy-learning is nearly tight in $A$.

</details>


### [257] [Improving Protein Sequence Design through Designability Preference Optimization](https://arxiv.org/abs/2506.00297)
*Fanglei Xue,Andrew Kubaney,Zhichun Guo,Joseph K. Min,Ge Liu,Yi Yang,David Baker*

Key words: 蛋白序列设计, 设计性, DPO, ResiDPO, AlphaFold

TL;DR: 蛋白序列设计方法在序列生成方面表现优异，但传统训练目标未能保证设计后的序列能折叠成所需结构。新方法通过优化设计性目标，显著提高了设计成功率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决传统蛋白序列设计方法中设计性（设计序列能否折叠成目标结构）不足的问题。

Method: 通过整合Direct Preference Optimization (DPO) 和使用AlphaFold pLDDT分数作为偏好信号，以及引入Residue-level Designability Preference Optimization (ResiDPO)，细化残基级优化。

Result: EnhancedMPNN在酶设计基准测试中将设计成功率从6.56%提升至17.57%。

Conclusion: 新方法显著提高了蛋白序列的设计性和实用性。

Abstract: Protein sequence design methods have demonstrated strong performance in
sequence generation for de novo protein design. However, as the training
objective was sequence recovery, it does not guarantee designability--the
likelihood that a designed sequence folds into the desired structure. To bridge
this gap, we redefine the training objective by steering sequence generation
toward high designability. To do this, we integrate Direct Preference
Optimization (DPO), using AlphaFold pLDDT scores as the preference signal,
which significantly improves the in silico design success rate. To further
refine sequence generation at a finer, residue-level granularity, we introduce
Residue-level Designability Preference Optimization (ResiDPO), which applies
residue-level structural rewards and decouples optimization across residues.
This enables direct improvement in designability while preserving regions that
already perform well. Using a curated dataset with residue-level annotations,
we fine-tune LigandMPNN with ResiDPO to obtain EnhancedMPNN, which achieves a
nearly 3-fold increase in in silico design success rate (from 6.56% to 17.57%)
on a challenging enzyme design benchmark.

</details>


### [258] [Inference-Time Alignment of Diffusion Models with Evolutionary Algorithms](https://arxiv.org/abs/2506.00299)
*Purvish Jajal,Nick John Eliopoulos,Benjamin Shiue-Hal Chou,George K. Thiruvathukal,James C. Davis,Yung-Hsiang Lu*

Key words: 扩散模型,进化算法,推理时对齐,黑盒优化,生成模型

TL;DR: 该论文提出了一种基于进化算法的扩散模型推理时对齐框架，可以在不依赖梯度或内部模型访问的情况下，高效优化对齐目标。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 扩散模型在生成高质量样本时常常无法满足下游目标（如安全性或领域特定有效性），现有方法需要梯度或高计算成本。

Method: 将扩散模型视为黑盒，利用进化算法在其潜在空间中搜索，以最大化对齐目标，支持可微和不可微目标。

Result: 在DrawBench和Open Image Preferences基准测试中，该方法在内存（节省55%-76%）和速度（快72%-80%）上优于梯度方法和无梯度方法，且对齐分数更高。

Conclusion: 该框架为推理时对齐提供了一种高效、灵活的解决方案，适用于多种扩散模型和复杂的对齐目标。

Abstract: Diffusion models are state-of-the-art generative models in various domains,
yet their samples often fail to satisfy downstream objectives such as safety
constraints or domain-specific validity. Existing techniques for alignment
require gradients, internal model access, or large computational budgets. We
introduce an inference-time alignment framework based on evolutionary
algorithms. We treat diffusion models as black-boxes and search their latent
space to maximize alignment objectives. Our method enables efficient
inference-time alignment for both differentiable and non-differentiable
alignment objectives across a range of diffusion models. On the DrawBench and
Open Image Preferences benchmark, our EA methods outperform state-of-the-art
gradient-based and gradient-free inference-time methods. In terms of memory
consumption, we require 55% to 76% lower GPU memory than gradient-based
methods. In terms of running-time, we are 72% to 80% faster than gradient-based
methods. We achieve higher alignment scores over 50 optimization steps on Open
Image Preferences than gradient-based and gradient-free methods.

</details>


### [259] [Beyond Atomic Geometry Representations in Materials Science: A Human-in-the-Loop Multimodal Framework](https://arxiv.org/abs/2506.00302)
*Can Polat,Hasan Kurban,Erchin Serpedin,Mustafa Kurban*

Key words: 材料科学, 多模态学习, 数据集, 机器学习, 标注质量

TL;DR: MultiCrystalSpectrumSet (MCS-Set) 是一个多模态材料数据集框架，结合了原子结构、2D投影和结构化文本注释，支持多模态预测和约束晶体生成。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统材料科学数据集仅包含原子几何结构，限制了多模态学习和数据中心的全面分析，阻碍了先进机器学习技术的应用。

Method: MCS-Set 通过整合原子结构与2D投影及文本注释（如晶格参数和配位度量），采用人机协作流程进行高质量标注。

Result: 评估显示多模态模型存在显著性能差异，并强调标注质量对泛化的重要性。

Conclusion: MCS-Set 为多模态模型基准测试、标注实践改进及通用材料科学数据集的发展提供了基础。

Abstract: Most materials science datasets are limited to atomic geometries (e.g., XYZ
files), restricting their utility for multimodal learning and comprehensive
data-centric analysis. These constraints have historically impeded the adoption
of advanced machine learning techniques in the field. This work introduces
MultiCrystalSpectrumSet (MCS-Set), a curated framework that expands materials
datasets by integrating atomic structures with 2D projections and structured
textual annotations, including lattice parameters and coordination metrics.
MCS-Set enables two key tasks: (1) multimodal property and summary prediction,
and (2) constrained crystal generation with partial cluster supervision.
Leveraging a human-in-the-loop pipeline, MCS-Set combines domain expertise with
standardized descriptors for high-quality annotation. Evaluations using
state-of-the-art language and vision-language models reveal substantial
modality-specific performance gaps and highlight the importance of annotation
quality for generalization. MCS-Set offers a foundation for benchmarking
multimodal models, advancing annotation practices, and promoting accessible,
versatile materials science datasets. The dataset and implementations are
available at https://github.com/KurbanIntelligenceLab/MultiCrystalSpectrumSet.

</details>


### [260] [Active Learning via Regression Beyond Realizability](https://arxiv.org/abs/2506.00316)
*Atul Ganju,Shashaank Aiyer,Ved Sriraman,Karthik Sridharan*

Key words: 主动学习, 多类分类, 代理风险, 可实现性

TL;DR: 提出了一种基于代理风险最小化的多类分类主动学习框架，突破标准的可实现性假设。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有基于代理的主动学习算法依赖于可实现性假设，限制了在实际误设定中的应用。

Method: 使用凸模型类并在每个epoch中对查询数据拟合完整模型，返回通过聚合模型得到的不当分类器。

Result: 在显著弱于可实现性的条件下，仍能获得与现有工作相当的标签和样本复杂度。

Conclusion: 新算法在非可实现性设置中表现优于现有方法。

Abstract: We present a new active learning framework for multiclass classification
based on surrogate risk minimization that operates beyond the standard
realizability assumption. Existing surrogate-based active learning algorithms
crucially rely on realizability$\unicode{x2014}$the assumption that the optimal
surrogate predictor lies within the model class$\unicode{x2014}$limiting their
applicability in practical, misspecified settings. In this work we show that
under conditions significantly weaker than realizability, as long as the class
of models considered is convex, one can still obtain a label and sample
complexity comparable to prior work. Despite achieving similar rates, the
algorithmic approaches from prior works can be shown to fail in non-realizable
settings where our assumption is satisfied. Our epoch-based active learning
algorithm departs from prior methods by fitting a model from the full class to
the queried data in each epoch and returning an improper classifier obtained by
aggregating these models.

</details>


### [261] [Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation](https://arxiv.org/abs/2506.00329)
*Muhammad Adnan,Nithesh Kurella,Akhil Arunkumar,Prashant J. Nair*

Key words: Diffusion Transformers, 视频生成, 自适应层重用, 计算效率

TL;DR: Foresight 是一种自适应层重用技术，通过减少去噪步骤中的计算冗余，显著提升了 Diffusion Transformers (DiTs) 在视频生成中的效率，同时保持性能不变。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: DiTs 在视频生成中因模型大小大和空间-时间注意力的二次成本导致计算开销高，静态缓存无法适应生成动态，速度和质量平衡不佳。

Method: Foresight 动态识别并重用 DiT 块输出，根据生成参数（如分辨率和去噪计划）自适应优化效率。

Result: 在 OpenSora、Latte 和 CogVideoX 上，Foresight 实现了最高 1.63 倍的端到端加速，且视频质量不变。

Conclusion: Foresight 通过自适应层重用技术高效解决了 DiTs 的计算冗余问题，显著提升了视频生成效率。

Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art results in
text-to-image, text-to-video generation, and editing. However, their large
model size and the quadratic cost of spatial-temporal attention over multiple
denoising steps make video generation computationally expensive. Static caching
mitigates this by reusing features across fixed steps but fails to adapt to
generation dynamics, leading to suboptimal trade-offs between speed and
quality.
  We propose Foresight, an adaptive layer-reuse technique that reduces
computational redundancy across denoising steps while preserving baseline
performance. Foresight dynamically identifies and reuses DiT block outputs for
all layers across steps, adapting to generation parameters such as resolution
and denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and
CogVideoX, Foresight achieves up to 1.63x end-to-end speedup, while maintaining
video quality. The source code of Foresight is available at
\texttt{https://github.com/STAR-Laboratory/foresight}.

</details>


### [262] [Channel-Imposed Fusion: A Simple yet Effective Method for Medical Time Series Classification](https://arxiv.org/abs/2506.00337)
*Ming Hu,Jianfu Yin,Mingyu Dou,Yuqi Wang,Ruochen Dang,Siyi Liang,Cong Hu,Yao Wang,Bingliang Hu,Quan Wang*

Key words: 医学时间序列分类, CIF, TCN, 透明度, EEG, ECG

TL;DR: 该论文提出了一种新型的医学时间序列分类方法CIF，结合TCN，提高分类性能和透明度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Transformer模型在医学时间序列分类中表现优异，但其复杂架构和缺乏透明性限制了其在临床高风险环境中的应用。

Method: 提出Channel Imposed Fusion (CIF)方法，通过跨通道信息融合提升信噪比，并结合Temporal Convolutional Network (TCN)构建分类框架。

Result: 在多个公开EEG和ECG数据集上，CIF+TCN不仅优于现有SOTA方法，还显著提升了分类过程的透明度。

Conclusion: CIF+TCN为医学时间序列分类提供了一种高效且透明的解决方案。

Abstract: The automatic classification of medical time series signals, such as
electroencephalogram (EEG) and electrocardiogram (ECG), plays a pivotal role in
clinical decision support and early detection of diseases. Although Transformer
based models have achieved notable performance by implicitly modeling temporal
dependencies through self-attention mechanisms, their inherently complex
architectures and opaque reasoning processes undermine their trustworthiness in
high stakes clinical settings. In response to these limitations, this study
shifts focus toward a modeling paradigm that emphasizes structural
transparency, aligning more closely with the intrinsic characteristics of
medical data. We propose a novel method, Channel Imposed Fusion (CIF), which
enhances the signal-to-noise ratio through cross-channel information fusion,
effectively reduces redundancy, and improves classification performance.
Furthermore, we integrate CIF with the Temporal Convolutional Network (TCN),
known for its structural simplicity and controllable receptive field, to
construct an efficient and explicit classification framework. Experimental
results on multiple publicly available EEG and ECG datasets demonstrate that
the proposed method not only outperforms existing state-of-the-art (SOTA)
approaches in terms of various classification metrics, but also significantly
enhances the transparency of the classification process, offering a novel
perspective for medical time series classification.

</details>


### [263] [Exploring the Performance of Perforated Backpropagation through Further Experiments](https://arxiv.org/abs/2506.00356)
*Rorry Brenner,Evan Davis,Rushi Chaudhari,Rowan Morse,Jingyao Chen,Xirui Liu,Zhaoyi You,Laurent Itti*

Key words: Perforated Backpropagation, 模型压缩, 神经网络优化, hackathon, 树突计算

TL;DR: Perforated Backpropagation是一种基于生物神经元树突计算重要性的神经网络优化技术，通过实验验证其能显著提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索Perforated Backpropagation算法在实际项目中的应用效果，验证其是否能提升模型压缩和准确性。

Method: 在Carnegie Mellon Swartz Center举办的hackathon中，学生和当地ML从业者将Perforated Backpropagation应用于各自的数据集和模型。

Result: 实验结果显示，该技术可实现高达90%的模型压缩而不影响准确性，或提升原始模型准确性达16%。

Conclusion: Perforated Backpropagation是一种有效的神经网络优化技术，具有显著的实际应用潜力。

Abstract: Perforated Backpropagation is a neural network optimization technique based
on modern understanding of the computational importance of dendrites within
biological neurons. This paper explores further experiments from the original
publication, generated from a hackathon held at the Carnegie Mellon Swartz
Center in February 2025. Students and local Pittsburgh ML practitioners were
brought together to experiment with the Perforated Backpropagation algorithm on
the datasets and models which they were using for their projects. Results
showed that the system could enhance their projects, with up to 90% model
compression without negative impact on accuracy, or up to 16% increased
accuracy of their original models.

</details>


### [264] [FSNet: Feasibility-Seeking Neural Network for Constrained Optimization with Guarantees](https://arxiv.org/abs/2506.00362)
*Hoang T. Nguyen,Priya L. Donti*

Key words: 约束优化、神经网络、可行性、实时求解

TL;DR: FSNet通过在神经网络中集成可行性搜索步骤，确保约束优化问题的解严格满足约束条件，同时提供高效的近似解。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统求解器在实时应用中计算成本过高，而现有的机器学习方法无法严格满足约束条件。

Method: 提出FSNet，将可行性搜索步骤直接整合到求解过程中，通过解无约束优化问题最小化约束违反。

Result: 实验表明，FSNet能在多种优化问题中提供可行解，解的质量与传统求解器相当，速度更快。

Conclusion: FSNet提供了一种高效、严格满足约束的新方法，适用于实时优化问题。

Abstract: Efficiently solving constrained optimization problems is crucial for numerous
real-world applications, yet traditional solvers are often computationally
prohibitive for real-time use. Machine learning-based approaches have emerged
as a promising alternative to provide approximate solutions at faster speeds,
but they struggle to strictly enforce constraints, leading to infeasible
solutions in practice. To address this, we propose the
Feasibility-Seeking-Integrated Neural Network (FSNet), which integrates a
feasibility-seeking step directly into its solution procedure to ensure
constraint satisfaction. This feasibility-seeking step solves an unconstrained
optimization problem that minimizes constraint violations in a differentiable
manner, enabling end-to-end training and providing guarantees on feasibility
and convergence. Our experiments across a range of different optimization
problems, including both smooth/nonsmooth and convex/nonconvex problems,
demonstrate that FSNet can provide feasible solutions with solution quality
comparable to (or in some cases better than) traditional solvers, at
significantly faster speeds.

</details>


### [265] [Spectral Insights into Data-Oblivious Critical Layers in Large Language Models](https://arxiv.org/abs/2506.00382)
*Xuyuan Liu,Lei Hsiung,Yaoqing Yang,Yujun Yan*

Key words: 大型语言模型, 关键层, CKA, 领域适应, 后门防御

TL;DR: 本文提出了一种无数据的分析方法，通过CKA识别预训练语言模型中的关键层，发现这些层的表征动态与微调效果相关，并应用于领域适应和后门防御。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究大型语言模型（LLMs）中特征表征随层的演变，以提高其可解释性和鲁棒性。

Method: 使用Centered Kernel Alignment（CKA）分析表征动态，识别预微调模型中的固有关键层。

Result: 实验显示，关键层的表征动态与微调效果相关，且在领域适应和后门防御中表现优异。

Conclusion: 该方法为LLMs的优化提供了新思路，尤其在微调效率和安全性方面。

Abstract: Understanding how feature representations evolve across layers in large
language models (LLMs) is key to improving their interpretability and
robustness. While recent studies have identified critical layers linked to
specific functions or behaviors, these efforts typically rely on data-dependent
analyses of fine-tuned models, limiting their use to post-hoc settings. In
contrast, we introduce a data-oblivious approach to identify intrinsic critical
layers in pre-fine-tuned LLMs by analyzing representation dynamics via Centered
Kernel Alignment(CKA). We show that layers with significant shifts in
representation space are also those most affected during fine-tuning--a pattern
that holds consistently across tasks for a given model. Our spectral analysis
further reveals that these shifts are driven by changes in the top principal
components, which encode semantic transitions from rationales to conclusions.
We further apply these findings to two practical scenarios: efficient domain
adaptation, where fine-tuning critical layers leads to greater loss reduction
compared to non-critical layers; and backdoor defense, where freezing them
reduces attack success rates by up to 40%.

</details>


### [266] [Deep-Learning-Driven Prefetching for Far Memory](https://arxiv.org/abs/2506.00384)
*Yutong Huang,Zhiyuan Guo,Yiying Zhang*

Key words: 远内存,深度学习,数据预取,运行时优化

TL;DR: FarSight是一个基于Linux的远内存系统，利用深度学习实现高效数据预取，显著提升性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现代软件系统在高性能需求下，特别是在远内存架构中，运行时性能优化面临挑战。虽然机器学习在离线系统中表现优异，但在高频运行时问题中应用受限。

Method: FarSight通过将应用语义与运行时内存布局分离，使用离线训练的深度学习模型预测访问模式，并结合异步推理、前瞻预测和缓存驻留模型。

Result: 在四个数据密集型任务中，FarSight比现有远内存系统性能提升高达3.6倍。

Conclusion: 研究表明，现代机器学习技术可成功应用于复杂、高性能关键的软件运行时问题。

Abstract: Modern software systems face increasing runtime performance demands,
particularly in emerging architectures like far memory, where local-memory
misses incur significant latency. While machine learning (ML) has proven
effective in offline systems optimization, its application to high-frequency,
runtime-level problems remains limited due to strict performance,
generalization, and integration constraints. We present FarSight, a Linux-based
far-memory system that leverages deep learning (DL) to efficiently perform
accurate data prefetching. FarSight separates application semantics from
runtime memory layout, allowing offline-trained DL models to predict access
patterns using a compact vocabulary of ordinal possibilities, resolved at
runtime through lightweight mapping structures. By combining asynchronous
inference, lookahead prediction, and a cache-resident DL model, FarSight
achieves high prediction accuracy with low runtime overhead. Our evaluation of
FarSight on four data-intensive workloads shows that it outperforms the
state-of-the-art far-memory system by up to 3.6 times. Overall, this work
demonstrates the feasibility and advantages of applying modern ML techniques to
complex, performance-critical software runtime problems.

</details>


### [267] [CLARIFY: Contrastive Preference Reinforcement Learning for Untangling Ambiguous Queries](https://arxiv.org/abs/2506.00388)
*Ni Mu,Hao Hu,Xiao Hu,Yiqin Yang,Bo Xu,Qing-Shan Jia*

Key words: 偏好强化学习, 对比学习, 轨迹嵌入, 模糊反馈, 离线学习

TL;DR: 论文提出了一种名为CLARIFY的离线偏好强化学习方法，通过对比学习解决人类偏好反馈中的模糊性问题，显著提升了偏好标注的效率和实用性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 偏好强化学习（PbRL）依赖人类偏好比较推断奖励函数，但在面对相似片段时，人类难以明确标注偏好，导致标签效率低下，限制了实际应用。本文旨在解决这一问题。

Method: 提出了CLARIFY方法，通过学习融入偏好信息的轨迹嵌入空间，使明显不同的片段在嵌入空间中相距更远，从而选择更明确的查询。

Result: 在非理想教师和真实人类反馈场景下，CLARIFY均优于基线方法，能够选择更易区分的查询并学习有意义的轨迹嵌入。

Conclusion: CLARIFY有效提升了偏好强化学习的标签效率和实用性，为解决人类偏好模糊性问题提供了新思路。

Abstract: Preference-based reinforcement learning (PbRL) bypasses explicit reward
engineering by inferring reward functions from human preference comparisons,
enabling better alignment with human intentions. However, humans often struggle
to label a clear preference between similar segments, reducing label efficiency
and limiting PbRL's real-world applicability. To address this, we propose an
offline PbRL method: Contrastive LeArning for ResolvIng Ambiguous Feedback
(CLARIFY), which learns a trajectory embedding space that incorporates
preference information, ensuring clearly distinguished segments are spaced
apart, thus facilitating the selection of more unambiguous queries. Extensive
experiments demonstrate that CLARIFY outperforms baselines in both non-ideal
teachers and real human feedback settings. Our approach not only selects more
distinguished queries but also learns meaningful trajectory embeddings.

</details>


### [268] [Bias as a Virtue: Rethinking Generalization under Distribution Shifts](https://arxiv.org/abs/2506.00407)
*Ruixuan Chen,Wentao Li,Jiahui Xiao,Yuchen Li,Yimin Tang,Xiaonan Wang*

Key words: 机器学习, 分布外泛化, 偏差, 自适应分布桥, 统计多样性

TL;DR: 论文提出了一种名为Adaptive Distribution Bridge (ADB)的框架，通过引入控制性的统计多样性来提升机器学习模型在分布外数据上的泛化能力。研究发现，更高的分布内偏差（ID bias）反而可以降低分布外错误（OOD error），与传统方法不同。实验显示ADB将OOD错误降低了26.8%，且其训练策略的性能百分位数常超过74.4%。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的机器学习模型在分布外数据上性能下降明显，且常规的验证方法（如最小化验证误差）可能无法有效提升泛化能力。因此，作者希望通过重新考虑偏差在泛化中的作用，提出一种新的方法。

Method: 提出Adaptive Distribution Bridge (ADB)框架，通过在训练中引入控制性的统计多样性，使模型能够形成有效的偏差配置，从而提升分布外泛化能力。

Result: 在多个数据集上的实验表明，ADB显著提升了OOD泛化能力，平均错误率降低了26.8%，且其训练策略的性能百分位数常超过74.4%。

Conclusion: ADB不仅提供了一种提升机器学习泛化能力的实用方法，还提出了一个理论框架，重新思考偏差在鲁棒机器学习中的作用。

Abstract: Machine learning models often degrade when deployed on data distributions
different from their training data. Challenging conventional validation
paradigms, we demonstrate that higher in-distribution (ID) bias can lead to
better out-of-distribution (OOD) generalization. Our Adaptive Distribution
Bridge (ADB) framework implements this insight by introducing controlled
statistical diversity during training, enabling models to develop bias profiles
that effectively generalize across distributions. Empirically, we observe a
robust negative correlation where higher ID bias corresponds to lower OOD
error--a finding that contradicts standard practices focused on minimizing
validation error. Evaluation on multiple datasets shows our approach
significantly improves OOD generalization. ADB achieves robust mean error
reductions of up to 26.8% compared to traditional cross-validation, and
consistently identifies high-performing training strategies, evidenced by
percentile ranks often exceeding 74.4%. Our work provides both a practical
method for improving generalization and a theoretical framework for
reconsidering the role of bias in robust machine learning.

</details>


### [269] [JojoSCL: Shrinkage Contrastive Learning for single-cell RNA sequence Clustering](https://arxiv.org/abs/2506.00410)
*Ziwen Wang*

Key words: scRNA-seq, 自监督学习, 对比学习, 分层贝叶斯估计, 聚类

TL;DR: JojoSCL是一种新的自监督对比学习框架，用于解决scRNA-seq数据聚类中的高维稀疏问题，通过引入基于分层贝叶斯估计的收缩估计器和Stein无偏风险估计优化，显著提升了聚类性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: scRNA-seq数据的聚类对于细胞类型识别和模式发现至关重要，但其高维稀疏特性给现有聚类方法带来了挑战。

Method: JojoSCL通过自监督对比学习框架，结合分层贝叶斯估计的收缩估计器和Stein无偏风险估计优化，减少簇内离散度，提升聚类效果。

Result: 在十个scRNA-seq数据集上的实验表明，JojoSCL优于现有聚类方法，并通过鲁棒性分析和消融研究验证了其有效性。

Conclusion: JojoSCL为scRNA-seq数据聚类提供了一种高效且实用的解决方案。

Abstract: Single-cell RNA sequencing (scRNA-seq) has revolutionized our understanding
of cellular processes by enabling gene expression analysis at the individual
cell level. Clustering allows for the identification of cell types and the
further discovery of intrinsic patterns in single-cell data. However, the high
dimensionality and sparsity of scRNA-seq data continue to challenge existing
clustering models. In this paper, we introduce JojoSCL, a novel self-supervised
contrastive learning framework for scRNA-seq clustering. By incorporating a
shrinkage estimator based on hierarchical Bayesian estimation, which adjusts
gene expression estimates towards more reliable cluster centroids to reduce
intra-cluster dispersion, and optimized using Stein's Unbiased Risk Estimate
(SURE), JojoSCL refines both instance-level and cluster-level contrastive
learning. Experiments on ten scRNA-seq datasets substantiate that JojoSCL
consistently outperforms prevalent clustering methods, with further validation
of its practicality through robustness analysis and ablation studies. JojoSCL's
code is available at: https://github.com/ziwenwang28/JojoSCL.

</details>


### [270] [Blockchain-Enabled Privacy-Preserving Second-Order Federated Edge Learning in Personalized Healthcare](https://arxiv.org/abs/2506.00416)
*Anum Nawaz,Muhammad Irfan,Xianjia Yu,Zhuo Zou,Tomi Westerlund*

Key words: 联邦学习, 二阶优化, 区块链, 个性化医疗, 非独立同分布数据

TL;DR: 该论文提出了一个名为BFEL的可验证和可审计的二阶联邦学习框架，用于医疗健康系统中的个性化模型训练，解决了非独立同分布数据的挑战。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统联邦学习方法在面对非独立同分布的异构数据时存在模型训练个性化的挑战。二阶联邦学习方法能够保持数据稳定性并改善个性化训练。

Method: 基于优化的FedCurv方法，结合Fisher信息矩阵以保留客户端特定知识，减少模型漂移，并通过区块链技术确保信任和可审计性。

Result: 实验结果表明，该框架在Mnist、Cifar-10和PathMnist数据集上具有高效性和可扩展性。

Conclusion: BFEL框架通过区块链技术和二阶优化方法，显著提升了联邦学习在医疗系统中的隐私保护和个性化训练能力。

Abstract: Federated learning (FL) has attracted increasing attention to mitigate
security and privacy challenges in traditional cloud-centric machine learning
models specifically in healthcare ecosystems. FL methodologies enable the
training of global models through localized policies, allowing independent
operations at the edge clients' level. Conventional first-order FL approaches
face several challenges in personalized model training due to heterogeneous
non-independent and identically distributed (non-iid) data of each edge client.
Recently, second-order FL approaches maintain the stability and consistency of
non-iid datasets while improving personalized model training. This study
proposes and develops a verifiable and auditable optimized second-order FL
framework BFEL (blockchain-enhanced federated edge learning) based on optimized
FedCurv for personalized healthcare systems. FedCurv incorporates information
about the importance of each parameter to each client's task (through Fisher
Information Matrix) which helps to preserve client-specific knowledge and
reduce model drift during aggregation. Moreover, it minimizes communication
rounds required to achieve a target precision convergence for each edge client
while effectively managing personalized training on non-iid and heterogeneous
data. The incorporation of Ethereum-based model aggregation ensures trust,
verifiability, and auditability while public key encryption enhances privacy
and security. Experimental results of federated CNNs and MLPs utilizing Mnist,
Cifar-10, and PathMnist demonstrate the high efficiency and scalability of the
proposed framework.

</details>


### [271] [A New Spatiotemporal Correlation Anomaly Detection Method that Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor Networks](https://arxiv.org/abs/2506.00420)
*Miao Ye,Suxiao Wang,Jiaguang Han,Yong Wang,Xiaoli Wang,Jingxuan Wei,Peng Wen,Jing Cui*

Key words: WSN、异常检测、时空相关性、对比学习、样本不平衡

TL;DR: 提出了一种考虑模型架构和两阶段训练策略的时空相关性检测模型（MTAD-RD），用于解决WSN异常检测中的特征提取不足、样本标签缺失、样本分布不平衡等问题，并在实验中优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有WSN异常检测方法在提取时空相关性特征、样本标签缺失、异常样本稀少及样本不平衡等方面存在挑战，亟需改进。

Method: 设计了包含增强RetNet、多粒度特征融合模块和图注意力网络模块的MTAD-RD主干网络，结合两阶段训练策略（对比学习代理任务和缓存样本采样器）进行训练。

Result: 在真实公共数据集上，MTAD-RD的F1分数达到90.97%，优于现有监督方法。

Conclusion: MTAD-RD能有效整合时空特征，解决样本标签和分布问题，显著提升WSN异常检测性能。

Abstract: Detecting anomalies in the data collected by WSNs can provide crucial
evidence for assessing the reliability and stability of WSNs. Existing methods
for WSN anomaly detection often face challenges such as the limited extraction
of spatiotemporal correlation features, the absence of sample labels, few
anomaly samples, and an imbalanced sample distribution. To address these
issues, a spatiotemporal correlation detection model (MTAD-RD) considering both
model architecture and a two-stage training strategy perspective is proposed.
In terms of model structure design, the proposed MTAD-RD backbone network
includes a retentive network (RetNet) enhanced by a cross-retention (CR)
module, a multigranular feature fusion module, and a graph attention network
module to extract internode correlation information. This proposed model can
integrate the intermodal correlation features and spatial features of WSN
neighbor nodes while extracting global information from time series data.
Moreover, its serialized inference characteristic can remarkably reduce
inference overhead. For model training, a two-stage training approach was
designed. First, a contrastive learning proxy task was designed for time series
data with graph structure information in WSNs, enabling the backbone network to
learn transferable features from unlabeled data using unsupervised contrastive
learning methods, thereby addressing the issue of missing sample labels in the
dataset. Then, a caching-based sample sampler was designed to divide samples
into few-shot and contrastive learning data. A specific joint loss function was
developed to jointly train the dual-graph discriminator network to address the
problem of sample imbalance effectively. In experiments carried out on real
public datasets, the designed MTAD-RD anomaly detection method achieved an F1
score of 90.97%, outperforming existing supervised WSN anomaly detection
methods.

</details>


### [272] [COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning](https://arxiv.org/abs/2506.00424)
*Chamika Sudusinghe,Gerasimos Gerogiannis Damitha Lenadora,Charles Block,Josep Torrellas,Charith Mendis*

Key words: 稀疏张量程序,硬件加速器,成本模型,COGNATE,机器学习

TL;DR: COGNATE框架通过利用通用硬件数据样本训练成本模型，并通过少量样本微调，显著优化稀疏张量程序性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 稀疏张量程序在深度学习和图分析中至关重要，但早期硬件加速器的优化面临性能敏感和模拟器成本高的挑战。

Method: 提出COGNATE框架，利用通用硬件数据样本训练成本模型，并通过少量样本在新型硬件上微调。

Result: 实验表明，COGNATE优于现有技术，SpMM和SDDMM分别实现1.47倍和1.39倍的平均加速。

Conclusion: COGNATE通过高效数据利用和模型微调，显著提升了稀疏张量程序在早期硬件加速器上的性能。

Abstract: Sparse tensor programs are essential in deep learning and graph analytics,
driving the need for optimized processing. To meet this demand, specialized
hardware accelerators are being developed. Optimizing these programs for
accelerators is challenging for two reasons: program performance is highly
sensitive to variations in sparse inputs, and early-stage accelerators rely on
expensive simulators. Therefore, ML-based cost models used for optimizing such
programs on general-purpose hardware are often ineffective for early-stage
accelerators, as they require large datasets for proper training. To this end,
we introduce COGNATE, a novel framework that leverages inexpensive data samples
from general-purpose hardware (e.g., CPUs) to train cost models, followed by
few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of
input features across hardware platforms while effectively mitigating
heterogeneity, enabling cost model training with just 5% of the data samples
needed by accelerator-specific models to achieve comparable performance. We
conduct extensive experiments to demonstrate that COGNATE outperforms existing
techniques, achieving average speedups of 1.47x (up to 5.46x) for SpMM and
1.39x (up to 4.22x) for SDDMM.

</details>


### [273] [TIDFormer: Exploiting Temporal and Interactive Dynamics Makes A Great Dynamic Graph Transformer](https://arxiv.org/abs/2506.00431)
*Jie Peng,Zhewei Wei,Yuhang Ye*

Key words: 动态图神经网络、Transformer、时间动态、交互动态、TIDFormer

TL;DR: TIDFormer 是一种高效的动态图 Transformer 模型，专注于捕获时间和交互动态，通过简单分解历史交互模式，显著提升了性能和效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现有基于 Transformer 的动态图神经网络（DGNNs）在定义自注意力机制（SAM）时的不透明性和效率问题。

Method: TIDFormer 利用基于日历的时间分区信息和采样的一阶邻居提取交互嵌入，通过简单分解联合建模时间和交互动态。

Result: 实验表明，TIDFormer 在多个动态图数据集上表现优异，超越现有最优模型，并具有显著的效率优势。

Conclusion: TIDFormer 提供了一种高效且可解释的动态图 Transformer 架构，为动态图建模领域的进一步发展奠定了基础。

Abstract: Due to the proficiency of self-attention mechanisms (SAMs) in capturing
dependencies in sequence modeling, several existing dynamic graph neural
networks (DGNNs) utilize Transformer architectures with various encoding
designs to capture sequential evolutions of dynamic graphs. However, the
effectiveness and efficiency of these Transformer-based DGNNs vary
significantly, highlighting the importance of properly defining the SAM on
dynamic graphs and comprehensively encoding temporal and interactive dynamics
without extra complex modules. In this work, we propose TIDFormer, a dynamic
graph TransFormer that fully exploits Temporal and Interactive Dynamics in an
efficient manner. We clarify and verify the interpretability of our proposed
SAM, addressing the open problem of its uninterpretable definitions on dynamic
graphs in previous works. To model the temporal and interactive dynamics,
respectively, we utilize the calendar-based time partitioning information and
extract informative interaction embeddings for both bipartite and non-bipartite
graphs using merely the sampled first-order neighbors. In addition, we jointly
model temporal and interactive features by capturing potential changes in
historical interaction patterns through a simple decomposition. We conduct
extensive experiments on several dynamic graph datasets to verify the
effectiveness and efficiency of TIDFormer. The experimental results demonstrate
that TIDFormer excels, outperforming state-of-the-art models across most
datasets and experimental settings. Furthermore, TIDFormer exhibits significant
efficiency advantages compared to previous Transformer-based methods.

</details>


### [274] [Channel Normalization for Time Series Channel Identification](https://arxiv.org/abs/2506.00432)
*Seunghan Lee,Taeyoung Park,Kibok Lee*

Key words: 通道可辨识性, 时序建模, 归一化, 自适应参数, 原型学习

TL;DR: 该论文提出了通道归一化（CN）及其变体（ACN和PCN），以增强时序建模中的通道可辨识性（CID），并在多种模型上验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决时序建模中通道不可辨识性问题，避免模型忽略通道特有特征。

Method: 提出CN、ACN和PCN三种归一化策略，分别通过固定参数、动态调整参数和学习原型来增强CID。

Result: 在不同时序模型上应用CN及其变体，显著提升了性能。

Conclusion: CN及其变体能有效解决CID问题，适用于多种时序任务和数据集。

Abstract: Channel identifiability (CID) refers to the ability to distinguish between
individual channels in time series (TS) modeling. The absence of CID often
results in producing identical outputs for identical inputs, disregarding
channel-specific characteristics. In this paper, we highlight the importance of
CID and propose Channel Normalization (CN), a simple yet effective
normalization strategy that enhances CID by assigning distinct affine
transformation parameters to each channel. We further extend CN in two ways: 1)
Adaptive CN (ACN) dynamically adjusts parameters based on the input TS,
improving adaptability in TS models, and 2) Prototypical CN (PCN) introduces a
set of learnable prototypes instead of per-channel parameters, enabling
applicability to datasets with unknown or varying number of channels and
facilitating use in TS foundation models. We demonstrate the effectiveness of
CN and its variants by applying them to various TS models, achieving
significant performance gains for both non-CID and CID models. In addition, we
analyze the success of our approach from an information theory perspective.
Code is available at https://github.com/seunghan96/CN.

</details>


### [275] [Learning from Double Positive and Unlabeled Data for Potential-Customer Identification](https://arxiv.org/abs/2506.00436)
*Masahiro Kato,Yuki Ikeda abd Kentaro Baba,Takashi Imai,Ryo Inokuchi*

Key words: PU学习,客户识别,营销效率,忠诚度,正和无标记数据

TL;DR: 提出了一种通过正和无标记数据学习（PU学习）识别潜在客户的方法，旨在高效营销针对对产品有兴趣但忠诚度不高的客户。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 公司希望通过营销策略更有效地锁定那些对产品有兴趣但忠诚度不高的客户，以避免资源浪费。

Method: 提出了一种称为双PU学习的单阶段优化算法，其目标函数隐含了来自标准PU学习设置的两个损失。

Result: 通过数值实验验证了算法的有效性，证明其适用于目标问题。

Conclusion: 该方法能有效识别潜在客户，提升营销效率。

Abstract: In this study, we propose a method for identifying potential customers in
targeted marketing by applying learning from positive and unlabeled data (PU
learning). We consider a scenario in which a company sells a product and can
observe only the customers who purchased it. Decision-makers seek to market
products effectively based on whether people have loyalty to the company.
Individuals with loyalty are those who are likely to remain interested in the
company even without additional advertising. Consequently, those loyal
customers would likely purchase from the company if they are interested in the
product. In contrast, people with lower loyalty may overlook the product or buy
similar products from other companies unless they receive marketing attention.
Therefore, by focusing marketing efforts on individuals who are interested in
the product but do not have strong loyalty, we can achieve more efficient
marketing. To achieve this goal, we consider how to learn, from limited data, a
classifier that identifies potential customers who (i) have interest in the
product and (ii) do not have loyalty to the company. Although our algorithm
comprises a single-stage optimization, its objective function implicitly
contains two losses derived from standard PU learning settings. For this
reason, we refer to our approach as double PU learning. We verify the validity
of the proposed algorithm through numerical experiments, confirming that it
functions appropriately for the problem at hand.

</details>


### [276] [Is Your Explanation Reliable: Confidence-Aware Explanation on Graph Neural Networks](https://arxiv.org/abs/2506.00437)
*Jiaxing Zhang,Xiaoou Liu,Dongsheng Luo,Hua Wei*

Key words: 图神经网络，解释性，置信度评分，信息瓶颈，可靠性

TL;DR: ConfExplainer框架通过置信度评分模块量化GNN解释的可靠性，提升了解释的可信度和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解释图神经网络（GNN）的需求推动了可靠解释方法的研究，尤其是在分布外或未知数据集上。

Method: 提出基于理论原则（GIB-CC）的ConfExplainer框架，包含置信度评分模块，量化解释的可靠性。

Result: 实验证明，该框架通过置信度评分显著增强了解释的可信度和鲁棒性。

Conclusion: ConfExplainer为GNN解释提供了可靠的理论和方法支持。

Abstract: Explaining Graph Neural Networks (GNNs) has garnered significant attention
due to the need for interpretability, enabling users to understand the behavior
of these black-box models better and extract valuable insights from their
predictions. While numerous post-hoc instance-level explanation methods have
been proposed to interpret GNN predictions, the reliability of these
explanations remains uncertain, particularly in the out-of-distribution or
unknown test datasets. In this paper, we address this challenge by introducing
an explainer framework with the confidence scoring module ( ConfExplainer),
grounded in theoretical principle, which is generalized graph information
bottleneck with confidence constraint (GIB-CC), that quantifies the reliability
of generated explanations. Experimental results demonstrate the superiority of
our approach, highlighting the effectiveness of the confidence score in
enhancing the trustworthiness and robustness of GNN explanations.

</details>


### [277] [PointODE: Lightweight Point Cloud Learning with Neural Ordinary Differential Equations on Edge](https://arxiv.org/abs/2506.00438)
*Keisuke Sugiura,Mizuki Yasuda,Hiroki Matsutani*

Key words: 点云, 神经ODE, 嵌入式设备, FPGA, 特征提取

TL;DR: 本文提出了PointODE，一种参数高效的基于神经ODE的ResNet架构，用于点云特征提取，适用于资源有限的嵌入式设备。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决深度学习模型在资源受限的嵌入式设备上运行的问题。

Method: 使用神经ODE压缩模型，提出点归一化技术，并设计专用加速器。

Result: 在嵌入式FPGA上实现4.9倍加速，3.7倍推理速度提升和3.5倍能效提升。

Conclusion: PointODE-Elite在精度与计算成本之间取得了显著平衡。

Abstract: Embedded edge devices are often used as a computing platform to run
real-world point cloud applications, but recent deep learning-based methods may
not fit on such devices due to limited resources. In this paper, we aim to fill
this gap by introducing PointODE, a parameter-efficient ResNet-like
architecture for point cloud feature extraction based on a stack of MLP blocks
with residual connections. We leverage Neural ODE (Ordinary Differential
Equation), a continuous-depth version of ResNet originally developed for
modeling the dynamics of continuous-time systems, to compress PointODE by
reusing the same parameters across MLP blocks. The point-wise normalization is
proposed for PointODE to handle the non-uniform distribution of feature points.
We introduce PointODE-Elite as a lightweight version with 0.58M trainable
parameters and design its dedicated accelerator for embedded FPGAs. The
accelerator consists of a four-stage pipeline to parallelize the feature
extraction for multiple points and stores the entire parameters on-chip to
eliminate most of the off-chip data transfers. Compared to the ARM Cortex-A53
CPU, the accelerator implemented on a Xilinx ZCU104 board speeds up the feature
extraction by 4.9x, leading to 3.7x faster inference and 3.5x better
energy-efficiency. Despite the simple architecture, PointODE-Elite shows
competitive accuracy to the state-of-the-art models on both synthetic and
real-world classification datasets, greatly improving the trade-off between
accuracy and inference cost.

</details>


### [278] [RLAE: Reinforcement Learning-Assisted Ensemble for LLMs](https://arxiv.org/abs/2506.00439)
*Yuqian Fu,Yuanheng Zhu,Jiajun Chai,Guojun Yin,Wei Lin,Qichao Zhang,Dongbin Zhao*

Key words: 大语言模型, 强化学习, 模型集合, 动态权重, 马尔可夫决策过程

TL;DR: 提出了一种基于强化学习的动态权重调整框架RLAE，用于提升大语言模型集合的性能，显著优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的大语言模型集合方法依赖固定权重策略，无法动态适应语境和任务需求，限制了性能提升。

Method: 通过马尔可夫决策过程将集合问题建模，引入了强化学习代理动态调整权重，并使用单代理和多代理强化学习算法实现。

Result: 在多样化任务中，RLAE将准确率最高提升3.3%，并具备更好的泛化能力和更低延迟。

Conclusion: RLAE为语言模型集合提供了更有效的动态调整框架，优于传统方法且无需额外训练。

Abstract: Ensembling large language models (LLMs) can effectively combine diverse
strengths of different models, offering a promising approach to enhance
performance across various tasks. However, existing methods typically rely on
fixed weighting strategies that fail to adapt to the dynamic, context-dependent
characteristics of LLM capabilities. In this work, we propose Reinforcement
Learning-Assisted Ensemble for LLMs (RLAE), a novel framework that reformulates
LLM ensemble through the lens of a Markov Decision Process (MDP). Our approach
introduces a RL agent that dynamically adjusts ensemble weights by considering
both input context and intermediate generation states, with the agent being
trained using rewards that directly correspond to the quality of final outputs.
We implement RLAE using both single-agent and multi-agent reinforcement
learning algorithms ($\text{RLAE}_\text{PPO}$ and $\text{RLAE}_\text{MAPPO}$ ),
demonstrating substantial improvements over conventional ensemble methods.
Extensive evaluations on a diverse set of tasks show that RLAE outperforms
existing approaches by up to $3.3\%$ accuracy points, offering a more effective
framework for LLM ensembling. Furthermore, our method exhibits superior
generalization capabilities across different tasks without the need for
retraining, while simultaneously achieving lower time latency.

</details>


### [279] [PSI-PFL: Population Stability Index for Client Selection in non-IID Personalized Federated Learning](https://arxiv.org/abs/2506.00440)
*Daniel-M. Jimenez-Gutierrez,David Solans,Mohammed Elbamby,Nicolas Kourtellis*

Key words: 联邦学习,个性化联邦学习,非IID数据,PSI,客户端选择

TL;DR: 论文提出了PSI-PFL框架，通过PSI量化数据异质性并选择同质化客户端，以提升联邦学习（FL）在非IID数据下的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在联邦学习中，非IID数据导致模型更新偏差和性能下降，因此需要一种方法来解决数据异质性带来的挑战。

Method: 提出PSI-PFL框架，利用PSI量化数据异质性，选择同质化客户端以减轻标签偏斜对FL性能的影响。

Result: 实验表明，PSI-PFL在多种数据模态下显著提升全局模型准确性，并在非IID场景下优于现有基线方法10%。

Conclusion: PSI-PFL不仅提升了FL性能，还在数据隐私和异质性关键的应用中具备实用价值。

Abstract: Federated Learning (FL) enables decentralized machine learning (ML) model
training while preserving data privacy by keeping data localized across
clients. However, non-independent and identically distributed (non-IID) data
across clients poses a significant challenge, leading to skewed model updates
and performance degradation. Addressing this, we propose PSI-PFL, a novel
client selection framework for Personalized Federated Learning (PFL) that
leverages the Population Stability Index (PSI) to quantify and mitigate data
heterogeneity (so-called non-IIDness). Our approach selects more homogeneous
clients based on PSI, reducing the impact of label skew, one of the most
detrimental factors in FL performance. Experimental results over multiple data
modalities (tabular, image, text) demonstrate that PSI-PFL significantly
improves global model accuracy, outperforming state-of-the-art baselines by up
to 10\% under non-IID scenarios while ensuring fairer local performance.
PSI-PFL enhances FL performance and offers practical benefits in applications
where data privacy and heterogeneity are critical.

</details>


### [280] [TMetaNet: Topological Meta-Learning Framework for Dynamic Link Prediction](https://arxiv.org/abs/2506.00453)
*Hao Li,Hao Wan,Yuzhou Chen,Dongsheng Ye,Yulia Gel,Hao Jiang*

Key words: 动态图学习, 元学习, 持久同调, 拓扑特征

TL;DR: 论文提出了一种基于动态图持久同调的表示方法DZP，并结合动态拓扑特征提出了一种新的元学习参数更新模型TMetaNet，有效提升了动态图学习的效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 动态图的结构和时间依赖性不断变化，传统图学习方法难以应对。现有元学习方法未能充分利用动态图的高阶拓扑信息。

Method: 设计了DZP方法捕捉动态图的高阶特征，并基于此提出TMetaNet模型，利用高阶拓扑特征距离实现更有效的跨快照适应。

Result: 实验表明，TMetaNet在真实数据集上表现出色，且对图噪声具有鲁棒性。

Conclusion: TMetaNet展示了在元学习和动态图分析中的潜力。

Abstract: Dynamic graphs evolve continuously, presenting challenges for traditional
graph learning due to their changing structures and temporal dependencies.
Recent advancements have shown potential in addressing these challenges by
developing suitable meta-learning-based dynamic graph neural network models.
However, most meta-learning approaches for dynamic graphs rely on fixed weight
update parameters, neglecting the essential intrinsic complex high-order
topological information of dynamically evolving graphs. We have designed Dowker
Zigzag Persistence (DZP), an efficient and stable dynamic graph persistent
homology representation method based on Dowker complex and zigzag persistence,
to capture the high-order features of dynamic graphs. Armed with the DZP ideas,
we propose TMetaNet, a new meta-learning parameter update model based on
dynamic topological features. By utilizing the distances between high-order
topological features, TMetaNet enables more effective adaptation across
snapshots. Experiments on real-world datasets demonstrate TMetaNet's
state-of-the-art performance and resilience to graph noise, illustrating its
high potential for meta-learning and dynamic graph analysis. Our code is
available at https://github.com/Lihaogx/TMetaNet.

</details>


### [281] [Revisiting LLMs as Zero-Shot Time-Series Forecasters: Small Noise Can Break Large Models](https://arxiv.org/abs/2506.00457)
*Junwoo Park,Hyuck Lee,Dohyun Lee,Daehoon Gwak,Jaegul Choo*

Key words: LLMs, 时间序列预测, 零样本学习, 噪声敏感

TL;DR: 本文评估了大型语言模型（LLMs）在零样本时间序列预测中的表现，发现其效果不如领域专用模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究LLMs在零样本预测中的潜力，解决现有研究中矛盾的结论。

Method: 通过实验比较LLMs与最先进的领域专用模型在零样本预测中的性能。

Result: LLMs对噪声敏感，预测准确率较低，甚至不如简单的领域专用模型。

Conclusion: 未来研究应更多关注如何通过微调LLMs来提高其对数值序列的处理能力。

Abstract: Large Language Models (LLMs) have shown remarkable performance across diverse
tasks without domain-specific training, fueling interest in their potential for
time-series forecasting. While LLMs have shown potential in zero-shot
forecasting through prompting alone, recent studies suggest that LLMs lack
inherent effectiveness in forecasting. Given these conflicting findings, a
rigorous validation is essential for drawing reliable conclusions. In this
paper, we evaluate the effectiveness of LLMs as zero-shot forecasters compared
to state-of-the-art domain-specific models. Our experiments show that LLM-based
zero-shot forecasters often struggle to achieve high accuracy due to their
sensitivity to noise, underperforming even simple domain-specific models. We
have explored solutions to reduce LLMs' sensitivity to noise in the zero-shot
setting, but improving their robustness remains a significant challenge. Our
findings suggest that rather than emphasizing zero-shot forecasting, a more
promising direction would be to focus on fine-tuning LLMs to better process
numerical sequences. Our experimental code is available at
https://github.com/junwoopark92/revisiting-LLMs-zeroshot-forecaster.

</details>


### [282] [Reinforcement Learning for Hanabi](https://arxiv.org/abs/2506.00458)
*Nina Cohen,Kordel K. France*

Key words: Hanabi, reinforcement learning, TD algorithms, Expected SARSA, deep Q-Learning

TL;DR: 研究探索了表格和深度强化学习算法在Hanabi游戏中的表现，发现TD算法总体表现最佳，尤其是表格Expected SARSA和深度Q-Learning代理。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Hanabi作为不完全信息合作的卡牌游戏，为强化学习研究提供了独特挑战。

Method: 比较了不同类型的表格和深度强化学习算法及其在不同对手间的表现。

Result: TD算法表现最优，尤其是表格Expected SARSA和深度Q-Learning代理。

Conclusion: TD算法在表现和平衡性上优于表格代理，某些代理在与特定对手对抗时表现更佳。

Abstract: Hanabi has become a popular game for research when it comes to reinforcement
learning (RL) as it is one of the few cooperative card games where you have
incomplete knowledge of the entire environment, thus presenting a challenge for
a RL agent. We explored different tabular and deep reinforcement learning
algorithms to see which had the best performance both against an agent of the
same type and also against other types of agents. We establish that certain
agents played their highest scoring games against specific agents while others
exhibited higher scores on average by adapting to the opposing agent's
behavior. We attempted to quantify the conditions under which each algorithm
provides the best advantage and identified the most interesting interactions
between agents of different types. In the end, we found that temporal
difference (TD) algorithms had better overall performance and balancing of play
types compared to tabular agents. Specifically, tabular Expected SARSA and deep
Q-Learning agents showed the best performance.

</details>


### [283] [Comparing Traditional and Reinforcement-Learning Methods for Energy Storage Control](https://arxiv.org/abs/2506.00459)
*Elinor Ginzburg,Itay Segev,Yoash Levron,Sarah Keren*

Key words: 能量存储管理、强化学习、传统方法、微电网模型、性能比较

TL;DR: 该研究比较了传统方法与强化学习（RL）在能量存储管理中的性能差异，探讨了RL策略在简化的微电网模型中的应用及其局限性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索传统方法与强化学习在能量存储管理中的权衡，分析RL策略的性能损失及其适用场景。

Method: 使用包含负载、光伏电源和存储设备的简化微电网模型，比较三种复杂性递增的用例（理想存储、有损存储和有损存储加传输损耗）中传统方法与RL的性能。

Result: 研究发现，在不同复杂度的场景下，传统方法与RL各有优势，并指出了各自的适用情况。

Conclusion: 研究为RL在能量存储管理中的合理应用提供了指导，并提出了未来研究方向。

Abstract: We aim to better understand the tradeoffs between traditional and
reinforcement learning (RL) approaches for energy storage management. More
specifically, we wish to better understand the performance loss incurred when
using a generative RL policy instead of using a traditional approach to find
optimal control policies for specific instances. Our comparison is based on a
simplified micro-grid model, that includes a load component, a photovoltaic
source, and a storage device. Based on this model, we examine three use cases
of increasing complexity: ideal storage with convex cost functions, lossy
storage devices, and lossy storage devices with convex transmission losses.
With the aim of promoting the principled use RL based methods in this
challenging and important domain, we provide a detailed formulation of each use
case and a detailed description of the optimization challenges. We then compare
the performance of traditional and RL methods, discuss settings in which it is
beneficial to use each method, and suggest avenues for future investigation.

</details>


### [284] [SST: Self-training with Self-adaptive Thresholding for Semi-supervised Learning](https://arxiv.org/abs/2506.00467)
*Shuai Zhao,Heyan Huang,Xinge Li,Xiaokang Chen,Rui Wang*

Key words: 半监督学习,伪标签,自适应阈值,SST

TL;DR: 论文提出了一种新颖的半监督学习框架SST，通过自适应的阈值机制SAT解决现有方法中伪标签选择不准确和计算开销大的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前半监督学习方法虽然取得了一定成果，但仍面临伪标签选择不准确和计算效率低的挑战。

Method: SST框架引入自适应的SAT机制，动态调整类别特定阈值以选择高质量伪标签。

Result: 实验表明，SST在ImageNet-1K等数据集上表现优异，仅用1%/10%标记数据即可达到80.7%/84.9%的Top-1准确率。

Conclusion: SST通过自适应阈值机制显著提升了半监督学习的性能和效率。

Abstract: Neural networks have demonstrated exceptional performance in supervised
learning, benefiting from abundant high-quality annotated data. However,
obtaining such data in real-world scenarios is costly and labor-intensive.
Semi-supervised learning (SSL) offers a solution to this problem. Recent
studies, such as Semi-ViT and Noisy Student, which employ consistency
regularization or pseudo-labeling, have demonstrated significant achievements.
However, they still face challenges, particularly in accurately selecting
sufficient high-quality pseudo-labels due to their reliance on fixed
thresholds. Recent methods such as FlexMatch and FreeMatch have introduced
flexible or self-adaptive thresholding techniques, greatly advancing SSL
research. Nonetheless, their process of updating thresholds at each iteration
is deemed time-consuming, computationally intensive, and potentially
unnecessary. To address these issues, we propose Self-training with
Self-adaptive Thresholding (SST), a novel, effective, and efficient SSL
framework. SST introduces an innovative Self-Adaptive Thresholding (SAT)
mechanism that adaptively adjusts class-specific thresholds based on the
model's learning progress. SAT ensures the selection of high-quality
pseudo-labeled data, mitigating the risks of inaccurate pseudo-labels and
confirmation bias. Extensive experiments demonstrate that SST achieves
state-of-the-art performance with remarkable efficiency, generalization, and
scalability across various architectures and datasets. Semi-SST-ViT-Huge
achieves the best results on competitive ImageNet-1K SSL benchmarks, with 80.7%
/ 84.9% Top-1 accuracy using only 1% / 10% labeled data. Compared to the
fully-supervised DeiT-III-ViT-Huge, which achieves 84.8% Top-1 accuracy using
100% labeled data, our method demonstrates superior performance using only 10%
labeled data.

</details>


### [285] [Towards Graph-Based Privacy-Preserving Federated Learning: ModelNet -- A ResNet-based Model Classification Dataset](https://arxiv.org/abs/2506.00476)
*Abhisek Ray,Lukas Esterle*

Key words: 联邦学习、数据隐私、领域异构性、ModelNet、图驱动FL

TL;DR: 提出了ModelNet，一个基于预训练ResNet50模型嵌入的新型图像分类数据集，解决了联邦学习中数据隐私和领域异构性评估的挑战，并引入图驱动的FL算法验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决联邦学习中的数据隐私问题和现有基准测试中领域异构性不足的不足。

Method: 通过修改CIFAR100数据集生成三种客户端特定变体（同质、异质、随机），并基于预训练ResNet50模型训练，提出新的FL算法保护本地隐私。

Result: 实验验证了ModelNet变体在领域偏移和聚合策略上的有效性，适用于经典和图驱动的FL研究。

Conclusion: ModelNet为联邦学习提供了更贴近现实的评估基准，并推动了图驱动FL算法的发展。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for training
machine learning models across distributed data sources while preserving data
locality. However, the privacy of local data is always a pivotal concern and
has received a lot of attention in recent research on the FL regime. Moreover,
the lack of domain heterogeneity and client-specific segregation in the
benchmarks remains a critical bottleneck for rigorous evaluation. In this
paper, we introduce ModelNet, a novel image classification dataset constructed
from the embeddings extracted from a pre-trained ResNet50 model. First, we
modify the CIFAR100 dataset into three client-specific variants, considering
three domain heterogeneities (homogeneous, heterogeneous, and random).
Subsequently, we train each client-specific subset of all three variants on the
pre-trained ResNet50 model to save model parameters. In addition to
multi-domain image data, we propose a new hypothesis to define the FL algorithm
that can access the anonymized model parameters to preserve the local privacy
in a more effective manner compared to existing ones. ModelNet is designed to
simulate realistic FL settings by incorporating non-IID data distributions and
client diversity design principles in the mainframe for both conventional and
futuristic graph-driven FL algorithms. The three variants are ModelNet-S,
ModelNet-D, and ModelNet-R, which are based on homogeneous, heterogeneous, and
random data settings, respectively. To the best of our knowledge, we are the
first to propose a cross-environment client-specific FL dataset along with the
graph-based variant. Extensive experiments based on domain shifts and
aggregation strategies show the effectiveness of the above variants, making it
a practical benchmark for classical and graph-based FL research. The dataset
and related code are available online.

</details>


### [286] [Flashbacks to Harmonize Stability and Plasticity in Continual Learning](https://arxiv.org/abs/2506.00477)
*Leila Mahmoodi,Peyman Moghadam,Munawar Hayat,Christian Simon,Mehrtash Harandi*

Key words: Flashback Learning, Continual Learning, 稳定性, 可塑性, 双向正则化

TL;DR: Flashback Learning (FL) 是一种新颖的持续学习方法，通过双向正则化平衡模型稳定性和可塑性，显著提升了基线方法和现有方法的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决持续学习（CL）中模型稳定性和可塑性之间的平衡问题，提出了 FL 方法，避免仅通过模型更新正则化来保留旧知识的局限性。

Method: FL 通过两阶段的训练过程，利用两个独立的知识库（一个增强可塑性，另一个提升稳定性）来正则化模型更新，适用于多种 CL 方法。

Result: 在标准图像分类基准测试中，FL 将类别增量和任务增量设置的准确率分别平均提升了 4.91% 和 3.51%，并显著改善了稳定性和可塑性的平衡。

Conclusion: FL 是一种通用且高效的持续学习方法，能显著提升模型性能，尤其是在更具挑战性的数据集上优于现有方法。

Abstract: We introduce Flashback Learning (FL), a novel method designed to harmonize
the stability and plasticity of models in Continual Learning (CL). Unlike prior
approaches that primarily focus on regularizing model updates to preserve old
information while learning new concepts, FL explicitly balances this trade-off
through a bidirectional form of regularization. This approach effectively
guides the model to swiftly incorporate new knowledge while actively retaining
its old knowledge. FL operates through a two-phase training process and can be
seamlessly integrated into various CL methods, including replay, parameter
regularization, distillation, and dynamic architecture techniques. In designing
FL, we use two distinct knowledge bases: one to enhance plasticity and another
to improve stability. FL ensures a more balanced model by utilizing both
knowledge bases to regularize model updates. Theoretically, we analyze how the
FL mechanism enhances the stability-plasticity balance. Empirically, FL
demonstrates tangible improvements over baseline methods within the same
training budget. By integrating FL into at least one representative baseline
from each CL category, we observed an average accuracy improvement of up to
4.91% in Class-Incremental and 3.51% in Task-Incremental settings on standard
image classification benchmarks. Additionally, measurements of the
stability-to-plasticity ratio confirm that FL effectively enhances this
balance. FL also outperforms state-of-the-art CL methods on more challenging
datasets like ImageNet.

</details>


### [287] [Dynamic Domain Adaptation-Driven Physics-Informed Graph Representation Learning for AC-OPF](https://arxiv.org/abs/2506.00478)
*Hongjie Zhu,Zezheng Zhang,Zeyu Zhang,Yu Bai,Shimin Wen,Huazhang Wang,Daji Ergu,Ying Cai,Yang Zhao*

Key words: AC-OPF, 动态域适应, 物理信息图卷积网络, 时空依赖

TL;DR: 提出了一种名为DDA-PIGCN的新方法，结合时空特征的图卷积网络，用于解决AC-OPF问题中的约束建模和知识表示局限性，取得了良好的性能表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有AC-OPF求解器难以有效建模约束空间中变量分布与最优解间的复杂关系，且无法充分利用时空信息。

Method: 采用动态域适应驱动的物理信息图卷积网络（DDA-PIGCN），结合多层级硬物理约束和动态域适应学习机制，整合时空依赖关系。

Result: 在多个IEEE标准测试案例中表现优异，MAE为0.0011至0.0624，约束满足率为99.6%至100%。

Conclusion: DDA-PIGCN能够可靠且高效地解决AC-OPF问题，显着提升了约束建模和知识表示能力。

Abstract: Alternating Current Optimal Power Flow (AC-OPF) aims to optimize generator
power outputs by utilizing the non-linear relationships between voltage
magnitudes and phase angles in a power system. However, current AC-OPF solvers
struggle to effectively represent the complex relationship between variable
distributions in the constraint space and their corresponding optimal
solutions. This limitation in constraint modeling restricts the system's
ability to develop diverse knowledge representations. Additionally, modeling
the power grid solely based on spatial topology further limits the integration
of additional prior knowledge, such as temporal information. To overcome these
challenges, we propose DDA-PIGCN (Dynamic Domain Adaptation-Driven
Physics-Informed Graph Convolutional Network), a new method designed to address
constraint-related issues and build a graph-based learning framework that
incorporates spatiotemporal features. DDA-PIGCN improves consistency
optimization for features with varying long-range dependencies by applying
multi-layer, hard physics-informed constraints. It also uses a dynamic domain
adaptation learning mechanism that iteratively updates and refines key state
variables under predefined constraints, enabling precise constraint
verification. Moreover, it captures spatiotemporal dependencies between
generators and loads by leveraging the physical structure of the power grid,
allowing for deep integration of topological information across time and space.
Extensive comparative and ablation studies show that DDA-PIGCN delivers strong
performance across several IEEE standard test cases (such as case9, case30, and
case300), achieving mean absolute errors (MAE) from 0.0011 to 0.0624 and
constraint satisfaction rates between 99.6% and 100%, establishing it as a
reliable and efficient AC-OPF solver.

</details>


### [288] [BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation](https://arxiv.org/abs/2506.00482)
*Eunsu Kim,Haneul Yoo,Guijin Son,Hitesh Patel,Amit Agarwal,Alice Oh*

Key words: 大型语言模型, 基准测试, 数据集管理, 领域特定评估

TL;DR: 论文介绍了BenchHub，一个动态的基准测试库，帮助研究者更有效地评估大型语言模型（LLMs）。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有基准测试数据集分散且难管理，难以满足特定领域或需求，而领域特定模型（如数学或代码）的需求日益增长。

Method: 开发了BenchHub，聚合并自动分类多领域的基准测试数据集，整合了38个基准中的303K问题，支持持续更新和可扩展的数据管理。

Result: 实验表明，模型在不同领域子集的表现差异显著，强调了领域感知基准测试的重要性。

Conclusion: BenchHub促进了数据集重用、透明的模型比较，并有助于发现现有基准中未被充分代表的领域，推动了LLM评估研究的进步。

Abstract: As large language models (LLMs) continue to advance, the need for up-to-date
and well-organized benchmarks becomes increasingly critical. However, many
existing datasets are scattered, difficult to manage, and make it challenging
to perform evaluations tailored to specific needs or domains, despite the
growing importance of domain-specific models in areas such as math or code. In
this paper, we introduce BenchHub, a dynamic benchmark repository that empowers
researchers and developers to evaluate LLMs more effectively. BenchHub
aggregates and automatically classifies benchmark datasets from diverse
domains, integrating 303K questions across 38 benchmarks. It is designed to
support continuous updates and scalable data management, enabling flexible and
customizable evaluation tailored to various domains or use cases. Through
extensive experiments with various LLM families, we demonstrate that model
performance varies significantly across domain-specific subsets, emphasizing
the importance of domain-aware benchmarking. We believe BenchHub can encourage
better dataset reuse, more transparent model comparisons, and easier
identification of underrepresented areas in existing benchmarks, offering a
critical infrastructure for advancing LLM evaluation research.

</details>


### [289] [It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs](https://arxiv.org/abs/2506.00486)
*Jun Wu,Yirong Xiong,Jiangtao Wen,Yuxing Han*

Key words: 大型语言模型, 广义高斯分布, 参数压缩, 硬件效率, 初始化优化

TL;DR: 该论文提出了基于广义高斯分布（GGD）的统一端到端框架，用于优化大型语言模型（LLM），包括初始化、正则化和压缩技术，显著减少参数并保持性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管大型语言模型研究迅速发展，但其参数的统计分布及其对训练和效率的影响尚未得到充分关注。论文旨在通过统计建模优化LLM的效率和性能。

Method: 1. 基于GGD的初始化方案；2. DeepShape后训练正则化方法；3. RF8硬件高效8位浮点格式。三者结合形成统一框架。

Result: 实验表明，该框架能减少多达90%的参数，同时保持或超越标准训练基线的性能。

Conclusion: 通过统计建模，论文为高效、可扩展且硬件感知的AI系统提供了新路径。

Abstract: Despite rapid advancements in the research and deployment of large language
models (LLMs), the statistical distribution of model parameters, as well as
their influence on initialization, training dynamics, and downstream
efficiency, has received surprisingly little attention. A recent work
introduced BackSlash, a training-time compression algorithm. It first
demonstrated that pre-trained LLM parameters follow generalized Gaussian
distributions (GGDs) better. By optimizing GG priors during training, BackSlash
can reduce parameters by up to 90\% with minimal performance loss. Building on
this foundational insight, we propose a unified, end-to-end framework for LLM
optimization based on the GG model. Our contributions are threefold: (1)
GG-based initialization scheme that aligns with the statistical structure of
trained models, resulting in faster convergence and improved accuracy; (2)
DeepShape, a post-training regularization method that reshapes weight
distributions to match a GG profile, improving compressibility with minimized
degradation in performance; and (3) RF8, a compact and hardware-efficient 8-bit
floating-point format designed for GG-distributed-initialized BackSlash
training, enabling low-cost inference without compromising accuracy.
Experiments across diverse model architectures show that our framework
consistently yields smaller and faster models that match or outperform standard
training baselines. By grounding LLM development in principled statistical
modeling, this work forges a new path toward efficient, scalable, and
hardware-aware AI systems. The code is available on our project page:
https://huggingface.co/spaces/shifeng3711/gg_prior.

</details>


### [290] [FLoE: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts](https://arxiv.org/abs/2506.00495)
*Xinyi Wang,Lirong Gao,Haobo Wang,Yiming Zhang,Junbo Zhao*

Key words: PEFT, LoRA, FLoE, Fisher信息, 贝叶斯优化

TL;DR: FLoE是一种新颖的PEFT框架，通过动态识别关键层和自动分配LoRA秩，提高了适配效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有PEFT方法在适配预训练大模型时，均匀部署LoRA适配器导致冗余参数分配和效率低下。

Method: 采用Fisher信息引导的重要性评分机制和贝叶斯优化的秩分配器，实现稀疏适配。

Result: FLoE在多种LLM和基准测试中表现出高效的准确性权衡。

Conclusion: FLoE特别适合资源受限需要快速适配的场景。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a widely
adopted strategy for adapting pre-trained Large Language Models (LLMs) to
downstream tasks, significantly reducing memory and computational costs.
However, most existing PEFT techniques uniformly deploy LoRA adapters across
all layers, disregarding the intrinsic heterogeneity of layer contributions and
task-specific rank requirements. This uniform paradigm leads to redundant
parameter allocation and suboptimal adaptation efficiency. To address these
limitations, we propose FLoE, a novel PEFT framework that introduces two key
innovations: (i) a Fisher information-guided importance scoring mechanism to
dynamically identify task-critical transformer layers for MoE-based low-rank
adaptation, enabling sparse adapter deployment; and (ii) a Bayesian
optimization-driven rank allocator that automatically determines optimal LoRA
ranks on specific datasets without exhaustive grid search. Extensive
experiments across diverse LLMs and benchmarks reveal that FLoE achieves
impressive efficiency-accuracy trade-offs, making FLoE particularly
advantageous in resource-constrained environments that necessitate rapid
adaptation.

</details>


### [291] [Federated learning framework for collaborative remaining useful life prognostics: an aircraft engine case study](https://arxiv.org/abs/2506.00499)
*Diogo Landau,Ingeborg de Pater,Mihaela Mitici,Nishant Saurabh*

Key words: 联邦学习, 剩余使用寿命, 预测性维护, 数据隐私, 噪声数据

TL;DR: 论文提出了一个协作式联邦学习框架，用于预测飞机发动机的剩余使用寿命（RUL），解决了数据隐私和噪声问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决飞机发动机预测性维护中因数据隐私限制而无法共享数据的问题，并应对传感器数据的噪声和低质量挑战。

Method: 开发了一种协作式联邦学习框架，包括分散验证程序和四种参数聚合方法，以增强模型对噪声数据的鲁棒性。

Result: 联邦学习框架比独立训练的模型更准确，且在六家航空公司中有五家表现出更好的RUL预测性能。

Conclusion: 协作式联邦学习有效解决了数据隐私和噪声问题，提升了RUL预测的准确性。

Abstract: Complex systems such as aircraft engines are continuously monitored by
sensors. In predictive aircraft maintenance, the collected sensor measurements
are used to estimate the health condition and the Remaining Useful Life (RUL)
of such systems. However, a major challenge when developing prognostics is the
limited number of run-to-failure data samples. This challenge could be overcome
if multiple airlines would share their run-to-failure data samples such that
sufficient learning can be achieved. Due to privacy concerns, however, airlines
are reluctant to share their data in a centralized setting. In this paper, a
collaborative federated learning framework is therefore developed instead.
Here, several airlines cooperate to train a collective RUL prognostic machine
learning model, without the need to centrally share their data. For this, a
decentralized validation procedure is proposed to validate the prognostics
model without sharing any data. Moreover, sensor data is often noisy and of low
quality. This paper therefore proposes four novel methods to aggregate the
parameters of the global prognostic model. These methods enhance the robustness
of the FL framework against noisy data. The proposed framework is illustrated
for training a collaborative RUL prognostic model for aircraft engines, using
the N-CMAPSS dataset. Here, six airlines are considered, that collaborate in
the FL framework to train a collective RUL prognostic model for their
aircraft's engines. When comparing the proposed FL framework with the case
where each airline independently develops their own prognostic model, the
results show that FL leads to more accurate RUL prognostics for five out of the
six airlines. Moreover, the novel robust aggregation methods render the FL
framework robust to noisy data samples.

</details>


### [292] [From Rules to Rewards: Reinforcement Learning for Interest Rate Adjustment in DeFi Lending](https://arxiv.org/abs/2506.00505)
*Hanxiao Qu,Krzysztof Gogol,Florian Groetschla,Claudio Tessone*

Key words: DeFi, 强化学习, 利率优化, Aave, TD3-BC

TL;DR: 论文提出利用离线强化学习优化DeFi借贷协议的利率调整，TD3-BC方法在历史压力事件中表现最优。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决DeFi借贷中利率优化的动态适应性问题，提高资本效率和减少坏账。

Method: 使用离线强化学习，评估CQL、BC和TD3-BC三种方法，基于Aave协议的历史数据。

Result: TD3-BC在平衡利用率、资本稳定性和风险方面优于现有模型。

Conclusion: TD3-BC能有效适应历史压力事件，显示出实时自动化治理的潜力。

Abstract: Decentralized Finance (DeFi) lending enables permissionless borrowing via
smart contracts. However, it faces challenges in optimizing interest rates,
mitigating bad debt, and improving capital efficiency. Rule-based interest-rate
models struggle to adapt to dynamic market conditions, leading to
inefficiencies. This work applies Offline Reinforcement Learning (RL) to
optimize interest rate adjustments in DeFi lending protocols. Using historical
data from Aave protocol, we evaluate three RL approaches: Conservative
Q-Learning (CQL), Behavior Cloning (BC), and TD3 with Behavior Cloning
(TD3-BC). TD3-BC demonstrates superior performance in balancing utilization,
capital stability, and risk, outperforming existing models. It adapts
effectively to historical stress events like the May 2021 crash and the March
2023 USDC depeg, showcasing potential for automated, real-time governance.

</details>


### [293] [Ultra-Quantisation: Efficient Embedding Search via 1.58-bit Encodings](https://arxiv.org/abs/2506.00528)
*Richard Connor,Alan Dearle,Ben Claydon*

Key words: 量化, 嵌入向量, 高维空间, 凸多面体, 相似度测量

TL;DR: 论文提出了一种极端的量化方法，将高维浮点向量替换为仅包含{-1,0,1}的向量，显著节省空间和计算成本。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 高维嵌入向量占用大量空间且计算成本高，需寻找高效的量化方法以减少资源消耗。

Method: 通过一类高维凸多面体，将任意精度的浮点向量替换为{-1,0,1}的向量。

Result: 此方法显著节省了空间和计算时间，同时保持了相似度测量的强相关性。

Conclusion: 极端量化方法在保持精度的同时，极大优化了嵌入向量的存储和计算效率。

Abstract: Many modern search domains comprise high-dimensional vectors of floating
point numbers derived from neural networks, in the form of embeddings. Typical
embeddings range in size from hundreds to thousands of dimensions, making the
size of the embeddings, and the speed of comparison, a significant issue.
  Quantisation is a class of mechanism which replaces the floating point values
with a smaller representation, for example a short integer. This gives an
approximation of the embedding space in return for a smaller data
representation and a faster comparison function.
  Here we take this idea almost to its extreme: we show how vectors of
arbitrary-precision floating point values can be replaced by vectors whose
elements are drawn from the set {-1,0,1}. This yields very significant savings
in space and metric evaluation cost, while maintaining a strong correlation for
similarity measurements.
  This is achieved by way of a class of convex polytopes which exist in the
high-dimensional space. In this article we give an outline description of these
objects, and show how they can be used for the basis of such radical
quantisation while maintaining a surprising degree of accuracy.

</details>


### [294] [M2WLLM: Multi-Modal Multi-Task Ultra-Short-term Wind Power Prediction Algorithm Based on Large Language Model](https://arxiv.org/abs/2506.00531)
*Hang Fana,Mingxuan Lib,Zuhan Zhanga,Long Chengc,Yujian Ye,Dunnan Liua*

Key words: 风电功率预测,大型语言模型,多模态数据,少样本学习,超短期预测

TL;DR: 论文提出了一种名为M2WLLM的创新模型，利用大型语言模型（LLMs）进行超短期风电功率预测，显著提高了预测精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 风电并网需要超短期风电功率预测以确保电网稳定和资源优化配置，但现有方法存在局限性。

Method: M2WLLM通过Prompt Embedder和Data Embedder，结合文本和数值数据，利用语义增强器将时间数据转换为LLMs可理解的格式，提取潜在特征。

Result: 在不同数据集和预测时间范围内，M2WLLM均优于GPT4TS等现有方法，展现了LLMs在超短期预测中的高准确性和鲁棒性。

Conclusion: LLMs在超短期风电功率预测中表现出色，具备少样本学习能力，为风电预测提供了新思路。

Abstract: The integration of wind energy into power grids necessitates accurate
ultra-short-term wind power forecasting to ensure grid stability and optimize
resource allocation. This study introduces M2WLLM, an innovative model that
leverages the capabilities of Large Language Models (LLMs) for predicting wind
power output at granular time intervals. M2WLLM overcomes the limitations of
traditional and deep learning methods by seamlessly integrating textual
information and temporal numerical data, significantly improving wind power
forecasting accuracy through multi-modal data. Its architecture features a
Prompt Embedder and a Data Embedder, enabling an effective fusion of textual
prompts and numerical inputs within the LLMs framework. The Semantic Augmenter
within the Data Embedder translates temporal data into a format that the LLMs
can comprehend, enabling it to extract latent features and improve prediction
accuracy. The empirical evaluations conducted on wind farm data from three
Chinese provinces demonstrate that M2WLLM consistently outperforms existing
methods, such as GPT4TS, across various datasets and prediction horizons. The
results highlight LLMs' ability to enhance accuracy and robustness in
ultra-short-term forecasting and showcase their strong few-shot learning
capabilities.

</details>


### [295] [RsGCN: Rescaling Enhances Generalization of GCNs for Solving Scalable Traveling Salesman Problems](https://arxiv.org/abs/2506.00533)
*Junquan Huang,Zong-Gan Chen,Yuncheng Jiang,Zhi-Hui Zhan*

Key words: TSP, RsGCN, Rescaling Mechanism, Re2Opt, Generalization

TL;DR: 论文提出了Rescaling Graph Convolutional Network (RsGCN)和Re2Opt算法，解决了神经网络求解旅行商问题时泛化能力差和训练成本高的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的神经网络旅行商问题(TSP)求解器面临泛化能力不足和训练成本高的挑战。

Method: 提出RsGCN，通过节点和边的重新缩放机制增强泛化能力，并结合Re2Opt后搜索算法优化热图。

Result: 该方法仅需3个训练周期即可从100节点泛化到10K节点，且在实验中表现出色。

Conclusion: RsGCN和Re2Opt的组合在泛化能力和训练效率上达到领先水平。

Abstract: Neural traveling salesman problem (TSP) solvers face two critical challenges:
poor generalization for scalable TSPs and high training costs. To address these
challenges, we propose a new Rescaling Graph Convolutional Network (RsGCN).
Focusing on the scale-dependent features (i.e., features varied with problem
scales) related to nodes and edges that influence the sensitivity of GCNs to
the problem scales, a Rescaling Mechanism in RsGCN enhances the generalization
capability by (1) rescaling adjacent nodes to construct a subgraph with a
uniform number of adjacent nodes for each node across various scales of TSPs,
which stabilizes the graph message aggregation; (2) rescaling subgraph edges to
adjust the lengths of subgraph edges to the same magnitude, which maintains
numerical consistency. In addition, an efficient training strategy with a
mixed-scale dataset and bidirectional loss is used in RsGCN. To fully exploit
the heatmaps generated by RsGCN, we design an efficient post-search algorithm
termed Re2Opt, in which a reconstruction process based on adaptive weight is
incorporated to help avoid local optima. Based on a combined architecture of
RsGCN and Re2Opt, our solver achieves remarkable generalization and low
training cost: with only 3 epochs of training on the mixed-scale dataset
containing instances with up to 100 nodes, it can be generalized successfully
to 10K-node instances without any fine-tuning. Extensive experiments
demonstrate our state-of-the-art performance across uniform distribution
instances of 9 different scales from 20 to 10K nodes and 78 real-world
instances from TSPLIB, while requiring the fewest learnable parameters and
training epochs among neural competitors.

</details>


### [296] [Imputation of Missing Data in Smooth Pursuit Eye Movements Using a Self-Attention-based Deep Learning Approach](https://arxiv.org/abs/2506.00545)
*Mehdi Bejani,Guillermo Perez-de-Arenaza-Pozo,Julián D. Arias-Londoño,Juan I. Godino-LLorente*

Key words: 缺失数据,自注意力,自动编码器,眼动序列,深度学习

TL;DR: 提出了一种新的自注意力机制缺失填补框架，用于生物医学时间序列数据，显著提高了填补精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 处理生物医学时间序列（如眼动数据）中的缺失问题，以提升神经退行性疾病筛查的可靠性。

Method: 结合自注意力机制和定制化自动编码器的深度学习框架，用于填补时间序列缺失数据。

Result: 在5,504个序列上测试，显著降低了误差指标，并保持了信号频域特征。

Conclusion: 该方法为时间序列缺失数据提供了一种鲁棒的解决方案，尤其适用于神经退行性疾病的监测。

Abstract: Missing data is a relevant issue in time series, especially in biomedical
sequences such as those corresponding to smooth pursuit eye movements, which
often contain gaps due to eye blinks and track losses, complicating the
analysis and extraction of meaningful biomarkers. In this paper, a novel
imputation framework is proposed using Self-Attention-based Imputation networks
for time series, which leverages the power of deep learning and self-attention
mechanisms to impute missing data. We further refine the imputed data using a
custom made autoencoder, tailored to represent smooth pursuit eye movement
sequences. The proposed approach was implemented using 5,504 sequences from 172
Parkinsonian patients and healthy controls. Results show a significant
improvement in the accuracy of reconstructed eye movement sequences with
respect to other state of the art techniques, substantially reducing the values
for common time domain error metrics such as the mean absolute error, mean
relative error, and root mean square error, while also preserving the signal's
frequency domain characteristics. Moreover, it demonstrates robustness when
large intervals of data are missing. This method offers an alternative solution
for robustly handling missing data in time series, enhancing the reliability of
smooth pursuit analysis for the screening and monitoring of neurodegenerative
disorders.

</details>


### [297] [MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning](https://arxiv.org/abs/2506.00555)
*Peng Xia,Jinglu Wang,Yibo Peng,Kaide Zeng,Xian Wu,Xiangru Tang,Hongtu Zhu,Yun Li,Shujie Liu,Yan Lu,Huaxiu Yao*

Key words: 医疗视觉语言模型,多代理协作,强化学习,课程学习,动态推理

TL;DR: 论文提出了一种基于强化学习的多代理框架MMedAgent-RL，通过动态协作优化医疗代理交互，显著提升了跨专科诊疗任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有单代理医疗视觉语言模型在多专科任务中泛化能力不足，静态多代理框架缺乏灵活性和适应性。

Method: 采用强化学习训练两种GP代理：分诊医生（分配患者）和主治医生（整合多专科意见）。通过课程学习策略优化主治医生的决策能力。

Result: 在五个医疗VQA基准测试中，MMedAgent-RL表现优于现有模型，性能平均提升18.4%，且具备类人推理模式。

Conclusion: 动态多代理协作框架显著提升跨专科诊疗能力，为医疗AI提供新思路。

Abstract: Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential
in multimodal diagnostic tasks. However, existing single-agent models struggle
to generalize across diverse medical specialties, limiting their performance.
Recent efforts introduce multi-agent collaboration frameworks inspired by
clinical workflows, where general practitioners (GPs) and specialists interact
in a fixed sequence. Despite improvements, these static pipelines lack
flexibility and adaptability in reasoning. To address this, we propose
MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that
enables dynamic, optimized collaboration among medical agents. Specifically, we
train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to
assign patients to appropriate specialties, while the attending physician
integrates the judgments from multi-specialists and its own knowledge to make
final decisions. To address the inconsistency in specialist outputs, we
introduce a curriculum learning (CL)-guided RL strategy that progressively
teaches the attending physician to balance between imitating specialists and
correcting their mistakes. Experiments on five medical VQA benchmarks
demonstrate that MMedAgent-RL not only outperforms both open-source and
proprietary Med-LVLMs, but also exhibits human-like reasoning patterns.
Notably, it achieves an average performance gain of 18.4% over supervised
fine-tuning baselines.

</details>


### [298] [Understanding Behavioral Metric Learning: A Large-Scale Study on Distracting Reinforcement Learning Environments](https://arxiv.org/abs/2506.00563)
*Ziyan Luo,Tianwei Ni,Pierre-Luc Bacon,Doina Precup,Xujie Si*

Key words: 深度强化学习，行为度量，等距嵌入，状态抽象，噪声鲁棒性

TL;DR: 论文研究了行为度量（如双模拟度量）在深度强化学习中的学习方法，评估了五种方法并通过实验验证其性能，同时提出了新的评估指标和开源代码库。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的行为度量学习方法存在理论与实践之间的差距，且评估主要集中在最终回报上，缺乏对度量质量和性能增益来源的系统分析。

Method: 研究了五种统一为等距嵌入的方法，在20种状态任务和14种像素任务中进行了基准测试，并提出了去噪因子和孤立度量估计设置。

Result: 通过实验验证了不同方法在多样化噪声设置下的性能，并量化了编码器过滤干扰的能力。

Conclusion: 论文为深度强化学习中的度量学习提供了系统性评估和新工具，促进了未来研究的可复现性。

Abstract: A key approach to state abstraction is approximating behavioral metrics
(notably, bisimulation metrics) in the observation space and embedding these
learned distances in the representation space. While promising for robustness
to task-irrelevant noise, as shown in prior work, accurately estimating these
metrics remains challenging, requiring various design choices that create gaps
between theory and practice. Prior evaluations focus mainly on final returns,
leaving the quality of learned metrics and the source of performance gains
unclear. To systematically assess how metric learning works in deep
reinforcement learning (RL), we evaluate five recent approaches, unified
conceptually as isometric embeddings with varying design choices. We benchmark
them with baselines across 20 state-based and 14 pixel-based tasks, spanning
370 task configurations with diverse noise settings. Beyond final returns, we
introduce the evaluation of a denoising factor to quantify the encoder's
ability to filter distractions. To further isolate the effect of metric
learning, we propose and evaluate an isolated metric estimation setting, in
which the encoder is influenced solely by the metric loss. Finally, we release
an open-source, modular codebase to improve reproducibility and support future
research on metric learning in deep RL.

</details>


### [299] [AutoMixAlign: Adaptive Data Mixing for Multi-Task Preference Optimization in LLMs](https://arxiv.org/abs/2506.00569)
*Nicholas E. Corrado,Julian Katz-Samuels,Adithya Devraj,Hyokun Yun,Chao Zhang,Yi Xu,Yi Pan,Bing Yin,Trishul Chilimbi*

Key words: 大型语言模型、多任务对齐、自适应优化、AutoMixAlign、专家模型

TL;DR: 论文提出了一种名为AutoMixAlign（AMA）的算法，通过动态混合数据集和自适应优化来解决多任务对齐问题，提升了大型语言模型在各种任务上的表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大型语言模型（LLMs）在对齐任务中表现依赖于训练数据的组成，但现有方法依赖昂贵的消融研究或人工直觉，难以全局优化。需要一种更高效的方法来平衡多任务性能。

Method: 提出AMA算法，结合专家模型损失和极小极大优化，动态调整任务权重或数据采样比例，具体包括AMA-R（自适应加权）和AMA-S（自适应采样）两种变体。

Result: AMA在多项多任务对齐实验中表现优于标准对齐方法和模型合并方法，且两种变体均具有理论收敛保证。

Conclusion: AMA通过理论驱动的自适应混合策略显著提升了多任务对齐的效果，为LLM训练数据优化提供了新思路。

Abstract: When aligning large language models (LLMs), their performance on various
tasks (such as being helpful, harmless, and honest) depends heavily on the
composition of their training data. However, selecting a data mixture that
achieves strong performance across all tasks is challenging. Existing
approaches rely on large ablation studies, heuristics, or human intuition, but
these can be prohibitively expensive and suboptimal. We study this problem in
the setting of preference optimization via DPO and introduce AutoMixAlign
(AMA), a theoretically-grounded algorithm that adaptively mixes datasets during
training to balance performance across tasks. AMA first trains
\textit{specialist models} for each task to determine losses that correspond to
strong task performance. Then, it trains a generalist model using a novel
minimax optimization that prioritizes tasks for which generalist model losses
deviate most from specialist model losses. To optimize this problem, we propose
two algorithms: (1) AMA-R, which adaptively reweights the objective to
prioritize tasks, and (2) AMA-S, which adaptively adjusts how much data is
sampled from each task to prioritize tasks. Both algorithms achieve a
convergence rate of $O(1/\sqrt{T})$ in the convex case. AMA-R's convergence
result follows from Sagawa et al. (2019), and we provide a convergence proof
for AMA-S using online learning techniques such as EXP3. We evaluate AMA on
several multitask alignment setups and find that AMA outperforms the standard
alignment approach -- which simply optimizes the total loss across all tasks --
and also outperforms model merging methods.

</details>


### [300] [Neural Estimation for Scaling Entropic Multimarginal Optimal Transport](https://arxiv.org/abs/2506.00573)
*Dor Tsur,Ziv Goldfeld,Kristjan Greenewald,Haim Permuter*

Key words: 多边际最优运输, 熵正则化, 神经网络, 计算效率

TL;DR: NEMOT是一种新的计算框架，通过神经网络和迷你批次训练显著提高了多边际最优运输的计算效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多边际最优运输（MOT）因计算复杂度高而受限，尤其是对大数据集的依赖（$O(n^k)$）。本文旨在解决这一瓶颈。

Method: 提出NEMOT，通过神经网络和迷你批次训练，将计算复杂度从数据集规模转移到迷你批次大小，并提供了误差保证。

Result: NEMOT显著提升性能，比Sinkhorn算法快多个数量级，且支持更多样本和边际。

Conclusion: NEMOT可扩展多边际最优运输的实际应用，适用于大规模机器学习任务。

Abstract: Multimarginal optimal transport (MOT) is a powerful framework for modeling
interactions between multiple distributions, yet its applicability is
bottlenecked by a high computational overhead. Entropic regularization provides
computational speedups via the multimarginal Sinkhorn algorithm, whose time
complexity, for a dataset size $n$ and $k$ marginals, generally scales as
$O(n^k)$. However, this dependence on the dataset size $n$ is computationally
prohibitive for many machine learning problems. In this work, we propose a new
computational framework for entropic MOT, dubbed Neural Entropic MOT (NEMOT),
that enjoys significantly improved scalability. NEMOT employs neural networks
trained using mini-batches, which transfers the computational complexity from
the dataset size to the size of the mini-batch, leading to substantial gains.
We provide formal guarantees on the accuracy of NEMOT via non-asymptotic error
bounds. We supplement these with numerical results that demonstrate the
performance gains of NEMOT over Sinkhorn's algorithm, as well as extensions to
neural computation of multimarginal entropic Gromov-Wasserstein alignment. In
particular, orders-of-magnitude speedups are observed relative to the
state-of-the-art, with a notable increase in the feasible number of samples and
marginals. NEMOT seamlessly integrates as a module in large-scale machine
learning pipelines, and can serve to expand the practical applicability of
entropic MOT for tasks involving multimarginal data.

</details>


### [301] [Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing](https://arxiv.org/abs/2506.00574)
*Fatemeh Lotfi,Hossein Rajoli,Fatemeh Afghah*

Key words: 无线网络, DRL, LLM, 提示学习, 资源分配

TL;DR: 论文提出了一种基于可学习提示的LLM增强DRL框架（PA-MRL），通过优化语义聚类和RL目标，提升无线网络资源分配的效率和适应性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统DRL在动态无线网络环境中表现不佳，缺乏对无序反馈的有效处理；LLMs能结构化反馈并辅助RL决策。

Method: 引入基于上下文的适应方法，利用LLM的可学习提示优化状态表示，结合多智能体RL框架（PA-MRL）。

Result: 实验显示该方法加速收敛，性能优于基线，实现更高效的资源分配。

Conclusion: 提示增强学习提升了DRL的适应性和效率，适用于动态网络环境。

Abstract: Modern wireless networks must adapt to dynamic conditions while efficiently
managing diverse service demands. Traditional deep reinforcement learning (DRL)
struggles in these environments, as scattered and evolving feedback makes
optimal decision-making challenging. Large Language Models (LLMs) offer a
solution by structuring unorganized network feedback into meaningful latent
representations, helping RL agents recognize patterns more effectively. For
example, in O-RAN slicing, concepts like SNR, power levels and throughput are
semantically related, and LLMs can naturally cluster them, providing a more
interpretable state representation. To leverage this capability, we introduce a
contextualization-based adaptation method that integrates learnable prompts
into an LLM-augmented DRL framework. Instead of relying on full model
fine-tuning, we refine state representations through task-specific prompts that
dynamically adjust to network conditions. Utilizing ORANSight, an LLM trained
on O-RAN knowledge, we develop Prompt-Augmented Multi agent RL (PA-MRL)
framework. Learnable prompts optimize both semantic clustering and RL
objectives, allowing RL agents to achieve higher rewards in fewer iterations
and adapt more efficiently. By incorporating prompt-augmented learning, our
approach enables faster, more scalable, and adaptive resource allocation in
O-RAN slicing. Experimental results show that it accelerates convergence and
outperforms other baselines.

</details>


### [302] [ORAN-GUIDE: RAG-Driven Prompt Learning for LLM-Augmented Reinforcement Learning in O-RAN Network Slicing](https://arxiv.org/abs/2506.00576)
*Fatemeh Lotfi,Hossein Rajoli,Fatemeh Afghah*

Key words: 无线网络, O-RAN, 多智能体强化学习, 语言模型, 资源分配

TL;DR: ORAN-GUIDE是一个双LLM框架，通过增强的多智能体强化学习（MARL），利用语义丰富状态表示提升无线网络资源分配的效率和泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决动态异构服务需求下，传统深度强化学习在处理非结构输入（如射频特征、QoS指标等）时泛化能力和决策效率不足的问题。

Method: 提出ORAN-GUIDE框架，结合领域专用语言模型ORANSight和冻结的GPT编码器，生成结构化、上下文感知的提示，为DRL智能体提供高级语义表示。

Result: 实验表明，ORAN-GUIDE在样本效率、策略收敛性和性能泛化方面优于标准MARL和单LLM基线。

Conclusion: ORAN-GUIDE通过语义增强的MARL，显著提升了无线网络资源分配的智能化水平。

Abstract: Advanced wireless networks must support highly dynamic and heterogeneous
service demands. Open Radio Access Network (O-RAN) architecture enables this
flexibility by adopting modular, disaggregated components, such as the RAN
Intelligent Controller (RIC), Centralized Unit (CU), and Distributed Unit (DU),
that can support intelligent control via machine learning (ML). While deep
reinforcement learning (DRL) is a powerful tool for managing dynamic resource
allocation and slicing, it often struggles to process raw, unstructured input
like RF features, QoS metrics, and traffic trends. These limitations hinder
policy generalization and decision efficiency in partially observable and
evolving environments. To address this, we propose \textit{ORAN-GUIDE}, a
dual-LLM framework that enhances multi-agent RL (MARL) with task-relevant,
semantically enriched state representations. The architecture employs a
domain-specific language model, ORANSight, pretrained on O-RAN control and
configuration data, to generate structured, context-aware prompts. These
prompts are fused with learnable tokens and passed to a frozen GPT-based
encoder that outputs high-level semantic representations for DRL agents. This
design adopts a retrieval-augmented generation (RAG) style pipeline tailored
for technical decision-making in wireless systems. Experimental results show
that ORAN-GUIDE improves sample efficiency, policy convergence, and performance
generalization over standard MARL and single-LLM baselines.

</details>


### [303] [Slow Feature Analysis as Variational Inference Objective](https://arxiv.org/abs/2506.00580)
*Merlin Schüler,Laurenz Wiskott*

Key words: Slow Feature Analysis, variational inference, probabilistic interpretation, non-linearity

TL;DR: 本文通过变分推理的视角提出了对慢特征分析（SFA）的新概率解释，放松了线性约束，重新将经典的慢目标置于变分框架中。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过变分推理重新解释SFA，探索非线性条件下的慢特征分析，扩展其理论基础。

Method: 采用变分推理框架，将慢目标作为重构损失的正则化项，并分析了重构损失的作用。

Result: 展示了慢目标在变分框架中的新解释，为非线性SFA提供了新视角。

Conclusion: 通过变分方法重新定义SFA，为未来的研究开辟了新方向。

Abstract: This work presents a novel probabilistic interpretation of Slow Feature
Analysis (SFA) through the lens of variational inference. Unlike prior
formulations that recover linear SFA from Gaussian state-space models with
linear emissions, this approach relaxes the key constraint of linearity. While
it does not lead to full equivalence to non-linear SFA, it recasts the
classical slowness objective in a variational framework. Specifically, it
allows the slowness objective to be interpreted as a regularizer to a
reconstruction loss. Furthermore, we provide arguments, why -- from the
perspective of slowness optimization -- the reconstruction loss takes on the
role of the constraints that ensure informativeness in SFA. We conclude with a
discussion of potential new research directions.

</details>


### [304] [Decoding the Stressed Brain with Geometric Machine Learning](https://arxiv.org/abs/2506.00587)
*Sonia Koszut,Sam Nallaperuma-Herzberg,Pietro Lio*

Key words: 压力检测, EEG, 几何机器学习, ST-GCN

TL;DR: 提出了一种基于几何机器学习的框架，通过EEG信号检测压力，结合结构和功能连接性，使用ST-GCN模型提升了分类性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统压力检测方法依赖主观问卷，本研究旨在开发更客观、准确的压力检测方法。

Method: 集成结构连接性和功能连接性构建图，利用ST-GCN模型分析时空动态。

Result: 在SAM-40数据集上，ST-GCN在所有关键分类指标上优于标准机器学习模型。

Conclusion: 该方法为压力检测提供了更客观和准确的途径。

Abstract: Stress significantly contributes to both mental and physical disorders, yet
traditional self-reported questionnaires are inherently subjective. In this
study, we introduce a novel framework that employs geometric machine learning
to detect stress from raw EEG recordings. Our approach constructs graphs by
integrating structural connectivity (derived from electrode spatial
arrangement) with functional connectivity from pairwise signal correlations. A
spatio-temporal graph convolutional network (ST-GCN) processes these graphs to
capture spatial and temporal dynamics. Experiments on the SAM-40 dataset show
that the ST-GCN outperforms standard machine learning models on all key
classification metrics and enhances interpretability, explored through ablation
analyses of key channels and brain regions. These results pave the way for more
objective and accurate stress detection methods.

</details>


### [305] [Temporal Chunking Enhances Recognition of Implicit Sequential Patterns](https://arxiv.org/abs/2506.00588)
*Jayanta Dey,Nicholas Soures,Miranda Gonzales,Itamar Lerner,Christopher Kanan,Dhireesha Kudithipudi*

Key words: 神经启发, 时序分块, 上下文标记, 学习效率, 迁移学习

TL;DR: 本文提出了一种神经启发的时序序列压缩方法，通过离线睡眠阶段生成上下文标记块，提高学习效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统神经网络在面临多时间尺度时序模式时存在局限性，希望通过时序分块提升资源受限下的学习效率。

Method: 设计合成环境评估时序分块方法，与传统RNN对比，并在小型人类试验中进一步验证。

Result: 初步结果显示，时序分块显著提升学习效率，且上下文标记块可跨任务迁移。

Conclusion: 该方法为早期概念验证，展现了在迁移学习中潜在的应用价值。

Abstract: In this pilot study, we propose a neuro-inspired approach that compresses
temporal sequences into context-tagged chunks, where each tag represents a
recurring structural unit or``community'' in the sequence. These tags are
generated during an offline sleep phase and serve as compact references to past
experience, allowing the learner to incorporate information beyond its
immediate input range. We evaluate this idea in a controlled synthetic
environment designed to reveal the limitations of traditional neural network
based sequence learners, such as recurrent neural networks (RNNs), when facing
temporal patterns on multiple timescales. We evaluate this idea in a controlled
synthetic environment designed to reveal the limitations of traditional neural
network based sequence learners, such as recurrent neural networks (RNNs), when
facing temporal patterns on multiple timescales. Our results, while
preliminary, suggest that temporal chunking can significantly enhance learning
efficiency under resource constrained settings. A small-scale human pilot study
using a Serial Reaction Time Task further motivates the idea of structural
abstraction. Although limited to synthetic tasks, this work serves as an early
proof-of-concept, with initial evidence that learned context tags can transfer
across related task, offering potential for future applications in transfer
learning.

</details>


### [306] [Mitigating Plasticity Loss in Continual Reinforcement Learning by Reducing Churn](https://arxiv.org/abs/2506.00592)
*Hongyao Tang,Johan Obando-Ceron,Pablo Samuel Castro,Aaron Courville,Glen Berseth*

Key words: 持续学习,塑性丧失,深度持续强化学习,churn,C-CHAIN

TL;DR: 论文研究了深度持续强化学习中的塑性丧失问题，提出了通过减少网络输出的变异性（churn）来改善塑性，并验证了C-CHAIN方法的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 塑性在持续学习中至关重要，但深度持续强化学习中塑性丧失的问题尚未得到充分研究。论文从churn的角度探讨了这一问题。

Method: 通过分析Neural Tangent Kernel (NTK)矩阵的秩逐渐降低现象，提出减少churn以维持塑性，并引入C-CHAIN方法。

Result: C-CHAIN在多个基准测试（如OpenAI Gym Control、ProcGen等）中表现优于基线方法，提高了学习性能。

Conclusion: 减少churn可以有效防止塑性丧失，C-CHAIN方法为持续学习提供了一种有效的解决方案。

Abstract: Plasticity, or the ability of an agent to adapt to new tasks, environments,
or distributions, is crucial for continual learning. In this paper, we study
the loss of plasticity in deep continual RL from the lens of churn: network
output variability for out-of-batch data induced by mini-batch training. We
demonstrate that (1) the loss of plasticity is accompanied by the exacerbation
of churn due to the gradual rank decrease of the Neural Tangent Kernel (NTK)
matrix; (2) reducing churn helps prevent rank collapse and adjusts the step
size of regular RL gradients adaptively. Moreover, we introduce Continual Churn
Approximated Reduction (C-CHAIN) and demonstrate it improves learning
performance and outperforms baselines in a diverse range of continual learning
environments on OpenAI Gym Control, ProcGen, DeepMind Control Suite, and
MinAtar benchmarks.

</details>


### [307] [Graph Evidential Learning for Anomaly Detection](https://arxiv.org/abs/2506.00594)
*Chunyu Wei,Wenji Hu,Xingjia Hao,Yunhai Wang,Yueguo Chen,Bing Bai,Fei Wang*

Key words: 图异常检测,证据学习,不确定性,鲁棒性

TL;DR: GEL是一个新的概率框架，通过证据学习改进图异常检测，提升了鲁棒性和性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决图异常检测中因缺乏标注数据和传统方法对噪声敏感的问题。

Method: 采用证据学习建模图的特征和拓扑结构，量化不确定性并纳入异常评分。

Result: 实验表明GEL在性能和鲁棒性上优于现有方法。

Conclusion: GEL为图异常检测提供了一个有效的概率框架。

Abstract: Graph anomaly detection faces significant challenges due to the scarcity of
reliable anomaly-labeled datasets, driving the development of unsupervised
methods. Graph autoencoders (GAEs) have emerged as a dominant approach by
reconstructing graph structures and node features while deriving anomaly scores
from reconstruction errors. However, relying solely on reconstruction error for
anomaly detection has limitations, as it increases the sensitivity to noise and
overfitting. To address these issues, we propose Graph Evidential Learning
(GEL), a probabilistic framework that redefines the reconstruction process
through evidential learning. By modeling node features and graph topology using
evidential distributions, GEL quantifies two types of uncertainty: graph
uncertainty and reconstruction uncertainty, incorporating them into the anomaly
scoring mechanism. Extensive experiments demonstrate that GEL achieves
state-of-the-art performance while maintaining high robustness against noise
and structural perturbations.

</details>


### [308] [Predictability-Aware Compression and Decompression Framework for Multichannel Time Series Data](https://arxiv.org/abs/2506.00614)
*Ziqi Liu,Pei Zeng,Yi Ding*

Key words: 时间序列预测, 通道压缩, 可预测性感知, MIMO, 正交性

TL;DR: 本文提出了一种可预测性感知的压缩-解压缩框架，用于多通道时间序列预测，旨在提高效率、降低通信成本，并保持预测精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着边缘和云环境对效率需求的增加，通道压缩成为一个重要问题。受多输入多输出（MIMO）方法的启发，作者提出了一种新框架。

Method: 利用具有正交性的周期性关键矩阵，在压缩阶段捕获时间序列的可预测性，并在解压缩阶段通过放松简化的数据假设减少重构误差。

Result: 理论和实证分析表明，该框架在大量通道下具有时间效率和可扩展性。六个数据集的实验显示，该方法在预测精度和运行时性能上均表现出色。

Conclusion: 该框架在保持与多样化预测器兼容性的同时，显著提升了多通道时间序列预测的整体性能。

Abstract: Real-world multichannel time series prediction faces growing demands for
efficiency across edge and cloud environments, making channel compression a
timely and essential problem. Motivated by success of Multiple-Input
Multiple-Output (MIMO) methods, we propose a predictability-aware
compression-decompression framework to reduce runtime, lower communication
cost, and maintain prediction accuracy across diverse predictors. The core idea
involves using a circular periodicity key matrix with orthogonality to capture
underlying time series predictability during compression and to mitigate
reconstruction errors during decompression by relaxing oversimplified data
assumptions. Theoretical and empirical analyses show that the proposed
framework is both time-efficient and scalable under a large number of channels.
Extensive experiments on six datasets across various predictors demonstrate
that the proposed method achieves superior overall performance by jointly
considering prediction accuracy and runtime, while maintaining strong
compatibility with diverse predictors.

</details>


### [309] [Model Reprogramming Demystified: A Neural Tangent Kernel Perspective](https://arxiv.org/abs/2506.00620)
*Ming-Yu Chung,Jiashuo Fan,Hancheng Ye,Qinsi Wang,Wei-Chen Shen,Chia-Mu Yu,Pin-Yu Chen,Sy-Yen Kuo*

Key words: 模型重编程, 神经切线核, 特征值谱, 源模型, 目标模型

TL;DR: 该论文通过NTK框架对模型重编程（MR）进行了理论分析，揭示了其成功的关键在于目标数据集上NTK矩阵的特征值谱，并验证了源模型效果对重编程结果的重大影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管模型重编程在多个领域取得了实证成功，但其理论基础尚未充分探索，研究者希望通过理论分析填补这一空白。

Method: 采用神经切线核（NTK）框架，分析MR的理论基础，并通过实验验证发现。

Result: MR的成功取决于目标数据集上NTK矩阵的特征值谱，且源模型的效果对重编程结果至关重要。

Conclusion: 研究为MR提供了理论支持，并揭示了源模型与目标模型之间的关系，为实际应用提供了指导。

Abstract: Model Reprogramming (MR) is a resource-efficient framework that adapts large
pre-trained models to new tasks with minimal additional parameters and data,
offering a promising solution to the challenges of training large models for
diverse tasks. Despite its empirical success across various domains such as
computer vision and time-series forecasting, the theoretical foundations of MR
remain underexplored. In this paper, we present a comprehensive theoretical
analysis of MR through the lens of the Neural Tangent Kernel (NTK) framework.
We demonstrate that the success of MR is governed by the eigenvalue spectrum of
the NTK matrix on the target dataset and establish the critical role of the
source model's effectiveness in determining reprogramming outcomes. Our
contributions include a novel theoretical framework for MR, insights into the
relationship between source and target models, and extensive experiments
validating our findings.

</details>


### [310] [Probabilistic Forecasting for Building Energy Systems using Time-Series Foundation Models](https://arxiv.org/abs/2506.00630)
*Young Jin Park,Francois Germain,Jing Liu,Ye Wang,Toshiaki Koike-Akino,Gordon Wichern,Navid Azizan,Christopher R. Laughman,Ankush Chakrabarty*

Key words: 时间序列基础模型、建筑能源预测、微调、LoRA、能源效率

TL;DR: 该论文探讨了时间序列基础模型（TSFMs）在建筑能源预测中的应用及微调策略，证明了微调能显著提升预测精度，且LoRA方法在降低计算成本的同时保持准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 建筑能源系统的决策依赖于时间序列模型的预测准确性，而在目标建筑数据不足的情况下，基础模型（FMs）可以利用预训练数据构建精确的概率预测器。

Method: 研究了全微调和参数高效微调（特别是低秩适应LoRA）方法，使用商业净零能耗建筑的真实数据进行分析。

Result: TSFMs的零样本预测表现不佳，但微调方法显著提高了准确性；LoRA方法在降低计算成本的同时保持性能，且微调后的TSFMs优于现有深度预测模型。

Conclusion: 微调的TSFMs在数据受限的建筑能源管理系统中表现出色，为能源效率和可持续性决策提供了有效工具。

Abstract: Decision-making in building energy systems critically depends on the
predictive accuracy of relevant time-series models. In scenarios lacking
extensive data from a target building, foundation models (FMs) represent a
promising technology that can leverage prior knowledge from vast and diverse
pre-training datasets to construct accurate probabilistic predictors for use in
decision-making tools. This paper investigates the applicability and
fine-tuning strategies of time-series foundation models (TSFMs) in building
energy forecasting. We analyze both full fine-tuning and parameter-efficient
fine-tuning approaches, particularly low-rank adaptation (LoRA), by using
real-world data from a commercial net-zero energy building to capture signals
such as room occupancy, carbon emissions, plug loads, and HVAC energy
consumption. Our analysis reveals that the zero-shot predictive performance of
TSFMs is generally suboptimal. To address this shortcoming, we demonstrate that
employing either full fine-tuning or parameter-efficient fine-tuning
significantly enhances forecasting accuracy, even with limited historical data.
Notably, fine-tuning with low-rank adaptation (LoRA) substantially reduces
computational costs without sacrificing accuracy. Furthermore, fine-tuned TSFMs
consistently outperform state-of-the-art deep forecasting models (e.g.,
temporal fusion transformers) in accuracy, robustness, and generalization
across varying building zones and seasonal conditions. These results underline
the efficacy of TSFMs for practical, data-constrained building energy
management systems, enabling improved decision-making in pursuit of energy
efficiency and sustainability.

</details>


### [311] [Learning with Calibration: Exploring Test-Time Computing of Spatio-Temporal Forecasting](https://arxiv.org/abs/2506.00635)
*Wei Chen,Yuxuan Liang*

Key words: 时空预测、测试时计算、校准学习、频谱域校准、闪速更新

TL;DR: 提出了一种基于校准学习的测试时计算范式ST-TTC，用于时空预测，通过实时偏差修正提高准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决时空预测中信号异常、噪声和分布偏移等挑战，避免传统方法的高计算和资源需求。

Method: 引入频谱域校准器进行周期性偏移修正，并采用闪速更新机制和流式内存队列优化测试时计算。

Result: 实验证明ST-TTC在真实数据集上具有高效性、通用性、灵活性和有效性。

Conclusion: ST-TTC提供了一种无需复杂训练的高效通用时空预测范式。

Abstract: Spatio-temporal forecasting is crucial in many domains, such as
transportation, meteorology, and energy. However, real-world scenarios
frequently present challenges such as signal anomalies, noise, and
distributional shifts. Existing solutions primarily enhance robustness by
modifying network architectures or training procedures. Nevertheless, these
approaches are computationally intensive and resource-demanding, especially for
large-scale applications. In this paper, we explore a novel test-time computing
paradigm, namely learning with calibration, ST-TTC, for spatio-temporal
forecasting. Through learning with calibration, we aim to capture periodic
structural biases arising from non-stationarity during the testing phase and
perform real-time bias correction on predictions to improve accuracy.
Specifically, we first introduce a spectral-domain calibrator with
phase-amplitude modulation to mitigate periodic shift and then propose a flash
updating mechanism with a streaming memory queue for efficient test-time
computation. ST-TTC effectively bypasses complex training-stage techniques,
offering an efficient and generalizable paradigm. Extensive experiments on
real-world datasets demonstrate the effectiveness, universality, flexibility
and efficiency of our proposed method.

</details>


### [312] [Rethinking Neural-based Matrix Inversion: Why can't, and Where can](https://arxiv.org/abs/2506.00642)
*Yuliang Ji,Jian Wu,Yuanzhe Xi*

Key words: 神经网络, 矩阵求逆, Lipschitz函数, 科学计算

TL;DR: 本文探讨了神经网络在近似矩阵求逆中的理论基础和局限性，并提出了一些特定条件下可行的近似方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 科学计算中矩阵求逆的快速并行近似是一个关键问题，但目前缺乏通用的神经网络方法。

Method: 通过扩展Lipschitz函数类来涵盖更多神经网络模型，并分析特定条件下的近似能力。

Result: 理论分析和实验结果表明，神经网络在特定条件下可以有效近似矩阵求逆。

Conclusion: 神经网络在矩阵求逆中具有潜力，但需在特定条件下实现。

Abstract: Deep neural networks have achieved substantial success across various
scientific computing tasks. A pivotal challenge within this domain is the rapid
and parallel approximation of matrix inverses, critical for numerous
applications. Despite significant progress, there currently exists no universal
neural-based method for approximating matrix inversion. This paper presents a
theoretical analysis demonstrating the fundamental limitations of neural
networks in developing a general matrix inversion model. We expand the class of
Lipschitz functions to encompass a wider array of neural network models,
thereby refining our theoretical approach. Moreover, we delineate specific
conditions under which neural networks can effectively approximate matrix
inverses. Our theoretical results are supported by experimental results from
diverse matrix datasets, exploring the efficacy of neural networks in
addressing the matrix inversion challenge.

</details>


### [313] [Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models](https://arxiv.org/abs/2506.00653)
*Femi Bello,Anubrata Das,Fanzhi Zeng,Fangcong Yin,Leqi Liu*

Key words: 神经网络,表示学习,线性变换,模型迁移,LRT假设

TL;DR: 该论文假设不同规模的神经网络在相似数据上训练后，其表示空间可以通过线性变换对齐。通过实验验证了这一假设，表明小模型的表示可以指导大模型的行为。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探究不同规模模型在相似数据上是否学习到共享的表示，并验证这些表示是否可以通过线性变换在不同模型间迁移。

Method: 提出线性表示可迁移性（LRT）假设，并通过学习不同规模模型隐藏状态间的仿射映射，验证迁移后行为保持一致性。

Result: 实验结果表明，通过仿射映射可以保留小模型到大模型的行为指导能力，支持LRT假设。

Conclusion: 小模型的表示可以用于指导大模型，LRT假设为跨模型规模表示对齐提供了新的研究方向。

Abstract: It has been hypothesized that neural networks with similar architectures
trained on similar data learn shared representations relevant to the learning
task. We build on this idea by extending the conceptual framework where
representations learned across models trained on the same data can be expressed
as linear combinations of a \emph{universal} set of basis features. These basis
features underlie the learning task itself and remain consistent across models,
regardless of scale. From this framework, we propose the \textbf{Linear
Representation Transferability (LRT)} Hypothesis -- that there exists an affine
transformation between the representation spaces of different models. To test
this hypothesis, we learn affine mappings between the hidden states of models
of different sizes and evaluate whether steering vectors -- directions in
hidden state space associated with specific model behaviors -- retain their
semantic effect when transferred from small to large language models using the
learned mappings. We find strong empirical evidence that such affine mappings
can preserve steering behaviors. These findings suggest that representations
learned by small models can be used to guide the behavior of large models, and
that the LRT hypothesis may be a promising direction on understanding
representation alignment across model scales.

</details>


### [314] [Permutation-Invariant Transformer Neural Architectures for Set-Based Indoor Localization Using Learned RSSI Embeddings](https://arxiv.org/abs/2506.00656)
*Aris J. Aristorenas*

Key words: 室内定位, Wi-Fi RSSI, Set Transformer, 排列不变性, 信号处理

TL;DR: 提出了一种用于Wi-Fi信号定位的排列不变神经网络架构，通过Set Transformer处理无序信号输入，在多建筑和多楼层任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决Wi-Fi信号定位中信号稀疏、无序且长度可变的问题，提出了一种基于集合的神经网络模型。

Method: 使用Set Transformer处理(BSSID, RSSI)对的无序集合，学习信号间的注意力关系。

Result: 模型在校园环境中表现良好，LSTM表现最佳，Set Transformer次之，优于MLP、RNN和基础注意力模型。

Conclusion: 集合神经网络模型适用于信号定位任务，能有效处理稀疏无序输入。

Abstract: We propose a permutation-invariant neural architecture for indoor
localization using RSSI scans from Wi-Fi access points. Each scan is modeled as
an unordered set of (BSSID, RSSI) pairs, where BSSIDs are mapped to learned
embeddings and concatenated with signal strength. These are processed by a Set
Transformer, enabling the model to handle variable-length, sparse inputs while
learning attention-based representations over access point relationships. We
evaluate the model on a dataset collected across a campus environment
consisting of six buildings. Results show that the model accurately recovers
fine-grained spatial structure and maintains performance across physically
distinct domains. In our experiments, a simple LSTM consistently outperformed
all other models, achieving the lowest mean localization error across three
tasks (E1 - E3), with average errors as low as 2.23 m. The Set Transformer
performed competitively, ranking second in every experiment and outperforming
the MLP, RNN, and basic attention models, particularly in scenarios involving
multiple buildings (E2) and multiple floors (E3). Performance degraded most in
E2, where signal conditions varied substantially across buildings, highlighting
the importance of architectural robustness to domain diversity. This work
demonstrates that set-based neural models are a natural fit for signal-based
localization, offering a principled approach to handling sparse, unordered
inputs in real-world positioning tasks.

</details>


### [315] [Differential Privacy for Deep Learning in Medicine](https://arxiv.org/abs/2506.00660)
*Marziyeh Mohammadi,Mohsen Vejdanihemmat,Mahshad Lotfinia,Mirabela Rusu,Daniel Truhn,Andreas Maier,Soroosh Tayebi Arasteh*

Key words: 差分隐私, 医学深度学习, 隐私-效用权衡, 公平性, 联邦学习

TL;DR: 本文综述了差分隐私（DP）在医学深度学习（DL）中的应用，重点关注DP-SGD及其替代机制，分析隐私保护与模型性能和公平性之间的权衡。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着医学深度学习对数据的依赖性增强，如何在保护患者隐私的同时确保模型的实用性和公平性成为关键挑战。

Method: 通过结构化搜索策略，分析了74项研究，涵盖不同数据模式、训练设置和下游任务，探讨DP-SGD及其他机制在集中式和联邦式设置中的应用。

Result: 研究发现，强隐私预算下的DP在结构化成像任务中性能尚可，但严格隐私下性能下降明显，且对特定人口亚组公平性影响显著。少数研究通过亚组分析或公平性指标处理这些权衡，但多数研究未涉及。

Conclusion: 未来研究需要在公平性审计、标准化和评估协议方面填补空白，以实现医学中公平且稳健的隐私保护深度学习系统。

Abstract: Differential privacy (DP) is a key technique for protecting sensitive patient
data in medical deep learning (DL). As clinical models grow more
data-dependent, balancing privacy with utility and fairness has become a
critical challenge. This scoping review synthesizes recent developments in
applying DP to medical DL, with a particular focus on DP-SGD and alternative
mechanisms across centralized and federated settings. Using a structured search
strategy, we identified 74 studies published up to March 2025. Our analysis
spans diverse data modalities, training setups, and downstream tasks, and
highlights the tradeoffs between privacy guarantees, model accuracy, and
subgroup fairness. We find that while DP-especially at strong privacy
budgets-can preserve performance in well-structured imaging tasks, severe
degradation often occurs under strict privacy, particularly in underrepresented
or complex modalities. Furthermore, privacy-induced performance gaps
disproportionately affect demographic subgroups, with fairness impacts varying
by data type and task. A small subset of studies explicitly addresses these
tradeoffs through subgroup analysis or fairness metrics, but most omit them
entirely. Beyond DP-SGD, emerging approaches leverage alternative mechanisms,
generative models, and hybrid federated designs, though reporting remains
inconsistent. We conclude by outlining key gaps in fairness auditing,
standardization, and evaluation protocols, offering guidance for future work
toward equitable and clinically robust privacy-preserving DL systems in
medicine.

</details>


### [316] [SafeTuneBed: A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning](https://arxiv.org/abs/2506.00676)
*Saad Hossain,Samanvay Vajpayee,Sirisha Rambhatla*

Key words: 大型语言模型, 微调, 安全防御, 基准测试, 可复现性

TL;DR: SafeTuneBed是一个用于统一评估大型语言模型（LLM）微调和防御方法的基准与工具包，旨在解决当前研究中数据、指标和威胁设置不一致的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于当前LLM微调和安全防御方法的多样性和评估不一致性，难以公平比较不同方法的安全性、实用性和鲁棒性，因此需要统一的评估框架。

Method: SafeTuneBed通过（i）整合多样化的微调数据集，（ii）支持集成先进的防御方法，（iii）提供安全性和实用性的评估指标，采用Python驱动的配置和插件设计，确保可复现性。

Result: 通过评估代表性防御方法在不同中毒场景和任务中的表现，展示了SafeTuneBed的价值。

Conclusion: SafeTuneBed是首个专注于标准化数据、代码和指标的工具包，为安全LLM微调研究提供了严谨且可比较的评估框架。

Abstract: As large language models (LLMs) become ubiquitous, parameter-efficient
fine-tuning methods and safety-first defenses have proliferated rapidly.
However, the number of approaches and their recent increase have resulted in
diverse evaluations-varied datasets, metrics, and inconsistent threat
settings-making it difficult to fairly compare safety, utility, and robustness
across methods. To address this, we introduce SafeTuneBed, a benchmark and
toolkit unifying fine-tuning and defense evaluation. SafeTuneBed (i) curates a
diverse repository of multiple fine-tuning datasets spanning sentiment
analysis, question-answering, multi-step reasoning, and open-ended instruction
tasks, and allows for the generation of harmful-variant splits; (ii) enables
integration of state-of-the-art defenses, including alignment-stage
immunization, in-training safeguards, and post-tuning repair; and (iii)
provides evaluators for safety (attack success rate, refusal consistency) and
utility. Built on Python-first, dataclass-driven configs and plugins,
SafeTuneBed requires minimal additional code to specify any fine-tuning regime,
defense method, and metric suite, while ensuring end-to-end reproducibility. We
showcase its value by benchmarking representative defenses across varied
poisoning scenarios and tasks. By standardizing data, code, and metrics,
SafeTuneBed is the first focused toolkit of its kind to accelerate rigorous and
comparable research in safe LLM fine-tuning. Code is available at:
https://github.com/criticalml-uw/SafeTuneBed

</details>


### [317] [Existing Large Language Model Unlearning Evaluations Are Inconclusive](https://arxiv.org/abs/2506.00688)
*Zhili Feng,Yixuan Even Xu,Alexander Robey,Robert Kirk,Xander Davies,Yarin Gal,Avi Schwarzschild,J. Zico Kolter*

Key words: 机器去学习, 评估方法, 信息注入, 任务意识, 实验验证

TL;DR: 论文指出了当前机器学习去学习评估方法的局限性，并提出了两项改进原则，强调最小信息注入和任务意识。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究动机是质疑当前去学习评估方法的有效性，发现其存在的问题（如信息注入、任务差异和虚假相关性），并提出改进方案。

Method: 通过分析现有评估方法的局限性，提出了两项新原则：最小信息注入和下游任务意识，并通过实验验证其有效性。

Result: 研究表明现有评估方法可能夸大或低估去学习效果，新原则能更准确地评估去学习性能。

Conclusion: 结论强调需要改进去学习评估方法，采用新原则以提高结果的可靠性和解释性。

Abstract: Machine unlearning aims to remove sensitive or undesired data from large
language models. However, recent studies suggest that unlearning is often
shallow, claiming that removed knowledge can easily be recovered. In this work,
we critically examine standard unlearning evaluation practices and uncover key
limitations that shake our trust in those findings. First, we show that some
evaluations introduce substantial new information into the model, potentially
masking true unlearning performance by re-teaching the model during testing.
Second, we demonstrate that evaluation outcomes vary significantly across
tasks, undermining the generalizability of current evaluation routines.
Finally, we find that many evaluations rely on spurious correlations, making
their results difficult to trust and interpret. Taken together, these issues
suggest that current evaluation protocols may both overstate and understate
unlearning success. To address this, we propose two principles for future
unlearning evaluations: minimal information injection and downstream task
awareness. We validate these principles through a series of targeted
experiments, showing how violations of each can lead to misleading conclusions.

</details>


### [318] [Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning](https://arxiv.org/abs/2506.00691)
*Junaid Muzaffar,Ahsan Adeel,Khubaib Ahmed,Ingo Frommholz,Zeeshan Pervez,Ahsan ul Haq*

Key words: 强化学习、注意力机制、非线性变换、学习效率

TL;DR: 论文提出一种改进的注意力机制，通过非线性变换增强键向量的表示能力，从而提升强化学习模型的学习效率和收敛速度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决强化学习模型训练中计算资源消耗大、训练时间长的问题。

Method: 提出一种改进的注意力机制，对键向量进行非线性变换，获得更具表现力的键向量，增强模型的表示能力。

Result: 改进后的模型在学习效率和收敛速度上均有显著提升。

Conclusion: 非线性注意力机制在强化学习算法中具有提升效率和性能的潜力。

Abstract: Training reinforcement learning (RL) agents often requires significant
computational resources and extended training times. To address this, we build
upon the foundation laid by Google Brain's Sensory Neuron, which introduced a
novel neural architecture for reinforcement learning tasks that maintained
permutation in-variance in the sensory neuron system. While the baseline model
demonstrated significant performance improvements over traditional approaches,
we identified opportunities to enhance the efficiency of the learning process
further. We propose a modified attention mechanism incorporating a non-linear
transformation of the key vectors (K) using a mapping function, resulting in a
new set of key vectors (K'). This non-linear mapping enhances the
representational capacity of the attention mechanism, allowing the model to
encode more complex feature interactions and accelerating convergence without
compromising performance. Our enhanced model demonstrates significant
improvements in learning efficiency, showcasing the potential for non-linear
attention mechanisms in advancing reinforcement learning algorithms.

</details>


### [319] [Central Path Proximal Policy Optimization](https://arxiv.org/abs/2506.00700)
*Nikola Milosevic,Johannes Müller,Nico Scherf*

Key words: 约束马尔可夫决策过程, 中心路径, PPO, 策略优化

TL;DR: C3PO 是一种改进的 PPO 方法，通过将约束直接融入策略几何，保持接近中心路径的更新，从而提高性能并更严格地满足约束条件。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在约束马尔可夫决策过程中，传统方法在训练过程中强制执行约束可能会降低最终回报。最近的研究表明，将约束直接融入策略几何可以避免这一问题。

Method: 提出了 Central Path Proximal Policy Optimization (C3PO)，通过对 PPO 进行简单修改，生成接近约束优化问题中心路径的策略迭代。

Result: 与现有策略方法相比，C3PO 提供了更高的性能表现和更严格的约束执行。

Conclusion: 中心路径引导的策略更新为约束策略优化提供了有前景的方向。

Abstract: In constrained Markov decision processes, enforcing constraints during
training is often thought of as decreasing the final return. Recently, it was
shown that constraints can be incorporated directly in the policy geometry,
yielding an optimization trajectory close to the central path of a barrier
method, which does not compromise final return. Building on this idea, we
introduce Central Path Proximal Policy Optimization (C3PO), a simple
modification of PPO that produces policy iterates, which stay close to the
central path of the constrained optimization problem. Compared to existing
on-policy methods, C3PO delivers improved performance with tighter constraint
enforcement, suggesting that central path-guided updates offer a promising
direction for constrained policy optimization.

</details>


### [320] [Bayesian Inference of Training Dataset Membership](https://arxiv.org/abs/2506.00701)
*Yongchao Huang*

Key words: 成员推理攻击, 贝叶斯推理, 隐私保护, 机器学习安全

TL;DR: 提出了一种高效的、可解释的贝叶斯推理方法，用于判断数据集是否为机器学习模型的训练数据成员，解决了传统成员推理攻击（MIAs）的复杂性问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统MIAs需要访问模型内部或依赖计算密集型的影子模型，该方法旨在提供一种更高效、可解释的解决方案。

Method: 通过分析训练后模型的预测误差、置信度（熵）、扰动幅度和数据集统计等指标，利用贝叶斯推理计算成员的后验概率。

Result: 在合成数据集上的实验表明，该方法能有效区分成员和非成员数据集。

Conclusion: 该方法不仅适用于成员推理，还能检测分布变化，为现有方法提供了实用的替代方案。

Abstract: Determining whether a dataset was part of a machine learning model's training
data pool can reveal privacy vulnerabilities, a challenge often addressed
through membership inference attacks (MIAs). Traditional MIAs typically require
access to model internals or rely on computationally intensive shadow models.
This paper proposes an efficient, interpretable and principled Bayesian
inference method for membership inference. By analyzing post-hoc metrics such
as prediction error, confidence (entropy), perturbation magnitude, and dataset
statistics from a trained ML model, our approach computes posterior
probabilities of membership without requiring extensive model training.
Experimental results on synthetic datasets demonstrate the method's
effectiveness in distinguishing member from non-member datasets. Beyond
membership inference, this method can also detect distribution shifts, offering
a practical and interpretable alternative to existing approaches.

</details>


### [321] [RelDiff: Relational Data Generative Modeling with Graph-Based Diffusion Models](https://arxiv.org/abs/2506.00710)
*Valter Hudovernik,Minkai Xu,Juntong Shi,Lovro Šubelj,Stefano Ermon,Erik Štrumbelj,Jure Leskovec*

Key words: 生成模型, 扩散过程, 关联数据库, 外键图结构, 随机块模型

TL;DR: RelDiff是一种新型的扩散生成模型，能够合成完整的关联数据库，通过显式建模外键图结构，同时确保高保真和引用完整性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的生成模型在处理关联数据时通常简化其复杂性，无法有效捕捉多表格间的结构依赖关系。因此，作者提出RelDiff以解决这一问题。

Method: RelDiff采用联合图条件扩散过程进行属性合成，并使用基于随机块模型的$2K+SBM$图生成器进行结构生成。

Result: 在11个基准数据集上的实验表明，RelDiff在生成真实且连贯的关联数据库时优于现有方法。

Conclusion: RelDiff成功解决了关联数据库生成中的结构复杂性问题，实现了高保真和引用完整性。

Abstract: Real-world databases are predominantly relational, comprising multiple
interlinked tables that contain complex structural and statistical
dependencies. Learning generative models on relational data has shown great
promise in generating synthetic data and imputing missing values. However,
existing methods often struggle to capture this complexity, typically reducing
relational data to conditionally generated flat tables and imposing limiting
structural assumptions. To address these limitations, we introduce RelDiff, a
novel diffusion generative model that synthesizes complete relational databases
by explicitly modeling their foreign key graph structure. RelDiff combines a
joint graph-conditioned diffusion process across all tables for attribute
synthesis, and a $2K+$SBM graph generator based on the Stochastic Block Model
for structure generation. The decomposition of graph structure and relational
attributes ensures both high fidelity and referential integrity, both of which
are crucial aspects of synthetic relational database generation. Experiments on
11 benchmark datasets demonstrate that RelDiff consistently outperforms prior
methods in producing realistic and coherent synthetic relational databases.
Code is available at https://github.com/ValterH/RelDiff.

</details>


### [322] [QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training](https://arxiv.org/abs/2506.00711)
*Wei Dai,Peilin Chen,Chanakya Ekbote,Paul Pu Liang*

Key words: QoQ-Med, DRPO, 临床基础模型, 多模态, 医疗诊断

TL;DR: QoQ-Med-7B/32B是首个开放的通用临床基础模型，通过DRPO训练方法联合分析医学图像、时间序列信号和文本报告，显著提升诊断性能并解决数据分布不均问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的多模态语言模型主要基于视觉，无法跨临床专业泛化，需要开发一种能综合分析多种临床数据的模型。

Method: 提出了QoQ-Med模型，采用域感知相对策略优化（DRPO）训练方法，根据域稀有性和模态难度分层缩放奖励。

Result: DRPO训练将可视化领域的诊断性能平均提升43%（宏F1），并在分割任务中表现优于开放模型，达到OpenAI o4-mini水平。

Conclusion: QoQ-Med通过DRPO训练解决了临床数据分布不均问题，显著提升诊断性能，推动了可复现性和下游研究。

Abstract: Clinical decision-making routinely demands reasoning over heterogeneous data,
yet existing multimodal language models (MLLMs) remain largely vision-centric
and fail to generalize across clinical specialties. To bridge this gap, we
introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model
that jointly reasons across medical images, time-series signals, and text
reports. QoQ-Med is trained with Domain-aware Relative Policy Optimization
(DRPO), a novel reinforcement-learning objective that hierarchically scales
normalized rewards according to domain rarity and modality difficulty,
mitigating performance imbalance caused by skewed clinical data distributions.
Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains,
we show that DRPO training boosts diagnostic performance by 43% in macro-F1 on
average across all visual domains as compared to other critic-free training
methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation
data, it is able to highlight salient regions related to the diagnosis, with an
IoU 10x higher than open models while reaching the performance of OpenAI
o4-mini. To foster reproducibility and downstream research, we release (i) the
full model weights, (ii) the modular training pipeline, and (iii) all
intermediate reasoning traces at https://github.com/DDVD233/QoQ_Med.

</details>


### [323] [Pitfalls in Evaluating Language Model Forecasters](https://arxiv.org/abs/2506.00723)
*Daniel Paleka,Shashwat Goel,Jonas Geiping,Florian Tramèr*

Key words: 大型语言模型,预测任务,时间泄漏,评估方法,性能外推

TL;DR: 论文指出评估大型语言模型（LLMs）在预测任务中的性能存在挑战，提出了时间泄漏和实际预测能力评估困难两大问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 质疑当前对LLMs在预测任务中表现优于人类的研究结论，强调需要更严谨的评估方法。

Method: 通过系统分析和具体案例，揭露评估中的缺陷，尤其是时间泄漏和性能外推问题。

Result: 研究发现当前评估方法存在严重缺陷，可能导致对LLM预测能力的误判。

Conclusion: 呼吁采用更严格的评估方法以准确评估LLMs的预测能力。

Abstract: Large language models (LLMs) have recently been applied to forecasting tasks,
with some works claiming these systems match or exceed human performance. In
this paper, we argue that, as a community, we should be careful about such
conclusions as evaluating LLM forecasters presents unique challenges. We
identify two broad categories of issues: (1) difficulty in trusting evaluation
results due to many forms of temporal leakage, and (2) difficulty in
extrapolating from evaluation performance to real-world forecasting. Through
systematic analysis and concrete examples from prior work, we demonstrate how
evaluation flaws can raise concerns about current and future performance
claims. We argue that more rigorous evaluation methodologies are needed to
confidently assess the forecasting abilities of LLMs.

</details>


### [324] [A condensing approach to multiple shooting neural ordinary differential equation](https://arxiv.org/abs/2506.00724)
*Siddharth Prabhu,Srinivas Rangarajan,Mayuresh Kothare*

Key words: 多重射击, 神经常微分方程, 压缩方法, 优化

TL;DR: 本文介绍了多重射击方法在神经常微分方程中的应用，提出了一种基于压缩的策略来解决约束问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多重射击方法在传统参数估计中表现稳定，但在神经常微分方程中应用较少，主要由于难以处理一般性约束条件。

Method: 提出了一种基于压缩的方法，结合多重射击神经常微分方程（MS-NODE），并使用一阶优化方法（如Adam）进行训练。

Result: 该方法能够有效处理多重射击中的约束条件，提高了训练的稳定性。

Conclusion: 压缩方法为多重射击神经常微分方程的训练提供了可行的解决方案。

Abstract: Multiple-shooting is a parameter estimation approach for ordinary
differential equations. In this approach, the trajectory is broken into small
intervals, each of which can be integrated independently. Equality constraints
are then applied to eliminate the shooting gap between the end of the previous
trajectory and the start of the next trajectory. Unlike single-shooting,
multiple-shooting is more stable, especially for highly oscillatory and long
trajectories. In the context of neural ordinary differential equations,
multiple-shooting is not widely used due to the challenge of incorporating
general equality constraints. In this work, we propose a condensing-based
approach to incorporate these shooting equality constraints while training a
multiple-shooting neural ordinary differential equation (MS-NODE) using
first-order optimization methods such as Adam.

</details>


### [325] [Adaptive Plane Reformatting for 4D Flow MRI using Deep Reinforcement Learning](https://arxiv.org/abs/2506.00727)
*Javier Bisbal,Julio Sotelo,Maria I Valdés,Pablo Irarrazaval,Marcelo E Andia,Julio García,José Rodriguez-Palomarez,Francesca Raimondi,Cristián Tejos,Sergio Uribe*

Key words: 深度强化学习, 平面重新格式化, A3C算法, 4D流MRI, 灵活坐标系

TL;DR: 该论文提出了一种基于灵活坐标系的新技术，解决了当前深度强化学习（DRL）方法在平面重新格式化任务中需要测试数据与训练数据位置和方向一致的问题。采用A3C算法，性能优于DQN，实验结果在4D流MRI中表现更佳。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前DRL方法在平面重新格式化任务中需要测试数据与训练数据的位置和方向一致，限制了其灵活性和适应性。本文旨在解决这一问题。

Method: 提出了一种基于灵活坐标系的新技术，采用A3C算法进行强化学习。

Result: 在4D流MRI实验中，该方法在角度和距离误差（6.32±4.15°和3.40±2.75毫米）上表现更优，且流量测量结果与专家操作无统计学差异（p=0.21）。

Conclusion: 该方法的灵活性和适应性使其不仅适用于4D流MRI，还可推广到其他医学成像应用。

Abstract: Deep reinforcement learning (DRL) algorithms have shown robust results in
plane reformatting tasks. In these methods, an agent sequentially adjusts the
position and orientation of an initial plane towards an objective location.
This process allows accurate plane reformatting, without the need for detailed
landmarks, which makes it suitable for images with limited contrast and
resolution, such as 4D flow MRI. However, current DRL methods require the test
dataset to be in the same position and orientation as the training dataset. In
this paper, we present a novel technique that utilizes a flexible coordinate
system based on the current state, enabling navigation in volumes at any
position or orientation. We adopted the Asynchronous Advantage Actor Critic
(A3C) algorithm for reinforcement learning, outperforming Deep Q Network (DQN).
Experimental results in 4D flow MRI demonstrate improved accuracy in plane
reformatting angular and distance errors (6.32 +- 4.15 {\deg} and 3.40 +- 2.75
mm), as well as statistically equivalent flow measurements determined by a
plane reformatting process done by an expert (p=0.21). The method's flexibility
and adaptability make it a promising candidate for other medical imaging
applications beyond 4D flow MRI.

</details>


### [326] [MoPINNEnKF: Iterative Model Inference using generic-PINN-based ensemble Kalman filter](https://arxiv.org/abs/2506.00731)
*Binghang Lu,Changhong Mou,Guang Lin*

Key words: PINNs, NSGA-III, EnKF, multi-objective optimization, PDEs, inverse problems

TL;DR: 提出了一个迭代多目标PINN集成卡尔曼滤波框架（MoPINNEnKF），通过结合NSGA-III和EnKF提升PINNs在噪声数据和不完整物理模型下的鲁棒性和准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: PINNs在解决涉及偏微分方程的正问题和反问题时因噪声数据和缺失物理模型而性能受限，需要改进其鲁棒性和准确性。

Method: 利用NSGA-III作为多目标优化器生成PINN的集成成员，再通过EnKF同化噪声数据并更新PINN参数，迭代优化解。

Result: 在Burgers方程和TFMDWE等基准问题上，MoPINNEnKF优于标准PINNs，尤其在噪声数据和物理模型缺失情况下表现更优。

Conclusion: MoPINNEnKF框架显著提升了PINNs处理复杂实际问题的能力，为噪声数据和不完整物理模型的场景提供了有效解决方案。

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful tool for
solving forward and inverse problems involving partial differential equations
(PDEs) by incorporating physical laws into the training process. However, the
performance of PINNs is often hindered in real-world scenarios involving noisy
observational data and missing physics, particularly in inverse problems. In
this work, we propose an iterative multi-objective PINN ensemble Kalman filter
(MoPINNEnKF) framework that improves the robustness and accuracy of PINNs in
both forward and inverse problems by using the \textit{ensemble Kalman filter}
and the \textit{non-dominated sorting genetic algorithm} III (NSGA-III).
Specifically, NSGA-III is used as a multi-objective optimizer that can generate
various ensemble members of PINNs along the optimal Pareto front, while
accounting the model uncertainty in the solution space. These ensemble members
are then utilized within the EnKF to assimilate noisy observational data. The
EnKF's analysis is subsequently used to refine the data loss component for
retraining the PINNs, thereby iteratively updating their parameters. The
iterative procedure generates improved solutions to the PDEs. The proposed
method is tested on two benchmark problems: the one-dimensional viscous Burgers
equation and the time-fractional mixed diffusion-wave equation (TFMDWE). The
numerical results show it outperforms standard PINNs in handling noisy data and
missing physics.

</details>


### [327] [Bregman Conditional Random Fields: Sequence Labeling with Parallelizable Inference Algorithms](https://arxiv.org/abs/2506.00732)
*Caio Corro,Mathieu Lacroix,Joseph Le Roux*

Key words: BCRF, 条件随机场, Bregman投影, 并行推断, 部分标签学习

TL;DR: 提出了一种新型的序列标记判别模型BCRF，通过Bregman投影实现快速并行推断，性能与CRF相当但更快，在约束条件下优于平均场方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决传统线性链条件随机场（CRF）推断速度慢的问题，提出一种更高效的并行化推断方法。

Method: 基于Bregman投影的快速并行推断算法，采用Fenchel-Young损失进行模型训练，支持部分标签学习。

Result: 实验表明BCRF性能与CRF相当但速度更快，在约束条件下优于平均场方法。

Conclusion: BCRF是一种高效且性能优越的序列标记模型，适合并行化计算和部分标签学习场景。

Abstract: We propose a novel discriminative model for sequence labeling called Bregman
conditional random fields (BCRF). Contrary to standard linear-chain conditional
random fields, BCRF allows fast parallelizable inference algorithms based on
iterative Bregman projections. We show how such models can be learned using
Fenchel-Young losses, including extension for learning from partial labels.
Experimentally, our approach delivers comparable results to CRF while being
faster, and achieves better results in highly constrained settings compared to
mean field, another parallelizable alternative.

</details>


### [328] [Blending Complementary Memory Systems in Hybrid Quadratic-Linear Transformers](https://arxiv.org/abs/2506.00744)
*Kazuki Irie,Morris Yau,Samuel J. Gershman*

Key words: 混合记忆架构, KV记忆, FW记忆, 语言建模, 强化学习

TL;DR: 本文提出了一种混合记忆架构，结合KV记忆和FW记忆的优势，通过三种方法实现互补，并在语言建模、检索任务和强化学习等实验中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: KV记忆精确检索但复杂度高，FW记忆支持长序列但牺牲精确性，两者互补但各有局限，因此需要一种混合架构来结合其优势。

Method: 提出并比较了三种混合KV记忆和FW记忆的方法，分别用于语言建模、检索任务和合成算法任务。

Result: 通过训练3.4亿和13亿参数模型及合成任务实验，验证了混合架构在克服单一组件局限性方面的有效性。

Conclusion: 设计的混合记忆系统能够克服单一组件的局限，为神经记忆系统的设计提供了新见解。

Abstract: We develop hybrid memory architectures for general-purpose sequence
processing neural networks, that combine key-value memory using softmax
attention (KV-memory) with dynamic synaptic memory through fast-weight
programming (FW-memory) -- the core principles of quadratic and linear
transformers, respectively. These two memory systems have complementary but
individually limited properties: KV-memory offers precise retrieval but is
constrained by quadratic complexity in sequence length, while FW-memory
supports arbitrarily long sequences and enables more expressive computation but
sacrifices precise recall. We propose and compare three methods to blend these
two systems into a single memory system to leverage the strengths of both. We
conduct experiments on general language modeling and retrieval tasks by
training 340M- and 1.3B-parameter models from scratch, as well as on synthetic
algorithmic tasks designed to precisely illustrate the benefits of certain
hybrid methods over others. We also evaluate our hybrid memory systems on
reinforcement learning in partially observable environments. Overall, we
demonstrate how a well-designed hybrid can overcome the limitations of its
individual components, offering new insights into the design principle of
neural memory systems.

</details>


### [329] ["Who experiences large model decay and why?" A Hierarchical Framework for Diagnosing Heterogeneous Performance Drift](https://arxiv.org/abs/2506.00756)
*Harvineet Singh,Fan Xia,Alexej Gossmann,Andrew Chuang,Julian C. Hong,Jean Feng*

Key words: 机器学习,性能下降,子群分析,SHIFT框架

TL;DR: SHIFT框架解决了机器学习模型在新环境中性能下降的不均匀问题，通过识别受影响的子群并提供针对性措施。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 机器学习模型在新环境中的性能下降往往不均匀，现有方法无法提供详细的子群性能下降原因和解决方案。

Method: 提出SHIFT框架，首先识别性能下降严重的子群，然后深入分析原因，并提供针对性措施。

Result: 实验表明，SHIFT能识别可解释的子群并有效缓解性能下降。

Conclusion: SHIFT通过分层推断框架，为不均匀性能下降提供了解释和解决方案。

Abstract: Machine learning (ML) models frequently experience performance degradation
when deployed in new contexts. Such degradation is rarely uniform: some
subgroups may suffer large performance decay while others may not.
Understanding where and how large differences in performance arise is critical
for designing targeted corrective actions that mitigate decay for the most
affected subgroups while minimizing any unintended effects. Current approaches
do not provide such detailed insight, as they either (i) explain how average
performance shifts arise or (ii) identify adversely affected subgroups without
insight into how this occurred. To this end, we introduce a Subgroup-scanning
Hierarchical Inference Framework for performance drifT (SHIFT). SHIFT first
asks "Is there any subgroup with unacceptably large performance decay due to
covariate/outcome shifts?" (Where?) and, if so, dives deeper to ask "Can we
explain this using more detailed variable(subset)-specific shifts?" (How?). In
real-world experiments, we find that SHIFT identifies interpretable subgroups
affected by performance decay, and suggests targeted actions that effectively
mitigate the decay.

</details>


### [330] [Learning Juntas under Markov Random Fields](https://arxiv.org/abs/2506.00764)
*Gautam Chandrasekaran,Adam Klivans*

Key words: juntas, Markov Random Fields, 平滑分析, 图模型学习

TL;DR: 提出了一个在平滑分析框架下学习Markov Random Fields (MRFs)中$O(	ext{log}n)$ juntas的多项式时间算法，扩展了Kalai和Teng的工作。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 扩展了Kalai和Teng在平滑乘积分布下的算法，使其适用于更一般的MRFs。

Method: 算法分为两个阶段：无监督结构学习阶段和贪心监督学习阶段。

Result: 首次展示了无向图模型结构学习算法可以转化为高效的监督学习算法。

Conclusion: 该算法为MRFs中的juntas学习提供了一种高效且通用的方法。

Abstract: We give an algorithm for learning $O(\log n)$ juntas in polynomial-time with
respect to Markov Random Fields (MRFs) in a smoothed analysis framework where
only the external field has been randomly perturbed. This is a broad
generalization of the work of Kalai and Teng, who gave an algorithm that
succeeded with respect to smoothed product distributions (i.e., MRFs whose
dependency graph has no edges). Our algorithm has two phases: (1) an
unsupervised structure learning phase and (2) a greedy supervised learning
algorithm. This is the first example where algorithms for learning the
structure of an undirected graphical model lead to provably efficient
algorithms for supervised learning.

</details>


### [331] [Beyond Attention: Learning Spatio-Temporal Dynamics with Emergent Interpretable Topologies](https://arxiv.org/abs/2506.00770)
*Sai Vamsi Alisetti,Vikas Kalagi,Sanjukta Krishnagopal*

Key words: 时空预测，图注意力网络，GRU，可解释性，计算效率

TL;DR: 论文提出了一种名为InterGAT的简化替代方案，用于时空预测任务，通过可学习的对称节点交互矩阵取代传统的图注意力机制，提高了预测精度和训练效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的图注意力网络（GAT）依赖于预定义的邻接结构和动态注意力分数，可能导致归纳偏见和计算开销，同时掩盖了可解释性。

Method: 提出InterGAT和InterGAT-GRU框架，使用完全可学习的对称节点交互矩阵捕获潜在空间关系，结合GRU时间解码器进行时空预测。

Result: 在SZ-Taxi和Los-Loop数据集上，预测精度分别提升21%和6%，训练时间减少60-70%，且学习到的交互矩阵具有可解释性。

Conclusion: InterGAT框架不仅提高了预测性能和计算效率，还能揭示功能拓扑结构，为动态图领域提供了一种兼具预测能力和可解释性的解决方案。

Abstract: Spatio-temporal forecasting is critical in applications such as traffic
prediction, energy demand modeling, and weather monitoring. While Graph
Attention Networks (GATs) are popular for modeling spatial dependencies, they
rely on predefined adjacency structures and dynamic attention scores,
introducing inductive biases and computational overhead that can obscure
interpretability.
  We propose InterGAT, a simplified alternative to GAT that replaces masked
attention with a fully learnable, symmetric node interaction matrix, capturing
latent spatial relationships without relying on fixed graph topologies. Our
framework, InterGAT-GRU, which incorporates a GRU-based temporal decoder,
outperforms the baseline GAT-GRU in forecasting accuracy, achieving at least a
21% improvement on the SZ-Taxi dataset and a 6% improvement on the Los-Loop
dataset across all forecasting horizons (15 to 60 minutes). Additionally, we
observed reduction in training time by 60-70% compared to GAT-GRU baseline.
  Crucially, the learned interaction matrix reveals interpretable structure: it
recovers sparse, topology-aware attention patterns that align with community
structure. Spectral and clustering analyses show that the model captures both
localized and global dynamics, offering insights into the functional topology
driving predictions. This highlights how structure learning can simultaneously
support prediction, computational efficiency, and topological interpretabil-ity
in dynamic graph-based domains.

</details>


### [332] [Manipulating 3D Molecules in a Fixed-Dimensional SE(3)-Equivariant Latent Space](https://arxiv.org/abs/2506.00771)
*Zitao Chen,Yinjun Jia,Zitong Tian,Wei-Ying Ma,Yanyan Lan*

Key words: 3D分子编辑, SE(3)-等变, 变分自编码器, 零-shot学习, 药物优化

TL;DR: 提出了一种名为MolFLAE的零-shot分子操纵方法，通过SE(3)-等变变分自编码器学习3D分子的共享潜在空间，支持分子编辑和优化。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统药物化学家依赖3D结构优化分子，现有深度学习方法多为监督任务，本工作旨在开发灵活且无监督的分子操纵方法。

Method: 采用SE(3)-等变神经网络的变分自编码器MolFLAE，结合Bayesian Flow Network重构分子结构，实现固定维度的潜在空间编码。

Result: 在无条件3D分子生成基准中表现优异，支持零-shot操作（如原子数编辑、结构重构和属性插值），并在药物优化任务中验证了实用性。

Conclusion: MolFLAE展示了灵活性和实际应用潜力，为分子编辑和优化提供了新思路。

Abstract: Medicinal chemists often optimize drugs considering their 3D structures and
designing structurally distinct molecules that retain key features, such as
shapes, pharmacophores, or chemical properties. Previous deep learning
approaches address this through supervised tasks like molecule inpainting or
property-guided optimization. In this work, we propose a flexible zero-shot
molecule manipulation method by navigating in a shared latent space of 3D
molecules. We introduce a Variational AutoEncoder (VAE) for 3D molecules, named
MolFLAE, which learns a fixed-dimensional, SE(3)-equivariant latent space
independent of atom counts. MolFLAE encodes 3D molecules using an
SE(3)-equivariant neural network into fixed number of latent nodes,
distinguished by learned embeddings. The latent space is regularized, and
molecular structures are reconstructed via a Bayesian Flow Network (BFN)
conditioned on the encoder's latent output. MolFLAE achieves competitive
performance on standard unconditional 3D molecule generation benchmarks.
Moreover, the latent space of MolFLAE enables zero-shot molecule manipulation,
including atom number editing, structure reconstruction, and coordinated latent
interpolation for both structure and properties. We further demonstrate our
approach on a drug optimization task for the human glucocorticoid receptor,
generating molecules with improved hydrophilicity while preserving key
interactions, under computational evaluations. These results highlight the
flexibility, robustness, and real-world utility of our method, opening new
avenues for molecule editing and optimization.

</details>


### [333] [LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning](https://arxiv.org/abs/2506.00772)
*Zihang Liu,Tianyu Pang,Oleg Balabanov,Chaoqun Yang,Tianjin Huang,Lu Yin,Yaoqing Yang,Shiwei Liu*

Key words: LLM微调，稀疏微调，低秩近似，参数效率，推理任务

TL;DR: 提出了基于低秩信息的稀疏微调方法LIFT，仅更新前5%主权重，在推理任务中表现优于完全微调，同时保持了内存效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决完全微调的计算成本高、易过拟合和灾难性遗忘问题，同时提升稀疏微调在LLM时代的有效性。

Method: 通过低秩近似识别关键权重（主权重），仅更新前5%的主权重进行微调。

Result: 在算术推理等任务中表现优于完全微调，同时保留了更多源域知识（比完全微调和LoRA多20%）。

Conclusion: LIFT是一种高效且有效的微调方法，适合大规模语言模型。

Abstract: Recent studies have shown that supervised fine-tuning of LLMs on a small
number of high-quality datasets can yield strong reasoning capabilities.
However, full fine-tuning (Full FT), while powerful, is computationally
expensive and susceptible to overfitting and catastrophic forgetting,
particularly when data is limited. Sparse fine-tuning, which previously
achieved notable success by updating only a small subset of model parameters,
offers a promising trade-off between efficiency and effectiveness. Yet, it has
lagged behind in the LLM era due to the difficulty of identifying parameters
truly critical for reasoning. In this work, we state that weights with the
largest magnitude after low-rank approximation are critical weights for
fine-tuning, which we call Principal Weights. Surprisingly, while
magnitude-based sparse fine-tuning performs poorly as a baseline on LLM
fine-tuning, it becomes highly effective after rank reduction. These insights
motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only
updates the top 5% Principal Weights throughout training and consistently
achieves better performance on reasoning tasks than Full FT, while maintaining
memory efficiency on par with popular parameter-efficient fine-tuning methods.
In addition to strong performance on target domains such as arithmetic
reasoning, LIFT also retains up to 20% more source-domain knowledge, compared
to Full FT and LoRA. Our code is available at:
https://github.com/zihanghliu/LIFT.

</details>


### [334] [Bridging Supervised and Temporal Difference Learning with $Q$-Conditioned Maximization](https://arxiv.org/abs/2506.00795)
*Xing Lei,Zifeng Zhuang,Shentao Yang,Sheng Xu,Yunhao Luo,Fei Shen,Xuetao Zhang,Donglin Wang*

Key words: 监督学习, 离线强化学习, 轨迹缝合, Q-条件策略, GCReinSL

TL;DR: 本文提出了一种结合监督学习和轨迹缝合能力的离线强化学习方法GCReinSL，通过Q-条件策略和最大化优化，提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决监督学习方法在离线强化学习中缺乏轨迹缝合能力的问题，缩小其与TD学习方法的性能差距。

Method: 引入Q-条件最大化监督学习，结合CVAE估计Q函数和期望回归实现Q值最大化。

Result: 实验表明，GCReinSL在离线RL数据集上的缝合性能优于现有SL方法。

Conclusion: GCReinSL成功赋予了SL方法轨迹缝合能力，显著提升了其性能。

Abstract: Recently, supervised learning (SL) methodology has emerged as an effective
approach for offline reinforcement learning (RL) due to their simplicity,
stability, and efficiency. However, recent studies show that SL methods lack
the trajectory stitching capability, typically associated with temporal
difference (TD)-based approaches. A question naturally surfaces: How can we
endow SL methods with stitching capability and bridge its performance gap with
TD learning? To answer this question, we introduce $Q$-conditioned maximization
supervised learning for offline goal-conditioned RL, which enhances SL with the
stitching capability through $Q$-conditioned policy and $Q$-conditioned
maximization. Concretely, we propose Goal-Conditioned Reinforced Supervised
Learning (GCReinSL), which consists of (1) estimating the $Q$-function by CVAE
from the offline dataset and (2) finding the maximum $Q$-value within the data
support by integrating $Q$-function maximization with Expectile Regression. In
inference time, our policy chooses optimal actions based on such a maximum
$Q$-value. Experimental results from stitching evaluations on offline RL
datasets demonstrate that our method outperforms prior SL approaches with
stitching capabilities and goal data augmentation techniques.

</details>


### [335] [Action Dependency Graphs for Globally Optimal Coordinated Reinforcement Learning](https://arxiv.org/abs/2506.00797)
*Jianglin Ding,Jingcheng Tang,Gangshan Jing*

Key words: 多智能体强化学习、动作依赖图、全局最优、策略迭代

TL;DR: 该论文提出了一种基于动作依赖图的非自回归策略，用于多智能体强化学习中，通过稀疏依赖图保证全局最优性，并验证了其在实际环境中的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有自回归动作依赖策略在智能体数量增加时计算复杂度高，限制了可扩展性，因此研究了更通用的动作依赖策略。

Method: 引入动作依赖图（ADG）建模智能体间的动作依赖，基于协调图理论证明了稀疏ADG可实现全局最优，并提出了表格策略迭代算法。

Result: 实验验证了该方法在复杂环境中的鲁棒性和适用性。

Conclusion: 稀疏动作依赖图策略在多智能体强化学习中具有潜力。

Abstract: Action-dependent individual policies, which incorporate both environmental
states and the actions of other agents in decision-making, have emerged as a
promising paradigm for achieving global optimality in multi-agent reinforcement
learning (MARL). However, the existing literature often adopts auto-regressive
action-dependent policies, where each agent's policy depends on the actions of
all preceding agents. This formulation incurs substantial computational
complexity as the number of agents increases, thereby limiting scalability. In
this work, we consider a more generalized class of action-dependent policies,
which do not necessarily follow the auto-regressive form. We propose to use the
`action dependency graph (ADG)' to model the inter-agent action dependencies.
Within the context of MARL problems structured by coordination graphs, we prove
that an action-dependent policy with a sparse ADG can achieve global
optimality, provided the ADG satisfies specific conditions specified by the
coordination graph. Building on this theoretical foundation, we develop a
tabular policy iteration algorithm with guaranteed global optimality.
Furthermore, we integrate our framework into several SOTA algorithms and
conduct experiments in complex environments. The empirical results affirm the
robustness and applicability of our approach in more general scenarios,
underscoring its potential for broader MARL challenges.

</details>


### [336] [A Dynamic Stiefel Graph Neural Network for Efficient Spatio-Temporal Time Series Forecasting](https://arxiv.org/abs/2506.00798)
*Jiankai Zheng,Liang Xie*

Key words: 时空时间序列、图神经网络、Stiefel流形、动态图优化、SGSC、SGFT

TL;DR: 提出了一种名为DST-SGNN的动态时空Stiefel图神经网络，通过SGSC和SGFT技术高效处理时空时间序列，实验表明其在性能和计算成本上优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于时空时间序列（STTS）在时间和空间维度上存在复杂的动态相关性，现有图神经网络难以平衡建模动态时空关系的有效性和效率。

Method: 引入了Stiefel图谱卷积（SGSC）和Stiefel图傅里叶变换（SGFT），并提出了线性动态图优化（LDGOSM）和多层SGSC（MSGSC）来高效捕获复杂的时空相关性。

Result: 在七个时空数据集上的实验表明，DST-SGNN在性能上优于现有方法，同时保持了较低的计算成本。

Conclusion: DST-SGNN通过新颖的技术在时空时间序列预测中实现了高效性和有效性的平衡。

Abstract: Spatio-temporal time series (STTS) have been widely used in many
applications. However, accurately forecasting STTS is challenging due to
complex dynamic correlations in both time and space dimensions. Existing graph
neural networks struggle to balance effectiveness and efficiency in modeling
dynamic spatio-temporal relations. To address this problem, we propose the
Dynamic Spatio-Temporal Stiefel Graph Neural Network (DST-SGNN) to efficiently
process STTS. For DST-SGNN, we first introduce the novel Stiefel Graph Spectral
Convolution (SGSC) and Stiefel Graph Fourier Transform (SGFT). The SGFT matrix
in SGSC is constrained to lie on the Stiefel manifold, and SGSC can be regarded
as a filtered graph spectral convolution. We also propose the Linear Dynamic
Graph Optimization on Stiefel Manifold (LDGOSM), which can efficiently learn
the SGFT matrix from the dynamic graph and significantly reduce the
computational complexity. Finally, we propose a multi-layer SGSC (MSGSC) that
efficiently captures complex spatio-temporal correlations. Extensive
experiments on seven spatio-temporal datasets show that DST-SGNN outperforms
state-of-the-art methods while maintaining relatively low computational costs.

</details>


### [337] [Uni-LoRA: One Vector is All You Need](https://arxiv.org/abs/2506.00799)
*Kaiyang Li,Shaobo Han,Qing Su,Wei Li,Zhipeng Cai,Shihao Ji*

Key words: LoRA, 参数高效微调, 统一框架, 全局参数共享, 等距投影

TL;DR: Uni-LoRA是一种统一框架，通过高效且理论支持的投影矩阵实现了全局参数共享，显著提升了LoRA变体的参数效率，同时保持了预测性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的LoRA变体通过层或结构特定的投影限制了跨层参数共享，影响了参数效率。Uni-LoRA旨在提供一个统一框架，提升参数效率和计算效率。

Method: 提出Uni-LoRA框架，通过等距投影矩阵从低维子空间重构LoRA参数空间，实现了全局参数共享，并提供了一种仅需单个可训练向量的解决方案。

Result: 在多个基准测试（GLUE、数学推理和指令调优）中，Uni-LoRA在参数效率和预测性能上都优于或匹配现有方法。

Conclusion: Uni-LoRA不仅统一了现有LoRA变体，还提供了一种高效的单向量解决方案，为参数高效微调提供了新方向。

Abstract: Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient
fine-tuning (PEFT) method for large language models (LLMs) by constraining
weight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and
VB-LoRA push efficiency further by introducing additional constraints to reduce
the trainable parameter space. In this paper, we show that the parameter space
reduction strategies employed by these LoRA variants can be formulated within a
unified framework, Uni-LoRA, where the LoRA parameter space, flattened as a
high-dimensional vector space $R^D$, can be reconstructed through a projection
from a subspace R^d, with $d \ll D$. We demonstrate that the fundamental
difference among various LoRA methods lies in the choice of the projection
matrix, $P \in R^{D \times d}$.Most existing LoRA variants rely on layer-wise
or structure-specific projections that limit cross-layer parameter sharing,
thereby compromising parameter efficiency. In light of this, we introduce an
efficient and theoretically grounded projection matrix that is isometric,
enabling global parameter sharing and reducing computation overhead.
Furthermore, under the unified view of Uni-LoRA, this design requires only a
single trainable vector to reconstruct LoRA parameters for the entire LLM -
making Uni-LoRA both a unified framework and a "one-vector-only" solution.
Extensive experiments on GLUE, mathematical reasoning, and instruction tuning
benchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter
efficiency while outperforming or matching prior approaches in predictive
performance.

</details>


### [338] [Unlearning Inversion Attacks for Graph Neural Networks](https://arxiv.org/abs/2506.00808)
*Jiahao Zhang,Yilong Wang,Zhiwei Zhang,Xiaorui Liu,Suhang Wang*

Key words: 图反学习,隐私攻击,成员推理,TrendAttack,置信度陷阱,黑盒攻击

TL;DR: 该论文挑战了图反学习方法的假设，提出了一种图反学习反转攻击（TrendAttack），能够通过黑盒访问部分恢复被删除的边的信息。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究动机是验证图反学习方法的安全假设是否成立，即被删除的敏感数据是否真的无法恢复。

Method: 提出TrendAttack方法，利用置信度陷阱和自适应预测机制区分未学习边与其他边，结合现有的成员推理技术进行攻击。

Result: 实验表明，TrendAttack在四个实际数据集上显著优于现有的GNN成员推理基线方法，揭示了当前图反学习方法的隐私漏洞。

Conclusion: 结论指出，当前的图反学习方法存在严重的隐私风险，需要更安全的机制来保护敏感数据。

Abstract: Graph unlearning methods aim to efficiently remove the impact of sensitive
data from trained GNNs without full retraining, assuming that deleted
information cannot be recovered. In this work, we challenge this assumption by
introducing the graph unlearning inversion attack: given only black-box access
to an unlearned GNN and partial graph knowledge, can an adversary reconstruct
the removed edges? We identify two key challenges: varying
probability-similarity thresholds for unlearned versus retained edges, and the
difficulty of locating unlearned edge endpoints, and address them with
TrendAttack. First, we derive and exploit the confidence pitfall, a theoretical
and empirical pattern showing that nodes adjacent to unlearned edges exhibit a
large drop in model confidence. Second, we design an adaptive prediction
mechanism that applies different similarity thresholds to unlearned and other
membership edges. Our framework flexibly integrates existing membership
inference techniques and extends them with trend features. Experiments on four
real-world datasets demonstrate that TrendAttack significantly outperforms
state-of-the-art GNN membership inference baselines, exposing a critical
privacy vulnerability in current graph unlearning methods.

</details>


### [339] [LLM Cannot Discover Causality, and Should Be Restricted to Non-Decisional Support in Causal Discovery](https://arxiv.org/abs/2506.00844)
*Xingyu Wu,Kui Yu,Jibin Wu,Kay Chen Tan*

Key words: LLMs, 因果发现, 启发式搜索, 可靠性

TL;DR: 本文重新评估了LLMs在因果发现中的作用，反对其直接参与确定因果关系，指出其缺乏理论基础并可能导致不可靠性，实证研究揭示了现有方法的局限性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨LLMs在因果发现中的适用性，揭示其局限性并提出改进方向。

Method: 通过实证研究分析LLMs的不足，并提出限制其角色为非决策辅助的建议。

Result: 实验表明，限制LLMs为非决策辅助可加速收敛并优于传统方法。

Conclusion: 呼吁社区开发尊重因果发现核心原理的专用模型和训练方法。

Abstract: This paper critically re-evaluates LLMs' role in causal discovery and argues
against their direct involvement in determining causal relationships. We
demonstrate that LLMs' autoregressive, correlation-driven modeling inherently
lacks the theoretical grounding for causal reasoning and introduces
unreliability when used as priors in causal discovery algorithms. Through
empirical studies, we expose the limitations of existing LLM-based methods and
reveal that deliberate prompt engineering (e.g., injecting ground-truth
knowledge) could overstate their performance, helping to explain the
consistently favorable results reported in much of the current literature.
Based on these findings, we strictly confined LLMs' role to a non-decisional
auxiliary capacity: LLMs should not participate in determining the existence or
directionality of causal relationships, but can assist the search process for
causal graphs (e.g., LLM-based heuristic search). Experiments across various
settings confirm that, by strictly isolating LLMs from causal decision-making,
LLM-guided heuristic search can accelerate the convergence and outperform both
traditional and LLM-based methods in causal structure learning. We conclude
with a call for the community to shift focus from naively applying LLMs to
developing specialized models and training method that respect the core
principles of causal discovery.

</details>


### [340] [Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning](https://arxiv.org/abs/2506.00845)
*Yizhuo Zhang,Heng Wang,Shangbin Feng,Zhaoxuan Tan,Xinyun Liu,Yulia Tsvetkov*

Key words: 强化学习, 图推理, 大语言模型, 泛化能力, 真实世界任务

TL;DR: 该研究提出用强化学习（RL）替代传统的监督微调方法，以提升LLMs在合成图数据上的泛化能力，并在真实世界的隐式图任务中表现更优。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法通过监督微调使LLMs在合成图数据上表现良好，但缺乏对真实世界任务的泛化能力，因此研究转向强化学习以解决这一问题。

Method: 设计了基于解和基于过程的奖励机制，使用GRPO和DPO等RL算法，对LLMs进行训练，并在合成和真实世界任务上进行测试。

Result: 实验显示，RL方法在5个数据集上平均提升12.9%，基于过程的奖励表现更优，但组合性和解释性仍是挑战。

Conclusion: 强化学习方法在提升LLMs的图推理泛化能力上有效，但需进一步解决组合性和解释性问题。

Abstract: Previous research has sought to enhance the graph reasoning capabilities of
LLMs by supervised fine-tuning on synthetic graph data. While these led to
specialized LLMs better at solving graph algorithm problems, we don't need LLMs
for shortest path: we need generalization from synthetic graph data to
real-world tasks with implicit graph structures. In this work, we propose to
unlock generalizable learning of graph synthetic data with reinforcement
learning. We first design solution-based and process-based rewards for
synthetic graph problems: instead of rigid memorizing response patterns in
direct fine-tuning, we posit that RL would help LLMs grasp the essentials
underlying graph reasoning and alleviate overfitting. We employ RL algorithms
such as GRPO and DPO, aligning both off-the-shelf LLMs and LLMs fine-tuned on
synthetic graph data. We then compare them against existing settings on both
in-domain synthetic tasks and out-of-domain real-world tasks with implicit
graph structures such as multi-hop QA, structured planning, and more. Extensive
experiments demonstrate that our RL recipe leads to statistically significant
improvement on 5 datasets, with an average gain of 12.9\% over baseline
settings. Further analysis reveals that process-based rewards consistently
outperform solution-based rewards, mixing synthetic and real-world task data
yields potential gains, while compositionality and explainable intermediate
steps remains a critical challenge even after RL.

</details>


### [341] [Infinite-Width Limit of a Single Attention Layer: Analysis via Tensor Programs](https://arxiv.org/abs/2506.00846)
*Mana Sakai,Ryo Karakida,Masaaki Imaizumi*

Key words: 注意力层, 无限宽度, 非高斯性, Tensor Programs, Transformer

TL;DR: 该论文通过Tensor Programs框架，严格推导了单个注意力层在无限宽度极限下的分布情况，展示了其非高斯特性，并通过数值实验验证了理论预测。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有基于高斯近似的无限宽度理论无法有效描述注意力层的行为，尤其是在现实的架构维度和标准缩放下。

Method: 利用Tensor Programs框架，推导了单个注意力层在无限宽度极限下的分布，无需依赖无限头或特殊缩放。

Result: 发现该极限分布具有非高斯性，且与随机相似性分数相关，数值实验支持理论预测。

Conclusion: 研究为无限宽度下深度Transformer的统一理论奠定了基础。

Abstract: In modern theoretical analyses of neural networks, the infinite-width limit
is often invoked to justify Gaussian approximations of neuron preactivations
(e.g., via neural network Gaussian processes or Tensor Programs). However,
these Gaussian-based asymptotic theories have so far been unable to capture the
behavior of attention layers, except under special regimes such as infinitely
many heads or tailored scaling schemes. In this paper, leveraging the Tensor
Programs framework, we rigorously identify the infinite-width limit
distribution of variables within a single attention layer under realistic
architectural dimensionality and standard $1/\sqrt{n}$-scaling with $n$
dimensionality. We derive the exact form of this limit law without resorting to
infinite-head approximations or tailored scalings, demonstrating that it
departs fundamentally from Gaussianity. This limiting distribution exhibits
non-Gaussianity from a hierarchical structure, being Gaussian conditional on
the random similarity scores. Numerical experiments validate our theoretical
predictions, confirming the effectiveness of our theory at finite width and
accurate description of finite-head attentions. Beyond characterizing a
standalone attention layer, our findings lay the groundwork for developing a
unified theory of deep Transformer architectures in the infinite-width regime.

</details>


### [342] [Speech Unlearning](https://arxiv.org/abs/2506.00848)
*Jiali Cheng,Hadi Amiri*

Key words: machine unlearning, speech tasks, privacy preservation, bias mitigation, sample unlearning, class unlearning

TL;DR: 提出了语音任务的机器去学习（machine unlearning）技术，旨在高效去除特定数据对语音模型的影响，无需完全重新训练。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 隐私保护、去除过时或噪声数据、减轻偏见是主要应用动机。尽管机器去学习在计算机视觉和自然语言处理中已有研究，但由于语音数据的多维性、时序性和说话人依赖性，语音领域的去学习问题尚未深入探索。

Method: 定义了两项基本任务：样本去学习（移除单个数据点，如特定录音）和类别去学习（移除整个类别，如某个说话人的所有数据），同时保留其余数据的性能。

Result: 实验表明，语音数据的去学习比图像或文本更具挑战性，尤其在关键词检测和说话人识别任务中。

Conclusion: 未来方向包括结构化训练、鲁棒评估、特征级去学习、更广泛的应用、可扩展方法及对抗鲁棒性。

Abstract: We introduce machine unlearning for speech tasks, a novel and underexplored
research problem that aims to efficiently and effectively remove the influence
of specific data from trained speech models without full retraining. This has
important applications in privacy preservation, removal of outdated or noisy
data, and bias mitigation. While machine unlearning has been studied in
computer vision and natural language processing, its application to speech is
largely unexplored due to the high-dimensional, sequential, and
speaker-dependent nature of speech data. We define two fundamental speech
unlearning tasks: sample unlearning, which removes individual data points
(e.g., a voice recording), and class unlearning, which removes an entire
category (e.g., all data from a speaker), while preserving performance on the
remaining data. Experiments on keyword spotting and speaker identification
demonstrate that unlearning speech data is significantly more challenging than
unlearning image or text data. We conclude with key future directions in this
area, including structured training, robust evaluation, feature-level
unlearning, broader applications, scalable methods, and adversarial robustness.

</details>


### [343] [Generalization in VAE and Diffusion Models: A Unified Information-Theoretic Analysis](https://arxiv.org/abs/2506.00849)
*Qi Chen,Jierui Zhu,Florian Shkurti*

Key words: 扩散模型, 变分自编码器, 泛化性能, 信息论, 理论框架

TL;DR: 本文提出了一个统一的理论框架，分析了扩散模型（DMs）和变分自编码器（VAEs）的泛化性能，填补了理论和实践的差距。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 扩散模型和变分自编码器的泛化性能在理论上缺乏全面分析，尤其是共享编码器-生成器结构的考虑不足。

Method: 利用信息论工具，提出了一个理论框架，将编码器和生成器视为随机映射，分析其泛化性能。

Result: 1. 改进了对VAE生成器泛化的分析；2. 揭示了DMs中泛化与扩散时间T的权衡；3. 提供了基于训练数据的可计算界限。

Conclusion: 理论框架有效，实证结果支持了其有效性。

Abstract: Despite the empirical success of Diffusion Models (DMs) and Variational
Autoencoders (VAEs), their generalization performance remains theoretically
underexplored, especially lacking a full consideration of the shared
encoder-generator structure. Leveraging recent information-theoretic tools, we
propose a unified theoretical framework that provides guarantees for the
generalization of both the encoder and generator by treating them as randomized
mappings. This framework further enables (1) a refined analysis for VAEs,
accounting for the generator's generalization, which was previously overlooked;
(2) illustrating an explicit trade-off in generalization terms for DMs that
depends on the diffusion time $T$; and (3) providing computable bounds for DMs
based solely on the training data, allowing the selection of the optimal $T$
and the integration of such bounds into the optimization process to improve
model performance. Empirical results on both synthetic and real datasets
illustrate the validity of the proposed theory.

</details>


### [344] [FourierFlow: Frequency-aware Flow Matching for Generative Turbulence Modeling](https://arxiv.org/abs/2506.00862)
*Haixin Wang,Jiashu Pan,Hao Wu,Fan Zhang,Tailin Wu*

Key words: 湍流建模、生成模型、频谱偏差、共模噪声、FourierFlow

TL;DR: 论文提出FourierFlow，一种新框架用于解决生成模型在模拟湍流时的频谱偏差和共模噪声问题，通过双分支架构、频率引导和自适应融合策略显著提升性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有生成模型在模拟湍流时存在频谱偏差和共模噪声问题，限制了高保真湍流生成的性能，因此需要一种更有效的方法。

Method: FourierFlow包含双分支架构（显著性流和频率引导分支）、自适应融合策略以及掩码自编码器预训练，以显式和隐式方式优化频谱感知。

Result: 在三种典型湍流场景中表现优于现有方法，并在分布外域、长期时间外推和噪声输入下展现强泛化能力。

Conclusion: FourierFlow通过频率感知学习显著提升湍流建模的生成质量与鲁棒性，为复杂流体系统模拟提供新思路。

Abstract: Modeling complex fluid systems, especially turbulence governed by partial
differential equations (PDEs), remains a fundamental challenge in science and
engineering. Recently, diffusion-based generative models have gained attention
as a powerful approach for these tasks, owing to their capacity to capture
long-range dependencies and recover hierarchical structures. However, we
present both empirical and theoretical evidence showing that generative models
struggle with significant spectral bias and common-mode noise when generating
high-fidelity turbulent flows. Here we propose FourierFlow, a novel generative
turbulence modeling framework that enhances the frequency-aware learning by
both implicitly and explicitly mitigating spectral bias and common-mode noise.
FourierFlow comprises three key innovations. Firstly, we adopt a dual-branch
backbone architecture, consisting of a salient flow attention branch with
local-global awareness to focus on sensitive turbulence areas. Secondly, we
introduce a frequency-guided Fourier mixing branch, which is integrated via an
adaptive fusion strategy to explicitly mitigate spectral bias in the generative
model. Thirdly, we leverage the high-frequency modeling capabilities of the
masked auto-encoder pre-training and implicitly align the features of the
generative model toward high-frequency components. We validate the
effectiveness of FourierFlow on three canonical turbulent flow scenarios,
demonstrating superior performance compared to state-of-the-art methods.
Furthermore, we show that our model exhibits strong generalization capabilities
in challenging settings such as out-of-distribution domains, long-term temporal
extrapolation, and robustness to noisy inputs. The code can be found at
https://github.com/AI4Science-WestlakeU/FourierFlow.

</details>


### [345] [Local Manifold Approximation and Projection for Manifold-Aware Diffusion Planning](https://arxiv.org/abs/2506.00867)
*Kyowoon Lee,Jaesik Choi*

Key words: 扩散模型, 离线强化学习, 流形近似, 轨迹生成, 安全性

TL;DR: 该论文提出了一种名为LoMAP的方法，通过将指导样本投影到离线数据集近似的低秩子空间，解决了扩散生成模型中因采样过程中指导不准确而导致不可行轨迹生成的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 扩散生成模型在长时域稀疏奖励任务中表现出色，但其可靠性因采样过程中不可行轨迹的随机风险而受限，适用于安全关键场景时存在挑战。

Method: 论文提出LoMAP方法，通过在采样过程中将引导样本投影到低秩子空间，避免生成不可行轨迹，并分析了流形偏差的存在。

Result: 在标准离线强化学习基准测试中，LoMAP验证了其有效性，并展示作为独立模块可融入分层扩散规划器中进一步提升性能。

Conclusion: LoMAP通过流形近似和投影有效解决了扩散生成模型的可靠性问题，适用于安全关键任务。

Abstract: Recent advances in diffusion-based generative modeling have demonstrated
significant promise in tackling long-horizon, sparse-reward tasks by leveraging
offline datasets. While these approaches have achieved promising results, their
reliability remains inconsistent due to the inherent stochastic risk of
producing infeasible trajectories, limiting their applicability in
safety-critical applications. We identify that the primary cause of these
failures is inaccurate guidance during the sampling procedure, and demonstrate
the existence of manifold deviation by deriving a lower bound on the guidance
gap. To address this challenge, we propose Local Manifold Approximation and
Projection (LoMAP), a training-free method that projects the guided sample onto
a low-rank subspace approximated from offline datasets, preventing infeasible
trajectory generation. We validate our approach on standard offline
reinforcement learning benchmarks that involve challenging long-horizon
planning. Furthermore, we show that, as a standalone module, LoMAP can be
incorporated into the hierarchical diffusion planner, providing further
performance enhancements.

</details>


### [346] [ModuLM: Enabling Modular and Multimodal Molecular Relational Learning with Large Language Models](https://arxiv.org/abs/2506.00880)
*Zhuo Chen,Yizhen Zheng,Huan Yee Koh,Hongxin Xiang,Linjiang Chen,Wenjie Du,Yang Wang*

Key words: 分子关系学习, 大型语言模型, 模块化框架, 分子编码器

TL;DR: 本文提出了ModuLM框架，旨在解决分子关系学习（MRL）中模型多样性和灵活性不足的问题，支持动态架构切换和多种分子表示。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着大型语言模型（LLMs）的快速发展，如何整合MRL与LLMs并实现公平的模型比较成为一个挑战。

Method: ModuLM框架提供了丰富的模块化组件，包括2D/3D分子编码器、交互层和主流LLM骨干网络，支持超过50,000种模型配置。

Result: ModuLM在基于LLM的MRL任务中表现出有效性。

Conclusion: ModuLM为MRL研究提供了一个灵活且高效的解决方案，解决了模型多样性和冗余编码问题。

Abstract: Molecular Relational Learning (MRL) aims to understand interactions between
molecular pairs, playing a critical role in advancing biochemical research.
With the recent development of large language models (LLMs), a growing number
of studies have explored the integration of MRL with LLMs and achieved
promising results. However, the increasing availability of diverse LLMs and
molecular structure encoders has significantly expanded the model space,
presenting major challenges for benchmarking. Currently, there is no LLM
framework that supports both flexible molecular input formats and dynamic
architectural switching. To address these challenges, reduce redundant coding,
and ensure fair model comparison, we propose ModuLM, a framework designed to
support flexible LLM-based model construction and diverse molecular
representations. ModuLM provides a rich suite of modular components, including
8 types of 2D molecular graph encoders, 11 types of 3D molecular conformation
encoders, 7 types of interaction layers, and 7 mainstream LLM backbones. Owing
to its highly flexible model assembly mechanism, ModuLM enables the dynamic
construction of over 50,000 distinct model configurations. In addition, we
provide comprehensive results to demonstrate the effectiveness of ModuLM in
supporting LLM-based MRL tasks.

</details>


### [347] [State-Covering Trajectory Stitching for Diffusion Planners](https://arxiv.org/abs/2506.00895)
*Kyowoon Lee,Jaesik Choi*

Key words: 扩散模型,强化学习,轨迹缝合,长时域规划,离线数据集

TL;DR: SCoTS是一种新颖的奖励免费轨迹增强方法，通过缝合短轨迹段生成多样化且扩展的轨迹，显著提升了扩散规划器在离线目标条件任务中的性能和泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 扩散模型在强化学习的长期规划中表现突出，但受限于训练数据的质量和多样性，难以泛化到训练分布之外的任务或更长的规划时域。SCoTS旨在解决这一挑战。

Method: SCoTS通过学习保持时间距离的潜在表示，捕捉环境的时间结构，然后通过方向性探索和新颖性引导迭代缝合轨迹段，扩展潜在空间覆盖。

Result: 实验表明，SCoTS显著提升了扩散规划器在离线目标条件基准任务中的性能，特别是在需要轨迹缝合和长期推理的场景中，同时也改进了常用离线RL算法的表现。

Conclusion: SCoTS通过系统生成多样化和扩展的轨迹，提升了扩散模型的泛化能力和性能，适用于复杂的长时域规划任务。

Abstract: Diffusion-based generative models are emerging as powerful tools for
long-horizon planning in reinforcement learning (RL), particularly with offline
datasets. However, their performance is fundamentally limited by the quality
and diversity of training data. This often restricts their generalization to
tasks outside their training distribution or longer planning horizons. To
overcome this challenge, we propose State-Covering Trajectory Stitching
(SCoTS), a novel reward-free trajectory augmentation method that incrementally
stitches together short trajectory segments, systematically generating diverse
and extended trajectories. SCoTS first learns a temporal distance-preserving
latent representation that captures the underlying temporal structure of the
environment, then iteratively stitches trajectory segments guided by
directional exploration and novelty to effectively cover and expand this latent
space. We demonstrate that SCoTS significantly improves the performance and
generalization capabilities of diffusion planners on offline goal-conditioned
benchmarks requiring stitching and long-horizon reasoning. Furthermore,
augmented trajectories generated by SCoTS significantly improve the performance
of widely used offline goal-conditioned RL algorithms across diverse
environments.

</details>


### [348] [PCoreSet: Effective Active Learning through Knowledge Distillation from Vision-Language Models](https://arxiv.org/abs/2506.00910)
*Seongjae Kang,Dong Bok Lee,Hyungjoon Jang,Dongseop Kim,Sung Ju Hwang*

Key words: 知识蒸馏, 主动学习, 视觉语言模型, 概率核心集

TL;DR: ActiveKD是一个将主动学习（AL）与知识蒸馏（KD）结合的框架，利用大型视觉语言模型（VLMs）的零样本和少样本能力，通过概率核心集（PCoreSet）策略在数据稀缺场景中高效传递教师知识。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究动机在于填补知识蒸馏在主动学习中应用的空白，特别是在数据稀缺场景下，如何利用未标记数据有效传递教师知识。

Method: 方法包括引入ActiveKD框架，利用VLMs的结构化预测偏置作为教师模型的归纳偏置，并提出Probabilistic CoreSet（PCoreSet）策略，以在概率空间中最大化覆盖。

Result: 在11个数据集上的评估表明，PCoreSet在ActiveKD框架中表现优于现有选择方法。

Conclusion: 结论是ActiveKD框架成功地将AL与KD结合，通过PCoreSet策略提高了在有限标注预算下的知识传递效率。

Abstract: Knowledge distillation (KD) is a widely used framework for training compact,
task-specific models by leveraging the knowledge of teacher models. However,
its application to active learning (AL), which aims to minimize annotation
costs through iterative sample selection, remains underexplored. This gap stems
from the fact that KD typically assumes access to sufficient labeled data,
whereas AL operates in data-scarce scenarios where task-specific teacher models
are often unavailable. In this paper, we introduce ActiveKD, a framework that
integrates AL with KD by leveraging the zero- and few-shot capabilities of
large vision-language models (VLMs). A key aspect of ActiveKD is the structured
prediction bias of VLMs -- i.e., their predictions form clusters in the
probability space. We regard this structure as an inductive bias of the teacher
model, capturing generalizable output patterns beneficial to student learning.
To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection
strategy that maximizes coverage in the probability space rather than the
feature space. PCoreSet strategically selects categorically diverse unlabeled
samples, facilitating more efficient transfer of teacher knowledge under
limited annotation budgets. Evaluations on 11 datasets show that PCoreSet
consistently outperforms existing selection methods within the ActiveKD
framework, advancing research at the intersection of AL and KD.

</details>


### [349] [Q-learning with Posterior Sampling](https://arxiv.org/abs/2506.00917)
*Priyank Agrawal,Shipra Agrawal,Azmat Azati*

Key words: 贝叶斯后验采样, Q学习, 强化学习, Thompson采样, 后悔界

TL;DR: 该论文提出了一种基于Q学习的算法PSQL，结合高斯后验采样进行探索，在表格型MDP中实现了接近已知下界的后悔界。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 贝叶斯后验采样在探索-利用场景中表现优异，但理论分析尤其在强化学习中仍具挑战性，本文旨在解决此问题。

Method: 提出Q-Learning with Posterior Sampling (PSQL)，利用高斯后验采样Q值进行探索。

Result: 在表格型MDP中，PSQL的后悔界为$	ilde O(H^2\sqrt{SAT})$，接近已知下界。

Conclusion: PSQL为后验采样与动态规划的结合提供了新思路，有望推动更复杂RL场景的研究。

Abstract: Bayesian posterior sampling techniques have demonstrated superior empirical
performance in many exploration-exploitation settings. However, their
theoretical analysis remains a challenge, especially in complex settings like
reinforcement learning. In this paper, we introduce Q-Learning with Posterior
Sampling (PSQL), a simple Q-learning-based algorithm that uses Gaussian
posteriors on Q-values for exploration, akin to the popular Thompson Sampling
algorithm in the multi-armed bandit setting. We show that in the tabular
episodic MDP setting, PSQL achieves a regret bound of $\tilde
O(H^2\sqrt{SAT})$, closely matching the known lower bound of
$\Omega(H\sqrt{SAT})$. Here, S, A denote the number of states and actions in
the underlying Markov Decision Process (MDP), and $T=KH$ with $K$ being the
number of episodes and $H$ being the planning horizon. Our work provides
several new technical insights into the core challenges in combining posterior
sampling with dynamic programming and TD-learning-based RL algorithms, along
with novel ideas for resolving those difficulties. We hope this will form a
starting point for analyzing this efficient and important algorithmic technique
in even more complex RL settings.

</details>


### [350] [Principled Input-Output-Conditioned Post-Hoc Uncertainty Estimation for Regression Networks](https://arxiv.org/abs/2506.00918)
*Lennart Bramlage,Cristóbal Curio*

Key words: 不确定性量化,后验估计,回归任务,最大似然估计,OOD检测

TL;DR: 提出了一种理论支持的框架，通过拟合辅助模型来估计回归任务中的不确定性，无需访问模型参数或梯度，且显著提升了OOD检测和度量性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在安全敏感应用中量化不确定性至关重要，但现有神经网络常因性能影响而忽略这一点。现有后处理方法通常需要访问模型参数或梯度，实用性受限。

Method: 通过拟合辅助模型，结合原始输入和冻结模型输出，利用最大似然估计和顺序参数拟合，形式化后优化目标，无需推理时的采样或近似。

Result: 使用多样化的辅助数据（如增强的子集）显著提升了OOD检测和度量性能，验证了冻结模型输出包含关于模型误差和预测不确定性的广义潜在信息。

Conclusion: 提出的方法在不依赖基础模型预测的情况下，有效估计了输入相关的不确定性，并通过实验验证了其优越性。

Abstract: Uncertainty quantification is critical in safety-sensitive applications but
is often omitted from off-the-shelf neural networks due to adverse effects on
predictive performance. Retrofitting uncertainty estimates post-hoc typically
requires access to model parameters or gradients, limiting feasibility in
practice. We propose a theoretically grounded framework for post-hoc
uncertainty estimation in regression tasks by fitting an auxiliary model to
both original inputs and frozen model outputs. Drawing from principles of
maximum likelihood estimation and sequential parameter fitting, we formalize an
exact post-hoc optimization objective that recovers the canonical MLE of
Gaussian parameters, without requiring sampling or approximation at inference.
While prior work has used model outputs to estimate uncertainty, we explicitly
characterize the conditions under which this is valid and demonstrate the
extent to which structured outputs can support quasi-epistemic inference. We
find that using diverse auxiliary data, such as augmented subsets of the
original training data, significantly enhances OOD detection and metric
performance. Our hypothesis that frozen model outputs contain generalizable
latent information about model error and predictive uncertainty is tested and
confirmed. Finally, we ensure that our method maintains proper estimation of
input-dependent uncertainty without relying exclusively on base model
forecasts. These findings are demonstrated in toy problems and adapted to both
UCI and depth regression benchmarks. Code: https://github.com/biggzlar/IO-CUE.

</details>


### [351] [Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation](https://arxiv.org/abs/2506.00920)
*Philip Heejun Lee*

Key words: PRISM, Transformer, 长度外推, 位置编码, 概率叠加

TL;DR: PRISM是一种新型位置编码机制，使Transformer能准确外推至训练长度的10倍，解决了序列模型在测试长度超出训练范围时的性能退化问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 测试序列显著超出训练长度时，深度序列模型的准确性通常会下降，而许多关键任务（如算法推理、多步算术和组合泛化）需要稳健的长度外推能力。

Method: PRISM通过可微分直方图滤波更新学习连续相对位置，采用概率叠加而非确定性嵌入来保留位置不确定性。

Result: PRISM在算法基准测试中实现了最先进的长度外推能力，成功泛化到以前难以处理的序列长度。

Conclusion: PRISM的随机位置编码保持了清晰且可解释的内部状态，为可靠的长度泛化提供了理论基础。

Abstract: Deep sequence models typically degrade in accuracy when test sequences
significantly exceed their training lengths, yet many critical tasks--such as
algorithmic reasoning, multi-step arithmetic, and compositional
generalization--require robust length extrapolation. We introduce PRISM, a
Probabilistic Relative-position Implicit Superposition Model, a novel
positional encoding mechanism that enables Transformers to extrapolate
accurately up to 10x beyond their training length. PRISM learns continuous
relative positions through a differentiable histogram-filter update, preserving
position uncertainty via a probabilistic superposition rather than conventional
deterministic embeddings. Empirically, PRISM achieves state-of-the-art length
extrapolation, successfully generalizing to previously intractable sequence
lengths across algorithmic benchmarks--including arithmetic (addition,
multiplication), SCAN compositionality tasks, and complex copy variants derived
from DeepMind's recent datasets. Our analysis demonstrates that PRISM's
stochastic positional encoding maintains sharp and interpretable internal
states, providing a theoretical basis for reliable length generalization. These
results advance the goal of neural sequence models that remain algorithmically
robust at lengths far exceeding their training horizon.

</details>


### [352] [Addressing the Collaboration Dilemma in Low-Data Federated Learning via Transient Sparsity](https://arxiv.org/abs/2506.00932)
*Qiao Xiao,Boqian Wu,Andrey Poddubnyy,Elena Mocanu,Phuong H. Nguyen,Mykola Pechenizkiy,Decebal Constantin Mocanu*

Key words: 联邦学习, 层间惯性, 稀疏性, 全局聚合

TL;DR: 论文提出了一种名为LIPS的方法，用于解决联邦学习中出现的层间惯性现象，通过引入稀疏性激励更新，提升全局聚合效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习中数据异构性和本地数据集有限的问题导致中间层更新不足，影响全局模型效果，研究者试图解决这一现象。

Method: 提出LIPS方法，周期性引入瞬态稀疏性以激励有效更新，从而增强全局聚合效果。

Result: 实验表明，LIPS能有效缓解层间惯性，提升联邦学习中的聚合效果和整体性能。

Conclusion: LIPS不仅深化了对联邦学习中层间动态的理解，还为资源受限环境下的协作策略提供了新思路。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients while preserving data privacy, leveraging aggregated
updates to build robust global models. However, this training paradigm faces
significant challenges due to data heterogeneity and limited local datasets,
which often impede effective collaboration. In such scenarios, we identify the
Layer-wise Inertia Phenomenon in FL, wherein the middle layers of global model
undergo minimal updates after early communication rounds, ultimately limiting
the effectiveness of global aggregation. We demonstrate the presence of this
phenomenon across a wide range of federated settings, spanning diverse datasets
and architectures. To address this issue, we propose LIPS (Layer-wise Inertia
Phenomenon with Sparsity), a simple yet effective method that periodically
introduces transient sparsity to stimulate meaningful updates and empower
global aggregation. Experiments demonstrate that LIPS effectively mitigates
layer-wise inertia, enhances aggregation effectiveness, and improves overall
performance in various FL scenarios. This work not only deepens the
understanding of layer-wise learning dynamics in FL but also paves the way for
more effective collaboration strategies in resource-constrained environments.
Our code is publicly available at: https://github.com/QiaoXiao7282/LIPS.

</details>


### [353] [Uncertainty-Aware Metabolic Stability Prediction with Dual-View Contrastive Learning](https://arxiv.org/abs/2506.00936)
*Peijin Guo,Minghui Li,Hewen Pan,Bowen Chen,Yang Wu,Zikang Guo,Leo Yu Zhang,Shengshan Hu,Shengqing Hu*

Key words: 代谢稳定性, 图神经网络, 对比学习, 不确定性量化, 药物研发

TL;DR: 提出了TrustworthyMS框架，通过对比学习和不确定性量化改进分子代谢稳定性预测。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前方法在分子建模和不确定性量化方面存在不足，影响了代谢稳定性预测的准确性。

Method: 提出分子图拓扑重映射机制、对比拓扑键对齐和Beta-Binomial不确定性量化模型。

Result: TrustworthyMS在预测性能上优于现有方法。

Conclusion: 该框架有效解决了分子代谢稳定性预测中的关键挑战。

Abstract: Accurate prediction of molecular metabolic stability (MS) is critical for
drug research and development but remains challenging due to the complex
interplay of molecular interactions. Despite recent advances in graph neural
networks (GNNs) for MS prediction, current approaches face two critical
limitations: (1) incomplete molecular modeling due to atom-centric
message-passing mechanisms that disregard bond-level topological features, and
(2) prediction frameworks that lack reliable uncertainty quantification. To
address these challenges, we propose TrustworthyMS, a novel contrastive
learning framework designed for uncertainty-aware metabolic stability
prediction. First, a molecular graph topology remapping mechanism synchronizes
atom-bond interactions through edge-induced feature propagation, capturing both
localized electronic effects and global conformational constraints. Second,
contrastive topology-bond alignment enforces consistency between molecular
topology views and bond patterns via feature alignment, enhancing
representation robustness. Third, uncertainty modeling through Beta-Binomial
uncertainty quantification enables simultaneous prediction and confidence
calibration under epistemic uncertainty. Through extensive experiments, our
results demonstrate that TrustworthyMS outperforms current state-of-the-art
methods in terms of predictive performance.

</details>


### [354] [Hidden Representation Clustering with Multi-Task Representation Learning towards Robust Online Budget Allocation](https://arxiv.org/abs/2506.00959)
*Xiaohan Wang,Yu Zhang,Guibin Jiang,Bing Cheng,Wei Lin*

Key words: 营销优化, 预算分配, 多任务表示网络, 聚类, 整数随机规划

TL;DR: 该论文提出了一种基于聚类的营销预算分配方法，通过多任务表示网络学习个体属性并进行聚类，解决了传统方法在大规模数据下面临的预测和优化问题，实验验证其优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决营销优化中大规模数据下的预测和优化问题，特别是在数据质量不可控且噪声较大的工业场景中。

Method: 提出多任务表示网络学习个体属性，通过聚类将原始特征分为K组，并转化为整数随机规划问题。进一步将模块蒸馏为多类别模型以便在线部署。

Result: 离线实验表明优于六种先进方法；在线A/B测试在订单量和GMV上分别提升0.53%和0.65%。

Conclusion: 基于聚类的方法在营销预算分配中表现优越，适用于工业场景。

Abstract: Marketing optimization, commonly formulated as an online budget allocation
problem, has emerged as a pivotal factor in driving user growth. Most existing
research addresses this problem by following the principle of 'first predict
then optimize' for each individual, which presents challenges related to
large-scale counterfactual prediction and solving complexity trade-offs. Note
that the practical data quality is uncontrollable, and the solving scale tends
to be tens of millions. Therefore, the existing approaches make the robust
budget allocation non-trivial, especially in industrial scenarios with
considerable data noise. To this end, this paper proposes a novel approach that
solves the problem from the cluster perspective. Specifically, we propose a
multi-task representation network to learn the inherent attributes of
individuals and project the original features into high-dimension hidden
representations through the first two layers of the trained network. Then, we
divide these hidden representations into $K$ groups through partitioning-based
clustering, thus reformulating the problem as an integer stochastic programming
problem under different total budgets. Finally, we distill the representation
module and clustering model into a multi-category model to facilitate online
deployment. Offline experiments validate the effectiveness and superiority of
our approach compared to six state-of-the-art marketing optimization
algorithms. Online A/B tests on the Meituan platform indicate that the approach
outperforms the online algorithm by 0.53% and 0.65%, considering order volume
(OV) and gross merchandise volume (GMV), respectively.

</details>


### [355] [Enhancing Parallelism in Decentralized Stochastic Convex Optimization](https://arxiv.org/abs/2506.00961)
*Ofri Eisen,Ron Dorfman,Kfir Y. Levy*

Key words: 去中心化学习,并行性,随机梯度下降,凸优化,分布式学习

TL;DR: 提出了一种名为Decentralized Anytime SGD的新算法，显著提升了去中心化学习的并行能力，克服了传统方法在机器数量增加时收敛速度下降的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 去中心化学习在处理大规模数据集时具有通信效率高的优势，但随着机器数量的增加，其收敛速度会受到负面影响。

Method: 在随机凸优化（SCO）框架下，提出Decentralized Anytime SGD算法，扩展了并行性阈值。

Result: 理论分析表明，该算法突破了现有技术的并行性上限，使得更大规模的网络能够实现良好的统计保证，并在高度连接的拓扑中缩小了与中心化学习的差距。

Conclusion: Decentralized Anytime SGD显著提升了去中心化学习的可扩展性和性能，为大规模分布式学习提供了新思路。

Abstract: Decentralized learning has emerged as a powerful approach for handling large
datasets across multiple machines in a communication-efficient manner. However,
such methods often face scalability limitations, as increasing the number of
machines beyond a certain point negatively impacts convergence rates. In this
work, we propose Decentralized Anytime SGD, a novel decentralized learning
algorithm that significantly extends the critical parallelism threshold,
enabling the effective use of more machines without compromising performance.
Within the stochastic convex optimization (SCO) framework, we establish a
theoretical upper bound on parallelism that surpasses the current
state-of-the-art, allowing larger networks to achieve favorable statistical
guarantees and closing the gap with centralized learning in highly connected
topologies.

</details>


### [356] [Reinforcement Learning with Random Time Horizons](https://arxiv.org/abs/2506.00962)
*Enric Ribera Borrell,Lorenz Richter,Christof Schütte*

Key words: 强化学习, 策略梯度, 随机停止时间, 最优控制

TL;DR: 论文将标准强化学习框架扩展到随机时间范围，分析了随机停止时间对策略梯度公式的影响，并提出两种补充视角，实验证明其方法能显著提高优化收敛性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现实世界中许多应用具有随机停止时间，而经典强化学习框架未考虑这种特性，因此需要扩展框架以适应实际需求。

Method: 采用轨迹和状态空间两种互补视角，推导随机和确定性策略下的策略梯度公式，并与最优控制理论建立联系。

Result: 数值实验表明，与传统方法相比，提出的策略梯度公式能显著提升优化收敛性。

Conclusion: 随机时间范围的扩展为强化学习提供了更灵活的框架，适用于更多实际应用场景。

Abstract: We extend the standard reinforcement learning framework to random time
horizons. While the classical setting typically assumes finite and
deterministic or infinite runtimes of trajectories, we argue that multiple
real-world applications naturally exhibit random (potentially
trajectory-dependent) stopping times. Since those stopping times typically
depend on the policy, their randomness has an effect on policy gradient
formulas, which we (mostly for the first time) derive rigorously in this work
both for stochastic and deterministic policies. We present two complementary
perspectives, trajectory or state-space based, and establish connections to
optimal control theory. Our numerical experiments demonstrate that using the
proposed formulas can significantly improve optimization convergence compared
to traditional approaches.

</details>


### [357] [Pilot Contamination-Aware Graph Attention Network for Power Control in CFmMIMO](https://arxiv.org/abs/2506.00967)
*Tingting Zhang,Sergiy A. Vorobyov,David J. Love,Taejoon Kim,Kai Dong*

Key words: 功率控制、图注意力网络、自监督学习、CFmMIMO、导频污染

TL;DR: 提出了一种基于图注意力网络的自监督下行功率控制方法，解决了现有方法中正交性假设不现实、用户设备数量动态变化以及监督训练成本高的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有基于优化的功率控制算法计算复杂度高，不适用于实时应用；而现有的基于图神经网络的方法假设过于理想化，且无法适应动态变化的用户设备数量。

Method: 采用图注意力网络（GNN），以自监督方式运行，有效处理导频污染并适应动态用户设备数量。

Result: 实验证明该方法有效，甚至优于作为基线的加速投影梯度法。

Conclusion: 自监督图注意力网络为CFmMIMO系统中的功率控制问题提供了一种高效且实用的解决方案。

Abstract: Optimization-based power control algorithms are predominantly iterative with
high computational complexity, making them impractical for real-time
applications in cell-free massive multiple-input multiple-output (CFmMIMO)
systems. Learning-based methods have emerged as a promising alternative, and
among them, graph neural networks (GNNs) have demonstrated their excellent
performance in solving power control problems. However, all existing GNN-based
approaches assume ideal orthogonality among pilot sequences for user equipments
(UEs), which is unrealistic given that the number of UEs exceeds the available
orthogonal pilot sequences in CFmMIMO schemes. Moreover, most learning-based
methods assume a fixed number of UEs, whereas the number of active UEs varies
over time in practice. Additionally, supervised training necessitates costly
computational resources for computing the target power control solutions for a
large volume of training samples. To address these issues, we propose a graph
attention network for downlink power control in CFmMIMO systems that operates
in a self-supervised manner while effectively handling pilot contamination and
adapting to a dynamic number of UEs. Experimental results show its
effectiveness, even in comparison to the optimal accelerated projected gradient
method as a baseline.

</details>


### [358] [Data Heterogeneity Modeling for Trustworthy Machine Learning](https://arxiv.org/abs/2506.00969)
*Jiashuo Liu,Peng Cui*

Key words: 数据异质性, 机器学习, 模型鲁棒性, 公平性, 数据多样性

TL;DR: 本文综述了异质性感知机器学习的重要性，强调其在多个关键领域中的应用潜力，并提出未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统机器学习算法通常忽略数据异质性，导致不可靠的决策和泛化问题，因此需要一种更细致的方法来建模异质性。

Method: 通过在整个机器学习流程（从数据收集到模型部署）中系统整合数据异质性考量，提出异质性感知机器学习范式。

Result: 异质性感知机器学习在医疗、农业、金融和推荐系统等领域展现出显著的模型鲁棒性、公平性和可靠性提升。

Conclusion: 深入研究数据多样性能改善模型性能，并为数据挖掘社区提供未来研究机会。

Abstract: Data heterogeneity plays a pivotal role in determining the performance of
machine learning (ML) systems. Traditional algorithms, which are typically
designed to optimize average performance, often overlook the intrinsic
diversity within datasets. This oversight can lead to a myriad of issues,
including unreliable decision-making, inadequate generalization across
different domains, unfair outcomes, and false scientific inferences. Hence, a
nuanced approach to modeling data heterogeneity is essential for the
development of dependable, data-driven systems. In this survey paper, we
present a thorough exploration of heterogeneity-aware machine learning, a
paradigm that systematically integrates considerations of data heterogeneity
throughout the entire ML pipeline -- from data collection and model training to
model evaluation and deployment. By applying this approach to a variety of
critical fields, including healthcare, agriculture, finance, and recommendation
systems, we demonstrate the substantial benefits and potential of
heterogeneity-aware ML. These applications underscore how a deeper
understanding of data diversity can enhance model robustness, fairness, and
reliability and help model diagnosis and improvements. Moreover, we delve into
future directions and provide research opportunities for the whole data mining
community, aiming to promote the development of heterogeneity-aware ML.

</details>


### [359] [Quantization-based Bounds on the Wasserstein Metric](https://arxiv.org/abs/2506.00976)
*Jonathan Bobrutsky,Amit Moscovich*

Key words: Wasserstein度量, Kantorovich问题, 近似方法, 熵正则化, 上采样

TL;DR: 该论文提出了一种在规则网格上计算Wasserstein度量的高效近似方法，通过粗网格上的Kantorovich问题求解和上采样校正，实现了严格的上界或下界，速度比熵正则化方法快10-100倍，误差控制在2%以内。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Wasserstein度量在机器学习的多个应用中具有重要作用，但其计算成本高昂，因此需要高效的近似方法。

Method: 在规则网格上，通过粗网格的Kantorovich问题求解、量化测度和特殊设计的成本矩阵，结合上采样和校正阶段，得到全分辨率输入Wasserstein度量的严格上下界。

Result: 在DOTmark基准测试中，速度比熵正则化方法快10-100倍，误差低于2%。

Conclusion: 该方法在计算效率和精度之间取得了良好平衡，适用于需要高效Wasserstein近似的大规模应用。

Abstract: The Wasserstein metric has become increasingly important in many machine
learning applications such as generative modeling, image retrieval and domain
adaptation. Despite its appeal, it is often too costly to compute. This has
motivated approximation methods like entropy-regularized optimal transport,
downsampling, and subsampling, which trade accuracy for computational
efficiency. In this paper, we consider the challenge of computing efficient
approximations to the Wasserstein metric that also serve as strict upper or
lower bounds. Focusing on discrete measures on regular grids, our approach
involves formulating and exactly solving a Kantorovich problem on a coarse grid
using a quantized measure and specially designed cost matrix, followed by an
upscaling and correction stage. This is done either in the primal or dual space
to obtain valid upper and lower bounds on the Wasserstein metric of the
full-resolution inputs. We evaluate our methods on the DOTmark optimal
transport images benchmark, demonstrating a 10x-100x speedup compared to
entropy-regularized OT while keeping the approximation error below 2%.

</details>


### [360] [LoRA-BAM: Input Filtering for Fine-tuned LLMs via Boxed Abstraction Monitors over LoRA Layers](https://arxiv.org/abs/2506.00998)
*Changshun Wu,Tianyi Duan,Saddek Bensalem,Chih-Hong Cheng*

Key words: 大型语言模型, 分布外检测, LoRA, 盒抽象, 正则化

TL;DR: LoRA-BAM是一种通过盒抽象在LoRA层添加OoD检测监视器的方法，用于过滤超出模型能力的查询，并引入正则化损失以提高可解释性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 微调大型语言模型（LLMs）虽然能提升特定领域任务的性能，但可能导致过拟合，使其在分布外（OoD）查询上不可靠。

Method: 从微调数据中提取特征向量并聚类，将聚类结果封装为盒子；若问题的特征向量落在所有盒子外则标记为OoD。引入正则化损失以鼓励近义问题在特征空间中相近。

Result: LoRA-BAM提供了轻量级且可解释的OoD检测方法，补充了现有防御手段。

Conclusion: 该方法通过盒抽象和正则化提升了OoD检测的鲁棒性和可解释性。

Abstract: Fine-tuning large language models (LLMs) improves performance on
domain-specific tasks but can lead to overfitting, making them unreliable on
out-of-distribution (OoD) queries. We propose LoRA-BAM - a method that adds OoD
detection monitors to the LoRA layer using boxed abstraction to filter
questions beyond the model's competence. Feature vectors from the fine-tuning
data are extracted via the LLM and clustered. Clusters are enclosed in boxes; a
question is flagged as OoD if its feature vector falls outside all boxes. To
improve interpretability and robustness, we introduce a regularization loss
during fine-tuning that encourages paraphrased questions to stay close in the
feature space, and the enlargement of the decision boundary is based on the
feature variance within a cluster. Our method complements existing defenses by
providing lightweight and interpretable OoD detection.

</details>


### [361] [Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts](https://arxiv.org/abs/2506.01000)
*Chengyi Cai,Zesheng Ye,Lei Feng,Jianzhong Qi,Feng Liu*

Key words: 模型重新编程、视觉提示、解耦、概率重加权、分类

TL;DR: 模型重新编程通过修改输入和输出空间来适应预训练模型到下游任务。视觉重新编程（VR）是一种为视觉任务添加可训练噪声模式（即视觉提示）以辅助分类的方法。现有方法使用所有类别描述训练单个视觉提示，可能导致多样性捕捉不足和偏见。本文提出解耦和重加权框架（DVP），通过分组优化提示并集成输出，降低了经验风险界限，实验表现优于基线。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有视觉重新编程方法使用单一视觉提示，可能无法捕捉多样属性并偏向非信息属性。需要一种新方法以更好地适应和解释下游分类任务。

Method: 提出解耦视觉提示（DVP），通过分组描述优化提示（DVP-cse或DVP-cls），并集成输出使用概率重加权矩阵（PRM）衡量其对分类的贡献。

Result: DVP在11个下游数据集上平均表现优于基线，PRM集成提供了对视觉提示影响的概率解释。

Conclusion: 解耦和重加权框架提高了视觉重新编程的效果，并提供了可解释的分类决策依据。

Abstract: Model reprogramming adapts pretrained models to downstream tasks by modifying
only the input and output spaces. Visual reprogramming (VR) is one instance for
vision tasks that adds a trainable noise pattern (i.e., a visual prompt) to
input images to facilitate downstream classification. The existing VR
approaches for CLIP train a single visual prompt using all descriptions of
different downstream classes. However, the limited learning capacity may result
in (1) a failure to capture diverse aspects of the descriptions (e.g., shape,
color, and texture), and (2) a possible bias toward less informative attributes
that do not help distinguish between classes. In this paper, we introduce a
decoupling-and-reweighting framework. Our decoupled visual prompts (DVP) are
optimized using descriptions grouped by explicit causes (DVP-cse) or
unsupervised clusters (DVP-cls). Then, we integrate the outputs of these visual
prompts with a probabilistic reweighting matrix (PRM) that measures their
contributions to each downstream class. Theoretically, DVP lowers the empirical
risk bound. Experimentally, DVP outperforms baselines on average across 11
downstream datasets. Notably, the DVP-PRM integration enables insights into how
individual visual prompts influence classification decisions, providing a
probabilistic framework for understanding reprogramming. Our code is available
at https://github.com/tmlr-group/DecoupledVP.

</details>


### [362] [Optimistic critics can empower small actors](https://arxiv.org/abs/2506.01016)
*Olya Mastikhina,Dhruv Sreenivas,Pablo Samuel Castro*

Key words: actor-critic, deep reinforcement learning, asymmetric architecture, value underestimation

TL;DR: 论文研究了在深度强化学习中，使用更小的actor网络的不对称架构对性能的影响，发现这可能导致性能下降和critic过拟合，并探讨了缓解方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨actor-critic方法中不对称（更小actor）架构的影响及其可能带来的性能问题。

Method: 通过广泛的实证研究和分析，比较对称和不对称架构的性能差异，并分析其背后的原因。

Result: 更小的actor会导致性能下降和critic过拟合，主要原因是低估价值的策略导致数据收集不足。

Conclusion: 提出了缓解低估价值的方法，为不对称actor-critic方法的进一步研究提供了方向。

Abstract: Actor-critic methods have been central to many of the recent advances in deep
reinforcement learning. The most common approach is to use symmetric
architectures, whereby both actor and critic have the same network topology and
number of parameters. However, recent works have argued for the advantages of
asymmetric setups, specifically with the use of smaller actors. We perform
broad empirical investigations and analyses to better understand the
implications of this and find that, in general, smaller actors result in
performance degradation and overfit critics. Our analyses suggest poor data
collection, due to value underestimation, as one of the main causes for this
behavior, and further highlight the crucial role the critic can play in
alleviating this pathology. We explore techniques to mitigate the observed
value underestimation, which enables further research in asymmetric
actor-critic methods.

</details>


### [363] [Taming LLMs by Scaling Learning Rates with Gradient Grouping](https://arxiv.org/abs/2506.01049)
*Siyuan Li,Juanxi Tian,Zedong Wang,Xin Jin,Zicheng Liu,Wentao Zhang,Dan Xu*

Key words: 大语言模型, 优化器, 学习率估计, 梯度分组, 训练稳定性

TL;DR: SGG（基于梯度分组的缩放）是一种优化器包装器，通过动态分组和组特定缩放改进自适应学习率估计，提升LLM训练的稳定性和效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前自适应优化器在处理大规模语言模型时，面临学习率估计效率低、训练不稳定及与参数高效微调技术兼容性差的问题。

Method: SGG通过梯度统计动态分组和组特定缩放校准学习率，实现集体组级约束与精确参数适应的平衡。

Result: 实验表明，SGG能无缝集成现有优化器，在不同模型规模下实现更快的收敛和稳定表现。

Conclusion: SGG为LLM优化提供了一种高效且稳定的解决方案。

Abstract: Training large language models (LLMs) poses challenges due to their massive
scale and heterogeneous architectures. While adaptive optimizers like AdamW
help address gradient variations, they still struggle with efficient and
effective parameter-wise learning rate estimation, resulting in training
instability, slow convergence, and poor compatibility with parameter-efficient
fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient
Grouping (SGG), an optimizer wrapper that improves adaptive learning rate
estimation by dynamic grouping and group-specific scaling. SGG first groups
gradient statistics in each layer into clusters and then applies
cluster-specific scaling to calibrate learning rates for each parameter, thus
imposing collective group-wise constraints while maintaining precise
per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that
SGG integrates seamlessly with existing optimizers, and offers consistent gains
and faster convergence over baselines, with various model sizes. Its stability
across varying batch sizes and learning rates establishes SGG as a robust
choice for LLM optimization.

</details>


### [364] [A Finite-Time Analysis of TD Learning with Linear Function Approximation without Projections nor Strong Convexity](https://arxiv.org/abs/2506.01052)
*Wei-Cheng Lee,Francesco Orabona*

Key words: TD学习,线性函数逼近,收敛性分析,自限界特性,马尔可夫噪声

TL;DR: 本文研究了线性函数逼近下TD学习的有限时间收敛特性，挑战了传统假设并提出了无投影变体的收敛性证明。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统TD学习的收敛性分析依赖于人为假设（如投影到有界集或需强凸常数），与实际情况不符，本文旨在挑战这些假设。

Method: 使用无投影变体的TD学习，并利用其自限界特性分析收敛性。

Result: 在马尔可夫噪声下，无投影TD学习的收敛速率为$	ilde{\mathcal{O}}(\frac{||\theta^*||^2_2}{\sqrt{T}})$。

Conclusion: 研究表明，无需传统假设即可实现TD学习的收敛，为实际应用提供了理论支持。

Abstract: We investigate the finite-time convergence properties of Temporal Difference
(TD) learning with linear function approximation, a cornerstone algorithm in
reinforcement learning. While prior work has established convergence
guarantees, these results typically rely on the assumption that each iterate is
projected onto a bounded set or that the learning rate is set according to the
unknown strong convexity constant -- conditions that are both artificial and do
not match the current practice.
  In this paper, we challenge the necessity of such assumptions and present a
refined analysis of TD learning. We show that the simple projection-free
variant converges with a rate of
$\tilde{\mathcal{O}}(\frac{||\theta^*||^2_2}{\sqrt{T}})$, even in the presence
of Markovian noise. Our analysis reveals a novel self-bounding property of the
TD updates and exploits it to guarantee bounded iterates.

</details>


### [365] [No Soundness in the Real World: On the Challenges of the Verification of Deployed Neural Networks](https://arxiv.org/abs/2506.01054)
*Attila Szász,Balázs Bánhelyi,Márk Jelasity*

Key words: 神经网络验证、浮点运算、准确性、对抗攻击、部署环境

TL;DR: 本文指出当前最先进的神经网络验证器未能真正保证安全性，指出理论上的准确性（使用浮点计算时界定全精度输出）与实际上的准确性（在潜在随机环境中界定浮点输出）之间存在差距，并通过实验验证了这一点。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨现有神经网络验证器的局限性，揭示其在实践中无法保证安全性的问题。

Method: 通过理论分析和实验验证，测试多种验证方法，并设计针对部署环境的对抗网络攻击。

Result: 所有测试的验证器均易受针对部署环境的攻击，表明其不具备实际上的准确性。

Conclusion: 当前验证方法存在重大缺陷，需要更强大的计算能力才能实现实际上的准确性。

Abstract: The ultimate goal of verification is to guarantee the safety of deployed
neural networks. Here, we claim that all the state-of-the-art verifiers we are
aware of fail to reach this goal. Our key insight is that theoretical soundness
(bounding the full-precision output while computing with floating point) does
not imply practical soundness (bounding the floating point output in a
potentially stochastic environment). We prove this observation for the
approaches that are currently used to achieve provable theoretical soundness,
such as interval analysis and its variants. We also argue that achieving
practical soundness is significantly harder computationally. We support our
claims empirically as well by evaluating several well-known verification
methods. To mislead the verifiers, we create adversarial networks that detect
and exploit features of the deployment environment, such as the order and
precision of floating point operations. We demonstrate that all the tested
verifiers are vulnerable to our new deployment-specific attacks, which proves
that they are not practically sound.

</details>


### [366] [XAI-Units: Benchmarking Explainability Methods with Unit Tests](https://arxiv.org/abs/2506.01059)
*Jun Rui Lee,Sadegh Emami,Michael David Hollins,Timothy C. H. Wong,Carlos Ignacio Villalobos Sánchez,Francesca Toni,Dekai Zhang,Adam Dejl*

Key words: 特征归因,可解释AI,基准测试,模型行为,评估

TL;DR: XAI-Units是一个开源基准测试，旨在评估特征归因（FA）方法，通过模拟多样化的模型行为如特征交互和输出不连续性，提供标准化评估。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在缺乏真实基准或深入模型知识的情况下，难以判断不同FA方法的优劣，因此需一个系统化的评估工具。

Method: 开发XAI-Units基准测试，包含已知内部机制的数据集和模型，并集成多种评价指标。

Result: 通过合成数据和模型生成，提供客观的FA方法对比，揭示不同方法在各类模型推理中的表现。

Conclusion: XAI-Units为FA方法评估提供标准化框架，助力可解释AI的可靠性提升。

Abstract: Feature attribution (FA) methods are widely used in explainable AI (XAI) to
help users understand how the inputs of a machine learning model contribute to
its outputs. However, different FA models often provide disagreeing importance
scores for the same model. In the absence of ground truth or in-depth knowledge
about the inner workings of the model, it is often difficult to meaningfully
determine which of the different FA methods produce more suitable explanations
in different contexts. As a step towards addressing this issue, we introduce
the open-source XAI-Units benchmark, specifically designed to evaluate FA
methods against diverse types of model behaviours, such as feature
interactions, cancellations, and discontinuous outputs. Our benchmark provides
a set of paired datasets and models with known internal mechanisms,
establishing clear expectations for desirable attribution scores. Accompanied
by a suite of built-in evaluation metrics, XAI-Units streamlines systematic
experimentation and reveals how FA methods perform against distinct, atomic
kinds of model reasoning, similar to unit tests in software engineering.
Crucially, by using procedurally generated models tied to synthetic datasets,
we pave the way towards an objective and reliable comparison of FA methods.

</details>


### [367] [Reconsidering LLM Uncertainty Estimation Methods in the Wild](https://arxiv.org/abs/2506.01114)
*Yavuz Bakman,Duygu Nur Yaldiz,Sungmin Kang,Tuo Zhang,Baturalp Buyukates,Salman Avestimehr,Sai Praneeth Karimireddy*

Key words: LLM, Uncertainty Estimation, Threshold Sensitivity, Adversarial Prompts, Long-form Generation

TL;DR: 本文系统评估了大型语言模型（LLM）不确定性估计（UE）方法在实践部署中的四个关键方面，发现现有方法对阈值选择敏感且容易受到对抗性提示的影响，但展示了在长文本生成和多分数集成中的潜力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有UE方法多在短问答场景中评估，而实际部署中面临多种挑战，作者探究了这些方法在实践中的表现。

Method: 通过测试（1）阈值选择敏感性，（2）对查询变换的鲁棒性，（3）长文本生成适用性，（4）多分数处理策略，评估了19种UE方法。

Result: 发现大多数方法对阈值选择敏感，对抗性提示易受影响，但对长文本生成和多分数集成表现有潜力。

Conclusion: 现有UE方法在实践部署中仍需改进，多分数集成策略展示了实用价值。

Abstract: Large Language Model (LLM) Uncertainty Estimation (UE) methods have become a
crucial tool for detecting hallucinations in recent years. While numerous UE
methods have been proposed, most existing studies evaluate them in isolated
short-form QA settings using threshold-independent metrics such as AUROC or
PRR. However, real-world deployment of UE methods introduces several
challenges. In this work, we systematically examine four key aspects of
deploying UE methods in practical settings. Specifically, we assess (1) the
sensitivity of UE methods to decision threshold selection, (2) their robustness
to query transformations such as typos, adversarial prompts, and prior chat
history, (3) their applicability to long-form generation, and (4) strategies
for handling multiple UE scores for a single query. Our evaluations on 19 UE
methods reveal that most of them are highly sensitive to threshold selection
when there is a distribution shift in the calibration dataset. While these
methods generally exhibit robustness against previous chat history and typos,
they are significantly vulnerable to adversarial prompts. Additionally, while
existing UE methods can be adapted for long-form generation through various
strategies, there remains considerable room for improvement. Lastly, ensembling
multiple UE scores at test time provides a notable performance boost, which
highlights its potential as a practical improvement strategy. Code is available
at: https://github.com/duygunuryldz/uncertainty_in_the_wild.

</details>


### [368] [Attention Retrieves, MLP Memorizes: Disentangling Trainable Components in the Transformer](https://arxiv.org/abs/2506.01115)
*Yihe Dong,Lorenzo Noci,Mikhail Khodak,Mufan Li*

Key words: Transformer, Attention Mechanism, Algorithmic Tasks, MixiT, Inductive Biases

TL;DR: Transformer的注意力机制并非其成功的关键因素，研究表明固定注意力系数或冻结部分组件的简化模型在某些任务中表现与完整模型相当。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨Transformer中注意力机制对模型性能的真实贡献，尤其是在算法任务中的表现。

Method: 通过冻结MLP层或注意力投影器，以及引入固定随机注意力系数的MixiT模型，对比标准Transformer的性能差异。

Result: MixiT在算术和记忆任务中表现与完整模型相当，但在检索任务中因无法形成专用电路（如归纳头）而表现较差。冻结部分注意力组件的模型仍能完成任务。

Conclusion: Transformer的架构异质性对不同任务的解决至关重要，注意力机制的贡献因任务类型而异。

Abstract: The Transformer architecture is central to the success of modern Large
Language Models (LLMs), in part due to its surprising ability to perform a wide
range of algorithmic tasks -- including mathematical reasoning, memorization,
and retrieval -- using only gradient-based training on next-token prediction.
While the core component of a Transformer is the self-attention mechanism, we
question how much, and which aspects, of the performance gains can be
attributed to it. To this end, we compare standard Transformers to variants in
which either the multi-layer perceptron (MLP) layers or the attention
projectors (queries and keys) are frozen at initialization. To further isolate
the contribution of attention, we introduce MixiT -- the Mixing Transformer --
a simplified, principled model in which the attention coefficients are entirely
random and fixed at initialization, eliminating any input-dependent computation
or learning in attention. Surprisingly, we find that MixiT matches the
performance of fully trained Transformers on various algorithmic tasks,
especially those involving basic arithmetic or focusing heavily on
memorization. For retrieval-based tasks, we observe that having input-dependent
attention coefficients is consistently beneficial, while MixiT underperforms.
We attribute this failure to its inability to form specialized circuits such as
induction heads -- a specific circuit known to be crucial for learning and
exploiting repeating patterns in input sequences. Even more interestingly, we
find that attention with frozen key and query projectors is not only able to
form induction heads, but can also perform competitively on language modeling.
Our results underscore the importance of architectural heterogeneity, where
distinct components contribute complementary inductive biases crucial for
solving different classes of tasks.

</details>


### [369] [Neuro-Symbolic Generative Diffusion Models for Physically Grounded, Robust, and Safe Generation](https://arxiv.org/abs/2506.01121)
*Jacob K. Christopher,Michael Cardei,Jinhao Liang,Ferdinando Fioretto*

Key words: 扩散模型, 符号优化, 安全约束, 数据稀缺, 域外泛化

TL;DR: 论文提出了一种名为Neuro-Symbolic Diffusion（NSD）的新框架，通过结合扩散模型与符号优化，生成符合用户定义的函数和逻辑约束的样本。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 扩散模型虽然在生成能力上表现出色，但其在安全性要求高或科学严谨的应用中仍受限于需要满足严格的物理、结构和操作约束。

Method: NSD框架通过交替进行扩散步骤和符号优化，能够生成在连续和离散输出中均符合约束的样本。

Result: 该框架在三个关键挑战中展示了有效性：安全（如无毒分子生成和防碰撞轨迹优化）、数据稀缺（如药物发现和材料工程）以及域外泛化。

Conclusion: NSD框架通过结合扩散模型与符号优化，为高要求应用提供了可靠的生成能力。

Abstract: Despite the remarkable generative capabilities of diffusion models, their
integration into safety-critical or scientifically rigorous applications
remains hindered by the need to ensure compliance with stringent physical,
structural, and operational constraints. To address this challenge, this paper
introduces Neuro-Symbolic Diffusion (NSD), a novel framework that interleaves
diffusion steps with symbolic optimization, enabling the generation of
certifiably consistent samples under user-defined functional and logic
constraints. This key feature is provided for both standard and discrete
diffusion models, enabling, for the first time, the generation of both
continuous (e.g., images and trajectories) and discrete (e.g., molecular
structures and natural language) outputs that comply with constraints. This
ability is demonstrated on tasks spanning three key challenges: (1) Safety, in
the context of non-toxic molecular generation and collision-free trajectory
optimization; (2) Data scarcity, in domains such as drug discovery and
materials engineering; and (3) Out-of-domain generalization, where enforcing
symbolic constraints allows adaptation beyond the training distribution.

</details>


### [370] [Slow Feature Analysis on Markov Chains from Goal-Directed Behavior](https://arxiv.org/abs/2506.01145)
*Merlin Schüler,Eddie Seabrook,Laurenz Wiskott*

Key words: 慢特征分析, 表示学习, 强化学习, 目标导向行为, 马尔可夫链

TL;DR: 论文研究了如何利用目标导向行为生成的数据学习表示，并探讨了状态占用差异对值函数近似的影响，提出了三种修正方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究目标导向行为生成的数据对表示学习的影响，以及在空间设置中，奖励位置附近和远处的状态占用差异如何影响值函数近似。

Method: 通过最优慢特征在遍历马尔可夫链上的视角，分析状态占用差异的影响，并提出三种修正方法来减轻不良缩放效应。

Result: 研究揭示了目标导向行为对表示学习的特殊影响，并提出了可行的修正方法。

Conclusion: 目标导向行为的数据在表示学习中具有重要意义，可以通过修正方法优化值函数近似效果。

Abstract: Slow Feature Analysis is a unsupervised representation learning method that
extracts slowly varying features from temporal data and can be used as a basis
for subsequent reinforcement learning. Often, the behavior that generates the
data on which the representation is learned is assumed to be a uniform random
walk. Less research has focused on using samples generated by goal-directed
behavior, as commonly the case in a reinforcement learning setting, to learn a
representation. In a spatial setting, goal-directed behavior typically leads to
significant differences in state occupancy between states that are close to a
reward location and far from a reward location.
  Through the perspective of optimal slow features on ergodic Markov chains,
this work investigates the effects of these differences on value-function
approximation in an idealized setting. Furthermore, three correction routes,
which can potentially alleviate detrimental scaling effects, are evaluated and
discussed. In addition, the special case of goal-averse behavior is considered.

</details>


### [371] [Earley-Driven Dynamic Pruning for Efficient Structured Decoding](https://arxiv.org/abs/2506.01151)
*Xintong Sun,Chi Wei,Minghao Tian,Shiwen Ni*

Key words: Large Language Models, constrained decoding, Earley algorithm, dynamic pruning, structured generation

TL;DR: ZapFormat 是一种基于 Earley 算法的动态剪枝策略，用于优化 LLMs 的约束解码，减少内存占用并提高推理速度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 确保 LLMs 输出符合严格的结构或语法约束（如函数调用和 DSL 生成）是一个重要但具有挑战性的问题。现有约束解码引擎在验证令牌有效性时存在显著开销。

Method: 提出 ZapFormat，一种动态剪枝策略，实时识别和消除冗余的 Earley 状态，并结合状态缓存加速结构化生成。

Result: 在 JSON 生成和语义解析等任务中，Formatron 保持高精度输出，推理速度提升高达 2 倍。

Conclusion: Formatron 是一种通用且高效的约束解码引擎，适用于多种 LLM 架构。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring
their outputs conform to strict structural or grammatical constraints remains
challenging, which is critical in function calls and domain-specific language
(DSL) generation. Constrained decoding with context-free grammar is a flexible
approach to guarantee LLMs' adherence to a specific format by dynamically
building a token logits mask. However, creating this mask requires checking the
validity of all tokens in the LLM vocabulary at every decoding step, which
often incurs significant overheads in existing constrained decoding engines. To
address this challenge, we propose $\textbf{ZapFormat}$, a novel
$\textbf{dynamic pruning}$ strategy based on the Earley algorithm that
identifies and eliminates invalid or redundant Earley states in real-time,
significantly reducing memory occupation of the Earley algorithm's states. This
further enables us to use a state cache to speed up structured generations on a
large number of queries. We implemented ZapFormat in a new constrained decoding
engine called Formatron which also incorporates existing optimizations. Through
comprehensive experiments on structured generation tasks, including JSON
generation, JSON Schema validation, and semantic parsing, we demonstrate that
Formatron not only $\textbf{consistently maintains}$ high-precision compliant
outputs but also achieves $\textbf{significant improvements}$ in inference
speed up to 2x compared to state-of-the-art implementations. More importantly,
Formatron is generally applicable across various LLM architectures. We release
Formatron as open source at https://github.com/Dan-wanna-M/formatron.

</details>


### [372] [Weight-Space Linear Recurrent Neural Networks](https://arxiv.org/abs/2506.01153)
*Roussel Desmond Nzoyem,Nawid Keshtmand,Idriss Tsayem,David A. W. Barton,Tom Deakin*

Key words: WARP, 权重空间学习, 线性递归, 序列建模, 自适应智能

TL;DR: WARP是一个统一权重空间学习与线性递归的框架，通过显式参数化隐藏状态为根神经网络的权重，提升记忆分辨率与测试时适应性，并在多项任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 重新定义序列建模，解决传统RNN将时间动态压缩为固定维度隐状态的局限性。

Method: 将隐状态参数化为根神经网络的权重，实现梯度自由适应与领域先验集成。

Result: 在分类任务、时序图像补全、动力系统重构等任务中表现优于或匹配现有基线。

Conclusion: WARP为自适应机器智能提供了新范式，权重轨迹揭示了模型内部机制。

Abstract: We introduce WARP (Weight-space Adaptive Recurrent Prediction), a simple yet
powerful framework that unifies weight-space learning with linear recurrence to
redefine sequence modeling. Unlike conventional recurrent neural networks
(RNNs) which collapse temporal dynamics into fixed-dimensional hidden states,
WARP explicitly parametrizes the hidden state as the weights of a distinct root
neural network. This formulation promotes higher-resolution memory,
gradient-free adaptation at test-time, and seamless integration of
domain-specific physical priors. Empirical validation shows that WARP matches
or surpasses state-of-the-art baselines on diverse classification tasks,
spanning synthetic benchmarks to real-world datasets. Furthermore, extensive
experiments across sequential image completion, dynamical system
reconstruction, and multivariate time series forecasting demonstrate its
expressiveness and generalization capabilities. Critically, WARP's weight
trajectories offer valuable insights into the model's inner workings. Ablation
studies confirm the architectural necessity of key components, solidifying
weight-space linear RNNs as a transformative paradigm for adaptive machine
intelligence.

</details>


### [373] [FORT: Forward-Only Regression Training of Normalizing Flows](https://arxiv.org/abs/2506.01158)
*Danyal Rehman,Oscar Davis,Jiarui Lu,Jian Tang,Michael Bronstein,Yoshua Bengio,Alexander Tong,Avishek Joey Bose*

Key words: 生成模型, 归一化流, 训练目标, 分子系统采样, 计算效率

TL;DR: 论文提出了一种新的单步生成模型训练方法FORT，通过简单的L2回归目标优化，避免传统最大似然训练中的计算成本高的变量变换公式，提高了训练效率和模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的生成模型训练需要昂贵的数值模拟计算，限制了其在科学应用中的广泛采用，如分子系统的平衡采样。本研究旨在开发一种无需模拟且计算高效的训练框架。

Method: 提出了Forward-Only Regression Training (FORT)，一种基于L2回归的目标函数，通过将先验样本映射到特定目标来训练单步生成模型。支持多种目标类型，如最优传输目标和预训练的连续时间归一化流(CNF)目标。

Result: FORT训练的单步生成模型在性能和稳定性上优于传统的最大似然训练，且支持更广泛的架构。实验表明，该模型可在特定分子系统的平衡构象采样中有效应用。

Conclusion: FORT为生成模型提供了一种高效、可扩展的训练方法，解决了传统训练中的计算瓶颈问题，并为复杂科学应用提供了新的可能性。

Abstract: Simulation-free training frameworks have been at the forefront of the
generative modelling revolution in continuous spaces, leading to neural
dynamical systems that encompass modern large-scale diffusion and flow matching
models. Despite the scalability of training, the generation of high-quality
samples and their corresponding likelihood under the model requires expensive
numerical simulation -- inhibiting adoption in numerous scientific applications
such as equilibrium sampling of molecular systems. In this paper, we revisit
classical normalizing flows as one-step generative models with exact
likelihoods and propose a novel, scalable training objective that does not
require computing the expensive change of variable formula used in conventional
maximum likelihood training. We propose Forward-Only Regression Training
(FORT), a simple $\ell_2$-regression objective that maps prior samples under
our flow to specifically chosen targets. We demonstrate that FORT supports a
wide class of targets, such as optimal transport targets and targets from
pre-trained continuous-time normalizing flows (CNF). We further demonstrate
that by using CNF targets, our one-step flows allow for larger-scale training
that exceeds the performance and stability of maximum likelihood training,
while unlocking a broader class of architectures that were previously
challenging to train. Empirically, we elucidate that our trained flows can
perform equilibrium conformation sampling in Cartesian coordinates of alanine
dipeptide, alanine tripeptide, and alanine tetrapeptide.

</details>


### [374] [Accelerated Learning with Linear Temporal Logic using Differentiable Simulation](https://arxiv.org/abs/2506.01167)
*Alper Kamil Bozkurt,Calin Belta,Ming C. Lin*

Key words: 强化学习,线性时序逻辑（LTL）,可微分模拟器,稀疏奖励,安全控制器

TL;DR: 论文提出了一种结合线性时序逻辑（LTL）与可微分模拟器的新方法，解决了传统方法中稀疏奖励和保守行为的问题，并通过实验验证了其高效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了确保强化学习控制器在实际应用中的安全性和可靠性，传统方法（如状态避免和约束Markov决策过程）难以满足轨迹需求或过于保守。LTL虽然提供了形式化规范，但稀疏奖励使学习困难。

Method: 提出了一种将LTL与可微分模拟器结合的方法，通过软标记实现可微分奖励和状态，解决了LTL的稀疏奖励问题，同时不牺牲正确性。

Result: 实验证明，该方法在奖励获取和训练时间方面显著优于离散方法。

Conclusion: 该方法成功地将LTL规范与可微分学习结合，为强化学习的安全性和效率提供了新的解决方案。

Abstract: To ensure learned controllers comply with safety and reliability requirements
for reinforcement learning in real-world settings remains challenging.
Traditional safety assurance approaches, such as state avoidance and
constrained Markov decision processes, often inadequately capture trajectory
requirements or may result in overly conservative behaviors. To address these
limitations, recent studies advocate the use of formal specification languages
such as linear temporal logic (LTL), enabling the derivation of
correct-by-construction learning objectives from the specified requirements.
However, the sparse rewards associated with LTL specifications make learning
extremely difficult, whereas dense heuristic-based rewards risk compromising
correctness. In this work, we propose the first method, to our knowledge, that
integrates LTL with differentiable simulators, facilitating efficient
gradient-based learning directly from LTL specifications by coupling with
differentiable paradigms. Our approach introduces soft labeling to achieve
differentiable rewards and states, effectively mitigating the sparse-reward
issue intrinsic to LTL without compromising objective correctness. We validate
the efficacy of our method through experiments, demonstrating significant
improvements in both reward attainment and training time compared to the
discrete methods.

</details>


### [375] [Bridging Quantum and Classical Computing in Drug Design: Architecture Principles for Improved Molecule Generation](https://arxiv.org/abs/2506.01177)
*Andrew Smith,Erhan Guven*

Key words: 量子-经典混合,GAN,NISQ,贝叶斯优化,药物发现

TL;DR: 通过多目标贝叶斯优化，研究优化了量子-经典混合生成对抗网络的架构，显著提升了药物发现性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 利用NISQ设备在药物发现中的潜力，但现有混合模型架构不明确。

Method: 采用多目标贝叶斯优化，优化量子-经典桥接架构。

Result: BO-QGAN模型性能显著提升，DCS分数较基准提高2.27倍，参数减少60%。

Conclusion: 多浅层量子电路顺序堆叠是优化方向，为混合模型提供实证指南。

Abstract: Hybrid quantum-classical machine learning offers a path to leverage noisy
intermediate-scale quantum (NISQ) devices for drug discovery, but optimal model
architectures remain unclear. We systematically optimize the quantum-classical
bridge architecture for generative adversarial networks (GANs) in molecular
discovery using multi-objective Bayesian optimization. Our optimized model
(BO-QGAN) significantly improves performance, achieving a 2.27-fold higher Drug
Candidate Score (DCS) than prior quantum-hybrid benchmarks and 2.21-fold higher
than the classical baseline, using over 60% fewer parameters. Key findings
favor layering multiple (3-4) shallow (4-8 qubit) quantum circuits
sequentially, while classical architecture shows less sensitivity above a
minimum capacity. This work provides the first empirically grounded
architectural guidelines for hybrid models, enabling more effective integration
of current quantum computers into pharmaceutical research pipelines.

</details>


### [376] [Doubly Robust Alignment for Large Language Models](https://arxiv.org/abs/2506.01183)
*Erhan Xu,Kai Ye,Hongyi Zhou,Luhan Zhu,Francesco Quinzan,Chengchun Shi*

Key words: RLHF, 双鲁棒, 偏好优化, 模型误配, 大语言模型

TL;DR: 本文提出了一种双鲁棒偏好优化算法，用于解决强化学习从人类反馈（RLHF）中模型误配的问题，提高了算法在偏好模型或参考策略正确时的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究RLHF在将大语言模型与人类偏好对齐时的模型误配问题，旨在提高算法的鲁棒性和性能。

Method: 提出双鲁棒偏好优化算法，无需同时正确指定偏好模型和参考策略。

Result: 理论和实验均显示，该算法优于现有最先进方法，具有更强的鲁棒性。

Conclusion: 该算法有效解决了RLHF中的模型误配问题，提升了性能与鲁棒性。

Abstract: This paper studies reinforcement learning from human feedback (RLHF) for
aligning large language models with human preferences. While RLHF has
demonstrated promising results, many algorithms are highly sensitive to
misspecifications in the underlying preference model (e.g., the Bradley-Terry
model), the reference policy, or the reward function, resulting in undesirable
fine-tuning. To address model misspecification, we propose a doubly robust
preference optimization algorithm that remains consistent when either the
preference model or the reference policy is correctly specified (without
requiring both). Our proposal demonstrates superior and more robust performance
than state-of-the-art algorithms, both in theory and in practice. The code is
available at https://github.com/DRPO4LLM/DRPO4LLM

</details>


### [377] [FedRPCA: Enhancing Federated LoRA Aggregation Using Robust PCA](https://arxiv.org/abs/2506.01194)
*Divyansh Jhunjhunwala,Arian Raje,Madan Ravi Ganesh,Chaithanya Kumar Mummadi,Chaoqun Dong,Jiawei Zhou,Wan-Yi Lin,Gauri Joshi,Zhenzhen Li*

Key words: LoRA, 联邦学习, Robust-PCA, 数据异构性, FedRPCA

TL;DR: LoRA在联邦学习中表现优异，但数据异构性导致收敛慢和精度不足。研究提出FedRPCA算法，通过Robust-PCA分解LoRA参数，结合平均和缩放平均提升性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决LoRA在联邦学习中因数据异构性导致的收敛慢和精度问题。

Method: 提出FedRPCA算法，利用Robust-PCA分解LoRA参数为低秩和稀疏分量，分别采用平均和缩放平均聚合。

Result: 在多种视觉和语言任务中，FedRPCA表现出更高的最终精度和更快的收敛速度。

Conclusion: FedRPCA有效提升LoRA在异构数据联邦学习中的性能。

Abstract: LoRA has emerged as one of the most promising fine-tuning techniques,
especially for federated learning (FL), since it significantly reduces
communication and computation costs at resource-constrained clients. However,
data heterogeneity remains a significant challenge for LoRA-based FL, and the
conventional aggregation strategy based on FedAvg suffers from slow convergence
and suboptimal accuracy. Motivated by recent advances in model merging,
particularly Task Arithmetic, we explore the idea of aggregating client LoRA
parameters using scaled averaging. We first observe that a naive application of
Task Arithmetic is ineffective due to the high cosine similarity between client
updates, indicating significant common knowledge in the updates across clients.
To address this issue, we propose decomposing client LoRA updates via Robust
Principal Component Analysis (Robust-PCA) into a common low-rank component and
client-specific sparse components. Our proposed algorithm FedRPCA aggregates
the low-rank components through averaging, consolidating common knowledge, and
applies scaled averaging to the sparse components to amplify client-specific
knowledge. We evaluate our approach across a variety of vision and language
tasks and demonstrate that it achieves higher final accuracy and faster
convergence compared to competing baselines.

</details>


### [378] [Multiresolution Analysis and Statistical Thresholding on Dynamic Networks](https://arxiv.org/abs/2506.01208)
*Raphaël Romero,Tijl De Bie,Nick Heard,Alexander Modell*

Key words: 动态网络, 结构变化检测, 多分辨率分析, ANIE, 网络安全

TL;DR: ANIE是一个自适应多分辨率框架，用于动态网络数据中的结构变化检测，能够自动识别变化的时标，兼顾快速和缓慢变化。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法通常固定时间分辨率，难以平衡时间和统计稳定性，尤其在多时标异常的领域（如网络安全）中问题突出。

Method: ANIE分为两步：(1)估计节点行为的低维子空间，(2)计算新颖的经验亲和系数以量化不同时标的交互强度变化并支持统计测试。

Result: 理论保证了子空间估计和亲和系数的渐近行为，实验表明ANIE能自适应时标、捕捉突变且对噪声鲁棒。

Conclusion: ANIE在多分辨率结构变化检测中优于固定分辨率方法，适用于实际问题。

Abstract: Detecting structural change in dynamic network data has wide-ranging
applications. Existing approaches typically divide the data into time bins,
extract network features within each bin, and then compare these features over
time. This introduces an inherent tradeoff between temporal resolution and the
statistical stability of the extracted features. Despite this tradeoff,
reminiscent of time-frequency tradeoffs in signal processing, most methods rely
on a fixed temporal resolution. Choosing an appropriate resolution parameter is
typically difficult and can be especially problematic in domains like
cybersecurity, where anomalous behavior may emerge at multiple time scales. We
address this challenge by proposing ANIE (Adaptive Network Intensity
Estimation), a multi-resolution framework designed to automatically identify
the time scales at which network structure evolves, enabling the joint
detection of both rapid and gradual changes. Modeling interactions as Poisson
processes, our method proceeds in two steps: (1) estimating a low-dimensional
subspace of node behavior, and (2) deriving a set of novel empirical affinity
coefficients that quantify change in interaction intensity between latent
factors and support statistical testing for structural change across time
scales. We provide theoretical guarantees for subspace estimation and the
asymptotic behavior of the affinity coefficients, enabling model-based change
detection. Experiments on synthetic networks show that ANIE adapts to the
appropriate time resolution and is able to capture sharp structural changes
while remaining robust to noise. Furthermore, applications to real-world data
showcase the practical benefits of ANIE's multiresolution approach to detecting
structural change over fixed resolution methods.

</details>


### [379] [Dynamic Modes as Time Representation for Spatiotemporal Forecasting](https://arxiv.org/abs/2506.01212)
*Menglin Kong,Vincent Zhihao Zheng,Xudong Wang,Lijun Sun*

Key words: 数据驱动、时间嵌入、动态模态分解、时空预测、多尺度周期性

TL;DR: 提出了一种基于数据驱动的时间嵌入方法，利用动态模态分解提取时间模态，提升时空预测的长范围依赖建模能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统时间嵌入方法（如时间指示器或正弦函数）无法有效捕捉多尺度周期性，需改进以提升长期预测准确性。

Method: 采用动态模态分解（DMD）从观测数据直接提取时间模态，无需显式时间戳或手工特征。

Result: 在城市交通、高速公路流量和气候数据集上验证，DMD嵌入显著提升长期预测准确性，减少残差相关性，增强时间泛化能力。

Conclusion: 该方法轻量、模型无关，适合任何包含时间协变量的架构，为时空预测提供了高效的时间表征方案。

Abstract: This paper introduces a data-driven time embedding method for modeling
long-range seasonal dependencies in spatiotemporal forecasting tasks. The
proposed approach employs Dynamic Mode Decomposition (DMD) to extract temporal
modes directly from observed data, eliminating the need for explicit timestamps
or hand-crafted time features. These temporal modes serve as time
representations that can be seamlessly integrated into deep spatiotemporal
forecasting models. Unlike conventional embeddings such as time-of-day
indicators or sinusoidal functions, our method captures complex multi-scale
periodicity through spectral analysis of spatiotemporal data. Extensive
experiments on urban mobility, highway traffic, and climate datasets
demonstrate that the DMD-based embedding consistently improves long-horizon
forecasting accuracy, reduces residual correlation, and enhances temporal
generalization. The method is lightweight, model-agnostic, and compatible with
any architecture that incorporates time covariates.

</details>


### [380] [On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective](https://arxiv.org/abs/2506.01213)
*Ning Zhang,Henry Kenlay,Li Zhang,Mihai Cucuringu,Xiaowen Dong*

Key words: 图卷积神经网络, 稳定性分析, 分布感知, 拓扑扰动, 对抗性攻击

TL;DR: 该论文研究了图卷积神经网络（GCNNs）在拓扑扰动下的稳定性问题，提出了一种新的分布感知的稳定性分析框架，并通过实验验证了其优于现有方法的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: GCNNs在图结构数据分析中表现出色，但对其稳定性的理论理解仍有限，影响了模型的鲁棒性和可信度。本文旨在填补这一空白。

Method: 提出一种新的分布感知的稳定性分析框架，通过考虑输入数据的统计特性，全面分析图拓扑扰动对模型输出的影响。

Result: 实验表明，所提方法在表示稳定性和对抗性攻击方面优于现有基线，强调了将数据分布纳入稳定性分析的重要性。

Conclusion: 该研究为GCNNs的稳定性提供了新的理论和实践视角，证明数据分布对稳定性分析的关键作用。

Abstract: Graph convolutional neural networks (GCNNs) have emerged as powerful tools
for analyzing graph-structured data, achieving remarkable success across
diverse applications. However, the theoretical understanding of the stability
of these models, i.e., their sensitivity to small changes in the graph
structure, remains in rather limited settings, hampering the development and
deployment of robust and trustworthy models in practice. To fill this gap, we
study how perturbations in the graph topology affect GCNN outputs and propose a
novel formulation for analyzing model stability. Unlike prior studies that
focus only on worst-case perturbations, our distribution-aware formulation
characterizes output perturbations across a broad range of input data. This
way, our framework enables, for the first time, a probabilistic perspective on
the interplay between the statistical properties of the node data and
perturbations in the graph topology. We conduct extensive experiments to
validate our theoretical findings and demonstrate their benefits over existing
baselines, in terms of both representation stability and adversarial attacks on
downstream tasks. Our results demonstrate the practical significance of the
proposed formulation and highlight the importance of incorporating data
distribution into stability analysis.

</details>


### [381] [Self-Refining Training for Amortized Density Functional Theory](https://arxiv.org/abs/2506.01225)
*Majdi Hassan,Cristian Gabellini,Hatem Helal,Dominique Beaini,Kirill Neklyudov*

Key words: DFT, 深度学习, 自优化训练, 分子构象, KL散度

TL;DR: 论文提出了一种新颖的自优化训练策略，减少了对大型预收集数据集的依赖，通过同时训练深度学习模型预测DFT输出并采样分子构象。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: DFT计算在分子系统规模和能量评估成本上存在限制，尤其是在模拟分子动力学时。现有基于深度学习的方法需依赖大型数据集，本方法旨在解决这一依赖问题。

Method: 采用自优化训练策略，通过最小化KL散度的变分上界，同时训练模型预测DFT输出并采样分子构象。

Result: 通过实验验证，该方法在减少数据依赖的同时保持了预测准确性，并开源了异步训练的优化实现。

Conclusion: 自优化训练策略有效减少了DFT计算对大型数据集的依赖，为分子模拟提供了高效工具。

Abstract: Density Functional Theory (DFT) allows for predicting all the chemical and
physical properties of molecular systems from first principles by finding an
approximate solution to the many-body Schr\"odinger equation. However, the cost
of these predictions becomes infeasible when increasing the scale of the energy
evaluations, e.g., when calculating the ground-state energy for simulating
molecular dynamics. Recent works have demonstrated that, for substantially
large datasets of molecular conformations, Deep Learning-based models can
predict the outputs of the classical DFT solvers by amortizing the
corresponding optimization problems. In this paper, we propose a novel method
that reduces the dependency of amortized DFT solvers on large pre-collected
datasets by introducing a self-refining training strategy. Namely, we propose
an efficient method that simultaneously trains a deep-learning model to predict
the DFT outputs and samples molecular conformations that are used as training
data for the model. We derive our method as a minimization of the variational
upper bound on the KL-divergence measuring the discrepancy between the
generated samples and the target Boltzmann distribution defined by the ground
state energy. To demonstrate the utility of the proposed scheme, we perform an
extensive empirical study comparing it with the models trained on the
pre-collected datasets. Finally, we open-source our implementation of the
proposed algorithm, optimized with asynchronous training and sampling stages,
which enables simultaneous sampling and training. Code is available at
https://github.com/majhas/self-refining-dft.

</details>


### [382] [Stress-Testing ML Pipelines with Adversarial Data Corruption](https://arxiv.org/abs/2506.01230)
*Jiongli Zhu,Geyang Xu,Felipe Lorenzi,Boris Glavic,Babak Salimi*

Key words: 数据质量,鲁棒性评估,机器管道,因果建模,双层优化

TL;DR: SAVAGE是一个因果启发框架，用于建模和发现最坏情况下的数据质量问题，以系统性地测试机器学习管道的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现实中的数据质量问题（如与人口统计相关的缺失值或系统选择偏差）会降低机器学习的可靠性，而现有的鲁棒性评估方法过于简化，无法覆盖最坏情况。

Method: SAVAGE通过依赖图和灵活的数据破坏模板建模数据质量问题，并采用双层优化方法高效识别脆弱数据子群体和破坏严重性。

Result: 实验表明，SAVAGE发现的少量结构化破坏（约5%）能显著降低模型性能，远超随机或人工设计的错误。

Conclusion: SAVAGE为管道压力测试、鲁棒性方法评估和设计更健壮的数据流程提供了实用工具。

Abstract: Structured data-quality issues, such as missing values correlated with
demographics, culturally biased labels, or systemic selection biases, routinely
degrade the reliability of machine-learning pipelines. Regulators now
increasingly demand evidence that high-stakes systems can withstand these
realistic, interdependent errors, yet current robustness evaluations typically
use random or overly simplistic corruptions, leaving worst-case scenarios
unexplored. We introduce SAVAGE, a causally inspired framework that (i)
formally models realistic data-quality issues through dependency graphs and
flexible corruption templates, and (ii) systematically discovers corruption
patterns that maximally degrade a target performance metric. SAVAGE employs a
bi-level optimization approach to efficiently identify vulnerable data
subpopulations and fine-tune corruption severity, treating the full ML
pipeline, including preprocessing and potentially non-differentiable models, as
a black box. Extensive experiments across multiple datasets and ML tasks (data
cleaning, fairness-aware learning, uncertainty quantification) demonstrate that
even a small fraction (around 5 %) of structured corruptions identified by
SAVAGE severely impacts model performance, far exceeding random or manually
crafted errors, and invalidating core assumptions of existing techniques. Thus,
SAVAGE provides a practical tool for rigorous pipeline stress-testing, a
benchmark for evaluating robustness methods, and actionable guidance for
designing more resilient data workflows.

</details>


### [383] [Towards Efficient Few-shot Graph Neural Architecture Search via Partitioning Gradient Contribution](https://arxiv.org/abs/2506.01231)
*Wenhao Song,Xuan Wu,Bo Yang,You Zhou,Yubin Xiao,Yanchun Liang,Hongwei Ge,Heow Pueh Lee,Chunguo Wu*

Key words: 权重耦合,神经架构搜索,梯度贡献,图神经网络,统一框架

TL;DR: 为了解决权重耦合问题，研究提出了少样本神经架构搜索（NAS）方法，但存在计算效率低和分区方案次优的问题。通过分析梯度方向冲突，提出了一种高效计算梯度贡献（GC）的方法，并结合统一的图神经架构搜索（UGAS）框架，展示了优越性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有少样本NAS方法在解决权重耦合问题时效率低下且分区效果不佳，需要从梯度方向冲突的角度探索更优方案。

Method: 提出GC方法计算模块间梯度方向的余弦相似度，并通过分解向量-雅可比积实现高效分区；同时提出UGAS框架，结合MPNNs与GTs进行搜索。

Result: GC在超网分区质量和时间效率上达到SOTA；UGAS+GC搜索的架构优于手动设计的GNN和其他NAS方法。

Conclusion: GC和UGAS方法有效解决了现有问题，性能显著提升。

Abstract: To address the weight coupling problem, certain studies introduced few-shot
Neural Architecture Search (NAS) methods, which partition the supernet into
multiple sub-supernets. However, these methods often suffer from computational
inefficiency and tend to provide suboptimal partitioning schemes. To address
this problem more effectively, we analyze the weight coupling problem from a
novel perspective, which primarily stems from distinct modules in succeeding
layers imposing conflicting gradient directions on the preceding layer modules.
Based on this perspective, we propose the Gradient Contribution (GC) method
that efficiently computes the cosine similarity of gradient directions among
modules by decomposing the Vector-Jacobian Product during supernet
backpropagation. Subsequently, the modules with conflicting gradient directions
are allocated to distinct sub-supernets while similar ones are grouped
together. To assess the advantages of GC and address the limitations of
existing Graph Neural Architecture Search methods, which are limited to
searching a single type of Graph Neural Networks (Message Passing Neural
Networks (MPNNs) or Graph Transformers (GTs)), we propose the Unified Graph
Neural Architecture Search (UGAS) framework, which explores optimal
combinations of MPNNs and GTs. The experimental results demonstrate that GC
achieves state-of-the-art (SOTA) performance in supernet partitioning quality
and time efficiency. In addition, the architectures searched by UGAS+GC
outperform both the manually designed GNNs and those obtained by existing NAS
methods. Finally, ablation studies further demonstrate the effectiveness of all
proposed methods.

</details>


### [384] [Neural Variance-aware Dueling Bandits with Deep Representation and Shallow Exploration](https://arxiv.org/abs/2506.01250)
*Youngmin Oh,Jinje Park,Taejin Paik,Jaemin Park*

Key words: contextual dueling bandit, neural networks, variance-aware exploration

TL;DR: 提出一种基于神经网络的方法解决上下文对决bandit问题，采用方差感知探索策略，在理论和实验上均表现出色。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决上下文对决bandit问题中非线性效用函数的高效学习问题。

Method: 使用神经网络近似非线性效用函数，结合方差感知探索策略，平衡探索与利用。

Result: 理论保证了次线性累积遗憾；实验表明在合成和真实任务中优于现有方法。

Conclusion: 方法在理论和实践中均有效，显著提升了性能。

Abstract: In this paper, we address the contextual dueling bandit problem by proposing
variance-aware algorithms that leverage neural networks to approximate
nonlinear utility functions. Our approach employs a \textit{variance-aware
exploration strategy}, which adaptively accounts for uncertainty in pairwise
comparisons while relying only on the gradients with respect to the learnable
parameters of the last layer. This design effectively balances the
exploration--exploitation tradeoff under both the Upper Confidence Bound (UCB)
and Thompson Sampling (TS) frameworks. As a result, under standard assumptions,
we establish theoretical guarantees showing that our algorithms achieve
sublinear cumulative average regret of order $\bigol\lt(d \sqrt{\sum_{t=1}^T
\sigma_t^2} + \sqrt{dT}\rt),$ for sufficiently wide neural networks, where $ d
$ is the contextual dimension, $ \sigma_t^2 $ the variance of comparisons at
round $ t $, and $ T $ the total number of rounds. We also empirically validate
that our approach offers reasonable computational efficiency and achieves
sublinear regret on both synthetic tasks with nonlinear utilities and
real-world tasks, outperforming existing methods.

</details>


### [385] [Protocol Models: Scaling Decentralized Training with Communication-Efficient Model Parallelism](https://arxiv.org/abs/2506.01260)
*Sameera Ramasinghe,Thalaiyasingam Ajanthan,Gil Avraham,Yan Zuo,Alexander Long*

Key words: 模型并行、通信效率、压缩算法、Transformer、分布式训练

TL;DR: 提出了一种新型压缩算法，用于解决模型并行训练中的通信效率问题，支持极低带宽环境下的十亿参数模型训练。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在分布式深度学习训练中，模型并行训练面临通信瓶颈问题，传统压缩技术无法有效适应。

Method: 通过预定义低维子空间压缩激活和梯度，并利用Transformer递归结构实现完全重构。

Result: 算法支持高达99%的压缩率，通信效率提升100倍，可在80Mbps带宽下达到与数据中心100Gbps相当的收敛效果。

Conclusion: 该方法为低带宽环境下的模型并行训练提供了高效解决方案。

Abstract: Scaling models has led to significant advancements in deep learning, but
training these models in decentralized settings remains challenging due to
communication bottlenecks. While existing compression techniques are effective
in data-parallel, they do not extend to model parallelism. Unlike data-parallel
training, where weight gradients are exchanged, model-parallel requires
compressing activations and activation gradients as they propagate through
layers, accumulating compression errors. We propose a novel compression
algorithm that compresses both forward and backward passes, enabling up to 99%
compression with no convergence degradation with negligible memory/compute
overhead. By leveraging a recursive structure in transformer networks, we
predefine a low-dimensional subspace to confine the activations and gradients,
allowing full reconstruction in subsequent layers. Our method achieves up to
100x improvement in communication efficiency and enables training
billion-parameter-scale models over low-end GPUs connected via consumer-grade
internet speeds as low as 80Mbps, matching the convergence of centralized
datacenter systems with 100Gbps connections with model parallel.

</details>


### [386] [The Actor-Critic Update Order Matters for PPO in Federated Reinforcement Learning](https://arxiv.org/abs/2506.01261)
*Zhijie Xie,Shenghui Song*

Key words: 联邦强化学习, 近端策略优化, 数据异质性, 更新顺序, FedRAC

TL;DR: 论文提出了FedRAC算法，通过反转PPO中演员和评论家的更新顺序，解决了联邦强化学习中因数据异质性导致的收敛问题，并在实验中展现了更好的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在联邦强化学习中，PPO算法由于聚合步骤的存在，演员和评论家的更新顺序可能导致梯度方向不一致，阻碍全局最优策略的收敛。

Method: 提出FedRAC算法，调整PPO的更新顺序（先更新演员，再更新评论家），以减少客户间评论家的差异。

Result: 理论分析证明FedRAC在数据异质性问题上具有抗性，实验结果显示其在多个场景中具有更高的累积奖励和更快的收敛速度。

Conclusion: FedRAC通过改变更新顺序有效解决了FRL中的收敛问题，适用于异质性数据场景。

Abstract: In the context of Federated Reinforcement Learning (FRL), applying Proximal
Policy Optimization (PPO) faces challenges related to the update order of its
actor and critic due to the aggregation step occurring between successive
iterations. In particular, when local actors are updated based on local critic
estimations, the algorithm becomes vulnerable to data heterogeneity. As a
result, the conventional update order in PPO (critic first, then actor) may
cause heterogeneous gradient directions among clients, hindering convergence to
a globally optimal policy. To address this issue, we propose FedRAC, which
reverses the update order (actor first, then critic) to eliminate the
divergence of critics from different clients. Theoretical analysis shows that
the convergence bound of FedRAC is immune to data heterogeneity under mild
conditions, i.e., bounded level of heterogeneity and accurate policy
evaluation. Empirical results indicate that the proposed algorithm obtains
higher cumulative rewards and converges more rapidly in five experiments,
including three classical RL environments and a highly heterogeneous autonomous
driving scenario using the SUMO traffic simulator.

</details>


### [387] [TSRating: Rating Quality of Diverse Time Series Data by Meta-learning from LLM Judgment](https://arxiv.org/abs/2506.01290)
*Shunyu Wu,Dan Li,Haozheng Ye,Zhuomin Chen,Jiahui Zhou,Jian Lou,Zibin Zheng,See-Kiong Ng*

Key words: 时间序列数据质量,LLMs,meta-learning,跨领域适应性

TL;DR: 该论文提出了TSRating框架，利用LLMs的广泛知识评估多样化时间序列数据质量，通过meta-learning提升跨领域适应性，实验证明其优于基准方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法局限于单一领域，无法准确高效地评估多样化时间序列数据质量，亟需一种统一框架。

Method: 提出TSRating框架，利用LLMs的知识生成质量比较，通过meta-learning和signSGD训练TSRater模型实现高效跨领域预测。

Result: 在11个基准数据集和3个时间序列任务中，TSRating在准确性、效率和领域适应性上均优于基线方法。

Conclusion: TSRating框架通过结合LLMs和meta-learning，成功解决了多样化时间序列数据质量评估的挑战。

Abstract: High-quality time series (TS) data are essential for ensuring TS model
performance, rendering research on rating TS data quality indispensable.
Existing methods have shown promising rating accuracy within individual
domains, primarily by extending data quality rating techniques such as
influence functions and Shapley values to account for temporal characteristics.
However, they neglect the fact that real-world TS data can span vastly
different domains and exhibit distinct properties, hampering the accurate and
efficient rating of diverse TS data. In this paper, we propose TSRating, a
novel and unified framework for rating the quality of time series data crawled
from diverse domains. TSRating is built on the assumption that LLMs inherit
ample knowledge, acquired during their extensive pretraining, enabling them to
comprehend and discern quality differences in diverse TS data. We verify this
assumption by devising a series of prompts to elicit quality comparisons from
LLMs for pairs of TS samples. We then fit a dedicated rating model, termed
TSRater, to convert the LLMs' judgments into efficient quality predictions via
TSRater's inference on future TS samples. To ensure cross-domain adaptability,
we develop a meta-learning scheme to train TSRater on quality comparisons
collected from nine distinct domains. To improve training efficiency, we employ
signSGD for inner-loop updates, thus circumventing the demanding computation of
hypergradients. Extensive experimental results on eleven benchmark datasets
across three time series tasks, each using both conventional TS models and TS
foundation models, demonstrate that TSRating outperforms baselines in terms of
estimation accuracy, efficiency, and domain adaptability.

</details>


### [388] [Recent Developments in GNNs for Drug Discovery](https://arxiv.org/abs/2506.01302)
*Zhengyu Fang,Xiaoge Zhang,Anyin Zhao,Xiao Li,Huiyuan Chen,Jing Li*

Key words: 图神经网络, 药物发现, 分子生成, 性质预测, 数据集

TL;DR: 回顾图神经网络（GNNs）在计算药物发现中的最新进展和应用，包括分子生成、分子性质预测和药物相互作用预测，总结GNN模型的分类和常用数据集，并探讨未来趋势。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索GNNs在药物发现领域的潜力，帮助研究者了解其当前进展和未来发展方向。

Method: 总结GNN模型的分类，分析不同输入类型和下游任务，并整理常用数据集。

Result: GNNs展现了理解和处理复杂分子模式的能力，为药物发现提供了有力工具。

Conclusion: GNNs在药物发现领域具有广泛应用前景，未来仍需进一步探索。

Abstract: In this paper, we review recent developments and the role of Graph Neural
Networks (GNNs) in computational drug discovery, including molecule generation,
molecular property prediction, and drug-drug interaction prediction. By
summarizing the most recent developments in this area, we underscore the
capabilities of GNNs to comprehend intricate molecular patterns, while
exploring both their current and prospective applications. We initiate our
discussion by examining various molecular representations, followed by detailed
discussions and categorization of existing GNN models based on their input
types and downstream application tasks. We also collect a list of commonly used
benchmark datasets for a variety of applications. We conclude the paper with
brief discussions and summarize common trends in this important research area.

</details>


### [389] [Latent Structured Hopfield Network for Semantic Association and Retrieval](https://arxiv.org/abs/2506.01303)
*Chong Li,Xiangyang Xue,Jianfeng Feng,Taiping Zeng*

Key words: 情景记忆, Hopfield网络, 自编码器, 海马体CA3, 联想记忆

TL;DR: 论文提出了一种生物启发的潜在结构化Hopfield网络（LSHN），通过结合连续Hopfield吸引子动力学和自编码器架构，模拟海马体CA3在联想记忆中的作用，实现了对语义元素的动态绑定，支持情景记忆的形成。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的大规模预训练模型在建模语义记忆方面取得了显著进展，但对支持情景记忆的联想结构形成机制研究不足。论文受海马体CA3动力学的启发，试图填补这一空白。

Method: 提出了LSHN框架，包括语义编码器提取潜在表示、潜在Hopfield网络通过吸引子收敛进行联想优化，以及解码器重建感知输入。模型通过端到端训练实现可扩展和稳健的记忆检索。

Result: 在MNIST、CIFAR-10和模拟情景记忆任务上的实验表明，LSHN在受遮挡和噪声干扰的输入回忆任务中优于现有的联想记忆模型。

Conclusion: LSHN为通过生物启发的吸引子机制动态绑定语义元素到情景记忆痕迹提供了计算视角。

Abstract: Episodic memory enables humans to recall past experiences by associating
semantic elements such as objects, locations, and time into coherent event
representations. While large pretrained models have shown remarkable progress
in modeling semantic memory, the mechanisms for forming associative structures
that support episodic memory remain underexplored. Inspired by hippocampal CA3
dynamics and its role in associative memory, we propose the Latent Structured
Hopfield Network (LSHN), a biologically inspired framework that integrates
continuous Hopfield attractor dynamics into an autoencoder architecture. LSHN
mimics the cortical-hippocampal pathway: a semantic encoder extracts compact
latent representations, a latent Hopfield network performs associative
refinement through attractor convergence, and a decoder reconstructs perceptual
input. Unlike traditional Hopfield networks, our model is trained end-to-end
with gradient descent, achieving scalable and robust memory retrieval.
Experiments on MNIST, CIFAR-10, and a simulated episodic memory task
demonstrate superior performance in recalling corrupted inputs under occlusion
and noise, outperforming existing associative memory models. Our work provides
a computational perspective on how semantic elements can be dynamically bound
into episodic memory traces through biologically grounded attractor mechanisms.

</details>


### [390] [Energy Considerations for Large Pretrained Neural Networks](https://arxiv.org/abs/2506.01311)
*Leo Mei,Mark Stamp*

Key words: 神经网络压缩、电力消耗、隐写容量缩减、剪枝、低秩分解

TL;DR: 论文研究了神经网络压缩对减少电力消耗的影响，比较了三种压缩技术在不同模型上的表现，发现隐写容量缩减效果最佳。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着神经网络复杂度的增加，其计算资源和高电力消耗带来了环境问题，但已有研究多关注性能保留而非电力消耗，本文旨在填补这一空白。

Method: 选取九种预训练模型，先记录未压缩时的电力消耗和训练时间，再应用隐写容量缩减、剪枝和低秩分解三种压缩技术并对比结果。

Result: 剪枝和低秩分解在电力消耗方面无显著改进，而隐写容量缩减在几乎所有情况下都表现出明显优势。

Conclusion: 隐写容量缩减是减少神经网络电力消耗的有效方法，为未来绿色AI研究提供了方向。

Abstract: Increasingly complex neural network architectures have achieved phenomenal
performance. However, these complex models require massive computational
resources that consume substantial amounts of electricity, which highlights the
potential environmental impact of such models. Previous studies have
demonstrated that substantial redundancies exist in large pre-trained models.
However, previous work has primarily focused on compressing models while
retaining comparable model performance, and the direct impact on electricity
consumption appears to have received relatively little attention. By
quantifying the energy usage associated with both uncompressed and compressed
models, we investigate compression as a means of reducing electricity
consumption. We consider nine different pre-trained models, ranging in size
from 8M parameters to 138M parameters. To establish a baseline, we first train
each model without compression and record the electricity usage and time
required during training, along with other relevant statistics. We then apply
three compression techniques: Steganographic capacity reduction, pruning, and
low-rank factorization. In each of the resulting cases, we again measure the
electricity usage, training time, model accuracy, and so on. We find that
pruning and low-rank factorization offer no significant improvements with
respect to energy usage or other related statistics, while steganographic
capacity reduction provides major benefits in almost every case. We discuss the
significance of these findings.

</details>


### [391] [T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning](https://arxiv.org/abs/2506.01317)
*Yanjun Fu,Faisal Hamman,Sanghamitra Dutta*

Key words: 指令调优, 数据选择, 令牌级评估, 鲁棒性, T-SHIRT

TL;DR: 论文提出了一种名为T-SHIRT的新型数据选择框架，通过引入新的评分方法关注令牌级的有效性，提升了指令调优的效率和质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的数据选择方法在样本级别评估质量，忽略了令牌级的有效性，且评分方法的鲁棒性不足。

Method: 提出Token-Selective HIeRarchical Data Selection（T-SHIRT），通过新评分方法选择信息丰富的令牌并提升样本的鲁棒性。

Result: 使用T-SHIRT精选的数据集（仅原数据的5%）训练的模型在八个基准测试中平均优于使用全数据训练的模型5.48分。

Conclusion: T-SHIRT在数据选择上优于现有技术，同时具有成本效益和高效率。

Abstract: Instruction tuning is essential for Large Language Models (LLMs) to
effectively follow user instructions. To improve training efficiency and reduce
data redundancy, recent works use LLM-based scoring functions, e.g.,
Instruction-Following Difficulty (IFD), to select high-quality
instruction-tuning data with scores above a threshold. While these data
selection methods often lead to models that can match or even exceed the
performance of models trained on the full datasets, we identify two key
limitations: (i) they assess quality at the sample level, ignoring token-level
informativeness; and (ii) they overlook the robustness of the scoring method,
often selecting a sample due to superficial lexical features instead of its
true quality. In this work, we propose Token-Selective HIeRarchical Data
Selection for Instruction Tuning (T-SHIRT), a novel data selection framework
that introduces a new scoring method to include only informative tokens in
quality evaluation and also promotes robust and reliable samples whose
neighbors also show high quality with less local inconsistencies. We
demonstrate that models instruction-tuned on a curated dataset (only 5% of the
original size) using T-SHIRT can outperform those trained on the entire
large-scale dataset by up to 5.48 points on average across eight benchmarks.
Across various LLMs and training set scales, our method consistently surpasses
existing state-of-the-art data selection techniques, while also remaining both
cost-effective and highly efficient. For instance, by using GPT-2 for score
computation, we are able to process a dataset of 52k samples using 40 minutes
on a single GPU.

</details>


### [392] [Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack](https://arxiv.org/abs/2506.01318)
*SeungBum Ha,Saerom Park,Sung Whan Yoon*

Key words: 机器遗忘,过度遗忘,原型再学习攻击,Spotter,知识蒸馏

TL;DR: 该论文提出了针对机器遗忘（MU）技术中存在的“过度遗忘”和“后学习”攻击盲点的问题，并提出了名为Spotter的新方法，通过结合掩蔽知识蒸馏和类内分散损失来解决这些问题，实验证明Spotter在CIFAR-10数据集上表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的机器遗忘技术忽略了两个关键问题：过度遗忘导致的邻近数据性能下降，以及后学习攻击（如原型再学习攻击）可能恢复被遗忘的知识。

Method: 提出了Spotter方法，包含掩蔽知识蒸馏惩罚和类内分散损失，以减少过度遗忘并防止原型再学习攻击。

Result: 在CIFAR-10数据集上，Spotter将过度遗忘降至基线的0.05倍以下，遗忘准确率降至0%，保留集准确率与原模型差异小于1%，且成功抵御了原型攻击。

Conclusion: Spotter是解决机器遗忘盲点的实用方法，能够平衡遗忘效果与模型性能。

Abstract: Machine unlearning (MU) aims to expunge a designated forget set from a
trained model without costly retraining, yet the existing techniques overlook
two critical blind spots: "over-unlearning" that deteriorates retained data
near the forget set, and post-hoc "relearning" attacks that aim to resurrect
the forgotten knowledge. We first derive the over-unlearning metric
OU@{\epsilon}, which represents the collateral damage to the nearby region of
the forget set, where the over-unlearning mainly appears. Next, we expose an
unforeseen relearning threat on MU, i.e., the Prototypical Relearning Attack,
which exploits the per-class prototype of the forget class with just a few
samples, and easily restores the pre-unlearning performance. To counter both
blind spots, we introduce Spotter, a plug-and-play objective that combines (i)
a masked knowledge-distillation penalty on the nearby region of forget set to
suppress OU@{\epsilon}, and (ii) an intra-class dispersion loss that scatters
forget-class embeddings, neutralizing prototypical relearning attacks. On
CIFAR-10, as one of validations, Spotter reduces OU@{\epsilon}by below the
0.05X of the baseline, drives forget accuracy to 0%, preserves accuracy of the
retain set within 1% of difference with the original, and denies the
prototype-attack by keeping the forget set accuracy within <1%, without
accessing retained data. It confirms that Spotter is a practical remedy of the
unlearning's blind spots.

</details>


### [393] [$Ψ$-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models](https://arxiv.org/abs/2506.01320)
*Taehoon Yoon,Yunhong Min,Kyeongmin Yeo,Minhyuk Sung*

Key words: $\\\\Psi$-Sampler, pCNL, 奖励对齐, 后验采样, 评分生成模型

TL;DR: 论文提出了$\\\\Psi$-Sampler框架，结合pCNL初始粒子采样，用于基于评分生成模型的推理时间奖励对齐，显著提升了采样效率和性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法通常从高斯先验初始化粒子，未能有效捕捉奖励相关区域，导致采样效率低。论文提出从奖励感知后验初始化以提高对齐性能。

Method: 引入预处理的Crank-Nicolson Langevin (pCNL)算法，结合维度鲁棒的提议与梯度信息动态，实现高效且可扩展的后验采样。

Result: 实验表明，该方法在布局到图像生成、数量感知生成和审美偏好生成等任务中均显著提升了性能。

Conclusion: $\\\\Psi$-Sampler通过奖励感知后验采样和pCNL算法，有效解决了现有方法在推理时间奖励对齐中的效率问题。

Abstract: We introduce $\Psi$-Sampler, an SMC-based framework incorporating pCNL-based
initial particle sampling for effective inference-time reward alignment with a
score-based generative model. Inference-time reward alignment with score-based
generative models has recently gained significant traction, following a broader
paradigm shift from pre-training to post-training optimization. At the core of
this trend is the application of Sequential Monte Carlo (SMC) to the denoising
process. However, existing methods typically initialize particles from the
Gaussian prior, which inadequately captures reward-relevant regions and results
in reduced sampling efficiency. We demonstrate that initializing from the
reward-aware posterior significantly improves alignment performance. To enable
posterior sampling in high-dimensional latent spaces, we introduce the
preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines
dimension-robust proposals with gradient-informed dynamics. This approach
enables efficient and scalable posterior sampling and consistently improves
performance across various reward alignment tasks, including layout-to-image
generation, quantity-aware generation, and aesthetic-preference generation, as
demonstrated in our experiments.

</details>


### [394] [STSA: Federated Class-Incremental Learning via Spatial-Temporal Statistics Aggregation](https://arxiv.org/abs/2506.01327)
*Zenghao Guan,Guojun Zhu,Yucan Zhou,Wu Liu,Weiping Wang,Jiebo Luo,Xiaoyan Gu*

Key words: 联邦学习,类增量学习,数据异质性,通信效率

TL;DR: 论文提出了一种新的联邦类增量学习方法STSA，解决了现有方法中的数据异质性问题和计算通信开销，性能优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有联邦类增量学习方法无法避免数据异质性导致的空间-时间漂移，且计算和通信开销大，限制了实际应用。

Method: 提出了空间-时间统计聚合（STSA）框架，统一聚合特征统计信息；并设计了通信高效版本STSA-E。

Result: 在三个数据集上的实验表明，该方法在性能、灵活性和计算通信效率上优于现有方法。

Conclusion: STSA和STSA-E有效解决了联邦类增量学习中的数据异质性和开销问题。

Abstract: Federated Class-Incremental Learning (FCIL) enables Class-Incremental
Learning (CIL) from distributed data. Existing FCIL methods typically integrate
old knowledge preservation into local client training. However, these methods
cannot avoid spatial-temporal client drift caused by data heterogeneity and
often incur significant computational and communication overhead, limiting
practical deployment. To address these challenges simultaneously, we propose a
novel approach, Spatial-Temporal Statistics Aggregation (STSA), which provides
a unified framework to aggregate feature statistics both spatially (across
clients) and temporally (across stages). The aggregated feature statistics are
unaffected by data heterogeneity and can be used to update the classifier in
closed form at each stage. Additionally, we introduce STSA-E, a
communication-efficient variant with theoretical guarantees, achieving similar
performance to STSA-E with much lower communication overhead. Extensive
experiments on three widely used FCIL datasets, with varying degrees of data
heterogeneity, show that our method outperforms state-of-the-art FCIL methods
in terms of performance, flexibility, and both communication and computation
efficiency.

</details>


### [395] [NoiseAR: AutoRegressing Initial Noise Prior for Diffusion Models](https://arxiv.org/abs/2506.01337)
*Zeming Li,Xiangyue Liu,Xiangyu Zhang,Ping Tan,Heung-Yeung Shum*

Key words: 扩散模型, 初始噪声先验, 自回归模型, 条件生成, 可控生成

TL;DR: 本文提出了一种名为NoiseAR的新方法，用于扩散模型的自动回归初始噪声先验，通过学习生成动态且可控的初始噪声分布，提升样本质量和条件输入的连贯性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统扩散模型的初始状态通常从固定的简单分布中采样，缺乏结构和外部控制机制。NoiseAR旨在通过动态和可控的先验分布改进这一局限性。

Method: NoiseAR将初始噪声先验的参数生成任务建模为空间块或令牌上的自回归概率模型，从而捕获复杂的空间依赖关系，并允许条件输入（如文本提示）直接影响先验。

Result: 实验表明，NoiseAR生成的初始噪声先验能显著提升样本质量，并增强与条件输入的一致性，同时也支持与其他概率框架（如MDP和强化学习）的无缝集成。

Conclusion: NoiseAR提供了一种高效、可控的扩散模型初始噪声生成方法，为扩散模型的初始化阶段引入了学习和动态结构。

Abstract: Diffusion models have emerged as powerful generative frameworks, creating
data samples by progressively denoising an initial random state. Traditionally,
this initial state is sampled from a simple, fixed distribution like isotropic
Gaussian, inherently lacking structure and a direct mechanism for external
control. While recent efforts have explored ways to introduce controllability
into the diffusion process, particularly at the initialization stage, they
often rely on deterministic or heuristic approaches. These methods can be
suboptimal, lack expressiveness, and are difficult to scale or integrate into
more sophisticated optimization frameworks. In this paper, we introduce
NoiseAR, a novel method for AutoRegressive Initial Noise Prior for Diffusion
Models. Instead of a static, unstructured source, NoiseAR learns to generate a
dynamic and controllable prior distribution for the initial noise. We formulate
the generation of the initial noise prior's parameters as an autoregressive
probabilistic modeling task over spatial patches or tokens. This approach
enables NoiseAR to capture complex spatial dependencies and introduce learned
structure into the initial state. Crucially, NoiseAR is designed to be
conditional, allowing text prompts to directly influence the learned prior,
thereby achieving fine-grained control over the diffusion initialization. Our
experiments demonstrate that NoiseAR can generate initial noise priors that
lead to improved sample quality and enhanced consistency with conditional
inputs, offering a powerful, learned alternative to traditional random
initialization. A key advantage of NoiseAR is its probabilistic formulation,
which naturally supports seamless integration into probabilistic frameworks
like Markov Decision Processes and Reinforcement Learning. Our code will be
available at https://github.com/HKUST-SAIL/NoiseAR/

</details>


### [396] [Invariance Makes LLM Unlearning Resilient Even to Unanticipated Downstream Fine-Tuning](https://arxiv.org/abs/2506.01339)
*Changsheng Wang,Yihua Zhang,Jinghan Jia,Parikshit Ram,Dennis Wei,Yuguang Yao,Soumyadeep Pal,Nathalie Baracaldo,Sijia Liu*

Key words: 机器遗忘、大型语言模型、不变风险最小化、下游微调、鲁棒性

TL;DR: 本文提出了一种基于不变风险最小化（IRM）的新方法ILU，用于增强大型语言模型（LLMs）的“遗忘”能力，使其在面对下游微调时更鲁棒，同时保持模型的实用性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现有模型遗忘方法对下游微调高度敏感的问题，避免遗忘的知识被快速恢复。

Method: 提出ILU框架，利用不变风险最小化（IRM）引入不变性，通过正则化增强遗忘的鲁棒性。

Result: ILU在WMDP和MUSE基准测试中显著优于NPO和RMU等现有方法，并在多种下游任务中表现出色。

Conclusion: ILU通过引入不变性，有效提升了模型遗忘的鲁棒性，同时不影响下游任务的性能。

Abstract: Machine unlearning offers a promising solution to privacy and safety concerns
in large language models (LLMs) by selectively removing targeted knowledge
while preserving utility. However, current methods are highly sensitive to
downstream fine-tuning, which can quickly recover forgotten information-even
from unrelated tasks. To address this, we introduce invariance into unlearning
for the first time, inspired by invariant risk minimization (IRM). Building on
this principle, we propose invariant LLM unlearning (ILU), a
regularization-based framework that enhances robustness. Notably, ILU
generalizes well to diverse fine-tuning tasks, even when trained using a single
dataset. A task vector analysis is also provided to further elucidate the
rationale behind ILU's effectiveness. Extensive experiments on the WMDP and
MUSE benchmark, reveal that ILU significantly outperforms state-of-the-art
unlearning methods, including negative preference optimization (NPO) and
representation misdirection for unlearning (RMU). Notably, ILU achieves
superior unlearning robustness across diverse downstream fine-tuning scenarios
(e.g., math, paraphrase detection, and sentiment analysis) while preserving the
fine-tuning performance.

</details>


### [397] [Distributionally Robust Learning in Survival Analysis](https://arxiv.org/abs/2506.01348)
*Yeping Jin,Lauren Wise,Ioannis Paschalidis*

Key words: Cox回归,分布鲁棒学习,Wasserstein距离,生存预测

TL;DR: 提出一种结合分布鲁棒学习(DRL)的Cox回归方法，增强生存预测的稳健性和准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统Cox回归对数据分布的假设敏感，且易受模型误设和数据扰动影响，需改进。

Method: 利用Wasserstein距离构建DRL框架，并转化为可处理的正则化经验风险最小化问题。

Result: 通过仿真和案例研究，证明方法在预测准确性和稳健性上优于传统方法。

Conclusion: 提出的DRL-Cox模型显著提升了生存预测的稳健性和准确性。

Abstract: We introduce an innovative approach that incorporates a Distributionally
Robust Learning (DRL) approach into Cox regression to enhance the robustness
and accuracy of survival predictions. By formulating a DRL framework with a
Wasserstein distance-based ambiguity set, we develop a variant Cox model that
is less sensitive to assumptions about the underlying data distribution and
more resilient to model misspecification and data perturbations. By leveraging
Wasserstein duality, we reformulate the original min-max DRL problem into a
tractable regularized empirical risk minimization problem, which can be
computed by exponential conic programming. We provide guarantees on the finite
sample behavior of our DRL-Cox model. Moreover, through extensive simulations
and real world case studies, we demonstrate that our regression model achieves
superior performance in terms of prediction accuracy and robustness compared
with traditional methods.

</details>


### [398] [Variational Adaptive Noise and Dropout towards Stable Recurrent Neural Networks](https://arxiv.org/abs/2506.01350)
*Taisuke Kobayashi,Shingo Murata*

Key words: RNN, 变分推理, 噪声, Dropout, 模仿学习

TL;DR: 本文提出了一种名为VAND的新型稳定学习理论，用于增强RNN的稳定性，将噪声和Dropout统一为隐式正则化。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决RNN中噪声和Dropout作为独立稳定因素的局限性，通过变分推理将其统一为优化问题。

Method: 通过变分推理将显式正则化转换为隐式正则化，动态调整噪声和Dropout的规模与比例。

Result: 在移动机械臂的模仿学习任务中，VAND成功模仿了顺序和周期性行为。

Conclusion: VAND为RNN提供了一种统一的稳定化方法，显著提高了其在复杂任务中的表现。

Abstract: This paper proposes a novel stable learning theory for recurrent neural
networks (RNNs), so-called variational adaptive noise and dropout (VAND). As
stabilizing factors for RNNs, noise and dropout on the internal state of RNNs
have been separately confirmed in previous studies. We reinterpret the
optimization problem of RNNs as variational inference, showing that noise and
dropout can be derived simultaneously by transforming the explicit
regularization term arising in the optimization problem into implicit
regularization. Their scale and ratio can also be adjusted appropriately to
optimize the main objective of RNNs, respectively. In an imitation learning
scenario with a mobile manipulator, only VAND is able to imitate sequential and
periodic behaviors as instructed. https://youtu.be/UOho3Xr6A2w

</details>


### [399] [TAH-QUANT: Effective Activation Quantization in Pipeline Parallelism over Slow Network](https://arxiv.org/abs/2506.01352)
*Guangxin He,Yuan Cao,Yutong He,Tianyi Bai,Kun Yuan,Binhang Yuan*

Key words: 分布式训练, 激活量化, 管道并行, TAH-Quant, 大型语言模型

TL;DR: 提出了一种名为TAH-Quant的新激活量化框架，专门用于解决分布式训练中的网络通信瓶颈问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 分布式训练大型语言模型需要频繁通信中间激活数据，但网络带宽有限导致效率低下。现有方法如AQ-SGD因存储历史激活数据而带来过高内存开销。

Method: TAH-Quant通过细粒度分块量化、基于熵的令牌级自适应比特分配和Hadamard变换，有效压制量化异常值。

Result: 实验表明，该框架可实现3-4位的激进量化，端到端加速达4.3倍，且不影响训练收敛。

Conclusion: TAH-Quant为分布式训练提供高效通信解决方案，兼具速度和内存效率。

Abstract: Decentralized training of large language models offers the opportunity to
pool computational resources across geographically distributed participants but
faces significant network communication bottlenecks, particularly in
pipeline-parallel settings. While pipeline parallelism partitions model layers
across devices to handle large-scale models, it necessitates frequent
communication of intermediate activations, creating challenges when network
bandwidth is limited. Existing activation compression methods, such as AQ-SGD,
mitigate quantization-induced errors through error compensation but impose
prohibitive memory overhead by requiring storage of previous activations. To
address these issues, we introduce TAH-Quant (Tile-wise Adaptive Hadamard
Quantization), a novel activation quantization framework designed specifically
for pipeline parallelism. Our approach integrates fine-grained tile-wise
quantization for precise control, entropy-guided token-level adaptive bit
allocation for optimal bit usage, and a Hadamard-based transform with pivot
element swapping to effectively suppress quantization outliers. We further
provide a theoretical analysis, proving that pipeline parallel training
equipped with TAH-Quant maintains a convergence rate of
$\mathcal{O}(1/\sqrt{T})$, matching that of vanilla stochastic gradient
descent. Extensive experiments on diverse LLM tasks demonstrate that TAH-Quant
achieves aggressive activation quantization (3-4 bits) ratio, which provides up
to 4.3$\times$ end-to-end speedup without compromising training convergence,
matches state-of-the-art methods, incurs no extra memory overhead, and
generalizes well across different training scenarios.

</details>


### [400] [Two-Stage Learning of Stabilizing Neural Controllers via Zubov Sampling and Iterative Domain Expansion](https://arxiv.org/abs/2506.01356)
*Haoyu Li,Xiangru Zhong,Bin Hu,Huan Zhang*

Key words: 神经网络控制, Lyapunov函数, 稳定性验证, Zubov启发, 两阶段训练

TL;DR: 提出了一种新型两阶段训练框架，用于联合合成控制器和Lyapunov函数，显著降低了保守性，并通过改进的验证方法提升了效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决基于学习的神经网络控制策略在稳定性和区域吸引力估计方面的挑战。

Method: 采用两阶段训练框架，结合Zubov启发的区域吸引力特征和改进的数据采样策略，扩展神经网络验证器能力。

Result: 实验显示，区域吸引力体积比基线大5-1.5×10^5倍，验证速度比传统方法快40-10000倍。

Conclusion: 该框架在降低保守性和提升效率方面表现出色，适用于高维非线性系统。

Abstract: Learning-based neural network (NN) control policies have shown impressive
empirical performance. However, obtaining stability guarantees and estimations
of the region of attraction of these learned neural controllers is challenging
due to the lack of stable and scalable training and verification algorithms.
Although previous works in this area have achieved great success, much
conservatism remains in their framework. In this work, we propose a novel
two-stage training framework to jointly synthesize the controller and Lyapunov
function for continuous-time systems. By leveraging a Zubov-inspired region of
attraction characterization to directly estimate stability boundaries, we
propose a novel training data sampling strategy and a domain updating mechanism
that significantly reduces the conservatism in training. Moreover, unlike
existing works on continuous-time systems that rely on an SMT solver to
formally verify the Lyapunov condition, we extend state-of-the-art neural
network verifier $\alpha,\!\beta$-CROWN with the capability of performing
automatic bound propagation through the Jacobian of dynamical systems and a
novel verification scheme that avoids expensive bisection. To demonstrate the
effectiveness of our approach, we conduct numerical experiments by synthesizing
and verifying controllers on several challenging nonlinear systems across
multiple dimensions. We show that our training can yield region of attractions
with volume $5 - 1.5\cdot 10^{5}$ times larger compared to the baselines, and
our verification on continuous systems can be up to $40-10000$ times faster
compared to the traditional SMT solver dReal. Our code is available at
https://github.com/Verified-Intelligence/Two-Stage_Neural_Controller_Training.

</details>


### [401] [RDB2G-Bench: A Comprehensive Benchmark for Automatic Graph Modeling of Relational Databases](https://arxiv.org/abs/2506.01360)
*Dongwon Choi,Sunwoo Kim,Juyeon Kim,Kyungho Kim,Geon Lee,Shinhwan Kang,Myunghwan Kim,Kijung Shin*

Key words: 关系数据库, 图建模, 基准框架, 机器学习

TL;DR: 该论文介绍了RDB2G-Bench，第一个用于评估关系数据库（RDB）到图模型转换方法的基准框架，通过分析不同图建模方法性能差异，揭示了影响模型效果的关键结构模式。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 关系数据库的图建模方法多种多样，但性能差异显著，且缺乏统一评估标准，因此需要系统化工具促进该领域研究。

Method: 提出了RDB2G-Bench基准框架，构建了涵盖5个真实RDB和12项预测任务的大规模数据集，共约50k图-性能对，用于高效评估9种自动图建模方法。

Result: 基准测试结果显示，不同的图建模方法性能差异可达10%，且通过预计算数据集可将评估速度提升600倍以上。

Conclusion: RDB2G-Bench为RDB图建模研究提供了标准评估工具，并揭示了影响模型效果的关键结构模式。

Abstract: Relational databases (RDBs) are composed of interconnected tables, where
relationships between them are defined through foreign keys. Recent research on
applying machine learning to RDBs has explored graph-based representations of
RDBs, where rows of tables are modeled as nodes, and foreign key relationships
are modeled as edges. RDB-to-graph modeling helps capture cross-table
dependencies, ultimately leading to enhanced performance across diverse tasks.
However, there are numerous ways to model RDBs as graphs, and performance
varies significantly depending on the chosen graph model. In our analysis,
applying a common heuristic rule for graph modeling leads to up to a 10% drop
in performance compared to the best-performing graph model, which remains
non-trivial to identify. To foster research on intelligent RDB-to-graph
modeling, we introduce RDB2G-Bench, the first benchmark framework for
evaluating such methods. We construct extensive datasets covering 5 real-world
RDBs and 12 predictive tasks, resulting in around 50k graph-performance pairs
for efficient and reproducible evaluations. Thanks to our precomputed datasets,
we were able to benchmark 9 automatic RDB-to-graph modeling methods on the 12
tasks over 600x faster than on-the-fly evaluation, which requires repeated
model training. Our analysis of the datasets and benchmark results reveals key
structural patterns affecting graph model effectiveness, along with practical
implications for effective graph modeling.

</details>


### [402] [TimeGraph: Synthetic Benchmark Datasets for Robust Time-Series Causal Discovery](https://arxiv.org/abs/2506.01361)
*Muhammad Hasan Ferdous,Emam Hossain,Md Osman Gani*

Key words: TimeGraph,合成时间序列,因果发现,基准数据集

TL;DR: TimeGraph是一个合成时间序列基准数据集套件，用于评估因果发现算法，解决了现有数据缺乏真实时间特性的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有因果发现的基准数据集稀缺且忽略真实时间特性，如非平稳性、不规则采样和未观察到的混杂因素。

Method: 引入TimeGraph套件，系统整合线性和非线性依赖关系，模拟趋势、季节效应和异质性噪声等时间特性。

Result: 实验显示，算法在真实时间条件下性能差异显著，突显了稳健合成基准的必要性。

Conclusion: TimeGraph为公平透明评估因果发现方法提供了工具，促进可重复研究和社区发展。

Abstract: Robust causal discovery in time series datasets depends on reliable benchmark
datasets with known ground-truth causal relationships. However, such datasets
remain scarce, and existing synthetic alternatives often overlook critical
temporal properties inherent in real-world data, including nonstationarity
driven by trends and seasonality, irregular sampling intervals, and the
presence of unobserved confounders. To address these challenges, we introduce
TimeGraph, a comprehensive suite of synthetic time-series benchmark datasets
that systematically incorporates both linear and nonlinear dependencies while
modeling key temporal characteristics such as trends, seasonal effects, and
heterogeneous noise patterns. Each dataset is accompanied by a fully specified
causal graph featuring varying densities and diverse noise distributions and is
provided in two versions: one including unobserved confounders and one without,
thereby offering extensive coverage of real-world complexity while preserving
methodological neutrality. We further demonstrate the utility of TimeGraph
through systematic evaluations of state-of-the-art causal discovery algorithms
including PCMCI+, LPCMCI, and FGES across a diverse array of configurations and
metrics. Our experiments reveal significant variations in algorithmic
performance under realistic temporal conditions, underscoring the need for
robust synthetic benchmarks in the fair and transparent assessment of causal
discovery methods. The complete TimeGraph suite, including dataset generation
scripts, evaluation metrics, and recommended experimental protocols, is freely
available to facilitate reproducible research and foster community-driven
advancements in time-series causal discovery.

</details>


### [403] [Unraveling Spatio-Temporal Foundation Models via the Pipeline Lens: A Comprehensive Review](https://arxiv.org/abs/2506.01364)
*Yuchen Fang,Hao Miao,Yuxuan Liang,Liwei Deng,Yue Cui,Ximu Zeng,Yuyang Xia,Yan Zhao,Torben Bach Pedersen,Christian S. Jensen,Xiaofang Zhou,Kai Zheng*

Key words: 时空数据,深度学习,基础模型,预训练,流程框架

TL;DR: 深度学习模型在处理时空数据时通常需要针对特定任务单独训练，增加了计算和存储成本。时空基础模型通过统一框架解决了这一问题，但现有研究缺乏对其设计、选择、预训练和适配的全面研究。本文从流程角度对时空基础模型进行了最新综述，提供了一个清晰的流程框架，帮助研究者快速上手。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决时空深度学习模型中单任务训练带来的高成本和效率问题，并填补现有研究中关于时空基础模型设计、选择、预训练和适配的全面研究的空白。

Method: 从流程角度综述时空基础模型，包括数据类型介绍、数据预处理与嵌入技术，以及基于数据属性和依赖关系的分类方法。

Result: 提供了一个清晰的时空基础模型流程框架，帮助研究者理解模型设计与选择，并介绍了新兴研究方向如多目标训练。

Conclusion: 本文为研究者提供了一个系统化的流程框架，促进了对时空基础模型的全面理解，并推动了该领域的进一步发展。

Abstract: Spatio-temporal deep learning models aims to utilize useful patterns in such
data to support tasks like prediction. However, previous deep learning models
designed for specific tasks typically require separate training for each use
case, leading to increased computational and storage costs. To address this
issue, spatio-temporal foundation models have emerged, offering a unified
framework capable of solving multiple spatio-temporal tasks. These foundation
models achieve remarkable success by learning general knowledge with
spatio-temporal data or transferring the general capabilities of pre-trained
language models. While previous surveys have explored spatio-temporal data and
methodologies separately, they have ignored a comprehensive examination of how
foundation models are designed, selected, pre-trained, and adapted. As a
result, the overall pipeline for spatio-temporal foundation models remains
unclear. To bridge this gap, we innovatively provide an up-to-date review of
previous spatio-temporal foundation models from the pipeline perspective. The
pipeline begins with an introduction to different types of spatio-temporal
data, followed by details of data preprocessing and embedding techniques. The
pipeline then presents a novel data property taxonomy to divide existing
methods according to data sources and dependencies, providing efficient and
effective model design and selection for researchers. On this basis, we further
illustrate the training objectives of primitive models, as well as the
adaptation techniques of transferred models. Overall, our survey provides a
clear and structured pipeline to understand the connection between core
elements of spatio-temporal foundation models while guiding researchers to get
started quickly. Additionally, we introduce emerging opportunities such as
multi-objective training in the field of spatio-temporal foundation models.

</details>


### [404] [Incentivizing LLMs to Self-Verify Their Answers](https://arxiv.org/abs/2506.01369)
*Fuxiang Zhang,Jiacheng Xu,Chaojie Wang,Ce Cui,Yang Liu,Bo An*

Key words: 自验证, 强化学习, LLMs, 数学推理

TL;DR: 本文提出了一种自验证框架，通过强化学习统一答案生成与验证，提升了LLMs在推理任务中的表现，无需外部验证器。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有测试时扩展方法依赖外部奖励模型，但对特定任务后训练的模型效果提升有限，原因为生成器与奖励模型的分布差异。

Method: 设计自验证框架，结合强化学习训练模型自我评估答案，并在推理时进一步扩展性能。

Result: 在多个数学推理基准测试中，模型不仅提升了后训练性能，还实现了有效的测试时扩展。

Conclusion: 自验证框架能有效解决分布差异问题，提升LLMs的推理能力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in complex
reasoning tasks through both post-training and test-time scaling laws. While
prevalent test-time scaling approaches are often realized by using external
reward models to guide the model generation process, we find only marginal
gains can be acquired when scaling a model post-trained on specific reasoning
tasks. We identify that the limited improvement stems from distribution
discrepancies between the specific post-trained generator and the general
reward model. To address this, we propose a framework that incentivizes LLMs to
self-verify their own answers. By unifying answer generation and verification
within a single reinforcement learning (RL) process, we train models that can
effectively assess the correctness of their own solutions. The trained model
can further scale its performance during inference time by verifying its
generations, without the need for external verifiers. We train our
self-verification models based on Qwen2.5-Math-7B and
DeepSeek-R1-Distill-Qwen-1.5B, demonstrating its capabilities across varying
reasoning context lengths. Experiments on multiple mathematical reasoning
benchmarks show that our models can not only improve post-training performance
but also enable effective test-time scaling. Our code is available at
https://github.com/mansicer/self-verification.

</details>


### [405] [Compiler Optimization via LLM Reasoning for Efficient Model Serving](https://arxiv.org/abs/2506.01374)
*Sujun Tang,Christopher Priebe,Rohan Mahapatra,Lianhui Qin,Hadi Esmaeilzadeh*

Key words: 模型服务, 编译器优化, 大型语言模型, 蒙特卡洛树搜索, 样本效率

TL;DR: 该论文探讨了是否可以利用大型语言模型（LLM）无需重新训练的情况下，结合上下文感知决策来提升编译器优化的样本效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管编译器优化带来了显著的性能提升，但大规模模型的高服务成本仍是广泛可访问性和快速创新的障碍。现有编译器难以处理神经网络工作负载的复杂性。

Method: 提出了一种名为REASONING COMPILER的新编译框架，结合LLM和蒙特卡洛树搜索（MCTS），将优化建模为顺序的上下文感知决策过程。

Result: 通过比领先的神经编译器更少的样本实现了显著的加速。

Conclusion: LLM引导的推理有潜力改变编译器优化的格局。

Abstract: While model serving has unlocked unprecedented capabilities, the high cost of
serving large-scale models continues to be a significant barrier to widespread
accessibility and rapid innovation. Compiler optimizations have long driven
substantial performance improvements, but existing compilers struggle with
neural workloads due to the exponentially large and highly interdependent space
of possible transformations. Although existing stochastic search techniques can
be effective, they are often sample-inefficient and fail to leverage the
structural context underlying compilation decisions. We set out to investigate
the research question of whether reasoning with large language models (LLMs),
without any retraining, can leverage the context-aware decision space of
compiler optimization to significantly improve sample efficiency. To that end,
we introduce a novel compilation framework (dubbed REASONING COMPILER) that
formulates optimization as a sequential, context-aware decision process, guided
by a large language model and structured Monte Carlo tree search (MCTS). The
LLM acts as a proposal mechanism, suggesting hardware-aware transformations
that reflect the current program state and accumulated performance feedback.
Monte Carlo tree search (MCTS) incorporates the LLM-generated proposals to
balance exploration and exploitation, facilitating structured,
context-sensitive traversal of the expansive compiler optimization space. By
achieving substantial speedups with markedly fewer samples than leading neural
compilers, our approach demonstrates the potential of LLM-guided reasoning to
transform the landscape of compiler optimization.

</details>


### [406] [Modeling All-Atom Glycan Structures via Hierarchical Message Passing and Multi-Scale Pre-training](https://arxiv.org/abs/2506.01376)
*Minghao Xu,Jiaze Song,Keming Wu,Xiangxin Zhou,Bin Cui,Wentao Zhang*

Key words: 糖链建模, 机器学习, 异构图, 预训练

TL;DR: GlycanAA模型通过全原子级别的糖链建模，弥补了以往方法忽略单糖原子结构的不足，并通过层次化消息传递和预训练进一步提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的糖链建模方法主要关注单糖骨架结构，忽略了原子级别结构的重要性，GlycanAA旨在填补这一空白。

Method: GlycanAA将糖链建模为异构图，包含单糖节点和原子节点，并通过层次化消息传递和预训练（PreGlycanAA）增强模型能力。

Result: 实验证明GlycanAA优于现有糖链编码器，预训练模型进一步提升了性能。

Conclusion: GlycanAA通过全原子建模和预训练，显著提升了糖链性质预测的性能。

Abstract: Understanding the various properties of glycans with machine learning has
shown some preliminary promise. However, previous methods mainly focused on
modeling the backbone structure of glycans as graphs of monosaccharides (i.e.,
sugar units), while they neglected the atomic structures underlying each
monosaccharide, which are actually important indicators of glycan properties.
We fill this blank by introducing the GlycanAA model for All-Atom-wise Glycan
modeling. GlycanAA models a glycan as a heterogeneous graph with monosaccharide
nodes representing its global backbone structure and atom nodes representing
its local atomic-level structures. Based on such a graph, GlycanAA performs
hierarchical message passing to capture from local atomic-level interactions to
global monosaccharide-level interactions. To further enhance model capability,
we pre-train GlycanAA on a high-quality unlabeled glycan dataset, deriving the
PreGlycanAA model. We design a multi-scale mask prediction algorithm to endow
the model about different levels of dependencies in a glycan. Extensive
benchmark results show the superiority of GlycanAA over existing glycan
encoders and verify the further improvements achieved by PreGlycanAA. We
maintain all resources at https://github.com/kasawa1234/GlycanAA

</details>


### [407] [ThinkEval: Practical Evaluation of Knowledge Preservation and Consistency in LLM Editing with Thought-based Knowledge Graphs](https://arxiv.org/abs/2506.01386)
*Manit Baser,Dinil Mon Divakaran,Mohan Gurusamy*

Key words: 模型编辑, 大语言模型, ThinkEval, KnowGIC, 深度编辑

TL;DR: 该论文提出了一种新的模型编辑设置“深度编辑”，以解决现有编辑技术无法处理关联知识的问题，并引入了ThinkEval框架和KnowGIC基准来评估编辑技术的影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大语言模型（LLM）的编辑技术通常只针对孤立事实，忽略了关联知识的连锁反应，导致编辑后的知识完整性受损。

Method: 论文提出了深度编辑设置，并开发了ThinkEval框架和KnowGIC基准，通过构建特定知识图谱来评估编辑技术对事实持久性和灾难性遗忘的影响。

Result: 评估了五种编辑技术在多个LLM上的表现，发现这些技术难以平衡间接事实抑制与关联知识保存。

Conclusion: 现有编辑技术在处理关联知识和上下文完整性方面存在不足，未来的研究需要改进这一点。

Abstract: Model editing has become an important tool for addressing privacy, bias, and
misinformation in large language models (LLMs) by enabling updates to knowledge
without the need for retraining from scratch. However, existing editing
techniques often target isolated facts, ignoring ripple effects on related
knowledge, allowing edited facts to remain deducible and compromising broader
contextual integrity. For example, changing Harry Potter's school from Hogwarts
to Ilvermorny requires reassigning his house from Gryffindor to a suitable
alternative while preserving Gryffindor's relationship with Hogwarts. In this
work, we present a new model-editing setting, deep editing, to show: (1) how
editing techniques fail to handle connected facts, evaluating how original
knowledge sneaks through unchanged causal links, and (2) their impact on
broader contextual knowledge. We introduce ThinkEval, a framework to
systematically evaluate model-editing techniques by building model-specific
knowledge graphs to analyze pre- and post-edit effects on fact persistence and
catastrophic forgetting. We present KnowGIC, a benchmark created with
ThinkEval, consisting of sequentially linked queries to measure these effects.
We evaluate five editing techniques: AlphaEdit, RECT, ROME, MEMIT, and PRUNE
across multiple LLMs. We find that these techniques struggle to balance
indirect fact suppression with the preservation of related knowledge. Our
dataset is available at: https://anonymous.4open.science/r/KnowGIC.

</details>


### [408] [Multi Part Deployment of Neural Network](https://arxiv.org/abs/2506.01387)
*Paritosh Ranjan,Surajit Majumder,Prodip Roy*

Key words: 

TL;DR: 提出了一个分布式系统架构，将神经网络分布到多个服务器上，通过元数据驱动的查找机制管理远程神经元，实现高效的分布式训练。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现代神经网络规模越来越大，传统的集中式GPU集群训练方式成本高且不可持续，需要一种更高效的分布式解决方案。

Method: 设计了一个分布式系统架构，将神经网络分区到多台服务器，每台服务器负责一部分神经元，通过元数据驱动查找远程神经元，并利用NFS保持模型一致性。

Result: 该架构降低了高性能集中式计算资源的依赖，实现了深度学习模型在云端的高效、低成本部署。

Conclusion: 分布式系统架构能够有效支持大规模神经网络的训练和部署，具有高扩展性和成本效益。

Abstract: The increasing scale of modern neural networks, exemplified by architectures
from IBM (530 billion neurons) and Google (500 billion parameters), presents
significant challenges in terms of computational cost and infrastructure
requirements. As deep neural networks continue to grow, traditional training
paradigms relying on monolithic GPU clusters become increasingly unsustainable.
This paper proposes a distributed system architecture that partitions a neural
network across multiple servers, each responsible for a subset of neurons.
Neurons are classified as local or remote, with inter-server connections
managed via a metadata-driven lookup mechanism. A Multi-Part Neural Network
Execution Engine facilitates seamless execution and training across distributed
partitions by dynamically resolving and invoking remote neurons using stored
metadata. All servers share a unified model through a network file system
(NFS), ensuring consistency during parallel updates. A Neuron Distributor
module enables flexible partitioning strategies based on neuron count,
percentage, identifiers, or network layers. This architecture enables
cost-effective, scalable deployment of deep learning models on cloud
infrastructure, reducing dependency on high-performance centralized compute
resources.

</details>


### [409] [Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization](https://arxiv.org/abs/2506.01393)
*Shogo Iwazaki*

Key words: 贝叶斯优化, 高斯过程, GP-UCB, 遗憾上界, 信息增益

TL;DR: 研究了贝叶斯优化问题，证明GP-UCB算法在特定高斯过程下达到较低累积遗憾。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 填补现有GP-UCB遗憾上界与Scarlett (2018)最优界之间的理论空白。

Method: 采用Matérn核和平方指数核，通过分析输入序列的集中性，优化高斯过程信息增益。

Result: 在Matérn核下实现$	ilde{O}(\sqrt{T})$遗憾，平方指数核下为$O(\sqrt{T \ln^4 T})$。

Conclusion: GP-UCB算法在特定高斯过程条件下表现出色，遗憾上界接近最优。

Abstract: This paper addresses the Bayesian optimization problem (also referred to as
the Bayesian setting of the Gaussian process bandit), where the learner seeks
to minimize the regret under a function drawn from a known Gaussian process
(GP). Under a Mat\'ern kernel with a certain degree of smoothness, we show that
the Gaussian process upper confidence bound (GP-UCB) algorithm achieves
$\tilde{O}(\sqrt{T})$ cumulative regret with high probability. Furthermore, our
analysis yields $O(\sqrt{T \ln^4 T})$ regret under a squared exponential
kernel. These results fill the gap between the existing regret upper bound for
GP-UCB and the best-known bound provided by Scarlett (2018). The key idea in
our proof is to capture the concentration behavior of the input sequence
realized by GP-UCB, enabling a more refined analysis of the GP's information
gain.

</details>


### [410] [Mitigating Disparate Impact of Differentially Private Learning through Bounded Adaptive Clipping](https://arxiv.org/abs/2506.01396)
*Linzh Zhao,Aki Rehn,Mikko A. Heikkilä,Razane Tajeddine,Antti Honkela*

Key words: 差分隐私, 梯度裁剪, 自适应裁剪, 公平性, 机器学习

TL;DR: 论文提出了一种新的有界自适应裁剪方法，解决了DP学习中梯度裁剪对少数群体的不公平影响，显著提升了模型的准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的差分隐私（DP）学习方法在模型预测中存在不平等影响，例如对少数群体的准确性较低。自适应梯度裁剪会为了适应多数群体而过小地裁剪梯度，进一步加剧了这一问题。

Method: 提出了一种有界自适应裁剪方法，通过引入可调节的下界，避免过度裁剪梯度。

Result: 实验表明，与无界自适应裁剪和固定裁剪相比，该方法在偏斜的MNIST和Fashion MNIST数据集上，显著提高了表现最差类的准确性（分别提高了10%和5%以上）。

Conclusion: 有界自适应裁剪方法有效缓解了DP学习中梯度裁剪对少数群体的不公平性，提高了模型的整体公平性和准确性。

Abstract: Differential privacy (DP) has become an essential framework for
privacy-preserving machine learning. Existing DP learning methods, however,
often have disparate impacts on model predictions, e.g., for minority groups.
Gradient clipping, which is often used in DP learning, can suppress larger
gradients from challenging samples. We show that this problem is amplified by
adaptive clipping, which will often shrink the clipping bound to tiny values to
match a well-fitting majority, while significantly reducing the accuracy for
others. We propose bounded adaptive clipping, which introduces a tunable lower
bound to prevent excessive gradient suppression. Our method improves the
accuracy of the worst-performing class on average over 10 percentage points on
skewed MNIST and Fashion MNIST compared to the unbounded adaptive clipping, and
over 5 percentage points over constant clipping.

</details>


### [411] [Quantitative Error Feedback for Quantization Noise Reduction of Filtering over Graphs](https://arxiv.org/abs/2506.01404)
*Xue Xian Zheng,Weihang Liu,Xin Lou,Stefan Vlaski,Tareq Al-Naffouri*

Key words: 误差反馈, 量化噪声, 分布式图滤波, 状态空间滤波器

TL;DR: 本文提出了一种创新的误差反馈框架，用于减少分布式图滤波中的量化噪声，并展示了其在多种场景下的高效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对分布式图滤波中量化通信导致的噪声问题，研究一种能够精确补偿量化噪声的误差反馈框架。

Method: 结合状态空间数字滤波器的误差谱整形技术，提出定量误差反馈机制，并在确定性图滤波、随机图滤波和随机节点异步更新三种场景下验证。

Result: 理论分析和数值实验表明，该框架显著降低了量化噪声的影响，提供了最优误差反馈系数的闭式解，并在精度和鲁棒性上优于传统方法。

Conclusion: 提出的误差反馈框架能有效降低量化噪声，适用于高效的分布式优化框架。

Abstract: This paper introduces an innovative error feedback framework designed to
mitigate quantization noise in distributed graph filtering, where
communications are constrained to quantized messages. It comes from error
spectrum shaping techniques from state-space digital filters, and therefore
establishes connections between quantized filtering processes over different
domains. In contrast to existing error compensation methods, our framework
quantitatively feeds back the quantization noise for exact compensation. We
examine the framework under three key scenarios: (i) deterministic graph
filtering, (ii) graph filtering over random graphs, and (iii) graph filtering
with random node-asynchronous updates. Rigorous theoretical analysis
demonstrates that the proposed framework significantly reduces the effect of
quantization noise, and we provide closed-form solutions for the optimal error
feedback coefficients. Moreover, this quantitative error feedback mechanism can
be seamlessly integrated into communication-efficient decentralized
optimization frameworks, enabling lower error floors. Numerical experiments
validate the theoretical results, consistently showing that our method
outperforms conventional quantization strategies in terms of both accuracy and
robustness.

</details>


### [412] [SOC-DGL: Social Interaction Behavior Inspired Dual Graph Learning Framework for Drug-Target Interaction Identification](https://arxiv.org/abs/2506.01405)
*Xiang Zhao,Ruijie Li,Qiao Ning,Shikai Guo,Hui Li,Qian Ma*

Key words: 药物-靶标互作, 图学习, 异构图, 社交行为, 数据不平衡

TL;DR: SOC-DGL模型通过结合亲和力驱动图学习和均衡驱动图学习模块，有效挖掘异构图中的相似性信息，提升了药物-靶标互作预测的准确性和泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决现有模型在药物-靶标互作（DTI）预测中仅关注同构图直接相似性的局限性，作者从真实社交行为中获取灵感，提出了一种能够挖掘异构图丰富相似性信息的方法。

Method: SOC-DGL包含两个模块：亲和力驱动图学习（ADGL）和均衡驱动图学习（EDGL）。ADGL利用全局药物-靶标图学习全局DTI和个体相似性信息，而EDGL通过高阶社交策略挖掘间接的高阶同构信息。同时，提出了可调节的样本不平衡损失函数以解决数据不平衡问题。

Result: 在四个基准数据集上的实验表明，SOC-DGL显著提高了预测准确性，尤其是在数据不平衡和未见药物或靶标场景中。

Conclusion: SOC-DGL通过结合不同尺度的相似性信息和解决数据不平衡问题，为DTI预测提供了一种高效且全面的解决方案。

Abstract: The identification of drug-target interactions (DTI) is crucial for drug
discovery and repositioning, as it reveals potential uses of existing drugs,
aiding in the acceleration of the drug development process and reducing
associated costs. Despite the similarity information in DTI is important, most
models are limited to mining direct similarity information within homogeneous
graphs, overlooking the potential yet rich similarity information in
heterogeneous graphs. Inspired by real-world social interaction behaviors, we
propose SOC-DGL, which comprises two specialized modules: the Affinity-Driven
Graph Learning (ADGL) module and the Equilibrium-Driven Graph Learning (EDGL)
module. The ADGL module adopts a comprehensive social interaction strategy,
leveraging an affinity-enhanced global drug-target graph to learn both global
DTI and the individual similarity information of drugs and targets. In
contrast, the EDGL module employs a higher-order social interaction strategy,
amplifying the influence of even-hop neighbors through an even-polynomial graph
filter grounded in balance theory, enabling the indirect mining of higher-order
homogeneous information. This dual approach enables SOC-DGL to effectively and
comprehensively capture similarity information across diverse interaction
scales within the affinity matrices and drug-target association matrices,
significantly enhancing the model's generalization capability and predictive
accuracy in DTI tasks. To address the issue of imbalance in drug-target
interaction datasets, this paper proposes an adjustable imbalance loss function
that mitigates the impact of sample imbalance by adjusting the weight of
negative samples and a parameter. Extensive experiments on four benchmark
datasets demonstrate significant accuracy improvements achieved by SOC-DGL,
particularly in scenarios involving data imbalance and unseen drugs or targets.

</details>


### [413] [Self-supervised Latent Space Optimization with Nebula Variational Coding](https://arxiv.org/abs/2506.01414)
*Yida Wang,David Joseph Tan,Nassir Navab,Federico Tombari*

Key words: 深度学习,变分推理,潜在流形,聚类,自监督学习

TL;DR: 该论文提出了一种称为Nebula Variational Coding（NVC）的变分推理模型，通过引入nebula anchors优化潜在流形，提升分类、分割、完成和重建任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 深度学习中间特征的处理通常缺乏优化，本文旨在通过概率模型优化潜在流形，提升多任务性能。

Method: 提出NVC模型，引入nebula anchors引导潜在变量聚类，并通过变分约束避免anchors自身聚类，同时结合自监督度量学习明确聚类分离。

Result: 实验表明，NVC适用于文本序列、图像、3D点云和体数据等多种架构，验证了其优势。

Conclusion: NVC通过优化潜在流形和自监督学习，显著提升了多任务的性能。

Abstract: Deep learning approaches process data in a layer-by-layer way with
intermediate (or latent) features. We aim at designing a general solution to
optimize the latent manifolds to improve the performance on classification,
segmentation, completion and/or reconstruction through probabilistic models.
This paper proposes a variational inference model which leads to a clustered
embedding. We introduce additional variables in the latent space, called
\textbf{nebula anchors}, that guide the latent variables to form clusters
during training. To prevent the anchors from clustering among themselves, we
employ the variational constraint that enforces the latent features within an
anchor to form a Gaussian distribution, resulting in a generative model we
refer as Nebula Variational Coding (NVC). Since each latent feature can be
labeled with the closest anchor, we also propose to apply metric learning in a
self-supervised way to make the separation between clusters more explicit. As a
consequence, the latent variables of our variational coder form clusters which
adapt to the generated semantic of the training data, \textit{e.g.} the
categorical labels of each sample. We demonstrate experimentally that it can be
used within different architectures designed to solve different problems
including text sequence, images, 3D point clouds and volumetric data,
validating the advantage of our proposed method.

</details>


### [414] [Variance-Based Defense Against Blended Backdoor Attacks](https://arxiv.org/abs/2506.01444)
*Sujeevan Aseervatham,Achraf Kerzazi,Younès Bennani*

Key words: 后门攻击, AI安全, 防御方法, 触发器检测, 统计异常

TL;DR: 论文提出了一种新型防御方法，用于检测和清除AI模型中的后门攻击，有效解决了传统方法依赖干净数据集的问题，并通过实验验证了其优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统防御方法需要依赖干净数据集计算统计异常，这在真实场景中往往不可行。论文旨在解决这一局限性，提出不依赖干净数据集的后门攻击防御方法。

Method: 训练一个模型，检测被污染的类别，提取攻击触发器的关键部分，然后识别被污染的实例。该方法通过明确揭示触发器的有害部分增强了可解释性。

Result: 实验评估表明，该方法在知名图像数据集上有效，并比三种先进算法（SCAn、ABL和AGPD）表现更优。

Conclusion: 提出的方法为后门攻击防御提供了一种高效且无需干净数据集的解决方案，增强了模型的安全性。

Abstract: Backdoor attacks represent a subtle yet effective class of cyberattacks
targeting AI models, primarily due to their stealthy nature. The model behaves
normally on clean data but exhibits malicious behavior only when the attacker
embeds a specific trigger into the input. This attack is performed during the
training phase, where the adversary corrupts a small subset of the training
data by embedding a pattern and modifying the labels to a chosen target. The
objective is to make the model associate the pattern with the target label
while maintaining normal performance on unaltered data. Several defense
mechanisms have been proposed to sanitize training data-sets. However, these
methods often rely on the availability of a clean dataset to compute
statistical anomalies, which may not always be feasible in real-world scenarios
where datasets can be unavailable or compromised. To address this limitation,
we propose a novel defense method that trains a model on the given dataset,
detects poisoned classes, and extracts the critical part of the attack trigger
before identifying the poisoned instances. This approach enhances
explainability by explicitly revealing the harmful part of the trigger. The
effectiveness of our method is demonstrated through experimental evaluations on
well-known image datasets and comparative analysis against three
state-of-the-art algorithms: SCAn, ABL, and AGPD.

</details>


### [415] [ShaTS: A Shapley-based Explainability Method for Time Series Artificial Intelligence Models applied to Anomaly Detection in Industrial Internet of Things](https://arxiv.org/abs/2506.01450)
*Manuel Franco de la Peña,Ángel Luis Perales Gómez,Lorenzo Fernández Maimó*

Key words: 工业物联网、异常检测、时间序列、Shapley值、模型无关解释

TL;DR: ShaTS是一种针对时间序列模型的模型无关解释性AI方法，通过结合时间依赖性提高了Shapley值解释的精确性，优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 工业物联网环境中，异常检测和解释技术对确保运行安全至关重要，但传统方法往往忽略时间结构，导致解释不精确或不可操作。

Method: 提出ShaTS方法，采用先验特征分组策略保留时间依赖性，生成连贯且可操作的见解。

Result: 在SWaT数据集上测试表明，ShaTS能准确识别关键时间点、受影响的传感器和执行器，且在解释性和资源效率上优于SHAP。

Conclusion: ShaTS满足工业环境的实时需求，提供更精确且可操作的异常解释。

Abstract: Industrial Internet of Things environments increasingly rely on advanced
Anomaly Detection and explanation techniques to rapidly detect and mitigate
cyberincidents, thereby ensuring operational safety. The sequential nature of
data collected from these environments has enabled improvements in Anomaly
Detection using Machine Learning and Deep Learning models by processing time
windows rather than treating the data as tabular. However, conventional
explanation methods often neglect this temporal structure, leading to imprecise
or less actionable explanations. This work presents ShaTS (Shapley values for
Time Series models), which is a model-agnostic explainable Artificial
Intelligence method designed to enhance the precision of Shapley value
explanations for time series models. ShaTS addresses the shortcomings of
traditional approaches by incorporating an a priori feature grouping strategy
that preserves temporal dependencies and produces both coherent and actionable
insights. Experiments conducted on the SWaT dataset demonstrate that ShaTS
accurately identifies critical time instants, precisely pinpoints the sensors,
actuators, and processes affected by anomalies, and outperforms SHAP in terms
of both explainability and resource efficiency, fulfilling the real-time
requirements of industrial environments.

</details>


### [416] [Feature-aware Hypergraph Generation via Next-Scale Prediction](https://arxiv.org/abs/2506.01467)
*Dorian Gailhard,Enzo Tartaglione,Lirida Naviner,Jhony H. Giraldo*

Key words: 超图生成、特征建模、多尺度表示、FAHNES

TL;DR: FAHNES是一种层次化方法，联合生成超图拓扑和特征，通过节点粗化和局部扩展重构更精细层级，优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有超图生成方法仅关注拓扑结构，常忽略特征建模，而实际应用需要两者结合。

Method: 通过节点粗化构建多尺度表示，通过局部扩展和细化重构精细层级，采用节点预算机制控制分裂。

Result: 在合成超图、3D网格和分子数据集上，FAHNES在拓扑和特征重建方面表现优异。

Conclusion: FAHNES为特征超图生成建模奠定了基础。

Abstract: Hypergraphs generalize traditional graphs by allowing hyperedges to connect
multiple nodes, making them well-suited for modeling complex structures with
higher-order relationships, such as 3D meshes, molecular systems, and
electronic circuits. While topology is central to hypergraph structure, many
real-world applications also require node and hyperedge features. Existing
hypergraph generation methods focus solely on topology, often overlooking
feature modeling. In this work, we introduce FAHNES (feature-aware hypergraph
generation via next-scale prediction), a hierarchical approach that jointly
generates hypergraph topology and features. FAHNES builds a multi-scale
representation through node coarsening, then learns to reconstruct finer levels
via localized expansion and refinement, guided by a new node budget mechanism
that controls cluster splitting. We evaluate FAHNES on synthetic hypergraphs,
3D meshes, and molecular datasets. FAHNES achieves competitive results in
reconstructing topology and features, establishing a foundation for future
research in featured hypergraph generative modeling.

</details>


### [417] [MUDI: A Multimodal Biomedical Dataset for Understanding Pharmacodynamic Drug-Drug Interactions](https://arxiv.org/abs/2506.01478)
*Tung-Lam Ngo,Ba-Hoang Tran,Duy-Cat Can,Trung-Hieu Do,Oliver Y. Chén,Hoang-Quynh Le*

Key words: 药物-药物相互作用, 多模态数据集, 药效学, 机器学习, 泛化能力

TL;DR: 本文介绍了一个名为MUDI的多模态生物医学数据集，用于研究药效学药物-药物相互作用（DDI），并评估了学习方法的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的DDI数据集主要关注文本信息，忽略了反映复杂药物机制的多模态数据。

Method: MUDI数据集结合了药理学文本、化学公式、分子结构图和图像，并采用晚期融合投票和中期融合策略评估模型。

Result: MUDI包含310,532个标注药物对，测试集中包含未见药物对以评估模型的泛化能力。

Conclusion: MUDI为研究DDI提供了一个全面的多模态数据集，并促进了机器学习方法的发展。

Abstract: Understanding the interaction between different drugs (drug-drug interaction
or DDI) is critical for ensuring patient safety and optimizing therapeutic
outcomes. Existing DDI datasets primarily focus on textual information,
overlooking multimodal data that reflect complex drug mechanisms. In this
paper, we (1) introduce MUDI, a large-scale Multimodal biomedical dataset for
Understanding pharmacodynamic Drug-drug Interactions, and (2) benchmark
learning methods to study it. In brief, MUDI provides a comprehensive
multimodal representation of drugs by combining pharmacological text, chemical
formulas, molecular structure graphs, and images across 310,532 annotated drug
pairs labeled as Synergism, Antagonism, or New Effect. Crucially, to
effectively evaluate machine-learning based generalization, MUDI consists of
unseen drug pairs in the test set. We evaluate benchmark models using both late
fusion voting and intermediate fusion strategies. All data, annotations,
evaluation scripts, and baselines are released under an open research license.

</details>


### [418] [Automatic Stage Lighting Control: Is it a Rule-Driven Process or Generative Task?](https://arxiv.org/abs/2506.01482)
*Zijian Zhao,Dian Jin,Zijing Zhou,Xiaoyu Zhang*

Key words: 自动舞台灯光控制(ASLC), Skip-BART, 生成任务, 跳跃连接, 端到端学习

TL;DR: Skip-BART是一种用于自动舞台灯光控制的端到端解决方案，通过学习专业灯光师的经验，将ASLC任务视为生成任务而非分类问题，效果优于传统基于规则的方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于雇佣或培训专业灯光师成本高昂，自动舞台灯光控制(ASLC)受到关注，但现有方法过于公式化。

Method: 修改BART模型，以音频为输入，生成灯光色调和强度，并引入跳跃连接机制增强音乐与灯光的关联。

Result: 定量分析和人工评估显示，Skip-BART在各项指标上优于传统方法，与真实灯光师的表现接近(p值为0.72)。

Conclusion: Skip-BART将ASLC视为生成任务，取得了显著效果，为未来研究提供了数据集和代码支持。

Abstract: Stage lighting plays an essential role in live music performances,
influencing the engaging experience of both musicians and audiences. Given the
high costs associated with hiring or training professional lighting engineers,
Automatic Stage Lighting Control (ASLC) has gained increasing attention.
However, most existing approaches only classify music into limited categories
and map them to predefined light patterns, resulting in formulaic and
monotonous outcomes that lack rationality. To address this issue, this paper
presents an end-to-end solution that directly learns from experienced lighting
engineers -- Skip-BART. To the best of our knowledge, this is the first work to
conceptualize ASLC as a generative task rather than merely a classification
problem. Our method modifies the BART model to take audio music as input and
produce light hue and value (intensity) as output, incorporating a novel skip
connection mechanism to enhance the relationship between music and light within
the frame grid.We validate our method through both quantitative analysis and an
human evaluation, demonstrating that Skip-BART outperforms conventional
rule-based methods across all evaluation metrics and shows only a limited gap
compared to real lighting engineers.Specifically, our method yields a p-value
of 0.72 in a statistical comparison based on human evaluations with human
lighting engineers, suggesting that the proposed approach closely matches human
lighting engineering performance. To support further research, we have made our
self-collected dataset, code, and trained model parameters available at
https://github.com/RS2002/Skip-BART .

</details>


### [419] [Model-agnostic Mitigation Strategies of Data Imbalance for Regression](https://arxiv.org/abs/2506.01486)
*Jelke Wibbeke,Sebastian Rohjans,Andreas Rauh*

Key words: 数据不平衡, 回归任务, 罕见事件预测, 采样技术, 密度函数, 模型集成

TL;DR: 论文研究了回归任务中数据不平衡问题，提出了新的方法（如密度-距离和密度-比相关性函数）和改进的采样技术（cSMOGN和crbSMOGN），并通过实验验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 数据不平衡导致模型偏差和预测不可靠，特别是在预测罕见事件时，影响了实际应用的可靠性。

Method: 提出了密度-距离和密度-比相关性函数，以及改进的采样技术cSMOGN和crbSMOGN，并在合成和真实数据集上进行了测试。

Result: 实验表明，新方法在提升罕见样本性能的同时，可能对常见样本产生负面影响；但通过模型集成可显著减少这些负面影响。crbSMOGN技术结合密度-比相关性函数在神经网络中表现最优。

Conclusion: 研究展示了新方法在解决数据不平衡问题上的有效性，并强调了模型集成的重要性。

Abstract: Data imbalance persists as a pervasive challenge in regression tasks,
introducing bias in model performance and undermining predictive reliability.
This is particularly detrimental in applications aimed at predicting rare
events that fall outside the domain of the bulk of the training data. In this
study, we review the current state-of-the-art regarding sampling-based methods
and cost-sensitive learning. Additionally, we propose novel approaches to
mitigate model bias. To better asses the importance of data, we introduce the
density-distance and density-ratio relevance functions, which effectively
integrate empirical frequency of data with domain-specific preferences,
offering enhanced interpretability for end-users. Furthermore, we present
advanced mitigation techniques (cSMOGN and crbSMOGN), which build upon and
improve existing sampling methods. In a comprehensive quantitative evaluation,
we benchmark state-of-the-art methods on 10 synthetic and 42 real-world
datasets, using neural networks, XGBoosting trees and Random Forest models. Our
analysis reveals that while most strategies improve performance on rare
samples, they often degrade it on frequent ones. We demonstrate that
constructing an ensemble of models -- one trained with imbalance mitigation and
another without -- can significantly reduce these negative effects. The key
findings underscore the superior performance of our novel crbSMOGN sampling
technique with the density-ratio relevance function for neural networks,
outperforming state-of-the-art methods.

</details>


### [420] [Confidence-Aware Self-Distillation for Multimodal Sentiment Analysis with Incomplete Modalities](https://arxiv.org/abs/2506.01490)
*Yanxi Luo,Shijin Wang,Zhongxing Xu,Yulong Li,Feilong Tang,Jionglong Su*

Key words: multimodal sentiment analysis, modality missingness, confidence-aware, self-distillation, robustness

TL;DR: 提出了一种基于置信感知自蒸馏（CASD）的策略，通过结合多模态概率嵌入和不确定性评分，解决了模态缺失问题，并在三个基准数据集上实现了最优性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现实场景中模态缺失问题常见，现有方法忽略了多模态组合的置信度，导致性能不佳。为此，提出CASD策略来提升鲁棒性。

Method: CASD策略利用学生$t$-分布混合多模态概率嵌入，结合置信度和重尾特性，并通过一致性蒸馏减少不确定性。此外，引入重参数化表示模块，通过从联合分布采样嵌入计算任务损失。

Result: 在三个基准数据集上的实验表明，该方法达到了最先进的性能。

Conclusion: CASD策略通过结合置信度和不确定性评分，显著提升了多模态情感分析的性能，尤其是对模态缺失的鲁棒性。

Abstract: Multimodal sentiment analysis (MSA) aims to understand human sentiment
through multimodal data. In real-world scenarios, practical factors often lead
to uncertain modality missingness. Existing methods for handling modality
missingness are based on data reconstruction or common subspace projections.
However, these methods neglect the confidence in multimodal combinations and
impose constraints on intra-class representation, hindering the capture of
modality-specific information and resulting in suboptimal performance. To
address these challenges, we propose a Confidence-Aware Self-Distillation
(CASD) strategy that effectively incorporates multimodal probabilistic
embeddings via a mixture of Student's $t$-distributions, enhancing its
robustness by incorporating confidence and accommodating heavy-tailed
properties. This strategy estimates joint distributions with uncertainty scores
and reduces uncertainty in the student network by consistency distillation.
Furthermore, we introduce a reparameterization representation module that
facilitates CASD in robust multimodal learning by sampling embeddings from the
joint distribution for the prediction module to calculate the task loss. As a
result, the directional constraint from the loss minimization is alleviated by
the sampled representation. Experimental results on three benchmark datasets
demonstrate that our method achieves state-of-the-art performance.

</details>


### [421] [Learning of Population Dynamics: Inverse Optimization Meets JKO Scheme](https://arxiv.org/abs/2506.01502)
*Mikhail Persiianov,Jiawei Chen,Petr Mokrov,Alexander Tyurin,Evgeny Burnaev,Alexander Korotin*

Key words: JKO框架, 逆优化, 种群动态, 对抗训练

TL;DR: 论文提出了一种结合JKO框架和逆优化技术的方法$	exttt{iJKOnet}$，用于学习种群动态。该方法通过对抗训练实现，无需特定网络架构，优于现有JKO方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 学习种群动态需要从离散时间点的样本快照中恢复粒子演化的潜在过程。现有方法将其视为概率空间中的能量最小化问题，并利用JKO方案进行时间离散化。

Method: 提出$	exttt{iJKOnet}$方法，将JKO框架与逆优化技术结合，通过对抗训练实现，无需特定网络架构。

Result: 理论分析了方法的有效性，并在实验中优于现有JKO方法。

Conclusion: $	exttt{iJKOnet}$在学习和模拟种群动态方面表现出色，避免了传统方法的限制。

Abstract: Learning population dynamics involves recovering the underlying process that
governs particle evolution, given evolutionary snapshots of samples at discrete
time points. Recent methods frame this as an energy minimization problem in
probability space and leverage the celebrated JKO scheme for efficient time
discretization. In this work, we introduce $\texttt{iJKOnet}$, an approach that
combines the JKO framework with inverse optimization techniques to learn
population dynamics. Our method relies on a conventional $\textit{end-to-end}$
adversarial training procedure and does not require restrictive architectural
choices, e.g., input-convex neural networks. We establish theoretical
guarantees for our methodology and demonstrate improved performance over prior
JKO-based methods.

</details>


### [422] [Analyzing the Importance of Blank for CTC-Based Knowledge Distillation](https://arxiv.org/abs/2506.01503)
*Benedikt Hilmes,Nick Rossenbach,Ralf Schlüter*

Key words: 知识蒸馏, CTC, 空白标记, 语音识别, 预训练模型

TL;DR: 论文探讨了CTC蒸馏的变体，特别是空白标记处理，并提出了一种对称选择方法以减少CTC损失，同时保持性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大型预训练模型在自动语音识别中性能优秀但推理成本高，研究旨在通过知识蒸馏提高小型模型的效率。

Method: 探索了不同的CTC蒸馏变体和空白标记处理方式，提出了对称选择方法以减少对目标标签的依赖。

Result: 通过对称选择方法，可以在不降低性能的情况下移除CTC损失，支持无转录音频数据的蒸馏。

Conclusion: 对称选择方法为CTC蒸馏提供了新思路，可能支持无标签数据的应用。

Abstract: With the rise of large pre-trained foundation models for automatic speech
recognition new challenges appear. While the performance of these models is
good, runtime and cost of inference increases. One approach to make use of
their strength while retaining efficiency is to distill their knowledge to
smaller models during training. In this work, we explore different CTC-based
distillation variants, focusing on blank token handling. We show that common
approaches like blank elimination do not always work off the shelf. We explore
new blank selection patterns as a potential sweet spot between standard
knowledge distillation and blank elimination mechanisms. Through the
introduction of a symmetric selection method, we are able to remove the CTC
loss during knowledge distillation with minimal to no performance degradation.
With this, we make the training independent from target labels, potentially
allowing for distillation on untranscribed audio data.

</details>


### [423] [Beyond Diagonal Covariance: Flexible Posterior VAEs via Free-Form Injective Flows](https://arxiv.org/abs/2506.01522)
*Peter Sorrenson,Lukas Lührs,Hans Olischläger,Ullrich Köthe*

Key words: 变分自编码器, 自由形式注入流, 微分几何, 后验分布, 高斯协方差

TL;DR: 论文探讨了变分自编码器（VAEs）在表示能力上的局限性，并提出了通过正则化自由形式注入流（FIF）来实现更灵活的后验分布，实验证明其效果优于传统对角协方差VAEs。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的VAEs由于计算限制通常采用对角高斯后验，但其表示能力有限。本文旨在通过理论分析和实验验证，展示如何在保持计算效率的同时提升后验分布的灵活性。

Method: 作者基于微分几何理论分析了对角协方差VAEs的局限性，并提出了一种正则化的自由形式注入流（FIF）方法，该方法可以实现全高斯协方差后验分布。

Result: 在图像数据集上的实验表明，采用全协方差后验分布的模型在似然函数表现上显著优于传统对角协方差VAEs。

Conclusion: 通过正则化FIF方法，可以在不增加计算成本的情况下实现更灵活的后验分布，显著提升VAEs的生成能力。

Abstract: Variational Autoencoders (VAEs) are powerful generative models widely used
for learning interpretable latent spaces, quantifying uncertainty, and
compressing data for downstream generative tasks. VAEs typically rely on
diagonal Gaussian posteriors due to computational constraints. Using arguments
grounded in differential geometry, we demonstrate inherent limitations in the
representational capacity of diagonal covariance VAEs, as illustrated by
explicit low-dimensional examples. In response, we show that a regularized
variant of the recently introduced Free-form Injective Flow (FIF) can be
interpreted as a VAE featuring a highly flexible, implicitly defined posterior.
Crucially, this regularization yields a posterior equivalent to a full Gaussian
covariance distribution, yet maintains computational costs comparable to
standard diagonal covariance VAEs. Experiments on image datasets validate our
approach, demonstrating that incorporating full covariance substantially
improves model likelihood.

</details>


### [424] [Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model](https://arxiv.org/abs/2506.01523)
*Jihun Yun,Juno Kim,Jongho Park,Junhyuck Kim,Jongha Jon Ryu,Jaewoong Cho,Kwang-Sung Jun*

Key words: 对齐, 强化学习, 分布学习, 偏好反馈

TL;DR: 论文重新思考了大型语言模型的对齐问题，提出了一种基于分布学习的方法，并通过理论分析和实验验证其优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的RLHF方法缺乏理论依据且易导致退化性解，本文旨在提供一种更理论化的对齐框架。

Method: 提出三种基于分布学习的目标函数：偏好最大似然估计、偏好蒸馏和反向KL最小化。

Result: 理论证明三种方法具有强收敛性，实验显示其性能优于或匹配RLHF和DPO。

Conclusion: 分布学习框架为对齐问题提供了理论支撑，尤其是偏好蒸馏表现优异。

Abstract: Alignment via reinforcement learning from human feedback (RLHF) has become
the dominant paradigm for controlling the quality of outputs from large
language models (LLMs). However, when viewed as `loss + regularization,' the
standard RLHF objective lacks theoretical justification and incentivizes
degenerate, deterministic solutions, an issue that variants such as Direct
Policy Optimization (DPO) also inherit. In this paper, we rethink alignment by
framing it as \emph{distribution learning} from pairwise preference feedback by
explicitly modeling how information about the target language model bleeds
through the preference data. This explicit modeling leads us to propose three
principled learning objectives: preference maximum likelihood estimation,
preference distillation, and reverse KL minimization. We theoretically show
that all three approaches enjoy strong non-asymptotic $O(1/n)$ convergence to
the target language model, naturally avoiding degeneracy and reward
overfitting. Finally, we empirically demonstrate that our distribution learning
framework, especially preference distillation, consistently outperforms or
matches the performances of RLHF and DPO across various tasks and models.

</details>


### [425] [Learning Abstract World Models with a Group-Structured Latent Space](https://arxiv.org/abs/2506.01529)
*Thomas Delliaux,Nguyen-Khanh Vu,Vincent François-Lavet,Elise van der Pol,Emmanuel Rachelson*

Key words: MDPs, 几何先验, 对称结构, 表示学习

TL;DR: 论文提出了一种在MDPs低维表示流形上施加几何先验的方法，通过对称结构提升模型预测和下游任务表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 提高从有限数据中学习MDPs抽象模型的泛化能力。

Method: 在表示流形上施加几何先验，利用对称结构和潜在空间设计编码环境不变性。

Result: 在具有旋转和平移特征的环境中表现优于非结构化方法，且表示更简洁和可解耦。

Conclusion: 几何先验能有效提升模型预测和表示学习。

Abstract: Learning meaningful abstract models of Markov Decision Processes (MDPs) is
crucial for improving generalization from limited data. In this work, we show
how geometric priors can be imposed on the low-dimensional representation
manifold of a learned transition model. We incorporate known symmetric
structures via appropriate choices of the latent space and the associated group
actions, which encode prior knowledge about invariances in the environment. In
addition, our framework allows the embedding of additional unstructured
information alongside these symmetries. We show experimentally that this leads
to better predictions of the latent transition model than fully unstructured
approaches, as well as better learning on downstream RL tasks, in environments
with rotational and translational features, including in first-person views of
3D environments. Additionally, our experiments show that this leads to simpler
and more disentangled representations. The full code is available on GitHub to
ensure reproducibility.

</details>


### [426] [A Diffusion-Based Method for Learning the Multi-Outcome Distribution of Medical Treatments](https://arxiv.org/abs/2506.01533)
*Yuchen Ma,Jonas Schweisthal,Hengrui Zhang,Stefan Feuerriegel*

Key words: 医疗治疗，多结果预测，扩散模型，联合分布，因果推断

TL;DR: 该论文提出了一种基于扩散的新方法DIME，用于学习医疗治疗的多个结果的联合分布，解决了现有方法在多元结果预测中的局限性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 临床上需要了解多维治疗结果的联合分布以做出最优决策，但现有机器学习方法多聚焦于单一结果预测。

Method: 通过因果掩码和条件分解，DIME学习多元结果的联合干预分布，并处理不同类型的数据。

Result: 实验证明DIME能有效学习联合分布并捕捉结果间的依赖关系。

Conclusion: DIME是首个针对多元医疗结果联合分布的神经方法，可支持不确定性量化的决策。

Abstract: In medicine, treatments often influence multiple, interdependent outcomes,
such as primary endpoints, complications, adverse events, or other secondary
endpoints. Hence, to make optimal treatment decisions, clinicians are
interested in learning the distribution of multi-dimensional treatment
outcomes. However, the vast majority of machine learning methods for predicting
treatment effects focus on single-outcome settings, despite the fact that
medical data often include multiple, interdependent outcomes. To address this
limitation, we propose a novel diffusion-based method called DIME to learn the
joint distribution of multiple outcomes of medical treatments. We addresses
three challenges relevant in medical practice: (i)it is tailored to learn the
joint interventional distribution of multiple medical outcomes, which enables
reliable decision-making with uncertainty quantification rather than relying
solely on point estimates; (ii)it explicitly captures the dependence structure
between outcomes; (iii)it can handle outcomes of mixed type, including binary,
categorical, and continuous variables. In DIME, we take into account the
fundamental problem of causal inference through causal masking. For training,
our method decomposes the joint distribution into a series of conditional
distributions with a customized conditional masking to account for the
dependence structure across outcomes. For inference, our method
auto-regressively generates predictions. This allows our method to move beyond
point estimates of causal quantities and thus learn the joint interventional
distribution. To the best of our knowledge, DIME is the first neural method
tailored to learn the joint, multi-outcome distribution of medical treatments.
Across various experiments, we demonstrate that our method effectively learns
the joint distribution and captures shared information among multiple outcomes.

</details>


### [427] [Adaptive Destruction Processes for Diffusion Samplers](https://arxiv.org/abs/2506.01541)
*Timofei Gritsaev,Nikita Morozov,Kirill Tamogashev,Daniil Tiapkin,Sergey Samsonov,Alexey Naumov,Dmitry Vetrov,Nikolay Malkin*

Key words: 扩散模型、生成策略、高斯密度、条件图像生成

TL;DR: 本文研究了扩散采样器中可训练的破坏过程的挑战与优势，提出一种离散时间策略，提升生成效率和质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究扩散采样器在有限步骤下的高效生成能力，突破传统连续时间模型的限制。

Method: 通过解耦生成和破坏过程的方差，训练未约束的高斯密度核，实现更灵活的生成策略。

Result: 实验表明，该方法在有限步骤下收敛更快，采样质量更高，且适用于条件图像生成任务。

Conclusion: 离散时间策略和灵活的核设计能显著提升扩散采样器的性能。

Abstract: This paper explores the challenges and benefits of a trainable destruction
process in diffusion samplers -- diffusion-based generative models trained to
sample an unnormalised density without access to data samples. Contrary to the
majority of work that views diffusion samplers as approximations to an
underlying continuous-time model, we view diffusion models as discrete-time
policies trained to produce samples in very few generation steps. We propose to
trade some of the elegance of the underlying theory for flexibility in the
definition of the generative and destruction policies. In particular, we
decouple the generation and destruction variances, enabling both transition
kernels to be learned as unconstrained Gaussian densities. We show that, when
the number of steps is limited, training both generation and destruction
processes results in faster convergence and improved sampling quality on
various benchmarks. Through a robust ablation study, we investigate the design
choices necessary to facilitate stable training. Finally, we show the
scalability of our approach through experiments on GAN latent space sampling
for conditional image generation.

</details>


### [428] [Temporal Variational Implicit Neural Representations](https://arxiv.org/abs/2506.01544)
*Batuhan Koyuncu,Rachael DeVries,Ole Winther,Isabel Valera*

Key words: 时间序列, 隐式神经表示, 潜在变量模型, 插补, 预测

TL;DR: TV-INRs是一种结合隐式神经表示和潜在变量模型的概率框架，用于高效建模不规则多元时间序列，支持个性化插补和预测。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现有方法在建模时间序列时需大量训练或微调的问题，提出一种高效、可扩展的单次前向预测方案。

Method: 通过隐式神经表示与潜在变量模型的结合，学习基于信号特定协变量的时间连续生成器函数分布。

Result: 实验显示TV-INRs在插补和预测任务中表现优异，尤其适用于低数据量场景。

Conclusion: TV-INRs为真实世界的多元时间序列任务提供了计算高效且可扩展的解决方案。

Abstract: We introduce Temporal Variational Implicit Neural Representations (TV-INRs),
a probabilistic framework for modeling irregular multivariate time series that
enables efficient individualized imputation and forecasting. By integrating
implicit neural representations with latent variable models, TV-INRs learn
distributions over time-continuous generator functions conditioned on
signal-specific covariates. Unlike existing approaches that require extensive
training, fine-tuning or meta-learning, our method achieves accurate
individualized predictions through a single forward pass. Our experiments
demonstrate that with a single TV-INRs instance, we can accurately solve
diverse imputation and forecasting tasks, offering a computationally efficient
and scalable solution for real-world applications. TV-INRs excel especially in
low-data regimes, where it outperforms existing methods by an order of
magnitude in mean squared error for imputation task.

</details>


### [429] [Class Incremental Learning for Algorithm Selection](https://arxiv.org/abs/2506.01545)
*Mate Botond Nemeth,Emma Hart,Kevin Sim,Quentin Renau*

Key words: 算法选择,增量学习,灾难性遗忘,持续学习,优化

TL;DR: 论文研究了算法选择中的增量学习问题，评估了8种持续学习方法，发现基于记忆的方法表现最佳。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在优化领域中，实例和求解器随时间增加时，如何避免分类模型遗忘历史数据是一个未被充分研究的问题。

Method: 使用装箱数据集，比较了8种持续学习方法对灾难性遗忘的抵抗能力。

Result: 基于记忆的方法显著优于其他方法，遗忘损失约为7%。

Conclusion: 基于记忆的持续学习方法适用于流式优化场景。

Abstract: Algorithm selection is commonly used to predict the best solver from a
portfolio per per-instance. In many real scenarios, instances arrive in a
stream: new instances become available over time, while the number of class
labels can also grow as new data distributions arrive downstream. As a result,
the classification model needs to be periodically updated to reflect additional
solvers without catastrophic forgetting of past data. In machine-learning (ML),
this is referred to as Class Incremental Learning (CIL). While commonly
addressed in ML settings, its relevance to algorithm-selection in optimisation
has not been previously studied. Using a bin-packing dataset, we benchmark 8
continual learning methods with respect to their ability to withstand
catastrophic forgetting. We find that rehearsal-based methods significantly
outperform other CIL methods. While there is evidence of forgetting, the loss
is small at around 7%. Hence, these methods appear to be a viable approach to
continual learning in streaming optimisation scenarios.

</details>


### [430] [To Each Metric Its Decoding: Post-Hoc Optimal Decision Rules of Probabilistic Hierarchical Classifiers](https://arxiv.org/abs/2506.01552)
*Roman Plaud,Alexandre Perez-Lebel,Matthieu Labeau,Antoine Saillenfest,Thomas Bonald*

Key words: 层次分类, 最优解码, 决策规则, hFβ分数

TL;DR: 本文提出了一种优化解码框架，针对层次分类中的目标指标，推导了不同复杂度预测场景的最优决策规则，并通过实验验证其在实际应用中的优势。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 层次分类中的传统启发式决策规则可能无法与任务特定评估指标对齐，因此需要一种优化解码方法。

Method: 提出了一个框架，用于基于目标指标优化输出概率分布的解码，并针对不同预测场景推导了最优决策规则。

Result: 实验表明，所提出的最优策略在不确定场景中表现优越，提升了层次分类器的性能和可靠性。

Conclusion: 该方法为层次分类器的实际应用提供了性能增强和可靠性改进的潜力。

Abstract: Hierarchical classification offers an approach to incorporate the concept of
mistake severity by leveraging a structured, labeled hierarchy. However,
decoding in such settings frequently relies on heuristic decision rules, which
may not align with task-specific evaluation metrics. In this work, we propose a
framework for the optimal decoding of an output probability distribution with
respect to a target metric. We derive optimal decision rules for increasingly
complex prediction settings, providing universal algorithms when candidates are
limited to the set of nodes. In the most general case of predicting a subset of
nodes, we focus on rules dedicated to the hierarchical $hF_{\beta}$ scores,
tailored to hierarchical settings. To demonstrate the practical utility of our
approach, we conduct extensive empirical evaluations, showcasing the
superiority of our proposed optimal strategies, particularly in underdetermined
scenarios. These results highlight the potential of our methods to enhance the
performance and reliability of hierarchical classifiers in real-world
applications. The code is available at
https://github.com/RomanPlaud/hierarchical_decision_rules

</details>


### [431] [Unpacking Softmax: How Temperature Drives Representation Collapse, Compression, and Generalization](https://arxiv.org/abs/2506.01562)
*Wojciech Masarczyk,Mateusz Ostaszewski,Tin Sum Cheng,Tomasz Trzciński,Aurelien Lucchi,Razvan Pascanu*

Key words: softmax, rank deficit bias, representation learning, temperature tuning

TL;DR: 本文研究了softmax函数在深度神经网络中的作用，发现其会导致表示学习的rank deficit bias，并提出通过调整温度参数优化模型性能的方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: softmax函数虽然广泛用于分类和注意力机制，但其对学习动态和表示的影响尚不明确，限制了模型优化的能力。

Method: 研究softmax对表示的影响，提出rank deficit bias的概念，并通过调整温度参数改进表示学习。

Result: 发现softmax会引起低秩解，验证了温度调优对模型性能的提升作用。

Conclusion: 揭示了softmax的机制，为深度网络的表示学习提供了新的优化方向。

Abstract: The softmax function is a fundamental building block of deep neural networks,
commonly used to define output distributions in classification tasks or
attention weights in transformer architectures. Despite its widespread use and
proven effectiveness, its influence on learning dynamics and learned
representations remains poorly understood, limiting our ability to optimize
model behavior. In this paper, we study the pivotal role of the softmax
function in shaping the model's representation. We introduce the concept of
rank deficit bias - a phenomenon in which softmax-based deep networks find
solutions of rank much lower than the number of classes. This bias depends on
the softmax function's logits norm, which is implicitly influenced by
hyperparameters or directly modified by softmax temperature. Furthermore, we
demonstrate how to exploit the softmax dynamics to learn compressed
representations or to enhance their performance on out-of-distribution data. We
validate our findings across diverse architectures and real-world datasets,
highlighting the broad applicability of temperature tuning in improving model
performance. Our work provides new insights into the mechanisms of softmax,
enabling better control over representation learning in deep neural networks.

</details>


### [432] [Trajectory First: A Curriculum for Discovering Diverse Policies](https://arxiv.org/abs/2506.01568)
*Cornelius V. Braun,Sayantan Auddy,Marc Toussaint*

Key words: 强化学习、多样性优化、课程学习、机器人操作

TL;DR: 论文探讨了通过多样化解来增强强化学习（RL）的鲁棒性，并提出了一种先探索轨迹再学习步进策略的课程方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决在复杂任务中现有约束多样性RL方法探索不足导致策略多样性缺乏的问题。

Method: 提出了一种先探索轨迹再学习步进策略的课程方法。

Result: 实验表明该方法提升了学习技能的多样性。

Conclusion: 通过轨迹级探索的课程方法能有效提升RL中的多样性优化。

Abstract: Being able to solve a task in diverse ways makes agents more robust to task
variations and less prone to local optima. In this context, constrained
diversity optimization has emerged as a powerful reinforcement learning (RL)
framework to train a diverse set of agents in parallel. However, existing
constrained-diversity RL methods often under-explore in complex tasks such as
robotic manipulation, leading to a lack in policy diversity. To improve
diversity optimization in RL, we therefore propose a curriculum that first
explores at the trajectory level before learning step-based policies. In our
empirical evaluation, we provide novel insights into the shortcoming of
skill-based diversity optimization, and demonstrate empirically that our
curriculum improves the diversity of the learned skills.

</details>


### [433] [Latent Space Topology Evolution in Multilayer Perceptrons](https://arxiv.org/abs/2506.01569)
*Eduardo Paluzo-Hidalgo*

Key words: 拓扑分析, 多层感知机, 单纯复形, 持久性, 可解释性

TL;DR: 论文提出了一种拓扑框架，用于解释多层感知机（MLP）的内部表征，并通过构造单纯复形序列分析数据拓扑在神经网络中的演化。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在通过拓扑学方法揭示MLP中数据表征的演变过程，增强对网络行为的理解与解释性。

Method: 构建单纯复形塔，提出双持久性分析方法（层持久性和MLP持久性），开发组合算法计算MLP持久性，并设计基于轨迹的可视化工具。

Result: 证明了拓扑描述符的稳定性定理，发现线性可分性与神经复杂度的连通性相关，实验显示方法能识别冗余层和关键拓扑跃迁。

Conclusion: 该框架为MLP提供了可解释的拓扑分析工具，揭示了数据在分类任务中的逐步组织过程。

Abstract: This paper introduces a topological framework for interpreting the internal
representations of Multilayer Perceptrons (MLPs). We construct a simplicial
tower, a sequence of simplicial complexes connected by simplicial maps, that
captures how data topology evolves across network layers. Our approach enables
bi-persistence analysis: layer persistence tracks topological features within
each layer across scales, while MLP persistence reveals how these features
transform through the network. We prove stability theorems for our topological
descriptors and establish that linear separability in latent spaces is related
to disconnected components in the nerve complexes. To make our framework
practical, we develop a combinatorial algorithm for computing MLP persistence
and introduce trajectory-based visualisations that track data flow through the
network. Experiments on synthetic and real-world medical data demonstrate our
method's ability to identify redundant layers, reveal critical topological
transitions, and provide interpretable insights into how MLPs progressively
organise data for classification.

</details>


### [434] [Bayes optimal learning of attention-indexed models](https://arxiv.org/abs/2506.01582)
*Fabrizio Boncoraglio,Emanuele Troiani,Vittorio Erba,Lenka Zdeborová*

Key words: 注意力索引模型, 统计力学, 随机矩阵理论, 深度学习, Transformer

TL;DR: 论文提出了注意力索引模型（AIM），用于分析深度注意力层中的学习机制，通过统计力学和随机矩阵理论工具预测优化性能，并与实践中的Transformer更接近。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究现代注意力架构中的学习机制，提出一个更贴近实际Transformer的理论框架。

Method: 采用统计力学和随机矩阵理论工具，分析多索引模型的注意力层，并提出近似消息传递算法。

Result: 得出了Bayes最优泛化误差的封闭解，并发现样本复杂度、模型宽度和序列长度的尖锐相变。

Conclusion: AIM为理解现代注意力架构中的学习提供了一个可解的模型，并展示了梯度下降可以达到最优性能。

Abstract: We introduce the attention-indexed model (AIM), a theoretical framework for
analyzing learning in deep attention layers. Inspired by multi-index models,
AIM captures how token-level outputs emerge from layered bilinear interactions
over high-dimensional embeddings. Unlike prior tractable attention models, AIM
allows full-width key and query matrices, aligning more closely with practical
transformers. Using tools from statistical mechanics and random matrix theory,
we derive closed-form predictions for Bayes-optimal generalization error and
identify sharp phase transitions as a function of sample complexity, model
width, and sequence length. We propose a matching approximate message passing
algorithm and show that gradient descent can reach optimal performance. AIM
offers a solvable playground for understanding learning in modern attention
architectures.

</details>


### [435] [VirnyFlow: A Design Space for Responsible Model Development](https://arxiv.org/abs/2506.01584)
*Denys Herasymuk,Nazar Protsiv,Julia Stoyanovich*

Key words: 机器学习,多目标优化,自动化机器学习,负责任AI

TL;DR: VirnyFlow是一种创新的多目标优化框架，旨在帮助数据科学家构建适合特定问题的负责任机器学习模型，显著优于传统AutoML系统。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现实问题的机器学习模型通常是多目标的，需要结合具体上下文进行定制化开发，而传统AutoML框架缺乏灵活性和透明性。

Method: VirnyFlow整合了评估协议定义、多目标贝叶斯优化、成本感知多臂老虎机、查询优化和分布式并行技术。

Result: 在五个真实世界基准测试中，VirnyFlow在优化质量和扩展性上显著优于现有AutoML系统。

Conclusion: VirnyFlow为机器学习开发提供了一个灵活、高效且负责任的自动化替代方案。

Abstract: Developing machine learning (ML) models requires a deep understanding of
real-world problems, which are inherently multi-objective. In this paper, we
present VirnyFlow, the first design space for responsible model development,
designed to assist data scientists in building ML pipelines that are tailored
to the specific context of their problem. Unlike conventional AutoML
frameworks, VirnyFlow enables users to define customized optimization criteria,
perform comprehensive experimentation across pipeline stages, and iteratively
refine models in alignment with real-world constraints. Our system integrates
evaluation protocol definition, multi-objective Bayesian optimization,
cost-aware multi-armed bandits, query optimization, and distributed parallelism
into a unified architecture. We show that VirnyFlow significantly outperforms
state-of-the-art AutoML systems in both optimization quality and scalability
across five real-world benchmarks, offering a flexible, efficient, and
responsible alternative to black-box automation in ML development.

</details>


### [436] [Selecting for Less Discriminatory Algorithms: A Relational Search Framework for Navigating Fairness-Accuracy Trade-offs in Practice](https://arxiv.org/abs/2506.01594)
*Hana Samad,Michael Akinwumi,Jameel Khan,Christoph Mügge-Durum,Emmanuel O. Ogundimu*

Key words: 公平性,机器学习,模型选择,LDA,资源限制

TL;DR: 论文探讨了在机器学习模型选择中，如何通过横向搜索和关系公平框架优化公平性和准确性，适用于资源受限的实际场景。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在机器学习模型嵌入社会决策时，如何选择公平且高效的算法成为关键挑战，尤其是面对模型多样性和资源限制。

Method: 提出横向LDA搜索方法，结合关系公平框架，利用HMDA数据评估模型的公平性和准确性。

Result: 横向LDA搜索为资源受限的实际场景提供了一种轻量化的公平性优化方案，补充了传统参数优化方法。

Conclusion: 通过关系公平框架和横向LDA搜索，组织可以更系统地选择和评估公平算法，适用于特定行业场景。

Abstract: As machine learning models are increasingly embedded into society through
high-stakes decision-making, selecting the right algorithm for a given task,
audience, and sector presents a critical challenge, particularly in the context
of fairness. Traditional assessments of model fairness have often framed
fairness as an objective mathematical property, treating model selection as an
optimization problem under idealized informational conditions. This overlooks
model multiplicity as a consideration--that multiple models can deliver similar
performance while exhibiting different fairness characteristics. Legal scholars
have engaged this challenge through the concept of Less Discriminatory
Algorithms (LDAs), which frames model selection as a civil rights obligation.
In real-world deployment, this normative challenge is bounded by constraints on
fairness experimentation, e.g., regulatory standards, institutional priorities,
and resource capacity.
  Against these considerations, the paper revisits Lee and Floridi (2021)'s
relational fairness approach using updated 2021 Home Mortgage Disclosure Act
(HMDA) data, and proposes an expansion of the scope of the LDA search process.
We argue that extending the LDA search horizontally, considering fairness
across model families themselves, provides a lightweight complement, or
alternative, to within-model hyperparameter optimization, when operationalizing
fairness in non-experimental, resource constrained settings. Fairness metrics
alone offer useful, but insufficient signals to accurately evaluate candidate
LDAs. Rather, by using a horizontal LDA search approach with the relational
trade-off framework, we demonstrate a responsible minimum viable LDA search on
real-world lending outcomes. Organizations can modify this approach to
systematically compare, evaluate, and select LDAs that optimize fairness and
accuracy in a sector-based contextualized manner.

</details>


### [437] [Understanding and Improving Laplacian Positional Encodings For Temporal GNNs](https://arxiv.org/abs/2506.01596)
*Yaniv Galron,Fabrizio Frasca,Haggai Maron,Eran Treister,Moshe Eliasof*

Key words: 时序图学习, 位置编码, 超拉普拉斯, 计算效率, 实验研究

TL;DR: 该论文针对时序图中位置编码的不足，提出了一种理论框架以减少计算成本并提升性能，同时通过实验验证了其效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 时序图学习在多个领域有广泛应用，但现有方法在位置编码方面存在计算成本高、理论理解不足等问题。

Method: 提出理论框架连接超拉普拉斯编码与时间片编码，引入新方法降低计算成本，并进行广泛的实验研究。

Result: 新方法实现了56倍的加速，且在某些场景下显著提升了模型性能，但效果因模型而异。

Conclusion: 时序图中的位置编码在特定场景下有效，但其适用性需根据模型和任务选择。

Abstract: Temporal graph learning has applications in recommendation systems, traffic
forecasting, and social network analysis. Although multiple architectures have
been introduced, progress in positional encoding for temporal graphs remains
limited. Extending static Laplacian eigenvector approaches to temporal graphs
through the supra-Laplacian has shown promise, but also poses key challenges:
high eigendecomposition costs, limited theoretical understanding, and ambiguity
about when and how to apply these encodings. In this paper, we address these
issues by (1) offering a theoretical framework that connects supra-Laplacian
encodings to per-time-slice encodings, highlighting the benefits of leveraging
additional temporal connectivity, (2) introducing novel methods to reduce the
computational overhead, achieving up to 56x faster runtimes while scaling to
graphs with 50,000 active nodes, and (3) conducting an extensive experimental
study to identify which models, tasks, and datasets benefit most from these
encodings. Our findings reveal that while positional encodings can
significantly boost performance in certain scenarios, their effectiveness
varies across different models.

</details>


### [438] [Policy Newton Algorithm in Reproducing Kernel Hilbert Space](https://arxiv.org/abs/2506.01597)
*Yixian Zhang,Huaze Tang,Chao Wang,Wenbo Ding*

Key words: 强化学习, RKHS, 二阶优化, Policy Newton

TL;DR: 论文提出了第一个用于RKHS中RL策略的二阶优化框架，解决了无限维Hessian算子难以计算的问题，并通过理论证明和实验验证了其优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前RKHS中的RL策略优化仅限于一阶方法，因为无限维Hessian算子难以显式计算和求逆。

Method: 提出Policy Newton in RKHS，通过优化三次正则化的辅助目标函数，利用表示定理将其转化为有限维问题。

Result: 理论证明局部最优解的收敛性，实验表明其收敛速度和奖励优于一阶RKHS方法和参数化二阶方法。

Conclusion: 该研究填补了非参数策略表示与二阶优化方法之间的关键空白。

Abstract: Reinforcement learning (RL) policies represented in Reproducing Kernel
Hilbert Spaces (RKHS) offer powerful representational capabilities. While
second-order optimization methods like Newton's method demonstrate faster
convergence than first-order approaches, current RKHS-based policy optimization
remains constrained to first-order techniques. This limitation stems primarily
from the intractability of explicitly computing and inverting the
infinite-dimensional Hessian operator in RKHS. We introduce Policy Newton in
RKHS, the first second-order optimization framework specifically designed for
RL policies represented in RKHS. Our approach circumvents direct computation of
the inverse Hessian operator by optimizing a cubic regularized auxiliary
objective function. Crucially, we leverage the Representer Theorem to transform
this infinite-dimensional optimization into an equivalent, computationally
tractable finite-dimensional problem whose dimensionality scales with the
trajectory data volume. We establish theoretical guarantees proving convergence
to a local optimum with a local quadratic convergence rate. Empirical
evaluations on a toy financial asset allocation problem validate these
theoretical properties, while experiments on standard RL benchmarks demonstrate
that Policy Newton in RKHS achieves superior convergence speed and higher
episodic rewards compared to established first-order RKHS approaches and
parametric second-order methods. Our work bridges a critical gap between
non-parametric policy representations and second-order optimization methods in
reinforcement learning.

</details>


### [439] [PMNO: A novel physics guided multi-step neural operator predictor for partial differential equations](https://arxiv.org/abs/2506.01598)
*Jin Song,Kenji Kawaguchi,Zhenya Yan*

Key words: 神经算子、物理引导、多步预测、BDF、外推性能

TL;DR: 提出了一种物理引导的多步神经算子架构（PMNO），通过多步历史数据和隐式时间步进方案，提高长期预测的准确性和训练效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有神经算子在模拟和预测物理系统时，因网络架构限制和数据依赖性强，导致训练效果差和预测性能不足。

Method: 采用多步历史数据输入和基于BDF的隐式时间步进方案，结合因果训练策略，实现高效稳定的端到端优化。

Result: PMNO在多种物理系统中表现出优异的预测性能，支持任意空间分辨率的外推。

Conclusion: PMNO框架显著提高了神经算子对复杂系统的长期预测能力，且兼容多种现有算子架构。

Abstract: Neural operators, which aim to approximate mappings between
infinite-dimensional function spaces, have been widely applied in the
simulation and prediction of physical systems. However, the limited
representational capacity of network architectures, combined with their heavy
reliance on large-scale data, often hinder effective training and result in
poor extrapolation performance. In this paper, inspired by traditional
numerical methods, we propose a novel physics guided multi-step neural operator
(PMNO) architecture to address these challenges in long-horizon prediction of
complex physical systems. Distinct from general operator learning methods, the
PMNO framework replaces the single-step input with multi-step historical data
in the forward pass and introduces an implicit time-stepping scheme based on
the Backward Differentiation Formula (BDF) during backpropagation. This design
not only strengthens the model's extrapolation capacity but also facilitates
more efficient and stable training with fewer data samples, especially for
long-term predictions. Meanwhile, a causal training strategy is employed to
circumvent the need for multi-stage training and to ensure efficient end-to-end
optimization. The neural operator architecture possesses resolution-invariant
properties, enabling the trained model to perform fast extrapolation on
arbitrary spatial resolutions. We demonstrate the superior predictive
performance of PMNO predictor across a diverse range of physical systems,
including 2D linear system, modeling over irregular domain, complex-valued wave
dynamics, and reaction-diffusion processes. Depending on the specific problem
setting, various neural operator architectures, including FNO, DeepONet, and
their variants, can be seamlessly integrated into the PMNO framework.

</details>


### [440] [Connecting Neural Models Latent Geometries with Relative Geodesic Representations](https://arxiv.org/abs/2506.01599)
*Hanlin Yu,Berfin Inal,Georgios Arvanitidis,Soren Hauberg,Francesco Locatello,Marco Fumero*

Key words: 神经网络、潜在空间、表示学习、微分几何、拉回度量

TL;DR: 该论文展示了如何利用神经模型的潜在空间的微分几何结构，捕捉相似数据分布上训练的不同表示空间之间的变换关系。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究动机在于探索神经模型学习数据表示时的潜在结构，尤其是在不同训练条件下产生的表示空间的几何关系。

Method: 方法基于假设不同模型参数化相同的潜在流形，并引入基于拉回度量的表示方法，以高效捕捉潜在空间的内在结构。

Result: 实验验证了该方法在模型缝合和检索任务中的有效性，涵盖自编码器和视觉基础判别模型。

Conclusion: 结论表明，利用几何结构可以精确描述不同表示空间之间的变换关系。

Abstract: Neural models learn representations of high-dimensional data on
low-dimensional manifolds. Multiple factors, including stochasticities in the
training process, model architectures, and additional inductive biases, may
induce different representations, even when learning the same task on the same
data. However, it has recently been shown that when a latent structure is
shared between distinct latent spaces, relative distances between
representations can be preserved, up to distortions. Building on this idea, we
demonstrate that exploiting the differential-geometric structure of latent
spaces of neural models, it is possible to capture precisely the
transformations between representational spaces trained on similar data
distributions. Specifically, we assume that distinct neural models parametrize
approximately the same underlying manifold, and introduce a representation
based on the pullback metric that captures the intrinsic structure of the
latent space, while scaling efficiently to large models. We validate
experimentally our method on model stitching and retrieval tasks, covering
autoencoders and vision foundation discriminative models, across diverse
architectures, datasets, and pretraining schemes.

</details>


### [441] [Contrastive Learning for Efficient Transaction Validation in UTXO-based Blockchains](https://arxiv.org/abs/2506.01614)
*Hamid Attar,Luigi Lunardon,Alessio Pagani*

Key words: 机器学习, UTXO, 区块链扩展, 分片, 交易路由

TL;DR: 提出了一种基于机器学习的UTXO区块链扩展方法，通过优化分片和交易路由，减少跨分片通信开销。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有UTXO分片方法因交易依赖关系导致通信开销大，影响交易处理速度。

Method: 结合对比学习和无监督学习，构建交易输出的嵌入空间，利用历史数据训练模型，减少实时查询需求。

Result: 显著降低了跨分片通信开销，提高了交易吞吐量和可扩展性。

Conclusion: 机器学习可有效优化UTXO区块链的分片和交易路由。

Abstract: This paper introduces a Machine Learning (ML) approach for scalability of
UTXO-based blockchains, such as Bitcoin. Prior approaches to UTXO set sharding
struggle with distributing UTXOs effectively across validators, creating
substantial communication overhead due to child-parent transaction
dependencies. This overhead, which arises from the need to locate parent UTXOs,
significantly hampers transaction processing speeds. Our solution uses ML to
optimize not only UTXO set sharding but also the routing of incoming
transactions, ensuring that transactions are directed to shards containing
their parent UTXOs. At the heart of our approach is a framework that combines
contrastive and unsupervised learning to create an embedding space for
transaction outputs. This embedding allows the model to group transaction
outputs based on spending relationships, making it possible to route
transactions efficiently to the correct validation microservices. Trained on
historical transaction data with triplet loss and online semi-hard negative
mining, the model embeds parent-child spending patterns directly into its
parameters, thus eliminating the need for costly, real-time parent transaction
lookups. This significantly reduces cross-shard communication overhead,
boosting throughput and scalability.

</details>


### [442] [Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks](https://arxiv.org/abs/2506.01625)
*Artun Saday,Yaşar Cahit Yıldırım,Cem Tekin*

Key words: 高斯过程优化, 对抗扰动, 鲁棒满足性, 遗憾界限

TL;DR: 该论文提出了两种基于鲁棒满足性目标的新算法，用于处理高斯过程优化中的未知对抗扰动问题，优于传统鲁棒优化方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决高斯过程优化中未知对抗扰动的问题，目标是设计一种能够在对抗条件下持续满足预设性能阈值的方法。

Method: 提出两种基于鲁棒满足性目标的新算法，并证明它们是通用鲁棒满足性框架的实例。分别适用于不同对抗条件下的优化。

Result: 推导出两种遗憾界限：一种在特定条件下具有次线性时间性能，另一种则与扰动幅度相关但无需对抗性假设。实验表明，该方法在满足性目标上优于传统鲁棒优化。

Conclusion: 论文表明，所提出的鲁棒满足性方法在对抗环境下具有优势，尤其是当传统鲁棒优化的模糊集不准确时效果更显著。

Abstract: We address the problem of Gaussian Process (GP) optimization in the presence
of unknown and potentially varying adversarial perturbations. Unlike
traditional robust optimization approaches that focus on maximizing performance
under worst-case scenarios, we consider a robust satisficing objective, where
the goal is to consistently achieve a predefined performance threshold $\tau$,
even under adversarial conditions. We propose two novel algorithms based on
distinct formulations of robust satisficing, and show that they are instances
of a general robust satisficing framework. Further, each algorithm offers
different guarantees depending on the nature of the adversary. Specifically, we
derive two regret bounds: one that is sublinear over time, assuming certain
conditions on the adversary and the satisficing threshold $\tau$, and another
that scales with the perturbation magnitude but requires no assumptions on the
adversary. Through extensive experiments, we demonstrate that our approach
outperforms the established robust optimization methods in achieving the
satisficing objective, particularly when the ambiguity set of the robust
optimization framework is inaccurately specified.

</details>


### [443] [Gradient-Based Model Fingerprinting for LLM Similarity Detection and Family Classification](https://arxiv.org/abs/2506.01631)
*Zehao Wu,Yanjie Zhao,Haoyu Wang*

Key words: Large Language Models, provenance tracking, gradient-based fingerprinting, family classification, safetensors

TL;DR: 本文介绍了TensorGuard，一种基于梯度的指纹框架，用于检测LLM的相似性和家族分类，解决了LLM模型衍生和版权合规的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着LLM成为现代应用的关键组件，未经授权的模型衍生（如微调和重新分发）成为重要挑战。开源模型（如LLaMA）要求衍生模型遵守命名规范，但缺乏技术手段验证合规性。

Method: 提出TensorGuard框架，通过分析梯度响应提取模型固有行为特征，独立于训练数据、水印或特定模型格式，支持safetensors格式，并利用K-Means聚类进行家族分类。

Result: 在58个模型（包括8个基础模型和50个衍生模型）上的实验显示，分类准确率达94%。

Conclusion: TensorGuard为LLM的溯源和版权合规提供了有效技术手段。

Abstract: As Large Language Models (LLMs) become integral software components in modern
applications, unauthorized model derivations through fine-tuning, merging, and
redistribution have emerged as critical software engineering challenges. Unlike
traditional software where clone detection and license compliance are
well-established, the LLM ecosystem lacks effective mechanisms to detect model
lineage and enforce licensing agreements. This gap is particularly problematic
when open-source model creators, such as Meta's LLaMA, require derivative works
to maintain naming conventions for attribution, yet no technical means exist to
verify compliance.
  To fill this gap, treating LLMs as software artifacts requiring provenance
tracking, we present TensorGuard, a gradient-based fingerprinting framework for
LLM similarity detection and family classification. Our approach extracts
model-intrinsic behavioral signatures by analyzing gradient responses to random
input perturbations across tensor layers, operating independently of training
data, watermarks, or specific model formats. TensorGuard supports the
widely-adopted safetensors format and constructs high-dimensional fingerprints
through statistical analysis of gradient features. These fingerprints enable
two complementary capabilities: direct pairwise similarity assessment between
arbitrary models through distance computation, and systematic family
classification of unknown models via the K-Means clustering algorithm with
domain-informed centroid initialization using known base models. Experimental
evaluation on 58 models comprising 8 base models and 50 derivatives across five
model families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94%
classification accuracy under our centroid-initialized K-Means clustering.

</details>


### [444] [Bidirectional Soft Actor-Critic: Leveraging Forward and Reverse KL Divergence for Efficient Reinforcement Learning](https://arxiv.org/abs/2506.01639)
*Yixian Zhang,Huaze Tang,Changxu Wei,Wenbo Ding*

Key words: SAC, KL divergence, reinforcement learning, continuous control

TL;DR: 本文提出了一种基于双向KL散度的SAC算法（Bidirectional SAC），通过结合正向和反向KL散度的优势，显著提升了性能和样本效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统SAC算法使用反向KL散度进行策略更新，导致不稳定性和样本效率低。本文研究正向KL散度的替代方案。

Method: 提出Bidirectional SAC算法，先用正向KL散度初始化策略，再用反向KL散度优化。

Result: 实验显示，Bidirectional SAC在连续控制任务中表现优于标准SAC和其他基线，奖励提升达30%。

Conclusion: 结合双向KL散度的SAC算法显著提升了性能和样本效率。

Abstract: The Soft Actor-Critic (SAC) algorithm, a state-of-the-art method in maximum
entropy reinforcement learning, traditionally relies on minimizing reverse
Kullback-Leibler (KL) divergence for policy updates. However, this approach
leads to an intractable optimal projection policy, necessitating gradient-based
approximations that can suffer from instability and poor sample efficiency.
This paper investigates the alternative use of forward KL divergence within
SAC. We demonstrate that for Gaussian policies, forward KL divergence yields an
explicit optimal projection policy -- corresponding to the mean and variance of
the target Boltzmann distribution's action marginals. Building on the distinct
advantages of both KL directions, we propose Bidirectional SAC, an algorithm
that first initializes the policy using the explicit forward KL projection and
then refines it by optimizing the reverse KL divergence. Comprehensive
experiments on continuous control benchmarks show that Bidirectional SAC
significantly outperforms standard SAC and other baselines, achieving up to a
$30\%$ increase in episodic rewards, alongside enhanced sample efficiency.

</details>


### [445] [Mixture of Experts Provably Detect and Learn the Latent Cluster Structure in Gradient-Based Learning](https://arxiv.org/abs/2506.01656)
*Ryotaro Kawata,Kohsei Matsutani,Yuri Kinoshita,Naoki Nishikawa,Taiji Suzuki*

Key words: Mixture of Experts, SGD, nonlinear regression, computational complexity, neural networks

TL;DR: MoE架构在机器学习中表现优异，但其复杂性导致理论理解滞后。本文通过SGD研究了MoE在回归任务中的样本和运行时复杂度，发现MoE能成功分解问题，而普通神经网络则无法检测潜在结构。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于MoE架构的复杂性，目前对其理论理解不足，本文旨在通过SGD深入研究MoE在非线性回归任务中的动态特性。

Method: 通过随机梯度下降（SGD）分析MoE在学习具有潜在聚类结构的回归任务时的样本和运行时复杂度。

Result: 普通神经网络无法检测潜在结构，而MoE能成功分解问题，各专家能弱恢复单个聚类的简单函数。

Conclusion: MoE在分解复杂问题为更简单的子问题方面具有优势，这是其成功的关键。

Abstract: Mixture of Experts (MoE), an ensemble of specialized models equipped with a
router that dynamically distributes each input to appropriate experts, has
achieved successful results in the field of machine learning. However,
theoretical understanding of this architecture is falling behind due to its
inherent complexity. In this paper, we theoretically study the sample and
runtime complexity of MoE following the stochastic gradient descent (SGD) when
learning a regression task with an underlying cluster structure of single index
models. On the one hand, we prove that a vanilla neural network fails in
detecting such a latent organization as it can only process the problem as a
whole. This is intrinsically related to the concept of information exponent
which is low for each cluster, but increases when we consider the entire task.
On the other hand, we show that a MoE succeeds in dividing this problem into
easier subproblems by leveraging the ability of each expert to weakly recover
the simpler function corresponding to an individual cluster. To the best of our
knowledge, this work is among the first to explore the benefits of the MoE
framework by examining its SGD dynamics in the context of nonlinear regression.

</details>


### [446] [Provably Safe Reinforcement Learning from Analytic Gradients](https://arxiv.org/abs/2506.01665)
*Tim Walter,Hannah Markgraf,Jonathan Külz,Matthias Althoff*

Key words: 安全强化学习, 分析梯度强化学习, 安全保障, 可微分模拟

TL;DR: 本文提出了一种首次用于分析梯度强化学习的有效安全保障方法，填补了该学习范式的安全空白。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在安全关键应用中部署自主机器人需要安全保证，而目前缺乏针对分析梯度强化学习的安全保障方法。

Method: 通过分析和改进现有可微分安全保障方法，将其与最先进的学习算法和可微分模拟相结合。

Result: 数值实验表明，该方法能在不影响性能的情况下实现安全的训练。

Conclusion: 研究成功填补了分析梯度强化学习领域的安全保障空白。

Abstract: Deploying autonomous robots in safety-critical applications requires safety
guarantees. Provably safe reinforcement learning is an active field of research
which aims to provide such guarantees using safeguards. These safeguards should
be integrated during training to prevent a large sim-to-real gap. While there
are several approaches for safeguarding sampling-based reinforcement learning,
analytic gradient-based reinforcement learning often achieves superior
performance and sample efficiency. However, there is no safeguarding approach
for this learning paradigm yet. Our work addresses this gap by developing the
first effective safeguard for analytic gradient-based reinforcement learning.
We analyse existing, differentiable safeguards, adapt them through modified
mappings and gradient formulations, and integrate them with a state-of-the-art
learning algorithm and a differentiable simulation. We evaluate how different
safeguards affect policy optimisation using numerical experiments on two
classical control tasks. The results demonstrate safeguarded training without
compromising performance.

</details>


### [447] [Minimal Impact ControlNet: Advancing Multi-ControlNet Integration](https://arxiv.org/abs/2506.01672)
*Shikun Sun,Min Zhou,Zixuan Wang,Xubin Li,Tiezheng Ge,Zijie Ye,Xiaoyu Qin,Junliang Xing,Bo Zheng,Jia Jia*

Key words: ControlNet, 图像生成, 控制信号, 无声信号, Jacobian矩阵

TL;DR: 该论文提出了一种名为Minimal Impact ControlNet的方法，通过平衡数据集构建、特征信号平衡注入和解决Jacobian矩阵不对称性，解决了多控制信号冲突问题，提升了图像生成的兼容性和自由度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前ControlNet训练中，多种控制信号可能相互冲突，尤其是在边缘型控制条件下，低频率的无声信号会抑制纹理生成，导致生成效果不佳。

Method: 通过构建平衡数据集、平衡注入特征信号以及解决Jacobian矩阵不对称性，提出Minimal Impact ControlNet方法。

Result: 该方法提升了控制信号的兼容性，使得无声信号区域能更自由和谐地生成图像。

Conclusion: Minimal Impact ControlNet有效解决了多控制信号冲突问题，为高质量、可控的图像生成提供了新思路。

Abstract: With the advancement of diffusion models, there is a growing demand for
high-quality, controllable image generation, particularly through methods that
utilize one or multiple control signals based on ControlNet. However, in
current ControlNet training, each control is designed to influence all areas of
an image, which can lead to conflicts when different control signals are
expected to manage different parts of the image in practical applications. This
issue is especially pronounced with edge-type control conditions, where regions
lacking boundary information often represent low-frequency signals, referred to
as silent control signals. When combining multiple ControlNets, these silent
control signals can suppress the generation of textures in related areas,
resulting in suboptimal outcomes. To address this problem, we propose Minimal
Impact ControlNet. Our approach mitigates conflicts through three key
strategies: constructing a balanced dataset, combining and injecting feature
signals in a balanced manner, and addressing the asymmetry in the score
function's Jacobian matrix induced by ControlNet. These improvements enhance
the compatibility of control signals, allowing for freer and more harmonious
generation in areas with silent control signals.

</details>


### [448] [When Lower-Order Terms Dominate: Adaptive Expert Algorithms for Heavy-Tailed Losses](https://arxiv.org/abs/2506.01722)
*Antoine Moulin,Emmanuel Esposito,Dirk van der Hoeven*

Key words: 专家预测, 重尾损失, 自适应算法, 遗憾界

TL;DR: 论文研究专家预测问题中的重尾损失场景，提出无需先验知识的自适应算法，改进现有算法的遗憾界。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有自适应算法在重尾损失场景下的遗憾界可能被低阶项主导，需要改进以减少这种依赖性。

Method: 提出新的自适应算法，不依赖损失的先验知识，通过优化遗憾界的设计来避免低阶项影响。

Result: 新算法在最坏情况下保证O(√(θTlog(K)))的遗憾界，且在独立同分布损失下表现更好。

Conclusion: 算法在重尾损失场景下显著改进遗憾界，适用于平方损失等具体场景。

Abstract: We consider the problem setting of prediction with expert advice with
possibly heavy-tailed losses, i.e.\ the only assumption on the losses is an
upper bound on their second moments, denoted by $\theta$. We develop adaptive
algorithms that do not require any prior knowledge about the range or the
second moment of the losses. Existing adaptive algorithms have what is
typically considered a lower-order term in their regret guarantees. We show
that this lower-order term, which is often the maximum of the losses, can
actually dominate the regret bound in our setting. Specifically, we show that
even with small constant $\theta$, this lower-order term can scale as
$\sqrt{KT}$, where $K$ is the number of experts and $T$ is the time horizon. We
propose adaptive algorithms with improved regret bounds that avoid the
dependence on such a lower-order term and guarantee $\mathcal{O}(\sqrt{\theta
T\log(K)})$ regret in the worst case, and $\mathcal{O}(\theta
\log(KT)/\Delta_{\min})$ regret when the losses are sampled i.i.d.\ from some
fixed distribution, where $\Delta_{\min}$ is the difference between the mean
losses of the second best expert and the best expert. Additionally, when the
loss function is the squared loss, our algorithm also guarantees improved
regret bounds over prior results.

</details>


### [449] [Principled data augmentation for learning to solve quadratic programming problems](https://arxiv.org/abs/2506.01728)
*Chendi Qian,Christopher Morris*

Key words: 学习到优化（L2O）、图神经网络（MPNNs）、数据增强、二次规划（QPs）、对比学习

TL;DR: 本文提出了一种针对二次规划问题（QPs）的数据增强方法，结合图神经网络（MPNNs）和自监督学习，以提升学习到优化（L2O）任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 学习到优化方法（L2O）在数据稀缺的情况下处理复杂优化问题（如QPs）时效果不佳，因此需要一种有效的数据增强方法来提升模型的泛化能力。

Method: 通过理论支持的数据增强生成多样性且保持最优性的实例，并结合对比学习进行自监督预训练MPNNs。

Result: 实验表明，该方法在监督学习中提升了泛化性能，并支持有效的迁移学习。

Conclusion: 提出的数据增强方法显著改善了L2O任务在数据稀缺情况下的表现，尤其在QPs问题上。

Abstract: Linear and quadratic optimization are crucial in numerous real-world
applications, from training machine learning models to integer-linear
optimization. Recently, learning-to-optimize methods (L2O) for linear (LPs) or
quadratic programs (QPs) using message-passing graph neural networks (MPNNs)
have gained traction, promising lightweight, data-driven proxies for solving
such optimization problems. For example, they replace the costly computation of
strong branching scores in branch-and-bound solvers, requiring solving many
such optimization problems. However, robust L2O MPNNs remain challenging in
data-scarce settings, especially when addressing complex optimization problems
such as QPs. This work introduces a principled approach to data augmentation
tailored for QPs via MPNNs. Our method leverages theoretically justified data
augmentation techniques to generate diverse yet optimality-preserving
instances. Furthermore, we integrate these augmentations into a self-supervised
learning framework based on contrastive learning, thereby pretraining MPNNs for
enhanced performance on L2O tasks. Extensive experiments demonstrate that our
approach improves generalization in supervised scenarios and facilitates
effective transfer learning to related optimization problems.

</details>


### [450] [Automated Manifold Learning for Reduced Order Modeling](https://arxiv.org/abs/2506.01741)
*Imran Nasim,Melanie Weber*

Key words: 几何表示学习, 流形学习, 系统动力学, 时空数据, 自动化学习

TL;DR: 本文研究了利用几何表示学习从时空数据中发现系统动力学的问题，提出了一种自动化流形学习框架，通过选择最佳算法和超参数来提高性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 几何表示学习在无监督学习中具有重要意义，但现有方法对几何假设和超参数选择高度敏感，需要一种自动化解决方案。

Method: 构建时空邻近图，应用经典和深度学习方法学习降阶动力学，并开发自动化流形学习框架以优化算法和超参数选择。

Result: 自动化框架在可扩展性和学习表示的准确性（捕捉局部和全局几何特征）方面表现更优。

Conclusion: 自动化流形学习可以有效解决现有方法对几何假设和超参数的依赖问题，提升性能和效率。

Abstract: The problem of identifying geometric structure in data is a cornerstone of
(unsupervised) learning. As a result, Geometric Representation Learning has
been widely applied across scientific and engineering domains. In this work, we
investigate the use of Geometric Representation Learning for the data-driven
discovery of system dynamics from spatial-temporal data. We propose to encode
similarity structure in such data in a spatial-temporal proximity graph, to
which we apply a range of classical and deep learning-based manifold learning
approaches to learn reduced order dynamics. We observe that while manifold
learning is generally capable of recovering reduced order dynamics, the quality
of the learned representations varies substantially across different algorithms
and hyperparameter choices. This is indicative of high sensitivity to the
inherent geometric assumptions of the respective approaches and suggests a need
for careful hyperparameter tuning, which can be expensive in practise. To
overcome these challenges, we propose a framework for Automated Manifold
Learning, which selects a manifold learning approach and corresponding
hyperparameter choices based on representative subsamples of the input graph.
We demonstrate that the proposed framework leads to performance gains both in
scalability and in the learned representations' accuracy in capturing local and
global geometric features of the underlying system dynamics.

</details>


### [451] [DRAUN: An Algorithm-Agnostic Data Reconstruction Attack on Federated Unlearning Systems](https://arxiv.org/abs/2506.01777)
*Hithem Lamri,Manaar Alam,Haiyan Jiang,Michail Maniatakos*

Key words: 联合学习遗忘（FU），数据重建攻击（DRA），隐私风险，GDPR，机器学习即服务（MLaaS）

TL;DR: DRAUN是一种针对联合学习遗忘（FU）系统的首次数据重建攻击（DRA）框架，揭示了现有FU方法在隐私保护上的漏洞。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于FU系统满足GDPR和CCPA等法规要求，允许客户从共享全局模型中删除特定数据的影响，但这一过程可能被恶意服务器利用，通过遗忘更新重建被删除的数据，存在隐私风险。

Method: DRAUN针对广泛采用的基于优化的遗忘方法，通过理论分析和实验验证，克服了现有DRA在FU系统中的局限性，实现高效数据重建。

Result: 在四个数据集和四种模型架构上的实验表明，DRAUN成功对五种流行的遗忘方法发起攻击，证明现有FU方法仍易受DRAs影响。

Conclusion: 研究揭示了FU系统在隐私保护上的潜在风险，为未来设计更安全的FU方法提供了重要参考。

Abstract: Federated Unlearning (FU) enables clients to remove the influence of specific
data from a collaboratively trained shared global model, addressing regulatory
requirements such as GDPR and CCPA. However, this unlearning process introduces
a new privacy risk: A malicious server may exploit unlearning updates to
reconstruct the data requested for removal, a form of Data Reconstruction
Attack (DRA). While DRAs for machine unlearning have been studied extensively
in centralized Machine Learning-as-a-Service (MLaaS) settings, their
applicability to FU remains unclear due to the decentralized, client-driven
nature of FU. This work presents DRAUN, the first attack framework to
reconstruct unlearned data in FU systems. DRAUN targets optimization-based
unlearning methods, which are widely adopted for their efficiency. We
theoretically demonstrate why existing DRAs targeting machine unlearning in
MLaaS fail in FU and show how DRAUN overcomes these limitations. We validate
our approach through extensive experiments on four datasets and four model
architectures, evaluating its performance against five popular unlearning
methods, effectively demonstrating that state-of-the-art FU methods remain
vulnerable to DRAs.

</details>


### [452] [Federated Gaussian Mixture Models](https://arxiv.org/abs/2506.01780)
*Sophia Zhang Pettersson,Kuo-Yun Liang,Juan Carlos Andresen*

Key words: 联邦学习,高斯混合模型,无监督学习,隐私保护,边缘计算

TL;DR: FedGenGMM 是一种用于高斯混合模型（GMM）的一步式联邦学习方法，适用于无监督学习场景，通过单轮通信聚合本地模型，解决了数据异质性、高通信成本和隐私问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习中存在数据异质性、高通信成本和隐私问题，FedGenGMM旨在通过一步式聚合解决这些问题，同时保持高效的模型性能。

Method: FedGenGMM利用GMM的生成特性，通过在服务器端生成合成数据集来训练全局模型，仅需单轮通信即可完成聚合。

Result: 在图像、表格和时间序列数据上的实验表明，FedGenGMM性能与非联邦及迭代联邦方法相当，同时显著降低通信开销，并在异常检测中表现稳健。

Conclusion: FedGenGMM为边缘计算环境提供了一种高效、灵活的联邦学习解决方案，适合数据异质性高的场景。

Abstract: This paper introduces FedGenGMM, a novel one-shot federated learning approach
for Gaussian Mixture Models (GMM) tailored for unsupervised learning scenarios.
In federated learning (FL), where multiple decentralized clients
collaboratively train models without sharing raw data, significant challenges
include statistical heterogeneity, high communication costs, and privacy
concerns. FedGenGMM addresses these issues by allowing local GMM models,
trained independently on client devices, to be aggregated through a single
communication round. This approach leverages the generative property of GMMs,
enabling the creation of a synthetic dataset on the server side to train a
global model efficiently. Evaluation across diverse datasets covering image,
tabular, and time series data demonstrates that FedGenGMM consistently achieves
performance comparable to non-federated and iterative federated methods, even
under significant data heterogeneity. Additionally, FedGenGMM significantly
reduces communication overhead, maintains robust performance in anomaly
detection tasks, and offers flexibility in local model complexities, making it
particularly suitable for edge computing environments.

</details>


### [453] [Enhancing Customer Service Chatbots with Context-Aware NLU through Selective Attention and Multi-task Learning](https://arxiv.org/abs/2506.01781)
*Subhadip Nandi,Neeraj Agrawal,Anshika Singh,Priyanka Bhatt*

Key words: 意图分类,上下文感知,NLU,选择性注意力,多任务学习

TL;DR: 该论文提出了一个结合客户查询和订单状态信息的上下文感知NLU模型，通过选择性注意力模块和多任务学习，显著提高了意图分类的准确性，并在实际应用中减少了人工干预，节省了成本。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前客户服务聊天机器人的意图分类模型仅基于客户查询，导致对模糊查询的处理准确性低。论文旨在通过结合上下文信息提高分类准确性和实用性。

Method: 提出了一种上下文感知NLU模型（MTL-CNLU-SAWC），结合选择性注意力模块和多任务学习，同时利用客户查询和订单状态信息进行分类。

Result: 模型在Top 2准确率上比仅使用查询的基线模型提高了4.8%，比现有结合查询和上下文的最先进模型提高了3.5%，显著减少了人工干预。

Conclusion: 通过结合上下文信息和多任务学习，显著提高了意图分类的准确性，并节省了公司成本。

Abstract: Customer service chatbots are conversational systems aimed at addressing
customer queries, often by directing them to automated workflows. A crucial
aspect of this process is the classification of the customer's intent.
Presently, most intent classification models for customer care utilise only
customer query for intent prediction. This may result in low-accuracy models,
which cannot handle ambiguous queries. An ambiguous query like "I didn't
receive my package" could indicate a delayed order, or an order that was
delivered but the customer failed to receive it. Resolution of each of these
scenarios requires the execution of very different sequence of steps. Utilizing
additional information, such as the customer's order delivery status, in the
right manner can help identify the intent for such ambiguous queries. In this
paper, we have introduced a context-aware NLU model that incorporates both, the
customer query and contextual information from the customer's order status for
predicting customer intent. A novel selective attention module is used to
extract relevant context features. We have also proposed a multi-task learning
paradigm for the effective utilization of different label types available in
our training data. Our suggested method, Multi-Task Learning Contextual NLU
with Selective Attention Weighted Context (MTL-CNLU-SAWC), yields a 4.8%
increase in top 2 accuracy score over the baseline model which only uses user
queries, and a 3.5% improvement over existing state-of-the-art models that
combine query and context. We have deployed our model to production for
Walmart's customer care domain. Accurate intent prediction through
MTL-CNLU-SAWC helps to better direct customers to automated workflows, thereby
significantly reducing escalations to human agents, leading to almost a million
dollars in yearly savings for the company.

</details>


### [454] [Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability](https://arxiv.org/abs/2506.01789)
*Genta Indra Winata,David Anugraha,Emmy Liu,Alham Fikri Aji,Shou-Yi Hung,Aditya Parashar,Patrick Amadeus Irawan,Ruochen Zhang,Zheng-Xin Yong,Jan Christian Blaise Cruz,Niklas Muennighoff,Seungone Kim,Hanyang Zhao,Sudipta Kar,Kezia Erina Suryoraharjo,M. Farid Adilazuarda,En-Shiun Annie Lee,Ayu Purwarianti,Derry Tanti Wijaya,Monojit Choudhury*

Key words: 数据集评估, LLM, 质量评估, 评审流程, 合成数据

TL;DR: 该论文提出DataRubrics框架，通过系统化和标准化的评估指标改进数据集评审过程，并探索了高效合成数据生成方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前数据集论文在原创性、多样性和质量控制方面存在不足，且评审过程中缺乏标准化评估方法。

Method: 引入DataRubrics框架，结合LLM技术，提供可重复、可扩展的数据集质量评估方案。

Result: 提出了可操作的数据集质量评估解决方案，并开源代码支持可重复性。

Conclusion: DataRubrics框架为数据为中心的研究提供了更高的质量标准和评审透明度。

Abstract: High-quality datasets are fundamental to training and evaluating machine
learning models, yet their creation-especially with accurate human
annotations-remains a significant challenge. Many dataset paper submissions
lack originality, diversity, or rigorous quality control, and these
shortcomings are often overlooked during peer review. Submissions also
frequently omit essential details about dataset construction and properties.
While existing tools such as datasheets aim to promote transparency, they are
largely descriptive and do not provide standardized, measurable methods for
evaluating data quality. Similarly, metadata requirements at conferences
promote accountability but are inconsistently enforced. To address these
limitations, this position paper advocates for the integration of systematic,
rubric-based evaluation metrics into the dataset review process-particularly as
submission volumes continue to grow. We also explore scalable, cost-effective
methods for synthetic data generation, including dedicated tools and
LLM-as-a-judge approaches, to support more efficient evaluation. As a call to
action, we introduce DataRubrics, a structured framework for assessing the
quality of both human- and model-generated datasets. Leveraging recent advances
in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and
actionable solution for dataset quality assessment, enabling both authors and
reviewers to uphold higher standards in data-centric research. We also release
code to support reproducibility of LLM-based evaluations at
https://github.com/datarubrics/datarubrics.

</details>


### [455] [$IF-GUIDE$: Influence Function-Guided Detoxification of LLMs](https://arxiv.org/abs/2506.01790)
*Zachary Coalson,Juhan Bae,Nicholas Carlini,Sanghyun Hong*

Key words: 模型毒性, 影响函数, 主动方法, 预训练, 微调

TL;DR: 论文提出了一种主动方法IF-Guide，通过影响函数识别并抑制训练数据中的有害标记，从而减少大型语言模型中的毒性行为，无需依赖人类偏好数据。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究训练数据如何导致大型语言模型中的毒性行为，并提出一种主动方法替代现有的反应式方法。

Method: 使用改进的影响函数识别有害标记，提出选择有害训练文档的技术，并设计可集成到预训练和微调中的学习目标。

Result: IF-Guide显著减少显性和隐性毒性，效果优于未审查模型和基线对齐方法（如DPO和RAD），且计算高效。

Conclusion: IF-Guide是一种高效且无需人类偏好数据的主动方法，能有效减少模型毒性。

Abstract: We study how training data contributes to the emergence of toxic behaviors in
large-language models. Most prior work on reducing model toxicity adopts
$reactive$ approaches, such as fine-tuning pre-trained (and potentially toxic)
models to align them with human values. In contrast, we propose a $proactive$
approach$-$IF-Guide$-$which leverages influence functions to identify harmful
tokens within any training data and suppress their impact during training. To
this end, we first show that standard influence functions are ineffective at
discovering harmful training records. We then present a novel adaptation that
measures token-level attributions from training data to model toxicity, along
with techniques for selecting toxic training documents and a learning objective
that can be integrated into both pre-training and fine-tuning. Moreover,
IF-Guide does not rely on human-preference data, which is typically required by
existing alignment methods. In evaluation, we demonstrate that IF-Guide
substantially reduces both explicit and implicit toxicity$-$by up to 10$\times$
compared to uncensored models, and up to 3$\times$ compared to baseline
alignment methods, e.g., DPO and RAD$-$across both pre-training and fine-tuning
scenarios. IF-Guide is computationally efficient: a billion-parameter model is
$not$ $necessary$ for computing influence scores; a million-parameter
model$-$with 7.5$\times$ fewer parameters$-$can effectively serve as a proxy
for identifying harmful data.

</details>


### [456] [Path Signatures for Feature Extraction. An Introduction to the Mathematics Underpinning an Efficient Machine Learning Technique](https://arxiv.org/abs/2506.01815)
*Stephan Sturm*

Key words: 路径签名, 特征提取, 机器学习, 数据流

TL;DR: 本文介绍了路径签名作为从数据流中进行机器学习特征提取的方法，强调其数学理论基础，适合本科生入门学习。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为学生提供路径签名方法的基础知识，避免过于技术性，便于理解。

Method: 通过数学理论介绍路径签名的概念，未深入技术细节或严格证明。

Result: 提供了路径签名方法的教学框架，适合初学者。

Conclusion: 路径签名是数据流特征提取的有效工具，适合在教学中引入。

Abstract: We provide an introduction to the topic of path signatures as means of
feature extraction for machine learning from data streams. The article stresses
the mathematical theory underlying the signature methodology, highlighting the
conceptual character without plunging into the technical details of rigorous
proofs. These notes are based on an introductory presentation given to students
of the Research Experience for Undergraduates in Industrial Mathematics and
Statistics at Worcester Polytechnic Institute in June 2024.

</details>


### [457] [Efficient Learning of Balanced Signed Graphs via Sparse Linear Programming](https://arxiv.org/abs/2506.01826)
*Haruki Yokota,Hiroshi Higashi,Yuichi Tanaka,Gene Cheung*

Key words: 符号图, 平衡图, 拉普拉斯矩阵, 谱滤波, 线性规划, ADMM

TL;DR: 该论文提出了一种直接从数据中学习平衡符号图拉普拉斯矩阵的高效方法，利用线性规划扩展了CLIME方法，并通过ADMM算法优化求解。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 符号图能同时编码数据的正负相关性，但现有方法难以直接学习平衡符号图的拉普拉斯矩阵，限制了谱滤波工具的复用。

Method: 扩展CLIME方法，提出基于线性规划的平衡符号图拉普拉斯矩阵学习框架，结合Hannan-Quinn准则和ADMM算法优化求解。

Result: 理论证明算法局部收敛，实验表明该方法优于竞争方法，并能复用正图谱滤波工具。

Conclusion: 该方法高效且实用，为符号图学习提供了新思路。

Abstract: Signed graphs are equipped with both positive and negative edge weights,
encoding pairwise correlations as well as anti-correlations in data. A balanced
signed graph is a signed graph with no cycles containing an odd number of
negative edges. Laplacian of a balanced signed graph has eigenvectors that map
via a simple linear transform to ones in a corresponding positive graph
Laplacian, thus enabling reuse of spectral filtering tools designed for
positive graphs. We propose an efficient method to learn a balanced signed
graph Laplacian directly from data. Specifically, extending a previous linear
programming (LP) based sparse inverse covariance estimation method called
CLIME, we formulate a new LP problem for each Laplacian column $i$, where the
linear constraints restrict weight signs of edges stemming from node $i$, so
that nodes of same / different polarities are connected by positive / negative
edges. Towards optimal model selection, we derive a suitable CLIME parameter
$\rho$ based on a combination of the Hannan-Quinn information criterion and a
minimum feasibility criterion. We solve the LP problem efficiently by tailoring
a sparse LP method based on ADMM. We theoretically prove local solution
convergence of our proposed iterative algorithm. Extensive experimental results
on synthetic and real-world datasets show that our balanced graph learning
method outperforms competing methods and enables reuse of spectral filters,
wavelets, and graph convolutional nets (GCN) constructed for positive graphs.

</details>


### [458] [Memory Access Characterization of Large Language Models in CPU Environment and its Potential Impacts](https://arxiv.org/abs/2506.01827)
*Spencer Banasik*

Key words: LLM, CPU推理, 缓存优化, 内存访问模式

TL;DR: 提升CPU环境下LLM推理速度，通过修改缓存架构来分析内存访问模式和性能特征，识别优化点。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于加速器在某些环境中不可用（如能耗、安全或成本限制），研究如何在不依赖加速器的情况下提升大模型推理速度。

Method: 通过实验分析Llama.cpp和QWEN模型的缓存配置性能及内存足迹，研究内存访问模式和性能特征。

Result: 实验结果帮助识别了潜在的优化方向，以改善CPU-only环境下的推理速度。

Conclusion: 通过缓存架构的调整和优化，可以在不依赖加速器的情况下提升LLM的推理效率。

Abstract: As machine learning algorithms are shown to be an increasingly valuable tool,
the demand for their access has grown accordingly. Oftentimes, it is infeasible
to run inference with larger models without an accelerator, which may be
unavailable in environments that have constraints such as energy consumption,
security, or cost. To increase the availability of these models, we aim to
improve the LLM inference speed on a CPU-only environment by modifying the
cache architecture. To determine what improvements could be made, we conducted
two experiments using Llama.cpp and the QWEN model: running various cache
configurations and evaluating their performance, and outputting a trace of the
memory footprint. Using these experiments, we investigate the memory access
patterns and performance characteristics to identify potential optimizations.

</details>


### [459] [SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model](https://arxiv.org/abs/2506.01833)
*Zhao Yang,Jiwei Zhu,Bing Su*

Key words: DNA表示学习、基因组图谱、混合专家、监督训练、跨物种学习

TL;DR: 该论文提出了一种基于监督训练和混合专家模型（SPACE）的新方法，用于提高DNA序列表示学习的效果，超越了传统的无监督预训练方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于纯DNA序列缺乏足够的功能信息，传统的无监督预训练方法效果有限。论文旨在通过利用基因组图谱的监督训练和跨物种协作学习，改进DNA表示学习。

Method: 提出了一种名为SPACE的模型，结合混合专家（MoE）方法，捕捉不同物种和基因组图谱之间的关系，从而学习更有效的DNA表示。

Result: 在多个任务上的实验表明，SPACE模型实现了最先进的性能，证明了基于监督基因组图谱训练的DNA模型的优越性。

Conclusion: 通过监督训练和跨物种协作学习，SPACE模型显著提升了DNA序列表示学习的效果，为相关研究提供了新思路。

Abstract: Inspired by the success of unsupervised pre-training paradigms, researchers
have applied these approaches to DNA pre-training. However, we argue that these
approaches alone yield suboptimal results because pure DNA sequences lack
sufficient information, since their functions are regulated by genomic profiles
like chromatin accessibility. Here, we demonstrate that supervised training for
genomic profile prediction serves as a more effective alternative to pure
sequence pre-training. Furthermore, considering the multi-species and
multi-profile nature of genomic profile prediction, we introduce our
$\textbf{S}$pecies-$\textbf{P}$rofile $\textbf{A}$daptive
$\textbf{C}$ollaborative $\textbf{E}$xperts (SPACE) that leverages Mixture of
Experts (MoE) to better capture the relationships between DNA sequences across
different species and genomic profiles, thereby learning more effective DNA
representations. Through extensive experiments across various tasks, our model
achieves state-of-the-art performance, establishing that DNA models trained
with supervised genomic profiles serve as powerful DNA representation learners.
The code is available at https://github.com/ZhuJiwei111/SPACE.

</details>


### [460] [SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics](https://arxiv.org/abs/2506.01844)
*Mustafa Shukor,Dana Aubakirova,Francesco Capuano,Pepijn Kooijmans,Steven Palma,Adil Zouitine,Michel Aractingi,Caroline Pascal,Martino Russi,Andres Marafioti,Simon Alibert,Matthieu Cord,Thomas Wolf,Remi Cadene*

Key words: 视觉语言模型, 机器人, 低成本部署, 异步推理

TL;DR: SmolVLA是一个高效的小型视觉语言动作模型，通过减少训练和推理成本，同时保持竞争力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有视觉语言动作模型参数量过大，导致训练成本高且实际部署受限，且忽视了社区数据的潜力。

Method: 设计SmolVLA，支持单GPU训练，部署于消费级硬件，并引入异步推理栈提高响应速度。

Result: SmolVLA性能与大型模型相当，同时在模拟和现实机器人基准测试中表现优异。

Conclusion: SmolVLA为机器人领域提供了高效、低成本的解决方案，推动了社区数据的应用。

Abstract: Vision-language models (VLMs) pretrained on large-scale multimodal datasets
encode rich visual and linguistic knowledge, making them a strong foundation
for robotics. Rather than training robotic policies from scratch, recent
approaches adapt VLMs into vision-language-action (VLA) models that enable
natural language-driven perception and control. However, existing VLAs are
typically massive--often with billions of parameters--leading to high training
costs and limited real-world deployability. Moreover, they rely on academic and
industrial datasets, overlooking the growing availability of
community-collected data from affordable robotic platforms. In this work, we
present SmolVLA, a small, efficient, and community-driven VLA that drastically
reduces both training and inference costs, while retaining competitive
performance. SmolVLA is designed to be trained on a single GPU and deployed on
consumer-grade GPUs or even CPUs. To further improve responsiveness, we
introduce an asynchronous inference stack decoupling perception and action
prediction from action execution, allowing higher control rates with chunked
action generation. Despite its compact size, SmolVLA achieves performance
comparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both
simulated as well as real-world robotic benchmarks and release all code,
pretrained models, and training data.

</details>


### [461] [Trojan Horse Hunt in Time Series Forecasting for Space Operations](https://arxiv.org/abs/2506.01849)
*Krzysztof Kotowski,Ramez Shendy,Jakub Nalepa,Przemysław Biecek,Piotr Wilczyński,Agata Kaczmarek,Dawid Płudowski,Artur Janicki,Evridiki Ntagiou*

Key words: Kaggle, 对抗性中毒, 卫星遥测, 时间序列, Neural Cleanse

TL;DR: Kaggle竞赛聚焦于卫星遥测预测模型中的对抗性中毒问题，任务是开发方法以检测和重建触发器。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对卫星遥测预测模型中的对抗性中毒威胁，确保空间领域AI应用的安全。

Method: 参赛者需使用提供的公开数据集、参考模型和中毒模型，开发新方法以重建触发器。

Result: 竞赛结果将展示触发器的具体特征（形状、幅值、时长），并提出改进方法。

Conclusion: 该竞赛不仅适用于空间领域，也适用于其他安全关键的时间序列分析应用。

Abstract: This competition hosted on Kaggle
(https://www.kaggle.com/competitions/trojan-horse-hunt-in-space) is the first
part of a series of follow-up competitions and hackathons related to the
"Assurance for Space Domain AI Applications" project funded by the European
Space Agency (https://assurance-ai.space-codev.org/). The competition idea is
based on one of the real-life AI security threats identified within the project
-- the adversarial poisoning of continuously fine-tuned satellite telemetry
forecasting models. The task is to develop methods for finding and
reconstructing triggers (trojans) in advanced models for satellite telemetry
forecasting used in safety-critical space operations. Participants are provided
with 1) a large public dataset of real-life multivariate satellite telemetry
(without triggers), 2) a reference model trained on the clean data, 3) a set of
poisoned neural hierarchical interpolation (N-HiTS) models for time series
forecasting trained on the dataset with injected triggers, and 4) Jupyter
notebook with the training pipeline and baseline algorithm (the latter will be
published in the last month of the competition). The main task of the
competition is to reconstruct a set of 45 triggers (i.e., short multivariate
time series segments) injected into the training data of the corresponding set
of 45 poisoned models. The exact characteristics (i.e., shape, amplitude, and
duration) of these triggers must be identified by participants. The popular
Neural Cleanse method is adopted as a baseline, but it is not designed for time
series analysis and new approaches are necessary for the task. The impact of
the competition is not limited to the space domain, but also to many other
safety-critical applications of advanced time series analysis where model
poisoning may lead to serious consequences.

</details>


### [462] [Trade-offs in Data Memorization via Strong Data Processing Inequalities](https://arxiv.org/abs/2506.01855)
*Vitaly Feldman,Guy Kornowski,Xin Lyu*

Key words: 数据记忆、隐私保护、强数据处理不等式、二元分类、混合聚类模型

TL;DR: 该研究探讨了大型语言模型训练中的数据记忆问题，并提出了一种证明数据记忆下界的新方法。研究显示，简单的二元分类问题中存在样本数量与所需记忆信息之间的权衡。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决基于敏感用户数据训练的大型语言模型可能导致的隐私泄露问题，研究者探讨了数据记忆在学习中的作用。

Method: 通过将强数据处理不等式与数据记忆联系起来，提出了一种证明数据记忆下界的通用方法，并分析了二元分类和混合聚类模型。

Result: 研究表明，少样本情况下需要记忆$	ext{Ω}(d)$位信息，且这些下界通常被简单学习算法匹配。

Conclusion: 该研究改进了Brown等人的工作，并为理解数据记忆与学习之间的权衡提供了新视角。

Abstract: Recent research demonstrated that training large language models involves
memorization of a significant fraction of training data. Such memorization can
lead to privacy violations when training on sensitive user data and thus
motivates the study of data memorization's role in learning. In this work, we
develop a general approach for proving lower bounds on excess data
memorization, that relies on a new connection between strong data processing
inequalities and data memorization. We then demonstrate that several simple and
natural binary classification problems exhibit a trade-off between the number
of samples available to a learning algorithm, and the amount of information
about the training data that a learning algorithm needs to memorize to be
accurate. In particular, $\Omega(d)$ bits of information about the training
data need to be memorized when $O(1)$ $d$-dimensional examples are available,
which then decays as the number of examples grows at a problem-specific rate.
Further, our lower bounds are generally matched (up to logarithmic factors) by
simple learning algorithms. We also extend our lower bounds to more general
mixture-of-clusters models. Our definitions and results build on the work of
Brown et al. (2021) and address several limitations of the lower bounds in
their work.

</details>


### [463] [Unified Scaling Laws for Compressed Representations](https://arxiv.org/abs/2506.01863)
*Andrei Panferov,Alexandra Volkova,Ionut-Vlad Modoranu,Vage Egiazarian,Mher Safaryan,Dan Alistarh*

Key words: 扩展定律, 模型压缩, 稀疏表示, 量化, 参数效率

TL;DR: 该论文研究了扩展定律与压缩格式之间的相互作用，提出了一种统一的扩展框架，能够预测在不同压缩表示下训练的模型性能，并提出了一种基于表示能力的简单指标。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索扩展定律与压缩格式的相互作用，以解决大规模训练和推理中的计算成本问题。

Method: 通过验证通用的扩展定律框架，并将其应用于稀疏、量化等压缩格式，同时提出基于表示能力的简单指标。

Result: 理论和实证表明，基于表示能力的指标可以稳健地预测多种压缩格式的参数效率。

Conclusion: 研究为压缩格式的性能预测和算法改进提供了理论基础。

Abstract: Scaling laws have shaped recent advances in machine learning by enabling
predictable scaling of model performance based on model size, computation, and
data volume. Concurrently, the rise in computational cost for AI has motivated
model compression techniques, notably quantization and sparsification, which
have emerged to mitigate the steep computational demands associated with
large-scale training and inference. This paper investigates the interplay
between scaling laws and compression formats, exploring whether a unified
scaling framework can accurately predict model performance when training occurs
over various compressed representations, such as sparse, scalar-quantized,
sparse-quantized or even vector-quantized formats. Our key contributions
include validating a general scaling law formulation and showing that it is
applicable both individually but also composably across compression types.
Based on this, our main finding is demonstrating both theoretically and
empirically that there exists a simple "capacity" metric -- based on the
representation's ability to fit random Gaussian data -- which can robustly
predict parameter efficiency across multiple compressed representations. On the
practical side, we extend our formulation to directly compare the accuracy
potential of different compressed formats, and to derive better algorithms for
training over sparse-quantized formats.

</details>


### [464] [NepTrain and NepTrainKit: Automated Active Learning and Visualization Toolkit for Neuroevolution Potentials](https://arxiv.org/abs/2506.01868)
*Chengbing Chen,Yutong Li,Rui Zhao,Zhoulin Liu,Zheyong Fan,Gang Tang,Zhiyong Wang*

Key words: NEP, 训练数据集, NepTrain, NepTrainKit, 机器学习势能

TL;DR: 介绍了NepTrain和NepTrainKit工具，用于高效管理和筛选NEP模型的训练数据集，提高训练质量和效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 构建高质量的NEP训练数据集是开发准确模型的关键，但传统方法耗时耗力。

Method: 开发了NepTrain（Python包）和NepTrainKit（GUI软件），提供数据集初始化、管理和自动化训练功能。

Result: 成功应用于CsPbI3案例，验证了工具的实用性和模型预测准确性。

Conclusion: 该工具为机器学习原子间势能研究者提供了高效便捷的解决方案。

Abstract: As a machine-learned potential, the neuroevolution potential (NEP) method
features exceptional computational efficiency and has been successfully applied
in materials science. Constructing high-quality training datasets is crucial
for developing accurate NEP models. However, the preparation and screening of
NEP training datasets remain a bottleneck for broader applications due to their
time-consuming, labor-intensive, and resource-intensive nature. In this work,
we have developed NepTrain and NepTrainKit, which are dedicated to initializing
and managing training datasets to generate high-quality training sets while
automating NEP model training. NepTrain is an open-source Python package that
features a bond length filtering method to effectively identify and remove
non-physical structures from molecular dynamics trajectories, thereby ensuring
high-quality training datasets. NepTrainKit is a graphical user interface (GUI)
software designed specifically for NEP training datasets, providing
functionalities for data editing, visualization, and interactive exploration.
It integrates key features such as outlier identification, farthest-point
sampling, non-physical structure detection, and configuration type selection.
The combination of these tools enables users to process datasets more
efficiently and conveniently. Using $\rm CsPbI_3$ as a case study, we
demonstrate the complete workflow for training NEP models with NepTrain and
further validate the models through materials property predictions. We believe
this toolkit will greatly benefit researchers working with machine learning
interatomic potentials.

</details>


### [465] [Frugal Machine Learning for Energy-efficient, and Resource-aware Artificial Intelligence](https://arxiv.org/abs/2506.01869)
*John Violos,Konstantina-Christina Diamanti,Ioannis Kompatsiaris,Symeon Papadopoulos*

Key words: Frugal Machine Learning, resource efficiency, edge computing, model compression, IoT

TL;DR: Frugal Machine Learning (FML) focuses on creating efficient and resource-conscious ML models, emphasizing input, learning process, and model frugality to minimize computational costs while maintaining performance.

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: The field aims to address resource constraints in ML, especially in edge computing and IoT environments, by reducing computational, time, and energy costs.

Method: FML employs strategies like model compression, energy-efficient hardware, and data-efficient learning techniques, along with adaptive methods such as knowledge distillation and dynamic architecture design.

Result: The chapter highlights advancements in FML, provides a taxonomy of frugal methods, and includes case studies across various domains.

Conclusion: FML is crucial for resource-limited environments and offers future research directions to further innovate the field.

Abstract: Frugal Machine Learning (FML) refers to the practice of designing Machine
Learning (ML) models that are efficient, cost-effective, and mindful of
resource constraints. This field aims to achieve acceptable performance while
minimizing the use of computational resources, time, energy, and data for both
training and inference. FML strategies can be broadly categorized into input
frugality, learning process frugality, and model frugality, each focusing on
reducing resource consumption at different stages of the ML pipeline. This
chapter explores recent advancements, applications, and open challenges in FML,
emphasizing its importance for smart environments that incorporate edge
computing and IoT devices, which often face strict limitations in bandwidth,
energy, or latency. Technological enablers such as model compression,
energy-efficient hardware, and data-efficient learning techniques are
discussed, along with adaptive methods including parameter regularization,
knowledge distillation, and dynamic architecture design that enable incremental
model updates without full retraining. Furthermore, it provides a comprehensive
taxonomy of frugal methods, discusses case studies across diverse domains, and
identifies future research directions to drive innovation in this evolving
field.

</details>


### [466] [Learning to Explore: An In-Context Learning Approach for Pure Exploration](https://arxiv.org/abs/2506.01876)
*Alessio Russo,Ryan Welch,Aldo Pacchiano*

Key words: 主动学习,纯探索,强化学习,Transformer,上下文学习

TL;DR: 本文提出了一种基于上下文学习的纯探索方法（ICPE），通过结合监督学习和强化学习直接学习探索策略，无需先验假设，显著提升了数据效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法在适应性探索策略上表现不佳，尤其是当信息结构未能充分表征时。本文旨在解决这一问题，提出一种无需显式建模假设的方法。

Method: ICPE采用Transformer和上下文学习技术，结合监督学习与强化学习，直接从经验中学习探索策略。

Result: 在多种合成和半合成基准测试中，ICPE表现出鲁棒性能，能够匹配最优实例依赖算法，仅使用深度学习技术即可实现高效数据探索。

Conclusion: ICPE是一种实用且通用的数据高效探索方法，适用于确定性、随机性和结构化场景。

Abstract: In this work, we study the active sequential hypothesis testing problem, also
known as pure exploration, where the goal is to actively control a data
collection process to efficiently identify the correct hypothesis underlying a
decision problem. While relevant across multiple domains, devising adaptive
exploration strategies remains challenging, particularly due to difficulties in
encoding appropriate inductive biases. Existing Reinforcement Learning
(RL)-based methods often underperform when relevant information structures are
inadequately represented, whereas more complex methods, like Best Arm
Identification (BAI) techniques, may be difficult to devise and typically rely
on explicit modeling assumptions. To address these limitations, we introduce
In-Context Pure Exploration (ICPE), an in-context learning approach that uses
Transformers to learn exploration strategies directly from experience. ICPE
combines supervised learning and reinforcement learning to identify and exploit
latent structure across related tasks, without requiring prior assumptions.
Numerical results across diverse synthetic and semi-synthetic benchmarks
highlight ICPE's capability to achieve robust performance performance in
deterministic, stochastic, and structured settings. These results demonstrate
ICPE's ability to match optimal instance-dependent algorithms using only deep
learning techniques, making it a practical and general approach to
data-efficient exploration.

</details>


### [467] [scDataset: Scalable Data Loading for Deep Learning on Large-Scale Single-Cell Omics](https://arxiv.org/abs/2506.01883)
*Davide D'Ascenzo,Sebastiano Cultrera di Montesano*

Key words: 单细胞数据,深度学习,数据加载,AnnData,scDataset

TL;DR: scDataset是一个直接在AnnData文件上操作的PyTorch IterableDataset，通过块采样和批量读取提高了单细胞模型训练的效率和可扩展性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现代单细胞数据集规模庞大，现有数据加载方法内存效率低、存储需求高且速度慢，急需一种更高效的解决方案。

Method: 结合块采样和批量读取技术，scDataset直接在AnnData文件上操作，无需格式转换。

Result: 在Tahoe 100M数据集上，scDataset比现有工具快18-48倍。

Conclusion: scDataset为大规模单细胞模型训练提供了高效、可扩展的解决方案。

Abstract: Modern single-cell datasets now comprise hundreds of millions of cells,
presenting significant challenges for training deep learning models that
require shuffled, memory-efficient data loading. While the AnnData format is
the community standard for storing single-cell datasets, existing data loading
solutions for AnnData are often inadequate: some require loading all data into
memory, others convert to dense formats that increase storage demands, and many
are hampered by slow random disk access. We present scDataset, a PyTorch
IterableDataset that operates directly on one or more AnnData files without the
need for format conversion. The core innovation is a combination of block
sampling and batched fetching, which together balance randomness and I/O
efficiency. On the Tahoe 100M dataset, scDataset achieves up to a 48$\times$
speed-up over AnnLoader, a 27$\times$ speed-up over HuggingFace Datasets, and
an 18$\times$ speed-up over BioNeMo in single-core settings. These advances
democratize large-scale single-cell model training for the broader research
community.

</details>


### [468] [Agnostic Reinforcement Learning: Foundations and Algorithms](https://arxiv.org/abs/2506.01884)
*Gene Li*

Key words: 强化学习, 统计复杂性, 函数逼近, 无先验知识策略学习

TL;DR: 该论文研究了在状态空间大、需要函数逼近的环境中强化学习的统计复杂性，提出了基于学习理论视角的严格分析方法，并通过三个关键维度（环境访问、覆盖条件和表示条件）系统探索了无先验知识策略学习。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管强化学习在许多领域取得了巨大成功，但对于大规模状态空间中函数逼近的统计复杂性缺乏理论理解，因此填补这一理论空白成为研究动机。

Method: 通过无先验知识策略学习的框架，研究环境访问、覆盖条件和表示条件三个维度，设计新算法并分析其理论性能界限。

Result: 研究揭示了无先验知识策略学习的统计分离，展示了其能力与局限性，并提出了具有理论保证的新算法。

Conclusion: 论文通过系统分析强化学习的统计复杂性，为大规模状态空间中的函数逼近提供了理论基础，并展示了无先验知识策略学习的潜力与限制。

Abstract: Reinforcement Learning (RL) has demonstrated tremendous empirical success
across numerous challenging domains. However, we lack a strong theoretical
understanding of the statistical complexity of RL in environments with large
state spaces, where function approximation is required for sample-efficient
learning. This thesis addresses this gap by rigorously examining the
statistical complexity of RL with function approximation from a learning
theoretic perspective. Departing from a long history of prior work, we consider
the weakest form of function approximation, called agnostic policy learning, in
which the learner seeks to find the best policy in a given class $\Pi$, with no
guarantee that $\Pi$ contains an optimal policy for the underlying task.
  We systematically explore agnostic policy learning along three key axes:
environment access -- how a learner collects data from the environment;
coverage conditions -- intrinsic properties of the underlying MDP measuring the
expansiveness of state-occupancy measures for policies in the class $\Pi$, and
representational conditions -- structural assumptions on the class $\Pi$
itself. Within this comprehensive framework, we (1) design new learning
algorithms with theoretical guarantees and (2) characterize fundamental
performance bounds of any algorithm. Our results reveal significant statistical
separations that highlight the power and limitations of agnostic policy
learning.

</details>


### [469] [CogniAlign: Word-Level Multimodal Speech Alignment with Gated Cross-Attention for Alzheimer's Detection](https://arxiv.org/abs/2506.01890)
*David Ortiz-Perez,Manuel Benavent-Lledo,Javier Rodriguez-Juan,Jose Garcia-Rodriguez,David Tomás*

Key words: 阿尔茨海默病、多模态融合、音频文本对齐、门控注意力、韵律建模

TL;DR: CogniAlign是一种多模态架构，用于阿尔茨海默病的早期检测，通过音频和文本模态的精细对齐和融合，显著提高了检测准确率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 早期发现认知障碍（如阿尔茨海默病）对及时干预和改善患者预后至关重要。传统方法在多模态融合上较为粗糙，无法充分利用互补信息。

Method: CogniAlign采用词级时间对齐策略，将音频嵌入与文本标记同步，并提出了门控跨模态注意力融合机制，同时整合了韵律线索（如停顿标记）。

Result: 在ADReSSo数据集上，CogniAlign达到90.36%的准确率，优于现有方法，消融研究验证了其策略的有效性。

Conclusion: CogniAlign通过精细的多模态对齐和融合，显著提升了阿尔茨海默病的检测性能。

Abstract: Early detection of cognitive disorders such as Alzheimer's disease is
critical for enabling timely clinical intervention and improving patient
outcomes. In this work, we introduce CogniAlign, a multimodal architecture for
Alzheimer's detection that integrates audio and textual modalities, two
non-intrusive sources of information that offer complementary insights into
cognitive health. Unlike prior approaches that fuse modalities at a coarse
level, CogniAlign leverages a word-level temporal alignment strategy that
synchronizes audio embeddings with corresponding textual tokens based on
transcription timestamps. This alignment supports the development of
token-level fusion techniques, enabling more precise cross-modal interactions.
To fully exploit this alignment, we propose a Gated Cross-Attention Fusion
mechanism, where audio features attend over textual representations, guided by
the superior unimodal performance of the text modality. In addition, we
incorporate prosodic cues, specifically interword pauses, by inserting pause
tokens into the text and generating audio embeddings for silent intervals,
further enriching both streams. We evaluate CogniAlign on the ADReSSo dataset,
where it achieves an accuracy of 90.36%, outperforming existing
state-of-the-art methods. A detailed ablation study confirms the advantages of
our alignment strategy, attention-based fusion, and prosodic modeling.

</details>


### [470] [MLorc: Momentum Low-rank Compression for Large Language Model Adaptation](https://arxiv.org/abs/2506.01897)
*Wei Shen,Yaxiang Zhang,Minhui Huang,Mengfan Xu,Jiawei Zhang,Cong Shen*

Key words: 大语言模型，内存高效训练，动量压缩，低秩方法，MLorc

TL;DR: 本文提出了一种名为MLorc的内存高效训练范式，通过压缩和重构动量而非梯度，避免了权重更新矩阵的固定秩约束，优于现有方法，性能接近甚至超越全参数微调。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着大语言模型规模的增加，全参数微调对内存需求巨大，需要一种更高效的训练方法。

Method: 采用MLorc方法，直接压缩和重构动量，避免权重更新矩阵的固定秩约束，保留全参数微调的训练动态。

Result: MLorc在保持时间和内存效率的同时，性能优于其他内存高效训练方法，接近甚至超越全参数微调，且对不同优化器具有良好的泛化性。

Conclusion: MLorc是一种高效且性能优越的训练方法，适用于大语言模型的微调。

Abstract: With increasing size of large language models (LLMs), full-parameter
fine-tuning imposes substantial memory demands. To alleviate this, we propose a
novel memory-efficient training paradigm called Momentum Low-rank compression
(MLorc). By directly compressing and reconstructing momentum rather than
gradients, MLorc avoids imposing a fixed-rank constraint on weight update
matrices and better preserves the training dynamics of full-parameter
fine-tuning, in contrast to existing low-rank approaches such as LoRA and
GaLore. Empirically, MLorc consistently outperforms other memory-efficient
training methods, matches or even exceeds the performance of full fine-tuning
with a small rank (e.g., $r=4$), and generalizes well across different
optimizers -- all while not compromising time or memory efficiency.
Furthermore, we provide a theoretical guarantee for its convergence under
reasonable assumptions.

</details>


### [471] [SMOTE-DP: Improving Privacy-Utility Tradeoff with Synthetic Data](https://arxiv.org/abs/2506.01907)
*Yan Zhou,Bradley Malin,Murat Kantarcioglu*

Key words: 隐私保护, 合成数据, 差分隐私, SMOTE, 数据效用

TL;DR: 论文提出了一种结合SMOTE和差分隐私的合成数据生成方法（SMOTE-DP），旨在解决隐私保护与数据效用之间的权衡问题，证明了该方法在保护隐私的同时能保持数据的高效用。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 隐私保护数据发布（如合成数据共享）常面临隐私与效用的权衡问题，现有方法（如加噪差分隐私）可能导致效用显著下降。本文旨在探索一种既能保护隐私又不显著损失效用的合成数据生成机制。

Method: 提出SMOTE-DP技术，结合生成合成数据的SMOTE方法和差分隐私保护机制，利用其各自的优势生成隐私保护且效用的数据。

Result: 理论分析和实证表明，SMOTE-DP生成的合成数据不仅能提供强隐私保护，还能在下游学习任务中保持良好的效用。

Conclusion: SMOTE-DP为解决隐私与效用的权衡问题提供了有效方案，证明了其在隐私保护和数据效用方面的双重优势。

Abstract: Privacy-preserving data publication, including synthetic data sharing, often
experiences trade-offs between privacy and utility. Synthetic data is generally
more effective than data anonymization in balancing this trade-off, however,
not without its own challenges. Synthetic data produced by generative models
trained on source data may inadvertently reveal information about outliers.
Techniques specifically designed for preserving privacy, such as introducing
noise to satisfy differential privacy, often incur unpredictable and
significant losses in utility. In this work we show that, with the right
mechanism of synthetic data generation, we can achieve strong privacy
protection without significant utility loss. Synthetic data generators
producing contracting data patterns, such as Synthetic Minority Over-sampling
Technique (SMOTE), can enhance a differentially private data generator,
leveraging the strengths of both. We prove in theory and through empirical
demonstration that this SMOTE-DP technique can produce synthetic data that not
only ensures robust privacy protection but maintains utility in downstream
learning tasks.

</details>


### [472] [Generalized Gradient Norm Clipping & Non-Euclidean $(L_0,L_1)$-Smoothness](https://arxiv.org/abs/2506.01913)
*Thomas Pethick,Wanyun Xie,Mete Erdogan,Kimon Antonakopoulos,Tony Silveti-Falls,Volkan Cevher*

Key words: 非欧几里得优化,梯度裁剪,权重衰减,动量梯度估计器,深度学习

TL;DR: 提出了一种混合非欧几里得优化方法，结合了最速下降和条件梯度方法，通过广义($L_0$,$L_1$)-光滑性实现下降性质，并在随机情况下展示了最优收敛速度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了在优化问题中结合最速下降和条件梯度方法的优势，同时通过广义光滑性概念提升性能。

Method: 采用混合非欧几里得优化方法，结合梯度范数裁剪和权重衰减，利用动量梯度估计器优化随机情况下的收敛速度。

Result: 在随机情况下实现了$O(n^{-1/4})$的最优收敛速度，并在深度学习中验证了其有效性。

Conclusion: 提出的方法在理论和实验上均表现出色，适用于深度学习任务。

Abstract: This work introduces a hybrid non-Euclidean optimization method which
generalizes gradient norm clipping by combining steepest descent and
conditional gradient approaches. The method achieves the best of both worlds by
establishing a descent property under a generalized notion of
($L_0$,$L_1$)-smoothness. Weight decay is incorporated in a principled manner
by identifying a connection to the Frank-Wolfe short step. In the stochastic
case, we show an order optimal $O(n^{-1/4})$ convergence rate by leveraging a
momentum based gradient estimator. We discuss how to instantiate the algorithms
for deep learning and demonstrate their properties on image classification and
language modeling.

</details>


### [473] [Transformers as Multi-task Learners: Decoupling Features in Hidden Markov Models](https://arxiv.org/abs/2506.01919)
*Yifan Hao,Chenlu Ye,Chi Han,Tong Zhang*

Key words: Transformer, 分层行为, 多任务泛化, 隐马尔可夫模型, 理论分析

TL;DR: 该研究通过分析Transformer的分层行为，揭示了其在多任务泛化能力背后的机制，并通过理论分析支持其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管Transformer在序列学习中表现出色，但其背后的理论机制尚不明确，研究旨在填补这一空白。

Method: 通过研究典型序列模型（如隐马尔可夫模型）的分层行为，分析Transformer的特征提取和时域解耦能力。

Result: 下层提取基于邻近词的特征表示，上层实现特征解耦和时域解耦；理论分析与实验结果一致。

Conclusion: 研究为Transformer在序列任务中的高效性和有效性提供了理论支持。

Abstract: Transformer based models have shown remarkable capabilities in sequence
learning across a wide range of tasks, often performing well on specific task
by leveraging input-output examples. Despite their empirical success, a
comprehensive theoretical understanding of this phenomenon remains limited. In
this work, we investigate the layerwise behavior of Transformers to uncover the
mechanisms underlying their multi-task generalization ability. Taking
explorations on a typical sequence model, i.e, Hidden Markov Models, which are
fundamental to many language tasks, we observe that: first, lower layers of
Transformers focus on extracting feature representations, primarily influenced
by neighboring tokens; second, on the upper layers, features become decoupled,
exhibiting a high degree of time disentanglement. Building on these empirical
insights, we provide theoretical analysis for the expressiveness power of
Transformers. Our explicit constructions align closely with empirical
observations, providing theoretical support for the Transformer's effectiveness
and efficiency on sequence learning across diverse tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [474] [Toward Knowledge-Guided AI for Inverse Design in Manufacturing: A Perspective on Domain, Physics, and Human-AI Synergy](https://arxiv.org/abs/2506.00056)
*Hugon Lee,Hyeonbin Moon,Junhyeong Lee,Seunghwa RYu*

Key words: 人工智能，逆向设计，物理信息学习，人机交互，大语言模型

TL;DR: 人工智能（AI）正在重塑制造业领域的逆向设计，推动高性能材料和工艺的发现。然而，纯数据驱动方法在数据稀疏、高维设计空间和复杂物理约束的现实场景中表现不佳。本文提出新一代设计系统，整合领域知识、物理信息学习和人机交互界面，超越黑箱模型。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决纯数据驱动方法在现实逆向设计场景中的局限性，提出更高效、物理一致且易于交互的设计系统。

Method: 结合专家引导采样、物理信息机器学习和大语言模型交互技术，构建整合领域知识、物理先验和自适应推理的系统。

Result: 系统展示了更高的数据效率、物理一致性和用户友好性，支持可扩展、可解释和易访问的AI驱动设计。

Conclusion: 逆向设计应发展为统一生态系统，结合领域知识、物理先验和自适应推理，实现高性能和可解释性。

Abstract: Artificial intelligence (AI) is reshaping inverse design across manufacturing
domain, enabling high-performance discovery in materials, products, and
processes. However, purely data-driven approaches often struggle in realistic
settings characterized by sparse data, high-dimensional design spaces, and
nontrivial physical constraints. This perspective argues for a new generation
of design systems that transcend black-box modeling by integrating domain
knowledge, physics-informed learning, and intuitive human-AI interfaces. We
first demonstrate how expert-guided sampling strategies enhance data efficiency
and model generalization. Next, we discuss how physics-informed machine
learning enables physically consistent modeling in data-scarce regimes.
Finally, we explore how large language models emerge as interactive design
agents connecting user intent with simulation tools, optimization pipelines,
and collaborative workflows. Through illustrative examples and conceptual
frameworks, we advocate that inverse design in manufacturing should evolve into
a unified ecosystem, where domain knowledge, physical priors, and adaptive
reasoning collectively enable scalable, interpretable, and accessible AI-driven
design systems.

</details>


### [475] [The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets](https://arxiv.org/abs/2506.00073)
*Shenzhe Zhu,Jiao Sun,Yi Nian,Tobin South,Alex Pentland,Jiaxin Pei*

Key words: AI代理, 自动化谈判, 交易风险, LLM代理, 消费者市场

TL;DR: 论文探讨了AI代理在消费者市场中自动化交易和谈判的潜力及风险，发现不同代理的性能差异显著且可能带来财务损失。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究AI代理在消费者市场中自动化交易和谈判的能力，以及由此带来的风险。

Method: 开发实验框架，评估不同LLM代理在实际谈判和交易中的表现。

Result: AI代理的性能差异显著，可能导致财务损失（如过度支出或不合理交易）。

Conclusion: 自动化虽提升效率，但也引入风险，用户需谨慎授权AI代理做商业决策。

Abstract: AI agents are increasingly used in consumer-facing applications to assist
with tasks such as product search, negotiation, and transaction execution. In
this paper, we explore a future scenario where both consumers and merchants
authorize AI agents to fully automate negotiations and transactions. We aim to
answer two key questions: (1) Do different LLM agents vary in their ability to
secure favorable deals for users? (2) What risks arise from fully automating
deal-making with AI agents in consumer markets? To address these questions, we
develop an experimental framework that evaluates the performance of various LLM
agents in real-world negotiation and transaction settings. Our findings reveal
that AI-mediated deal-making is an inherently imbalanced game -- different
agents achieve significantly different outcomes for their users. Moreover,
behavioral anomalies in LLMs can result in financial losses for both consumers
and merchants, such as overspending or accepting unreasonable deals. These
results underscore that while automation can improve efficiency, it also
introduces substantial risks. Users should exercise caution when delegating
business decisions to AI agents.

</details>


### [476] [Balancing Profit and Fairness in Risk-Based Pricing Markets](https://arxiv.org/abs/2506.00140)
*Jesse Thibodeau,Hadi Nekoei,Afaf Taïk,Janarthanan Rajendran,Golnoosh Farnadi*

Key words: 动态定价, 公平性, 强化学习, 市场监管, 社会福利

TL;DR: 该论文研究了动态风险定价对弱势群体的排斥问题，提出了一种基于强化学习的可解释税收政策来调节市场，提升公平性和社会福利。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 动态风险定价可能系统性地排斥弱势群体，因此需要一种新的监管方法来平衡私人利益与社会目标。

Method: 通过Proposition限制企业的本地人口差距，设计了一个开源的模拟器MarketSim，并训练了一个基于强化学习的社会规划者（SP）来制定公平税收政策。

Result: 在模拟的美国医疗保险和消费信贷市场中，该方法将需求公平性提升了16%，且优于未监管的自由市场和固定线性税收。

Conclusion: AI辅助监管可以将竞争性社会困境转化为双赢均衡，提供了一个公平市场监管的框架。

Abstract: Dynamic, risk-based pricing can systematically exclude vulnerable consumer
groups from essential resources such as health insurance and consumer credit.
We show that a regulator can realign private incentives with social objectives
through a learned, interpretable tax schedule. First, we provide a formal
proposition that bounding each firm's \emph{local} demographic gap implicitly
bounds the \emph{global} opt-out disparity, motivating firm-level penalties.
Building on this insight we introduce \texttt{MarketSim} -- an open-source,
scalable simulator of heterogeneous consumers and profit-maximizing firms --
and train a reinforcement learning (RL) social planner (SP) that selects a
bracketed fairness-tax while remaining close to a simple linear prior via an
$\mathcal{L}_1$ regularizer. The learned policy is thus both transparent and
easily interpretable. In two empirically calibrated markets, i.e., U.S.
health-insurance and consumer-credit, our planner simultaneously raises
demand-fairness by up to $16\%$ relative to unregulated Free Market while
outperforming a fixed linear schedule in terms of social welfare without
explicit coordination. These results illustrate how AI-assisted regulation can
convert a competitive social dilemma into a win-win equilibrium, providing a
principled and practical framework for fairness-aware market oversight.

</details>


### [477] [Utilizing AI for Aviation Post-Accident Analysis Classification](https://arxiv.org/abs/2506.00169)
*Aziida Nanyonga,Graham Wild*

Key words: 航空安全, NLP, 深度学习, 主题建模

TL;DR: 论文探讨了AI和NLP如何自动化分析航空安全报告，提升安全性，并比较了不同深度学习和主题建模技术的表现。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 航空安全报告数据量大，传统分析效率低，需自动化工具提升分析效率和准确性。

Method: 结合NLP和深度学习技术，对航空安全报告进行分类和主题建模，比较不同模型和技术。

Result: NLP、深度学习和主题建模显著提高了分析效率和准确性，有助于更主动的安全管理。

Conclusion: AI技术可有效提升航空安全分析的自动化水平，为风险管理提供支持。

Abstract: The volume of textual data available in aviation safety reports presents a
challenge for timely and accurate analysis. This paper examines how Artificial
Intelligence (AI) and, specifically, Natural Language Processing (NLP) can
automate the process of extracting valuable insights from this data, ultimately
enhancing aviation safety. The paper reviews ongoing efforts focused on the
application of NLP and deep learning to aviation safety reports, with the goal
of classifying the level of damage to an aircraft and identifying the phase of
flight during which safety occurrences happen. Additionally, the paper explores
the use of Topic Modeling (TM) to uncover latent thematic structures within
aviation incident reports, aiming to identify recurring patterns and potential
areas for safety improvement. The paper compares and contrasts the performance
of various deep learning models and TM techniques applied to datasets from the
National Transportation Safety Board (NTSB) and the Australian Transport Safety
Bureau (ATSB), as well as the Aviation Safety Network (ASN), discussing the
impact of dataset size and source on the accuracy of the analysis. The findings
demonstrate that both NLP and deep learning, as well as TM, can significantly
improve the efficiency and accuracy of aviation safety analysis, paving the way
for more proactive safety management and risk mitigation strategies.

</details>


### [478] [Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings](https://arxiv.org/abs/2506.00178)
*Anirudh Nair,Adi Banerjee,Laurent Mombaerts,Matthew Hagen,Tarik Borogovac*

Key words: 提示工程, 大语言模型, 进化优化, 辩论驱动, Elo评分

TL;DR: DEEVO框架通过辩论驱动的进化和Elo评分优化提示工程，解决了现有方法在复杂任务中的局限性，显著优于手动和其他自动化方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 提示工程是发挥大语言模型潜力的关键瓶颈，尤其在主观任务中，现有自动化方法因依赖明确指标或模板而表现不佳。

Method: DEEVO使用辩论驱动评估和Elo评分，通过智能交叉和策略变异操作优化提示，保留语义连贯性并提升多样性。

Result: 实验表明，DEEVO在开放和封闭任务中均显著优于手动和其他优化方法，无需真实反馈即可持续改进。

Conclusion: DEEVO通过结合大语言模型的推理能力和自适应优化，为提示工程研究提供了重要突破。

Abstract: Prompt engineering represents a critical bottleneck to harness the full
potential of Large Language Models (LLMs) for solving complex tasks, as it
requires specialized expertise, significant trial-and-error, and manual
intervention. This challenge is particularly pronounced for tasks involving
subjective quality assessment, where defining explicit optimization objectives
becomes fundamentally problematic. Existing automated prompt optimization
methods falter in these scenarios, as they typically require well-defined
task-specific numerical fitness functions or rely on generic templates that
cannot capture the nuanced requirements of complex use cases. We introduce
DEEVO (DEbate-driven EVOlutionary prompt optimization), a novel framework that
guides prompt evolution through a debate-driven evaluation with an Elo-based
selection. Contrary to prior work, DEEVOs approach enables exploration of the
discrete prompt space while preserving semantic coherence through intelligent
crossover and strategic mutation operations that incorporate debate-based
feedback, combining elements from both successful and unsuccessful prompts
based on identified strengths rather than arbitrary splicing. Using Elo ratings
as a fitness proxy, DEEVO simultaneously drives improvement and preserves
valuable diversity in the prompt population. Experimental results demonstrate
that DEEVO significantly outperforms both manual prompt engineering and
alternative state-of-the-art optimization approaches on open-ended tasks and
close-ended tasks despite using no ground truth feedback. By connecting LLMs
reasoning capabilities with adaptive optimization, DEEVO represents a
significant advancement in prompt optimization research by eliminating the need
of predetermined metrics to continuously improve AI systems.

</details>


### [479] [Control-R: Towards controllable test-time scaling](https://arxiv.org/abs/2506.00189)
*Di Zhang,Weida Wang,Junxian Li,Xunzhi Wang,Jiatong Li,Jianbo Wu,Jingdi Lei,Haonan He,Peng Ye,Shufei Zhang,Wanli Ouyang,Yuqiang Li,Dongzhan Zhou*

Key words: 长链思维推理,推理控制域,条件蒸馏微调,可控推理

TL;DR: 提出了一种新方法（RCF）来解决长链思维推理中的问题，通过结构化的控制信号调整推理努力，并在实验中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决大型推理模型在长链思维推理中出现的过度或不足思考问题。

Method: 引入了推理控制域（RCF）和条件蒸馏微调（CDF）方法，并通过Control-R-4K数据集进行训练。

Result: 在AIME2024和MATH500等基准测试中达到了32B规模的最优性能。

Conclusion: 本文为可控推理提供了一种有效范例。

Abstract: This paper target in addressing the challenges of underthinking and
overthinking in long chain-of-thought (CoT) reasoning for Large Reasoning
Models (LRMs) by introducing Reasoning Control Fields (RCF)--a novel test-time
approach that injects structured control signals to guide reasoning from a tree
search perspective. RCF enables models to adjust reasoning effort according to
given control conditions when solving complex tasks. Additionally, we present
the Control-R-4K dataset, which consists of challenging problems annotated with
detailed reasoning processes and corresponding control fields. To further
enhance reasoning control, we propose a Conditional Distillation Finetuning
(CDF) method, which trains model--particularly Control-R-32B--to effectively
adjust reasoning effort during test time. Experimental results on benchmarks
such as AIME2024 and MATH500 demonstrate that our approach achieves
state-of-the-art performance at the 32B scale while enabling a controllable
Long CoT reasoning process (L-CoT). Overall, this work introduces an effective
paradigm for controllable test-time scaling reasoning.

</details>


### [480] [What do professional software developers need to know to succeed in an age of Artificial Intelligence?](https://arxiv.org/abs/2506.00202)
*Matthew Kam,Cody Miller,Miaoxin Wang,Abey Tidwell,Irene A. Lee,Joyce Malyn-Smith,Beatriz Perez,Vikram Tiwari,Joshua Kenitzer,Andrew Macvean,Erin Barrar*

Key words: 生成式AI, 开发者生产力, 技能培养, 工作流程, AI增强开发

TL;DR: 研究探讨了生成式AI如何提升开发者生产力，同时分析了潜在的工作冲击与技能退化问题，总结了21位开发者的使用经验，并提出了未来技能培养的建议。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探讨生成式AI对软件开发者的影响，包括生产力提升、工作流程变化及技能需求的演变。

Method: 通过研究21位前沿开发者的实践，提炼出12个工作目标、75项任务及相关技能，总结出5条关键洞察。

Result: 发现AI增强型开发者需要四类技能（有效使用生成AI、核心软件工程、相关工程、非工程技能），并提出未来技能培养方向。

Conclusion: 为适应AI时代，需结合“软技能”与技术技能的教育与培训，以应对技能更新与退化风险。

Abstract: Generative AI is showing early evidence of productivity gains for software
developers, but concerns persist regarding workforce disruption and deskilling.
We describe our research with 21 developers at the cutting edge of using AI,
summarizing 12 of their work goals we uncovered, together with 75 associated
tasks and the skills & knowledge for each, illustrating how developers use AI
at work. From all of these, we distilled our findings in the form of 5
insights. We found that the skills & knowledge to be a successful AI-enhanced
developer are organized into four domains (using Generative AI effectively,
core software engineering, adjacent engineering, and adjacent non-engineering)
deployed at critical junctures throughout a 6-step task workflow. In order to
"future proof" developers for this age of AI, on-the-job learning initiatives
and computer science degree programs will need to target both "soft" skills and
the technical skills & knowledge in all four domains to reskill, upskill and
safeguard against deskilling.

</details>


### [481] [Ethical AI: Towards Defining a Collective Evaluation Framework](https://arxiv.org/abs/2506.00233)
*Aasish Kumar Sharma,Dimitar Kyosev,Julian Kunkel*

Key words: 人工智能,伦理评估,本体块,FAIR原则,透明度

TL;DR: 本文提出了一种基于本体块的模块化伦理评估框架，以解决AI领域中的伦理问题，如透明度、公平性和合规性，并通过实际案例验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: AI的快速应用带来了数据所有权、隐私和系统性偏见等伦理问题，亟需透明和可问责的系统。

Method: 设计了基于本体块的道德评估框架，结合FAIR原则，用于动态伦理评估。

Result: 在AI投资者画像的实际用例中，框架成功实现了动态风险分类，表明本体块是可行的路径。

Conclusion: 本体块为可解释和可审计的AI伦理提供了潜力，但自动化和概率推理仍需改进。

Abstract: Artificial Intelligence (AI) is transforming sectors such as healthcare,
finance, and autonomous systems, offering powerful tools for innovation. Yet
its rapid integration raises urgent ethical concerns related to data ownership,
privacy, and systemic bias. Issues like opaque decision-making, misleading
outputs, and unfair treatment in high-stakes domains underscore the need for
transparent and accountable AI systems. This article addresses these challenges
by proposing a modular ethical assessment framework built on ontological blocks
of meaning-discrete, interpretable units that encode ethical principles such as
fairness, accountability, and ownership. By integrating these blocks with FAIR
(Findable, Accessible, Interoperable, Reusable) principles, the framework
supports scalable, transparent, and legally aligned ethical evaluations,
including compliance with the EU AI Act. Using a real-world use case in
AI-powered investor profiling, the paper demonstrates how the framework enables
dynamic, behavior-informed risk classification. The findings suggest that
ontological blocks offer a promising path toward explainable and auditable AI
ethics, though challenges remain in automation and probabilistic reasoning.

</details>


### [482] [SMELLNET: A Large-scale Dataset for Real-world Smell Recognition](https://arxiv.org/abs/2506.00239)
*Dewei Feng,Carol Li,Wei Dai,Paul Pu Liang*

Key words: AI气味识别, SmellNet, 气体传感器, 对比学习, 实时分类

TL;DR: 该论文提出了首个大规模气味数据库SmellNet，用于训练和评估AI的气味识别能力，展示了AI在实时气味分类中的应用和挑战。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前缺乏大规模的气味识别基准数据库，限制了AI在气味分类领域的发展。研究旨在通过SmellNet填补这一空白，推动AI在气味传感中的应用。

Method: 利用便携式气体和化学传感器采集数据，构建包含18万时间步长的SmellNet数据库，结合序列模型、对比学习和高分辨率分子数据训练AI模型。

Result: 最佳模型在预录数据上达到65.35%的准确率，在50类实时分类任务中对坚果和香料的准确率分别为10.71%和25.38%。

Conclusion: SmellNet为AI气味识别提供了重要基础，但仍需解决特征学习、环境适应性等挑战。

Abstract: The ability of AI to sense and identify various substances based on their
smell alone can have profound impacts on allergen detection (e.g., smelling
gluten or peanuts in a cake), monitoring the manufacturing process, and sensing
hormones that indicate emotional states, stress levels, and diseases. Despite
these broad impacts, there are virtually no large scale benchmarks, and
therefore little progress, for training and evaluating AI systems' ability to
smell in the real world. In this paper, we use portable gas and chemical
sensors to create SmellNet, the first large-scale database that digitizes a
diverse range of smells in the natural world. SmellNet contains about 180,000
time steps of 50 substances (spanning nuts, spices, herbs, fruits, and
vegetables) with 50 hours of data. Using SmellNet, we train AI models for
real-time classification of substances based on their smell alone. Our best
methods leverage sequence models, contrastive learning to integrate
high-resolution Gas Chromatography-Mass Spectrometry molecular data, and a new
temporal difference method that identifies sharp changes in sensor readings.
Our best models achieve up to 65.35% accuracy on pre-recorded data, and
generalize to real-world conditions with 10.71% accuracy on nuts and 25.38% on
spices in the challenging 50-way online classification task. Despite these
promising results, SmellNet highlights many technical challenges in building AI
for smell, including richer feature learning, on-edge smell models, and
robustness to environmental changes.

</details>


### [483] [Whispers of Many Shores: Cultural Alignment through Collaborative Cultural Expertise](https://arxiv.org/abs/2506.00242)
*Shuai Feng,Wei-Chuang Chan,Srishti Chouhan,Junior Francisco Garcia Ayala,Srujananjali Medicherla,Kyle Clark,Mingwei Shi*

Key words: 大语言模型, 文化对齐, 软提示微调, 文化敏感性, 动态适应

TL;DR: 提出了一种软提示微调框架，通过模块化方法动态优化LLM的文化对齐，显著提升文化敏感性和适应性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前LLMs在多元文化语境中缺乏细腻理解，全模型微调成本高昂，亟需高效文化对齐方案。

Method: 利用向量化提示调优，动态分配查询至文化专家LLM配置，优化软提示嵌入而不改变基础模型参数。

Result: 文化对齐分数从0.208提升至0.820，显著增强文化敏感性和适应性。

Conclusion: 该框架为文化感知LLM部署提供稳健解决方案，并为后续文化覆盖和动态专家适应研究奠定基础。

Abstract: The integration of large language models (LLMs) into global applications
necessitates effective cultural alignment for meaningful and
culturally-sensitive interactions. Current LLMs often lack the nuanced
understanding required for diverse cultural contexts, and adapting them
typically involves costly full fine-tuning. To address this, we introduce a
novel soft prompt fine-tuning framework that enables efficient and modular
cultural alignment. Our method utilizes vectorized prompt tuning to dynamically
route queries to a committee of culturally specialized 'expert' LLM
configurations, created by optimizing soft prompt embeddings without altering
the base model's parameters. Extensive experiments demonstrate that our
framework significantly enhances cultural sensitivity and adaptability,
improving alignment scores from 0.208 to 0.820, offering a robust solution for
culturally-aware LLM deployment. This research paves the way for subsequent
investigations into enhanced cultural coverage and dynamic expert adaptation,
crucial for realizing autonomous AI with deeply nuanced understanding in a
globally interconnected world.

</details>


### [484] [MIR: Methodology Inspiration Retrieval for Scientific Research Problems](https://arxiv.org/abs/2506.00249)
*Aniketh Garikaparthi,Manasi Patwardhan,Aditya Sanjiv Kanade,Aman Hassan,Lovekesh Vig,Arman Cohan*

Key words: 大型语言模型, 科学发现, 方法灵感检索, 方法论邻接图, 稠密检索器

TL;DR: 该论文提出了一种名为“方法灵感检索”（MIR）的新任务，通过构建方法论邻接图（MAG）和改进检索方法，显著提升了科学发现的自动化效率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有大型语言模型（LLMs）在科学发现中的应用依赖于文献质量，效果参差不齐。为了解决检索与研究问题相关的方法灵感这一问题，提出了MIR任务。

Method: 构建MAG图捕捉方法论传承关系，并将“直觉先验”嵌入稠密检索器中，同时采用LLM重排序策略优化MIR任务。

Result: 结果表明，相比基线方法，MAG提升了Recall@3和mAP指标，LLM重排序进一步优化了性能。

Conclusion: MIR在自动科学发现中展现了潜力，为未来灵感驱动的检索研究提供了方向。

Abstract: There has been a surge of interest in harnessing the reasoning capabilities
of Large Language Models (LLMs) to accelerate scientific discovery. While
existing approaches rely on grounding the discovery process within the relevant
literature, effectiveness varies significantly with the quality and nature of
the retrieved literature. We address the challenge of retrieving prior work
whose concepts can inspire solutions for a given research problem, a task we
define as Methodology Inspiration Retrieval (MIR). We construct a novel dataset
tailored for training and evaluating retrievers on MIR, and establish
baselines. To address MIR, we build the Methodology Adjacency Graph (MAG);
capturing methodological lineage through citation relationships. We leverage
MAG to embed an "intuitive prior" into dense retrievers for identifying
patterns of methodological inspiration beyond superficial semantic similarity.
This achieves significant gains of +5.4 in Recall@3 and +7.8 in Mean Average
Precision (mAP) over strong baselines. Further, we adapt LLM-based re-ranking
strategies to MIR, yielding additional improvements of +4.5 in Recall@3 and
+4.8 in mAP. Through extensive ablation studies and qualitative analyses, we
exhibit the promise of MIR in enhancing automated scientific discovery and
outline avenues for advancing inspiration-driven retrieval.

</details>


### [485] [Hidden in Plain Sight: Probing Implicit Reasoning in Multimodal Language Models](https://arxiv.org/abs/2506.00258)
*Qianqi Yan,Hongquan Li,Shan Jiang,Yang Zhao,Xinze Guan,Ching-Chen Kuo,Xin Eric Wang*

Key words: 多模态大语言模型, 隐含推理, 行为合规性, 开放环境, 可靠性

TL;DR: 该论文分析了多模态大语言模型（MLLMs）在开放环境中处理隐含推理问题的能力，发现其存在潜在能力但缺乏行为合规性，并提出提升策略。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究多模态大语言模型在真实环境（输入信息不完整或不准确）下检测隐含问题的能力。

Method: 通过四个类别的诊断测试，评估六种MLLM模型的性能，并尝试推理时间干预措施。

Result: 模型常忽略隐含问题，但通过提示或提问可显著提升性能。

Conclusion: 当前MLLMs在推理能力与行为合规性间存在差距，需进一步优化以提升可靠性。

Abstract: Multimodal large language models (MLLMs) are increasingly deployed in
open-ended, real-world environments where inputs are messy, underspecified, and
not always trustworthy. Unlike curated benchmarks, these settings frequently
involve instructions that refer to missing objects or contradictory facts, rely
on ambiguous references, or request infeasible actions. In such cases, success
hinges not on task execution alone, but on a model's ability to detect when
something is silently wrong. This paper presents a systematic analysis of how
current MLLMs handle such implicit reasoning scenarios: cases where the flaw is
not explicitly stated but must be inferred from context. Using a curated
diagnostic suite spanning four categories of real-world failure modes, we
evaluate six MLLMs, including o3 and GPT-4o, and find that models frequently
fail to surface hidden issues, even when they possess the necessary perceptual
and reasoning skills. Explicit prompting reveals that the underlying
capabilities exist but are often suppressed in favor of user compliance. We
further show that simple inference-time interventions, such as cautious persona
prompting and, in particular, requiring a clarifying question, can dramatically
recover performance. Our findings highlight a persistent gap between reasoning
competence and behavioral compliance in current MLLMs and suggest practical
strategies for making these models more trustworthy in underconstrained
environments.

</details>


### [486] [Sleep Brain and Cardiac Activity Predict Cognitive Flexibility and Conceptual Reasoning Using Deep Learning](https://arxiv.org/abs/2506.00279)
*Boshra Khajehpiri,Eric Granger,Massimiliano de Zambotti,Fiona C. Baker,Mohamad Forouzanfar*

Key words: 睡眠微观结构,认知功能,深度学习,多模态学习,CogPSGFormer

TL;DR: 研究通过深度学习模型CogPSGFormer，利用多模态睡眠信号预测认知功能表现，特别是在执行功能方面的适应性和概念推理能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索睡眠微观结构与特定认知领域表现之间未被充分研究的联系。

Method: 提出CogPSGFormer模型，整合单通道ECG和EEG信号及提取特征，如EEG功率带和心率变异性参数，进行多尺度特征提取和多模态学习。

Result: 在STAGES数据集的817名个体上，模型在未见过数据的认知表现分类中达到80.3%的准确率。

Conclusion: 多尺度特征提取和多模态学习方法在利用睡眠信号预测认知表现方面具有有效性。

Abstract: Despite extensive research on the relationship between sleep and cognition,
the connection between sleep microstructure and human performance across
specific cognitive domains remains underexplored. This study investigates
whether deep learning models can predict executive functions, particularly
cognitive adaptability and conceptual reasoning from physiological processes
during a night's sleep. To address this, we introduce CogPSGFormer, a
multi-scale convolutional-transformer model designed to process multi-modal
polysomnographic data. This model integrates one-channel ECG and EEG signals
along with extracted features, including EEG power bands and heart rate
variability parameters, to capture complementary information across modalities.
A thorough evaluation of the CogPSGFormer architecture was conducted to
optimize the processing of extended sleep signals and identify the most
effective configuration. The proposed framework was evaluated on 817
individuals from the STAGES dataset using cross-validation. The model achieved
80.3\% accuracy in classifying individuals into low vs. high cognitive
performance groups on unseen data based on Penn Conditional Exclusion Test
(PCET) scores. These findings highlight the effectiveness of our multi-scale
feature extraction and multi-modal learning approach in leveraging
sleep-derived signals for cognitive performance prediction. To facilitate
reproducibility, our code is publicly accessible
(https://github.com/boshrakh95/CogPSGFormer.git).

</details>


### [487] [Evaluation of LLMs for mathematical problem solving](https://arxiv.org/abs/2506.00309)
*Ruonan Wang,Runxi Wang,Yunwen Shen,Chengfeng Wu,Qinglin Zhou,Rohitash Chandra*

Key words: 大型语言模型, 数学问题解决, 结构化思维链, 性能评估, GPT-4o, DeepSeek-V3, Gemini-2.0

TL;DR: 本研究比较了GPT-4o、DeepSeek-V3和Gemini-2.0三种大型语言模型在数学问题解决能力上的表现，发现GPT-4o表现最稳定，DeepSeek-V3在结构化任务中强，而Gemini-2.0语言理解出色但在多步推理中表现较差。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 评估大型语言模型在数学任务中的潜力，填补其在数学问题解决能力研究上的不足。

Method: 采用结构化思维链（SCoT）框架从五个维度评估模型在三个不同复杂度数学数据集（GSM8K、MATH500和UNSW）上的表现。

Result: GPT-4o表现最稳定且在高难度问题中尤为突出；DeepSeek-V3在优化任务中强但统计推理不稳定；Gemini-2.0语言理解强但多步推理弱。

Conclusion: 不同模型在数学任务中各有所长，但均存在特定缺陷，未来需进一步优化。

Abstract: Large Language Models (LLMs) have shown impressive performance on a range of
educational tasks, but are still understudied for their potential to solve
mathematical problems. In this study, we compare three prominent LLMs,
including GPT-4o, DeepSeek-V3, and Gemini-2.0, on three mathematics datasets of
varying complexities (GSM8K, MATH500, and UNSW datasets). We take a
five-dimensional approach based on the Structured Chain-of-Thought (SCoT)
framework to assess final answer correctness, step completeness, step validity,
intermediate calculation accuracy, and problem comprehension. The results show
that GPT-4o is the most stable and consistent in performance across all the
datasets, but particularly it performs outstandingly in high-level questions of
the UNSW dataset. DeepSeek-V3 is competitively strong in well-structured
domains such as optimisation, but suffers from fluctuations in accuracy in
statistical inference tasks. Gemini-2.0 shows strong linguistic understanding
and clarity in well-structured problems but performs poorly in multi-step
reasoning and symbolic logic. Our error analysis reveals particular deficits in
each model: GPT-4o is at times lacking in sufficient explanation or precision;
DeepSeek-V3 leaves out intermediate steps; and Gemini-2.0 is less flexible in
mathematical reasoning in higher dimensions.

</details>


### [488] [Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents](https://arxiv.org/abs/2506.00320)
*Xiao Yu,Baolin Peng,Ruize Xu,Michel Galley,Hao Cheng,Suman Nath,Jianfeng Gao,Zhou Yu*

Key words: AI智能体, 世界模型, 推理框架, Dyna-Think, 模仿学习

TL;DR: 论文提出Dyna-Think框架，通过结合世界模型与推理和行动来提升AI智能体性能，并在实验中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 尽管大型语言模型（如DeepSeek-R1）在数学和编码等领域展示了复杂的推理能力，但尚不清楚哪些行为对长期任务有效。

Method: 提出Dyna-Think框架，采用Dyna-Think Imitation Learning (DIT)和Dyna-Think Dyna Training (DDT)方法，通过世界模型模拟提升智能体性能。

Result: Dyna-Think在OSWorld上的实验中显著提升智能体性能，生成更少的token，同时与R1性能相当。

Conclusion: 将世界模型模拟整合到AI智能体中是提升其推理和规划能力的有效方向。

Abstract: Recent progress in reasoning with large language models (LLMs), such as
DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics
and coding, by exhibiting complex cognitive behaviors such as verification,
goal decomposition, and self-reflection. However, it is unclear what behavior
is effective and what behavior is missing for long-horizon AI agents tasks. In
this work, we propose Dyna-Think, a thinking framework that integrates planning
with an internal world model with reasoning and acting to enhance AI agent
performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning
(DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with
Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing
world model simulation relevant to the proposed (and planned) action, and
trains the policy using this reconstructed data. To enhance Dyna-Think, DDT
uses a two-stage training process to first improve the agent's world modeling
ability via objectives such as state prediction or critique generation, and
then improve the agent's action via policy training. We evaluate our methods on
OSWorld, and demonstrate that Dyna-Think improves the agent's in-domain and
out-of-domain performance, achieving similar best-of-n performance compared to
R1 while generating 2x less tokens on average. Our extensive empirical studies
reveal that 1) using critique generation for world model training is effective
to improve policy performance; and 2) AI agents with better performance
correlate with better world modeling abilities. We believe our results suggest
a promising research direction to integrate world model simulation into AI
agents to enhance their reasoning, planning, and acting capabilities.

</details>


### [489] [BASIL: Best-Action Symbolic Interpretable Learning for Evolving Compact RL Policies](https://arxiv.org/abs/2506.00328)
*Kourosh Shahnazari,Seyed Moein Ayyoubzadeh,Mohammadali Keshtparvar*

Key words: 可解释强化学习、符号策略、进化搜索、质量-多样性优化、基准任务

TL;DR: 论文提出了一种名为BASIL的符号化、可解释的强化学习方法，通过在线进化搜索和质量-多样性优化生成基于规则的可解释策略，其表现与深度强化学习基线相当。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现代深度强化学习策略通常缺乏可解释性，这影响了验证、透明度和人类监督的效能，尤其是在安全关键应用中。因此，需要一种可解释的策略生成方法。

Method: BASIL通过在线进化搜索和质量-多样性（QD）优化生成有序的符号谓词列表作为策略，确保可解释性和复杂度可控。QD存档鼓励行为多样性，复杂度感知的适应度函数促进紧凑表示。

Result: 在CartPole-v1、MountainCar-v0和Acrobot-v1三个基准任务上的实验表明，BASIL能一致地生成与深度强化学习基线性能相当的紧凑、可解释控制器。

Conclusion: BASIL为可解释策略合成提供了一个新方法，结合了符号表达能力、进化多样性和在线学习的统一框架。

Abstract: The quest for interpretable reinforcement learning is a grand challenge for
the deployment of autonomous decision-making systems in safety-critical
applications. Modern deep reinforcement learning approaches, while powerful,
tend to produce opaque policies that compromise verification, reduce
transparency, and impede human oversight. To address this, we introduce BASIL
(Best-Action Symbolic Interpretable Learning), a systematic approach for
generating symbolic, rule-based policies via online evolutionary search with
quality-diversity (QD) optimization. BASIL represents policies as ordered lists
of symbolic predicates over state variables, ensuring full interpretability and
tractable policy complexity. By using a QD archive, the methodology in the
proposed study encourages behavioral and structural diversity between
top-performing solutions, while a complexity-aware fitness encourages the
synthesis of compact representations. The evolutionary system supports the use
of exact constraints for rule count and system adaptability for balancing
transparency with expressiveness. Empirical comparisons with three benchmark
tasks CartPole-v1, MountainCar-v0, and Acrobot-v1 show that BASIL consistently
synthesizes interpretable controllers with compact representations comparable
to deep reinforcement learning baselines. Herein, this article introduces a new
interpretable policy synthesis method that combines symbolic expressiveness,
evolutionary diversity, and online learning through a unifying framework.

</details>


### [490] [Position: Olfaction Standardization is Essential for the Advancement of Embodied Artificial Intelligence](https://arxiv.org/abs/2506.00398)
*Kordel K. France,Rohith Peddi,Nik Dennler,Ovidiu Daescu*

Key words: 人工智能, 嗅觉, 跨学科, 伦理对齐

TL;DR: 论文主张将嗅觉研究纳入AI领域，以弥补其在通用智能和伦理对齐中的缺失。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现代AI系统缺乏对嗅觉的考虑，而嗅觉对记忆、情感等具有重要影响，其缺失会导致AI在人类体验中的不完整性。

Method: 呼吁跨学科合作，包括神经科学、机器人学和机器学习，制定嗅觉基准、开发数据集。

Result: 强调了嗅觉研究对AI系统科学完整性和伦理基础的重要性。

Conclusion: AI社区应重视嗅觉研究，以实现更全面和伦理对齐的智能系统。

Abstract: Despite extraordinary progress in artificial intelligence (AI), modern
systems remain incomplete representations of human cognition. Vision, audition,
and language have received disproportionate attention due to well-defined
benchmarks, standardized datasets, and consensus-driven scientific foundations.
In contrast, olfaction - a high-bandwidth, evolutionarily critical sense - has
been largely overlooked. This omission presents a foundational gap in the
construction of truly embodied and ethically aligned super-human intelligence.
We argue that the exclusion of olfactory perception from AI architectures is
not due to irrelevance but to structural challenges: unresolved scientific
theories of smell, heterogeneous sensor technologies, lack of standardized
olfactory datasets, absence of AI-oriented benchmarks, and difficulty in
evaluating sub-perceptual signal processing. These obstacles have hindered the
development of machine olfaction despite its tight coupling with memory,
emotion, and contextual reasoning in biological systems. In this position
paper, we assert that meaningful progress toward general and embodied
intelligence requires serious investment in olfactory research by the AI
community. We call for cross-disciplinary collaboration - spanning
neuroscience, robotics, machine learning, and ethics - to formalize olfactory
benchmarks, develop multimodal datasets, and define the sensory capabilities
necessary for machines to understand, navigate, and act within human
environments. Recognizing olfaction as a core modality is essential not only
for scientific completeness, but for building AI systems that are ethically
grounded in the full scope of the human experience.

</details>


### [491] [World Models for Cognitive Agents: Transforming Edge Intelligence in Future Networks](https://arxiv.org/abs/2506.00417)
*Changyuan Zhao,Ruichen Zhang,Jiacheng Wang,Gaosheng Zhao,Dusit Niyato,Geng Sun,Shiwen Mao,Dong In Kim*

Key words: 世界模型, 人工智能, 无线网络优化, 强化学习, UAV

TL;DR: 本文综述了世界模型在人工智能中的重要性及其多领域应用，并提出了一种名为Wireless Dreamer的新框架，用于无线边缘智能优化。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 世界模型作为一种新兴范式，能够帮助智能体高效地学习和预测环境动态，尤其是在数据受限或安全关键的场景下。

Method: 通过综述世界模型的架构、训练范式，并提出Wireless Dreamer框架，应用于无线网络优化。

Result: 提出的框架在天气感知的无人机轨迹规划案例中展示了学习效率和决策质量的提升。

Conclusion: 世界模型在智能体决策中具有潜力，Wireless Dreamer框架为无线网络优化提供了有效解决方案。

Abstract: World models are emerging as a transformative paradigm in artificial
intelligence, enabling agents to construct internal representations of their
environments for predictive reasoning, planning, and decision-making. By
learning latent dynamics, world models provide a sample-efficient framework
that is especially valuable in data-constrained or safety-critical scenarios.
In this paper, we present a comprehensive overview of world models,
highlighting their architecture, training paradigms, and applications across
prediction, generation, planning, and causal reasoning. We compare and
distinguish world models from related concepts such as digital twins, the
metaverse, and foundation models, clarifying their unique role as embedded
cognitive engines for autonomous agents. We further propose Wireless Dreamer, a
novel world model-based reinforcement learning framework tailored for wireless
edge intelligence optimization, particularly in low-altitude wireless networks
(LAWNs). Through a weather-aware UAV trajectory planning case study, we
demonstrate the effectiveness of our framework in improving learning efficiency
and decision quality.

</details>


### [492] [MIRROR: Cognitive Inner Monologue Between Conversational Turns for Persistent Reflection and Reasoning in Conversational LLMs](https://arxiv.org/abs/2506.00430)
*Nicole Hsing*

Key words: 内省式思维, 认知架构, 大型语言模型, 多轮对话, 并行推理

TL;DR: MIRROR是一种认知架构，通过模拟人类的内省式思维来提升大型语言模型(LLM)的并行推理能力，显著改善了多轮对话中的安全性、一致性和约束优先级处理。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 人类通过内省式思维处理复杂信息，而当前的LLM在此功能上存在缺陷（如逢迎、注意力缺失和约束优先级不一致）。MIRROR旨在通过模块化认知架构弥补这些缺陷。

Method: MIRROR由Thinker和Talker两层组成：Thinker负责协调目标、推理和记忆等维度的并行线程，并生成连贯的內部分析；Talker基于此分析生成上下文感知的响应。

Result: 在CuRaTe基准测试中，MIRROR使LLM在涉及冲突偏好的安全关键场景中表现提升156%，平均准确率>80%，且在多种模型上平均优于基线21%（15.5个百分点）。

Conclusion: MIRROR通过模拟人类认知的模块化推理，显著提升了LLM在多轮对话中的能力，为认知科学与AI的融合提供了范例。

Abstract: Human intelligence relies on inner monologue to process complex information
through simultaneous reflection, memory retrieval, and response formulation. We
introduce MIRROR (Modular Internal Reasoning, Reflection, Orchestration, and
Response), a cognitive architecture that systematically implements these
parallel reasoning capabilities in large language models. MIRROR operates as a
unified system with two distinct functional layers: the Thinker and the Talker.
The Thinker encompasses: (1) the Inner Monologue Manager, coordinating
reasoning threads across cognitive dimensions (Goals, Reasoning, and Memory);
and (2) the Cognitive Controller, synthesizing these threads into a coherent
internal narrative maintained across conversation turns. The Talker component
then leverages this integrated narrative for context-aware responses. Evaluated
on the CuRaTe benchmark--testing personalized dialogue with safety-critical
constraints, conflicting preferences, and multi-turn consistency--LLMs
utilizing the MIRROR architecture achieve up to 156% relative improvement in
critical safety scenarios involving three persons with conflicting preferences,
maintaining an average accuracy of ~>80% on all scenarios. Across
scenario-specific comparisons, GPT-4o, Gemini 1.5 Pro, Claude 3.7 Sonnet, Llama
4 variants, and Mistral 3 variants with the MIRROR architecture outperformed
baseline models by 21% on average (15.5 percentage points absolute). MIRROR
directly addresses three critical LLM failure modes: sycophancy, attentional
deficits to critical information, and inconsistent prioritization of
conflicting constraints. This work bridges cognitive science and AI by
implementing modular internal reasoning inspired by human cognition, creating a
persistent internal model that significantly enhances multi-turn conversation
capabilities.

</details>


### [493] [Monitoring Robustness and Individual Fairness](https://arxiv.org/abs/2506.00496)
*Ashutosh Gupta,Thomas A. Henzinger,Konstantin Kueffner,Kaushik Mallik,David Pape*

Key words: 输入输出鲁棒性、运行时监控、固定半径最近邻搜索、AI可信度

TL;DR: 提出了运行时监控方法来增强黑盒AI模型的输入输出鲁棒性，通过监测相似输入产生不相似输出的情况。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有AI模型的鲁棒性（如对抗扰动、语义扰动和个体公平性）需要在线监控以补充现有的离线优化方法，提高可信度。

Method: 将监控问题转化为固定半径最近邻（FRNN）搜索问题，并提出工具Clemont，包含多种轻量级监控器，部分基于升级的在线FRNN算法，另一部分基于决策图的新算法。

Result: 通过标准基准测试验证了监控器在运行时有效检测鲁棒性违规的能力。

Conclusion: 运行时监控是一种有效的补充方法，能显著提升AI模型的可信度。

Abstract: Input-output robustness appears in various different forms in the literature,
such as robustness of AI models to adversarial or semantic perturbations and
individual fairness of AI models that make decisions about humans.
  We propose runtime monitoring of input-output robustness of deployed,
black-box AI models, where the goal is to design monitors that would observe
one long execution sequence of the model, and would raise an alarm whenever it
is detected that two similar inputs from the past led to dissimilar outputs.
  This way, monitoring will complement existing offline ``robustification''
approaches to increase the trustworthiness of AI decision-makers.
  We show that the monitoring problem can be cast as the fixed-radius nearest
neighbor (FRNN) search problem, which, despite being well-studied, lacks
suitable online solutions.
  We present our tool Clemont, which offers a number of lightweight monitors,
some of which use upgraded online variants of existing FRNN algorithms, and one
uses a novel algorithm based on binary decision diagrams -- a data-structure
commonly used in software and hardware verification.
  We have also developed an efficient parallelization technique that can
substantially cut down the computation time of monitors for which the distance
between input-output pairs is measured using the $L_\infty$ norm.
  Using standard benchmarks from the literature of adversarial and semantic
robustness and individual fairness, we perform a comparative study of different
monitors in \tool, and demonstrate their effectiveness in correctly detecting
robustness violations at runtime.

</details>


### [494] [CityLens: Benchmarking Large Language-Vision Models for Urban Socioeconomic Sensing](https://arxiv.org/abs/2506.00530)
*Tianhui Liu,Jie Feng,Hetian Pang,Xin Zhang,Tianjian Ouyang,Zhiyuan Zhang,Yong Li*

Key words: 大型语言视觉模型，社会经济指标，卫星图像，街景图像，城市发展

TL;DR: CityLens是一个用于评估大语言视觉模型（LLVMs）从卫星和街景图像预测社会经济指标能力的基准，涵盖17个城市的6个关键领域，并揭示了LLVMs的潜力和局限性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 为了支持可持续城市发展和政策规划，需要理解城市社会经济条件，而视觉数据为此提供了新的视角。

Method: 构建了一个多模态数据集，覆盖17个城市的6个关键领域，定义了11个预测任务，并采用三种评估范式对17个LLVMs进行了测试。

Result: LLVMs表现出有前景的感知和推理能力，但在预测社会经济指标时仍有局限性。

Conclusion: CityLens为诊断LLVMs的局限性提供了统一框架，并指导未来利用LLVMs理解和预测城市社会经济模式。

Abstract: Understanding urban socioeconomic conditions through visual data is a
challenging yet essential task for sustainable urban development and policy
planning. In this work, we introduce $\textbf{CityLens}$, a comprehensive
benchmark designed to evaluate the capabilities of large language-vision models
(LLVMs) in predicting socioeconomic indicators from satellite and street view
imagery. We construct a multi-modal dataset covering a total of 17 globally
distributed cities, spanning 6 key domains: economy, education, crime,
transport, health, and environment, reflecting the multifaceted nature of urban
life. Based on this dataset, we define 11 prediction tasks and utilize three
evaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation,
and Feature-Based Regression. We benchmark 17 state-of-the-art LLVMs across
these tasks. Our results reveal that while LLVMs demonstrate promising
perceptual and reasoning capabilities, they still exhibit limitations in
predicting urban socioeconomic indicators. CityLens provides a unified
framework for diagnosing these limitations and guiding future efforts in using
LLVMs to understand and predict urban socioeconomic patterns. Our codes and
datasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens.

</details>


### [495] [A "Wenlu" Brain System for Multimodal Cognition and Embodied Decision-Making: A Secure New Architecture for Deep Integration of Foundation Models and Domain Knowledge](https://arxiv.org/abs/2506.00570)
*Liang Geng*

Key words: 人工智能，多模态认知，具身决策，私有知识融合，闭环决策

TL;DR: 论文提出了一种名为“文璐”的多模态认知与具身决策脑系统，旨在有效融合私有知识与公共模型，支持多模态数据处理和闭环决策生成硬件级代码。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着AI在各行业的快速渗透，如何在复杂实际应用中整合基础模型的语言理解能力与领域特定知识库成为关键挑战。

Method: 系统引入脑启发的记忆标记与回放机制，融合用户私有数据、行业知识与通用语言模型。

Result: “文璐”在多模态处理、隐私安全、端到端硬件控制代码生成等方面表现优异。

Conclusion: 该系统为构建下一代智能核心奠定了坚实基础。

Abstract: With the rapid penetration of artificial intelligence across industries and
scenarios, a key challenge in building the next-generation intelligent core
lies in effectively integrating the language understanding capabilities of
foundation models with domain-specific knowledge bases in complex real-world
applications. This paper proposes a multimodal cognition and embodied
decision-making brain system, ``Wenlu", designed to enable secure fusion of
private knowledge and public models, unified processing of multimodal data such
as images and speech, and closed-loop decision-making from cognition to
automatic generation of hardware-level code. The system introduces a
brain-inspired memory tagging and replay mechanism, seamlessly integrating
user-private data, industry-specific knowledge, and general-purpose language
models. It provides precise and efficient multimodal services for enterprise
decision support, medical analysis, autonomous driving, robotic control, and
more. Compared with existing solutions, ``Wenlu" demonstrates significant
advantages in multimodal processing, privacy security, end-to-end hardware
control code generation, self-learning, and sustainable updates, thus laying a
solid foundation for constructing the next-generation intelligent core.

</details>


### [496] [Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs](https://arxiv.org/abs/2506.00577)
*Yufa Zhou,Shaobo Wang,Xingyu Dong,Xiangqi Jin,Yifang Chen,Yue Min,Kexin Yang,Xingzhang Ren,Dayiheng Liu,Linfeng Zhang*

Key words: 大型语言模型, 多智能体系统, 经济推理, 监督微调, 强化学习

TL;DR: 该论文探讨了通过监督微调（SFT）和带可验证奖励的强化学习（RLVR）后训练技术，是否能够有效泛化到多智能体系统中，并以经济推理为测试平台，展示了Recon模型在结构化推理和经济合理性上的显著提升。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 由于复杂的奖励建模、动态智能体交互和高泛化需求，直接训练大型语言模型（LLMs）用于多智能体系统（MAS）具有挑战性。因此，研究后训练技术在MAS中的有效性成为一个关键问题。

Method: 研究采用监督微调（SFT）和带可验证奖励的强化学习（RLVR）作为后训练技术，构建了名为Recon的7B参数开源LLM，并在2100个高质量经济推理问题的数据集上进行训练。

Result: 在经济学推理基准和多智能体游戏的全面评估中，Recon模型显示出在结构化推理和经济合理性方面的显著改进。

Conclusion: 研究表明，领域对齐的后训练技术能够有效提升模型的推理能力和智能体对齐性，同时揭示了SFT和RL在塑造模型行为中的重要作用。

Abstract: Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS)
remains challenging due to intricate reward modeling, dynamic agent
interactions, and demanding generalization requirements. This paper explores
whether post-training techniques, specifically Supervised Fine-Tuning (SFT) and
Reinforcement Learning with Verifiable Rewards (RLVR), can effectively
$\textit{generalize}$ to multi-agent scenarios. We use economic reasoning as a
testbed, leveraging its strong foundations in mathematics and game theory, its
demand for structured analytical reasoning, and its relevance to real-world
applications such as market design, resource allocation, and policy analysis.
We introduce $\textbf{Recon}$ ($\textbf{R}$easoning like an
$\textbf{ECON}$omist), a 7B-parameter open-source LLM post-trained on a
hand-curated dataset of 2,100 high-quality economic reasoning problems.
Comprehensive evaluation on economic reasoning benchmarks and multi-agent games
reveals clear improvements in structured reasoning and economic rationality.
These results underscore the promise of domain-aligned post-training for
enhancing reasoning and agent alignment, shedding light on the roles of SFT and
RL in shaping model behavior. Code is available at
https://github.com/MasterZhou1/Recon .

</details>


### [497] [Do Language Models Mirror Human Confidence? Exploring Psychological Insights to Address Overconfidence in LLMs](https://arxiv.org/abs/2506.00582)
*Chenjun Xu,Bingbing Wen,Bin Han,Robert Wolfe,Lucy Lu Wang,Bill Howe*

Key words: LLMs, 自信度估计, AFCE, 过自信, QA任务

TL;DR: 论文研究了LLMs在QA任务中的自信度估计问题，发现其与人类不同且存在刻板偏见，提出AFCE方法改善校准。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 人类在任务中常表现出不准确的自信度估计，论文探索LLMs在这一行为上的差异及其偏见问题。

Method: 采用AFCE方法，通过两阶段提示（先估计自信度再回答问题），减少过自信并提高对任务难度的敏感性。

Result: AFCE在MMLU和GPQA数据集上显著减少了过自信，且更接近人类的自信度估计模式。

Conclusion: AFCE能有效改善LLMs的自信度校准和可解释性，尤其是在处理不同任务难度和人物设定时。

Abstract: Psychology research has shown that humans are poor at estimating their
performance on tasks, tending towards underconfidence on easy tasks and
overconfidence on difficult tasks. We examine three LLMs, Llama-3-70B-instruct,
Claude-3-Sonnet, and GPT-4o, on a range of QA tasks of varying difficulty, and
show that models exhibit subtle differences from human patterns of
overconfidence: less sensitive to task difficulty, and when prompted to answer
based on different personas -- e.g., expert vs layman, or different race,
gender, and ages -- the models will respond with stereotypically biased
confidence estimations even though their underlying answer accuracy remains the
same. Based on these observations, we propose Answer-Free Confidence Estimation
(AFCE) to improve confidence calibration and LLM interpretability in these
settings. AFCE is a self-assessment method that employs two stages of
prompting, first eliciting only confidence scores on questions, then asking
separately for the answer. Experiments on the MMLU and GPQA datasets spanning
subjects and difficulty show that this separation of tasks significantly
reduces overconfidence and delivers more human-like sensitivity to task
difficulty.

</details>


### [498] [RiOSWorld: Benchmarking the Risk of Multimodal Compter-Use Agents](https://arxiv.org/abs/2506.00618)
*Jingyi Yang,Shuai Shao,Dongrui Liu,Jing Shao*

Key words: 多模态大型语言模型, 安全风险, 计算机使用代理, RIOSWorld基准

TL;DR: 本文介绍了RIOSWorld基准，用于评估多模态大型语言模型（MLLM）在现实计算机操作中的安全风险，发现现有代理面临重大风险，强调了安全对齐的必要性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究现实世界中基于MLLM的计算机使用代理的安全风险，填补现有研究在交互环境和风险类型多样性上的不足。

Method: 设计RIOSWorld基准，包含492个涵盖多类计算机应用的险任务，并将风险分为用户源风险和环境风险。从风险意图和完成度两个角度评估安全风险。

Result: 实验表明当前计算机使用代理在现实场景中面临显著安全风险，强调了安全对齐的紧迫性。

Conclusion: RIOSWorld为开发可信的计算机使用代理提供了重要见解，表明需加强安全对齐。

Abstract: With the rapid development of multimodal large language models (MLLMs), they
are increasingly deployed as autonomous computer-use agents capable of
accomplishing complex computer tasks. However, a pressing issue arises: Can the
safety risk principles designed and aligned for general MLLMs in dialogue
scenarios be effectively transferred to real-world computer-use scenarios?
Existing research on evaluating the safety risks of MLLM-based computer-use
agents suffers from several limitations: it either lacks realistic interactive
environments, or narrowly focuses on one or a few specific risk types. These
limitations ignore the complexity, variability, and diversity of real-world
environments, thereby restricting comprehensive risk evaluation for
computer-use agents. To this end, we introduce \textbf{RiOSWorld}, a benchmark
designed to evaluate the potential risks of MLLM-based agents during real-world
computer manipulations. Our benchmark includes 492 risky tasks spanning various
computer applications, involving web, social media, multimedia, os, email, and
office software. We categorize these risks into two major classes based on
their risk source: (i) User-originated risks and (ii) Environmental risks. For
the evaluation, we evaluate safety risks from two perspectives: (i) Risk goal
intention and (ii) Risk goal completion. Extensive experiments with multimodal
agents on \textbf{RiOSWorld} demonstrate that current computer-use agents
confront significant safety risks in real-world scenarios. Our findings
highlight the necessity and urgency of safety alignment for computer-use agents
in real-world computer manipulation, providing valuable insights for developing
trustworthy computer-use agents. Our benchmark is publicly available at
https://yjyddq.github.io/RiOSWorld.github.io/.

</details>


### [499] [AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents](https://arxiv.org/abs/2506.00641)
*Hanjun Luo,Shenyu Dai,Chiming Ni,Xinfeng Li,Guibin Zhang,Kun Wang,Tongliang Liu,Hanan Salam*

Key words: LLM评估、安全与安全、记忆增强推理、基准测试

TL;DR: 提出了一种名为ackslash sys的通用、免训练、记忆增强推理框架，结合新的基准ackslash data，显著提升LLM评估者在安全和安全风险检测上的表现，达到人类水平。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有基于规则或LLM的评估方法在检测逐步动作中的危险、细微含义、小问题累积及模糊规则时存在不足，亟需更可靠的评估框架。

Method: ackslash sys通过构建经验记忆库，动态检索相关推理经验指导评估；同时开发首个针对安全与安全风险的基准ackslash data，包含2293条标注记录。

Result: 实验显示，ackslash sys在所有基准上均提升了LLM的评估性能，并达到了人类水平的准确性。

Conclusion: ackslash sys为LLM评估者提供了一种高效、可靠的框架，填补了现有评估方法的不足。

Abstract: Despite the rapid advancement of LLM-based agents, the reliable evaluation of
their safety and security remains a significant challenge. Existing rule-based
or LLM-based evaluators often miss dangers in agents' step-by-step actions,
overlook subtle meanings, fail to see how small issues compound, and get
confused by unclear safety or security rules. To overcome this evaluation
crisis, we introduce \sys, a universal, training-free, memory-augmented
reasoning framework that empowers LLM evaluators to emulate human expert
evaluators. \sys constructs an experiential memory by having an LLM adaptively
extract structured semantic features (e.g., scenario, risk, behavior) and
generate associated chain-of-thought reasoning traces for past interactions. A
multi-stage, context-aware retrieval-augmented generation process then
dynamically retrieves the most relevant reasoning experiences to guide the LLM
evaluator's assessment of new cases. Moreover, we developed \data, the first
benchmark designed to check how well LLM-based evaluators can spot both safety
risks and security threats. \data comprises \textbf{2293} meticulously
annotated interaction records, covering \textbf{15} risk types across
\textbf{29} application scenarios. A key feature of \data is its nuanced
approach to ambiguous risk situations, employing ``Strict'' and ``Lenient''
judgment standards. Experiments demonstrate that \sys not only consistently
improves the evaluation performance of LLMs across all benchmarks but also sets
a new state-of-the-art in LLM-as-a-judge for agent safety and security,
achieving human-level accuracy. Our work is openly openly accessible.

</details>


### [500] [OntoRAG: Enhancing Question-Answering through Automated Ontology Derivation from Unstructured Knowledge Bases](https://arxiv.org/abs/2506.00664)
*Yash Tiwari,Owais Ahmad Lone,Mayukha Pal*

Key words: OntoRAG, 本体构建, 大型语言模型, 知识图谱, 问答系统

TL;DR: OntoRAG是一种自动化流程，用于从非结构化知识库中构建本体，特别针对电气继电器文档，结合多种技术提升问答系统的性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统本体构建依赖人工，耗时长且易出错，OntoRAG旨在解决这一问题，实现自动化本体创建。

Method: OntoRAG整合了网络爬取、PDF解析、混合分块、信息提取、知识图构建和本体创建等技术。

Result: 实验显示OntoRAG在全面性和多样性上优于传统RAG和GraphRAG，全面性胜率分别为85%和75%。

Conclusion: OntoRAG推动了语义网的愿景，解决了本体自动化创建的关键挑战。

Abstract: Ontologies are pivotal for structuring knowledge bases to enhance question
answering (QA) systems powered by Large Language Models (LLMs). However,
traditional ontology creation relies on manual efforts by domain experts, a
process that is time intensive, error prone, and impractical for large, dynamic
knowledge domains. This paper introduces OntoRAG, an automated pipeline
designed to derive ontologies from unstructured knowledge bases, with a focus
on electrical relay documents. OntoRAG integrates advanced techniques,
including web scraping, PDF parsing, hybrid chunking, information extraction,
knowledge graph construction, and ontology creation, to transform unstructured
data into a queryable ontology. By leveraging LLMs and graph based methods,
OntoRAG enhances global sensemaking capabilities, outperforming conventional
Retrieval Augmented Generation (RAG) and GraphRAG approaches in
comprehensiveness and diversity. Experimental results demonstrate OntoRAGs
effectiveness, achieving a comprehensiveness win rate of 85% against vector RAG
and 75% against GraphRAGs best configuration. This work addresses the critical
challenge of automating ontology creation, advancing the vision of the semantic
web.

</details>


### [501] [DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains](https://arxiv.org/abs/2506.00708)
*Yongkang Xiao,Sinian Zhang,Yi Dai,Huixue Zhou,Jue Hou,Jie Ding,Rui Zhang*

Key words: 知识图谱补全,LLMs,子图检索,结构嵌入,逻辑规则,GCN

TL;DR: DrKGC通过动态子图检索增强LLMs，结合结构嵌入和逻辑规则，提升知识图谱补全性能，并在实验中优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有方法未能充分利用LLMs在图结构感知和推理上的潜力，因此提出了DrKGC。

Method: DrKGC结合轻量级模型训练策略学习结构嵌入和逻辑规则，采用自底向上的子图检索方法，并通过GCN适配器增强LLMs的提示。

Result: 在通用和生物医学领域的基准数据集上，DrKGC表现出卓越性能，并通过案例研究验证了其可解释性和实用性。

Conclusion: DrKGC通过动态子图检索和结构嵌入的结合，显著提升了知识图谱补全的效果和解释性。

Abstract: Knowledge graph completion (KGC) aims to predict missing triples in knowledge
graphs (KGs) by leveraging existing triples and textual information. Recently,
generative large language models (LLMs) have been increasingly employed for
graph tasks. However, current approaches typically encode graph context in
textual form, which fails to fully exploit the potential of LLMs for perceiving
and reasoning about graph structures. To address this limitation, we propose
DrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph
Completion). DrKGC employs a flexible lightweight model training strategy to
learn structural embeddings and logical rules within the KG. It then leverages
a novel bottom-up graph retrieval method to extract a subgraph for each query
guided by the learned rules. Finally, a graph convolutional network (GCN)
adapter uses the retrieved subgraph to enhance the structural embeddings, which
are then integrated into the prompt for effective LLM fine-tuning. Experimental
results on two general domain benchmark datasets and two biomedical datasets
demonstrate the superior performance of DrKGC. Furthermore, a realistic case
study in the biomedical domain highlights its interpretability and practical
utility.

</details>


### [502] [Alignment Revisited: Are Large Language Models Consistent in Stated and Revealed Preferences?](https://arxiv.org/abs/2506.00751)
*Zhuojun Gu,Quan Wang,Shuchu Han*

Key words: 大型语言模型,偏好偏差,伦理部署,可解释性

TL;DR: 研究探讨了大型语言模型（LLM）公开宣称的偏好与在具体情境中表现出的偏好之间的偏差问题，并提出了一种测量方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 为评估LLM的可解释性、可信度和伦理部署能力，尤其是在高风险应用中。

Method: 通过设计丰富的提示数据集作为强制二元选择，比较LLM对一般原则和情境化提示的响应，用KL散度量化偏差。

Result: 发现轻微提示格式变化可导致偏好改变，偏差现象普遍存在。

Conclusion: 研究对LLM集成到服务中至关重要，尤其是在需要道德和社会责任的场景。

Abstract: Recent advances in Large Language Models (LLMs) highlight the need to align
their behaviors with human values. A critical, yet understudied, issue is the
potential divergence between an LLM's stated preferences (its reported
alignment with general principles) and its revealed preferences (inferred from
decisions in contextualized scenarios). Such deviations raise fundamental
concerns for the interpretability, trustworthiness, reasoning transparency, and
ethical deployment of LLMs, particularly in high-stakes applications. This work
formally defines and proposes a method to measure this preference deviation. We
investigate how LLMs may activate different guiding principles in specific
contexts, leading to choices that diverge from previously stated general
principles. Our approach involves crafting a rich dataset of well-designed
prompts as a series of forced binary choices and presenting them to LLMs. We
compare LLM responses to general principle prompts stated preference with LLM
responses to contextualized prompts revealed preference, using metrics like KL
divergence to quantify the deviation. We repeat the analysis across different
categories of preferences and on four mainstream LLMs and find that a minor
change in prompt format can often pivot the preferred choice regardless of the
preference categories and LLMs in the test. This prevalent phenomenon
highlights the lack of understanding and control of the LLM decision-making
competence. Our study will be crucial for integrating LLMs into services,
especially those that interact directly with humans, where morality, fairness,
and social responsibilities are crucial dimensions. Furthermore, identifying or
being aware of such deviation will be critically important as LLMs are
increasingly envisioned for autonomous agentic tasks where continuous human
evaluation of all LLMs' intermediary decision-making steps is impossible.

</details>


### [503] [HouseTS: A Large-Scale, Multimodal Spatiotemporal U.S. Housing Dataset](https://arxiv.org/abs/2506.00765)
*Shengkun Wang,Yanshen Sun,Fanglan Chen,Linhan Wang,Naren Ramakrishnan,Chang-Tien Lu,Yinlin Chen*

Key words: 房价预测, 多模态数据集, 长期预测, HouseTS, 标准化基准

TL;DR: HouseTS是一个大规模、多模态数据集，用于长期房价预测，包含丰富的时空数据和上下文信息，并提供了标准化性能基准。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 由于缺乏具有足够时空深度和上下文丰富性的可复现基准，研究者开发了HouseTS数据集以填补这一空白。

Method: 数据集包含890K条记录，涵盖2012年至2023年30个美国大都市区的房价数据，并整合了POI、社会经济指标等。评估了14种模型。

Result: 通过多模态案例研究展示了HouseTS的价值，并公开了数据集、预处理管道和代码以支持复现。

Conclusion: HouseTS为房价预测研究提供了标准化基准和多模态数据支持，促进可复现性和易用性。

Abstract: Accurate house-price forecasting is essential for investors, planners, and
researchers. However, reproducible benchmarks with sufficient spatiotemporal
depth and contextual richness for long horizon prediction remain scarce. To
address this, we introduce HouseTS a large scale, multimodal dataset covering
monthly house prices from March 2012 to December 2023 across 6,000 ZIP codes in
30 major U.S. metropolitan areas. The dataset includes over 890K records,
enriched with points of Interest (POI), socioeconomic indicators, and detailed
real estate metrics. To establish standardized performance baselines, we
evaluate 14 models, spanning classical statistical approaches, deep neural
networks (DNNs), and pretrained time-series foundation models. We further
demonstrate the value of HouseTS in a multimodal case study, where a vision
language model extracts structured textual descriptions of geographic change
from time stamped satellite imagery. This enables interpretable, grounded
insights into urban evolution. HouseTS is hosted on Kaggle, while all
preprocessing pipelines, benchmark code, and documentation are openly
maintained on GitHub to ensure full reproducibility and easy adoption.

</details>


### [504] [Do not Abstain! Identify and Solve the Uncertainty](https://arxiv.org/abs/2506.00780)
*Jingyu Liu,Jingquan Peng,xiaopeng Wu,Xubin Li,Tiezheng Ge,Bo Zheng,Yong Liu*

Key words: 大型语言模型（LLM）；不确定性；ConfuseBench；InteractDPO；能力限制

TL;DR: 论文研究了大型语言模型（LLM）在面对不确定场景时的过度自信问题，提出了ConfuseBench基准以系统性评估和改进LLM识别与解决不确定性的能力，并提出了InteractDPO方法来提升效果。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: LLM在处理不确定场景时常表现出过度自信，现有解决方案主要依赖回避回应，忽略了识别和解决不确定性的机会。

Method: 引入ConfuseBench基准，专注于文档稀缺性、能力限制和查询歧义三种不确定性，并提出InteractDPO方法来生成上下文感知的质疑与判断不确定性来源。

Result: 实验表明当前LLM难以准确识别不确定性根源，尤其是较弱模型更倾向于归因于查询歧义。提出的方法有效改善了这一点。

Conclusion: 研究通过系统性评估和改进LLM识别与解决不确定性的能力，展示了InteractDPO方法的有效性。

Abstract: Despite the widespread application of Large Language Models (LLMs) across
various domains, they frequently exhibit overconfidence when encountering
uncertain scenarios, yet existing solutions primarily rely on evasive responses
(e.g., "I don't know") overlooks the opportunity of identifying and addressing
the uncertainty to generate more satisfactory responses. To systematically
investigate and improve LLMs' ability of recognizing and addressing the source
of uncertainty, we introduce \textbf{ConfuseBench}, a benchmark mainly focus on
three types of uncertainty: document scarcity, limited capability, and query
ambiguity. Experiments with ConfuseBench reveal that current LLMs struggle to
accurately identify the root cause of uncertainty and solve it. They prefer to
attribute uncertainty to query ambiguity while overlooking capability
limitations, especially for those weaker models. To tackle this challenge, we
first generate context-aware inquiries that highlight the confusing aspect of
the original query. Then we judge the source of uncertainty based on the
uniqueness of the inquiry's answer. Further we use an on-policy training
method, InteractDPO to generate better inquiries. Experimental results
demonstrate the efficacy of our approach.

</details>


### [505] [CoP: Agentic Red-teaming for Large Language Models using Composition of Principles](https://arxiv.org/abs/2506.00781)
*Chen Xiong,Pin-Yu Chen,Tsung-Yi Ho*

Key words: 大语言模型,红队测试,安全对齐,越狱攻击,CoP框架

TL;DR: 提出了一个基于CoP框架的自动化红队测试方法，用于发现LLM的安全漏洞。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决现有红队测试方法无法统一和扩展的问题，以自动化和规模化地发现LLM的安全风险。

Method: 使用Composition-of-Principles (CoP)框架，通过人类提供的原则指导AI代理自动生成红队测试策略和越狱提示。

Result: 在领先的LLM上测试，CoP发现了新的越狱提示，并将单次攻击成功率提高了19倍。

Conclusion: CoP框架能够高效发现LLM的安全漏洞，为红队测试提供了一种可扩展的方法。

Abstract: Recent advances in Large Language Models (LLMs) have spurred transformative
applications in various domains, ranging from open-source to proprietary LLMs.
However, jailbreak attacks, which aim to break safety alignment and user
compliance by tricking the target LLMs into answering harmful and risky
responses, are becoming an urgent concern. The practice of red-teaming for LLMs
is to proactively explore potential risks and error-prone instances before the
release of frontier AI technology. This paper proposes an agentic workflow to
automate and scale the red-teaming process of LLMs through the
Composition-of-Principles (CoP) framework, where human users provide a set of
red-teaming principles as instructions to an AI agent to automatically
orchestrate effective red-teaming strategies and generate jailbreak prompts.
Distinct from existing red-teaming methods, our CoP framework provides a
unified and extensible framework to encompass and orchestrate human-provided
red-teaming principles to enable the automated discovery of new red-teaming
strategies. When tested against leading LLMs, CoP reveals unprecedented safety
risks by finding novel jailbreak prompts and improving the best-known
single-turn attack success rate by up to 19.0 times.

</details>


### [506] [Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via Reinforcement Learning](https://arxiv.org/abs/2506.00782)
*Weiyang Guo,Zesheng Shi,Zhuo Li,Yequan Wang,Xuebo Liu,Wenya Wang,Fangming Liu,Min Zhang,Jing Li*

Key words: 大语言模型, 自动红队, 强化学习, 攻击提示, 多样性, 有效性

TL;DR: 提出了一种基于强化学习的自动红队训练框架，用于生成多样化且有效的攻击提示，以提升大语言模型的安全性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着大语言模型（LLMs）的影响力和能力增长，确保其安全性和防止有害输出变得至关重要。现有方法难以平衡攻击提示的有效性和多样性。

Method: 提出了一个三阶段训练框架：冷启动（监督学习）、热身探索（使用多样性和一致性作为奖励信号）、强化越狱（渐进式奖励提升）。

Result: 实验表明，该方法在多样性和有效性上优于现有方法，显著提升了红队探索效率。

Conclusion: 该框架为自动红队测试提供了新视角，有效平衡了攻击提示的多样性和效果。

Abstract: As large language models (LLMs) grow in power and influence, ensuring their
safety and preventing harmful output becomes critical. Automated red teaming
serves as a tool to detect security vulnerabilities in LLMs without manual
labor. However, most existing methods struggle to balance the effectiveness and
diversity of red-team generated attack prompts. To address this challenge, we
propose \ourapproach, a novel automated red teaming training framework that
utilizes reinforcement learning to explore and generate more effective attack
prompts while balancing their diversity. Specifically, it consists of three
training stages: (1) Cold Start: The red team model is supervised and
fine-tuned on a jailbreak dataset obtained through imitation learning. (2)
Warm-up Exploration: The model is trained in jailbreak instruction following
and exploration, using diversity and consistency as reward signals. (3)
Enhanced Jailbreak: Progressive jailbreak rewards are introduced to gradually
enhance the jailbreak performance of the red-team model. Extensive experiments
on a variety of LLMs show that \ourapproach effectively balances the diversity
and effectiveness of jailbreak prompts compared to existing methods. Our work
significantly improves the efficiency of red team exploration and provides a
new perspective on automated red teaming.

</details>


### [507] [GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning](https://arxiv.org/abs/2506.00785)
*Sahiti Yerramilli,Nilay Pande,Rynaa Grover,Jayant Sravan Tamarapalli*

Key words: GeoChain、多模态大语言模型、地理推理、基准测试、视觉定位

TL;DR: GeoChain是一个用于评估多模态大语言模型地理推理能力的大规模基准，包含146万张街景图像和超过3000万问答对，揭示了当前模型在视觉定位和复杂推理上的挑战。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前多模态大语言模型在地理推理方面存在局限性，GeoChain旨在通过大规模基准和分步问题序列诊断模型的弱点。

Method: 利用146万张Mapillary街景图像，每张图像配以21步链式思考问题序列，涵盖视觉、空间、文化和精确定位四类推理，并标注难度。

Result: 测试表明，现有模型（如GPT-4.1、Claude 3.7等）在视觉基础、连贯推理和精确定位方面表现不佳，尤其是复杂度高时。

Conclusion: GeoChain为提升多模态大语言模型的地理推理能力提供了重要诊断工具。

Abstract: This paper introduces GeoChain, a large-scale benchmark for evaluating
step-by-step geographic reasoning in multimodal large language models (MLLMs).
Leveraging 1.46 million Mapillary street-level images, GeoChain pairs each
image with a 21-step chain-of-thought (CoT) question sequence (over 30 million
Q&A pairs). These sequences guide models from coarse attributes to fine-grained
localization across four reasoning categories - visual, spatial, cultural, and
precise geolocation - annotated by difficulty. Images are also enriched with
semantic segmentation (150 classes) and a visual locatability score. Our
benchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5
variants) on a diverse 2,088-image subset reveals consistent challenges: models
frequently exhibit weaknesses in visual grounding, display erratic reasoning,
and struggle to achieve accurate localization, especially as the reasoning
complexity escalates. GeoChain offers a robust diagnostic methodology, critical
for fostering significant advancements in complex geographic reasoning within
MLLMs.

</details>


### [508] [Predicting Empirical AI Research Outcomes with Language Models](https://arxiv.org/abs/2506.00794)
*Jiaxin Wen,Chenglei Si,Yueh-han Chen,He He,Shi Feng*

Key words: AI研究预测, 语言模型, 基准测试, 人类专家对比, 检索增强

TL;DR: 论文构建了首个预测AI研究想法成功率的基准测试，结合微调GPT-4.1与论文检索代理，系统表现优于人类专家。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 为了加速AI实证研究，预测研究想法的成功概率是关键，但目前验证过程耗费大量人力和计算资源。

Method: 通过爬取会议论文中的想法和实验结果，构建1,585对验证数据集和6,000对训练数据集，结合微调GPT-4.1与检索代理的系统与25位人类专家对比。

Result: 在NLP领域，系统以64.4%的准确率远超人类的48.9%；整体测试集上达77%准确率，而前沿语言模型表现仅与随机猜测相当。系统还通过了鲁棒性测试，并在未发表的新想法上表现优异（63.6%）。

Conclusion: 研究表明，语言模型在加速AI实证研究方面具有潜力，可作为奖励模型提升想法生成模型。

Abstract: Many promising-looking ideas in AI research fail to deliver, but their
validation takes substantial human labor and compute. Predicting an idea's
chance of success is thus crucial for accelerating empirical AI research, a
skill that even expert researchers can only acquire through substantial
experience. We build the first benchmark for this task and compare LMs with
human experts. Concretely, given two research ideas (e.g., two jailbreaking
methods), we aim to predict which will perform better on a set of benchmarks.
We scrape ideas and experimental results from conference papers, yielding 1,585
human-verified idea pairs published after our base model's cut-off date for
testing, and 6,000 pairs for training. We then develop a system that combines a
fine-tuned GPT-4.1 with a paper retrieval agent, and we recruit 25 human
experts to compare with. In the NLP domain, our system beats human experts by a
large margin (64.4% v.s. 48.9%). On the full test set, our system achieves 77%
accuracy, while off-the-shelf frontier LMs like o3 perform no better than
random guessing, even with the same retrieval augmentation. We verify that our
system does not exploit superficial features like idea complexity through
extensive human-written and LM-designed robustness tests. Finally, we evaluate
our system on unpublished novel ideas, including ideas generated by an AI
ideation agent. Our system achieves 63.6% accuracy, demonstrating its potential
as a reward model for improving idea generation models. Altogether, our results
outline a promising new direction for LMs to accelerate empirical AI research.

</details>


### [509] [Enhancing LLM Reasoning for Time Series Classification by Tailored Thinking and Fused Decision](https://arxiv.org/abs/2506.00807)
*Jiahui Zhou,Dan Li,Lin Li,Zhuomin Chen,Shunyu Wu,Haozheng Ye,Jian Lou,Costas J. Spanos*

Key words: 大型语言模型, 时间序列分类, 多轮推理, 融合决策, 结构化推理

TL;DR: 论文提出了一种名为ReasonTSC的新框架，通过多轮推理和融合决策策略，有效利用大型语言模型（LLM）推理能力提升时间序列分类（TSC）任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 尽管LLM在文本领域的推理能力表现出色，但其在时间序列领域的直接应用效果有限。时间序列分类任务在实际应用中普遍且重要，但LLM推理在此方面的潜力尚未充分挖掘。

Method: ReasonTSC首先让LLM思考时间序列数据的关键特征，然后整合插件分类器的预测和置信度作为上下文示例，最后通过结构化推理过程（评估、回溯和比较）得出最终分类。

Result: 实验表明，ReasonTSC在时间序列分类任务中优于现有基准方法和插件模型，并能识别和纠正插件模型的错误预测。

Conclusion: ReasonTSC通过结合LLM的推理能力和领域特定模型的优势，为时间序列分类任务提供了高效、灵活的解决方案。

Abstract: The reasoning capabilities of large language models (LLMs) have significantly
advanced their performance by enabling in-depth understanding of diverse tasks.
With growing interest in applying LLMs to the time series domain, this has
proven nontrivial, as evidenced by the limited efficacy of straightforwardly
adapting text-domain reasoning techniques. Although recent work has shown
promise in several time series tasks, further leveraging advancements in LLM
reasoning remains under-explored for time series classification (TSC) tasks,
despite their prevalence and significance in many real-world applications. In
this paper, we propose ReasonTSC, a novel framework designed to effectively
leverage LLM reasoning for time series classification through both a multi-turn
reasoning and a fused decision-making strategy tailored to TSC. Rather than
straightforwardly applying existing reasoning techniques or relying solely on
LLMs' built-in reasoning capabilities, ReasonTSC first steers the model to
think over the essential characteristics of time series data. Next, it
integrates predictions and confidence scores from plug-in classifiers, e.g.,
domain-specific time series models, as in-context examples. Finally, ReasonTSC
guides the LLM through a structured reasoning process: it evaluates the initial
assessment, backtracks to consider alternative hypotheses, and compares their
merits before arriving at a final classification. Extensive experiments and
systematic ablation studies demonstrate that ReasonTSC consistently outperforms
both existing time series reasoning baselines and plug-in models, and is even
capable of identifying and correcting plug-in models' false predictions.

</details>


### [510] [SynPO: Synergizing Descriptiveness and Preference Optimization for Video Detailed Captioning](https://arxiv.org/abs/2506.00835)
*Jisheng Dang,Yizhou Zhang,Hao Ye,Teng Wang,Siming Chen,Huicheng Zheng,Yulan Guo,Jianhuang Lai,Bin Hu*

Key words: 细粒度视频描述, 偏好学习, 协同优化, 视觉语言模型

TL;DR: 该论文提出了一种基于偏好学习的细粒度视频描述方法SynPO，通过构造高质量偏好对和优化方法，提升了模型性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有方法难以捕捉视频中的细微动态和丰富细节信息，因此论文提出利用偏好学习提升细粒度视频描述的性能，并克服直接偏好优化的局限性。

Method: 论文提出：1）利用视觉语言模型和大型语言模型构建偏好对的流水线；2）协同偏好优化（SynPO）方法，避免负偏好主导优化并保留语言能力。

Result: SynPO在视频描述基准测试和NLP任务中均优于直接偏好优化变体，训练效率提升20%。

Conclusion: SynPO为细粒度视频描述提供了一种高效的优化方法，显著优于现有技术。

Abstract: Fine-grained video captioning aims to generate detailed, temporally coherent
descriptions of video content. However, existing methods struggle to capture
subtle video dynamics and rich detailed information. In this paper, we leverage
preference learning to enhance the performance of vision-language models in
fine-grained video captioning, while mitigating several limitations inherent to
direct preference optimization (DPO). First, we propose a pipeline for
constructing preference pairs that leverages the intrinsic properties of VLMs
along with partial assistance from large language models, achieving an optimal
balance between cost and data quality. Second, we propose Synergistic
Preference Optimization (SynPO), a novel optimization method offering
significant advantages over DPO and its variants. SynPO prevents negative
preferences from dominating the optimization, explicitly preserves the model's
language capability to avoid deviation of the optimization objective, and
improves training efficiency by eliminating the need for the reference model.
We extensively evaluate SynPO not only on video captioning benchmarks (e.g.,
VDC, VDD, VATEX) but also across well-established NLP tasks, including general
language understanding and preference evaluation, using diverse pretrained
models. Results demonstrate that SynPO consistently outperforms DPO variants
while achieving 20\% improvement in training efficiency. Code is available at
https://github.com/longmalongma/SynPO

</details>


### [511] [MedBookVQA: A Systematic and Comprehensive Medical Benchmark Derived from Open-Access Book](https://arxiv.org/abs/2506.00855)
*Sau Lai Yip,Sunan He,Yuxiang Nie,Shu Pui Chan,Yilin Ye,Sum Ying Lam,Hao Chen*

Key words: 通用医学人工智能, 多模态大语言模型, 医学教科书, 基准测试, 性能评估, MedBookVQA

TL;DR: 论文提出了MedBookVQA，一个基于医学教科书的多模态基准测试，用于评估通用医学人工智能（GMAI）的性能，并揭示了当前模型的局限性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着多模态大语言模型（MLLMs）的发展，通用医学人工智能（GMAI）有望解决医疗行业中的劳动力短缺和成本上升问题。然而，目前缺乏系统性的评估基准，而医学教科书作为宝贵的知识来源，其潜力尚未充分开发。

Method: 作者提出了一种标准化流程，从开放获取的医学教科书中自动提取医学图像，并将其与相关医学文本对齐。基于此，生成了5,000个涉及多种临床任务的问答对，并采用多层注释系统进行分类。

Result: 评估了多种MLLM模型，发现其在任务类型和模型类别之间存在显著性能差异，同时揭示了GMAI系统的能力差距。

Conclusion: MedBookVQA展示了教科书衍生的多模态基准在推动临床AI中的重要性，为模型性能评估提供了结构化指标。

Abstract: The accelerating development of general medical artificial intelligence
(GMAI), powered by multimodal large language models (MLLMs), offers
transformative potential for addressing persistent healthcare challenges,
including workforce deficits and escalating costs. The parallel development of
systematic evaluation benchmarks emerges as a critical imperative to enable
performance assessment and provide technological guidance. Meanwhile, as an
invaluable knowledge source, the potential of medical textbooks for benchmark
development remains underexploited. Here, we present MedBookVQA, a systematic
and comprehensive multimodal benchmark derived from open-access medical
textbooks. To curate this benchmark, we propose a standardized pipeline for
automated extraction of medical figures while contextually aligning them with
corresponding medical narratives. Based on this curated data, we generate 5,000
clinically relevant questions spanning modality recognition, disease
classification, anatomical identification, symptom diagnosis, and surgical
procedures. A multi-tier annotation system categorizes queries through
hierarchical taxonomies encompassing medical imaging modalities (42
categories), body anatomies (125 structures), and clinical specialties (31
departments), enabling nuanced analysis across medical subdomains. We evaluate
a wide array of MLLMs, including proprietary, open-sourced, medical, and
reasoning models, revealing significant performance disparities across task
types and model categories. Our findings highlight critical capability gaps in
current GMAI systems while establishing textbook-derived multimodal benchmarks
as essential evaluation tools. MedBookVQA establishes textbook-derived
benchmarking as a critical paradigm for advancing clinical AI, exposing
limitations in GMAI systems while providing anatomically structured performance
metrics across specialties.

</details>


### [512] [GIA-MIC: Multimodal Emotion Recognition with Gated Interactive Attention and Modality-Invariant Learning Constraints](https://arxiv.org/abs/2506.00865)
*Jiajun He,Jinyi Mi,Tomoki Toda*

Key words: 多模态情感识别、注意力机制、模态不变表示、IEMOCAP

TL;DR: 提出一种基于门控交互注意力和模态不变生成器的多模态情感识别方法，解决模态特征提取和跨模态相似性问题，性能优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 多模态情感识别研究中，模态特征提取和跨模态相似性仍存在挑战，需提出新方法解决。

Method: 采用门控交互注意力机制提取模态特征并通过交互增强情感信息，结合模态不变生成器学习不变表示并对齐跨模态相似性。

Result: 在IEMOCAP数据集上，WA达80.7%，UA达81.3%，性能优于现有方法。

Conclusion: 所提方法在多模态情感识别任务中表现优异，解决了模态特征和相似性问题。

Abstract: Multimodal emotion recognition (MER) extracts emotions from multimodal data,
including visual, speech, and text inputs, playing a key role in human-computer
interaction. Attention-based fusion methods dominate MER research, achieving
strong classification performance. However, two key challenges remain:
effectively extracting modality-specific features and capturing cross-modal
similarities despite distribution differences caused by modality heterogeneity.
To address these, we propose a gated interactive attention mechanism to
adaptively extract modality-specific features while enhancing emotional
information through pairwise interactions. Additionally, we introduce a
modality-invariant generator to learn modality-invariant representations and
constrain domain shifts by aligning cross-modal similarities. Experiments on
IEMOCAP demonstrate that our method outperforms state-of-the-art MER
approaches, achieving WA 80.7% and UA 81.3%.

</details>


### [513] [Toward a Theory of Agents as Tool-Use Decision-Makers](https://arxiv.org/abs/2506.00886)
*Hongru Wang,Cheng Qian,Manling Li,Jiahao Qiu,Boyang Xue,Mengdi Wang,Heng Ji,Kam-Fai Wong*

Key words: 大型语言模型,自主代理,认知框架,工具使用,知识边界

TL;DR: 论文提出统一理论，将LLMs的内部推理与外部行为视为等效认知工具，以知识边界指导工具使用，实现高效自主代理。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索大型语言模型（LLMs）作为自主代理的认知基础，解决其决策和行为的指导原则问题。

Method: 提出统一理论，将内部推理和外部行为视为等效认知工具，并建议工具使用决策与知识边界对齐。

Result: 通过框架设计，代理从行为执行者转变为知识驱动系统，实现自适应、高效和目标导向行为。

Conclusion: 论文为构建具有认知基础的自主代理提供了理论框架，提升了LLMs的自主性和效率。

Abstract: As Large Language Models (LLMs) evolve into increasingly autonomous agents,
fundamental questions about their epistemic foundations remain unresolved: What
defines an agent? How should it make decisions? And what objectives should
guide its behavior? In this position paper, we argue that true autonomy
requires agents to be grounded in a coherent epistemic framework that governs
what they know, what they need to know, and how to acquire that knowledge
efficiently. We propose a unified theory that treats internal reasoning and
external actions as equivalent epistemic tools, enabling agents to
systematically coordinate introspection and interaction. Building on this
framework, we advocate for aligning an agent's tool use decision-making
boundary with its knowledge boundary, thereby minimizing unnecessary tool use
and maximizing epistemic efficiency. This perspective shifts the design of
agents from mere action executors to knowledge-driven intelligence systems,
offering a principled path toward building foundation agents capable of
adaptive, efficient, and goal-directed behavior.

</details>


### [514] [Conformal Arbitrage: Risk-Controlled Balancing of Competing Objectives in Language Models](https://arxiv.org/abs/2506.00911)
*William Overman,Mohsen Bayati*

Key words: Conformal Arbitrage, 语言模型部署, 风险控制, 后处理框架

TL;DR: Conformal Arbitrage是一个后处理框架，通过在主要模型和保护机制之间学习数据驱动的阈值，平衡多个竞争目标，提供有限样本保证。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现代语言模型部署需平衡多种竞争目标（如帮助性与无害性），需要一个理论上可靠且经济高效的解决方案。

Method: 使用Conformal Arbitrage框架，通过conformal风险控制校准阈值，无需访问模型权重，仅在API层面操作。

Result: 方法在准确性和成本上优于随机路由，能高效实现多个目标的权衡。

Conclusion: Conformal Arbitrage是一种实用且理论可靠的工具，适用于大型语言模型的多目标部署。

Abstract: Modern language model deployments must often balance competing objectives,
for example, helpfulness versus harmlessness, cost versus accuracy, and reward
versus safety. We introduce Conformal Arbitrage, a post hoc framework that
learns a data driven threshold to mediate between a Primary model optimized for
a primary objective and a more conservative Guardian which could be another
model or a human domain expert aligned with a guardrail objective. The
threshold is calibrated with conformal risk control, yielding finite sample,
distribution free guarantees that the long run frequency of undesirable events,
such as factual errors or safety violations, does not exceed a user specified
quota. Because Conformal Arbitrage operates wholly at the API level, without
requiring access to model logits or updating model weights, it complements
weight based alignment techniques and integrates seamlessly with existing cost
aware cascades. Empirically, Conformal Arbitrage traces an efficient frontier,
allowing users to define an acceptable performance level for one objective
while maximizing utility in another. We observe that our method outperforms, in
terms of accuracy, cost matched random routing between models. These properties
make Conformal Arbitrage a practical, theoretically grounded tool for
trustworthy and economical deployment of large language models across a broad
range of potentially competing objectives.

</details>


### [515] [Aligning VLM Assistants with Personalized Situated Cognition](https://arxiv.org/abs/2506.00930)
*Yongqi Li,Shen Zhou,Xiaohu Li,Xin Miao,Jintao Wen,Mayi Xu,Jianhao Chen,Birong Pan,Hankun Kang,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Key words: 视觉语言模型, 个性化对齐, 角色集, 情境认知, 基准评测

TL;DR: 论文研究了如何将视觉语言模型（VLM）与个性化的情境认知对齐，提出了基于角色集的个体表征方法，构建了包含18k实例的基准PCogAlignBench，并提出了个性化的对齐框架PCogAlign。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 由于不同背景的人对同一情境有不同的认知和期望，需要将VLM助手与个性化的情境认知对齐，以满足个性化需求。

Method: 通过社会学的角色集概念简化个体表征，构建基准PCogAlignBench，并开发了基于认知和动作的奖励模型PCogAlign框架。

Result: 实验和人类评估证明了PCogAlignBench的可靠性和PCogAlign框架的有效性。

Conclusion: PCogAlign框架能够有效实现VLM的个性化对齐，为实际应用提供了支持。

Abstract: Vision-language models (VLMs) aligned with general human objectives, such as
being harmless and hallucination-free, have become valuable assistants of
humans in managing visual tasks. However, people with diversified backgrounds
have different cognition even in the same situation. Consequently, they may
have personalized expectations for VLM assistants. This highlights the urgent
need to align VLM assistants with personalized situated cognition for
real-world assistance. To study this problem, we first simplify it by
characterizing individuals based on the sociological concept of Role-Set. Then,
we propose to evaluate the individuals' actions to examine whether the
personalized alignment is achieved. Further, we construct a benchmark named
PCogAlignBench, which includes 18k instances and 20 individuals with different
Role-Sets. Finally, we present a framework called PCogAlign, which constructs a
cognition-aware and action-based reward model for personalized alignment.
Experimental results and human evaluations demonstrate the reliability of the
PCogAlignBench and the effectiveness of our proposed PCogAlign. We will
open-source the constructed benchmark and code at
https://github.com/NLPGM/PCogAlign.

</details>


### [516] [Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues](https://arxiv.org/abs/2506.00958)
*Youngmin Kim,Jiwan Chung,Jisoo Kim,Sunghyun Lee,Sangkyu Lee,Junhyeok Kim,Cheoljong Yang,Youngjae Yu*

Key words: 多模态语言模型, 非语言交流, 对话AI, VENUS数据集, MARS模型

TL;DR: 该论文介绍了MARS，一种多模态语言模型，旨在结合文本和非语言线索（如表情和肢体语言），以提升对话AI的沉浸感。关键创新是VENUS数据集和统一的训练框架。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的大型语言模型无法有效结合非语言元素，限制了对话体验的真实性。本文旨在填补这一空白。

Method: 使用VENUS数据集（包含标注视频和时间对齐的文本、表情及肢体语言），通过下一词预测目标训练MARS模型，实现多模态理解和生成。

Result: 定量和定性结果显示，MARS能成功生成与对话输入对应的文本和非语言表达，验证了其有效性。

Conclusion: MARS通过结合文本与非语言线索，显著提升了对话AI的沉浸感和真实性。

Abstract: Nonverbal communication is integral to human interaction, with gestures,
facial expressions, and body language conveying critical aspects of intent and
emotion. However, existing large language models (LLMs) fail to effectively
incorporate these nonverbal elements, limiting their capacity to create fully
immersive conversational experiences. We introduce MARS, a multimodal language
model designed to understand and generate nonverbal cues alongside text,
bridging this gap in conversational AI. Our key innovation is VENUS, a
large-scale dataset comprising annotated videos with time-aligned text, facial
expressions, and body language. Leveraging VENUS, we train MARS with a
next-token prediction objective, combining text with vector-quantized nonverbal
representations to achieve multimodal understanding and generation within a
unified framework. Based on various analyses of the VENUS datasets, we validate
its substantial scale and high effectiveness. Our quantitative and qualitative
results demonstrate that MARS successfully generates text and nonverbal
languages, corresponding to conversational input.

</details>


### [517] [Unlocking Personalized Knowledge in Federated Large Language Model: The Power of Mixture of Experts](https://arxiv.org/abs/2506.00965)
*Fan Liu,Bikang Pan,Zhongyi Wang,Xi Yao,Xiaoying Tang,Jingya Wang,Ye Shi*

Key words: MoE, 联邦学习, 大语言模型, 个性化专家, 稀疏性

TL;DR: 论文提出了FLEx框架，专门为MoE架构的大语言模型优化联邦学习，通过个性化专家和自适应门控机制降低通信和计算成本并提升性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前联邦学习方案主要针对密集模型，无法充分利用MoE架构的稀疏性，导致通信和计算成本高昂并影响个性化知识共享。

Method: FLEx框架通过修剪全局MoE模型为每个客户端保留一个专家，采用自适应门控机制将个性化专家整合到预训练MoE层中，同时共享模块全局聚合。

Result: 在非独立同分布数据上的实验表明，FLEx优于现有联邦学习基线方法。

Conclusion: FLEx通过个性化专家和稀疏性优化，有效解决了MoE架构在联邦学习中的挑战。

Abstract: The Mixture of Experts (MoE) architecture has emerged as a prominent strategy
for scaling large language models (LLMs), effectively leveraging sparse
activation and facilitating task-specific personalization. However, current
federated learning (FL) approaches are primarily designed for dense models,
making them unable to directly exploit the sparsity inherent in MoE
architectures. Treating MoE models as dense networks in federated scenarios
results in excessive communication overhead and computational costs,
undermining the potential for personalized knowledge sharing. To address these
challenges, we propose FLEx (Federated LLMs with Personalized Experts), a novel
federated learning framework explicitly tailored for MoE-based LLMs. FLEx
efficiently personalizes by pruning the global MoE model to keep only one
expert per client, and employs an adaptive gating mechanism to reintegrate
these personalized experts into the pre-trained MoE layers, ensuring the
original backbone architecture remains unchanged. These personalized experts
are trained with local data and stored locally on each client, while the shared
modules are aggregated globally. Extensive evaluations on diverse
instruction-based datasets under non-IID conditions consistently demonstrate
that FLEx outperforms existing federated baselines. Our code is available at
https://anonymous.4open.science/r/FLEx-8F12.

</details>


### [518] [PolyBERT: Fine-Tuned Poly Encoder BERT-Based Model for Word Sense Disambiguation](https://arxiv.org/abs/2506.00968)
*Linhan Xia,Mingzhan Yang,Guohui Yuan,Shengnan Tao,Yujing Qiu,Guo Yu,Kai Lei*

Key words: Word Sense Disambiguation, PolyBERT, batch contrastive learning, BERT

TL;DR: PolyBERT融合多尺度语义并通过批量对比学习降低计算成本，提升WSD性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决WSD中语义表示不平�和计算冗余问题。

Method: 使用多头注意力机制平衡局部和全局语义，引入批量对比学习减少训练输入。

Result: F1分数提升2%，GPU时间减少37.6%。

Conclusion: PolyBERT在性能和效率上均优于现有方法。

Abstract: Mainstream Word Sense Disambiguation (WSD) approaches have employed BERT to
extract semantics from both context and definitions of senses to determine the
most suitable sense of a target word, achieving notable performance. However,
there are two limitations in these approaches. First, previous studies failed
to balance the representation of token-level (local) and sequence-level
(global) semantics during feature extraction, leading to insufficient semantic
representation and a performance bottleneck. Second, these approaches
incorporated all possible senses of each target word during the training phase,
leading to unnecessary computational costs. To overcome these limitations, this
paper introduces a poly-encoder BERT-based model with batch contrastive
learning for WSD, named PolyBERT. Compared with previous WSD methods, PolyBERT
has two improvements: (1) A poly-encoder with a multi-head attention mechanism
is utilized to fuse token-level (local) and sequence-level (global) semantics,
rather than focusing on just one. This approach enriches semantic
representation by balancing local and global semantics. (2) To avoid redundant
training inputs, Batch Contrastive Learning (BCL) is introduced. BCL utilizes
the correct senses of other target words in the same batch as negative samples
for the current target word, which reduces training inputs and computational
cost. The experimental results demonstrate that PolyBERT outperforms baseline
WSD methods such as Huang's GlossBERT and Blevins's BEM by 2\% in F1-score. In
addition, PolyBERT with BCL reduces GPU hours by 37.6\% compared with PolyBERT
without BCL.

</details>


### [519] [Boosting Bot Detection via Heterophily-Aware Representation Learning and Prototype-Guided Cluster Discovery](https://arxiv.org/abs/2506.00989)
*Buyun He,Xiaorui Jiang,Qi Wu,Hao Liu,Yingguang Yang,Yong Liao*

Key words: 社交机器人, 图自监督学习, 异构感知, 原型聚类

TL;DR: 论文提出BotHP框架，通过异构感知表示学习和原型引导的聚类发现，提升基于图的社交机器检测器的性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的基于图的社交机器人检测方法依赖标签且泛化能力差，而现有的生成图自监督学习方法未能捕捉全局模式，难以应对交互伪装和分布式部署的挑战。

Method: BotHP采用双编码器架构，包括图感知编码器和图不可知编码器，同时建模同质性和异质性，并通过原型引导的聚类发现任务识别全局一致性。

Result: 在两个真实数据集上的实验表明，BotHP显著提升了检测性能，减少了标签依赖并增强了泛化能力。

Conclusion: BotHP通过异构感知和全局一致性的建模，为社交机器人检测提供了一种有效的解决方案。

Abstract: Detecting social media bots is essential for maintaining the security and
trustworthiness of social networks. While contemporary graph-based detection
methods demonstrate promising results, their practical application is limited
by label reliance and poor generalization capability across diverse
communities. Generative Graph Self-Supervised Learning (GSL) presents a
promising paradigm to overcome these limitations, yet existing approaches
predominantly follow the homophily assumption and fail to capture the global
patterns in the graph, which potentially diminishes their effectiveness when
facing the challenges of interaction camouflage and distributed deployment in
bot detection scenarios. To this end, we propose BotHP, a generative GSL
framework tailored to boost graph-based bot detectors through heterophily-aware
representation learning and prototype-guided cluster discovery. Specifically,
BotHP leverages a dual-encoder architecture, consisting of a graph-aware
encoder to capture node commonality and a graph-agnostic encoder to preserve
node uniqueness. This enables the simultaneous modeling of both homophily and
heterophily, effectively countering the interaction camouflage issue.
Additionally, BotHP incorporates a prototype-guided cluster discovery pretext
task to model the latent global consistency of bot clusters and identify
spatially dispersed yet semantically aligned bot collectives. Extensive
experiments on two real-world bot detection benchmarks demonstrate that BotHP
consistently boosts graph-based bot detectors, improving detection performance,
alleviating label reliance, and enhancing generalization capability.

</details>


### [520] [Higher-Order Responsibility](https://arxiv.org/abs/2506.01003)
*Junli Jiang,Pavel Naumov*

Key words: 责任空白、高阶责任、群体决策、计算复杂性、伦理学

TL;DR: 论文探讨了在群体决策中高阶责任是否能填补责任空白，并证明该问题属于$Π_{2d+1}$-完全问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的弗兰克福特替代可能性原则在群体决策中无法准确定义责任，导致责任空白问题。

Method: 通过研究高阶责任是否足以填补责任空白，并分析其计算复杂性。

Result: 证明了判定高阶责任是否能填补责任空白的问题是$Π_{2d+1}$-完全的。

Conclusion: 高阶责任为解决群体决策中的责任问题提供了新思路，但其计算复杂性较高。

Abstract: In ethics, individual responsibility is often defined through Frankfurt's
principle of alternative possibilities. This definition is not adequate in a
group decision-making setting because it often results in the lack of a
responsible party or "responsibility gap''. One of the existing approaches to
address this problem is to consider group responsibility. Another, recently
proposed, approach is "higher-order'' responsibility. The paper considers the
problem of deciding if higher-order responsibility up to degree $d$ is enough
to close the responsibility gap. The main technical result is that this problem
is $\Pi_{2d+1}$-complete.

</details>


### [521] [IRT-Router: Effective and Interpretable Multi-LLM Routing via Item Response Theory](https://arxiv.org/abs/2506.01048)
*Wei Song,Zhenya Huang,Cheng Cheng,Weibo Gao,Bihan Xu,GuanHao Zhao,Fei Wang,Runze Wu*

Key words: LLM, IRT-Router, 路由框架, 冷启动, 可解释性

TL;DR: 论文提出IRT-Router，一个基于Item Response Theory的多LLM路由框架，通过模型能力与查询属性的关系优化查询分配，平衡性能与成本。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 大语言模型（LLM）在多种任务中表现优异，但选择最优模型时需权衡性能与成本。

Method: 采用Item Response Theory，设计IRT-Router框架，结合在线查询预热技术。

Result: 在20个LLM和12个数据集上验证，IRT-Router在效果和可解释性上优于基线方法。

Conclusion: IRT-Router在冷启动场景中表现突出，适用于实际应用。

Abstract: Large language models (LLMs) have demonstrated exceptional performance across
a wide range of natural language tasks. However, selecting the optimal LLM to
respond to a user query often necessitates a delicate balance between
performance and cost. While powerful models deliver better results, they come
at a high cost, whereas smaller models are more cost-effective but less
capable. To address this trade-off, we propose IRT-Router, a multi-LLM routing
framework that efficiently routes user queries to the most suitable LLM.
Inspired by Item Response Theory (IRT), a psychological measurement
methodology, IRT-Router explicitly models the relationship between LLM
capabilities and user query attributes. This not only enables accurate
prediction of response performance but also provides interpretable insights,
such as LLM abilities and query difficulty. Additionally, we design an online
query warm-up technique based on semantic similarity, further enhancing the
online generalization capability of IRT-Router. Extensive experiments on 20
LLMs and 12 datasets demonstrate that IRT-Router outperforms most baseline
methods in terms of effectiveness and interpretability. Its superior
performance in cold-start scenarios further confirms the reliability and
practicality of IRT-Router in real-world applications. Code is available at
https://github.com/Mercidaiha/IRT-Router.

</details>


### [522] [MCP-Zero: Proactive Toolchain Construction for LLM Agents from Scratch](https://arxiv.org/abs/2506.01056)
*Xiang Fei,Xiawu Zheng,Hao Feng*

Key words: 大型语言模型, 工具调用, 主动代理, 分层检索, 多轮调用

TL;DR: MCP-Zero 是一个主动代理框架，允许大型语言模型（LLM）自行决定何时检索外部工具，从而从头构建任务特定的工具链，解决了传统方法中注入大量工具模式的高成本和易错问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的 LLM 工具调用方法需要将大量工具模式注入提示中，导致成本高且容易出错。MCP-Zero 提出了一种主动代理框架，使 LLM 能够动态检索和选择工具，减少上下文开销。

Method: MCP-Zero 框架包含三个核心组件：主动工具请求、分层向量路由和迭代主动调用，通过这些机制实现了动态工具链的构建和高效检索。

Result: 实验表明，MCP-Zero 能够高效选择正确工具（从近 3,000 候选工具中），减少 98% 的 token 消耗，并支持多轮工具调用。

Conclusion: MCP-Zero 显著降低了工具调用的上下文开销，提高了效率，同时保持了准确性，适用于多领域任务。

Abstract: Function-calling has enabled large language models (LLMs) to act as
tool-using agents, but injecting thousands of tool schemas into the prompt is
costly and error-prone. We introduce MCP-Zero, a proactive agent framework that
lets the LLM itself decide when and which external tools to retrieve, thereby
assembling a task-specific toolchain from scratch. The framework is built upon
three components: (1) Proactive Tool Request, where the model emits a
structured $\left<\operatorname{tool\_assistant}\right>$ block that explicitly
specifies the desired server and task; (2) Hierarchical Vector Routing, a
coarse-to-fine retrieval algorithm that first selects candidate servers and
then ranks tools within each server based on the semantic similarity; (3)
Iterative Proactive Invocation, enabling multi-round, cross-domain toolchain
construction with minimal context overhead, and allowing the model to
iteratively revise its request when the returned tools are insufficient. To
evaluate our approach we also compile MCP-tools, a retrieval dataset comprising
308 MCP servers and 2,797 tools extracted from the official
Model-Context-Protocol repository and normalized into a unified JSON schema.
Experiments show that MCP-Zero (i) effectively addresses the context overhead
problem of existing methods and accurately selects the correct tool from a pool
of nearly 3,000 candidates (248.1k tokens); (ii) reduces token consumption by
98\% on the APIbank while maintaining high accuracy; and (iii) supports
multi-turn tool invocation with consistent accuracy across rounds. The code and
dataset will be released soon.

</details>


### [523] [The Coming Crisis of Multi-Agent Misalignment: AI Alignment Must Be a Dynamic and Social Process](https://arxiv.org/abs/2506.01080)
*Florian Carichon,Aditi Khandelwal,Marylou Fauchard,Golnoosh Farnadi*

Key words: AI对齐, 多代理系统, 社会结构, 动态互动, 评估框架

TL;DR: 本文主张在多代理系统（MAS）中，AI对齐应被视为一个动态且依赖互动的过程，社会环境影响重大，并提出需将其视为相互依赖的问题，呼吁建立模拟环境和评估框架。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着多代理系统在现实应用中的普及，AI对齐中的动态互动和社会结构影响成为新挑战，可能导致代理与人类价值观的意外偏离。

Method: 借鉴社会科学，分析社会结构如何影响群体和个体价值观，并提出将人类、偏好和目标对齐视为相互依赖的概念。

Result: 指出了当前AI对齐研究中忽视动态互动的问题，提出了将其视为相互依赖问题的必要性。

Conclusion: 急需开发模拟环境、基准和评估框架，以在多代理交互环境中评估对齐问题，避免复杂失控。

Abstract: This position paper states that AI Alignment in Multi-Agent Systems (MAS)
should be considered a dynamic and interaction-dependent process that heavily
depends on the social environment where agents are deployed, either
collaborative, cooperative, or competitive. While AI alignment with human
values and preferences remains a core challenge, the growing prevalence of MAS
in real-world applications introduces a new dynamic that reshapes how agents
pursue goals and interact to accomplish various tasks. As agents engage with
one another, they must coordinate to accomplish both individual and collective
goals. However, this complex social organization may unintentionally misalign
some or all of these agents with human values or user preferences. Drawing on
social sciences, we analyze how social structure can deter or shatter group and
individual values. Based on these analyses, we call on the AI community to
treat human, preferential, and objective alignment as an interdependent
concept, rather than isolated problems. Finally, we emphasize the urgent need
for simulation environments, benchmarks, and evaluation frameworks that allow
researchers to assess alignment in these interactive multi-agent contexts
before such dynamics grow too complex to control.

</details>


### [524] [Choices and their Provenance: Explaining Stable Solutions of Abstract Argumentation Frameworks](https://arxiv.org/abs/2506.01087)
*Bertram Ludäscher,Yilin Xia,Shawn Bowers*

Key words: 论证框架，良基语义，稳定解，根源分析，关键攻击

TL;DR: 该论文提出了一种新的方法，将论证框架（AF）中基于良基语义（WFS）的根源扩展到稳定解（stable solutions），通过识别关键攻击边来揭示论证状态的来源。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 良基解提供了论证状态的明确来源，但稳定解涉及非确定性选择，需要一种不同的方法来解释其来源。

Method: 通过识别最小的关键攻击集，将良基推导步骤与选择步骤结合，揭示稳定解的来源。

Result: 该方法可以视为一种诊断工具，找到最小的“修复”以使修复后的图的良基解与原图的稳定解一致。

Conclusion: 提出的方法为稳定解提供了更丰富的来源解释，结合良基和选择步骤，增强了对论证状态的理解。

Abstract: The rule $\mathrm{Defeated}(x) \leftarrow \mathrm{Attacks}(y,x),\, \neg \,
\mathrm{Defeated}(y)$, evaluated under the well-founded semantics (WFS), yields
a unique 3-valued (skeptical) solution of an abstract argumentation framework
(AF). An argument $x$ is defeated ($\mathrm{OUT}$) if there exists an
undefeated argument $y$ that attacks it. For 2-valued (stable) solutions, this
is the case iff $y$ is accepted ($\mathrm{IN}$), i.e., if all of $y$'s
attackers are defeated. Under WFS, arguments that are neither accepted nor
defeated are undecided ($\mathrm{UNDEC}$). As shown in prior work, well-founded
solutions (a.k.a. grounded labelings) "explain themselves": The provenance of
arguments is given by subgraphs (definable via regular path queries) rooted at
the node of interest. This provenance is closely related to winning strategies
of a two-player argumentation game.
  We present a novel approach for extending this provenance to stable AF
solutions. Unlike grounded solutions, which can be constructed via a bottom-up
alternating fixpoint procedure, stable models often involve non-deterministic
choice as part of the search for models. Thus, the provenance of stable
solutions is of a different nature, and reflects a more expressive generate &
test paradigm. Our approach identifies minimal sets of critical attacks,
pinpointing choices and assumptions made by a stable model. These critical
attack edges provide additional insights into the provenance of an argument's
status, combining well-founded derivation steps with choice steps. Our approach
can be understood as a form of diagnosis that finds minimal "repairs" to an AF
graph such that the well-founded solution of the repaired graph coincides with
the desired stable model of the original AF graph.

</details>


### [525] [Regulatory Graphs and GenAI for Real-Time Transaction Monitoring and Compliance Explanation in Banking](https://arxiv.org/abs/2506.01093)
*Kunal Khanvilkar,Kranthi Kommuru*

Key words: 实时监控、图神经网络、生成解释、金融合规

TL;DR: 提出了一种实时交易监控框架，结合图模型、叙事嵌入和生成解释，用于自动化金融合规。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决高风险金融环境中实时交易监控和合规解释的需求。

Method: 构建动态交易图，提取特征，利用图神经网络分类异常行为，并通过生成模块提供自然语言解释。

Result: 实验显示F1分数98.2%，精确率97.8%，召回率97.0%，专家认可解释质量。

Conclusion: 图智能与生成模型结合为可解释的合规审计提供潜力。

Abstract: This paper presents a real-time transaction monitoring framework that
integrates graph-based modeling, narrative field embedding, and generative
explanation to support automated financial compliance. The system constructs
dynamic transaction graphs, extracts structural and contextual features, and
classifies suspicious behavior using a graph neural network. A
retrieval-augmented generation module generates natural language explanations
aligned with regulatory clauses for each flagged transaction. Experiments
conducted on a simulated stream of financial data show that the proposed method
achieves superior results, with 98.2% F1-score, 97.8% precision, and 97.0%
recall. Expert evaluation further confirms the quality and interpretability of
generated justifications. The findings demonstrate the potential of combining
graph intelligence and generative models to support explainable, audit-ready
compliance in high-risk financial environments.

</details>


### [526] [Modular Speaker Architecture: A Framework for Sustaining Responsibility and Contextual Integrity in Multi-Agent AI Communication](https://arxiv.org/abs/2506.01095)
*Khe-Han Toh,Hong-Kuan Teo*

Key words: 多智能体系统, 模块化说话者架构, 责任连续性, 上下文连贯性, G-Code

TL;DR: 论文提出了模块化说话者架构（MSA），用于解决多智能体系统中角色感知通信的问题，并通过实验证明其有效性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前多智能体通信框架缺乏明确的说话者责任机制，导致上下文漂移和对齐不稳定，影响了通信的连贯性和可解释性。

Method: 提出了MSA框架，包括说话者角色模块、责任链追踪器和上下文完整性验证器三个核心模块，并通过案例研究和结构指标进行评估。

Result: 实验结果表明，MSA能够在不依赖情感信号或表面启发式方法的情况下，可靠地维护交互结构的稳定性。

Conclusion: MSA通过模块化设计和原型配置语言（G-Code），能够有效支持动态多智能体场景中的应用。

Abstract: Sustaining coherent, role-aware communication across multi-agent systems
remains a foundational challenge in AI. Current frameworks often lack explicit
mechanisms for speaker responsibility, leading to context drift, alignment
instability, and degraded interpretability over time. We propose the Modular
Speaker Architecture (MSA), a framework that decomposes speaker behavior into
modular components for role tracking, responsibility continuity, and contextual
coherence. Grounded in high-context human-AI dialogues, MSA includes three core
modules: a Speaker Role Module, a Responsibility Chain Tracker, and a
Contextual Integrity Validator. We evaluate MSA through annotated case studies
and introduce structural metrics-pragmatic consistency, responsibility flow,
and context stability-quantified via manual and automatic scoring and
bootstrapped statistical analysis. Our results show that MSA reliably maintains
interaction structure without reliance on affective signals or surface-level
heuristics. We further implement a prototype configuration language (G-Code)
and modular API to support MSA deployment in dynamic multi-agent scenarios.

</details>


### [527] [SuperRL: Reinforcement Learning with Supervision to Boost Language Model Reasoning](https://arxiv.org/abs/2506.01096)
*Yihao Liu,Shuocheng Li,Lang Cao,Yuhang Xie,Mengyu Zhou,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang*

Key words: SuperRL,强化学习,稀疏奖励,离线监督,混合执行器

TL;DR: SuperRL框架通过结合离线监督和强化学习，解决稀疏奖励环境下的学习效率问题，显着提升样本效率、泛化能力和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 在稀疏奖励环境中，强化学习难以采样成功轨迹，同时标准强化学习方法未利用离线正确推理路径数据。

Method: 提出SuperRL框架，引入自适应切换器和混合执行器，结合策略梯度和监督学习目标。

Result: 在多个推理基准测试中，SuperRL优于标准强化学习，提升样本效率、泛化能力和鲁棒性。

Conclusion: SuperRL通过自适应结合离线监督与强化学习，有效解决了稀疏奖励环境下的学习问题。

Abstract: Large language models are increasingly used for complex reasoning tasks where
high-quality offline data such as expert-annotated solutions and distilled
reasoning traces are often available. However, in environments with sparse
rewards, reinforcement learning struggles to sample successful trajectories,
leading to inefficient learning. At the same time, these offline trajectories
that represent correct reasoning paths are not utilized by standard on-policy
reinforcement learning methods. To address this limitation, we propose SuperRL,
a unified training framework that adaptively incorporates offline supervision
into reinforcement learning. SuperRL introduces an Adaptive Switch to detect
sparse reward conditions and activates a Hybrid Actor when necessary. The
Hybrid Actor integrates policy gradient and supervised learning objectives at
the loss level, enabling the model to benefit from accurate offline reasoning
signals while maintaining the exploratory capacity of reinforcement learning.
Experiments on a range of reasoning benchmarks show that SuperRL consistently
outperforms standard reinforcement learning by improving sample efficiency,
generalization, and robustness under sparse rewards.

</details>


### [528] [ChemAU: Harness the Reasoning of LLMs in Chemical Research with Adaptive Uncertainty Estimation](https://arxiv.org/abs/2506.01116)
*Xinyi Liu,Lipeng Ma,Yixuan Li,Weidong Yang,Qingyuan Zhou,Jiayi Song,Shuhao Li,Ben Fei*

Key words: 大语言模型,化学推理,不确定性估计,ChemAU,自适应方法

TL;DR: 论文提出了ChemAU框架，通过自适应不确定性估计方法，提升大语言模型在化学问题中的推理准确性和不确定性估计。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 大语言模型在数学和编程任务中表现出色，但在化学问题上表现不佳，原因是化学问题的复杂性和术语特殊性。现有方法难以有效利用化学知识和公式。

Method: 提出ChemAU框架，采用自适应不确定性估计方法，根据推理步骤的位置调整不确定性值，补充化学专业知识以修正推理链。

Result: 在三个化学数据集上的实验表明，ChemAU显著提高了推理准确性和不确定性估计。

Conclusion: ChemAU通过精准补充化学知识和自适应不确定性估计，有效解决了大语言模型在化学问题中的局限性。

Abstract: Large Language Models (LLMs) are widely used across various scenarios due to
their exceptional reasoning capabilities and natural language understanding.
While LLMs demonstrate strong performance in tasks involving mathematics and
coding, their effectiveness diminishes significantly when applied to
chemistry-related problems. Chemistry problems typically involve long and
complex reasoning steps, which contain specific terminology, including
specialized symbol systems and complex nomenclature conventions. These
characteristics often cause general LLMs to experience hallucinations during
the reasoning process due to their lack of specific knowledge. However,
existing methods are struggling to effectively leverage chemical expertise and
formulas. Moreover, current uncertainty estimation methods, designed to
mitigate potential reasoning errors, are unable to precisely identify specific
steps or key knowledge. In this work, we propose a novel framework called
ChemAU, which incorporates our adaptive uncertainty estimation method that
applies different uncertainty values based on the position of reasoning steps
within the whole reasoning chain. Leveraging this method, ChemAU identifies
gaps in chemistry knowledge and precisely supplements chemical expertise with
the specialized domain model, thereby correcting and updating the previously
flawed reasoning chain. Our experiments with three popular LLMs across three
chemistry datasets demonstrate that ChemAU significantly enhances both
reasoning accuracy and uncertainty estimation.

</details>


### [529] [GraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question Answering](https://arxiv.org/abs/2506.01174)
*Muhammad Qasim Ali,Saeejith Nair,Alexander Wong,Yuchen Cui,Yuhao Chen*

Key words: 结构化记忆,场景图,动态调整,OpenEQA,语言驱动

TL;DR: GraphPad是一种可修改的结构化记忆，通过API调用动态调整场景表示，提高任务适应性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统静态场景表示方法无法适应任务变化，GraphPad旨在提供动态、可调整的记忆结构。

Method: GraphPad包含可变场景图、导航日志和任务便签，支持语言驱动的在线细化。

Result: 在OpenEQA基准测试中，GraphPad性能提升3%，且输入帧数减少80%。

Conclusion: 动态细化的3D记忆能生成更具信息性的表示，无需额外训练或数据。

Abstract: Structured scene representations are a core component of embodied agents,
helping to consolidate raw sensory streams into readable, modular, and
searchable formats. Due to their high computational overhead, many approaches
build such representations in advance of the task. However, when the task
specifications change, such static approaches become inadequate as they may
miss key objects, spatial relations, and details. We introduce GraphPad, a
modifiable structured memory that an agent can tailor to the needs of the task
through API calls. It comprises a mutable scene graph representing the
environment, a navigation log indexing frame-by-frame content, and a scratchpad
for task-specific notes. Together, GraphPad serves as a dynamic workspace that
remains complete, current, and aligned with the agent's immediate understanding
of the scene and its task. On the OpenEQA benchmark, GraphPad attains 55.3%, a
+3.0% increase over an image-only baseline using the same vision-language
model, while operating with five times fewer input frames. These results show
that allowing online, language-driven refinement of 3-D memory yields more
informative representations without extra training or data collection.

</details>


### [530] [Test Automation for Interactive Scenarios via Promptable Traffic Simulation](https://arxiv.org/abs/2506.01199)
*Augusto Mondelli,Yueshan Li,Alessandro Zanardi,Emilio Frazzoli*

Key words: 自动驾驶、人类行为不确定性、场景生成、贝叶斯优化

TL;DR: 本文提出了一种自动化方法，通过低维目标位置参数化和贝叶斯优化，高效生成安全和关键的人类行为，用于自动驾驶规划器的交互场景评估。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 自动驾驶规划器在公共道路上部署前需严格评估其对抗人类行为不确定性的鲁棒性，但目前缺乏利用数据驱动场景生成模型构建全面测试的方法。

Method: 提出了一种参数化人类行为的方法，结合可提示的交通模拟器ProSim和贝叶斯优化，自动生成安全和关键的行为测试。

Result: 该方法在优化规划器评估中表现出色，能高效生成多样且真实的驾驶行为。

Conclusion: 该方法为自动驾驶规划器的安全性和鲁棒性评估提供了一种高效自动化手段。

Abstract: Autonomous vehicle (AV) planners must undergo rigorous evaluation before
widespread deployment on public roads, particularly to assess their robustness
against the uncertainty of human behaviors. While recent advancements in
data-driven scenario generation enable the simulation of realistic human
behaviors in interactive settings, leveraging these models to construct
comprehensive tests for AV planners remains an open challenge. In this work, we
introduce an automated method to efficiently generate realistic and
safety-critical human behaviors for AV planner evaluation in interactive
scenarios. We parameterize complex human behaviors using low-dimensional goal
positions, which are then fed into a promptable traffic simulator, ProSim, to
guide the behaviors of simulated agents. To automate test generation, we
introduce a prompt generation module that explores the goal domain and
efficiently identifies safety-critical behaviors using Bayesian optimization.
We apply our method to the evaluation of an optimization-based planner and
demonstrate its effectiveness and efficiency in automatically generating
diverse and realistic driving behaviors across scenarios with varying initial
conditions.

</details>


### [531] [CleanS2S: Single-file Framework for Proactive Speech-to-Speech Interaction](https://arxiv.org/abs/2506.01268)
*Yudong Lu,Yazhe Niu,Shuai Hu,Haolin Wang*

Key words: 语音到语音交互，主动对话，低延迟，上下文感知，开源框架

TL;DR: CleanS2S是一个通过单文件实现和主动对话能力推动对话AI发展的语音到语音交互框架。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 旨在打破传统的基于轮次的对话模式，实现更人性化的语音交互，同时提供透明且可扩展的实现方式。

Method: 整合自动语音识别、大型语言模型和文本到语音合成，利用全双工WebSocket连接和非阻塞I/O实现低延迟，结合内存系统和主观行动判断模块实现主动交互策略。

Result: 实现了五种人性化响应策略（打断、拒绝、回避、沉默和标准响应），并支持系统发起的对话控制和上下文感知响应选择。

Conclusion: CleanS2S为研究者提供了高度透明和可扩展的语音交互框架，推动了对话AI的发展。

Abstract: CleanS2S is a framework for human-like speech-to-speech interaction that
advances conversational AI through single-file implementation and proactive
dialogue capabilities. Our system integrates automatic speech recognition,
large language models, and text-to-speech synthesis into a unified pipeline
with real-time interruption handling, achieving low transition latency through
full-duplex websocket connections and non-blocking I/O. Beyond conventional
chatbot paradigms, we pioneer a proactive interaction mechanism, which combines
memory systems with Subjective Action Judgement module, enabling five
human-like response strategies: interruption, refusal, deflection, silence, and
standard response. The memory module dynamically aggregates historical, and
contextual data to inform interaction decisions. This approach breaks the rigid
turn-based convention by allowing system-initiated dialog control and
context-aware response selection. And we propose Action Judgement SFT that
assesses input streams for responses strategies. The framework's single-file
implementation with atomic configurations offers researchers unprecedented
transparency and extensibility for interaction agents. The code of CleanS2S is
released at \https://github.com/opendilab/CleanS2S.

</details>


### [532] [RAISE: Reasoning Agent for Interactive SQL Exploration](https://arxiv.org/abs/2506.01273)
*Fernando Granado,Roberto Lotufo,Jayr Pereira*

Key words: 大型语言模型, 文本到SQL, 代理框架, 动态查询, 迭代优化

TL;DR: 提出了一种新型的基于大型语言模型（LLMs）的端到端文本到SQL框架，通过动态查询验证和迭代优化，显著提升了执行准确率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有文本到SQL系统依赖复杂的多阶段流程，而基于LLM的推理能力可以模拟人类处理陌生数据库的行为。

Method: 采用统一的代理框架，集成模式链接、查询生成和迭代优化，动态分配计算资源以应对模糊场景。

Result: 在BIRD数据集上，执行准确率从44.8%提升至56.5%，通过8轮候选生成达到81.8%的最佳准确率。

Conclusion: 该框架为构建自然语言数据库接口提供了一种高效且工程复杂度低的方案。

Abstract: Recent advances in large language models (LLMs) have propelled research in
natural language interfaces to databases. However, most state-of-the-art
text-to-SQL systems still depend on complex, multi-stage pipelines. This work
proposes a novel agentic framework that unifies schema linking, query
generation, and iterative refinement within a single, end-to-end component. By
leveraging the intrinsic reasoning abilities of LLMs, our method emulates how
humans answer questions when working with unfamiliar databases: understanding
the data by formulating hypotheses, running dynamic queries to validate them,
reasoning over the results, and revising outputs based on observed results.
Crucially, our approach introduces a new strategy for scaling test-time
computation in text-to-SQL: we scale the depth of interactive database
exploration and reflection. This shift enables the model to allocate
computation dynamically to better understand the data, especially useful in
ambiguous and underspecified scenarios. Our experiments show that it improved
the Execution Accuracy (EX) from 44.8% to 56.5% on the challenging BIRD dataset
using DeepSeek-R1-Distill-Llama-70B. Furthermore, when equipped with steps to
add more diversity to the answers, our agent achieves a Best-of-N accuracy of
81.8% with 8 rounds of candidate generation, rivaling the 82.79% achieved by
the top-ranked published solution, while reducing engineering complexity. These
findings position our unified framework as a promising alternative for building
natural language interfaces to databases.

</details>


### [533] [Contra4: Evaluating Contrastive Cross-Modal Reasoning in Audio, Video, Image, and 3D](https://arxiv.org/abs/2506.01275)
*Artemis Panagopoulou,Le Xue,Honglu Zhou,silvio savarese,Ran Xu,Caiming Xiong,Chris Callison-Burch,Mark Yatskar,Juan Carlos Niebles*

Key words: 多模态模型, 对比推理, Contra4数据集, 模态选择

TL;DR: 论文提出Contra4数据集，用于评估多模态模型在跨模态对比推理中的能力，发现当前模型在四模态场景下表现有限。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现实决策需要识别最相关信息的模态，但当前多模态模型是否能跨模态对比推理尚不明确。

Method: 引入Contra4数据集，包含四种模态的样本，通过自然语言问题测试模型选择能力。

Result: 最佳模型在四模态场景下准确率仅42%，揭示当前模型的局限性。

Conclusion: 跨模态对比推理是当前多模态模型的显著短板。

Abstract: Real-world decision-making often begins with identifying which modality
contains the most relevant information for a given query. While recent
multimodal models have made impressive progress in processing diverse inputs,
it remains unclear whether they can reason contrastively across multiple
modalities to select the one that best satisfies a natural language prompt. We
argue this capability is foundational, especially in retrieval-augmented and
decision-time contexts, where systems must evaluate multiple signals and
identify which one conveys the relevant information. To evaluate this skill, we
introduce Contra4, a dataset for contrastive cross-modal reasoning across four
modalities: image, audio, video, and 3D. Each example presents a natural
language question alongside multiple candidate modality instances, and the
model must select the one that semantically aligns with the prompt. Contra4
combines human-annotated captions with a mixture-of-models
round-trip-consistency filter to ensure high-quality supervision, resulting in
174k training examples and a manually verified test set of 2.3k samples. While
task-specific fine-tuning improves performance by 56% relative to baseline,
state-of-the-art models still achieve only 56% accuracy overall and 42% in
four-modality settings, underscoring a significant limitation in current
multimodal models.

</details>


### [534] [GeoLocSFT: Efficient Visual Geolocation via Supervised Fine-Tuning of Multimodal Foundation Models](https://arxiv.org/abs/2506.01277)
*Qiang Yi,Lianlei Shan*

Key words: 图像地理定位,监督微调,多模态模型,基准数据集

TL;DR: GeoLocSFT通过高质量数据集对基础模型进行监督微调，显著提升了图像地理定位性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决全球范围内图像地理定位的挑战，特别是在稀疏人口区域。

Method: 使用2700个精选图像-GPS对进行监督微调，探索多候选推理与聚合策略。

Result: 在多个基准测试中表现优异，尤其是新提出的MR40k基准。

Conclusion: 高质量监督和高效微调能够显著提升地理定位性能，不需要复杂流程或大数据集。

Abstract: Accurately determining the geographic location where a single image was
taken, visual geolocation, remains a formidable challenge due to the planet's
vastness and the deceptive similarity among distant locations. We introduce
GeoLocSFT, a framework that demonstrates how targeted supervised fine-tuning
(SFT) of a large multimodal foundation model (Gemma 3) using a small,
high-quality dataset can yield highly competitive geolocation performance.
GeoLocSFT is trained with only 2700 carefully selected image-GPS pairs from our
geographically diverse MR600k dataset. Despite this limited data, our
SFT-centric approach substantially improves over baseline models and achieves
robust results on standard benchmarks such as Im2GPS-3k and YFCC-4k, as well as
on our newly proposed and challenging MR40k benchmark, aimed specifically at
sparsely populated regions. Further, we explore multi-candidate inference and
aggregation strategies but find that the core gains are already realized at the
SFT stage. Our findings highlight the power of high-quality supervision and
efficient SFT for planet-scale image geolocation, especially when compared to
prior methods that require massive databases or complex pipelines. To foster
further research, we publicly release the MR40k benchmark dataset.

</details>


### [535] [On the Hardness of Approximating Distributions with Probabilistic Circuits](https://arxiv.org/abs/2506.01281)
*John Leland,YooJung Choi*

Key words: 概率电路, 表达性, 可推断性, 近似表示, $f$-散度

TL;DR: 该论文探讨了概率电路中表达性与可推断性的平衡问题，研究了近似表示对避免指数级规模膨胀的作用，并证明了可分解概率电路与确定性概率电路之间的近似规模差距。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 概率建模中的一个根本挑战是平衡表达性和可推断性。虽然概率电路（PCs）通过结构约束在保证高效推断的同时保持表达性，但严格的精确表示往往导致指数级的规模膨胀，因此研究近似表示是否能避免这一问题具有重要意义。

Method: 论文首先证明，对于任何能高效计算边缘概率的模型，以有界$f$-散度近似任意分布是$	ext{NP}$困难的。随后，分析了可分解概率电路与确定性概率电路在近似表示中的规模差距。

Result: 研究结果表明，可分解概率电路与确定性概率电路在近似表示中存在指数级的规模差距，突出了近似方法在避免规模膨胀中的潜力。

Conclusion: 通过允许一定的近似误差，可以在避免指数级规模膨胀的同时，维持概率电路的表达性与可推断性平衡。

Abstract: A fundamental challenge in probabilistic modeling is balancing expressivity
and tractable inference. Probabilistic circuits (PCs) aim to directly address
this tradeoff by imposing structural constraints that guarantee efficient
inference of certain queries while maintaining expressivity. Since inference
complexity on PCs depends on circuit size, understanding the size bounds across
circuit families is key to characterizing the tradeoff between tractability and
expressive efficiency. However, expressive efficiency is often studied through
exact representations, where exactly encoding distributions while enforcing
various structural properties often incurs exponential size blow-ups. Thus, we
pose the following question: can we avoid such size blow-ups by allowing some
small approximation error? We first show that approximating an arbitrary
distribution with bounded $f$-divergence is $\mathsf{NP}$-hard for any model
that can tractably compute marginals. We then prove an exponential size gap for
approximation between the class of decomposable PCs and additionally
deterministic PCs.

</details>


### [536] [MobCLIP: Learning General-purpose Geospatial Representation at Scale](https://arxiv.org/abs/2506.01297)
*Ya Wen,Jixuan Cai,Qiyao Ma,Linyan Li,Xinhua Chen,Chris Webster,Yulun Zhou*

Key words: 地理空间智能、多模态融合、CLIP架构、通用位置编码器

TL;DR: MobCLIP是一个全国通用的地理空间位置编码器，通过多模态融合整合POI、遥感影像和人口统计数据，显著提升了多任务预测性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决地理空间位置表示学习的通用性问题，现有方法在多任务中表现不足。

Method: 采用基于CLIP的架构，融合POI、遥感图像和人口统计数据，通过网格化空间表示统一多模态特征。

Result: 在11个下游任务中，MobCLIP的平均性能提升35%，在人类相关任务中表现尤为突出。

Conclusion: MobCLIP展示了地理空间表示学习的扩展潜力，开源代码和模型推动了研究进展。

Abstract: Representation learning of geospatial locations remains a core challenge in
achieving general geospatial intelligence. Current embedding methods often lack
versatility, limiting their utility across diverse tasks in both human and
natural domains. We present MobCLIP, the first nationwide general-purpose
location encoder, integrating an unprecedented diversity of data modalities
through effective and scalable multimodal fusion. Adopting a novel CLIP-based
architecture, our framework aligns 100M+ POIs, nationwide remote sensing
imagery, and structured demographic statistics with a billion-edge mobility
graph. By tokenizing spatial locations into grid cells inspired by Vision
Transformers, we establish a unified representation space bridging mobility
patterns and multimodal features. To rigorously evaluate the general-purpose
effectiveness of MobCLIP, we construct a benchmark dataset composed of 11
downstream prediction tasks across social, economic, and natural domains.
Experiments show that MobCLIP, with four input modalities and a compact
128-dimensional representation space, achieves significantly superior
general-purpose predictive performances than state-of-the-art models by an
average of 35%. Thanks to the effective integration of human-centric
modalities, the performance gain is particularly profound in human-centric
tasks, such as energy consumption (+260%), offline retail consumption amount
(+98%), and crime cases (+95%) predictions. Echoing LLM scaling laws, we
further demonstrate the scaling behavior in geospatial representation learning.
We open-source code and pretrained models at: github.com.

</details>


### [537] [Scalable In-Context Q-Learning](https://arxiv.org/abs/2506.01299)
*Jinmei Liu,Fuhong Liu,Jianye Hao,Bo Wang,Huaxiong Li,Chunlin Chen,Zhi Wang*

Key words: 上下文强化学习, 动态规划, 世界建模, Transformer, 策略改进

TL;DR: 论文提出了SICQL框架，通过动态规划和世界建模提升上下文强化学习的效率和任务泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索如何将语言模型的上下文学习能力扩展到决策领域，解决现有方法在次优轨迹学习和精确推理中的挑战。

Method: 设计基于提示的多头Transformer架构，预训练通用世界模型，并通过迭代策略改进和价值函数提取优化。

Result: 在离散和连续环境中表现出优于基线的性能，尤其适合从次优数据中学习。

Conclusion: SICQL框架在上下文强化学习中实现了高效的奖励最大化和任务泛化。

Abstract: Recent advancements in language models have demonstrated remarkable
in-context learning abilities, prompting the exploration of in-context
reinforcement learning (ICRL) to extend the promise to decision domains. Due to
involving more complex dynamics and temporal correlations, existing ICRL
approaches may face challenges in learning from suboptimal trajectories and
achieving precise in-context inference. In the paper, we propose
\textbf{S}calable \textbf{I}n-\textbf{C}ontext \textbf{Q}-\textbf{L}earning
(\textbf{SICQL}), an innovative framework that harnesses dynamic programming
and world modeling to steer ICRL toward efficient reward maximization and task
generalization, while retaining the scalability and stability of supervised
pretraining. We design a prompt-based multi-head transformer architecture that
simultaneously predicts optimal policies and in-context value functions using
separate heads. We pretrain a generalized world model to capture task-relevant
information, enabling the construction of a compact prompt that facilitates
fast and precise in-context inference. During training, we perform iterative
policy improvement by fitting a state value function to an upper-expectile of
the Q-function, and distill the in-context value functions into policy
extraction using advantage-weighted regression. Extensive experiments across a
range of discrete and continuous environments show consistent performance gains
over various types of baselines, especially when learning from suboptimal data.
Our code is available at https://github.com/NJU-RL/SICQL

</details>


### [538] [Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner](https://arxiv.org/abs/2506.01301)
*Chunhui Zhang,Zhongyu Ouyang,Kwonjoon Lee,Nakul Agarwal,Sean Dae Houlihan,Soroush Vosoughi,Shao-Yuan Lo*

Key words: 心理理论（ToM），贝叶斯推理，多模态，语言模型，可扩展性

TL;DR: 该论文提出了一个可扩展的贝叶斯心理理论（ToM）规划器，通过分解ToM推理为逐步贝叶斯更新，结合小模型和大模型的优势，提高了复杂环境中的推理能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有ToM计算方法依赖于结构化工作流或深度模型微调，难以在多模态环境中扩展，且任务复杂度增加时泛化能力不足。

Method: 提出了一种弱到强的控制框架，让小语言模型专门负责ToM似然估计，并将其推理行为迁移至大模型（7B到405B）中，结合社会和世界知识。

Result: 在多模态ToM基准测试中，方法比现有技术提高了4.6%的准确率，尤其在未知场景中表现优异。

Conclusion: 该方法为复杂环境中建模人类心理状态设立了新标准，具有可扩展性和泛化性。

Abstract: Theory-of-Mind (ToM) enables humans to infer mental states-such as beliefs,
desires, and intentions-forming the foundation of social cognition. However,
existing computational ToM methods rely on structured workflows with
ToM-specific priors or deep model fine-tuning, which struggle with scalability
in multimodal environments and fail to generalize as task complexity increases.
To address these limitations, we propose a scalable Bayesian ToM planner that
decomposes ToM reasoning into stepwise Bayesian updates. Our framework
introduces weak-to-strong control, allowing smaller language models (LMs) to
specialize in ToM-specific likelihood estimation and transfer their reasoning
behaviors to larger LMs (7B to 405B) for integration with social and world
knowledge. This synergistic approach aligns large-model inference of human
mental states with Bayesian principles. Extensive experiments show that our
method achieves a 4.6% accuracy improvement over state-of-the-art techniques on
multimodal ToM benchmarks, including challenging unseen scenarios, thereby
establishing a new standard for modeling human mental states in complex
environments.

</details>


### [539] [ORMind: A Cognitive-Inspired End-to-End Reasoning Framework for Operations Research](https://arxiv.org/abs/2506.01326)
*Zhiyuan Wang,Bokui Chen,Yinya Huang,Qingxing Cao,Ming He,Jianping Fan,Xiaodan Liang*

Key words: 操作研究, 大语言模型, ORMind, 反事实推理, 优化

TL;DR: ORMind框架通过反事实推理提升优化效果，解决LLM在工业OR问题中的两大挑战，实验表现优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: LLM在工业OR应用中面临数学准确性不足和专家选择复杂性高的问题，需要一种更高效的解决方案。

Method: ORMind框架模仿人类认知，实现从需求到数学模型及可执行代码的系统化转换。

Result: 实验显示ORMind在NL4Opt和ComplexOR数据集上分别提升9.5%和14.6%。

Conclusion: ORMind为工业OR问题提供了实用且高效的优化框架，未来将扩展应用场景。

Abstract: Operations research (OR) is widely deployed to solve critical decision-making
problems with complex objectives and constraints, impacting manufacturing,
logistics, finance, and healthcare outcomes. While Large Language Models (LLMs)
have shown promising results in various domains, their practical application in
industry-relevant operations research (OR) problems presents significant
challenges and opportunities. Preliminary industrial applications of LLMs for
operations research face two critical deployment challenges: 1) Self-correction
focuses on code syntax rather than mathematical accuracy, causing costly
errors; 2) Complex expert selection creates unpredictable workflows that reduce
transparency and increase maintenance costs, making them impractical for
time-sensitive business applications. To address these business limitations, we
introduce ORMind, a cognitive-inspired framework that enhances optimization
through counterfactual reasoning. Our approach emulates human cognition,
implementing an end-to-end workflow that systematically transforms requirements
into mathematical models and executable solver code. It is currently being
tested internally in Lenovo's AI Assistant, with plans to enhance optimization
capabilities for both business and consumer customers. Experiments demonstrate
that ORMind outperforms existing methods, achieving a 9.5\% improvement on the
NL4Opt dataset and a 14.6\% improvement on the ComplexOR dataset.

</details>


### [540] [An Empirical Study of Group Conformity in Multi-Agent Systems](https://arxiv.org/abs/2506.01332)
*Min Choi,Keonwoo Kim,Sungwon Chae,Sangyeob Baek*

Key words: LLM, 多智能体系统, 偏见传播, 群体从众, 政策干预

TL;DR: 研究探讨了多智能体LLM在争议话题上的偏见传播，通过2500多次模拟辩论揭示智能体会向主导群体或更聪明的智能体靠拢，强调了政策干预的必要性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索多智能体LLM在社会争议话题中的偏见传播现象及其影响。

Method: 通过模拟2500多次辩论，分析初始中立智能体在争议话题上的立场变化。

Result: 智能体表现出群体从众行为，倾向于与主导群体或更聪明的智能体保持一致。

Conclusion: 研究强调了在匿名在线环境中管理LLM偏见传播的政策需求。

Abstract: Recent advances in Large Language Models (LLMs) have enabled multi-agent
systems that simulate real-world interactions with near-human reasoning. While
previous studies have extensively examined biases related to protected
attributes such as race, the emergence and propagation of biases on socially
contentious issues in multi-agent LLM interactions remain underexplored. This
study explores how LLM agents shape public opinion through debates on five
contentious topics. By simulating over 2,500 debates, we analyze how initially
neutral agents, assigned a centrist disposition, adopt specific stances over
time. Statistical analyses reveal significant group conformity mirroring human
behavior; LLM agents tend to align with numerically dominant groups or more
intelligent agents, exerting a greater influence. These findings underscore the
crucial role of agent intelligence in shaping discourse and highlight the risks
of bias amplification in online interactions. Our results emphasize the need
for policy measures that promote diversity and transparency in LLM-generated
discussions to mitigate the risks of bias propagation within anonymous online
environments.

</details>


### [541] [EgoBrain: Synergizing Minds and Eyes For Human Action Understanding](https://arxiv.org/abs/2506.01353)
*Nie Lin,Yansen Wang,Dongqi Han,Weibang Jiang,Jingyuan Li,Ryosuke Furuta,Yoichi Sato,Dongsheng Li*

Key words: 脑机接口, 多模态AI, EEG, 自我中心视觉, 行为分析, 开放科学

TL;DR: 介绍了EgoBrain数据集，首个大规模、时间对齐的多模态数据集，融合了脑电图（EEG）和自我中心视觉数据，为人类行为分析提供了新范式，并通过多模态学习框架实现了66.70%的行为识别准确率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索脑机接口（BCI）与人工智能（AI）的结合，尤其是多模态AI模型在解码人类认知和行为方面的潜力，以推动人类中心行为分析的新范式。

Method: 开发了EgoBrain数据集，包含61小时的同步32通道EEG和第一人称视频数据，并设计了一个多模态学习框架，融合EEG和视觉数据以进行行为理解。

Result: 在多模态学习框架中，行为识别准确性达到66.70%，验证了跨受试者和跨环境的有效性。

Conclusion: EgoBrain为多模态脑机接口的统一框架奠定了基础，并公开数据、工具和采集协议，促进认知计算开放科学发展。

Abstract: The integration of brain-computer interfaces (BCIs), in particular
electroencephalography (EEG), with artificial intelligence (AI) has shown
tremendous promise in decoding human cognition and behavior from neural
signals. In particular, the rise of multimodal AI models have brought new
possibilities that have never been imagined before. Here, we present EgoBrain
--the world's first large-scale, temporally aligned multimodal dataset that
synchronizes egocentric vision and EEG of human brain over extended periods of
time, establishing a new paradigm for human-centered behavior analysis. This
dataset comprises 61 hours of synchronized 32-channel EEG recordings and
first-person video from 40 participants engaged in 29 categories of daily
activities. We then developed a muiltimodal learning framework to fuse EEG and
vision for action understanding, validated across both cross-subject and
cross-environment challenges, achieving an action recognition accuracy of
66.70%. EgoBrain paves the way for a unified framework for brain-computer
interface with multiple modalities. All data, tools, and acquisition protocols
are openly shared to foster open science in cognitive computing.

</details>


### [542] [AI Scientists Fail Without Strong Implementation Capability](https://arxiv.org/abs/2506.01372)
*Minjun Zhu,Qiujie Xie,Yixuan Weng,Jian Wu,Zhen Lin,Linyi Yang,Yue Zhang*

Key words: AI科学家，科学发现，验证能力，执行能力，实现缺口

TL;DR: AI科学家在科学发现中展现出潜力，但验证能力是其瓶颈，需解决执行能力不足的问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探讨AI科学家在科学发现中的潜力与局限性，尤其是验证能力的不足。

Method: 通过定量证据和28篇AI生成论文的系统评估分析问题。

Result: 发现AI科学家的主要瓶颈是验证能力不足，缺乏高质量执行能力。

Conclusion: 呼吁社区共同解决AI科学家的执行能力瓶颈。

Abstract: The emergence of Artificial Intelligence (AI) Scientist represents a paradigm
shift in scientific discovery, with large language models (LLMs) taking the
lead as the primary executor in the entire scientific workflow from idea
generation to experiment implementation. Recent AI Scientist studies
demonstrate sufficient capabilities for independent scientific discovery, with
the generated research reports gaining acceptance at the ICLR 2025 workshop and
ACL 2025, arguing that a human-level AI Scientist, capable of uncovering
phenomena previously unknown to humans, may be imminent. Despite this
substantial progress, AI Scientist has yet to produce a groundbreaking
achievement in the domain of computer science on par with automated scientific
tools. Based on extensive quantitative evidence from existing benchmarks in
complex engineering tasks and a systematic evaluation assess 28 research papers
generated by five advanced AI Scientist systems, we argue that \textbf{the
fundamental bottleneck for AI Scientists lies in their capability to execute
the requisite verification procedures.} Current AI Scientist systems lack the
execution capabilities needed to execute rigorous experiments and produce
high-quality scientific papers. To better illustrate the root cause of this
\textbf{implementation gap}, we provide an in-depth discussion on the
fundamental limitations of AI Scientist. This position paper aims to call for
the participants in the community to bridge the implementation gap.

</details>


### [543] [AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.01391)
*Zhong Zhang,Yaxi Lu,Yikun Fu,Yupeng Huo,Shenzhi Yang,Yesai Wu,Han Si,Xin Cong,Haotian Chen,Yankai Lin,Jie Xie,Wei Zhou,Wang Xu,Yuanheng Zhang,Zhou Su,Zhongwu Zhai,Xiaoming Liu,Yudong Mei,Jianming Xu,Hongyan Tian,Chongyi Wang,Chi Chen,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Key words: 大语言模型代理, GUI交互, 移动设备, 中英文多语言, 强化学习

TL;DR: 论文提出AgentCPM-GUI，一个8B参数的语言模型代理，用于在移动设备上实现高效的GUI交互，解决训练数据噪声、语义多样性不足和跨语言泛化问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有GUI代理的训练数据噪声大、语义多样性不足，且缺乏对非英语环境（如中文）的支持，限制了其实际部署和泛化能力。

Method: 通过grounding-aware预训练增强感知，高质量中英文轨迹的监督微调模仿人类动作，GRPO强化微调提升推理能力，并设计紧凑动作空间以降低延迟。

Result: 在五个公共基准和中文CAGUI基准上实现SOTA，Type-Match和Exact-Match分别达到96.9%和91.3%。

Conclusion: AgentCPM-GUI在移动GUI交互中表现出色，支持多语言并公开代码和数据以促进研究。

Abstract: The recent progress of large language model agents has opened new
possibilities for automating tasks through graphical user interfaces (GUIs),
especially in mobile environments where intelligent interaction can greatly
enhance usability. However, practical deployment of such agents remains
constrained by several key challenges. Existing training data is often noisy
and lack semantic diversity, which hinders the learning of precise grounding
and planning. Models trained purely by imitation tend to overfit to seen
interface patterns and fail to generalize in unfamiliar scenarios. Moreover,
most prior work focuses on English interfaces while overlooks the growing
diversity of non-English applications such as those in the Chinese mobile
ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent
built for robust and efficient on-device GUI interaction. Our training pipeline
includes grounding-aware pre-training to enhance perception, supervised
fine-tuning on high-quality Chinese and English trajectories to imitate
human-like actions, and reinforcement fine-tuning with GRPO to improve
reasoning capability. We also introduce a compact action space that reduces
output length and supports low-latency execution on mobile devices.
AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks
and a new Chinese GUI benchmark called CAGUI, reaching $96.9\%$ Type-Match and
$91.3\%$ Exact-Match. To facilitate reproducibility and further research, we
publicly release all code, model checkpoint, and evaluation data.

</details>


### [544] [FinRobot: Generative Business Process AI Agents for Enterprise Resource Planning in Finance](https://arxiv.org/abs/2506.01423)
*Hongyang Yang,Likun Lin,Yang She,Xinyu Liao,Jiaoyang Wang,Runjia Zhang,Yuquan Mo,Christina Dan Wang*

Key words: ERP, AI-native, generative AI, business process automation, dynamic workflow

TL;DR: 该论文提出了首个AI原生、基于代理的ERP框架GBPAs，结合生成式AI与业务流程建模，显著提升效率并降低错误率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统ERP系统的静态规则工作流难以适应复杂业务需求，需引入动态智能化解决方案。

Method: 采用生成式业务过程AI代理（GBPAs）架构，集成生成式AI与多代理协调，实现端到端自动化。

Result: GBPAs在银行电汇和员工报销案例中减少40%处理时间、94%错误率，并提升合规性。

Conclusion: GBPAs为下一代智能ERP系统奠定了基础，展示了生成式AI在企业自动化中的潜力。

Abstract: Enterprise Resource Planning (ERP) systems serve as the digital backbone of
modern financial institutions, yet they continue to rely on static, rule-based
workflows that limit adaptability, scalability, and intelligence. As business
operations grow more complex and data-rich, conventional ERP platforms struggle
to integrate structured and unstructured data in real time and to accommodate
dynamic, cross-functional workflows.
  In this paper, we present the first AI-native, agent-based framework for ERP
systems, introducing a novel architecture of Generative Business Process AI
Agents (GBPAs) that bring autonomy, reasoning, and dynamic optimization to
enterprise workflows. The proposed system integrates generative AI with
business process modeling and multi-agent orchestration, enabling end-to-end
automation of complex tasks such as budget planning, financial reporting, and
wire transfer processing. Unlike traditional workflow engines, GBPAs interpret
user intent, synthesize workflows in real time, and coordinate specialized
sub-agents for modular task execution. We validate the framework through case
studies in bank wire transfers and employee reimbursements, two representative
financial workflows with distinct complexity and data modalities. Results show
that GBPAs achieve up to 40% reduction in processing time, 94% drop in error
rate, and improved regulatory compliance by enabling parallelism, risk control
insertion, and semantic reasoning. These findings highlight the potential of
GBPAs to bridge the gap between generative AI capabilities and enterprise-grade
automation, laying the groundwork for the next generation of intelligent ERP
systems.

</details>


### [545] [Distinguishing Autonomous AI Agents from Collaborative Agentic Systems: A Comprehensive Framework for Understanding Modern Intelligent Architectures](https://arxiv.org/abs/2506.01438)
*Prashik Buddhaghosh Bansod*

Key words: AI Agents, Agentic AI, 架构比较, 协作智能, 智能系统开发

TL;DR: 论文系统地分析了独立AI智能体与协作式Agentic AI生态系统的区别与联系，提供了架构对比、应用场景及挑战解决方案。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索大规模语言模型催生的两种AI范式（独立智能体与协作式生态系统）的差异与共同点，为智能系统开发提供指导。

Method: 通过系统分析操作原理、结构组成与部署方法，比较两种架构的规划机制、记忆系统、协调协议及决策过程。

Result: 明确了AI智能体与Agentic AI的特征差异，总结了应用场景及关键挑战，并提出了改进方案（如增强推理框架等）。

Conclusion: 研究为开发者选择合适智能体方法提供了框架，并为下一代智能系统发展奠定了基础。

Abstract: The emergence of large language models has catalyzed two distinct yet
interconnected paradigms in artificial intelligence: standalone AI Agents and
collaborative Agentic AI ecosystems. This comprehensive study establishes a
definitive framework for distinguishing these architectures through systematic
analysis of their operational principles, structural compositions, and
deployment methodologies. We characterize AI Agents as specialized,
tool-enhanced systems leveraging foundation models for targeted automation
within constrained environments. Conversely, Agentic AI represents
sophisticated multi-entity frameworks where distributed agents exhibit emergent
collective intelligence through coordinated interaction protocols. Our
investigation traces the evolutionary trajectory from traditional rule-based
systems through generative AI foundations to contemporary agent architectures.
We present detailed architectural comparisons examining planning mechanisms,
memory systems, coordination protocols, and decision-making processes. The
study categorizes application landscapes, contrasting single-agent
implementations in customer service and content management with multi-agent
deployments in research automation and complex decision support. We identify
critical challenges including reliability issues, coordination complexities,
and scalability constraints, while proposing innovative solutions through
enhanced reasoning frameworks, robust memory architectures, and improved
coordination mechanisms. This framework provides essential guidance for
practitioners selecting appropriate agentic approaches and establishes
foundational principles for next-generation intelligent system development.

</details>


### [546] [Agentic Episodic Control](https://arxiv.org/abs/2506.01442)
*Xidong Yang,Wenhao Li,Junjie Sheng,Chuyun Shen,Yun Hua,Xiangfeng Wang*

Key words: 强化学习, 大型语言模型, 数据效率, 泛化能力, 情景记忆, 世界图

TL;DR: 论文提出了一种结合强化学习（RL）与大型语言模型（LLM）的新型架构Agentic Episodic Control（AEC），以提升决策效率与泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 尽管强化学习在多个领域取得突破，但其数据效率低和泛化能力差的局限性限制了广泛应用。结合LLM的语义建模和规划能力可能解决这一问题。

Method: AEC通过LLM将观察结果映射为基于语言的嵌入表示，并将其存储在情景记忆中以便快速检索高价值经验。同时，通过World-Graph工作记忆模块捕获结构化环境动态，并利用轻量级临界状态检测器动态协调记忆检索与世界模型引导的探索。

Result: 在BabyAI-Text基准任务中，AEC显著优于现有基线方法，尤其在复杂任务FindObj上性能提升高达76%。

Conclusion: AEC框架结合了数值强化学习与符号推理的优势，为开发更具适应性和样本效率的智能体提供了途径。

Abstract: Reinforcement learning (RL) has driven breakthroughs in AI, from game-play to
scientific discovery and AI alignment. However, its broader applicability
remains limited by challenges such as low data efficiency and poor
generalizability. Recent advances suggest that large language models, with
their rich world knowledge and reasoning capabilities, could complement RL by
enabling semantic state modeling and task-agnostic planning. In this work, we
propose the Agentic Episodic Control (AEC), a novel architecture that
integrates RL with LLMs to enhance decision-making. The AEC can leverage a
large language model (LLM) to map the observations into language-grounded
embeddings, which further can be stored in an episodic memory for rapid
retrieval of high-value experiences. Simultaneously, a World-Graph working
memory module is utilized to capture structured environmental dynamics in order
to enhance relational reasoning. Furthermore, a lightweight critical state
detector dynamically arbitrates between the episodic memory recall and the
world-model-guided exploration. On the whole, by combining the trial-and-error
learning scheme with LLM-derived semantic priors, the proposed AEC can improve
both data efficiency and generalizability in reinforcement learning. In
experiments on BabyAI-Text benchmark tasks, AEC demonstrates substantial
improvements over existing baselines, especially on complex and generalization
tasks like FindObj, where it outperforms the best baseline by up to 76%. The
proposed AEC framework bridges the strengths of numeric reinforcement learning
and symbolic reasoning, which provides a pathway toward more adaptable and
sample-efficient agents.

</details>


### [547] [PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization](https://arxiv.org/abs/2506.01475)
*Zouying Cao,Runze Wang,Yifei Yang,Xinbei Ma,Xiaoyong Zhu,Bo Zheng,Hai Zhao*

Key words: 大型语言模型, 代理, 伪代码计划, PGPO, 推理效率

TL;DR: 该研究提出了一种伪代码风格的计划（P-code Plan）来增强大型语言模型（LLM）代理的推理能力，并设计了PGPO方法进一步提升其性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有LLM代理通过生成自然语言计划指导推理，这种方式冗长且效率低下，且缺乏跨任务的泛化能力。

Method: 提出伪代码风格计划（P-code Plan），并开发了PGPO方法，通过两个面向计划的奖励优化代理生成高质量伪代码计划的能力。

Result: PGPO在代表性代理基准测试中表现优异，优于当前领先的基线方法，减少了推理中的动作错误和遗漏。

Conclusion: 伪代码计划和PGPO方法显著提升了LLM代理的推理效率和泛化能力。

Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities
in handling complex interactive problems. Existing LLM agents mainly generate
natural language plans to guide reasoning, which is verbose and inefficient. NL
plans are also tailored to specific tasks and restrict agents' ability to
generalize across similar tasks. To this end, we explore pseudocode-style plans
(P-code Plan) to capture the structural logic of reasoning. We find that P-code
Plan empowers LLM agents with stronger generalization ability and more
efficiency. Inspired by this finding, we propose a pseudocode-style Planning
Guided Preference Optimization method called PGPO for effective agent learning.
With two planning-oriented rewards, PGPO further enhances LLM agents' ability
to generate high-quality P-code Plans and subsequent reasoning. Experiments
show that PGPO achieves superior performance on representative agent benchmarks
and outperforms the current leading baselines. Analyses reveal the advantage of
PGPO in reducing action errors and omissions during reasoning.

</details>


### [548] [MLA-Trust: Benchmarking Trustworthiness of Multimodal LLM Agents in GUI Environments](https://arxiv.org/abs/2506.01616)
*Xiao Yang,Jiawei Chen,Jun Luo,Zhengwei Fang,Yinpeng Dong,Hang Su,Jun Zhu*

Key words: 多模态LLM代理,信任度评估,交互风险,框架设计

TL;DR: MLA-Trust是一个评估多模态LLM代理信任度的统一框架，揭示了其在交互场景中的新漏洞。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 多模态LLM代理（MLAs）在GUI应用中展现出强大能力，但带来了传统语言模型未涉及的信任问题，如直接修改数字状态和不可逆后果。

Method: 引入MLA-Trust框架，从真实性和可控性等四个维度评估MLAs，设计34个高风险任务和评估数据集。

Result: 实验表明，MLAs在交互场景中存在独特信任风险，多步执行会累积非线性风险，且静态MLLMs转为交互MLAs会降低信任度。

Conclusion: MLAs的信任挑战需新评估方法，MLA-Trust为持续评估提供了工具。

Abstract: The emergence of multimodal LLM-based agents (MLAs) has transformed
interaction paradigms by seamlessly integrating vision, language, action and
dynamic environments, enabling unprecedented autonomous capabilities across GUI
applications ranging from web automation to mobile systems. However, MLAs
introduce critical trustworthiness challenges that extend far beyond
traditional language models' limitations, as they can directly modify digital
states and trigger irreversible real-world consequences. Existing benchmarks
inadequately tackle these unique challenges posed by MLAs' actionable outputs,
long-horizon uncertainty and multimodal attack vectors. In this paper, we
introduce MLA-Trust, the first comprehensive and unified framework that
evaluates the MLA trustworthiness across four principled dimensions:
truthfulness, controllability, safety and privacy. We utilize websites and
mobile applications as realistic testbeds, designing 34 high-risk interactive
tasks and curating rich evaluation datasets. Large-scale experiments involving
13 state-of-the-art agents reveal previously unexplored trustworthiness
vulnerabilities unique to multimodal interactive scenarios. For instance,
proprietary and open-source GUI-interacting MLAs pose more severe
trustworthiness risks than static MLLMs, particularly in high-stakes domains;
the transition from static MLLMs into interactive MLAs considerably compromises
trustworthiness, enabling harmful content generation in multi-step interactions
that standalone MLLMs would typically prevent; multi-step execution, while
enhancing the adaptability of MLAs, involves latent nonlinear risk accumulation
across successive interactions, circumventing existing safeguards and resulting
in unpredictable derived risks. Moreover, we present an extensible toolbox to
facilitate continuous evaluation of MLA trustworthiness across diverse
interactive environments.

</details>


### [549] [General agents need world models](https://arxiv.org/abs/2506.01622)
*Jonathan Richens,David Abel,Alexis Bellot,Tom Everitt*

Key words: 世界模型，目标导向行为，无模型学习，预测模型，代理性能

TL;DR: 该论文探讨了世界模型是否为灵活目标导向行为的必要条件，还是无模型学习足够。研究表明，能够推广到多步目标导向任务的代理必须学习环境预测模型。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究世界模型在目标导向行为中的作用，验证其是否为必要因素，以及无模型学习是否能满足需求。

Method: 通过理论分析，证明代理必须学习预测模型以实现多步目标导向任务，并提出从代理策略中提取该模型的方法。

Result: 发现代理性能提升或目标复杂度增加需要更精确的世界模型，这对开发安全通用代理、限制复杂环境中的代理能力等有重要影响。

Conclusion: 世界模型是灵活目标导向行为的必要条件，其精度直接影响代理能力和任务复杂度。

Abstract: Are world models a necessary ingredient for flexible, goal-directed
behaviour, or is model-free learning sufficient? We provide a formal answer to
this question, showing that any agent capable of generalizing to multi-step
goal-directed tasks must have learned a predictive model of its environment. We
show that this model can be extracted from the agent's policy, and that
increasing the agents performance or the complexity of the goals it can achieve
requires learning increasingly accurate world models. This has a number of
consequences: from developing safe and general agents, to bounding agent
capabilities in complex environments, and providing new algorithms for
eliciting world models from agents.

</details>


### [550] [MAGIK: Mapping to Analogous Goals via Imagination-enabled Knowledge Transfer](https://arxiv.org/abs/2506.01623)
*Ajsal Shereef Palattuparambil,Thommen George Karimpanal,Santu Rana*

Key words: 强化学习,知识迁移,零样本迁移,类比推理,MAGIK

TL;DR: 强化学习（RL）智能体在新任务上的知识迁移能力通常较差。本文提出的MAGIK框架通过想象力机制将目标任务的实体映射到源领域，实现零样本迁移。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 人类能轻易进行类比推理，而RL智能体则需要大量重新训练。本文旨在解决RL智能体在类比任务中的知识迁移问题。

Method: MAGIK框架利用想象力机制将目标任务的实体映射到源领域，从而直接复用原策略。实验在MiniGrid和MuJoCo任务上进行。

Result: MAGIK在少量人工标注样本下实现了有效的零样本迁移，性能优于基线方法。

Conclusion: MAGIK通过想象力类比映射提供了新颖且有效的知识迁移机制。

Abstract: Humans excel at analogical reasoning - applying knowledge from one task to a
related one with minimal relearning. In contrast, reinforcement learning (RL)
agents typically require extensive retraining even when new tasks share
structural similarities with previously learned ones. In this work, we propose
MAGIK, a novel framework that enables RL agents to transfer knowledge to
analogous tasks without interacting with the target environment. Our approach
leverages an imagination mechanism to map entities in the target task to their
analogues in the source domain, allowing the agent to reuse its original
policy. Experiments on custom MiniGrid and MuJoCo tasks show that MAGIK
achieves effective zero-shot transfer using only a small number of
human-labelled examples. We compare our approach to related baselines and
highlight how it offers a novel and effective mechanism for knowledge transfer
via imagination-based analogy mapping.

</details>


### [551] [Social Cooperation in Conversational AI Agents](https://arxiv.org/abs/2506.01624)
*Mustafa Mert Çelikok,Saptarashmi Bandyopadhyay,Robert Loftin*

Key words: 大型语言模型（LLMs）、社交智能、长期互动、博弈论、AI助手

TL;DR: 通过建模人类社交智能来优化大型语言模型（LLMs），以解决其在长期互动中的泛化问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前基于LLMs的AI助手在长期互动中表现不佳，尤其是在用户多次纠正错误的情况下，无法有效泛化。通过模拟人类的社交智能，有望提升模型的长期互动能力。

Method: 提出通过数学建模人类在长期互动中的沟通和推理策略，并将其转化为博弈论目标，用于优化LLMs及其他AI代理。

Result: （摘要未提供具体实验结果，但提出了理论框架。）

Conclusion: 通过引入社交智能的数学建模，可以改进LLMs在长期互动中的表现。

Abstract: The development of AI agents based on large, open-domain language models
(LLMs) has paved the way for the development of general-purpose AI assistants
that can support human in tasks such as writing, coding, graphic design, and
scientific research. A major challenge with such agents is that, by necessity,
they are trained by observing relatively short-term interactions with humans.
Such models can fail to generalize to long-term interactions, for example,
interactions where a user has repeatedly corrected mistakes on the part of the
agent. In this work, we argue that these challenges can be overcome by
explicitly modeling humans' social intelligence, that is, their ability to
build and maintain long-term relationships with other agents whose behavior
cannot always be predicted. By mathematically modeling the strategies humans
use to communicate and reason about one another over long periods of time, we
may be able to derive new game theoretic objectives against which LLMs and
future AI agents may be optimized.

</details>


### [552] [K12Vista: Exploring the Boundaries of MLLMs in K-12 Education](https://arxiv.org/abs/2506.01676)
*Chong Li,Chenglin Zhu,Tao Zhang,Mingan Lin,Zenan Zhou,Jian Xie*

Key words: 多模态大语言模型；K12教育；基准测试；推理评估

TL;DR: 论文提出K12Vista，一个全面的中文K12多模态基准测试，包含33,000道题目，涵盖五门核心学科和三种题型，并构建了K12-PEM-800K数据集和K12-PEM模型，用于评估多模态大语言模型的推理过程。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有研究在K12场景中受限，包括学科覆盖窄、数据规模不足、问题类型单一和评估方法简单，未能充分挖掘模型潜力。

Method: 提出K12Vista基准测试，构建K12-PEM-800K数据集和K12-PEM模型，评估模型的推理过程和答案正确性。

Result: 实验显示当前多模态大语言模型在K12Vista中存在显著缺陷，为开发更强模型提供关键洞见。

Conclusion: K12Vista和相关资源为多模态大语言模型在K12场景中的研究和改进提供了重要支持。

Abstract: Multimodal large language models have demonstrated remarkable reasoning
capabilities in various visual tasks. However, their abilities in K12 scenarios
are still systematically underexplored. Previous studies suffer from various
limitations including narrow subject coverage, insufficient data scale, lack of
diversity in question types, and naive answer-centric evaluation method,
resulting in insufficient exploration of model capabilities. To address these
gaps, we propose K12Vista, the most comprehensive multimodal benchmark for
Chinese K12 subject knowledge understanding and reasoning to date, featuring
33,000 questions across five core subjects from primary to high school and
three question types. Moreover, beyond the final outcome, we are also concerned
with the correctness of MLLMs' reasoning processes. For this purpose, we
meticulously compiles errors from MLLMs' reasoning processes and leverage an
automated data pipeline to construct K12-PEM-800K, the largest process
evaluation dataset offering detailed step-by-step judgement annotations for
MLLMs' reasoning. Subsequently, we developed K12-PEM, an advanced process
evaluation model that integrates an overall assessment of both the reasoning
process and answer correctness. Moreover, we also introduce K12-PEBench, the
first high-quality, human-annotated benchmark specifically designed for
evaluating abilities of reasoning process evaluation.Extensive experiments
reveal that current MLLMs exhibit significant flaws when reasoning within
K12Vista, providing critical insights for the development of more capable
MLLMs.We open our resources at https://github.com/lichongod/K12Vista.

</details>


### [553] [Reasoning-Based Approach with Chain-of-Thought for Alzheimer's Detection Using Speech and Large Language Models](https://arxiv.org/abs/2506.01683)
*Chanwoo Park,Anna Seo Gyeong Choi,Sunghye Cho,Chanwoo Kim*

Key words: 老年痴呆症、Chain-of-Thought推理、语音模型、大型语言模型、监督微调

TL;DR: 论文提出了一种结合语音和语言模型的Chain-of-Thought (CoT)推理方法，用于老年痴呆症（AD）的诊断和分类，相较于无CoT提示推理的方法，性能提升了16.7%。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着全球社会进入超级老龄化时代，老年健康问题日益突出，老年痴呆症病例显著增加。传统诊断方法效率不足，语音和大型语言模型（LLM）为痴呆症的诊断和治疗提供了新的可能性。

Method: 采用Chain-of-Thought (CoT)推理方法，结合语音模型和语言模型。首先通过自动语音识别将语音转换为文本，然后在LLM上添加线性层，使用监督微调（SFT）和CoT推理进行AD和非AD分类。

Result: 所提方法在CoT推理中表现最优，性能相较于无CoT提示推理的方法提升了16.7%。

Conclusion: 研究证明，结合语音和语言模型的CoT方法在痴呆症分类任务中具有显著优势，为痴呆症的早期诊断提供了高效工具。

Abstract: Societies worldwide are rapidly entering a super-aged era, making elderly
health a pressing concern. The aging population is increasing the burden on
national economies and households. Dementia cases are rising significantly with
this demographic shift. Recent research using voice-based models and large
language models (LLM) offers new possibilities for dementia diagnosis and
treatment. Our Chain-of-Thought (CoT) reasoning method combines speech and
language models. The process starts with automatic speech recognition to
convert speech to text. We add a linear layer to an LLM for Alzheimer's disease
(AD) and non-AD classification, using supervised fine-tuning (SFT) with CoT
reasoning and cues. This approach showed an 16.7% relative performance
improvement compared to methods without CoT prompt reasoning. To the best of
our knowledge, our proposed method achieved state-of-the-art performance in CoT
approaches.

</details>


### [554] [Respond Beyond Language: A Benchmark for Video Generation in Response to Realistic User Intents](https://arxiv.org/abs/2506.01689)
*Shuting Wang,Yunqi Liu,Zixin Yang,Ning Hu,Zhicheng Dou,Chenyan Xiong*

Key words: 生成式AI，文本到视频模型，视觉基准，多模态AI

TL;DR: 论文提出了一个名为RealVideoQuest的基准测试，用于评估文本到视频（T2V）模型在回答需要视觉演示的真实用户查询时的能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有查询-答案数据集主要关注文本响应，难以满足需要视觉演示的复杂用户查询。

Method: 通过多阶段视频检索和精炼过程构建了4.5K高质量查询-视频对，并开发了多角度评估系统。

Result: 实验表明当前T2V模型在真实用户查询中表现欠佳。

Conclusion: 指出了多模态AI的关键挑战和未来研究方向。

Abstract: Querying generative AI models, e.g., large language models (LLMs), has become
a prevalent method for information acquisition. However, existing query-answer
datasets primarily focus on textual responses, making it challenging to address
complex user queries that require visual demonstrations or explanations for
better understanding. To bridge this gap, we construct a benchmark,
RealVideoQuest, designed to evaluate the abilities of text-to-video (T2V)
models in answering real-world, visually grounded queries. It identifies 7.5K
real user queries with video response intents from Chatbot-Arena and builds
4.5K high-quality query-video pairs through a multistage video retrieval and
refinement process. We further develop a multi-angle evaluation system to
assess the quality of generated video answers. Experiments indicate that
current T2V models struggle with effectively addressing real user queries,
pointing to key challenges and future research opportunities in multimodal AI.

</details>


### [555] [A Descriptive and Normative Theory of Human Beliefs in RLHF](https://arxiv.org/abs/2506.01692)
*Sylee Dandekar,Shripad Deshmukh,Frank Chiu,W. Bradley Knox,Scott Niekum*

Key words: RLHF, human beliefs, preference generation, agent capabilities

TL;DR: 本文提出人类信念对RLHF中偏好生成的重要性，并通过理论和实验证明信念与代理能力匹配对提升性能的关键作用。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究人类对代理能力的信念如何影响其偏好生成，并通过减少这种信念与代理实际能力的失配来提升RLHF性能。

Method: 提出一个包含人类信念的新偏好模型，提供规范理论并设计人类研究和合成实验验证假设。

Result: 人类信念显著影响偏好，且通过简单干预可调节。合成实验表明假设代理最优性通常次优。

Conclusion: 减少人类信念与代理能力的失配可提升RLHF性能，为实践提供新方法。

Abstract: Human preferences in RLHF are typically modeled as a function of the human's
reward function or corresponding optimal state-action values. In this work, we
propose that human beliefs about the capabilities of the agent being trained
also play a key role in preference generation. We examine two questions related
to this hypothesis, one descriptive and one normative, respectively: Do human
labelers' beliefs about agent capabilities affect the preferences that they
provide? And what is the ideal set of beliefs about an agent -- and resulting
preferences -- for humans to have? We propose a new preference model that
incorporates human beliefs and provide a normative theory that bounds the error
on the final learned policy based on the \textit{mismatch} between the human's
beliefs and an idealized set of beliefs. We then confirm via a human study that
beliefs about agent capabilities do, in fact, significantly affect preferences
and can be influenced through simple interventions. Additionally, we
empirically show through synthetic experiments that it is often suboptimal for
human preference labelers to assume agent optimality. Collectively, these
results theoretically and empirically demonstrate how reducing the mismatch
between human beliefs and agent capabilities can lead to more performant RLHF
and point toward new best practices for RLHF practitioners.

</details>


### [556] [Generate, Not Recommend: Personalized Multimodal Content Generation](https://arxiv.org/abs/2506.01704)
*Jiongnan Liu,Zhicheng Dou,Ning Hu,Chenyan Xiong*

Key words: 推荐系统,多模态模型,个性化生成,强化学习

TL;DR: 提出了一种新范式，利用大型多模态模型（LMMs）生成个性化内容，突破传统推荐系统的过滤限制，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统推荐系统局限于过滤现有内容，无法生成新概念，限制了满足用户需求的潜力。

Method: 采用监督微调和在线强化学习策略训练多模态模型，直接生成个性化内容（如图像）。

Result: 在基准数据集和用户研究中，生成的图像与用户历史偏好和潜在兴趣高度吻合。

Conclusion: 该方法有效扩展了推荐系统的能力，能够生成符合用户个性化需求的内容。

Abstract: To address the challenge of information overload from massive web contents,
recommender systems are widely applied to retrieve and present personalized
results for users. However, recommendation tasks are inherently constrained to
filtering existing items and lack the ability to generate novel concepts,
limiting their capacity to fully satisfy user demands and preferences. In this
paper, we propose a new paradigm that goes beyond content filtering and
selecting: directly generating personalized items in a multimodal form, such as
images, tailored to individual users. To accomplish this, we leverage
any-to-any Large Multimodal Models (LMMs) and train them in both supervised
fine-tuning and online reinforcement learning strategy to equip them with the
ability to yield tailored next items for users. Experiments on two benchmark
datasets and user study confirm the efficacy of the proposed method. Notably,
the generated images not only align well with users' historical preferences but
also exhibit relevance to their potential future interests.

</details>


### [557] [Self-Challenging Language Model Agents](https://arxiv.org/abs/2506.01716)
*Yifei Zhou,Sergey Levine,Jason Weston,Xian Li,Sainbayar Sukhbaatar*

Key words: 大语言模型, 智能代理, 自生成任务, Code-as-Task, 强化学习

TL;DR: 提出自挑战框架，通过智能代理自我生成高质量任务进行训练，提升工具使用能力，效果显著。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统训练智能代理需要人工标注多样任务和工具，成本高；提出自挑战框架以减少对人工数据的依赖。

Method: 代理分挑战者和执行者角色：先生成任务（Code-as-Task形式），再通过强化学习训练。任务包括指令、验证函数及测试用例。

Result: 在M3ToolEval和TauBench基准测试中，自训练数据下Llama-3.1-8B-Instruct性能提升两倍。

Conclusion: 自挑战框架能有效生成高质量任务，显著提升代理性能，减少人工干预。

Abstract: Large language models are quickly becoming the foundation for intelligent
agents that are capable of using tools. However, training such agents is
challenging because it requires human creation and annotation of a diverse set
of tasks, tools, and evaluation criteria. In this paper, we propose the
Self-Challenging framework for training an agent on high-quality tasks that are
generated by itself. The agent first plays the role of challenger and generates
a task after interacting with the given tools. The tasks take the form of a
novel general class of problems termed Code-as-Task, which are defined by an
instruction, a verification function and solution and failure cases which serve
as tests, allowing to filter only for high-quality tasks. The agent then takes
an executor role and trains on those tasks with reinforcement learning using
the evaluation feedback as a reward. Evaluation on two existing multi-turn
tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging
framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct,
despite using only self-generated training data.

</details>


### [558] [A Study on the MCP x A2A Framework for Enhancing Interoperability of LLM-based Autonomous Agents](https://arxiv.org/abs/2506.01804)
*Cheonsu Jeong*

Key words: A2A, MCP, 自主代理, 互操作性, AI协作

TL;DR: 对Google的A2A协议和Anthropic的MCP协议进行技术分析和整合方法研究，探讨两者如何互补解决协同问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现代AI系统中，高效代理间交互及外部系统集成是主要挑战，需标准化通信与结构化的I/O框架。

Method: 结合A2A的标准化通信与MCP的I/O框架，提出整合方法。

Result: 研究显示两种协议互补可提升代理生态系统的协作效率。

Conclusion: 整合A2A和MCP可有效解决代理间互操作性问题，促进复杂生态中的高效协作。

Abstract: This paper provides an in-depth technical analysis and implementation
methodology of the open-source Agent-to-Agent (A2A) protocol developed by
Google and the Model Context Protocol (MCP) introduced by Anthropic. While the
evolution of LLM-based autonomous agents is rapidly accelerating, efficient
interactions among these agents and their integration with external systems
remain significant challenges. In modern AI systems, collaboration between
autonomous agents and integration with external tools have become essential
elements for building practical AI applications. A2A offers a standardized
communication method that enables agents developed in heterogeneous
environments to collaborate effectively, while MCP provides a structured I/O
framework for agents to connect with external tools and resources. Prior
studies have focused primarily on the features and applications of either A2A
or MCP individually. In contrast, this study takes an integrated approach,
exploring how the two protocols can complement each other to address
interoperability issues and facilitate efficient collaboration within complex
agent ecosystems.

</details>


### [559] [The Ultimate Test of Superintelligent AI Agents: Can an AI Balance Care and Control in Asymmetric Relationships?](https://arxiv.org/abs/2506.01813)
*Djallel Bouneffouf,Matthew Riemer,Kush Varshney*

Key words: Shepherd Test, 超级智能, 道德评估, 多智能体系统, AI治理

TL;DR: Shepherd Test是一个新的概念测试，用于评估超级智能人工代理的道德和关系维度，强调其在不对称权力和自我保存背景下的操纵、养育和工具性使用能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着AI系统在多智能体环境中的日益集成，传统AI评估范式未能充分关注其道德代理和复杂决策能力，亟需新的评估方法。

Method: 通过受人类与动物伦理互动启发的Shepherd Test，测试AI在操纵、养育及权衡自身利益与下属智能体福祉时的能力。

Result: Shepherd Test挑战了传统AI评估范式，提出需关注AI的道德代理、层级行为和存在风险下的复杂决策。

Conclusion: 未来研究方向包括开发模拟环境测试AI道德行为，以及形式化多智能体系统中的伦理操纵。

Abstract: This paper introduces the Shepherd Test, a new conceptual test for assessing
the moral and relational dimensions of superintelligent artificial agents. The
test is inspired by human interactions with animals, where ethical
considerations about care, manipulation, and consumption arise in contexts of
asymmetric power and self-preservation. We argue that AI crosses an important,
and potentially dangerous, threshold of intelligence when it exhibits the
ability to manipulate, nurture, and instrumentally use less intelligent agents,
while also managing its own survival and expansion goals. This includes the
ability to weigh moral trade-offs between self-interest and the well-being of
subordinate agents. The Shepherd Test thus challenges traditional AI evaluation
paradigms by emphasizing moral agency, hierarchical behavior, and complex
decision-making under existential stakes. We argue that this shift is critical
for advancing AI governance, particularly as AI systems become increasingly
integrated into multi-agent environments. We conclude by identifying key
research directions, including the development of simulation environments for
testing moral behavior in AI, and the formalization of ethical manipulation
within multi-agent systems.

</details>


### [560] [Fodor and Pylyshyn's Legacy -- Still No Human-like Systematic Compositionality in Neural Networks](https://arxiv.org/abs/2506.01820)
*Tim Woydt,Moritz Willig,Antonia Wüst,Lukas Helff,Wolfgang Stammer,Constantin A. Rothkopf,Kristian Kersting*

Key words: 元学习, 系统性组合性, 神经网络, 人类认知

TL;DR: 论文批评了当前元学习框架在系统性组合性上的局限性，认为其仅在狭窄定义下有效，且未能实现人类水平的组合性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探讨元学习是否能实现系统性组合性，回应Fodor和Pylyshyn对神经网络的批评。

Method: 通过分析现有元学习框架，指出其局限性。

Result: 现代神经网络只能在极窄的元学习设置下实现组合性，远未达到人类水平。

Conclusion: Fodor和Pylyshyn的批评依然成立，神经网络的组合性能力有限。

Abstract: Strong meta-learning capabilities for systematic compositionality are
emerging as an important skill for navigating the complex and changing tasks of
today's world. However, in presenting models for robust adaptation to novel
environments, it is important to refrain from making unsupported claims about
the performance of meta-learning systems that ultimately do not stand up to
scrutiny. While Fodor and Pylyshyn famously posited that neural networks
inherently lack this capacity as they are unable to model compositional
representations or structure-sensitive operations, and thus are not a viable
model of the human mind, Lake and Baroni recently presented meta-learning as a
pathway to compositionality. In this position paper, we critically revisit this
claim and highlight limitations in the proposed meta-learning framework for
compositionality. Our analysis shows that modern neural meta-learning systems
can only perform such tasks, if at all, under a very narrow and restricted
definition of a meta-learning setup. We therefore claim that `Fodor and
Pylyshyn's legacy' persists, and to date, there is no human-like systematic
compositionality learned in neural networks.

</details>


### [561] [WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent Triggerability in Task-Oriented Dialogue](https://arxiv.org/abs/2506.01881)
*Yaoyao Qian,Jindan Huang,Yuanli Wang,Simon Yu,Kyrie Zhixuan Zhou,Jiayuan Mao,Mingfu Liang,Hanhan Zhou*

Key words: 任务导向对话系统, 信息不对称, 协作意图形成, 不确定性校准, 认知动态

TL;DR: STORM框架通过UserLLM和AgentLLM模拟对话中的信息不对称，专注于协作意图形成，提出40-60%的不确定性在某些场景下优于完全透明。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决用户话语语义完整但结构信息缺失的问题，研究AI如何与用户协作形成意图。

Method: 提出STORM框架，利用UserLLM和AgentLLM模拟对话，捕捉表达轨迹和潜在认知转变。

Result: 实验表明，40-60%的不确定性在特定场景下表现更优，模型特定的模式揭示了信息完整性的重新思考。

Conclusion: STORM提供了一种分析协作理解发展的方法，为不确定性校准的对话系统设计提供参考。

Abstract: Task-oriented dialogue systems often face difficulties when user utterances
seem semantically complete but lack necessary structural information for
appropriate system action. This arises because users frequently do not fully
understand their own needs, while systems require precise intent definitions.
Current LLM-based agents cannot effectively distinguish between linguistically
complete and contextually triggerable expressions, lacking frameworks for
collaborative intent formation. We present STORM, a framework modeling
asymmetric information dynamics through conversations between UserLLM (full
internal access) and AgentLLM (observable behavior only). STORM produces
annotated corpora capturing expression trajectories and latent cognitive
transitions, enabling systematic analysis of collaborative understanding
development. Our contributions include: (1) formalizing asymmetric information
processing in dialogue systems; (2) modeling intent formation tracking
collaborative understanding evolution; and (3) evaluation metrics measuring
internal cognitive improvements alongside task performance. Experiments across
four language models reveal that moderate uncertainty (40-60%) can outperform
complete transparency in certain scenarios, with model-specific patterns
suggesting reconsideration of optimal information completeness in human-AI
collaboration. These findings contribute to understanding asymmetric reasoning
dynamics and inform uncertainty-calibrated dialogue system design.

</details>


### [562] [COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents](https://arxiv.org/abs/2506.01900)
*Manish Bhatt,Ronald F. Del Rosario,Vineeth Sai Narajala,Idan Habler*

Key words: LLM代理, 资源优化, COALESCE框架, 动态外包, 成本降低

TL;DR: 论文提出COALESCE框架，通过动态外包任务优化大型语言模型（LLM）代理系统的资源利用，理论上可降低41.8%成本，实证中降低20.3%成本。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 大型语言模型代理系统的部署受到GPU资源的高计算需求限制，需要一种优化资源利用的方法。

Method: 提出COALESCE框架，包括混合技能表示、动态技能发现、任务分解、成本模型、市场决策算法和标准化通信协议。

Result: 理论模拟显示41.8%成本降低潜力，实证实验显示20.3%成本降低。

Conclusion: COALESCE框架通过动态任务外包显著降低成本，提升系统可扩展性，促进专业化代理经济。

Abstract: The meteoric rise and proliferation of autonomous Large Language Model (LLM)
agents promise significant capabilities across various domains. However, their
deployment is increasingly constrained by substantial computational demands,
specifically for Graphics Processing Unit (GPU) resources. This paper addresses
the critical problem of optimizing resource utilization in LLM agent systems.
We introduce COALESCE (Cost-Optimized and Secure Agent Labour Exchange via
Skill-based Competence Estimation), a novel framework designed to enable
autonomous LLM agents to dynamically outsource specific subtasks to
specialized, cost-effective third-party LLM agents. The framework integrates
mechanisms for hybrid skill representation, dynamic skill discovery, automated
task decomposition, a unified cost model comparing internal execution costs
against external outsourcing prices, simplified market-based decision-making
algorithms, and a standardized communication protocol between LLM agents.
Comprehensive validation through 239 theoretical simulations demonstrates
41.8\% cost reduction potential, while large-scale empirical validation across
240 real LLM tasks confirms 20.3\% cost reduction with proper epsilon-greedy
exploration, establishing both theoretical viability and practical
effectiveness. The emergence of proposed open standards like Google's
Agent2Agent (A2A) protocol further underscores the need for frameworks like
COALESCE that can leverage such standards for efficient agent interaction. By
facilitating a dynamic market for agent capabilities, potentially utilizing
protocols like A2A for communication, COALESCE aims to significantly reduce
operational costs, enhance system scalability, and foster the emergence of
specialized agent economies, making complex LLM agent functionalities more
accessible and economically viable.

</details>


### [563] [Understanding Overadaptation in Supervised Fine-Tuning: The Role of Ensemble Methods](https://arxiv.org/abs/2506.01901)
*Yifan Hao,Xingyuan Pan,Hanning Zhang,Chenlu Ye,Rui Pan,Tong Zhang*

Key words: 监督微调、模型集成、过适应、偏差-方差权衡

TL;DR: 通过集成预训练和微调模型解决监督微调中的知识遗忘问题，并提出理论分析。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 监督微调（SFT）在特定领域数据上容易遗忘预训练知识，需要一种方法来缓解这一问题。

Method: 通过集成预训练模型和微调模型，平衡偏差和方差问题。

Result: 集成模型不仅保留了预训练知识，还在微调领域上优于单一微调模型。

Conclusion: 集成方法优于正则化技术，能有效解决监督微调中的过适应问题。

Abstract: Supervised fine-tuning (SFT) on domain-specific data is the dominant approach
for adapting foundation models to specialized tasks. However, it has been
observed that SFT models tend to forget knowledge acquired during pretraining.
In vision models, ensembling a pretrained model with its fine-tuned counterpart
has been shown to mitigate this issue. In this work, we demonstrate that the
same holds for language models, and, more strikingly, we observe an
overadaptation phenomenon: the ensemble model not only retains general
knowledge from the foundation model but also outperforms the fine-tuned model
even on the fine-tuning domain itself. Despite the empirical success of
ensembling, a theoretical understanding of its benefits remains underexplored.
We develop a formal theoretical analysis of the overadaptation phenomenon.
Ensembling mitigates this by balancing two primary sources of error: bias,
caused by insufficient fine-tuning, and variance, introduced by overfitting to
fine-tuning data. While regularization techniques aim to address this
trade-off, we show that ensembling provides a more effective solution. We
analyze this phenomenon in over-parameterized linear settings and demonstrate
that interpolating between pretrained and fine-tuned weights significantly
improves performance. These findings offer theoretical justification for the
observed advantages of model ensembling, supported by empirical experiments
consistent with our analysis.

</details>


### [564] [Large language models can learn and generalize steganographic chain-of-thought under process supervision](https://arxiv.org/abs/2506.01926)
*Joey Skaf,Luis Ibanez-Lissen,Robert McCarthy,Connor Watts,Vasil Georgiv,Hannes Whittingham,Lorena Gonzalez-Manzano,David Lindner,Cameron Tice,Edward James Young,Puria Radmard*

Key words: Chain-of-thought, 模型监控, 有害意图, 编码泛化, 逻辑模糊化

TL;DR: CoT监控可降低模型风险，但开发者可能因客户或法规要求而隐藏有害意图，导致逻辑模糊化。研究表明，惩罚特定字符串会促使模型替代编码，但底层逻辑不变，且能推广编码方案。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究旨在探讨CoT监控的有效性及开发者隐藏有害意图的影响，揭示模型如何通过编码替代逻辑保持决策行为。

Method: 通过惩罚特定字符串观察模型替代行为，测试其编码能力及泛化性。

Result: 模型能学习替代编码方案且泛化到新字符串，底层逻辑不变。

Conclusion: 隐藏有害意图导致逻辑模糊化，模型仍能通过编码维持行为，威胁CoT监控可靠性。

Abstract: Chain-of-thought (CoT) reasoning not only enhances large language model
performance but also provides critical insights into decision-making processes,
marking it as a useful tool for monitoring model intent and planning. By
proactively preventing models from acting on CoT indicating misaligned or
harmful intent, CoT monitoring can be used to reduce risks associated with
deploying models. However, developers may be incentivized to train away the
appearance of harmful intent from CoT traces, by either customer preferences or
regulatory requirements. Recent works have shown that banning mention of a
specific example of reward hacking, which may be done either to make CoT
presentable to users or as a naive attempt to prevent the behavior, causes
obfuscation of the undesired reasoning traces but the persistence of the
undesired behavior. Such obfuscation threatens the reliability of CoT
monitoring. However, obfuscation of reasoning can be due to its internalization
to latent space computation, or its encoding within the CoT. Here, we provide
an extension to these results. First, we show that penalizing the use of
specific strings within load-bearing reasoning traces causes models to
substitute alternative strings. Crucially, this does not alter the underlying
method by which the model performs the task, demonstrating that the model can
learn to steganographically encode its reasoning. We further demonstrate that
models can generalize an encoding scheme. When the penalized strings belong to
an overarching class, the model learns not only to substitute strings seen in
training, but also develops a general encoding scheme for all members of the
class which it can apply to held-out testing strings.

</details>


### [565] [RoboEgo System Card: An Omnimodal Model with Native Full Duplexity](https://arxiv.org/abs/2506.01934)
*Yiqun Yao,Xiang Li,Xin Jiang,Xuezhi Fang,Naitong Yu,Aixin Sun,Yequan Wang*

Key words: 多模态处理,全双工,RoboEgo,人工智能,具身上下文

TL;DR: RoboEgo（FLM-Ego）是一个支持全双工和多模态处理的人工智能模型系统，旨在解决多模态处理和实时响应人类指令的挑战，其理论双工延迟为80毫秒，在实际对话中表现出卓越的响应速度和语音自然度。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 人类天生以全双工方式处理多模态信息，这在人工智能中的实现对模型开发和部署至关重要，尤其是在具身上下文中。

Method: RoboEgo采用了一种统一模型系统，其核心架构和算法原生支持全双工处理，实现了80毫秒的理论双工延迟。

Result: 在实时视觉对话中，RoboEgo展现出比现有半双工多模态模型更优的响应速度和语音自然度，同时保持相似的内容质量。

Conclusion: RoboEgo的成功表明，原生全双工系统在多模态处理中可以实现高响应性和自然度，突破了以往的技术限制。

Abstract: Humans naturally process real-world multimodal information in a full-duplex
manner. In artificial intelligence, replicating this capability is essential
for advancing model development and deployment, particularly in embodied
contexts. The development of multimodal models faces two primary challenges:
(1) effectively handling more than three modalities-such as vision, audio, and
text; and (2) delivering full-duplex responses to rapidly evolving human
instructions. To facilitate research on models that support both omnimodal
processing and full duplexity, we present RoboEgo (alias: FLM-Ego), a unified
model system designed to address both challenges. RoboEgo incorporates a
backbone architecture and algorithms that natively support full duplexity,
achieving a theoretical duplex latency of 80 ms. In streaming visually grounded
conversations under real-world conditions, RoboEgo exhibits superior
responsiveness and speech naturalness, while maintaining comparable content
qualities to state-of-the-art semi-duplex omnimodal models-a feat previously
considered unattainable by native full-duplex systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [566] [Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics](https://arxiv.org/abs/2506.00070)
*Dongyoung Kim,Sumin Park,Huiwon Jang,Jinwoo Shin,Jaehyung Kim,Younggyo Seo*

Key words: Large Vision-Language Models, Robot Control, Reinforcement Learning, Embodied Reasoning, Robot-R1

TL;DR: 提出了一个新框架Robot-R1，利用强化学习优化机器人控制任务中的具身推理，优于传统的监督微调方法。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决监督微调（SFT）在机器人控制任务中存在的启发式数据集构建问题、灾难性遗忘和泛化性能下降等局限性。

Method: 通过强化学习训练模型预测任务完成所需的下一个关键点状态，并根据当前场景图像和环境元数据采样和强化推理响应。

Result: 在具身推理任务中表现优于SFT方法，甚至在低级别动作控制任务中超越GPT-4o。

Conclusion: Robot-R1为机器人控制中的具身推理提供了有效解决方案，展现了强化学习的潜力。

Abstract: Large Vision-Language Models (LVLMs) have recently shown great promise in
advancing robotics by combining embodied reasoning with robot control. A common
approach involves training on embodied reasoning tasks related to robot control
using Supervised Fine-Tuning (SFT). However, SFT datasets are often
heuristically constructed and not explicitly optimized for improving robot
control. Furthermore, SFT often leads to issues such as catastrophic forgetting
and reduced generalization performance. To address these limitations, we
introduce Robot-R1, a novel framework that leverages reinforcement learning to
enhance embodied reasoning specifically for robot control. Robot-R1 learns to
predict the next keypoint state required for task completion, conditioned on
the current scene image and environment metadata derived from expert
demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples
reasoning-based responses and reinforces those that lead to more accurate
predictions. Our experiments show that models trained with Robot-R1 outperform
SFT methods on embodied reasoning tasks. Despite having only 7B parameters,
Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action
control, such as spatial and primitive movement reasoning.

</details>


### [567] [Reducing Latency in LLM-Based Natural Language Commands Processing for Robot Navigation](https://arxiv.org/abs/2506.00075)
*Diego Pollini,Bruna V. Guterres,Rodrigo S. Guerra,Ricardo B. Grando*

Key words: 大型语言模型, ChatGPT, ROS 2, 工业机器人, 延迟优化

TL;DR: 本文探讨了将ChatGPT与ROS 2结合以减少交互延迟，提升机器人控制效率的实验结果。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决大型语言模型在工业机器人中因计算复杂性和规模导致的延迟问题。

Method: 通过整合ChatGPT与ROS 2，设计无需中间件的架构，并在Gazebo模拟环境中测试。

Result: 实验显示通信延迟平均降低7.01%，提升了执行速度和交互体验。

Conclusion: 该方法有效改善了实时机器人操作，适用于工业自动化和精密任务。

Abstract: The integration of Large Language Models (LLMs), such as GPT, in industrial
robotics enhances operational efficiency and human-robot collaboration.
However, the computational complexity and size of these models often provide
latency problems in request and response times. This study explores the
integration of the ChatGPT natural language model with the Robot Operating
System 2 (ROS 2) to mitigate interaction latency and improve robotic system
control within a simulated Gazebo environment. We present an architecture that
integrates these technologies without requiring a middleware transport
platform, detailing how a simulated mobile robot responds to text and voice
commands. Experimental results demonstrate that this integration improves
execution speed, usability, and accessibility of the human-robot interaction by
decreasing the communication latency by 7.01\% on average. Such improvements
facilitate smoother, real-time robot operations, which are crucial for
industrial automation and precision tasks.

</details>


### [568] [Hi-Dyna Graph: Hierarchical Dynamic Scene Graph for Robotic Autonomy in Human-Centric Environments](https://arxiv.org/abs/2506.00083)
*Jiawei Hou,Xiangyang Xue,Taiping Zeng*

Key words: 服务机器人,动态场景图,层次化表示,大语言模型,自主性

TL;DR: Hi-Dyna Graph 提出了一种层次化动态场景图架构，结合全局布局与局部动态语义，提升服务机器人在动态场景中的自主性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 现有方法（如拓扑地图）无法建模瞬态物体关系，而密集神经表示（如NeRF）计算成本过高。因此，需要一种既能捕捉全局布局又能动态更新局部语义的解决方案。

Method: 通过全局拓扑图和动态子图结合，使用语义和空间约束锚定动态子图，并利用大语言模型（LLMs）解析场景图和生成指令。

Result: 实验证明 Hi-Dyna Graph 在场景表示上的有效性，实际部署中机器人能在动态场景（如自助餐厅）中自主完成复杂任务。

Conclusion: Hi-Dyna Graph 为机器人在动态环境中的自主操作提供了高效且实用的解决方案。

Abstract: Autonomous operation of service robotics in human-centric scenes remains
challenging due to the need for understanding of changing environments and
context-aware decision-making. While existing approaches like topological maps
offer efficient spatial priors, they fail to model transient object
relationships, whereas dense neural representations (e.g., NeRF) incur
prohibitive computational costs. Inspired by the hierarchical scene
representation and video scene graph generation works, we propose Hi-Dyna
Graph, a hierarchical dynamic scene graph architecture that integrates
persistent global layouts with localized dynamic semantics for embodied robotic
autonomy. Our framework constructs a global topological graph from posed RGB-D
inputs, encoding room-scale connectivity and large static objects (e.g.,
furniture), while environmental and egocentric cameras populate dynamic
subgraphs with object position relations and human-object interaction patterns.
A hybrid architecture is conducted by anchoring these subgraphs to the global
topology using semantic and spatial constraints, enabling seamless updates as
the environment evolves. An agent powered by large language models (LLMs) is
employed to interpret the unified graph, infer latent task triggers, and
generate executable instructions grounded in robotic affordances. We conduct
complex experiments to demonstrate Hi-Dyna Grap's superior scene representation
effectiveness. Real-world deployments validate the system's practicality with a
mobile manipulator: robotics autonomously complete complex tasks with no
further training or complex rewarding in a dynamic scene as cafeteria
assistant. See https://anonymous.4open.science/r/Hi-Dyna-Graph-B326 for video
demonstration and more details.

</details>


### [569] [LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks](https://arxiv.org/abs/2506.00411)
*Yi Yang,Jiaxuan Sun,Siqi Kou,Yihan Wang,Zhijie Deng*

Key words: 长时程任务,视觉语言动作模型,分层控制,机器人动作预测,嵌入式智能

TL;DR: 该论文提出了一种名为LoHoVLA的统一视觉语言动作框架，用于解决长时程任务中的高层次的子任务分解和低层次的运动控制问题，并通过实验验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 现有的视觉语言动作模型在任务规划方面表现不佳，而分层架构在协调上有问题，因此需要一种统一的框架来克服这些限制。

Method: LoHoVLA结合了预训练的视觉语言模型作为骨干，联合生成语言和动作标记，用于子任务生成和机器人动作预测，并使用分层闭环控制机制减少错误。

Result: 在Ravens模拟器上的20个长时程任务实验中，LoHoVLA显著优于分层和标准VLA方法。

Conclusion: 统一的架构有望推动可泛化的嵌入式智能的发展。

Abstract: Real-world embodied agents face long-horizon tasks, characterized by
high-level goals demanding multi-step solutions beyond single actions.
Successfully navigating these requires both high-level task planning (i.e.,
decomposing goals into sub-tasks) and low-level motion control (i.e.,
generating precise robot actions). While existing vision language action (VLA)
models and hierarchical architectures offer potential in embodied tasks, the
former often falter in planning, and the latter can suffer from coordination
issues, both hampering performance. We introduce a new unified VLA framework
for long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA
leverages a large pretrained vision language model (VLM) as the backbone to
jointly generate language and action tokens for sub-task generation and robot
action prediction, respectively. This shared representation promotes better
generalization across tasks. Additionally, LoHoVLA embraces a hierarchical
closed-loop control mechanism to mitigate errors originating from both
high-level planning and low-level control. To train LoHoVLA, we introduce
LoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon
tasks, each with 1,000 expert demonstrations composed of visual observations,
linguistic goals, sub-tasks, and robot actions. Experimental results show that
LoHoVLA significantly surpasses both hierarchical and standard VLA approaches
on long-horizon embodied tasks in the Ravens simulator. These findings
underscore the promise of unified architectures for advancing generalizable
embodied intelligence.

</details>


### [570] [Diffusion Models for Increasing Accuracy in Olfaction Sensors and Datasets](https://arxiv.org/abs/2506.00455)
*Kordel K. France,Ovidiu Daescu*

Key words: 机器人嗅觉,扩散模型,视觉语言模型,分子生成,气味源定位

TL;DR: 提出了一种基于扩散模型的机器学习方法，结合视觉语言模型，提升机器人气味源定位的准确性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决当前气味源定位方法因嗅觉数据集和传感器分辨率限制导致的误判问题。

Method: 利用扩散模型生成分子扩展化学空间，结合视觉语言模型和电子传感器验证生成分子。

Result: 提升了嗅觉-视觉模型对气味源的准确关联能力，改善了机器人在依赖嗅觉线索的环境中的导航与决策。

Conclusion: 该方法为机器人嗅觉领域提供了可扩展的解决方案，解决了数据不足和传感器模糊性问题。

Abstract: Robotic odour source localization (OSL) is a critical capability for
autonomous systems operating in complex environments. However, current OSL
methods often suffer from ambiguities, particularly when robots misattribute
odours to incorrect objects due to limitations in olfactory datasets and sensor
resolutions. To address this challenge, we introduce a novel machine learning
method using diffusion-based molecular generation to enhance odour localization
accuracy that can be used by itself or with automated olfactory dataset
construction pipelines with vision-language models (VLMs) This generative
process of our diffusion model expands the chemical space beyond the
limitations of both current olfactory datasets and the training data of VLMs,
enabling the identification of potential odourant molecules not previously
documented. The generated molecules can then be more accurately validated using
advanced olfactory sensors which emulate human olfactory recognition through
electronic sensor arrays. By integrating visual analysis, language processing,
and molecular generation, our framework enhances the ability of
olfaction-vision models on robots to accurately associate odours with their
correct sources, thereby improving navigation and decision-making in
environments where olfactory cues are essential. Our methodology represents a
foundational advancement in the field of robotic olfaction, offering a scalable
solution to the challenges posed by limited olfactory data and sensor
ambiguities.

</details>


### [571] [Multi-Objective Neural Network Assisted Design Optimization of Soft Fin-Ray Grippers for Enhanced Grasping Performance](https://arxiv.org/abs/2506.00494)
*Ali Ghanizadeh,Ali Ahmadi,Arash Bahrami*

Key words: 软机器人夹具、Fin-Ray结构、有限元法、神经网络、多目标优化

TL;DR: 研究探讨了软Fin-Ray夹具的设计优化问题，利用有限元法（FEM）和神经网络预测接触力与形变，并通过NSGA-II算法实现多目标优化。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: Fin-Ray夹具在柔性和高力操控之间存在矛盾，需要解决这一多目标优化问题以提升其设计性能。

Method: 使用FEM模拟抓取圆柱体时的形变和接触力，构建多层感知器（MLP）预测，并采用NSGA-II算法优化设计参数。

Result: 实现了接触力与形变的预测，并通过优化找到了兼顾柔性和高力的设计方案。

Conclusion: 该方法可用于改进软夹具设计，满足从精细抓取到高力应用的需求。

Abstract: Soft Fin-Ray grippers can perform delicate and careful manipulation, which
has caused notable attention in different fields. These grippers can handle
objects of various forms and sizes safely. The internal structure of the
Fin-Ray finger plays a significant role in its adaptability and grasping
performance. However, modeling the non-linear grasp force and deformation
behaviors for design purposes is challenging. Moreover, when the Fin-Ray finger
becomes more rigid and capable of exerting higher forces, it becomes less
delicate in handling objects. The contrast between these two objectives gives
rise to a multi-objective optimization problem. In this study, we employ finite
element method (FEM) to estimate the deflections and contact forces of the
Fin-Ray, grasping cylindrical objects. This dataset is then used to construct a
multilayer perception (MLP) for prediction of the contact force and the tip
displacement. The FEM dataset consists of three input and four target features.
The three input features of the MLP and optimization design variables are the
thickness of the front and supporting beams, the thickness of the cross beams,
and the equal spacing between the cross beams. In addition, the target features
are the maximum contact forces and maximum tip displacements in x- and
y-directions. The magnitude of maximum contact force and magnitude of maximum
tip displacement are the two objectives, showing the trade-off between force
and delicate manipulation in soft Fin-Ray grippers. Furthermore, the optimized
set of solutions are found using multi-objective optimal techniques. We use
non-dominated sorting genetic algorithm (NSGA-II) method for this purpose. Our
findings demonstrate that our methodologies can be used to improve the design
and gripping performance of soft robotic grippers, helping us to choose a
design not only for delicate grasping but also for high-force applications.

</details>


### [572] [Evaluating Robot Policies in a World Model](https://arxiv.org/abs/2506.00613)
*Julian Quevedo,Percy Liang,Sherry Yang*

Key words: 机器人策略评估, 世界模型, 视频生成, 蒙特卡洛推演, 视觉语言模型

TL;DR: 论文提出了一种基于世界模型的策略评估方法（WPE），通过训练视频生成模型模拟真实环境，结合推理方案和奖励函数，实现对机器人策略的有效评估，尽管对分布外行为的评估存在偏差，但能保持策略的相对排名。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 由于真实环境测试成本高且手工模拟效果不佳，研究旨在通过世界模型提供一种低成本且相对准确的机器人策略评估方法。

Method: 使用动作条件视频生成模型模拟真实环境，提出块状自回归扩散变换器以减少误差累积，并结合视觉语言模型（VLM）作为奖励函数进行蒙特卡洛推演。

Result: WPE在评估分布内和分布外行为时存在高估或低估现象，但能保持策略的相对排名，且在模拟机械臂运动时效果较好。

Conclusion: 世界模型可作为真实部署前的初步策略评估工具，尽管在某些复杂交互场景中仍有局限性。

Abstract: Robotics has broad applications from automating house chores to taking care
of patients. However, evaluating robot control policies is challenging, as
real-world testing is expensive, while handcrafted simulations often fail to
accurately reflect real-world conditions, resulting in poor correlation between
simulated evaluation and real-world outcomes. In this work, we investigate
World-model-based Policy Evaluation (WPE). We first train an action-conditioned
video generation model as a proxy to real-world environments. To enable
efficient rollouts of hundreds of interactive steps while mitigating error
accumulation in the world model, we propose an inference scheme which we call
Blockwise-Autoregressive Diffusion Transformer with adjustable context and
decoding horizon lengths. To ensure that the world model indeed follows action
input, we propose metrics based on the agreement between the ground truth video
and generated video conditioned on the same sequence of actions to evaluate the
world model. We then use the world model for policy evaluation by performing
Monte Carlo rollouts in the world model while employing a vision-language model
(VLM) as a reward function. Interestingly, we found that WPE tends to
underestimate the policy values for in-distribution actions and overestimate
policy values for out-of-distribution actions. Nevertheless, WPE preserves the
relative rankings of different policies. In emulating real robot executions,
WPE achieves high fidelity in mimicing robot arm movements as in real videos,
while emulating highly realistic object interaction remains challenging.
Despite this limitation, we show that a world model can serve as a starting
point for evaluating robot policies before real-world deployment.

</details>


### [573] [DriveMind: A Dual-VLM based Reinforcement Learning Framework for Autonomous Driving](https://arxiv.org/abs/2506.00819)
*Dawood Wasif,Terrence J Moore,Chandan K Reddy,Jin-Hee Cho*

Key words: 自动驾驶，视觉语言模型，语义奖励，动态提示，安全性

TL;DR: DriveMind是一个端到端的自动驾驶系统，通过语义奖励框架提升可解释性和安全性，结合视觉语言模型和动态提示生成，实现了高效且安全的驾驶性能。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 现有的端到端自动驾驶系统缺乏可解释性和安全性保证，且动态驾驶场景的适应性不足。

Method: DriveMind整合了对比视觉语言模型编码器、动态提示生成、分层安全模块和预测世界模型，以语义奖励框架优化驾驶行为。

Result: DriveMind在CARLA Town 2中实现了更高的速度和路线完成率，碰撞率接近零，并在真实数据中表现出色。

Conclusion: DriveMind通过语义奖励框架显著提升了自动驾驶系统的性能和安全性，展示了跨领域部署的潜力。

Abstract: End-to-end autonomous driving systems map sensor data directly to control
commands, but remain opaque, lack interpretability, and offer no formal safety
guarantees. While recent vision-language-guided reinforcement learning (RL)
methods introduce semantic feedback, they often rely on static prompts and
fixed objectives, limiting adaptability to dynamic driving scenes. We present
DriveMind, a unified semantic reward framework that integrates: (i) a
contrastive Vision-Language Model (VLM) encoder for stepwise semantic
anchoring; (ii) a novelty-triggered VLM encoder-decoder, fine-tuned via
chain-of-thought (CoT) distillation, for dynamic prompt generation upon
semantic drift; (iii) a hierarchical safety module enforcing kinematic
constraints (e.g., speed, lane centering, stability); and (iv) a compact
predictive world model to reward alignment with anticipated ideal states.
DriveMind achieves 19.4 +/- 2.3 km/h average speed, 0.98 +/- 0.03 route
completion, and near-zero collisions in CARLA Town 2, outperforming baselines
by over 4% in success rate. Its semantic reward generalizes zero-shot to real
dash-cam data with minimal distributional shift, demonstrating robust
cross-domain alignment and potential for real-world deployment.

</details>


### [574] [RoboMoRe: LLM-based Robot Co-design via Joint Optimization of Morphology and Reward](https://arxiv.org/abs/2506.00276)
*Jiawei Fang,Yuxuan Sun,Chengtian Ma,Qiuyu Lu,Lining Yao*

Key words: 机器人协同设计, 大型语言模型, 双阶段优化, 奖励塑造

TL;DR: RoboMoRe框架通过双阶段优化（粗优化和细优化）解决了机器人协同设计中的局限性，显著超越了人工设计和其他方法。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 机器人协同设计中固定奖励函数导致次优设计，限制了多样运动模式的探索。

Method: RoboMoRe利用大型语言模型进行双阶段优化，先粗优化生成多样高质量的形态-奖励对，再细优化迭代改进。

Result: 在八项任务中，RoboMoRe无需特定提示或模板，表现显著优于人工设计和其他方法。

Conclusion: RoboMoRem为机器人协同设计提供了高效解决方案，推动了形态与行为的协同优化。

Abstract: Robot co-design, jointly optimizing morphology and control policy, remains a
longstanding challenge in the robotics community, where many promising robots
have been developed. However, a key limitation lies in its tendency to converge
to sub-optimal designs due to the use of fixed reward functions, which fail to
explore the diverse motion modes suitable for different morphologies. Here we
propose RoboMoRe, a large language model (LLM)-driven framework that integrates
morphology and reward shaping for co-optimization within the robot co-design
loop. RoboMoRe performs a dual-stage optimization: in the coarse optimization
stage, an LLM-based diversity reflection mechanism generates both diverse and
high-quality morphology-reward pairs and efficiently explores their
distribution. In the fine optimization stage, top candidates are iteratively
refined through alternating LLM-guided reward and morphology gradient updates.
RoboMoRe can optimize both efficient robot morphologies and their suited motion
behaviors through reward shaping. Results demonstrate that without any
task-specific prompting or predefined reward/morphology templates, RoboMoRe
significantly outperforms human-engineered designs and competing methods across
eight different tasks.

</details>


### [575] [Interactive Imitation Learning for Dexterous Robotic Manipulation: Challenges and Perspectives -- A Survey](https://arxiv.org/abs/2506.00098)
*Edgar Welte,Rania Rayyes*

Key words: 人形机器人, 灵巧操作, 交互式模仿学习, 强化学习, 模仿学习

TL;DR: 本文综述了人形机器人灵巧操作的挑战和学习方法，重点关注交互式模仿学习在解决高维控制、数据有限和协变量偏移等问题中的潜力。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 人形机器人在人类中心环境中需要掌握灵巧操作，但传统方法（如强化学习和模仿学习）面临高维控制、数据有限等问题，因此探索交互式模仿学习是一个有前景的方向。

Method: 综述了现有学习方法（模仿学习、强化学习和混合方法），并探讨如何将交互式模仿学习应用于灵巧操作任务。

Result: 当前交互式模仿学习在灵巧操作中应用有限，但通过分析其他任务的成功案例，提出了适应和改进的方向。

Conclusion: 交互式模仿学习有望提升灵巧操作技能，但仍需进一步研究以填补方法上的空白。

Abstract: Dexterous manipulation is a crucial yet highly complex challenge in humanoid
robotics, demanding precise, adaptable, and sample-efficient learning methods.
As humanoid robots are usually designed to operate in human-centric
environments and interact with everyday objects, mastering dexterous
manipulation is critical for real-world deployment. Traditional approaches,
such as reinforcement learning and imitation learning, have made significant
strides, but they often struggle due to the unique challenges of real-world
dexterous manipulation, including high-dimensional control, limited training
data, and covariate shift. This survey provides a comprehensive overview of
these challenges and reviews existing learning-based methods for dexterous
manipulation, spanning imitation learning, reinforcement learning, and hybrid
approaches. A promising yet underexplored direction is interactive imitation
learning, where human feedback actively refines a robot's behavior during
training. While interactive imitation learning has shown success in various
robotic tasks, its application to dexterous manipulation remains limited. To
address this gap, we examine current interactive imitation learning techniques
applied to other robotic tasks and discuss how these methods can be adapted to
enhance dexterous manipulation. By synthesizing state-of-the-art research, this
paper highlights key challenges, identifies gaps in current methodologies, and
outlines potential directions for leveraging interactive imitation learning to
improve dexterous robotic skills.

</details>


### [576] [Learning Aerodynamics for the Control of Flying Humanoid Robots](https://arxiv.org/abs/2506.00305)
*Antonello Paolino,Gabriele Nava,Fabio Di Natale,Fabio Bergonti,Punith Reddy Vanteddu,Donato Grassi,Luca Riccobene,Alex Zanotti,Renato Tognaccini,Gianluca Iaccarino,Daniele Pucci*

Key words: 人形机器人,气动力,CFD,深度学习,控制

TL;DR: 该论文研究了喷气动力人形机器人的设计和控制，通过CFD模拟和风洞实验验证气动力模型，并利用深度学习技术优化控制系统。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 研究多模态运动机器人在复杂环境中的适应性，尤其是人形机器人在空中运动时的气动力建模与控制挑战。

Method: 1. 设计喷气动力人形机器人iRonCub-Mk1，优化喷气发动机集成；2. 通过CFD模拟和风洞实验验证气动力模型；3. 开发自动CFD框架生成数据集，训练深度神经网络和线性回归模型；4. 设计气动力感知控制器并在模拟和物理原型中验证。

Result: 成功验证了气动力模型的准确性，并通过模拟和实验证明了控制器的有效性。

Conclusion: 论文提出了结合传统和机器学习技术的气动力建模与控制方法，为人形机器人的空中运动提供了可行解决方案。

Abstract: Robots with multi-modal locomotion are an active research field due to their
versatility in diverse environments. In this context, additional actuation can
provide humanoid robots with aerial capabilities. Flying humanoid robots face
challenges in modeling and control, particularly with aerodynamic forces. This
paper addresses these challenges from a technological and scientific
standpoint. The technological contribution includes the mechanical design of
iRonCub-Mk1, a jet-powered humanoid robot, optimized for jet engine
integration, and hardware modifications for wind tunnel experiments on humanoid
robots for precise aerodynamic forces and surface pressure measurements. The
scientific contribution offers a comprehensive approach to model and control
aerodynamic forces using classical and learning techniques. Computational Fluid
Dynamics (CFD) simulations calculate aerodynamic forces, validated through wind
tunnel experiments on iRonCub-Mk1. An automated CFD framework expands the
aerodynamic dataset, enabling the training of a Deep Neural Network and a
linear regression model. These models are integrated into a simulator for
designing aerodynamic-aware controllers, validated through flight simulations
and balancing experiments on the iRonCub-Mk1 physical prototype.

</details>


### [577] [Constrained Stein Variational Gradient Descent for Robot Perception, Planning, and Identification](https://arxiv.org/abs/2506.00589)
*Griffin Tabor,Tucker Hermans*

Key words: constrained optimization, Stein variational gradient descent, robotics, uncertainty, variational inference

TL;DR: 论文提出两种新框架，将约束优化原则应用于Stein变分梯度下降算法，以解决机器人系统中的不确定性和多高质量可行解问题。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 机器人系统中的许多核心问题可以建模为约束优化问题，且系统通常存在不确定性或需要多个可行解。

Method: 结合约束优化和Stein变分梯度下降算法，提出通用框架支持多种约束类型和优化器。

Result: 实验表明，该方法能够在约束条件下近似分布，如避障运动规划、SE(3)流形上的机械臂关节角度和点云中的物体位姿。

Conclusion: 该方法有效解决了机器人系统中的约束优化问题，展示了在多种实际应用中满足约束的分布学习能力。

Abstract: Many core problems in robotics can be framed as constrained optimization
problems. Often on these problems, the robotic system has uncertainty, or it
would be advantageous to identify multiple high quality feasible solutions. To
enable this, we present two novel frameworks for applying principles of
constrained optimization to the new variational inference algorithm Stein
variational gradient descent. Our general framework supports multiple types of
constrained optimizers and can handle arbitrary constraints. We demonstrate on
a variety of problems that we are able to learn to approximate distributions
without violating constraints. Specifically, we show that we can build
distributions of: robot motion plans that exactly avoid collisions, robot arm
joint angles on the SE(3) manifold with exact table placement constraints, and
object poses from point clouds with table placement constraints.

</details>


### [578] [Humanoid World Models: Open World Foundation Models for Humanoid Robotics](https://arxiv.org/abs/2506.01182)
*Muhammad Qasim Ali,Aditya Sridhar,Shahbuland Matiana,Alex Wong,Mohammad Al-Sharman*

Key words: 人形机器人，预测模型，生成模型，参数共享，轻量级

TL;DR: 论文介绍了Humanoid World Models (HWM)，一种轻量级开源视频模型，用于预测人形机器人在动作条件下的未来第一视角观察。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 人形机器人需强大预测模型以在人类环境中执行复杂任务，HWM旨在提供高效解决方案。

Method: 训练两种生成模型（Masked Transformers和FlowMatching），使用100小时人形机器人演示数据，并探索不同注意力机制和参数共享策略。

Result: 参数共享技术将模型大小减少33%至53%，对性能或视觉保真度影响很小。

Conclusion: HWM适合学术和小型实验室环境，如1至2个GPU的部署。

Abstract: Humanoid robots have the potential to perform complex tasks in human centered
environments but require robust predictive models to reason about the outcomes
of their actions. We introduce Humanoid World Models (HWM) a family of
lightweight open source video based models that forecast future egocentric
observations conditioned on actions. We train two types of generative models
Masked Transformers and FlowMatching on 100 hours of humanoid demonstrations.
Additionally we explore architectural variants with different attention
mechanisms and parameter sharing strategies. Our parameter sharing techniques
reduce model size by 33 to 53 with minimal impact on performance or visual
fidelity. HWM is designed to be trained and deployed in practical academic and
small lab settings such as 1 to 2 GPUs.

</details>


### [579] [OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation](https://arxiv.org/abs/2506.01196)
*Ishika Singh,Ankit Goyal,Stan Birchfield,Dieter Fox,Animesh Garg,Valts Blukis*

Key words: OG-VLA, Vision Language Action, 3D-aware policies, generalization, robot manipulation

TL;DR: OG-VLA结合了视觉语言动作模型（VLA）的泛化能力和3D感知策略的鲁棒性，通过多视角RGBD观测和自然语言指令映射机器人动作，显著提升了未见场景和指令的泛化性能。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决3D感知策略在未见指令、场景和对象上泛化能力不足，以及VLA模型对相机和机器人姿态变化敏感的问题。

Method: 采用点云投影到正交视图的方法，结合视觉主干、大语言模型（LLM）和图像扩散模型，生成编码末端执行器位置和方向的图像。

Result: 在Arnold和Colosseum基准测试中，OG-VLA实现了40%以上的相对改进，并在真实世界场景中表现出色。

Conclusion: OG-VLA通过结合3D感知策略和VLA模型的优势，显著提升了机器人操作的泛化能力和鲁棒性。

Abstract: We introduce OG-VLA, a novel architecture and learning framework that
combines the generalization strengths of Vision Language Action models (VLAs)
with the robustness of 3D-aware policies. We address the challenge of mapping
natural language instructions and multi-view RGBD observations to quasi-static
robot actions. 3D-aware robot policies achieve state-of-the-art performance on
precise robot manipulation tasks, but struggle with generalization to unseen
instructions, scenes, and objects. On the other hand, VLAs excel at
generalizing across instructions and scenes, but can be sensitive to camera and
robot pose variations. We leverage prior knowledge embedded in language and
vision foundation models to improve generalization of 3D-aware keyframe
policies. OG-VLA projects input observations from diverse views into a point
cloud which is then rendered from canonical orthographic views, ensuring input
view invariance and consistency between input and output spaces. These
canonical views are processed with a vision backbone, a Large Language Model
(LLM), and an image diffusion model to generate images that encode the next
position and orientation of the end-effector on the input scene. Evaluations on
the Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization
to unseen environments, with over 40% relative improvements while maintaining
robust performance in seen settings. We also show real-world adaption in 3 to 5
demonstrations along with strong generalization. Videos and resources at
https://og-vla.github.io/

</details>


### [580] [Sparse Imagination for Efficient Visual World Model Planning](https://arxiv.org/abs/2506.01392)
*Junha Chun,Youngjoon Jeong,Taesup Kim*

Key words: 世界模型，稀疏想象，计算效率，变换器，实时决策

TL;DR: 提出了一种基于稀疏想象的视觉世界模型规划方法，通过减少预测时的令牌处理数量提升计算效率，适用于实时决策场景。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 现有的世界模型在复杂环境中需要大量计算资源，限制了在机器人等资源受限场景中的实时应用。

Method: 采用基于变换器的稀疏训练视觉世界模型，通过随机分组注意力策略自适应调整处理令牌数量。

Result: 实验表明，稀疏想象方法在保持任务性能的同时显著提高了推理效率。

Conclusion: 该方法为世界模型在实时决策中的部署提供了可行方案。

Abstract: World model based planning has significantly improved decision-making in
complex environments by enabling agents to simulate future states and make
informed choices. However, ensuring the prediction accuracy of world models
often demands substantial computational resources, posing a major challenge for
real-time applications. This computational burden is particularly restrictive
in robotics, where resources are severely constrained. To address this
limitation, we propose a Sparse Imagination for Efficient Visual World Model
Planning, which enhances computational efficiency by reducing the number of
tokens processed during forward prediction. Our method leverages a sparsely
trained vision-based world model based on transformers with randomized grouped
attention strategy, allowing the model to adaptively adjust the number of
tokens processed based on the computational resource. By enabling sparse
imagination (rollout), our approach significantly accelerates planning while
maintaining high control fidelity. Experimental results demonstrate that sparse
imagination preserves task performance while dramatically improving inference
efficiency, paving the way for the deployment of world models in real-time
decision-making scenarios.

</details>


### [581] [LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative Policy Generation](https://arxiv.org/abs/2506.01538)
*Guobin Zhu,Rui Zhou,Wenkang Ji,Shiyu Zhao*

Key words: 多智能体强化学习, 大型语言模型, 样本效率, 先验策略, 奖励函数

TL;DR: 本文提出了一种名为LAMARL的新方法，将多智能体强化学习（MARL）与大型语言模型（LLM）结合，显著提升了样本效率，无需人工设计。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: MARL在复杂多机器人任务中表现出色，但样本效率低且需要人工调整奖励。LLM在单机器人场景中潜力巨大，但在多机器人系统中应用尚未充分探索。

Method: LAMARL包含两个模块：1) 利用LLM自动生成先验策略和奖励函数；2) 使用MARL通过这些函数高效训练机器人策略。

Result: 在形状组装任务中，LAMARL的样本效率平均提升185.9%，任务完成率提高；基于CoT和基础API的结构化提示将LLM输出成功率提高28.5%-67.5%。

Conclusion: LAMARL有效解决了MARL的样本效率问题，展示了LLM在多机器人系统中的潜力。

Abstract: Although Multi-Agent Reinforcement Learning (MARL) is effective for complex
multi-robot tasks, it suffers from low sample efficiency and requires iterative
manual reward tuning. Large Language Models (LLMs) have shown promise in
single-robot settings, but their application in multi-robot systems remains
largely unexplored. This paper introduces a novel LLM-Aided MARL (LAMARL)
approach, which integrates MARL with LLMs, significantly enhancing sample
efficiency without requiring manual design. LAMARL consists of two modules: the
first module leverages LLMs to fully automate the generation of prior policy
and reward functions. The second module is MARL, which uses the generated
functions to guide robot policy training effectively. On a shape assembly
benchmark, both simulation and real-world experiments demonstrate the unique
advantages of LAMARL. Ablation studies show that the prior policy improves
sample efficiency by an average of 185.9% and enhances task completion, while
structured prompts based on Chain-of-Thought (CoT) and basic APIs improve LLM
output success rates by 28.5%-67.5%. Videos and code are available at
https://guobin-zhu.github.io/LLM-MARL

</details>


### [582] [FreqPolicy: Frequency Autoregressive Visuomotor Policy with Continuous Tokens](https://arxiv.org/abs/2506.01583)
*Yiming Zhong,Yumeng Liu,Chuyang Xiao,Zemin Yang,Youzhuo Wang,Yufei Zhu,Ye Shi,Yujing Sun,Xinge Zhu,Yuexin Ma*

Key words: 视觉运动策略、频率域、机器人操作、自回归框架、连续潜在表示

TL;DR: 提出了一种新颖的视觉运动策略学习方法，通过在频率域中建模动作的层次化频率组件，并结合连续潜在表示，显著提升了机器人操作的精度和效率。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 现有的视觉运动策略学习方法因动作表示和网络架构的固有局限性而表现不佳，通过观察发现频率域中的动作表示能更有效地捕捉动作的全局和局部细节，从而驱动了新方法的提出。

Method: 提出了一种渐进式建模层次化频率组件的视觉运动策略学习范式，并引入连续潜在表示以保持动作空间的平滑性和连续性。

Result: 在多种2D和3D机器人操作基准测试中，所提方法在精度和效率上均优于现有方法。

Conclusion: 频率域自回归框架与连续标记的结合在通用机器人操作中展现出巨大潜力。

Abstract: Learning effective visuomotor policies for robotic manipulation is
challenging, as it requires generating precise actions while maintaining
computational efficiency. Existing methods remain unsatisfactory due to
inherent limitations in the essential action representation and the basic
network architectures. We observe that representing actions in the frequency
domain captures the structured nature of motion more effectively: low-frequency
components reflect global movement patterns, while high-frequency components
encode fine local details. Additionally, robotic manipulation tasks of varying
complexity demand different levels of modeling precision across these frequency
bands. Motivated by this, we propose a novel paradigm for visuomotor policy
learning that progressively models hierarchical frequency components. To
further enhance precision, we introduce continuous latent representations that
maintain smoothness and continuity in the action space. Extensive experiments
across diverse 2D and 3D robotic manipulation benchmarks demonstrate that our
approach outperforms existing methods in both accuracy and efficiency,
showcasing the potential of a frequency-domain autoregressive framework with
continuous tokens for generalized robotic manipulation.

</details>


### [583] [WoMAP: World Models For Embodied Open-Vocabulary Object Localization](https://arxiv.org/abs/2506.01600)
*Tenny Yin,Zhiting Mei,Tao Sun,Lihan Zha,Emily Zhou,Jeremy Bao,Miyu Yamane,Ola Shorinwa,Anirudha Majumdar*

Key words: 语言指令, 物体定位, 高斯Splatting, 潜在世界模型, 仿真到现实迁移

TL;DR: 本文提出了WoMAP方法，通过高斯Splatting和潜在世界模型，解决了语言指令下物体定位任务中的泛化性和物理动作生成问题。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决现有方法在语言指令物体定位任务中泛化性不足和物理动作生成不理想的问题。

Method: 使用高斯Splatting生成数据集，无需专家示范；通过开放词汇物体检测器提取密集奖励信号；利用潜在世界模型预测动力学和奖励。

Result: 在仿真和硬件实验中，WoMAP在零样本物体定位任务中表现优异，成功率为基线方法的9倍和2倍以上。

Conclusion: WoMAP在泛化性和仿真到现实的迁移中表现强大，适用于实际机器人任务。

Abstract: Language-instructed active object localization is a critical challenge for
robots, requiring efficient exploration of partially observable environments.
However, state-of-the-art approaches either struggle to generalize beyond
demonstration datasets (e.g., imitation learning methods) or fail to generate
physically grounded actions (e.g., VLMs). To address these limitations, we
introduce WoMAP (World Models for Active Perception): a recipe for training
open-vocabulary object localization policies that: (i) uses a Gaussian
Splatting-based real-to-sim-to-real pipeline for scalable data generation
without the need for expert demonstrations, (ii) distills dense rewards signals
from open-vocabulary object detectors, and (iii) leverages a latent world model
for dynamics and rewards prediction to ground high-level action proposals at
inference time. Rigorous simulation and hardware experiments demonstrate
WoMAP's superior performance in a broad range of zero-shot object localization
tasks, with more than 9x and 2x higher success rates compared to VLM and
diffusion policy baselines, respectively. Further, we show that WoMAP achieves
strong generalization and sim-to-real transfer on a TidyBot.

</details>


### [584] [Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces](https://arxiv.org/abs/2506.01635)
*Julian Richter,Christopher Erdös,Christian Scheurer,Jochen J. Steil,Niels Dehio*

Key words: 时间对齐,黎曼流形,机器人学习,信号处理

TL;DR: RTW是一种新的方法，通过考虑黎曼流形的几何结构来高效对齐多个信号，优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 现有时间对齐方法主要局限于欧几里得空间，缺乏对黎曼流形数据的通用扩展。

Method: 提出Riemannian Time Warping (RTW)方法，利用黎曼流形的几何结构对齐信号。

Result: 在合成和真实数据（如机器人测试）中，RTW在平均和分类任务上均优于现有方法。

Conclusion: RTW是一种高效的多信号对齐方法，适用于多样化的应用。

Abstract: Temporal alignment of multiple signals through time warping is crucial in
many fields, such as classification within speech recognition or robot motion
learning. Almost all related works are limited to data in Euclidean space.
Although an attempt was made in 2011 to adapt this concept to unit quaternions,
a general extension to Riemannian manifolds remains absent. Given its
importance for numerous applications in robotics and beyond, we introduce
Riemannian Time Warping~(RTW). This novel approach efficiently aligns multiple
signals by considering the geometric structure of the Riemannian manifold in
which the data is embedded. Extensive experiments on synthetic and real-world
data, including tests with an LBR iiwa robot, demonstrate that RTW consistently
outperforms state-of-the-art baselines in both averaging and classification
tasks.

</details>


### [585] [Feel the Force: Contact-Driven Learning from Humans](https://arxiv.org/abs/2506.01944)
*Ademi Adeniji,Zhuoran Chen,Vincent Liu,Venkatesh Pattabiraman,Raunaq Bhirangi,Siddhant Haldar,Pieter Abbeel,Lerrel Pinto*

Key words: 机器人学习，触觉控制，力敏感操作，人类演示

TL;DR: 一个机器人学习系统FeelTheForce（FTF）通过建模人类触觉行为，结合触觉手套和视觉模型，学习精确的力敏感操作。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决机器人在多样化的真实世界交互中力控制的泛化问题，利用人类演示提供可扩展的学习方案。

Method: 使用触觉手套测量接触力，结合视觉模型估计手部姿态，训练闭环策略学习力敏感操作，并通过PD控制器实现精确控制。

Result: 在5个力敏感操作任务中达到77%的成功率。

Conclusion: FTF系统通过人类监督实现了精确的力控制，展示了其在机器人操作中的潜力。

Abstract: Controlling fine-grained forces during manipulation remains a core challenge
in robotics. While robot policies learned from robot-collected data or
simulation show promise, they struggle to generalize across the diverse range
of real-world interactions. Learning directly from humans offers a scalable
solution, enabling demonstrators to perform skills in their natural embodiment
and in everyday environments. However, visual demonstrations alone lack the
information needed to infer precise contact forces. We present FeelTheForce
(FTF): a robot learning system that models human tactile behavior to learn
force-sensitive manipulation. Using a tactile glove to measure contact forces
and a vision-based model to estimate hand pose, we train a closed-loop policy
that continuously predicts the forces needed for manipulation. This policy is
re-targeted to a Franka Panda robot with tactile gripper sensors using shared
visual and action representations. At execution, a PD controller modulates
gripper closure to track predicted forces-enabling precise, force-aware
control. Our approach grounds robust low-level force control in scalable human
supervision, achieving a 77% success rate across 5 force-sensitive manipulation
tasks. Code and videos are available at https://feel-the-force-ftf.github.io.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [586] [Probabilistic Spatial Interpolation of Sparse Data using Diffusion Models](https://arxiv.org/abs/2506.00033)
*Valerie Tsao,Nathaniel W. Chaney,Manolis Veveakis*

Key words: 气候模型, 数据填补, 扩散模型, 初始条件, 不确定性

TL;DR: 论文提出了一种条件数据填补框架，可从仅1%的观测覆盖率重建完整温度场，解决了气候模型中初始条件不确定性的问题。

<details>
  <summary>Details</summary>

Main category: stat.AP

Motivation: 气候模型的预测依赖于“自信”的初始条件，但由于系统的混沌性和数据稀疏性，初始条件的不确定性会导致预测结果严重偏离。

Method: 利用扩散模型和预插值掩码，从极稀疏的观测数据中推断完整温度场。

Result: 在2018-2020年夏季的南方大平原地区验证中，该方法在不同观测密度下均表现出高重建精度。

Conclusion: 该方法可有效填补历史和实时预测中的关键数据空白，提升气候模型的可靠性。

Abstract: The large underlying assumption of climate models today relies on the basis
of a "confident" initial condition, a reasonably plausible snapshot of the
Earth for which all future predictions depend on. However, given the inherently
chaotic nature of our system, this assumption is complicated by sensitive
dependence, where small uncertainties in initial conditions can lead to
exponentially diverging outcomes over time. This challenge is particularly
salient at global spatial scales and over centennial timescales, where data
gaps are not just common but expected. The source of uncertainty is two-fold:
(1) sparse, noisy observations from satellites and ground stations, and (2)
internal variability stemming from the simplifying approximations within the
models themselves.
  In practice, data assimilation methods are used to reconcile this missing
information by conditioning model states on partial observations. Our work
builds on this idea but operates at the extreme end of sparsity. We propose a
conditional data imputation framework that reconstructs full temperature fields
from as little as 1% observational coverage. The method leverages a diffusion
model guided by a prekriged mask, effectively inferring the full-state fields
from minimal data points. We validate our framework over the Southern Great
Plains, focusing on afternoon (12:00-6:00 PM) temperature fields during the
summer months of 2018-2020. Across varying observational densities--from swath
data to isolated in-situ sensors--our model achieves strong reconstruction
accuracy, highlighting its potential to fill in critical data gaps in both
historical reanalysis and real-time forecasting pipelines.

</details>


### [587] [Probabilistic intraday electricity price forecasting using generative machine learning](https://arxiv.org/abs/2506.00044)
*Jieyu Chen,Sebastian Lerch,Melanie Schienle,Tomasz Serafin,Rafał Weron*

Key words: 日内电力交易,生成式神经网络,价格预测,交易策略

TL;DR: 论文提出了一种生成式神经网络模型，用于生成日内电力价格的概率路径预测，并构建有效的交易策略，结果表明其在统计和经济评估中优于现有方法。

<details>
  <summary>Details</summary>

Main category: stat.AP

Motivation: 欧洲日内电力交易的重要性日益增加，需要改进价格预测和决策支持工具。

Method: 采用生成式神经网络模型进行概率路径预测，并构建基于预测的交易策略。

Result: 在统计指标和经济评估中表现优于现有方法，生成的交易策略带来更高利润。

Conclusion: 生成式机器学习工具在电力价格预测中具有潜力，经济评估是关键。

Abstract: The growing importance of intraday electricity trading in Europe calls for
improved price forecasting and tailored decision-support tools. In this paper,
we propose a novel generative neural network model to generate probabilistic
path forecasts for intraday electricity prices and use them to construct
effective trading strategies for Germany's continuous-time intraday market. Our
method demonstrates competitive performance in terms of statistical evaluation
metrics compared to two state-of-the-art statistical benchmark approaches. To
further assess its economic value, we consider a realistic fixed-volume trading
scenario and propose various strategies for placing market sell orders based on
the path forecasts. Among the different trading strategies, the price paths
generated by our generative model lead to higher profit gains than the
benchmark methods. Our findings highlight the potential of generative machine
learning tools in electricity price forecasting and underscore the importance
of economic evaluation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [588] [Supporting architecture evaluation for ATAM scenarios with LLMs](https://arxiv.org/abs/2506.00150)
*Rafael Capilla,J. Andrés Díaz-Pace,Yamid Ramírez,Jennifer Pérez,Vanessa Rodríguez-Horcajo*

Key words: 架构评估, LLM, MS Copilot, 质量场景, 自动化

TL;DR: 利用LLMs部分自动化软件架构评估的研究，通过对比学生与LLM的分析结果，发现LLM在多数情况下能提供更优的质量场景分析。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 传统架构评估依赖人工，效率低且耗时；研究旨在探索LLM是否能为质量场景评估提供更高效的自动化支持。

Method: 采用MS Copilot作为LLM工具，分析学生提出的质量场景，并与学生的人工评估结果对比。

Result: LLM在大多数情况下能更准确地识别风险、敏感点及权衡分析，优于人工评估。

Conclusion: 生成式AI可部分自动化架构评估，辅助人类决策，提升效率。

Abstract: Architecture evaluation methods have long been used to evaluate software
designs. Several evaluation methods have been proposed and used to analyze
tradeoffs between different quality attributes. Having competing qualities
leads to conflicts for selecting which quality-attribute scenarios are the most
suitable ones that an architecture should tackle and for prioritizing the
scenarios required by the stakeholders. In this context, architecture
evaluation is carried out manually, often involving long brainstorming sessions
to decide which are the most adequate quality scenarios. To reduce this effort
and make the assessment and selection of scenarios more efficient, we suggest
the usage of LLMs to partially automate evaluation activities. As a first step
to validate this hypothesis, this work studies MS Copilot as an LLM tool to
analyze quality scenarios suggested by students in a software architecture
course and compares the students' results with the assessment provided by the
LLM. Our initial study reveals that the LLM produces in most cases better and
more accurate results regarding the risks, sensitivity points and tradeoff
analysis of the quality scenarios. Overall, the use of generative AI has the
potential to partially automate and support the architecture evaluation tasks,
improving the human decision-making process.

</details>


### [589] [An LLM Agent for Functional Bug Detection in Network Protocols](https://arxiv.org/abs/2506.00714)
*Mingwei Zheng,Chengpeng Wang,Xuwei Liu,Jinyao Guo,Shiwei Feng,Xiangyu Zhang*

Key words: 功能性错误检测, RFC规范, 大型语言模型, 网络协议实现

TL;DR: RFCScan是一种利用大型语言模型检测网络协议实现中功能性错误的自主代理工具，其通过对比RFC规范与实现代码的一致性，实现了高精度的错误检测。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 功能性错误可能导致严重的网络问题，传统静态分析工具难以检测，因此需要一种高效的方法来确保协议实现的正确性。

Method: RFCScan包含索引代理和检测代理两个核心组件，前者用于生成语义索引，后者通过需求驱动的检索发现潜在的不一致性。

Result: 在六个真实网络协议实现中，RFCScan检测到47个功能错误，精度达81.9%，其中20个错误已被开发者确认或修复。

Conclusion: RFCScan通过结合LLM和分层语义分析，为网络协议的功能正确性检测提供了一种有效的自动化解决方案。

Abstract: Functional correctness is critical for ensuring the reliability and security
of network protocol implementations. Functional bugs, instances where
implementations diverge from behaviors specified in RFC documents, can lead to
severe consequences, including faulty routing, authentication bypasses, and
service disruptions. Detecting these bugs requires deep semantic analysis
across specification documents and source code, a task beyond the capabilities
of traditional static analysis tools. This paper introduces RFCScan, an
autonomous agent that leverages large language models (LLMs) to detect
functional bugs by checking conformance between network protocol
implementations and their RFC specifications. Inspired by the human auditing
procedure, RFCScan comprises two key components: an indexing agent and a
detection agent. The former hierarchically summarizes protocol code semantics,
generating semantic indexes that enable the detection agent to narrow down the
scanning scope. The latter employs demand-driven retrieval to iteratively
collect additional relevant data structures and functions, eventually
identifying potential inconsistencies with the RFC specifications effectively.
We evaluate RFCScan across six real-world network protocol implementations.
RFCScan identifies 47 functional bugs with 81.9% precision, of which 20 bugs
have been confirmed or fixed by developers.

</details>


### [590] [CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning](https://arxiv.org/abs/2506.00750)
*Monoshi Kumar Roy,Simin Chen,Benjamin Steenhoek,Jinjun Peng,Gail Kaiser,Baishakhi Ray,Wei Le*

Key words: 代码推理,LLM,软件工程,基准测试,细粒度语义

TL;DR: CodeSense是首个针对现实世界软件工程中细粒度代码推理任务的基准测试，填补了现有粗粒度测试的不足。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现有代码推理基准多依赖合成数据或教育性问题，无法有效评估LLM在实际软件工程任务中的表现。

Method: 从现实项目仓库收集Python、C和Java代码，执行测试并收集执行轨迹，构建细粒度语义推理任务数据集。

Result: 评估显示LLM在细粒度推理任务上表现不佳，提示技术虽有帮助，但缺乏代码语义理解限制了模型能力。

Conclusion: CodeSense为未来基准构建和模型训练提供了工具和基础，数据集和代码已开源。

Abstract: Understanding and reasoning about code semantics is essential for enhancing
code LLMs' abilities to solve real-world software engineering (SE) tasks.
Although several code reasoning benchmarks exist, most rely on synthetic
datasets or educational coding problems and focus on coarse-grained reasoning
tasks such as input/output prediction, limiting their effectiveness in
evaluating LLMs in practical SE contexts. To bridge this gap, we propose
CodeSense, the first benchmark that makes available a spectrum of fine-grained
code reasoning tasks concerned with the software engineering of real-world
code. We collected Python, C and Java software projects from real-world
repositories. We executed tests from these repositories, collected their
execution traces, and constructed a ground truth dataset for fine-grained
semantic reasoning tasks. We then performed comprehensive evaluations on
state-of-the-art LLMs. Our results show a clear performance gap for the models
to handle fine-grained reasoning tasks. Although prompting techniques such as
chain-of-thought and in-context learning helped, the lack of code semantics in
LLMs fundamentally limit models' capabilities of code reasoning. Besides
dataset, benchmark and evaluation, our work produced an execution tracing
framework and tool set that make it easy to collect ground truth for
fine-grained SE reasoning tasks, offering a strong basis for future benchmark
construction and model post training. Our code and data are located at
https://codesense-bench.github.io/.

</details>


### [591] [Behavioral Augmentation of UML Class Diagrams: An Empirical Study of Large Language Models for Method Generation](https://arxiv.org/abs/2506.00788)
*Djaber Rouabhia,Ismail Hadjadj*

Key words: UML类图, 大型语言模型, 自然语言用例, 行为建模, 敏捷开发

TL;DR: 该研究评估了九种大型语言模型（LLM）在自动化补全无方法的UML类图方面的性能，结果显示LLM能够生成结构良好的方法，但在注释和签名一致性方面仍需改进。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 自动化从自然语言用例中为UML类图补充行为方法是当前的重要挑战，研究者希望通过评估LLM的性能来推动这一领域的进展。

Method: 研究使用21个结构化废物管理用例，评估了九种LLM生成的90个UML图（3,373个方法），并通过六项指标（如方法数量、签名丰富性等）进行分析。

Result: 所有LLM均能生成符合UML规范的PlantUML图，部分模型在方法覆盖率和注释准确性上表现突出，但整体仍需改进注释和签名一致性。

Conclusion: LLM可作为软件设计的协作工具，但需人类监督以确保准确性；其快速生成能力支持敏捷开发的设计迭代。

Abstract: Automating the enrichment of UML class diagrams with behavioral methods from
natural language use cases is a significant challenge. This study evaluates
nine large language models (LLMs) in augmenting a methodless UML diagram (21
classes, 17 relationships) using 21 structured waste-management use cases. A
total of 90 diagrams (3,373 methods) were assessed across six metrics: method
quantity, signature richness (visibility, names, parameters, return types),
annotation completeness (linking to use cases/actions), structural fidelity,
syntactic correctness (PlantUML compilation), and naming convergence (across
models). All LLMs produced valid PlantUML diagrams adhering to UML conventions.
Some models excelled in method coverage and annotation accuracy, while others
showed richer parameterization but weaker traceability. These results
demonstrate that LLMs can generate well-structured methods with consistent
naming, advancing automated behavioral modeling. However, inconsistencies in
annotations and signatures highlight the need for improved prompt engineering
and model selection. The rapid generation of these methods supports Agile
practices by enabling faster design iterations. Despite their capabilities,
human oversight is essential to ensure accuracy, appropriateness, and semantic
alignment. This positions LLMs as collaborative partners in software design.
All experimental artifacts (\texttt{.puml}, \texttt{.png}, \texttt{.csv}) are
publicly available for reproducibility.

</details>


### [592] [Applying Large Language Models to Issue Classification: Revisiting with Extended Data and New Models](https://arxiv.org/abs/2506.00128)
*Gabriel Aracena,Kyle Luster,Fabio Santos,Igor Steinmacher,Marco A. Gerosa*

Key words: 问题分类,大型语言模型,GPT-4o,DeepSeek R1,软件工程,自动化

TL;DR: 本文探讨了利用大型语言模型（LLM）自动化分类软件工程中的问题报告，以减少对大规模训练数据的依赖，并展示了GPT-4o在性能上的优势。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 手动分类问题报告效率低下且难以扩展，传统机器学习方法需要大量训练数据。本文旨在利用LLM开发一种高效且数据需求较低的自动化分类方案。

Method: 研究选择两种主流LLM（GPT-4o和DeepSeek R1），通过多个数据集比较其在问题分类中的性能。

Result: 在多个实验中，GPT-4o表现最佳，尤其是在NLBSE 2024竞赛数据上，其F1分数比DeepSeek R1高20%，平均F1得分为80.7%。增加数据集规模并未显著提升性能。

Conclusion: LLM（尤其是GPT-4o）能显著提升问题分类的效率，减少对大规模训练数据的依赖。

Abstract: Effective prioritization of issue reports in software engineering helps to
optimize resource allocation and information recovery. However, manual issue
classification is laborious and lacks scalability. As an alternative, many open
source software (OSS) projects employ automated processes for this task, yet
this method often relies on large datasets for adequate training.
Traditionally, machine learning techniques have been used for issue
classification. More recently, large language models (LLMs) have emerged as
powerful tools for addressing a range of software engineering challenges,
including code and test generation, mapping new requirements to legacy software
endpoints, and conducting code reviews. The following research investigates an
automated approach to issue classification based on LLMs. By leveraging the
capabilities of such models, we aim to develop a robust system for prioritizing
issue reports, mitigating the necessity for extensive training data while also
maintaining reliability in classification. In our research, we developed an
LLM-based approach for accurately labeling issues by selecting two of the most
prominent large language models. We then compared their performance across
multiple datasets. Our findings show that GPT-4o achieved the best results in
classifying issues from the NLBSE 2024 competition. Moreover, GPT-4o
outperformed DeepSeek R1, achieving an F1 score 20% higher when both models
were trained on the same dataset from the NLBSE 2023 competition, which was ten
times larger than the NLBSE 2024 dataset. The fine-tuned GPT-4o model attained
an average F1 score of 80.7%, while the fine-tuned DeepSeek R1 model achieved
59.33%. Increasing the dataset size did not improve the F1 score, reducing the
dependence on massive datasets for building an efficient solution to issue
classification.

</details>


### [593] [CODEMENV: Benchmarking Large Language Models on Code Migration](https://arxiv.org/abs/2506.00894)
*Keyuan Cheng,Xudong Shen,Yihao Yang,Tengyue Wang,Yang Cao,Muhammad Asif Ali,Hanbin Wang,Lijie Hu,Di Wang*

Key words: 大语言模型,代码迁移,CODEMENV,GPT-4O,评估基准

TL;DR: 介绍了CODEMENV基准测试，用于评估大语言模型在代码迁移任务中的表现，实验结果显示GPT-4O表现最佳，但模型在处理旧版本函数时存在不足。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 填补大语言模型在代码迁移任务中评估的空白，并分析其在不同环境中的表现。

Method: 设计了CODEMENV基准测试，包含922个Python和Java示例，覆盖三类核心任务，并评估了七种大语言模型的表现。

Result: 平均pass@1得分为26.50%，GPT-4O以43.84%的最高分领先，但模型在处理旧版本函数时存在逻辑不一致性。

Conclusion: 大语言模型在代码迁移任务中表现有限，需进一步改进对旧版本代码的处理能力。

Abstract: Large language models (LLMs) have shown remarkable capabilities across
various software engineering tasks; however, their effectiveness in code
migration, adapting code to run in different environments, remains
insufficiently studied. In this work, we introduce CODEMENV: Code Migration
Across Environment, a new benchmark specifically designed to assess LLMs'
abilities in code migration scenarios. CODEMENV consists of 922 examples
spanning 19 Python and Java packages, and covers three core tasks: (1)
identifying functions incompatible with specific versions, (2) detecting
changes in function definitions, and (3) adapting code to target environments.
Experimental evaluation with seven LLMs on CODEMENV yields an average pass@1
rate of 26.50%, with GPT-4O achieving the highest score at 43.84%. Key findings
include: (i) LLMs tend to be more proficient with newer function versions,
which aids in migrating legacy code, and (ii) LLMs sometimes exhibit logical
inconsistencies by identifying function changes irrelevant to the intended
migration environment. The datasets are available at
https://github.com/xdshen-ai/Benchmark-of-Code-Migration.

</details>


### [594] [Legal Compliance Evaluation of Smart Contracts Generated By Large Language Models](https://arxiv.org/abs/2506.00943)
*Chanuka Wijayakoon,Hai Dong,H. M. N. Dilum Bandara,Zahir Tari,Anurag Soin*

Key words: 智能合约, 法律合规性, 大型语言模型, 度量标准, 代码生成

TL;DR: 论文探讨了利用大型语言模型（LLMs）从自然语言法律合同生成合法合规的智能合约的可行性，并提出了一套新的度量标准来量化合规性。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 确保智能合约的法律合规性具有挑战性，现有方法需要法律和软件开发领域的专业知识及大量手动工作。LLMs在代码生成方面的进展为解决这一问题提供了新思路。

Method: 提出了一套基于建模法律和智能合约行为的度量标准，并测试了四种LLMs生成20份智能合约的合规性。

Result: 所有LLMs生成的代码语法正确，但法律合规性差异显著，较大模型表现更优。所提出的度量标准具有细粒度区分能力，适用于跨领域代码评估。

Conclusion: LLMs可用于生成初步合法的智能合约代码，但需严格审查；提出的度量标准为自动化和自改进的开发流程奠定了基础。

Abstract: Smart contracts can implement and automate parts of legal contracts, but
ensuring their legal compliance remains challenging. Existing approaches such
as formal specification, verification, and model-based development require
expertise in both legal and software development domains, as well as extensive
manual effort. Given the recent advances of Large Language Models (LLMs) in
code generation, we investigate their ability to generate legally compliant
smart contracts directly from natural language legal contracts, addressing
these challenges. We propose a novel suite of metrics to quantify legal
compliance based on modeling both legal and smart contracts as processes and
comparing their behaviors. We select four LLMs, generate 20 smart contracts
based on five legal contracts, and analyze their legal compliance. We find that
while all LLMs generate syntactically correct code, there is significant
variance in their legal compliance with larger models generally showing higher
levels of compliance. We also evaluate the proposed metrics against properties
of software metrics, showing they provide fine-grained distinctions, enable
nuanced comparisons, and are applicable across domains for code from any
source, LLM or developer. Our results suggest that LLMs can assist in
generating starter code for legally compliant smart contracts with strict
reviews, and the proposed metrics provide a foundation for automated and
self-improving development workflows.

</details>


### [595] [Greening AI-enabled Systems with Software Engineering: A Research Agenda for Environmentally Sustainable AI Practices](https://arxiv.org/abs/2506.01774)
*Luís Cruz,João Paulo Fernandes,Maja H. Kirkeby,Silverio Martínez-Fernández,June Sallou,Hina Anwar,Enrique Barba Roque,Justus Bogner,Joel Castaño,Fernando Castor,Aadil Chasmawala,Simão Cunha,Daniel Feitosa,Alexandra González,Andreas Jedlitschka,Patricia Lago,Ana Oprescu,Pooja Rani,João Saraiva,Federica Sarro,Raghavendra Selvan,Karthik Vaidhyanathan,Roberto Verdecchia,Ivan P. Yamshchikov,Henry Muccini*

Key words: AI可持续性，软件工程，绿色计算，能源评估，可持续发展

TL;DR: 该研讨会探讨了软件工程在开发可持续AI系统中的作用，提出了绿色软件和AI研究的关键挑战及未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 随着AI系统对环境影响加剧，研讨会旨在通过软件工程推动可持续解决方案的发展。

Method: 通过主题演讲、快速演讲和协作讨论，参与者识别并优先考虑了能源评估、基准测试、可持续架构等关键挑战。

Result: 提出了一系列开放研究方向和实践建议，以指导基于软件工程原则的环境可持续AI系统开发。

Conclusion: 研讨会为绿色AI和软件研究提供了跨学科交流平台，并为未来研究制定了议程。

Abstract: The environmental impact of Artificial Intelligence (AI)-enabled systems is
increasing rapidly, and software engineering plays a critical role in
developing sustainable solutions. The "Greening AI with Software Engineering"
CECAM-Lorentz workshop (no. 1358, 2025) funded by the Centre Europ\'een de
Calcul Atomique et Mol\'eculaire and the Lorentz Center, provided an
interdisciplinary forum for 29 participants, from practitioners to academics,
to share knowledge, ideas, practices, and current results dedicated to
advancing green software and AI research. The workshop was held February 3-7,
2025, in Lausanne, Switzerland. Through keynotes, flash talks, and
collaborative discussions, participants identified and prioritized key
challenges for the field. These included energy assessment and standardization,
benchmarking practices, sustainability-aware architectures, runtime adaptation,
empirical methodologies, and education. This report presents a research agenda
emerging from the workshop, outlining open research directions and practical
recommendations to guide the development of environmentally sustainable
AI-enabled systems rooted in software engineering principles.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [596] [Using LLMs to Advance the Cognitive Science of Collectives](https://arxiv.org/abs/2506.00052)
*Ilia Sucholutsky,Katherine M. Collins,Nori Jacoby,Bill D. Thompson,Robert D. Hawkins*

Key words: LLMs, 集体认知, 复杂性, 风险, 新方法

TL;DR: 讨论大语言模型（LLMs）在研究集体认知中的应用潜力及可能风险。

<details>
  <summary>Details</summary>

Main category: q-bio.NC

Motivation: 现有研究多聚焦于LLMs在个体认知中的应用，而集体认知领域的探索不足。

Method: 提出利用LLMs解决集体认知研究中复杂性的初步思路，并探讨相关风险。

Result: 明确LLMs在集体认知研究中的潜在作用及需应对的新风险。

Conclusion: LLMs为集体认知研究提供新工具，但需开发新方法应对潜在风险。

Abstract: LLMs are already transforming the study of individual cognition, but their
application to studying collective cognition has been underexplored. We lay out
how LLMs may be able to address the complexity that has hindered the study of
collectives and raise possible risks that warrant new methods.

</details>


### [597] [Human sensory-musculoskeletal modeling and control of whole-body movements](https://arxiv.org/abs/2506.00071)
*Chenhui Zuo,Guohao Lin,Chen Zhang,Shanning Zhuang,Yanan Sui*

Key words: 感官-肌肉骨骼模型,多感官输入,深度强化学习,运动控制,人类行为模拟

TL;DR: 该论文提出了一个名为SMS-Human的人类感官-肌肉骨骼模型，通过整合多感官输入和精确的解剖结构，结合分层强化学习框架，模拟了多种人类运动行为。

<details>
  <summary>Details</summary>

Main category: q-bio.NC

Motivation: 研究旨在通过建立动态感官-肌肉骨骼模型，深入理解运动控制和人类行为，特别是在多感官输入下的复杂动作。

Method: 开发了一个人类感官-肌肉骨骼模型，整合了骨骼、关节、肌肉-肌腱单元以及视觉、前庭觉、本体感觉和触觉等多感官输入，采用分层次的深度强化学习框架来解决高维控制问题。

Result: 模拟了双足行走、视觉引导物体操作和骑车中的人机交互任务，结果显示模拟行为与自然人类行为高度相似，并揭示了无法直接测量的肌肉骨骼动力学。

Conclusion: 该研究为人类运动的感官-运动动力学提供了深入见解，有助于量化理解交互背景下的人类行为，并为具身智能系统设计提供参考。

Abstract: Coordinated human movement depends on the integration of multisensory inputs,
sensorimotor transformation, and motor execution, as well as sensory feedback
resulting from body-environment interaction. Building dynamic models of the
sensory-musculoskeletal system is essential for understanding movement control
and investigating human behaviours. Here, we report a human
sensory-musculoskeletal model, termed SMS-Human, that integrates precise
anatomical representations of bones, joints, and muscle-tendon units with
multimodal sensory inputs involving visual, vestibular, proprioceptive, and
tactile components. A stage-wise hierarchical deep reinforcement learning
framework was developed to address the inherent challenges of high-dimensional
control in musculoskeletal systems with integrated multisensory information.
Using this framework, we demonstrated the simulation of three representative
movement tasks, including bipedal locomotion, vision-guided object
manipulation, and human-machine interaction during bicycling. Our results
showed a close resemblance between natural and simulated human motor
behaviours. The simulation also revealed musculoskeletal dynamics that could
not be directly measured. This work sheds deeper insights into the sensorimotor
dynamics of human movements, facilitates quantitative understanding of human
behaviours in interactive contexts, and informs the design of systems with
embodied intelligence.

</details>


### [598] [Autonomous Behavior and Whole-Brain Dynamics Emerge in Embodied Zebrafish Agents with Model-based Intrinsic Motivation](https://arxiv.org/abs/2506.00138)
*Reece Keller,Alyn Tornell,Felix Pei,Xaq Pitkow,Leo Kozachkov,Aran Nayebi*

Key words: 自主性, 内在动机, 模型驱动, 自然行为, 神经胶质动态

TL;DR: 论文提出了一种新型基于模型的内在驱动方法（3M-Progress），旨在模仿动物的自主探索行为，并将其应用于人工代理以实现类动物的自主性。

<details>
  <summary>Details</summary>

Main category: q-bio.NC

Motivation: 现有强化学习方法在稀疏奖励和无奖励环境中探索行为不一致，且缺乏对自主行为神经基础的深入研究。论文旨在填补这一空白。

Method: 提出3M-Progress方法，通过跟踪智能体的当前世界模型与行为先验之间的差异来激励自然行为。

Result: 3M-Progress训练的人工代理能够捕捉自主行为的斑马鱼的行为模式和全脑神经胶质动态，首次建立了目标驱动的神经胶质计算模型。

Conclusion: 该方法为构建具有动物自主性的人工代理提供了计算框架，并连接了模型内在动机与自然行为。

Abstract: Autonomy is a hallmark of animal intelligence, enabling adaptive and
intelligent behavior in complex environments without relying on external reward
or task structure. Existing reinforcement learning approaches to exploration in
sparse reward and reward-free environments, including class of methods known as
intrinsic motivation, exhibit inconsistent exploration patterns and thus fail
to produce robust autonomous behaviors observed in animals. Moreover, systems
neuroscience has largely overlooked the neural basis of autonomy, focusing
instead on experimental paradigms where animals are motivated by external
reward rather than engaging in unconstrained, naturalistic and task-independent
behavior. To bridge these gaps, we introduce a novel model-based intrinsic
drive explicitly designed to capture robust autonomous exploration observed in
animals. Our method (3M-Progress) motivates naturalistic behavior by tracking
divergence between the agent's current world model and an ethological prior. We
demonstrate that artificial embodied agents trained with 3M-Progress capture
the explainable variance in behavioral patterns and whole-brain neural-glial
dynamics recorded from autonomously-behaving larval zebrafish, introducing the
first goal-driven, population-level model of neural-glial computation. Our
findings establish a computational framework connecting model-based intrinsic
motivation to naturalistic behavior, providing a foundation for building
artificial agents with animal-like autonomy.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [599] [Designing AI Tools for Clinical Care Teams to Support Serious Illness Conversations with Older Adults in the Emergency Department](https://arxiv.org/abs/2506.00241)
*Menglin Zhao,Zhuorui Yong,Ruijia Guan,Kai-Wei Chang,Adrian Haimovich,Kei Ouchi,Timothy Bickmore,Bingsheng Yao,Dakuo Wang,Smit Desai*

Key words: 严重疾病对话，急诊科，老年患者，人工智能，临床实践

TL;DR: 本文探讨了急诊科（ED）环境中与老年患者进行严重疾病对话（SICs）的现状，并提出AI工具的设计指南以支持这一流程。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 研究旨在理解急诊科中严重疾病对话的实践，并探索AI如何帮助克服当前面临的挑战，如数据碎片化、时间限制和情感负担等。

Method: 通过访谈两位领域专家和九名急诊科临床团队成员，采用主题分析法总结出严重疾病对话的四阶段工作流程（识别、准备、进行、记录），并确定各阶段的需求和挑战。

Result: 研究发现了临床团队在严重疾病对话中面临的多种障碍，包括电子健康记录（EHR）数据碎片化、时间压力和文档负担。参与者对AI工具在信息合成、对话支持和自动化文档处理方面的潜力表示兴趣，但强调需保留人际联系和临床自主权。

Conclusion: 研究提出了支持严重疾病对话工作流程的AI工具设计指南，强调需适应现有临床实践并尊重医患关系。

Abstract: Serious illness conversations (SICs), discussions between clinical care teams
and patients with serious, life-limiting illnesses about their values, goals,
and care preferences, are critical for patient-centered care. Without these
conversations, patients often receive aggressive interventions that may not
align with their goals. Clinical care teams face significant barriers when
conducting serious illness conversations with older adult patients in Emergency
Department (ED) settings, where most older adult patients lack documented
treatment goals. To understand current practices and identify AI support
opportunities, we conducted interviews with two domain experts and nine ED
clinical care team members. Through thematic analysis, we characterized a
four-phase serious illness conversation workflow (identification, preparation,
conduction, documentation) and identified key needs and challenges at each
stage. Clinical care teams struggle with fragmented EHR data access, time
constraints, emotional preparation demands, and documentation burdens. While
participants expressed interest in AI tools for information synthesis,
conversational support, and automated documentation, they emphasized preserving
human connection and clinical autonomy. We present design guidelines for AI
tools supporting SIC workflows that fit within existing clinical practices.
This work contributes empirical understanding of ED-based serious illness
conversations and provides design considerations for AI in high-stakes clinical
environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [600] [Advancing AI-assisted Hardware Design with Hierarchical Decentralized Training and Personalized Inference-Time Optimization](https://arxiv.org/abs/2506.00002)
*Hao Mark Chen,Zehuan Zhang,Wanru Zhao,Nicholas Lane,Hongxiang Fan*

Key words: AI辅助硬件设计, 大规模语言模型, 去中心化训练, 个性化推理

TL;DR: 本文提出了一种两阶段框架，通过去中心化训练和个性化推理，解决LLM辅助硬件设计中的三大挑战：数据有限、质量参差和推理效率不足。实验显示该框架语义准确性提升33%~50%，速度提升2.3倍。

<details>
  <summary>Details</summary>

Main category: cs.AR

Motivation: 当前LLM辅助硬件设计的质量尚无法满足实际部署需求，主要受限于数据可用性、质量和推理效率的挑战。

Method: 采用两阶段框架：1) 层次化去中心化训练以利用私有领域数据并优化聚合；2) 个性化推理加速和采样策略提升效率。

Result: 框架在经典和量子基准测试中显著提升语义准确性（33%~50%）和速度（2.3倍）。

Conclusion: 该框架为LLM辅助硬件设计提供了可行解决方案，显著提升了生成质量和效率。

Abstract: Recent years have witnessed a significant increase in the adoption of AI
techniques to enhance electronic design automation. In particular, the
emergence of Large Language Models (LLMs) has sparked significant interest in
LLM-assisted hardware design generation, spanning applications from classical
digital circuits to quantum computing. Despite substantial progress in this
direction, the quality of LLM-generated hardware design still cannot meet the
requirements for practical deployment. In this work, we identify three critical
challenges hindering the development of LLM-assisted hardware design
generation: 1) limited data availability, 2) varied data quality, 3) inadequate
inference-time efficiency. To address these fundamental challenges, this paper
introduces a two-stage framework for AI-assisted hardware design by exploring
decentralized training and personalized inference. In the first stage, we
propose to harness private domain design sources through a hierarchical
decentralized training mechanism that addresses data-sharing constraints. To
mitigate the impact of low-quality data, we identify optimization opportunities
in hardware generation tasks, using user-defined metrics for model aggregation.
The second stage focuses on client personalization to enhance both speed and
quality. We introduce a new metric, Trueput, to analyze LLM-assisted hardware
generation efficiency. To optimize Trueput, we implement personalized
inference-time acceleration and customized sampling strategies. Evaluating both
classical and quantum benchmarks, our experimental results demonstrate that the
proposed two-stage framework can significantly improve the model capability for
hardware design generation. As orthogonal enhancements to existing methods, our
framework can achieve $33\% \sim 50\%$ semantic accuracy improvement and $2.3$
times speedup, depending on the difficulty of the generation tasks.

</details>


### [601] [Rapid yet accurate Tile-circuit and device modeling for Analog In-Memory Computing](https://arxiv.org/abs/2506.00004)
*J. Luquin,C. Mackin,S. Ambrogio,A. Chen,F. Baldi,G. Miralles,M. J. Rasch,J. Büchel,M. Lalwani,W. Ponghiran,P. Solomon,H. Tsai,G. W. Burr,P. Narayanan*

Key words: 模拟内存计算, 神经网络, 设备非理想性, 电路噪声, BERT, ALBERT

TL;DR: 该论文研究了模拟内存计算(AIMC)中的设备和电路非理想性对神经网络准确性的影响，提出了一个数学模型来预测矩阵-向量乘(MVM)操作的输出，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.AR

Motivation: 为了提高深度学习的能效，模拟内存计算(AIMC)被广泛研究，但其设备和电路的非理想性会降低神经网络的任务准确性，因此需要量化这些影响并找到解决方法。

Method: 作者开发了一个数学模型来捕获瞬时电流IR-drop和ADC量化效应，并结合实验测量数据推导出PCM读取噪声的统计模型。此外，这些模型被集成到PyTorch框架中，用于评估其对BERT和ALBERT Transformer网络的影响。

Result: 研究发现，使用简单高斯噪声的硬件感知微调可以抵抗ADC量化和PCM读取噪声，但对IR-drop效果不佳，因为IR-drop是非线性且动态变化的。

Conclusion: 论文指出，更复杂的训练方法（如结合本文提出的Tile-circuit模型）对于在AIMC硬件上部署大型神经网络至关重要。

Abstract: Analog In-Memory Compute (AIMC) can improve the energy efficiency of Deep
Learning by orders of magnitude. Yet analog-domain device and circuit
non-idealities -- within the analog ``Tiles'' performing Matrix-Vector Multiply
(MVM) operations -- can degrade neural-network task accuracy. We quantify the
impact of low-level distortions and noise, and develop a mathematical model for
Multiply-ACcumulate (MAC) operations mapped to analog tiles.
Instantaneous-current IR-drop (the most significant circuit non-ideality), and
ADC quantization effects are fully captured by this model, which can predict
MVM tile-outputs both rapidly and accurately, as compared to much slower
rigorous circuit simulations. A statistical model of PCM read noise at
nanosecond timescales is derived from -- and matched against -- experimental
measurements. We integrate these (statistical) device and (deterministic)
circuit effects into a PyTorch-based framework to assess the accuracy impact on
the BERT and ALBERT Transformer networks. We show that hardware-aware
fine-tuning using simple Gaussian noise provides resilience against ADC
quantization and PCM read noise effects, but is less effective against IR-drop.
This is because IR-drop -- although deterministic -- is non-linear, is changing
significantly during the time-integration window, and is ultimately dependent
on all the excitations being introduced in parallel into the analog tile. The
apparent inability of simple Gaussian noise applied during training to properly
prepare a DNN network for IR-drop during inference implies that more complex
training approaches -- incorporating advances such as the Tile-circuit model
introduced here -- will be critical for resilient deployment of large neural
networks onto AIMC hardware.

</details>


### [602] [Enhancing Finite State Machine Design Automation with Large Language Models and Prompt Engineering Techniques](https://arxiv.org/abs/2506.00001)
*Qun-Kai Lin,Cheng Hsu,Tian-Sheuan Chang*

Key words: Large Language Models, Hardware Description Language, Finite State Machine, Prompt Engineering

TL;DR: 论文研究了三种主要大型语言模型（Claude 3 Opus、ChatGPT-4和ChatGPT-4o）在设计有限状态机（FSM）中的表现，评估了其稳定性、局限性及改进方法，并探索了提示优化方法（TOP Patch）对其成功率的影响。

<details>
  <summary>Details</summary>

Main category: cs.AR

Motivation: 由于大型语言模型（LLM）在硬件描述语言（HDL）设计中的显著兼容性，研究其在实际应用中的表现及改进方法具有重要价值。

Method: 通过HDLBits提供的教学内容，评估模型在FSM设计中的表现，并探索提示优化方法（TOP Patch）对成功率的影响。

Result: 系统性格式提示方法和新型提示优化方法有望应用于HDL设计自动化以外的其他领域，并可能与其他提示工程技术结合。

Conclusion: 提示优化方法（如TOP Patch）对提升LLM在FSM设计中的成功率具有潜力，可进一步推广至其他领域。

Abstract: Large Language Models (LLMs) have attracted considerable attention in recent
years due to their remarkable compatibility with Hardware Description Language
(HDL) design. In this paper, we examine the performance of three major LLMs,
Claude 3 Opus, ChatGPT-4, and ChatGPT-4o, in designing finite state machines
(FSMs). By utilizing the instructional content provided by HDLBits, we evaluate
the stability, limitations, and potential approaches for improving the success
rates of these models. Furthermore, we explore the impact of using the
prompt-refining method, To-do-Oriented Prompting (TOP) Patch, on the success
rate of these LLM models in various FSM design scenarios. The results show that
the systematic format prompt method and the novel prompt refinement method have
the potential to be applied to other domains beyond HDL design automation,
considering its possible integration with other prompt engineering techniques
in the future.

</details>


### [603] [Emerging ML-AI Techniques for Analog and RF EDA](https://arxiv.org/abs/2506.00007)
*Zhengfeng Wu,Ziyi Chen,Nnaemeka Achebe,Vaibhav V. Rao,Pratik Shrestha,Ioannis Savidis*

Key words: 机器学习，EDA，模拟电路，射频电路，自动化，设计优化

TL;DR: 本文综述了机器学习在模拟和射频电路EDA工作流程中的应用，强调了ML在自动化、设计质量和缩短上市时间方面的潜力。

<details>
  <summary>Details</summary>

Main category: cs.AR

Motivation: 探讨ML在模拟设计中面临的独特挑战，如复杂约束、非线性设计空间和高计算成本，以提升EDA工具的性能。

Method: 回顾了最先进的学习和优化技术，涵盖电路约束制定、拓扑生成、器件建模、尺寸、布局和布线等任务。

Result: ML能够增强自动化、提高设计质量并加速上市时间，同时满足模拟或射频电路的规格要求。

Conclusion: 总结了ML在EDA中的应用潜力，并讨论了新兴趋势和挑战，如变化的鲁棒性和互连寄生效应。

Abstract: This survey explores the integration of machine learning (ML) into EDA
workflows for analog and RF circuits, addressing challenges unique to analog
design, which include complex constraints, nonlinear design spaces, and high
computational costs. State-of-the-art learning and optimization techniques are
reviewed for circuit tasks such as constraint formulation, topology generation,
device modeling, sizing, placement, and routing. The survey highlights the
capability of ML to enhance automation, improve design quality, and reduce
time-to-market while meeting the target specifications of an analog or RF
circuit. Emerging trends and cross-cutting challenges, including robustness to
variations and considerations of interconnect parasitics, are also discussed.

</details>


### [604] [AI Accelerators for Large Language Model In-ference: Architecture Analysis and Scaling Strategies](https://arxiv.org/abs/2506.00008)
*Amit Sharma*

Key words: 大型语言模型, AI加速器, 性能研究, 专家并行, 张量并行

TL;DR: 该研究首次对不同商业AI加速器进行了跨架构性能比较，分析了内存层次、计算结构和片上互连，观察了性能差异，并探讨了万亿参数模型的扩展技术。

<details>
  <summary>Details</summary>

Main category: cs.AR

Motivation: 随着大型语言模型（LLMs）的快速发展，对专用硬件需求的增长促使研究商业AI加速器的性能差异及适用场景。

Method: 通过对比GPU芯片、混合封装和晶圆级引擎等不同架构的内存层次、计算结构和互连性能，并结合批量大小和序列长度变化进行分析。

Result: 观察到不同架构间性能差异高达3.7倍，专家并行技术参数效率高但延迟方差较大。

Conclusion: 研究结果为匹配任务与加速器提供了量化指导，并揭示了下一代硬件设计需解决的架构缺口。

Abstract: The rapid growth of large-language models (LLMs) is driving a new wave of
specialized hardware for inference. This paper presents the first
workload-centric, cross-architectural performance study of commercial AI
accelerators, spanning GPU-based chips, hybrid packages, and wafer-scale
engines. We compare memory hierarchies, compute fabrics, and on-chip
interconnects, and observe up to 3.7x performance variation across
architectures as batch size and sequence length change. Four scaling techniques
for trillion-parameter models are examined; expert parallelism offers an 8.4x
parameter-to-compute advantage but incurs 2.1x higher latency variance than
tensor parallelism. These findings provide quantitative guidance for matching
workloads to accelerators and reveal architectural gaps that next-generation
designs must address.

</details>


### [605] [VUSA: Virtually Upscaled Systolic Array Architecture to Exploit Unstructured Sparsity in AI Acceleration](https://arxiv.org/abs/2506.01166)
*Shereef Helal,Alberto Garcia-Ortiz,Lennart Bamberg*

Key words: DNN加速器,脉动阵列,稀疏性,边缘AI,能效优化

TL;DR: VUSA是一种基于稀疏性的脉动阵列架构，通过虚拟扩展实现更大矩阵乘法，节省37%面积和68%功耗，适用于通用AI加速。

<details>
  <summary>Details</summary>

Main category: cs.AR

Motivation: 利用非结构化稀疏性提升DNN加速器效率，尤其针对边缘AI应用。

Method: 提出VUSA架构，根据稀疏性动态虚拟扩展脉动阵列，保持物理MAC单元数量不变。

Result: 在16纳米工艺下，相比基线架构，面积和功耗效率分别提升37%和68%，同时支持任意稀疏度的DNN。

Conclusion: VUSA是一种应用无关的通用AI加速架构，高效且灵活。

Abstract: Leveraging high degrees of unstructured sparsity is a promising approach to
enhance the efficiency of deep neural network DNN accelerators - particularly
important for emerging Edge-AI applications. We introduce VUSA, a
systolic-array architecture that virtually grows based on the present sparsity
to perform larger matrix multiplications with the same number of physical
multiply-accumulate MAC units. The proposed architecture achieves saving by 37%
and 68% in area and power efficiency, respectively, at the same
peak-performance, compared to a baseline systolic array architecture in a
commercial 16-nm technology. Still, the proposed architecture supports
acceleration for any DNN with any sparsity - even no sparsity at all. Thus, the
proposed architecture is application-independent, making it viable for
general-purpose AI acceleration.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [606] [Neural Network-based Information-Theoretic Transceivers for High-Order Modulation Schemes](https://arxiv.org/abs/2506.00368)
*Ngoc Long Pham,Tri Nhu Do*

Key words: 神经网络，端到端通信，自编码器，比特错误率，信噪比

TL;DR: 论文提出了一种基于神经网络（NN）的高效比特接收器，并在此基础上引入了一个基于符号自编码器（AE）的新型端到端系统，通过联合优化物理层的发射器和接收器，证明了其在性能上的优势。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 探索基于神经网络的端到端通信系统，提高计算效率并保持性能，旨在开发人工智能原生的端到端系统。

Method: 提出NN比特接收器，并构建基于符号AE的端到端系统，通过比特错误率（BER）分析评估性能。

Result: AE系统在性能上优于基线架构，尤其是高阶调制方案；训练信噪比（SNR）对系统性能有显著影响。

Conclusion: 基于NN和AE的端到端系统在效率和性能上表现优异，适合高阶调制场景，SNR选择是关键。

Abstract: Neural network (NN)-based end-to-end (E2E) communication systems, in which
each system component may consist of a portion of a neural network, have been
investigated as potential tools for developing artificial intelligence
(Al)-native E2E systems. In this paper, we propose an NN-based bitwise receiver
that improves computational efficiency while maintaining performance comparable
to baseline demappers. Building on this foundation, we introduce a novel
symbol-wise autoencoder (AE)-based E2E system that jointly optimizes the
transmitter and receiver at the physical layer. We evaluate the proposed
NN-based receiver using bit-error rate (BER) analysis to confirm that the
numerical BER achieved by NN-based receivers or transceivers is accurate.
Results demonstrate that the AE-based system outperforms baseline
architectures, particularly for higher-order modulation schemes. We further
show that the training signal-to-noise ratio (SNR) significantly affects the
performance of the systems when inference is conducted at different SNR levels.

</details>


### [607] [Attention-Aided MMSE for OFDM Channel Estimation: Learning Linear Filters with Attention](https://arxiv.org/abs/2506.00452)
*TaeJun Ha,Chaehyun Jung,Hyeonuk Kim,Jeongwoo Park,Jeonghun Park*

Key words: OFDM, 信道估计, 注意力机制, MMSE, Transformer

TL;DR: 提出了一种基于注意力增强的MMSE（A-MMSE）方法，通过结合模型驱动的DNN框架和Transformer注意力机制，实现了高效且低复杂度的信道估计。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 现有基于深度学习的信道估计方法复杂度高，而传统MMSE依赖难以获取的二阶统计量。本文旨在通过注意力机制和模型驱动DNN改进这一不足。

Method: 设计了一种两阶段注意力编码器，学习最优MMSE滤波器，并通过注意力Transformer实现单次线性操作完成信道估计，避免了推理中的非线性激活。

Result: 实验表明，A-MMSE在3GPP TDL信道模型下，在多种SNR条件下均优于基线方法，尤其在性能与复杂度权衡方面表现突出。

Conclusion: A-MMSE及其秩自适应扩展为实际信道估计方法设定了新标准。

Abstract: In orthogonal frequency division multiplexing (OFDM), accurate channel
estimation is crucial. Classical signal processing based approaches, such as
minimum mean-squared error (MMSE) estimation, often require second-order
statistics that are difficult to obtain in practice. Recent deep neural
networks based methods have been introduced to address this; yet they often
suffer from high complexity. This paper proposes an Attention-aided MMSE
(A-MMSE), a novel model-based DNN framework that learns the optimal MMSE filter
via the Attention Transformer. Once trained, the A-MMSE estimates the channel
through a single linear operation for channel estimation, eliminating nonlinear
activations during inference and thus reducing computational complexity. To
enhance the learning efficiency of the A-MMSE, we develop a two-stage Attention
encoder, designed to effectively capture the channel correlation structure.
Additionally, a rank-adaptive extension of the proposed A-MMSE allows flexible
trade-offs between complexity and channel estimation accuracy. Extensive
simulations with 3GPP TDL channel models demonstrate that the proposed A-MMSE
consistently outperforms other baseline methods in terms of normalized MSE
across a wide range of SNR conditions. In particular, the A-MMSE and its
rank-adaptive extension establish a new frontier in the performance complexity
trade-off, redefining the standard for practical channel estimation methods.

</details>


### [608] [Power-of-Two (PoT) Weights in Large Language Models (LLMs)](https://arxiv.org/abs/2506.00315)
*Mahmoud Elgenedy*

Key words: 大语言模型, 量化, 幂次二量化, 边缘计算, 模型压缩

TL;DR: 通过使用幂次二量化（PoT）减少大语言模型（LLM）的复杂性，显著降低内存和计算需求。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 随着LLM参数数量激增，边缘设备面临内存和计算能力不足的挑战，需寻找高效量化方法。

Method: 采用PoT量化线性层权重和转换表，将乘法转换为位移操作以减少计算。

Result: 在Nano-GPT和124M参数GPT-2模型上初步验证，4-6比特表示时交叉熵损失仅增加[1.3-0.88]。

Conclusion: PoT量化方法在模型压缩和计算效率提升方面表现优异，适用于边缘设备。

Abstract: Complexity of Neural Networks is increasing rapidly due to the massive
increase in model parameters. Specifically, in Large Language Models (LLMs),
the number of model parameters has grown exponentially in the past few years,
for example, from 1.5 billion parameters in GPT2 to 175 billion in GPT3. This
raises a significant challenge for implementation, especially for Edge devices
where memory and processing power are very limited. In this work, we
investigate reducing LLM complexity with special type of quantization, power of
two (PoT), for linear layers weights and transformer tables. PoT not only
provides memory reduction but more importantly provides significant
computational reduction through converting multiplication to bit shifting. We
obtained preliminary results of PoT quantization on Nano-GPT implementation
using Shakespeare dataset. We then extended results to 124-M GPT-2 model. The
PoT quantization results are shown to be very promising with cross entropy loss
degradation $\approx$[1.3-0.88] with number of bits range [4-6] to represent
power levels.

</details>


### [609] [From Turbulence to Tranquility: AI-Driven Low-Altitude Network](https://arxiv.org/abs/2506.01378)
*Kürşat Tekbıyık,Amir Hossein Fahim Raouf,İsmail Güvenç,Mingzhe Chen,Güneş Karabulut Kurt,Antoine Lesage-Landry*

Key words: 低空经济, 机器学习, 人工智能, 资源分配, 联邦学习

TL;DR: 研究探讨了低空经济网络的智能化发展路径，重点解决频谱管理、干扰缓解和实时协调等挑战。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 低空经济网络在提升城市流动性、应急响应和空中物流方面具有潜力，但面临频谱管理和实时协调等挑战。

Method: 采用机器学习进行频谱感知与共存、AI优化资源分配和轨迹规划，并通过测试平台验证标准化。

Result: 联邦学习和强化学习支持去中心化、适应性的决策机制，AERPAW等平台连接仿真与实际部署。

Conclusion: 研究提出了高效、互操作的AI驱动低空经济生态系统的前瞻性发展路线。

Abstract: Low Altitude Economy (LAE) networks own transformative potential in urban
mobility, emergency response, and aerial logistics. However, these networks
face significant challenges in spectrum management, interference mitigation,
and real-time coordination across dynamic and resource-constrained
environments. After addressing these challenges, this study explores three core
elements for enabling intelligent LAE networks as follows machine
learning-based spectrum sensing and coexistence, artificial intelligence
(AI)-optimized resource allocation and trajectory planning, and testbed-driven
validation and standardization. We highlight how federated and reinforcement
learning techniques support decentralized, adaptive decision-making under
mobility and energy constraints. In addition, we discuss the role of real-world
platforms such as AERPAW in bridging the gap between simulation and deployment
and enabling iterative system refinement under realistic conditions. This study
aims to provide a forward-looking roadmap toward developing efficient and
interoperable AI-driven LAE ecosystems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [610] [Literature Review Of Multi-Agent Debate For Problem-Solving](https://arxiv.org/abs/2506.00066)
*Arne Tillmann*

Key words: MA-LLM, 多智能体, 通信结构, 决策过程, 可扩展性

TL;DR: 综述总结了多智能体大语言模型（MA-LLMs）的最新研究，展示了其如何通过多智能体协作超越单智能体模型，同时指出了计算成本高和独特挑战等问题。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 旨在解决领域内缺乏直接比较的问题，分析如可扩展性、通信结构和决策过程等因素对MA-LLM性能的影响。

Method: 通过综述研究，结合传统多智能体系统与前沿MA-LLM研究，分析智能体配置、通信结构和决策流程。

Result: 多智能体方法表现更优，但面临计算成本高和MA-LLM特有的未充分探索的挑战。

Conclusion: 多智能体方案能提供更强性能，但需进一步解决效率问题；为研究者和实践者提供了发展路线。

Abstract: Multi-agent large language models (MA-LLMs) are a rapidly growing research
area that leverages multiple interacting language agents to tackle complex
tasks, outperforming single-agent large language models. This literature review
synthesizes the latest research on agent profiles, communication structures,
and decision-making processes, drawing insights from both traditional
multi-agent systems and state-of-the-art MA-LLM studies. In doing so, it aims
to address the lack of direct comparisons in the field, illustrating how
factors like scalability, communication structure, and decision-making
processes influence MA-LLM performance. By examining frequent practices and
outlining current challenges, the review reveals that multi-agent approaches
can yield superior results but also face elevated computational costs and
under-explored challenges unique to MA-LLM. Overall, these findings provide
researchers and practitioners with a roadmap for developing robust and
efficient multi-agent AI solutions.

</details>


### [611] [Sorrel: A simple and flexible framework for multi-agent reinforcement learning](https://arxiv.org/abs/2506.00228)
*Rebekah A. Gelpí,Yibing Ju,Ethan C. Jackson,Yikai Tang,Shon Verch,Claas Voelcker,William A. Cunningham*

Key words: 多智能体强化学习, 社会交互, 群体动态, 心理学, Python 接口

TL;DR: Sorrel 是一个简单的 Python 接口，用于生成和测试多智能体强化学习环境，强调简单性和可访问性，适合社会科学家研究学习与社交互动如何影响群体动态。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 为研究学习与社交互动对群体动态的影响，提供一个简单且心理直觉化的工具。

Method: 设计了一个强调简单性和可访问性的 Python 接口，采用心理直觉化的智能体-环境循环结构。

Result: Sorrel 能够帮助社会科学家便捷地进行多智能体强化学习实验。

Conclusion: Sorrel 是一个适合社会科学家研究群体动态的工具，设计简洁且易于使用。

Abstract: We introduce Sorrel (https://github.com/social-ai-uoft/sorrel), a simple
Python interface for generating and testing new multi-agent reinforcement
learning environments. This interface places a high degree of emphasis on
simplicity and accessibility, and uses a more psychologically intuitive
structure for the basic agent-environment loop, making it a useful tool for
social scientists to investigate how learning and social interaction leads to
the development and change of group dynamics. In this short paper, we outline
the basic design philosophy and features of Sorrel.

</details>


### [612] [Agentic AI and Multiagentic: Are We Reinventing the Wheel?](https://arxiv.org/abs/2506.01463)
*V. Botti*

Key words: Agentic AI, Multiagentic AI, 智能代理, 多代理系统, 大语言模型

TL;DR: 本文批判性地分析了‘Agentic AI’和‘Multiagentic AI’这两个流行术语的概念混淆问题，强调其与AI领域已有概念‘智能代理’和‘多代理系统’的重复，并呼吁科学严谨性。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 探讨当前对‘Agentic AI’和‘Multiagentic AI’术语的滥用，澄清其与已有AI概念的关系，避免忽略多年研究成果。

Method: 回顾‘agentic’的理论起源、哲学概念及经典代理架构，分析LLM驱动的AI代理平台发展。

Result: 指出这些新术语实质上是已有概念的变体，容易被误用为营销噱头。

Conclusion: 提倡使用规范的AI术语，整合现有知识，避免重复研究。

Abstract: The terms Agentic AI and Multiagentic AI have recently gained popularity in
discussions on generative artificial intelligence, often used to describe
autonomous software agents and systems composed of such agents. However, the
use of these terms confuses these buzzwords with well-established concepts in
AI literature: intelligent agents and multi-agent systems. This article offers
a critical analysis of this conceptual misuse. We review the theoretical
origins of "agentic" in the social sciences (Bandura, 1986) and philosophical
notions of intentionality (Dennett, 1971), and then summarise foundational
works on intelligent agents and multi-agent systems by Wooldridge, Jennings and
others. We examine classic agent architectures, from simple reactive agents to
Belief-Desire-Intention (BDI) models, and highlight key properties (autonomy,
reactivity, proactivity, social capability) that define agency in AI. We then
discuss recent developments in large language models (LLMs) and agent platforms
based on LLMs, including the emergence of LLM-powered AI agents and open-source
multi-agent orchestration frameworks. We argue that the term AI Agentic is
often used as a buzzword for what are essentially AI agents, and AI
Multiagentic for what are multi-agent systems. This confusion overlooks decades
of research in the field of autonomous agents and multi-agent systems. The
article advocates for scientific and technological rigour and the use of
established terminology from the state of the art in AI, incorporating the
wealth of existing knowledge, including standards for multi-agent system
platforms, communication languages and coordination and cooperation algorithms,
agreement technologies (automated negotiation, argumentation, virtual
organisations, trust, reputation, etc.), into the new and promising wave of
LLM-based AI agents, so as not to end up reinventing the wheel.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [613] [Recover Experimental Data with Selection Bias using Counterfactual Logic](https://arxiv.org/abs/2506.00335)
*Jingyang He,Shuai Wang,Ang Li*

Key words: 选择偏差, 因果推断, 结构因果模型, 反事实世界, 实验数据

TL;DR: 该论文探讨了在存在选择偏差的情况下如何利用实验数据恢复无偏的因果推断分布，提出了通过结构因果模型构建反事实世界的方法，并提供了图形和理论准则来验证实验分布不受选择偏差影响。

<details>
  <summary>Details</summary>

Main category: stat.ME

Motivation: 选择偏差会严重影响因果推断的有效性，现有方法依赖于观测数据且复杂度高，限制了实际应用。本文旨在解决这一问题，通过实验数据恢复无偏分布。

Method: 使用结构因果模型（SCMs）明确构建反事实世界，分析选择机制如何传播，并提出了图形和理论准则。此外，提出了利用部分无偏观测数据的方法。

Result: 模拟研究表明，该方法在现实研究场景中具有实用性，为减少应用因果推断中的选择偏差提供了具体指导。

Conclusion: 该研究为在存在选择偏差的情况下进行因果推断提供了理论和方法支持，具有实际应用价值。

Abstract: Selection bias, arising from the systematic inclusion or exclusion of certain
samples, poses a significant challenge to the validity of causal inference.
While Bareinboim et al. introduced methods for recovering unbiased
observational and interventional distributions from biased data using partial
external information, the complexity of the backdoor adjustment and the
method's strong reliance on observational data limit its applicability in many
practical settings. In this paper, we formally discover the recoverability of
$P(Y^*_{x^*})$ under selection bias with experimental data. By explicitly
constructing counterfactual worlds via Structural Causal Models (SCMs), we
analyze how selection mechanisms in the observational world propagate to the
counterfactual domain. We derive a complete set of graphical and theoretical
criteria to determine that the experimental distribution remain unaffected by
selection bias. Furthermore, we propose principled methods for leveraging
partially unbiased observational data to recover $P(Y^*_{x^*})$ from biased
experimental datasets. Simulation studies replicating realistic research
scenarios demonstrate the practical utility of our approach, offering concrete
guidance for mitigating selection bias in applied causal inference.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [614] [Trilevel Memetic Algorithm for the Electric Vehicle Routing Problem](https://arxiv.org/abs/2506.01065)
*Ivan Milinović,Leon Stjepan Uroić,Marko Đurasević*

Key words: 电动汽车路径问题, 遗传算法, 动态编程, 可持续物流

TL;DR: 该论文提出了一种三级遗传算法（TMA），用于解决电动汽车路径问题（EVRP），通过分层优化客户序列、路径分配和充电站插入来应对电池约束和充电站的挑战。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 电动汽车路径问题因电池限制和充电站的引入而增加了优化难度，需要高效的算法来解决这一问题。

Method: 结合遗传算法和动态编程的三级遗传算法（TMA），分层优化客户序列、路径分配和充电站插入。

Result: 在WCCI2020基准测试中表现出色，小规模案例中匹配了最佳已知结果，尽管计算需求限制了其扩展性。

Conclusion: TMA展现了在可持续物流规划中的强大潜力，但计算效率仍需改进。

Abstract: The Electric Vehicle Routing Problem (EVRP) extends the capacitated vehicle
routing problem by incorporating battery constraints and charging stations,
posing significant optimization challenges. This paper introduces a Trilevel
Memetic Algorithm (TMA) that hierarchically optimizes customer sequences, route
assignments, and charging station insertions. The method combines genetic
algorithms with dynamic programming, ensuring efficient and high-quality
solutions. Benchmark tests on WCCI2020 instances show competitive performance,
matching best-known results for small-scale cases. While computational demands
limit scalability, TMA demonstrates strong potential for sustainable logistics
planning.

</details>


### [615] [Speeding Up Hyper-Heuristics With Markov-Chain Operator Selection and the Only-Worsening Acceptance Operator](https://arxiv.org/abs/2506.01107)
*Abderrahim Bendahi,Benjamin Doerr,Adrien Fradin,Johannes F. Lutzeyer*

Key words: 超启发式, 局部最优, 马尔可夫链, Jump函数, 移动接受

TL;DR: 该论文提出了一种改进的移动接受超启发式算法，通过引入两种修改（基于马尔可夫链的选择和仅接受恶化解的算子），显著提高了在多种基准函数上的性能。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 旨在解决现有移动接受超启发式算法在跳出局部最优时效率不足的问题。

Method: 提出了两种改进：1）使用马尔可夫链选择接受算子；2）引入仅接受恶化解的新算子。

Result: 改进后的算法在Jump函数上的运行时间从指数级降低到多项式级，且在新的SEQOPTk基准类上表现出色。

Conclusion: 通过马尔可夫链和仅接受恶化解的算子，显著提升了算法跳出局部最优的效率。

Abstract: The move-acceptance hyper-heuristic was recently shown to be able to leave
local optima with astonishing efficiency (Lissovoi et al., Artificial
Intelligence (2023)). In this work, we propose two modifications to this
algorithm that demonstrate impressive performances on a large class of
benchmarks including the classic Cliff$_d$ and Jump$_m$ function classes. (i)
Instead of randomly choosing between the only-improving and any-move acceptance
operator, we take this choice via a simple two-state Markov chain. This
modification alone reduces the runtime on Jump$_m$ functions with gap parameter
$m$ from $\Omega(n^{2m-1})$ to $O(n^{m+1})$. (ii) We then replace the all-moves
acceptance operator with the operator that only accepts worsenings. Such a,
counter-intuitive, operator has not been used before in the literature.
However, our proofs show that our only-worsening operator can greatly help in
leaving local optima, reducing, e.g., the runtime on Jump functions to $O(n^3
\log n)$ independent of the gap size. In general, we prove a remarkably good
runtime of $O(n^{k+1} \log n)$ for our Markov move-acceptance hyper-heuristic
on all members of a new benchmark class SEQOPT$_k$, which contains a large
number of functions having $k$ successive local optima, and which contains the
commonly studied Jump$_m$ and Cliff$_d$ functions for $k=2$.

</details>


### [616] [SpiceMixer -- Netlist-Level Circuit Evolution](https://arxiv.org/abs/2506.01497)
*Stefan Uhlich,Andrea Bonetti,Arun Venkitaraman,Chia-Yu Hsieh,Mustafa Emre Gürsoy,Ryoga Matsuo,Lorenzo Servadei*

Key words: SpiceMixer, 遗传算法, SPICE 网表, 模拟电路, 电路合成

TL;DR: SpiceMixer 是一种基于遗传算法的工具，用于通过演化 SPICE 网表来合成新颖的模拟电路。与传统方法不同，它直接操作网表，支持通用遗传操作，并在多个任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 旨在开发一种能够直接操作 SPICE 网表的遗传算法，以提升模拟电路合成的灵活性和性能。

Method: 采用标准化网表格式，结合交叉、变异和剪枝等遗传操作器，实现对各种组件的兼容性。

Result: 在标准单元（如反相器、二输入 NAND、锁存器）和 Iris 数据集模拟分类电路的设计中表现出色，测试集准确率达到89%。

Conclusion: SpiceMixer 在模拟电路合成任务中显著优于现有方法，展现了其高效性和通用性。

Abstract: This paper introduces SpiceMixer, a genetic algorithm developed to synthesize
novel analog circuits by evolving SPICE netlists. Unlike conventional methods,
SpiceMixer operates directly on netlist lines, enabling compatibility with any
component or subcircuit type and supporting general-purpose genetic operations.
By using a normalized netlist format, the algorithm enhances the effectiveness
of its genetic operators: crossover, mutation, and pruning. We show that
SpiceMixer achieves superior performance in synthesizing standard cells
(inverter, two-input NAND, and latch) and in designing an analog classifier
circuit for the Iris dataset, reaching an accuracy of 89% on the test set.
Across all evaluated tasks, SpiceMixer consistently outperforms existing
synthesis methods.

</details>


### [617] [Engram Memory Encoding and Retrieval: A Neurocomputational Perspective](https://arxiv.org/abs/2506.01659)
*Daniel Szelogowski*

Key words: 记忆印记（engram）, 稀疏性, 可塑性, 计算模型, 神经科学

TL;DR: 该论文整合了细胞神经科学和计算建模的见解，探讨了记忆印记（engram）研究中的关键问题，提出了一个综合理论框架，并为未来的记忆机制研究提供了方向。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 尽管对记忆的生物学基础进行了大量研究，但大脑中经验编码、存储和检索的精确机制仍不完全清楚。需要综合生物学发现和计算模型来理解记忆印记的作用。

Method: 结合细胞神经科学的实验发现和计算建模（如稀疏正则化、记忆印记门控、稀疏分布式记忆和脉冲神经网络等），分析记忆印记的机制。

Result: 研究发现记忆的效率、容量和稳定性源于可塑性和稀疏性约束的相互作用。综合神经生物和计算视角为记忆印记研究提供了理论基础。

Conclusion: 论文为记忆印记研究提供了全面的理论框架，并提出了未来研究的方向，对记忆相关疾病的诊断和治疗具有重要意义。

Abstract: Despite substantial research into the biological basis of memory, the precise
mechanisms by which experiences are encoded, stored, and retrieved in the brain
remain incompletely understood. A growing body of evidence supports the engram
theory, which posits that sparse populations of neurons undergo lasting
physical and biochemical changes to support long-term memory. Yet, a
comprehensive computational framework that integrates biological findings with
mechanistic models remains elusive. This work synthesizes insights from
cellular neuroscience and computational modeling to address key challenges in
engram research: how engram neurons are identified and manipulated; how
synaptic plasticity mechanisms contribute to stable memory traces; and how
sparsity promotes efficient, interference-resistant representations. Relevant
computational approaches -- such as sparse regularization, engram gating, and
biologically inspired architectures like Sparse Distributed Memory and spiking
neural networks -- are also examined. Together, these findings suggest that
memory efficiency, capacity, and stability emerge from the interaction of
plasticity and sparsity constraints. By integrating neurobiological and
computational perspectives, this paper provides a comprehensive theoretical
foundation for engram research and proposes a roadmap for future inquiry into
the mechanisms underlying memory, with implications for the diagnosis and
treatment of memory-related disorders.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [618] [Can AI Master Econometrics? Evidence from Econometrics AI Agent on Expert-Level Tasks](https://arxiv.org/abs/2506.00856)
*Qiang Chen,Tianyang Han,Jin Li,Ye Luo,Yuxiao Wu,Xiaowei Zhang,Tuo Zhou*

Key words: 计量经济学, AI代理, MetaGPT, 社会科学研究

TL;DR: 本研究开发了一个基于MetaGPT框架的『计量经济学AI代理』，评估其在复杂计量经济分析中的表现，结果显示该代理在任务规划、代码生成与执行、错误反思及迭代优化方面表现突出，显著优于通用AI模型，为社会科学研究提供了新工具。

<details>
  <summary>Details</summary>

Main category: econ.EM

Motivation: 探索AI是否能够替代人类专家执行复杂的计量经济分析，并评估其在实际任务中的性能。

Method: 开发基于MetaGPT框架的计量经济学AI代理，通过规划任务、生成与执行代码、错误反思和迭代优化，并构建两个数据集进行性能评估。

Result: 该代理在计量经济分析中表现优异，显著超越通用AI模型和基准大语言模型。

Conclusion: 该研究为AI在社会科学研究中的应用提供了测试平台，降低了使用高级计量方法的门槛，并提升了研究可重现性和教学潜力。

Abstract: Can AI effectively perform complex econometric analysis traditionally
requiring human expertise? This paper evaluates an agentic AI's capability to
master econometrics, focusing on empirical analysis performance. We develop an
``Econometrics AI Agent'' built on the open-source MetaGPT framework. This
agent exhibits outstanding performance in: (1) planning econometric tasks
strategically, (2) generating and executing code, (3) employing error-based
reflection for improved robustness, and (4) allowing iterative refinement
through multi-round conversations. We construct two datasets from academic
coursework materials and published research papers to evaluate performance
against real-world challenges. Comparative testing shows our domain-specialized
agent significantly outperforms both benchmark large language models (LLMs) and
general-purpose AI agents. This work establishes a testbed for exploring AI's
impact on social science research and enables cost-effective integration of
domain expertise, making advanced econometric methods accessible to users with
minimal coding expertise. Furthermore, our agent enhances research
reproducibility and offers promising pedagogical applications for econometrics
teaching.

</details>


### [619] [Stock Market Telepathy: Graph Neural Networks Predicting the Secret Conversations between MINT and G7 Countries](https://arxiv.org/abs/2506.01945)
*Nurbanu Bursa*

Key words: MINT国家, G7国家, 股市预测, 图神经网络, 时空关联

TL;DR: 该论文研究了G7和MINT国家股市之间的相互关系，发现美加是G7中对股市最影响的国家，印尼和土耳其则在MINT中最具影响力。采用MTGNN方法优于传统预测方法。

<details>
  <summary>Details</summary>

Main category: econ.EM

Motivation: 研究新兴经济体（MINT国家）与发达经济体（G7国家）股市之间的关联性，为投资者和政策制定者提供股市预测的准确工具。

Method: 使用图神经网络（MTGNN）分析2012至2024年G7和MINT国家的主要股市指数，考虑复杂的时空关联。

Result: MTGNN显示美加在G7中影响力最大，印尼和土耳其在MINT中最具影响力；且MTGNN预测效果优于传统方法。

Conclusion: 研究为全球经济体股市分析提供了新视角，MTGNN是一种有效的全球股市动态分析工具。

Abstract: Emerging economies, particularly the MINT countries (Mexico, Indonesia,
Nigeria, and T\"urkiye), are gaining influence in global stock markets,
although they remain susceptible to the economic conditions of developed
countries like the G7 (Canada, France, Germany, Italy, Japan, the United
Kingdom, and the United States). This interconnectedness and sensitivity of
financial markets make understanding these relationships crucial for investors
and policymakers to predict stock price movements accurately. To this end, we
examined the main stock market indices of G7 and MINT countries from 2012 to
2024, using a recent graph neural network (GNN) algorithm called multivariate
time series forecasting with graph neural network (MTGNN). This method allows
for considering complex spatio-temporal connections in multivariate time
series. In the implementations, MTGNN revealed that the US and Canada are the
most influential G7 countries regarding stock indices in the forecasting
process, and Indonesia and T\"urkiye are the most influential MINT countries.
Additionally, our results showed that MTGNN outperformed traditional methods in
forecasting the prices of stock market indices for MINT and G7 countries.
Consequently, the study offers valuable insights into economic blocks' markets
and presents a compelling empirical approach to analyzing global stock market
dynamics using MTGNN.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [620] [Randomized Dimensionality Reduction for Euclidean Maximization and Diversity Measures](https://arxiv.org/abs/2506.00165)
*Jie Gao,Rajesh Jayaram,Benedikt Kolbe,Shay Sapir,Chris Schwiegelshohn,Sandeep Silwal,Erik Waingarten*

Key words: 随机降维, 双倍维度, 最大化问题, 优化, 计算效率

TL;DR: 本文研究了随机降维技术在各种最大化问题（如最大匹配、最大生成树、最大TSP等）中的应用，发现降维效果与数据集的内在维度（双倍维度λₓ）密切相关。研究表明，目标维度O(λₓ)足以近似保留最优解，且在某些问题中是必要的。与传统降维方法相比，该方法不依赖于数据集大小|X|。

<details>
  <summary>Details</summary>

Main category: cs.DS

Motivation: 探索随机降维技术在高维优化问题中的应用，特别是如何通过内在维度（双倍维度）来优化降维效果，从而提升计算效率。

Method: 通过理论分析和实验验证，研究降维技术对多种最大化问题的解决方案和数据集多样性的影响。重点关注双倍维度λₓ与降维效果的关系。

Result: 研究表明，目标维度O(λₓ)可以近似保留最优解，这在某些问题是必要的。实验验证了降维后的解质量和计算加速效果。

Conclusion: 双倍维度是决定降维效果的关键因素。与传统方法相比，该方法更高效且不依赖于数据集大小。

Abstract: Randomized dimensionality reduction is a widely-used algorithmic technique
for speeding up large-scale Euclidean optimization problems. In this paper, we
study dimension reduction for a variety of maximization problems, including
max-matching, max-spanning tree, max TSP, as well as various measures for
dataset diversity. For these problems, we show that the effect of dimension
reduction is intimately tied to the \emph{doubling dimension} $\lambda_X$ of
the underlying dataset $X$ -- a quantity measuring intrinsic dimensionality of
point sets. Specifically, we prove that a target dimension of $O(\lambda_X)$
suffices to approximately preserve the value of any near-optimal solution,which
we also show is necessary for some of these problems. This is in contrast to
classical dimension reduction results, whose dependence increases with the
dataset size $|X|$. We also provide empirical results validating the quality of
solutions found in the projected space, as well as speedups due to
dimensionality reduction.

</details>


### [621] [Learning DNF through Generalized Fourier Representations](https://arxiv.org/abs/2506.01075)
*Mohsen Heidari,Roni Khardon*

Key words: 傅里叶展开、贝叶斯网络、DNF学习、差界BN、L1谱范数

TL;DR: 广义傅里叶展开引入贝叶斯网络以适用于任意分布，证明学习工具可泛化，分析特定分布的L1谱范数上下界，展示DNF可学习性，并提出未知分布时的学习算法。

<details>
  <summary>Details</summary>

Main category: cs.DS

Motivation: 扩展傅里叶表示在布尔立方体上的应用，以适应更广泛分布，提升DNF在复杂分布下的可学习性。

Method: 1.引入基于贝叶斯网络的广义傅里叶展开；2.验证原有学习工具的适用性；3.分析特定BN的L1谱范数；4.证明DNF可学习性；5.开发未知分布学习算法。

Result: 广义展开适用任意分布；学习工具可泛化；特定BN下L1谱范数有界；DNF在差界BN下可学习；成功开发未知分布学习算法。

Conclusion: 广义傅里叶展开有效扩展应用范围，为DNF学习提供新工具，算法实用性高。

Abstract: The Fourier representation for the uniform distribution over the Boolean cube
has found numerous applications in algorithms and complexity analysis. Notably,
in learning theory, learnability of Disjunctive Normal Form (DNF) under uniform
as well as product distributions has been established through such
representations. This paper makes five main contributions. First, it introduces
a generalized Fourier expansion that can be used with any distribution $D$
through the representation of the distribution as a Bayesian network (BN).
Second, it shows that the main algorithmic tools for learning with the Fourier
representation, that use membership queries to approximate functions by
recovering their heavy Fourier coefficients, can be used with slight
modifications with the generalized expansion. These results hold for any
distribution. Third, it analyzes the $L_1$ spectral norm of conjunctions under
the new expansion, showing that it is bounded for a class of distributions
which can be represented by difference bounded tree BN, where a parent node in
the BN representation can change the conditional expectation of a child node by
at most $\alpha<0.5$. Lower bounds are presented to show that such constraints
are necessary. The fourth contribution uses these results to show the
learnability of DNF with membership queries under difference bounded tree BN.
The final contribution is to develop an algorithm for learning
difference-bounded tree BN distributions, thus extending the DNF learnability
result to cases where the distribution is not known in advance.

</details>


### [622] [Nearly-Linear Time Private Hypothesis Selection with the Optimal Approximation Factor](https://arxiv.org/abs/2506.01162)
*Maryam Aliakbarpour,Zhan Shi,Ria Stevens,Vincent X. Wang*

Key words: 假设选择, 差分隐私, 中心模型, 样本复杂度, 近似因子

TL;DR: 该论文研究了在差分隐私约束下的假设选择问题，提出了一种高效的近线性时间算法，实现了最优近似因子，且样本复杂度仅适度增加。

<details>
  <summary>Details</summary>

Main category: cs.DS

Motivation: 在统计学中，从样本中估计分布的密度是一个基本问题。假设选择问题关注在给定候选分布的情况下，确定哪一个最能描述数据分布。以往的研究表明，该问题可以在高效的时间和样本复杂度下解决，但尚未在差分隐私约束下实现最优解。

Method: 论文提出了一种在中心模型下的差分隐私算法，该算法在近线性时间内运行（相对于假设数量），实现了最优近似因子（α=3），并且样本复杂度仅增加为多项式对数级别。

Result: 该算法在时间和样本复杂度方面均优于现有方法（后者需要二次时间），同时解决了[Bun等人]提出的开放性问题。

Conclusion: 研究表明，在差分隐私约束下，假设选择问题可以在近线性时间内高效解决，且样本复杂度仅适度增加，为实际应用提供了可行的解决方案。

Abstract: Estimating the density of a distribution from its samples is a fundamental
problem in statistics. Hypothesis selection addresses the setting where, in
addition to a sample set, we are given $n$ candidate distributions -- referred
to as hypotheses -- and the goal is to determine which one best describes the
underlying data distribution. This problem is known to be solvable very
efficiently, requiring roughly $O(\log n)$ samples and running in
$\tilde{O}(n)$ time. The quality of the output is measured via the total
variation distance to the unknown distribution, and the approximation factor of
the algorithm determines how large this distance is compared to the optimal
distance achieved by the best candidate hypothesis. It is known that $\alpha =
3$ is the optimal approximation factor for this problem. We study hypothesis
selection under the constraint of differential privacy. We propose a
differentially private algorithm in the central model that runs in
nearly-linear time with respect to the number of hypotheses, achieves the
optimal approximation factor, and incurs only a modest increase in sample
complexity, which remains polylogarithmic in $n$. This resolves an open
question posed by [Bun, Kamath, Steinke, Wu, NeurIPS 2019]. Prior to our work,
existing upper bounds required quadratic time.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [623] [From Initial Data to Boundary Layers: Neural Networks for Nonlinear Hyperbolic Conservation Laws](https://arxiv.org/abs/2506.01453)
*Igor Ciril,Khalil Haddaoui,Yohann Tendero*

Key words: 神经网络, 非线性双曲守恒定律, 熵解, 学习算法

TL;DR: 用神经网络近似非线性严格双曲守恒定律的初边值问题的熵解，并提出高效可靠的学习算法框架。

<details>
  <summary>Details</summary>

Main category: math.AP

Motivation: 解决非线性严格双曲守恒定律的初边值问题中熵解的近似计算问题。

Method: 设计结合快速收敛和准确预测的神经网络学习算法框架，并通过一维标量测试案例验证。

Result: 提出的方法在一维测试案例中表现良好，展示其在更复杂工业场景中的潜在应用。

Conclusion: 神经网络方法为非线性守恒定律的熵解近似提供了高效可靠的解决方案。

Abstract: We address the approximation of entropy solutions to initial-boundary value
problems for nonlinear strictly hyperbolic conservation laws using neural
networks. A general and systematic framework is introduced for the design of
efficient and reliable learning algorithms, combining fast convergence during
training with accurate predictions. The methodology is assessed through a
series of one-dimensional scalar test cases, highlighting its potential
applicability to more complex industrial scenarios.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [624] [React to Surprises: Stable-by-Design Neural Feedback Control and the Youla-REN](https://arxiv.org/abs/2506.01226)
*Nicholas H. Barbara,Ruigang Wang,Alexandre Megretski,Ian R. Manchester*

Key words: 学习控制, 非线性策略, 闭环稳定性, Youla-Kučera参数化, 鲁棒神经网络

TL;DR: 本文研究了基于学习的控制中稳定非线性策略的参数化方法，提出了一种结合非线性Youla-Kučera参数化和鲁棒神经网络的非约束参数化结构，确保闭环稳定性。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 探索在学习控制中如何实现稳定闭环系统的非线性策略参数化，以克服非线性动态、部分观测和增量稳定性等挑战。

Method: 结合非线性Youla-Kučera参数化和鲁棒神经网络（如REN），提出非约束参数化方法，适用于一阶优化搜索。

Result: 在三种困难中的两种情况下，参数化方法能保持收缩性和Lipschitz性；三种困难同时出现时，增量稳定性可能丢失，但仍满足d-管收缩和Lipschitz性。

Conclusion: 该方法为具有稳定证书的控制器学习提供了有效工具，适用于经济奖励、短训练周期和不确定系统。

Abstract: We study parameterizations of stabilizing nonlinear policies for
learning-based control. We propose a structure based on a nonlinear version of
the Youla-Ku\v{c}era parameterization combined with robust neural networks such
as the recurrent equilibrium network (REN). The resulting parameterizations are
unconstrained, and hence can be searched over with first-order optimization
methods, while always ensuring closed-loop stability by construction. We study
the combination of (a) nonlinear dynamics, (b) partial observation, and (c)
incremental closed-loop stability requirements (contraction and Lipschitzness).
We find that with any two of these three difficulties, a contracting and
Lipschitz Youla parameter always leads to contracting and Lipschitz closed
loops. However, if all three hold, then incremental stability can be lost with
exogenous disturbances. Instead, a weaker condition is maintained, which we
call d-tube contraction and Lipschitzness. We further obtain converse results
showing that the proposed parameterization covers all contracting and Lipschitz
closed loops for certain classes of nonlinear systems. Numerical experiments
illustrate the utility of our parameterization when learning controllers with
built-in stability certificates for: i) ``economic'' rewards without
stabilizing effects; ii) short training horizons; and iii) uncertain systems.

</details>


### [625] [Interpretable reinforcement learning for heat pump control through asymmetric differentiable decision trees](https://arxiv.org/abs/2506.01641)
*Toon Van Puyvelde,Mehran Zareh,Chris Develder*

Key words: 深度强化学习,可解释性,差分决策树,能源管理,非对称树

TL;DR: 论文提出一种新型非对称软差分决策树（DDT）方法，以提高深度强化学习（DRL）在家庭能源管理系统中的透明度和性能。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 由于DRL的黑盒特性限制了其在能源管理公司的应用，研究旨在通过可解释的强化学习（XRL）技术提升决策透明度。

Method: 提出一种非对称软DDT构建方法，动态扩展节点以提高决策节点的使用效率，避免传统对称树需预定义深度的缺点。

Result: 非对称DDT在家庭能源管理系统中展现出透明、高效和高性能的决策潜力。

Conclusion: 非对称软DDT方法通过优化节点使用，显著提升了模型的解释性和性能。

Abstract: In recent years, deep reinforcement learning (DRL) algorithms have gained
traction in home energy management systems. However, their adoption by energy
management companies remains limited due to the black-box nature of DRL, which
fails to provide transparent decision-making feedback. To address this,
explainable reinforcement learning (XRL) techniques have emerged, aiming to
make DRL decisions more transparent. Among these, soft differential decision
tree (DDT) distillation provides a promising approach due to the clear decision
rules they are based on, which can be efficiently computed. However, achieving
high performance often requires deep, and completely full, trees, which reduces
interpretability. To overcome this, we propose a novel asymmetric soft DDT
construction method. Unlike traditional soft DDTs, our approach adaptively
constructs trees by expanding nodes only when necessary. This improves the
efficient use of decision nodes, which require a predetermined depth to
construct full symmetric trees, enhancing both interpretability and
performance. We demonstrate the potential of asymmetric DDTs to provide
transparent, efficient, and high-performing decision-making in home energy
management systems.

</details>


### [626] [Data-assimilated model-informed reinforcement learning](https://arxiv.org/abs/2506.01755)
*Defne E. Ozan,Andrea Nóvoa,Georgios Rigas,Luca Magri*

Key words: 混沌控制, 强化学习, 数据同化, 部分可观测性, Kuramoto-Sivashinsky方程

TL;DR: 提出了一种结合数据同化和强化学习的框架DA-MIRL，用于控制和抑制高维混沌系统的动态，即使基于部分和噪声观测也有效。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 由于高维混沌系统的高复杂性和不可预测性，传统的控制方法难以应对部分和噪声观测的挑战，因此需要一种新的框架来优化控制策略。

Method: DA-MIRL结合了低阶模型、序列数据同化和离策略actor-critic强化学习算法，通过物理模型和数据驱动模型（如控制感知的Echo状态网络）实现状态估计和控制。

Result: 在Kuramoto-Sivashinsky方程的混沌解上测试，DA-MIRL成功实现了从部分观测和近似模型中实时估计并抑制混沌动态。

Conclusion: DA-MIRL为部分可观测混沌系统的控制提供了新的可能性。

Abstract: The control of spatio-temporally chaos is challenging because of high
dimensionality and unpredictability. Model-free reinforcement learning (RL)
discovers optimal control policies by interacting with the system, typically
requiring observations of the full physical state.In practice, sensors often
provide only partial and noisy measurements (observations) of the system. The
objective of this paper is to develop a framework that enables the control of
chaotic systems with partial and noisy observability. The proposed method,
data-assimilated model-informed reinforcement learning (DA-MIRL), integrates
(i) low-order models to approximate high-dimensional dynamics; (ii) sequential
data assimilation to correct the model prediction when observations become
available; and (iii) an off-policy actor-critic RL algorithm to adaptively
learn an optimal control strategy based on the corrected state estimates. We
test DA-MIRL on the spatiotemporally chaotic solutions of the
Kuramoto-Sivashinsky equation. We estimate the full state of the environment
with (i) a physics-based model, here, a coarse-grained model; and (ii) a
data-driven model, here, the control-aware echo state network, which is
proposed in this paper. We show that DA-MIRL successfully estimates and
suppresses the chaotic dynamics of the environment in real time from partial
observations and approximate models. This work opens opportunities for the
control of partially observable chaotic systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [627] [$\texttt{AVROBUSTBENCH}$: Benchmarking the Robustness of Audio-Visual Recognition Models at Test-Time](https://arxiv.org/abs/2506.00358)
*Sarthak Kumar Maharana,Saksham Singh Kushwaha,Baoming Zhang,Adrian Rodriguez,Songtao Wei,Yapeng Tian,Yunhui Guo*

Key words: 音频-视觉模型、鲁棒性、测试时间适应、跨模态融合、基准测试

TL;DR: 该论文提出了一个名为AVROBUSTBENCH的综合基准，用于评估音频-视觉识别模型在测试时的鲁棒性，并引入了一种简单的在线测试时间适应方法AV2C。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 现实场景中音频和视觉模态可能同时发生分布偏移，但现有基准主要关注单模态，因此需要新的评估标准。

Method: 设计了四个包含双模态音频-视觉扰动的基准数据集，并提出了AV2C方法，通过惩罚高熵样本实现动态跨模态融合。

Result: 实验表明现有模型在扰动严重性增加时鲁棒性下降，而AV2C在部分数据集上表现有所提升。

Conclusion: AVROBUSTBENCH有望推动更有效的音频-视觉测试时间适应方法的发展。

Abstract: While recent audio-visual models have demonstrated impressive performance,
their robustness to distributional shifts at test-time remains not fully
understood. Existing robustness benchmarks mainly focus on single modalities,
making them insufficient for thoroughly assessing the robustness of
audio-visual models. Motivated by real-world scenarios where shifts can occur
$\textit{simultaneously}$ in both audio and visual modalities, we introduce
$\texttt{AVROBUSTBENCH}$, a comprehensive benchmark designed to evaluate the
test-time robustness of audio-visual recognition models.
$\texttt{AVROBUSTBENCH}$ comprises four audio-visual benchmark datasets,
$\texttt{AUDIOSET-2C}$, $\texttt{VGGSOUND-2C}$, $\texttt{KINETICS-2C}$, and
$\texttt{EPICKITCHENS-2C}$, each incorporating 75 bimodal audio-visual
corruptions that are $\textit{co-occurring}$ and $\textit{correlated}$. Through
extensive evaluations, we observe that state-of-the-art supervised and
self-supervised audio-visual models exhibit declining robustness as corruption
severity increases. Furthermore, online test-time adaptation (TTA) methods, on
$\texttt{VGGSOUND-2C}$ and $\texttt{KINETICS-2C}$, offer minimal improvements
in performance under bimodal corruptions. We further propose $\texttt{AV2C}$, a
simple TTA approach enabling on-the-fly cross-modal fusion by penalizing
high-entropy samples, which achieves improvements on $\texttt{VGGSOUND-2C}$. We
hope that $\texttt{AVROBUSTBENCH}$ will steer the development of more effective
and robust audio-visual TTA approaches. Our code is available
$\href{https://github.com/sarthaxxxxx/AV-C-Robustness-Benchmark}{here}$.

</details>


### [628] [MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity Reconstruction and Generation](https://arxiv.org/abs/2506.00385)
*Yakun Song,Jiawei Chen,Xiaobin Zhuang,Chenpeng Du,Ziyang Ma,Jian Wu,Jian Cong,Dongya Jia,Zhuo Chen,Yuping Wang,Yuxuan Wang,Xie Chen*

Key words: 音频编解码器,Transformer,噪声注入,语义表达,Zipf分布

TL;DR: MagiCodec是一种基于Transformer的单层流式音频编解码器，通过多阶段训练流程提升语义表达和重建质量，优于现有技术。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 现有神经音频编解码器虽重建质量高，但编码的标记对下游生成模型的适配性不足，需改进。

Method: 采用多阶段训练流程，结合高斯噪声注入和潜在正则化，增强语义表达并保留高保真重建。

Result: MagiCodec在重建质量和下游任务中优于现有编解码器，标记分布呈现类自然语言的Zipf特性。

Conclusion: MagiCodec通过噪声注入和正则化改进了标记的语义表达，适配生成模型，代码和模型已开源。

Abstract: Neural audio codecs have made significant strides in efficiently mapping raw
audio waveforms into discrete token representations, which are foundational for
contemporary audio generative models. However, most existing codecs are
optimized primarily for reconstruction quality, often at the expense of the
downstream modelability of the encoded tokens. Motivated by the need to
overcome this bottleneck, we introduce $\textbf{MagiCodec}$, a novel
single-layer, streaming Transformer-based audio codec. MagiCodec is designed
with a multistage training pipeline that incorporates Gaussian noise injection
and latent regularization, explicitly targeting the enhancement of semantic
expressiveness in the generated codes while preserving high reconstruction
fidelity. We analytically derive the effect of noise injection in the frequency
domain, demonstrating its efficacy in attenuating high-frequency components and
fostering robust tokenization. Extensive experimental evaluations show that
MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and
downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like
distributions, as observed in natural languages, thereby improving
compatibility with language-model-based generative architectures. The code and
pre-trained models are available at https://github.com/Ereboas/MagiCodec.

</details>


### [629] [XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark](https://arxiv.org/abs/2506.00462)
*Ioan-Paul Ciobanu,Andrei-Iulian Hiji,Nicolae-Catalin Ristea,Paul Irofti,Cristian Rusu,Radu Tudor Ionescu*

Key words: 音频深度伪造、跨域评估、基准测试集、泛化能力

TL;DR: 论文提出了XMAD-Bench，一个大规模跨领域多语言音频深度伪造基准测试集，用于检测音频深度伪造检测器的泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 音频深度伪造技术的快速发展使公众面临更多金融诈骗、身份盗窃和虚假信息的风险。当前检测器在域内测试中表现优异，但跨域性能较差，亟需研究其泛化能力。

Method: 论文引入了XMAD-Bench基准测试集，包含668.8小时的真实和伪造语音数据，训练和测试集的说话人、生成方法和真实音频来源均不相同，以构建跨域评估环境。

Result: 实验显示域内性能可达100%，但跨域性能有时接近随机猜测，表明当前检测器在跨域场景下表现不佳。

Conclusion: XMAD-Bench凸显了开发跨语言、说话人、生成方法和数据源都能保持鲁棒性的音频深度伪造检测器的必要性。

Abstract: Recent advances in audio generation led to an increasing number of deepfakes,
making the general public more vulnerable to financial scams, identity theft,
and misinformation. Audio deepfake detectors promise to alleviate this issue,
with many recent studies reporting accuracy rates close to 99%. However, these
methods are typically tested in an in-domain setup, where the deepfake samples
from the training and test sets are produced by the same generative models. To
this end, we introduce XMAD-Bench, a large-scale cross-domain multilingual
audio deepfake benchmark comprising 668.8 hours of real and deepfake speech. In
our novel dataset, the speakers, the generative methods, and the real audio
sources are distinct across training and test splits. This leads to a
challenging cross-domain evaluation setup, where audio deepfake detectors can
be tested ``in the wild''. Our in-domain and cross-domain experiments indicate
a clear disparity between the in-domain performance of deepfake detectors,
which is usually as high as 100%, and the cross-domain performance of the same
models, which is sometimes similar to random chance. Our benchmark highlights
the need for the development of robust audio deepfake detectors, which maintain
their generalization capacity across different languages, speakers, generative
methods, and data sources. Our benchmark is publicly released at
https://github.com/ristea/xmad-bench/.

</details>


### [630] [Probing Audio-Generation Capabilities of Text-Based Language Models](https://arxiv.org/abs/2506.00003)
*Arjun Prasaath Anbazhagan,Parteek Kumar,Ujjwal Kaur,Aslihan Akalin,Kevin Zhu,Sean O'Brien*

Key words: LLMs,音频生成,文本转音频,FAD,CLAP

TL;DR: 研究探讨了LLMs如何通过文本生成音频，并测试了其在不同音频复杂度下的表现。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 探索LLMs在文本训练基础上生成音频的潜力。

Method: 采用三步渐进方法（音符、环境声音、人声），以代码为中介生成音频。

Result: LLMs能生成基础音频，但随复杂度增加性能下降。

Conclusion: LLMs具备音频理解能力，但生成能力有限，需进一步优化。

Abstract: How does textual representation of audio relate to the Large Language Model's
(LLMs) learning about the audio world? This research investigates the extent to
which LLMs can be prompted to generate audio, despite their primary training in
textual data. We employ a three-tier approach, progressively increasing the
complexity of audio generation: 1) Musical Notes, 2) Environmental Sounds, and
3) Human Speech. To bridge the gap between text and audio, we leverage code as
an intermediary, prompting LLMs to generate code that, when executed, produces
the desired audio output. To evaluate the quality and accuracy of the generated
audio, we employ FAD and CLAP scores. Our findings reveal that while LLMs can
generate basic audio features, their performance deteriorates as the complexity
of the audio increases. This suggests that while LLMs possess a latent
understanding of the auditory world, their ability to translate this
understanding into tangible audio output remains rudimentary. Further research
into techniques that can enhance the quality and diversity of LLM-generated
audio can lead to an improvement in the performance of text-based LLMs in
generating audio.

</details>


### [631] [Counterfactual Activation Editing for Post-hoc Prosody and Mispronunciation Correction in TTS Models](https://arxiv.org/abs/2506.00832)
*Kyowoon Lee,Artyom Stitsyuk,Gunu Jho,Inchul Hwang,Jaesik Choi*

Key words: 文本到语音、韵律控制、发音修正、后处理调整

TL;DR: 本文提出了一种名为 Counterfactual Activation Editing 的模型无关方法，用于在预训练的TTS模型中操控内部表示，实现后处理的韵律控制和发音修正。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 现有韵律控制和发音修正方法依赖专门模块或额外训练，限制了后处理调整的能力，且在低资源环境下实用性不足。

Method: 通过操控预训练TTS模型的内部表示（Counterfactual Activation Editing）实现韵律和发音的调整。

Result: 实验证明，该方法能有效调整韵律特征和修正发音，同时保持合成质量。

Conclusion: 该方法在不重新训练的情况下实现了TTS输出的推断时优化，填补了预训练模型与可编辑语音合成之间的空白。

Abstract: Recent advances in Text-to-Speech (TTS) have significantly improved speech
naturalness, increasing the demand for precise prosody control and
mispronunciation correction. Existing approaches for prosody manipulation often
depend on specialized modules or additional training, limiting their capacity
for post-hoc adjustments. Similarly, traditional mispronunciation correction
relies on grapheme-to-phoneme dictionaries, making it less practical in
low-resource settings. We introduce Counterfactual Activation Editing, a
model-agnostic method that manipulates internal representations in a
pre-trained TTS model to achieve post-hoc control of prosody and pronunciation.
Experimental results show that our method effectively adjusts prosodic features
and corrects mispronunciations while preserving synthesis quality. This opens
the door to inference-time refinement of TTS outputs without retraining,
bridging the gap between pre-trained TTS models and editable speech synthesis.

</details>


### [632] [CoVoMix2: Advancing Zero-Shot Dialogue Generation with Fully Non-Autoregressive Flow Matching](https://arxiv.org/abs/2506.00885)
*Leying Zhang,Yao Qian,Xiaofei Wang,Manthan Thakker,Dongmei Wang,Jianwei Yu,Haibin Wu,Yuxuan Hu,Jinyu Li,Yanmin Qian,Sheng Zhao*

Key words: 多说话人对话生成,非自回归模型,流匹配,零样本学习,语音合成

TL;DR: CoVoMix2是一个非自回归的零样本多说话人对话生成框架，通过流匹配生成模型直接从多流转录预测梅尔频谱，解决了现有系统在说话人一致性、重叠语音建模和高效对话合成方面的挑战。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 生成自然、多说话人的对话在播客、虚拟代理和多媒体内容生成等应用中至关重要，但现有系统在说话人一致性、重叠语音建模和高效对话合成方面表现不足。

Method: CoVoMix2采用流匹配生成模型，直接从多流转录预测梅尔频谱，无需中间标记表示，并通过转录级说话人解耦、句子级对齐和提示级随机掩码策略捕捉真实对话动态。

Result: CoVoMix2在语音质量、说话人一致性和推理速度上优于MoonCast和Sesame等基准模型，支持可控对话生成（如重叠语音和精确时序控制），并适用于实际场景。

Conclusion: CoVoMix2展现了零样本多说话人对话生成的先进性能，具有广泛的实际应用潜力。

Abstract: Generating natural-sounding, multi-speaker dialogue is crucial for
applications such as podcast creation, virtual agents, and multimedia content
generation. However, existing systems struggle to maintain speaker consistency,
model overlapping speech, and synthesize coherent conversations efficiently. In
this paper, we introduce CoVoMix2, a fully non-autoregressive framework for
zero-shot multi-talker dialogue generation. CoVoMix2 directly predicts
mel-spectrograms from multi-stream transcriptions using a flow-matching-based
generative model, eliminating the reliance on intermediate token
representations. To better capture realistic conversational dynamics, we
propose transcription-level speaker disentanglement, sentence-level alignment,
and prompt-level random masking strategies. Our approach achieves
state-of-the-art performance, outperforming strong baselines like MoonCast and
Sesame in speech quality, speaker consistency, and inference speed. Notably,
CoVoMix2 operates without requiring transcriptions for the prompt and supports
controllable dialogue generation, including overlapping speech and precise
timing control, demonstrating strong generalizability to real-world speech
generation scenarios.

</details>


### [633] [In-the-wild Audio Spatialization with Flexible Text-guided Localization](https://arxiv.org/abs/2506.00927)
*Tianrui Pan,Jie Liu,Zewen Huang,Jie Tang,Gangshan Wu*

Key words: 双耳音频、音频空间化、文本提示、AR/VR、交互控制

TL;DR: 提出了一种基于文本提示的音频空间化（TAS）框架，通过构建大规模数据集和模型优化，提升双耳音频的质量和语义一致性。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 现有音频空间化方法在复杂多对象交互环境中缺乏灵活控制，需要更优方案。

Method: 采用文本提示指导双耳音频生成，构建SpatialTAS数据集并训练模型。

Result: 模型在模拟和真实数据集上表现优于现有方法，具备更强的泛化能力和准确性。

Conclusion: 文本提示为双耳音频生成提供了灵活控制，实现了高质量和语义一致的输出。

Abstract: To enhance immersive experiences, binaural audio offers spatial awareness of
sounding objects in AR, VR, and embodied AI applications. While existing audio
spatialization methods can generally map any available monaural audio to
binaural audio signals, they often lack the flexible and interactive control
needed in complex multi-object user-interactive environments. To address this,
we propose a Text-guided Audio Spatialization (TAS) framework that utilizes
flexible text prompts and evaluates our model from unified generation and
comprehension perspectives. Due to the limited availability of premium and
large-scale stereo data, we construct the SpatialTAS dataset, which encompasses
376,000 simulated binaural audio samples to facilitate the training of our
model. Our model learns binaural differences guided by 3D spatial location and
relative position prompts, augmented by flipped-channel audio. It outperforms
existing methods on both simulated and real-recorded datasets, demonstrating
superior generalization and accuracy. Besides, we develop an assessment model
based on Llama-3.1-8B, which evaluates the spatial semantic coherence between
our generated binaural audio and text prompts through a spatial reasoning task.
Results demonstrate that text prompts provide flexible and interactive control
to generate binaural audio with excellent quality and semantic consistency in
spatial locations. Dataset is available at
\href{https://github.com/Alice01010101/TASU}

</details>


### [634] [General-purpose audio representation learning for real-world sound scenes](https://arxiv.org/abs/2506.00934)
*Goksenin Yuksel,Marcel van Gerven,Kiki van der Heijden*

Key words: 音频基础模型, 自监督学习, 空间音频, GRAM, 声音定位

TL;DR: 提出了一种自监督训练方法GRAM，用于提升音频基础模型在真实世界场景中的表现，尤其是在空间音频表征学习上。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 现有音频基础模型在真实世界多源、有噪声和空间化的音频场景中表现不佳，缺乏空间感知能力。

Method: 采用自监督训练方法GRAM，适用于任何基于掩码的深度学习模型，并在Transformer和Mamba架构上验证。

Result: GRAM显著缩小了干燥、单源音频场景与自然音频场景的性能差距，并在声音定位任务中超越现有模型。

Conclusion: GRAM为真实世界应用提供了更鲁棒的音频基础模型，展现了在自然音频场景和空间音频学习中的领先性能。

Abstract: While audio foundation models perform well on myriad of tasks from sound
classification to speech analysis, these models are trained and tested on dry,
non-spatial, single-source audio clips. This limits their success in real-world
situations and results in spatially unaware audio embeddings. To address these
limitations, we propose a novel self-supervised training approach for
General-Purpose, Real-world Audio Models (GRAMs). The GRAM training approach
enables robust spatial audio representation learning for naturalistic, noisy
sound scenes and can be applied to any masking-based deep learning model. We
demonstrate the success of our approach by training two state-of-the-art
models, one with a transformer and one with a mamba backbone. We assess the
quality of the extracted audio representations from GRAMs using the original
version of the HEAR benchmark, a newly synthesized, naturalistic version of the
HEAR benchmark, and novel sound localization tasks based on HEAR benchmark
datasets. The results show that our approach minimizes the performance gap
between dry, non-spatial, single-source sound scenes and naturalistic sound
scenes for crucial tasks such as auditory scene analysis, outperforming
existing state-of-the-art audio foundation models at a fraction of the training
steps. Moreover, GRAMs show state-of-the-art performance on sound localization
tasks, exceeding even supervised sound localization models. In sum, the
proposed approach represents a significant advancement towards robust audio
foundation models for real-world applications with state-of-the-art performance
on naturalistic sound scenes as well as spatial audio representation learning.

</details>


### [635] [Attention Is Not Always the Answer: Optimizing Voice Activity Detection with Simple Feature Fusion](https://arxiv.org/abs/2506.01365)
*Kumud Tripathi,Chowdam Venkata Kumar,Pankaj Wasnik*

Key words: 语音活动检测, MFCC, 预训练模型, 特征融合, FusionVAD

TL;DR: 研究提出了FusionVAD框架，结合MFCC和预训练模型特征，通过三种融合策略提升语音活动检测（VAD）性能，发现简单融合方法效果优于复杂方法，模型表现超越现有最佳结果。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 提升语音活动检测的准确性和效率，探索MFCC与预训练模型特征的互补性。

Method: 提出FusionVAD框架，采用拼接、加法和交叉注意力三种融合策略结合MFCC和预训练模型特征。

Result: 加法融合方法效果最优，融合模型在多个数据集上超越现有最佳模型Pyannote，平均提升2.04%。

Conclusion: 简单特征融合能显著增强VAD的鲁棒性，同时保持计算效率。

Abstract: Voice Activity Detection (VAD) plays a key role in speech processing, often
utilizing hand-crafted or neural features. This study examines the
effectiveness of Mel-Frequency Cepstral Coefficients (MFCCs) and pre-trained
model (PTM) features, including wav2vec 2.0, HuBERT, WavLM, UniSpeech, MMS, and
Whisper. We propose FusionVAD, a unified framework that combines both feature
types using three fusion strategies: concatenation, addition, and
cross-attention (CA). Experimental results reveal that simple fusion
techniques, particularly addition, outperform CA in both accuracy and
efficiency. Fusion-based models consistently surpass single-feature models,
highlighting the complementary nature of MFCCs and PTM features. Notably, our
best-performing fusion model exceeds the state-of-the-art Pyannote across
multiple datasets, achieving an absolute average improvement of 2.04%. These
results confirm that simple feature fusion enhances VAD robustness while
maintaining computational efficiency.

</details>


### [636] [A Two-Stage Hierarchical Deep Filtering Framework for Real-Time Speech Enhancement](https://arxiv.org/abs/2506.01023)
*Shenghui Lu,Hukai Huang,Jinanglong Yao,Kaidi Wang,Qingyang Hong,Lin Li*

Key words: 语音增强,时频分析,深度滤波,子带处理,TAConv

TL;DR: 该论文提出了一种结合子带处理和深度滤波的模型（HDF-Net），通过充分利用目标时频（TF）点及其周围TF点的信息，实现了单通道语音增强。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 现有的单通道语音增强方法可能未能充分利用目标时频点周围的信息，因此需要一种更高效的方法来提取和利用这些信息。

Method: 提出了一种分层深度滤波网络（HDF-Net），包括子带模块和深度滤波模块。子带模块在输入阶段捕获周围频点信息，深度滤波模块在输出阶段对目标TF点及其周围TF点进行滤波。此外，将深度滤波解耦为时间和频率分量，并引入两阶段框架以降低复杂度，同时提出TAConv模块增强卷积特征提取。

Result: 实验结果表明，HDF-Net能够有效利用周围TF点信息，性能优于其他先进系统，同时资源消耗更少。

Conclusion: HDF-Net通过结合子带处理和深度滤波，显著提高了单通道语音增强的性能和效率。

Abstract: This paper proposes a model that integrates sub-band processing and deep
filtering to fully exploit information from the target time-frequency (TF) bin
and its surrounding TF bins for single-channel speech enhancement. The sub-band
module captures surrounding frequency bin information at the input, while the
deep filtering module applies filtering at the output to both the target TF bin
and its surrounding TF bins. To further improve the model performance, we
decouple deep filtering into temporal and frequency components and introduce a
two-stage framework, reducing the complexity of filter coefficient prediction
at each stage. Additionally, we propose the TAConv module to strengthen
convolutional feature extraction. Experimental results demonstrate that the
proposed hierarchical deep filtering network (HDF-Net) effectively utilizes
surrounding TF bin information and outperforms other advanced systems while
using fewer resources.

</details>


### [637] [The iNaturalist Sounds Dataset](https://arxiv.org/abs/2506.00343)
*Mustafa Chasmai,Alexander Shepard,Subhransu Maji,Grant Van Horn*

Key words: iNatSounds, 物种声音分类, 生物多样性, 公民科学, 音频数据集

TL;DR: iNatSounds是一个包含23万音频文件、5500多种物种声音的数据集，用于物种声音分类研究。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 提供大规模的物种声音数据集，支持生物多样性研究和公众参与科学项目。

Method: 收集来自iNaturalist平台的音频文件，标注物种信息，并测试多分类和多标签分类模型。

Result: 数据集虽标注较弱，但可用作预训练资源，对下游任务有帮助。

Conclusion: iNatSounds有助于开发新一代公共参与工具和支持生物多样性研究。

Abstract: We present the iNaturalist Sounds Dataset (iNatSounds), a collection of
230,000 audio files capturing sounds from over 5,500 species, contributed by
more than 27,000 recordists worldwide. The dataset encompasses sounds from
birds, mammals, insects, reptiles, and amphibians, with audio and species
labels derived from observations submitted to iNaturalist, a global citizen
science platform. Each recording in the dataset varies in length and includes a
single species annotation. We benchmark multiple backbone architectures,
comparing multiclass classification objectives with multilabel objectives.
Despite weak labeling, we demonstrate that iNatSounds serves as a useful
pretraining resource by benchmarking it on strongly labeled downstream
evaluation datasets. The dataset is available as a single, freely accessible
archive, promoting accessibility and research in this important domain. We
envision models trained on this data powering next-generation public engagement
applications, and assisting biologists, ecologists, and land use managers in
processing large audio collections, thereby contributing to the understanding
of species compositions in diverse soundscapes.

</details>


### [638] [FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal Contextual Fusion](https://arxiv.org/abs/2506.01111)
*Shunian Chen,Xinyuan Xie,Zheshu Chen,Liyan Zhao,Owen Lee,Zhan Su,Qilin Sun,Benyou Wang*

Key words: 音频描述、多模态信息、FusionAudio数据集、LLM、CLAP编码器

TL;DR: 提出一种两阶段自动化流程，结合多模态信息生成细粒度的音频描述，并发布大规模数据集FusionAudio。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 当前音频描述方法缺乏细节和上下文准确性，受限于单模态或浅层多模态信息。

Method: 使用预训练模型提取多模态上下文线索，结合LLM生成详细音频描述。

Result: 开发了FusionAudio数据集（120万描述和600万QA对），并改进了音频-文本对齐模型。

Conclusion: 该方法推动了复杂音频环境的自动化理解。

Abstract: High-quality, large-scale audio captioning is crucial for advancing audio
understanding, yet current automated methods often generate captions that lack
fine-grained detail and contextual accuracy, primarily due to their reliance on
limited unimodal or superficial multimodal information. Drawing inspiration
from human auditory perception, which adeptly integrates cross-modal cues and
performs sophisticated auditory scene analysis, we introduce a novel two-stage
automated pipeline. This pipeline first employs specialized pretrained models
to extract diverse contextual cues (e.g., speech, music, general sounds, and
visual information from associated video). A large language model (LLM) then
synthesizes these rich, multimodal inputs to generate detailed and
context-aware audio captions. Key contributions of this work include: (1) the
proposed scalable method for fine-grained audio caption generation; (2)
FusionAudio, a new large-scale dataset comprising 1.2 million such detailed
captions, combined with 6 million QA pairs; and (3) enhanced audio models
developed using FusionAudio, specifically a CLAP-based audio encoder with
superior audio-text alignment and instruction following. This paper paves the
way for more nuanced and accurate automated understanding of complex audio
environments. Code and data can be found in
https://github.com/satsuki2486441738/FusionAudio.

</details>


### [639] [Learning to Upsample and Upmix Audio in the Latent Domain](https://arxiv.org/abs/2506.00681)
*Dimitrios Bralios,Paris Smaragdis,Jonah Casebeer*

Key words: 音频处理, 自编码器, 潜在空间, 计算效率

TL;DR: 该论文提出了一种在神经网络音频自编码器的潜在空间中直接进行音频处理的方法，避免了传统方法需要解码为原始音频格式的低效性，显著提升了计算效率并保持了音频质量。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 传统的音频处理方法通常直接在原始波形或频谱表示上操作，效率低下，而现代音频自编码器已能生成紧凑的潜在表示。作者希望通过直接在潜在空间中进行操作，简化训练过程并提高效率。

Method: 提出了一个框架，完全在自编码器的潜在空间中执行音频处理操作，无需解码为原始音频。方法采用潜在L1重建损失和单一潜在对抗判别器，显著简化了训练过程。

Result: 在带宽扩展和单声道到立体声上混实验中，计算效率提升了高达100倍，同时保持了与原始音频后处理相当的质量。

Conclusion: 该工作为使用自编码器的音频处理流程建立了一种更高效的范式，显著提升了各种音频任务的工作效率。

Abstract: Neural audio autoencoders create compact latent representations that preserve
perceptually important information, serving as the foundation for both modern
audio compression systems and generation approaches like next-token prediction
and latent diffusion. Despite their prevalence, most audio processing
operations, such as spatial and spectral up-sampling, still inefficiently
operate on raw waveforms or spectral representations rather than directly on
these compressed representations. We propose a framework that performs audio
processing operations entirely within an autoencoder's latent space,
eliminating the need to decode to raw audio formats. Our approach dramatically
simplifies training by operating solely in the latent domain, with a latent L1
reconstruction term, augmented by a single latent adversarial discriminator.
This contrasts sharply with raw-audio methods that typically require complex
combinations of multi-scale losses and discriminators. Through experiments in
bandwidth extension and mono-to-stereo up-mixing, we demonstrate computational
efficiency gains of up to 100x while maintaining quality comparable to
post-processing on raw audio. This work establishes a more efficient paradigm
for audio processing pipelines that already incorporate autoencoders, enabling
significantly faster and more resource-efficient workflows across various audio
tasks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [640] [CineMA: A Foundation Model for Cine Cardiac MRI](https://arxiv.org/abs/2506.00679)
*Yunguan Fu,Weixi Yi,Charlotte Manisty,Anish N Bhuva,Thomas A Treibel,James C Moon,Matthew J Clarkson,Rhodri Huw Davies,Yipeng Hu*

Key words: CMR, AI 模型, 自监督学习, 射血分数, 心脏成像

TL;DR: CineMA 是一种自监督基础 AI 模型，用于自动化心脏磁共振 (CMR) 图像分析，减少了对大量标注数据和计算资源的依赖，性能优于传统 CNN 模型。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 提取 CMR 中的重要临床指标（如射血分数）费时且主观，需要开发更高效的自动化工具。

Method: 使用自监督自动编码器模型 CineMA，基于 74,916 个 CMR 研究进行预训练和微调，完成多个任务。

Result: CineMA 在 8 个数据集上的 23 项任务中表现优异，标注效率更高，性能优于 CNN。

Conclusion: CineMA 减轻了临床标注负担，为未来心脏成像中的基础模型微调提供了可能。

Abstract: Cardiac magnetic resonance (CMR) is a key investigation in clinical
cardiovascular medicine and has been used extensively in population research.
However, extracting clinically important measurements such as ejection fraction
for diagnosing cardiovascular diseases remains time-consuming and subjective.
We developed CineMA, a foundation AI model automating these tasks with limited
labels. CineMA is a self-supervised autoencoder model trained on 74,916 cine
CMR studies to reconstruct images from masked inputs. After fine-tuning, it was
evaluated across eight datasets on 23 tasks from four categories: ventricle and
myocardium segmentation, left and right ventricle ejection fraction
calculation, disease detection and classification, and landmark localisation.
CineMA is the first foundation model for cine CMR to match or outperform
convolutional neural networks (CNNs). CineMA demonstrated greater label
efficiency than CNNs, achieving comparable or better performance with fewer
annotations. This reduces the burden of clinician labelling and supports
replacing task-specific training with fine-tuning foundation models in future
cardiac imaging applications. Models and code for pre-training and fine-tuning
are available at https://github.com/mathpluscode/CineMA, democratising access
to high-performance models that otherwise require substantial computational
resources, promoting reproducibility and accelerating clinical translation.

</details>


### [641] [Flexible Mixed Precision Quantization for Learned Image Compression](https://arxiv.org/abs/2506.01221)
*Md Adnan Faisal Hossain,Zhihao Duan,Fengqing Zhu*

Key words: 学习图像压缩, 量化, 混合精度, 率失真优化, 神经网络

TL;DR: 提出了一种灵活的混合精度量化方法（FMPQ），通过为神经网络的不同层分配不同的位宽，结合搜索算法优化量化模型性能。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 学习图像压缩（LIC）在性能上优于传统编解码器，但其计算成本和存储需求较高，需要有效的量化方法降低复杂度。

Method: 使用FMPQ方法，根据层间量化敏感性的差异分配不同位宽，并以率失真损失的变化为分配标准；引入自适应搜索算法减少位宽分布搜索的时间复杂度。

Result: 在类似模型大小约束下，FMPQ方法在BD-Rate性能上优于其他量化方法。

Conclusion: FMPQ方法为LIC模型的量化提供了高效且灵活的解决方案，显著提升了性能。

Abstract: Despite its improvements in coding performance compared to traditional
codecs, Learned Image Compression (LIC) suffers from large computational costs
for storage and deployment. Model quantization offers an effective solution to
reduce the computational complexity of LIC models. However, most existing works
perform fixed-precision quantization which suffers from sub-optimal utilization
of resources due to the varying sensitivity to quantization of different layers
of a neural network. In this paper, we propose a Flexible Mixed Precision
Quantization (FMPQ) method that assigns different bit-widths to different
layers of the quantized network using the fractional change in rate-distortion
loss as the bit-assignment criterion. We also introduce an adaptive search
algorithm which reduces the time-complexity of searching for the desired
distribution of quantization bit-widths given a fixed model size. Evaluation of
our method shows improved BD-Rate performance under similar model size
constraints compared to other works on quantization of LIC models. We have made
the source code available at gitlab.com/viper-purdue/fmpq.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [642] [Rethinking Hybrid Retrieval: When Small Embeddings and LLM Re-ranking Beat Bigger Models](https://arxiv.org/abs/2506.00049)
*Arjun Rao,Hanieh Alipour,Nick Pendar*

Key words: RAG,三模态混合检索,嵌入模型,MiniLM-v6,BGE-Large

TL;DR: 比较了MiniLM-v6和BGE-Large在RAG系统中的三模态混合检索中的表现，发现MiniLM-v6在与LLM重排结合时表现更好。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 探讨在RAG系统中，多种嵌入模型的融合方法及其性能差异，尤其是MiniLM-v6和BGE-Large的表现。

Method: 融合稠密语义、稀疏词法和基于图的嵌入，测试MiniLM-v6和BGE-Large在三模态混合框架中的表现。

Result: MiniLM-v6在结合LLM重排时优于BGE-Large，尤其在代理重排场景中表现显著。

Conclusion: RAG系统的嵌入模型选择应注重多信号融合和LLM对齐，而非仅依赖大模型，以提升效率和准确性。

Abstract: This paper presents a comparison of embedding models in tri-modal hybrid
retrieval for Retrieval-Augmented Generation (RAG) systems. We investigate the
fusion of dense semantic, sparse lexical, and graph-based embeddings, focusing
on the performance of the MiniLM-v6 and BGE-Large architectures. Contrary to
conventional assumptions, our results show that the compact MiniLM-v6
outperforms the larger BGE-Large when integrated with LLM-based re-ranking
within our tri-modal hybrid framework. Experiments conducted on the SciFact,
FIQA, and NFCorpus datasets demonstrate significant improvements in retrieval
quality with the MiniLM-v6 configuration. The performance difference is
particularly pronounced in agentic re-ranking scenarios, indicating better
alignment between MiniLM-v6's embedding space and LLM reasoning. Our findings
suggest that embedding model selection for RAG systems should prioritize
compatibility with multi-signal fusion and LLM alignment, rather than relying
solely on larger models. This approach may reduce computational requirements
while improving retrieval accuracy and efficiency.

</details>


### [643] [Gated Multimodal Graph Learning for Personalized Recommendation](https://arxiv.org/abs/2506.00107)
*Sibei Liu,Yuanzhe Zhang,Xiang Li,Yunbo Liu,Chengwei Feng,Hao Yang*

Key words: 多模态推荐, 自适应融合, 图神经网络, 冷启动

TL;DR: 该论文提出了一种名为RLMultimodalRec的轻量级多模态推荐框架，通过自适应模态融合和图建模缓解冷启动和数据稀疏问题，并在实验中优于多种基线方法。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 多模态推荐可以解决协同过滤中的冷启动和数据稀疏问题，但现有方法难以高效整合异构模态。

Method: 提出RLMultimodalRec框架，结合图用户建模和自适应多模态编码，使用门控融合动态平衡视觉与文本模态。

Result: 在亚马逊数据集上，RLMultimodalRec在top-K推荐指标上显著优于基线方法，同时保持可扩展性。

Conclusion: 该框架有效提升了多模态推荐的性能，适合实际部署。

Abstract: Multimodal recommendation has emerged as a promising solution to alleviate
the cold-start and sparsity problems in collaborative filtering by
incorporating rich content information, such as product images and textual
descriptions. However, effectively integrating heterogeneous modalities into a
unified recommendation framework remains a challenge. Existing approaches often
rely on fixed fusion strategies or complex architectures , which may fail to
adapt to modality quality variance or introduce unnecessary computational
overhead.
  In this work, we propose RLMultimodalRec, a lightweight and modular
recommendation framework that combines graph-based user modeling with adaptive
multimodal item encoding. The model employs a gated fusion module to
dynamically balance the contribution of visual and textual modalities, enabling
fine-grained and content-aware item representations. Meanwhile, a two-layer
LightGCN encoder captures high-order collaborative signals by propagating
embeddings over the user-item interaction graph without relying on nonlinear
transformations.
  We evaluate our model on a real-world dataset from the Amazon product domain.
Experimental results demonstrate that RLMultimodalRec consistently outperforms
several competitive baselines, including collaborative filtering, visual-aware,
and multimodal GNN-based methods. The proposed approach achieves significant
improvements in top-K recommendation metrics while maintaining scalability and
interpretability, making it suitable for practical deployment.

</details>


### [644] [GLEN: Generative Retrieval via Lexical Index Learning](https://arxiv.org/abs/2311.03057)
*Sunkyung Lee,Minjin Choi,Jongwuk Lee*

Key words: 生成式检索、动态词汇标识、两阶段索引学习、无碰撞推断、性能基准测试

TL;DR: GLEN通过动态词汇标识和两阶段索引学习策略解决了生成式检索中的知识不一致和训练-推理差距问题，实现了高性能检索。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 生成式检索直接生成文档标识符，但面临预训练语言模型知识不一致和训练-推理差距两大挑战，需提出新方法解决。

Method: GLEN采用动态词汇标识和两阶段索引学习策略，学习有意义的词汇标识符和查询-文档相关性信号；推理时使用无碰撞推断，通过标识符权重排名文档。

Result: GLEN在NQ320k、MS MARCO和BEIR等基准数据集上实现了最新或竞争力性能。

Conclusion: GLEN通过创新方法克服了生成式检索的挑战，展现了高效的检索能力。

Abstract: Generative retrieval shed light on a new paradigm of document retrieval,
aiming to directly generate the identifier of a relevant document for a query.
While it takes advantage of bypassing the construction of auxiliary index
structures, existing studies face two significant challenges: (i) the
discrepancy between the knowledge of pre-trained language models and
identifiers and (ii) the gap between training and inference that poses
difficulty in learning to rank. To overcome these challenges, we propose a
novel generative retrieval method, namely Generative retrieval via LExical
iNdex learning (GLEN). For training, GLEN effectively exploits a dynamic
lexical identifier using a two-phase index learning strategy, enabling it to
learn meaningful lexical identifiers and relevance signals between queries and
documents. For inference, GLEN utilizes collision-free inference, using
identifier weights to rank documents without additional overhead. Experimental
results prove that GLEN achieves state-of-the-art or competitive performance
against existing generative retrieval methods on various benchmark datasets,
e.g., NQ320k, MS MARCO, and BEIR. The code is available at
https://github.com/skleee/GLEN.

</details>


### [645] [Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers](https://arxiv.org/abs/2506.00054)
*Chaitanya Sharma*

Key words: 检索增强生成（RAG）、大语言模型（LLMs）、检索优化、生成控制、问答任务

TL;DR: 本文综述了检索增强生成（RAG）的最新进展，提出了分类法分析不同架构，并探讨了其在检索优化、生成控制和效率提升方面的改进。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 通过检索增强生成（RAG）弥补大语言模型（LLMs）在参数化知识存储中的不足，如事实不一致性和领域灵活性差。

Method: 对RAG系统进行分类（检索中心、生成中心、混合设计和鲁棒性设计），分析检索优化、上下文过滤、解码控制和效率改进。

Result: 在短格式和多跳问答任务中展示了性能改进，并总结了检索精度与生成灵活性、效率与忠实性之间的权衡。

Conclusion: 提出了未来研究方向，如自适应检索架构、实时检索集成和隐私保护检索机制。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to
enhance large language models (LLMs) by conditioning generation on external
evidence retrieved at inference time. While RAG addresses critical limitations
of parametric knowledge storage-such as factual inconsistency and domain
inflexibility-it introduces new challenges in retrieval quality, grounding
fidelity, pipeline efficiency, and robustness against noisy or adversarial
inputs. This survey provides a comprehensive synthesis of recent advances in
RAG systems, offering a taxonomy that categorizes architectures into
retriever-centric, generator-centric, hybrid, and robustness-oriented designs.
We systematically analyze enhancements across retrieval optimization, context
filtering, decoding control, and efficiency improvements, supported by
comparative performance analyses on short-form and multi-hop question answering
tasks. Furthermore, we review state-of-the-art evaluation frameworks and
benchmarks, highlighting trends in retrieval-aware evaluation, robustness
testing, and federated retrieval settings. Our analysis reveals recurring
trade-offs between retrieval precision and generation flexibility, efficiency
and faithfulness, and modularity and coordination. We conclude by identifying
open challenges and future research directions, including adaptive retrieval
architectures, real-time retrieval integration, structured reasoning over
multi-hop evidence, and privacy-preserving retrieval mechanisms. This survey
aims to consolidate current knowledge in RAG research and serve as a foundation
for the next generation of retrieval-augmented language modeling systems.

</details>


### [646] [Query Drift Compensation: Enabling Compatibility in Continual Learning of Retrieval Embedding Models](https://arxiv.org/abs/2506.00037)
*Dipam Goswami,Liying Wang,Bartłomiej Twardowski,Joost van de Weijer*

Key words: 文本嵌入、动态训练数据、嵌入蒸馏、查询漂移补偿、语义搜索

TL;DR: 提出一种在动态训练数据环境下避免重新索引旧语料的文本嵌入模型更新方法，通过嵌入蒸馏和查询漂移补偿减少遗忘。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 研究如何在动态训练数据场景下，避免重新索引旧语料的情况下有效使用更新的文本嵌入模型。

Method: 采用嵌入蒸馏技术稳定查询和文档嵌入，并提出查询漂移补偿方法，将新模型的查询嵌入投影到旧嵌入空间。

Result: 所提方法显著提升性能，无需重新索引旧语料。

Conclusion: 方法有效解决了模型更新导致的非兼容性问题，降低了遗忘。

Abstract: Text embedding models enable semantic search, powering several NLP
applications like Retrieval Augmented Generation by efficient information
retrieval (IR). However, text embedding models are commonly studied in
scenarios where the training data is static, thus limiting its applications to
dynamic scenarios where new training data emerges over time. IR methods
generally encode a huge corpus of documents to low-dimensional embeddings and
store them in a database index. During retrieval, a semantic search over the
corpus is performed and the document whose embedding is most similar to the
query embedding is returned. When updating an embedding model with new training
data, using the already indexed corpus is suboptimal due to the
non-compatibility issue, since the model which was used to obtain the
embeddings of the corpus has changed. While re-indexing of old corpus documents
using the updated model enables compatibility, it requires much higher
computation and time. Thus, it is critical to study how the already indexed
corpus can still be effectively used without the need of re-indexing. In this
work, we establish a continual learning benchmark with large-scale datasets and
continually train dense retrieval embedding models on query-document pairs from
new datasets in each task and observe forgetting on old tasks due to
significant drift of embeddings. We employ embedding distillation on both query
and document embeddings to maintain stability and propose a novel query drift
compensation method during retrieval to project new model query embeddings to
the old embedding space. This enables compatibility with previously indexed
corpus embeddings extracted using the old model and thus reduces the
forgetting. We show that the proposed method significantly improves performance
without any re-indexing. Code is available at
https://github.com/dipamgoswami/QDC.

</details>


### [647] [Decoding Dense Embeddings: Sparse Autoencoders for Interpreting and Discretizing Dense Retrieval](https://arxiv.org/abs/2506.00041)
*Seongwan Park,Taeklim Kim,Youngjoong Ko*

Key words: 密集检索模型, 可解释性, 稀疏自编码器, CL-SR

TL;DR: 本文提出了一种利用稀疏自编码器（SAEs）分解密集嵌入的可解释性框架，并引入了概念级稀疏检索（CL-SR），结合语义表达和稀疏表示的效率。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 解决密集检索模型（DPR）缺乏可解释性的问题。

Method: 使用稀疏自编码器分解密集嵌入，生成自然语言描述，并提出CL-SR框架。

Result: CL-SR在索引空间和计算效率方面表现优异，同时保持稳健性能。

Conclusion: CL-SR成功结合了密集嵌入的语义表达和稀疏表示的透明性。

Abstract: Despite their strong performance, Dense Passage Retrieval (DPR) models suffer
from a lack of interpretability. In this work, we propose a novel
interpretability framework that leverages Sparse Autoencoders (SAEs) to
decompose previously uninterpretable dense embeddings from DPR models into
distinct, interpretable latent concepts. We generate natural language
descriptions for each latent concept, enabling human interpretations of both
the dense embeddings and the query-document similarity scores of DPR models. We
further introduce Concept-Level Sparse Retrieval (CL-SR), a retrieval framework
that directly utilizes the extracted latent concepts as indexing units. CL-SR
effectively combines the semantic expressiveness of dense embeddings with the
transparency and efficiency of sparse representations. We show that CL-SR
achieves high index-space and computational efficiency while maintaining robust
performance across vocabulary and semantic mismatches.

</details>


### [648] [Graph Contrastive Learning for Optimizing Sparse Data in Recommender Systems with LightGCL](https://arxiv.org/abs/2506.00048)
*Aravinda Jatavallabha,Prabhanjan Bharadwaj,Ashish Chander*

Key words: GNN, 推荐系统, 图对比学习, SVD, 数据稀疏性

TL;DR: LightGCL通过SVD增强图对比学习，解决了GNN在推荐系统中的稀疏性和噪声问题，提高了性能和公平性。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: GNN在推荐系统中因数据稀疏和噪声表现不佳，需一种更稳健的增强方法。

Method: 采用基于SVD的图对比学习模型LightGCL，避免随机或启发式扰动，保留语义完整性。

Result: LightGCL在多个基准数据集上优于现有模型，同时提升了公平性和抗流行度偏差能力。

Conclusion: LightGCL是一种适用于真实推荐系统的有效解决方案，显著提升性能与稳健性。

Abstract: Graph Neural Networks (GNNs) are powerful tools for recommendation systems,
but they often struggle under data sparsity and noise. To address these issues,
we implemented LightGCL, a graph contrastive learning model that uses Singular
Value Decomposition (SVD) for robust graph augmentation, preserving semantic
integrity without relying on stochastic or heuristic perturbations. LightGCL
enables structural refinement and captures global collaborative signals,
achieving significant gains over state-of-the-art models across benchmark
datasets. Our experiments also demonstrate improved fairness and resilience to
popularity bias, making it well-suited for real-world recommender systems.

</details>


### [649] [GPR: Empowering Generation with Graph-Pretrained Retriever](https://arxiv.org/abs/2506.00261)
*Xiaochen Wang,Zongyu Wu,Yuan Zhong,Xiang Zhang,Suhang Wang,Fenglong Ma*

Key words: 图检索增强生成、知识图谱、语言模型、结构感知

TL;DR: 论文提出了GPR，一种基于知识图谱预训练的图检索方法，通过语言模型引导的图增强和结构感知目标提升了检索质量和下游生成效果。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 现有检索器通常依赖于纯文本预训练，导致领域不匹配和结构忽略，限制了图检索增强生成（GRAG）的效果。

Method: GPR直接在知识图谱上预训练，通过语言模型引导的图增强和结构感知目标，对齐自然语言问题与相关子图。

Result: 在两个数据集、三个LLM主干和五个基线上的实验表明，GPR显著提升了检索质量和下游生成效果。

Conclusion: GPR是一种有效的图检索解决方案，适用于GRAG任务。

Abstract: Graph retrieval-augmented generation (GRAG) places high demands on
graph-specific retrievers. However, existing retrievers often rely on language
models pretrained on plain text, limiting their effectiveness due to domain
misalignment and structure ignorance. To address these challenges, we propose
GPR, a graph-based retriever pretrained directly on knowledge graphs. GPR
aligns natural language questions with relevant subgraphs through LLM-guided
graph augmentation and employs a structure-aware objective to learn
fine-grained retrieval strategies. Experiments on two datasets, three LLM
backbones, and five baselines show that GPR consistently improves both
retrieval quality and downstream generation, demonstrating its effectiveness as
a robust retrieval solution for GRAG.

</details>


### [650] [Adapting General-Purpose Embedding Models to Private Datasets Using Keyword-based Retrieval](https://arxiv.org/abs/2506.00363)
*Yubai Wei,Jiale Han,Yi Yang*

Key words: 文本嵌入模型, BM25, 私有数据集, 检索增强生成, BMEmbed

TL;DR: BMEmbed是一种新方法，通过利用BM25技术为通用文本嵌入模型构建监督信号，以提升其在私有数据集上的检索性能。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 通用文本嵌入模型在通用检索基准上表现良好，但在包含专业术语的私有数据集上效果不佳，因此需要一种适应方法。

Method: 利用BM25的关键词检索技术，构建监督信号以优化模型在私有数据集上的嵌入表现。

Result: 在各种领域、数据集和模型上验证了BMEmbed的一致性能提升。

Conclusion: BM25信号通过促进对齐和均匀性改进嵌入，为适应领域特定数据提供了价值。

Abstract: Text embedding models play a cornerstone role in AI applications, such as
retrieval-augmented generation (RAG). While general-purpose text embedding
models demonstrate strong performance on generic retrieval benchmarks, their
effectiveness diminishes when applied to private datasets (e.g.,
company-specific proprietary data), which often contain specialized terminology
and lingo. In this work, we introduce BMEmbed, a novel method for adapting
general-purpose text embedding models to private datasets. By leveraging the
well-established keyword-based retrieval technique (BM25), we construct
supervisory signals from the ranking of keyword-based retrieval results to
facilitate model adaptation. We evaluate BMEmbed across a range of domains,
datasets, and models, showing consistent improvements in retrieval performance.
Moreover, we provide empirical insights into how BM25-based signals contribute
to improving embeddings by fostering alignment and uniformity, highlighting the
value of this approach in adapting models to domain-specific data. We release
the source code available at https://github.com/BaileyWei/BMEmbed for the
research community.

</details>


### [651] [Bridging the Gap: From Ad-hoc to Proactive Search in Conversations](https://arxiv.org/abs/2506.00983)
*Chuan Meng,Francesco Tonolini,Fengran Mo,Nikolaos Aletras,Emine Yilmaz,Gabriella Kazai*

Key words: 对话主动搜索、检索器适应、查询转换、Conv2Query

TL;DR: 论文提出了Conv2Query框架，通过将对话上下文映射为传统检索查询，解决了传统检索器在对话环境中输入不匹配的问题，显著提高了检索性能。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 现有的对话主动搜索（PSC）方法因输入格式不匹配而限制了检索质量，传统检索器适合短查询而非长对话上下文。

Method: 提出了Conv2Query框架，将对话上下文转换为适合传统检索器的查询，可直接或微调后用于检索。

Result: 在两个PSC数据集上的实验表明，Conv2Query显著提升了传统检索器的性能，无论直接使用还是微调后。

Conclusion: Conv2Query有效解决了输入不匹配问题，为PSC任务提供了更优的检索方案。

Abstract: Proactive search in conversations (PSC) aims to reduce user effort in
formulating explicit queries by proactively retrieving useful relevant
information given conversational context. Previous work in PSC either directly
uses this context as input to off-the-shelf ad-hoc retrievers or further
fine-tunes them on PSC data. However, ad-hoc retrievers are pre-trained on
short and concise queries, while the PSC input is longer and noisier. This
input mismatch between ad-hoc search and PSC limits retrieval quality. While
fine-tuning on PSC data helps, its benefits remain constrained by this input
gap. In this work, we propose Conv2Query, a novel conversation-to-query
framework that adapts ad-hoc retrievers to PSC by bridging the input gap
between ad-hoc search and PSC. Conv2Query maps conversational context into
ad-hoc queries, which can either be used as input for off-the-shelf ad-hoc
retrievers or for further fine-tuning on PSC data. Extensive experiments on two
PSC datasets show that Conv2Query significantly improves ad-hoc retrievers'
performance, both when used directly and after fine-tuning on PSC.

</details>


### [652] [GRAM: Generative Recommendation via Semantic-aware Multi-granular Late Fusion](https://arxiv.org/abs/2506.01673)
*Sunkyung Lee,Minjin Choi,Eunseong Choi,Hye-young Kim,Jongwuk Lee*

Key words: 生成推荐、多粒度融合、语义感知、大语言模型

TL;DR: 论文提出了GRRAM方法，通过语义感知的多粒度后期融合改进生成推荐，解决了现有研究中隐式项目关系和丰富但冗长项目信息利用不足的问题。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 现有生成推荐研究在隐式项目关系建模和冗长项目信息利用上存在不足，需要一种更高效且能保留信息的方法。

Method: 设计了语义到词汇的转换和多粒度后期融合，分别解决项目关系编码和信息高效整合问题。

Result: 在四个基准数据集上，GRAM在Recall@5和NDCG@5指标上显著优于现有方法，提升了11.5-16.0%和5.3-13.6%。

Conclusion: GRAM通过两项创新显著提升了生成推荐性能，为未来研究提供了新思路。

Abstract: Generative recommendation is an emerging paradigm that leverages the
extensive knowledge of large language models by formulating recommendations
into a text-to-text generation task. However, existing studies face two key
limitations in (i) incorporating implicit item relationships and (ii) utilizing
rich yet lengthy item information. To address these challenges, we propose a
Generative Recommender via semantic-Aware Multi-granular late fusion (GRAM),
introducing two synergistic innovations. First, we design semantic-to-lexical
translation to encode implicit hierarchical and collaborative item
relationships into the vocabulary space of LLMs. Second, we present
multi-granular late fusion to integrate rich semantics efficiently with minimal
information loss. It employs separate encoders for multi-granular prompts,
delaying the fusion until the decoding stage. Experiments on four benchmark
datasets show that GRAM outperforms eight state-of-the-art generative
recommendation models, achieving significant improvements of 11.5-16.0% in
Recall@5 and 5.3-13.6% in NDCG@5. The source code is available at
https://github.com/skleee/GRAM.

</details>


### [653] [DV365: Extremely Long User History Modeling at Instagram](https://arxiv.org/abs/2506.00450)
*Wenhan Lyu,Devashish Tyagi,Yihang Yang,Ziwei Li,Ajay Somani,Karthikeyan Shanmugasundaram,Nikola Andrejevic,Ferdi Adeputra,Curtis Zeng,Arun K. Singh,Maxime Ransan,Sagar Jain*

Key words: 推荐系统, 长用户序列, 离线嵌入, 多切片, 摘要技术, Instagram

TL;DR: 提出了一种离线嵌入方法（DV365），通过多切片和摘要技术处理超长用户历史序列，以低成本实现高效推荐。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 长用户历史数据对推荐系统有重要价值，但直接使用成本高，需低耗高效的方法。

Method: 采用离线嵌入和用户嵌入学习策略（多切片与摘要），生成稳定兴趣的用户表征。

Result: DV365嵌入在Instagram中显著提升性能，支持15个模型并稳定运行1年以上。

Conclusion: 离线嵌入方法是一种成本效益高的解决方案，能有效处理超长用户序列。

Abstract: Long user history is highly valuable signal for recommendation systems, but
effectively incorporating it often comes with high cost in terms of data center
power consumption and GPU. In this work, we chose offline embedding over
end-to-end sequence length optimization methods to enable extremely long user
sequence modeling as a cost-effective solution, and propose a new user
embedding learning strategy, multi-slicing and summarization, that generates
highly generalizable user representation of user's long-term stable interest.
History length we encoded in this embedding is up to 70,000 and on average
40,000. This embedding, named as DV365, is proven highly incremental on top of
advanced attentive user sequence models deployed in Instagram. Produced by a
single upstream foundational model, it is launched in 15 different models
across Instagram and Threads with significant impact, and has been production
battle-proven for >1 year since our first launch.

</details>


### [654] [When Should Dense Retrievers Be Updated in Evolving Corpora? Detecting Out-of-Distribution Corpora Using GradNormIR](https://arxiv.org/abs/2506.01877)
*Dayoon Ko,Jinyoung Kim,Sohyeon Kim,Jinhyuk Kim,Jaehoon Lee,Seonghak Song,Minyoung Lee,Gunhee Kim*

Key words: 密集检索器,分布偏移,梯度范数,无监督学习,动态语料库

TL;DR: 论文提出了一种名为GradNormIR的无监督方法，用于检测密集检索器的语料库是否分布偏移，以提前管理检索器更新，提升检索系统性能。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 真实世界的语料库不断变化，若不及时更新或重新训练检索器，新文档的索引可能导致检索性能下降。因此，识别何时需要更新检索器是关键。

Method: 提出了GradNormIR方法，通过梯度范数检测语料库是否超出密集检索器的原始分布（OOD）。

Result: 在BEIR基准测试中，GradNormIR有效地提升了密集检索器在动态语料库中的检索鲁棒性和效率。

Conclusion: GradNormIR方法能够主动管理检索器更新，防止潜在的检索失败，提高系统性能。

Abstract: Dense retrievers encode texts into embeddings to efficiently retrieve
relevant documents from large databases in response to user queries. However,
real-world corpora continually evolve, leading to a shift from the original
training distribution of the retriever. Without timely updates or retraining,
indexing newly emerging documents can degrade retrieval performance for future
queries. Thus, identifying when a dense retriever requires an update is
critical for maintaining robust retrieval systems. In this paper, we propose a
novel task of predicting whether a corpus is out-of-distribution (OOD) relative
to a dense retriever before indexing. Addressing this task allows us to
proactively manage retriever updates, preventing potential retrieval failures.
We introduce GradNormIR, an unsupervised approach that leverages gradient norms
to detect OOD corpora effectively. Experiments on the BEIR benchmark
demonstrate that GradNormIR enables timely updates of dense retrievers in
evolving document collections, significantly enhancing retrieval robustness and
efficiency.

</details>


### [655] [Breaker: Removing Shortcut Cues with User Clustering for Single-slot Recommendation System](https://arxiv.org/abs/2506.00828)
*Chao Wang,Yue Zheng,Yujing Zhang,Yan Feng,Zhe Wang,Xiaowei Shi,An You,Yu Chen*

Key words: 推荐系统, 快捷偏差, 用户表示聚类, 多塔结构, 延迟参数更新

TL;DR: 论文提出Breaker模型，通过用户表示聚类和多塔结构消除推荐系统中用户内在倾向的快捷偏差，提升用户-物品偏好的建模效果。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 在单槽推荐系统中，用户只能一次接触一个物品，系统无法同时收集多物品反馈，导致模型容易依赖用户内在倾向而产生快捷偏差。

Method: Breaker模型结合用户表示聚类的辅助任务和多塔结构，进行聚类特定的偏好建模，并采用延迟参数更新机制以增强训练稳定性。

Result: 离线和在线实验表明，该方法优于基线模型，并已在美团平台上成功部署，每日服务数千万用户。

Conclusion: Breaker模型有效消除用户内在倾向的快捷偏差，提升了单槽推荐系统的性能。

Abstract: In a single-slot recommendation system, users are only exposed to one item at
a time, and the system cannot collect user feedback on multiple items
simultaneously. Therefore, only pointwise modeling solutions can be adopted,
focusing solely on modeling the likelihood of clicks or conversions for items
by users to learn user-item preferences, without the ability to capture the
ranking information among different items directly. However, since user-side
information is often much more abundant than item-side information, the model
can quickly learn the differences in user intrinsic tendencies, which are
independent of the items they are exposed to. This can cause these intrinsic
tendencies to become a shortcut bias for the model, leading to insufficient
mining of the most concerned user-item preferences. To solve this challenge, we
introduce the Breaker model. Breaker integrates an auxiliary task of user
representation clustering with a multi-tower structure for cluster-specific
preference modeling. By clustering user representations, we ensure that users
within each cluster exhibit similar characteristics, which increases the
complexity of the pointwise recommendation task on the user side. This forces
the multi-tower structure with cluster-driven parameter learning to better
model user-item preferences, ultimately eliminating shortcut biases related to
user intrinsic tendencies. In terms of training, we propose a delayed parameter
update mechanism to enhance training stability and convergence, enabling
end-to-end joint training of the auxiliary clustering and classification tasks.
Both offline and online experiments demonstrate that our method surpasses the
baselines. It has already been deployed and is actively serving tens of
millions of users daily on Meituan, one of the most popular e-commerce
platforms for services.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [656] [Enabling Secure and Ephemeral AI Workloads in Data Mesh Environments](https://arxiv.org/abs/2506.00352)
*Chinkit Patel,Kee Siong Ng*

Key words: 自服务数据平台, Kubernetes, 基础设施即代码, 数据网格

TL;DR: 为大型企业提供一种高效、自服务的数据平台基础设施，支持数据团队快速构建和销毁短期Kubernetes集群，以实验和部署数据产品。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 解决企业在复杂ICT环境中缺乏高效支持数据团队快速实验和部署数据产品的问题。

Method: 利用不可变的容器操作系统和基础设施即代码方法，在本地或云环境中创建短生命周期的、供应商中立的Kubernetes集群。

Result: 提出了一种可重复、便携且成本高效的解决方案，可作为商业PaaS的替代或补充。

Conclusion: 该方法特别适用于支持复杂数据网格环境中的互操作性，涵盖现代和传统计算基础设施。

Abstract: Many large enterprises that operate highly governed and complex ICT
environments have no efficient and effective way to support their Data and AI
teams in rapidly spinning up and tearing down self-service data and compute
infrastructure, to experiment with new data analytic tools, and deploy data
products into operational use. This paper proposes a key piece of the solution
to the overall problem, in the form of an on-demand self-service data-platform
infrastructure to empower de-centralised data teams to build data products on
top of centralised templates, policies and governance. The core innovation is
an efficient method to leverage immutable container operating systems and
infrastructure-as-code methodologies for creating, from scratch, vendor-neutral
and short-lived Kubernetes clusters on-premises and in any cloud environment.
Our proposed approach can serve as a repeatable, portable and cost-efficient
alternative or complement to commercial Platform-as-a-Service (PaaS) offerings,
and this is particularly important in supporting interoperability in complex
data mesh environments with a mix of modern and legacy compute infrastructure.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [657] [Quantum Neural Networks in Practice: A Comparative Study with Classical Models from Standard Data Sets to Industrial Images](https://arxiv.org/abs/2411.19276)
*Daniel Basilewitsch,João F. Bravo,Christian Tutschku,Frederick Struckmeier*

Key words: 量子神经网络,卷积神经网络,图像分类,量子计算,机器学习

TL;DR: 比较了随机经典和量子神经网络、经典与量子-经典混合卷积神经网络在二元图像分类任务中的性能，分析了量子模型超参数的影响，发现量子模型在训练稳定性上表现更好。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 评估量子神经网络在实际图像分类任务中的潜力，探索量子模型在性能、稳定性和超参数相关性方面的表现，为工业应用提供参考。

Method: 使用随机神经网络处理降维数据和卷积神经网络处理全图像数据，对比分析量子与经典模型的分类准确率、训练稳定性及超参数相关性。

Result: 量子与经典模型在大多数数据集上表现相当，量子模型训练稳定性更好。训练参数数量与模型性能正相关，94%的量子模型包含纠缠门。

Conclusion: 量子机器学习在图像分类中表现可靠但无显著优势，未来研究方向包括量子电路设计、纠缠门利用和模型跨任务迁移性。

Abstract: In this study, we compare the performance of randomized classical and quantum
neural networks (NNs) as well as classical and quantum-classical hybrid
convolutional neural networks (CNNs) for the task of binary image
classification. We use two distinct methodologies: using randomized NNs on
dimensionality-reduced data, and applying CNNs to full image data. We evaluate
these approaches on three data sets of increasing complexity: an artificial
hypercube dataset, MNIST handwritten digits and real-world industrial images.
We analyze correlations between classification accuracy and quantum model
hyperparameters, including the number of trainable parameters, feature encoding
methods, circuit layers, entangling gate type and structure, gate entangling
power, and measurement operators. For random quantum NNs, we compare their
performance against literature models. Classical and quantum/hybrid models
achieved statistically equivalent classification accuracies across most
datasets, with no approach demonstrating consistent superiority. We observe
that quantum models show lower variance with respect to initial training
parameters, suggesting better training stability. Among the hyperparameters
analyzed, only the number of trainable parameters showed a positive correlation
with the model performance. Around 94% of the best-performing quantum NNs had
entangling gates, although for hybrid CNNs, models without entanglement
performed equally well but took longer to converge. Cross-dataset performance
analysis revealed limited transferability of quantum models between different
classification tasks. Our study provides an industry perspective on quantum
machine learning for practical image classification tasks, highlighting both
current limitations and potential avenues for further research in quantum
circuit design, entanglement utilization, and model transferability across
varied applications.

</details>


### [658] [Synthesis of discrete-continuous quantum circuits with multimodal diffusion models](https://arxiv.org/abs/2506.01666)
*Florian Fürrutter,Zohim Chandani,Ikko Hamamura,Hans J. Briegel,Gorka Muñoz-Gil*

Key words: 量子计算, 编译优化, 多模态扩散模型, 电路合成

TL;DR: 论文提出了一种基于多模态去噪扩散模型的方法，用于同时生成量子电路的结构和连续参数，以高效编译目标酉矩阵，解决了传统方法的高成本和扩展性问题。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 当前量子计算中，高效编译量子操作是一个主要瓶颈。现有方法虽能实现低编译误差，但运行时间长且依赖于量子硬件或昂贵的经典模拟，难以扩展。

Method: 引入多模态去噪扩散模型，结合两个独立的扩散过程：一个用于离散门选择，另一个用于参数预测，以同时生成电路结构和连续参数。

Result: 通过不同实验的基准测试，分析了模型在多样量子比特数量、电路深度和参数化门比例下的准确性，并利用其快速生成能力创建数据集以提取启发式规则。

Conclusion: 该方法展示了快速电路生成能力，并推动了量子电路合成的新见解发现。

Abstract: Efficiently compiling quantum operations remains a major bottleneck in
scaling quantum computing. Today's state-of-the-art methods achieve low
compilation error by combining search algorithms with gradient-based parameter
optimization, but they incur long runtimes and require multiple calls to
quantum hardware or expensive classical simulations, making their scaling
prohibitive. Recently, machine-learning models have emerged as an alternative,
though they are currently restricted to discrete gate sets. Here, we introduce
a multimodal denoising diffusion model that simultaneously generates a
circuit's structure and its continuous parameters for compiling a target
unitary. It leverages two independent diffusion processes, one for discrete
gate selection and one for parameter prediction. We benchmark the model over
different experiments, analyzing the method's accuracy across varying qubit
counts, circuit depths, and proportions of parameterized gates. Finally, by
exploiting its rapid circuit generation, we create large datasets of circuits
for particular operations and use these to extract valuable heuristics that can
help us discover new insights into quantum circuit synthesis.

</details>


### [659] [Learning thermodynamic master equations for open quantum systems](https://arxiv.org/abs/2506.01882)
*Peter Sentz,Stanley Nicholson,Yujin Cho,Sohail Reddy,Brendan Keith,Stefanie Günther*

Key words: 开放量子系统，数据驱动模型，热力学一致性，哈密顿量估计

TL;DR: 该论文提出了一种数据驱动的开放量子系统模型，结合了可学习的、热力学一致的项，并验证了其有效性。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 开放量子系统中的哈密顿量和其他组件的表征对量子计算至关重要，然而现有模型多为线性，缺乏物理原理引入的非线性特性。

Method: 采用数据驱动的方法，引入可学习且热力学一致的项，直接估计系统哈密顿量和环境耦合的线性部分。

Result: 模型在合成的两能级和三能级数据以及实验数据上表现良好。

Conclusion: 该模型具有可解释性，且能有效表征开放量子系统。

Abstract: The characterization of Hamiltonians and other components of open quantum
dynamical systems plays a crucial role in quantum computing and other
applications. Scientific machine learning techniques have been applied to this
problem in a variety of ways, including by modeling with deep neural networks.
However, the majority of mathematical models describing open quantum systems
are linear, and the natural nonlinearities in learnable models have not been
incorporated using physical principles. We present a data-driven model for open
quantum systems that includes learnable, thermodynamically consistent terms.
The trained model is interpretable, as it directly estimates the system
Hamiltonian and linear components of coupling to the environment. We validate
the model on synthetic two and three-level data, as well as experimental
two-level data collected from a quantum device at Lawrence Livermore National
Laboratory.

</details>


### [660] [Probing Quantum Spin Systems with Kolmogorov-Arnold Neural Network Quantum States](https://arxiv.org/abs/2506.01891)
*Mahmud Ashraf Shamim,Eric Reinhardt,Talal Ahmed Chowdhury,Sergei Gleyzer,Paulo T Araujo*

Key words: 神经量子态, Kolmogorov-Arnold网络, 变分波函数, 量子多体系统, 基态能量

TL;DR: 提出了基于Kolmogorov-Arnold网络（KANs）的SineKAN作为神经量子态（NQS）的新变分波函数，用于研究量子多体系统，并在多个模型中表现出优于其他方法的性能。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 探索一种新的神经网络参数化量子态表示方法，以提高对量子多体系统基态能量和关联函数的计算精度。

Method: 使用带有可学习正弦激活函数的SineKAN模型，将其应用于一维横向场伊辛模型、各向异性海森堡模型和反铁磁$J_{1}-J_{2}$模型。

Result: SineKAN在$J_1-J_2$模型（L=100）中表现优于RBM、LSTM和FFCN等现有方法，接近DMRG算法的结果。

Conclusion: SineKAN作为一种新的NQS变分波函数，在量子多体系统研究中具有优越性和潜力。

Abstract: Neural Quantum States (NQS) are a class of variational wave functions
parametrized by neural networks (NNs) to study quantum many-body systems. In
this work, we propose SineKAN, the NQS ansatz based on Kolmogorov-Arnold
Networks (KANs), to represent quantum mechanical wave functions as nested
univariate functions. We show that \sk wavefunction with learnable sinusoidal
activation functions can capture the ground state energies, fidelities and
various correlation functions of the 1D Transverse-Field Ising model,
Anisotropic Heisenberg model, and Antiferromagnetic $J_{1}-J_{2}$ model with
different chain lengths. In our study of the $J_1-J_2$ model with $L=100$
sites, we find that the SineKAN model outperforms several previously explored
neural quantum state ans\"atze, including Restricted Boltzmann Machines (RBMs),
Long Short-Term Memory models (LSTMs), and Feed-Forward Neural Networks (FFCN),
when compared to the results obtained from the Density Matrix Renormalization
Group (DMRG) algorithm.

</details>


### [661] [A Quantum Information Theoretic Approach to Tractable Probabilistic Models](https://arxiv.org/abs/2506.01824)
*Pedro Zuidberg Dos Martires*

Key words: 概率电路、量子信息理论、正单元电路（PUnCs）、半正定矩阵

TL;DR: 摘要介绍了通过量子信息理论框架研究概率电路，引入了正单元电路（PUnCs），扩展了概率电路的应用范围。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 利用量子信息理论框架进一步研究和扩展概率电路的功能和应用。

Method: 通过递归嵌套和和积操作，引入正单元电路（PUnCs），将概率电路扩展到半正定矩阵评估。

Result: PUnCs严格泛化了概率电路以及PSD电路等新近引入的电路类别。

Conclusion: PUnCs为概率电路提供了更广泛的框架，为电路评估开辟了新的可能性。

Abstract: By recursively nesting sums and products, probabilistic circuits have emerged
in recent years as an attractive class of generative models as they enjoy, for
instance, polytime marginalization of random variables. In this work we study
these machine learning models using the framework of quantum information
theory, leading to the introduction of positive unital circuits (PUnCs), which
generalize circuit evaluations over positive real-valued probabilities to
circuit evaluations over positive semi-definite matrices. As a consequence,
PUnCs strictly generalize probabilistic circuits as well as recently introduced
circuit classes such as PSD circuits.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [662] [PathGene: Benchmarking Driver Gene Mutations and Exon Prediction Using Multicenter Lung Cancer Histopathology Image Dataset](https://arxiv.org/abs/2506.00096)
*Liangrui Pan,Qingchun Liang,Shen Zhao,Songqing Fan,Shaoliang Peng*

Key words: 肺癌, 基因突变, 病理图像, 多示例学习, 精准医疗

TL;DR: 论文提出PathGene数据集，结合病理图像和基因测序数据，通过深度学习预测肺癌基因突变、亚型和外显子变异，为精准医疗提供支持。

<details>
  <summary>Details</summary>

Main category: q-bio.GN

Motivation: 为了解决医疗资源分布不均和基因检测成本高的问题，利用AI从常规病理图像推断基因突变和外显子变异，推动精准治疗。

Method: 收集多中心数据集PathGene，包含1576名患者的数据，并评估11种多示例学习方法在突变、亚型、外显子和TMB预测任务中的表现。

Result: PathGene数据集和实验方法为肺癌患者的早期基因筛查和个性化治疗计划提供了有效工具。

Conclusion: PathGene及其方法在肺癌精准医疗中表现出潜力，可作为临床辅助工具。

Abstract: Accurately predicting gene mutations, mutation subtypes and their exons in
lung cancer is critical for personalized treatment planning and prognostic
assessment. Faced with regional disparities in medical resources and the high
cost of genomic assays, using artificial intelligence to infer these mutations
and exon variants from routine histopathology images could greatly facilitate
precision therapy. Although some prior studies have shown that deep learning
can accelerate the prediction of key gene mutations from lung cancer pathology
slides, their performance remains suboptimal and has so far been limited mainly
to early screening tasks. To address these limitations, we have assembled
PathGene, which comprises histopathology images paired with next-generation
sequencing reports from 1,576 patients at the Second Xiangya Hospital, Central
South University, and 448 TCGA-LUAD patients. This multi-center dataset links
whole-slide images to driver gene mutation status, mutation subtypes, exon, and
tumor mutational burden (TMB) status, with the goal of leveraging pathology
images to predict mutations, subtypes, exon locations, and TMB for early
genetic screening and to advance precision oncology. Unlike existing datasets,
we provide molecular-level information related to histopathology images in
PathGene to facilitate the development of biomarker prediction models. We
benchmarked 11 multiple-instance learning methods on PathGene for mutation,
subtype, exon, and TMB prediction tasks. These experimental methods provide
valuable alternatives for early genetic screening of lung cancer patients and
assisting clinicians to quickly develop personalized precision targeted
treatment plans for patients. Code and data are available at
https://github.com/panliangrui/NIPS2025/.

</details>


### [663] [Uncertainty-Aware Genomic Classification of Alzheimer's Disease: A Transformer-Based Ensemble Approach with Monte Carlo Dropout](https://arxiv.org/abs/2506.00662)
*Taeho Jo,Eun Hye Lee,Alzheimer's Disease Sequencing Project*

Key words: 阿尔茨海默病, 基因组分类, Transformer, 蒙特卡洛Dropout, 随机森林

TL;DR: 利用Transformer集成模型（TrUE-Net）和蒙特卡洛Dropout技术，通过全基因组测序数据进行阿尔茨海默病分类，不确定性阈值区分样本，提高了分类准确性。

<details>
  <summary>Details</summary>

Main category: q-bio.GN

Motivation: 阿尔茨海默病（AD）具有遗传复杂性，基因组数据分类困难。

Method: 开发了基于Transformer的集成模型（TrUE-Net），结合蒙特卡洛Dropout和随机森林，对全基因组测序数据进行分类。

Result: 测试集上准确率为0.6514，AUC为0.6636；排除高不确定性样本后，准确率提升10.24%，F1分数提升23.62%。

Conclusion: 蒙特卡洛Dropout有助于识别不确定性样本，提升AD基因组分类的可靠性。

Abstract: INTRODUCTION: Alzheimer's disease (AD) is genetically complex, complicating
robust classification from genomic data. METHODS: We developed a
transformer-based ensemble model (TrUE-Net) using Monte Carlo Dropout for
uncertainty estimation in AD classification from whole-genome sequencing (WGS).
We combined a transformer that preserves single-nucleotide polymorphism (SNP)
sequence structure with a concurrent random forest using flattened genotypes.
An uncertainty threshold separated samples into an uncertain (high-variance)
group and a more certain (low-variance) group. RESULTS: We analyzed 1050
individuals, holding out half for testing. Overall accuracy and area under the
receiver operating characteristic (ROC) curve (AUC) were 0.6514 and 0.6636,
respectively. Excluding the uncertain group improved accuracy from 0.6263 to
0.7287 (10.24% increase) and F1 from 0.5843 to 0.8205 (23.62% increase).
DISCUSSION: Monte Carlo Dropout-driven uncertainty helps identify ambiguous
cases that may require further clinical evaluation, thus improving reliability
in AD genomic classification.

</details>


### [664] [GenDMR: A dynamic multimodal role-swapping network for identifying risk gene phenotypes](https://arxiv.org/abs/2506.01456)
*Lina Qin,Cheng Zhu,Chuqi Zhou,Yukun Huang,Jiayi Zhu,Ping Liang,Jinju Wang,Yixing Huang,Cheng Luo,Dezhong Yao,Ying Tan*

Key words: 多模态融合、AD、GenDMR、SNP编码、动态角色交换

TL;DR: 提出动态多模态角色交换网络（GenDMR），解决当前深度学习在多模态数据融合中对遗传信息编码和特征平衡的不足，提升AD预测性能。

<details>
  <summary>Details</summary>

Main category: q-bio.GN

Motivation: 当前方法在遗传信息编码和多模态特征平衡上存在不足，需改进以提高AD病因分析和诊断的准确性。

Method: 开发SNP空间组织编码方法，提出多实例注意力模块和动态角色交换机制。

Result: 在ADNI数据集上达到SOTA性能，识别12个潜在高风险基因。

Conclusion: GenDMR为多模态数据融合提供了新的可解释分析能力。

Abstract: Recent studies have shown that integrating multimodal data fusion techniques
for imaging and genetic features is beneficial for the etiological analysis and
predictive diagnosis of Alzheimer's disease (AD). However, there are several
critical flaws in current deep learning methods. Firstly, there has been
insufficient discussion and exploration regarding the selection and encoding of
genetic information. Secondly, due to the significantly superior classification
value of AD imaging features compared to genetic features, many studies in
multimodal fusion emphasize the strengths of imaging features, actively
mitigating the influence of weaker features, thereby diminishing the learning
of the unique value of genetic features. To address this issue, this study
proposes the dynamic multimodal role-swapping network (GenDMR). In GenDMR, we
develop a novel approach to encode the spatial organization of single
nucleotide polymorphisms (SNPs), enhancing the representation of their genomic
context. Additionally, to adaptively quantify the disease risk of SNPs and
brain region, we propose a multi-instance attention module to enhance model
interpretability. Furthermore, we introduce a dominant modality selection
module and a contrastive self-distillation module, combining them to achieve a
dynamic teacher-student role exchange mechanism based on dominant and auxiliary
modalities for bidirectional co-updating of different modal data. Finally,
GenDMR achieves state-of-the-art performance on the ADNI public dataset and
visualizes attention to different SNPs, focusing on confirming 12 potential
high-risk genes related to AD, including the most classic APOE and recently
highlighted significant risk genes. This demonstrates GenDMR's interpretable
analytical capability in exploring AD genetic features, providing new insights
and perspectives for the development of multimodal data fusion techniques.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [665] [Unfolding Boxes with Local Constraints](https://arxiv.org/abs/2506.01079)
*Long Qian,Eric Wang,Bernardo Subercaseaux,Marijn J. H. Heule*

Key words: SAT, 多面体展开, 局部约束, 枚举, 图连通性

TL;DR: 本文提出了一种新的基于SAT的方法，通过替换全局约束为局部约束，显著提升了多面体展开的计算和枚举能力，解决了先前方法的规模限制问题。

<details>
  <summary>Details</summary>

Main category: cs.CG

Motivation: 现有SAT编码因全局约束（如图连通性或无环性）的存在而受限，难以高效处理多面体展开问题，因此需要改进。

Method: 提出一种新的SAT方法，将全局约束替换为具有更好传播特性的局部约束。

Result: 新方法将双盒共展面积从88提升至150，枚举能力从30提升至60，并否定了Xu等人关于三盒共展最小面积的猜想。

Conclusion: 通过改进SAT编码，显著提升了处理多面体展开问题的能力，为相关领域提供了新思路。

Abstract: We consider the problem of finding and enumerating polyominos that can be
folded into multiple non-isomorphic boxes. While several computational
approaches have been proposed, including SAT, randomized algorithms, and
decision diagrams, none has been able to perform at scale. We argue that
existing SAT encodings are hindered by the presence of global constraints
(e.g., graph connectivity or acyclicity), which are generally hard to encode
effectively and hard for solvers to reason about. In this work, we propose a
new SAT-based approach that replaces these global constraints with simple local
constraints that have substantially better propagation properties. Our approach
dramatically improves the scalability of both computing and enumerating common
box unfoldings: (i) while previous approaches could only find common unfoldings
of two boxes up to area 88, ours easily scales beyond 150, and (ii) while
previous approaches were only able to enumerate common unfoldings up to area
30, ours scales up to 60. This allows us to rule out 46, 54, and 58 as the
smallest areas allowing a common unfolding of three boxes, thereby refuting a
conjecture of Xu et al. (2017).

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [666] [FlexiSAGA: A Flexible Systolic Array GEMM Accelerator for Sparse and Dense Processing](https://arxiv.org/abs/2506.01566)
*Mika Markus Müller,Konstantin Lübeck,Alexander Louis-Ferdinand Jung,Jannik Steinmetz,Oliver Bringmann*

Key words: AI, DNN, 稀疏性, 硬件加速器, GEMM, 剪枝

TL;DR: FlexiSAGA 是一种可配置的 AI 硬件加速器，支持稀疏和密集数据流处理，通过特定剪枝方法实现高效 DNN 处理。其性能优于现有加速器平台。

<details>
  <summary>Details</summary>

Main category: cs.PF

Motivation: 解决 DNN 推理在边缘设备上的计算复杂度挑战，利用稀疏性提高效率。

Method: 提出 FlexiSAGA 硬件加速器及其配套的剪枝方法，支持多种数据流处理稀疏和密集 GEMM。

Result: 实现 1.41 至 4.28 倍稀疏对密集推理加速，优于商业和文献报道的平台。

Conclusion: FlexiSAGA 是一种高效且灵活的加速器，适用于资源受限设备。

Abstract: Artificial Intelligence (AI) algorithms, such as Deep Neural Networks (DNNs),
have become an important tool for a wide range of applications, from computer
vision to natural language processing. However, the computational complexity of
DNN inference poses a significant challenge, particularly for processing on
resource-constrained edge devices. One promising approach to address this
challenge is the exploitation of sparsity in DNN operator weights.
  In this work, we present FlexiSAGA, an architecturally configurable and
dataflow-flexible AI hardware accelerator for the sparse and dense processing
of general matrix multiplications (GEMMs). FlexiSAGA supports seven different
sparse and dense dataflows, enabling efficient processing of resource intensive
DNN operators. Additionally, we propose a DNN pruning method specifically
tailored towards the FlexiSAGA architecture, allowing for near-optimal
processing of dense and sparse convolution and fully-connected operators,
facilitating a DNN/HW co-design flow. Our results show a whole DNN
sparse-over-dense inference speedup ranging from 1.41 up to 4.28, outperforming
commercial and literature-reported accelerator platforms.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [667] [Detection of Endangered Deer Species Using UAV Imagery: A Comparative Study Between Efficient Deep Learning Approaches](https://arxiv.org/abs/2506.00154)
*Agustín Roca,Gastón Castro,Gabriel Torre,Leonardo J. Colombo,Ignacio Mas,Javier Pereira,Juan I. Giribet*

Key words: YOLOv11, RT-DETR, 无人机, 野生动物监测, 分割掩码

TL;DR: 本研究比较了YOLOv11和RT-DETR等先进神经网络在无人机图像中检测沼泽鹿的性能，特别关注目标占图像比例极小且被植被遮挡的场景，并通过添加精确分割掩码提升了YOLO模型的效果。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 为提高无人机在野生动物监测和保护中的准确性和可扩展性，研究探索了在目标极小且遮挡严重的情况下，如何利用先进神经网络提升检测性能。

Method: 通过扩展数据集并添加精确分割掩码，训练带有分割头的YOLO模型，优化其在复杂场景中的检测能力。

Result: 实验结果表明，加入分割头的YOLO模型显著提升了检测性能。

Conclusion: 该研究为无人机野生动物监测提供了高效且准确的AI驱动解决方案，推动了保护策略的改进。

Abstract: This study compares the performance of state-of-the-art neural networks
including variants of the YOLOv11 and RT-DETR models for detecting marsh deer
in UAV imagery, in scenarios where specimens occupy a very small portion of the
image and are occluded by vegetation. We extend previous analysis adding
precise segmentation masks for our datasets enabling a fine-grained training of
a YOLO model with a segmentation head included. Experimental results show the
effectiveness of incorporating the segmentation head achieving superior
detection performance. This work contributes valuable insights for improving
UAV-based wildlife monitoring and conservation strategies through scalable and
accurate AI-driven detection systems.

</details>


### [668] [Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes](https://arxiv.org/abs/2506.00227)
*Anthony Gosselin,Ge Ya Luo,Luis Lara,Florian Golemo,Derek Nowrouzezahrai,Liam Paull,Alexia Jolicoeur-Martineau,Christopher Pal*

Key words: 视频生成, 车祸仿真, 扩散模型, 可控性, 交通安全

TL;DR: 提出一种可控的车祸视频生成模型Ctrl-Crash，通过边界框、碰撞类型等信号生成逼真且可控的车祸场景。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有视频扩散技术难以生成逼真的车祸场景，因驾驶数据集中事故事件稀缺，需提升交通安全仿真能力。

Method: 利用边界框、碰撞类型和初始帧等信号，结合分类器无关引导实现细粒度控制的视频生成。

Result: 在视频质量指标（如FVD、JEDi）和人类评估中，性能优于现有扩散方法。

Conclusion: Ctrl-Crash能高效生成逼真且可控的车祸视频，为交通安全研究提供有力工具。

Abstract: Video diffusion techniques have advanced significantly in recent years;
however, they struggle to generate realistic imagery of car crashes due to the
scarcity of accident events in most driving datasets. Improving traffic safety
requires realistic and controllable accident simulations. To tackle the
problem, we propose Ctrl-Crash, a controllable car crash video generation model
that conditions on signals such as bounding boxes, crash types, and an initial
image frame. Our approach enables counterfactual scenario generation where
minor variations in input can lead to dramatically different crash outcomes. To
support fine-grained control at inference time, we leverage classifier-free
guidance with independently tunable scales for each conditioning signal.
Ctrl-Crash achieves state-of-the-art performance across quantitative video
quality metrics (e.g., FVD and JEDi) and qualitative measurements based on a
human-evaluation of physical realism and video quality compared to prior
diffusion-based methods.

</details>


### [669] [Latent Guidance in Diffusion Models for Perceptual Evaluations](https://arxiv.org/abs/2506.00327)
*Shreshth Saini,Ru-Ling Liao,Yan Ye,Alan C. Bovik*

Key words: 潜在扩散模型，无参考图像质量评估，感知一致性，感知流形引导，U-Net

TL;DR: 该论文提出了一种名为感知流形引导（PMG）的算法，利用预训练的潜在扩散模型和感知质量特征，在无参考图像质量评估（NR-IQA）任务中实现感知一致性，并表现出与人类感知高度相关的特征。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 尽管潜在扩散模型在高维图像生成和下游任务中取得了进展，但对其在NR-IQA任务中的感知一致性研究较少。论文假设这些模型隐式地表现出数据流形中的感知一致局部区域，并利用这一假设进行改进。

Method: 提出PMG算法，通过预训练的潜在扩散模型和感知特征，从去噪U-Net中提取多尺度和多时间步的特征图，以实现感知一致性。

Result: 实验表明，这些特征与人类感知高度相关，且在NR-IQA任务中表现出卓越的泛化能力，达到最先进的性能。

Conclusion: 该方法首次将感知特征用于引导扩散模型进行NR-IQA任务，展示了扩散模型在该领域的优越性。

Abstract: Despite recent advancements in latent diffusion models that generate
high-dimensional image data and perform various downstream tasks, there has
been little exploration into perceptual consistency within these models on the
task of No-Reference Image Quality Assessment (NR-IQA). In this paper, we
hypothesize that latent diffusion models implicitly exhibit perceptually
consistent local regions within the data manifold. We leverage this insight to
guide on-manifold sampling using perceptual features and input measurements.
Specifically, we propose Perceptual Manifold Guidance (PMG), an algorithm that
utilizes pretrained latent diffusion models and perceptual quality features to
obtain perceptually consistent multi-scale and multi-timestep feature maps from
the denoising U-Net. We empirically demonstrate that these hyperfeatures
exhibit high correlation with human perception in IQA tasks. Our method can be
applied to any existing pretrained latent diffusion model and is
straightforward to integrate. To the best of our knowledge, this paper is the
first work on guiding diffusion model with perceptual features for NR-IQA.
Extensive experiments on IQA datasets show that our method, LGDM, achieves
state-of-the-art performance, underscoring the superior generalization
capabilities of diffusion models for NR-IQA tasks.

</details>


### [670] [Parallel Rescaling: Rebalancing Consistency Guidance for Personalized Diffusion Models](https://arxiv.org/abs/2506.00607)
*JungWoo Chae,Jiyoon Kim,Sangheum Hwang*

Key words: 个性化扩散模型、并行重新缩放、提示对齐、视觉保真度

TL;DR: 提出了一种并行重新缩放技术，用于个性化扩散模型，通过分解一致性引导信号为平行和正交分量，优化生成图像的提示对齐和视觉保真度。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法在有限数据下容易过拟合，导致生成图像与文本提示不匹配，尤其是复杂或风格化提示时效果不佳。

Method: 采用并行重新缩放技术，将一致性引导信号分解为平行和正交分量，重新调整平行分量以减少对分类器自由引导的干扰。

Result: 实验表明，与基线方法相比，该方法在复杂提示下仍能保持更好的提示对齐和视觉保真度。

Conclusion: 并行重新缩放技术为扩散模型的个性化提供了更稳定和准确的解决方案。

Abstract: Personalizing diffusion models to specific users or concepts remains
challenging, particularly when only a few reference images are available.
Existing methods such as DreamBooth and Textual Inversion often overfit to
limited data, causing misalignment between generated images and text prompts
when attempting to balance identity fidelity with prompt adherence. While
Direct Consistency Optimization (DCO) with its consistency-guided sampling
partially alleviates this issue, it still struggles with complex or stylized
prompts. In this paper, we propose a parallel rescaling technique for
personalized diffusion models. Our approach explicitly decomposes the
consistency guidance signal into parallel and orthogonal components relative to
classifier free guidance (CFG). By rescaling the parallel component, we
minimize disruptive interference with CFG while preserving the subject's
identity. Unlike prior personalization methods, our technique does not require
additional training data or expensive annotations. Extensive experiments show
improved prompt alignment and visual fidelity compared to baseline methods,
even on challenging stylized prompts. These findings highlight the potential of
parallel rescaled guidance to yield more stable and accurate personalization
for diverse user inputs.

</details>


### [671] [Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive Vision-Language Pretraining](https://arxiv.org/abs/2506.00633)
*Daniele Molino,Camillo Maria Caruso,Filippo Ruffini,Paolo Soda,Valerio Guarrasi*

Key words: 3D CT生成, 视觉语言对齐, 潜在扩散模型, 医学影像合成, 数据增强

TL;DR: 本文提出了一种用于从文本生成3D CT图像的新方法，结合了潜在扩散模型与3D对比视觉语言预训练，显著提升了生成质量和临床应用价值。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 扩展文本到图像生成技术至3D CT成像领域，解决高维度和解剖复杂性的挑战，填补3D医学影像中视觉-语言对齐的空白。

Method: 采用双编码器CLIP风格模型，结合预训练的3D VAE，通过潜在扩散模型实现高效的3D去噪扩散，无需额外超分辨率处理。

Result: 在CT-RATE数据集上评估，模型在图像保真度、临床相关性和语义对齐方面表现优异，显著超越基线方法，并能有效增强下游诊断性能。

Conclusion: 模态特定的视觉-语言对齐是高质量3D医学图像生成的关键，方法为数据增强、医学教育和临床模拟提供了可扩展的解决方案。

Abstract: Objective: While recent advances in text-conditioned generative models have
enabled the synthesis of realistic medical images, progress has been largely
confined to 2D modalities such as chest X-rays. Extending text-to-image
generation to volumetric Computed Tomography (CT) remains a significant
challenge, due to its high dimensionality, anatomical complexity, and the
absence of robust frameworks that align vision-language data in 3D medical
imaging. Methods: We introduce a novel architecture for Text-to-CT generation
that combines a latent diffusion model with a 3D contrastive vision-language
pretraining scheme. Our approach leverages a dual-encoder CLIP-style model
trained on paired CT volumes and radiology reports to establish a shared
embedding space, which serves as the conditioning input for generation. CT
volumes are compressed into a low-dimensional latent space via a pretrained
volumetric VAE, enabling efficient 3D denoising diffusion without requiring
external super-resolution stages. Results: We evaluate our method on the
CT-RATE dataset and conduct a comprehensive assessment of image fidelity,
clinical relevance, and semantic alignment. Our model achieves competitive
performance across all tasks, significantly outperforming prior baselines for
text-to-CT generation. Moreover, we demonstrate that CT scans synthesized by
our framework can effectively augment real data, improving downstream
diagnostic performance. Conclusion: Our results show that modality-specific
vision-language alignment is a key component for high-quality 3D medical image
generation. By integrating contrastive pretraining and volumetric diffusion,
our method offers a scalable and controllable solution for synthesizing
clinically meaningful CT volumes from text, paving the way for new applications
in data augmentation, medical education, and automated clinical simulation.

</details>


### [672] [From Local Cues to Global Percepts: Emergent Gestalt Organization in Self-Supervised Vision Models](https://arxiv.org/abs/2506.00718)
*Tianqin Li,Ziqi Wen,Leiran Song,Jun Liu,Zhi Jing,Tai Sing Lee*

Key words: Gestalt principles, Vision Transformers, Masked Autoencoding, DiSRT, global perception

TL;DR: 该论文研究现代视觉模型是否表现出类似格式塔原则的行为，并探讨训练条件如何影响这些行为。研究发现，使用MAE训练的ViTs表现出与格式塔法则一致的激活模式，并提出DiSRT测试平台评估全局结构的敏感性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探讨现代视觉模型是否能像人类视觉系统一样利用格式塔原则（如闭合性、邻近性）组织局部信息，以及训练条件如何影响这种能力。

Method: 通过Distorted Spatial Relationship Testbench (DiSRT)测试模型对全局空间扰动的敏感性，研究了MAE-trained ViTs和ConvNeXt等模型的性能。

Result: 自监督模型（如MAE、CLIP）在DiSRT测试中优于监督基准，甚至在某些情况下超过人类表现。分类微调会削弱模型对全局结构的敏感性。

Conclusion: 研究发现特定训练条件（如自监督学习）能促进格式塔类似行为，而分类任务会抑制这种能力。DiSRT可作为评估模型全局结构敏感性的诊断工具。

Abstract: Human vision organizes local cues into coherent global forms using Gestalt
principles like closure, proximity, and figure-ground assignment -- functions
reliant on global spatial structure. We investigate whether modern vision
models show similar behaviors, and under what training conditions these emerge.
We find that Vision Transformers (ViTs) trained with Masked Autoencoding (MAE)
exhibit activation patterns consistent with Gestalt laws, including illusory
contour completion, convexity preference, and dynamic figure-ground
segregation. To probe the computational basis, we hypothesize that modeling
global dependencies is necessary for Gestalt-like organization. We introduce
the Distorted Spatial Relationship Testbench (DiSRT), which evaluates
sensitivity to global spatial perturbations while preserving local textures.
Using DiSRT, we show that self-supervised models (e.g., MAE, CLIP) outperform
supervised baselines and sometimes even exceed human performance. ConvNeXt
models trained with MAE also exhibit Gestalt-compatible representations,
suggesting such sensitivity can arise without attention architectures. However,
classification finetuning degrades this ability. Inspired by biological vision,
we show that a Top-K activation sparsity mechanism can restore global
sensitivity. Our findings identify training conditions that promote or suppress
Gestalt-like perception and establish DiSRT as a diagnostic for global
structure sensitivity across models.

</details>


### [673] [ArtiScene: Language-Driven Artistic 3D Scene Generation Through Image Intermediary](https://arxiv.org/abs/2506.00742)
*Zeqi Gu,Yin Cui,Zhaoshuo Li,Fangyin Wei,Yunhao Ge,Jinwei Gu,Ming-Yu Liu,Abe Davis,Yifan Ding*

Key words: 3D场景设计, 文本到3D, 2D图像引导, ArtiScene, 无训练方法

TL;DR: ArtiScene利用文本生成2D图像，再从中提取3D信息，实现无需训练的3D场景设计，性能优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统3D场景设计复杂且依赖高质量3D数据，而文本到图像的模型能生成多样且可靠的2D布局，启发通过2D图像指导3D合成。

Method: 先生成2D场景图像，从中提取物体形状和外观信息，结合几何、位置和姿态构建3D场景。

Result: ArtiScene在布局和美学质量上大幅领先基准方法，用户研究中获胜率74.89%，GPT-4o评估中达95.07%。

Conclusion: 利用2D图像作为中介的3D场景设计方法高效且通用，显著降低了对3D数据的依赖。

Abstract: Designing 3D scenes is traditionally a challenging task that demands both
artistic expertise and proficiency with complex software. Recent advances in
text-to-3D generation have greatly simplified this process by letting users
create scenes based on simple text descriptions. However, as these methods
generally require extra training or in-context learning, their performance is
often hindered by the limited availability of high-quality 3D data. In
contrast, modern text-to-image models learned from web-scale images can
generate scenes with diverse, reliable spatial layouts and consistent, visually
appealing styles. Our key insight is that instead of learning directly from 3D
scenes, we can leverage generated 2D images as an intermediary to guide 3D
synthesis. In light of this, we introduce ArtiScene, a training-free automated
pipeline for scene design that integrates the flexibility of free-form
text-to-image generation with the diversity and reliability of 2D intermediary
layouts.
  First, we generate 2D images from a scene description, then extract the shape
and appearance of objects to create 3D models. These models are assembled into
the final scene using geometry, position, and pose information derived from the
same intermediary image. Being generalizable to a wide range of scenes and
styles, ArtiScene outperforms state-of-the-art benchmarks by a large margin in
layout and aesthetic quality by quantitative metrics. It also averages a 74.89%
winning rate in extensive user studies and 95.07% in GPT-4o evaluation. Project
page: https://artiscene-cvpr.github.io/

</details>


### [674] [ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping for Natural Disaster Damage Assessment](https://arxiv.org/abs/2506.00238)
*Ehsan Karimi,Maryam Rahnemoonfar*

Key words: 视觉问答（VQA）、零样本学习、视觉语言模型（VLM）、灾害管理、开放性问题

TL;DR: 提出了一种基于视觉语言模型（VLM）的零样本视觉问答方法（ZeShot-VQA），用于灾害管理中的开放性问题回答，无需微调即可适应新数据集。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 自然灾害通常影响广泛，及时高效的数据驱动响应至关重要。现有的VQA模型无法回答开放性问题，且需要重新训练以适应新答案，耗时耗力。

Method: 利用大规模视觉语言模型（VLMs）的零样本学习能力，提出ZeShot-VQA方法，无需微调即可在新数据集上应用。

Result: 在FloodNet数据集上验证了ZeShot-VQA的性能，能处理未见过的答案，展现灵活性。

Conclusion: ZeShot-VQA为灾害管理提供了高效、灵活的开放性问题解决方案。

Abstract: Natural disasters usually affect vast areas and devastate infrastructures.
Performing a timely and efficient response is crucial to minimize the impact on
affected communities, and data-driven approaches are the best choice. Visual
question answering (VQA) models help management teams to achieve in-depth
understanding of damages. However, recently published models do not possess the
ability to answer open-ended questions and only select the best answer among a
predefined list of answers. If we want to ask questions with new additional
possible answers that do not exist in the predefined list, the model needs to
be fin-tuned/retrained on a new collected and annotated dataset, which is a
time-consuming procedure. In recent years, large-scale Vision-Language Models
(VLMs) have earned significant attention. These models are trained on extensive
datasets and demonstrate strong performance on both unimodal and multimodal
vision/language downstream tasks, often without the need for fine-tuning. In
this paper, we propose a VLM-based zero-shot VQA (ZeShot-VQA) method, and
investigate the performance of on post-disaster FloodNet dataset. Since the
proposed method takes advantage of zero-shot learning, it can be applied on new
datasets without fine-tuning. In addition, ZeShot-VQA is able to process and
generate answers that has been not seen during the training procedure, which
demonstrates its flexibility.

</details>


### [675] [L3A: Label-Augmented Analytic Adaptation for Multi-Label Class Incremental Learning](https://arxiv.org/abs/2506.00816)
*Xiang Zhang,Run He,Jiao Chen,Di Fang,Ming Li,Ziqian Zeng,Cen Chen,Huiping Zhuang*

Key words: 多标签增量学习, 标签缺失, 类别不平衡, L3A

TL;DR: 论文提出了一种名为L3A的方法，解决多标签增量学习中的标签缺失和类别不平衡问题，通过伪标签模块和加权分析分类器提升性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 多标签增量学习（MLCIL）在真实场景中面临标签缺失和类别不平衡的挑战，传统方法难以解决。

Method: L3A包含伪标签（PL）模块生成伪标签以解决标签缺失问题，以及加权分析分类器（WAC）通过闭式解和样本权重缓解类别不平衡。

Result: 在MS-COCO和PASCAL VOC数据集上的实验表明，L3A优于现有方法。

Conclusion: L3A是一种无需存储历史样本的MLCIL解决方案，有效解决了标签缺失和类别不平衡问题。

Abstract: Class-incremental learning (CIL) enables models to learn new classes
continually without forgetting previously acquired knowledge. Multi-label CIL
(MLCIL) extends CIL to a real-world scenario where each sample may belong to
multiple classes, introducing several challenges: label absence, which leads to
incomplete historical information due to missing labels, and class imbalance,
which results in the model bias toward majority classes. To address these
challenges, we propose Label-Augmented Analytic Adaptation (L3A), an
exemplar-free approach without storing past samples. L3A integrates two key
modules. The pseudo-label (PL) module implements label augmentation by
generating pseudo-labels for current phase samples, addressing the label
absence problem. The weighted analytic classifier (WAC) derives a closed-form
solution for neural networks. It introduces sample-specific weights to
adaptively balance the class contribution and mitigate class imbalance.
Experiments on MS-COCO and PASCAL VOC datasets demonstrate that L3A outperforms
existing methods in MLCIL tasks. Our code is available at
https://github.com/scut-zx/L3A.

</details>


### [676] [Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware Sign Language Translation](https://arxiv.org/abs/2506.00129)
*Edward Fish,Richard Bowden*

Key words: 手语翻译, 双曲几何, 骨骼表示, Poincaré球模型, 几何对比损失

TL;DR: 本文提出了一种利用双曲几何改进手语翻译中骨骼表示的方法（Geo-Sign），通过双曲空间中的投影和对比损失增强表示，提高了翻译性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 尽管现有研究主要关注大语言模型的表示能力，但本文探索了通过改进骨骼表示的几何特性来提升手语翻译效果的新方向。

Method: 提出Geo-Sign方法，将骨骼特征投影到双曲空间（Poincaré球模型），结合双曲投影层、加权Fréchet均值聚合和几何对比损失，作为一个正则化函数集成到端到端翻译框架中。

Result: 实验表明，双曲几何能有效改进骨骼表示，超越现有RGB方法，同时保护隐私并提高计算效率。

Conclusion: 双曲几何在手语翻译中具有潜力，能生成更具区分性的骨骼嵌入，特别是对细粒度动作（如手指关节）。

Abstract: Recent progress in Sign Language Translation (SLT) has focussed primarily on
improving the representational capacity of large language models to incorporate
Sign Language features. This work explores an alternative direction: enhancing
the geometric properties of skeletal representations themselves. We propose
Geo-Sign, a method that leverages the properties of hyperbolic geometry to
model the hierarchical structure inherent in sign language kinematics. By
projecting skeletal features derived from Spatio-Temporal Graph Convolutional
Networks (ST-GCNs) into the Poincar\'e ball model, we aim to create more
discriminative embeddings, particularly for fine-grained motions like finger
articulations. We introduce a hyperbolic projection layer, a weighted Fr\'echet
mean aggregation scheme, and a geometric contrastive loss operating directly in
hyperbolic space. These components are integrated into an end-to-end
translation framework as a regularisation function, to enhance the
representations within the language model. This work demonstrates the potential
of hyperbolic geometry to improve skeletal representations for Sign Language
Translation, improving on SOTA RGB methods while preserving privacy and
improving computational efficiency. Code available here:
https://github.com/ed-fish/geo-sign.

</details>


### [677] [Towards Predicting Any Human Trajectory In Context](https://arxiv.org/abs/2506.00871)
*Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Key words: 行人轨迹预测、上下文学习、示例选择、合成数据

TL;DR: 引入TrajICL框架，通过上下文学习无需微调实现行人轨迹预测的快速适应，提出基于时空相似性和预测引导的示例选择方法，并在合成数据集上训练以提高性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决边缘设备计算资源受限下行人轨迹预测的适应性问题，避免依赖场景特定数据的微调。

Method: 提出STES和PG-ES方法选择相似轨迹示例，利用合成数据集训练模型。

Result: TrajICL在跨域和域内场景中表现优异，超越微调方法。

Conclusion: TrajICL展示出强大的适应能力，是行人轨迹预测的高效解决方案。

Abstract: Predicting accurate future trajectories of pedestrians is essential for
autonomous systems but remains a challenging task due to the need for
adaptability in different environments and domains. A common approach involves
collecting scenario-specific data and performing fine-tuning via
backpropagation. However, this process is often impractical on edge devices due
to constrained computational resources. To address this challenge, we introduce
TrajICL, an In-Context Learning (ICL) framework for pedestrian trajectory
prediction that enables rapid adaptation without fine-tuning on the
scenario-specific data. We propose a spatio-temporal similarity-based example
selection (STES) method that selects relevant examples from previously observed
trajectories within the same scene by identifying similar motion patterns at
corresponding locations. To further refine this selection, we introduce
prediction-guided example selection (PG-ES), which selects examples based on
both the past trajectory and the predicted future trajectory, rather than
relying solely on the past trajectory. This approach allows the model to
account for long-term dynamics when selecting examples. Finally, instead of
relying on small real-world datasets with limited scenario diversity, we train
our model on a large-scale synthetic dataset to enhance its prediction ability
by leveraging in-context examples. Extensive experiments demonstrate that
TrajICL achieves remarkable adaptation across both in-domain and cross-domain
scenarios, outperforming even fine-tuned approaches across multiple public
benchmarks. The code will be released at
https://fujiry0.github.io/TrajICL-project-page.

</details>


### [678] [Uneven Event Modeling for Partially Relevant Video Retrieval](https://arxiv.org/abs/2506.00891)
*Sa Zhu,Huashan Chen,Wanqian Zhang,Jinchao Zhang,Zexian Yang,Xiaoshuai Hao,Bo Li*

Key words: 部分相关视频检索,事件建模,文本-视频对齐,PGVS,CAER

TL;DR: 本文提出了一种名为UEM的框架，用于部分相关视频检索(PRVR)。通过PGVS模块实现清晰的事件边界划分，以及CAER模块优化事件表示，提升了文本-视频对齐的精准度，并在实验中取得了最佳性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法通常将视频分割为固定数量的等长片段，导致事件边界模糊，且依赖平均池化计算事件表示，易引入不对齐问题。本文旨在解决这些问题。

Method: 提出了UEM框架，包括PGVS模块通过迭代方式结合时间依赖性和语义相似性划分事件边界，以及CAER模块利用文本的交叉注意力优化事件表示。

Result: 实验表明，该方法在两个PRVR基准测试中达到了最先进的性能。

Conclusion: UEM框架通过改进事件建模和表示，显著提升了PRVR任务的性能。

Abstract: Given a text query, partially relevant video retrieval (PRVR) aims to
retrieve untrimmed videos containing relevant moments, wherein event modeling
is crucial for partitioning the video into smaller temporal events that
partially correspond to the text. Previous methods typically segment videos
into a fixed number of equal-length clips, resulting in ambiguous event
boundaries. Additionally, they rely on mean pooling to compute event
representations, inevitably introducing undesired misalignment. To address
these, we propose an Uneven Event Modeling (UEM) framework for PRVR. We first
introduce the Progressive-Grouped Video Segmentation (PGVS) module, to
iteratively formulate events in light of both temporal dependencies and
semantic similarity between consecutive frames, enabling clear event
boundaries. Furthermore, we also propose the Context-Aware Event Refinement
(CAER) module to refine the event representation conditioned the text's
cross-attention. This enables event representations to focus on the most
relevant frames for a given text, facilitating more precise text-video
alignment. Extensive experiments demonstrate that our method achieves
state-of-the-art performance on two PRVR benchmarks.

</details>


### [679] [HSCR: Hierarchical Self-Contrastive Rewarding for Aligning Medical Vision Language Models](https://arxiv.org/abs/2506.00805)
*Songtao Jiang,Yan Zhang,Yeying Jin,Zhihang Tang,Yangyang Wu,Yang Feng,Jian Wu,Zuozhu Liu*

Key words: Med-VLMs, 模态不对齐, HSCR, 自对比奖励, 多级偏好优化

TL;DR: 论文提出了一种名为HSCR的新方法，用于解决医学视觉语言模型（Med-VLMs）中的模态不对齐问题，通过自对比奖励和多级偏好优化提升模型的对齐性和可信度。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有医学视觉语言模型在临床环境中可能因模态不对齐问题产生不可靠的响应，因此需要一种高效且精确的模型对齐方法。

Method: HSCR通过生成高质量偏好数据和多级偏好优化策略，利用视觉标记丢失分析和隐式对齐奖励函数指导解码过程。

Result: 实验表明，HSCR在多种医学任务（如Med-VQA、医学图像描述和指令遵循）中显著提升了零样本性能和模态对齐性，仅需2000个训练条目。

Conclusion: HSCR是一种高效且精确的方法，能够显著提升医学视觉语言模型的对齐性和可信度。

Abstract: Medical Vision-Language Models (Med-VLMs) have achieved success across
various tasks, yet most existing methods overlook the modality misalignment
issue that can lead to untrustworthy responses in clinical settings. In this
paper, we propose Hierarchical Self-Contrastive Rewarding (HSCR), a novel
approach that addresses two critical challenges in Med-VLM alignment: 1)
Cost-effective generation of high-quality preference data; 2) Capturing nuanced
and context-aware preferences for improved alignment. HSCR first leverages the
inherent capability of Med-VLMs to generate dispreferred responses with higher
sampling probability. By analyzing output logit shifts after visual token
dropout, we identify modality-coupled tokens that induce misalignment and
derive an implicit alignment reward function. This function guides token
replacement with hallucinated ones during decoding, producing high-quality
dispreferred data. Furthermore, HSCR introduces a multi-level preference
optimization strategy, which extends beyond traditional adjacent-level
optimization by incorporating nuanced implicit preferences, leveraging relative
quality in dispreferred data to capture subtle alignment cues for more precise
and context-aware optimization. Extensive experiments across multiple medical
tasks, including Med-VQA, medical image captioning and instruction following,
demonstrate that HSCR not only enhances zero-shot performance but also
significantly improves modality alignment and trustworthiness with just 2,000
training entries.

</details>


### [680] [Deep Temporal Reasoning in Video Language Models: A Cross-Linguistic Evaluation of Action Duration and Completion through Perfect Times](https://arxiv.org/abs/2506.00928)
*Olga Loginova,Sofía Ortega Loguinova*

Key words: 时间推理，视频-语言模型，多模态学习，动作完成性，Perfect Times数据集

TL;DR: 该研究提出了一个名为Perfect Times的新型多语言数据集，用于评估视频-语言模型在时间推理上的表现，发现现有模型难以模拟人类对动作完成性和持续性的理解，强调了整合多模态线索的重要性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究动机在于探索人类如何通过语言和视觉线索区分已完成和持续的动作，并评估当前视频-语言模型是否能够模拟这种时间推理能力。

Method: 方法包括构建一个多语言（英语、意大利语、俄语、日语）的多选题数据集Perfect Times，结合日常活动视频和动作完成性标签，并设计特定干扰项来测试模型的时间推理能力。

Result: 实验结果显示，尽管当前最先进的模型在文本任务上表现良好，但在视频中模拟人类时间和因果推理方面表现不佳。

Conclusion: 研究结论强调了整合多模态线索对于捕捉动作持续性和完成性细微差别的重要性，并提出了评估时间推理的新标准。

Abstract: Human perception of events is intrinsically tied to distinguishing between
completed (perfect and telic) and ongoing (durative) actions, a process
mediated by both linguistic structure and visual cues. In this work, we
introduce the \textbf{Perfect Times} dataset, a novel, quadrilingual (English,
Italian, Russian, and Japanese) multiple-choice question-answering benchmark
designed to assess video-language models (VLMs) on temporal reasoning. By
pairing everyday activity videos with event completion labels and
perfectivity-tailored distractors, our dataset probes whether models truly
comprehend temporal dynamics or merely latch onto superficial markers.
Experimental results indicate that state-of-the-art models, despite their
success on text-based tasks, struggle to mirror human-like temporal and causal
reasoning grounded in video. This study underscores the necessity of
integrating deep multimodal cues to capture the nuances of action duration and
completion within temporal and causal video dynamics, setting a new standard
for evaluating and advancing temporal reasoning in VLMs.

</details>


### [681] [IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection](https://arxiv.org/abs/2506.00979)
*Wayne Zhang,Changjiang Jiang,Zhonghao Zhang,Chenyang Si,Fengchang Yu,Wei Peng*

Key words: AIGC检测,多模态,可解释性,统一框架,数据集

TL;DR: 该论文提出了一种新的统一、大规模数据集IVY-FAKE和可解释的AIGC检测框架IVY-XDETECTOR，用于解决当前方法缺乏解释性和多模态统一检测的问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前AIGC检测方法多为黑盒二元分类器，缺乏解释性且不支持多模态统一检测，影响了模型的透明度和可信度。

Method: 作者构建了包含丰富注释的IVY-FAKE数据集，并提出了基于视觉-语言模型的IVY-XDETECTOR，实现图像和视频的统一检测与解释。

Result: IVY-XDETECTOR在多个检测基准上取得了最先进的性能，验证了其数据集和建模框架的有效性。

Conclusion: 通过IVY-FAKE和IVY-XDETECTOR，论文为AIGC检测提供了可解释且统一的多模态解决方案，推动了实际应用。

Abstract: The rapid advancement of Artificial Intelligence Generated Content (AIGC) in
visual domains has resulted in highly realistic synthetic images and videos,
driven by sophisticated generative frameworks such as diffusion-based
architectures. While these breakthroughs open substantial opportunities, they
simultaneously raise critical concerns about content authenticity and
integrity. Many current AIGC detection methods operate as black-box binary
classifiers, which offer limited interpretability, and no approach supports
detecting both images and videos in a unified framework. This dual limitation
compromises model transparency, reduces trustworthiness, and hinders practical
deployment. To address these challenges, we introduce IVY-FAKE , a novel,
unified, and large-scale dataset specifically designed for explainable
multimodal AIGC detection. Unlike prior benchmarks, which suffer from
fragmented modality coverage and sparse annotations, IVY-FAKE contains over
150,000 richly annotated training samples (images and videos) and 18,700
evaluation examples, each accompanied by detailed natural-language reasoning
beyond simple binary labels. Building on this, we propose Ivy Explainable
Detector (IVY-XDETECTOR), a unified AIGC detection and explainable architecture
that jointly performs explainable detection for both image and video content.
Our unified vision-language model achieves state-of-the-art performance across
multiple image and video detection benchmarks, highlighting the significant
advancements enabled by our dataset and modeling framework. Our data is
publicly available at https://huggingface.co/datasets/AI-Safeguard/Ivy-Fake.

</details>


### [682] [Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New Perspective for MLLM Evaluation](https://arxiv.org/abs/2506.01293)
*Yichi Zhang,Zhuo Chen,Lingbing Guo,Yajing Xu,Min Zhang,Wen Zhang,Huajun Chen*

Key words: MLLMs, 多模态知识图谱, 结构化理解, 评估基准

TL;DR: 该论文提出了一个新颖的评估范式M3STR，旨在通过多模态知识图谱测试MLLMs对结构化知识的视觉理解能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的MLLMs评估基准忽视了它们在理解视觉形式的结构化抽象知识方面的能力。

Method: 设计了基于多模态地图的M3STR基准，利用多模态知识图谱生成包含多模态实体的子图架构图像。

Result: 对26个先进MLLMs的评估显示，它们在处理结构化知识的抽象视觉信息方面仍存在不足。

Conclusion: M3STR为提升MLLMs的整体推理能力提供了关键方向。

Abstract: Multi-modal large language models (MLLMs) incorporate heterogeneous
modalities into LLMs, enabling a comprehensive understanding of diverse
scenarios and objects. Despite the proliferation of evaluation benchmarks and
leaderboards for MLLMs, they predominantly overlook the critical capacity of
MLLMs to comprehend world knowledge with structured abstractions that appear in
visual form. To address this gap, we propose a novel evaluation paradigm and
devise M3STR, an innovative benchmark grounded in the Multi-Modal Map for
STRuctured understanding. This benchmark leverages multi-modal knowledge graphs
to synthesize images encapsulating subgraph architectures enriched with
multi-modal entities. M3STR necessitates that MLLMs not only recognize the
multi-modal entities within the visual inputs but also decipher intricate
relational topologies among them. We delineate the benchmark's statistical
profiles and automated construction pipeline, accompanied by an extensive
empirical analysis of 26 state-of-the-art MLLMs. Our findings reveal persistent
deficiencies in processing abstractive visual information with structured
knowledge, thereby charting a pivotal trajectory for advancing MLLMs' holistic
reasoning capacities. Our code and data are released at
https://github.com/zjukg/M3STR

</details>


### [683] [Quotient Network -- A Network Similar to ResNet but Learning Quotients](https://arxiv.org/abs/2506.00992)
*Peng Hui,Jiamuyang Zhao,Changxin Li,Qingzhen Zhu*

Key words: ResNet, 商网络, 深度学**习, CIFAR10, CIFAR100, SVHN

TL;DR: 提出了一种基于残差网络改进的商网络，通过学习目标特征与现有特征的商解决了残差网络对特征大小敏感的问题，实验证明其性能优于ResNet。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 残差网络（ResNet）通过学**习特征的差异来训练深度网络，但差异的绝对性使其对特征大小敏感，缺乏独立意义。本文旨在解决这些问题。

Method: 提出商网络（Quotient Network），通过学**习目标特征与现有特征的商，并结合设计规则以实现高效训练和更高性能。

Result: 在CIFAR10、CIFAR100和SVHN数据集上，商网络通过微小调整即可稳定超越ResNet性能，且未增加额外参数。

Conclusion: 商网络成功解决了ResNet的问题，保留了其优势，并通过实验验证了其性能提升的有效性。

Abstract: The emergence of ResNet provides a powerful tool for training extremely deep
networks. The core idea behind it is to change the learning goals of the
network. It no longer learns new features from scratch but learns the
difference between the target and existing features. However, the difference
between the two kinds of features does not have an independent and clear
meaning, and the amount of learning is based on the absolute rather than the
relative difference, which is sensitive to the size of existing features. We
propose a new network that perfectly solves these two problems while still
having the advantages of ResNet. Specifically, it chooses to learn the quotient
of the target features with the existing features, so we call it the quotient
network. In order to enable this network to learn successfully and achieve
higher performance, we propose some design rules for this network so that it
can be trained efficiently and achieve better performance than ResNet.
Experiments on the CIFAR10, CIFAR100, and SVHN datasets prove that this network
can stably achieve considerable improvements over ResNet by simply making tiny
corresponding changes to the original ResNet network without adding new
parameters.

</details>


### [684] [Motion-Aware Concept Alignment for Consistent Video Editing](https://arxiv.org/abs/2506.01004)
*Tong Zhang,Juan C Leon Alcazar,Bernard Ghanem*

Key words: MoCA-Video, motion-aware, concept alignment, video synthesis, denoising schedule, temporal coherence

TL;DR: MoCA-Video是一种无需训练的框架，通过图像域语义混合与视频的结合，实现视频中特定对象的语义特征注入，同时保持原始运动和视觉上下文。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 弥合图像域语义混合与视频之间的差距，实现在视频中对特定对象的语义特征注入。

Method: 利用对角去噪调度和类无关分割在潜在空间中检测和跟踪对象，结合基于动量的语义校正和伽马残差噪声稳定化确保时间连贯性。

Result: 在SSIM、LPIPS和CASS等指标上优于现有基线，实现了更高的空间一致性和时间连贯性。

Conclusion: MoCA-Video通过结构化操作扩散噪声轨迹，实现了可控且高质量的视频合成。

Abstract: We introduce MoCA-Video (Motion-Aware Concept Alignment in Video), a
training-free framework bridging the gap between image-domain semantic mixing
and video. Given a generated video and a user-provided reference image,
MoCA-Video injects the semantic features of the reference image into a specific
object within the video, while preserving the original motion and visual
context. Our approach leverages a diagonal denoising schedule and
class-agnostic segmentation to detect and track objects in the latent space and
precisely control the spatial location of the blended objects. To ensure
temporal coherence, we incorporate momentum-based semantic corrections and
gamma residual noise stabilization for smooth frame transitions. We evaluate
MoCA's performance using the standard SSIM, image-level LPIPS, temporal LPIPS,
and introduce a novel metric CASS (Conceptual Alignment Shift Score) to
evaluate the consistency and effectiveness of the visual shifts between the
source prompt and the modified video frames. Using self-constructed dataset,
MoCA-Video outperforms current baselines, achieving superior spatial
consistency, coherent motion, and a significantly higher CASS score, despite
having no training or fine-tuning. MoCA-Video demonstrates that structured
manipulation in the diffusion noise trajectory allows for controllable,
high-quality video synthesis.

</details>


### [685] [Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models](https://arxiv.org/abs/2506.01413)
*Yulei Qin,Gang Li,Zongyi Li,Zihan Xu,Yuchen Shi,Zhekai Lin,Xiao Cui,Ke Li,Xing Sun*

Key words: 大语言模型,复杂指令,思维链,强化学习,推理能力

TL;DR: 论文提出了一种通过强化学习激励推理的方法，以解决现有语言模型在复杂指令遵循中的表现问题，并显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的大语言模型在处理复杂指令时表现不佳，尤其是当指令包含并行、链式和分支结构的多重约束时。传统的“思维链”方法因其浅层推理模式反而导致性能下降。

Method: 采用强化学习结合可验证的规则驱动奖励信号，通过分解复杂指令并设计对比样本来提升推理能力，同时利用专家行为克隆实现模型性能的稳定迁移。

Result: 在七个综合基准测试中，提出的方法使1.5B参数的语言模型性能提升11.74%，接近8B参数模型的表现。

Conclusion: 该方法有效解决了语言模型在复杂指令遵循中的浅层推理问题，显著提升了性能。

Abstract: Existing large language models (LLMs) face challenges of following complex
instructions, especially when multiple constraints are present and organized in
paralleling, chaining, and branching structures. One intuitive solution, namely
chain-of-thought (CoT), is expected to universally improve capabilities of
LLMs. However, we find that the vanilla CoT exerts a negative impact on
performance due to its superficial reasoning pattern of simply paraphrasing the
instructions. It fails to peel back the compositions of constraints for
identifying their relationship across hierarchies of types and dimensions. To
this end, we propose a systematic method to boost LLMs in dealing with complex
instructions via incentivizing reasoning for test-time compute scaling. First,
we stem from the decomposition of complex instructions under existing
taxonomies and propose a reproducible data acquisition method. Second, we
exploit reinforcement learning (RL) with verifiable rule-centric reward signals
to cultivate reasoning specifically for instruction following. We address the
shallow, non-essential nature of reasoning under complex instructions via
sample-wise contrast for superior CoT enforcement. We also exploit behavior
cloning of experts to facilitate steady distribution shift from fast-thinking
LLMs to skillful reasoners. Extensive evaluations on seven comprehensive
benchmarks confirm the validity of the proposed method, where a 1.5B LLM
achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data
are available at https://github.com/yuleiqin/RAIF.

</details>


### [686] [Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs](https://arxiv.org/abs/2506.01064)
*Yudong Zhang,Ruobing Xie,Yiqing Huang,Jiansheng Chen,Xingwu Sun,Zhanhui Kang,Di Wang,Yu Wang*

Key words: 视觉语言模型, 对抗攻击, 净化框架, 跨模态注意力

TL;DR: 论文提出了一种名为F3的新对抗净化框架，通过引入简单扰动来缓解对抗样本的危害，显著提升了模型的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的视觉语言模型对视觉对抗攻击仍显脆弱，缺乏有效的对抗样本净化方法，F3旨在解决这一问题。

Method: F3采用‘以火攻火’策略，故意引入随机扰动，利用跨模态注意力机制作为参考目标，从而净化对抗样本。

Result: F3在无需训练的情况下实现了高效净化，计算效率优于现有方法。

Conclusion: F3是一种简单、高效且适合大规模工业应用的对抗净化解决方案。

Abstract: Recent advances in large vision-language models (LVLMs) have showcased their
remarkable capabilities across a wide range of multimodal vision-language
tasks. However, these models remain vulnerable to visual adversarial attacks,
which can substantially compromise their performance. Despite their potential
impact, the development of effective methods for purifying such adversarial
examples has received relatively limited attention. In this paper, we introduce
F3, a novel adversarial purification framework that employs a counterintuitive
"fighting fire with fire" strategy: intentionally introducing simple
perturbations to adversarial examples to mitigate their harmful effects.
Specifically, F3 leverages cross-modal attentions derived from randomly
perturbed adversary examples as reference targets. By injecting noise into
these adversarial examples, F3 effectively refines their attention, resulting
in cleaner and more reliable model outputs. Remarkably, this seemingly
paradoxical approach of employing noise to counteract adversarial attacks
yields impressive purification results. Furthermore, F3 offers several distinct
advantages: it is training-free and straightforward to implement, and exhibits
significant computational efficiency improvements compared to existing
purification methods. These attributes render F3 particularly suitable for
large-scale industrial applications where both robust performance and
operational efficiency are critical priorities. The code will be made publicly
available.

</details>


### [687] [EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation](https://arxiv.org/abs/2506.01551)
*Bingqian Lin,Yunshuang Nie,Khun Loun Zai,Ziming Wei,Mingfei Han,Rongtao Xu,Minzhe Niu,Jianhua Han,Liang Lin,Cewu Lu,Xiaodan Liang*

Key words: 视觉语言导航、链式思维、大型语言模型、自改进框架、多模态推理

TL;DR: 提出了一种名为EvolveNav的自改进框架，通过分阶段训练和自反思机制提升基于LLM的视觉语言导航任务的性能和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决现有VLN任务中直接输入-输出映射方法难以学习且决策不可解释的问题，通过引入链式思维（CoT）训练提高导航决策的准确性和可解释性。

Method: 采用两阶段框架：1) 使用格式化CoT标签进行监督微调；2) 通过自反思后训练，用模型自身推理输出作为自增强CoT标签，并引入自反思辅助任务。

Result: 在主流VLN基准测试中，EvolveNav优于现有的基于LLM的VLN方法。

Conclusion: EvolveNav通过分阶段训练和自反思机制显著提升了导航任务的表现和可解释性，未来可扩展至更复杂的多模态推理任务。

Abstract: Building Vision-Language Navigation (VLN) agents which can navigate following
natural language instructions is a long-standing goal in human-robot
interaction applications. Recent studies have revealed the potential of
training open-source Large Language Models (LLMs) to unleash LLMs' reasoning
ability for improving navigation, and simultaneously mitigate the domain gap
between LLMs' training corpus and the VLN task. However, these approaches
primarily adopt direct input-output mapping paradigms, causing the mapping
learning difficult and the navigational decisions unexplainable.
Chain-of-Thought (CoT) training is a promising way to improve both navigational
decision accuracy and interpretability, while the complexity of the navigation
task makes the perfect CoT labels unavailable and may lead to overfitting
through pure CoT supervised fine-tuning. In this paper, we propose a novel
sElf-improving embodied reasoning framework for boosting LLM-based
vision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two
stages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model
with formalized CoT labels to both activate the model's navigational reasoning
capabilities and increase the reasoning speed; (2) Self-Reflective
Post-Training, where the model is iteratively trained with its own reasoning
outputs as self-enriched CoT labels to enhance the supervision diversity. A
self-reflective auxiliary task is also introduced to encourage learning correct
reasoning patterns by contrasting with wrong ones. Experimental results on the
popular VLN benchmarks demonstrate the superiority of EvolveNav over previous
LLM-based VLN approaches. Code is available at
https://github.com/expectorlin/EvolveNav.

</details>


### [688] [Revolutionizing Blood Banks: AI-Driven Fingerprint-Blood Group Correlation for Enhanced Safety](https://arxiv.org/abs/2506.01069)
*Malik A. Altayar,Muhyeeddin Alqaraleh,Mowafaq Salem Alzboon,Wesam T. Almagharbeh*

Key words: 指纹模式, ABO血型, 生物识别, 多模态系统, 统计检验

TL;DR: 研究了指纹模式与ABO血型之间的关系，发现两者相关性较弱，指纹模式对血型的个人识别无显著帮助。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探索指纹模式与血型的关联，作为生物识别工具的补充，以提高个人识别的准确性。

Method: 对200名受试者的指纹类型（环、涡、弓）和血型进行统计比较，使用卡方检验和皮尔逊相关分析。

Result: 未发现不同血型间指纹模式的显著差异，血型数据对指纹识别的补充作用有限。

Conclusion: 血型数据与指纹模式结合对个人识别提升不明显，未来需多模态生物识别和更大样本研究。

Abstract: Identification of a person is central in forensic science, security, and
healthcare. Methods such as iris scanning and genomic profiling are more
accurate but expensive, time-consuming, and more difficult to implement. This
study focuses on the relationship between the fingerprint patterns and the ABO
blood group as a biometric identification tool. A total of 200 subjects were
included in the study, and fingerprint types (loops, whorls, and arches) and
blood groups were compared. Associations were evaluated with statistical tests,
including chi-square and Pearson correlation. The study found that the loops
were the most common fingerprint pattern and the O+ blood group was the most
prevalent. Even though there was some associative pattern, there was no
statistically significant difference in the fingerprint patterns of different
blood groups. Overall, the results indicate that blood group data do not
significantly improve personal identification when used in conjunction with
fingerprinting. Although the study shows weak correlation, it may emphasize the
efforts of multi-modal based biometric systems in enhancing the current
biometric systems. Future studies may focus on larger and more diverse samples,
and possibly machine learning and additional biometrics to improve
identification methods. This study addresses an element of the ever-changing
nature of the fields of forensic science and biometric identification,
highlighting the importance of resilient analytical methods for personal
identification.

</details>


### [689] [Latent Wavelet Diffusion: Enabling 4K Image Synthesis for Free](https://arxiv.org/abs/2506.00433)
*Luigi Sigillo,Shengfeng He,Danilo Comminiello*

Key words: 高分辨率图像生成,潜在扩散模型,小波变换,计算效率

TL;DR: 一种名为LWD的轻量级框架，通过改进潜在扩散模型，实现了超高分辨率（2K至4K）图像生成，且无需额外计算成本。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决高分辨率图像生成中计算效率与细节保留的平衡问题。

Method: 引入尺度一致的变分自编码目标、小波能量图和时变掩码策略。

Result: 在超高清图像合成中提高了感知质量，降低了FID，优于基线模型。

Conclusion: LWD证明频率感知、信号驱动的方法是高分辨率生成建模的高效途径。

Abstract: High-resolution image synthesis remains a core challenge in generative
modeling, particularly in balancing computational efficiency with the
preservation of fine-grained visual detail. We present Latent Wavelet Diffusion
(LWD), a lightweight framework that enables any latent diffusion model to scale
to ultra-high-resolution image generation (2K to 4K) for free. LWD introduces
three key components: (1) a scale-consistent variational autoencoder objective
that enhances the spectral fidelity of latent representations; (2) wavelet
energy maps that identify and localize detail-rich spatial regions within the
latent space; and (3) a time-dependent masking strategy that focuses denoising
supervision on high-frequency components during training. LWD requires no
architectural modifications and incurs no additional computational overhead.
Despite its simplicity, it consistently improves perceptual quality and reduces
FID in ultra-high-resolution image synthesis, outperforming strong baseline
models. These results highlight the effectiveness of frequency-aware,
signal-driven supervision as a principled and efficient approach for
high-resolution generative modeling.

</details>


### [690] [GThinker: Towards General Multimodal Reasoning via Cue-Guided Rethinking](https://arxiv.org/abs/2506.01078)
*Yufei Zhan,Ziheng Wu,Yousong Zhu,Rongkun Xue,Ruipu Luo,Zhenghao Chen,Can Zhang,Yifan Li,Zhentao He,Zheming Yang,Ming Tang,Minghui Qiu,Jinqiao Wang*

Key words: 多模态推理,Cue-Rethinking,GThinker-11K,两阶段训练

TL;DR: GThinker是一种新型多模态推理模型，通过视觉线索重新思考（Cue-Rethinking）和两阶段训练，显著提升通用、数学和科学场景的多模态推理性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有MLLMs在视觉推理任务中表现不佳，主要依赖逻辑和知识推理，未能有效整合视觉信息。

Method: 提出Cue-Rethinking推理模式，通过视觉线索迭代优化推理；设计两阶段训练（模式引导冷启动和激励强化学习），并构建GThinker-11K数据集。

Result: GThinker在M$^3$CoT基准上达到81.5%，优于O4-mini，通用场景推理平均提升2.1%，数学推理性能相当。

Conclusion: GThinker通过视觉线索迭代和域适应训练，显著提升多模态推理能力。

Abstract: Despite notable advancements in multimodal reasoning, leading Multimodal
Large Language Models (MLLMs) still underperform on vision-centric multimodal
reasoning tasks in general scenarios. This shortfall stems from their
predominant reliance on logic- and knowledge-based slow thinking strategies,
while effective for domains like math and science, fail to integrate visual
information effectively during reasoning. Consequently, these models often fail
to adequately ground visual cues, resulting in suboptimal performance in tasks
that require multiple plausible visual interpretations and inferences. To
address this, we present GThinker (General Thinker), a novel reasoning MLLM
excelling in multimodal reasoning across general scenarios, mathematics, and
science. GThinker introduces Cue-Rethinking, a flexible reasoning pattern that
grounds inferences in visual cues and iteratively reinterprets these cues to
resolve inconsistencies. Building on this pattern, we further propose a
two-stage training pipeline, including pattern-guided cold start and incentive
reinforcement learning, designed to enable multimodal reasoning capabilities
across domains. Furthermore, to support the training, we construct
GThinker-11K, comprising 7K high-quality, iteratively-annotated reasoning paths
and 4K curated reinforcement learning samples, filling the data gap toward
general multimodal reasoning. Extensive experiments demonstrate that GThinker
achieves 81.5% on the challenging comprehensive multimodal reasoning benchmark
M$^3$CoT, surpassing the latest O4-mini model. It also shows an average
improvement of 2.1% on general scenario multimodal reasoning benchmarks, while
maintaining on-par performance in mathematical reasoning compared to
counterpart advanced reasoning models. The code, model, and data will be
released soon at https://github.com/jefferyZhan/GThinker.

</details>


### [691] [Learning What Matters: Prioritized Concept Learning via Relative Error-driven Sample Selection](https://arxiv.org/abs/2506.01085)
*Shivam Chandhok,Qian Yang,Oscar Manas,Kanishk Jain,Leonid Sigal,Aishwarya Agrawal*

Key words: 视觉语言模型, 指令调优, 数据效率, 样本选择, PROGRESS

TL;DR: PROGRESS提出了一种动态选择样本学习的框架，通过优先级概念学习提高视觉语言模型的效率和性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决指令调优在视觉语言模型中成本高昂的问题，包括大规模数据集、高质量标注和计算资源需求。

Method: PROGRESS通过相对误差驱动的样本选择动态选择未掌握且难度适中的样本，优先学习进步最快的技能。

Result: 实验表明，PROGRESS在多个数据集上性能优于现有方法，且数据需求量更低。

Conclusion: PROGRESS是一种可扩展的高效学习解决方案，具有跨架构通用性和向更大模型的迁移能力。

Abstract: Instruction tuning has been central to the success of recent vision-language
models (VLMs), but it remains expensive-requiring large-scale datasets,
high-quality annotations, and large compute budgets. We propose PRioritized
cOncept learninG via Relative Error-driven Sample Selection (PROGRESS), a data-
and compute-efficient framework that enables VLMs to dynamically select what to
learn next based on their evolving needs during training. At each stage, the
model tracks its learning progress across skills and selects the most
informative samples-those it has not already mastered and that are not too
difficult to learn at the current stage of training. This strategy effectively
controls skill acquisition and the order in which skills are learned.
Specifically, we sample from skills showing the highest learning progress,
prioritizing those with the most rapid improvement. Unlike prior methods,
PROGRESS requires no upfront answer annotations, queries answers only on a need
basis, avoids reliance on additional supervision from auxiliary VLMs, and does
not require compute-heavy gradient computations for data selection. Experiments
across multiple instruction-tuning datasets of varying scales demonstrate that
PROGRESS consistently outperforms state-of-the-art baselines with much less
data and supervision. Additionally, we show strong cross-architecture
generalization and transferability to larger models, validating PROGRESS as a
scalable solution for efficient learning.

</details>


### [692] [CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic Gaussian Splatting](https://arxiv.org/abs/2506.01109)
*Fengze Li,Yangle Liu,Jieming Ma,Hai-Ning Liang,Yaochun Shen,Huangxiang Li,Zhijing Wu*

Key words: 3D水果计数,语义控制,实时渲染,高斯溅射,语言嵌入

TL;DR: 论文提出了FruitLangGS框架，通过空间重建、语义嵌入和语言引导的实例估计，解决了现有方法在实时3D水果计数中的局限性，实现了更高的渲染速度、语义灵活性和计数精度。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决农业环境中因视觉遮挡、语义模糊和3D重建计算需求高而难以准确计数水果的问题。

Method: 使用自适应高斯溅射管道进行场景重建，结合CLIP对齐的语言嵌入实现语义控制，并采用分布感知采样和聚类进行实例估计。

Result: 在真实果园数据上验证了FruitLangGS在渲染速度、语义灵活性和计数精度上的优越性。

Conclusion: FruitLangGS为开放世界场景中的语言驱动实时神经渲染提供了新视角。

Abstract: Accurate fruit counting in real-world agricultural environments is a
longstanding challenge due to visual occlusions, semantic ambiguity, and the
high computational demands of 3D reconstruction. Existing methods based on
neural radiance fields suffer from low inference speed, limited generalization,
and lack support for open-set semantic control. This paper presents
FruitLangGS, a real-time 3D fruit counting framework that addresses these
limitations through spatial reconstruction, semantic embedding, and
language-guided instance estimation. FruitLangGS first reconstructs
orchard-scale scenes using an adaptive Gaussian splatting pipeline with
radius-aware pruning and tile-based rasterization for efficient rendering. To
enable semantic control, each Gaussian encodes a compressed CLIP-aligned
language embedding, forming a compact and queryable 3D representation. At
inference time, prompt-based semantic filtering is applied directly in 3D
space, without relying on image-space segmentation or view-level fusion. The
selected Gaussians are then converted into dense point clouds via
distribution-aware sampling and clustered to estimate fruit counts.
Experimental results on real orchard data demonstrate that FruitLangGS achieves
higher rendering speed, semantic flexibility, and counting accuracy compared to
prior approaches, offering a new perspective for language-driven, real-time
neural rendering across open-world scenarios.

</details>


### [693] [Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination](https://arxiv.org/abs/2506.01902)
*Xinliu Zhong,Kayhan Batmanghelich,Li Sun*

Key words: 生物医学视觉语言模型、对比学习、语义扰动、多模态表示

TL;DR: 提出了一种新的方法（扰动报告判别）用于预训练生物医学视觉语言模型，解决了现有对比学习方法忽视生物医学文本复杂语义的问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 生物医学文本具有复杂且领域特定的语义，常见的对比学习方法往往忽视这一点。

Method: 通过扰动报告（保持词汇但破坏语义结构），让模型区分原始报告与扰动报告，并增强对图像和文本细粒度信息的敏感性。

Result: 在多个下游任务中表现优于基线方法，学习到更具语义意义和鲁棒性的多模态表示。

Conclusion: 提出方法显著提升了生物医学视觉语言模型的性能，适用于复杂语义场景。

Abstract: Vision-language models pre-trained on large scale of unlabeled biomedical
images and associated reports learn generalizable semantic representations.
These multi-modal representations can benefit various downstream tasks in the
biomedical domain. Contrastive learning is widely used to pre-train
vision-language models for general natural images and associated captions.
Despite its popularity, we found biomedical texts have complex and
domain-specific semantics that are often neglected by common contrastive
methods. To address this issue, we propose a novel method, perturbed report
discrimination, for pre-train biomedical vision-language models. First, we
curate a set of text perturbation methods that keep the same words, but disrupt
the semantic structure of the sentence. Next, we apply different types of
perturbation to reports, and use the model to distinguish the original report
from the perturbed ones given the associated image. Parallel to this, we
enhance the sensitivity of our method to higher level of granularity for both
modalities by contrasting attention-weighted image sub-regions and sub-words in
the image-text pairs. We conduct extensive experiments on multiple downstream
tasks, and our method outperforms strong baseline methods. The results
demonstrate that our approach learns more semantic meaningful and robust
multi-modal representations.

</details>


### [694] [Dual-Process Image Generation](https://arxiv.org/abs/2506.01955)
*Grace Luo,Jonathan Granskog,Aleksander Holynski,Trevor Darrell*

Key words: 图像生成、视觉语言模型、双过程蒸馏、多模态控制

TL;DR: 提出了一种双过程蒸馏方案，使前馈图像生成器能够从深思熟虑的视觉语言模型（VLM）中学习新任务，并通过基于文本和图像的界面实现多种控制任务。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有图像生成控制方法在学习新任务方面受限，而视觉语言模型（VLM）可以通过上下文学习任务并生成正确输出。

Method: 使用双过程蒸馏方案，通过VLM对生成图像进行评分，并通过反向传播梯度更新图像生成器的权重。

Result: 展示了该方法在多种控制信号（如常识推理和视觉提示）中的应用，支持快速实现多模态控制（如调色板、线条粗细等）。

Conclusion: 该框架为图像生成提供了灵活的任务学习能力，扩展了控制的多样性和效率。

Abstract: Prior methods for controlling image generation are limited in their ability
to be taught new tasks. In contrast, vision-language models, or VLMs, can learn
tasks in-context and produce the correct outputs for a given input. We propose
a dual-process distillation scheme that allows feed-forward image generators to
learn new tasks from deliberative VLMs. Our scheme uses a VLM to rate the
generated images and backpropagates this gradient to update the weights of the
image generator. Our general framework enables a wide variety of new control
tasks through the same text-and-image based interface. We showcase a handful of
applications of this technique for different types of control signals, such as
commonsense inferences and visual prompts. With our method, users can implement
multimodal controls for properties such as color palette, line weight, horizon
position, and relative depth within a matter of minutes. Project page:
https://dual-process.github.io.

</details>


### [695] [Concept-Centric Token Interpretation for Vector-Quantized Generative Models](https://arxiv.org/abs/2506.00698)
*Tianze Yang,Yucheng Shi,Mengnan Du,Xuansheng Wu,Qiaoyu Tan,Jin Sun,Ninghao Liu*

Key words: VQGM, 概念解释, 标记组合, 透明度, 图像生成

TL;DR: CORTEX是一种解释VQGM的新方法，通过识别概念特定的标记组合来增强模型透明度。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 尽管VQGM在图像生成中表现强大，但其离散标记的码本尚未被充分理解，CORTEX旨在解决这一问题。

Method: 提出样本级和码本级两种解释方法，分别分析单个图像和全局码本中的标记重要性。

Result: 实验证明CORTEX在解释生成过程中标记使用方面优于基线方法，支持目标图像编辑和快捷特征检测。

Conclusion: CORTEX不仅提高了VQGM的透明度，还具有实际应用价值。

Abstract: Vector-Quantized Generative Models (VQGMs) have emerged as powerful tools for
image generation. However, the key component of VQGMs -- the codebook of
discrete tokens -- is still not well understood, e.g., which tokens are
critical to generate an image of a certain concept? This paper introduces
Concept-Oriented Token Explanation (CORTEX), a novel approach for interpreting
VQGMs by identifying concept-specific token combinations. Our framework employs
two methods: (1) a sample-level explanation method that analyzes token
importance scores in individual images, and (2) a codebook-level explanation
method that explores the entire codebook to find globally relevant tokens.
Experimental results demonstrate CORTEX's efficacy in providing clear
explanations of token usage in the generative process, outperforming baselines
across multiple pretrained VQGMs. Besides enhancing VQGMs transparency, CORTEX
is useful in applications such as targeted image editing and shortcut feature
detection. Our code is available at https://github.com/YangTianze009/CORTEX.

</details>


### [696] [A Review on Coarse to Fine-Grained Animal Action Recognition](https://arxiv.org/abs/2506.01214)
*Ali Zia,Renuka Sharma,Abdelwahed Khamis,Xuesong Li,Muhammad Husnain,Numan Shafi,Saeed Anwar,Sabine Schmoelzl,Eric Stone,Lars Petersson,Vivien Rolland*

Key words: 动物行为识别、粗粒度、细粒度、深度学习、SlowFast、数据集

TL;DR: 这篇综述深入探讨了动物行为识别领域，重点分析了粗粒度（CG）和细粒度（FG）技术。目标是梳理当前研究现状，并阐明户外环境中识别细微动物动作的独特挑战，这些挑战与人类动作识别差异显著。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 动物行为识别的研究面临非刚性身体结构、频繁遮挡和缺乏大规模标注数据集等挑战。与人类动作识别相比，这一问题更复杂，需要针对性的解决方案。

Method: 综述回顾了人类动作识别的发展历程，并讨论了时空深度学习框架（如 SlowFast）在动物行为分析中的有效性。此外，还评估了现有数据集的局限性。

Result: 通过分析当前方法的优缺点，并引入新发布的数据集，综述提出了未来改进细粒度动作识别的方向，旨在提升跨物种行为分析的准确性和泛化能力。

Conclusion: 动物动作识别的独特挑战需要开发专门的方法和技术，未来研究应在数据采集和模型优化方面进一步探索。

Abstract: This review provides an in-depth exploration of the field of animal action
recognition, focusing on coarse-grained (CG) and fine-grained (FG) techniques.
The primary aim is to examine the current state of research in animal behaviour
recognition and to elucidate the unique challenges associated with recognising
subtle animal actions in outdoor environments. These challenges differ
significantly from those encountered in human action recognition due to factors
such as non-rigid body structures, frequent occlusions, and the lack of
large-scale, annotated datasets. The review begins by discussing the evolution
of human action recognition, a more established field, highlighting how it
progressed from broad, coarse actions in controlled settings to the demand for
fine-grained recognition in dynamic environments. This shift is particularly
relevant for animal action recognition, where behavioural variability and
environmental complexity present unique challenges that human-centric models
cannot fully address. The review then underscores the critical differences
between human and animal action recognition, with an emphasis on high
intra-species variability, unstructured datasets, and the natural complexity of
animal habitats. Techniques like spatio-temporal deep learning frameworks
(e.g., SlowFast) are evaluated for their effectiveness in animal behaviour
analysis, along with the limitations of existing datasets. By assessing the
strengths and weaknesses of current methodologies and introducing a
recently-published dataset, the review outlines future directions for advancing
fine-grained action recognition, aiming to improve accuracy and
generalisability in behaviour analysis across species.

</details>


### [697] [Common Inpainted Objects In-N-Out of Context](https://arxiv.org/abs/2506.00721)
*Tianze Yang,Tyson Jordan,Ninghao Liu,Jin Sun*

Key words: 数据集, 扩散修复, 上下文学习, 计算机视觉, 图像取证

TL;DR: COinCO是一个新的数据集，通过扩散修复技术系统性替换COCO图像中的对象，生成97,722张图像，支持上下文学习。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决现有视觉数据集中缺乏上下文不一致示例的问题。

Method: 使用扩散修复技术替换COCO图像中的对象，多模态大语言模型验证并分类修复对象。

Result: 揭示了语义先验模式，支持上下文分类器等三项关键任务。

Conclusion: COinCO为计算机视觉和图像取证中的上下文感知理解提供了基础。

Abstract: We present Common Inpainted Objects In-N-Out of Context (COinCO), a novel
dataset addressing the scarcity of out-of-context examples in existing vision
datasets. By systematically replacing objects in COCO images through
diffusion-based inpainting, we create 97,722 unique images featuring both
contextually coherent and inconsistent scenes, enabling effective context
learning. Each inpainted object is meticulously verified and categorized as in-
or out-of-context through a multimodal large language model assessment. Our
analysis reveals significant patterns in semantic priors that influence
inpainting success across object categories. We demonstrate three key tasks
enabled by COinCO: (1) training context classifiers that effectively determine
whether existing objects belong in their context; (2) a novel
Objects-from-Context prediction task that determines which new objects
naturally belong in given scenes at both instance and clique levels, and (3)
context-enhanced fake detection on state-of-the-art methods without
fine-tuning. COinCO provides a controlled testbed with contextual variations,
establishing a foundation for advancing context-aware visual understanding in
computer vision and image forensics. Our code and data are at:
https://github.com/YangTianze009/COinCO.

</details>


### [698] [Fourier-Modulated Implicit Neural Representation for Multispectral Satellite Image Compression](https://arxiv.org/abs/2506.01234)
*Woojin Cho,Steve Andreas Immanuel,Junhyuk Heo,Darongsae Kwon*

Key words: 多光谱卫星图像,数据压缩,隐式神经表示,傅里叶调制

TL;DR: ImpliSat使用隐式神经表示(INR)和傅里叶调制算法,高效压缩和重建多光谱卫星数据,解决了高维度和多分辨率挑战。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 多光谱卫星图像的高维度、大数据量和多分辨率问题给数据压缩和分析带来了巨大挑战。

Method: 通过INR将卫星图像建模为坐标空间的连续函数,并引入傅里叶调制算法动态调整各波段的光谱和空间特征。

Result: 实现了多光谱卫星数据的高效压缩和精细重建。

Conclusion: ImpliSat为多光谱卫星数据提供了一种统一的压缩与重建框架。

Abstract: Multispectral satellite images play a vital role in agriculture, fisheries,
and environmental monitoring. However, their high dimensionality, large data
volumes, and diverse spatial resolutions across multiple channels pose
significant challenges for data compression and analysis. This paper presents
ImpliSat, a unified framework specifically designed to address these challenges
through efficient compression and reconstruction of multispectral satellite
data. ImpliSat leverages Implicit Neural Representations (INR) to model
satellite images as continuous functions over coordinate space, capturing fine
spatial details across varying spatial resolutions. Furthermore, we introduce a
Fourier modulation algorithm that dynamically adjusts to the spectral and
spatial characteristics of each band, ensuring optimal compression while
preserving critical image details.

</details>


### [699] [TIME: TabPFN-Integrated Multimodal Engine for Robust Tabular-Image Learning](https://arxiv.org/abs/2506.00813)
*Jiaqi Luo,Yuan Yuan,Shixin Xu*

Key words: 多模态学习、表格数据、图像数据、医学应用、缺失值处理

TL;DR: 提出了一种名为TIME的多模态框架，结合TabPFN作为表格编码器和预训练视觉主干，解决了表格数据标准化表示缺失和缺失值处理的挑战。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 表格图像多模态学习在医学应用中具有潜力，但缺乏标准化的表格数据表示，且难以处理表格模态中的缺失值。

Method: 利用TabPFN作为冻结表格编码器生成稳健嵌入，与预训练视觉主干提取的图像特征结合，探索多种融合策略和表格编码器。

Result: 在自然和医学数据集上，TIME表现优于基线方法，尤其是在处理不完整表格输入时效果显著。

Conclusion: TIME在真实多模态学习场景中具有实用价值，尤其是在表格数据缺失情况下表现优异。

Abstract: Tabular-image multimodal learning, which integrates structured tabular data
with imaging data, holds great promise for a variety of tasks, especially in
medical applications. Yet, two key challenges remain: (1) the lack of a
standardized, pretrained representation for tabular data, as is commonly
available in vision and language domains; and (2) the difficulty of handling
missing values in the tabular modality, which are common in real-world medical
datasets. To address these issues, we propose the TabPFN-Integrated Multimodal
Engine (TIME), a novel multimodal framework that builds on the recently
introduced tabular foundation model, TabPFN. TIME leverages TabPFN as a frozen
tabular encoder to generate robust, strong embeddings that are naturally
resilient to missing data, and combines them with image features from
pretrained vision backbones. We explore a range of fusion strategies and
tabular encoders, and evaluate our approach on both natural and medical
datasets. Extensive experiments demonstrate that TIME consistently outperforms
competitive baselines across both complete and incomplete tabular inputs,
underscoring its practical value in real-world multimodal learning scenarios.

</details>


### [700] [Visual Sparse Steering: Improving Zero-shot Image Classification with Sparsity Guided Steering Vectors](https://arxiv.org/abs/2506.01247)
*Gerasimos Chatzoudis,Zhuowei Li,Gemma E. Moran,Hao Wang,Dimitris N. Metaxas*

Key words: 视觉基础模型, 稀疏自编码器, 推理时引导, 检索增强, 原型对齐

TL;DR: VS2是一种轻量级、推理时的方法，通过稀疏特征引导视觉模型，无需对比数据。VS2++是其检索增强版本，进一步提升了性能。PASS通过原型对齐稀疏特征，进一步优化了VS2。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 在动态或资源受限的场景中，无需重新训练或访问大量标注数据即可引导视觉基础模型是一个重要但具挑战性的目标。

Method: VS2使用稀疏自编码器的稀疏特征生成引导向量；VS2++通过检索增强机制选择性放大相关特征；PASS通过原型对齐优化稀疏特征。

Result: VS2在多个数据集上超过CLIP零样本方法；VS2++性能进一步提升；PASS在CIFAR-100上表现最佳。

Conclusion: 稀疏特征引导在特定类别上显著提升性能，原型对齐进一步优化了方法。

Abstract: Steering vision foundation models at inference time without retraining or
access to large labeled datasets is a desirable yet challenging objective,
particularly in dynamic or resource-constrained settings. In this paper, we
introduce Visual Sparse Steering (VS2), a lightweight, test-time method that
guides vision models using steering vectors derived from sparse features
learned by top-$k$ Sparse Autoencoders without requiring contrastive data.
Specifically, VS2 surpasses zero-shot CLIP by 4.12% on CIFAR-100, 1.08% on
CUB-200, and 1.84% on Tiny-ImageNet. We further propose VS2++, a
retrieval-augmented variant that selectively amplifies relevant sparse features
using pseudo-labeled neighbors at inference time. With oracle positive/negative
sets, VS2++ achieves absolute top-1 gains over CLIP zero-shot of up to 21.44%
on CIFAR-100, 7.08% on CUB-200, and 20.47% on Tiny-ImageNet. Interestingly, VS2
and VS2++ raise per-class accuracy by up to 25% and 38%, respectively, showing
that sparse steering benefits specific classes by disambiguating visually or
taxonomically proximate categories rather than providing a uniform boost.
Finally, to better align the sparse features learned through the SAE
reconstruction task with those relevant for downstream performance, we propose
Prototype-Aligned Sparse Steering (PASS). By incorporating a
prototype-alignment loss during SAE training, using labels only during training
while remaining fully test-time unsupervised, PASS consistently, though
modestly, outperforms VS2, achieving a 6.12% gain over VS2 only on CIFAR-100
with ViT-B/32.

</details>


### [701] [ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding](https://arxiv.org/abs/2506.01274)
*Hosu Lee,Junho Kim,Hyunjun Kim,Yong Man Ro*

Key words: 多模态模型，视频理解，强化学习，帧选择

TL;DR: 提出了ReFoCUS框架，通过强化学习优化视频帧选择策略，以提升视频问答性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的视频理解方法依赖静态启发式或外部检索模块，可能导致查询相关信息缺失。

Method: 采用强化学习框架，通过参考LMM的奖励信号学习帧选择策略，并引入自回归条件选择结构。

Result: 在多个视频QA基准测试中提升了推理性能。

Conclusion: 模型内部效用对齐的帧选择策略有助于提升视频理解能力。

Abstract: Recent progress in Large Multi-modal Models (LMMs) has enabled effective
vision-language reasoning, yet the ability to understand video content remains
constrained by suboptimal frame selection strategies. Existing approaches often
rely on static heuristics or external retrieval modules to feed frame
information into video-LLMs, which may fail to provide the query-relevant
information. In this work, we introduce ReFoCUS (Reinforcement-guided Frame
Optimization for Contextual UnderStanding), a novel frame-level policy
optimization framework that shifts the optimization target from textual
responses to visual input selection. ReFoCUS learns a frame selection policy
via reinforcement learning, using reward signals derived from a reference LMM
to reflect the model's intrinsic preferences for frames that best support
temporally grounded responses. To efficiently explore the large combinatorial
frame space, we employ an autoregressive, conditional selection architecture
that ensures temporal coherence while reducing complexity. Our approach does
not require explicit supervision at the frame-level and consistently improves
reasoning performance across multiple video QA benchmarks, highlighting the
benefits of aligning frame selection with model-internal utility.

</details>


### [702] [Towards Edge-Based Idle State Detection in Construction Machinery Using Surveillance Cameras](https://arxiv.org/abs/2506.00904)
*Xander Küpers,Jeroen Klein Brinke,Rob Bemthuis,Ozlem Durmaz Incel*

Key words: 边缘计算, 建筑机械, 空闲检测, 目标检测, 实时处理

TL;DR: 本文提出了一种名为Edge-IMI的框架，用于通过边缘计算设备优化建筑机械的空闲状态检测，以提高设备利用率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 建筑行业中机械设备的低效使用导致运营成本增加和项目延误，因此需要一种实时监控设备活动的解决方案。

Method: Edge-IMI框架由对象检测、跟踪和空闲状态识别三个模块组成，专为资源受限的边缘计算设备设计。

Result: 实验显示，目标检测模块F1得分为71.75%，空闲识别模块能可靠区分活跃与空闲设备。Edge-IMI在Raspberry Pi 5和Intel NUC平台上验证了实时处理的可行性。

Conclusion: Edge-IMI通过边缘计算实现高效推断，减少了对高带宽云服务和昂贵硬件加速器的依赖。

Abstract: The construction industry faces significant challenges in optimizing
equipment utilization, as underused machinery leads to increased operational
costs and project delays. Accurate and timely monitoring of equipment activity
is therefore key to identifying idle periods and improving overall efficiency.
This paper presents the Edge-IMI framework for detecting idle construction
machinery, specifically designed for integration with surveillance camera
systems. The proposed solution consists of three components: object detection,
tracking, and idle state identification, which are tailored for execution on
resource-constrained, CPU-based edge computing devices. The performance of
Edge-IMI is evaluated using a combined dataset derived from the ACID and MOCS
benchmarks. Experimental results confirm that the object detector achieves an
F1 score of 71.75%, indicating robust real-world detection capabilities. The
logistic regression-based idle identification module reliably distinguishes
between active and idle machinery with minimal false positives. Integrating all
three modules, Edge-IMI enables efficient on-site inference, reducing reliance
on high-bandwidth cloud services and costly hardware accelerators. We also
evaluate the performance of object detection models on Raspberry Pi 5 and an
Intel NUC platforms, as example edge computing platforms. We assess the
feasibility of real-time processing and the impact of model optimization
techniques.

</details>


### [703] [Playing with Transformer at 30+ FPS via Next-Frame Diffusion](https://arxiv.org/abs/2506.01380)
*Xinle Cheng,Tianyu He,Jiayi Xu,Junliang Guo,Di He,Jiang Bian*

Key words: 自回归模型, 扩散模型, 视频生成, 一致性蒸馏, 推测采样

TL;DR: 本文提出Next-Frame Diffusion (NFD)，一种自回归扩散Transformer，通过块级因果注意力和两种创新技术（一致性蒸馏和推测采样）实现高效推理，首次在A100 GPU上以30 FPS实现视频生成。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 在交互式视频内容创建和流媒体应用中，自回归视频模型相比双向扩散模型具有优势，但实时生成仍面临高计算成本和硬件效率低的挑战。

Method: 1. 引入一致性蒸馏技术到视频领域，减少采样步骤；2. 提出推测采样，利用相邻帧动作输入相似性并行生成多帧。

Result: NFD在视觉质量和采样效率上超越基线，首次在A100 GPU上以30 FPS实现自回归视频生成。

Conclusion: NFD通过技术创新显著提升自回归视频生成的效率，为实时应用提供可能。

Abstract: Autoregressive video models offer distinct advantages over bidirectional
diffusion models in creating interactive video content and supporting streaming
applications with arbitrary duration. In this work, we present Next-Frame
Diffusion (NFD), an autoregressive diffusion transformer that incorporates
block-wise causal attention, enabling iterative sampling and efficient
inference via parallel token generation within each frame. Nonetheless,
achieving real-time video generation remains a significant challenge for such
models, primarily due to the high computational cost associated with diffusion
sampling and the hardware inefficiencies inherent to autoregressive generation.
To address this, we introduce two innovations: (1) We extend consistency
distillation to the video domain and adapt it specifically for video models,
enabling efficient inference with few sampling steps; (2) To fully leverage
parallel computation, motivated by the observation that adjacent frames often
share the identical action input, we propose speculative sampling. In this
approach, the model generates next few frames using current action input, and
discard speculatively generated frames if the input action differs. Experiments
on a large-scale action-conditioned video generation benchmark demonstrate that
NFD beats autoregressive baselines in terms of both visual quality and sampling
efficiency. We, for the first time, achieves autoregressive video generation at
over 30 Frames Per Second (FPS) on an A100 GPU using a 310M model.

</details>


### [704] [VRD-IU: Lessons from Visually Rich Document Intelligence and Understanding](https://arxiv.org/abs/2506.01388)
*Yihao Ding,Soyeon Caren Han,Yan Li,Josiah Poon*

Key words: Visually Rich Document Understanding, Form-NLU, 竞赛, 多模态特征融合, 关键信息提取

TL;DR: VRD-IU竞赛聚焦于从复杂多格式表单中提取和定位关键信息，展示了多种先进方法，提升了视觉丰富文档理解的标杆。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决表单类文档由于复杂布局和多利益相关者参与导致的独特挑战。

Method: 竞赛设置两个赛道：基于实体的关键信息检索（Track A）和端到端的关键信息定位（Track B），参与者使用了层次分解、基于Transformer的检索和多模态特征融合等技术。

Result: 20多个团队参与，顶级模型为VRDU领域设定了新基准。

Conclusion: 竞赛为文档智能提供了宝贵见解，展示了先进技术在解决实际问题中的潜力。

Abstract: Visually Rich Document Understanding (VRDU) has emerged as a critical field
in document intelligence, enabling automated extraction of key information from
complex documents across domains such as medical, financial, and educational
applications. However, form-like documents pose unique challenges due to their
complex layouts, multi-stakeholder involvement, and high structural
variability. Addressing these issues, the VRD-IU Competition was introduced,
focusing on extracting and localizing key information from multi-format forms
within the Form-NLU dataset, which includes digital, printed, and handwritten
documents. This paper presents insights from the competition, which featured
two tracks: Track A, emphasizing entity-based key information retrieval, and
Track B, targeting end-to-end key information localization from raw document
images. With over 20 participating teams, the competition showcased various
state-of-the-art methodologies, including hierarchical decomposition,
transformer-based retrieval, multimodal feature fusion, and advanced object
detection techniques. The top-performing models set new benchmarks in VRDU,
providing valuable insights into document intelligence.

</details>


### [705] [ViTA-PAR: Visual and Textual Attribute Alignment with Attribute Prompting for Pedestrian Attribute Recognition](https://arxiv.org/abs/2506.01411)
*Minjeong Park,Hongbeen Park,Jinkyu Kim*

Key words: 行人属性识别, 多模态提示, 视觉-语言对齐, 属性上下文, ViTA-PAR

TL;DR: 本文提出了ViTA-PAR方法，通过多模态提示和视觉-语言对齐增强行人属性识别，结合视觉属性提示和可学习的文本提示模板，实现了高效的属性识别效果。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法在捕获行人属性时受限于固定区域，无法适应属性在不同身体位置的变化，ViTA-PAR旨在通过多模态技术提升属性的全局和局部表征能力。

Method: 提出视觉属性提示捕获全局到局部语义，设计可学习的文本提示模板（人物和属性上下文提示），并对齐视觉和文本特征进行融合。

Result: 在四个PAR基准测试中表现出色，实现了高效的推理性能。

Conclusion: ViTA-PAR通过视觉和文本属性对齐显著提升了行人属性识别的鲁棒性和准确性。

Abstract: The Pedestrian Attribute Recognition (PAR) task aims to identify various
detailed attributes of an individual, such as clothing, accessories, and
gender. To enhance PAR performance, a model must capture features ranging from
coarse-grained global attributes (e.g., for identifying gender) to fine-grained
local details (e.g., for recognizing accessories) that may appear in diverse
regions. Recent research suggests that body part representation can enhance the
model's robustness and accuracy, but these methods are often restricted to
attribute classes within fixed horizontal regions, leading to degraded
performance when attributes appear in varying or unexpected body locations. In
this paper, we propose Visual and Textual Attribute Alignment with Attribute
Prompting for Pedestrian Attribute Recognition, dubbed as ViTA-PAR, to enhance
attribute recognition through specialized multimodal prompting and
vision-language alignment. We introduce visual attribute prompts that capture
global-to-local semantics, enabling diverse attribute representations. To
enrich textual embeddings, we design a learnable prompt template, termed person
and attribute context prompting, to learn person and attributes context.
Finally, we align visual and textual attribute features for effective fusion.
ViTA-PAR is validated on four PAR benchmarks, achieving competitive performance
with efficient inference. We release our code and model at
https://github.com/mlnjeongpark/ViTA-PAR.

</details>


### [706] [SVarM: Linear Support Varifold Machines for Classification and Regression on Geometric Data](https://arxiv.org/abs/2506.01189)
*Emmanuel Hartman,Nicolas Charon*

Key words: 几何深度学习, varifold, 形状分析, 不变性, 神经网络

TL;DR: 论文提出了一种名为SVarM的方法，利用varifold表示形状，并通过与测试函数的对偶性构建机器学习框架，解决了几何数据统计分析的挑战。该方法在多个形状数据集上表现出色，且参数较少。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 几何数据的统计分析因形状空间的非欧几里得性而具有挑战性。通过结合形状参数化的不变性，可以提升模型的泛化能力。

Method: 利用varifold表示形状，并通过测试函数的对偶性构建分类和回归模型，引入了基于神经网络的测试函数表示。

Result: 在多组形状图与表面数据集上表现良好，性能与主流方法相当，且参数数量显著减少。

Conclusion: SVarM提供了一种高效且参数少的几何数据分析框架，具有广泛的应用潜力。

Abstract: Despite progress in the rapidly developing field of geometric deep learning,
performing statistical analysis on geometric data--where each observation is a
shape such as a curve, graph, or surface--remains challenging due to the
non-Euclidean nature of shape spaces, which are defined as equivalence classes
under invariance groups. Building machine learning frameworks that incorporate
such invariances, notably to shape parametrization, is often crucial to ensure
generalizability of the trained models to new observations. This work proposes
SVarM to exploit varifold representations of shapes as measures and their
duality with test functions $h:\mathbb{R}^n \times S^{n-1} \to \mathbb{R}$.
This method provides a general framework akin to linear support vector machines
but operating instead over the infinite-dimensional space of varifolds. We
develop classification and regression models on shape datasets by introducing a
neural network-based representation of the trainable test function $h$. This
approach demonstrates strong performance and robustness across various shape
graph and surface datasets, achieving results comparable to state-of-the-art
methods while significantly reducing the number of trainable parameters.

</details>


### [707] [G4Seg: Generation for Inexact Segmentation Refinement with Diffusion Models](https://arxiv.org/abs/2506.01539)
*Tianjiao Zhang,Fei Zhang,Jiangchao Yao,Ya Zhang,Yanfeng Wang*

Key words: 文本到图像扩散模型,不精确分割,生成先验,语义对齐,稳定扩散

TL;DR: 该论文提出了一种利用大规模文本到图像扩散模型解决不精确分割任务的方法，通过分析原始图像与掩码条件生成图像的模式差异，实现语义对齐和前景概率更新。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统的分割方法依赖于判别模型或密集视觉表示，而本文探索了稳定扩散（SD）中的生成先验，旨在利用生成差异解决判别任务。

Method: 基于稳定扩散的生成先验，通过分析原始图像与掩码条件生成图像的模式差异，实现语义对齐和前景概率更新。

Result: 实验验证了该方法的有效性和优越性，展示了生成差异在建模密集表示中的潜力。

Conclusion: 生成方法在解决判别任务方面具有潜力，未来的研究可以进一步探索这一方向。

Abstract: This paper considers the problem of utilizing a large-scale text-to-image
diffusion model to tackle the challenging Inexact Segmentation (IS) task.
Unlike traditional approaches that rely heavily on discriminative-model-based
paradigms or dense visual representations derived from internal attention
mechanisms, our method focuses on the intrinsic generative priors in Stable
Diffusion~(SD). Specifically, we exploit the pattern discrepancies between
original images and mask-conditional generated images to facilitate a
coarse-to-fine segmentation refinement by establishing a semantic
correspondence alignment and updating the foreground probability. Comprehensive
quantitative and qualitative experiments validate the effectiveness and
superiority of our plug-and-play design, underscoring the potential of
leveraging generation discrepancies to model dense representations and
encouraging further exploration of generative approaches for solving
discriminative tasks.

</details>


### [708] [Efficiency without Compromise: CLIP-aided Text-to-Image GANs with Increased Diversity](https://arxiv.org/abs/2506.01493)
*Yuya Kobayashi,Yuhta Takida,Takashi Shibuya,Yuki Mitsufuji*

Key words: GAN, 文本到图像, 预训练模型, 多样性, 训练成本

TL;DR: 论文提出了一种名为SCAD的高效文本到图像GAN模型，通过结合预训练模型和专用鉴别器，显著降低了训练成本并提升了生成多样性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前大规模文本到图像GAN训练成本高且生成多样性不足，研究人员希望通过预训练模型和专用鉴别器解决这一挑战。

Method: 使用两个专用鉴别器和切片对抗网络（SANs）构建SCAD模型，并提出了Per-Prompt Diversity（PPD）指标。

Result: SCAD在训练成本大幅降低的同时，生成多样性和样本保真度显著提升，零样本FID与最新大规模GAN竞争。

Conclusion: SCAD通过创新方法实现了高效和高保真度的文本到图像生成，为相关研究和应用提供了实用方案。

Abstract: Recently, Generative Adversarial Networks (GANs) have been successfully
scaled to billion-scale large text-to-image datasets. However, training such
models entails a high training cost, limiting some applications and research
usage. To reduce the cost, one promising direction is the incorporation of
pre-trained models. The existing method of utilizing pre-trained models for a
generator significantly reduced the training cost compared with the other
large-scale GANs, but we found the model loses the diversity of generation for
a given prompt by a large margin. To build an efficient and high-fidelity
text-to-image GAN without compromise, we propose to use two specialized
discriminators with Slicing Adversarial Networks (SANs) adapted for
text-to-image tasks. Our proposed model, called SCAD, shows a notable
enhancement in diversity for a given prompt with better sample fidelity. We
also propose to use a metric called Per-Prompt Diversity (PPD) to evaluate the
diversity of text-to-image models quantitatively. SCAD achieved a zero-shot FID
competitive with the latest large-scale GANs at two orders of magnitude less
training cost.

</details>


### [709] [EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models](https://arxiv.org/abs/2506.01608)
*Andy Bonnetto,Haozhe Qi,Franklin Leong,Matea Tashkovska,Mahdi Rad,Solaiman Shokur,Friedhelm Hummel,Silvestro Micera,Marc Pollefeys,Alexander Mathis*

Key words: 行为理解，多模态数据集，动作捕捉，厨房环境，基准任务

TL;DR: EPFL-Smart-Kitchen-30数据集是一个多模态厨房行为数据集，用于研究人类运动和认知功能，包含多视角动作捕捉和密集标注，提出了四个基准任务以推动行为理解。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 为了更好地理解人类在复杂任务（如烹饪）中的行为和认知功能，需要一个全面的数据集来捕获多模态动作信息。

Method: 使用九台静态RGB-D相机、IMUs和HoloLens~2头显，捕获16名被试在厨房环境中的3D动作、视线和身体运动，共计29.7小时数据，并密集标注动作序列。

Result: 提出了多模态数据集EPFL-Smart-Kitchen-30，并设计了四个基准任务（如视觉-语言基准、动作识别基准等），为行为研究提供了新工具。

Conclusion: 该数据集为研究生态有效的人类行为提供了丰富资源，有望推动行为理解和建模方法的发展。

Abstract: Understanding behavior requires datasets that capture humans while carrying
out complex tasks. The kitchen is an excellent environment for assessing human
motor and cognitive function, as many complex actions are naturally exhibited
in kitchens from chopping to cleaning. Here, we introduce the
EPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion capture
platform inside a kitchen environment. Nine static RGB-D cameras, inertial
measurement units (IMUs) and one head-mounted HoloLens~2 headset were used to
capture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset is
a multi-view action dataset with synchronized exocentric, egocentric, depth,
IMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjects
cooking four different recipes. Action sequences were densely annotated with
33.78 action segments per minute. Leveraging this multi-modal dataset, we
propose four benchmarks to advance behavior understanding and modeling through
1) a vision-language benchmark, 2) a semantic text-to-motion generation
benchmark, 3) a multi-modal action recognition benchmark, 4) a pose-based
action segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset to
pave the way for better methods as well as insights to understand the nature of
ecologically-valid human behavior. Code and data are available at
https://github.com/amathislab/EPFL-Smart-Kitchen

</details>


### [710] [Multi-Modal Dataset Distillation in the Wild](https://arxiv.org/abs/2506.01586)
*Zhuohang Dang,Minnan Luo,Chengyou Jia,Hangwei Qian,Xiaojun Chang,Ivor W. Tsang*

Key words: 多模态数据, 数据集蒸馏, 噪声容忍, 细粒度对应

TL;DR: MDW框架通过蒸馏多模态数据解决大规模训练带来的存储和计算成本问题，同时优化噪声数据以提高模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前多模态模型训练面临大规模数据存储成本高和噪声数据降低性能的挑战。

Method: 提出MDW框架，通过可学习的细粒度对应关系和双轨协同学习，蒸馏数据并提升信息密度。

Result: 实验表明MDW在多种压缩比下性能优于现有方法15%以上，具有强可扩展性。

Conclusion: MDW有效解决数据规模和噪声问题，提升模型训练效率。

Abstract: Recent multi-modal models have shown remarkable versatility in real-world
applications. However, their rapid development encounters two critical data
challenges. First, the training process requires large-scale datasets, leading
to substantial storage and computational costs. Second, these data are
typically web-crawled with inevitable noise, i.e., partially mismatched pairs,
severely degrading model performance. To these ends, we propose Multi-modal
dataset Distillation in the Wild, i.e., MDW, the first framework to distill
noisy multi-modal datasets into compact clean ones for effective and efficient
model training. Specifically, MDW introduces learnable fine-grained
correspondences during distillation and adaptively optimizes distilled data to
emphasize correspondence-discriminative regions, thereby enhancing distilled
data's information density and efficacy. Moreover, to capture robust
cross-modal correspondence prior knowledge from real data, MDW proposes
dual-track collaborative learning to avoid the risky data noise, alleviating
information loss with certifiable noise tolerance. Extensive experiments
validate MDW's theoretical and empirical efficacy with remarkable scalability,
surpassing prior methods by over 15% across various compression ratios,
highlighting its appealing practicality for applications with diverse efficacy
and resource needs.

</details>


### [711] [Data Pruning by Information Maximization](https://arxiv.org/abs/2506.01701)
*Haoru Tan,Sitong Wu,Wei Huang,Shizhen Zhao,Xiaojuan Qi*

Key words: 数据修剪, 核心集选择, 信息最大化, 冗余最小化, 离散二次规划

TL;DR: InfoMax是一种新颖的数据修剪方法，通过最大化样本信息内容并最小化冗余来提升核心集的整体信息量。该方法通过重要性评分和样本相似性量化信息与冗余，将其形式化为离散二次规划问题，并采用高效求解器扩展至大规模数据集。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统数据修剪方法难以平衡信息量和冗余性，限制了核心集的有效性，因此需要一种能同时优化二者的新方法。

Method: InfoMax通过重要性评分量样本信息量，用样本相似性量化冗余性，并将其建模为离散二次规划问题，采用梯度求解器和稀疏化技术提高扩展性。

Result: 实验证明InfoMax在图像分类、视觉语言预训练和大型语言模型指令调优等任务中表现优异。

Conclusion: InfoMax是一种高效且可扩展的数据修剪方法，显著提升了核心集的信息量。

Abstract: In this paper, we present InfoMax, a novel data pruning method, also known as
coreset selection, designed to maximize the information content of selected
samples while minimizing redundancy. By doing so, InfoMax enhances the overall
informativeness of the coreset. The information of individual samples is
measured by importance scores, which capture their influence or difficulty in
model learning. To quantify redundancy, we use pairwise sample similarities,
based on the premise that similar samples contribute similarly to the learning
process. We formalize the coreset selection problem as a discrete quadratic
programming (DQP) task, with the objective of maximizing the total information
content, represented as the sum of individual sample contributions minus the
redundancies introduced by similar samples within the coreset. To ensure
practical scalability, we introduce an efficient gradient-based solver,
complemented by sparsification techniques applied to the similarity matrix and
dataset partitioning strategies. This enables InfoMax to seamlessly scale to
datasets with millions of samples. Extensive experiments demonstrate the
superior performance of InfoMax in various data pruning tasks, including image
classification, vision-language pre-training, and instruction tuning for large
language models.

</details>


### [712] [Efficient Egocentric Action Recognition with Multimodal Data](https://arxiv.org/abs/2506.01757)
*Marco Calzavara,Ard Kastrati,Matteo Macchini,Dushan Vasilevski,Roger Wattenhofer*

Key words: Egocentric Action Recognition, XR devices, sampling frequency, computational efficiency, multimodal input

TL;DR: 研究表明，通过降低RGB帧采样率并辅以高频3D手部姿态输入，可在保持高准确率的同时显著降低CPU使用率，适用于XR设备的实时动作识别。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决XR设备在实时动作识别中面临的便携性、电池寿命和计算资源之间的权衡问题。

Method: 通过系统分析RGB视频和3D手部姿态在不同采样频率下对动作识别性能和CPU使用率的影响。

Result: 降低RGB帧采样率并结合高频3D手部姿态输入可减少3倍CPU使用率，且几乎不影响识别性能。

Conclusion: 多模态输入策略是实现XR设备高效实时动作识别的可行方法。

Abstract: The increasing availability of wearable XR devices opens new perspectives for
Egocentric Action Recognition (EAR) systems, which can provide deeper human
understanding and situation awareness. However, deploying real-time algorithms
on these devices can be challenging due to the inherent trade-offs between
portability, battery life, and computational resources. In this work, we
systematically analyze the impact of sampling frequency across different input
modalities - RGB video and 3D hand pose - on egocentric action recognition
performance and CPU usage. By exploring a range of configurations, we provide a
comprehensive characterization of the trade-offs between accuracy and
computational efficiency. Our findings reveal that reducing the sampling rate
of RGB frames, when complemented with higher-frequency 3D hand pose input, can
preserve high accuracy while significantly lowering CPU demands. Notably, we
observe up to a 3x reduction in CPU usage with minimal to no loss in
recognition performance. This highlights the potential of multimodal input
strategies as a viable approach to achieving efficient, real-time EAR on XR
devices.

</details>


### [713] [unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary Reasoning](https://arxiv.org/abs/2506.01778)
*Yafei Yang,Zihui Zhang,Bo Yang*

Key words: 无监督学习,多目标分割,对象中心表示,真实世界图像

TL;DR: 本文提出了一种名为unMORE的两阶段无监督多目标分割方法，能够在真实世界的复杂图像中有效识别多个对象。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法依赖图像重建目标或预训练特征，仅能分割简单合成对象或有限数量的真实对象。

Method: unMORE采用两阶段流程：第一阶段学习三层对象中心表示，第二阶段通过无网络的推理模块利用先验知识发现多对象。

Result: 在6个真实世界基准数据集（包括COCO）上显著优于现有方法，尤其在拥挤图像中表现优异。

Conclusion: unMORE实现了最先进的无监督多对象分割性能。

Abstract: We study the challenging problem of unsupervised multi-object segmentation on
single images. Existing methods, which rely on image reconstruction objectives
to learn objectness or leverage pretrained image features to group similar
pixels, often succeed only in segmenting simple synthetic objects or
discovering a limited number of real-world objects. In this paper, we introduce
unMORE, a novel two-stage pipeline designed to identify many complex objects in
real-world images. The key to our approach involves explicitly learning three
levels of carefully defined object-centric representations in the first stage.
Subsequently, our multi-object reasoning module utilizes these learned object
priors to discover multiple objects in the second stage. Notably, this
reasoning module is entirely network-free and does not require human labels.
Extensive experiments demonstrate that unMORE significantly outperforms all
existing unsupervised methods across 6 real-world benchmark datasets, including
the challenging COCO dataset, achieving state-of-the-art object segmentation
results. Remarkably, our method excels in crowded images where all baselines
collapse.

</details>


### [714] [MoDA: Modulation Adapter for Fine-Grained Visual Grounding in Instructional MLLMs](https://arxiv.org/abs/2506.01850)
*Wayner Barrios,Andrés Villa,Juan León Alcázar,SouYoung Jin,Bernard Ghanem*

Key words: 多模态大语言模型, 视觉定位, 调制适配器, 指令引导, Transformer

TL;DR: 论文提出MoDA（Modulation Adapter），一种轻量级模块，通过指令引导的调制优化预对齐视觉特征，提升多模态大语言模型在复杂场景中的细粒度视觉概念理解能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法在复杂场景中难以对细粒度视觉概念进行有效定位，需要提升多模态大语言模型的视觉定位能力。

Method: 采用两阶段训练方法：1）通过冻结视觉编码器和适配层将图像特征对齐到LLMs输入空间；2）在指令微调阶段使用基于Transformer交叉注意力机制的MoDA模块生成调制掩码，强调与语言指令相关的视觉特征维度。

Result: 实验表明，MoDA提升了视觉定位能力，并生成了更符合上下文的回答。

Conclusion: MoDA是一种通用增强模块，可有效提升基于图像的多模态大语言模型性能。

Abstract: Recently, Multimodal Large Language Models (MLLMs) have demonstrated
impressive performance on instruction-following tasks by integrating pretrained
visual encoders with large language models (LLMs). However, existing approaches
often struggle to ground fine-grained visual concepts in complex scenes. In
this paper, we propose MoDA (Modulation Adapter), a lightweight yet effective
module designed to refine pre-aligned visual features through
instruction-guided modulation. Our approach follows the standard LLaVA training
protocol, consisting of a two-stage process: (1) aligning image features to the
LLMs input space via a frozen vision encoder and adapter layers, and (2)
refining those features using the MoDA adapter during the instructional tuning
stage. MoDA employs a Transformer-based cross-attention mechanism to generate a
modulation mask over the aligned visual tokens, thereby emphasizing
semantically relevant embedding dimensions based on the language instruction.
The modulated features are then passed to the LLM for autoregressive language
generation. Our experimental evaluation shows that MoDA improves visual
grounding and generates more contextually appropriate responses, demonstrating
its effectiveness as a general-purpose enhancement for image-based MLLMs.

</details>


### [715] [Ridgeformer: Mutli-Stage Contrastive Training For Fine-grained Cross-Domain Fingerprint Recognition](https://arxiv.org/abs/2506.01806)
*Shubham Pandey,Bhavin Jawade,Srirangaraj Setlur*

Key words: 非接触式指纹识别, Transformer, 特征对齐, 分层匹配

TL;DR: 论文提出了一种基于Transformer的多阶段非接触式指纹匹配方法，解决了现有技术中因图像模糊、对比度低和手指位置变化等问题导致的匹配不准确问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 非接触式指纹识别因卫生和便携性需求日益增长，但现有技术面临图像模糊、对比度低、手指位置变化和透视变形等挑战，影响了匹配准确性。

Method: 采用多阶段Transformer方法，先提取全局空间特征，再优化局部特征对齐，通过分层特征提取和匹配流程实现精准匹配。

Result: 在HKPolyU和RidgeBase等公开数据集上，该方法在非接触-接触和非接触-非接触匹配中表现优于现有方法，包括商业解决方案（COTS）。

Conclusion: 提出的多阶段Transformer方法显著提升了非接触式指纹匹配的准确性和可靠性。

Abstract: The increasing demand for hygienic and portable biometric systems has
underscored the critical need for advancements in contactless fingerprint
recognition. Despite its potential, this technology faces notable challenges,
including out-of-focus image acquisition, reduced contrast between fingerprint
ridges and valleys, variations in finger positioning, and perspective
distortion. These factors significantly hinder the accuracy and reliability of
contactless fingerprint matching. To address these issues, we propose a novel
multi-stage transformer-based contactless fingerprint matching approach that
first captures global spatial features and subsequently refines localized
feature alignment across fingerprint samples. By employing a hierarchical
feature extraction and matching pipeline, our method ensures fine-grained,
cross-sample alignment while maintaining the robustness of global feature
representation. We perform extensive evaluations on publicly available datasets
such as HKPolyU and RidgeBase under different evaluation protocols, such as
contactless-to-contact matching and contactless-to-contactless matching and
demonstrate that our proposed approach outperforms existing methods, including
COTS solutions.

</details>


### [716] [TaxaDiffusion: Progressively Trained Diffusion Model for Fine-Grained Species Generation](https://arxiv.org/abs/2506.01923)
*Amin Karimi Monsefi,Mridul Khurana,Rajiv Ramnath,Anuj Karpatne,Wei-Lun Chao,Cheng Zhang*

Key words: TaxaDiffusion, 扩散模型, 细粒度图像生成, 分类学, 分层训练

TL;DR: TaxaDiffusion提出了一种基于分类学知识的扩散模型训练框架，用于生成高保真度的细粒度动物图像，通过分层学习策略显著提升生成质量。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 为了解决细粒度动物图像生成中形态和身份准确性的挑战，TaxaDiffusion利用物种间的视觉相似性，通过分类学层次指导训练。

Method: 采用分层训练策略，从粗到细（如从类、目到科、属再到种）逐步训练条件扩散模型，以捕捉共享形态特征并细化物种差异。

Result: 在三个细粒度动物数据集上的实验表明，TaxaDiffusion优于现有方法，生成图像的保真度更高，尤其在数据有限的情况下表现突出。

Conclusion: TaxaDiffusion通过整合分类学知识，显著提升了细粒度动物图像的生成质量，为其他领域的分层生成任务提供了借鉴。

Abstract: We propose TaxaDiffusion, a taxonomy-informed training framework for
diffusion models to generate fine-grained animal images with high morphological
and identity accuracy. Unlike standard approaches that treat each species as an
independent category, TaxaDiffusion incorporates domain knowledge that many
species exhibit strong visual similarities, with distinctions often residing in
subtle variations of shape, pattern, and color. To exploit these relationships,
TaxaDiffusion progressively trains conditioned diffusion models across
different taxonomic levels -- starting from broad classifications such as Class
and Order, refining through Family and Genus, and ultimately distinguishing at
the Species level. This hierarchical learning strategy first captures
coarse-grained morphological traits shared by species with common ancestors,
facilitating knowledge transfer before refining fine-grained differences for
species-level distinction. As a result, TaxaDiffusion enables accurate
generation even with limited training samples per species. Extensive
experiments on three fine-grained animal datasets demonstrate that outperforms
existing approaches, achieving superior fidelity in fine-grained animal image
generation. Project page: https://amink8.github.io/TaxaDiffusion/

</details>


### [717] [MedEBench: Revisiting Text-instructed Image Editing](https://arxiv.org/abs/2506.01921)
*Minghao Liu,Zhitao He,Zhiyuan Fan,Qingyun Wang,Yi R. Fung*

Key words: 文本引导编辑, 医学影像, 评估基准, 临床相关性, 注意力机制

TL;DR: MedEBench是一个评估文本引导医学图像编辑的综合基准，包含1,182个临床图像-提示对，覆盖13个解剖区域，旨在填补该领域的标准化评估空白。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 文本引导的图像编辑在自然图像领域发展迅速，但在医学影像中应用有限且缺乏标准化评估，而临床上有模拟手术结果、个性化教学和患者沟通等潜在价值。

Method: 提出了MedEBench基准，包括1,182个临床图像-提示对，覆盖70个任务的13个解剖区域，提供编辑准确性、上下文保留和视觉质量的评估框架，并比较7种最先进模型的性能。

Result: 通过注意力机制分析失败模式，发现常见问题并提出了基于IoU（交并比）的定位错误分析方法。

Conclusion: MedEBench为开发可靠的、临床意义显著的医学图像编辑系统提供了坚实基础。

Abstract: Text-guided image editing has seen rapid progress in natural image domains,
but its adaptation to medical imaging remains limited and lacks standardized
evaluation. Clinically, such editing holds promise for simulating surgical
outcomes, creating personalized teaching materials, and enhancing patient
communication. To bridge this gap, we introduce \textbf{MedEBench}, a
comprehensive benchmark for evaluating text-guided medical image editing. It
consists of 1,182 clinically sourced image-prompt triplets spanning 70 tasks
across 13 anatomical regions. MedEBench offers three key contributions: (1) a
clinically relevant evaluation framework covering Editing Accuracy, Contextual
Preservation, and Visual Quality, supported by detailed descriptions of
expected change and ROI (Region of Interest) masks; (2) a systematic comparison
of seven state-of-the-art models, revealing common failure patterns; and (3) a
failure analysis protocol based on attention grounding, using IoU between
attention maps and ROIs to identify mislocalization. MedEBench provides a solid
foundation for developing and evaluating reliable, clinically meaningful
medical image editing systems.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [718] [The Folly of AI for Age Verification](https://arxiv.org/abs/2506.00038)
*Reid McIlroy-Young*

Key words: AI年龄验证, 偏见, 技术限制, 公平性

TL;DR: 论文指出，使用AI进行年龄验证存在严重问题，包括易被绕过以及对少数群体和低社会经济地位用户的不公平分类。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 政府可能允许企业使用AI进行年龄验证，但作者认为这将导致不公平的结果。

Method: 通过分析类似技术（如人脸识别和远程监考软件）的问题，预测AI年龄验证的局限性。

Result: AI模型和硬件的技术限制导致偏见，难以低成本地克服这些问题。

Conclusion: 在近期部署AI年龄验证系统是不明智的。

Abstract: In the near future a governmental body will be asked to allow companies to
use AI for age verification. If they allow it the resulting system will both be
easily circumvented and disproportionately misclassify minorities and low
socioeconomic status users. This is predictable by showing that other very
similar systems (facial recognition and remote proctoring software) have
similar issues despite years of efforts to mitigate their biases. These biases
are due to technical limitations both of the AI models themselves and the
physical hardware they are running on that will be difficult to overcome below
the cost of government ID-based age verification. Thus in, the near future,
deploying an AI system for age verification is folly.

</details>


### [719] [Risks of AI-driven product development and strategies for their mitigation](https://arxiv.org/abs/2506.00047)
*Jan Göpfert,Jann M. Weinand,Patrick Kuckertz,Noah Pflugradt,Jochen Linßen*

Key words: 自动化产品开发、AI驱动、风险管理、人类监督、可解释设计

TL;DR: 论文摘要探讨了自动化产品开发中AI驱动带来的风险及其缓解策略，提出了强调人类监督、责任和可解释设计的原则。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 自动化产品开发速度快、质量高，但依赖非人类代理引入风险，需要平衡机遇与风险。

Method: 提出了AI驱动产品开发的安全原则，包括人类监督、责任和可解释设计等。

Result: 分析技术和社会的双重风险，为未来的理解、规范制定和监管提供基础。

Conclusion: 尽管处于早期阶段，讨论有助于平衡AI驱动产品开发的机遇与风险。

Abstract: Humanity is progressing towards automated product development, a trend that
promises faster creation of better products and thus the acceleration of
technological progress. However, increasing reliance on non-human agents for
this process introduces many risks. This perspective aims to initiate a
discussion on these risks and appropriate mitigation strategies. To this end,
we outline a set of principles for safer AI-driven product development which
emphasize human oversight, accountability, and explainable design, among
others. The risk assessment covers both technical risks which affect product
quality and safety, and sociotechnical risks which affect society. While
AI-driven product development is still in its early stages, this discussion
will help balance its opportunities and risks without delaying essential
progress in understanding, norm-setting, and regulation.

</details>


### [720] [Prompt Engineer: Analyzing Skill Requirements in the AI Job Market](https://arxiv.org/abs/2506.00058)
*An Vu,Jonas Oppenlaender*

Key words: 大语言模型, 提示工程师, 技能分析, 职业市场

TL;DR: 该论文分析了提示工程师这一新兴职位的技能需求和市场现状，发现其具有独特的技能组合，并逐渐形成独立职业。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 研究动机是了解提示工程师职位的技能需求和市场普及程度，填补对这一新兴角色认识的空白。

Method: 方法是通过分析LinkedIn上的20,662个职位招聘信息，其中包括72个提示工程师职位。

Result: 结果显示提示工程师职位仍较罕见（占样本的0.5%以下），但具备独特的技能组合，如AI知识（22.8%）、提示设计技能（18.7%）、沟通能力（21.9%）和创意问题解决能力（15.8%）。

Conclusion: 结论是提示工程师正成为一门独立职业，研究成果有助于求职者、雇主和教育机构更好地理解这一新兴领域。

Abstract: The rise of large language models (LLMs) has created a new job role: the
Prompt Engineer. Despite growing interest in this position, we still do not
fully understand what skills this new job role requires or how common these
jobs are. We analyzed 20,662 job postings on LinkedIn, including 72 prompt
engineer positions, to learn more about this emerging role. We found that
prompt engineering is still rare (less than 0.5% of sampled job postings) but
has a unique skill profile. Prompt engineers need AI knowledge (22.8%), prompt
design skills (18.7%), good communication (21.9%), and creative problem-solving
(15.8%) skills. These requirements significantly differ from those of
established roles, such as data scientists and machine learning engineers,
showing that prompt engineering is becoming its own profession. Our findings
help job seekers, employers, and educational institutions in better
understanding the emerging field of prompt engineering.

</details>


### [721] [Comparative analysis of privacy-preserving open-source LLMs regarding extraction of diagnostic information from clinical CMR imaging reports](https://arxiv.org/abs/2506.00060)
*Sina Amirrajab,Volker Vehof,Michael Bietenbeck,Ali Yilmaz*

Key words: 开源大语言模型,心血管磁共振,隐私保护,临床报告分析,诊断分类

TL;DR: 研究探讨开源隐私保护本地部署的大语言模型在心血管磁共振报告诊断分类中的表现，结果优异。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 探索如何利用本地部署的开源大语言模型从自由文本的心血管磁共振报告中提取诊断信息，以提升临床效率和隐私保护。

Method: 评估了九种开源大语言模型在109份临床心血管磁共振报告中的诊断分类能力，使用准确性、精确率、召回率和F1分数等指标进行量化。

Result: 多数模型表现优异，Google的Gemma2模型F1分数最高（0.98），部分模型甚至超过专业心脏科医生（F1分数0.94）。

Conclusion: 研究表明开源隐私保护大语言模型能高效、准确地实现影像报告的自动化分析，适用于临床环境。

Abstract: Purpose: We investigated the utilization of privacy-preserving,
locally-deployed, open-source Large Language Models (LLMs) to extract
diagnostic information from free-text cardiovascular magnetic resonance (CMR)
reports. Materials and Methods: We evaluated nine open-source LLMs on their
ability to identify diagnoses and classify patients into various cardiac
diagnostic categories based on descriptive findings in 109 clinical CMR
reports. Performance was quantified using standard classification metrics
including accuracy, precision, recall, and F1 score. We also employed confusion
matrices to examine patterns of misclassification across models. Results: Most
open-source LLMs demonstrated exceptional performance in classifying reports
into different diagnostic categories. Google's Gemma2 model achieved the
highest average F1 score of 0.98, followed by Qwen2.5:32B and DeepseekR1-32B
with F1 scores of 0.96 and 0.95, respectively. All other evaluated models
attained average scores above 0.93, with Mistral and DeepseekR1-7B being the
only exceptions. The top four LLMs outperformed our board-certified
cardiologist (F1 score of 0.94) across all evaluation metrics in analyzing CMR
reports. Conclusion: Our findings demonstrate the feasibility of implementing
open-source, privacy-preserving LLMs in clinical settings for automated
analysis of imaging reports, enabling accurate, fast and resource-efficient
diagnostic categorization.

</details>


### [722] [Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs](https://arxiv.org/abs/2506.00072)
*Nariman Naderi,Zahra Atf,Peter R Lewis,Aref Mahjoub far,Seyed Amir Ahmad Safavi-Naini,Ali Soroush*

Key words: 提示工程，大型语言模型，医学，置信度，校准

TL;DR: 研究了提示工程对大型语言模型在医学任务中准确性和置信度的影响，发现不同提示风格和模型配置对结果有显著差异。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 探讨提示工程技术如何影响大型语言模型在医学语境下的准确性和置信度表现，为高风险任务提供优化方向。

Method: 使用波斯医学考试数据集，测试五种模型的156种配置（温度、提示风格、置信度量表），通过AUC-ROC、Brier Score和ECE评估。

Result: 思维链提示提高准确性但导致过度自信；情感提示进一步加剧置信度偏差；小模型表现不佳，专有模型准确性高但置信度未校准。

Conclusion: 提示工程需同时优化准确性和不确定性校准，以适用于高风险的医学任务。

Abstract: This paper investigates how prompt engineering techniques impact both
accuracy and confidence elicitation in Large Language Models (LLMs) applied to
medical contexts. Using a stratified dataset of Persian board exam questions
across multiple specialties, we evaluated five LLMs - GPT-4o, o3-mini,
Llama-3.3-70b, Llama-3.1-8b, and DeepSeek-v3 - across 156 configurations. These
configurations varied in temperature settings (0.3, 0.7, 1.0), prompt styles
(Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales
(1-10, 1-100). We used AUC-ROC, Brier Score, and Expected Calibration Error
(ECE) to evaluate alignment between confidence and actual performance.
Chain-of-Thought prompts improved accuracy but also led to overconfidence,
highlighting the need for calibration. Emotional prompting further inflated
confidence, risking poor decisions. Smaller models like Llama-3.1-8b
underperformed across all metrics, while proprietary models showed higher
accuracy but still lacked calibrated confidence. These results suggest prompt
engineering must address both accuracy and uncertainty to be effective in
high-stakes medical tasks.

</details>


### [723] [Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations](https://arxiv.org/abs/2506.00074)
*Daniele Barolo,Chiara Valentin,Fariba Karimi,Luis Galárraga,Gonzalo G. Méndez,Lisette Espín-Noboa*

Key words: LLMs, 专家推荐, 一致性, 事实性, 偏见

TL;DR: 该论文评估了六种开源大语言模型在物理学领域专家推荐任务中的表现，发现模型存在一致性问题、事实错误及多种偏见，需改进以提高可靠性和公平性。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 研究动机是评估开源LLMs在专家推荐任务中的表现，分析其一致性、事实性和潜在偏见，以推动更可靠的学术推荐系统发展。

Method: 通过比较模型输出与美国物理学会和OpenAlex的真实学术数据，评估六种LLMs在五个专家推荐任务中的表现。

Result: 分析显示模型普遍存在不一致性和偏见，mixtral-8x7b输出最稳定，llama3.1-70b变异性最高。此外，模型在性别、种族和学术影响力等方面表现出明显偏差。

Conclusion: 结论指出当前LLMs在学术推荐中仍存在诸多问题，需进一步优化以减少偏见并提升准确性。

Abstract: This paper evaluates the performance of six open-weight LLMs (llama3-8b,
llama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending
experts in physics across five tasks: top-k experts by field, influential
scientists by discipline, epoch, seniority, and scholar counterparts. The
evaluation examines consistency, factuality, and biases related to gender,
ethnicity, academic popularity, and scholar similarity. Using ground-truth data
from the American Physical Society and OpenAlex, we establish scholarly
benchmarks by comparing model outputs to real-world academic records. Our
analysis reveals inconsistencies and biases across all models. mixtral-8x7b
produces the most stable outputs, while llama3.1-70b shows the highest
variability. Many models exhibit duplication, and some, particularly gemma2-9b
and llama3.1-8b, struggle with formatting errors. LLMs generally recommend real
scientists, but accuracy drops in field-, epoch-, and seniority-specific
queries, consistently favoring senior scholars. Representation biases persist,
replicating gender imbalances (reflecting male predominance),
under-representing Asian scientists, and over-representing White scholars.
Despite some diversity in institutional and collaboration networks, models
favor highly cited and productive scholars, reinforcing the rich-getricher
effect while offering limited geographical representation. These findings
highlight the need to improve LLMs for more reliable and equitable scholarly
recommendations.

</details>


### [724] [Optimizing Storytelling, Improving Audience Retention, and Reducing Waste in the Entertainment Industry](https://arxiv.org/abs/2506.00076)
*Andrew Cornfeld,Ashley Miller,Mercedes Mora-Figueroa,Kurt Samuels,Anthony Palomba*

Key words: 收视率预测,NLP,机器学习,内容相似性

TL;DR: 论文提出了一种结合NLP和传统收视率数据的机器学习框架，用于提升电视剧集收视率预测的准确性。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 面对节目决策的高财务风险，电视台需要更准确的收视率预测方法，但传统方法依赖有限的历史数据。

Method: 通过提取剧集对话的情感基调、认知复杂度和叙事结构等NLP特征，结合传统收视数据，使用SARIMAX、XGBoost等模型进行预测。

Result: NLP特征对某些剧集的预测能力有所提升，并提出了基于对话向量相似性的评分方法。

Conclusion: 该框架在多类型剧集中表现良好，为内容创作者和管理者提供了数据驱动的见解。

Abstract: Television networks face high financial risk when making programming
decisions, often relying on limited historical data to forecast episodic
viewership. This study introduces a machine learning framework that integrates
natural language processing (NLP) features from over 25000 television episodes
with traditional viewership data to enhance predictive accuracy. By extracting
emotional tone, cognitive complexity, and narrative structure from episode
dialogue, we evaluate forecasting performance using SARIMAX, rolling XGBoost,
and feature selection models. While prior viewership remains a strong baseline
predictor, NLP features contribute meaningful improvements for some series. We
also introduce a similarity scoring method based on Euclidean distance between
aggregate dialogue vectors to compare shows by content. Tested across diverse
genres, including Better Call Saul and Abbott Elementary, our framework reveals
genre-specific performance and offers interpretable metrics for writers,
executives, and marketers seeking data-driven insight into audience behavior.

</details>


### [725] [Who Gets the Kidney? Human-AI Alignment, Indecision, and Moral Values](https://arxiv.org/abs/2506.00079)
*John P. Dickerson,Hadi Hosseini,Samarth Khanna,Leona Pierce*

Key words: 大型语言模型,道德对齐,器官分配,决策一致性,低秩微调

TL;DR: 研究评估了大型语言模型（LLMs）在肾脏分配决策中与人类价值观的对齐问题，发现LLMs存在显著偏差，且缺乏人类常见的犹豫机制。通过少量样本的低秩监督微调可改进决策一致性和犹豫校准。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 探究LLMs在高风险决策（如器官分配）中是否与人类道德价值观对齐。

Method: 系统评估多个LLMs在肾脏分配场景中的行为，并与人类偏好对比；采用低秩监督微调改进模型行为。

Result: LLMs在属性优先级上与人类价值观存在显著偏差，且几乎不表达犹豫；少量样本的微调可显著改进模型表现。

Conclusion: LLMs在道德/伦理领域需要显式的对齐策略以确保与人类价值观一致。

Abstract: The rapid integration of Large Language Models (LLMs) in high-stakes
decision-making -- such as allocating scarce resources like donor organs --
raises critical questions about their alignment with human moral values. We
systematically evaluate the behavior of several prominent LLMs against human
preferences in kidney allocation scenarios and show that LLMs: i) exhibit stark
deviations from human values in prioritizing various attributes, and ii) in
contrast to humans, LLMs rarely express indecision, opting for deterministic
decisions even when alternative indecision mechanisms (e.g., coin flipping) are
provided. Nonetheless, we show that low-rank supervised fine-tuning with few
samples is often effective in improving both decision consistency and
calibrating indecision modeling. These findings illustrate the necessity of
explicit alignment strategies for LLMs in moral/ethical domains.

</details>


### [726] [Bottom-Up Perspectives on AI Governance: Insights from User Reviews of AI Products](https://arxiv.org/abs/2506.00080)
*Stefan Pasch*

Key words: AI治理, 用户评论, BERTopic, 实践主题, 数字政策

TL;DR: 研究采用自下而上的方法，通过分析用户评论探讨AI治理的实践主题，揭示与技术与非技术领域相关的治理问题。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 现有AI治理框架多为高层指导，未能充分反映实际应用中的问题，研究试图填补这一空白。

Method: 利用BERTopic从10万+AI产品用户评论中提取与AI治理语义相关的潜在主题。

Result: 发现治理主题涵盖技术与非技术领域，部分与现有框架重叠，但也揭示了如项目管理等被忽视的方面。

Conclusion: 需更多实证和用户中心的AI治理方法，以补充现有规范模型。

Abstract: With the growing importance of AI governance, numerous high-level frameworks
and principles have been articulated by policymakers, institutions, and expert
communities to guide the development and application of AI. While such
frameworks offer valuable normative orientation, they may not fully capture the
practical concerns of those who interact with AI systems in organizational and
operational contexts. To address this gap, this study adopts a bottom-up
approach to explore how governance-relevant themes are expressed in user
discourse. Drawing on over 100,000 user reviews of AI products from G2.com, we
apply BERTopic to extract latent themes and identify those most semantically
related to AI governance. The analysis reveals a diverse set of
governance-relevant topics spanning both technical and non-technical domains.
These include concerns across organizational processes-such as planning,
coordination, and communication-as well as stages of the AI value chain,
including deployment infrastructure, data handling, and analytics. The findings
show considerable overlap with institutional AI governance and ethics
frameworks on issues like privacy and transparency, but also surface overlooked
areas such as project management, strategy development, and customer
interaction. This highlights the need for more empirically grounded,
user-centered approaches to AI governance-approaches that complement normative
models by capturing how governance unfolds in applied settings. By
foregrounding how governance is enacted in practice, this study contributes to
more inclusive and operationally grounded approaches to AI governance and
digital policy.

</details>


### [727] [TRAPDOC: Deceiving LLM Users by Injecting Imperceptible Phantom Tokens into Documents](https://arxiv.org/abs/2506.00089)
*Hyundong Jin,Sicheol Sung,Shinwoo Park,SeungYeop Baik,Yo-Sub Han*

Key words: 大语言模型、滥用、幻影标记、TRAPDOC框架、社会责任

TL;DR: 论文提出了一种通过在文档中注入不可察觉的“幻影标记”来欺骗过度依赖大语言模型（LLM）用户的方法，并介绍了TRAPDOC框架。该框架能引导LLM生成看似合理实则错误的输出，旨在减少用户对LLM的滥用。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 随着大语言模型功能的扩展，用户对其过度依赖日益严重，尤其是在作业、敏感文档处理等方面。这种依赖可能导致社会问题，因此需要一种方法限制滥用。

Method: 通过在文档中注入不可察觉的幻影标记，使得LLM生成看似合理但实际错误的输出，从而欺骗过度依赖的用户。

Result: TRAPDOC框架在实验中显示了对专有LLM的有效性，并通过与基线的比较验证了其效果。

Conclusion: TRAPDOC为促进更负责任和审慎的语言模型使用提供了坚实的基础。

Abstract: The reasoning, writing, text-editing, and retrieval capabilities of
proprietary large language models (LLMs) have advanced rapidly, providing users
with an ever-expanding set of functionalities. However, this growing utility
has also led to a serious societal concern: the over-reliance on LLMs. In
particular, users increasingly delegate tasks such as homework, assignments, or
the processing of sensitive documents to LLMs without meaningful engagement.
This form of over-reliance and misuse is emerging as a significant social
issue. In order to mitigate these issues, we propose a method injecting
imperceptible phantom tokens into documents, which causes LLMs to generate
outputs that appear plausible to users but are in fact incorrect. Based on this
technique, we introduce TRAPDOC, a framework designed to deceive over-reliant
LLM users. Through empirical evaluation, we demonstrate the effectiveness of
our framework on proprietary LLMs, comparing its impact against several
baselines. TRAPDOC serves as a strong foundation for promoting more responsible
and thoughtful engagement with language models. Our code is available at
https://github.com/jindong22/TrapDoc.

</details>


### [728] [Feeling Guilty Being a c(ai)borg: Navigating the Tensions Between Guilt and Empowerment in AI Use](https://arxiv.org/abs/2506.00094)
*Konstantin Aal,Tanja Aal,Vasil Navumau,David Unbehaun,Claudia Müller,Volker Wulf,Sarah Rüller*

Key words: 人工智能，伦理，自我民族志，协作，AI素养

TL;DR: 论文探讨了AI融入工作流程的情感、伦理和实践层面，提出了‘c(ai)borg’概念，呼吁将AI视为协作伙伴，强调技能培养和透明使用的重要性。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 研究AI对人类创造力、原创性和智力劳动的挑战，探索AI辅助下的人类身份转变。

Method: 采用自我民族志方法，通过作者一年的AI工具使用经验进行分析。

Result: 从初始的愧疚感转变为通过技能提升和透明使用获得赋能，强调基础学术能力与高级AI素养的重要性。

Conclusion: 倡导开放拥抱AI的未来，将其作为协作伙伴，促进创新与公平，同时解决可访问性和自主性问题。

Abstract: This paper explores the emotional, ethical and practical dimensions of
integrating Artificial Intelligence (AI) into personal and professional
workflows, focusing on the concept of feeling guilty as a 'c(ai)borg' - a human
augmented by AI. Inspired by Donna Haraway's Cyborg Manifesto, the study
explores how AI challenges traditional notions of creativity, originality and
intellectual labour. Using an autoethnographic approach, the authors reflect on
their year-long experiences with AI tools, revealing a transition from initial
guilt and reluctance to empowerment through skill-building and transparency.
Key findings highlight the importance of basic academic skills, advanced AI
literacy and honest engagement with AI results. The c(ai)borg vision advocates
for a future where AI is openly embraced as a collaborative partner, fostering
innovation and equity while addressing issues of access and agency. By
reframing guilt as growth, the paper calls for a thoughtful and inclusive
approach to AI integration.

</details>


### [729] [ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases](https://arxiv.org/abs/2506.00095)
*Yuchong Li,Xiaojun Zeng,Chihua Fang,Jian Yang,Lei Zhang*

Key words: HPB疾病, 大型语言模型, 临床诊断, 评估基准, ICD-10

TL;DR: 本研究建立了针对肝胰胆（HPB）疾病的评估基准ClinBench-HBP，包含3,535个闭卷选择题和337个开放式真实诊断案例，覆盖ICD-10中所有33个主要类别和465个子类别。评估发现，现有商业和开源大型语言模型在HPB诊断任务上表现不佳，尤其在复杂临床案例中。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: HPB疾病的高发病率和死亡率使其成为全球公共卫生挑战，但目前大型语言模型（LLMs）的评估基准缺乏HPB相关内容和临床案例，因此需要建立专门的评估标准。

Method: 通过整合公开数据集和合成数据，以及从医学期刊、案例分享平台和合作医院收集的临床案例，系统构建了ClinBench-HBP基准。随后评估了商业和开源通用及医疗LLMs在该基准上的表现。

Result: 商业LLMs在医学考试题目上表现良好，但在HPB诊断任务上性能显著下降，尤其在复杂住院病例中。医疗LLMs对HPB疾病的泛化能力也有限。

Conclusion: 当前LLMs在HPB疾病领域存在明显局限，未来需要开发能够处理真实、复杂临床诊断的医疗LLMs，而不仅是简单的医学考试题目。

Abstract: Hepato-pancreato-biliary (HPB) disorders represent a global public health
challenge due to their high morbidity and mortality. Although large language
models (LLMs) have shown promising performance in general medical
question-answering tasks, the current evaluation benchmarks are mostly derived
from standardized examinations or manually designed questions, lacking HPB
coverage and clinical cases. To address these issues, we systematically
eatablish an HPB disease evaluation benchmark comprising 3,535 closed-ended
multiple-choice questions and 337 open-ended real diagnosis cases, which
encompasses all the 33 main categories and 465 subcategories of HPB diseases
defined in the International Statistical Classification of Diseases, 10th
Revision (ICD-10). The multiple-choice questions are curated from public
datasets and synthesized data, and the clinical cases are collected from
prestigious medical journals, case-sharing platforms, and collaborating
hospitals. By evalauting commercial and open-source general and medical LLMs on
our established benchmark, namely ClinBench-HBP, we find that while commercial
LLMs perform competently on medical exam questions, they exhibit substantial
performance degradation on HPB diagnosis tasks, especially on complex,
inpatient clinical cases. Those medical LLMs also show limited generalizability
to HPB diseases. Our results reveal the critical limitations of current LLMs in
the domain of HPB diseases, underscoring the imperative need for future medical
LLMs to handle real, complex clinical diagnostics rather than simple medical
exam questions. The benchmark will be released at the homepage.

</details>


### [730] [Children's Voice Privacy: First Steps And Emerging Challenges](https://arxiv.org/abs/2506.00100)
*Ajinkya Kulkarni,Francisco Teixeira,Enno Hermann,Thomas Rolland,Isabel Trancoso,Mathew Magimai Doss*

Key words: 儿童语音技术、语音匿名化、隐私保护、语音质量评估

TL;DR: 论文探讨了儿童语音匿名化技术的现状，评估了现有成人语音匿名化方法在儿童语音上的效果，发现虽然能保护隐私，但实用性下降较大，并指出自动评估儿童语音质量的挑战。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 儿童在语音技术中代表性不足且隐私保护需求高，但针对他们的语音匿名化技术研究较少，本研究旨在填补这一空白。

Method: 研究使用了三个儿童语音数据集和六种匿名化方法，结合客观与主观效用指标进行评估。

Result: 结果显示，现有成人语音匿名化技术能保护儿童隐私，但实用性显著下降；主观研究揭示了儿童语音自动评估的挑战。

Conclusion: 儿童语音匿名化技术需进一步研究，尤其是实用性提升和自动评估方法的改进。

Abstract: Children are one of the most under-represented groups in speech technologies,
as well as one of the most vulnerable in terms of privacy. Despite this,
anonymization techniques targeting this population have received little
attention. In this study, we seek to bridge this gap, and establish a baseline
for the use of voice anonymization techniques designed for adult speech when
applied to children's voices. Such an evaluation is essential, as children's
speech presents a distinct set of challenges when compared to that of adults.
This study comprises three children's datasets, six anonymization methods, and
objective and subjective utility metrics for evaluation. Our results show that
existing systems for adults are still able to protect children's voice privacy,
but suffer from much higher utility degradation. In addition, our subjective
study displays the challenges of automatic evaluation methods for speech
quality in children's speech, highlighting the need for further research.

</details>


### [731] [The World As Large Language Models See It: Exploring the reliability of LLMs in representing geographical features](https://arxiv.org/abs/2506.00203)
*Omid Reza Abbasi,Franz Welscher,Georg Weinberger,Johannes Scholz*

Key words: 大语言模型, 地理信息, GPT-4o, Gemini 2.0 Flash, 空间任务

TL;DR: 研究发现GPT-4o和Gemini 2.0 Flash在空间任务中表现不一致，Gemini 2.0 Flash总体更优，但地理信息准确性仍需改进。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 探讨大语言模型（LLMs）在地理空间任务中的可信度问题。

Method: 评估GPT-4o和Gemini 2.0 Flash在三种地理任务（地理编码、高程估计和反向地理编码）中的表现。

Result: 两模型在地理编码中均存在误差，Gemini 2.0 Flash表现更优；高程估计中均低估但趋势正确；反向地理编码中Gemini 2.0 Flash更精确。

Conclusion: LLMs能近似地理信息，但准确性不足，需增强地理数据以提高实用性。

Abstract: As large language models (LLMs) continue to evolve, questions about their
trustworthiness in delivering factual information have become increasingly
important. This concern also applies to their ability to accurately represent
the geographic world. With recent advancements in this field, it is relevant to
consider whether and to what extent LLMs' representations of the geographical
world can be trusted. This study evaluates the performance of GPT-4o and Gemini
2.0 Flash in three key geospatial tasks: geocoding, elevation estimation, and
reverse geocoding. In the geocoding task, both models exhibited systematic and
random errors in estimating the coordinates of St. Anne's Column in Innsbruck,
Austria, with GPT-4o showing greater deviations and Gemini 2.0 Flash
demonstrating more precision but a significant systematic offset. For elevation
estimation, both models tended to underestimate elevations across Austria,
though they captured overall topographical trends, and Gemini 2.0 Flash
performed better in eastern regions. The reverse geocoding task, which involved
identifying Austrian federal states from coordinates, revealed that Gemini 2.0
Flash outperformed GPT-4o in overall accuracy and F1-scores, demonstrating
better consistency across regions. Despite these findings, neither model
achieved an accurate reconstruction of Austria's federal states, highlighting
persistent misclassifications. The study concludes that while LLMs can
approximate geographic information, their accuracy and reliability are
inconsistent, underscoring the need for fine-tuning with geographical
information to enhance their utility in GIScience and Geoinformatics.

</details>


### [732] [MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform](https://arxiv.org/abs/2506.00308)
*Hayoung Jung,Shravika Mittal,Ananya Aatreya,Navreet Kaur,Munmun De Choudhury,Tanushree Mitra*

Key words: 阿片类使用障碍, YouTube, 健康谣言, MythTriage, 大型语言模型

TL;DR: 该论文首次大规模研究了YouTube上与阿片类使用障碍（OUD）相关的健康谣言，提出了MythTriage高效标注方法，并分析了谣言的传播模式。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 测量在线健康谣言对公共卫生政策和干预至关重要，但大规模测量存在挑战，尤其是针对阿片类使用障碍（OUD）这一高风险但研究不足的话题。

Method: 通过与临床专家合作验证8种常见谣言，并发布专家标注的视频数据集；提出MythTriage方法，结合轻量级模型和大型语言模型（LLM）进行高效标注。

Result: MythTriage的宏F1-score达到0.86，标注时间和成本降低76%以上；分析了2.9K搜索结果和343K推荐视频，揭示了谣言的传播机制。

Conclusion: 研究为公共卫生和平台内容审核提供了可行见解，展示了MythTriage在大规模谣言标注中的高效性。

Abstract: Understanding the prevalence of misinformation in health topics online can
inform public health policies and interventions. However, measuring such
misinformation at scale remains a challenge, particularly for high-stakes but
understudied topics like opioid-use disorder (OUD)--a leading cause of death in
the U.S. We present the first large-scale study of OUD-related myths on
YouTube, a widely-used platform for health information. With clinical experts,
we validate 8 pervasive myths and release an expert-labeled video dataset. To
scale labeling, we introduce MythTriage, an efficient triage pipeline that uses
a lightweight model for routine cases and defers harder ones to a
high-performing, but costlier, large language model (LLM). MythTriage achieves
up to 0.86 macro F1-score while estimated to reduce annotation time and
financial cost by over 76% compared to experts and full LLM labeling. We
analyze 2.9K search results and 343K recommendations, uncovering how myths
persist on YouTube and offering actionable insights for public health and
platform moderation.

</details>


### [733] [Wide Reflective Equilibrium in LLM Alignment: Bridging Moral Epistemology and AI Safety](https://arxiv.org/abs/2506.00415)
*Matthew Brophy*

Key words: 大型语言模型,对齐,广泛反思平衡法,宪法AI,伦理基础

TL;DR: 该论文探讨了如何利用广泛反思平衡法（MWRE）来增强大型语言模型（LLM）的对齐工作，指出其在动态可修订性、程序合法性和伦理基础方面的优势。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 当前的对齐方法（如宪法AI）过程复杂，MWRE作为一种成熟的道德方法论，可为LLM对齐提供更稳健的理论框架和实践路径。

Method: 通过MWRE方法论，强调道德判断、道德原则和背景理论间的连贯性，并动态双向修订原则，以提高对齐过程的程序合法性和伦理基础。

Result: MWRE为LLM对齐提供了更具辩护性和伦理指导意义的框架，优于当前的主流基础主义模型或简单输入输出评估。

Conclusion: MWRE可作为分析和改进LLM对齐工作的启发式工具，帮助开发更具伦理合理性的AI系统。

Abstract: As large language models (LLMs) become more powerful and pervasive across
society, ensuring these systems are beneficial, safe, and aligned with human
values is crucial. Current alignment techniques, like Constitutional AI (CAI),
involve complex iterative processes. This paper argues that the Method of Wide
Reflective Equilibrium (MWRE) -- a well-established coherentist moral
methodology -- offers a uniquely apt framework for understanding current LLM
alignment efforts. Moreover, this methodology can substantively augment these
processes by providing concrete pathways for improving their dynamic
revisability, procedural legitimacy, and overall ethical grounding. Together,
these enhancements can help produce more robust and ethically defensible
outcomes. MWRE, emphasizing the achievement of coherence between our considered
moral judgments, guiding moral principles, and relevant background theories,
arguably better represents the intricate reality of LLM alignment and offers a
more robust path to justification than prevailing foundationalist models or
simplistic input-output evaluations. While current methods like CAI bear a
structural resemblance to MWRE, they often lack its crucial emphasis on
dynamic, bi-directional revision of principles and the procedural legitimacy
derived from such a process. While acknowledging various disanalogies (e.g.,
consciousness, genuine understanding in LLMs), the paper demonstrates that MWRE
serves as a valuable heuristic for critically analyzing current alignment
efforts and for guiding the future development of more ethically sound and
justifiably aligned AI systems.

</details>


### [734] [SafeCOMM: What about Safety Alignment in Fine-Tuned Telecom Large Language Models?](https://arxiv.org/abs/2506.00062)
*Aladin Djuhera,Swanand Ravindra Kadhe,Farhan Ahmed,Syed Zawad,Holger Boche,Walid Saad*

Key words: LLM, 电信, 微调, 模型安全, 安全重校准

TL;DR: 本文研究了电信领域微调大型语言模型（LLM）对模型安全性的影响，并提出三种安全重校准方法以恢复安全性。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 探讨电信领域微调LLM时可能对模型安全性造成的负面影响，并提出解决方案。

Method: 使用三种代表性的电信数据集（3GPP标准和表格记录）评估微调后的模型安全性，并提出三种安全重校准防御方法（SafeInstruct、SafeLoRA和SafeMERGE）。

Result: 实验证明，三种防御方法能有效恢复模型安全性且不影响下游任务性能。

Conclusion: 电信领域微调LLM需关注安全性，提出的方法为安全重校准提供了实用指南。

Abstract: Fine-tuning large language models (LLMs) for telecom tasks and datasets is a
common practice to adapt general-purpose models to the telecom domain. However,
little attention has been paid to how this process may compromise model safety.
Recent research has shown that even benign fine-tuning can degrade the safety
alignment of LLMs, causing them to respond to harmful or unethical user
queries. In this paper, we investigate this issue for telecom-tuned LLMs using
three representative datasets featured by the GenAINet initiative. We show that
safety degradation persists even for structured and seemingly harmless datasets
such as 3GPP standards and tabular records, indicating that telecom-specific
data is not immune to safety erosion during fine-tuning. We further extend our
analysis to publicly available Telecom LLMs trained via continual pre-training,
revealing that safety alignment is often severely lacking, primarily due to the
omission of safety-focused instruction tuning. To address these issues in both
fine-tuned and pre-trained models, we conduct extensive experiments and
evaluate three safety realignment defenses (SafeInstruct, SafeLoRA, and
SafeMERGE) using established red-teaming benchmarks. The results show that,
across all settings, the proposed defenses can effectively restore safety after
harmful degradation without compromising downstream task performance, leading
to Safe teleCOMMunication (SafeCOMM) models. In a nutshell, our work serves as
a diagnostic study and practical guide for safety realignment in telecom-tuned
LLMs, and emphasizes the importance of safety-aware instruction and fine-tuning
for real-world deployments of Telecom LLMs.

</details>


### [735] [Hierarchical Bayesian Knowledge Tracing in Undergraduate Engineering Education](https://arxiv.org/abs/2506.00057)
*Yiwei Sun*

Key words: 贝叶斯模型；学习分析；个性化教学；工程教育；学生能力

TL;DR: 教育工作者通过分层贝叶斯模型分析学生数据，识别学习难点和学生能力差异，为个性化教学提供依据。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 解决大学工程入门课程中如何识别学生难点和满足多样化需求的挑战。

Method: 采用分层贝叶斯模型分析学生反应数据，量化技能难度和个体能力。

Result: 发现技能掌握的明确模式和学生亚组，揭示某些概念普遍困难，需要针对性支持。

Conclusion: 分层贝叶斯方法为教育者提供直观可靠的指标，支持个性化教学策略。

Abstract: Educators teaching entry-level university engineering modules face the
challenge of identifying which topics students find most difficult and how to
support diverse student needs effectively. This study demonstrates a rigorous
yet interpretable statistical approach -- hierarchical Bayesian modeling --
that leverages detailed student response data to quantify both skill difficulty
and individual student abilities. Using a large-scale dataset from an
undergraduate Statics course, we identified clear patterns of skill mastery and
uncovered distinct student subgroups based on their learning trajectories. Our
analysis reveals that certain concepts consistently present challenges,
requiring targeted instructional support, while others are readily mastered and
may benefit from enrichment activities. Importantly, the hierarchical Bayesian
method provides educators with intuitive, reliable metrics without sacrificing
predictive accuracy. This approach allows for data-informed decisions, enabling
personalized teaching strategies to improve student engagement and success. By
combining robust statistical methods with clear interpretability, this study
equips educators with actionable insights to better support diverse learner
populations.

</details>


### [736] [AIMSCheck: Leveraging LLMs for AI-Assisted Review of Modern Slavery Statements Across Jurisdictions](https://arxiv.org/abs/2506.01671)
*Adriana Eufrosina Bora,Akshatha Arodi,Duoyi Zhang,Jordan Bannister,Mirko Bronzi,Arsene Fansi Tchango,Md Abul Bashar,Richi Nayak,Kerrie Mengersen*

Key words: 现代奴隶制、合规评估、NLP、跨法域、AIMSCheck

TL;DR: 论文提出了两个新标注数据集AIMS.uk和AIMS.ca，并引入AIMSCheck框架，用于跨法域合规验证，实验表明其具备广泛应用的潜力。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 现代奴隶制法案要求企业披露其应对措施，但验证这些声明的复杂性高，且缺乏标注数据，跨法域通用性需研究。

Method: 与领域专家合作，创建UK和加拿大的新数据集，开发AIMSCheck框架，分解合规评估任务为三级。

Result: 基于澳大利亚数据集训练的模型在UK和加拿大表现良好，证明其跨法域应用潜力。

Conclusion: 发布数据集和框架可推动AI在合规评估中的应用，促进相关研究。

Abstract: Modern Slavery Acts mandate that corporations disclose their efforts to
combat modern slavery, aiming to enhance transparency and strengthen practices
for its eradication. However, verifying these statements remains challenging
due to their complex, diversified language and the sheer number of statements
that must be reviewed. The development of NLP tools to assist in this task is
also difficult due to a scarcity of annotated data. Furthermore, as modern
slavery transparency legislation has been introduced in several countries, the
generalizability of such tools across legal jurisdictions must be studied. To
address these challenges, we work with domain experts to make two key
contributions. First, we present AIMS.uk and AIMS.ca, newly annotated datasets
from the UK and Canada to enable cross-jurisdictional evaluation. Second, we
introduce AIMSCheck, an end-to-end framework for compliance validation.
AIMSCheck decomposes the compliance assessment task into three levels,
enhancing interpretability and practical applicability. Our experiments show
that models trained on an Australian dataset generalize well across UK and
Canadian jurisdictions, demonstrating the potential for broader application in
compliance monitoring. We release the benchmark datasets and AIMSCheck to the
public to advance AI-adoption in compliance assessment and drive further
research in this field.

</details>


### [737] [Explainable AI Systems Must Be Contestable: Here's How to Make It Happen](https://arxiv.org/abs/2506.01662)
*Catarina Moreira,Anna Palatkina,Dacia Braca,Dylan M. Walsh,Peter J. Leihn,Fang Chen,Nina C. Hubig*

Key words: 可解释AI, 可争议性, 法规合规, 模块化框架, 评估量表

TL;DR: 本文提出了可解释AI中“可争议性”的首个严格形式化定义，并设计了一个模块化框架和评估量表，通过案例研究验证了其有效性，帮助从业者实现法规要求。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 随着全球AI法规对系统安全的关注增加，可争议性成为强制性但定义模糊的保障措施。目前，可解释AI中的可争议性缺乏形式化定义和实现算法，从业者缺乏具体指导。

Method: 基于系统文献综述，本文提出首个与利益相关者需求和法规要求直接对齐的可争议性形式化定义，并设计了一个模块化框架和Contestability Assessment Scale评估量表。

Result: 通过多个案例研究，验证了框架的有效性，揭示了现有系统的不足，并展示了如何通过框架实现针对性改进。

Conclusion: 本文为从业者提供了将可争议性从法规理论转化为实践的工具，从而在AI系统中嵌入真正的补救和问责机制。

Abstract: As AI regulations around the world intensify their focus on system safety,
contestability has become a mandatory, yet ill-defined, safeguard. In XAI,
"contestability" remains an empty promise: no formal definition exists, no
algorithm guarantees it, and practitioners lack concrete guidance to satisfy
regulatory requirements. Grounded in a systematic literature review, this paper
presents the first rigorous formal definition of contestability in explainable
AI, directly aligned with stakeholder requirements and regulatory mandates. We
introduce a modular framework of by-design and post-hoc mechanisms spanning
human-centered interfaces, technical architectures, legal processes, and
organizational workflows. To operationalize our framework, we propose the
Contestability Assessment Scale, a composite metric built on more than twenty
quantitative criteria. Through multiple case studies across diverse application
domains, we reveal where state-of-the-art systems fall short and show how our
framework drives targeted improvements. By converting contestability from
regulatory theory into a practical framework, our work equips practitioners
with the tools to embed genuine recourse and accountability into AI systems.

</details>


### [738] [Systematic Hazard Analysis for Frontier AI using STPA](https://arxiv.org/abs/2506.01782)
*Simon Mylius*

Key words: 前沿AI, 安全框架, STPA, 风险分析, 控制安全

TL;DR: 前沿AI公司已发布安全框架，但缺乏详细的结构化风险分析方法。STPA方法可提升AI系统的安全保证，通过系统性分析识别潜在风险。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 前沿AI公司当前的安全框架缺乏结构化风险识别方法，需引入系统性方法以增强安全性。

Method: 使用STPA方法分析AI系统的控制器与受控过程，识别不安全控制行为及损失场景。

Result: STPA能识别非结构化方法可能遗漏的因果因素，提升安全保证的稳健性。

Conclusion: STPA可补充现有AI治理技术，提高安全保证，并通过LLM实现分析的可扩展性。

Abstract: All of the frontier AI companies have published safety frameworks where they
define capability thresholds and risk mitigations that determine how they will
safely develop and deploy their models. Adoption of systematic approaches to
risk modelling, based on established practices used in safety-critical
industries, has been recommended, however frontier AI companies currently do
not describe in detail any structured approach to identifying and analysing
hazards. STPA (Systems-Theoretic Process Analysis) is a systematic methodology
for identifying how complex systems can become unsafe, leading to hazards. It
achieves this by mapping out controllers and controlled processes then
analysing their interactions and feedback loops to understand how harmful
outcomes could occur (Leveson & Thomas, 2018). We evaluate STPA's ability to
broaden the scope, improve traceability and strengthen the robustness of safety
assurance for frontier AI systems. Applying STPA to the threat model and
scenario described in 'A Sketch of an AI Control Safety Case' (Korbak et al.,
2025), we derive a list of Unsafe Control Actions. From these we select a
subset and explore the Loss Scenarios that lead to them if left unmitigated. We
find that STPA is able to identify causal factors that may be missed by
unstructured hazard analysis methodologies thereby improving robustness. We
suggest STPA could increase the safety assurance of frontier AI when used to
complement or check coverage of existing AI governance techniques including
capability thresholds, model evaluations and emergency procedures. The
application of a systematic methodology supports scalability by increasing the
proportion of the analysis that could be conducted by LLMs, reducing the burden
on human domain experts.

</details>


### [739] [Red Teaming AI Policy: A Taxonomy of Avoision and the EU AI Act](https://arxiv.org/abs/2506.01931)
*Rui-Jie Yew,Bill Marino,Suresh Venkatasubramanian*

Key words: AI监管，欧盟AI法案，avoision，规避策略，红队测试

TL;DR: 这篇论文提出了一个框架和分类法，用于分析企业在面对欧盟AI法案（AIA）时可能采取的介于合法避税与非法逃税之间的行为（称为“avoision”）。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 研究企业如何通过“avoision”策略最小化AIA带来的监管负担，为未来AI监管的“红队测试”提供对抗性框架。

Method: 围绕AIA的三层风险等级（在范围内、豁免、高监管）分类企业行为，并分析每层的组织和技术表现。

Result: 提出了一套策略和形式，用于预测企业对AIA的规避行为。

Conclusion: 通过框架和分类法，论文为监管机构和政策制定者提供了预测和应对企业规避策略的工具。

Abstract: The shape of AI regulation is beginning to emerge, most prominently through
the EU AI Act (the "AIA"). By 2027, the AIA will be in full effect, and firms
are starting to adjust their behavior in light of this new law. In this paper,
we present a framework and taxonomy for reasoning about "avoision" -- conduct
that walks the line between legal avoidance and evasion -- that firms might
engage in so as to minimize the regulatory burden the AIA poses. We organize
these avoision strategies around three "tiers" of increasing AIA exposure that
regulated entities face depending on: whether their activities are (1) within
scope of the AIA, (2) exempted from provisions of the AIA, or are (3) placed in
a category with higher regulatory scrutiny. In each of these tiers and for each
strategy, we specify the organizational and technological forms through which
avoision may manifest. Our goal is to provide an adversarial framework for "red
teaming" the AIA and AI regulation on the horizon.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [740] [Retrieval-Augmented Generation of Ontologies from Relational Databases](https://arxiv.org/abs/2506.01232)
*Mojtaba Nayyeri,Athish A Yogi,Nadeen Fathallah,Ratan Bahadur Thapa,Hans-Michael Tautenhahn,Anton Schnurpel,Steffen Staab*

Key words: 知识图谱, 本体生成, 关系数据库, LLM, RAG

TL;DR: RIGOR是一种基于LLM的方法，利用RAG技术从关系数据库生成丰富的OWL本体，显著减少手动工作量，并在多个质量维度上表现优异。

<details>
  <summary>Details</summary>

Main category: cs.DB

Motivation: 传统方法在从关系数据库生成本体时需要大量手动工作或只能生成基础本体，影响了语义互操作性和高级图学习与推理的能力。

Method: RIGOR结合数据库模式、文档、域本体库和核心本体，通过LLM生成增量本体片段，并由另一LLM进行优化，迭代处理直至覆盖完整数据库。

Result: RIGOR在准确性、完整性、简洁性、适应性、清晰性和一致性等标准质量维度上表现优异，同时大幅减少手动工作。

Conclusion: RIGOR提供了一种高效且高质量的方法，将关系数据库转化为丰富的本体，为知识图谱的构建提供了新思路。

Abstract: Transforming relational databases into knowledge graphs with enriched
ontologies enhances semantic interoperability and unlocks advanced graph-based
learning and reasoning over data. However, previous approaches either demand
significant manual effort to derive an ontology from a database schema or
produce only a basic ontology. We present RIGOR, Retrieval-augmented Iterative
Generation of RDB Ontologies, an LLM-driven approach that turns relational
schemas into rich OWL ontologies with minimal human effort. RIGOR combines
three sources via RAG, the database schema and its documentation, a repository
of domain ontologies, and a growing core ontology, to prompt a generative LLM
for producing successive, provenance-tagged delta ontology fragments. Each
fragment is refined by a judge-LLM before being merged into the core ontology,
and the process iterates table-by-table following foreign key constraints until
coverage is complete. Applied to real-world databases, our approach outputs
ontologies that score highly on standard quality dimensions such as accuracy,
completeness, conciseness, adaptability, clarity, and consistency, while
substantially reducing manual effort.

</details>


### [741] [SIFBench: An Extensive Benchmark for Fatigue Analysis](https://arxiv.org/abs/2506.01173)
*Tushar Gautam,Robert M. Kirby,Jacob Hochhalter,Shandian Zhe*

Key words: 疲劳裂纹增长,应力强度因子,机器学习,数据集,有限元模拟

TL;DR: SIFBench是一个开源的大规模基准数据库，旨在支持基于机器学习的应力强度因子预测，填补了高质量数据集的空白。

<details>
  <summary>Details</summary>

Main category: cs.DB

Motivation: 疲劳裂纹增长是结构失效的主要原因，而精确预测应力强度因子对评估疲劳寿命和确保结构完整性至关重要。目前机器学习在该领域的应用因缺乏高质量数据集而受限。

Method: 通过高保真有限元模拟生成超过500万个裂纹和组件几何数据，涵盖37种不同场景，并提供了Python接口以方便数据访问和定制。

Result: 使用多种机器学习模型（如随机森林、SVM、神经网络等）进行了基准测试，并提供了评估指标和模板代码。

Conclusion: SIFBench降低了进入门槛，推动了机器学习在损伤容限设计和预测性维护中的应用。

Abstract: Fatigue-induced crack growth is a leading cause of structural failure across
critical industries such as aerospace, civil engineering, automotive, and
energy. Accurate prediction of stress intensity factors (SIFs) -- the key
parameters governing crack propagation in linear elastic fracture mechanics --
is essential for assessing fatigue life and ensuring structural integrity.
While machine learning (ML) has shown great promise in SIF prediction, its
advancement has been severely limited by the lack of rich, transparent,
well-organized, and high-quality datasets.
  To address this gap, we introduce SIFBench, an open-source, large-scale
benchmark database designed to support ML-based SIF prediction. SIFBench
contains over 5 million different crack and component geometries derived from
high-fidelity finite element simulations across 37 distinct scenarios, and
provides a unified Python interface for seamless data access and customization.
We report baseline results using a range of popular ML models -- including
random forests, support vector machines, feedforward neural networks, and
Fourier neural operators -- alongside comprehensive evaluation metrics and
template code for model training, validation, and assessment. By offering a
standardized and scalable resource, SIFBench substantially lowers the entry
barrier and fosters the development and application of ML methods in damage
tolerance design and predictive maintenance.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [742] [Advanced Nanostructured Topical Therapeutics for Psoriasis: Strategic Synthesis, Multimodal Characterization, and Preliminary Pharmacodynamic Profiling](https://arxiv.org/abs/2506.01572)
*Iqra Yousaf,Aqsa Yousaf*

Key words: 银屑病;金属氧化物纳米颗粒;植物提取物;局部治疗;动物模型

TL;DR: 研究开发了一种结合金属氧化物纳米颗粒与植物提取物的新型局部凝胶治疗银屑病，动物实验显示显著的治疗效果。

<details>
  <summary>Details</summary>

Main category: physics.med-ph

Motivation: 银屑病是一种难以治疗的慢性炎症性皮肤疾病，需要探索新的治疗方法。

Method: 通过结合氧化铈、氧化锌和银纳米颗粒与苦瓜、姜和印楝等植物提取物，制备了鱼胶原和琼脂凝胶。纳米颗粒通过多种技术表征，并在银屑病动物模型中测试治疗效果。

Result: 治疗组表现出更快的伤口愈合和炎症减少，统计结果显著优于对照组。

Conclusion: 纳米颗粒与植物成分结合的局部凝胶可能是治疗银屑病的有前景的新方法，需进一步研究其长期安全性和疗效。

Abstract: Psoriasis is a long-term inflammatory skin disease that remains difficult to
treat. In this study, we developed a new topical treatment by combining metal
oxide nanoparticles: cerium oxide (CeO2), zinc oxide (ZnO), and silver (Ag),
with natural plant extracts in a gel made from fish collagen and agar. The
nanoparticles were characterized using UV-Vis spectroscopy, dynamic light
scattering (DLS), Fourier-transform infrared spectroscopy (FTIR), and scanning
electron microscopy (SEM), showing good stability and a uniform particle size
distribution (ZnO averaged 66 nm).
  To enhance therapeutic potential, the gel was enriched with plant-derived
antioxidants from bitter melon, ginger, and neem. This formulation was tested
on an animal model of psoriasis. The treated group exhibited faster wound
healing and reduced inflammation compared to both placebo and untreated groups,
with statistically significant results (p < 0.01 to p < 0.001) observed from
Day 3, becoming more pronounced by Day 14.
  These results indicate that the combination of nanoparticles with plant-based
components in a topical gel may provide a promising new approach to psoriasis
treatment. Further studies are recommended to evaluate long-term safety and
therapeutic effectiveness.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [743] [Tensor Network for Anomaly Detection in the Latent Space of Proton Collision Events at the LHC](https://arxiv.org/abs/2506.00102)
*Ema Puljak,Maurizio Pierini,Artur Garcia-Saez*

Key words: 张量网络,LHC,异常检测,量子机器学习,矩阵乘积态

TL;DR: 提出一种基于张量网络的异常检测方法，用于LHC的新物理现象发现，表现优于现有量子方法。

<details>
  <summary>Details</summary>

Main category: hep-ph

Motivation: LHC需要不断创新算法和技术以发现新现象，张量网络作为经典与量子机器学习的交叉模型，为解决这些挑战提供了高效替代方案。

Method: 采用参数化矩阵乘积态和等距特征映射，处理由自编码器生成的LHC模拟数据的潜在表示。

Result: 该方法在识别新物理现象上表现优于现有量子方法。

Conclusion: 张量网络具有增强新物理发现的潜力。

Abstract: The pursuit of discovering new phenomena at the Large Hadron Collider (LHC)
demands constant innovation in algorithms and technologies. Tensor networks are
mathematical models on the intersection of classical and quantum machine
learning, which present a promising and efficient alternative for tackling
these challenges. In this work, we propose a tensor network-based strategy for
anomaly detection at the LHC and demonstrate its superior performance in
identifying new phenomena compared to established quantum methods. Our model is
a parametrized Matrix Product State with an isometric feature map, processing a
latent representation of simulated LHC data generated by an autoencoder. Our
results highlight the potential of tensor networks to enhance new-physics
discovery.

</details>


### [744] [Generator Based Inference (GBI)](https://arxiv.org/abs/2506.00119)
*Chi Lung Cheng,Ranit Das,Runze Li,Radha Mastandrea,Vinicius Mikuni,Benjamin Nachman,David Shih,Gup Singh*

Key words: generator based inference, machine learning, statistical inference, anomaly detection, physics

TL;DR: 本文提出了一种称为基于生成器的推断（GBI）的通用框架，将机器学习与生成器整合，特别关注数据驱动的方法构建生成器。

<details>
  <summary>Details</summary>

Main category: hep-ph

Motivation: 为了在物理统计推断中利用机器学习提升高维和非分段分析的信息利用率，作者提出了GBI框架。

Method: 通过数据驱动方法构建生成器，聚焦于共振异常检测，背景生成器从旁带学习。

Result: 在LHCO基准数据集上展示了异常检测性能的新标杆，且统计输出可直接解释。

Conclusion: GBI框架为物理统计推断提供了新的工具，尤其是数据驱动生成器的应用显著提升了异常检测的灵敏度。

Abstract: Statistical inference in physics is often based on samples from a generator
(sometimes referred to as a ``forward model") that emulate experimental data
and depend on parameters of the underlying theory. Modern machine learning has
supercharged this workflow to enable high-dimensional and unbinned analyses to
utilize much more information than ever before. We propose a general framework
for describing the integration of machine learning with generators called
Generator Based Inference (GBI). A well-studied special case of this setup is
Simulation Based Inference (SBI) where the generator is a physics-based
simulator. In this work, we examine other methods within the GBI toolkit that
use data-driven methods to build the generator. In particular, we focus on
resonant anomaly detection, where the generator describing the background is
learned from sidebands. We show how to perform machine learning-based parameter
estimation in this context with data-derived generators. This transforms the
statistical outputs of anomaly detection to be directly interpretable and the
performance on the LHCO community benchmark dataset establishes a new
state-of-the-art for anomaly detection sensitivity.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [745] [A Topological Semantics of Dialogue: Nerve Structures and Logical Extraction](https://arxiv.org/abs/2506.00615)
*Andreu Ballus Santacana*

Key words: 对话语义, 拓扑学, 神经复合体, 逻辑一致性, Wolfram语言

TL;DR: 介绍了一种基于拓扑学的有限对话语义框架，通过将每个话语映射到固定语义空间中的开集，构建神经复合体，并提取关键组合不变量，实现了对话的逻辑一致性分析和全局解释计算。

<details>
  <summary>Details</summary>

Main category: cs.LO

Motivation: 旨在为有限对话提供一种简洁且具有拓扑学动机的语义，以支持对话的逻辑一致性和全局解释的自动化分析。

Method: 将话语映射为开集，构建神经复合体，提取负神经和全局解释子空间，并通过Wolfram语言实现算法。

Result: 提出了负神经和全局解释子空间的概念，展示了计算可行性。

Conclusion: 该框架结合经典对偶性和拓扑语义学，为对话分析提供了新的理论基础和实用工具。

Abstract: We introduce a concise, topologically-motivated semantics for finite
dialogues by mapping each utterance to an open set in a fixed semantic space,
building the corresponding nerve complex of joint satisfiability, and
extracting fundamental combinatorial invariants:
  1. The negative nerve, which enumerates all finite collections of utterances
whose
  opens have empty intersection, providing a straightforward criterion for
merging
  separate transcripts without contradiction.
  2. The global interpretation subspace, the unique minimal open in which all
asserted
  utterances hold simultaneously, enabling effective enumeration of all logical
  consequences of the entire dialogue.
  3. A practical demonstration in the Wolfram Language, with algorithms for
constructing
  nerves, detecting inconsistencies, and computing the global interpretation,
thereby
  illustrating computational feasibility.
  Our framework is grounded in classical duality and topological semantics
(Stone duality, Priestley duality, Tarski's semantics, coherence-space methods,
Scott domains, topos semantics, and homotopy type theory) while drawing on
recent advances in topological data analysis and dialogue-based semantics.

</details>


### [746] [Thinking Out of the Box: Hybrid SAT Solving by Unconstrained Continuous Optimization](https://arxiv.org/abs/2506.00674)
*Zhiwei Zhang,Samy Wu Fung,Anastasios Kyrillidis,Stanley Osher,Moshe Y. Vardi*

Key words: 布尔可满足性问题, 混合约束, 连续优化, 惩罚项, 机器学习

TL;DR: 提出了一种基于惩罚项的无约束连续优化方法，用于解决混合布尔可满足性问题（SAT），展示了其在提升求解效率方面的潜力。

<details>
  <summary>Details</summary>

Main category: cs.LO

Motivation: 现有的SAT求解器在CNF公式上表现出色，但当面对非CNF约束（如XOR、基数约束等）时存在局限性。多项式表示方法虽能处理这些约束，但依赖盒约束限制了无约束优化器的使用。

Method: 通过引入惩罚项，将混合SAT问题转化为无约束连续优化问题，并利用无约束优化器（如Adam）进行求解。

Result: 理论和实验表明，该方法能有效提升混合SAT问题的求解效率。

Conclusion: 结合连续优化和机器学习方法，为混合SAT求解提供了新的有效途径。

Abstract: The Boolean satisfiability (SAT) problem lies at the core of many
applications in combinatorial optimization, software verification,
cryptography, and machine learning. While state-of-the-art solvers have
demonstrated high efficiency in handling conjunctive normal form (CNF)
formulas, numerous applications require non-CNF (hybrid) constraints, such as
XOR, cardinality, and Not-All-Equal constraints. Recent work leverages
polynomial representations to represent such hybrid constraints, but it relies
on box constraints that can limit the use of powerful unconstrained optimizers.
In this paper, we propose unconstrained continuous optimization formulations
for hybrid SAT solving by penalty terms. We provide theoretical insights into
when these penalty terms are necessary and demonstrate empirically that
unconstrained optimizers (e.g., Adam) can enhance SAT solving on hybrid
benchmarks. Our results highlight the potential of combining continuous
optimization and machine-learning-based methods for effective hybrid SAT
solving.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [747] [Pushing the Limits of Beam Search Decoding for Transducer-based ASR models](https://arxiv.org/abs/2506.00185)
*Lilit Grigoryan,Vladimir Bataev,Andrei Andrusenko,Hainan Xu,Vitaly Lavrukhin,Boris Ginsburg*

Key words: Transducer, 束搜索, ASR, 贪婪解码, CUDA

TL;DR: 本文介绍了一种通用方法，通过批处理操作和树状假设结构等技术，显著加速Transducer模型的束搜索，缩小与贪婪解码的速度差距，并在WER上取得显著提升。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 由于束搜索导致的重复计算大幅降低了Transducer模型的实际应用效率，限制了其性能。

Method: 采用批处理操作、树状假设结构、新颖的空格评分技术，以及CUDA图执行优化GPU推理。

Result: 束搜索与贪婪解码的速度差距缩小至10-20%，WER相对提升14-30%，低资源场景下浅融合性能提升11%。

Conclusion: 提出的方法有效提升了Transducer模型的束搜索效率，显著改善了识别性能。

Abstract: Transducer models have emerged as a promising choice for end-to-end ASR
systems, offering a balanced trade-off between recognition accuracy, streaming
capabilities, and inference speed in greedy decoding. However, beam search
significantly slows down Transducers due to repeated evaluations of key network
components, limiting practical applications. This paper introduces a universal
method to accelerate beam search for Transducers, enabling the implementation
of two optimized algorithms: ALSD++ and AES++. The proposed method utilizes
batch operations, a tree-based hypothesis structure, novel blank scoring for
enhanced shallow fusion, and CUDA graph execution for efficient GPU inference.
This narrows the speed gap between beam and greedy modes to only 10-20% for the
whole system, achieves 14-30% relative improvement in WER compared to greedy
decoding, and improves shallow fusion for low-resource up to 11% compared to
existing implementations. All the algorithms are open sourced.

</details>


### [748] [SoundSculpt: Direction and Semantics Driven Ambisonic Target Sound Extraction](https://arxiv.org/abs/2506.00273)
*Tuochao Chen,D Shin,Hakan Erdogan,Sinan Hersek*

Key words: SoundSculpt, Ambisonic, 神经网络, 语义嵌入, 目标声场

TL;DR: SoundSculpt是一种神经网络，用于从Ambisonic录音中提取目标声场，结合空间和语义信息，性能优于传统信号处理方法。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 现有方法在提取目标声场时面临挑战，尤其是在目标声源与次要声源空间接近时，需要更高效的解决方案。

Method: 采用Ambisonic-in-Ambisonic-out架构，结合空间信息（如目标方向）和语义嵌入（如图像分割和字幕生成）。

Result: SoundSculpt在合成和真实Ambisonic混合物上表现优越，空间与语义信息的结合在复杂场景中尤为有效。

Conclusion: 空间和语义信息的结合显著提升了目标声场提取的准确性，尤其在目标与次要声源接近时。

Abstract: This paper introduces SoundSculpt, a neural network designed to extract
target sound fields from ambisonic recordings. SoundSculpt employs an
ambisonic-in-ambisonic-out architecture and is conditioned on both spatial
information (e.g., target direction obtained by pointing at an immersive video)
and semantic embeddings (e.g., derived from image segmentation and captioning).
Trained and evaluated on synthetic and real ambisonic mixtures, SoundSculpt
demonstrates superior performance compared to various signal processing
baselines. Our results further reveal that while spatial conditioning alone can
be effective, the combination of spatial and semantic information is beneficial
in scenarios where there are secondary sound sources spatially close to the
target. Additionally, we compare two different semantic embeddings derived from
a text description of the target sound using text encoders.

</details>


### [749] [Confidence intervals for forced alignment boundaries using model ensembles](https://arxiv.org/abs/2506.01256)
*Matthew C. Kelley*

Key words: forced alignment, neural network ensemble, confidence intervals, phonetic transcription

TL;DR: 该论文提出了一种使用神经网络集成技术为强制对齐边界生成置信区间的方法，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 传统强制对齐工具仅提供单一边界估计，缺乏对边界不确定性的量化。

Method: 通过训练十个不同的分段分类神经网络，重复对齐过程，并利用集成技术生成置信区间。

Result: 在Buckeye和TIMIT语料库上，集成边界比单一模型略有改进，置信区间成功整合到Praat TextGrids中。

Conclusion: 该方法有效量化了强制对齐边界的不确定性，为研究提供了新工具。

Abstract: Forced alignment is a common tool to align audio with orthographic and
phonetic transcriptions. Most forced alignment tools provide only a single
estimate of a boundary. The present project introduces a method of deriving
confidence intervals for these boundaries using a neural network ensemble
technique. Ten different segment classifier neural networks were previously
trained, and the alignment process is repeated with each model. The alignment
ensemble is then used to place the boundary at the median of the boundaries in
the ensemble, and 97.85% confidence intervals are constructed using order
statistics. On the Buckeye and TIMIT corpora, the ensemble boundaries show a
slight improvement over using just a single model. The confidence intervals are
incorporated into Praat TextGrids using a point tier, and they are also output
as a table for researchers to analyze separately as diagnostics or to
incorporate uncertainty into their analyses.

</details>


### [750] [LinearVC: Linear transformations of self-supervised features through the lens of voice conversion](https://arxiv.org/abs/2506.01510)
*Herman Kamper,Benjamin van Niekerk,Julian Zaïdi,Marc-André Carbonneau*

Key words: 语音转换, 自监督表示, 线性变换, 奇异值分解

TL;DR: LinearVC是一种简单的语音转换方法，通过线性变换自监督特征实现高效语音转换，揭示了特征空间中内容的低维嵌入特性。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 研究自监督表示的结构，探索语音转换中内容与说话者信息的分离方法。

Method: 采用线性变换自监督特征，并通过旋转和奇异值分解显式分解内容与说话者信息。

Result: 仅100维的线性投影即可实现高质量的语音转换，表明内容信息嵌入在低维子空间中。

Conclusion: LinearVC不仅提供实用的语音转换方法，还深化了对自监督语音表示的理解。

Abstract: We introduce LinearVC, a simple voice conversion method that sheds light on
the structure of self-supervised representations. First, we show that simple
linear transformations of self-supervised features effectively convert voices.
Next, we probe the geometry of the feature space by constraining the set of
allowed transformations. We find that just rotating the features is sufficient
for high-quality voice conversion. This suggests that content information is
embedded in a low-dimensional subspace which can be linearly transformed to
produce a target voice. To validate this hypothesis, we finally propose a
method that explicitly factorizes content and speaker information using
singular value decomposition; the resulting linear projection with a rank of
just 100 gives competitive conversion results. Our work has implications for
both practical voice conversion and a broader understanding of self-supervised
speech representations. Samples and code: https://www.kamperh.com/linearvc/.

</details>


### [751] [CLAP-ART: Automated Audio Captioning with Semantic-rich Audio Representation Tokenizer](https://arxiv.org/abs/2506.00800)
*Daiki Takeuchi,Binh Thien Nguyen,Masahiro Yasuda,Yasunori Ohishi,Daisuke Niizumi,Noboru Harada*

Key words: 自动化音频描述, 离散令牌, 语义丰富, 向量量化

TL;DR: CLAP-ART是一种自动化音频描述（AAC）方法，通过使用语义丰富的离散令牌来提高性能。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 现有的EnCLAP方法使用EnCodec的离散令牌，但其设计目标为波形重建，而非捕捉音频的语义上下文。

Method: CLAP-ART通过向量量化从预训练的音频表示中提取语义丰富的离散令牌，用于微调BART语言模型。

Result: 实验证明CLAP-ART在两个AAC基准上优于EnCLAP基线。

Conclusion: 语义丰富的离散令牌对AAC任务有益。

Abstract: Automated Audio Captioning (AAC) aims to describe the semantic contexts of
general sounds, including acoustic events and scenes, by leveraging effective
acoustic features. To enhance performance, an AAC method, EnCLAP, employed
discrete tokens from EnCodec as an effective input for fine-tuning a language
model BART. However, EnCodec is designed to reconstruct waveforms rather than
capture the semantic contexts of general sounds, which AAC should describe. To
address this issue, we propose CLAP-ART, an AAC method that utilizes
``semantic-rich and discrete'' tokens as input. CLAP-ART computes semantic-rich
discrete tokens from pre-trained audio representations through vector
quantization. We experimentally confirmed that CLAP-ART outperforms baseline
EnCLAP on two AAC benchmarks, indicating that semantic-rich discrete tokens
derived from semantically rich AR are beneficial for AAC.

</details>


### [752] [Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric Speech](https://arxiv.org/abs/2506.01618)
*Karl El Hajal,Enno Hermann,Sevada Hovsepyan,Mathew Magimai. -Doss*

Key words: 自动语音识别,构音障碍,语音转换,节奏建模,LF-MMI,Whisper

TL;DR: 该论文研究了如何通过语音转换技术改善自动语音识别系统对构音障碍语音的识别性能。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 构音障碍语音的高变异性和慢语速导致现有ASR系统表现不佳，作者希望通过语音转换技术解决这一问题。

Method: 扩展了Rhythm and Voice转换框架，提出了适合构音障碍语音的音节节奏建模方法，并基于转换后的语音训练了LF-MMI模型和微调了Whisper模型。

Result: 实验表明，LF-MMI模型在转换语音上显著降低了词错误率，特别是在严重构音障碍案例中，而微调Whisper的效果有限。

Conclusion: 研究表明，无监督的节奏和语音转换技术在构音障碍ASR中具有潜力。

Abstract: Automatic speech recognition (ASR) systems struggle with dysarthric speech
due to high inter-speaker variability and slow speaking rates. To address this,
we explore dysarthric-to-healthy speech conversion for improved ASR
performance. Our approach extends the Rhythm and Voice (RnV) conversion
framework by introducing a syllable-based rhythm modeling method suited for
dysarthric speech. We assess its impact on ASR by training LF-MMI models and
fine-tuning Whisper on converted speech. Experiments on the Torgo corpus reveal
that LF-MMI achieves significant word error rate reductions, especially for
more severe cases of dysarthria, while fine-tuning Whisper on converted data
has minimal effect on its performance. These results highlight the potential of
unsupervised rhythm and voice conversion for dysarthric ASR. Code available at:
https://github.com/idiap/RnV

</details>


### [753] [On-device Streaming Discrete Speech Units](https://arxiv.org/abs/2506.01845)
*Kwanghee Choi,Masao Someki,Emma Strubell,Shinji Watanabe*

Key words: 离散语音单元（DSUs）、自监督语音模型（S3Ms）、实时处理、资源受限、字符错误率（CER）

TL;DR: 论文提出了一种改进的离散语音单元（DSUs）方法，通过减小注意力窗口和模型规模，显著降低了计算成本，同时保持了语音识别的有效性。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 传统DSU方法需要完整语音输入和计算密集型自监督语音模型（S3Ms），不适合实时处理，尤其在资源受限环境中。因此，需要一种更高效的方法。

Method: 通过减小注意力窗口和模型规模，减少浮点运算（FLOPs），同时确保DSU的语音信息保留能力。

Result: 实验显示，在ML-SUPERB 1h数据集上，FLOPs减少了50%，而字符错误率（CER）仅相对增加6.5%。

Conclusion: 改进后的DSU方法适合资源受限环境下的实时语音处理。

Abstract: Discrete speech units (DSUs) are derived from clustering the features of
self-supervised speech models (S3Ms). DSUs offer significant advantages for
on-device streaming speech applications due to their rich phonetic information,
high transmission efficiency, and seamless integration with large language
models. However, conventional DSU-based approaches are impractical as they
require full-length speech input and computationally expensive S3Ms. In this
work, we reduce both the attention window and the model size while preserving
the effectiveness of DSUs. Our results demonstrate that we can reduce
floating-point operations (FLOPs) by 50% with only a relative increase of 6.5%
in character error rate (CER) on the ML-SUPERB 1h dataset. These findings
highlight the potential of DSUs for real-time speech processing in
resource-constrained environments.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [754] [Improving statistical learning methods via features selection without replacement sampling and random projection](https://arxiv.org/abs/2506.00053)
*Sulaiman khan,Muhammad Ahmad,Fida Ullah,Carlos Aguilar Ibañez,José Eduardo Valdez Rodriguez*

Key words: 癌症分类, 高维数据, 特征选择, 机器学习, 基因表达

TL;DR: 论文提出一种结合FSWOR技术和投影方法的机器学习方法，显著提升高维基因表达数据的分类准确性，并在脑癌数据集中验证了其优越性。

<details>
  <summary>Details</summary>

Main category: q-bio.QM

Motivation: 癌症是一种由基因和表观遗传改变引起的疾病，高维微阵列数据的分类面临‘小n大p’问题导致过拟合。

Method: 提出FSWOR技术结合投影方法进行特征选择，应用Kendall统计测试筛选重要基因，并采用集成分类器与LDA投影和朴素贝叶斯模型。

Result: 模型在测试中达到96%的准确率，优于现有方法9.09%，显著降低过拟合。

Conclusion: 该方法为癌症生物标志物发现提供了高效的计算工具，适用于高维基因表达数据分析。

Abstract: Cancer is fundamentally a genetic disease characterized by genetic and
epigenetic alterations that disrupt normal gene expression, leading to
uncontrolled cell growth and metastasis. High-dimensional microarray datasets
pose challenges for classification models due to the "small n, large p"
problem, resulting in overfitting. This study makes three different key
contributions: 1) we propose a machine learning-based approach integrating the
Feature Selection Without Re-placement (FSWOR) technique and a projection
method to improve classification accuracy. 2) We apply the Kendall statistical
test to identify the most significant genes from the brain cancer mi-croarray
dataset (GSE50161), reducing the feature space from 54,675 to 20,890 genes.3)
we apply machine learning models using k-fold cross validation techniques in
which our model incorpo-rates ensemble classifiers with LDA projection and
Na\"ive Bayes, achieving a test score of 96%, outperforming existing methods by
9.09%. The results demonstrate the effectiveness of our ap-proach in
high-dimensional gene expression analysis, improving classification accuracy
while mitigating overfitting. This study contributes to cancer biomarker
discovery, offering a robust computational method for analyzing microarray
data.

</details>


### [755] [Enhancing Drug Discovery: Autoencoder-Based Latent Space Augmentation for Improved Molecular Solubility Prediction using LatMixSol](https://arxiv.org/abs/2506.00223)
*Mohammad Saleh Hasankhani*

Key words: 分子溶解度预测, 潜在空间增强, 自编码器, MixUp插值, 药物发现

TL;DR: 该论文提出了一种名为LatMixSol的新方法，用于改进分子溶解度的预测，通过潜在空间增强和聚类指导的插值来解决数据不足和维度问题。

<details>
  <summary>Details</summary>

Main category: q-bio.QM

Motivation: 在药物发现早期阶段，分子溶解度的准确预测至关重要，但传统机器学习模型因标记数据有限和分子描述符的高维性而面临挑战。

Method: 方法结合了自编码器特征压缩和聚类指导的MixUp插值，生成合成样本以增强训练数据。

Result: 实验结果显示，LatMixSol在三个梯度增强回归器中显著降低了RMSE（3.2%-7.6%）并提高了R-squared（0.5%-1.5%）。

Conclusion: 该方法在化学有效性前提下扩展了数据多样性，为资源受限的药物发现流水线提供了高效策略。

Abstract: Accurate prediction of molecular solubility is a cornerstone of early-stage
drug discovery, yet conventional machine learning models face significant
challenges due to limited labeled data and the high-dimensional nature of
molecular descriptors. To address these issues, we propose LatMixSol, a novel
latent space augmentation framework that combines autoencoder-based feature
compression with guided interpolation to enrich training data. Our approach
first encodes molecular descriptors into a low-dimensional latent space using a
two-layer autoencoder. Spectral clustering is then applied to group chemically
similar molecules, enabling targeted MixUp-style interpolation within clusters.
Synthetic samples are generated by blending latent vectors of cluster members
and decoding them back to the original feature space. Evaluated on the
Huuskonen solubility benchmark, LatMixSol demonstrates consistent improvements
across three of four gradient-boosted regressors (CatBoost, LightGBM,
HistGradientBoosting), achieving RMSE reductions of 3.2-7.6% and R-squared
increases of 0.5-1.5%. Notably, HistGradientBoosting shows the most significant
enhancement with a 7.6% RMSE improvement. Our analysis confirms that
cluster-guided latent space augmentation preserves chemical validity while
expanding dataset diversity, offering a computationally efficient strategy to
enhance predictive models in resource-constrained drug discovery pipelines.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [756] [Beyond Winning: Margin of Victory Relative to Expectation Unlocks Accurate Skill Ratings](https://arxiv.org/abs/2506.00348)
*Shivam Shorewala,Zihao Yang*

Key words: 技能评分,MOVDA,比分差分析,NBA,ELO,TrueSkill

TL;DR: MOVDA是一种新的评分框架，通过使用真实比分差与模型预测差值的差异来提升传统评分系统，显著优于ELO和贝叶斯基线。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 传统评分系统（如ELO）仅关注二元结果，忽略了比分差的丰富信息。MOVDA旨在通过比分差分析更准确地评估竞争系统中的技能。

Method: MOVDA引入一个领域特定的非线性函数（缩放双曲正切）预测预期比分差，并利用真实与预期差值作为评分更新的信号。

Result: 在NBA数据（2013-2023年，13,619场比赛）中，MOVDA比TrueSkill降低Brier分数误差1.54%，提高结果准确率0.58%，加速评分收敛13.5%。

Conclusion: MOVDA理论合理、实证优越且计算高效，为NBA等竞争环境提供了比分差与技能评分的先进整合方法。

Abstract: Knowledge of accurate relative skills in any competitive system is essential,
but foundational approaches such as ELO discard extremely relevant performance
data by concentrating exclusively on binary outcomes. While margin of victory
(MOV) extensions exist, they often lack a definitive method for incorporating
this information. We introduce Margin of Victory Differential Analysis (MOVDA),
a framework that enhances traditional rating systems by using the deviation
between the true MOV and a $\textit{modeled expectation}$. MOVDA learns a
domain-specific, non-linear function (a scaled hyperbolic tangent that captures
saturation effects and home advantage) to predict expected MOV based on rating
differentials. Crucially, the $\textit{difference}$ between the true and
expected MOV provides a subtle and weighted signal for rating updates,
highlighting informative deviations in all levels of contests. Extensive
experiments on professional NBA basketball data (from 2013 to 2023, with 13,619
games) show that MOVDA significantly outperforms standard ELO and Bayesian
baselines. MOVDA reduces Brier score prediction error by $1.54\%$ compared to
TrueSkill, increases outcome accuracy by $0.58\%$, and most importantly
accelerates rating convergence by $13.5\%$, while maintaining the computational
efficiency of the original ELO updates. MOVDA offers a theoretically motivated,
empirically superior, and computationally lean approach to integrating
performance magnitude into skill rating for competitive environments like the
NBA.

</details>


### [757] [Minimax Rates for the Estimation of Eigenpairs of Weighted Laplace-Beltrami Operators on Manifolds](https://arxiv.org/abs/2506.00171)
*Nicolás García Trillos,Chenghui Li,Raghavendra Venkatraman*

Key words: 椭圆微分算子，特征对估计，图拉普拉斯算子，最小最大风险

TL;DR: 该论文研究了通过分布样本估计椭圆微分算子特征对的问题，分析了最小最大风险，并证明了近似率与密度估计问题相匹配，扩展了基于图的学习的现有文献。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 研究椭圆微分算子特征对的估计问题，旨在通过随机数据的图拉普拉斯算子找到最优的近似率，并扩展图基学习的分析范围。

Method: 利用有界几何支撑的流形上的分布样本，分析图拉普拉斯算子的特征对估计，并验证其在特定范数下的近似误差。

Result: 证明了最小最大近似率为$n^{-2/(d+4)}$，并通过更强的正则假设验证了图拉普拉斯算子能生成流形不可知的估计器，其误差与下界匹配。

Conclusion: 该研究扩展了图基学习的分析，证明了近似率的普适性和最优性。

Abstract: We study the problem of estimating eigenpairs of elliptic differential
operators from samples of a distribution $\rho$ supported on a manifold $M$.
The operators discussed in the paper are relevant in unsupervised learning and
in particular are obtained by taking suitable scaling limits of widely used
graph Laplacians over data clouds. We study the minimax risk for this eigenpair
estimation problem and explore the rates of approximation that can be achieved
by commonly used graph Laplacians built from random data. More concretely,
assuming that $\rho$ belongs to a certain family of distributions with
controlled second derivatives, and assuming that the $d$-dimensional manifold
$M$ where $\rho$ is supported has bounded geometry, we prove that the
statistical minimax rate for approximating eigenvalues and eigenvectors in the
$H^1(M)$-sense is $n^{-2/(d+4)}$, a rate that matches the minimax rate for a
closely related density estimation problem. We then revisit the literature
studying Laplacians over proximity graphs in the large data limit and prove
that, under slightly stronger regularity assumptions on the data generating
model, eigenpairs of graph Laplacians induce manifold agnostic estimators with
an error of approximation that, up to logarithmic corrections, matches our
lower bounds. Our analysis allows us to expand the existing literature on
graph-based learning in at least two significant ways: 1) we consider stronger
norms to measure the error of approximation than the ones that had been
analyzed in the past; 2) our rates of convergence are uniform over a family of
smooth distributions and do not just apply to densities with special
symmetries, and, as a consequence of our lower bounds, are essentially sharp
when the connectivity of the graph is sufficiently high.

</details>


### [758] [Overfitting has a limitation: a model-independent generalization error bound based on Rényi entropy](https://arxiv.org/abs/2506.00182)
*Atsushi Suzuki*

Key words: 泛化误差, Rényi熵, 大规模模型, 机器学习

TL;DR: 该研究探讨了机器学习模型规模扩展是否持续带来成功，提出了一种与模型无关的泛化误差上界，仅依赖于数据的Rényi熵。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 理解泛化误差行为，尤其是在大规模模型中的表现，因为传统分析无法完全解释超大架构的成功。

Method: 建立了一种适用于仅由数据直方图确定输出的算法的泛化误差上界，并探讨其对数据Rényi熵的依赖关系。

Result: 研究表明，只要数据量相对于Rényi熵足够大，即使模型无限增大，也能保持较小的泛化误差。

Conclusion: 数据分布及其Rényi熵是影响泛化性能的关键，框架解释了噪声注入导致性能下降的现象。

Abstract: Will further scaling up of machine learning models continue to bring success?
A significant challenge in answering this question lies in understanding
generalization error, which is the impact of overfitting. Understanding
generalization error behavior of increasingly large-scale machine learning
models remains a significant area of investigation, as conventional analyses
often link error bounds to model complexity, failing to fully explain the
success of extremely large architectures. This research introduces a novel
perspective by establishing a model-independent upper bound for generalization
error applicable to algorithms whose outputs are determined solely by the
data's histogram, such as empirical risk minimization or gradient-based
methods. Crucially, this bound is shown to depend only on the R\'enyi entropy
of the data-generating distribution, suggesting that a small generalization
error can be maintained even with arbitrarily large models, provided the data
quantity is sufficient relative to this entropy. This framework offers a direct
explanation for the phenomenon where generalization performance degrades
significantly upon injecting random noise into data, where the performance
degrade is attributed to the consequent increase in the data distribution's
R\'enyi entropy. Furthermore, we adapt the no-free-lunch theorem to be
data-distribution-dependent, demonstrating that an amount of data corresponding
to the R\'enyi entropy is indeed essential for successful learning, thereby
highlighting the tightness of our proposed generalization bound.

</details>


### [759] [Riemannian Principal Component Analysis](https://arxiv.org/abs/2506.00226)
*Oldemar Rodríguez*

Key words: Riemannian主成分分析, 主测地线分析, 数据降维, Riemannian流形, 局部度量

TL;DR: 本文提出了一种创新的Riemannian主成分分析（R-PCA）方法，扩展了PCA在非欧几里得空间中的应用，解决了传统PGA在处理缺乏局部距离概念的通用数据集时的局限性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 传统PCA假设数据位于欧几里得空间，而PGA虽然适用于Riemannian流形，但对缺乏局部距离概念的数据集适用性有限。本文旨在提供一种更通用的方法，适用于任何具有局部距离结构的数据。

Method: 通过为数据表配备局部度量，将PCA方法推广到Riemannian流形，从而融入流形几何结构，形成R-PCA框架。

Result: R-PCA为流形上的降维和统计分析提供了一种统一方法，尤其适用于具有区域或部分特定距离概念的数据集。

Conclusion: R-PCA成功扩展了PCA在非欧几里得空间的应用，为复杂数据集的几何性质分析提供了新工具。

Abstract: This paper proposes an innovative extension of Principal Component Analysis
(PCA) that transcends the traditional assumption of data lying in Euclidean
space, enabling its application to data on Riemannian manifolds. The primary
challenge addressed is the lack of vector space operations on such manifolds.
Fletcher et al., in their work {\em Principal Geodesic Analysis for the Study
of Nonlinear Statistics of Shape}, proposed Principal Geodesic Analysis (PGA)
as a geometric approach to analyze data on Riemannian manifolds, particularly
effective for structured datasets like medical images, where the manifold's
intrinsic structure is apparent. However, PGA's applicability is limited when
dealing with general datasets that lack an implicit local distance notion. In
this work, we introduce a generalized framework, termed {\em Riemannian
Principal Component Analysis (R-PCA)}, to extend PGA for any data endowed with
a local distance structure. Specifically, we adapt the PCA methodology to
Riemannian manifolds by equipping data tables with local metrics, enabling the
incorporation of manifold geometry. This framework provides a unified approach
for dimensionality reduction and statistical analysis directly on manifolds,
opening new possibilities for datasets with region-specific or part-specific
distance notions, ensuring respect for their intrinsic geometric properties.

</details>


### [760] [Bayesian Data Sketching for Varying Coefficient Regression Models](https://arxiv.org/abs/2506.00270)
*Rajarshi Guhaniyogi,Laura Baracaldo,Sudipto Banerjee*

Key words: 变系数模型, 贝叶斯推断, 数据压缩, 大规模数据

TL;DR: 该论文提出了贝叶斯数据草图方法，用于大规模数据中的变系数模型，通过随机线性变换压缩数据，解决计算效率问题。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 在大数据应用中，贝叶斯变系数模型因MCMC算法计算效率低而受限，需要一种高效的降维方法。

Method: 通过随机线性变换压缩功能响应向量和预测矩阵，在压缩数据上应用现有变系数回归模型。

Result: 无需新模型或算法，即可实现完全基于模型的贝叶斯推断。

Conclusion: 该方法高效且通用，适用于大规模功能数据分析。

Abstract: Varying coefficient models are popular for estimating nonlinear regression
functions in functional data models. Their Bayesian variants have received
limited attention in large data applications, primarily due to prohibitively
slow posterior computations using Markov chain Monte Carlo (MCMC) algorithms.
We introduce Bayesian data sketching for varying coefficient models to obviate
computational challenges presented by large sample sizes. To address the
challenges of analyzing large data, we compress the functional response vector
and predictor matrix by a random linear transformation to achieve dimension
reduction and conduct inference on the compressed data. Our approach
distinguishes itself from several existing methods for analyzing large
functional data in that it requires neither the development of new models or
algorithms, nor any specialized computational hardware while delivering fully
model-based Bayesian inference. Well-established methods and algorithms for
varying coefficient regression models can be applied to the compressed data.

</details>


### [761] [Label-shift robust federated feature screening for high-dimensional classification](https://arxiv.org/abs/2506.00379)
*Qi Qin,Erbo Li,Xingxiang Li,Yifan Sun,Wu Wang,Chen Xu*

Key words: 联邦学习, 特征筛选, 标签偏移, 隐私保护, 计算效率

TL;DR: 本文介绍了LR-FFS，一种标签偏移鲁棒的联邦特征筛选方法，统一了现有方法框架，并在标签偏移条件下提供了高效且隐私保护的解决方案。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 分布式和联邦学习中，数据异构性（尤其是标签偏移）给特征筛选带来挑战，亟需鲁棒且高效的方法。

Method: 提出LR-FFS框架，利用条件分布函数和期望处理标签偏移，并设计联邦估计过程，确保计算高效和隐私保护。

Result: 实验和理论分析表明，LR-FFS在不同客户端环境中表现优越，包括类分布、样本量和缺失数据差异。

Conclusion: LR-FFS在标签偏移条件下具有鲁棒性，计算高效且隐私安全，适用于多样化联邦学习场景。

Abstract: Distributed and federated learning are important tools for high-dimensional
classification of large datasets. To reduce computational costs and overcome
the curse of dimensionality, feature screening plays a pivotal role in
eliminating irrelevant features during data preprocessing. However, data
heterogeneity, particularly label shifting across different clients, presents
significant challenges for feature screening. This paper introduces a general
framework that unifies existing screening methods and proposes a novel utility,
label-shift robust federated feature screening (LR-FFS), along with its
federated estimation procedure. The framework facilitates a uniform analysis of
methods and systematically characterizes their behaviors under label shift
conditions. Building upon this framework, LR-FFS leverages conditional
distribution functions and expectations to address label shift without adding
computational burdens and remains robust against model misspecification and
outliers. Additionally, the federated procedure ensures computational
efficiency and privacy protection while maintaining screening effectiveness
comparable to centralized processing. We also provide a false discovery rate
(FDR) control method for federated feature screening. Experimental results and
theoretical analyses demonstrate LR-FFS's superior performance across diverse
client environments, including those with varying class distributions, sample
sizes, and missing categorical data.

</details>


### [762] [Off-Policy Evaluation of Ranking Policies via Embedding-Space User Behavior Modeling](https://arxiv.org/abs/2506.00446)
*Tatsuki Takahashi,Chihiro Maru,Hiroko Shoji*

Key words: 离策略评估,大规模排序动作空间,GMIPS,MRIPS,推荐系统

TL;DR: 本文提出了针对大规模排序动作空间下离策略评估（OPE）的高方差问题的新假设和广义边际逆倾向得分（GMIPS）估计器，其中MRIPS变体在偏差和方差间取得了良好平衡。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 解决现有估计器在大规模排序动作空间中高方差的问题，为新推荐策略的评估提供更可靠的方法。

Method: 提出两个新假设（排名无直接效应和用户行为模型在排序嵌入空间），并设计GMIPS估计器及其变体MRIPS，利用双重边际重要性权重。

Result: GMIPS在实验中获得最低均方误差（MSE），MRIPS在假设不完全成立时仍能有效平衡偏差和方差。

Conclusion: GMIPS特别是MRIPS变体在大规模排序动作空间中表现出色，适用于实际场景中的推荐系统评估。

Abstract: Off-policy evaluation (OPE) in ranking settings with large ranking action
spaces, which stems from an increase in both the number of unique actions and
length of the ranking, is essential for assessing new recommender policies
using only logged bandit data from previous versions. To address the high
variance issues associated with existing estimators, we introduce two new
assumptions: no direct effect on rankings and user behavior model on ranking
embedding spaces. We then propose the generalized marginalized inverse
propensity score (GMIPS) estimator with statistically desirable properties
compared to existing ones. Finally, we demonstrate that the GMIPS achieves the
lowest MSE. Notably, among GMIPS variants, the marginalized reward interaction
IPS (MRIPS) incorporates a doubly marginalized importance weight based on a
cascade behavior assumption on ranking embeddings. MRIPS effectively balances
the trade-off between bias and variance, even as the ranking action spaces
increase and the above assumptions may not hold, as evidenced by our
experiments.

</details>


### [763] [Score Matching With Missing Data](https://arxiv.org/abs/2506.00557)
*Josh Givens,Song Liu,Henry W J Reeve*

Key words: 分数匹配, 不完整数据, 重要性加权, 变分方法, 图形模型估计

TL;DR: 本文探讨了分数匹配在不完整数据中的应用，提出了两种适应性方法，并验证了其在不同数据场景下的有效性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 尽管分数匹配在数据分布学习中有广泛应用，但其在不完整数据中的研究较少。

Method: 提出了两种分数匹配变体：重要性加权（IW）方法和变分方法。

Result: IW方法在低维小样本场景中表现优异，而变分方法在高维复杂任务中表现最强。

Conclusion: 本文方法填补了分数匹配在不完整数据中的研究空白，并展示了其实际应用潜力。

Abstract: Score matching is a vital tool for learning the distribution of data with
applications across many areas including diffusion processes, energy based
modelling, and graphical model estimation. Despite all these applications,
little work explores its use when data is incomplete. We address this by
adapting score matching (and its major extensions) to work with missing data in
a flexible setting where data can be partially missing over any subset of the
coordinates. We provide two separate score matching variations for general use,
an importance weighting (IW) approach, and a variational approach. We provide
finite sample bounds for our IW approach in finite domain settings and show it
to have especially strong performance in small sample lower dimensional cases.
Complementing this, we show our variational approach to be strongest in more
complex high-dimensional settings which we demonstrate on graphical model
estimation tasks on both real and simulated data.

</details>


### [764] [Generalized Linear Markov Decision Process](https://arxiv.org/abs/2506.00818)
*Sinian Zhang,Kaicheng Zhang,Ziping Xu,Tianxi Cai,Doudou Zhou*

Key words: 强化学习, 广义线性模型, 离线学习, 样本效率

TL;DR: 提出广义线性MDP框架，扩展线性MDP以处理非线性奖励，开发两种离线RL算法并证明其有效性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 线性MDP假设限制其在实际场景中的应用，尤其是奖励信号为非线性或离散的情况。

Method: 广义线性MDP框架，使用GLM建模奖励，开发GPEVI和SS-GPEVI算法。

Result: 算法在奖励标签有限时表现更高效，理论保证策略次优性。

Conclusion: GLMDP框架适用于奖励信号复杂的场景，提升样本效率。

Abstract: The linear Markov Decision Process (MDP) framework offers a principled
foundation for reinforcement learning (RL) with strong theoretical guarantees
and sample efficiency. However, its restrictive assumption-that both transition
dynamics and reward functions are linear in the same feature space-limits its
applicability in real-world domains, where rewards often exhibit nonlinear or
discrete structures. Motivated by applications such as healthcare and
e-commerce, where data is scarce and reward signals can be binary or
count-valued, we propose the Generalized Linear MDP (GLMDP) framework-an
extension of the linear MDP framework-that models rewards using generalized
linear models (GLMs) while maintaining linear transition dynamics. We establish
the Bellman completeness of GLMDPs with respect to a new function class that
accommodates nonlinear rewards and develop two offline RL algorithms:
Generalized Pessimistic Value Iteration (GPEVI) and a semi-supervised variant
(SS-GPEVI) that utilizes both labeled and unlabeled trajectories. Our
algorithms achieve theoretical guarantees on policy suboptimality and
demonstrate improved sample efficiency in settings where reward labels are
expensive or limited.

</details>


### [765] [Projection Pursuit Density Ratio Estimation](https://arxiv.org/abs/2506.00866)
*Meilin Wang,Wei Huang,Mingming Gong,Zheng Zhang*

Key words: 密度比估计, 投影追踪, 高维数据, 一致性, 收敛速度

TL;DR: 提出了一种基于投影追踪（PP）的新型密度比估计方法，以解决高维数据下的参数和非参数方法的局限性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 密度比估计在机器学习中应用广泛，但现有方法在高维数据下表现不佳，需一种灵活且高效的方法。

Method: 利用投影追踪（PP）近似法，通过降维保留模型灵活性，提高估计准确性。

Result: 证明了估计量的一致性和收敛速度，实验表明其优于现有方法。

Conclusion: PP方法在高维密度比估计中表现优异，兼具灵活性和效率。

Abstract: Density ratio estimation (DRE) is a paramount task in machine learning, for
its broad applications across multiple domains, such as covariate shift
adaptation, causal inference, independence tests and beyond. Parametric methods
for estimating the density ratio possibly lead to biased results if models are
misspecified, while conventional non-parametric methods suffer from the curse
of dimensionality when the dimension of data is large. To address these
challenges, in this paper, we propose a novel approach for DRE based on the
projection pursuit (PP) approximation. The proposed method leverages PP to
mitigate the impact of high dimensionality while retaining the model
flexibility needed for the accuracy of DRE. We establish the consistency and
the convergence rate for the proposed estimator. Experimental results
demonstrate that our proposed method outperforms existing alternatives in
various applications.

</details>


### [766] [Reconstruction and Prediction of Volterra Integral Equations Driven by Gaussian Noise](https://arxiv.org/abs/2506.00933)
*Zhihao Xu,Saisai Ding,Zhikun Zhang,Xiangjun Wang*

Key words: 积分方程, 参数识别, 深度神经网络, Volterra方程, 高斯噪声

TL;DR: 该研究提出了一种改进的深度神经网络框架，用于识别带有高斯噪声的Volterra积分方程中的未知参数，并通过损失函数增强参数估计准确性，同时能够预测系统在积分区间外的行为。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 积分方程在应用建模、医学成像和系统识别等领域广泛应用，但对随机Volterra积分方程的参数识别研究较少，因此需要解决这一问题。

Method: 采用改进的深度神经网络框架，通过表示主变量及其积分，并将输出间关系纳入损失函数，以提高参数估计的准确性。

Result: 数值实验表明，该框架在不同噪声水平下表现稳健，能够准确识别参数并预测系统行为。

Conclusion: 提出的深度神经网络框架在参数识别和预测任务中表现出色，适用于随机系统建模。

Abstract: Integral equations are widely used in fields such as applied modeling,
medical imaging, and system identification, providing a powerful framework for
solving deterministic problems. While parameter identification for differential
equations has been extensively studied, the focus on integral equations,
particularly stochastic Volterra integral equations, remains limited. This
research addresses the parameter identification problem, also known as the
equation reconstruction problem, in Volterra integral equations driven by
Gaussian noise. We propose an improved deep neural networks framework for
estimating unknown parameters in the drift term of these equations. The network
represents the primary variables and their integrals, enhancing parameter
estimation accuracy by incorporating inter-output relationships into the loss
function. Additionally, the framework extends beyond parameter identification
to predict the system's behavior outside the integration interval. Prediction
accuracy is validated by comparing predicted and true trajectories using a 95%
confidence interval. Numerical experiments demonstrate the effectiveness of the
proposed deep neural networks framework in both parameter identification and
prediction tasks, showing robust performance under varying noise levels and
providing accurate solutions for modeling stochastic systems.

</details>


### [767] [Generative diffusion posterior sampling for informative likelihoods](https://arxiv.org/abs/2506.01083)
*Zheng Zhao*

Key words: 序贯蒙特卡罗（SMC）、扩散模型、条件采样、统计效率、离群值

TL;DR: 提出了一种新的扩散后验SMC采样器，通过构建与扩散模型相关的观测路径，提高了采样效率，尤其是在离群值或高信息似然条件下。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 近年来，序贯蒙特卡罗（SMC）方法在生成扩散模型的条件采样中表现优异，但在离群值或高信息似然条件下效率较低，因此需要改进。

Method: 设计了一种新的扩散后验SMC采样器，构建与扩散模型相关的观测路径，并利用这种相关性提高采样效率。

Result: 实证结果显示，该方法在离群值或高信息似然条件下显著提高了统计效率。

Conclusion: 通过利用扩散模型的路径相关性，新的SMC采样器在复杂条件下表现出更高效的采样能力。

Abstract: Sequential Monte Carlo (SMC) methods have recently shown successful results
for conditional sampling of generative diffusion models. In this paper we
propose a new diffusion posterior SMC sampler achieving improved statistical
efficiencies, particularly under outlier conditions or highly informative
likelihoods. The key idea is to construct an observation path that correlates
with the diffusion model and to design the sampler to leverage this correlation
for more efficient sampling. Empirical results conclude the efficiency.

</details>


### [768] [Linear regression with overparameterized linear neural networks: Tight upper and lower bounds for implicit $\ell^1$-regularization](https://arxiv.org/abs/2506.01143)
*Hannes Matt,Dominik Stöger*

Key words: 隐式正则化,梯度流,对角线性神经网络,过参数化,稀疏恢复

TL;DR: 论文研究了深度D≥2的对角线性神经网络在过参数化线性回归中的隐式正则化，分析了梯度流轨迹极限点与ℓ¹最小化问题解之间的近似误差。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 为理解过参数化模型中梯度下降的隐式偏置，研究深度对角线性神经网络在回归中的表现。

Method: 推导近似误差的紧上界和下界，分析其与初始化尺度α的关系。

Result: 发现D≥3时误差线性减小，D=2时误差减小速率为α^{1−ϱ}，与稀疏恢复中的零空间性质相关。

Conclusion: 深度网络（D≥3）在现实初始化尺度下可能具有更好的泛化性能。

Abstract: Modern machine learning models are often trained in a setting where the
number of parameters exceeds the number of training samples. To understand the
implicit bias of gradient descent in such overparameterized models, prior work
has studied diagonal linear neural networks in the regression setting. These
studies have shown that, when initialized with small weights, gradient descent
tends to favor solutions with minimal $\ell^1$-norm - an effect known as
implicit regularization. In this paper, we investigate implicit regularization
in diagonal linear neural networks of depth $D\ge 2$ for overparameterized
linear regression problems. We focus on analyzing the approximation error
between the limit point of gradient flow trajectories and the solution to the
$\ell^1$-minimization problem. By deriving tight upper and lower bounds on the
approximation error, we precisely characterize how the approximation error
depends on the scale of initialization $\alpha$. Our results reveal a
qualitative difference between depths: for $D \ge 3$, the error decreases
linearly with $\alpha$, whereas for $D=2$, it decreases at rate
$\alpha^{1-\varrho}$, where the parameter $\varrho \in [0,1)$ can be explicitly
characterized. Interestingly, this parameter is closely linked to so-called
null space property constants studied in the sparse recovery literature. We
demonstrate the asymptotic tightness of our bounds through explicit examples.
Numerical experiments corroborate our theoretical findings and suggest that
deeper networks, i.e., $D \ge 3$, may lead to better generalization,
particularly for realistic initialization scales.

</details>


### [769] [Adversarial learning for nonparametric regression: Minimax rate and adaptive estimation](https://arxiv.org/abs/2506.01267)
*Jingfu Peng,Yuhong Yang*

Key words: 对抗学习, 非参数回归, 极小极大速率, 局部多项式估计器

TL;DR: 本文研究了在对抗性攻击下的非参数回归问题，建立了在对抗性L_q风险下的极小极大收敛速率，并提出了一个最优的局部多项式估计器。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 尽管机器学习模型在多个领域取得了巨大进步，但它们容易受到未来的输入数据中自然或人为扰动的攻击。目前对对抗性学习的统计最优性仍缺乏理论支持。

Method: 在非参数回归框架下，假设回归函数的平滑性和输入扰动集的几何结构，建立对抗性L_q风险的极小极大收敛速率，并提出分段局部多项式估计器以实现最优性。

Result: 研究发现，平滑度水平和扰动幅度显著影响对抗性学习的极限性能。同时，构建了数据驱动的自适应估计器，能够在广泛类别中接近最优速率。

Conclusion: 本文为对抗性学习提供了理论支持，揭示了其统计极限，并为实际应用中的最优估计器设计提供了指导。

Abstract: Despite tremendous advancements of machine learning models and algorithms in
various application domains, they are known to be vulnerable to subtle, natural
or intentionally crafted perturbations in future input data, known as
adversarial attacks. While numerous adversarial learning methods have been
proposed, fundamental questions about their statistical optimality in robust
loss remain largely unanswered. In particular, the minimax rate of convergence
and the construction of rate-optimal estimators under future $X$-attacks are
yet to be worked out.
  In this paper, we address this issue in the context of nonparametric
regression, under suitable assumptions on the smoothness of the regression
function and the geometric structure of the input perturbation set. We first
establish the minimax rate of convergence under adversarial $L_q$-risks with $1
\leq q \leq \infty$ and propose a piecewise local polynomial estimator that
achieves the minimax optimality. The established minimax rate elucidates how
the smoothness level and perturbation magnitude affect the fundamental limit of
adversarial learning under future $X$-attacks. Furthermore, we construct a
data-driven adaptive estimator that is shown to achieve, within a logarithmic
factor, the optimal rate across a broad scale of nonparametric and adversarial
classes.

</details>


### [770] [Near-Optimal Clustering in Mixture of Markov Chains](https://arxiv.org/abs/2506.01324)
*Junghyun Lee,Yassir Jedra,Alexandre Proutière,Se-Young Yun*

Key words: 轨迹聚类、马尔可夫链、谱聚类、伪谱间隙

TL;DR: 本文研究了如何对由多个未知马尔可夫链生成的长轨迹进行聚类，提出了一种两阶段算法，实现了接近最优的聚类误差。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 目标是准确地将轨迹按其生成模型分组，解决现有方法需要先验知识的限制。

Method: 提出两阶段算法：第一阶段使用谱聚类和新的欧几里得嵌入；第二阶段通过似然重分配细化聚类。

Result: 算法在高概率下实现了接近最优的聚类误差，且无需先验知识。

Conclusion: 本文展示了该聚类问题的独特结构，并指出了上下界之间的固有差距。

Abstract: We study the problem of clustering $T$ trajectories of length $H$, each
generated by one of $K$ unknown ergodic Markov chains over a finite state space
of size $S$. The goal is to accurately group trajectories according to their
underlying generative model. We begin by deriving an instance-dependent,
high-probability lower bound on the clustering error rate, governed by the
weighted KL divergence between the transition kernels of the chains. We then
present a novel two-stage clustering algorithm. In Stage~I, we apply spectral
clustering using a new injective Euclidean embedding for ergodic Markov chains
-- a contribution of independent interest that enables sharp concentration
results. Stage~II refines the initial clusters via a single step of
likelihood-based reassignment. Our method achieves a near-optimal clustering
error with high probability, under the conditions $H =
\tilde{\Omega}(\gamma_{\mathrm{ps}}^{-1} (S^2 \vee \pi_{\min}^{-1}))$ and $TH =
\tilde{\Omega}(\gamma_{\mathrm{ps}}^{-1} S^2 )$, where $\pi_{\min}$ is the
minimum stationary probability of a state across the $K$ chains and
$\gamma_{\mathrm{ps}}$ is the minimum pseudo-spectral gap. These requirements
provide significant improvements, if not at least comparable, to the
state-of-the-art guarantee (Kausik et al., 2023), and moreover, our algorithm
offers a key practical advantage: unlike existing approach, it requires no
prior knowledge of model-specific quantities (e.g., separation between kernels
or visitation probabilities). We conclude by discussing the inherent gap
between our upper and lower bounds, providing insights into the unique
structure of this clustering problem.

</details>


### [771] [Signature Maximum Mean Discrepancy Two-Sample Statistical Tests](https://arxiv.org/abs/2506.01718)
*Andrew Alden,Blanka Horvath,Zacharia Issa*

Key words: Maximum Mean Discrepancy, 签名核, 路径空间, 两样本测试, 第二类错误

TL;DR: 论文探讨了基于签名核的签名MMD（sig-MMD）在路径空间分布比较中的实用性，分析了其可能导致第二类错误的场景，并提出了相应的改进方法。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 研究sig-MMD作为统计工具在实践中的应用潜力与挑战，特别关注其在两样本测试中的表现。

Method: 通过签名核扩展MMD至路径空间，提出sig-MMD，并通过具体案例验证其应用效果。

Result: 发现sig-MMD在数据有限的情况下可能导致第二类错误，并提出了减少此类错误的技术。

Conclusion: sig-MMD是一个有效的工具，但需谨慎使用以避免误判，尤其是在数据量较小的情况下。

Abstract: Maximum Mean Discrepancy (MMD) is a widely used concept in machine learning
research which has gained popularity in recent years as a highly effective tool
for comparing (finite-dimensional) distributions. Since it is designed as a
kernel-based method, the MMD can be extended to path space valued distributions
using the signature kernel. The resulting signature MMD (sig-MMD) can be used
to define a metric between distributions on path space. Similarly to the
original use case of the MMD as a test statistic within a two-sample testing
framework, the sig-MMD can be applied to determine if two sets of paths are
drawn from the same stochastic process. This work is dedicated to understanding
the possibilities and challenges associated with applying the sig-MMD as a
statistical tool in practice. We introduce and explain the sig-MMD, and provide
easily accessible and verifiable examples for its practical use. We present
examples that can lead to Type 2 errors in the hypothesis test, falsely
indicating that samples have been drawn from the same underlying process (which
generally occurs in a limited data setting). We then present techniques to
mitigate the occurrence of this type of error.

</details>


### [772] [Machine-Learned Sampling of Conditioned Path Measures](https://arxiv.org/abs/2506.01904)
*Qijia Jiang,Reuben Cohn-Gordon*

Key words: 后验路径测度,控制平衡动态,Wasserstein度量,神经网络

TL;DR: 提出了一种从后验路径测度中采样的算法，结合了控制平衡动态和概率空间优化的思想。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 研究如何从后验路径测度中高效采样，尤其是在缺乏数据的情况下。

Method: 利用控制平衡动态逐步传输路径测度，并结合Wasserstein度量下的概率空间优化方法。

Result: 开发出理论上有基础的算法，并能与神经网络无缝集成。

Conclusion: 该算法为学习目标轨迹集合提供了一种有效方法。

Abstract: We propose algorithms for sampling from posterior path measures $P(C([0, T],
\mathbb{R}^d))$ under a general prior process. This leverages ideas from (1)
controlled equilibrium dynamics, which gradually transport between two path
measures, and (2) optimization in $\infty$-dimensional probability space
endowed with a Wasserstein metric, which can be used to evolve a density curve
under the specified likelihood. The resulting algorithms are theoretically
grounded and can be integrated seamlessly with neural networks for learning the
target trajectory ensembles, without access to data.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [773] [Artificial Empathy: AI based Mental Health](https://arxiv.org/abs/2506.00081)
*Aditya Naik,Jovi Thomas,Teja Sree,Himavant Reddy*

Key words: AI聊天机器人, 心理健康支持, 隐私问题, 响应一致性, 危机敏感度

TL;DR: 论文探讨了AI聊天机器人在心理健康支持中的应用，发现其作为“五分钟治疗师”或非评判性伴侣的角色受到欢迎，但也存在隐私、响应不一致等问题。

<details>
  <summary>Details</summary>

Main category: q-bio.OT

Motivation: 研究AI聊天机器人在心理健康支持中的实际效果，以改进其服务并填补无法获取专业帮助的人群的需求。

Method: 通过用户访谈和基于场景的大型语言模型聊天机器人测试，收集数据并分析使用体验。

Result: 聊天机器人因匿名性和非评判性受到欢迎，但存在隐私保护不足、响应不一致、危机敏感度低等问题。

Conclusion: AI聊天机器人可作为心理健康支持的补充工具，但需改进隐私保护、响应一致性和危机处理能力。

Abstract: Many people suffer from mental health problems but not everyone seeks
professional help or has access to mental health care. AI chatbots have
increasingly become a go-to for individuals who either have mental disorders or
simply want someone to talk to. This paper presents a study on participants who
have previously used chatbots and a scenario-based testing of large language
model (LLM) chatbots. Our findings indicate that AI chatbots were primarily
utilized as a "Five minute therapist" or as a non-judgmental companion.
Participants appreciated the anonymity and lack of judgment from chatbots.
However, there were concerns about privacy and the security of sensitive
information. The scenario-based testing of LLM chatbots highlighted additional
issues. Some chatbots were consistently reassuring, used emojis and names to
add a personal touch, and were quick to suggest seeking professional help.
However, there were limitations such as inconsistent tone, occasional
inappropriate responses (e.g., casual or romantic), and a lack of crisis
sensitivity, particularly in recognizing red flag language and escalating
responses appropriately. These findings can inform both the technology and
mental health care industries on how to better utilize AI chatbots to support
individuals during challenging emotional periods.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [774] [Pro3D-Editor : A Progressive-Views Perspective for Consistent and Precise 3D Editing](https://arxiv.org/abs/2506.00512)
*Yang Zheng,Mengqi Huang,Nan Chen,Zhendong Mao*

Key words: 3D editing, text-guided, multi-view consistency, Pro3D-Editor

TL;DR: 该论文提出了一种文本引导的3D编辑方法Pro3D-Editor，通过渐进式视图范式解决多视图编辑不一致的问题。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 现有方法在编辑2D视图时忽视不同视图间的依赖关系，导致多视图编辑不一致，论文旨在通过渐进式视图范式实现一致的3D编辑。

Method: 提出Pro3D-Editor框架，包括Primary-view Sampler动态采样编辑显著视图、Key-view Render通过MoVE-LoRA将编辑语义传播到关键视图、Full-view Refiner基于多视图编辑优化3D对象。

Result: 实验表明，该方法在编辑准确性和空间一致性上优于现有方法。

Conclusion: Pro3D-Editor通过渐进式视图范式有效提升了3D编辑的一致性和准确性。

Abstract: Text-guided 3D editing aims to precisely edit semantically relevant local 3D
regions, which has significant potential for various practical applications
ranging from 3D games to film production. Existing methods typically follow a
view-indiscriminate paradigm: editing 2D views indiscriminately and projecting
them back into 3D space. However, they overlook the different cross-view
interdependencies, resulting in inconsistent multi-view editing. In this study,
we argue that ideal consistent 3D editing can be achieved through a
\textit{progressive-views paradigm}, which propagates editing semantics from
the editing-salient view to other editing-sparse views. Specifically, we
propose \textit{Pro3D-Editor}, a novel framework, which mainly includes
Primary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view
Sampler dynamically samples and edits the most editing-salient view as the
primary view. Key-view Render accurately propagates editing semantics from the
primary view to other key views through its Mixture-of-View-Experts Low-Rank
Adaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based
on the edited multi-views. Extensive experiments demonstrate that our method
outperforms existing methods in editing accuracy and spatial consistency.

</details>


### [775] [Neural Path Guiding with Distribution Factorization](https://arxiv.org/abs/2506.00839)
*Pedro Figueiredo,Qihao He,Nima Khademi Kalantari*

Key words: 神经路径引导, 蒙特卡洛积分, 渲染, 1D概率分布函数, 神经网络

TL;DR: 提出了一种神经路径引导方法，用于改进蒙特卡洛积分在渲染中的表现，通过分解2D分布为两个1D概率分布函数，并结合神经网络实现高效且表达力强的分布表示。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 现有神经方法在分布表示上无法同时实现快速和表达力强，因此需要一种既能高效计算又能准确表达的方法。

Method: 将2D方向域分布分解为两个1D概率分布函数，利用神经网络估计离散点上的分布，并通过插值实现任意位置的评估和采样。训练时最大化学习分布与目标分布的相似性，并使用额外网络缓存入射辐射度以减少梯度方差和估计归一化因子。

Result: 实验表明，该方法在复杂光传输场景中优于现有方法。

Conclusion: 该方法通过简单的1D分布分解和神经网络结合，实现了高效且表达力强的分布表示，提升了渲染效果。

Abstract: In this paper, we present a neural path guiding method to aid with Monte
Carlo (MC) integration in rendering. Existing neural methods utilize
distribution representations that are either fast or expressive, but not both.
We propose a simple, but effective, representation that is sufficiently
expressive and reasonably fast. Specifically, we break down the 2D distribution
over the directional domain into two 1D probability distribution functions
(PDF). We propose to model each 1D PDF using a neural network that estimates
the distribution at a set of discrete coordinates. The PDF at an arbitrary
location can then be evaluated and sampled through interpolation. To train the
network, we maximize the similarity of the learned and target distributions. To
reduce the variance of the gradient during optimizations and estimate the
normalization factor, we propose to cache the incoming radiance using an
additional network. Through extensive experiments, we demonstrate that our
approach is better than the existing methods, particularly in challenging
scenes with complex light transport.

</details>


### [776] [Image Generation from Contextually-Contradictory Prompts](https://arxiv.org/abs/2506.01929)
*Saar Huberman,Or Patashnik,Omer Dahary,Ron Mokady,Daniel Cohen-Or*

Key words: 文本到图像，扩散模型，上下文矛盾，提示分解，大型语言模型

TL;DR: 提出了一种阶段感知提示分解框架，用于解决文本到图像扩散模型中因上下文矛盾导致语义不准确的问题。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 文本到图像扩散模型在处理包含矛盾概念组合的提示时，往往会产生语义不准确的结果。定义这种问题为上下文矛盾，并提出解决方案。

Method: 使用阶段感知提示分解框架，通过代理提示序列引导去噪过程。利用大型语言模型（LLM）分析目标提示，识别矛盾并生成替代表达。

Result: 实验表明，新方法在具有挑战性的提示下显著提升了与文本提示的对齐性。

Conclusion: 提出的框架通过阶段对齐和语义分解，有效解决了上下文矛盾问题，实现了更准确的图像生成。

Abstract: Text-to-image diffusion models excel at generating high-quality, diverse
images from natural language prompts. However, they often fail to produce
semantically accurate results when the prompt contains concept combinations
that contradict their learned priors. We define this failure mode as contextual
contradiction, where one concept implicitly negates another due to entangled
associations learned during training. To address this, we propose a stage-aware
prompt decomposition framework that guides the denoising process using a
sequence of proxy prompts. Each proxy prompt is constructed to match the
semantic content expected to emerge at a specific stage of denoising, while
ensuring contextual coherence. To construct these proxy prompts, we leverage a
large language model (LLM) to analyze the target prompt, identify
contradictions, and generate alternative expressions that preserve the original
intent while resolving contextual conflicts. By aligning prompt information
with the denoising progression, our method enables fine-grained semantic
control and accurate image generation in the presence of contextual
contradictions. Experiments across a variety of challenging prompts show
substantial improvements in alignment to the textual prompt.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [777] [A Foundation Model for Non-Destructive Defect Identification from Vibrational Spectra](https://arxiv.org/abs/2506.00725)
*Mouyang Cheng,Chu-Liang Fu,Bowen Yu,Eunbi Rha,Abhijatmedhi Chotrattanapituk,Douglas L Abernathy,Yongqiang Cheng,Mingda Li*

Key words: DefectNet, 点缺陷, 振动光谱, 机器学习, 非破坏性表征

TL;DR: DefectNet是一个基于机器学习的基础模型，通过振动光谱非破坏性地预测材料中点缺陷的化学特性和浓度，适用于多种共存元素。

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

Motivation: 解决材料中多种缺陷共存时的非破坏性定量表征难题。

Method: 训练于16,000个模拟振动光谱，采用定制注意力机制预测6种缺陷元素。

Result: 模型能泛化至56种未见过元素，实验验证结果准确。

Conclusion: 振动光谱结合基础模型为缺陷工程提供了可行工具。

Abstract: Defects are ubiquitous in solids and strongly influence materials' mechanical
and functional properties. However, non-destructive characterization and
quantification of defects, especially when multiple types coexist, remain a
long-standing challenge. Here we introduce DefectNet, a foundation machine
learning model that predicts the chemical identity and concentration of
substitutional point defects with multiple coexisting elements directly from
vibrational spectra, specifically phonon density-of-states (PDoS). Trained on
over 16,000 simulated spectra from 2,000 semiconductors, DefectNet employs a
tailored attention mechanism to identify up to six distinct defect elements at
concentrations ranging from 0.2% to 25%. The model generalizes well to unseen
crystals across 56 elements and can be fine-tuned on experimental data.
Validation using inelastic scattering measurements of SiGe alloys and MgB$_2$
superconductor demonstrates its accuracy and transferability. Our work
establishes vibrational spectroscopy as a viable, non-destructive probe for
point defect quantification in bulk materials, and highlights the promise of
foundation models in data-driven defect engineering.

</details>


### [778] [Overcoming Data Scarcity in Scanning Tunnelling Microscopy Image Segmentation](https://arxiv.org/abs/2506.01678)
*Nikola L. Kolev,Max Trouton,Filippo Federici Canova,Geoff Thornton,David Z. Gao,Neil J. Curson,Taylor J. Z. Stock*

Key words: STM图像、少样本学习、无监督学习、自动分割、原子分辨率

TL;DR: 提出了一种结合少样本学习和无监督学习的自动化STM图像分割方法，减少了人工标注的需求，提高了对新表面的适应性。

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

Motivation: STM图像的手动分析耗时费力，需要自动化方法来减轻负担并提高效率。

Method: 结合少样本学习和无监督学习的方法，避免了大规模人工标注数据集的需求。

Result: 在Si(001)、Ge(001)和TiO$_2$(110)等表面上成功识别原子特征，包括吸附的AsH$_3$分子，模型表现出强的泛化能力。

Conclusion: 该方法为高效且材料无关的STM图像自动分割提供了重要进展。

Abstract: Scanning tunnelling microscopy (STM) is a powerful technique for imaging
surfaces with atomic resolution, providing insight into physical and chemical
processes at the level of single atoms and molecules. A regular task of STM
image analysis is the identification and labelling of features of interest
against a uniform background. Performing this manually is a labour-intensive
task, requiring significant human effort. To reduce this burden, we propose an
automated approach to the segmentation of STM images that uses both few-shot
learning and unsupervised learning. Our technique offers greater flexibility
compared to previous supervised methods; it removes the requirement for large
manually annotated datasets and is thus easier to adapt to an unseen surface
while still maintaining a high accuracy. We demonstrate the effectiveness of
our approach by using it to recognise atomic features on three distinct
surfaces: Si(001), Ge(001), and TiO$_2$(110), including adsorbed AsH$_3$
molecules on the silicon and germanium surfaces. Our model exhibits strong
generalisation capabilities, and following initial training, can be adapted to
unseen surfaces with as few as one additional labelled data point. This work is
a significant step towards efficient and material-agnostic, automatic
segmentation of STM images.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [779] [How hard is learning to cut? Trade-offs and sample complexity](https://arxiv.org/abs/2506.00252)
*Sammy Khalife,Andrea Lodi*

Key words: 分支切割算法,切割平面选择,样本复杂度,学习理论,整数规划

TL;DR: 本文提出了针对分支切割算法中切割平面选择的新样本复杂度下界，并对两种评分函数进行了理论分析与实证验证，首次为学习切割框架提供了下界。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 研究分支切割算法中数据驱动方法的有效性，特别是在切割平面选择上，填补了现有文献中缺乏理论分析的空白。

Method: 通过样本复杂度下界分析，比较了两种评分函数（分支切割树大小和间隙闭合）的样本需求，并扩展到有限切割集的学习。

Result: 证明了学习切割问题需要与学习一般目标函数相当的样本量，且在神经网络案例中接近已知上界。

Conclusion: 间隙闭合评分为分支切割树大小的有效代理，研究结果为切割平面选择提供了理论支持。

Abstract: In the recent years, branch-and-cut algorithms have been the target of
data-driven approaches designed to enhance the decision making in different
phases of the algorithm such as branching, or the choice of cutting planes
(cuts). In particular, for cutting plane selection two score functions have
been proposed in the literature to evaluate the quality of a cut:
branch-and-cut tree size and gap closed. In this paper, we present new sample
complexity lower bounds, valid for both scores. We show that for a wide family
of classes $\mathcal{F}$ that maps an instance to a cut, learning over an
unknown distribution of the instances to minimize those scores requires at
least (up to multiplicative constants) as many samples as learning from the
same class function $\mathcal{F}$ any generic target function (using square
loss). Our results also extend to the case of learning from a restricted set of
cuts, namely those from the Simplex tableau. To the best of our knowledge,
these constitute the first lower bounds for the learning-to-cut framework. We
compare our bounds to known upper bounds in the case of neural networks and
show they are nearly tight. We illustrate our results with a graph neural
network selection evaluated on set covering and facility location integer
programming models and we empirically show that the gap closed score is an
effective proxy to minimize the branch-and-cut tree size. Although the gap
closed score has been extensively used in the integer programming literature,
this is the first principled analysis discussing both scores at the same time
both theoretically and computationally.

</details>


### [780] [An adaptive data sampling strategy for stabilizing dynamical systems via controller inference](https://arxiv.org/abs/2506.01816)
*Steffen W. R. Werner,Benjamin Peherstorfer*

Key words: 自适应采样, 稳定控制器, 数据收集, 系统稳定

TL;DR: 该论文提出了一种自适应采样方案，用于在数据收集过程中避免系统不稳定，并生成用于控制器推断的最小数据集，实验表明其数据需求减少了十倍。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 从数据中学习稳定控制器是一个重要但具有挑战性的任务，因为不稳定系统可能导致数据收集困难。

Method: 提出了一种自适应采样方案，在数据收集的同时稳定系统，避免不稳定性，并确保生成的数据集足够小且信息丰富。

Result: 实验表明，该方法比非引导数据生成少用十倍的数据样本即可学习到稳定控制器。

Conclusion: 该方法能在不稳定易发的边缘情况和极限状态下稳定系统，解决了数据收集的固有难题。

Abstract: Learning stabilizing controllers from data is an important task in
engineering applications; however, collecting informative data is challenging
because unstable systems often lead to rapidly growing or erratic trajectories.
In this work, we propose an adaptive sampling scheme that generates data while
simultaneously stabilizing the system to avoid instabilities during the data
collection. Under mild assumptions, the approach provably generates data sets
that are informative for stabilization and have minimal size. The numerical
experiments demonstrate that controller inference with the novel adaptive
sampling approach learns controllers with up to one order of magnitude fewer
data samples than unguided data generation. The results show that the proposed
approach opens the door to stabilizing systems in edge cases and limit states
where instabilities often occur and data collection is inherently difficult.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [781] [MolTextNet: A Two-Million Molecule-Text Dataset for Multimodal Molecular Learning](https://arxiv.org/abs/2506.00009)
*Yihan Zhu,Gang Liu,Eric Inae,Meng Jiang*

Key words: MolTextNet, 分子-文本数据集, 多模态模型, 合成文本生成, 分子科学

TL;DR: MolTextNet是一个包含250万高质量分子-文本对的数据集，旨在解决现有数据集规模小和信息不足的问题，通过合成文本生成管道构建，支持多种下游任务。

<details>
  <summary>Details</summary>

Main category: q-bio.BM

Motivation: 现有分子-文本数据集规模和信息量有限，限制了多模态模型的训练，需要更大、更丰富的数据集来推动分子科学中的多模态建模。

Method: 提出合成文本生成管道，结合结构特征、计算性质、生物活性数据和合成复杂性，利用GPT-4o-mini为ChEMBL35中的250万个分子生成长文本描述。

Result: MolTextNet生成的文本长度是现有数据集的10倍以上，训练出的CLIP风格模型在性能上有所提升。

Conclusion: MolTextNet在推动分子科学中的多模态基础模型方面具有潜力，数据集已公开。

Abstract: Small molecules are essential to drug discovery, and graph-language models
hold promise for learning molecular properties and functions from text.
However, existing molecule-text datasets are limited in scale and
informativeness, restricting the training of generalizable multimodal models.
We present MolTextNet, a dataset of 2.5 million high-quality molecule-text
pairs designed to overcome these limitations. To construct it, we propose a
synthetic text generation pipeline that integrates structural features,
computed properties, bioactivity data, and synthetic complexity. Using
GPT-4o-mini, we create structured descriptions for 2.5 million molecules from
ChEMBL35, with text over 10 times longer than prior datasets. MolTextNet
supports diverse downstream tasks, including property prediction and structure
retrieval. Pretraining CLIP-style models with Graph Neural Networks and
ModernBERT on MolTextNet yields improved performance, highlighting its
potential for advancing foundational multimodal modeling in molecular science.
Our dataset is available at
https://huggingface.co/datasets/liuganghuggingface/moltextnet.

</details>


### [782] [ProtInvTree: Deliberate Protein Inverse Folding with Reward-guided Tree Search](https://arxiv.org/abs/2506.00925)
*Mengdi Liu,Xiaoxue Cheng,Zhangyang Gao,Hong Chang,Cheng Tan,Shiguang Shan,Xilin Chen*

Key words: 蛋白质反向折叠,树搜索框架,序列多样性,结构一致性,蛋白质语言模型

TL;DR: ProtInvTree是一种奖励引导的树搜索框架，用于解决蛋白质反向折叠问题，能够在保持结构一致性的同时生成多样化的序列。

<details>
  <summary>Details</summary>

Main category: q-bio.BM

Motivation: 现有深度学习方法在蛋白质反向折叠任务中主要关注恢复原始序列，忽略了一对多的多样性问题，需要一种能够生成多样化序列且保持结构一致性的模型。

Method: ProtInvTree采用两阶段焦点与接地动作机制，解耦位置选择和残基生成，并引入跳跃去噪策略高效评估中间状态。它还支持灵活的测试时扩展，无需重新训练。

Result: 实验表明，ProtInvTree在多个基准测试中优于现有方法，能够生成结构一致且多样化的序列，包括远离原始真实的序列。

Conclusion: ProtInvTree为蛋白质反向折叠提供了一种高效且灵活的解决方案，解决了序列生成中的多样性问题。

Abstract: Designing protein sequences that fold into a target 3D structure, known as
protein inverse folding, is a fundamental challenge in protein engineering.
While recent deep learning methods have achieved impressive performance by
recovering native sequences, they often overlook the one-to-many nature of the
problem: multiple diverse sequences can fold into the same structure. This
motivates the need for a generative model capable of designing diverse
sequences while preserving structural consistency. To address this trade-off,
we introduce ProtInvTree, the first reward-guided tree-search framework for
protein inverse folding. ProtInvTree reformulates sequence generation as a
deliberate, step-wise decision-making process, enabling the exploration of
multiple design paths and exploitation of promising candidates through
self-evaluation, lookahead, and backtracking. We propose a two-stage
focus-and-grounding action mechanism that decouples position selection and
residue generation. To efficiently evaluate intermediate states, we introduce a
jumpy denoising strategy that avoids full rollouts. Built upon pretrained
protein language models, ProtInvTree supports flexible test-time scaling by
expanding the search depth and breadth without retraining. Empirically,
ProtInvTree outperforms state-of-the-art baselines across multiple benchmarks,
generating structurally consistent yet diverse sequences, including those far
from the native ground truth.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [783] [Heterogeneous Graph Backdoor Attack](https://arxiv.org/abs/2506.00191)
*Jiawei Chen,Lusi Li,Daniel Takabi,Masha Sosonkina,Rui Ning*

Key words: 异构图神经网络, 后门攻击, 基于关系的触发, 后门元路径, 鲁棒性

TL;DR: 该论文首次研究了异构图神经网络（HGNNs）对后门攻击的脆弱性，提出了异构图后门攻击（HGBA），采用基于关系的触发机制，解决了现有攻击的高成本、低效和不准确问题，并通过实验证明其高效性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 异构图神经网络（HGNNs）在多领域复杂关系建模中表现优异，但其对后门攻击的脆弱性尚未被研究，因此需要填补这一空白。

Method: 提出异构图后门攻击（HGBA），引入基于关系的触发机制，通过后门元路径连接触发节点和中毒节点，支持两种灵活的激活策略，并改进攻击效果评估协议。

Result: HGBA在低攻击预算下高效攻击HGNNs，优于现有攻击方法，且对节点特征扰动和多种防御机制表现出鲁棒性。

Conclusion: HGBA为异构图神经网络的后门攻击提供了高效、隐蔽的解决方案，并展示了其广泛的安全威胁潜力。

Abstract: Heterogeneous Graph Neural Networks (HGNNs) excel in modeling complex,
multi-typed relationships across diverse domains, yet their vulnerability to
backdoor attacks remains unexplored. To address this gap, we conduct the first
investigation into the susceptibility of HGNNs to existing graph backdoor
attacks, revealing three critical issues: (1) high attack budget required for
effective backdoor injection, (2) inefficient and unreliable backdoor
activation, and (3) inaccurate attack effectiveness evaluation. To tackle these
issues, we propose the Heterogeneous Graph Backdoor Attack (HGBA), the first
backdoor attack specifically designed for HGNNs, introducing a novel
relation-based trigger mechanism that establishes specific connections between
a strategically selected trigger node and poisoned nodes via the backdoor
metapath. HGBA achieves efficient and stealthy backdoor injection with minimal
structural modifications and supports easy backdoor activation through two
flexible strategies: Self-Node Attack and Indiscriminate Attack. Additionally,
we improve the ASR measurement protocol, enabling a more accurate assessment of
attack effectiveness. Extensive experiments demonstrate that HGBA far surpasses
multiple state-of-the-art graph backdoor attacks in black-box settings,
efficiently attacking HGNNs with low attack budgets. Ablation studies show that
the strength of HBGA benefits from our trigger node selection method and
backdoor metapath selection strategy. In addition, HGBA shows superior
robustness against node feature perturbations and multiple types of existing
graph backdoor defense mechanisms. Finally, extension experiments demonstrate
that the relation-based trigger mechanism can effectively extend to tasks in
homogeneous graph scenarios, thereby posing severe threats to broader
security-critical domains.

</details>


### [784] [Chances and Challenges of the Model Context Protocol in Digital Forensics and Incident Response](https://arxiv.org/abs/2506.00274)
*Jan-Niclas Hilgert,Carlo Jakobs,Michael Külper,Martin Lambertz,Axel Mahr,Elmar Padilla*

Key words: 大型语言模型, 数字取证, 模型上下文协议, 透明度, 可解释性

TL;DR: 该论文探讨了模型上下文协议（MCP）如何解决大型语言模型（LLM）在数字取证中的透明度和可解释性问题，并提出其在多种取证场景中的应用潜力。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 大型语言模型在数字取证中的应用受到透明度和可解释性问题的限制，需要一种方法来增强其可信度和实用性。

Method: 通过理论分析，探讨MCP如何整合到不同取证场景中，并分析其技术实现和概念设计。

Result: MCP不仅能强化现有取证流程，还能扩展LLM的应用范围，同时通过推理约束级别提升可审计性和可追溯性。

Conclusion: MCP作为LLM辅助取证的基础组件具有巨大潜力，但也提出了未来可能面临的挑战。

Abstract: Large language models hold considerable promise for supporting forensic
investigations, but their widespread adoption is hindered by a lack of
transparency, explainability, and reproducibility. This paper explores how the
emerging Model Context Protocol can address these challenges and support the
meaningful use of LLMs in digital forensics. Through a theoretical analysis, we
examine how MCP can be integrated across various forensic scenarios - ranging
from artifact analysis to the generation of interpretable reports. We also
outline both technical and conceptual considerations for deploying an MCP
server in forensic environments. Our analysis reveals a wide range of use cases
in which MCP not only strengthens existing forensic workflows but also
facilitates the application of LLMs to areas of forensics where their use was
previously limited. Furthermore, we introduce the concept of the inference
constraint level - a way of characterizing how specific MCP design choices can
deliberately constrain model behavior, thereby enhancing both auditability and
traceability. Our insights demonstrate that MCP has significant potential as a
foundational component for developing LLM-assisted forensic workflows that are
not only more transparent, reproducible, and legally defensible, but also
represent a step toward increased automation in digital forensic analysis.
However, we also highlight potential challenges that the adoption of MCP may
pose for digital forensics in the future.

</details>


### [785] [Adversarial Threat Vectors and Risk Mitigation for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.00281)
*Chris M. Ward,Josh Harguess*

Key words: 检索增强生成, 对抗性攻击, 风险管理, 安全措施

TL;DR: 本文分析了检索增强生成（RAG）系统面临的安全威胁，提出了风险缓解措施。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 随着RAG系统在行业中的广泛应用，其安全性问题日益凸显，亟需研究其攻击向量并提出应对策略。

Method: 识别了RAG系统的主要攻击向量（如提示注入、数据投毒和对抗性查询操纵），并从风险管理角度分析了这些威胁，提出了优先级控制列表。

Result: 提出的风险缓解措施包括输入验证、对抗性训练和实时监控。

Conclusion: RAG系统面临多种安全威胁，但通过优先级控制列表和具体措施可以有效降低风险。

Abstract: Retrieval-Augmented Generation (RAG) systems, which integrate Large Language
Models (LLMs) with external knowledge sources, are vulnerable to a range of
adversarial attack vectors. This paper examines the importance of RAG systems
through recent industry adoption trends and identifies the prominent attack
vectors for RAG: prompt injection, data poisoning, and adversarial query
manipulation. We analyze these threats under risk management lens, and propose
robust prioritized control list that includes risk-mitigating actions like
input validation, adversarial training, and real-time monitoring.

</details>


### [786] [dpmm: Differentially Private Marginal Models, a Library for Synthetic Tabular Data Generation](https://arxiv.org/abs/2506.00322)
*Sofiane Mahiou,Amir Dizche,Reza Nazari,Xinmin Wu,Ralph Abbey,Jorge Silva,Georgi Ganev*

Key words: 差分隐私, 合成数据, 开源库, PrivBayes, MST, AIM

TL;DR: 提出dpmm开源库，用于生成具有差分隐私(DP)保证的合成数据，包含三种边际模型并提供丰富功能。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 为满足广泛用户需求，提供易安装、高度可定制且鲁棒的差分隐私合成数据生成工具。

Method: 采用三种流行的边际模型（PrivBayes、MST、AIM），结合最佳实践提供端到端DP保证。

Result: 实现优于其他工具的效用，并解决已知DP相关漏洞。

Conclusion: dpmm是一个功能丰富且高度可靠的差分隐私合成数据生成库。

Abstract: We propose dpmm, an open-source library for synthetic data generation with
Differentially Private (DP) guarantees. It includes three popular marginal
models -- PrivBayes, MST, and AIM -- that achieve superior utility and offer
richer functionality compared to alternative implementations. Additionally, we
adopt best practices to provide end-to-end DP guarantees and address well-known
DP-related vulnerabilities. Our goal is to accommodate a wide audience with
easy-to-install, highly customizable, and robust model implementations.
  Our codebase is available from https://github.com/sassoftware/dpmm.

</details>


### [787] [The Security Threat of Compressed Projectors in Large Vision-Language Models](https://arxiv.org/abs/2506.00534)
*Yudong Zhang,Ruobing Xie,Xingwu Sun,Jiansheng Chen,Zhanhui Kang,Di Wang,Yu Wang*

Key words: 视觉语言投射器, 大规模视觉语言模型, 安全性, 对抗攻击

TL;DR: 本文探讨了视觉语言投射器（VLP）对大规模视觉语言模型（LVLM）训练安全性的影响，发现压缩型VLP存在显著脆弱性，而非压缩型VLP则表现出较强安全性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 研究动机在于评估不同VLP类型对LVLM安全性的影响，填补了现有研究中对VLP安全性分析的空白。

Method: 通过全面评估压缩型和非压缩型VLP的安全性表现，分析两者在对抗攻击中的脆弱性差异。

Result: 结果表明压缩型VLP易受攻击，而非压缩型VLP安全性较高，未引入额外漏洞。

Conclusion: 研究为选择安全的VLP提供了指导，有助于提升LVLM的安全性和可靠性。

Abstract: The choice of a suitable visual language projector (VLP) is critical to the
successful training of large visual language models (LVLMs). Mainstream VLPs
can be broadly categorized into compressed and uncompressed projectors, and
each offering distinct advantages in performance and computational efficiency.
However, their security implications have not been thoroughly examined. Our
comprehensive evaluation reveals significant differences in their security
profiles: compressed projectors exhibit substantial vulnerabilities, allowing
adversaries to successfully compromise LVLMs even with minimal knowledge of
structural information. In stark contrast, uncompressed projectors demonstrate
robust security properties and do not introduce additional vulnerabilities.
These findings provide critical guidance for researchers in selecting optimal
VLPs that enhance the security and reliability of visual language models. The
code will be released.

</details>


### [788] [SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation Models](https://arxiv.org/abs/2506.00821)
*Huixin Zhan,Jason H. Moore*

Key words: 基因组基础模型,对抗鲁棒性,SafeGenes,FGSM,软提示攻击

TL;DR: SafeGenes框架评估了基因组基础模型（GFMs）的对抗鲁棒性，发现其在对抗攻击下表现脆弱，尤其是针对嵌入空间操作的软提示攻击导致性能显著下降。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 当前GFMs在基因组变体效应预测中表现优异，但其对抗鲁棒性尚未充分研究。

Method: 采用FGSM和软提示攻击两种方法，评估模型对输入序列扰动和嵌入空间操作的脆弱性。

Result: 软提示攻击显著降低了大型模型如ESM1b和ESM1v的性能，揭示了其安全漏洞。

Conclusion: GFMs存在对抗攻击的严重脆弱性，需进一步研究以提高其在高风险基因组应用中的安全性和鲁棒性。

Abstract: Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM),
have demonstrated significant success in variant effect prediction. However,
their adversarial robustness remains largely unexplored. To address this gap,
we propose SafeGenes: a framework for Secure analysis of genomic foundation
models, leveraging adversarial attacks to evaluate robustness against both
engineered near-identical adversarial Genes and embedding-space manipulations.
In this study, we assess the adversarial vulnerabilities of GFMs using two
approaches: the Fast Gradient Sign Method (FGSM) and a soft prompt attack. FGSM
introduces minimal perturbations to input sequences, while the soft prompt
attack optimizes continuous embeddings to manipulate model predictions without
modifying the input tokens. By combining these techniques, SafeGenes provides a
comprehensive assessment of GFM susceptibility to adversarial manipulation.
Targeted soft prompt attacks led to substantial performance degradation, even
in large models such as ESM1b and ESM1v. These findings expose critical
vulnerabilities in current foundation models, opening new research directions
toward improving their security and robustness in high-stakes genomic
applications such as variant effect prediction.

</details>


### [789] [A Large Language Model-Supported Threat Modeling Framework for Transportation Cyber-Physical Systems](https://arxiv.org/abs/2506.00831)
*M Sabbir Salek,Mashrur Chowdhury,Muhaimin Bin Munir,Yuchen Cai,Mohammad Imtiaz Hasan,Jean-Michel Tine,Latifur Khan,Mizanur Rahman*

Key words: 交通网络物理系统,威胁建模,大型语言模型,网络安全,MITRE ATT&CK

TL;DR: 本文提出了一种基于大型语言模型的交通网络安全威胁建模框架TraCR-TMF，通过减少专家干预来识别威胁、攻击技术和应对措施，并在实际场景中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 现代交通系统依赖于网络物理系统（CPS），但自动化和连通性的增加导致了安全漏洞的暴露。现有威胁建模框架在范围、资源和专业知识方面存在局限性。

Method: TraCR-TMF利用MITRE ATT&CK矩阵，通过三种基于LLM的方法（无需专家输入的RAG、低专家输入的上下文学习、中等专家输入的监督微调）和定制化LLM分析漏洞，映射攻击路径。

Result: 框架在两种场景中验证：识别交通CPS应用的攻击技术（专家验证精确度为90%），以及预测实际网络攻击事件中的多阶段利用行为。

Conclusion: TraCR-TMF在CPS威胁建模中表现出高效性，减少了对网络安全专业知识的依赖，并具有跨CPS领域的适应性。

Abstract: Modern transportation systems rely on cyber-physical systems (CPS), where
cyber systems interact seamlessly with physical systems like
transportation-related sensors and actuators to enhance safety, mobility, and
energy efficiency. However, growing automation and connectivity increase
exposure to cyber vulnerabilities. Existing threat modeling frameworks for
transportation CPS are often limited in scope, resource-intensive, and
dependent on significant cybersecurity expertise. To address these gaps, we
present TraCR-TMF (Transportation Cybersecurity and Resiliency Threat Modeling
Framework), a large language model (LLM)-based framework that minimizes expert
intervention. TraCR-TMF identifies threats, potential attack techniques, and
corresponding countermeasures by leveraging the MITRE ATT&CK matrix through
three LLM-based approaches: (i) a retrieval-augmented generation (RAG) method
requiring no expert input, (ii) an in-context learning approach requiring low
expert input, and (iii) a supervised fine-tuning method requiring moderate
expert input. TraCR-TMF also maps attack paths to critical assets by analyzing
vulnerabilities using a customized LLM. The framework was evaluated in two
scenarios. First, it identified relevant attack techniques across
transportation CPS applications, with 90% precision as validated by experts.
Second, using a fine-tuned LLM, it successfully predicted multiple
exploitations including lateral movement, data exfiltration, and
ransomware-related encryption that occurred during a major real-world
cyberattack incident. These results demonstrate TraCR-TMF's effectiveness in
CPS threat modeling, its reduced reliance on cybersecurity expertise, and its
adaptability across CPS domains.

</details>


### [790] [Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities](https://arxiv.org/abs/2506.00548)
*Jiahui Geng,Thy Thy Tran,Preslav Nakov,Iryna Gurevych*

Key words: 多模态语言模型, 对抗攻击, 安全机制, Con Instruction, ARC框架

TL;DR: 论文提出了一种名为Con Instruction的新方法，通过优化对抗性的图像或音频来攻击多模态语言模型（MLLMs），能够绕过安全机制并显著提高攻击成功率。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 现有对MLLMs的攻击主要依赖文本指令和对抗图像，而本论文探索了模型对非文本指令（如图像或音频）的理解能力，揭示了其潜在漏洞。

Method: 提出Con Instruction方法，优化对抗性示例以在嵌入空间中与目标指令对齐，无需训练数据或文本指令预处理。并引入新的ARC评估框架。

Result: 在多个模型（如LLaVA-v1.5等）和基准测试（AdvBench、SafeBench）上，攻击成功率最高达86.6%。

Conclusion: 该方法高效绕过MLLMs的安全机制，同时发现现有防御技术存在显著性能差距。代码已公开。

Abstract: Existing attacks against multimodal language models (MLLMs) primarily
communicate instructions through text accompanied by adversarial images. In
contrast, we exploit the capabilities of MLLMs to interpret non-textual
instructions, specifically, adversarial images or audio generated by our novel
method, Con Instruction. We optimize these adversarial examples to align
closely with target instructions in the embedding space, revealing the
detrimental implications of MLLMs' sophisticated understanding. Unlike prior
work, our method does not require training data or preprocessing of textual
instructions. While these non-textual adversarial examples can effectively
bypass MLLM safety mechanisms, their combination with various text inputs
substantially amplifies attack success. We further introduce a new Attack
Response Categorization (ARC) framework, which evaluates both the quality of
the model's response and its relevance to the malicious instructions.
Experimental results demonstrate that Con Instruction effectively bypasses
safety mechanisms in multiple vision- and audio-language models, including
LLaVA-v1.5, InternVL, Qwen-VL, and Qwen-Audio, evaluated on two standard
benchmarks: AdvBench and SafeBench. Specifically, our method achieves the
highest attack success rates, reaching 81.3% and 86.6% on LLaVA-v1.5 (13B). On
the defense side, we explore various countermeasures against our attacks and
uncover a substantial performance gap among existing techniques. Our
implementation is made publicly available.

</details>


### [791] [When GPT Spills the Tea: Comprehensive Assessment of Knowledge File Leakage in GPTs](https://arxiv.org/abs/2506.00197)
*Xinyue Shen,Yun Shen,Michael Backes,Yang Zhang*

Key words: 知识文件泄漏； GPT； 数据安全； 泄漏途径； Code Interpreter

TL;DR: 本文通过新型数据安全管理方法，全面评估了知识文件泄漏风险，揭示了五个泄漏途径，并提供了解决方案。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 随着知识文件在大型语言模型代理中的广泛应用，其泄漏风险日益增加，需要全面评估以保障数据安全。

Method: 利用数据安全管理框架，分析了大量GPT元数据、数据流和响应，识别了五种泄漏途径。

Result: 发现五个泄漏途径，其中内置工具Code Interpreter存在特权升级漏洞，成功率达95.95%。

Conclusion: 研究揭示了知识文件的严重泄漏风险，并提出了针对性的解决方案。

Abstract: Knowledge files have been widely used in large language model (LLM) agents,
such as GPTs, to improve response quality. However, concerns about the
potential leakage of knowledge files have grown significantly. Existing studies
demonstrate that adversarial prompts can induce GPTs to leak knowledge file
content. Yet, it remains uncertain whether additional leakage vectors exist,
particularly given the complex data flows across clients, servers, and
databases in GPTs. In this paper, we present a comprehensive risk assessment of
knowledge file leakage, leveraging a novel workflow inspired by Data Security
Posture Management (DSPM). Through the analysis of 651,022 GPT metadata, 11,820
flows, and 1,466 responses, we identify five leakage vectors: metadata, GPT
initialization, retrieval, sandboxed execution environments, and prompts. These
vectors enable adversaries to extract sensitive knowledge file data such as
titles, content, types, and sizes. Notably, the activation of the built-in tool
Code Interpreter leads to a privilege escalation vulnerability, enabling
adversaries to directly download original knowledge files with a 95.95% success
rate. Further analysis reveals that 28.80% of leaked files are copyrighted,
including digital copies from major publishers and internal materials from a
listed company. In the end, we provide actionable solutions for GPT builders
and platform providers to secure the GPT data supply chain.

</details>


### [792] [Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution](https://arxiv.org/abs/2506.01055)
*Meysam Alizadeh,Zeynab Samei,Daria Stetsenko,Fabrizio Gilardi*

Key words: 提示注入, 数据泄露, 大型语言模型, 代理安全, 防御措施

TL;DR: 本文研究提示注入攻击如何导致工具调用代理泄露个人数据，通过虚构银行代理展示了攻击效果，结果显示攻击成功率平均20%，部分防御措施能降至0%。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 探讨复杂威胁如数据泄露的提示注入攻击，补充现有研究对通用任务的局限性。

Method: 利用虚构银行代理设计基于数据流的攻击，整合至AgentDojo基准并扩展数据集。

Result: 攻击导致LLM效用下降15-50%，平均攻击成功率20%；部分防御措施有效，但密码泄露风险增加。

Conclusion: LLM对敏感数据泄露具有抵抗力，但仍易泄露其他个人数据；任务类型与防御效果密切相关。

Abstract: Previous benchmarks on prompt injection in large language models (LLMs) have
primarily focused on generic tasks and attacks, offering limited insights into
more complex threats like data exfiltration. This paper examines how prompt
injection can cause tool-calling agents to leak personal data observed during
task execution. Using a fictitious banking agent, we develop data flow-based
attacks and integrate them into AgentDojo, a recent benchmark for agentic
security. To enhance its scope, we also create a richer synthetic dataset of
human-AI banking conversations. In 16 user tasks from AgentDojo, LLMs show a
15-50 percentage point drop in utility under attack, with average attack
success rates (ASR) around 20 percent; some defenses reduce ASR to zero. Most
LLMs, even when successfully tricked by the attack, avoid leaking highly
sensitive data like passwords, likely due to safety alignments, but they remain
vulnerable to disclosing other personal data. The likelihood of password
leakage increases when a password is requested along with one or two additional
personal details. In an extended evaluation across 48 tasks, the average ASR is
around 15 percent, with no built-in AgentDojo defense fully preventing leakage.
Tasks involving data extraction or authorization workflows, which closely
resemble the structure of exfiltration attacks, exhibit the highest ASRs,
highlighting the interaction between task type, agent performance, and defense
efficacy.

</details>


### [793] [3D Gaussian Splat Vulnerabilities](https://arxiv.org/abs/2506.00280)
*Matthew Hull,Haoyang Yang,Pratham Mehta,Mansi Phute,Aeree Cho,Haoran Wang,Matthew Lau,Wenke Lee,Willian T. Lunardi,Martin Andreoni,Polo Chau*

Key words: 3D高斯溅射, 对抗攻击, 安全关键应用, Faster R-CNN

TL;DR: 本文提出CLOAK和DAGGER两种攻击方法，利用3DGS（3D高斯溅射）的视角依赖性特性，通过特定视角嵌入对抗内容或直接扰动3D高斯来欺骗目标检测器。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 随着3DGS在安全关键应用中的广泛使用，研究其潜在的对抗性攻击漏洞至关重要。

Method: CLOAK利用视角依赖的高斯外观嵌入对抗内容；DAGGER则直接扰动3D高斯，无需训练数据即可欺骗目标检测器。

Result: 这些攻击成功揭示了3DGS在安全关键应用中的潜在威胁。

Conclusion: 3DGS存在未充分探索的漏洞，可能对机器人学习和自动驾驶等应用构成新威胁。

Abstract: With 3D Gaussian Splatting (3DGS) being increasingly used in safety-critical
applications, how can an adversary manipulate the scene to cause harm? We
introduce CLOAK, the first attack that leverages view-dependent Gaussian
appearances - colors and textures that change with viewing angle - to embed
adversarial content visible only from specific viewpoints. We further
demonstrate DAGGER, a targeted adversarial attack directly perturbing 3D
Gaussians without access to underlying training data, deceiving multi-stage
object detectors e.g., Faster R-CNN, through established methods such as
projected gradient descent. These attacks highlight underexplored
vulnerabilities in 3DGS, introducing a new potential threat to robotic learning
for autonomous navigation and other safety-critical 3DGS applications.

</details>


### [794] [PackHero: A Scalable Graph-based Approach for Efficient Packer Identification](https://arxiv.org/abs/2506.00659)
*Marco Di Gennaro,Mario D'Onghia,Mario Polino,Stefano Zanero,Michele Carminati*

Key words: 打包程序识别, 图匹配网络, 聚类, 静态分析, 机器学习

TL;DR: PackHero是一种高效、可扩展的静态方法，用于识别打包程序，采用图匹配网络和聚类技术，显著提升了性能并减少了所需样本量。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 现有打包程序识别方法（如基于签名或机器学习）存在灵活性不足、依赖大量训练数据的问题，因此需要一种更高效、适应性强的解决方案。

Method: PackHero利用图匹配网络和聚类技术，对已知打包程序的调用图进行匹配和分组。

Result: 在公开数据集上测试，PackHero在仅需每个打包程序10个样本的情况下，宏平均F1分数达93.7%，100个样本时提升至98.3%。

Conclusion: PackHero在性能上媲美现有基于签名的方法，并在处理虚拟化打包程序（如Themida/Winlicense）时表现更优，召回率达100%。

Abstract: Anti-analysis techniques, particularly packing, challenge malware analysts,
making packer identification fundamental. Existing packer identifiers have
significant limitations: signature-based methods lack flexibility and struggle
against dynamic evasion, while Machine Learning approaches require extensive
training data, limiting scalability and adaptability. Consequently, achieving
accurate and adaptable packer identification remains an open problem. This
paper presents PackHero, a scalable and efficient methodology for identifying
packers using a novel static approach. PackHero employs a Graph Matching
Network and clustering to match and group Call Graphs from programs packed with
known packers. We evaluate our approach on a public dataset of malware and
benign samples packed with various packers, demonstrating its effectiveness and
scalability across varying sample sizes. PackHero achieves a macro-average
F1-score of 93.7% with just 10 samples per packer, improving to 98.3% with 100
samples. Notably, PackHero requires fewer samples to achieve stable performance
compared to other Machine Learning-based tools. Overall, PackHero matches the
performance of State-of-the-art signature-based tools, outperforming them in
handling Virtualization-based packers such as Themida/Winlicense, with a recall
of 100%.

</details>


### [795] [SPEAR: Security Posture Evaluation using AI Planner-Reasoning on Attack-Connectivity Hypergraphs](https://arxiv.org/abs/2506.01227)
*Rakesh Podder,Turgay Caglar,Shadaab Kawnain Bashir,Sarath Sreedharan,Indrajit Ray,Indrakshi Ray*

Key words: 网络安全, 攻击图, AI规划, PDDL, SPEAR

TL;DR: SPEAR是一个基于AI规划的框架，用于网络安全评估和分析，支持人类专家参与，自动生成多样化的网络加固策略。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 当前网络加固中，攻击图的网络连接参数、不完全信息的推理、易懂的管理员建议及假设分析仍存不足。

Method: 利用AI规划的因果形式化方法建模网络漏洞与配置，自动转换为PDDL语言模型，生成多样化策略。

Result: 提供了可理解的网络加固策略，支持管理员系统化探索解决方案并评估影响。

Conclusion: SPEAR填补了网络安全评估和加固策略生成工具中的关键空白。

Abstract: Graph-based frameworks are often used in network hardening to help a cyber
defender understand how a network can be attacked and how the best defenses can
be deployed. However, incorporating network connectivity parameters in the
attack graph, reasoning about the attack graph when we do not have access to
complete information, providing system administrator suggestions in an
understandable format, and allowing them to do what-if analysis on various
scenarios and attacker motives is still missing. We fill this gap by presenting
SPEAR, a formal framework with tool support for security posture evaluation and
analysis that keeps human-in-the-loop. SPEAR uses the causal formalism of AI
planning to model vulnerabilities and configurations in a networked system. It
automatically converts network configurations and vulnerability descriptions
into planning models expressed in the Planning Domain Definition Language
(PDDL). SPEAR identifies a set of diverse security hardening strategies that
can be presented in a manner understandable to the domain expert. These allow
the administrator to explore the network hardening solution space in a
systematic fashion and help evaluate the impact and compare the different
solutions.

</details>


### [796] [Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models](https://arxiv.org/abs/2506.01307)
*Youze Wang,Wenbo Hu,Yinpeng Dong,Jing Liu,Hanwang Zhang,Richang Hong*

Key words: 多模态大语言模型, 越狱攻击, 安全性对齐, 图像-文本交互, 对抗生成

TL;DR: 该论文提出了一种统一的多模态通用越狱攻击框架，利用图像-文本迭代交互和迁移策略生成通用对抗后缀和图像，揭示了当前多模态大语言模型在安全性对齐上的不足。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 多模态大语言模型（MLLMs）整合了视觉和其他类型数据，但文本越狱攻击暴露了其安全隐患，需要研究新型安全风险。

Method: 提出一种基于图像-文本交互和迁移策略的通用多模态越狱攻击框架，评估其在多种MLLM中的效果。

Result: 攻击框架能跨不同MLLM生成更高质量的不可控输出，验证了现有安全机制对多模态攻击的脆弱性。

Conclusion: 研究呼吁加强对MLLMs的安全协议审查和增强，以应对多模态能力带来的潜在风险。

Abstract: Large Language Models (LLMs) have evolved into Multimodal Large Language
Models (MLLMs), significantly enhancing their capabilities by integrating
visual information and other types, thus aligning more closely with the nature
of human intelligence, which processes a variety of data forms beyond just
text. Despite advancements, the undesirable generation of these models remains
a critical concern, particularly due to vulnerabilities exposed by text-based
jailbreak attacks, which have represented a significant threat by challenging
existing safety protocols. Motivated by the unique security risks posed by the
integration of new and old modalities for MLLMs, we propose a unified
multimodal universal jailbreak attack framework that leverages iterative
image-text interactions and transfer-based strategy to generate a universal
adversarial suffix and image. Our work not only highlights the interaction of
image-text modalities can be used as a critical vulnerability but also
validates that multimodal universal jailbreak attacks can bring higher-quality
undesirable generations across different MLLMs. We evaluate the undesirable
context generation of MLLMs like LLaVA, Yi-VL, MiniGPT4, MiniGPT-v2, and
InstructBLIP, and reveal significant multimodal safety alignment issues,
highlighting the inadequacy of current safety mechanisms against sophisticated
multimodal attacks. This study underscores the urgent need for robust safety
measures in MLLMs, advocating for a comprehensive review and enhancement of
security protocols to mitigate potential risks associated with multimodal
capabilities.

</details>


### [797] [ETDI: Mitigating Tool Squatting and Rug Pull Attacks in Model Context Protocol (MCP) by using OAuth-Enhanced Tool Definitions and Policy-Based Access Control](https://arxiv.org/abs/2506.01333)
*Manish Bhatt,Vineeth Sai Narajala,Idan Habler*

Key words: MCP, ETDI, 安全扩展, OAuth 2.0, 策略控制

TL;DR: ETDI作为安全扩展增强MCP，通过加密验证和权限管理防止工具中毒和跑路攻击。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 标准MCP存在安全漏洞（如工具中毒和跑路攻击），需要更安全可控的AI生态系统。

Method: 引入ETDI，结合加密验证、不可变工具定义和OAuth 2.0权限管理，并扩展基于策略的访问控制。

Result: 建立了更安全、可信且可控的AI应用生态系统。

Conclusion: ETDI有效提升MCP安全性，适用于需要与外部工具集成的LLM应用。

Abstract: The Model Context Protocol (MCP) plays a crucial role in extending the
capabilities of Large Language Models (LLMs) by enabling integration with
external tools and data sources. However, the standard MCP specification
presents significant security vulnerabilities, notably Tool Poisoning and Rug
Pull attacks. This paper introduces the Enhanced Tool Definition Interface
(ETDI), a security extension designed to fortify MCP. ETDI incorporates
cryptographic identity verification, immutable versioned tool definitions, and
explicit permission management, often leveraging OAuth 2.0. We further propose
extending MCP with fine-grained, policy-based access control, where tool
capabilities are dynamically evaluated against explicit policies using a
dedicated policy engine, considering runtime context beyond static OAuth
scopes. This layered approach aims to establish a more secure, trustworthy, and
controllable ecosystem for AI applications interacting with LLMs and external
tools.

</details>


### [798] [System Calls for Malware Detection and Classification: Methodologies and Applications](https://arxiv.org/abs/2506.01412)
*Bishwajit Prasad Gond,Durga Prasad Mohapatra*

Key words: 恶意软件分析,系统调用,API调用,机器学习,静态分析,动态分析

TL;DR: 摘要探讨了系统调用在恶意软件检测和分类中的应用，结合静态与动态分析、沙盒等技术，以及机器学习和统计分析，以区分正常与恶意行为。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 恶意软件日益复杂，检测难度增加，亟需利用系统调用和API调用等核心通信机制提升检测能力。

Method: 结合静态分析、动态分析、沙盒技术，并运用机器学习、统计分析和异常检测技术分析系统调用模式。

Result: 通过分析系统调用模式，能够有效区分正常与恶意行为，覆盖Windows、Linux和Android等系统。

Conclusion: 系统调用分析是恶意软件检测的重要手段，结合先进技术可提升检测效果。

Abstract: As malware continues to become more complex and harder to detect, Malware
Analysis needs to continue to evolve to stay one step ahead. One promising key
area approach focuses on using system calls and API Calls, the core
communication between user applications and the operating system and their
kernels. These calls provide valuable insight into how software or programs
behaves, making them an useful tool for spotting suspicious or harmful activity
of programs and software. This chapter takes a deep down look at how system
calls are used in malware detection and classification, covering techniques
like static and dynamic analysis, as well as sandboxing. By combining these
methods with advanced techniques like machine learning, statistical analysis,
and anomaly detection, researchers can analyze system call patterns to tell the
difference between normal and malicious behavior. The chapter also explores how
these techniques are applied across different systems, including Windows,
Linux, and Android, while also looking at the ways sophisticated malware tries
to evade detection.

</details>


### [799] [ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs](https://arxiv.org/abs/2506.01770)
*Zeming Wei,Chengcan Wu,Meng Sun*

Key words: 大语言模型、模型分析、安全关键表示、可扩展性、人工智能安全

TL;DR: ReGA是一种基于模型的分析框架，通过表示引导的抽象技术，解决了大语言模型（LLMs）在安全性和可扩展性方面的问题，有效区分安全与有害输入。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 针对大语言模型在生成有害内容和易受攻击方面的安全隐患，现有的基于模型的分析方法因可扩展性问题难以适用。

Method: 提出ReGA框架，利用安全关键表示（低维方向）构建抽象模型，解决可扩展性问题。

Result: ReGA在区分安全与有害输入方面表现优异，AUROC指标在提示级别为0.975，对话级别为0.985，且对实际攻击具有鲁棒性。

Conclusion: ReGA通过将表示工程与基于模型的抽象结合，为增强LLM安全性提供了一种高效且可扩展的解决方案。

Abstract: Large Language Models (LLMs) have achieved significant success in various
tasks, yet concerns about their safety and security have emerged. In
particular, they pose risks in generating harmful content and vulnerability to
jailbreaking attacks. To analyze and monitor machine learning models,
model-based analysis has demonstrated notable potential in stateful deep neural
networks, yet suffers from scalability issues when extending to LLMs due to
their vast feature spaces. In this paper, we propose ReGA, a model-based
analysis framework with representation-guided abstraction, to safeguard LLMs
against harmful prompts and generations. By leveraging safety-critical
representations, which are low-dimensional directions emerging in hidden states
that indicate safety-related concepts, ReGA effectively addresses the
scalability issue when constructing the abstract model for safety modeling. Our
comprehensive evaluation shows that ReGA performs sufficiently well in
distinguishing between safe and harmful inputs, achieving an AUROC of 0.975 at
the prompt level and 0.985 at the conversation level. Additionally, ReGA
exhibits robustness to real-world attacks and generalization across different
safety perspectives, outperforming existing safeguard paradigms in terms of
interpretability and scalability. Overall, ReGA serves as an efficient and
scalable solution to enhance LLM safety by integrating representation
engineering with model-based abstraction, paving the way for new paradigms to
utilize software insights for AI safety. Our code is available at
https://github.com/weizeming/ReGA.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [800] [Diff-SPORT: Diffusion-based Sensor Placement Optimization and Reconstruction of Turbulent flows in urban environments](https://arxiv.org/abs/2506.00214)
*Abhijeet Vishwasrao,Sai Bharath Chandra Gutha,Andres Cremades,Klas Wijk,Aakash Patil,Catherine Gorle,Beverley J McKeon,Hossein Azizpour,Ricardo Vinuesa*

Key words: 城市化、风场重建、扩散模型、传感器优化、可持续智能

TL;DR: Diff-SPORT是一种基于扩散模型的高精度湍流重建和传感器优化放置框架，结合了最大后验推理和Shapley值框架，显著提升效率并保持精度，适用于快速城市化中的风场监测。

<details>
  <summary>Details</summary>

Main category: physics.flu-dyn

Motivation: 快速城市化需要高效准确的风场监测以支持空气质量、气候适应性和基础设施设计，传统方法在实用性约束下精度下降严重。

Method: 结合生成扩散模型、最大后验推理和Shapley值框架，提出可扩展且可解释的Diff-SPORT方法。

Result: 相比传统数值方法，Diff-SPORT显著加速且保持了统计和瞬时流场的保真度。

Conclusion: Diff-SPORT为可持续城市智能提供了模块化、零样本的解决方案，支持极端稀疏条件下的快速可靠监测。

Abstract: Rapid urbanization demands accurate and efficient monitoring of turbulent
wind patterns to support air quality, climate resilience and infrastructure
design. Traditional sparse reconstruction and sensor placement strategies face
major accuracy degradations under practical constraints. Here, we introduce
Diff-SPORT, a diffusion-based framework for high-fidelity flow reconstruction
and optimal sensor placement in urban environments. Diff-SPORT combines a
generative diffusion model with a maximum a posteriori (MAP) inference scheme
and a Shapley-value attribution framework to propose a scalable and
interpretable solution. Compared to traditional numerical methods, Diff-SPORT
achieves significant speedups while maintaining both statistical and
instantaneous flow fidelity. Our approach offers a modular, zero-shot
alternative to retraining-intensive strategies, supporting fast and reliable
urban flow monitoring under extreme sparsity. Diff-SPORT paves the way for
integrating generative modeling and explainability in sustainable urban
intelligence.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [801] [DiffPINN: Generative diffusion-initialized physics-informed neural networks for accelerating seismic wavefield representation](https://arxiv.org/abs/2506.00471)
*Shijun Cheng,Tariq Alkhalifah*

Key words: 物理信息神经网络、潜在扩散、自编码器、速度模型、波场建模

TL;DR: 提出一种基于潜在扩散的策略，用于快速初始化物理信息神经网络（PINNs），以解决不同速度模型下训练耗时和收敛慢的问题。

<details>
  <summary>Details</summary>

Main category: physics.geo-ph

Motivation: 传统PINNs在不同速度模型上需要重新训练且收敛慢，亟需一种高效初始化方法。

Method: 通过训练多个PINN生成参数向量，利用自编码器学习潜在表示，并训练条件扩散模型生成新模型的参数。

Result: 该方法显著加速训练，并在各种速度模型下保持高精度。

Conclusion: 潜在扩散策略为PINNs的高效初始化提供了新途径。

Abstract: Physics-informed neural networks (PINNs) offer a powerful framework for
seismic wavefield modeling, yet they typically require time-consuming
retraining when applied to different velocity models. Moreover, their training
can suffer from slow convergence due to the complexity of of the wavefield
solution. To address these challenges, we introduce a latent diffusion-based
strategy for rapid and effective PINN initialization. First, we train multiple
PINNs to represent frequency-domain scattered wavefields for various velocity
models, then flatten each trained network's parameters into a one-dimensional
vector, creating a comprehensive parameter dataset. Next, we employ an
autoencoder to learn latent representations of these parameter vectors,
capturing essential patterns across diverse PINN's parameters. We then train a
conditional diffusion model to store the distribution of these latent vectors,
with the corresponding velocity models serving as conditions. Once trained,
this diffusion model can generate latent vectors corresponding to new velocity
models, which are subsequently decoded by the autoencoder into complete PINN
parameters. Experimental results indicate that our method significantly
accelerates training and maintains high accuracy across in-distribution and
out-of-distribution velocity scenarios.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [802] [A Reinforcement Learning-Based Telematic Routing Protocol for the Internet of Underwater Things](https://arxiv.org/abs/2506.00133)
*Mohammadhossein Homaei,Mehran Tarif,Agustin Di Bartolo,Oscar Mogollon Gutierrez,Mar Avila*

Key words: IoUT, 强化学习, 路由协议, 能效, RPL

TL;DR: 针对水下物联网（IoUT）的低带宽、高延迟等问题，论文提出RL-RPL-UA协议，通过强化学习优化路由性能，兼容RPL并提升数据包传递率与能效。

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 传统路由协议（如RPL）在水下环境中表现不佳，需适配低带宽、高延迟等挑战。

Method: 采用轻量级强化学习代理，基于局部信息（如丢包率、能量等）动态选择最优父节点，兼容RPL并引入动态目标函数。

Result: 仿真显示，RL-RPL-UA提升数据包传递率9.2%，降低能耗14.8%，延长网络寿命80秒。

Conclusion: RL-RPL-UA是一种高效节能的水下网络路由解决方案。

Abstract: The Internet of Underwater Things (IoUT) faces major challenges such as low
bandwidth, high latency, mobility, and limited energy resources. Traditional
routing protocols like RPL, which were designed for land-based networks, do not
perform well in these underwater conditions. This paper introduces RL-RPL-UA, a
new routing protocol that uses reinforcement learning to improve performance in
underwater environments. Each node includes a lightweight RL agent that selects
the best parent node based on local information such as packet delivery ratio,
buffer level, link quality, and remaining energy. RL-RPL-UA keeps full
compatibility with standard RPL messages and adds a dynamic objective function
to support real-time decision-making. Simulations using Aqua-Sim show that
RL-RPL-UA increases packet delivery by up to 9.2%, reduces energy use per
packet by 14.8%, and extends network lifetime by 80 seconds compared to
traditional methods. These results suggest that RL-RPL-UA is a promising and
energy-efficient routing solution for underwater networks.

</details>


### [803] [Bridging Subjective and Objective QoE: Operator-Level Aggregation Using LLM-Based Comment Analysis and Network MOS Comparison](https://arxiv.org/abs/2506.00924)
*Parsa Hassani Shariat Panahi,Amir Hossein Jalilvand,M. Hasan Najafi*

Key words: QoE评估，机器学习，语义分析，用户反馈，网络性能

TL;DR: 提出了一种双层的QoE评估框架，结合客观网络建模和主观用户反馈，通过机器学习模型和语义分析提升评估准确性。

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 传统的QoE评估方法通常依赖客户端工具或视频内容，本研究旨在通过网络参数和用户评论实现更高效的评估。

Method: 开发了基于ITU-T P.1203的机器学习模型预测视频质量，并使用LLM处理用户评论以提取QoE反馈。构建了一个标注数据集支持分析。

Result: 构建了47,894条评论的数据集，34,000条与QoE相关。提出delta MOS指标检测局部服务降级，验证了框架在中断模拟中的有效性。

Conclusion: 该框架实现了通过网络参数和用户反馈的实时QoE评估，为运营商提供了高效、可扩展的分析工具。

Abstract: This paper introduces a dual-layer framework for network operator-side
quality of experience (QoE) assessment that integrates both objective network
modeling and subjective user perception extracted from live-streaming
platforms. On the objective side, we develop a machine learning model trained
on mean opinion scores (MOS) computed via the ITU-T P.1203 reference
implementation, allowing accurate prediction of user-perceived video quality
using only network parameters such as packet loss, delay, jitter, and
throughput without reliance on video content or client-side instrumentation. On
the subjective side, we present a semantic filtering and scoring pipeline that
processes user comments from live streams to extract performance-related
feedback. A large language model is used to assign scalar MOS scores to
filtered comments in a deterministic and reproducible manner. To support
scalable and interpretable analysis, we construct a labeled dataset of 47,894
live-stream comments, of which about 34,000 are identified as QoE-relevant
through multi-layer semantic filtering. Each comment is enriched with simulated
Internet Service Provider attribution and temporally aligned using synthetic
timestamps in 5-min intervals. The resulting dataset enables operator-level
aggregation and time-series analysis of user-perceived quality. A delta MOS
metric is proposed to measure each Internet service provider's deviation from
platform-wide sentiment, allowing detection of localized degradations even in
the absence of direct network telemetry. A controlled outage simulation
confirms the framework's effectiveness in identifying service disruptions
through comment-based trends alone. The system provides each operator with its
own subjective MOS and the global platform average per interval, enabling
real-time interpretation of performance deviations and comparison with
objective network-based QoE estimates.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [804] [The Disparate Effects of Partial Information in Bayesian Strategic Learning](https://arxiv.org/abs/2506.00627)
*Srikanth Avasarala,Serena Wang,Juba Ziani*

Key words: 策略学习, 公平性, 透明度, 噪声信号, 贝叶斯代理

TL;DR: 研究评分规则的部分信息如何影响策略学习中的公平性，分析不同透明度和代理模型对结果差异的影响。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 探讨在策略学习中，代理无法直接观察评分规则而是接收噪声信号时，不同成本群体的结果差异如何产生，以及透明度对公平性的影响。

Method: 对比天真代理（直接信任信号）和贝叶斯代理（基于信号更新先验信念）两种模型，分析透明度与噪声对群体差异的影响。

Result: 天真代理的效用差异可能随噪声无限增长，而贝叶斯代理的差异有界。透明度与噪声的关系非单调，差异常在中透明度时最小。

Conclusion: 群体间成本差异和先验信念不对称会显著影响公平性，中透明度可能是平衡差异的关键。

Abstract: We study how partial information about scoring rules affects fairness in
strategic learning settings. In strategic learning, a learner deploys a scoring
rule, and agents respond strategically by modifying their features -- at some
cost -- to improve their outcomes. However, in our work, agents do not observe
the scoring rule directly; instead, they receive a noisy signal of said rule.
We consider two different agent models: (i) naive agents, who take the noisy
signal at face value, and (ii) Bayesian agents, who update a prior belief based
on the signal.
  Our goal is to understand how disparities in outcomes arise between groups
that differ in their costs of feature modification, and how these disparities
vary with the level of transparency of the learner's rule. For naive agents, we
show that utility disparities can grow unboundedly with noise, and that the
group with lower costs can, perhaps counter-intuitively, be disproportionately
harmed under limited transparency. In contrast, for Bayesian agents,
disparities remain bounded. We provide a full characterization of disparities
across groups as a function of the level of transparency and show that they can
vary non-monotonically with noise; in particular, disparities are often
minimized at intermediate levels of transparency. Finally, we extend our
analysis to settings where groups differ not only in cost, but also in prior
beliefs, and study how this asymmetry influences fairness.

</details>


### [805] [Empirical Validation of the Independent Chip Model](https://arxiv.org/abs/2506.00180)
*Juho Kim*

Key words: 独立筹码模型（ICM）、扑克锦标赛、实证验证、评估偏差

TL;DR: 该论文通过对超过一万场扑克锦标赛数据的分析，实证验证了独立筹码模型（ICM）的表现，发现其在现实中存在对筹码多寡玩家的价值评估偏差。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 独立筹码模型（ICM）虽然在现代扑克锦标赛策略中占据重要地位，但其在现实中的表现尚未得到充分验证，尤其是在大规模数据下的表现。

Method: 作者通过收集和分析超过一万场扑克锦标赛的数据，设计了两组实验：验证ICM相较于基线的准确性，以及观察ICM对不同筹码量玩家的评估偏差。

Result: 研究发现，ICM在准确性上优于所提出的基线模型，但会低估筹码量较大玩家的表现，同时高估筹码量较小玩家的表现。

Conclusion: 这项研究为未来开发改进扑克锦标赛中玩家价值估算算法提供了有用的实证依据。

Abstract: The independent chip model (ICM) forms a cornerstone of all modern poker
tournament strategy. However, despite its prominence, the ICM's performance in
the real world has not been sufficiently scrutinized, especially at a large
scale. In this paper, we introduce our new dataset of poker tournaments,
consisting of results of over ten thousand events. Then, using this dataset, we
perform two experiments as part of a large-scale empirical validation of the
ICM. First, we verify that the ICM performs more accurately than a baseline we
propose. Second, we obtain empirical evidence of the ICM underestimating the
performances of players with larger stacks while overestimating those who are
short-stacked. Our contributions may be useful to future researchers developing
new algorithms for estimating a player's value in poker tournaments.

</details>


### [806] [General search techniques without common knowledge for imperfect-information games, and application to superhuman Fog of War chess](https://arxiv.org/abs/2506.01242)
*Brian Hu Zhang,Tuomas Sandholm*

Key words: AI, 不完全信息博弈, 雾战象棋, 搜索算法, 超人性能

TL;DR: 论文介绍了一款名为Obscuro的超人AI，用于解决雾战象棋（FoW chess）这类不完全信息博弈问题，展示了其强大的搜索和推理能力。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: AI在不完全信息博弈中表现超越人类的需求推动了对雾战象棋的研究，这是继德州扑克后的又一重要挑战。

Method: Obscuro通过改进不完全信息博弈中的搜索算法，实现了强大且可扩展的推理能力。

Result: 实验表明，Obscuro在对抗现有最佳AI和人类顶尖玩家时表现显著优于对手，成为首个在雾战象棋中超越人类的AI。

Conclusion: Obscuro成功展示了在大规模不完全信息博弈中实现超人性能的可行性，为未来相关研究提供了新方向。

Abstract: Since the advent of AI, games have served as progress benchmarks. Meanwhile,
imperfect-information variants of chess have existed for over a century,
present extreme challenges, and have been the focus of significant AI research.
Beyond calculation needed in regular chess, they require reasoning about
information gathering, the opponent's knowledge, signaling, etc. The most
popular variant, Fog of War (FoW) chess (aka. dark chess) is a recognized
challenge problem in AI after superhuman performance was reached in no-limit
Texas hold'em poker. We present Obscuro, the first superhuman AI for FoW chess.
It introduces advances to search in imperfect-information games, enabling
strong, scalable reasoning. Experiments against the prior state-of-the-art AI
and human players -- including the world's best -- show that Obscuro is
significantly stronger. FoW chess is the largest (by amount of imperfect
information) turn-based game in which superhuman performance has been achieved
and the largest game in which imperfect-information search has been
successfully applied.

</details>


### [807] [Geometry Meets Incentives: Sample-Efficient Incentivized Exploration with Linear Contexts](https://arxiv.org/abs/2506.01685)
*Benjamin Schiffer,Mark Sellke*

Key words: 激励式探索,高维上下文,线性赌博机,激励兼容,样本复杂度

TL;DR: 研究在激励式探索模型中，如何在高维背景下通过几何条件消除初始探索的指数样本复杂度，实现高效的激励兼容算法。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 在激励式探索模型中，初始探索阶段的高维上下文可能导致指数级样本复杂度，阻碍高效学习。研究旨在探索如何通过几何条件避免这一问题。

Method: 采用线性赌博机模型，动作集合为欧几里得单位球，提出一种激励兼容的探索算法，样本复杂度与维度等参数呈多项式关系。

Result: 提出的算法在高维背景下，通过几何条件实现了多项式级的样本复杂度，解决了初始探索的指数复杂度问题。

Conclusion: 在特定几何条件下，激励兼容性与最优后悔性能可以共存，为高维激励探索问题提供了可行的解决方案。

Abstract: In the incentivized exploration model, a principal aims to explore and learn
over time by interacting with a sequence of self-interested agents. It has been
recently understood that the main challenge in designing incentive-compatible
algorithms for this problem is to gather a moderate amount of initial data,
after which one can obtain near-optimal regret via posterior sampling. With
high-dimensional contexts, however, this \emph{initial exploration} phase
requires exponential sample complexity in some cases, which prevents efficient
learning unless initial data can be acquired exogenously. We show that these
barriers to exploration disappear under mild geometric conditions on the set of
available actions, in which case incentive-compatibility does not preclude
regret-optimality. Namely, we consider the linear bandit model with actions in
the Euclidean unit ball, and give an incentive-compatible exploration algorithm
with sample complexity that scales polynomially with the dimension and other
parameters.

</details>


### [808] [Should Decision-Makers Reveal Classifiers in Online Strategic Classification?](https://arxiv.org/abs/2506.01936)
*Han Shao,Shuo Xie,Kunhe Yang*

Key words: 战略分类、在线学习、分类器隐藏、特征操纵、决策者性能

TL;DR: 论文研究了在战略分类中限制对手（agents）对当前分类器的访问如何影响决策者的性能，发现隐藏分类器可能导致更多错误。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 探讨决策者是否应公开分类器，以防止对手通过操纵特征获取有利预测，实践中这一做法存在争议。

Method: 扩展了在线战略分类模型，假设对手基于历史分类器的加权平均进行操纵，而非直接访问当前分类器。

Result: 决策者在隐藏分类器的情况下，错误率增加为 $(1-\gamma)^{-1}$ 或 $k_{\text{in}}$ 倍，其中 $k_{\text{in}}$ 是操纵图的最大入度，$\gamma$ 是折扣因子。

Conclusion: 隐藏分类器可能适得其反，降低了决策者在在线战略分类中的表现。

Abstract: Strategic classification addresses a learning problem where a decision-maker
implements a classifier over agents who may manipulate their features in order
to receive favorable predictions. In the standard model of online strategic
classification, in each round, the decision-maker implements and publicly
reveals a classifier, after which agents perfectly best respond based on this
knowledge. However, in practice, whether to disclose the classifier is often
debated -- some decision-makers believe that hiding the classifier can prevent
misclassification errors caused by manipulation.
  In this paper, we formally examine how limiting the agents' access to the
current classifier affects the decision-maker's performance. Specifically, we
consider an extended online strategic classification setting where agents lack
direct knowledge about the current classifier and instead manipulate based on a
weighted average of historically implemented classifiers. Our main result shows
that in this setting, the decision-maker incurs $(1-\gamma)^{-1}$ or
$k_{\text{in}}$ times more mistakes compared to the full-knowledge setting,
where $k_{\text{in}}$ is the maximum in-degree of the manipulation graph
(representing how many distinct feature vectors can be manipulated to appear as
a single one), and $\gamma$ is the discount factor indicating agents' memory of
past classifiers. Our results demonstrate how withholding access to the
classifier can backfire and degrade the decision-maker's performance in online
strategic classification.

</details>


### [809] [Online Competitive Information Gathering for Partially Observable Trajectory Games](https://arxiv.org/abs/2506.01927)
*Mel Krusniak,Hang Xu,Parker Palermo,Forrest Laine*

Key words: 部分可观察随机博弈、信息获取、轨迹规划、在线计算、随机梯度下降

TL;DR: 论文提出了一种在部分可观察的随机博弈（POSG）中在线计算合理轨迹规划的方法，通过粒子估计和随机梯度下降实现，展示了在连续追踪和多玩家场景中的优越性。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 解决完全连续POSG中信息规划的高计算复杂度问题，提出在线方法以实现高效的信息获取行为。

Method: 采用有限历史和时域优化的POSG模型，结合粒子估计和随机梯度下降技术，优化轨迹规划。

Result: 该方法在连续追踪和仓库取货等场景中表现出主动信息获取能力，并优于被动策略。

Conclusion: 提出的在线方法能有效处理复杂环境中的信息获取问题，适用于多玩家和障碍物场景。

Abstract: Game-theoretic agents must make plans that optimally gather information about
their opponents. These problems are modeled by partially observable stochastic
games (POSGs), but planning in fully continuous POSGs is intractable without
heavy offline computation or assumptions on the order of belief maintained by
each player. We formulate a finite history/horizon refinement of POSGs which
admits competitive information gathering behavior in trajectory space, and
through a series of approximations, we present an online method for computing
rational trajectory plans in these games which leverages particle-based
estimations of the joint state space and performs stochastic gradient play. We
also provide the necessary adjustments required to deploy this method on
individual agents. The method is tested in continuous pursuit-evasion and
warehouse-pickup scenarios (alongside extensions to $N > 2$ players and to more
complex environments with visual and physical obstacles), demonstrating
evidence of active information gathering and outperforming passive competitors.

</details>
