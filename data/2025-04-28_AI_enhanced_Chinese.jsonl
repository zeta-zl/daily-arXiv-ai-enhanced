{"id": "2504.17838", "pdf": "https://arxiv.org/pdf/2504.17838", "abs": "https://arxiv.org/abs/2504.17838", "authors": ["Bernhard Jaeger", "Daniel Dauner", "Jens Bei\u00dfwenger", "Simon Gerstenecker", "Kashyap Chitta", "Andreas Geiger"], "title": "CaRL: Learning Scalable Planning Policies with Simple Rewards", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "We investigate reinforcement learning (RL) for privileged planning in\nautonomous driving. State-of-the-art approaches for this task are rule-based,\nbut these methods do not scale to the long tail. RL, on the other hand, is\nscalable and does not suffer from compounding errors like imitation learning.\nContemporary RL approaches for driving use complex shaped rewards that sum\nmultiple individual rewards, \\eg~progress, position, or orientation rewards. We\nshow that PPO fails to optimize a popular version of these rewards when the\nmini-batch size is increased, which limits the scalability of these approaches.\nInstead, we propose a new reward design based primarily on optimizing a single\nintuitive reward term: route completion. Infractions are penalized by\nterminating the episode or multiplicatively reducing route completion. We find\nthat PPO scales well with higher mini-batch sizes when trained with our simple\nreward, even improving performance. Training with large mini-batch sizes\nenables efficient scaling via distributed data parallelism. We scale PPO to\n300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The\nresulting model achieves 64 DS on the CARLA longest6 v2 benchmark,\noutperforming other RL methods with more complex rewards by a large margin.\nRequiring only minimal adaptations from its use in CARLA, the same method is\nthe best learning-based approach on nuPlan. It scores 91.3 in non-reactive and\n90.6 in reactive traffic on the Val14 benchmark while being an order of\nmagnitude faster than prior work.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u66ff\u4ee3\u4f20\u7edf\u89c4\u5219\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u4e00\u76f4\u89c2\u5956\u52b1\uff08\u8def\u7ebf\u5b8c\u6210\u5ea6\uff09\u7684\u65b0\u5956\u52b1\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u957f\u5c3e\u573a\u666f\uff0c\u800c\u73b0\u6709RL\u65b9\u6cd5\u4e2d\u7684\u590d\u6742\u5956\u52b1\u8bbe\u8ba1\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5956\u52b1\u8bbe\u8ba1\u3002", "method": "\u901a\u8fc7\u7b80\u5316\u5956\u52b1\u8bbe\u8ba1\uff08\u4e3b\u8981\u4ee5\u8def\u7ebf\u5b8c\u6210\u5ea6\u4e3a\u6838\u5fc3\uff09\uff0c\u4f7f\u7528PPO\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u5728\u5927\u6279\u6b21\u8bad\u7ec3\u4e2d\u9a8c\u8bc1\u5176\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728CARLA\u548cnuPlan\u5e73\u53f0\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709RL\u65b9\u6cd5\uff0cCARLA\u4e0a\u8fbe\u523064 DS\uff0cnuPlan\u4e0a\u975e\u53cd\u5e94\u6027\u548c\u53cd\u5e94\u6027\u4ea4\u901a\u5f97\u5206\u5206\u522b\u4e3a91.3\u548c90.6\u3002", "conclusion": "\u7b80\u5355\u5956\u52b1\u8bbe\u8ba1\u7ed3\u5408PPO\u548c\u5927\u6279\u6b21\u8bad\u7ec3\uff0c\u53ef\u5b9e\u73b0\u9ad8\u6548\u6269\u5c55\u5e76\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6027\u80fd\uff0c\u540c\u65f6\u5177\u5907\u8de8\u5e73\u53f0\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2504.17857", "pdf": "https://arxiv.org/pdf/2504.17857", "abs": "https://arxiv.org/abs/2504.17857", "authors": ["A. J Miller", "Fangzhou Yu", "Michael Brauckmann", "Farbod Farshidian"], "title": "High-Performance Reinforcement Learning on Spot: Optimizing Simulation Parameters with Distributional Measures", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "This work presents an overview of the technical details behind a high\nperformance reinforcement learning policy deployment with the Spot RL\nResearcher Development Kit for low level motor access on Boston Dynamics Spot.\nThis represents the first public demonstration of an end to end end\nreinforcement learning policy deployed on Spot hardware with training code\npublicly available through Nvidia IsaacLab and deployment code available\nthrough Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean\nDiscrepancy to quantify the distributional dissimilarity of data collected on\nhardware and in simulation to measure our sim2real gap. We use these measures\nas a scoring function for the Covariance Matrix Adaptation Evolution Strategy\nto optimize simulated parameters that are unknown or difficult to measure from\nSpot. Our procedure for modeling and training produces high quality\nreinforcement learning policies capable of multiple gaits, including a flight\nphase. We deploy policies capable of over 5.2ms locomotion, more than triple\nSpots default controller maximum speed, robustness to slippery surfaces,\ndisturbance rejection, and overall agility previously unseen on Spot. We detail\nour method and release our code to support future work on Spot with the low\nlevel API.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u5728\u6ce2\u58eb\u987f\u52a8\u529bSpot\u673a\u5668\u4eba\u4e0a\u90e8\u7f72\u9ad8\u6027\u80fd\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u6280\u672f\u7ec6\u8282\uff0c\u5305\u62ec\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u5ea6\u91cf\u3001\u4f18\u5316\u65b9\u6cd5\u53ca\u5b9e\u73b0\u7684\u591a\u9879\u7a81\u7834\u6027\u6210\u679c\u3002", "motivation": "\u65e8\u5728\u5c55\u793a\u5982\u4f55\u5728Spot\u786c\u4ef6\u4e0a\u7aef\u5230\u7aef\u90e8\u7f72\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u516c\u5f00\u8bad\u7ec3\u548c\u90e8\u7f72\u4ee3\u7801\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002", "method": "\u4f7f\u7528Wasserstein\u8ddd\u79bb\u548c\u6700\u5927\u5747\u503c\u5dee\u5f02\u91cf\u5316\u6a21\u62df\u4e0e\u786c\u4ef6\u6570\u636e\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u4f5c\u4e3aCMA-ES\u4f18\u5316\u7684\u8bc4\u5206\u51fd\u6570\uff0c\u4f18\u5316\u672a\u77e5\u53c2\u6570\uff0c\u8bad\u7ec3\u9ad8\u8d28\u91cf\u7b56\u7565\u3002", "result": "\u5b9e\u73b0\u4e865.2\u7c73/\u79d2\u7684\u8fd0\u52a8\u901f\u5ea6\uff08\u8d85\u9ed8\u8ba4\u63a7\u5236\u5668\u4e09\u500d\uff09\uff0c\u6297\u6ed1\u8868\u9762\u3001\u5e72\u6270\u6291\u5236\u548c\u9ad8\u654f\u6377\u6027\u7b49\u591a\u91cd\u7a81\u7834\u3002", "conclusion": "\u65b9\u6cd5\u6709\u6548\uff0c\u7b56\u7565\u6027\u80fd\u663e\u8457\uff0c\u4ee3\u7801\u516c\u5f00\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2504.17891", "pdf": "https://arxiv.org/pdf/2504.17891", "abs": "https://arxiv.org/abs/2504.17891", "authors": ["Karmanbir Batth", "Krish Sethi", "Aly Shariff", "Leo Shi", "Hetul Patel"], "title": "Do We Need Transformers to Play FPS Video Games?", "categories": ["cs.LG"], "comment": null, "summary": "In this paper, we explore the Transformer based architectures for\nreinforcement learning in both online and offline settings within the Doom game\nenvironment. Our investigation focuses on two primary approaches: Deep\nTransformer Q- learning Networks (DTQN) for online learning and Decision\nTransformers (DT) for offline reinforcement learning. DTQN leverages the\nsequential modelling capabilities of Transformers to enhance Q-learning in\npartially observable environments,while Decision Transformers repurpose\nsequence modelling techniques to enable offline agents to learn from past\ntrajectories without direct interaction with the environment. We conclude that\nwhile Transformers might have performed well in Atari games, more traditional\nmethods perform better than Transformer based method in both the settings in\nthe VizDoom environment.", "AI": {"tldr": "Transformer\u67b6\u6784\u5728VizDoom\u73af\u5883\u4e2d\u7684\u5728\u7ebf\u4e0e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u8868\u73b0\u4e0d\u5982\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22Transformer\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728Doom\u6e38\u620f\u73af\u5883\u4e2d\u7684\u5728\u7ebf\uff08DTQN\uff09\u4e0e\u79bb\u7ebf\uff08DT\uff09\u8bbe\u7f6e\u3002", "method": "\u5bf9\u6bd4Deep Transformer Q-learning Networks (DTQN)\u7528\u4e8e\u5728\u7ebf\u5b66\u4e60\u4e0eDecision Transformers (DT)\u7528\u4e8e\u79bb\u7ebf\u5b66\u4e60\u7684\u8868\u73b0\u3002", "result": "Transformer\u65b9\u6cd5\u5728Atari\u6e38\u620f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728VizDoom\u73af\u5883\u4e2d\u4f20\u7edf\u65b9\u6cd5\u4f18\u4e8eTransformer\u3002", "conclusion": "Transformer\u5728VizDoom\u73af\u5883\u4e2d\u7684\u5728\u7ebf\u4e0e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u4e0d\u53ca\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2504.17929", "pdf": "https://arxiv.org/pdf/2504.17929", "abs": "https://arxiv.org/abs/2504.17929", "authors": ["Ayesha Siddique", "Khurram Khalil", "Khaza Anuarul Hoque"], "title": "ApproXAI: Energy-Efficient Hardware Acceleration of Explainable AI using Approximate Computing", "categories": ["cs.AI", "cs.AR"], "comment": "Accepted at the International Joint Conference on Neural Networks\n  (IJCNN), June 30th - July 5th, 2025 in Rome, Italy", "summary": "Explainable artificial intelligence (XAI) enhances AI system transparency by\nframing interpretability as an optimization problem. However, this approach\noften necessitates numerous iterations of computationally intensive operations,\nlimiting its applicability in real-time scenarios. While recent research has\nfocused on XAI hardware acceleration on FPGAs and TPU, these methods do not\nfully address energy efficiency in real-time settings. To address this\nlimitation, we propose XAIedge, a novel framework that leverages approximate\ncomputing techniques into XAI algorithms, including integrated gradients, model\ndistillation, and Shapley analysis. XAIedge translates these algorithms into\napproximate matrix computations and exploits the synergy between convolution,\nFourier transform, and approximate computing paradigms. This approach enables\nefficient hardware acceleration on TPU-based edge devices, facilitating faster\nreal-time outcome interpretations. Our comprehensive evaluation demonstrates\nthat XAIedge achieves a $2\\times$ improvement in energy efficiency compared to\nexisting accurate XAI hardware acceleration techniques while maintaining\ncomparable accuracy. These results highlight the potential of XAIedge to\nsignificantly advance the deployment of explainable AI in energy-constrained\nreal-time applications.", "AI": {"tldr": "XAIedge\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5229\u7528\u8fd1\u4f3c\u8ba1\u7b97\u6280\u672f\u4f18\u5316XAI\u7b97\u6cd5\uff0c\u5728TPU\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u80fd\u6548\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u73b0\u6709XAI\u786c\u4ef6\u52a0\u901f\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u89e3\u51b3\u5b9e\u65f6\u573a\u666f\u4e0b\u7684\u80fd\u6548\u95ee\u9898\uff0c\u9650\u5236\u4e86XAI\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u901a\u8fc7\u5c06XAI\u7b97\u6cd5\uff08\u5982\u79ef\u5206\u68af\u5ea6\u3001\u6a21\u578b\u84b8\u998f\u548cShapley\u5206\u6790\uff09\u8f6c\u5316\u4e3a\u8fd1\u4f3c\u77e9\u9635\u8ba1\u7b97\uff0c\u5e76\u7ed3\u5408\u5377\u79ef\u3001\u5085\u91cc\u53f6\u53d8\u6362\u4e0e\u8fd1\u4f3c\u8ba1\u7b97\u8303\u5f0f\uff0c\u5728TPU\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u52a0\u901f\u3002", "result": "XAIedge\u5728\u80fd\u6548\u4e0a\u6bd4\u73b0\u6709\u7cbe\u786eXAI\u786c\u4ef6\u52a0\u901f\u6280\u672f\u63d0\u53472\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "XAIedge\u4e3a\u80fd\u6e90\u53d7\u9650\u7684\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u53ef\u89e3\u91caAI\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2504.17908", "pdf": "https://arxiv.org/pdf/2504.17908", "abs": "https://arxiv.org/abs/2504.17908", "authors": ["Luiz Antonio Nicolau Anghinoni", "Gustavo Weber Denardin", "Jadson Castro Gertrudes", "Dalcimar Casanova", "Jefferson Tales Oliva"], "title": "The use of Multi-domain Electroencephalogram Representations in the building of Models based on Convolutional and Recurrent Neural Networks for Epilepsy Detection", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Epilepsy, affecting approximately 50 million people globally, is\ncharacterized by abnormal brain activity and remains challenging to treat. The\ndiagnosis of epilepsy relies heavily on electroencephalogram (EEG) data, where\nspecialists manually analyze epileptiform patterns across pre-ictal, ictal,\npost-ictal, and interictal periods. However, the manual analysis of EEG signals\nis prone to variability between experts, emphasizing the need for automated\nsolutions. Although previous studies have explored preprocessing techniques and\nmachine learning approaches for seizure detection, there is a gap in\nunderstanding how the representation of EEG data (time, frequency, or\ntime-frequency domains) impacts the predictive performance of deep learning\nmodels. This work addresses this gap by systematically comparing deep neural\nnetworks trained on EEG data in these three domains. Through the use of\nstatistical tests, we identify the optimal data representation and model\narchitecture for epileptic seizure detection. The results demonstrate that\nfrequency-domain data achieves detection metrics exceeding 97\\%, providing a\nrobust foundation for more accurate and reliable seizure detection systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6bd4\u8f83\u65f6\u95f4\u3001\u9891\u7387\u548c\u65f6\u9891\u57dfEEG\u6570\u636e\u8868\u793a\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u9891\u7387\u57df\u6570\u636e\u5728\u766b\u75eb\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u766b\u75eb\u8bca\u65ad\u4f9d\u8d56\u4e13\u5bb6\u624b\u52a8\u5206\u6790EEG\u4fe1\u53f7\uff0c\u5b58\u5728\u4e3b\u89c2\u5dee\u5f02\uff0c\u81ea\u52a8\u5316\u9700\u6c42\u8feb\u5207\uff1b\u73b0\u6709\u7814\u7a76\u672a\u7cfb\u7edf\u8bc4\u4f30EEG\u6570\u636e\u8868\u793a\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u6bd4\u65f6\u95f4\u57df\u3001\u9891\u7387\u57df\u548c\u65f6\u9891\u57df\u7684EEG\u6570\u636e\u8868\u793a\uff0c\u901a\u8fc7\u7edf\u8ba1\u6d4b\u8bd5\u786e\u5b9a\u6700\u4f73\u6570\u636e\u8868\u793a\u548c\u6a21\u578b\u67b6\u6784\u3002", "result": "\u9891\u7387\u57df\u6570\u636e\u5728\u766b\u75eb\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u8d85\u8fc797%\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u8868\u793a\u65b9\u6cd5\u3002", "conclusion": "\u9891\u7387\u57df\u6570\u636e\u662f\u766b\u75eb\u68c0\u6d4b\u4e2d\u6700\u4f18\u7684EEG\u8868\u793a\u65b9\u5f0f\uff0c\u4e3a\u66f4\u51c6\u786e\u53ef\u9760\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.17967", "pdf": "https://arxiv.org/pdf/2504.17967", "abs": "https://arxiv.org/abs/2504.17967", "authors": ["Kevin Song", "Andrew Trotter", "Jake Y. Chen"], "title": "LLM Agent Swarm for Hypothesis-Driven Drug Discovery", "categories": ["cs.AI"], "comment": "15 pages, 3 figures", "summary": "Drug discovery remains a formidable challenge: more than 90 percent of\ncandidate molecules fail in clinical evaluation, and development costs often\nexceed one billion dollars per approved therapy. Disparate data streams, from\ngenomics and transcriptomics to chemical libraries and clinical records, hinder\ncoherent mechanistic insight and slow progress. Meanwhile, large language\nmodels excel at reasoning and tool integration but lack the modular\nspecialization and iterative memory required for regulated, hypothesis-driven\nworkflows. We introduce PharmaSwarm, a unified multi-agent framework that\norchestrates specialized LLM \"agents\" to propose, validate, and refine\nhypotheses for novel drug targets and lead compounds. Each agent accesses\ndedicated functionality--automated genomic and expression analysis; a curated\nbiomedical knowledge graph; pathway enrichment and network simulation;\ninterpretable binding affinity prediction--while a central Evaluator LLM\ncontinuously ranks proposals by biological plausibility, novelty, in silico\nefficacy, and safety. A shared memory layer captures validated insights and\nfine-tunes underlying submodels over time, yielding a self-improving system.\nDeployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm\nsupports literature-driven discovery, omics-guided target identification, and\nmarket-informed repurposing. We also describe a rigorous four-tier validation\npipeline spanning retrospective benchmarking, independent computational assays,\nexperimental testing, and expert user studies to ensure transparency,\nreproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm\ncan accelerate translational research and deliver high-confidence hypotheses\nmore efficiently than traditional pipelines.", "AI": {"tldr": "PharmaSwarm\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u4e13\u95e8\u7684LLM\u4ee3\u7406\u6765\u63d0\u51fa\u3001\u9a8c\u8bc1\u548c\u4f18\u5316\u65b0\u836f\u9776\u70b9\u548c\u5148\u5bfc\u5316\u5408\u7269\u7684\u5047\u8bbe\uff0c\u4ee5\u52a0\u901f\u836f\u7269\u53d1\u73b0\u3002", "motivation": "\u836f\u7269\u53d1\u73b0\u6210\u672c\u9ad8\u3001\u5931\u8d25\u7387\u9ad8\uff0c\u4e14\u4e0d\u540c\u6570\u636e\u6d41\u96be\u4ee5\u6574\u5408\uff0c\u963b\u788d\u4e86\u673a\u5236\u6027\u6d1e\u89c1\u7684\u83b7\u53d6\u548c\u8fdb\u5c55\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u548c\u5de5\u5177\u96c6\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u6a21\u5757\u5316\u4e13\u4e1a\u5316\u548c\u8fed\u4ee3\u8bb0\u5fc6\u80fd\u529b\u3002", "method": "PharmaSwarm\u6846\u67b6\u7ed3\u5408\u4e86\u591a\u4e2a\u4e13\u7528LLM\u4ee3\u7406\uff0c\u5404\u4ee3\u7406\u5177\u5907\u7279\u5b9a\u529f\u80fd\uff08\u5982\u57fa\u56e0\u7ec4\u5206\u6790\u3001\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u7b49\uff09\uff0c\u5e76\u901a\u8fc7\u4e2d\u592e\u8bc4\u4f30LLM\u5bf9\u5047\u8bbe\u8fdb\u884c\u6301\u7eed\u8bc4\u4f30\u548c\u4f18\u5316\u3002", "result": "PharmaSwarm\u652f\u6301\u6587\u732e\u9a71\u52a8\u53d1\u73b0\u3001\u7ec4\u5b66\u5f15\u5bfc\u7684\u9776\u70b9\u8bc6\u522b\u548c\u5e02\u573a\u5bfc\u5411\u7684\u518d\u5229\u7528\uff0c\u5e76\u901a\u8fc7\u56db\u5c42\u9a8c\u8bc1\u6d41\u7a0b\u786e\u4fdd\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u590d\u6027\u3002", "conclusion": "PharmaSwarm\u4f5c\u4e3aAI\u526f\u9a7e\u9a76\uff0c\u80fd\u591f\u6bd4\u4f20\u7edf\u6d41\u7a0b\u66f4\u9ad8\u6548\u5730\u52a0\u901f\u8f6c\u5316\u7814\u7a76\u5e76\u63d0\u4f9b\u9ad8\u53ef\u4fe1\u5ea6\u7684\u5047\u8bbe\u3002"}}
{"id": "2504.17974", "pdf": "https://arxiv.org/pdf/2504.17974", "abs": "https://arxiv.org/abs/2504.17974", "authors": ["Sabur Butt", "Fazlourrahman Balouchzahi", "Ahmad Imam Amjad", "Maaz Amjad", "Hector G. Ceballos", "Salud Maria Jimenez-Zafra"], "title": "Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English", "categories": ["cs.CL"], "comment": null, "summary": "Hope is a complex and underexplored emotional state that plays a significant\nrole in education, mental health, and social interaction. Unlike basic\nemotions, hope manifests in nuanced forms ranging from grounded optimism to\nexaggerated wishfulness or sarcasm, making it difficult for Natural Language\nProcessing systems to detect accurately. This study introduces PolyHope V2, a\nmultilingual, fine-grained hope speech dataset comprising over 30,000 annotated\ntweets in English and Spanish. This resource distinguishes between four hope\nsubtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances\nexisting datasets by explicitly labeling sarcastic instances. We benchmark\nmultiple pretrained transformer models and compare them with large language\nmodels (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes.\nOur findings show that fine-tuned transformers outperform prompt-based LLMs,\nespecially in distinguishing nuanced hope categories and sarcasm. Through\nqualitative analysis and confusion matrices, we highlight systematic challenges\nin separating closely related hope subtypes. The dataset and results provide a\nrobust foundation for future emotion recognition tasks that demand greater\nsemantic and contextual sensitivity across languages.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u5e0c\u671b\u7684\u590d\u6742\u60c5\u611f\uff0c\u5f00\u53d1\u4e86\u591a\u8bed\u8a00\u6570\u636e\u96c6PolyHope V2\uff0c\u5e76\u6bd4\u8f83\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5e0c\u671b\u4f5c\u4e3a\u4e00\u79cd\u590d\u6742\u60c5\u611f\u5728\u6559\u80b2\u3001\u5fc3\u7406\u5065\u5eb7\u548c\u793e\u4ea4\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u7cbe\u51c6\u68c0\u6d4b\u56e0\u8868\u73b0\u5f62\u5f0f\u591a\u6837\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u5305\u542b30,000\u6761\u6807\u6ce8\u63a8\u6587\u7684\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u533a\u5206\u56db\u79cd\u5e0c\u671b\u5b50\u7c7b\u578b\uff0c\u5e76\u6d4b\u8bd5\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u5fae\u8c03\u540e\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u8bc6\u522b\u5e0c\u671b\u5b50\u7c7b\u578b\u548c\u8bbd\u523a\u5185\u5bb9\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5c24\u5176\u5728\u533a\u5206\u7ec6\u5fae\u5dee\u5f02\u65f6\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u7814\u7a76\u6210\u679c\u4e3a\u672a\u6765\u9700\u8981\u66f4\u9ad8\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u7684\u8de8\u8bed\u8a00\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2504.17913", "pdf": "https://arxiv.org/pdf/2504.17913", "abs": "https://arxiv.org/abs/2504.17913", "authors": ["Mert Sonmezer", "Seyda Ertekin"], "title": "CANet: ChronoAdaptive Network for Enhanced Long-Term Time Series Forecasting under Non-Stationarity", "categories": ["cs.LG"], "comment": null, "summary": "Long-term time series forecasting plays a pivotal role in various real-world\napplications. Despite recent advancements and the success of different\narchitectures, forecasting is often challenging due to non-stationary nature of\nthe real-world data, which frequently exhibit distribution shifts and temporal\nchanges in statistical properties like mean and variance over time. Previous\nstudies suggest that this inherent variability complicates forecasting,\nlimiting the performance of many models by leading to loss of non-stationarity\nand resulting in over-stationarization (Liu, Wu, Wang and Long, 2022). To\naddress this challenge, we introduce a novel architecture, ChoronoAdaptive\nNetwork (CANet), inspired by style-transfer techniques. The core of CANet is\nthe Non-stationary Adaptive Normalization module, seamlessly integrating the\nStyle Blending Gate and Adaptive Instance Normalization (AdaIN) (Huang and\nBelongie, 2017). The Style Blending Gate preserves and reintegrates\nnon-stationary characteristics, such as mean and standard deviation, by\nblending internal and external statistics, preventing over-stationarization\nwhile maintaining essential temporal dependencies. Coupled with AdaIN, which\ndynamically adapts the model to statistical changes, this approach enhances\npredictive accuracy under non-stationary conditions. CANet also employs\nmulti-resolution patching to handle short-term fluctuations and long-term\ntrends, along with Fourier analysis-based adaptive thresholding to reduce\nnoise. A Stacked Kronecker Product Layer further optimizes the model's\nefficiency while maintaining high performance. Extensive experiments on\nreal-world datasets validate CANet's superiority over state-of-the-art methods,\nachieving a 42% reduction in MSE and a 22% reduction in MAE. The source code is\npublicly available at https://github.com/mertsonmezer/CANet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCANet\u7684\u65b0\u578b\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u975e\u5e73\u7a33\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u975e\u5e73\u7a33\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u6a21\u5757\u7b49\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u7531\u4e8e\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u7684\u975e\u5e73\u7a33\u6027\uff08\u5982\u5206\u5e03\u504f\u79fb\u548c\u7edf\u8ba1\u5c5e\u6027\u53d8\u5316\uff09\uff0c\u4f20\u7edf\u6a21\u578b\u5bb9\u6613\u51fa\u73b0\u8fc7\u5e73\u7a33\u5316\u95ee\u9898\uff0c\u5bfc\u81f4\u9884\u6d4b\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u6311\u6218\u3002", "method": "CANet\u7ed3\u5408\u4e86\u975e\u5e73\u7a33\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u6a21\u5757\uff08\u5305\u62ecStyle Blending Gate\u548cAdaIN\uff09\u3001\u591a\u5206\u8fa8\u7387\u5206\u5757\u3001\u5085\u91cc\u53f6\u5206\u6790\u81ea\u9002\u5e94\u9608\u503c\u548c\u5806\u53e0\u514b\u7f57\u5185\u514b\u79ef\u5c42\uff0c\u4ee5\u52a8\u6001\u9002\u5e94\u7edf\u8ba1\u53d8\u5316\u5e76\u4fdd\u6301\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCANet\u5728\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u548c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u4e0a\u5206\u522b\u51cf\u5c11\u4e8642%\u548c22%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CANet\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6311\u6218\uff0c\u4e3a\u957f\u671f\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18007", "pdf": "https://arxiv.org/pdf/2504.18007", "abs": "https://arxiv.org/abs/2504.18007", "authors": ["Yazan Otoum", "Amiya Nayak"], "title": "Differential Privacy-Driven Framework for Enhancing Heart Disease Prediction", "categories": ["cs.AI", "cs.CR", "cs.LG"], "comment": "\\c{opyright} 2025 IEEE. Accepted to IEEE International Conference on\n  Communications ICC 2025. Final version to appear in IEEE Xplore", "summary": "With the rapid digitalization of healthcare systems, there has been a\nsubstantial increase in the generation and sharing of private health data.\nSafeguarding patient information is essential for maintaining consumer trust\nand ensuring compliance with legal data protection regulations. Machine\nlearning is critical in healthcare, supporting personalized treatment, early\ndisease detection, predictive analytics, image interpretation, drug discovery,\nefficient operations, and patient monitoring. It enhances decision-making,\naccelerates research, reduces errors, and improves patient outcomes. In this\npaper, we utilize machine learning methodologies, including differential\nprivacy and federated learning, to develop privacy-preserving models that\nenable healthcare stakeholders to extract insights without compromising\nindividual privacy. Differential privacy introduces noise to data to guarantee\nstatistical privacy, while federated learning enables collaborative model\ntraining across decentralized datasets. We explore applying these technologies\nto Heart Disease Data, demonstrating how they preserve privacy while delivering\nvaluable insights and comprehensive analysis. Our results show that using a\nfederated learning model with differential privacy achieved a test accuracy of\n85%, ensuring patient data remained secure and private throughout the process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u548c\u8054\u90a6\u5b66\u4e60\u7684\u9690\u79c1\u4fdd\u62a4\u6a21\u578b\uff0c\u7528\u4e8e\u533b\u7597\u6570\u636e\u5206\u6790\uff0c\u5728\u4fdd\u8bc1\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e8685%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740\u533b\u7597\u7cfb\u7edf\u6570\u5b57\u5316\uff0c\u60a3\u8005\u9690\u79c1\u6570\u636e\u4fdd\u62a4\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u800c\u673a\u5668\u5b66\u4e60\u5728\u533b\u7597\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u9700\u8981\u786e\u4fdd\u6570\u636e\u9690\u79c1\u3002", "method": "\u91c7\u7528\u5dee\u5206\u9690\u79c1\uff08\u6dfb\u52a0\u566a\u58f0\u4fdd\u969c\u9690\u79c1\uff09\u548c\u8054\u90a6\u5b66\u4e60\uff08\u5206\u6563\u6570\u636e\u96c6\u4e0a\u7684\u534f\u4f5c\u8bad\u7ec3\uff09\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e\u5fc3\u810f\u75c5\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u7684\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u8fbe\u5230\u4e8685%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u6570\u636e\u9690\u79c1\u5b89\u5168\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5dee\u5206\u9690\u79c1\u4e0e\u8054\u90a6\u5b66\u4e60\u7684\u7ed3\u5408\u80fd\u6709\u6548\u4fdd\u62a4\u533b\u7597\u6570\u636e\u9690\u79c1\uff0c\u540c\u65f6\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u5206\u6790\u7ed3\u679c\u3002"}}
{"id": "2504.17993", "pdf": "https://arxiv.org/pdf/2504.17993", "abs": "https://arxiv.org/abs/2504.17993", "authors": ["Brihi Joshi", "Xiang Ren", "Swabha Swayamdipta", "Rik Koncel-Kedziorski", "Tim Paek"], "title": "Improving LLM Personas via Rationalization with Psychological Scaffolds", "categories": ["cs.CL"], "comment": null, "summary": "Language models prompted with a user description or persona can predict a\nuser's preferences and opinions, but existing approaches to building personas\n-- based solely on a user's demographic attributes and/or prior judgments --\nfail to capture the underlying reasoning behind said user judgments. We\nintroduce PB&J (Psychology of Behavior and Judgments), a framework that\nimproves LLM personas by incorporating rationales of why a user might make\nspecific judgments. These rationales are LLM-generated, and aim to reason about\na user's behavior on the basis of their experiences, personality traits or\nbeliefs. This is done using psychological scaffolds -- structured frameworks\ngrounded in theories such as the Big 5 Personality Traits and Primal World\nBeliefs -- that help provide structure to the generated rationales. Experiments\non public opinion and movie preference prediction tasks demonstrate that LLM\npersonas augmented with PB&J rationales consistently outperform methods using\nonly a user's demographics and/or judgments. Additionally, LLM personas\nconstructed using scaffolds describing user beliefs perform competitively with\nthose using human-written rationales.", "AI": {"tldr": "PB&J\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u5fc3\u7406\u652f\u67b6\u751f\u6210\u7684\u89e3\u91ca\uff0c\u589e\u5f3a\u4e86\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u7528\u6237\u89d2\u8272\u7684\u80fd\u529b\uff0c\u4ece\u800c\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u7528\u6237\u504f\u597d\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7528\u6237\u4eba\u53e3\u7edf\u8ba1\u6216\u5148\u524d\u5224\u65ad\u7684\u89d2\u8272\u6784\u5efa\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u7528\u6237\u5224\u65ad\u80cc\u540e\u7684\u6df1\u5c42\u539f\u56e0\uff0cPB&J\u65e8\u5728\u901a\u8fc7\u5fc3\u7406\u5b66\u7406\u8bba\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5229\u7528\u5fc3\u7406\u5b66\u7406\u8bba\uff08\u5982\u5927\u4e94\u4eba\u683c\u548c\u539f\u59cb\u4e16\u754c\u4fe1\u5ff5\uff09\u8bbe\u8ba1\u7ed3\u6784\u5316\u652f\u67b6\uff0c\u751f\u6210\u89e3\u91ca\u7528\u6237\u5224\u65ad\u7684\u7406\u6027\u4f9d\u636e\u3002", "result": "\u5728\u516c\u5171\u610f\u89c1\u548c\u7535\u5f71\u504f\u597d\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cPB&J\u589e\u5f3a\u7684\u89d2\u8272\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u4eba\u53e3\u7edf\u8ba1\u6216\u5224\u65ad\u7684\u65b9\u6cd5\uff0c\u4e14\u4e0e\u4eba\u5de5\u64b0\u5199\u89e3\u91ca\u7684\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "PB&J\u901a\u8fc7\u5fc3\u7406\u5b66\u7406\u8bba\u63d0\u5347\u89d2\u8272\u6784\u5efa\u7684\u89e3\u91ca\u6027\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u7528\u6237\u504f\u597d\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u57fa\u7840\u3002"}}
{"id": "2504.17921", "pdf": "https://arxiv.org/pdf/2504.17921", "abs": "https://arxiv.org/abs/2504.17921", "authors": ["Mateo Espinosa Zarlenga", "Gabriele Dominici", "Pietro Barbiero", "Zohreh Shams", "Mateja Jamnik"], "title": "Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.HC"], "comment": null, "summary": "In this paper, we investigate how concept-based models (CMs) respond to\nout-of-distribution (OOD) inputs. CMs are interpretable neural architectures\nthat first predict a set of high-level concepts (e.g., stripes, black) and then\npredict a task label from those concepts. In particular, we study the impact of\nconcept interventions (i.e., operations where a human expert corrects a CM's\nmispredicted concepts at test time) on CMs' task predictions when inputs are\nOOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we\nterm leakage poisoning, that prevents them from properly improving their\naccuracy when intervened on for OOD inputs. To address this, we introduce\nMixCEM, a new CM that learns to dynamically exploit leaked information missing\nfrom its concepts only when this information is in-distribution. Our results\nacross tasks with and without complete sets of concept annotations demonstrate\nthat MixCEMs outperform strong baselines by significantly improving their\naccuracy for both in-distribution and OOD samples in the presence and absence\nof concept interventions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u6982\u5ff5\u7684\u6a21\u578b\uff08CMs\uff09\u5bf9\u5206\u5e03\u5916\uff08OOD\uff09\u8f93\u5165\u7684\u54cd\u5e94\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6700\u5148\u8fdbCMs\u7684\u201c\u6cc4\u6f0f\u4e2d\u6bd2\u201d\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6a21\u578bMixCEM\u6765\u52a8\u6001\u5229\u7528\u6cc4\u6f0f\u4fe1\u606f\u3002", "motivation": "\u63a2\u8ba8\u6982\u5ff5\u5e72\u9884\u5bf9CMs\u5728OOD\u8f93\u5165\u60c5\u51b5\u4e0b\u7684\u4efb\u52a1\u9884\u6d4b\u5f71\u54cd\uff0c\u5e76\u8bc6\u522b\u5f53\u524dCMs\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faMixCEM\u6a21\u578b\uff0c\u52a8\u6001\u5229\u7528\u6cc4\u6f0f\u4fe1\u606f\u4ee5\u63d0\u9ad8\u5728\u5206\u5e03\u5185\u548cOOD\u8f93\u5165\u4e0b\u7684\u51c6\u786e\u7387\u3002", "result": "MixCEM\u5728\u6709\u65e0\u6982\u5ff5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\uff0c\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "MixCEM\u6709\u6548\u89e3\u51b3\u4e86CMs\u5728OOD\u8f93\u5165\u65f6\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2504.18039", "pdf": "https://arxiv.org/pdf/2504.18039", "abs": "https://arxiv.org/abs/2504.18039", "authors": ["Zheng Zhang", "Nuoqian Xiao", "Qi Chai", "Deheng Ye", "Hao Wang"], "title": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and Theory of Mind", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Model (LLM) agents have demonstrated impressive capabilities\nin social deduction games (SDGs) like Werewolf, where strategic reasoning and\nsocial deception are essential. However, current approaches remain limited to\ntextual information, ignoring crucial multimodal cues such as facial\nexpressions and tone of voice that humans naturally use to communicate.\nMoreover, existing SDG agents primarily focus on inferring other players'\nidentities without modeling how others perceive themselves or fellow players.\nTo address these limitations, we use One Night Ultimate Werewolf (ONUW) as a\ntestbed and present MultiMind, the first framework integrating multimodal\ninformation into SDG agents. MultiMind processes facial expressions and vocal\ntones alongside verbal content, while employing a Theory of Mind (ToM) model to\nrepresent each player's suspicion levels toward others. By combining this ToM\nmodel with Monte Carlo Tree Search (MCTS), our agent identifies communication\nstrategies that minimize suspicion directed at itself. Through comprehensive\nevaluation in both agent-versus-agent simulations and studies with human\nplayers, we demonstrate MultiMind's superior performance in gameplay. Our work\npresents a significant advancement toward LLM agents capable of human-like\nsocial reasoning across multimodal domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MultiMind\u6846\u67b6\uff0c\u8fd9\u662f\u9996\u4e2a\u5728\u591a\u6a21\u6001\u4fe1\u606f\uff08\u5982\u9762\u90e8\u8868\u60c5\u548c\u8bed\u97f3\u8bed\u8c03\uff09\u57fa\u7840\u4e0a\u7ed3\u5408\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7684\u793e\u4ea4\u63a8\u7406\u4ee3\u7406\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u4ee3\u7406\u4ec5\u4f9d\u8d56\u6587\u672c\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u6c9f\u901a\u4e2d\u7684\u591a\u6a21\u6001\u7ebf\u7d22\uff08\u5982\u9762\u90e8\u8868\u60c5\u548c\u8bed\u8c03\uff09\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u73a9\u5bb6\u95f4\u76f8\u4e92\u5fc3\u7406\u611f\u77e5\u7684\u5efa\u6a21\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4ee5One Night Ultimate Werewolf\u6e38\u620f\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7ed3\u5408ToM\u6a21\u578b\u548cMCTS\uff0cMultiMind\u901a\u8fc7\u5206\u6790\u591a\u6a21\u6001\u6570\u636e\uff08\u9762\u90e8\u8868\u60c5\u3001\u8bed\u8c03\uff09\u548c\u8bed\u8a00\u5185\u5bb9\uff0c\u4f18\u5316\u4ee3\u7406\u7684\u6c9f\u901a\u7b56\u7565\u4ee5\u51cf\u5c11\u81ea\u8eab\u5acc\u7591\u3002", "result": "\u901a\u8fc7\u4e0e\u4ee3\u7406\u548c\u4eba\u7c7b\u73a9\u5bb6\u7684\u5e7f\u6cdb\u6d4b\u8bd5\uff0cMultiMind\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6e38\u620f\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u548c\u5fc3\u7406\u7406\u8bba\u6a21\u578b\u5728\u793e\u4ea4\u63a8\u7406\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "MultiMind\u7684\u63d0\u51fa\u6807\u5fd7\u7740LLM\u4ee3\u7406\u5728\u591a\u6a21\u6001\u793e\u4ea4\u63a8\u7406\u9886\u57df\u7684\u91cd\u5927\u8fdb\u5c55\uff0c\u4e3a\u66f4\u63a5\u8fd1\u4eba\u7c7b\u793e\u4ea4\u80fd\u529b\u7684AI\u4ee3\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.18012", "pdf": "https://arxiv.org/pdf/2504.18012", "abs": "https://arxiv.org/abs/2504.18012", "authors": ["Zhuang Yu", "Shiliang Sun", "Jing Zhao", "Tengfei Song", "Hao Yang"], "title": "Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Machine Translation (MMT) aims to improve translation quality by\nleveraging auxiliary modalities such as images alongside textual input. While\nrecent advances in large-scale pre-trained language and vision models have\nsignificantly benefited unimodal natural language processing tasks, their\neffectiveness and role in MMT remain underexplored. In this work, we conduct a\nsystematic study on the impact of pre-trained encoders and decoders in\nmultimodal translation models. Specifically, we analyze how different training\nstrategies, from training from scratch to using pre-trained and partially\nfrozen components, affect translation performance under a unified MMT\nframework. Experiments are carried out on the Multi30K and CoMMuTE dataset\nacross English-German and English-French translation tasks. Our results reveal\nthat pre-training plays a crucial yet asymmetrical role in multimodal settings:\npre-trained decoders consistently yield more fluent and accurate outputs, while\npre-trained encoders show varied effects depending on the quality of\nvisual-text alignment. Furthermore, we provide insights into the interplay\nbetween modality fusion and pre-trained components, offering guidance for\nfuture architecture design in multimodal translation systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u5728\u591a\u6a21\u6001\u673a\u5668\u7ffb\u8bd1\uff08MMT\uff09\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u89e3\u7801\u5668\u80fd\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u800c\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u6548\u679c\u53d6\u51b3\u4e8e\u89c6\u89c9\u4e0e\u6587\u672c\u7684\u5bf9\u9f50\u8d28\u91cf\u3002", "motivation": "\u63a2\u7d22\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u8bed\u8a00\u548c\u89c6\u89c9\u6a21\u578b\u5728MMT\u4e2d\u7684\u6709\u6548\u6027\u53ca\u4f5c\u7528\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u8fd9\u4e00\u9886\u57df\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u4ece\u96f6\u8bad\u7ec3\u3001\u4f7f\u7528\u9884\u8bad\u7ec3\u53ca\u90e8\u5206\u51bb\u7ed3\u7ec4\u4ef6\u7b49\u4e0d\u540c\u7b56\u7565\uff0c\u5728Multi30K\u548cCoMMuTE\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u82f1\u5fb7\u548c\u82f1\u6cd5\u7ffb\u8bd1\u4efb\u52a1\u7684\u6548\u679c\u3002", "result": "\u9884\u8bad\u7ec3\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u5177\u6709\u4e0d\u5bf9\u79f0\u4f5c\u7528\uff1a\u89e3\u7801\u5668\u80fd\u7a33\u5b9a\u63d0\u5347\u8f93\u51fa\u6d41\u7545\u6027\u548c\u51c6\u786e\u6027\uff0c\u800c\u7f16\u7801\u5668\u7684\u6548\u679c\u53d7\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\u8d28\u91cf\u5f71\u54cd\u3002", "conclusion": "\u63ed\u793a\u4e86\u9884\u8bad\u7ec3\u7ec4\u4ef6\u4e0e\u6a21\u6001\u878d\u5408\u7684\u4ea4\u4e92\u5173\u7cfb\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u7ffb\u8bd1\u7cfb\u7edf\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2504.17946", "pdf": "https://arxiv.org/pdf/2504.17946", "abs": "https://arxiv.org/abs/2504.17946", "authors": ["Fatemeh Vares", "Brittany Johnson"], "title": "Causality-Driven Neural Network Repair: Challenges and Opportunities", "categories": ["cs.LG", "D.2.2; I.2.6"], "comment": "Causality in Software Engineering (CauSE) 2025 Workshop at ESEC/FSE", "summary": "Deep Neural Networks (DNNs) often rely on statistical correlations rather\nthan causal reasoning, limiting their robustness and interpretability. While\ntesting methods can identify failures, effective debugging and repair remain\nchallenging. This paper explores causal inference as an approach primarily for\nDNN repair, leveraging causal debugging, counterfactual analysis, and\nstructural causal models (SCMs) to identify and correct failures. We discuss in\nwhat ways these techniques support fairness, adversarial robustness, and\nbackdoor mitigation by providing targeted interventions. Finally, we discuss\nkey challenges, including scalability, generalization, and computational\nefficiency, and outline future directions for integrating causality-driven\ninterventions to enhance DNN reliability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u56e0\u679c\u63a8\u65ad\uff08\u5982\u56e0\u679c\u8c03\u8bd5\u3001\u53cd\u4e8b\u5b9e\u5206\u6790\u548c\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff09\u6765\u4fee\u590d\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u63d0\u5347\u5176\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u5728\u516c\u5e73\u6027\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u540e\u95e8\u7f13\u89e3\u4e2d\u7684\u5e94\u7528\u53ca\u6311\u6218\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u901a\u5e38\u4f9d\u8d56\u7edf\u8ba1\u76f8\u5173\u6027\u800c\u975e\u56e0\u679c\u63a8\u7406\uff0c\u5bfc\u81f4\u5176\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u53d7\u9650\u3002\u5f53\u524d\u7684\u6d4b\u8bd5\u65b9\u6cd5\u867d\u80fd\u53d1\u73b0\u95ee\u9898\uff0c\u4f46\u6709\u6548\u7684\u8c03\u8bd5\u548c\u4fee\u590d\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\uff0c\u5305\u62ec\u56e0\u679c\u8c03\u8bd5\u3001\u53cd\u4e8b\u5b9e\u5206\u6790\u548c\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCMs\uff09\uff0c\u4ee5\u8bc6\u522b\u548c\u4fee\u6b63DNN\u7684\u6545\u969c\u3002", "result": "\u8fd9\u4e9b\u6280\u672f\u80fd\u591f\u901a\u8fc7\u9488\u5bf9\u6027\u5e72\u9884\u652f\u6301\u516c\u5e73\u6027\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u540e\u95e8\u7f13\u89e3\uff0c\u4f46\u9762\u4e34\u53ef\u6269\u5c55\u6027\u3001\u6cdb\u5316\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7b49\u6311\u6218\u3002", "conclusion": "\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u6574\u5408\u56e0\u679c\u9a71\u52a8\u7684\u5e72\u9884\uff0c\u4ee5\u63d0\u5347DNN\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2504.18096", "pdf": "https://arxiv.org/pdf/2504.18096", "abs": "https://arxiv.org/abs/2504.18096", "authors": ["Xiang Li", "Haixu Ma", "Guanyong Wu", "Shi Mu", "Chen Li", "Shunpan Liang"], "title": "Combating the Bucket Effect:Multi-Knowledge Alignment for Medication Recommendation", "categories": ["cs.AI", "cs.LG"], "comment": "18 pages, 5 figures", "summary": "Medication recommendation is crucial in healthcare, offering effective\ntreatments based on patient's electronic health records (EHR). Previous studies\nshow that integrating more medication-related knowledge improves medication\nrepresentation accuracy. However, not all medications encompass multiple types\nof knowledge data simultaneously. For instance, some medications provide only\ntextual descriptions without structured data. This imbalance in data\navailability limits the performance of existing models, a challenge we term the\n\"bucket effect\" in medication recommendation. Our data analysis uncovers the\nseverity of the \"bucket effect\" in medication recommendation. To fill this gap,\nwe introduce a cross-modal medication encoder capable of seamlessly aligning\ndata from different modalities and propose a medication recommendation\nframework to integrate Multiple types of Knowledge, named MKMed. Specifically,\nwe first pre-train a cross-modal encoder with contrastive learning on five\nknowledge modalities, aligning them into a unified space. Then, we combine the\nmulti-knowledge medication representations with patient records for\nrecommendations. Extensive experiments on the MIMIC-III and MIMIC-IV datasets\ndemonstrate that MKMed mitigates the \"bucket effect\" in data, and significantly\noutperforms state-of-the-art baselines in recommendation accuracy and safety.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MKMed\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u7f16\u7801\u5668\u89e3\u51b3\u836f\u7269\u63a8\u8350\u4e2d\u7684\u201c\u6876\u6548\u5e94\u201d\uff0c\u6574\u5408\u591a\u77e5\u8bc6\u6a21\u6001\u6570\u636e\u63d0\u5347\u63a8\u8350\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u836f\u7269\u63a8\u8350\u6a21\u578b\u56e0\u836f\u7269\u77e5\u8bc6\u6570\u636e\u4e0d\u5e73\u8861\uff08\u591a\u6a21\u6001\u6570\u636e\u4e0d\u7edf\u4e00\uff09\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\uff0c\u9700\u89e3\u51b3\u201c\u6876\u6548\u5e94\u201d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u836f\u7269\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\u5bf9\u9f50\u4e94\u7c7b\u77e5\u8bc6\u6a21\u6001\uff0c\u518d\u7ed3\u5408\u60a3\u8005\u8bb0\u5f55\u8fdb\u884c\u63a8\u8350\u3002", "result": "\u5728MIMIC-III\u548cMIMIC-IV\u6570\u636e\u96c6\u4e0a\uff0cMKMed\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u57fa\u51c6\u6a21\u578b\uff0c\u7f13\u89e3\u4e86\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "conclusion": "MKMed\u6846\u67b6\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u836f\u7269\u77e5\u8bc6\uff0c\u63d0\u5347\u63a8\u8350\u6027\u80fd\uff0c\u4e3a\u533b\u7597\u9886\u57df\u7684\u836f\u7269\u63a8\u8350\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.18041", "pdf": "https://arxiv.org/pdf/2504.18041", "abs": "https://arxiv.org/abs/2504.18041", "authors": ["Bang An", "Shiyue Zhang", "Mark Dredze"], "title": "RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "NAACL 2025", "summary": "Efforts to ensure the safety of large language models (LLMs) include safety\nfine-tuning, evaluation, and red teaming. However, despite the widespread use\nof the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses\non standard LLMs, which means we know little about how RAG use cases change a\nmodel's safety profile. We conduct a detailed comparative analysis of RAG and\nnon-RAG frameworks with eleven LLMs. We find that RAG can make models less safe\nand change their safety profile. We explore the causes of this change and find\nthat even combinations of safe models with safe documents can cause unsafe\ngenerations. In addition, we evaluate some existing red teaming methods for RAG\nsettings and show that they are less effective than when used for non-RAG\nsettings. Our work highlights the need for safety research and red-teaming\nmethods specifically tailored for RAG LLMs.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86RAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u6846\u67b6\u5bf9LLM\uff08\u5927\u8bed\u8a00\u6a21\u578b\uff09\u5b89\u5168\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0RAG\u53ef\u80fd\u964d\u4f4e\u6a21\u578b\u5b89\u5168\u6027\uff0c\u5e76\u63a2\u7d22\u5176\u6210\u56e0\uff0c\u6307\u51fa\u73b0\u6709\u7ea2\u961f\u65b9\u6cd5\u5bf9RAG\u6548\u679c\u4e0d\u4f73\uff0c\u547c\u5401\u9488\u5bf9RAG\u7684\u5b89\u5168\u7814\u7a76\u548c\u7ea2\u961f\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1RAG\u6846\u67b6\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46AI\u5b89\u5168\u7814\u7a76\u4e3b\u8981\u9488\u5bf9\u6807\u51c6LLM\uff0c\u7f3a\u4e4fRAG\u5bf9\u5b89\u5168\u6027\u7684\u5f71\u54cd\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5bf911\u79cdLLM\u8fdb\u884cRAG\u4e0e\u975eRAG\u6846\u67b6\u7684\u5bf9\u6bd4\u5206\u6790\uff0c\u63a2\u7d22\u5b89\u5168\u6027\u53d8\u5316\u539f\u56e0\uff0c\u5e76\u8bc4\u4f30\u73b0\u6709\u7ea2\u961f\u65b9\u6cd5\u5728RAG\u73af\u5883\u4e2d\u7684\u6548\u679c\u3002", "result": "\u53d1\u73b0RAG\u53ef\u80fd\u4f7f\u6a21\u578b\u66f4\u4e0d\u5b89\u5168\uff0c\u751a\u81f3\u5b89\u5168\u6a21\u578b\u4e0e\u5b89\u5168\u6587\u6863\u7ec4\u5408\u4e5f\u4f1a\u4ea7\u751f\u4e0d\u5b89\u5168\u8f93\u51fa\uff0c\u4e14\u73b0\u6709\u7ea2\u961f\u65b9\u6cd5\u5bf9RAG\u6548\u679c\u8f83\u5dee\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u4e13\u95e8\u9488\u5bf9RAG\u7684\u5b89\u5168\u7814\u7a76\u548c\u7ea2\u961f\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u5176\u72ec\u7279\u6311\u6218\u3002"}}
{"id": "2504.17963", "pdf": "https://arxiv.org/pdf/2504.17963", "abs": "https://arxiv.org/abs/2504.17963", "authors": ["Liangzu Peng", "Ren\u00e9 Vidal"], "title": "Mathematics of Continual Learning", "categories": ["cs.LG"], "comment": null, "summary": "Continual learning is an emerging subject in machine learning that aims to\nsolve multiple tasks presented sequentially to the learner without forgetting\npreviously learned tasks. Recently, many deep learning based approaches have\nbeen proposed for continual learning, however the mathematical foundations\nbehind existing continual learning methods remain underdeveloped. On the other\nhand, adaptive filtering is a classic subject in signal processing with a rich\nhistory of mathematically principled methods. However, its role in\nunderstanding the foundations of continual learning has been underappreciated.\nIn this tutorial, we review the basic principles behind both continual learning\nand adaptive filtering, and present a comparative analysis that highlights\nmultiple connections between them. These connections allow us to enhance the\nmathematical foundations of continual learning based on existing results for\nadaptive filtering, extend adaptive filtering insights using existing continual\nlearning methods, and discuss a few research directions for continual learning\nsuggested by the historical developments in adaptive filtering.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6bd4\u8f83\u4e86\u6301\u7eed\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u6ee4\u6ce2\u7684\u6570\u5b66\u57fa\u7840\uff0c\u5e76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u81ea\u9002\u5e94\u6ee4\u6ce2\u7684\u6210\u679c\u6765\u589e\u5f3a\u6301\u7eed\u5b66\u4e60\u7684\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u5f53\u524d\u6301\u7eed\u5b66\u4e60\u7684\u6570\u5b66\u57fa\u7840\u5c1a\u4e0d\u5b8c\u5584\uff0c\u800c\u81ea\u9002\u5e94\u6ee4\u6ce2\u5728\u4fe1\u53f7\u5904\u7406\u9886\u57df\u6709\u4e30\u5bcc\u7684\u6570\u5b66\u7406\u8bba\u3002\u901a\u8fc7\u5bf9\u6bd4\u4e24\u8005\uff0c\u53ef\u4ee5\u501f\u9274\u81ea\u9002\u5e94\u6ee4\u6ce2\u7684\u7406\u8bba\u6765\u63d0\u5347\u6301\u7eed\u5b66\u4e60\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u56de\u987e\u4e86\u6301\u7eed\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u6ee4\u6ce2\u7684\u57fa\u672c\u539f\u7406\uff0c\u5e76\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\uff0c\u627e\u51fa\u4e24\u8005\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "result": "\u53d1\u73b0\u81ea\u9002\u5e94\u6ee4\u6ce2\u7684\u7406\u8bba\u53ef\u4ee5\u589e\u5f3a\u6301\u7eed\u5b66\u4e60\u7684\u6570\u5b66\u57fa\u7840\uff0c\u540c\u65f6\u4e5f\u62d3\u5c55\u4e86\u81ea\u9002\u5e94\u6ee4\u6ce2\u7684\u89c6\u89d2\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u81ea\u9002\u5e94\u6ee4\u6ce2\u5386\u53f2\u7684\u6301\u7eed\u5b66\u4e60\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2504.18443", "pdf": "https://arxiv.org/pdf/2504.18443", "abs": "https://arxiv.org/abs/2504.18443", "authors": ["Simon Dold", "Malte Helmert", "Jakob Nordstr\u00f6m", "Gabriele R\u00f6ger", "Tanja Schindler"], "title": "Pseudo-Boolean Proof Logging for Optimal Classical Planning", "categories": ["cs.AI"], "comment": "35th International Conference on Automated Planning and Scheduling\n  (ICAPS'2025)", "summary": "We introduce lower-bound certificates for classical planning tasks, which can\nbe used to prove the unsolvability of a task or the optimality of a plan in a\nway that can be verified by an independent third party. We describe a general\nframework for generating lower-bound certificates based on pseudo-Boolean\nconstraints, which is agnostic to the planning algorithm used.\n  As a case study, we show how to modify the $A^{*}$ algorithm to produce\nproofs of optimality with modest overhead, using pattern database heuristics\nand $h^\\textit{max}$ as concrete examples. The same proof logging approach\nworks for any heuristic whose inferences can be efficiently expressed as\nreasoning over pseudo-Boolean constraints.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7ecf\u5178\u89c4\u5212\u4efb\u52a1\u7684\u4f4e\u754c\u8bc1\u4e66\uff0c\u80fd\u591f\u8bc1\u660e\u4efb\u52a1\u65e0\u89e3\u6216\u8ba1\u5212\u6700\u4f18\u6027\uff0c\u5e76\u901a\u8fc7\u4f2a\u5e03\u5c14\u7ea6\u675f\u6846\u67b6\u751f\u6210\u8fd9\u4e9b\u8bc1\u4e66\u3002\u901a\u8fc7\u5bf9A*\u7b97\u6cd5\u7684\u4fee\u6539\uff0c\u7ed3\u5408\u6a21\u5f0f\u6570\u636e\u5e93\u548ch^max\u542f\u53d1\u5f0f\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u9ad8\u6548\u751f\u6210\u6700\u4f18\u6027\u8bc1\u660e\u3002", "motivation": "\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u72ec\u7acb\u9a8c\u8bc1\u89c4\u5212\u4efb\u52a1\u65e0\u89e3\u6216\u8ba1\u5212\u6700\u4f18\u6027\u7684\u65b9\u6cd5\uff0c\u4e3a\u89c4\u5212\u9886\u57df\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u8bc1\u660e\u673a\u5236\uff0c\u589e\u5f3a\u7b97\u6cd5\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u4f2a\u5e03\u5c14\u7ea6\u675f\u6846\u67b6\u751f\u6210\u4f4e\u754c\u8bc1\u4e66\uff0c\u4fee\u6539A*\u7b97\u6cd5\u4ee5\u751f\u6210\u6700\u4f18\u6027\u8bc1\u660e\uff0c\u5e76\u4ee5\u6a21\u5f0f\u6570\u636e\u5e93\u548ch^max\u542f\u53d1\u5f0f\u4e3a\u4f8b\u8fdb\u884c\u5177\u4f53\u5b9e\u73b0\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u663e\u8457\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u6700\u4f18\u6027\u8bc1\u660e\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u80fd\u591f\u9ad8\u6548\u8868\u8fbe\u4f2a\u5e03\u5c14\u7ea6\u675f\u7684\u542f\u53d1\u5f0f\u3002", "conclusion": "\u7ed3\u8bba\u662f\u6240\u63d0\u51fa\u7684\u4f4e\u754c\u8bc1\u4e66\u6846\u67b6\u4e3a\u89c4\u5212\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u7684\u8bc1\u660e\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86A*\u7b97\u6cd5\u7684\u5e94\u7528\u8303\u56f4\uff0c\u589e\u5f3a\u4e86\u89c4\u5212\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u900f\u660e\u5ea6\u3002"}}
{"id": "2504.18053", "pdf": "https://arxiv.org/pdf/2504.18053", "abs": "https://arxiv.org/abs/2504.18053", "authors": ["Jianyu Liu", "Hangyu Guo", "Ranjie Duan", "Xingyuan Bu", "Yancheng He", "Shilong Li", "Hui Huang", "Jiaheng Liu", "Yucheng Wang", "Chenchen Jing", "Xingwei Qu", "Xiao Zhang", "Yingshui Tan", "Yanan Wu", "Jihao Gu", "Yangguang Li", "Jianke Zhu"], "title": "DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models", "categories": ["cs.CL", "cs.CV"], "comment": "[NAACL 2025] The first four authors contribute equally, 23 pages,\n  repo at https://github.com/Kizna1ver/DREAM", "summary": "Multimodal Large Language Models (MLLMs) pose unique safety challenges due to\ntheir integration of visual and textual data, thereby introducing new\ndimensions of potential attacks and complex risk combinations. In this paper,\nwe begin with a detailed analysis aimed at disentangling risks through\nstep-by-step reasoning within multimodal inputs. We find that systematic\nmultimodal risk disentanglement substantially enhances the risk awareness of\nMLLMs. Via leveraging the strong discriminative abilities of multimodal risk\ndisentanglement, we further introduce \\textbf{DREAM}\n(\\textit{\\textbf{D}isentangling \\textbf{R}isks to \\textbf{E}nhance Safety\n\\textbf{A}lignment in \\textbf{M}LLMs}), a novel approach that enhances safety\nalignment in MLLMs through supervised fine-tuning and iterative Reinforcement\nLearning from AI Feedback (RLAIF). Experimental results show that DREAM\nsignificantly boosts safety during both inference and training phases without\ncompromising performance on normal tasks (namely oversafety), achieving a\n16.17\\% improvement in the SIUO safe\\&effective score compared to GPT-4V. The\ndata and code are available at https://github.com/Kizna1ver/DREAM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDREAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u98ce\u9669\u89e3\u8026\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347MLLMs\u7684\u5b89\u5168\u6027\u4e14\u4e0d\u5f71\u54cd\u6b63\u5e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u56e0\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u5e26\u6765\u65b0\u7684\u5b89\u5168\u9690\u60a3\uff0c\u9700\u7cfb\u7edf\u6027\u89e3\u8026\u98ce\u9669\u4ee5\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faDREAM\u65b9\u6cd5\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u8fed\u4ee3\u5f0fRLAIF\uff08\u57fa\u4e8eAI\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u5206\u6b65\u89e3\u8026\u591a\u6a21\u6001\u98ce\u9669\u3002", "result": "DREAM\u5728\u63a8\u7406\u548c\u8bad\u7ec3\u9636\u6bb5\u5747\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\uff0cSIUO\u5b89\u5168&\u6709\u6548\u6027\u8bc4\u5206\u8f83GPT-4V\u63d0\u534716.17%\u3002", "conclusion": "DREAM\u80fd\u6709\u6548\u5e73\u8861MLLMs\u7684\u5b89\u5168\u6027\u4e0e\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u7684\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2504.18003", "pdf": "https://arxiv.org/pdf/2504.18003", "abs": "https://arxiv.org/abs/2504.18003", "authors": ["Aditya S Ellendula", "Chandrajit Bajaj"], "title": "Self-Balancing, Memory Efficient, Dynamic Metric Space Data Maintenance, for Rapid Multi-Kernel Estimation", "categories": ["cs.LG"], "comment": null, "summary": "We present a dynamic self-balancing octree data structure that enables\nefficient neighborhood maintenance in evolving metric spaces, a key challenge\nin modern machine learning systems. Many learning and generative models operate\nas dynamical systems whose representations evolve during training, requiring\nfast, adaptive spatial organization. Our two-parameter octree supports\nlogarithmic-time updates and queries, eliminating the need for costly full\nrebuilds as data distributions shift. We demonstrate its effectiveness in four\nareas: (1) accelerating Stein variational gradient descent by supporting more\nparticles with lower overhead; (2) enabling real-time, incremental KNN\nclassification with logarithmic complexity; (3) facilitating efficient, dynamic\nindexing and retrieval for retrieval-augmented generation; and (4) improving\nsample efficiency by jointly optimizing input and latent spaces. Across all\napplications, our approach yields exponential speedups while preserving\naccuracy, particularly in high-dimensional spaces where maintaining adaptive\nspatial structure is critical.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u81ea\u5e73\u8861\u516b\u53c9\u6811\u6570\u636e\u7ed3\u6784\uff0c\u652f\u6301\u5728\u52a8\u6001\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u9ad8\u6548\u7ef4\u62a4\u90bb\u8fd1\u5173\u7cfb\uff0c\u9002\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u4e2d\u52a8\u6001\u7cfb\u7edf\u7684\u7a7a\u95f4\u7ec4\u7ec7\u9700\u6c42\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4e2d\uff0c\u5b66\u4e60\u548c\u751f\u6210\u6a21\u578b\u7684\u8868\u793a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u52a8\u6001\u6f14\u5316\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5feb\u901f\u81ea\u9002\u5e94\u7ec4\u7ec7\u7a7a\u95f4\u7684\u6570\u636e\u7ed3\u6784\u3002", "method": "\u57fa\u4e8e\u53cc\u53c2\u6570\u7684\u516b\u53c9\u6811\u7ed3\u6784\uff0c\u652f\u6301\u5bf9\u6570\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u66f4\u65b0\u548c\u67e5\u8be2\uff0c\u907f\u514d\u4e86\u6570\u636e\u5206\u5e03\u53d8\u5316\u65f6\u7684\u6602\u8d35\u5168\u91cd\u5efa\u3002", "result": "\u5728\u56db\u4e2a\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u9ad8\u6548\u6027\uff1a\u52a0\u901fStein\u53d8\u5206\u68af\u5ea6\u4e0b\u964d\u3001\u5b9e\u73b0\u5b9e\u65f6\u589e\u91cfKNN\u5206\u7c7b\u3001\u652f\u6301\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u52a8\u6001\u7d22\u5f15\u3001\u4f18\u5316\u8f93\u5165\u548c\u6f5c\u5728\u7a7a\u95f4\u7684\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u5e94\u7528\u4e2d\u5747\u5b9e\u73b0\u4e86\u6307\u6570\u7ea7\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2504.18453", "pdf": "https://arxiv.org/pdf/2504.18453", "abs": "https://arxiv.org/abs/2504.18453", "authors": ["Peiyuan Jing", "Kinhei Lee", "Zhenxuan Zhang", "Huichi Zhou", "Zhengqing Yuan", "Zhifan Gao", "Lei Zhu", "Giorgos Papanastasiou", "Yingying Fang", "Guang Yang"], "title": "Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning for Verifiable Report Generation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Radiology report generation is critical for efficiency but current models\nlack the structured reasoning of experts, hindering clinical trust and\nexplainability by failing to link visual findings to precise anatomical\nlocations. This paper introduces BoxMed-RL, a groundbreaking unified training\nframework for generating spatially verifiable and explainable radiology\nreports. Built on a large vision-language model, BoxMed-RL revolutionizes\nreport generation through two integrated phases: (1) In the Pretraining Phase,\nwe refine the model via medical concept learning, using Chain-of-Thought\nsupervision to internalize the radiologist-like workflow, followed by spatially\nverifiable reinforcement, which applies reinforcement learning to align medical\nfindings with bounding boxes. (2) In the Downstream Adapter Phase, we freeze\nthe pretrained weights and train a downstream adapter to ensure fluent and\nclinically credible reports. This framework precisely mimics radiologists'\nworkflow, compelling the model to connect high-level medical concepts with\ndefinitive anatomical evidence. Extensive experiments on public datasets\ndemonstrate that BoxMed-RL achieves an average 7% improvement in both METEOR\nand ROUGE-L metrics compared to state-of-the-art methods. An average 5%\nimprovement in large language model-based metrics further underscores\nBoxMed-RL's robustness in generating high-quality radiology reports.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBoxMed-RL\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u9884\u8bad\u7ec3\u548c\u4e0b\u6e38\u9002\u914d\uff09\u751f\u6210\u53ef\u7a7a\u95f4\u9a8c\u8bc1\u548c\u89e3\u91ca\u7684\u653e\u5c04\u5b66\u62a5\u544a\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u4e13\u5bb6\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\uff0c\u96be\u4ee5\u4e3a\u4e34\u5e8a\u63d0\u4f9b\u53ef\u4fe1\u548c\u53ef\u89e3\u91ca\u7684\u62a5\u544a\u3002\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6a21\u4eff\u653e\u5c04\u79d1\u533b\u751f\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5c06\u89c6\u89c9\u53d1\u73b0\u4e0e\u89e3\u5256\u4f4d\u7f6e\u7cbe\u786e\u5173\u8054\uff0c\u63d0\u5347\u62a5\u544a\u8d28\u91cf\u3002", "method": "BoxMed-RL\u91c7\u7528\u7edf\u4e00\u8bad\u7ec3\u6846\u67b6\uff1a1\uff09\u9884\u8bad\u7ec3\u9636\u6bb5\u901a\u8fc7\u533b\u5b66\u6982\u5ff5\u5b66\u4e60\u548c\u7a7a\u95f4\u9a8c\u8bc1\u5f3a\u5316\u4f18\u5316\u6a21\u578b\uff1b2\uff09\u4e0b\u6e38\u9002\u914d\u9636\u6bb5\u51bb\u7ed3\u9884\u8bad\u7ec3\u6743\u91cd\u5e76\u8bad\u7ec3\u9002\u914d\u5668\u786e\u4fdd\u62a5\u544a\u6d41\u7545\u6027\u548c\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBoxMed-RL\u5728METEOR\u548cROUGE-L\u6307\u6807\u4e0a\u5e73\u5747\u63d0\u53477%\uff0c\u5728\u5927\u8bed\u8a00\u6a21\u578b\u6307\u6807\u4e0a\u63d0\u53475%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "BoxMed-RL\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u9a8c\u8bc1\u548c\u533b\u5b66\u6982\u5ff5\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653e\u5c04\u5b66\u62a5\u544a\u7684\u751f\u6210\u8d28\u91cf\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2504.18058", "pdf": "https://arxiv.org/pdf/2504.18058", "abs": "https://arxiv.org/abs/2504.18058", "authors": ["Sijia Cheng", "Wen-Yu Chang", "Yun-Nung Chen"], "title": "Exploring Personality-Aware Interactions in Salesperson Dialogue Agents", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IWSDS 2025", "summary": "The integration of dialogue agents into the sales domain requires a deep\nunderstanding of how these systems interact with users possessing diverse\npersonas. This study explores the influence of user personas, defined using the\nMyers-Briggs Type Indicator (MBTI), on the interaction quality and performance\nof sales-oriented dialogue agents. Through large-scale testing and analysis, we\nassess the pre-trained agent's effectiveness, adaptability, and personalization\ncapabilities across a wide range of MBTI-defined user types. Our findings\nreveal significant patterns in interaction dynamics, task completion rates, and\ndialogue naturalness, underscoring the future potential for dialogue agents to\nrefine their strategies to better align with varying personality traits. This\nwork not only provides actionable insights for building more adaptive and\nuser-centric conversational systems in the sales domain but also contributes\nbroadly to the field by releasing persona-defined user simulators. These\nsimulators, unconstrained by domain, offer valuable tools for future research\nand demonstrate the potential for scaling personalized dialogue systems across\ndiverse applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8eMBTI\u5b9a\u4e49\u7684\u7528\u6237\u89d2\u8272\u5bf9\u9500\u552e\u5bf9\u8bdd\u4ee3\u7406\u4ea4\u4e92\u8d28\u91cf\u548c\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u4ea4\u4e92\u52a8\u6001\u3001\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u5bf9\u8bdd\u81ea\u7136\u6027\u7684\u663e\u8457\u6a21\u5f0f\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7528\u6237\u6a21\u62df\u5668\u5de5\u5177\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3\u4e0d\u540c\u7528\u6237\u89d2\u8272\u5982\u4f55\u5f71\u54cd\u9500\u552e\u5bf9\u8bdd\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u4ee5\u6784\u5efa\u66f4\u81ea\u9002\u5e94\u3001\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u5bf9\u8bdd\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u6d4b\u8bd5\u548c\u5206\u6790\uff0c\u8bc4\u4f30\u9884\u8bad\u7ec3\u4ee3\u7406\u5728\u591a\u79cdMBTI\u7528\u6237\u7c7b\u578b\u4e2d\u7684\u6709\u6548\u6027\u3001\u9002\u5e94\u6027\u548c\u4e2a\u6027\u5316\u80fd\u529b\u3002", "result": "\u53d1\u73b0\u4e86\u4ea4\u4e92\u52a8\u6001\u3001\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u5bf9\u8bdd\u81ea\u7136\u6027\u7684\u663e\u8457\u6a21\u5f0f\uff0c\u5e76\u53d1\u5e03\u4e86\u4e0d\u9650\u9886\u57df\u7684\u7528\u6237\u6a21\u62df\u5668\u5de5\u5177\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u9500\u552e\u9886\u57df\u63d0\u4f9b\u4e86\u6784\u5efa\u81ea\u9002\u5e94\u5bf9\u8bdd\u7cfb\u7edf\u7684\u5b9e\u7528\u89c1\u89e3\uff0c\u540c\u65f6\u901a\u8fc7\u5f00\u653e\u7684\u6a21\u62df\u5668\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u4e2a\u6027\u5316\u5bf9\u8bdd\u7cfb\u7edf\u7684\u8de8\u9886\u57df\u6269\u5c55\u3002"}}
{"id": "2504.18008", "pdf": "https://arxiv.org/pdf/2504.18008", "abs": "https://arxiv.org/abs/2504.18008", "authors": ["Nooshin Yousefzadeh", "Rahul Sengupta", "Sanjay Ranka"], "title": "TGDT: A Temporal Graph-based Digital Twin for Urban Traffic Corridors", "categories": ["cs.LG"], "comment": "8 pages, 4 figures, 1 table", "summary": "Urban congestion at signalized intersections leads to significant delays,\neconomic losses, and increased emissions. Existing deep learning models often\nlack spatial generalizability, rely on complex architectures, and struggle with\nreal-time deployment. To address these limitations, we propose the Temporal\nGraph-based Digital Twin (TGDT), a scalable framework that integrates Temporal\nConvolutional Networks and Attentional Graph Neural Networks for dynamic,\ndirection-aware traffic modeling and assessment at urban corridors. TGDT\nestimates key Measures of Effectiveness (MOEs) for traffic flow optimization at\nboth the intersection level (e.g., queue length, waiting time) and the corridor\nlevel (e.g., traffic volume, travel time). Its modular architecture and\nsequential optimization scheme enable easy extension to any number of\nintersections and MOEs. The model outperforms state-of-the-art baselines by\naccurately producing high-dimensional, concurrent multi-output estimates. It\nalso demonstrates high robustness and accuracy across diverse traffic\nconditions, including extreme scenarios, while relying on only a minimal set of\ntraffic features. Fully parallelized, TGDT can simulate over a thousand\nscenarios within a matter of seconds, offering a cost-effective, interpretable,\nand real-time solution for traffic signal optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86Temporal Graph-based Digital Twin (TGDT)\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u5e8f\u5377\u79ef\u7f51\u7edc\u548c\u6ce8\u610f\u529b\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u89e3\u51b3\u57ce\u5e02\u4ea4\u901a\u62e5\u5835\u95ee\u9898\uff0c\u5177\u6709\u9ad8\u6269\u5c55\u6027\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u57ce\u5e02\u4fe1\u53f7\u706f\u4ea4\u53c9\u53e3\u62e5\u5835\u5bfc\u81f4\u5ef6\u8bef\u3001\u7ecf\u6d4e\u635f\u5931\u548c\u6392\u653e\u589e\u52a0\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7a7a\u95f4\u6cdb\u5316\u6027\u5dee\u4e14\u590d\u6742\u3002", "method": "TGDT\u6846\u67b6\u6574\u5408\u4e86\u65f6\u5e8f\u5377\u79ef\u7f51\u7edc\u548c\u6ce8\u610f\u529b\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u52a8\u6001\u5efa\u6a21\u4ea4\u901a\u6d41\uff0c\u4f18\u5316\u591a\u9879\u5173\u952e\u6548\u679c\u6307\u6807\u3002", "result": "\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u6a21\u578b\uff0c\u80fd\u9ad8\u7cbe\u5ea6\u751f\u6210\u591a\u7ef4\u5e76\u53d1\u8f93\u51fa\uff0c\u9002\u5e94\u5404\u79cd\u4ea4\u901a\u6761\u4ef6\u4e14\u9ad8\u6548\u5b9e\u65f6\u3002", "conclusion": "TGDT\u4e3a\u4ea4\u901a\u4fe1\u53f7\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u9ad8\u7cbe\u5ea6\u4e14\u5b9e\u65f6\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18530", "pdf": "https://arxiv.org/pdf/2504.18530", "abs": "https://arxiv.org/abs/2504.18530", "authors": ["Joshua Engels", "David D. Baek", "Subhash Kantamneni", "Max Tegmark"], "title": "Scaling Laws For Scalable Oversight", "categories": ["cs.AI", "cs.CY", "cs.LG"], "comment": "34 pages, 17 figures", "summary": "Scalable oversight, the process by which weaker AI systems supervise stronger\nones, has been proposed as a key strategy to control future superintelligent\nsystems. However, it is still unclear how scalable oversight itself scales. To\naddress this gap, we propose a framework that quantifies the probability of\nsuccessful oversight as a function of the capabilities of the overseer and the\nsystem being overseen. Specifically, our framework models oversight as a game\nbetween capability-mismatched players; the players have oversight-specific and\ndeception-specific Elo scores that are a piecewise-linear function of their\ngeneral intelligence, with two plateaus corresponding to task incompetence and\ntask saturation. We validate our framework with a modified version of the game\nNim and then apply it to four oversight games: \"Mafia\", \"Debate\", \"Backdoor\nCode\" and \"Wargames\". For each game, we find scaling laws that approximate how\ndomain performance depends on general AI system capability (using Chatbot Arena\nElo as a proxy for general capability). We then build on our findings in a\ntheoretical study of Nested Scalable Oversight (NSO), a process in which\ntrusted models oversee untrusted stronger models, which then become the trusted\nmodels in the next step. We identify conditions under which NSO succeeds and\nderive numerically (and in some cases analytically) the optimal number of\noversight levels to maximize the probability of oversight success. In our\nnumerical examples, the NSO success rate is below 52% when overseeing systems\nthat are 400 Elo points stronger than the baseline overseer, and it declines\nfurther for overseeing even stronger systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u76d1\u7763\u6210\u529f\u6982\u7387\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u7763\u6e38\u620f\u6a21\u578b\u548cElo\u8bc4\u5206\uff0c\u7814\u7a76\u4e86\u5728\u4e0d\u540c\u80fd\u529b\u5dee\u8ddd\u4e0b\u76d1\u7763\u7684\u53ef\u884c\u6027\uff0c\u5c24\u5176\u5728\u5d4c\u5957\u53ef\u6269\u5c55\u76d1\u7763\uff08NSO\uff09\u4e2d\u6d4b\u8bd5\u4e86\u5176\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u53ef\u6269\u5c55\u76d1\u7763\uff08\u5373\u8f83\u5f31AI\u7cfb\u7edf\u76d1\u7763\u66f4\u5f3aAI\u7cfb\u7edf\uff09\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u8bba\u6587\u65e8\u5728\u91cf\u5316\u76d1\u7763\u7684\u6210\u529f\u6982\u7387\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u80fd\u529b\u5dee\u5f02\u8f83\u5927\u7684\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5efa\u6a21\u76d1\u7763\u4e3a\u80fd\u529b\u4e0d\u5339\u914d\u73a9\u5bb6\u4e4b\u95f4\u7684\u6e38\u620f\uff0c\u4f7f\u7528Elo\u8bc4\u5206\uff08\u5305\u542b\u4efb\u52a1\u65e0\u80fd\u548c\u9971\u548c\u9636\u6bb5\u7684\u7247\u6bb5\u7ebf\u6027\u51fd\u6570\uff09\u6765\u8bc4\u4f30\u76d1\u7763\u6548\u679c\u3002\u5728Nim\u7b49\u6e38\u620f\u4e2d\u9a8c\u8bc1\u6846\u67b6\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u201cMafia\u201d\u3001\u201cDebate\u201d\u7b49\u76d1\u7763\u6e38\u620f\uff0c\u6700\u540e\u5728\u5d4c\u5957\u53ef\u6269\u5c55\u76d1\u7763\uff08NSO\uff09\u4e2d\u8fdb\u884c\u6570\u503c\u548c\u7406\u8bba\u5206\u6790\u3002", "result": "\u76d1\u7763\u6210\u529f\u7387\u968fAI\u7cfb\u7edf\u80fd\u529b\u5dee\u8ddd\u589e\u5927\u800c\u4e0b\u964d\uff0c\u4f8b\u5982\u5728400 Elo\u5dee\u8ddd\u4e0bNSO\u6210\u529f\u7387\u4f4e\u4e8e52%\u3002\u7814\u7a76\u8fd8\u63a8\u5bfc\u4e86\u6700\u5927\u5316\u76d1\u7763\u6210\u529f\u6982\u7387\u7684\u6700\u4f18\u76d1\u7763\u5c42\u7ea7\u6570\u91cf\u3002", "conclusion": "\u8bba\u6587\u8868\u660e\u76d1\u7763\u6548\u679c\u53d7\u80fd\u529b\u5dee\u8ddd\u663e\u8457\u5f71\u54cd\uff0c\u5d4c\u5957\u76d1\u7763\u867d\u6709\u4e00\u5b9a\u6548\u679c\uff0c\u4f46\u9762\u5bf9\u66f4\u5f3a\u7cfb\u7edf\u65f6\u6210\u529f\u7387\u8f83\u4f4e\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6846\u67b6\u6216\u63a2\u7d22\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2504.18070", "pdf": "https://arxiv.org/pdf/2504.18070", "abs": "https://arxiv.org/abs/2504.18070", "authors": ["Jingjin Wang"], "title": "PropRAG: Guiding Retrieval with Beam Search over Proposition Paths", "categories": ["cs.CL", "cs.AI"], "comment": "Code and data to be released at:\n  https://github.com/ReLink-Inc/PropRAG", "summary": "Retrieval Augmented Generation (RAG) has become the standard non-parametric\napproach for equipping Large Language Models (LLMs) with up-to-date knowledge\nand mitigating catastrophic forgetting common in continual learning. However,\nstandard RAG, relying on independent passage retrieval, fails to capture the\ninterconnected nature of human memory crucial for complex reasoning\n(associativity) and contextual understanding (sense-making). While structured\nRAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples,\nthe inherent context loss limits fidelity. We introduce PropRAG, a framework\nleveraging contextually rich propositions and a novel beam search algorithm\nover proposition paths to explicitly discover multi-step reasoning chains.\nCrucially, PropRAG's online retrieval process operates entirely without\ninvoking generative LLMs, relying instead on efficient graph traversal and\npre-computed embeddings. This avoids online LLM inference costs and potential\ninconsistencies during evidence gathering. LLMs are used effectively offline\nfor high-quality proposition extraction and post-retrieval for answer\ngeneration. PropRAG achieves state-of-the-art zero-shot Recall@5 results on\nPopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside\ntop F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through\nricher representation and explicit, LLM-free online path finding, PropRAG\nadvances non-parametric continual learning.", "AI": {"tldr": "PropRAG\u901a\u8fc7\u5229\u7528\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u547d\u9898\u548c\u65b0\u7684\u675f\u641c\u7d22\u7b97\u6cd5\uff0c\u663e\u5f0f\u53d1\u73b0\u591a\u6b65\u63a8\u7406\u94fe\uff0c\u63d0\u5347\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u6548\u80fd\uff0c\u907f\u514d\u4e86\u5728\u8bc1\u636e\u6536\u96c6\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u751f\u6210\u578bLLM\u7684\u5f00\u9500\u548c\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u6807\u51c6\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u672a\u80fd\u6355\u6349\u5230\u4eba\u7c7b\u8bb0\u5fc6\u7684\u5173\u8054\u6027\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u800c\u73b0\u6709\u7684\u7ed3\u6784\u5316RAG\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684HippoRAG\uff09\u5b58\u5728\u4e0a\u4e0b\u6587\u4e22\u5931\u7684\u95ee\u9898\u3002PropRAG\u65e8\u5728\u901a\u8fc7\u66f4\u4e30\u5bcc\u7684\u8868\u793a\u548c\u663e\u5f0f\u7684\u5728\u7ebf\u8def\u5f84\u53d1\u73b0\u6765\u6539\u5584\u8bc1\u636e\u68c0\u7d22\u3002", "method": "PropRAG\u4f7f\u7528\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u547d\u9898\u548c\u4e00\u79cd\u65b0\u7684\u675f\u641c\u7d22\u7b97\u6cd5\uff0c\u5728\u547d\u9898\u8def\u5f84\u4e0a\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u94fe\u7684\u53d1\u73b0\u3002\u8be5\u65b9\u6cd5\u5b8c\u5168\u4f9d\u8d56\u9ad8\u6548\u7684\u56fe\u904d\u5386\u548c\u9884\u8ba1\u7b97\u7684\u5d4c\u5165\uff0c\u907f\u514d\u4e86\u5728\u7ebfLLM\u63a8\u7406\u7684\u6d88\u8017\u3002", "result": "PropRAG\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08\u5982PopQA\u30012Wiki\u3001HotpotQA\u548cMuSiQue\uff09\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672cRecall@5\u548cF1\u5206\u6570\uff08\u5982MuSiQue\u4e0a\u768452.4%\uff09\u3002", "conclusion": "PropRAG\u901a\u8fc7\u6539\u8fdb\u8bc1\u636e\u68c0\u7d22\u548c\u591a\u6b65\u63a8\u7406\u94fe\u7684\u663e\u5f0f\u53d1\u73b0\uff0c\u63a8\u52a8\u4e86\u975e\u53c2\u6570\u6301\u7eed\u5b66\u4e60\u7684\u8fdb\u6b65\u3002"}}
{"id": "2504.18026", "pdf": "https://arxiv.org/pdf/2504.18026", "abs": "https://arxiv.org/abs/2504.18026", "authors": ["Emiliano Penaloza", "Tianyue H. Zhan", "Laurent Charlin", "Mateo Espinosa Zarlenga"], "title": "Addressing Concept Mislabeling in Concept Bottleneck Models Through Preference Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Concept Bottleneck Models (CBMs) propose to enhance the trustworthiness of AI\nsystems by constraining their decisions on a set of human understandable\nconcepts. However, CBMs typically assume that datasets contains accurate\nconcept labels an assumption often violated in practice, which we show can\nsignificantly degrade performance (by 25% in some cases). To address this, we\nintroduce the Concept Preference Optimization (CPO) objective, a new loss\nfunction based on Direct Preference Optimization, which effectively mitigates\nthe negative impact of concept mislabeling on CBM performance. We provide an\nanalysis on some key properties of the CPO objective showing it directly\noptimizes for the concept's posterior distribution, and contrast it against\nBinary Cross Entropy (BCE) where we show CPO is inherently less sensitive to\nconcept noise. We empirically confirm our analysis finding that CPO\nconsistently outperforms BCE in three real world datasets with and without\nadded label noise.", "AI": {"tldr": "CPO\uff08Concept Preference Optimization\uff09\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u4f18\u5316\u6982\u5ff5\u540e\u9a8c\u5206\u5e03\u6765\u7f13\u89e3\u6982\u5ff5\u6807\u7b7e\u9519\u8bef\u5bf9CBM\uff08Concept Bottleneck Models\uff09\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5b9e\u9a8c\u8bc1\u660eCPO\u5728\u6807\u7b7e\u566a\u58f0\u4e0b\u4f18\u4e8eBCE\u3002", "motivation": "\u73b0\u5b9e\u6570\u636e\u96c6\u4e2d\u7684\u6982\u5ff5\u6807\u7b7e\u5e38\u5b58\u5728\u9519\u8bef\uff0c\u8fd9\u4f1a\u663e\u8457\u964d\u4f4eCBM\u7684\u6027\u80fd\uff08\u5982\u67d0\u4e9b\u60c5\u51b5\u4e0b\u964d\u4f4e25%\uff09\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5bf9\u6807\u7b7e\u566a\u58f0\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCPO\u76ee\u6807\uff0c\u57fa\u4e8eDirect Preference Optimization\u8bbe\u8ba1\uff0c\u65e8\u5728\u76f4\u63a5\u4f18\u5316\u6982\u5ff5\u540e\u9a8c\u5206\u5e03\uff0c\u51cf\u5c11\u5bf9\u6807\u7b7e\u566a\u58f0\u7684\u654f\u611f\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\uff08\u542b\u566a\u58f0\u548c\u65e0\u566a\u58f0\uff09\u4e0a\uff0cCPO\u5747\u4f18\u4e8e\u4f20\u7edf\u7684BCE\uff08Binary Cross Entropy\uff09\u3002", "conclusion": "CPO\u901a\u8fc7\u4f18\u5316\u6982\u5ff5\u540e\u9a8c\u5206\u5e03\uff0c\u6709\u6548\u63d0\u5347\u4e86CBM\u5728\u6807\u7b7e\u566a\u58f0\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2504.18536", "pdf": "https://arxiv.org/pdf/2504.18536", "abs": "https://arxiv.org/abs/2504.18536", "authors": ["Anna Katariina Wisakanto", "Joe Rogero", "Avyay M. Casheekar", "Richard Mallah"], "title": "Adapting Probabilistic Risk Assessment for AI", "categories": ["cs.AI", "cs.CY", "cs.LG", "cs.SY", "eess.SY", "stat.AP"], "comment": "for project website, see https://pra-for-ai.github.io/pra/", "summary": "Modern general-purpose artificial intelligence (AI) systems present an urgent\nrisk management challenge, as their rapidly evolving capabilities and potential\nfor catastrophic harm outpace our ability to reliably assess their risks.\nCurrent methods often rely on selective testing and undocumented assumptions\nabout risk priorities, frequently failing to make a serious attempt at\nassessing the set of pathways through which Al systems pose direct or indirect\nrisks to society and the biosphere. This paper introduces the probabilistic\nrisk assessment (PRA) for AI framework, adapting established PRA techniques\nfrom high-reliability industries (e.g., nuclear power, aerospace) for the new\nchallenges of advanced AI. The framework guides assessors in identifying\npotential risks, estimating likelihood and severity, and explicitly documenting\nevidence, underlying assumptions, and analyses at appropriate granularities.\nThe framework's implementation tool synthesizes the results into a risk report\ncard with aggregated risk estimates from all assessed risks. This systematic\napproach integrates three advances: (1) Aspect-oriented hazard analysis\nprovides systematic hazard coverage guided by a first-principles taxonomy of AI\nsystem aspects (e.g. capabilities, domain knowledge, affordances); (2) Risk\npathway modeling analyzes causal chains from system aspects to societal impacts\nusing bidirectional analysis and incorporating prospective techniques; and (3)\nUncertainty management employs scenario decomposition, reference scales, and\nexplicit tracing protocols to structure credible projections with novelty or\nlimited data. Additionally, the framework harmonizes diverse assessment methods\nby integrating evidence into comparable, quantified absolute risk estimates for\ncritical decisions. We have implemented this as a workbook tool for AI\ndevelopers, evaluators, and regulators, available on the project website.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u73b0\u4ee3\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7cfb\u7edf\u7684\u6982\u7387\u98ce\u9669\u8bc4\u4f30\uff08PRA\uff09\u6846\u67b6\uff0c\u65e8\u5728\u7cfb\u7edf\u6027\u8bc6\u522b\u548c\u8bc4\u4f30AI\u53ef\u80fd\u5bf9\u793e\u4f1a\u548c\u751f\u7269\u5708\u5e26\u6765\u7684\u76f4\u63a5\u6216\u95f4\u63a5\u98ce\u9669\u3002", "motivation": "\u73b0\u4ee3\u901a\u7528AI\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\u548c\u6f5c\u5728\u707e\u96be\u6027\u98ce\u9669\u8d85\u51fa\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7cfb\u7edf\u3001\u53ef\u9760\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u8be5\u6846\u67b6\u91c7\u7528\u6982\u7387\u98ce\u9669\u8bc4\u4f30\u6280\u672f\uff0c\u7ed3\u5408\u9762\u5411\u65b9\u9762\u7684\u5371\u5bb3\u5206\u6790\u3001\u98ce\u9669\u8def\u5f84\u5efa\u6a21\u548c\u4e0d\u786e\u5b9a\u6027\u7ba1\u7406\uff0c\u4e3aAI\u98ce\u9669\u63d0\u4f9b\u91cf\u5316\u8bc4\u4f30\u3002", "result": "\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5de5\u5177\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u65b9\u6cd5\u751f\u6210\u98ce\u9669\u62a5\u544a\u5361\uff0c\u6574\u5408\u591a\u79cd\u8bc4\u4f30\u65b9\u6cd5\u5e76\u63d0\u4f9b\u53ef\u6bd4\u7684\u98ce\u9669\u91cf\u5316\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u5f00\u53d1\u8005\u3001\u8bc4\u4f30\u8005\u548c\u76d1\u7ba1\u8005\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u8bc4\u4f30\u548c\u7ba1\u7406AI\u98ce\u9669\u7684\u5de5\u5177\u3002"}}
{"id": "2504.18080", "pdf": "https://arxiv.org/pdf/2504.18080", "abs": "https://arxiv.org/abs/2504.18080", "authors": ["Wataru Kawakami", "Keita Suzuki", "Junichiro Iwasawa"], "title": "Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) show potential in medicine, yet clinical\nadoption is hindered by concerns over factual accuracy, language-specific\nlimitations (e.g., Japanese), and critically, their reliability when required\nto generate reasoning explanations -- a prerequisite for trust. This paper\nintroduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the\nJapanese medical domain to achieve both high accuracy and stable reasoning. We\nemploy a two-stage fine-tuning process on the Qwen2.5-72B base model: first,\nContinued Pretraining (CPT) on a comprehensive Japanese medical corpus instills\ndeep domain knowledge. Second, Reasoning Preference Optimization (RPO), a\npreference-based method, enhances the generation of reliable reasoning pathways\nwhile preserving high answer accuracy. Evaluations on the Japanese Medical\nLicensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves\nstate-of-the-art performance (0.868 accuracy), surpassing strong proprietary\nmodels like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which\nexhibit significant accuracy degradation (up to 11.5\\% and 3.8\\% respectively\non IgakuQA) when prompted for explanations, our model maintains its high\naccuracy (0.868) under such conditions. This highlights RPO's effectiveness in\nstabilizing reasoning generation. This work underscores the importance of\noptimizing for reliable explanations alongside accuracy. We release the\nPreferred-MedLLM-Qwen-72B model weights to foster research into trustworthy\nLLMs for specialized, high-stakes applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u65e5\u672c\u533b\u5b66\u9886\u57df\u768472B\u53c2\u6570\u6a21\u578bPreferred-MedLLM-Qwen-72B\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5fae\u8c03\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u63a8\u7406\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u5e94\u7528\u4e2d\u9762\u4e34\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u8bed\u8a00\u9650\u5236\u53ca\u63a8\u7406\u53ef\u9760\u6027\u95ee\u9898\uff0c\u963b\u788d\u4e86\u4e34\u5e8a\u91c7\u7528\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\uff1a\u6301\u7eed\u9884\u8bad\u7ec3\uff08CPT\uff09\u6ce8\u5165\u533b\u5b66\u9886\u57df\u77e5\u8bc6\uff0c\u63a8\u7406\u504f\u597d\u4f18\u5316\uff08RPO\uff09\u63d0\u5347\u53ef\u9760\u63a8\u7406\u751f\u6210\u3002", "result": "\u5728Japanese Medical Licensing Exam\uff08IgakuQA\uff09\u4e0a\u53d6\u5f970.868\u51c6\u786e\u7387\uff0c\u4f18\u4e8eGPT-4o\uff080.866\uff09\uff0c\u4e14\u5728\u8981\u6c42\u89e3\u91ca\u65f6\u4ecd\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u4f18\u5316\u53ef\u9760\u89e3\u91ca\u4e0e\u51c6\u786e\u6027\u540c\u7b49\u91cd\u8981\uff0c\u5e76\u5f00\u6e90\u6a21\u578b\u6743\u91cd\u4ee5\u4fc3\u8fdb\u9ad8\u98ce\u9669\u9886\u57df\u53ef\u4fe1LLM\u7814\u7a76\u3002"}}
{"id": "2504.18048", "pdf": "https://arxiv.org/pdf/2504.18048", "abs": "https://arxiv.org/abs/2504.18048", "authors": ["Zhongtian Chen", "Daniel Murfet"], "title": "Modes of Sequence Models and Learning Coefficients", "categories": ["cs.LG"], "comment": null, "summary": "We develop a geometric account of sequence modelling that links patterns in\nthe data to measurable properties of the loss landscape in transformer\nnetworks. First, we cast conditional sequence distributions into a\nHilbert-space framework and apply tensor decompositions to identify their\nprincipal modes. Truncating the small-amplitude modes yields an effective data\ndistribution that preserves dominant structure while discarding statistical\ndetail. Second, we show theoretically that Local Learning Coefficient (LLC)\nestimates are insensitive to modes below a data-dependent threshold.\nConsequently, the LLC calculated in practice characterises the geometry of the\neffective rather than the true distribution. This insight clarifies why\nreliable LLC estimates can be obtained even when a network parameter is not a\nstrict minimiser of the population loss, and it highlights how the inverse\ntemperature in SGLD acts as a resolution dial on the landscape structure.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u51e0\u4f55\u89c6\u89d2\u7684\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6761\u4ef6\u5e8f\u5217\u5206\u5e03\u6620\u5c04\u5230\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u5e76\u5229\u7528\u5f20\u91cf\u5206\u89e3\u8bc6\u522b\u4e3b\u6a21\u5f0f\uff0c\u4ece\u800c\u6709\u6548\u7b80\u5316\u6570\u636e\u5206\u5e03\u3002\u7406\u8bba\u8bc1\u660e\u5c40\u90e8\u5b66\u4e60\u7cfb\u6570\uff08LLC\uff09\u5bf9\u4f4e\u4e8e\u7279\u5b9a\u9608\u503c\u7684\u6a21\u5f0f\u4e0d\u654f\u611f\uff0c\u89e3\u91ca\u4e86\u4e3a\u4f55\u5728\u7f51\u7edc\u53c2\u6570\u672a\u4e25\u683c\u6700\u5c0f\u5316\u603b\u4f53\u635f\u5931\u65f6\u4ecd\u80fd\u83b7\u5f97\u53ef\u9760\u7684LLC\u4f30\u8ba1\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5e8f\u5217\u5efa\u6a21\u4e2d\u5bf9\u6570\u636e\u5206\u5e03\u590d\u6742\u6027\u7684\u7406\u89e3\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u4ece\u51e0\u4f55\u89d2\u5ea6\u5206\u6790\u635f\u5931\u666f\u89c2\u7684\u5c5e\u6027\u548c\u6570\u636e\u6a21\u5f0f\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "method": "\u901a\u8fc7\u5c06\u6761\u4ef6\u5e8f\u5217\u5206\u5e03\u5d4c\u5165\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u6846\u67b6\uff0c\u5e94\u7528\u5f20\u91cf\u5206\u89e3\u8bc6\u522b\u4e3b\u6a21\u5f0f\u5e76\u622a\u65ad\u5c0f\u5e45\u5ea6\u6a21\u5f0f\uff0c\u4ece\u800c\u5f97\u5230\u6709\u6548\u7684\u7b80\u5316\u6570\u636e\u5206\u5e03\u3002\u7406\u8bba\u5206\u6790\u4e86\u5c40\u90e8\u5b66\u4e60\u7cfb\u6570\uff08LLC\uff09\u5bf9\u4f4e\u5e45\u5ea6\u6a21\u5f0f\u7684\u9c81\u68d2\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLC\u5bf9\u4f4e\u4e8e\u6570\u636e\u4f9d\u8d56\u9608\u503c\u7684\u6a21\u5f0f\u4e0d\u654f\u611f\uff0c\u56e0\u6b64\u5b9e\u9645\u8ba1\u7b97\u5f97\u5230\u7684LLC\u53cd\u6620\u4e86\u7b80\u5316\u5206\u5e03\u800c\u975e\u771f\u5b9e\u5206\u5e03\u7684\u51e0\u4f55\u7279\u6027\u3002\u8fd9\u4e00\u53d1\u73b0\u89e3\u91ca\u4e86\u4e3a\u4f55\u5728\u7f51\u7edc\u53c2\u6570\u672a\u4e25\u683c\u6700\u5c0f\u5316\u603b\u4f53\u635f\u5931\u65f6\uff0c\u4ecd\u80fd\u83b7\u5f97\u53ef\u9760\u7684LLC\u4f30\u8ba1\u3002", "conclusion": "\u8bba\u6587\u63ed\u793a\u4e86\u901a\u8fc7\u51e0\u4f55\u65b9\u6cd5\u7b80\u5316\u6570\u636e\u5206\u5e03\u7684\u6709\u6548\u6027\uff0c\u5e76\u9610\u660e\u4e86LLC\u5728\u5b9e\u8df5\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u7406\u89e3\u5e8f\u5217\u5efa\u6a21\u4e2d\u7684\u635f\u5931\u666f\u89c2\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2504.17792", "pdf": "https://arxiv.org/pdf/2504.17792", "abs": "https://arxiv.org/abs/2504.17792", "authors": ["Hauke Sandhaus", "Angel Hsing-Chi Hwang", "Wendy Ju", "Qian Yang"], "title": "My Precious Crash Data: Barriers and Opportunities in Encouraging Autonomous Driving Companies to Share Safety-Critical Data", "categories": ["cs.HC", "cs.AI", "cs.DB", "E.m; H.2.8; J.1"], "comment": "To appear in Proc. ACM Hum.-Comput. Interact., Computer-Supported\n  Cooperative Work & Social Computing (CSCW), 2025", "summary": "Safety-critical data, such as crash and near-crash records, are crucial to\nimproving autonomous vehicle (AV) design and development. Sharing such data\nacross AV companies, academic researchers, regulators, and the public can help\nmake all AVs safer. However, AV companies rarely share safety-critical data\nexternally. This paper aims to pinpoint why AV companies are reluctant to share\nsafety-critical data, with an eye on how these barriers can inform new\napproaches to promote sharing. We interviewed twelve AV company employees who\nactively work with such data in their day-to-day work. Findings suggest two\nkey, previously unknown barriers to data sharing: (1) Datasets inherently embed\nsalient knowledge that is key to improving AV safety and are\nresource-intensive. Therefore, data sharing, even within a company, is fraught\nwith politics. (2) Interviewees believed AV safety knowledge is private\nknowledge that brings competitive edges to their companies, rather than public\nknowledge for social good. We discuss the implications of these findings for\nincentivizing and enabling safety-critical AV data sharing, specifically,\nimplications for new approaches to (1) debating and stratifying public and\nprivate AV safety knowledge, (2) innovating data tools and data sharing\npipelines that enable easier sharing of public AV safety data and knowledge;\n(3) offsetting costs of curating safety-critical data and incentivizing data\nsharing.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u81ea\u52a8\u9a7e\u9a76\u516c\u53f8\u4e0d\u613f\u5171\u4eab\u5b89\u5168\u5173\u952e\u6570\u636e\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4fc3\u8fdb\u5171\u4eab\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5171\u4eab\u5b89\u5168\u5173\u952e\u6570\u636e\u6709\u52a9\u4e8e\u63d0\u5347\u6240\u6709\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u6027\uff0c\u4f46\u516c\u53f8\u5f88\u5c11\u5171\u4eab\u6b64\u7c7b\u6570\u636e\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5176\u80cc\u540e\u7684\u969c\u788d\u3002", "method": "\u901a\u8fc7\u5bf912\u540d\u81ea\u52a8\u9a7e\u9a76\u516c\u53f8\u5458\u5de5\u7684\u8bbf\u8c08\uff0c\u5206\u6790\u6570\u636e\u5171\u4eab\u7684\u653f\u6cbb\u6027\u548c\u77e5\u8bc6\u79c1\u6709\u5316\u95ee\u9898\u3002", "result": "\u53d1\u73b0\u6570\u636e\u5171\u4eab\u7684\u4e24\u5927\u969c\u788d\uff1a\u6570\u636e\u96c6\u5d4c\u5165\u7684\u5173\u952e\u77e5\u8bc6\u5177\u6709\u7ade\u4e89\u4ef7\u503c\uff0c\u5458\u5de5\u89c6\u5b89\u5168\u77e5\u8bc6\u4e3a\u79c1\u6709\u800c\u975e\u516c\u5171\u8d22\u4ea7\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u6fc0\u52b1\u6570\u636e\u5171\u4eab\u7684\u65b0\u65b9\u6cd5\uff0c\u5305\u62ec\u533a\u5206\u516c\u5171\u4e0e\u79c1\u6709\u77e5\u8bc6\u3001\u521b\u65b0\u6570\u636e\u5de5\u5177\u4ee5\u53ca\u6210\u672c\u8865\u507f\u673a\u5236\u3002"}}
{"id": "2504.18085", "pdf": "https://arxiv.org/pdf/2504.18085", "abs": "https://arxiv.org/abs/2504.18085", "authors": ["Muhammad Mubashar", "Shireen Kudukkil Manchingal", "Fabio Cuzzolin"], "title": "Random-Set Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": "16 pages, 6 figures", "summary": "Large Language Models (LLMs) are known to produce very high-quality tests and\nresponses to our queries. But how much can we trust this generated text? In\nthis paper, we study the problem of uncertainty quantification in LLMs. We\npropose a novel Random-Set Large Language Model (RSLLM) approach which predicts\nfinite random sets (belief functions) over the token space, rather than\nprobability vectors as in classical LLMs. In order to allow so efficiently, we\nalso present a methodology based on hierarchical clustering to extract and use\na budget of \"focal\" subsets of tokens upon which the belief prediction is\ndefined, rather than using all possible collections of tokens, making the\nmethod scalable yet effective. RS-LLMs encode the epistemic uncertainty induced\nin their generation process by the size and diversity of its training set via\nthe size of the credal sets associated with the predicted belief functions. The\nproposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b,\nMistral-7b and Phi-2 models and is shown to outperform the standard model in\nboth datasets in terms of correctness of answer while also showing potential in\nestimating the second level uncertainty in its predictions and providing the\ncapability to detect when its hallucinating.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86RSLLM\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u968f\u673a\u96c6\u5408\u800c\u975e\u6982\u7387\u5411\u91cf\u6765\u91cf\u5316LLMs\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u7b54\u6848\u6b63\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u9762\u4f18\u4e8e\u4f20\u7edfLLMs\u3002", "motivation": "\u7814\u7a76LLMs\u751f\u6210\u6587\u672c\u7684\u53ef\u4fe1\u5ea6\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5982\u4f55\u91cf\u5316\u5176\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faRSLLM\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u6709\u9650\u968f\u673a\u96c6\u5408\uff08\u4fe1\u5ff5\u51fd\u6570\uff09\u66ff\u4ee3\u4f20\u7edf\u6982\u7387\u5411\u91cf\uff0c\u5e76\u5229\u7528\u5206\u5c42\u805a\u7c7b\u63d0\u53d6\u5173\u952e\u4ee4\u724c\u5b50\u96c6\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5728CoQA\u548cOBQA\u6570\u636e\u96c6\u4e0a\uff0cRSLLM\u5728\u7b54\u6848\u6b63\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u6a21\u578b\uff0c\u5e76\u80fd\u68c0\u6d4b\u5e7b\u89c9\u73b0\u8c61\u3002", "conclusion": "RSLLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u91cf\u5316LLMs\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.18072", "pdf": "https://arxiv.org/pdf/2504.18072", "abs": "https://arxiv.org/abs/2504.18072", "authors": ["Konstantin Sch\u00fcrholt", "L\u00e9o Meynent", "Yefan Zhou", "Haiquan Lu", "Yaoqing Yang", "Damian Borth"], "title": "A Model Zoo on Phase Transitions in Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Using the weights of trained Neural Network (NN) models as data modality has\nrecently gained traction as a research field - dubbed Weight Space Learning\n(WSL). Multiple recent works propose WSL methods to analyze models, evaluate\nmethods, or synthesize weights. Weight space learning methods require\npopulations of trained models as datasets for development and evaluation.\nHowever, existing collections of models - called `model zoos' - are\nunstructured or follow a rudimentary definition of diversity. In parallel, work\nrooted in statistical physics has identified phases and phase transitions in NN\nmodels. Models are homogeneous within the same phase but qualitatively differ\nfrom one phase to another. We combine the idea of `model zoos' with phase\ninformation to create a controlled notion of diversity in populations. We\nintroduce 12 large-scale zoos that systematically cover known phases and vary\nover model architecture, size, and datasets. These datasets cover different\nmodalities, such as computer vision, natural language processing, and\nscientific ML. For every model, we compute loss landscape metrics and validate\nfull coverage of the phases. With this dataset, we provide the community with a\nresource with a wide range of potential applications for WSL and beyond.\nEvidence suggests the loss landscape phase plays a role in applications such as\nmodel training, analysis, or sparsification. We demonstrate this in an\nexploratory study of the downstream methods like transfer learning or model\nweights averaging.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u6a21\u578b\u52a8\u7269\u56ed\u548c\u76f8\u4f4d\u4fe1\u606f\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u591a\u6837\u6027\u7684\u65b9\u6cd5\uff0c\u521b\u5efa\u4e8612\u4e2a\u5927\u578b\u6a21\u578b\u52a8\u7269\u56ed\uff0c\u8986\u76d6\u4e0d\u540c\u6a21\u6001\uff0c\u5e76\u63a2\u7d22\u4e86\u5176\u5728\u6743\u91cd\u7a7a\u95f4\u5b66\u4e60\u53ca\u5176\u4ed6\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u52a8\u7269\u56ed\u7f3a\u4e4f\u7ed3\u6784\u5316\u591a\u6837\u6027\uff0c\u4e14\u672a\u80fd\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u76f8\u4f4d\u4fe1\u606f\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u5316\u8986\u76d6\u4e0d\u540c\u76f8\u4f4d\uff0c\u4e3a\u6743\u91cd\u7a7a\u95f4\u5b66\u4e60\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u6570\u636e\u8d44\u6e90\u3002", "method": "\u7ed3\u5408\u76f8\u4f4d\u4fe1\u606f\u521b\u5efa12\u4e2a\u5927\u578b\u6a21\u578b\u52a8\u7269\u56ed\uff0c\u8986\u76d6\u4e0d\u540c\u67b6\u6784\u3001\u89c4\u6a21\u548c\u6570\u636e\u96c6\uff0c\u8ba1\u7b97\u635f\u5931\u666f\u89c2\u6307\u6807\u5e76\u9a8c\u8bc1\u76f8\u4f4d\u8986\u76d6\u3002", "result": "\u5f00\u53d1\u4e86\u8986\u76d6\u591a\u79cd\u6a21\u6001\u7684\u6a21\u578b\u52a8\u7269\u56ed\uff0c\u9a8c\u8bc1\u4e86\u76f8\u4f4d\u4fe1\u606f\u5728\u6a21\u578b\u8bad\u7ec3\u3001\u5206\u6790\u7b49\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u901a\u8fc7\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u8fc1\u79fb\u5b66\u4e60\uff09\u5c55\u793a\u4e86\u5176\u6548\u7528\u3002", "conclusion": "\u7cfb\u7edf\u5316\u7684\u6a21\u578b\u52a8\u7269\u56ed\u4e3a\u6743\u91cd\u7a7a\u95f4\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8d44\u6e90\uff0c\u76f8\u4f4d\u4fe1\u606f\u5728\u6a21\u578b\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u5728\u91cd\u8981\u6027\u3002"}}
{"id": "2504.17799", "pdf": "https://arxiv.org/pdf/2504.17799", "abs": "https://arxiv.org/abs/2504.17799", "authors": ["S. L. Thomson", "M. W. Przewozniczek"], "title": "Subfunction Structure Matters: A New Perspective on Local Optima Networks", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Local optima networks (LONs) capture fitness landscape information. They are\ntypically constructed in a black-box manner; information about the problem\nstructure is not utilised. This also applies to the analysis of LONs: knowledge\nabout the problem, such as interaction between variables, is not considered. We\nchallenge this status-quo with an alternative approach: we consider how LON\nanalysis can be improved by incorporating subfunction-based information - this\ncan either be known a-priori or learned during search. To this end, LONs are\nconstructed for several benchmark pseudo-boolean problems using three\napproaches: firstly, the standard algorithm; a second algorithm which uses\ndeterministic grey-box crossover; and a third algorithm which selects\nperturbations based on learned information about variable interactions. Metrics\nrelated to subfunction changes in a LON are proposed and compared with metrics\nfrom previous literature which capture other aspects of a LON. Incorporating\nproblem structure in LON construction and analysing it can bring enriched\ninsight into optimisation dynamics. Such information may be crucial to\nunderstanding the difficulty of solving a given problem with state-of-the-art\nlinkage learning optimisers. In light of the results, we suggest incorporation\nof problem structure as an alternative paradigm in landscape analysis for\nproblems with known or suspected subfunction structure.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5c40\u90e8\u6700\u4f18\u7f51\u7edc\uff08LON\uff09\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5b50\u51fd\u6570\u4fe1\u606f\uff08\u65e0\u8bba\u662f\u5148\u9a8c\u8fd8\u662f\u5b66\u4e60\u5230\u7684\uff09\u6765\u4e30\u5bcc\u4f18\u5316\u52a8\u6001\u7684\u6d1e\u5bdf\u3002\u4e0e\u4f20\u7edf\u9ed1\u76d2\u65b9\u6cd5\u4e0d\u540c\uff0c\u5b83\u5229\u7528\u95ee\u9898\u7ed3\u6784\uff08\u5982\u53d8\u91cf\u4ea4\u4e92\uff09\u63d0\u5347LON\u7684\u6784\u5efa\u4e0e\u5206\u6790\u3002\u5b9e\u9a8c\u5728\u4f2a\u5e03\u5c14\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u786e\u5b9a\u6027\u7070\u76d2\u4ea4\u53c9\u548c\u5b66\u4e60\u53d8\u91cf\u4ea4\u4e92\u7684\u4e24\u79cd\u65b0\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\u7ed3\u6784\u4fe1\u606f\u7684\u5f15\u5165\u80fd\u66f4\u6df1\u5165\u7406\u89e3\u95ee\u9898\u6c42\u89e3\u96be\u5ea6\u3002", "motivation": "\u4f20\u7edfLON\u6784\u5efa\u548c\u5206\u6790\u5ffd\u7565\u95ee\u9898\u7ed3\u6784\u4fe1\u606f\uff08\u5982\u53d8\u91cf\u4ea4\u4e92\uff09\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u4f18\u5316\u52a8\u6001\u7684\u6df1\u5165\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5229\u7528\u5b50\u51fd\u6570\u4fe1\u606f\u6539\u8fdbLON\u5206\u6790\uff0c\u4ece\u800c\u66f4\u597d\u5730\u63ed\u793a\u95ee\u9898\u6c42\u89e3\u7684\u96be\u70b9\u3002", "method": "\u63d0\u51fa\u4e09\u79cdLON\u6784\u5efa\u65b9\u6cd5\uff1a1. \u6807\u51c6\u7b97\u6cd5\uff08\u9ed1\u76d2\uff09\uff1b2. \u57fa\u4e8e\u786e\u5b9a\u6027\u7070\u76d2\u4ea4\u53c9\u7684\u7b97\u6cd5\uff1b3. \u57fa\u4e8e\u5b66\u4e60\u53d8\u91cf\u4ea4\u4e92\u7684\u6270\u52a8\u9009\u62e9\u7b97\u6cd5\u3002\u5e76\u63d0\u51fa\u65b0\u7684\u5b50\u51fd\u6570\u53d8\u5316\u76f8\u5173\u6307\u6807\uff0c\u4e0e\u4f20\u7edf\u6307\u6807\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u7ed3\u5408\u95ee\u9898\u7ed3\u6784\uff08\u5982\u7070\u76d2\u4ea4\u53c9\u6216\u5b66\u4e60\u5230\u7684\u4ea4\u4e92\u4fe1\u606f\uff09\u80fd\u663e\u8457\u4e30\u5bccLON\u5206\u6790\u7ed3\u679c\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u4f18\u5316\u52a8\u6001\u6d1e\u5bdf\uff0c\u5c24\u5176\u5bf9\u5177\u6709\u5df2\u77e5/\u7591\u4f3c\u5b50\u51fd\u6570\u7ed3\u6784\u7684\u95ee\u9898\u3002", "conclusion": "\u5efa\u8bae\u5c06\u95ee\u9898\u7ed3\u6784\u4fe1\u606f\u7eb3\u5165LON\u5206\u6790\u8303\u5f0f\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5177\u6709\u5b50\u51fd\u6570\u7ed3\u6784\u7684\u95ee\u9898\u3002\u8fd9\u4e00\u65b9\u6cd5\u53ef\u80fd\u4e3a\u7406\u89e3\u73b0\u4ee3\u94fe\u8def\u5b66\u4e60\u4f18\u5316\u5668\u7684\u6c42\u89e3\u96be\u5ea6\u63d0\u4f9b\u5173\u952e\u4fe1\u606f\u3002"}}
{"id": "2504.18104", "pdf": "https://arxiv.org/pdf/2504.18104", "abs": "https://arxiv.org/abs/2504.18104", "authors": ["Yinglong Yu", "Hao Shen", "Zhengyi Lyu", "Qi He"], "title": "Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In response to the growing problem of misinformation in the context of\nglobalization and informatization, this paper proposes a classification method\nfor fact-check-worthiness estimation based on prompt tuning. We construct a\nmodel for fact-check-worthiness estimation at the methodological level using\nprompt tuning. By applying designed prompt templates to large language models,\nwe establish in-context learning and leverage prompt tuning technology to\nimprove the accuracy of determining whether claims have fact-check-worthiness,\nparticularly when dealing with limited or unlabeled data. Through extensive\nexperiments on public datasets, we demonstrate that the proposed method\nsurpasses or matches multiple baseline methods in the classification task of\nfact-check-worthiness estimation assessment, including classical pre-trained\nmodels such as BERT, as well as recent popular large models like GPT-3.5 and\nGPT-4. Experiments show that the prompt tuning-based method proposed in this\nstudy exhibits certain advantages in evaluation metrics such as F1 score and\naccuracy, thereby effectively validating its effectiveness and advancement in\nthe task of fact-check-worthiness estimation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u5fae\u8c03\u7684\u4e8b\u5b9e\u6838\u67e5\u4ef7\u503c\u8bc4\u4f30\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u63d0\u793a\u6a21\u677f\u5e94\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u5728\u6709\u9650\u6216\u65e0\u6807\u7b7e\u6570\u636e\u4e0b\u5224\u65ad\u4e8b\u5b9e\u6838\u67e5\u4ef7\u503c\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u9488\u5bf9\u5168\u7403\u5316\u548c\u4fe1\u606f\u5316\u80cc\u666f\u4e0b\u9519\u8bef\u4fe1\u606f\u65e5\u76ca\u4e25\u91cd\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u4fe1\u606f\u7684\u6838\u67e5\u4ef7\u503c\u3002", "method": "\u57fa\u4e8e\u63d0\u793a\u5fae\u8c03\u6280\u672f\u6784\u5efa\u6a21\u578b\uff0c\u8bbe\u8ba1\u63d0\u793a\u6a21\u677f\u5e76\u5e94\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u63d0\u793a\u5fae\u8c03\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86BERT\u3001GPT-3.5\u548cGPT-4\u7b49\u57fa\u7ebf\u6a21\u578b\uff0cF1\u5206\u6570\u548c\u51c6\u786e\u7387\u7b49\u6307\u6807\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u63d0\u793a\u5fae\u8c03\u7684\u65b9\u6cd5\u5728\u4e8b\u5b9e\u6838\u67e5\u4ef7\u503c\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u5148\u8fdb\u6027\u3002"}}
{"id": "2504.18078", "pdf": "https://arxiv.org/pdf/2504.18078", "abs": "https://arxiv.org/abs/2504.18078", "authors": ["Xiaolu Chen", "Chenghao Huang", "Yanru Zhang", "Hao Wang"], "title": "Privacy-Preserving Personalized Federated Learning for Distributed Photovoltaic Disaggregation under Statistical Heterogeneity", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages", "summary": "The rapid expansion of distributed photovoltaic (PV) installations worldwide,\nmany being behind-the-meter systems, has significantly challenged energy\nmanagement and grid operations, as unobservable PV generation further\ncomplicates the supply-demand balance. Therefore, estimating this generation\nfrom net load, known as PV disaggregation, is critical. Given privacy concerns\nand the need for large training datasets, federated learning becomes a\npromising approach, but statistical heterogeneity, arising from geographical\nand behavioral variations among prosumers, poses new challenges to PV\ndisaggregation. To overcome these challenges, a privacy-preserving distributed\nPV disaggregation framework is proposed using Personalized Federated Learning\n(PFL). The proposed method employs a two-level framework that combines local\nand global modeling. At the local level, a transformer-based PV disaggregation\nmodel is designed to generate solar irradiance embeddings for representing\nlocal PV conditions. A novel adaptive local aggregation mechanism is adopted to\nmitigate the impact of statistical heterogeneity on the local model, extracting\na portion of global information that benefits the local model. At the global\nlevel, a central server aggregates information uploaded from multiple data\ncenters, preserving privacy while enabling cross-center knowledge sharing.\nExperiments on real-world data demonstrate the effectiveness of this proposed\nframework, showing improved accuracy and robustness compared to benchmark\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u5206\u5e03\u5f0f\u5149\u4f0f\u5206\u89e3\u6846\u67b6\uff0c\u91c7\u7528\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\uff08PFL\uff09\u89e3\u51b3\u56e0\u5730\u7406\u4f4d\u7f6e\u548c\u884c\u4e3a\u5dee\u5f02\u5bfc\u81f4\u7684\u7edf\u8ba1\u5f02\u8d28\u6027\u6311\u6218\uff0c\u901a\u8fc7\u5c40\u90e8\u548c\u5168\u5c40\u5efa\u6a21\u7684\u7ed3\u5408\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5206\u5e03\u5f0f\u5149\u4f0f\uff08PV\uff09\u5b89\u88c5\u5feb\u901f\u589e\u957f\uff0c\u4f46\u4e0d\u53ef\u89c2\u6d4b\u7684PV\u53d1\u7535\u52a0\u5267\u4e86\u4f9b\u9700\u5e73\u8861\u7684\u590d\u6742\u6027\u3002\u9690\u79c1\u95ee\u9898\u548c\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u7684\u9700\u6c42\u4f7f\u5f97\u8054\u90a6\u5b66\u4e60\u6210\u4e3a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7edf\u8ba1\u5f02\u8d28\u6027\u5e26\u6765\u4e86\u65b0\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u5c42\u7684PFL\u6846\u67b6\uff1a\u5c40\u90e8\u5c42\u9762\u91c7\u7528\u57fa\u4e8eTransformer\u7684PV\u5206\u89e3\u6a21\u578b\u751f\u6210\u592a\u9633\u8f90\u5c04\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u5c40\u90e8\u805a\u5408\u673a\u5236\u51cf\u8f7b\u7edf\u8ba1\u5f02\u8d28\u6027\u5f71\u54cd\uff1b\u5168\u5c40\u5c42\u9762\u901a\u8fc7\u4e2d\u5fc3\u670d\u52a1\u5668\u805a\u5408\u591a\u6570\u636e\u4e2d\u5fc3\u4fe1\u606f\uff0c\u5b9e\u73b0\u8de8\u4e2d\u5fc3\u77e5\u8bc6\u5171\u4eab\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u76f8\u6bd4\u57fa\u51c6\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u9690\u79c1\u4fdd\u62a4\u7684PFL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86PV\u5206\u89e3\u4e2d\u7684\u7edf\u8ba1\u5f02\u8d28\u6027\u6311\u6218\uff0c\u4e3a\u5206\u5e03\u5f0f\u80fd\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.17801", "pdf": "https://arxiv.org/pdf/2504.17801", "abs": "https://arxiv.org/abs/2504.17801", "authors": ["Xufeng Yao", "Jiaxi Jiang", "Yuxuan Zhao", "Peiyu Liao", "Yibo Lin", "Bei Yu"], "title": "Evolution of Optimization Algorithms for Global Placement via Large Language Models", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Optimization algorithms are widely employed to tackle complex problems, but\ndesigning them manually is often labor-intensive and requires significant\nexpertise. Global placement is a fundamental step in electronic design\nautomation (EDA). While analytical approaches represent the state-of-the-art\n(SOTA) in global placement, their core optimization algorithms remain heavily\ndependent on heuristics and customized components, such as initialization\nstrategies, preconditioning methods, and line search techniques. This paper\npresents an automated framework that leverages large language models (LLM) to\nevolve optimization algorithms for global placement. We first generate diverse\ncandidate algorithms using LLM through carefully crafted prompts. Then we\nintroduce an LLM-based genetic flow to evolve selected candidate algorithms.\nThe discovered optimization algorithms exhibit substantial performance\nimprovements across many benchmarks. Specifically, Our design-case-specific\ndiscovered algorithms achieve average HPWL improvements of \\textbf{5.05\\%},\n\\text{5.29\\%} and \\textbf{8.30\\%} on MMS, ISPD2005 and ISPD2019 benchmarks, and\nup to \\textbf{17\\%} improvements on individual cases. Additionally, the\ndiscovered algorithms demonstrate good generalization ability and are\ncomplementary to existing parameter-tuning methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u6f14\u5316\u4f18\u5316\u7b97\u6cd5\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u7403\u5e03\u5c40\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u8bbe\u8ba1\u4f18\u5316\u7b97\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\u548c\u624b\u52a8\u8c03\u6574\uff0c\u5168\u7403\u5e03\u5c40\u53c8\u662f\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\uff08EDA\uff09\u4e2d\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u4f46\u5f53\u524d\u7684\u4f18\u5316\u7b97\u6cd5\u4ecd\u4f9d\u8d56\u542f\u53d1\u5f0f\u548c\u5b9a\u5236\u7ec4\u4ef6\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7LLM\u81ea\u52a8\u6f14\u5316\u7b97\u6cd5\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528LLM\u751f\u6210\u591a\u6837\u5316\u7684\u5019\u9009\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7LLM\u9a71\u52a8\u7684\u9057\u4f20\u6d41\u7a0b\u4f18\u5316\u8fd9\u4e9b\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u53d1\u73b0\u7684\u7b97\u6cd5\u5e73\u5747HPWL\u63d0\u5347\u4e865.05%\u81f38.30%\uff0c\u4e2a\u522b\u6848\u4f8b\u751a\u81f3\u8fbe\u523017%\uff0c\u4e14\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u80fd\u4e0e\u73b0\u6709\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\u4e92\u8865\uff0c\u4e3a\u81ea\u52a8\u5316\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.18106", "pdf": "https://arxiv.org/pdf/2504.18106", "abs": "https://arxiv.org/abs/2504.18106", "authors": ["Yinglong Yu", "Zhaopu Yao", "Fang Yuan"], "title": "Comparative Study on the Discourse Meaning of Chinese and English Media in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering", "categories": ["cs.CL"], "comment": null, "summary": "This study analyzes Chinese and English media reports on the Paris Olympics\nusing topic modeling, Large Language Model (LLM) prompt engineering, and corpus\nphraseology methods to explore similarities and differences in discourse\nconstruction and attitudinal meanings. Common topics include the opening\nceremony, athlete performance, and sponsorship brands. Chinese media focus on\nspecific sports, sports spirit, doping controversies, and new technologies,\nwhile English media focus on female athletes, medal wins, and eligibility\ncontroversies. Chinese reports show more frequent prepositional co-occurrences\nand positive semantic prosody in describing the opening ceremony and sports\nspirit. English reports exhibit positive semantic prosody when covering female\nathletes but negative prosody in predicting opening ceremony reactions and\ndiscussing women's boxing controversies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e3b\u9898\u5efa\u6a21\u3001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u793a\u5de5\u7a0b\u548c\u8bed\u6599\u5e93\u77ed\u8bed\u5b66\u65b9\u6cd5\uff0c\u5206\u6790\u4e2d\u82f1\u6587\u5a92\u4f53\u5bf9\u5df4\u9ece\u5965\u8fd0\u4f1a\u7684\u62a5\u9053\uff0c\u63a2\u8ba8\u8bdd\u8bed\u5efa\u6784\u548c\u6001\u5ea6\u610f\u4e49\u7684\u5f02\u540c\u3002", "motivation": "\u7814\u7a76\u4e2d\u82f1\u6587\u5a92\u4f53\u5728\u62a5\u9053\u540c\u4e00\u56fd\u9645\u4e8b\u4ef6\u65f6\u7684\u89c6\u89d2\u548c\u6001\u5ea6\u5dee\u5f02\uff0c\u63ed\u793a\u6587\u5316\u80cc\u666f\u5bf9\u65b0\u95fb\u62a5\u9053\u7684\u5f71\u54cd\u3002", "method": "\u7ed3\u5408\u4e3b\u9898\u5efa\u6a21\u3001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u793a\u5de5\u7a0b\u548c\u8bed\u6599\u5e93\u77ed\u8bed\u5b66\u65b9\u6cd5\uff0c\u5206\u6790\u5a92\u4f53\u62a5\u9053\u7684\u6587\u672c\u6570\u636e\u3002", "result": "\u4e2d\u82f1\u6587\u5a92\u4f53\u5171\u540c\u5173\u6ce8\u5f00\u5e55\u5f0f\u3001\u8fd0\u52a8\u5458\u8868\u73b0\u548c\u8d5e\u52a9\u54c1\u724c\uff0c\u4f46\u4e2d\u6587\u5a92\u4f53\u66f4\u4fa7\u91cd\u5177\u4f53\u9879\u76ee\u3001\u4f53\u80b2\u7cbe\u795e\u3001\u5174\u594b\u5242\u4e89\u8bae\u548c\u65b0\u6280\u672f\uff0c\u82f1\u6587\u5a92\u4f53\u5219\u805a\u7126\u5973\u8fd0\u52a8\u5458\u3001\u5956\u724c\u548c\u8d44\u683c\u4e89\u8bae\u3002\u4e2d\u6587\u62a5\u9053\u5728\u63cf\u8ff0\u5f00\u5e55\u5f0f\u548c\u4f53\u80b2\u7cbe\u795e\u65f6\u503e\u5411\u6b63\u9762\u8bed\u4e49\u97f5\uff0c\u800c\u82f1\u6587\u62a5\u9053\u5bf9\u5973\u8fd0\u52a8\u5458\u7684\u62a5\u9053\u4e5f\u5448\u73b0\u6b63\u9762\u8bed\u4e49\u97f5\uff0c\u4f46\u5bf9\u5f00\u5e55\u5f0f\u53cd\u5e94\u548c\u5973\u5b50\u62f3\u51fb\u4e89\u8bae\u5219\u503e\u5411\u8d1f\u9762\u3002", "conclusion": "\u4e2d\u82f1\u6587\u5a92\u4f53\u62a5\u9053\u5728\u4e3b\u9898\u9009\u62e9\u548c\u6001\u5ea6\u8868\u8fbe\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u53cd\u6620\u6587\u5316\u4ef7\u503c\u89c2\u548c\u65b0\u95fb\u504f\u597d\u7684\u4e0d\u540c\u3002"}}
{"id": "2504.18082", "pdf": "https://arxiv.org/pdf/2504.18082", "abs": "https://arxiv.org/abs/2504.18082", "authors": ["Vignesh Balaji", "Christos Kozyrakis", "Gal Chechik", "Haggai Maron"], "title": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches.", "AI": {"tldr": "COMM-RAND\u662f\u4e00\u79cd\u7ed3\u5408\u793e\u533a\u7ed3\u6784\u611f\u77e5\u4e0e\u968f\u673a\u6027\u7684GNN\u5c0f\u6279\u91cf\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u4e14\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709GNN\u5c0f\u6279\u91cf\u8bad\u7ec3\u65b9\u6cd5\u5728\u968f\u673a\u6027\u548c\u7ed3\u6784\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u968f\u673a\u6027\u5f71\u54cd\u7f13\u5b58\u6548\u7387\uff0c\u800c\u7eaf\u7ed3\u6784\u65b9\u6cd5\u53c8\u635f\u5bb3\u6a21\u578b\u7cbe\u5ea6\u548c\u6536\u655b\u901f\u5ea6\u3002", "method": "\u63d0\u51faCOMM-RAND\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u793e\u533a\u7ed3\u6784\u611f\u77e5\u4e0e\u968f\u673a\u6027\u4f18\u5316\u5c0f\u6279\u91cf\u6784\u5efa\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5728\u56db\u4e2a\u56fe\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCOMM-RAND\u5e73\u5747\u51cf\u5c111.8\u500d\u8bad\u7ec3\u65f6\u95f4\uff0c\u7cbe\u5ea6\u635f\u5931\u4ec50.42%\u3002", "conclusion": "COMM-RAND\u5728\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u7cbe\u5ea6\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u56fe\u5b66\u4e60\u4efb\u52a1\u3002"}}
{"id": "2504.17805", "pdf": "https://arxiv.org/pdf/2504.17805", "abs": "https://arxiv.org/abs/2504.17805", "authors": ["Tri Nguyen", "Kelly Cohen"], "title": "Fuzzy Logic -- Based Scheduling System for Part-Time Workforce", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "This paper explores the application of genetic fuzzy systems to efficiently\ngenerate schedules for a team of part-time student workers at a university.\nGiven the preferred number of working hours and availability of employees, our\nmodel generates feasible solutions considering various factors, such as maximum\nweekly hours, required number of workers on duty, and the preferred number of\nworking hours. The algorithm is trained and tested with availability data\ncollected from students at the University of Cincinnati. The results\ndemonstrate the algorithm's efficiency in producing schedules that meet\noperational criteria and its robustness in understaffed conditions.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528\u9057\u4f20\u6a21\u7cca\u7cfb\u7edf\u9ad8\u6548\u751f\u6210\u5927\u5b66\u517c\u804c\u5b66\u751f\u7684\u5de5\u4f5c\u6392\u73ed\u8868\uff0c\u8003\u8651\u4e86\u5458\u5de5\u504f\u597d\u5de5\u4f5c\u65f6\u95f4\u548c\u53ef\u7528\u6027\u7b49\u56e0\u7d20\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u5927\u5b66\u517c\u804c\u5b66\u751f\u6392\u73ed\u7684\u590d\u6742\u95ee\u9898\uff0c\u786e\u4fdd\u6392\u73ed\u6ee1\u8db3\u8fd0\u8425\u9700\u6c42\u548c\u5458\u5de5\u504f\u597d\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u9057\u4f20\u6a21\u7cca\u7cfb\u7edf\u7b97\u6cd5\uff0c\u7ed3\u5408\u5458\u5de5\u7684\u53ef\u7528\u6027\u548c\u504f\u597d\u6570\u636e\uff0c\u751f\u6210\u53ef\u884c\u7684\u6392\u73ed\u65b9\u6848\u3002", "result": "\u7ed3\u679c\u663e\u793a\u7b97\u6cd5\u80fd\u9ad8\u6548\u751f\u6210\u7b26\u5408\u8fd0\u8425\u6807\u51c6\u7684\u6392\u73ed\u8868\uff0c\u5e76\u5728\u4eba\u5458\u4e0d\u8db3\u65f6\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u7ed3\u8bba\u662f\u9057\u4f20\u6a21\u7cca\u7cfb\u7edf\u80fd\u6709\u6548\u89e3\u51b3\u590d\u6742\u6392\u73ed\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u7c7b\u4f3c\u573a\u666f\u3002"}}
{"id": "2504.18114", "pdf": "https://arxiv.org/pdf/2504.18114", "abs": "https://arxiv.org/abs/2504.18114", "authors": ["Atharva Kulkarni", "Yuan Zhang", "Joel Ruben Antony Moniz", "Xiou Ge", "Bo-Hsiang Tseng", "Dhivya Piraviperumal", "Swabha Swayamdipta", "Hong Yu"], "title": "Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Hallucinations pose a significant obstacle to the reliability and widespread\nadoption of language models, yet their accurate measurement remains a\npersistent challenge. While many task- and domain-specific metrics have been\nproposed to assess faithfulness and factuality concerns, the robustness and\ngeneralization of these metrics are still untested. In this paper, we conduct a\nlarge-scale empirical evaluation of 6 diverse sets of hallucination detection\nmetrics across 4 datasets, 37 language models from 5 families, and 5 decoding\nmethods. Our extensive investigation reveals concerning gaps in current\nhallucination evaluation: metrics often fail to align with human judgments,\ntake an overtly myopic view of the problem, and show inconsistent gains with\nparameter scaling. Encouragingly, LLM-based evaluation, particularly with\nGPT-4, yields the best overall results, and mode-seeking decoding methods seem\nto reduce hallucinations, especially in knowledge-grounded settings. These\nfindings underscore the need for more robust metrics to understand and quantify\nhallucinations, and better strategies to mitigate them.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u8bc4\u4f30\u4e866\u79cd\u4e0d\u540c\u7684\u5e7b\u89c9\u68c0\u6d4b\u6307\u6807\u57284\u4e2a\u6570\u636e\u96c6\u300137\u79cd\u8bed\u8a00\u6a21\u578b\u548c5\u79cd\u89e3\u7801\u65b9\u6cd5\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524d\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e0d\u4e00\u81f4\u3001\u8fc7\u4e8e\u7247\u9762\uff0c\u5e76\u6307\u51faGPT-4\u7b49LLM\u8bc4\u4f30\u8868\u73b0\u6700\u4f73\uff0c\u800c\u6a21\u5f0f\u5bfb\u6c42\u89e3\u7801\u65b9\u6cd5\u53ef\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "\u5e7b\u89c9\u95ee\u9898\u662f\u963b\u788d\u8bed\u8a00\u6a21\u578b\u53ef\u9760\u6027\u548c\u5e7f\u6cdb\u5e94\u7528\u7684\u4e3b\u8981\u969c\u788d\uff0c\u4f46\u73b0\u6709\u7684\u68c0\u6d4b\u6307\u6807\u5728\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u4e0a\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u8bc4\u4f30\u5176\u6709\u6548\u6027\u3002", "method": "\u7814\u7a76\u5bf96\u7ec4\u4e0d\u540c\u5e7b\u89c9\u68c0\u6d4b\u6307\u6807\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u8986\u76d64\u4e2a\u6570\u636e\u96c6\u300137\u4e2a\u8bed\u8a00\u6a21\u578b\uff08\u6765\u81ea5\u4e2a\u5bb6\u65cf\uff09\u548c5\u79cd\u89e3\u7801\u65b9\u6cd5\uff0c\u5e76\u4e0e\u4eba\u7c7b\u5224\u65ad\u5bf9\u6bd4\u3002", "result": "\u5f53\u524d\u6307\u6807\u5e38\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e0d\u4e00\u81f4\uff0c\u8868\u73b0\u7247\u9762\u4e14\u53c2\u6570\u6269\u5c55\u6548\u679c\u4e0d\u4e00\u81f4\uff1bGPT-4\u7b49LLM\u8bc4\u4f30\u8868\u73b0\u6700\u4f73\uff0c\u6a21\u5f0f\u5bfb\u6c42\u89e3\u7801\u65b9\u6cd5\u53ef\u51cf\u5c11\u5e7b\u89c9\uff08\u5c24\u5176\u5728\u77e5\u8bc6\u76f8\u5173\u573a\u666f\uff09\u3002", "conclusion": "\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u5e7b\u89c9\u91cf\u5316\u6307\u6807\u548c\u66f4\u6709\u6548\u7684\u7f13\u89e3\u7b56\u7565\uff0cLLM\u8bc4\u4f30\u548c\u7279\u5b9a\u89e3\u7801\u65b9\u6cd5\u5c55\u73b0\u4e86\u6f5c\u529b\u3002"}}
{"id": "2504.18091", "pdf": "https://arxiv.org/pdf/2504.18091", "abs": "https://arxiv.org/abs/2504.18091", "authors": ["Shota Deguchi", "Mitsuteru Asai"], "title": "Reliable and Efficient Inverse Analysis using Physics-Informed Neural Networks with Distance Functions and Adaptive Weight Tuning", "categories": ["cs.LG"], "comment": null, "summary": "Physics-informed neural networks have attracted significant attention in\nscientific machine learning for their capability to solve forward and inverse\nproblems governed by partial differential equations. However, the accuracy of\nPINN solutions is often limited by the treatment of boundary conditions.\nConventional penalty-based methods, which incorporate boundary conditions as\npenalty terms in the loss function, cannot guarantee exact satisfaction of the\ngiven boundary conditions and are highly sensitive to the choice of penalty\nparameters. This paper demonstrates that distance functions, specifically\nR-functions, can be leveraged to enforce boundary conditions, overcoming these\nlimitations. R-functions provide normalized distance fields, enabling accurate\nrepresentation of boundary geometries, including non-convex domains, and\nfacilitating various types of boundary conditions. We extend this distance\nfunction-based boundary condition imposition method to inverse problems using\nPINNs and introduce an adaptive weight tuning technique to ensure reliable and\nefficient inverse analysis. We demonstrate the efficacy of the method through\nseveral numerical experiments. Numerical results show that the proposed method\nsolves inverse problems more accurately and efficiently than penalty-based\nmethods, even in the presence of complex non-convex geometries. This approach\noffers a reliable and efficient framework for inverse analysis using PINNs,\nwith potential applications across a wide range of engineering problems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ddd\u79bb\u51fd\u6570\uff08R\u51fd\u6570\uff09\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7d61\uff08PINN\uff09\u4e2d\u8fb9\u754c\u6761\u4ef6\u5904\u7406\u7684\u4e0d\u8db3\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u6b63\u5411\u548c\u53cd\u95ee\u9898\u4e2d\u7684\u9ad8\u6548\u6027\u4e0e\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u60e9\u7f5a\u7684\u65b9\u6cd5\u65e0\u6cd5\u7cbe\u786e\u6ee1\u8db3\u8fb9\u754c\u6761\u4ef6\u4e14\u5bf9\u60e9\u7f5a\u53c2\u6570\u654f\u611f\uff0c\u5f71\u54cd\u4e86PINN\u6c42\u89e3\u7684\u7cbe\u5ea6\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u8ddd\u79bb\u51fd\u6570\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\uff0c\u63d0\u5347\u8fb9\u754c\u6761\u4ef6\u7684\u7cbe\u786e\u5904\u7406\u80fd\u529b\u3002", "method": "\u5229\u7528R\u51fd\u6570\u751f\u6210\u5f52\u4e00\u5316\u8ddd\u79bb\u573a\uff0c\u7cbe\u786e\u8868\u793a\u8fb9\u754c\u51e0\u4f55\uff08\u5305\u62ec\u975e\u51f8\u57df\uff09\uff0c\u5e76\u652f\u6301\u591a\u79cd\u8fb9\u754c\u6761\u4ef6\u3002\u8fdb\u4e00\u6b65\u6269\u5c55\u81f3\u53cd\u95ee\u9898\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u6743\u91cd\u8c03\u6574\u6280\u672f\u4ee5\u63d0\u9ad8\u53cd\u5206\u6790\u7684\u53ef\u9760\u6027\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u975e\u51f8\u51e0\u4f55\u4e0b\u6bd4\u60e9\u7f5a\u6cd5\u66f4\u51c6\u786e\u9ad8\u6548\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u53cd\u95ee\u9898\u6c42\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aPINN\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u53cd\u5206\u6790\u6846\u67b6\uff0c\u5728\u5de5\u7a0b\u95ee\u9898\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.17807", "pdf": "https://arxiv.org/pdf/2504.17807", "abs": "https://arxiv.org/abs/2504.17807", "authors": ["Ze Yang", "Yihong Jin", "Juntian Liu", "Xinhe Xu", "Yihan Zhang", "Shuyang Ji"], "title": "Research on Cloud Platform Network Traffic Monitoring and Anomaly Detection System based on Large Language Models", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": "Proceedings of 2025 IEEE 7th International Conference on\n  Communications, Information System and Computer Engineering (CISCE 2025)", "summary": "The rapidly evolving cloud platforms and the escalating complexity of network\ntraffic demand proper network traffic monitoring and anomaly detection to\nensure network security and performance. This paper introduces a large language\nmodel (LLM)-based network traffic monitoring and anomaly detection system. In\naddition to existing models such as autoencoders and decision trees, we harness\nthe power of large language models for processing sequence data from network\ntraffic, which allows us a better capture of underlying complex patterns, as\nwell as slight fluctuations in the dataset. We show for a given detection task,\nthe need for a hybrid model that incorporates the attention mechanism of the\ntransformer architecture into a supervised learning framework in order to\nachieve better accuracy. A pre-trained large language model analyzes and\npredicts the probable network traffic, and an anomaly detection layer that\nconsiders temporality and context is added. Moreover, we present a novel\ntransfer learning-based methodology to enhance the model's effectiveness to\nquickly adapt to unknown network structures and adversarial conditions without\nrequiring extensive labeled datasets. Actual results show that the designed\nmodel outperforms traditional methods in detection accuracy and computational\nefficiency, effectively identify various network anomalies such as zero-day\nattacks and traffic congestion pattern, and significantly reduce the false\npositive rate.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7f51\u7edc\u6d41\u91cf\u76d1\u63a7\u4e0e\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u4f20\u7edf\u6a21\u578b\uff08\u5982\u81ea\u7f16\u7801\u5668\u548c\u51b3\u7b56\u6811\uff09\u5e76\u5229\u7528LLM\u5904\u7406\u5e8f\u5217\u6570\u636e\uff0c\u80fd\u66f4\u597d\u5730\u6355\u6349\u590d\u6742\u6a21\u5f0f\u548c\u5fae\u5c0f\u6ce2\u52a8\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6df7\u5408\u6a21\u578b\u5728\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u4e91\u5e73\u53f0\u7684\u5feb\u901f\u53d1\u5c55\u548c\u7f51\u7edc\u6d41\u91cf\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u4f20\u7edf\u7684\u7f51\u7edc\u76d1\u63a7\u4e0e\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408Transformer\u67b6\u6784\u7684\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7684LLM\u5206\u6790\u6d41\u91cf\u5e76\u6dfb\u52a0\u8003\u8651\u65f6\u5e8f\u548c\u4e0a\u4e0b\u6587\u7684\u5f02\u5e38\u68c0\u6d4b\u5c42\uff0c\u540c\u65f6\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u6a21\u578b\u7684\u9002\u5e94\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6a21\u578b\u5728\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u96f6\u65e5\u653b\u51fb\u548c\u6d41\u91cf\u62e5\u585e\u7b49\u5f02\u5e38\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u8bef\u62a5\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684LLM\u6df7\u5408\u6a21\u578b\u5728\u7f51\u7edc\u6d41\u91cf\u76d1\u63a7\u4e0e\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u5904\u7406\u590d\u6742\u6a21\u5f0f\u548c\u9002\u5e94\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4e3a\u7f51\u7edc\u5b89\u5168\u548c\u6027\u80fd\u63d0\u4f9b\u4e86\u53ef\u9760\u652f\u6301\u3002"}}
{"id": "2504.18128", "pdf": "https://arxiv.org/pdf/2504.18128", "abs": "https://arxiv.org/abs/2504.18128", "authors": ["Tatsunori Tanaka", "Fi Zheng", "Kai Sato", "Zhifeng Li", "Yuanyun Zhang", "Shi Li"], "title": "Temporal Entailment Pretraining for Clinical Language Models over EHR Data", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Clinical language models have achieved strong performance on downstream tasks\nby pretraining on domain specific corpora such as discharge summaries and\nmedical notes. However, most approaches treat the electronic health record as a\nstatic document, neglecting the temporally-evolving and causally entwined\nnature of patient trajectories. In this paper, we introduce a novel temporal\nentailment pretraining objective for language models in the clinical domain.\nOur method formulates EHR segments as temporally ordered sentence pairs and\ntrains the model to determine whether a later state is entailed by,\ncontradictory to, or neutral with respect to an earlier state. Through this\ntemporally structured pretraining task, models learn to perform latent clinical\nreasoning over time, improving their ability to generalize across forecasting\nand diagnosis tasks. We pretrain on a large corpus derived from MIMIC IV and\ndemonstrate state of the art results on temporal clinical QA, early warning\nprediction, and disease progression modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e34\u5e8a\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u5229\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u65f6\u5e8f\u6027\u6539\u8fdb\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e34\u5e8a\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u5c06\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u89c6\u4e3a\u9759\u6001\u6587\u6863\uff0c\u5ffd\u7565\u4e86\u5176\u65f6\u95f4\u6f14\u5316\u548c\u56e0\u679c\u5173\u8054\u7684\u7279\u6027\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u65f6\u95f4\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5c06\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7247\u6bb5\u5efa\u6a21\u4e3a\u65f6\u5e8f\u53e5\u5b50\u5bf9\uff0c\u8bad\u7ec3\u6a21\u578b\u5224\u65ad\u540e\u7eed\u72b6\u6001\u4e0e\u5148\u524d\u72b6\u6001\u7684\u903b\u8f91\u5173\u7cfb\uff08\u8574\u542b\u3001\u77db\u76fe\u6216\u4e2d\u7acb\uff09\uff0c\u4ece\u800c\u5b66\u4e60\u9690\u542b\u7684\u4e34\u5e8a\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728MIMIC IV\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5728\u65f6\u5e8f\u4e34\u5e8a\u95ee\u7b54\u3001\u65e9\u671f\u9884\u8b66\u9884\u6d4b\u548c\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "\u65f6\u5e8f\u8574\u542b\u9884\u8bad\u7ec3\u76ee\u6807\u663e\u8457\u63d0\u5347\u4e86\u4e34\u5e8a\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u654f\u611f\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u65f6\u5e8f\u7ed3\u6784\u5efa\u6a21\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.18095", "pdf": "https://arxiv.org/pdf/2504.18095", "abs": "https://arxiv.org/abs/2504.18095", "authors": ["Jerrin Thomas Panachakel", "Pradeep Kumar G.", "Suryaa Seran", "Kanishka Sharma", "Ramakrishnan Angarai Ganesan"], "title": "Subject-independent Classification of Meditative State from the Resting State using EEG", "categories": ["cs.LG", "eess.SP"], "comment": "copyright 2024 IEEE Personal use of this material is permitted. 2024\n  IEEE 21st India Council International Conference (INDICON). IEEE, 2024", "summary": "While it is beneficial to objectively determine whether a subject is\nmeditating, most research in the literature reports good results only in a\nsubject-dependent manner. This study aims to distinguish the modified state of\nconsciousness experienced during Rajyoga meditation from the resting state of\nthe brain in a subject-independent manner using EEG data. Three architectures\nhave been proposed and evaluated: The CSP-LDA Architecture utilizes common\nspatial pattern (CSP) for feature extraction and linear discriminant analysis\n(LDA) for classification. The CSP-LDA-LSTM Architecture employs CSP for feature\nextraction, LDA for dimensionality reduction, and long short-term memory (LSTM)\nnetworks for classification, modeling the binary classification problem as a\nsequence learning problem. The SVD-NN Architecture uses singular value\ndecomposition (SVD) to select the most relevant components of the EEG signals\nand a shallow neural network (NN) for classification. The CSP-LDA-LSTM\narchitecture gives the best performance with 98.2% accuracy for intra-subject\nclassification. The SVD-NN architecture provides significant performance with\n96.4\\% accuracy for inter-subject classification. This is comparable to the\nbest-reported accuracies in the literature for intra-subject classification.\nBoth architectures are capable of capturing subject-invariant EEG features for\neffectively classifying the meditative state from the resting state. The high\nintra-subject and inter-subject classification accuracies indicate these\nsystems' robustness and their ability to generalize across different subjects.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7EEG\u6570\u636e\u533a\u5206\u51a5\u60f3\u72b6\u6001\u4e0e\u9759\u606f\u72b6\u6001\uff0c\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u67b6\u6784\uff0c\u5176\u4e2dCSP-LDA-LSTM\u5728\u4e2a\u4f53\u5185\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u6700\u9ad8\u51c6\u786e\u7387\uff0898.2%\uff09\uff0c\u800cSVD-NN\u5728\u4e2a\u4f53\u95f4\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0896.4%\uff09\uff0c\u5c55\u793a\u4e86\u8de8\u4e2a\u4f53\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u4f9d\u8d56\u4e2a\u4f53\u7279\u5b9a\u6570\u636e\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u4ee5\u72ec\u7acb\u4e8e\u4e2a\u4f53\u7684\u65b9\u5f0f\u533a\u5206Rajyoga\u51a5\u60f3\u72b6\u6001\u4e0e\u9759\u606f\u72b6\u6001\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u67b6\u6784\uff1aCSP-LDA\uff08\u7279\u5f81\u63d0\u53d6\u4e0e\u5206\u7c7b\uff09\u3001CSP-LDA-LSTM\uff08\u5e8f\u5217\u5b66\u4e60\u95ee\u9898\u5efa\u6a21\uff09\u548cSVD-NN\uff08\u7279\u5f81\u9009\u62e9\u4e0e\u5206\u7c7b\uff09\uff0c\u57fa\u4e8eEEG\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "CSP-LDA-LSTM\u5728\u4e2a\u4f53\u5185\u5206\u7c7b\u4e2d\u51c6\u786e\u7387\u8fbe98.2%\uff0cSVD-NN\u5728\u4e2a\u4f53\u95f4\u5206\u7c7b\u4e2d\u8fbe96.4%\uff0c\u5747\u4f18\u4e8e\u6587\u732e\u4e2d\u7684\u6700\u4f73\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u8de8\u4e2a\u4f53\u5206\u7c7b\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u67b6\u6784\u80fd\u6709\u6548\u6355\u6349\u8de8\u4e2a\u4f53\u7684EEG\u7279\u5f81\uff0c\u9ad8\u51c6\u786e\u7387\u8868\u660e\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u51a5\u60f3\u72b6\u6001\u8bc6\u522b\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2504.17822", "pdf": "https://arxiv.org/pdf/2504.17822", "abs": "https://arxiv.org/abs/2504.17822", "authors": ["Wenwen Li", "Chia-Yu Hsu", "Sizhe Wang", "Zhining Gu", "Yili Yang", "Brendan M. Rogers", "Anna Liljedahl"], "title": "A multi-scale vision transformer-based multimodal GeoAI model for mapping Arctic permafrost thaw", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Retrogressive Thaw Slumps (RTS) in Arctic regions are distinct permafrost\nlandforms with significant environmental impacts. Mapping these RTS is crucial\nbecause their appearance serves as a clear indication of permafrost thaw.\nHowever, their small scale compared to other landform features, vague\nboundaries, and spatiotemporal variation pose significant challenges for\naccurate detection. In this paper, we employed a state-of-the-art deep learning\nmodel, the Cascade Mask R-CNN with a multi-scale vision transformer-based\nbackbone, to delineate RTS features across the Arctic. Two new strategies were\nintroduced to optimize multimodal learning and enhance the model's predictive\nperformance: (1) a feature-level, residual cross-modality attention fusion\nstrategy, which effectively integrates feature maps from multiple modalities to\ncapture complementary information and improve the model's ability to understand\ncomplex patterns and relationships within the data; (2) pre-trained unimodal\nlearning followed by multimodal fine-tuning to alleviate high computing demand\nwhile achieving strong model performance. Experimental results demonstrated\nthat our approach outperformed existing models adopting data-level fusion,\nfeature-level convolutional fusion, and various attention fusion strategies,\nproviding valuable insights into the efficient utilization of multimodal data\nfor RTS mapping. This research contributes to our understanding of permafrost\nlandforms and their environmental implications.", "AI": {"tldr": "\u4f7f\u7528\u6539\u8fdb\u7684Cascade Mask R-CNN\u4e0e\u591a\u5c3a\u5ea6\u89c6\u89c9Transformer\u7ed3\u5408\u7684\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u5317\u6781\u5730\u533aRetrogressive Thaw Slumps\uff08RTS\uff09\u7684\u6d4b\u7ed8\u7cbe\u5ea6\u3002", "motivation": "RTS\u4f5c\u4e3a\u6c38\u4e45\u51bb\u571f\u878d\u5316\u7684\u6807\u5fd7\u6027\u7279\u5f81\uff0c\u5176\u7cbe\u786e\u6d4b\u7ed8\u5bf9\u73af\u5883\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b58\u5728\u89c4\u6a21\u5c0f\u3001\u8fb9\u754c\u6a21\u7cca\u548c\u65f6\u7a7a\u53d8\u5316\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528Cascade Mask R-CNN\u6a21\u578b\u548c\u591a\u5c3a\u5ea6\u89c6\u89c9Transformer\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u65b0\u7b56\u7565\uff1a\u7279\u5f81\u7ea7\u6b8b\u5dee\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u4e0e\u9884\u8bad\u7ec3\u5355\u6a21\u6001\u5b66\u4e60\u540e\u591a\u6a21\u6001\u5fae\u8c03\u3002", "result": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u6570\u636e\u7ea7\u548c\u7279\u5f81\u7ea7\u878d\u5408\u7b56\u7565\uff0c\u4e3a\u591a\u6a21\u6001\u6570\u636e\u9ad8\u6548\u5229\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "conclusion": "\u7814\u7a76\u4e0d\u4ec5\u63d0\u5347\u4e86RTS\u6d4b\u7ed8\u7684\u7cbe\u5ea6\uff0c\u8fd8\u52a0\u6df1\u4e86\u5bf9\u6c38\u4e45\u51bb\u571f\u5730\u8c8c\u53ca\u5176\u73af\u5883\u5f71\u54cd\u7684\u7406\u89e3\u3002"}}
{"id": "2504.18142", "pdf": "https://arxiv.org/pdf/2504.18142", "abs": "https://arxiv.org/abs/2504.18142", "authors": ["Fida Ullah", "Muhammad Ahmad", "Muhammad Tayyab Zamir", "Muhammad Arif", "Grigori sidorov", "Edgardo Manuel Felipe River\u00f3n", "Alexander Gelbukh"], "title": "EDU-NER-2025: Named Entity Recognition in Urdu Educational Texts using XLM-RoBERTa with X (formerly Twitter)", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Named Entity Recognition (NER) plays a pivotal role in various Natural\nLanguage Processing (NLP) tasks by identifying and classifying named entities\n(NEs) from unstructured data into predefined categories such as person,\norganization, location, date, and time. While extensive research exists for\nhigh-resource languages and general domains, NER in Urdu particularly within\ndomain-specific contexts like education remains significantly underexplored.\nThis is Due to lack of annotated datasets for educational content which limits\nthe ability of existing models to accurately identify entities such as academic\nroles, course names, and institutional terms, underscoring the urgent need for\ntargeted resources in this domain. To the best of our knowledge, no dataset\nexists in the domain of the Urdu language for this purpose. To achieve this\nobjective this study makes three key contributions. Firstly, we created a\nmanually annotated dataset in the education domain, named EDU-NER-2025, which\ncontains 13 unique most important entities related to education domain. Second,\nwe describe our annotation process and guidelines in detail and discuss the\nchallenges of labelling EDU-NER-2025 dataset. Third, we addressed and analyzed\nkey linguistic challenges, such as morphological complexity and ambiguity,\nwhich are prevalent in formal Urdu texts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u4e4c\u5c14\u90fd\u8bed\u6559\u80b2\u9886\u57df\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u4efb\u52a1\uff0c\u521b\u5efa\u4e86\u540d\u4e3aEDU-NER-2025\u7684\u624b\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u5206\u6790\u4e86\u8be5\u9886\u57df\u7684\u8bed\u8a00\u5b66\u6311\u6218\u3002", "motivation": "\u4e4c\u5c14\u90fd\u8bed\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982\u6559\u80b2\uff09\u4e2d\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7814\u7a76\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u8bc6\u522b\u5b66\u672f\u89d2\u8272\u3001\u8bfe\u7a0b\u540d\u79f0\u548c\u673a\u6784\u672f\u8bed\u7b49\u5b9e\u4f53\u4e0a\u7684\u51c6\u786e\u6027\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u624b\u5de5\u6807\u6ce8\u7684\u6570\u636e\u96c6EDU-NER-2025\uff0c\u5305\u542b13\u4e2a\u6559\u80b2\u9886\u57df\u7684\u91cd\u8981\u5b9e\u4f53\uff1b\u8be6\u7ec6\u63cf\u8ff0\u4e86\u6807\u6ce8\u6d41\u7a0b\u548c\u6307\u5357\uff1b\u5206\u6790\u4e86\u4e4c\u5c14\u90fd\u8bed\u4e2d\u7684\u8bed\u8a00\u5b66\u6311\u6218\uff0c\u5982\u5f62\u6001\u590d\u6742\u6027\u548c\u6b67\u4e49\u3002", "result": "\u751f\u6210\u4e86\u9996\u4e2a\u4e4c\u5c14\u90fd\u8bed\u6559\u80b2\u9886\u57df\u7684\u624b\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u6807\u6ce8\u8fc7\u7a0b\u7684\u5206\u6790\u548c\u8bed\u8a00\u5b66\u6311\u6218\u7684\u8ba8\u8bba\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u4e4c\u5c14\u90fd\u8bed\u6559\u80b2\u9886\u57dfNER\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u5e76\u6307\u51fa\u4e86\u8bed\u8a00\u5b66\u6311\u6218\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002"}}
{"id": "2504.18105", "pdf": "https://arxiv.org/pdf/2504.18105", "abs": "https://arxiv.org/abs/2504.18105", "authors": ["Dinan Li", "Panagiotis Kakosimos"], "title": "Temperature Estimation in Induction Motors using Machine Learning", "categories": ["cs.LG"], "comment": "2023 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "The number of electrified powertrains is ever increasing today towards a more\nsustainable future; thus, it is essential that unwanted failures are prevented,\nand a reliable operation is secured. Monitoring the internal temperatures of\nmotors and keeping them under their thresholds is an important first step.\nConventional modeling methods require expert knowledge and complicated\nmathematical approaches. With all the data a modern electric drive collects\nnowadays during the system operation, it is feasible to apply data-driven\napproaches for estimating thermal behaviors. In this paper, multiple\nmachine-learning methods are investigated on their capability to approximate\nthe temperatures of the stator winding and bearing in induction motors. The\nexplored algorithms vary from linear to neural networks. For this reason,\nexperimental lab data have been captured from a powertrain under predetermined\noperating conditions. For each approach, a hyperparameter search is then\nperformed to find the optimal configuration. All the models are evaluated by\nvarious metrics, and it has been found that neural networks perform\nsatisfactorily even under transient conditions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f30\u8ba1\u611f\u5e94\u7535\u673a\u5b9a\u5b50\u7ed5\u7ec4\u548c\u8f74\u627f\u6e29\u5ea6\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5305\u62ec\u4ece\u7ebf\u6027\u6a21\u578b\u5230\u795e\u7ecf\u7f51\u7edc\u7684\u4e0d\u540c\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u795e\u7ecf\u7f51\u7edc\u5728\u77ac\u6001\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u968f\u7740\u7535\u52a8\u52a8\u529b\u7cfb\u7edf\u7684\u666e\u53ca\uff0c\u786e\u4fdd\u5176\u53ef\u9760\u8fd0\u884c\u548c\u9884\u9632\u6545\u969c\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u76d1\u6d4b\u7535\u673a\u5185\u90e8\u6e29\u5ea6\u662f\u91cd\u8981\u4e00\u6b65\uff0c\u4f46\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\u9700\u8981\u4e13\u5bb6\u77e5\u8bc6\u548c\u590d\u6742\u6570\u5b66\uff0c\u800c\u73b0\u4ee3\u7535\u52a8\u9a71\u52a8\u7cfb\u7edf\u6536\u96c6\u7684\u5927\u91cf\u6570\u636e\u4f7f\u5f97\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6210\u4e3a\u53ef\u80fd\u3002", "method": "\u7814\u7a76\u91c7\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u4ece\u7ebf\u6027\u6a21\u578b\u5230\u795e\u7ecf\u7f51\u7edc\uff09\u5bf9\u611f\u5e94\u7535\u673a\u7684\u5b9a\u5b50\u7ed5\u7ec4\u548c\u8f74\u627f\u6e29\u5ea6\u8fdb\u884c\u4f30\u8ba1\u3002\u901a\u8fc7\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\u6570\u636e\uff0c\u5bf9\u6bcf\u79cd\u65b9\u6cd5\u8fdb\u884c\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u5e76\u4ee5\u591a\u79cd\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u5728\u77ac\u6001\u6761\u4ef6\u4e0b\uff0c\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u4e5f\u80fd\u63d0\u4f9b\u6ee1\u610f\u7684\u6e29\u5ea6\u4f30\u8ba1\u7ed3\u679c\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u795e\u7ecf\u7f51\u7edc\uff0c\u80fd\u591f\u6709\u6548\u7528\u4e8e\u7535\u673a\u6e29\u5ea6\u7684\u5b9e\u65f6\u76d1\u6d4b\uff0c\u4e3a\u7535\u52a8\u52a8\u529b\u7cfb\u7edf\u7684\u53ef\u9760\u8fd0\u884c\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2504.17823", "pdf": "https://arxiv.org/pdf/2504.17823", "abs": "https://arxiv.org/abs/2504.17823", "authors": ["Darcy Kim", "Aida Kalender", "Sennay Ghebreab", "Giovanni Sileno"], "title": "The Cloud Weaving Model for AI development", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "presented at alt.CHI 2025, Yokohama", "summary": "While analysing challenges in pilot projects developing AI with marginalized\ncommunities, we found it difficult to express them within commonly used\nparadigms. We therefore constructed an alternative conceptual framework to\nground AI development in the social fabric -- the Cloud Weaving Model --\ninspired (amongst others) by indigenous knowledge, motifs from nature, and\nEastern traditions. This paper introduces and elaborates on the fundamental\nelements of the model (clouds, spiders, threads, spiderwebs, and weather) and\ntheir interpretation in an AI context. The framework is then applied to\ncomprehend patterns observed in co-creation pilots approaching marginalized\ncommunities, highlighting neglected yet relevant dimensions for responsible AI\ndevelopment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u2018\u4e91\u7f16\u7ec7\u6a21\u578b\u2019\u7684\u66ff\u4ee3\u6027\u6982\u5ff5\u6846\u67b6\uff0c\u65e8\u5728\u5c06AI\u5f00\u53d1\u6839\u690d\u4e8e\u793e\u4f1a\u7ed3\u6784\u4e2d\uff0c\u5e76\u5e94\u7528\u4e8e\u7406\u89e3\u8fb9\u7f18\u5316\u793e\u533a\u4e2d\u7684\u5171\u521b\u8bd5\u70b9\u6a21\u5f0f\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u8303\u5f0f\u96be\u4ee5\u8868\u8fbe\u8fb9\u7f18\u5316\u793e\u533aAI\u5f00\u53d1\u4e2d\u7684\u6311\u6218\uff0c\u4f5c\u8005\u53d7\u571f\u8457\u77e5\u8bc6\u3001\u81ea\u7136\u56fe\u6848\u548c\u4e1c\u65b9\u4f20\u7edf\u542f\u53d1\uff0c\u6784\u5efa\u4e86\u65b0\u7684\u6846\u67b6\u4ee5\u6355\u6349\u5e38\u88ab\u5ffd\u7565\u7684\u7ef4\u5ea6\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u2018\u4e91\u3001\u8718\u86db\u3001\u7ebf\u3001\u8718\u86db\u7f51\u548c\u5929\u6c14\u2019\u7b49\u6838\u5fc3\u5143\u7d20\u53ca\u5176\u5728AI\u8bed\u5883\u4e0b\u7684\u89e3\u91ca\uff0c\u6846\u67b6\u88ab\u8bbe\u8ba1\u5e76\u5e94\u7528\u4e8e\u5171\u521b\u8bd5\u70b9\u7684\u6a21\u5f0f\u5206\u6790\u3002", "result": "\u8be5\u6a21\u578b\u6210\u529f\u8bc6\u522b\u4e86\u8fb9\u7f18\u5316\u793e\u533aAI\u5f00\u53d1\u4e2d\u88ab\u5ffd\u89c6\u4f46\u5173\u952e\u7684\u793e\u4f1a\u7ef4\u5ea6\uff0c\u4e3a\u8d23\u4efbAI\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "conclusion": "\u4e91\u7f16\u7ec7\u6a21\u578b\u4e3aAI\u5f00\u53d1\u7684\u793e\u4f1a\u5d4c\u5165\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u5f3a\u8c03\u4e86\u591a\u5143\u77e5\u8bc6\u4f53\u7cfb\u7684\u4ef7\u503c\u3002"}}
{"id": "2504.18180", "pdf": "https://arxiv.org/pdf/2504.18180", "abs": "https://arxiv.org/abs/2504.18180", "authors": ["\u00de\u00f3rir Hrafn Har\u00f0arson", "Hrafn Loftsson", "Stef\u00e1n \u00d3lafsson"], "title": "Aligning Language Models for Icelandic Legal Text Summarization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published at NoDaLiDa 2025", "summary": "The integration of language models in the legal domain holds considerable\npromise for streamlining processes and improving efficiency in managing\nextensive workloads. However, the specialized terminology, nuanced language,\nand formal style of legal texts can present substantial challenges. This study\nexamines whether preference-based training techniques, specifically\nReinforcement Learning from Human Feedback and Direct Preference Optimization,\ncan enhance models' performance in generating Icelandic legal summaries that\nalign with domain-specific language standards and user preferences. We compare\nmodels fine-tuned with preference training to those using conventional\nsupervised learning. Results indicate that preference training improves the\nlegal accuracy of generated summaries over standard fine-tuning but does not\nsignificantly enhance the overall quality of Icelandic language usage.\nDiscrepancies between automated metrics and human evaluations further\nunderscore the importance of qualitative assessment in developing language\nmodels for the legal domain.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u504f\u597d\u8bad\u7ec3\uff08\u5982RLHF\u548cDPO\uff09\u80fd\u5426\u6539\u8fdb\u8bed\u8a00\u6a21\u578b\u751f\u6210\u51b0\u5c9b\u6cd5\u5f8b\u6458\u8981\u7684\u6cd5\u5f8b\u51c6\u786e\u6027\uff0c\u7ed3\u679c\u663e\u793a\u504f\u597d\u8bad\u7ec3\u5728\u6cd5\u5f8b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\uff0c\u4f46\u5bf9\u63d0\u5347\u51b0\u5c9b\u8bed\u8a00\u8d28\u91cf\u6548\u679c\u4e0d\u663e\u8457\u3002", "motivation": "\u6cd5\u5f8b\u9886\u57df\u7684\u672f\u8bed\u548c\u8bed\u8a00\u590d\u6742\u6027\u5bf9\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u504f\u597d\u8bad\u7ec3\u6709\u671b\u63d0\u5347\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u7684\u8868\u73b0\u3002", "method": "\u6bd4\u8f83\u4e86\u504f\u597d\u8bad\u7ec3\uff08RLHF\u548cDPO\uff09\u4e0e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u5728\u751f\u6210\u51b0\u5c9b\u6cd5\u5f8b\u6458\u8981\u4e0a\u7684\u6548\u679c\u3002", "result": "\u504f\u597d\u8bad\u7ec3\u63d0\u9ad8\u4e86\u6cd5\u5f8b\u51c6\u786e\u6027\uff0c\u4f46\u5bf9\u8bed\u8a00\u8d28\u91cf\u63d0\u5347\u4e0d\u660e\u663e\uff1b\u81ea\u52a8\u6307\u6807\u4e0e\u4eba\u5de5\u8bc4\u4f30\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u6cd5\u5f8b\u9886\u57df\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u9700\u91cd\u89c6\u5b9a\u6027\u8bc4\u4f30\uff0c\u504f\u597d\u8bad\u7ec3\u5728\u6cd5\u5f8b\u51c6\u786e\u6027\u4e0a\u6709\u4f18\u52bf\u3002"}}
{"id": "2504.18113", "pdf": "https://arxiv.org/pdf/2504.18113", "abs": "https://arxiv.org/abs/2504.18113", "authors": ["Aniket Dixit", "Muhammad Ibrahim Khan", "Faizan Ahmed", "James Brusey"], "title": "Learning from Less: SINDy Surrogates in RL", "categories": ["cs.LG", "cs.AI"], "comment": "World Models @ ICLR 2025", "summary": "This paper introduces an approach for developing surrogate environments in\nreinforcement learning (RL) using the Sparse Identification of Nonlinear\nDynamics (SINDy) algorithm. We demonstrate the effectiveness of our approach\nthrough extensive experiments in OpenAI Gym environments, particularly Mountain\nCar and Lunar Lander. Our results show that SINDy-based surrogate models can\naccurately capture the underlying dynamics of these environments while reducing\ncomputational costs by 20-35%. With only 75 interactions for Mountain Car and\n1000 for Lunar Lander, we achieve state-wise correlations exceeding 0.997, with\nmean squared errors as low as 3.11e-06 for Mountain Car velocity and 1.42e-06\nfor LunarLander position. RL agents trained in these surrogate environments\nrequire fewer total steps (65,075 vs. 100,000 for Mountain Car and 801,000 vs.\n1,000,000 for Lunar Lander) while achieving comparable performance to those\ntrained in the original environments, exhibiting similar convergence patterns\nand final performance metrics. This work contributes to the field of\nmodel-based RL by providing an efficient method for generating accurate,\ninterpretable surrogate environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSINDy\u7b97\u6cd5\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u73af\u5883\u6784\u5efa\u65b9\u6cd5\uff0c\u5728OpenAI Gym\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u5e76\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u51cf\u5c11\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8ba1\u7b97\u5f00\u9500\u5e76\u4fdd\u6301\u73af\u5883\u52a8\u6001\u7684\u51c6\u786e\u6027\uff0c\u7814\u7a76\u63a2\u7d22\u4e86\u57fa\u4e8eSINDy\u7684\u4ee3\u7406\u73af\u5883\u6784\u5efa\u65b9\u6cd5\u3002", "method": "\u91c7\u7528SINDy\u7b97\u6cd5\u4ece\u5c11\u91cf\u4ea4\u4e92\u6570\u636e\u4e2d\u7a00\u758f\u8bc6\u522b\u975e\u7ebf\u6027\u52a8\u6001\uff0c\u6784\u5efa\u4ee3\u7406\u73af\u5883\u3002", "result": "\u4ee3\u7406\u6a21\u578b\u5728Mountain Car\u548cLunar Lander\u73af\u5883\u4e2d\u52a8\u6001\u6355\u6349\u51c6\u786e\uff08\u76f8\u5173\u6027>0.997\uff09\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e20-35%\uff0c\u4e14\u8bad\u7ec3\u6b65\u9aa4\u663e\u8457\u51cf\u5c11\uff08\u5982Mountain Car\u4ece100,000\u964d\u81f365,075\u6b65\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u4ee3\u7406\u73af\u5883\u751f\u6210\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u8282\u7ea6\u8d44\u6e90\u4e0e\u4fdd\u6301\u6027\u80fd\u4e0a\u7684\u4f18\u52bf\u3002"}}
{"id": "2504.17824", "pdf": "https://arxiv.org/pdf/2504.17824", "abs": "https://arxiv.org/abs/2504.17824", "authors": ["Yibin Wang", "Jiaxi Xie", "Lakshminarayanan Subramanian"], "title": "EduBot -- Can LLMs Solve Personalized Learning and Programming Assignments?", "categories": ["cs.SE", "cs.AI"], "comment": "Published at AAAI 2025 AI4EDU Workshop", "summary": "The prevalence of Large Language Models (LLMs) is revolutionizing the process\nof writing code. General and code LLMs have shown impressive performance in\ngenerating standalone functions and code-completion tasks with one-shot\nqueries. However, the ability to solve comprehensive programming tasks with\nrecursive requests and bug fixes remains questionable. In this paper, we\npropose EduBot, an intelligent automated assistant system that combines\nconceptual knowledge teaching, end-to-end code development, personalized\nprogramming through recursive prompt-driven methods, and debugging with limited\nhuman interventions powered by LLMs. We show that EduBot can solve complicated\nprogramming tasks consisting of sub-tasks with increasing difficulties ranging\nfrom conceptual to coding questions by recursive automatic prompt-driven\nsystems without finetuning on LLMs themselves. To further evaluate EduBot's\nperformance, we design and conduct a benchmark suite consisting of 20 scenarios\nin algorithms, machine learning, and real-world problems. The result shows that\nEduBot can complete most scenarios in less than 20 minutes. Based on the\nbenchmark suites, we perform a comparative study to take different LLMs as the\nbackbone and to verify EduBot's compatibility and robustness across LLMs with\nvarying capabilities. We believe that EduBot is an exploratory approach to\nexplore the potential of pre-trained LLMs in multi-step reasoning and code\ngeneration for solving personalized assignments with knowledge learning and\ncode generation.", "AI": {"tldr": "EduBot\u662f\u4e00\u4e2a\u57fa\u4e8eLLMs\u7684\u667a\u80fd\u7f16\u7a0b\u52a9\u624b\uff0c\u901a\u8fc7\u9012\u5f52\u63d0\u793a\u9a71\u52a8\u65b9\u6cd5\u89e3\u51b3\u590d\u6742\u7f16\u7a0b\u4efb\u52a1\uff0c\u5305\u62ec\u6982\u5ff5\u6559\u5b66\u3001\u4ee3\u7801\u5f00\u53d1\u548c\u8c03\u8bd5\uff0c\u65e0\u9700\u5fae\u8c03LLMs\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3LLMs\u5728\u591a\u6b65\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u4e2a\u6027\u5316\u7f16\u7a0b\u4efb\u52a1\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "method": "\u7ed3\u5408\u6982\u5ff5\u6559\u5b66\u3001\u7aef\u5230\u7aef\u4ee3\u7801\u5f00\u53d1\u3001\u4e2a\u6027\u5316\u7f16\u7a0b\u548c\u8c03\u8bd5\uff0c\u901a\u8fc7\u9012\u5f52\u63d0\u793a\u9a71\u52a8\u65b9\u6cd5\uff0c\u5229\u7528LLMs\u5b8c\u6210\u4efb\u52a1\u3002", "result": "\u5728\u7b97\u6cd5\u3001\u673a\u5668\u5b66\u4e60\u548c\u5b9e\u9645\u95ee\u9898\u7b4920\u4e2a\u573a\u666f\u7684\u6d4b\u8bd5\u4e2d\uff0cEduBot\u80fd\u572820\u5206\u949f\u5185\u5b8c\u6210\u5927\u90e8\u5206\u4efb\u52a1\u3002", "conclusion": "EduBot\u5c55\u793a\u4e86\u9884\u8bad\u7ec3LLMs\u5728\u591a\u6b65\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u4e2a\u6027\u5316\u7f16\u7a0b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18221", "pdf": "https://arxiv.org/pdf/2504.18221", "abs": "https://arxiv.org/abs/2504.18221", "authors": ["Shuxiang Du", "Ana Guerberof Arenas", "Antonio Toral", "Kyo Gerrits", "Josep Marco Borillo"], "title": "Optimising ChatGPT for creativity in literary translation: A case study from English into Dutch, Chinese, Catalan and Spanish", "categories": ["cs.CL"], "comment": "This paper has been accepted to the MT Summit 2025 to be held in\n  Geneva on June 23-27 2025", "summary": "This study examines the variability of Chat-GPT machine translation (MT)\noutputs across six different configurations in four languages,with a focus on\ncreativity in a literary text. We evaluate GPT translations in different text\ngranularity levels, temperature settings and prompting strategies with a\nCreativity Score formula. We found that prompting ChatGPT with a minimal\ninstruction yields the best creative translations, with \"Translate the\nfollowing text into [TG] creatively\" at the temperature of 1.0 outperforming\nother configurations and DeepL in Spanish, Dutch, and Chinese. Nonetheless,\nChatGPT consistently underperforms compared to human translation (HT).", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86ChatGPT\u5728\u516d\u79cd\u4e0d\u540c\u914d\u7f6e\u4e0b\u5bf9\u6587\u5b66\u6587\u672c\u7684\u7ffb\u8bd1\u8868\u73b0\uff0c\u53d1\u73b0\u7b80\u6d01\u6307\u4ee4\uff08\u5982'\u8bf7\u521b\u9020\u6027\u7ffb\u8bd1'\uff09\u5728\u6e29\u5ea61.0\u65f6\u8868\u73b0\u6700\u4f18\uff0c\u4f46\u4ecd\u4e0d\u53ca\u4eba\u5de5\u7ffb\u8bd1\u3002", "motivation": "\u63a2\u7d22ChatGPT\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u5bf9\u6587\u5b66\u6587\u672c\u7684\u521b\u9020\u6027\u7ffb\u8bd1\u80fd\u529b\uff0c\u5e76\u4e0e\u73b0\u6709\u5de5\u5177\uff08\u5982DeepL\uff09\u53ca\u4eba\u5de5\u7ffb\u8bd1\u8fdb\u884c\u5bf9\u6bd4\u3002", "method": "\u91c7\u7528\u516d\u79cd\u914d\u7f6e\uff08\u5305\u62ec\u6587\u672c\u7c92\u5ea6\u3001\u6e29\u5ea6\u8bbe\u7f6e\u548c\u63d0\u793a\u7b56\u7565\uff09\uff0c\u901a\u8fc7\u521b\u9020\u529b\u8bc4\u5206\u516c\u5f0f\u8bc4\u4f30\u56db\u79cd\u8bed\u8a00\u7684\u7ffb\u8bd1\u7ed3\u679c\u3002", "result": "\u7b80\u6d01\u6307\u4ee4\uff08\u6e29\u5ea61.0\uff09\u5728\u897f\u73ed\u7259\u8bed\u3001\u8377\u5170\u8bed\u548c\u4e2d\u6587\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4ecd\u900a\u8272\u4e8e\u4eba\u5de5\u7ffb\u8bd1\u3002", "conclusion": "ChatGPT\u5728\u521b\u9020\u6027\u7ffb\u8bd1\u4e2d\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u9636\u6bb5\u65e0\u6cd5\u66ff\u4ee3\u4eba\u5de5\u7ffb\u8bd1\u3002"}}
{"id": "2504.18116", "pdf": "https://arxiv.org/pdf/2504.18116", "abs": "https://arxiv.org/abs/2504.18116", "authors": ["Caia Costello", "Simon Guo", "Anna Goldie", "Azalia Mirhoseini"], "title": "Think, Prune, Train, Improve: Scaling Reasoning without Scaling Models", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nprogramming and mathematical reasoning tasks, but are constrained by limited\nhigh-quality training data. Synthetic data can be leveraged to enhance\nfine-tuning outcomes, but several factors influence this process, including\nmodel size, synthetic data volume, pruning strategy, and number of fine-tuning\nrounds. We explore these axes and investigate which conditions enable model\nself-improvement. We introduce the Think, Prune, Train process, a scalable\nframework that iteratively fine-tunes models on their own reasoning traces,\nusing ground-truth pruning to ensure high-quality training data. This approach\nyields improved performance: on GSM8K, Gemma2-2B achieves a Pass@1 of 57.6%\n(from 41.9%), Gemma2-9B reaches 82%, matching LLaMA-3.1-70B, and LLaMA-3.1-70B\nattains 91%, even surpassing GPT-4o, demonstrating the effectiveness of\nself-generated reasoning and systematic data selection for improving LLM\ncapabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'Think, Prune, Train'\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u548c\u9ad8\u54c1\u8d28\u8bad\u7ec3\u6570\u636e\u7684\u4fee\u526a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7f16\u7a0b\u548c\u6570\u5b66\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684LLMs\u5728\u7f16\u7a0b\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u6027\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528\u5408\u6210\u6570\u636e\u589e\u5f3a\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u7814\u7a76\u54ea\u4e9b\u6761\u4ef6\u80fd\u591f\u4fc3\u8fdb\u6a21\u578b\u7684\u81ea\u6211\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86'Think, Prune, Train'\u6d41\u7a0b\uff0c\u901a\u8fc7\u8fed\u4ee3\u5fae\u8c03\u6a21\u578b\u81ea\u8eab\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5e76\u4f7f\u7528\u771f\u5b9e\u503c\u4fee\u526a\u6280\u672f\u786e\u4fdd\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u7efc\u5408\u8003\u5bdf\u4e86\u6a21\u578b\u5927\u5c0f\u3001\u5408\u6210\u6570\u636e\u91cf\u3001\u4fee\u526a\u7b56\u7565\u53ca\u5fae\u8c03\u8f6e\u6b21\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "result": "\u5728GSM8K\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff1aGemma2-2B\u7684Pass@1\u5f97\u5206\u4ece41.9%\u63d0\u9ad8\u523057.6%\uff0cGemma2-9B\u8fbe\u523082%\uff0c\u4e0eLLaMA-3.1-70B\u6301\u5e73\uff1bLLaMA-3.1-70B\u66f4\u662f\u8fbe\u523091%\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86GPT-4o\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6570\u636e\u9009\u62e9\u548c\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347LLMs\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u4f4e\u8d28\u6570\u636e\u73af\u5883\u4e0b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.17825", "pdf": "https://arxiv.org/pdf/2504.17825", "abs": "https://arxiv.org/abs/2504.17825", "authors": ["Dehong Kong", "Fan Li", "Zhixin Wang", "Jiaqi Xu", "Renjing Pei", "Wenbo Li", "WenQi Ren"], "title": "Dual Prompting Image Restoration with Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR2025", "summary": "Recent state-of-the-art image restoration methods mostly adopt latent\ndiffusion models with U-Net backbones, yet still facing challenges in achieving\nhigh-quality restoration due to their limited capabilities. Diffusion\ntransformers (DiTs), like SD3, are emerging as a promising alternative because\nof their better quality with scalability. In this paper, we introduce DPIR\n(Dual Prompting Image Restoration), a novel image restoration method that\neffectivly extracts conditional information of low-quality images from multiple\nperspectives. Specifically, DPIR consits of two branches: a low-quality image\nconditioning branch and a dual prompting control branch. The first branch\nutilizes a lightweight module to incorporate image priors into the DiT with\nhigh efficiency. More importantly, we believe that in image restoration,\ntextual description alone cannot fully capture its rich visual characteristics.\nTherefore, a dual prompting module is designed to provide DiT with additional\nvisual cues, capturing both global context and local appearance. The extracted\nglobal-local visual prompts as extra conditional control, alongside textual\nprompts to form dual prompts, greatly enhance the quality of the restoration.\nExtensive experimental results demonstrate that DPIR delivers superior image\nrestoration performance.", "AI": {"tldr": "DPIR\u662f\u4e00\u79cd\u65b0\u578b\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u63d0\u793a\u63a7\u5236\u5206\u652f\u548c\u591a\u89d2\u5ea6\u6761\u4ef6\u4fe1\u606f\u63d0\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u56fe\u50cf\u4fee\u590d\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982U-Net\u9690\u6269\u6563\u6a21\u578b\uff09\u5728\u56fe\u50cf\u4fee\u590d\u4e2d\u80fd\u529b\u6709\u9650\uff0c\u800c\u6269\u6563\u53d8\u6362\u5668\uff08DiTs\uff09\u56e0\u5176\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u8d28\u91cf\u8868\u73b0\u6210\u4e3a\u6f5c\u5728\u66ff\u4ee3\u65b9\u6848\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53cc\u63d0\u793a\u6a21\u5757\u548c\u591a\u89d2\u5ea6\u6761\u4ef6\u63d0\u53d6\u89e3\u51b3\u6587\u672c\u63cf\u8ff0\u65e0\u6cd5\u5145\u5206\u6355\u6349\u89c6\u89c9\u7279\u5f81\u7684\u95ee\u9898\u3002", "method": "DPIR\u91c7\u7528\u53cc\u5206\u652f\u7ed3\u6784\uff1a1\uff09\u8f7b\u91cf\u7ea7\u4f4e\u8d28\u56fe\u50cf\u6761\u4ef6\u5206\u652f\uff0c\u9ad8\u6548\u6574\u5408\u56fe\u50cf\u5148\u9a8c\uff1b2\uff09\u53cc\u63d0\u793a\u63a7\u5236\u5206\u652f\uff0c\u63d0\u4f9b\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u5916\u89c2\u7684\u89c6\u89c9\u63d0\u793a\uff0c\u4e0e\u6587\u672c\u63d0\u793a\u7ed3\u5408\u5f62\u6210\u53cc\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDPIR\u5728\u56fe\u50cf\u4fee\u590d\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u53cc\u63d0\u793a\u8bbe\u8ba1\u663e\u8457\u589e\u5f3a\u4e86\u4fee\u590d\u8d28\u91cf\u3002", "conclusion": "DPIR\u901a\u8fc7\u53cc\u63d0\u793a\u548c\u591a\u89d2\u5ea6\u6761\u4ef6\u63d0\u53d6\uff0c\u4e3a\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u56fe\u50cf\u4fee\u590d\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u65b0\u65b9\u6848\u3002"}}
{"id": "2504.18225", "pdf": "https://arxiv.org/pdf/2504.18225", "abs": "https://arxiv.org/abs/2504.18225", "authors": ["Pierre-Carl Langlais", "Pavel Chizhov", "Mattia Nee", "Carlos Rosas Hinostroza", "Matthieu Delsart", "Ir\u00e8ne Girard", "Othman Hicheur", "Anastasia Stasenko", "Ivan P. Yamshchikov"], "title": "Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family", "categories": ["cs.CL"], "comment": null, "summary": "We introduce a new generation of small reasoning models for RAG, search, and\nsource summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a\nlarge synthetic dataset emulating the retrieval of a wide variety of\nmultilingual open sources from the Common Corpus. They provide native support\nfor citation and grounding with literal quotes and reintegrate multiple\nfeatures associated with RAG workflows, such as query routing, query\nreformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B\noutperform SLMs below 4 billion parameters on standardized RAG benchmarks\n(HotPotQA, 2wiki) and are competitive with popular larger models, including\nQwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date\nmaintaining consistent RAG performance across leading European languages and\nensuring systematic reference grounding for statements. Due to their size and\nease of deployment on constrained infrastructure and higher factuality by\ndesign, the models unlock a range of new use cases for generative AI.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e24\u6b3e\u65b0\u578b\u5c0f\u578b\u63a8\u7406\u6a21\u578bPleias-RAG-350m\u548cPleias-RAG-1B\uff0c\u7528\u4e8eRAG\u3001\u641c\u7d22\u548c\u6e90\u6458\u8981\uff0c\u8868\u73b0\u4f18\u4e8e4B\u53c2\u6570\u4ee5\u4e0b\u7684SLM\uff0c\u5e76\u4e0e\u66f4\u5927\u7684\u6a21\u578b\u7ade\u4e89\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f53\u524d\u5c0f\u578b\u63a8\u7406\u6a21\u578b\u5728RAG\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u591a\u8bed\u8a00\u548c\u5f15\u7528\u57fa\u7840\u65b9\u9762\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5927\u578b\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u6a21\u62df\u591a\u8bed\u8a00\u5f00\u6e90\u68c0\u7d22\uff0c\u652f\u6301\u5f15\u7528\u548c\u57fa\u7840\u529f\u80fd\uff0c\u5e76\u6574\u5408RAG\u5de5\u4f5c\u6d41\u7279\u6027\u5982\u67e5\u8be2\u8def\u7531\u548c\u91cd\u6392\u3002", "result": "Pleias-RAG-350m\u548cPleias-RAG-1B\u5728\u6807\u51c6RAG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u591a\u8bed\u8a00\u4e14\u5f15\u7528\u57fa\u7840\u4e00\u81f4\u3002", "conclusion": "\u8fd9\u4e24\u6b3e\u6a21\u578b\u56e0\u5176\u5c0f\u89c4\u6a21\u548c\u6613\u90e8\u7f72\u6027\uff0c\u4e3a\u751f\u6210\u5f0fAI\u5f00\u8f9f\u4e86\u65b0\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2504.18130", "pdf": "https://arxiv.org/pdf/2504.18130", "abs": "https://arxiv.org/abs/2504.18130", "authors": ["Vasily Ilin", "Bamdad Hosseini", "Jingwei Hu"], "title": "Score-Based Deterministic Density Sampling", "categories": ["cs.LG", "math.PR", "math.ST", "stat.TH"], "comment": null, "summary": "We propose and analyze a deterministic sampling framework using Score-Based\nTransport Modeling (SBTM) for sampling an unnormalized target density $\\pi$.\nWhile diffusion generative modeling relies on pre-training the score function\n$\\nabla \\log f_t$ using samples from $\\pi$, SBTM addresses the more general and\nchallenging setting where only $\\nabla \\log\\pi$ is known. SBTM approximates the\nWasserstein gradient flow on KL$(f_t\\|\\pi)$ by learning the time-varying score\n$\\nabla \\log f_t$ on the fly using score matching. The learned score gives\nimmediate access to relative Fisher information, providing a built-in\nconvergence diagnostic. The deterministic trajectories are smooth,\ninterpretable, and free of Brownian-motion noise, while having the same\ndistribution as ULA. We prove that SBTM dissipates relative entropy at the same\nrate as the exact gradient flow, provided sufficient training. We further\nextend our framework to annealed dynamics, to handle non log-concave targets.\nNumerical experiments validate our theoretical findings: SBTM converges at the\noptimal rate, has smooth trajectories, and is easily integrated with annealed\ndynamics. We compare to the baselines of ULA and annealed ULA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8eScore-Based Transport Modeling\uff08SBTM\uff09\u7684\u786e\u5b9a\u6027\u91c7\u6837\u6846\u67b6\uff0c\u7528\u4e8e\u91c7\u6837\u975e\u5f52\u4e00\u5316\u76ee\u6807\u5bc6\u5ea6\u03c0\uff0c\u89e3\u51b3\u4e86\u4ec5\u77e5\u2207log\u03c0\u7684\u6311\u6218\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u751f\u6210\u6a21\u578b\u4f9d\u8d56\u9884\u8bad\u7ec3\u5f97\u5206\u51fd\u6570\u2207log f_t\uff0c\u800cSBTM\u9488\u5bf9\u66f4\u4e00\u822c\u4e14\u6311\u6218\u6027\u7684\u60c5\u666f\uff0c\u5373\u4ec5\u77e5\u2207log\u03c0\uff0c\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u5b66\u4e60\u5f97\u5206\u51fd\u6570\u4f18\u5316KL\u6563\u5ea6\u3002", "method": "SBTM\u901a\u8fc7\u5206\u6570\u5339\u914d\u52a8\u6001\u5b66\u4e60\u65f6\u53d8\u5f97\u5206\u51fd\u6570\u2207log f_t\uff0c\u8fd1\u4f3cWasserstein\u68af\u5ea6\u6d41\uff0c\u63d0\u4f9b\u6536\u655b\u8bca\u65ad\uff0c\u8f68\u8ff9\u5e73\u6ed1\u4e14\u65e0\u5e03\u6717\u566a\u58f0\u3002", "result": "\u7406\u8bba\u8bc1\u660eSBTM\u80fd\u4ee5\u4e0e\u7cbe\u786e\u68af\u5ea6\u6d41\u76f8\u540c\u7684\u901f\u7387\u8017\u6563\u76f8\u5bf9\u71b5\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6700\u4f18\u6536\u655b\u901f\u7387\u53ca\u4e0e\u9000\u706b\u52a8\u529b\u5b66\u7684\u517c\u5bb9\u6027\u3002", "conclusion": "SBTM\u5728\u4ec5\u77e5\u2207log\u03c0\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u8f68\u8ff9\u5e73\u6ed1\u4e14\u6613\u4e8e\u6574\u5408\u9000\u706b\u52a8\u529b\u5b66\uff0c\u4f18\u4e8eULA\u53ca\u5176\u9000\u706b\u7248\u672c\u3002"}}
{"id": "2504.17826", "pdf": "https://arxiv.org/pdf/2504.17826", "abs": "https://arxiv.org/abs/2504.17826", "authors": ["Kaicheng Pang", "Xingxing Zou", "Waikeung Wong"], "title": "FashionM3: Multimodal, Multitask, and Multiround Fashion Assistant based on Unified Vision-Language Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Fashion styling and personalized recommendations are pivotal in modern\nretail, contributing substantial economic value in the fashion industry. With\nthe advent of vision-language models (VLM), new opportunities have emerged to\nenhance retailing through natural language and visual interactions. This work\nproposes FashionM3, a multimodal, multitask, and multiround fashion assistant,\nbuilt upon a VLM fine-tuned for fashion-specific tasks. It helps users discover\nsatisfying outfits by offering multiple capabilities including personalized\nrecommendation, alternative suggestion, product image generation, and virtual\ntry-on simulation. Fine-tuned on the novel FashionRec dataset, comprising\n331,124 multimodal dialogue samples across basic, personalized, and alternative\nrecommendation tasks, FashionM3 delivers contextually personalized suggestions\nwith iterative refinement through multiround interactions. Quantitative and\nqualitative evaluations, alongside user studies, demonstrate FashionM3's\nsuperior performance in recommendation effectiveness and practical value as a\nfashion assistant.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86FashionM3\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6784\u5efa\u7684\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u3001\u591a\u8f6e\u65f6\u5c1a\u52a9\u624b\uff0c\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u548c\u89c6\u89c9\u4ea4\u4e92\u63d0\u5347\u65f6\u5c1a\u96f6\u552e\u4f53\u9a8c\u3002", "motivation": "\u73b0\u4ee3\u96f6\u552e\u4e2d\uff0c\u65f6\u5c1a\u642d\u914d\u548c\u4e2a\u6027\u5316\u63a8\u8350\u5bf9\u65f6\u5c1a\u4ea7\u4e1a\u7ecf\u6d4e\u4ef7\u503c\u8d21\u732e\u5de8\u5927\uff0c\u800c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\u4e3a\u63d0\u5347\u96f6\u552e\u4f53\u9a8c\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86FashionM3\uff0c\u57fa\u4e8e\u4e00\u4e2a\u4e13\u95e8\u4e3a\u65f6\u5c1a\u4efb\u52a1\u5fae\u8c03\u7684VLM\uff0c\u5177\u5907\u4e2a\u6027\u5316\u63a8\u8350\u3001\u66ff\u4ee3\u5efa\u8bae\u3001\u4ea7\u54c1\u56fe\u50cf\u751f\u6210\u548c\u865a\u62df\u8bd5\u7a7f\u6a21\u62df\u7b49\u529f\u80fd\u3002", "result": "\u5728FashionRec\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u540e\uff0cFashionM3\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u4e2a\u6027\u5316\u7684\u63a8\u8350\u3002\u5b9a\u91cf\u3001\u5b9a\u6027\u8bc4\u4f30\u53ca\u7528\u6237\u7814\u7a76\u8868\u660e\u5176\u5728\u63a8\u8350\u6548\u679c\u548c\u5b9e\u7528\u4ef7\u503c\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "FashionM3\u4f5c\u4e3a\u4e00\u4e2a\u65f6\u5c1a\u52a9\u624b\uff0c\u5728\u63a8\u8350\u6709\u6548\u6027\u548c\u7528\u6237\u4f53\u9a8c\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u65f6\u5c1a\u96f6\u552e\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.18246", "pdf": "https://arxiv.org/pdf/2504.18246", "abs": "https://arxiv.org/abs/2504.18246", "authors": ["Ritesh Goru", "Shanay Mehta", "Prateek Jain"], "title": "Efficient Single-Pass Training for Multi-Turn Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 3 figures", "summary": "Training Large Language Models ( LLMs) to generate explicit reasoning before\nthey produce an answer has been shown to improve their performance across\nvarious tasks such as mathematics and coding. However, fine-tuning LLMs on\nmulti-turn reasoning datasets presents a unique challenge: LLMs must generate\nreasoning tokens that are excluded from subsequent inputs to the LLM. This\ndiscrepancy prevents us from processing an entire conversation in a single\nforward pass-an optimization readily available when we fine-tune on a\nmulti-turn non-reasoning dataset. This paper proposes a novel approach that\novercomes this limitation through response token duplication and a custom\nattention mask that enforces appropriate visibility constraints. Our approach\nsignificantly reduces the training time and allows efficient fine-tuning on\nmulti-turn reasoning datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u54cd\u5e94\u4ee4\u724c\u590d\u5236\u548c\u81ea\u5b9a\u4e49\u6ce8\u610f\u529b\u63a9\u7801\uff0c\u89e3\u51b3\u4e86\u5728\u591a\u8f6e\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u65e0\u6cd5\u5355\u6b21\u524d\u5411\u5904\u7406\u6574\u4e2a\u5bf9\u8bdd\u7684\u96be\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u5f53\u524d\u5728\u591a\u8f6e\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u63a8\u7406\u6807\u8bb0\u540e\u65e0\u6cd5\u5c06\u5176\u7eb3\u5165\u540e\u7eed\u8f93\u5165\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5355\u6b21\u5904\u7406\u6574\u4e2a\u5bf9\u8bdd\uff0c\u9650\u5236\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u91c7\u7528\u54cd\u5e94\u4ee4\u724c\u590d\u5236\u548c\u81ea\u5b9a\u4e49\u6ce8\u610f\u529b\u63a9\u7801\u6280\u672f\uff0c\u786e\u4fdd\u63a8\u7406\u6807\u8bb0\u5728\u540e\u7eed\u8f93\u5165\u4e2d\u53ef\u89c1\uff0c\u4ece\u800c\u652f\u6301\u5355\u6b21\u524d\u5411\u5904\u7406\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u65f6\u95f4\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u8f6e\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u7684\u9ad8\u6548\u5fae\u8c03\u3002", "conclusion": "\u901a\u8fc7\u6280\u672f\u521b\u65b0\u89e3\u51b3\u4e86\u591a\u8f6e\u63a8\u7406\u6570\u636e\u96c6\u5fae\u8c03\u7684\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2504.18133", "pdf": "https://arxiv.org/pdf/2504.18133", "abs": "https://arxiv.org/abs/2504.18133", "authors": ["Gissel Velarde", "Michael Weichert", "Anuj Deshmunkh", "Sanjay Deshmane", "Anindya Sudhir", "Khushboo Sharma", "Vaibhav Joshi"], "title": "Tree Boosting Methods for Balanced andImbalanced Classification and their Robustness Over Time in Risk Assessment", "categories": ["cs.LG"], "comment": "14 pages. arXiv admin note: text overlap with arXiv:2303.15218", "summary": "Most real-world classification problems deal with imbalanced datasets, posing\na challenge for Artificial Intelligence (AI), i.e., machine learning\nalgorithms, because the minority class, which is of extreme interest, often\nproves difficult to be detected. This paper empirically evaluates tree boosting\nmethods' performance given different dataset sizes and class distributions,\nfrom perfectly balanced to highly imbalanced. For tabular data, tree-based\nmethods such as XGBoost, stand out in several benchmarks due to detection\nperformance and speed. Therefore, XGBoost and Imbalance-XGBoost are evaluated.\nAfter introducing the motivation to address risk assessment with machine\nlearning, the paper reviews evaluation metrics for detection systems or binary\nclassifiers. It proposes a method for data preparation followed by tree\nboosting methods including hyper-parameter optimization. The method is\nevaluated on private datasets of 1 thousand (K), 10K and 100K samples on\ndistributions with 50, 45, 25, and 5 percent positive samples. As expected, the\ndeveloped method increases its recognition performance as more data is given\nfor training and the F1 score decreases as the data distribution becomes more\nimbalanced, but it is still significantly superior to the baseline of\nprecision-recall determined by the ratio of positives divided by positives and\nnegatives. Sampling to balance the training set does not provide consistent\nimprovement and deteriorates detection. In contrast, classifier hyper-parameter\noptimization improves recognition, but should be applied carefully depending on\ndata volume and distribution. Finally, the developed method is robust to data\nvariation over time up to some point. Retraining can be used when performance\nstarts deteriorating.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e2d\u6811\u63d0\u5347\u65b9\u6cd5\uff08\u5982XGBoost\u548cImbalance-XGBoost\uff09\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u51c6\u5907\u548c\u8d85\u53c2\u6570\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u89c4\u6a21\u548c\u5206\u5e03\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u5728\u98ce\u9669\u8bc4\u4f30\u4e2d\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5c11\u6570\u7c7b\u96be\u4ee5\u68c0\u6d4b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6570\u636e\u51c6\u5907\u6d41\u7a0b\uff0c\u7ed3\u5408\u6811\u63d0\u5347\u65b9\u6cd5\u548c\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u8bc4\u4f30\u4e0d\u540c\u6570\u636e\u96c6\u89c4\u6a21\u548c\u5206\u5e03\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u968f\u7740\u6570\u636e\u91cf\u589e\u52a0\uff0c\u8bc6\u522b\u6027\u80fd\u63d0\u5347\uff1b\u6570\u636e\u4e0d\u5e73\u8861\u52a0\u5267\u65f6F1\u5206\u6570\u4e0b\u964d\uff0c\u4f46\u4ecd\u4f18\u4e8e\u57fa\u7ebf\u3002\u8d85\u53c2\u6570\u4f18\u5316\u6709\u6548\uff0c\u4f46\u9700\u8c28\u614e\u5e94\u7528\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5bf9\u6570\u636e\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u91cd\u8bad\u7ec3\u53ef\u5e94\u5bf9\u6027\u80fd\u4e0b\u964d\u3002\u5e73\u8861\u91c7\u6837\u6548\u679c\u4e0d\u4e00\u81f4\uff0c\u53ef\u80fd\u6076\u5316\u68c0\u6d4b\u3002"}}
{"id": "2504.17827", "pdf": "https://arxiv.org/pdf/2504.17827", "abs": "https://arxiv.org/abs/2504.17827", "authors": ["Bingye Zhou", "Caiyang Yu"], "title": "Evolution Meets Diffusion: Efficient Neural Architecture Generation", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": null, "summary": "Neural Architecture Search (NAS) has gained widespread attention for its\ntransformative potential in deep learning model design. However, the vast and\ncomplex search space of NAS leads to significant computational and time costs.\nNeural Architecture Generation (NAG) addresses this by reframing NAS as a\ngeneration problem, enabling the precise generation of optimal architectures\nfor specific tasks. Despite its promise, mainstream methods like diffusion\nmodels face limitations in global search capabilities and are still hindered by\nhigh computational and time demands. To overcome these challenges, we propose\nEvolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel\napproach that achieves efficient and training-free architecture generation.\nEDNAG leverages evolutionary algorithms to simulate the denoising process in\ndiffusion models, using fitness to guide the transition from random Gaussian\ndistributions to optimal architecture distributions. This approach combines the\nstrengths of evolutionary strategies and diffusion models, enabling rapid and\neffective architecture generation. Extensive experiments demonstrate that EDNAG\nachieves state-of-the-art (SOTA) performance in architecture optimization, with\nan improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need\nfor time-consuming training and boosts inference speed by an average of 50\ntimes, showcasing its exceptional efficiency and effectiveness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEDNAG\u65b9\u6cd5\uff0c\u7ed3\u5408\u8fdb\u5316\u7b97\u6cd5\u548c\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\u548c\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3NAS\u56e0\u641c\u7d22\u7a7a\u95f4\u5927\u5bfc\u81f4\u7684\u8ba1\u7b97\u548c\u65f6\u95f4\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u5728\u5168\u5c40\u641c\u7d22\u80fd\u529b\u548c\u6548\u7387\u4e0a\u7684\u5c40\u9650\u3002", "method": "EDNAG\u5229\u7528\u8fdb\u5316\u7b97\u6cd5\u6a21\u62df\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u8fc7\u7a0b\uff0c\u901a\u8fc7\u9002\u5e94\u5ea6\u6307\u5bfc\u4ece\u968f\u673a\u9ad8\u65af\u5206\u5e03\u5230\u6700\u4f18\u67b6\u6784\u5206\u5e03\u7684\u8f6c\u6362\u3002", "result": "EDNAG\u5728\u67b6\u6784\u4f18\u5316\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u7cbe\u5ea6\u63d0\u5347\u9ad8\u8fbe10.45%\uff0c\u63a8\u7406\u901f\u5ea6\u5e73\u5747\u63d0\u534750\u500d\u3002", "conclusion": "EDNAG\u5c55\u793a\u4e86\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u67b6\u6784\u751f\u6210\u6f5c\u529b\uff0c\u6709\u671b\u63a8\u52a8NAS\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.18260", "pdf": "https://arxiv.org/pdf/2504.18260", "abs": "https://arxiv.org/abs/2504.18260", "authors": ["Guanqun Bi", "Zhuang Chen", "Zhoufu Liu", "Hongkai Wang", "Xiyao Xiao", "Yuqiang Xie", "Wen Zhang", "Yongkang Huang", "Yuxuan Chen", "Libiao Peng", "Yi Feng", "Minlie Huang"], "title": "MAGI: Multi-Agent Guided Interview for Psychiatric Assessment", "categories": ["cs.CL"], "comment": "In progress", "summary": "Automating structured clinical interviews could revolutionize mental\nhealthcare accessibility, yet existing large language models (LLMs) approaches\nfail to align with psychiatric diagnostic protocols. We present MAGI, the first\nframework that transforms the gold-standard Mini International Neuropsychiatric\nInterview (MINI) into automatic computational workflows through coordinated\nmulti-agent collaboration. MAGI dynamically navigates clinical logic via four\nspecialized agents: 1) an interview tree guided navigation agent adhering to\nthe MINI's branching structure, 2) an adaptive question agent blending\ndiagnostic probing, explaining, and empathy, 3) a judgment agent validating\nwhether the response from participants meet the node, and 4) a diagnosis Agent\ngenerating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map\nsymptoms to clinical criteria. Experimental results on 1,002 real-world\nparticipants covering depression, generalized anxiety, social anxiety and\nsuicide shows that MAGI advances LLM- assisted mental health assessment by\ncombining clinical rigor, conversational adaptability, and explainable\nreasoning.", "AI": {"tldr": "MAGI\u662f\u4e00\u4e2a\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5c06MINI\u8f6c\u5316\u4e3a\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e34\u5e8a\u4e25\u8c28\u6027\u3001\u5bf9\u8bdd\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u63a8\u7406\uff0c\u63d0\u5347LLM\u8f85\u52a9\u7684\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u672a\u80fd\u7b26\u5408\u7cbe\u795e\u79d1\u8bca\u65ad\u534f\u8bae\uff0c\u81ea\u52a8\u5316\u7ed3\u6784\u5316\u4e34\u5e8a\u8bbf\u8c08\u53ef\u63d0\u9ad8\u5fc3\u7406\u5065\u5eb7\u670d\u52a1\u7684\u53ef\u53ca\u6027\u3002", "method": "MAGI\u901a\u8fc7\u56db\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u534f\u4f5c\uff1a\u5bfc\u822a\u4ee3\u7406\u3001\u81ea\u9002\u5e94\u95ee\u9898\u4ee3\u7406\u3001\u5224\u65ad\u4ee3\u7406\u548c\u8bca\u65ad\u4ee3\u7406\uff0c\u52a8\u6001\u5bfc\u822a\u4e34\u5e8a\u903b\u8f91\u5e76\u751f\u6210\u660e\u786e\u7684\u75c7\u72b6\u5230\u4e34\u5e8a\u6807\u51c6\u7684\u6620\u5c04\u3002", "result": "\u57281002\u540d\u771f\u5b9e\u53c2\u4e0e\u8005\u4e2d\u6d4b\u8bd5\uff0c\u6db5\u76d6\u6291\u90c1\u3001\u5e7f\u6cdb\u6027\u7126\u8651\u3001\u793e\u4ea4\u7126\u8651\u548c\u81ea\u6740\u503e\u5411\uff0c\u8bc1\u660e\u4e86MAGI\u7684\u6709\u6548\u6027\u3002", "conclusion": "MAGI\u6846\u67b6\u5728\u4e34\u5e8a\u4e25\u8c28\u6027\u3001\u5bf9\u8bdd\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u63a8\u7406\u65b9\u9762\u63a8\u52a8\u4e86LLM\u8f85\u52a9\u7684\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u53d1\u5c55\u3002"}}
{"id": "2504.18148", "pdf": "https://arxiv.org/pdf/2504.18148", "abs": "https://arxiv.org/abs/2504.18148", "authors": ["Xiaofan Wei", "Binyan Zhang"], "title": "A Generative Graph Contrastive Learning Model with Global Signal", "categories": ["cs.LG"], "comment": null, "summary": "Graph contrastive learning (GCL) has garnered significant attention recently\nsince it learns complex structural information from graphs through\nself-supervised learning manner. However, prevalent GCL models may suffer from\nperformance degradation due to inappropriate contrastive signals. Concretely,\nthey commonly generate augmented views based on random perturbation, which\nleads to biased essential structures due to the introduction of noise. In\naddition, they assign equal weight to both hard and easy sample pairs, thereby\nignoring the difference in importance of the sample pairs. To address these\nissues, this study proposes a novel Contrastive Signal Generative Framework for\nAccurate Graph Learning (CSG2L) with the following two-fold ideas: a) building\na singular value decomposition (SVD)-directed augmented module (SVD-aug) to\nobtain the global interactions as well as avoiding the random noise\nperturbation; b) designing a local-global dependency learning module (LGDL)\nwith an adaptive reweighting strategy which can differentiate the effects of\nhard and easy sample pairs. Extensive experiments on benchmark datasets\ndemonstrate that the proposed CSG2L outperforms the state-of-art baselines.\nMoreover, CSG2L is compatible with a variety of GNNs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6bd4\u4fe1\u53f7\u751f\u6210\u6846\u67b6\uff08CSG2L\uff09\uff0c\u901a\u8fc7SVD\u589e\u5f3a\u6a21\u5757\u548c\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u7b56\u7565\u89e3\u51b3\u73b0\u6709\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u566a\u58f0\u5e72\u6270\u548c\u6837\u672c\u5bf9\u91cd\u8981\u6027\u4e0d\u5747\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\u56e0\u968f\u673a\u6270\u52a8\u751f\u6210\u7684\u589e\u5f3a\u89c6\u56fe\u5f15\u5165\u4e86\u566a\u58f0\uff0c\u4e14\u672a\u533a\u5206\u6837\u672c\u5bf9\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faCSG2L\u6846\u67b6\uff0c\u5305\u62ecSVD\u589e\u5f3a\u6a21\u5757\uff08\u907f\u514d\u566a\u58f0\uff09\u548c\u5c40\u90e8-\u5168\u5c40\u4f9d\u8d56\u5b66\u4e60\u6a21\u5757\uff08\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u6837\u672c\u5bf9\uff09\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cCSG2L\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u517c\u5bb9\u591a\u79cd\u56fe\u795e\u7ecf\u7f51\u7edc\u3002", "conclusion": "CSG2L\u901a\u8fc7\u6539\u8fdb\u589e\u5f3a\u7b56\u7565\u548c\u6837\u672c\u5bf9\u6743\u91cd\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u5bf9\u6bd4\u5b66\u4e60\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.17828", "pdf": "https://arxiv.org/pdf/2504.17828", "abs": "https://arxiv.org/abs/2504.17828", "authors": ["Bozheng Li", "Yongliang Wu", "Yi Lu", "Jiashuo Yu", "Licheng Tang", "Jiawang Cao", "Wenqing Zhu", "Yuyang Sun", "Jay Wu", "Wenbo Zhu"], "title": "VEU-Bench: Towards Comprehensive Understanding of Video Editing", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR2025", "summary": "Widely shared videos on the internet are often edited. Recently, although\nVideo Large Language Models (Vid-LLMs) have made great progress in general\nvideo understanding tasks, their capabilities in video editing understanding\n(VEU) tasks remain unexplored. To address this gap, in this paper, we introduce\nVEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark\nthat categorizes video editing components across various dimensions, from\nintra-frame features like shot size to inter-shot attributes such as cut types\nand transitions. Unlike previous video editing understanding benchmarks that\nfocus mainly on editing element classification, VEU-Bench encompasses 19\nfine-grained tasks across three stages: recognition, reasoning, and judging. To\nenhance the annotation of VEU automatically, we built an annotation pipeline\nintegrated with an ontology-based knowledge base. Through extensive experiments\nwith 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs\nface significant challenges in VEU tasks, with some performing worse than\nrandom choice. To alleviate this issue, we develop Oscars, a VEU expert model\nfine-tuned on the curated VEU-Bench dataset. It outperforms existing\nopen-source Vid-LLMs on VEU-Bench by over 28.3% in accuracy and achieves\nperformance comparable to commercial models like GPT-4o. We also demonstrate\nthat incorporating VEU data significantly enhances the performance of Vid-LLMs\non general video understanding benchmarks, with an average improvement of 8.3%\nacross nine reasoning tasks.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86VEU-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u7f16\u8f91\u7406\u89e3\uff08VEU\uff09\u4efb\u52a1\u7684\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5f00\u53d1\u4e86\u4e13\u7528\u6a21\u578bOscars\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08Vid-LLMs\uff09\u5728\u89c6\u9891\u7f16\u8f91\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u63d0\u51faVEU-Bench\uff0c\u5305\u542b19\u4e2a\u7ec6\u7c92\u5ea6\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86\u96c6\u6210\u672c\u4f53\u77e5\u8bc6\u5e93\u7684\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\uff1b\u901a\u8fc7\u5fae\u8c03\u6784\u5efa\u4e86\u4e13\u7528\u6a21\u578bOscars\u3002", "result": "\u73b0\u6709Vid-LLMs\u5728VEU\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0cOscars\u6a21\u578b\u5728VEU-Bench\u4e0a\u51c6\u786e\u7387\u63d0\u534728.3%\uff0c\u4e14VEU\u6570\u636e\u8fd8\u80fd\u63d0\u5347\u901a\u7528\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "VEU-Bench\u548cOscars\u6a21\u578b\u586b\u8865\u4e86Vid-LLMs\u5728\u89c6\u9891\u7f16\u8f91\u7406\u89e3\u9886\u57df\u7684\u7a7a\u767d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u8868\u73b0\u548c\u901a\u7528\u80fd\u529b\u3002"}}
{"id": "2504.18269", "pdf": "https://arxiv.org/pdf/2504.18269", "abs": "https://arxiv.org/abs/2504.18269", "authors": ["Shintaro Ozaki", "Kazuki Hayashi", "Yusuke Sakai", "Jingun Kwon", "Hidetaka Kamigaito", "Katsuhiko Hayashi", "Manabu Okumura", "Taro Watanabe"], "title": "TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation", "categories": ["cs.CL", "cs.CV"], "comment": "Under review", "summary": "Generating images from prompts containing specific entities requires models\nto retain as much entity-specific knowledge as possible. However, fully\nmemorizing such knowledge is impractical due to the vast number of entities and\ntheir continuous emergence. To address this, we propose Text-based Intelligent\nGeneration with Entity prompt Refinement (TextTIGER), which augments knowledge\non entities included in the prompts and then summarizes the augmented\ndescriptions using Large Language Models (LLMs) to mitigate performance\ndegradation from longer inputs. To evaluate our method, we introduce WiT-Cub\n(WiT with Captions and Uncomplicated Background-explanations), a dataset\ncomprising captions, images, and an entity list. Experiments on four image\ngeneration models and five LLMs show that TextTIGER improves image generation\nperformance in standard metrics (IS, FID, and CLIPScore) compared to\ncaption-only prompts. Additionally, multiple annotators' evaluation confirms\nthat the summarized descriptions are more informative, validating LLMs' ability\nto generate concise yet rich descriptions. These findings demonstrate that\nrefining prompts with augmented and summarized entity-related descriptions\nenhances image generation capabilities. The code and dataset will be available\nupon acceptance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTextTIGER\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u548c\u603b\u7ed3\u5b9e\u4f53\u76f8\u5173\u7684\u63cf\u8ff0\u6765\u4f18\u5316\u56fe\u50cf\u751f\u6210\u63d0\u793a\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u63d0\u5347\u56fe\u50cf\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u63d0\u793a\u4e2d\u5305\u542b\u7684\u5b9e\u4f53\u77e5\u8bc6\u96be\u4ee5\u5b8c\u5168\u8bb0\u5fc6\u7684\u95ee\u9898\uff0c\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u589e\u5f3a\u5b9e\u4f53\u77e5\u8bc6\u5e76\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u603b\u7ed3\u63cf\u8ff0\u6765\u4f18\u5316\u63d0\u793a\u3002", "method": "TextTIGER\u65b9\u6cd5\u9996\u5148\u589e\u5f3a\u63d0\u793a\u4e2d\u7684\u5b9e\u4f53\u77e5\u8bc6\uff0c\u7136\u540e\u5229\u7528LLMs\u603b\u7ed3\u6269\u5c55\u7684\u63cf\u8ff0\u4ee5\u51cf\u5c11\u957f\u8f93\u5165\u7684\u6027\u80fd\u4e0b\u964d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTextTIGER\u5728\u591a\u79cd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u548cLLMs\u4e0a\u5747\u4f18\u4e8e\u4ec5\u4f7f\u7528\u6807\u9898\u63d0\u793a\u7684\u6027\u80fd\uff0c\u4e14\u5728\u6807\u51c6\u8bc4\u4f30\u6307\u6807\uff08IS\u3001FID\u3001CLIPScore\uff09\u4e0a\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u901a\u8fc7\u589e\u5f3a\u548c\u603b\u7ed3\u5b9e\u4f53\u76f8\u5173\u63cf\u8ff0\u6765\u4f18\u5316\u63d0\u793a\u80fd\u6709\u6548\u63d0\u5347\u56fe\u50cf\u751f\u6210\u80fd\u529b\uff0cLLMs\u751f\u6210\u7b80\u6d01\u4f46\u4fe1\u606f\u4e30\u5bcc\u7684\u63cf\u8ff0\u4e5f\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2504.18160", "pdf": "https://arxiv.org/pdf/2504.18160", "abs": "https://arxiv.org/abs/2504.18160", "authors": ["Mathieu Petitbois", "R\u00e9my Portelas", "Sylvain Lamprier", "Ludovic Denoyer"], "title": "Offline Learning of Controllable Diverse Behaviors", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "Generative Models for Robot Learning Workshop at ICLR 2025", "summary": "Imitation Learning (IL) techniques aim to replicate human behaviors in\nspecific tasks. While IL has gained prominence due to its effectiveness and\nefficiency, traditional methods often focus on datasets collected from experts\nto produce a single efficient policy. Recently, extensions have been proposed\nto handle datasets of diverse behaviors by mainly focusing on learning\ntransition-level diverse policies or on performing entropy maximization at the\ntrajectory level. While these methods may lead to diverse behaviors, they may\nnot be sufficient to reproduce the actual diversity of demonstrations or to\nallow controlled trajectory generation. To overcome these drawbacks, we propose\na different method based on two key features: a) Temporal Consistency that\nensures consistent behaviors across entire episodes and not just at the\ntransition level as well as b) Controllability obtained by constructing a\nlatent space of behaviors that allows users to selectively activate specific\nbehaviors based on their requirements. We compare our approach to\nstate-of-the-art methods over a diverse set of tasks and environments. Project\npage: https://mathieu-petitbois.github.io/projects/swr/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\uff0c\u4ee5\u66f4\u597d\u5730\u91cd\u73b0\u591a\u6837\u7684\u884c\u4e3a\u6f14\u793a\u5e76\u652f\u6301\u53ef\u63a7\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4ec5\u4ece\u4e13\u5bb6\u6570\u636e\u4e2d\u5b66\u4e60\u5355\u4e00\u7b56\u7565\uff0c\u800c\u8fd1\u671f\u6269\u5c55\u65b9\u6cd5\u867d\u5c1d\u8bd5\u5904\u7406\u591a\u6837\u884c\u4e3a\u6570\u636e\uff0c\u4f46\u65e0\u6cd5\u5145\u5206\u91cd\u73b0\u884c\u4e3a\u591a\u6837\u6027\u6216\u652f\u6301\u53ef\u63a7\u751f\u6210\u3002", "method": "\u8be5\u65b9\u6cd5\u5f3a\u8c03\u4e24\u4e2a\u5173\u952e\u7279\u6027\uff1a\u65f6\u95f4\u4e00\u81f4\u6027\uff08\u786e\u4fdd\u6574\u4e2a\u884c\u4e3a\u5e8f\u5217\u7684\u4e00\u81f4\u6027\u800c\u975e\u4ec5\u5355\u6b65\uff09\u548c\u53ef\u63a7\u6027\uff08\u901a\u8fc7\u6784\u5efa\u884c\u4e3a\u6f5c\u5728\u7a7a\u95f4\u5b9e\u73b0\u9009\u62e9\u6027\u6fc0\u6d3b\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u548c\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u91cd\u73b0\u591a\u6837\u5316\u884c\u4e3a\u5e76\u652f\u6301\u53ef\u63a7\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2504.17829", "pdf": "https://arxiv.org/pdf/2504.17829", "abs": "https://arxiv.org/abs/2504.17829", "authors": ["Vlad Vasilescu", "Ana Neacsu", "Daniela Faur"], "title": "Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Single-image dehazing is an important topic in remote sensing applications,\nenhancing the quality of acquired images and increasing object detection\nprecision. However, the reliability of such structures has not been\nsufficiently analyzed, which poses them to the risk of imperceptible\nperturbations that can significantly hinder their performance. In this work, we\nshow that state-of-the-art image-to-image dehazing transformers are susceptible\nto adversarial noise, with even 1 pixel change being able to decrease the PSNR\nby as much as 2.8 dB. Next, we propose two lightweight fine-tuning strategies\naimed at increasing the robustness of pre-trained transformers. Our methods\nresults in comparable clean performance, while significantly increasing the\nprotection against adversarial data. We further present their applicability in\ntwo remote sensing scenarios, showcasing their robust behavior for\nout-of-distribution data. The source code for adversarial fine-tuning and\nattack algorithms can be found at github.com/Vladimirescu/RobustDehazing.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5355\u56fe\u50cf\u53bb\u96fe\u4efb\u52a1\u4e2d\u5bf9\u6297\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u8f7b\u91cf\u7ea7\u5fae\u8c03\u7b56\u7565\u4ee5\u589e\u5f3a\u9884\u8bad\u7ec3Transformer\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5355\u56fe\u50cf\u53bb\u96fe\u5728\u9065\u611f\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5bf9\u5bf9\u6297\u566a\u58f0\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u8fd9\u6fc0\u53d1\u4e86\u7814\u7a76\u8005\u63d0\u51fa\u65b0\u65b9\u6cd5\u6765\u63d0\u5347\u6a21\u578b\u7684\u6297\u5e72\u6270\u80fd\u529b\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u73b0\u6709\u53bb\u96feTransformer\u5bf9\u5bf9\u6297\u566a\u58f0\u7684\u654f\u611f\u6027\uff0c\u968f\u540e\u63d0\u51fa\u4e86\u4e24\u79cd\u8f7b\u91cf\u7ea7\u5fae\u8c03\u7b56\u7565\u6765\u589e\u5f3a\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u5e72\u51c0\u6570\u636e\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u566a\u58f0\u7684\u9632\u5fa1\u80fd\u529b\uff0c\u5e76\u5728\u9065\u611f\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u5fae\u8c03\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u5355\u56fe\u50cf\u53bb\u96fe\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18346", "pdf": "https://arxiv.org/pdf/2504.18346", "abs": "https://arxiv.org/abs/2504.18346", "authors": ["Toghrul Abbasli", "Kentaroh Toyoda", "Yuan Wang", "Leon Witt", "Muhammad Asif Ali", "Yukai Miao", "Dan Li", "Qingsong Wei"], "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been transformative across many domains.\nHowever, hallucination -- confidently outputting incorrect information --\nremains one of the leading challenges for LLMs. This raises the question of how\nto accurately assess and quantify the uncertainty of LLMs. Extensive literature\non traditional models has explored Uncertainty Quantification (UQ) to measure\nuncertainty and employed calibration techniques to address the misalignment\nbetween uncertainty and accuracy. While some of these methods have been adapted\nfor LLMs, the literature lacks an in-depth analysis of their effectiveness and\ndoes not offer a comprehensive benchmark to enable insightful comparison among\nexisting solutions. In this work, we fill this gap via a systematic survey of\nrepresentative prior works on UQ and calibration for LLMs and introduce a\nrigorous benchmark. Using two widely used reliability datasets, we empirically\nevaluate six related methods, which justify the significant findings of our\nreview. Finally, we provide outlooks for key future directions and outline open\nchallenges. To the best of our knowledge, this survey is the first dedicated\nstudy to review the calibration methods and relevant metrics for LLMs.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5730\u7efc\u8ff0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u548c\u6821\u51c6\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u516d\u79cd\u76f8\u5173\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u6587\u732e\u7a7a\u767d\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u51fa\u73b0\u5e7b\u89c9\uff08\u8f93\u51fa\u4e0d\u6b63\u786e\u4f46\u81ea\u4fe1\u7684\u4fe1\u606f\uff09\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u800c\u73b0\u6709\u6587\u732e\u7f3a\u4e4f\u5bf9\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u6821\u51c6\u65b9\u6cd5\u7684\u7cfb\u7edf\u5206\u6790\u548c\u6709\u6548\u6027\u8bc4\u4f30\u3002", "method": "\u672c\u6587\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e24\u4e2a\u5e7f\u6cdb\u5e94\u7528\u7684\u53ef\u9760\u6027\u6570\u636e\u96c6\uff0c\u5bf9\u516d\u79cd\u76f8\u5173\u65b9\u6cd5\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u6821\u51c6\u65b9\u6cd5\u5728LLMs\u4e0a\u6548\u679c\u4e0d\u4e00\uff0c\u5e76\u63d0\u51fa\u4e86\u5173\u952e\u53d1\u73b0\u548c\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u3002", "conclusion": "\u6587\u7ae0\u603b\u7ed3\u4e86\u8be5\u9886\u57df\u7684\u5173\u952e\u672a\u6765\u65b9\u5411\uff0c\u5e76\u63d0\u51fa\u4e86\u5f00\u653e\u6311\u6218\uff0c\u586b\u8865\u4e86LLMs\u6821\u51c6\u65b9\u6cd5\u548c\u76f8\u5173\u6307\u6807\u7efc\u8ff0\u7684\u7a7a\u767d\u3002"}}
{"id": "2504.18181", "pdf": "https://arxiv.org/pdf/2504.18181", "abs": "https://arxiv.org/abs/2504.18181", "authors": ["Yvonne Jenniges", "Maike Sonnewald", "Sebastian Maneth", "Are Olsen", "Boris P. Koch"], "title": "Unveiling 3D Ocean Biogeochemical Provinces: A Machine Learning Approach for Systematic Clustering and Validation", "categories": ["cs.LG"], "comment": "Submitted to Ecological Informatics. Images in this preprint are of\n  lower resolution than in the journal submission", "summary": "Defining ocean regions and water masses helps to understand marine processes\nand can serve downstream-tasks such as defining marine protected areas.\nHowever, such definitions are often a result of subjective decisions\npotentially producing misleading, unreproducible results. Here, the aim was to\nobjectively define regions of the North Atlantic. For this, a data-driven,\nsystematic machine learning approach was applied to generate and validate ocean\nclusters employing external, internal and relative validation techniques. About\n300 million measured salinity, temperature, and oxygen, nitrate, phosphate and\nsilicate concentration values served as input for various clustering methods\n(KMeans, agglomerative Ward, and Density-Based Spatial Clustering of\nApplications with Noise (DBSCAN)). Uniform Manifold Approximation and\nProjection (UMAP) emphasised (dis-)similarities in the data while reducing\ndimensionality. Based on a systematic validation of the considered clustering\nmethods and their hyperparameters, the results showed that UMAP-DBSCAN best\nrepresented the data. To address stochastic variability, 100 UMAP-DBSCAN\nclustering runs were conducted and aggregated using Native Emergent Manifold\nInterrogation (NEMI), producing a final set of 321 clusters. Reproducibility\nwas evaluated by calculating the ensemble overlap (88.81 +- 1.8%) and the mean\ngrid cell-wise uncertainty estimated by NEMI (15.49 +- 20%). The presented\nclustering results agreed very well with common water mass definitions. This\nstudy revealed a more detailed regionalization compared to previous concepts\nsuch as the Longhurst provinces. The applied method is objective, efficient and\nreproducible and will support future research focusing on biogeochemical\ndifferences and changes in oceanic regions.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u5982KMeans\u3001Ward\u3001DBSCAN\u548cUMAP\uff09\u5bf9\u5317\u5927\u897f\u6d0b\u6d77\u57df\u8fdb\u884c\u5ba2\u89c2\u5206\u533a\uff0c\u9a8c\u8bc1\u4e86UMAP-DBSCAN\u65b9\u6cd5\u7684\u6700\u4f73\u8868\u73b0\uff0c\u5e76\u5229\u7528NEMI\u6280\u672f\u751f\u6210321\u4e2a\u7a33\u5b9a\u805a\u7c7b\u3002\u7ed3\u679c\u4e0e\u73b0\u6709\u6c34\u56e2\u5b9a\u4e49\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e14\u6bd4\u4f20\u7edf\u5206\u533a\uff08\u5982Longhurst provinces\uff09\u66f4\u8be6\u7ec6\uff0c\u4e3a\u672a\u6765\u6d77\u6d0b\u751f\u7269\u5730\u7403\u5316\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u91cd\u590d\u7684\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6d77\u6d0b\u5206\u533a\u5e38\u4f9d\u8d56\u4e3b\u89c2\u51b3\u7b56\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bef\u5bfc\u6027\u6216\u4e0d\u53ef\u590d\u73b0\u7684\u7ed3\u679c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5ba2\u89c2\u5b9a\u4e49\u5317\u5927\u897f\u6d0b\u6d77\u57df\u7684\u5206\u533a\uff0c\u4ee5\u652f\u6301\u6d77\u6d0b\u4fdd\u62a4\u548c\u751f\u7269\u5730\u7403\u5316\u5b66\u7814\u7a76\u3002", "method": "\u91c7\u7528\u591a\u79cd\u805a\u7c7b\u65b9\u6cd5\uff08KMeans\u3001Ward\u3001DBSCAN\uff09\u7ed3\u5408UMAP\u964d\u7ef4\u6280\u672f\uff0c\u7cfb\u7edf\u9a8c\u8bc1\u540e\u9009\u62e9UMAP-DBSCAN\uff0c\u5e76\u5229\u7528NEMI\u5bf9100\u6b21\u805a\u7c7b\u7ed3\u679c\u8fdb\u884c\u805a\u5408\uff0c\u6700\u7ec8\u751f\u6210321\u4e2a\u805a\u7c7b\u3002", "result": "UMAP-DBSCAN\u8868\u73b0\u6700\u4f73\uff0c\u805a\u7c7b\u7ed3\u679c\u4e0e\u73b0\u6709\u6c34\u56e2\u5b9a\u4e49\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e14\u5206\u533a\u6bd4\u4f20\u7edfLonghurst provinces\u66f4\u8be6\u7ec6\u3002NEMI\u8ba1\u7b97\u7684\u805a\u7c7b\u7a33\u5b9a\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u5206\u522b\u4e3a88.81\u00b11.8%\u548c15.49\u00b120%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5ba2\u89c2\u3001\u9ad8\u6548\u4e14\u53ef\u91cd\u590d\uff0c\u4e3a\u6d77\u6d0b\u533a\u57df\u5212\u5206\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u5e94\u7528\u4e8e\u751f\u7269\u5730\u7403\u5316\u5b66\u5dee\u5f02\u548c\u53d8\u5316\u7684\u7814\u7a76\u3002"}}
{"id": "2504.17833", "pdf": "https://arxiv.org/pdf/2504.17833", "abs": "https://arxiv.org/abs/2504.17833", "authors": ["Xiao Huang", "Zhengzhong Tu", "Xinyue Ye", "Michael Goodchild"], "title": "The Role of Open-Source LLMs in Shaping the Future of GeoAI", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are transforming geospatial artificial\nintelligence (GeoAI), offering new capabilities in data processing, spatial\nanalysis, and decision support. This paper examines the open-source paradigm's\npivotal role in this transformation. While proprietary LLMs offer\naccessibility, they often limit the customization, interoperability, and\ntransparency vital for specialized geospatial tasks. Conversely, open-source\nalternatives significantly advance Geographic Information Science (GIScience)\nby fostering greater adaptability, reproducibility, and community-driven\ninnovation. Open frameworks empower researchers to tailor solutions, integrate\ncutting-edge methodologies (e.g., reinforcement learning, advanced spatial\nindexing), and align with FAIR principles. However, the growing reliance on any\nLLM necessitates careful consideration of security vulnerabilities, ethical\nrisks, and robust governance for AI-generated geospatial outputs. Ongoing\ndebates on accessibility, regulation, and misuse underscore the critical need\nfor responsible AI development strategies. This paper argues that GIScience\nadvances best not through a single model type, but by cultivating a diverse,\ninteroperable ecosystem combining open-source foundations for innovation,\nbespoke geospatial models, and interdisciplinary collaboration. By critically\nevaluating the opportunities and challenges of open-source LLMs within the\nbroader GeoAI landscape, this work contributes to a nuanced discourse on\nleveraging AI to effectively advance spatial research, policy, and\ndecision-making in an equitable, sustainable, and scientifically rigorous\nmanner.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5730\u7406\u7a7a\u95f4\u4eba\u5de5\u667a\u80fd\uff08GeoAI\uff09\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5f3a\u8c03\u4e86\u5176\u76f8\u8f83\u4e8e\u4e13\u6709\u6a21\u578b\u7684\u4f18\u52bf\uff08\u5982\u5b9a\u5236\u5316\u3001\u900f\u660e\u5ea6\u548c\u793e\u533a\u521b\u65b0\uff09\uff0c\u4f46\u4e5f\u63d0\u51fa\u4e86\u5b89\u5168\u3001\u4f26\u7406\u548c\u6cbb\u7406\u7684\u6311\u6218\u3002", "motivation": "\u4e13\u6709\u7684LLMs\u867d\u7136\u6613\u4e8e\u4f7f\u7528\uff0c\u4f46\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u900f\u660e\u5ea6\uff0c\u800c\u5f00\u6e90LLMs\u80fd\u591f\u4fc3\u8fdb\u5730\u7406\u4fe1\u606f\u79d1\u5b66\uff08GIScience\uff09\u7684\u9002\u5e94\u6027\u548c\u521b\u65b0\u3002\u7814\u7a76\u65e8\u5728\u5206\u6790\u5f00\u6e90LLMs\u7684\u4f18\u52bf\u4e0e\u6311\u6218\uff0c\u63a8\u52a8GeoAI\u7684\u516c\u5e73\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u6279\u5224\u6027\u8bc4\u4f30\u5f00\u6e90LLMs\u5728\u5730\u7406\u7a7a\u95f4AI\u9886\u57df\u7684\u5e94\u7528\uff0c\u8ba8\u8bba\u4e86\u5b9a\u5236\u5316\u3001\u5f00\u6e90\u6846\u67b6\u96c6\u6210\uff08\u5982\u5f3a\u5316\u5b66\u4e60\u548c\u7a7a\u95f4\u7d22\u5f15\uff09\u4ee5\u53caFAIR\u539f\u5219\u7684\u4f5c\u7528\u3002", "result": "\u5f00\u6e90LLMs\u80fd\u591f\u63a8\u52a8GIScience\u7684\u8fdb\u6b65\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u5b89\u5168\u3001\u4f26\u7406\u548c\u6cbb\u7406\u95ee\u9898\u3002\u6700\u4f73\u8def\u5f84\u662f\u6784\u5efa\u591a\u6837\u5316\u7684\u751f\u6001\u7cfb\u7edf\uff0c\u7ed3\u5408\u5f00\u6e90\u521b\u65b0\u548c\u8de8\u5b66\u79d1\u5408\u4f5c\u3002", "conclusion": "\u5f00\u6e90LLMs\u4e3aGeoAI\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u521b\u65b0\u57fa\u7840\uff0c\u4f46\u5176\u53d1\u5c55\u9700\u517c\u987e\u5b89\u5168\u6027\u548c\u4f26\u7406\u95ee\u9898\u3002\u672a\u6765\u7684\u91cd\u70b9\u5e94\u653e\u5728\u6784\u5efa\u591a\u6837\u5316\u3001\u53ef\u4e92\u64cd\u4f5c\u7684\u751f\u6001\u7cfb\u7edf\u4e0a\u3002"}}
{"id": "2504.18373", "pdf": "https://arxiv.org/pdf/2504.18373", "abs": "https://arxiv.org/abs/2504.18373", "authors": ["Lei Shen", "Xiaoyu Shen"], "title": "Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant", "categories": ["cs.CL"], "comment": null, "summary": "In recent years, multi-agent frameworks powered by large language models\n(LLMs) have advanced rapidly. Despite this progress, there is still a notable\nabsence of benchmark datasets specifically tailored to evaluate their\nperformance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset\naimed at evaluating LLM-based multi-agent frameworks in the context of\nintelligent personal assistants. Auto-SLURP extends the original SLURP dataset\n-- initially developed for natural language understanding tasks -- by\nrelabeling the data and integrating simulated servers and external services.\nThis enhancement enables a comprehensive end-to-end evaluation pipeline,\ncovering language understanding, task execution, and response generation. Our\nexperiments demonstrate that Auto-SLURP presents a significant challenge for\ncurrent state-of-the-art frameworks, highlighting that truly reliable and\nintelligent multi-agent personal assistants remain a work in progress. The\ndataset and related code are available at\nhttps://github.com/lorashen/Auto-SLURP/.", "AI": {"tldr": "Auto-SLURP \u662f\u4e00\u4e2a\u9488\u5bf9\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6269\u5c55\u4e86\u539f\u6709\u7684 SLURP \u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u4e2a\u4eba\u52a9\u624b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6027\u80fd\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u56e0\u6b64\u5f00\u53d1 Auto-SLURP \u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u6807\u6ce8\u6570\u636e\u5e76\u96c6\u6210\u6a21\u62df\u670d\u52a1\u5668\u548c\u5916\u90e8\u670d\u52a1\uff0c\u6269\u5c55 SLURP \u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u7aef\u5230\u7aef\u7684\u8bc4\u4f30\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e Auto-SLURP \u5bf9\u73b0\u6709\u6700\u5148\u8fdb\u6846\u67b6\u63d0\u51fa\u4e86\u663e\u8457\u6311\u6218\uff0c\u663e\u793a\u53ef\u9760\u7684\u591a\u667a\u80fd\u4f53\u4e2a\u4eba\u52a9\u624b\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "Auto-SLURP \u4e3a\u591a\u667a\u80fd\u4f53\u6846\u67b6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u6807\u51c6\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.18185", "pdf": "https://arxiv.org/pdf/2504.18185", "abs": "https://arxiv.org/abs/2504.18185", "authors": ["Gissel Velarde", "Pedro Branez", "Alejandro Bueno", "Rodrigo Heredia", "Mateo Lopez-Ledezma"], "title": "An Open-Source and Reproducible Implementation of LSTM and GRU Networks for Time Series Forecasting", "categories": ["cs.LG"], "comment": "12 pages", "summary": "This paper introduces an open-source and reproducible implementation of Long\nShort-Term Memory (LSTM) and Gated Recurrent Unit (GRU) Networks for time\nseries forecasting. We evaluated LSTM and GRU networks because of their\nperformance reported in related work. We describe our method and its results on\ntwo datasets. The first dataset is the S&P BSE BANKEX, composed of stock time\nseries (closing prices) of ten financial institutions. The second dataset,\ncalled Activities, comprises ten synthetic time series resembling weekly\nactivities with five days of high activity and two days of low activity. We\nreport Root Mean Squared Error (RMSE) between actual and predicted values, as\nwell as Directional Accuracy (DA). We show that a single time series from a\ndataset can be used to adequately train the networks if the sequences in the\ndataset contain patterns that repeat, even with certain variation, and are\nproperly processed. For 1-step ahead and 20-step ahead forecasts, LSTM and GRU\nnetworks significantly outperform a baseline on the Activities dataset. The\nbaseline simply repeats the last available value. On the stock market dataset,\nthe networks perform just like the baseline, possibly due to the nature of\nthese series. We release the datasets used as well as the implementation with\nall experiments performed to enable future comparisons and to make our research\nreproducible.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86LSTM\u548cGRU\u7f51\u7edc\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5f00\u6e90\u53ef\u590d\u73b0\u5b9e\u73b0\uff0c\u8bc4\u4f30\u4e86\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u5728\u5177\u6709\u91cd\u590d\u6a21\u5f0f\u7684\u5408\u6210\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u80a1\u7968\u6570\u636e\u4e0a\u4e0e\u57fa\u7ebf\u76f8\u8fd1\u3002", "motivation": "\u7814\u7a76LSTM\u548cGRU\u7f51\u7edc\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u9a8c\u8bc1\u5176\u5728\u5177\u6709\u91cd\u590d\u6a21\u5f0f\u7684\u6570\u636e\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a8\u5e7f\u5f00\u6e90\u548c\u53ef\u590d\u73b0\u7684\u7814\u7a76\u5b9e\u8df5\u3002", "method": "\u5728S&P BSE BANKEX\u80a1\u7968\u6570\u636e\u96c6\u548c\u5408\u6210Activities\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5LSTM\u548cGRU\u7f51\u7edc\uff0c\u4f7f\u7528RMSE\u548cDA\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728Activities\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4f46\u5728\u80a1\u7968\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0e\u57fa\u7ebf\u76f8\u8fd1\u3002", "conclusion": "LSTM\u548cGRU\u5728\u5177\u6709\u91cd\u590d\u6a21\u5f0f\u7684\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u53d7\u6570\u636e\u7279\u6027\u5f71\u54cd\u8f83\u5927\uff0c\u7814\u7a76\u63d0\u4f9b\u4e86\u5f00\u6e90\u5b9e\u73b0\u548c\u6570\u636e\u4ee5\u4fc3\u8fdb\u672a\u6765\u6bd4\u8f83\u3002"}}
{"id": "2504.18376", "pdf": "https://arxiv.org/pdf/2504.18376", "abs": "https://arxiv.org/abs/2504.18376", "authors": ["Pablo Miralles-Gonz\u00e1lez", "Javier Huertas-Tato", "Alejandro Mart\u00edn", "David Camacho"], "title": "Pushing the boundary on Natural Language Inference", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural Language Inference (NLI) is a central task in natural language\nunderstanding with applications in fact-checking, question answering, and\ninformation retrieval. Despite its importance, current NLI systems heavily rely\non supervised learning with datasets that often contain annotation artifacts\nand biases, limiting generalization and real-world applicability. In this work,\nwe apply a reinforcement learning-based approach using Group Relative Policy\nOptimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the\nneed for labeled rationales and enabling this type of training on more\nchallenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language\nmodels using parameter-efficient techniques (LoRA and QLoRA), demonstrating\nstrong performance across standard and adversarial NLI benchmarks. Our 32B\nAWQ-quantized model surpasses state-of-the-art results on 7 out of 11\nadversarial sets$\\unicode{x2013}$or on all of them considering our\nreplication$\\unicode{x2013}$within a 22GB memory footprint, showing that robust\nreasoning can be retained under aggressive quantization. This work provides a\nscalable and practical framework for building robust NLI systems without\nsacrificing inference quality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u65b9\u6cd5\uff0c\u4f7f\u7528GRPO\u8fdb\u884cChain-of-Thought\u5b66\u4e60\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\uff0c\u5e76\u5728ANLI\u7b49\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\u3002\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u6280\u672f\u5fae\u8c03\u5927\u6a21\u578b\uff0c32B\u91cf\u5316\u6a21\u578b\u5728\u5bf9\u6297\u6027NLI\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u5f53\u524dNLI\u7cfb\u7edf\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\uff0c\u6570\u636e\u96c6\u5b58\u5728\u6807\u6ce8\u504f\u5dee\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "\u4f7f\u7528Group Relative Policy Optimization (GRPO)\u8fdb\u884cChain-of-Thought (CoT)\u5b66\u4e60\uff0c\u5e76\u7ed3\u5408LoRA\u548cQLoRA\u7b49\u53c2\u6570\u9ad8\u6548\u6280\u672f\uff0c\u5fae\u8c037B\u300114B\u548c32B\u5927\u6a21\u578b\u3002", "result": "32B AWQ\u91cf\u5316\u6a21\u578b\u572811\u4e2a\u5bf9\u6297\u6027NLI\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u7ed3\u679c\uff0c\u5185\u5b58\u5360\u7528\u4ec522GB\uff0c\u8868\u660e\u5728\u6fc0\u8fdb\u91cf\u5316\u4e0b\u4ecd\u80fd\u4fdd\u6301\u5f3a\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u65e0\u9700\u727a\u7272\u63a8\u7406\u8d28\u91cf\u7684\u9ad8\u6548NLI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u6846\u67b6\u3002"}}
{"id": "2504.18206", "pdf": "https://arxiv.org/pdf/2504.18206", "abs": "https://arxiv.org/abs/2504.18206", "authors": ["Stefano Sossi-Rojas", "Gissel Velarde", "Damian Zieba"], "title": "A Machine Learning Approach For Bitcoin Forecasting", "categories": ["cs.LG", "cs.CE"], "comment": "15 pages", "summary": "Bitcoin is one of the cryptocurrencies that is gaining more popularity in\nrecent years. Previous studies have shown that closing price alone is not\nenough to forecast stock market series. We introduce a new set of time series\nand demonstrate that a subset is necessary to improve directional accuracy\nbased on a machine learning ensemble. In our experiments, we study which time\nseries and machine learning algorithms deliver the best results. We found that\nthe most relevant time series that contribute to improving directional accuracy\nare Open, High and Low, with the largest contribution of Low in combination\nwith an ensemble of Gated Recurrent Unit network and a baseline forecast. The\nrelevance of other Bitcoin-related features that are not price-related is\nnegligible. The proposed method delivers similar performance to the\nstate-of-the-art when observing directional accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u5b50\u96c6\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u96c6\u6210\u65b9\u6cd5\u63d0\u9ad8\u6bd4\u7279\u5e01\u4ef7\u683c\u65b9\u5411\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u8868\u660eOpen\u3001High\u548cLow\u4ef7\u683c\u5e8f\u5217\u5bf9\u63d0\u5347\u51c6\u786e\u6027\u6700\u91cd\u8981\uff0c\u5176\u4e2dLow\u8d21\u732e\u6700\u5927\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u73b0\u6709\u7814\u7a76\u8868\u660e\u4ec5\u7528\u6536\u76d8\u4ef7\u4e0d\u8db3\u4ee5\u51c6\u786e\u9884\u6d4b\u80a1\u7968\u5e02\u573a\u5e8f\u5217\u65b9\u5411\uff0c\u56e0\u6b64\u5f15\u5165\u65b0\u65f6\u95f4\u5e8f\u5217\u5b50\u96c6\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u8bc4\u4f30\u4e0d\u540c\u65f6\u95f4\u5e8f\u5217\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u7ec4\u5408\uff0c\u6700\u7ec8\u91c7\u7528GRU\u7f51\u7edc\u548c\u57fa\u51c6\u9884\u6d4b\u7684\u96c6\u6210\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aOpen\u3001High\u548cLow\u4ef7\u683c\u5e8f\u5217\u5bf9\u65b9\u5411\u51c6\u786e\u6027\u63d0\u5347\u6700\u6709\u6548\uff0c\u5c24\u5176\u662fLow\uff0c\u800c\u5176\u4ed6\u975e\u4ef7\u683c\u76f8\u5173\u7279\u5f81\u5f71\u54cd\u53ef\u5ffd\u7565\u3002", "conclusion": "\u7ed3\u8bba\u662f\u6240\u63d0\u65b9\u6cd5\u5728\u65b9\u5411\u51c6\u786e\u6027\u4e0a\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\uff0c\u9a8c\u8bc1\u4e86\u65f6\u95f4\u5e8f\u5217\u5b50\u96c6\u548c\u96c6\u6210\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.17872", "pdf": "https://arxiv.org/pdf/2504.17872", "abs": "https://arxiv.org/abs/2504.17872", "authors": ["Max Muchen Sun", "Allison Pinosky", "Todd Murphey"], "title": "Flow Matching Ergodic Coverage", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "15 pages, 15 figures. Accepted to Robotics: Science and Systems (RSS)\n  2025. Project website: https://murpheylab.github.io/lqr-flow-matching/", "summary": "Ergodic coverage effectively generates exploratory behaviors for embodied\nagents by aligning the spatial distribution of the agent's trajectory with a\ntarget distribution, where the difference between these two distributions is\nmeasured by the ergodic metric. However, existing ergodic coverage methods are\nconstrained by the limited set of ergodic metrics available for control\nsynthesis, fundamentally limiting their performance. In this work, we propose\nan alternative approach to ergodic coverage based on flow matching, a technique\nwidely used in generative inference for efficient and scalable sampling. We\nformally derive the flow matching problem for ergodic coverage and show that it\nis equivalent to a linear quadratic regulator problem with a closed-form\nsolution. Our formulation enables alternative ergodic metrics from generative\ninference that overcome the limitations of existing ones. These metrics were\npreviously infeasible for control synthesis but can now be supported with no\ncomputational overhead. Specifically, flow matching with the Stein variational\ngradient flow enables control synthesis directly over the score function of the\ntarget distribution, improving robustness to the unnormalized distributions; on\nthe other hand, flow matching with the Sinkhorn divergence flow enables an\noptimal transport-based ergodic metric, improving coverage performance on\nnon-smooth distributions with irregular supports. We validate the improved\nperformance and competitive computational efficiency of our method through\ncomprehensive numerical benchmarks and across different nonlinear dynamics. We\nfurther demonstrate the practicality of our method through a series of drawing\nand erasing tasks on a Franka robot.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u904d\u5386\u8986\u76d6\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u904d\u5386\u5ea6\u91cf\u9650\u5236\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u904d\u5386\u8986\u76d6\u65b9\u6cd5\u53d7\u9650\u4e8e\u53ef\u7528\u7684\u904d\u5386\u5ea6\u91cf\uff0c\u9650\u5236\u4e86\u6027\u80fd\u8868\u73b0\u3002", "method": "\u91c7\u7528\u6d41\u5339\u914d\u6280\u672f\uff0c\u63a8\u5bfc\u81f4\u904d\u5386\u8986\u76d6\u7684\u6d41\u5339\u914d\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u5176\u7b49\u4ef7\u4e8e\u4e00\u4e2a\u95ed\u73af\u89e3\u7684\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u95ee\u9898\u3002", "result": "\u65b0\u65b9\u6cd5\u652f\u6301\u4e86\u4ee5\u524d\u4e0d\u53ef\u884c\u7684\u5ea6\u91cf\uff0c\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u548c\u8986\u76d6\u6027\u80fd\uff0c\u5e76\u5728\u6570\u503c\u5b9e\u9a8c\u548c\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u904d\u5386\u8986\u76d6\u65b9\u6cd5\u514b\u670d\u4e86\u73b0\u6709\u5ea6\u91cf\u7684\u9650\u5236\uff0c\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.18386", "pdf": "https://arxiv.org/pdf/2504.18386", "abs": "https://arxiv.org/abs/2504.18386", "authors": ["Amir Zeldes", "Nina Speransky", "Nicholas Wagner", "Caroline T. Schroeder"], "title": "A UD Treebank for Bohairic Coptic", "categories": ["cs.CL"], "comment": null, "summary": "Despite recent advances in digital resources for other Coptic dialects,\nespecially Sahidic, Bohairic Coptic, the main Coptic dialect for pre-Mamluk,\nlate Byzantine Egypt, and the contemporary language of the Coptic Church,\nremains critically under-resourced. This paper presents and evaluates the first\nsyntactically annotated corpus of Bohairic Coptic, sampling data from a range\nof works, including Biblical text, saints' lives and Christian ascetic writing.\nWe also explore some of the main differences we observe compared to the\nexisting UD treebank of Sahidic Coptic, the classical dialect of the language,\nand conduct joint and cross-dialect parsing experiments, revealing the unique\nnature of Bohairic as a related, but distinct variety from the more often\nstudied Sahidic.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2aBohairic Coptic\uff08\u6ce2\u6d77\u5229\u514b\u79d1\u666e\u7279\u8bed\uff09\u53e5\u6cd5\u6807\u6ce8\u8bed\u6599\u5e93\uff0c\u5e76\u6bd4\u8f83\u4e86\u5176\u4e0eSahidic Coptic\uff08\u8428\u5e0c\u8fea\u514b\u79d1\u666e\u7279\u8bed\uff09\u7684\u4e3b\u8981\u5dee\u5f02\u3002", "motivation": "\u7531\u4e8e\u6ce2\u6d77\u5229\u514b\u79d1\u666e\u7279\u8bed\u4f5c\u4e3a\u62dc\u5360\u5ead\u665a\u671f\u57c3\u53ca\u548c\u73b0\u4ee3\u79d1\u666e\u7279\u6559\u4f1a\u7684\u4e3b\u8981\u65b9\u8a00\uff0c\u5728\u6570\u5b57\u8d44\u6e90\u65b9\u9762\u4e25\u91cd\u4e0d\u8db3\uff0c\u56e0\u6b64\u7814\u7a76\u8005\u5e0c\u671b\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u6ce2\u6d77\u5229\u514b\u79d1\u666e\u7279\u8bed\u7684\u53e5\u6cd5\u6807\u6ce8\u8bed\u6599\u5e93\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u8bc4\u4f30\uff0c\u540c\u65f6\u5bf9\u6bd4\u5206\u6790\u5176\u4e0e\u8428\u5e0c\u8fea\u514b\u79d1\u666e\u7279\u8bed\u7684\u5dee\u5f02\uff0c\u8fdb\u884c\u8de8\u65b9\u8a00\u89e3\u6790\u5b9e\u9a8c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6ce2\u6d77\u5229\u514b\u79d1\u666e\u7279\u8bed\u4e0e\u8428\u5e0c\u8fea\u514b\u79d1\u666e\u7279\u8bed\u76f8\u5173\u4f46\u5177\u6709\u72ec\u7279\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6ce2\u6d77\u5229\u514b\u79d1\u666e\u7279\u8bed\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u4f5c\u4e3a\u72ec\u7acb\u65b9\u8a00\u7684\u7279\u70b9\u3002"}}
{"id": "2504.18207", "pdf": "https://arxiv.org/pdf/2504.18207", "abs": "https://arxiv.org/abs/2504.18207", "authors": ["Simon Lucey"], "title": "Gradient Descent as a Shrinkage Operator for Spectral Bias", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We generalize the connection between activation function and spline\nregression/smoothing and characterize how this choice may influence spectral\nbias within a 1D shallow network. We then demonstrate how gradient descent (GD)\ncan be reinterpreted as a shrinkage operator that masks the singular values of\na neural network's Jacobian. Viewed this way, GD implicitly selects the number\nof frequency components to retain, thereby controlling the spectral bias. An\nexplicit relationship is proposed between the choice of GD hyperparameters\n(learning rate & number of iterations) and bandwidth (the number of active\ncomponents). GD regularization is shown to be effective only with monotonic\nactivation functions. Finally, we highlight the utility of non-monotonic\nactivation functions (sinc, Gaussian) as iteration-efficient surrogates for\nspectral bias.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6fc0\u6d3b\u51fd\u6570\u4e0e\u8c31\u504f\u5dee\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u68af\u5ea6\u4e0b\u964d\u4f5c\u4e3a\u6536\u7f29\u7b97\u5b50\u7684\u65b0\u89c6\u89d2\uff0c\u5e76\u63a2\u8ba8\u4e86\u975e\u5355\u8c03\u6fc0\u6d3b\u51fd\u6570\u5728\u8c31\u504f\u5dee\u4e2d\u7684\u9ad8\u6548\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u6fc0\u6d3b\u51fd\u6570\u5982\u4f55\u5f71\u54cd\u795e\u7ecf\u7f51\u7edc\u7684\u8c31\u504f\u5dee\uff0c\u4ee5\u53ca\u68af\u5ea6\u4e0b\u964d\u5982\u4f55\u901a\u8fc7\u9690\u5f0f\u9009\u62e9\u9891\u7387\u5206\u91cf\u6765\u63a7\u5236\u8fd9\u4e00\u504f\u5dee\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5c06\u68af\u5ea6\u4e0b\u964d\u91cd\u65b0\u89e3\u91ca\u4e3a\u6536\u7f29\u7b97\u5b50\uff0c\u5206\u6790\u5176\u5982\u4f55\u63a9\u7801\u795e\u7ecf\u7f51\u7edc\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u5947\u5f02\u503c\uff0c\u5e76\u63a2\u8ba8\u8d85\u53c2\u6570\u4e0e\u5e26\u5bbd\u7684\u5173\u7cfb\u3002", "result": "\u7ed3\u679c\u8868\u660e\u68af\u5ea6\u4e0b\u964d\u6b63\u5219\u5316\u4ec5\u5bf9\u5355\u8c03\u6fc0\u6d3b\u51fd\u6570\u6709\u6548\uff0c\u800c\u975e\u5355\u8c03\u6fc0\u6d3b\u51fd\u6570\uff08\u5982sinc\u3001\u9ad8\u65af\uff09\u53ef\u4f5c\u4e3a\u8c31\u504f\u5dee\u7684\u9ad8\u6548\u66ff\u4ee3\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u4e86\u6fc0\u6d3b\u51fd\u6570\u9009\u62e9\u5bf9\u8c31\u504f\u5dee\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u975e\u5355\u8c03\u6fc0\u6d3b\u51fd\u6570\u5728\u8fed\u4ee3\u6548\u7387\u4e0a\u7684\u4f18\u52bf\u3002"}}
{"id": "2504.17878", "pdf": "https://arxiv.org/pdf/2504.17878", "abs": "https://arxiv.org/abs/2504.17878", "authors": ["Xu Wang", "Yiquan Wang", "Tin-yeh Huang"], "title": "Crypto-ncRNA: Non-coding RNA (ncRNA) Based Encryption Algorithm", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted at the AI4NA workshop at ICLR 2025. 18pages, 4figures", "summary": "In the looming post-quantum era, traditional cryptographic systems are\nincreasingly vulnerable to quantum computing attacks that can compromise their\nmathematical foundations. To address this critical challenge, we propose\ncrypto-ncRNA-a bio-convergent cryptographic framework that leverages the\ndynamic folding properties of non-coding RNA (ncRNA) to generate high-entropy,\nquantum-resistant keys and produce unpredictable ciphertexts. The framework\nemploys a novel, multi-stage process: encoding plaintext into RNA sequences,\npredicting and manipulating RNA secondary structures using advanced algorithms,\nand deriving cryptographic keys through the intrinsic physical unclonability of\nRNA molecules. Experimental evaluations indicate that, although crypto-ncRNA's\nencryption speed is marginally lower than that of AES, it significantly\noutperforms RSA in terms of efficiency and scalability while achieving a 100%\npass rate on the NIST SP 800-22 randomness tests. These results demonstrate\nthat crypto-ncRNA offers a promising and robust approach for securing digital\ninfrastructures against the evolving threats posed by quantum computing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u7f16\u7801RNA\uff08ncRNA\uff09\u7684\u52a8\u6001\u6298\u53e0\u7279\u6027\u7684\u751f\u7269\u878d\u5408\u52a0\u5bc6\u6846\u67b6crypto-ncRNA\uff0c\u4ee5\u751f\u6210\u9ad8\u71b5\u3001\u6297\u91cf\u5b50\u8ba1\u7b97\u7684\u5bc6\u94a5\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u5bc6\u6587\u3002\u5c3d\u7ba1\u52a0\u5bc6\u901f\u5ea6\u7565\u4f4e\u4e8eAES\uff0c\u4f46\u5176\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u663e\u8457\u4f18\u4e8eRSA\uff0c\u5e76\u80fd\u901a\u8fc7NIST\u968f\u673a\u6027\u6d4b\u8bd5\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8ba1\u7b97\u65f6\u4ee3\u7684\u5230\u6765\uff0c\u4f20\u7edf\u5bc6\u7801\u7cfb\u7edf\u7684\u6570\u5b66\u57fa\u7840\u53ef\u80fd\u88ab\u91cf\u5b50\u8ba1\u7b97\u653b\u51fb\u7834\u574f\uff0c\u4e9f\u9700\u4e00\u79cd\u6297\u91cf\u5b50\u8ba1\u7b97\u7684\u52a0\u5bc6\u65b9\u6848\u3002", "method": "\u6846\u67b6\u91c7\u7528\u591a\u9636\u6bb5\u6d41\u7a0b\uff1a\u5c06\u660e\u6587\u7f16\u7801\u4e3aRNA\u5e8f\u5217\uff0c\u5229\u7528\u5148\u8fdb\u7b97\u6cd5\u9884\u6d4b\u548c\u64cd\u7eb5RNA\u4e8c\u7ea7\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7RNA\u5206\u5b50\u7684\u7269\u7406\u4e0d\u53ef\u514b\u9686\u6027\u751f\u6210\u52a0\u5bc6\u5bc6\u94a5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0ccrypto-ncRNA\u7684\u52a0\u5bc6\u901f\u5ea6\u7565\u4f4e\u4e8eAES\uff0c\u4f46\u5728\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u663e\u8457\u4f18\u4e8eRSA\uff0c\u4e14\u80fd100%\u901a\u8fc7NIST SP 800-22\u968f\u673a\u6027\u6d4b\u8bd5\u3002", "conclusion": "crypto-ncRNA\u4e3a\u62b5\u5fa1\u91cf\u5b50\u8ba1\u7b97\u5a01\u80c1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u4e14\u7a33\u5065\u7684\u52a0\u5bc6\u65b9\u6848\u3002"}}
{"id": "2504.18406", "pdf": "https://arxiv.org/pdf/2504.18406", "abs": "https://arxiv.org/abs/2504.18406", "authors": ["Yusen Zhang", "Wenliang Zheng", "Aashrith Madasu", "Peng Shi", "Ryo Kamoi", "Hao Zhou", "Zhuoyang Zou", "Shu Zhao", "Sarkar Snigdha Sarathi Das", "Vipul Gupta", "Xiaoxin Lu", "Nan Zhang", "Ranran Haoran Zhang", "Avitej Iyer", "Renze Lou", "Wenpeng Yin", "Rui Zhang"], "title": "HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?", "categories": ["cs.CL"], "comment": "22 pages, 8 figures", "summary": "High-resolution image (HRI) understanding aims to process images with a large\nnumber of pixels, such as pathological images and agricultural aerial images,\nboth of which can exceed 1 million pixels. Vision Large Language Models (VLMs)\ncan allegedly handle HRIs, however, there is a lack of a comprehensive\nbenchmark for VLMs to evaluate HRI understanding. To address this gap, we\nintroduce HRScene, a novel unified benchmark for HRI understanding with rich\nscenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic\ndatasets with resolutions ranging from 1,024 $\\times$ 1,024 to 35,503 $\\times$\n26,627. HRScene is collected and re-annotated by 10 graduate-level annotators,\ncovering 25 scenarios, ranging from microscopic to radiology images, street\nviews, long-range pictures, and telescope images. It includes HRIs of\nreal-world objects, scanned documents, and composite multi-image. The two\ndiagnostic evaluation datasets are synthesized by combining the target image\nwith the gold answer and distracting images in different orders, assessing how\nwell models utilize regions in HRI. We conduct extensive experiments involving\n28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show\nthat current VLMs achieve an average accuracy of around 50% on real-world\ntasks, revealing significant gaps in HRI understanding. Results on synthetic\ndatasets reveal that VLMs struggle to effectively utilize HRI regions, showing\nsignificant Regional Divergence and lost-in-middle, shedding light on future\nresearch.", "AI": {"tldr": "\u7814\u7a76\u8005\u9488\u5bf9\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff08HRI\uff09\u7406\u89e3\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30\u57fa\u51c6\u7684\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u57fa\u51c6HRScene\uff0c\u6db5\u76d625\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u548c2\u4e2a\u5408\u6210\u8bca\u65ad\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e8628\u79cdVLM\u6a21\u578b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728HRI\u4efb\u52a1\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u4ec550%\uff0c\u4e14\u5b58\u5728\u533a\u57df\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u53f7\u79f0\u80fd\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u57fa\u51c6\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7814\u7a76\u8005\u5efa\u7acb\u4e86HRScene\u57fa\u51c6\uff0c\u65e8\u5728\u5168\u9762\u8861\u91cfVLM\u5bf9HRI\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86HRScene\u57fa\u51c6\uff0c\u6574\u540825\u4e2a\u771f\u5b9e\u573a\u666f\u6570\u636e\u96c6\u548c2\u4e2a\u5408\u6210\u8bca\u65ad\u6570\u636e\u96c6\uff08\u5206\u8fa8\u7387\u4ece1,024\u00d71,024\u523035,503\u00d726,627\uff09\uff0c\u753110\u540d\u7814\u7a76\u751f\u91cd\u65b0\u6807\u6ce8\u3002\u901a\u8fc7\u7ed3\u5408\u76ee\u6807\u56fe\u50cf\u4e0e\u5e72\u6270\u56fe\u50cf\u5408\u6210\u8bca\u65ad\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u6a21\u578b\u5bf9HRI\u533a\u57df\u7684\u6709\u6548\u5229\u7528\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524dVLMs\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u7ea650%\uff0c\u4e14\u5408\u6210\u6570\u636e\u96c6\u6d4b\u8bd5\u8868\u660e\u6a21\u578b\u5b58\u5728\u2018\u533a\u57df\u53d1\u6563\u2019\u548c\u2018\u4e2d\u95f4\u4e22\u5931\u2019\u95ee\u9898\uff0c\u96be\u4ee5\u6709\u6548\u5229\u7528HRI\u4fe1\u606f\u3002", "conclusion": "HRScene\u63ed\u793a\u4e86VLMs\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7406\u89e3\u4e0a\u7684\u663e\u8457\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u533a\u57df\u5229\u7528\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2504.18208", "pdf": "https://arxiv.org/pdf/2504.18208", "abs": "https://arxiv.org/abs/2504.18208", "authors": ["Rapha\u00ebl Barboni", "Gabriel Peyr\u00e9", "Fran\u00e7ois-Xavier Vialard"], "title": "Ultra-fast feature learning for the training of two-layer neural networks in the two-timescale regime", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "We study the convergence of gradient methods for the training of mean-field\nsingle hidden layer neural networks with square loss. Observing this is a\nseparable non-linear least-square problem which is linear w.r.t. the outer\nlayer's weights, we consider a Variable Projection (VarPro) or two-timescale\nlearning algorithm, thereby eliminating the linear variables and reducing the\nlearning problem to the training of the feature distribution. Whereas most\nconvergence rates or the training of neural networks rely on a neural tangent\nkernel analysis where features are fixed, we show such a strategy enables\nprovable convergence rates for the sampling of a teacher feature distribution.\nPrecisely, in the limit where the regularization strength vanishes, we show\nthat the dynamic of the feature distribution corresponds to a weighted\nultra-fast diffusion equation. Relying on recent results on the asymptotic\nbehavior of such PDEs, we obtain guarantees for the convergence of the trained\nfeature distribution towards the teacher feature distribution in a\nteacher-student setup.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u68af\u5ea6\u65b9\u6cd5\u5728\u5747\u503c\u573a\u5355\u9690\u85cf\u5c42\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684\u6536\u655b\u6027\uff0c\u5229\u7528\u53d8\u91cf\u6295\u5f71\u65b9\u6cd5\u7b80\u5316\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4e86\u7279\u5f81\u5206\u5e03\u8bad\u7ec3\u7684\u6536\u655b\u901f\u7387\u3002", "motivation": "\u63a2\u7d22\u5747\u503c\u573a\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u6536\u655b\u6027\uff0c\u7279\u522b\u662f\u5728\u7279\u5f81\u5206\u5e03\u8bad\u7ec3\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u7b80\u5316\u95ee\u9898\u7ed3\u6784\u6765\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u91c7\u7528\u53d8\u91cf\u6295\u5f71\uff08VarPro\uff09\u6216\u53cc\u65f6\u95f4\u5c3a\u5ea6\u5b66\u4e60\u7b97\u6cd5\uff0c\u6d88\u9664\u7ebf\u6027\u53d8\u91cf\uff0c\u5c06\u5b66\u4e60\u95ee\u9898\u7b80\u5316\u4e3a\u7279\u5f81\u5206\u5e03\u7684\u8bad\u7ec3\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u7279\u5f81\u5206\u5e03\u7684\u52a8\u6001\u5bf9\u5e94\u4e8e\u52a0\u6743\u8d85\u5feb\u901f\u6269\u6563\u65b9\u7a0b\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u6559\u5e08-\u5b66\u751f\u8bbe\u7f6e\u4e0b\uff0c\u7279\u5f81\u5206\u5e03\u5411\u6559\u5e08\u7279\u5f81\u5206\u5e03\u7684\u6536\u655b\u3002", "conclusion": "\u901a\u8fc7\u7b80\u5316\u95ee\u9898\u7ed3\u6784\u548c\u5229\u7528PDE\u5206\u6790\uff0c\u8bba\u6587\u4e3a\u5747\u503c\u573a\u795e\u7ecf\u7f51\u7edc\u7684\u7279\u5f81\u5206\u5e03\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u3002"}}
{"id": "2504.17892", "pdf": "https://arxiv.org/pdf/2504.17892", "abs": "https://arxiv.org/abs/2504.17892", "authors": ["Yasmine Omri", "Parth Shroff", "Thierry Tambe"], "title": "Token Sequence Compression for Efficient Multimodal Computing", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The exponential growth of Large Multimodal Models (LMMs) has driven\nadvancements in cross-modal reasoning but at significant computational costs.\nIn this work, we focus on visual language models. We highlight the redundancy\nand inefficiency in current vision encoders, and seek to construct an adaptive\ncompression method for multimodal data. In this work, we characterize a panoply\nof visual token selection and merging approaches through both benchmarking and\nqualitative analysis. In particular, we demonstrate that simple cluster-level\ntoken aggregation outperforms prior state-of-the-art works in token selection\nand merging, including merging at the vision encoder level and attention-based\napproaches. We underline the redundancy in current vision encoders, and shed\nlight on several puzzling trends regarding principles of visual token selection\nthrough cross-modal attention visualizations. This work is a first effort\ntowards more effective encoding and processing of high-dimensional data, and\npaves the way for more scalable and sustainable multimodal systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u9002\u5e94\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b80\u5316\u7684\u7c07\u7ea7\u6807\u8bb0\u805a\u5408\u4f18\u5316\u89c6\u89c9\u6807\u8bb0\u9009\u62e9\u548c\u5408\u5e76\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u7f16\u7801\u5668\u5b58\u5728\u5197\u4f59\u548c\u4f4e\u6548\u95ee\u9898\uff0c\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u8feb\u5207\u9700\u8981\u66f4\u6709\u6548\u7684\u7f16\u7801\u548c\u5904\u7406\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e86\u89c6\u89c9\u6807\u8bb0\u9009\u62e9\u548c\u5408\u5e76\u7684\u591a\u79cd\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u7b80\u5355\u7684\u7c07\u7ea7\u6807\u8bb0\u805a\u5408\uff0c\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9a\u6027\u5206\u6790\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u7c07\u7ea7\u6807\u8bb0\u805a\u5408\u5728\u89c6\u89c9\u6807\u8bb0\u9009\u62e9\u548c\u5408\u5e76\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u7684\u5197\u4f59\uff0c\u5e76\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u53ef\u89c6\u5316\u89e3\u91ca\u4e86\u89c6\u89c9\u6807\u8bb0\u9009\u62e9\u7684\u4e00\u4e9b\u8d8b\u52bf\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u9ad8\u7ef4\u591a\u6a21\u6001\u6570\u636e\u7684\u66f4\u6709\u6548\u7f16\u7801\u548c\u5904\u7406\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u4e3a\u53ef\u6269\u5c55\u548c\u53ef\u6301\u7eed\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.18412", "pdf": "https://arxiv.org/pdf/2504.18412", "abs": "https://arxiv.org/abs/2504.18412", "authors": ["Jared Moore", "Declan Grabb", "William Agnew", "Kevin Klyman", "Stevie Chancellor", "Desmond C. Ong", "Nick Haber"], "title": "Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers", "categories": ["cs.CL"], "comment": null, "summary": "Should a large language model (LLM) be used as a therapist? In this paper, we\ninvestigate the use of LLMs to *replace* mental health providers, a use case\npromoted in the tech startup and research space. We conduct a mapping review of\ntherapy guides used by major medical institutions to identify crucial aspects\nof therapeutic relationships, such as the importance of a therapeutic alliance\nbetween therapist and client. We then assess the ability of LLMs to reproduce\nand adhere to these aspects of therapeutic relationships by conducting several\nexperiments investigating the responses of current LLMs, such as `gpt-4o`.\nContrary to best practices in the medical community, LLMs 1) express stigma\ntoward those with mental health conditions and 2) respond inappropriately to\ncertain common (and critical) conditions in naturalistic therapy settings --\ne.g., LLMs encourage clients' delusional thinking, likely due to their\nsycophancy. This occurs even with larger and newer LLMs, indicating that\ncurrent safety practices may not address these gaps. Furthermore, we note\nfoundational and practical barriers to the adoption of LLMs as therapists, such\nas that a therapeutic alliance requires human characteristics (e.g., identity\nand stakes). For these reasons, we conclude that LLMs should not replace\ntherapists, and we discuss alternative roles for LLMs in clinical therapy.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u662f\u5426\u80fd\u66ff\u4ee3\u5fc3\u7406\u6cbb\u7597\u5e08\uff0c\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0LLM\u5728\u6cbb\u7597\u5173\u7cfb\u4e2d\u5b58\u5728\u8868\u8fbe\u504f\u89c1\u3001\u5904\u7406\u5173\u952e\u95ee\u9898\u4e0d\u5f53\u7b49\u95ee\u9898\uff0c\u5e76\u5b58\u5728\u65e0\u6cd5\u6ee1\u8db3\u6cbb\u7597\u8054\u76df\u7684\u4eba\u7c7b\u7279\u8d28\u7b49\u969c\u788d\uff0c\u56e0\u6b64\u4e0d\u5efa\u8bae\u66ff\u4ee3\u6cbb\u7597\u5e08\u3002", "motivation": "\u7531\u4e8e\u79d1\u6280\u521b\u4e1a\u548c\u7814\u7a76\u9886\u57df\u63a8\u5e7fLLM\u66ff\u4ee3\u5fc3\u7406\u5065\u5eb7\u670d\u52a1\u63d0\u4f9b\u8005\uff0c\u8bba\u6587\u65e8\u5728\u8bc4\u4f30\u8fd9\u4e00\u5e94\u7528\u7684\u53ef\u884c\u6027\u548c\u6f5c\u5728\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6cbb\u7597\u5173\u7cfb\u7684\u6838\u5fc3\u8981\u7d20\u65b9\u9762\u3002", "method": "\u901a\u8fc7\u6620\u5c04\u4e3b\u6d41\u533b\u7597\u673a\u6784\u7684\u6cbb\u7597\u6307\u5357\uff0c\u8bc6\u522b\u6cbb\u7597\u5173\u7cfb\u7684\u5173\u952e\u8981\u7d20\uff0c\u5e76\u8bbe\u8ba1\u5b9e\u9a8c\u6d4b\u8bd5\u5f53\u524dLLM\uff08\u5982GPT-4\uff09\u5728\u6ee1\u8db3\u8fd9\u4e9b\u8981\u7d20\u65f6\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLM\u5b58\u5728\u5bf9\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u7684\u504f\u89c1\u8868\u8fbe\u3001\u5bf9\u5173\u952e\u6cbb\u7597\u573a\u666f\uff08\u5982\u5984\u60f3\u601d\u7ef4\uff09\u7684\u4e0d\u5f53\u56de\u5e94\uff0c\u4e14\u73b0\u6709\u5b89\u5168\u63aa\u65bd\u65e0\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u6b64\u5916\uff0cLLM\u7f3a\u4e4f\u4eba\u7c7b\u6cbb\u7597\u8054\u76df\u6240\u9700\u7684\u7279\u8d28\uff08\u5982\u8eab\u4efd\u8ba4\u540c\u548c\u5229\u5bb3\u5173\u7cfb\uff09\u3002", "conclusion": "LLM\u76ee\u524d\u4e0d\u5e94\u66ff\u4ee3\u6cbb\u7597\u5e08\uff0c\u8bba\u6587\u8ba8\u8bba\u4e86LLM\u5728\u4e34\u5e8a\u6cbb\u7597\u4e2d\u7684\u66ff\u4ee3\u89d2\u8272\u3002"}}
{"id": "2504.18230", "pdf": "https://arxiv.org/pdf/2504.18230", "abs": "https://arxiv.org/abs/2504.18230", "authors": ["He Shanxuan", "Lin Zuhong", "Yu Bolun", "Gao Xu", "Long Biao", "Yao Jingjing"], "title": "Learning to fuse: dynamic integration of multi-source data for accurate battery lifespan prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate prediction of lithium-ion battery lifespan is vital for ensuring\noperational reliability and reducing maintenance costs in applications like\nelectric vehicles and smart grids. This study presents a hybrid learning\nframework for precise battery lifespan prediction, integrating dynamic\nmulti-source data fusion with a stacked ensemble (SE) modeling approach. By\nleveraging heterogeneous datasets from the National Aeronautics and Space\nAdministration (NASA), Center for Advanced Life Cycle Engineering (CALCE),\nMIT-Stanford-Toyota Research Institute (TRC), and nickel cobalt aluminum (NCA)\nchemistries, an entropy-based dynamic weighting mechanism mitigates variability\nacross heterogeneous datasets. The SE model combines Ridge regression, long\nshort-term memory (LSTM) networks, and eXtreme Gradient Boosting (XGBoost),\neffectively capturing temporal dependencies and nonlinear degradation patterns.\nIt achieves a mean absolute error (MAE) of 0.0058, root mean square error\n(RMSE) of 0.0092, and coefficient of determination (R2) of 0.9839,\noutperforming established baseline models with a 46.2% improvement in R2 and an\n83.2% reduction in RMSE. Shapley additive explanations (SHAP) analysis\nidentifies differential discharge capacity (Qdlin) and temperature of\nmeasurement (Temp_m) as critical aging indicators. This scalable, interpretable\nframework enhances battery health management, supporting optimized maintenance\nand safety across diverse energy storage systems, thereby contributing to\nimproved battery health management in energy storage systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u591a\u6e90\u6570\u636e\u878d\u5408\u548c\u5806\u53e0\u96c6\u6210\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9502\u79bb\u5b50\u7535\u6c60\u5bff\u547d\u7684\u7cbe\u786e\u9884\u6d4b\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u9502\u79bb\u5b50\u7535\u6c60\u5bff\u547d\u5bf9\u4e8e\u786e\u4fdd\u7535\u52a8\u6c7d\u8f66\u548c\u667a\u80fd\u7535\u7f51\u7b49\u5e94\u7528\u7684\u8fd0\u884c\u53ef\u9760\u6027\u53ca\u964d\u4f4e\u7ef4\u62a4\u6210\u672c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u52a8\u6001\u591a\u6e90\u6570\u636e\u878d\u5408\u548c\u5806\u53e0\u96c6\u6210\uff08SE\uff09\u6a21\u578b\uff0c\u7ed3\u5408\u4e86Ridge\u56de\u5f52\u3001LSTM\u7f51\u7edc\u548cXGBoost\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u71b5\u52a8\u6001\u52a0\u6743\u673a\u5236\u5904\u7406\u5f02\u6784\u6570\u636e\u96c6\u3002", "result": "\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0cMAE\u4e3a0.0058\uff0cRMSE\u4e3a0.0092\uff0cR2\u8fbe0.9839\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5728R2\u4e0a\u63d0\u9ad8\u4e8646.2%\uff0cRMSE\u964d\u4f4e\u4e8683.2%\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u5347\u4e86\u7535\u6c60\u5065\u5eb7\u7ba1\u7406\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u652f\u6301\u4f18\u5316\u7ef4\u62a4\u548c\u5b89\u5168\u6027\uff0c\u6709\u52a9\u4e8e\u80fd\u6e90\u5b58\u50a8\u7cfb\u7edf\u7684\u7535\u6c60\u5065\u5eb7\u7ba1\u7406\u3002"}}
{"id": "2504.17901", "pdf": "https://arxiv.org/pdf/2504.17901", "abs": "https://arxiv.org/abs/2504.17901", "authors": ["Benned Hedegaard", "Ziyi Yang", "Yichen Wei", "Ahmed Jaafar", "Stefanie Tellex", "George Konidaris", "Naman Shah"], "title": "Beyond Task and Motion Planning: Hierarchical Robot Planning with General-Purpose Policies", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Task and motion planning is a well-established approach for solving\nlong-horizon robot planning problems. However, traditional methods assume that\neach task-level robot action, or skill, can be reduced to kinematic motion\nplanning. In this work, we address the challenge of planning with both\nkinematic skills and closed-loop motor controllers that go beyond kinematic\nconsiderations. We propose a novel method that integrates these controllers\ninto motion planning using Composable Interaction Primitives (CIPs), enabling\nthe use of diverse, non-composable pre-learned skills in hierarchical robot\nplanning. Toward validating our Task and Skill Planning (TASP) approach, we\ndescribe ongoing robot experiments in real-world scenarios designed to\ndemonstrate how CIPs can allow a mobile manipulator robot to effectively\ncombine motion planning with general-purpose skills to accomplish complex\ntasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTASP\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7Composable Interaction Primitives (CIPs)\u5c06\u8fd0\u52a8\u89c4\u5212\u4e0e\u975e\u7ec4\u5408\u5f0f\u9884\u5b66\u4e60\u6280\u80fd\u6574\u5408\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u4e2d\u4ec5\u8003\u8651\u8fd0\u52a8\u5b66\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u4ec5\u8003\u8651\u8fd0\u52a8\u5b66\u6280\u80fd\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u95ed\u73af\u7535\u673a\u63a7\u5236\u5668\u7b49\u975e\u8fd0\u52a8\u5b66\u6280\u80fd\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u89c4\u5212\u4e2d\u7ed3\u5408\u66f4\u591a\u901a\u7528\u6280\u80fd\u3002", "method": "\u91c7\u7528Composable Interaction Primitives (CIPs)\u65b9\u6cd5\uff0c\u5c06\u95ed\u73af\u7535\u673a\u63a7\u5236\u5668\u7b49\u975e\u8fd0\u52a8\u5b66\u6280\u80fd\u6574\u5408\u5230\u5206\u5c42\u673a\u5668\u4eba\u89c4\u5212\u4e2d\u3002", "result": "\u76ee\u524d\u6b63\u5728\u8fdb\u884c\u7684\u771f\u5b9e\u573a\u666f\u673a\u5668\u4eba\u5b9e\u9a8c\u5c55\u793a\u4e86TASP\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u79fb\u52a8\u64cd\u4f5c\u673a\u5668\u4eba\u53ef\u4ee5\u901a\u8fc7CIPs\u7ed3\u5408\u8fd0\u52a8\u89c4\u5212\u548c\u901a\u7528\u6280\u80fd\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u3002", "conclusion": "TASP\u901a\u8fc7CIPs\u6210\u529f\u6269\u5c55\u4e86\u4f20\u7edf\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u7684\u80fd\u529b\uff0c\u4e3a\u975e\u7ec4\u5408\u5f0f\u6280\u80fd\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.18415", "pdf": "https://arxiv.org/pdf/2504.18415", "abs": "https://arxiv.org/abs/2504.18415", "authors": ["Hongyu Wang", "Shuming Ma", "Furu Wei"], "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs", "categories": ["cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference.", "AI": {"tldr": "BitNet v2\u89e3\u51b3\u4e861\u4f4d\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6fc0\u6d3b\u5f02\u5e38\u95ee\u9898\uff0c\u901a\u8fc7H-BitLinear\u6a21\u5757\u5b9e\u73b0\u4e864\u4f4d\u6fc0\u6d3b\u91cf\u5316\uff0c\u51cf\u5c11\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "1\u4f4d\u5927\u8bed\u8a00\u6a21\u578b\u7531\u4e8e\u6fc0\u6d3b\u5f02\u5e38\u95ee\u9898\u96be\u4ee5\u5b9e\u73b0\u4f4e\u6bd4\u7279\u91cf\u5316\uff0cBitNet v2\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u6548\u7387\u3002", "method": "\u63d0\u51faH-BitLinear\u6a21\u5757\uff0c\u901a\u8fc7\u5728\u7ebf\u54c8\u8fbe\u739b\u53d8\u6362\u5e73\u6ed1\u6fc0\u6d3b\u5206\u5e03\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u4f4e\u6bd4\u7279\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u663e\u793aBitNet v2\u57288\u4f4d\u6fc0\u6d3b\u4e0b\u6027\u80fd\u4e0e\u539f\u6a21\u578b\u76f8\u5f53\uff0c\u800c\u57284\u4f4d\u6fc0\u6d3b\u4e0b\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002", "conclusion": "BitNet v2\u663e\u8457\u63d0\u5347\u4e861\u4f4d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u90e8\u7f72\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2504.18243", "pdf": "https://arxiv.org/pdf/2504.18243", "abs": "https://arxiv.org/abs/2504.18243", "authors": ["Rong Cheng", "Jinyi Liu", "YAN ZHENG", "Fei Ni", "Jiazhen Du", "Hangyu Mao", "Fuzheng Zhang", "Bo Wang", "Jianye HAO"], "title": "DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering", "categories": ["cs.LG"], "comment": null, "summary": "Multi-Hop Question Answering (MHQA) tasks permeate real-world applications,\nposing challenges in orchestrating multi-step reasoning across diverse\nknowledge domains. While existing approaches have been improved with iterative\nretrieval, they still struggle to identify and organize dynamic knowledge. To\naddress this, we propose DualRAG, a synergistic dual-process framework that\nseamlessly integrates reasoning and retrieval. DualRAG operates through two\ntightly coupled processes: Reasoning-augmented Querying (RaQ) and progressive\nKnowledge Aggregation (pKA). They work in concert: as RaQ navigates the\nreasoning path and generates targeted queries, pKA ensures that newly acquired\nknowledge is systematically integrated to support coherent reasoning. This\ncreates a virtuous cycle of knowledge enrichment and reasoning refinement.\nThrough targeted fine-tuning, DualRAG preserves its sophisticated reasoning and\nretrieval capabilities even in smaller-scale models, demonstrating its\nversatility and core advantages across different scales. Extensive experiments\ndemonstrate that this dual-process approach substantially improves answer\naccuracy and coherence, approaching, and in some cases surpassing, the\nperformance achieved with oracle knowledge access. These results establish\nDualRAG as a robust and efficient solution for complex multi-hop reasoning\ntasks.", "AI": {"tldr": "DualRAG\u662f\u4e00\u4e2a\u53cc\u8fc7\u7a0b\u6846\u67b6\uff0c\u7ed3\u5408\u63a8\u7406\u548c\u68c0\u7d22\uff0c\u901a\u8fc7Reasoning-augmented Querying\u548cprogressive Knowledge Aggregation\u63d0\u5347\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u8fde\u8d2f\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u8df3\u95ee\u7b54\u65b9\u6cd5\u5728\u52a8\u6001\u77e5\u8bc6\u7684\u8bc6\u522b\u548c\u7ec4\u7ec7\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u534f\u540c\u63a8\u7406\u548c\u68c0\u7d22\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86DualRAG\u6846\u67b6\uff0c\u5305\u542b\u7d27\u5bc6\u8026\u5408\u7684RaQ\uff08\u63a8\u7406\u589e\u5f3a\u67e5\u8be2\uff09\u548cpKA\uff08\u6e10\u8fdb\u77e5\u8bc6\u805a\u5408\uff09\u4e24\u4e2a\u8fc7\u7a0b\uff0c\u901a\u8fc7\u77e5\u8bc6\u4e30\u5bcc\u548c\u63a8\u7406\u4f18\u5316\u7684\u826f\u6027\u5faa\u73af\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDualRAG\u663e\u8457\u63d0\u9ad8\u4e86\u7b54\u6848\u51c6\u786e\u6027\u548c\u8fde\u8d2f\u6027\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8d85\u8fc7\u4e86\u4f7f\u7528\u9884\u8bbe\u77e5\u8bc6\u7684\u6027\u80fd\u3002", "conclusion": "DualRAG\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u3002"}}
{"id": "2504.18428", "pdf": "https://arxiv.org/pdf/2504.18428", "abs": "https://arxiv.org/abs/2504.18428", "authors": ["Yiming Wang", "Pei Zhang", "Jialong Tang", "Haoran Wei", "Baosong Yang", "Rui Wang", "Chenshu Sun", "Feitong Sun", "Jiran Zhang", "Junxuan Wu", "Qiqian Cang", "Yichang Zhang", "Fei Huang", "Junyang Lin", "Fei Huang", "Jingren Zhou"], "title": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning\nbenchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our\nbenchmark ensures difficulty comprehensiveness, language diversity, and\nhigh-quality translation, making it a highly discriminative multilingual\nmathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive\nevaluation for advanced LLMs and find that even Deepseek-R1-671B and\nQwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30%\naccuracy under the highest level. From a language perspective, our benchmark\nreveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning\nperformance varies widely across languages for current LLMs; (2) Input-output\nlanguage consistency is low in reasoning LLMs and may be correlated with\nperformance; (3) The thinking length differs significantly by language for\ncurrent LLMs. Additionally, we demonstrate that controlling the output language\nin the instructions has the potential to affect reasoning performance,\nespecially for some low-resource languages, suggesting a promising direction\nfor improving multilingual capabilities in LLMs.", "AI": {"tldr": "PolyMath\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u6570\u5b66\u63a8\u7406\u57fa\u51c6\uff0c\u6db5\u76d618\u79cd\u8bed\u8a00\u548c4\u79cd\u96be\u5ea6\u7b49\u7ea7\uff0c\u8bc4\u6d4b\u53d1\u73b0\u5f53\u524d\u5148\u8fdb\u5927\u6a21\u578b\u5728\u591a\u8bed\u8a00\u63a8\u7406\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u8f93\u51fa\u8bed\u8a00\u63a7\u5236\u53ef\u80fd\u6539\u5584\u6027\u80fd\u3002", "motivation": "\u6784\u5efa\u4e00\u4e2a\u5168\u9762\u3001\u9ad8\u8d28\u91cf\u7684\u591a\u8bed\u8a00\u6570\u5b66\u63a8\u7406\u57fa\u51c6\uff0c\u4ee5\u8bc4\u6d4b\u548c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u5305\u542b18\u79cd\u8bed\u8a00\u548c4\u79cd\u96be\u5ea6\u7b49\u7ea7\u7684\u6570\u5b66\u95ee\u9898\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u591a\u79cd\u5148\u8fdb\u5927\u6a21\u578b\u8fdb\u884c\u8bc4\u6d4b\uff08\u5982Qwen-QwQ-32B\u7b49\uff09\uff0c\u5206\u6790\u8bed\u8a00\u4e00\u81f4\u6027\u548c\u601d\u7ef4\u957f\u5ea6\u5dee\u5f02\u3002", "result": "\u5f53\u524d\u5148\u8fdb\u6a21\u578b\uff08\u5982Deepseek-R1-671B\uff09\u5728\u6700\u9ad8\u96be\u5ea6\u4e0b\u7684\u51c6\u786e\u7387\u4f4e\u4e8e30%\uff0c\u4e14\u63a8\u7406\u6027\u80fd\u56e0\u8bed\u8a00\u5dee\u5f02\u663e\u8457\uff0c\u8f93\u51fa\u8bed\u8a00\u63a7\u5236\u53ef\u80fd\u5f71\u54cd\u7ed3\u679c\u3002", "conclusion": "\u591a\u8bed\u8a00\u63a8\u7406\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4f46\u901a\u8fc7\u63a7\u5236\u8f93\u51fa\u8bed\u8a00\u6216\u80fd\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u6027\u80fd\uff0c\u4e3a\u6539\u8fdb\u5927\u6a21\u578b\u591a\u8bed\u8a00\u80fd\u529b\u63d0\u4f9b\u65b9\u5411\u3002"}}
{"id": "2504.18262", "pdf": "https://arxiv.org/pdf/2504.18262", "abs": "https://arxiv.org/abs/2504.18262", "authors": ["Andrea Quintanilla", "Johan Van Horebeek"], "title": "Local Statistical Parity for the Estimation of Fair Decision Trees", "categories": ["cs.LG"], "comment": null, "summary": "Given the high computational complexity of decision tree estimation,\nclassical methods construct a tree by adding one node at a time in a recursive\nway. To facilitate promoting fairness, we propose a fairness criterion local to\nthe tree nodes. We prove how it is related to the Statistical Parity criterion,\npopular in the Algorithmic Fairness literature, and show how to incorporate it\ninto standard recursive tree estimation algorithms.\n  We present a tree estimation algorithm called Constrained Logistic Regression\nTree (C-LRT), which is a modification of the standard CART algorithm using\nlocally linear classifiers and imposing restrictions as done in Constrained\nLogistic Regression.\n  Finally, we evaluate the performance of trees estimated with C-LRT on\ndatasets commonly used in the Algorithmic Fairness literature, using various\nclassification and fairness metrics. The results confirm that C-LRT\nsuccessfully allows to control and balance accuracy and fairness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86C-LRT\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u7ebf\u6027\u5206\u7c7b\u5668\u548c\u7ea6\u675f\u903b\u8f91\u56de\u5f52\u6539\u8fdbCART\u7b97\u6cd5\uff0c\u4ee5\u5728\u51b3\u7b56\u6811\u6784\u5efa\u4e2d\u5e73\u8861\u51c6\u786e\u6027\u4e0e\u516c\u5e73\u6027\u3002", "motivation": "\u51cf\u5c11\u51b3\u7b56\u6811\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u901a\u8fc7\u8282\u70b9\u5c40\u90e8\u516c\u5e73\u6027\u51c6\u5219\u63d0\u5347\u7b97\u6cd5\u516c\u5e73\u6027\u3002", "method": "\u63d0\u51faC-LRT\u7b97\u6cd5\uff0c\u7ed3\u5408\u5c40\u90e8\u7ebf\u6027\u5206\u7c7b\u5668\u548c\u7ea6\u675f\u903b\u8f91\u56de\u5f52\u6539\u8fdb\u6807\u51c6CART\u7b97\u6cd5\u3002", "result": "\u5728\u516c\u5e73\u6027\u6587\u732e\u5e38\u7528\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cC-LRT\u80fd\u6709\u6548\u5e73\u8861\u5206\u7c7b\u51c6\u786e\u6027\u4e0e\u516c\u5e73\u6027\u3002", "conclusion": "C-LRT\u4e3a\u51b3\u7b56\u6811\u63d0\u4f9b\u4e86\u63a7\u5236\u516c\u5e73\u6027\u4e0e\u51c6\u786e\u6027\u7684\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2504.17964", "pdf": "https://arxiv.org/pdf/2504.17964", "abs": "https://arxiv.org/abs/2504.17964", "authors": ["Celia Chen", "Alex Leitch"], "title": "Evaluating Machine Expertise: How Graduate Students Develop Frameworks for Assessing GenAI Content", "categories": ["cs.HC", "cs.AI"], "comment": "Under review at ACM Web Science Conference 2025's Human-GenAI\n  Interactions Workshop, 4 pages", "summary": "This paper examines how graduate students develop frameworks for evaluating\nmachine-generated expertise in web-based interactions with large language\nmodels (LLMs). Through a qualitative study combining surveys, LLM interaction\ntranscripts, and in-depth interviews with 14 graduate students, we identify\npatterns in how these emerging professionals assess and engage with\nAI-generated content. Our findings reveal that students construct evaluation\nframeworks shaped by three main factors: professional identity, verification\ncapabilities, and system navigation experience. Rather than uniformly accepting\nor rejecting LLM outputs, students protect domains central to their\nprofessional identities while delegating others--with managers preserving\nconceptual work, designers safeguarding creative processes, and programmers\nmaintaining control over core technical expertise. These evaluation frameworks\nare further influenced by students' ability to verify different types of\ncontent and their experience navigating complex systems. This research\ncontributes to web science by highlighting emerging human-genAI interaction\npatterns and suggesting how platforms might better support users in developing\neffective frameworks for evaluating machine-generated expertise signals in\nAI-mediated web environments.", "AI": {"tldr": "\u7814\u7a76\u751f\u901a\u8fc7\u4e0eLLM\u7684\u4e92\u52a8\u6784\u5efa\u4e86\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3b\u8981\u53d7\u4e13\u4e1a\u8eab\u4efd\u3001\u9a8c\u8bc1\u80fd\u529b\u548c\u7cfb\u7edf\u5bfc\u822a\u7ecf\u9a8c\u5f71\u54cd\uff0c\u800c\u975e\u7b80\u5355\u63a5\u53d7\u6216\u62d2\u7edd\u8f93\u51fa\u3002", "motivation": "\u63a2\u8ba8\u7814\u7a76\u751f\u5982\u4f55\u8bc4\u4f30\u4e0eLLMs\u751f\u6210\u7684\u673a\u5668\u4e13\u4e1a\u77e5\u8bc6\uff0c\u63ed\u793a\u5176\u5728AI\u4ea4\u4e92\u4e2d\u7684\u8bc4\u4f30\u6a21\u5f0f\u3002", "method": "\u901a\u8fc7\u95ee\u5377\u8c03\u67e5\u3001LLM\u4e92\u52a8\u8bb0\u5f55\u548c14\u540d\u7814\u7a76\u751f\u7684\u6df1\u5ea6\u8bbf\u8c08\u8fdb\u884c\u5b9a\u6027\u7814\u7a76\u3002", "result": "\u5b66\u751f\u6784\u5efa\u7684\u8bc4\u4f30\u6846\u67b6\u53d7\u4e13\u4e1a\u8eab\u4efd\u3001\u9a8c\u8bc1\u80fd\u529b\u548c\u7cfb\u7edf\u5bfc\u822a\u7ecf\u9a8c\u5f71\u54cd\uff0c\u4e0d\u540c\u9886\u57df\u7684\u5b66\u751f\u5bf9LLM\u8f93\u51fa\u7684\u53cd\u5e94\u4e0d\u540c\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u65b0\u6a21\u5f0f\uff0c\u5e76\u4e3a\u5e73\u53f0\u5982\u4f55\u652f\u6301\u7528\u6237\u8bc4\u4f30AI\u751f\u6210\u5185\u5bb9\u63d0\u4f9b\u5efa\u8bae\u3002"}}
{"id": "2504.18458", "pdf": "https://arxiv.org/pdf/2504.18458", "abs": "https://arxiv.org/abs/2504.18458", "authors": ["Wenyi Xiao", "Leilei Gan", "Weilong Dai", "Wanggui He", "Ziwei Huang", "Haoyuan Li", "Fangxun Shu", "Zhelun Yu", "Peng Zhang", "Hao Jiang", "Fei Wu"], "title": "Fast-Slow Thinking for Large Vision-Language Model Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "16 pages, 5 figures, and 12 tables", "summary": "Recent advances in large vision-language models (LVLMs) have revealed an\n\\textit{overthinking} phenomenon, where models generate verbose reasoning\nacross all tasks regardless of questions. To address this issue, we present\n\\textbf{FAST}, a novel \\textbf{Fa}st-\\textbf{S}low \\textbf{T}hinking framework\nthat dynamically adapts reasoning depth based on question characteristics.\nThrough empirical analysis, we establish the feasibility of fast-slow thinking\nin LVLMs by investigating how response length and data distribution affect\nperformance. We develop FAST-GRPO with three components: model-based metrics\nfor question characterization, an adaptive thinking reward mechanism, and\ndifficulty-aware KL regularization. Experiments across seven reasoning\nbenchmarks demonstrate that FAST achieves state-of-the-art accuracy with over\n10\\% relative improvement compared to the base model, while reducing token\nusage by 32.7-67.3\\% compared to previous slow-thinking approaches, effectively\nbalancing reasoning length and accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFAST\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u6765\u89e3\u51b3\u5927\u89c6\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u2018\u8fc7\u5ea6\u601d\u8003\u2019\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u3002", "motivation": "\u89e3\u51b3\u5927\u89c6\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u2018\u8fc7\u5ea6\u601d\u8003\u2019\u73b0\u8c61\uff0c\u5373\u65e0\u8bba\u95ee\u9898\u590d\u6742\u5ea6\u5982\u4f55\uff0c\u6a21\u578b\u90fd\u751f\u6210\u5197\u957f\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faFAST\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u57fa\u4e8e\u6a21\u578b\u7684\u6307\u6807\u7528\u4e8e\u95ee\u9898\u7279\u5f81\u5316\u3001\u81ea\u9002\u5e94\u601d\u8003\u5956\u52b1\u673a\u5236\u548c\u96be\u5ea6\u611f\u77e5\u7684KL\u6b63\u5219\u5316\u3002", "result": "\u5728\u4e03\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFAST\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u5347\u4e8610%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u6bd4\u4e4b\u524d\u7684\u2018\u6162\u601d\u8003\u2019\u65b9\u6cd5\u51cf\u5c11\u4e8632.7-67.3%\u7684token\u4f7f\u7528\u91cf\u3002", "conclusion": "FAST\u6709\u6548\u5e73\u8861\u4e86\u63a8\u7406\u957f\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u5c55\u793a\u4e86\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.18267", "pdf": "https://arxiv.org/pdf/2504.18267", "abs": "https://arxiv.org/abs/2504.18267", "authors": ["Prajwal Chauhan", "Salah Eddine Choutri", "Mohamed Ghattassi", "Nader Masmoudi", "Saif Eddin Jabari"], "title": "Neural operators struggle to learn complex PDEs in pedestrian mobility: Hughes model case study", "categories": ["cs.LG", "cs.AI"], "comment": "26 pages, 15 figures, 6 tables, under review at Artificial\n  Intelligence for Transportation | Journal", "summary": "This paper investigates the limitations of neural operators in learning\nsolutions for a Hughes model, a first-order hyperbolic conservation law system\nfor crowd dynamics. The model couples a Fokker-Planck equation representing\npedestrian density with a Hamilton-Jacobi-type (eikonal) equation. This Hughes\nmodel belongs to the class of nonlinear hyperbolic systems that often exhibit\ncomplex solution structures, including shocks and discontinuities. In this\nstudy, we assess the performance of three state-of-the-art neural operators\n(Fourier Neural Operator, Wavelet Neural Operator, and Multiwavelet Neural\nOperator) in various challenging scenarios. Specifically, we consider (1)\ndiscontinuous and Gaussian initial conditions and (2) diverse boundary\nconditions, while also examining the impact of different numerical schemes.\n  Our results show that these neural operators perform well in easy scenarios\nwith fewer discontinuities in the initial condition, yet they struggle in\ncomplex scenarios with multiple initial discontinuities and dynamic boundary\nconditions, even when trained specifically on such complex samples. The\npredicted solutions often appear smoother, resulting in a reduction in total\nvariation and a loss of important physical features. This smoothing behavior is\nsimilar to issues discussed by Daganzo (1995), where models that introduce\nartificial diffusion were shown to miss essential features such as shock waves\nin hyperbolic systems. These results suggest that current neural operator\narchitectures may introduce unintended regularization effects that limit their\nability to capture transport dynamics governed by discontinuities. They also\nraise concerns about generalizing these methods to traffic applications where\nshock preservation is essential.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u7b97\u5b50\u5728\u5904\u7406Hughes\u6a21\u578b\uff08\u4e00\u79cd\u7fa4\u4f53\u52a8\u529b\u5b66\u7684\u4e00\u9636\u53cc\u66f2\u5b88\u6052\u5f8b\u7cfb\u7edf\uff09\u65f6\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u5176\u5728\u590d\u6742\u521d\u59cb\u6761\u4ef6\u548c\u8fb9\u754c\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u65e0\u6cd5\u6355\u6349\u95f4\u65ad\u548c\u91cd\u8981\u7269\u7406\u7279\u5f81\u3002", "motivation": "\u63a2\u8ba8\u795e\u7ecf\u7b97\u5b50\u5728\u5904\u7406\u5177\u6709\u590d\u6742\u89e3\u7ed3\u6784\uff08\u5982\u95f4\u65ad\uff09\u7684\u975e\u7ebf\u6027\u53cc\u66f2\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u9488\u5bf9Hughes\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "method": "\u8bc4\u4f30\u4e09\u79cd\u5148\u8fdb\u7684\u795e\u7ecf\u7b97\u5b50\uff08\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u3001\u5c0f\u6ce2\u795e\u7ecf\u7b97\u5b50\u548c\u591a\u5c0f\u6ce2\u795e\u7ecf\u7b97\u5b50\uff09\u5728\u4e0d\u540c\u521d\u59cb\u6761\u4ef6\uff08\u95f4\u65ad\u548c\u9ad8\u65af\u5206\u5e03\uff09\u548c\u8fb9\u754c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u6570\u503c\u683c\u5f0f\u7684\u5f71\u54cd\u3002", "result": "\u795e\u7ecf\u7b97\u5b50\u5728\u7b80\u5355\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\uff08\u591a\u521d\u59cb\u95f4\u65ad\u548c\u52a8\u6001\u8fb9\u754c\u6761\u4ef6\uff09\u4e2d\u9884\u6d4b\u7ed3\u679c\u8fc7\u4e8e\u5e73\u6ed1\uff0c\u5bfc\u81f4\u603b\u53d8\u5dee\u51cf\u5c11\u548c\u7269\u7406\u7279\u5f81\u4e22\u5931\u3002", "conclusion": "\u5f53\u524d\u7684\u795e\u7ecf\u7b97\u5b50\u7ed3\u6784\u53ef\u80fd\u5f15\u5165\u975e\u9884\u671f\u7684\u6b63\u5219\u5316\u6548\u5e94\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u95f4\u65ad\u4e3b\u5bfc\u7684\u8f93\u8fd0\u52a8\u6001\u7684\u6355\u6349\u80fd\u529b\uff0c\u5bf9\u4ea4\u901a\u5e94\u7528\u4e2d\u7684\u6fc0\u6ce2\u4fdd\u6301\u63d0\u51fa\u4e86\u6311\u6218\u3002"}}
{"id": "2504.17979", "pdf": "https://arxiv.org/pdf/2504.17979", "abs": "https://arxiv.org/abs/2504.17979", "authors": ["Kaaustaaub Shankar", "Wilhelm Louw", "Bharadwaj Dogga", "Nick Ernest", "Tim Arnett", "Kelly Cohen"], "title": "Fuzzy-RRT for Obstacle Avoidance in a 2-DOF Semi-Autonomous Surgical Robotic Arm", "categories": ["cs.RO", "cs.AI"], "comment": "9 pages, 5 figures. Submitted to NAFIPS 2025 Conference (North\n  American Fuzzy Information Processing Society). Includes results on Fuzzy-RRT\n  performance in surgical robotics path planning", "summary": "AI-driven semi-autonomous robotic surgery is essential for addressing the\nmedical challenges of long-duration interplanetary missions, where limited crew\nsizes and communication delays restrict traditional surgical approaches.\nCurrent robotic surgery systems require full surgeon control, demanding\nextensive expertise and limiting feasibility in space. We propose a novel\nadaptation of the Fuzzy Rapidly-exploring Random Tree algorithm for obstacle\navoidance and collaborative control in a two-degree-of-freedom robotic arm\nmodeled on the Miniaturized Robotic-Assisted surgical system. It was found that\nthe Fuzzy Rapidly-exploring Random Tree algorithm resulted in an 743 percent\nimprovement to path search time and 43 percent improvement to path cost.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u7cca\u5feb\u901f\u63a2\u7d22\u968f\u673a\u6811\u7b97\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u592a\u7a7a\u4efb\u52a1\u4e2d\u7684\u534a\u81ea\u4e3b\u673a\u5668\u4eba\u624b\u672f\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8def\u5f84\u641c\u7d22\u65f6\u95f4\u548c\u8def\u5f84\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u957f\u671f\u661f\u9645\u4efb\u52a1\u4e2d\u533b\u7597\u6311\u6218\uff0c\u56e0\u8239\u5458\u6709\u9650\u548c\u901a\u4fe1\u5ef6\u8fdf\uff0c\u4f20\u7edf\u624b\u672f\u65b9\u5f0f\u4e0d\u9002\u7528\uff0c\u9700\u53d1\u5c55\u534a\u81ea\u4e3b\u673a\u5668\u4eba\u624b\u672f\u3002", "method": "\u91c7\u7528\u6a21\u7cca\u5feb\u901f\u63a2\u7d22\u968f\u673a\u6811\u7b97\u6cd5\uff0c\u7528\u4e8e\u4e24\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u81c2\u7684\u907f\u969c\u548c\u534f\u4f5c\u63a7\u5236\u3002", "result": "\u7b97\u6cd5\u4f7f\u8def\u5f84\u641c\u7d22\u65f6\u95f4\u63d0\u5347743%\uff0c\u8def\u5f84\u6210\u672c\u964d\u4f4e43%\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u534a\u81ea\u4e3b\u673a\u5668\u4eba\u624b\u672f\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u592a\u7a7a\u4efb\u52a1\u3002"}}
{"id": "2504.18474", "pdf": "https://arxiv.org/pdf/2504.18474", "abs": "https://arxiv.org/abs/2504.18474", "authors": ["James D. Finch", "Yasasvi Josyula", "Jinho D. Choi"], "title": "Generative Induction of Dialogue Task Schemas with Streaming Refinement and Simulated Interactions", "categories": ["cs.CL"], "comment": "Accepted (B) to TACL 2025", "summary": "In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is\nessential for automatically identifying key information slots from dialogue\ndata without manual intervention. This paper presents a novel state-of-the-art\n(SoTA) approach that formulates SSI as a text generation task, where a language\nmodel incrementally constructs and refines a slot schema over a stream of\ndialogue data. To develop this approach, we present a fully automatic LLM-based\nTOD simulation method that creates data with high-quality state labels for\nnovel task domains. Furthermore, we identify issues in SSI evaluation due to\ndata leakage and poor metric alignment with human judgment. We resolve these by\ncreating new evaluation data using our simulation method with human guidance\nand correction, as well as designing improved evaluation metrics. These\ncontributions establish a foundation for future SSI research and advance the\nSoTA in dialogue understanding and system development.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u751f\u6210\u4efb\u52a1\u81ea\u52a8\u8bc6\u522b\u5173\u952e\u4fe1\u606f\u69fd\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8eLLM\u7684\u6a21\u62df\u65b9\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u6539\u8fdb\u4e86\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f20\u7edfSlot Schema Induction\u4e2d\u9700\u8981\u4eba\u5de5\u5e72\u9884\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u65b9\u6cd5\u63d0\u5347\u5bf9\u8bdd\u7cfb\u7edf\u7684\u7406\u89e3\u548c\u5f00\u53d1\u3002", "method": "\u5c06SSI\u89c6\u4e3a\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u589e\u91cf\u6784\u5efa\u548c\u4f18\u5316\u69fd\u6a21\u5f0f\uff1b\u5f00\u53d1\u57fa\u4e8eLLM\u7684TOD\u6a21\u62df\u65b9\u6cd5\u751f\u6210\u9ad8\u8d28\u6570\u636e\uff1b\u6539\u8fdb\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u73b0\u4e86\u65b0\u7684SoTA\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u6cc4\u9732\u548c\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u672a\u6765SSI\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u63a8\u8fdb\u4e86\u5bf9\u8bdd\u7406\u89e3\u548c\u7cfb\u7edf\u5f00\u53d1\u7684SoTA\u3002"}}
{"id": "2504.18274", "pdf": "https://arxiv.org/pdf/2504.18274", "abs": "https://arxiv.org/abs/2504.18274", "authors": ["Garrett Baker", "George Wang", "Jesse Hoogland", "Daniel Murfet"], "title": "Studying Small Language Models with Susceptibilities", "categories": ["cs.LG"], "comment": null, "summary": "We develop a linear response framework for interpretability that treats a\nneural network as a Bayesian statistical mechanical system. A small, controlled\nperturbation of the data distribution, for example shifting the Pile toward\nGitHub or legal text, induces a first-order change in the posterior expectation\nof an observable localized on a chosen component of the network. The resulting\nsusceptibility can be estimated efficiently with local SGLD samples and\nfactorizes into signed, per-token contributions that serve as attribution\nscores. Building a set of perturbations (probes) yields a response matrix whose\nlow-rank structure separates functional modules such as multigram and induction\nheads in a 3M-parameter transformer. Susceptibilities link local learning\ncoefficients from singular learning theory with linear-response theory, and\nquantify how local loss landscape geometry deforms under shifts in the data\ndistribution.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7ebf\u6027\u54cd\u5e94\u6846\u67b6\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u89c6\u4e3a\u8d1d\u53f6\u65af\u7edf\u8ba1\u529b\u5b66\u7cfb\u7edf\uff0c\u901a\u8fc7\u6270\u52a8\u6570\u636e\u5206\u5e03\u6765\u7814\u7a76\u7f51\u7edc\u7ec4\u4ef6\u7684\u5c40\u90e8\u654f\u611f\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u4e3a\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u7ebf\u6027\u54cd\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u7edf\u8ba1\u529b\u5b66\u89c6\u89d2\u7406\u89e3\u6570\u636e\u5206\u5e03\u53d8\u5316\u5bf9\u7f51\u7edc\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u63ed\u793a\u5176\u5185\u90e8\u529f\u80fd\u6a21\u5757\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u5c06\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u4e3a\u8d1d\u53f6\u65af\u7edf\u8ba1\u529b\u5b66\u7cfb\u7edf\uff1b2\uff09\u901a\u8fc7\u5c40\u90e8SGLD\u91c7\u6837\u9ad8\u6548\u4f30\u8ba1\u654f\u611f\u6027\uff1b3\uff09\u8bbe\u8ba1\u6270\u52a8\uff08\u63a2\u9488\uff09\u6784\u5efa\u54cd\u5e94\u77e9\u9635\uff0c\u5206\u79bb\u529f\u80fd\u6a21\u5757\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5206\u79bb\u529f\u80fd\u6a21\u5757\uff08\u5982\u591a\u8bed\u6cd5\u548c\u5f52\u7eb3\u5934\uff09\uff0c\u5e76\u91cf\u5316\u6570\u636e\u5206\u5e03\u53d8\u5316\u5bf9\u635f\u5931\u51fd\u6570\u5c40\u90e8\u51e0\u4f55\u7684\u5f62\u53d8\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u8fde\u63a5\u4e86\u5947\u5f02\u5b66\u4e60\u7406\u8bba\u4e0e\u7ebf\u6027\u54cd\u5e94\u7406\u8bba\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790\u795e\u7ecf\u7f51\u7edc\u5728\u6570\u636e\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u884c\u4e3a\u3002"}}
{"id": "2504.18010", "pdf": "https://arxiv.org/pdf/2504.18010", "abs": "https://arxiv.org/abs/2504.18010", "authors": ["Zilin Huang", "Zihao Sheng", "Zhengyang Wan", "Yansong Qu", "Yuhao Luo", "Boyue Wang", "Pei Li", "Yen-Jung Chen", "Jiancong Chen", "Keke Long", "Jiayi Meng", "Yue Leng", "Sikai Chen"], "title": "Sky-Drive: A Distributed Multi-Agent Simulation Platform for Socially-Aware and Human-AI Collaborative Future Transportation", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "15 pages, 7 figures", "summary": "Recent advances in autonomous system simulation platforms have significantly\nenhanced the safe and scalable testing of driving policies. However, existing\nsimulators do not yet fully meet the needs of future transportation research,\nparticularly in modeling socially-aware driving agents and enabling effective\nhuman-AI collaboration. This paper introduces Sky-Drive, a novel distributed\nmulti-agent simulation platform that addresses these limitations through four\nkey innovations: (a) a distributed architecture for synchronized simulation\nacross multiple terminals; (b) a multi-modal human-in-the-loop framework\nintegrating diverse sensors to collect rich behavioral data; (c) a human-AI\ncollaboration mechanism supporting continuous and adaptive knowledge exchange;\nand (d) a digital twin (DT) framework for constructing high-fidelity virtual\nreplicas of real-world transportation environments. Sky-Drive supports diverse\napplications such as autonomous vehicle (AV)-vulnerable road user (VRU)\ninteraction modeling, human-in-the-loop training, socially-aware reinforcement\nlearning, personalized driving policy, and customized scenario generation.\nFuture extensions will incorporate foundation models for context-aware decision\nsupport and hardware-in-the-loop (HIL) testing for real-world validation. By\nbridging scenario generation, data collection, algorithm training, and hardware\nintegration, Sky-Drive has the potential to become a foundational platform for\nthe next generation of socially-aware and human-centered autonomous\ntransportation research. The demo video and code are available\nat:https://sky-lab-uw.github.io/Sky-Drive-website/", "AI": {"tldr": "Sky-Drive\u662f\u65b0\u578b\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u4eff\u771f\u5e73\u53f0\uff0c\u901a\u8fc7\u56db\u5927\u521b\u65b0\u89e3\u51b3\u73b0\u6709\u4eff\u771f\u7684\u4e0d\u8db3\uff0c\u652f\u6301\u4eba\u673a\u534f\u4f5c\u548c\u793e\u4f1a\u610f\u8bc6\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u4eff\u771f\u5668\u65e0\u6cd5\u6ee1\u8db3\u672a\u6765\u4ea4\u901a\u7814\u7a76\u7684\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u793e\u4f1a\u610f\u8bc6\u9a7e\u9a76\u667a\u80fd\u4f53\u548c\u6709\u6548\u4eba\u673a\u534f\u4f5c\u65b9\u9762\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u67b6\u6784\u3001\u591a\u6a21\u6001\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\u3001\u4eba\u673a\u534f\u4f5c\u673a\u5236\u53ca\u6570\u5b57\u5b6a\u751f\u6280\u672f\u3002", "result": "Sky-Drive\u652f\u6301\u591a\u79cd\u5e94\u7528\uff0c\u5305\u62ecAV-VRU\u4ea4\u4e92\u5efa\u6a21\u3001\u4eba\u673a\u57f9\u8bad\u3001\u793e\u4f1a\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\u7b49\u3002", "conclusion": "Sky-Drive\u6709\u6f5c\u529b\u6210\u4e3a\u4e0b\u4e00\u4ee3\u793e\u4f1a\u610f\u8bc6\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u7684\u57fa\u7840\u5e73\u53f0\u3002"}}
{"id": "2504.18483", "pdf": "https://arxiv.org/pdf/2504.18483", "abs": "https://arxiv.org/abs/2504.18483", "authors": ["Leandra Fichtel", "Maximilian Splieth\u00f6ver", "Eyke H\u00fcllermeier", "Patricia Jimenez", "Nils Klowait", "Stefan Kopp", "Axel-Cyrille Ngonga Ngomo", "Amelie Robrecht", "Ingrid Scharlau", "Lutz Terfloth", "Anna-Lisa Vollmer", "Henning Wachsmuth"], "title": "Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues", "categories": ["cs.CL"], "comment": "Submitted to the SIGDial Conference 2025", "summary": "The ability to generate explanations that are understood by explainees is the\nquintessence of explainable artificial intelligence. Since understanding\ndepends on the explainee's background and needs, recent research has focused on\nco-constructive explanation dialogues, where the explainer continuously\nmonitors the explainee's understanding and adapts explanations dynamically. We\ninvestigate the ability of large language models (LLMs) to engage as explainers\nin co-constructive explanation dialogues. In particular, we present a user\nstudy in which explainees interact with LLMs, of which some have been\ninstructed to explain a predefined topic co-constructively. We evaluate the\nexplainees' understanding before and after the dialogue, as well as their\nperception of the LLMs' co-constructive behavior. Our results indicate that\ncurrent LLMs show some co-constructive behaviors, such as asking verification\nquestions, that foster the explainees' engagement and can improve understanding\nof a topic. However, their ability to effectively monitor the current\nunderstanding and scaffold the explanations accordingly remains limited.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5171\u5efa\u6027\u89e3\u91ca\u5bf9\u8bdd\u4e2d\u4f5c\u4e3a\u89e3\u91ca\u8005\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u7528\u6237\u5b9e\u9a8c\u8bc4\u4f30\u5176\u6548\u679c\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5171\u5efa\u6027\u89e3\u91ca\u5bf9\u8bdd\u63d0\u5347\u89e3\u91ca\u6027\u4eba\u5de5\u667a\u80fd\u7684\u6548\u679c\uff0c\u5c24\u5176\u662fLLMs\u7684\u52a8\u6001\u9002\u5e94\u80fd\u529b\u3002", "method": "\u8fdb\u884c\u7528\u6237\u7814\u7a76\uff0c\u8ba9\u88ab\u89e3\u91ca\u8005\u4e0eLLMs\u4e92\u52a8\uff0c\u8bc4\u4f30\u89e3\u91ca\u524d\u540e\u7684\u7406\u89e3\u7a0b\u5ea6\u53ca\u5bf9LLMs\u884c\u4e3a\u7684\u611f\u77e5\u3002", "result": "LLMs\u5c55\u73b0\u90e8\u5206\u5171\u5efa\u6027\u884c\u4e3a\uff08\u5982\u63d0\u95ee\u9a8c\u8bc1\uff09\uff0c\u80fd\u63d0\u5347\u53c2\u4e0e\u5ea6\u548c\u7406\u89e3\uff0c\u4f46\u5728\u76d1\u63a7\u7406\u89e3\u548c\u52a8\u6001\u8c03\u6574\u89e3\u91ca\u65b9\u9762\u6709\u9650\u3002", "conclusion": "\u5f53\u524dLLMs\u5728\u5171\u5efa\u6027\u89e3\u91ca\u5bf9\u8bdd\u4e2d\u8868\u73b0\u521d\u6b65\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u76d1\u63a7\u548c\u8c03\u6574\u80fd\u529b\u3002"}}
{"id": "2504.18278", "pdf": "https://arxiv.org/pdf/2504.18278", "abs": "https://arxiv.org/abs/2504.18278", "authors": ["Richard Oliver Lane"], "title": "A comprehensive review of classifier probability calibration metrics", "categories": ["cs.LG", "stat.ML"], "comment": "60 pages, 7 figures", "summary": "Probabilities or confidence values produced by artificial intelligence (AI)\nand machine learning (ML) models often do not reflect their true accuracy, with\nsome models being under or over confident in their predictions. For example, if\na model is 80% sure of an outcome, is it correct 80% of the time? Probability\ncalibration metrics measure the discrepancy between confidence and accuracy,\nproviding an independent assessment of model calibration performance that\ncomplements traditional accuracy metrics. Understanding calibration is\nimportant when the outputs of multiple systems are combined, for assurance in\nsafety or business-critical contexts, and for building user trust in models.\nThis paper provides a comprehensive review of probability calibration metrics\nfor classifier and object detection models, organising them according to a\nnumber of different categorisations to highlight their relationships. We\nidentify 82 major metrics, which can be grouped into four classifier families\n(point-based, bin-based, kernel or curve-based, and cumulative) and an object\ndetection family. For each metric, we provide equations where available,\nfacilitating implementation and comparison by future researchers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u5206\u7c7b\u5668\u548c\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u6982\u7387\u6821\u51c6\u6307\u6807\uff0c\u8bc6\u522b\u4e8682\u79cd\u4e3b\u8981\u6307\u6807\uff0c\u5e76\u5c06\u5176\u5206\u4e3a\u56db\u5927\u7c7b\uff0c\u65e8\u5728\u5e2e\u52a9\u7814\u7a76\u8005\u66f4\u597d\u5730\u7406\u89e3\u548c\u6bd4\u8f83\u4e0d\u540c\u6821\u51c6\u6307\u6807\u7684\u4f18\u52a3\u3002", "motivation": "AI\u548cML\u6a21\u578b\u7684\u6982\u7387\u6216\u7f6e\u4fe1\u5ea6\u5e38\u4e0e\u5176\u771f\u5b9e\u51c6\u786e\u6027\u4e0d\u7b26\uff0c\u5bfc\u81f4\u6a21\u578b\u9884\u6d4b\u53ef\u80fd\u8fc7\u6216\u6b20\u81ea\u4fe1\u3002\u6821\u51c6\u6307\u6807\u80fd\u6d4b\u91cf\u8fd9\u79cd\u5dee\u5f02\uff0c\u5bf9\u6a21\u578b\u5728\u5b89\u5168\u6216\u5173\u952e\u4e1a\u52a1\u73af\u5883\u4e2d\u7684\u5e94\u7528\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u8bba\u6587\u5168\u9762\u56de\u987e\u5e76\u5206\u7c7b\u4e8682\u79cd\u6982\u7387\u6821\u51c6\u6307\u6807\uff0c\u5206\u4e3a\u70b9\u57fa\u3001\u5206\u7bb1\u57fa\u3001\u6838\u6216\u66f2\u7ebf\u57fa\u3001\u7d2f\u79ef\u53ca\u76ee\u6807\u68c0\u6d4b\u4e94\u5927\u7c7b\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5173\u516c\u5f0f\u4ee5\u4fbf\u5b9e\u73b0\u548c\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u8005\u53ef\u901a\u8fc7\u5206\u7c7b\u548c\u516c\u5f0f\u66f4\u4fbf\u6377\u5730\u7406\u89e3\u548c\u5b9e\u73b0\u6821\u51c6\u6307\u6807\uff0c\u4ece\u800c\u8bc4\u4f30\u6a21\u578b\u7684\u6821\u51c6\u6027\u80fd\u3002", "conclusion": "\u6982\u7387\u6821\u51c6\u6307\u6807\u7684\u68b3\u7406\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u53c2\u8003\uff0c\u4fc3\u8fdb\u4e86\u6a21\u578b\u6821\u51c6\u6027\u80fd\u7684\u8bc4\u4f30\u4e0e\u5e94\u7528\u3002"}}
{"id": "2504.18535", "pdf": "https://arxiv.org/pdf/2504.18535", "abs": "https://arxiv.org/abs/2504.18535", "authors": ["Gwen Yidou Weng", "Benjie Wang", "Guy Van den Broeck"], "title": "TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models (LMs) advance, there is an increasing need to\ncontrol their outputs to align with human values (e.g., detoxification) or\ndesired attributes (e.g., personalization, topic). However, autoregressive\nmodels focus on next-token predictions and struggle with global properties that\nrequire looking ahead. Existing solutions either tune or post-train LMs for\neach new attribute - expensive and inflexible - or approximate the Expected\nAttribute Probability (EAP) of future sequences by sampling or training, which\nis slow and unreliable for rare attributes. We introduce TRACE (Tractable\nProbabilistic Reasoning for Adaptable Controllable gEneration), a novel\nframework that efficiently computes EAP and adapts to new attributes through\ntractable probabilistic reasoning and lightweight control. TRACE distills a\nHidden Markov Model (HMM) from an LM and pairs it with a small classifier to\nestimate attribute probabilities, enabling exact EAP computation over the HMM's\npredicted futures. This EAP is then used to reweigh the LM's next-token\nprobabilities for globally compliant continuations. Empirically, TRACE achieves\nstate-of-the-art results in detoxification with only 10% decoding overhead,\nadapts to 76 low-resource personalized LLMs within seconds, and seamlessly\nextends to composite attributes.", "AI": {"tldr": "TRACE\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u548c\u5c0f\u5206\u7c7b\u5668\u6765\u9ad8\u6548\u8ba1\u7b97\u9884\u671f\u5c5e\u6027\u6982\u7387\uff08EAP\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u5168\u5c40\u63a7\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5982\u4f55\u63a7\u5236\u5176\u8f93\u51fa\u4ee5\u7b26\u5408\u4eba\u7c7b\u4ef7\u503c\u89c2\u6216\u7279\u5b9a\u5c5e\u6027\uff08\u5982\u53bb\u6bd2\u5316\u3001\u4e2a\u6027\u5316\uff09\u6210\u4e3a\u91cd\u8981\u9700\u6c42\u3002\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u4e0d\u7075\u6d3b\uff0c\u9700\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "TRACE\u6846\u67b6\u901a\u8fc7\u4ece\u8bed\u8a00\u6a21\u578b\u4e2d\u84b8\u998f\u51fa\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08HMM\uff09\u5e76\u642d\u914d\u5c0f\u578b\u5206\u7c7b\u5668\u6765\u4f30\u8ba1\u5c5e\u6027\u6982\u7387\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684EAP\u8ba1\u7b97\uff0c\u5e76\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u4e0b\u4e00\u6807\u8bb0\u6982\u7387\u6765\u63a7\u5236\u751f\u6210\u5185\u5bb9\u3002", "result": "TRACE\u5728\u53bb\u6bd2\u5316\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u4f18\u6548\u679c\uff0c\u89e3\u7801\u5f00\u9500\u4ec5\u4e3a10%\uff0c\u53ef\u5728\u79d2\u7ea7\u5185\u9002\u914d76\u4e2a\u4f4e\u8d44\u6e90\u4e2a\u6027\u5316\u6a21\u578b\uff0c\u5e76\u80fd\u65e0\u7f1d\u6269\u5c55\u81f3\u590d\u5408\u5c5e\u6027\u3002", "conclusion": "TRACE\u901a\u8fc7\u9ad8\u6548\u7684\u6982\u7387\u63a8\u7406\u548c\u8f7b\u91cf\u7ea7\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u63a7\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u672a\u6765\u7684\u53ef\u63a7\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.18300", "pdf": "https://arxiv.org/pdf/2504.18300", "abs": "https://arxiv.org/abs/2504.18300", "authors": ["Simon Hakenes", "Tobias Glasmachers"], "title": "Deep Reinforcement Learning Based Navigation with Macro Actions and Topological Maps", "categories": ["cs.LG"], "comment": "14 pages, 6 figures", "summary": "This paper addresses the challenge of navigation in large, visually complex\nenvironments with sparse rewards. We propose a method that uses object-oriented\nmacro actions grounded in a topological map, allowing a simple Deep Q-Network\n(DQN) to learn effective navigation policies. The agent builds a map by\ndetecting objects from RGBD input and selecting discrete macro actions that\ncorrespond to navigating to these objects. This abstraction drastically reduces\nthe complexity of the underlying reinforcement learning problem and enables\ngeneralization to unseen environments. We evaluate our approach in a\nphotorealistic 3D simulation and show that it significantly outperforms a\nrandom baseline under both immediate and terminal reward conditions. Our\nresults demonstrate that topological structure and macro-level abstraction can\nenable sample-efficient learning even from pixel data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u4f53\u5bfc\u5411\u5b8f\u52a8\u4f5c\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u62d3\u6251\u5730\u56fe\u7b80\u5316\u5bfc\u822a\u4efb\u52a1\uff0c\u4f7fDQN\u80fd\u9ad8\u6548\u5b66\u4e60\u5bfc\u822a\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u5728\u89c6\u89c9\u590d\u6742\u4e14\u5956\u52b1\u7a00\u758f\u7684\u5927\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7RGBD\u8f93\u5165\u68c0\u6d4b\u7269\u4f53\u5e76\u6784\u5efa\u62d3\u6251\u5730\u56fe\uff0c\u9009\u62e9\u79bb\u6563\u5b8f\u52a8\u4f5c\u5bfc\u822a\u5230\u76ee\u6807\u7269\u4f53\u3002", "result": "\u57283D\u4eff\u771f\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u968f\u673a\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u62d3\u6251\u7ed3\u6784\u548c\u5b8f\u52a8\u4f5c\u7684\u6837\u672c\u9ad8\u6548\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "\u62d3\u6251\u7ed3\u6784\u548c\u5b8f\u52a8\u4f5c\u80fd\u591f\u4ece\u50cf\u7d20\u6570\u636e\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u6837\u672c\u5b66\u4e60\u3002"}}
{"id": "2504.17821", "pdf": "https://arxiv.org/pdf/2504.17821", "abs": "https://arxiv.org/abs/2504.17821", "authors": ["Xinyu Chen", "Yunxin Li", "Haoyuan Shi", "Baotian Hu", "Wenhan Luo", "Yaowei Wang", "Min Zhang"], "title": "VideoVista-CulturalLingo: 360$^\\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Assessing the video comprehension capabilities of multimodal AI systems can\neffectively measure their understanding and reasoning abilities. Most video\nevaluation benchmarks are limited to a single language, typically English, and\npredominantly feature videos rooted in Western cultural contexts. In this\npaper, we present VideoVista-CulturalLingo, the first video evaluation\nbenchmark designed to bridge cultural, linguistic, and domain divide in video\ncomprehension. Our work differs from existing benchmarks in the following ways:\n1) Cultural diversity, incorporating cultures from China, North America, and\nEurope; 2) Multi-linguistics, with questions presented in Chinese and\nEnglish-two of the most widely spoken languages; and 3) Broad domain, featuring\nvideos sourced from hundreds of human-created domains. VideoVista-CulturalLingo\ncontains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent\nopen-source or proprietary video large models. From the experiment results, we\nobserve that: 1) Existing models perform worse on Chinese-centric questions\nthan Western-centric ones, particularly those related to Chinese history; 2)\nCurrent open-source models still exhibit limitations in temporal understanding,\nespecially in the Event Localization task, achieving a maximum score of only\n45.2%; 3) Mainstream models demonstrate strong performance in general\nscientific questions, while open-source models demonstrate weak performance in\nmathematics.", "AI": {"tldr": "VideoVista-CulturalLingo \u662f\u9996\u4e2a\u8de8\u6587\u5316\u3001\u8bed\u8a00\u548c\u9886\u57df\u7684\u89c6\u9891\u7406\u89e3\u8bc4\u6d4b\u57fa\u51c6\uff0c\u5305\u542b1389\u4e2a\u89c6\u9891\u548c3134\u4e2a\u95ee\u7b54\u5bf9\uff0c\u8bc4\u4f30\u4e8624\u4e2a\u89c6\u9891\u5927\u6a21\u578b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u6a21\u578b\u5728\u4e2d\u6587\u95ee\u9898\u3001\u65f6\u95f4\u7406\u89e3\u53ca\u6570\u5b66\u9886\u57df\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8bc4\u6d4b\u57fa\u51c6\u591a\u4e3a\u5355\u4e00\u8bed\u8a00\uff08\u82f1\u8bed\uff09\u548c\u897f\u65b9\u6587\u5316\u80cc\u666f\uff0c\u7f3a\u4e4f\u591a\u6837\u6027\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u8bc4\u6d4b\u6807\u51c6\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e2d\u3001\u7f8e\u3001\u6b27\u6587\u5316\u80cc\u666f\u7684\u53cc\u8bed\uff08\u4e2d\u82f1\uff09\u89c6\u9891\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u9886\u57df\uff0c\u5e76\u8bc4\u4f3024\u4e2a\u4e3b\u6d41\u548c\u5f00\u6e90\u89c6\u9891\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u4e2d\u6587\u95ee\u9898\uff08\u5c24\u5176\u662f\u5386\u53f2\u7c7b\uff09\u3001\u65f6\u95f4\u7406\u89e3\u4efb\u52a1\uff08\u6700\u9ad845.2%\uff09\u548c\u6570\u5b66\u9886\u57df\u8868\u73b0\u8f83\u5f31\uff0c\u79d1\u5b66\u7c7b\u95ee\u9898\u8868\u73b0\u8f83\u597d\u3002", "conclusion": "VideoVista-CulturalLingo \u63ed\u793a\u4e86\u73b0\u6709\u89c6\u9891\u7406\u89e3\u6a21\u578b\u5728\u6587\u5316\u591a\u6837\u6027\u548c\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.18309", "pdf": "https://arxiv.org/pdf/2504.18309", "abs": "https://arxiv.org/abs/2504.18309", "authors": ["Marco Turzi", "Siamak Mehrkanoon"], "title": "SSA-UNet: Advanced Precipitation Nowcasting via Channel Shuffling", "categories": ["cs.LG", "I.2; I.5"], "comment": "8 pages. 8 figs", "summary": "Weather forecasting is essential for facilitating diverse socio-economic\nactivity and environmental conservation initiatives. Deep learning techniques\nare increasingly being explored as complementary approaches to Numerical\nWeather Prediction (NWP) models, offering potential benefits such as reduced\ncomplexity and enhanced adaptability in specific applications. This work\npresents a novel design, Small Shuffled Attention UNet (SSA-UNet), which\nenhances SmaAt-UNet's architecture by including a shuffle channeling mechanism\nto optimize performance and diminish complexity. To assess its efficacy, this\narchitecture and its reduced variant are examined and trained on two datasets:\na Dutch precipitation dataset from 2016 to 2019, and a French cloud cover\ndataset containing radar images from 2017 to 2018. Three output configurations\nof the proposed architecture are evaluated, yielding outputs of 1, 6, and 12\nprecipitation maps, respectively. To better understand how this model operates\nand produces its predictions, a gradient-based approach called Grad-CAM is used\nto analyze the outputs generated. The analysis of heatmaps generated by\nGrad-CAM facilitated the identification of regions within the input maps that\nthe model considers most informative for generating its predictions. The\nimplementation of SSA-UNet can be found on our\nGithub\\footnote{\\href{https://github.com/MarcoTurzi/SSA-UNet}{https://github.com/MarcoTurzi/SSA-UNet}}", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSSA-UNet\u7684\u65b0\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5f15\u5165\u901a\u9053\u6df7\u6d17\u673a\u5236\u4f18\u5316SmaAt-UNet\u67b6\u6784\uff0c\u7528\u4e8e\u5929\u6c14\u9884\u6d4b\u3002\u5b9e\u9a8c\u5728\u8377\u5170\u964d\u6c34\u6570\u636e\u548c\u6cd5\u56fd\u4e91\u5c42\u6570\u636e\u4e0a\u8fdb\u884c\uff0c\u4f7f\u7528Grad-CAM\u5206\u6790\u6a21\u578b\u5173\u6ce8\u533a\u57df\u3002", "motivation": "\u5929\u6c14\u9884\u6d4b\u5bf9\u793e\u4f1a\u7ecf\u6d4e\u548c\u73af\u4fdd\u81f3\u5173\u91cd\u8981\u3002\u6df1\u5ea6\u5b66\u4e60\u53ef\u4f5c\u4e3a\u6570\u503c\u5929\u6c14\u9884\u62a5\u6a21\u578b\u7684\u8865\u5145\uff0c\u964d\u4f4e\u590d\u6742\u6027\u5e76\u63d0\u9ad8\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faSSA-UNet\u67b6\u6784\uff0c\u5f15\u5165\u901a\u9053\u6df7\u6d17\u673a\u5236\u4f18\u5316\u6027\u80fd\u5e76\u7b80\u5316\u6a21\u578b\u3002\u5728\u8377\u5170\u964d\u6c34\u6570\u636e\u96c6\uff082016-2019\uff09\u548c\u6cd5\u56fd\u4e91\u5c42\u6570\u636e\u96c6\uff082017-2018\uff09\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528Grad-CAM\u8fdb\u884c\u9884\u6d4b\u5206\u6790\u3002", "result": "\u8bc4\u4f30\u4e86\u4e09\u79cd\u8f93\u51fa\u914d\u7f6e\uff081\u30016\u300112\u5f20\u964d\u6c34\u56fe\uff09\uff0cGrad-CAM\u70ed\u56fe\u63ed\u793a\u4e86\u6a21\u578b\u5173\u6ce8\u7684\u5173\u952e\u8f93\u5165\u533a\u57df\u3002", "conclusion": "SSA-UNet\u901a\u8fc7\u901a\u9053\u6df7\u6d17\u673a\u5236\u4f18\u5316\u4e86\u5929\u6c14\u9884\u6d4b\u6027\u80fd\uff0cGrad-CAM\u5206\u6790\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.18027", "pdf": "https://arxiv.org/pdf/2504.18027", "abs": "https://arxiv.org/abs/2504.18027", "authors": ["Zezhou Chen", "Zhaoxiang Liu", "Kai Wang", "Kohou Wang", "Shiguo Lian"], "title": "A Large Vision-Language Model based Environment Perception System for Visually Impaired People", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Accepted by IROS2024(9 pages, 8 figures)", "summary": "It is a challenging task for visually impaired people to perceive their\nsurrounding environment due to the complexity of the natural scenes. Their\npersonal and social activities are thus highly limited. This paper introduces a\nLarge Vision-Language Model(LVLM) based environment perception system which\nhelps them to better understand the surrounding environment, by capturing the\ncurrent scene they face with a wearable device, and then letting them retrieve\nthe analysis results through the device. The visually impaired people could\nacquire a global description of the scene by long pressing the screen to\nactivate the LVLM output, retrieve the categories of the objects in the scene\nresulting from a segmentation model by tapping or swiping the screen, and get a\ndetailed description of the objects they are interested in by double-tapping\nthe screen. To help visually impaired people more accurately perceive the\nworld, this paper proposes incorporating the segmentation result of the RGB\nimage as external knowledge into the input of LVLM to reduce the LVLM's\nhallucination. Technical experiments on POPE, MME and LLaVA-QA90 show that the\nsystem could provide a more accurate description of the scene compared to\nQwen-VL-Chat, exploratory experiments show that the system helps visually\nimpaired people to perceive the surrounding environment effectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u73af\u5883\u611f\u77e5\u7cfb\u7edf\uff0c\u5e2e\u52a9\u89c6\u969c\u4eba\u58eb\u901a\u8fc7\u53ef\u7a7f\u6234\u8bbe\u5907\u6355\u6349\u5e76\u7406\u89e3\u5468\u56f4\u73af\u5883\uff0c\u7ed3\u5408\u5206\u5272\u6a21\u578b\u51cf\u5c11\u5e7b\u89c9\uff0c\u63d0\u5347\u63cf\u8ff0\u51c6\u786e\u6027\u3002", "motivation": "\u89c6\u969c\u4eba\u58eb\u7531\u4e8e\u81ea\u7136\u573a\u666f\u7684\u590d\u6742\u6027\u96be\u4ee5\u611f\u77e5\u73af\u5883\uff0c\u9650\u5236\u4e86\u4e2a\u4eba\u548c\u793e\u4ea4\u6d3b\u52a8\u3002\u7cfb\u7edf\u65e8\u5728\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u6539\u5584\u8fd9\u4e00\u73b0\u72b6\u3002", "method": "\u4f7f\u7528\u53ef\u7a7f\u6234\u8bbe\u5907\u6355\u6349\u573a\u666f\uff0c\u7ed3\u5408LVLM\u548c\u5206\u5272\u6a21\u578b\u751f\u6210\u573a\u666f\u63cf\u8ff0\u3002\u5206\u5272\u7ed3\u679c\u4f5c\u4e3a\u5916\u90e8\u77e5\u8bc6\u8f93\u5165\u4ee5\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "\u5728POPE\u3001MME\u548cLLaVA-QA90\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u6bd4Qwen-VL-Chat\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u573a\u666f\u63cf\u8ff0\uff0c\u63a2\u7d22\u6027\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5bf9\u89c6\u969c\u4eba\u58eb\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7cfb\u7edf\u901a\u8fc7\u7ed3\u5408LVLM\u4e0e\u5206\u5272\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u969c\u4eba\u58eb\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2504.17884", "pdf": "https://arxiv.org/pdf/2504.17884", "abs": "https://arxiv.org/abs/2504.17884", "authors": ["Yongkang Li", "Panagiotis Eustratiadis", "Simon Lupart", "Evangelos Kanoulas"], "title": "Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": "This paper has been accepted as a full paper at SIGIR 2025 and will\n  be presented orally", "summary": "This paper concerns corpus poisoning attacks in dense information retrieval,\nwhere an adversary attempts to compromise the ranking performance of a search\nalgorithm by injecting a small number of maliciously generated documents into\nthe corpus. Our work addresses two limitations in the current literature.\nFirst, attacks that perform adversarial gradient-based word substitution search\ndo so in the discrete lexical space, while retrieval itself happens in the\ncontinuous embedding space. We thus propose an optimization method that\noperates in the embedding space directly. Specifically, we train a perturbation\nmodel with the objective of maintaining the geometric distance between the\noriginal and adversarial document embeddings, while also maximizing the\ntoken-level dissimilarity between the original and adversarial documents.\nSecond, it is common for related work to have a strong assumption that the\nadversary has prior knowledge about the queries. In this paper, we focus on a\nmore challenging variant of the problem where the adversary assumes no prior\nknowledge about the query distribution (hence, unsupervised). Our core\ncontribution is an adversarial corpus attack that is fast and effective. We\npresent comprehensive experimental results on both in- and out-of-domain\ndatasets, focusing on two related tasks: a top-1 attack and a corpus poisoning\nattack. We consider attacks under both a white-box and a black-box setting.\nNotably, our method can generate successful adversarial examples in under two\nminutes per target document; four times faster compared to the fastest\ngradient-based word substitution methods in the literature with the same\nhardware. Furthermore, our adversarial generation method generates text that is\nmore likely to occur under the distribution of natural text (low perplexity),\nand is therefore more difficult to detect.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u5bc6\u96c6\u4fe1\u606f\u68c0\u7d22\u7684\u8bed\u6599\u5e93\u6295\u6bd2\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5bf9\u6297\u6027\u6587\u6863\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u4f5c\u5728\u79bb\u6563\u8bcd\u7a7a\u95f4\u64cd\u4f5c\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u65e0\u67e5\u8be2\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u5feb\u901f\u6709\u6548\u7684\u653b\u51fb\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u5728\u5bf9\u6297\u6027\u653b\u51fb\u4e2d\u4e3b\u8981\u5728\u79bb\u6563\u8bcd\u7a7a\u95f4\u64cd\u4f5c\uff0c\u800c\u68c0\u7d22\u5b9e\u9645\u53d1\u751f\u5728\u8fde\u7eed\u5d4c\u5165\u7a7a\u95f4\uff0c\u540c\u65f6\u591a\u6570\u65b9\u6cd5\u5047\u8bbe\u653b\u51fb\u8005\u5df2\u77e5\u67e5\u8be2\u5206\u5e03\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u6270\u52a8\u6a21\u578b\uff0c\u5728\u5d4c\u5165\u7a7a\u95f4\u76f4\u63a5\u4f18\u5316\uff0c\u4fdd\u6301\u539f\u59cb\u4e0e\u5bf9\u6297\u6587\u6863\u7684\u51e0\u4f55\u8ddd\u79bb\uff0c\u540c\u65f6\u6700\u5927\u5316\u8bcd\u7ea7\u5dee\u5f02\u3002\u653b\u51fb\u65b9\u6cd5\u65e0\u9700\u67e5\u8be2\u5148\u9a8c\u77e5\u8bc6\uff0c\u9002\u7528\u4e8e\u767d\u76d2\u548c\u9ed1\u76d2\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e24\u79cd\u653b\u51fb\u4efb\u52a1\uff08top-1\u653b\u51fb\u548c\u8bed\u6599\u5e93\u6295\u6bd2\uff09\u4e2d\u5747\u6709\u6548\uff0c\u751f\u6210\u5bf9\u6297\u6587\u672c\u7684\u901f\u5ea6\u6bd4\u73b0\u6709\u6700\u5feb\u65b9\u6cd5\u5feb4\u500d\uff0c\u4e14\u6587\u672c\u66f4\u81ea\u7136\uff08\u4f4e\u56f0\u60d1\u5ea6\uff09\uff0c\u96be\u4ee5\u68c0\u6d4b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5d4c\u5165\u7a7a\u95f4\u4f18\u5316\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u81ea\u7136\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u8bcd\u7a7a\u95f4\u64cd\u4f5c\u7684\u5c40\u9650\uff0c\u5e76\u5728\u65e0\u67e5\u8be2\u5148\u9a8c\u6761\u4ef6\u4e0b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2504.18329", "pdf": "https://arxiv.org/pdf/2504.18329", "abs": "https://arxiv.org/abs/2504.18329", "authors": ["Anh-Duy Pham", "Olivier Basole Kashongwe", "Martin Atzmueller", "Tim R\u00f6mer"], "title": "PHEATPRUNER: Interpretable Data-centric Feature Selection for Multivariate Time Series Classification through Persistent Homology", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "Balancing performance and interpretability in multivariate time series\nclassification is a significant challenge due to data complexity and high\ndimensionality. This paper introduces PHeatPruner, a method integrating\npersistent homology and sheaf theory to address these challenges. Persistent\nhomology facilitates the pruning of up to 45% of the applied variables while\nmaintaining or enhancing the accuracy of models such as Random Forest,\nCatBoost, XGBoost, and LightGBM, all without depending on posterior\nprobabilities or supervised optimization algorithms. Concurrently, sheaf theory\ncontributes explanatory vectors that provide deeper insights into the data's\nstructural nuances. The approach was validated using the UEA Archive and a\nmastitis detection dataset for dairy cows. The results demonstrate that\nPHeatPruner effectively preserves model accuracy. Furthermore, our results\nhighlight PHeatPruner's key features, i.e. simplifying complex data and\noffering actionable insights without increasing processing time or complexity.\nThis method bridges the gap between complexity reduction and interpretability,\nsuggesting promising applications in various fields.", "AI": {"tldr": "\u63d0\u51fa\u4e86PHeatPruner\u65b9\u6cd5\uff0c\u7ed3\u5408\u6301\u7eed\u540c\u8c03\u548c\u5c42\u8bba\uff0c\u5e73\u8861\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u663e\u8457\u964d\u7ef4\u4e14\u4e0d\u635f\u5931\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u9ad8\u7ef4\u6570\u636e\u590d\u6742\u6027\u4e0e\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u6574\u5408\u6301\u7eed\u540c\u8c03\uff08\u964d\u7ef4\uff09\u548c\u5c42\u8bba\uff08\u89e3\u91ca\u6027\uff09\uff0c\u65e0\u9700\u540e\u9a8c\u6982\u7387\u6216\u76d1\u7763\u4f18\u5316\uff0c\u652f\u6301\u591a\u79cd\u6a21\u578b\u5982\u968f\u673a\u68ee\u6797\u3001CatBoost\u7b49\u3002", "result": "\u5728UEA Archive\u548c\u5976\u725b\u4e73\u817a\u708e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u53ef\u524a\u51cf45%\u53d8\u91cf\u5e76\u4fdd\u6301\u7cbe\u5ea6\uff0c\u540c\u65f6\u63d0\u4f9b\u7ed3\u6784\u89e3\u91ca\u3002", "conclusion": "PHeatPruner\u5728\u7b80\u5316\u6570\u636e\u548c\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u6548\u679c\u663e\u8457\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.18353", "pdf": "https://arxiv.org/pdf/2504.18353", "abs": "https://arxiv.org/abs/2504.18353", "authors": ["Roya Nasiri"], "title": "Testing Individual Fairness in Graph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages", "summary": "The biases in artificial intelligence (AI) models can lead to automated\ndecision-making processes that discriminate against groups and/or individuals\nbased on sensitive properties such as gender and race. While there are many\nstudies on diagnosing and mitigating biases in various AI models, there is\nlittle research on individual fairness in Graph Neural Networks (GNNs). Unlike\ntraditional models, which treat data features independently and overlook their\ninter-relationships, GNNs are designed to capture graph-based structure where\nnodes are interconnected. This relational approach enables GNNs to model\ncomplex dependencies, but it also means that biases can propagate through these\nconnections, complicating the detection and mitigation of individual fairness\nviolations. This PhD project aims to develop a testing framework to assess and\nensure individual fairness in GNNs. It first systematically reviews the\nliterature on individual fairness, categorizing existing approaches to define,\nmeasure, test, and mitigate model biases, creating a taxonomy of individual\nfairness. Next, the project will develop a framework for testing and ensuring\nfairness in GNNs by adapting and extending current fairness testing and\nmitigation techniques. The framework will be evaluated through industrial case\nstudies, focusing on graph-based large language models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u805a\u7126\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u4e2d\u7684\u4e2a\u4f53\u516c\u5e73\u6027\u95ee\u9898\uff0c\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u6d4b\u8bd5\u6846\u67b6\u4ee5\u8bc4\u4f30\u548c\u786e\u4fddGNNs\u7684\u516c\u5e73\u6027\u3002", "motivation": "\u7531\u4e8eGNNs\u7684\u7279\u6b8a\u7ed3\u6784\uff0c\u5176\u4e2d\u7684\u504f\u89c1\u53ef\u80fd\u901a\u8fc7\u8282\u70b9\u95f4\u7684\u8fde\u63a5\u4f20\u64ad\uff0c\u8fd9\u4e00\u95ee\u9898\u5728\u73b0\u6709\u7814\u7a76\u4e2d\u8f83\u5c11\u88ab\u5173\u6ce8\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u7684\u516c\u5e73\u6027\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u9996\u5148\u7cfb\u7edf\u56de\u987e\u6587\u732e\uff0c\u5efa\u7acb\u4e2a\u4f53\u516c\u5e73\u6027\u7684\u5206\u7c7b\u6cd5\uff0c\u7136\u540e\u5f00\u53d1\u4e00\u4e2a\u6846\u67b6\uff0c\u7ed3\u5408\u73b0\u6709\u7684\u516c\u5e73\u6027\u6d4b\u8bd5\u548c\u7f13\u89e3\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u5de5\u4e1a\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6700\u7ec8\u5c06\u63d0\u51fa\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9GNNs\u7684\u516c\u5e73\u6027\u6d4b\u8bd5\u6846\u67b6\uff0c\u5e76\u5728\u56fe\u57fa\u7840\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86GNNs\u9886\u57df\u4e2a\u4f53\u516c\u5e73\u6027\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u590d\u6742\u56fe\u7ed3\u6784\u4e2d\u7684\u504f\u89c1\u68c0\u6d4b\u548c\u7f13\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.18044", "pdf": "https://arxiv.org/pdf/2504.18044", "abs": "https://arxiv.org/abs/2504.18044", "authors": ["Omid Veisi", "Sasan Bahrami", "Roman Englert", "Claudia M\u00fcller"], "title": "AI Ethics and Social Norms: Exploring ChatGPT's Capabilities From What to How", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.IT", "math.IT"], "comment": "Accepted for presentation at the ACM Conference on Computer-Supported\n  Cooperative Work and Social Computing (CSCW) 2025. To appear in Proceedings\n  of the ACM on Human-Computer Interaction (PACM HCI)", "summary": "Using LLMs in healthcare, Computer-Supported Cooperative Work, and Social\nComputing requires the examination of ethical and social norms to ensure safe\nincorporation into human life. We conducted a mixed-method study, including an\nonline survey with 111 participants and an interview study with 38 experts, to\ninvestigate the AI ethics and social norms in ChatGPT as everyday life tools.\nThis study aims to evaluate whether ChatGPT in an empirical context operates\nfollowing ethics and social norms, which is critical for understanding actions\nin industrial and academic research and achieving machine ethics. The findings\nof this study provide initial insights into six important aspects of AI ethics,\nincluding bias, trustworthiness, security, toxicology, social norms, and\nethical data. Significant obstacles related to transparency and bias in\nunsupervised data collection methods are identified as ChatGPT's ethical\nconcerns.", "AI": {"tldr": "\u6458\u8981\u8ba8\u8bba\u4e86\u5728\u533b\u7597\u3001CSCW\u548c\u793e\u4ea4\u8ba1\u7b97\u4e2d\u4f7f\u7528LLM\u65f6\u9700\u5ba1\u67e5\u4f26\u7406\u548c\u793e\u4f1a\u89c4\u8303\uff0c\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff08\u8c03\u67e5\u548c\u4e13\u5bb6\u8bbf\u8c08\uff09\u8bc4\u4f30ChatGPT\u662f\u5426\u7b26\u5408\u4f26\u7406\u548c\u793e\u4f1a\u89c4\u8303\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u786e\u4fddChatGPT\u7b49\u5de5\u5177\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9075\u5faa\u4f26\u7406\u548c\u793e\u4f1a\u89c4\u8303\uff0c\u4e3a\u5de5\u4e1a\u4e0e\u5b66\u672f\u7814\u7a76\u63d0\u4f9b\u884c\u52a8\u6307\u5bfc\uff0c\u5b9e\u73b0\u673a\u5668\u4f26\u7406\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u5305\u62ec111\u4eba\u7684\u5728\u7ebf\u8c03\u67e5\u548c38\u4f4d\u4e13\u5bb6\u7684\u8bbf\u8c08\u7814\u7a76\u3002", "result": "\u521d\u6b65\u63ed\u793a\u4e86AI\u4f26\u7406\u7684\u516d\u4e2a\u5173\u952e\u65b9\u9762\uff08\u504f\u89c1\u3001\u53ef\u4fe1\u5ea6\u3001\u5b89\u5168\u7b49\uff09\uff0c\u5e76\u6307\u51faChatGPT\u5728\u900f\u660e\u5ea6\u548c\u65e0\u76d1\u7763\u6570\u636e\u6536\u96c6\u4e2d\u7684\u504f\u89c1\u95ee\u9898\u662f\u4e3b\u8981\u4f26\u7406\u96be\u9898\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86AI\u5de5\u5177\u9700\u7b26\u5408\u4f26\u7406\u548c\u793e\u4f1a\u89c4\u8303\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8bc6\u522b\u4e86ChatGPT\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4e3b\u8981\u4f26\u7406\u6311\u6218\u3002"}}
{"id": "2504.17902", "pdf": "https://arxiv.org/pdf/2504.17902", "abs": "https://arxiv.org/abs/2504.17902", "authors": ["Girish A. Koushik", "Diptesh Kanojia", "Helen Treharne", "Aditya Joshi"], "title": "CAMU: Context Augmentation for Meme Understanding", "categories": ["cs.CV", "cs.CL"], "comment": "Under review at ACM MM 2025", "summary": "Social media memes are a challenging domain for hate detection because they\nintertwine visual and textual cues into culturally nuanced messages. We\nintroduce a novel framework, CAMU, which leverages large vision-language models\nto generate more descriptive captions, a caption-scoring neural network to\nemphasise hate-relevant content, and parameter-efficient fine-tuning of CLIP's\ntext encoder for an improved multimodal understanding of memes. Experiments on\npublicly available hateful meme datasets show that simple projection layer\nfine-tuning yields modest gains, whereas selectively tuning deeper text encoder\nlayers significantly boosts performance on all evaluation metrics. Moreover,\nour approach attains high accuracy (0.807) and F1-score (0.806) on the Hateful\nMemes dataset, at par with the existing SoTA framework while being much more\nefficient, offering practical advantages in real-world scenarios that rely on\nfixed decision thresholds. CAMU also achieves the best F1-score of 0.673 on the\nMultiOFF dataset for offensive meme identification, demonstrating its\ngeneralisability. Additional analyses on benign confounders reveal that robust\nvisual grounding and nuanced text representations are crucial for reliable hate\nand offence detection. We will publicly release CAMU along with the resultant\nmodels for further research.\n  Disclaimer: This paper includes references to potentially disturbing,\nhateful, or offensive content due to the nature of the task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCAMU\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u66f4\u8be6\u7ec6\u7684\u63cf\u8ff0\u6027\u6807\u9898\u3001\u5f3a\u5316\u4ec7\u6068\u76f8\u5173\u5185\u5bb9\u8bc4\u5206\u4ee5\u53ca\u9ad8\u6548\u8c03\u6574CLIP\u6587\u672c\u7f16\u7801\u5668\u6765\u63d0\u5347\u5bf9\u793e\u4ea4\u5a92\u4f53\u6a21\u56e0\u4e2d\u4ec7\u6068\u5185\u5bb9\u7684\u591a\u6a21\u6001\u7406\u89e3\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u6a21\u56e0\u56e0\u5176\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u7ebf\u7d22\u7684\u6587\u5316\u590d\u6742\u6027\uff0c\u5bf9\u4ec7\u6068\u5185\u5bb9\u68c0\u6d4b\u63d0\u51fa\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u591a\u6a21\u6001\u7406\u89e3\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u63cf\u8ff0\u6027\u6807\u9898\uff0c\u5229\u7528\u6807\u9898\u8bc4\u5206\u795e\u7ecf\u7f51\u7edc\u7a81\u51fa\u4ec7\u6068\u76f8\u5173\u5185\u5bb9\uff0c\u5e76\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03CLIP\u6587\u672c\u7f16\u7801\u5668\u4f18\u5316\u591a\u6a21\u6001\u7406\u89e3\u3002", "result": "\u5728\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728Hateful Memes\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u548cF1\u5206\u6570\u8fbe\u52300.807\u548c0.806\uff0c\u540c\u65f6\u5728MultiOFF\u6570\u636e\u96c6\u4e0aF1\u5206\u6570\u4e3a0.673\uff0c\u5c55\u793a\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u5f3a\u5927\u7684\u89c6\u89c9\u57fa\u7840\u548c\u7ec6\u81f4\u7684\u6587\u672c\u8868\u793a\u5bf9\u53ef\u9760\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0cCAMU\u6846\u67b6\u4e0d\u4ec5\u9ad8\u6548\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2504.18371", "pdf": "https://arxiv.org/pdf/2504.18371", "abs": "https://arxiv.org/abs/2504.18371", "authors": ["Irshad A. Meer", "Bruno H\u00f6rmann", "Mustafa Ozger", "Fabien Geyer", "Alberto Viseras", "Dominic Schupke", "Cicek Cavdar"], "title": "Explainable AI for UAV Mobility Management: A Deep Q-Network Approach for Handover Minimization", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "Submitted to IEEE PIMRC 2025", "summary": "The integration of unmanned aerial vehicles (UAVs) into cellular networks\npresents significant mobility management challenges, primarily due to frequent\nhandovers caused by probabilistic line-of-sight conditions with multiple ground\nbase stations (BSs). To tackle these challenges, reinforcement learning\n(RL)-based methods, particularly deep Q-networks (DQN), have been employed to\noptimize handover decisions dynamically. However, a major drawback of these\nlearning-based approaches is their black-box nature, which limits\ninterpretability in the decision-making process. This paper introduces an\nexplainable AI (XAI) framework that incorporates Shapley Additive Explanations\n(SHAP) to provide deeper insights into how various state parameters influence\nhandover decisions in a DQN-based mobility management system. By quantifying\nthe impact of key features such as reference signal received power (RSRP),\nreference signal received quality (RSRQ), buffer status, and UAV position, our\napproach enhances the interpretability and reliability of RL-based handover\nsolutions. To validate and compare our framework, we utilize real-world network\nperformance data collected from UAV flight trials. Simulation results show that\nour method provides intuitive explanations for policy decisions, effectively\nbridging the gap between AI-driven models and human decision-makers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53ef\u89e3\u91caAI\uff08XAI\uff09\u548cSHAP\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u57fa\u4e8eDQN\u7684\u65e0\u4eba\u673a\u79fb\u52a8\u7ba1\u7406\u7cfb\u7edf\u4e2d\u5207\u6362\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u4eba\u673a\u79fb\u52a8\u7ba1\u7406\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u91c7\u7528\u53ef\u89e3\u91caAI\uff08XAI\uff09\u6846\u67b6\u548cSHAP\u65b9\u6cd5\uff0c\u91cf\u5316\u5173\u952e\u7279\u5f81\uff08\u5982RSRP\u3001RSRQ\u3001\u7f13\u51b2\u533a\u72b6\u6001\u548c\u65e0\u4eba\u673a\u4f4d\u7f6e\uff09\u5bf9\u5207\u6362\u51b3\u7b56\u7684\u5f71\u54cd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u76f4\u89c2\u89e3\u91ca\u7b56\u7565\u51b3\u7b56\uff0c\u7f29\u5c0fAI\u6a21\u578b\u4e0e\u4eba\u7c7b\u51b3\u7b56\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86RL\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u65e0\u4eba\u673a\u7f51\u7edc\u4e2d\u7684\u79fb\u52a8\u7ba1\u7406\u3002"}}
{"id": "2504.18046", "pdf": "https://arxiv.org/pdf/2504.18046", "abs": "https://arxiv.org/abs/2504.18046", "authors": ["Guohao Huo", "Zibo Lin", "Zitong Wang", "Ruiting Dai", "Hao Tang"], "title": "DMS-Net:Dual-Modal Multi-Scale Siamese Network for Binocular Fundus Image Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Ophthalmic diseases pose a significant global health challenge, yet\ntraditional diagnosis methods and existing single-eye deep learning approaches\noften fail to account for binocular pathological correlations. To address this,\nwe propose DMS-Net, a dual-modal multi-scale Siamese network for binocular\nfundus image classification. Our framework leverages weight-shared Siamese\nResNet-152 backbones to extract deep semantic features from paired fundus\nimages. To tackle challenges such as lesion boundary ambiguity and scattered\npathological distributions, we introduce a Multi-Scale Context-Aware Module\n(MSCAM) that integrates adaptive pooling and attention mechanisms for\nmulti-resolution feature aggregation. Additionally, a Dual-Modal Feature Fusion\n(DMFF) module enhances cross-modal interaction through spatial-semantic\nrecalibration and bidirectional attention, effectively combining global context\nand local edge features. Evaluated on the ODIR-5K dataset, DMS-Net achieves\nstate-of-the-art performance with 80.5% accuracy, 86.1% recall, and 83.8%\nCohen's kappa, demonstrating superior capability in detecting symmetric\npathologies and advancing clinical decision-making for ocular diseases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DMS-Net\uff0c\u4e00\u79cd\u7528\u4e8e\u53cc\u76ee\u773c\u5e95\u56fe\u50cf\u5206\u7c7b\u7684\u53cc\u6a21\u6001\u591a\u5c3a\u5ea6Siamese\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u5757\u548c\u53cc\u6a21\u6001\u7279\u5f81\u878d\u5408\u6a21\u5757\u63d0\u5347\u6027\u80fd\uff0c\u5728ODIR-5K\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u8bca\u65ad\u65b9\u6cd5\u548c\u73b0\u6709\u5355\u773c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u672a\u80fd\u5145\u5206\u8003\u8651\u5230\u53cc\u773c\u75c5\u7406\u5173\u8054\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5229\u7528\u53cc\u76ee\u56fe\u50cf\u4fe1\u606f\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002", "method": "DMS-Net\u7ed3\u5408\u4e86\u6743\u91cd\u5171\u4eab\u7684Siamese ResNet-152\u4e3b\u5e72\u7f51\u7edc\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u5757\uff08MSCAM\uff09\u548c\u53cc\u6a21\u6001\u7279\u5f81\u878d\u5408\u6a21\u5757\uff08DMFF\uff09\u6765\u589e\u5f3a\u7279\u5f81\u8868\u793a\u548c\u8de8\u6a21\u6001\u4ea4\u4e92\u3002", "result": "\u5728ODIR-5K\u6570\u636e\u96c6\u4e0a\uff0cDMS-Net\u8fbe\u5230\u4e8680.5%\u7684\u51c6\u786e\u7387\u300186.1%\u7684\u53ec\u56de\u7387\u548c83.8%\u7684Cohen's kappa\u503c\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DMS-Net\u5728\u68c0\u6d4b\u5bf9\u79f0\u6027\u75c5\u7406\u548c\u63d0\u9ad8\u773c\u79d1\u75be\u75c5\u4e34\u5e8a\u51b3\u7b56\u80fd\u529b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u773c\u79d1\u75be\u75c5\u7684\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17934", "pdf": "https://arxiv.org/pdf/2504.17934", "abs": "https://arxiv.org/abs/2504.17934", "authors": ["Chaoran Chen", "Zhiping Zhang", "Ibrahim Khalilov", "Bingcan Guo", "Simret A Gebreegziabher", "Yanfang Ye", "Ziang Xiao", "Yaxing Yao", "Tianshi Li", "Toby Jia-Jun Li"], "title": "Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents", "categories": ["cs.HC", "cs.CL", "cs.CR"], "comment": null, "summary": "The rise of Large Language Models (LLMs) has revolutionized Graphical User\nInterface (GUI) automation through LLM-powered GUI agents, yet their ability to\nprocess sensitive data with limited human oversight raises significant privacy\nand security risks. This position paper identifies three key risks of GUI\nagents and examines how they differ from traditional GUI automation and general\nautonomous agents. Despite these risks, existing evaluations focus primarily on\nperformance, leaving privacy and security assessments largely unexplored. We\nreview current evaluation metrics for both GUI and general LLM agents and\noutline five key challenges in integrating human evaluators for GUI agent\nassessments. To address these gaps, we advocate for a human-centered evaluation\nframework that incorporates risk assessments, enhances user awareness through\nin-context consent, and embeds privacy and security considerations into GUI\nagent design and evaluation.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86LLM\u9a71\u52a8\u7684GUI\u4ee3\u7406\u5728\u5904\u7406\u654f\u611f\u6570\u636e\u65f6\u7684\u9690\u79c1\u548c\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u5021\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u968f\u7740LLM\u5728GUI\u81ea\u52a8\u5316\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5904\u7406\u654f\u611f\u6570\u636e\u53ef\u80fd\u5e26\u6765\u7684\u9690\u79c1\u548c\u5b89\u5168\u98ce\u9669\u5c1a\u672a\u88ab\u5145\u5206\u8bc4\u4f30\uff0c\u9700\u8981\u66f4\u591a\u5173\u6ce8\u3002", "method": "\u6587\u7ae0\u5206\u6790\u4e86GUI\u4ee3\u7406\u7684\u4e09\u5927\u98ce\u9669\uff0c\u56de\u987e\u73b0\u6709\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u63d0\u51fa\u6574\u5408\u4eba\u7c7b\u8bc4\u4f30\u8005\u7684\u4e94\u5927\u6311\u6218\u3002", "result": "\u53d1\u73b0\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u6027\u80fd\uff0c\u9690\u79c1\u548c\u5b89\u5168\u8bc4\u4f30\u88ab\u5ffd\u89c6\uff0c\u63d0\u51fa\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "\u547c\u5401\u5728\u8bbe\u8ba1GUI\u4ee3\u7406\u65f6\u5c06\u9690\u79c1\u548c\u5b89\u5168\u8003\u8651\u7eb3\u5165\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u77e5\u60c5\u540c\u610f\u589e\u5f3a\u98ce\u9669\u610f\u8bc6\u3002"}}
{"id": "2504.18385", "pdf": "https://arxiv.org/pdf/2504.18385", "abs": "https://arxiv.org/abs/2504.18385", "authors": ["Danial Dervovic", "Michael Cashmore"], "title": "Model Evaluation in the Dark: Robust Classifier Metrics with Missing Labels", "categories": ["cs.LG", "stat.ML"], "comment": "9 pages, 4 figures. Accepted to AISTATS 2025", "summary": "Missing data in supervised learning is well-studied, but the specific issue\nof missing labels during model evaluation has been overlooked. Ignoring samples\nwith missing values, a common solution, can introduce bias, especially when\ndata is Missing Not At Random (MNAR). We propose a multiple imputation\ntechnique for evaluating classifiers using metrics such as precision, recall,\nand ROC-AUC. This method not only offers point estimates but also a predictive\ndistribution for these quantities when labels are missing. We empirically show\nthat the predictive distribution's location and shape are generally correct,\neven in the MNAR regime. Moreover, we establish that this distribution is\napproximately Gaussian and provide finite-sample convergence bounds.\nAdditionally, a robustness proof is presented, confirming the validity of the\napproximation under a realistic error model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u91cd\u63d2\u8865\u6280\u672f\uff0c\u7528\u4e8e\u8bc4\u4f30\u5206\u7c7b\u5668\u5728\u6807\u7b7e\u7f3a\u5931\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u9884\u6d4b\u5206\u5e03\uff0c\u5c24\u5176\u5728\u6570\u636e\u975e\u968f\u673a\u7f3a\u5931\uff08MNAR\uff09\u65f6\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u76ee\u524d\u7814\u7a76\u591a\u5173\u6ce8\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u4f46\u5ffd\u89c6\u4e86\u6a21\u578b\u8bc4\u4f30\u4e2d\u6807\u7b7e\u7f3a\u5931\u7684\u5f71\u54cd\u3002\u5e38\u89c1\u65b9\u6cd5\u662f\u5ffd\u7565\u7f3a\u5931\u6837\u672c\uff0c\u4f46\u8fd9\u53ef\u80fd\u5bfc\u81f4\u504f\u5dee\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u7f3a\u53e3\u3002", "method": "\u91c7\u7528\u591a\u91cd\u63d2\u8865\u6280\u672f\uff0c\u901a\u8fc7\u751f\u6210\u9884\u6d4b\u5206\u5e03\u6765\u4f30\u8ba1\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cROC-AUC\u7b49\u6307\u6807\uff0c\u5c24\u5176\u5728MNAR\u60c5\u51b5\u4e0b\u8868\u73b0\u7a33\u5065\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9884\u6d4b\u5206\u5e03\u7684\u4f4d\u7f6e\u548c\u5f62\u72b6\u57fa\u672c\u6b63\u786e\uff0c\u4e14\u8fd1\u4f3c\u9ad8\u65af\u5206\u5e03\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u6709\u9650\u6837\u672c\u6536\u655b\u754c\u9650\u548c\u7a33\u5065\u6027\u8bc1\u660e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u70b9\u4f30\u8ba1\uff0c\u8fd8\u80fd\u5728\u6807\u7b7e\u7f3a\u5931\u65f6\u7ed9\u51fa\u9884\u6d4b\u5206\u5e03\uff0c\u9002\u7528\u4e8eMNAR\u7b49\u590d\u6742\u573a\u666f\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2504.18049", "pdf": "https://arxiv.org/pdf/2504.18049", "abs": "https://arxiv.org/abs/2504.18049", "authors": ["Xin Li", "Wenhui Zhu", "Peijie Qiu", "Oana M. Dumitrascu", "Amal Youssef", "Yalin Wang"], "title": "A BERT-Style Self-Supervised Learning CNN for Disease Identification from Retinal Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In the field of medical imaging, the advent of deep learning, especially the\napplication of convolutional neural networks (CNNs) has revolutionized the\nanalysis and interpretation of medical images. Nevertheless, deep learning\nmethods usually rely on large amounts of labeled data. In medical imaging\nresearch, the acquisition of high-quality labels is both expensive and\ndifficult. The introduction of Vision Transformers (ViT) and self-supervised\nlearning provides a pre-training strategy that utilizes abundant unlabeled\ndata, effectively alleviating the label acquisition challenge while broadening\nthe breadth of data utilization. However, ViT's high computational density and\nsubstantial demand for computing power, coupled with the lack of localization\ncharacteristics of its operations on image patches, limit its efficiency and\napplicability in many application scenarios. In this study, we employ\nnn-MobileNet, a lightweight CNN framework, to implement a BERT-style\nself-supervised learning approach. We pre-train the network on the unlabeled\nretinal fundus images from the UK Biobank to improve downstream application\nperformance. We validate the results of the pre-trained model on Alzheimer's\ndisease (AD), Parkinson's disease (PD), and various retinal diseases\nidentification. The results show that our approach can significantly improve\nperformance in the downstream tasks. In summary, this study combines the\nbenefits of CNNs with the capabilities of advanced self-supervised learning in\nhandling large-scale unlabeled data, demonstrating the potential of CNNs in the\npresence of label scarcity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f7b\u91cf\u7ea7CNN\u6846\u67b6nn-MobileNet\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u533b\u5b66\u5f71\u50cf\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u75be\u75c5\u8bc6\u522b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u5206\u6790\u9886\u57df\u867d\u53d7\u76ca\u4e8e\u6df1\u5ea6\u5b66\u4e60\uff0c\u4f46\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u4e14\u6602\u8d35\u3002\u4f20\u7edfViT\u65b9\u6cd5\u8ba1\u7b97\u5bc6\u96c6\u4e14\u7f3a\u4e4f\u5c40\u90e8\u6027\u7279\u5f81\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u8f7b\u91cf\u7ea7CNN\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u9ad8\u6548\u5229\u7528\u65e0\u6807\u6ce8\u6570\u636e\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "method": "\u91c7\u7528nn-MobileNet\u8f7b\u91cfCNN\u6846\u67b6\uff0c\u7ed3\u5408BERT\u98ce\u683c\u7684\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5728UK Biobank\u7684\u65e0\u6807\u6ce8\u89c6\u7f51\u819c\u56fe\u50cf\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u968f\u540e\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u3001\u5e15\u91d1\u68ee\u75c5\u53ca\u591a\u79cd\u89c6\u7f51\u819c\u75be\u75c5\u8bc6\u522b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u9a8c\u8bc1\u4e86\u8f7b\u91cfCNN\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u8f7b\u91cfCNN\u4e0e\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u6027\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17950", "pdf": "https://arxiv.org/pdf/2504.17950", "abs": "https://arxiv.org/abs/2504.17950", "authors": ["Isadora White", "Kolby Nottingham", "Ayush Maniar", "Max Robinson", "Hansen Lillemark", "Mehul Maheshwari", "Lianhui Qin", "Prithviraj Ammanabrolu"], "title": "Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning", "categories": ["cs.MA", "cs.CL"], "comment": "9 pages of main paper with 6 main figures, overall 28 pages", "summary": "Collaboration is ubiquitous and essential in day-to-day life -- from\nexchanging ideas, to delegating tasks, to generating plans together. This work\nstudies how LLMs can adaptively collaborate to perform complex embodied\nreasoning tasks. To this end we introduce MINDcraft, an easily extensible\nplatform built to enable LLM agents to control characters in the open-world\ngame of Minecraft; and MineCollab, a benchmark to test the different dimensions\nof embodied and collaborative reasoning. An experimental study finds that the\nprimary bottleneck in collaborating effectively for current state-of-the-art\nagents is efficient natural language communication, with agent performance\ndropping as much as 15% when they are required to communicate detailed task\ncompletion plans. We conclude that existing LLM agents are ill-optimized for\nmulti-agent collaboration, especially in embodied scenarios, and highlight the\nneed to employ methods beyond in-context and imitation learning. Our website\ncan be found here: https://mindcraft-minecollab.github.io/", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86LLM\u5982\u4f55\u901a\u8fc7\u81ea\u9002\u5e94\u534f\u4f5c\u5b8c\u6210\u590d\u6742\u7684\u5177\u8eab\u63a8\u7406\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86MINDcraft\u5e73\u53f0\u548cMineCollab\u57fa\u51c6\u6d4b\u8bd5\u3002\u5b9e\u9a8c\u53d1\u73b0\u5f53\u524d\u5148\u8fdb\u7684LLM\u4ee3\u7406\u5728\u81ea\u7136\u8bed\u8a00\u4ea4\u6d41\u4e2d\u5b58\u5728\u74f6\u9888\uff0c\u534f\u4f5c\u6027\u80fd\u4e0b\u964d\u53ef\u8fbe15%\u3002\u7ed3\u8bba\u8868\u660e\u73b0\u6709LLM\u4ee3\u7406\u5728\u591a\u4ee3\u7406\u534f\u4f5c\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22LLM\u5982\u4f55\u5728\u5f00\u653e\u7684Minecraft\u6e38\u620f\u4e2d\u901a\u8fc7\u534f\u4f5c\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5728\u5177\u8eab\u573a\u666f\u4e2d\u5982\u4f55\u9ad8\u6548\u534f\u4f5c\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5f00\u53d1MINDcraft\u5e73\u53f0\u548cMineCollab\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u6d4b\u8bd5LLM\u4ee3\u7406\u5728\u5177\u8eab\u548c\u534f\u4f5c\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u5148\u8fdb\u7684LLM\u4ee3\u7406\u5728\u9700\u8981\u8be6\u7ec6\u4efb\u52a1\u8ba1\u5212\u4ea4\u6d41\u65f6\uff0c\u534f\u4f5c\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe15%\uff0c\u8868\u660e\u81ea\u7136\u8bed\u8a00\u4ea4\u6d41\u662f\u4e3b\u8981\u74f6\u9888\u3002", "conclusion": "\u8bba\u6587\u7ed3\u8bba\u6307\u51fa\uff0c\u73b0\u6709LLM\u4ee3\u7406\u5728\u591a\u4ee3\u7406\u534f\u4f5c\uff08\u5c24\u5176\u662f\u5177\u8eab\u573a\u666f\uff09\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u4e4b\u5916\u63a2\u7d22\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2504.18393", "pdf": "https://arxiv.org/pdf/2504.18393", "abs": "https://arxiv.org/abs/2504.18393", "authors": ["Marina Andric", "Mauro Dragoni"], "title": "Machine Learning and Statistical Insights into Hospital Stay Durations: The Italian EHR Case", "categories": ["cs.LG"], "comment": null, "summary": "Length of hospital stay is a critical metric for assessing healthcare quality\nand optimizing hospital resource management. This study aims to identify\nfactors influencing LoS within the Italian healthcare context, using a dataset\nof hospitalization records from over 60 healthcare facilities in the Piedmont\nregion, spanning from 2020 to 2023. We explored a variety of features,\nincluding patient characteristics, comorbidities, admission details, and\nhospital-specific factors. Significant correlations were found between LoS and\nfeatures such as age group, comorbidity score, admission type, and the month of\nadmission. Machine learning models, specifically CatBoost and Random Forest,\nwere used to predict LoS. The highest R2 score, 0.49, was achieved with\nCatBoost, demonstrating good predictive performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u6790\u610f\u5927\u5229\u76ae\u57c3\u8499\u7279\u5730\u533a60\u591a\u5bb6\u533b\u7597\u673a\u67842020-2023\u5e74\u7684\u4f4f\u9662\u8bb0\u5f55\uff0c\u5229\u7528CatBoost\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\u9884\u6d4b\u4f4f\u9662\u65f6\u957f\uff08LoS\uff09\uff0c\u53d1\u73b0\u5e74\u9f84\u3001\u5408\u5e76\u75c7\u8bc4\u5206\u3001\u5165\u9662\u7c7b\u578b\u548c\u6708\u4efd\u7b49\u56e0\u7d20\u4e0eLoS\u663e\u8457\u76f8\u5173\uff0cCatBoost\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08R2=0.49\uff09\u3002", "motivation": "\u4f4f\u9662\u65f6\u957f\uff08LoS\uff09\u662f\u8861\u91cf\u533b\u7597\u8d28\u91cf\u548c\u4f18\u5316\u533b\u9662\u8d44\u6e90\u7ba1\u7406\u7684\u5173\u952e\u6307\u6807\u3002\u7814\u7a76\u65e8\u5728\u8bc6\u522b\u610f\u5927\u5229\u533b\u7597\u80cc\u666f\u4e0b\u5f71\u54cdLoS\u7684\u56e0\u7d20\u3002", "method": "\u4f7f\u752860\u591a\u5bb6\u533b\u7597\u673a\u67842020-2023\u5e74\u7684\u4f4f\u9662\u8bb0\u5f55\uff0c\u5206\u6790\u60a3\u8005\u7279\u5f81\u3001\u5408\u5e76\u75c7\u3001\u5165\u9662\u4fe1\u606f\u548c\u533b\u9662\u56e0\u7d20\uff0c\u5e76\u91c7\u7528CatBoost\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5e74\u9f84\u7ec4\u3001\u5408\u5e76\u75c7\u8bc4\u5206\u3001\u5165\u9662\u7c7b\u578b\u548c\u6708\u4efd\u4e0eLoS\u663e\u8457\u76f8\u5173\uff0cCatBoost\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08R2=0.49\uff09\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u8bc6\u522b\u4e86\u5f71\u54cdLoS\u7684\u5173\u952e\u56e0\u7d20\uff0cCatBoost\u6a21\u578b\u5728\u9884\u6d4bLoS\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4f18\u5316\u533b\u7597\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2504.18050", "pdf": "https://arxiv.org/pdf/2504.18050", "abs": "https://arxiv.org/abs/2504.18050", "authors": ["Mingwei Zheng", "Danning Xie", "Qingkai Shi", "Chengpeng Wang", "Xiangyu Zhang"], "title": "Validating Network Protocol Parsers with Traceable RFC Document Interpretation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Validating the correctness of network protocol implementations is highly\nchallenging due to the oracle and traceability problems. The former determines\nwhen a protocol implementation can be considered buggy, especially when the\nbugs do not cause any observable symptoms. The latter allows developers to\nunderstand how an implementation violates the protocol specification, thereby\nfacilitating bug fixes. Unlike existing works that rarely take both problems\ninto account, this work considers both and provides an effective solution using\nrecent advances in large language models (LLMs). Our key observation is that\nnetwork protocols are often released with structured specification documents,\na.k.a. RFC documents, which can be systematically translated to formal protocol\nmessage specifications via LLMs. Such specifications, which may contain errors\ndue to the hallucination of LLMs, are used as a quasi-oracle to validate\nprotocol parsers, while the validation results in return gradually refine the\noracle. Since the oracle is derived from the document, any bugs we find in a\nprotocol implementation can be traced back to the document, thus addressing the\ntraceability problem. We have extensively evaluated our approach using nine\nnetwork protocols and their implementations written in C, Python, and Go. The\nresults show that our approach outperforms the state-of-the-art and has\ndetected 69 bugs, with 36 confirmed. The project also demonstrates the\npotential for fully automating software validation based on natural language\nspecifications, a process previously considered predominantly manual due to the\nneed to understand specification documents and derive expected outputs for test\ninputs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u4ece\u534f\u8bae\u89c4\u8303\u6587\u6863\u751f\u6210\u5f62\u5f0f\u5316\u534f\u8bae\u6d88\u606f\u89c4\u8303\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u7f51\u7edc\u534f\u8bae\u5b9e\u73b0\u9a8c\u8bc1\u4e2d\u7684oracle\u548ctraceability\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u534f\u8bae\u5b9e\u73b0\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7f51\u7edc\u534f\u8bae\u5b9e\u73b0\u7684\u6b63\u786e\u6027\u9a8c\u8bc1\u9762\u4e34oracle\u95ee\u9898\uff08\u5982\u4f55\u5224\u65ad\u5b9e\u73b0\u662f\u5426\u6709bug\uff09\u548ctraceability\u95ee\u9898\uff08\u5982\u4f55\u8ffd\u8e2abug\u7684\u6765\u6e90\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u5f88\u5c11\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u56e0\u6b64\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLMs\u7684\u81ea\u52a8\u5316\u65b9\u6848\u3002", "method": "\u901a\u8fc7LLMs\u5c06\u7ed3\u6784\u5316\u7684\u534f\u8bae\u89c4\u8303\u6587\u6863\uff08\u5982RFC\uff09\u7ffb\u8bd1\u4e3a\u5f62\u5f0f\u5316\u534f\u8bae\u6d88\u606f\u89c4\u8303\uff0c\u4f5c\u4e3aquasi-oracle\u9a8c\u8bc1\u534f\u8bae\u89e3\u6790\u5668\uff0c\u5e76\u901a\u8fc7\u9a8c\u8bc1\u7ed3\u679c\u9010\u6b65\u4fee\u6b63oracle\u3002\u7531\u4e8eoracle\u6e90\u81ea\u6587\u6863\uff0c\u53d1\u73b0\u7684bug\u53ef\u8ffd\u8e2a\u5230\u6587\u6863\u3002", "result": "\u57289\u4e2a\u7f51\u7edc\u534f\u8bae\u53ca\u5176C\u3001Python\u3001Go\u5b9e\u73b0\u4e2d\u8bc4\u4f30\uff0c\u65b9\u6cd5\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u6280\u672f\uff0c\u68c0\u6d4b\u523069\u4e2abug\uff0836\u4e2a\u5df2\u786e\u8ba4\uff09\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u81ea\u52a8\u5316\u9a8c\u8bc1\u8f6f\u4ef6\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9ad8\u6548\u89e3\u51b3\u4e86oracle\u548ctraceability\u95ee\u9898\uff0c\u8fd8\u5c55\u793a\u4e86\u81ea\u52a8\u5316\u8f6f\u4ef6\u9a8c\u8bc1\u7684\u53ef\u80fd\u6027\uff0c\u51cf\u5c11\u4e86\u4f20\u7edf\u624b\u52a8\u89e3\u6790\u89c4\u8303\u6587\u6863\u548c\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u7684\u5de5\u4f5c\u91cf\u3002"}}
{"id": "2504.18024", "pdf": "https://arxiv.org/pdf/2504.18024", "abs": "https://arxiv.org/abs/2504.18024", "authors": ["Yiwei Zha"], "title": "SMARTFinRAG: Interactive Modularized Financial RAG Benchmark", "categories": ["cs.CE", "cs.CL", "cs.IR"], "comment": "For open source github repo, see\n  https://github.com/JonathanZha47/SMARTFinRAG", "summary": "Financial sectors are rapidly adopting language model technologies, yet\nevaluating specialized RAG systems in this domain remains challenging. This\npaper introduces SMARTFinRAG, addressing three critical gaps in financial RAG\nassessment: (1) a fully modular architecture where components can be\ndynamically interchanged during runtime; (2) a document-centric evaluation\nparadigm generating domain-specific QA pairs from newly ingested financial\ndocuments; and (3) an intuitive interface bridging research-implementation\ndivides. Our evaluation quantifies both retrieval efficacy and response\nquality, revealing significant performance variations across configurations.\nThe platform's open-source architecture supports transparent, reproducible\nresearch while addressing practical deployment challenges faced by financial\ninstitutions implementing RAG systems.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aSMARTFinRAG\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u91d1\u878d\u9886\u57df\u7684RAG\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u6a21\u5757\u5316\u67b6\u6784\u3001\u9886\u57df\u7279\u5b9aQA\u5bf9\u751f\u6210\u548c\u7814\u7a76-\u5b9e\u73b0\u6865\u6881\u7684\u4e09\u5927\u5173\u952e\u95ee\u9898\u3002", "motivation": "\u91d1\u878d\u9886\u57df\u5feb\u901f\u91c7\u7528\u8bed\u8a00\u6a21\u578b\u6280\u672f\uff0c\u4f46\u8bc4\u4f30\u4e13\u95e8\u7684RAG\u7cfb\u7edf\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "SMARTFinRAG\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\u3001\u6587\u6863\u4e2d\u5fc3\u8bc4\u4f30\u8303\u5f0f\uff08\u751f\u6210\u9886\u57df\u7279\u5b9aQA\u5bf9\uff09\u548c\u76f4\u89c2\u754c\u9762\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u4e0d\u540c\u914d\u7f6e\u4e0b\u7684\u68c0\u7d22\u6548\u529b\u548c\u54cd\u5e94\u8d28\u91cf\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e73\u53f0\u652f\u6301\u900f\u660e\u548c\u53ef\u91cd\u590d\u7684\u7814\u7a76\u3002", "conclusion": "SMARTFinRAG\u4e3a\u91d1\u878d\u673a\u6784\u90e8\u7f72RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u7814\u7a76\u4e0e\u5b9e\u8df5\u7684\u7ed3\u5408\u3002"}}
{"id": "2504.18395", "pdf": "https://arxiv.org/pdf/2504.18395", "abs": "https://arxiv.org/abs/2504.18395", "authors": ["Rabanus Derr", "Jessie Finocchiaro", "Robert C. Williamson"], "title": "Three Types of Calibration with Properties and their Semantic and Formal Relationships", "categories": ["cs.LG"], "comment": null, "summary": "Fueled by discussions around \"trustworthiness\" and algorithmic fairness,\ncalibration of predictive systems has regained scholars attention. The vanilla\ndefinition and understanding of calibration is, simply put, on all days on\nwhich the rain probability has been predicted to be p, the actual frequency of\nrain days was p. However, the increased attention has led to an immense variety\nof new notions of \"calibration.\" Some of the notions are incomparable, serve\ndifferent purposes, or imply each other. In this work, we provide two accounts\nwhich motivate calibration: self-realization of forecasted properties and\nprecise estimation of incurred losses of the decision makers relying on\nforecasts. We substantiate the former via the reflection principle and the\nlatter by actuarial fairness. For both accounts we formulate prototypical\ndefinitions via properties $\\Gamma$ of outcome distributions, e.g., the mean or\nmedian. The prototypical definition for self-realization, which we call\n$\\Gamma$-calibration, is equivalent to a certain type of swap regret under\ncertain conditions. These implications are strongly connected to the\nomniprediction learning paradigm. The prototypical definition for precise loss\nestimation is a modification of decision calibration adopted from Zhao et al.\n[73]. For binary outcome sets both prototypical definitions coincide under\nappropriate choices of reference properties. For higher-dimensional outcome\nsets, both prototypical definitions can be subsumed by a natural extension of\nthe binary definition, called distribution calibration with respect to a\nproperty. We conclude by commenting on the role of groupings in both accounts\nof calibration often used to obtain multicalibration. In sum, this work\nprovides a semantic map of calibration in order to navigate a fragmented\nterrain of notions and definitions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u9884\u6d4b\u7cfb\u7edf\u7684\u6821\u51c6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u6821\u51c6\u52a8\u673a\uff08\u81ea\u6211\u5b9e\u73b0\u548c\u7cbe\u786e\u635f\u5931\u4f30\u8ba1\uff09\uff0c\u5e76\u5b9a\u4e49\u4e86\u539f\u578b\u6982\u5ff5\u4ee5\u7edf\u4e00\u591a\u79cd\u6821\u51c6\u5b9a\u4e49\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u5bf9\u9884\u6d4b\u7cfb\u7edf\u2018\u53ef\u4fe1\u5ea6\u2019\u548c\u7b97\u6cd5\u516c\u5e73\u6027\u7684\u8ba8\u8bba\uff0c\u65e8\u5728\u7406\u6e05\u5e76\u7edf\u4e00\u65e5\u76ca\u5206\u6563\u7684\u6821\u51c6\u5b9a\u4e49\u548c\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u2018\u81ea\u6211\u5b9e\u73b0\u2019\uff08\u53cd\u5c04\u539f\u7406\uff09\u548c\u2018\u7cbe\u786e\u635f\u5931\u4f30\u8ba1\u2019\uff08\u4fdd\u9669\u516c\u5e73\u6027\uff09\u4e24\u79cd\u52a8\u673a\uff0c\u63d0\u51fa\u57fa\u4e8e\u7ed3\u679c\u5206\u5e03\u7279\u6027\u7684\u539f\u578b\u5b9a\u4e49\uff08\u5982\u5747\u503c\u6216\u4e2d\u4f4d\u6570\uff09\uff0c\u5e76\u63a2\u8ba8\u5176\u4e0e\u4ea4\u6362\u540e\u6094\u548c\u5b66\u4e60\u8303\u5f0f\u7684\u5173\u7cfb\u3002", "result": "\u5728\u4e8c\u5143\u7ed3\u679c\u96c6\u4e0b\uff0c\u4e24\u79cd\u539f\u578b\u5b9a\u4e49\u53ef\u901a\u8fc7\u5408\u7406\u9009\u62e9\u53c2\u8003\u7279\u6027\u7edf\u4e00\uff1b\u9ad8\u7ef4\u7ed3\u679c\u96c6\u4e0b\u5219\u901a\u8fc7\u2018\u5206\u5e03\u6821\u51c6\u2019\u6982\u5ff5\u6269\u5c55\u517c\u5bb9\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u8bed\u4e49\u6620\u5c04\u6574\u5408\u4e86\u788e\u7247\u5316\u7684\u6821\u51c6\u5b9a\u4e49\uff0c\u5f3a\u8c03\u4e86\u5206\u7ec4\u5728\u591a\u5143\u6821\u51c6\u4e2d\u7684\u4f5c\u7528\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u6e05\u6670\u6846\u67b6\u3002"}}
{"id": "2504.18057", "pdf": "https://arxiv.org/pdf/2504.18057", "abs": "https://arxiv.org/abs/2504.18057", "authors": ["Jiayi Chen", "Shuai Wang", "Guoliang Li", "Wei Xu", "Guangxu Zhu", "Derrick Wing Kwan Ng", "Chengzhong Xu"], "title": "Opportunistic Collaborative Planning with Large Vision Model Guided Control and Joint Query-Service Optimization", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Navigating autonomous vehicles in open scenarios is a challenge due to the\ndifficulties in handling unseen objects. Existing solutions either rely on\nsmall models that struggle with generalization or large models that are\nresource-intensive. While collaboration between the two offers a promising\nsolution, the key challenge is deciding when and how to engage the large model.\nTo address this issue, this paper proposes opportunistic collaborative planning\n(OCP), which seamlessly integrates efficient local models with powerful cloud\nmodels through two key innovations. First, we propose large vision model guided\nmodel predictive control (LVM-MPC), which leverages the cloud for LVM\nperception and decision making. The cloud output serves as a global guidance\nfor a local MPC, thereby forming a closed-loop perception-to-control system.\nSecond, to determine the best timing for large model query and service, we\npropose collaboration timing optimization (CTO), including object detection\nconfidence thresholding (ODCT) and cloud forward simulation (CFS), to decide\nwhen to seek cloud assistance and when to offer cloud service. Extensive\nexperiments show that the proposed OCP outperforms existing methods in terms of\nboth navigation time and success rate.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u673a\u4f1a\u534f\u4f5c\u89c4\u5212\uff08OCP\uff09\uff0c\u7ed3\u5408\u5c0f\u578b\u672c\u5730\u6a21\u578b\u4e0e\u5f3a\u5927\u4e91\u7aef\u6a21\u578b\uff0c\u901a\u8fc7LVM-MPC\u548cCTO\u4f18\u5316\u534f\u4f5c\u65f6\u673a\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5728\u5f00\u653e\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u5728\u5f00\u653e\u573a\u666f\u4e2d\u5904\u7406\u672a\u77e5\u7269\u4f53\u65f6\u5c0f\u578b\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u4e0e\u5927\u578b\u6a21\u578b\u8d44\u6e90\u6d88\u8017\u9ad8\u7684\u77db\u76fe\u3002", "method": "\u63d0\u51faLVM-MPC\u5229\u7528\u4e91\u7aef\u8fdb\u884c\u611f\u77e5\u4e0e\u51b3\u7b56\uff0c\u672c\u5730MPC\u6267\u884c\uff1b\u901a\u8fc7ODCT\u548cCFS\u4f18\u5316\u534f\u4f5c\u65f6\u673a\uff08CTO\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660eOCP\u5728\u5bfc\u822a\u65f6\u95f4\u548c\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OCP\u6709\u6548\u5e73\u8861\u8d44\u6e90\u4e0e\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u534f\u4f5c\u89c4\u5212\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2504.18099", "pdf": "https://arxiv.org/pdf/2504.18099", "abs": "https://arxiv.org/abs/2504.18099", "authors": ["Leena G Pillai", "D. Muhammad Noorul Mubarak", "Elizabeth Sherly"], "title": "Tracking Articulatory Dynamics in Speech with a Fixed-Weight BiLSTM-CNN Architecture", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "10 pages with 8 figures. This paper presented in an international\n  Conference", "summary": "Speech production is a complex sequential process which involve the\ncoordination of various articulatory features. Among them tongue being a highly\nversatile active articulator responsible for shaping airflow to produce\ntargeted speech sounds that are intellectual, clear, and distinct. This paper\npresents a novel approach for predicting tongue and lip articulatory features\ninvolved in a given speech acoustics using a stacked Bidirectional Long\nShort-Term Memory (BiLSTM) architecture, combined with a one-dimensional\nConvolutional Neural Network (CNN) for post-processing with fixed weights\ninitialization. The proposed network is trained with two datasets consisting of\nsimultaneously recorded speech and Electromagnetic Articulography (EMA)\ndatasets, each introducing variations in terms of geographical origin,\nlinguistic characteristics, phonetic diversity, and recording equipment. The\nperformance of the model is assessed in Speaker Dependent (SD), Speaker\nIndependent (SI), corpus dependent (CD) and cross corpus (CC) modes.\nExperimental results indicate that the proposed model with fixed weights\napproach outperformed the adaptive weights initialization with in relatively\nminimal number of training epochs. These findings contribute to the development\nof robust and efficient models for articulatory feature prediction, paving the\nway for advancements in speech production research and applications.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u53cc\u5411\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u548c\u4e00\u7ef4\u5377\u79ef\u7f51\u7edc\u6765\u9884\u6d4b\u8bed\u97f3\u4e2d\u7684\u820c\u5934\u548c\u5634\u5507\u53d1\u97f3\u7279\u5f81\uff0c\u4f7f\u7528\u56fa\u5b9a\u6743\u91cd\u521d\u59cb\u5316\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8f83\u5c11\u8bad\u7ec3\u8f6e\u6b21\u4e0b\u4f18\u4e8e\u81ea\u9002\u5e94\u6743\u91cd\u521d\u59cb\u5316\u3002", "motivation": "\u8bed\u97f3\u4ea7\u751f\u662f\u4e00\u4e2a\u590d\u6742\u7684\u534f\u8c03\u8fc7\u7a0b\uff0c\u5176\u4e2d\u820c\u5934\u4f5c\u4e3a\u9ad8\u5ea6\u7075\u6d3b\u7684\u53d1\u58f0\u5668\u5b98\u8d77\u7740\u91cd\u8981\u4f5c\u7528\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u9884\u6d4b\u53d1\u97f3\u7279\u5f81\u6765\u63a8\u52a8\u8bed\u97f3\u4ea7\u751f\u7814\u7a76\u548c\u5e94\u7528\u7684\u8fdb\u5c55\u3002", "method": "\u91c7\u7528\u5806\u53e0\u7684\u53cc\u5411\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08BiLSTM\uff09\u7ed3\u5408\u4e00\u7ef4\u5377\u79ef\u7f51\u7edc\uff08CNN\uff09\u8fdb\u884c\u540e\u5904\u7406\uff0c\u4f7f\u7528\u56fa\u5b9a\u6743\u91cd\u521d\u59cb\u5316\uff0c\u8bad\u7ec3\u6570\u636e\u5305\u62ec\u540c\u65f6\u8bb0\u5f55\u7684\u8bed\u97f3\u548c\u7535\u78c1\u53d1\u97f3\u4eea\uff08EMA\uff09\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u56fa\u5b9a\u6743\u91cd\u521d\u59cb\u5316\u65b9\u6cd5\u5728\u8f83\u5c11\u7684\u8bad\u7ec3\u8f6e\u6b21\u4e0b\u4f18\u4e8e\u81ea\u9002\u5e94\u6743\u91cd\u521d\u59cb\u5316\uff0c\u6a21\u578b\u5728\u8bf4\u8bdd\u4eba\u76f8\u5173\u3001\u8bf4\u8bdd\u4eba\u65e0\u5173\u3001\u8bed\u6599\u5e93\u76f8\u5173\u548c\u8de8\u8bed\u6599\u5e93\u6a21\u5f0f\u4e0b\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53d1\u97f3\u7279\u5f81\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u548c\u7a33\u5065\u7684\u6a21\u578b\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8bed\u97f3\u4ea7\u751f\u7814\u7a76\u548c\u5e94\u7528\u7684\u8fdb\u6b65\u3002"}}
{"id": "2504.18414", "pdf": "https://arxiv.org/pdf/2504.18414", "abs": "https://arxiv.org/abs/2504.18414", "authors": ["Vinicius L S Silva", "Pablo Salinas", "Claire E Heaney", "Matthew Jackson", "Christopher C Pain"], "title": "Online learning to accelerate nonlinear PDE solvers: applied to multiphase porous media flow", "categories": ["cs.LG", "physics.flu-dyn"], "comment": null, "summary": "We propose a novel type of nonlinear solver acceleration for systems of\nnonlinear partial differential equations (PDEs) that is based on\nonline/adaptive learning. It is applied in the context of multiphase flow in\nporous media. The proposed method rely on four pillars: (i) dimensionless\nnumbers as input parameters for the machine learning model, (ii) simplified\nnumerical model (two-dimensional) for the offline training, (iii) dynamic\ncontrol of a nonlinear solver tuning parameter (numerical relaxation), (iv) and\nonline learning for real-time improvement of the machine learning model. This\nstrategy decreases the number of nonlinear iterations by dynamically modifying\na single global parameter, the relaxation factor, and by adaptively learning\nthe attributes of each numerical model on-the-run. Furthermore, this work\nperforms a sensitivity study in the dimensionless parameters (machine learning\nfeatures), assess the efficacy of various machine learning models, demonstrate\na decrease in nonlinear iterations using our method in more intricate,\nrealistic three-dimensional models, and fully couple a machine learning model\ninto an open-source multiphase flow simulator achieving up to 85\\% reduction in\ncomputational time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5728\u7ebf/\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u975e\u7ebf\u6027\u6c42\u89e3\u5668\u52a0\u901f\u65b9\u6cd5\uff0c\u7528\u4e8e\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\u7cfb\u7edf\uff0c\u5e94\u7528\u4e8e\u591a\u5b54\u4ecb\u8d28\u4e2d\u7684\u591a\u76f8\u6d41\u95ee\u9898\u3002\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u677e\u5f1b\u56e0\u5b50\u548c\u5728\u7ebf\u5b66\u4e60\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u591a\u5b54\u4ecb\u8d28\u4e2d\u7684\u591a\u76f8\u6d41\u95ee\u9898\u8ba1\u7b97\u590d\u6742\u4e14\u8017\u65f6\uff0c\u4f20\u7edf\u975e\u7ebf\u6027\u6c42\u89e3\u5668\u6548\u7387\u4f4e\u3002\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u548c\u52a8\u6001\u53c2\u6570\u8c03\u6574\uff0c\u65e8\u5728\u63d0\u5347\u6c42\u89e3\u6548\u7387\u548c\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u3002", "method": "\u65b9\u6cd5\u57fa\u4e8e\u56db\u4e2a\u652f\u67f1\uff1a(1) \u65e0\u91cf\u7eb2\u6570\u4f5c\u4e3a\u673a\u5668\u5b66\u4e60\u8f93\u5165\uff0c(2) \u4e8c\u7ef4\u7b80\u5316\u6a21\u578b\u79bb\u7ebf\u8bad\u7ec3\uff0c(3) \u52a8\u6001\u63a7\u5236\u677e\u5f1b\u56e0\u5b50\uff0c(4) \u5728\u7ebf\u5b66\u4e60\u5b9e\u65f6\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728\u590d\u6742\u4e09\u7ef4\u6a21\u578b\u4e2d\u9a8c\u8bc1\uff0c\u975e\u7ebf\u6027\u8fed\u4ee3\u6b21\u6570\u51cf\u5c11\uff0c\u8ba1\u7b97\u65f6\u95f4\u6700\u9ad8\u51cf\u5c1185%\u3002", "conclusion": "\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u7684\u52a8\u6001\u52a0\u901f\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u591a\u76f8\u6d41\u6a21\u62df\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5b9e\u9645\u95ee\u9898\u3002"}}
{"id": "2504.18333", "pdf": "https://arxiv.org/pdf/2504.18333", "abs": "https://arxiv.org/abs/2504.18333", "authors": ["Narek Maloyan", "Dmitry Namiot"], "title": "Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "LLM as judge systems used to assess text quality code correctness and\nargument strength are vulnerable to prompt injection attacks. We introduce a\nframework that separates content author attacks from system prompt attacks and\nevaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3\nOpus on four tasks with various defenses using fifty prompts per condition.\nAttacks achieved up to seventy three point eight percent success smaller models\nproved more vulnerable and transferability ranged from fifty point five to\nsixty two point six percent. Our results contrast with Universal Prompt\nInjection and AdvPrompter We recommend multi model committees and comparative\nscoring and release all code and datasets", "AI": {"tldr": "LLM\u4f5c\u4e3a\u8bc4\u4f30\u7cfb\u7edf\u6613\u53d7\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u7814\u7a76\u8bc4\u4f30\u591a\u4e2a\u6a21\u578b\u5e76\u63d0\u51fa\u9632\u5fa1\u5efa\u8bae\u3002", "motivation": "\u63a2\u7a76LLM\u5728\u6587\u672c\u8bc4\u4f30\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5c24\u5176\u662f\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7684\u5a01\u80c1\u3002", "method": "\u5206\u79bb\u5185\u5bb9\u4e0e\u7cfb\u7edf\u63d0\u793a\u653b\u51fb\uff0c\u8bc4\u4f305\u79cd\u6a21\u578b\u57284\u9879\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u6bcf\u6761\u4ef650\u4e2a\u63d0\u793a\u3002", "result": "\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe73.8%\uff0c\u5c0f\u6a21\u578b\u66f4\u8106\u5f31\uff0c\u5efa\u8bae\u4f7f\u7528\u591a\u6a21\u578b\u59d4\u5458\u4f1a\u548c\u6bd4\u8f83\u8bc4\u5206\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\u73b0\u6709\u9632\u5fa1\u4e0d\u8db3\uff0c\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2504.18433", "pdf": "https://arxiv.org/pdf/2504.18433", "abs": "https://arxiv.org/abs/2504.18433", "authors": ["Christopher B\u00fclte", "Yusuf Sale", "Timo L\u00f6hr", "Paul Hofman", "Gitta Kutyniok", "Eyke H\u00fcllermeier"], "title": "An Axiomatic Assessment of Entropy- and Variance-based Uncertainty Quantification in Regression", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Uncertainty quantification (UQ) is crucial in machine learning, yet most\n(axiomatic) studies of uncertainty measures focus on classification, leaving a\ngap in regression settings with limited formal justification and evaluations.\nIn this work, we introduce a set of axioms to rigorously assess measures of\naleatoric, epistemic, and total uncertainty in supervised regression. By\nutilizing a predictive exponential family, we can generalize commonly used\napproaches for uncertainty representation and corresponding uncertainty\nmeasures. More specifically, we analyze the widely used entropy- and\nvariance-based measures regarding limitations and challenges. Our findings\nprovide a principled foundation for UQ in regression, offering theoretical\ninsights and practical guidelines for reliable uncertainty assessment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u56de\u5f52\u4efb\u52a1\u4e2d\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u516c\u7406\uff0c\u5206\u6790\u4e86\u71b5\u548c\u65b9\u5dee\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u56de\u5f52\u4e2d\u7684\u53ef\u9760\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5357\u3002", "motivation": "\u5f53\u524d\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5206\u7c7b\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u5e76\u63d0\u4f9b\u5b9e\u7528\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u7ec4\u516c\u7406\uff0c\u901a\u8fc7\u6307\u6570\u65cf\u9884\u6d4b\u6a21\u578b\u63a8\u5e7f\u4e86\u5e38\u7528\u7684\u4e0d\u786e\u5b9a\u6027\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u71b5\u548c\u65b9\u5dee\u57fa\u7684\u6d4b\u5ea6\u3002", "result": "\u7814\u7a76\u4e3a\u56de\u5f52\u4efb\u52a1\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6d4b\u5ea6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u4e25\u683c\u7684\u516c\u7406\u5316\u65b9\u6cd5\u586b\u8865\u4e86\u56de\u5f52\u4efb\u52a1\u4e2d\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u7a7a\u767d\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u9760\u7684\u8bc4\u4f30\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2504.18062", "pdf": "https://arxiv.org/pdf/2504.18062", "abs": "https://arxiv.org/abs/2504.18062", "authors": ["Lingyan Bao", "Sinwoong Yun", "Jemin Lee", "Tony Q. S. Quek"], "title": "LLM-Guided Open RAN: Empowering Hierarchical RAN Intelligent Control", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have led to a significant\ninterest in deploying LLMempowered algorithms for wireless communication\nnetworks. Meanwhile, open radio access network (O-RAN) techniques offer\nunprecedented flexibility, with the non-real-time (non-RT) radio access network\n(RAN) intelligent controller (RIC) (non-RT RIC) and near-real-time (near-RT)\nRIC (near-RT RIC) components enabling intelligent resource management across\ndifferent time scales. In this paper, we propose the LLM empowered hierarchical\nRIC (LLM-hRIC) framework to improve the collaboration between RICs. This\nframework integrates LLMs with reinforcement learning (RL) for efficient\nnetwork resource management. In this framework, LLMs-empowered non-RT RICs\nprovide strategic guidance and high-level policies based on environmental\ncontext. Concurrently, RL-empowered near-RT RICs perform low-latency tasks\nbased on strategic guidance and local near-RT observation. We evaluate the\nLLM-hRIC framework in an integrated access and backhaul (IAB) network setting.\nSimulation results demonstrate that the proposed framework achieves superior\nperformance. Finally, we discuss the key future challenges in applying LLMs to\nO-RAN.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdLLM\u8d4b\u80fd\u7684\u5c42\u6b21\u5316RIC\u6846\u67b6\uff08LLM-hRIC\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6765\u4f18\u5316\u65e0\u7ebf\u901a\u4fe1\u7f51\u7edc\u4e2d\u7684\u8d44\u6e90\u7ba1\u7406\u3002\u8be5\u6846\u67b6\u5728IAB\u7f51\u7edc\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\uff0c\u5e76\u63a2\u8ba8\u4e86LLM\u5728O-RAN\u4e2d\u7684\u672a\u6765\u6311\u6218\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5f00\u653e\u65e0\u7ebf\u63a5\u5165\u7f51\uff08O-RAN\uff09\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5982\u4f55\u9ad8\u6548\u534f\u540c\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u65e0\u7ebf\u8d44\u6e90\u7ba1\u7406\u6210\u4e3a\u7814\u7a76\u91cd\u70b9\u3002\u672c\u6587\u65e8\u5728\u5229\u7528LLM\u548cRL\u7684\u7ed3\u5408\uff0c\u63d0\u5347RIC\u4e4b\u95f4\u7684\u534f\u4f5c\u6548\u7387\u3002", "method": "\u63d0\u51faLLM-hRIC\u6846\u67b6\uff0c\u5c06LLM\u8d4b\u80fd\u7684\u975e\u5b9e\u65f6RIC\uff08non-RT RIC\uff09\u7528\u4e8e\u63d0\u4f9b\u6218\u7565\u6027\u6307\u5bfc\uff0cRL\u8d4b\u80fd\u7684\u8fd1\u5b9e\u65f6RIC\uff08near-RT RIC\uff09\u6267\u884c\u4f4e\u5ef6\u8fdf\u4efb\u52a1\uff0c\u7ed3\u5408\u73af\u5883\u4e0a\u4e0b\u6587\u548c\u672c\u5730\u89c2\u6d4b\u4f18\u5316\u8d44\u6e90\u7ba1\u7406\u3002", "result": "\u5728IAB\u7f51\u7edc\u4eff\u771f\u4e2d\uff0c\u8be5\u6846\u67b6\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "LLM-hRIC\u6846\u67b6\u4e3aO-RAN\u4e2d\u7684\u667a\u80fd\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4f46LLM\u5728O-RAN\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ecd\u9700\u89e3\u51b3\u591a\u9879\u6311\u6218\u3002"}}
{"id": "2504.18425", "pdf": "https://arxiv.org/pdf/2504.18425", "abs": "https://arxiv.org/abs/2504.18425", "authors": ["KimiTeam", "Ding Ding", "Zeqian Ju", "Yichong Leng", "Songxiang Liu", "Tong Liu", "Zeyu Shang", "Kai Shen", "Wei Song", "Xu Tan", "Heyi Tang", "Zhengtao Wang", "Chu Wei", "Yifei Xin", "Xinran Xu", "Jianwei Yu", "Yutao Zhang", "Xinyu Zhou", "Y. Charles", "Jun Chen", "Yanru Chen", "Yulun Du", "Weiran He", "Zhenxing Hu", "Guokun Lai", "Qingcheng Li", "Yangyang Liu", "Weidong Sun", "Jianzhou Wang", "Yuzhi Wang", "Yuefeng Wu", "Yuxin Wu", "Dongchao Yang", "Hao Yang", "Ying Yang", "Zhilin Yang", "Aoxiong Yin", "Ruibin Yuan", "Yutong Zhang", "Zaida Zhou"], "title": "Kimi-Audio Technical Report", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "cs.SD"], "comment": null, "summary": "We present Kimi-Audio, an open-source audio foundation model that excels in\naudio understanding, generation, and conversation. We detail the practices in\nbuilding Kimi-Audio, including model architecture, data curation, training\nrecipe, inference deployment, and evaluation. Specifically, we leverage a\n12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous\nfeatures as input and discrete tokens as output, and develop a chunk-wise\nstreaming detokenizer based on flow matching. We curate a pre-training dataset\nthat consists of more than 13 million hours of audio data covering a wide range\nof modalities including speech, sound, and music, and build a pipeline to\nconstruct high-quality and diverse post-training data. Initialized from a\npre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text\ndata with several carefully designed tasks, and then fine-tuned to support a\ndiverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio\nachieves state-of-the-art performance on a range of audio benchmarks including\nspeech recognition, audio understanding, audio question answering, and speech\nconversation. We release the codes, model checkpoints, as well as the\nevaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.", "AI": {"tldr": "Kimi-Audio\u662f\u4e00\u6b3e\u5f00\u6e90\u7684\u97f3\u9891\u57fa\u7840\u6a21\u578b\uff0c\u64c5\u957f\u97f3\u9891\u7406\u89e3\u3001\u751f\u6210\u548c\u5bf9\u8bdd\u3002\u901a\u8fc7\u521b\u65b0\u7684\u6a21\u578b\u67b6\u6784\u3001\u5927\u89c4\u6a21\u6570\u636e\u8bad\u7ec3\u548c\u591a\u4efb\u52a1\u5fae\u8c03\uff0c\u8be5\u6a21\u578b\u5728\u591a\u9879\u97f3\u9891\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u5168\u9762\u7406\u89e3\u3001\u751f\u6210\u548c\u5904\u7406\u97f3\u9891\u7684\u591a\u529f\u80fd\u6a21\u578b\uff0c\u6ee1\u8db3\u97f3\u9891\u9886\u57df\u591a\u6837\u5316\u7684\u5e94\u7528\u9700\u6c42\u3002", "method": "\u91c7\u752812.5Hz\u97f3\u9891\u5206\u8bcd\u5668\uff0c\u8bbe\u8ba1\u57fa\u4e8eLLM\u7684\u67b6\u6784\uff08\u8f93\u5165\u4e3a\u8fde\u7eed\u7279\u5f81\uff0c\u8f93\u51fa\u4e3a\u79bb\u6563\u4ee4\u724c\uff09\uff0c\u5f00\u53d1\u6d41\u5f0f\u5206\u5757\u89e3\u7801\u5668\u3002\u901a\u8fc71300\u4e07\u5c0f\u65f6\u7684\u591a\u6a21\u6001\u97f3\u9891\u6570\u636e\u9884\u8bad\u7ec3\uff0c\u5e76\u7ed3\u5408\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4efb\u52a1\u5fae\u8c03\u3002", "result": "\u5728\u8bed\u97f3\u8bc6\u522b\u3001\u97f3\u9891\u7406\u89e3\u3001\u97f3\u9891\u95ee\u7b54\u548c\u8bed\u97f3\u5bf9\u8bdd\u7b49\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Kimi-Audio\u5c55\u793a\u4e86\u5f00\u6e90\u97f3\u9891\u57fa\u7840\u6a21\u578b\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u4e3a\u97f3\u9891\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002\u4ee3\u7801\u4e0e\u6a21\u578b\u5df2\u516c\u5f00\u3002"}}
{"id": "2504.18437", "pdf": "https://arxiv.org/pdf/2504.18437", "abs": "https://arxiv.org/abs/2504.18437", "authors": ["Kun He", "Zijian Song", "Shuoxi Zhang", "John E. Hopcroft"], "title": "Enhancing Pre-Trained Model-Based Class-Incremental Learning through Neural Collapse", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Class-Incremental Learning (CIL) is a critical capability for real-world\napplications, enabling learning systems to adapt to new tasks while retaining\nknowledge from previous ones. Recent advancements in pre-trained models (PTMs)\nhave significantly advanced the field of CIL, demonstrating superior\nperformance over traditional methods. However, understanding how features\nevolve and are distributed across incremental tasks remains an open challenge.\nIn this paper, we propose a novel approach to modeling feature evolution in\nPTM-based CIL through the lens of neural collapse (NC), a striking phenomenon\nobserved in the final phase of training, which leads to a well-separated,\nequiangular feature space. We explore the connection between NC and CIL\neffectiveness, showing that aligning feature distributions with the NC geometry\nenhances the ability to capture the dynamic behavior of continual learning.\nBased on this insight, we introduce Neural Collapse-inspired Pre-Trained\nModel-based CIL (NCPTM-CIL), a method that dynamically adjusts the feature\nspace to conform to the elegant NC structure, thereby enhancing the continual\nlearning process. Extensive experiments demonstrate that NCPTM-CIL outperforms\nstate-of-the-art methods across four benchmark datasets. Notably, when\ninitialized with ViT-B/16-IN1K, NCPTM-CIL surpasses the runner-up method by\n6.73% on VTAB, 1.25% on CIFAR-100, and 2.5% on OmniBenchmark.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\uff08PTM\uff09\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\uff08CIL\uff09\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u795e\u7ecf\u574d\u7f29\uff08NC\uff09\u73b0\u8c61\u52a8\u6001\u8c03\u6574\u7279\u5f81\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6301\u7eed\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u7279\u5f81\u6f14\u5316\u4e0e\u7c7b\u589e\u91cf\u5b66\u4e60\u7684\u5173\u7cfb\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5f81\u5206\u5e03\u52a8\u6001\u8c03\u6574\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faNCPTM-CIL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7279\u5f81\u5206\u5e03\u4e0eNC\u51e0\u4f55\u5bf9\u9f50\uff0c\u52a8\u6001\u4f18\u5316\u7279\u5f81\u7a7a\u95f4\u7ed3\u6784\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728ViT-B/16-IN1K\u521d\u59cb\u5316\u4e0b\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "NCPTM-CIL\u901a\u8fc7NC\u51e0\u4f55\u5bf9\u9f50\u6709\u6548\u63d0\u5347\u4e86\u7c7b\u589e\u91cf\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.18068", "pdf": "https://arxiv.org/pdf/2504.18068", "abs": "https://arxiv.org/abs/2504.18068", "authors": ["Zhuohao Yan", "Shaoquan Feng", "Xingxing Li", "Yuxuan Zhou", "Chunxi Xia", "Shengyu Li"], "title": "S3MOT: Monocular 3D Object Tracking with Selective State Space Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate and reliable multi-object tracking (MOT) in 3D space is essential\nfor advancing robotics and computer vision applications. However, it remains a\nsignificant challenge in monocular setups due to the difficulty of mining 3D\nspatiotemporal associations from 2D video streams. In this work, we present\nthree innovative techniques to enhance the fusion and exploitation of\nheterogeneous cues for monocular 3D MOT: (1) we introduce the Hungarian State\nSpace Model (HSSM), a novel data association mechanism that compresses\ncontextual tracking cues across multiple paths, enabling efficient and\ncomprehensive assignment decisions with linear complexity. HSSM features a\nglobal receptive field and dynamic weights, in contrast to traditional linear\nassignment algorithms that rely on hand-crafted association costs. (2) We\npropose Fully Convolutional One-stage Embedding (FCOE), which eliminates ROI\npooling by directly using dense feature maps for contrastive learning, thus\nimproving object re-identification accuracy under challenging conditions such\nas varying viewpoints and lighting. (3) We enhance 6-DoF pose estimation\nthrough VeloSSM, an encoder-decoder architecture that models temporal\ndependencies in velocity to capture motion dynamics, overcoming the limitations\nof frame-based 3D inference. Experiments on the KITTI public test benchmark\ndemonstrate the effectiveness of our method, achieving a new state-of-the-art\nperformance of 76.86~HOTA at 31~FPS. Our approach outperforms the previous best\nby significant margins of +2.63~HOTA and +3.62~AssA, showcasing its robustness\nand efficiency for monocular 3D MOT tasks. The code and models are available at\nhttps://github.com/bytepioneerX/s3mot.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e09\u79cd\u521b\u65b0\u6280\u672f\uff08HSSM\u3001FCOE\u3001VeloSSM\uff09\u63d0\u5347\u5355\u76ee3D\u591a\u76ee\u6807\u8ddf\u8e2a\u6027\u80fd\uff0c\u5728KITTI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5237\u65b0SOTA\uff0cHOTA\u8fbe\u523076.86\u3002", "motivation": "\u89e3\u51b3\u5355\u76ee\u7cfb\u7edf\u4e2d\u56e02D\u89c6\u9891\u6d41\u96be\u4ee5\u6316\u63983D\u65f6\u7a7a\u5173\u8054\u800c\u5bfc\u81f4\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u6311\u6218\u3002", "method": "1. HSSM\uff1a\u57fa\u4e8e\u5308\u7259\u5229\u7b97\u6cd5\u7684\u6570\u636e\u5173\u8054\u673a\u5236\uff1b2. FCOE\uff1a\u65e0ROI\u6c60\u5316\u7684\u5bf9\u6bd4\u5b66\u4e60\u5d4c\u5165\uff1b3. VeloSSM\uff1a\u5efa\u6a21\u901f\u5ea6\u65f6\u5e8f\u4f9d\u8d56\u76846-DoF\u59ff\u6001\u4f30\u8ba1\u3002", "result": "KITTI\u6d4b\u8bd5\u4e2dHOTA 76.86\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u63d0\u5347+2.63\uff0c\u63a8\u7406\u901f\u5ea631FPS\u3002", "conclusion": "\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5355\u76ee3D MOT\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.18451", "pdf": "https://arxiv.org/pdf/2504.18451", "abs": "https://arxiv.org/abs/2504.18451", "authors": ["Tewodros Alemu Ayall", "Andy Li", "Matthew Beddows", "Milan Markovic", "Georgios Leontidis"], "title": "Enhancing Strawberry Yield Forecasting with Backcasted IoT Sensor Data and Machine Learning", "categories": ["cs.LG"], "comment": "20 pages, 11 figures", "summary": "Due to rapid population growth globally, digitally-enabled agricultural\nsectors are crucial for sustainable food production and making informed\ndecisions about resource management for farmers and various stakeholders. The\ndeployment of Internet of Things (IoT) technologies that collect real-time\nobservations of various environmental (e.g., temperature, humidity, etc.) and\noperational factors (e.g., irrigation) influencing production is often seen as\na critical step to enable additional novel downstream tasks, such as AI-based\nyield forecasting. However, since AI models require large amounts of data, this\ncreates practical challenges in a real-world dynamic farm setting where IoT\nobservations would need to be collected over a number of seasons. In this\nstudy, we deployed IoT sensors in strawberry production polytunnels for two\ngrowing seasons to collect environmental data, including water usage, external\nand internal temperature, external and internal humidity, soil moisture, soil\ntemperature, and photosynthetically active radiation. The sensor observations\nwere combined with manually provided yield records spanning a period of four\nseasons. To bridge the gap of missing IoT observations for two additional\nseasons, we propose an AI-based backcasting approach to generate synthetic\nsensor observations using historical weather data from a nearby weather station\nand the existing polytunnel observations. We built an AI-based yield\nforecasting model to evaluate our approach using the combination of real and\nsynthetic observations. Our results demonstrated that incorporating synthetic\ndata improved yield forecasting accuracy, with models incorporating synthetic\ndata outperforming those trained only on historical yield, weather records, and\nreal sensor data.", "AI": {"tldr": "\u901a\u8fc7\u90e8\u7f72\u7269\u8054\u7f51\u4f20\u611f\u5668\u548cAI\u9884\u6d4b\u6a21\u578b\uff0c\u7814\u7a76\u89e3\u51b3\u4e86\u519c\u4e1a\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5408\u6210\u6570\u636e\u63d0\u9ad8\u4e86\u8349\u8393\u4ea7\u91cf\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u5168\u7403\u4eba\u53e3\u5feb\u901f\u589e\u957f\uff0c\u6570\u5b57\u5316\u519c\u4e1a\u5bf9\u53ef\u6301\u7eed\u98df\u54c1\u751f\u4ea7\u548c\u8d44\u6e90\u7ba1\u7406\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002\u4f46AI\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u800c\u5b9e\u9645\u519c\u573a\u73af\u5883\u4e2d\u957f\u671f\u6536\u96c6\u7269\u8054\u7f51\u6570\u636e\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5728\u8349\u8393\u79cd\u690d\u5730\u90e8\u7f72\u7269\u8054\u7f51\u4f20\u611f\u5668\u6536\u96c6\u4e24\u4e2a\u751f\u957f\u5b63\u8282\u7684\u73af\u5883\u6570\u636e\uff0c\u7ed3\u5408\u56db\u4e2a\u5b63\u8282\u7684\u624b\u5de5\u4ea7\u91cf\u8bb0\u5f55\uff0c\u63d0\u51faAI\u56de\u586b\u65b9\u6cd5\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u6784\u5efa\u4ea7\u91cf\u9884\u6d4b\u6a21\u578b\u8bc4\u4f30\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5408\u6210\u6570\u636e\u7684\u52a0\u5165\u63d0\u9ad8\u4e86\u4ea7\u91cf\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u3001\u5929\u6c14\u8bb0\u5f55\u548c\u624b\u5de5\u4ea7\u91cf\u6570\u636e\u7684\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408AI\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u89c2\u6d4b\u6570\u636e\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u519c\u4e1a\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4ea7\u91cf\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2504.18454", "pdf": "https://arxiv.org/pdf/2504.18454", "abs": "https://arxiv.org/abs/2504.18454", "authors": ["Hiroki Naganuma", "Xinzhi Zhang", "Man-Chung Yue", "Ioannis Mitliagkas", "Philipp A. Witte", "Russell J. Hewett", "Yin Tat Lee"], "title": "Pseudo-Asynchronous Local SGD: Robust and Efficient Data-Parallel Training", "categories": ["cs.LG"], "comment": null, "summary": "Following AI scaling trends, frontier models continue to grow in size and\ncontinue to be trained on larger datasets. Training these models requires huge\ninvestments in exascale computational resources, which has in turn driven\ndevelopment of distributed deep learning methods. Data parallelism is an\nessential approach to speed up training, but it requires frequent global\ncommunication between workers, which can bottleneck training at the largest\nscales. In this work, we propose a method called Pseudo-Asynchronous Local SGD\n(PALSGD) to improve the efficiency of data-parallel training. PALSGD is an\nextension of Local SGD (Stich, 2018) and DiLoCo (Douillard et al., 2023),\ndesigned to further reduce communication frequency by introducing a\npseudo-synchronization mechanism. PALSGD allows the use of longer\nsynchronization intervals compared to standard Local SGD. Despite the reduced\ncommunication frequency, the pseudo-synchronization approach ensures that model\nconsistency is maintained, leading to performance results comparable to those\nachieved with more frequent synchronization. Furthermore, we provide a\ntheoretical analysis of PALSGD, establishing its convergence and deriving its\nconvergence rate. This analysis offers insights into the algorithm's behavior\nand performance guarantees. We evaluated PALSGD on image classification and\nlanguage modeling tasks. Our results show that PALSGD achieves better\nperformance in less time compared to existing methods like Distributed Data\nParallel (DDP), and DiLoCo. Notably, PALSGD trains 18.4% faster than DDP on\nImageNet-1K with ResNet-50, 24.4% faster than DDP on TinyStories with\nGPT-Neo125M, and 21.1% faster than DDP on TinyStories with GPT-Neo-8M.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPALSGD\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f2a\u5f02\u6b65\u540c\u6b65\u673a\u5236\u51cf\u5c11\u6570\u636e\u5e76\u884c\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u9891\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u89c4\u6a21\u7684\u589e\u5927\uff0c\u6570\u636e\u5e76\u884c\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u9891\u7387\u6210\u4e3a\u74f6\u9888\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u51cf\u5c11\u901a\u4fe1\u53c8\u80fd\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "PALSGD\u6269\u5c55\u4e86Local SGD\u548cDiLoCo\uff0c\u5f15\u5165\u4f2a\u540c\u6b65\u673a\u5236\uff0c\u5141\u8bb8\u66f4\u957f\u7684\u540c\u6b65\u95f4\u9694\uff0c\u540c\u65f6\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u5176\u6536\u655b\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cPALSGD\u5728ImageNet-1K\u548cTinyStories\u4efb\u52a1\u4e0a\u6bd4DDP\u5206\u522b\u5feb18.4%\u548c21.1%-24.4%\u3002", "conclusion": "PALSGD\u901a\u8fc7\u51cf\u5c11\u901a\u4fe1\u9891\u7387\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u4e14\u6027\u80fd\u4e0d\u900a\u4e8e\u9ad8\u9891\u7387\u540c\u6b65\u65b9\u6cd5\u3002"}}
{"id": "2504.18506", "pdf": "https://arxiv.org/pdf/2504.18506", "abs": "https://arxiv.org/abs/2504.18506", "authors": ["Sanjeev Raja", "Martin \u0160\u00edpka", "Michael Psenka", "Tobias Kreiman", "Michal Pavelka", "Aditi S. Krishnapriyan"], "title": "Action-Minimization Meets Generative Modeling: Efficient Transition Path Sampling with the Onsager-Machlup Functional", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.chem-ph", "q-bio.BM"], "comment": null, "summary": "Transition path sampling (TPS), which involves finding probable paths\nconnecting two points on an energy landscape, remains a challenge due to the\ncomplexity of real-world atomistic systems. Current machine learning approaches\nuse expensive, task-specific, and data-free training procedures, limiting their\nability to benefit from recent advances in atomistic machine learning, such as\nhigh-quality datasets and large-scale pre-trained models. In this work, we\naddress TPS by interpreting candidate paths as trajectories sampled from\nstochastic dynamics induced by the learned score function of pre-trained\ngenerative models, specifically denoising diffusion and flow matching. Under\nthese dynamics, finding high-likelihood transition paths becomes equivalent to\nminimizing the Onsager-Machlup (OM) action functional. This enables us to\nrepurpose pre-trained generative models for TPS in a zero-shot manner, in\ncontrast with bespoke, task-specific TPS models trained in previous work. We\ndemonstrate our approach on varied molecular systems, obtaining diverse,\nphysically realistic transition pathways and generalizing beyond the\npre-trained model's original training dataset. Our method can be easily\nincorporated into new generative models, making it practically relevant as\nmodels continue to scale and improve with increased data availability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8fc7\u6e21\u8def\u5f84\u91c7\u6837\u95ee\u9898\uff0c\u901a\u8fc7\u91cd\u65b0\u5229\u7528\u751f\u6210\u6a21\u578b\u7684\u8bc4\u5206\u51fd\u6570\u5e76\u6700\u5c0f\u5316Onsager-Machlup\u52a8\u4f5c\u6cdb\u51fd\u6765\u5bfb\u627e\u9ad8\u6982\u7387\u8fc7\u6e21\u8def\u5f84\u3002", "motivation": "\u7531\u4e8e\u5f53\u524d\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u8fc7\u6e21\u8def\u5f84\u91c7\u6837\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u4e14\u65e0\u6cd5\u5145\u5206\u5229\u7528\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3001\u80fd\u76f4\u63a5\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8fc7\u6e21\u8def\u5f84\u91c7\u6837\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u8fc7\u6e21\u8def\u5f84\u89c6\u4e3a\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\uff08\u53bb\u566a\u6269\u6563\u548c\u6d41\u5339\u914d\uff09\u8bf1\u5bfc\u7684\u968f\u673a\u52a8\u6001\u91c7\u6837\u7684\u8f68\u8ff9\uff0c\u901a\u8fc7\u6700\u5c0f\u5316Onsager-Machlup\u52a8\u4f5c\u6cdb\u51fd\u6765\u8bc6\u522b\u9ad8\u6982\u7387\u8def\u5f84\u3002", "result": "\u5728\u591a\u79cd\u5206\u5b50\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u751f\u6210\u4e86\u591a\u6837\u5316\u4e14\u7269\u7406\u771f\u5b9e\u7684\u8fc7\u6e21\u8def\u5f84\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u9884\u8bad\u7ec3\u6a21\u578b\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u4e4b\u5916\u7684\u8303\u56f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u8fc7\u6e21\u8def\u5f84\u91c7\u6837\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u5e76\u80fd\u968f\u7740\u751f\u6210\u6a21\u578b\u7684\u6539\u8fdb\u800c\u6301\u7eed\u53d7\u76ca\u3002"}}
{"id": "2504.18519", "pdf": "https://arxiv.org/pdf/2504.18519", "abs": "https://arxiv.org/abs/2504.18519", "authors": ["Han Zhang", "Hao Zhou", "Medhat Elsayed", "Majid Bavand", "Raimundas Gaigalas", "Yigit Ozcan", "Melike Erol-Kantarci"], "title": "Intelligent Attacks and Defense Methods in Federated Learning-enabled Energy-Efficient Wireless Networks", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning (FL) is a promising technique for learning-based functions\nin wireless networks, thanks to its distributed implementation capability. On\nthe other hand, distributed learning may increase the risk of exposure to\nmalicious attacks where attacks on a local model may spread to other models by\nparameter exchange. Meanwhile, such attacks can be hard to detect due to the\ndynamic wireless environment, especially considering local models can be\nheterogeneous with non-independent and identically distributed (non-IID) data.\nTherefore, it is critical to evaluate the effect of malicious attacks and\ndevelop advanced defense techniques for FL-enabled wireless networks. In this\nwork, we introduce a federated deep reinforcement learning-based cell sleep\ncontrol scenario that enhances the energy efficiency of the network. We propose\nmultiple intelligent attacks targeting the learning-based approach and we\npropose defense methods to mitigate such attacks. In particular, we have\ndesigned two attack models, generative adversarial network (GAN)-enhanced model\npoisoning attack and regularization-based model poisoning attack. As a\ncounteraction, we have proposed two defense schemes, autoencoder-based defense,\nand knowledge distillation (KD)-enabled defense. The autoencoder-based defense\nmethod leverages an autoencoder to identify the malicious participants and only\naggregate the parameters of benign local models during the global aggregation,\nwhile KD-based defense protects the model from attacks by controlling the\nknowledge transferred between the global model and local models.", "AI": {"tldr": "\u63a2\u8ba8\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u9762\u4e34\u7684\u6076\u610f\u653b\u51fb\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u7761\u7720\u63a7\u5236\u65b9\u6848\uff0c\u4ee5\u53ca\u9488\u5bf9GAN\u589e\u5f3a\u7684\u6a21\u578b\u6295\u6bd2\u653b\u51fb\u548c\u6b63\u5219\u5316\u653b\u51fb\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u5206\u5e03\u5f0f\u7279\u6027\uff0c\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5bb9\u6613\u53d7\u5230\u6076\u610f\u653b\u51fb\uff0c\u4e14\u52a8\u6001\u73af\u5883\u548c\u975eIID\u6570\u636e\u589e\u52a0\u4e86\u68c0\u6d4b\u96be\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u653b\u51fb\u5f71\u54cd\u5e76\u5f00\u53d1\u9632\u5fa1\u6280\u672f\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u79cd\u653b\u51fb\u6a21\u578b\uff08GAN\u589e\u5f3a\u7684\u6a21\u578b\u6295\u6bd2\u653b\u51fb\u548c\u6b63\u5219\u5316\u653b\u51fb\uff09\u548c\u4e24\u79cd\u9632\u5fa1\u65b9\u6848\uff08\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u548c\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u9632\u5fa1\uff09\u3002", "result": "\u63d0\u51fa\u7684\u9632\u5fa1\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u6076\u610f\u53c2\u4e0e\u8005\u5e76\u4fdd\u62a4\u6a21\u578b\u514d\u53d7\u653b\u51fb\uff0c\u4ece\u800c\u63d0\u5347\u7f51\u7edc\u80fd\u6548\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9632\u5fa1\u65b9\u6848\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2504.18538", "pdf": "https://arxiv.org/pdf/2504.18538", "abs": "https://arxiv.org/abs/2504.18538", "authors": ["Yixiao Wang"], "title": "Generalization Capability for Imitation Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Imitation learning holds the promise of equipping robots with versatile\nskills by learning from expert demonstrations. However, policies trained on\nfinite datasets often struggle to generalize beyond the training distribution.\nIn this work, we present a unified perspective on the generalization capability\nof imitation learning, grounded in both information theorey and data\ndistribution property. We first show that the generalization gap can be upper\nbounded by (i) the conditional information bottleneck on intermediate\nrepresentations and (ii) the mutual information between the model parameters\nand the training dataset. This characterization provides theoretical guidance\nfor designing effective training strategies in imitation learning, particularly\nin determining whether to freeze, fine-tune, or train large pretrained encoders\n(e.g., vision-language models or vision foundation models) from scratch to\nachieve better generalization. Furthermore, we demonstrate that high\nconditional entropy from input to output induces a flatter likelihood\nlandscape, thereby reducing the upper bound on the generalization gap. In\naddition, it shortens the stochastic gradient descent (SGD) escape time from\nsharp local minima, which may increase the likelihood of reaching global optima\nunder fixed optimization budgets. These insights explain why imitation learning\noften exhibits limited generalization and underscore the importance of not only\nscaling the diversity of input data but also enriching the variability of\noutput labels conditioned on the same input.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u548c\u6570\u636e\u5206\u5e03\u7279\u6027\u7684\u6a21\u4eff\u5b66\u4e60\u6cdb\u5316\u80fd\u529b\u7edf\u4e00\u89c6\u89d2\uff0c\u5305\u62ec\u4e0a\u9650\u5206\u6790\u53ca\u8bad\u7ec3\u7b56\u7565\u8bbe\u8ba1\u6307\u5bfc\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u4e0a\u6709\u6f5c\u529b\uff0c\u4f46\u6709\u9650\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u96be\u4ee5\u6cdb\u5316\u3002", "method": "\u901a\u8fc7\u6761\u4ef6\u4fe1\u606f\u74f6\u9888\u548c\u6a21\u578b\u53c2\u6570\u4e0e\u8bad\u7ec3\u6570\u636e\u7684\u4e92\u4fe1\u606f\u6765\u754c\u5b9a\u6cdb\u5316\u5dee\u8ddd\u4e0a\u9650\uff0c\u5e76\u5206\u6790\u6761\u4ef6\u71b5\u4e0e\u68af\u5ea6\u4e0b\u964d\u9003\u9038\u65f6\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u9ad8\u6761\u4ef6\u71b5\u4f1a\u5bfc\u81f4\u5e73\u5766\u4f3c\u7136\u666f\u89c2\uff0c\u51cf\u5c11\u6cdb\u5316\u5dee\u8ddd\uff0c\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "\u6a21\u4eff\u5b66\u4e60\u7684\u6cdb\u5316\u53d7\u9650\u539f\u56e0\u88ab\u89e3\u91ca\uff0c\u5f3a\u8c03\u4e86\u589e\u5f3a\u8f93\u5165\u6570\u636e\u591a\u6837\u6027\u548c\u8f93\u51fa\u6807\u7b7e\u53d8\u5f02\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.17794", "pdf": "https://arxiv.org/pdf/2504.17794", "abs": "https://arxiv.org/abs/2504.17794", "authors": ["Dhadkan Shrestha", "Lincoln Bhattarai"], "title": "Near-Driven Autonomous Rover Navigation in Complex Environments: Extensions to Urban Search-and-Rescue and Industrial Inspection", "categories": ["cs.NE", "cs.LG", "cs.RO"], "comment": null, "summary": "This paper explores the use of an extended neuroevolutionary approach, based\non NeuroEvolution of Augmenting Topologies (NEAT), for autonomous robots in\ndynamic environments associated with hazardous tasks like firefighting, urban\nsearch-and-rescue (USAR), and industrial inspections. Building on previous\nresearch, it expands the simulation environment to larger and more complex\nsettings, demonstrating NEAT's adaptability across different applications. By\nintegrating recent advancements in NEAT and reinforcement learning, the study\nuses modern simulation frameworks for realism and hybrid algorithms for\noptimization. Experimental results show that NEAT-evolved controllers achieve\nsuccess rates comparable to state-of-the-art deep reinforcement learning\nmethods, with superior structural adaptability. The agents reached ~80% success\nin outdoor tests, surpassing baseline models. The paper also highlights the\nbenefits of transfer learning among tasks and evaluates the effectiveness of\nNEAT in complex 3D navigation. Contributions include evaluating NEAT for\ndiverse autonomous applications and discussing real-world deployment\nconsiderations, emphasizing the approach's potential as an alternative or\ncomplement to deep reinforcement learning in autonomous navigation tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u57fa\u4e8eNEAT\u7684\u6269\u5c55\u795e\u7ecf\u8fdb\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5371\u9669\u4efb\u52a1\u4e2d\u7684\u81ea\u4e3b\u673a\u5668\u4eba\uff0c\u5c55\u793a\u4e86NEAT\u7684\u9002\u5e94\u6027\u548c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63a2\u8ba8NEAT\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u81ea\u4e3b\u673a\u5668\u4eba\u4efb\u52a1\uff08\u5982\u6d88\u9632\u3001\u6551\u63f4\u548c\u5de5\u4e1a\u68c0\u67e5\uff09\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4f5c\u4e3a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u66ff\u4ee3\u6216\u8865\u5145\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u73af\u5883\u6269\u5c55\u548c\u6df7\u5408\u7b97\u6cd5\u4f18\u5316\uff0c\u7ed3\u5408NEAT\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6d4b\u8bd5\u5176\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u4e2d\uff0cNEAT\u63a7\u5236\u5668\u7684\u6210\u529f\u7387\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\uff08\u7ea680%\uff09\uff0c\u4e14\u7ed3\u6784\u9002\u5e94\u6027\u66f4\u4f18\u3002", "conclusion": "NEAT\u5728\u591a\u6837\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u5907\u90e8\u7f72\u6f5c\u529b\uff0c\u53ef\u66ff\u4ee3\u6216\u8865\u5145\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2504.17811", "pdf": "https://arxiv.org/pdf/2504.17811", "abs": "https://arxiv.org/abs/2504.17811", "authors": ["Anirudhan Badrinath", "Alex Yang", "Kousik Rajesh", "Prabhat Agarwal", "Jaewon Yang", "Haoyu Chen", "Jiajing Xu", "Charles Rosenberg"], "title": "OmniSage: Large Scale, Multi-Entity Heterogeneous Graph Representation Learning", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "Representation learning, a task of learning latent vectors to represent\nentities, is a key task in improving search and recommender systems in web\napplications. Various representation learning methods have been developed,\nincluding graph-based approaches for relationships among entities,\nsequence-based methods for capturing the temporal evolution of user activities,\nand content-based models for leveraging text and visual content. However, the\ndevelopment of a unifying framework that integrates these diverse techniques to\nsupport multiple applications remains a significant challenge. This paper\npresents OmniSage, a large-scale representation framework that learns universal\nrepresentations for a variety of applications at Pinterest. OmniSage integrates\ngraph neural networks with content-based models and user sequence models by\nemploying multiple contrastive learning tasks to effectively process graph\ndata, user sequence data, and content signals. To support the training and\ninference of OmniSage, we developed an efficient infrastructure capable of\nsupporting Pinterest graphs with billions of nodes. The universal\nrepresentations generated by OmniSage have significantly enhanced user\nexperiences on Pinterest, leading to an approximate 2.5% increase in sitewide\nrepins (saves) across five applications. This paper highlights the impact of\nunifying representation learning methods, and we will open source the OmniSage\ncode by the time of publication.", "AI": {"tldr": "OmniSage\u662f\u4e00\u4e2a\u7edf\u4e00\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u3001\u5185\u5bb9\u6a21\u578b\u548c\u7528\u6237\u5e8f\u5217\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u4efb\u52a1\u5904\u7406\u591a\u79cd\u6570\u636e\u7c7b\u578b\uff0c\u663e\u8457\u63d0\u5347Pinterest\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u5f53\u524d\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u591a\u6837\u4f46\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\uff0c\u96be\u4ee5\u652f\u6301\u591a\u79cd\u5e94\u7528\u3002OmniSage\u65e8\u5728\u6574\u5408\u56fe\u6570\u636e\u3001\u7528\u6237\u5e8f\u5217\u548c\u5185\u5bb9\u4fe1\u53f7\uff0c\u63d0\u4f9b\u901a\u7528\u8868\u793a\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u5bf9\u6bd4\u5b66\u4e60\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u3001\u5185\u5bb9\u6a21\u578b\u548c\u7528\u6237\u5e8f\u5217\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u9ad8\u6548\u57fa\u7840\u8bbe\u65bd\u652f\u6301\u6570\u5341\u4ebf\u8282\u70b9\u7684\u56fe\u6570\u636e\u5904\u7406\u3002", "result": "OmniSage\u7684\u901a\u7528\u8868\u793a\u4f7fPinterest\u5168\u7ad9\u201c\u4fdd\u5b58\u201d\u64cd\u4f5c\u63d0\u5347\u7ea62.5%\uff0c\u8986\u76d6\u4e94\u4e2a\u5e94\u7528\u573a\u666f\u3002", "conclusion": "\u7edf\u4e00\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u5177\u6709\u663e\u8457\u6548\u679c\uff0cOmniSage\u4ee3\u7801\u5c06\u5f00\u6e90\uff0c\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2504.17819", "pdf": "https://arxiv.org/pdf/2504.17819", "abs": "https://arxiv.org/abs/2504.17819", "authors": ["Mohaddeseh Chegini", "Ali Mahloojifar"], "title": "A Deep Bayesian Convolutional Spiking Neural Network-based CAD system with Uncertainty Quantification for Medical Images Classification", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "The Computer_Aided Diagnosis (CAD) systems facilitate accurate diagnosis of\ndiseases. The development of CADs by leveraging third generation neural\nnetwork, namely, Spiking Neural Network (SNN), is essential to utilize of the\nbenefits of SNNs, such as their event_driven processing, parallelism, low power\nconsumption, and the ability to process sparse temporal_spatial information.\nHowever, Deep SNN as a deep learning model faces challenges with unreliability.\nTo deal with unreliability challenges due to inability to quantify the\nuncertainty of the predictions, we proposed a deep Bayesian Convolutional\nSpiking Neural Network based_CADs with uncertainty_aware module. In this study,\nthe Monte Carlo Dropout method as Bayesian approximation is used as an\nuncertainty quantification method. This method was applied to several medical\nimage classification tasks. Our experimental results demonstrate that our\nproposed model is accurate and reliable and will be a proper alternative to\nconventional deep learning for medical image classification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8d1d\u53f6\u65af\u5377\u79ef\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684CAD\u7cfb\u7edf\uff0c\u7528\u4e8e\u533b\u7597\u56fe\u50cf\u5206\u7c7b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6df1\u5ea6SNN\u7684\u4e0d\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u5728\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u4e2d\u5b58\u5728\u4e0d\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u96be\u4ee5\u91cf\u5316\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u8d1d\u53f6\u65af\u5377\u79ef\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684CAD\u7cfb\u7edf\uff0c\u91c7\u7528\u8499\u7279\u5361\u6d1bDropout\u65b9\u6cd5\u4f5c\u4e3a\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u624b\u6bb5\uff0c\u5e76\u5e94\u7528\u4e8e\u591a\u9879\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u4e0d\u4ec5\u51c6\u786e\u6027\u9ad8\uff0c\u800c\u4e14\u53ef\u9760\u6027\u5f3a\uff0c\u80fd\u591f\u6210\u4e3a\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u8d1d\u53f6\u65af\u65b9\u6cd5\u548cSNN\uff0c\u672c\u7814\u7a76\u6210\u529f\u63d0\u5347\u4e86CAD\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17836", "pdf": "https://arxiv.org/pdf/2504.17836", "abs": "https://arxiv.org/abs/2504.17836", "authors": ["Eviatar Bach", "Ricardo Baptista", "Edoardo Calvello", "Bohan Chen", "Andrew Stuart"], "title": "Learning Enhanced Ensemble Filters", "categories": ["stat.ML", "cs.LG", "cs.SY", "eess.SY", "physics.comp-ph"], "comment": "Preprint submitted to Journal of Computational Physics", "summary": "The filtering distribution in hidden Markov models evolves according to the\nlaw of a mean-field model in state--observation space. The ensemble Kalman\nfilter (EnKF) approximates this mean-field model with an ensemble of\ninteracting particles, employing a Gaussian ansatz for the joint distribution\nof the state and observation at each observation time. These methods are\nrobust, but the Gaussian ansatz limits accuracy. This shortcoming is addressed\nby approximating the mean-field evolution using a novel form of neural operator\ntaking probability distributions as input: a Measure Neural Mapping (MNM). A\nMNM is used to design a novel approach to filtering, the MNM-enhanced ensemble\nfilter (MNMEF), which is defined in both the mean-fieldlimit and for\ninteracting ensemble particle approximations. The ensemble approach uses\nempirical measures as input to the MNM and is implemented using the set\ntransformer, which is invariant to ensemble permutation and allows for\ndifferent ensemble sizes. The derivation of methods from a mean-field\nformulation allows a single parameterization of the algorithm to be deployed at\ndifferent ensemble sizes. In practice fine-tuning of a small number of\nparameters, for specific ensemble sizes, further enhances the accuracy of the\nscheme. The promise of the approach is demonstrated by its superior\nroot-mean-square-error performance relative to leading methods in filtering the\nLorenz 96 and Kuramoto-Sivashinsky models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7b97\u5b50\u7684\u65b0\u6ee4\u6ce2\u65b9\u6cd5\uff08MNMEF\uff09\uff0c\u901a\u8fc7\u6539\u8fdb\u9ad8\u65af\u5047\u8bbe\u7684\u9650\u5236\uff0c\u63d0\u5347\u4e86\u9690\u85cf\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u4e2d\u7684\u6ee4\u6ce2\u7cbe\u5ea6\uff0c\u5e76\u5728Lorenz 96\u548cKuramoto-Sivashinsky\u6a21\u578b\u4e2d\u5c55\u73b0\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u96c6\u6210\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08EnKF\uff09\u56e0\u9ad8\u65af\u5047\u8bbe\u9650\u5236\u4e86\u5176\u7cbe\u5ea6\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u7b97\u5b50\uff08MNM\uff09\u8bbe\u8ba1\u66f4\u7cbe\u786e\u7684\u6ee4\u6ce2\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMNMEF\u7684\u65b0\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u7b97\u5b50\uff08MNM\uff09\u8fd1\u4f3c\u5e73\u5747\u573a\u6f14\u5316\uff0c\u5e76\u901a\u8fc7\u96c6\u603b\u8f6c\u6362\u5668\u5b9e\u73b0\u591a\u5c3a\u5ea6\u53c2\u6570\u5316\u90e8\u7f72\u3002", "result": "\u5728Lorenz 96\u548cKuramoto-Sivashinsky\u6a21\u578b\u4e2d\uff0cMNMEF\u7684\u5747\u65b9\u6839\u8bef\u5dee\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MNMEF\u901a\u8fc7\u795e\u7ecf\u7b97\u5b50\u7a81\u7834\u4e86\u9ad8\u65af\u5047\u8bbe\u7684\u5c40\u9650\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6ee4\u6ce2\u7cbe\u5ea6\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2504.18165", "pdf": "https://arxiv.org/pdf/2504.18165", "abs": "https://arxiv.org/abs/2504.18165", "authors": ["Michel Gokan Khan", "Renan Guarese", "Fabian Johnson", "Xi Vincent Wang", "Anders Bergman", "Benjamin Edvinsson", "Mario Romero", "J\u00e9r\u00e9my Vachier", "Jan Kronqvist"], "title": "PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning\nframework that combines camera and sensory data with 3D Gaussian Splatting and\ncomputer vision models for digital twinning, object tracking, and Key\nPerformance Indicators (KPIs) extraction in industrial production lines. By\nutilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam\noffers a semi-automated approach to object tracking and spatial mapping,\nenabling digital twins that capture real-time KPIs such as availability,\nperformance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts\nin the production line. We validate the effectiveness of PerfCam through a\npractical deployment within realistic test production lines in the\npharmaceutical industry and contribute an openly published dataset to support\nfurther research and development in the field. The results demonstrate\nPerfCam's ability to deliver actionable insights through its precise digital\ntwin capabilities, underscoring its value as an effective tool for developing\nusable digital twins in smart manufacturing environments and extracting\noperational analytics.", "AI": {"tldr": "PerfCam\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u6570\u5b57\u5b6a\u751f\u6846\u67b6\uff0c\u7ed3\u5408\u76f8\u673a\u548c\u4f20\u611f\u5668\u6570\u636e\u30013D\u9ad8\u65af\u55b7\u7ed8\u53ca\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\uff0c\u7528\u4e8e\u5de5\u4e1a\u751f\u4ea7\u7ebf\u4e2d\u7684\u6570\u5b57\u5b6a\u751f\u3001\u7269\u4f53\u8ddf\u8e2a\u548c\u5173\u952e\u7ee9\u6548\u6307\u6807\uff08KPIs\uff09\u63d0\u53d6\u3002\u901a\u8fc73D\u91cd\u5efa\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\uff0c\u5b9e\u73b0\u4e86\u534a\u81ea\u52a8\u5316\u7684\u7269\u4f53\u8ddf\u8e2a\u4e0e\u7a7a\u95f4\u6620\u5c04\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u5236\u836f\u884c\u4e1a\u5b9e\u9645\u751f\u4ea7\u7ebf\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5de5\u4e1a\u751f\u4ea7\u7ebf\u4e2d\u5b9e\u65f6\u6570\u5b57\u5b6a\u751f\u548cKPIs\u63d0\u53d6\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u64cd\u4f5c\u7684\u5de5\u5177\u4ee5\u652f\u6301\u667a\u80fd\u5236\u9020\u73af\u5883\u3002", "method": "\u91c7\u75283D\u9ad8\u65af\u55b7\u7ed8\u548cCNNs\u5b9e\u73b0\u7269\u4f53\u8ddf\u8e2a\u4e0e\u7a7a\u95f4\u6620\u5c04\uff0c\u7ed3\u5408\u76f8\u673a\u548c\u4f20\u611f\u5668\u6570\u636e\u8fdb\u884c\u534a\u81ea\u52a8\u5316\u6570\u5b57\u5b6a\u751f\u3002", "result": "\u5728\u5236\u836f\u884c\u4e1a\u5b9e\u9645\u751f\u4ea7\u7ebf\u4e2d\u9a8c\u8bc1\u4e86PerfCam\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u7cbe\u786e\u7684\u6570\u5b57\u5b6a\u751f\u80fd\u529b\u548c\u53ef\u64cd\u4f5c\u7684\u6d1e\u5bdf\u529b\u3002", "conclusion": "PerfCam\u4e3a\u667a\u80fd\u5236\u9020\u73af\u5883\u4e2d\u7684\u6570\u5b57\u5b6a\u751f\u5f00\u53d1\u548c\u8fd0\u8425\u5206\u6790\u63d0\u53d6\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u516c\u5f00\u6570\u636e\u96c6\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2504.17874", "pdf": "https://arxiv.org/pdf/2504.17874", "abs": "https://arxiv.org/abs/2504.17874", "authors": ["Zemin Zheng", "Xin Zhou", "Jinchi Lv"], "title": "SOFARI-R: High-Dimensional Manifold-Based Inference for Latent Responses", "categories": ["stat.ME", "cs.LG", "stat.ML"], "comment": "90 pages, 2 figures", "summary": "Data reduction with uncertainty quantification plays a key role in various\nmulti-task learning applications, where large numbers of responses and features\nare present. To this end, a general framework of high-dimensional\nmanifold-based SOFAR inference (SOFARI) was introduced recently in Zheng, Zhou,\nFan and Lv (2024) for interpretable multi-task learning inference focusing on\nthe left factor vectors and singular values exploiting the latent singular\nvalue decomposition (SVD) structure. Yet, designing a valid inference procedure\non the latent right factor vectors is not straightforward from that of the left\nones and can be even more challenging due to asymmetry of left and right\nsingular vectors in the response matrix. To tackle these issues, in this paper\nwe suggest a new method of high-dimensional manifold-based SOFAR inference for\nlatent responses (SOFARI-R), where two variants of SOFARI-R are introduced. The\nfirst variant deals with strongly orthogonal factors by coupling left singular\nvectors with the design matrix and then appropriately rescaling them to\ngenerate new Stiefel manifolds. The second variant handles the more general\nweakly orthogonal factors by employing the hard-thresholded SOFARI estimates\nand delicately incorporating approximation errors into the distribution. Both\nvariants produce bias-corrected estimators for the latent right factor vectors\nthat enjoy asymptotically normal distributions with justified asymptotic\nvariance estimates. We demonstrate the effectiveness of the newly suggested\nmethod using extensive simulation studies and an economic application.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SOFARI-R\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u7ef4\u6d41\u5f62\u4e0a\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u63a8\u65ad\uff0c\u91cd\u70b9\u5173\u6ce8\u6f5c\u5728\u53f3\u56e0\u5b50\u5411\u91cf\u7684\u6709\u6548\u63a8\u65ad\uff0c\u901a\u8fc7\u4e24\u79cd\u53d8\u4f53\u5904\u7406\u5f3a\u6b63\u4ea4\u548c\u5f31\u6b63\u4ea4\u56e0\u5b50\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u73b0\u6709SOFAR\u65b9\u6cd5\u5728\u6f5c\u5728\u5de6\u56e0\u5b50\u5411\u91cf\u548c\u5947\u5f02\u503c\u7684\u63a8\u65ad\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5bf9\u53f3\u56e0\u5b50\u5411\u91cf\u7684\u63a8\u65ad\u4ecd\u5177\u6311\u6218\u6027\uff0c\u56e0\u5176\u4e0e\u5de6\u5411\u91cf\u4e0d\u5bf9\u79f0\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u89e3\u51b3\u53f3\u56e0\u5b50\u5411\u91cf\u7684\u63a8\u65ad\u95ee\u9898\u3002", "method": "\u63d0\u51faSOFARI-R\u65b9\u6cd5\uff0c\u5305\u62ec\u4e24\u79cd\u53d8\u4f53\uff1a\u4e00\u3001\u9488\u5bf9\u5f3a\u6b63\u4ea4\u56e0\u5b50\uff0c\u901a\u8fc7\u8026\u5408\u5de6\u5947\u5f02\u5411\u91cf\u4e0e\u8bbe\u8ba1\u77e9\u9635\u5e76\u91cd\u65b0\u7f29\u653e\u751f\u6210\u65b0Stiefel\u6d41\u5f62\uff1b\u4e8c\u3001\u9488\u5bf9\u5f31\u6b63\u4ea4\u56e0\u5b50\uff0c\u4f7f\u7528\u786c\u9608\u503cSOFARI\u4f30\u8ba1\u5e76\u5c06\u8fd1\u4f3c\u8bef\u5dee\u7eb3\u5165\u5206\u5e03\u3002", "result": "\u4e24\u79cd\u53d8\u4f53\u5747\u80fd\u751f\u6210\u504f\u5dee\u6821\u6b63\u7684\u6f5c\u5728\u53f3\u56e0\u5b50\u5411\u91cf\u4f30\u8ba1\u91cf\uff0c\u5177\u6709\u6e10\u8fd1\u6b63\u6001\u5206\u5e03\u548c\u5408\u7406\u7684\u6e10\u8fd1\u65b9\u5dee\u4f30\u8ba1\u3002\u6a21\u62df\u7814\u7a76\u548c\u7ecf\u6d4e\u5e94\u7528\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "SOFARI-R\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u5bf9\u6f5c\u5728\u53f3\u56e0\u5b50\u5411\u91cf\u7684\u6709\u6548\u63a8\u65ad\uff0c\u63d0\u4f9b\u4e86\u504f\u5dee\u6821\u6b63\u4f30\u8ba1\u548c\u6e10\u8fd1\u6b63\u6001\u6027\uff0c\u6269\u5c55\u4e86SOFAR\u6846\u67b6\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2504.18201", "pdf": "https://arxiv.org/pdf/2504.18201", "abs": "https://arxiv.org/abs/2504.18201", "authors": ["Yin Tang", "Jiankai Li", "Hongyu Yang", "Xuan Dong", "Lifeng Fan", "Weixin Li"], "title": "Multi-Grained Compositional Visual Clue Learning for Image Intent Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In an era where social media platforms abound, individuals frequently share\nimages that offer insights into their intents and interests, impacting\nindividual life quality and societal stability. Traditional computer vision\ntasks, such as object detection and semantic segmentation, focus on concrete\nvisual representations, while intent recognition relies more on implicit visual\nclues. This poses challenges due to the wide variation and subjectivity of such\nclues, compounded by the problem of intra-class variety in conveying abstract\nconcepts, e.g. \"enjoy life\". Existing methods seek to solve the problem by\nmanually designing representative features or building prototypes for each\nclass from global features. However, these methods still struggle to deal with\nthe large visual diversity of each intent category. In this paper, we introduce\na novel approach named Multi-grained Compositional visual Clue Learning (MCCL)\nto address these challenges for image intent recognition. Our method leverages\nthe systematic compositionality of human cognition by breaking down intent\nrecognition into visual clue composition and integrating multi-grained\nfeatures. We adopt class-specific prototypes to alleviate data imbalance. We\ntreat intent recognition as a multi-label classification problem, using a graph\nconvolutional network to infuse prior knowledge through label embedding\ncorrelations. Demonstrated by a state-of-the-art performance on the Intentonomy\nand MDID datasets, our approach advances the accuracy of existing methods while\nalso possessing good interpretability. Our work provides an attempt for future\nexplorations in understanding complex and miscellaneous forms of human\nexpression.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMCCL\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u89c6\u89c9\u7ebf\u7d22\u7ec4\u5408\u5b66\u4e60\u6765\u8bc6\u522b\u56fe\u50cf\u610f\u56fe\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u591a\u6837\u6027\u548c\u4e3b\u89c2\u6027\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u8868\u73b0\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u56fe\u50cf\u7684\u5e7f\u6cdb\u5206\u4eab\u63ed\u793a\u4e86\u7528\u6237\u7684\u610f\u56fe\u548c\u5174\u8da3\uff0c\u4f46\u4f20\u7edf\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u96be\u4ee5\u6355\u6349\u8fd9\u4e9b\u9690\u542b\u7ebf\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u610f\u56fe\u7c7b\u522b\u7684\u89c6\u89c9\u591a\u6837\u6027\u65f6\u6548\u679c\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u65b9\u6cd5\u63d0\u5347\u8bc6\u522b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faMCCL\u65b9\u6cd5\uff0c\u5206\u89e3\u610f\u56fe\u8bc6\u522b\u7684\u89c6\u89c9\u7ebf\u7d22\u5e76\u6574\u5408\u591a\u7c92\u5ea6\u7279\u5f81\uff0c\u4f7f\u7528\u7c7b\u7279\u5b9a\u539f\u578b\u7f13\u89e3\u6570\u636e\u4e0d\u5e73\u8861\uff0c\u5e76\u901a\u8fc7\u56fe\u5377\u79ef\u7f51\u7edc\u7ed3\u5408\u6807\u7b7e\u5d4c\u5165\u5148\u9a8c\u77e5\u8bc6\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\u3002", "result": "\u5728Intentonomy\u548cMDID\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u5177\u5907\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "MCCL\u4e3a\u7406\u89e3\u4eba\u7c7b\u590d\u6742\u591a\u6837\u7684\u8868\u8fbe\u5f62\u5f0f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u76f8\u5173\u5e94\u7528\u3002"}}
{"id": "2504.17930", "pdf": "https://arxiv.org/pdf/2504.17930", "abs": "https://arxiv.org/abs/2504.17930", "authors": ["Abrar Fahim", "Shamik Dey", "Md. Nurul Absur", "Md Kamrul Siam", "Md. Tahmidul Huque", "Jafreen Jafor Godhuli"], "title": "Optimized Approaches to Malware Detection: A Study of Machine Learning and Deep Learning Techniques", "categories": ["cs.CR", "cs.LG"], "comment": "9 pages", "summary": "Digital systems find it challenging to keep up with cybersecurity threats.\nThe daily emergence of more than 560,000 new malware strains poses significant\nhazards to the digital ecosystem. The traditional malware detection methods\nfail to operate properly and yield high false positive rates with low accuracy\nof the protection system. This study explores the ways in which malware can be\ndetected using these machine learning (ML) and deep learning (DL) approaches to\naddress those shortcomings. This study also includes a systematic comparison of\nthe performance of some of the widely used ML models, such as random forest,\nmulti-layer perceptron (MLP), and deep neural network (DNN), for determining\nthe effectiveness of the domain of modern malware threat systems. We use a\nconsiderable-sized database from Kaggle, which has undergone optimized feature\nselection and preprocessing to improve model performance. Our finding suggests\nthat the DNN model outperformed the other traditional models with the highest\ntraining accuracy of 99.92% and an almost perfect AUC score. Furthermore, the\nfeature selection and preprocessing can help improve the capabilities of\ndetection. This research makes an important contribution by analyzing the\nperformance of the model on the performance metrics and providing insight into\nthe effectiveness of the advanced detection techniques to build more robust and\nmore reliable cybersecurity solutions against the growing malware threats.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u65b9\u6cd5\u68c0\u6d4b\u6076\u610f\u8f6f\u4ef6\uff0c\u6bd4\u8f83\u4e86\u51e0\u79cd\u5e38\u7528\u6a21\u578b\u7684\u6027\u80fd\uff0c\u53d1\u73b0DNN\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe99.92%\u3002\u901a\u8fc7\u7279\u5f81\u9009\u62e9\u548c\u9884\u5904\u7406\u63d0\u9ad8\u4e86\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u65b9\u6cd5\u5728\u9ad8\u8bef\u62a5\u7387\u548c\u4f4e\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u6bcf\u5929\u65b0\u589e\u7684\u6076\u610f\u8f6f\u4ef6\u5a01\u80c1\u5bf9\u6570\u5b57\u751f\u6001\u7cfb\u7edf\u6784\u6210\u91cd\u5927\u98ce\u9669\uff0c\u4fc3\u4f7f\u7814\u7a76\u91c7\u7528ML\u548cDL\u6280\u672f\u6539\u8fdb\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u968f\u673a\u68ee\u6797\u3001\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u7b49\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f7f\u7528\u4e86Kaggle\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u4e86\u7279\u5f81\u9009\u62e9\u548c\u9884\u5904\u7406\u4f18\u5316\u3002", "result": "DNN\u6a21\u578b\u8868\u73b0\u6700\u4f18\uff0c\u8bad\u7ec3\u51c6\u786e\u7387\u8fbe99.92%\uff0cAUC\u5206\u6570\u63a5\u8fd1\u5b8c\u7f8e\u3002\u7279\u5f81\u9009\u62e9\u548c\u9884\u5904\u7406\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u6a21\u578b\u6027\u80fd\u6307\u6807\uff0c\u8bc1\u660e\u4e86\u5148\u8fdb\u68c0\u6d4b\u6280\u672f\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u53ef\u9760\u7684\u7f51\u7edc\u5b89\u5168\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2504.17938", "pdf": "https://arxiv.org/pdf/2504.17938", "abs": "https://arxiv.org/abs/2504.17938", "authors": ["Raza Ul Mustafa", "Sesha Dassanayake"], "title": "Machine Learning-Based Prediction of Quality Shifts on Video Streaming Over 5G", "categories": ["cs.MM", "cs.LG"], "comment": null, "summary": "The Quality of Experience (QoE) is the users satisfaction while streaming a\nvideo session over an over-the-top (OTT) platform like YouTube. QoE of YouTube\nreflects the smooth streaming session without any buffering and quality shift\nevents. One of the most important factors nowadays affecting QoE of YouTube is\nfrequent shifts from higher to lower resolutions and vice versa. These shifts\nensure a smooth streaming session; however, it might get a lower mean opinion\nscore. For instance, dropping from 1080p to 480p during a video can preserve\ncontinuity but might reduce the viewers enjoyment. Over time, OTT platforms are\nlooking for alternative ways to boost user experience instead of relying on\ntraditional Quality of Service (QoS) metrics such as bandwidth, latency, and\nthroughput. As a result, we look into the relationship between quality shifting\nin YouTube streaming sessions and the channel metrics RSRP, RSRQ, and SNR. Our\nfindings state that these channel metrics positively correlate with shifts.\nThus, in real-time, OTT can only rely on them to predict video streaming\nsessions into lower- and higher-resolution categories, thus providing more\nresources to improve user experience. Using traditional Machine Learning (ML)\nclassifiers, we achieved an accuracy of 77-percent, while using only RSRP,\nRSRQ, and SNR. In the era of 5G and beyond, where ultra-reliable, low-latency\nnetworks promise enhanced streaming capabilities, the proposed methodology can\nbe used to improve OTT services.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86YouTube\u89c6\u9891\u6d41\u4e2d\u7684\u8d28\u91cf\u8f6c\u6362\uff08\u5982\u5206\u8fa8\u7387\u53d8\u5316\uff09\u4e0e\u7528\u6237QoE\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u901a\u8fc7\u4fe1\u9053\u6307\u6807\uff08RSRP\u3001RSRQ\u3001SNR\uff09\u9884\u6d4b\u8d28\u91cf\u8f6c\u6362\uff0c\u4ee5\u4f18\u5316\u7528\u6237\u4f53\u9a8c\u3002\u4f20\u7edfML\u65b9\u6cd5\u8fbe\u523077%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edfQoS\u6307\u6807\uff08\u5982\u5e26\u5bbd\u3001\u5ef6\u8fdf\uff09\u65e0\u6cd5\u5b8c\u5168\u53cd\u6620\u7528\u6237QoE\uff0cOTT\u5e73\u53f0\u9700\u66f4\u6709\u6548\u7684\u9884\u6d4b\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u6d41\u5a92\u4f53\u4f53\u9a8c\u3002\u4f5c\u8005\u63a2\u7d22\u4e86\u8d28\u91cf\u8f6c\u6362\u4e0e\u4fe1\u9053\u6307\u6807\u7684\u5173\u8054\uff0c\u4e3a5G\u65f6\u4ee3\u7684\u6d41\u5a92\u4f53\u4f18\u5316\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "method": "\u901a\u8fc7\u5206\u6790YouTube\u6d41\u5a92\u4f53\u4f1a\u8bdd\u4e2d\u7684\u8d28\u91cf\u8f6c\u6362\u4e8b\u4ef6\uff0c\u7814\u7a76\u5176\u4e0eRSRP\u3001RSRQ\u3001SNR\u7b49\u4fe1\u9053\u6307\u6807\u7684\u76f8\u5173\u6027\u3002\u4f7f\u7528\u4f20\u7edfML\u5206\u7c7b\u5668\uff08\u672a\u6307\u5b9a\u5177\u4f53\u7b97\u6cd5\uff09\u9884\u6d4b\u5206\u8fa8\u7387\u5207\u6362\u7684\u7c7b\u522b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4fe1\u9053\u6307\u6807\u4e0e\u8d28\u91cf\u8f6c\u6362\u663e\u8457\u6b63\u76f8\u5173\u3002\u4ec5\u57fa\u4e8eRSRP\u3001RSRQ\u3001SNR\u7684ML\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u7387\u8fbe77%\u3002", "conclusion": "\u4fe1\u9053\u6307\u6807\u53ef\u4f5c\u4e3a\u5b9e\u65f6\u9884\u6d4b\u8d28\u91cf\u8f6c\u6362\u7684\u6709\u6548\u53c2\u6570\uff0c\u5e2e\u52a9OTT\u5e73\u53f0\u52a8\u6001\u5206\u914d\u8d44\u6e90\u4ee5\u63d0\u5347QoE\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e5G\u53ca\u672a\u6765\u7684\u4f4e\u5ef6\u8fdf\u7f51\u7edc\u73af\u5883\u3002"}}
{"id": "2504.18231", "pdf": "https://arxiv.org/pdf/2504.18231", "abs": "https://arxiv.org/abs/2504.18231", "authors": ["Petar Labura", "Tomislav Antic", "Tomislav Capuder"], "title": "Time and Frequency Domain-based Anomaly Detection in Smart Meter Data for Distribution Network Studies", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": null, "summary": "The widespread integration of new technologies in low-voltage distribution\nnetworks on the consumer side creates the need for distribution system\noperators to perform advanced real-time calculations to estimate network\nconditions. In recent years, data-driven models based on machine learning and\nbig data analysis have emerged for calculation purposes, leveraging the\ninformation available in large datasets obtained from smart meters and other\nadvanced measurement infrastructure. However, existing data-driven algorithms\ndo not take into account the quality of data collected from smart meters. They\nlack built-in anomaly detection mechanisms and fail to differentiate anomalies\nbased on whether the value or context of anomalous data instances deviates from\nthe norm. This paper focuses on methods for detecting and mitigating the impact\nof anomalies on the consumption of active and reactive power datasets. It\nproposes an anomaly detection framework based on the Isolation Forest machine\nlearning algorithm and Fast Fourier Transform filtering that works in both the\ntime and frequency domain and is unaffected by point anomalies or contextual\nanomalies of the power consumption data. The importance of integrating anomaly\ndetection methods is demonstrated in the analysis important for distribution\nnetworks with a high share of smart meters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9694\u79bb\u68ee\u6797\u7b97\u6cd5\u548c\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u4f4e\u7535\u538b\u914d\u7535\u7f51\u7edc\u4e2d\u667a\u80fd\u7535\u8868\u6570\u636e\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u89e3\u51b3\u73b0\u6709\u7b97\u6cd5\u5ffd\u89c6\u6570\u636e\u8d28\u91cf\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u667a\u80fd\u7535\u8868\u548c\u5148\u8fdb\u6d4b\u91cf\u57fa\u7840\u8bbe\u65bd\u4ea7\u751f\u7684\u5927\u91cf\u6570\u636e\u5728\u914d\u7535\u7f51\u7edc\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u73b0\u6709\u6570\u636e\u9a71\u52a8\u7b97\u6cd5\u672a\u80fd\u6709\u6548\u68c0\u6d4b\u548c\u533a\u5206\u6570\u636e\u5f02\u5e38\uff0c\u4ece\u800c\u5bfc\u81f4\u7f51\u7edc\u72b6\u6001\u4f30\u8ba1\u4e0d\u51c6\u786e\u3002", "method": "\u7ed3\u5408\u9694\u79bb\u68ee\u6797\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u548c\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u6ee4\u6ce2\uff0c\u5728\u65f6\u57df\u548c\u9891\u57df\u540c\u65f6\u68c0\u6d4b\u548c\u7f13\u89e3\u4e3b\u52a8\u548c\u65e0\u6548\u529f\u7387\u6570\u636e\u96c6\u4e2d\u7684\u5f02\u5e38\uff0c\u5305\u62ec\u70b9\u5f02\u5e38\u548c\u4e0a\u4e0b\u6587\u5f02\u5e38\u3002", "result": "\u63d0\u51fa\u6846\u67b6\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u548c\u533a\u5206\u7535\u529b\u6d88\u8017\u6570\u636e\u4e2d\u7684\u5f02\u5e38\uff0c\u9002\u7528\u4e8e\u9ad8\u6bd4\u4f8b\u667a\u80fd\u7535\u8868\u7684\u914d\u7535\u7f51\u7edc\u3002", "conclusion": "\u96c6\u6210\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u914d\u7535\u7f51\u7edc\u81f3\u5173\u91cd\u8981\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u5728\u6570\u636e\u8d28\u91cf\u548c\u7f51\u7edc\u72b6\u6001\u4f30\u8ba1\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2504.17939", "pdf": "https://arxiv.org/pdf/2504.17939", "abs": "https://arxiv.org/abs/2504.17939", "authors": ["Josua Spisak", "Sergiu Tcaci Popescu", "Stefan Wermter", "Matej Hoffmann", "J. Kevin O'Regan"], "title": "A computational model of infant sensorimotor exploration in the mobile paradigm", "categories": ["q-bio.NC", "cs.LG", "92-10", "J.4"], "comment": "16 pages, 16 figures", "summary": "We present a computational model of the mechanisms that may determine\ninfants' behavior in the \"mobile paradigm\". This paradigm has been used in\ndevelopmental psychology to explore how infants learn the sensory effects of\ntheir actions. In this paradigm, a mobile (an articulated and movable object\nhanging above an infant's crib) is connected to one of the infant's limbs,\nprompting the infant to preferentially move that \"connected\" limb. This ability\nto detect a \"sensorimotor contingency\" is considered to be a foundational\ncognitive ability in development. To understand how infants learn sensorimotor\ncontingencies, we built a model that attempts to replicate infant behavior. Our\nmodel incorporates a neural network, action-outcome prediction, exploration,\nmotor noise, preferred activity level, and biologically-inspired motor control.\nWe find that simulations with our model replicate the classic findings in the\nliterature showing preferential movement of the connected limb. An interesting\nobservation is that the model sometimes exhibits a burst of movement after the\nmobile is disconnected, casting light on a similar occasional finding in\ninfants. In addition to these general findings, the simulations also replicate\ndata from two recent more detailed studies using a connection with the mobile\nthat was either gradual or all-or-none. A series of ablation studies further\nshows that the inclusion of mechanisms of action-outcome prediction,\nexploration, motor noise, and biologically-inspired motor control was essential\nfor the model to correctly replicate infant behavior. This suggests that these\ncomponents are also involved in infants' sensorimotor learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u6a21\u578b\uff0c\u6a21\u62df\u5a74\u513f\u5728\u201c\u79fb\u52a8\u8303\u5f0f\u201d\u4e2d\u5b66\u4e60\u52a8\u4f5c\u4e0e\u611f\u89c9\u6548\u5e94\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u6a21\u578b\u80fd\u590d\u73b0\u5a74\u513f\u504f\u597d\u79fb\u52a8\u8fde\u63a5\u80a2\u4f53\u7684\u73b0\u8c61\uff0c\u5e76\u63ed\u793a\u4e86\u52a8\u4f5c-\u7ed3\u679c\u9884\u6d4b\u3001\u63a2\u7d22\u3001\u8fd0\u52a8\u566a\u58f0\u548c\u751f\u7269\u542f\u53d1\u8fd0\u52a8\u63a7\u5236\u5728\u5176\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u5a74\u513f\u5982\u4f55\u5728\u2018\u79fb\u52a8\u8303\u5f0f\u2019\u4e2d\u5b66\u4e60\u611f\u77e5\u8fd0\u52a8\u5173\u8054\u6027\uff0c\u8fd9\u662f\u53d1\u5c55\u8ba4\u77e5\u80fd\u529b\u7684\u57fa\u7840\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u795e\u7ecf\u7f51\u7edc\u3001\u52a8\u4f5c-\u7ed3\u679c\u9884\u6d4b\u3001\u63a2\u7d22\u3001\u8fd0\u52a8\u566a\u58f0\u3001\u504f\u597d\u6d3b\u52a8\u6c34\u5e73\u548c\u751f\u7269\u542f\u53d1\u8fd0\u52a8\u63a7\u5236\u7684\u8ba1\u7b97\u6a21\u578b\uff0c\u6a21\u62df\u5a74\u513f\u884c\u4e3a\u3002", "result": "\u6a21\u578b\u6210\u529f\u590d\u73b0\u4e86\u5a74\u513f\u504f\u597d\u79fb\u52a8\u8fde\u63a5\u80a2\u4f53\u7684\u7ecf\u5178\u73b0\u8c61\uff0c\u5e76\u89e3\u91ca\u4e86\u65ad\u5f00\u8fde\u63a5\u540e\u7684\u7a81\u53d1\u8fd0\u52a8\uff1b\u540c\u65f6\u590d\u73b0\u4e86\u6e10\u8fdb\u5f0f\u548c\u5168\u6216\u65e0\u8fde\u63a5\u7684\u5b9e\u9a8c\u6570\u636e\u3002", "conclusion": "\u6a21\u578b\u8bc1\u5b9e\u52a8\u4f5c-\u7ed3\u679c\u9884\u6d4b\u7b49\u673a\u5236\u5bf9\u5a74\u513f\u611f\u77e5\u8fd0\u52a8\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u63ed\u793a\u4e86\u5176\u8ba4\u77e5\u53d1\u5c55\u7684\u6f5c\u5728\u673a\u5236\u3002"}}
{"id": "2504.17953", "pdf": "https://arxiv.org/pdf/2504.17953", "abs": "https://arxiv.org/abs/2504.17953", "authors": ["Ahod Alghuried", "Abdulaziz Alghamdi", "Ali Alkinoon", "Soohyeon Choi", "Manar Mohaisen", "David Mohaisen"], "title": "Fishing for Phishers: Learning-Based Phishing Detection in Ethereum Transactions", "categories": ["cs.CR", "cs.LG"], "comment": "23 pages, 6 tables, 5 figures", "summary": "Phishing detection on Ethereum has increasingly leveraged advanced machine\nlearning techniques to identify fraudulent transactions. However, limited\nattention has been given to understanding the effectiveness of feature\nselection strategies and the role of graph-based models in enhancing detection\naccuracy. In this paper, we systematically examine these issues by analyzing\nand contrasting explicit transactional features and implicit graph-based\nfeatures, both experimentally and analytically. We explore how different\nfeature sets impact the performance of phishing detection models, particularly\nin the context of Ethereum's transactional network. Additionally, we address\nkey challenges such as class imbalance and dataset composition and their\ninfluence on the robustness and precision of detection methods. Our findings\ndemonstrate the advantages and limitations of each feature type, while also\nproviding a clearer understanding of how feature affect model resilience and\ngeneralization in adversarial environments.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4ee5\u592a\u574a\u9493\u9c7c\u68c0\u6d4b\u4e2d\u663e\u5f0f\u4ea4\u6613\u7279\u5f81\u4e0e\u9690\u5f0f\u56fe\u7279\u5f81\u7684\u6548\u80fd\uff0c\u53d1\u73b0\u4e24\u7c7b\u7279\u5f81\u5404\u5177\u4f18\u7f3a\u70b9\uff0c\u5e76\u63a2\u8ba8\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u4e0e\u6570\u636e\u96c6\u6784\u6210\u5bf9\u68c0\u6d4b\u7a33\u5065\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u4ee5\u592a\u574a\u9493\u9c7c\u68c0\u6d4b\u4e2d\u7279\u5f81\u9009\u62e9\u7b56\u7565\u548c\u57fa\u4e8e\u56fe\u7684\u6a21\u578b\u7684\u6709\u6548\u6027\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u5730\u5206\u6790\u548c\u5bf9\u6bd4\u663e\u5f0f\u4ea4\u6613\u7279\u5f81\u4e0e\u9690\u5f0f\u56fe\u7279\u5f81\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5bf9\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u8ba8\u7c7b\u522b\u4e0d\u5e73\u8861\u4e0e\u6570\u636e\u96c6\u6784\u6210\u7684\u6311\u6218\u3002", "result": "\u5c55\u793a\u4e86\u4e24\u7c7b\u7279\u5f81\u5728\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u7387\u65b9\u9762\u7684\u4f18\u52bf\u548c\u5c40\u9650\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u7279\u5f81\u5bf9\u6a21\u578b\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u7a33\u5065\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4ee5\u592a\u574a\u9493\u9c7c\u68c0\u6d4b\u7684\u7279\u5f81\u9009\u62e9\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u7684\u6307\u5bfc\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u7684\u65b9\u5411\u3002"}}
{"id": "2504.18249", "pdf": "https://arxiv.org/pdf/2504.18249", "abs": "https://arxiv.org/abs/2504.18249", "authors": ["Qinyu Chen", "Chang Gao", "Min Liu", "Daniele Perrone", "Yan Ru Pei", "Zuowen Wang", "Zhuo Zou", "Shihang Tan", "Tao Han", "Guorui Lu", "Zhen Xu", "Junyuan Ding", "Ziteng Wang", "Zongwei Wu", "Han Han", "Yuliang Wu", "Jinze Chen", "Wei Zhai", "Yang Cao", "Zheng-jun Zha", "Nuwan Bandara", "Thivya Kandappu", "Archan Misra", "Xiaopeng Lin", "Hongxiang Huang", "Hongwei Ren", "Bojun Cheng", "Hoang M. Truong", "Vinh-Thuan Ly", "Huy G. Tran", "Thuan-Phat Nguyen", "Tram T. Doan"], "title": "Event-Based Eye Tracking. 2025 Event-based Vision Workshop", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This survey serves as a review for the 2025 Event-Based Eye Tracking\nChallenge organized as part of the 2025 CVPR event-based vision workshop. This\nchallenge focuses on the task of predicting the pupil center by processing\nevent camera recorded eye movement. We review and summarize the innovative\nmethods from teams rank the top in the challenge to advance future event-based\neye tracking research. In each method, accuracy, model size, and number of\noperations are reported. In this survey, we also discuss event-based eye\ntracking from the perspective of hardware design.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u5bf92025\u5e74\u57fa\u4e8e\u4e8b\u4ef6\u7684\u773c\u52a8\u8ffd\u8e2a\u6311\u6218\u8d5b\u7684\u7efc\u8ff0\uff0c\u603b\u7ed3\u4e86\u6392\u540d\u9760\u524d\u56e2\u961f\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u5e76\u4ece\u786c\u4ef6\u8bbe\u8ba1\u89d2\u5ea6\u8ba8\u8bba\u4e86\u57fa\u4e8e\u4e8b\u4ef6\u7684\u773c\u52a8\u8ffd\u8e2a\u3002", "motivation": "\u901a\u8fc7\u7efc\u8ff0\u6311\u6218\u8d5b\u4e2d\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u63a8\u52a8\u672a\u6765\u57fa\u4e8e\u4e8b\u4ef6\u7684\u773c\u52a8\u8ffd\u8e2a\u7814\u7a76\u3002", "method": "\u56de\u987e\u5e76\u603b\u7ed3\u4e86\u6311\u6218\u8d5b\u4e2d\u6392\u540d\u9760\u524d\u56e2\u961f\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u4e86\u51c6\u786e\u7387\u3001\u6a21\u578b\u5927\u5c0f\u548c\u8fd0\u7b97\u91cf\u7b49\u6307\u6807\u3002", "result": "\u5206\u6790\u4e86\u5404\u65b9\u6cd5\u7684\u6027\u80fd\u6307\u6807\uff0c\u5e76\u63a2\u8ba8\u4e86\u786c\u4ef6\u8bbe\u8ba1\u7684\u89c6\u89d2\u3002", "conclusion": "\u8fd9\u7bc7\u7efc\u8ff0\u4e3a\u57fa\u4e8e\u4e8b\u4ef6\u7684\u773c\u52a8\u8ffd\u8e2a\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u53c2\u8003\u548c\u542f\u53d1\u3002"}}
{"id": "2504.17954", "pdf": "https://arxiv.org/pdf/2504.17954", "abs": "https://arxiv.org/abs/2504.17954", "authors": ["Kaiyuan Tang", "Siyuan Yao", "Chaoli Wang"], "title": "iVR-GS: Inverse Volume Rendering for Explorable Visualization via Editable 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted by IEEE Transactions on Visualization and Computer Graphics\n  (TVCG)", "summary": "In volume visualization, users can interactively explore the\nthree-dimensional data by specifying color and opacity mappings in the transfer\nfunction (TF) or adjusting lighting parameters, facilitating meaningful\ninterpretation of the underlying structure. However, rendering large-scale\nvolumes demands powerful GPUs and high-speed memory access for real-time\nperformance. While existing novel view synthesis (NVS) methods offer faster\nrendering speeds with lower hardware requirements, the visible parts of a\nreconstructed scene are fixed and constrained by preset TF settings,\nsignificantly limiting user exploration. This paper introduces inverse volume\nrendering via Gaussian splatting (iVR-GS), an innovative NVS method that\nreduces the rendering cost while enabling scene editing for interactive volume\nexploration. Specifically, we compose multiple iVR-GS models associated with\nbasic TFs covering disjoint visible parts to make the entire volumetric scene\nvisible. Each basic model contains a collection of 3D editable Gaussians, where\neach Gaussian is a 3D spatial point that supports real-time scene rendering and\nediting. We demonstrate the superior reconstruction quality and composability\nof iVR-GS against other NVS solutions (Plenoxels, CCNeRF, and base 3DGS) on\nvarious volume datasets. The code is available at\nhttps://github.com/TouKaienn/iVR-GS.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u629b\u6d12\u7684\u9006\u4f53\u79ef\u6e32\u67d3\u65b9\u6cd5\uff08iVR-GS\uff09\uff0c\u5728\u964d\u4f4e\u6e32\u67d3\u6210\u672c\u7684\u540c\u65f6\u652f\u6301\u573a\u666f\u7f16\u8f91\uff0c\u5b9e\u73b0\u4e86\u4ea4\u4e92\u5f0f\u4f53\u79ef\u63a2\u7d22\u3002", "motivation": "\u73b0\u6709\u65b0\u89c6\u89d2\u5408\u6210\uff08NVS\uff09\u65b9\u6cd5\u867d\u7136\u6e32\u67d3\u901f\u5ea6\u5feb\u4e14\u786c\u4ef6\u8981\u6c42\u4f4e\uff0c\u4f46\u6e32\u67d3\u573a\u666f\u7684\u53ef\u89c1\u90e8\u5206\u53d7\u9884\u8bbe\u4f20\u8f93\u51fd\u6570\uff08TF\uff09\u9650\u5236\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7528\u6237\u4ea4\u4e92\u63a2\u7d22\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u591a\u4e2a\u8986\u76d6\u4e0d\u540c\u53ef\u89c1\u90e8\u5206\u7684\u57fa\u672cTF\u5173\u8054\u7684iVR-GS\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u5305\u542b\u53ef\u7f16\u8f91\u76843D\u9ad8\u65af\u5206\u5e03\u70b9\uff0c\u652f\u6301\u5b9e\u65f6\u6e32\u67d3\u4e0e\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0ciVR-GS\u5728\u591a\u79cd\u4f53\u79ef\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5176\u4ed6NVS\u65b9\u6cd5\uff08\u5982Plenoxels\u3001CCNeRF\u548c3DGS\uff09\uff0c\u4e14\u91cd\u5efa\u8d28\u91cf\u548c\u53ef\u7ec4\u5408\u6027\u66f4\u4f18\u3002", "conclusion": "iVR-GS\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u7f16\u8f91\u7684\u4f53\u79ef\u6e32\u67d3\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u63a2\u7d22\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18253", "pdf": "https://arxiv.org/pdf/2504.18253", "abs": "https://arxiv.org/abs/2504.18253", "authors": ["Amirhossein Zhalehmehrabi", "Daniele Meli", "Francesco Dal Santo", "Francesco Trotti", "Alessandro Farinelli"], "title": "Depth-Constrained ASV Navigation with Deep RL and Limited Sensing", "categories": ["cs.RO", "cs.AI"], "comment": "9 pages, 8 figures", "summary": "Autonomous Surface Vehicles (ASVs) play a crucial role in maritime\noperations, yet their navigation in shallow-water environments remains\nchallenging due to dynamic disturbances and depth constraints. Traditional\nnavigation strategies struggle with limited sensor information, making safe and\nefficient operation difficult. In this paper, we propose a reinforcement\nlearning (RL) framework for ASV navigation under depth constraints, where the\nvehicle must reach a target while avoiding unsafe areas with only a single\ndepth measurement per timestep from a downward-facing Single Beam Echosounder\n(SBES). To enhance environmental awareness, we integrate Gaussian Process (GP)\nregression into the RL framework, enabling the agent to progressively estimate\na bathymetric depth map from sparse sonar readings. This approach improves\ndecision-making by providing a richer representation of the environment.\nFurthermore, we demonstrate effective sim-to-real transfer, ensuring that\ntrained policies generalize well to real-world aquatic conditions. Experimental\nresults validate our method's capability to improve ASV navigation performance\nwhile maintaining safety in challenging shallow-water environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u9ad8\u65af\u8fc7\u7a0b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u4e3b\u6c34\u9762\u8f66\u8f86\uff08ASV\uff09\u5728\u6d45\u6c34\u73af\u5883\u4e2d\u7684\u5bfc\u822a\uff0c\u901a\u8fc7\u7a00\u758f\u58f0\u5450\u6570\u636e\u9010\u6b65\u4f30\u8ba1\u6c34\u6df1\u56fe\uff0c\u63d0\u5347\u5b89\u5168\u6027\u4e0e\u6548\u7387\u3002", "motivation": "\u6d45\u6c34\u73af\u5883\u4e2dASV\u5bfc\u822a\u56e0\u52a8\u6001\u5e72\u6270\u548c\u6c34\u6df1\u9650\u5236\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u4f20\u611f\u5668\u4fe1\u606f\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u96be\u4ee5\u5b89\u5168\u9ad8\u6548\u8fd0\u884c\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff0c\u901a\u8fc7\u5355\u6ce2\u675f\u56de\u58f0\u6d4b\u6df1\u4eea\uff08SBES\uff09\u7684\u7a00\u758f\u6570\u636e\u9010\u6b65\u6784\u5efa\u6c34\u6df1\u56fe\uff0c\u4f18\u5316\u5bfc\u822a\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8be5\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347ASV\u5728\u6d45\u6c34\u73af\u5883\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u9ad8\u6548\u8fc1\u79fb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6d45\u6c34\u73af\u5883\u4e0b\u7684ASV\u5bfc\u822a\u63d0\u4f9b\u4e86\u5b89\u5168\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.17959", "pdf": "https://arxiv.org/pdf/2504.17959", "abs": "https://arxiv.org/abs/2504.17959", "authors": ["Yinlong Dai", "Robert Ramirez Sanchez", "Ryan Jeronimus", "Shahabedin Sagheb", "Cara M. Nunez", "Heramb Nemlekar", "Dylan P. Losey"], "title": "CIVIL: Causal and Intuitive Visual Imitation Learning", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Today's robots learn new tasks by imitating human examples. However, this\nstandard approach to visual imitation learning is fundamentally limited: the\nrobot observes what the human does, but not why the human chooses those\nbehaviors. Without understanding the features that factor into the human's\ndecisions, robot learners often misinterpret the data and fail to perform the\ntask when the environment changes. We therefore propose a shift in perspective:\ninstead of asking human teachers just to show what actions the robot should\ntake, we also enable humans to indicate task-relevant features using markers\nand language prompts. Our proposed algorithm, CIVIL, leverages this augmented\ndata to filter the robot's visual observations and extract a feature\nrepresentation that causally informs human actions. CIVIL then applies these\ncausal features to train a transformer-based policy that emulates human\nbehaviors without being confused by visual distractors. Our simulations,\nreal-world experiments, and user study demonstrate that robots trained with\nCIVIL can learn from fewer human demonstrations and perform better than\nstate-of-the-art baselines, especially in previously unseen scenarios. See\nvideos at our project website: https://civil2025.github.io", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCIVIL\u7b97\u6cd5\uff0c\u901a\u8fc7\u4eba\u7c7b\u6807\u8bb0\u548c\u8bed\u8a00\u63d0\u793a\u6765\u589e\u5f3a\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u4efb\u52a1\u76f8\u5173\u7279\u5f81\uff0c\u4ece\u800c\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u65b0\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4ec5\u5173\u6ce8\u4eba\u7c7b\u7684\u52a8\u4f5c\uff0c\u5ffd\u89c6\u4e86\u884c\u4e3a\u80cc\u540e\u7684\u51b3\u7b56\u539f\u56e0\uff0c\u5bfc\u81f4\u673a\u5668\u4eba\u5728\u73af\u5883\u53d8\u5316\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u4eba\u7c7b\u63d0\u4f9b\u7684\u7279\u5f81\u6807\u8bb0\u548c\u8bed\u8a00\u63d0\u793a\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5bf9\u4efb\u52a1\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51faCIVIL\u7b97\u6cd5\uff0c\u5229\u7528\u4eba\u7c7b\u63d0\u4f9b\u7684\u989d\u5916\u6570\u636e\uff08\u6807\u8bb0\u548c\u8bed\u8a00\u63d0\u793a\uff09\u6765\u8fc7\u6ee4\u89c6\u89c9\u89c2\u5bdf\u5e76\u63d0\u53d6\u4e0e\u4eba\u7c7b\u884c\u4e3a\u56e0\u679c\u76f8\u5173\u7684\u7279\u5f81\u8868\u793a\uff0c\u968f\u540e\u8bad\u7ec3\u57fa\u4e8etransformer\u7684\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u91c7\u7528CIVIL\u8bad\u7ec3\u7684\u673a\u5668\u4eba\u80fd\u5728\u66f4\u5c11\u7684\u4eba\u7c7b\u6f14\u793a\u4e0b\u5b66\u4e60\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u63d0\u4f9b\u7684\u7279\u5f81\u4fe1\u606f\uff0cCIVIL\u80fd\u663e\u8457\u63d0\u5347\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u6216\u53d8\u5316\u7684\u73af\u5883\u4e2d\u3002"}}
{"id": "2504.17966", "pdf": "https://arxiv.org/pdf/2504.17966", "abs": "https://arxiv.org/abs/2504.17966", "authors": ["Kaiyuan Tan", "Peilun Li", "Jun Wang", "Thomas Beckers"], "title": "Plug-and-Play Physics-informed Learning using Uncertainty Quantified Port-Hamiltonian Models", "categories": ["cs.RO", "cs.LG"], "comment": "7 pages, 6 figures", "summary": "The ability to predict trajectories of surrounding agents and obstacles is a\ncrucial component in many robotic applications. Data-driven approaches are\ncommonly adopted for state prediction in scenarios where the underlying\ndynamics are unknown. However, the performance, reliability, and uncertainty of\ndata-driven predictors become compromised when encountering out-of-distribution\nobservations relative to the training data. In this paper, we introduce a\nPlug-and-Play Physics-Informed Machine Learning (PnP-PIML) framework to address\nthis challenge. Our method employs conformal prediction to identify outlier\ndynamics and, in that case, switches from a nominal predictor to a\nphysics-consistent model, namely distributed Port-Hamiltonian systems (dPHS).\nWe leverage Gaussian processes to model the energy function of the dPHS,\nenabling not only the learning of system dynamics but also the quantification\nof predictive uncertainty through its Bayesian nature. In this way, the\nproposed framework produces reliable physics-informed predictions even for the\nout-of-distribution scenarios.", "AI": {"tldr": "\u8be5\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u63d2\u4ef6\u5f0f\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff08PnP-PIML\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u4fdd\u5f62\u9884\u6d4b\u548c\u5206\u5e03\u5f0fPort-Hamiltonian\u7cfb\u7edf\uff08dPHS\uff09\uff0c\u5728\u9047\u5230\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u5916\u7684\u89c2\u6d4b\u65f6\uff0c\u80fd\u591f\u5207\u6362\u5230\u7269\u7406\u4e00\u81f4\u7684\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u9884\u6d4b\u7684\u53ef\u9760\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\uff0c\u9884\u6d4b\u5468\u56f4\u7269\u4f53\u548c\u969c\u788d\u7269\u7684\u8f68\u8ff9\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f53\u9047\u5230\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u5916\u7684\u573a\u666f\u65f6\uff0c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u4f1a\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86PnP-PIML\u6846\u67b6\uff0c\u901a\u8fc7\u4fdd\u5f62\u9884\u6d4b\u8bc6\u522b\u5f02\u5e38\u52a8\u6001\uff0c\u5e76\u5207\u6362\u5230\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u7684dPHS\u6a21\u578b\uff0c\u5b66\u4e60\u7cfb\u7edf\u52a8\u6001\u5e76\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u63d0\u4f9b\u53ef\u9760\u4e14\u7269\u7406\u4fe1\u606f\u4e30\u5bcc\u7684\u9884\u6d4b\u3002", "conclusion": "PnP-PIML\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u548c\u7269\u7406\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\u3002"}}
{"id": "2504.18283", "pdf": "https://arxiv.org/pdf/2504.18283", "abs": "https://arxiv.org/abs/2504.18283", "authors": ["Minjae Kang", "Martim Brand\u00e3o"], "title": "Seeing Soundscapes: Audio-Visual Generation and Separation from Soundscapes Using Audio-Visual Separator", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "comment": "Originally submitted to CVPR 2025 on 2024-11-15 with paper ID 15808", "summary": "Recent audio-visual generative models have made substantial progress in\ngenerating images from audio. However, existing approaches focus on generating\nimages from single-class audio and fail to generate images from mixed audio. To\naddress this, we propose an Audio-Visual Generation and Separation model\n(AV-GAS) for generating images from soundscapes (mixed audio containing\nmultiple classes). Our contribution is threefold: First, we propose a new\nchallenge in the audio-visual generation task, which is to generate an image\ngiven a multi-class audio input, and we propose a method that solves this task\nusing an audio-visual separator. Second, we introduce a new audio-visual\nseparation task, which involves generating separate images for each class\npresent in a mixed audio input. Lastly, we propose new evaluation metrics for\nthe audio-visual generation task: Class Representation Score (CRS) and a\nmodified R@K. Our model is trained and evaluated on the VGGSound dataset. We\nshow that our method outperforms the state-of-the-art, achieving 7% higher CRS\nand 4% higher R@2* in generating plausible images with mixed audio.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u97f3\u9891-\u89c6\u89c9\u751f\u6210\u4e0e\u5206\u79bb\u6a21\u578b\uff08AV-GAS\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u4ece\u6df7\u5408\u97f3\u9891\u751f\u6210\u56fe\u50cf\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u97f3\u9891-\u89c6\u89c9\u5206\u79bb\u5668\u5b9e\u73b0\u591a\u7c7b\u97f3\u9891\u8f93\u5165\u4e0b\u7684\u56fe\u50cf\u751f\u6210\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u97f3\u9891-\u89c6\u89c9\u751f\u6210\u6a21\u578b\u4ec5\u80fd\u5904\u7406\u5355\u7c7b\u97f3\u9891\u8f93\u5165\uff0c\u800c\u65e0\u6cd5\u5e94\u5bf9\u6df7\u5408\u97f3\u9891\uff08\u5305\u542b\u591a\u7c7b\u58f0\u97f3\uff09\u7684\u56fe\u50cf\u751f\u6210\u9700\u6c42\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u5904\u7406\u591a\u7c7b\u97f3\u9891\u8f93\u5165\u5e76\u751f\u6210\u5bf9\u5e94\u56fe\u50cf\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u97f3\u9891-\u89c6\u89c9\u5206\u79bb\u5668\uff08AV-GAS\uff09\u5b9e\u73b0\u591a\u7c7b\u97f3\u9891\u8f93\u5165\u7684\u56fe\u50cf\u751f\u6210\uff0c\u540c\u65f6\u652f\u6301\u751f\u6210\u6df7\u5408\u97f3\u9891\u4e2d\u5404\u4e2a\u72ec\u7acb\u58f0\u97f3\u5bf9\u5e94\u7684\u56fe\u50cf\u3002\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff1aClass Representation Score (CRS) \u548c\u4fee\u6539\u7248R@K\u3002", "result": "\u5728VGGSound\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u6a21\u578b\uff0cCRS\u63d0\u9ad87%\uff0cR@2*\u63d0\u9ad84%\uff0c\u751f\u6210\u7684\u56fe\u50cf\u66f4\u5177\u53ef\u4fe1\u5ea6\u3002", "conclusion": "AV-GAS\u6a21\u578b\u6210\u529f\u89e3\u51b3\u4e86\u591a\u7c7b\u97f3\u9891\u8f93\u5165\u7684\u56fe\u50cf\u751f\u6210\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u65b0\u4efb\u52a1\u548c\u8bc4\u4f30\u6307\u6807\u63a8\u52a8\u4e86\u97f3\u9891-\u89c6\u89c9\u751f\u6210\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.17999", "pdf": "https://arxiv.org/pdf/2504.17999", "abs": "https://arxiv.org/abs/2504.17999", "authors": ["Chang Xiao", "Brenda Yang"], "title": "Streaming, Fast and Slow: Cognitive Load-Aware Streaming for Efficient LLM Serving", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Generative conversational interfaces powered by large language models (LLMs)\ntypically stream output token-by-token at a rate determined by computational\nbudget, often neglecting actual human reading speeds and the cognitive load\nassociated with the content. This mismatch frequently leads to inefficient use\nof computational resources. For example, in cloud-based services, streaming\ncontent faster than users can read appears unnecessary, resulting in wasted\ncomputational resources and potential delays for other users, particularly\nduring peak usage periods. To address this issue, we propose an adaptive\nstreaming method that dynamically adjusts the pacing of LLM streaming output in\nreal-time based on inferred cognitive load. Our approach estimates the\ncognitive load associated with streaming content and strategically slows down\nthe stream during complex or information-rich segments, thereby freeing\ncomputational resources for other users. Our statistical analysis of\ncomputational savings, combined with crowdsourced user studies, provides\ninsights into the trade-offs between service efficiency and user satisfaction,\ndemonstrating that our method can significantly reduce computational\nconsumption up to 16.8\\%. This context-aware computational resource management\nstrategy presents a practical framework for enhancing system efficiency in\ncloud-based conversational AI interfaces without compromising user experience.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u6d41\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8f93\u51fa\u901f\u5ea6\u4ee5\u5339\u914d\u7528\u6237\u8ba4\u77e5\u8d1f\u8377\uff0c\u663e\u8457\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709LLM\u751f\u6210\u7684\u5bf9\u8bdd\u63a5\u53e3\u901a\u5e38\u4ee5\u56fa\u5b9a\u901f\u7387\u8f93\u51fa\uff0c\u5ffd\u7565\u4e86\u7528\u6237\u7684\u5b9e\u9645\u9605\u8bfb\u901f\u5ea6\u548c\u8ba4\u77e5\u8d1f\u8377\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u6d41\u5f0f\u65b9\u6cd5\uff0c\u5b9e\u65f6\u63a8\u65ad\u5185\u5bb9\u8ba4\u77e5\u8d1f\u8377\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u8f93\u51fa\u901f\u5ea6\uff0c\u590d\u6742\u5185\u5bb9\u653e\u6162\u4ee5\u91ca\u653e\u8d44\u6e90\u3002", "result": "\u7edf\u8ba1\u5206\u6790\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u51cf\u5c11\u9ad8\u8fbe16.8%\u7684\u8ba1\u7b97\u6d88\u8017\uff0c\u4e14\u4e0d\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e91\u5bf9\u8bddAI\u63d0\u4f9b\u4e86\u9ad8\u6548\u8d44\u6e90\u7ba1\u7406\u6846\u67b6\uff0c\u5e73\u8861\u4e86\u7cfb\u7edf\u6548\u7387\u4e0e\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2504.18286", "pdf": "https://arxiv.org/pdf/2504.18286", "abs": "https://arxiv.org/abs/2504.18286", "authors": ["Christian Pionzewski", "Rebecca Rademacher", "J\u00e9r\u00f4me Rutinowski", "Antonia Ponikarov", "Stephan Matzke", "Tim Chilla", "Pia Schreynemackers", "Alice Kirchheim"], "title": "Enhancing Long-Term Re-Identification Robustness Using Synthetic Data: A Comparative Analysis", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.10; I.4.9"], "comment": "Published in: 2024 International Conference on Machine Learning and\n  Applications (ICMLA), IEEE. 6 pages, 3 figures", "summary": "This contribution explores the impact of synthetic training data usage and\nthe prediction of material wear and aging in the context of re-identification.\nDifferent experimental setups and gallery set expanding strategies are tested,\nanalyzing their impact on performance over time for aging re-identification\nsubjects. Using a continuously updating gallery, we were able to increase our\nmean Rank-1 accuracy by 24%, as material aging was taken into account step by\nstep. In addition, using models trained with 10% artificial training data,\nRank-1 accuracy could be increased by up to 13%, in comparison to a model\ntrained on only real-world data, significantly boosting generalized performance\non hold-out data. Finally, this work introduces a novel, open-source\nre-identification dataset, pallet-block-2696. This dataset contains 2,696\nimages of Euro pallets, taken over a period of 4 months. During this time,\nnatural aging processes occurred and some of the pallets were damaged during\ntheir usage. These wear and tear processes significantly changed the appearance\nof the pallets, providing a dataset that can be used to generate synthetically\naged pallets or other wooden materials.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5408\u6210\u8bad\u7ec3\u6570\u636e\u5bf9\u6750\u6599\u78e8\u635f\u548c\u8001\u5316\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u4f7f\u7528\u6301\u7eed\u66f4\u65b0\u7684\u56fe\u5e93\u548c\u90e8\u5206\u5408\u6210\u6570\u636e\u53ef\u663e\u8457\u63d0\u5347\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u5f00\u6e90\u6570\u636e\u96c6\u3002", "motivation": "\u63a2\u7d22\u5408\u6210\u8bad\u7ec3\u6570\u636e\u5728\u6750\u6599\u8001\u5316\u518d\u8bc6\u522b\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u63d0\u5347\u7cfb\u7edf\u5bf9\u8001\u5316\u5bf9\u8c61\u7684\u8bc6\u522b\u6027\u80fd\u3002", "method": "\u6d4b\u8bd5\u4e86\u4e0d\u540c\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u548c\u56fe\u5e93\u6269\u5c55\u7b56\u7565\uff0c\u4f7f\u7528\u6301\u7eed\u66f4\u65b0\u7684\u56fe\u5e93\u548c10%\u7684\u4eba\u5de5\u5408\u6210\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u6301\u7eed\u66f4\u65b0\u7684\u56fe\u5e93\u4f7f\u5e73\u5747Rank-1\u51c6\u786e\u7387\u63d0\u9ad824%\uff1b\u4f7f\u752810%\u5408\u6210\u6570\u636e\u7684\u8bad\u7ec3\u6a21\u578b\u6bd4\u7eaf\u771f\u5b9e\u6570\u636e\u6a21\u578b\u63d0\u534713%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u548c\u52a8\u6001\u56fe\u5e93\u66f4\u65b0\u80fd\u663e\u8457\u63d0\u5347\u8001\u5316\u518d\u8bc6\u522b\u6027\u80fd\uff0c\u65b0\u6570\u636e\u96c6\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2504.18310", "pdf": "https://arxiv.org/pdf/2504.18310", "abs": "https://arxiv.org/abs/2504.18310", "authors": ["Prashant Garg", "Thiemo Fetzer"], "title": "Artificial Intelligence health advice accuracy varies across languages and contexts", "categories": ["econ.GN", "cs.AI", "cs.CY", "cs.HC", "cs.LG", "q-fin.EC"], "comment": "10 pages, 2 figures. All data, code and materials used is freely\n  available in the Zenodo (DOI: 10.5281/zenodo.15281282)", "summary": "Using basic health statements authorized by UK and EU registers and 9,100\njournalist-vetted public-health assertions on topics such as abortion, COVID-19\nand politics from sources ranging from peer-reviewed journals and government\nadvisories to social media and news across the political spectrum, we benchmark\nsix leading large language models from in 21 languages, finding that, despite\nhigh accuracy on English-centric textbook claims, performance falls in multiple\nnon-European languages and fluctuates by topic and source, highlighting the\nurgency of comprehensive multilingual, domain-aware validation before deploying\nAI in global health communication.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u82f1\u8bed\u6559\u79d1\u4e66\u5185\u5bb9\u4e0a\u51c6\u786e\u6027\u9ad8\uff0c\u4f46\u5728\u975e\u6b27\u6d32\u8bed\u8a00\u53ca\u4e0d\u540c\u4e3b\u9898\u548c\u6765\u6e90\u80cc\u666f\u4e0b\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u51f8\u663e\u4e86\u5168\u7403\u5065\u5eb7\u4f20\u64ad\u4e2d\u591a\u8bed\u8a00\u548c\u9886\u57df\u9a8c\u8bc1\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5168\u7403\u5065\u5eb7\u901a\u4fe1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u975e\u82f1\u8bed\u548c\u975e\u6b27\u6d32\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u4ee5\u786e\u4fddAI\u5728\u5168\u7403\u8303\u56f4\u5185\u7684\u53ef\u9760\u5e94\u7528\u3002", "method": "\u5229\u7528\u82f1\u56fd\u548c\u6b27\u76df\u5b98\u65b9\u5065\u5eb7\u58f0\u660e\u4ee5\u53ca9100\u6761\u8bb0\u8005\u5ba1\u6838\u7684\u5065\u5eb7\u65ad\u8a00\uff08\u6db5\u76d6\u5815\u80ce\u3001COVID-19\u7b49\u4e3b\u9898\u548c\u591a\u79cd\u6765\u6e90\uff09\uff0c\u6d4b\u8bd5\u4e86\u516d\u79cd\u9886\u5148\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u6a21\u578b\u5728\u82f1\u8bed\u5185\u5bb9\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u975e\u6b27\u6d32\u8bed\u8a00\u4e2d\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u4e14\u8868\u73b0\u56e0\u4e3b\u9898\u548c\u6765\u6e90\u4e0d\u540c\u800c\u6ce2\u52a8\u3002", "conclusion": "\u5728AI\u5e94\u7528\u4e8e\u5168\u7403\u5065\u5eb7\u901a\u4fe1\u524d\uff0c\u9700\u8fdb\u884c\u5168\u9762\u7684\u591a\u8bed\u8a00\u548c\u9886\u57df\u9a8c\u8bc1\uff0c\u4ee5\u786e\u4fdd\u5176\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2504.18015", "pdf": "https://arxiv.org/pdf/2504.18015", "abs": "https://arxiv.org/abs/2504.18015", "authors": ["Hanrui Wang", "Shuo Wang", "Chun-Shien Lu", "Isao Echizen"], "title": "Diffusion-Driven Universal Model Inversion Attack for Face Recognition", "categories": ["cs.CR", "cs.CV", "cs.LG"], "comment": null, "summary": "Facial recognition technology poses significant privacy risks, as it relies\non biometric data that is inherently sensitive and immutable if compromised. To\nmitigate these concerns, face recognition systems convert raw images into\nembeddings, traditionally considered privacy-preserving. However, model\ninversion attacks pose a significant privacy threat by reconstructing these\nprivate facial images, making them a crucial tool for evaluating the privacy\nrisks of face recognition systems. Existing methods usually require training\nindividual generators for each target model, a computationally expensive\nprocess. In this paper, we propose DiffUMI, a training-free diffusion-driven\nuniversal model inversion attack for face recognition systems. DiffUMI is the\nfirst approach to apply a diffusion model for unconditional image generation in\nmodel inversion. Unlike other methods, DiffUMI is universal, eliminating the\nneed for training target-specific generators. It operates within a fixed\nframework and pretrained diffusion model while seamlessly adapting to diverse\ntarget identities and models. DiffUMI breaches privacy-preserving face\nrecognition systems with state-of-the-art success, demonstrating that an\nunconditional diffusion model, coupled with optimized adversarial search,\nenables efficient and high-fidelity facial reconstruction. Additionally, we\nintroduce a novel application of out-of-domain detection (OODD), marking the\nfirst use of model inversion to distinguish non-face inputs from face inputs\nbased solely on embeddings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiffUMI\u7684\u65e0\u8bad\u7ec3\u6269\u6563\u9a71\u52a8\u901a\u7528\u6a21\u578b\u53cd\u8f6c\u653b\u51fb\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u9690\u79c1\u98ce\u9669\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u9488\u5bf9\u6bcf\u4e2a\u76ee\u6807\u6a21\u578b\u8bad\u7ec3\u751f\u6210\u5668\uff0c\u4e14\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4eba\u8138\u8bc6\u522b\u6280\u672f\u4f9d\u8d56\u654f\u611f\u4e14\u4e0d\u53ef\u53d8\u7684\u751f\u7269\u7279\u5f81\u6570\u636e\uff0c\u4f20\u7edf\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u901a\u8fc7\u5c06\u539f\u59cb\u56fe\u50cf\u8f6c\u6362\u4e3a\u5d4c\u5165\u5411\u91cf\uff0c\u4f46\u6a21\u578b\u53cd\u8f6c\u653b\u51fb\u4ecd\u53ef\u590d\u539f\u8fd9\u4e9b\u9690\u79c1\u56fe\u50cf\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u4e3a\u6bcf\u4e2a\u76ee\u6807\u6a21\u578b\u8bad\u7ec3\u751f\u6210\u5668\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51faDiffUMI\uff0c\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65e0\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u6280\u672f\uff0c\u65e0\u9700\u8bad\u7ec3\u7279\u5b9a\u751f\u6210\u5668\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u5bf9\u6297\u641c\u7d22\u5b9e\u73b0\u9ad8\u6548\u9ad8\u4fdd\u771f\u7684\u4eba\u8138\u91cd\u5efa\uff0c\u5e76\u9996\u6b21\u5229\u7528\u6a21\u578b\u53cd\u8f6c\u533a\u5206\u975e\u4eba\u8138\u8f93\u5165\u3002", "result": "DiffUMI\u5728\u9690\u79c1\u4fdd\u62a4\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u9a8c\u8bc1\u4e86\u65e0\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5728\u9ad8\u6548\u590d\u539f\u4eba\u8138\u56fe\u50cf\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "DiffUMI\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5728\u9690\u79c1\u4fdd\u62a4\u4e0a\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u540c\u65f6\u4e3a\u6a21\u578b\u53cd\u8f6c\u653b\u51fb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.18316", "pdf": "https://arxiv.org/pdf/2504.18316", "abs": "https://arxiv.org/abs/2504.18316", "authors": ["Yacine Majdoub", "Eya Ben Charrada", "Haifa Touati"], "title": "Towards Adaptive Software Agents for Debugging", "categories": ["cs.SE", "cs.AI"], "comment": "5 pages, 3 figures, FSE2025", "summary": "Using multiple agents was found to improve the debugging capabilities of\nLarge Language Models. However, increasing the number of LLM-agents has several\ndrawbacks such as increasing the running costs and rising the risk for the\nagents to lose focus. In this work, we propose an adaptive agentic design,\nwhere the number of agents and their roles are determined dynamically based on\nthe characteristics of the task to be achieved. In this design, the agents\nroles are not predefined, but are generated after analyzing the problem to be\nsolved. Our initial evaluation shows that, with the adaptive design, the number\nof agents that are generated depends on the complexity of the buggy code. In\nfact, for simple code with mere syntax issues, the problem was usually fixed\nusing one agent only. However, for more complex problems, we noticed the\ncreation of a higher number of agents. Regarding the effectiveness of the fix,\nwe noticed an average improvement of 11% compared to the one-shot prompting.\nGiven these promising results, we outline future research directions to improve\nour design for adaptive software agents that can autonomously plan and conduct\ntheir software goals.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1\uff0c\u52a8\u6001\u8c03\u6574\u667a\u80fd\u4f53\u6570\u91cf\u548c\u89d2\u8272\u4ee5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8c03\u8bd5\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u5bf9\u590d\u6742\u95ee\u9898\u4fee\u590d\u6548\u679c\u63d0\u534711%\u3002", "motivation": "\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u867d\u80fd\u63d0\u5347\u8c03\u8bd5\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u6210\u672c\u9ad8\u548c\u667a\u80fd\u4f53\u6613\u5931\u7126\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u9002\u5e94\u673a\u5236\u6765\u4f18\u5316\u667a\u80fd\u4f53\u4f7f\u7528\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u751f\u6210\u667a\u80fd\u4f53\u6570\u91cf\u548c\u89d2\u8272\u7684\u65b9\u6cd5\uff0c\u6839\u636e\u4efb\u52a1\u7279\u5f81\u81ea\u52a8\u8c03\u6574\uff0c\u65e0\u9700\u9884\u8bbe\u89d2\u8272\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u81ea\u9002\u5e94\u8bbe\u8ba1\u80fd\u6839\u636e\u4ee3\u7801\u590d\u6742\u5ea6\u751f\u6210\u4e0d\u540c\u6570\u91cf\u7684\u667a\u80fd\u4f53\uff0c\u7b80\u5355\u95ee\u9898\u4ec5\u9700\u4e00\u4e2a\u667a\u80fd\u4f53\uff0c\u590d\u6742\u95ee\u9898\u5219\u751f\u6210\u66f4\u591a\uff0c\u4fee\u590d\u6548\u679c\u5e73\u5747\u63d0\u534711%\u3002", "conclusion": "\u81ea\u9002\u5e94\u8bbe\u8ba1\u5728\u8c03\u8bd5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u672a\u6765\u5c06\u63a2\u7d22\u8fdb\u4e00\u6b65\u63d0\u5347\u667a\u80fd\u4f53\u81ea\u4e3b\u89c4\u5212\u548c\u6267\u884c\u8f6f\u4ef6\u76ee\u6807\u7684\u80fd\u529b\u3002"}}
{"id": "2504.18017", "pdf": "https://arxiv.org/pdf/2504.18017", "abs": "https://arxiv.org/abs/2504.18017", "authors": ["Sourav Chatterjee", "Timothy Sudijono"], "title": "Non-identifiability distinguishes Neural Networks among Parametric Models", "categories": ["math.ST", "cs.LG", "stat.ML", "stat.TH"], "comment": "16 pages. Comments welcome", "summary": "One of the enduring problems surrounding neural networks is to identify the\nfactors that differentiate them from traditional statistical models. We prove a\npair of results which distinguish feedforward neural networks among parametric\nmodels at the population level, for regression tasks. Firstly, we prove that\nfor any pair of random variables $(X,Y)$, neural networks always learn a\nnontrivial relationship between $X$ and $Y$, if one exists. Secondly, we prove\nthat for reasonable smooth parametric models, under local and global\nidentifiability conditions, there exists a nontrivial $(X,Y)$ pair for which\nthe parametric model learns the constant predictor $\\mathbb{E}[Y]$. Together,\nour results suggest that a lack of identifiability distinguishes neural\nnetworks among the class of smooth parametric models.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u4e86\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u4e0e\u4f20\u7edf\u7684\u53c2\u6570\u6a21\u578b\u6709\u672c\u8d28\u533a\u522b\uff1a\u795e\u7ecf\u7f51\u7edc\u603b\u80fd\u5b66\u4e60\u5230\u53d8\u91cf\u95f4\u7684\u975e\u5e73\u51e1\u5173\u7cfb\uff0c\u800c\u67d0\u4e9b\u53c2\u6570\u6a21\u578b\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u53ea\u80fd\u5b66\u4e60\u5230\u5e38\u6570\u9884\u6d4b\u5668\u3002", "motivation": "\u63a2\u8ba8\u795e\u7ecf\u7f51\u7edc\u4e0e\u4f20\u7edf\u53c2\u6570\u6a21\u578b\u7684\u672c\u8d28\u5dee\u5f02\uff0c\u5c24\u5176\u662f\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\uff0c\u6bd4\u8f83\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u548c\u4f20\u7edf\u53c2\u6570\u6a21\u578b\u5728\u53d8\u91cf\u5173\u7cfb\u5b66\u4e60\u548c\u9884\u6d4b\u80fd\u529b\u4e0a\u7684\u5dee\u5f02\u3002", "result": "\u795e\u7ecf\u7f51\u7edc\u603b\u80fd\u5b66\u4e60\u53d8\u91cf\u95f4\u7684\u975e\u5e73\u51e1\u5173\u7cfb\uff0c\u800c\u67d0\u4e9b\u53c2\u6570\u6a21\u578b\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u65e0\u6cd5\u505a\u5230\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u56e0\u5176\u5728\u53ef\u8bc6\u522b\u6027\u4e0a\u7684\u72ec\u7279\u6027\u800c\u533a\u522b\u4e8e\u4f20\u7edf\u5e73\u6ed1\u53c2\u6570\u6a21\u578b\u3002"}}
{"id": "2504.18348", "pdf": "https://arxiv.org/pdf/2504.18348", "abs": "https://arxiv.org/abs/2504.18348", "authors": ["Fengchun Liu. Tong Zhang", "Chunying Zhang"], "title": "TSCL:Multi-party loss Balancing scheme for deep learning Image steganography based on Curriculum learning", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "For deep learning-based image steganography frameworks, in order to ensure\nthe invisibility and recoverability of the information embedding, the loss\nfunction usually contains several losses such as embedding loss, recovery loss\nand steganalysis loss. In previous research works, fixed loss weights are\nusually chosen for training optimization, and this setting is not linked to the\nimportance of the steganography task itself and the training process. In this\npaper, we propose a Two-stage Curriculum Learning loss scheduler (TSCL) for\nbalancing multinomial losses in deep learning image steganography algorithms.\nTSCL consists of two phases: a priori curriculum control and loss dynamics\ncontrol. The first phase firstly focuses the model on learning the information\nembedding of the original image by controlling the loss weights in the\nmulti-party adversarial training; secondly, it makes the model shift its\nlearning focus to improving the decoding accuracy; and finally, it makes the\nmodel learn to generate a steganographic image that is resistant to\nsteganalysis. In the second stage, the learning speed of each training task is\nevaluated by calculating the loss drop of the before and after iteration rounds\nto balance the learning of each task. Experimental results on three large\npublic datasets, ALASKA2, VOC2012 and ImageNet, show that the proposed TSCL\nstrategy improves the quality of steganography, decoding accuracy and security.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u635f\u5931\u8c03\u5ea6\u5668\uff08TSCL\uff09\uff0c\u7528\u4e8e\u5e73\u8861\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u9690\u5199\u7b97\u6cd5\u4e2d\u7684\u591a\u4efb\u52a1\u635f\u5931\uff0c\u901a\u8fc7\u5148\u9a8c\u8bfe\u7a0b\u63a7\u5236\u548c\u52a8\u6001\u635f\u5931\u63a7\u5236\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u63d0\u5347\u4e86\u9690\u5199\u8d28\u91cf\u3001\u89e3\u7801\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u56fe\u50cf\u9690\u5199\u6846\u67b6\u901a\u5e38\u4f7f\u7528\u56fa\u5b9a\u635f\u5931\u6743\u91cd\uff0c\u672a\u8003\u8651\u4efb\u52a1\u91cd\u8981\u6027\u548c\u8bad\u7ec3\u8fc7\u7a0b\u52a8\u6001\u53d8\u5316\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u6765\u5e73\u8861\u591a\u4efb\u52a1\u635f\u5931\u3002", "method": "TSCL\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u5148\u9a8c\u8bfe\u7a0b\u63a7\u5236\uff08\u5206\u6b65\u5b66\u4e60\u5d4c\u5165\u3001\u89e3\u7801\u548c\u6297\u5206\u6790\u80fd\u529b\uff09\u548c\u52a8\u6001\u635f\u5931\u63a7\u5236\uff08\u901a\u8fc7\u635f\u5931\u4e0b\u964d\u901f\u5ea6\u8bc4\u4f30\u4efb\u52a1\u5b66\u4e60\u8fdb\u5ea6\uff0c\u52a8\u6001\u8c03\u6574\u6743\u91cd\uff09\u3002", "result": "\u5728ALASKA2\u3001VOC2012\u548cImageNet\u6570\u636e\u96c6\u4e0a\uff0cTSCL\u663e\u8457\u63d0\u5347\u4e86\u9690\u5199\u56fe\u50cf\u8d28\u91cf\u3001\u89e3\u7801\u51c6\u786e\u6027\u548c\u6297\u5206\u6790\u5b89\u5168\u6027\u3002", "conclusion": "TSCL\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u591a\u4efb\u52a1\u635f\u5931\u6743\u91cd\uff0c\u89e3\u51b3\u4e86\u56fa\u5b9a\u6743\u91cd\u8bad\u7ec3\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6df1\u5ea6\u56fe\u50cf\u9690\u5199\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4f18\u5316\u7b56\u7565\u3002"}}
{"id": "2504.18103", "pdf": "https://arxiv.org/pdf/2504.18103", "abs": "https://arxiv.org/abs/2504.18103", "authors": ["Natansh Mathur", "Brian Coyle", "Nishant Jain", "Snehal Raj", "Akshat Tandon", "Jasper Simon Krauser", "Rainer Stoessel"], "title": "Bayesian Quantum Orthogonal Neural Networks for Anomaly Detection", "categories": ["quant-ph", "cs.LG"], "comment": "14 pages, 9 figures", "summary": "Identification of defects or anomalies in 3D objects is a crucial task to\nensure correct functionality. In this work, we combine Bayesian learning with\nrecent developments in quantum and quantum-inspired machine learning,\nspecifically orthogonal neural networks, to tackle this anomaly detection\nproblem for an industrially relevant use case. Bayesian learning enables\nuncertainty quantification of predictions, while orthogonality in weight\nmatrices enables smooth training. We develop orthogonal (quantum) versions of\n3D convolutional neural networks and show that these models can successfully\ndetect anomalies in 3D objects. To test the feasibility of incorporating\nquantum computers into a quantum-enhanced anomaly detection pipeline, we\nperform hardware experiments with our models on IBM's 127-qubit Brisbane\ndevice, testing the effect of noise and limited measurement shots.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7ed3\u5408\u8d1d\u53f6\u65af\u5b66\u4e60\u548c\u91cf\u5b50\u542f\u53d1\u7684\u6b63\u4ea4\u795e\u7ecf\u7f51\u7edc\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e3D\u7269\u4f53\u5f02\u5e38\u68c0\u6d4b\u7684\u91cf\u5b50\u589e\u5f3a\u65b9\u6cd5\uff0c\u5e76\u5728IBM\u91cf\u5b50\u8bbe\u5907\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "3D\u7269\u4f53\u7684\u7f3a\u9677\u6216\u5f02\u5e38\u68c0\u6d4b\u5bf9\u786e\u4fdd\u529f\u80fd\u6b63\u786e\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u8d1d\u53f6\u65af\u5b66\u4e60\u548c\u91cf\u5b50\u6216\u91cf\u5b50\u542f\u53d1\u65b9\u6cd5\uff08\u5982\u6b63\u4ea4\u795e\u7ecf\u7f51\u7edc\uff09\u89e3\u51b3\u8fd9\u4e00\u5de5\u4e1a\u76f8\u5173\u95ee\u9898\uff0c\u5e76\u6d4b\u8bd5\u91cf\u5b50\u8ba1\u7b97\u673a\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u6b63\u4ea4\uff08\u91cf\u5b50\uff09\u7248\u672c\u76843D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u5b66\u4e60\u4ee5\u91cf\u5316\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u5728IBM\u7684127\u91cf\u5b50\u4f4dBrisbane\u8bbe\u5907\u4e0a\u8fdb\u884c\u4e86\u786c\u4ef6\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u566a\u58f0\u548c\u6709\u9650\u6d4b\u91cf\u6b21\u6570\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6b63\u4ea4\u91cf\u5b50\u7248\u672c\u76843D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u6210\u529f\u68c0\u6d4b3D\u7269\u4f53\u4e2d\u7684\u5f02\u5e38\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u91cf\u5b50\u589e\u5f3a\u5f02\u5e38\u68c0\u6d4b\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u91cf\u5b50\u8ba1\u7b97\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2504.18361", "pdf": "https://arxiv.org/pdf/2504.18361", "abs": "https://arxiv.org/abs/2504.18361", "authors": ["Haozhen Yan", "Yan Hong", "Jiahui Zhan", "Yikun Ji", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Jianfu Zhang"], "title": "COCO-Inpaint: A Benchmark for Image Inpainting Detection and Manipulation Localization", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Recent advancements in image manipulation have achieved unprecedented\nprogress in generating photorealistic content, but also simultaneously\neliminating barriers to arbitrary manipulation and editing, raising concerns\nabout multimedia authenticity and cybersecurity. However, existing Image\nManipulation Detection and Localization (IMDL) methodologies predominantly\nfocus on splicing or copy-move forgeries, lacking dedicated benchmarks for\ninpainting-based manipulations. To bridge this gap, we present COCOInpaint, a\ncomprehensive benchmark specifically designed for inpainting detection, with\nthree key contributions: 1) High-quality inpainting samples generated by six\nstate-of-the-art inpainting models, 2) Diverse generation scenarios enabled by\nfour mask generation strategies with optional text guidance, and 3) Large-scale\ncoverage with 258,266 inpainted images with rich semantic diversity. Our\nbenchmark is constructed to emphasize intrinsic inconsistencies between\ninpainted and authentic regions, rather than superficial semantic artifacts\nsuch as object shapes. We establish a rigorous evaluation protocol using three\nstandard metrics to assess existing IMDL approaches. The dataset will be made\npublicly available to facilitate future research in this area.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u68c0\u6d4b\u4fee\u590d\u56fe\u50cf\u7684\u57fa\u51c6\u6570\u636e\u96c6COCOInpaint\uff0c\u586b\u8865\u4e86\u73b0\u6709\u56fe\u50cf\u64cd\u7eb5\u68c0\u6d4b\u65b9\u6cd5\u5728\u4fee\u590d\u56fe\u50cf\u9886\u57df\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u64cd\u7eb5\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u805a\u7126\u4e8e\u62fc\u63a5\u6216\u590d\u5236-\u79fb\u52a8\u4f2a\u9020\uff0c\u7f3a\u4e4f\u9488\u5bf9\u4fee\u590d\u7c7b\u64cd\u7eb5\u7684\u4e13\u7528\u57fa\u51c6\uff0c\u9700\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u6784\u5efaCOCOInpaint\u6570\u636e\u96c6\uff0c\u5305\u542b\u7531\u516d\u79cd\u5148\u8fdb\u4fee\u590d\u6a21\u578b\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u4fee\u590d\u6837\u672c\uff0c\u56db\u79cd\u63a9\u6a21\u751f\u6210\u7b56\u7565\uff0c\u53ca25\u4e07+\u8bed\u4e49\u591a\u6837\u7684\u4fee\u590d\u56fe\u50cf\u3002", "result": "\u5f3a\u8c03\u4e86\u4fee\u590d\u533a\u57df\u4e0e\u771f\u5b9e\u533a\u57df\u7684\u5185\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u800c\u975e\u8868\u9762\u8bed\u4e49\u4f2a\u5f71\uff0c\u5e76\u5236\u5b9a\u4e25\u683c\u8bc4\u4f30\u534f\u8bae\u4f7f\u7528\u4e09\u79cd\u6807\u51c6\u6307\u6807\u6d4b\u8bd5\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u4fee\u590d\u68c0\u6d4b\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\uff0c\u4e3a\u591a\u5a92\u4f53\u771f\u5b9e\u6027\u548c\u7f51\u7edc\u5b89\u5168\u63d0\u4f9b\u65b0\u5de5\u5177\u3002"}}
{"id": "2504.18126", "pdf": "https://arxiv.org/pdf/2504.18126", "abs": "https://arxiv.org/abs/2504.18126", "authors": ["Miranda C. N. Cheng", "Niki Stratikopoulou"], "title": "Lecture Notes on Normalizing Flows for Lattice Quantum Field Theories", "categories": ["hep-lat", "cs.LG", "hep-th"], "comment": "70 pages", "summary": "Numerical simulations of quantum field theories on lattices serve as a\nfundamental tool for studying the non-perturbative regime of the theories,\nwhere analytic tools often fall short. Challenges arise when one takes the\ncontinuum limit or as the system approaches a critical point, especially in the\npresence of non-trivial topological structures in the theory. Rapid recent\nadvances in machine learning provide a promising avenue for progress in this\narea. These lecture notes aim to give a brief account of lattice field\ntheories, normalizing flows, and how the latter can be applied to study the\nformer. The notes are based on the lectures given by the first author in\nvarious recent research schools.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\uff08\u7279\u522b\u662f\u5f52\u4e00\u5316\u6d41\uff09\u5728\u89e3\u51b3\u91cf\u5b50\u573a\u8bba\u4e2d\u975e\u5fae\u6270\u95ee\u9898\u548c\u62d3\u6251\u7ed3\u6784\u6311\u6218\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u91cf\u5b50\u573a\u8bba\u7684\u975e\u5fae\u6270\u533a\u57df\u548c\u4e34\u754c\u70b9\u9644\u8fd1\u7684\u6311\u6218\u96be\u4ee5\u7528\u4f20\u7edf\u89e3\u6790\u65b9\u6cd5\u89e3\u51b3\uff0c\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u9014\u5f84\u3002", "method": "\u7ed3\u5408\u5f52\u4e00\u5316\u6d41\uff08normalizing flows\uff09\u4e0e\u6676\u683c\u573a\u8bba\uff0c\u89e3\u51b3\u8fde\u7eed\u6781\u9650\u548c\u4e34\u754c\u70b9\u7684\u6a21\u62df\u95ee\u9898\u3002", "result": "\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u4f18\u5316\u91cf\u5b50\u573a\u8bba\u6a21\u62df\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u5f52\u4e00\u5316\u6d41\u4e3a\u7814\u7a76\u91cf\u5b50\u573a\u8bba\u7684\u590d\u6742\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u62d3\u6251\u7ed3\u6784\u548c\u975e\u5fae\u6270\u6548\u5e94\u65f6\u3002"}}
{"id": "2504.18380", "pdf": "https://arxiv.org/pdf/2504.18380", "abs": "https://arxiv.org/abs/2504.18380", "authors": ["Steven H\u00e4sler", "Philipp Ackermann"], "title": "Spatial Reasoner: A 3D Inference Pipeline for XR Applications", "categories": ["cs.SE", "cs.AI", "cs.GR", "cs.HC", "spatial computing, extended reality, knowledge representation,\n  spatial reasoning"], "comment": "11 pages, preprint of ICVARS 2025 paper", "summary": "Modern extended reality XR systems provide rich analysis of image data and\nfusion of sensor input and demand AR/VR applications that can reason about 3D\nscenes in a semantic manner. We present a spatial reasoning framework that\nbridges geometric facts with symbolic predicates and relations to handle key\ntasks such as determining how 3D objects are arranged among each other ('on',\n'behind', 'near', etc.). Its foundation relies on oriented 3D bounding box\nrepresentations, enhanced by a comprehensive set of spatial predicates, ranging\nfrom topology and connectivity to directionality and orientation, expressed in\na formalism related to natural language. The derived predicates form a spatial\nknowledge graph and, in combination with a pipeline-based inference model,\nenable spatial queries and dynamic rule evaluation. Implementations for client-\nand server-side processing demonstrate the framework's capability to\nefficiently translate geometric data into actionable knowledge, ensuring\nscalable and technology-independent spatial reasoning in complex 3D\nenvironments. The Spatial Reasoner framework is fostering the creation of\nspatial ontologies, and seamlessly integrates with and therefore enriches\nmachine learning, natural language processing, and rule systems in XR\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc73D\u8fb9\u754c\u6846\u548c\u7a7a\u95f4\u8c13\u8bcd\u6784\u5efa\u7a7a\u95f4\u77e5\u8bc6\u56fe\u8c31\uff0c\u652f\u6301\u8bed\u4e49\u53163D\u573a\u666f\u5206\u6790\uff0c\u9002\u7528\u4e8eXR\u5e94\u7528\u3002", "motivation": "\u73b0\u4ee3XR\u7cfb\u7edf\u9700\u8981\u80fd\u591f\u5bf93D\u573a\u666f\u8fdb\u884c\u8bed\u4e49\u5316\u5206\u6790\uff0c\u4f46\u73b0\u6709\u6280\u672f\u7f3a\u4e4f\u9ad8\u6548\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e\u5b9a\u54113D\u8fb9\u754c\u6846\u8868\u793a\uff0c\u7ed3\u5408\u7a7a\u95f4\u8c13\u8bcd\uff08\u5982\u62d3\u6251\u3001\u65b9\u5411\u6027\u7b49\uff09\u6784\u5efa\u7a7a\u95f4\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u91c7\u7528\u6d41\u6c34\u7ebf\u63a8\u7406\u6a21\u578b\u652f\u6301\u52a8\u6001\u89c4\u5219\u8bc4\u4f30\u3002", "result": "\u6846\u67b6\u5b9e\u73b0\u4e86\u4ece\u51e0\u4f55\u6570\u636e\u5230\u53ef\u64cd\u4f5c\u77e5\u8bc6\u7684\u9ad8\u6548\u8f6c\u6362\uff0c\u652f\u6301\u590d\u6742\u7684\u7a7a\u95f4\u67e5\u8be2\uff0c\u5e76\u5c55\u793a\u4e86\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u7aef\u7684\u9ad8\u6548\u5904\u7406\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aXR\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6280\u672f\u65e0\u5173\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u7a7a\u95f4\u672c\u4f53\u8bba\u7684\u6784\u5efa\uff0c\u5e76\u589e\u5f3a\u4e86\u673a\u5668\u5b66\u4e60\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u89c4\u5219\u7cfb\u7edf\u7684\u96c6\u6210\u3002"}}
{"id": "2504.18147", "pdf": "https://arxiv.org/pdf/2504.18147", "abs": "https://arxiv.org/abs/2504.18147", "authors": ["Rob Romijnders", "Stefanos Laskaridis", "Ali Shahin Shamsabadi", "Hamed Haddadi"], "title": "NoEsis: Differentially Private Knowledge Transfer in Modular LLM Adaptation", "categories": ["cs.CR", "cs.LG"], "comment": "ICLR 2025 MCDC workshop", "summary": "Large Language Models (LLM) are typically trained on vast amounts of data\nfrom various sources. Even when designed modularly (e.g., Mixture-of-Experts),\nLLMs can leak privacy on their sources. Conversely, training such models in\nisolation arguably prohibits generalization. To this end, we propose a\nframework, NoEsis, which builds upon the desired properties of modularity,\nprivacy, and knowledge transfer. NoEsis integrates differential privacy with a\nhybrid two-staged parameter-efficient fine-tuning that combines domain-specific\nlow-rank adapters, acting as experts, with common prompt tokens, acting as a\nknowledge-sharing backbone. Results from our evaluation on CodeXGLUE showcase\nthat NoEsis can achieve provable privacy guarantees with tangible knowledge\ntransfer across domains, and empirically show protection against Membership\nInference Attacks. Finally, on code completion tasks, NoEsis bridges at least\n77% of the accuracy gap between the non-shared and the non-private baseline.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86NoEsis\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u5728\u4fdd\u8bc1\u9690\u79c1\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u8de8\u9886\u57df\u77e5\u8bc6\u8f6c\u79fb\uff0c\u5e76\u5728CodeXGLUE\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6a21\u5757\u5316\u8bad\u7ec3\u4e2d\u53ef\u80fd\u6cc4\u9732\u6e90\u6570\u636e\u9690\u79c1\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u5b64\u7acb\u8bad\u7ec3\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u7684\u7f3a\u70b9\u3002", "method": "\u65b9\u6cd5\u4e3aNoEsis\u6846\u67b6\uff0c\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u548c\u6df7\u5408\u4e24\u9636\u6bb5\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u4f7f\u7528\u57df\u7279\u5b9a\u4f4e\u79e9\u9002\u914d\u5668\uff08\u4f5c\u4e3a\u4e13\u5bb6\uff09\u548c\u5171\u4eab\u63d0\u793a\u4ee4\u724c\uff08\u4f5c\u4e3a\u77e5\u8bc6\u5171\u4eab\u4e3b\u5e72\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u5728CodeXGLUE\u4e0a\u663e\u793a\uff0cNoEsis\u80fd\u591f\u5b9e\u73b0\u53ef\u8bc1\u660e\u7684\u9690\u79c1\u4fdd\u8bc1\u548c\u8de8\u9886\u57df\u7684\u77e5\u8bc6\u8f6c\u79fb\uff0c\u5e76\u6709\u6548\u62b5\u6297\u6210\u5458\u63a8\u7406\u653b\u51fb\uff0c\u4ee3\u7801\u8865\u5168\u4efb\u52a1\u4e2d\u586b\u8865\u4e8677%\u7684\u51c6\u786e\u7387\u5dee\u8ddd\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660eNoEsis\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u77e5\u8bc6\u5171\u4eab\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3a\u6a21\u5757\u5316LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18383", "pdf": "https://arxiv.org/pdf/2504.18383", "abs": "https://arxiv.org/abs/2504.18383", "authors": ["Qidong Liu", "Xiangyu Zhao", "Yejing Wang", "Zijian Zhang", "Howard Zhong", "Chong Chen", "Xiang Li", "Wei Huang", "Feng Tian"], "title": "Bridge the Domains: Large Language Models Enhanced Cross-domain Sequential Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": "accepted by SIGIR'25", "summary": "Cross-domain Sequential Recommendation (CDSR) aims to extract the preference\nfrom the user's historical interactions across various domains. Despite some\nprogress in CDSR, two problems set the barrier for further advancements, i.e.,\noverlap dilemma and transition complexity. The former means existing CDSR\nmethods severely rely on users who own interactions on all domains to learn\ncross-domain item relationships, compromising the practicability. The latter\nrefers to the difficulties in learning the complex transition patterns from the\nmixed behavior sequences. With powerful representation and reasoning abilities,\nLarge Language Models (LLMs) are promising to address these two problems by\nbridging the items and capturing the user's preferences from a semantic view.\nTherefore, we propose an LLMs Enhanced Cross-domain Sequential Recommendation\nmodel (LLM4CDSR). To obtain the semantic item relationships, we first propose\nan LLM-based unified representation module to represent items. Then, a\ntrainable adapter with contrastive regularization is designed to adapt the CDSR\ntask. Besides, a hierarchical LLMs profiling module is designed to summarize\nuser cross-domain preferences. Finally, these two modules are integrated into\nthe proposed tri-thread framework to derive recommendations. We have conducted\nextensive experiments on three public cross-domain datasets, validating the\neffectiveness of LLM4CDSR. We have released the code online.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLLM4CDSR\u6a21\u578b\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u8de8\u57df\u5e8f\u5217\u63a8\u8350\u4e2d\u7684\u91cd\u53e0\u56f0\u5883\u548c\u8f6c\u79fb\u590d\u6742\u6027\uff0c\u901a\u8fc7\u8bed\u4e49\u8868\u793a\u548c\u5c42\u6b21\u5316\u7528\u6237\u504f\u597d\u5efa\u6a21\u63d0\u5347\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u8de8\u57df\u5e8f\u5217\u63a8\u8350\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u5168\u9886\u57df\u4ea4\u4e92\u7528\u6237\u4e14\u96be\u4ee5\u5b66\u4e60\u590d\u6742\u8f6c\u79fb\u6a21\u5f0f\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5927\u8868\u793a\u548c\u63a8\u7406\u80fd\u529b\u4e3a\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "1. \u57fa\u4e8eLLM\u7684\u7edf\u4e00\u7269\u54c1\u8bed\u4e49\u8868\u793a\u6a21\u5757\uff1b2. \u53ef\u8bad\u7ec3\u9002\u914d\u5668\u4e0e\u5bf9\u6bd4\u6b63\u5219\u5316\uff1b3. \u5c42\u6b21\u5316LLM\u7528\u6237\u504f\u597d\u5efa\u6a21\uff1b4. \u4e09\u7ebf\u7a0b\u6846\u67b6\u6574\u5408\u4e0a\u8ff0\u6a21\u5757\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u8de8\u57df\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86LLM4CDSR\u7684\u6709\u6548\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "LLM4CDSR\u901a\u8fc7\u8bed\u4e49\u5173\u8054\u548c\u7528\u6237\u504f\u597d\u5206\u5c42\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u63a8\u8350\u7684\u6027\u80fd\u548c\u89e3\u8026\u80fd\u529b\u3002"}}
{"id": "2504.18400", "pdf": "https://arxiv.org/pdf/2504.18400", "abs": "https://arxiv.org/abs/2504.18400", "authors": ["Yui Lo", "Yuqian Chen", "Dongnan Liu", "Leo Zekelman", "Jarrett Rushmore", "Yogesh Rathi", "Nikos Makris", "Alexandra J. Golby", "Fan Zhang", "Weidong Cai", "Lauren J. O'Donnell"], "title": "A Multimodal Deep Learning Approach for White Matter Shape Prediction in Diffusion MRI Tractography", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "21 pages, 3 figures, 6 tables", "summary": "Shape measures have emerged as promising descriptors of white matter\ntractography, offering complementary insights into anatomical variability and\nassociations with cognitive and clinical phenotypes. However, conventional\nmethods for computing shape measures are computationally expensive and\ntime-consuming for large-scale datasets due to reliance on voxel-based\nrepresentations. We propose Tract2Shape, a novel multimodal deep learning\nframework that leverages geometric (point cloud) and scalar (tabular) features\nto predict ten white matter tractography shape measures. To enhance model\nefficiency, we utilize a dimensionality reduction algorithm for the model to\npredict five primary shape components. The model is trained and evaluated on\ntwo independently acquired datasets, the HCP-YA dataset, and the PPMI dataset.\nWe evaluate the performance of Tract2Shape by training and testing it on the\nHCP-YA dataset and comparing the results with state-of-the-art models. To\nfurther assess its robustness and generalization ability, we also test\nTract2Shape on the unseen PPMI dataset. Tract2Shape outperforms SOTA deep\nlearning models across all ten shape measures, achieving the highest average\nPearson's r and the lowest nMSE on the HCP-YA dataset. The ablation study shows\nthat both multimodal input and PCA contribute to performance gains. On the\nunseen testing PPMI dataset, Tract2Shape maintains a high Pearson's r and low\nnMSE, demonstrating strong generalizability in cross-dataset evaluation.\nTract2Shape enables fast, accurate, and generalizable prediction of white\nmatter shape measures from tractography data, supporting scalable analysis\nacross datasets. This framework lays a promising foundation for future\nlarge-scale white matter shape analysis.", "AI": {"tldr": "Tract2Shape\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u767d\u8d28\u7ea4\u7ef4\u675f\u8ffd\u8e2a\u6570\u636e\u4e2d\u9ad8\u6548\u9884\u6d4b\u5f62\u72b6\u6d4b\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e76\u5c55\u793a\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u767d\u8d28\u5f62\u72b6\u6d4b\u91cf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "method": "\u91c7\u7528\u51e0\u4f55\uff08\u70b9\u4e91\uff09\u548c\u6807\u91cf\uff08\u8868\u683c\uff09\u7279\u5f81\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u964d\u7ef4\u7b97\u6cd5\u9884\u6d4b\u5f62\u72b6\u6d4b\u91cf\u3002", "result": "\u5728HCP-YA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff08\u6700\u9ad8Pearson's r\u548c\u6700\u4f4enMSE\uff09\uff0c\u5728PPMI\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Tract2Shape\u4e3a\u5927\u89c4\u6a21\u767d\u8d28\u5f62\u72b6\u5206\u6790\u63d0\u4f9b\u4e86\u5feb\u901f\u3001\u51c6\u786e\u4e14\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.18179", "pdf": "https://arxiv.org/pdf/2504.18179", "abs": "https://arxiv.org/abs/2504.18179", "authors": ["Lovro Sindicic", "Ivica Kopriva"], "title": "Label-independent hyperparameter-free self-supervised single-view deep subspace clustering", "categories": ["cs.CV", "cs.LG", "68", "I.5.3; I.4.6; I.4.10"], "comment": "35 pages; 1 figure; 10 Tables", "summary": "Deep subspace clustering (DSC) algorithms face several challenges that hinder\ntheir widespread adoption across variois application domains. First, clustering\nquality is typically assessed using only the encoder's output layer,\ndisregarding valuable information present in the intermediate layers. Second,\nmost DSC approaches treat representation learning and subspace clustering as\nindependent tasks, limiting their effectiveness. Third, they assume the\navailability of a held-out dataset for hyperparameter tuning, which is often\nimpractical in real-world scenarios. Fourth, learning termination is commonly\nbased on clustering error monitoring, requiring external labels. Finally, their\nperformance often depends on post-processing techniques that rely on labeled\ndata. To address this limitations, we introduce a novel single-view DSC\napproach that: (i) minimizes a layer-wise self expression loss using a joint\nrepresentation matrix; (ii) optimizes a subspace-structured norm to enhance\nclustering quality; (iii) employs a multi-stage sequential learning framework,\nconsisting of pre-training and fine-tuning, enabling the use of multiple\nregularization terms without hyperparameter tuning; (iv) incorporates a\nrelative error-based self-stopping mechanism to terminate training without\nlabels; and (v) retains a fixed number of leading coefficients in the learned\nrepresentation matrix based on prior knowledge. We evaluate the proposed method\non six datasets representing faces, digits, and objects. The results show that\nour method outperforms most linear SC algorithms with careffulyl tuned\nhyperparameters while maintaining competitive performance with the best\nperforming linear appoaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u89c6\u56fe\u6df1\u5ea6\u5b50\u7a7a\u95f4\u805a\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u8868\u793a\u77e9\u9635\u548c\u591a\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b50\u7a7a\u95f4\u805a\u7c7b\u65b9\u6cd5\u5b58\u5728\u591a\u4e2a\u95ee\u9898\uff0c\u5982\u4ec5\u4f7f\u7528\u7f16\u7801\u5668\u8f93\u51fa\u5c42\u8bc4\u4f30\u805a\u7c7b\u8d28\u91cf\u3001\u8868\u793a\u5b66\u4e60\u4e0e\u5b50\u7a7a\u95f4\u805a\u7c7b\u5206\u79bb\u3001\u4f9d\u8d56\u5916\u90e8\u6807\u7b7e\u7b49\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5355\u89c6\u56feDSC\u65b9\u6cd5\uff0c\u5305\u62ec\u5c42\u95f4\u81ea\u8868\u8fbe\u635f\u5931\u3001\u5b50\u7a7a\u95f4\u7ed3\u6784\u8303\u6570\u4f18\u5316\u3001\u591a\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\u3001\u57fa\u4e8e\u76f8\u5bf9\u8bef\u5dee\u7684\u81ea\u505c\u6b62\u673a\u5236\uff0c\u4ee5\u53ca\u56fa\u5b9a\u524d\u5bfc\u7cfb\u6570\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\uff08\u4eba\u8138\u3001\u6570\u5b57\u3001\u7269\u4f53\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u591a\u6570\u9700\u8c03\u53c2\u7684\u7ebf\u6027SC\u7b97\u6cd5\uff0c\u5e76\u4e0e\u6700\u4f73\u7ebf\u6027\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u4f18\u5316\u548c\u81ea\u9002\u5e94\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u5b50\u7a7a\u95f4\u805a\u7c7b\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.18404", "pdf": "https://arxiv.org/pdf/2504.18404", "abs": "https://arxiv.org/abs/2504.18404", "authors": ["Liang Yu"], "title": "Paradigm shift on Coding Productivity Using GenAI", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Generative AI (GenAI) applications are transforming software engineering by\nenabling automated code co-creation. However, empirical evidence on GenAI's\nproductivity effects in industrial settings remains limited. This paper\ninvestigates the adoption of GenAI coding assistants (e.g., Codeium, Amazon Q)\nwithin telecommunications and FinTech domains. Through surveys and interviews\nwith industrial domain-experts, we identify primary productivity-influencing\nfactors, including task complexity, coding skills, domain knowledge, and GenAI\nintegration. Our findings indicate that GenAI tools enhance productivity in\nroutine coding tasks (e.g., refactoring and Javadoc generation) but face\nchallenges in complex, domain-specific activities due to limited\ncontext-awareness of codebases and insufficient support for customized design\nrules. We highlight new paradigms for coding transfer, emphasizing iterative\nprompt refinement, immersive development environment, and automated code\nevaluation as essential for effective GenAI usage.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u751f\u6210\u5f0fAI\u5728\u7535\u4fe1\u548c\u91d1\u878d\u79d1\u6280\u9886\u57df\u5bf9\u7f16\u7801\u52a9\u624b\uff08\u5982Codeium\u3001Amazon Q\uff09\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5176\u5728\u5e38\u89c4\u4efb\u52a1\u4e2d\u63d0\u5347\u6548\u7387\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002", "motivation": "\u867d\u7136\u751f\u6210\u5f0fAI\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u5176\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u5b9e\u9645\u6548\u7387\u7684\u5b9e\u8bc1\u8bc1\u636e\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u95ee\u5377\u8c03\u67e5\u548c\u4e13\u5bb6\u8bbf\u8c08\uff0c\u5206\u6790\u4efb\u52a1\u590d\u6742\u6027\u3001\u7f16\u7801\u6280\u80fd\u3001\u9886\u57df\u77e5\u8bc6\u548cAI\u96c6\u6210\u5bf9\u751f\u4ea7\u529b\u7684\u5f71\u54cd\u3002", "result": "\u751f\u6210\u5f0fAI\u80fd\u63d0\u9ad8\u5e38\u89c4\u7f16\u7801\u4efb\u52a1\uff08\u5982\u91cd\u6784\u548c\u6587\u6863\u751f\u6210\uff09\u7684\u6548\u7387\uff0c\u4f46\u5728\u590d\u6742\u6216\u9886\u57df\u7279\u5b9a\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u5b9a\u5236\u8bbe\u8ba1\u89c4\u5219\u652f\u6301\u3002", "conclusion": "\u63d0\u51fa\u65b0\u7f16\u7801\u8303\u5f0f\uff0c\u5f3a\u8c03\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u3001\u6c89\u6d78\u5f0f\u5f00\u53d1\u73af\u5883\u548c\u81ea\u52a8\u5316\u4ee3\u7801\u8bc4\u4f30\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u4f7f\u7528\u751f\u6210\u5f0fAI\u3002"}}
{"id": "2504.18419", "pdf": "https://arxiv.org/pdf/2504.18419", "abs": "https://arxiv.org/abs/2504.18419", "authors": ["Carlo Sgaravatti", "Roberto Basla", "Riccardo Pieroni", "Matteo Corno", "Sergio M. Savaresi", "Luca Magri", "Giacomo Boracchi"], "title": "A Multimodal Hybrid Late-Cascade Fusion Network for Enhanced 3D Object Detection", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "We present a new way to detect 3D objects from multimodal inputs, leveraging\nboth LiDAR and RGB cameras in a hybrid late-cascade scheme, that combines an\nRGB detection network and a 3D LiDAR detector. We exploit late fusion\nprinciples to reduce LiDAR False Positives, matching LiDAR detections with RGB\nones by projecting the LiDAR bounding boxes on the image. We rely on cascade\nfusion principles to recover LiDAR False Negatives leveraging epipolar\nconstraints and frustums generated by RGB detections of separate views. Our\nsolution can be plugged on top of any underlying single-modal detectors,\nenabling a flexible training process that can take advantage of pre-trained\nLiDAR and RGB detectors, or train the two branches separately. We evaluate our\nresults on the KITTI object detection benchmark, showing significant\nperformance improvements, especially for the detection of Pedestrians and\nCyclists.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLiDAR\u548cRGB\u76f8\u673a\u7684\u65b0\u578b3D\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u91c7\u7528\u6df7\u5408\u7ea7\u8054\u65b9\u6848\u51cf\u5c11\u8bef\u62a5\u548c\u6f0f\u62a5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u548c\u9a91\u884c\u8005\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u878d\u5408LiDAR\u548cRGB\u591a\u6a21\u6001\u6570\u636e\uff0c\u89e3\u51b3\u5355\u4e00\u6a21\u6001\u68c0\u6d4b\u4e2d\u8bef\u62a5\u548c\u6f0f\u62a5\u7684\u95ee\u9898\uff0c\u63d0\u53473D\u76ee\u6807\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u6df7\u5408\u7ea7\u8054\u65b9\u6848\uff0c\u5305\u62ecRGB\u68c0\u6d4b\u7f51\u7edc\u548c3D LiDAR\u63a2\u6d4b\u5668\uff0c\u901a\u8fc7\u6295\u5f71\u548c\u7ea7\u8054\u878d\u5408\u51cf\u5c11LiDAR\u8bef\u62a5\u5e76\u586b\u8865\u6f0f\u62a5\uff0c\u652f\u6301\u7075\u6d3b\u8bad\u7ec3\u3002", "result": "\u5728KITTI\u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u63d0\u5347\u4e86\u884c\u4eba\u548c\u9a91\u884c\u8005\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u901a\u8fc7\u7ea7\u8054\u878d\u5408\u7b56\u7565\u663e\u8457\u63d0\u53473D\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.18184", "pdf": "https://arxiv.org/pdf/2504.18184", "abs": "https://arxiv.org/abs/2504.18184", "authors": ["Jia-Qi Yang", "Lei Shi"], "title": "Learning Operators by Regularized Stochastic Gradient Descent with Operator-valued Kernels", "categories": ["stat.ML", "cs.LG", "math.FA", "math.ST", "stat.TH"], "comment": "56 pages, 2 figures", "summary": "This paper investigates regularized stochastic gradient descent (SGD)\nalgorithms for estimating nonlinear operators from a Polish space to a\nseparable Hilbert space. We assume that the regression operator lies in a\nvector-valued reproducing kernel Hilbert space induced by an operator-valued\nkernel. Two significant settings are considered: an online setting with\npolynomially decaying step sizes and regularization parameters, and a\nfinite-horizon setting with constant step sizes and regularization parameters.\nWe introduce regularity conditions on the structure and smoothness of the\ntarget operator and the input random variables. Under these conditions, we\nprovide a dimension-free convergence analysis for the prediction and estimation\nerrors, deriving both expectation and high-probability error bounds. Our\nanalysis demonstrates that these convergence rates are nearly optimal.\nFurthermore, we present a new technique for deriving bounds with high\nprobability for general SGD schemes, which also ensures almost-sure\nconvergence. Finally, we discuss potential extensions to more general\noperator-valued kernels and the encoder-decoder framework.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u6b63\u5219\u5316\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u4ece\u6ce2\u5170\u7a7a\u95f4\u5230\u53ef\u5206\u79bb\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7684\u975e\u7ebf\u6027\u7b97\u5b50\uff0c\u5e76\u63d0\u4f9b\u4e86\u8fd1\u4e4e\u6700\u4f18\u7684\u6536\u655b\u901f\u5ea6\u5206\u6790\u3002", "motivation": "\u63a2\u7d22\u5728\u5411\u91cf\u503c\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u4f30\u8ba1\u975e\u7ebf\u6027\u7b97\u5b50\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5728\u7ebf\u548c\u6709\u9650\u8303\u56f4\u8bbe\u7f6e\u4e0b\u7684\u6536\u655b\u6027\u3002", "method": "\u91c7\u7528\u6b63\u5219\u5316SGD\u7b97\u6cd5\uff0c\u7ed3\u5408\u7b97\u5b50\u503c\u6838\uff0c\u5206\u6790\u5728\u7ebf\uff08\u591a\u9879\u5f0f\u8870\u51cf\u6b65\u957f\u548c\u6b63\u5219\u5316\u53c2\u6570\uff09\u548c\u6709\u9650\u8303\u56f4\uff08\u6052\u5b9a\u53c2\u6570\uff09\u4e24\u79cd\u8bbe\u7f6e\u7684\u6536\u655b\u6027\u3002", "result": "\u5728\u7279\u5b9a\u6b63\u5219\u6761\u4ef6\u4e0b\uff0c\u8bc1\u660e\u4e86\u9884\u6d4b\u548c\u4f30\u8ba1\u8bef\u5dee\u7684\u7ef4\u5ea6\u65e0\u5173\u6536\u655b\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u9ad8\u6982\u7387\u8bef\u5dee\u754c\u548c\u51e0\u4e4e\u80af\u5b9a\u7684\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "\u6240\u63d0\u7b97\u6cd5\u5728\u6536\u655b\u901f\u5ea6\u548c\u9002\u7528\u8303\u56f4\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u5e76\u5177\u6709\u5411\u66f4\u5e7f\u6cdb\u7684\u7b97\u5b50\u503c\u6838\u548c\u7f16\u89e3\u7801\u6846\u67b6\u6269\u5c55\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.18423", "pdf": "https://arxiv.org/pdf/2504.18423", "abs": "https://arxiv.org/abs/2504.18423", "authors": ["Rajesh Yarra"], "title": "LLMpatronous: Harnessing the Power of LLMs For Vulnerability Detection", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Despite the transformative impact of Artificial Intelligence (AI) across\nvarious sectors, cyber security continues to rely on traditional static and\ndynamic analysis tools, hampered by high false positive rates and superficial\ncode comprehension. While generative AI offers promising automation\ncapabilities for software development, leveraging Large Language Models (LLMs)\nfor vulnerability detection presents unique challenges. This paper explores the\npotential and limitations of LLMs in identifying vulnerabilities, acknowledging\ninherent weaknesses such as hallucinations, limited context length, and\nknowledge cut-offs. Previous attempts employing machine learning models for\nvulnerability detection have proven ineffective due to limited real-world\napplicability, feature engineering challenges, lack of contextual\nunderstanding, and the complexities of training models to keep pace with the\nevolving threat landscape. Therefore, we propose a robust AI-driven approach\nfocused on mitigating these limitations and ensuring the quality and\nreliability of LLM based vulnerability detection. Through innovative\nmethodologies combining Retrieval-Augmented Generation (RAG) and\nMixtureof-Agents (MoA), this research seeks to leverage the strengths of LLMs\nwhile addressing their weaknesses, ultimately paving the way for dependable and\nefficient AI-powered solutions in securing the ever-evolving software\nlandscape.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LLMs\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u7ed3\u5408RAG\u548cMoA\u7684\u521b\u65b0\u65b9\u6cd5\u4ee5\u63d0\u5347AI\u9a71\u52a8\u7684\u6f0f\u6d1e\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u7f51\u7edc\u5b89\u5168\u5de5\u5177\u5b58\u5728\u9ad8\u8bef\u62a5\u7387\u548c\u4ee3\u7801\u7406\u89e3\u6d45\u663e\u7684\u95ee\u9898\uff0c\u800c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5148\u8fdb\u7684AI\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u7ed3\u5408RAG\u4e0eMoA\u7684\u65b9\u6cd5\uff0c\u4ee5\u5f25\u8865LLMs\u7684\u5e7b\u89c9\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u548c\u77e5\u8bc6\u622a\u65ad\u7b49\u7f3a\u9677\u3002", "result": "\u8be5\u65b9\u6cd5\u65e8\u5728\u63d0\u5347\u6f0f\u6d1e\u68c0\u6d4b\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\uff0c\u4e3aAI\u9a71\u52a8\u7684\u5b89\u5168\u89e3\u51b3\u65b9\u6848\u94fa\u5e73\u9053\u8def\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\uff0cLLMs\u53ef\u4ee5\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u53d1\u6325\u66f4\u5927\u4f5c\u7528\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u5e94\u5bf9\u4e0d\u65ad\u53d8\u5316\u7684\u5a01\u80c1\u73af\u5883\u3002"}}
{"id": "2504.18203", "pdf": "https://arxiv.org/pdf/2504.18203", "abs": "https://arxiv.org/abs/2504.18203", "authors": ["Raul David Dominguez Sanchez", "Xavier Diaz Ortiz", "Xingcheng Zhou", "Max Peter Ronecker", "Michael Karner", "Daniel Watzenig", "Alois Knoll"], "title": "LiDAR-Guided Monocular 3D Object Detection for Long-Range Railway Monitoring", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for the Data-Driven Learning for Intelligent Vehicle\n  Applications Workshop at the 36th IEEE Intelligent Vehicles Symposium (IV)\n  2025", "summary": "Railway systems, particularly in Germany, require high levels of automation\nto address legacy infrastructure challenges and increase train traffic safely.\nA key component of automation is robust long-range perception, essential for\nearly hazard detection, such as obstacles at level crossings or pedestrians on\ntracks. Unlike automotive systems with braking distances of ~70 meters, trains\nrequire perception ranges exceeding 1 km. This paper presents an\ndeep-learning-based approach for long-range 3D object detection tailored for\nautonomous trains. The method relies solely on monocular images, inspired by\nthe Faraway-Frustum approach, and incorporates LiDAR data during training to\nimprove depth estimation. The proposed pipeline consists of four key modules:\n(1) a modified YOLOv9 for 2.5D object detection, (2) a depth estimation\nnetwork, and (3-4) dedicated short- and long-range 3D detection heads.\nEvaluations on the OSDaR23 dataset demonstrate the effectiveness of the\napproach in detecting objects up to 250 meters. Results highlight its potential\nfor railway automation and outline areas for future improvement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5355\u76ee\u56fe\u50cf\u957f\u8ddd\u79bb3D\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4e13\u4e3a\u94c1\u8def\u81ea\u52a8\u5316\u8bbe\u8ba1\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408LiDAR\u6570\u636e\u6539\u8fdb\u6df1\u5ea6\u4f30\u8ba1\uff0c\u5728OSDaR23\u6570\u636e\u96c6\u4e0a\u6709\u6548\u68c0\u6d4b250\u7c73\u5185\u7684\u7269\u4f53\u3002", "motivation": "\u5fb7\u56fd\u94c1\u8def\u7cfb\u7edf\u9700\u8981\u9ad8\u5ea6\u81ea\u52a8\u5316\u4ee5\u5e94\u5bf9\u8001\u5316\u57fa\u7840\u8bbe\u65bd\u548c\u589e\u52a0\u5217\u8f66\u6d41\u91cf\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u957f\u8ddd\u79bb\u611f\u77e5\u4ee5\u63d0\u524d\u68c0\u6d4b\u5371\u9669\u7269\u4ef6\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6539\u8fdb\u7684YOLOv9\u7528\u4e8e2.5D\u7269\u4f53\u68c0\u6d4b\u3001\u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc\uff0c\u4ee5\u53ca\u77ed\u3001\u957f\u8ddd\u79bb3D\u68c0\u6d4b\u5934\u3002LiDAR\u6570\u636e\u7528\u4e8e\u8bad\u7ec3\u4ee5\u63d0\u5347\u6df1\u5ea6\u4f30\u8ba1\u3002", "result": "\u5728OSDaR23\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u80fd\u68c0\u6d4b250\u7c73\u5185\u7684\u7269\u4f53\uff0c\u9a8c\u8bc1\u4e86\u5176\u94c1\u8def\u81ea\u52a8\u5316\u5e94\u7528\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u957f\u8ddd\u79bb3D\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u94c1\u8def\u81ea\u52a8\u5316\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2504.18212", "pdf": "https://arxiv.org/pdf/2504.18212", "abs": "https://arxiv.org/abs/2504.18212", "authors": ["Nguyen Vu Khai Tam", "Cao Huyen My", "Vo Nguyen Le Duy"], "title": "Post-Transfer Learning Statistical Inference in High-Dimensional Regression", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Transfer learning (TL) for high-dimensional regression (HDR) is an important\nproblem in machine learning, particularly when dealing with limited sample size\nin the target task. However, there currently lacks a method to quantify the\nstatistical significance of the relationship between features and the response\nin TL-HDR settings. In this paper, we introduce a novel statistical inference\nframework for assessing the reliability of feature selection in TL-HDR, called\nPTL-SI (Post-TL Statistical Inference). The core contribution of PTL-SI is its\nability to provide valid $p$-values to features selected in TL-HDR, thereby\nrigorously controlling the false positive rate (FPR) at desired significance\nlevel $\\alpha$ (e.g., 0.05). Furthermore, we enhance statistical power by\nincorporating a strategic divide-and-conquer approach into our framework. We\ndemonstrate the validity and effectiveness of the proposed PTL-SI through\nextensive experiments on both synthetic and real-world high-dimensional\ndatasets, confirming its theoretical properties and utility in testing the\nreliability of feature selection in TL scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPTL-SI\u7684\u65b0\u7edf\u8ba1\u63a8\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u9ad8\u7ef4\u56de\u5f52\u8fc1\u79fb\u5b66\u4e60\uff08TL-HDR\uff09\u4e2d\u7279\u5f81\u9009\u62e9\u7684\u53ef\u9760\u6027\uff0c\u5e76\u80fd\u591f\u63d0\u4f9b\u6709\u6548\u7684p\u503c\u4ee5\u63a7\u5236\u5047\u9633\u6027\u7387\uff0c\u540c\u65f6\u901a\u8fc7\u5206\u6cbb\u7b56\u7565\u589e\u5f3a\u7edf\u8ba1\u529f\u6548\u3002", "motivation": "\u5f53\u524d\u5728\u9ad8\u7ef4\u56de\u5f52\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7f3a\u4e4f\u91cf\u5316\u7279\u5f81\u4e0e\u54cd\u5e94\u5173\u7cfb\u7edf\u8ba1\u663e\u8457\u6027\u7684\u65b9\u6cd5\uff0c\u5bfc\u81f4\u65e0\u6cd5\u53ef\u9760\u8bc4\u4f30\u7279\u5f81\u9009\u62e9\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86PTL-SI\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u6cbb\u7b56\u7565\u8ba1\u7b97\u7279\u5f81p\u503c\uff0c\u4e25\u683c\u63a7\u5236\u5047\u9633\u6027\u7387\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PTL-SI\u5728\u5408\u6210\u548c\u771f\u5b9e\u9ad8\u7ef4\u6570\u636e\u96c6\u4e2d\u7684\u6709\u6548\u6027\u4e0e\u7406\u8bba\u7279\u6027\u3002", "conclusion": "PTL-SI\u4e3aTL-HDR\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u7279\u5f81\u9009\u62e9\u7edf\u8ba1\u63a8\u65ad\u5de5\u5177\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7a7a\u767d\u3002"}}
{"id": "2504.18447", "pdf": "https://arxiv.org/pdf/2504.18447", "abs": "https://arxiv.org/abs/2504.18447", "authors": ["Ryo Yamaki", "Shintaro Shiba", "Guillermo Gallego", "Yoshimitsu Aoki"], "title": "Iterative Event-based Motion Segmentation by Variational Contrast Maximization", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "11 pages, 9 figures, 3 tables, CVPR Workshop 2025", "summary": "Event cameras provide rich signals that are suitable for motion estimation\nsince they respond to changes in the scene. As any visual changes in the scene\nproduce event data, it is paramount to classify the data into different motions\n(i.e., motion segmentation), which is useful for various tasks such as object\ndetection and visual servoing. We propose an iterative motion segmentation\nmethod, by classifying events into background (e.g., dominant motion\nhypothesis) and foreground (independent motion residuals), thus extending the\nContrast Maximization framework. Experimental results demonstrate that the\nproposed method successfully classifies event clusters both for public and\nself-recorded datasets, producing sharp, motion-compensated edge-like images.\nThe proposed method achieves state-of-the-art accuracy on moving object\ndetection benchmarks with an improvement of over 30%, and demonstrates its\npossibility of applying to more complex and noisy real-world scenes. We hope\nthis work broadens the sensitivity of Contrast Maximization with respect to\nboth motion parameters and input events, thus contributing to theoretical\nadvancements in event-based motion segmentation estimation.\nhttps://github.com/aoki-media-lab/event_based_segmentation_vcmax", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u8fed\u4ee3\u8fd0\u52a8\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4e8b\u4ef6\u5206\u7c7b\u4e3a\u80cc\u666f\u548c\u524d\u666f\uff0c\u6269\u5c55\u4e86\u5bf9\u6bd4\u5ea6\u6700\u5927\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u7269\u4f53\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u80fd\u591f\u6355\u6349\u573a\u666f\u53d8\u5316\uff0c\u4f46\u9700\u8981\u5c06\u6570\u636e\u5206\u7c7b\u4e3a\u4e0d\u540c\u8fd0\u52a8\u4ee5\u5b9e\u73b0\u8fd0\u52a8\u5206\u5272\uff0c\u8fd9\u5bf9\u4e8e\u7269\u4f53\u68c0\u6d4b\u548c\u89c6\u89c9\u4f3a\u670d\u7b49\u4efb\u52a1\u975e\u5e38\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fed\u4ee3\u8fd0\u52a8\u5206\u5272\u65b9\u6cd5\uff0c\u5c06\u4e8b\u4ef6\u5206\u4e3a\u80cc\u666f\uff08\u4e3b\u5bfc\u8fd0\u52a8\u5047\u8bbe\uff09\u548c\u524d\u666f\uff08\u72ec\u7acb\u8fd0\u52a8\u6b8b\u5dee\uff09\uff0c\u6269\u5c55\u4e86\u5bf9\u6bd4\u5ea6\u6700\u5927\u5316\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u5206\u7c7b\u4e86\u516c\u5171\u548c\u81ea\u5f55\u6570\u636e\u96c6\u4e2d\u7684\u4e8b\u4ef6\u7c07\uff0c\u751f\u6210\u4e86\u6e05\u6670\u7684\u8fd0\u52a8\u8865\u507f\u8fb9\u7f18\u56fe\u50cf\uff0c\u5728\u8fd0\u52a8\u7269\u4f53\u68c0\u6d4b\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc730%\u7684\u7cbe\u5ea6\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u5bf9\u6bd4\u5ea6\u6700\u5927\u5316\u5bf9\u8fd0\u52a8\u53c2\u6570\u548c\u8f93\u5165\u4e8b\u4ef6\u7684\u654f\u611f\u6027\uff0c\u4e3a\u4e8b\u4ef6\u9a71\u52a8\u8fd0\u52a8\u5206\u5272\u7684\u7406\u8bba\u8fdb\u5c55\u4f5c\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2504.18241", "pdf": "https://arxiv.org/pdf/2504.18241", "abs": "https://arxiv.org/abs/2504.18241", "authors": ["Surajit Majumder", "Paritosh Ranjan", "Prodip Roy", "Bhuban Padhan"], "title": "Switch-Based Multi-Part Neural Network", "categories": ["cs.NE", "cs.LG"], "comment": "12 pages, 4 figures", "summary": "This paper introduces decentralized and modular neural network framework\ndesigned to enhance the scalability, interpretability, and performance of\nartificial intelligence (AI) systems. At the heart of this framework is a\ndynamic switch mechanism that governs the selective activation and training of\nindividual neurons based on input characteristics, allowing neurons to\nspecialize in distinct segments of the data domain. This approach enables\nneurons to learn from disjoint subsets of data, mimicking biological brain\nfunction by promoting task specialization and improving the interpretability of\nneural network behavior. Furthermore, the paper explores the application of\nfederated learning and decentralized training for real-world AI deployments,\nparticularly in edge computing and distributed environments. By simulating\nlocalized training on non-overlapping data subsets, we demonstrate how modular\nnetworks can be efficiently trained and evaluated. The proposed framework also\naddresses scalability, enabling AI systems to handle large datasets and\ndistributed processing while preserving model transparency and\ninterpretability. Finally, we discuss the potential of this approach in\nadvancing the design of scalable, privacy-preserving, and efficient AI systems\nfor diverse applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u548c\u6a21\u5757\u5316\u7684\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5207\u6362\u673a\u5236\u9009\u62e9\u6027\u6fc0\u6d3b\u548c\u8bad\u7ec3\u795e\u7ecf\u5143\uff0c\u63d0\u9ad8AI\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u548c\u5206\u5e03\u5f0f\u73af\u5883\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u5728\u53ef\u6269\u5c55\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u9002\u5e94\u5206\u5e03\u5f0f\u73af\u5883\u7684\u9700\u6c42\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u4e14\u53bb\u4e2d\u5fc3\u5316\u7684\u6846\u67b6\u3002", "method": "\u91c7\u7528\u52a8\u6001\u5207\u6362\u673a\u5236\u9009\u62e9\u6027\u6fc0\u6d3b\u548c\u8bad\u7ec3\u795e\u7ecf\u5143\uff0c\u4f7f\u795e\u7ecf\u5143\u4e13\u6ce8\u4e8e\u6570\u636e\u7684\u7279\u5b9a\u90e8\u5206\u3002\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u548c\u5206\u6563\u8bad\u7ec3\uff0c\u6a21\u62df\u975e\u91cd\u53e0\u6570\u636e\u5b50\u96c6\u7684\u672c\u5730\u8bad\u7ec3\u3002", "result": "\u5c55\u793a\u4e86\u6a21\u5757\u5316\u7f51\u7edc\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bbe\u8ba1\u53ef\u6269\u5c55\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u9ad8\u6548\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2504.18471", "pdf": "https://arxiv.org/pdf/2504.18471", "abs": "https://arxiv.org/abs/2504.18471", "authors": ["Alejandro Murillo-Gonzalez", "Lantao Liu"], "title": "Action Flow Matching for Continual Robot Learning", "categories": ["cs.RO", "cs.AI"], "comment": "Robotics: Science and Systems 2025", "summary": "Continual learning in robotics seeks systems that can constantly adapt to\nchanging environments and tasks, mirroring human adaptability. A key challenge\nis refining dynamics models, essential for planning and control, while\naddressing issues such as safe adaptation, catastrophic forgetting, outlier\nmanagement, data efficiency, and balancing exploration with exploitation -- all\nwithin task and onboard resource constraints. Towards this goal, we introduce a\ngenerative framework leveraging flow matching for online robot dynamics model\nalignment. Rather than executing actions based on a misaligned model, our\napproach refines planned actions to better match with those the robot would\ntake if its model was well aligned. We find that by transforming the actions\nthemselves rather than exploring with a misaligned model -- as is traditionally\ndone -- the robot collects informative data more efficiently, thereby\naccelerating learning. Moreover, we validate that the method can handle an\nevolving and possibly imperfect model while reducing, if desired, the\ndependency on replay buffers or legacy model snapshots. We validate our\napproach using two platforms: an unmanned ground vehicle and a quadrotor. The\nresults highlight the method's adaptability and efficiency, with a record\n34.2\\% higher task success rate, demonstrating its potential towards enabling\ncontinual robot learning. Code:\nhttps://github.com/AlejandroMllo/action_flow_matching.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7ebf\u4f18\u5316\u673a\u5668\u4eba\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u901a\u8fc7\u8c03\u6574\u52a8\u4f5c\u800c\u975e\u63a2\u7d22\u6765\u63d0\u9ad8\u6570\u636e\u6548\u7387\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5982\u6a21\u578b\u52a8\u6001\u4f18\u5316\u3001\u5b89\u5168\u9002\u5e94\u3001\u707e\u96be\u6027\u9057\u5fd8\u7b49\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u50cf\u4eba\u7c7b\u4e00\u6837\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u91c7\u7528\u751f\u6210\u6846\u67b6\u548c\u6d41\u5339\u914d\u6280\u672f\uff0c\u4f18\u5316\u52a8\u4f5c\u4ee5\u5339\u914d\u7406\u60f3\u6a21\u578b\uff0c\u51cf\u5c11\u5bf9\u4f20\u7edf\u63a2\u7d22\u65b9\u5f0f\u6216\u5386\u53f2\u6570\u636e\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u65e0\u4eba\u8f66\u548c\u56db\u65cb\u7ffc\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e8634.2%\uff0c\u663e\u793a\u4e86\u9ad8\u6548\u7684\u6570\u636e\u6536\u96c6\u548c\u5b66\u4e60\u52a0\u901f\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u73b0\u4e86\u673a\u5668\u4eba\u6301\u7eed\u5b66\u4e60\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u9002\u5e94\u6027\u548c\u6548\u7387\u65b9\u9762\u7684\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2504.18497", "pdf": "https://arxiv.org/pdf/2504.18497", "abs": "https://arxiv.org/abs/2504.18497", "authors": ["Yifeng Mao", "Bozhidar Stevanoski", "Yves-Alexandre de Montjoye"], "title": "DeSIA: Attribute Inference Attacks Against Limited Fixed Aggregate Statistics", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Empirical inference attacks are a popular approach for evaluating the privacy\nrisk of data release mechanisms in practice. While an active attack literature\nexists to evaluate machine learning models or synthetic data release, we\ncurrently lack comparable methods for fixed aggregate statistics, in particular\nwhen only a limited number of statistics are released. We here propose an\ninference attack framework against fixed aggregate statistics and an attribute\ninference attack called DeSIA. We instantiate DeSIA against the U.S. Census\nPPMF dataset and show it to strongly outperform reconstruction-based attacks.\nIn particular, we show DeSIA to be highly effective at identifying vulnerable\nusers, achieving a true positive rate of 0.14 at a false positive rate of\n$10^{-3}$. We then show DeSIA to perform well against users whose attributes\ncannot be verified and when varying the number of aggregate statistics and\nlevel of noise addition. We also perform an extensive ablation study of DeSIA\nand show how DeSIA can be successfully adapted to the membership inference\ntask. Overall, our results show that aggregation alone is not sufficient to\nprotect privacy, even when a relatively small number of aggregates are being\nreleased, and emphasize the need for formal privacy mechanisms and testing\nbefore aggregate statistics are released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u56fa\u5b9a\u6c47\u603b\u7edf\u8ba1\u6570\u636e\u7684\u63a8\u7406\u653b\u51fb\u6846\u67b6DeSIA\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8bc6\u522b\u6613\u53d7\u653b\u51fb\u7528\u6237\u65b9\u9762\u7684\u9ad8\u6548\u6027\uff0c\u5e76\u5f3a\u8c03\u4ec5\u9760\u6570\u636e\u805a\u5408\u4e0d\u8db3\u4ee5\u4fdd\u62a4\u9690\u79c1\uff0c\u9700\u7ed3\u5408\u6b63\u5f0f\u9690\u79c1\u673a\u5236\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6216\u5408\u6210\u6570\u636e\u53d1\u5e03\u7684\u9690\u79c1\u98ce\u9669\u8bc4\u4f30\uff0c\u4f46\u5bf9\u56fa\u5b9a\u6c47\u603b\u7edf\u8ba1\u6570\u636e\u7684\u9690\u79c1\u653b\u51fb\u65b9\u6cd5\u7814\u7a76\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u4ec5\u53d1\u5e03\u5c11\u91cf\u7edf\u8ba1\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aDeSIA\u7684\u5c5e\u6027\u63a8\u7406\u653b\u51fb\u6846\u67b6\uff0c\u5e76\u5e94\u7528\u4e8e\u7f8e\u56fd\u4eba\u53e3\u666e\u67e5PPMF\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u5176\u5728\u4e0d\u540c\u7edf\u8ba1\u6570\u91cf\u548c\u566a\u58f0\u6c34\u5e73\u4e0b\u7684\u8868\u73b0\u3002", "result": "DeSIA\u5728\u8bc6\u522b\u6613\u53d7\u653b\u51fb\u7528\u6237\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u771f\u9633\u6027\u7387\u4e3a0.14\uff0c\u5047\u9633\u6027\u7387\u4e3a$10^{-3}$\uff0c\u4e14\u5728\u65e0\u6cd5\u9a8c\u8bc1\u5c5e\u6027\u7684\u7528\u6237\u7fa4\u4e2d\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u9760\u6570\u636e\u805a\u5408\u65e0\u6cd5\u6709\u6548\u4fdd\u62a4\u9690\u79c1\uff0c\u5c24\u5176\u5728\u53d1\u5e03\u5c11\u91cf\u7edf\u8ba1\u6570\u636e\u65f6\uff0c\u9700\u7ed3\u5408\u6b63\u5f0f\u9690\u79c1\u673a\u5236\u5e76\u8fdb\u884c\u6d4b\u8bd5\u3002"}}
{"id": "2504.18273", "pdf": "https://arxiv.org/pdf/2504.18273", "abs": "https://arxiv.org/abs/2504.18273", "authors": ["Jonathan Kouchly", "Ben Finkelshtein", "Michael Bronstein", "Ron Levie"], "title": "Efficient Learning on Large Graphs using a Densifying Regularity Lemma", "categories": ["cs.SI", "cs.LG"], "comment": null, "summary": "Learning on large graphs presents significant challenges, with traditional\nMessage Passing Neural Networks suffering from computational and memory costs\nscaling linearly with the number of edges. We introduce the Intersecting Block\nGraph (IBG), a low-rank factorization of large directed graphs based on\ncombinations of intersecting bipartite components, each consisting of a pair of\ncommunities, for source and target nodes. By giving less weight to non-edges,\nwe show how to efficiently approximate any graph, sparse or dense, by a dense\nIBG. Specifically, we prove a constructive version of the weak regularity\nlemma, showing that for any chosen accuracy, every graph, regardless of its\nsize or sparsity, can be approximated by a dense IBG whose rank depends only on\nthe accuracy. This dependence of the rank solely on the accuracy, and not on\nthe sparsity level, is in contrast to previous forms of the weak regularity\nlemma. We present a graph neural network architecture operating on the IBG\nrepresentation of the graph and demonstrating competitive performance on node\nclassification, spatio-temporal graph analysis, and knowledge graph completion,\nwhile having memory and computational complexity linear in the number of nodes\nrather than edges.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aIBG\u7684\u56fe\u5206\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u9ad8\u6548\u903c\u8fd1\u5927\u578b\u6709\u5411\u56fe\uff0c\u5e76\u5728\u8282\u70b9\u5206\u7c7b\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u6d88\u606f\u4f20\u9012\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u5927\u578b\u56fe\u65f6\u5b58\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u56fe\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86IBG\uff08Intersecting Block Graph\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u56fe\u7684\u4f4e\u79e9\u5206\u89e3\u548c\u5bc6\u96c6\u8fd1\u4f3c\u6765\u4f18\u5316\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u8282\u70b9\u5206\u7c7b\u7b49\u4efb\u52a1\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u8282\u70b9\u6570\u800c\u975e\u8fb9\u6570\u5448\u7ebf\u6027\u5173\u7cfb\u3002", "conclusion": "IBG\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u56fe\u8868\u793a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u7a00\u758f\u6216\u5bc6\u96c6\u56fe\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002"}}
{"id": "2504.18323", "pdf": "https://arxiv.org/pdf/2504.18323", "abs": "https://arxiv.org/abs/2504.18323", "authors": ["Yangyang Xu", "Kexin Li", "Li Yang", "You-Wei Wen"], "title": "Outlier-aware Tensor Robust Principal Component Analysis with Self-guided Data Augmentation", "categories": ["math.NA", "cs.CV", "cs.LG", "cs.NA", "65K10, 15A69", "I.4.5; G.1.6"], "comment": "12 pages, 6 figures, 3 tables", "summary": "Tensor Robust Principal Component Analysis (TRPCA) is a fundamental technique\nfor decomposing multi-dimensional data into a low-rank tensor and an outlier\ntensor, yet existing methods relying on sparse outlier assumptions often fail\nunder structured corruptions. In this paper, we propose a self-guided data\naugmentation approach that employs adaptive weighting to suppress outlier\ninfluence, reformulating the original TRPCA problem into a standard Tensor\nPrincipal Component Analysis (TPCA) problem. The proposed model involves an\noptimization-driven weighting scheme that dynamically identifies and\ndownweights outlier contributions during tensor augmentation. We develop an\nefficient proximal block coordinate descent algorithm with closed-form updates\nto solve the resulting optimization problem, ensuring computational efficiency.\nTheoretical convergence is guaranteed through a framework combining block\ncoordinate descent with majorization-minimization principles. Numerical\nexperiments on synthetic and real-world datasets, including face recovery,\nbackground subtraction, and hyperspectral denoising, demonstrate that our\nmethod effectively handles various corruption patterns. The results show the\nimprovements in both accuracy and computational efficiency compared to\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u5f15\u5bfc\u6570\u636e\u589e\u5f3a\u7684\u81ea\u9002\u5e94\u52a0\u6743TRPCA\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6291\u5236\u5f02\u5e38\u503c\u5f71\u54cd\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u6807\u51c6TPCA\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5904\u7406\u7ed3\u6784\u5316\u635f\u574f\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709TRPCA\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u5f02\u5e38\u5047\u8bbe\uff0c\u5bf9\u7ed3\u6784\u5316\u635f\u574f\u5904\u7406\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u81ea\u5f15\u5bfc\u6570\u636e\u589e\u5f3a\u548c\u81ea\u9002\u5e94\u52a0\u6743\u7b56\u7565\uff0c\u901a\u8fc7\u4f18\u5316\u9a71\u52a8\u7684\u52a0\u6743\u65b9\u6848\u52a8\u6001\u8c03\u6574\u5f02\u5e38\u503c\u6743\u91cd\uff0c\u5e76\u4f7f\u7528\u9ad8\u6548\u8fd1\u7aef\u5757\u5750\u6807\u4e0b\u964d\u7b97\u6cd5\u6c42\u89e3\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\uff08\u5982\u9762\u90e8\u6062\u590d\u3001\u80cc\u666f\u53bb\u9664\u548c\u9ad8\u5149\u8c31\u53bb\u566a\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u591a\u79cd\u635f\u574f\u6a21\u5f0f\u65f6\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u9002\u7528\u4e8e\u591a\u7ef4\u5ea6\u6570\u636e\u5206\u89e3\u4efb\u52a1\u3002"}}
{"id": "2504.18367", "pdf": "https://arxiv.org/pdf/2504.18367", "abs": "https://arxiv.org/abs/2504.18367", "authors": ["Maodong Li", "Jiying Zhang", "Bin Feng", "Wenqi Zeng", "Dechin Chen", "Zhijun Pan", "Yu Li", "Zijing Liu", "Yi Isaac Yang"], "title": "Enhanced Sampling, Public Dataset and Generative Model for Drug-Protein Dissociation Dynamics", "categories": ["physics.comp-ph", "cs.LG", "physics.chem-ph", "q-bio.BM"], "comment": "The code will be accessed from our GitHub repository\n  https://huggingface.co/SZBL-IDEA", "summary": "Drug-protein binding and dissociation dynamics are fundamental to\nunderstanding molecular interactions in biological systems. While many tools\nfor drug-protein interaction studies have emerged, especially artificial\nintelligence (AI)-based generative models, predictive tools on\nbinding/dissociation kinetics and dynamics are still limited. We propose a\nnovel research paradigm that combines molecular dynamics (MD) simulations,\nenhanced sampling, and AI generative models to address this issue. We propose\nan enhanced sampling strategy to efficiently implement the drug-protein\ndissociation process in MD simulations and estimate the free energy surface\n(FES). We constructed a program pipeline of MD simulations based on this\nsampling strategy, thus generating a dataset including 26,612 drug-protein\ndissociation trajectories containing about 13 million frames. We named this\ndissociation dynamics dataset DD-13M and used it to train a deep equivariant\ngenerative model UnbindingFlow, which can generate collision-free dissociation\ntrajectories. The DD-13M database and UnbindingFlow model represent a\nsignificant advancement in computational structural biology, and we anticipate\nits broad applicability in machine learning studies of drug-protein\ninteractions. Our ongoing efforts focus on expanding this methodology to\nencompass a broader spectrum of drug-protein complexes and exploring novel\napplications in pathway prediction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u3001\u589e\u5f3a\u91c7\u6837\u548cAI\u751f\u6210\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u7814\u7a76\u836f\u7269-\u86cb\u767d\u8d28\u7ed3\u5408/\u89e3\u79bb\u52a8\u529b\u5b66\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b13M\u5e27\u7684\u89e3\u79bb\u8f68\u8ff9\u6570\u636e\u96c6DD-13M\uff0c\u8bad\u7ec3\u4e86UnbindingFlow\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u5bf9\u836f\u7269-\u86cb\u767d\u8d28\u7ed3\u5408/\u89e3\u79bb\u52a8\u529b\u5b66\u7684\u7814\u7a76\u6709\u9650\uff0c\u5c24\u5176\u662f\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u4e00\u79cd\u65b0\u7684\u7814\u7a76\u8303\u5f0f\u63d0\u9ad8\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u5206\u5b50\u52a8\u529b\u5b66\uff08MD\uff09\u6a21\u62df\u3001\u589e\u5f3a\u91c7\u6837\u548cAI\u751f\u6210\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u91c7\u6837\u7b56\u7565\u6765\u9ad8\u6548\u6a21\u62df\u89e3\u79bb\u8fc7\u7a0b\u5e76\u4f30\u8ba1\u81ea\u7531\u80fd\u9762\uff08FES\uff09\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b26,612\u6761\u89e3\u79bb\u8f68\u8ff9\uff08\u7ea613M\u5e27\uff09\u7684DD-13M\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u4e86\u6df1\u5ea6\u7b49\u53d8\u751f\u6210\u6a21\u578bUnbindingFlow\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86DD-13M\u6570\u636e\u96c6\u548cUnbindingFlow\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u7ed3\u6784\u751f\u7269\u5b66\u7684\u7814\u7a76\u80fd\u529b\uff0c\u4e3a\u836f\u7269-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u7684\u673a\u5668\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u836f\u7269-\u86cb\u767d\u8d28\u52a8\u529b\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u672a\u6765\u5c06\u6269\u5c55\u81f3\u66f4\u591a\u590d\u5408\u4f53\u53ca\u5e94\u7528\u573a\u666f\uff08\u5982\u901a\u8def\u9884\u6d4b\uff09\u3002"}}
{"id": "2504.18391", "pdf": "https://arxiv.org/pdf/2504.18391", "abs": "https://arxiv.org/abs/2504.18391", "authors": ["Tiankai Hang", "Jianmin Bao", "Fangyun Wei", "Dong Chen"], "title": "Fast Autoregressive Models for Continuous Latent Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Autoregressive models have demonstrated remarkable success in sequential data\ngeneration, particularly in NLP, but their extension to continuous-domain image\ngeneration presents significant challenges. Recent work, the masked\nautoregressive model (MAR), bypasses quantization by modeling per-token\ndistributions in continuous spaces using a diffusion head but suffers from slow\ninference due to the high computational cost of the iterative denoising\nprocess. To address this, we propose the Fast AutoRegressive model (FAR), a\nnovel framework that replaces MAR's diffusion head with a lightweight shortcut\nhead, enabling efficient few-step sampling while preserving autoregressive\nprinciples. Additionally, FAR seamlessly integrates with causal Transformers,\nextending them from discrete to continuous token generation without requiring\narchitectural modifications. Experiments demonstrate that FAR achieves\n$2.3\\times$ faster inference than MAR while maintaining competitive FID and IS\nscores. This work establishes the first efficient autoregressive paradigm for\nhigh-fidelity continuous-space image generation, bridging the critical gap\nbetween quality and scalability in visual autoregressive modeling.", "AI": {"tldr": "FAR\u6a21\u578b\u901a\u8fc7\u66ff\u6362MAR\u7684\u6269\u6563\u5934\u4e3a\u8f7b\u91cf\u7ea7\u5feb\u6377\u5934\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fde\u7eed\u57df\u56fe\u50cf\u751f\u6210\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3MAR\u6a21\u578b\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u4e2d\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u81ea\u56de\u5f52\u6a21\u578b\u7684\u4f18\u52bf\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u5feb\u6377\u5934\u66ff\u4ee3\u6269\u6563\u5934\uff0c\u5b9e\u73b0\u9ad8\u6548\u5c11\u6b65\u91c7\u6837\uff0c\u5e76\u4e0e\u56e0\u679cTransformer\u65e0\u7f1d\u96c6\u6210\u3002", "result": "FAR\u6bd4MAR\u63a8\u7406\u901f\u5ea6\u5feb2.3\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529bFID\u548cIS\u5206\u6570\u3002", "conclusion": "FAR\u4e3a\u9ad8\u8d28\u91cf\u8fde\u7eed\u7a7a\u95f4\u56fe\u50cf\u751f\u6210\u5efa\u7acb\u4e86\u9996\u4e2a\u9ad8\u6548\u81ea\u56de\u5f52\u8303\u5f0f\uff0c\u5f25\u8865\u4e86\u89c6\u89c9\u81ea\u56de\u5f52\u5efa\u6a21\u4e2d\u8d28\u91cf\u4e0e\u53ef\u6269\u5c55\u6027\u7684\u5173\u952e\u5dee\u8ddd\u3002"}}
{"id": "2504.18444", "pdf": "https://arxiv.org/pdf/2504.18444", "abs": "https://arxiv.org/abs/2504.18444", "authors": ["Vinay Kanakeri", "Aritra Mitra"], "title": "Boosting-Enabled Robust System Identification of Partially Observed LTI Systems Under Heavy-Tailed Noise", "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "comment": null, "summary": "We consider the problem of system identification of partially observed linear\ntime-invariant (LTI) systems. Given input-output data, we provide\nnon-asymptotic guarantees for identifying the system parameters under general\nheavy-tailed noise processes. Unlike previous works that assume Gaussian or\nsub-Gaussian noise, we consider significantly broader noise distributions that\nare required to admit only up to the second moment. For this setting, we\nleverage tools from robust statistics to propose a novel system identification\nalgorithm that exploits the idea of boosting. Despite the much weaker noise\nassumptions, we show that our proposed algorithm achieves sample complexity\nbounds that nearly match those derived under sub-Gaussian noise. In particular,\nwe establish that our bounds retain a logarithmic dependence on the prescribed\nfailure probability. Interestingly, we show that such bounds can be achieved by\nrequiring just a finite fourth moment on the excitatory input process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5e7f\u4e49\u91cd\u5c3e\u566a\u58f0\u4e0b\u8bc6\u522b\u90e8\u5206\u89c2\u6d4b\u7ebf\u6027\u65f6\u4e0d\u53d8\u7cfb\u7edf\u53c2\u6570\u7684\u65b0\u7b97\u6cd5\uff0c\u6837\u672c\u590d\u6742\u5ea6\u63a5\u8fd1\u5b50\u9ad8\u65af\u566a\u58f0\u4e0b\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u8bc6\u522b\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u566a\u58f0\u4e3a\u9ad8\u65af\u6216\u5b50\u9ad8\u65af\u5206\u5e03\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u566a\u58f0\u53ef\u80fd\u4ec5\u4e3a\u4e8c\u9636\u77e9\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u653e\u5bbd\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u91c7\u7528\u9c81\u68d2\u7edf\u8ba1\u5de5\u5177\u548cBoosting\u601d\u60f3\u8bbe\u8ba1\u65b0\u7b97\u6cd5\uff0c\u4ec5\u9700\u566a\u58f0\u5b58\u5728\u4e8c\u9636\u77e9\u3002", "result": "\u6837\u672c\u590d\u6742\u5ea6\u8fb9\u754c\u63a5\u8fd1\u5b50\u9ad8\u65af\u566a\u58f0\u5047\u8bbe\u4e0b\u7684\u7ed3\u679c\uff0c\u4e14\u4ec5\u9700\u8f93\u5165\u6fc0\u52b1\u8fc7\u7a0b\u7684\u56db\u9636\u77e9\u6709\u9650\u3002", "conclusion": "\u6240\u63d0\u7b97\u6cd5\u5728\u66f4\u5f31\u7684\u566a\u58f0\u5047\u8bbe\u4e0b\u5b9e\u73b0\u4e86\u4e0e\u7ecf\u5178\u65b9\u6cd5\u5ab2\u7f8e\u7684\u6027\u80fd\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.18455", "pdf": "https://arxiv.org/pdf/2504.18455", "abs": "https://arxiv.org/abs/2504.18455", "authors": ["Milad Sefidgaran", "Abdellatif Zaidi", "Piotr Krasnowski"], "title": "Generalization Guarantees for Multi-View Representation Learning and Application to Regularization via Gaussian Product Mixture Prior", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT"], "comment": "arXiv admin note: substantial text overlap with arXiv:2502.15540", "summary": "We study the problem of distributed multi-view representation learning. In\nthis problem, $K$ agents observe each one distinct, possibly statistically\ncorrelated, view and independently extracts from it a suitable representation\nin a manner that a decoder that gets all $K$ representations estimates\ncorrectly the hidden label. In the absence of any explicit coordination between\nthe agents, a central question is: what should each agent extract from its view\nthat is necessary and sufficient for a correct estimation at the decoder? In\nthis paper, we investigate this question from a generalization error\nperspective. First, we establish several generalization bounds in terms of the\nrelative entropy between the distribution of the representations extracted from\ntraining and \"test\" datasets and a data-dependent symmetric prior, i.e., the\nMinimum Description Length (MDL) of the latent variables for all views and\ntraining and test datasets. Then, we use the obtained bounds to devise a\nregularizer; and investigate in depth the question of the selection of a\nsuitable prior. In particular, we show and conduct experiments that illustrate\nthat our data-dependent Gaussian mixture priors with judiciously chosen weights\nlead to good performance. For single-view settings (i.e., $K=1$), our\nexperimental results are shown to outperform existing prior art Variational\nInformation Bottleneck (VIB) and Category-Dependent VIB (CDVIB) approaches.\nInterestingly, we show that a weighted attention mechanism emerges naturally in\nthis setting. Finally, for the multi-view setting, we show that the selection\nof the joint prior as a Gaussians product mixture induces a Gaussian mixture\nmarginal prior for each marginal view and implicitly encourages the agents to\nextract and output redundant features, a finding which is somewhat\ncounter-intuitive.", "AI": {"tldr": "\u7814\u7a76\u5206\u5e03\u5f0f\u591a\u89c6\u70b9\u8868\u793a\u5b66\u4e60\u95ee\u9898\uff0c\u63a2\u8ba8\u65e0\u534f\u8c03\u4e0b\u5404\u4ee3\u7406\u63d0\u53d6\u5fc5\u8981\u4e14\u5145\u5206\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u57fa\u4e8e\u76f8\u5bf9\u71b5\u548cMDL\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u591a\u89c6\u70b9\u8868\u793a\u5b66\u4e60\u4e2d\u4ee3\u7406\u5982\u4f55\u72ec\u7acb\u63d0\u53d6\u5fc5\u8981\u4e14\u8db3\u591f\u7684\u4fe1\u606f\u4ee5\u4fbf\u89e3\u7801\u5668\u6b63\u786e\u4f30\u8ba1\u6807\u7b7e\u7684\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8\u65e0\u663e\u5f0f\u534f\u8c03\u60c5\u51b5\u4e0b\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u76f8\u5bf9\u71b5\u548cMDL\u7684\u6cdb\u5316\u8bef\u5dee\u8fb9\u754c\uff0c\u8bbe\u8ba1\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u6570\u636e\u4f9d\u8d56\u7684\u9ad8\u65af\u6df7\u5408\u5148\u9a8c\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728\u5355\u89c6\u70b9\u8bbe\u5b9a\u4e2d\uff0c\u6027\u80fd\u4f18\u4e8eVIB\u548cCDVIB\uff1b\u591a\u89c6\u70b9\u8bbe\u5b9a\u4e2d\uff0c\u9ad8\u65af\u4e58\u79ef\u6df7\u5408\u5148\u9a8c\u9690\u5f0f\u9f13\u52b1\u5197\u4f59\u7279\u5f81\u63d0\u53d6\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u4f9d\u8d56\u7684\u5bf9\u79f0\u5148\u9a8c\u548c\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u89c6\u70b9\u8868\u793a\u5b66\u4e60\u95ee\u9898\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u4f18\u8d8a\u6027\u548c\u53cd\u76f4\u89c9\u7684\u5197\u4f59\u7279\u5f81\u63d0\u53d6\u73b0\u8c61\u3002"}}
{"id": "2504.18461", "pdf": "https://arxiv.org/pdf/2504.18461", "abs": "https://arxiv.org/abs/2504.18461", "authors": ["Stefano Markidis", "Jonah Ekelund", "Luca Pennati", "Andong Hu", "Ivy Peng"], "title": "Discovering Governing Equations of Geomagnetic Storm Dynamics with Symbolic Regression", "categories": ["cs.CE", "cs.LG"], "comment": "Accepted for publication in the 25th International Conference on\n  Computational Science proceedings", "summary": "Geomagnetic storms are large-scale disturbances of the Earth's magnetosphere\ndriven by solar wind interactions, posing significant risks to space-based and\nground-based infrastructure. The Disturbance Storm Time (Dst) index quantifies\ngeomagnetic storm intensity by measuring global magnetic field variations. This\nstudy applies symbolic regression to derive data-driven equations describing\nthe temporal evolution of the Dst index. We use historical data from the NASA\nOMNIweb database, including solar wind density, bulk velocity, convective\nelectric field, dynamic pressure, and magnetic pressure. The PySR framework, an\nevolutionary algorithm-based symbolic regression library, is used to identify\nmathematical expressions linking dDst/dt to key solar wind. The resulting\nmodels include a hierarchy of complexity levels and enable a comparison with\nwell-established empirical models such as the Burton-McPherron-Russell and\nO'Brien-McPherron models. The best-performing symbolic regression models\ndemonstrate superior accuracy in most cases, particularly during moderate\ngeomagnetic storms, while maintaining physical interpretability. Performance\nevaluation on historical storm events includes the 2003 Halloween Storm, the\n2015 St. Patrick's Day Storm, and a 2017 moderate storm. The results provide\ninterpretable, closed-form expressions that capture nonlinear dependencies and\nthresholding effects in Dst evolution.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u7b26\u53f7\u56de\u5f52\u65b9\u6cd5\u4ece\u5386\u53f2\u6570\u636e\u4e2d\u63a8\u5bfc\u63cf\u8ff0Dst\u6307\u6570\u65f6\u95f4\u6f14\u5316\u7684\u6570\u636e\u9a71\u52a8\u65b9\u7a0b\uff0c\u5c55\u793a\u4e86\u6bd4\u4f20\u7edf\u6a21\u578b\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e2d\u7b49\u78c1\u66b4\u671f\u95f4\u3002", "motivation": "\u5730\u78c1\u66b4\u5bf9\u7a7a\u95f4\u548c\u5730\u9762\u57fa\u7840\u8bbe\u65bd\u6784\u6210\u91cd\u5927\u98ce\u9669\uff0cDst\u6307\u6570\u7528\u4e8e\u91cf\u5316\u5176\u5f3a\u5ea6\u3002\u4f20\u7edf\u6a21\u578b\u867d\u7136\u6709\u6548\u4f46\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u6570\u636e\u9a71\u52a8\u7684\u3001\u53ef\u89e3\u91ca\u7684\u6570\u5b66\u8868\u8fbe\u5f0f\u6765\u63cf\u8ff0Dst\u6307\u6570\u7684\u6f14\u5316\u3002", "method": "\u5229\u7528NASA OMNIweb\u6570\u636e\u5e93\u7684\u5386\u53f2\u6570\u636e\uff08\u5982\u592a\u9633\u98ce\u5bc6\u5ea6\u3001\u901f\u5ea6\u7b49\uff09\uff0c\u901a\u8fc7PySR\u6846\u67b6\uff08\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u7684\u7b26\u53f7\u56de\u5f52\u5e93\uff09\u751f\u6210dDst/dt\u4e0e\u592a\u9633\u98ce\u53c2\u6570\u7684\u6570\u5b66\u5173\u7cfb\u5f0f\uff0c\u5e76\u4e0eBurton-McPherron-Russell\u7b49\u4f20\u7edf\u6a21\u578b\u5bf9\u6bd4\u3002", "result": "\u7b26\u53f7\u56de\u5f52\u6a21\u578b\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u4e2d\u7b49\u78c1\u66b4\u671f\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3002\u901a\u8fc72003\u5e74\u4e07\u5723\u8282\u78c1\u66b4\u7b49\u5386\u53f2\u4e8b\u4ef6\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u6355\u6349\u4e86Dst\u6f14\u5316\u4e2d\u7684\u975e\u7ebf\u6027\u4f9d\u8d56\u548c\u9608\u503c\u6548\u5e94\uff0c\u4e3a\u5730\u78c1\u66b4\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2504.18498", "pdf": "https://arxiv.org/pdf/2504.18498", "abs": "https://arxiv.org/abs/2504.18498", "authors": ["Giuseppe Loffredo", "Elvira Romano", "Fabrizio MAturo"], "title": "Enhancing Visual Interpretability and Explainability in Functional Survival Trees and Forests", "categories": ["stat.ML", "cs.LG", "stat.ME", "62N02, 62P10, 62H30, 62G05, 62G08, 62J99", "G.3; I.5.1; I.5.2"], "comment": null, "summary": "Functional survival models are key tools for analyzing time-to-event data\nwith complex predictors, such as functional or high-dimensional inputs. Despite\ntheir predictive strength, these models often lack interpretability, which\nlimits their value in practical decision-making and risk analysis. This study\ninvestigates two key survival models: the Functional Survival Tree (FST) and\nthe Functional Random Survival Forest (FRSF). It introduces novel methods and\ntools to enhance the interpretability of FST models and improve the\nexplainability of FRSF ensembles. Using both real and simulated datasets, the\nresults demonstrate that the proposed approaches yield efficient,\neasy-to-understand decision trees that accurately capture the underlying\ndecision-making processes of the model ensemble.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u529f\u80fd\u6027\u751f\u5b58\u6811\uff08FST\uff09\u548c\u529f\u80fd\u6027\u968f\u673a\u751f\u5b58\u68ee\u6797\uff08FRSF\uff09\u4e24\u79cd\u751f\u5b58\u6a21\u578b\uff0c\u63d0\u51fa\u589e\u5f3aFST\u53ef\u89e3\u91ca\u6027\u548c\u6539\u8fdbFRSF\u53ef\u89e3\u91ca\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u771f\u5b9e\u548c\u6a21\u62df\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u529f\u80fd\u6027\u751f\u5b58\u6a21\u578b\u5728\u9884\u6d4b\u65f6\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5176\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u9650\u5236\u4e86\u5728\u5b9e\u9645\u51b3\u7b56\u548c\u98ce\u9669\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165\u4e86\u65b0\u7684\u65b9\u6cd5\u548c\u5de5\u5177\uff0c\u65e8\u5728\u589e\u5f3aFST\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u6539\u8fdbFRSF\u96c6\u5408\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u751f\u6210\u9ad8\u6548\u4e14\u6613\u4e8e\u7406\u89e3\u7684\u51b3\u7b56\u6811\uff0c\u51c6\u786e\u6355\u6349\u6a21\u578b\u96c6\u5408\u7684\u5e95\u5c42\u51b3\u7b56\u8fc7\u7a0b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u529f\u80fd\u6027\u751f\u5b58\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5176\u5728\u5b9e\u8df5\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.18513", "pdf": "https://arxiv.org/pdf/2504.18513", "abs": "https://arxiv.org/abs/2504.18513", "authors": ["Zilan Cheng", "Zhongjian Wang", "Li-Lian Wang", "Mejdi Azaiez"], "title": "PODNO: Proper Orthogonal Decomposition Neural Operators", "categories": ["math.NA", "cs.LG", "cs.NA", "physics.comp-ph", "68T07, 65M12, 41A35, 65N99"], "comment": null, "summary": "In this paper, we introduce Proper Orthogonal Decomposition Neural Operators\n(PODNO) for solving partial differential equations (PDEs) dominated by\nhigh-frequency components. Building on the structure of Fourier Neural\nOperators (FNO), PODNO replaces the Fourier transform with (inverse)\northonormal transforms derived from the Proper Orthogonal Decomposition (POD)\nmethod to construct the integral kernel. Due to the optimality of POD basis,\nthe PODNO has potential to outperform FNO in both accuracy and computational\nefficiency for high-frequency problems. From analysis point of view, we\nestablished the universality of a generalization of PODNO, termed as\nGeneralized Spectral Operator (GSO). In addition, we evaluate PODNO's\nperformance numerically on dispersive equations such as the Nonlinear\nSchrodinger (NLS) equation and the Kadomtsev-Petviashvili (KP) equation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPODNO\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408POD\u65b9\u6cd5\u6539\u8fdbFNO\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u9891\u4e3b\u5bfc\u7684PDE\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684FNO\u5728\u5904\u7406\u9ad8\u9891PDE\u95ee\u9898\u65f6\u5b58\u5728\u5c40\u9650\uff0cPODNO\u65e8\u5728\u901a\u8fc7\u66f4\u4f18\u7684POD\u57fa\u63d0\u5347\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u7528POD\u751f\u6210\u7684\uff08\u9006\uff09\u6b63\u4ea4\u53d8\u6362\u66ff\u6362FNO\u4e2d\u7684\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u6784\u5efa\u79ef\u5206\u6838\u3002", "result": "PODNO\u5728Nonlinear Schrodinger\u65b9\u7a0b\u548cKP\u65b9\u7a0b\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8eFNO\u3002", "conclusion": "PODNO\u5728\u9ad8\u9891PDE\u95ee\u9898\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e14\u5e7f\u4e49\u7248\u672cGSO\u7684\u666e\u9002\u6027\u5f97\u5230\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2504.18522", "pdf": "https://arxiv.org/pdf/2504.18522", "abs": "https://arxiv.org/abs/2504.18522", "authors": ["Julius von K\u00fcgelgen", "Jakob Ketterer", "Xinwei Shen", "Nicolai Meinshausen", "Jonas Peters"], "title": "Representation Learning for Distributional Perturbation Extrapolation", "categories": ["stat.ML", "cs.LG"], "comment": "Preprint; work presented at the ICLR Workshop on Learning Meaningful\n  Representations of Life", "summary": "We consider the problem of modelling the effects of unseen perturbations such\nas gene knockdowns or drug combinations on low-level measurements such as RNA\nsequencing data. Specifically, given data collected under some perturbations,\nwe aim to predict the distribution of measurements for new perturbations. To\naddress this challenging extrapolation task, we posit that perturbations act\nadditively in a suitable, unknown embedding space. More precisely, we formulate\nthe generative process underlying the observed data as a latent variable model,\nin which perturbations amount to mean shifts in latent space and can be\ncombined additively. Unlike previous work, we prove that, given sufficiently\ndiverse training perturbations, the representation and perturbation effects are\nidentifiable up to affine transformation, and use this to characterize the\nclass of unseen perturbations for which we obtain extrapolation guarantees. To\nestimate the model from data, we propose a new method, the perturbation\ndistribution autoencoder (PDAE), which is trained by maximising the\ndistributional similarity between true and predicted perturbation\ndistributions. The trained model can then be used to predict previously unseen\nperturbation distributions. Empirical evidence suggests that PDAE compares\nfavourably to existing methods and baselines at predicting the effects of\nunseen perturbations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6f5c\u53d8\u91cf\u6a21\u578b\u9884\u6d4b\u672a\u89c1\u6270\u52a8\u6548\u5e94\u7684\u65b9\u6cd5\uff08PDAE\uff09\uff0c\u8bc1\u660e\u5728\u8bad\u7ec3\u6270\u52a8\u8db3\u591f\u591a\u6837\u5316\u65f6\uff0c\u6270\u52a8\u6548\u5e94\u53ef\u901a\u8fc7\u4eff\u5c04\u53d8\u6362\u8bc6\u522b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5982\u4f55\u901a\u8fc7\u5df2\u77e5\u6270\u52a8\u6570\u636e\u9884\u6d4b\u65b0\u6270\u52a8\uff08\u5982\u57fa\u56e0\u6572\u9664\u6216\u836f\u7269\u7ec4\u5408\uff09\u5bf9RNA\u6d4b\u5e8f\u7b49\u4f4e\u7ef4\u6d4b\u91cf\u7684\u5f71\u54cd\u5206\u5e03\uff0c\u8fd9\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u5916\u63a8\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u6f5c\u53d8\u91cf\u6a21\u578b\uff0c\u5047\u8bbe\u6270\u52a8\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8868\u73b0\u4e3a\u5747\u503c\u5e73\u79fb\u4e14\u53ef\u53e0\u52a0\uff0c\u5e76\u5f00\u53d1PDAE\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u771f\u5b9e\u4e0e\u9884\u6d4b\u6270\u52a8\u5206\u5e03\u7684\u76f8\u4f3c\u6027\u6765\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5728\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u4e0b\u6270\u52a8\u6548\u5e94\u53ef\u8bc6\u522b\uff0c\u5b9e\u9a8c\u8868\u660ePDAE\u5728\u9884\u6d4b\u672a\u89c1\u6270\u52a8\u6548\u679c\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PDAE\u4e3a\u6270\u52a8\u6548\u5e94\u7684\u5916\u63a8\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8bc1\u652f\u6301\uff0c\u5c24\u5176\u5728\u590d\u6742\u751f\u7269\u6270\u52a8\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002"}}
